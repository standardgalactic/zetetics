
Stochastic Mechanics
Random Media
Signal Processing
and Image Synthesis
Mathematical Economics and Finance
Stochastic Optimization
Stochastic Control
Stochastic Models in Life Sciences
Stochastic Modelling
and Applied Probability
(Formerly:
Applications of Mathematics)
68
Edited by
P.W. Glynn
Y. Le Jan
Advisory Board
M. Hairer
I. Karatzas
F.P. Kelly
A. Kyprianou
B. Øksendal
G. Papanicolaou
E. Pardoux
E. Perkins
H.M. Soner

For further volumes:
www.springer.com/series/602

Carl Graham r Denis Talay
Stochastic
Simulation and
Monte Carlo
Methods
Mathematical Foundations
of Stochastic Simulation

Carl Graham
Centre de Mathématiques Appliquées
École Polytechnique, CNRS
Palaiseau, France
Denis Talay
INRIA
Sophia Antipolis, France
ISSN 0172-4568
Stochastic Modelling and Applied Probability
ISBN 978-3-642-39362-4
ISBN 978-3-642-39363-1 (eBook)
DOI 10.1007/978-3-642-39363-1
Springer Heidelberg New York Dordrecht London
Library of Congress Control Number: 2013945076
Mathematics Subject Classiﬁcation: 60H10, 65U05, 65C05, 60J30, 60E7, 65R20
© Springer-Verlag Berlin Heidelberg 2013
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection
with reviews or scholarly analysis or material supplied speciﬁcally for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of
this publication or parts thereof is permitted only under the provisions of the Copyright Law of the
Publisher’s location, in its current version, and permission for use must always be obtained from Springer.
Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations
are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of pub-
lication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any
errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect
to the material contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface
The extraordinary increase of computer capacity now encourages physicists, biolo-
gists, economists, and engineers to model and simulate numerically tremendously
complex phenomena, in order to answer scientiﬁc questions, industrial needs, and
societal requirements for risk evaluation and control. Therefore, experts in various
ﬁelds aim to solve incredibly complex high dimensional systems of partial differ-
ential equations (PDE), couplings of local stochastic dynamics and deterministic
macroscopic equations, etc.
Stochastic approaches appear to be useful, and sometimes mandatory, in the fol-
lowing two contexts. First, one cannot expect that very complex phenomena lead to
perfectly calibrated mathematical models, or even to perfect mathematical models,
so that uncertainties or stochastic components are involved in the equations. Sec-
ond, stochastic numerical methods allow one to solve deterministic problems, of
which the high dimension or singularities render classical deterministic methods of
resolution intractable or inaccurate, provided that the solutions can be represented
in terms of probability distributions of random variables or stochastic processes.
The combination of stochastic analysis and PDE theory are necessary to:
• obtain stochastic representations of solutions of deterministic PDE,
• construct effective stochastic numerical methods,
• obtain precise error estimates in terms of the numerical parameters of these meth-
ods, under the constraint that they take into account the critical situations where
the stochastic numerical methods are used and the objectives of their users.
This monograph aims to introduce the reader to these difﬁcult issues in a self-
contained way. We particularly emphasize the essential role played by martingale
theory in all the theoretical and numerical aspects of these issues. We also have de-
voted a substantial part of the book to the construction of simulation algorithms: the
readers who are mainly interested in numerical issues may skip the mathematical
proofs and concentrate on the algorithms and the convergence rate estimates.
Compared to other textbooks, this monograph presents the speciﬁcity of devel-
oping the mathematical tools which are necessary to construct effective stochastic
v

vi
Preface
simulation methods and obtain accurate error estimates, and of reuniting in a non-
classical way several advanced theoretical topics, some of which we now list.
We focus on non-asymptotic error estimates for Monte Carlo methods, on the
backward martingale technique to prove the Strong Law of Large Numbers, and on
elementary notions on logarithmic Sobolev inequalities to prove basic concentration
inequalities. We place great emphasis on the pathwise construction and simulation
of Poisson processes, discrete space Markov processes, and solutions of Itô stochas-
tic differential equations.
We use the notions of inﬁnitesimal generators and stochastic ﬂows to estab-
lish stochastic representations of parabolic partial differential equations or integro-
differential equations. We intensively use these stochastic representations of evolu-
tion equations to prove optimal error estimates for stochastic simulation methods.
As explained in Chap. 1, stochastic numerical methods are used to compute quan-
tities expressed in terms of the probability distribution of stochastic processes; we
therefore essentially consider numerical errors in the weak sense rather than in an
Lp-norm or in a pathwise sense which only provide crude information on the accu-
racy of practical simulations.
We present variance reduction techniques in nontrivial situations, which leads us
to use optimized Girsanov transformations and to introduce the reader to stochastic
optimization procedures.
For further information on the contents of the ﬁrst two chapters, the reader is ad-
vised to consult the huge literature which concerns Central Limit Theorems, Edge-
worth expansions, Large Deviations Principles, concentration inequalities, and sim-
ulation algorithms for ﬁnite-dimensional random variables. See, e.g., Devroye [10],
Feller [15, 16], Petrov [43], Shiryayev [44], and references therein.
The ﬁrst book on the discretization of stochastic differential equations is due to
Milstein [37]. Several other books have been published on this topic with a rather
different point of view to ours: most of them focus on particular applications or
various discretization methods whereas, as already emphasized, we concentrate on
the mathematical methodologies which allow one to get sharp convergence rates.
Therefore we encourage the reader to consult the selected references below, ref-
erences therein, and other useful references, to get further algorithmic or applied
information on stochastic simulations.
For time dependent models, Monte Carlo methods are derived from the simula-
tion of Markov processes, possibly discretized. In this context, e.g., Asmussen and
Glynn [4] treat the mathematics of queueing theory and some related areas, with
an emphasis on stationary regimes. Glasserman [20] focuses on numerical meth-
ods for ﬁnancial models. Kloeden and Platen [28] present an extended catalog of
variants of the Euler and Milstein discretization schemes for stochastic differential
equations. Lapeyre, Pardoux and Sentis [32] present an overview of applications
of Monte Carlo simulations of stochastic processes. Milstein and Tretyakov [38]
notably study discretization methods for stochastic differential systems with sym-
plectic structure, Hamiltonian systems, and small noise systems, layer simulation
methods and random walk simulations for stochastic systems with boundary condi-
tions. The CIME volume [21] developed the weak convergence of discretized pro-

Preface
vii
cesses, with a strong emphasis on stochastic interacting particle systems related to
non-linear partial differential equations.
We here have not tackled such important topics as Malliavin calculus techniques
to get optimal convergence rates and develop variance reduction methods, long time
simulations of ergodic Markov processes and the stochastic approximation of their
invariant probability distributions, approximation methods for reﬂected and stopped
diffusion processes, simulation of Lévy processes and discretization of Lévy driven
stochastic differential equations, quantization simulation techniques, or exact sim-
ulation methods. These subjects need mathematical tools which are beyond the ob-
jectives of this ﬁrst volume, and we will address them in a forthcoming volume.
Our other volumes will concern the stochastic simulation methods for Partial Dif-
ferential Equations with non-smooth coefﬁcients and the stochastic particle methods
for the analysis and the numerical resolution of non-linear Partial Differential Equa-
tions.
We hope that Master or Ph.D. students with notions on stochastic calculus and
researchers interested in the mathematical or numerical aspects of stochastic sim-
ulations will ﬁnd this series of monographs useful, in order to be introduced into
some advanced topics in Probability theory, to improve the accuracy and the con-
ﬁdence intervals of their simulations, or to acquire some fundamental knowledge
before reading advanced research papers on numerical probability.
Carl Graham
Denis Talay
Palaiseau, France
Sophia-Antipolis, France
February 2012

Acknowledgements
We warmly thank Benjamin Jourdain for his very valuable comments on the ﬁrst
draft of this book, and he and Caroline Hillairet for having taught its contents with
us at the École Polytechnique (Palaiseau, France).
ix

Contents
Part I
Principles of Monte Carlo Methods
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
Why Use Probabilistic Models and Simulations? . . . . . . . . . .
3
1.1.1
What Are the Reasons for Probabilistic Models? . . . . . .
4
1.1.2
What Are the Objectives of Random Simulations? . . . . .
6
1.2
Organization of the Monograph . . . . . . . . . . . . . . . . . . .
9
2
Strong Law of Large Numbers and Monte Carlo Methods . . . . . .
13
2.1
Strong Law of Large Numbers, Examples of Monte Carlo Methods
13
2.1.1
Strong Law of Large Numbers, Almost Sure Convergence .
13
2.1.2
Buffon’s Needle . . . . . . . . . . . . . . . . . . . . . . .
15
2.1.3
Neutron Transport Simulations . . . . . . . . . . . . . . .
15
2.1.4
Stochastic Numerical Methods for Partial Differential
Equations
. . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2
Simulation Algorithms for Simple Probability Distributions . . . .
18
2.2.1
Uniform Distributions . . . . . . . . . . . . . . . . . . . .
19
2.2.2
Discrete Distributions . . . . . . . . . . . . . . . . . . . .
20
2.2.3
Gaussian Distributions . . . . . . . . . . . . . . . . . . . .
21
2.2.4
Cumulative Distribution Function Inversion, Exponential
Distributions . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.2.5
Rejection Method . . . . . . . . . . . . . . . . . . . . . .
23
2.3
Discrete-Time Martingales, Proof of the SLLN . . . . . . . . . . .
25
2.3.1
Reminders on Conditional Expectation . . . . . . . . . . .
25
2.3.2
Martingales and Sub-martingales, Backward Martingales
.
27
2.3.3
Proof of the Strong Law of Large Numbers . . . . . . . . .
30
2.4
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
3
Non-asymptotic Error Estimates for Monte Carlo Methods
. . . . .
37
3.1
Convergence in Law and Characteristic Functions
. . . . . . . . .
37
3.2
Central Limit Theorem
. . . . . . . . . . . . . . . . . . . . . . .
40
3.2.1
Asymptotic Conﬁdence Intervals . . . . . . . . . . . . . .
41
3.3
Berry–Esseen’s Theorem
. . . . . . . . . . . . . . . . . . . . . .
42
xi

xii
Contents
3.4
Bikelis’ Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.4.1
Absolute Conﬁdence Intervals . . . . . . . . . . . . . . . .
45
3.5
Concentration Inequalities . . . . . . . . . . . . . . . . . . . . . .
47
3.5.1
Logarithmic Sobolev Inequalities . . . . . . . . . . . . . .
48
3.5.2
Concentration Inequalities, Absolute Conﬁdence Intervals .
50
3.6
Elementary Variance Reduction Techniques
. . . . . . . . . . . .
54
3.6.1
Control Variate . . . . . . . . . . . . . . . . . . . . . . . .
54
3.6.2
Importance Sampling
. . . . . . . . . . . . . . . . . . . .
55
3.7
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
Part II
Exact and Approximate Simulation of Markov Processes
4
Poisson Processes as Particular Markov Processes . . . . . . . . . . .
67
4.1
Quick Introduction to Markov Processes
. . . . . . . . . . . . . .
67
4.1.1
Some Issues in Markovian Modeling . . . . . . . . . . . .
67
4.1.2
Rudiments on Processes, Sample Paths, and Laws . . . . .
68
4.2
Poisson Processes: Characterization, Properties . . . . . . . . . . .
69
4.2.1
Point Processes and Poisson Processes . . . . . . . . . . .
69
4.2.2
Simple and Strong Markov Property
. . . . . . . . . . . .
75
4.2.3
Superposition and Decomposition . . . . . . . . . . . . . .
77
4.3
Simulation and Approximation
. . . . . . . . . . . . . . . . . . .
80
4.3.1
Simulation of Inter-arrivals
. . . . . . . . . . . . . . . . .
80
4.3.2
Simulation of Independent Poisson Processes . . . . . . . .
81
4.3.3
Long Time or Large Intensity Limit, Applications . . . . .
82
4.4
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
5
Discrete-Space Markov Processes . . . . . . . . . . . . . . . . . . . .
89
5.1
Characterization, Speciﬁcation, Properties
. . . . . . . . . . . . .
89
5.1.1
Measures, Functions, and Transition Matrices
. . . . . . .
89
5.1.2
Simple and Strong Markov Property
. . . . . . . . . . . .
91
5.1.3
Semigroup, Inﬁnitesimal Generator, and Evolution Law . .
95
5.2
Constructions, Existence, Simulation, Equations . . . . . . . . . .
99
5.2.1
Fundamental Constructions . . . . . . . . . . . . . . . . .
99
5.2.2
Explosion or Existence for a Markov Process . . . . . . . . 101
5.2.3
Fundamental Simulation, Fictitious Jump Method . . . . . 103
5.2.4
Kolmogorov Equations, Feynman–Kac Formula . . . . . . 105
5.2.5
Generators and Semigroups in Bounded Operator Algebras
107
5.2.6
A Few Case Studies . . . . . . . . . . . . . . . . . . . . . 112
5.3
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
6
Continuous-Space Markov Processes with Jumps . . . . . . . . . . . 121
6.1
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
6.1.1
Measures, Functions, and Transition Kernels . . . . . . . . 121
6.1.2
Markov Property, Finite-Dimensional Marginals . . . . . . 123
6.1.3
Semigroup, Inﬁnitesimal Generator . . . . . . . . . . . . . 125
6.2
Markov Processes Evolving Only by Isolated Jumps . . . . . . . . 126
6.2.1
Semigroup, Inﬁnitesimal Generator, and Evolution Law . . 126

Contents
xiii
6.2.2
Construction, Simulation, Existence . . . . . . . . . . . . . 130
6.2.3
Kolmogorov Equations, Feynman–Kac Formula, Bounded
Generator Case . . . . . . . . . . . . . . . . . . . . . . . . 133
6.3
Markov Processes Following an Ordinary Differential Equation
Between Jumps: PDMP . . . . . . . . . . . . . . . . . . . . . . . 136
6.3.1
Sample Paths, Evolution, Integro-Differential Generator . . 136
6.3.2
Construction, Simulation, Existence . . . . . . . . . . . . . 141
6.3.3
Kolmogorov Equations, Feynman–Kac Formula . . . . . . 144
6.3.4
Application to Kinetic Equations . . . . . . . . . . . . . . 146
6.3.5
Further Extensions . . . . . . . . . . . . . . . . . . . . . . 149
6.4
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
7
Discretization of Stochastic Differential Equations
. . . . . . . . . . 155
7.1
Reminders on Itô’s Stochastic Calculus . . . . . . . . . . . . . . . 155
7.1.1
Stochastic Integrals and Itô Processes . . . . . . . . . . . . 155
7.1.2
Itô’s Formula, Existence and Uniqueness of Solutions
of Stochastic Differential Equations . . . . . . . . . . . . . 160
7.1.3
Markov Properties, Martingale Problems and Fokker–
Planck Equations
. . . . . . . . . . . . . . . . . . . . . . 162
7.2
Euler and Milstein Schemes . . . . . . . . . . . . . . . . . . . . . 165
7.3
Moments of the Solution and of Its Approximations . . . . . . . . 168
7.4
Convergence Rates in Lp(Ω) Norm and Almost Surely
. . . . . . 173
7.5
Monte Carlo Methods for Parabolic Partial Differential Equations . 176
7.5.1
The Principle of the Method . . . . . . . . . . . . . . . . . 176
7.5.2
Introduction of the Error Analysis . . . . . . . . . . . . . . 177
7.6
Optimal Convergence Rate: The Talay–Tubaro Expansion . . . . . 180
7.7
Romberg–Richardson Extrapolation Methods . . . . . . . . . . . . 185
7.8
Probabilistic Interpretation and Estimates for Parabolic Partial
Differential Equations . . . . . . . . . . . . . . . . . . . . . . . . 186
7.9
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
Part III
Variance Reduction, Girsanov’s Theorem, and Stochastic
Algorithms
8
Variance Reduction and Stochastic Differential Equations . . . . . . 199
8.1
Preliminary Reminders on the Girsanov Theorem
. . . . . . . . . 199
8.2
Control Variates Method . . . . . . . . . . . . . . . . . . . . . . . 200
8.3
Variance Reduction for Sensitivity Analysis
. . . . . . . . . . . . 202
8.3.1
Differentiable Terminal Conditions . . . . . . . . . . . . . 202
8.3.2
Non-differentiable Terminal Conditions . . . . . . . . . . . 204
8.4
Importance Sampling Method . . . . . . . . . . . . . . . . . . . . 206
8.5
Statistical Romberg Method . . . . . . . . . . . . . . . . . . . . . 209
8.6
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
9
Stochastic Algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . . 213
9.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
9.2
Study in an Idealized Framework . . . . . . . . . . . . . . . . . . 214

xiv
Contents
9.2.1
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . 214
9.2.2
The Ordinary Differential Equation Method, Martingale
Increments . . . . . . . . . . . . . . . . . . . . . . . . . . 216
9.2.3
Long-Time Behavior of the Algorithm
. . . . . . . . . . . 217
9.3
Variance Reduction for Monte Carlo Methods
. . . . . . . . . . . 221
9.3.1
Searching for an Importance Sampling . . . . . . . . . . . 221
9.3.2
Variance Reduction and Stochastic Algorithms . . . . . . . 223
9.4
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
Appendix
Solutions to Selected Problems . . . . . . . . . . . . . . . . . 231
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
Index
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257

Conventions and General Notation
A probability space (Ω,F,P) will be given throughout the book. It supports all
events, random variables, and stochastic processes in question. Often, P-a.s. will be
shortened to a.s. or omitted altogether.
Often a ﬁltration (Ft) = (Ft)t∈R+ or (Fn) = (Fn)n∈N will be given on
(Ω,F,P). If not stated otherwise, processes will be assumed to be adapted to the
ﬁltration, stopping times, Brownian motions, (sub-, super-) martingales, Markov
processes will be for the ﬁltration, etc.
Similarly as for ﬁltrations, sequences and processes will be denoted indifferently
by (Xn), (Xn,n ≥1), (Xt), (Xt)t∈R+, etc. For continuous time, sample paths will
always be assumed to be right-continuous and have left limits.
In random variable or process constructions, draws are always assumed to be
independent if not stated otherwise.
Paragraphs labeled “Algorithm” describe, in a compact fashion, either a precise
mathematical construction, or with more practical details the basis for an actual
simulation algorithm.
Abbreviations
a.s.
almost surely
c.d.f.
cumulative distribution function
i.i.d.
independent identically distributed
l.h.s.
left-hand side
r.h.s.
right-hand side
r.v.
random variable
resp.
respectively
s.t.
such that
w.r.t.
with respect to
Acronyms
CLT
Central Limit Theorem
LSI
logarithmic Sobolev inequality
ODE
ordinary differential equation
xv

xvi
Conventions and General Notation
PDE
partial differential equation
SDE
stochastic differential equation
SLLN
Strong Law of Large Numbers
General Mathematical Notation
a ∨b
max(a,b) for real a and b
a ∧b
min(a,b) for real a and b
B(S )
Borel σ-ﬁeld generated by the topology of the space S
σ(S )
smallest σ-ﬁeld containing the set S
σ(X)
smallest σ-ﬁeld such that the random variable X is measurable
V
state space; it is either discrete, or a closed subset of Rd with B(V )
Number Sets, Euclidean Space, Matrix Space
C
set of complex numbers, of the form x + iy for x and y in R
N
set of natural numbers, i.e., of non-negative integers {0,1,...}
N∗
set of non-null natural numbers N −{0}, i.e., of positive integers {1,2,...}
Q, Q+
set of rational numbers, set of non-negative rational numbers
R, R+
set of real numbers, set of non-negative real numbers
Rd
Euclidean space of dimension d, with norm denoted by | · |
Rd⊗r
space of d × r real matrices M, with norm ∥M∥= Trace(MM∗)
Sample Path Spaces
C (R+,V )
space of continuous paths from R+ into V , with product σ-ﬁeld
D(R+,V )
Skorohod space of right-continuous and left limited paths, with prod-
uct σ-ﬁeld

Part I
Principles of Monte Carlo Methods
The ﬁrst part of this monograph starts by introducing and discussing the underlying
principles of Monte Carlo methods.
The companion subject of the effective simulation on computers of random vari-
ables is then developed in several directions.
Discrete-time martingale theory is introduced, and the Strong Law of Large
Numbers (SLLN) is given a proof using backward martingale convergence and the
Kolmogorov zero-one law.
The random errors in Monte Carlo approximations are then discussed.
The Central Limit Theorem is stated and proved, and asymptotic conﬁdence in-
tervals are derived from it. Then, various rigorous results yielding non-asymptotic
conﬁdence intervals are given and partly proved: Berry–Esseen’s and Bikelis’ theo-
rems, and conﬁdence intervals based on concentration inequalities.
Lastly, the difﬁcult topic of variance reduction is discussed, and several methods
are explained.

Chapter 1
Introduction
Abstract This introduction starts by describing some motivations for using proba-
bilistic models and stochastic simulations. Some examples are used to illustrate clas-
sical objectives of random simulations, and to derive relevant convergence criteria
for which error estimates need to be developed. Then the goals and the organization
of this monograph are presented.
1.1 Why Use Probabilistic Models and Simulations?
Probability theory was born from the study of games of chance and their remark-
able properties. The issue was the construction of a mathematical model able to
explain phenomena which seem universal, in the sense that they do not depend on
a particular instance of a game, e.g., the convergence of empirical frequencies to
deterministic values and the statistical properties of the random ﬂuctuations around
these limits, the occurrence of arbitrarily long sequences of events, etc.
In order to model games of chance and compute expected gains or loss risks, it
was enough to deﬁne probability distributions on ﬁnite sets. However, to analyze
the random ﬂuctuations of the gains, as well as to model measurement uncertainties
during physical experiments, the need to construct probability distributions on ﬁnite-
dimensional spaces such as Rd arose at the end of the 18th century.
These constructions could not handle questions which appeared at the beginning
of the 20th century. In order to model random time dependent quantities such as
the history of a ﬁnancial price or a temperature, one needs to construct probability
distributions on suitable inﬁnite-dimensional spaces. One can easily imagine that
a rigorous mathematical deﬁnition of “sampling a continuous function” presents
technical difﬁculties, and that other difﬁculties arise when one desires to design an
algorithm to sample such a function on a computer.
Hence, the objective of this monograph is to present:
• examples of probability distributions on the space of continuous or piecewise
continuous functions;
• examples of sampling methods for these probability distributions, and theoretical
estimates describing sampling errors;
• a selection of applications which justify the examples and the need for theoretical
sampling error estimates.
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1_1, © Springer-Verlag Berlin Heidelberg 2013
3

4
1
Introduction
The applications which we will study are typical of the situations where proba-
bilistic models are used in practice. In the next section we emphasize two important
issues which are not always well addressed in the literature, and then explain our
selection of topics, results and techniques presented in the text.
1.1.1 What Are the Reasons for Probabilistic Models?
Probabilistic models are useful for critical situations only, that is, situations where
deterministic models are inefﬁcient or irrelevant. Such situations are becoming com-
mon in fundamental sciences and in engineering: it now is reasonable to use com-
puters to simulate physical systems with large numbers of degrees of freedom, com-
plex dynamics and short time scales; however, classical modeling procedures for
such systems are inefﬁcient. Let us give a few illustrative examples:
• A physical experiment which can be described by well-established physical laws,
but for which certain model parameters are difﬁcult to calibrate with accuracy.
For example, the oil industry wishes to measure the geology and the cracks of the
subsoil. For obvious reasons, the lack or inaccuracy of data is also inherent to the
modeling of biological systems.
• A physical experiment which can be described by well-established physical laws,
but which is submitted to a huge number of rapidly and unpredictably varying
forces. For example, the shock absorbers of a car, whose mechanical fatigue stud-
ies are critical industrial issues, are submitted to the holes and bumps of the roads.
• A physical experiment which can be described by well-established physical laws,
but has a number of degrees of freedom which is so large that one cannot list the
reactions of each component of the system. For example, a huge set of colliding
electrons and nuclei.
• A phenomenon driven by incompletely known physical laws. For example, ex-
changes of energy between particles and turbulent ﬂuids, or interactions between
solvents and polymers.
• A phenomenon which is not driven by physical laws and involves a huge num-
ber of heterogeneous sources of changes. For example, the price of a ﬁnancial
asset (one cannot establish a physical law which would describe the traders’ psy-
chological behavior when critical events occur) or the size of data ﬂows on the
Internet.
In addition, the complexity of certain phenomena necessitates to couple descrip-
tions at micro and macro scales which combine random dynamics for the small
scales and macroscopic equations for the large scales: such models are used to study
the fatigue of mechanical structures with large numbers of degrees of freedom and
submitted to wind and tide effects, the motion of huge assembles of atoms consti-
tuting a polymer in a turbulent ﬂuid, etc.
So far we have not properly deﬁned the probabilistic objects involved in the mod-
els we have in mind. The above examples illustrate the fact that probabilistic models

1.1
Why Use Probabilistic Models and Simulations?
5
are often applied to time-varying phenomena. It is thus natural that stochastic pro-
cesses are essential to probabilistic modeling.
To summarize, the imperfectly known parameters and geometries are considered
as random, the poorly understood forces and the fast deformations of complex sys-
tems are described by random noises, the time evolution of systems which cannot
be accurately measured is modeled by stochastic processes. Let us illustrate this
summary by two particularly interesting examples.
Our ﬁrst example concerns Molecular Dynamics. We quote the Ph.D. thesis of
S. Park,1 prepared under the supervision of K. Schulten who is a great specialist in
the ﬁeld:
A protein is an unbranched chain of amino acids. (. . . ) When placed in water (or, for some
proteins, in a membrane), a protein folds into a particular structure which is also uniquely
determined by the amino acid sequence. (. . . ) In principle, the dynamics of biomolecules
such as proteins must be described by solving Schrödinger’s equation for every constituent
particle (nuclei and electrons). However, this is impractical because of the large size of
biomolecules. The Born–Oppenheimer approximation provides a more practical way to
model large molecules. The basic idea is that since electrons are much lighter than nuclei,
electrons can rapidly adjust to the motion of nuclei. (. . . ) A model of molecular interac-
tions in terms of nuclear coordinates (. . . ) is called a force ﬁeld. In principle, since proteins
are supposed to fold by themselves, a good force ﬁeld should be enough to obtain naive
protein structures from simulations. But, whereas protein folding typically takes millisec-
onds or longer, today’s molecular dynamics simulation is limited to the nanosecond time
scale. This is because fast vibrational motions of atoms allow only a small time step for
the integration of the equation of the motion. (. . . ) Steered Molecular Dynamics is an efﬁ-
cient method which permits us to focus on important degrees of freedom while minimizing
computational cost (. . . ) The reaction coordinate is basically the most important degree of
freedom, with the others being considered as ﬂuctuations. With all the other degrees of free-
dom averaged out, the motion along the reaction coordinate is often well approximated by
a diffusive Brownian motion on an effective potential.
Our second example concerns multi-scaled meteorological systems. The text
below2 has been written by a great specialist of stochastic models in Clima-
tology, C. Penland of the NOAA Climate Diagnostics Center (Boulder, Col-
orado):
The need to numerically model the interaction between geophysical processes having dif-
ferent timescales has led many modelers to represent rapidly varying components of the
system as stochastic forcing. The methods these stochastic modelers use to do this are
almost as numerous as the modelers themselves. Yet, there does exist a prescription for
making the stochastic approximation in a systematic manner consistent with the multi-scale
dynamics.
Many of us are familiar with some of the Central Limit Theorem, which states how sums
of weakly dependent quantities are approximately Gaussian distributed. There is another
version that states the conditions under which a multi-scale dynamical system may be ap-
1Park, S.: Extracting equilibrium from nonequilibrium: free energy calculation from steered molec-
ular dynamics simulations. Ph.D. thesis, University of Illinois at Urbana–Champaign (2004)
2Penland, C.: A stochastic approach to nonlinear dynamics: a review. Bull. Am. Meteorol. Soc. 84,
ES43–ES52 (2003)

6
1
Introduction
proximated as depending on the realizations of a white noise process, that is, as a stochastic
differential equation.
(. . . )
The importance of stochastic differential equations to probabilistic forecasting is only
now beginning to be appreciated in meteorology and climate research. The advances
in the subject have already allowed us to make real-time predictions of climate sys-
tems much more complicated than the simple univariate, additive-noise case consid-
ered by Hassellman (1976) in his landmark paper. We hope, and expect, that climate
researchers will increasingly value the great utility and intrinsic beauty of the the-
ory.
1.1.2 What Are the Objectives of Random Simulations?
We have already emphasized that Probability theory allows to model complex phe-
nomena whose states cannot be precisely deduced from accurate measurements.
The study of such phenomena cannot be classical. Stochastic modeling consists in
choosing the probability distributions of the data and aims to compute the probabil-
ity distributions of important characteristics of the phenomena under consideration.
Note that it would thus be useless and inconsistent to try to determine particular
random states, since they would not give sufﬁcient information on the desired prob-
ability distributions.
Let us revisit some of the examples we have already mentioned:
• In a (purely theoretical) perfect ﬁnancial market (that is, complete and allowing
no arbitrage), one is given a probability distribution to describe the time evolution
of future stock prices, and one desires to compute mean values of contingent
claims based on these stocks.
• In Neurosciences, one is given a probability distribution to describe the physical
and chemical reactions along an axon, and one desires to determine the proba-
bility distribution of the passage times of the axon electric potential at the ﬁring
threshold.
• In network studies, one is given a probability distribution to describe the sizes
and the emission dates of packets of bits sent to a network and one desires to
estimate the probability that the network falls into congestion within a certain
time interval.
To summarize, one desires statistical information on the system under considera-
tion: the mean values of the velocity, the position, the energy, of the price; the prob-
abilities of critical events such as long cracks, large ﬁnancial losses, large number of
individuals infected by an epidemic; the density of locations and speeds of particles,
the probability distribution of neuronal ﬁrings. In all cases, one needs to compute a
probability distribution, or an expectation, or probabilities of critical events.
The preceding quantities may often be obtained by solving deterministic equa-
tions. For example, contingent claims prices can often be expressed in terms of
solutions of parabolic partial differential equations. It is actually the case of any

1.1
Why Use Probabilistic Models and Simulations?
7
quantity of the type EΨ (XT ), where Ψ is a given function and (Xt,t ∈[0,T ]) the
solution of a stochastic differential equation.
The numerical resolution of these partial differential equations by deterministic
methods is possible (and recommended) when the dimension of the state space is
small enough. When the state space dimension increases, typically the numerical
complexity of deterministic methods explodes and stochastic numerical methods are
necessary to obtain quantitative information on probabilistic models and to solve
fully deterministic equations, including equations describing purely deterministic
phenomena.
The simplest situation of a deterministic quantity which requires probabilistic ap-
proximations is the following one: given an integrable function f in the hypercube
[0,1]d, one desires to approximate the integral
I :=

[0,1]d f (x1,...,xd)dx1 ···dxd.
A classic deterministic method is the following.
Algorithm (Quadrature method) Discretize the hypercube to obtain a set of points
(αℓ
1,...,αℓ
d) in [0,1]d and choose weights ρℓ, appropriately for 1 ≤ℓ≤L; approx-
imate I by the weighted sum
L

ℓ=1
ρℓf

αℓ
1,...,αℓ
d

.
How should one choose the number of points L?
The answer depends on the desired accuracy and on the dimension d ≥1: typi-
cally, given the function f , if the desired accuracy ε > 0 requires that the quadrature
method under consideration uses M(ε) points when d = 1, then for the same accu-
racy M(ε)d points are necessary for dimension d.
Even for d = 6 (the dimension of the phase space in Mechanics) this number of
points may be much too large to make the computation possible. Worse, in Statistical
Physics the dimension d may well be of the order of hundreds or thousands.
In addition, the accuracies of classical quadrature methods are sensitive to the
smoothness of the function f .
A probabilistic approach allows one to develop numerical methods whose
complexity increases linearly only w.r.t. d and whose accuracy depends on the
L2([0,1]d) norm of f rather than the L∞([0,1]d) norm of its derivatives. Indeed,
the integral I admits the following probabilistic representation:
I = Ef (U1,...,Ud),
where the Ui are independent random variables uniformly distributed on [0,1]. The
Strong Law of Large Numbers yields that I can be approximated as follows.

8
1
Introduction
Algorithm (Monte Carlo method) Simulate independent and uniformly distributed
on [0,1] random variables U(ℓ)
i
for 1 ≤i ≤d and 1 ≤ℓ≤N. Approximate I by the
random sum
1
N
N

ℓ=1
f

U(ℓ)
1 ,...,U(ℓ)
d

.
Such a technique, consisting in approximating a deterministic quantity by an
average of random values, is called a Monte Carlo method; N should be “large
enough” to obtain a good accuracy, and typically increases linearly in d.
We cannot limit ourselves to consider this elementary situation. As we have seen
in the preceding subsection, probabilistic models are often based on stochastic pro-
cesses; therefore, we need to understand how to develop Monte Carlo methods based
on sampled stochastic processes. In order to give a rigorous meaning to “sampling
a stochastic process”, we will deﬁne the probability law of a stochastic process as a
probability measure on the space of the trajectories of the process.
We will limit ourselves to processes which are obtained by transformations of the
Poisson process or of the Brownian motion, in particular the solutions of stochastic
differential equations, because they are the core of most of the stochastic simulations
used by practitioners.
We will carefully study the convergence rates of Monte Carlo methods in terms
of the number of the simulations and other numerical parameters, particularly the
discretization step of stochastic differential equations. We emphasize that our error
analyses deeply reﬂect the objective of the numerical procedure, that is, the approxi-
mation of expectations of random variables or of functionals of stochastic processes.
To illustrate the relevance of our goal, let us quote the recommendations of the
“Theory and Modeling in Nanoscience Report of the May 10–11 2002 Workshop”
edited by the Basic Energy Sciences and Advanced Scientiﬁc Computing Advisory
Committees to the Ofﬁce of Science, U.S. Department of Energy:
It was noted that upscaling (going to coarser length and time scales) typically results in the
introduction of stochastic terms to reﬂect high-frequency motion at smaller scales; thus,
more rigorous methods for including stochasm and more effective methods for charac-
terizing and solving stochastic differential equations are required. New developments in
nonequilibrium statistical mechanics, both of classical and quantum systems, are needed.
For example, the recent discovery by nonequilibrium molecular dynamics, conﬁrmed by
nonlinear response theory, of violations of the second law of thermodynamics for nanoscale
systems indicates the importance of new theoretical developments focused on nanoscale
systems. Quantifying errors in calculated properties was identiﬁed as a signiﬁcant problem
that consists of two parts characterization of uncertainty due to inaccuracies inherent in the
calculation (such as limitations of a force ﬁeld in molecular dynamics or of a basis set in
electronic structure calculations) and performing useful sensitivity analyses. The important
aspect of nanoscale systems that makes them such a Theory Model Simulation (TMS) chal-
lenge is that the characterization of uncertainty will often have to be done in the absence
of experimental data, since many of the property measurement experiments one would like
to perform on nanoscale systems are impossible in many cases or unlikely to be done in
cases where they are possible. Hence, the concept of self-validating TMS methods arises as
a signiﬁcant challenge in nanoscience. (. . . )

1.2
Organization of the Monograph
9
Some degree of stochasticity is intrinsic at atomistic scales; hence, a Monte Carlo simula-
tion will yield a ﬂuctuation with statistics that must be carried to larger length scales. The
numerical analysis of stochastic partial differential equations could enter here, the goal be-
ing a coarse solution with the right statistics. The theory of large deviation for stochastic
PDEs allows the designing of sampling methods for so-called rare events. The long time
integrations that are burdensome to the accuracy of deterministic models may actually be
a beneﬁt to the accuracy of statistical models. It is axiomatic to a mathematician that there
are better ways than brute-force propagation of ﬁne scales to attack a multiscale problem. A
mathematician would argue that anyone requiring a billion degrees of freedom for a week of
wall clock execution time is running the wrong algorithm. Indeed, the nanoscale community
must learn to compute smarter and not just harder.
1.2 Organization of the Monograph
This monograph is organized so as to fulﬁll two main objectives:
• to describe the fundamental probabilistic numerical methods and analyze their
convergence rates in terms of the numerical parameters of the simulations: e.g.,
the number of independent samples used in a Monte Carlo method and the time
discretization step used in the discretization of a stochastic differential equation,
• to introduce in a self-contained way the advanced notions in Probability Theory
and Stochastic Calculus (concentration inequalities, stochastic ﬂows, probabilis-
tic interpretation of partial differential equations, etc.) which are necessary to
obtain sharp convergence rates.
In this perspective, the bibliography is voluntarily limited, and some ﬂeeting ref-
erences occur in footnotes. Exercises are proposed within the text, and usually bring
complementary results. In addition, each chapter ends with problems which allow
the reader to deepen the topics of the current chapter; the most difﬁcult ones are
ﬂagged by the symbol (⋆).
We now give a list of the various topics treated in this monograph after this
introduction (Chap. 1).
Chapter 2 deals with the principles of Monte Carlo methods based on the Strong
Law of Large Numbers (SLLN). A number of examples are described, some coming
from concrete important applications. The description of algorithms of simulation
of random variables follows; these are involved in the probabilistic numerical tech-
niques which will be developed. The SLLN is proved using martingale techniques.
These techniques play an essential role in modern Probability theory. Our presenta-
tion thus allows the reader to grow familiar with martingale theory, which is a core
tool of the book.
Chapter 3 deals with the convergence rates for Monte Carlo methods. Its contents
are different of most textbooks on Probability, which usually (and reasonably!) limit
themselves to the classic Central Limit Theorem. Here, our goal is to provide the-
oretical results allowing to estimate the number of Monte Carlo simulations which
are necessary to obtain a desired accuracy with a prescribed conﬁdence interval. We

10
1
Introduction
therefore develop non-asymptotic versions of the Central Limit Theorem (Berry–
Esseen’s and Bikelis’ theorems, and concentration inequalities). We also tackle the
difﬁcult subject of variance reduction techniques for Monte Carlo methods.
Chapter 4 ﬁrst introduces some practical and theoretical issues of modeling by
means of Markov processes. Point processes are introduced in order to model jump
instants. The Poisson process is then characterized as a point process without mem-
ory. The rest of the chapter consists in its rather detailed study, including various
results concerning its simulation and approximation. This study is essential to un-
derstand the abstract constructions and the simulation methods for jump Markov
processes developed in the following chapters.
Chapter 5 constitutes a rather detailed study of Markov processes with discrete
state space. It focuses on sample path techniques in a perspective inspired by sim-
ulation needs. The relationship of these processes with Poisson processes and with
discrete-time Markov chains is shown. Rigorous constructions and results are pro-
vided for Markov process with uniformly bounded jump rates. To this end, elements
of the theory of bounded operators are introduced, which explain the relation be-
tween generator and semigroup, and provide a useful framework for the forward
and backward Kolmogorov equations and the Feynman–Kac formula.
From Chap. 6 on, Markov processes with values in continuous space (Rd or
one of its closed subsets) are considered. Their rigorous study requires advanced
measure-theoretic tools, but we limit ourselves to developing the reader’s intuition,
notably by pathwise constructions leading to simulations.
In Chap. 6 we ﬁrst emphasize the strong similarity between such Markov pro-
cesses with constant trajectories between isolated jumps and discrete space ones.
We then introduce Markov processes with sample paths following an ordinary dif-
ferential equation between isolated jumps; these are often called piecewise deter-
ministic Markov processes (PDMP). In both cases, the Kolmogorov equations and
Feynman–Kac formula are established. This is applied to kinetic equations coming
from statistical Mechanics. These describe the time evolution of the instantaneous
distribution of particles in phase space (position-velocity), when the particle veloc-
ity jumps at random instants in function of the particle position and velocity.
Chapter 7 develops discretization schemes for stochastic differential equations
and their applications to the probabilistic numerical resolution of deterministic
parabolic partial differential equations. It starts with some important properties of
Itô’s Brownian stochastic calculus, and the existence and uniqueness theorem for
stochastic differential equations with Lipschitz coefﬁcients.
Then, using probabilistic techniques only, existence, uniqueness, and smoothness
properties are proved for solutions of parabolic partial differential equations. To this
end, we show that stochastic differential equations with smooth coefﬁcients deﬁne
stochastic ﬂows, and we prove some properties of such ﬂows. We are then in a
position to prove an optimal convergence rate result for the discretization schemes.
Chapter 8 deepens the variance reduction subject and focuses on the Monte Carlo
methods for deterministic parabolic partial differential equations. This topic requires
advanced notions in stochastic calculus, particularly the Girsanov theorem, that we
state and discuss at the beginning of the chapter.

1.2
Organization of the Monograph
11
Chapter 9 introduces a few theoretical and practical issues raised by stochastic
optimization algorithms. These algorithms are efﬁcient numerical tools in various
applications and they are at the basis of the variance reduction techniques studied
in the preceding chapter. We limit ourselves to a simple framework which allows
to obtain convergence results by means of the dynamical system and martingale
theories, without too many technical details.
Remark We do not tackle the difﬁcult subject of testing, validating and compar-
ing various simulation methods on full-scale problems encountered in Engineering,
Physics, Biology, etc. Many academic papers report on numerical experiments for
toy models. However we emphasize that each full-scale problem requires speciﬁc
numerical approaches and strong expertise to control the discretization error and
reduce the variance in an effective way. An entire volume written by practitioners
would be necessary to present these topics and provide useful numerical recipes.
The accuracy analysis of these numerical methods relies on the theoretical method-
ology which we develop here.
We also emphasize that full-scale stochastic problems concern complex models
and, therefore, the question of accuracy should be completed by the question of
robustness of the results with respect to modeling errors.

Chapter 2
Strong Law of Large Numbers and Monte Carlo
Methods
Abstract The principles of Monte Carlo methods based on the Strong Law of Large
Numbers (SLLN) are detailed. A number of examples are described, some of which
correspond to concrete problems in important application ﬁelds. This is followed by
the discussion and description of various algorithms of simulation, ﬁrst for uniform
random variables, then using these for general random variables. Eventually, the
more advanced topic of martingale theory is introduced, and the SLLN is proved
using a backward martingale technique and the Kolmogorov zero-one law.
2.1 Strong Law of Large Numbers, Examples of Monte Carlo
Methods
The fundamental result for the numerical probability ﬁeld is the Strong Law of Large
Numbers, which will be proved at the end of the chapter.
2.1.1 Strong Law of Large Numbers, Almost Sure Convergence
A fundamental convergence result will now be stated.
Theorem 2.1 (Strong Law of Large Numbers) Let (ξ(ℓ),ℓ≥1) be a sequence of
independent and identically distributed random variables with values in Rd. Assume
that
E
ξ(1) < ∞.
(2.1)
For N ≥1, denote the empirical mean of (ξ(1),...,ξ(N)) by
ˆSN := 1
N
N

ℓ=1
ξ(ℓ).
Then, the Strong Law of Large Numbers holds true:
lim
N→∞
ˆSN = E

ξ(1)
,
P-a.s.
(2.2)
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1_2, © Springer-Verlag Berlin Heidelberg 2013
13

14
2
Strong Law of Large Numbers and Monte Carlo Methods
Remark 2.1 The Strong Law of Large Numbers admits a reciprocal statement,
which we admit: if E|ξ(1)| = ∞then the sequence ( ˆSN,N ≥1) diverges P-a.s.
The Strong Law of Large Numbers can be stated as follows: the sequence of
empirical means ( ˆSN(ω),N ≥1) converges to E(ξ(1)) almost surely in ω. This is a
particular case of almost sure convergence, the deﬁnition of which we now recall.
Deﬁnition 2.1 A property is said to hold almost surely (a.s.) if it holds except on an
event of probability zero. The notation P-a.s. is used to stress the underlying proba-
bility measure. In particular, a sequence (ξN, N ≥0) of random variables converges
almost surely to a random variable ξ deﬁned on the same probability space if
P

ω ∈Ω : lim
N→∞ξN(ω) = ξ(ω)

= 1.
The Strong Law of Large Numbers is at the core of the following Monte Carlo
method. Let γ be some quantity which must be approximated numerically. Assume
that there exists a function f and a family (X(1),...,X(N)) of independent and
identically distributed random variables, which are easy to simulate on computers1
and satisfy
Ef

X(1)
= γ.
(2.3)
Then, except on an event of probability zero, γ can be approximated as follows.
Algorithm (Monte Carlo method) Draw a sample (X(1)(ω),...,X(N)(ω)), and ap-
proximate γ by the empirical mean:
γ ≃ˆSN(ω) := 1
N
N

ℓ=1
f

X(ℓ)(ω)

.
This is a “good” approximation as soon as N is chosen “large enough”. However
the SLLN does not make precise the convergence rate of ˆSN. Rigorously proving
the SLLN and ﬁnding its precise convergence rate is one of our main goals in this
chapter and the next one.
To summarize: the Monte Carlo methods in this book consist in:
• exhibiting a probabilistic representation of γ of the type (2.3) such that the
probability distribution of X(1) can efﬁciently be simulated,
• and then applying the Strong Law of Large Numbers in order to approximate γ .
1This means that there exists a low complexity algorithm for generating sequences of independent
samples from their common probability distribution.

2.1
Strong Law of Large Numbers, Examples of Monte Carlo Methods
15
Versions of the Strong Law of Large Numbers, under various sets of hypotheses
can be proved in many ways. In particular, it is unnecessary to assume that the
random variables ξ(ℓ) are independent or identically distributed.
We will prove Theorem 2.1 in Sect. 2.3. We choose to use martingale techniques
because this family of processes plays an important role in the sequel.
In the rest of this section we present some examples of Monte Carlo methods.
2.1.2 Buffon’s Needle
Divide the two-dimensional space into vertical strips whose width is 1 cm. Throw at
random a needle whose length is also 1 cm. What is the probability that the needle
intersects one of the vertical lines?
To answer this question, one needs to make precise the probabilistic model. For
instance, we deﬁne the random throwing of the needle as follows: the distance X
of the center of the needle to the next line at its left side is a random variable with
uniform distribution on [0,1], and the angle θ between the needle and the horizontal
axis is a random variable with uniform distribution on [−π
2 , π
2 ] which is indepen-
dent of X. The needle intersects a vertical line if
X(ω) ∈
	
0, 1
2 cos

θ(ω)

∪
	
1 −1
2 cos

θ(ω)

,1

.
An easy calculation then shows that the desired probability is 2
π .
In 1850, an astronomer from Zürich, R. Wolf, approximated π by the Monte
Carlo method using 5000 samples: he set ξ(ℓ)(ω) = 1 when the needle intersected a
vertical line at the sample ℓ, computed the average
2 × 5000
ξ(1) + ··· + ξ(5000) ,
and obtained 3.1596 as an approximation.
2.1.3 Neutron Transport Simulations
Consider a bounded continuous map λ from Rd × Rd to R+ (in neutron transport
theory this map is called a scattering diffusion cross-section). In addition, for any
(x,y) in Rd × Rd a continuous probability density πx,y on Rd is given.
The random time evolution of the position of a neutron is described by the solu-
tion (Xt) of the differential equation
dXt
dt = Yt,
(X0,Y0) = (x,y),
(2.4)

16
2
Strong Law of Large Numbers and Monte Carlo Methods
where the velocity (Yt) is a pure jump process in the following sense: for any ω, the
map t →Yt(ω) is piecewise constant and right continuous; the jump times of (Yt)
and the jump amplitudes are random. For any integer n and pair (x,y), the value of
(Yt) at the nth jump is a random variable with probability density πx,y, where (x,y)
is the state of (Xt,Yt) at the time immediately preceding this jump; in other words,
for any bounded continuous function f , if Sn is the nth jump time, the conditional
expectation of f (YSn) knowing that the state of (Xt,Yt) immediately before Sn is
(x,y), is equal to

Rd f (z)πx,y(z)dz.
Denote by Tn the time interval between Sn and Sn+1, that is, Tn := Sn+1 −Sn.
Knowing that the state of (Xt,Yt) at time Sn is (x′,y′), the distribution function of
the random variable Tn is
F x′,y′(t) = 1 −exp

−
 t
0
λ

Xx′,y′
s
,y′
ds

,
where Xx′,y′
s
solves (2.4) with Xx′,y′
0
= x′ and Ys = y′ for any s. Note that the
function F x′,y′ is independent of n. In addition, for any i ≥0, the random variables
Ti and YSi+1 are independent.
The stochastic process (Xt,Yt) is constructed by recursively solving (2.4) on
each time interval [Sn,Sn+1[ with Yt = YSn. The pair (Xt,Yt) is a homogeneous
Markov process, and called a transport process. When λ and π do not depend on
the space variable x, (Yt) is called a pure jump process and describes the motion of
particles in a homogeneous environment.
To simplify the notation we now limit ourselves to the case d = 1. Let g be
a function from R2 to R. Suppose that there exists a function u(t,x,y) of class
C ∞(R+ × R2), bounded with bounded derivatives of all orders, and such that
∂u
∂t (t,x,y) = y ∂u
∂x (t,x,y) −λ(x,y)u(t,x,y) + λ(x,y)

R
u(t,x,z)πx,y(z)dz,
t > 0, x ∈R, y ∈R,
(2.5)
u(0,x,y) = g(x,y).
One can show that
u(t,x,y) = Ex,yg(Xt,Yt),
(2.6)
where Ex,y denotes the conditional expectation knowing that the position and ve-
locity at time 0 respectively are x and y.
The Monte Carlo method to approximate u(t,x,y) consists in simulating large
number of trajectories of the process (Xt,Yt). The above construction of the process
provides an algorithm of simulation of each trajectory.
This topic will be further developed in Sect. 6.3.4.

2.1
Strong Law of Large Numbers, Examples of Monte Carlo Methods
17
2.1.4 Stochastic Numerical Methods for Partial Differential
Equations
The probabilistic representation (2.6) for the integro-differential equation (2.5) al-
lowed us to construct a Monte Carlo method. This methodology can be extended
to numerous linear and non-linear partial differential equations, provided that their
solutions satisfy representations of the type
u(t,x) = EΨ

Z(t,x)

,
where (Z(t,x)) is a family of suitable random variables.
Let us give an elementary example. Let ν be a strictly positive number and u(t,x)
be the solution of the heat equation
∂u
∂t (t,x) = νΔu(t,x),
∀(t,x) ∈]0,T ] × Rd,
whose initial condition u(0,·) = u0(·) is assumed, say, to be continuous and
bounded. By using the analytical expression of the Gaussian density of Wt one
readily checks that
∀(t,x) ∈[0,T ] × Rd,
u(t,x) = Eu0(x +
√
2νWt),
where (Wt) is an Rd valued standard Brownian motion (thus, for any t, the compo-
nents of the random vector Wt are independent and Gaussian, have zero mean and
variance equal to t). Therefore one can approximate u(t,x) by
1
N
N

ℓ=1
u0

x +
√
2νtξ(ℓ)(ω)

,
where the {ξ(ℓ)} are Rd valued independent Gaussian vectors with zero mean and
unit covariance matrix.
The linear parabolic partial differential equations related to European option
prices in classical diffusion models are examples of equations whose solutions ad-
mit probabilistic representations. However these representations, which are called
Feynman–Kac’s formulas, involve processes which are much more complex than
Brownian motions, that is, the solutions of stochastic differential equations (see
Chap. 7).
Another example is the Poisson equation in Rd
Lu(x) := div

a(x)∇u(x)

= f (x),
where a(x) is a real-valued function. This equation arises in various ﬁelds, e.g., in
Geophysics and in Molecular Dynamics. When the function a(x) is smooth, under

18
2
Strong Law of Large Numbers and Monte Carlo Methods
suitable other hypotheses, one can prove the following equality which is analogous
to (2.6):
u(x) =
 ∞
0
Ex

f (Xt) −

f (ξ)μ(dξ)

dt,
(2.7)
where (Xt) is the solution to a certain stochastic differential equation, Ex denotes
the conditional expectation knowing that X0 is equal to x, and μ is the limit proba-
bility law, when t tends to inﬁnity, of the law of Xt. The stochastic numerical method
combines the standard Monte Carlo method and long time simulations of (Xt): this
leads to important numerical difﬁculties which are current subjects of research.
In addition, one often needs to consider discontinuous functions a(x). For exam-
ple, in Geophysics, the discontinuities of a(x) reﬂect the soil heterogeneity. In such
cases, the formula (2.7) does not involve the solution of a classical stochastic dif-
ferential equation and, when the state space is multi-dimensional, the construction
of easy-to-simulate processes (Xt) satisfying (2.7) is being investigated by many
authors.
Stochastic numerical methods are being developed for various Partial Differential
Equations, including non-linear ones such as Boltzmann equations, Vlasov equa-
tions, Navier–Stokes equations, Burgers equation, variational inequalities, etc. This
difﬁcult subject is out of the scope of this monograph.
We conclude this subsection by emphasizing three advantages of the numerical
resolution of partial differential equations by stochastic methods: it not only allows
one to solve problems in large dimension, but also:
• Monte Carlo methods allow to compute solutions whose gradient is locally very
large: whereas deterministic methods require thin grids in the areas where the
gradient of the solution is large, stochastic particles methods are grid-free and
concentrate the simulated particles, and therefore the numerical information, in
these areas.
• Most often, Monte Carlo methods are simpler and faster to code than determin-
istic methods, and the computer programs for stochastic numerical methods are
easier to modify and adapt.
• Monte Carlo methods are naturally propitious for parallel or grid computing.
2.2 Simulation Algorithms for Simple Probability Distributions
Before introducing the more advanced material necessary for the proof of the SLLN,
we pursue the subject of stochastic simulation, which will play a key role in the
actual implementation of the Monte Carlo methods developed in the sequel.
We describe various methods to simulate samples, ﬁrst from the uniform distri-
bution on [0,1], and then from classical probability distributions on R or Rd.
For more insight in this topic, we recommend for instance the books of De-
vroye [10] and Asmussen and Glynn [4].

2.2
Simulation Algorithms for Simple Probability Distributions
19
2.2.1 Uniform Distributions
The following theorem, which has many variants (see, e.g., Kuipers and Nieder-
reiter [29]) allows one to produce sequences (un) on [0,1] which are uniformly
distributed in the following sense:
∀0 ≤a ≤b ≤1,
lim
N→∞
1
N
N

j=1
1(a,b)(uj) = b −a.
Theorem 2.2 Let θ be a positive irrational number. The sequence
un = nθ
(mod 1)
is dense everywhere in [0,1] and is uniformly distributed in [0,1].
The sequences (un) in the above theorem have poor statistical properties: in par-
ticular, two consecutive terms in the sequence are strongly correlated. In addition,
irrational numbers cannot be represented exactly in computers.
Many algorithms have been designed to generate sequences of “pseudo-random”
numbers with statistical properties close to those of sequences of samples of in-
dependent and uniformly distributed random variables. The most frequently used
pseudo-random number generators are congruential methods.
Algorithm (Congruential method) Chose a triple (a,m,v0) of integers, and com-
pute inductively the successive samples uk from the formula
vk = avk−1 = akv0
(mod m),
uk = vk
m .
In practice, often m = 2α is chosen, where α is the number of bits of the com-
puter: then the congruence calculation reduces to truncating a bit sequence. The
following simple statement indicates relevant choices of the parameters in order to
maximize the periodicity of the method.2
Proposition 2.1 If m ≥2α and α ≥4, then the period of the congruential method
is less than m
4 , and this upper bound is attained when v0 is odd and a = 3 (mod 8)
or a = 5 (mod 8).
The preceding choices do not sufﬁce to generate sequences with good statistical
properties, that is, which statistically behave as sequences of independent samples
from the uniform distribution. An example of a poor generator is
vn+1 =

216 + 3

vn

mod 232
.
2This result is originally due to M. Greenberger, “Notes on a new pseudo-random number genera-
tor”, J. Assoc. Comput. Mach. 8, 163–167 (1961).

20
2
Strong Law of Large Numbers and Monte Carlo Methods
For a survey on random number generators and a discussion on statistical tests is-
sues, see, e.g., L’Ecuyer [33] or Gentle [19] and references therein. For theoretical
issues, we refer to Niederreiter [40].
Initialization of the Samples
Monte Carlo simulations require very long sampling sequences. The root v0 of the
generator must be chosen once only, before the very ﬁrst trial.
Given a root, a good generator will produce a sampling sequence with good sta-
tistical properties; however, two different sequences issued from different roots may
be correlated.
We do not recommend the use of automatic initializations, e.g., by means of the
computer internal clock. Being able to choose the same root in several runs of a
simulation program may be useful to correct programming errors.
A Natural Question
Under which conditions is a simulation method of the uniform distribution satisfy-
ing? This is a critical issue without a universal answer.
In practice, one tests the uniform distribution hypothesis and the independence
hypothesis of sampling sequences by using classical statistical procedures such as
the Kolmogorov–Smirnov test, the χ2 test, etc.
For an extended discussion on this subject and for analyses of efﬁcient statistical
tests, see Asmussen and Glynn [4], L’Ecuyer and Hellekaleke [34] and L’Ecuyer
and Simard [35], for example.
Modern generators are often non-linear: for a survey on this issue, see for in-
stance Niederreiter and Shparlinski [41].
2.2.2 Discrete Distributions
A probability distribution on a discrete set {x1,x2,...} is given by the corresponding
probability weights p1,p2,... , and a random variable X has this distribution if
P(X = x1) = p1,
P(X = x2) = p2,
....
Such a random variable X can be simulated by the following procedure:
Algorithm (Discrete distribution) To obtain a sample x from a discrete distribution
giving weight pi to xi for i ≥1: draw a sample u from the uniform distribution
on [0,1], and set x = xn for the n ≥1 satisfying n−1
i=1 pi < u ≤n
i=1 pi.

2.2
Simulation Algorithms for Simple Probability Distributions
21
2.2.3 Gaussian Distributions
There are various simulation methods for the standard N (0,1) Gaussian distribu-
tion, with density
1
√
2π e−x2/2 on R.
An approximate simulation method uses the Central Limit Theorem, which will
be recalled in Chap. 3:
Algorithm (Gaussian distribution, approximate) To obtain an approximate sample
x of the N (0,1) distribution, draw N independent samples u1,...,uN of the uni-
form distribution on [0,1], and compute
x =

12
N
 N

i=1
ui −N
2

.
In practice, the choice N = 12 is often made: empirical studies show that the
corresponding sampling sequences have satisfying statistical properties, and the pre-
ceding formula simpliﬁes into 12
i=1 ui −6.
The Box–Muller Method
This is an exact simulation technique, which moreover seems more efﬁcient than
the preceding algorithm in terms of computational time.
Two independent samples of the N (0,1) Gaussian distribution are obtained by
using the following result (due to Wiener):
Proposition 2.2 Let U and V be independent random variables uniformly dis-
tributed on [0,1]. Then the random variables X and Y deﬁned by
X =

−2logU sin(2πV ),
Y =

−2logU cos(2πV ),
have a N (0,1) Gaussian distribution and are independent.
From this we deduce the following simulation algorithm.
Algorithm (Gaussian distribution, Box–Muller) To obtain two independent sam-
ples x and y from the N (0,1) Gaussian distribution: draw two independent sam-
ples u and v from the uniform distribution on [0,1], and compute
x =

−2logusin(2πv),
y =

−2logucos(2πv).
In order to simulate a random variable X with N (m,σ 2) Gaussian distribution,
observe that if Y is a N (0,1) Gaussian variable then X := σY + m has the ap-
propriate distribution. More generally, the following allows to simulate a Gaussian
random vector X = (X1,...,Xd) with mean vector m and covariance matrix C.

22
2
Strong Law of Large Numbers and Monte Carlo Methods
Algorithm (Gaussian vector distribution) To obtain a sample x from the N (m,C)
distribution, where m is in Rd and C is a symmetric non-negative d × d matrix:
compute
σ := (σij)1≤i,j≤d =
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
σi1 :=
Ci1
√C11 ,
1 ≤i ≤d,
σii := √Cii −i−1
j=1 |Cij|2,
1 < i ≤d,
σij := Cij −j−1
k=1 σikσjk
σjj
,
1 < j < i ≤d,
σij := 0,
i < j ≤d,
draw a vector of independent samples y = (y1,...,yd) from the N (0,1) Gaussian
distribution, and compute x = σy + m.
2.2.4 Cumulative Distribution Function Inversion, Exponential
Distributions
Monte Carlo methods for neutron transport partial differential equations, network
models, neurons ﬁring train models, etc., require to sample the exponential distribu-
tion E (λ) with parameter λ > 0, with density λe−λx1{x≥0} on R (actually, R+).
This issue will be seen to be crucial in the sequel, and we will solve it using a
general result for simulation of real random variables: the cumulative distribution
function inversion method.
Deﬁnition 2.2 The cumulative distribution function (c.d.f.) of a probability distri-
bution P on R, or of a real random variable X with distribution P , is the function
F : x ∈R →F(x) = P

(−∞,x]

= P(X ≤x).
The (left-continuous) inverse of a c.d.f. F is the function
F ←: u ∈[0,1] →F ←(u) = inf

y ∈R : F(y) ≥u

.
Note that if F is a bijection then F ←= F −1, and that the c.d.f. F characterizes
the probability distribution P .
Theorem 2.3 Let P be a probability distribution on R with c.d.f. F , and F ←be
its inverse. If U is a uniformly random variable on [0,1], then the random variable
X := F ←(U) has c.d.f. F and hence distribution P .
Exercise 2.1 Prove this result.
An important application of the c.d.f. inversion method is the simulation of ex-
ponential E (λ) random variables.

2.2
Simulation Algorithms for Simple Probability Distributions
23
Algorithm (Exponential distribution)
To obtain a sample x from the E (λ) expo-
nential distribution, λ > 0: draw a sample u from the uniform distribution on [0,1],
and compute
x = −1
λ log(u).
Another similar application is for Cauchy random variables.
Algorithm (Cauchy distribution) To obtain a sample x from the Cauchy distribu-
tion with density function
σ
π(x2+σ 2) on R, for σ > 0: draw a sample u from the
uniform distribution on [0,1], and compute x = σ tan(πu).
The actual implementation of the c.d.f. inversion method requires an explicit
representation of the function F ←(i.e., of F −1 when F is a bijection).
In practice, an alternative procedure consists in the numerical resolution of the
equation F(x) = u for any sampled value u of U, but the numerical cost may be
high. The Newton–Raphson method is an example of this procedure.
Algorithm (Newton–Raphson method) To obtain a sample x from a strictly posi-
tive density f on R, with c.d.f. F(·) =
 ·
0 f (y)dy: draw a sample u from the uni-
form distribution on [0,1], set x0 = u, compute
xk+1 = xk −F(xk) −u
f (xk)
,
k ≥0
up to the step ℓat which |xℓ+1 −xℓ| is less than a prescribed threshold, and set
x = xℓ+1.
2.2.5 Rejection Method
The rejection method is often used to simulate a random vector with density f on
Rd. It consists in choosing a density g on Rd such that:
• the random vectors with density g are easy to simulate (for instance, g is a Gaus-
sian density),
• there exists ε > 0 such that h(x) := ε f (x)
g(x) ≤1 for all x.
Then we proceed as follows.
Algorithm (Rejection method) To obtain a sample x from the density f :
• draw independent samples y from the density g and u from the uniform distribu-
tion on [0,1], then
• – if u ≤h(y) := ε f (y)
g(y) , accept the sample y for x (i.e., set x = y)
– else, reject it and start over again,
(repeat until successful).

24
2
Strong Law of Large Numbers and Monte Carlo Methods
Note that a random number of rejections occur before a sample is accepted. This
number of trials depends on the sampling sequence and on the chosen density g.
This observation is important in practice: the rejection method is efﬁcient if the
acceptation rate is high.
The justiﬁcation of the rejection method, and the control on the number of itera-
tions, is based on what follows.
Proposition 2.3 Let the random variables Y1,Y2,... have density g and U1,U2,...
be uniform on [0,1], and all be independent. Let the rank and value of the ﬁrst
accepted sample be given by the random variables
M := inf

k ≥1 : Uk ≤εf (Y)
g(Y)

,
X := YM.
Then M is a.s. ﬁnite, P(M = k) = ε(1 −ε)k−1 for k ≥1 (geometric distribution),
and X has density f and is independent of M. In particular E(M) = 1/ε.
Proof By deﬁnition, for any k ≥2 and open set A in Rd,
P(M = k,YM ∈A) = P(M = k,Yk ∈A)
=
k−1

ℓ=1
P

Uℓ> εf (Yℓ)
g(Yℓ)

P

Uk ≤εf (Yk)
g(Yk) ,Yk ∈A

and, since f is a probability density,
P

U1 ≤εf (Y)
g(Y)

=

εf (y)
g(y) g(y)dy = ε

f (y)dy = ε.
Therefore, for all k ≥1,
P(M = k,YM ∈A) = (1 −ε)k−1ε

A
f (y)
g(y) g(y)dy = ε(1 −ε)k−1

A
f (y)dy.
Choosing A = Ω leads to the distribution of M and shows that M is a.s. ﬁnite; in
addition, the product form of the right-hand side shows that M and YM are indepen-
dent. Finally, we also deduce
P(YM = A) =

A
f (y)dy,
which shows that YM has density f .
□
The preceding result and its proof can easily be extended, for instance as in the
following proposition.

2.3
Discrete-Time Martingales, Proof of the SLLN
25
Proposition 2.4 Let (S,S ) be a measurable space. Let ν and μ be two probability
measures on this space. Suppose that μ is absolutely continuous w.r.t. ν and
∃ε > 0,
h(x) := εdμ
dν (x) ≤1,
ν-a.s.
Let (Yn,In) be a sequence of independent and identically distributed random vari-
ables taking values in S × {0,1}. Suppose that the common probability distribution
of the Yi is ν and that
P(I1 = 1|Y1) = h(Y1),
a.s.
Set
M := inf{k ≥1 : Ik = 1},
X := YM.
Then M is a.s. ﬁnite, E(M) < ∞, and the probability distribution of X is μ.
2.3 Discrete-Time Martingales, Proof of the SLLN
In this section we introduce the important notion of martingale processes, which
plays a key role in the sequel. We then prove the Strong Law of Large Numbers
using a technique which relies on the convergence of a backward martingale.
Our presentation is inspired by Jacod and Protter [24]. For a rigorous construc-
tion of the abstract conditional expectation and a systematic study of discrete-time
martingales, an excellent reference book is Williams [47].
2.3.1 Reminders on Conditional Expectation
A probability space (Ω,F,P) is given. Let X be an integrable Rn-valued random
variable:
X ∈L1
Ω,F,P;Rn
.
For a sub-σ-ﬁeld G of F, the conditional expectation E(X |G ) of X knowing G
is deﬁned as (a representative of) the class of random variables in L1(Ω,G ,P;Rn)
satisfying the characteristic property
E

E(X |G )Z

= E(XZ),
∀Z ∈L∞(Ω,G ,P;R),
(2.8)
where it is in fact enough to consider all Z of the form 1A for A in G .
It can be proved that any two random variables in this class are equal except on a
null probability set. The fact that statements involving conditional expectations hold
true P-a.s. is often left implicit.

26
2
Strong Law of Large Numbers and Monte Carlo Methods
If Y is an Rk-valued random variable, the conditional expectation of X knowing
Y is deﬁned by
E(X |Y) := E

X |σ(Y)

∈L1
Ω,σ(Y),P;Rn
,
where σ(Y) is the σ-ﬁeld generated by Y, constituted of all events {Y ∈A} for sets
A in B(Rk). Doob’s lemma shows that any σ(Y)-measurable random variable is of
the form f (Y) for some Borel function f . Therefore, E(X |Y) can be characterized
as the P-a.s. unique integrable random variable of the form f (Y) for some Borel
function f satisfying
E

f (Y)g(Y)

= E

Xg(Y)

for every real bounded Borel function g.
One can then set E(X |Y = y) := f (y). Note that all suitable functions f are
identical except on sets A such that P(Y ∈A) = 0. As expected, if P(Y = y) ̸= 0
then
E(X |Y = y) = E(X1{Y=y})
P(Y = y) .
If A ∈F then P(A|G ) and P(A|Y) and P(A|Y = y) are respectively used as
notations for E(1A |G ) and E(1A |Y) and E(1A |Y = y).
We now recall some important properties of conditional expectation, which hold
P-a.s. Let G and H be two sub-σ-ﬁelds of F.
• Most properties of expectation carry over to conditional expectation: linearity,
positivity, Jensen’s and Hölder inequalities, monotone and dominated conver-
gence, Fatou’s lemma, etc.
• One may “take out what is known”: if Y is a G -measurable random variable such
that XY is integrable, then
E(XY |G ) = YE(X |G ).
(2.9)
• The “tower property” holds:
if G ⊂H
then E

E(X |H )|G

= E(X |G ).
(2.10)
In particular E(E(X |H )) = E(X) (take G = {∅,Ω}).
• Knowing facts independent of our purpose does not help:
if H is independent of σ(X,G ),
then E

X |σ(G ,H )

= E(X |G ),
(2.11)
where σ(X,G ) [resp. σ(G ,H )] denotes the smallest σ-ﬁeld containing σ(X)
and G [resp. G and H ].
Exercise 2.2 Prove the statements in this subsection using the characteristic prop-
erty of conditional expectation (2.8).

2.3
Discrete-Time Martingales, Proof of the SLLN
27
2.3.2 Martingales and Sub-martingales, Backward Martingales
Deﬁnition 2.3
Let (Fn,n ∈N) be a ﬁltration, i.e., a collection of sub-σ-ﬁelds of
F such that Fn ⊂Fn+1 for all n. Let (Mn,n ∈N) be a discrete-time process which
is (Fn)-adapted and integrable, i.e., such that Mn ∈L1(Ω,Fn,P) for all n.
Then, the process (Mn,n ∈N) is called a martingale if
Mn = E(Mn+1 |Fn),
P-a.s.,
(2.12)
and a sub-martingale if
Mn ≤E(Mn+1 |Fn),
P-a.s.
(2.13)
The related notion of super-martingale is obtained by changing the sense of the
inequality, so that (Mn,n ∈N) is a super-martingale if and only if (−Mn,n ∈N) is a
sub-martingale. Note that (Mn,n ∈N) is a martingale if and only if it is both a sub-
martingale and a super-martingale. Therefore, we concentrate on sub-martingales
for most statements, except to make explicit some reinforcements for martingales.
Exercise 2.3 Let (Mn,n ∈N) be a sub-martingale [resp. a martingale], and φ be an
increasing convex function [resp. a convex function] such that E|φ(Mn)| < ∞for
every n. Prove that (φ(Mn),n ∈N) is again a sub-martingale.
One often needs to consider martingales and sub-martingales at random times,
such as the ﬁrst time at which the process hits a given threshold. Certain speciﬁc
random times, called stopping times, have very useful properties.
Deﬁnition 2.4 A random variable T taking values in N ∪{+∞} is a stopping time
for the ﬁltration (Fn) if {T ≤n} ∈Fn for every n in N.
If T is a stopping time, then FT is deﬁned as the σ-ﬁeld constituted of all events
A in F∞:= σ(∪n∈NFn) such that A ∩{T ≤n} ∈Fn for every n in N.
Our next result generalizes the sub-martingale property.
Theorem 2.4 (Optional sampling) Let (Mn,n ∈N) be a sub-martingale. If S and T
are two stopping times satisfying S(ω) ≤T (ω) ≤K, ω-a.s., for some deterministic
integer K, then
E(MS) ≤E(MT |FS),
E(MS) ≤E(MT ).
Proof Observe that
MS =
K

j=0
Mj1{S=j}1{T ≥j}.

28
2
Strong Law of Large Numbers and Monte Carlo Methods
Since {T ≥K + 1} = ∅, expressing Mj1{T ≥j} as a telescopic sum yields
MS =
K

j=0
K

k=j
(Mk1{T ≥k} −Mk+11{T ≥k+1})1{S=j}
=
K

j=0
K

k=j
Mk1{T =k}1{S=j} +
K

j=0
K

k=j
(Mk −Mk+1)1{T ≥k+1}1{S=j}.
Moreover,
K

j=0
K

k=j
Mk1{T =k}1{S=j} = MT
K

j=0
K

k=j
1{T =k}1{S=j} = MT
and hence
MS = MT +
K

j=0
K

k=j
(Mk −Mk+1)1{T ≥k+1}1{S=j}.
Now, if A is in FS then
(MS −MT )1A =
K

j=0
K

k=j
(Mk −Mk+1)1{T ≥k+1}1{S=j}∩A.
Deﬁnition 2.4 implies that {T ≥k + 1} = {T ≤k}c ∈Fk and
{S = j} ∩A = {S ≤j} ∩A −{S ≤j −1} ∩A ∈Fj ⊂Fk
so that taking expectations, (2.13) yields
E

(MS −MT )1A

=
K

j=0
K

k=j
E

(Mk −Mk+1)1{T ≥k+1}1{S=j}∩A

≤0.
Since A is arbitrary, the characteristic property (2.8) allows to conclude the ﬁrst
inequality. The second follows by taking A = Ω.
□
Deﬁnition 2.5 Let real numbers a < b be ﬁxed. For n ≥1, the number of upcross-
ings of [a,b] between times 0 and n by a sequence of real numbers (Mk,k ∈N) is
deﬁned as
Un := max{j ≥0 : Tj ≤n}
where the (Tj)j≥0 are recursively deﬁned as follows: T0 = 0 and
τj+1 = inf{k ≥Tj : Mk ≤a},
Tj+1 = inf{k > τj+1 : Mk ≥b}.

2.3
Discrete-Time Martingales, Proof of the SLLN
29
Theorem 2.5 In the framework of Deﬁnition 2.5, if (Mk,k ∈N) is a sub-martingale,
then
E(Un) ≤
1
b −a E

(Mn −a)+
.
Proof Set Yn := (Mn −a)+. As τn+1 > n, one has Yτn+1∧n = Yn, which can be
expressed as the telescopic sum
Yn = Yτ1∧n +
n

i=1
(Yτi+1∧n −Yτi∧n)
= Yτ1∧n +
n

i=1
(Yτi+1∧n −YTi∧n) +
n

i=1
(YTi∧n −Yτi∧n).
Observe that
Yτ1∧n ≥0,
n

i=1
(YTi∧n −Yτi∧n) ≥(b −a)Un,
and thus
Yn ≥
n

i=1
(Yτi+1∧n −YTi∧n) + (b −a)Un.
The result of Question 2 of Exercise 2.3 applied to the increasing convex function
φ(x) = (x −a)+ yields that (Yn,n ∈N) is a sub-martingale. Taking expectations
and applying Theorem 2.4 to the stopping times Ti ∧n ≤τi+1 ∧n then yields that
E(Yn) ≥(b −a)E(Un),
from which the result follows.
□
The notion of ﬁltration expresses the practical fact that information is increasing
with time: the σ-ﬁelds satisfy Fn ⊂Fn+1. In the proof of Theorem 2.1, we will
consider the σ-ﬁelds σ( ˆSN, ˆSN+1,...), which decrease w.r.t. time N ≥1, but how-
ever deﬁne a ﬁltration when time is run backwards. We thus are led to introduce the
following deﬁnition.
Deﬁnition 2.6 Let (G−N,N ≥1) be a family of sub-σ-ﬁelds satisfying
G−(N+1) ⊂G−N,
N ≥1.
(2.14)
A process (M−N,N ≥1) is a (G−N)-backward martingale if the M−N are inte-
grable, G−N-measurable, and
M−(N+1) = E(M−N |G−(N+1)),
P-a.s.

30
2
Strong Law of Large Numbers and Monte Carlo Methods
Theorem 2.5 can then be easily adapted as follows. We leave the details as an
exercise for the interested reader.
Theorem 2.6 Let (G−N,N ≥1) be a family of sub-σ-ﬁelds satisfying (2.14), and
(M−N,N ≥1) be a (G−N)-backward martingale. Let a < b be ﬁxed, and for N ≥1
let U−N denote the number of upcrossings of the interval [a,b] between times 0 and
N −1 by the sequence
( ¯Mk,k ∈N) = (M−N,...,M−1,M−1,...)
(see Deﬁnition 2.5). Then
E(U−N) ≤
1
b −a E

(M−1 −a)+
.
2.3.3 Proof of the Strong Law of Large Numbers
Now, Theorem 2.1 will be proved. Recall that (ξ(ℓ),ℓ≥1) is a sequence of inde-
pendent identically distributed random variables such that E|ξ(1)| < ∞, and that
ˆSN := 1
N
N
ℓ=1 ξ(ℓ) for N ≥1.
Step 1
The key idea consists in considering
G−N := σ( ˆSN, ˆSN+1,...),
N ≥1,
which clearly satisﬁes (2.14), and observing that ( ˆSN,N ≥1) = (M−N,N ≥1) for
an appropriately deﬁned (G−N)-backward martingale (M−N,N ≥1).
Indeed, from (2.10) it follows that
M−N := E

ξ(1) |G−N

(2.15)
deﬁnes a (G−N)-backward martingale (M−N,N ≥1). In addition, by symmetry,
∀1 ≤ℓ≤N,
M−N = E

ξ(ℓ) |G−N

,
and therefore, since ˆSN is G−N measurable,
M−N = 1
N
N

ℓ=1
E

ξ(ℓ) |G−N

= E( ˆSN |G−N) = ˆSN.
Note that in particular M−1 = ξ(1).

2.3
Discrete-Time Martingales, Proof of the SLLN
31
Step 2
We prove that backward martingales such as (M−N,N ≥1) converge a.s. to an in-
tegrable random variable M−∞.
Fix a < b and use the notation of Theorem 2.6. For every ω in Ω, the increas-
ing sequence (U−N(ω),N ≥1) has a limit U−∞(ω) in R ∪{∞}. The monotone
convergence theorem and Theorem 2.6 yield
E(U−∞) = lim
N→∞E(U−N) ≤
1
b −a E

(M−1 −a)+
< ∞.
Thus the random variable U−∞is ﬁnite a.s., and the sequence (M−N,N ≥1) almost
surely crosses [a,b] a ﬁnite number of times, which implies that
P

liminf
N→∞M−N < a < b < limsup
N→∞
M−N

= 0.
This being true for all rational a < b, necessarily
P

liminf
N→∞M−N < limsup
N→∞
M−N

= 0
and thus, in R ∪{−∞,∞},
lim
N→∞M−N = M−∞:= liminf
N→∞M−N,
P-a.s.
Using (2.15) and (2.10) yields
E

|M−N|

= E
E

ξ(1) |G−N

≤E

E
ξ(1)|G−N

= E
ξ(1)
and Fatou’s lemma yields
E

|M−∞|

≤liminf
N→∞E

|M−N|

≤E
ξ(1)
< ∞.
Step 3
We show that backward martingales such as (M−N,N ≥1) converge in L1(Ω).
This readily follows from a.s. convergence and (2.15) and classic uniform inte-
grability results (see Problem 2.3), but we will prove it using less advanced notions.
For any N ≥1 and C ∈R+, it holds that
E

|M−∞−M−N|

≤E

|M−∞−M−N|1{|M−N|≤C}

+ E

|M−∞| + |M−N|

1{|M−N|>C}

.
In view of (2.15), (2.9) and (2.10),

32
2
Strong Law of Large Numbers and Monte Carlo Methods
|M−N|1{|M−N|>C} =
E

ξ(1) |G−N

1{|M−N|>C}
 =
E

ξ(1)1{|M−N|>C} |G−N

≤E
ξ(1)1{|M−N|>C} |G−N

and hence
E

|M−N|1{|M−N|>C}

≤E

E
ξ(1)1{|M−N|>C} |G−N

= E
ξ(1)1{|M−N|>C}

.
Thus, for Y := |M−∞| + |ξ(1)| it holds that
E

|M−∞−M−N|

≤E

|M−∞−M−N|1{|M−N|≤C}

+ E(Y1{|M−N|>C}).
(2.16)
Since M−∞is integrable, by the Dominated Convergence Theorem,
lim
N→∞E

|M−∞−M−N|1{|M−N|≤C}

= 0.
In addition, for any B ∈R+,
E(Y1{|M−N|>C}) ≤E(Y1{Y>B}) + BP

|M−N| > C

and by dominated convergence, since Y is integrable, for any ε > 0 one can choose
B and then C large enough to have
E(Y1{Y>B}) < ε,
BP

|M−∞| > C

< ε,
and moreover
lim
N→∞BP

|M−N| > C

= BP

|M−∞| > C

< ε.
Since ε > 0 is arbitrarily chosen, (2.16) and the results that follow it yield
lim
N→∞E

|M−∞−M−N|

= 0.
Note that this L1(Ω) convergence and E(M−N) = E(E(ξ(1) |G−N)) = E(ξ(1)),
see (2.15) and (2.10), yield that
E(M−∞) = lim
N→∞E(M−N) = E

ξ(1)
.
(2.17)
Step 4
For every k ≥1,
M−∞= lim
N→∞
ˆSN := lim
N→∞
ξ(1) + ··· + ξ(N)
N
= lim
N→∞
ξ(k) + ··· + ξ(N)
N
,
P-a.s.,
so that M−∞is measurable w.r.t. the tail σ-ﬁeld 
k≥1 σ(ξ(k),ξ(k+1),...).

2.4
Problems
33
We admit the following classic result, which is an ingredient in most proofs of
the Strong Law of Large Numbers (see, e.g., Williams [47] for a proof using mar-
tingales).
Theorem 2.7 (Kolmogorov zero-one law) Let (ξ(ℓ),ℓ≥1) be a sequence of inde-
pendent random variables, and T := 
k≥1 σ(ξ(k),ξ(k+1),...) be its tail σ-ﬁeld.
Then, any T -measurable random variable is a.s. constant.
Applying this theorem to M−∞yields that this random variable is a.s. constant.
It must then be a.s. equal to its expectation E(ξ(1)), see (2.17).
This concludes the proof of the SLLN.
2.4 Problems
2.1 (A Poisson Distribution Simulation Method) Recall that the Laplace transform
of a non-negative random variable X, or of its law PX, is given by E(e−θX) =

e−θxPX(dx) for all θ ≥0 such that the expectation is ﬁnite.
1. Compute the Laplace transform of the distribution function of the exponential
probability law with parameter λ > 0.
2. Let (Xk,k ≥1) be a family of independent random variables with the same ex-
ponential probability distribution with parameter λ > 0. Set
SN :=
N

i=1
Xk.
Compute the Laplace transform of the distribution function of SN. Deduce that
the probability density function of SN is
pN(x) :=
λN
(N −1)!xN−1e−λx1x≥0.
3. Let M be the smallest integer N such that SN+1 > λ. Show that this random
variable has a Poisson distribution.
4. Propose a simulation method of the Poisson distribution which requires samples
only of the uniform distribution on [0,1].
2.2 (Lyapunov Exponent of Linear Random Recursive Sequences) Let a and b be
two real numbers. Let 1 > h > 0 be a time discretization step. For any integer p set
¯Xh
p+1(x) =

1 + b
√
hGp+1 +

a + b2
2

h

¯Xh
p(x),
where the Gp are mutually independent and centered Gaussian random variables
with unit variance, and where ¯Xh
0(x) = x a.s.

34
2
Strong Law of Large Numbers and Monte Carlo Methods
1. Check that the functions |log(|x|)|exp(−x2) and (log(|x|))2 exp(−x2) are inte-
grable over R.
Hint: Use that (log(x))2 is the derivative of x(log(x))2 −2x log(x) + 2x.
2. Show that
∃¯λh ∈R for all x ∈Rd −{0},
¯λh =
lim
N−→+∞
1
Nh log| ¯Xh
N(x)|,
a.s.
3. Show that, for all x in R −{0},
∃Ch ∈R,
1
N2 E

log| ¯Xh
N(x)|
2 < Ch
for all N ∈N −{0}.
Problem 2.3 shows that the preceding inequality implies
∀x ∈R −{0},
¯λh =
lim
N−→+∞
1
NhElog
 ¯Xh
N(x)
.
Deduce from this result that
¯λh = 1
hElog
1 + b
√
hG1 +

a + b2
2

h

for all h small enough and all N.
4. Let
Y := b
√
hG1 +

a + b2
2

h.
Prove that
Elog|1 + Y| = E
	
Y −Y 2
2 + Y 3
3

+ E
	
1|Y|<1

log(1 + Y) −Y + Y 2
2 −Y 3
3

+ E
	
1|Y|≥1

log|1 + Y| −Y + Y 2
2 −Y 3
3

.
Deduce that ¯λh = a + O(h).
2.3 (Uniformly Integrable Random Variables (⋆)) Consider a sequence (Xn) of ran-
dom variables with ﬁnite expectations which are uniformly integrable, that is,
lim
C→∞sup
n≥0
E

|Xn|1|Xn|≥C

= 0.
(2.18)
1. Show that (2.18) is equivalent to the conjunction of the two following ones:
sup
n

E|Xn|

< ∞,
(2.19)
∀ε > 0, ∃δ(ε), ∀A ∈F,
P(A) ≤δ(ε) ⇒sup
n
E

|Xn|1A

< ε.
(2.20)

2.4
Problems
35
2. Let (Xn) be a uniformly integrable sequence, and let X be an integrable random
variable. Show that the sequence (|Xn −X|) is uniformly integrable.
Hint: Observe that |Xn −X| ≤|Xn| + |X|, and apply the preceding question.
3. Now assume in addition that (Xn) has an almost sure limit X. Show that
E|X| < ∞.
Hint: Start with observing that
E

|Xn|

= E

|Xn|1|Xn|≤C

+ E

|Xn|1|Xn|≥C

;
(2.21)
then use Fatou’s lemma and Lebesgue’s Dominated Convergence Theorem.
4. Deduce from Questions 1 and 2 that
lim
n→∞E|Xn −X| = 0.
(2.22)
Hint: Observe that, for all positive ε,
E|Xn −X| ≤E

|Xn −X|1|Xn−X|≥ε

+ ε.
5. Write an alternative proof to Step 3 in Sect. 2.3.3 for
lim
N→∞E

|M−∞−M−N|

= 0.
Hint: Show that supn≥0 E(Xn)2 < ∞implies uniform integrability.

Chapter 3
Non-asymptotic Error Estimates for Monte
Carlo Methods
Abstract In order to effectively implement Monte Carlo methods, the random ap-
proximation errors must be controlled. For this purpose, theoretical results are pro-
vided for the estimation of the number of simulations necessary to obtain a de-
sired accuracy with a prescribed conﬁdence interval. Therefore absolute, i.e., non-
asymptotic, versions of the Central Limit Theorem (CLT) are developed: Berry–
Esseen’s and Bikelis’ theorems, as well as concentration inequalities obtained from
logarithmic Sobolev inequalities. The difﬁcult subject of variance reduction tech-
niques for Monte Carlo methods arises naturally in this context, and is discussed at
the end of this chapter.
Reference books for this chapter are Petrov [43] and Shiryayev [44], as well as
Williams [47] up to the CLT result.
3.1 Convergence in Law and Characteristic Functions
In the preceding chapter we have proved the almost sure convergence of the Monte
Carlo methods. Now our goal is to describe the approximation error. Almost sure
bounds cannot be obtained; however, one can prove precise estimates on the proba-
bility distribution of this error.
Before treating this question, we start with a few reminders on the probability
laws of random variables.
Deﬁnition 3.1 Let X := (X1,...,Xd) be an Rd-valued random variable. The
law or distribution of X is the probability measure PX on the measurable space
(Rd,B(Rd)) deﬁned by
∀A ∈B

Rd
,
PX(A) := P

ω;X(ω) ∈A

.
In set-theoretic terms PX = P ◦X−1.
The following result states a classical property of laws of random variables.
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1_3, © Springer-Verlag Berlin Heidelberg 2013
37

38
3
Non-asymptotic Error Estimates for Monte Carlo Methods
Theorem 3.1 For any measurable non-negative function f from Rd to R,
Ef (X) :=

Ω
f

X(ω)

P(dω) =

Rd f (x)PX(dx).
In practice, it will often be seen that the exact Monte Carlo method cannot be im-
plemented, due to the fact that a probabilistic representation γ = Ef (X), see (2.3),
can only be found for a random variable X which is difﬁcult (or impossible) to
simulate. It is then necessary to ﬁnd some other easily simulable random variable ¯X
which satisﬁes Ef ( ¯X) ≃Ef (X). For such approximations, we introduce the funda-
mental notion of convergence in law of random variables, i.e., of weak convergence
of their laws. It will be seen that the limitations of continuity and of boundedness
on the test functions can be sometimes removed.
Deﬁnition 3.2 Let ( ¯Xn,n ∈N) be a sequence of Rd-valued random variables. It
is said to converge in law to a random variable X, or to its law PX, if the laws
(P ¯Xn,n ∈N) converge weakly to PX, i.e., if for every bounded continuous function
f from Rd to R,
lim
n→∞

f (x)P ¯Xn(dx) =

f (x)PX(dx).
In terms of the random variables themselves, this can be written as
lim
n→∞Ef
 ¯Xn
= Ef (X).
The approximation of γ = Ef (X) by a Monte Carlo method often depends on
two parameters:
• the index n of the term ¯Xn actually chosen as an approximation, in the sequence
( ¯Xn) which converges in law to X,
• the number N of the simulated independent and identically distributed copies
¯Xn(1),..., ¯Xn(N) of ¯Xn.
More precisely, the global approximation error can be decomposed as follows:
γ −1
N
N

ℓ=1
f
 ¯Xn(ℓ)
= Ef (X) −Ef
 ¯Xn

 !
"
εd(n)
+Ef
 ¯Xn
−1
N
N

ℓ=1
f
 ¯Xn(ℓ)

 !
"
εs(n,N)
,
(3.1)
• where the term εd(n) measures the approximation error on the exact probability
distribution, due to convergence in law,
• and the term εs(n,N) measures the statistical error resulting from the application
of the Strong Law of Large Numbers.
In the sequel to this monograph we study the behavior w.r.t. n and N of these
two errors. The behavior w.r.t. N is described by the Central Limit Theorem and its
variants, whose proofs rely on the notion of characteristic function.

3.1
Convergence in Law and Characteristic Functions
39
Deﬁnition 3.3 The characteristic function of the Rd-valued random variable X, or
of its probability law PX, is given by
φX : t ∈Rd →φX(t) :=

Rd eit·XPX(dx) = Eeit·X.
Indeed, Deﬁnition 3.2 is inconvenient because the space of continuous bounded
functions is huge. Fourier analysis of abstract measures allows one to limit oneself
to the parametrized family of functions x →eit·x, yielding the following important
result (see Jacod and Protter [24, Thm 19.1 p. 167], e.g.).
Theorem 3.2 A sequence ( ¯Xn) of Rd-valued random variables converges in law to
a random variable X if and only if
lim
n→∞φ ¯Xn(t) = φX(t),
∀t ∈Rd.
Notably, two random variables with the same characteristic function have the same
law.
The next classic result will be useful.
Proposition 3.1 Let X be a real-valued random variable. Suppose that there exists
an integer K in N −{0} such that E|X|K < ∞. Then the function φX is everywhere
differentiable up to the order K and
φX(t) =
K

k=1
(it)k
k! E

Xk
+ tK
K!εK(t),
∀t ∈R,
where supt∈R |εK(t)| ≤3E|X|K and limt→0 εK(t) = 0.
Proof For any x in R, Taylor’s formula with integral remainder term yields
eix = cos(x) + isin(x)
=
K−1

k=0
(ix)k
k!
+ xK
 1
0
(1 −θ)K−1
(K −1)!

cos(K)(θx) + isin(K)(θx)

dθ
and therefore the formula is valid for
εK(t) = KE

XK
 1
0
(1 −θ)K−1
cos(K)
θtX(ω)

+ isin(K)
θtX(ω)

−iK
dθ

which satisﬁes supt |εK(t)| ≤3E|X|K. One then proves that limt→0 εK(t) = 0 by
successively examining the cases where K is odd or even and by using Lebesgue’s
dominated convergence theorem.
□

40
3
Non-asymptotic Error Estimates for Monte Carlo Methods
A last result on weak convergence of real random variables involves the cumula-
tive distribution function (c.d.f.), see Deﬁnition 2.2. It will be useful for the future
notion of conﬁdence intervals, or to consider histogram convergence. See Jacod and
Protter [24, Thm 18.4 p. 153], e.g., also for the fact that a c.d.f. is right-continuous
and has left limits. Let FY denote the c.d.f. of a random variable Y.
Theorem 3.3 A sequence ( ¯Xn) of real-valued random variables converges in law
to a random variable X if and only if
lim
n→∞F ¯Xn(x) = FX(x),
∀x ∈R s.t. FX(x) = FX(x−).
In particular, if FX is continuous then there is convergence for all x.
3.2 Central Limit Theorem
The Strong Law of Large Numbers ensures the convergence of Monte Carlo meth-
ods, but does not allow one to describe the accuracy of the method or the number
N of simulations in terms of the desired accuracy. We already emphasized that the
approximation error is random. The Central Limit Theorem makes precise the limit
behavior, when N tends to inﬁnity, of the normalized error probability distribution.
Theorem 3.4 (Central Limit Theorem) Let (ξ(ℓ),ℓ≥1) be a sequence of real-
valued independent and identically distributed random variables. Assume they are
square integrable and set σ 2 := var(ξ(1)). Consider the random variables
YN :=
√
N
σ

1
N
N

ℓ=1
ξ(ℓ) −E

ξ(1)

,
N ≥1.
The sequence (YN) converges in law to the Gaussian law N (0,1).
Proof We wish to apply Theorem 3.2, and prove the convergence of the characteris-
tic function of YN to t →exp(−t2
2 ), which classically is the characteristic function
of the N (0,1) law. Set
˜ξ(ℓ) := ξ(ℓ) −E

ξ(ℓ)
and observe that, since the random variables ξ(ℓ) are independent and identically
distributed,
φYN (t) = E

exp

it
σ
√
N
N

ℓ=1
˜ξ(ℓ)

=

φ˜ξ(1)

t
σ
√
N
N
.
(3.2)
To conclude, it then remains to use Proposition 3.1 with K = 2.
□

3.2
Central Limit Theorem
41
In practice, one needs to estimate the probability that the approximation error of
the Monte Carlo method for E(ξ(1)) is larger than a desired accuracy ε > 0. This
error is given by
εs(N) := E

ξ(1)
−1
N
N

ℓ=1
ξ(ℓ) = −σ
√
N
YN,
and thus the probability that E(ξ(1)) does not belong to the interval I, centered in
1
N
N
ℓ=1 ξ(ℓ) with radius ε, should be evaluated. If this probability can be bounded
above by δ, then I is called a conﬁdence interval with risk level δ, or conﬁdence
level 1 −δ.
Our objective is to identify the dependency of each one of the parameters ε, δ,
and N in terms of the others in such circumstances.
3.2.1 Asymptotic Conﬁdence Intervals
The Central Limit Theorem is classically invoked to provide asymptotic conﬁdence
intervals, according to the following coarse analysis:
• For any “large enough” N, the probability law of εs(N) is “close to” the Gaussian
law with zero mean and variance equal to
σ
√
N , and in this “sense”, if Y∞denotes
an arbitrary N (0,1) Gaussian variable,
P

1
N
N

ℓ=1
ξ(ℓ) −E

ξ(1)
 ≥ε

≃P

|Y∞| ≥
√
N
σ ε

.
• Classical statistical tables (coded in computer programs) provide precise evalua-
tions of the r.h.s. as a function of
√
N
σ ε.
• This evaluation is used for an approximation of the actual risk level δ.
This procedure yields an approximate relation between the accuracy ε, number N
of samples, risk level δ, and variance σ 2. For instance, this yields approximate val-
ues for the number N of samples necessary to achieve a prescribed risk level δ in
terms of the rest; as an application, if δ = 5 % = 0.05 then P(|Y∞| ≥1.96) ≃0.05
and thus
N ≥1.962 σ 2
ε2 .
Remark 3.1 Note that:
• this bound is proportional to the variance σ 2 = E((ξ(1))2) −E(ξ(1))2 and in-
versely proportional to the square of the desired accuracy,
• the proportionality ratio depends only on the level of risk δ,

42
3
Non-asymptotic Error Estimates for Monte Carlo Methods
• in practice the variance σ 2 is unknown, but should be estimated by a Monte Carlo
method simultaneously with E(ξ(1)), or bounded a priori.
Obviously, in order to validate the preceding coarse analysis, it is necessary to
make precise the meaning of “large enough N” and “close to a Gaussian law”.
The next sections concern extensions of the Central Limit Theorem. These allow
to estimate the distance between the c.d.f. of YN and the c.d.f. of the Gaussian
N (0,1) law, and thus to obtain accurate non-asymptotic conﬁdence intervals, valid
for all N (see also Theorem 3.3). Motivations may be:
• Simulations or statistical measurements which are difﬁcult or costly (in terms of
time, money, availability, ...). Then N needs to be chosen as small as possible.
• Estimation of probabilities of rare events, say of order 10−8 for air control pur-
poses. This requires to take ε and δ extremely small. It is crucial to be sure that
the corresponding high N is compatible with realistic simulation program run-
ning times.
• Applications (including the preceding example) requiring to choose δ extremely
small, but for which the tails of the law of εs(N) may not be well approximated
by the tails of the N (0,1) law, even when N is quite large. Section 3.4 will
show that this is the case when the law of ξ(1) has fat tails, for example when
E(|ξ(1)|2+γ ) is very large for every 0 < γ < 1.
In such cases, the blind use of asymptotic results valid only for N tending to
inﬁnity can lead to gross undervaluation of risk levels.
3.3 Berry–Esseen’s Theorem
The Central Limit Theorem gives a precise asymptotic rate of convergence for the
Strong Law of Large Numbers. The Berry–Esseen theorem provides non-asymptotic
error estimates.
Its proof uses the following result, which is admitted. The very technical proof
can be found in the textbooks by Petrov [43] or Shiryayev [44].
Theorem 3.5 (Esseen) Let X and Y be two real-valued random variables. Let
T > 0 be an arbitrary number. For all b > 1
2π , there exists a positive number C(b)
(depending on b only) such that
sup
x∈R
P(X ≤x) −P(Y ≤x)

≤b
 T
−T

φX(t) −φY (t)
t
dt + b
T sup
x

|y|≤C(b)
P(Y ≤x + y) −P(Y ≤x)
dy.
(3.3)
We now state and prove the Berry–Esseen theorem.

3.3
Berry–Esseen’s Theorem
43
Theorem 3.6 (Berry–Esseen) Let (ξ(ℓ),ℓ≥1) be a sequence of real-valued inde-
pendent and identically distributed random variables. Assume that Eξ(1) = 0 and
E|ξ(1)|3 < +∞. Let σ 2 := var(ξ(1)) and Φ denote the c.d.f. of the N (0,1) Gaus-
sian law. Then
sup
x∈R
P
ξ(1) + ··· + ξ(N)
σ
√
N
≤x

−Φ(x)
 ≤CE|ξ(1)|3
σ 3√
N
.
It has been proved that 0.398 ≤C ≤0.8.
Remark 3.2 The hypothesis that Eξ(1) = 0 is not a restriction: the result can be
applied to ˜ξ(ℓ) := ξ(ℓ) −Eξ(1), see the following section.
Proof Observe that Φ′(x) ≤
1
√
2π for all x. Set
ΦN(x) := P
ξ(1) + ··· + ξ(N)
σ
√
N
≤x

,
LN := E(|ξ(1)|3)
σ 3√
N
,
(3.4)
and apply Esseen’s theorem 3.5 with b = 1
π and T =
1
4LN . Denote by φN(t) the
characteristic function of
1
σ
√
N
N
ℓ=1 ξ(ℓ). There exists C > 0 such that
sup
x
ΦN(x) −Φ(x)
 ≤1
π

|t|≤
1
4LN

φn(t) −e−t2/2
t
dt + CLN.
To conclude it then remains to apply the following lemma.
□
Lemma 3.1 Under the assumptions of Theorem 3.6, let φN(t) be the characteristic
function of
1
σ
√
N
N
ℓ=1 ξ(ℓ). Then, for LN deﬁned in (3.4),
∀|t| ≤
1
4LN
,
φN(t) −e−t2/2 ≤16LN|t|3e−t2/3.
(3.5)
Proof We start by dealing with the case where, in addition to being less that
1
4LN ,
the number t satisﬁes |t| ≥1
2L−1/3
N
. Then 16LN|t|3 ≥2 and
φN(t) −e−t2/2 ≤
φN(t)
 + e−t2/2.
Therefore it sufﬁces to show that |φN(t)| ≤e−t2/3. Let φξ(t) be the common char-
acteristic function of the ξ(ℓ)’s. The function |φξ(t)|2 is the characteristic function
of ξ(ℓ) −ξ(j) for any j ̸= ℓand, in addition,
E
ξ(ℓ) −ξ(j)3
≤8E
ξ(1)3
.

44
3
Non-asymptotic Error Estimates for Monte Carlo Methods
Applying Proposition 3.1, we deduce
φξ(t)
2 ≤1 −σ 2t2 + 4
3|t|3E
ξ(1)3
≤exp

−σ 2t2 + 4
3|t|3E
ξ(1)3
.
As |t| ≤
1
4LN by hypothesis, we have obtained |φN(t)| ≤e−t2/3.
We now consider the case where |t| ≤
1
4LN and |t| ≤1
2L−1/3
N
and observe that
σ
σ
√
N
|t| ≤(E(|ξ(1)|3))1/3
σ
√
N
≤L1/3
N |t|.
Since
eix = cos(x) + isin(x)
= 1 +
2

k=1
(ix)k
k!
+ x3
 1
0
(1 −θ)2
2

cos(3)(θx) + isin(3)(θx)

dθ,
we also have
φξ

t
σ
√
N

= 1 −σ 2t2
2σ 2N + η(N)
with
η(N)
 ≤E(|ξ(1)|3)
6σ 3N
√
N
|t|3.
Therefore
 σ 2t2
2σ 2N + η(N)
2
≤2
 σ 2t2
2σ 2N
2
+ 2η(N)2
≤
1
4 +
1
126
E(|ξ(1)|3)
σ 3N
√
N
|t|3 ≤E(|ξ(1)|3)
3σ 3N
√
N
|t|3.
One also has
σ 2t2
2σ 2N +
η(N)
 ≤1
6,
and thus
log

φξ

t
σ
√
N

≤−t2
2N +
η(N)
,
from which
log

φN(t)

≤−t2
2 + LN|t|3
2
.

3.4
Bikelis’ Theorem
45
From LN|t|3 ≤1
8 it follows that exp( LN|t|3
2
) ≤2. Consequently we have obtained
φN(t) −e−t2/2 ≤LN
2 |t|3 exp

−t2
2 + LN
2 |t|3

≤LN|t|3e−t2/2,
which implies (3.5).
□
3.4 Bikelis’ Theorem
The bounds in Bikelis’ theorem are more precise than those in Berry–Esseen’s the-
orem, since now the error estimate is not uniform w.r.t. x. We refer to Petrov [43]
for a proof and some extensions.
Theorem 3.7 (Bikelis) Let (ξ(ℓ),ℓ≥1) be a sequence of real-valued independent
random variables (which are not necessarily identically distributed). Assume that
Eξ(ℓ) = 0 for all ℓ, and that there exists 0 < γ ≤1 such that E|ξ(ℓ)|2+γ < +∞for
all ℓ. Let
BN :=
N

ℓ=1
var

ξ(ℓ)
,
ΦN(x) := P
ξ(1) + ··· + ξ(N)
√BN
≤x

,
and Φ denote the c.d.f. of the N (0,1) Gaussian law. Then, there exists a universal
constant A such that, for all x in R,
ΦN(x) −Φ(x)
 ≤
A
B1+γ/2
N
(1 + |x|)2+γ
N

ℓ=1
E
ξ(ℓ)2+γ .
(3.6)
It has been proved that
1
√
2π ≤A < 1.
3.4.1 Absolute Conﬁdence Intervals
Let (ξ(ℓ),ℓ≥1) be a sequence of integrable real-valued independent and identically
distributed random variables, such that Eξ(1) is not necessarily null. In order to
achieve an accuracy of order ε with a conﬁdence level 1 −δ for the Monte Carlo
estimation of Eξ(1), one has to choose a number N of simulations large enough that
P
E

ξ(1)
−1
N
N

ℓ=1
ξ(ℓ)
 ≥ε

≤δ.
(3.7)
Let us show how the Bikelis theorem can be applied to establish absolute, or
non-asymptotic, conﬁdence intervals, under the further assumption E|ξ(1)|3 < ∞.

46
3
Non-asymptotic Error Estimates for Monte Carlo Methods
Let m := Eξ(1) and σ 2 := var(ξ(1)) and
˜ξ(ℓ) := ξ(ℓ) −m,
ZN :=
N

ℓ=1
˜ξ(ℓ) =
N

ℓ=1
ξ(ℓ) −Nm.
Note that the ˜ξ(ℓ) are i.i.d., and that E˜ξ(1) = 0 and E|˜ξ(1)|3 < +∞and
P
E

ξ(1)
−1
N
N

ℓ=1
ξ(ℓ)
 ≥ε

= P

|ZN| ≥εN

.
Setting
ΦN(x) := P
 ˜ξ(1) + ··· + ˜ξ(N)
σ
√
N
≤x

,
an application of Bikelis’ theorem yields that
P

|ZN| ≥εN

= 2

1 −ΦN
ε
√
N
σ

≤2

1 −Φ
ε
√
N
σ

+
2AE|˜ξ(1)|3
√
N(σ + ε
√
N)3
≤
2
√
2π
 +∞
ε
√
N
σ
e−y2/2 dy +
2AE|˜ξ(1)|3
√
N(σ + ε
√
N)3
so that, in order to achieve the bound (3.7), it sufﬁces to choose N such that
2
√
2π
 +∞
ε
√
N
σ
e−y2/2 dy +
2AE|˜ξ(1)|3
√
N(σ + ε
√
N)3 ≤δ.
For actual computation of numerical values, statistical tables can again be used,
as well as bounds such as Komatsu’s inequality:1 for all x > 0, it holds that
2
x +
√
x2 + 4
e−x2/2 ≤
 +∞
x
e−y2/2 dy ≤
2
x +
√
x2 + 2
e−x2/2 ≤
√
2e−x2/2.
In practice, σ and E|˜ξ(1)|3 are unknown (otherwise, approximating E˜ξ(1) would
not be an issue). In order to effectively estimate the order of magnitude of a suit-
able N, two strategies are often followed:
• to estimate or bound σ and E|˜ξ(1)|3 starting from the law of ˜ξ(1),
• or, to proceed numerically in an iterative way, as in the following algorithm.
1Itô, K., McKean Jr., H.P.: Diffusion Processes and Their Sample Paths. Die Grundlehren der
Mathematischen Wissenschaften, vol. 125, 2nd edn. Springer, Berlin (1974).

3.5
Concentration Inequalities
47
Algorithm (Estimating number of samples) Start from a ﬁrst set of N1 samples
simulated from the law of ξ(1);
• compute an approximation for m and σ 2 and E|ξ(1)|3,
• use these for Bikelis’ theorem, which provides a value N2 for the number of
simulations that should be needed,
• – If N2 ≤N1 then all is well, terminate the simulation,
– else, simulate a new set of N2 −N1 samples, and proceed from the start using
the N2 samples instead of N1,
(repeat until successful).
Even though N1 may be an educated guess, it may grossly underestimate the
needed number of simulations.
In contrast, even though the estimation of m and σ 2 and E|ξ(1)|3 from N1 sam-
ples may be poor, Bikelis’ theorem should provide an N2 quite close to the mark.
Using the N2 samples yields a (statistically) better approximation for m and σ 2 and
E|ξ(1)|3, but seldom would these estimations differ enough that Bikelis’ theorem
would indicate a value N3 for the desired number of simulations substantially larger
than N2, so that an additional N3 −N2 samples would be required to straighten
things out. Thus, this algorithm should stop after two or at most three iterations.
Exercise 3.1 During an election between two candidates, the proportion p of vot-
ers who will vote for Candidate number 1 is estimated using an opinion poll.
Suppose that the answers are independent identically distributed random variables
ξ(1),...,ξ(N) satisfying P(ξ(1) = 1) = p = 1 −P(ξ(1) = 0).
1. Explain why p should be estimated by ˆpN := 1
N
N
ℓ=1 ξ(ℓ).
2. Fix N = 1024. Use various methods to estimate the probability that Candidate
number 2 wins the election when ˆpN = 51.5 % or ˆpN = 53 %.
3.5 Concentration Inequalities
The goal of this section is to estimate the probability that the Monte Carlo method
error exceeds a given threshold, in the case where the simulated probability distri-
bution satisﬁes an abstract property: the logarithmic Sobolev inequality. Although
this property may be difﬁcult to prove, it often is satisﬁed in practice.
The concentration inequalities in Theorem 3.9 have important numerical conse-
quences: if the logarithmic Sobolev inequality is satisﬁed, then the probability that
the Monte Carlo method errors take unacceptable values is small, in quantiﬁable
terms.
Most of the material in this section comes from the collective pedagogical mono-
graph Ané et al. [2] on the subject.

48
3
Non-asymptotic Error Estimates for Monte Carlo Methods
3.5.1 Logarithmic Sobolev Inequalities
The logarithmic Sobolev inequality relies on the notion of entropy which is classical
in information theory. For a probability measure on Rd and a positive or integrable
function g on Rd, the notation Eμ(g) :=

g dμ is used.
Deﬁnition 3.4 Let μ be a probability measure on Rd. For any positive measurable
function f on Rd, the entropy of f w.r.t. μ is deﬁned by
Entμ(f ) := Eμ

f log(f )

−Eμ(f )log

Eμ(f )

.
Jensen’s inequality implies that Entμ(f ) is non-negative, and vanishes if and
only if the function f is μ-a.s. constant. Moreover, Entμ(f ) is ﬁnite if and only if
the function f (log(f ))+ is μ-integrable.
Exercise 3.2 Check that
Entμ(f ) = sup

Eμ(fg); g s.t. Eμ

eg
= 1

.
(3.8)
Hint: Use the inequality uv ≤ulog(u) −u + ev for u ≥0 and v ∈R.
The following proposition shows that a product measure enjoys the so-called ten-
sorization property.
Proposition 3.2 Let μ1,...,μN be probability measures on R, and μN := μ1 ⊗
···⊗μN be their product measure on RN. Then, for any positive function f on RN,
EntμN (f ) ≤
N

i=1
EμN

Entμi(f )

,
(3.9)
where Entμi(f ) is computed by integrating only w.r.t. the xi variable, leaving the
other ﬁxed.
Proof Let g be a real-valued function deﬁned on RN such that EμN (eg) = 1, and
set
g1 := g −log

egμ1(dx1),
gi := log

egμ1(dx1)···μi−1(dxi−1)

egμ1(dx1)···μi(dxi)
,
i ≥2.
As Eμiegi = 1, the formula (3.8) implies
N

i=1
Eμi(fgi) ≤
N

i=1
Entμi(f ).

3.5
Concentration Inequalities
49
It then sufﬁces to observe that g = gi and thus
EμN (fg) =
N

i=1
EμN

Eμi(fgi)

,
and then to again use the formula (3.8).
□
We now are in a position to introduce the logarithmic Sobolev inequalities.
Deﬁnition 3.5 A measure ν on Rd satisﬁes the logarithmic Sobolev inequality
(LSI) with constant C > 0 for the family of functions A if, for every function f
in A ,
Entν

f 2
≤CEν

|∇f |2
.
(3.10)
Many measures satisfy an LSI. The next theorem shows two elementary exam-
ples.
Theorem 3.8 Let A be the family of the functions of class C 2(Rd,R) with compact
support.
(a) The Bernoulli law on {0,1} with parameter p satisﬁes the LSI for the family A ,
with constant C = 2 if p = 1
2 and C = log(1−p)−log(p)
1−2p
otherwise.
(b) The standard Gaussian law on Rd satisﬁes the LSI with constant C = 2 for any
dimension d.
Proof We observe that if the result holds true for the standard Gaussian law on R,
then the tensorization inequality (3.9) implies that the result also holds true for the
standard Gaussian law on Rd.
We now prove, using the Central Limit Theorem, that if the result holds for the
Bernoulli laws, then it holds for the standard Gaussian law on R. We consider the
Bernoulli law of parameter 1
2, with the notation
νi := 1
2(δ0 + δ1),
i ≥1.
Let f be a function of class C 2(Rd,R) with compact support. Apply the tensoriza-
tion inequality (3.9) to the function f ◦ΦN, where
ΦN(x1,...,xN) :=
2
√
N
 N

ℓ=1
xi −N
2

,
xi ∈{0,1}.
Now observe that, for any function F deﬁned on {0,1}N,
Entνi

F 2
= 1
2

Fi(1)2 −Fi(0)2
log

Fi(1) −log

Fi(0)

,

50
3
Non-asymptotic Error Estimates for Monte Carlo Methods
where Fi(a) := Fi(x1,...,xi−1,a,xi+1,...,xN), and that
(f ◦ΦN)i(1) −(f ◦ΦN)i(0)
 ≤
2
√
N
f ′ ◦ΦN,i(x)
 + C
√
N
where
ΦN,i(x) :=
2
√
N

x1 + ··· + xi−1 + xi+1 + ··· + xN −N
2

,
since the second derivative of f is bounded.
In order to obtain an LSI for the standard Gaussian law, it then sufﬁces to observe
that the probability distribution νN ◦(ΦN,i)−1 does not depend on i, and to apply the
Central Limit Theorem to this probability distribution by letting N tend to inﬁnity.
Let us now return to the proof for the Bernoulli law. By symmetry one can sup-
pose 0 < p ≤1
2. One then use the formula (3.8) to compute the quantity
sup

Eν(f 2g)
1
2|f ′(1)|2 + 1
2|f ′(0)|2 ; g s.t. eg(1) + eg(0) = 2

,
which is a simple but tedious calculation.
□
3.5.2 Concentration Inequalities, Absolute Conﬁdence Intervals
Theorem 3.9 below, which is interesting in its own, will be used to analyze the
Monte Carlo method error. To prove it, we will twice need the following elementary
result:
Lemma 3.2 Let μ be a probability measure on Rd, and let F be a measurable
bounded function. Suppose that there exists γ > 0 such that
∀λ ∈R,
Eμ

eλ(F−Eμ(F))
≤exp

γ λ2
2

.
(3.11)
Then
∀r > 0,
μ
F −Eμ(F)
 ≥r

≤2exp

−r2
2γ

.
(3.12)
Proof Using the hypothesis and the Markov inequality,
μ

F −Eμ(F) ≥r

= μ

eλ(F−Eμ(F)) ≥eλr
≤e−λrEμeλ(F−Eμ(F)) ≤eγ λ2
2 −λr.
Optimize the last term in the r.h.s. by choosing λ = r
γ , from which
∀r > 0,
μ

F −Eμ(F) ≥r

≤e−r2
2γ .

3.5
Concentration Inequalities
51
It is then easy to obtain (3.12): one observes that
μ
F −Eμ(F)
 ≥r

≤μ

F −Eμ(F) ≥r

+ μ

−F + Eμ(F) ≥r

and applies the preceding inequality to the functions F and ˜F := −F .
□
Deﬁnition 3.6 In the rest of this subsection, we will write that a real function F on
Rd has Lipschitz constant L if
F(x1,...,xd) −F(y1,...,yd)
 ≤L
#
|x1 −y1|2 + ··· + |xd −yd|2.
Theorem 3.9 Let μ be a measure on Rd satisfying the logarithmic Sobolev inequal-
ity with constant C for the family A of the bounded functions of class C ∞(Rd,R)
with bounded partial derivatives of all orders. Then, for any function F with Lips-
chitz constant smaller than 1 and r > 0,
μ

F −Eμ(F) ≥r

≤exp

−r2
C

,
(3.13)
μ
F −Eμ(F)
 ≥r

≤2exp

−r2
C

.
(3.14)
Remark 3.3 For a function F with an arbitrary Lipschitz constant L, similar in-
equalities are obtained by considering the function F/L.
Proof Let us ﬁrst prove (3.13). We limit ourselves to functions F with Lipschitz
constant smaller than 1 which are bounded and of class C ∞(Rd). The general case
will follow by applying Lemma 3.3 below to the approximating sequence
Fn := g1/n ∗max

−n,min(n,F)

,
where g1/n is the N (0, 1
n) Gaussian density and ∗is the convolution operator.
Our objective is to apply Lemma 3.2. Set H(λ) := Eμ(eλF). For λ > 0 we apply
the LSI with constant C to f := e
1
2 λF : using that |∇F| ≤1 it follows that
λH ′(λ) −H(λ)logH(λ) ≤C λ2
4

eλF |∇F|2dμ ≤C λ2
4 H(λ).
Therefore
H ′(λ)
λH(λ) −logH(λ)
λ2
≤C
4 .
Set ˜H(λ) := 1
λ logH(λ). The preceding inequality shows that ˜H ′(λ) ≤C
4 . In addi-
tion, in a neighborhood of 0, one has ˜H(λ) ≃H(λ)−1
λ
≃Eμ(F). Thus
˜H(λ) −˜H(0) =
 λ
0
˜H ′(α)dα ≤C
4 λ,

52
3
Non-asymptotic Error Estimates for Monte Carlo Methods
which is equivalent to
H(λ) ≤exp
C
4 λ2 + λEμ(F)

.
It then sufﬁces to apply Lemma 3.2 to get (3.13).
One ﬁnally obtains (3.14) by applying (3.13) to F and −F .
□
The following lemma has been used in the preceding proof.
Lemma 3.3 Let (Fn) be a sequence of μ-integrable and Lipschitz functions with
Lipschitz constant smaller than 1 and tending to F μ-a.s. Assume that there exists
γ > 0 such that
∀n ∈N, ∀λ > 0,
Eμ

eλ(Fn−Eμ(Fn))
≤eγ λ2/2.
Then F is integrable and
∀λ > 0,
Eμ

eλ(F−Eμ(F))
≤eγ λ2/2.
Proof Suppose ﬁrst that the sequence (Fn) converges to F in L1(Ω,F,P). In view
of the bound which is assumed, Eμ(Fn) tends to Eμ(F). Applying Fatou’s lemma
yields
Eμ

eλF 
≤liminf
n
Eμ

eλFn
≤liminf
n
eλEμ(Fn)+γ λ2/2 ≤eλEμ(F)+γ λ2/2,
which provides the desired result.
To prove that (Fn), which converges to F a.s., also converges in L1(Ω,F,P),
it classically sufﬁces to show that M := supn Eμ((Fn)2) < ∞, for the following
reason: since, for all C ≥0,
E

|Fn −F|

≤E

|Fn −F|1{|Fn|≤C}

+ E

|Fn|1{|Fn|>C}

+ E

|F|1{|Fn|>C}

,
by choosing C > M
ε where ε > 0 is arbitrarily ﬁxed,
E

|Fn|1{|Fn|>C}

≤1
C E

|Fn|21{|Fn|>C}

≤1
C E

|Fn|2
< ε,
and thus, by dominated convergence,
limsup
n
E

|Fn −F|

< ε.
This observation leads us to seek bounds which are uniform w.r.t. n for An :=
Eμ((Fn −Eμ(Fn))2) and Bn := Eμ((Fn)2).
Lemma 3.2 and our hypotheses imply
μ
Fn −Eμ(Fn)
 ≥r

≤2e−r2
2γ .

3.5
Concentration Inequalities
53
Classically, the Fubini theorem yields that, for any probability measure ν,
 ∞
0
x2ν(dx) = 2
 ∞
0
xν(x,+∞)dx.
(3.15)
Choose ν := μ ◦|Fn −Eμ(Fn)|−1 and use Theorem 3.1. It then follows that
Eμ

Fn −Eμ(Fn)
2
= 2
 ∞
0
rμ
Fn −Eμ(Fn)
 ≥r

dr ≤4
 ∞
0
re−r2
2γ dr.
Now consider Bn. Let m be large enough to have μ(|F| ≤m) ≥3
4, and let n0 be
such that μ(|Fn −F| ≥1) ≤1
4 for all n ≥n0. As
{Fn ≥m + 1} ⊂

|Fn −F| + |F| ≥m + 1

⊂

|F| ≥m

∪

|F| ≤m and |Fn −F| ≥1

⊂

|F| ≥m

∪

|Fn −F| ≥1

,
we have, for all n ≥n0,
P(Fn ≥m + 1) ≤1
4 + 1
4 = 1
2.
Let r1 be such that e−r2
1/2γ < 1
2. We also have
P

{Fn ≥m + 1} ∪

|Fn −Eμ(Fn)| ≤r1

> 1,
so that the events {Fn ≥m + 1} and {|Fn −Eμ(Fn)| ≤r1} have a non-empty
intersection. For all ω in this intersection, we have Eμ(Fn) ≤r1 + Fn(ω) and
Fn(ω) ≤m + 1. It results that
Eμ(Fn) ≤r1 + m + 1.
Now gather the preceding bounds from above for An and Bn: we have obtained
M := sup
n
Eμ

(Fn)2
≤4
 ∞
0
re−r2
2γ dr + r1 + m + 1 < ∞,
which ends the proof.
□
We are now in a position to state the following result. It allows to bound the
probability that the Monte Carlo method approximation error takes large values,
and thus to construct conﬁdence intervals.
Theorem 3.10 (Concentration inequalities for the SLLN) Let (X(ℓ),ℓ≥1) be a
sequence of Rd-valued independent random variables with law μ. Assume that μ

54
3
Non-asymptotic Error Estimates for Monte Carlo Methods
satisﬁes a logarithmic Sobolev inequality with constant C for the family A of func-
tions with Lipschitz constant smaller than 1. Then, for any f in A and r > 0,
P

1
N
N

ℓ=1
f

X(ℓ)
−Eμ(f )
 ≥r

≤2exp

−Nr2
C

.
(3.16)
Proof The tensorization property in Proposition 3.2 implies that (X(ℓ),1 ≤ℓ≤N)
satisﬁes a logarithmic Sobolev inequality with constant C. In addition, in view of
Deﬁnition 3.6 and the following consequence of Cauchy–Schwarz’s inequality:
N

j=1
|aj| =
N

j=1
|aj| × 1 ≤
√
N
$
%
%
%
&
N

j=1
|aj|2,
the function FN(x) := 1
N
N
ℓ=1 f (xℓ) is Lipschitz on RdN with Lipschitz constant
1
√
N . To conclude, it sufﬁces to apply (3.14) to the function
√
NFN.
□
3.6 Elementary Variance Reduction Techniques
The Central Limit Theorem and its non-asymptotic versions show that, the number
N of simulations being ﬁxed, a Monte Carlo approximation is all the more effec-
tive when the variance σ 2 of ξ(1) is small. Equivalently, the desired accuracy being
ﬁxed, the number of simulations which is necessary to achieve this accuracy is all
the weaker when the σ 2 is small. The quantity σ 2 = var(ξ(1)) will be called the
“variance of the simulation”.
Some variance reduction techniques are going to be described using some exam-
ples. We refer for instance to the book of Asmussen and Glynn [4] and the references
therein for more insight into this delicate matter.
3.6.1 Control Variate
Consider a random variable X of which the probability distribution can be simu-
lated, and a measurable function f such that f (X) has ﬁnite expectation. The goal
is to compute Ef (X).
The control variate variance reduction method consists in using the Monte Carlo
method to approximate E((f −g)(X)), where the function g satisﬁes:
• the value of Eg(X) is explicitly known;
• the variance of f (X) −g(X) is substantially smaller than the variance of f (X).
In practice, this double constraint is not easy to satisfy.

3.6
Elementary Variance Reduction Techniques
55
However the computation of European options provides a concrete application
example. Consider the price at time 0 of a European call:
C0 := e−rT E∗
(ST −K)+

,
where T is the maturity of the option and K its exercise price, and where the ex-
pectation is computed under the risk neutral probability. The variance of the simu-
lation is
e−2rT E∗
(ST −K)+
2 −C2
0.
Denote by P0 the price of a European put option. We have
P0 := e−rT E∗
(K −ST )+

,
and thus, in view of the Put–Call parity,
C0 = P0 + S0 −Ke−rT .
Consequently the deterministic quantity S0 −Ke−rT can be chosen as a control
variate. The variance of the simulation for the Monte Carlo method which approxi-
mates P0 is
e−2rT E∗
(K −ST )+
2 −C2
0
and is bounded from above by e−2rT K2 −C2
0. Therefore, when
K2 ≪E∗
(ST −K)+
2,
it is efﬁcient to change the computation of a call price into the computation of a put
price.
3.6.2 Importance Sampling
In this subsection, we start by presenting the importance sampling variance reduc-
tion technique in the following elementary situation. As above we wish to compute
Ef (X), but now we assume in addition that X is real-valued and that its law PX has
a density pX.
For any real-valued random variable Y whose probability distribution has a
strictly positive density pY it holds that
Ef (X)=

f (x)pX(x)dx =

f (x)
pY (x)pY (x)pX(x)dx =E
	 f (Y)
pY (Y)pX(Y)

=EZ,

56
3
Non-asymptotic Error Estimates for Monte Carlo Methods
where the random variable
Z := f (Y)
pY (Y)pX(Y)
has variance
var(Z) =
 f 2(y)
pY (y)

pX(y)
2 dy −

Ef (X)
2.
It may happen that one can suitably choose pY in such a way that var(Z) is much
smaller than var(X), and such that Y can be easily simulated.
In such a case, it is worth computing EZ rather than Ef (X) by the Monte Carlo
method since, at ﬁxed desired accuracy, the computation of EZ requires less simu-
lations than the computation of Ef (X).
In practice, it appears difﬁcult to fulﬁl the preceding requirements. The following
examples allow one to understand this limitation of the method.
Let I :=
 1
0 exp(x)dx. Suppose that one has the curious objective to approx-
imate I by a Monte Carlo method. The standard method leads one to write I =
Ef (X), where X is a uniformly distributed on [0,1] random variable; an easy cal-
culation shows that var(f (U)) = 0.24.
Now choose pY (x) := 2
3(1+x)1[0,1](x) and apply the importance sampling vari-
ance reduction technique. One then has var(Z) = 0.027.
One can improve the variance reduction by looking for a pY (·) of the (non-
natural) type
pY (x) = 1 + βx
1 + 1
2β
1[0,1](x).
An easy calculation shows that the optimal choice of β is β = 1.81 for which
var(Z) = 0.004.
Of course, such choices are impossible when the density pX and the function f
are algebraically complex or are not explicitly known, which is the typical situation
where Monte Carlo simulations are useful.
In particular, in Chap. 8 we will see the difﬁculties raised by the importance sam-
pling variance reduction technique when applied to the computation of expectation
of functionals of stochastic processes, and examples of numerical methodologies
which, in favorable circumstances, allow one to overcome these difﬁculties.
Remark 3.4
The stratiﬁcation variance reduction technique is a particular impor-
tance sampling technique for integrals of the type I :=

D f (x)dx, where D is a
domain in Rd. The integration domain D is ﬁrst partitioned into appropriate sub-
domains. Then I is expressed as the sum of integrals on each sub-domain, and each
of these new integrals is approximated by the Monte Carlo method, the respec-
tive numbers of simulations being adjusted to the variation of the function f on
each sub-domain (few simulations for a sub-domain where f varies little, and many
where f varies a lot).

3.6
Elementary Variance Reduction Techniques
57
Bermuda European Options and Gaussian Changes of Measure
We now consider the situation of a Bermuda European option: its risk neutral price
has the form
V0 = e−rTdE∗f (ST1,...,STd)
where the dates Ti are deterministic, f is the pay-off function of the option, and
(St) is the underlying asset price process. Here, the expectation is computed under
the risk neutral probability P∗.
The Monte Carlo method to approximate V0 consists in simulating N indepen-
dent simulations of the probability distribution under P∗of the random variables
ST1,...,STd , and in computing
¯V0 := e−rTd
N

ℓ=1
f

S(ℓ)
T1 ,...,S(ℓ)
Td

.
The variance of the simulation is the variance under P∗of f (ST1,...,STd).
We will see in Chap. 8 that, by using Girsanov’s theorem 8.1, one can, at least
theoretically, construct an importance sampling variance reduction technique when
(St) solves a stochastic differential equation, and that it sometimes is possible to
optimize the variance reduction.
We describe this mechanism in the very particular case where (St) follows the
log-normal Black and Scholes model. (This case presents no numerical interest but
allows us to introduce a mathematically complex issue in a simpliﬁed framework.)
In this model, the asset price process under P∗is
St = S0 exp

r −σ 2
2 t + σWt

,
where (Wt) is a P∗standard Brownian motion (the deﬁnition is given in Chap. 7;
here we only need to know that, for all 0 < s < t, Wt −Ws has a Gaussian distribu-
tion with zero mean and variance t −s and is independent of Ws). Set
F(x1,...,xd) = e−rTdf

S0 exp

r −σ 2
2 T1 +σx1

,...,S0 exp

r −σ 2
2 Td +σxd

.
We can then rewrite the price V0 as follows:
V0 = E∗F(WT1,...,WTd).
Let θ be a real number and consider the new probability measure
dQ
dP∗= exp

θWTd −θ2Td
2

.

58
3
Non-asymptotic Error Estimates for Monte Carlo Methods
Changes of variables in integrals w.r.t. Gaussian densities (which avoids using Gir-
sanov’s theorem 8.1 in this simple setting) show that
V0 = EQ
	
F( ˜WT1 + θT1,..., ˜WTd + θTd)exp

−θ ˜WTd −θ2Td
2

= E∗
	
F(WT1 + θT1,...,WTd + θTd)exp

−θWTd −θ2Td
2

= E∗F(WT1,...,WTd).
(3.17)
One thus can approximate V0 by the following Monte Carlo method:
˜V0 := e−rTd 1
N
N

ℓ=1
F

W (ℓ)
T1 + θT1,...,W (ℓ)
Td + θTd

exp

−θW (ℓ)
Td −θ2Td
2

.
The smaller variance of the simulation is obtained by minimizing the quantity
H(θ) := E∗
F

WT1 + θT1,...,WTd + θTd
2 exp

−2θWTd −θ2Td

.
Proposition 3.3 Suppose that the pay-off function f has an at most polynomial
growth at inﬁnity. Suppose also that there exists a ball B(0,R) with radius R in Rd
such that

B(0,R)

f (x1,...,xd)
2 dx1 ···dxd > 0.
(3.18)
Then the function H is strictly convex and there exists a unique real number θ∗such
that
H(θ∗) = min
θ∈R H(θ).
Proof We start by proving that H is everywhere twice differentiable and by com-
puting explicitly its second derivative. To this end, we observe that
H(θ) = E∗
F(WT1 + θT1,...,WTd + θTd)
2 exp

−2θ(WTd + θTd) + θ2Td

= E∗
	
F(WT1 + θT1,...,WTd + θTd)
2
× exp

−θWTd −θ2Td
2
−θ(WTd + θTd) + θ2Td
2

so that (3.17) yields
H(θ) = E∗
	
F(WT1,...,WTd)
2 exp

−θWTd + θ2Td
2

.
(3.19)

3.6
Elementary Variance Reduction Techniques
59
As the pay-off function f has an at most polynomial growth at inﬁnity, there exists
C > 0 such that |F(x1,...,xd)| ≤CeC|x|1+···C|x|d for all x, and one can differentiate
under the integral sign in the r.h.s. of (3.19); it follows that
d
dθ H(θ) = E∗
	
F(WT1,...,WTd)
2(θ −WTd)exp

−θWTd + θ2Td
2

.
Similarly,
d2
dθ2 H(θ) = E∗
	
F(WT1,...,WTd)
2
1 + (θ −WTd)2
exp

−θWTd + θ2Td
2

.
Therefore, denoting by gd the standard Gaussian density on Rd,
d2
dθ2 H(θ) ≥

B(0,R)
exp

−|θ|R + θ2 Td
2

×

F(

T1x1,...,

Tdxd)
2gd(x1,...,xd)dx1 ···dxd.
As the hypothesis (3.18) implies that

B(0,R)

F(x1,...,xd)
2 dx1 ···dxd > 0,
(3.20)
we are in a position to deduce that the second derivative of H is everywhere strictly
positive.
To obtain the existence and uniqueness of the optimal value θ∗it therefore suf-
ﬁces to show that H(θ) tends to inﬁnity when |θ| tends to inﬁnity. We have:
H(θ) ≥exp

−|θ|R + θ2 Td
2

×

B(0,R)

F(

T1x1,...,

Tdxd)
2gd(x1,...,xd)dx1 ···dxd.
We again use (3.20) to conclude.
□
We have just shown that to reduce the variance of the Monte Carlo method
for Bermuda European options prices, one can optimize the importance sampling
technique by perturbing the original Brownian motion by the deterministic function
t →θt. It however remains to compute the optimal parameter θ∗.
In Chap. 9 we will see that θ∗can be approximated by means of an optimization
deterministic or stochastic algorithm before running the Monte Carlo procedure to
approximate V0. We will even develop an adaptive Monte Carlo method, due to
B. Arouna [3], which approximates both θ∗and V0 by using one single set of simu-
lations.

60
3
Non-asymptotic Error Estimates for Monte Carlo Methods
Remark 3.5 In theory one can easily construct many other importance sampling
techniques to approximate V0. For instance, one may use other parametrized Gir-
sanov transformations than the perturbation of (Wt) by a linear function of time, and
optimize w.r.t. the new parameters. However, even for the log-normal model and a
fortiori for less elementary models, it is not obvious to do so in a way which sub-
stantially decreases the variance of the simulation without substantially increasing
the complexity of the simulations, and for which optimal values of the parameters
can be approximated with a reasonable numerical cost. We refer to Chap. 8 for more
on this topic.
3.7 Problems
3.1 (Convergence Rate in Donsker’s Theorem) Bikelis’ theorem allows to make
precise the convergence rate in Donsker’s invariance principle for appropriately nor-
malized centered random walks, which converge to a standard Brownian motion.
Let (Xn,n ≥1) be a sequence of independent identically distributed random vari-
ables, with zero mean and standard deviation σ and satisfying E(|X1|3) < +∞. For
T > 0 let
W (N)
T
=
√
T
σ
√
N
(X1 + ··· + XN).
Let f be a differentiable function with bounded derivative.
1. Prove that

 a
−a
f (x)P

W (N)
T
∈dx

−
 a
−a
f (x)P(WT ∈dx)

≤
 a
−a
P

W (N)
T
≤x

−P(WT ≤x)
f ′(x)
dx
+
f (a)
P

W (N)
T
≤a

−P(WT ≤a)

+
f (−a)
P

W (N)
T
≤−a

−P(WT ≤−a)
.
2. Deduce that
Ef

W (N)
T

−Ef (WT )
 ≤
''f ′''
∞
C′E|X1|3
σ 3√
N
.
3.2 (Pointwise Approximation of a Density) Let X be a real-valued random variable
with a bounded continuous density pX which is not explicitly known.
1. Let Gε be a family of Gaussian random variables with zero mean and standard
deviation ε. Show that this family converges in law to 0.

3.7
Problems
61
2. Let x0 be an arbitrary real number. By considering E[pX(x0 −εG)], construct a
Monte Carlo method to approximate pX(x0). Estimate the variance of the simu-
lation.
3. Now suppose that the density pX is everywhere strictly positive and differen-
tiable. Identify classes of functions f such that
Ef ′(X) = −E

f (X)p′(X)
p(X)

.
4. In which cases does this equality allow one to develop an efﬁcient variance reduc-
tion technique for the preceding Monte Carlo approximation of Ef ′(X)? Treat
the particular case of the pointwise approximation of a standard Gaussian den-
sity.
3.3 (Approximation of Quantiles) Let X be a real-valued random variable with an
everywhere continuous and strictly positive density. For any 0 < a < 1 denote by
ρ(a) its quantile of order a: it is deﬁned by F(ρ(a)) = a, where F is the cumulative
distribution function of X.
Let N independent samplings of the law of X be ordered increasingly as
X1 ≤X2 ≤··· ≤XN.
Let 0 < a < 1 be such that Na has a non-integer value. Denote by μ its integer part
and set ρN(a) := Xμ+1.
1. For any x and h in R, compute P(ρN(a) ∈[x,x + h]). By letting h tend to 0,
deduce that the law of ρN(a) has density
gN(x) = Cμ
N(N −μ)

F(x)
μ
1 −F(x)
N−μ−1F ′(x).
2. Set
Y :=
√
N
√a(1 −a)F ′
ρ(a)

ρN(a) −ρ(a)

.
Prove that the density qN of Y is
qN(x) :=
1
F ′(ρ(a))
√a(1 −a)
√
N
gN

ρ(a) +
√a(1 −a)
√
N
x
F ′(ρ(a))

.
3. By using the classic limit
lim
N→∞
√a(1 −a)
√
N
Cμ
Naμ(1 −a)N−μ N −μ
1 −a =
1
√
2π
,
show that, when N tends to inﬁnity, the ρN(a) converge in law to a Gaussian
distribution with mean ρ(a) and standard deviation
1
F ′(ρ(a))
√a(1 −a)
√
N
.

62
3
Non-asymptotic Error Estimates for Monte Carlo Methods
Deduce an error estimate for the approximation of quantiles by a Monte Carlo
method.
3.4 (Concentration Inequalities (⋆)) Let U be a strictly convex function from R to
R with bounded continuous second derivative. Assume there exists λ > 0 such that
U′′(x) ≥λ
for all x ∈R.
(3.21)
1. Let (Gn) be a sequence of independent real-valued standard Gaussian random
variables. For any 0 < h < 1
λ and x0 in R, deﬁne a random sequence by X0 = x
and
Xn+1 = Xn −U′(Xn)h +
√
hGn.
Let K be the operator which maps any bounded continuous function f to the
function Kf deﬁned as
Kf (x) := Ef (X1).
Show that
∀x ∈R,

d
dx K(f )(x)
 ≤(1 −λh)K
f ′
(x).
(3.22)
2. Denote by νn the law of the random variable Xn. Show that ν1 satisﬁes a loga-
rithmic Sobolev inequality with constant 2h.
3. Set K0(f ) := f , K2(f ) := K(K(f )), ... , Kp(f ) := K(Kp−1(f )). Using Def-
inition 3.4 of the entropy, show that
Entνn
f 2
=
n

i=1

Ki
Kn−i
f 2
logKn−i
f 2
−Ki−1
Kn−i+1
f 2
logKn−i+1
f 2
.
4. Set gn−i :=

Kn−i(f 2). Show
Entνn
f 2
≤2h
n

i=1
Kig′
n−i
2
.
5. Using (3.22) show
g′
n−i
2 ≤(1 −λh)2K
g′
n−i−1
2
.
Deduce
g′
n−i
2 ≤(1 −λh)2(n−i)Kn−if ′2
.
6. Show
Entνn
f 2
≤
2
λ(2 −λh)

1 −(1 −λh)2n
Knf ′2
.

3.7
Problems
63
7. Show
g′
n−i
2 ≤(1 −λh)2K
g′
n−i−1
2
,
and deduce
g′
n−i
2 ≤(1 −λh)2(n−i)Kn−if ′2
.
8. Show
Entνn
f 2
≤
2
λ(2 −λh)

1 −(1 −λh)2n
Knf ′2
.
Deduce a concentration inequality for the Monte Carlo method which approxi-
mates Ef (Xn), where f is a Lipschitz function with Lipschitz constant smaller
than 1, and then an estimate for
P

1
N
N

ℓ=1
f

X(ℓ)
n

−Eνn(f )
 ≥r

in terms of n and the number N of simulations.

Part II
Exact and Approximate Simulation
of Markov Processes
Markov processes of increasingly complex type will be introduced, from discrete to
continuous space, and from pure jump to diffusion type. The study focuses on their
concrete pathwise constructions and simulations.
This culminates with the solutions of Itô stochastic differential equations, for
which implementable discretization schemes are derived, justiﬁed, and evaluated.
The generators of these Markov processes are successively computed, and the
related equations (forward and backward Kolmogorov equations, Fokker–Planck
equations, and the Feynman–Kac formula for solving terminal value equations) are
given and discussed in a synthetic way. Several incursions are made into the realm
of the profound relationship between Markov processes and martingales.
These Markov processes are used for Monte Carlo methods for the approxi-
mation of solutions of equations, ranging from systems of ordinary equations to
integro-differential equations and parabolic partial differential equations, for which
a number of results are proved using probabilistic techniques.

Chapter 4
Poisson Processes as Particular Markov
Processes
Abstract We ﬁrst introduce some practical and theoretical issues of modeling by
means of Markov processes. Point processes are introduced in order to model jump
instants. The Poisson process is then characterized as a point process without mem-
ory. The rest of the chapter consists in its rather detailed study, including various
results concerning its simulation and approximation. This study is essential to un-
derstand the abstract constructions and the simulation methods for jump Markov
processes developed in the following chapters.
4.1 Quick Introduction to Markov Processes
Markovian modeling of randomly evolving systems raises several issues. After giv-
ing some examples, we discuss a reasonably simple mathematical framework in
which results can actually be proved.
4.1.1 Some Issues in Markovian Modeling
Many application ﬁelds feature random phenomena which evolve in continuous
time, and in a continuous fashion between sudden state changes. The latter are called
jumps, occur at jump instants, and may for instance be due to:
• arrivals, ends of service, and transfers of customers in communication networks,
• contagion, recovery, and deaths of individuals in epidemiology models,
• births, deaths, mutations, and transfers of individuals in ecological models,
• interactions or reactions between particles in physical or chemical models.
The natural state space may be discrete, as is the case when counting customers at
the various network resources, or infected individuals in an epidemic. The evolution
is then necessarily constant between the jumps.
The state space may also be continuous, as is the case when measuring pheno-
types such as size or speed of individuals in ecology, or the energy in certain models
of physics. The evolution may then be purely continuous, or mix jumps with con-
tinuous (possibly constant) stretches.
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1_4, © Springer-Verlag Berlin Heidelberg 2013
67

68
4
Poisson Processes as Particular Markov Processes
The goal here will be to model random phenomena having a property of lack of
memory of the past conditional on the present. A Markov process will be deﬁned
as a random evolution such that at any instant the law of the future evolution only
depends on the past through the present state. The main case considered is when
this law does not depend on the value of the present instant; else, the process will be
said to be inhomogeneous (in time).
An important element of the study of Markov process with jumps is their set of
discontinuities. In this chapter, point processes will be introduced as a representa-
tion for a sequence of jump instants without accumulation point. This is followed
by detailed study of Poisson processes, which are point processes having a certain
property of lack of memory. This study will be fundamental in the sequel, for ab-
stract comprehension and construction as well as for effective simulation of Markov
processes with jumps.
The next subsection is a quick introduction to some theoretical material.
4.1.2 Rudiments on Processes, Sample Paths, and Laws
Continuous time and state models for random phenomena necessitate a much more
complex mathematical theory than discrete ones, and some abstract concepts must
be introduced. Let (Ω,F,P) be a probability space.
A (stochastic) process with values in a state space V , which here will be either
a discrete set or a closed subset of Rd with its Borel σ-ﬁeld B(V ), is a random
function from R+, most often representing time, to V :
(Xt)t∈R+ : ω ∈Ω →

Xt(ω)

t∈R+,
Xt(ω) ∈V .
It will always be assumed here that the sample paths (Xt(ω))t∈R+ belong to the
Skorohod space D(R+,V ) of functions from R+ to V which are right-continuous
and have left limits.
This regularity assumption corresponds to mathematical constraints, as well as to
the fact that the random phenomena on hand surprise us when they jump. For contin-
uous evolutions, the sample paths belong to the subspace C (R+,V ) of continuous
functions.
The minimal measurability assumption is that each Xt : R+ →V is a random
variable (r.v.), i.e., is measurable. Hence D(R+,V ) is equipped with the smallest
such σ-ﬁeld, called the product σ-ﬁeld, engendered by the “cylindrical sets”
C =

(xt)t∈R+ ∈D(R+,V ) : xt1 ∈A1,...,xtn ∈An

for n ≥1 and 0 ≤t1 < ··· < tn and A1,...,An in B(V ).
By a limit procedure, a function in D(R+,V ) can be determined by its values
on Q+, or on all ﬁnite subsets of R+, and this measurable space suits our purposes.

4.2
Poisson Processes: Characterization, Properties
69
The law of a process (Xt)t∈R+ is a probability measure on D(R+,V ), and hence
is characterized by the family of the laws of all random vectors
(Xt1,...,Xtn) = (Xtk)1≤k≤n,
n ≥1, 0 ≤t1 < ··· < tn,
called the ﬁnite-dimensional marginals of the process, or of its law.
Conversely, consider a family of laws
M =

πt1,...,tn ∈P

V n
: n ≥1, 0 ≤t1 < ··· < tn

satisfying the following natural compatibility property: if 0 ≤s1 < ··· < sk is a sub-
sequence of 0 ≤t1 < ··· < tn, then πs1,...,sk is the corresponding marginal of πt1,...,tn.
Then the Kolmogorov extension theorem (see, e.g., Theorem 2.2 in Karatzas and
Shreve [26, Chap. 2]) ensures that there exists a (unique) law on V R+ with product
σ-ﬁeld having M for ﬁnite-dimensional marginals. Under further assumptions, it
can be shown that this law corresponds to a process with sample paths in D(R+,V ).
This book elects to construct the process sample paths from a simulation per-
spective, which enables to directly prove the existence of a law of a process with
sample paths in D(R+,V ) without invoking this theorem.
4.2 Poisson Processes: Characterization, Properties
Jump instants of stochastic processes will be modeled using point processes. Then,
preﬁguring the rigorous notion of Markov processes, the Poisson process will be
characterized as a memoryless point process.
4.2.1 Point Processes and Poisson Processes
For pedagogical purposes, non-degenerate point processes are ﬁrst introduced; the
general case will be treated as an extension.
Deﬁnition 4.1 (Point process) A point process on R+ with an inﬁnite number of
(strictly) positive jump instants without accumulation point, is a process (Nt)t∈R+
with values in N := {0,1,...}, vanishing at 0, non-decreasing, right continuous,
with unit jumps, and with inﬁnite limit: for 0 ≤s ≤t < ∞,
0 = N0 ≤Ns ≤Nt = Nt+ ∈N,
Nt −Nt−∈{0,1},
lim
t→∞Nt = ∞,
with the notation Nt+ = limu→t+ Nu and Nt−= limu→t−Nu and N0−= N0.
This kind of random object has two other equivalent descriptions, to which it
makes implicit reference. All this is illustrated in Fig. 4.1.
A point process as in Deﬁnition 4.1 can be given equivalently by any of the two
following random objects:

70
4
Poisson Processes as Particular Markov Processes
Fig. 4.1 A sample path of a point process, such as a Poisson process
(a) A sequence (Tn)n≥1 of positive r.v.’s increasing to inﬁnity:
0 < T1 < T2 < ··· < ∞,
lim
n→∞Tn = ∞.
The Tn are the jump instants of (Nt)t∈R+, and are also called the arrival instants
or the points of the point process (hence its name). Then (Nt)t∈R+ is given by
their counting process:
Nt =

n≥1
1{Tn≤t} = sup{n ≥1 : Tn ≤t},
{Nt ≥n} = {Tn ≤t},
etc.
(b) A sequence (Sn)n≥1 of positive r.v.’s such that 
n≥1 Sn = ∞. The bijection
between the inter-arrivals Sn and the jump instants Tn is given by
S1 = T1,
Sn = Tn −Tn−1,
n ≥2,
or
Tn = S1 + ··· + Sn,
n ≥1.
Poisson Processes
An essential fact is that if a point process has the following property of lack of
memory and of stability in time, then its law is determined up to time unit.
Deﬁnition 4.2 A point process (Nt)t∈R+ has independent and stationary incre-
ments if for every t,h ≥0 the r.v. Nt+h −Nt is independent of (Ns)0≤s≤t and has
the same law as Nh. To check this, it is enough to check that for every n ≥1 and
0 = t0 < t1 < ··· < tn the Ntk −Ntk−1 for 1 ≤k ≤n are independent and have the
same law as Ntk−tk−1.

4.2
Poisson Processes: Characterization, Properties
71
Theorem 4.1 (Poisson process characterization)
(a) Let (Nt)t∈R+ be a point process with independent and stationary increments.
Then there exists λ > 0 such that the law of Nt is Poisson P(λt), i.e.,
P(Nt = k) = e−λt (λt)k
k! ,
k ∈N,
and the inter-arrivals (Sn)n≥1 are i.i.d. with E (λ) exponential law, i.e., have
density 1{t>0}λe−λt.
(b) Conversely, the counting process (Nt)t∈R+ for such inter-arrivals (Sn)n≥1 is a
point process with independent and stationary increments.
Such a process (Nt)t∈R+ is called a Poisson process with intensity or rate λ, which
can be interpreted as a mean number of jumps per unit of time; recall that E(Nt) =
λt and E(S1) = 1/λ.
Proof (a) The proof introduces a time-scaling method on a grid with mesh ε > 0,
which will be very useful in the sequel. This allows to exploit the links between
binomial and Poisson random variables, and between geometric and exponential
r.v.’s. Recall the notation
⌊s⌋= sup{n ∈N : n ≤s},
⌈s⌉= inf{n ∈N : s ≤n},
s ∈R+.
(4.1)
Let
Y ε
k = 1{Nkε>N(k−1)ε},
k ≥1;
Nε
t =
⌊t/ε⌋

k=1
Y ε
k ,
t ≥0.
Since the increments of (Nt)t∈R+ are independent and stationary, the (Y ε
k )k≥1 are
i.i.d. and
P

Y ε
1 = 1

= pε := P(Nε ≥1),
so that Nε
t has binomial law B(⌊t/ε⌋,pε) with generating function
E

zNε
t 
= E

zY ε
1 ⌊t/ε⌋= (1 −pε + pεz)⌊t/ε⌋,
0 ≤z ≤1.
The fact that (Nt)t∈R+ is right-continuous with isolated discontinuities implies that
limε→0 Nε
ε⌈t/ε⌉= Nt a.s., and thus also in law. In particular
P(Nt = 0) = lim
ε→0P

Nε
ε⌈t/ε⌉= 0

= lim
ε→0(1 −pε)⌈t/ε⌉= lim
ε→0e
log(1−pε)
ε
t ∈[0,1]
and there exists λ in [0,∞] such that

72
4
Poisson Processes as Particular Markov Processes
λ := −lim
ε→0
log(1 −pε)
ε
= lim
ε→0
pε
ε ,
P(Nt = 0) = e−λt,
and limt→0 Nt = 0 and limt→∞Nt = ∞imply that 0 < λ < ∞. Then, the generat-
ing function of Nt is
E

zNt 
= lim
ε→0E

zNε
ε⌈t/ε⌉
= lim
ε→0

1 + pε(z −1)
⌈t/ε⌉= eλt(z−1),
0 ≤z ≤1,
which corresponds to the Poisson law P(λt).
Let us now prove the result on (Sn)n≥1. For n ≥1 we deﬁne approximate inter-
arrivals Gε
n in mesh numbers and Sε
n in natural time by
Gε
n = inf

j ≥1 : Y ε
Gε
1+···+Gε
n−1+j = 1

,
Sε
n = εGε
n.
Clearly, a.s., for all n, limε→0(Sε
n)n≥1 = (Sn)n≥1. In addition, for all jk ≥1 for
k ≥1,
P

Gε
k = jk : 1 ≤k ≤n

= P

Y ε
j1+···+jk−1+1 = ··· = Y ε
j1+···+jk−1 = 0,Y ε
j1+···+jk = 1 : 1 ≤k ≤n

=
n

k=1
(1 −pε)jk−1pε,
thus the (Gε
n)n≥1 are i.i.d., hence so are the (Sε
n)n≥1. Taking limits, the (Sn)n≥1 are
i.i.d., and their common law is E (λ) since
P(S1 > t) = P(Nt = 0) = e−λt.
(This shows that the law of Gε
1 is geometric on N∗, and that the limit of the laws of
the εGε
1 is E (λ), which can be easily proved directly.)
(b) Let (Sn)n≥1 be a sequence of i.i.d. inter-arrivals with law E (λ), and (Nt)t∈R+
the corresponding point process. Let n ≥1 and 0 = t0 ≤t1 < ··· < tn be arbitrary.
For all jk ≥0 for k ≥1, setting Jk = j1 + ··· + jk it holds that
{Ntk −Ntk−1 = jk : 1 ≤k ≤n} =
(
1≤k≤n
{tk−1 < TJk−1+1,...,TJk ≤tk}∩{tn < TJn+1}
and since Tj = S1 + ··· + Sj we obtain that
P(Ntk −Ntk−1 = jk : 1 ≤k ≤n) = λJn+1

D
e−λ(s1+···+sJn+1) ds1 ···dsJn+1,
D =
	 
1≤k≤n
jk̸=0
{tk−1 < s1 + ··· + sJk−1+1,...,s1 + ··· + sJk ≤tk}

× {tn < s1 + ··· + sJn+1}.

4.2
Poisson Processes: Characterization, Properties
73
The change of variables s1 + ··· + sj = uj, with Jacobian 1, yields
P(Ntk −Ntk−1 = jk : 1 ≤k ≤n) = λJn+1

E
e−λuJn+1 du1 ···duJn+1,
E =
	 
1≤k≤n
jk̸=0
{tk−1 < uJk−1+1 ≤··· ≤uJk ≤tk}

× {tn < uJn+1},
and the Fubini theorem allows to conclude that
P(Ntk −Ntk−1 = jk : 1 ≤k ≤n)
=
	 
1≤k≤n
jk̸=0
λjk

tk−1<z1<···<zjk ≤tk
dz1 ···dzjk

λ

tn<z
e−λz dz.
This product form proves that the increments are independent, and the invariance of
the Lebesgue measure under translations that they are stationary.
□
Inﬁnitesimal Point of View
The following inﬁnitesimal point of view is made rigorous in the proof. For ε small,
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
P(Nt+ε −Nt = 0) = e−λε = 1 −λε + O

ε2
,
P(Nt+ε −Nt = 1) = e−λελε = λε + O

ε2
,
P(Nt+ε −Nt ≥2) = 1 −e−λε −e−λελε ≤1
2λ2ε2 = O

ε2
.
(4.2)
Law of the Poisson Process
The law of (Nt)t∈R+ is entirely determined by the fact that its inter-arrivals (Sn)n≥1
are i.i.d. E (λ) r.v.’s, which also allows to construct this process.
From a theoretical perspective, the law of (Nt)t∈R+ is also characterized by its
ﬁnite-dimensional marginals, which can be computed from the fact that the law of
(Ntk −Ntk−1)1≤k≤n is the product of the Poisson laws P(λ(tk −tk−1)), for every
n ≥1 and 0 = t0 < t1 < ··· < tn.
The end of the proof of Theorem 4.1 is an example of a computation on laws
performed by expressing events in terms of (Sn)n≥1. It can be used to obtain results
on the jump instants (Tn)n≥1 such as those in Problem 4.1.
Exercise 4.1 Use it to prove that the law of Ntk −Ntk−1 is Poisson P(λ(tk −tk−1)).

74
4
Poisson Processes as Particular Markov Processes
Memoryless Laws
The property of the Poisson process is a generalization of the following classic re-
sult: the only memoryless R∗
+-valued random variables are exponential.
Theorem 4.2 (Memoryless r.v.’s on R+) A r.v. S with values in R+ ∪{∞} is mem-
oryless, in the sense that
P(S > t + h|S > t) = P(S > h),
t,h ≥0,
if and only if either there exists λ > 0 such that S has E (λ) exponential law, or else
S is a degenerate r.v. such that either S = ∞a.s. or S = 0 a.s.
Proof With notation (4.1), for all t > 0 and ε > 0 the memoryless assumption yields
that
P

S > ⌈t/ε⌉ε

= P(S > ε)P

S > ⌈t/ε⌉ε |S > ε

= P(S > ε)P

S >

⌈t/ε⌉−1

ε

so that by induction
P

S > ⌈t/ε⌉ε

= P(S > ε)⌈t/ε⌉.
The right-continuity of s →P(S > s) implies that
P(S > t) = lim
ε→0P

S > ⌈t/ε⌉ε

= lim
ε→0P(S > ε)⌈t/ε⌉= lim
ε→0e⌈t/ε⌉logP(S>ε)
which yields the equality and the existence of the limit
P(S > t) = e−λt,
λ = −lim
ε→0
logP(S > ε)
ε
∈[0,∞].
Hence, according to whether 0 < λ < ∞, λ = 0, or λ = ∞, the r.v. S has law E (λ),
is inﬁnite a.s, or vanishes a.s. Conversely, a simple computation shows that these
r.v.’s are memoryless.
□
Some Extensions
The proof of Theorem 4.1 easily extends to point processes which may have a ﬁnite
number of jumps, or not vanish at zero. It yields two degenerate point processes:
• the ﬁrst for λ = 0 with Nt = 0 and Tn = ∞and Sn = ∞;
• the second for λ = ∞with Nt = ∞and Tn = 0 and Sn = 0.
A common abuse of language is to say that a Poisson process with intensity 0
corresponds to the ﬁrst of these, a Poisson process with intensity ∞to the second,
and that an E (0) exponential r.v. is inﬁnite and an E (∞) exponential r.v. vanishes,

4.2
Poisson Processes: Characterization, Properties
75
a.s. This allows to avoid being explicit about some trivial particular cases in some
statements, but must be used with caution since it contradicts statements such as
“exponential r.v.’s have a density”. It is safer to be speciﬁc about the fact that these
are degenerate.
The following exercise corresponds to i.i.d. jump sizes, instead of unit ones.
Problem 4.3 will study a case with time-varying intensity.
Exercise 4.2 (Compound Poisson process) Take a Poisson process (Nt)t∈R+ with
intensity λ, and independently a sequence (Xn)n≥1 of i.i.d. real r.v.’s with character-
istic function φ. Let Zt = Nt
n=1 Xn. Prove for every n ≥1 and 0 = t0 < t1 < ··· <
tn that
E

ei n
k=1 uk(Ztk −Ztk−1)
=
n

k=1
eλ(tk−tk−1)(φ(uk)−1),
u1,...,un ∈R.
Deduce from this that (Zt)t≥0 has independent and stationary increments.
4.2.2 Simple and Strong Markov Property
Let (Nt)t∈R+ be a Poisson process with intensity λ. For any t ≥0, the process
(Nt+h −Nt)h∈R+
counting all instants after time t is a point process with independent and stationary
increments, and moreover E(Nt+1 −Nt) = λ. Theorem 4.1 yields that it is then also
a Poisson process with intensity λ. Moreover, it is independent of (Ns)0≤s≤t. See
Fig. 4.2.
This fundamental property of lack of memory given the present and of temporal
stability is a special case of the Markov property to be seen later.
It remains true if time t (which does not depend on chance) is replaced by certain
random times T , for instance for T independent of (Nt)t∈R+, or T = Tn for n ≥1.
On the contrary, for example it is a simple matter to check that for
M = inf

n ≥2 : |Tn+1 −Tn−1| < 1

,
T = TM,
it holds that P(M < ∞) = 1, but that the process (NT +h −NT )h∈R+ is neither
Poisson nor independent of (Ns)0≤s≤T .
The following assumption on T prevents that it carry such a knowledge of the
future behavior of the process, which transmits a memory.
Deﬁnition 4.3 A stopping time for (Nt)t∈R+ is a r.v. T with values in R+ ∪{∞},
such that for every t ∈R+ the event {T ≤t} belongs to the σ-ﬁeld σ((Ns)0≤s≤t).
Technically, this means that there exists a set of paths At which is measurable for
the product σ-ﬁeld and such that {T ≤t} = {(Ns)0≤s≤t ∈At}.

76
4
Poisson Processes as Particular Markov Processes
Fig. 4.2 The strong Markov property of a Poisson process of intensity λ, where T is a stopping
time. The simple Markov property corresponds to the case when T = t, a.s.
Obviously, a deterministic r.v. T (such that P(T = t) = 1 for some t ∈R+) is a
stopping time. The jump instants Tn are also stopping times.
Exercise 4.3 Prove that the Tns are stopping times.
The Markov property for stopping times will be called the strong Markov prop-
erty. The one for deterministic times is called the simple Markov property if such
emphasis is needed.
Theorem 4.3 (Strong Markov property) Let (Nt)t∈R+ be a Poisson process with
intensity λ, and T be a stopping time. Conditional on T < ∞, the process
(NT +h −NT )h∈R+
is a Poisson process with intensity λ independent of (Ns)0≤s≤T , i.e., independent of
T and (NT ∧s)s≥0. See Fig. 4.2.
Proof Conditioning allows to assume that T < ∞, a.s. First, it is enough to prove
the result for stopping times with values in a discrete set: indeed, since T is the limit
of the non-increasing sequence of stopping times
T n =

k∈N
k
2n 1{ k−1
2n <T ≤k
2n },
n ∈N.
If the statement is true for all these T n then the right-continuity of sample-paths
allows to take limits and shows that the statement is also true for T .

4.2
Poisson Processes: Characterization, Properties
77
Now, let T take values in an increasing sequence (tj)j≥0. Since
{T = tj} = {T ≤tj} −{T ≤tj−1} ∈σ

(Ns)0≤s≤tj

,
the simple Markov property at the times tj yields, for 0 ≤s1 < ··· < sn ≤t and
0 ≤h1 < ··· < hn and x1,...,xn,y1,...,yn ∈N, that
P

T ≤t,(NT ∧sk)1≤k≤n = (xk)1≤k≤n,(NT +hk −NT )1≤k≤n = (yk)1≤k≤n

=

tj ≤t
P

T = tj,(Ntj ∧sk)1≤k≤n = (xk)1≤k≤n,(Ntj +hk −Ntj )1≤k≤n = (yk)1≤k≤n

=

tj ≤t
P

T = tj,(Ntj ∧sk)1≤k≤n = (xk)1≤k≤n

P

(Nhk)1≤k≤n = (yk)1≤k≤n

= P

T ≤t,(NT ∧sk)1≤k≤n = (xk)1≤k≤n

P

(Nhk)1≤k≤n = (yk)1≤k≤n

,
which sufﬁces to conclude in this special case.
□
Remark 4.1 This result is still true for T such that the {T ≤t} can be expressed
not only in terms of (Ns)0≤s≤t but also of events which are independent of
(Nt+h −Nt)h∈R+. For other generalizations, see below the proof of Theorem 4.4
or Exercise 5.2.
4.2.3 Superposition and Decomposition
The following are two natural operations on point processes:
• the sum or superposition, which adds the counting processes, and reunites the
points (which must then be renumbered) into a single process;
• the decomposition or marking, which marks the points of a process, then reunites
points accordingly to form a family of processes indexed by the marks.
In the following, Theorem 4.1 readily yields that the resulting processes are Pois-
son. The remarkable independence properties achieve the symmetry of the state-
ment, and are related to the strong Markov property.
Theorem 4.4 (Superposition-decomposition) Let I be a countable set.
(a) Let (Ni
t )t≥0 be independent Poisson processes with intensities λi > 0 for i in
I satisfying λ := 
i∈I λi < ∞. Then (Nt)t≥0 deﬁned by
Nt :=

i∈I
Ni
t
is a Poisson process with intensity λ, and is called the superposition, or the sum,
of the (Ni
t )t≥0.

78
4
Poisson Processes as Particular Markov Processes
Moreover, for n ≥1, let Tn be the nth jump instant of (Nt)t≥0, and Yn the
I -valued r.v. given by
{Yn = i} :=

Ni
Tn ̸= Ni
Tn−

,
i ∈I .
Then (Yn)n≥1 is a sequence of i.i.d. r.v.’s independent of (Nt)t≥0, and
P(Y1 = i) = λi
λ =
λi

j∈I λj
,
i ∈I .
(b) Let (Nt)t≥0 be a Poisson process with intensity λ > 0 and (Tn)n≥1 its jump
instants. Let (Yn)n≥1 be a sequence of i.i.d. r.v.’s with values in I , independent
of (Nt)t≥0 and such that P(Y1 = i) = pi > 0 for i in I . Then the (Ni
t )t≥0
deﬁned by
Ni
t :=

n≥1
1{Tn≤t,Yn=i},
i ∈I ,
are independent Poisson processes with intensities λi = λpi, and are called the
decomposition, or marking, of (Nt)t≥0 according to (Yn)n≥1.
Notably, the sampling or erasing property holds: retaining among the in-
stants of a Poisson process with intensity λ > 0 only those marked in i.i.d. fash-
ion with probability p > 0 yields a Poisson process with intensity λp.
Proof (a) Let us ﬁrst assume that I is ﬁnite. Recall Deﬁnitions 4.1 and 4.2. The
(Ni
t )t∈R+ are point processes with independent and stationary increments (The-
orem 4.1) and are independent, hence their ﬁnite sum (Nt)t∈R+ is also such a
process; the only fact which is not immediate is that it has unit jumps, i.e., that
Nt −Nt−∈{0,1}, which we now proceed to prove. For ε > 0 and k ≥1 and i ̸= j,
independence implies that
P

Ni
kε −Ni
(k−1)ε ≥1,Nj
kε −Nj
(k−1)ε ≥1

=

1 −eλiε
1 −eλj ε
≤λiλjε2
which with (4.2) applied to the Ni yields
P(Nkε −N(k−1)ε ≥2) ≤1
2

i∈I
λ2
i ε2 + 1
2

i̸=j
λiλjε2 = 1
2λ2ε2,
P

∃t ∈[0,T ] : Nt −Nt−≥2

≤1
2⌈T /ε⌉λ2ε2 −−→
ε→0 0,
∀T > 0.
Then, Theorem 4.1 yields that (Nt)t∈R+ is a Poisson process with intensity
E(N1) =

i∈I
E

Ni
1

=

i∈I
λi = λ.
The case when I is countably inﬁnite follows by taking increasing limit of sums
on ﬁnite subsets and using the convergence of the series 
i∈I λi. In particular Ni
and Nj do not jump simultaneously for i ̸= j, and the Yn are well deﬁned.

4.2
Poisson Processes: Characterization, Properties
79
Now, for n ≥1 and t ≥0 it holds that
{Tn ≤t} =

i∈I
Ni
t ≥n

so that Tn is a stopping time for the family of processes ((Ni
t )t≥0 : i ∈I ). The proof
of Theorem 4.3 (strong Markov property) can be easily extended to show that the
family

Ni
Tn+h −Ni
Tn

h≥0,
i ∈I ,
is constituted of independent Poisson processes with intensities λi, and is indepen-
dent of the family (Ni
s)0≤s≤Tn for i in I . This yields that (Yn+1,Tn+1) is inde-
pendent of the (Ni
s)0≤s≤Tn and hence of (Yk,Tk)1≤k≤n, and has the same law as
(Y1,T1). Moreover, for i in I and t ≥0,
P(Y1 = i,T1 > t) = P

Sj
1 > Si
1 > t : j ̸= i

= λi
 ∞
t
e−λis 
j̸=i
P

Sj
1 > s : j ̸= i

ds
= λi
 ∞
t
e−
j∈I λj s ds
= λi
λ e−λt
which characterizes the law of Y1 and shows that Y1 and T1 are independent. Induc-
tively, the sequence (Yn)n≥1 is constituted of i.i.d. r.v. with said law, and is indepen-
dent of (Tn)n≥1 and hence of (Nt)t∈R+.
(b) For i in I , clearly (Ni
t )t≥0 is a point process with independent and stationary
increments, and hence a Poisson process with intensity
E

Ni
1

=

n≥1
E(1{Tn≤1,Yn=i}) = pi

n≥1
E(1{Tn≤1}) = piE(N1) = λi.
In order to prove that the (Ni
t )t≥0 are independent, it is sufﬁcient to prove that the
(Ni
tk −Ni
tk−1)1≤k≤n are independent for every 0 = t0 < t1 < ··· < tn, and since the
increments of (Ni
t )t≥0 are independent and stationary, it is sufﬁcient to prove that
the Ni
t are independent for every t ≥0. Using the multinomial law, for all ni ∈N
for i in I such that n = 
i∈I ni < ∞it holds that
P

Ni
t = ni,i ∈I

= P(Nt = n)P

Ni
t = ni,i ∈I |Nt = n

= e−λt (λt)n
n!
n!
)
i∈I ni!

i∈I
pni
i = e−λt 
i∈I
(λpit)ni
ni!
,
and hence the Ni
t are independent.
□

80
4
Poisson Processes as Particular Markov Processes
Application to Exponential Random Variables
The construction, and hence simulation, of Poisson processes relies on exponential
random variables, and we state an important corollary.
Corollary 4.1 Let I be a countable set, λi > 0 for i in I be such that λ :=

i∈I λi < ∞, and Si be independent r.v.’s with E (λi) exponential laws. Then the
r.v.
S := inf
i∈I Si
has E (λ) exponential law, the inﬁmum is attained at a unique random index Y in-
dependent of S and with law given by
P(Y = i) = λi
λ =
λi

j∈I λj
,
i ∈I ,
and conditionally on Y = i, the Sj −Si for j ̸= i form a family of independent r.v.’s
with E (λj) exponential laws which is independent of S.
Exercise 4.4 Prove this corollary using Theorem 4.4.
4.3 Simulation and Approximation
Exact and approximate simulation of Poisson processes will be discussed, as well
as some statistical and computational issues.
4.3.1 Simulation of Inter-arrivals
As seen in Theorem 4.1, the inter-arrivals (Sn)n≥1 of a Poisson process of intensity
λ > 0 are i.i.d. E (λ) exponential r.v.’s, and the jump instants (Tn)n≥1 and counting
process (Nt)t∈R+ can be reconstituted from this sequence.
As seen in Sect. 2.2.4, (Sn)n≥1 can be simulated using an i.i.d. sequence (Un)n≥1
of uniform r.v.’s on [0,1] by setting
Sn = −1
λ logUn,
n ≥1.
From a programming perspective, such a sequence (Un)n≥1 is obtained by repeated
calls of a function such as rand. Many programming languages have ready-made
functions that yield draws according to the E (λ) law; if this is not the case, such a
function must be carefully deﬁned.

4.3
Simulation and Approximation
81
Algorithm (Poisson process simulation) To simulate a sample path of a Poisson
process of intensity λ: set t0 = 0, then iteratively for n ≥1:
• draw a sample sn from the E (λ) exponential law, and set tn = tn−1 + sn for the
value taken by Tn, the nth jump instant,
• for tn ≤t < tn+1, the counting function Nt takes the value n.
This terminates either when the simulated instant goes beyond some temporal
horizon that was ﬁxed beforehand, or due to some other criterion.
In order to simulate a Poisson P(λ) r.v. with a parameter λ which is not “too
large”, the most effective algorithm is usually to simulate a Poisson process (Nt)t≥0
with intensity 1 for a duration λ and take its ﬁnal value Nλ.
Approximations for large values of λ will be given in Sect. 4.3.3.
Exercise 4.5 Let (Un)n≥1 be i.i.d. uniform r.v.’s on [0,1]. Give the law of
N = min
*
n ∈N :
n+1

k=1
Uk < e−λ
+
.
What is the use of such a formulation?
4.3.2 Simulation of Independent Poisson Processes
Theorem 4.4 and its Corollary 4.1 are essential for simulation. In order to simulate,
for i in I , independent Poisson processes (Ni
t )t≥0 with intensities λi that have a
ﬁnite sum, the by far most economical and practical method is to simulate a Poisson
process (Nt)t≥0 with intensity λ and i.i.d. r.v.’s Yn with values in I for n ≥1 such
that
λ =

i∈I
λi < ∞,
P(Y1 = i) = λi
λ ,
i ∈I ,
and then to recover the (Ni
t )t≥0 using Theorem 4.4(b). The corresponding algorithm
can be summarized as follows.
Algorithm (Independent Poisson process simulation) To simulate the sample paths
of independent Poisson processes of intensity λi, for i in I : compute λ = 
i∈I λi,
set t0 = 0 and xi = 0 for i in I , then iteratively for n ≥1:
• draw a sample sn from the E (λ) exponential law and set tn = tn−1 + sn (the value
of Tn, the nth jump instant of the superposition),
• draw a sample k in I from the discrete law (λi/λ)i∈I , set xk
n = xk−1
n
+ 1 and
xi
n = xi−1
n
for i ̸= k,
• for tn ≤t < tn+1, the Ni
t take the value xi
n for all i in I .

82
4
Poisson Processes as Particular Markov Processes
In contrast, the most direct and naive simulation algorithm, numerically much
less efﬁcient, would be as follows.
Algorithm (Simulation of renewal processes) To simulate the sample paths of
independent Poisson processes of intensity λi, for i in I : set t0 = 0 and draw
samples si
1 from the E (λi) exponential law for i in I , then iteratively for
n ≥1:
• determine the least inter-arrival and corresponding index
sn := min
i∈I si
n,
kn := argmin
i∈I
si
n,
and set tkn
n = tkn
n−1 + sn for the next jump instant of (Nkn
t )t≥0,
• set si
n+1 = si
n −sn for i ̸= kn and draw a new sample skn
n+1 from the E (λkn)
law.
This method can be used to simulate an independent family of point process with
arbitrary i.i.d. inter-arrivals, which are called renewal processes. This exempliﬁes
that it does not exploit at all the strong Markov property of the Poisson processes;
for instance, another inefﬁcient algorithm would be as follows.
Algorithm (Intermediate method) To simulate the sample paths of independent
Poisson processes of intensity λi, for i in I : set t0 = 0, then iteratively for n ≥1:
• draw samples si
n from the E (λi) exponential law for i in I ,
• determine the least inter-arrival and corresponding index
sn := min
i∈I si
n,
kn := argmin
i∈I
si
n,
and set tkn
n = tkn
n−1 + sn for the next jump instant of (Nkn
t )t≥0,
The initial, and most efﬁcient, algorithm is obtained by putting to best use the
superposition-decomposition property.
Exercise 4.6 Justify that the three algorithms allow to sample trajectories of the
Poisson process.
4.3.3 Long Time or Large Intensity Limit, Applications
The asymptotic behavior of Poisson processes in long times, or with large intensi-
ties, is of interest. Recall that E(Nt) = var(Nt) = λt.

4.3
Simulation and Approximation
83
Theorem 4.5 (SLLN and CLT)
(a) A Poisson process (Nt)t≥0 with intensity λ satisﬁes the Strong Law of Large
Numbers and the Central Limit Theorem
Nt
t
a.s.
−−−→
t→∞λ,
Nt −λt
√
λt
in law
−−−→
t→∞N (0,1).
(b) Poisson processes (Nλ
t )t≥0 with intensities λ > 0 satisfy the Weak Law of Large
Numbers and the Central Limit Theorem for every ﬁxed t > 0
Nλ
t
λ
in probab
−−−−−→
λ→∞
t,
Nλ
t −λt
√
λt
in law
−−−→
λ→∞N (0,1).
Proof (a) For n ≤t < n + 1 it holds that
n
n + 1
1
n
n

k=1
(Nk −Nk−1) =
Nn
n + 1 ≤Nt
t ≤Nn+1
n
= n + 1
n
1
n + 1
n+1

k=1
(Nk −Nk−1)
and since the increments of (Nt)t≥0 are independent and stationary, the strong law
of large numbers for i.i.d. r.v., applied to the lower and upper bound, yields the a.s.
result. To prove the Central Limit Theorem, we use that, for n ≥0,
Nt −λt
√
λt
= Nn −λn
√
λn
n
t + Nt −Nn
√
λt
+ n −t
√t
√
λ,
n ≤t < n + 1,
where limt→∞n−t
√t = 0 and limt→∞
#
n
t = 1. Moreover, for any ε > 0,
P

Nt −Nn
√
λt
 > ε

= P(Nt−n > ε
√
λt) ≤P(N1 > ε
√
λt) −−−→
t→∞0.
Lastly, the Central Limit Theorem for i.i.d. r.v. yields
Nn −λn
√
λn
=
1
√
λn
n

k=1
(Nk −Nk−1 −λ)
in law
−−−→
n→∞N (0,1).
From these facts, the characteristic function of Nλ
t −λt
√
λt
converges pointwise to the
characteristic function of the N (0,1) law, yielding convergence in law.
(b) These results in law follow immediately from (a), using the fact that Nλ
t and
Nt are Poisson P(θ)-r.v.’s with θ = λt going to inﬁnity when one of λ or t goes to
inﬁnity and the other is ﬁxed.
□
Problem 4.2 will provide a direct proof for the Central Limit Theorem, as well as
concentration inequalities, for Poisson r.v.’s.
Problem 4.4 will state and provide elements of proof for a stronger version of
the Central Limit Theorem: the Brownian limit for an appropriately recentered and
normalized Poisson process.

84
4
Poisson Processes as Particular Markov Processes
Estimation of the Intensity and Simulation Duration
It appears that the important parameter in these results is θ := λt. From a statistical
perspective:
• the SLLN provides an unbiased consistent estimator for the intensity λ of an
observed Poisson process of rate λ on a time interval [0,t],
• the CLT provides asymptotic conﬁdence intervals for this estimation, which are
very good except for small values of θ.
Algorithm (Estimation of intensity) To estimate the intensity λ of a Poisson pro-
cess (Ns)s≥0 observed on a time interval [0,t]:
• estimate λ by ˆλt = Nt
t ,
• derive asymptotic conﬁdence intervals, etc., by pretending that Nt
t −λ follows the
N (0,
#
ˆλt/t) Gaussian law.
Concentration inequalities are available if need be, see Problem 4.2.
After this incursion into statistics, let us come back to Monte Carlo simulation:
• the SLLN shows that the duration of the exact simulation of a Poisson process
(Ns)s≥0 of intensity λ on a time interval [0,t] is of approximately θ simulation
step durations,
• the CLT yields asymptotic conﬁdence intervals for this estimation, which are very
good except for small values of θ (for which the simulation is very short).
This allows to estimate the cost (in machine time) of such a simulation, which is
proportional to θ.
Exercise 4.7 For not too small θ, show that the probability that this duration ex-
ceeds θ + 1.96
√
θ simulation steps is approximately 2.5 %.
Approximate Simulation
Clearly, problems arise for large values of θ := λt.
These are even more apparent when one wishes to simulate a P(θ) Poisson r.v.,
since then the numerous intermediate instants on [0,θ] of the Poisson process with
intensity 1, computed by the classic simulation method, are not further used.
Approximate simulation methods based on Theorem 4.5 may then be used. For
large θ, the lengthy exact simulation of a P(θ) Poisson r.v. is then replaced:
• by the deterministic approximation given by its expectation θ with asymptotic
conﬁdence intervals,
• or, better still, by the more precise random approximation obtained by adding to
θ a N (0,1) Gaussian r.v. multiplied by
√
θ,
rounding appropriately if one needs an integer value.

4.4
Problems
85
4.4 Problems
4.1 (Law of the Instants of a Poisson Process) Let (Nt)t∈R+ be a Poisson process
with intensity λ > 0 and instants (Tn)n≥1.
1. Prove that the law of (T1,...,Tn) has density λn10<t1<···<tne−λtn on Rn, and that
the law of Tn is gamma Γ (n,λ), with density
λn
(n−1)!1t>0tn−1e−λt on R.
2. Prove that conditionally on Nt = n the vector (T1,...,Tn) has uniform law on
the simplex {(t1,...,tn) : 0 < t1 < ··· < tn < t} of Rn. Prove that this law is also
the law of the increasing reordering of n i.i.d. uniform r.v.’s on [0,t].
3. Prove that conditionally on Tn+1 = t the vector (T1,...,Tn) again has uniform
law on the simplex {(t1,...,tn) : 0 < t1 < ··· < tn < t} of Rn.
4. Compute the law of ( T1
Tn+1 ,...,
Tn
Tn+1 ).
4.2 (Central Limit Theorem and Concentration Inequalities for Poisson Laws) For
each λ > 0, let Xλ be a Poisson P(λ) random variable and Wλ = Xλ−λ
√
λ .
1. Compute the generalized Laplace transform ψλ(z) := E(ezXλ) for z ∈C.
2. Deduce from this that limλ→∞Wλ in law exists, and give the limit law.
3. Prove that, for a ≥0 and θ > 0,
P(Xλ −λ ≥a) ≤eλeθ−λ−θ(λ+a),
P(Xλ −λ ≥a) ≤ea−(λ+a)log(1+ a
λ ) ≤e−a2
2λ (1−a
λ ).
Deduce from this a bound for P(Wλ ≥c) for c ≥0.
4. Prove that, for b ≥0 and θ > 0,
P(Xλ −λ ≤−b) ≤eλe−θ−λ+θ(λ−b),
P(Xλ −λ ≤−b) ≤e−b−(λ−b)log(1−b
λ ),
and then (one must keep b far from λ, the inequality becoming weak)
P(Xλ −λ ≤−b) ≤e−b2
2λ (1−b
λ )+(λ−b) 8b3
3λ3 ,
0 ≤b ≤λ
2.
Deduce from this a bound for P(Wλ ≤d) for d ≤0.
4.3 (Inhomogeneous Poisson Process (⋆)) Let θ : R+ →R+ be a locally integrable
function such that
 ∞
0 θ(s)ds = ∞. The objective is to construct a point process
(Nt)t∈R+ with independent increments and such that for t,h ≥0 the law of Nt+h −
Nt is a function of the quantity
 t+h
t
θ(s)ds only (and not t).
Let (Tn)n≥1 be the jump instants of (Nt)t∈R+, and ( ˆTn)n≥1 those of the point
process ( ˆNt)t∈R+ given by
Θ(t) =
 t
0
θ(s)ds,
A(t) = inf

u ∈R+ : Θ(u) ≥t

,
ˆNt = NA(t).

86
4
Poisson Processes as Particular Markov Processes
1. Let t ∈R+. Prove that
Θ

A(t)

= t,
A

Θ(t)

= inf

u ∈R+ : Θ(u) = Θ(t)

≤t,
Nt = ˆNΘ(t).
Deduce from this that Tn = A( ˆTn) and ˆTn = Θ(Tn) for n ≥1.
2. Prove that ( ˆNt)t∈R+ is a Poisson process.
This determines the law of (Nt)t∈R+ up to the intensity λ > 0 of ( ˆNt)t∈R+, i.e.,
up to time scale or a multiplicative factor for θ, which must be ﬁxed. A natural
choice is λ = 1, so that E(Nt) = Θ(t).
3. Suggest a method for simulating (Nt)t∈R+. What problem can arise in practice?
4. Describe the law of (Nt1,Nt2 −Nt1,...,Ntn −Ntn−1) for 0 < t1 < ··· < tn.
5. Prove that
P(Tn+1 −Tn ≥t |T1,...,Tn) = P(Tn+1 −Tn ≥t |Tn) = eΘ(Tn+t)−Θ(Tn).
6. Let (Uk)k≥1 be i.i.d. uniform r.v. on [0,1]. Prove that (Tn)n≥1 can be simulated
by setting
Tn = inf
*
t ≥0 : Θ(t) ≥−
n

k=1
logUk
+
= inf
*
t ≥0 :
n

k=1
Uk ≥e−Θ(t)
+
.
7. Assume that supt>0 θ(t) ≤λ. Let (N∗
t )t∈R+ be a Poisson process with inten-
sity λ and instants (T ∗
n )n≥1, and independently let (Vk)k≥1 be i.i.d. uniform r.v.
on [0,1]. Prove that (Nt)t∈R+ can be simulated by setting
Nt =

n≥1
1{T ∗n ≤t,λVn≤θ(T ∗n )}.
4.4 (Brownian Limit for the Poisson Process (⋆)) The goal of this problem is to
establish the principal elements for the proof of the following result, which gives an
approximation in certain scales for the Poisson process.
Theorem 4.6 Let (Nt)t≥0 be a Poisson process with intensity 1. Then

W θ
t

t≥0 :=
Nθt −θt
√
θ

t≥0
in law
−−−→
θ→∞(Wt)t≥0
where (Wt)t≥0 is a standard Brownian motion.
1. Let Xx be a Poisson P(x) random variable, with arbitrary parameter x > 0.
(a) Compute the generalized Laplace transform ψx(z) := E(ezXx) for z ∈C.
(b) Deduce from this that
E(Xx) = x,
E

X2
x

= x + x2,
E

X3
x

= x + 3x2 + x3,
E

X4
x

= x + 7x2 + 6x3 + x4.

4.4
Problems
87
2. We are going to prove a convergence result for ﬁnite-dimensional marginals.
(a) Prove for all t ≥0 that
W θ
t
in law
−−−→
θ→∞Wt.
(b) Prove for all 0 ≤t1 < ··· < tn that (with t0 := 0 by convention)

W θ
t1 −W θ
t0,...,W θ
tn −W θ
tn−1
 in law
−−−→
θ→∞(Wt1 −Wt0,...,Wtn −Wtn−1).
(c) Prove for all 0 ≤t1 < ··· < tn that

W θ
t1,...,W θ
tn
 in law
−−−→
θ→∞(Wt1,...,Wtn).
3. In the sequel, T > 0 and ε > 0 and a > 0 are arbitrary.
(a) Prove for K(T,ε) ∈N given by K(T,ε)ε < T ≤K(T,ε)ε + ε that
sup
0≤s,t≤T,|t−s|≤ε
W θ
t −W θ
s

≤3sup
,
sup
kε≤t≤kε+ε
W θ
t −W θ
kε
 : k = 0,...,K(T,ε)
-
.
(b) Prove that
P

sup
0≤s,t≤T,|t−s|≤ε
W θ
t −W θ
s
 ≥3a

≤T + ε
ε
P

sup
0≤t≤ε
W θ
t
 ≥a

.
4. We are going to prove a classic result. Let (Mn)n∈N be a discrete-time non-
negative sub-martingale. Let m ≥1 be an integer, c > 0 a real number, and
τ = inf{n ∈N : Mn ≥c} ∧m
(τ = m if Mn < c for 0 ≤n ≤m).
Prove that
P

sup
0≤n≤m
Mn ≥c

= P(Mτ ≥c) ≤1
c E(Mτ) ≤1
c E(Mm).
We admit that this can be generalized to continuous time as follows:
Lemma 4.1 (Doob’s maximal sub-martingale inequality) Let (Mt)t∈R+ be a non-
negative sub-martingale, right-hand continuous with left-hand limits. Let u > 0 and
c > 0 be real numbers. Then
P

sup
0≤t≤u
Mt ≥c

≤1
c E(Mu).

88
4
Poisson Processes as Particular Markov Processes
5. We are going to obtain a fundamental bound.
(a) Prove that (W θ
t )t≥0 is a martingale, for every θ > 0.
(b) Prove that (|W θ
t |p)t≥0 is a sub-martingale, for every θ > 0 and p ≥1 .
(c) Prove that
P

sup
0≤s,t≤T,|t−s|≤ε
W θ
t −W θ
s
 ≥3a

≤T + ε
a4
1
θ + 3ε

.
Complementary Elements for the Interested Reader
This theorem is very
powerful, and concerns the approximation of functionals of the whole process, such
as extrema or hitting times. A complete rigorous proof—and even an actual under-
standing of the statement—requires tools far beyond those introduced in this course,
since it bears on convergence in law of processes, i.e., of weak convergence of their
laws on the Skorokhod space D(R+,R).
The last bound we have established concerns the modulus of continuity of
(W θ
t )t∈[0,T ]. For every T > 0 and a > 0 and δ > 0, it allows to ﬁnd ε > 0 small
enough that, for θ large, δ is a bound for the probability that this modulus of conti-
nuity exceeds 2a. Moreover, W θ
0 = 0.
The Ascoli–Arzela theorem characterizes the subsets with compact closure of
C([0,T ],R), for the uniform norm, as being uniformly bounded and uniformly con-
tinuous. An adaptation of this result shows that here there exists a compact subset
A(δ) of C([0,T ],R), and a measurable subset B(δ) of D([0,T ],R) within uniform
distance δ of A(δ), such that for θ large
P

W θ
t

t∈[0,T ] /∈B(δ)

≤δ.
This and an advanced result in probability theory, the Prokhorov theorem, allow
to show that any sub-sequence in θ of the laws of (W θ
t )t∈[0,T ] contains a further
sub-sequence weakly converging to a probability measure carried by C([0,T ],R).
The result on the convergence of ﬁnite-dimensional marginals, together with the
fact that these characterize the law of a process, show that the limit of every such
converging sub-sequence is the law of a standard Brownian motion.
This compactness-uniqueness method proves the convergence result. For a full
treatment of this important subject, see Billingsley [6].

Chapter 5
Discrete-Space Markov Processes
Abstract A rather detailed study of Markov processes with discrete state space is
provided. It focuses on sample path techniques in a perspective inspired by simu-
lation needs. The relationship of these processes with Poisson processes and with
discrete-time Markov chains is shown. Rigorous constructions and results are pro-
vided for Markov process with uniformly bounded jump rates. To this end, elements
of the theory of bounded operators are introduced, which explain the relation be-
tween generator and semigroup, and provide a useful framework for the forward
and backward Kolmogorov equations and the Feynman–Kac formula.
Anderson [1] is a reference book on the topic, with a much wider scope than here.
Asmussen and Glynn [4] provides a short primer on the theory, and many queueing
examples.
5.1 Characterization, Speciﬁcation, Properties
We start with some general properties of a Markov process assuming that it exists.
Measure theory on a discrete space reduces to series summation, and we give the
deﬁnitions in this framework.
5.1.1 Measures, Functions, and Transition Matrices
Positive and Signed Measures, Integrals of Functions
For μ = (μ(x))x∈V such that μ(x) ∈R+, let
μ(A) :=

x∈A
μ(x) ∈R+ ∪{∞} = [0,∞],
A ⊂V ,
be deﬁned in the sense of positive series. Such a set-function is called a measure
on V , and sometimes a positive measure for precision. The measure μ is said to be
ﬁnite if its total mass μ(V ) is ﬁnite, and to be a probability measure, or a law, if
μ(V ) = 1. The space of probability measures is denoted by P = P(V ).
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1_5, © Springer-Verlag Berlin Heidelberg 2013
89

90
5
Discrete-Space Markov Processes
For μ = (μ(x))x∈V with μ(x) ∈R such that 
x∈V |μ(x)| < ∞, let
μ(A) :=

x∈A
μ(x) ∈R,
A ⊂V ,
be deﬁned in the sense of absolutely convergent series. Such a set-function is called
a signed measure on V . The vector space of signed measures is denoted by M =
M (V ). Among positive measures, only ﬁnite ones belong to M .
A duality bracket between measures μ and functions f is given by,
(μ,f ) →⟨μ,f ⟩:=

x∈V
μ(x)f (x),
(5.1)
in the sense of positive series if μ and f are positive (wide sense), and in the sense
of absolutely convergent series if μ belongs to M and f to the Banach space of
bounded functions L∞= L∞(V ).
Markovian Matrices and Their Actions, Line and Column Vectors
In the following, classic matrix multiplication conventions are used. A positive or
signed measure μ on V is considered as a line vector (μ(x))x∈V , a function f on
V as a column vector (f (x))x∈V , both of inﬁnite length if Card(V ) = ∞, and their
matrix product corresponds to the above duality bracket
μf =
··· μ(x) ···
⎛
⎜⎜⎝
...
f (x)
...
⎞
⎟⎟⎠=

x∈V
μ(x)f (x) = ⟨μ,f ⟩.
(5.2)
A matrix P is said to be a transition or Markovian matrix if
P =

P(x,y)

x,y∈V ,
P(x,y) ≥0,

y∈V
P(x,y) = 1,
i.e., if its line vectors P(x,·) = (P(x,y))y∈V constitute probability measures on V
for all x. Multiplying a Markovian matrix P on its right by a non-negative [resp.
bounded] function f yields a non-negative [resp. bounded] function Pf , on its left
by a positive [resp. signed] measure μ yields a positive [resp. signed] measure μP ,
speciﬁcally
Pf : x ∈V →Pf (x) = P(x,·)f =

y∈V
P(x,y)f (y),
μP =

μP(y)

y∈V ,
μP(y) =

x∈V
μ(x)P(x,y),

5.1
Characterization, Speciﬁcation, Properties
91
and in matrix notation
Pf =
⎛
⎜⎜⎝
...
...
...
··· P(x,y) ···
...
...
...
⎞
⎟⎟⎠
⎛
⎜⎜⎝
...
f (y)
...
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
...
Pf (x) = 
y∈V P(x,y)f (y)
...
⎞
⎟⎟⎠,
μP =
··· μ(x) ···
⎛
⎜⎜⎝
...
...
...
··· P(x,y) ···
...
...
...
⎞
⎟⎟⎠=
··· μP(y) = 
x∈V μ(x)P(x,y) ···
.
Moreover, in [0,∞] [resp. R],
μPf =

x,y∈V
μ(x)P(x,y)f (y) =
··· μ(x) ···
⎛
⎜⎜⎝
...
...
...
··· P(x,y) ···
...
...
...
⎞
⎟⎟⎠
⎛
⎜⎜⎝
...
f (y)
...
⎞
⎟⎟⎠.
Remark 5.1 A Markovian matrix P = (P(x,y))x,y∈V thus corresponds to an op-
erator on L∞and to another on M . The ﬁrst operator has matrix P in the base
in which g ∈L∞has for coordinates its values (g(x))x∈V , the second has matrix
P ∗, the transpose of P , in the base in which ν ∈M has for coordinates its atoms
(ν(x))x∈V . Identifying these operators to their matrices,
P : f ∈L∞→Pf ∈L∞,
P ∗: μ ∈M →μP ∈M .
These operators are adjoint for the duality bracket (5.1) since
⟨μ,Pf ⟩= μPf =
4
P ∗μ,f
5
.
5.1.2 Simple and Strong Markov Property
Markov Property, Transition Matrices, and Instantaneous Laws
This book studies mainly homogeneous (in time) Markov processes, and not more
general inhomogeneous Markov processes, for which in Deﬁnition 5.1 below
P(Xh = y |X0 = x) would be replaced by P(Xt+h = y |Xt = x).
In the sequel, the term “homogeneous” will be implicitly understood, as well as
the fact that the conditional probabilities are limited to the cases when they are well
deﬁned, i.e., when the conditioning event has non-zero probability.
A Markov process with values in a discrete space is sometimes also called a
continuous-time Markov chain.

92
5
Discrete-Space Markov Processes
Fig. 5.1 The strong Markov property (b), where T is a stopping time. The simple Markov property
corresponds to taking T = t, a.s.
Deﬁnition 5.1 A process (Xt)t∈R+ with values in a discrete space V is a Markov
process if, for all n ≥0 and 0 ≤s1 < ··· < sn < t and h ≥0 and x1,...,xn, x, y
in V ,
P(Xt+h = y |Xs1 = x1,...,Xsn = xn,Xt = x) = P(Xh = y |X0 = x).
Its transition matrices are deﬁned for t ∈R+ by
Pt =

Pt(x,y)

x,y∈V ,
Pt(x,y) = P(Xt = y |X0 = x).
The line vector Pt(x,·) is the conditional law of Xt given that X0 = x, as well as of
Xs+t given that Xs = x for any s ≥0.
A fundamental result is that this property is equivalent to properties which seem
at ﬁrst stronger, and which are of more immediate use for proofs.
Theorem 5.1 (Markov property) Let (Xt)t∈R+ be a Markov process with values in
a discrete space V , and (Pt)t∈R+ its transition matrices. It satisﬁes the following
properties, which are equivalent.
(a) The property of Deﬁnition 5.1.
(b) Conditional on Xt = x, the process (Xt+h)h≥0 is independent of (Xs)0≤s≤t and
has the same law as (Xh)h≥0 for X0 = x. See Fig. 5.1.

5.1
Characterization, Speciﬁcation, Properties
93
(c) For all n ≥1 and 0 = t0 ≤t1 < ··· < tn and x1, ... , xn in V ,
P(Xt1 = x1,...,Xtn = xn) =

x0∈V
P(X0 = x0)
n

k=1
Ptk−tk−1(xk−1,xk).
Proof Property (b) implies (a) as a special case. Property (a) implies that
P(Xt1 = x1,...,Xtn = xn)
= P(Xt1 = x1,...,Xtn−1 = xn−1)P(Xtn = xn |Xt1 = x1,...,Xtn−1 = xn−1)
= P(Xt1 = x1,...,Xtn−1 = xn−1)Ptn−tn−1(xn−1,xn)
which implies (c) by induction. Property (c) implies that, for arbitrary n ≥1 and
0 ≤s1 < ··· < sn < t and 0 < h1 < ··· < hn and x1,...,xn, x, y1,...,yn in V ,
P

(Xsk)1≤k≤n = (xk)1≤k≤n,Xt = x,(Xt+hk)1≤k≤n = (yk)1≤k≤n

=

x0∈V
P(X0 = x0)
n

k=1
Psk−sk−1(xk−1,xk)Pt−sn(xn,x)
× Ph1(x,y1)
n

k=2
Phk−hk−1(yk−1,yk)
= P

(Xsk)1≤k≤n = (xk)1≤k≤n,Xt = x

P

(Xhk)1≤k≤n = (yk)1≤k≤n |X0 = x

,
which implies (b).
□
Remark 5.2 The law of a Markov process (Xt)t≥0 is characterized by its ﬁnite-
dimensional marginals, which are determined in terms of the law of X0 and its tran-
sition matrices (Pt)t≥0 by Property (c). One remaining problem is that this cannot
yield existence of the process, since the (Pt)t≥0 themselves are assumed to exist.
The notations Px and Ex are used to indicate that X0 = x, and Pμ and Eμ that
X0 is of law μ ∈P. Thus
Pμ :=

x∈V
μ(x)Px,
Eμ :=

x∈V
μ(x)Ex.
For y in V , f ∈L∞and μ ∈P,
Px(Xt = y) := Pt(x,y),
Pμ(Xt = y) := μPt(y) = P ∗
t μ(y),
Ex

f (Xt)

:= Ptf (x) = Pt(x,·)f,
Eμ

f (Xt)

:= μPtf.
The notation P and E avoids stating the law of X0. A frequent abuse of notation is
P(·|X0 = x) = Px(·),
E(·|X0 = x) = Ex(·).

94
5
Discrete-Space Markov Processes
The laws of the Xt are called the instantaneous laws of the process and denoted by
πt, and π0 is called its initial law. It holds that
πt(y) := P(Xt = y)
for y ∈V ,
πt := π0Pt,
πt := Pt(x,·)
when π0 = δx.
Stopping Times, Strong Markov Property
The Markov property can again be extended to stopping times, and is then called the
strong Markov property. If it must be stressed, the property for deterministic times
is called the simple Markov property.
Deﬁnition 5.2 A stopping time for the process X = (Xt)t∈R+ is a r.v. T with values
in R+ ∪{∞} such that for every t ∈R+ the event {T ≤t} belongs to the σ-ﬁeld
F X
t := σ((Xs)0≤s≤t).
Technically, this means that there exists a set of paths At which is measurable for
the product σ-ﬁeld and such that {T ≤t} = {(Xt)0≤s≤t ∈At}.
Deterministic (non-random) times are obviously stopping times. More interesting
examples are the hitting time and the strict future hitting time of E ⊂V , given by
TE = inf{t ≥0 : Xt ∈E},
T +
E = inf{t > 0 : Xt−/∈E,Xt ∈E}.
Exercise 5.1 Prove this; recall that At ∈σ((Xs)0≤s≤t) must be expressed in terms
of a countable set of Xs due to the deﬁnition of the product σ-algebra.
Recall that on {T < ∞} the r.v. XT is deﬁned by XT (ω) = (XT (ω))(ω).
Theorem 5.2 (Strong Markov property) Let (Xt)t∈R+ be a Markov process with
values in a discrete space V , and T be a stopping time. Conditional on T < ∞
and XT = x, the process (XT +h)h≥0 is independent of (Xs)0≤s≤T , i.e., of T and
(XT ∧s)s≥0, and has the same law as (Xh)h≥0 for X0 = x. See Fig. 5.1.
Proof As for Theorem 4.3, we may consider that T takes values in an increasing
sequence (tj)j≥0. Since
{T = tj} = {T ≤tj} −{T ≤tj−1} ∈σ

(Xs)0≤s≤tj

,
the Markov property of Theorem 5.1 5.1) applied to the t = tj implies, for all
n ∈N and 0 ≤s1 < ··· < sn ≤t and 0 ≤h1 < ··· < hn and x1,...,xn and x and
y1,...,yn in V , that
P

T ≤t,(XT ∧sk)1≤k≤n = (xk)1≤k≤n,XT = x,(XT +hk)1≤k≤n = (yk)1≤k≤n

=

tj ≤t
P

T =tj,(Xtj ∧sk)1≤k≤n =(xk)1≤k≤n,Xtj =x,(Xtj +hk)1≤k≤n =(yk)1≤k≤n


5.1
Characterization, Speciﬁcation, Properties
95
=

tj ≤t
P

T =tj,(Xtj ∧sk)1≤k≤n =(xk)1≤k≤n,Xtj =x

Px

(Xhk)1≤k≤n =(yk)1≤k≤n

= P

T ≤t,(XT ∧sk)1≤k≤n = (xk)1≤k≤n,XT = x

Px

(Xhk)1≤k≤n = (yk)1≤k≤n

,
which allows to conclude.
□
Exercise 5.2 Let (Ft)t≥0 be a ﬁltration, i.e., a non-decreasing family of σ-ﬁelds. It
is said that (Xt)t≥0 is a (Ft)t≥0-Markov process if
P(Xt+h = y |Ft) = Ph(Xt,y),
t,h ≥0, y ∈V ,
and that T is a (Ft)t≥0 stopping time if {T ≤t} ∈Ft for all t. Moreover, FT is
deﬁned as the σ-ﬁeld constituted of the events A in ∨t≥0Ft such that A ∩{T ≤
t} ∈Ft. Prove that: Conditional on T < ∞and XT = x, the process (XT +h)h≥0 is
independent of FT , and that
P(XT +t+h = y |T < ∞,FT +t) = Ph(XT +t,y),
t,h ≥0, y ∈V .
5.1.3 Semigroup, Inﬁnitesimal Generator, and Evolution Law
Semigroup Property, Generator and Jump Instants
Using P(X0 = x,Xt = y) = 
z∈V P(X0 = x,Xt = z,Xt+s = y) and Theorem 5.1,
the transition matrices Pt of a Markov process satisfy the Chapman–Kolmogorov
equations
Pt+s(x,y) =

z∈V
Pt(x,z)Ps(z,y),
x,y ∈V , t,s ≥0,
in matrix notation
Pt+s = PtPs,
t,s ≥0.
Thus, (Pt)t≥0 constitutes a semigroup for matrix multiplication, which is called the
semigroup (of transition matrices) of the Markov process.
Our next theorem gathers other important structure properties of Markov pro-
cesses: the time spent in each state is exponentially distributed; the pathwise con-
struction of (Xt) is determined by the generator of its semigroup.
Theorem 5.3 Let (Xt)t∈R+ be a Markov process with values on a discrete space V ,
and (Pt)t∈R+ its semigroup. Let the jump instants (Tn)n≥1 of the process be deﬁned
by
Tn = inf{t > Tn−1 : Xt ̸= Xt−}
with the conventions T0 = 0 and XTn = XTn−1 if Tn = ∞.

96
5
Discrete-Space Markov Processes
(a) Then
⎧
⎪⎪⎨
⎪⎪⎩
0 = T0 < T1 ≤T2 ≤··· ≤∞
and lim
n→∞Tn = ∞,
Xt =

n≥0
XTn1{Tn≤t<Tn+1}
for t ≥0.
(b) There exists a matrix Q = (Q(x,y))x,y∈V such that Q = limε→0+ Pε−I
ε
in the
sense that, for every f in L∞and x ∈V ,
lim
ε→0+
Pεf (x) −f (x)
ε
= Qf (x) =

y̸=x
Q(x,y)

f (y) −f (x)

.
It satisﬁes
Q(x,y) ≥0
for x ̸= y,
q(x) := −Q(x,x) =

y̸=x
Q(x,y) ≥0,
(5.3)
the latter being equivalent to 
y∈V Q(x,y) = 0, in matrix notation Q1 = 0.
(c) Conditional on Tn−1 < ∞and XTn−1 = x, the r.v.’s
Tn −Tn−1,
XTn,
(Tk,XTk)0≤k≤n−1,
are independent. If q(x) = 0 then Tn −Tn−1 = ∞and by convention XTn = x
and Π(x,x) = 1 and Π(x,y) = 0 for y ̸= x. If q(x) > 0 then
⎧
⎪⎨
⎪⎩
Tn −Tn−1 has E

q(x)

exponential law,
XTn has law Π(x,·) =

Π(x,y)

y∈V
for Π(x,y) = Q(x,y)
q(x) 1{y̸=x},
see Fig. 5.2. These quantities satisfy
q(x) ≥0,
Π(x,x) = 1{q(x)=0},
Π(x,y) ≥0,

y∈V
Π(x,y) = 1,
(5.4)
and are related to those of (5.3) by
Q(x,y) = q(x)Π(x,y),
x ̸= y ∈V .
(5.5)
Remark 5.3 In particular, for all x ̸= y in V ,
Pε(x,y) = Q(x,y)ε + o(ε),
Pε(x,x) = 1 + Q(x,x)ε + o(ε),
and the process has probability Q(x,y)ε + o(ε) to jump from x to y ̸= x within ε.

5.1
Characterization, Speciﬁcation, Properties
97
Fig. 5.2 Structure and construction of a Markov process with generator Q, with x ̸= y and
q(x) := 
z̸=x Q(x,z) > 0 and Π(x,y) = Q(x,y)/q(x)
The remarkable independence properties recall Theorem 4.4. One calls:
• the matrix Q: the (inﬁnitesimal) generator of the Markov process or of its semi-
group,
• the non-negative real number Q(x,y) for y ̸= x: the intensity or rate of jumps
from x to y,
• the non-negative real number q(x): the intensity or rate of jumps from x; if
q(x) = 0 then x is said to be an absorbing state,
• the matrix Π = (Π(x,y))x,y∈V : the induced (transition) matrix,
• the sequence (XTn)n∈N visiting the states in the same order as (Xt)t∈R+: the in-
duced Markov chain, which has transition matrix Π.
The notation Q is so common that the expression “Q-matrix for a process” or “q-
matrix for a process” is often used, as in Anderson [1].
Proof (a) Since V is discrete, limn→∞Tn = ∞a.s., since else there could not be a
left limit for the process sample-path at that instant.
(b) Time will be discretized by a grid of mesh ε > 0. With notation (4.1), for
t > 0,

T1 > ⌈t/ε⌉ε

⊂{X0 = Xε = ··· = X⌈t/ε⌉ε} ⊂

T1 > ⌈t/ε⌉ε

∪{T2 −T1 < ε}
and limε→0+ P(T2 −T1 < ε) = 0 by right continuity, and hence for x ∈V , by the
squeeze rule,
Px(T1 > t) = lim
ε→0+ Px(X0 = Xε = ··· = X⌈t/ε⌉ε)
= lim
ε→0+ Pε(x,x)⌈t/ε⌉= lim
ε→0+ e⌈t/ε⌉logPε(x,x)

98
5
Discrete-Space Markov Processes
which implies the existence of q(x) := −Q(x,x) ∈R+ and the equalities in
q(x) := −lim
ε→0+
logPε(x,x)
ε
= lim
ε→0+
1 −Pε(x,x)
ε
,
Px(T1 > t) = e−q(x)t.
If q(x) = 0 then Px(T1 = ∞) = 1, and if q(x) > 0 then T1 has law E (q(x)). In the
latter case, if y ̸= x then likewise
6
1≤k≤⌈t/ε⌉
{X0 = Xε = ··· = X(k−1)ε = x,Xkε = y}
⊂{T2 −T1 < ε} ∪

T1 ≤⌈t/ε⌉ε,X0 = x,XT1 = y

⊂{T2 −T1 < ε} ∪
6
1≤k≤⌈t/ε⌉
{X0 = Xε = ··· = X(k−1)ε = x,Xkε = y}
and there is existence of the limits
Px(T1 ≤t,XT1 = y) = lim
ε→0+

1≤k≤⌈t/ε⌉
Pε(x,x)k−1Pε(x,y)
= lim
ε→0+
1 −Pε(x,x)⌈t/ε⌉
1 −Pε(x,x)
Pε(x,y)
and hence, using the previous limits, of
Q(x,y) := lim
ε→0+
Pε(x,y)
ε
,
Px(T1 ≤t,XT1 = y) = Px(T1 ≤t)
q(x)
Q(x,y).
This product form shows that T1 and XT1 are independent, and that, starting at x,
the law (Π(x,y))y∈V of XT1 is given by
Π(x,y) = Q(x,y)
q(x) ,
y ∈V −{x},
Π(x,x) = 0,
which implies that q(x) = 
y̸=x Q(x,y).
More generally, one may likewise prove for f ∈L∞that
Ex

1{T1≤t}

f (XT1) −f (x)

= Px(T1 ≤t)
1
q(x) lim
ε→0+
Pεf (x) −f (x)
ε
and that this limit is equal to
q(x)Ex

f (XT1) −f (x)

=

y̸=x
Q(x,y)

f (y) −f (x)

= Qf (x).
(c) The strong Markov property (Theorem 5.2) allows to conclude.
□
The strong Markov property applied at the ﬁrst jump instant T1 of the Markov
process allows to establish certain equations on its law, in terms of its generator.

5.2
Constructions, Existence, Simulation, Equations
99
The next exercise uses this to obtain a set of equations for the semigroup called
the backward Kolmogorov equations. The following one provides a probabilistic
solution for some Dirichlet problems, a theme that will not be further developed in
this book.
Exercise 5.3 Let (Xt)t≥0 be a Markov process with values in a discrete space V
and generator Q. Prove that the semigroup (Pt)t≥0 satisﬁes
Pt(x,y) = 1{x=y}e−q(x)t +
 t
0
e−q(x)s 
z̸=x
Q(x,z)Pt−s(z,y)ds,
x,y ∈V .
Prove that this system of equations is equivalent to the system
d
dt Pt(x,y) =

z∈V
Q(x,z)Pt(z,y),
x,y ∈V .
Exercise 5.4 Let (Xt)t≥0 be a Markov process with values in a discrete space V
and generator Q. Let A ⊂V and TA = inf{t ≥0 : Xt ∈A}. Prove that, for any
λ ≥0 and non-negative or bounded function f : A →R, the function
uf
λ : x ∈V →uf
λ (x) = Ex

e−λTAf (XTA)1{TA<∞}

is a solution to the Dirichlet problem, in which u : V →R is a function,
u = f
on A,
(λI −Q)u = 0
on V −A.
Assuming you knew how to simulate (Xt)t≥0, propose a Monte Carlo method yield-
ing approximate solutions to this equation, and discuss the natural issues involved.
5.2 Constructions, Existence, Simulation, Equations
We now provide effective construction and simulation techniques for a Markov pro-
cess, starting from its generator.
5.2.1 Fundamental Constructions
According to Theorem 5.3, the random evolution of a Markov process (Xt)t≥0 can
be characterized and described in terms of
• its generator Q = (Q(x,y))x,y∈V satisfying (5.3),
• or its rates (q(x))x∈V and induced transition matrix Π = (Π(x,y))x,y∈V satis-
fying (5.4),
see also Fig. 5.2. These quantities are related by formula (5.5).

100
5
Discrete-Space Markov Processes
Conversely, Theorem 5.3 is key to try to construct and simulate a Markov pro-
cess (Xt)t≥0 starting from such quantities. In the following constructions, draws are
always assumed to be independent.
Algorithm (First construction) Draw a sample x0 for the initial value; then itera-
tively, if the process is in state x:
• draw a sample from an E (q(x)) exponential law for the sojourn duration at x,
• draw the next state y ̸= x from the law Π(x,·).
(If q(x) = 0 then the process is absorbed at x.)
This construction can be used for simulation. The results on superposition-
decomposition (Theorem 4.4 and its Corollary 4.1) provide constructions which
are equivalent in law, and are useful for theoretical comprehension and results, but
which must be avoided for simulation.
Algorithm (Second construction) Draw a sample x0 for the initial value; then iter-
atively, if the process is in state x:
• draw samples from the E (Q(x,y)) exponential laws for y ̸= x, and determine
their minimum, which is the sojourn duration of the process at x,
• the next state y′ ̸= x is determined by the fact that the sample from E (Q(x,y′))
achieves this inﬁnimum.
(If Q(x,y) = 0 for all y ̸= x, then the process is absorbed at x.)
The following interpretations of these two constructions deﬁne globally the pro-
cess (Xt)t∈R+ in terms of Poisson processes and sequences of i.i.d. r.v.’s.
Algorithm (First construction, bis) For every x in V , consider a Poisson process
Nx with intensity q(x) and jump instants (T x
n )n≥1, and a sequence (Y x
n )n≥1 of i.i.d.
r.v.’s with laws Π(x,·), independently. Draw a sample x0 for the initial value; then
iteratively, if the process reaches state x, let it stay there until the next jump instant
of Nx, and if this is T x
m then let the process jump to Y x
m.
Algorithm (Second construction, bis) For every x ̸= y in V , consider a Poisson
process Nx,y with intensity Q(x,y), independently. Draw a sample x0 for the initial
value; then iteratively, if the process reaches state x, let it stay there until the ﬁrst
(least) of the next jump instants of the Nx,y for y ̸= x, and if this is a jump of Nx,y′
then let the process jump to y′.
Interpreting a random evolution in terms of Theorem 4.4 and of one of these
constructions often allows to show that the underlying process is Markov and to
identify its generator Q, or equivalently its jump intensities q(x) and its induced
transition matrix Π.
The above exponential random variables, or Poisson processes, are often referred
to as exponential, or Poisson, clocks.

5.2
Constructions, Existence, Simulation, Equations
101
5.2.2 Explosion or Existence for a Markov Process
These constructions are all equivalent in law. They deﬁne an increasing sequence
constituted of T0 = 0 and the jump instants (Tn)n≥1, between which the process is
constant, and a Markov chain (XTn)n∈N with matrix Π giving the successive distinct
values taken by the process. This allows to deﬁne Xt for 0 ≤t < limn→∞Tn.
Theorem 5.4 Let Q = (Q(x,y))x,y∈V be a matrix satisfying (5.3). If any of the
above constructions (equivalent in law) is such that
lim
n→∞Tn = ∞,
a.s.,
then the resulting process (Xt)t∈R+ is Markov and has generator Q.
Proof The “Second construction, bis” is used. The resulting process (Xt)t∈R+ can
readily be shown to be Markov using the Markov property of the Poisson processes
Nx,y (Sect. 4.2.2). The evolution description in Theorem 5.3 allows to identify Q
as its generator.
□
The following result provides a criterion for this abstract existence result.
Theorem 5.5 Let Q = (Q(x,y))x,y∈V be a matrix satisfying (5.3). For any of the
above constructions, a.s.,
lim
n→∞Tn = ∞
if and only if

n∈N
1
q(XTn) = ∞.
Proof Conditionally on (XTk)k∈N the Tn+1 −Tn for n ≥0 are independent expo-
nential r.v.’s of parameters q(XTn) (Theorem 5.3). Moreover
lim
n→∞Tn =

n∈N
(Tn+1−Tn) ∈[0,∞],
A :=

n∈N
1
q(XTn) = ∞

∈σ

(XTk)k∈N

.
On the one hand, using the Lebesgue Monotone Convergence Theorem or the
Fubini theorem,
E

lim
n→∞Tn |(XTk)k∈N

=

n∈N
E

Tn+1 −Tn |(XTk)k∈N

=

n∈N
1
q(XTn).
This is ﬁnite on Ac := Ω −A, hence
E

1Ac lim
n→∞Tn |(XTk)k∈N

= 1AcE

lim
n→∞Tn |(XTk)k∈N

< ∞
which implies
P

1Ac lim
n→∞Tn < ∞|(XTk)k∈N

= 1.

102
5
Discrete-Space Markov Processes
The fact that
lim
n→∞Tn < ∞
on Ac,
a.s.,
follows by taking expectations.
On the other hand, using the Lebesgue Dominated Convergence Theorem,
E

e−limn→∞Tn |(XTk)k∈N

= E

n∈N
e−(Tn+1−Tn) |(XTk)k∈N

=

n∈N
q(XTn)
1 + q(XTn).
This vanishes on A since

n∈N
q(XTn)
1 + q(XTn) =

n∈N

1 +
1
q(XTn)
−1
≤

1 +

n∈N
1
q(XTn)
−1
,
hence
E

1Ae−limn→∞Tn |(XTk)k∈N

= 1AE

e−limn→∞Tn |(XTk)k∈N

= 0.
The fact that
lim
n→∞Tn < ∞
on A,
a.s.,
follows by taking expectations.
□
This yields the two following existence results, under veriﬁable assumptions
which are always true if V is ﬁnite. The ﬁrst one uses only the jump intensities.
Theorem 5.6 Let Q = (Q(x,y))x,y∈V be a matrix satisfying (5.3). If
sup
x∈V
q(x) < ∞
then any of the above constructions is such that limn→∞Tn = ∞a.s., and yields a
Markov process (Xt)t∈R+ with generator Q.
Note that this is always true if V is ﬁnite.
Proof This follows from

n∈N
1
q(XTn) ≥

n∈N
1
supx∈V q(x) = ∞
and Theorems 5.5 and 5.4.
□
The second one uses only the induced transition matrix Π, through the induced
Markov chain (XTn)n∈N it generates. It uses a recurrence assumption, for which the
theory of discrete time Markov chains gives several veriﬁable criteria.

5.2
Constructions, Existence, Simulation, Equations
103
Theorem 5.7 Let Q = (Q(x,y))x,y∈V be a matrix satisfying (5.3), and consider
one of the above constructions. If there exists a random ﬁnite subset F of V such
that

n∈N
1{XTn∈F} = ∞
a.s.,
then limn→∞Tn = ∞a.s., and the construction yields a Markov process (Xt)t∈R+
with generator Q.
Proof If limn→∞Tn < ∞then Theorem 5.5 implies that 
n∈N
1
q(XTn) < ∞and
hence limn→∞q(XTn) = ∞, and thus any ﬁnite subset F may only contain XTn for
a ﬁnite number of n since q(F) being ﬁnite is upper-bounded.
□
As a consequence, if T∞:= limn Tn < ∞then Xt remains outside of any ﬁnite
set F for large enough t < T∞(random, depending on F ), which explains why it is
said that there is explosion at time T∞. The existence condition “limn Tn = ∞” a.s.
is often referred to as “non-explosion”.
5.2.3 Fundamental Simulation, Fictitious Jump Method
Section 4.3 contains many useful facts for the sequel.
Fundamental Simulation
The “First construction” in Sect. 5.2.1 can be used to simulate a Markov process
taking values in a discrete space. This natural method relies on the fundamental
description in Theorem 5.3, see Fig. 5.2, and uses the jump rates q(x) and induced
transition matrix Π, which can be obtained from the generator Q by
q(x) =

y̸=x
Q(x,y),
Q(x,y) = q(x)Π(x,y),
x ̸= y ∈V .
This method simulates recursively the jumps of the process. In an actual code,
this is obtained by a loop.
Algorithm (Fundamental simulation) Draw a sample x0 for the initial value; then
iteratively, if the process is in state x:
• compute the intensity q(x), and draw a sample from the E (q(x)) exponential law
for the duration that the process stays at x,
• compute Π(x,y) = Q(x,y)/q(x) for y ̸= x, and a draw a sample from this dis-
crete law for the next state of the process.
(If q(x) = 0 then the process is absorbed at x.)

104
5
Discrete-Space Markov Processes
It is sometimes possible to perform the latter draw in steps without having to
compute the Π(x,y) for all y ̸= x beforehand, by regrouping the y into appropriate
subsets.
To avoid repetitions, in a preliminary phase one can decide to compute sys-
tematically the q(x), and perhaps even the Π(x,·), for all x ∈V , and store them
in memory to be accessed as needed at each iteration. This may be much more
efﬁcient when computations are lengthy, but is possible only for small sample
spaces.
Fictitious Jump Method
Here is a family of variant methods of construction, which may be useful for simu-
lation. If ˆq(x) ≥q(x) for all x ∈V then the matrix ˆΠ deﬁned by
ˆΠ(x,y) = q(x)
ˆq(x)Π(x,y)
for y ̸= x ∈V ,
ˆΠ(x,x) = 1 −q(x)
ˆq(x),
(5.6)
is Markovian. Note that (5.5) can be generalized to
Q(x,y) = q(x)Π(x,y) = ˆq(x) ˆΠ(x,y),
x ̸= y ∈V .
A special interesting case is when
ˆq(x) = λ ≥sup
y∈V
q(y),
x ∈V ,
particularly so when the inequality is an equality.
It is then possible to construct (Xt)t∈R+ similarly to the “First construction” in
Sect. 5.2.1, but using ˆq and ˆΠ instead of q and Π. In the second step, the draw
according to ˆΠ is decomposed into two separate steps, as would be the case in
actual code. All draws are independent.
Algorithm (Fictitious jump method) Draw a sample x0 for the initial value; then
iteratively, if the process is in state x:
• compute the intensity ˆq(x), and draw a sample from the E (ˆq(x)) exponential law
for a duration after which:
• – either, with probability 1 −q(x)/ˆq(x), the process remains at x,
– or else (with complementary probability q(x)/ˆq(x)), the process leaves x;
compute Π(x,y) = Q(x,y)/q(x) for y ̸= x, and draw a sample from this dis-
crete law for the next state.
(If ˆq(x) = 0 then the process is absorbed at x.)
This method can be advantageous for simulation, when there are good upper
bounds ˆq(x) for the q(x) and the former are much easier to compute than the

5.2
Constructions, Existence, Simulation, Equations
105
latter; a natural choice is to take ˆq(x) constant in x, so as not to have to com-
pute it at each actual jump of the process. We shall see an example at the end of
Sect. 5.2.6.
The sampling result in Theorem 4.4(b), and a simple conditioning show that the
ﬁctitious jump method is equivalent in law to the “First construction”. Its name
comes from the fact that it provides too many “potential” jump instants for the pro-
cess, some of which are then rejected (and are thus “ﬁctitious”). This is reminiscent
of the rejection method in Sect. 2.2.5, but the justiﬁcation goes much deeper.
The results on superposition-decomposition (Theorem 4.4 and its Corollary 4.1)
provide constructions which are equivalent in law. These must be avoided for simu-
lation, but may be useful for theoretical comprehension and results.
For instance, a proof of Theorem 5.6 can be thus obtained, by embedding the
jump instants among those of a Poisson process with intensity λ = supx∈V q(x)
(which are known not to accumulate).
5.2.4 Kolmogorov Equations, Feynman–Kac Formula
Theorem 5.3 has extensions in terms of linear differential equations, in inﬁnite di-
mensions for inﬁnite V . Under the assumptions of Theorem 5.6, which ensures the
existence of a Markov process of generator Q and of its semigroup, results can be
stated and proved without too many complications.
Theorem 5.8 Let Q = (Q(x,y))x,y∈V be a matrix satisfying (5.3) and
sup
x∈V
q(x) < ∞.
Let (Pt)t≥0 be the semigroup of the corresponding Markov process (Xt)t∈R+.
(a) The semigroup (Pt)t≥0 is the unique solution of the backward Kolmogorov
equation
⎧
⎪⎪⎨
⎪⎪⎩
d
dt Pt = QPt,
i.e.,
d
dt Pt(x,y) =

z∈V
Q(x,z)Pt(z,y),
x,y ∈V ,
P0 = I.
In L∞, for f ∈L∞, the vt : x →Ptf (x) = Ex(f (Xt)) for t ≥0 constitute the
unique solution of the backward Kolmogorov equation
⎧
⎪⎪⎨
⎪⎪⎩
d
dt vt = Qvt,
i.e.,
d
dt vt(x) =

z∈V
Q(x,y)vt(y),
x ∈V ,
v0 = f.

106
5
Discrete-Space Markov Processes
(b) The semigroup (Pt)t≥0 is the unique solution of the forward Kolmogorov equa-
tion
⎧
⎪⎪⎨
⎪⎪⎩
d
dt Pt = PtQ,
i.e.,
d
dt Pt(x,y) =

z∈V
Pt(x,z)Q(z,y),
x,y ∈V ,
P0 = I.
In M , for any initial law π, the instantaneous laws πt = πPt for t ≥0 consti-
tute the unique solution of the forward Kolmogorov equation (or Fokker–Planck
equation)
⎧
⎪⎪⎨
⎪⎪⎩
d
dt πt = πtQ,
i.e.,
d
dt πt(y) =

x∈V
πt(x)Q(x,y),
y ∈V ,
π0 = π.
This equation with initial condition μ ∈M has unique solution (μPt)t≥0.
The proof of a more precise statement will be given at the end of the following
subsection. When supx∈V q(x) = ∞, this theorem remains valid in a weaker sense,
statements and proofs are more delicate, and the equations must usually be inter-
preted termwise. The backward equation is always true, see Exercise 5.3, but not
necessarily the forward equation. For all this we refer to Anderson [1].
The terms “backward” and “forward” come from the fact that Q acts on Pt =
(Pt(x,y))x,y∈V respectively on the “departure” state x and the “arrival” state y. In
the proof, this will correspond respectively to a differentiation performed respec-
tively “at the beginning” and “at the end” of the path.
Hence, the backward equation is studied and interpreted often in reversed time,
as in the following result for a terminal-value equation.
Corollary 5.1 (Feynman–Kac formula) Let the assumptions and notation of Theo-
rem 5.8 hold. For T > 0 and f ∈L∞, the backward equation
⎧
⎨
⎩
d
dt ut + Qut = 0,
t ∈[0,T ],
uT = f,
has a unique differentiable solution t ∈[0,T ] →ut ∈L∞, given by
x →ut(x) = E

f (XT )|Xt = x

= PT −tf (x).
Proof The time change t →T −t on [0,T ] yields a bijection between the solutions
of this equation and those of the backward Kolmogorov equation in L∞.
□
A differential equation (possibly of inﬁnite dimension) arising in applications
may have the requisite structure to be interpreted as a forward or backward equation

5.2
Constructions, Existence, Simulation, Equations
107
for a discrete-space Markov process. It can thus be given a probabilistic representa-
tion. This allows its approximate solution by Monte Carlo methods.
Such a probabilistic representation can also be useful for theoretical results, such
as those in the corollary.
Forward Kolmogorov Equation as Balance Equation
The forward equation on M can be written
d
dt πt =

x̸=y
πt(x)Q(x,y) −πt(y)q(y),
y ∈V
and has the following interpretation as a balance equation. The instantaneous varia-
tion of πt is due to mass exchanges, and for any state y:
• mass ﬂows into y from each state x ̸= y at rate πt(x) (probability for the process
to be at x) times Q(x,y) (jump rate from x to y);
• mass ﬂows out of state y at rate πt(y) (probability for the process to be at y)
times q(y) = −Q(x,x) = 
x̸=y Q(x,y) (jump rate from y).
Generators and Martingales
Modern Markov process theory goes much further than these equations, and exploits
strong links between Markov processes and martingales, established using the gen-
erator, which extend the Kolmogorov equations. Problem 5.3 provides a short initi-
ation to this topic.
5.2.5 Generators and Semigroups in Bounded Operator Algebras
This section, which can be skipped in a ﬁrst reading, introduces some rudiments
about algebras of bounded operators on a Banach space, which will allow to better
understand the relation between semigroup and generator, and prove Theorem 5.8
in a more precise form.
This relies heavily on the boundedness assumption on the rates in the theorem,
but allows for straightforward proofs once the set-up is perfected. We do not insist
further than necessary for our purposes on this matter.
Anderson [1] develops the general theory of Markovian semigroups and their
generators without this boundedness assumption.
Measure-Function Duality, Total Variation Norm
The integral by a measure μ of a function f given in (5.1), see also (5.2), provides a
natural duality bracket between M and the Banach space L∞of bounded functions
with the uniform norm, and M with the dual norm

108
5
Discrete-Space Markov Processes
∥μ∥VT :=
sup
∥f ∥∞≤1
⟨μ,f ⟩=
sup
∥f ∥∞≤1

x∈V
μ(x)f (x),
μ ∈M ,
(5.7)
called the total variation norm, is a Banach space. It is straightforward to check that
∥μ∥VT =

x∈V
μ(x)
,
μ ∈M .
(5.8)
We could identify M to the space ℓ1(V ) of summable sequences, and L∞to the
space ℓ∞(V ) of bounded sequences, but avoid this for the sake of clarity.
The space P of probability measures is the intersection of the unit sphere of M
for the total variation norm and of the cone of positive measures, and hence is a
closed subset of M . The norm induces a complete metric, and it is straightforward
to check that
∥μ −ν∥VT = 2 sup
A⊂V

μ(A) −ν(A)

,
μ,ν ∈P.
Due to this and other reasons, some authors introduce a factor 1/2 in the deﬁnition.
Remark 5.4 The duality bracket (5.1) allows to identify the topological dual of M
with L∞. The topological dual of L∞is constituted of the ﬁnitely additive set func-
tions, and contains strictly M when V is inﬁnite. Nevertheless, it is straightforward
to prove that
∥f ∥∞=
sup
μ∈M ,∥μ∥VT≤1
⟨μ,f ⟩.
(5.9)
Exercise 5.5 Prove the various formulas given for ∥· ∥VT and ∥· ∥∞.
Bounded Operator Algebras, Generators with Bounded Jump Rates
The operator norm of an operator A on a Banach space V with norm ∥· ∥is deﬁned
by
∥A∥op := sup
∥v∥≤1
∥Av∥
(the supremum is over all v ∈V satisfying ∥v∥≤1). If A and B are two operators
such that the composed operator AB is deﬁned, then clearly
∥AB∥op ≤∥A∥op∥B∥op.
(5.10)
An operator is said to be bounded if its operator norm is ﬁnite, and the set of all
bounded operators on a given Banach space constitutes a Banach algebra, i.e., a
Banach space which is an associative algebra for a multiplication (here operator
composition) satisfying the bound (5.10).

5.2
Constructions, Existence, Simulation, Equations
109
In the sequel, the notation ∥· ∥op will be used for the operator norm on various
Banach spaces, and care will be taken to specify the space under interest.
In an extension of Remark 5.1, any real matrix A = (A(x,y))x,y∈V satisfying
sup
x∈V

y∈V
A(x,y)
 < ∞
(5.11)
corresponds to operators (identiﬁed with their matrices, see the remark)
A : f ∈L∞→Af ∈L∞,
A∗: μ ∈M →μA ∈M ,
(5.12)
which are adjoint for the duality bracket (5.1) since
⟨μ,Af ⟩= μAf =
4
A∗μ,f
5
,
μ ∈M , f ∈L∞.
(5.13)
(Some bounded operators on L∞may not be of this form, see Remark 5.4.)
For any x ∈V it holds that Af (x) = ⟨A(x,·),f ⟩for A(x,·) = (A(x,y))y∈V in
M , and using |Af (x)| = max{Af (x),A(−f )(x)} and (5.7), (5.8) and (5.11),
∥A∥op :=
sup
∥f ∥∞≤1
∥Af ∥∞= sup
x∈V
sup
∥f ∥∞≤1
Af (x)
= sup
x∈V
''A(x,·)
''
VT = sup
x∈V

y∈V
A(x,y)
 < ∞
and (5.7), (5.13) and (5.9) yield
''A∗''
op :=
sup
∥μ∥VT≤1
''A∗μ
''
VT =
sup
∥μ∥VT≤1
sup
∥f ∥∞≤1
μAf =
sup
∥f ∥∞≤1
∥Af ∥∞:= ∥A∥op
(5.14)
so that A and A∗have the same bounded operator norm.
The transition semigroup (Pt)t≥0 of a Markov process is constituted of Marko-
vian matrices which enter in this framework. A matrix Q satisfying (5.3) enters this
framework if and only if it satisﬁes
∥Q∥op := 2 sup
x∈V
q(x) < ∞,
(5.15)
i.e., the assumption in Theorems 5.6 and 5.8.
Proof of a Generalization of Theorem 5.8
Lemma 5.1 Let the assumptions and notation of Theorem 5.8 hold. In the Banach
algebra of operators on L∞with the operator norm,
lim
ε→0+
Pε −I
ε
= Q,
and more generally
d
dt Pt = QPt = PtQ,
t ≥0.
In the Banach algebra of operators on M , the analogous results for the adjoint
operators hold.

110
5
Discrete-Space Markov Processes
Proof Let us construct a Markov process (Xt)t≥0 with generator Q using the “Fic-
titious jump method” of Sect. 5.2.3, with jump instants (Tn)n≥1 chosen among those
of a Poisson process (Nt)t≥0 of intensity
λ := sup
x∈V
q(x) = 1
2∥Q∥op < ∞.
Let us ﬁrst prove that its semigroup (Pt)t≥0 is continuous. For x ∈V and 0 ≤
s ≤t and f ∈L∞such that ∥f ∥∞≤1,
(Pt −Ps)f (x) = Ex

f (Xt) −f (Xs)

≤2P(Nt −Ns ≥1) = 2

1 −e−λ(t−s)
so that, in the bounded operator algebra of L∞,
∥Pt −Ps∥op =
sup
∥f ∥∞≤1
sup
x∈V
(Pt −Ps)f (x) ≤2

1 −e−λ(t−s)
≤2λ(t −s).
Let us now prove differentiability at t = 0. For x ∈V and ε > 0 and f ∈L∞
such that ∥f ∥∞≤1,
(Pε −I)f (x) = Ex

f (Xε) −f (x)

= Ex

f (XT1) −f (x)

1{T1≤ε}

+ Ex

f (Xε) −f (XT1)

1{T2≤ε}

in which the independence properties and Q(x,y) = q(x)Π(x,y) imply that
Ex

f (XT1) −f (x)

1{T1≤ε}

=

1 −e−q(x)ε 
y̸=x
Π(x,y)

f (y) −f (x)

= εQf (x) + O

λ2ε2
and moreover (4.2) implies that
Ex

f (Xε) −f (XT1)

1{T2≤ε}

≤2P(Nε ≥2) = 2

1 −e−λε −λεe−λε
≤λ2ε2.
Hence,
∥Pε −I −εQ∥op =
sup
∥f ∥∞≤1
sup
x∈V
(Pε −I −εQ)f (x) = O

λ2ε2
and thus limε→0+ Pε−I
ε
= Q for the operator norm for L∞.
The semigroup property and the preceding results yield
Pt+ε −Pt
ε
=
⎧
⎪⎨
⎪⎩
Pε−I
ε
Pt −−−→
ε→0+ QPt,
Pt Pε−I
ε
−−−→
ε→0+ PtQ,
Pt−ε −Pt
−ε
=
⎧
⎪⎨
⎪⎩
I−Pε
−ε Pt−ε −−−→
ε→0+ QPt,
Pt−ε I−Pε
−ε −−−→
ε→0+ PtQ,
in the bounded operator algebra for L∞, (where (5.10) allows to take limits of prod-
ucts), and this concludes the proof in this setting.
The corresponding results for the adjoint operators follow using (5.14).
□

5.2
Constructions, Existence, Simulation, Equations
111
Completion of the Proof of Theorem 5.8
Lemma 5.1 shows that (Pt)t≥0 solves the differential equations
d
dt Pt = QPt and
d
dt Pt = PtQ given in Theorem 5.8, each of these both in the Banach algebra of
bounded operators on L∞and in the Banach algebra of bounded operators on M .
Using (5.10), the operators P →QP and P →PQ on these algebras are
bounded with operator norm ∥Q∥op, and the Gronwall lemma yields uniqueness
of solution for these equations.
Multiplication on the right by f ∈L∞of the backward equation d
dt Pt = QPt for
operators on L∞yields that (Ptf )t≥0 solves the backward equation on L∞. Multi-
plication on the left by μ ∈M of the forward equation d
dt Pt = PtQ for operators
on M yields that (μPt)t≥0 solves the forward equation on M .
Uniqueness for these equations follows from the fact that Q is a bounded operator
and the Gronwall lemma.
Operator Exponential
Under the assumptions of Theorem 5.8, one can deﬁne
etQ =

k≥0
tkQk
k! ,
t ≥0,
(5.16)
in the Banach algebra of bounded operators on L∞, in which then
d
dt etQ = QetQ = etQQ,
e(s+t)Q = esQetQ,
s,t ≥0.
These results can be proved using the bounds obtained using (5.10), which are uni-
form on bounded time intervals,

k≥0
''''
tkQk
k!
''''
op
≤

k≥0
tk∥Q∥k
op
k!
= et∥Q∥op.
Recall that (Pt)t≥0 is deﬁned as the semigroup of the Markov process with gen-
erator Q, which exists as the result of a probabilistic pathwise construction. Hence,
Pt is automatically given by a Markovian matrix.
From a theoretical point of view, it is not a priori clear that the operator etQ on
L∞corresponds to a Markovian matrix, see Remark 5.4. Nevertheless, the unique-
ness result in Theorem 5.8 implies that
Pt = etQ
as an operator on L∞, and also as an operator on M deﬁned by duality or by right-
multiplication by measures.
From a practical perspective, this equality is rarely of use, since the effective
computation of the series deﬁning etQ requires ﬁnding a tractable spectral decom-
position for Q, and is most often impossible.

112
5
Discrete-Space Markov Processes
5.2.6 A Few Case Studies
In all the following examples, the description of a random evolution phenomena will
allow to establish the generator for a Markov process which could represent it. It is
then left to prove the actual existence of this process.
The M/M/∞Queue
A new customer arrives at each instant of a Poisson process with intensity λ > 0,
is served starting at their arrival time for a duration with E (μ) exponential law,
and then leaves. This corresponds to the M/M/∞queue in Kendall’s nomencla-
ture: Markovian (or Memoryless) arrivals, Markovian (or Memoryless) services, ∞
number of servers.
Let Xt be the number of customers in the system at time t ∈R+. When the
process (Xt)t∈R+ is in state x ∈N, it waits there until the ﬁrst among the next jump
instants of the arrival Poisson process and of x Poisson processes of intensity μ
corresponding to the service times of the x customers. Then it jumps to either x + 1
if this ﬁrst instant belonged to the arrival process, or else to x −1 (in which case
x ≥1). The generator thus satisﬁes
Q(x,x + 1) = λ,
Q(x,x −1) = xμ,
x ∈N,
and all other non-diagonal terms vanish. Hence
q(x) = λ + xμ,
Π(x,x + 1) =
λ
λ + xμ,
Π(x,x −1) =
xμ
λ + xμ,
x ∈N.
The problem is now to prove existence of the corresponding Markov process.
Since supx∈N q(x) = ∞, Theorem 5.6 does not apply.
A possibility would be to use Theorem 5.7 and a recurrence criterion for Markov
chains; for instance, the detailed balance equations for the induced transition matrix
Π yield an invariant measure, which is readily seen to be summable, and this proves
that Π is positive recurrent.
A more direct proof follows.
The process (Xt)t∈R+ can be constructed using independent Poisson processes,
(At)t∈R+ of intensity λ for the arrivals, and (Dk
t )t∈R+ of intensity μ for k ≥1 for
the departures, so that (D1
t + ···+ Dk
t )t∈R+ is used for the next end of service when
there are k customers in the system.
The issue is to prove that the jump instants of (Xt)t∈R+ do not accumulate, a.s.,
for any X0 = x. For any n ≥1 there is yn ∈N large enough that
P(An ≤yn) ≥1 −1/n.
On the event {An ≤y},
sup
0≤s≤n
Xs ≤x + yn

5.2
Constructions, Existence, Simulation, Equations
113
and hence the jump instants (Tk)k≥1 of (Xt)t∈R+ on [0,n] are among those of

At + D1
t + ··· + Dx+yn
t

t∈R+.
This is a Poisson process with intensity λ + (x + yn)μ < ∞, and its jump instants
do not accumulate. Hence, by the monotone limit theorem,
P

lim
k→∞Tk = ∞

= lim
n→∞↓P

lim
k→∞Tk > n

≥lim
n→∞↓P(An ≤yn) = 1.
Pure Birth and Birth and Death Processes
A process with values in N with generator Q such that
Q(x,x + 1) = q(x) ≥0,
x ∈N,
and all other non-diagonal terms vanish is called a pure birth process. The only non-
vanishing terms of the induced transition matrix Π are Π(x,x + 1) = 1 for x ≥0.
Theorems 5.5 and 5.4 show that the corresponding Markov process exists if and
only if

x∈V
1
q(x) = ∞.
More generally, a process with values in N with generator Q such that
Q(x,x + 1) = λx ≥0,
x ∈N,
Q(x,x −1) = μx ≥0,
x ≥1,
and its other non-diagonal terms vanish is called a birth and death process. With the
convention μ0 = 0 and 0
0 = 0,
q(x) = λx + μx,
Π(x,x + 1) =
λx
λx + μx
,
Π(x,x −1) =
μx
λx + μx
,
x ∈N.
Necessary and sufﬁcient conditions of non-explosion in terms of these coefﬁcients
can be found in Anderson [1]. Here, we are going to give a simple proof that a
sufﬁcient condition is

x∈V
1
λx
= ∞.
(5.17)
It is satisﬁed by the M/M/∞queue process, which is an example of birth and death
process, which gives another existence proof for it.
Let ˆQ be the pure jump generator such that
ˆQ(x,x + 1) = Q(x,x + 1) := λx,
x ∈N.
The “Second construction, bis” in Sect. 5.2.1 can be used to construct simultane-
ously the processes (Xt)t∈R+ and ( ˆXt)t∈R+ with respective generators Q and ˆQ,

114
5
Discrete-Space Markov Processes
with X0 = ˆX0 and the same Poisson processes Nx,x+1 of intensity λx for determin-
ing the jumps from x to x + 1 for both processes. Let (Tn)n≥1 and ( ˆTn)n≥1 denote
the respective sequences of jump instants.
Assume that (5.17) holds. Theorem 5.5 yields that limn→∞ˆTn = ∞, a.s. The
processes have unit jumps, and if they are both at x then they can jump from x
to x + 1 only simultaneously by construction, hence Xt ≤ˆXt for t < limn→∞Tn.
Theorem 5.7 then implies that limn→∞Tn = ∞, a.s.
A Network in Which Customers Join the Shortest Available Queue
There are K ≥1 queues, numbered by k from 1 to K, and queue number k has a
unique server at rate μk and a waiting queue with inﬁnite capacity. By convention,
queue K + 1 denotes queue 1, and queue 0 denotes queue K.
For 1 ≤k ≤K, customers arrive at the instants of a Poisson process with in-
tensity λk, and join either queue k or k + 1 according to their sizes xk and xk+1:
queue k with probability pk(xk,xk+1) ∈[0,1], or queue k + 1 with complementary
probability. The case pk(xk,xk+1) ∈{0,1} constitutes a deterministic routing rule.
One may wish to use simulation in order to determine the functions pk in terms
of the other parameters, so as to ensure an approximately optimal operation for this
network.
Let Xt = (Xk
t )1≤k≤K for t ∈R+, where Xk
t is the size of queue k (including the
customer in service) at time t. The state space is NK, and (ek)1≤k≤K denotes the
canonical basis. If the process is at x = (xk)1≤k≤K then:
• it will either jump to a state x +ej when a customer joins queue j for 1 ≤j ≤K,
at rates
Q(x,x + ej) = λjpj(xj,xj+1) + λj−1

1 −pj−1(xj−1,xj)

;
• or else to a state x −ej when a customer service ends at queue j for 1 ≤j ≤K,
at rates μj if xj > 0, so that
Q(x,x −ej) = μj1{xj >0}.
Thus
q(x) =
K

j=1
(λj + μj1{xj >0}),
sup
x∈NK q(x) =
K

j=1
(λj + μj) < ∞,
and Theorem 5.6 ensures existence of the corresponding Markov process.
For the practical simulation of the network, care should be made to avoid useless
computations, and some draws ought to be performed in two steps, as follows.
Algorithm (Fundamental simulation) Draw a sample x0 for the initial value; then
iteratively, if the process is in state x = (xk)1≤k≤K:

5.3
Problems
115
• compute the intensity q(x), and draw a sample from the E (q(x)) exponential law
for the duration that the process stays at x,
• the jump corresponds, for some 1 ≤j ≤K, either to an arrival concerning both
queues j and j + 1, or to a departure from queue j, with respective probabilities
λj/q(x) and μj1{xj >0}/q(x); determine which is the case, then:
– for an arrival as above, either with probability pj(xj,xj+1) increment the size
of queue j by 1, or else increment the size of queue j + 1 by 1,
– for a departure from queue j, decrement its size by 1.
Note that only one probability pj(xj,xj+1) has to be computed, and only in
case of an arrival. Obviously, the value of q(x) should be updated at each itera-
tion, by incrementing or decrementing it by 1 whenever there is one more or one
less non-empty queue. The following method avoids doing so, but has more itera-
tions.
Algorithm (Fictitious jump method) Compute λ = K
j=1(λj + μj), and draw
a sample x0 for the initial value; then iteratively, if the process is in state x =
(xk)1≤k≤K:
• draw a sample from the E (λ) exponential law,
• the jump corresponds, for some 1 ≤j ≤K, either to an arrival concerning both
queues j and j + 1, or to a potential departure from queue j, with respective
probabilities λj/λ and μj/λ; determine which is the case, then:
– for an arrival as above, either with probability pj(xj,xj+1) increment the size
of queue j by 1, or else increment the size of queue j + 1 by 1,
– for a potential departure from queue j, decrement its size by 1 if it is non-zero,
else leave it untouched (ﬁctitious jump).
Conclusion to Be Drawn
The previous examples show that, for obtaining existence results for a Markov pro-
cess as well as for its efﬁcient simulation, one should think ahead and ﬁnd inspira-
tion in the phenomenon of interest. Theorem 4.4 (superposition-decomposition of
Poisson processes) is fundamental for ﬁnding efﬁcient simulations for processes,
respecting the adequate Markovian evolution.
5.3 Problems
5.1 (Assymetric Ehrenfest Urn) Molecules labeled by 1,...,N are put in a box,
constituted of two compartments A and B separated by an asymmetric porous bar-
rier. Independently, any molecule in B passes into A at rate λ > 0, any molecule in
A passes into B at rate μ > 0. For 1 ≤i ≤N and t ∈R+, let

116
5
Discrete-Space Markov Processes
Yi(t) = 1
if molecule i is in A at time t,
Yi(t) = 0
if not;
Xt :=

1≤i≤N
Yi(t).
The processes (Yi(t),1 ≤i ≤N)t≥0 and (Xt)t≥0 provide microscopic and macro-
scopic representations of the system.
1. Show that (Yi(t),1 ≤i ≤N)t≥0 is a Markov process on {0,1}N such that, start-
ing from a state (yi,1 ≤i ≤N) in {0,1}N, for 1 ≤k ≤N,
• the rate of jumps to (yi + 1{i=k},1 ≤i ≤N) is λ(1 −yk);
• the rate of jumps to (yi −1{i=k},1 ≤i ≤N) is μyk.
2. Prove that (Xt)t≥0 is a Markov process on {0,...,N} with inﬁnitesimal genera-
tor Q = (Q(x,y))x,y=0,...,N speciﬁed by its non-null off-diagonal terms
Q(x,x + 1) = λ(N −x),
Q(x,x −1) = μx,
x = 0,...,N.
3. Let πt = (πt(x))x=0,...,N denote the law of Xt, and ft = (ft(x))x=0,...,N be given
by ft(x) := πt(0) + ··· + πt(x).
(a) Write an ordinary differential equation satisﬁed by (πt)t≥0.
(b) Prove that d
dt πt = 0 ⇔d
dt ft = 0.
(c) Write d
dt ft(x) in terms of πt.
(d) Find a simple recurrence equation (in x) satisﬁed by any probability measure
˜π = ( ˜π(x))x=0,...,N such that if π0 = ˜π then πt = ˜π for every t.
(e) Compute ˜π from this; do not forget that it is a probability measure.
5.2 (Chain Reaction) Let be given a probability space, on which all random vari-
ables are constructed, a constant λ > 0, and a law η = (η(k))k∈N on N satisfying
η(1) ̸= 1,
m :=

k∈N
kη(k) < ∞.
(5.18)
In a model for a nuclear reaction, every particle exists for a duration with expo-
nential law of parameter λ > 0, and then disappears in a collision with the medium
resulting in the emission of k ≥0 new particles with probability η(k). The initial
number of particles, the existence durations, and the numbers of particles emitted
during the collisions are independent.
A Markov process (Xt)t∈R+ with values in N, where Xt is the number of particles
at time t, is to be constructed. Let Tn for n ≥1 be its jump instants, T0 = 0, and
( ˆXn)n∈N be the induced Markov chain given by ˆXn = XTn.
Let (Zk)k≥1 be a sequence of i.i.d. r.v.’s of law η, independent of X0 and of
the existence durations, giving the numbers of new particles emitted during the
successive collisions; we recall that an existing particle disappears at every col-
lision.

5.3
Problems
117
1. Prove that there is λ′ > 0 and a law η′ = (η′(k))k∈N on N satisfying η′(1) =
0, to be computed in terms of λ and η = (η(k))k∈N, such that the law of the
evolution of (Xt)t∈R+ is the same as that in which λ and η are replaced by λ′
and η′.
In the sequel it is always assumed that η(1) = 0.
2. (a) Explain in detail how to construct and simulate (Xt)t∈R+, and com-
pute its inﬁnitesimal generator Q = (Q(x,y))x,y∈N. What problem can
arise?
(b) Give a simple expression for
ˆXn in terms of X0, the Zk for k ≥1,
and
M = inf{k ≥0 : X0 + Z1 + ··· + Zk −k = 0}.
Deduce from it that ˆXn = O(n), i.e., ( 1
n ˆXn,n ≥1) is uniformly bounded,
a.s.
(c) Compute E(e−Tn |( ˆXk)0≤k≤n−1), then prove that limn→∞E(e−Tn) = 0.
Deduce from this that the Markov process (Xt)t∈R+ is well deﬁned.
The generating functions of the emission law and of the instantaneous laws
are
g : z ∈[0,1] →g(z) =

k∈N
zkη(k) ∈[0,1],
ft : z ∈[0,1] →ft(z) = E1

zXt 
∈[0,1].
Let h : z ∈[0,1] →h(z) = λ(g(z) −z).
3. (a) Explain how to obtain the case when X0 is arbitrary from the case
X0 = 1.
Deduce from this Ex(zXt ) in terms of ft(z) and x ∈N.
(b) Prove that
ft+ε(z) = fε

ft(z)

,
t,ε ≥0, z ∈[0,1],
(c) Let X0 = 1. Prove that the probability that (Xt)t∈R+ jumps one or more
times on [0,ε] is λε + O(λ2ε2), then that the probability that it jumps
two or more times on [0,ε] is of order O(mλ2ε2) (where m is given in
(5.18)).
(d) Prove that
ft+ε(z) −ft(z) = h

ft(z)

ε + O

(m + 1)λ2ε2
,
t,ε ≥0, z ∈[0,1],
then that
∂
∂t ft(z) = h

ft(z)

,
t ≥0, z ∈[0,1].
To which result in the book does this correspond?

118
5
Discrete-Space Markov Processes
4. Let E = {∃t ≥0 : Xt = 0} and θ = P1(E ) denote the event of extinction of the
reaction and its probability.
(a) Prove that θ = limt→∞ft(0), then that h(θ) = 0 (i.e., θ is a root for
h).
(b) Prove that if m ≤1 then θ = 1.
(c) Prove that if m > 1 then θ is the unique root for h on [0,1[.
5. (a) Prove that
∂
∂zft(z) = exp
 t
0
h′
fs(z)

ds

,
t ≥0, z ∈[0,1].
(b) Deduce from this that E1(Xt) = eλ(m−1)t.
(c) Prove that if m = 1 then E1(supt≥0 Xt) = ∞.
6. Consider the special case when η(0) = 1 −p and η(2) = p, for 0 < p < 1.
(a) Compute ft for t ≥0.
(b) Deduce from this the value of θ.
5.3 (Markov process, generator, and martingales (⋆)) Let (Xt)t∈R+ be a Markov
process with values in a discrete space V , with bounded generator Q. For every
f ∈L∞, deﬁne

Mf
t

t≥0,
Mf
t = f (Xt) −f (X0) −
 t
0
Qf (Xs)ds.
1. Prove that (Mf
t )t≥0 is a martingale w.r.t. (Xt)t≥0, i.e., that E(Mf
t+h |(Xs)0≤s≤t)=
Mf
t for t,h ≥0.
This expresses the deterministic tendency of the evolution of (Xt)t≥0 in terms of
Q, with random ﬂuctuations given by the martingales (Mf
t )t≥0. Let
(At)t≥0,
At =
 t
0

Q

f 2
(Xs) −2f (Xs)Qf (Xs)

ds.
2. Assume ﬁrst that X0 = x and f (x) = 0 (to simplify computations).
(a) Prove that
Mf 2
t
=

Mf
t
2 + 2Mf
t
 t
0
Qf (Xs)ds
+
 t
0
Qf (Xs)ds
2
−
 t
0
Q

f 2
(Xs)ds.
(b) Prove that
 t
0
Qf (Xs)ds
2
= 2
 t
0
Qf (Xs)
 s
0
Qf (Xu)duds.

5.3
Problems
119
(c) Prove that a martingale is obtained by setting
Mf
t
 t
0
Qf (Xs)ds −
 t
0
Mf
s Qf (Xs)ds,
t ≥0.
(d) Prove that ((Mf
t )2 −At)t≥0 is a martingale vanishing at time zero, and then
that Var(Mf
t ) = E(At).
3. Prove that all this still holds for f (x) = a ̸= 0, then when X0 is arbitrary.
With the notation At = ⟨Mf ⟩t, the process (⟨Mf ⟩t)t≥0 can be written in terms
of Q, and allows to further evaluate the martingale (Mf
t )t≥0 by the fact that
(Mf
t )2 −⟨Mf ⟩t is a martingale. The following computation yields a more explicit
expression.
4. Prove that
4
Mf 5
t =
 t
0

y̸=Xs
Q(Xs,y)

f (y) −f (Xs)
2 ds
=
 t
0

y∈V
Q(Xs,y)

f (y) −f (Xs)
2 ds.
All this can be appropriately generalized to Markov processes taking values in a
continuous state space.

Chapter 6
Continuous-Space Markov Processes
with Jumps
Abstract From now on, Markov processes with continuous state space (Rd for
some or one of its closed subsets) are considered. Their rigorous study requires
advanced measure-theoretic tools, but we limit ourselves to developing the reader’s
intuition, notably by pathwise constructions leading to simulations. We ﬁrst empha-
size the strong similarity between such Markov processes with constant trajectories
between isolated jumps and discrete space ones. We then introduce Markov pro-
cesses with sample paths following an ordinary differential equation between iso-
lated jumps. In both cases, the Kolmogorov equations and Feynman–Kac formula
are established. This is applied to kinetic equations coming from statistical Mechan-
ics. These describe the time evolution of the instantaneous distribution of particles in
phase space (position-velocity), when the particle velocity jumps at random instants
in function of the particle position and velocity.
6.1 Preliminaries
The construction of continuous-space Markov processes requires the use of mea-
sure and integration theory, and it is always assumed implicitly that functions and
subsets are measurable w.r.t. the Borel σ-ﬁeld B(V ) of V . Intrinsic notations will
be used for integrals and adjoint operators, as well as more intuitive notations which
establish links with the discrete space theory and allow to quickly write formulas;
there will be some redundancy.
6.1.1 Measures, Functions, and Transition Kernels
Positive and Signed Measures, Integrals of Functions
A positive measure μ on V is a σ-additive function from B(V ) to [0,∞]. It is ﬁnite
if its total mass μ(V ) is ﬁnite, and a probability measure, or a law, if μ(V ) = 1.
The space of probability measures is denoted by P = P(V ).
A signed measure μ is a σ-additive function from B(V ) to R. It can be written
as the difference μ = μ+ −μ−of two positive ﬁnite measures which are mutually
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1_6, © Springer-Verlag Berlin Heidelberg 2013
121

122
6
Continuous-Space Markov Processes with Jumps
singular, i.e., such that μ+(A)μ−(A) = 0 for every A ∈B(V ). The vector space of
signed measures is denoted by M = M (V ). A positive measure belongs to M if
and only if it is ﬁnite.
The Lebesgue integral by a measure μ of a function f is deﬁned in [0,∞] if μ
and f are positive (wide-sense), and in R if μ belongs to M and f to the space of
bounded functions L∞= L∞(V ). It provides a natural duality bracket
(μ,f ) →⟨μ,f ⟩:=

f dμ =

f (x)μ(dx) =

μ(dx)f (x)
(6.1)
and all these notations will be used. The notations

μ(dx)f (x) and ⟨μ,f ⟩are close
to the discrete state space matrix notation, and allow to recover some formulas in an
easier way.
Transition Kernels and Their Actions
A transition or Markovian kernel P is a measurable mapping
x ∈V →P(x,·) = P(x,dy) ∈P(V ).
A kernel of positive measures or of signed measures is deﬁned similarly. The abuse
of notation P(x,y) = P(x,{y}) is common, as it is for any measure.
A transition kernel P = (P(x,dy))x∈V can be identiﬁed with the operator f →
Pf on L∞(or on [0,∞]-valued functions) deﬁned by the formula
Pf : x ∈V →Pf (x) =
4
P(x,·),f
5
:=

f (y)P(x,dy) :=

P(x,dy)f (y).
The adjoint operator P ∗: μ →P ∗μ on M of P for the duality bracket (6.1) satisﬁes
⟨P ∗μ,f ⟩= ⟨μ,Pf ⟩. Developing Pf , by the Fubini theorem
⟨μ,Pf ⟩=

f (y)P(x,dy)μ(dx) :=

μ(dx)P(x,dy)f (y).
Thus P ∗can be expressed in terms of the kernel (P(x,dy))x∈V by the formula
P ∗μ(dy) =

x
P(x,dy)μ(dx) :=

x
μ(dx)P(x,dy) := μP(dy).
The last notation can be interpreted as deﬁning the operation of P on its left on a
measure μ, which allows to recover quickly this and future formulas.
Remark 6.1 If P and R are both Markovian kernels, then the composed operator
PR : f →PRf on L∞is given for x ∈V by
PRf (x) =

PR(x,dy)f (y),
PR(x,dy) =

P(x,dz)R(z,dy).
This deﬁnes a natural product for kernels, analogous to the one for matrices.

6.1
Preliminaries
123
6.1.2 Markov Property, Finite-Dimensional Marginals
Transition Kernels, Instantaneous Laws, Markov Property
Abstract conditional expectation must now be used, see Sect. 2.3.1. The r.v.’s thus
deﬁned are deﬁned only a.s., but this is mainly left implicit.
Deﬁnition 6.1 A process (Xt)t∈R+ with values in V is a Markov process if there
exists a family (Pt)t≥0 of Markovian kernels Pt = (Pt(x,dy))x∈V such that, for all
s,t ≥0 and A ∈B(V ),
P

Xs+t ∈A|(Xθ)0≤θ≤s

= Pt(Xs,A).
The Pt are called the transition kernels of the process.
Remark 6.2 Then
P(Xs+t ∈A|Xs) = Pt(Xs,A).
Moreover, if f is non-negative or bounded,
E

f (Xs+t)|(Xθ)0≤θ≤s

= E

f (Xs+t)|Xs

= Ptf (Xs) :=

Pt(Xs,dy)f (y),
and Pt(x,dy) is the conditional law of Xs+t given Xs = x, and in particular of Xt
given X0 = x.
The notation for discrete spaces (see Sect. 5.1.2) is adapted to this framework: Px
and Ex are used to indicate that X0 = x, and Pμ and Eμ that X0 is of law μ ∈P.
For f ∈L∞and μ ∈P and A ∈B(V ),
Px(Xt ∈A) := Pt(x,A),
Pμ(Xt ∈A) := P ∗
t μ(A) = μPt(A),
Ex

f (Xt)

:= Ptf (x) =
4
P(x,·),f
5
,
Eμ

f (Xt)

:= ⟨μ,Pf ⟩,
and denoting by πt the law of Xt,
πt(A) := P(Xt ∈A),
πt := P ∗
t π0 = π0Pt,
πt := Pt(x,·)
for π0 = δx.
A fundamental result is that Deﬁnition 6.1 is equivalent to apparently stronger
properties, quite useful for proofs.
Theorem 6.1 (Markov property) Let (Xt)t∈R+ be a Markov process with values in
V , and (Pt)t∈R+ denote its transition kernels. Then it satisﬁes the following prop-
erties, which are equivalent.
(a) The property in Deﬁnition 6.1.

124
6
Continuous-Space Markov Processes with Jumps
(b) For any measurable C ⊂D(R+,V ), let
F C : x ∈V →F C(x) = Px

(Xh)h≥0 ∈C

.
Then
P

(Xt+h)h≥0 ∈C |(Xs)0≤s≤t

= F C(Xt) : ω →PXt(ω)

(Xh)h≥0 ∈C

.
This can be interpreted as follows: at time t, chance provides a value x = Xt(ω),
and given that, the law of the future evolution (Xt+h)h≥0 is that of a copy of the
Markov process (Xh)h≥0 started at x and independent from the past (Xs)0≤s≤t.
In this speciﬁc sense, conditional on Xt, the process (Xt+h)h≥0 is independent
of (Xs)0≤s≤t and has the same law as the Markov process starting at Xt.
(c) For all 0 = t0 ≤t1 < ··· < tn and A0,A1,...,An ∈B(V ) and law π0 of X0,
P(Xt1 ∈A1,...,Xtn ∈An)
=

V
π0(dx0)

A1
Pt1−t0(x0,dx1)···

An
Ptn−tn−1(xn−1,dxn)
=

V
π0(dx0)
n

k=1

Ak
Ptk−tk−1(xk−1,dxk).
Proof Property (b) implies (a) as a special case. Since
P(Xt1 ∈A1,...,Xtn ∈An) = E

E

1{Xt1∈A1,...,Xtn∈An} |(Xs)0≤s≤tn−1

= E

1{Xt1∈A1,...,Xtn−1∈An−1}E

1{Xtn∈An} |(Xs)0≤s≤tn−1

.
Property (a) implies that
P(Xt1 ∈A1,...,Xtn ∈An) = E

1{Xt1∈A1,...,Xtn−1∈An−1}Ptn−tn−1(Xtn−1,An)

which implies (c) by induction. Now, Property (b) will be obtained from (c).
Since the product σ-ﬁeld on D(R+,V ) is generated by the cylindrical sets, it is
enough to prove the result when C is such a set, i.e., of the form
C =

(xh)h≥0 ∈D(R+,V ) : xh1 ∈A1,...,xhn ∈An

for 0 ≤h1 < ··· < hn and A1,...,An ∈B(V ). Then, applying three times Prop-
erty (c), for all 0 = s0 ≤s1 < ··· < sn < t and B1,...,Bn and B in B(V ),
P

Xs1 ∈B1,...,Xsn ∈Bn,Xt ∈B,(Xt+h)h≥0 ∈C

=

V
π0(dx0)
n

k=1

Bk
Psk−sk−1(xk−1,dxk)

6.1
Preliminaries
125
×

B
Pt−sn(xn,dxn+1)
n

k=1

Ak
Phk(xn+k,dxn+k+1)
=

V
π0(dx0)
n

k=1

Bk
Psk−sk−1(xk−1,dxk)

B
Pt−sn(xn,dxn+1)F C(xn+1)
= E

1{Xs1∈B1,...,Xsn∈Bn,Xt∈B}F C(Xt)

and hence F C(Xt) satisﬁes the characteristic property (2.8) of conditional expecta-
tions.
□
Remark 6.3 The law of a Markov process (Xt)t≥0 is characterized by its ﬁnite-
dimensional marginals, which are determined in terms of the law of X0 and its
transition kernels (Pt)t≥0 by Property (c).
Strong Markov Property
It is natural to try to extend the Markov property by replacing t by a stopping time,
but the situation for a continuous state space is much more delicate than for a dis-
crete space. This would exceed the scope of this book, and our true needs, since the
construction and simulation of the processes we consider will necessitate only to
use the property for very special stopping times: the jump instants of the process.
6.1.3 Semigroup, Inﬁnitesimal Generator
Theorem 6.1 implies that the transition kernels Pt of a Markov process satisfy the
Chapman–Kolmogorov equation
Pt+s(x,dy) =

z∈V
Pt(x,dz)Ps(z,dy),
x,y ∈V , t,s ≥0,
which translates for the kernel product of Remark 6.1, or for operator composition,
into
Pt+s = PtPs,
t,s ≥0.
Hence, (Pt)t≥0 forms a semigroup of Markovian kernels, called the (transition)
semigroup of the Markov process.
This semigroup induces an operator semigroup f →Ptf on L∞and another
μ →P ∗
t μ = μPt on M by duality, or letting it operate on its left on measures. The
theory of such semigroups shows that they are characterized by their left derivative
at 0.

126
6
Continuous-Space Markov Processes with Jumps
Speciﬁcally, the (inﬁnitesimal) generator of the Markov process, or of its semi-
group, is the operator deﬁned by
⎧
⎪⎨
⎪⎩
A : f ∈D(A ) ⊂L∞→A f ∈L∞,
A f : x ∈V →A f (x) = lim
ε→0+
Pεf (x) −f (x)
ε
= lim
ε→0+
Ex(f (Xε) −f (x))
ε
,
(6.2)
where its domain D(A ) is constituted of the functions such that these limits exist
and deﬁne a function A f in L∞.
It characterizes the law of the evolution of the Markov process, and we can hope
to obtain it in explicit form from the model under study.
6.2 Markov Processes Evolving Only by Isolated Jumps
The simplest class of Markov processes on V is now studied, and the relation with
Markov processes on discrete space made clear.
6.2.1 Semigroup, Inﬁnitesimal Generator, and Evolution Law
Let us ﬁrst consider Markov processes (Xt)t∈R+ which evolve only by isolated
jumps. These can be written in the form
⎧
⎪⎨
⎪⎩
Xt =

n≥0
XTn1{Tn≤t<Tn+1}
for t ≥0,
0 = T0 < T1 ≤T2 ≤··· ≤∞,
lim
n→∞Tn = ∞,
Tn < ∞⇔XTn ̸= XTn−1.
(6.3)
For n ≥1, if Tn < ∞then it is a true jump instant (as opposed to a ﬁctitious one),
and necessarily Tn > Tn−1. All this is analogous to Theorem 5.3.
The theory of these processes is quite similar to that of Markov processes with
a discrete state space. The hypothesis of “isolated jumps” is a serious limitation: a
process evolving by jumps in continuous space can have converging jump instants
as well as states at those instants, and then “start over” and be deﬁned for all times.
Theorem 6.2 Let (Xt)t∈R+ be a Markov process with values in V of the form (6.3),
and (Pt)t∈R+ denote its semigroup.
(a) The generator A of the Markov process, deﬁned in (6.2), is given by a kernel
(α(x,dy))x∈V of ﬁnite positive measures on V as follows: for x ∈V and f ∈
L∞,

6.2
Markov Processes Evolving Only by Isolated Jumps
127
⎧
⎪⎨
⎪⎩
α(x,dy) ≥0,
α

x,{x}

= 0,
q(x) := α(x,V ) < ∞,
A f (x) =
 
f (y) −f (x)

α(x,dy) =

f (y)α(x,dy) −q(x)f (x).
(6.4)
In particular
Pε(x,A) = α(x,A)ε + o(ε)
for A ∈B

V −{x}

,
Pε

x,{x}

= 1 −q(x)ε + o(ε).
(b) When X0 = x the r.v.’s T1 and XT1 are independent. If q(x) = 0 then T1 = ∞
and we set XT1 = x and Π(x,dy) = δx(dy). If q(x) > 0 then
⎧
⎪⎨
⎪⎩
T1 has E

q(x)

exponential law,
XT1 has law Π(x,dy) = α(x,dy)
q(x)
.
These quantities satisfy
q(x) ≥0,
Π

x,{x}

= 1{q(x)=0},
Π(x,dy) ≥0,
Π(x,V ) = 1,
(6.5)
and are related to those of (6.4) by
α(x,dy) = q(x)Π(x,dy).
(6.6)
Moreover, for A ∈B(R+) and B ∈B(V ),
P

Tn −Tn−1 ∈A,XTn ∈B |(Tk,XTk)0≤k≤n−1

= P(Tn −Tn−1 ∈A,XTn ∈B |XTn−1)
= q(XTn−1)

A
e−q(XTn−1)s ds × Π(XTn−1,B).
Thus, in order to construct Tn and XTn starting from (Tk,XTk)0≤k≤n−1, it suf-
ﬁces to use the observed value XTn−1 = x and draw Tn −Tn−1 and XTn indepen-
dently according to the laws E (q(x)) and Π(x,dy). Figure 5.2 can be readily
adapted.
Remark 6.4 In particular, for all A in B(V ) not containing x,
Pε(x,A) = Q(x,A)ε + o(ε),
Pε(x,x) = 1 + Q(x,x)ε + o(ε),
and the process has probability Q(x,A)ε + o(ε) to jump from x to A within ε.

128
6
Continuous-Space Markov Processes with Jumps
One calls:
• the operator A : the generator of the Markov process or of its semigroup,
• its kernel α = (α(x,dy))x∈V : the jump measure kernel,
• the non-negative real number q(x): the jump intensity or rate from x; if q(x) = 0
then x is an absorbing state,
• the kernel Π = (Π(x,dy))x∈V : the jump law kernel,
• the sequence (XTn)n∈N visiting the states in the same order as (Xt)t∈R+: the in-
duced Markov chain of transition kernel Π (discrete time, continuous space).
Proof (a) As in the proof for Theorem 5.3, a discretization of time by a grid with
mesh ε > 0 is used to yield existence of
q(x) := lim
ε→0+
1 −Pε(x,{x})
ε
∈R+,
Px(T1 > t) = e−q(x)t.
The case q(x) = 0 corresponds to Px(T1 = ∞) = 1 and the case q(x) = ∞is elim-
inated since T1 > T0 = 0 by hypothesis. Likewise, for q(x) > 0 and f ∈L∞this
discretization yields existence of the limit
Ex

1{T1≤t}

f (XT1) −f (x)

= Px(T1 ≤t)
q(x)
lim
ε→0+
Pεf (x) −f (x)
ε
which is thus equal to
q(x)Ex

f (XT1) −f (x)

=
 
f (y) −f (x)

α(x,dy).
The result for Pε(x,A) corresponds to f = 1A, for Pε(x,{x}) to f = 1{x}.
(b) We give a proof which avoids using the strong Markov property. Since the Tn
are isolated, using right continuity and setting Jk = j1 + ··· + jk results in
P(Tk −Tk−1 ≤sk,XTk ∈Ak : 1 ≤k ≤n)
= lim
ε→0+

0≤jk≤⌈sk/ε⌉
1≤k≤n
P(XJk−1ε = ··· = X(Jk−1)ε ̸= XJkε,XJkε ∈Ak : 1 ≤k ≤n)
and the Markov property in Theorem 6.1(c)yields
P(XJk−1ε = ··· = X(Jk−1)ε ̸= XJkε,XJkε ∈Ak : 1 ≤k ≤n)
=

V
π0(dx0)Pε

x0,{x0}
j1−1

A1−{x0}
Pε(x0,dx1)Pε

x1,{x1}
j2−1
···

An−1−{xn−2}
Pε(xn−2,dxn−1)Pε

xn−1,{xn−1}
jn−1

An−{xn−1}
Pε(xn−1,dxn)

6.2
Markov Processes Evolving Only by Isolated Jumps
129
and hence, similarly to (a),
P(Tk −Tk−1 ≤sk,XTk ∈Ak : 1 ≤k ≤n)
=

V
π0(dx0)

1 −e−q(x0)s1
A1
Π(x0,dx1)
···

An−1
Π(xn−2,dxn−1)

1 −e−q(xn−1)sn
An
Π(xn−1,dxn).
The conclusion follows from the conditional expectation characteristic prop-
erty (2.8).
□
Operator Kernels
The operator A in (6.4) is an integral operator, and as such is said to have a
kernel (Q(x,dy))x∈V of signed measures, satisfying A f (x) = ⟨Q(x,·),f ⟩:=

f (y)Q(x,dy) and hence
Q(x,dy) = α(x,dy) −α(x,V )δx(dy) = α(x,dy) −q(x)δx(dy).
Such an operator is often identiﬁed with its kernel, but Markov process theory
stresses instead the jump measure kernel (α(x,dy))x∈V , constituted of positive
measures describing more directly the instantaneous evolution by jumps. One of
the reasons is that the case α(x,V ) = ∞is also of interest.
Back to a Discrete State Space
When V is discrete, identifying a function f to the sequence (f (x))x∈V of its
values speciﬁes a basis, in which an operator A satisfying (6.4) has matrix Q =
(Q(x,y))x,y∈V satisfying
Q(x,y) = α

x,{y}

≥0,
x ̸= y;
Q(x,x) = −α(x,V ) = −

y̸=x
Q(x,y),
and thus (5.3). Conversely, given a matrix Q = (Q(x,y))x,y∈V satisfying (5.3), an
operator A satisfying (6.4) is obtained using the jump kernel
α(x,dy) =

z̸=x
Q(x,z)δz(dy),
x ∈V ,
and A will have matrix Q in the aforementioned basis.

130
6
Continuous-Space Markov Processes with Jumps
6.2.2 Construction, Simulation, Existence
Fundamental Constructions
According to Theorem 6.2, the random evolution of a Markov process (Xt)t≥0 can
be characterized and described in terms of
• its generator A , or corresponding jump kernel (α(x,dy))x∈V of positive mea-
sures, satisfying (6.4),
• or the jump rates (q(x))x∈V and the induced transition kernel (Π(x,dy))x∈V
satisfying (6.5),
and Fig. 5.2 can readily be adapted. These quantities are related by (6.6).
Conversely, Theorem 6.2 is key to try to construct and simulate a Markov process
(Xt)t≥0 starting from such quantities. In the following two constructions, which are
direct transcriptions of the relevant ones in Sect. 5.2.1, draws are always assumed to
be independent.
Algorithm (First construction) Draw a sample for the initial value x0; then itera-
tively, if the process is in state x:
• draw a sample from an E (q(x)) exponential law for the sojourn duration at x,
• draw the next state y ̸= x from the law Π(x,dy) = α(x,dy)
q(x) .
(If q(x) = 0 then the process is absorbed at x.)
This construction constitutes a practical simulation method for the process. It
has the following interpretation in terms of Poisson processes, which is useful for
comprehension as well as for some theoretical results.
Algorithm (First construction, bis) For every x in V , consider a Poisson process
Nx with intensity q(x) and jump instants (T x
n )n≥1, and a sequence (Y x
n )n≥1 of i.i.d.
r.v.’s with laws Π(x,dy), independently. Draw a sample for the initial value x0; then
iteratively, if the process reaches state x, let it stay there until the next jump instant
of Nx, and if this is T x
m then let the process jump to Y x
m.
From a theoretical perspective, in order to construct the (Y x
n )n≥1, a non-countable
product of spaces indexed by x ∈V and equipped with the product σ-ﬁeld can
be used. Any subset of the product space which is a cylinder set based only on a
countable set of coordinates is measurable for the product σ-ﬁeld, and these are the
only kind of sets used in the construction.
The “Second construction” and the “Second construction, bis” in Sect. 5.2.1 are
more difﬁcult to adapt. This can be done using the more advanced tool of Poisson
point processes, also called Poisson random measures.
Interpreting a random jump evolution in terms of one of these constructions often
allows to prove that it corresponds to a Markov process and to identify its generator
A , through its jump kernel α or its jump rates q(x) and its jump law kernel Π.

6.2
Markov Processes Evolving Only by Isolated Jumps
131
Non-explosion and Existence of the Markov Process
Section 5.2.2 can be readily adapted, and we refer to it for explanations. Let (Tn)n≥1
be the sequence of jump instants obtained in one of the above construction attempts.
Theorem 6.3 Let A be an operator satisfying (6.4). If any of the above (equivalent
in law) constructions is such that
lim
n→∞Tn = ∞,
a.s.,
then the resulting process (Xt)t∈R+ is Markov and has generator A .
Proof The above “First construction, bis” will be used. The conclusion follows from
the Markov property of the Poisson processes Nx, the independence of these and
the (T x
n )n≥1, and the fact that the latter are all i.i.d. of appropriate law. Details are
left to the interested reader.
□
The following result provides a criterion for this abstract existence result.
Theorem 6.4 Let A be an operator satisfying (6.4). For any of the above construc-
tions, a.s.,
lim
n→∞Tn = ∞
if and only if

n∈N
1
q(XTn) = ∞.
Proof See the proof for Theorem 5.5.
□
This yields the two following existence results, under veriﬁable assumptions. The
ﬁrst one uses the jump intensities, the second the induced kernel.
Theorem 6.5 Let A be an operator satisfying (6.4). If it is bounded in the sense
that
sup
x∈V
q(x) < ∞,
then any of the above constructions is such that limn→∞Tn = ∞a.s., and yields a
Markov process (Xt)t∈R+ with generator A .
Proof See the proof for Theorem 5.6.
□
Theorem 6.6 Let A be an operator satisfying (6.4), and consider one of the above
constructions. If the jump rates are locally bounded, i.e., if
sup
x∈B
q(x) < ∞
for every bounded set B ⊂V ,

132
6
Continuous-Space Markov Processes with Jumps
and if there exists a random bounded subset B of V such that

n∈N
1{XTn∈B} = ∞,
a.s.,
then limn→∞Tn = ∞a.s., and the construction yields a Markov process (Xt)t∈R+
of generator A .
Proof The proof for Theorem 5.7 is readily adapted.
□
This again allows to understand the terminology “explosion”.
Fundamental Simulation
The “First construction” at the beginning of this Sect. 6.2.2 allows to simulate ef-
ﬁciently a Markov process evolving by isolated jumps on a continuous state space,
with generator A satisfying (6.3). This natural method relies on the fundamental
construction in Theorem 6.2, see also Fig. 5.2, and uses the jump rates q(x) and the
induced transition kernel Π(x,dy), which can be deduced from the jump measure
kernel α of the generator A by
q(x) = α(x,V ),
α(x,dy) = q(x)Π(x,dy).
This method simulates recursively the jumps of the process. In an actual code,
this is obtained by a loop.
Algorithm (Fundamental simulation) Draw a sample for the initial value x0; then
iteratively, if the process is in state x:
• compute the intensity q(x), and draw a sample from the E (q(x)) exponential law
for the duration that the process stays at x,
• compute Π(x,dy) = α(x,dy)
q(x) , and a draw a sample from this law for the next state.
(If q(x) = 0 then the process is absorbed at x.)
It is sometimes possible to perform the latter draw more economically, by steps,
by regrouping the y into adequate subsets.
Fictitious Jump Method
We give another construction method which can be useful in simulations. If ˆq(x) ≥
q(x) for x ∈V then a Markovian kernel ˆΠ(x,dy) is given by
ˆΠ(x,dy) = q(x)
ˆq(x)Π(x,dy),
ˆΠ

x,{x}

= 1 −q(x)
ˆq(x);
(6.7)

6.2
Markov Processes Evolving Only by Isolated Jumps
133
note that (6.6) can be extended to
α(x,dy)1{y̸=x} = q(x)Π(x,dy)1{y̸=x} = ˆq(x) ˆΠ(x,dy)1{y̸=x}
and that
A f (x) = q(x)
 
f (y) −f (x)

Π(x,dy) = ˆq(x)
 
f (y) −f (x)
 ˆΠ(x,dy).
An important special case is when
ˆq(x) = λ ≥sup
y∈V
q(y),
x ∈V .
Here is a method constructing (Xt)t∈R+ much like in the above “First construc-
tion”, but using ˆq and ˆΠ instead of q and Π; the second step (the draw according to
ˆΠ) is decomposed into two separate steps, as would be done in an actual algorithm.
Algorithm (Fictitious jump method) Draw a sample for the initial value x0; then
iteratively, if the process is in state x:
• compute the intensity ˆq(x), and draw a sample from the E (ˆq(x)) exponential law
for a duration after which:
• – either, with probability 1 −q(x)
ˆq(x), the process remains at x,
– or else (with complementary probability q(x)
ˆq(x)), the process leaves x; compute
Π(x,dy) = α(x,dy)
q(x) , and draw a sample from this law for the next state.
(If ˆq(x) = 0 then the process is absorbed at x.)
This method can be advantageous for simulation, when there are good upper
bounds ˆq(x) for the q(x) and the former are much easier to compute than the latter;
a natural choice is to take ˆq(x) constant in x, so as not to have to compute it at each
actual jump of the process.
The sampling result in Theorem 4.4(b), and a simple conditioning show that the
ﬁctitious jump method is equivalent in law to the “First construction”. Its name
comes from the fact that it provides too many “potential” jump instants for the pro-
cess, some of which are then rejected (and are thus “ﬁctitious”).
For instance, a proof of Theorem 6.5 can be thus obtained, by embedding the
jump instants among those of a Poisson process with intensity λ = supx∈V q(x)
(which are known not to accumulate).
6.2.3 Kolmogorov Equations, Feynman–Kac Formula, Bounded
Generator Case
The computations in Sect. 6.1.1 are readily generalized to integral operators with
signed measure kernels. If A is an operator satisfying (6.4) with jump kernel α,

134
6
Continuous-Space Markov Processes with Jumps
then its adjoint operator A ∗acting on measures μ is given by
A ∗μ(dy) =

x
α(x,dy)μ(dx)−q(y)μ(dy) =

x

α(x,dy)μ(dx)−α(y,dx)μ(dy)

(6.8)
(using q(y) = α(y,V ) =

x α(y,dx)). This can be readily recovered, in the form
where μ is written before q or α, from the formal expression A ∗μ = μA .
For the interested reader, we leave as an exercise to obtain these expressions
rigorously using the duality formula.
Theorem 6.2 again has extensions in terms of linear differential equations. Sec-
tions 5.2.4 and 5.2.5 are readily adapted, notably Lemma 5.1. We use the duality
bracket (6.1), and
∥μ∥VT =
sup
∥f ∥∞≤1
⟨μ,f ⟩=
sup
∥f ∥∞≤1

f dμ,
μ ∈M ,
an operator A satisfying (6.4) has operator norm
∥A ∥op =
''A ∗''
op = 2 sup
x∈V
''α(x,dy)
''
VT = 2 sup
x∈V
α(x,V ) = 2 sup
x∈V
q(x),
(6.9)
and hence A is bounded if and only if supx∈V q(x) < ∞. In this case, Theorem 6.4
proves existence of the Markov process with generator A .
Theorem 6.7 Let A be an operator satisfying (6.4), with jump measure kernel α,
and such that
sup
x∈V
q(x) < ∞.
Let (Pt)t≥0 be the semigroup of the Markov process (Xt)t≥0 with generator A .
(a) In the Banach algebra of the operators on L∞, the semigroup (Pt)t≥0 is the
unique solution of the backward Kolmogorov equation
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
d
dt Pt = A Pt,
i.e.,
d
dt Pt(x,dy) =

z
α(x,dz)Pt(z,dy) −q(x)Pt(x,dy),
x ∈V ,
P0 = I.
In the Banach space L∞, for f ∈L∞, the collection (vt)t≥0 of functions vt :
x →Ptf (x) = Ex(f (Xt)) is the unique solution of the backward Kolmogorov
equation
⎧
⎪⎨
⎪⎩
d
dt vt = A vt,
i.e.,
d
dt vt(x) =

vt(y)α(x,dy) −q(x)vt(x),
x ∈V ,
v0 = f.

6.2
Markov Processes Evolving Only by Isolated Jumps
135
(b) In the Banach algebra of the operators on M , the adjoint semigroup (P ∗
t )t≥0
is the unique solution of the forward Kolmogorov equation
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
d
dt P ∗
t = A ∗P ∗
t ,
i.e.,
d
dt Pt(x,dy)
=

z
Pt(x,dz)α(z,dy) −Pt(x,dy)q(y), x ∈V ,
P ∗
0 = I.
In the Banach space M , for any initial law π, the collection (πt)t≥0 of in-
stantaneous laws, given by πt = P ∗
t π = πPt where πt(A) = Pπ(Xt ∈A) for
A ∈B(V ), is the unique solution of the forward Kolmogorov equation (or
Fokker–Planck equation)
⎧
⎪⎨
⎪⎩
d
dt πt = A ∗πt,
i.e.,
d
dt πt(dy) =

z
πt(dx)α(x,dy) −πt(dy)q(y),
π0 = π.
This equation with initial condition μ ∈M has unique solution (μPt)t≥0.
Proof The proof of Theorem 5.8 is readily adapted.
□
The comments following Theorem 5.8 can be readily adapted, notably that weak-
ened results are true when supx∈V q(x) = ∞, and the following result for some
terminal-value equations.
Corollary 6.1 (Feynman–Kac formula) Under the assumptions and with the nota-
tion of Theorem 6.7, for T > 0 and f ∈L∞the backward equation
⎧
⎨
⎩
d
dt ut + A ut = 0,
t ∈[0,T ],
uT = f,
has a unique differentiable solution t ∈[0,T ] →ut ∈L∞, which is given by
x →ut(x) = E

f (XT )|Xt = x

= PT −tf (x).
Proof The time change t →T −t on [0,T ] yields a bijection between the solutions
of this equation and those of the backward Kolmogorov equation in L∞.
□
An integral equation arising in applications, usually through balance considera-
tions, may have the requisite structure to be interpreted as a forward or backward
equation for a pure-jump Markov process. This probabilistic representation allows
its approximate solution by Monte Carlo methods, and can also be useful for theo-
retical results, such as those in the corollary.

136
6
Continuous-Space Markov Processes with Jumps
Forward Kolmogorov Equation as Balance Equation
The forward equation in M can again be interpreted as a balance equation. When
all measures have densities, i.e., when
α(x,dy) = a(x,y)dy,
πt(dx) = pt(x)dx,
this can be written as a functional integral equation: for y ∈V ,
d
dt pt(y) =

x
a(x,y)pt(x)dx −q(y)pt(y) =

x

a(x,y)pt(x) −a(y,x)pt(y)

dx
(using q(y) = α(y,V ) =

a(y,x)dx). It is often obtained in this form in physics
by balance arguments such as the following: the (massic, numerical, etc.) density
pt(y) of particles at y increases due to arrivals from x at rate pt(x) (density at
x) times a(x,y) (transition rate from x to y) and decreases at rate pt(y) times
q(y) =

x a(y,x)dx (departure rate from y).
Generators and Martingales
Modern Markov process theory goes much further than these equations, and ex-
ploits strong links between Markov processes and martingales, established using the
generator, which extend the Kolmogorov equations. Problem 5.3 provided a short
initiation to this topic, which can readily be extended here.
6.3 Markov Processes Following an Ordinary Differential
Equation Between Jumps: PDMP
A more general class of Markov processes is introduced. The processes in this class
are often called piecewise deterministic Markov processes (PDMP).
One of our goals will be to obtain probabilistic representations for kinetic equa-
tions arising in statistical Mechanics, which thus can be given approximate solu-
tions by Monte Carlo methods. These equations describe the time evolution of the
instantaneous distribution of particles in phase space (position-velocity), when the
position evolves according to the velocity, and the velocity jumps at random instants
in function of the particle position and velocity.
6.3.1 Sample Paths, Evolution, Integro-Differential Generator
A family of deterministic evolutions between jumps will now be speciﬁed.
Deﬁnition 6.2 (Ordinary differential equation) Let b : Rd →Rd. It is said that
(xt)t∈I : t ∈I →xt ∈Rd

6.3
Ordinary Differential Equation Between Jumps: PDMP
137
is a local solution, started at x, on the interval of deﬁnition I, of the ordinary dif-
ferential equation (ODE) with vector ﬁeld b, if the interval of deﬁnition I contains
the origin 0 and if, for all t in I,
d
dt xt = b(xt)
and
x0 = x,
and more precisely
xt = x +
 t
0
b(xs)ds.
(6.10)
It is said that there is uniqueness if any two local solutions started at x coincide
on the intersection of their intervals of deﬁnition, and then a maximal solution is
deﬁned on the reunion of all such intervals.
The function b : Rd →Rd is Lipschitz with constant L < ∞if
b(x) −b(y)
 ≤L|x −y|,
∀x,y ∈Rd.
It is locally Lipschitz if for every z ∈Rd there exists a neighborhood Vz of z such
that the restriction of b on Vz is Lipschitz, or equivalently if the restriction of b on
any compact set C is Lipschitz. If b is differentiable then b is locally Lipschitz, and
if the differential is bounded then b is Lipschitz.
The function b has at most afﬁne growth if there exist α,β in R such that
b(x)
 ≤α|x| + β,
∀x ∈Rd,
which is the case if b is Lipschitz.
Theorem 6.8 (Cauchy–Lipschitz) Consider the ODE of Deﬁnition 6.2, and assume
that the vector ﬁeld b is locally Lipschitz. Then:
(a) For every starting point x, there is uniqueness and existence of a maximal so-
lution on a neighborhood of 0.
(b) If these maximal solutions are deﬁned on R+, let φt : x ∈Rd →φt(x) ∈Rd,
where (φt(x))t∈R+ is the solution starting at x, which satisﬁes
φt(x) = x +
 t
0
b

φs(x)

ds.
This deﬁnes the ﬂow of homeomorphisms (φt)t∈R+ of the ODE: the φt are bi-
jective bicontinuous mappings from Rd into φt(Rd) and form a semigroup, i.e.,
φt ◦φs = φt+s for t,s ≥0.
Proof (Main elements) Uniqueness and continuity use the Gronwall inequality. Ex-
istence uses the Picard iteration method. We refer to classic textbooks on ordinary
differential equations.
□
Deﬁnition of a Class of Markov Processes
It will always be assumed that the ODE in Deﬁnition 6.2 has a ﬂow of homeomor-
phisms (φt)t≥0, see Theorem 6.8 for some veriﬁable sufﬁcient conditions.

138
6
Continuous-Space Markov Processes with Jumps
We consider Markov processes with values in Rd which evolve solely under
this ﬂow and by isolated jumps, and which can speciﬁcally be written in the form,
extending (6.3),
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
Xt =

n≥0
φt−Tn(XTn)1{Tn≤t<Tn+1}
for t ≥0,
0 = T0 < T1 ≤T2 ≤··· ≤∞,
limTn = ∞,
Tn < ∞⇒Tn > Tn−1
and
XTn ̸= XTn−.
(6.11)
For n ≥1, if Tn < ∞then it is an actual jump instant. The fact that these have
no accumulation point is again a limitation. However, generally there is no natural
deﬁnition for XTn if Tn = ∞.
In order to study these processes, we shall not try to be wholly rigorous, but will
make rather general ad hoc assumptions so as to introduce the essential parameters.
We shall later give veriﬁable sufﬁcient hypotheses, which will allow to construct
rigorously a sub-class of these processes, in a way appropriate for simulations.
Law of the First Jump Instant
Since T1 is a stopping time, the Markov property of Theorem 6.1(b), implies for any
u and v in R+ that
Px(T1 > u + v) = Px

T1 > u,Xu = φu(x),T1 > u + v

= Px

T1 > u,Xu = φu(x)

Pφu(x)(T1 > v)
= Px(T1 > u)Pφu(x)(T1 > v)
and the properties of the ﬂow (φt)t≥0 and induction yield, for n ≥1 and ε > 0,
Px(T1 > nε) =
n

k=1
Pφ(k−1)ε(x)(T1 > ε) = exp

n

k=1
logPφ(k−1)ε(x)(T1 > ε)

.
As the right continuity of sample paths and the Monotone Convergence Theorem
imply that
∀y,
lim
ε→0+ Py(T1 > ε) = 1,
Py(T1 > t) = lim
ε→0+ Py

T1 > ⌈t/ε⌉ε

,
we deduce
logPx(T1 > t) = lim
ε→0+ ε
⌈t/ε⌉

k=1
logPφ(k−1)ε(x)(T1 > ε)
ε
.

6.3
Ordinary Differential Equation Between Jumps: PDMP
139
Heuristically, this leads us to expect that there exists q : Rd →R+ satisfying
lim
ε→0+
logPx(T1 > ε)
ε
= q(x),
or equivalently
lim
ε→0+
Px(T1 ≤ε)
ε
= q(x),
and further that
Px(T1 > t) = exp

−
 t
0
q

φs(x)

ds

.
(6.12)
Then, the law of T1 on R+ ∪{∞} is given by
PT1(dt) = q

φt(x)

exp

−
 t
0
q

φs(x)

ds

dt +exp

−
 ∞
0
q

φs(x)

ds

δ∞(dt)
and
Px(T1 ≤ε) = q(x)ε + o(ε),
Px(T2 ≤ε) =

Px(t < T2 ≤ε |T1 = t)PT1(dt)
=
 ε
0
Pφt(x)(T1 ≤ε −t)PT1(dt) = o(ε),
so that
Pε

x,

φε(x)

= 1 −q(x)ε + o(ε).
Heuristic Computation of the Generator, Inﬁnitesimal Evolution
The deﬁnition of the generator A and of its domain D(A ) are given in (6.2), and
Pεf (x) −f (x) =
 
f (y) −f (x)

Pε(x,dy)
=

f

φε(x)

−f (x)

Pε

x,

φε(x)

+
 
f (y) −f (x)

1{y̸=φε(x)}Pε(x,dy).
Note that limε→0+ Pε(x,{φε(x)}) = 1. In addition, the chain rule yields that, for all
bounded functions f with continuous bounded differentials,
lim
ε→0+
f (φε(x)) −f (x)
ε
=
d

i=1
bi(x)∂if (x).
Therefore, for all smooth enough functions f in D(A ) and all x, the limit
1
ε
 
f (y) −f (x)

1{y̸=φε(x)}Pε(x,dy)

140
6
Continuous-Space Markov Processes with Jumps
exists. Heuristically, we expect that there exists a positive measure kernel
(α(x,dy))x∈Rd such that
lim
ε→0+
1
ε
 
f (y) −f (x)

1{y̸=φε(x)}Pε(x,dy) =

y

f (y) −f (x)

α(x,dy).
Assuming that
α

x,{x}

= 0
and
α

x,Rd
< ∞,
we obtain
q(x) = α

x,Rd
.
Then the generator acts on f in C1
b(Rd) as an integro-differential operator which can
be given under various forms such as, with the classical notation ∇for the gradient,
A f (x) =
d

i=1
bi(x)∂if (x) +

y

f (y) −f (x)

α(x,dy)
= b(x) · ∇f (x) +

y
α(x,dy)f (y) −q(x)f (x).
(6.13)
Note that the singularity in φε(x), which converges to x, shows that this conver-
gence can be problematic if there is an accumulation of “small jumps”.
For a rigorous characterization of the generator, we refer to Davis [9, Chap. 2].
Remark 6.5 The probabilistic interpretation is that the Markov process is subject to
two inﬁnitesimal tendencies:
• a continuous evolution according to the ﬂow (φt)t≥0 of the ODE with vector ﬁeld
b, called the drift of the process, which during an inﬁnitesimal time dt tends to
make it go from a state x to the state x + b(x)dt,
• a jump evolution determined equivalently by the positive measure jump kernel α,
or by the jump intensities q and the jump law kernel Π, these quantities being
related by
α(x,dy) = q(x)Π(x,dy),
x ∈Rd,
(6.14)
which during an inﬁnitesimal time dt tends to make it jump from a state x with
probability q(x)dt to go to a state chosen according to the law Π(x,dy).
There is a kind of separation of scales: the evolution due to the ODE and the proba-
bility of undertaking a jump are of inﬁnitesimal order dt, but the sizes of the jumps
are macroscopic.

6.3
Ordinary Differential Equation Between Jumps: PDMP
141
6.3.2 Construction, Simulation, Existence
Let be given b and α, or equivalently b and q and Π, see (6.14). These specify
the generator A of the Markov process (Xt)t∈R+ using (6.13), and describe its
inﬁnitesimal evolution tendencies.
We wish to construct the process, mathematically as well as in an effective way
for simulations, under veriﬁable assumptions. We must hence concatenate two kinds
of evolutions, which separately are well understood:
• The continuous evolution corresponding to the differential part of the generator.
We shall assume that the ODE in Deﬁnition 6.2 has a ﬂow of homeomorphisms
(φt)t≥0, see Theorem 6.8 for some veriﬁable sufﬁcient conditions.
• The jump evolution corresponding to the integral part of the generator. For effec-
tive simulation of jumps in the following mathematical constructions, it is advised
to carefully reread Sects. 5.2.3 and 6.2.2, and even Sect. 4.3.
An actual simulation requires either to have an explicit expression for the ﬂow, or
to approximate it in a well-controlled fashion, for instant using an Euler discretiza-
tion scheme, or higher order generalizations such as Runge–Kutta methods.
Several construction methods for the Markov process will now be described.
Fictitious Jump Method
Assume that
sup
x∈Rd q(x) := sup
x∈Rd α

x,Rd
< ∞.
(6.15)
The jump instants (Tn)n≥1 of the Markov process (Xt)t∈R+ will be chosen among
those (τn)n≥1 of a Poisson process (Nt)t∈R+ with ﬁnite intensity λ ≥supx∈Rd q(x)
by a sampling or erasing principle (a “ﬁctitious jump method”).
The sample path is iteratively constructed from one jump instant to the next jump
instant of (Nt)t∈R+, and if (Xt)t∈R+ has value x just before one of these instants,
then its value at that instant will be chosen according to the Markovian kernel
ˆΠ(x,dy) = q(x)
λ Π(x,dy) +

1 −q(x)
λ

δx(dy).
Here is a concrete implementation of this method.
Algorithm (Fictitious jump method) Draw the initial value X0; then iteratively for
n ≥1, when the process is deﬁned for 0 ≤t ≤τn−1:
• draw Sn according to an E (λ) exponential law and set
*τn = τn−1 + Sn,
Xt = φt−τn−1(Xτn−1)
for τn−1 ≤t < τn, and thus Xτn−= φSn(Xτn−1),

142
6
Continuous-Space Markov Processes with Jumps
• – either, with probability 1 −q(Xτn−)
λ
, set Xτn = Xτn−,
– or else (with complementary probability q(Xτn−)
λ
) draw Xτn according to the
law Π(Xτn−,dy),
which determines the process for 0 ≤t ≤τn.
The τn are the jump instants of a Poisson process of intensity λ, and thus
limn→∞τn = ∞and hence limn→∞Tn = ∞, so that this construction indeed de-
ﬁnes a process (Xt)t∈R+. This process is Markov and has generator (6.13). Indeed,
T1 := inf{t > 0 : Xt ̸= Xt−} > t if and only if there is no true jump at the instants τn
preceding t, and classically (see Problem 4.1)
P(T1 > t) =

n≥1
e−λt λntn
n!
1
t
 t
0

1−q(φs(x))
λ

ds
n
= exp

−
 t
0
q

φs(x)

ds

so that (6.12) holds; moreover, the evolution on the time interval [0,T1[ and the law
of the ﬁrst jump XT1 −XT1−are indeed as they should be; then, the strong Markov
property of the Poisson process allows to conclude. Details are left as an exercise.
For an efﬁcient simulation, a good upper bound λ for the jump intensities must
be found, but there will always be a problem if the product of supx∈Rd q(x) and the
temporal horizon of the simulation is large.
In the latter case, if the function q varies greatly then the following methods
might result in faster simulations.
Sub-domain Method
Assume that for a sub-domain S of Rd it holds that
sup
x∈S
q(x) ≤σ ≪sup
x∈Rd q(x) ≤λ.
A Poisson process of intensity σ can then be used as long as the process evolves
within S ; in between its jump instants, one must test whether the ﬂow of home-
omorphisms has remained inside S , and else start the simulation anew from the
instant and the state of exit (strong Markov property of the Poisson process).
A Poisson process of intensity λ must be used when the process is outside S .
Here is a concrete implementation of this method when S is an open set.
Algorithm (Sub-domain method) Draw the initial value X0; then iteratively for
n ≥1, when the process is deﬁned for 0 ≤t ≤τn−1:
• if Xτn−1 is in S , draw Sn according to an E (σ) exponential law, set
*τ ′
n = τn−1 + Sn,
Xt = φt−τn−1(Xτn−1)
for τn−1 ≤t < τ ′
n, and thus Xτ ′n−= φSn(Xτn−1),

6.3
Ordinary Differential Equation Between Jumps: PDMP
143
• check whether (Xt)τn−1<t≤τ ′n stays inside S , and
– if so, set τn = τ ′
n and:
– either, with probability 1 −q(Xτn−)
σ
, set Xτn = Xτn−,
– or else (with complementary probability q(Xτn−)
σ
) draw Xτn according to the
law Π(Xτn−,dy),
– if not, deﬁne τn to be the ﬁrst exit time of S ,
• if Xτn−1 is not in S then proceed as in the previous algorithm (with E (λ)),
which determines the process for 0 ≤t ≤τn.
In order to try to do even better, one can proceed to decompose the state space
in sub-domains within which the function q does not vary much. For instance, as
long as the process evolves within {x ∈Rd : n −1 ≤q(x) ≤n}, a Poisson process
with intensity n can be used; or else, as long as the process evolves within {x ∈Rd :
n −1 ≤∥x∥≤n}, a Poisson process with intensity supn−1≤∥x∥≤n q(x).
Starting from a sub-domain, one must test at each draw of a new jump instant
whether the ﬂow remains inside, and else start the simulation anew from the instant
and the state of exit into a sub-domain in which q can take higher values, using the
higher bound for q there for the next jump instant. This last point is important: if S
were closed, then the previous algorithm would remain trapped at the boundary.
This method can be implemented even if supx∈Rd q(x) = ∞if q is locally
bounded, and results in an actual process (Xt)t∈R+ if there is no explosion, i.e.,
lim
n→∞Tn = ∞
a.s.,
which is automatic if supx∈Rd q(x) < ∞.
It is a simple matter then to prove that (Xt)t∈R+ is a Markov process of generator
given by (6.13), and we leave this as an exercise.
From a practical point of view in actual simulations, problems arise from the
multiplication of tests needed to check when the process exits a given sub-domain,
and the restarts at the instants of exit into a domain with larger bound on q.
True Jump Method
Let us push this idea to its limit, or take inspiration in pure jump processes and
in (6.12), or in Problem 4.3. The jump instants (Tn)n≥1 of the Markov process
(Xt)t∈R+ can be obtained by a time change from the jump instants (τn)n≥1 of a
Poisson process with intensity 1; all these will be used, hence the name “true jump
method”. The sample path is iteratively constructed from one jump instant to the
next jump instant.
Algorithm (True jump method) Draw the initial value X0; then iteratively for
n ≥1, when the process is deﬁned for 0 ≤t ≤Tn−1:

144
6
Continuous-Space Markov Processes with Jumps
• draw Sn according to an E (1) exponential law and set
⎧
⎪⎨
⎪⎩
Tn = inf

u > Tn−1 :
 u
Tn−1
q

φs(XTn−1)

ds = Sn

,
Xt = φt−Tn−1(XTn−1)
for Tn−1 ≤t < Tn, and thus XTn−= φTn−Tn−1(XTn−1),
• draw XTn according to the law Π(XTn−,dy),
which determines the process for 0 ≤t ≤Tn.
This method can be implemented even if supx∈Rd q(x) = ∞, when q is locally
bounded, and results in an actual process (Xt)t∈R+ if there is no explosion, i.e.,
lim
n→∞Tn = ∞
a.s.,
which is automatic if supx∈Rd q(x) < ∞.
It is a simple matter then to prove that (Xt)t∈R+ is a Markov process of generator
given by (6.13), and we leave this as an exercise.
From a practical point of view in actual simulations, problems arise from the
necessity to compute integrals along the sample paths in order to determine the Tn
by time-change.
Remark 6.6 The ﬁctitious jump method is reminiscent of the rejection method
(Sect. 2.2.5), the sub-domain method of the stratiﬁcation variance reduction tech-
nique (Remark 3.4), and the true jump method of the c.d.f. inversion method (The-
orem 2.3), but the justiﬁcation goes much deeper.
6.3.3 Kolmogorov Equations, Feynman–Kac Formula
All statements in Sect. 6.2.3 can be adapted to this framework.
Note that if b does not constantly vanish, then the integro-differential operator
cannot be a bounded operator, and hence does not belong to the corresponding Ba-
nach algebra; nevertheless, the results are true in a weakened form. No rigorous
statement or proof will be given, even though these can be obtained rather simply if
the ﬂow of homeomorphisms (φt)t≥0 of the ODE is well deﬁned (see Theorem 6.8,
e.g.), and the integral part of the operator is bounded (see (6.15)).
An integro-differential arising in applications may have the requisite structure
to be interpreted as a forward or backward equation for a Markov process in the
class we have just studied. This probabilistic representation allows its approximate
solution by Monte Carlo methods, and can also be useful for theoretical results.
Such equations, notably those of the forward Kolmogorov type, are often ob-
tained by balance considerations. An example will be detailed for a kinetic equation
in the next subsection.

6.3
Ordinary Differential Equation Between Jumps: PDMP
145
Recall that the generator A of the Markov process acts on f in C1
b(Rd) as the
integro-differential operator, given in (6.13),
A f (x) =
d

i=1
bi(x)∂if (x) +

y

f (y) −f (x)

α(x,dy).
Its adjoint A ∗acting on measures is obtained by duality for the bracket (6.1).
The adjoint of the integral part of A has already been established in (6.8). The
adjoint of its differential part can be written, in distributional sense as
(bi∂i)∗μ = −∂i(biμ),
this derivative of a measure being interpreted by its action on appropriate test-
functions f as
4
∂i(biμ),f
5
= −
4
(bi∂i)∗μ,f
5
= −⟨μ,bi∂if ⟩
(extending integration by parts when μ has a differentiable density and b is differ-
entiable and f has bounded support). Thus, A ∗is the integro-differential operator
A ∗μ(dx) = −
d

i=1
∂i

bi(x)μ(dx)

+

y

α(y,dx)μ(dy) −α(x,dy)μ(dx)

= −
d

i=1
∂i

bi(x)μ(dx)

+

y
α(y,dx)μ(dy) −q(x)μ(dx).
(6.16)
If μ(dx) = m(x)dx and α(y,dx) = a(y,x)dx then A ∗is made to operate on
the densities by setting (with a classic abuse of notation)
A ∗μ(dx) = A ∗
m(x)dx

=

A ∗m(x)

dx
and, since ∂i(bi(x)m(x)) = bi(x)∂im(x) + ∂ibi(x)m(x),
A ∗m(x) = −
d

i=1
bi(x)∂im(x) −
d

i=1
∂ibi(x)m(x)
+

y

a(y,x)m(y) −a(x,y)m(x)

dy
= −
d

i=1
bi(x)∂im(x) +

y
a(y,x)m(y)dy −
 d

i=1
∂ibi(x) + q(x)

m(x)
= −b(x) · ∇m(x) +

y
a(y,x)m(y)dy −

∇· b(x) + q(x)

m(x)
(6.17)
which features the divergence ∇· b of the vector ﬁeld b.

146
6
Continuous-Space Markov Processes with Jumps
6.3.4 Application to Kinetic Equations
Section 2.1.3 will now be revisited and completed, with some change of notation. A
reference for what follows is the book by Lapeyre et al. [32].
Kinetic equations (also called transport equations in some situations) are statis-
tical physics equations, which notably appear in nuclear physics models. Indistin-
guishable particles (neutrons, photons, charged particles, ...) evolve, according to
classical mechanics and its fundamental principle, in a medium. The medium in-
teracts with the particles by submitting them to accelerations which change their
speeds. The effect of the particles on the medium is neglected.
Any particle is represented in phase space by its position r ∈R3 and its velocity
v ∈R3 at time t ∈R+. This allows to obtain a ﬁrst order differential equation for
the motion, starting from the fundamental principle of dynamics. The point in phase
space can be denoted as
x = (r,v) = (r1,r2,r3,v1,v2,v3) ∈R6
and adequate notation will be used, such as for the ODE vector ﬁeld
b(x) = b(r,v) =

br(r,v),bv(r,v)

=

v,bv(r,v)

∈R6.
(6.18)
Here br(r,v) = v since the position of the particle evolves according to its veloc-
ity, and bv(r,v) is the acceleration of the particle caused by a force ﬁeld, due to the
medium, which may depend both on velocity and on position.
Given the huge number of particles (typically of the order of the Avogadro num-
ber), it is impossible and useless to keep track of all their positions and velocities,
and a statistical mechanics perspective is taken.
Speciﬁcally, a limit procedure is performed in an adequate scale (determined by
the physics of the problem), in which the number of particles goes to inﬁnity and
some signiﬁcant quantities remain ﬁxed.
After this limit is taken, from a probabilistic perspective the study bears on the
probability πt(dr,dv) of ﬁnding in position r a particle of velocity v at time t,
and the corresponding density pt(r,v) if it exists. Physicists and numerical analysts
usually consider the density (e.g., of mass or of number) ft(r,v) of particles in
position r of velocity v at time t. An equation derived in this limit for the evolution
of these laws or densities is called a kinetic equation.
Some physical phenomena are such that the particle velocities vary progressively
for macroscopic durations, at the end of which they undergo brusque deviations due
to rare but brutal interactions with the medium, which are called collisions. In such
media, which are called collisional, there is a separation of scale between the smooth
and the brusque variations of velocities.
Then, the mean free path is deﬁned as the typical distance a particle travels before
its velocity is signiﬁcantly deviated (e.g., above some threshold). A limit is taken in
which this important quantity remains ﬁxed, after which the particle will travel a
random distance of order this quantity before having its velocity undergo a jump.

6.3
Ordinary Differential Equation Between Jumps: PDMP
147
The jump measure and jump law kernels act only on the velocity variable, and
α

r,v,dr′ dv′
= δr

dr′
α

r,v,dv′
,
q(r,v) =

α

r,v,dv′
,
Π

r,v,dr′ dv′
= δr

dr′
Π

r,v,dv′
= δr

dr′α(r,v,dv′)
q(r,v)
,
with the abuse of notation α(r,v,dv′) =

r′ α(r,v,dr′ dv′). The jump measure ker-
nels are obtained by physical considerations, and often have densities:
α

r,v,dv′
= a

r,v,v′
dv′.
The medium is said to be purely collisional if in the limit the velocity evolves
only by jumps, i.e., if
b(x) = b(r,v) = (v,0)
so that the particle position evolves with constant speed between the velocity jumps
(this is called free transport, and the equation is called a transport equation). The
general form (6.18) allows to take into account continuous velocity variations due
to force ﬁelds, for instance an external gravitation ﬁeld, and for a charged particle
an electrical and a magnetic ﬁeld.
The kinetic or transport equations in statistical physics are given for laws
πt(dr,dv), and for their densities pt(r,v) if these exist: πt(dr,dv) = pt(r,v)dr dv.
These integro-differential equations correspond to the forward Kolmogorov (or
Fokker–Planck) equations for the integro-differential operator A in (6.13) corre-
sponding to the above situation, and feature the adjoint operator A ∗, which also
is an integro-differential operator given in (6.16) and (6.17). For densities, these
equations are often written as
∂
∂t pt(r,v) + v · ∇rpt(r,v) + bv(r,v) · ∇vpt(r,v) +

∇v · bv(r,v)

pt(r,v)
=

v′ a

r,v′,v

pt

r,v′
dv′ −q(r,v)pt(r,v)
(6.19)
where the r.h.s. contains the collision terms. In the l.h.s., the ﬁrst two terms are the
only ones in the purely collisional case bv(r,v) = 0 (free transport).
Kinetic Equation as a Balance Equation
These equations are often derived directly by balance considerations from the mod-
els in statistical mechanics.
In this speciﬁc situation, the collisional term derivation (and interpretation) is
sometimes obscured by the symmetries in physical collisions w.r.t. inversion of time.
The particle density pt(r,v) in position r and with velocity v has rates of

148
6
Continuous-Space Markov Processes with Jumps
• increase, due to particles in the same position r and with velocity v′ before a col-
lision which sends them into velocity v, which happens at rate pt(r,v′)a(r,v′,v),
and this should be integrated over all possible v′:

v′ a

r,v′,v

dt pt

r,v′
dv′,
• decrease, due to particles with the same position r and velocity v before a colli-
sion which sends them into some other velocity v′:
pt(r,v)

v′ a

r,v,v′
dv′ = pt(r,v)q(r,v).
The differential terms can be interpreted analogously.
A heuristic derivation from balance considerations of the equation, using the in-
ﬁnitesimal point of view on Markov processes, goes as follows: up to o(dt) terms,
pt+dt(r,v)−pt(r,v) = pt(r −v dt,v)−pt(r,v)+pt

r,v −bv(r,v)dt

−pt(r,v)
+

v′ a

r,v′,v

dt pt

r,v′
dv′ −q(r,v)dt pt(r,v)
where the ﬁrst two terms on the r.h.s. come respectively from transport of the po-
sition according to velocity, and to velocity change due to its drift (due to a force
ﬁeld); the last term (already computed above) comes from collisional change of
velocity. By a ﬁrst order Taylor expansion and the chain rule, up to o(dt) terms,
pt(r −v dt,v) = pt(r,v) −v · ∇rpt(r,v)dt,
pt

r,v −bv(r,v)dt

= pt(r,v)−

bv(r,v)·∇vpt(r,v)+

∇v ·bv(r,v)

pt(r,v)

dt,
so that dividing through by dt tending to 0 yields (6.19) after some rearrangement.
Remark 6.7 Such a heuristic derivation requires little probabilistic knowledge.
However, the Markovian interpretation captures much more of the underlying sta-
tistical physics models, and is quite appropriate for these phenomena. It allows a
much richer description than the kinetic equation, which is solely for instantaneous
laws, whereas the law of the process carries incomparably more information, and
notably allows to consider ﬁrst hitting times, maxima, etc., in scenarios of interest.
Monte Carlo Methods vs. Curse of Dimensionality
This class of integro-differential equations arises in important physical and indus-
trial applications. These equations have the requisite structure so as to be interpreted
as forward or backward equations for a Markov process, which we have described
in detail. This probabilistic representation is of no great surprise for a probabilist,
considering the underlying statistical mechanics models.

6.3
Ordinary Differential Equation Between Jumps: PDMP
149
Such an equation can then thus be given a probabilistic representation, which
allows its approximate solution by Monte Carlo methods.
The efﬁcient computation of approximations for solutions has colossal implica-
tions for scientiﬁc as well as technological and industrial issues. Such Monte Carlo
methods are intensively (and almost exclusively) used, since the high dimension
renders deterministic methods prohibitive.
Indeed, these integro-differential equations were introduced here in dimension 6
for simplicity. More elaborate physical models may be in a much higher dimen-
sion d, since they may take into account degrees of freedom or parameters such
as internal energies or relative positions of atoms in molecules, or (even worse but
quite natural) mixtures of multi-type particles.
Deterministic methods are most often computationally untractable due to their
prohibitive cost in computing time and memory, since these typically scale like 1/εd
if ε is the space step-size.
This is an example of what is called the “curse of dimensionality”.
Probabilistic representations allow to develop and prove convergence of Monte
Carlo methods for approximating the solutions of these equations, and give rates
of convergence. These methods scale well with dimension, since typically only the
variance is affected. They are much used in practice since they are virtually the only
efﬁcient methods.
Physicists have intuitively introduced what they called “Direct Simulation Monte
Carlo” (DSMC) methods, by arguing that they are similar to the physical phenom-
ena. Numerical analysts have sometimes tried to justify these methods by reducing
them to a mere probabilistic approximate quadrature method for the integral part of
the operator.
All this does not capture the subtlety of the stochastic Markovian approach,
which sets up an appropriate framework and allows to speak of important path-
wise quantities, outside the scope of a deterministic approach, such as hitting times,
extrema, etc.
6.3.5 Further Extensions
Boltzmann Equations
The Boltzmann equation has been derived for the study of similar phenomena, in
which particles evolve in a medium composed of like particles, resulting in feed-
back. The interaction between similar particles must be studied.
It is a nonlinear equation, in which the collision operator is quadratic in the law
or its density, due to the fact that the evolution of the density in a point depends on
collisions with similar particles having the same density.
Speciﬁcally, the term a(r,v′,v) in (6.19), giving the effect of the medium on a
particle in r and with velocity v′, is replaced by a term of the form

v∗a

r,v∗,v′,v

pt

r,v∗
dv∗

150
6
Continuous-Space Markov Processes with Jumps
since in position r, the transition of the velocity of a particle from v′ to v is due to
a collision with a particle of velocity v∗, which is there with density pt(r,v∗). Its
rigorous study is very arduous due to the local behavior in r of the collision term,
all the more so that the coefﬁcients obtained in the limit from physical interactions
have singularities of varied types.
Nevertheless Monte Carlo methods can be adapted to this equation. They cannot
any longer be based on independence of particles and the Strong Law of Large
Numbers, but on a related asymptotic property called propagation of chaos; they are
often called mean-ﬁeld Monte Carlo methods.
An important reference book on the Boltzmann equation is Cercignani, Illner and
Pulvirenti [7]. Remarkable probabilistic studies on mean-ﬁeld models, including
those related to this equation, can be found in Sznitman [45] and the references
therein. See also, e.g., Graham and Méléard [22] and the references therein.
Again, the efﬁcient computation of approximations for solutions has colossal
scientiﬁc and technological implications, and Monte Carlo methods are intensively
(and almost exclusively) used.
Linearized versions of the Boltzmann equation (for example around a stationary
solution) are very similar to transport equations. This is why the latter are sometimes
called linear Boltzmann equations, but this oxymoron can be confusing.
Accumulation of Jumps, Compensated Jumps
The most simple and natural assumption on the jump measure kernel α is that it is
constituted of ﬁnite measures, and its mathematical construction and simulation are
greatly simpliﬁed under the uniform bound assumption (6.15). The latter assumption
ensures that the jump instants are isolated, and enables to recursively construct the
process from jump to jump.
However, when the integral operator corresponding to the generator given in The-
orem 6.2, which is also the integral part of the operator (6.13), is written in the form
 
f (y) −f (x)

α(x,dy)
it acts on functions f in C1
b if the kernel α of measures is not necessarily ﬁnite but
satisﬁes the weaker integrability assumption

min

1,|y −x|

α(x,dy) < ∞.
Inspired by the central limit theorem, martingales, Itô’s stochastic calculus, and
the differential terms in the generator, one can even try to go further by “compensat-
ing” the small jumps by their mean. One must then arbitrarily separate small jumps
from large ones, and introduce an operator of the form
 
f (y) −f (x) −1|y−x|<1(y −x) · ∇f (x)

α(x,dy),

6.4
Problems
151
which acts on f in C2
b under the still weaker integrability assumption

min

1,|y −x|2
α(x,dy) < ∞.
The probabilistic interpretation of such kernels is that they describe accumula-
tions of small jumps, of which there may be a summable inﬁnity in a bounded time
interval. The jump instants are no longer isolated, but may accumulate in ﬁnite time;
nevertheless, as in Zeno’s paradox, the Markov process may still proceed beyond
that time. The mathematical construction and the effective simulation of Markov
processes having such a generator is extremely delicate, and there are few general
assumptions guaranteeing its existence.
Problems 6.2 and 6.3 will give some indications about this problem.
6.4 Problems
6.1 (Binary Energy Exchanges) A system of N ≥2 particles is modeled by
a Markov process (Xt)t∈R+ with values in RN
+, in which Xt = (Xi
t)1≤i≤N =
(X1
t ,...,XN
t ) and Xi
t ∈R+ represents the state (energy) of particle number i at
time t.
It evolves under the superposition of gains, losses, and binary exchanges of en-
ergy, which are described as follows, where α ≥0, β ≥0, and γ ≥0 are parameters,
and all draws are independent:
(a) Each particle has a dedicated Poisson process with intensity α, at the jump in-
stants of which its energy is incremented by 1.
(b) Each particle of energy x loses a fraction θ chosen uniformly in [0,1] of it to
the medium after an exponential duration of parameter βx.
(c) For each 1 ≤i < j ≤N, the two particles of indices i and j, while in states
x and y, interact by exchanging energy after an exponential duration of inten-
sity γ xy, and then particle i takes a state z uniformly chosen in [0,x + y] and
particle j takes state x + y −z.
1. Compute the generator of (Xt)t∈R+. Is it bounded?
2. Prove rapidly that (Xt)t∈R+ is well deﬁned.
Hint: Introduce Bk := {(x1,...,xn) ∈Rn : x1 + ··· + xn ≤k} for k ∈N.
3. Describe a natural simulation method for this process, and show that it may be
implemented using summations over N terms at each iteration.
4. Suggest a simple ﬁctitious jump method using fewer computations at each itera-
tion. What could you try to possibly further accelerate the method?
6.2 (Jump Accumulation) Let 0 = a0 < a1 < a2 < ··· be a sequence converging
to 1. Let (Nn
t )t≥0 and (Nt)t≥0 be Poisson processes of intensities λn and λ satisfying

n≥1
1
λn < ∞, and Uk for k ≥1 be uniform on [0,1] for k ≥1. Let X0 take values

152
6
Continuous-Space Markov Processes with Jumps
in [0,1], all these processes and r.v. be independent, and (Xt)t∈R+ be given by
Xt = X0 +

0≤s≤t
(Xs −Xs−),
Xs −Xs−=

n≥1
1{an−1≤Xs−<an}
 an −Xs−
an −an−1
an + Xs−−an−1
an −an−1
an+1 −Xs−

×

Nn
s −Nn
s−

−1{Xs−=1}UNs(Ns −Ns−).
The justiﬁcations may be given in a not wholly rigorous fashion.
1. Describe the process evolution.
2. Show that it is a well-deﬁned Markov process on [0,1]. Compute its generator
and discuss its domain.
3. Say whether the process can be simulated exactly, or approximately, and explain
how.
6.3 (Generalized Kac Equation (⋆)) The velocities of an interacting system of
N ≥2 particles evolve as follows.
A Poisson process with intensity 1 is dedicated to each particle, and at each jump
instant the particle collides with a particle chosen uniformly among the N −1 other
particles.
At each such collision, if the (real-valued) velocities of the particles just before
collision were v1 ∈R and v2 ∈R, then the collision transforms them respectively
into
v′
1 = v1 cosθ −v2 sinθ,
v′
2 = v1 sinθ + v2 cosθ,
for an angle θ drawn in [−π,π] according to a probability measure β(θ)dθ with
even density:
β : θ ∈[−π,π] →β(θ) ∈R+,
β(θ) = β(−θ),
∥β∥1 :=
 π
−π
β(θ)dθ = 1.
Let (Vt)t∈R+ denote the RN-valued process given by Vt = (V k
t )1≤i≤N, where V k
t
denotes the velocity of particle number k at time t.
Let v = (vk)1≤k≤N be the generic point in RN, and for 1 ≤i ≤N let ei =
(0,...,0,1,0,...,0) ∈RN, with a 1 in ith position, denote the ith vector of the
canonical basis.
1. Prove that the kinetic energy Kt = N
k=1(V k
t )2 of the system is constant.
2. (a) Describe how to efﬁciently simulate (Vt)t∈R+. Which problems may arise?
(b) Prove that (Vt)t∈R+ is a pure jump Markov process, with generator A acting
on f ∈L∞(RN) as follows: for v ∈RN,

6.4
Problems
153
A f (v) =
2
N −1

1≤i<j≤N
 π
−π

f

v +

vi(cosθ −1) −vj sinθ

ei
+

vi sinθ + vj(cosθ −1)

ej

−f (v)

β(θ)dθ.
Is this Markov process well deﬁned?
3. Let h : v = (vk)1≤k≤N →h(v) = N
k=1 vk. Let Mt = h(Vt) be the impetus of the
system, and mt = E(Mt) its expectation, t ≥0.
(a) Compute A h in terms of the parameter c =
 π
−π cosθβ(θ)dθ.
(b) Prove that
d
dt mt = 2(c −1)mt,
t ≥0,
then that mt = m0e2(c−1)t.
(c) Prove that c < 1.
In the sequel, β does not necessarily satisfy ∥β∥1 :=
 π
−π β(θ)dθ = 1.
4. Assume here that 0 < ∥β∥1 < ∞.
(a) Is (Vt)t∈R+ a well-deﬁned pure jump Markov process with isolated jumps?
(b) Describe how to simulate it efﬁciently.
(c) What practical problem arises when ∥β∥1 is large?
5. It is now assumed that ∥β∥1 = ∞but
 π
−π θβ(θ)dη < ∞.
The operator A is made to act on the space C1
b(RN) of real bounded functions
with continuous and bounded partial derivatives.
For 0 < ε ≤π let bε = 2
 ε
0 θ2β(θ)dθ < ∞, and deﬁne Aε acting on f ∈
C1
b(RN) as follows: for v ∈RN,
Aεf (v) =
2
N −1

1≤i<j≤N
 π
−π

f

v +

vi(cosθ −1) −vj sinθ

ei
+

vi sinθ + vj(cosθ −1)

ej

−f (v)

β(θ)1|θ|≥ε dθ
−bε
N

i=1
vi
∂
∂vi
f (v).
(a) Prove that A f ∈L∞(RN) for f ∈C1
b(RN).
(b) Prove that limε→0 Aεf (v) = A f (v) for f ∈C1
b(RN) and v ∈RN.
(c) To what kind of Markov process does the generator Aε correspond, and is it
well deﬁned?
(d) How can one simulate it efﬁciently? What practical problem arises when ε
is small?

Chapter 7
Discretization of Stochastic Differential
Equations
Abstract This chapter develops discretization schemes for stochastic differential
equations and their applications to the probabilistic numerical resolution of de-
terministic parabolic partial differential equations. It starts with some important
properties of Itô’s Brownian stochastic calculus, and the existence and uniqueness
theorem for stochastic differential equations with Lipschitz coefﬁcients. Then, us-
ing probabilistic techniques only, existence, uniqueness, and smoothness properties
are proved for solutions of parabolic partial differential equations. To this end, we
show that stochastic differential equations with smooth coefﬁcients deﬁne stochas-
tic ﬂows, and we prove some properties of such ﬂows. We are then in a position to
prove an optimal convergence rate result for the discretization schemes.
7.1 Reminders on Itô’s Stochastic Calculus
Notable reference books on Brownian motion and on Itô’s stochastic integral calcu-
lus are Ikeda and Watanabe [23] and Karatzas and Shreve [26].
7.1.1 Stochastic Integrals and Itô Processes
In this chapter and the next, it is always assumed that a probability space (Ω,F,P)
is given, as well as a ﬁltration on it, i.e., a family (Ft) := (Ft)t∈R+ of time-indexed
sub-σ-ﬁelds of F satisfying
0 ≤s ≤t ⇒Fs ⊂Ft.
For technical reasons and without loss of generality, it is further assumed that
the ﬁltration satisﬁes the so-called usual conditions: for every t in R+, Ft con-
tains all the P-negligible sets in F, and Ft = Ft+ := 
θ>t Fθ. The collection
(Ω,F,P,(Ft)) is called a ﬁltered probability space.
A stochastic process (φt) := (φt)t∈R+ taking values in Rd equipped with its
Borel sigma-ﬁeld B(Rd) is said to be (Ft)-adapted if the random variable φt is
Ft-measurable for every t. To simplify, most often we write “adapted” without
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1_7, © Springer-Verlag Berlin Heidelberg 2013
155

156
7
Discretization of Stochastic Differential Equations
mentioning the underlying ﬁltration. The maps
t ∈R+ →φt(ω)
are called the trajectories or the sample paths of (φt), and play an important role in
its analysis. A process is said to be continuous [resp. right-continuous] if, ω-a.s., its
trajectories are continuous [resp. right-continuous]. To avoid heavy technicalities,
we implicitly limit ourselves to processes (φt) which are measurable, in the sense
that the mapping
(t,ω) ∈R+ × Ω →φt(ω)
is B(R+) ⊗F-measurable. This is not a severe restriction: in particular, all the
processes which we need to consider in practice satisfy it.
A very important process in all the sequel is the following.
Deﬁnition 7.1 (Standard Brownian motion) A real process (Wt) := (Wt)t≥0 is a
standard Brownian motion, or a Wiener process, if it is (Ft)-adapted and continuous
and
(i) W0 = 0, P-a.s.,
(ii) for all 0 ≤s ≤t, the random variable Wt −Ws is independent of Fs,
(iii) for all 0 ≤s ≤t, the random variable Wt −Ws has a N (0,t −s) Gaussian
distribution (zero mean, variance t −s).
Remark 7.1 If (Wt)t≥0 is a standard Brownian motion, then (Wt+h −Wt)h≥0 is a
standard Brownian motion independent of Ft for every t ≥0. This remains true if
t is replaced by an a.s. ﬁnite stopping time.
Itô’s Stochastic Integral
Now consider a ﬁnite time horizon T > 0. Our goal is to construct stochastic inte-
grals w.r.t. a standard Brownian motion (Wt) for an appropriate class of real pro-
cesses. This will be then be straightforwardly extended to vector-valued Brownian
motions and processes at the end of this section.
Consider the class L 2
F (0,T ) of the adapted processes (φt) such that
 T
0
(φθ)2 dθ < +∞,
P-a.s.
An example of such a process is an elementary process, which is a piecewise con-
stant process of the type
¯φt(ω) = ¯¯φ0(ω)1{0}(t) +
n−1

k=0
¯¯φk(ω)1(tk,tk+1](t),

7.1
Reminders on Itô’s Stochastic Calculus
157
where 0 = t0 < t1 < ··· < tn = T and for all k = 0,...,n −1 the random variable
¯¯φk is Ftk-measurable. The Itô stochastic integral process of an elementary process
( ¯φt) w.r.t. (Wt) is deﬁned as
∀0 ≤t ≤T,
It( ¯φ) :=
n−1

k=0
¯¯φk(ω)(Wt∧tk+1 −Wtk∧t).
For any process (φt) in L 2
F (0,T ), it is possible to construct a sequence of el-
ementary processes ( ¯φn
t ) such that
 T
0 |φs −¯φn
s |2 ds converges to 0 in probability
(for P), and that there exists an adapted continuous process (It(φ)), which does not
depend on the particular such sequence, such that
lim
n→∞sup
0≤t≤T
It(φ) −It
 ¯φn = 0,
in probability for P.
For all t in the time interval [0,T ], by deﬁnition
 t
0
φθ dWθ := It(φ)
is called the Itô stochastic integral of (φt) w.r.t. (Wt), which deﬁnes a continuous
adapted process. By construction, this integral is linear w.r.t. the integrated process:
for any (φt) and (ψt) in L 2
F (0,T ) and real a and b,
∀0 ≤t ≤T,
 t
0
(aφθ + bψθ)dWθ = a
 t
0
φθ dWθ + b
 t
0
ψθ dWθ.
A more sophisticated property concerns stopped stochastic integrals. If τ is a
stopping time for the ﬁltration (Ft), then
∀0 ≤t ≤T,
 t∧τ
0
φθ dWθ =
 t
0
φθ∧τ dWθ
(7.1)
(it is straightforward to extend Deﬁnition 2.4 in continuous time).
The Itô stochastic integral on L 2
F (0,T ) does not enjoy many other properties
which allow one to develop exact computations, notably because its expectation
may not be deﬁned.
However, in practice the process which must be integrated often belongs to a
large subclass of L 2
F (0,T ), namely the class M 2
F (0,T ) constituted of the adapted
processes (φt) such that
E
 T
0
(φθ)2 dθ < +∞.
The corresponding stochastic integrals have remarkable properties. For any pro-
cess (φt) in M 2
F (0,T ), it is possible to construct a sequence of elementary pro-

158
7
Discretization of Stochastic Differential Equations
cesses ( ¯φn
t ) which are bounded in (t,ω), and moreover such that the convergence is
stronger than the previous one:
lim
n→∞E
 T
0
φθ −¯φn
θ
2 dθ = 0.
Martingale Properties
The previous approximation result allows to prove the following fundamental theo-
rem on Itô stochastic integrals, which is at the basis of stochastic calculus.
Theorem 7.1 Let (Wt) and ( ˜Wt) be two independent standard Brownian motions.
Let (φt) and (ψt) be two processes in M 2
F (0,T ). Then, P-a.s.,
∀0 ≤s ≤t ≤T,
E
	 t
s
φθ dWθ |Fs

= 0,
(7.2a)
∀0 ≤s ≤t ≤T,
E
	 t
s
φθ dWθ
 t
s
ψθ dWθ |Fs

= E
	 t
s
φθψθ dθ |Fs

,
(7.2b)
and
∀0 ≤s ≤t ≤T,
E
	 t
s
φθ dWθ
 t
s
ψθd ˜Wθ |Fs

= 0.
(7.2c)
Remark 7.2 Under the assumptions of Theorem 7.1, let
Mφ
t :=
 t
0
φθ dWθ,
Mψ
t :=
 t
0
ψθ dWθ,
˜Mψ
t :=
 t
0
ψθ d ˜Wθ.
Equation (7.2a) indicates that (Mφ
t ) is a continuous (Ft)-martingale; so are (Mψ
t )
and ( ˜Mψ
t ). By deﬁnition, the Doob–Meyer brackets (⟨Mφ,Mψ⟩t) and (⟨Mφ, ˜Mψ⟩t)
are the continuous processes with bounded variation and vanishing at time 0 such
that (Mφ
t Mψ
t −⟨Mφ,Mψ⟩t) and (Mφ
t
˜Mψ
t −⟨Mφ, ˜Mψ⟩t) are martingales. Equali-
ties (7.2b) and (7.2c) show that
4
Mφ,Mψ5
t =
 t
0
φθψθ dθ,
4
Mφ, ˜Mψ5
t = 0.
Notably, (Mφ
t )t∈[0,T ] is square integrable and ⟨Mφ⟩t =
 t
0(φθ)2 dθ.
The toolbox of martingale theory now opens for us. Let us recall a few impor-
tant results which will be used, some of them directly in terms of the Itô integrals
involved. The ﬁrst one is the direct consequence of (7.1) and Theorem 7.1, or of
classic results on stopped martingales.

7.1
Reminders on Itô’s Stochastic Calculus
159
Theorem 7.2 (Optional sampling) Under the assumption of Theorem 7.1, if τ is a
(Ft)-stopping time, then
∀0 ≤s ≤t ≤T,
E
	 t∧τ
s∧τ
φθ dWθ |Fs

= 0,
∀0 ≤s ≤t ≤T,
E
	 t∧τ
s∧τ
φθ dWθ
2
|Fs

= E
	 t∧τ
s∧τ
(φθ)2 dθ |Fs

.
The following one can be applied to the Itô integrals in Remark 7.2; see, e.g.,
Karatzas and Shreve [26, Thm 1.3.8].
Theorem 7.3 (Doob’s Lp maximal inequalities) Let (Mt)t≥0 be a martingale with
right-continuous paths. For any real number p > 1, it holds that
E

sup
0≤s≤t
|Ms|p
≤

p
p −1
p
E

|Mt|p
.
Some more advanced martingale inequalities come last; see, e.g., Karatzas and
Shreve [26, Thm 3.3.28].
Theorem 7.4 (Burkholder–Davis–Gundy (BDG) inequalities) Let m ≥1 be an ar-
bitrary real number. There exist universal constants km and Km such that, under the
assumption of Theorem 7.1,
kmE
	 t
0
(φθ)2 dθ
m
≤E
	
sup
0≤s≤t

 s
0
φθ dWθ

2m
≤KmE
	 t
0
(φθ)2 dθ
m
.
Exercise 7.1 Deduce the BDG inequality for m = 1 from the Doob inequalities,
giving the best constants k1 and K1 you can.
Multidimensional Extension
Vector-valued extensions are straightforward. Let d,r ≥1 be two integers.
An r-dimensional, or Rr-valued, standard Brownian motion (Wt)t≥0 is of the
form Wt = (W 1
t ,...,W r
t ) where (W 1
t )t≥0,...,(W r
t )t≥0 are r independent standard
Brownian motions. This is preserved by change of orthonormal basis.
A process (σt) = (σ ij
t ) with values in the space Rd⊗r of matrices with d rows
and r columns is in L 2
F (0,T ) or in M 2
F (0,T ) if all its terms (or coordinates) are,
and then the d-dimensional Itô integral of (σt) w.r.t. (Wt) is deﬁned by
 t
0
σθ dWθ :=
 r

j=1
 t
0
σ ij
θ dW j
θ

1≤i≤d
.

160
7
Discretization of Stochastic Differential Equations
7.1.2 Itô’s Formula, Existence and Uniqueness of Solutions
of Stochastic Differential Equations
Let (bt) be an Rd-valued adapted process such that
 T
0
|bθ|dθ < ∞,
P-a.s.,
and (σt) = (σ ij
t ) be an Rd⊗r-valued process such that each coordinate process (σ ij
t )
belongs to L 2
F (0,T ). The Rd⊗d-valued process at := σt · σ ∗
t has symmetric non-
negative values; σ ∗
t denotes the transposed matrix of σt, and we use matrix multi-
plication.
Consider an Rr-valued standard Brownian motion (Wt)t≥0 and an Rd-valued and
F0-measurable random vector X0. Let (Xt) be the Rd-valued process deﬁned by
∀1 ≤i ≤d,
Xi
t = Xi
0 +
 t
0
bi
θ dθ +
r

j=1
 t
0
σ ij
θ dW j
θ
(7.3a)
which can be written in vector form as
Xt = X0 +
 t
0
bθ dθ +
 t
0
σθ dWθ.
(7.3b)
This is called an Itô process.
Itô’s Formula for Change of Variable
Given a smooth function u(t,x), the following fundamental theorem allows to ex-
press u(t,Xt) by means of stochastic integrals; see, e.g., Karatzas and Shreve [26,
Thm 3.3.3]. For a real function u : (t,x) ∈[0,T ] × Rd →u(t,x) of class C 2 in the
space coordinate x = (xi)1≤i≤d, the corresponding partial derivatives are denoted
by
∂iu := ∂u
∂xi
,
∂2
iju :=
∂2u
∂xi∂xj
,
1 ≤i,j ≤d.
Theorem 7.5 (Itô’s formula) Let u(t,x) be a real function in C 1,2([0,T ] × Rd).
Let (Xt)t≥0 be given as in (7.3a). Then, P-a.s., for all t in [0,T ],
u(t,Xt) = u(0,X0) +
 t
0
∂u
∂θ (θ,Xθ)dθ +
d

i=1
 t
0
∂iu(θ,Xθ)bi
θ dθ
+
d

i=1
r

j=1
 t
0
∂iu(θ,Xθ)σ ij
θ dW j
θ + 1
2
d

i,k=1
 t
0
∂2
iku(θ,Xθ)aik
θ dθ.
(7.4)

7.1
Reminders on Itô’s Stochastic Calculus
161
The equality (7.4) is the celebrated Itô formula, often written compactly as
du(θ,Xθ) = ∂u
∂θ (θ,Xθ)dθ + ∇u(θ,Xθ)bθ dXθ + 1
2 Trace

∇∇∗u(θ,Xθ)aθ

dθ
where ∇u denotes the (column) vector in Rd with coordinates ∂i, accordingly ∇∇∗u
denotes the d × d matrix with entries ∂2
iju. This can be seen as an extension of the
chain rule for differentiation to stochastic integrals.
Stochastic Differential Equations
The next theorem provides useful existence and uniqueness results.
Theorem 7.6 Assume that a ﬁltered probability space (Ω,F,P,(Ft)) and an Rr-
valued standard Brownian motion (Wt) are given. Let b be a Lipschitz function from
Rd to Rd, and σ a Lipschitz function from Rd to Rd⊗r.
Then, there is pathwise existence and uniqueness for the following stochastic
differential equation:
For any Rd-valued random variable X0 which is F0-measurable (thus inde-
pendent of (Wt)) and has ﬁnite second moment E|X0|2 < ∞, and any arbitrary
deterministic ﬁnal time T > 0, there exists a unique Rd-valued process (Xt) in
M 2
F (0,T ) such that, P-a.s., for all t in [0,T ],
∀1 ≤i ≤d,
Xi
t = Xi
0 +
 t
0
bi(Xθ)dθ +
r

j=1
 t
0
σ ij(Xθ)dW j
θ ,
(7.5a)
or in vector notation
Xt = X0 +
 t
0
b(Xθ)dθ +
 t
0
σ(Xθ)dWθ,
(7.5b)
in short dXt = b(Xt)dt + σ(Xt)dWt.
Moreover, there is uniqueness in law: the law of (Xt) depends only on b, σ, and
the law of X0, but not on the speciﬁc choice of (Ω,F,P,(Ft),(Wt)).
The process (Xt) is called the unique strong or pathwise solution of the stochastic
differential equation (7.5a)–(7.5b) starting at X0.
The proof of the preceding theorem uses the Itô formula, martingale inequalities,
and (as for Theorem 6.8) the Gronwall inequality for uniqueness and a Picard ap-
proximation sequence for existence: see, e.g., Sect. 5.2 in Karatzas and Shreve [26].
This “constructive” procedure yields that, ω-a.s., (Xt(ω)) can be expressed as a
measurable functional of X0(ω) and the trajectory (Wt(ω)). This functional is not
explicit, but allows to express the law of (Xt) as the image measure of the law of
(X0,(Wt)), yielding uniqueness in law.

162
7
Discretization of Stochastic Differential Equations
The fact that pathwise uniqueness implies uniqueness in law is actually a spe-
cial case of a very general result, the Yamada–Watanabe theorem, see Karatzas and
Shreve [26, Prop 5.3.20].
Remark 7.3 Theorem 7.6 provides an existence and uniqueness result for strong
solutions, that is, when the ﬁltered probability space and the Brownian motion are
ﬁxed. A related question can be asked: given b, σ, and the law of X0, can one ﬁnd
(Ω,F,P,(Ft),(Wt)) and an adapted process (Xt) such that (7.5a)–(7.5b) holds?
The quantity of interest is then the law of the process (Xt). Such solutions in law
are often referred to as weak solutions. For a discussion of these issues, see, e.g.,
Karatzas and Shreve [26, Sects. 5.2 and 5.3].
Deﬁnition 7.2 Given a time origin 0 ≤s, we deﬁne the ﬂow (Xt(x)) from time s
to time T > s as the family of solutions (when the deterministic initial condition x
varies) to the equations
Xt = x +
 t
s
b(Xθ)dθ +
 t
s
σ(Xθ)dWθ,
s ≤t ≤T.
(7.6)
7.1.3 Markov Properties, Martingale Problems and Fokker–Planck
Equations
Under the assumptions of Theorem 7.6, for all t,h ≥0 it holds that
Xt+h = Xt +
 t+h
t
b(Xθ)dθ +
 t+h
t
σ(Xθ)dWθ.
Let us admit for the time being that Xt has a second moment (which will be proved
soon), and recall Remarks 7.1 and 7.3.
Then, (Xt+h)h≥0 is conditionally independent of Ft knowing Xt, and has the
same law as the solution of the stochastic differential equation starting at Xt.
Hence, (Xt)t≥0 is a (Ft)-Markov process, see Deﬁnition 6.1 and Theorem 6.1.
It is even strong Markov, see Remark 7.1.
Moreover, Itô’s formula yields for every real f in C2
b(Rd) and t ≥0 that
f (Xt) = f (X0) +
d

i=1
 t
0
bi(Xθ)∂if (Xθ)dθ +
d

i=1
r

j=1
 t
0
σ ij(Xθ)∂if (Xθ)dW j
θ
+ 1
2
d

i,k=1
 t
0
aik(Xθ)∂2
ikf (Xθ)dθ

7.1
Reminders on Itô’s Stochastic Calculus
163
and, deﬁning the second order differential operator
L =
d

i=1
bi(·)∂i + 1
2
d

i,k=1
aik(·)∂2
ik
which acts on f in C2
b(Rd) by
L f : x ∈Rd →
d

i=1
bi(x)∂if (x) + 1
2
d

i,k=1
aik(x)∂2
ikf (x),
(7.7)
this can be written as
f (Xt) = f (X0) +
 t
0
L f (Xθ)dθ +
d

i=1
r

j=1
 t
0
σ ij(Xθ)∂if (Xθ)dW j
θ .
(7.8)
Note that the sum of stochastic integrals may seem messy, but it deﬁnes a mar-
tingale. We have just proved that (Xt) (more precisely its law) satisﬁes a martingale
problem.
Martingale problems have become a central topic in modern probability theory
for the study of Markov processes, and are profoundly related to weak solutions
such as in Remark 7.3.
Generator and Fokker–Planck Equation
Taking expectation of (7.8) yields that, for all f in C2
b(Rd),
Ef (Xt) = Ef (X0) +
 t
0
EL f (Xθ)dθ
(7.9)
and hence, for X0 = x and t = ε > 0,
Ef (Xε) −f (x)
ε
= 1
ε
 ε
0
EL f (Xθ)dθ →[ε →0]L f (x)
so that, considering (6.2), the generator of the Markov process (Xt)t≥0 is (an exten-
sion of) L , acting on a domain containing C2
b(Rd).
Such a Markov process is often called a diffusion process.
More can be said. Let πt denote the law of Xt. Using the duality bracket (6.1)
given by integration, we can rewrite (7.9) as
⟨πt,f ⟩= ⟨π0,f ⟩+
 t
0
⟨πθ,L f ⟩dθ.

164
7
Discretization of Stochastic Differential Equations
This is a weak formulation, with C2
b test functions, for the Fokker–Planck equation
for measures
∂πt
∂t = L ∗πt
where the adjoint L ∗is taken w.r.t. the duality bracket with C2
b(Rd) and can be
written in the sense of the distributions, as
L ∗πt(dx) = −
d

i=1
∂i

bi(x)πt(dx)

+ 1
2
d

i,k=1
∂2
ik

aik(x)πt(dx)

.
This is a forward Kolmogorov equation, akin to those we have already encountered
in Theorems 5.8 and 6.7.
If b is C1 and a is C2 and πt(dx) = pt(x)dx for a C2 density p(·) then (pt)
satisﬁes the Fokker–Planck partial differential equation
∂pt
∂t = L ∗pt(x) = −
d

i=1
∂i

bi(x)pt(x)

+ 1
2
d

i,k=1
∂2
ik

aik(x)pt(x)

.
Reciprocally, given a parabolic partial differential equation of one of the above
forms (for instance derived by physical balance considerations), we can give it a
probabilistic representation using the solution (Xt)t≥0 of the corresponding stochas-
tic differential equation. This allows us to approximate the solution of the parabolic
equation by a Monte Carlo method: for any f in Cb,
⟨πt,f ⟩= lim
N→∞
1
N
N

i=1
f

X(i)
t

,
a.s.,
where the (X(i)
t )t≥0 are independent copies of (Xt)t≥0.
Since most stochastic differential equations cannot be solved explicitly, the issue
is now to be able to effectively simulate good approximations (Xn
t )t≥0 for (Xt)t≥0,
where n denotes a precision parameter destined to become large.
The next section introduces such approximations. Their convergence properties
will then be studied in detail. This will be applied to the analysis of Monte Carlo
methods using the probabilistic representation given by the Feynman–Kac formula,
the transposition for diffusion processes of Corollaries 5.1 and 6.1 related to the
backward Kolmogorov equations in Theorems 5.8 and 6.7.
Generalizations
The study in Sect. 6.3 can be adapted to obtain a Markov process (Xt)t∈R+ which
evolves according to a stochastic differential equation between isolated jumps. The

7.2
Euler and Milstein Schemes
165
generator A of the resulting process can be seen to act on f in C2
b(Rd), generalizing
(7.7) into
A f (x) =
d

i=1
bi(x)∂if (x) + 1
2
d

i,k=1
aik(x)∂2
ikf (x) +
 
f (y) −f (x)

α(x,dy).
Many operators issuing from physics exhibit such a combination of an integral
term and a second order differential term. The latter is usually interpreted as a dif-
fusive term, due for instance to an accumulation of small jumps on velocity, or to
analogies with the heat equation.
This can happen, for various models, in the kinetic equations we have studied in
Sect. 6.3.4, for the velocity as well as for the position variable, yielding for instance
so-called transport-diffusion equations.
7.2 Euler and Milstein Schemes
When the function σ is null, (7.5a)–(7.5b) is an ordinary differential equation; even
then, for most functions b an exact solution (Xt) cannot be exhibited under the
form Xt = φ(X0,t) with a function φ explicitly deﬁned using usual functions only.
The situation is even worse when σ is not null, and seldom can one write Xt =
φ(X0,t,(Ws,s ≤t)) for some function φ satisfying the same constraint as above.
It is thus necessary to approximate (Xt) numerically, in a way for which simula-
tions can be implemented simply and quickly on a computer.
In our introduction we have insisted on the fact that, in practice, one simulates
stochastic processes with the aim of computing information relating to their law,
that is, computing by Monte Carlo methods expectations of the type Ef (XT ) or,
more generally, Ef (XT1,...,XTk) or Ef (Xs,0 ≤s ≤T ), where f is a function
respectively deﬁned on Rd or (Rd)k or C ([0,T ],Rd).
Our ﬁrst objective is to construct discrete time Markov chains Xn which are easy
to simulate1 and are such that Ef (Xn
T ) ≃Ef (XT ) for all f in a large class of
functions. The processes Xn which we will construct depend on a step-size T /n,
where n is destined to become large.
Our second objective is to estimate the approximation error Ef (Xn
T ) −Ef (XT )
in terms of f and n and T .
The convergence rate results which will be proved below sustain the following
good news: without particular reason due to local singularities or non-linear growth
of the coefﬁcients b and σ, one can obtain satisfying convergence rates by using the
simpler possible approximation method, the Euler scheme.
1This means that there exists a low complexity algorithm for generating sequences of independent
draws of sample paths of Xn.

166
7
Discretization of Stochastic Differential Equations
Let us intuitively derive this scheme. Observing that
X(p+1)T/n = XpT/n +
 (p+1)T/n
pT/n
b(Xs)ds +
 (p+1)T/n
pT/n
σ(Xs)dWs
≃XpT/n + b(XpT/n)T
n + σ(XpT/n)(W(p+1)T/n −WpT/n) (7.10)
justiﬁes the following discretization scheme.
Algorithm (Euler’s scheme) Select T > 0 and n ≥1, and construct iteratively
(Xn
pT/n,0 ≤p ≤n) by setting
*Xn
0 = X0,
Xn
(p+1)T/n = Xn
pT/n + b

Xn
pT/n
T
n + σ

Xn
pT/n

W(p+1)T/n −WpT/n

.
(7.11)
For actual simulation of one realization, draw a sample for X0 from the initial law,
then independent samples of W(p+1)T/n −WpT/n for p = 0,...,n −1 from the
N (0,T /n) Gaussian law (zero mean, variance T /n).
Note that no partial derivative of the coefﬁcients b and σ appears in the scheme.
We now construct another discretization scheme, the Milstein scheme, by adding
terms to the expansion in (7.10). A naive analysis shows that the drift increment is
of order T /n and the Brownian increment is of order √T /n, and we complement
only the latter.
Denote by σj the jth column vector of the matrix σ and by ∂σj its Jacobian
matrix (
∂σ i
j
∂xn )1≤i,n≤d. Applying Itô’s formula to σ(Xs) −σ(XpT/n) yields
X(p+1)T/n = XpT/n +
 (p+1)T/n
pT/n
b(Xs)ds +
 (p+1)T/n
pT/n
σ(Xs)dWs
≃XpT/n + b(XpT/n)T
n + σ(XpT/n)(W(p+1)T/n −WpT/n)
+
r

j,k=1
∂σj(XpT/n)σk(XpT/n)
 (p+1)T/n
pT/n

W k
s −W k
pT/n

dW j
s
(7.12)
and this leads to the following new scheme.
Algorithm (Milstein’s scheme) Select T > 0 and n ≥1, and construct iteratively
( ¯Xn
pT/n,0 ≤p ≤n) by setting

7.2
Euler and Milstein Schemes
167
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
¯Xn
0 = X0,
¯Xn
(p+1)T/n = ¯Xn
pT/n + b
 ¯Xn
pT/n
T
n +
r

j=1
σj
 ¯Xn
pT/n

W j
(p+1)T/n −W j
pT/n

+
r

j,k=1
∂σj
 ¯Xn
pT/n

σk
 ¯Xn
pT/n
 (p+1)T/n
pT/n

W k
s −W k
pT/n

dW j
s .
(7.13)
For actual simulation of one realization, it is necessary to be able to efﬁciently draw
samples for the last stochastic integrals.
This scheme has been introduced by Milstein to improve the Euler scheme con-
vergence rate in Lp(Ω) norm, see below. Unfortunately, its interest is more theo-
retical than practical since, when r > 1, its algorithmic complexity is substantially
larger than the complexity of the Euler scheme.
Indeed, in addition to the fact that it requires the explicit knowledge or the nu-
merical approximation of partial derivatives of the functions σj (which may have
a prohibitive cost in the simulation of large dimensional processes), the Milstein
scheme requires to simulate the many stochastic integrals
 (p+1)T/n
pT/n

W k
s −W k
pT/n

dW j
s
and major difﬁculties arise for k ̸= j: the joint probability distribution of these in-
tegrals and of the increments W k
(p+1)T/n −W k
pT/n seems impossible to simulate
exactly.
In addition, even if we knew how to simulate this joint probability distribution,
we would not improve the Euler scheme convergence rate for the approximation of
Ef (XT ): we will see that, for the family of problems of interest, the Milstein and
Euler schemes generically are of identical accuracy.
When r = 1, the Milstein scheme simpliﬁes notably and can easily be simulated.
Indeed, the Itô formula yields that
 (p+1)T/n
pT/n
(Ws −WpT/n)dWs = 1
2
	
(W(p+1)T/n −WpT/n)2 −T
n

and the Milstein scheme simpliﬁes into
¯Xn
(p+1)T/n = ¯Xn
pT/n + b
 ¯Xn
pT/n
T
n + σ
 ¯Xn
pT/n

(W(p+1)T/n −WpT/n)
+ 1
2σ
 ¯Xn
pT/n

σ ′ ¯Xn
pT/n

(W(p+1)T/n −WpT/n)2 −T
n

(7.14)
which requires only the simulation of the Brownian increments.
A similar situation arises for r > 1 when the column vectors of the matrix σ
satisfy the restrictive algebraic condition
∀i, ∀j,
∂σj(·)σi(·) = ∂σi(·)σj(·).
(7.15)

168
7
Discretization of Stochastic Differential Equations
This condition is obviously satisﬁed when r = 1 and when the function σ is con-
stant.
Indeed, Itô’s formula implies for j ̸= k that
 (p+1)T/n
pT/n

W k
s −W k
pT/n

dW j
s +
 (p+1)T/n
pT/n

W j
s −W j
pT/n

dW k
s
=

W k
(i+1)T/n −W k
pT/n

W j
(p+1)T/n −W j
pT/n

and if (7.15) holds then the Milstein scheme simpliﬁes into
¯Xn
(p+1)T/n
= ¯Xn
pT/n + b
 ¯Xn
pT/n
T
n + σ
 ¯Xn
pT/n

(W(p+1)T/n −WpT/n)
+
r

k=2

j<k
∂σj
 ¯Xn
pT/n

σk
 ¯Xn
pT/n

W k
(p+1)T/n −W k
pT/n

W j
(p+1)T/n −W j
pT/n

+ 1
2
r

j=1
∂σj
 ¯Xn
pT/n

σj
 ¯Xn
pT/n
	
W j
(p+1)T/n −W j
pT/n
2 −T
n

(7.16)
which requires only the simulation of the Brownian increments.
A beautiful result by Clark and Cameron [8] shows that, under the commutativ-
ity hypothesis (7.15), the Milstein scheme leads to the optimal convergence rate in
L2(Ω) norm among all the discretization schemes which use only random variables
equal to the increments of the process (W(t)) at times {pT /n,0 ≤p ≤n}.
7.3 Moments of the Solution and of Its Approximations
We now show that the solution of a stochastic differential equation with Lipschitz
coefﬁcients has the same moments as its initial condition, and then that the Euler
scheme shares the same property uniformly w.r.t. n.
Notation 7.7 From now on, the Euclidean norm | · | on Rd and the norm ∥M∥2 :=
Trace(MM∗) = Trace(M∗M) for matrices M will be used. In addition, C will de-
note any constant depending only on the discretization parameter n. The value of C
may change from line to line, but in every instance it remains uniform w.r.t. n.
Let us now recall the classic Gronwall inequality, which will be of much use.
Lemma 7.1 (Gronwall’s inequality) Let g be a continuous function on [0,T ]. Sup-
pose that there exists a constant β > 0 and a positive integrable function α on [0,T ]

7.3
Moments of the Solution and of Its Approximations
169
such that
g(t) ≤α(t) + β
 t
0
g(s)ds,
∀0 ≤t ≤T.
Then
g(t) ≤α(t) + β
 t
0
α(s)eβ(t−s)s ds,
∀0 ≤t ≤T.
If in addition α is an increasing function,
g(t) ≤α(t)eβt,
∀0 ≤t ≤T.
Proof Then
d
dt

e−βt
 t
0
g(s)ds

= e−βtg(t) −βe−βt
 t
0
g(s)ds ≤α(t)e−βt
and hence
e−βt
 t
0
g(s)ds ≤
 t
0
α(s)e−βs ds,
which we use on the r.h.s. of the inequality in the assumption to conclude.
□
Theorem 7.8 Let Yt be a d-dimensional Itô process as in (7.3a), of the type
Yt = Y0 +
 t
0
bs ds +
 t
0
σs dWs,
where (Ws) is an Rr-valued standard Brownian motion, and (bs) and (σs) are
adapted processes taking values respectively in Rd and Rd⊗r. Let m ≥1 be an
integer such that E|Y0|2m < ∞and s →E|bs|2m + E∥σs∥2m is bounded on [0,T ].
Then, there exists a constant C > 0 such that
E|Yt|2m ≤eCtE|Y0|2m + CeCt
 t
0

E|bs|2m + E∥σs∥2m
ds,
∀0 ≤t ≤T. (7.17)
Proof The function |x|2m = (d
i=1 x2
i )m is in C∞(Rd) and has ﬁrst and second
order partial derivatives given by
∂i|x|2m = 2mxi|x|2(m−1),
∂2
ij|x|2m = 4m(m −1)xixj|x|2(m−2) + 2m|x|2(m−1)1{i=j}.
(7.18)
Note that x →x2m is C2 but not C2
b, and that x →x2m is of class C 2 but not C 2
b . We
are going to apply Itô’s formula. To avoid integrability issues in each of its terms,
a classic localization procedure using stopping times allows to deal with bounded
processes.

170
7
Discretization of Stochastic Differential Equations
Deﬁne τK := inf{s > 0,|Ys| ≥K} for K ≥1. This localizing sequence of stop-
ping times is such that |Yt∧τK | ≤K for all t and limK→∞Yt∧τK = Yt. Using the
localization property (7.1) of stochastic integrals, Itô’s formula and equalities (7.18)
yield
|Yt∧τK |2m = |Y0|2m + 2m
 t∧τK
0
|Ys∧τK |2m−2Ys∧τK · bs ds
+ 2m
 t∧τK
0
|Ys∧τK|2m−2Ys∧τK · σs dWs
+ m
 t∧τK
0
|Ys∧τK |2m−2∥σs∥2 ds
+ 2m(m −1)
 t∧τK
0
|Ys∧τK|2m−4|σ ∗
s Ys∧τK|2 ds
(the last term vanishes for m = 1). The stochastic integral in the r.h.s. is a martingale
since |Ys∧τK| ≤K for all s, and since the stopping time t ∧τK is bounded from
above by T , optional sampling (Theorem 7.2) yields
E
 t∧τK
0
|Ys∧τK |2m−2Ys∧τK · σs dWs = 0.
In addition, there exists a constant Cm such that, for all non-negative x and y,
x2m−2y2 ≤Cm(x2m + y2m).2 One deduces that, possibly changing Cm,
E|Yt∧τK |2m ≤E|Y0|2m + Cm
 t
0
E|bs|2m ds + Cm
 t
0
E∥σs∥2m ds
+ Cm
 t
0
E|Ys∧τK |2m ds.
(7.19)
The second version of Gronwall’s inequality (Lemma 7.1) yields
E|Yt∧τK|2m ≤eCmtE|Y0|2m + CmeCmt
 t
0

E|bs|2m + E∥σs∥2m
ds.
(7.20)
It then remains to let K tend to inﬁnity, observe that
liminf
K→∞|Yt∧τK |2m = lim
K→∞|Yt∧τK|2m = |Yt|2m,
and apply Fatou’s lemma to conclude.
□
This result on Itô processes will be now used to prove a similar result for the
solution (Xt)t≥0 of a stochastic differential equation, which is our central interest.
2Actually x2m−2y2 ≤max(x2m,y2m).

7.3
Moments of the Solution and of Its Approximations
171
Theorem 7.9 Under the assumptions of Theorem 7.6, let m ≥1 be an integer such
that E|X0|2m < ∞. Then, there exists C < ∞only depending on m, T , and the
Lipschitz constants of b and σ, such that
E|Xt|2m ≤

1 + E|X0|2m
eCt,
∀0 ≤t ≤T,
(7.21a)
E sup
0≤t≤T
|Xt|2m ≤C

1 + E|X0|2m
eCT ,
(7.21b)
E|Xt −Xs|2m ≤C

1 + E|X0|2m
(t −s)m,
∀0 ≤s ≤t ≤T.
(7.21c)
Proof Theorem 7.8 cannot be directly applied, since so far we do not know anything
about the moments of bs := b(Xs) and σs := σ(Xs), and we instead use (7.19):
E|Xt∧τK|2m ≤E|X0|2m + C
 t
0
E|bs|2m ds
+ C
 t
0
E∥σs∥2m ds + C
 t
0
E|Xs∧τK|2m ds.
The functions b and σ are Lipschitz, thus there exists L > 0 such that
b(x)
 +
''σ(x)
'' ≤L

1 + |x|

,
∀x ∈Rd,
(7.22)
from which
b(x)
2m +
''σ(x)
''2m ≤Cm

1 + |x|2m
,
∀x ∈Rd.
(7.23)
Consequently, there exists C > 0 depending on m and L only such that
E|Xt∧τK|2m ≤E|X0|2m + C
 t
0

1 + E|Xs∧τK|2m
ds.
Then (7.21a) follows by applying Gronwall’s inequality to the function g(t) :=
1 + E|Xt∧τK|2m and using Fatou’s lemma to let K go to inﬁnity. Moreover (7.21b)
follows from (7.5a)–(7.5b) by noting that
 k

i=1
|xi|
2m
≤k2m−1
k

i=1
|xi|2m,
∀xi ∈R,
(7.24)
and by applying the Hölder inequality to the integral
 t
0 b(Xs)ds and the Burkholder–
Davis–Gundy inequality (Theorem 7.4) to the stochastic integral
 t
0 σ(Xs)dWs.
Similar arguments allow to prove (7.21c).
□
We now extend the inequalities of this last theorem to the Euler scheme, taking
care to have constants C which are uniform w.r.t. the discretization parameter n.

172
7
Discretization of Stochastic Differential Equations
In order to simplify computations, we introduce a continuous-time interpolation
process: for all n, we deﬁne a continuous process (Xn
t ) which takes the values Xn
pT/n
at times pT /n; speciﬁcally,
Xn
t = Xn
pT/n + b

Xn
pT/n

t −pT
n

+ σ

Xn
pT/n

(Wt −WpT/n),
(7.25a)
pT
n ≤t < (p + 1)T
n
.
Observe that
Xn
t = X0 +
 t
0
b

Xn
ηn(s)

ds +
 t
0
σ

Xn
ηn(s)

dWs
(7.25b)
for
ηn(s) := pT
n ,
pT
n ≤s < (p + 1)T
n
.
(7.25c)
Remark 7.4 The above interpolation process will allow us to apply Itô’s formula to
the error process Xt −Xn
t which, by construction, is an Itô process. In order to apply
Gronwall’s inequality, we will make appear the local error term which involves the
dynamics of the interpolation scheme between times s and ηn(s), which governs the
global convergence rate.
This technique allows us to present in an elegant and concise way the error prop-
agation of the discretization error from step to step.
The computation can also be done using only the Euler scheme dynamics: it
consists in establishing a precise induction formula relying on the approximation
error at time (p + 1)T /n in terms of the approximation error at time pT /n and
a local approximation error, and then using a discrete-time version of Gronwall’s
inequality.
We are now in a position to estimate the moments of the Euler scheme.
Theorem 7.10 Under the assumptions of Theorem 7.9, there exists C < ∞which
is uniform w.r.t. all n ≤T and satisﬁes
E
Xn
t
2m ≤

1 + E|X0|2m
eCt,
0 ≤t ≤T.
(7.26)
Proof Similarly to the beginning of the proof of Theorem 7.9 but setting bs :=
b(Xηn(s)) and σs := σ(Xηn(s)), there exists C > 0 depending on m and L only (and
not on n) such that
E
Xn
t∧τK
2m ≤E|X0|2m + C
 t
0

1 + E
Xn
ηn(s)∧τK
2m
ds.
(7.27)

7.4
Convergence Rates in Lp(Ω) Norm and Almost Surely
173
Since Xn
ηn(s) is independent of Ws −Wηn(s), using (7.23),
E
Xn
s −Xn
ηn(s)
2m ≤CE
b

Xn
ηn(s)

s −ηn(s)

+ σ

Xn
ηn(s)

(Ws −Wηn(s))
2m
≤C

1 + E
Xn
ηn(s)
2m
s −ηn(s)
2m + E(Ws −Wηn(s))2m
≤C
T
n
m
.
(7.28)
Coming back to the inequality (7.27) we deduce
E
Xn
t∧τK
2m ≤E|X0|2m + C
 t
0

1 + E
Xn
s∧τK
2m
ds + Ct
T
n
m
.
(7.29)
Recall that T /n ≤1; applying Gronwall’s inequality (Lemma 7.1) for g(t) :=
1 + E|Xn
t |2m, letting K go to inﬁnity, and ﬁnally using Fatou’s lemma allow to
conclude.
□
Similar arguments to those which allowed us to deduce (7.21b) from (7.21a)
easily allow to deduce from (7.26) the following result:
Corollary 7.1 Under the hypotheses of Theorem 7.9, there exists a positive real
number C which is uniform w.r.t. all n ≥T such that
E sup
0≤t≤T
|Xn
t |2m ≤C

1 + E|X0|2m
eCT .
(7.30)
Exercise 7.2 Consider the one-dimensional stochastic differential equation
dXt = μXt dt + σXt dWt
(7.31)
where μ and σ are non-negative real numbers and (Wt) is a standard Brownian
motion. Let Xn be the Euler scheme with step-size T /n for this equation. Show that
Xn
T can be written as the product of independent random variables. Deduce a bound
from above of E|Xn
T |2m uniform w.r.t. n. Solve the same questions for the Milstein
scheme.
7.4 Convergence Rates in Lp(Ω) Norm and Almost Surely
The objective of this section is to estimate the error
E max
0≤p≤n
XpT/n −Xn
pT/n
2m
where Xn is the result of the Euler or of the Milstein scheme. We even obtain con-
vergence rates for the interpolated processes which imply the same rates for the
discrete time schemes.

174
7
Discretization of Stochastic Differential Equations
We again emphasize that these rates do not optimally describe the actual accu-
racy of stochastic simulations. On the one hand, such simulations aim to provide
information on the laws of processes. On the other hand, pathwise error estimates
assume that the exact process is driven by the speciﬁc Brownian motion which is
constructed along the simulations, which is unreasonable (see also our discussion at
the beginning of Sect. 7.5.2).
The following theorem originally comes from Kanagawa [25] and the Ph.D. the-
sis of Faure [14].
Theorem 7.11 Suppose that the functions b and σ are Lipschitz and there exists an
integer m ≥1 such that E|X0|2m < ∞. Then there exists a function K such that, for
any integer n ≥T , the Euler scheme satisﬁes
E sup
0≤t≤T
Xt −Xn
t
2m ≤K(T )
nm .
(7.32)
The function K only depends on the Lipschitz constants of the functions b and σ, on
m, and on E|X0|2m.
Proof As we observed in the proof of (7.21b) and (7.30), it sufﬁces to prove
sup
0≤t≤T
E
Xt −Xn
t
2m
≤K(T )
nm .
(7.33)
To this end, let us set εn
t := Xt −Xn
t and note that
εn
t =
 t
0

b(Xs) −b

Xn
ηn(s)

ds +
 t
0

σ(Xs) −σ

Xn
ηn(s)

dWs
=
 t
0

b(Xs) −b

Xn
s

ds +
 t
0

σ(Xs) −σ

Xn
s

dWs
+
 t
0

b

Xn
s

−b

Xn
ηn(s)

ds +
 t
0

σ

Xn
s

−σ

Xn
ηn(s)

dWs.
(7.34)
We now proceed as in the proof of Theorem 7.10, in particular concerning (7.28)
and (7.29), and obtain:
E
εn
t
2m ≤C
 t
0
E
εn
s
2m ds + C T m
nm .
Gronwall’s inequality (Lemma 7.1) allows us to conclude.
□
Borel–Cantelli’s lemma leads to the following result.

7.4
Convergence Rates in Lp(Ω) Norm and Almost Surely
175
Theorem 7.12 Suppose that the hypotheses of Theorem 7.11 are satisﬁed. For any
m > 1 it holds that
∀0 ≤α < 1
2 −1
2m,
nα sup
0≤t≤T
Xt −Xn
t
 −→
n→∞0,
a.s.
(7.35)
In particular, if X0 has ﬁnite moments of all orders, then
∀0 ≤α < 1
2,
nα sup
0≤t≤T
Xt −Xn
t
 −→
n→∞0,
a.s.
(7.36)
Proof Set Zn := nα sup0≤t≤T |Xt −Xn
t | and observe that (7.32) and the Markov
inequality imply for any ε > 0 that

n≥T
P(Zn ≥ε) ≤K(T )
ε2m

n≥T
1
nm(1−2α) < ∞.
Borel–Cantelli’s lemma implies that P almost all ω belong to a ﬁnite number of
events {Zn ≥ε}. This implies that the sequence (Zn) almost surely converges to 0,
from which we deduce (7.35). The result (7.36) is then obvious.
□
Remark 7.5 All the convergence results in this section concern the interpolation
scheme. Therefore, they bring more information than necessary from a numerical
point of view, since it is impossible to simulate continuous time trajectories on com-
puters. The essential practical information concerns the errors
max
0≤p≤n
XpT/n −Xn
pT/n
 ≤sup
0≤t≤T
Xt −Xn
t
.
The Milstein scheme leads to convergence rates in Lp(Ω) norm and in the a.s.
sense which are better than those of the Euler scheme. We admit the next result, of
which the proof mainly consists in improving the estimate (7.28); it originally was
obtained in Milstein [36].
Theorem 7.13 Suppose that the functions b and σ are twice continuously differen-
tiable and that their partial derivatives up to order 2 are bounded. Let m ≥1 be
an integer such that E|X0|4m < ∞. The Milstein scheme satisﬁes: There exists a
non-decreasing function K such that
sup
0≤t≤T
E
Xt −Xn
t
2m ≤K(T )
n2m .
(7.37)
Moreover,
∀0 ≤α < 1
2 −1
m,
nα sup
0≤t≤T
E
Xt −Xn
t
 −→
n→∞0,
a.s.
(7.38)
Exercise 7.3 Consider the situation and the hypotheses of Exercise 7.2. Show that
the convergence rates in Theorems 7.11, 7.12, and 7.13 are optimal.

176
7
Discretization of Stochastic Differential Equations
7.5 Monte Carlo Methods for Parabolic Partial Differential
Equations
We are now in a position to develop a Monte Carlo method for the solutions of a
class of parabolic partial differential equations.
7.5.1 The Principle of the Method
Let a = (ai
j) be the function on Rd taking values in the space of d × d matrices
deﬁned as
a := σσ ∗.
Consider the second order differential operator
L :=
d

i=1
bi(·)∂i + 1
2
d

i,j=1
ai
j(·)∂2
ij
(7.39)
(the generator of the solution of the stochastic differential equation (7.5a)–(7.5b),
see Sect. 7.1.3) and the parabolic partial differential equation with terminal condi-
tion
⎧
⎨
⎩
∂u
∂t (t,x) + L u(t,x) = 0,
(t,x) ∈[0,T ) × Rd,
u(T,x) = f (x),
x ∈Rd.
(7.40)
Under the hypotheses that we will formulate below on the functions b, σ and f ,
the problem (7.40) has a unique solution which is continuous on [0,T ] × Rd and
belongs to the space C 1,2([0,T )×Rd). We will see that this unique solution satisﬁes
u(t,x) = Ef

XT −t(x)

,
∀(t,x) ∈[0,T ] × Rd,
where (Xθ(x)) is the solution to the (7.5a)–(7.5b) with initial condition X0 = x, i.e.,
Xθ(x) = x +
 θ
0
b

Xs(x)

ds +
 θ
0
σ

Xs(x)

dWs,
0 ≤θ ≤T.
(7.41)
The preceding consideration suggests the following Monte Carlo method. If
{X(i),i ∈N} is a family of independent copies of the process X, the Strong Law
of Large Numbers implies that
u(t,x) = lim
N→∞
1
N
N

i=1
f

X(i)
T −t(x)

,
a.s.
As, in general, it is impossible to obtain the exact values of the X(i)
pT/n(x), we replace
them with the approximations resulting from the Euler scheme.

7.5
Monte Carlo Methods for Parabolic Partial Differential Equations
177
Algorithm (Monte Carlo for terminal value PDE) To obtain an approximation for
the solution (u(t,x)) for (7.40): simulate N independent copies (Xn(i),1 ≤i ≤N)
of the Markov chain Xn given by the Euler scheme, and take
un,N(t,x) := 1
N
N

i=1
f

Xn(i)
T −t(x)

.
(7.42)
Why is this Monte Carlo method interesting? Does it compete with deterministic
numerical methods such as ﬁnite difference or ﬁnite elements methods?
The answers to these questions depend on the dimension d and the functions
b and σ. As emphasized in Chap. 1, numerical experiments show that a Monte
Carlo method cannot have better performances than an effective deterministic nu-
merical method. Fortunately for the specialists in probabilistic numerical methods,
situations do exist where deterministic methods are ineffective and probabilistic ap-
proaches are satisfying.
An example of such a case concerns high dimensional partial differential equa-
tions. The numerical cost of the deterministic algorithms exponentially increases
with the dimension d of the state space as the number of points in the discretiza-
tion grids which are involved in these algorithms. On the contrary, the representa-
tion (7.42) illustrates that the complexity of a probabilistic method often increases
linearly only with d.
This being so, the method (7.42) may be interesting even in small dimensions
when one desires to approximate the solution u(t,x) in one single point or in a
small number of points: this typically occurs in ﬁnancial option prices (these prices
are functions of the asset spot prices which underlie the optional contracts), and in
the numerical resolution of certain inverse problems (when one desires to calibrate
unknown parameters of the differential operator L by using measurements at points
xi which provide values u(t,xi)).
Similarly, Monte Carlo methods may be effective to compute artiﬁcial Dirichlet
conditions at a small number of points located at the boundary of a bounded domain
where one limits the numerical resolution by a deterministic method.
There exist other situations, which we do not detail here, where Monte Carlo
methods are effective: for example, when operators L are locally degenerate, de-
terministic methods need thin grids around the degeneracy manifolds in order to
preserve stability conditions, which may dramatically increase the number of dis-
cretization points; for certain McKean–Vlasov non-linear partial differential equa-
tions, interacting stochastic particles methods, which generalize Monte Carlo meth-
ods and consist in approximating the solutions by particles empirical distributions,
are effective because the particle dynamics make them naturally concentrate where
the gradients of the exact solutions are large.
7.5.2 Introduction of the Error Analysis
Deﬁne un,N(t,x) as in (7.42). Our objective is to estimate the approximation error
u(T,x) −un,N(T,x)


178
7
Discretization of Stochastic Differential Equations
in terms of T and the numerical parameters N and n. This error is decomposed into
u(T,x) −un,N(T,x)
 ≤
u(T,x) −Ef

Xn
T (x)


 !
"
α(n)
+
Ef

Xn
T (x)

−un,N(T,x)


 !
"
β(n,N)
.
(7.43)
The analysis of the statistical error β(n,N) consists in proving that the con-
stants in the Berry–Esseen and Bikelis theorems and concentration inequalities (see
Chap. 3) are bounded from above uniformly w.r.t. n: this typically results from the
estimates on the discretization error α(n) which we get below.
Let us start with a coarse grained analysis of α(n). When the function f is Lips-
chitz, the error estimate (7.32) with 2m = 1 leads to
α(n)
 ≤C
√n.
However this rate is a poor estimate, notably because E|X −Y| generically is
a bad approximation of |E(X) −E(Y)|. Note also that the error analysis of
Ef (XT (x)) −Ef (Xn
T (x)) should not suppose that XT (x) and Xn
T (x) belong to
the same probability space, which is implicitly required in (7.32).
Exercise 7.4 Consider the situation of Exercise 7.2. Let Xn be the Euler scheme
with step-size T /n. Show that E(XT ) −E(Xn
T ) and E((XT )2) −E((Xn
T )2) are of
order 1/n. Show that the Milstein scheme satisﬁes the same property.
We now show that the convergence rate 1/n is very general. We will even get
a much better result: the discretization error α(n) admits a Taylor expansion w.r.t.
1/n. We will see that this theoretical result has important numerical consequences.
A reasonable methodology to precisely estimate the error α(n) seems to be to
study the time evolution of Ef (XpT/n)−Ef (Xn
pT/n). Unfortunately this technique
is not successful, so that we need to develop another approach which reﬂects that,
at each time step, the transition operator of the Markov process X is approximated
by the transition operator of the Markov chain Xn.
The proof deeply uses the following classical result yielding a probabilistic rep-
resentation for the solution.
Theorem 7.14 (Feynman–Kac formula) Suppose that the functions b and σ are
Lipschitz and that the function f has an at most polynomial growth at inﬁnity.
Suppose also that the partial differential equation (7.40) admits a solution of class
C 1,2([0,T ]×Rd) whose partial derivatives of all orders have an at most polynomial
growth at inﬁnity. Then
u(t,x) = Ef

XT (t,x)

,
∀(t,x) ∈[0,T ] × Rd,
(7.44)
where
Xθ(t,x) = x +
 θ
t
b

Xs(t,x)

ds +
 θ
t
σ

Xs(t,x)

dWs,
0 ≤t ≤θ ≤T.
(7.45)

7.5
Monte Carlo Methods for Parabolic Partial Differential Equations
179
Similarly, given the solution (Xθ(x)) to the stochastic differential equation (7.41),
u(t,x) = Ef

XT −t(x)

,
∀(t,x) ∈[0,T ] × Rd,
(7.46)
where (Xθ(x)) is the solution of (7.41).
Proof Apply Itô’s formula (7.4) to u(s,Xs(t,x)) from time t to T . The function
u solves the partial differential equation (7.40), hence the integral w.r.t. Lebesgue
measure vanishes, and we obtain
u

T,XT (t,x)

= u(t,x) +
 T
t
∇u

s,Xs(t,x)

· σ

Xs(t,x)

dWs.
In view of Theorem 7.9 and inequality (7.22) the preceding stochastic integral is a
martingale, and taking expectations yields
u(t,x) = Eu

T,XT (t,x)

.
Equality (7.44) follows by noting that, by hypothesis, u(T,y) = f (y).
Then, (7.46) follows from the fact that the laws of (Xθ(x),0 ≤θ ≤T −t) and
(Xθ(t,x),t ≤θ ≤T ) are identical, since (Wt+s −Wt,0 ≤s ≤T −t) is a Brownian
motion, and there is uniqueness in law of the solutions to the stochastic differential
equations (7.41) and (7.45), see Theorem 7.6.
□
Remark 7.6 Various sets of hypotheses on the functions b, σ and f imply the con-
ditions imposed on u(t,x) in Theorem 7.14. Classical results, obtained by PDE
analysis techniques, require ellipticity or hypo-ellipticity conditions on the differ-
ential operator L . By using probabilistic techniques only, we will see below that
smoothness hypotheses on the functions b, σ and f also allow one to satisfy the
desired conditions on u(t,x) without any further condition on L .
Before proving the optimal convergence rate for the Euler scheme, we present a
short computation which introduces the main ingredient of the technical proof and
shows that the discretization error is of order 1/n. It relies on the following property:
the two ﬁrst moments of the increments of the Euler scheme satisfy
E

Xn
(p+1)T/n(x) −Xn
pT/n(x)

= Eb

Xn
pT/n(x)
T
n ,
(7.47a)
E

Xn
(p+1)T/n(x) −Xn
pT/n(x)

·

Xn
(p+1)T/n(x) −Xn
pT/n(x)
t
= Ea

Xn
pT/n(x)
T
n + O
 1
n2

,
(7.47b)
and moreover it is simple to similarly check that the product of three coordinates
with indices 1 ≤ℓ1,ℓ2,ℓ3 ≤d of an Xn increment

180
7
Discretization of Stochastic Differential Equations

Xn
(p+1)T/n(x) −Xn
pT/n(x)

ℓ1

Xn
(p+1)T/n(x) −Xn
pT/n(x)

ℓ2
×

Xn
(p+1)T/n(x) −Xn
pT/n(x)

ℓ3
has an expectation of order
1
n2 , and that all products of four coordinates or more
have expectations with the same order or a better one.
We now observe that (7.46) and a telescopic sum yield
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Ef

Xn
T (x)

−Ef

XT (x)

= Eu

T,Xn
T (x)

−u(0,x) =
n−1

p=0
δn
p,
δn
p := E

u

(p + 1)T /n,Xn
(p+1)T/n(x)

−u

pT /n,Xn
pT/n(x)

.
(7.48)
A Taylor expansion of u around (pT /n,Xn
pT/n(x)) and the equality ∂u
∂t + L u = 0
lead to
Ef

Xn
T (x)

−Ef

XT (x)

=
n−1

p=0
E

u

(p + 1)T /n,Xn
pT/n(x)

−u

pT /n,Xn
pT/n(x)

+ T
n
n−1

p=0
E

L u

(p + 1)T /n,Xn
pT/n(x)

+
n−1

p=0
O
 1
n2

= T
n
n−1

p=0
E

L u

pT /n,Xn
pT/n(x)

+ ∂u
∂t

pT /n,Xn
pT/n(x)

+
n−1

p=0
O
 1
n2

= O
1
n

.
Remark 7.7 To render the preceding expansion rigorous, we need to prove that the
function u is smooth enough, all the expectations in the r.h.s. terms are ﬁnite and
bounded from above uniformly w.r.t. n, etc. This is done in the next section.
7.6 Optimal Convergence Rate: The Talay–Tubaro Expansion
The goal of this section is to prove the following theorem in Talay and Tubaro [46].
Theorem 7.15 Suppose that
(H) The functions b and σ are of class C ∞(Rd), and their partial derivatives of
all orders are bounded (b and σ are not supposed bounded themselves),

7.6
Optimal Convergence Rate: The Talay–Tubaro Expansion
181
(H’) The function f is of class C ∞(Rd) and, for any multi-index α, there exists an
integer s and a positive real number K such that
∀x ∈Rd,
∂|α|
α f (x)
 ≤K

1 + |x|s
.
(7.49)
Then, the discretization error of the Euler scheme satisﬁes
Ef

XT (x)

−Ef

Xn
T (x)

= −T
n
 T
0
EΨ

s,Xs(x)

ds + Qn
T (x)
n2
(7.50)
where
Ψ (t,·)= 1
2
d

i,j=1
bibj∂2
iju(t,·)+ 1
2
d

i,j,k=1
biaj
k ∂3
ijku(t,·)+ 1
8
d

i,j,k,l=1
ai
jak
l ∂4
ijklu(t,·)
+ 1
2
∂2
∂t2 u(t,·) +
d

i=1
bi ∂
∂t ∂iu(t,·) + 1
2
d

i,j=1
ai
j
∂
∂t ∂2
iju(t,·).
(7.51)
In addition, there exists an increasing function K and an integer m such that
Qn
T (x)
 ≤K(T )

1 + |x|m
.
(7.52)
Remark 7.8 No ellipticity condition is made on the differential operator L , which
therefore can be degenerate.
The proof of the preceding theorem is technical. To help the reader we now use
the notation already used in Sects. 5.1.2 and 6.1.1.
Notation 7.16 For any measurable function φ from R2d to R with an at most poly-
nomial growth at inﬁnity, let
Exφ

Xt,Xn
t

:= Eφ

Xt(x),Xn
t (x)

.
Note that Theorems 7.9 and 7.10 imply that these expectations are well deﬁned.
Notation 7.17 Denote by (Pθ) the transition operators of the Markov process (Xt):
for any measurable function φ from Rd to R with an at most polynomial growth at
inﬁnity and θ ≥0,
Pθφ(x) := Exφ(Xθ),
x ∈Rd.
The proof of Theorem 7.15 uses the following lemma.
Lemma 7.2 The discretization error satisﬁes:
Exf

Xn
T

−Exf (XT ) = T 2
n2
n−2

p=0
ExΨ

pT /n,Xn
pT/n

+
n−1

p=0
Rn
p(x),
(7.53)

182
7
Discretization of Stochastic Differential Equations
where
Rn
n−1(x) := Exf

Xn
T

−Ex

(PT/nf )

Xn
T −T/n

and Rn
k (x) for k < n −1 is a sum of terms of the form
Ex
	
ϕ♮
α

Xn
pT/n
 (p+1)T/n
pT/n
 s1
pT/n
 s2
pT/n

ϕ♯
α

Xn
s3

∂|α|
α u

s3,Xn
s3

+ ϕ♭
α(Xs3)∂|α|
α u(s3,Xs3)

ds3 ds2 ds1

(7.54)
in which the multi-indices satisfy |α| ≤6, and the functions ϕ♮
α, ϕ♯
α, ϕ♭
α are products
of partial derivatives of order less than 3 of the functions aij and bi.
Proof For all z ∈Rd deﬁne the differential operator Lz by
Lz :=
d

i=1
bi(z)∂i + 1
2
d

i,j=1
ai
j(z)∂2
ij
which are a version of (7.39) in which the coefﬁcients are “frozen”.
Let us go back to (7.48). Itô’s formula (7.4) and (7.40) yield
δn
p = Ex
 (p+1)T/n
pT/n

∂tu

t,Xn
t

+ Lzu

t,Xn
t

z=Xn
pT/n

dt
= Ex
 (p+1)T/n
pT/n

−L u

t,Xn
t

+ Lzu

t,Xn
t

z=Xn
pT/n

dt.
Deﬁning
I n
p(t) : = Lzu

t,Xn
t

z=Xn
pT/n −Lzu

pT /n,Xn
pT/n

z=Xn
pT/n,
J n
p(t) : = Lzu

pT /n,Xn
pT/n

z=Xn
pT/n −L u

t,Xn
t

= L u

pT /n,Xn
pT/n

−L u

t,Xn
t

,
we then have
δn
p = Ex
 (p+1)T/n
pT/n

I n
p(t) + J n
p(t)

dt.
Now consider I n
k (t) and J n
k (t) as smooth functions of Xn
t and apply Itô’s formula;
observe also that ∂
∂t L u + L (L u) = 0 since ∂
∂t u + L u = 0 by (7.40). It is then a
simple matter to conclude.
□

7.6
Optimal Convergence Rate: The Talay–Tubaro Expansion
183
Proof of Theorem 7.15 The expansion (7.53) can be rewritten as
Exf

Xn
T

−Exf (XT ) = T
n
 T
0
ExΨ (s,Xs)ds
+ T 2
n2
n−2

p=0
ExΨ (pT /n,XpT/n) −T
n
 T
0
ExΨ (s,Xs)ds
+ T 2
n2
n−2

p=0
Ex

Ψ

pT /n,Xn
pT/n

−Ψ (pT /n,XpT/n)

+
n−2

p=0
Rn
p(x) + Exf

Xn
T

−Ex

(PT/nf )

Xn
T −T/n

= T
n
 T
0
ExΨ (s,Xs)ds + An + Bn +
n−2

p=0
Rn
p(x) + Cn.
(7.55)
Thus it is reasonable to expect that the error Exf (Xn
T ) −Exf (XT ) is equal to
T
n
 T
0
ExΨ (s,Xs)ds
up to a remaining term of order 1/n2 since:
• for any function φ from [0,T ] to R of class C 1, the approximation error
 T
0
φ(s)ds −T
n
n−1

p=0
φ(pT /n)
is of order 1/n (this convergence rate of Riemann sums results from a ﬁrst order
Taylor expansion),
• the remaining term
n−2

p=0
Ex

Ψ

pT /n,Xn
pT/n

−Ψ (pT /n,XpT/n)

should be uniformly bounded w.r.t. n, each term seeming to be of order 1/n.
We now rigorously prove this last assertion. For all 1 ≤p ≤n −2, use the ex-
pansion (7.55) and substitute the function
fn,p(·) := Ψ (pT /n,·)

184
7
Discretization of Stochastic Differential Equations
to f . Set un,p(t,x) := PpT/n−tfn,p(x), and let Ψn,p(t,·) be the function ob-
tained by replacing u by un,p and T by pT /n in (7.51). There exist functions
gλ ∈C ∞
b (Rd) such that, for all t ≤pT /n,
Ψn,p(t,·) =

λ
gλ∂λ

PpT/n−tΨ (pT /n,·)

.
We thus have obtained
ExΨ

pT /n,Xn
pT/n

−ExΨ (pT /n,XpT/n)
= T 2
n2
p−2

j=0
ExΨn,p
jT
n ,Xn
jT/n

+
p−1

j=0
Rn,p
j
(x)
(7.56)
where the Rn,p
j
(x) are sums of terms of the type (7.54) with un,p in place of u.
The key issue therefore is as follows. Let γ and λ be arbitrary multi-indices, and
gγ and gλ be smooth functions whose partial derivatives of all orders have an at
most polynomial growth at inﬁnity. Set
ϕ(θ,·) := gγ (·)∂γ PT −θf (·).
We have to prove that any quantity of the type
Ex

gλ

Xn
t

∂λPθ−tϕ(θ,·)(z)

z=Xn
t

(7.57)
is bounded uniformly w.r.t. n ≥1 and θ ∈[0,T −T
n ] and t ∈[0,θ −T
n ].
In Sect. 7.8 we will use a probabilistic technique to show that, for any multi-index
α, there exists an increasing function Kα and an integer mα such that
∀t ∈[0,T ], ∀x ∈Rd,
∂|α|
α u(t,x)
 ≤Kα(T )

1 + |x|mα
.
(7.58)
Now let HT denote the class of the functions φ : [0,T ] × Rd →R having the
following properties: φ is of class C ∞([0,t]×Rd) and, for any multi-index α, there
exists an integer s and an increasing function K such that
∀θ ∈[0,T ], ∀x ∈Rd,
∂|α|
α φ(θ,x)
 ≤K(T )

1 + |x|s
.
(7.59)
Let φ be a function in HT and θ be a real number in [0,T ]. The function u(θ;t,x)
deﬁned as
u(θ;t,x) := Eφ

θ,XT −t(x)

= Exφ(θ,XT −t)
belongs to HT and satisﬁes
⎧
⎨
⎩
∂u
∂t (θ;t,x) + L u(θ;t,x) = 0,
0 ≤t < T,
u(θ;T,x) = φ(θ,x).
(7.60)

7.7
Romberg–Richardson Extrapolation Methods
185
One can easily obtain inequalities similar to (7.58): for any multi-index α, there
exist an increasing function Kα and an integer mα such that
∀θ ∈[0,T ],
∂|α|
α u(θ;t,x)
 ≤Kα(T )

1 + |x|kα
.
This result allows us to claim: for any arbitrary multi-indices γ and λ, all continuous
functions f and gγ with an at most polynomial growth at inﬁnity, there exist an
increasing function K and an integer m such that
Ex

gλ

Xn
t

∂λPθ−tϕ(θ,·)(z)

z=Xn
t
 ≤K(T )

1 + |x|m
,
(7.61)
where
ϕ(θ,·) := gγ ∂γ PT −θf.
Applying this estimate to (7.55) and (7.56) yields the desired result.
□
Remark 7.9 The above convergence rate is preserved when using suitable discrete
random variables instead of the Brownian increments. Consider the scheme
⎧
⎨
⎩
Xn
0 = X0,
Xn
(p+1)T/n = Xn
pT/n + b

Xn
pT/n
T
n + σ

Xn
pT/n

T
n ξp+1,
where the random vectors ξp+1 are independent and identically distributed, have a
ﬁnite moment of 6th order, and have the same ﬁrst ﬁve moments as the increments
of W. One can easily check that all the above calculations still hold true. The only
change concerns the remainder term Qn
T (x), but the bound (7.52) remains true.
Remark 7.10 The convergence rates of the Euler scheme (7.11) and the Milstein
scheme (7.13) for Ef (XT ) are the same. The reason is that the Milstein scheme
satisﬁes (7.47a)–(7.47b) but does not improve the order of the remaining terms.
However the functions Ψ in the leading terms of the Euler and Milstein scheme
error expansions are different.
7.7 Romberg–Richardson Extrapolation Methods
The expansions (7.50) allow to justify the Romberg–Richardson extrapolation pro-
cedure. Note that
Exf (XT ) −Exf

Xn
T

= T
n e(T ) + O
 1
n2

,
Exf (XT ) −Exf

X2n
T

= T
2ne(T ) + O
 1
n2

,

186
7
Discretization of Stochastic Differential Equations
where
e(T ) := −T
n
 T
0
ExΨ (s,Xs)ds,
so that the new approximation
Zn
T := 2Exf

X2n
T

−Exf

Xn
T

(7.62)
satisﬁes
Exf (XT ) −Zn
T = O
 1
n2

.
Consequently, a convergence rate of order 1/n2 is achieved by interpolating lin-
early the values produced by the Euler scheme with two step-sizes T /n and T /2n.
This impressive gain of accuracy is thus obtained with a weak augmentation of
the numerical complexity.
7.8 Probabilistic Interpretation and Estimates for Parabolic
Partial Differential Equations
The aim of this section is to show the following theorem which has been used in the
proof of Theorem 7.15: see (7.58).
Theorem 7.18 Under the hypotheses (H) and (H’), for all 0 ≤t ≤T and x in Rd,
∂iu(t,x) = ∂iEf

XT −t(x)

= E
d

j=1
∂jf

XT −t(x)

∂iXj
T −t(x),
(7.63)
where the vector-valued process (∂iXj
θ (x),j = 1,...,d) is the unique solution in
M 2
F (0,T ) to the system
∂iXθ(x) = ei +
 θ
0
∂b

Xs(x)

∂iXs(x)ds
+
r

j=1
 θ
0
∂σj

Xs(x)

∂iXs(x)dW j
s ,
0 ≤θ ≤T.
(7.64)
Here, ei is the vector (0,...,0,1,0,...,0) with 1 in position i, ∂b is the matrix
( ∂bi
∂xj ,i = 1,...,d;j = 1,...,d), and ∂σj is the matrix (
∂σ ℓ
j
∂xm ,ℓ= 1,...,d;m =
1,...,d).
Proof We start by proving that the system (7.64) has a unique strong solution for
all x. Under our hypotheses, Theorem 7.6 applies and (7.5a)–(7.5b) has a unique

7.8
Probabilistic Interpretation and Estimates
187
strong solution. One then constructs the following Picard sequence of successive
approximations:
Y (0)
θ
= ei
and
Y (k+1)
θ
= ei +
 θ
0
∂b(Xs)Y (k)
s
ds +
r

j=1
 θ
0
∂σj(Xs)Y (k)
s
dW j
s .
As all the matrices ∂b and ∂σj have bounded coefﬁcients, it is easy to obtain
E

max
0≤s≤t
Y (k+1)
s
−Y (k)
s
2
≤C
 t
0
E
Y (k)
s
−Y (k−1)
s
2
ds.
One can then proceed as in the proof of Theorem 7.6 (see the proof of Theorem 5.2.9
in Karatzas and Shreve [26]): one gets that the sequence (Y (k)
t
) a.s. converges to a
solution to (7.64) and that such a solution is unique.
We now prove the stochastic representation (7.63). To simplify notation, we start
by examining the case d = r = 1 and write Xx
t instead of Xt(x):
Xx
t = x +
 t
0
b

Xx
s

ds +
 t
0
σ

Xx
s

dWs,
where (Wt) is a one-dimensional standard Brownian motion. We aim to prove
∂
∂x u(t,x) = E

φ′
Xx
t

Y x
t

,
(7.65)
where (Y x
t ) solves the stochastic differential equation
Y x
t = 1 +
 t
0
b′
Xx
s

Y x
s ds +
 t
0
σ ′
Xx
s

Y x
s dWs,
(7.66)
which means that Y x
t stands for ∂xXx
t .
Given ε > 0 deﬁne
Zx,ε
t
:= 1
ε

Xx+ε
t
−Xx
t

.
We are going to prove that (Zx,ε
t
) solves an equation which is similar to (7.66).
Observe that, for any function f of class C1(R),
f (y) −f (x) =
 1
0
f ′
x + α(y −x)

(y −x)dα
for all x and y in R: for this, integrate the function α →f ′(x + α(y −x))(y −x).
By explicitly expressing Xx+ε
t
and Xx
t and setting
φε
s :=
 1
0
b′
Xx
s + α

Xx+ε
s
−Xx
s

dα,

188
7
Discretization of Stochastic Differential Equations
ψε
s :=
 1
0
σ ′
Xx
s + α

Xx+ε
s
−Xx
s

dα,
we obtain
Zx,ε
t
= 1 +
 t
0
φε
s Zx,ε
s
ds +
 t
0
ψε
s Zx,ε
s
dWs.
(7.67)
Note that
 T
0 (ψε
s Zx,ε
s
)2 ds < ∞so that the stochastic integral is well deﬁned.
Itô’s formula leads to
E

Zx,ε
t
2 = 1 + E
 t
0

2φε
s +
ψε
s
2
Zx,ε
s
2 ds.
Observe that the processes (φε
t ) and (ψε
t ) are uniformly bounded w.r.t. ω, t, x and ε.
Therefore there exists a positive constant C such that
E

Zx,ε
t
2 ≤1 + C
 t
0
E

Zx,ε
s
2 ds.
Gronwall’s lemma 7.1 then implies
E

Zx,ε
t
2 ≤exp(Ct),
from which, using the deﬁnition of Zx,ε
t
, there exists C < ∞such that
E

Xx+ε
t
−Xx
t
2 ≤Cε2,
∀t ∈[0,T ].
(7.68)
Note that
Y x
t −Zx,ε
t
=
 t
0

b′
Xx
s

Y x
s −φε
s Zx,ε
s

ds +
 t
0

σ ′
Xx
s

Y x
s −ψε
s Zx,ε
s

dWs,
from which
E

Y x
t −Zx,ε
t
2 ≤2tE
 t
0

b′
Xx
s

Y x
s −φε
s Zx,ε
s
2 ds
+ 2E
 t
0

σ ′
Xx
s

Y x
s −ψε
s Zx,ε
s
2 ds.
For each one of the integrals in the r.h.s., we use the inequality

ab −a′b′2 ≤2a2
b −b′2 + 2b′2
a −a′2

7.8
Probabilistic Interpretation and Estimates
189
which results from ab −a′b′ = a(b −b′) + b′(a −a′). We thus obtain that there
exists C < ∞such that
E

Y x
t −Zx,ε
t
2 ≤CE
 t
0

b′
Xx
s

−φε
s
2
Y x
s
2 ds +CE
 t
0

σ ′
Xx
s

−ψε
s
2
Y x
s
2 ds
+ CE
 T
0

Y x
s −Zx,ε
s
2 ds,
∀t ∈[0,T ].
We then again apply Gronwall’s lemma 7.1: it shows that, if the two ﬁrst terms in
the right-hand side of the preceding equality tend to 0, then
lim
ε→0E

Y x
t −Zx,ε
t
2 = 0.
(7.69)
To study the convergence of the two ﬁrst terms under consideration, observe that
Xx+ε
t
converges in probability to Xx
t (this is a consequence of (7.68)), and thus

b′
Xx
s

−φε
s
2 +

σ ′
Xx
s

−ψε
s
2
converges in probability to 0. Thus (7.69) results from Lebesgue’s Dominated Con-
vergence theorem.
Now, for any bounded function φ of class C1(R) with bounded derivative
1
ε

φ

Xx+ε
t

−φ

Xx
t

= Zx,ε
t
 1
0
φ′
Xx
t + αεZx,ε
t

dα
=

Zx,ε
t
−Y x
t
 1
0
φ′
Xx
t + αεZx,ε
t

dα
+ Y x
t
 1
0

φ′
Xx
t + αεZx,ε
t

−φ′
Xx
t

dα + φ′
Xx
t

Y x
t
=: A + B + φ′
Xx
t

Y x
t .
In view of (7.69), the Cauchy–Schwarz inequality implies that E|A| tends to 0
with ε. In addition, again using (7.69), one can show that εZx,ε
t
converges in prob-
ability to 0; the Dominated Convergence theorem then implies that E|B| tends to 0.
Consequently,
E1
ε

φ

Xx+ε
t

−φ

Xx
t

converges when ε tends to 0, and the limit is E[φ′(Xx
t )Y x
t ].
In arbitrary dimension, under the hypotheses (H) and (H’), the same technique as
above allows to obtain (7.63). In addition, from the hypotheses (H) and (H’), (7.63)
and Theorem 7.9, there exist an increasing function K and an integer m such that
∂iu(t,x)
 ≤K(T )

1 + |x|m
.
□

190
7
Discretization of Stochastic Differential Equations
Remark 7.11 From (7.69) it is easy to prove
∀T > 0,
lim
ε→0E sup
0≤t≤T

Y x
t −Zx,ε
t
2 = 0.
(7.70)
Exercise 7.5 In the case d = r = 1, set
Ux
t :=
 t
0

b′
Xx
s

−1
2σ ′
Xx
s
2

ds +
 t
0
σ ′
Xx
s

dWs,
and apply Itô’s formula to exp(Ux
t ). Deduce that (7.66) has solution
Y x
t := exp

Ux
t

= exp
	 t
0

b′
Xx
s

−1
2σ ′
Xx
s
2

ds +
 t
0
σ ′
Xx
s

dWs

.
Observe that (7.64) can be intuited by formally differentiating all the terms
in (7.41) and permuting differentiation and integrals. One can iterate the procedure
to intuit the systems solved by differentials of order larger than 2, which leads to
the probabilistic interpretation of ∂|α|
α u(t,x) for any multi-index α. In addition, this
allows one to show by induction that, for any multi-index α,
∀t ≤T, ∀x ∈Rd,
∂|α|
α u(t,x)
 ≤Kα(T )

1 + |x|mα
(7.71)
for some increasing function Kα and integer mα.
The preceding construction of ∂iXt(x) and its extension to ∂ijXt(x) allow one
to prove that the function Ef (Xx
T −t) is a smooth solution of the partial differential
equation (7.40), which closes the statement of Theorem 7.14. The proof is not so
easy. A key argument in the proof is the stochastic ﬂow property of the map x →
Xt(x) when the coefﬁcients b and σj are Lipschitz, that is, there exists a solution
such that, up to an event with null probability which does not depend on x in Rd and
0 ≤s ≤t ≤T , this map is continuous at every point x and satisﬁes the stochastic
ﬂow property
P-a.s.,
∀x ∈Rd, ∀0 ≤s ≤t ≤T,
Xt−s ◦Xs(x) = Xt(x).
If, in addition, the coefﬁcients b and σj are differentiable with bounded deriva-
tives then, up to an event with null probability which does not depend on x in Rd
and t in [0,T ], the map x →Xt(x) is differentiable at all point x and its almost sure
partial derivatives solve the inﬁnite set of (7.64) parametered by the initial condi-
tion x. Of course, these derivatives may (and should) be chosen instead of any other
family of solutions ∂iXt(x) (deﬁned for each ﬁxed x on an event of full probability
which depends on x) which gives sense to (7.63). However, we emphasize the sub-
stantial difference between the study of stochastic ﬂows and the way we followed
to get (7.63): this latter approach, which is much more limited but sufﬁces to prove
the differentiability w.r.t. x of the function Ef (XT −t(x)) and obtain its stochastic
representation, avoids the delicate question of proving that the stochastic ﬂow Xt(x)

7.9
Problems
191
is almost surely differentiable w.r.t. x at all points x and at any time t, the critical
null event being allowed to be chosen independent of t and x.
We refer to Kunita [30] for the demanding proofs of the almost sure ﬂow property
and differentiability of the map x →Xt(x), and the derivation of a stochastic proof
of the existence of a smooth solution to the parabolic problem (7.40).
7.9 Problems
7.1 (Long Time Behavior of the Euler Scheme Error) Let μ and σ be two strictly
positive real numbers, and
St = 1 + μ
 t
0
Sθ dθ + σ
 t
0
Sθ dWθ,
t ≥0.
(7.72)
1. Write explicitly E(St)2.
2. (a) Discretize the (7.72) by the Euler scheme with step-size T /n.
Let ( ¯Sn
pT/n) denote the corresponding Markov chain.
(b) Represent ¯Sn
T as a product of terms.
(c) Compute the exact value of E( ¯Sn
T )2.
(d) Compute a function C1(T ) such that
E(ST )2 −E
 ¯Sn
T
2 = C1(T )
n
+ O
 1
n2

.
(e) Prove that |C1(T )| tends to inﬁnity with T .
3. Now consider
rt = −μ
 t
0
rθ dθ + σWt,
t ≥0.
(a) Compute the exact value of the second moments of rpT/n and of the Euler
scheme ¯rn
pT/n.
(b) Deduce the value of the approximation error in terms of n and p.
(c) Prove that, for all n large enough, this error satisﬁes
∃C > 0,
E(rpT/n)2 −E

¯rn
pT/n
2 ≤C
n ,
∀p.
(d) Compare with the situation in Question 2.
7.2 (Implicit Euler Scheme) Consider the stochastic differential equation
Zt = −
 t
0
Z3
s ds + Wt,
t ≥0.

192
7
Discretization of Stochastic Differential Equations
Although the drift coefﬁcient is not Lipschitz, one can prove the existence and
uniqueness of the solution to this equation. Admit this result.
1. (a) Let τN be the ﬁrst exit time of (Zt) from the interval [−N,N], for N ≥1.
Apply Itô’s formula to (Zt∧τN )2.
(b) Deduce that
E|Zt|2 + 2E
 t
0
|Zs|4 ds ≤t,
∀t.
(c) Come back to the equality obtained from Itô’s formula and conclude that
E|Zt|2 + 2
 t
0
E|Zs|4 ds = t,
∀t.
2. (a) Prove that
z4 ≥z2 −1
4,
∀z ∈R.
(b) Deduce a bound from above for d
dt E|Zt|2.
(c) Finally, differentiating exp(t)E|Zt|2, prove that
E|Zt|2 ≤3
2,
∀t ≥0.
3. In the following h > 0, and for instance can be of the form h = T /n.
(a) Write the Euler scheme ( ¯Zh
(p+1)h).
(b) Prove that
E
 ¯Zh
(p+1)h
2 = E
 ¯Zh
ph
2 + hE
 ¯Zh
ph
6h −2
 ¯Zh
ph
4 + 1

.
4. (a) Prove that there exists ε > 0 such that if h large enough then hz3 −2z2 +1 >
ε > 0 for all z > 0.
(b) Deduce that, in this case, there exists no uniform bound w.r.t. p for the error
E
Zph
2 −E
 ¯Zh
ph
2.
5. Consider the implicit Euler scheme
˜Zh
(p+1)h = ˜Zh
ph −
 ˜Zh
(p+1)h
3h + W(p+1)h −Wph.
(a) Prove by induction on p that this sequence of random variables is well de-
ﬁned, and that all the moments of ˜Zh
ph exist for all p and h.
(b) Let xp := E| ˜Zh
ph|2. Show that
xp+1 + 2hx2
p+1 ≤xp + h.

7.9
Problems
193
(c) Let q be smallest integer such that xq >
√
2/2. Prove that
xq+1 ≤
√
2
2 + h,
and deduce that
E
 ˜Zh
ph
2 ≤
√
2
2 + h,
∀p ∈N, ∀h ∈R+.
(d) To what property of the implicit Euler scheme can we conclude?
7.3 (Monotonicity and Convexity of x →Xt(x) (⋆)) Let be given a ﬁltered proba-
bility space (Ω,F,P,(Ft)) and a standard Brownian motion (Wt) on this space.
Let b and σ be bounded functions of class C 1(R) with bounded derivatives. Fix
T > 0. Let (Xx
t ) be the one-dimensional process solution to
Xt = x +
 t
0
b(Xθ)dθ +
 t
0
σ(Xθ)dWθ,
0 ≤t ≤T.
(7.73)
1. Justify that, for all x and t, the derivative w.r.t. x of the map x →Xt(x) almost
surely exists and satisﬁes the (7.66).
Hint: Approximate the left-hand side by 1
ε(Xt(x + ε) −Xt(x)).
Denote this derivative by
d
dx Xt(x) and admit that there exists a process
(Xt(x)) solution to (7.73) such that, except on a null event which does not depend
on t in [0,T ] and x in R, the map x →Xt(x) is differentiable and its derivative
is equal to d
dx Xt(x). Using Exercise 7.5, show that, for all 0 ≤t ≤T ,
d
dx Xt(x) = Y x
t
= exp
 t
0

b′
Xθ(x)

−1
2

σ ′
Xθ(x)
2

dθ +
 t
0
σ ′
Xθ(x)

dWθ

.
Deduce that the map x →Xt(x) is almost surely a one-to-one map of x in R.
2. Is the ﬂow ¯Xn
kT/n(x) of the Euler scheme almost surely a one-to-one map of x?
3. From now on we suppose that b and σ are bounded functions of class C 2(R)
with bounded ﬁrst and second derivatives. Explicitly state the second derivative
w.r.t. x of the ﬂow Xt(x).
4. Let f be a twice continuously differentiable function from R to R such that
∃C > 0, ∃m ∈N,
f ′′(x)
 ≤C

1 + |x|2m
.
Show that |f ′| and |f | can also be bounded from above by polynomial functions.
5. Set u(t,x) := Ef (Xt(x)). Show that, for all t, the second derivative w.r.t. x of
u(t,x) can be bounded from above by a polynomial function.
Hint: Apply Theorem 7.18 and use the explicit formula for the second deriva-
tive of the ﬂow Xt(x). To solve integrability issues, use the localization technique

194
7
Discretization of Stochastic Differential Equations
with stopping times which has been introduced in this chapter, and get the fol-
lowing inequality for any adapted bounded process (φt):
∃C > 0, ∀0 ≤t ≤T,
Eexp
 t
0
φθ dWθ

≤C.
6. Now suppose that b is a convex function and σ is a linear function. Deduce from
the three preceding questions that, if f is convex and increasing, then, for all t,
u(t,x) is a convex function of x.
7.4 (Estimates for the Derivatives of x →Xt(x) (⋆)) Let be given a ﬁltered proba-
bility space and a standard one-dimensional Brownian motion (Wt).
1. Let (Mt) be a continuous martingale. Show that, for all t > 0 and each positive
increasing function g(t),
max
θ≤t (Mθ) −g(t) ≤max
0≤θ≤t

Mθ −g(θ)

.
2. Now ﬁx Mt :=
 t
0 φsdWs, where (φs) is an adapted process satisfying
∃c > 0,
sup
t≥0
|φt|2 ≤c
P-a.s.
Show that, for all real numbers α,
˜Mt := exp

αMt −α2
2
 t
0
(φs)2ds

is a square integrable martingale.
3. Show the following Doob’s inequality: for all continuous martingales (Nt),
∀λ > 0,
P

max
0≤θ≤t |Nθ| > λ

≤1
λE

|Nt|

.
Using now Question 1 and the deﬁnition of the martingale ( ˜Mt), show that,
for all a > 0 and all α > 0,
P

max
0≤θ≤t Mθ ≥at

≤exp

−αat + α2
2 ct

.
Deduce that
P

max
0≤θ≤t Mθ ≥at

≤exp

−a2t
2c

,
and then
∀y > 0,
P

|Mt| ≥y

≤2exp

−y2
2ct

.

7.9
Problems
195
Show that, for all ε small enough,
Eexp

ε(Mt)2
2

< ∞.
Hint: For all integer k, ﬁnd a suitable upper bound for
E
	
exp

ε(Mt)2
2

1k≤|Mt|<(k+1)

.
4. We now consider the framework and assumptions of Question 1 in Problem 7.3.
We again set Y x
t := ∂
∂x Xt(x). Deduce from Question 3 that, for all T > 0, there
exists a positive constant CT such that, for all 0 < t ≤T and all y > 0 large
enough,
P

Y x
t > y

≤2exp

−(log(y) −μt)2
2CT t

.
5. Another application of Question 3 is the following. Let ψ(x) be a Borel function
such that
∃m ∈N, ∃C > 0, ∀x ∈R,
ψ(x)
 ≤C

1 + |x|2m
.
Use Question 3 to show that E|ψ(Xx
T )Y x
T |3 is ﬁnite.
Remark In view of Sect. 3.4 this result is a ﬁrst step to estimate the number N of
Monte Carlo approximations of E(ψ(Xx
T )Y x
T ) to obtain an accuracy of order ε with
conﬁdence interval 1−δ (0 < δ < 1). In view of (7.65), this issue appears when one
develops a Monte Carlo method to approximate ∂u
∂x (t,x).

Part III
Variance Reduction, Girsanov’s Theorem,
and Stochastic Algorithms
Variance reduction techniques are developed for the use of Monte Carlo methods
involving Itô stochastic differential equations and used for the approximation of
solutions of certain parabolic partial differential equations.
First a control variate method is described on a simple method. Then an applica-
tion to sensitivity analysis is given. Lastly, an importance sampling method based
on a well-chosen class of Girsanov transforms is developed, the optimal transform
being determined by a stochastic algorithm.
The fact that the stochastic algorithm may be implemented simultaneously to
the Monte Carlo method, using the same samples, is then explained. The complete
algorithm provides us with an example of an adaptive Monte Carlo method.
A simple framework is then developed in order to provide a rigorous study of a
class of stochastic algorithms.

Chapter 8
Variance Reduction and Stochastic Differential
Equations
Abstract This chapter deepens the variance reduction subject, and focuses on the
Monte Carlo methods for deterministic parabolic partial differential equations. This
topic requires advanced notions in stochastic calculus, particularly the Girsanov the-
orem, which we state and discuss ﬁrst. We strongly emphasize that universal tech-
niques do not exist: most often, effective variance reduction methods depend on the
numerical analyst’s knowledge and experience. We will see that it is rather easy to
construct perfect variance reduction methods which are irrelevant from a numeri-
cal point of view; a contrario, the construction of an effective method often lies on
the approximation of a perfect method, the approximation method needing to be
adapted to each particular case. Interesting examples can be found in Dufﬁe and
Glynn (Ann. Appl. Probab. 5(4), 897–905, 1995).
8.1 Preliminary Reminders on the Girsanov Theorem
In this section we present and comment on the Girsanov theorem in the restrictive
framework of stochastic differential equations drift changes, see, e.g., Karatzas and
Shreve [26, Chap. 3.5]. This will be used later for variance reduction.
Theorem 8.1 (Girsanov Theorem) Consider the stochastic differential equation
in Rd, with Lipschitz coefﬁcients, driven by a d-dimensional standard Brownian
motion,
Xt = X0 +
 t
0
b(Xs)ds +
 t
0
σ(Xs)dWs.
Given T > 0, let (θs)0≤s≤T be an Rd valued measurable adapted process such that
 T
0
|θs|2 ds < ∞,
a.s.,
(8.1)
and
Eexp
 T
0
θs · dWs −1
2
 T
0
|θs|2 ds

= 1.
(8.2)
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1_8, © Springer-Verlag Berlin Heidelberg 2013
199

200
8
Variance Reduction and Stochastic Differential Equations
The probability measure Pθ on (Ω,FT ) deﬁned by its Radon–Nikodym derivative
w.r.t. P
dPθ
dP := exp
 T
0
θs · dWs −1
2
 T
0
|θs|2 ds

is equivalent to P. In addition, the process
W θ
t := Wt −
 t
0
θs ds,
0 ≤t ≤T,
is an (Ω,FT ,Pθ) Brownian motion and, Pθ-a.s., for 0 ≤t ≤T ,
Xt = X0 +
 t
0

b(Xs) + σ(Xs)θs

ds +
 t
0
σ(Xs)dW θ
s .
(8.3)
Remark 8.1 The equality (8.2) is implied by the Novikov condition
Eexp
1
2
 T
0
|θs|2 ds

< ∞.
The change of the reference probability measure P to Pθ is called a Girsanov
transform.
The Girsanov theorem has the following important consequence: for any Borel
functional Ψ on the space of the continuous functions from [0,T ] to Rd such that
E|Ψ (X·)| < ∞,
EΨ (X·) = Eθ

Ψ (X·) dP
dPθ

where Eθ stands for the expectation computed under the probability measure Pθ.
Equivalently,
EΨ (X·) = Eθ

Ψ (X·)exp

−
 T
0
θs · dWs + 1
2
 T
0
|θs|2 ds

= Eθ

Ψ (X·)exp

−
 T
0
θs · dW θ
s −1
2
 T
0
|θs|2 ds

.
(8.4)
8.2 Control Variates Method
As in Chap. 7 we consider the second order differential operator
L :=
d

i=1
bi(·)∂i + 1
2
d

i,j=1
ai
j(·)∂ij,
(8.5)

8.2
Control Variates Method
201
and the parabolic partial differential equation
⎧
⎨
⎩
∂u
∂t (t,x) + L u(t,x) = 0,
(t,x) ∈[0,T ) × Rd,
u(T,x) = f (x),
x ∈Rd.
(8.6)
We recall (7.46): under suitable hypotheses,
u(t,x) = Ef

XT −t(x)

,
where
Xt(x) = x +
 t
0
b

Xs(x)

ds +
 t
0
σ

Xs(x)

dWs,
0 ≤t ≤T.
(8.7)
In particular, one has u(0,x) = Ef (XT (x)). Consequently, denoting by (∂u)(t,x)
the vector with coordinates ∂iu(t,x),
u

T,XT (x)

= f

XT (x)

= Ef

XT (x)

+
 T
0
(∂u)

t,Xt(x)

·σ

Xt(x)

dWt, a.s.
Set
Z := f

XT (x)

−
 T
0
(∂u)

t,Xt(x)

· σ

Xt(x)

dWt.
Obviously Z is a zero bias estimator of Ef (XT (x)) with null variance. The Monte
Carlo method consisting in simulating Z would then have a null error. Of course,
this remark is of no interest since the function u(t,x) is unknown: else, there would
be no need to develop a Monte Carlo method to approximate Ef (XT (x))!
Nevertheless, suppose that we can get an explicit approximation ¯v of ∂u. Then it
is natural to choose ¯v(t,x) as a control variable and to simulate the random variable
¯Z := f

XT (x)

−
 T
0
¯v

t,Xt(x)

· σ

Xt(x)

dWt.
This random variable is a zero bias estimator of Ef (XT (x)). This property is sub-
ject to the fact that the stochastic integral in the deﬁnition of Z is a martingale.
Reinforcing the integrability hypotheses, one can easily check that the Monte Carlo
method variance now is different to the variance of f (XT (x)): one has
E
 ¯Z −Ef

XT (x)
2
= E
f

XT (x)

−
 T
0
¯v

t,Xt(x)

σ

Xt(x)

dWt −Ef

XT (x)

2
= E
 T
0

(∂u)

t,Xt(x)

−¯v

t,Xt(x)

· σ

Xt(x)
2 dt.

202
8
Variance Reduction and Stochastic Differential Equations
Thus the new variance is small when ¯v is a good approximation of the function
u in the sense that the r.h.s. of the preceding equality is small. In that case, one
approximates the quantity
 T
0
(∂u)

t,Xt(x)

σ

Xt(x)

dWt
by the sum
n−1

p=0
¯v

pT /n,Xn
pT/n(x)

σ

Xn
pT/n(x)

(W(p+1)T/n −WpT/n).
The main difﬁculty in this approach introduced by Newton [39] consists in choos-
ing a suitable function ¯v. There does not exist a universal methodology to achieve
this objective. An effective way may be to choose a diffusion process ( ¯Xt) whose
probability distribution at time T is explicitly known and is close to the law of XT .
For example, consider the computation of the price of a European option in a
stochastic volatility model with stochastic interest rate
dSt = rtSt dt + σtSt dWt,
where the pair (rt,σt) solves a stochastic differential equation of the type
*
drt = βr(rt)dt + γr(rt)dBr
t ,
dσt = βσ(σt)dt + γσ(σt)dBσ
t .
In this model, the process (Wt,Br
t ,Bσ
t ) is a Brownian motion whose components
are weakly correlated, and the coefﬁcients βr, γr, βσ , γσ are such that rt is close to
an afﬁne model for short-term interest rates and is independent of (St), so that (σt)
can be interpreted as a ﬂuctuation process around a constant and deterministic value.
It then is reasonable to choose ¯v(t,St) as the product of the price of a zero coupon
bond in the afﬁne model under consideration and the price of the option in the Black
and Scholes model.
8.3 Variance Reduction for Sensitivity Analysis
This section explains how to compute both the solution and some of its partial
derivatives for some of its parameters in an effective way.
8.3.1 Differentiable Terminal Conditions
Again consider the partial differential equation (8.6). It often occurs that one desires
to compute both the solution and some of its partial derivatives or its sensitivity to

8.3
Variance Reduction for Sensitivity Analysis
203
model parameters. For example, in ﬁnancial mathematics, the spatial derivative of
the solution provides the hedging strategy for the option.
A naive Monte Carlo method consists in using a ﬁnite difference approximation
of the derivative under consideration, that is,
∂iu(t,x) ≃u(t,x + εei) −u(t,x)
ε
,
where ei is the vector (0,...,0,1,0,...,0) with 1 at position i, and then in ap-
proximating each term in the numerator by means of a Monte Carlo method. This
algorithm is numerically unstable and is rather ineffective: as ε needs to be chosen
small to get a good approximation of the derivative, one needs to approximate the
numerator with a high accuracy and therefore to use large numbers of simulations
and very small time discretization steps.
To overcome this difﬁculty and reduce the variance, one can use the same sam-
ples of the Brownian increments to approximate u(t,x + εei) and u(t,x). This is
equivalent to perturbing the initial condition of each path of the Euler scheme. It
often is more effective to start with Theorem 7.18:
∂iu(t,x) = ∂iEf

XT −t(x)

= E
d

j=1
∂jf

XT −t(x)

∂iXj
T −t(x).
(8.8)
We recall that, in this equality, the vector ∂iXθ(x) satisﬁes
∂iXθ(x) = ei +
 θ
0
∂b

Xs(x)

∂iXs(x)ds +
r

j=1
 θ
0
∂σj

Xs(x)

∂iXs(x)dW j
s .
(8.9)
Consequently, the partial derivatives of u(t,x) can be approximated by means of a
Monte Carlo method and the simulation of the Euler scheme which discretizes (8.7)
and the linear system
Yt(x) = Id+
 t
0
∂b

Xs(x)

Ys(x)ds +
r

ℓ=1
∂σℓ

Xs(x)

Ys(x)dW ℓ
s
with matrix valued solution

Yt(x)
j
i = ∂jXi
t(x).
Remark 8.2 In the above we have differentiated u(t,x) w.r.t. x. One can also differ-
entiate u(t,x) w.r.t. parameters in the coefﬁcients b and σ: if these coefﬁcients de-
pend smoothly on their parameters, one can extend the equalities (8.8) and (8.9) by
replacing ∂i by the derivation operator w.r.t. the parameter under consideration. This
observation allows one, e.g., to develop Monte Carlo methods to compute Greeks in
Financial Mathematics.

204
8
Variance Reduction and Stochastic Differential Equations
8.3.2 Non-differentiable Terminal Conditions
The methodology presented in the preceding subsection supposes that the function
f is everywhere differentiable. How to extend it to the case where f is, e.g., piece-
wise smooth and has a ﬁnite number of discontinuities? It often is ineffective to
approximate it by a smooth function: for example, consider the smooth approxima-
tion gε ∗f obtained by convoluting f and the Gaussian density gε with zero mean
and variance ε; the derivatives of gε ∗f take large values in the neighborhoods of
the discontinuity points of f (as expected since, in the limit ε = 0, the derivatives
at these points are inﬁnite). To circumvent this difﬁculty, one can use a kind of in-
tegration by parts which extends the following observation: let X be a real-valued
random variable with an everywhere differentiable and strictly positive density p;
for any function ϕ of class C 1(R) such that all the integrals below are well deﬁned,
Eϕ′(X) =

R
ϕ′(x)p(x)dx = −

R
ϕ(x)p′(x)dx = −

R
ϕ(x)p′(x)
p(x) p(x)dx
= −E
	
ϕ(X)p′(X)
p(X)

.
Consequently, if ϕ′ locally takes large values, rather than using a Monte Carlo
method to approximate Eϕ′(X), it may be effective to reduce the variance by us-
ing a Monte Carlo method to approximate E[ϕ(X) p′(X)
p(X) ].
Let us now come back to the case of the stochastic differential equations. To
simplify the presentation, let d = 1.
Theorem 8.2 Let the coefﬁcients b and σ be Lipschitz functions, and σ satisfy
∃C1 > 0, ∃C2 > 0,
C1 ≤σ(x) ≤C2
for all x ∈R.
Let f be a differentiable function with derivative f ′ having an at most polynomial
growth at inﬁnity. Then
E

f ′(XT )YT

= E

f (XT ) 1
T
 T
0
Ys
σ(Xs) dWs

,
where
Yt = 1 +
 t
0
b′(Xs)Ys ds + σ ′(Xs)Ys dWs,
0 ≤t ≤T.
(8.10)
Proof Let (Ft) be the Brownian motion (Wt) ﬁltration. Let (θs) be an (Ft) adapted
process which satisﬁes the condition (8.1) and such that, for all 0 ≤ε ≤1,
Eexp

ε
 T
0
θs · dWs −ε2
2
 T
0
|θs|2 ds

= 1.

8.3
Variance Reduction for Sensitivity Analysis
205
For any ε > 0 deﬁne a new probability measure on (Ω,FT ) by
dPε
dP := exp

ε
 T
0
θs · dWs −ε2
2
 T
0
|θs|2 ds

.
Setting
W ε
t := Wt −ε
 t
0
θs ds,
the equality (8.4) becomes (Girsanov transform)
Ef (XT ) = Eε

f (XT )exp

−ε
 T
0
θs dW ε
s −ε2
2
 T
0
|θs|2 ds

.
(8.11)
By deﬁnition, the joint probability distribution under Pε of the process (W ε
t ,Xt) is
identical to the joint probability distribution under P of the process (Wt,Xε
t ), where
Xε
t = X0 +
 t
0

b

Xε
s

+ εσ

Xε
s

θs

ds +
 t
0
σ

Xε
s

dWs.
(8.12)
Therefore
Ef (XT ) = E

f

Xε
T

exp

−ε
 T
0
θs dWs −ε2
2
 T
0
|θs|2 ds

.
(8.13)
Observe that the r.h.s. does not depend on ε. Consequently the derivative w.r.t. ε of
the l.h.s. is null. This derivative can be made explicit by using Remark 8.2. One thus
obtains
0 = E

f ′
Xε
T

∂εXε
T exp

−ε
 T
0
θs dWs −ε2
2
 T
0
|θs|2 ds

+ E

f

Xε
T

ξε
T

,
(8.14)
where (see the ﬁrst question of Problem 7.3 for a similar calculation)
∂εXε
t =
 t
0

b′(Xs)+εσ ′
Xε
s

θs

∂εXε
s ds+
 t
0
σ

Xε
s

θs ds+
 t
0
σ ′
Xε
s

∂εXε
s dWs,
and
ξε
T =

−
 T
0
θs dWs −ε
 T
0
|θs|2 ds

exp

−ε
 T
0
θs dWs −ε2
2
 T
0
|θs|2 ds

.
When ε tends to 0, classical results on the convergence of stochastic integrals (see
Sect. 7.1.1) imply that the process (Xε
t ) converges in probability to (Xε
t ), and the
random variable ξε
T converges in probability to −
 T
0 θs dWs. In addition, the process
(∂εXε
t ) converges in probability to the solution (Ut) of
Ut =
 t
0

b′(Xs)Us + σ(Xs)θs

ds +
 t
0
σ ′(Xs)Us dWs.

206
8
Variance Reduction and Stochastic Differential Equations
Itô’s formula shows that
UT = YT
 T
0
θsσ(Xs)
Ys
ds,
where (Yt) is the process deﬁned in (8.10).
In conclusion, in view of (8.14), we have obtained that
0 = E

f ′(XT )YT
 T
0
θsσ(Xs)
Ys
ds −f (XT )
 T
0
θs dWs

.
It then remains to choose
θs :=
Ys
σ(Xs)
to obtain the desired result.
□
Exercise 8.1 Identify situations where the process (θs) satisﬁes the hypothe-
ses (8.1) and (8.2).
Remark 8.3 Multi-dimensional extensions of the integration by parts formula in
Theorem 8.2 can be proved. However the resulting formula is more complex than
in the preceding one-dimensional context and, in full generality, its proof is based
on Malliavin calculus which is far beyond the scope of this monograph. For a sum-
mary, examples and comments, see Fournié et al. [17, 18]. A classical textbook on
Malliavin calculus is Nualart [42].
8.4 Importance Sampling Method
In this section we develop applications of the Girsanov theorem to variance reduc-
tion. To simplify the notation, let d = 1 and
M := Ef (XT ).
We aim to approximate M using a Monte Carlo method. Under probability P, the
variance of the simulation is
E

f (XT )
2
−M2.
Let us show that, under suitable hypotheses on the function u(t,x), in theory
one can construct a process (θ∗
t ) and the corresponding Girsanov transform which
lead one to a perfect importance sampling variance reduction method. Actually,
suppose that u(t,x) is bounded from below by a strictly positive number for all t
(in particular, f is supposed to satisfy this property). Then, for all (θt),
Ef (XT ) = Eθ
	
f (XT )Ef (XT )
f (XT )

.

8.4
Importance Sampling Method
207
This equality suggests to look for a process (θt) such that the random variable
Ef (XT )
f (XT ) deﬁnes a Girsanov transform with Radon–Nikodym density
dP
dPθ . As seen
in theorem 7.14 we have
u(0,X0) = Eu(T,XT ) = Ef (XT ).
We thus also have
Ef (XT )
f (XT ) = exp

logu(0,x0) −logu(T,XT )

,
from which, applying Itô’s formula to logu(t,Xt) between times 0 and T , us-
ing (8.6) and choosing
θt := σ(Xt) ∂
∂x u(t,Xt)
u(t,Xt)
,
we get
Ef (XT )
f (XT ) = exp

−
 T
0
θs dWs + 1
2
 T
0
|θs|2 ds

.
In view of (8.4), our choice for (θt) deﬁnes a proper Girsanov transform if the con-
ditions (8.1) and (8.2) are satisﬁed. The variance of the simulation under Pθ then
is
Eθ

f (XT ) dP
dPθ
2
−M2 = E

f (XT )
2 dP
dPθ

−M2
= E

f (XT )
2 Ef (XT )
f (XT )

−M2 = 0.
We thus have constructed a perfect variance reduction. Unfortunately, it requires to
know the function u(t,x) which we aim to approximate...
We thus now limit ourselves to a class of processes (θt) which can be simulated.
Under any probability measure Pθ, the variance of the simulation is
Eθ

f (XT )
2 exp

−2
 T
0
θs · dW θ
s −
 T
0
|θs|2 ds

−M2,
where (Xt) is the solution to (8.3). Observe that the expression of
dP
dPθ and the
deﬁnition of W θ imply that this variance is also equal to
E

f (XT )
2 exp

−
 T
0
θs · dWs + 1
2
 T
0
|θs|2 ds

−M2.
Therefore our objective is to seek a process (θt) such that
E

f (XT )
2 exp

−
 T
0
θs · dWs + 1
2
 T
0
|θs|2 ds

≪E

f (XT )
2
.

208
8
Variance Reduction and Stochastic Differential Equations
We formulate this issue as the following optimization problem:
min
(θt)∈A E

f (XT )
2 exp

−
 T
0
θs · dWs + 1
2
 T
0
|θs|2 ds

,
where A is a class of adapted processes satisfying suitable integrability conditions.
Obviously this problem is impossible to solve in practice if the class A is too large.
One thus needs to ﬁnd a good compromise between restricting the class A in order
to be able to compute a minimum (θt) and choosing A large enough to obtain an
effective variance reduction.
In Chap. 3, within the framework of the Black and Scholes model, we have pre-
sented a variance reduction method based on constant processes (θt), see (3.17). We
may extend this approach to more general models and to non-constant deterministic
functions (θt): the time step T /n being ﬁxed, the Euler scheme may be considered
as a deterministic (and complex!) function Fn of the initial condition X0 and the
vector of the Brownian increments
ΔW := (W(p+1)T/n −WpT/n, p = 0,...,n −1).
Therefore, extending the methodology introduced in Chap. 3 we are led to deter-
mine the piecewise constant function
Θ := (θ(p−1)T/n, p = 1,...,n)
which minimizes
H(Θ) := E
	
f ◦Fn(ΔW)
2 exp

−Θ · ΔW + T
2n∥Θ∥2

.
In the next chapter we will see stochastic algorithms which provide the optimal
value of Θ. We will also see how to modify the Monte Carlo method to simultane-
ously optimize the choice of Θ and approximate M.
A rigorous analysis shows that the exponential term in the expression of H(Θ)
imposes that these algorithms include a rather complex projection procedure at each
time step in order to converge to the desired optimal value.
In order to deﬁne a low complexity algorithm, we can reduce our ambition to
compute sub-optimal values Θ. To this end, we observe that the preceding optimiza-
tion problem is equivalent to minimizing a particular entropic distance between P
and Pθ, namely the distance deﬁned on the set of probability measures as
R2(P,Q) =
*
1
2
  dP
dQ
2 −1

dQ
if P is absolutely continuous w.r.t. Q,
+∞
otherwise.
One may hope to reduce the variance of the simulation by minimizing another en-
tropic distance, for example the Kullback–Leibler distance, between Pθ and Pθ∗,

8.5
Statistical Romberg Method
209
where θ∗is the process θ which minimizes H(Θ). The Kullback–Leibler distance
is deﬁned as
R0(P,Q) =
*
log
 dP
dQ

dP
if P is absolutely continuous w.r.t. Q,
+∞
otherwise.
For details, see the thesis of O. Bardou.1
In Chap. 9 we will see that the adaptive Monte Carlo method does not require a
projection procedure at each time step.
Exercise 8.2 Check that R0(P,Q) ≤2R2(P,Q).
Hint: Consider the function x2 −1 + x −1 −x log(x). Deduce that solving the
preceding minimization problem actually allows one to decrease the variance of the
simulation. Exhibit an example where this variance reduction is less effective than
the one obtained by minimizing H(Θ).
8.5 Statistical Romberg Method
In Chap. 7 we have shown that, under suitable hypotheses on the coefﬁcients b
and σ, the Euler scheme discretization error satisﬁes: for any smooth enough func-
tion f and time horizon T , there exists a real number CT
f such that, for all n ≥1,
Ef

Xn
T

−Ef (XT ) =
CT
f
n + O
 1
n2

.
We admit the complementary following weak convergence result, due to Dufﬁe and
Glynn [11], and we do not make precise the hypotheses under which it holds true:
if N = N(n) = n2, then
n

1
n2
n2

ℓ=1
f

Xn(ℓ)
T

−Ef (XT )

in law
−−−→
n→∞σG + Cf
T ,
where G is a standard Gaussian random variable, σ 2 := Var(f (XT )) and CT
f is
deﬁned as in the preceding equality.
Therefore, to achieve an accuracy of order 1/n, the global complexity of the
simulation is of order n2 ×n = n3. This observation suggests to use the Richardson–
Romberg extrapolation technique (see Chap. 7) not only to reduce the discretization
error but also to reduce the variance of the simulation. The principle of this new
method due to Kebaier [27] is as follows.
1Bardou, O.: Contrôle dynamique des erreurs de simulation et d’estimation de processus de diffu-
sion. Ph.D. thesis, Université de Provence (2005).

210
8
Variance Reduction and Stochastic Differential Equations
Choose a thin grid with step-size T /n and a coarse grid with step-size T /mn
where mn ≪n. The corresponding Euler schemes are respectively denoted by Xn
and Xmn. Suppose that the quantity ¯Mmn := Ef (Xmn
T ) is explicitly known and set
Y := f

Xn
T

−f

Xmn
T

+ ¯Mmn.
Then EY = Ef (Xn
T ), and the variance of Y is of order 1/mn. It then remains to
suitably choose ¯Mmn. It is effective to estimate it by means of a Monte Carlo method
based on Nmn independent simulations of Xmn. One then uses Nn new independent
simulations to approximate EY.
To summarize, the algorithm consists in computing
¯Y :=
1
Nmn
Nmn

ℓ=1
f
 ˆXmn(ℓ)
T

+ 1
Nn
Nn

ℓ=1

f

Xn(ℓ)
T

−f

Xmn(ℓ)
T

.
We now have to optimally choose the parameters n, Nmn and Nn to achieve a
global accuracy of order 1/n. A Central Limit Theorem, which we do not state
here, shows that good choices are
Nmn = n2,
Nn = n3/2.
For mn = √n the global complexity is of order n5/2, which is a substantial reduction
of the standard Monte Carlo method complexity, which we have seen is of order n3.
Remark 8.4 So far we have supposed that the function f is smooth. However one
can prove the following result: if the coefﬁcients b and σ are of class C ∞(Rd) with
smooth bounded derivatives of all orders, then, for any differentiable function f ,
lim
n→∞
√n

Ef (XT ) −Ef

Xn
T

= 0.
8.6 Problems
8.1 (Importance Sampling for Stochastic Differential Equations) We are given a
time horizon T > 0, coefﬁcients b and σ from R to R, a real-valued Brownian
motion (Wt), and a real number x. We suppose that b and σ are bounded functions of
class C ∞(R) with bounded derivatives of all orders. Consider the solution (Xt(x))
to the stochastic differential equation
Xt(x) = x +
 t
0
b

Xs(x)

ds +
 t
0
σ

Xs(x)

dWs,
0 ≤t ≤T.

8.6
Problems
211
1. Let f be a function of class C ∞(R) with bounded derivatives. Identify a function
φ such that
f

XT (x)

= Ef

XT (x)

+
 T
0
φ

Xs(x)

dWs.
Hint: Express φ in terms of the function σ and the x-derivative of the solution of
a well-chosen partial differential equation.
2. We are given coefﬁcients ˆb and ˆσ which are algebraically simpler than b and σ
and satisfy the same smoothness properties. Consider the solution ( ˆXt(x)) to the
stochastic differential equation
ˆXt(x) = x +
 t
0
ˆb
 ˆXs(x)

ds +
 t
0
ˆσ
 ˆXs(x)

dWs,
0 ≤t ≤T.
Show that there exists a real number C depending on x, T , b and σ, but not on ˆb
and ˆσ, such that
E
XT (x) −ˆXT (x)
2 ≤C∥b −ˆb∥2
∞+ C∥σ −ˆσ∥2
∞.
3. Show that there exists a real number C depending on x, T , b, σ, ˆb and ˆσ, such
that
E

d
dx XT (x) −d
dx
ˆXT (x)

2
≤C
''b′ −ˆb′''2
∞+ C
''σ ′ −ˆσ ′''2
∞.
4. Let ˆφ be chosen as above such that
f
 ˆXT (x)

= Ef
 ˆXT (x)

+
 T
0
ˆφ( ˆXs)dWs.
Exhibit, in terms of ∥b −ˆb∥∞, ∥σ −ˆσ∥∞, ∥b′ −ˆb′∥∞, and ∥σ ′ −ˆσ ′∥∞, a bound
for
E

 T
0
φ

Xs(x)

dWs −
 T
0
ˆφ
 ˆXs(x)

dWs

2
.
5. Develop an application of the preceding estimate to reduce the variance of the
standard Monte Carlo method which approximates Ef (XT ).
8.2 (Variance Reduction for the Computation of the Delta of a European Option (⋆))
Consider the stochastic volatility model for asset prices
St(ξ) = ξ + r
 T
0
Sθ dθ +
 t
0
σ(Sθ)Sθ dWθ,
where r stands for the interest rate which is assumed deterministic and constant, and
σ is a function which is bounded from below and from above by strictly positive
constants and has bounded derivatives of all orders.

212
8
Variance Reduction and Stochastic Differential Equations
We are given a maturity T > 0 and a constant K > 0. For 0 ≤t ≤T and ξ > 0
let
u(t,ξ) := E

1{ST −t(ξ)<K}

= P

ST −t(ξ) < K

.
We are interested in constructing a Monte Carlo method for ∂u
∂ξ (t,ξ).
1. Exhibit a probabilistic representation of ∂u
∂ξ (t,ξ), by approximating the function
x →1{x<K} by a sequence of smooth functions and by applying Theorem 8.2 to
Xt := log(St(ξ)).
2. Deduce a variance reduction method for the Monte Carlo approximation of
∂u
∂ξ (t,ξ).
3. Examine the particular case of the log-normal model in which the function σ is
constant.

Chapter 9
Stochastic Algorithms
Abstract This chapter addresses a few theoretical and practical issues raised by
a class of stochastic optimization or approximation algorithms. These algorithms
are efﬁcient numerical tools in various applications and they are at the basis of the
variance reduction techniques studied in the preceding chapter. We limit ourselves
to a simple framework which allows us to get convergence results by means of
dynamical systems and martingales theories without too many technical details. In
this, we follow the very pedagogical approach in Benaïm and El Karoui (Chaînes
de Markov et simulations; Martingales et stratégies, 2004).
Classic reference books on the topic are, e.g., Kusher and Yin [31] and Duﬂo
[12, 13], see also Asmussen and Glynn [4].
9.1 Introduction
The terminology “stochastic algorithm” (sometimes “optimization” or “approxima-
tion” is added between these terms) corresponds to a vast and imprecisely delimited
class of stochastic recursive sequences, which can easily be implemented using a
computer.
The task on hand is to solve reasonably well a problem featuring a parameter
on which there is some control. A stochastic algorithm is devised to update the
parameter iteratively, so as to make it close to some “nice” value, which is possibly
optimal in some way to be speciﬁed.
The structure of the algorithm and the precise sense of optimality may vary ac-
cording to the applications, and inﬂuence the assumptions and mathematical tools
for proofs, of convergence or other. The nature of the stochasticity may be very
diverse, and can for instance:
• be due to the fact that the observation of the phenomenon is only partial or noisy,
• come from a modelization trying to simplify the complexity of the phenomenon,
as for certain statistical physics models, or for certain chaotic behaviors,
• be due to the fact that the function to optimize is ill known, or more generally that
one does not know how to devise a deterministic optimization method (which
requires precise knowledge and regularity),
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1_9, © Springer-Verlag Berlin Heidelberg 2013
213

214
9
Stochastic Algorithms
• be introduced, for a function with numerous local optima (tormented energy land-
scape) that would trap deterministic optimization methods (such as gradient de-
scent), so as to escape local optima and ﬁnd global ones (stochastic gradient,
Hastings–Metropolis algorithms, simulated annealing, ...),
• be inherent to the modelization, as in case of the behavior of individuals or the
effects of environment or mutations in ecological or evolutionary scenarios, or for
customer behavior in communication networks.
9.2 Study in an Idealized Framework
Given the wide variety of situations and of techniques found in this domain, we
study a framework sufﬁciently simple to remain accessible, and sufﬁciently wide to
illustrate the main ideas in play.
9.2.1 Deﬁnitions
The class under study can be deﬁned as follows.
Deﬁnition 9.1 A stochastic algorithm is a random sequence (θn)n≥0 with values in
Rd, adapted to a ﬁltration (Fn)n≥0, that can be written in the form
θn+1 = θn + γn+1

F(θn) + Un+1

,
n ≥0,
for a function F : Rd →Rd and a real sequence (γn)n≥1 which are deterministic
and satisfy
γn > 0,
lim
n→∞γn = 0,

n≥1
γn = ∞,
and an (Fn)n≥0-adapted sequence (Un)n≥1 such that E(Un+1 |Fn) = 0 for n ≥0.
The γn are said to be the gains of the algorithm, and the Un to be martingale
increments. Regularity assumptions must be made on F , the least of which is that it
be locally Lipschitz.
Motivation and Examples
The purpose of such an algorithm is the search for a speciﬁc root of the equation
F(θ) = 0.

9.2
Study in an Idealized Framework
215
For instance, in order to reach a desired level v for a function f of a control θ ∈R,
one may consider
F(θ) = v −f (θ)
if f is non-decreasing,
F(θ) = f (θ) −v
if f is non-increasing;
to seek an extremum of a function V ∈C1, one may consider
F(θ) = −∇V (θ)
for a minimum,
F(θ) = ∇V (θ)
for a maximum.
Varied recursively-deﬁned random sequences may be written in the form in Def-
inition 9.1, as in the next fundamental result. This allows implementing stochastic
algorithms in which F and the Un are implicit, but F can be deﬁned by an expecta-
tion, which one does not know how to compute, of a simulable r.v. This is a crucial
fact in a Monte Carlo framework.
Lemma 9.1 Let a function H : Rd × Rm →Rd and an i.i.d. sequence (Xn)n≥1 of
r.v.’s with values in Rm and law P be such that
F(θ) := E

H(θ,X1)

=

H(θ,x)P(dx)
is deﬁned for all θ ∈Rd,
and a sequence (γn)n≥1 be as in Deﬁnition 9.1. Then, the sequence (θn)n≥0 of Rd-
valued r.v.’s deﬁned by θ0 independent of (Xn)n≥1 and
θn+1 = θn + γn+1H(θn,Xn+1),
n ≥0,
is a stochastic algorithm in the sense of Deﬁnition 9.1. Speciﬁcally, Fn = σ(θ0,X1,
...,Xn) and F is given above and Un+1 = H(θn,Xn+1) −F(θn).
Proof Then E(H(θn,Xn+1)|Fn) = F(θn) since θn is Fn-measurable and Xn+1 is
independent of Fn, hence
θn+1 = θn + γn+1

E

H(θn,Xn+1)|Fn

+ H(θn,Xn+1) −E

H(θn,Xn+1)|Fn

= θn + γn+1

F(θn) + Un+1

with E(Un+1 |Fn) = E(H(θn,Xn+1) −E(H(θn,Xn+1)|Fn)|Fn) = 0.
□
Results on stochastic algorithms will be given with assumptions bearing on the
form given in Deﬁnition 9.1. In order to apply them using Lemma 9.1, these as-
sumptions will have to be checked starting from the given P and H.
Instability and Projection
In practice, stochastic algorithms are often unstable, but may be stabilized by ade-
quate projection operations, as was done for the variance reduction method previ-

216
9
Stochastic Algorithms
ously described in this book. The parameter θ is then limited to a set Θ of admissible
parameters, which is usually compact.
There may be other practical reasons for such projections, as when one cannot
supply arbitrary values of θ to the actual system (which could be an input rate, a
temperature, etc.), or if one seeks an optimum and knows that values outside Θ are
far from optimal.
The projection operation consists in not accepting θn+1 if it is outside Θ, and
letting the algorithm start anew from a new value θ′
n+1 ∈Θ. The choice of this
value may be varied, and involve θn and θn+1, as for θ′
n+1 = θn, or when θ′
n+1 is a
boundary point of Θ on the segment between θn and θn+1.
The projection greatly complexiﬁes the mathematical analysis, and we shall not
try to give proofs in this case, but it is often intuitive and easy to code.
The results and proofs that we shall give thus correspond to a “standard” frame-
work which does not take into account such features.
9.2.2 The Ordinary Differential Equation Method, Martingale
Increments
Consider the ordinary differential equation (ODE) in Deﬁnition 6.2 for t ∈R+ with
a change of notation, replacing the vector ﬁeld b by F : Rd →Rd, i.e., for t ∈R+,
d
dt xt = F(xt)
and
x0 = x,
and precisely
xt = x +
 t
0
F(xs)ds.
(9.1)
Assume that there exists a ﬂow of homeomorphisms (φt)t∈R+: for all x ∈Rd and
t ∈R+ it holds that φt(x) = x +
 t
0 F(φs(x))ds (see Theorem 6.8, e.g.).
This differential equation plays an important role in the study of the stochastic
algorithm in Deﬁnition 9.1, through the continuous process (Xt)t∈R+ given by
⎧
⎪⎨
⎪⎩
τ0 = 0,
τn+1 = τn + γn+1 = γ1 + ··· + γn+1,
n ∈N,
Xt = θn +(t −τn)θn+1 −θn
γn+1
= Xτn +(t −τn)(F(Xτn)+Un+1),
t ∈[τn,τn+1].
(9.2)
This is an interpolation for the algorithm in an adequate time-scale. It looks like
an Euler scheme for the ordinary differential equation, on an irregular grid with
mesh going to 0 (see (9.3) later), perturbed by martingale increments.
One may then hope to relate its behavior for large times, and hence the behavior
for a large number of iterations of the algorithm, to the behavior for large times
of the ODE. The latter involves the stable ﬁxed points of the ODE, which are the
solutions of F(θ) = 0 such that the vector ﬁeld locally points toward this root; a
sufﬁcient condition is that locally (x −θ) · F(x) < 0.
The study of the stochastic algorithm will thus use tools related to ordinary differ-
ential equations and other tools related to martingales, such as the following classic
result, see [5, Thm 4.3.3 p. 138], e.g.

9.2
Study in an Idealized Framework
217
Theorem 9.1 (Convergence of L2-bounded martingales) Let (Mn)n∈N be a square
integrable martingale for a ﬁltration (Fn)n∈N. Then
E

M2
n

= E

M2
0

+
n

i=1
E

(Mi −Mi−1)2
∈[0,∞]
and if (Mn)n∈N is bounded in L2, i.e., if
sup
n∈N
E

M2
n

= E

M2
0

+

i≥1
E

(Mi −Mi−1)2
< ∞,
then the martingale converges a.s. and in L2.
9.2.3 Long-Time Behavior of the Algorithm
The following result is given under tractable assumptions, except for the a.s. bound-
edness assumption on (θn)n≥0, which must be proved on a case-by-case basis and
might be tough. One can try a comparison with the ODE, if its solution remains
bounded, in a way similar to the proof.
Theorem 9.2 [5, Thm 5.3.5 p. 173] Consider a stochastic algorithm (θn)n∈N writ-
ten as in Deﬁnition 9.1, and its interpolation (Xt)t∈R+ given in (9.2). Assume that
F is locally Lipschitz, the ODE (9.1) with vector ﬁeld F has a ﬂow of homeomor-
phisms (φt)t∈R+ (see Theorem 6.8, e.g.), and one of the following:
(a) either
sup
n≥1
E

|Un|2
< ∞,

n≥1
γ 2
n < ∞,
(b) or, the sequence (γn)n≥1 is non-increasing, and for some Γ > 0 and all θ ∈Rd
and c > 0,
sup
n≥1
E

eθ·Un |Fn−1

≤e
Γ
2 |θ|2,

n≥1
e−c/γn < ∞.
Assume moreover that
sup
n≥0
θn < ∞,
a.s.
Then
sup
0≤h≤T
Xt+h −φh(Xt)

a.s.
−−−→
t→∞0,
∀T ∈R+.

218
9
Stochastic Algorithms
Proof Let the “inverse map” of n →τn (see (9.2)) be deﬁned by
m : t ∈R+ →m(t) = sup{n ∈N : τn ≤t} ∈N.
Observe that (9.2) can be written as
Xt = X0 +
 t
0

F(Xτm(s)) + Um(s)+1

ds.
(9.3)
Let us ﬁrst prove that
Δ(t,T ) := sup
0≤h≤T

 t+h
t
Um(s)+1 ds

a.s.
−−−→
t→∞0,
∀T ∈R+.
(9.4)
Let t ∈R+ and T ′ > T ≥0. Since (γn)n≥1 goes to 0, if t is large enough then
τm(t) ≤t < t + T < τm(τm(t)+T ′)
so that
Δ(t,T ) ≤2
max
m(t)≤k≤m(τm(t)+T ′)

k

i=m(t)+1
γiUi
.
Thus, proof of (9.4) will follow from proof of
max
n≤k≤m(τn+T )

k

i=n+1
γiUi

a.s.
−−−→
n→∞0,
∀T ∈R+,
(9.5)
which could replace the assumption “(a) or (b)” in the statement. Under Hypoth-
esis (a), (9.5) follows from Theorem 9.1 applied to the martingale (Mn)n∈N with
Mn = n
i=1 γiUi. Problem 9.2 will show how to deduce it under Hypothesis (b);
see also the proof in [5].
Now, the conclusion will follow from (9.4) by a purely deterministic study of the
ODE. For t and h in R+, the deﬁnition of the ﬂow and (9.3) yield
φh(Xt) = Xt +
 h
0
F

φs(Xt)

ds,
Xt+h = Xt +
 h
0
F(Xt+s)ds + At(h) + Bt(h),
with At(h) =
 t+h
t

F(Xτm(s)) −F(Xs)

ds, Bt(h) =
 t+h
t
Um(s)+1 ds.
For the Lipschitz constant L = LC for F on a compact neighborhood C of the sam-
ple path (Xt)t∈R+ (which is bounded, a.s., since (θn)n≥0 is bounded by hypothesis)

9.2
Study in an Idealized Framework
219
it holds that
Xt+h −φ(Xt)
 ≤
At(h)
 +
Bt(h)
 + L
 h
0
Xt+s −φs(Xt)
ds
and Gronwall’s lemma yields
sup
0≤h≤T
Xt+h −φ(Xt)
 ≤eLT
sup
0≤h≤T
At(h)
 +
Bt(h)

.
(9.6)
Moreover
sup
0≤h≤T
At(h)
 ≤LT
sup
t≤s≤t+T
Xs −Xτm(s)

and (9.3) implies for K = supC |F| that
|Xs −Xτm(s)| =

 s
τm(s)

F(Xτm(u))+Um(u)+1

du
 ≤Kγm(s)+1 +

 s
τm(s)
Um(u)+1 du

and for t large enough and t ≤s ≤t + T it holds that γm(s)+1 ≤1 and

 s
τm(s)
Um(u)+1 du
 ≤

 τm(s)
t−1
Um(u)+1 du
 +

 s
t−1
Um(u)+1 du
 ≤2Δ(t −1,T + 1)
and hence
sup
0≤h≤T
At(h)
 ≤KLT
sup
t≤s≤t+T
γm(s)+1 + 2LT Δ(t −1,T + 1)
which, jointly with (9.6) and sup0≤h≤T |Bt(h)| = Δ(t,T ) and (9.4), allows to con-
clude.
□
The preceding result relates the large time behaviors of the interpolation
(Xt)t∈R+ of (θn)n∈N and of the ﬂow (φt)t∈R+. The following result completes The-
orem 9.2 in a remarkable way, in the sense that it provides crucial information on
curves which satisfy its conclusion.
One says that V ∈C1(Rd,R) is a strict Lyapunov function for F if
∇V (x) · F(x) ≤0,
∇V (x) · F(x) = 0 ⇔F(x) = 0,
∀x ∈Rd.
If there exists such a function and if the ODE (9.1) has a ﬂow of homeomorphisms
(φt)t∈R+, then
d
dt V

φt(x)

= ∇V

φt(x)

· F

φt(x)

≤0,
d
dt V

φt(x)

= 0 ⇔F

φt(x)

= 0,
so that V is non-increasing along the ﬂow, and strictly so except if the starting point
x is a ﬁxed point for the ODE, i.e., if F(x) = 0: assume that for some t > 0 it holds
that φt(x) = y and F(y) = 0, then φt(x) = y = φt(y) and, by injectivity, x = y.

220
9
Stochastic Algorithms
Theorem 9.3 [5, Thm 5.3.10 p. 179] Assume that the ODE (9.1) with vector ﬁeld
F has a ﬂow of homeomorphisms (φt)t∈R+ (see Theorem 6.8, e.g.), that F has a
strict Lyapunov function V , and that the ﬁxed-point set E = {x ∈Rd : F(x) = 0} of
the ODE is constituted of isolated points. If (yt)t∈R+ is a continuous bounded curve
satisfying
sup
0≤h≤T
yt+h −φh(yt)
 −−−→
t→∞0,
∀T ∈R+,
then limt→∞yt exists and belongs to E .
Proof (Elements of proof (see [5])) Let the set of accumulation points of (yt) be
L = L

(yt)t∈R+

=
(
t∈R+

(yt+h)h∈R+

.
We start by checking that L ∩E ̸= ∅. For that, for any neighborhood U of E , it
holds that ∇V (x) · F(x) ≤−δ(U) for some δ(U) > 0 on the compact set L ∩Uc
(see Lemma 9.2), and it can be proved (by a rather technical argument) that it is
not true that L = L ∩Uc, since then infL V = −∞which is impossible, and we
conclude from the fact that L is closed. We then prove that L ⊂E . For that we prove
(by an analogous more technical argument) that if there were some p0 ∈L ∩E c
then again infL V = −∞which is impossible. At last, the connectedness of L (see
Lemma 9.2 below) and the fact that E is constituted of isolated points shows that
L must contain only one point, and then the boundedness assumption on (yt)t∈R+
implies that it converges to its unique accumulation point.
□
In the proofs of the two preceding theorems we have used the following lemma.
Lemma 9.2 [5, Lemma 5.3.7 p. 177] Assume that the ODE (9.1) with vector ﬁeld
F has a ﬂow of homeomorphisms (φt)t∈R+ (see Theorem 6.8, e.g.). If (yt)t∈R+ is a
continuous bounded curve satisfying
yt+h −φh(yt)
 −−−→
t→∞0,
∀h ∈R+,
then the set of its accumulation points
L = L

(yt)t∈R+

=
(
t∈R+

(yt+h)h∈R+

is non-empty, compact, connected, and ﬂow invariant: φt(L) = L, all t ≥0.
Proof Since (yt)t∈R+ is bounded, the closures {(yt+h)h∈R+} are decreasing com-
pact sets, therefore their intersection L is non-empty and compact. To prove it is
connected, assume by contradiction that U and V are two disjoint open sets such
that L ⊂U ∪V and L∩U ̸= ∅and L∩V ̸= ∅. The curve (yt)t∈R+ would go an inﬁ-
nite number of times from U to V , and by continuity would pass each time through

9.3
Variance Reduction for Monte Carlo Methods
221
the closed non-empty set (U ∪V )c, and the boundedness of (yt)t∈R+ would imply
that L ∩(U ∪V )c ̸= ∅, which is a contradiction.
We now show that L = φh(L).
Let y ∈L and h ∈R+. There is (tn)n≥1 increasing to inﬁnity such that
limn→∞ytn = y. The continuity of φh and the asymptotic assumption on (yt)t∈R+
yield
φh(y) = lim
n→∞φh(ytn) = lim
n→∞ytn+h ∈L,
and hence φh(L) ⊂L.
Reciprocally, for large n the values ytn−h are deﬁned and bounded and thus have
an accumulation point z ∈L, by assumption
ytn −φh(ytn−h)
 =
ytn−h+h −φh(ytn−h)
 −−−→
n→∞0.
By continuity y = φh(z) and hence L ⊂φh(L).
□
9.3 Variance Reduction for Monte Carlo Methods
Variance reduction can be obtained using stochastic algorithms that use the same
random variables as the Monte Carlo method, and it is tempting implement both
algorithms at the same time with the same samples. This part is largely inspired by
Arouna [3].
9.3.1 Searching for an Importance Sampling
We have previously given an example of the use of stochastic algorithms for a vari-
ance reduction method, in the framework of a Monte Carlo computation. After dis-
cretization, the problem can be reduced to the following.
One wishes to ﬁnd an approximation for an expectation E(f (X)) that one does
not know how to compute exactly. In this, X is a d-dimensional random vector that
one knows how to simulate, and the function f : Rd →R is known. Speciﬁcally, X
was a standard normal vector.
The problem was to ﬁnd a simulable r.v. Z such that E(f (X)) = E(Z) and the
variance reduction
Var

f (X)

−Var(Z) = E

f (X)2
−E

Z2
is relatively large compared to Var(f (X)), or at least positive.
It can be an arduous task to ﬁnd good candidates for Z, and the corresponding
general optimization problem is trivial in theory and ill-posed in practice, since the
optimum is found tautologically for the deterministic Z = E(f (X)), which surely
has variance zero, but cannot be simulated since it requires to know what is precisely
to be approximated by the Monte Carlo method.

222
9
Stochastic Algorithms
It is reasonable then to limit the search within an adequate family of r.v.’s, possi-
bly parametrized. The family can be given by a family of densities w.r.t. a reference
measure, or correspond to some other changes of law.
For instance, let us re-examine the situation presented in Sect. 3.6.2. For X hav-
ing a density pX on Rd, and Y a density pY on Rd such that pY (x) > 0 if pX(x) > 0,
E

f (X)

=

f (x)pX(x)dx =

f (x)pX(x)
pY (x) pY (x)dx = E

f (Y)pX(Y)
pY (Y)

(introducing the Radon–Nikodym derivative pX/pY of the law of X w.r.t. the law
of Y), and the problem is to ﬁnd pY such that
E

f (Y)pX(Y)
pY (Y)
2
=
 
f (x)pX(x)
pY (x)
2
pY (x)dx =
 f (x)2pX(x)2
pY (x)
dx
is small. The minimization problem is again trivial in theory and ill-posed in prac-
tice: for f ≥0, the Cauchy–Schwarz inequality and

pY = 1 yield

f (x)pX(x)dx
2
=
 f (x)pX(x)
√pY (x)

pY (x)dx
2
≤
 f (x)2pX(x)2
pY (x)
dx.
We have equality if and only if pY is proportional to fpX. Then again the variance
is zero, but

pY = 1 ⇒pY = fpX

fpX
=
fpX
E(f (X)),
so that to simulate according to pY one must know E(f (X)). Moreover the function
f pX
pY which must be applied to the draws is constant equal to E(f (X)) (hence the
vanishing variance), necessitating again to know precisely the quantity one tries to
approximate.
For f taking values in R, this theoretical minimum is reached for |f |pX/

|f |pX
and this leads us to search for some pY close to this quotient among a family of
densities according to which it is easy to simulate and for which pX/pY can be
easily computed. This is called an importance sampling technique.
For pX > 0, a simple special idea is to take Y = X + θ for a parameter θ ∈Rd,
which has density pY : y →pX(y −θ), so that
E

f (X)

= E

f (Y)
pX(Y)
pX(Y −θ)

= E

f (X + θ)pX(X + θ)
pX(X)

which can be found directly using the translation invariance
E

f (X)

=

f (x)pX(x)dx
=

f (x + θ)pX(x + θ)dx
=

f (x + θ)pX(x + θ)
pX(x)
pX(x)dx.

9.3
Variance Reduction for Monte Carlo Methods
223
9.3.2 Variance Reduction and Stochastic Algorithms
In greater generality, assume that it holds that
E

f (X)

= E

g(θ,X)

,
g : Rm × Rd →R,
θ ∈Rm.
Using a sequence (Xn)n≥1 of i.i.d. r.v.’s with the same law as X, the Strong Law of
Large Numbers
lim
N→∞
1
N
N

n=1
g(θ,Xn) = E

f (X)

,
a.s.,
(9.7)
allows to estimate E(f (X)) without bias. Moreover, if g(θ,X) is in L2 and σ 2
θ :=
Var(g(θ,X)) then the Central Limit Theorem yields
lim
N→∞
√
N

1
N
N

n=1
g(θ,Xn) −E

f (X)


= N

0,σ 2
θ

,
in law,
(9.8)
which in theory provides asymptotic conﬁdence intervals for the estimation.
In practice, σ 2
θ should be unknown as well as E(f (X)), but elementary compu-
tations and the Strong Law of Large Numbers
lim
N→∞
1
N −1
N

j=1

g(θ,Xj) −1
N
N

n=1
g(θ,Xn)
2
= σ 2
θ ,
a.s.,
allow to estimate without bias σ 2
θ simultaneously to E(f (X)).
Heuristically, the estimated value of σ 2
θ can be used for the conﬁdence inter-
vals for (9.7) obtained using (9.8). Obtaining rigorous conﬁdence intervals for this
estimated value would necessitate knowledge of fourth-order moments, which are
unavailable and should be estimated, and so on.
Finding θ∗which equivalently minimizes the variance or the second moment of
g(θ,X), respectively given by
σ 2
θ = E

g(θ,X)2
−E

f (X)
2,
E

g(θ,X)2
,
allows to make the choice θ = θ∗in order to have asymptotic conﬁdence inter-
vals for E(f (X)) which are minimal in this framework. If this is difﬁcult to attain,
the goal is at least to ﬁnd some θ such that E(g(θ,X)2) < E(f (X)2), and better
E(g(θ,X)2) ≪E(f (X)2).
Deterministic methods, such as gradient descent, involve
F(θ) = −∇E

g(θ,X)2
and there are reasonable assumptions under which differentiation and expectation
can be interchanged so that
F(θ) = −E

∇θg2(θ,X)

.

224
9
Stochastic Algorithms
The computation of F is expected to be at least as difﬁcult as the computation of
E(f (X)) = E(g(θ,X)), so that deterministic methods cannot be implemented in
practice.
On the contrary, the stochastic algorithm of Lemma 9.1, for H such that
F(θ) = −E

∇θg2(θ,X)

= E

H(θ,X)

and some i.i.d. r.v.’s Xn with the same law as X, only necessitates the computation
of the H(θn,Xn+1) and not of their expectations, the price to pay being the intro-
duction of the “noise” due to the martingale increments Un+1. There are several
possible choices for H, one of which being −∇θg2. In theory one can write the al-
gorithm as in Deﬁnition 9.1, but in practice it is necessary to check the assumptions
using the known H and law of X.
The “optimal” Monte Carlo method involves the computation of g(θ∗,Xn) for
some “other” (Xn)n≥1 which also are i.i.d. with the same law as X. Moreover, the
computations of H(θ,x) and g(θ,x) will be seen to be rather similar.
It is thus very tempting to implement the stochastic algorithm simultaneously to
the Monte Carlo method. A theoretical justiﬁcation for using this adaptive Monte
Carlo method and the “optimal” conﬁdence intervals would require to prove that
lim
N→∞
1
N
N

n=1
g(θn−1,Xn) = E

f (X)

,
a.s.,
lim
N→∞
√
N

1
N
N

n=1
g(θn−1,Xn) −E

f (X)


= N

0,τ 2
,
in law,
for some τ which should be bounded in terms of the data and not too much larger
than σθ∗.
The r.v.’s (θn)n≥0 given by the stochastic algorithm are not independent, and we
are completely outside the framework for the classical law of large numbers and
central limit theorems. Theoretical justiﬁcations are available, but are beyond the
scope of this book; an example will be given in Problem 9.1.
It is also possible to invoke and implement numerical experimentation, and jus-
tify the methods by its good results.
The Gaussian Case
We now extend the Gaussian situation in the second part of Sect. 3.6.2 to the multi-
dimensional setting. Consider the special case in which X has standard normal law
N (0,Id). Then
g(θ,x) = f (x + θ)e−1
2 |x+θ|2+ 1
2 |x|2 = f (x + θ)e−θ·x−1
2 |θ|2

9.4
Problems
225
(similar to the Girsanov transform). The task is to minimize
E

g(θ,X)2
= E

f (X + θ)2 e−2θ·X−|θ|2
= E

f (X)2 e−θ·X+ 1
2 |θ|2
in which the second equality follows for the simple change of variables

f (x + θ)2 e−2θ·x−|θ|2 e−1
2 |x|2 dx =

f (y)2 e−θ·y+ 1
2 |θ|2 e−1
2 |y|2 dy.
(9.9)
It can be shown that if E(f (X)2+ε) < ∞for some ε > 0 the dominated conver-
gence theorem allows to prove the validity of the interchange
∇E

g(θ,X)2
= E

∇θg2(θ,X)

= E

f (X)2(θ −X)e−θ·X+ 1
2 |θ|2
.
Moreover, as seen in Proposition 3.3, θ →(θ −X)e−θ·X+ 1
2 |θ|2 is increasing in each
coordinate, and hence θ →E(g(θ,X)2) is a convex function. If P(f (X) ̸= 0) > 0
then it is strictly convex and can easily be proved to go to inﬁnity (in norm) at inﬁn-
ity, so that minθ E(g(θ,X)2) is reached at the unique root θ∗of E(∇θg(θ,X)2) = 0.
Let us assume that we are in this “nice” situation. The computation of F(θ) =
−∇E(g(θ,X)2) is at least as difﬁcult as that of E(g(θ,X)2) which was at the
start of the study, but a stochastic algorithm corresponding to Lemma 9.1 can be
devised, with H such that F(θ) = E(H(θ,X)) and (Xn)n≥1 constituted of i.i.d.
N (0,1) r.v.’s.
To choose H, we may use the fact that, as for (9.9),
F(θ) = −E

f (X)2(θ −X)e−θ·X+ 1
2 |θ|2
= E

f (X + θ)2X e−2θ·X−|θ|2
:= E

Xg(θ,X)2
yielding two choices, among which H(θ,x) = xg(θ,x)2 for which, in a vari-
ance reduction method implemented simultaneously to the Monte Carlo method,
g(θn,Xn+1) and then H(θn,Xn+1) = Xn+1g(θn,Xn+1)2 would be computed in or-
der to obtain θn+1.
Moreover, it can be checked that the θ →E(g(θ,X)2) is a strict Lyapunov func-
tion in the sense of Theorem 9.3.
Actually, this algorithm is unstable as presented, and must be stabilized by further
projection operations, but we shall not detail this very technical issue.
9.4 Problems
9.1 (Adaptive Monte Carlo Method) The goal is to obtain a good approximation by
a Monte Carlo method of the quantity E(φ(X)), where X is a simulable random
variable (r.v.), φ : R →R is a computable function, and E(φ(X)2) < ∞.
Let (Xn)n≥1 be a sequence of i.i.d. r.v.’s with the same law as X, and Fn =
σ(X1,...,Xn) for n ≥0. We assume that, in order to implement a variance reduc-
tion method, we have found a function g : R × R →R satisfying:

226
9
Stochastic Algorithms
(a) For every θ ∈R,
E

g(θ,X)

= E

φ(X)

.
(b) For every θ ∈R,
m2(θ) := E

g(θ,X)2
< ∞,
there exists θ∗∈R such that
m2

θ∗
= min
θ∈R m2(θ) < E

φ(X)2
,
the function θ →m2(θ) is continuous at θ∗, and
σ 2 := Var

g

θ∗,X

= min
θ∈R Var

g(θ,X)

> 0.
(c) There exists an (Fn)n≥0-adapted simulable sequence (θn)n≥0 converging to θ∗,
a.s. (For instance given by a stochastic algorithm involving (Xn)n≥1.)
We wish to prove, under reasonable assumptions, that
1
n
n

k=1
g(θk−1,Xk)
a.s.
−−−→
n→∞E

φ(X)

,
(9.10)
√n

1
n
n

k=1
g(θk−1,Xk) −E

φ(X)


in law
−−−→
n→∞N

0,σ 2
,
(9.11)
and moreover to estimate σ 2.
If we succeed, we will be able to estimate E(φ(X)) with an asymptotic precision
as good as the one for the Monte Carlo method with minimal variance among this
family, without having to approximate ﬁrst the optimal parameter θ∗, all this while
estimating the minimal variance so as to be able to provide asymptotic conﬁdence
intervals. This constitutes an adaptive Monte Carlo method.
We recall the following classic result. Let (Mn)n≥0 be a square-integrable
(Fn)n≥0-martingale, and ⟨M⟩n for n ≥0 be deﬁned by ⟨M⟩0 = 0 and ⟨M⟩n −
⟨M⟩n−1 = E((Mn −Mn−1)2 |Fn−1). The following convergence results hold:
(1) If limn→∞⟨M⟩n = ∞a.s., then limn→∞
Mn
⟨M⟩n = 0 a.s. ([5, Thm 4.3.9 p. 141]);
(2) If moreover supn≥1 |Mn −Mn−1| is bounded, then limn→∞
Mn
√⟨M⟩n = N (0,1)
in law.
1. (a) Prove that a square-integrable martingale (Mn)n≥0 is deﬁned by
M0 = 0,
Mn =
n

k=1

g(θk−1,Xk) −E

φ(X)

,
n ≥1.
(b) Compute ⟨M⟩n for n ≥0, and ﬁnd a simple equivalent for large n.
(c) Prove the convergence result (9.10) for the adaptive Monte Carlo method.

9.4
Problems
227
2. From now on, g : R × R →R is assumed to be uniformly bounded.
Prove the convergence result (9.11) yielding the rate of convergence.
3. From now on, θ →m4(θ) := E(g(θ,X)4) < ∞is assumed continuous at θ∗.
(a) Prove that a square-integrable martingale ( ˆMn)n≥0 is deﬁned by
ˆM0 = 0,
ˆMn =
n

k=1

g(θk−1,Xk)2 −m2(θk−1)

,
n ≥1.
(b) Compute ⟨ˆM⟩n for n ≥0, and ﬁnd a simple equivalent for large n.
(c) Suggest an estimator for σ 2 and prove it is convergent, a.s.
9.2 (Hypothesis (b) for Theorem 9.2) The goal is to prove (9.5) when Hypothesis (b)
is true, but not necessarily Hypothesis (a). We assume the other hypotheses in this
theorem are true, and use the notation there and in its proof, with d = 1 in order to
simplify notation. The following two classic results are admitted.
Claim (Doob maximal inequality) If (Xk)k∈N is a non-negative sub-martingale
then, for all N ∈N and a ≥0,
aP

max
0≤k≤N Xk ≥a

≤E(XN).
Claim (Doob decomposition) If (Xk)k∈N is a super-martingale then there exists
a martingale (Mk)k∈N and an increasing predictable process (Ak)k∈N such that
A0 = 0 and
Xk = Mk −Ak,
k ∈N.
1. Let θ ∈R and n ∈N and, with Z0 = 1 by convention,
Zk = exp

θ
n+k

i=n+1
γiUi −Γ
2 θ2
n+k

i=n+1
γ 2
i

,
k ≥0.
(a) Prove that (Zk)k∈N is a non-negative super-martingale.
(b) Deduce from this for all N ∈N and a ≥0 that
aP

max
0≤k≤N Zk ≥a

≤E(Z0) = 1.
2. Prove for every θ ∈R and integer n ≤N and b ∈R that
P

max
n≤k≤N θ
k

i=n+1
γiUi ≥b

≤exp

−b + Γ
2 θ2
N

i=n+1
γ 2
i

.

228
9
Stochastic Algorithms
3. Prove for e = ±1 and every integer n ≤N and α > 0 that
P

max
n≤k≤N e
k

i=n+1
γiUi ≥α

≤exp

−
α2
2Γ N
i=n+1 γ 2
i

and deduce from this that
P

max
n≤k≤N

k

i=n+1
γiUi
 ≥α

≤2exp

−
α2
2Γ N
i=n+1 γ 2
i

.
4. Prove that (9.5) is true.
9.3 (Principal Eigenvector and Oja’s Algorithm) Let be given vectors ak =
(ai
k)1≤i≤d ∈Rd for 1 ≤k ≤N, and the symmetric positive matrix
C = (Cij)1≤i,j≤d,
Cij = 1
N
N

k=1
ai
kaj
k .
We assume that the eigenvalues satisfy λ1 > λ2 > ··· > λd > 0, and denote by
(v1,...,vd) an orthonormal basis of corresponding eigenvectors.
For many applications, it is important to obtain approximations for these eigen-
values and eigenvectors, staring with λ1 (the largest) and v1 (deﬁned up to sign),
which are called the principal eigenvalue and eigenvector. We wish to provide ele-
ments of the proof for the following result.
For x = (xi)1≤i≤d and y = (yi)1≤i≤d in Rd, let ⟨x,y⟩= d
i=1 xiyi denote the
scalar product and ∥x∥2 = ⟨x,x⟩.
Theorem 9.4 Let θ0 be an Rd-valued r.v., (Kn)n≥1 be a sequence of uniform r.v.’s
in {1,2,...,N}, all these r.v.’s being independent, and Xn = aKn (i.e., Xn = ak if
Kn = k). Let (γn)n≥1 be a deterministic decreasing sequence of positive real num-
bers, and (θn)n≥0 be deﬁned by
θn+1 = θn + γn+1

⟨Xn+1,θn⟩Xn+1 −⟨Xn+1,θn⟩2θn

,
n ≥0.
Let B = sup1≤i≤N ∥ai∥2 < ∞. Assume that
0 < ∥θ0∥2 ≤5,
γ1 ≤1
2B ,

n≥1
γn = ∞,

n≥1
γ 2
n < ∞.
Then (θn)n≥0 converges either to v1 or to −v1, a.s.
We shall need the next result, which will be proved in the problem.

9.4
Problems
229
Theorem 9.5 Let (Vn)n≥0, (ηn)n≥0 and (εn)n≥0 be sequences of r.v.’s and be
adapted to a ﬁltration (Fn)n≥0. Assume that
Vn ≥0,
ηn ≥0,
E(Vn+1 |Fn) ≤Vn −ηn + εn,

n≥0
|εn| < ∞.
Then (Vn)n≥0 converges to a ﬁnite r.v. V∞and 
n≥0 ηn < ∞, a.s.
Reminders and Results Which Are Admitted
A sequence (Yn)n≥0 of R-valued
r.v.’s is a super-martingale for a ﬁltration (Fn)n≥0 if it is adapted, integrable, and
satisﬁes E(Yn+1 |Fn) ≤Yn for n ≥0. Let x−:= max(−x,0).
We recall Doob’s convergence theorem: If (Yn)n≥0 is a super-martingale such
that supn E(Y −
n ) < ∞, then (Yn)n≥0 converges a.s. to an integrable r.v. Y∞. We
recall also Theorem 9.1 on convergence of martingales which are bounded in L2.
Part I—Proof of Theorem 9.5
1. Prove that (Yn)n≥0 deﬁned by Yn = Vn −n−1
k=0(εk −ηk) is a super-martingale.
2. For a ≥0 let Ta = inf{n ≥1 : n
k=0(εk −ηk) ≥a}. Prove that Ta is a stopping
time, and that (Yn)n≥0 converges on {Ta = ∞} to a ﬁnite limit, a.s.
Hint: Introduce the sequence (Yn∧Ta)n≥0, where x ∧y := min(x,y).
3. On {Ta = ∞}, prove that 
n ηn < ∞, then that (Vn)n≥0 converges, a.s.
4. Conclude that Theorem 9.5 is true.
Part II—Proof of Theorem 9.4, the Validity of Theorem 9.5 Being Assumed
1. Prove for n ≥0 that
∥θn+1∥2 = ∥θn∥2 +

2γn+1

1 −∥θn∥2
+ γ 2
n+1

∥Xn+1∥2 +

∥θn∥2 −2

⟨Xn+1,θn⟩2
⟨Xn+1,θn⟩2
and ∥θn∥2 ≤5.
2. (a) Prove for n ≥0 that if ∥θn∥2 ≤1 then ∥θn+1∥2 ≥∥θn∥2.
(b) Assuming that if 1 < ∥θn∥≤5 then θn+1 ̸= 0, prove that ∥θn∥cannot con-
verge to 0, a.s.
3. (a) Write the recurrence relation between θn+1 and θn in terms of
Cn+1 =

Xi
n+1Xj
n+1

1≤i,j≤d.
(b) Justify that it can be written as the stochastic algorithm
θn+1 = θn + γn+1

F(θn) + Un+1

,
n ≥0,
where F : θ ∈Rd →F(θ) = Cθ −⟨θ,Cθ⟩θ ∈Rd, and the Un+1 are uni-
formly bounded martingale increments.
4. Prove that (Mn)n≥0 deﬁned by Mn = n
k=1 γkUk is a martingale which con-
verges a.s. and in L2, and thus that the series 
n≥0 γn+1Un+1 converges.

230
9
Stochastic Algorithms
5. (a) Prove for n ≥0 that, a.s.,
∥θn+1∥2 = ∥θn∥2 + 2γn+1

1 −∥θn∥2
⟨θn,Cθn⟩+ 2⟨θn,γn+1Un+1⟩
+ O

γ 2
n+1

,
E

∥θn+1∥2 −1
2 |Fn

=

∥θn∥2 −1
2 −4γn+1

1 −∥θn∥22⟨θn,Cθn⟩
+ O

γ 2
n+1

.
(b) Deduce from this that

n≥0
γn+1

1 −∥θn∥22⟨θn,Cθn⟩< ∞.
(c) Deduce from this that limn→∞∥θn∥2 = 1.
(d) Deduce from this that

n≥0
γn+1

1 −∥θn∥2
⟨θn,Cθn⟩
converges, a.s.
6. Consider the function
V : θ ∈Rd −{0} →V (θ) = exp(∥θ∥2)
⟨θ,Cθ⟩.
(a) Prove that ∇V (θ) = −2V (θ)
⟨θ,Cθ⟩F(θ).
(b) Prove that
E

V (θn+1)|Fn

= V (θn) −2γn+1
2V (θn)
⟨θn,Cθn⟩
''F(θn)
''2 + O

γ 2
n+1

,
n ≥0.
(c) Prove that (V (θn))n≥0 and the series 
n≥0 γn+1 2V (θn)
⟨θn,Cθn⟩∥F(θn)∥2 con-
verge, a.s.
(d) Deduce from this that limn→∞F(θn) = 0, a.s.
(e) Prove that (θn)n≥0 converges, a.s., to an r.v. θ∞with values in {v1,−v1,...,
vd,−vd}.
(f) Prove that the only local minima of V are at v1 and −v1.
One can prove from this that θ∞takes values in {v1,−v1}, a.s., and hence con-
clude the proof of Theorem 9.4, but this would lead us too far.

Appendix
Solutions to Selected Problems
2.1 (A Poisson Distribution Simulation Method)
1. From EeθX =
 ∞
0 eθxλe−λx dx we deduce that the Laplace transform of X is
λ
λ−θ for all θ < λ.
2. A simple computation shows that, for all θ < λ,
 ∞
0
eθxpN(x)dx =

λ
λ −θ
N
= EeθSN .
3. The result easily follows from
P(M = k) = P(M ≤k) −P(M ≤k −1) =
 ∞
λ

pk+1(y) −pk(y)

dy
and the above explicit formula for pk(y).
4. Let (Ui) be independent random variables uniformly distributed on [0,1]. Since
Ui and e−Xi have the same probability distribution, the preceding question shows
that the smallest integer M such that )M+1
i=1 Ui < e−λ has Poisson distribution.
2.2 (Lyapunov Exponent of Linear Random Recursive Sequences)
1. The integrability at inﬁnity is obvious since log|x| ≤|x| for |x| ≥1.
To check the integrability around 0, we observe that, for all z > 0,
 z
0
log(x)dx = zlog(z) −z,
from which an integration by parts leads to
 z
0

log(x)
2 dx =

log(x)

x log(x) −x
x
0 −
 z
0

log(x) −1

dx
= z

log(z)
2 −2zlog(z) + 2z.
The right-hand side has a ﬁnite limit when x tends to 0.
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1, © Springer-Verlag Berlin Heidelberg 2013
231

232
Solutions to Selected Problems
2. We have
¯Xh
N(x) = x
N−1

p=0

1 + b
√
hGp+1 +

a + b2
2

h

,
from which
1
Nh log
 ¯Xh
N(x)
 = log|x|
Nh
+ 1
Nh
N−1

p=0
log
1 + b
√
hGp+1 +

a + b2
2

h
.
Note that the random variables in the right-hand side are independent and identi-
cally distributed. In addition, as log(|x|) is integrable around 0, one easily checks
that the function
x →log
1 + b
√
hx +

a + b2
2

h
exp

−x2
2

is integrable over R. Thus the Strong Law of Large Numbers applies:
∃¯λh ∈R, ∀x ∈Rd −{0},
¯λh =
lim
N→+∞
1
Nh log
 ¯Xh
N(x)

a.s.
3. Observe that

αp
2
=

α2
p + 2

p<q
αpαq.
Since the random variables log|1 + b
√
hGp+1 + (a + b2
2 )h| are independent, it
follows that
∃Ch ∈R,
1
N2 E

log
 ¯Xh
N(x)
2 < Ch
for all N ≥1.
To obtain the desired equality for ¯λh it now sufﬁces to use the above represen-
tation of
1
Nh(log| ¯Xh
N(x)| −log|x|) as a sum of integrable identically distributed
random variables, compute the expectation, and let N go to inﬁnity.
4. Observe that
Z1 = Z2 + 1C(Z1 −Z2) + 1Ω−C(Z1 −Z2)
for all random variables Z1 and Z2 and all events C. The desired result follows.
The Cauchy–Schwarz inequality implies that there exist C > 0 and α > 0
such that, for all h small enough,
E
	
1|Y|≥1

log|1 + Y| −Y + Y 2
2 −Y 3
3

 ≤C exp

−α
h

.

Solutions to Selected Problems
233
Expanding log(1 + Y) around 0 yields that
E
	
1|Y|<1

log(1 + Y) −Y + Y 2
2 −Y 3
3

 = O

h2
.
It ﬁnally follows that
¯λh = a + O(h).
3.4 (Concentration Inequalities)
1. The desired inequality comes from
d
dx K(f )(x) =

1 −hU′′(x)

K

f ′
(x).
2. By Theorem 3.8, the standard Gaussian law on Rd satisﬁes a logarithmic Sobolev
inequality with constant C = 2. In view of the deﬁnition of the entropy, one easily
deduces that a Gaussian law with mean vector m and covariance matrix hIdRd
satisﬁes a logarithmic Sobolev inequality with constant C = 2h.
3. The right-hand side is a telescopic sum.
4. Observe that
Entνn
f 2
=
n

i=1
Ki−1
Entν1

g2
n−i

,
and use Question 2.
5. Using Question 1 one obtains
g′
n−i
2 = | d
dx Kn−i(f 2)|2
4Kn−i(f 2)
≤(1 −λh)2 (K| d
dx Kn−i−1(f 2)|)2
4K(Kn−i−1(f 2))
.
The Cauchy–Schwarz inequality implies
(K(f ))2
K(g)
≤K
f 2
g

,
from which
g′
n−i
2 ≤(1 −λh)2K
g′
n−i−1
2
.
Since K(φ) ≤K(ψ) when φ ≤ψ, an iterative procedure leads to
g′
n−i
2 ≤(1 −λh)2(n−i)Kn−if ′2
.
6. Observe that
Entνn
f 2
≤h
n−1

i=0
(1 −λh)2i

Knf ′2

234
Solutions to Selected Problems
= h1 −(1 −λh)2n
1 −(1 −λh)2 Knf ′2
=
2
λ(2 −λh)

1 −(1 −λh)2n
Knf ′2
.
7. Apply Theorem 3.10 with
C :=
2
λ(2 −λh)

1 −(1 −λh)2n
.
4.2 (Central Limit Theorem and Concentration Inequalities for Poisson Laws)
1. Then ψλ(z) := E(ezXλ) = e−λ 
k∈N ezk λk
k! = e−λeλez = eλ(ez−1).
2. For every z ∈C
E

ezWλ
= e−zλ
√
λ ψλ
 z
√
λ

= eλ(e
z
√
λ −1−z
√
λ ) −−−→
λ→∞e
z2
2 ,
which is the Laplace transform of the N (0,1) law, hence Wλ converges in law to
N (0,1); it sufﬁces to consider characteristic functions and take z = iu, u ∈R.
3. The monotonicity of the exponential function and the Markov inequality yield
P(Xλ −λ ≥a) = P

eθXλ ≥eθ(λ+a)
≤e−θ(λ+a)E

eθXλ
and thus the ﬁrst inequality. The second follows by optimizing over θ > 0. The
last follows using log(1 + a
λ) ≥a
λ −a2
2λ2 . Taking a = c
√
λ for c ≥0 yields that
P(Wλ ≥c) ≤e−c2
2 (1−c
√
λ ).
4. Similarly P(Xλ −λ ≤−b) = P(e−θXλ ≥e−θ(λ−b)) and the ﬁrst and second
inequalities hold. If −1
2 ≤b
λ < 0 then the Taylor–Lagrange expansion yields
log(1 −b
λ) ≥−b
λ −b2
2λ2 −8b3
3λ3 and the third inequality follows. Taking b = d
√
λ
for d ≤0 yields that P(Wλ ≤d) ≤e−d2
2 (1−d
√
λ )+(λ−b) 8d3
3λ3/2 .
4.3 (Inhomogeneous Poisson Process)
1. The continuity of Θ implies A(t) = inf{u ∈R+ : Θ(u) = t} and Θ(A(t)) = t.
Also, A is left-hand continuous and A(Θ(t)) = inf{u ∈R+ : Θ(u) = Θ(t)} ≤t.
By deﬁnition Nt −ˆNΘ(t) = Nt −NA(Θ(t)) and the law of Nt −NA(Θ(t)) de-
pends only on Θ(t) −Θ(A(Θ(t))). Moreover Θ(A(Θ(t))) = Θ(t) by the pre-
vious results. Hence the law of Nt −ˆNΘ(t) is the same as the law of N0 := 0.
By deﬁnition Tn = inf{u ≥0 : Nu = n} = inf{u ≥0 : ˆNΘ(u) = n} = inf{u ≥
0 : Θ(u) = ˆTn} = A( ˆTn). Hence Θ(Tn) = Θ(A( ˆTn)) = ˆTn.
2. For all t,h ≥0, the r.v.
ˆNt+h −ˆNt = NA(t+h) −NA(t) is independent of
(Nu)0≤u≤A(t), and thus of ( ˆNs1,..., ˆNsn) = (NA(s1),...,NA(sn)) for any 0 ≤
s1 < ··· < sn ≤t. This implies it is independent of ( ˆNs)0≤s≤t. Moreover, its law
depends only on
 A(t+h)
A(t)
θ(s)ds = Θ(A(t +h))−Θ(A(t)) = t +h−t = h, and

Solutions to Selected Problems
235
must be the same as the law of Nh. Such a process with independent stationary
increments must be a Poisson process.
3. Simulate a Poisson process ( ˆNt)t∈R+ of intensity 1, compute (Θ(t))t∈R+, and
set Nt = ˆNΘ(t). The problem is to compute (Θ(t))t∈R+ in practice.
4. The r.v.’s Nt1,Nt2 −Nt1,...,Ntn −Ntn−1 are independent, and Nti −Nti−1 =
ˆNΘ(ti)−ˆNΘ(ti−1) has Poisson law with parameter Θ(ti)−Θ(ti−1) =
 ti−1
ti
θ(s)ds.
5. The strong Markov property of ( ˆNt)t∈R+ yields that
P(Tn+1 −Tn ≥t |T1,...,Tn) = P

A( ˆTn+1) −A( ˆTn) ≥t |A( ˆT1),...,A( ˆTn)

= P

A( ˆTn+1) −A( ˆTn) ≥t |A( ˆTn)

= P(Tn+1 −Tn ≥t |Tn),
=
 ∞
0
1{A(Θ(Tn)+s)−Tn≥t}e−s ds
= exp

−min

u : A

Θ(Tn) + u

≥Tn + t

= exp

−

Θ(Tn + t) −Θ(Tn)

.
6. This is obvious: ( ˆNt)t∈R+ is indeed a Poisson process with intensity 1 since
Θ(A(t)) = t, and Nt = ˆNΘ(t).
7. Use the sampling property for Poisson processes.
4.4 (Brownian Limit for the Poisson Process)
1. (a) Classically ψx(z) = e−x 
k∈N ezk xk
k! = ex(ez−1).
(b) A Taylor expansion of order 4 of the moment generation function ψx(z) =
ex(ez−1) yields the result.
2. (a) For any z ∈C it holds that E(ezW θ
t ) = e−
√
θtzE(e
z
√
θ Nθt ), and Nθt has Poisson
law P(θt) and hence
E

ezW θ
t 
= e−
√
θtzeθt(e
z
√
θ −1) = eθt(e
z
√
θ −1−z
√
θ ) = et z2
2 +o(θ).
This implies that W θ
t converges in law to a N (0,t) random variable such
as Wt; it is enough to consider characteristic functions, and take z = is for
s ∈R.
(b) The processes have independent and stationary increments. Hence these vec-
tors have independent coordinates, so that the convergence in law of the vec-
tors follows immediately from that of their coordinates (for instance by using
multivariate characteristic functions).
(c) These random vectors are the image by a given linear (hence continuous)
function of the previous random vectors.
3. (a) For all 0 ≤s ≤t ≤T such that |t −s| ≤ε, there exists k ∈N such that
k ≤K(T,ε) and kε ≤s ≤t ≤kε + ε or kε ≤s ≤kε + ε ≤t ≤kε + ε. We

236
Solutions to Selected Problems
bound |W θ
t −W θ
s | in the ﬁrst case by |W θ
t −W θ
kε| + |W θ
kε −W θ
s | and in the
second by |W θ
t −W θ
kε+ε|+|W θ
kε+ε −W θ
kε|+|W θ
kε −W θ
s |, hence the factor 3.
(b) We use the previous result, and set Ak = {supkε≤t≤kε+ε |W θ
t −W θ
kε| ≥a}.
Note that P(Ak) = P(A0) since the process has independent stationary incre-
ments. We conclude using the elementary inequalities K(T,ε) ≤T /ε and
P
K(T,ε)
6
k=0
Ak

≤
K(T,ε)

k=0
P(Ak) =

K(T,ε) + 1

P(A0).
4. By deﬁnition P(sup0≤n≤m Mn ≥c) = P(Mτ ≥c), and P(Mτ ≥c) ≤1
cE(Mτ) by
the Markov inequality. We then note that τ is a stopping time bounded by m, and
that thus, classically for a sub-martingale, E(Mτ) ≤E(Mm): for instance
E(Mτ) =
m

i=0
E(Mi1{τ=i}) ≤
m

i=0
E

E(Mm |M0,...,Mi)1{τ=i}

=
m

i=0
E

E(Mm1{τ=i} |M0,...,Mi)

= E(Mm).
5. (a) It is a process with centered independent increments, and such a process is
classically a martingale.
(b) Classically, this can be deduced from the previous result by using either the
Jensen or the Hölder inequality.
(c) We come back to the bound obtained at the end of Question 3.
We bound P(sup0≤t≤ε |W θ
t | ≥a) = P(sup0≤t≤ε |W θ
t |4 ≥a4) using the
previous result for p = 4, and the lemma for Mt = |W θ
t |4 and c = a4 and
u = ε.
Note that E|W θ
ε |4 := 1
θ2 E((Xx −x)4) for x = θε and that E((Xx −x)4) =
E(X4
x) −4xE(X3
x) + 6x2E(X2
x) −4x3E(Xx) + x4 = x + 3x2. Thus
E
W θ
ε
4 = 1
θ2 E

(Xθε −θε)4
= 1
θ2

θε + 3θ2ε2
=
ε
θ + 3ε2

.
5.1 (Assymetric Ehrenfest Urn)
1. This is obtained from the description, by superposition of Poisson processes.
2. Clearly (Xt)t≥0 takes values in {0,...,N}, and its jumps are of size +1 or
−1. When it is in state x, and hence (Yi(t),1 ≤i ≤N)t≥0 is in some state
(yi,1 ≤i ≤N) such that 
1≤i≤N yi = x (there are many such states):
• it jumps to x + 1 at rate λ
1≤k≤N(1 −yk) = λ(N −x);
• it jumps to x −1 at rate μ
1≤k≤N yk = μx.
This is indeed the evolution of a Markov process with generator Q.

Solutions to Selected Problems
237
3. (a) The forward Kolmogorov equation can be written as
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
d
dt πt(0) = −πt(0)λN + πt(1)μ,
d
dt πt(x) = πt(x −1)λ(N −x + 1) −πt(x)

λ(N −x) + μx

+ πt(x + 1)μ(x + 1),
d
dt πt(N)= πt(N −1)λ −πt(N)μN.
(b) Use ft(x) = πt(0) + ··· + πt(x) and πt(x) = ft(x) −ft(x −1), 0 ≤i ≤N.
(c) By summation d
dt ft(x) = −πt(x)λ(N −x) + πt(x + 1)μ(x + 1).
(d) It is necessary and sufﬁcient that
d
dt πt = 0 and hence that
d
dt ft = 0, and
hence ˜π(x + 1)μ(x + 1) = ˜π(x)λ(N −x) for x = 0,...,N −1.
(e) Iteration yields ˜π(x) = ˜π(0)( λ
μ)x N···(N−x+1)
x!
:= ˜π(0)( λ
μ)xN
x

, and com-
puting N
x=0( λ
μ)xN
x

= N
x=0
N
x

( λ
μ)x1N−x = ( λ
μ + 1)N yields ˜π(0) =
( λ
μ + 1)−N.
5.2 (Chain Reaction)
1. Then 
k̸=1 η(k) = 1 −η(1) > 0. By sampling from a Poisson process and an el-
ementary conditioning, the process for λ and η has the same stochastic evolution
as the process for λ′ = λ(1 −η(1)) > 0 and the law η′ on N given by η′(1) = 0
and η′(k) = η(k)/(1 −η(1)) for k ̸= 1.
2. (a) Non-zero non-diagonal terms of the generator: Q(x,y) = λxη(y −x + 1)
for x ̸= y such that y ≥x −1 in N. From a state x, the process waits an
exponential time of parameter λx and then jumps to y ̸= x with probability
η(y −x + 1). This generator is not bounded, and there could be explosion.
(b) For instance
ˆXn = X0 +
n

k=1
(Zk −1)1k≥M.
Use the strong law of large numbers limn→∞1
n
n
k=1(Zk −1) = m −1, a.s.
(c) Conditional on ( ˆXk)0≤k≤n−1, the law of Tn is that of a sum of n independent
exponential r.v.’s of parameters λ ˆX0,...,λ ˆXn−1, and hence
E

e−Tn |( ˆXk)0≤k≤n−1

=
n−1

k=0
λ ˆXk
1 + λ ˆXk
.
Taking expectations, using ˆXn = O(n), and by dominated convergence,
E

e−Tn
= E
n−1

k=0
λ ˆXk
1+λ ˆXk

= E

exp
n−1

k=0
log

1−
1
1+λ ˆXk

−−−→
n→∞0
which implies that Tn −−−→
n→∞∞, a.s.

238
Solutions to Selected Problems
3. (a) Conditional on X0 = x, it is enough to consider the sum of x independent
copies of the process started at 1. Hence Ex(zXt ) = ft(z)x.
(b) Conditional on Xε, the process (Xt+ε)t≥0 is a Markov process with the same
stochastic evolution as the original process but started at Xε, so that
ft+ε(z) := E1

zXt+ε
=

x∈N
Ex

zXt 
P1(Xε = x) =

x∈N
ft(z)xP1(Xε = x)
= fε

ft(z)

.
(c) The process has a jump in [0,ε] with probability 1 −e−λε = λε + O(λ2ε2),
and conditional on that, its ﬁrst jump leads it to k with probability η(k) and
the probability of it jumping another time within [0,ε] is bounded by

k∈N

1 −e−kλε
η(k) ≤

k∈N
kλεη(k) = mλε,
hence the probability that it has two or more jumps within [0,ε] is
O(mλ2ε2).
(d) The previous results allow us to consider the case t = 0 and to justify that
fε(z) −f0(z) = E1

zXε
−z = −λεz + λε

k≥0
zkη(k) + O

(m + 1)λ2ε2
= h(z)ε + O

(m + 1)λ2ε2
.
From this, the differential equation is obvious.
It is the line for x = 1 of the backward Kolmogorov equation for f :
y →zy:
d
dt ft(z) = d
dt vt(1) =

y∈N
Q(1,y)vt(y) =

y∈N
Q(1,y)ft(z)y
= λg

ft(z)

−λft(z).
4. Since E = 7
t∈R+ ↑{Xt = 0}, by monotone limit P1(E )= limt→∞↑P1(Xt =0),
and P1(Xt = 0) = ft(0). We conclude by a simple study of ∂
∂t ft(0) = u(ft(0)).
5. Then
∂
∂t
 ∂
∂zft(z)

= ∂
∂z
∂
∂t ft(z) = ∂
∂z

h

ft(z)

=
 ∂
∂zft(z)

h′
ft(z)

,
E1(Xt) = ∂
∂zft(z = 0) = exp
 t
0
h′(1)ds

= eλ(m−1)t.
If m = 1 then E1(Xt) = 1 and limt→∞Xt = 0 a.s., and the Dominated Conver-
gence Theorem implies that E1(supt≥0 Xt) = ∞.

Solutions to Selected Problems
239
6. The function t →ft(z) is solution of f ′ = λ(pf 2 −f + 1 −p) with f0 = z, and
pf 2 −f + 1 −p = p(f −1)

f −1 −p
p

.
If p = 1/2 then f ′ = λ
2(f −1)2 and hence (
1
f −1)′ = −λ
2 and we conclude that
1
ft(z) −1 =
1
z −1 −λ
2t,
ft(z) = 1 +
1
1
z−1 −λ
2t
= 1 −
1 −z
1 + λ
2t(1 −z)
,
and P1(E ) = limt→∞ft(0) = 1. If p ̸= 1/2 then the decomposition
1
(f −1)(f −1−p
p )
=
p
2p −1

1
f −1 −
1
f −1−p
p

leads to the differential equation (log
f −1
f −1−p
p
)′ = (2p −1)λ which is solved by
ft(z) −1
ft(z) −1−p
p
=
z −1
z −1−p
p
e(2p−1)λt,
z ̸= 1 −p
p
,
and hence
ft(z) =
1 −1−p
p
z−1
z−1−p
p
e(2p−1)λt
1 −
z−1
z−1−p
p
e(2p−1)λt
= 1 + 2p −1
p
z−1
z−1−p
p
e(2p−1)λt
1 −
z−1
z−1−p
p
e(2p−1)λt
= 1 −2p −1
p
1 −z
(z −1−p
p )e−(2p−1)λt + 1 −z
.
Hence, if p < 1/2 then P1(E ) = limt→∞ft(0) = 1, and if p > 1/2 then
P1(E ) = limt→∞ft(0) = 1 −2p−1
p
= 1−p
p .
5.3 (Markov process, generator, and martingales)
1. Since Mf
t is (Xs)0≤s≤t-measurable, the Markov property yields
E

Mf
t+h |(Xs)0≤s≤t

−Mf
t
= E

Mf
t+h −Mf
t |(Xs)0≤s≤t

= E

f (Xt+h) −f (Xt) −
 t+h
t
Qf (Xs)ds |(Xs)0≤s≤t

= F f
h (Xt)
where
F f
h : x →Ex

f (Xh) −f (x) −
 h
0
Qf (Xs)ds


240
Solutions to Selected Problems
= Phf (x) −f (x) −
 h
0
PsQf (Xs)ds
vanishes considering the backward Kolmogorov equation since PsQ = QPs, or
by the forward Kolmogorov equation by letting it integrate f .
2. (a) This can be readily deduced from
Mf 2
t
= f 2(Xt) −
 t
0
Q

f 2
(Xs)ds,
Mf
t = f (Xt) −
 t
0
Qf (Xs)ds,

Mf
t
2 = f 2(Xt) −2f (Xt)
 t
0
Qf (Xs)ds +
 t
0
Qf (Xs)ds
2
.
(b) This follows from the fact that a differentiable function can be expressed as
the integral of its derivative.
(c) The increment between t ≥0 and t + h for h ≥0 is given by
Mf
t+h
 t+h
0
Qf (Xs)ds −
 t+h
0
Mf
s Qf (Xs)ds
−Mf
t
 t
0
Qf (Xs)ds +
 t
0
Mf
s Qf (Xs)ds
=

Mf
t+h −Mf
t
 t
0
Qf (Xs)ds + Mf
t+h
 t+h
t
Qf (Xs)ds
−
 t+h
t
Mf
s Qf (Xs)ds.
Take the conditional expectation w.r.t. (Xu)0≤u≤t. The ﬁrst term vanishes
by the martingale property of (Mf
t )t≥0, and the rest yields, using the Fubini
theorem, the tower property of conditional expectations, and the ﬁrst result,
E
 t+h
t

Mf
t+h −Mf
s

Qf (Xs)ds |(Xu)0≤u≤t

=
 t+h
t
E

Mf
t+h −Mf
s

Qf (Xs)|(Xu)0≤u≤t

ds
=
 t+h
t
E

E

Mf
t+h −Mf
s |(Xu)0≤u≤s

Qf (Xs)|(Xu)0≤u≤t

ds = 0.
Another “stochastic calculus type” method uses the integration by parts
Mf
t
 t
0
Qf (Xs)ds =
 t
0
Mf
s Qf (Xs)ds +
 t
0
 s
0
Qf (Xu)dudMf
s
where the last integral is of Stieljes type (actually, a series), and can be easily
shown to deﬁne a martingale.

Solutions to Selected Problems
241
(d) It is enough to collect the previous results. The expectation of a martingale
vanishing at zero is zero.
3. Use that Qa = 0 and hence Q(f −a)2 −2(f −a)Q(f −a) = Qf 2 −2f Qf ,
then condition on X0.
4. It holds that
Q

f 2
(x)−2f (x)Qf (x) =

y̸=x
Q(x,y)

f 2(y)−f 2(x)−2f (x)

f (y)−f (x)

=

y̸=x
Q(x,y)

f (y) −f (x)
2.
6.1 (Binary Energy Exchanges)
1. Let (ei)1≤i≤N be the canonical basis of RN
+. The generator acting on f ∈
L∞(R+) yields the function, of x = (xi)1≤i≤N in RN
+,

1≤i≤N

α

f (x + ei) −f (x)

+ βxi
 1
0

f (x −θxiei) −f (x)

dθ

+

1≤i<j≤N
γ xixj
 xi+xj
0

f

x + (z −xi)ei + (xi −z)ej

−f (x)

dz.
This is clearly not bounded.
2. The generator is bounded on each Bk, the jump rate from Bk to Bk+1 is αN, and
the jump rate from Bk to Bk+m is null for m ≥2. Thus, explosion is impossible.
3. Let the process be at x = (xi)1≤i≤N in RN
+. A natural simulation method lets the
process stay at x for an exponential duration of parameter
q(x) := αN + β

1≤i≤N
xi + γ

1≤i<j≤N
xixj.
Note that

1≤i<j≤N
xixj = 1
2

1≤i̸=j≤N
xixj = 1
2

1≤i≤N
xi

1≤j≤N
xj −1
2

1≤i≤N
x2
i
allows to compute q(x) in terms of s(x) = 
1≤i≤N xi and s2(x) = 
1≤i≤N x2
i .
After this, a coordinate 1 ≤i ≤N is chosen with probability α/q(x) to be
incremented by 1, with probability βxi/q(x) to be decreased by θxi for θ drawn
uniformly from [0,1], or with probability γ (xis(x) −x2
i )/2q(x) to be involved
in a collision, with a particle 1 ≤j ≤N for j ̸= i chosen with probability
xj/(s(x) −xj) and resulting in the replacement of xi and xj by z drawn uni-
formly in [0,xi + xj] and xi + xj −z.
4. One can avoid computing s2(x) by replacing q(x) by λ(x) := αN + βs(x) +
γ
2 s(x)2 in the above and further choosing with probability γ (xis(x))/2λ(x) that

242
Solutions to Selected Problems
particle i is involved in a collision, with particle 1 ≤j ≤N with probability
xj/s(x), and if j = i then the collision is ﬁctitious.
The sub-domain method with the Bk −Bk−1 can be tried to accelerate the
method.
6.2 (Jump Accumulation)
1. When at a point x in [an−1,an), the process waits an exponential time of pa-
rameter λn and then jumps to the point of [an,an+1) with the same barycentric
coordinates. When at 1, the process waits an exponential time of parameter λ and
jumps to a uniformly chosen point in [0,1].
2. The condition 
n≥1
1
λn < ∞implies that the jump instants accumulate in ﬁnite
time (the expectation of the limit is ﬁnite), and the evolution implies that 1 is
reached in ﬁnite time from any state. This implies that the process is deﬁned for
all times. Moreover, the description of the evolution is Markovian. Naively, the
generator acting on a bounded f yields the function of x given by

n≥1
1{an−1≤x<an}λn

f

an −x
an −an−1
an + x −an−1
an −an−1
an+1

−f (x)

−1{x=1}λ
 1
0

f (u) −f (x)

du
which is not necessarily bounded since limn→∞λn = ∞. If f is assumed to
be constant on a neighborhood of 0 then the resulting function is bounded, but
depending on the rate of increase of λn more general functions can be considered,
for instance with vanishing derivatives up to some order at 0.
3. The process cannot be simulated exactly, due to the accumulation of jumps. It
can be simulated approximately: select some threshold close enough to 0, and
above it replace the actual jumps with the mean evolution of the process, which
for an−1 ≤x < an is at speed
λn

an −x
an −an−1
an + x −an−1
an −an−1
an+1 −x

= λn
a2
n −an−1an+1 + x(an+1 −2an + an−1)
an −an−1
.
6.3 (Generalized Kac Equation)
1. This follows from (v1 cosθ −v2 sinθ)2 + (v1 sinθ + v2 cosθ)2 = (v1)2 + (v2)2.
2. (a) We use the superposition-decomposition properties of the Poisson processes.
Iteratively, from a state v = (vk)1≤k≤N, we draw an exponential r.v. of pa-
rameter N to determine the duration until the next jump instant, then i uni-
formly in {1,...,N}, then j uniformly in {1,...,N} −{i}, then θ accord-
ing to β(θ)dθ, and then we determine the next state by replacing vi by
vi cosθ −vj sinθ and vj by vi sinθ + vj cosθ in v. This simulation always
functions, since there cannot be any accumulation of jump instants.

Solutions to Selected Problems
243
(b) Using again the superposition-decomposition properties of the Poisson pro-
cesses, the random evolution is indeed that of a pure-jump Markov process
with said generator A . This generator is bounded by
2
N−1 × N(N−1)
2
× 2 =
2N, hence the process is well-deﬁned.
3. Let h : v = (vk)1≤k≤N →h(v) = N
k=1 vk. Let Mt = h(Vt) be the impetus of the
system, and mt = E(Mt) its expectation, t ≥0.
(a) The function β is even and thus
 π
−π sinθ β(θ)dθ = 0. Hence
A h(v) =
2
N −1(c −1)

1≤i<j≤N
(vi + vj) =
2
N −1(c −1)

1≤i̸=j≤N
vi
= 2(c −1)h(v).
(b) This follows from the forward Kolmogorov equation and the preceding re-
sult. Solving the differential equation is trivial.
(c) This follows from the fact that cosθ ≤1 with equality only for θ = 0, which
has null Lebesgue measure.
4. Assume here that 0 < ∥β∥1 < ∞.
(a) This is still a Markov process with generator bounded now by 2N∥β∥1.
(b) It is simulated as above, but with exponential r.v. with parameter N∥β∥1 and
draws of θ according to the probability density β(θ)
∥β∥1 .
(c) There will be a large number of iterations for simulating for any reasonable
duration of time.
5. (a) Then sinθ = O(θ) and cosθ −1 = O(θ2) uniformly, and we use a multi-
dimensional Taylor formula of order 1 (considering coordinates i and j for
1 ≤i < j ≤N).
(b) Use the above Taylor expansion, and the fact that β is even and sinθ = O(θ)
uneven, so that
 ε
0 sinθ β(θ)dθ = 0, and cosθ = 1 −θ2
2 + O(θ4).
(c) This is a Markov process which follows the ordinary differential equation
d
dt vi(t) = −bεvi(t),
1 ≤i ≤N,
in between jumps, similar to the above, taken with intensity N∥β(θ)1|θ|≥ε∥1.
It is well-deﬁned due to the bounded jump rate.
(d) The simulation of jumps is as above, replacing β(θ) by β(θ)1|θ|≥ε. In be-
tween jumps, use the solution vi(t) = vi(0)e−bεt of the differential equation.
Since limε→0∥β(θ)1|θ|≥ε∥1 = ∞, for ε small the jump rate becomes large.
7.1 (Long Time Behavior of the Euler Scheme Error)
1. From Itô’s formula,
S2
t = 1 +

2μ + σ 2 t
0
S2
θ dθ + 2σ
 t
0
S2
θ dWθ.

244
Solutions to Selected Problems
Since the coefﬁcients of the stochastic differential equation satisﬁed by (St) are
Lipschitz, the solution has ﬁnite moments of all order. Thus the stochastic inte-
gral in the right-hand side is a martingale and
ES2
t = 1 +

2μ + σ 2 t
0
ES2
θ dθ.
We deduce ES2
T = exp((2μ + σ 2)T ).
2. From the Euler scheme deﬁnition,
¯Sn
(p+1)T/n = ¯Sn
pT/n

1 + μT
n + σ(W(p+1)T/n −WpT/n)

=
p

j=0

1 + μT
n + σ(W(j+1)T/n −WjT/n)

.
Since the Brownian increments are Gaussian and independent random variables,
E
 ¯Sn
T
2 =

1 +

2μ + σ 2T
n + μ2 T 2
n2
n
= exp

2μ + σ 2
T

1 + C T 2
n + O
T 2
n2

,
where C is a constant which only depends on μ and σ. Consequently, one has
|C1(T )| = |C|T 2 exp((2μ + σ 2)T ). This function tends to inﬁnity with T .
3. Itô’s formula implies that
Er2
t = σ 2t −2μ
 t
0
Er2
θ dθ.
The solution to this linear differential equation is
Er2
t = σ 2
2μ

1 −exp(−2μt)

,
from which
Er2
p/n = σ 2
2μ

1 −exp(−2μp/n)

.
Now, in view of the Euler scheme deﬁnition,
E

¯rn
p/n
2 = E

¯rn
(p−1)/n
2

1 −μ
n
2
+ σ 2
n
= σ 2
n
1 −(1 −μ
n )2p
1 −(1 −μ
n )2

Solutions to Selected Problems
245
=
σ 2
2μ + O( 1
n)

1 −exp

2p log

1 −μ
n

.
For all a > 0 and b > 0 one has |e−a −e−b| ≤|b −a|. Therefore, there exists
a positive real number C which only depends on μ such that, for all n such that
1 −μ
n > 0, for all integer p,
Er2
p/n −E

¯rn
p/n
2 ≤C
n .
We observe that, contrary to Question 2, the error constant is bounded uni-
formly w.r.t. the numerical integration time horizon.
7.3 (Monotonicity and Convexity of x →Xt(x))
1. In view of (7.70), the sequence of processes Zx,ε converges in probability uni-
formly on compact time intervals. Thus a subsequence converges almost surely
to the limit explicited in Exercise 7.5. Thus the ﬂow Xt(x) is an almost surely
increasing (consequently, one-to-one) function of x.
2. At the ﬁrst discretization step one has
¯Xn
T/n(x) = x + b(x)T
n + σ(x)WT/n.
Choose b(x) ≡0 and σ(x) = sin(x). In such a case, the ﬂow of the Euler scheme
clearly is not almost surely one-to-one.
3. We deduce from the ﬁrst question that
d2
dx2 Xt(x) = exp
 t
0

b′
Xθ(x)

−1
2

σ ′
Xθ(x)
2

dθ +
 t
0
σ ′
Xθ(x)

dWθ

×
 t
0

b′′
Xθ(x)

−σ ′
Xθ(x)

σ ′′
Xθ(x)
 d
dx Xθ(x)dθ
+
 t
0
σ ′′
Xθ(x)
 d
dx Xθ(x)dWθ

.
4. Since f ′(x) = f ′(0) +
 x
0 f ′′(y)dy, the assumption on f ′′ implies that |f ′| is
bounded from above by a polynomial function in |x|. Thus, f does likewise.
5. Theorem 7.18 shows that
∂u
∂x (t,x) = E
	
f ′
Xt(x)
 d
dx Xt(x)

.
We deduce
∂2u
∂x2 (t,x) = E
	
f ′′
Xt(x)
 d
dx Xt(x)
2
+ E
	
f ′
Xt(x)
 d2
dx2 Xt(x)

.

246
Solutions to Selected Problems
Now, Proposition 7.9 shows that all moments of Xt(x) are bounded from above
by a polynomial function in |x|. In addition, Proposition 7.4 shows that, for all
adapted and bounded process (φt) and all integers m,
∃C > 0,
sup
0≤t≤T
E

 t
0
φθ dWθ

2m
≤C.
Observe ﬁnally
exp
 t
0
φθ dWθ

= 1 + 1
2
 t
0
exp
 θ
0
φs dWs

φ2
θ dθ
+
 t
0
exp
 θ
0
φs dWs

φθ dWθ.
Set TN := inf{t > 0;|
 t
0 φθ dWθ| ≥N}. From the preceding equality we have
Eexp
 t∧TN
0
φθ dWθ

= 1 + 1
2E
 t∧TN
0
exp
 θ
0
φs dWs

φ2
θ dθ
≤1 + C
 t
0
Eexp
 θ∧TN
0
φs dWs

dθ.
As at the end of the proof of Proposition 7.8, successively apply Gronwall’s
lemma and Fatou’s lemma. It follows that
∃C > 0, ∀0 ≤t ≤T,
Eexp
 t
0
φθ dWθ

≤C.
It is then easy to deduce that the function ∂2u
∂x2 (t,x) is bounded from above by a
polynomial function in |x|.
6. When σ = λx, one deduces from the third question
d2
dx2 Xt(x) = exp
 t
0

b′
Xθ(x)

−λ2
2

dθ +λWt
 t
0
b′′
Xθ(x)
 d
dx Xθ(x)dθ.
If in addition b′′, f ′ and f ′′ are positive functions, it results from the ﬁrst and
ﬁfth questions that d2
dx2 Xt(x) and thus ∂2u
∂x2 (t,x) are also positive functions.
7.4 (Estimates for the Derivatives of x →Xt(x))
1. Since the function g(t) is increasing, g(t) ≥g(θ) for all 0 ≤θ ≤t. Thus
max
θ≤t (Mθ) −g(t) ≤max
0≤θ≤t

Mθ −g(θ)

.
2. Itô’s formula implies
˜Mt := 1 + α
 t
0
˜Msφs dWs.

Solutions to Selected Problems
247
The process ( ˜Mt) therefore is a local martingale. In fact, it is a martingale: this
fact results from Novikov’s criterion (see Remark 8.1) since (φt) is a bounded
process. Alternatively, one can introduce the stopping times τN := inf{θ >
0; ˜Mθ ≥N} and observe
˜M2
t∧τN = 1 + α2
 t∧τN
0
˜M2
s φ2
s ds + 2α
 t∧τN
0
˜M2
s φs dWs
= 1 + α2
 t
0
˜M2
s∧τN φ2
s∧τN 1s≤τN ds + 2α
 t
0
˜M2
s∧τN φs∧τN 1s≤τN dWs,
from which
E ˜M2
t∧τN ≤1 + cα2
 t
0
E ˜M2
s∧τN ds.
Using Gronwall and Fatou’s lemmae as in the proof of Theorem 7.8, one obtains
E ˜M2
t < ∞, from which we conclude that (Mt) is a square integrable martingale.
3. Set ⟨M⟩t :=
 t
0(φs)2 ds. We have
P

max
0≤θ≤t Mθ ≥at

= P

max
0≤θ≤t αMθ −α2
2 ⟨M⟩t ≥αat −α2
2 ⟨M⟩t

≤P

max
0≤θ≤t

αMθ −α2
2 ⟨M⟩θ

≥αat −α2
2 ct

= P( max
0≤θ≤t

˜Mθ ≥exp

αat −α2
2 ct

≤exp

−αat + α2
2 ct

E ˜Mt,
where we used Theorem 7.3 to obtain the last inequality. Since ( ˜Mt) is a martin-
gale, E ˜Mt = E ˜M0 = 1, and thus
P

max
0≤θ≤t Mθ ≥at

≤exp

−αat + α2
2 ct

.
The right-hand side takes its minimal value at α = a
c , from which
P

max
0≤θ≤t Mθ ≥at

≤exp

−a2t
2c

.
For a := y
t , where y is a given positive number, we obtain
P

max
0≤θ≤t Mθ ≥y

≤exp

−y2
2ct

.

248
Solutions to Selected Problems
Since (−Mt) is a martingale, we deduce
P

|Mt| ≥y

≤P(Mt ≥y) + P(−Mt ≥y)
≤P

max
0≤θ≤t Mθ ≥y

+ P

max
0≤θ≤t(−Mθ) ≥y

≤2exp

−y2
2ct

.
Since
Eexp

ε(Mt)2
2

≤
∞

k=0
E
	
exp

ε(Mt)2
2

1k≤|Mt|<(k+1)

≤
∞

k=0
exp

ε(k + 1)2
2

P

k ≤|Mt|

≤2
∞

k=0
exp

ε(k + 1)2
2

exp

−k2
2ct

,
the expectation Eexp(ε (Mt)2
2
) is ﬁnite for all ε < 1
2c.
4. As seen in Exercise 7.5, we have
Y x
t = exp
 t
0

b′
Xx
θ

−1
2

σ ′2
Xx
θ

dθ +
 t
0
σ ′
Xx
θ

dWθ

,
0 ≤t ≤T.
Set μ := ∥b′ −1
2(σ ′)2∥∞, CT := ∥(σ ′)2∥∞T . For all y such that log(y) > μT ,
P

Y x
t > y

≤P
 t
0

b′
Xx
θ

−1
2σ 2
Xx
θ

dθ +
 t
0
σ ′
Xx
θ

dWθ ≥log(y)

≤P

 t
0
σ ′
Xx
θ

dWθ
 ≥log(y) −μt

≤2exp

−(log(y) −μt)2
2CT t

.
5. We have
E
ψ

Xx
T

Y x
T
3 ≤
#
E
ψ

Xx
T
6#
E

Y x
T
6.
In view of Theorem 7.9, Xx
T has ﬁnite moments of all orders, and Y x
T inherits the
same property (see Question 3).

Solutions to Selected Problems
249
8.1 (Importance Sampling for Stochastic Differential Equations)
1. As at the beginning of the proof of Theorem 7.14, let u(t,x) be the solution to
⎧
⎨
⎩
∂u
∂t (t,x) + L u(t,x) = 0
in [0,T ) × R,
u(T,x) = f (x),
x ∈R.
(A.1)
Denoting the function ∂u
∂x (t,x) by (∂xu)(t,x), Itô’s formula implies
f

XT (x)

= Ef

XT (x)

+
 T
0
(∂xu)

t,Xt(x)

· σ

Xt(x)

dWt,
a.s.
2. Observe that
E
XT (x) −ˆXT (x)
2
≤2E
 T
0
b(Xs) −ˆb( ˆXs)
ds
2
+ 2E
 T
0
σ(Xs) −ˆσ( ˆXs)
2 ds
≤CE
 T
0
b(Xs) −ˆb( ˆXs)
2 ds + 2E
 T
0
σ(Xs) −ˆσ( ˆXs)
2 ds.
In addition, as b is Lipschitz, denoting by C a constant which may change from
line to line but does not depend on ˆb nor on ˆσ, one gets
 T
0
b(Xs) −ˆb( ˆXs)
2 ds
≤2
 T
0
b(Xs) −b( ˆXs)
2 ds + 2
 T
0
b( ˆXs) −ˆb( ˆXs)
2 ds
≤C
 T
0
|Xs −ˆXs|2 ds + C∥b −ˆb∥2
∞.
Similar inequalities hold true when b is replaced by σ. One concludes by apply-
ing Gronwall’s lemma.
3. For example, one may use
d
dx XT (x) = exp
 t
0

b′(Xs) −1
2
σ ′(Xs)
2

ds +
 t
0
σ ′(Xs)dWs

,
a similar expression for d
dx ˆXT (x), a Taylor expansion of eα −eβ, and ﬁnally the
preceding question.
4. Since f is Lipschitz, one can bound from above
Ef

XT (x)

−Ef
 ˆXT (x)
2

250
Solutions to Selected Problems
by using the second question. Then, as the function σ is bounded,
E
 T
0

φ

Xs(x)

−ˆφ

Xs(x)

dWs
2
≤C
 T
0
E
∂xu

s,Xs(x)

−∂x ˆu(s,Xs)
2 ds
+ C∥σ −ˆσ∥2
∞
 T
0
E
∂x ˆu(s,Xs)
2 ds.
To conclude, it then remains to use
∂xu(t,x) = E

f ′
XT −t(x)
 d
dx XT −t(x)

,
and Question 3.
5. The random variable
Z := f

XT (x)

−
 T
0
(∂xu)

t,Xt(x)

σ

Xt(x)

dWt
has the same expectation as f (XT (x)) and has null variance (hence is determin-
istic). It is impossible to simulate since the function u(t,x) is unknown. One
instead chooses a proxy ( ˆXt) of (Xt) for which the function Ef ( ˆXT (x)) is ex-
plicitly known, and simulates
ˆZ := f
 ˆXT (x)

−
 T
0
(∂x ˆu)

t, ˆXt(x)

σ
 ˆXt(x)

dWt.
The preceding estimates allow one to estimate the variance reduction.
9.1 (Adaptive Monte Carlo Method)
1. (a) The increments Mn −Mn−1 = g(θn−1,Xn)−E(φ(X)) are square integrable,
hence so are the Mn. Also, E(Mn−Mn−1 |Fn−1)=E(g(θn−1,Xn)|Fn−1)−
E(φ(X)), and since θn−1 is Fn−1-measurable and Xn is independent from
Fn−1 and has the same law as X, it holds that E(g(θn−1,Xn)|Fn−1) =
F(θn−1) for F : θ ∈R →F(θ) = E(g(θ,X)) := E(φ(X)).
(b) Developing the square and using the above computations,
E

(Mn −Mn−1)2 |Fn−1

= E

g(θn−1,Xn) −E

φ(X)
2 |Fn−1

= E

g(θn−1,Xn)2 |Fn−1

−E

φ(X)
2
= m2(θn−1) −E

φ(X)
2
so that ⟨M⟩n = n
k=1(m2(θk−1) −E(φ(X))2). A simple equivalent is given
by (m2(θ∗) −E(φ(X))2)n = σ 2n.
(c) Using (1) of the admitted classical convergence result, limn→∞Mn
n = 0 a.s.

Solutions to Selected Problems
251
2. Using (2) of the classical convergence result, limn→∞Mn
σ√n = N (0,1) in law.
3. (a) Same arguments as for (Mn)n≥0.
(b) Same arguments as for (Mn)n≥0 yield ⟨¯M⟩n =n
k=1(m4(θk−1)−m2(θk−1)2).
A simple equivalent is (m4(θ∗) −m2(θ∗)2)n.
(c) The natural estimator is (with perhaps a factor
n
n−1)
1
n
n

k=1
g(θk−1,Xk)2 −
 n

k=1
g(θk−1,Xk)
2
.
The preceding results yield that 1
n
n
k=1 g(θk−1,Xk)2 and 1
n
n
k=1 m2(θk−1)
converge to the same limit, a.s. Clearly
1
n
n
k=1 m2(θk−1) converges to
m2(θ∗). Hence, this estimator converges to σ 2 = m2(θ∗) −E(φ(X))2, a.s.

References
1. Anderson, W.J.: Continuous-Time Markov Chains. Springer Series in Statistics: Probability
and Its Applications. Springer, New York (1991)
2. Ané, C., Blachère, S., Chafaï, D., Fougères, P., Gentil, I., Malrieu, F., Roberto, C., Scheffer,
G.: Sur les inégalités de Sobolev logarithmiques. Panoramas et Synthèses, vol. 10. Société
Mathématique de France, Paris (2000)
3. Arouna, B.: Robbins-Monro algorithms and variance reduction in ﬁnance. J. Comput. Finance
7(2), 35–61 (2003)
4. Asmussen, S., Glynn, P.W.: Stochastic Simulation: Algorithms and Analysis. Stochastic Mod-
elling and Applied Probability, vol. 57. Springer, New York (2007)
5. Benaïm, M., El Karoui, N.: Chaînes de Markov et simulations; Martingales et stratégies. Édi-
tions de l’École Polytechnique, Palaiseau (2004)
6. Billingsley, P.: Convergence of Probability Measures, 2nd edn. Wiley Series in Probability
and Statistics: Probability and Statistics. Wiley, New York (1999)
7. Cercignani, C., Illner, R., Pulvirenti, M.: The Mathematical Theory of Dilute Gases. Applied
Mathematical Sciences, vol. 106. Springer, New York (1994)
8. Clark, J.M.C., Cameron, R.J.: The maximum rate of convergence of discrete approximations
for stochastic differential equations. In: Stochastic Differential Systems. Lecture Notes in Con-
trol and Information Sciences, vol. 25, pp. 162–171. Springer, Berlin (1980)
9. Davis, M.H.A.: Markov Models and Optimization. Monographs on Statistics and Applied
Probability, vol. 49. Chapman & Hall, London (1993)
10. Devroye, L.: Nonuniform Random Variate Generation. Springer, New York (1986)
11. Dufﬁe, D., Glynn, P.: Efﬁcient Monte Carlo simulation of security prices. Ann. Appl. Probab.
5(4), 897–905 (1995)
12. Duﬂo, M.: Algorithmes Stochastiques. Mathématiques & Applications, vol. 23. Springer,
Berlin (1996)
13. Duﬂo, M.: Random Iterative Models. Applications of Mathematics, vol. 34. Springer, Berlin
(1997)
14. Faure, O.: Simulation du mouvement brownien et des diffusions. Ph.D. thesis, École Nationale
des Ponts et Chaussées (1992)
15. Feller, W.: An Introduction to Probability Theory and Its Applications, vol. I, 3rd edn. Wiley,
New York (1968)
16. Feller, W.: An Introduction to Probability Theory and Its Applications, vol. II, 2nd edn. Wiley,
New York (1971)
17. Fournié, E., Lasry, J.M., Lebuchoux, J., Lions, P.L.: Applications of Malliavin calculus to
Monte-Carlo methods in ﬁnance. II. Finance Stoch. 5(2), 201–236 (2001)
18. Fournié, E., Lasry, J.M., Lebuchoux, J., Lions, P.L., Touzi, N.: Applications of Malliavin
calculus to Monte Carlo methods in ﬁnance. Finance Stoch. 3(4), 391–412 (1999)
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1, © Springer-Verlag Berlin Heidelberg 2013
253

254
References
19. Gentle, J.E.: Random Number Generation and Monte Carlo Methods, 2nd edn. Statistics and
Computing. Springer, New York (2003)
20. Glasserman, P.: Monte Carlo Methods in Financial Engineering. Applications of Mathematics,
Stochastic Modelling and Applied Probability, vol. 53. Springer, New York (2004)
21. Graham, C., Kurtz, T.G., Méléard, S., Protter, P.E., Pulvirenti, M., Talay, D.: In: Talay, D.,
Tubaro, L. (eds.) Probabilistic Models for Nonlinear Partial Differential Equations, Lec-
tures given at the 1st Session and Summer School Held in Montecatini Terme, May 22–30,
1995. Lecture Notes in Mathematics, vol. 1627. Springer, Berlin (1996). Fondazione C.I.M.E.
[C.I.M.E. Foundation]
22. Graham, C., Méléard, S.: Stochastic particle approximations for generalized Boltzmann mod-
els and convergence estimates. Ann. Probab. 25(1), 115–132 (1997)
23. Ikeda, N., Watanabe, S.: Stochastic Differential Equations and Diffusion Processes, 2nd edn.
North-Holland Mathematical Library, vol. 24. North-Holland, Amsterdam (1989)
24. Jacod, J., Protter, P.: Probability Essentials, 2nd edn. Universitext. Springer, Berlin (2004)
25. Kanagawa, S.: On the rate of convergence for Maruyama’s approximate solutions of stochastic
differential equations. Yokohama Math. J. 36(1) (1988)
26. Karatzas, I., Shreve, S.E.: Brownian Motion and Stochastic Calculus, 2nd edn. Graduate Texts
in Mathematics, vol. 113. Springer, New York (1991)
27. Kebaier, A.: Statistical Romberg extrapolation: A new variance reduction method and appli-
cations to option pricing. Ann. Appl. Probab. 15(4), 2681–2705 (2005)
28. Kloeden, P.E., Platen, E.: Numerical Solution of Stochastic Differential Equations. Applica-
tions of Mathematics, vol. 23. Springer, Berlin (1992)
29. Kuipers, L., Niederreiter, H.: Uniform Distribution of Sequences. Pure and Applied Mathe-
matics. Wiley-Interscience, New York (1974)
30. Kunita, H.: Stochastic differential equations and stochastic ﬂows of diffeomorphisms. In:
École d’été de probabilités de Saint-Flour, XII—1982. Lecture Notes in Math., vol. 1097,
pp. 143–303. Springer, Berlin (1984)
31. Kushner, H.J., Yin, G.G.: Stochastic Approximation Algorithms and Applications. Applica-
tions of Mathematics, vol. 35. Springer, New York (1997)
32. Lapeyre, B., Pardoux, É., Sentis, R.: Introduction to Monte-Carlo Methods for Transport and
Diffusion Equations. Oxford Texts in Applied and Engineering Mathematics, vol. 6. Oxford
University Press, Oxford (2003). Translated from the 1998 French original by Alan Craig and
Fionn Craig
33. L’Ecuyer, P.: Random number generation. In: Handbook of Computational Statistics, pp. 35–
70. Springer, Berlin (2004)
34. L’Ecuyer, P., Hellekalek, P.: Random number generators: Selection criteria and testing. In:
Random and Quasi-random Point Sets. Lecture Notes in Statist., vol. 138, pp. 223–265.
Springer, New York (1998)
35. L’Ecuyer, P., Simard, R.: TestU01: A C library for empirical testing of random number
generators. ACM Trans. Math. Softw. 33(4), 40 (2007), Art. 22. http://dx.doi.org/10.1145/
1268776.1268777
36. Milstein, G.N.: A method with second order accuracy for the integration of stochastic differ-
ential equations. Teor. Verojatnost. i Primenen. 23(2), 414–419 (1978)
37. Milstein, G.N.: Numerical Integration of Stochastic Differential Equations. Mathematics and
Its Applications, vol. 313. Kluwer Academic, Dordrecht (1995). Translated and revised from
the 1988 Russian original
38. Milstein, G.N., Tretyakov, M.V.: Stochastic Numerics for Mathematical Physics. Scientiﬁc
Computation. Springer, Berlin (2004)
39. Newton, N.J.: Variance reduction for simulated diffusions. SIAM J. Appl. Math. 54(6), 1780–
1805 (1994)
40. Niederreiter, H.: Random Number Generation and Quasi-Monte Carlo Methods. CBMS-NSF
Regional Conference Series in Applied Mathematics, vol. 63. Society for Industrial and Ap-
plied Mathematics, Philadelphia (1992)

References
255
41. Niederreiter, H., Shparlinski, I.E.: Recent advances in the theory of nonlinear pseudorandom
number generators. In: Monte Carlo and Quasi-Monte Carlo methods, Hong Kong, 2000,
pp. 86–102. Springer, Berlin (2002)
42. Nualart, D.: Malliavin Calculus and Its Applications. CBMS Regional Conference Series in
Mathematics, vol. 110. Conference Board of the Mathematical Sciences, Washington (2009)
43. Petrov, V.V.: Sums of Independent Random Variables. Springer, New York (1975). Trans-
lated from the Russian by A.A. Brown, Ergebnisse der Mathematik und ihrer Grenzgebiete,
Band 82
44. Shiryayev, A.N.: Probability. Graduate Texts in Mathematics, vol. 95. Springer, New York
(1984). Translated from the Russian by R.P. Boas
45. Sznitman, A.S.: Topics in propagation of chaos. In: Hennequin, P.L. (ed.) Ecole d’été de Prob-
abilités de Saint-Flour XIX–1989. Lecture Notes in Mathematics, vol. 1464, Chap. 3, pp.
165–251. Springer, Berlin (1991)
46. Talay, D., Tubaro, L.: Expansion of the global error for numerical schemes solving stochastic
differential equations. Stoch. Anal. Appl. 8(4), 483–509 (1990)
47. Williams, D.: Probability with Martingales. Cambridge Mathematical Textbooks. Cambridge
University Press, Cambridge (1991)

Index
Symbols
M/M/∞queue, 112
A
Almost sure, almost surely (a.s.), 14
B
Bracket
Doob–Meyer, 158
duality, 90, 107, 109, 122, 134, 163
Brownian motion
standard, 57, 60, 86, 156
multidimensional, 17, 159
C
C.d.f., see Cumulative distribution function
Central Limit Theorem, 40
Characteristic function, 39
CLT, see Central Limit Theorem
Collision, 146, 147
Conditional expectation, 25
Conﬁdence interval
accuracy, risk or conﬁdence level, 41, 45
asymptotic, 41
non-asymptotic, 45, 53
Convergence
almost sure, 14
in law, 38
Cumulative distribution function, 22, 40
Curse of dimensionality, 7, 8, 148
Cylindrical set, 68
D
Distance
entropic, 208
Kullback–Leibler, 209
E
Empirical mean, 13
Entropy w.r.t. a law, 48
tensorization property, 48
Equation
Chapman–Kolmogorov, 95, 125
Fokker–Planck, 106, 135, 144, 147, 164
integro-differential, 17, 135, 144, 147, 148
kinetic, 15, 16, 136, 146–148, 165
as balance equation, 147
Boltzmann, 149
Kac, 152
transport-diffusion, 165
Kolmogorov, 99, 105, 109, 134, 144, 147,
164
as balance equation, 107, 135, 136, 144,
147
ordinary differential, 136, 216
parabolic partial differential, 164, 176
Feynman–Kac formula, 178
local estimates, 186
stochastic differential, 161, 164
existence and uniqueness, pathwise and
in law, 161
ﬂow, 193, 194
strong and weak solution, 162
transport, see Kinetic
Error analysis
Monte Carlo method, approximate, 38, 177
F
Filtration, xiii, 27, 29, 155
usual conditions, 155
Finite-dimensional marginals, 69
Formula
change of variables, 160
C. Graham, D. Talay, Stochastic Simulation and Monte Carlo Methods,
Stochastic Modelling and Applied Probability 68,
DOI 10.1007/978-3-642-39363-1, © Springer-Verlag Berlin Heidelberg 2013
257

258
Index
Formula (cont.)
Feynman–Kac, 106, 135, 144, 178
Itô, 160, 162
Free transport, 147
G
Girsanov transform, 57, 199, 200, 205, 225
I
Inequality
Burkholder–Davis–Gundy, 159
concentration, 53, 62, 85
Doob Lp maximal, 159
Gronwall, 168
logarithmic Sobolev, 49, 53
Intensity, 71, 97, 128
estimation of, 84
Inter-arrival, 70
Itô
process, 160
stochastic calculus, 155
stochastic integral, 155–157
J
Join the shortest queue, 114
Jump accumulation and compensation,
150–153
Jump instant, 70
K
Kernel
induced, 128
jump measure and law, 128
multiplication notation, 122
transition, and its actions, 122
Kolmogorov zero-one Law, 33
L
Lemma
Borel–Cantelli, 174
Gronwall, 168
Local martingale, see Localization
Localization, 169
LSI, see Inequality, logarithmic Sobolev
M
Markov chain
continuous-time, 91
induced, 97, 128
Markov process
generator and martingales, 163
homogeneous or inhomogeneous, 91
inﬁnitesimal point of view, 96, 127, 140,
148
semigroup and generator, 123, 125, 126
Markov process, diffusion type, 162, 163
semigroup and generator, 163
Markov process, discrete space, 89
constructions, 99
explosion or existence, 101–103, 117
ﬁctitious jump method, 104, 115
fundamental simulation, 103, 114
generator and martingales, 118
q-matrix, 97
semigroup and generator, 92, 95, 97, 129
Markov process, ODE with jumps, 137
explosion or existence, 143, 144
ﬁctitious jump method, 141
law ﬁrst jump instant, 138
semigroup and generator, 140, 142, 145
sub-domain method, 142
true jump method, 143
Markov process, pure jump, 126
explosion or existence, 131, 132
ﬁctitious jump method, 133
fundamental simulation, 132
semigroup and generator, 126, 128
Markov property, strong or simple
continuous space, 123, 125
diffusion type, 162
discrete space, 92, 94, 98
Poisson process, 75, 76, 79
Markovian modeling, 67
Martingale, 27, 118, 158, 217
backward, 29–31
sub —, super —, 27
Matrix
induced, 97
multiplication notation, 90
transition, and its actions, 90
Mean free path, 146
Moments
Euler scheme, 172, 173
solutions of SDE, 171
Monte Carlo method, 8, 14, 18, 38, 45, 53, 54,
107, 135, 136, 144, 148, 149, 153,
164, 177
adaptive, 59, 208, 224, 225
mean-ﬁeld, 150
N
Newton–Raphson method, 23
Norm
operator, 108, 134
total variation, 108, 134
Notation, general, xiii
Novikov condition, 200

Index
259
O
ODE, see Ordinary differential equation
Operator
bounded — algebra, 108, 133
exponential, 111
integro-differential, 140, 145, 147
kernel, 129
second order differential, 163, 164, 176
spectral decomposition, 111
Option
Bermuda European, 57, 59
European, 55, 202, 211
Ordinary differential equation
existence and uniqueness, 137
ﬂow, 137
P
PDMP, 136
Phase space, 7, 15, 146
Poisson or exponential clock, 100
Poisson process
Brownian limit, 86
characterization, 71
compound, 75
inﬁnitesimal point of view, 73
inhomogeneous, 85
law of the instants, 85
simulation, 80, 81
approximate, 84
estimation of duration, 84
of independent, 81
SLLN and CLT, 83
superposition-decomposition, 77
Probabilistic representation, 7, 14, 107, 135,
136, 144, 149, 164, 178
Probability space, xiii
ﬁltered, xiii, 155
Process
adapted, xiii, 27, 155
birth-death, 113
counting, 70
independent stationary increments, 70, 156
law of a, 69
measurable, 156
point, 69
degenerate, 74
Poisson
Markov property, 75
renewal, 82
simulation of renewal —, 82
Product σ-ﬁeld, 68
Propagation of chaos, 150
R
Random variable
exponential, 22, 23, 74
inﬁmum of independent, 80
gamma, 85
Gaussian, 21
law of a —, 37
memoryless on R+, 74
Rate, see Intensity
Romberg–Richardson extrapolation, 185, 209
S
Sample path, 156
continuous C (R+,V ), 68
right-continuous with left limits, Skorohod
space D(R+,V ), 68
Scheme
Euler, 166, 176, 216
convergence rate, 174, 175, 180, 185
implicit, 191
interpolation, 172
long time error analysis, 191
Milstein, 166
convergence rate, 175, 185
SDE, see Stochastic differential equation
Simulation of random variable
c.d.f. inversion method, 22
Cauchy, 23
congruential method, 19, 20
discrete, 20
exponential, 23, 80
Gaussian, 21
multidimensional, 22
Poisson, 33, 81
pseudo-random number, 19, 20
rejection method, 23
uniform, 20
SLLN, see Strong Law of Large Numbers
Stochastic algorithm, 208, 213–215
for variance reduction, 223
gain, 214
long time behavior, 217, 220
Lyapunov function, 219, 220
martingale increment, 214
Oja’s algorithm, 228
projection, 208, 209, 215, 225
Strong Law of Large Numbers, 13, 14, 30
T
Theorem
Berry–Esseen, 43
Bikelis, 45
Cauchy–Lipschitz, 137
central limit, see Central Limit Theorem

260
Index
Theorem (cont.)
Esseen, 42
Girsanov, see Girsanov transform
optional sampling, 27, 159
Time
hitting, 94, 148, 149
stopping, 27, 75, 76, 79, 92, 94, 156
Trajectory, see Sample path
U
Uniform integrability, 31, 34
Upcrossings, number of, 28
V
Variance of the simulation, 54
Variance reduction, 54
control variate, 54, 200
European Put–Call, 55
importance sampling, 55, 221
delta European option, 211
Girsanov transform, 58, 59, 206
stochastic algorithm, 223
stochastic differential equation, 210
stratiﬁcation, 56
sensitivity analysis, 202
statistical Romberg, 209
W
Wiener process, see Brownian motion

