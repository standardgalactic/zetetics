CSC 411 Lecture 19-20: Ensembles
Ethan Fetaya, James Lucas and Emad Andrews
University of Toronto
CSC411 Lec19
1 / 42

Today
Ensemble Methods
Bagging
Random forest
Boosting
CSC411 Lec19
2 / 42

Ensemble methods
Back to supervised learning.
Ensemble of predictors is a set of predictors whose individual decisions are
combined in some way to classify new examples
Simplest approach:
1. Generate multiple classiﬁers
2. Each votes on test instance
3. Take majority/average as prediction
Classiﬁers are diﬀerent due to diﬀerent sampling of training data, or
randomized parameters within the classiﬁcation algorithm
Aim: take simple mediocre algorithm and transform it into a super classiﬁer
without requiring any fancy new algorithm
CSC411 Lec19
3 / 42

Ensemble methods: Overview
Diﬀer in training strategy, and combination method
▶Parallel training with diﬀerent training sets
1. Bagging (bootstrap aggregation) – train separate models on
overlapping training sets, average their predictions
▶Sequential training, iteratively re-weighting training examples so
current classiﬁer focuses on hard examples: boosting
▶Parallel training with objective encouraging division of labor: mixture
of experts (not covered)
Notes:
▶Also known as meta-learning
CSC411 Lec19
4 / 42

Bias-Variance decomposition
Best way to understand Bagging is via the bias-variance decomposition
Consider regression with L2 loss and deﬁne h∗(x) = E[t|x] to be the
Bayes-optimal classiﬁer.
Deﬁne the ML algorithm prediction (trained on data D) as y(x; D)
We are interested in breaking ED,x,t[(t −y(x; D))2] into its core
components.
▶The expected test error when we sample a random training set.
First step:
Et[(t −y(x; D))2|D, x] = Et[(t −h∗(x) + h∗(x) −y(x; D))2|D, x] =
Et[(t −h∗(x))2|D, x] + Et[(h∗(x) −y(x; D))2|D, x]
The third term disappears because E[t|x] = h∗(x)
ED,x,t[(t −y(x; D))2] = ED,x,t[(t −h∗(x))2] + ED,x,t[(h∗(x) −y(x; D))2]
The ﬁrst term is called the noise and we have no control over it.
CSC411 Lec19
5 / 42

Bias-Variance decomposition
ED,x,t[(t −y(x; D))2] = ED,x,t[(t −h∗(x))2] + ED,x,t[(h∗(x) −y(x; D))2]
We can use the same trick to break down the second term
ED[(h∗(x) −y(x; D))2|x] = ED[(h∗(x) −ED[y(x; D)] + ED[y(x; D)] −y(x; D))2|x] =
ED[(h∗(x) −ED[y(x; D)])2|x] + ED[(y(x; D) −ED[y(x; D)])2|x]
We get: Error = (bias)2 + Variance + noise
(bias)2 = Ex[(h∗(x) −E[y(x; D)])2]
Variance = ED,x[(y(x; D) −E[y(x; D)])2]
noise = ED,x,t[(t −h∗(x))2]
If we overﬁt we have high variance
CSC411 Lec19
6 / 42

Example:
Fitting a polynomial to 1d input (linear regression) for various regularization
values.
We can sample diﬀerent datasets and see the variance in predictions and
bias (loss of averaged prediction)
Left: Predictions trained on various sampled datasets. Low variance
Right: The mean prediction. High bias
CSC411 Lec19
7 / 42

Example:
Left: Predictions trained on various sampled datasets. high variance
Right: The mean prediction. Low bias
CSC411 Lec19
8 / 42

Example:
Left: Predictions trained on various sampled datasets. Higher variance
Right: The mean prediction. Lower bias
CSC411 Lec19
9 / 42

Bagging
We can decompose the error to bias and variance.
Over-ﬁtting models have high variance (hopefully low bias).
How can we reduce variance?
Simple way - you average i.i.d samples - Var( 1
m
P
i xi) = 1
mVar(x1)
Problem: The samples are other training sets, we only have one.
Solution: Bootstrapping, generating many training sets from our one set.
How is this done? By sampling with replacement.
We can train a separate predictor for each training example and average
predictions.
If samples have correlation ρ: Var( 1
m
P
i xi) = 1
m(1 −ρ)σ2 + ρσ2
Works well if they have low correlation.
CSC411 Lec19
10 / 42

Bootstrapping
Bootstrapping is a classical statistics technique.
Example: We have an unbiased estimation of some parameter and want to
estimate the variance (e.g. for conﬁdence intervals).
How can you estimate variance? You sample more from the data
distribution and estimate the variance using these extra samples.
What do you do if you cannot sample more? Bootstrap! Replace the data
distribution with the empirical distribution.
Sampling from the empirical distribution is the same as sampling with
replacement from your dataset.
Some theoretical justiﬁcation (e.g. Glivenko-Cantelli theorem)
Bagging = Bootstrap Aggregation
CSC411 Lec19
11 / 42

Bagging algorithm
Input: dataset D, ML algorithm A, number of bags N.
For i = 1,...,N:
▶Generate dataset Di by sampling with replacement from D (same
number of elements).
▶fi = A(Di)
return: f1, ..., fN.
Doing predictions: given new example x return 1
N
P
i fi(x) (or majority for
classiﬁcation)
Thats it!
CSC411 Lec19
12 / 42

Random Forest
Bagging reduces overﬁtting by averaging predictions.
Works well with decision trees. Why?
▶They overﬁt easily (high variance to reduce).
▶Fast to perform inference.
▶Powerful method on relational data
Random forest is decision trees+ bagging + one more trick.
To reduce correlation even more - each split only considers a random subset
of features.
How do decision boundaries look like?
CSC411 Lec19
13 / 42

Out-of-bag estimation
In bagging there is a nice way to cheaply estimate test loss
Each training example only appears in some of the ”bagged” trees.
▶Probability of not being picked is ∼1/e
OOB estimation: We predict each training example using all the trees that
did not contain this in their training data.
CSC411 Lec19
14 / 42

Bagging Overview
Bagging reduces overﬁtting by averaging predictions.
Used in most competition winners
▶Even if a single model is great, a small ensemble usually helps.
Easy to parallelize.
Limitations:
▶Does not reduce bias.
▶There is still correlation between classiﬁers.
Random forest solution: Add more randomness.
OOB estimation reduces the need of validation/cross-validation.
CSC411 Lec19
15 / 42

Boosting overview
Boosting is another ensemble method.
It reduces bias by making each classiﬁer focus on previous mistakes
Training is sequentially.
We will talk about AdaBoost (binary classiﬁcation)
Started by a theoretical question: Can you take a ”weak” classiﬁer that does
ϵ better then chance and ”boost” it to get low training error?
Answer is yes! Our classiﬁer is H(x) = sign (P
i αih(xi)) with hi weak
classiﬁers.
Note that we sum predictions (after sign) so sum of linear classiﬁers isn’t a
linear classiﬁer!
CSC411 Lec19
16 / 42

AdaBoost
The ﬁrst practical boosting algorithm is adaBoost (adaptive boosting).
The idea: At each iteration you reweigh the training sample, giving larger
weight to points that where classiﬁed wrongly and train a new weak
classiﬁer.
The weak learner needs to minimize weighted accuracy.
Assume the weak learner can get ϵ better then chance (1/2).
CSC411 Lec19
17 / 42

AdaBoost Algorithm
Input: {x(n), t(n)}N
n=1, and WeakLearn: learning procedure, produces classiﬁer H(x)
Initialize example weights: Dm
n (x) = 1/N
For m=1:M
▶hm(x) = WeakLearn({x}, t, w), ﬁt classiﬁer by minimizing
Jm =
N
X
n=1
D(n)
m [hm(xn) ̸= t(n)]
▶Compute weighted error rate
ϵm =
Jm
P D(n)
m
▶Compute classiﬁer coeﬃcient αm = 1
2 log 1−ϵm
ϵm
▶Update data weights
D(n)
m+1 = D(n)
m exp

−t(n)αmhm(x(n))

Final model
H(x) = sign(F(x)) = sign(
M
X
m=1
αmhm(x))
CSC411 Lec19
18 / 42

AdaBoost Example
ϵt is the weighted error, assuming less then 1/2.
αt = 1
2 log( 1−ϵt
ϵt ) measures the classiﬁer quality.
Weight the binary prediction of each classiﬁer by the quality of that classiﬁer:
H(x) = sign(F(x)) = sign
 M
X
m=1
αmym(x)
!
This is how to do inference, i.e., how to compute the prediction for each
new example.
CSC411 Lec19
19 / 42

AdaBoost Example
Training data
[Slide credit: Verma & Thrun]
CSC411 Lec19
20 / 42

AdaBoost Example
Round 1
[Slide credit: Verma & Thrun]
CSC411 Lec19
21 / 42

AdaBoost Example
Round 2
[Slide credit: Verma & Thrun]
CSC411 Lec19
22 / 42

AdaBoost Example
Round 3
[Slide credit: Verma & Thrun]
CSC411 Lec19
23 / 42

AdaBoost Example
Final classiﬁer
[Slide credit: Verma & Thrun]
CSC411 Lec19
24 / 42

AdaBoost example
Each ﬁgure shows the number m of base learners trained so far, the decision
of the most recent learner (dashed black), and the boundary of the ensemble
(green)
CSC411 Lec19
25 / 42

Algorithm analysis
We will now show that the overall training error decreases exponentially
(and it will explain αt)
Theorem:
Let ϵm be the WL error at iteration m and deﬁne γm = 1/2 −ϵm. The
training loss of the boosted classiﬁer H(x) = sign
PM
m=1 αmhm(x)

LS(H) = 1
N
N
X
i=1
1[H(x(i)) ̸= t(i))] ≤exp
 
−2
M
X
m=1
γ2
m
!
If we assume γm ≥γ then we can simplify the bound to exp
 −2γ2M

CSC411 Lec19
26 / 42

Proof Intuition
The idea before we dive into the math:
The boosted classiﬁer does a (weighted) majority voting.
For it to make a mistake on x(i) most (weighted) rounds must be erroneous.
Weight of x(i) increases exponentially with mistakes so it has a large weight.
The weak classiﬁer is better than chance so the total weight decreases.
This means there can only be few items with large weight so few mistakes.
CSC411 Lec19
27 / 42

Proof I *
Proof: We have F(x) = PM
m=1 αmhm(x) and deﬁne H(x) = sign(F(x)).
We can write D(i)
M using the algorithm recursive formula:
D(i)
M = D(i)
M−1 exp(−αMt(i)hM(x)) =
D(i)
M−2 exp(−αM−1t(i)hM−1(x)) exp(−αMt(i)hM(x)) = D(i)
1 exp(−t(i)F(x(i)))
Next we note that the 0-1 loss is bounded by the exponential loss
1[H(x) ̸= t] ≤exp(−tF(x))
We now have
LS(H) =
N
X
i=1
D(i)
1 1[H(x(i)) ̸= t(i)] ≤
N
X
i=1
D(i)
1 exp(−t(i)F(x(i))) =
N
X
i=1
D(i)
M
The number of mistakes is bounded by the total weight!
CSC411 Lec19
28 / 42

Proof II *
We need to bound the total weight ZM = PN
i=1 D(i)
M .
Zm+1 =
N
X
i=1
D(i)
m+1 =
N
X
i=1
D(i)
m exp(−αm+1t(i)hm+1(x(i)))
=
X
t(i)=hm+1(x(i))
D(i)
m e−αm+1 +
X
t(i)̸=hm+1(x(i))
D(i)
m eαm+1
= Zm
 e−αm+1(1 −ϵm+1) + ϵm+1eαm+1
Can show αm+1 picked by the algorithm minimizes this term and it is equal to
Zm
q
1 −4γ2
m+1.
This gives a bound of QM
m=1
p
1 −4γ2m ≤exp(−2 P γ2
m) ﬁnishing the proof (with
a few last steps skipped)
CSC411 Lec19
29 / 42

AdaBoost generalization
We have seen how AdaBoost training loss converges to zero, what about
test loss?
Can show the complexity (deﬁned in some manner) grows linearly with
iterations.
If you run AdaBoost long enough it can overﬁt.
CSC411 Lec19
30 / 42

AdaBoost generalization
However, many times it does not.
Sometimes the test error decreases even after the training error is zero!
How does that happen?
CSC411 Lec19
31 / 42

AdaBoost alternative viewpoint
Another way to see AdaBoost sheds some light on this.
We deﬁned F(x) = PM
m=1 αmhm(x) and deﬁne H(x) = sign(F(x)). Can
think of AdaBoost as a greedy optimization of the exponential loss
exp(−t(i)F(x(i)))
Can show this leads to a large margin.
Can show the margin leads to good generalization.
The paper for whoever is interested https:
//www.cc.gatech.edu/~isbell/tutorials/boostingmargins.pdf
CSC411 Lec19
32 / 42

AdaBoost alternative viewpoint
How do we see this other viewpoint?
Deﬁne Ft(x) = Pt
m=1 αmhm(x). If we ﬁx Ft and try to ﬁnd Ft+1 that
maximizes
1
N
N
X
i=1
exp(−t(i)Ft+1(x(i))) = 1
N
N
X
i=1
exp(−t(i)Ft(x(i))) exp(−t(i)αt+1ht+1x(i))
N
X
i=1
D(i)
t exp(−t(i)αt+1ht+1x(i))
If ht+1 has weighted accuracy ϵ then the optimal α is the one used by
AdaBoost and the total loss is 2
p
ϵ(1 −ϵ)
This is minimized when ϵ is minimized - so ht+1 should minimized the
weighted accuracy which is what AdaBoost does.
CSC411 Lec19
33 / 42

Loss Functions
Misclassiﬁcation: 0/1 loss
Exponential loss: exp(−t · f (x)) (AdaBoost)
Squared error: (t −f (x))2
Soft-margin support vector (hinge loss): max(0, 1 −t · y)
CSC411 Lec19
34 / 42

An impressive example of boosting
Viola and Jones created a very fast face detector that can be scanned across
a large image to ﬁnd the faces.
The base classiﬁer/weak learner just compares the total intensity in two
rectangular pieces of the image.
▶There is a neat trick for computing the total intensity in a rectangle in
a few operations.
▶So its easy to evaluate a huge number of base classiﬁers and they are
very fast at runtime.
▶The algorithm adds classiﬁers greedily based on their quality on the
weighted training cases.
CSC411 Lec19
35 / 42

AdaBoost in Face Detection
Famous application of boosting: detecting faces in images
Few twists on standard algorithm
▶Pre-deﬁne weak classiﬁers, so optimization=selection
▶Change loss function for weak learners: false positives less costly than
misses
▶Smart way to do inference in real-time (in 2001 hardware)
CSC411 Lec19
36 / 42

AdaBoost Face Detection Results
CSC411 Lec19
37 / 42

Boosting recap
Boosting is an ensemble method that reduces bias
We have shown AdaBoost a boosting algorithm for binary classiﬁcation.
Viewing AdaBoost as a greedy optimization of the exponential loss lead to
many extensions.
▶Boosting for ranking (RankBoost)
▶Boosting for multiclass classiﬁcation
▶Boosting for regression
▶Gradient boosting (in tutorial)
Exponential loss also shows this isn’t robust to outliers (some extensions try
to ﬁx this)
Quiet resistant to overﬁtting but can still overﬁt
Usually used with decision stumps, axis aligned or linear classiﬁers.
CSC411 Lec19
38 / 42

Ensembles recap
Ensembles combine classiﬁers to improve performance.
Boosting
▶reduce bias.
▶Increases variance (large ensemble can cause overﬁtting).
▶Sequential.
▶High dependency between ensemble elements.
Bagging can reduce variance
▶reduce variance (large ensemble can’t cause overﬁtting).
▶Bias isn’t changed
▶Parallel.
▶Minimizes correlation between ensemble elements.
CSC411 Lec19
39 / 42

Supervised learning recap
This was the last lecture about supervised learning, what have we seen so far?
We have seen various ML algorithms - each has its pros and cons.
▶No silver-bullet (not even deep learning), need to ﬁt your solution to
the problem.
Need to understand the inductive bias of each algorithm
▶Can be explicit, e.g. linear classiﬁer.
▶Can be implicit, e.g. nearest neighbor.
Many times (classiﬁcation) you cannot optimize the loss you care about.
▶Be sure you understand what it is your optimizing.
▶Does it makes sense as a surrogate loss?
How do you optimize?
▶Analytic solution (rare cases)
▶Gradient descent/SGD
▶EM algorithm
▶Other alternatives exist, but SGD is the most common.
CSC411 Lec19
40 / 42

Kaggle survey
Recent survey on Kaggle on what ML methods people use
Classic methods like logistic regression still dominate!
Most where covered in this course.
CSC411 Lec19
41 / 42

Kaggle survey - data
Why isn’t everyone just using deep learning?
Deep learning is great for vision/text but not the best at relational data.
Vision benchmarks dominate the academic ML community, but in industry
there are a lot diﬀerent tasks.
Most kaggle competitions are won with random forest/gradient boosting.
CSC411 Lec19
42 / 42

