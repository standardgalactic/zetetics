Overview
Auditory Sensing Systems:
Overview
Rodica Curtu1,2 and Susan Denham3
1Department of Mathematics, University of Iowa,
Iowa City, IA, USA
2Iowa Neuroscience Institute, University of Iowa,
Iowa City, IA, USA
3Cognition Institute and School of Psychology,
Plymouth University, Plymouth, Devon, UK
Detailed Description
This second edition of the Encyclopedia of
Computational Neuroscience includes updates to
the ﬁrst edition’s entries based on new ﬁndings in
the ﬁeld, and publishes several new reviews on
recent topics of interest and modelling techniques
for the study of auditory sensing systems. We
thank the authors for their willingness to contrib-
ute to this section and thus provide a rich scientiﬁc
resource on auditory neuroscience, for young and
senior researchers alike.
Auditory sensing, or the sense of hearing, is
concerned with detecting and extracting informa-
tion from pressure waves in the surrounding
medium, typically air or water. Since waves are
generated by movements or collisions, this pri-
marily tells the perceiver about things happening
in the environment. In addition, since pressure
waves can be reﬂected, absorbed, and refracted
by other objects, these pressure waves also con-
tain a great deal of contextual information about
the
environment
and
the
objects
in
it.
A fundamental challenge for the auditory system
is to segregate the contributions of individual
sound sources to the sound pressure waves
received by the sensors as these are made up of
a combination of all concurrent sources and
their various reﬂections. Sounds unfold in time,
so modelers of auditory processing cannot ignore
time and the need to process signals within time;
this becomes especially challenging when consid-
ering the multiscale nature of the information
contained within sounds.
Possibly due to the complexity of the problem,
many aspects of auditory processing have not yet
been modeled at a detailed neurocomputational
level. In addition, a great deal of processing
occurs even before the incoming signals reach
cortex; with the result that there are more models
related to subcortical processing, than to higher
level, putatively cortical, functions. The articles
commissioned for the auditory sensing systems
section therefore span a range of topics from
qualitatively different point of view, with a strong
focus on the functionality of the auditory system.
Together they provide an interesting and insight-
ful view of current understanding of auditory pro-
cessing, with a great deal of useful information for
modelers
regarding
the
neuroanatomy
and
© Springer Science+Business Media, LLC, part of Springer Nature 2022
D. Jaeger, R. Jung (eds.), Encyclopedia of Computational Neuroscience,
https://doi.org/10.1007/978-1-0716-1006-0

neurophysiology of the auditory system, method-
ological approaches to studying the auditory sys-
tem, and functional requirements and perceptual
constraints on auditory processing.
Awell-illustrated overview of the anatomy and
physiology of the auditory system, including the
extensive but poorly understood efferent system
is covered by ▶“Anatomy and Physiology of
the Mammalian Auditory System.” A detailed
account of efferent control of the auditory sensor,
the cochlea, is presented in the article on the
▶“Physiology
and
Function
of
Cochlear
Efferents.” It is in the cochlea that the biological
system begins to analyze the pressure waves over
multiple time scales and to do so within the
time
constraints
of
the
ongoing
multiscale
information ﬂow.
Another
point
of
major
transformation
occurs at the gateway to the cortex, described
in
the
article
▶“Auditory
Thalamocortical
Transformations.” Three further overview articles
dealing with electrophysiological correlates of
auditory perception: ▶“Auditory Event-Related
Potentials,” ▶“Auditory Brainstem Responses,”
and ▶“Electrophysiological Indices of Speech
Processing” document methodological appro-
aches to studying high-level perceptual functions.
Speciﬁc aspects of the functional neurophysiol-
ogy of the thalamocortical auditory system are
addressed in a number of articles. The articles
▶“Associations and Rewards in the Auditory
Cortex” (revised) and ▶“Context-Dependent
Processing in Auditory Cortex” (revised) demon-
strate that the auditory cortical code needs to
be viewed in more complex terms than simple
acoustic feature representations. For example,
neural correlates of reward are found even within
primary auditory cortex (▶“Associations and
Rewards in the Auditory Cortex”). In contrast,
the
articles
▶“Spectrotemporal
Receptive
Fields,” ▶“Stimulus-Speciﬁc Information,” and
▶“Stimulus
Reconstruction
from
Cortical
Responses” show how auditory cortical activity
can in some circumstances be usefully interpreted
in terms of acoustic feature combinations.
An overview of theories and models of higher-
level aspects of auditory perception are presented
in
▶“Auditory
Perceptual
Organization,”
▶“Music Processing in the Brain,” ▶“Neural
Coding of Speech Sounds,” ▶“Pulse-Resonance
Sounds,”
▶“Auditory
Memory”
(revised),
▶“Acoustic Timbre Recognition“ ▶“Pitch Per-
ception, Models,” ▶“Rhythm Perception: Pulse
and Meter,” and ▶“Sound Localization in Mam-
mals and Models.” These diverse topics empha-
size
the
different
ways
in
which
sound
is interpreted by the brain, while the article
▶“Tinnitus, Models” shows how modelling may
help advance understanding of a perceptual phe-
nomenon that plagues 10–15% of the population.
The section also contains articles which pre-
sent more detailed neurocomputational models
and theories more closely related to the biology.
These tend either to relate to very speciﬁc
functions (▶“Sound Localization in Mammals
and
Models”
(revised),
▶“Stimulus-Speciﬁc
Adaptation, Models,” ▶“Auditory Precedence
Effect,” and ▶“Masking and Masking Release”
(revised)), or relate to processes at or near the
sensor (▶“Auditory-Nerve Response, Afferent
Signals” (revised) and ▶“Cochlear Inner Hair
Cell, Model”).
Finally, new entries in the Auditory Sensing
Systems section focus on neural features of sound
perception such as neuronal phase-locking to
the spectrotemporal components of the acoustic
signal
(▶“Auditory
Frequency-Following
Responses”), enhancement of the cortical repre-
sentation of auditory signals in the presence
of
noise
at
moderate
signal-to-noise
ratio
(▶“Auditory Cortex: Separating Signal from
Noise”), and perceptual organization of environ-
mental
sounds
(▶“Environmental
Sound
Perception:
Effects
of
Aging
and
Hearing
Loss”). They also present recent advancements
in mathematical and computational modeling of
auditory processing in the auditory pathway and
auditory perceptual representation. The articles
review modeling of the interaural and internal
time and level differences (▶“Internally Coupled
Ears
(ICE):
Biophysical
Consequences
and
Underlying Mechanisms”), modeling of sound-
evoked extracellular voltages in the auditory
brainstem (▶“Extracellular Voltage Recordings
in the Medial Superior Olive, Modeling of”),
as well computational and statistical methods
2
Auditory Sensing Systems: Overview

used
to
study
auditory
stream
segregation
(▶“Computational Models of Auditory Stream
Segregation”).
Cross-References
▶Acoustic Timbre Recognition
▶Anatomy and Physiology of the Mammalian
Auditory System
▶Associations and Rewards in the Auditory
Cortex
▶Auditory Brainstem Responses
▶Auditory Cortex: Separating Signal from Noise
▶Auditory Event-Related Potentials
▶Auditory Frequency-Following Responses
▶Auditory Memory
▶Auditory Perceptual Organization
▶Auditory Precedence Effect
▶Auditory Thalamocortical Transformations
▶Auditory-Nerve Response, Afferent Signals
▶Cochlear Inner Hair Cell, Model
▶Computational Models of Auditory Stream
Segregation
▶Context-Dependent Processing in Auditory
Cortex
▶Electrophysiological
Indices
of
Speech
Processing
▶Environmental Sound Perception: Effects of
Aging and Hearing Loss
▶Extracellular Voltage Recordings in the Medial
Superior Olive, Modeling of
▶Internally Coupled Ears (ICE): Biophysical
Consequences and Underlying Mechanisms
▶Masking and Masking Release
▶Music Processing in the Brain
▶Neural Coding of Speech Sounds
▶Physiology and Function of Cochlear Efferents
▶Pitch Perception, Models
▶Pulse-Resonance Sounds
▶Rhythm Perception: Pulse and Meter
▶Sound Localization in Mammals and Models
▶Spectrotemporal Receptive Fields
▶Stimulus
Reconstruction
from
Cortical
Responses
▶Stimulus-Speciﬁc Adaptation, Models
▶Stimulus-Speciﬁc Information
▶Tinnitus, Models
Basal Ganglia: Overview
Jonathan E. Rubin
Department of Mathematics, University of
Pittsburgh, Pittsburgh, PA, USA
Definition
The basal ganglia are a collection of four subcor-
tical nuclei, the striatum, substantia nigra, globus
pallidus, and subthalamic nucleus. They are com-
ponents of several apparently segregated circuits
that can be classiﬁed according to function
as motor, oculomotor, associative, and limbic.
Certain neurons in the basal ganglia are major
sources of the neurotransmitter dopamine, associ-
ated with reward, while others receive dopaminer-
gic inputs; thus, the basal ganglia have received
considerable attention in the context of learning.
Imbalances in activity across the basal ganglia
nuclei within the motor circuit are associated
with various motor disorders.
Detailed Description
This section considers on the motor aspects of the
basal ganglia, which have been the focus of most
computational studies. The articles can be classi-
ﬁed as those concentrating on activity within par-
ticular areas of basal ganglia, those concerning
basal ganglia function, and those about the roles
of basal ganglia in motor disorders, predomi-
nantly parkinsonism.
Basal Ganglia Structure
While the structures of the basal ganglia (Fig. 1)
appear to be common across a wide range of
vertebrate animals, from primates to lamprey
(Stephenson-Jones et al. 2011), there are some-
what different naming systems used in different
species, and there are some structural differences
across species (e.g., mammals versus birds). Fol-
lowing the nomenclature used in primates, the
striatum, encompassing the caudate nucleus and
the putamen, is the predominant recipient of
Basal Ganglia: Overview
3

cortical inputs. There are several types of neurons
within the striatum, most of which release the
neurotransmitter GABA, which has an inhibitory
effect on its targets (see entry ▶“Basal Ganglia:
Striatal Models Cellular Detail”). Striatal neurons
are also major recipients of dopamine. Different
types of dopamine receptors arise in different
striatal neurons, and this variability translates
into diverse impacts of dopamine release. In
coarse terms, dopamine promotes activity in
some striatal neurons and inhibits it in others,
but speciﬁc effects may be more complicated
and may depend on ongoing activity levels.
The substantia nigra has two main parts, the
substantia nigra pars reticulata (SNr) and the sub-
stantia nigra pars compacta (SNc). The former
sends inhibitory output to areas outside of the
basal ganglia that may be related to oculomotor
activity (see entry ▶“Basal Ganglia: Control of
Saccades”) or other motor activity, particularly in
rodents. The latter releases dopamine to the stria-
tum and other basal ganglia areas (see entry
▶“Basal Ganglia: Dopaminergic Cell Models”).
The globus pallidus is also a dichotomous
structure, with segments that are referred to as
external (GPe) and internal (GPi). Both parts pre-
dominantly contain GABAergic neurons that
exhibit high ﬁring rates, although with diverse
ﬁring patterns, under baseline conditions (see
entry ▶“Basal Ganglia: Globus Pallidus Cellular
Models”). The GPe receives projections from a
subset of striatal neurons as well as from the
subthalamic nucleus (STN), and it innervates
both the striatum and the STN in return, although
it appears that the subpopulations of GPe neurons
targeting these areas are distinct (Mallet et al.
2012). The GPi is subject to inputs from a differ-
ent subset of striatal neurons, from the STN, and
from the GPe and in turn projects to thalamic areas
outside of the basal ganglia.
The STN stands out as the only basal ganglia
area that releases the excitatory neurotransmitter
glutamate. STN neurons exhibit intrinsic ﬁring,
albeit with heterogeneous properties (see entry
▶“Basal Ganglia: Subthalamic Nucleus Cellular
Models”). Besides participating in a reciprocal
loop with GPe (see entry ▶“Subthalamopallidal
Loop and Oscillations”), the STN projects directly
to
GPi.
The
STN
also
receives
direct
glutamatergic input from the cortex and some
dopaminergic input from the SNc.
The routes along which activity ﬂows through
the basal ganglia have been classiﬁed into the
direct pathway, the indirect pathway, and the
hyperdirect pathway (Fig. 1). This nomenclature
refers to the number and types of steps between
the cortex, which provides input to the basal
ganglia, and the GPi, the primary source of
motor outputs from the primate basal ganglia.
The direct pathway is the stream from the cortex
to the striatum to GPi. In net, the direct pathway
inhibits basal ganglia output, since the cortex
excites the striatum, which inhibits GPi. Since
GPi inhibits downstream areas, reductions in its
output can be pro-kinetic. The indirect pathway
goes from the cortex through the striatum to GPe
and continues on to STN and then GPi; the GPe-
GPi connection may also be included in this
pathway.
Overall,
the
indirect
pathway
is
believed to promote basal ganglia output and
hence suppress movement. This view is based
cortex
striatum
SNc
GPe
STN
excitation
inhibition
GPi/SNr
thalamus
direct
indirect
hyperdirect
Basal Ganglia: Overview, Fig. 1 Major structures and
pathways of the basal ganglia. Basal ganglia areas are
colored in blue. Connecting arrows and lines represent
the existence of direct synaptic interactions between
areas. A projection from GPe to the striatum recently
discovered in rats is omitted here
4
Basal Ganglia: Overview

on the idea that the cortex excites the striatum and
thus inhibits GPe and disinhibits its target STN,
which in turn excites GPi; the reduction in GPe
activity also lowers its inhibition of GPi, yielding
additional enhancement of GPi ﬁring. This rea-
soning omits the complication of feedback path-
ways, such as the excitation that STN sends to
GPe, however. The hyperdirect pathway, like the
direct pathway, includes only one intermediary
between the cortex and GPi: it refers to the cor-
tical projection to STN together with the STN
link to GPi. Presumably, this pathway is called
“hyperdirect” because it features excitatory
transmission via glutamatergic synapses from
STN to GPi that is faster and has a more imme-
diate effect than the direct pathway’s inhibitory
signaling via GABAergic synapses from the stri-
atum to GPi. Like the indirect pathway, the
hyperdirect pathway promotes basal ganglia
output.
Finally, note that in rodent, which is a useful
experimental model, the output structure analo-
gous to the GPi is called the entopeduncular
nucleus, and the major motor output structure is
thought to be the SNr.
Basal Ganglia Function
The basal ganglia have been postulated to have a
wide range of functions related to motor learning
and other steps contributing to goal-directed
movements (Chakravarthy et al., 2010). Models
have been developed to represent and make pre-
dictions about the mechanisms involved in several
of these functions. In the Basal Ganglia section of
this encyclopedia, articles discuss background
information and computational work on the mech-
anisms for action selection, habit formation,
decision-making, and control of saccades. Fur-
thermore, additional articles treat the role of the
basal ganglia as an exploration engine and the
learning and production of songs by birds.
Several lines of evidence point to the basal
ganglia as a major site of action selection (see
entry ▶“Basal Ganglia: Mechanisms for Action
Selection”). In particular, the basal ganglia are
well positioned anatomically to orchestrate action
selection by integrating inputs from a broad range
of
cortical
areas
and
providing
outputs
to
downstream motor-related sites. The basic idea
of how action selection might work rests on the
observations that under rest conditions, the output
nuclei of the basal ganglia, GPi and SNr, exhibit
high activity levels and that these areas release
inhibitory
neurotransmitters,
which
are
well
suited to block motor activity. To perform a move-
ment, this inhibitory “gate” must be opened but
only in a way that allows the desired action while
maintaining a blockade on all other movements.
Models of action selection position the direct
pathway as the agent that removes inhibition and
suggest that the indirect and hyperdirect pathways
can act to suppress behavior and to participate in
behavior switching (see entry ▶“Basal Ganglia:
Mechanisms
for
Action
Selection”;
Nambu
et al. 2002).
Gradually, particular actions can become
habits, which are actions that are elicited by
certain stimuli in a way that lacks ﬂexibility.
Although established habits are less dependent
on external rewards than are other learned
behaviors, rewarding feedback that leads to
striatal activation and associated plasticity of
corticostriatal synapses are central to their being
formed, and the corresponding learning process
has been considered in computational models
(see entry ▶“Basal Ganglia: Habit Formation”).
Dopamine is central to this form of plasticity and
learning, and impairments in habit formation
and performance are noted in subjects with
Parkinson’s disease.
A basal ganglia function that is distinct from
pure action selection and habit learning arises in
perceptual decision-making. In this process, in
abstract terms, it is thought that evidence favoring
each of a number of choices accumulates until one
option becomes sufﬁciently supported to elicit a
corresponding behavior. The evidence is presum-
ably represented by ﬁring of various sets of corti-
cal neurons that can inﬂuence basal ganglia
activity, and basal ganglia activity is involved in
setting the selection threshold (see entry ▶“Basal
Ganglia: Decision-Making”). In this view, the
dopamine-based reward induced by a behavior is
involved in threshold adjustment and thereby
inﬂuences future choices. Computational models
consider how the activity in the pathways of the
Basal Ganglia: Overview
5

basal ganglia interacts with dopaminergic reward
signals to drive and adjust the decision-making
process, positing various roles for particular brain
areas in evidence accumulation, stimulus detec-
tion, threshold adjustment, and decision-making
and cancelation. Some also treat impairments in
parkinsonian conditions and alterations under
deep
brain
stimulation
(see
entry
▶“Basal
Ganglia: Decision-Making”).
Unlike other actions to which the basal ganglia
contribute, rapid eye movements known as sac-
cades in primates are gated by outputs from SNr
rather than GPi. Issues under investigation relat-
ing to saccade generation, which have been
treated in computational models, include mecha-
nisms of target selection, of learning of target
priorities, and of interactions of these processes
with
working
memory
(see
entry
▶“Basal
Ganglia: Control of Saccades”).
A less thoroughly investigated function of the
basal ganglia is the generation of variability in
motor responses. Such variability is certainly
observed in the behaviors of many species and
seems to be essential for optimizing behavior and
attaining maximal rewards. There is signiﬁcant
experimental
evidence implicating the basal
ganglia as a source of such motor exploration,
and some computational modeling work to instan-
tiate these ideas and study their implications has
been performed (see entry ▶“Basal Ganglia Sys-
tem as an Engine for Exploration”). In relevant
models, while the direct pathway takes on the
traditional role of implementing action selection,
the indirect pathway, and particularly the STN,
injects variability into this process. Learning
based on reward signals is critical to such models
and helps control the relative dominance of direct
and indirect pathway signals.
One example of the prominence of behavioral
variability arises in song production by song-
birds. Songs must be learned, which involves
exposure to the song of another bird, trial-and-
error behavior, and auditory feedback. This pro-
cess appears to heavily involve a basal ganglia
analogue that participates in learning, in the
introduction of behavioral variability, and in the
selection of rewarding behaviors. Certain song-
bird experimental preparations are advantageous
for studying this combination of processes, and
corresponding models of associated reinforce-
ment learning and song production have been
informed by observations from these prepara-
tions (see entry ▶“Basal Ganglia: Songbird
Models”).
Basal Ganglia and the Motor Symptoms of
Parkinsonism
The loss of dopaminergic neurons in the SNc
appears to be the critical trigger for a slew of
changes in neuronal activity within areas of the
basal ganglia that result in the motor symptoms of
parkinsonism. This loss is typically gradual, and
the causes appear to involve a complex combina-
tion of environmental and genetic factors that are
still under intensive investigation.
Classical models of parkinsonism attribute its
motor signs to an imbalance in direct and indirect
pathway outputs stemming from the loss of dopa-
minergic input to the striatum (Albin et al. 1989).
The loss of dopamine removes a source of excita-
tion to the striatal neurons with D1 receptors,
which project to GPi. Less ﬁring by these striatal
neurons, viewed as a weakening of the direct
pathway, results in disinhibition of GPi. Mean-
while, diminished dopamine translates to reduc-
tion in inhibition of the striatal neurons with D2
receptors, which in turn inhibit GPe. As a result,
GPe output is suppressed, which relieves GPe
inhibition of STN and allows enhanced STN ﬁr-
ing. The increased STN activity provides more
drive to GPi, viewed as a strengthening of the
indirect pathway. Together, these changes tip the
balance of direct and indirect pathway effects in a
way that favors the indirect pathway and promotes
GPi activity. GPi outputs inhibit the pallidal recip-
ient areas of the thalamus and are believed to
represent an inhibitory gate that shuts down
movement. Thus, pathologically enhanced GPi
ﬁring could interfere with initiating and carrying
out movements in a way that translates into some
motor signs of parkinsonism.
The complete explanation for the emergence of
parkinsonian motor signs likely involves phenom-
ena that are omitted from this classical descrip-
tion. It has become clear that loss of dopamine
leads to widespread modiﬁcations of neuronal
6
Basal Ganglia: Overview

activity patterns, rather than just changes in ﬁring
rates, in the basal ganglia. These effects include an
enhanced prevalence of bursting, altered oscilla-
tion structure, and increased correlation in outputs
of neurons within, and across, basal ganglia areas
(Rubin et al. 2012).
One potential source of oscillations within
basal
ganglia
is
the
reciprocal
excitatory-
inhibitory loop between the GPe and the STN
(see entry ▶“Subthalamopallidal Loop and Oscil-
lations”). Oscillations in the beta band (broadly
taken as 10–30 Hz) appear to be particularly prev-
alent in parkinsonian conditions and may be par-
ticularly effective at hijacking activity in thalamic
areas downstream from the basal ganglia, and
several different sources for such oscillations
have been posited (see entry ▶“Basal Ganglia:
Beta Oscillations”).
While oscillations observed in the activity of
basal ganglia neurons seem like a natural candi-
date to contribute to parkinsonian tremor, the link
between altered neuronal ﬁring and other parkin-
sonian symptoms may be less explicit. In particu-
lar,
the
slowing
of
movement
known
as
bradykinesia may emerge from the interactions
of all of the motor signaling pathways through
the basal ganglia, or it may be necessary to con-
sider basal ganglia interactions with corticospinal-
muscular pathways to explain its source (see entry
▶“Basal Ganglia: Bradykinesia Models”).
The predominant treatments for Parkinson’s
disease in its early stages are pharmacological,
aimed at replacing lost dopamine. In cases where
pharmacological treatments gradually lose efﬁ-
cacy or induce substantive side effects, deep
brain stimulation therapy represents a treatment
option that, while highly invasive, is at least par-
tially adjustable (by tuning of stimulation param-
eters) and reversible (by cessation of stimulation).
Deep brain stimulation for Parkinson’s disease
typically is targeted at STN or GPi and proves
effective for many patients; although the mecha-
nisms underlying its efﬁcacy are not known, the-
ories have honed in on its potential impact on
patterns of basal ganglia activity and outputs and
their downstream effects (see entry ▶“Computa-
tional Models of Deep Brain Stimulation (DBS)”;
Rubin et al. 2012).
Cross-References
▶Basal Ganglia System as an Engine for
Exploration
▶Basal Ganglia: Beta Oscillations
▶Basal Ganglia: Bradykinesia Models
▶Basal Ganglia: Control of Saccades
▶Basal Ganglia: Decision-Making
▶Basal Ganglia: Dopaminergic Cell Models
▶Basal
Ganglia:
Globus
Pallidus
Cellular
Models
▶Basal Ganglia: Habit Formation
▶Basal
Ganglia:
Mechanisms
for
Action
Selection
▶Basal Ganglia: Songbird Models
▶Basal Ganglia: Striatal Models Cellular Detail
▶Basal Ganglia: Subthalamic Nucleus Cellular
Models
▶Computational Models of Deep Brain Stimula-
tion (DBS)
▶Subthalamopallidal Loop and Oscillations
References
Albin RL, Young AB, Penney JB (1989) The functional
anatomy of basal ganglia disorders. Trends Neurosci
12:366–375
Chakravarthy VS, Joseph D, Bapi RS (2010) What do the
basal ganglia do? Biol Cybern 103:237–253
Mallet N, Micklem BR, Henny P, Brown MT, Williams C,
Bolam JP, Nakamura KC, Magill PJ (2012) Dichoto-
mous organization of the external globus pallidus. Neu-
ron 74:1075–1086
Nambu A, Tokuno H, Takada M (2002) Functional signif-
icance of the cortico-subthalamo-pallidal ‘hyperdirect’
pathway. Neurosci Res 43:111–117
Rubin JE, McIntyre CC, Turner RS, Wichmann T (2012)
Basal ganglia activity patterns in parkinsonism and
computational modeling of their downstream effects.
Eur J Neurosci 36:2213–2228
Stephenson-Jones
M,
Samuelsson
E,
Ericsson
J,
Robertson B, Grillner S (2011) Evolutionary conserva-
tion of the basal ganglia as a common vertebrate mech-
anism for action selection. Curr Biol 21:1081–1091
Further Reading
Alexander GE, Crutcher MD, DeLong MR (1990) Basal
ganglia-thalamocortical circuits: parallel substrates for
motor, oculomotor, ‘prefrontal’ and ‘limbic’ functions.
Prog Brain Res 85:119–146
Bergman H, Feingold A, Nini A, Raz A, Slovin H,
Abeles M, Vaadia E (1998) Physiological aspects of
Basal Ganglia: Overview
7

information processing in the basal ganglia of normal
and parkinsonian primates. Trends Neurosci 21:32–38
Gerfen CR, Wilson CJ (1996) The basal ganglia. Handb
Chem Neuroanat 12:371–468
Smith Y, Bevan MD, Shink E, Bolam JP (1998) Microcir-
cuitry of the direct and indirect pathways of the basal
ganglia. Neuroscience 86:353–387
Bayesian Approaches in
Computational Neuroscience:
Overview
Ulrik R. Beierholm
Psychology Department, Durham University,
Durham, UK
Definition
Bayesian approaches in Computational Neurosci-
ence rely on the properties of Bayesian statistics
for performing inference over unknown variables
given a data set generated through a stochastic
process.
Detailed Description
Given a set of observed data d1:n, generated from a
stochastic process P(d1:n|X) where X is a set of
unobserved variables, the posterior probability
distribution of X is P(X|d1:n) ¼ P(d1:n|X)P(X)/
P(d1:n) according to Bayes’ theorem. X can be a
set of ﬁxed parameters as well as a series of vari-
ables of the same size as the data itself X1:n.
Based on the posterior probability and a spec-
iﬁed utility function, an estimate of X can be made
that can be shown to be optimal, for example, by
minimizing the expected variance.
One common use of this principle within
computational
neuroscience
is
for
inferring
unobserved properties (hidden variables X) based
on observed data, d. These techniques can be used
for inference on any data sets but has in neurosci-
ence mostly been used for neurophysiological
recordings, imaging, and behavioral data.
By their nature electrophysiological recordings
are noisy and stochastic. Given the stochastic
responses of neurons to stimuli, Bayesian methods
can be used to infer the underlying stimuli or
activation of the neurons, X1:n (Bayesian Electro-
physiology Analysis).
For imaging data an underlying neural activity
X1:n, for example, ﬁring rate at the level of indi-
vidual columns of cortex, is assumed to give rise
to measured responses d1:n, for example, the
blood ﬂow measured by fMRI, through a stochastic
process. Inverting the process through the Bayesian
inference allows for estimating the unknown neural
activity (Bayesian Imaging Analysis).
The ideas can also be useful for inferring prop-
erties about the behavior of individual human
subjects, X, by the assumption of a stochastic
process, P(d1:n|X), through which characteristics
of each individual subject lead to individual
choices in an experimental task, d1:n (Bayesian
Behavioral Analysis).
The abovementioned techniques all use Bayes-
ian inference to infer underlying properties of
recorded data but are essentially used as very
effective tools for data analysis. An alternative
line of research takes as the working hypothesis
that the human brain has evolved to the point of
itself approximating an ideal Bayesian observer
(sometimes referred to colloquially as the Bayes-
ian Brain Hypothesis). Accordingly, this line of
research compares human behavior to the output
of such an ideal observer within, for example,
perceptual (Bayesian Models of Perception) or
cognitive tasks (Bayesian Models of Cognition).
A related effort has proposed that the compu-
tations necessary to perform the steps of Bayesian
inference can be done through populations of
biological neurons (Bayesan Inference with Spik-
ing Neurons). Recent works on artiﬁcial neural
networks have also reignited interest in combin-
ing ideas from the two areas of research including
means of designing artiﬁcial networks able to
perform Bayesian inference.
There is a continued effort to translate ideas on
Bayesian inference from Machine Learning and
Computer Science into Computational Neurosci-
ence, including, for example, recent advances in
sampling techniques for inference.
8
Bayesian Approaches in Computational Neuroscience: Overview

Cross-References
▶Bayesian Inference with Spiking Neurons
▶Behavioural Analysis, Bayesian
▶Cognition, Bayesian Models of
▶Electrophysiology Analysis, Bayesian
▶Imaging Analysis, Bayesian
▶Perception, Bayesian Models of
Biochemical Signaling
Pathways and Diffusion:
Overview
Kim T. Blackwell
Department of Bioengineering, George Mason
University, Fairfax, VA, USA
Molecular Neuroscience Department, Krasnow
Institute for Advanced Study, George Mason
University, Fairfax, VA, USA
Detailed Description
Signaling pathways modulate the function of neu-
rons and neuronal networks through diverse pro-
cesses.
The
most
well-known
function
of
signaling pathways is synaptic plasticity, which
controls neuronal networks via modulation of the
strength of synaptic connections. Signaling path-
ways also are critical for neuronal development,
axon guidance, and regulation of transcription and
translation. Signaling pathways are activated by
the G protein-coupled transmembrane receptors,
such as metabotropic glutamate receptors or nor-
adrenergic receptors; by the receptor tyrosine
kinases; and by calcium inﬂux through NMDA
receptors or voltage-dependent calcium channels.
Calcium
Due to the importance of calcium, its concentra-
tion is tightly regulated by buffers and pumps.
One of these calcium buffers, known as calmod-
ulin, is not inert; rather, it can activate diverse
enzymes such as adenylyl cyclase, calcineurin,
phosphodiesterase
type
1B,
and
calcium-
calmodulin-dependent protein kinase II. In addi-
tion to calcium inﬂux through plasma membrane
channels, both the mitochondria and the smooth
endoplasmic reticulum (SER) are sources of cal-
cium. Two types of calcium permeable channels
reside on the SER: the inositol trisphosphate
receptor channel and the ryanodine receptor chan-
nel. Calcium-dependent calcium release through
these channels can lead to oscillations or waves of
calcium, depending on various factors.
Kinases
The second messengers activated through trans-
membrane receptors have multiple downstream
targets, including ionic channels, kinases, and
phosphatases. Of the thousands of kinases and
phosphatases in the proteome, several have a dem-
onstrated role in synaptic plasticity, though their
relative importance depends on the brain region
and cell type. Protein kinase type C is a calcium-
and lipid-activated kinase which is critical for LTP
in the cerebellum. Protein kinase type A is a
cAMP-activated kinase, which is critical for LTP
in the striatum, and for several long-lasting forms
of LTP in the hippocampus. Gene transcription and
protein translation are required for both memory
and for long-lasting forms of synaptic plasticity.
One kinase that appears to bridge these other
kinases and transcription is the ERK1/2 forms of
MAPK (mitogen-activated protein kinase).
Modeling Techniques
Calcium dynamics and signaling pathways are
modeled as cascades of biochemical reactions,
both bimolecular reactions and enzyme reactions,
and diffusion. Many special purpose simulators
are available to implement these signaling path-
ways, either using stochastic techniques or deter-
ministic approaches. Modeling calcium inﬂux
requires the simulator to have capabilities for
modeling membrane potential. Due to the diver-
sity in temporal and spatial scale involved in
modeling neuronal electrical activity coupled to
reaction-diffusion pathways, very few models
Biochemical Signaling Pathways and Diffusion: Overview
9

incorporate signaling pathways in entire neurons,
though the number of such models is increasing as
computational power increases.
Cross-References
▶Bimolecular Reactions, Modeling of
▶Biophysical Models of Calcium-Dependent
Exocytosis
▶Calcium Buffering: Models of
▶Calcium Dynamics in Neuronal Microdomains:
Modeling, Stochastic Simulations, and Data
Analysis
▶Calcium Pumps, Models of
▶Calcium Release, Models of
▶Calcium Waves, Models of
▶Calmodulin, Models of
▶Cerebellum: Overview
▶Deterministic Reaction-Diffusion Simulators
▶Diffusion Equation
▶Enzyme Kinetics, Modeling of
▶Extracellular
Signal-Regulated
Kinases,
Models of
▶Gillespie Algorithm for Biochemical Reaction
Simulation
▶High-Voltage-Activated Calcium Channels
▶Metabotropic Receptors (G Protein Coupled
Receptors)
▶N-Methyl-D-Aspartate
(NMDA)
Receptors,
Conductance Models
▶Particle-Based Stochastic Simulators
▶Protein Kinase A, Models of
▶Protein Kinase C, Models of
▶Signaling Pathways, Modeling of
▶Stochastic Simulators
Brain Imaging: Overview
Jorge Riera
Department of Biomedical Engineering, Florida
International University, Miami, FL, USA
Synonyms
Functional neuroimaging
Definition
Brain imaging constitutes a set of techniques used
to measure functional activity in networks of
interacting brain cells as well as to reveal the
major underlying structural properties of these
networks. Functional data obtained with these
techniques reﬂect local changes in brain perfu-
sion, metabolism, and extracellular electric/mag-
netic potentials originated from the activity of
these brain cell networks. Brain imaging has
been traditionally used to explore both normal
and pathological brains in a large variety of spe-
cies ranging from rodents to primates. In princi-
ples, brain imaging could be applied for both
in vitro and in vivo situations.
Detailed Description
Techniques for brain imaging have been devel-
oped for two major physical brain scales, i.e., the
mesoscale
and
the
macroscale.
Theoretical
models useful to interpret how these two physical
scales interact have been developed in the past
(see ▶“Multiscale Brain Connectivity”). Brain
imaging modalities have been combined with spe-
ciﬁc stimulation techniques to study brain activity
in both mesoscopic (e.g., uncaging methods,
optogenetics – see ▶“Optogenetics”) and macro-
scopic (e.g., transcranial magnetic stimulation,
TMS) levels. Brain imaging constitutes one of
the most important building blocks in the devel-
opment
of
brain-machine
interfaces
(see
▶“Brain-Machine Interface and Neuroimaging”).
Neuroimaging at the Mesoscale
Functional refers to observations reﬂecting the
activity of populations of cells (e.g., neurons,
astrocytes) in a particular brain structure with the
resolution of single cells. Examples of functional
neuroimaging for this particular scale are confo-
cal/multiphoton (MP) microscopy, current source
density (CSD) analysis from intracranial record-
ings using MEA, intrinsic optical signal (IOS),
optical
coherence
tomography
(OCT),
and
10
Brain Imaging: Overview

voltage-sensitive dye imaging (VSDI). These
imaging techniques are based on either in vitro
or very invasive in vivo experimental protocols.
Some of the imaging techniques based on optical
phenomena (confocal/MP microscopy, VSDI –
see ▶“Voltage Sensitive Dye Imaging, Intrinsic
Optical Signals”) provide the ideal spatial resolu-
tion to explore the activity of single cells, but their
temporal resolution is in the order of hundreds of
milliseconds. In contrasts, images resulting from
the CSD analysis could reﬂect electrical phenom-
ena happening in the order of a few milliseconds,
but they are associated with the activity of syn-
chronized population of neurons in extended brain
regions. IOS and OCT are employed mainly to
record changes in blood ﬂow/volume in the brain
as well as alterations in blood oxygen concentra-
tion (see ▶“Voltage Sensitive Dye Imaging,
Intrinsic Optical Signals”).
Anatomical refers to static pictures of the cel-
lular networks directly reﬂecting structural prop-
erties. These images are obtained using different
modalities
of
sectioning
microscopy
(see
▶“Physical Sectioning Microscopy”).
Neuroimaging at the Macroscale
Functional refers to observations reﬂecting the
activity of large regions inside the brain. Examples
of functional neuroimaging for this particular scale
are functional magnetic resonance imaging (fMRI),
positron emission tomography (PET), single-
photon emission tomography (SPECT), functional
near-infrared spectroscopy (fNIRS), and electro
(EEG)/magneto (MEG) –encephalograms. The
spatiotemporal proﬁles of these brain imaging tech-
niques are quite different (Riera and Valdes-Sosa
2010). Deciphering the origin and nature of signa-
tures in the functional neuroimaging imprinted by
abnormal brain activity is important while using
them for diagnosing, monitoring, and treating
brain diseases and disorders (see ▶“Connectivity
Analysis in Normal and Pathological Brains”). To
that end, it is crucial to understand the physiologi-
cal mechanisms underlying these brain imaging
techniques
(see
▶“Biophysical
Models:
Neurovascular Coupling, Cortical Microcircuits,
and
Metabolism,”
▶“Kinetic
Models
for
PET/SPECT
Imaging,”
and
▶“Forward
and
Inverse Problems of MEG/EEG”). These tech-
niques
classify
either
as
slightly
invasive
(i.e., fMRI, fNIRS, EEG/MEG) or extremely inva-
sive because of the use of radioisotopes (i.e., PET,
SPECT, see ▶“Radiopharmaceuticals in Molecu-
lar Imaging”). A variety of methods have been
developed for the preprocessing and analysis of
brain imaging data, with resulting software and
platforms currently available (see ▶“Software for
Neuroimaging Data Analysis,” ▶“Statistical Anal-
ysis of Neuroimaging Data,” and ▶“Meta-analysis
in Neuroimaging”).
Anatomical refers to static pictures of the entire
brain, or part of it, directly reﬂecting structural
properties. Example of imaging modalities used
to study morphometric characteristics of brain
tissues are the T1/T2 magnetic resonance imaging
(MRI) and the computed tomography (CT). Dif-
fusion tensor imaging (DTI) has been commonly
employed to study the strengths of connections
between different brain regions.
Cross-References
▶Applications of Information Theory to Analysis
of Neural Data
▶Biophysical Models: Neurovascular Coupling,
Cortical Microcircuits, and Metabolism
▶Brain Atlases
▶Brain Extracellular Space: A Compartment for
Intercellular
Communication
and
Drug
Delivery
▶Brain-Machine Interface and Neuroimaging
▶Connectivity Analysis in Normal and Patholog-
ical Brains
▶Current Source Density (CSD) Analysis
▶Electrophysiology Analysis, Bayesian
▶Forward and Inverse Problems of MEG/EEG
▶Imaging Analysis, Bayesian
▶Independent Component Analysis of Images
▶Kinetic Models for PET/SPECT Imaging
▶Local Field Potential, Relationship to BOLD
Signal
▶Local Field Potential, Relationship to Electro-
encephalogram
(EEG)
and
Magnetoence-
phalogram (MEG)
Brain Imaging: Overview
11

▶Meta-analysis in Neuroimaging
▶Multiscale Brain Connectivity
▶Neuroimaging, Neural Population Models for
▶Noninvasive Brain-Computer Interfaces
▶Optogenetics
▶Physical Sectioning Microscopy
▶Radiopharmaceuticals in Molecular Imaging
▶Reconstruction, Electron Microscopy
▶Software for Neuroimaging Data Analysis
▶Statistical Analysis of Neuroimaging Data
▶Voltage Sensitive Dye Imaging, Intrinsic Opti-
cal Signals
References
Riera J, Valdes-Sosa P (2010) Mesoscale in neuroimaging:
creating bridges between the microscopic and system
levels. J Integr Neurosci 9(4):v–vii
Brain-Machine Interface:
Overview
Karim G. Oweiss
Electrical and Computer Engineering, Biomedical
Engineering, Neuroscience and Neurology,
University of Florida, Gainesville, FL, USA
Herbert Wertheim College of Engineering, The
McKnight Brain Institute, University of Florida,
Gainesville, FL, USA
The Norman Fixel Institute for Neurological
Disorders, University of Florida, Gainesville,
FL, USA
Electrical and Computer Engineering, Michigan
State University, East Lansing, MI, USA
Abbreviations
AP
Action potentials
BCI
Brain-computer interfaces
BMI
Brain-machine interface
ECoG
Electrocorticogram
EEG
Electroencephalogram
EMG
Electromyogram
FES
Functional electrical stimulation
LTD
Long-term depression
LTP
Long-term potentiation
MIMO
Multi-input multi-output
NI
Neural interfaces
SCI
Spinal cord injury
SISO
Single-input single-output
Synonyms
Brain-computer interfaces; Neural interfaces
Definition
A brain-machine interface (BMI) is a direct com-
munication pathway between the nervous system
and a man-made computing device. This commu-
nication is unidirectional in BMIs that either record
neural activity in the nervous system to affect the
state of an external device or stimulate neural activ-
ity to affect the state of the nervous system. It can
also be bidirectional, such as BMIs that record
activity from certain parts of the nervous system
and use this activity – or features extracted from it –
in real time to stimulate activity in other parts of
that system. This communication can occur at mul-
tiple levels, which may include muscles, peripheral
nerves, spinal cord, or the brain.
Detailed Description
BMIs fundamentally rely on the concept of cau-
sation between electricity and movement or
between electricity and cognition. The causal
link between electrical current injection into the
body and movement of parts of that body was ﬁrst
established in the late eighteenth century by Gal-
vani (1791), Fowler and Galvani (1793), who
could evoke muscle twitches in the frog legs by
direct current injection into the muscle ex vivo.
A similar discovery in the central nervous system
was made in 1870 by Fritsch and Hitzig (1870), in
which electrical stimulation of different areas of
the cerebral cortex caused muscular contractions
of speciﬁc parts of a dog’s body in vivo. It was not
until 1928 that recording of the all-or-none elec-
trical discharge of single neurons in the optic
nerve of the toad was made by Adrian (Adrian
12
Brain-Machine Interface: Overview

and Bronk 1928) and was found to be strongly
correlated with light stimuli. Ever since, countless
studies have revealed numerous mechanisms of
neural coding of sensory stimuli or movement
parameters, such as orientation tuning by V1 neu-
rons (Hubel and Wiesel 1962), spatial position by
hippocampal place cells (Ranck 1973; O’Keefe
1979) and movement direction by primary motor
cortex neurons (Georgopoulos et al. 1986).
The early 1990s nonetheless has witnessed a
paradigm shift in the way recording and stimulation
of neural activity is achieved. In particular, micro-
wire bundles have been used to record the activity
of a handful of neurons in awake behaving animals
(McNaughton et al. 1983). Striking advances in the
microfabrication technology of high-density micro-
electrode arrays (HDMEAs) (Drake et al. 1988;
Normann et al. 1999) have later permitted large-
scale simultaneous recording of many neurons (tens
to hundreds) in awake behaving subjects, which
eventually paved the way for these arrays to
become a key element in BMIs development in
subsequent years (Nicolelis 1999).
Unidirectional BMIs
Afferent BMIs
Afferent BMIs (ABMIs) rely on transforming fea-
tures of sensory stimuli (e.g., auditory, visual,
etc.) to electrical pulse trains in order to stimulate
neural activity in the central or the peripheral
nervous systems to cause artiﬁcial sensation to
compensate for some form of sensory loss (e.g.,
deafness or blindness) (Fig. 1). Auditory prosthe-
sis, such as cochlear implants (CIs) – dating back
to the ﬁrst implant in 1957 by Djorno and Eyries –
is the ﬁrst example of an afferent BMI that was
approved by the FDA in 1984 (House and Urban
1973).
CIs
transform
sound
features
(e.g.,
intensity or pitch) recorded through a microphone
to pulsatile currents in the spiral ganglia. This
eventually elicits action potentials from residual
hair cells in the auditory nerve of hearing-
impaired subjects.
Likewise, visual prosthesis rely on trans-
forming features of the visual scenes recorded
through a video camera to stimulate different
parts of the visual pathway in legally blind sub-
jects. The site of stimulation along this pathway is
a function of where neural degeneration occurs,
but the most promising demonstration of visual
prosthesis thus far has been through the stimula-
tion of the retinal ganglion cell layer that provide
input to the optic nerve in patients with retinitis
pigmentosa and age-related macular degeneration
(de Balthasar et al. 2008).
Efferent BMIs
EBMIs rely on extracting features from recorded
neural activity in real time that are subsequently
transformed into control signals for actuating an
external device (e.g. a paralyzed limb) (Fig. 2).
They can be categorized based on the recorded
signal modality, which is predominantly neural
(but see (Sitaram et al. 2009) for an example of
metabolic activity that utilizes blood-oxygen-
level dependent (BOLD)).
In EBMIs, the neural readout may differ in the
temporal and spatial scales of variations, as well
as in information content, as shown in Fig. 3.
While the 1–2 ms APs elicited by individual neu-
rons are known to contain large information about
behavioral covariates (typically measured in bits
per second), they tend to be highly variable and
can only be recorded thus far using penetrating
microelectrodes (McNaughton et al. 1983; Drake
et al. 1988; Normann et al. 1999; Nicolelis 1999),
or using voltage-sensitive dye-based calcium
imaging at shallow cortical depths (Koester and
Brain-Machine Interface: Overview, Fig. 1 Basic ele-
ments of an afferent BMI. Features are extracted from the
sensory stimuli and then converted to electrical pulses that
stimulate target areas in the afferent pathway of the
corresponding sensory modality
Brain-Machine Interface: Overview
13

Sakmann 2000; Stosiek et al. 2003) – although the
latter signals have not been used in BMIs. While
local ﬁeld potentials (LFPs) can also be recorded
using penetrating electrodes, they contain less
information than spike trains and are believed to
provide a glimpse over the synchronized activity
of large populations of neurons within a few
100 microns from the electrode tip (Mitzdorf
1985; Katzner et al. 2009). Macroelectrodes,
either implanted subdurally to record ECoG or
ﬁxed extracranially to record surface EEG, offer
another less risky alternative for EBMI since they
are less- or noninvasive (Wolpaw et al. 2002;
McFarland et al. 2005; Thongpang et al. 2011).
Historically, work in efferent BMIs (EBMIs) is
more recent than ABMIs. The ﬁrst EBMI dates
back to the pioneering work of Fetz (1969) and
was based on a single-neuron recording from a
monkey’s precentral “motor” cortex. In this set-
ting, the BMI was a single-input/single-output
(SISO) system where the ﬁring rate of a single
neuron was integrated over small time intervals
and used to control an illuminated meter whose
pointer deﬂection was proportional to the activity
integrator’s output. The animal had to volitionally
modulate the ﬁring rate of the selected neuron in
order to bring the meter to a predetermined
threshold for reward. This operant conditioning
paradigm constitutes the ﬁrst proof of concept of
an EBMI, though was not used to actuate any
limbs. This approach has been extended in mod-
ern EBMIs to involve the simultaneous recording
and use of hundreds of neurons that are subse-
quently transformed through a “decoding” ﬁlter to
generate control signals that actuate multiple
degrees of freedom (DOFs) (Hochberg et al.
2012). As such, these BMIs are considered
multi-input/multi-output systems (MIMOs).
Cerebral BMIs
Another class of unidirectional BMIs – neither
designed to directly cause sensation nor produce
movements – is designed to stimulate neurons in
the central nervous system to directly inhibit path-
ological behavior in subjects with neurological
impairment. An example of such systems is deep
brain stimulators (DBS), in which an electrode
array is implanted in the subthalamic nucleus
(STN) of a Parkinsonian patient, and macro-
stimulation of the STN through a few electrode
leads ameliorates the disease symptoms by reduc-
ing bradykinesia and tremor (Coffey 2009). Sim-
ilar designs are intended to regulate mood
disorders (Mayberg et al. 2005; Hajcak et al.
Brain-Machine Interface: Overview, Fig. 2 Basic ele-
ments of an efferent BMI. The read out of neural activity
can be a single signal source recorded from a target area
within an efferent pathway – or multiple sources fused
together within the transform block, aka the “decoder” –
to actuate an artiﬁcial device. The user is able to continu-
ously monitor changes in the state of the actuator in real
time, typically through visual feedback
10-1
10 −3
Action
Potentials
LFP
EEG
ECoG
10 −6
10
1
10 −3
1
Space
(mm)
Information
Rate (bps)
Time
(sec)
10 −2
1
10 2
Brain-Machine Interface:
Overview, Fig. 3 Spatial,
temporal, and information
characteristics of neural
signals recorded from the
brain for efferent BMI
operation. (Adapted from
Oweiss 2010)
14
Brain-Machine Interface: Overview

2010) or abate epileptic seizures (Theodore and
Fisher 2007; Halpern et al. 2008; Boon et al. 2009;
Jones 2010).
Bidirectional BMIs
Bidirectional – or sometimes referred to as “recur-
rent” – BMIs differ from unidirectional BMIs in
that neural measurements are used in the input to
the transform to compute stimulation parameters
in real time. As such, they are considered “closed
loop” compared to unidirectional BMIs that are
“open loop.” This speciﬁc feature allows stimula-
tion to be dynamic and to follow the dynamics of
the neural input to the transform, which may be
volitionally modulated by the subject in certain
applications (Fig. 4).
Sensorimotor BMIs
This class of bidirectional BMIs combines record-
ing and stimulation to restore one or more func-
tions. For example, motor signals can be recorded
from cortical areas and used to directly control
muscle contraction or hand grasp via FES
(Moritz et al. 2008; Ethier et al. 2012). These
signals can also be used to stimulate the spinal
cord below the injury site using parameters extra-
cted from cortical signals in real time (Harkema
et al. 2011; Moritz et al. 2007; Shanechi et al.
2014) or via stimulation of peripheral nerves
(Navarro et al. 2005; Micera and Navarro 2009).
In that respect, this is a class of bidirectional BMIs
that restores motor function through FES.
Another class of bidirectional BMIs combines
efferent and afferent BMIs to restore movements
as well as sensory feedback in the form of touch
and proprioception. The importance of such
design
is
the
indispensible
role
that
somatosensation plays in the integration and coor-
dination of limb movements, particularly for com-
plex upper limb functions such as reaching and
grasping. There is debate over which site(s) to
stimulate to restore somatosensory feedback, but
the majority of approaches have focused on stim-
ulation of peripheral nerves (Raspopovic et al.
2014). Peripheral stimulation, however, is not fea-
sible in patients with spinal cord injury (SCI).
Recent reports devised protocols for these patients
based on subcortical (Daly et al. 2012; Liu et al.
2011) or cortical stimulation (Berg et al. 2013;
Tabot et al. 2013).
A third class of bidirectional BMIs aims to
induce plastic changes in neural circuits by using
patterns of activity recorded from one brain site to
stimulate activity in another distal brain site in real
time (Jackson et al. 2006). Key to induce this
plasticity is to ensure stimulation is delivered
within a few milliseconds of recording spike
events so as to promote LTP and LTD of synaptic
connections, consistent with Hebbian plasticity
(Hebb 1949). It is contended that this type of
plasticity triggers functional as well as structural
changes in the targeted neural circuits (Lucas and
Fetz 2009); the longevity of this plasticity, how-
ever, is yet to be demonstrated, since neurons
return to their original encoding properties shortly
after stimulation is terminated.
Cognitive BMIs
Certain brain areas have speciﬁc cytoarchitectonic
architecture with known role in cognitive func-
tions, such as the hippocampus role in the forma-
tion and maintenance of long-term episodic
memory (Bliss and Collingridge 1993). This
knowledge is critical to the ability to restore
Brain-Machine Interface: Overview, Fig. 4 Basic ele-
ments of a bidirectional – or recurrent – BMI. Neural
activity is read out to control an actuator. In turn, changes
in the actuator state are measured and used to provide
information back to the nervous system through stimula-
tion patterns of relevant areas in the peripheral nerve, the
spinal cord, or the brain
Brain-Machine Interface: Overview
15

inter-areal as well as intra-areal information
exchange by means of artiﬁcial devices. In this
setting, neural activity is recorded from one
upstream
region
(e.g.,
hippocampus
CA3)
known to provide feedforward information to
downstream regions (e.g., CA1) (Yeckel and
Berger 1990). When the communication between
the two regions is disrupted due to malfunctioning
neural tissue, eventually causing memory loss,
activity recorded from CA3 is “decoded” to pre-
dict the input to the CA1 region. The BMI then
uses this prediction to derive the stimulation pat-
terns needed to evoke biomimetic activity at the
output of the CA1 region. Thus, the transform
model in this BMI design lumps both a “decoder”
of the input neural activity in CA3 and an
“encoder” of the patterns to be evoked in CA1
through
stimulation
in
one
block
(Song
et al. 2007).
Bidirectional BMIs for Neurological Disorders
This class of BMIs extends cerebral BMIs to
include a closed loop design that records neural
activity and uses features extracted from this
recordings to dynamically adjust stimulation pat-
terns in real time (Rosin et al. 2011) (Carlson et al.
2013; Afshar et al. 2012). A similar approach is
used to reduce or prevent seizures from occurring
in drug-resistant epileptic patients (Nagel and
Najm 2009) (Morrell and RNS System in Epi-
lepsy Study Group 2011). This strategy is more
advantageous compared to resection of the parts
of the brain where hypothesized epileptic foci
reside, which has been the standard clinical treat-
ment for many years. Other examples use a similar
concept to treat brain injury or psychiatric dis-
eases, such as traumatic brain injury (Schiff et al.
2007), major depression (Malone et al. 2009),
obsessive compulsive disorders (Goodman et al.
2010), among others (Mohr 2008).
The Computing Device
A key element in BMI design is the computational
capability of the device and the corresponding
number of inputs and outputs. For example, in
SISO BMIs, computations can be as simple as
the detection of APs presence in a noisy electrode
recording followed by a spike count within a ﬁxed
time interval (usually 50 ms). They can also consist
of complex time varying (Kalman 1965) – and
sometimes nonlinear (Hampson et al. 2013) – trans-
formation of multiple electrode recordings in
MIMO BMIs. EBMIs in particular are designed
to have the decoder extract neural features
(typically the ﬁring rate of individual cells) to
determine the “state” of the population. Because
this state varies in time, it can be described by a
“trajectory” in a neural state space (Oweiss 2010;
Badreldin et al. 2013; Churchland et al. 2012). The
transform then ﬁlters this state to reduce its dimen-
sion to a smaller number of variables that can be
subsequently used to control the actuator (Gilja
et al. 2012). As such, the subject is required to
learn this transformation with extended practice.
Likewise, in ABMIs, the computing device
extracts features from signals measured in the
outside world to inﬂuence the “trajectory” of the
stimulation parameter(s) in a stimulation parame-
ter space. Because the mapping between the tra-
jectory of natural stimuli in the task space and the
evoked response in the neural state space – which
is typically of much higher dimension – is
unknown, methods for ﬁnding the “best” stimula-
tion strategies in “open loop” BMIs are far from
optimal. As such, the experimenter/clinician
assumes the role of ﬁnding an optimal “tune-up”
of stimulation parameters to restore a speciﬁc
function by trial and error. For example, stimula-
tion parameters (e.g., pulse frequency) in DBS
systems are manually adjusted to ﬁt each patient
and reduce tremor (Kuncel et al. 2006). The lack
of selectivity of electrical stimulation and the
absence of knowledge of the dynamics of the
neural circuits affected by stimulation, however,
make this approach a real challenge. As such,
characterizing
the
dynamics
of
stimulation-
evoked neural activity in upstream or downstream
circuits becomes important in order to optimize
the stimulation “dose” for restoring a desired
function and to minimize any side effects over
the long term (Liu et al. 2010, 2011; Kuncel
et al. 2008). Thus, studying the neural system
being stimulated by an artiﬁcial device may offer
advantages in clinical BMI applications.
16
Brain-Machine Interface: Overview

Cross-References
▶Auditory Prosthesis
▶Cortical Motor Prosthesis
▶Decoding Field Potentials
▶Deep Brain Stimulation (Models, Theory,
Techniques): Overview
▶Hippocampal Memory Prosthesis
▶Memory Decoding Model
▶Neural Decoding
▶Noninvasive Brain-Computer Interfaces
▶Recurrent Brain-Computer Interfaces
▶Somatosensory Prosthesis
▶Vestibular Prosthesis, Interface
▶Vision Prosthesis
References
Adrian ED, Bronk DW (1928) The discharge of impulses
in motor nerve ﬁbres: part I. Impulses in single ﬁbres of
the phrenic nerve. J Physiol 66:81–101
Afshar P, Khambhati A, Stanslaski S, Carlson D, Jensen R,
Linde D et al (2012) A translational platform for pro-
totyping closed-loop neuromodulation systems. Front
Neural Circuits 6:117
Badreldin
I,
Sutherland
J,
Vaidya
M,
Elerya
A,
Balasubramanian K, Fagg A et al (2013) Unsupervised
decoder initialization for brain–machine interfaces
using neural state space dynamics. Presented at the
IEEE international conference on neural engineering,
San Diego, 2013
Berg JA, Dammann JF III, Tenore FV, Tabot GA, Boback
JL, Manfredi LR et al (2013) Behavioral demonstration
of a somatosensory neuroprosthesis. IEEE Trans Neu-
ral Syst Rehabil Eng 21:500–507
Bliss TV, Collingridge GL (1993) A synaptic model of
memory: long-term potentiation in the hippocampus.
Nature 361:31–39
Boon P, Raedt R, de Herdt V, Wyckhuys T, Vonck K (2009)
Electrical stimulation for the treatment of epilepsy.
Neurotherapeutics 6:218–227
Carlson D, Linde D, Isaacson B, Afshar P, Bourget D,
Stanslaski S et al (2013) A ﬂexible algorithm frame-
work for closed-loop neuromodulation research sys-
tems. Annu Int Conf IEEE Eng Med Biol Soc
2013:6146–6150
Churchland MM, Cunningham JP, Kaufman MT, Foster
JD, Nuyujukian P, Ryu SI et al (2012) Neural popula-
tion dynamics during reaching. Nature 487:51–56
Coffey RJ (2009) Deep brain stimulation devices: a brief
technical history and review. Artif Organs 33:208–220
Daly J, Liu J, Aghagolzadeh M, Oweiss K (2012) Optimal
space-time precoding of artiﬁcial sensory feedback
through
mutichannel
microstimulation
in
bi-directional brain–machine interfaces. J Neural Eng
9:065004
de Balthasar C, Patel S, Roy A, Freda R, Greenwald S,
Horsager A et al (2008) Factors affecting perceptual
thresholds in epiretinal prostheses. Invest Ophthalmol
Vis Sci 49:2303–2314
Drake KL, Wise KD, Farraye J, Anderson DJ, BeMent SL
(1988) Performance of planar multisite microprobes in
recording extracellular single-unit intracortical activity.
IEEE Trans Biomed Eng 35:719–732
Ethier C, Oby ER, Bauman MJ, Miller LE (2012) Restora-
tion of grasp following paralysis through brain-
controlled stimulation of muscles. Nature 485:368–371
Fetz EE (1969) Operant conditioning of cortical unit activ-
ity. Science 163:955–958
Fowler R, Galvani L (1793) Experiments and observations
relative
to
the
inﬂuence
lately
discovered
by
M. Galvani and commonly called animal electricity.
T. Duncan, Edinburgh
Fritsch G, Hitzig E (1870) Uber die elektrische erregbarkeit
des grosshirns. Arch Anat Physiol 37:300–332
Galvani L (1791) De viribus electricitatis in motu
musculari: commentarius. Tip. Istituto delle Scienze,
Bologna, p 58. 4 tavv. ft; in 4.; DCC. f. 70, vol 1
Georgopoulos AP, Schwartz AB, Kettner RE (1986) Neu-
ronal population coding of movement direction. Sci-
ence 233:1416–1419
Gilja V, Nuyujukian P, Chestek CA, Cunningham JP, Yu
BM, Fan JM et al (2012) A high-performance neural
prosthesis enabled by control algorithm design. Nat
Neurosci 15:1752–1757
Goodman WK, Foote KD, Greenberg BD, Ricciuti N,
Bauer R, Ward H et al (2010) Deep brain stimulation
for intractable obsessive compulsive disorder: pilot
study using a blinded, staggered-onset design. Biol
Psychiatry 67:535–542
Hajcak G, Anderson BS, Arana A, Borckardt J, Takacs I,
George MS et al (2010) Dorsolateral prefrontal cortex
stimulation modulates electrocortical measures of
visual attention: evidence from direct bilateral epidural
cortical stimulation in treatment-resistant mood disor-
der. Neuroscience 170:281–288
Halpern CH, Samadani U, Litt B, Jaggi JL, Baltuch GH
(2008) Deep brain stimulation for epilepsy. Neurother-
apeutics 5:59–67
Hampson RE, Song D, Opris I, Santos LM, Shin DC,
Gerhardt GA et al (2013) Facilitation of memory
encoding in primate hippocampus by a neuroprosthesis
that promotes task-speciﬁc neural ﬁring. J Neural Eng
10:066013
Harkema S, Gerasimenko Y, Hodes J, Burdick J,
Angeli C, Chen Y et al (2011) Effect of epidural stim-
ulation of the lumbosacral spinal cord on voluntary
movement, standing, and assisted stepping after motor
complete paraplegia: a case study. Lancet 377:
1938–1947
Hebb DO (1949) The organization of behavior; a neuro-
psychological theory. Wiley, New York
Hochberg LR, Bacher D, Jarosiewicz B, Masse NY,
Simeral JD, Vogel J et al (2012) Reach and grasp by
Brain-Machine Interface: Overview
17

people with tetraplegia using a neurally controlled
robotic arm. Nature 485:372–375
House WF, Urban J (1973) Long term results of electrode
implantation and electronic stimulation of the cochlea
in man. Ann Otol Rhinol Laryngol 82:504–517
Hubel DH, Wiesel TN (1962) Receptive ﬁelds, binocular
interaction and functional architecture in the cat’s
visual cortex. J Physiol 160:106
Jackson A, Mavoori J, Fetz EE (2006) Long-term motor
cortex plasticity induced by an electronic neural
implant. Nature 444:56–60
Jones N (2010) Epilepsy: DBS reduces seizure frequency
in refractory epilepsy. Nat Rev Neurol 6:238
Kalman RE (1965) Irreducible realizations and the degree
of a rational matrix. J Soc Ind Appl Math 13:520–544
Katzner S, Nauhaus I, Benucci A, Bonin V, Ringach DL,
Carandini M (2009) Local origin of ﬁeld potentials in
visual cortex. Neuron 61:35–41
Koester HJ, Sakmann B (2000) Calcium dynamics associ-
ated with action potentials in single nerve terminals of
pyramidal cells in layer 2/3 of the young rat neocortex.
J Physiol 529(Pt 3):625–646
Kuncel AM, Cooper SE, Wolgamuth BR, Clyde MA,
Snyder SA, Montgomery EB Jr et al (2006) Clinical
response to varying the stimulus parameters in deep
brain stimulation for essential tremor. Mov Disord
21:1920–1928
Kuncel AM, Cooper SE, Grill WM (2008) A method to
estimate the spatial extent of activation in thalamic deep
brain stimulation. Clin Neurophysiol 119:2148–2158
Liu J, Khalil H, Oweiss K (2010) Feedback control of the
spatiotemporal ﬁring pattern of a basal ganglia micro-
circuit model. BMC Neurosci 11:O16
Liu JB, Khalil HK, Oweiss KG (2011) Neural feedback for
instantaneous spatiotemporal modulation of afferent
pathways in bi-directional brain–machine interfaces.
IEEE Trans Neural Syst Rehabil Eng 19:521–533
Lucas TH, Fetz EE (2009) Motor cortex plasticity driven
by artiﬁcial feedback from an autonomous, closed-loop
neural implant. Neurosurgery 65:420–421
Malone DA Jr, Dougherty DD, Rezai AR, Carpenter LL,
Friehs GM, Eskandar EN et al (2009) Deep brain stimu-
lation of the ventral capsule/ventral striatum for treatment-
resistant depression. Biol Psychiatry 65:267–275
Mayberg HS, Lozano AM, Voon V, McNeely HE,
Seminowicz D, Hamani C et al (2005) Deep brain
stimulation for treatment-resistant depression. Neuron
45:651–660
McFarland DJ, Sarnacki WA, Vaughan TM, Wolpaw JR
(2005) Brain–computer interface (BCI) operation: sig-
nal and noise during early training sessions. Clin
Neurophysiol 116:56–62
McNaughton BL, O’Keefe J, Barnes CA (1983) The stereo-
trode: a new technique for simultaneous isolation of
several single units in the central nervous system from
multiple unit records. J Neurosci Methods 8:391–397
Micera S, Navarro X (2009) Bidirectional interfaces with
the peripheral nervous system. Int Rev Neurobiol
86:23–38
Mitzdorf U (1985) Current source-density method and
application in cat cerebral cortex: investigation of
evoked potentials and EEG phenomena. Physiol Rev
65:37–100
Mohr P (2008) Deep brain stimulation in psychiatry. Neuro
Endocrinol Lett 29(Suppl 1):123–132
Moritz CT, Lucas TH, Perlmutter SI, Fetz EE (2007) Fore-
limb movements and muscle responses evoked by
microstimulation of cervical spinal cord in sedated
monkeys. J Neurophysiol 97:110–120
Moritz CT, Perlmutter SI, Fetz EE (2008) Direct control of
paralysed
muscles
by
cortical
neurons.
Nature
456:639–642
Morrell MJ, RNS System in Epilepsy Study Group
(2011) Responsive cortical stimulation for the treat-
ment of medically intractable partial epilepsy. Neurol-
ogy 77:1295–1304
Nagel SJ, Najm IM (2009) Deep brain stimulation for
epilepsy. Neuromodulation 12:270–280
Navarro X, Krueger TB, Lago N, Micera S, Stieglitz T,
Dario P (2005) A critical review of interfaces with the
peripheral
nervous
system
for
the
control
of
neuroprostheses and hybrid bionic systems. J Peripher
Nerv Syst 10:229–258
Nicolelis MAL (1999) Methods for neural ensemble
recordings. CRC Press, Boca Raton
Normann R, Maynard EM, Rousche PJ, Warren DJ
(1999) A neural interface for a cortical vision prosthe-
sis. Vis Res 39:2577–2587
O’Keefe J (1979) A review of the hippocampal place cells.
Prog Neurobiol 13:419–439
Oweiss K (2010) Statistical signal processing for neurosci-
ence and neurotechnology, 1st edn. Academic/Elsevier,
Burlington
Ranck JB Jr (1973) Studies on single neurons in dorsal
hippocampal formation and septum in unrestrained
rats: part I. Behavioral correlates and ﬁring repertoires.
Exp Neurol 41:462–531
Raspopovic S, Capogrosso M, Petrini FM, Bonizzato M,
Rigosa J, Di Pino G et al (2014) Restoring natural
sensory feedback in real-time bidirectional hand pros-
theses. Sci Transl Med 6:222ra19
Rosin B, Slovik M, Mitelman R, Rivlin-Etzion M, Haber
SN, Israel Z et al (2011) Closed-loop deep brain stim-
ulation is superior in ameliorating parkinsonism. Neu-
ron 72:370–384
Schiff N, Giacino J, Kalmar K, Victor J, Baker K, Gerber
M et al (2007) Behavioural improvements with tha-
lamic stimulation after severe traumatic brain injury.
Nature 448:600–603
Shanechi MM, Hu RC, Williams ZM (2014) A cortical–
spinal prosthesis for targeted limb movement in para-
lysed primate avatars. Nat Commun 5:3237
Sitaram R, Caria A, Birbaumer N (2009) Hemodynamic
brain–computer interfaces for communication and
rehabilitation. Neural Netw 22:1320–1328
Song D, Chan RHM, Marmarelis VZ, Hampson RE,
Deadwyler SA, Berger TW (2007) Nonlinear dynamic
modeling
of
spike
train
transformations
for
18
Brain-Machine Interface: Overview

hippocampal-cortical prostheses. IEEE Trans Biomed
Eng 54:1053–1066
Stosiek C, Garaschuk O, Holthoff K, Konnerth A (2003) In
vivo two-photon calcium imaging of neuronal net-
works. Proc Natl Acad Sci U S A 100:7319
Tabot GA, Dammann JF, Berg JA, Tenore FV, Boback JL,
Vogelstein RJ et al (2013) Restoring the sense of touch
with a prosthetic hand through a brain interface. Proc
Natl Acad Sci U S A 110:18279–18284
Theodore WH, Fisher R (2007) Brain stimulation for epi-
lepsy. Acta Neurochir Suppl 97:261–272
Thongpang S, Richner TJ, Brodnick SK, Schendel A,
Kim
J,
Wilson
JA
et
al
(2011)
A
micro-
electrocorticography platform and deployment strate-
gies for chronic BCI applications. Clin EEG Neurosci
42:259–265
Wolpaw JR, Birbaumer N, McFarland DJ, Pfurtscheller G,
Vaughan TM (2002) Brain–computer interfaces for
communication
and
control.
Clin
Neurophysiol
113:767–791
Yeckel MF, Berger TW (1990) Feedforward excitation of
the hippocampus by afferents from the entorhinal cor-
tex: redeﬁnition of the role of the trisynaptic pathway.
Proc Natl Acad Sci U S A 87:5832–5836
Brain-Scale Networks:
Overview
Theoden I. Netoff
Department of Biomedical Engineering,
University of Minnesota, Minneapolis, MN, USA
Definition
The brain is composed of many neurons, func-
tional areas, and layers. Together these compo-
nents work as a network to produce behavior. At
minimum, the network behavior is determined by
four things: (1) the network inputs, (2) the dynam-
ics of the individual nodes, (3) the coupling func-
tions between the nodes, and (4) the topology.
This encyclopedia section will provide a brief
overview on characterizing brain-scale networks.
Often the coupled system will have emergent
behaviors, behaviors that could not be predicted
from analysis of the individual components alone.
Understanding how the brain functions requires
an understanding of how components work
together in a network. In many diseases the
cause cannot be pinpointed to dysfunction or
failure of a single component, such as an ion
channel mutation. Instead, subtle changes in cel-
lular behavior may lead homeostatic mechanisms
to alter the coupling between the neurons and
brain areas, resulting in pathological activity
such as synchronous population oscillations or
unchecked excitability. The ultimate goal in
applying the network theory to understanding
connections within the brain is to develop a mea-
sure that can explain the emergence of patholog-
ical behaviors and to perhaps develop approaches
to treating diseases.
But ﬁrst, we will introduce a few common
terms. A network is a collection of coupled com-
ponents.
Generally,
when
the
components
coupled together are different elements, it is
referred to as a system. If the components of the
system are similar and interchangeable, it is
instead called a network. The components in the
network are referred to as nodes, which can be
individual neurons or brain regions. The coupling
between the nodes is referred to as edges, which
can be synapses or ﬁber paths. Coupling in the
nervous system is generally through chemical
synapses which are directional, where the cou-
pling of neuron or region A onto B will be differ-
ent than B onto A. However, for networks of
neurons coupled through electrical synapses, the
coupling can be undirected. Networks are consid-
ered weighted if the strengths of coupling
between
nodes
have
a
distribution
and
unweighted if they are all the same.
Generally, the dynamics of the nodes and cou-
pling function are highly nonlinear. A linear
response is deﬁned as given twice the input, the
output will be twice as strong. However, because
neurons have thresholds and synapses are plastic,
the responses are very nonlinear. Furthermore,
neurons are a mixture of deterministic behavior,
where its behavior can be determined from its past
and its inputs, and stochastic, where the activity is
also due to some noise in the system which cannot
be accounted for.
The statistics of the coupling within a network
is called the topology. A list of all the connections
within a network is called the graph. If nodes
have a physical location in space, such as brain
regions, and coupling is dependent on the
Brain-Scale Networks: Overview
19

distance, the network is considered to have a
geometry.
In summary, neuronal networks are nonlinear,
directional weighted graphs with a geometry.
These networks are called complex networks,
and few tools have been developed to analyze
them. The development of these network analysis
tools is at the cutting edge.
Detailed Description
The goal of these encyclopedia entries is to pro-
vide an introduction into the network theory. In
the ﬁrst entry (▶“Network Theory in Neurosci-
ence”), there is an overview of the network theory
and its applications to diseases. In ▶“Functional
Network Observations of Diseased Brain States,”
there is an introduction to functional networks in
neuroscience. In ▶“Determining Network Struc-
ture from Data: Nonlinear Modeling Methods,”
we will introduce methods for reconstructing net-
works from the data using nonlinear measures. In
▶“Master Stability Function for Globally Syn-
chronized Systems,” we introduce a universal
approach to determining if a network will syn-
chronize given the dynamics of the individual
components and the network topology, through
the analysis of the master stability function. In
▶“Connectionist Models of CPG Networks,”
we
will
provide
an
introduction
to
non-
synchronous network behaviors, such as seen in
central pattern generators. In ▶“Neuropathol-
ogies and Networks,” we will introduce patholog-
ical network function to characterize diseases.
Cross-References
▶Connectionist Models of CPG Networks
▶Determining Network Structure from Data:
Nonlinear Modeling Methods
▶Functional Network Observations of Diseased
Brain States
▶Master Stability Function for Globally
Synchronized Systems
▶Network Theory in Neuroscience
▶Neuropathologies and Networks
Further Reading
Boccaletti S, Latora V, Moreno Y, Chavez M, Hwang DU
(2006) Complex networks: structure and dynamics.
Phys Rep Rev. Sect Phys Lett 424:175
Mirollo RE, Strogatz SH (1990) Synchronization of pulse-
coupled biological oscillators. SIAM J Appl Math 50:
1645
Sporns O (2011) Networks of the brain. MIT, Cambridge
Strogatz SH (2000) From Kuramoto to Crawford: explor-
ing the onset of synchronization in populations of
coupled oscillators. Phys D: Nonlinear Phenom 143:1
Strogatz SH (2003) Sync: the emerging science of sponta-
neous order, ﬁrst edn. Hyperion, New York
Watts DJ, Strogatz SH (1998) Collective dynamics of
‘small-world’ networks. Nature 393:440
Brainstem Processing:
Overview
Robert Butera1 and Kendall Morris2
1School of Electrical and Computer Engineering,
Laboratory for Neuroengineering, Georgia
Institute of Technology, Atlanta, GA, USA
2College of Medicine, Molecular Pharmacology
& Physiology, University of South Florida,
Tampa, FL, USA
Detailed Description
The brainstem has received considerably less
attention by computational neuroscientists than
other regions of the mammalian brain. This sec-
tion highlights key areas that have received atten-
tion: neural circuitry underlying the generation
and control of the respiratory rhythm, the
baroreﬂex underlying the regulation of blood
pressure, and pathways for pain processing.
Entries in this section cover all three of these
areas.
The
article
titled
▶“Baroreﬂex
Models”
reviews over two decades years of computational
models focused on neural pathways where blood
ﬂows in response to pressure changes in the cir-
culatory system mediated by the baroreﬂex
pathway.
Much attention has been given to the modeling
of respiratory rhythm generation. The generation
20
Brainstem Processing: Overview

and control of breathing is covered by two entries
in this section. The entry titled ▶“Pre-Botzinger
Complex Rhythm Generation” surveys computa-
tional models of rhythmic activity in the pre--
Bötzinger
complex,
which
underlies
the
inspiratory component of respiratory rhythm gen-
eration and is a central pattern-generating circuit
itself when isolated. The article titled ▶“Control
of Breathing, Integration of Adaptive Reﬂexes”
describes computational models of adaptation and
plasticity of the respiratory rhythm in response to
external perturbations. There are also relevant
articles in other sections of this encyclopedia,
including ▶“Computational Models of Mamma-
lian Respiratory CPG” and ▶“Brainstem Moto-
neurons, Models of”.
The area of pain processing has received much
less attention by computational neuroscientists.
The article titled ▶“Pain Processing Pathway
Models” summarizes computational efforts in
this area.
Cross-References
▶Baroreﬂex Models
▶Brainstem Motoneurons, Models of
▶Computational Models of Mammalian
Respiratory CPG
▶Control of Breathing, Integration of Adaptive
Reﬂexes
▶Pain Processing Pathway Models
▶Pre-Botzinger Complex Rhythm Generation
Cable Theory: Overview
William R. Holmes
Department of Biological Sciences, Ohio
University, Athens, OH, USA
Detailed Description
Cable theory has a prominent place in neurosci-
ence as it forms the basis of models of single
neurons, axons, and dendrites and has led to an
understanding of intraneuronal signaling. Starting
with the conceptual model of a patch of membrane
as an electric circuit, application of basic princi-
ples from physics leads to a mathematical equa-
tion, the cable equation. We start this section with
an article on the cable equation, discussing the
history of the cable equation in neuroscience,
showing the derivation of the cable equation, pro-
viding solutions for the inﬁnite cylinder, steady-
state solutions for semi-inﬁnite and ﬁnite cables
and branched dendritic trees and transient solu-
tions, concluding with a discussion of insights
from cable theory. Several parameters play key
roles in intraneuronal signaling including mem-
brane capacitance, axial resistivity, the space
(length) constant, and the time constant, and
there are articles on each of these topics. From
there we have two articles describing work done
by Wilfrid Rall, widely regarded as the father of
computational neuroscience. The ﬁrst describes
how Rall applied cable theory to dendritic trees
and showed that given certain conditions a highly
branched dendritic tree could be reduced mathe-
matically to an equivalent cylinder. The simplicity
and elegance of the equivalent cylinder model
permitted mathematical analyses and insights
regarding signaling in otherwise morphologically
complex dendritic trees. The second describes
various formulas Rall derived to estimate the elec-
trotonic length of dendritic trees from simple volt-
age transients, formulas that have been applied in
hundreds of studies. Finally, there are three arti-
cles that discuss extensions of cable theory in
various ways. The ﬁrst presents the morphoelec-
trotonic transform, a procedure that addresses the
issue that attenuation of voltage from a synaptic
input from the dendrite to the soma is not simply
given by the electrotonic distance because of the
effect of boundary conditions. The morphoelec-
trotonic transform provides a graphical means to
visualize voltage attenuation both toward the
soma and away from the soma, directly and intu-
itively. The second introduces the continuum the-
ory for modeling dendritic spines. While there are
various ways to incorporate the effect of dendritic
spines in models, the continuum theory describes
a novel way of doing this within the cable equa-
tion. Lastly, insights can be obtained from the
Cable Theory: Overview
21

quasi-active approximation of a nonlinear cable.
Here classic passive cable theory is extended by
linearizing
voltage-dependent
conductances
around a given membrane potential cable allowing
the effect of voltage-dependent conductances on
dendritic ﬁltering to be studied mathematically.
Almost all of these articles assume membrane is
passive, but it is clear that both dendrites and axons
contain voltage-dependent conductances. Never-
theless, cable theory is important for providing a
basic understanding of intraneuronal signaling that
forms the basis for understanding what happens
with active conductances.
Cross-References
▶Cable Equation
▶Capacitance, Membrane
▶Dendritic Spines: Continuum Theory
▶Electrotonic Length, Formulas and Estimates
▶Equivalent Cylinder Model (Rall)
▶Morphoelectrotonic Transform
▶Quasi-active Approximation of Nonlinear
Dendritic Cables
▶Resistivity, Axial
▶Space (Length) Constant, Lambda, in Neuronal
Signaling
▶Time Constant, Tau, in Neuronal Signaling
Cerebellum: Overview
Fidel Santamaria
UTSA Neurosciences Institute, The University of
Texas at San Antonio, San Antonio, TX, USA
Definition
The cerebellum processes sensory and motor
information. Structurally is divided in the cerebel-
lar cortex network and the deep cerebellar nuclei.
Inputs to the cerebellar cortex arrive via the mossy
ﬁbers from multiple sources; mossy ﬁbers contact
granule cells, Golgi cells and unipolar brush cells.
The second input to the cerebellar cortex comes
from the inferior olive in the form of climbing
ﬁbers that synapse Purkinje cells. The output of
the cerebellar cortex is provided by Purkinje cells
that integrate the synaptic activity of climbing
ﬁbers, granule cells, and inhibitory interneurons.
Golgi cells provide an inhibitory feedback mech-
anism to the granule cell layer. The deep cerebel-
lar nuclei are the ﬁnal integrator of cerebellar
information. Cells in the deep cerebellar nuclei
receive input from Purkinje cells, mossy ﬁbers
and climbing ﬁbers.
Detailed Description
Cerebellar Cortex
The widely studied cerebellar cortex is composed,
mainly, of four types of neurons: Granule cells,
Purkinje cells, inhibitory interneurons (stellate
and basket), and Golgi cells. There are two inputs
to the cerebellar cortex provided by mossy ﬁbers
and climbing ﬁbers. Climbing ﬁbers are axons
from inferior olivary neurons. Mossy ﬁbers are a
collection of axons from multiple areas that can
carry direct sensory or cerebro cortical informa-
tion. The arrangement of the cerebellar cortical
neurons is regular across the cortex. Mossy ﬁbers
contact granule cells and Golgi cells. Granule
cells send an axon into the molecular layer that
bifurcates. The bundle of bifurcated axons is
known as the parallel ﬁbers. Parallel ﬁbers contact
Golgi cells, inhibitory interneurons and Purkinje
cells as they traverse the cerebellar cortex.
Climbing ﬁbers synapse onto Purkinje cells.
The particular arrangement of granule cell-to-
Purkinje cell connectivity was very attractive to
computational scientist early on. The ‘Beam
Hypothesis’ stated that a focal stimulation of the
granule cells would be transformed into a sequen-
tial activation of Purkinje cells along the path of
the parallel ﬁbers (Braitenberg and Atwood
1958). The evoked inhibitory activity from paral-
lel ﬁber to inhibitory interneurons would reduce
the activity of the Purkinje cells on either side of
the beam of activated Purkinje cells. The feedback
mechanism provided by Golgi cells on to granule
cells would then reduce over-excitability. The
central mechanism of information processing in
22
Cerebellum: Overview

this model is the existence of a beam of activated
Purkinje cells following the local stimulation of
granule cells.
The lack of beams of Purkinje cell activity
reported by multiple teams over the years has
prompted a discussion on how the cerebellar cor-
tex processes information. Several reports have
shown that Purkinje cells are activated only
when they are directly above the cluster of stimu-
lated granule cells. The Purkinje cells along the
parallel ﬁber path are either inhibited or show no
change in ﬁring rate. Experimental and computa-
tional evidence has shown that this is due to fast
feed-forward inhibition from the inhibitory inter-
neuron on Purkinje cells concomitantly stimu-
lated by parallel ﬁbers. In fact, blocking of
inhibition reveals a beam of Purkinje cell activity
as predicted by the Beam Hypothesis (Santamaria
et al. 2007). Thus, the emergent understanding is
not that different from a center surround receptive
ﬁeld model. Under this perspective, granule cell
synapses are effective in driving Purkinje cells to
ﬁre only when the Purkinje cells are close to the
site of stimulation. Parallel ﬁbers provide excit-
atory and feed forward inhibitory input to
Purkinje cells far from the site of stimulation
without this stimulation being reﬂected in changes
of the Purkinje cell ﬁring rate. However, the den-
dritic stimulation results in activation of voltage
sensitive calcium channels and calcium activated
potassium channels, modifying the excitability of
the dendrite. This dendritic activation can then
modify the response of the Purkinje cell to direct
granule cell stimulation. Therefore, the long range
effect of parallel ﬁbers is to provide contextual
information.
The Purkinje cell was one of the ﬁrst neurons to
be studied using the compartmental modeling
approach. The large amount of information on
somatic and dendritic conductances permitted to
develop detailed compartmental models. These
models have been reused and enhanced over the
years to study different aspects of cerebellar func-
tion (De Schutter and Bower 1994; Steuber et al.
2007). There are also several models of inhibitory
interneurons, Golgi cells and granule cells. Recent
work suggests that the Purkinje cell might be
simpliﬁed to a perceptron (Clopath et al. 2012).
Synaptic Plasticity
Long term depression (LTD) in the parallel
ﬁber-to-Purkinje cell synapses is a model of
learning and memory (Ito 2006; Feil et al.
2003). LTD is induced by the concomitant acti-
vation of the climbing ﬁber input and stimula-
tion of a small bundle of parallel ﬁbers. It is
widely assumed that the climbing ﬁber stimuli
carries a training or error signal, while the
encoding signals, e.g., motor command, is
encoded in the parallel ﬁber activity. The bio-
chemical transduction cascade is very well
understood and has been modeled using mass
action and Monte Carlo models (Doi et al.
2005). These models and their experimental
tests have pointed out to positive feedback bio-
chemical loop that triggers and sustains LTD.
Brieﬂy, LTD is induced by the inﬂux of calcium
ions through voltage sensitive channels and
release from intracellular stores. Calcium ions
activate protein kinase C (PKC) which eventu-
ally activates mitogen associated protein kinase
(MAPK). MAPK interacts with other substrates
to produce arachidonic acid which in return acti-
vates more PKC. PKC then continues working
in the feedback loop or phosphorylate AMPA
receptors. Phosphorylated AMPA receptors are
then internalized and the synaptic strength is
reduced. The activation of the feedback loop is
not a linear integrator of calcium concentration,
instead it acts as a leaky integrator, thus the total
amount and the temporal release of calcium
determines the expression of LTD (Tanaka
et al. 2007; Xia et al. 2000.).
Other Cells in the Cerebellar Cortex
There are other types of cells not traditionally
included in cerebellar models. Lugaro cells are
interneurons that are located in the granule cell
layer (Lainé and Axelrad 1996). These neurons
receive input from Purkinje cells and could be
connected among themselves with gap junctions.
These cells seem to be inhibitory and hypothe-
sized to work in the burst frequency response of
the granule cell layer. Another type of neuron,
Cerebellum: Overview
23

primarily located in the vestibulo cerebellum, is
the unipolar brush cell. These cells receive inhib-
itory inputs from Golgi cells and excitatory inputs
from mossy ﬁbers. Unipolar brush cells have mul-
tiple excitatory targets in the granule cell layer
(Mugnaini and Floris 1994).
The Deep Cerebellar Nuclei
The deep cerebellar nuclei (DCN – dentate,
interpositus, and fastigii) receive inhibitory inputs
from Purkinje cells and excitatory activity from
mossy ﬁbers and climbing ﬁbers. The cell from
the cerebellar nuclei generates the bundles of
axons that go to other areas of the brain. The
cells of the DCN can be thought as the ﬁnal and
global integrator of the input and output signals to
the cerebellum. DCN cells integrate inputs from
the brain stem, inferior olive, and spinal cord with
the Purkinje cell output from the cerebellar cortex.
There are excitatory and inhibitory DCN neurons
with their electrophysiological and computational
contributions still being studied. The precise spike
timing control of DCN spiking by Purkinje cell is
believed to be the mechanism of the precise motor
skills (Gauck and Jaeger 2000).
Summary
Although the function of the cerebellum is still
being debated, how it processes information is
becoming clearer due to vast amount of experi-
mental information from ultra-structure to electro-
physiology and behavior. Altogether, this large
amount of data allows computational scientist to
build realistic models at different levels of detail
that can be experimentally tested.
Cross-References
▶Deep Cerebellar Nuclei
▶Long Term Depression in the Granule Cell-
Purkinje Cell Synapse
▶Modeling Cerebellar Learning: Input
Minimization
▶Multiscale Modeling of Purkinje Cells
▶Olivocerebellar Pathway
References
Braitenberg V, Atwood RP (1958) Morphological obser-
vations on the cerebellar cortex. J Comp Neurol 109:
1–33
Clopath C, Nadal JP, Brunel N (2012) Storage of correlated
patterns in standard and bistable Purkinje cell models.
PLoS Comput Biol 8:e1002448
De Schutter E, Bower JM (1994) An active membrane
model of the cerebellar Purkinje cell. I. Simulation of
current clamps in slice. J Neurophysiol 71:375–400
Doi T, Kuroda S, Michikawa T, Kawato M (2005) Inositol
1,4,5-trisphosphate-dependent Ca2+ threshold dynam-
ics detect spike timing in cerebellar Purkinje cells.
J Neurosci 25:950–961
Feil R, Hartmann J, Luo C, Wolfsgruber W, Schilling K,
Feil S, Barski JJ, Meyer M, Konnerth A, De Zeeuw CI,
Hofmann F (2003) Impairment of LTD and cere-
bellar learning by Purkinje cell-speciﬁc ablation of
cGMP-dependent protein kinase I. J Cell Biol 163:
295–302
Gauck V, Jaeger D (2000) The control of rate and timing of
spikes in the deep cerebellar nuclei by inhibition.
J Neurosci 20:3006–3016
Ito M (2006) Cerebellar circuitry as a neuronal machine.
Prog Neurobiol 78:272–303
Lainé J, Axelrad H (1996) Morphology of the Golgi-
impregnated lugaro cell in the rat cerebellar cortex: a
reappraisal with a description of its axon. J Comp
Neurol 375:618–640
Mugnaini E, Floris A (1994) The unipolar brush cell: a
neglected neuron of the mammalian cerebellar cortex.
J Comp Neurol 339:174–180
Santamaria F, Tripp PG, Bower JM (2007) Feedforward
inhibition controls the spread of granule cell-induced
Purkinje
cell
activity
in
the
cerebellar
cortex.
J Neurophysiol 97:248–263
Steuber V, Mittmann W, Hoebeek FE, Silver RA, De
Zeeuw CI, Häusser M, De Schutter E (2007) Cerebellar
LTD and pattern recognition by Purkinje cells. Neuron
54:121–136
Tanaka K, Khiroug L, Santamaria F, Doi T, Ogasawara H,
Ellis-Davies
GC,
Kawato
M,
Augustine
GJ
(2007) Ca2+ requirements for cerebellar long-term syn-
aptic depression: role for a postsynaptic leaky integra-
tor. Neuron 54:787–800
Xia J, Chung HJ, Wihler C, Huganir RL, Linden DJ
(2000)
Cerebellar
long-term
depression
requires
PKC-regulated interactions between GluR2/3 and
PDZ domain-containing proteins. Neuron 28:499–510
24
Cerebellum: Overview

Computational
Neuroanatomy: Overview
David Mayerich1,2 and Yoonsuck Choe3
1Department of Electrical and Computer
Engineering, University of Houston, Houston,
TX, USA
2Beckman Institute for Advanced Science and
Technology, University of Illinois at
Urbana-Champaign, Urbana, IL, USA
3Department of Computer Science and
Engineering, Texas A&M University, College
Station, TX, USA
Definition
Computational neuroanatomy is a subﬁeld of neu-
roscience that aims to create functionally and
structurally accurate models of the nervous sys-
tem. These models are based on a wide range of
input data, including microscopy and electrophys-
iology. These data acquisition methods encom-
pass
the
broad
scales
across
which
neuroanatomy plays a role: (1) nanostructure
describing individual connections between cells,
(2) microstructure describing neuronal shape and
cell type, and (3) gross structure describing cross-
brain connectivity patterns. Computing in this
ﬁeld draws largely from the need to synthesize
multiple modalities to statistically generate accu-
rate models of morphology. For example, proper-
ties such as morphological structure, connection
topology, wiring principles, and growth rules are
statistically drawn from multiple sources and
combined into motifs representing large-scale
connectivity. High-performance computing can
then be leveraged to simulate and verify the
resulting models, with the end goal of studying
neural behavior in silico.
Detailed Description
Computational neuroanatomy deals with the
development of structural and functional models
of the nervous system. The structural data
forming the basis for these models are collected
across multiple scales using a variety of data-
acquisition and imaging modalities. Electron
microscopy is used to acquire ultrastructural
information of synapses, axon terminals, and
dendritic
spines
at
the
nanometer
scale.
Advances in serial sectioning electron micros-
copy have enabled the investigation of whole
neurons at this scale, with large-scale efforts
focused on three-dimensional reconstruction.
Optical microscopy (bright ﬁeld and ﬂuores-
cence) is often leveraged at the micrometer
scale to study neuronal morphology, local cir-
cuits, and long-range projections. Both physical
and optical sectioning is routinely used to
obtain 3D volume data. Magnetic resonance
imaging (MRI) and other noninvasive tech-
niques are frequently employed for mapping
cortical areas, major nuclei and ﬁber tracts, as
well as functional data across the entire brain
network.
Topics of study in computational neuroanat-
omy focus largely on model reconstruction and
generation. This includes data mining from mul-
timodal images using segmentation algorithms,
morphometry,
and
statistics.
This
raw
reconstructed data must also be analyzed to
establish fundamental rules for constructing via-
ble large-scale models. This requires topological
analysis (small world, scale-free, etc.), growth
and development models, organizational and
optimization principles (e.g., wiring length min-
imization, self-organization, etc.), representa-
tion and ontology, and neuroinformatics. In
addition, tools must be designed to enable viable
exploration of neuroanatomical structures across
multiple scale, integrating visualization, and
comparative analysis. One of the major goals
of comparative tools is the study of diseased
versus healthy neuroanatomy, as well as infer-
ring function from structure. Early studies
in computational neuroanatomy, especially at
the cellular level, were strongly motivated
by
the
need
for
morphologically
detailed
multicompartmental models for use in neuron
simulators such as NEURON (Carnevale and
Computational Neuroanatomy: Overview
25

Hines 2006), GENESIS (Bower and Beeman
2012), and others (Ascoli 2002).
Overview of Chapters
The computational neuroanatomy section fea-
tures chapters that touch upon the topics listed
above. The chapters can be roughly grouped into
four
categories:
(1)
brain
atlases
and
the
connectome, (2) imaging, (3) geometric recon-
struction, and (4) wiring principles and synthetic
models.
The
ﬁrst
group
includes
four
chapters:
(1) ▶“Brain Atlases,” a collection of pointers to
other chapters that describe in detail available brain
atlases; (2) ▶“Connectome, Drosophila,” a chap-
ter with a detailed survey of the known partial
connectomes of the Drosophila, with perspectives
on the utility, progress, and future of Drosophila
connectome
research;
(3)
▶“Connectome,
Mouse,” with a survey of publicly available
mouse connectome data; and (4) ▶“Connectome,
General,” a general survey of connectomics, its
potentials, and its challenges.
The second group includes imaging related
chapters: (1) ▶“Imaging, Electron Microscopy”
discusses the use of electron microscopy for com-
putational neuroanatomy, and (2) ▶“Imaging,
Specimen Preparation” presents methods for
brain specimen preparation for subsequent sec-
tioning and imaging. (There are other imaging
modalities that are relevant to computational neu-
roanatomy
such
as
▶“Physical
Sectioning
Microscopy” and others discussed in the Brain
Imaging section.)
The third group of chapters includes recon-
struction algorithms for the extraction of geomet-
rical information from the raw image volumes:
(1) ▶“Reconstruction, Electron Microscopy” dis-
cusses reconstruction techniques for densely
packed neurons in electron microscopy image
stacks; and (2) ▶“Reconstruction, Techniques
and Validation” talks about general reconstruction
methods and validation techniques.
Finally, the fourth group of chapters consists of
theoretical insights and computational simulation
methods that analyze and utilize the quantiﬁed
neuroanatomical data: (1) Networks/Networks
discusses
various
methods
for
automated
generation of realistic neurons and neuronal cir-
cuits for use in computer simulations (also see
▶“NeuroMorpho.org”); and (2) ▶“Wiring Prin-
ciples, Optimization” presents optimization prin-
ciples
that
could
potentially
be
underlying
neuronal morphology and connectivity patterns
found in the brain.
Other Related Chapters and Resources
There are several chapters beyond this section
(and sometimes whole sections) in this encyclo-
pedia that are directly relevant to computational
neuroanatomy:
(1)
Brain
Imaging
(▶“Connectivity Analysis in Normal and Patho-
logical Brains,” ▶“Multiscale Brain Connectiv-
ity”), (2) Brian Scale Networks (▶“Network
Theory in Neuroscience,” ▶“Neuropathologies
and Networks”), (3) Databases in Computational
Neuroscience (▶“NeuroMorpho.org” and many
other chapters in this section are relevant), and
(4) Model Reproducibility (▶“NeuroML”).
Also see the following edited books and mono-
graphs on computational neuroanatomy: (Ascoli
2002; Capowski 2012; Chung 2013).
Notable omissions at the moment include
(1) dendritic spine morphology and statistics
(Bourne and Harris 2008), (2) delay (axonal con-
duction delay and integration time) and delay sta-
tistics (Lamme and Roelfsema 2000; Nowak and
Bullier 1997), and (3) how to utilize gene expres-
sion data combined with structural information to
infer function (Toledo-Rodriguez et al. 2004).
Conclusion
Computational
neuroanatomy
provides
the
foundational framework for building, quantify-
ing, and visualizing comprehensive models of
large neurological tissues. These models form
the foundation for understanding neural compu-
tation by providing the ability to validate
hypotheses in silico as well as compare net-
works for the study of aging and neurodegener-
ative diseases. Computational neuroanatomy
also
forms
the
backbone
of
connectomics
(Sporns et al. 2005), enabling the eventual
understanding, replication, and repair of neuro-
logical tissues.
26
Computational Neuroanatomy: Overview

Cross-References
▶Brain Atlases
▶Connectivity Analysis in Normal and Patholog-
ical Brains
▶Connectome, Drosophila
▶Connectome, Mouse
▶GENESIS, the GENeral NEural SImulation
System
▶Imaging, Electron Microscopy
▶Imaging, Specimen Preparation
▶Multiscale Brain Connectivity
▶Network Theory in Neuroscience
▶NeuroML
▶NeuroMorpho.org
▶NEURON Simulation Environment
▶Neuropathologies and Networks
▶Physical Sectioning Microscopy
▶Reconstruction, Electron Microscopy
▶Reconstruction, Techniques and Validation
▶Wiring Principles, Optimization
References
Ascoli GA (2002) Computational neuroanatomy: princi-
ples and methods. Springer Science & Business Media
Bourne JN, Harris KM (2008) Balancing structure and
function at hippocampal dendritic spines. Annu Rev
Neurosci 31:47–67
Bower JM, Beeman D (2012) The book of GENESIS:
exploring realistic neural models with the GEneral
NEural SImulation system. Springer Science & Busi-
ness Media
Capowski JJ (2012) Computer techniques in neuroanat-
omy. Springer Science & Business Media
Carnevale NT, Hines ML (2006) The NEURON book.
Cambridge University Press
Chung MK (2013) Computational neuroanatomy: the
methods. World Scientiﬁc
Lamme VA, Roelfsema PR (2000) The distinct modes of
vision offered by feedforward and recurrent processing.
Trends Neurosci 23:571–579
Nowak LG, Bullier J (1997) The timing of information
transfer in the visual system. In: Extrastriate cortex in
primates. Springer, pp 205–241
Sporns O, Tononi G, Kötter R (2005) The human
connectome: a structural description of the human
brain. PLoS Comput Biol 1:e42
Toledo-Rodriguez M, Blumenfeld B, Wu C, Luo J,
Attali B, Goodman P, Markram H (2004) Correlation
maps allow neuronal electrical properties to be pre-
dicted from single-cell gene expression proﬁles in rat
neocortex. Cereb Cortex 14:1310–1327
Cortex: Overview
Lars Schwabe
Department of Computer Science and Electrical
Engineering, Adaptive and Regenerative
Software Systems, Universität Rostock, Rostock,
Germany
Detailed Description
Anatomically, the cerebral cortex is the outer-
most neuronal tissue of the brain, and it is
believed to play a key role in sensation, percep-
tion, higher cognitive functions, and motor con-
trol. It is a layered structure referred to as the gray
matter, because it contains largely cell bodies as
compared to the white matter containing largely
myelinated axons. The evolutionary origin can
be traced back to reptiles, but it ﬁrst appeared as a
uniform
structure
in
early
mammals.
The
increase in the size of this layered cortical sheet
during evolution is believed to be crucial for the
development of human cognition and ultimately
human culture during human brain evolution.
Even though many entries in this section on
“Cortex: Models and Computation” are applica-
ble to the [hippocampus] as well, the focus is on
the
phylogenetically
younger
six-layered
neocortex.
Why is it so interesting and important to inves-
tigate
the
cortex
using
modeling?
If
the
co-occurrence of the expansion of the neocortex
during evolution of the emergence of human cog-
nition and culture is more than a suspicious coin-
cident, then understanding the cortex is essential
to understand the human condition. Identifying
brain and mind is certainly a too naïve approach,
but it is now widely accepted that – whatever the
relation between mind and brain is – an under-
standing of cortex will at least constrain theories
of how the mind works. Testable theories and
predictive models are needed to complement con-
ceptual modeling and colloquial talk on that mat-
ter. Moreover, understanding how the cortex
operates may help to diagnose neurological dis-
eases earlier, to develop more efﬁcient treatments,
Cortex: Overview
27

and to construct [brain-machine interfaces].
Models in general, and patient-speciﬁc models
in particular, will be useful to link measurements
of macroscopic brain activity obtained with brain
imaging techniques to the underlying causes.
This rather practical justiﬁcation for modeling
cortex is becoming even more relevant today as
a “deliverable” of computational neuroscience.
However, accurate and faithful modeling has to
deal with the complexity of the neural circuits
and the cellular and synaptic heterogeneity. Mul-
tiple large-scale initiatives currently address this
by collecting massive amounts of data to facili-
tate the development of faithful models of the
cortex. Here, the role of cortical models is in
organizing and summarizing the data, and they
could even be integrated into (semi)automatic
data-driven modeling pipelines with little inter-
vention of a “modeler.” I argue that modeling
cortex is interesting and exciting, because despite
massive amounts of available data, the activity of
modeling will remain in large parts an art: Find-
ing the right level of abstraction to arrive at
insights for the question at hand can hardly be
automated.
On the one hand, the neocortex is an umbrella
for a set of distinct structures that differ, for exam-
ple, in terms of their function, connectivity, and
cytoarchitecture. On the other hand, Mountcastle
(1978) suggested that similarities of these neocor-
tical regions point to a common computational
machinery in the sense that each region has the
same basic architecture and operates according to
the same computational principles. This is also the
guiding idea of this section, without accepting it
as a truism or dogma. It is indeed conceivable that
different
regions
of
the
neocortex
operate
according to fundamentally different principles,
or that conventional notions of “computation”
are not suited as a metaphor to understand the
cortex. The philosophy of science is not conclu-
sive regarding a clear distinction of theory
vs. model. However, as a tentative distinction for
modeling cortex, we may adopt the short-hand
deﬁnition
that
theory
provides
meaning
to
models,
while
models
explain
data.
Computational principles come close to the notion
of a theory. With that reading, the entries assem-
bled in this section cover the whole spectrum
between theories and models of cortex.
Robert Legenstein (▶“Recurrent Network
Models, Reservoir Computing”) summarizes
the state of the art in how artiﬁcial recurrent
neural networks (RNNs) address demanding
learning tasks. RNN researchers can be thought
of as being in the luxurious position to investi-
gate computations in cortex-inspired architec-
tures without the burden to comply with all the
constraints set by experimental neuroscience.
The performance of these architectures and learn-
ing algorithms can serve as a yardstick for
existing biologically more faithful models and
as a guideline for constructing new models.
Jochen Triesch’s contribution on ▶“Cortical
Function, Normative Models of” gives a bird’s
eye perspective on how to derive models from
computational
principles.
Cortical
modeling
involves determining model structure and param-
eterization, and for data-driven approaches, com-
putational neuroscience has developed a [rich
repertoire
of
methods].
The
normative
approach endorsed by Triesch is a complemen-
tary addition for doing this, which applies in
particular to explaining cortical networks in
terms of their genesis via [learning mecha-
nisms]. The contributions of Sophie Deneve
(▶“Bayesian Inference with Spiking Neurons”)
and
Walter
Senn
and
Jean-Pascal
Pﬁster
(▶“Reinforcement Learning in Cortical Net-
works”)
are
instances
of
this
normative
approach. Deneve shows how Bayesian infer-
ence can be carried out by spiking neurons. The
Bayesian approach turned out to be very fruitful
for understanding computations in the cortex as
evident by a whole section in this encyclopedia
being dedicated to the [“Bayesian brain”]. It
should be noted that both Deneve’s and Senn’s
and Pﬁster’s entries explicitly address computa-
tion with spiking neurons and thus represent an
explicit formal link between computational prin-
ciples and experimentally testable predictions at
the level of individual neurons. Both of these
28
Cortex: Overview

mutually compatible approaches make explicit a
notion of optimality as required within the nor-
mative approach: The Bayesian approach is
based on a principled way of conducing logical
inference under uncertainty, whereas reinforce-
ment learning is based on Bellman optimality,
i.e., [decision making] in dynamic and often
only partially observable environments.
In a similar way, Udo Ernst (▶“Center-
Surround Processing, Computational Role of”)
addresses the phenomenon of center-surround
processing (CSP) from a computational point of
view. Even though CSP has been investigated
mainly in the visual system, where it is exempli-
ﬁed, e.g., by the phenomenon of end stopping
already described by Hubel and Wiesen (Hubel
and Wiesel 1965), it is also a candidate for a
canonical cortical computation to be found in
various cortical regions. Ernst links CSP to the
laws formulated by Gestalt psychologists in the
early twentieth century but also to modern nor-
mative approaches that utilize the statistics of
natural visual scenes in explaining physiological
and perceptual phenomena. He points out that
CSP has been successfully implemented in
[cortex-inspired
artiﬁcial
vision
systems],
where it improved object detection and recogni-
tion of natural scenes. Such real-world tests of
cortical models are excellent yardsticks modelers
may want to consider in addition to reproducing
physiological or perceptual phenomena that are
usually observed in rather artiﬁcial laboratory
settings with less naturalistic stimuli. Michael
Spratling (▶“Predictive Coding”) reviews the
concept of predictive coding. This is an instance
of a theory (in the sense deﬁned above), but
models derived from this theory can predict
CSP as a by-product. The distinctive feature of
predictive coding is that downstream areas in the
hierarchically organized cortex continuously
predict activity in areas at a lower level of the
hierarchy. Given that downstream areas in sen-
sory cortices integrate signals from neuronal
populations
with
adjacent
receptive
ﬁelds
(RFs), the predictions carry not only information
about anticipated future inputs but also from the
neighboring RFs as in CSP. Models derived from
predictive coding are candidates for a canonical
cortical computation. Applications to sensory
cortices may be relatively straightforward, but
the crucial test for a theory is its predictions
when extrapolated beyond the postdoc explana-
tions of already known phenomena. Let me point
out three selected such extrapolations: First, it
has been applied to explain mirror neuron activ-
ity as the natural consequence of predictive cod-
ing in the hierarchically organized “social brain”
(Kilner et al. 2007). Second, it has been applied
to interoception from which predictions about
bodily
self-consciousness
could
be
derived
(Seth et al. 2011). Third, it has been suggested
that it may also be applicable to the [motor
system] with the surprising consequence that
the actual motor acts are carried out to fulﬁll
predictions about sensory consequences of just
these acts (Hawkins and Blakeslee 2004) as com-
pared to being only the output stage of a sensory-
to-motor transformation. Future experimental
studies will need to further test these predictions.
Interestingly, Spratling has shown that predictive
coding and the concept of biased competition can
be thought of as being just variants of the same
mathematical model. Thus, while the interpreta-
tion of models derived from the predictive coding
theory in terms of “prediction of inputs” may be
unusual in some cases, as in the case of the motor
system, the theory is still a rich framework to
systematically derive mathematical models and
testable predictions.
Computation cannot be considered in isola-
tion. Communication engineers and designers of
processors know this too well. Matthias Bethge
(▶“Efﬁcient
Population
Coding”) considers
how much information is communicated by cor-
tical networks. Bethge introduces the psycho-
metric and neurometric functions and highlights
that the information contained in the spiking
activity of populations of neurons is often
enough to predict the behavioral responses of
the whole organisms. This is an empirical ﬁnd-
ing, but computational neuroscience also needs
to ask more fundamental questions such as “how
Cortex: Overview
29

much information can be transmitted?” Only this
allows for assessing how close to optimality the
cortical
circuits
are
actually
operating.
To
address such questions, he reviews how the con-
cepts of Fisher and Shannon’s mutual informa-
tion can be applied to quantify the information
content of population activity. Combining the
approaches that focus on computation intro-
duced so far with these studies of communica-
tion and information content could be a very
fruitful direction for future studies, in particular
when factoring in limitations due to ﬁber bottle-
necks
between
cortical
areas
and
energy
expenditure.
RNNs are Turing-complete, which means that
ﬁnite RNNs could, in principle, approximate any
computation. However, to determine the compu-
tations actually performed by the cerebral net-
works,
it
is
imperative
to
develop
mechanistically plausible models that explicitly
respect the anatomical and physiological con-
straints set by experimental neuroscience. Sean
Hill (▶“Cortical Columns, Models of”) presents
models of the so-called cortical column, which
itself is a theoretical concept motivated by early
experimental studies that showed smooth varia-
tion of functional properties tangential to the cor-
tical surface but an invariance across cortical
layers at a given position. The notion of a cortical
column remains controversial, but for computa-
tional neuroscience it is certainly a goal to deliver
predictive mechanistic models of signal propaga-
tion across cortical layers within an area. Probably
the most prominent example of mechanistic net-
work modeling to explain a physiologically
observed phenomenon is the models for orienta-
tion tuning in primary visual cortex (V1), which
are reviewed by Nicholas Priebe and Benjamin
Scholl (▶“Emergence of Orientation Selectivity
in the Cerebral Cortex, Modeling”). Hubel and
Wiesel discovered orientation tuning (Hubel and
Wiesel 1959) and formulated a ﬁrst model,
namely, that the tuning derives from the pattern
of afferent connections from the thalamus onto
neurons in V1. The subsequently developed
models emphasized the role of intracortical con-
nections to account for experimentally observed
properties of orientation tuning such as contrast
invariance. Interestingly, the original feedforward
model by Hubel and Wiesel is still a guiding idea,
even though it had to be reﬁned. This highlights
that such more informal and conceptual models
remain valuable today, even though computa-
tional neuroscience has to show explicitly when
and how models fail as reviewed by Priebe and
Scholl.
In my ﬁrst entrie (▶“Center-Surround Pro-
cessing, Network Models of”), I take a similar
approach and address the question of how the
CSP in V1, as introduced by Ernst, may be real-
ized
by
cortical
circuits.
More
speciﬁcally,
I review network models of CSP that are distinct
in terms of the assumed pathways. Early models
emphasized the role of long-range connections
within an area, but later models came to acknowl-
edge the role of feedback from downstream areas.
The cortical operating mode in vivo is character-
ized by strong recurrent excitation and balanced
inhibition that affect how single neurons integrate
and propagate signals (reviewed in my second
short contribution ▶“Balanced State”). Interest-
ingly, more recent modeling studies investigated
the role of short-range local connections in CSP
and
found
that
the
properties
of
strongly
connected recurrent networks in a balanced state
need to be considered in models of CSP. Adapta-
tion is another phenomenon that seems to be
omnipresent
in
the
cerebral
cortex.
Klaus
Wimmer (▶“Adaptation in Sensory Cortices,
Models of”) reviews models of adaptation and
considers both their role in perception and how
plausible mechanisms such as short-term synaptic
depression mediate them. Since strongly recurrent
networks in a balanced state with static synaptic
connections may already exhibit counterintuitive
phenomena,
Wimmer
argues
for
systematic
modeling studies of structured networks with
adaptation mechanisms as an important approach
to understand adaptation in sensory cortices.
Selected examples of higher cognitive func-
tions are attention and working memory. Cortical
models of these functions are reviewed by
Philipp Schwedhelm and Stefan Treue (▶“Atten-
tional Top-Down Modulation, Models of”) and
Gianluigi
Mongillo
(▶“Working
Memory,
Models of”). Schwedhelm and Treue review
30
Cortex: Overview

models of attentional top-down modulation. They
highlight how phenomenological models have
guided experiments and how those fed back into
reﬁning the models. Some of the models assume
the mechanism of gain modulation but remain
intentionally agnostic regarding the biophysical
mechanisms.
This
exempliﬁes
that
cortical
modeling with a properly chosen level of descrip-
tion could be integrated closely with experimental
investigations. Working memory has also been
studied experimentally in great detail, but most
early network models of the persistent activity that
is characteristic for the physiological correlate of
working memory were variants of attractor net-
works, where a self-sustained “bump” of activity
was identiﬁed with the content of working mem-
ory. Only more recent modeling studies suggested
that self-sustained activity may not be restricted to
the spiking activity of groups of neurons, because
the state of synapses with short-term dynamics
can also be considered as an activity variable
that could be exploited to store self-sustained
activity. The idea that synaptic variables, which
are by multiple orders of magnitude more numer-
ous than single cell state variables, may be crucial
for cerebral information processing has been
around in the computational neuroscience com-
munity for a long time. However, explicit formal
models need to spell this out and show the poten-
tial beneﬁt in, for example, systematic simulation
studies even if the models are speculative and
experimentally very hard to test as in the case of
the synaptic theory of working memory. This also
applies for models of attention: Given that after
50 years of the discovery of orientation tuning in
V1, there is still no agreement on network models
of even such a basal response property, it may not
come as a surprise that the mechanisms of atten-
tion remain elusive. While, for example, mecha-
nistic models of top-down gain modulation via
synchronizing the discharges of inhibitory inter-
neurons may be consistent with the available ana-
tomical,
physiological,
and
biophysical
knowledge, recording multiple identiﬁed inhibi-
tory interneurons in vivo in attentional demanding
tasks remains to be achieved.
Another currently only poorly understood phe-
nomenon is how the so-called resting state of the
brain is generated and maintained. Computa-
tional Neuroscience research has already identi-
ﬁed the problem of explaining mechanistically
the ongoing low-activity state in recurrently
connected local networks (Brunel et al. 2013)
and derived models to explain them as a stable
attractor.
Joana
Cabral
and
Gustavo
Deco
(▶“Spontaneous Activity, Models of”) review
models of the global spontaneous activity that
exhibits characteristic temporal properties and
is found in the so-called default mode network.
This activity (and the default mode network) has
been studied intensively using functional mag-
netic resonance imaging (fMRI), but Cabral and
Deco correctly point out that a deeper analysis of
the network models is still needed to provide
insights into the dynamical properties of the rest-
ing state.
The entries in this section cover computation
and modeling of the cortex using different
approaches and models at various levels of
abstraction. Certainly, the cortex cannot be con-
sidered in isolation but needs to be modeled and
understood in concert with other structures, such
as the [thalamus] and [basal ganglia]. Will it be
possible to understand cortex without modeling
the whole brain or even closed sensory-motor
loops within an “enactive” approach (Noe 2006)
that states that to understand the brain – in our
case, only the cortex – one needs to look at more
than just the brain? This is indeed an open ques-
tion that is of special interest for philosophers of
science and mind. However, I argue that the cortex
considered as a complex and self-assembled adap-
tive structure will remain a challenge for any kind
of modeling conducted by Computational Neuro-
scientists who are open to empirical ﬁndings and
brave enough to ignore irrelevant details without
throwing out the baby with the bath water. The
reward shall be motivating: to gain an insight into
how the cortex works. Relevance and irrelevance
of details needs to be decided on a case-by-case
basis, which also depends on the taste of the
modeler (or theoretician). Unfortunately, despite
a multitude of models and some promising candi-
dates for theories of cortical function, one needs to
attest that we are not yet there: A uniﬁed theory of
cortical computation with associated models still
Cortex: Overview
31

needs to be derived and thoroughly tested. My
own requirement for accepting such a theory is
that it will cover at least the topics addressed by
the entries in this section.
Cross-References
▶Adaptation in Sensory Cortices, Models of
▶Attentional Top-Down Modulation, Models of
▶Balanced State
▶Bayesian Inference with Spiking Neurons
▶Center-Surround Processing, Computational
Role of
▶Center-Surround Processing, Network Models of
▶Cortical Columns, Models of
▶Cortical Function, Normative Models of
▶Efﬁcient Population Coding
▶Emergence of Orientation Selectivity in the
Cerebral Cortex, Modeling
▶Predictive Coding
▶Recurrent Network Models, Reservoir
Computing
▶Reinforcement Learning in Cortical Networks
▶Spontaneous Activity, Models of
▶Working Memory, Models of
References
Brunel N, del Giudice P, Fusi S, Parisi G, Tsodyks M (eds)
(2013) Selected papers of Daniel Amit. World Scien-
tiﬁc, Hackensack
Hawkins J, Blakeslee S (2004) On intelligence. Times
Books, New York
Hubel DH, Wiesel TN (1959) Receptive ﬁelds of single
neurones in the cat’s striate cortex. J Physiol 148:
574–591
Hubel DH, Wiesel TN (1965) Receptive ﬁelds and func-
tional architecture in two nonstriate visual areas (18 and
19) of the cat. J Neurophysiol 28(2):230–289
Kilner JM, Friston KJ, Frith CD (2007) The mirror-neuron
system: a Bayesian perspective. Neuroreport 18(6):
619–623
Mountcastle VB (1978) An organizing principle for cere-
bral function: the unit model and the distributed system.
In: Edelman GM, Mountcastle VB (eds) The mindful
brain. MIT Press, Cambridge, MA
Noe A (2006) Action in perception. Bradford Book
Seth AK, Suzuki K, Critchley HD (2011) An interoceptive
predictive coding model of conscious presence. Front
Psychol 2:395
Databases and Data
Repositories in
Computational Neuroscience:
Overview
Richard C. Gerkin1, Shreejoy J. Tripathy2,
Sharon M. Crook3 and Jeanette Kotaleski4
1School of Life Sciences, Arizona State
University, Tempe, AZ, USA
2Department of Psychiatry, University of Toronto,
Toronto, ON, Canada
3School of Mathematical and Statistical Sciences
and School of Life Sciences, Arizona State
University, Tempe, AZ, USA
4School of Computer Science and
Communication, Royal Institute of Technology,
Stockholm, Sweden
Definition
Computational neuroscience research often pro-
duces models that help explain empirical data and
provide predictions about the biological systems
that produce the data. Empirical and theoretical
researchers can beneﬁt from resources that facili-
tate model publication and exchange and provide
neuroscience data that informs and contains these
existing and future models.
Detailed Description
Databases of Computational Models
One category of databases in computational neuro-
science is those that focus on computational models.
The
BioModels
Database
(Li
et
al.
2010;
Vijayalakshmi et al. 2013) is a general repository
of computational models of biological processes
that includes some models from neuroscience. The
SenseLab database, ▶“ModelDB” (Migliore et al.
2003), provides a resource for published neurosci-
ence models in a variety of formats. Many of these
were developed speciﬁcally for simulating the elec-
trophysiological and neurochemical properties of
single neurons and networks of neurons, for exam-
ple, with multicompartment Hodgkin-Huxley type
32
Databases and Data Repositories in Computational Neuroscience: Overview

conductance-based models (Hodgkin and Huxley
1952). The Physiome Model Repository (see
▶“Physiome Repository”) (Yu et al. 2011) is a
software suite that facilitates storage and manage-
ment of models, focusing on those described in
▶“CellML,” a standard markup language for bio-
logical models. Similarly, ▶“Open Source Brain”
(Gleeson et al. 2010b) is a community platform for
collaborative development of computational neuron
and network models that utilizes open standards
such as ▶“NeuroML” (Gleeson et al. 2010a) to
facilitate interoperability and visualization of neuro-
science models developed by different researchers.
Thousands of models at multiple scales (ion chan-
nels, neurons, circuits) have been converted to the
NeuroML standard, and these can be obtained at
NeuroML-DB (Birgiolas et al. 2015), which also
reports the results of many standard physiological,
morphological, and computational measurements
and experiments on these models to better under-
stand how they behave.
Databases of Neuron Morphology and
Physiology
A second category of databases that are important
for the computational neuroscience community
includes those that contain structured information
speciﬁc to neurons and their properties. For exam-
ple, information on the detailed shapes of neurons
(morphology) is compiled by NeuroMorpho (see
▶“Neuromorpho.org”) (Ascoli et al. 2007) which
contains user-submitted neuron morphological
reconstructions made, for example, using the
Neurolucida
application
(Glaser
and
Glaser
1990). Similarly, other SenseLab databases (see
▶“SenseLab: Integration of Multidisciplinary
Neuroscience Data”) including NeuronDB and
CellPropDB (Crasto et al. 2007), contain informa-
tion on the ionic currents and neurotransmitters
expressed by each neuron and how these are dis-
tributed with respect to neuronal morphology.
Measured electrophysiological properties of
neuron types, and the metadata associated with
these
measurements,
are
cataloged
in
the
▶“NeuroElectro Project” (Tripathy et al. 2014).
Detailed information on ion channel subtypes,
including voltage and temporal dynamics, genetic
homology,
and
corresponding
literature
references, is available at Channelpedia (Ranjan
et al. 2011, http://channelpedia.net), a subproject
within the Blue Brain Project (Markram 2006).
Information about neurons in speciﬁc brain
areas, including many representative examples,
are also publicly available. For example, the Neo-
cortical Microcircuit Collaboration Portal con-
tains information about primary somatosensory
cortex (citation), The Allen Institute for Brain
Science Allen Brain Atlas includes a Cell Types
Database
that covers primary visual cortex
(Sunkin
2013),
and
▶“Hippocampome.org”
covers hippocampus (Wheeler et al. 2015).
Relevant to these databases that provide data
that are helpful for constraining computational
models,
the
performance
of
models
on
experimental-data-driven unit tests is reported at
SciDash.org, enabling direct model comparison
on properties or predictions of interest.
Resources for Brain Connectivity
Another category of databases focuses on the ana-
tomical organization of the brain. Here we provide
some widely used examples, focusing on those
that could provide quantitative constraints for
modeling efforts. ▶“BrainInfo” provides general
information about brain areas, including what they
do, where they are located, and what they contain.
The Allen Institute for Brain Science provides
brain-wide gene expression atlases, where the
expression of each of the genes in the mammalian
genome
has
been
systematically
quantiﬁed
throughout the brain for a number of animal spe-
cies and across stages of neural development, as
well as anatomical connectivity of different brain
regions (Lein et al. 2007, http://brain-map.org).
Parallel
to
this
effort
is
the
▶“Human
Connectome Project” (Marcus et al. 2013), a large-
scale effort to map complete structural and func-
tional neural connections in vivo in individual
humans. ▶“BrainMap” (Laird et al. 2004) consists
of a database and related software to search
published functional and structural human neuroim-
aging experiments. In contrast, CoCoMac (Stephan
et al. 2001; Bakker et al. 2012) is focused on the
primate brain, containing records of tracing studies
in the macaque. Finally, the ▶“Cell Centered Data-
base” (Martone et al. 2003, 2009) focuses on images
Databases and Data Repositories in Computational Neuroscience: Overview
33

from light and electron microscopy, ranging from
whole brain areas to subcellular compartments.
Other Resources
In addition to these neuroscience domain-speciﬁc
databases are federated databases that provide
linking facilities for cross-resource data integra-
tion. For example, NeuroLex (Larson and Martone
2013) provides a platform for community annota-
tion of neuron types on the basis of morphological,
neurochemical, or electrophysiological properties.
Given this wealth of neuroscience resources, the
▶“Neuroscience Information Framework (NIF)”
provides tools for semantic search across these
diverse databases (Gardner et al. 2008) through
the development and incorporation of neuroscience
domain-speciﬁc ontologies (Bug et al. 2008;
Larson and Martone 2009; Hamilton et al. 2012;
Imam et al. 2012). For example, in NIF, the search
query “mitral cell” returns a number of database
records including relevant research literature from
PubMed, computational models from ModelDB,
and connectivity information from BAMS.
The number and size of these databases continue
to grow with the collection and contribution of ever
more models and data. These databases give com-
putational neuroscientists a powerful tool for
constraining data-driven models and serve as an
alternative to conventional literature searches.
Cross-References
▶BrainInfo
▶BrainMap
▶Cell Centered Database
▶CellML
▶Hippocampome.org
▶Human Connectome Project
▶ModelDB
▶NeuroElectro
▶NeuroML
▶NeuroMorpho.org
▶Neuroscience Information Framework (NIF)
▶Open Source Brain
▶Physiome Repository
▶SenseLab:
Integration
of
Multidisciplinary
Neuroscience Data
References
Ascoli GA, Donohue DE, Halavi M (2007) Neuromorpho.
org: a central resource for neuronal morphologies.
J Neurosci 27(35):9247–9251
Bakker R, Wachtler T, Diesmann M (2012) Cocomac 2.0
and
the
future
of
tract-tracing
databases.
Front
Neuroinform 6:30
Birgiolas J, Dietrich S, Crook S, Rajadesingan A, Zhang C,
Velugoti Penchala S, Addepalli V (2015) Ontology-
assisted keyword search for NeuroML models. In:
Gupta A, Rathbun S (eds) Proceedings of the 27th
international conference on scientiﬁc and statistical
database management, ACM, New York, Article 37.
https://doi.org/10.1145/2791347.2791360
Bug WJ, Ascoli GA, Grethe JS, Gupta A, Fennema-
Notestine C, Laird AR, Larson SD, Rubin D, Shepherd
GM, Turner JA, Martone ME (2008) The NIFSTD
and BIRNLex vocabularies: building comprehensive
ontologies for neuroscience. Neuroinformatics 6(3):
175–194
Crasto CJ, Marenco LN, Liu N, Morse TM, Cheung K-H, Lai
PC, Bahl G, Masiar P, Lam HYK, Lim E, Chen H,
Nadkarni P, Migliore M, Miller PL, Shepherd GM
(2007) SenseLab: new developments in disseminating
neuroscience information. Brief Bioinform 8(3):150–162
Gardner D et al (2008) The neuroscience information
framework: a data and knowledge environment for
neuroscience. Neuroinformatics 6(3):149–165
Glaser JR, Glaser EM (1990) Neuron imaging with
neurolucida – a PC-based system for image combining
microscopy. Comput Med Imaging Graph 14(5):307–317
Gleeson P, Crook S, Cannon RC, Hines ML, Billings GO,
Farinella M, Morse TM, Davison AP, Ray S, Bhalla
US, Barnes SR, Dimitrova YD, Silver RA (2010a)
NeuroML: a language for describing data driven
models of neurons and networks with a high degree of
biological detail. PLoS Comput Biol 6(6):e1000815
Gleeson P, Piasini E, Crook S, Cannon R, Steuber V,
Jaeger D, Solinas S, D’Angelo E, Silver RA (2010b)
The open source brain initiative: enabling collaborative
modelling
in
computational
neuroscience.
BMC
Neurosci 13(1):1–2
Hamilton DJ, Shepherd GM, Martone ME, Ascoli GA
(2012) An ontological approach to describing neurons
and their relationships. Front Neuroinform 6:15
Hodgkin AL, Huxley AF (1952) A quantitative description
of membrane current and its application to conduction
and excitation in nerve. J Physiol 117(4):500–544
Imam FT, Larson SD, Bandrowski A, Grethe JS, Gupta A,
Martone ME (2012) Development and use of ontol-
ogies inside the neuroscience information framework:
a practical approach. Front Genet 3:111
Laird AR, Lancaster JL, Fox PT (2004) BrainMap: the
social evolution of a functional neuroimaging database.
Neuroinformatics 3:65–78
Larson SD, Martone ME (2009) Ontologies for neurosci-
ence: what are they and what are they good for? Front
Neurosci 3:1
34
Databases and Data Repositories in Computational Neuroscience: Overview

Larson SD, Martone ME (2013) NeuroLex.org: an online
framework
for
neuroscience
knowledge.
Front
Neuroinform 7:18
Lein ES et al (2007) Genome-wide atlas of gene expression
in the adult mouse brain. Nature 445(7124):168–176
Li C, Donizelli M, Rodriguez N, Dharuri H, Endler L,
Chelliah V, Li L, He E, Henry A, Stefan MI, Snoep
JL, Hucka M, Le Novere N, Laibe C (2010) BioModels
database: an enhanced, curated and annotated resource
for published quantitative kinetic models. BMC Syst
Biol 4:92
Marcus DS, Harms MP, Snyder AZ, Jenkinson M, Wilson
JA, Glasser MF, Barch DM, Archie KA, Burgess GC,
Ramaratnam M, Hodge M, Horton W, Herrick R,
Olsen T, McKay M, House M, Hileman M, Reid E,
Harwell J, Coalson T, Schindler T, Elam JS, Curtiss
SW, WU-Minn HCP Consortium, Van Essen DC
(2013) Human connectome project informatics: quality
control, database services, and data visualization.
NeuroImage 80:202–219
Markram H (2006) The blue brain project. Nat Rev
Neurosci 7(2):153–160
Martone ME, Zhang S, Gupta A, Qian X, He H, Price DL,
Wong M, Santini S, Ellisman MH (2003) The cell-
centered database: a database for multiscale structural
and protein localization data from light and electron
microscopy. Neuroinformatics 1(4):375–379
Martone ME, Tran J, Wong WW, Sargis J, Fong L, Larson
SD, Lamont SP, Gupta A, Ellisman MH (2009) Cell
centered database project: an update on building com-
munity resources for managing and sharing 3D imag-
ing data. J Struct Biol 161(3):220–231
Migliore M, Morse TM, Davison AP, Marenco L, Shep-
herd GM, Hines M (2003) ModelDB: making models
publicly accessible to support computational neurosci-
ence. Neuroinformatics 1(1):135–139
Ranjan R, Khazen G, Gambazzi L, Ramaswamy S, Hill SL,
Schrmann F, Markram H (2011) Channelpedia: an inte-
grative and interactive database for ion channels. Front
Neuroinform 5:36
Stephan KE, Kamper L, Bozkurt A, Burns GA, Young MP,
Ktter R (2001) Advanced database methodology for the
collation of connectivity data on the macaque brain
(CoCoMac). Philos Trans R Soc Lond Ser B Biol Sci
356(1412):1159–1186
Tripathy SJ, Savitskaya J, Burton SD, Urban NN, Gerkin
RC (2014) NeuroElectro: a window to the world’s
neuron electrophysiology data. Front Neuroinform
8:40
Vijayalakshmi C, Laibe C, Le Novere N (2013) BioModels
database: a repository of mathematical models of bio-
logical processes. Methods Mol Biol 1021:189–199
Wheeler DW, White CM, Rees CL, Komendantov AO,
Hamilton DJ, Ascoli GA (2015) Hippocampome.org:
a knowledge base of neuron types in the rodent hippo-
campus. elife 4:e09960
Yu T, Lloyd CM, Nickerson DP, Cooling MT, Miller AK,
Garny A, Terkildsen JR, Lawson J, Britten RD, Hunter
PJ, Nielsen PM (2011) The physiome model repository
2. Bioinformatics 27(5):743–744
Decision-Making: Overview
Emilio Salinas
Department of Neurobiology and Anatomy,
Wake Forest School of Medicine, Winston-Salem,
NC, USA
Detailed Description
Much of computational neuroscience begins and
ends with the responses of individual neurons.
The ﬁeld itself sprang from work in the 1950s
aimed at uncovering the biophysical mechanisms
underlying
spike
generation
(Hodgkin
and
Huxley 1952), and other classic studies focus on
the computational capabilities of dendritic trees
and on how neural activity encodes sensory stim-
uli or statistical information about the world, to
mention
a couple of well-known examples
(Dayan and Abbott 2001). A different point of
view, however, is one in which neurons are the
intermediaries between a subject and its environ-
ment. As the engines of behavior, neurons need to
be computationally powerful for the express pur-
pose of giving the subject an advantage, and hence
their efﬁciency or performance should be mea-
sured with respect to the subject’s success. So
the computational neuroscience of decision-
making is computational neuroscience in this con-
text; it is the quest to understand how single-
neuron
activity
contributes
to
nonreﬂexive
actions, actions that cannot be predicted from a
subject’s history alone.
The idea of quantitatively explaining a per-
son’s
subjective
experiences
based
on
the
responses of speciﬁc groups of neurons goes
back to the 1960s, with the work of Vernon
Mountcastle and colleagues in the somatosensory
modality. They found, for instance, that when a
small probe, similar to the tip of a ballpoint pen,
vibrates at a frequency of 2–40 Hz, the intensity of
the evoked sensation reported by a person is
directly related to the neural responses of a partic-
ular type of mechanosensory receptor under the
skin, now known as a Meissner corpuscle (Talbot
et al. 1968). Many design elements and analytical
Decision-Making: Overview
35

techniques used in contemporary experiments
were already laid out in those pioneering studies:
they applied information theory to determine the
coding capacity of sensory neurons (Werner and
Mountcastle 1965) very much as is done today,
except for the rudimentary computers, and they
recognized neuronal variability, as well as rate
versus temporal coding, as fundamental issues
for neural computation (Werner and Mountcastle
1963). This early work was limited, however,
because it compared psychophysical performance
in one system (behaving humans) with neural
activity in a different system (e.g., anesthetized
monkeys). It was later, with studies like those of
William Newsome and colleagues (Salzman et al.
1992), that vision became the more popular
modality and that both behavioral and neuronal
responses were simultaneously recorded in the
same subjects during the performance of percep-
tually based tasks. That became the standard and a
cornerstone for investigating the neural basis of
decision-making (Parker and Newsome 1998).
Thus, decision-making involves the study of
neurons and neural circuits as a subject captures
information about the sensory world; analyzes it;
combines it with other stored information about
current goals, priorities, and possible courses of
action; and makes a response. In the laboratory,
researchers attempt to simplify this as much as
possible while still maintaining the essence of
the process – an internal evaluation that is not
fully predictable – and while measuring three
quantities as accurately as possible: the sensory
stimuli, the relevant neuronal activity, and the
subject’s behavior (Parker and Newsome 1998).
So even the simplest possible decision-making
task requires various types of neural computa-
tions, and indeed, one common strategy in the
ﬁeld has been to try to break down the problem
into temporally discrete steps (e.g., ﬁxation, stim-
ulus presentation, response selection, reward
delivery), to investigate how neurons in various
areas participate in speciﬁc aspects of the
decision-making process (Romo and Salinas
2003). The articles that comprise this section of
the Springer Encyclopedia of Computational
Neuroscience review key components of any
such decision-making process.
Three of the entries (▶“Decision-Making
Tasks”; ▶“Choice Behavior”; ▶“Categorical
Decisions”) emphasize the repertoire of tasks
and associated mathematical models that have
been successfully used to characterize and quan-
tify behavior. This provides a foundation for
understanding how primary factors, such as
reward availability or the quality of sensory infor-
mation, drive a subject’s actions and how different
tasks place demands on different cognitive func-
tions, such as perceptual discrimination, conﬂict
resolution, or working memory capacity, to name
a few. Selection of a particular behavioral task
thus determines both the speciﬁc cognitive func-
tion and the corresponding neural circuits under
investigation. In this context, it has become clear
that individual choices are often inﬂuenced by a
variety of subtle factors that superﬁcially may
appear
inconsequential
(▶“Decision-Making,
Bias”). Such sources of bias are likely to attract
increasingly more attention in the future, as our
ability to correlate neuronal activity and behavior
becomes more sophisticated.
Most decision-making tasks have at least two
components: a perceptual step during which cur-
rent sensory information is analyzed (e.g., is that
spot red or green?) and a motor report whereby the
result of the perceptual judgment is indicated
(e.g., push a left or a right button). The rest of
the articles in the section focus either on percep-
tion (▶“Accumulation of Evidence in Decision-
Making”), motor planning (▶“Decision-Making,
Motor Planning”), or their interface and interac-
tion. Although some neurons clearly relate to
either perceptual or motor processing, it is often
the case that the cells that correlate most strongly
with a subject’s choices have elements of both
(▶“Perceptual Decision-Making”). In fact, deter-
mining the degree to which a neural response
encodes perceptual information versus a motor
plan turns out to be surprisingly difﬁcult (▶“Tar-
get Selection vs. Response Selection”); the ambi-
guity arises even at the psychophysical level
(▶“Perceptual-Motor Dissociation”). In spite of
this, a number of principles describing how neu-
rons participate in the generation of perceptual
judgments and ultimately produce choices have
been
identiﬁed
(▶“Perceptual
Decision-
36
Decision-Making: Overview

Making”). Furthermore, relatively simple quanti-
tative models have been highly successful at
reproducing not only the traditional behavioral
metrics of performance and reaction time but
also key aspects of neuronal activity (▶“Accu-
mulation of Evidence in Decision-Making”;
▶“Decision-Making, Models”). Because of this,
there is a relatively thorough understanding of at
least some of the computations that are key to
perceptual decision-making (▶“Accumulation
of Evidence in Decision-Making”; ▶“Speed-
Accuracy
Tradeoff”;
▶“Decision-Making,
Threshold”). This trend is expected to continue.
Cross-References
▶Accumulation of Evidence in Decision-Making
▶Categorical Decisions
▶Choice Behavior
▶Decision-Making Tasks
▶Decision-Making, Bias
▶Decision-Making, Models
▶Decision-Making, Motor Planning
▶Decision-Making, Threshold
▶Perceptual Decision-Making
▶Perceptual-Motor Dissociation
▶Speed-Accuracy Tradeoff
▶Target Selection vs. Response Selection
References
Dayan P, Abbott LF (2001) Theoretical neuroscience. MIT
Press, Cambridge, MA
Hodgkin AL, Huxley AF (1952) A quantitative description
of membrane current and its application to conduction
and excitation in nerve. J Physiol 117:500–544
Parker AJ, Newsome WT (1998) Sense and the single
neuron: probing the physiology of perception. Annu
Rev Neurosci 21:227–277
Romo R, Salinas E (2003) Flutter discrimination: neural
codes, perception, memory and decision making. Nat
Rev Neurosci 4:203–218
Salzman CD, Murasugi CM, Britten KH, Newsome WT
(1992) Microstimulation in visual area MT: effects on
direction discrimination performance. J Neurosci 12:
2331–2355
Talbot WH, Darian-Smith I, Kornhuber HH, Mountcastle
VB (1968) The sense of ﬂutter-vibration: comparison
of the human capacity with response patterns of
mechanoreceptive afferents from the monkey hand.
J Neurophysiol 31:301–334
Werner G, Mountcastle V (1963) The variability of central
neural activity in a sensory system, and its implications
for
the
central
reﬂection
of
sensory
events.
J Neurophysiol 26:958–977
Werner G, Mountcastle VB (1965) Neural activity in mech-
anoreceptive cutaneous afferents: stimulus–response
relations, Weber functions, and information transmis-
sion. J Neurophysiol 28:359–397
Deep Brain Stimulation
(Models, Theory, Techniques):
Overview
Peter Alexander Tass1,2 and
Christian Hauptmann3
1Department of Neurosurgery, Stanford
University, Stanford, CA, USA
2Department of Neuromodulation, University of
Cologne, Cologne, Germany
3Institute of Neuroscience and Medicine –
Neuromodulation (INM–7), Research Center
Jülich, Jülich, Germany
Detailed Description
Deep brain stimulation (DBS) of the subthalamic
nucleus (STN) is a well-established treatment for
medically
refractory
patients
with
advanced
Parkinson’s disease (PD) (Benabid et al. 1991;
Blond et al. 1992; Benabid et al. 2002; Deuschl
et al. 2006) as well as for patients with early motor
complications (Deuschl et al. 2006; Schuepbach
et al. 2013). Several neurological diseases, such as
Parkinson’s disease (PD) or essential tremor, are
characterized by pathological synchronization
(Nini et al. 1995; Brown et al. 2001). Parkinsonian
resting tremor, for example, seems to origin from
a pacemaker-like population of neurons of the
basal ganglia ﬁring in a synchronized and oscilla-
tory manner (Hutchison et al. 1997; Hurtado et al.
1999; Magill et al. 2001; Trottenberg et al. 2007).
In contrast, under healthy conditions these neu-
rons
are
active
in
an
uncorrelated
and
desynchronized manner (Nini et al. 1995; Magill
et al. 2001).
Deep Brain Stimulation (Models, Theory, Techniques): Overview
37

The standard DBS protocol employs perma-
nent high-frequency (>100 Hz) pulse train stim-
ulation (Benabid et al. 1991, 2002; Blond et al.
1992). Symptom suppression by DBS is strongly
dependent on stimulation frequency – with only
high frequencies (>100 Hz) being effective and
effects being rapidly reversible (Birdno and Grill
2008). High-frequency
DBS
was
developed
empirically, mainly based on clinical observations
and experimental results (Volkmann et al. 2006),
and the mechanism of high-frequency DBS is still
a matter of debate (Benabid et al. 2005).
Experimental observations indicate that during
high-frequency DBS, a regular bursting mode is
induced (Beurrier et al. 2002), and after a reduc-
tion of stimulation artifacts, robust bursting activ-
ity in STN neurons was observed in slice
experiments (Beurrier et al. 2001). In the same
experiments,
the
offset
of
stimulation
was
followed by a blockade of activity, i.e., a depolar-
ization blockade (Beurrier et al. 2001). These
observations were made in anesthetized animals
and are contradicted by measurements in awake
behaving
primates
(Anderson
et
al.
2003;
Hashimoto et al. 2003; Dorval et al. 2008) and
rats (McConnell et al. 2012). Other groups argue
that high-frequency DBS blocks neuronal activity
in relevant target areas during stimulation and
therefore mimics the effect of tissue lesioning
(Benabid et al. 2002). In 2005, Benabid and
coworkers summarized different hypothetical
mechanisms: membrane inhibition, excitation of
excitatory and inhibitory afferents, jamming,
excitation of efferents, and plasticity (Benabid
et al. 2005). Novel experimental techniques,
such as optogenetics, enabled to further reveal
the mechanism of DBS and, in particular, of the
stimulation of afferent axons projecting to the
target region (Gradinaru et al. 2009).
Spatially
extended
single-
and
multi-
compartment neuron models were used to evalu-
ate the contribution of these different mechanisms
(Grill and McIntyre 2001; Terman et al. 2002;
Rubin and Terman 2004). For example, Grill and
McIntyre (2001) showed that depending on the
stimulation amplitude and the shape of the stimu-
lation pulses, cells were either activated directly or
ﬁbers mediating excitatory or strong inhibitory
action were activated. The activation of a larger
number of structures takes place on the single-
neuron
level
with
different
and
possibly
conﬂicting impacts on single-neuron dynamics
(Grill and McIntyre 2001). For example, in the
same neuron, the cell body (soma) is inhibited as a
result of activation of presynaptic axons and
GABA release, while the efferent axon is acti-
vated by the stimulation pulses on an approxi-
mately one for one basis (McIntyre et al. 2004).
The various reactions of neurons toward stimula-
tion on the network level further add complexity
that is important for the creation of a sound model:
cells responding differently to external inputs,
such as somatosensory stimulation or stimulation
owing to active movements, are present in the
target tissue together with so-called no-response
cells
(Lenz
et
al.
1994).
Therefore,
high-
frequency stimulation has a complex impact on
these structures (Benabid et al. 2002; Shen et al.
2003). However, surprisingly, even single STN
model neurons – lacking synaptic dynamics, neu-
ral circuitry, and contributions of glial cells – sub-
jected to high-frequency stimulation reproduce
clinically
observed
response
characteristics
(Pyragas et al. 2013).
To study another aspect of DBS, several
groups use physical models based on Maxwell’s
equations to investigate the neuronal activation
proﬁle depending on electrode geometry and
stimulation parameters (Butson and McIntyre
2005, 2006; Miocinovic et al. 2009; Yousif et al.
2008;
Chaturvedi
et
al.
2010;
Buhlmann
et al. 2011).
While the standard DBS protocol was devel-
oped empirically (Volkmann et al. 2006), novel
stimulation approaches are based on electrophys-
iological as well as computational concepts. Per-
sonalizing
and
optimizing
high-frequency
stimulation in real time by demand-controlled,
adaptive DBS might constitute a superior high-
frequency stimulation mode, as shown in an acute
study in externalized patients (Little et al. 2013).
In addition, modeling studies are used to further
develop the stimulation algorithm, beyond stan-
dard high-frequency DBS, in order to ﬁnally
establish superior stimulation mechanisms. For
example, coordinated reset (CR), a patterned
38
Deep Brain Stimulation (Models, Theory, Techniques): Overview

stimulation protocol speciﬁcally targeting the
reduction of synchronized activity, was developed
by means of mathematical models (Tass 2003)
and essentially aims at an unlearning of both
abnormal synaptic connectivity and synchrony
(Tass and Majtanik 2006). CR was successfully
tested in a preclinical study, where 5 days of low-
dose CR stimulation induced long-lasting thera-
peutic effects for 30 days (Tass et al. 2012).
Another example is a closed-loop approach,
which was controlled by the extent of oscillatory
beta-band activity. During stimulus delivery, this
approach resulted in a better reduction of akinesia
as well as pallidal ﬁring rates as compared to
classical DBS in parkinsonian nonhuman MPTP-
treated primates (Rosin et al. 2011). Finally, mod-
iﬁcations of the standard HF protocol might offer
a novel approach to improve the efﬁcacy of deep
brain stimulation (Brocker et al. 2013; Hess
et al. 2013).
The combination of all these modeling and
experimental and technological approaches plays
a vital role in shaping our understanding and helps
to
improve
the
promising
therapeutic
intervention DBS.
Cross-References
▶Computational Model-Based Development of
Novel Stimulation Algorithms
▶Computational Models of Deep Brain
Stimulation (DBS)
▶Computational Models Supporting Parameter
Finding for Deep Brain Stimulation
▶Computational Models to Optimize the
Electrodes and Waveforms for Deep Brain
Stimulation
References
Anderson ME, Postupna N, Ruffo M (2003) Effects of
high-frequency stimulation in the internal globus
pallidus on the activity of thalamic neurons in the
awake monkey. J Neurophysiol 89:1150–1160
Benabid AL, Pollak P, Gervason C, Hoffmann D, Gao DM,
Hommel M, Perret JE, de Rougemount J (1991) Long-
term suppression of tremor by chronic stimulation of
ventral intermediate thalamic nucleus. Lancet 337:
403–406
Benabid AL, Benazzous A, Pollak P (2002) Mechanisms
of deep brain stimulation. Mov Disord 17:73–74
Benabid
AL,
Wallace
B,
Mitrofanis
J
et
al
(2005) A putative generalized model of the effects
and mechanism of action of high frequency electrical
stimulation of the central nervous system. Acta Neurol
Belg 105:149–157
Beurrier C, Bioulac B, Audin J, Hammond C (2001) High-
frequency stimulation produces a transient blockade of
voltage-gated
currents
in
subthalamic
neurons.
J Neurophysiol 85(4):1351–1356
Beurrier C, Garcia L, Bioulac B, Hammond C (2002)
Subthalamic nucleus: a clock inside basal ganglia?
Thalamus Relat Syst 2:1–8
Birdno MJ, Grill WM (2008) Mechanisms of deep brain
stimulation in movement disorders as revealedbychanges
in stimulus frequency. Neurotherapeutics 5(1):14–25
Blond S, Caparros-Lefebvre D, Parker F et al (1992) Con-
trol of tremor and involuntary movement disorders by
chronic stereotactic stimulation of the ventral interme-
diate thalamic nucleus. J Neurosurg 77:62–68
Brocker DT, Swan BD, Turner DA, Gross RE, Tatter SB,
Miller
KM,
Bronte-Stewart
H,
Grill
WM
(2013) Improved efﬁcacy of temporally non-regular
deep brain stimulation in Parkinson’s disease. Exp
Neurol 239:60–67
Brown P, Oliviero A, Mazzone P, Insola A, Tonali P, Di
Lazzaro V (2001) Dopamine dependency of oscilla-
tions between subthalamic nucleus and pallidum in
Parkinson’s disease. J Neurosci 21:1033–1038
Buhlmann J, Hofmann L, Tass PA, Hauptmann C (2011)
Modeling
of
a
segmented
electrode
for
desynchronizing
deep
brain
stimulation.
Front
Neuroeng 4:1–8
Butson CR, McIntyre CC (2005) Tissue and electrode capac-
itance reduce neural activationvolumes duringdeep brain
stimulation. Clin Neurophysiol 116:2490–2500
Butson CR, McIntyre CC (2006) Role of electrode design
on the volume of tissue activated during deep brain
stimulation. J Neural Eng 3:1–8
Chaturvedi A, Butson CR, Lempka SF, Cooper SE,
McIntyre CC (2010) Patient-speciﬁc models of deep
brain stimulation: inﬂuence of ﬁeld model complexity
on neural activation predictions. Brain Stimul 3:65–67
Deuschl
G,
Schade-Brittinger
C,
Krack
P
et
al
(2006) A randomized trial of deep-brain stimulation
for Parkinson’s disease. New Engl J Med 355:896–908
Dorval AD, Russo GS, Hashimoto T, Xu W, Grill WM,
Vitek JL (2008) Deep brain stimulation reduces neuro-
nal entropy in the MPTP-primate model of Parkinson’s
disease. J Neurophysiol 100:2807–2818
Gradinaru V, Mogri M, Thompson KR, Henderson JM,
Deisseroth K (2009) Optical deconstruction of parkin-
sonian neural circuitry. Science 324:354–359
Grill WM, McIntyre CC (2001) Extracellular excitation of
central neurons: implications for the mechanisms of
deep brain stimulation. Thalamus Relat Syst 1:269–277
Deep Brain Stimulation (Models, Theory, Techniques): Overview
39

Hashimoto T, Elder CM, Okun MS, Patrick SK, Vitek JL
(2003) Stimulation of the subthalamic nucleus changes
the ﬁring pattern of pallidal neurons. J Neurosci 23(5):
1916–1923
Hess CW, Vaillancourt DE, Okun MS (2013) The temporal
pattern of stimulation may be important to the mecha-
nism of deep brain stimulation. Exp Neurol 247:
296–302
Hurtado JM, Gray CM, Tamas TB et al (1999) Dynamic of
tremor-related oscillations in the human globus pallidus:
a single case study. PNAS USA 96:1674–1679
Hutchison WD, Lozano AM, Tasker RR, Lang AE,
Dostrovsky JO (1997) Identiﬁcation and characteriza-
tion of neurons with tremor-frequency activity in
human globus pallidus. Exp Brain Res 113:557–563
Lenz FA, Kwan HC, Martin RL et al (1994) Single unit
analysis of the human ventral thalamic nuclear group
tremor-related activity in functionally identiﬁed cells.
Brain 117:531–543
Little S, Pogosyan A, Neal S et al (2013) Adaptive deep
brain stimulation in advanced Parkinson disease. Ann
Neurol 74:449–457
Magill PJ, Bolam JP, Bevan MD (2001) Dopamine regu-
lates the impact of the cerebral cortex on the sub-
thalamic
nucleus-globus
pallidus
network.
Neuroscience 106(2):313–320
McConnell GC, So RQ, Hilliard JD, Lopomo P, Grill WM
(2012) Effective deep brain stimulation suppresses
low-frequency network oscillations in the basal ganglia
by regularizing neural ﬁring patterns. J Neurosci
32(45):15657–15668
McIntyre CC, Grill WM, Sherman DL, Thakor NV
(2004) Cellular effects of deep brain stimulation:
model-based analysis of activation and inhibition.
J Neurophys 91:1457–1469
Miocinovic S, Lempka SF, Russo GS et al (2009) Experi-
mental and theoretical characterization of the voltage
distribution generated by deep brain stimulation. Exp
Neurol 216:166–176
Nini A, Feingold A, Slovin H, Bergmann H (1995) Neu-
rons in the globus pallidus do not show correlated
activity in the normal monkey, but phase-locked oscil-
lations appear in the MPTP model of parkinsonism.
J Neurophysiol 74:1800–1805
Pyragas K, Novicenko V, Tass PA (2013) Mechanism of
suppression of spontaneous low-frequency oscillations
in high-frequency stimulated neurons. Biol Cybern
107:669–684
Rosin B, Slovik M, Mitelman R et al (2011) Closed-loop
deep brain stimulation is superior in ameliorating par-
kinsonism. Neuron 72:370–384
Rubin D, Terman J (2004) High frequency stimulation of
the subthalamic nucleus eliminates pathological tha-
lamic
rhythmicity
in
a
computational
model.
J Comput Neurosci 16:211–235
Schuepbach
WMM,
Rau
J,
Knudsen
K
et
al
(2013) Neurostimulation for Parkinson’s disease with
early motor complications. New Engl J Med 368:
610–622
Shen K, Zhu Z, Munhall A, Johnson SW (2003) Synaptic
plasticity in rat subthalamic nucleus induced by high-
frequency stimulation. Synapse 50:314–319
Tass PA (2003) A model of desynchronizing deep brain
stimulation with a demand-controlled coordinated reset
of neural subpopulations. Biol Cybern 89:81–88
Tass PA, Majtanik M (2006) Long-term anti-kindling
effects of desynchronizing brain stimulation: a theoret-
ical study. Biol Cybern 94:58–66
Tass PA, Qin L, Hauptmann C et al (2012) Coordinated
reset neuromodulation has sustained after-effects in
parkinsonian monkeys. Ann Neurol 72:816–820
Terman D, Rubin JE, Yew AC, Wilson CJ (2002) Activity
patterns in a model for the subthalamopallidal network
of the basal ganglia. J Neurosci 22:2963–2976
Trottenberg T, Kupsch A, Schneider GH, Brown P, Kuehn
AA (2007) Frequency-dependent distribution of local
ﬁeld potential activity within the subthalamic nucleus
in Parkinson’s disease. Exp Neurol 205:287–291
Volkmann J, Moro E, Pahwa R (2006) Basic algorithms for
the
programming
of
deep
brain
stimulation
in
Parkinson’s disease. Mov Disord 21(Suppl 14):S284–
S289
Yousif N, Bayford R, Liu X (2008) The inﬂuence of
reactivity of the electrode-brain interface on the cross-
ing electric current in therapeutic deep brain stimula-
tion. Neuroscience 156:597–606
Dynamical Systems: Overview
Alla Borisyuk
Department of Mathematics, University of Utah,
Salt Lake City, UT, USA
Definition
Many models of computational neuroscience are
formulated in terms of nonlinear dynamical sys-
tem (sometimes the dynamical system with noise)
and include a large number of parameters. There-
fore, it is difﬁcult to analyze dynamics and ﬁnd
correspondence between parameter values and
dynamical mode. Mathematical theory of dynam-
ical systems and bifurcations provide a valuable
tool for a qualitative study and ﬁnding regions in
parameter
space
corresponding
to
different
dynamical modes (e.g., oscillations or bistability).
Thus,
the
dynamical
systems
(or
nonlinear
dynamics) approach to analysis of neural systems
40
Dynamical Systems: Overview

has played a central role for computational neuro-
science for many years (summarized, e.g., in
recent
textbooks,
Izhikevich
(2007)
and
Ermentrout and Terman (2010)).
Detailed Description
Theory of dynamical systems and bifurcations
provides a list of universal scenarios of dynamics
changes under parameter variation. For example,
one scenario explaining the onset of oscillations is
described by Andronov-Hopf bifurcation. In
terms of neuroscience, this theory provides insight
into the mechanisms underlying different neural
response properties and ﬁring patterns. Even more
importantly, it allowed to elucidate the underlying
mathematical structure that might be common to
whole classes of ﬁring behaviors. Thus, conclu-
sions of such studies can be wide ranging, even if
speciﬁc biophysical details of implementation
may differ from case to case.
A common theme in all articles in this section
is that they use concepts from nonlinear dynam-
ics, especially geometrical methods like phase
planes and bifurcation diagrams. Many exploit
time scale differences to reduce dimensionality
by dissecting the dynamics using fast-slow anal-
ysis, i.e., to separately understand the behaviors
on the different time scales and then patch the
behaviors together.
The articles in this section for the most part
exploit the idealized model of neuron localized at
one point (i.e., electrically compact neuron),
focusing on the nonlinearities of spiking dynam-
ics, and using biophysically minimal but biologi-
cally plausible description of neural dynamics. An
article
on
▶“Fitzhugh–Nagumo
Model”
describes one of the ﬁrst examples where dynam-
ical systems approach has been applied to analysis
of neural dynamics (Fitzhugh 1955). Although
“Fitzhugh-Nagumo Model” is not biologically
grounded and formulated in terms of the cubic
nonlinearity, the model is still considered as one
of the prototype models for excitable systems.
An
article
on
▶“Morris–Lecar
Model”
describes the rich dynamic repertoire of the two-
variable Morris-Lecar model, as its biophysical
parameters are varied. The original detailed anal-
ysis of this model (Rinzel and Ermentrout 1998)
has laid ground for similar approaches in many
different contexts and provided a theoretical jus-
tiﬁcation for inﬂuential Hodgkin’s classiﬁcation
of “excitability types” (Hodgkin 1948, described
in ▶“Excitability: Types I, II, and III”).
The articles on ▶“Integrate and Fire Models,
Deterministic” and ▶“Theta Neuron Model”
models target the most idealized end of the model-
ling spectrum where the spiking activity is
represented by simple mathematical models.
This approach to modelling of spiking times is
fruitful for implementation in the large neural
network
and
when
mathematical
analysis
(in addition to numerical simulations) is desired.
Somewhat more intricate features of single cell
dynamics are described in articles on modelling of
▶“Postinhibitory Rebound and Facilitation” and
▶“Spike-Frequency
Adaptation”
phenomena.
Finally, the fast-slow dissection and bifurcation
analysis really shine in the description of bursting
behavior. The bursting dynamics (of individual
cells or of population activity) is dissected into
active and silent phases when trajectories are
restricted to lower dimensional manifolds, and
transitions between these phases correspond to
reaching the manifold’s boundary and jumping
to a different manifold.
While most of these examples are for single-
cell dynamics, the qualitative mathematical study
is also applicable to the dynamics of neuronal
networks and structures, especially in the mean-
ﬁeld approximations. One such example for
network-generated rhythms is presented in the
article on ▶“Spike-Frequency Adaptation.”
Of course, the models presented in this section
are rather idealized and simpliﬁed for mathemat-
ical study compared to many other neuronal
models that are designed to investigate the bio-
physical details of action potential generation:
interaction of many known ionic currents, or the
spatial propagation of activity, etc. Such mini-
malistic models however are invaluable when
the problem or question at hand require only qual-
itative or semiquantitative characterizations of
spiking activity. This is especially important in
studies of large networks of interacting cells.
Dynamical Systems: Overview
41

Cross-References
▶Excitability: Types I, II, and III
▶Fitzhugh–Nagumo Model
▶Integrate and Fire Models, Deterministic
▶Morris–Lecar Model
▶Postinhibitory Rebound and Facilitation
▶Spike-Frequency Adaptation
▶Theta Neuron Model
References
Ermentrout GB, Terman D (2010) Mathematical founda-
tions of neuroscience. Springer, New York
Fitzhugh R (1955) Mathematical models of threshold phe-
nomena in the nerve membrane. Bull Math Biophys
17(4):257–278
Hodgkin AL (1948) The local electric changes associated
with repetitive action in a non-medullated axon.
J Physiol Lond 107:165–181
Izhikevich EM (2007) Dynamical systems in neuroscience:
the geometry of excitability and bursting. MIT Press,
Cambridge, MA
Rinzel J, Ermentrout B (1998) Analysis of neural excitabil-
ity and oscillations. In: Koch C, Segev I (eds) Methods
in neuronal modeling: from ions to networks, 2nd edn.
MIT Press, Cambridge, MA, pp 251–291
Dynamics of Disease States:
Overview
John Milton
W.M. Keck Science Center, The Claremont
Colleges, Claremont, CA, USA
Synonyms
Dynamical diseases; Periodic diseases
Definition
Computational neuroscience provides insights
into the mechanisms that underlie dynamic dis-
eases
of
the
nervous
system.
Neuro-
computationally inspired therapeutic devices that
replace functions lost due to disease offer tangible
hope to many for a better life.
Detailed Description
The evolution of an illness is one of the clues that
a physician uses to arrive at a diagnosis and treat-
ment strategy for diseases that affect the nervous
system. Is the onset acute or subacute? the clinical
course self-limited, relapsing-remitting, cyclic,
chronic progressive? and so on. The impetus for
studying disease dynamics comes from the math-
ematics and physics communities: their long
experience has shown that insights into mecha-
nism often derive from examining how dynamics
change. Consequently, the time evolution of a
disease is modeled using differential equations
and disease processes are described in terms of
the stability and nature of the model’s dynamical
behaviors. In 1977, Michael Mackey and Leon
Glass associated changes in physiological dynam-
ics from healthy to unhealthy with changes in
underlying
control
parameters
(Mackey
and
Glass 1977). Subsequently, this concept of a
dynamical disease was extended to that of a
dynamic disease to account for the possibility
that mechanisms other than those associated with
changes in parameters may be involved (see entry
▶“Dynamic Diseases of the Brain”).
Computational neuroscience extends dynami-
cal approaches to neurological disease in two
ways (Erdi et al. 2017). First, by making it possi-
ble to include anatomical, physiological, and
molecular details, computational models provide
insights into how mechanisms acting at the level
of molecules and individual neurons translate into
phenomena manifested clinically at the bedside
(see entry ▶“Modeling of Disease: Physical and
Molecular Level, Overview”). Historically, the
two neurological diseases which provided the
most insight into cortical function were temporal
lobe epilepsy and classical migraine. Indeed the
study of the geometry of migraine, drug, and
ﬂicker-induced visual fortiﬁcation patterns (see
entries
▶“Stochastic
Neural
Field
Theory,”
▶“Visual Hallucinations and Migraine Auras,”
and ▶“Flicker-Induced Phosphenes”) and their
propagation (see entry ▶“Migraines and Cortical
Spreading
Depression”)
has
provided
deep
insights into the functional architecture of the
visual
cortex.
Surgical
approaches
for
the
42
Dynamics of Disease States: Overview

treatment of patients with medically intractable
epilepsy provided the impetus to directly record
from cortex of awake humans. This, in turn, moti-
vated studies into the ability of large populations
of neurons to synchronize and generate seizures
(see entry ▶“Neural Population Models and Cor-
tical Field Theory: Overview”). Moreover, com-
putational models are now making it possible to
gain insights into diseases ranging from blood
pressure
control
(see
entry
▶“Baroreﬂex
Models”) to the nature of cognitive, functional,
and psychiatric diseases of the nervous system
that up to now have largely remained mysterious
(see entry ▶“Computational Psychiatry”).
Second, by having “a disease in a computer
model,” it is possible to efﬁciently evaluate and
reﬁne treatment strategies in silico before apply-
ing them to humans (Jirsa et al. 2017). Computa-
tional challenges remain because of the presence
of multistability (see enties ▶“Multistability in
Seizure Dynamics” and ▶“Multistability: Stop-
ping Events with Single Pulses”) and time delays
(see entry ▶“Time-Delayed Neural Networks:
Stability and Oscillations”). Indeed time-delayed,
multistable dynamical systems have a tendency to
generate transient oscillations that can be easily
mistaken for limit cycle oscillations thus causing
confusion (see entry ▶“Delay-Induced Transient
Oscillation (DITO) and Metastable Behavior”).
Although the ultimate goal of medicine is cure,
one cannot overlook the need to improve the
patient’s quality of life when cure cannot be
achieved. The electrical properties of neurons
make it possible to use electrical stimuli as a
treatment modality. Applications range from
aborting seizures with electrical stimuli (see
entry ▶“Multistability: Stopping Events with
Single Pulses”) to improving the quality of move-
ments of patients with Parkinson’s disease with
deep brain stimulation (see entries ▶“Parkinson’s
Disease: Deep Brain Stimulation”, ▶“Computa-
tional Models of Deep Brain Stimulation (DBS)”,
and ▶“Deep Brain Stimulation (Models, Theory,
Techniques): Overview”). Even noisy stimuli can
beneﬁt the function of cochlear implants in the
hearing impaired or balance control in those with
peripheral neuropathies by making it easier for the
sensory nervous system to detect weak signals
(see entry ▶“Stochastic Resonance: Balance
Control and Cochlear Implants”). In the last few
years alone, these ideas have been translated into
devices that can be used by patients, e.g.,
NeuroPace RNS® system for patients with epi-
lepsy, Microsoft’s Emma Watch® for patients
with essential tremor, shoes with vibrating insoles
to improve gait and balance in the elderly (Lipsitz
et al. 2015).
As insight increases in our understanding of
neural encoding, it is becoming possible to
replace broken parts with electronic ones that
perform the same function. The large number of
articles in this encyclopedia point to the current
enthusiasm in this therapeutic approach. Applica-
tions include restoring vision to the visually
impaired (see entry ▶“Retinal/Visual Interfaces
(Models, theory, Techniques): Overview”), hear-
ing to those who cannot hear (see entry ▶“Periph-
eral
Nerve
Interface
Applications,
Cochlear
Implants”), continence to those incontinent (see
entry ▶“Methodologies for the Restoration of
Bladder and Bowel Functions”), and relief from
pain to those who suffer (see entries ▶“Pain Pro-
cessing Pathway Models” and ▶“Peripheral
Nerve
Interface
Applications,
Neuropathic
Pain”). Dramatically it has become possible to
interface the brain directly with electronic devices
and make it possible for a patient to move robotic
limbs by thought alone (see entries ▶“Cortical
Motor Prosthesis” and ▶“Functional Neurosci-
ence: Cortical Control of Limb Prostheses”).
Even direct brain-to-brain interfaces are possible
(Rao et al. 2014).
The frontier for dynamic disease is to under-
stand the collective behaviors of the nervous sys-
tem that emerge over the time scale of years. Can
the
development
of
an
epileptic
focus
(epileptogenesis) be halted early so that an indi-
vidual at risk never experiences a seizure? Can the
rate of learning of a complex voluntary skill, such
as the golf swing, by a patient with a robotic or
stem cell-derived limb prosthesis be sped up to the
point that the individual could enjoy the use of
these limbs throughout a lifetime? Large, complex
physical systems tend to self-organize dissipative
structures, namely dynamical entities whose exis-
tence is maintained far-from-equilibrium by a
Dynamics of Disease States: Overview
43

supply of energy. Already dynamical signatures of
this self-organization, including power laws, have
been observed in the bursting propagating activi-
ties of living neural populations (see entry
▶“Neuronal Avalanches”) and the dynamics of
human balance control (see entry ▶“Human
Balancing Tasks: Power Laws, Intermittency,
and Lévy Flights”).
Computational neuroscience provides the tools
for understanding how the nervous system learns
to exert control thereby bringing to many tangible
hope of a better life.
Key Entries
•
Computational Psychiatry
•
Dynamic Diseases of the Brain
•
Epilepsy: Computational Models
•
Flicker-Induced Phosphenes
•
Functional Neuroscience: Cortical Control of
Limb Prostheses
•
Human Balancing Tasks: Power Laws, Inter-
mittency, and Lévy Flights
•
Migraines and Cortical Spreading Depression
•
Multistability: Stopping Events with Single
Pulses
•
Neuronal Avalanches
•
Parkinson’s Disease: Deep Brain Stimulation
•
Stochastic Resonance: Balance Control and
Cochlear Implants
•
Time-Delayed Neural Networks: Stability and
Oscillations
•
Visual Hallucinations and Migraine Auras
Cross-References
▶Baroreﬂex Models
▶Computational Models of Deep Brain Stimula-
tion (DBS)
▶Cortical Motor Prosthesis
▶Deep Brain Stimulation (Models, Theory,
Techniques): Overview
▶Delay-Induced Transient Oscillation (DITO)
and Metastable Behavior
▶Intermittent Control of Movement and Balance
▶Intermittent Control Strategy for Stabilizing
Human Quiet Stance, A Model of the
▶Methods for Optimizing Stimulus Waveforms
for Electroceutical Control
▶Modeling of Disease: Physical and Molecular
Level, Overview
▶Multistability in Seizure Dynamics
▶Pain Processing Pathway Models
▶Peripheral
Nerve
Interface
Applications,
Cochlear Implants
▶Role of Delayed Feedback in Human Balancing
▶Stochastic Neural Field Theory
References
Erdi P, Bhattacharya BS, Cochran AL (eds) (2017) Com-
putational
neurology
and
psychiatry.
Springer,
New York
Jirsa VK, Proix T, Perdikis D, Woodman MM, Wang H,
Gonzalez-Martinez J, Bernard C, Bénar C, Guye M,
Chauvel P, Bartolomei F (2017) The virtual patient:
individualized
whole-brain
models.
NeuroImage
145:377–388
Lipsitz L, Lough M, Niemi J, Travison T, Howlett H,
Manor B (2015) A shoe insole delivery subsensory
vibration noise improves balance and gait in healthy
elderly people. Arch Phys Med Rehabil 96:423–439
Mackey MC, Glass L (1977) Oscillation and chaos in
physiological control systems. Science 197:287–289
Rao RPN, Stucco A, Bryan M, Sarma D, Youngquist TM,
Wu J, Prat CS (2014) A direct brain-to-brain interface
in humans. PLoS One 9:e111332
Gamma and Theta
Oscillations, Hippocampus:
Overview
Frances K. Skinner
Krembil Research Institute, University Health
Network, Toronto, ON, Canada
Department of Medicine (Neurology) and
Physiology, University of Toronto, Toronto, ON,
Canada
Detailed Description
The brain expresses several rhythms that are asso-
ciated with normal and pathological states. The
hippocampus is arguably the most heavily studied
44
Gamma and Theta Oscillations, Hippocampus: Overview

structure in the brain for many reasons including
its importance in learning and memory, and it
generates
several
population
activities
that
include theta and gamma rhythms. Many model-
ing studies have focused on these oscillatory
activities, and the entries in this section detail
much of this work. To develop and build models
of any biological system, an in-depth appreciation
of the biological and physiological basis of what is
being modeled is required. This is provided in the
▶“Hippocampus, Theta, Gamma, and Cross-
Frequency Coupling” entry, where functional
and experimental aspects are described, along
with data analysis aspects in ▶“Theta-Gamma
Cross-Frequency Analyses (Hippocampus).” In
developing models, the cellular units and how
they are connected need to be examined. Three
entries on ▶“Hippocampus, Model Inhibitory
Cells,”
▶“Hippocampus,
Model
Excitatory
Cells,” and ▶“Hippocampus, Model Network
Architecture” provide these details, and the chal-
lenges and complexities in these aspects are laid
bare. Three theoretical entries on ▶“Subthreshold
Antiresonance and Antiphasonance in Single
Neurons: 3D Models,” ▶“Quadratization: From
Conductance-Based Models to Caricature Models
with Parabolic Nonlinearities,” and ▶“Mixed-
Mode Oscillations in Single Neurons” provide
the reader with the types of theoretical approaches
that can be used to help understand how these
subthreshold and mixed-mode activities that are
present in hippocampal cells may contribute to
theta and gamma rhythms. Gamma rhythms have
been extensively studied both theoretically and
experimentally, and as such, there are well-
developed mechanistic understandings for their
generation. These mechanisms are described and
illustrated in the entry ▶“Hippocampal Oscilla-
tions, Mechanisms (PING, ING, Sparse).” This is
not the case for theta rhythms or nested theta/
gamma rhythms. However, network models have
been developed and these are described along
with the many considerations that arise in devel-
oping such network models in ▶“Hippocampal
Theta, Gamma, and Theta/Gamma Network
Models.”
In combination, the entries in this section pro-
vide the reader with a solid basis to learn about
theta and gamma oscillations in hippocampus
considering mathematical modeling and experi-
mental perspectives. An appreciation of the
many aspects that are involved, both general and
speciﬁc, can be gained from reading these entries.
Cross-References
▶Hippocampal
Oscillations,
Mechanisms
(PING, ING, Sparse)
▶Hippocampal
Theta,
Gamma,
and
Theta/
Gamma Network Models
▶Hippocampus, Model Excitatory Cells
▶Hippocampus, Model Inhibitory Cells
▶Hippocampus, Model Network Architecture
▶Hippocampus, Theta, Gamma, and Cross-
Frequency Coupling
▶Mixed-Mode Oscillations in Single Neurons
▶Quadratization:
From
Conductance-Based
Models to Caricature Models with Parabolic
Nonlinearities
▶Subthreshold
Antiresonance
and
Anti-
phasonance in Single Neurons: 3D Models
▶Subthreshold Resonance and Phasonance in
Single Neurons: 2D Models
▶Theta-Gamma
Cross-Frequency
Analyses
(Hippocampus)
General Overview of Spinal
Anatomy and Physiology
Organization
Marion Murray
Department of Neurobiology and Anatomy,
Drexel University College of Medicine,
Philadelphia, PA, USA
Abbreviations
CNS
Central nervous system
CPG
Central pattern generator
CST
Corticospinal tract
DRG
Dorsal root ganglion
General Overview of Spinal Anatomy and Physiology Organization
45

DSCT
Dorsal spinocerebellar tract
H-reﬂex
(Hoffmann
reﬂex)
Electrical analogue of the
monosynaptic stretch reﬂex
PRV
Pseudorabies virus
RV
Rabies virus
SCI
Spinal cord injury
STT
Spinothalamic tract
VSCT
Ventral spinocerebellar tract
Definition
Organization of the mammalian spinal cord and
activity-dependent and injury-induced plasticity
of spinal pathways.
Detailed Description
Somatic perceptions, coordinated movements,
and autonomic functions depend on the integrity
of the spinal cord and its projections. In Part I of
this entry, we describe the classic organization of
the human spinal cord which is similar in most
respects, except size, to other vertebrate spinal
cords (see Sengul et al. 2012; Watson et al.
2010). The organization of the cord determines
the accessibility of the neural structures for deliv-
ery of stimulation aimed at reengaging or modu-
lating
spinal
neuronal
output.
In
Part
II,
consideration is given to how spinal pathways
may be modiﬁed by activity, injury, and injury
followed by activity/stimulation, which is increas-
ingly important in the development and applica-
tion of rehabilitation protocols. Both type of
activity and severity of injury are important in
determining the optimal protocols for rehabilita-
tion of individuals with spinal cord injury.
The Spinal Cord Is Organized
Segmentally
The human spinal cord is comprised of 31 contin-
uous spinal segments (Fig. 1). These are divided
into cervical (C1 through C8) segments that sup-
ply the neck and arms, thoracic (T1 through T12)
segments innervating the trunk and sympathetic
ganglia, lumbar (L1 through L5) segments sup-
plying the legs, and sacral (S1 through S5) and
coccygeal (one segment) segments, supplying the
saddle region, buttocks, pelvic organs, and para-
sympathetic ganglia. Dorsal and ventral roots
enter and leave the vertebral column through
intervertebral foramina at vertebral segments
corresponding to the spinal segment. At its caudal
end, the cord tapers to form the conus medullaris
and the lumbar, sacral, and coccygeal roots
extending to their appropriate vertebral levels to
form a bundle, the cauda equina (“horse’s tail”).
The spinal cord is divided into cervical, tho-
racic, lumbar, and sacral segments. Note the exit
of
the
lumbar
and
sacral
roots
through
intervertebral foramina located caudal to the spi-
nal segment with which the roots are associated.
General Overview of Spinal Anatomy and Physiology
Organization, Fig. 1 Organization of the spinal cord
46
General Overview of Spinal Anatomy and Physiology Organization

A segment is deﬁned by dorsal roots that enter
the cord carrying sensory information and ventral
roots that exit the cord carrying motor commands
(Fig. 2). The axons in the dorsal roots arise from
dorsal root ganglion (DRG) cells located in paired
ganglia lateral to the vertebral column. The axon
from each DRG cell bifurcates to give rise to a
centrally directed process (dorsal root axon or
primary afferent) which projects into the spinal
cord to terminate on neurons within the CNS. The
peripherally directed axonal process enters a spi-
nal nerve. Some of these peripheral sensory axons
transmit information from sensory receptors in the
skin; the strip of skin supplied by the peripheral
process from cells in one dorsal root ganglion is
known as a dermatome (Fig. 3). Other peripher-
ally directed axons innervate the muscle spindles,
sensory organs within muscles that mediate
proprioception.
In this diagram of two segments of spinal cord,
three dorsal roots enter the dorsal lateral surface of
the cord, and three ventral roots exit. The dorsal
root ganglion contains dorsal root ganglion cells
whose axons bifurcate; one process enters the
spinal cord through the dorsal root and the other
extends peripherally to supply the skin and muscle
of the body. The ventral root is formed by axons
from motor neurons located in the spinal cord.
Axons from motor neurons in the spinal cord
exit in the ventral roots and innervate skeletal
muscles or autonomic ganglia. These ventral
root axons join with the peripheral processes of
the DRG cells to form spinal nerves, which con-
tain both sensory and motor axons. Several spinal
nerves may join to form a peripheral nerve.
Spinal Neurons Are Organized into Nuclei and
Laminae
In cross sections, the spinal cord is composed of a
butterﬂy-shaped core of gray matter, containing
neuronal and glial cell bodies and their processes
that is surrounded by white matter, composed of
axons and their associated glial cells. The gray
matter is subdivided into a sensory portion, the
dorsal (or posterior) horn, and a motor portion, the
ventral (or anterior) horn, separated by the inter-
mediate zone (Fig. 4, Table 1).
Comparison of the classiﬁcation schemes for
the neurons in the gray matter of the spinal cord.
On the right, a section stained to show cell size,
distribution, and location of neurons in the lumbar
spinal cord and, on the left, the related laminar
boundaries.
The neurons in the spinal gray matter are also
classiﬁed according to their projections. These
include sensory relay neurons, which receive dor-
sal root input and whose axons project into
ascending pathways that terminate in the brain or
spinal cord, motor neurons whose axons exit in
the ventral roots to synapse on muscle ﬁbers or on
neurons in autonomic ganglia, and propriospinal
neurons, spinal interneurons whose cell bodies
General Overview of
Spinal Anatomy and
Physiology
Organization,
Fig. 2 Spinal segments
General Overview of Spinal Anatomy and Physiology Organization
47

and axons are conﬁned to the spinal cord. Pro-
priospinal neurons are by far the most numerous,
accounting for about 90% of all spinal neurons,
and the most poorly understood.
Many of the neurons in the gray matter are
clustered in functionally related nuclei. These
nuclei may extend much of the length of the spinal
cord, forming columns of cells, e.g., Clarke’s
General Overview of
Spinal Anatomy and
Physiology
Organization,
Fig. 3 Dermatome.
A dermatome is the area of
skin supplied by axons from
a single dorsal root ganglion
48
General Overview of Spinal Anatomy and Physiology Organization

nucleus, the intermediolateral nucleus, and motor
neurons. In cross sections, the neurons in the gray
matter can also be seen to be distributed in a
laminar arrangement, particularly in the dorsal
horn. Because the histologic differences among
laminae reﬂect functional differences, the spinal
gray matter is sometimes also classiﬁed into
laminae.
The
dorsal
horn
and
intermediate
zones
(laminae I through VII) contain sensory relay
nuclei and include the marginal nucleus (lamina I),
the substantia gelatinosa (lamina II), and the
nucleus proprius (laminae III, IV). Motor neurons
are subdivided functionally into somatic and vis-
ceral motor neurons. Somatic motor neurons are
located in the ventral horn (laminae VIII and IX) at
all spinal levels and innervate striated muscle. All
visceral motor neurons are located in the interme-
diate zone (lamina VII) at C8 through L3
(sympathetic) or S2 through S4 (parasympathetic)
and innervate neurons in autonomic ganglia. Com-
missural neurons whose axons cross to the contra-
lateral side of the spinal cord are located primarily
in lamina X. These schemes for classifying spinal
neurons are compared in Fig. 4 and Table 1.
Ascending and Descending Tracts in the
White Matter
The white matter is subdivided into three funiculi:
dorsal (ascending pathways), lateral (ascending
and
descending
pathways),
and
ventral
(descending pathways), demarcated by the dorsal
medial sulcus, the dorsal root entry zone, the
ventral
roots,
and
the
ventromedial
sulcus
(Fig. 5). Propriospinal axons ascend and descend
in the fasciculus proprius bordering the gray mat-
ter. Short propriospinal axons are located medi-
ally, immediately adjacent to the gray matter, and
longer propriospinal axons ascend and descend,
adjacent to the shorter axons.
Cross section of cervical spinal cord shows
major landmarks and divisions of white matter.
Cells in the dorsal root ganglia and spinal gray
matter give rise to the axons that form the ascend-
ing pathways that transmit sensory information to
the brain. Cell bodies whose axons course in
descending tracts are located in many parts of
the brain. Their axons descend in the lateral and
ventral funiculi and terminate on motor neurons
or, more often, on propriospinal neurons that
Ventral
horn
IX
IX
X
VII
III
II
I
VIII
VI
IV
V
Intermediate
zone
Dorsal horn
General Overview of Spinal Anatomy and Physiology
Organization, Fig. 4 Organization of the spinal gray
matter
General Overview of Spinal Anatomy and Physiology
Organization, Table 1 Classiﬁcation of spinal neurons
Gray matter
subdivision
Lamina
Nuclei included in
laminae
Lamina I
Marginal nucleus
Dorsal horn
Lamina II
Substantia
gelatinosa
Lamina III, IV
Nucleus proprius
Lamina V
Reticular nucleus
Intermediate
zone
Lamina VI
Commissural
nuclei
Lamina VII
Clarke’s,
intermediolateral
Nuclei
Ventral horn
Lamina VIII
Medial motor
nuclei
Lamina IX
Lateral motor
nuclei
Commissure
Lamina X
Central gray
General Overview of Spinal Anatomy and Physiology
Organization,
Fig.
5 External
landmarks
of
the
spinal cord
General Overview of Spinal Anatomy and Physiology Organization
49

project to premotor or motor neurons within the
spinal gray matter. Descending systems provide
central control of movement and posture. Some
descending axons terminate on sensory relay neu-
rons in the dorsal horn and can therefore modify
sensory input to the brain. Propriospinal axons
may be very short, connecting one cell with its
neighbor, others (commissural ﬁbers) cross the
midline dorsal or ventral to the central canal, and
long propriospinal axons ascend or descend in the
white matter to connect distant segments of the
cord (e.g., segments supplying upper and lower
limbs).
Sensory Pathways
The sensory input to the CNS from the body is
organized according to information about the type
of sensation (modality) and location of the stimu-
lus. This information is used to produce appropri-
ate spinal reﬂexes and is transmitted to appropriate
parts of the brain for sensory processing. Informa-
tion about a noxious stimulus to the skin is dis-
tributed in pathways that are different from those
that transmit information about non-painful stim-
uli, such as light pressure or proprioception.
Upon entry into the spinal cord, the dorsal root
ﬁbers separate into a lateral and a medial division.
The lateral division (Lissauer’s tract) contains ﬁner
myelinated and unmyelinated ﬁbers, originating
from small dorsal root ganglion cells, and transmits
responses
to
nociceptive
(painful),
non-
discriminatory or crude touch and thermal stimula-
tion of the skin and viscera. The medial division
contains large-caliber ﬁbers from large dorsal root
ganglion cells, whose peripheral receptors lie in
muscle, joints, and skin. These ﬁbers relay infor-
mation about muscle length and tension to motor
neurons and to propriospinal neurons, at segmental
levels and to Clarke’s and the lateral cuneate nuclei
more rostrally. These pathways provide respectives
the basis for spinal reﬂexes and information about
somesthesia and joint position relayed to the brain,
providing
the
basis
for
stereognosis
(i.e., identiﬁcation of shape and size of an object).
The Lateral Division
Lissauer’s tract (Fig. 6) contains small-diameter
axons
that
convey
the
modalities
of
pain,
temperature, and crude or nondiscriminatory
touch. Some axons branch and the branches
ascend or descend in Lissauer’s tract for several
segments before entering into the lateral portion of
the dorsal horn to synapse on cells in the dorsal
horn (marginal nucleus, substantia gelatinosa,
nucleus proprius, laminae I to IV). This distribu-
tion of collaterals rostrally and caudally means
that activation of axons in one segment of the
lateral division may stimulate dorsal horn cells
over several adjacent segments. Neurons in the
dorsal horn relay sensory information received
from dorsal root axons to nuclei in the brain.
These second-order neurons transmit this infor-
mation through axons that cross and ascend in the
lateral funiculus. Sensory relay neurons also
receive input from various descending tracts,
either directly or via interneurons. In this way,
the neuron’s ability to respond to sensory input
is modiﬁed by the brain.
Entry and central course of axons of the lateral
division from one dorsal root ganglion. These
unmyelinated and thinly myelinated axons may
branch and ascend or descend several segments in
the tract of Lissauer before entering the gray mat-
ter and synapsing on second-order neurons in the
dorsal horn.
The formation of the spinothalamic tract (STT)
is shown in Fig. 7. Most axons in the STT arise
from the marginal cells in lamina I and from the
nucleus proprius in laminae III and IV and trans-
mit nociceptive and thermal information. The
axons of these second-order neurons cross to the
contralateral spinal cord through the ventral
General Overview of Spinal Anatomy and Physiology
Organization, Fig. 6 Lissauer’s tract
50
General Overview of Spinal Anatomy and Physiology Organization

commissure and ascend in the white matter as the
spinothalamic tract to terminate in the thalamus
and other targets in the brain. They travel with
other sensory spino-brainstem pathways in the
anterolateral system (ALS).
Lateral division axons entering the dorsal root
synapse on second-order neurons giving rise to
axons that cross the spinal cord. Some axons form
the spinothalamic tract (STT) and ascend to ter-
minate in the thalamus. Some of these axons
ascend to terminate in the reticular formation
(spinoreticular tract, SRT) or mesencephalon
(spinomesencephalic tract, SMT). These path-
ways travel together in the ventrolateral white
matter to form the anterolateral system (ALS).
The Medial Division
Dorsal root axons in the medial division have
local targets at their segments of entry. They
may also give off collateral axonal branches that
ascend to terminate in more distant targets in
somatosensory relay and cerebellar relay nuclei.
Axons with local segmental targets enter the gray
matter, synapse on propriospinal or motor neurons
at that segmental level, and subserve reﬂex orga-
nization (Fig. 8). The Ia ﬁbers of the medial divi-
sion whose peripheral processes innervate stretch
receptors in muscle spindles send central pro-
cesses into the ventral horn, to synapse on the
dendrites of motor neurons and propriospinal neu-
rons. The monosynaptic connection between sen-
sory axons and motor neurons is the anatomical
basis for an important reﬂex, the stretch reﬂex.
Other medial division axons, including collaterals
of axons with segmental targets, enter the white
matter in the dorsal funiculus, where they ascend
to terminate in relay nuclei for somatosensory or
cerebellar pathways. These relay nuclei receive
sensory information from the skin, muscles,
joints, fascia, and other tissues and then transmit
the information to nuclei in the thalamus. The
thalamus relays information to the cortex, where
it is consciously appreciated or to the cerebellum,
where it contributes to the control of posture and
movement, without being consciously perceived.
Axons forming the medial division enter the
spinal cord and ascend in the dorsal columns or
make local reﬂex connections on motoneurons or
propriospinal neurons or contact spinocerebellar
nuclei.
Dorsal root ﬁbers that project to somatosensory
relay nuclei enter the dorsal funiculus at each
segment, displacing medially the ﬁbers originat-
ing from more caudal ganglia. As a result, these
ﬁbers become laminated, i.e., topographically
ALS
(STT, SRT, SMT)
Thalamus
Group III,
IV fibers
General Overview of Spinal Anatomy and Physiology
Organization, Fig. 7 Formation of the spinothalamic
tract (STT)
General Overview of Spinal Anatomy and Physiology
Organization, Fig. 8 Medial division
General Overview of Spinal Anatomy and Physiology Organization
51

organized. In the cervical region, ﬁbers from
sacral dorsal roots are found nearest the midline
and those from cervical roots nearest the dorsal
root entry zone. Fibers representing the lower half
of the body (sacral to T5) ascend in the gracile
fasciculus; those from the upper half (T5 to C2)
comprise the cuneate fasciculus. Axons in both
bundles terminate ipsilaterally in nuclei of the
medulla for which they are named, the nucleus
gracilis and cuneatus. The gracilis and cuneate
nuclei (dorsal column nuclei) then relay informa-
tion to the thalamus (Fig. 9).
Axons mediating ﬁne tactile sensibility con-
tribute to the medial division. They ascend in the
dorsal columns to the brainstem, where they ter-
minate on second-order neurons in the dorsal col-
umn nuclei. The axons arising from lumbar and
low thoracic dorsal root ganglia ascend in the
fasciculus gracilis and terminate in the nucleus
gracilis. Axons arising from upper thoracic and
cervical ganglia ascend in the more laterally
located fasciculus cuneatus and terminate in the
nucleus cuneatus located lateral to the nucleus
gracilis in the medulla.
Other dorsal root axons enter the dorsal funic-
ulus to ascend for several segments before termi-
nating in relay nuclei for cerebellar pathways.
Axons arising from DRG cells in caudal thoracic,
lumbar, and sacral regions ascend in the dorsal
columns
to
terminate
in
Clarke’s
nucleus
(Fig. 10), located in segments T1 to L2. Those
arising from more rostral ganglia ascend in the
dorsal columns to terminate in the lateral (also
called the external or accessory) cuneate nucleus
in the medulla. The dorsal spinocerebellar tract
(DSCT) arises from neurons located in Clarke’s
nucleus; it ascends in the white matter ipsilaterally
and is therefore uncrossed. The DSCT terminates
in the cerebellum. Similarly, the cuneocerebellar
tract carries information from the lateral cuneate
nucleus to the cerebellum. Both nuclei therefore
General Overview of Spinal Anatomy and Physiology Organization, Fig. 9 Dorsal column system
General Overview of Spinal Anatomy and Physiology
Organization, Fig. 10 Spinocerebellar systems
52
General Overview of Spinal Anatomy and Physiology Organization

relay sensory information from the periphery to
the cerebellum. There is a third pathway to the
cerebellum,
the
ventral
spinocerebellar
tract
(VSCT). Cell bodies whose axons form the
VSCT are distributed throughout the dorsal horn
and intermediate zone; their axons cross in the
ventral commissure to ascend in the contralateral
VSCT, as part of the ALS, to the cerebellum,
where
they
cross again
before terminating.
Although the course of the VSCT differs from
that of the DSCT and cuneocerebellar pathways,
their functions appear to be related.
Axons conveying proprioceptive and muscle
information also form part of the medial division.
Axons from lumbar and caudal thoracic dorsal
root ganglia enter the spinal cord and ascend
ipsilaterally in the dorsal columns to the thoracic
level where they terminate on second-order neu-
rons in Clarke’s nucleus. The axons of Clarke’s
neurons ascend in the lateral funiculus as the
dorsal spinocerebellar tract (DSCT) which termi-
nates on third-order neurons in the cerebellum.
Axons from more rostral segments ascend syn-
apse in the lateral cuneate nucleus (not shown),
which then projects to third-order neurons in the
cerebellum.
Motor Pathways
Descending tracts are located in the lateral and
ventral funiculi. Most of the axons in these tracts
terminate on propriospinal neurons, which then
project to premotor or motor neurons; very few
terminate directly on motor neurons. The location
of
four
important
descending
tracts,
the
corticospinal,
rubrospinal,
reticulospinal,
and
vestibulospinal tracts, is shown in Fig. 11.
The corticospinal tract (CST) descends to the
spino-medullary junction, where 90% of the
axons cross, and then continues to descend as
the lateral corticospinal tract in the lateral funicu-
lus contralateral to the cell bodies of origin. Those
axons that do not cross at the spino-medullary
junction descend in the spinal cord as the ventral
corticospinal tract but then cross in the spinal cord
before their termination contralateral to their ori-
gin. In rodents, unlike primates and cats, the CST
crosses at the spino-medullary but descends in the
ventral portion of the dorsal columns within the
spinal cord. The rubrospinal tract is also largely
crossed and is located in the lateral funiculus. The
reticulospinal tract, including the modulatory
monoaminergic pathways (serotonergic and nor-
adrenergic), contains axons that arise from reticu-
lar nuclei ipsilaterally and contralaterally. The
vestibulospinal tract is almost entirely uncrossed.
Somatic
motoneurons
are
somatotopically
organized within the ventral horn (laminae VIII
and IX). Their axons exit in the ventral roots and
innervate striated muscle. A medial motor nucleus
is present in the ventral horn throughout the length
General Overview of
Spinal Anatomy and
Physiology
Organization,
Fig. 11 Location of
ascending (right) and
descending tracts (left)
General Overview of Spinal Anatomy and Physiology Organization
53

of the cord. These motor neurons innervate axial
(trunk) musculature. There are also prominent
groups of nuclei located laterally in the ventral
horn, which are particularly well developed in the
lumbar and cervical segments that supply the
limbs. The lateral group of nuclei is subdivided
functionally into ventral nuclei, which innervate
extensor muscles, and dorsal nuclei, which inner-
vate ﬂexors. Within these two subdivisions, the
neurons innervating proximal muscles are located
medially and those that supply the distal muscles
more laterally (Fig. 12).
Axial musculature is supplied by motor neu-
rons located medially and limb musculature by
motor neurons located laterally in the ventral
horn. Those innervating extensors are more ven-
tral and ﬂexors more dorsal; neurons innervating
proximal muscles are more medial than those
innervating distal muscles.
Visceral motoneurons (preganglionic neurons)
are located in the intermediate zone. They inner-
vate neurons in autonomic ganglia and the auto-
nomic ganglion cells (postganglionic neurons)
innervate visceral organs. Those preganglionic
neurons in the intermediolateral nucleus in lamina
VII at C8 to L3 send axons to the ganglia in the
sympathetic chain, providing central regulation of
the sympathetic nervous system. Neurons in the
intermediate zone at levels S2 through S4 form the
more
poorly
deﬁned
sacral
parasympathetic
nucleus. Their axons innervate the sacral para-
sympathetic ganglia and thus provide central con-
trol of the sacral portion of the parasympathetic
system that mediates bowel, bladder, and sexual
reﬂexes. Central control of more rostral portions
of the parasympathetic system is provided by
groups of cranial nerve nuclei.
Propriospinal Systems
A great deal of detailed information is available
about DRG cells, motor neurons, and their pro-
jections. The cell bodies of both motor neurons
and DRG cells can be identiﬁed morphologically
and by methods that label transmitters, receptors,
or other cell-type speciﬁc molecules. Axonal pro-
jections can be visualized and traced by injecting
dyes or viral vectors that are incorporated and
transported retrogradely, anterogradely, or in
both directions and can identify motor neurons
that project to speciﬁc muscles and DRG cells,
as reviewed in Lanciego and Wouterlood (2011).
Propriospinal neurons, however, make up the
vast majority of spinal neurons. Because pro-
priospinal neurons are engaged in complex spinal
networks, e.g., the central pattern generator
(CPG), and in pain transmission (see entry
▶“Sensory Input to Central Pattern Generators,”
this volume), a fuller understanding of their con-
nectivity will be important in appreciating how
these complex circuits form, how they function,
and how they may be modiﬁed by activity or
injury.
In the last few years, new technologies have
accelerated progress in understanding the connec-
tivity and functions of propriospinal neurons
(reviewed in chapters in Ziskind-Conhaim et al.
2010, 2013; Conta and Stelzner 2008). The advent
of transsynaptic tracing using pseudorabies virus
(PRV) and monosynaptic tracing using a geneti-
cally modiﬁed replication-deﬁcient rabies virus
(RV) has provided more detail about interneuronal
pathways (Arber 2012; Stepien et al. 2010;
Coulton et al. 2011). RV or PRV labeling can be
combined with immunocytochemical or other
methods to identify the phenotype of the premotor
propriospinal neurons.
Developmental neurobiology and molecular
genetics have combined to provide additional
General Overview of Spinal Anatomy and Physiology
Organization, Fig. 12 Somatotopic organization of ven-
tral motoneurons
54
General Overview of Spinal Anatomy and Physiology Organization

ways of differentiating subpopulations of pro-
priospinal neurons, based on lineage (time of ori-
gin,
birthdate),
and
the
expression
of
transcriptional markers that control neuronal dif-
ferentiation and migration to speciﬁc positions in
the adult CNS. Genetic deletion or pharmaco-
logic blockade can identify the consequences of
loss of function in these propriospinal neurons
and suggest their roles during development and
as adults in the organization of spinal networks
(reviewed in Arber 2012; Grossman et al. 2010;
Kiehn 2011). These advances have been particu-
larly fruitful in understanding the connectivity of
the central pattern generator (CPG). While the
network of propriospinal neurons that comprise
the CPG is normally modulated by supraspinal
projections and afferent input, it can function in
the absence of information from the brain or
periphery to produce rhythmic, patterned neural
activity
or
ﬁctive
locomotion
(Hägglund
et al. 2010).
The complexity of the propriospinal neuronal
connections is still only incompletely understood.
The combination and further development of
these and other methods that can detect activity
in ensembles of functionally related neuronal net-
work and provide 3-dimensional and live imag-
ing, promises a rapid acceleration of progress in
this area (Di Maio et al. 2011; Ertuk and Bradke
2013; Hinckley and Pfaff 2013; Laskowski and
Bradke 2013).
Spinal Pathways Are Plastic
While the ﬁrst part of this entry describes the
organization of the spinal cord as if it were static,
we know that the spinal cord retains the ability to
adapt to changes in its environment (plasticity)
even in adults. Signiﬁcant changes in spinal cord
neurons and their environment occur as a result of
active or passive exercise or training (activity-
dependent plasticity). Changes induced by injury
occur in afferent, descending, and propriospinal
systems and include sprouting, receptor regula-
tion, and alterations in neuronal properties, in
addition to the inﬂammatory and degenerative
responses resulting from the injury. When spinal
cord injury is followed by a program of exercise,
the functional and anatomical modiﬁcations in
spinal organization may be further altered or
even reversed sufﬁciently to support greater
recovery
of function.
Understanding
lesion-
induced
changes
and
promoting
plasticity
directed by activity can thus be exploited to
improve function.
Activity-Dependent Plasticity
Activity-dependent plasticity refers to functional
changes in response to activity related to move-
ment. This may include exercise and speciﬁc
training protocols (Harkema et al. 2012; Roy
et al. 2012) or operant conditioning (Pillai et al.
2008; Wolpaw and Chen 2009; Chen et al. 2011)
that modify motor output.
Plasticity Following Spinal Cord Injury (SCI)
An
injury
may
be
complete
(transection
spinalization), or it may spare some pathways
and
thus
be
incomplete
(contusion,
clip
compression, hemisection, tractotomy). Contu-
sion or clip-compression injuries are considered
to be clinically more relevant, while hemisections
or tractotomies are surgical injuries that allow
evaluation of speciﬁc pathways. The conse-
quences of an incomplete injury will depend on
which pathways are spared. Spinal cord injury
does
not
elicit
substantial
regeneration
of
axotomized neurons and will result in death of
some short projecting propriospinal neurons in
the vicinity of the injury (Conta Steencken and
Stelzer 2010), structural changes in neurons
located more distantly from the injury (Gazula
et al. 2004), and apoptosis of oligodendroglia
(Beattie et al. 2002). Axons that survive an incom-
plete injury may show long-lasting changes in
transmission (Arvanian et al. 2009; Cote et al.
2012) and deﬁcits (but perhaps not permanent)
in conduction properties resulting from demyelin-
ation (James et al. 2011).
Incomplete SCI is often followed by some
degree of spontaneous motor recovery, due in
part to reorganization of spared spinal pathway
mediated by sprouting and increased excitability
of surviving neurons. While gain of function
induced by injury may contribute to recovery, it
General Overview of Spinal Anatomy and Physiology Organization
55

may also be maladaptive when it contributes to
neuropathic pain (Ferguson et al. 2012; Tan et al.
2012), spasticity (Boulenguez and Vinay 2009;
Edgerton
and
Roy
2010),
and
autonomic
dysreﬂexia (Hou et al. 2008), disorders which
often accompany SCI. Complete injury is likely
to show little improvement of function in the
absence of therapies and is more likely to be
accompanied by maladaptive responses.
Activity-Dependent Plasticity After SCI
The functional beneﬁts of treatment based on
either
exercise/activity-
or
injury-induced
changes may be modest by themselves, but they
are noninvasive and can supplement traditional
physical therapy. Consequently, locomotor train-
ing after spinal injury (Harkema et al. 2012) is
now a widely used rehabilitative strategy to
improve function after SCI in humans. Locomotor
function on a treadmill can be recovered after
spinal transection in experimental animals if train-
ing, combined with body weight support, is pro-
vided. The success of this therapy is based in part
on the normalization of abnormal measures
induced by injury when it is followed by training.
This effect may relate to the relevance or predict-
ability of the sensory stimulation (Ferguson et al.
2012).
The
mechanisms
that
contribute
to
training-mediated recovery include levels of
neurotrophic factors or other molecules (Cote
et al. 2011) or changes in gene expression in
spinal neurons (Liu et al. 2012; Keeler et al.
2012) and also include changes in neuronal prop-
erties and sprouting where exercise/activity tend
to ameliorate some of the effects of injury (Cote
et al. 2011).
Summary
The spinal cord is contained within the vertebral
canal and access to the cord limited by the verte-
brae. The cord is somatotopically organized with
sensory information being primarily processed in
the dorsal horns and tracts, while motor informa-
tion is mostly contained within the ventral horn
and tracts. The intermediate region between the
posterior and anterior halves of the cord contains
the largest number of interneurons and is likely
the site of most computation processing although
the circuitry is still poorly understood. Targeting
the activation to certain portion of the circuitry
is complicated by both the proximity of the
elements forming the circuit and our lack of
understanding of its organization. A greater
understanding of the anatomical and computa-
tional modiﬁcations in the spinal circuitry in nor-
mal function will be essential for developing
ways of improving rehabilitative strategies after
injury.
Cross-References
▶Sensory Input to Central Pattern Generators
References
Arber S (2012) Motor circuits in action: speciﬁcation,
connectivity and function. Neuron 74:975–989
Arvanian VL, Schnell L, Lou L, Gollshani R, Hunanyan A,
Ghosh A, Pearse DD, Robinson JK, Schwab ME,
Fawcett JE (2009) Chronic spinal hemisection in rats
induces a progressive decline in transmission in
uninjured ﬁbers to motoneurons. Exp Neurol 216:
471–480
Beattie MS, Hermann GE, Rogers RC, Bresnahan JC
(2002) Cell death in models of spinal cord injury.
Prog Brain Res 137:37–47
Boulenguez P, Vinay L (2009) Strategies to restore motor
functions after spinal cord injury. Curr Opin Neurobiol
19:587–600
Chen Y, Chen L, Wang Y, Wolpaw JR, Chen XY
(2011) Operant conditioning of rat soleus H-reﬂex
oppositely affects another H-reﬂex and changes loco-
motor kinematics. J Neurosci 20:11370–11375
Conta Steencken AC, Stelzer DJ (2010) Loss of pro-
priospinal neurons after spinal contusion injury as
assessed by retrograde labeling. Neuroscience 170:
971–980
Conta AC, Stelzner DJ (2008) The propriospinal system. In
Watson C, Paxinos G, Kayalioglu G (eds) The spinal
cord. A Christopher and Dana Reeve Foundation text.
Elsevier
Cote MP, Azzam GA, Lemay MA, Zhukareva V, Houle JD
(2011) Activity –dependent increase in neurotrophic
factors is associated with an enhanced modulation of
spinal reﬂexes after spinal cord injury. J Neurotrauma
28:299–309
Cote MP, Detloff MR, Wade RE Jr, Lemay MA, Houle JD
(2012) Plasticity in ascending long propriospinal and
56
General Overview of Spinal Anatomy and Physiology Organization

descending supraspinal pathways in chronic cervical
spinal cord injured rats. Front Physiol 3:1–15
Coulton P, Bres H, Vinay L (2011) Characterization of last-
order premotor interneurons by transneuronal tracing
with rabies virus in the neonatal mouse spinal cord.
J Comp Neurol 519:3470–3487
Di Maio A, Skuba A, Himes BT, Bhagat SL, Hyun JK,
Tessler A, Bishop D, Son YJ (2011) In vivo imaging of
dorsal root regeneration: rapid immobilization and pre-
synaptic
differentiation
at
the
CNS/PNS
border.
J Neurosci 31:4569–4582
Edgerton VR, Roy RR (2010) Spasticity: a switch from
inhibition to excitation. Nat Med 16:270–271
Ertuk A, Bradke F (2013) High resolution imaging of
entire organs by 3-dimentsional imaging of solvent
cleared organs. Exp Neurol 242:57–64
Ferguson AR, Huiw JE, Crown ED, Baumbauer KM,
Hook MA, Garraway SM, Lee KH, Hoy KC, Grau
JW (2012) Maladaptive spinal plasticity opposes spinal
learning and recovery in spinal cord injury. Front
Physiol 3:1–15
Gazula VR, Roberts M, Luzzio C, Jawad AF, Kalb RG
(2004) Effects of limb exercise after spinal cord injury
on motor neuron dendrite structure. J Comp Neurol
476:130–145
Grossman KS, Giraudin A, Britz O, Zhang J, Goulding
M (2010) Genetic dissection of rhythmic motor net-
works in mice. Prog Brain Res 187:19–37
Hägglund M, Borgius L, Dougherty KJ, Kiehn O (2010)
Activation of groups of excitatory neurons in the mam-
malian spinal cord or hindbrain evokes locomotion. Nat
Neurosci 13:246–252
Harkema SJ, Hillyer J, Schmidt-Read M, Ardolino E, Sisto
SA, Behrman AL (2012) Locomotor training: as a
treatment of spinal cord injury and in the progression
of neurologic rehabilitation. Arch Phys Med Rehabil
93(9):1588–1597
Hinckley CA, Pfaff SL (2013) Imaging spinal neuron
ensembles active during locomotion with genetically
encoded calcium indictors. Ann N Y Acad Sci 1279:
71–79
Hou S, Duale H, Cameron AA, Abshire SM, Lyttle TS,
Rabhevsky AG (2008) Plasticity of lumbosacral pro-
priospinal neurons is associated with development of
autonomic dysreﬂexia after thoracic spinal cord tran-
section. J Comp Neurol 509:382–399
James ND, Bartus K, Grist J, Bennett DLH, McMahon SB,
Bradbury EJ (2011) Conduction failure following spi-
nal cord injury: functional and anatomical changes
from
acute
to
chronic
stages.
J
Neurosci
31:
18543–18555
Keeler BE, Liu G, Siegfried RN, Zhukareva V, Murray M,
Houlé JD (2012) Acute and prolonged hindlimb exer-
cise elicits different gene expression in motoneurons
than sensory neurons after spinal cord injury. Brain Res
1438:8–21
Kiehn O (2011) Development and functional organization
of spinal locomotor circuits. Curr Opin Neurobiol 21:
100–109
Lanciego JL, Wouterlood FG (2011) A half century of
experimental
neuroanatomical
tracing.
J
Chem
Neuroanat 42:157–183
Laskowski CJ, Bradke F (2013) In vivo imaging, a
dynamic imaging approach to study spinal cord regen-
eration. Exp Neurol 242:11–17
Liu G, Detloff MR, Miller KN, Santi L, Houlé JD
(2012) Exercise modulates microRNAs that affect the
PTEN/mTOR pathway in rats after spinal cord injury.
Exp Neurol 233:447–456
Pillai S, Wang Y, Wolpaw JR, Chen XY (2008) Effects of
H-reﬂex up-conditioning on GABAergic terminals on
rat soleus motoneurons. Eur J Neurosci 28:668–674
Roy RR, Harkema SJ, Edgerton VR (2012) Basic concepts
of activity –based interventions for improved recovery
of motor function after spinal cord injury. Arch Phys
Med Rehabil 93:1487–1496
Sengul G, Watson C, Tanaka I, Paxinos G (2012) Atlas of
the spinal chord of the rat, mouse, marmoset, rhesus,
and human. Academic Press/Elsevier
Stepien AE, Tripodi M, Arber S (2010) Monosynaptic
rabies virus reveals premotor network organization
and synapse speciﬁcity of cholinergic partition cells.
Neuron 68:456–472
Tan
AM,
Chakrabarty
S,
Kimura
H,
Martin
JH
(2012) Selective corticospinal tract injury in the rat
induces primary afferent ﬁber sprouting in the spinal
cord and hyperreﬂexia. J Neurosci 32:12896–12908
Watson C, Paxinos G, Kayalioglu G (2010) The spinal
cord. Elsevier
Wolpaw J, Chen XY (2009) Operant conditioning of
reﬂexes. In: Squire LR (ed) Encyclopedia of neurosci-
ence, vol 7. Academic, Oxford, pp 225–233
Ziskind-Conhaim L, MacDermott AB, Alvarez FJ, Houle
JD, Hochman S (eds) (2013) Neurons, circuitry, and
plasticity in the spinal cord and brainstem. Annals of
the New York Academy of Sciences 1279: This volume
contains a number of contemporary reviews that relate
to this chapter
Ziskind-Conhaim
L,
Fetcho
HR,
Hochman
S,
McDermott A, Stein PSG (2010) Neurons and net-
works in the spinal cord annals of the New York Acad-
emy of Sciences. An earlier volume that also contains a
number of contemporary reviews that relate to this
chapter
Further Reading
Girgis J, Merrett D, Kirkland S, Metz GAS, Verge V, Fouad
K (2007) Reaching training in rats with spinal cord
injury promotes plasticity and task speciﬁc recovery.
Brain 130:2993–3003
Ichiyama RM, Gerasimenko Y, Jindrich DL, Zhong H, Roy
RR, Edgerton VR (2008) Dose dependence of the 5-HT
agonist quipazine in facilitating spinal stepping in the rat
with epidural stimulation. Neurosci Lett 438:281–285
Shah PK, Gerasimenko Y, Shyu A, Lavrov I, Zhong H,
Roy RR, Edgerton VR (2012) Variability in step train-
ing enhances locomotor recovery after a spinal cord
injury. Eur J Neurosci 36:2054–2062
General Overview of Spinal Anatomy and Physiology Organization
57

Singh A, Balasubramanian S, Murray M, Lemay M, Houle
JD (2011) Role of spared pathways in locomotor recov-
ery after body weight supported treadmill training in
the confused rat. J Neurotrauma 28:2405–2416
Thompson AK, Pomerantz FR, Wolpaw JR (2013) Operant
conditioning of a spinal reﬂex can improve locomotion
after spinal cord injury in humans. J Neurosci 33:
2365–2375
Information Theory: Overview
Alexander Dimitrov
Department of Mathematics, Washington State
University, Vancouver, WA, USA
Definition
Information Theory started with Shannon’s semi-
nal paper “A Mathematical Theory of Communi-
cation” (Shannon 1948). Because its importance
and ﬂexibility were quickly recognized, there
were numerous attempts to apply it to diverse
ﬁeld outside of its original scope. The entries in
this section provide an overview of the current
state of Information Theory in Neuroscience.
Detailed Description
When discussing a ﬁeld, it is useful to review the
basic concepts and their properties. That has been
provided in multiple articles for Information The-
ory. To assist the reader, the entry ▶“Summary of
Information-Theoretic
Quantities”
provides
a
brief summary of the main information-theoretic
quantities. For a more thorough investigation, we
would direct interested readers to the most excel-
lent introduction to Information Theory by Cover
and Thomas (2006), now in its second edition.
Very soon after Shannon’s initial publication
(1948), a small number of manuscripts provided
the foundations for the current use of information
theory in neuroscience. MacKay and McCulloch
(1952) applied the concept of information to pro-
pose limits of the transmission capacity of a nerve
cell. This work foreshadowed future work on
what
can
be
termed
“Neural
Information
Flow” – how much information moves through
the nervous system, and the constraints that infor-
mation theory imposes on the capabilities of neu-
ral systems for communication, computation and
behavior. A second set of manuscripts, by
Attneave (1954) and Barlow (1961) discussed
information as a constraint on neural system struc-
ture and function, proposing that neural structure
in sensory systems is matched to statistical struc-
ture of the sensory environment, in a way to
optimize information transmission. This is the
main idea behind the “Structure from Informa-
tion” line of research that is still very active
today. A third thread, “Information Estimates,”
started with a forward-looking article (Miller
1955), which pointed out many potential pitfalls
of extracting information quantities from observa-
tions. Its signiﬁcance penetrated the mainstream
neuroscience research later; once information-
theoretic analysis became more widespread, the
biases noted by Miller were rediscovered, and
corrected.
Subsequent Developments
The theme that arguably has had the widest inﬂu-
ence on the neuroscience community is that of
“Neural Information Flow”. The initial works of
MacKay and McCulloch (1952) showed that neu-
rons are in principle able to relay large quantities
of information. That research also started the ﬁrst
major controversy in the ﬁeld, which still reso-
nates today: the debate about timing versus fre-
quency codes (Stein 1967). A steady stream of
articles followed, both discussing these hypothe-
sis and attempting to clarify the type of informa-
tion relayed by nerve cells (Abeles and Lass 1975;
Eagles and Purple 1974; Eckhorn and Pöpel 1974;
Harvey 1978; Norwich 1977; Poussart 1971;
Stark et al. 1969; Taylor 1975; Walloe 1970).
After the initial rise in interest, the application of
Information Theory to neuroscience was extended
to a few more systems and questions but did not
spread too broadly. This was presumably because,
despite strong theoretical advances in Information
Theory,
its
applicability
was
hampered
by
58
Information Theory: Overview

difﬁculty
in
measuring
and
interpreting
information-theoretic quantities.
The work of de Ruyter van Steveninck and
Bialek (1988) started what could be called the
modern era of information-theoretic analysis in
neuroscience, in which Information Theory is see-
ing more and more reﬁned applications. Their work
advanced the conceptual aspects of the application
of information theory to neuroscience and provided
an impetus to removing biases in information esti-
mates, discussed in detail in the entry ▶“Estimat-
ing Information-Theoretic Quantities.”
Current State
Information Theory found applications in the
study of neural processing as a theoretical and
practical system for the analysis of communicated
signals. A variety of information-theoretic quanti-
ties are in use in Neuroscience. The entry
▶“Applications of Information Theory to Analy-
sis of Neural Data” provides a general overview of
current applications of Information Theory, as an
analysis tool of neural information ﬂow.
The structure of neural activity often intro-
duces challenges that have been of marginal inter-
est to the engineering community. The entry
▶“Metric Space Analysis of Neural Information
Flow” exempliﬁes one such case, metrization. In a
metric-space
approach
to
analyzing
neural
response, the data is reduced from a set of com-
plicated objects, spike trains, to a much simpler
object, the matrix of distances between the spike
trains. When applied to neural information, this
matrix is used to estimate information theory
quantities for the corresponding spike train data.
The tools developed in that direction combine
metric properties of spike trains with their infor-
mation transmission function.
Several entries discuss speciﬁc applications in
neuroscience stemming from more recent devel-
opments in Information Theory. ▶“Directed
Information Flow and Causality in Neural Sys-
tems” discusses developments based on the natu-
ral ideas of information ﬂow in a causal direction.
Surprisingly, this idea has taken quite some time
to develop in the communication literature due to
various technical difﬁculties. The works origi-
nates form the ideas of Granger (1969) on causal
interactions. The information-theoretic perspec-
tive of (Massey 1990; Massey and Massey 2005)
removed Granger’s linearity assumptions.
And lastly, applications in neuroscience are
also pushing the development of new tools in
Information Theory. One of the examples pre-
sented here, ▶“Information Measures of Redun-
dancy
and
Synergy
in
Neural
Activity,”
summarizes the progress made in that direction,
for which the original deﬁnitions provided by
Shannon proved inadequate.
In conclusion, Information Theory is thriving
in the neuroscience community, and the long
efforts are bearing fruit, as diverse research ques-
tions are being approached with more elaborate
and reﬁned tools. As demonstrated by several
recent thematic journal issues (Dimitrov et al.
2011; Milenkovic et al. 2010), Information The-
ory is ﬁrmly integrated in the fabric of neurosci-
ence research, and a progressively wider range of
biological research in general, and will continue to
play an important role in these disciplines. Con-
versely, neuroscience is starting to serve as a
driver for further research in Information Theory,
opening interesting new directions of inquiry.
Cross-References
▶Applications of Information Theory to Analysis
of Neural Data
▶Directed Information Flow and Causality in
Neural Systems
▶Estimating Information-Theoretic Quantities
▶Information Measures of Redundancy and
Synergy in Neural Activity
▶Metric Space Analysis of Neural Information
Flow
▶Summary of Information-Theoretic Quantities
References
Abeles M, Lass Y (1975) Transmission of information by the
axon: II. The channel capacity. Biol Cybern 19:121–125
Attneave F (1954) Some information aspects of visual
perception. Psychol Rev 61:183–193
Information Theory: Overview
59

Barlow HB (1961) Possible principles underlying the
transformation of sensory messages. In: Rosenblith
WA (ed) Sensory communications. MIT Press, Cam-
bridge, MA
Cover T, Thomas J (2006) Elements of information theory.
In: Wiley series in communication and signal pro-
cessing, 2nd edn. Wiley, Hoboken
de Ruyter van Steveninck R, Bialek W (1988) Real-time
performance of a movement- sensitive neuron in the
blowﬂy visual system: coding and information transfer
in short spike sequences. Proc Royal Soc Ser B Biol Sci
234(1277):379–414
Dimitrov AG, Lazar AA, Victor JD (2011) Information
theory in neuroscience. J Comput Neurosci 30(1):1–5
Eagles JP, Purple RL (1974) Afferent ﬁbers with multiple
encoding sites. Brain Res 77(2):187–193
Eckhorn R, Pöpel B (1974) Rigorous and extended appli-
cation of information theory to the afferent visual sys-
tem of the cat. I. Basic concepts. Biol Cybern 16:
191–200
Granger C (1969) Investigating causal relations by econo-
metric
models
and
cross-spectral
methods.
Econometrica 37(3):424–438
Harvey R (1978) Patterns of output ﬁring generated by a
many-input neuronal model for different model param-
eters and patterns of synaptic drive. Brain Res 150(2):
259–276
MacKay DM, McCulloch WS (1952) The limiting infor-
mation capacity of a neuronal link. Bull Math Biophys
14:127–135
Massey J (1990) Causality, feedback and directed informa-
tion. In: Proceedings of the international symposium on
information theory applications (ISITA-90). Yokohama
National University, Yokohama, pp 303–305
Massey J, Massey P (2005) Conservation of mutual and
directed information. In: Proceedings of the interna-
tional symposium on information theory (ISIT 2005).
IEEE Publishing, New York NY, pp 157–158
Milenkovic O, Alterovitz G, Battail G, Coleman TP,
Hagenauer J, Meyn SP, Price N, Ramoni MF,
Shmulevich I, Szpankowski W (2010) Introduction to
the special issue on information theory in molecular
biology and neuroscience. IEEE Trans Inf Theory
56(2):649–652
Miller GA (1955) Note on the bias of information esti-
mates. In: Henry Q (ed) Information theory in psychol-
ogy: problems and methods, vol vol II-B. Free Press,
Glenco, IL, pp 95–100
Norwich K (1977) On the information received by sensory
receptors. Bull Math Biol 39:453–461
Poussart DJM (1971) Membrane current noise in lobster
axon under voltage clamp. Biophys J 11(2):211–234
Shannon CE (1948) A mathematical theory of communi-
cation. Bell Syst Tech J 27:623–656
Stark L, Negrete-Martinze J, Yankelevich G, Theodoridis
G (1969) Experiments on information coding in nerve
impulse trains. Math Biosci 4(3–4):451–485
Stein RB (1967) The information capacity of nerve cells
using a frequency code. Biophys J 7(6):797–826
Taylor RC (1975) Integration in the crayﬁsh antennal
neuropile: topographic representation and multiple-
channel coding of mechanoreceptive submodalities.
Dev Neurobiol 6(5):475–499
Walloe L (1970) On the transmission of information
through sensory neurons. Biophys J 10(8):745–763
Invertebrate Pattern
Generation: Overview
Farzan Nadim
Federated Department of Biological Sciences,
New Jersey Institute of Technology/Rutgers
University, Newark, NJ, USA
Department of Mathematical Sciences, New
Jersey Institute of Technology, Newark, NJ, USA
Detailed Description
Central pattern generators (CPGs) are networks of
neurons in the central nervous system (CNS) that
produce patterned activity, usually as coherent
oscillations, in the absence of external timing
cues. CPGs provide timing input to motor neurons
whose discharge dictates movements of muscles
that control rhythmic behavior such as respiration
or locomotion (Marder and Calabrese 1996). The
long-held debate between scientists who believed
half a century ago that rhythmic motor activity is
generated by reﬂex chains and those who held the
more radical view of centrally generated rhythms
was resolved conclusively, in favor of the latter
group, by demonstrating that neural networks
generating rhythmic motor activity can do so in
the isolated nervous system, in the absence of the
body and therefore sensory feedback. In the early
1960s, these “ﬁctive” motor patterns were ﬁrst
demonstrated to govern rhythmic activation in
two invertebrate model systems: the movement
of ﬂight wings in locusts and the beating of swim-
merets in crayﬁsh (Wilson 1961; Ikeda and
Wiersma 1964). The demonstration of ﬁctive
motor activity led to the development of in vitro
preparations in search of the underlying CPG
circuits. Although CPG networks have been
60
Invertebrate Pattern Generation: Overview

traditionally described in the context of control-
ling motor activity, a more contemporary view-
point of CPGs includes networks that subserve
brain oscillations connected to sensory, cognitive,
and memory tasks (Yuste et al. 2005).
CPGs have historically led systems neurosci-
ence in the understanding of neural circuit inter-
actions, partly because of the ease of identiﬁcation
of neurons and networks whose activity correlates
with a rhythmic motor activity. The identiﬁcation
of neural circuits has been more successful in
invertebrates where the number of neurons
involved in neural processing is lower, sometimes
by orders of magnitude, and the ability of identi-
fying synaptic connections among neurons is
facilitated by dual recordings. Vertebrate and
especially mammalian neural circuit analysis
was, until recently, performed with cruder tech-
niques such as lesions, but in the past decade or so,
genetic tools and the identiﬁcation of molecular
markers have allowed for more precise circuit
analysis in the large networks of vertebrate sys-
tems (Han 2012; Arrenberg and Driever 2013).
However, neural circuits have been identiﬁed only
in a few vertebrate model systems and simple
behaviors (Issa et al. 2011; Cangiano and Grillner
2005). The knowledge of the neural circuits and
the ability to record functionally identiﬁed neurons
in invertebrates have allowed for the in-depth anal-
ysis of the mechanisms underlying circuit dynam-
ics and plasticity (Marder et al. 2005) as well as a
rigorous
description
of
the
computations
performed by the neural circuits (Selverston 2010).
The computational description of pattern-
generating circuits evolved in parallel with the
in vitro experimental studies of cellular and syn-
aptic mechanisms. The complexities of circuit
analysis of invertebrate CPGs have led to numer-
ous modeling studies, some of which have been
inﬂuential in shaping our conceptual understand-
ing of both single-neuron and network operations.
For example, the computational description of
bursting oscillations, led by models of inverte-
brate CPG neurons (Plant and Kim 1976), paved
the way for the complete mathematical analysis of
bursting mechanisms in neurons and other excit-
able cells (Rinzel 1987; Rinzel and Lee 1987).
This
section
of
the
Encyclopedia
of
Computational Neuroscience provides an over-
view of the contributions of invertebrate pattern
generators
in
the
context
of
computational
models.
Cross-References
▶Automated Parameter Search in Small Network
Central Pattern Generators
▶Bifurcations Dynamics of Single Neurons and
Small Networks
▶Bursting in Neurons and Small Networks
▶Gap Junctions in Small Networks
▶Neuromodulation in Small Networks
▶Sensory Input to Central Pattern Generators
▶Short-Term Synaptic Plasticity in Central
Pattern Generators
▶Stability and Homeostasis in Small Network
Central Pattern Generators
References
Arrenberg AB, Driever W (2013) Integrating anatomy and
function for zebraﬁsh circuit analysis. Front Neural
Circuits 7:74
Cangiano L, Grillner S (2005) Mechanisms of rhythm
generation in a spinal locomotor network deprived of
crossed connections: the lamprey hemicord. J Neurosci
25(4):923–935
Han X (2012) In vivo application of optogenetics for neural
circuit analysis. ACS Chem Neurosci 3(8):577–584
Ikeda K, Wiersma CA (1964) Autogenic rhythmicity in the
abdominal ganglia of the crayﬁsh: the control of swim-
meret
movements.
Comp
Biochem
Physiol
12:
107–115
Issa FA, O’Brien G, Kettunen P, Sagasti A, Glanzman DL,
Papazian DM (2011) Neural circuit activity in freely
behaving
zebraﬁsh
(Danio
rerio).
J
Exp
Biol
214(Pt 6):1028–1038
Marder E, Calabrese RL (1996) Principles of rhythmic
motor pattern generation. Physiol Rev 76(3):687–717
Marder E, Bucher D, Schulz DJ, Taylor AL (2005) Inver-
tebrate central pattern generation moves along. Curr
Biol 15(17):R685–R699
Plant RE, Kim M (1976) Mathematical description of a
bursting pacemaker neuron by a modiﬁcation of the
Hodgkin-Huxley equations. Biophys J 16(3):227–244
Rinzel J (1987) A formal classiﬁcation of bursting mech-
anisms in excitable systems. Mathematical topics in
populations biology, morphogenesis. Lecture notes in
biomathematics, vol 71. Springer, Berlin
Rinzel J, Lee YS (1987) Dissection of a model for neuronal
parabolic bursting. J Math Biol 25(6):653–675
Invertebrate Pattern Generation: Overview
61

Selverston AI (2010) Invertebrate central pattern generator
circuits. Philos Trans R Soc Lond Ser B Biol Sci
365(1551):2329–2345
Wilson DM (1961) The central nervous control of ﬂight in
a locust. J Exp Biol 38:471–490
Yuste R, MacLean JN, Smith J, Lansner A (2005) The
cortex as a central pattern generator. Nat Rev Neurosci
6(6):477–483
Invertebrate Sensory Systems:
Overview
Fabrizio Gabbiani
Department of Neuroscience, Baylor College of
Medicine, Houston, TX, USA
Definition
Invertebrate sensory systems specialize in gather-
ing information from the surroundings allowing
animals to locomote and behave appropriately
given the current environmental conditions.
Detailed Description
Invertebrate neurobiology has long been at the
forefront of computational neuroscience, starting
with the mathematical model of the biophysical
basis of the action potential by Hodgkin and
Huxley in the squid giant axon almost 70 years
ago (Hodgkin and Huxley 1952). Many inverte-
brate systems are highly suitable for detailed
modeling because of their relatively compact
size and the fact that their neurons can often be
uniquely identiﬁed. This feature allows the for-
mulation of precise descriptions of their behav-
iors
and
simpliﬁes
the
interpretation
of
experimental results. Yet, as Hodgkin and
Huxley’s seminal analysis of action potential
mechanisms has proved, invertebrate models
possess characteristics similar to those of verte-
brates, and the computational principles derived
from either type of nervous systems have been
found universally applicable, even if details of
mechanistic implementations differ.
This section of the Encyclopedia focuses on
the
sensory
systems
of
invertebrates,
complementing the section on their motor sys-
tems. It presents an overview of computational
modeling emphasizing salient topics in the ﬁeld.
While the coverage is far from exhaustive, it is our
hope that these entries will provide the reader an
overview and an entry point to the rich literature
covering the modeling of invertebrate sensory
systems and related topics.
Among the different sensory systems, vision
has arguably been the most intensely investigated
in invertebrates and vertebrates alike. The entry
on ▶“Phototransduction Biophysics” summa-
rizes our understanding of the mechanisms by
which invertebrate photoreceptors translate light
signals into membrane potential changes. Most of
the work on this topic has been carried out in ﬂies
and particularly the genetically tractable fruit ﬂy,
Drosophila melanogaster. The biochemical cas-
cade mediating phototransduction has been stud-
ied in great detail and has recently led to a better
understanding of the implications for neural cod-
ing. Another classical topic of insect vision has
been the study of the mechanisms underlying
directionally selective motion detection. This
work originated in beetles and was later pursued
in ﬂies, leading to the correlation model of
Hassenstein and Reichardt (1956). The neural
circuits implementing this model, which is closely
related to the motion-energy model of mammalian
motion detection (van Santen and Sperling 1985),
have long proven tricky to investigate due to
difﬁculties in tracing the anatomical and physio-
logical connections between the neurons that per-
form the computations of the correlation model.
Progress has been made by combining anatom-
ical, electrophysiological, imaging, and genetic
techniques in Drosophila that are summarized in
the entry on ▶“Visual Motion Detection in Dro-
sophila.”
Although
Drosophila
has
recently
helped better understand the circuitry underlying
motion detection, a large body of work on the
topic was carried out in bigger ﬂies, leading to
detailed biophysical models of a class of neurons
involved in motion detection. These models are
introduced in the entry on ▶“Fly Lobula Plate
Tangential Cells (LPTCs), Models of.” Local
62
Invertebrate Sensory Systems: Overview

motion detection is only the ﬁrst step in pro-
cessing visual information generated by an animal
moving in its environment. Such information,
often called “optic ﬂow,” is particularly important
for ﬂying animals. The strategies and constraints
on the processing of such information are summa-
rized in the two entries on ▶“Optic Flow Pro-
cessing”
and
▶“Visual
Processing
in
Free
Flight”.
Another function of the visual system of criti-
cal importance to animals is predator evasion and
collision avoidance. Our understanding of the
neural mechanisms of collision avoidance has
rapidly progressed over the past decades in a
variety of invertebrate model systems, including
locusts, crabs, and fruit ﬂies. Similar collision
avoidance mechanisms have been documented in
vertebrate systems such as goldﬁsh, zebraﬁsh,
pigeons, and mice. The entry on ▶“Collision
Avoidance Models, Visually Guided” describes
our understanding of those systems. Animal nav-
igation and migration relies on an internal repre-
sentation
of
the
external
world
allowing
orientation over long distances. In insects, partic-
ularly in monarch butterﬂies, ants, and locusts, the
detection of polarized patterns of light in the sky
provides an internal compass for navigation. The
detection and processing of polarized light pat-
terns leads to a sort of “cognitive map” in the
central nervous system akin to those studied in
the rodent hippocampus. The entry on ▶“Polari-
zation Vision” provides a summary of our under-
standing of this fascinating and exotic sensory
modality.
Olfaction is a prominent and important sense in
invertebrates, particularly in insects. The mecha-
nisms of insect olfaction and their close relation to
vertebrate
olfaction
have
been
investigated
intensely, culminating in a detailed understanding
in several organisms, including moths, locusts,
and fruit ﬂies. Examples of models of olfactory
coding
are
presented
in
the
entry
entitled
▶“Insect Olfaction: A Model System for Neural
Circuit Modeling”. Another primary sense, audi-
tion, is used by animals to locate preys or preda-
tors and for communication with conspeciﬁcs.
Because of their small size, invertebrates and
insects such as crickets and locusts, which are
among the best-studied auditory model systems,
face physical constraints quite different from
those applying to vertebrates. Their study has led
to a wealth of information on how auditory infor-
mation is processed, both at the level of the audi-
tory periphery and in the brain. These results are
summarized in the entry on ▶“Auditory Pro-
cessing in Insects”.
Wind and air current sensing is carried out by a
specialized sensory system called the cercal sys-
tem in crickets and other orthopteran insects. This
system provides a fascinating example of how
information is processed by populations of sen-
sory neurons, as summarized in the entry entitled
▶“Cercal System”. More broadly, the entry on
▶“Computation with Population Codes” explains
the role such neural codes play in a variety of
other animals. The leech has proven to be one of
the best models to study these topics because of
the compact size of its nervous system, well-
understood natural behaviors, and the applicabil-
ity of large-scale imaging of neuronal activity
using voltage-sensitive dyes and other electro-
physiological techniques. Finally, invertebrate
somatosensation is introduced in the entry on
▶“Tactile Sensing in Insects”.
Adaptation is a fundamental property of neu-
ronal responses observed throughout sensory sys-
tems.
The
entry
entitled
▶“Biophysics
of
Adaptation in a Computational Model of the
Leech T Neuron” details the ionic mechanisms
of adaptation in one type of sensory neuron and
its consequences for the processing of sensory
stimuli.
In addition to the above-mentioned entries, the
section includes one entry – entitled ▶“Nitric
Oxide Neuromodulation” – on a somewhat exotic
and fascinating volume neuromodulator: nitric
oxide. The computational properties of nitric
oxide neuromodulation have been investigated in
detail in several invertebrate systems, leading to
models of its role in sensory processing and learn-
ing. Similar regulation of neuronal circuits by
volume neurotransmission has been described in
vertebrates as well (e.g., Oláh et al. 2009).
The last topic covered in this section is the
efﬁcient representation of neural information in
the
entry
entitled
▶“Sensory
Coding,
Invertebrate Sensory Systems: Overview
63

Efﬁciency”. The possibility that sensory neural
codes may be efﬁcient has been ﬁrst raised in the
1960s (Barlow 1961). It has since then been inves-
tigated in several systems, including the visual
system of invertebrates. The entry includes back-
ground information and a summary of results on
efﬁcient coding in these systems.
Cross-References
▶Auditory Processing in Insects
▶Biophysics of Adaptation in a Computational
Model of the Leech T Neuron
▶Cercal System
▶Collision Avoidance Models, Visually Guided
▶Computation with Population Codes
▶Fly Lobula Plate Tangential Cells (LPTCs),
Models of
▶Hodgkin-Huxley Model
▶Insect Olfaction: A Model System for Neural
Circuit Modeling
▶Nitric Oxide Neuromodulation
▶Optic Flow Processing
▶Phototransduction Biophysics
▶Polarization Vision
▶Spike-Frequency Adaptation
▶Tactile Sensing in Insects
▶Visual Motion Detection in Drosophila
▶Visual Processing in Free Flight
References
Barlow HB (1961) Chapter 13, Possible principles under-
lying the transformation of sensory messages. In:
Rosenblith W (ed) Sensory communication. MIT
Press, Cambridge, pp 217–234
Hassenstein B, Reichardt W (1956) Systemtheoretische
Analyse
der
Zeit-Reihenfolgen-
und
Vorzeiche-
nauswertung
bei
der
Bewegungsperzeption
des
Rüsselkäfers
Chlorophanus.
Z
Naturforsch
11b:513–524
Hodgkin AL, Huxley AF (1952) A quantitative description
of membrane current and its application to conduction
and excitation in nerve. J Physiol 117:500–544
Oláh S, Füle M, Komlósi G, Varga C, Báldi R, Barzó P,
Tamás G (2009) Regulation of cortical microcircuits by
unitary GABA-mediated volume transmission. Nature
461:1278–1281
van Santen JPH, Sperling G (1985) Elaborated Reichardt
detectors. J Opt Soc Am A 2:300–321
Learning Rules: Overview
Klaus Obermayer1 and Joaquín J. Torres2
1Neural Information Processing Group, Institute
of Software Engineering and Theoretical
Computer Science, Technische Universität Berlin,
Berlin, Germany
2Institute “Carlos I” for Theoretical and
Computational Physics and Department of
Electromagnetism and Matter Physics, Facultad
de Ciencias, Universidad de Granada, Granada,
Spain
Definition
The title of this entry, namely, “Learning Rules,”
refers to several type of methods and procedures
that have been used in different neuroscience
research areas to quantify how previous experi-
ences, training, and stimulations protocols affect
brain response and behavior in different animals
and at different levels of description. The concept
assumes the such previous experiences produce
changes or plasticity in the brain (i.e., the learning
rule) that affect its posterior behavior, e.g., under
the action of related stimuli (the retrieval of what
is learned). Such “Learning Rules” concept has
been also extended to neural networks modeling
and artiﬁcial intelligence research, providing pre-
cise mathematical deﬁnitions for different learn-
ing rules, a fact that has helped to gain a better
understanding of their consequences.
Detailed Description
Learning is the most important procedure by
which different stimuli in the environment alter
the behavior in animals and humans, and today its
neurobiological
base
is
widely
accepted
(Thompson 1986). In fact, neurobiological and
computational studies have stressed the signiﬁ-
cance of synaptic plasticity in the learning process
(Black et al. 1990; Fusi 2017): Experience and
training leads to modiﬁcations of synapses, which
in turn lead to changes in neuronal ﬁring patterns,
64
Learning Rules: Overview

causing changes in behavior. A full understanding
of the processes underlying learning, therefore,
requires an understanding on the neural as well
as on the systemic level.
In the behavioral literature, learning is studied
by means of different experimental procedures in
which animals or humans are exposed to sensory
stimuli
and
interact
with
the
environment
(Mackintosh 1974). The paradigms which have
been used gave rise to a categorization into asso-
ciative and nonassociative learning. In non-
associative learning an animal is repeatedly
exposed to a single type of stimulus, and learning
is described by the strength of its behavioral impact
leading, for example, to the phenomena of habitu-
ation and sensitization (Thompson and Spencer
1966; Rankin et al. 2009). In associative learning
animals learn predictive relationships in the pres-
ence of a reinforcement signal (Pearce and Bouton
2001). Historically, classical conditioning (where
reinforcements are delivered independent of any
action taken) is differentiated from operant condi-
tioning (where reinforcements depend on the action
and where behavior is associated with its outcome).
In the neural network literature (see, e.g.,
Haykin 2009), learning is often classiﬁedaccording
to what kind of information is available in a partic-
ular learning scenario. In the so-called supervised
learning, an agent (e.g., a neural network) forms
associations between two sets of events with the
goal to predict one by the other. Learning is based
on a given set of correct pairings. Pairings between
events are also formed in the so-called reinforce-
ment learning; however, evaluative feedback about
network performance is only provided in the form
of reinforcements, for example, reward or punish-
ment signals. Correct pairings imply high rewards;
hence, learning is driven by maximizing returns.
Reinforcement learning bears similarities with the
abovementioned associative learning paradigms,
and algorithms which were originally developed
by the neural network community are now widely
applied to quantify associative learning in animals
and humans. In the so-called unsupervised learn-
ing, responses of an agent are modiﬁed when stim-
uli are presented but in a nonassociative way. In an
artiﬁcial agent setting, supervised learning is often
applied for solving pattern recognition problems,
and reinforcement learning is used to learn action
sequences. Unsupervised learning, on the other
hand, is applied to learn internal representations
of the outside world. Statistical regularities are
extracted and are used to build representations of
stimuli and actions which are better suited for rec-
ognition,
planning,
memorization,
decision-
making, and other cognitive tasks.
In many areas of neuroscience where “real”
neurons and networks are studied, learning is
linked to changes of neuronal and synaptic proper-
ties. Activity-dependent synaptic plasticity is
widely believed to be the most important feature
underlying learning. It also plays an important role
during neural development, where the interplay of
neural activity with the so-called intrinsic processes
shapes the circuitry and the neural response prop-
erties. Learning at the neuronal level is character-
ized by how the activation history of pre- and
postsynaptic neurons relates to the efﬁcacy of the
synaptic connection between them. Different stim-
ulation protocols of neurons in (mostly) in vitro
preparations uncovered a wealth of phenomena
associated to learning processes, the most promi-
nent ones being long-term potentiation, long-term
depression, or spike-timing-dependent plasticity.
“Learning Rules,” i.e., the title of this entry,
refers to the quantiﬁcation of the effects of expe-
riences, training, and stimulation on the brain and
behavior on all abovementioned levels of descrip-
tion. Learning Rules inserted into computational
models then help to explore the consequences of
the observed plasticity during different learning
processes. Thus, one can explore, for example,
how activity-dependent synaptic plasticity inter-
acts with network responses which eventually
induce behavior, how a network can adapt its
parameters such that a desired function can be
performed, or how interindividual differences in
learning behavior can be related to interindividual
differences in their objectives. Thus, learning
rules serve as a computational tool to link
experience-induced changes observed on a sys-
tem level to the mechanisms operating on the level
of neurons and networks and vice versa.
At the core of this entry are entries for activity-
dependent learning rules, which quantify how pre-
and postsynaptic activity changes the strength of a
Learning Rules: Overview
65

synapse
(see
entries
▶“Hebbian
Learning,”
▶“Anti-Hebbian
Learning,”
▶“Spike-Timing
Dependent
Plasticity,
Learning
Rules,”
and
▶“Tempotron Learning”). Here, the learning rules
are formulated in a more abstract setting, and the
biophysical foundations are addressed in entry
▶“Synaptic Dynamics: Overview” (cf. ▶“Long-
Term Plasticity, Biophysical Models,” ▶“Short-
Term
Plasticity,
Biophysical
Models,”
and
▶“Spike-Timing Dependent Plasticity, Learning
Rules”). The effects of learning rules in terms of
network dynamics and computation
are then
addressed in a neural network setting (see entries
▶“Boltzmann Machine,” ▶“Hopﬁeld Network,”
▶“Perceptron Learning,” and ▶“Slow Feature
Analysis”) covering supervised and unsupervised
learning paradigms. They are also addressed in a
neural modeling setting, where the computational
models are used to understand some of the mecha-
nisms underlying neural development (see entries
▶“Cortical Maps, Activity-Dependent Develop-
ment” and ▶“Cortical Maps, Intrinsic Processes”).
The entry is complemented by one entry on rein-
forcement learning (see ▶“Reward-Based Learning,
Model-Based and Model-Free”), which summarizes
current computational approaches for quantifying
behavior for reward-based, associative learning par-
adigms. Possible neural implementations of rein-
forcement
learning
are
discussed
in
entry
▶“Cortex: Overview”(cf.▶“Reinforcement Learn-
ing in Cortical Networks”).
Cross-References
▶Anti-Hebbian Learning
▶Boltzmann Machine
▶Cortex: Overview
▶Cortical
Maps,
Activity-Dependent
Development
▶Cortical Maps, Intrinsic Processes
▶Hebbian Learning
▶Hopﬁeld Network
▶Long-Term Plasticity, Biophysical Models
▶Perceptron Learning
▶Reinforcement Learning in Cortical Networks
▶Reward-Based Learning, Model-Based and
Model-Free
▶Short-Term Plasticity, Biophysical Models
▶Slow Feature Analysis
▶Spike-Timing Dependent Plasticity (STDP),
Biophysical Models
▶Spike-Timing Dependent Plasticity, Learning
Rules
▶Synaptic Dynamics: Overview
▶Tempotron Learning
References
Black JE, Isaacs KR, Anderson BJ, Alcantara AA, Gree-
nough WT (1990) Learning causes synaptogenesis,
whereas motor activity causes angiogenesis, in cerebel-
lar cortex of adult rats. Proc Natl Acad Sci U S A
87:5568–5572
Fusi S (2017) Computational models of long term plastic-
ity and memory. arXiv preprint arXiv:1706.04946
Haykin S (2009) Neural networks and learning machines,
3rd edn. Pearson Prentice Hall, Upper Saddle River
Mackintosh NJ (1974) The psychology of animal learning.
Academic, Oxford
Pearce JM, Bouton ME (2001) Theories of associative
learning in animals. Annu Rev Psychol 52:111–139
Rankin HA et al (2009) Habituation revisited: an updated
and revised description of the behavioral characteristics
of habituation. Neurobiol Learn Mem 92(2):135–138
Thompson RF (1986) The neurobiology of learning and
memory. Science 233(4767):941–947
Thompson RF, Spencer WA (1966) Habituation: a model
phenomenon for the study of neuronal substrates of
behavior. Psychol Rev 73(1):16–43
LFP Analysis: Overview
Alain Destexhe1 and Joshua A. Goldberg2
1Unit of Neuroscience Information and
Complexity (UNIC), Paris-Saclay University,
Institute of Neuroscience (NeuroPSI), Centre
National de la Recherche Scientiﬁque (CNRS),
Gif-sur-Yvette, France
2Department of Medical Neurobiology, Institute
for Medical Research Israel-Canada, Faculty of
Medicine, The Hebrew University of Jerusalem,
Jerusalem, Israel
Detailed Description
Local ﬁeld potentials (LFPs) are low-frequency
electrical potentials recorded with micro- or
66
LFP Analysis: Overview

macro-electrodes throughout the brain. The upper
frequency cutoff of the LFP is often considered to
be around 100 Hz but may be as high as 500 Hz in
some studies. The normal amplitude of the LFP
can range from a few microvolts (e.g., in the basal
ganglia) (Goldberg et al. 2004) to hundreds of
microvolts in the cortex. Perhaps the earliest
recording of an LFP may be attributed to
Renshaw, Forbes, and Morison in 1940, who
described these potentials in the cortex and the
hippocampus of a cat under various anesthetics
(Renshaw et al. 1940). Extensive electrophysio-
logical studies have formed the current view that
cortical LFPs result from synaptic activity (Eccles
1951; Creutzfeldt et al. 1966; Elul 1971; Klee and
Rall 1977; Mitzdorf 1985; Bedard et al. 2004).
Correspondingly, the LFP ﬂuctuations tend to
have a strict phase relationship to cortical dis-
charge: negative deﬂections in the LFP coincide
with increases in the instantaneous ﬁring rates of
cortical neurons in both superﬁcial and deep
layers (Lass 1968; Gray and Singer 1989; Murthy
and Fetz 1996a; Donoghue et al. 1998; Destexhe
et al. 1999). In addition, these ﬂuctuations are
highly correlated across distances of several mil-
limeters in the cortex (Eckhorn and Obermueller
1993; Sanes and Donoghue 1993; Murthy and
Fetz 1996b; Contreras et al. 1997; Bullock 1999;
Destexhe et al. 1999), indicating that LFPs are not
always strictly local.
Theoretical attempts to account for the LFPs
and model their generation date back to Lorente
de Nó (1947) and include the seminal works of
Ulla Mitzdorf (1985), who implemented the tech-
nique of current source density (CSD), and
Wilfrid Rall, who calculated the potentials gener-
ated by various spatial organization of electrical
dipoles (Klee and Rall 1977; Goldberg et al.
2004). These theoretical accounts relied on the
large-scale repetitive columnar structure of the
cortical circuitry to generate large potentials due
to the summed contribution of many aligned neu-
ronal dipoles (Elul 1971; Klee and Rall 1977;
Abeles 1982; Mitzdorf 1985; Eggermont and
Smith 1995).
This “LFP analysis” section, which has contri-
butions from several leading authorities, covers
the following topics: (a) methods for measuring
the LFP; (b) modern accounts of how LFPs are
generated; (c) the relationship between the LFP
and other common measures of collective brain
activity such as the EEG, the MEG, and the fMRI,
as well as its relationship to and possible inﬂuence
on single-cell membrane potentials; (d) the power
spectral structure of the LFP, which often displays
a 1/f structure, or marked oscillation peaks under
various physiological and pathophysiological
conditions (e) LFP in the context of particular
brain functions such as vision and olfaction; and
(f) current modeling methods of the LFP. These
various topics are overviewed below.
Neurophysiological Basis of LFPs
A ﬁrst important aspect of the LFP is the relation
between this signal and other well-known signals
of the brain. In the entry ▶“Local Field Potential,
Relationship to BOLD Signal”, Nikos Logothetis
and Stefano Panzeri review the relation between
the LFP signals and the BOLD (blood oxygen
level dependent) signal, as recorded by MRI tech-
niques. The entry also discusses multiunit activity
in relation with the BOLD signal.
A similar large-scale approach is followed by
Stephanie Jones in ▶“Local Field Potential, Rela-
tionship to Electroencephalogram (EEG) and
Magnetoencephalogram
(MEG)”.
This
entry
reviews the biophysical bases of the genesis of
EEG and MEG signals and how such signals
relate to the LFP. The “inverse problem” of esti-
mating neuronal sources from EEG and MEG
signals is also reviewed.
The relationship to unit activity is further
described in detail in ▶“Local Field Potential,
Relationship to Unit Activity” by Bartosz Tele-
ńczuk and Alain Destexhe. This entry reviews the
relation between extracellularly recorded unit
activity and LFP, as a function of the frequency
of the different rhythmical activities found in
LFPs. It is shown that unit activity correlates
with the high-frequency (>200 Hz) components
of the LFP and, to a lesser extent, with the low-
frequency components (<10 Hz), whereas the
intermediate-frequency bands have a rather weak
correlation with unit activity.
LFP Analysis: Overview
67

In the entry ▶“Local Field Potential: Relation-
ship to Membrane Synaptic Potentials”, Aryeh
Taub, Ilan Lampl, and Michael Okun go a bit
more deeper in the neuron and review the relation
between the intracellularly recorded membrane
potential and the LFP signal. The entry considers
different types of brain activity, such as up-down
states, and discusses the role of inhibition.
In the entry “Local Field Potentials and
Ephaptic Coupling”, Costas Anastassiou dis-
cusses the hypothesis that the LFP is not merely
an epiphenomenon of electrical brain activity but
may actually serve as a global slow signal that can
couple to other neural elements (e.g., axons, syn-
apses). This ephaptic coupling results from the
fact that the membrane voltages in these elements
can be altered by ﬂuctuations in the LFP simply
because they are the difference between the intra-
cellular potential and the extracellular (local ﬁeld)
potential.
Finally, in the entry ▶“Local Field Potentials:
LFP”, Alain Destexhe and Claude Bedard review
general properties of LFPs, such as their spatial
coherence, namely, how LFPs recorded by neigh-
boring electrodes relate to each other. They also
review temporal (spectral) properties of LFPs,
such as their typical 1/f structure. The entry also
overviews
different
approaches
for
modeling LFPs.
Oscillatory Properties of LFPs
A fundamental property of LFP signals is their
propensity to display oscillations. In the entry
▶“Local Field Potential and Deep Brain Stimu-
lation (DBS)”, Manuela Rosa, Sara Marceglia,
Sergio Barbieri, and Alberto Priori review the
use of LFPs in the treatment of DBS in humans.
It describes how DBS provides LFP recordings in
humans, how they are related to oscillatory activ-
ity, and how DBS may interfere with these
(sometimes pathological) oscillations.
Pathological oscillations are further investi-
gated in the entry ▶“Local Field Potential and
Movement Disorders” by Annaelle Devergnas
and Thomas Wichmann. They review the use of
LFP recordings in movement disorders and show
that LFPs display various types of oscillations, at
different frequencies, for different pathologies
associated to movement disorders.
In the entry ▶“Local Field Potentials in Olfac-
tion”, Leslie Kay review the LFP signals recorded
in the olfactory system (olfactory bulb and pyri-
form cortex). Their review emphasizes the emer-
gence of beta and gamma oscillations in olfactory
structures.
The occurrence of LFP oscillations in the
visual
system
is
investigated
in
the
entry
▶“Local Field Potential in the Visual System”
by Gregor Rainer. This entry reviews the occur-
rence of LFPs in visual cortex, with an emphasis
on visually evoked potentials, and visually
evoked oscillations in the gamma frequency band.
Finally, in the entry ▶“Local Field Potential,
Synchrony
of”,
Ariana
Frederick,
Jonathan
Bourget-Murray,
and
Richard
Courtemanche
review how LFPs can reveal the presence of net-
work synchrony. Synchrony is here deﬁned as the
synchronization
between
different
networks,
which indicates a possible interaction between
them, for example, using oscillations at different
frequencies.
Modeling the Spectral Structure of LFPs
The modeling of LFP signals is ﬁrst considered by
Biyu Jade He in the entry ▶“Electrocorticogram
(ECoG)”. This entry reviews models of the ECoG
and how this surface signal relates to the LFP
recorded in depth. The entry also discusses the
spectral structure of these signals and their power-
law frequency scaling.
The modeling of LFPs is further developed in
the entry ▶“Local Field Potentials: Interaction
with the Extracellular Medium” by Claude
Bedard and Alain Destexhe. This entry reviews
models of LFPs by staying as general as possible
and includes the electrical nature of an extracellu-
lar medium. Both microscopic and macroscopic
(mean-ﬁeld) models of the LFP are considered. It
is shown that the frequency-ﬁltering properties of
the extracellular medium can fully explain the
spectral properties of LFPs (such as power law
or 1/f scaling).
68
LFP Analysis: Overview

Methodological Aspects of the LFP
Different methods to record LFPs are described in
the entry ▶“Local Field Potential, Methods of
Recording” by Andrew Sharott. This entry
describes important factors such as the type of
electrode, the role of the reference, and the record-
ing conditions. The entry discusses how such
factors can be determined to correctly interpret
the LFP signal.
In the entry ▶“Resistivity/Conductivity of
Extracellular Medium”, Scott Lempka and Cam-
eron McIntyre review another set of important
biophysical properties that contribute to LFP gen-
esis: the electrical properties of neural tissue, the
associated measurements of conductivity, the
frequency-ﬁltering properties, as well as the inﬂu-
ence of volume conduction.
Finally, in the entry ▶“Current Source Density
(CSD) Analysis”, Daniel Wójcik reviews the
physical bases of the CSD analysis, which con-
sists of estimating neuronal current sources from
LFP measurements performed by electrodes
located at equidistant points in space. Different
variants of the CSD analysis are presented and
discussed.
Cross-References
▶Current Source Density (CSD) Analysis
▶Electrocorticogram (ECoG)
▶Local Field Potential and Deep Brain
Stimulation (DBS)
▶Local Field Potential and Movement Disorders
▶Local Field Potential in the Visual System
▶Local Field Potential, Ephaptic Interactions
▶Local Field Potential, Methods of Recording
▶Local Field Potential, Relationship to BOLD
Signal
▶Local Field Potential, Relationship to
Electroencephalogram (EEG) and
Magnetoencephalogram (MEG)
▶Local Field Potential, Relationship to Unit
Activity
▶Local Field Potential, Synchrony of
▶Local Field Potential: Relationship to
Membrane Synaptic Potentials
▶Local Field Potentials in Olfaction
▶Local Field Potentials: Interaction with the
Extracellular Medium
▶Local Field Potentials: LFP
▶Resistivity/Conductivity of Extracellular
Medium
References
Abeles M (1982) Local cortical circuits: an electrophysio-
logical
study.
In:
Braitenberg
V,
Barlow
HB,
Bullock H, Florey E, Grusser O-J, Peters A (eds) .
Springer, New York
Bedard C, Kroger H, Destexhe A (2004) Modeling extracel-
lular ﬁeld potentials and the frequency-ﬁltering proper-
ties of extracellular space. Biophys J 86:1829–1842
Bullock TH (1999) Slow potentials in the brain: still little
understood but gradually getting analytical attention.
Brain Res Bull 50:315–316
Contreras D, Destexhe A, Sejnowski TJ, Steriade M (1997)
Spatiotemporal patterns of spindle oscillations in cortex
and thalamus. J Neurosci 17:1179–1196
Creutzfeldt OD, Watanabe S, Lux HD (1966) Relations
between EEG phenomena and potentials of single cor-
tical cells. II. Spontaneous and convulsoid activity.
Electroencephalogr Clin Neurophysiol 20:19–37
Destexhe A, Contreras D, Steriade M (1999) Spatiotempo-
ral analysis of local ﬁeld potentials and unit discharges
in cat cerebral cortex during natural wake and sleep
states. J Neurosci 19:4595–4608
Donoghue JP, Sanes JN, Hatsopoulos NG, Gaal G (1998)
Neural discharge and local ﬁeld potential oscillations in
primate motor cortex during voluntary movements.
J Neurophysiol 79:159–173
Eccles JC (1951) Interpretation of action potentials evoked
in the cerebral cortex. J Neurophysiol 3:449–464
Eckhorn R, Obermueller A (1993) Single neurons are
differently involved in stimulus-speciﬁc oscillations in
cat visual cortex. Exp Brain Res 95:177–182
Eggermont JJ, Smith GM (1995) Synchrony between
single-unit activity and local ﬁeld potentials in relation
to periodicity coding in primary auditory cortex.
J Neurophysiol 73:227–245
Elul R (1971) The genesis of the EEG. Int Rev Neurobiol
15:227–272
Goldberg JA, Rokni U, Boraud T, Vaadia E, Bergman
H (2004) Spike synchronization in the cortex/basal-
ganglia networks of parkinsonian primates reﬂects
global
dynamics
of
the
local
ﬁeld
potential.
J Neurosci 24(26):6003–6010
Gray CM, Singer W (1989) Stimulus-speciﬁc neuronal
oscillations in orientation columns of cat visual cortex.
Proc Natl Acad Sci U S A 86:1698–1702
Klee M, Rall W (1977) Computed potentials of cortically
arranged populations of neurons. J Neurophysiol 40:
647–666
LFP Analysis: Overview
69

Lass Y (1968) A quantitative approach to the correlation of
slow wave and unit electrical activity in the cerebral
cortex
of
the
cat.
Electroencephalogr
Clin
Neurophysiol 25:503–506
Lorente de Nó R (1947) Analysis of the distribution of
action currents of nerve in volume conductors. Stud
Rockefeller Inst Med Res 132:384–477
Mitzdorf U (1985) Current source-density method and
application in cat cerebral cortex: investigation of
evoked potentials and EEG phenomena. Physiol Rev
65:37–100
Murthy VN, Fetz EE (1996a) Oscillatory activity in sen-
sorimotor cortex of awake monkeys: synchronization
of local ﬁeld potentials and relation to behavior.
J Neurophysiol 76:3949–3967
Murthy VN, Fetz EE (1996b) Synchronization of neurons
during local ﬁeld potential oscillations in sensorimotor
cortex
of
awake
monkeys.
J
Neurophysiol
76:
3968–3982
Renshaw B, Forbes A, Morison BR (1940) Activity of
isocortex and hippocampus: electrical studies with
micro-electrodes. J Neurophysiol 3(1):74–105
Sanes JN, Donoghue JP (1993) Oscillations in local ﬁeld
potentials of the primate motor cortex during voluntary
movement. Proc Natl Acad Sci U S A 90:4470–4474
Low Frequency Oscillations
(Anesthesia and Sleep):
Overview
Diego Contreras
Department of Neuroscience, School of
Medicine, University of Pennsylvania,
Philadelphia, PA, USA
Synonyms
Delta oscillations; Relay mode; Sleep; Sleep
oscillations; Sleep spindles; Slow oscillations;
Slow rhythms; Thalamic bursting
Definition
The transition from waking to sleep or to anesthesia
is characterized by an increase in the amplitude and
a decrease in the frequency of the electrical activity
recorded in the electroencephalogram (EEG). The
spectral composition of the EEG changes from one
dominated by low-amplitude fast frequencies in the
beta gamma range to one dominated by the fre-
quency ranges of slow (0.1–1 Hz), delta (1–4 Hz),
and sigma (7–15 Hz, which corresponds with sleep
spindles) oscillations (da Silva and Schomer 2011).
The dramatic changes in the EEG during the tran-
sition from waking to sleep correlate with the
deafferentation of the forebrain from the external
world and the suppression of consciousness. This
section describes cellular mechanisms and com-
puter models of these oscillatory processes and
their functional consequences. In this overview
entry, some critical points are discussed about the
organizations of these rhythms in slow wave sleep
and anesthesia.
Detailed Description
Background
Slow sleep rhythms are generated within the vast
networks
connecting
thalamus
and
cortex
(Steriade et al. 1994; McCormick and Bal 1997).
The main building blocks of thalamocortical cir-
cuits (Steriade 2003; Jones 2007), which are
essential to understanding the basic principles of
rhythm generation (Fig. 1), are (i) reciprocal excit-
atory glutamatergic connections between cortical
pyramidal cells and thalamocortical neurons;
(ii) collaterals of excitatory thalamocortical and
corticothalamic axons onto reticular thalamic
cells, which are established as these axons cross
the reticular nucleus on their way in and out of the
thalamus; and (iii) inhibitory connections from
reticular thalamic cells onto thalamocortical neu-
rons. Reticular thalamic neurons surround the
thalamus in its dorsal and lateral surfaces and do
not project to the cerebral cortex but project only
back into all other thalamic nuclei (Ramon y Cajal
1911). The reciprocal excitation and feedback
inhibition combined with various degrees of con-
vergence and divergence are essential to the gen-
eration, distribution, and synchronization of the
rhythms that characterize sleep and anesthesia
(Jones 2001).
Cellular Mechanisms
The single most important cellular mechanism
underlying the dramatic transformation in brain
70
Low Frequency Oscillations (Anesthesia and Sleep): Overview

state during the transition from waking to sleep, or
to anesthesia, is the change in ﬁring mode of
thalamocortical cells (Fig. 2) from tonic to burst-
ing (Llinas and Jahnsen 1982; Jahnsen and Llinas
1984c; Steriade and Llinas 1988; Steriade et al.
1994; McCormick and Bal 1997). At depolarized
membrane potentials (Vm) during activated brain
states, such as waking, thalamocortical neurons
respond to inputs with spike trains that reﬂect
the amplitude and time of the inputs (Deschenes
et al. 1984; McCormick and Feeser 1990; McCor-
mick and Huguenard 1992). The membrane
hyperpolarization that results from the reduction
in cholinergic and adrenergic input during the
transition from waking to sleep (Oakson and
Steriade 1982; Steriade et al. 1982, 1990; Lu
et al. 1993; Steriade 1993) removes inactivation
of T-type calcium channels (Jahnsen and Llinas
1984a; Nowycky et al. 1985; Coulter et al. 1989;
McCormick and Huguenard 1992; Gutierrez et al.
2001). When fully de-inactivated, T-channels are
capable of generating an all-or-none calcium
spike, known as a low-threshold spike (LTS)
because it is triggered at thresholds 10 mV lower
than the sodium action potential (Deschenes et al.
1982; Llinas and Jahnsen 1982). The LTS in turn
generates sufﬁcient depolarization to trigger a
high-frequency (100–300 Hz) burst of sodium
action potentials (Llinas and Jahnsen 1982;
Deschenes et al. 1984; Jahnsen and Llinas 1984b).
The amplitude of the LTS and, therefore, of the
resulting burst of sodium action potentials depend
on
the
proportion
of
T-channels
that
are
de-inactivated (Deschenes et al. 1982, 1984;
Coulter et al. 1989; McCormick and Huguenard
1992), which reaches 100% at the hyperpolarized
Vm of late stages of sleep and deep anesthesia.
The mechanism by which LTS bursts are trig-
gered changes throughout the progressive stages
of sleep. This is due to the progressive hyperpo-
larization of thalamocortical cells as sleep (Hirsch
et al. 1983) or anesthesia (Contreras and Steriade
1997a) deepens and EEG activity becomes larger
and slower (Fig. 3). At early sleep stages, when
sleep spindles are the most conspicuous feature of
the EEG, thalamocortical cells are still relatively
depolarized
and
T-channels
are
not
fully
de-inactivated. De-inactivation depends on the
phasic strong chloride-dependent GABAA IPSPs
generated by the inhibitory input from reticular
thalamic cells during sleep spindles (von Krosigk
et al. 1993; Bal et al. 1995; McCormick and Bal
1997). During spindles, LTS bursts are generated
as rebound bursts at the offset of these large
GABAA IPSPs. At later, deeper (deﬁned by the
higher threshold to wake up) stages of sleep,
thalamocortical cells are further hyperpolarized
and T-channels are fully de-inactivated. At this
stage, T-channels are activated by the depolariza-
tion caused by the cationic inward rectiﬁer current
IH (McCormick and Pape 1990). IH is voltage
dependent and activated at the hyperpolarized
levels of deep sleep and deep anesthesia. This
IH-dependent depolarization, which is a ubiquitous
property of thalamocortical cells (Dossi et al.
1992), triggers an LTS and a spike burst, at the
Low Frequency Oscillations (Anesthesia and Sleep):
Overview, Fig. 1 A summary of the basic thalamo-
cortico-thalamic
circuit:
1.
Reciprocal
excitatory
(glutamatergic)
connections
between
thalamocortical
(TC) and cortical (Cx) neurons. 2. Both ascending and
descending axons leave collaterals in the reticular nucleus
(RE). 3. RE neurons are exclusively GABAergic and pro-
ject topographically only back into the dorsal thalamus (not
to cortex). The three cells used for the schematic were
recorded intracellularly in vivo and ﬁlled with biocytin
Low Frequency Oscillations (Anesthesia and Sleep): Overview
71

Low Frequency Oscillations (Anesthesia and Sleep):
Overview, Fig. 2 This ﬁgure shows a thalamocortical
neuron, ﬁlled with biocytin (right) in vivo, responding to
synaptic activation (top three traces) and direct current
injection (bottom four traces) through the microelectrode.
Hyperpolarization switches ﬁring mode from tonic to
bursting in both cases. Bottom trace shows a rebound
burst, a burst generated at the termination of a hyper-
polarizing pulse (From Contreras and Steriade 1995)
Low Frequency Oscillations (Anesthesia and Sleep):
Overview, Fig. 3 The transition from waking to sleep is
characterized in the EEG by a transition from high-
frequency (~40 Hz) low-amplitude activity to low-
frequency (<15 Hz) high-amplitude oscillations. TC cells
recorded simultaneously with the EEG show the transition
from tonic, relay mode of ﬁring during the waking state to
the bursting, all-or-none mode of ﬁring, incompatible with
information transfer to the cerebral cortex. In addition, as
shown in the ﬁgure, the bursting mode of ﬁring is associ-
ated with the presence of global slow oscillations that are
synchronized and widespread over large cortical and tha-
lamic territories (the ﬁgure shows how activity is synchro-
nized only during sleep between areas 4 and 5, for
example). The high-amplitude, synchronized oscillatory
activity together with the thalamic bursting mode of ﬁring
disconnect the cortex from the outside world during sleep
(From Contreras and Steriade 1997a)
72
Low Frequency Oscillations (Anesthesia and Sleep): Overview

end of which, the membrane returns to its hyper-
polarized level and reactivates IH, thus restarting a
rhythmic cycle at the delta frequency range
(McCormick and Pape 1990; Dossi et al. 1992;
Nunez et al. 1992a). At the deep stage of sleep
where thalamocortical cells generate delta oscilla-
tions, the EEG is also dominated by large ampli-
tude rhythmic waves at the delta frequency.
However, it is not clear what the relationship is, if
any, between the intrinsically generated delta oscil-
lations of thalamocortical cells and the cortical
generators of delta oscillations when this rhythm
is the predominant feature of the EEG (Amzica and
Steriade 1998; da Silva and Schomer 2011;
Carracedo et al. 2013).
A fundamental consequence of the progressive
hyperpolarization of the Vm of thalamocortical
cells throughout sleep (Fig. 3) is that, at the deep
sleep stages when delta oscillations dominate the
EEG (Fig. 3), the Vm is at or below the reversal
potential for chloride, and therefore sleep spindles
cannot occur. This leads to an incompatibility of
sleep and delta rhythms in thalamocortical cells,
with sleep spindles characterizing early stages
when the Vm is still relatively depolarized and
delta oscillations characterizing late stages when
the Vm is very hyperpolarized (Nunez et al. 1992b).
Functional Consequences of Slow
Rhythms
Three main consequences result from the transi-
tion in ﬁring mode from tonic to bursting during
the transition from waking to sleep or anesthesia:
1. Unreliability of Responses. Rather than reliable
and predictable responses to sensory, motor, or
cortical input, thalamic cells respond to inputs
with all-or-none bursts of spikes which bear
little information about the incoming input. Fur-
thermore, because voltage-dependent inactiva-
tion of T-channels lasts tens of milliseconds,
bursting responses to depolarizing inputs can
only occur at low frequencies and therefore
represent a strong frequency ﬁlter to incoming
inputs (McCormick and Feeser 1990).
2. Rhythmicity. The transition to burst ﬁring brings
about slow rhythmic activity in thalamocortical
networks. Three main slow rhythms characterize
sleep and anesthesia: sleep spindles (7–15 Hz),
delta oscillations (1–4 Hz), and slow oscillations
(0.1–1 Hz). Such oscillatory activity requires
participation of burst ﬁring by thalamocortical
cells and therefore extends the role of thalamic
burst ﬁring to the deafferentation of the forebrain.
Indeed, in addition to the bursting properties
described above, most thalamic spike bursts
occur NOT in response to stimuli but, instead,
as part of large oscillatory networks, further hin-
dering thalamocortical relay of inputs (Steriade
et al. 1994; Steriade 2000).
3. Synchronization. An inevitable consequence of
thalamic burst ﬁring and slow rhythm generation
is the broad synchronization of thalamocortical
networks (Contreras and Steriade 1997a, b). By
virtue of the divergence and convergence of con-
nections
between
reticular
thalamic
and
thalamocortical cells, between thalamocortical
and cortical neurons and within cortical net-
works, the strong bursting activity entrains
large networks into the slow oscillations of
sleep and anesthesia (Amzica and Steriade
1995).
As a result of the above three factors, the reli-
ably responding, rapidly engaged thalamocortical
networks that sustain information processing and
consciousness in the awake state are transformed
into networks that are slow and unreliable in
responding and are engaged mostly in slow and
synchronized oscillations. These oscillations are
incompatible with information processing and
ultimately determine the deafferentation of the
forebrain during slow wave sleep and anesthesia
(Steriade 2000).
Anesthesia and Slow Waves
In contrast with the organized and sequential
arrangement of oscillations during sleep, anes-
thetics mimic and exaggerate one or more rhythmic
patterns of sleep in different proportions. For exam-
ple, during barbiturate anesthesia, the EEG shows
spindles most prominently and virtually no delta or
slow oscillations. Furthermore, under barbiturate
anesthesia, spindle frequency is reduced and
Low Frequency Oscillations (Anesthesia and Sleep): Overview
73

duration increased with respect to naturally occur-
ring sleep spindles. In contrast, ketamine-xylazine
generates a strong and exaggerated slow oscillatory
pattern on top of which rapid spindles may be
triggered. In addition, the slow rhythm during
ketamine-xylazine may reach frequencies above
1 Hz and may be confused with EEG delta waves.
Urethane anesthesia generates a very stable pattern
of slow oscillations with prolonged depolarizing
and hyperpolarizing phases but very little spindle
or delta oscillations. The combination of propofol
and fentanyl produces a rich combination of
rhythms including slow oscillations, occasional
spindles, and delta waves. Detailed descriptions of
oscillatory patterns under different anesthetics are
far beyond this section.
Cross-References
▶Corticothalamic Feedback: Large-Scale
Synchrony
▶Delta Rhythms: Models and Physiology
▶Slow Oscillations and Epilepsy: Network
Models
▶Slow Oscillations: Models
▶Slow Oscillations: Physiology
▶Spindle Oscillations: Models
References
Amzica F, Steriade M (1995) Short- and long-range neu-
ronal synchronization of the slow (<1 Hz) cortical
oscillation. J Neurophysiol 73:20–38
Amzica F, Steriade M (1998) Electrophysiological corre-
lates of sleep delta waves. Electroencephalogr Clin
Neurophysiol 107:69–83
Bal T, von Krosigk M, McCormick DA (1995) Role of the
ferret perigeniculate nucleus in the generation of syn-
chronized
oscillations
in
vitro.
J
Physiol
483(Pt 3):665–685. PMCID: PMC1157809
Carracedo LM, Kjeldsen H, Cunnington L, Jenkins A,
Schoﬁeld I, Cunningham MO, Davies CH, Traub RD,
Whittington MA (2013) A neocortical delta rhythm
facilitates
reciprocal
interlaminar
interactions
via
nested theta rhythms. J Neurosci 33:10750–10761.
PMCID: PMC3693056
Contreras D, Steriade M (1995) Cellular basis of EEG slow
rhythms: a study of dynamic corticothalamic relation-
ships. J Neurosci 15:604–622
Contreras D, Steriade M (1997a) State-dependent ﬂuctua-
tions of low-frequency rhythms in corticothalamic net-
works. Neuroscience 76:25–38
Contreras D, Steriade M (1997b) Synchronization of low-
frequency rhythms in corticothalamic networks. Neu-
roscience 76:11–24
Coulter DA, Huguenard JR, Prince DA (1989) Calcium
currents in rat thalamocortical relay neurones: kinetic
properties of the transient, low-threshold current.
J Physiol 414:587–604. PMCID: PMC1189159
da Silva FL, Schomer DL (eds) (2011) Niedermeyer’s
electroencephalography:
basic
principles,
clinical
applications, and related ﬁelds. Lippincott Williams &
Wilkins, Philadelphia
Deschenes M, Roy JP, Steriade M (1982) Thalamic burst-
ing mechanism: an inward slow current revealed by
membrane hyperpolarization. Brain Res 239:289–293
Deschenes M, Paradis M, Roy JP, Steriade M (1984) Elec-
trophysiology of neurons of lateral thalamic nuclei in
cat:
resting
properties
and
burst
discharges.
J Neurophysiol 51:1196–1219
Dossi RC, Nunez A, Steriade M (1992) Electrophysiology
of a slow (0.5–4 Hz) intrinsic oscillation of cat
thalamocortical neurones in vivo. J Physiol 447:
215–234. PMCID: PMC1176033
Gutierrez C, Cox C, Rinzel J, Sherman S (2001) Dynamics
of low-threshold spike activation in relay neurons of the
cat
lateral
geniculate
nucleus.
J
Neurosci
21:
1022–1032
Hirsch JC, Fourment A, Marc ME (1983) Sleep-related
variations of membrane potential in the lateral genicu-
late body relay neurons of the cat. Brain Res 259:
308–312
Jahnsen H, Llinas R (1984a) Ionic basis for the electro-
responsiveness and oscillatory properties of guinea-pig
thalamic neurones in vitro. J Physiol 349:227–247.
PMCID: PMC1199335
Jahnsen H, Llinas R (1984b) Voltage-dependent burst-to-
tonic switching of thalamic cell activity: an in vitro
study. Arch Ital Biol 122:73–82
Jahnsen H, Llinas R (1984c) Electrophysiological proper-
ties of guinea-pig thalamic neurones: an in vitro study.
J Physiol 349:205–226. PMCID: PMC1199334
Jones EG (2001) The thalamic matrix and thalamocortical
synchrony. Trends Neurosci 24:595–601
Jones EG (2007) The thalamus. Cambridge University
Press, Cambridge
Llinas R, Jahnsen H (1982) Electrophysiology of mamma-
lian thalamic neurones in vitro. Nature 297:406–408
Lu SM, Guido W, Sherman SM (1993) The brain-stem
parabrachial region controls mode of response to visual
stimulation of neurons in the cat’s lateral geniculate
nucleus. Vis Neurosci 10:631–642
McCormick DA, Bal T (1997) Sleep and arousal:
thalamocortical mechanisms. Annu Rev Neurosci 20:
185–215
McCormick DA, Feeser HR (1990) Functional implica-
tions of burst ﬁring and single spike activity in lateral
geniculate relay neurons. Neuroscience 39:103–113
McCormick DA, Huguenard JR (1992) A model of the
electrophysiological properties of thalamocortical relay
neurons. J Neurophysiol 68:1384–1400
McCormick DA, Pape HC (1990) Properties of a
hyperpolarization-activated cation current and its role
74
Low Frequency Oscillations (Anesthesia and Sleep): Overview

in rhythmic oscillation in thalamic relay neurones.
J Physiol 431:291–318. PMCID: PMC1181775
Nowycky MC, Fox AP, Tsien RW (1985) Three types of
neuronal calcium channel with different calcium ago-
nist sensitivity. Nature 316:440–443
Nunez A, Amzica F, Steriade M (1992a) Intrinsic and synap-
tically generated delta (1–4 Hz) rhythms in dorsal lateral
geniculate neurons and their modulation by light-induced
fast (30–70 Hz) events. Neuroscience 51:269–284
Nunez A, Curro Dossi R, Contreras D, Steriade M (1992b)
Intracellular evidence for incompatibility between
spindle and delta oscillations in thalamocortical neu-
rons of cat. Neuroscience 48:75–85
Oakson G, Steriade M (1982) Slow rhythmic rate ﬂuctua-
tions of cat midbrain reticular neurons in synchronized
sleep and waking. Brain Res 247:277–288
Ramon y Cajal S (1911) Histologie du systeme nerveux de
l’homme et des vertebres. Malione, Paris
Steriade M (1993) Cholinergic blockage of network- and
intrinsically generated slow oscillations promotes wak-
ing and REM sleep activity patterns in thalamic and
cortical neurons. Prog Brain Res 98:345–355
Steriade M (2000) Corticothalamic resonance, states of
vigilance and mentation. Neuroscience 101:243–276
Steriade M (2003) Thalamus. Wiley
Steriade M, Llinas RR (1988) The functional states of the
thalamus and the associated neuronal interplay. Physiol
Rev 68:649–742
Steriade M, Oakson G, Ropert N (1982) Firing rates and
patterns of midbrain reticular neurons during steady
and transitional states of the sleep-waking cycle. Exp
Brain Res 46:37–51
Steriade M, Datta S, Pare D, Oakson G, Curro Dossi RC
(1990) Neuronal activities in brain-stem cholinergic
nuclei
related
to
tonic
activation
processes
in
thalamocortical systems. J Neurosci 10:2541–2559
Steriade M, Contreras D, Amzica F (1994) Synchronized
sleep oscillations and their paroxysmal developments.
Trends Neurosci 17:199–208
von Krosigk M, Bal T, McCormick DA (1993) Cellular
mechanisms of a synchronized oscillation in the thala-
mus. Science 261:361–364
Model Reproducibility:
Overview
Sharon M. Crook
School of Mathematical and Statistical Sciences
and School of Life Sciences, Arizona State
University, Tempe, AZ, USA
Definition
The ability to reproduce an experimental result is
the foundation of scientiﬁc inquiry. Similarly,
computational studies need to be reproducible to
serve the advance of science. However, computa-
tional scientists often ﬁnd it difﬁcult to reproduce
published computational results. Here we provide
an overview of efforts to support model reproduc-
ibility in computational neuroscience.
Detailed Description
Reproducing the simulation results of computa-
tional models and establishing the provenance of
results should be straightforward given that com-
putational studies do not suffer from the measure-
ment errors seen in the experimental sciences.
However, computational science has its own chal-
lenges for reproducibility, which are described
well by Crook et al. (2013). In particular, issues
such as the sensitivity of a model to numerics or
the publication of models that are computationally
under-speciﬁed lead to the need for criteria for
successful model reproduction in many cases.
These authors also make distinctions among:
•
Replicability, where the same code and tools
are used to reproduce results
•
Cross-replicability, where the same model is
used but implemented using different software
•
Reproducibility,
where
a
new
model
is
implemented and produces the same scientiﬁc
result
As noted in Crook et al. (2013), the boundary
between cross-replicability and reproducibility is
not always clear, but all efforts along this contin-
uum of reproducible research contribute to
advances in the computational sciences.
Model Sharing
Model sharing provides a means for promoting
replicability and transparency in the computa-
tional sciences. For many reasons, the best infra-
structure for sharing models involves curated
model repositories such as ModelDB (Migliore
et al. 2003), the BioModels Database: a Public
Repository for Sharing Models of Biological Pro-
cesses (Li et al. 2010; Vijayalakshmi et al. 2013),
the Physiome Repository (Yu et al. 2011), and
Open Source Brain (Gleeson et al. 2019).
Model Reproducibility: Overview
75

Simulator Independent Specification of
Models and Simulations
There are a number of ongoing efforts in support
of cross-replicability with the aim of providing
declarative descriptions of models that are simu-
lator independent. Many of these efforts focus on
the use of Extensible Markup Language (XML)
technology (Bray et al. 1998) as an ideal repre-
sentation for complex models.
The
Systems
Biology
Markup
Language
(SBML) (Hucka et al. 2003) and CellML (Lloyd
et al. 2004) are two popular languages for describing
systems of interacting biomolecules that comprise
models that are often used in systems biology. Both
languages also can be used for describing more
generic dynamical models, including those in neu-
roscience. FieldML follows a similar approach but
focuses on multivariate spatiotemporal models and
models based on the ﬁnite element method.
NeuroML (Goddard et al. 2001; Crook et al. 2007;
Gleeson et al. 2010) differs from these languages in
that it is domain speciﬁc, where neuroscience con-
cepts like cells, ion channels, and synapses form the
basis for objects described in the language. NineML
is another domain-speciﬁc language that focuses on
descriptions of spiking neural networks. Addition-
ally,
the
Simulation
Experiment
Description
Markup Language (SED-ML) (Köhn and Le
Novère 2008) provides a language for describing
the details of simulation experiments. These differ-
ent markup languages are complementary, and
together they cover the different spatial scales for
the majority of models in neuroscience.
Other efforts focus on code-based approaches
for simulator independent model descriptions.
PyNN is a Python-based, simulator independent
language for building neuronal network models
then simulating the model on any simulation plat-
form that PyNN supports such as NEURON,
NEST, and Brian (Davison et al. 2009).
The International Neuroinformatics Coordinat-
ing Facility, or INCF, advocates for FAIR
(Findable Accessible Interoperable Reproducible)
principles across all of their projects and activities
including neuroscience data and models. In sup-
port of this effort, the organization provides a
process for the formal endorsement of community
standards, including two for describing models,
NeuroML and PyNN.
Improved Research Reporting
In the systems biology community, MIRIAM
(Minimum Information Required in the Annota-
tion of Models, http://co.mbine.org/standards/
miriam) is a community-level effort to provide a
set of guidelines for use with any structured for-
mat for sharing models. The idea of using best
practices and some required metadata for sharing
and publishing models is outlined by Novère et al.
(2005). All of these requirements can be adapted
readily to models in neuroscience. Nordlie et al.
(2009) conducted a survey of neuronal network
models in the literature and found that current
approaches are diverse and inadequate. These
authors propose the adoption of best practices
for
model
descriptions
in
publications,
recommending the inclusion of the hypothesis,
model derivation, model description, implemen-
tation details, model analysis, and model justiﬁ-
cation. They also provide a concise tabular format
for summarizing network models in publications.
Publication standards such as those discussed by
Le Novère et al. and Nordlie et al. ensure that all
possible model details are provided; however, it is
important to be aware of the limits of replicability
and the impact on reproducibility.
Cross-References
▶CellML
▶FieldML
▶NeuroML
▶NineML
▶PyNN: A Python API for Neural Network
Modelling
▶Simulation Experiment Description Markup
Language (SED-ML)
▶Systems Biology Markup Language (SBML)
References
Bray T, Paoli J, Sperberg-McQueen C (1998) Extensible
markup language (XML) 1.0. http://www.w3.org/TR/
REC-xml
Crook S, Gleeson P, Howell F, Svitak J, Silver RA
(2007) MorphML: level 1 of the NeuroML standards
for neuronal morphology data and model speciﬁcation.
Neuroinformatics 5:96–104
76
Model Reproducibility: Overview

Crook S, Davison AP, Plesser HE (2013) Learning from
the past: approaches for reproducibility in computational
neuroscience.
In:
Bower
JM
(ed)
20
years
of
computational neuroscience, Springer series in compu-
tational neuroscience, vol 9. Springer, New York,
pp 73–102
Davison AP, Brüderle D, Eppler JM, Kremkow J, Muller E,
Pecevski DA, Perrinet L, Yger P (2009) PyNN: a
common interface for neuronal network simulators.
Front Neuroinform 2:11. https://doi.org/10.3389/
neuro.11.011.2008
Gleeson P, Crook S, Cannon RC, Hines ML, Billings GO,
Farinella M, Morse TM, Davison AP, Ray S, Bhalla
US,
Barnes
SR,
Dimitrova
YD,
Silver
RA
(2010) NeuroML: a language for describing data driven
models of neurons and networks with a high degree of
biological detail. PLoS Comput Biol 6(6):e1000815
Gleeson
P,
Cantarelli
M,
Marin
B,
Quintana
A,
Earnshaw M, Sadeh S, Piasini E, Birgiolas J, Cannon
RC, Cayco-Gajic NA, Crook S, Davison AP, Dura-
Bernal S, Ecker A, Hines ML, Idili G, Lanore F, Larson
SD, Lytton WW, Majumdar A, McDougal RA,
Sivagnanam S, Solinas S, Stanislovas R, van Albada
SJ, van Geit W, Silver RA (2019) Open source brain: a
collaborative resource for visualizing, analyzing, sim-
ulating, and developing standardized models of neu-
rons and circuits. Neuron 103(3):395–411
Goddard N, Hucka M, Howell F, Cornelis H, Shankar K,
Beeman
D
(2001)
NeuroML:
model
description
methods for collaborative modeling in neuroscience.
Philos Trans R Soc Lond Ser B Biol Sci 356:1209–1228
Hucka M, Finney A, Sauro H, Bolouri H, Doyle J,
Kitano H, Arkin A (2003) The systems biology markup
language (SMBL): a medium for representation and
exchange of biochemical network models. Bioinfor-
matics 19:524–531
Köhn D, Le Novère N (2008) SED-ML – an XML format
for the implementation of the MIASE guidelines. In:
Heiner M, Uhrmacher A (eds) Computational methods
in systems biology, Lecture notes in computer science,
vol 5307. Springer, Berlin, pp 176–190
Le
Novère
N,
Finney
A,
Hucka
M,
Bhalla
US,
Campagne F, Callado-Vides J, Crampin E, Halstead
M et al (2005) Minimum information requested in the
annotation of biochemical models (MIRIAM). Nat
Biotechnol 23(12):1509–1515
Li C, Donizelli M, Rodriguez N, Dharuri H, Endler L,
Chelliah V, Li L, He E, Henry A, Stefan MI, Snoep
JL, Hucka M, Le Novere N, Laibe C (2010) BioModels
database: an enhanced, curated and annotated resource
for published quantitative kinetic models. BMC Syst
Biol 4:92
Lloyd CM, Halstead MDB, Nielsen PF (2004) CellML: its
future, present and past. Prog Biophys Mol Biol
85(2–3):433–450
Migliore M, Morse TM, Davison AP, Marenco L, Shep-
herd
GM,
Hines
M
(2003)
ModelDB.
Neuroinformatics 1(1):135–139
Nordlie E, Gewaltig MO, Plesser HE (2009) Towards
reproducible descriptions of neuronal network models.
PLoS Comput Biol 5(8):e1000456
Vijayalakshmi C, Laibe C, Le Novere N (2013) Biomodels
database: a repository of mathematical models of bio-
logical processes. Methods Mol Biol 1021:189–199
Yu T, Lloyd CM, Nickerson DP, Cooling MT, Miller AK,
Garny A, Terk-ildsen JR, Lawson J, Britten RD, Hunter
PJ, Nielsen PM (2011) The physiome model repository
2. Bioinformatics 27(5):743–744
Modeling of Disease: Physical
and Molecular Level,
Overview
Adam John Hunter Newton1,2,3 and
William W. Lytton4,5
1Department of Physiology and Pharmacology,
SUNY Downstate Medical Center, Brooklyn,
NY, USA
2Department of Neuroscience, Yale University,
New Haven, CT, USA
3Yale School of Medicine, Yale Center for
Medical Informatics, New Haven, CT, USA
4Departments of Physiology and Pharmacology
and Neurology, SUNY Downstate Medical
Center, Brooklyn, NY, USA
5Department of Neurology, Kings County
Hospital, Brooklyn, NY, USA
Definition
Physical-, molecular-, and cellular-scale under-
standing is not only the ﬁrst step in the long ladder
of multiscale modeling for clinical translational
application, it is also the critical scale: “Where
the rubber meets the road.” The idiom from auto-
motive engineering refers to the critical endpoint
where all the ancillary engineering – fuel system,
chassis, pistons, etc. – is ﬁnally tested. In the
realm of translation, this point comes when we
translate basic research results to the clinical set-
ting. The physician intervenes at the molecular
scale, using pharmacological agents to alter phys-
iological activity only observed at far higher
scales: the macroscopic realms of clinical tests
and of signs and symptoms. The surgeon inter-
venes at a physical scale, for example, tying off or
ﬁlling up a vascular aneurysm to prevent its burst-
ing in the brain.
Modeling of Disease: Physical and Molecular Level, Overview
77

Given the vast scale gap between molecular or
physical treatments and macroscopic outcome,
there is enormous difﬁculty in understanding
causal relations, so as to design better drugs and
better drug combinations. Multiscale modeling
can help make these connections in order to pro-
vide a basis for rational pharmacological or surgi-
cal treatment.
Detailed Description
This section includes entries of three types:
Techniques: These simulations are performed
largely using techniques used elsewhere in
computational neuroscience. However, some
techniques have been introduced or expanded
in the context of their use in translational
studies.
Locations: Partitioning of the nervous system can
be done at many scales, from organelle to
major brain area.
Diseases: We make note of a few neurological and
psychiatric illnesses that have been examined
at this scale.
Techniques
The standard techniques of molecular and cellular
computational neuroscience are utilized for clinical
and translational modeling. The techniques that are
applicable to simulation at the molecular and cel-
lular level are listed here by their coverage in the
entries of this encyclopedia (loosely grouped from
micro to macro):
Stochastic Simulators
▶Gillespie Algorithm for Biochemical Reaction
Simulation
▶Deterministic Reaction-Diffusion Simulators
▶Bimolecular Reactions, Modeling of
▶Equivalent Cylinder Model (Rall)
In this section, we add three additional
methods that have been applied to clinical trans-
lational problems.
▶“Polypeptide and Protein Modeling for Drug
Design” takes us down to the lowest of the
multiple scales used in computational neurosci-
ence. Polypeptide modeling largely utilizes ball-
and-spring modeling, staying above the level of
quantum mechanical representations. Individual
atoms or atomic groups (e.g., amino acids) are
represented as moving in a force ﬁeld largely deter-
mined by the chemical bonds connecting them.
This research has direct importance for clinical
translation because the lock-and-key ﬁt of a ligand
(a drug) to a target is dependent on the physical
relations of the interacting species.
“Application of Declarative Programming in
Neurobiology to Decipher Normal and Patholog-
ical Processes” demonstrates the novel use of a
declarative programming language to address
problems in Alzheimer’s disease and in fear condi-
tioning. Declarative programming is distinguished
from the familiar imperative programming lan-
guages which we commonly utilize. Imperative
programs implement an algorithm. Declarative
programs describes relationships between ele-
ments, deﬁning what computations are possible
among these elements.
▶“Biomechanical Modeling of Traumatic Brain
Injury” provides physical-level models that con-
sider the material properties of tissue, its geome-
try, and boundary conditions for stresses applied
to the head, using ﬁnite element methods to pre-
dict injury from impact.
Locations
Disease strikes at many locations and at many
scales. Parsimony and clinical efﬁcacy depend on
ﬁnding the proper scale and proper locus for clinical
investigation and for clinical intervention – in some
cases these are not the same location. A recent
example from oncology is illustrative: a renal cancer
and a leukemia were previously classiﬁed according
to organ system and therefore treated by different
subspecialists with different therapeutic approaches.
These cancers have now been found to share the
same tumor mutation and to beneﬁt from similar
treatments. As in this oncology example, protean
neurological diseases will also express widely. For
example, diseases of mitochondria such as MELAS
(mitochondrial encephalomyopathy, lactic acidosis,
and stroke-like episodes) and MERRF (myoclonic
epilepsy with ragged red ﬁbers; the ragged ﬁbers
78
Modeling of Disease: Physical and Molecular Level, Overview

being broken mitochondria) produce neurological
disorders across many neural systems, beyond the
already broad scope of their acronyms (DiMauro
et al. 2013).
In addition to connoting locations in the brain
and body (macroscale), disease localization also
needs to consider the locus in the cell: particular
organelles such as mitochondria, particular clas-
ses of molecules such as ion channels, and partic-
ular
specialized
subcellular
spaces
such
as
synapses and spines. Some of these locations are
covered elsewhere in these volumes: see ▶“Syn-
aptic Dynamics: Overview” and ▶“Dendritic
Spines”. It is to be expected that disorders of
spine shape, and of proteins involved in synaptic
plasticity, will produce neurological or psychiatric
disorders. In this section, we consider the model-
ing of pharmacotherapeutic manipulation of ion
channels in ▶“Neuropharmacological Modeling
Alterations in Ionic Homeostasis” and ▶“Neuro-
pharmacological Modeling, Pharmacogenomics
and Ion Channel Modulation”.
A literal route to clinical application is described
here
in
▶“Brain
Extracellular
Space:
A Compartment for Intercellular Communication
and Drug Delivery”. The blood-brain barrier and
the extracellular space represent the major path-
ways for delivery of pharmacological agents. Due
to therapeutic index (therapeutic concentration/
toxic concentration ratio), it is important to con-
sider how a particular drug can be delivered so as to
arrive at adequate concentrations at the site of dis-
ease, without being present at excessive concentra-
tion at other locations (Brick and Erickson 2013).
At the cellular level, neurological disease has
substantial overlap with disease in other excitable
tissues. It is particularly valuable to consider the
extensive work that has been done in cardiology,
here represented by ▶“Cardiac Excitable Tissue
Pathology (Ion Channels)” and ▶“Cardiac Excit-
able Tissue Pathology (Ischemia)”. Cardiology
models are generally more mature than those in
neurology. They also have direct analogies to
diseases of the brain, e.g., myocardial infarction
and stroke, arrhythmias, and epilepsies. Similarly,
excitable tissue is important in the islets of the
pancreas and in other endocrine systems, where
production and release of hormone are directly
analogous to production and release of neuro-
transmitters: see entry on ▶“Endocrine Cell
Function and Dysfunction”. A broader physiolog-
ical perspective, relevant to many of these cell
types, is presented here in ▶“Neuropharmacolog-
ical Modeling Alterations in Ionic Homeostasis”.
Many neurological diseases present outside of
the brain or involve the brain in its association
with the body. In particular, the enteric nervous
system (ENS) and autonomic nervous system
(ANS) are prone to disorders (Rao and Gershon
2016) and will be important areas for modeling in
the future. In the present section, we feature an
entry on ▶“Pathological Changes in Peripheral
Nerve Excitability”, to consider disorders in the
peripheral nervous system (PNS). It is important
to note that myopathies (muscle diseases) also fall
within the bailiwick of neurology and will beneﬁt
from modeling, some of which can be borrowed
from the cardiology literature.
At the subcellular level, signaling occurs through
second and third through nth messengers. There has
been considerable modeling at this scale (Blackwell
2013), but not as much looking at dysfunction.
These topics are covered in detail in the entry on
▶“Biochemical Signaling Pathways and Diffusion:
Overview”. Particular note should be made of the
major role of calcium as a second messenger, cov-
ered in eight entries in that section. Certainly,
dysregulation of calcium control will play a major
role in the production and expression of disease that
has yet to be elucidated by modeling. A review of
the key role of calcium in Alzheimer’s disease,
Parkinson’s disease, and Huntington’s disease and
the current modeling efforts are presented in the
entry on ▶“Calcium Dysregulation in Neurodegen-
erative Diseases”.
At the subcellular and intracellular level, neu-
rons metabolic
dependence on astrocytes is
modeled in ▶“Brain Energy Metabolism”. The
entry details how dysfunction, particularly glucose
hypometabolism in neurodegenerative diseases,
could lead to cognitive impairments. It includes a
detailed example of a three-compartment meta-
bolic network model of energy metabolism and
the challenges of developing a spatial model.
A multiscale model of neuroinﬂammation is
described in the entry ▶“Neuroinﬂammation,
Modeling of Disease: Physical and Molecular Level, Overview
79

Glia, and Cytokines: Networks of Networks”. It
involves modeling networks of molecular interac-
tions at the subcellular level, which inﬂuence a
network of cell states that then feed into a network
of inﬂammation states at the tissue level. Complex
network interactions can lead to unexpected
results. The entry describes several modeling
approaches including machine learning, dynamic
systems, and control theory that reveal underlying
properties of the system.
All disease is to a greater or lesser extent mul-
tifactorial. Patient susceptibility is determined in
substantial part by the genome and its control, as
well as by the epigenome (and metagenome,
representing additional genomes that are involved
(mitochondrial, bacterial – commensal or patho-
logical, viral). In a neuron, communication
between genome and synapse is particularly com-
plex, insofar as this is information ﬂow between
the organism’s two primary loci of information
storage. Synaptic changes depend on elaboration,
transport, and insertion of membrane proteins,
which start with signals at the synapse that trigger
changes in transcription. In the present section, we
cover the role of second messengers in providing
communication with the cell nucleus: ▶“Tran-
scriptional Control Dysfunction, Modeling”, eval-
uating potential pathology in the linkage between
the cell and its nucleus at the molecular and organ-
elle scales.
Diseases
Evaluation of individual diseases is the natural
approach for clinical translation – as an end goal
we want to know how to treat a particular disease
or even a particular patient (personalized medi-
cine) or subgroup of patients (precision medi-
cine). We highlight here four diseases, although
there are several others where some cellular/
molecular modeling progress has been made and
many more where such progress is greatly needed.
A
model
of
molecular
dependencies
in
Alzheimer’s is presented in “Application of Declar-
ative Programming in Neurobiology to Decipher
Normal and Pathological Processes”. Examples of
modeling the role of calcium in this disease are
given in ▶“Calcium Dysregulation in Neurodegen-
erative Diseases”.
As a lens through which to view computational
neuroscience, disease modeling presents a number
of difﬁculties. First, diseases are generally multi-
organ, both in production and in expression. For
example, we present here an entry on ▶“Brain
Ischemia and Stroke”. Stroke is most often caused
by atherosclerosis, a disease of blood vessels.
A translational model would ideally include the
cellular changes of the arterial wall and the changes
in hemodynamics, along with factors that include
the blood-brain barrier (which breaks) and the role
of a broad dispersion of harmful neurohumoral
agents. Stroke frequently co-occurs with myocardial
infarction (MI). These coincident disorders, in the
brain, heart, and local microvasculature, affect one
another, e.g., MI reduces cardiac output and there-
fore increasing the extent of an incipient stroke (Gan
and Ramani 2008). Another difﬁculty in providing a
uniﬁed model of a disease is that the disease may
involve many disparate variants: hemorrhagic
vs. bland stroke, thrombotic vs. embolic stroke,
and cortical stroke (gray matter) vs. internal capsule
(white matter) stroke.
As mentioned in the “Deﬁnition” section
above, modeling at physical, molecular, and cel-
lular levels is only the lowest rung in the multi-
scale ladder of both downward and upward
causality. Disparate viewpoints at different scales
represent a challenge but are a necessary stage in
the development of future uniﬁed multiscale
views, as can be illustrated by the case of epi-
lepsy – contrast the entry in the section: ▶“Epi-
lepsy:
Abnormal
Ion
Channels”
with
the
perspective presented in entries on ▶“Epilepsy:
Computational Models”, ▶“Slow Oscillations
and Epilepsy: Network Models”, and ▶“Epi-
lepsy, Neural Population Models of”.
How are we to build multiscale models out of
these disparate pieces? Ideally it would be possi-
ble to embed one scale within the model for the
higher scale, thereby creating a single uniﬁed
multiscale model. In many cases this is not possi-
ble, so that the models remain separate, with each
level informing the other via emergent properties
discovered at the lower level. In the case of epi-
lepsy research, emergences from the effect of a
drug on a detailed model of an ion channel can be
used to change the parameterizations of an ion
80
Modeling of Disease: Physical and Molecular Level, Overview

channel, which could then be directly embedded
in a compartmental model. Emergences from the
compartment model can then be used in a reduced
model or to modify an integrate-and-ﬁre model.
Either of these models (or both as in a hybrid
network) can be directly embedded to produce
large network models. Emergences from the net-
work can then be used to modify a mean-ﬁeld
model. While this upward sequence is difﬁcult,
the downward sequence is even more difﬁcult: we
want to follow the scales back down in order to
apply lessons learned from the population model
in order to eventually suggest changes to drug
effect at the polypeptide level.
Conclusions
These initial efforts aimed at understanding the
base scale of physical, molecular, and cellular
interactions
for
understanding
brain
disease
should be considered in the context of the wider
ﬁeld of computational systems biology, a ﬁeld
concerned
with
multiscale
modeling
across
organ systems and medical specialties. Systems
biology has been catapulted forward with the real-
ization that the massive amounts of data being
collected as genomes, proteomes, and other
omes will not be interpretable without contexts,
contexts that can only be provided by sophisti-
cated computational models. Further future hope
springs from the potential of computer simulation
to provide personalized or precision medicine;
physiological data from a particular patient
would be used to adapt simulation parameters
for simulation, just as anatomical data for a par-
ticular patient is currently used to provide spe-
ciﬁcs of beam targeting in radiation therapy.
Cross-References
▶Biochemical Signaling Pathways and Diffu-
sion: Overview
▶Biomechanical Modeling of Traumatic Brain
Injury
▶Bimolecular Reactions, Modeling of
▶Brain Energy Metabolism
▶Brain Extracellular Space: A Compartment for
Intercellular
Communication
and
Drug
Delivery
▶Brain Ischemia and Stroke
▶Calcium Dysregulation in Neurodegenerative
Diseases
▶Cardiac
Excitable
Tissue
Pathology
(Ion
Channels)
▶Cardiac Excitable Tissue Pathology (Ischemia)
▶Dendritic Spines
▶Deterministic Reaction-Diffusion Simulators
▶Endocrine Cell Function and Dysfunction
▶Epilepsy, Neural Population Models of
▶Epilepsy: Abnormal Ion Channels
▶Epilepsy: Computational Models
▶Equivalent Cylinder Model (Rall)
▶Gillespie Algorithm for Biochemical Reaction
Simulation
▶Neuroinﬂammation, Glia, and Cytokines: Net-
works of Networks
▶Neuropharmacological Modeling Alterations in
Ionic Homeostasis
▶Neuropharmacological Modeling, Pharmaco-
genomics and Ion Channel Modulation
▶Pathological Changes in Peripheral Nerve
Excitability
▶Polypeptide and Protein Modeling for Drug
Design
▶Slow
Oscillations
and
Epilepsy:
Network
Models
▶Synaptic Dynamics: Overview
▶Transcriptional Control Dysfunction, Modeling
References
Blackwell KT (2013) Approaches and tools for modeling
signaling pathways and calcium dynamics in neurons.
J Neurosci Methods 220(2):131–140
Brick J, Erickson CK (2013) Drugs, the brain, and behav-
ior:
the
pharmacology
of
drug
use
disorders.
Routledge, New York
DiMauro S, Schon EA, Carelli V, Hirano M (2013) The
clinical maze of mitochondrial neurology. Nat Rev
Neurol 9:429–444
Gan RN, Ramani NV (2008) The stroke clinician’s hand-
book: a practical guide to the care of stroke patients.
World Scientiﬁc, Singapore
Rao M, Gershon MD (2016) The bowel and beyond: the
enteric nervous system in neurological disorders. Nat
Rev Gastroenterol Hepatol 13:517–528
Modeling of Disease: Physical and Molecular Level, Overview
81

Motoneurons and
Neuromuscular Systems:
Overview
Sharmila Venugopal
Department of Physiology, David Geffen School
of Medicine, University of California Los
Angeles, Los Angeles, CA, USA
Department of Integrative Biology and
Physiology, University of California, Los
Angeles, Los Angeles, CA, USA
Definition
Motoneuron models are mathematical representa-
tions of motoneuron structure and function. Neu-
romuscular
models
represent
computational
models of neural components integrated with
muscle models at various levels of detail and
complexities. These models provide a computa-
tional framework for investigating neural control
of movement. Together, motoneuron and neuro-
muscular models can guide the development of
biomimetic neuromuscular control systems and
neural prostheses.
Detailed Description
Motor neurons (or motoneurons) are the ﬁnal
common pathway for all neural drive to the skel-
etal musculature. Each motoneuron innervates
one or more muscle ﬁbers and the cell bodies of
motoneurons are somatotopically organized in
both vertebrates and invertebrates. Motoneurons
were among the ﬁrst neurons to be studied in great
detail by experimentalists largely due to their
unique position as output neurons of the nervous
system. The rich tradition of the experimental
work dates back to the early 1900s beginning
with the historical work by Sir John Eccles
(1903–1997)
and
Sir
Charles
Sherrington
(1857–1952), followed by a host of physiologists
to date whose work has laid the foundations for
realistic models of motoneurons.
Like many neurons in the nervous system,
motoneurons show repetitive ﬁring behavior in
response to excitation. However, like no other
neurons in the nervous system, their frequency
of spike discharge directly dictates the degree of
muscle contraction. In this way, they mediate con-
trol of complex behaviors. At the cellular level,
they possess complex morphologies that strongly
inﬂuence their integrative function. They house a
rich assortment of intrinsic membrane-bound ion
channels that dynamically interact with a wide
range of synaptic inputs to produce unique non-
linearities in their membrane properties. The het-
erogeneity of cellular properties within a motor
pool further confers complex population dynam-
ics. Taken together, motoneurons are an important
class of neurons in the nervous system that are
attractive candidates for modeling in order to gain
a deeper understanding of the neural basis of
motor control.
In this section, we discuss principles and meth-
odologies used to develop models of motoneurons
and motor effectors at various levels of complexi-
ties and scales. At the single cell level, articles
presented
describe
two-compartment
models
(▶Algorithmic Reconstruction of Motoneuron
Morphology), more complex multi-compartment
models (▶Brainstem Motoneurons, Models of),
and
morphologically
realistic
models
(▶Compartmental Models of Spinal Motoneu-
rons). Among vertebrate motoneurons, we explic-
itly
address
models
of
spinal
(▶Brainstem
Motoneurons, Models of) and brainstem motoneu-
rons (▶Computational Models of Motor Pools).
We further describe novel algorithms that can gen-
erate realistic motoneuron morphologies given a
simple set of experimentally derived morphometric
parameters
(▶Conductance-Based
Models
of
Nonlinear Dynamics in Vertebrate Motoneurons).
At the population level, we describe motor pool
models
(▶Mammalian
Motor
Nerve
Fibers,
Models of). We also describe the computational
principles underlying models of the muscles
(▶Morphologically
Detailed
Motoneuron
Models). Since motor ﬁbers traverse long distances
to provide electrochemical signals for these effec-
tors, models of motor nerve ﬁbers and propagation
82
Motoneurons and Neuromuscular Systems: Overview

of electrical signals along them are described
(▶Neuromuscular Control Systems, Models of).
Lastly, systems-level models of the neural control
systems that enable us to perform tasks by provid-
ing suitable input to motoneurons are described
(▶Physiology and Computational Principles of
Muscle Force Generation).
Cross-References
▶Algorithmic Reconstruction of Motoneuron
Morphology
▶Brainstem Motoneurons, Models of
▶Compartmental Models of Spinal Motoneurons
▶Computational Models of Motor Pools
▶Conductance-Based
Models
of
Nonlinear
Dynamics in Vertebrate Motoneurons
▶Mammalian Motor Nerve Fibers, Models of
▶Morphologically Detailed Cellular and Pool
Motoneuron Models
▶Neuromuscular Control Systems, Models of
▶Physiology and Computational Principles of
Muscle Force Generation
Multistability in
Neurodynamics: Overview
Gennady Cymbalyuk
The Neuroscience Institute, Georgia State
University, Atlanta, GA, USA
Synonyms
Bistability; Coexistence; Fold bifurcation; Hyster-
esis; Subcritical bifurcation
Definition
Multistability in neurodynamics is the coexis-
tence of two or more observable regimes of activ-
ity, i.e., attractors, in the phase space of a
neuronal system. In the absence of noise or per-
turbation, the neuronal system permanently
exhibits one of the regimes. Multistability sug-
gests that by an appropriate choice of perturba-
tion or by resetting the initial state of the system,
one could induce a switch from one regime into
another.
Detailed Description
Multistable neuronal system can exhibit two or
more regimes of activity, depending on its initial
state. Both isolated neurons and neuronal net-
works can exhibit coexistence of several activity
regimes. The coexistence of silence, subthreshold
oscillations, tonic spiking, and bursting regimes
with each other has been observed in a number of
theoretical
and
experimental
studies.
A multistable neuronal system can show long-
lasting responses to short transient signals. Such
systems also may respond to perturbations in a
state-dependent fashion, demonstrating hystere-
sis, which is the hallmark of (Nadim et al. 2008).
Multistability has clear implications for dynami-
cal memory and information processing in neu-
rons (Marder et al. 1996; Egorov et al. 2002;
Durstewitz and Seamans 2006). In the area of
motor control, multistability could be a major
mechanism of operation of multifunctional central
pattern generators (Getting 1989; Venugopal et al.
2007; Briggman and Kristan 2008). On the other
hand, multistability can be a pathological feature;
for example, seizure regimes can coexist with
normal regimes (Ziburkus et al. 2006; Cressman
et al. 2009; Frohlich et al. 2010). For the latter, the
electrogenic pump has been implicated to play a
crucial role (Cressman et al. 2009; Krishnan and
Bazhenov 2011).
Multistability can be categorized in terms of
the regimes of activity which coexist, e.g., coex-
istence of tonic spiking and silence (Rinzel 1978;
Guttman et al. 1980; Forger and Paydarfar 2004;
Hahn and Durand 2001), coexistence of bursting
and silence (Malashchenko et al. 2011a, b), coex-
istence of different bursting regimes (Wang 1994;
Butera 1998; Newman and Butera 2010), and
Multistability in Neurodynamics: Overview
83

coexistence of bursting activity and tonic spiking
(Canavier et al. 1994; Frohlich et al. 2010;
Cymbalyuk et al. 2002; Shilnikov et al. 2005;
Malashchenko et al. 2011a). These regimes can
coexist in the phase space of a neuronal system if
some unstable regime creates a barrier, i.e., a
separatrix demarcating a border between the
basins of attraction. The mechanism underlying
multistability can also be described and classiﬁed
in terms of the unstable regime that forms the
barrier (Rinzel 1978; Guttman et al. 1980). One
of the key ingredients of such descriptions is the
identiﬁcation of the transitions, i.e., bifurcations,
at which the unstable regimes appear and disap-
pear. Ubiquitously, a separating regime is either a
saddle equilibrium or a saddle orbit (Rinzel 1978;
Malashchenko et al. 2011a; Barnett et al. 2013;
Marin et al. 2013). For example, the stable man-
ifold of the saddle equilibrium separates tonic
spiking and silence observed in the simpliﬁed
model
of
the
cerebellar
Purkinje
neurons
(Loewenstein et al. 2005; Fernandez et al. 2007),
and the stable manifold of the saddle periodic
orbit is a cause of bistability of spiking and silence
in the giant squid axon (Rinzel 1978; Hahn and
Durand 2001).
The methods developed in the bifurcation the-
ory allow one to investigate stability of, evolu-
tion of, and transitions between the silent and
oscillatory
regimes
of
neuronal
models
in
response to variation of the system’s parameters
(Kuznetsov 2004; Terman and Ermentrout 2010;
Izhikevich 2010). Application of these methods
identiﬁes
and
explains
the
mechanisms
supporting multistability under normal and path-
ological conditions (Rinzel 1978; Guttman et al.
1980; Forger and Paydarfar 2004; Gutkin et al.
2009; Hahn and Durand 2001; Krishnan and
Bazhenov 2011; Shilnikov et al. 2005; Fröhlich
and Bazhenov 2006; Cressman et al. 2009;
Cymbalyuk and Shilnikov 2005; Malashchenko
et al. 2011a, b).
Cross-References
▶Coexistence of Bursting Regimes
▶Multistability Arising from Synaptic Dynamics
▶Multistability in Perception Dynamics
▶Multistability in Seizure Dynamics
▶Multistability of Coupled Neuronal Oscillators
Acknowledgments This work was supported by National
Science Foundation grant PHY-0750456.
References
Barnett WH, O’Brien G, Cymbalyuk GS (2013) Bistability
of silence and seizure-like bursting. J Neurosci
Methods 220(2):179–189
Briggman KL, Kristan WB (2008) Multifunctional pattern-
generating circuits. Annu Rev Neurosci 31:271–294
Butera RJ (1998) Multirhythmic bursting. Chaos 8:274–284
Canavier CC, Baxter DA, Clark JW, Byrne JH (1994) Mul-
tiple modes of activity in a model neuron suggest a
novel mechanism for the effects of neuromodulators.
J Neurophysiol 72:872–882
Cressman JR Jr, Ullah G, Ziburkus J, Schiff SJ, Barreto
E (2009) The inﬂuence of sodium and potassium
dynamics on excitability, seizures, and the stability of
persistent states: I. Single neuron dynamics. J Comput
Neurosci 26:159–170
Cymbalyuk G, Shilnikov A (2005) Coexistence of tonic
spiking oscillations in a leech neuron model. J Comput
Neurosci 18:255–263
Cymbalyuk GS, Gaudry Q, Masino MA, Calabrese RL
(2002) Bursting in leech heart interneurons: cell-
autonomous
and
network-based
mechanisms.
J Neurosci 22:10580–10592
Durstewitz D, Seamans JK (2006) Beyond bistability: bio-
physics and temporal dynamics of working memory.
Neuroscience 139:119–133
Egorov AV, Hamam BN, Fransen E, Hasselmo ME, Alonso
AA (2002) Graded persistent activity in entorhinal
cortex neurons. Nature 420:173–178
Fernandez FR, Engbers JD, Turner RW (2007) Firing
dynamics of cerebellar purkinje cells. J Neurophysiol
98:278–294. https://doi.org/10.1152/jn.00306.2007
Forger DB, Paydarfar D (2004) Starting, stopping, and
resetting biological oscillators: in search of optimum
perturbations. J Theor Biol 230:521–532
Fröhlich F, Bazhenov M (2006) Coexistence of tonic ﬁring
and bursting in cortical neurons. Phys Rev E 74:031922
Frohlich F, Sejnowski TJ, Bazhenov M (2010) Network
bistability mediates spontaneous transitions between
normal and pathological brain states. J Neurosci 30:
10734–10743
Getting PA (1989) Emerging principles governing the
operation of neural networks. Annu Rev Neurosci 12:
185–204
Gutkin BS, Jost J, Tuckwell HC (2009) Inhibition of rhyth-
mic neural spiking by noise: the occurrence of a
minimum in activity with increasing noise. Naturwis-
senschaften 96:1091–1097
84
Multistability in Neurodynamics: Overview

Guttman R, Lewis S, Rinzel J (1980) Control of repetitive
ﬁring in squid axon membrane as a model for a
neuroneoscillator. J Physiol 305:377–395
Hahn PJ, Durand DM (2001) Bistability dynamics in
simulations of neural activity in high-extracellular-
potassium conditions. J Comput Neurosci 11:5–18
Izhikevich EM (2010) Dynamical systems in neuroscience:
the geometry of excitability and bursting. The MIT
Press, Cambridge, MA/London
Krishnan GP, Bazhenov M (2011) Ionic dynamics mediate
spontaneous termination of seizures and postictal
depression state. J Neurosci 31:8870–8882
Kuznetsov YA (2004) Elements of applied bifurcation
theory. Springer, New York
Loewenstein Y, Mahon S, Chadderton P, Kitamura K,
Sompolinsky
H,
Yarom
Y,
Hðusser
M
(2005)
Bistability of cerebellar Purkinje cells modulated by
sensory stimulation. Nat Neurosci 8:202–211
Malashchenko T, Shilnikov A, Cymbalyuk G (2011a) Six
types of multistability in a neuronal model based on
slow calcium current. PLoS One 6:e21782
Malashchenko T, Shilnikov A, Cymbalyuk G (2011b)
Bistability of bursting and silence regimes in a model
of a leech heart interneuron. Phys Rev E 84:041910
Marder E, Abbott LF, Turrigiano GG, Liu Z, Golowasch
J (1996) Memory from the dynamics of intrinsic mem-
brane currents. Proc Natl Acad Sci U S A 93:
13481–13486
Marin BM, Barnett WH, Doloc-Mihu A, Calabrese RL,
Cymbalyuk GS (2013) High prevalence of multi-
stability of rest states and bursting in a database of a
model neuron. PLoS Comput Biol 9(3):e1002930.
https://doi.org/10.1371/journal.pcbi.1002930
Nadim F, Brezina V, Destexhe A, Linster C (2008) State
dependence of network output: modeling and experi-
ments. J Neurosci 28:11806–11813
Newman JP, Butera RJ (2010) Mechanism, dynamics, and
biological existence of multistability in a large class of
bursting neurons. Chaos 20:023118
Rinzel J (1978) On repetitive activity in nerve. Fed Proc
37:2793–2802
Shilnikov A, Calabrese RL, Cymbalyuk G (2005) Mecha-
nism of bistability: tonic spiking and bursting in a
neuron model. Phys Rev E Stat Nonlinear Soft Matter
Phys 71:056214
Terman DH, Ermentrout GB (2010) Mathematical founda-
tions of neuroscience. Springer, New York/Dordrecht/
Heidelberg/London
Venugopal
S,
Travers
JB,
Terman
DH
(2007) A computational model for motor pattern
switching between taste-induced ingestion and rejec-
tion oromotor behaviors. J Comput Neurosci 22:
223–238
Wang XJ (1994) Multiple dynamical modes of thalamic
relay neurons: rhythmic bursting and intermittent
phase-locking. Neuroscience 59:21–31
Ziburkus
J,
Cressman
JR,
Barreto
E,
Schiff
SJ
(2006) Interneuron and pyramidal cell interplay dur-
ing in vitro seizure-like events. J Neurophysiol 95:
3948–3954
Neural Population Models and
Cortical Field Theory:
Overview
Ingo Bojak
School of Systems Engineering, University of
Reading, Reading, UK
The term neural population models (NPMs) is
used here as catchall for a wide range of
approaches that have been variously called neural
mass models, mean ﬁeld models, neural ﬁeld
models, bulk models, and so forth. All NPMs
attempt to describe the collective action of neural
assemblies directly. Some NPMs treat the densely
populated tissue of cortex as an excitable medium,
leading to spatially continuous cortical ﬁeld theo-
ries (CFTs). An indirect approach would start by
modelling individual cells and then would explain
the collective action of a group of cells by cou-
pling many individual models together. In con-
trast, NPMs employ collective state variables,
typically deﬁned as averages over the group of
cells, in order to describe the population activity
directly in a single model. The strength and the
weakness of his approach are hence one and the
same: simpliﬁcation by bulk. Is this justiﬁed and
indeed useful, or does it lead to oversimpliﬁcation
which fails to capture the phenomena of interest?
In ▶“Mesoscopic Anatomy and Neural Popu-
lation Models,” Liley explains that the anatomy of
the brain is organized into connected modules at
the mesoscopic level, which can serve as a bio-
logical substrate for NPMs. In ▶“Neuroper-
colation and Neural Population Models” and
▶“Neural Mass Action,” Kozma describes how
this undergirds coherent neural activity. Brunel
and Hakim demonstrate in ▶“Population Density
Model” and ▶“Fokker-Planck Equation” how
NPMs can be derived mathematically from
existing descriptions of individual cells, and in
▶“Stochastic Neural Field Theory,” Bressloff
shows this for the CFT variant as well. Liley
then introduces the general NPM approach in
▶“Neural Population Model,” and Hutt the CFT
variant in ▶“Neural Field Model, Continuum.”
Neural Population Models and Cortical Field Theory: Overview
85

Some historically important and/or particularly
popular NPMs are showcased in ▶“Amari
Model” by Potthast, ▶“Down Under Neural Pop-
ulation Models” by Liley, ▶“Jansen-Rit Model”
by Knösche, and ▶“Wilson-Cowan Model” by
Kilpatrick.
The modelling of connectivity in NPMs has
been a topic of considerable development over the
years and is covered in ▶“Propagator, Axonal” and
▶“Synaptic Connectivity in Neural Population
Models” by Jirsa, as well as in ▶“Gap Junctions,
Neural Population Models and” by Sleigh and the
Steyn-Rosses. The dynamical properties of NPMs
are key for their application to various brain phe-
nomena. The quasi-linear and noise-driven mode is
described in ▶“Transfer Function, Electrocortical”
by Molaee-Ardekani and Wendling. Transitions in
dynamics, for example, to self-sustained oscilla-
tions, are discussed in ▶“Bifurcations, Neural Pop-
ulation Models and” by Knösche and ▶“Phase
Transitions, Neural Population Models and” by
Sleigh and the Steyn-Rosses. Whereas the emer-
gence of spatial structure is presented in ▶“Pattern
Formation in Neural Population Models” by Hutt
and the chaotic regime in ▶“Chaos, Neural Popu-
lation Models and” by Knösche.
A fundamental strength of NPMs is their abil-
ity to predict the data generated by neuroimaging
modalities like the electro- and magnetoence-
phalogram (EEG/MEG) and functional magnetic
resonance imaging (fMRI), as described in
▶“Neuroimaging, Neural Population Models
for” by Bojak and Breakspear. This allows the
comparison of model predictions with experimen-
tal data, be it for normal brain function as in
▶“Gamma Rhythm, Neural Population Models
of the” by Bojak and ▶“Sleep, Neural Population
Models of” by Phillips, under the inﬂuence of
drugs as in ▶“Anesthesia, Neural Population
Models of” by Sleigh and the Steyn-Rosses or in
disease as in ▶“Epilepsy, Neural Population
Models of” by Wendling and Molaee-Ardekani.
While many of the methods used to predict the
recorded signals are sound and even sophisti-
cated, some of the basics still require substantial
improvements,
as
Einevoll
reminds
us
in
▶“Extracellular Potentials, Forward Modeling
of” Much of the work so far has been in forward
prediction with NPMs; however, increasingly it is
important to estimate the NPM state and parame-
ters directly from the data. This is elucidated by
Moran in ▶“Dynamic Causal Modelling with
Neural Population Models” and by Potthast in
▶“Inverse
Problems
in
Neural
Population
Models.” Finally, it is exciting that NPMs, and
CFTs in particular, can be used to model percep-
tion and motor control, as Schöner describes in
▶“Embodied Cognition, Dynamic Field Theory
of,” with additional remarks by Schöner and
Nowak given in ▶“Coordination Dynamics.”
To answer the question that was posed in the
beginning, not only is the NPM approach justiﬁ-
able and has shown itself to be useful; we expect
that within a decade or two, NPMs will take their
rightful place as the primary means for describing
mesoscopic brain activity. The central problem we
face at this description level will not be to simulate
millions of brain cells in a reasonable amount of
time; the central problem is now, and will be then,
that we cannot specify the properties and interac-
tions of so many brain cells in a biologically
meaningful manner and cannot generate actual
human insight into principles of function from
the plethora of individual cell activities. Once
the computational power becomes readily avail-
able, this realization will be unavoidable. The way
forward will be multi-scale descriptions wherein
higher levels discard irrelevant detail of lower
levels for an effective and efﬁcient description
that remains accessible to the human mind. And
given their intimate connection to (noninvasive)
neuroimaging, we expect that NPMs will play a
privileged role in such a future scheme. This sec-
tion provides a mere snapshot of a ﬁeld that is
currently growing rapidly and will continue to do
so in the foreseeable future.
Cross-References
▶Amari Model
▶Anesthesia, Neural Population Models of
▶Bifurcations, Neural Population Models and
▶Chaos, Neural Population Models and
▶Coordination Dynamics
86
Neural Population Models and Cortical Field Theory: Overview

▶Down Under Neural Population Models
▶Dynamic Causal Modelling with Neural Popu-
lation Models
▶Embodied Cognition, Dynamic Field Theory of
▶Epilepsy, Neural Population Models of
▶Extracellular Potentials, Forward Modeling of
▶Fokker-Planck Equation
▶Gamma Rhythm, Neural Population Models
of the
▶Gap Junctions, Neural Population Models and
▶Inverse Problems in Neural Population Models
▶Jansen-Rit Model
▶Mesoscopic Anatomy and Neural Population
Models
▶Neural Field Model, Continuum
▶Neural Mass Action
▶Neural Population Model
▶Neuroimaging, Neural Population Models for
▶Neuropercolation
and
Neural
Population
Models
▶Pattern Formation in Neural Population Models
▶Phase Transitions, Neural Population Models
and
▶Population Density Model
▶Propagator, Axonal
▶Sleep, Neural Population Models of
▶Stochastic Neural Field Theory
▶Synaptic Connectivity in Neural Population
Models
▶Transfer Function, Electrocortical
▶Wilson-Cowan Model
Neuromodulation: Overview
Christiane Linster
Computational Physiology Lab, Department of
Neurobiology and Behavior, Cornell University,
Ithaca, NY, USA
Definition
Neuromodulation refers to the regulation of neural
and synaptic function by regulatory extrinsic or
intrinsic substances.
Detailed Description
Computational modeling of neural substrates pro-
vides an excellent theoretical framework for the
understanding of the computational roles of
neuromodulation.
Neuromodulation
can
be
deﬁned as biophysical processes that serve to
modify
–
or
modulate
–
the
computation
performed by a neuron or network as a function
of task demands and behavioral state of the ani-
mal. These modulatory effects often involve sub-
stances
such
as
acetylcholine
(ACh),
norepinephrine
(NE),
histamine,
serotonin
(5-HT), dopamine (DA), and a variety of neuro-
peptides. Modulatory effects are difﬁcult to
deﬁne, because they often originate from different
structures and have different spatial distributions
and time courses of action. Because of the wider
use of modeling techniques and growing interest
in systems neuroscience, the computational role
of neuromodulation in information processing has
helped elucidate neuromodulatory function, and
predictive theories have arisen from computa-
tional approaches.
Neuromodulation can be described by its spa-
tial and temporal characteristics, as well as the
speciﬁc
computational
function
ascribed
to
it. Spatial characteristics include extrinsic, origi-
nating from an area extrinsic to the network
under study, or intrinsic, originating from pro-
cesses within the area under study. The compu-
tational functions of extrinsic neuromodulation,
such as ACh, NE, 5HT, and DA, are usually
considered somewhat global, because they mod-
ulate many areas of the brain simultaneously.
Classically, ACh has been associated with atten-
tional processes, NE with signal-to-noise modu-
lation, DA with reward learning, and 5HT with
sleep-wake transitions. In other cases, modula-
tion is speciﬁc to the network under investigation
and
an
integral
part
of
the
computations
performed within. Second messenger systems,
plasticity processes, and gene regulation are
examples of such intrinsic modulation. From a
functional point of view, neuromodulation is
often regulatory, for example, in the cases of
second messenger systems or activity-dependent
regulation of conductances. In the sensory
Neuromodulation: Overview
87

systems, neuromodulation is often linked to
tuning of receptive ﬁelds (ACh) and regulation
of signal-to-noise ratio. A third highly important
function of most neuromodulators is the regula-
tion of plasticity, via excitability of neurons, syn-
aptic plasticity, and broader modulation of
network dynamics.
Exactly how neuromodulation is integrated in
computational studies depends widely on the
details of implementation of the computational
model itself. Effects of neuromodulators can be
implemented from the detailed biophysical level,
to broader regulation of network parameters in
case of more abstract large-scale models. For
example, speciﬁc effects on voltage-gated chan-
nels may be implemented in a biophysical model
by changing channel parameters, in a simpliﬁed
integrate and ﬁre model by changing a related
parameter such as ﬁring threshold. Each study
chooses the level of detail appropriate to the ques-
tion asked and data available. For a comprehen-
sive review of levels of implementation of
neuromodulation in computational models, we
refer the reader to Fellous and Linster (1998).
The chapters in this section cover overviews of
neuromodulatory computation divided by sub-
stance, nature of task, as well as overviews of
types of network implementations and speciﬁc
examples.
Cross-References
▶Computation with Dopaminergic Modulation
▶Computation with Serotonergic Modulation
▶Computational Models of Modulation of Oscil-
latory Dynamics
▶Computational Models of Neuromodulation
References
Fellous JM, Linster C (1998) Computational models of
neuromodulation. Neural Comput 10(4):771–805
Further Reading
Dayan P (2012) Twenty-ﬁve lessons from computational
neuromodulation. Neuron 76(1):240–256. https://doi.
org/10.1016/j.neuron.2012.09.027
Neuromorphic Engineering:
Overview
Sylvie Renaud
Institut Polytechnique de Bordeaux, Université de
Bordeaux, Talence, France
Definition
Neuromorphic engineering is a recent interdisci-
plinary ﬁeld involving biologists, physicists,
mathematicians, computer scientists, and engi-
neers to design hardware/physical models of neu-
ral systems. It aims at designing silicon-based
neural systems for computational or biomedical
purposes. The term “neuromorphic” relates to the
computational architecture, shaped to model bio-
logical neural systems: neuromorphic engineering
is by essence strongly linked to computational
neuroscience.
Detailed Description
Neuromorphic engineering offers an interesting
alternative to computer simulations of neural net-
works: while main components of computers are
high-precision high-speed digital hardware with
high-power dissipation, neural components are
slow
asynchronous
computational
elements
which use a combination of analog and digital
signal representations. Neuromorphic systems
are hardware implementations that operate in
physical time; they are inspired by the structure,
function, and plasticity of biological nervous sys-
tems. Their design is facilitated by similarities
between VLSI (very large-scale integrated cir-
cuits) hardware and neural bioware.
Since
decades,
researchers
have
been
implementing electronic models of neural circuits,
following the long history of artiﬁcial neural net-
works
that
started
in
the
1950s.
Original
neuromorphic circuits (Mead 1989) were VLSI
systems that used the nonlinear and graded prop-
erties of transistors to emulate protein channels in
neurons. More recently the term neuromorphic
88
Neuromorphic Engineering: Overview

has been used to describe analog, digital, and
mixed analog-digital VLSI as models of neural
computation.
Neuron model computation can be either ana-
log, with a continuous-time and continuous-value
representation, or digital using numerical algo-
rithms. In both cases, neuromorphic devices clas-
sically use spiking (asynchronous binary signal
analogs to neural spikes) representation for neuro-
nal communication and learning mechanisms. At
the neural network level, synaptic modiﬁcation
mechanisms are usually introduced to model learn-
ing and adaptation in neural systems. Furthermore,
similar to robustness of neural networks to changes
in the environment, dynamic synaptic modiﬁcation
in neuromorphic systems allows gaining a remark-
able robustness against transistor-level variations.
Whatever
the
technological
support
of
the
neuromorphic electronics, real-time computation
is mandatory for systems interacting with a biolog-
ical environment (sensory or motor).
To date, widely known neuromorphic systems
are highly integrated VLSI that emulate sensory
transduction (vision, olfaction, audition) or more
sophisticated and multimodal neural systems like
the head-direction system. Larger neuromorphic
systems emerged in the last decade, emulating
complex or cognitive biological functions and
suggesting new research directions for robotics
by the means of biomorphic engineering.
According to the VLSI technology road map,
promising low-power and high-density nanoscale
technologies,
including
organic
electronics,
memristors, or 3D technologies, are expected to
fulﬁll the need for large-scale neuromorphic sys-
tems,
currently
implemented
using
standard
CMOS VLSI technologies. Large neuromorphic
systems have performance measures (high den-
sity, high speed, low power) that are good enough
to enable the study of large-scale and biologically
inspired neural networks. Such large networks
represent a challenge for computational neurosci-
ence. Researchers have recently started investigat-
ing cognitive neuromorphic systems that are able
to perform adaptive behaviors and that merge
neural ﬁelds and single-neuron representations
courtesy of the increased computational power
of microelectronic devices.
Entries in this section review state-of-the-art
neuromorphic engineering computational princi-
ples, technologies, and applications. This trans-
disciplinary research ﬁeld, at its heart, combines
the physics of electronics and biology and algo-
rithms and then engineers it for solving tasks.
Despite the many challenges, neuromorphic
engineering may hold great promises for a new
generation of medicine or industry technologies.
Cross-References
▶Brain-Machine Interface: Overview
▶Dynamical Systems: Overview
▶Neuromorphic Cognition
▶Neuromorphic Hardware, Large-Scale
▶Neuromorphic Sensors, Cochlea
▶Neuromorphic Sensors, Head Direction
▶Neuromorphic Sensors, Olfaction
▶Neuromorphic Sensors, Vision
▶Neuromorphic Technologies, Memristors
▶Spike-Timing Dependent Plasticity, Learning
Rules
▶Spiking
Network
Models
and
Theory:
Overview
▶Synaptic Dynamics: Overview
Reference
Mead C (1989) Analog VLSI and neural systems.
Addison-Wesley, Reading
Neuronal Model
Optimization: Overview
Astrid A. Prinz
Department of Biology, Emory University,
Atlanta, GA, USA
Synonyms
Neuronal model hand-tuning
Neuronal Model Optimization: Overview
89

Definition
Neuronal Model Parameter Optimization is the
process of adjusting the parameters of a computa-
tional model of a neuron or neuronal network in
order to achieve model activity that mimics that of
the living neuron or network being modeled.
Detailed Description
This section contains entries that explain the need
for neuron and network model parameter optimi-
zation, discuss various optimization methods,
describe existing software tools for optimization
and visualization of the model databases that
result from some of the optimization methods,
and discuss related concepts that have emerged
from model optimization and exploration efforts
over the last few years.
Why Do Neuronal Model Parameters
Need to Be Optimized?
Computational models of neurons and neuronal
circuits are important investigative tools that allow
the study of neuronal signaling and information
processing mechanisms that would not be experi-
mentally accessible.However, constructing a model
neuron or network requires the speciﬁcation of
numerous parameters, such as maximal conduc-
tances, half-activation voltages, and time constants
of ionic membrane currents, as well as the strength
and dynamical properties of synapses. A complete
set of parameters to describe a given neuron or
network is virtually never available from experi-
mental data, because some of the parameters cannot
be directly measured. Combining parameters mea-
sured from different animals is also problematic
because almost all parameters vary between ani-
mals, and the distributions in the space of parameter
sets obtained from different animals are often non-
convex, leading to ▶“Failure of Averaging,” i.e.,
the problem that the set of parameter averages
obtained from a distribution of parameter sets can
fall outside that distribution in parameter space.
Neuronal models initially constructed from
incomplete and averaged parameter sets therefore
usually generate activity that does not match that
of the living system to be modeled. Such
dysfunctional models then require parameter opti-
mization, either through ▶“Neuronal Model
Hand-Tuning” by an expert employing educated
guessing and trial-and-error parameter adjust-
ments, or through one of the optimization
methods described in this section.
Neuronal Model Optimization Methods
One approach that allows the identiﬁcation of
neuronal model parameter regimes that produce
qualitatively different model behaviors (for exam-
ple, spiking versus bursting versus silence) is
▶“Bifurcation Analysis,” i.e., the use of dynam-
ical systems analysis to determine critical param-
eters that cause transitions from one behavior to
another. Bifurcation analysis is most powerful for
models of low dimensionality. For models with
many dynamic variables and parameters, the use
of bifurcation analysis may therefore require prior
model simpliﬁcation through ▶“Neuronal Model
Reduction,” ideally without the loss of essential
features of model activity.
A more agnostic approach to studying the
dependence of neuronal model activity on the
underlying model parameters is ▶“Neuronal
Parameter Space Exploration,” i.e., the brute-force
computational exploration of many – often tens of
millions – of parameter combinations that cover the
model’s high-dimensional parameter space. Char-
acteristics of the simulated model activity for each
parameter combination are then stored in ▶“Neu-
ronal Model Databases” that can be mined for
model versions with the desired activity, and to
address general questions about the relationship
between model parameters and output.
Less exploratory, more targeted model optimi-
zation methods are based on ▶“Evolutionary
Algorithms.” They require the deﬁnition of a
▶“Neuronal Model Output Fitness Function,”
which is a measure for how well a neuron or
network model’s activity replicates that of the
living system to be modeled. ▶“Multi-objective
Evolutionary Algorithms” constitute a class of
evolutionary
algorithms
that
simultaneously
90
Neuronal Model Optimization: Overview

optimize models to meet several different objec-
tives, rather than maximizing a single ﬁtness mea-
sure. An implementation of evolutionary neuronal
model optimization is available in the software
tool ▶“Neuroﬁtter.”
Finally, ▶“Hybrid Parameter Optimization
Methods” combine several of the above methods
of neuronal model optimization to beneﬁt from
their respective advantages.
Visualization of Neuronal Model
Parameter Spaces
Several
of
the
model
optimization
methods
described in this section produce long lists of
parameter sets and their corresponding neuronal
model activity that are often collected in ▶“Neu-
ronal Model Databases.” Because the underlying
models and their parameter spaces are high-
dimensional, it is difﬁcult to analyze how the activ-
ity of a model varies across parameter space. This
problem is addressed by recently developed
methods of ▶“Neuronal Parameter Space Visuali-
zation,” including the visualization tool ▶“NDVis-
Neuro,” which is designed for the visualization of
model databases that cover parameter space with a
regular grid of parameter sets.
Beyond Optimization
A number of recent neuronal model optimization
efforts, as well as a growing body of experimental
data, have revealed concepts that go beyond model
optimization in the sense of identifying a single
“optimal” model parameter set. Neuronal models
and the living systems they mimic display ▶“Neu-
ronal Parameter Non-uniqueness,” meaning that dif-
ferent parameter sets can produce similar and
functional neuron or network activity. The subset of
parameter space that contains such functional param-
eter sets is called the “Neuronal Solution Space.”
Analysis of the structure of neuronal solution spaces
has revealed that parameters do not necessarily vary
independently within the solution space, but often
show ▶“Neuronal Parameter Co-regulation.”
Apart from asking whether a given parameter
set does or does not support functional model
activity, parameter space exploration also allows
for the analysis of ▶“Neuronal Parameter Sensi-
tivity,” that is, in which direction and how
strongly activity characteristics such as spike fre-
quency depend on model parameters.
Together,
the
realization
of
▶“Neuronal
Parameter Non-uniqueness” and the connected
concept of “Neuronal Solution Space” have led
to the strategy of “Ensemble Modeling,” i.e., the
approach of analyzing an entire ensemble of
parameter sets and model versions that produce
functional activity, rather than focusing on a sin-
gle “optimal” model version.
Cross-References
▶Bifurcation Analysis
▶Evolutionary Algorithms
▶Failure of Averaging
▶Hybrid Parameter Optimization Methods
▶Multi-objective Evolutionary Algorithms
▶NDVis–Neuro
▶Neuroﬁtter
▶Neuronal Model Databases
▶Neuronal Model Hand-Tuning
▶Neuronal Model Output Fitness Function
▶Neuronal Model Reduction
▶Neuronal Parameter Co-regulation
▶Neuronal Parameter Non-uniqueness
▶Neuronal Parameter Sensitivity
▶Neuronal Parameter Space Exploration
▶Neuronal Parameter Space Visualization
Olfaction: Overview
Francesco Cavarretta1 and Christiane Linster2
1Department of Biology, Emory University,
Atlanta, GA, USA
2Computational Physiology Lab, Department of
Neurobiology and Behavior, Cornell University,
Ithaca, NY, USA
Definition
Olfaction refers to the sense of smell and the
perception of odorants. This process involves
Olfaction: Overview
91

multiple peripheral and central structures of the
nervous system, i.e., the olfactory pathway, and
ultimately results in the creation of odor images in
the brain as well as the generation of motor and
behavioral responses. In several decades, olfac-
tion has been investigated experimentally and the-
oretically, integrating physiological, behavioral,
and computational studies. Here we focus on
computational models of olfactory processing,
showing how they can promote an understanding
of olfaction that goes beyond the mere collection
of experimental data.
Detailed Description
In natural environments, airborne chemicals
stimulate
continually
the
olfactory
system,
with unpredictable occurrence in time and
space. The olfactory system enables the detec-
tion of these odorants out of a chemically noisy
environment, extracting relevant odor features,
and making a comparison with previously mem-
orized odors.
The multiple circuits forming the olfactory sys-
tem are thought to contribute distinct computations
to odor processing. Figure 1 shows a schematic
representation of the most prominent olfactory cir-
cuits in mammals. The short-axon (SA), external
tufted (ET), and periglomerular (PG) cells form a
subnetwork in the juxtaglomerular area, which has
been proposed to transform a disorganized odor
representation into a concentration invariant and
contrast-enhanced one (Cleland and Sethupathy
2006; Cleland 2010). In the next layer, mitral
(Mi) and granule (Gr) cells form a reciprocal
Distributed
patterns
Amplitude invariance
Contrast  enhancement
Contrast  enhancement
nACh
Dynamics and
synchronization
Signal-to-noise
ratio
Fast adaptation and
short term memory
Associative memory
Pyr
Gr
Mi
SA
ET
PG
OSN
Olfaction: Overview,
Fig. 1 Schematic
description of olfactory
structures. The olfactory
sensory neurons (OSN)
expressing a common
receptor converge onto the
common olfactory
glomeruli, where they make
excitatory synapses with the
mitral (Mi), periglomerular
(PG), and external tufted
(ET) cells. ET, PG, and
superﬁcial short-axon
(SA) cells are
interconnected, forming a
subnetwork in the
glomerular layer. The high
convergence between OSNs
and glomerular cells can
increase signal-to-noise
ratio in the system (Linster
and Cleland 2009). PG cells
make inhibitory
connections with Mi cell
primary dendrites, allowing
for contrast enhancement of
the olfactory signals ahead
of the Mi cell response
(Cleland and Sethupathy
2006)
92
Olfaction: Overview

excitatory/inhibitory
loop,
which
has
been
hypothesized to implement the neural substrate
for fast oscillations and synchrony (Freeman
1974; Brea et al. 2009). Mi cell axons project to
secondary olfactory structures, including the
piriform cortex, in a non-topographically orga-
nized fashion. In the cortex, Mi axons make distal
synapses with pyramidal (Pyr) cells, displaying
short-term plasticity (Wilson and Linster 2008).
The extensive excitatory connections between
cortical pyramidal (Pyr) cells, known to undergo
synaptic plasticity, have long been proposed to
implement an associative memory for odor stor-
age (Hasselmo et al. 1990).
Computational models of olfactory processing
have provided mechanistic and functional insights
into odor perception and learning. Mechanistical
models have long focused on generation and
maintenance of fast oscillations in the olfactory
system, and particularly in the gamma range (e.g.,
Freeman 1974; Lagier et al. 2004; Li and Cleland
2013). Functional models have focused on com-
putations such as signal-to-noise ratio modulation
(e.g., Hasselmo et al. 1997; Linster et al. 2011),
contrast enhancement (e.g., Linster and Hasselmo
1997; Urban 2002; Cleland and Sethupathy 2006;
Cavarretta et al. 2016), and odor learning (e.g.,
Hasselmo et al. 1990; Wilson and Linster 2008;
Cavarretta et al. 2016, 2018). A prominent sub-
class
of
functional
models
consists
of
biophysically detailed models, which accounts
for diverse neuron types and mechanisms (e.g.,
Li and Cleland 2013; Gilra and Bhalla 2015;
Cavarretta et al. 2018). Many models have
focused on linking neural and perceptual pro-
cesses (e.g., Mandairon et al. 2006), while others
have stayed abstract and predictive (e.g., Brea
et al. 2009).
In this section, most entries are about compu-
tational modeling of the olfactory bulb. Here,
different models address the biophysical mecha-
nisms and network properties of the system (see
entries
▶“Biophysical
Models
of
Olfactory
Mitral and Granule Cells”, ▶“Olfactory Compu-
tation in Mitral-Granule Cell Circuits”, ▶“Large-
Scale Models of the Olfactory Bulb”, and
▶“Olfactory
Computation
in
Glomerular
Microcircuits”), and explain how glomerular and
granule cell microcircuits organize odor response
in speciﬁc patterns of spiking activity (see entries
▶“Olfactory Computation in Mitral-Granule Cell
Circuits”, ▶“Large-Scale Models of the Olfac-
tory Bulb”, and ▶“Olfactory Computation in
Glomerular Microcircuits”). Similarly, one entry
is dedicated to a model of the insect olfactory
system that gives insight into its basic functioning
(see ▶“Olfactory Computation in Antennal Lobe
and Mushroom Bodies”). The olfactory sensory
neurons are treated in one entry, where a mathe-
matical model is proposed to predict the dose-
response relationship for generic odor mixtures
(see ▶“Olfactory Sensory Neurons to Odor Stim-
uli: Mathematical Modeling of the Response”).
All these entries are then complemented by a
short review on information processing in the
olfactory bulb (see ▶“Information Processing in
the Olfactory Bulb”), and a more speculative entry
on the functional implication of neurogenesis in
these olfactory areas (see ▶“Olfactory Computa-
tion and Adult Neurogenesis”). The olfactory cor-
tex is discussed in the setting of associative
networks (see ▶“Olfactory Cortical Associative
Memory Models”), providing a review of the
existing literature. Furthermore, three examples
show of how to cellular and neuromodulatory
data can explain behavior (see ▶“Computational
Modeling of Olfactory Behavior”), integrating
computational modeling of olfactory areas and
behavioral data.
References
Brea JN, Kay LM, Kopell NJ (2009) Biophysical model for
gamma rhythms in the olfactory bulb via subthreshold
oscillations.
Proc
Natl
Acad
Sci
USA
106:
21954–21959
Cavarretta F, Marasco A, Hines ML, Shepherd GM,
Migliore M (2016) Glomerular and mitral-granule cell
microcircuits coordinate temporal and spatial informa-
tion processing in the olfactory bulb. Front Comput
Neurosci 10:67
Cavarretta F, Burton SD, Igarashi KM, Shepherd GM,
Hines ML, Migliore M (2018) Parallel odor processing
by mitral and middle tufted cells in the olfactory bulb.
Sci Rep 8:7625
Olfaction: Overview
93

Cleland TA (2010) Early transformations in odor represen-
tation. Trends Neurosci 33(3):130–139
Cleland TA, Sethupathy P (2006) Non-topographical contrast
enhancement in the olfactory bulb. BMC Neurosci 7:7
Freeman WJ (1974) Dynamic patterns of brain cell assem-
blies. IV. Mixed systems. Oscillating ﬁelds and pulse
distributions. Pulse-wave problems. Neurosci Res Pro-
gram Bull 12:102–107
Gilra A, Bhalla US (2015) Bulbar microcircuit model pre-
dicts connectivity and roles of interneurons in odor
coding. PLoS One 10:e0098045
Hasselmo ME, Wilson MA, Anderson BP, Bower JM
(1990) Associative memory function in piriform
(olfactory) cortex: computational modeling and neuro-
pharmacology. Cold Spring Harb Symp Quant Biol 55:
599–610
Hasselmo ME, Linster C, Patil M, Ma D, Cekic M (1997)
Noradrenergic suppression of synaptic transmission
may
inﬂuence
cortical
signal-to-noise
ratio.
J Neurophysiol 77:3326–3339
Lagier S, Carleton A, Lledo PM (2004) Interplay between
local GABAergic interneurons and relay neurons gen-
erates gamma oscillations in the rat olfactory bulb.
J Neurosci 24:4382–4392
Li G, Cleland TA (2013) A two-layer biophysical model of
cholinergic
neuromodulation
in
olfactory
bulb.
J Neurosci 33:3037–3058
Linster C, Cleland TA (2009) Glomerular microcircuits in
the olfactory bulb. Neural Netw 22(8):1169–1173
Linster C, Hasselmo M (1997) Modulation of inhibition in
a model of olfactory bulb reduces overlap in the neural
representation of olfactory stimuli. Behav Brain Res
84:117–127
Linster C, Nai Q, Ennis M (2011) Nonlinear effects of
noradrenergic modulation of olfactory bulb function
in adult rodents. J Neurophysiol 105:1432–1443
Mandairon N, Ferretti CJ, Stack CM, Rubin DB,
Cleland TA, Linster C (2006) Cholinergic modulation
in the olfactory bulb inﬂuences spontaneous olfactory
discrimination in adult rats. Eur J Neurosci 24:
3234–3244
Urban NN (2002) Lateral inhibition in the olfactory bulb
and in olfaction. Physiol Behav 77:607–612
Wilson DA, Linster C (2008) Neurobiology of a simple
memory. J Neurophysiol 100:2–7
Further Reading
Cleland TA, Linster C (2005) Computation in the olfactory
system. Chem Senses 30(9):801–813
Crasto CJ (2009) Computational biology of olfactory
receptors. Curr Bioinforma 4(1):8–15
Laurent G, Stopfer M, Friedrich RW, Rabinovich MI,
Volkovskii A, Abarbanel HD (2001) Odor encoding
as
an
active,
dynamical
process:
experiments,
computation and theory. Annu Rev Neurosci 24:
263–297
Schoppa NE, Urban NN (2003) Dendritic processing
within olfactory bulb circuits. Trends Neurosci 26(9):
501–506
Peripheral Nerve Interfaces:
Overview
Douglas J. Weber1, Brian Wodlinger2 and
Wei Wang2
1Department of Mechanical Engineering and the
Neuroscience Institute, Carnegie Mellon
University, Pittsburgh, PA, USA
2Department of Physical Medicine and
Rehabilitation, University of Pittsburgh,
Pittsburgh, PA, USA
Definition
The peripheral nervous system (PNS) comprises
nerve ﬁbers and ganglia located outside of the
brain and spinal cord, including 11 pairs of cranial
nerves and 31 pairs of spinal nerves. The spinal
(or somatic) nerves originate as dorsal and ventral
spinal nerve rootlets at their junction with the spinal
cord. The dorsal spinal nerve roots contain afferent
(sensory) nerve ﬁbers and their cell bodies, which
are located in the dorsal root ganglia (DRG). The
ventral spinal nerve roots contain efferent (motor)
ﬁbers whose cell bodies are located in the ventral
horn of the spinal cord. The dorsal and ventral roots
merge to form the spinal nerve, which exits the
spinal column through the intervertebral foramen.
Upon exiting the spine, the nerve divides into dor-
sal and ventral rami, which may branch further and
mix with other nerve ﬁbers to form plexuses (e.g.,
brachial plexus) and eventually form distinct
peripheral nerves and their branches.
Detailed Description
This section begins with background information
on the anatomy and physiology of the peripheral
nervous system (▶“Peripheral Nerves, Anatomy
and Physiology of”) and mathematical models for
simulating the biophysical properties of periph-
eral nerves (▶“Peripheral Nerve Models”). Sub-
sequent entries describe devices and signal
processing methods for acquiring and analyzing
electrical
signals
from
peripheral
nerves.
94
Peripheral Nerve Interfaces: Overview

Techniques for stimulating activity in peripheral
nerve ﬁbers are also described. This section also
describes several research and clinical applica-
tions based on peripheral nerve recording and
stimulation techniques.
Peripheral Nerve Interface Methods and
Applications
Interfaces to the peripheral nervous system have
long been considered of medical value, beginning
in ancient Egypt where the pain-relieving proper-
ties of the electric catﬁsh were discussed and
continuing with the Mediterranean electric ray
and torpedo ﬁsh. The nineteenth century saw
renewed interest in electrical stimulation of the
PNS due to dramatic advances in the understand-
ing of electricity. Only relatively recently, how-
ever, through the pioneering modeling work of
researchers like Hodgkin and Huxley, and those
that have expanded and continued their work,
have we begun to understand the PNS well
enough to design safe and effective interfaces to it.
Awidevariety of interface technologies has been
developed to record or stimulate activity in periph-
eral nerves. Most interfaces rely on electrodes
placed on the nerve surface (epineural) or penetrate
the interior of the epineurium (intraneural) of the
nerve (▶“Peripheral Nerve Interface, Intraneural
Electrode”;
▶“Peripheral
Nerve
Interface,
Epineural Electrode”; ▶“Peripheral Nerve Signal
Processing, Multipole Cuff Methods”; ▶“Periph-
eral Nerve Interface, Regenerative”).
Peripheral nerve interface technologies have
been applied to the diagnosis or treatment of a
wide range of medical conditions. Examples of
diagnostic applications include electromyography
and nerve conduction tests used to identify and
localize nerve damage caused by carpal tunnel
syndromes, diabetic neuropathies, and other inju-
ries or diseases (▶“Peripheral Nerve Interface
Applications, EMG/ENG”).
Electrical stimulation of peripheral nerve ﬁbers
is used to evoke activity in sensory, motor, and
autonomic ﬁbers. The cochlear implant is a highly
successful example of a sensory prosthesis that
restores hearing to people with profound hearing
loss due to damage to the hair cells in the cochlea
(▶“Peripheral
Nerve
Interface
Applications,
Cochlear Implants”). Electrical stimulation of
cutaneous nerve ﬁbers with TENS/PENS devices
is used to treat a variety of pain syndromes
(▶“Peripheral
Nerve
Interface
Applications,
Neuropathic Pain”). Other types of sensory pros-
theses are being developed (▶“Peripheral Nerve
Interface Applications, Sensory Restoration”), for
example, to provide tactile and proprioceptive
sensations to users of prosthetic limbs.
Summary
The highly structured nature of the peripheral ner-
vous system makes it amenable to computational
modeling. Advances in these interfaces have
already made a profound impact on modern medi-
cal
technology
including
cochlear
implants
(▶“Peripheral
Nerve
Interface
Applications,
Cochlear Implants”); techniques to support blad-
der, bowel, and sexual function; and vagal nerve
stimulation (▶“Peripheral Nerve Interface Appli-
cations, Vagal Nerve Stimulation”). New tech-
niques currently being investigated will address a
variety of disorders throughout the body including
obesity (▶“Peripheral Nerve Interface Applica-
tions, Obesity”), neuropathic pain (▶“Peripheral
Nerve Interface Applications, Neuropathic Pain”),
sleep apnea (▶“Peripheral Nerve Interface Appli-
cations, Sleep Apnea”), and sensory restoration in
amputees (▶“Peripheral Nerve Interface Applica-
tions, Sensory Restoration”). The interfaces them-
selves come in a diverse array of forms from simple
cuff electrodes (▶“Peripheral Nerve Interface,
Epineural Electrode”) which wrap around the
nerve to more complex regenerative electrode
arrays (▶“Peripheral Nerve Interface, Regenera-
tive”), and utilize many advanced stimulation and
signal processing techniques (▶“Peripheral Nerve
Signal Processing, Denoising”).
Peripheral nerve interface technologies hold
great promise in the diagnosis and treatment of a
wide range of neurological disorders. Research in
this
area
requires
signiﬁcant
collaboration
between researchers in computational modeling,
neuroscience, biomedical engineering medicine,
and physiology.
Peripheral Nerve Interfaces: Overview
95

Cross-References
▶Peripheral
Nerve
Interface
Applications,
Cochlear Implants
▶Peripheral
Nerve
Interface
Applications,
EMG/ENG
▶Peripheral Nerve Interface Applications, Neu-
ropathic Pain
▶Peripheral
Nerve
Interface
Applications,
Obesity
▶Peripheral Nerve Interface Applications, Respi-
ratory Pacing
▶Peripheral Nerve Interface Applications, Sen-
sory Restoration
▶Peripheral Nerve Interface Applications, Sleep
Apnea
▶Peripheral Nerve Interface Applications, Vagal
Nerve Stimulation
▶Peripheral Nerve Interface, Epineural Electrode
▶Peripheral
Nerve
Interface,
Intraneural
Electrode
▶Peripheral Nerve Interface, Regenerative
▶Peripheral Nerve Models
▶Peripheral Nerve Signal Processing, Denoising
▶Peripheral Nerve Signal Processing, Multipole
Cuff Methods
▶Peripheral Nerve Signal Processing, Source
Localization
▶Peripheral Nerve Stimulation Technique, Nerve
Block
▶Peripheral Nerves, Anatomy and Physiology of
Phase Response Curves:
Overview
Carmen C. Canavier
Department of Cell Biology and Anatomy, LSU
Health Sciences Center, New Orleans, LA, USA
Detailed Description
Phase
response
curves
(PRCs,
alternatively
phase-resetting curves) are a powerful way of
characterizing and explaining the behavior of
nonlinear oscillators without knowing anything
about their speciﬁc internal dynamics. The phase
response curve represents the shortening or
lengthening of the cycle period caused by an
input depending upon at what point (phase) within
the cycle an input is received. In contrast, for a
linear time invariant system, the effect of an input
is independent of when it is applied. No matter
whether the oscillations represent the ﬂashing of
ﬁreﬂies, a pendulum-based clock, a cardiac cell,
or a neural oscillator, the phase response curve
predicts the phasic relationship of the oscillator to
periodic forcing or to coupling within a network
of other oscillators. Each oscillator can be reduced
to a phase oscillator whose angular velocity on a
circle is constant, except when it receives an input
that resets (advances or delays) its phase on the
circle. In a neural context, there is usually a
threshold event, often the action potential or
spike, which is used to demarcate the boundaries
of a cycle. If an input shortens the cycle, the next
spike occurs sooner than it otherwise would have,
and so the next spike is advanced. Conversely, if
the next spike occurs later than it otherwise would
have, the spike is delayed.
The phase-resetting curve is sometimes pre-
sented as the response to a speciﬁc input. We
have called this the general PRC. One example
might be to perturb the oscillator underlying the
circadian rhythm by exposing an animal to a
period of light during its usual period of dark-
ness; another example might be to stimulate a
particular synaptic input or set of synaptic inputs
to a neural oscillator. The ▶“Spike Time
Response Curve” is a way to plot the phase
response to a speciﬁc input, that is, a spike in
the presynaptic neuron, in a way that preserves
information about time intervals. Phase-resetting
theory is generally applied to neural circuits
under a simplifying assumption; two common
assumptions are that of pulsatile (brief) coupling
or weak coupling. The entry on ▶“Pulse-
Coupled Oscillators” uses the information in the
spike-time response curve (or the information in
the general phase-resetting curve combined with
the intrinsic period information) to predict the
response of an oscillator to periodic forcing or
mutual
coupling,
under
the
pulse-coupled
96
Phase Response Curves: Overview

assumption that the effect of each input dissipates
quickly compared to the cycle period.
A second use of the term phase-resetting curve
is to represent the response of the oscillator to an
inﬁnitesimal input, in other words as a change in
the period per unit of the input, called the inﬁni-
tesimal PRC or iPRC. The entry on ▶“Phase
Response Curve, Measurement of the Inﬁnitesi-
mal” explains how to measure the inﬁnitesimal
phase response curve (iPRC). The iPRC is useful
mostly in the context of the weak-coupling
assumption, which assumes that the phase reset-
ting due to two brief, sequential pulses summates
linearly, and the phase resetting due to a single
brief pulse rescales linearly with the height of the
pulse. Thus, a complicated input like a postsyn-
aptic current waveform can be conceptually bro-
ken up into a series of shifted and scaled pulses
(Dirac delta functions). The entry on ▶“Weak
Coupling
Theory”
explains
the
connection
between the iPRC and the general PRC. That is,
if the coupling is sufﬁciently weak, the general
phase resetting due to any arbitrary input wave-
form can be computed by summing the phase
resetting due to each individual pulse: the timing
of each pulse gives the phase, and the height of the
pulse gives the scale factor for the resetting. This
amounts to convolving the iPRC with the input
waveform. Since the coupling is weak, the
assumption is that the relative phases of the oscil-
lators change very slowly. Weak coupling can be
used to predict the synchronization tendencies of
networks of oscillators. The entry on ▶“Phase
Models, Noisy” explains the effect of noise
under the assumption of weak coupling.
Many factors, including the underlying bifur-
cation structure and the particular set of conduc-
tances expressed by the neuron, inﬂuence the PRC
shape as described in the entry on ▶“Phase
Response Curve, Measurement and Shape of
General”. The inﬂuence of the underlying bifur-
cation structure described in that entry led to the
classiﬁcation of all PRCs with a single sign (all
advances or all delays) as type 1 and those with
both advances and delays as type 2. A completely
separate, classiﬁcation scheme into type 0 and
type 1 PRCs is described in the entry on
▶“Phase Response Curve, Topology of”.
The two schemes are a potential source of con-
fusion because of the similar terminology, but these
schemes are unrelated. In the topology-based
scheme, which was developed ﬁrst, weak resetting
leads to type 1 phase resetting, in which the phase
immediately after an input can take on any value,
but strong resetting leads to type 0 resetting in
which only a limited set of phase values are allowed
immediately after a strong input. This approach to
classifying phase-resetting curves makes minimal
assumptions, including pulse coupling, and is
based on simple topological considerations.
In summary, phase response curves have been
deﬁned for both inﬁnitesimal and general inputs.
PRCs
have
been
classiﬁed
using
separate,
unrelated schemes based on bifurcation analysis
or topology. Finally, PRCs have been used to
analyze synchronization under assumptions of
either weak or pulsatile coupling.
Cross-References
▶Phase Models, Noisy
▶Phase Response Curve, Measurement and
Shape of General
▶Phase Response Curve, Measurement of the
Inﬁnitesimal
▶Phase Response Curve, Topology of
▶Pulse-Coupled Oscillators
▶Spike Time Response Curve
▶Weak Coupling Theory
Retinal/Visual Interfaces
(Models, Theory, Techniques):
Overview
Nigel H. Lovell
Graduate School of Biomedical Engineering,
University of New South Wales, Sydney, NSW,
Australia
Synonyms
Artiﬁcial
vision;
Prosthetic
vision;
Visual
neuroprosthesis
Retinal/Visual Interfaces (Models, Theory, Techniques): Overview
97

Definition
Retinal and visual interfaces encompass a range of
approaches and technologies with the most com-
mon being that of a visual prosthesis, which is a
subclass of sensory neuroprostheses. Such tech-
nologies can be used as a device therapeutic to
restore some form of patterned vision to those
suffering
from
profound
vision
impairment.
Other approaches to vision restoration can also
be included under the broad umbrella of a reti-
nal/visual interface. These include optogenetic
methods that use tissue engineering techniques
to transfect remaining pathways in the visual sys-
tem with photosensitive properties. Simulated
prosthetic vision, methods to assess prosthetic
vision, and computation models of the neural
retina play important roles in increasing our
understanding of how such approaches function
and can best be optimized.
Detailed Description
Vision is arguably the most feature rich and com-
plex of the senses, with visual cues being critical
to most activities of daily living. Vision impair-
ment results in momentous personal and eco-
nomic burdens to individuals and to society with
global estimates of 285 million persons impacted
(Pascolini and Mariotti 2012).
Figure 1 illustrates the human visual pathways.
Pathology or trauma to various elements of the
visual pathway results in vision impairment or
profound vision loss. In retinal degenerative dis-
eases, such as retinitis pigmentosa (RP) and age-
related
macular
degeneration
(AMD),
the
Retinal/Visual Interfaces (Models, Theory, Tech-
niques): Overview, Fig. 1 Right panel: an axial section
of the human brain outlining a trace of the visual pathway
from the retina to the primary visual cortex. Other panels:
possible intervention sites for artiﬁcial vision are catego-
rized by retinal, optic nerve, and cortical placement loca-
tions. (Adapted from Lovell et al. 2010)
98
Retinal/Visual Interfaces (Models, Theory, Techniques): Overview

photoreceptors in the retina progressively die. In
response to such diseases, there can be large-scale
reorganization of the retina with substantial
gliosis (Jones et al. 2003). Despite this, studies
have shown that human retinal ganglion cells
(RGCs) maintain their viability after the onset of
these degenerative diseases, that the surviving
retinal neurons are capable of being electrically
stimulated (Humayun et al. 1996), and that rudi-
mentary
phosphene
vision
is
achievable
in
humans with advanced retinal degeneration.
The etiology and/or site of insult to the visual
pathway will dictate the range of possible target
locations for a visual prosthesis (Guenther et al.
2012; Lovell et al. 2010). Possible sites for intro-
ducing a visual prosthesis include retinal (epiretinal
(Humayun et al. 2012), subretinal (Zrenner et al.
2011;
Palanker
et
al.
2005),
suprachoroidal
(Matteucci et al. 2013), trans-retinal (Fujikado
et al. 2012)), optic nerve (Veraart et al. 2003), and
cortical (Fernandez et al. 2005) locations (Fig. 1).
In cases of trauma and diseases such as glaucoma,
the RGCs can also be destroyed. As the RGC
axonal processes form the optic nerve, in these
cases electrical stimulation of visual pathways dis-
tal to the lateral geniculate nucleus (LGN) of the
thalamus is ineffective.
In the case of retinal prostheses, to effectively
replace vision with the same resolution as that of
normally sighted humans would require an image
capturing device to replace the function of the
photoreceptor cells, of which there are approxi-
mately 125 million in each eye, converging to
some one million RGCs. Current retinal devices,
depending onplacement and design, have electrode
numbers from tens to several thousand at most.
The desired outcome is to have the electrode
array placed in close proximity to the neural tar-
gets with the implanted components remaining
securely ﬁxed, even in the presence of rapid head
and eye movements performed by the recipient.
The device should maintain its position, integrity,
and functionality for several decades of everyday
usage, cause minimal injury, inﬂammation, or risk
of infection to nearby tissue and cause minimal
discomfort to the recipient. Also important in
visual prosthesis design are aspects of size and
battery life in the case of external componentry.
A typical vision prosthesis system comprises
an external unit which using a camera and micro-
processor performs the image capturing and pro-
cessing and an implanted unit consisting of the
microelectronic stimulator and the electrode array.
Power and communication between the external
and implanted units are normally facilitated by a
transcutaneous radio-frequency (RF) link. The
exceptions to this approach are typically sub-
retinal devices that usually have photodiodes
designed into the implantable component and
thus have no need for an external camera.
Other more experimental approaches, in terms
of readiness for human trials, are based around
optogenetic techniques (Degenaar et al. 2009).
This involves the use of viral transfection of rho-
dopsins to target various remaining cells in the
visual pathway, making them photosensitive.
Common to all device therapies and target
locations in the visual pathway are a list of chal-
lenges that must be overcome to improve efﬁcacy
and ensure device safety and longevity. These
include maintaining a viable and stable neural
interface over the long-term, device hermeticity
and safe stimulation paradigms that allow concur-
rent stimulation at numerous electrode sites.
Cross-References
▶Computational Models of Neural Retina
▶Prosthetic Vision, Assessment
▶Prosthetic Vision, Perceptual Effects
▶Retinal Disease and Remodeling
▶Retinal Neurophysiology
▶Retinal Prosthesis
▶Visual Prosthesis, Cortical Devices
▶Visual Prosthesis, Epiretinal Devices
▶Visual Prosthesis, Optic Nerve Approaches
▶Visual Prosthesis, Optogenetic Approaches
▶Visual Prosthesis, Subretinal Devices
▶Visual Prosthesis, Suprachoroidal and Trans-
retinal Devices
References
Degenaar P, Grossman N, Memon MA, Burrone J,
Dawson M, Drakakis E, Neil M, Nikolic K (2009)
Optobionic vision: a new genetically enhanced light
on retinal prosthesis. J Neural Eng 6(3):035007
Retinal/Visual Interfaces (Models, Theory, Techniques): Overview
99

Fernandez E et al (2005) Development of a cortical visual
neuroprosthesis
for
the
blind:
the
relevance
of
neuroplasticity. J Neural Eng 2(4):R1–R12
Fujikado T et al (2012) Clinical trial of chronic implanta-
tion of suprachoroidal-transretinal stimulation system
for retinal prosthesis. Sensor Mater 24(4):181–187
Guenther T, Lovell NH, Suaning GJ (2012) Bionic vision:
system architectures – a review. Expert Rev Med
Devices 9(1):33–48
Humayun MA, de Juan E, Dagnelie G, Greenberg RJ,
Propst RH, Phillips DH (1996) Visual perception
elicited by electrical stimulation of the retina in blind
humans. Arch Ophthalmol 1141:40–46
Humayun MS et al (2012) Interim results from the inter-
national trial of Second Sight’s visual prosthesis. Oph-
thalmology 119(4):779–788
Jones BW, Watt CB, Frederick JM, Baehr W, Chen CK,
Levine EM, Milam AH, LaVail MM, Marc RE
(2003) Retinal remodeling triggered by photoreceptor
degenerations. J Comp Neurol 464:1–16
Lovell NH, Morley JW, Chen SC, Hallum LE, Suaning GJ
(2010) Biological-machine systems integration: engi-
neering the neural interface. Proc IEEE 98(3):418–431
Matteucci PB, Chen SC, Tsai D, Dodds CW, Dokos S,
Morley JW, Lovell NH, Suaning GJ (2013) Current
steering in retinal stimulation via a quasimonopolar
stimulation paradigm. Invest Ophthalmol Vis Sci
54(6):4307–4320
Palanker D et al (2005) Design of a high resolution opto-
electronic retinal prosthesis. J Neural Eng 2:S105–S120
Pascolini D, Mariotti SP (2012) Global estimates of visual
impairment: 2010. Br J Ophthalmol 96(5):614–618
Veraart C, Wanet-Defalgue M, Gerard B, Vanlierde A,
Delbeke J (2003) Pattern recognition with the optic
nerve visual prosthesis. Artif Organs 27(11):996–1002
Zrenner E et al (2011) Subretinal electronic chips allow
blind patients to read letters and combine them to
words. Proc Biol Sci 278(1711):1489–1497
Software Tools for Modeling
in Computational
Neuroscience: Overview
Padraig Gleeson
Department of Neuroscience, Physiology and
Pharmacology, University College London,
London, UK
Definition
A number of software tools have been made freely
available to the neuroscience community to assist
in building and analyzing biologically constrained
computational models of neuronal systems. This
overview will present a number of the most
widely used applications for model development
in computational neuroscience.
Detailed Description
Modeling
in
computational
neuroscience
is
becoming a crucial tool for understanding how
experimentally observed properties of neural sys-
tems emerge from lower-level biophysical pro-
cesses. In the same way that processing of
information happens at multiple physical scales
in the nervous system, software applications have
been created which specialize in modeling differ-
ent aspects of neurons and networks.
This overview deals primarily with simulators
of spiking neural networks, but other applications
for modeling at lower levels (e.g., for biochemical
signaling networks or reaction–diffusion within
spines or whole cells) or higher scale (e.g., models
of cognitive processes) have also been developed.
There is also a focus on freely available, open-
source applications.
Simulators
Abstract Neuron Simulations
The emergent properties of networks are fre-
quently studied using highly simpliﬁed neurons
with complex network connectivity and synaptic
dynamics. NEST (chapter ▶“NEST”) is a widely
used simulator for large neuronal models (e.g.,
100,000 neurons and millions of synapses). It is
highly optimized to run in a parallel computing
environment.
Another
popular
simulator
of
abstract neuronal networks is Brian (chapter
▶“Brian Spiking Neural Network Simulator”),
which is Python based and allows deﬁnition of
new neuron and synapse models by writing the
equations for their dynamics in a simple string-
based
format.
Topographica
(chapter
▶“Topographica”) is a simulator which deals pri-
marily with neural maps. It facilitates investiga-
tion of the transformation of information between
layers of topographic maps as are found in sensory
100
Software Tools for Modeling in Computational Neuroscience: Overview

and motor systems. XPP (http://www.math.pitt.
edu/~bard/xpp/xpp.html) is a widely used tool for
simulation and analysis of dynamical systems and
has frequently been used to build and investigate
neuronal systems. Nengo (http://nengo.ca) is a
simulator for large-scale neuronal systems. It has
been used for creating the Spaun (Semantic
Pointer Architecture Uniﬁed Network) simulation
(Eliasmith et al. 2012).
Conductance-Based, Multicompartmental
Simulators
Single neurons possess a range of features
enabling them to perform computational transfor-
mations on inputs, including complex morpholog-
ical structures and a host of active membrane
conductances. A number of simulators have been
developed which allow researchers to construct
neuron models at this level of detail and poten-
tially link such cells together into networks
inspired
by
anatomical
circuits.
NEURON
(chapter ▶“NEURON Simulation Environment”)
is a general purpose neuronal simulation environ-
ment, which allows simulation of networks of
neurons of varying levels of detail, from artiﬁ-
cial/abstract cells to complex, morphologically
detailed neurons. A key feature of NEURON is
its extension language NMODL, which allows
new ion channel and synapse models to be
deﬁned. An important recent addition to NEU-
RON’s functionality has been the ability to run
large scale network models across multiple pro-
cessors. A number of other Python based applica-
tions have been created on top of NEURON,
including NetPyNE (http://netpyne.org), which
facilitates the creation of biophysically detailed
network models, and LFPy (chapter ▶“LFPy:
Multimodal Modeling of Extracellular Neuronal
Recordings
in
Python”),
which
allows
the
generation of higher level brain signals such as
electroencephalography (EEG) and magnetoence-
phalography (MEG) from NEURON simulations.
GENESIS (▶“GENESIS, The GEneral NEu-
ral SImulation System”) has also been widely
used creating and analyzing detailed models of
single
cells
and
networks.
MOOSE
(▶“MOOSE, the Multiscale Object-Oriented
Simulation Environment”) was initially based on
GENESIS version 2 but has been further devel-
oped with a Python-based scripting interface, a
new graphical user interface, and greater support
for interacting with standardized modeling lan-
guages like SBML (▶“Systems Biology Markup
Language
(SBML)”)
and
NeuroML
(▶“NeuroML”). PSICS (▶“PSICS: The Parallel
Stochastic Ion Channel Simulator”) can simulate
multicompartmental,
conductance-based
cell
models. Its particular strength is allowing efﬁcient
simulation of stochastic ion channels using a
kinetic scheme-based approach.
Reaction–Diffusion Modeling
Modeling of the diffusion and reaction of bio-
chemical substances in complex 3D structures
can be useful for helping to understand the phys-
ical processes by which inputs are transformed in
neurites and at synapses. Some of the applications
available to create such models include MCell
(MCell), STEPS (▶“STEPS: STochastic Engine
for Pathway Simulation”), and NeuroRD (▶“Sto-
chastic Simulators”). The NEURON simulator
has also recently added support for reaction–dif-
fusion simulations (McDougal et al. 2013).
Interoperability Frameworks
A diversity of simulation tools is beneﬁcial from
the point of view of offering greater choice to
users and encouraging diverse approaches to neu-
ronal simulation. There are a number of initiatives
which aim to make it easier to use computational
models
across
these
simulators.
PyNN
(▶“PyNN: A Python API for Neural Network
Modelling”) is a Python library for building neu-
ronal networks, which was inspired by an increas-
ing number of neuronal simulators using the
Python
language
as
a
scripting
interface.
A neuronal network can be created using PyNN
and then can be simulated on (and the behavior
compared across) multiple simulators. NeuroML
(▶“NeuroML”) is an XML-based language for
describing models in computational neuroscience.
This has been mainly used to encode 3D networks
of multicompartmental, conductance-based neu-
rons, but the recently developed version 2.0
Software Tools for Modeling in Computational Neuroscience: Overview
101

increases its scope to more abstract neuron
models. NineML (http://incf.github.io/nineml-
spec) is another initiative to create an XML lan-
guage for describing spiking neural networks.
neuroConstruct (▶“neuroConstruct”) is a graph-
ical application which allows the construction of
complex 3D networks of biophysically detailed
neurons. Simulation scripts can then be automat-
ically generated to execute the model in different
simulation environments.
Neuronal Morphology Databasing and
Generation
Many modeling studies use detailed neuronal
morphologies to examine how information is pro-
cessed
and
transformed
by
single
neurons,
through active membrane conductances and inte-
gration of synaptic input. Neuromorpho.Org
(▶“NeuroMorpho.org”) is the primary resource
for obtaining neuronal morphologies which have
been digitally reconstructed. It has contributions
from many labs worldwide and covers a wide
range of species and neuron types.
Detailed neuronal morphologies can also be
automatically generated, based on data obtained
from real neurons, for use in neuronal simulations.
TREES Toolbox (▶“TREES Toolbox: Code for
Neuronal Branching”), CX3D (▶“Cx3D: Cortex
Simulation in 3D”) and NeuGen (http://atlas.gcsc.
uni-frankfurt.de/~neugen) are just some of the
applications which can be used for this purpose.
More details on initiatives for generating artiﬁcial
neurons which could be used in neuronal simula-
tions can be found in the entry ▶“Synthetic Neu-
ronal Circuits/Networks.”
Useful Resources
Other useful resources where models, experimen-
tal data, and other software tools for computa-
tional neuroscience can be obtained include:
•
ModelDB (▶“ModelDB”): an archive of sim-
ulation scripts for published models in the for-
mat originally used by the model developers.
•
Open Source Brain (▶“Open Source Brain”):
a resource for sharing and collaboratively
developing neuronal models. Models reside in
open-source repositories, and reusing, modify-
ing, and converting the models to standardized
formats such as NeuroML and PyNN are
actively encouraged.
•
Channelpedia (http://channelpedia.epﬂ.ch) is a
resource developed by the Blue Brain Project
which provides structured information on ion
channels, and many of its entries have down-
loadable
computational
models
of
the
channels.
•
NeuroElectro (▶“NeuroElectro Project”) pro-
vides structured information on electrophysio-
logical properties of neurons obtained from the
literature.
•
NeuralEnsemble (http://neuralensemble.org) is
a resource which aims to promote open, col-
laborative software development in computa-
tional neuroscience and hosts a number of
related
software
packages
(e.g.,
Brian
and PyNN).
Other Software for Neuronal Simulation
Population-Based Modeling
While most of the simulation packages men-
tioned already use spiking neuron models, a sig-
niﬁcant amount of modeling work takes place in
computational neuroscience using population-
based models and mean-ﬁeld approaches to
model large-scale cognitive processes. Examples
of software packages which allow this type of
modeling are MIIND (▶“MIIND: A Population-
Level Neural Simulator Incorporating Stochastic
Point Neuron Models”), a simulator for high-
level population based modeling, and The Virtual
Brain (▶“The Virtual Brain (TVB): Simulation
Environment for Large-Scale Brain Networks”),
which provides a simulator and a web-based
interface for constructing neural population
models.
Hardware-Based Modeling Solutions
In addition to software-based neuronal simulators,
many groups are investigating hardware-based
102
Software Tools for Modeling in Computational Neuroscience: Overview

solutions to enable faster and larger-scale neuro-
nal simulations. Many of these use off-the-shelf
hardware like Graphics Processing Units (GPUs),
for example, NeMo (http://nemosim.sourceforge.
net) and GeNN (http://genn-team.github.io/genn),
but other initiatives are developing new hardware,
customized to simulate large-scale neuronal net-
works, for example, SpiNNaker (http://apt.cs.
man.ac.uk/projects/SpiNNaker).
Systems Biology/Bioinformatics Tools
\The Systems Biology community has been
actively developing applications and model
description languages in recent years to facilitate
building and exchanging models of biochemical
reactions, signaling pathways, and gene regula-
tory networks. SBML (▶“Systems Biology
Markup
Language
(SBML)”)
and
CellML
(▶“CellML”) are two widely used standards in
this area, and many models in these formats,
including neuronal models, can be obtained
from the BioModels database (▶“BioModels
Database: A Public Repository for Sharing
Models
of
Biological
Processes”)
and
the
CellML Model Repository (http://models.
cellml.org).
Conclusions
The development of tools for modeling in com-
putational neuroscience is a dynamic ﬁeld. The
overview here has no doubt left out a number of
simulators and applications, which would be of
use to the wider community. The author encour-
ages developers to get in contact with details of
their work for inclusion in future versions of this
entry.
References
Eliasmith C, Stewart TC, Choo X, Bekolay T, DeWolf T,
Tang Y, Rasmussen D (2012) A large-scale model of
the functioning brain. Science 338(6111):1202–1205.
https://doi.org/10.1126/science.1225266
McDougal R, Hines M, Lytton W (2013) Reaction–diffu-
sion in the NEURON simulator. Front Neuroinform
7:28. https://doi.org/10.3389/fninf.2013.00028
Somatosensory System:
Overview
Sliman J. Bensmaia
Department of Organismal Biology and Anatomy,
University of Chicago, Chicago, IL, USA
Detailed Description
Somatosensation includes multiple senses: pain
(nociception),
temperature
(thermoreception),
touch, and the sense of our limb position in
space (proprioception). Each submodality of
somatosensation relies on different types of recep-
tors embedded in the skin, muscle, and joints and
involves different structures in the spinal cord and
in the brain.
Pain
Pain is arguably one of the most vital senses as it
signals when our body is liable to being damaged.
There are many different types of receptors in the
skin that signal a potentially harmful stimulus.
Some receptors respond to intense mechanical
deformations of the skin, others to extreme tem-
peratures, and still others to different kinds of
chemicals. Pain comprises a sensory discrimina-
tive component, which provides information
about the location, duration, intensity, and quality
of the pain; an affective one, which signals its
unpleasantness; and a cognitive-evaluative one,
which is associated with cognitive variables such
as attention, which can modulate the sensory
experience.
Thermoreception
Our ability to sense whether an object is warm or
cold relies on two types of thermoreceptive
ﬁbers – so-called “cold” and “warm” ﬁbers –
embedded in the skin. As their names suggest,
“cold” ﬁbers are activated when the skin is cooled
Somatosensory System: Overview
103

and “warm” ﬁbers are activated when the skin is
warmed. In contrast to thermosensitive nocicep-
tive ﬁbers, which respond at extreme tempera-
tures, thermoreceptive ﬁbers only respond at
intermediate, non-noxious temperatures (with the
exception of the paradoxical response of some
cold ﬁbers to high temperatures).
Touch
The sense of touch plays a critical role in our
ability to grasp and manipulate objects. Indeed,
cutaneous signals provide information about the
forces we exert on objects and whether these are
slipping from our grasp. Without these signals, we
would routinely crush or drop objects. Touch also
plays an important role in emotional communica-
tion: We touch the people we care about and wish
to be touched by them. Finally, our sense of touch
plays a key role in embodiment, making our body
feel as a part of us.
The skin is innervated by several types of
mechanoreceptive afferents, each of which con-
veys different information about skin deforma-
tions (link: sensory innervation of the skin) and
conveys different types of information about
events impinging upon the skin. Merkel cells con-
vey information about the shape of objects
grasped in the hand (link: cutaneous mechanore-
ceptive
afferents:
neural
coding
of
shape),
Meissner corpuscles about motion of objects
across the skin, and Pacinian corpuscles about
surface texture (link: cutaneous mechanorecep-
tive afferents: neural coding of texture). Afferents
produce highly repeatable and temporally pat-
terned
responses
to
skin
stimulation
(link:
somatosensory
neurons:
spike
timing),
and
models have been developed that predict with
high accuracy the responses of somatosensory
neurons to spatiotemporal skin deformations
(link: mechanotransduction: models).
When we palpate an object, we obtain infor-
mation about its shape (link: somatosensory cor-
tex: neural coding of shape), its texture (link:
cutaneous mechanoreceptive afferents: neural
coding of texture), and its motion across the skin
(link: somatosensory cortex: neural coding of
motion).
Proprioception
Proprioception (link: proprioception) plays a crit-
ical role in guiding motor behavior. Individuals
with intact motor systems but compromised pro-
prioception have difﬁculty planning and execut-
ing movements, almost as if they had a motor
impairment. Proprioception, like touch, is also
important for our sense of embodiment.
There are several types of proprioceptive
receptors, located in muscles, in the skin, and in
joint capsules. Two types of muscle propriocep-
tors, muscle spindles and Golgi tendon organs,
are thought to be the primary contributors to
proprioception. One population of receptors in
the skin is sensitive to skin stretch and can con-
vey information about joint angle. Another type
of proprioceptor, the joint capsule receptor, ﬁres
at the extreme ends of the joint’s range and
may be involved in preventing overextension of
the joint.
Proprioceptive and cutaneous signals are then
processed in the dorsal column nuclei, then in the
ventroposterior lateral nucleus in the thalamus,
and then in primary and secondary somatosen-
sory
cortices
(link:
somatosensory
cortex:
organization).
Cross-References
▶Cutaneous Mechanoreceptive Afferents: Neu-
ral Coding of Texture
▶Mechanotransduction, Models
▶Proprioception
▶Somatosensory Cortex:
Neural Coding of
Motion
▶Somatosensory Cortex:
Neural Coding of
Shape
▶Somatosensory Cortex: Organization
▶Somatosensory Neurons: Spike-Timing
104
Somatosensory System: Overview

Spectral Methods in Neural
Data Analysis: Overview
Steven L. Bressler
Cognitive Neurodynamics Laboratory, Center for
Complex Systems and Brain Sciences,
Department of Psychology, Florida Atlantic
University, Boca Raton, FL, USA
Detailed Description
Spectral analysis is a powerful and widely used
approach to the study of time series data (Warner
1998; Bloomﬁeld 2000). It provides a useful com-
plement to other types of analysis in computa-
tional neuroscience. Spectral analysis refers to a
host of techniques relating to transformed time
series in the frequency domain. A spectral repre-
sentation of a time series is a function of fre-
quency, where frequency is expressed in units of
cycles per second, or hertz (Hz). Although spec-
tral analysis is applicable to deterministic time
functions, neural data is typically stochastic and
thus
requires
statistical
spectral
analysis
(Brillinger 2001; Bendat and Piersol 2010).
Neural data types that are subjected to spectral
analysis commonly include continuous time
series such as the electroencephalogram (EEG),
magnetoencephalogram (MEG), electrocortico-
gram (ECoG), and local ﬁeld potential (LFP) but
may also include point process time series such as
single-unit and multiunit spiking activity (Glaser
and Ruchkin 1976; Dumermuth and Molinari
1987; Hesselmann 1991).
Spectral methods in neural data analysis pro-
vide a frequency-based representation of neural
time series data. They may be used to identify
discrete,
narrowband
oscillations
in
time
series or to decompose a broadband time series
into frequency-speciﬁc components. Spectral
methods are also employed in the frequency-
based analysis of neural data arrayed in space
rather than time (see ▶“Spatial Spectral Analy-
sis”). Spatial spectra are representations of spatial
data, and spatial frequency is expressed in units of
cycles per unit distance.
The traditional approach to spectral analysis is
based on the pioneering work of Joseph Fourier
(1768–1830), the French mathematician who is
credited with having ﬁrst introduced the represen-
tation of a mathematical function as a sum of
trigonometric
functions.
Nowadays,
Fourier
methods play a major role in a multitude of appli-
cations in mathematics, science, and engineering.
Fourier analysis refers to the linear decomposition
of a function into elemental trigonometric basis
functions, whereas Fourier synthesis is the opera-
tion of rebuilding the original function from the
component basis functions. Although functions
subjected to Fourier analysis are often continuous
and potentially inﬁnite in length, they may also be
discrete and ﬁnite in length. The wide availability
of methods for Fourier analysis of digitized ﬁnite-
length time series on digital computers has made
spectral data analysis practical, rapid, and inex-
pensive (Marple 1987). As a result, spectral anal-
ysis of neural data has become increasingly
popular in modern times, with a growing list of
applications in theoretical, experimental, and clin-
ical neuroscience.
Fourier analysis of a time series assigns values
of amplitude and (absolute) phase to the trigono-
metric basis functions (sines and cosines) of the
decomposition. For univariate time series, that is,
from a single recording channel, Fourier analysis
speciﬁes a range of frequencies of the component
basis functions, along with amplitude (or power,
squared amplitude) and phase values for each fre-
quency component. This (polar) representation in
terms of amplitude and phase has an equivalent
(Cartesian) representation in terms of the real and
imaginary components of a complex quantity. The
simplicity, symmetry, and facility of use of the
complex Cartesian representation make complex
algebra a useful mathematical tool in spectral anal-
ysis. However, complex algebra is not necessary.
The polar and complex Cartesian representations
are equivalent in providing a complete description
of time series data, and Fourier synthesis can recon-
struct the time series from either one.
Spectral Methods in Neural Data Analysis: Overview
105

Fourier analysis is often used to quantify
interdependency relations between time series
that are derived from different sources (see
▶“Spectral
Interdependency
Methods”).
Frequency-based interdependency measures may
be derived from the complex-valued cross-
spectrum. The cross-spectrum yields two impor-
tant real-valued spectra. First is the relative phase
(i.e., difference in phase) spectrum, given by the
arctan of the ratio of imaginary to real components
of the cross-spectrum. Second is the normalized
modulus of the cross-spectrum, the coherence
spectrum, which is the frequency domain equiva-
lent of the time domain cross-correlation function.
The relative phase spectrum reﬂects the mean, and
the coherence spectrum reﬂects the variance, of
the distribution of relative phase values of two
time series. Coherence is roughly equivalent to
the phase-locking value, deﬁned as one minus
the circular variance of relative phase, in that
they both reﬂect relative phase variation. The
difference is that coherence additionally reﬂects
(to a lesser degree) amplitude covariation. Spec-
tral
methods
are
used
to
examine
neural
interdependency relations between continuous
signals (e.g., between LFPs), or discrete signals
(e.g., spiking activities), or between continuous
and discrete signals (e.g., spike-ﬁeld coherence).
Neural time series data are commonly multi-
variate, being derived from multiple sources, and
their spectral analysis often involves the assess-
ment of pairwise interdependency relations in the
frequency domain. Some interdependency rela-
tions between time series are directional, meaning
that the interdependency is directed from one time
series to another. Directed spectral methods quan-
tify directed interdependency in the frequency
domain (see ▶“Directed Spectral Methods”).
Because neural time series data commonly
evolve over time, time-frequency, or spectrographic,
representations are often useful (see ▶“Time-Fre-
quency Analysis of Analog Neural Signals”). This
type of representation, which allows one to indepen-
dently examine the temporal evolution of different
frequency components, may be used to track the
time-frequency
evolution
of
a
range
of
time-varying neural processes, including sleep, sen-
sory,
motor,
and
cognitive
processing.
Spectrographic representations of the electroen-
cephalogram (EEG) are heavily used in neurophar-
macology to assess the actions of neuroactive drugs.
Avariety of different time-frequency methods, such
as the short-time Fourier transform, Wigner-Ville
distribution, Cohen’s class distribution, or wavelet
transform (see ▶“Wavelet Analysis”), have been
applied in neural data analysis.
One important application of spectral methods
in neural data analysis is ﬁltering, the transforma-
tion of neural time series by the selection of cer-
tain frequency components and the exclusion of
others (see ▶“Digital Filtering”). Digital ﬁltering,
which refers to ﬁltering operations performed on
discretely sampled data in a digital computer, has
the advantage of allowing data at both past and
future time points to be used in determining the
ﬁltered value of a current time point, unlike analog
ﬁltering, where the ﬁltering operation depends
only on past time points. Although digital ﬁltering
may be performed in the time domain, it is more
easily performed in the frequency domain by the
selection of a range of frequencies (the passband)
and exclusion of another range (the stopband).
Data smoothing is an important operation in neu-
ral data analysis that is accomplished by digital
(low-pass) ﬁltering. Digital ﬁltering is also used to
separate spike and ﬁeld signals recorded from a
common microelectrode.
Neural time series are rarely deterministic,
where each time series value is exactly determined
by past values. For this reason, statistical spectral
methods are usually required for neural time series
analysis (Jenkins and Watts 1968; Percival and
Walden 1993). Statistical spectral analysis con-
siders time series data to be generated by stationary
stochastic (random) processes, and the various
spectral quantities are treated as data-derived sta-
tistical estimates of unknown population variables,
rather than as deterministic quantities. Unlike non-
parametric spectral analysis, which computes spec-
tra directly from time series data by Fourier
analysis,
parametric
spectral
analysis
is
an
approach that derives spectral quantities from a
statistical model of the time series (see ▶“Para-
metric Spectral Analysis”). In the model, the vari-
able at a particular time is expressed by statistical
relations with variables from past times. The
106
Spectral Methods in Neural Data Analysis: Overview

parametric model
is typically autoregressive,
meaning that each time series value is modeled as
a weighted sum of past values (the weights being
considered as the parameters of the model) (Kay
1988;
Chatﬁeld
2004).
Although
parametric
models may be nonlinear, those used in neurosci-
ence are typically linear because neural time series
are commonly locally linear. Parametric modeling
has some distinct advantages in neural data analy-
sis: It allows a precise time-frequency representa-
tion of time-varying neural time series (Ding et al.
2000), and it serves as a theoretically sound basis
for directed spectral analysis (Ding et al. 2006).
Spectral methods of neural data analysis utilize
frequency-based representations. Since rhythmic
activity, mostly in time but also in space, is ubiq-
uitous in neuroscience, frequency-based tech-
niques are very important tools in computational
neuroscience. Given that many good software
options exist for performing spectral data analysis,
these techniques are readily available to neurosci-
ence researchers working with many different
types of neural data.
Cross-References
▶Digital Filtering
▶Directed Spectral Methods
▶Parametric Spectral Analysis
▶Phase-Locking Methods
▶Spatial Spectral Analysis
▶Spectral Interdependency Methods
▶Time-Frequency Analysis of Analog Neural
Signals
▶Wavelet Analysis
References
Bendat JS, Piersol AG (2010) Random data: analysis and
measurement procedures, 4th edn. Wiley, Hoboken
Bloomﬁeld P (2000) Fourier analysis of time series. Wiley,
New York
Brillinger D (2001) Time series: data analysis and theory.
SIAM, Philadelphia
Chatﬁeld C (2004) The analysis of time series: an intro-
duction. Chapman & Hall/CRC, Boca Raton
Ding M, Bressler SL, Yang W, Liang H (2000) Short-
window spectral analysis of cortical event-related
potentials by adaptive multivariate autoregressive
modeling: data preprocessing, model validation, and
variability assessment. Biol Cybern 83:34–45
Ding M, Chen Y, Bressler SL (2006) Granger causality:
basic theory and application to neuroscience. In:
Schelter B, Winterhalder M, Timmer J (eds) Handbook
of time series analysis: recent theoretical developments
and applications. Wiley-VCH, Weinheim
Dumermuth G, Molinari L (1987) Spectral analysis of EEG
background activity. In: Gevins AS, Remond A (eds)
Methods of analysis of brain electrical and magnetic
signals. Handbook of electroencephalography and clin-
ical neurophysiology, revised series, vol 1. Elsevier,
Amsterdam
Glaser EM, Ruchkin DS (1976) Principles of neurobiolog-
ical signal analysis. Academic, New York
Hesselmann NL (1991) The fundamentals of discrete Fou-
rier analysis. In: Weitkunat R (ed) Digital biosignal
processing. Elsevier, Amsterdam/New York
Jenkins GM, Watts DG (1968) Spectral analysis and its
applications. Holden-Day, San Francisco
Kay SM (1988) Modern spectral estimation: theory and
application. Prentice-Hall, Englewood Cliffs
Marple SL (1987) Digital spectral analysis with applica-
tions. Prentice-Hall, Englewood Cliffs
Percival DB, Walden AT (1993) Spectral analysis for phys-
ical applications: multitaper and conventional univariate
techniques. Cambridge University Press, Cambridge
Warner RM (1998) Spectral analysis of time-series data.
The Guilford Press, New York
Further Reading
Wikipedia
Stochastic (random) process. http://en.wikipedia.org/wiki/
Stochastic_process
Spike Train Analysis:
Overview
Sonja Grün
Lab for Statistical Neuroscience, Institute of
Neuroscience and Medicine (INM-6, INM-10)
and Institute for Advanced Simulation (IAS-6),
Research Centre Jülich, Jülich, Germany
Theoretical Systems Neurobiology, RWTH
Aachen University, Aachen, Germany
Definition
A sequence of action potentials (“spike train”) is
the output of an individual neuron and is the input
Spike Train Analysis: Overview
107

to receiving neurons. It is assumed that neurons
interact by sending and receiving spikes. There-
fore, simultaneously observed spike trains are
analyzed for correlations to identify neuronal
interactions.
This
section
presents
analysis
approaches for various statistical aspects of spike
trains.
Detailed Description
The brain is composed of billions of neurons, the
elementary units of neuronal information pro-
cessing. The neocortex, which is critical to most
higher brain functions, is a highly complex net-
work of neurons, each of which receives signals
from thousands of other neurons and projects its
own output via sequences of spikes (▶“Spike
Train”)
to
thousands
of
other
neurons
(Braitenberg and Schüz 2009). In order to observe
neuronal activity in the active brain, a large vari-
ety of recording techniques are being employed,
ranging from recordings of individual neurons
(intra- or extracellularly) to recordings of neuro-
nal populations on mesoscopic or macroscopic
scales. Any particular choice of the recording
technique reﬂects the hypothesis the researcher
has in mind about the mechanisms of neuronal
processing. The focus on spike recordings from
individual neurons implies that one strives to
understand the elementary units of neuronal pro-
cessing. However, approaching the relationship of
different signal types may provide a means of
relating processing on different spatial scales
(▶“Spike Triggered Average”).
Early electrophysiological experiments were
bound to record from single neurons only. The
resulting insights are now the basis for the “clas-
sical” view of sensory coding: ﬁring rates are
modulated in a feed-forward hierarchy of pro-
cessing steps. Signals from sensory epithelia are
assumed to eventually converge to cortical detec-
tors for certain combinations of stimulus features
(▶“Neural Coding”) or ﬁnally transferred to
motor output. Speciﬁc percepts or motor actions
would be represented by the elevated ﬁring of a
single nerve cell (▶“Estimation of Neuronal Fir-
ing Rate”) or by changes of ﬁring rates of groups
of neurons which can be assessed by different
conceptual approaches (▶“Neuronal Population
Vector,” ▶“Population Encoding/Decoding,” and
▶“State-Space Models for the Analysis of Neural
Spike Train and Behavioral Data”).
An alternative concept of neuronal processing
by groups of neurons was suggested by Donald
Hebb (1949) who ﬁrst demonstrated the concep-
tual power of a brain theory based on cell assem-
blies. Inspired by Hebb and driven by more recent
physiological and anatomical evidence in favor of
a distributed network hypothesis, brain theorists
constructed models that rely on groups of neu-
rons, rather than single nerve cells, as the func-
tional building blocks for representation and
processing of information. Despite conceptual
similarities,
such
concepts
of
neuronal
cooperativity differ in their detailed assumptions
with respect to the spatiotemporal organization of
the neuronal activity. To understand the principles
of coordinated neuronal activity and its spatiotem-
poral scales, it is obligatory to observe the activity
of multiple single neurons simultaneously. Due to
recent technological developments in recording
methodology, this can regularly be done. Coordi-
nated activity of neurons is only visible in corre-
lations of their respective spike trains, which
typically admit no simple interpretation in terms
of ﬁxed synaptic wiring diagrams. Rather, it
became evident that the correlation dynamics
apparent in time-resolved multiple-channel mea-
surements reﬂect variable and context-dependent
coalitions among neurons and groups of neurons.
Thus, the analysis of simultaneously recorded
spike trains allows us to relate concerted activity
of ensembles of neurons to behavior and cogni-
tion. Different analysis approaches are thereby
relevant to distinguish different or even comple-
mentary spatio-temporal scales based on methods
ranging from pairwise analysis (▶“Spike Train
Distance,” ▶“Correlation Analysis of Parallel
Spike Trains,” and ▶“Joint Peri Stimulus Time
Histogram (JPSTH)”) to concepts for the analysis
of multiple parallel processes (▶“Gravity Analy-
sis of Parallel Spike Trains,” ▶“Unitary Event
Analysis,” ▶“Information Geometry as Applied
to Neural Spike Data,” ▶“Spatial Temporal Spike
Pattern Analysis,” ▶“Statistical Evaluation of
108
Spike Train Analysis: Overview

Spatio-Temporal Spike Patterns,” and ▶“Gener-
alized Linear Models for Point Process Analyses
of Neural Spiking Activity”). The evaluation of
signiﬁcance (▶“Signiﬁcance Evaluation”) is a
basic element in these analyses, in some cases
based on nonparametric methods (▶“Surrogate
Data for Evaluation of Spike Correlation”). Anal-
ysis of parallel spike trains (Grün and Rotter 2010;
Kass et al. 2014) is the logical next step to improve
our understanding of the neuronal mechanisms
underlying information processing in the brain.
Cross-References
▶Correlation Analysis of Parallel Spike Trains
▶Estimation of Neuronal Firing Rate
▶Generalized Linear Models for Point Process
Analyses of Neural Spiking Activity
▶Gravity Analysis of Parallel Spike Trains
▶Information Geometry as Applied to Neural
Spike Data
▶Joint Peri Stimulus Time Histogram (JPSTH)
▶Neural Coding
▶Neuronal Population Vector
▶Population Encoding/Decoding
▶Signiﬁcance Evaluation
▶Spatial Temporal Spike Pattern Analysis
▶Spike Train
▶Spike Train Distance
▶Spike Triggered Average
▶State-Space Models for the Analysis of Neural
Spike Train and Behavioral Data
▶Statistical Evaluation of Spatio-Temporal Spike
Patterns
▶Surrogate
Data
for
Evaluation
of
Spike
Correlation
▶Unitary Event Analysis
References
Braitenberg V, Schüz A (2009) Cortex: statistics and geom-
etry of neuronal connectivity, 2nd edn. Springer,
New York. ISBN 9783540638162
Grün S, Rotter S (eds) (2010) Analysis of parallel spike
trains. Springer series in computational neuroscience.
Springer,
New
York.
ISBN
978-1-4419-5674-3,
e-ISBN 978-1-4419-5675-0
Hebb D (1949) The organization of behavior. Wiley,
New York
Kass RE, Eden U, Brown EN (eds) (2014) Analysis of
neural data. Springer series in statistics. Springer,
New York. ISBN 9781461496014
Spiking Network Models and
Theory: Overview
Marc-Oliver Gewaltig
Blue Brain Project, École Polytechnique Fédéral
de Lausanne, Lausanne, Switzerland
Definition
Spiking neuronal networks are a type of neural
network model where the neurons interact by
sending and receiving the so-called spikes, short
pulses that are only deﬁned by their time of occur-
rence. Biologically, spikes correspond to the
action potentials of neurons.
Neuron models that produce spikes are called
spiking neuron models. Examples are the Integrate
and Fire Models, Deterministic; the Izhikevich
model; and the Hodgkin-Huxley Model.
The term spiking network was introduced to
distinguish these models from formal neuron
models which have graded activation functions.
Detailed Description
Historical Background
The ﬁrst spiking neuron models were developed at
the beginning of the twentieth century and
focused on explaining the electrical behavior of
isolated neurons. In 1907, Louis Lapicque pro-
posed an electrical circuit model to describe the
change in membrane potential after applying a
current step. He assumed a ﬁxed ﬁring threshold
to explain the occurrence of action potentials
(Lapicque 1907; Tuckwell 1988). Lapicque’s
model can therefore be seen as the ﬁrst integrate
and ﬁre model. In 1936, Arthur Hill extended
Lapicque’s model by adding an adaptive threshold
(Hill 1936). These models are phenomenological
Spiking Network Models and Theory: Overview
109

since they did not explain the biophysical mecha-
nisms producing action potentials. In the 1950s
Hodgkin and Huxley explained with a series of
experiments that action potentials are caused by
ionic currents which result from ion channels with
voltage-dependent conductances (Hodgkin and
Huxley 1952). But because of its simplicity,
Lapicque’s original threshold model is still used
by experimental and theoretical neuroscientists.
Theoretical network models initially focused
on explaining experimentally observed inter-
spike interval distributions of individual neurons
as well as the origin of the observed high variabil-
ity of neuronal responses. Recent theoretical
research mainly focuses on understanding low-
rate spontaneous activity as well as multistability
in the context of learning and memory.
Theoretical analysis of spiking neuronal net-
works is only possible if the network model is
sufﬁciently simple. In recent years, computer sim-
ulations have become increasingly important to
support and complement theoretical analysis,
because they allow the inclusion of more biolog-
ical detail and the analysis of heterogeneous net-
work architectures.
Spiking Neuron Models
Theoretical network models typically resort to
simpliﬁed neuron models, such as the leaky inte-
grate and ﬁre model (LIF), some variant of it, or
more abstract models such as pulse-coupled
oscillators.
All of these neuron models have in common
that they interact by sending and receiving the
so-called spikes which are abstractions of the
action potentials produced by excitable cells
such as neurons.
Spikes are temporal point events, characterized
by their time of occurrence bt and mathematically
expressed by the Dirac delta function δ(t).
A sequence of spikes S ¼
^t1, ^t2, . . .


is
called spike train and can then be written as
s tð Þ ¼
X
k
d t  ^tk


,
(1)
where the index k runs over all spikes in S.
Integrate and Fire Models
The leaky integrate and ﬁre (LIF) model constitutes
a whole class of models consisting of two parts.
The ﬁrst part consists of one or more differential
equations, describing the subthreshold behavior of
the neuron’s membrane potential. The second part
converts the continuous membrane potential into
discrete spike events. Here we will present the LIF
model in its most basic form and then discuss a
number of common variations.
Subthreshold Dynamics
The
subthreshold
membrane
potential
V is
described by the differential equation
t d
dt V tð Þ ¼ V0  V tð Þ þ RI tð Þ
ð2Þ
where V0 is the resting potential, t the membrane
time constant, and R the membrane resistance.
I(t) ¼ Isyn(t) þ Iext(t) þ . . . is the total current
across the cell membrane and comprises all exter-
nal inﬂuences on the neuron, such as synaptic
currents Isyn and externally applied currents Iext.
Spike Generation
Spikes are generated, when the membrane poten-
tial V crosses a threshold value Vθ from below.
The time of spike is then given by the relation
V t0
ð Þ ¼ Vy and d
dt V t0
ð Þ > 0
ð3Þ
In the following, we use t0 to denote the times
of endogenous spikes and bt to denote input spikes
from other neurons.
After a spike is generated, the membrane
potential is reset to Vreset, a value which is often
chosen to match the resting potential V0.
Immediately after a spike, neurons are unable
to produce another spike for a short period of time
tr, called refractory period. Many models imple-
ment the refractory period by holding the mem-
brane potential at its reset value, such that V(t) ¼
Vreset for t  (t0, t0 þ tr].
Threshold Adaptation
Many neurons adapt to sustained input by reduc-
ing their ﬁring rate. In integrate and ﬁre models,
110
Spiking Network Models and Theory: Overview

this is often captured by a mechanism called
threshold adaptation.
The spike threshold Vθ is then no longer con-
stant, but increases with every spike by an amount
a and decays back to its resting value θ with a time
constant tθ:
ty d
dt Vy ¼ y  Vy þ a
X
k
d t  t0k


ð4Þ
where the sum runs over all endogenous spikes t0k
of the neuron.
Synaptic Input
Synaptic input enters the membrane potential in
the
form
of
the
synaptic
current
Isyn(t).
A presynaptic spike at time btcauses a conductance
change at the postsynaptic membrane, which in
turn results in a postsynaptic current of the form
Isyn t,V
ð
Þ ¼ Esyn  V


 gsyn t  ^t
ð
Þ
(5)
where Esyn is the reversal potential of the synapse
and gsyn(t) the time course of the conductance
change.
Many models implicitly assume that the synap-
tic conductance changes much faster that the mem-
brane potential V. Then V in Eq. 5 can be treated as
constant, and the synaptic current no longer
depends on the membrane potential which greatly
simpliﬁes analytical and numerical analysis.
In recent years, the terms COBA and CUBA
have become popular to distinguish the two cases.
COBA refers to the original Eq. 5 and stands for
conductance based, while CUBA stands for cur-
rent based and refers to simpliﬁed synaptic cur-
rents of the form
Isyn tð Þ ¼ psc t  bt


ð6Þ
where the postsynaptic current psc(t) is only a
function of time (Vogels and Abbot 2005).
Since Eq. 2 is a linear differential equation with
constant coefﬁcients, its solution for arbitrary
postsynaptic currents psc(t) is given by
psc tð Þ ¼ V0 þ
ð1
0
H t  s
ð
Þ exp
 t  s
ð
Þ
t

	
 psc sð Þds
ð7Þ
where H(t) is the Heaviside step function and
psp(t) is the so-called postsynaptic potential.
In the case where the postsynaptic current is a
delta function psc tð Þ≔d t  bt


, Eq. 7 reduces to
psp tð Þ ¼ V0 þ exp
 t  bt


t

	
:
ð8Þ
In response to a spike train s(t), we obtain
psp tð Þ ¼ V0 þ
X
k
H t  bt
k


exp
 t  bt
k


t
0
@
1
A:
ð9Þ
Spike-Response Model
The spike-response model (SRM) is a generaliza-
tion of the LIF model (Gerstner et al. 1996a;
Gerstner and Kistler 2002). It exploits the linearity
of Eq. 2 and expresses the membrane potential as
convolution:
V tð Þ ¼  t  bt


þ
ð1
0
k t, t0
ð
ÞIsyn t  t0
ð
Þdt0
ð10Þ
where (t  t0) is a kernel that describes the action
potential as well as the after hyperpolarization of
the last spike of the neuron at t ¼ t0. k t, bt


is a
kernel that describes the response of the mem-
brane potential to a presynaptic spike at time bt.
Like in the LIF model, a spike is generated when
the membrane potential V(t) crosses a threshold
value Vθ from below.
For theoretical analysis of large spiking net-
work, the spike-response model is more conve-
nient than the original LIF model, since it captures
the effects of incoming as well as self-generated
action potentials in a closed mathematical form
(Gerstner and Kistler 2002).
Stochastic Spike Generation and Escape Noise
For most neuron models with a deterministic spike
threshold, it is also possible to use a stochastic spike
generation mechanism by adding the so-called
escape noise (Plesser and Gerstner 2000). This is
done by relating the variable V(t) to the probability
density for observing an action potential in the
Spiking Network Models and Theory: Overview
111

inﬁnitesimal time interval (t, t þ δt] (Plesser and
Gerstner 2000; Mensi et al. 2012). In this picture,
the spike generation is a random point process with
a conditional density function
l tjV, Vy
ð
Þ ¼ l0 exp
V tð Þ  Vy
DV

	
,
ð11Þ
where l0 is a scaling factor with unit s  1, Vθ is
the ﬁring threshold, and ΔV is a factor that deter-
mines the steepness of the exponential function
and therefore also the sensitivity of the ﬁring
threshold to small ﬂuctuations.
The spike-response model with escape noise
belongs to the larger class of generalized linear
models (GLM) which have recently been used suc-
cessfully also in other areas of neuroscience (see,
e.g., Pillow et al. (2008), Truccolo et al. (2011)).
Stochastic Models of Neural Activity
In many regions of the brain, neurons ﬁre con-
stantly at a low rate, even if they are not directly
stimulated. In the cortex, this spontaneous activity
is very irregular. Moreover, even if a cortical neu-
ron in vitro or in vivo is stimulated repeatedly, its
response will differ for each stimulus presentation.
One of the earliest approaches to explain this
variability of neuronal ﬁring is to consider the
ﬂuctuation of the membrane potential of a neuron,
caused by randomly arriving spikes from the sur-
rounding network. In the simplest case, each neu-
ron in the surrounding network will ﬁre at some
constant rate n. The network, thus, generates a
noisy background activity which then inﬂuences
the ﬁring probability of our neuron.
The number of excitatory and inhibitory spikes
which arrive at neuron i during the interval (0, t]
can be written as
nE tð Þ ¼
X
j  Ei
ðt
0
s j t0  d
ð
Þdt0,
nI tð Þ ¼
X
j  Ii
ðt
0
s j t0  t
ð
Þdt0
where Ei and Ii denote the sets of all excitatory and
inhibitory
neurons
projecting
to
neuron
i,
respectively. The membrane potential of a neuron
can then be written as (Stein 1965; Tuckwell
1988)
dV
dt ¼  1
t V þ JE dnE
dt þ JI dnI
dt :
ð12Þ
For sufﬁciently large networks, we can assume
that nE and nI are Poisson processes with rates nE
and nI, respectively.
In the absence of a threshold, and for constant
ﬁring rates, the mean and variance of the mem-
brane depolarization are (Tuckwell 1988; Stein
1965)
E V
½  ¼ t JEvE þ JIvI
ð
Þ
ð13Þ
Var V
½  ¼ 1
2 t J2
EvE þ J2
I vI


:
ð14Þ
More generally, we can describe the randomly
arriving spikes as a shot noise process (Papoulis
and Pillai 2002). This allows us to compute the
mean and variance of the resulting membrane
potential as a function of the background ﬁring
rate n(t) and the synaptic response kernel psp(t).
The mean membrane potential is then
E V
½ t ¼
ðþ1
0
v t  s
ð
Þpsp sð Þds
ð15Þ
with variance
Var V
½ t ¼
ðþ1
0
v t  s
ð
Þpsp2 sð Þds
ð16Þ
When an embedding into a cortical region is
considered, random input comes from excitatory
and inhibitory populations. Due to the linearity of
the
equations,
the
contributions
from
both
populations superimpose, and the mean and vari-
ance of the combined membrane potential are
given by
E V
½ t ¼ E VE
½
t þ E VI
½
t
ð17Þ
and
Var V
½ t ¼ Var VE
½
t þ Var VI
½
t0
ð18Þ
respectively.
112
Spiking Network Models and Theory: Overview

Connections
A network consists of a set of N neurons and their
connections. In a connection between two neurons,
the sending neuron is called presynaptic and the
receiving neuron is called postsynaptic. We write
the set of presynaptic neurons of neuron i as Ni.
Static Connections
In the simplest case, the synaptic current simply
adds the spikes of all presynaptic cells:
Isyn,i tð Þ ¼
X
j  Ni
Jijs j t  dij


ð19Þ
¼
X
j  Ni
Jij
X
k¼1
d t  dij  bt
k
j


ð20Þ
where dij is the propagation delay between neuron
j and neuron i andbt
k
j the kth spike of neuron j. Jij is
the weight or efﬁcacy of the connection between
neurons j and i which is in many cases assumed to
be constant.
Dale’s Principle
Dale’s principle states that the efferent synapses of
a neuron are all of the same type. Thus, if one
efferent synapse of a neuron is excitatory, we
know that all other efferent synapses of this neu-
ron will also be excitatory. The same applies to
inhibition.
Dale’s principle introduces correlations into
the connectivity matrix of the network, which
are visible in the eigenvalue distribution of the
matrix in the complex plane (Rajan and Abbot
2006). Moreover, Dale’s principle also affects
the correlation structure of random spiking net-
works. In random networks without Dale’s prin-
ciple, correlations between neurons will vanish in
the limit of inﬁnitely large networks. In such
networks, Dale’s principle prevents the correla-
tions from disappearing (Kriener et al. 2008).
Short-Term Plasticity
The weight of a connection can depend on the
spike history of the presynaptic neuron such that
the weight decreases or increases if several
presynaptic spikes arrive in short succession.
These effects are called short-term depression
(STD) and short-term facilitation (STF), respec-
tively. Both can be described by a system of
kinetic equations that model the depletion and
replenishing
of
synaptic
resources
(Tsodyks
et al. 1998, 2000).
Assume that there is a ﬁnite amount of
resources available and that each spike uses a
certain fraction of these resources which are then
unavailable for a certain amount of time. Depleted
resources are replenished at a constant rate D. The
second factor in the model is the ability of a
synapse to use its resources, expressed by a vari-
able u(t). In stochastic models of synaptic trans-
mission, u corresponds to the release probability
of a synaptic release site (Fuhrmann et al. 2002). If
u is a constant, the synapse will be depressing. For
facilitating synapses, u increases with each spike
up to a maximum and decays at a constant rate F.
In the context of the STP literature, the maxi-
mal synaptic efﬁcacy is usually denoted by
A which corresponds to the synaptic weight J,
used throughout the rest of this entry. The effec-
tive weight for the nth spike can then be
expressed as
An ¼ AunRn
ð21Þ
with initial values
ui≔U,
ð22Þ
R1≔1:
ð23Þ
u and R are then iteratively updated according to
unþ1 ¼ U þ un 1  U
ð
Þ exp
Dtn
F

	
,
ð24Þ
Rnþ1 ¼ 1
þ Rn  Rnun  1
ð
Þ exp
Dtn
D

	
, ð25Þ
where Dtn≔bt
n  bt
n1.
The relation of the time constants D and
F
determines
whether
a
synapse
will
be
Spiking Network Models and Theory: Overview
113

depressing, facilitating, or a combination of the
two. However, closer analysis of the equations as
well as experimental results shows that the
detailed behavior of dynamic synapses depends
also on the spike frequency of the presynaptic
neuron (Fuhrmann et al. 2002).
Dynamic synapses have a strong effect on the
behavior of a network. Networks with facilitating
synapses can show collective synchronization of
all neurons, called population bursts (Tsodyks
et al. 2000). By contrast, in networks where the
facilitating and depressing synapses are distrib-
uted as indicated by experimental results, activity
is more stable as the synapses exert some gain
control on the network (Sussillo et al. 2007).
Short-term plasticity has also been linked to the
persistence of network states with elevated ﬁring
rates. These are thought to play an important role
in working memory (Mongillo et al. 2008).
Short-term plasticity is not related to Hebbian
plasticity, since it depends only on the activity of
the presynaptic neuron rather than on the activity
of both the pre- and postsynaptic neurons.
Spike-Timing-Dependent Plasticity
Spike-timing-dependent plasticity (STDP) is a
form of synaptic plasticity where the synaptic
efﬁcacy changes according to the relative timing
of pre- and postsynaptic action potentials.
A typical experimental protocol for spike-
timing-dependent plasticity involves two neurons
with a synaptic connection. Action potentials are
induced in both neurons in a deﬁned temporal
sequence by injecting current pulses. At the same
time, the strength of the synaptic potential is mea-
sured in the postsynaptic neuron. This procedure is
called pairing. After a number of pairings, each
with the same timing relation between pre- and
postsynaptic neurons, a change in the amplitude
of the PSP can be observed. Systematic variation of
the relative timing between pre- and postsynaptic
action potentials then reveals that the change in
PSP amplitude depends on the timing relation.
The classical results of Markram et al. (1997)
and Bi and Poo (1998) show that potentiation is
largest if the postsynaptic neuron spikes shortly
after the presynaptic neuron, while the synapse
became depressed if the postsynaptic neuron
ﬁred shortly before the presynaptic neuron. This
is captured by the following model (Gerstner et al.
1996b):
DJ ij ¼
X
^tj  Sj
X
^ti  Si
W ^t i  ^t j


(26)
where the sums run over all pre- and postsynaptic
spikes, respectively. W(Δt) is the so-called learn-
ing window, with
W Dt
ð
Þ ¼ Aþ exp
Dt
tþ

	
forDt > 0,
ð27Þ
W Dt
ð
Þ ¼ A exp
Dt
t

	
forDt < 0,
ð28Þ
and
Dt≔tpost  tpre:
ð29Þ
The learning window is modeled as an expo-
nential function which scales the degree to which
the weight changes, depending on the time inter-
val Δt. If the spike of the presynaptic neuron pre-
cedes the spike of the postsynaptic neuron, Δt is
positive and Eq. 27 will increase the weight W by
a value that is largest for short and smallest for
large intervals. If the postsynaptic neuron spikes
ﬁrst, Δt is negative and Eq. 28 will decrease W.
There are a number of variations to this model
as well as a number of alternative models which
incorporate hypothesized biophysical mechanisms,
underlying STDP (Morrison et al. 2008; Sjöström
and Gerstner 2010). But so far, the experimental
evidence does not allow to identify one of the
models as authoritative (Feldman 2012).
Spike-timing-dependent plasticity is a mecha-
nism that acts in addition to the synaptic short-
term plasticity, described in the previous section.
To obtain the ﬁnal synaptic efﬁcacy which com-
bines both short-term and spike-timing-dependent
plasticity, we replace A in Eq. 21 with J þ ΔJ:
An ¼ unRn J þ DJ
ð
Þ
ð30Þ
where J is the equilibrium weight.
114
Spiking Network Models and Theory: Overview

Network Topologies
In a spiking network, the weighted, directed graph
of the connections deﬁnes the topology of the
network.
Common
network
topologies
are
feedforward networks, recurrent networks, and
networks with spatial topologies.
Feedforward Networks
Feedforward networks can be broken down into
disjunct groups or layers of neurons G1, G2, . . .
where each neuron in group Gi is only connected
to neurons in group Gj with j  i.
The
propagation
of
spiking
activity
in
feedforward networks is directed and in the direc-
tion of ascending group numbers. As a result, the
total number of spikes in feedforward networks is
limited, and at any given time, only a fraction of
the neurons are active (Grifﬁth 1963).
Synfire Chains
Examples of feedforward networks are the com-
plete transmission line, also known as synﬁre
chain (Grifﬁth 1963; Abeles 1991). It consists
of l mutually exclusive groups Gi, with 1  i,
Gl, each containing w neurons. Each neuron in
group i projects to a certain number of neurons in
group i þ 1, thus forming a chain of groups of
neurons. If sufﬁciently many neurons in the ﬁrst
group ﬁre near simultaneously, they will ignite
the neurons in the second group and so on. The
spikes of the ﬁrst group will therefore travel
along the chain until either the activity disperses
or the chain comes to an end. Theoretically, this
can be described by the so-called pulse packets,
propagating
from
one
group
to
the
next
(Diesmann et al. 1999). In the ideal case, that is,
given a sufﬁcient number of spikes with a sufﬁ-
ciently narrow temporal spread, the pulse packet
will propagate unchanged from one group to the
next. And due to the divergent-convergent con-
nectivity, small deviations from this invariant
shape are then automatically repaired. However,
depending on the number of neurons in a group
and the connections between the groups, the
pulse packet may also win or lose spikes or
change its temporal precision (Diesmann et al.
1999; Câteau and Fukai 2001).
Recurrent Networks
Networks with feedback connections are called
recurrent. Recurrent networks are often random
with mixed excitation and inhibition. In random
networks with uniform connection probability,
each neuron has an equal probability to be
connected to another neuron. In other models,
the
connection
probability
between
neurons
i and j depends on their distance.
Neighborhood Preserving Topologies
Networks in which neurons have a well-deﬁned
position are called topology networks. In such
networks, the connection probability of two neu-
rons usually depends on their distance. For exam-
ple, the closer two neurons are, the higher is their
probability to be connected. The neuronal posi-
tions may correspond to the actual positions of the
neurons within a brain region, but they may also
correspond to positions in some abstract space, for
example, the orientation angle of a visual stimulus
or the direction of movement in a reaching task.
Thus,
neighboring
locations
in
stimulus
or
response coordinates are mapped to neighboring
neurons.
The simplest examples of topological networks
are line or ring networks, where the neurons are
aligned on a one-dimensional axis. If the two ends
of the axis are connected back to each other, the
line turns into a circle. This is useful for
representing periodic coordinates such as orienta-
tion angle or simply to avoid boundary effects at
the end of the line.
Models of visual processing often use two-
dimensional sheets or layers of neurons. Here
the coordinates can correspond to visual space,
e.g., the center of the neuron’s receptive ﬁeld or
cortical space, that is, the actual position of the
neuron in the brain. To ameliorate processing at
the boundaries of the sheet, the two pairs of
opposing sides are often connected such that the
rectangular sheet turns into a torus.
Typical models of visual processing will com-
bine several sheets into a functional architecture
(e.g., Masquelier and Thorpe 2007; Grossberg
and Versace 2008). Topological networks can be
feedforward, recurrent, or a combination of
the two.
Spiking Network Models and Theory: Overview
115

Network Dynamics
Spiking neural network model are an important
tool to study the dynamics of cortical network
activity
and
to
understand
physiologically
observed phenomena such as spontaneous activ-
ity, neuronal synchronization, and network oscil-
lations.
These
are
macroscopic
network
phenomena which we call the macro state of the
network. Each macro state usually comprises a
large ensemble of micro states, deﬁned by the
individual states of all neurons and synapses.
Many macro states, observed in cortical net-
works, can be understood, using a simple network
with randomly connected excitatory and inhibitory
neurons (Brunel 2000). The model consists of three
populations of neurons: one population of excit-
atory neurons, one population of inhibitory neu-
rons, and a third population of excitatory neurons,
representing the long-range input from other brain
regions. The excitatory and inhibitory populations
are mutually coupled, and both receive input from
the background population. We can now study the
activity patterns which emerge, depending on the
ratio of inhibition to excitation and the strength of
the background population.
If the amount of excitation and inhibition is
roughly equal so that on average the respective
synaptic currents cancel each other, we speak of
balanced network. In this regime, network activ-
ity is highly irregular, since spikes are generated
by the difference in ﬂuctuations of the excitatory
and inhibitory currents (Tsodyks and Sejnowski
1995).
If all neurons ﬁre independently of each other
and their spike patterns are essentially random, we
speak of asynchronous-irregular activity. This
state is of particular interest, because it corre-
sponds best to the state of low-rate spontaneous
activity, observed in cortical networks.
Networks in the asynchronous-irregular state
exhibit chaotic activity (van Vreeswijk and
Sompolinsky 1996). Thus, even small perturba-
tion such as adding or removing one spike will
quickly result in a completely different micro
state. Whether this critical dependence on the
initial conditions is useful or detrimental for
spike-based processing is still under debate
(Izhikevich 2006; London et al. 2010).
If the neurons ﬁre independently, but each at
regular intervals, we speak of asynchronous-
regular activity. In this state, each neuron oscil-
lates at a different frequency, and occasionally,
large parts of the network will ﬁre synchronously.
The intervals of these population events depend
on the common multiples of the individual ﬁring
periods. It is also possible that all neurons ﬁre at
the same frequency, but out of phase. In this case,
neurons will not be able to occasionally synchro-
nize their spikes.
If all neurons ﬁre synchronously with the same
frequency, the network essentially oscillates and
we speak of synchronous-regular activity. Oscil-
latory states are also of interest since they can
serve as models for epileptic activity or for
stimulus-evoked synchronization.
Theoretically,
there
is
also
a
state
of
synchronous-irregular activity, a state where
each neuron produces the same irregular spike
train. Then the spikes of all neurons will occur in
synchrony, but the intervals between the spikes
will be irregular. However, this state is very
unlikely to occur in random networks because it
requires a well-chosen connectivity.
Cross-References
▶Bayesian Inference with Spiking Neurons
▶Excitability: Types I, II, and III
▶Fitzhugh–Nagumo Model
▶Hodgkin-Huxley Model
▶Integrate and Fire Models, Deterministic
▶Morris–Lecar Model
▶Multistability in Neurodynamics: Overview
▶Neuronal Avalanches
▶Pulse-Coupled Oscillators
▶Recurrent
Network
Models,
Reservoir
Computing
▶Spike-Frequency Adaptation
▶Spike-Timing Dependent Plasticity, Learning
Rules
▶Spontaneous Activity, Models of
▶Theta Neuron Model
Acknowledgments This work was supported by the Blue
Brain Project and EU grant FP7-269921 (BrainScaleS).
116
Spiking Network Models and Theory: Overview

References
Abeles M (1991) Corticonics: neural circuits of the cere-
bral cortex. Cambridge University Press, Cambridge
Bi GQ, Poo MM (1998) Synaptic modiﬁcations in cultured
hippocampal neurons: dependence on spike timing,
synaptic
strength,
and
postsynaptic
cell
type.
J Neurosci 18(24):10464–10472
Brunel N (2000) Dynamics of sparsely connected networks
of excitatory and inhibitory spiking neurons. J Comput
Neurosci 8(3):183–208
Câteau H, Fukai T (2001) Characteristic futures of pulse
packet propagation in synﬁre chain revealed by Fokker-
Planck method. Neural Netw 14:675–685
Diesmann M, Gewaltig MO, Aertsen A (1999) Stable
propagation of synchronous spiking in cortical neural
networks. Nature 402(6761):529–533
Feldman DE (2012) The spike-timing dependence of plas-
ticity. Neuron 75(4):556–571
Fuhrmann G, Segev I, Markram H, Tsodyks MV
(2002) Coding of temporal information by activity-
dependent synapses. J Neurophysiol 87(1):140–148
Gerstner W, Kistler WM (2002) Spiking neuron models:
single neurons, populations, plasticity. Cambridge Uni-
versity Press, Cambridge
Gerstner W, Hemmen JV, Cowan J (1996a) What matters in
neuronal locking? Neural Comput 8:1653–1676
Gerstner W, Kempter R, van Hemmen J, Wagner
H (1996b) A neuronal learning rule for sub-millisecond
temporal coding. Nature 383(6595):76–78
Grifﬁth JS (1963) On the stability of brain-like structures.
Biophys J 3:299–308
Grossberg S, Versace M (2008) Spikes, synchrony, and
attentive learning by laminar thalamocortical circuits.
Brain Res 1218:278–312
Hill A (1936) Excitation and accommodation in nerve.
Proc R Soc Lond B 814:305–355
Hodgkin A, Huxley A (1952) A quantitative description of
membrane current and its application to conduction and
excitation in nerve. J Physiol 117(4):500–544
Izhikevich EM (2006) Polychronization: computation with
spikes. Neural Comput 18(2):245–282
Kriener B, Tetzlaff T, Aertsen A, Diesmann M, Rotter
S (2008) Correlations and population dynamics in cor-
tical networks. Neural Comput 20(9):2185–2226
Lapicque
L
(1907)
Recherches
quantitatives
sur
l’excitation electrique des nerfs traitee comme une
polarization. J Physiol Pathol Gen 9:620–635
London M, Roth A, Beeren L, Hausser M, Latham PE
(2010) Sensitivity to perturbations in vivo implies
high noise and suggests rate coding in cortex. Nature
466(7302):123–127
Markram H, Lübke J, Frotscher M, Sakmann B (1997)
Regulation of synaptic efﬁcacy by coincidence of post-
synaptic APs and EPSPs. Science 275:213–215
Masquelier T, Thorpe SJ (2007) Unsupervised learning of
visual features through spike timing dependent plastic-
ity. PLoS Comput Biol 3(2):e31
Mensi S, Naud R, Pozzorini C, Avermann M, Petersen
CCH, Gerstner W (2012) Parameter extraction and
classiﬁcation of three cortical neuron types reveals
two distinct adaptation mechanisms. J Neurophysiol
107(6):1756–1775
Mongillo G, Barak O, Tsodyks M (2008) Synaptic theory
of
working
memory.
Science
(NY)
319(5869):
1543–1546
Morrison A, Diesmann M, Gerstner W (2008) Phenome-
nological models of synaptic plasticity based on spike
timing. Biol Cybern 98(6):459–478
Papoulis A, Pillai SU (2002) Probability, random variables
and stochastic, 4th edn. McGraw-Hill, Boston
Pillow JW, Shlens J, Paninski L, Sher A, Litke AM,
Chichilnisky EJ, Simoncelli EP (2008) Spatio-temporal
correlations and visual signalling in a complete neuro-
nal population. Nature 454(7207):995–999
Plesser HE, Gerstner W (2000) Noise in integrate-and-ﬁre
neurons: from stochastic input to escape rates. Neural
Comput 12(2):367–384
Rajan K, Abbot LF (2006) Eigenvalue spectra of random
matrices for neural networks. Phys Rev Lett 97(18):
188104
Sjöström J, Gerstner W (2010) Spike-timing dependent
plasticity. Scholarpedia 5(2):1362
Stein R (1965) A theoretical analysis of neuronal variabil-
ity. Biophys J 5(2):173–194
Sussillo D, Toyoizumi T, Maass W (2007) Self-tuning of
neural circuits through short-term synaptic plasticity.
J Neurophysiol 97(6):4079–4095
Truccolo W, Donoghue JA, Hochberg LR, Eskandar EN,
Madsen JR, Anderson WS, Brown EN, Halgren E,
Cash SS (2011) Single-neuron dynamics in human
focal epilepsy. Nat Neurosci 14(5):635–641
Tsodyks MV, Sejnowski T (1995) Rapid state switching in
balanced cortical network models. Netw Comput Neu-
ral Syst 6(2):111–124
Tsodyks M, Pawelzik K, Markram H (1998) Neural net-
works with dynamic synapses. Neural Comput 10(4):
821–835
Tsodyks MV, Markram H, Uziel A (2000) Synchrony
generation in recurrent networks with frequency-
dependent synapses. J Neurosci 20(1):50
Tuckwell HC (1988) Introduction to theoretical neurobiol-
ogy: volume 2 nonlinear and stochastic theories. Cam-
bridge University Press, Cambridge
van Vreeswijk C, Sompolinsky H (1996) Chaos in neuro-
nal networks with balanced excitatory and inhibitory
activity. Science 274(5293):1724–1726
Vogels T, Abbot LF (2005) Signal propagation and logic
gating in networks of integrate-and-ﬁre neurons.
J Neurosci 25(46):10786–10795
Further Reading
A thorough introduction to the theory of spiking networks
can be found in the somewhat dated but still highly
valuable textbooks Introduction to theoretical neurobi-
ology by Tuckwell (1988). An equally thorough and
more recent reference is the book Spiking neuron
models: Single neurons, populations, plasticity by
Gerstner and Kistler (2002) which also contains
Spiking Network Models and Theory: Overview
117

extensive treatment of learning and plasticity in spiking
networks. A broader overview is given in the textbook
Theoretical Neuroscience by Dayan and Abbot (2001)
Burkitt AN (2006) A review of the integrate-and-ﬁre neu-
ron model: I. Homogeneous synaptic input. Biol
Cybern 95(1):1–19
Burkitt AN (2006) A review of the integrate-and-ﬁre neu-
ron model: II. Inhomogeneous synaptic input and net-
work properties. Biol Cybern 95(2):97–112
Burkitt (2006a, b) has written a set of comprehensive
reviews of the integrate and ﬁre neuron. The dynamic
properties of spiking networks have been reviewed by
Vogels et al. (2005)
Dayan P, Abbot LF (2001) Theoretical neuroscience: com-
putational and mathematical modeling of neural sys-
tems. MIT Press, Cambridge, MA
Feldman (2012) provides an extensive review of spike-
timing-dependent plasticity, with possible underlying
synaptic and cellular mechanisms, as well as its poten-
tial role in learning. The reviews of Morrison et al.
(2008) and Sjöström and Gerstner (2010) give good
overview over theoretical models of spike-timing-
dependent plasticity
Vogels TP, Rajan K, Abbot LF (2005) Neural network
dynamics. Annu Rev. Neurosci 28:357–376
Spinal and Neuromechanical
Integration: Overview
Matthew Tresch1 and Devin L. Jindrich2
1Department of Biomedical Engineering and
Physical Medicine and Rehabilitation,
Northwestern University, Evanston, IL, USA
2Department of Kinesiology, California State
University, San Marcos, CA, USA
Detailed Description
To interact with the environment or other organ-
isms, the nervous system must move. Whether it is
a fundamental protective reﬂex, a stabilizing pos-
tural adjustment, a rapid prey strike, or an expres-
sive dance gesture, the motor repertoire of an
organism deﬁnes the nature of its environmental
interactions. Structures in the periphery of the
motor system, the musculoskeletal system and
spinal
cord,
most
directly
mediate
these
environmental interactions. The actions of central
brain regions such as cortex, cerebellum, or basal
ganglia all ultimately have to pass through these
peripheral structures. Understanding the proper-
ties of these peripheral systems is therefore critical
for our understanding of the neural control of
movement.
How should we consider these peripheral com-
ponents of the motor system? In one common
perspective, these systems are problems that the
CNS must overcome. In this perspective, the com-
plex properties of muscles, limb mechanics,
motor neurons, and spinal circuits require that
central motor systems develop strategies that
invert, bypass, or suppress these complexities.
Complexity could therefore lead to greater com-
plexity: evolutionary changes to the periphery
could require the co-evolution of more complex
mechanisms to maintain performance.
In an alternate perspective, peripheral systems
might simplify for motor control. The complexi-
ties of spinal systems or nonlinear properties of
the musculoskeletal system might reﬂect adapta-
tions that allow simpliﬁed control by descending
systems. For example, passive mechanics can be
used to assist movements (Collins et al. 2005),
muscle properties can contribute to stability
(Nichols and Houk 1973), basic reﬂexes allow
for rapid control (Loeb et al. 1999), and deﬁning
adaptive muscle coordination patterns can poten-
tially simplify movement (Tresch and Jarc 2009).
In this perspective, energetic costs associated with
inefﬁcient, complex control might lead to evolu-
tionary adaptations that simplify control and neu-
ral processing.
These two perspectives are not mutually exclu-
sive, and both emphasize the importance of under-
standing the contribution of peripheral systems to
motor control and brain function. Entries in other
sections of this Encyclopedia and the Encyclope-
dia of Neuroscience describe the basic physiolog-
ical properties of muscles (see Heckman et al.
2009), motor units (see Burke 2009), reﬂexes
(see Grey and Nielsen 2009), and pattern generat-
ing spinal networks (see ▶Vertebrate Pattern
Generation: Overview). The entries in this section
118
Spinal and Neuromechanical Integration: Overview

expand on those descriptions, characterizing how
these systems function and how they interact with
descending systems. Spinal interneuronal systems
have been studied extensively for more than a
century, and although much remains unknown
about their role in behavior, these studies have
provided basic information that can be used to
guide computational studies of motor control
(▶Spinal Cord, Integrated (Non CPG) Models
of). Proprioceptors provide the nervous system
with information about body state – the position
of the limb, the forces produced by muscles, and
the interactions with the environment. Under-
standing how these state variables are encoded
in the activity of proprioceptor afferents is
critical
in
understanding
what
information
the
nervous
system
has
access
to
when
interpreting and interacting with the environ-
ment (▶“Proprioceptor Models”). Although
spinal systems are capable of a great deal of
motor coordination on their own, including cen-
tral pattern generators for basic protective
reﬂexes and for locomotion, voluntary behav-
iors are accomplished by descending pathways
from the brain including the cortex. How these
systems interact with one another to produce
movement, however, remains poorly under-
stood. One central computational issue in these
interactions is how information about task goals
(e.g., the location of food to be grasped) is trans-
lated into motor commands (e.g., the muscle
activations that result in movement of the arm
and hand to the food) (Coordinate Transforma-
tions, Role of Spinal Circuitry in).
Ultimately motor control is the result of a single
dynamic system including both the nervous system
and body, coupled together through intrinsic sen-
sory feedback also through dynamic interactions
with the environment. Neuromechanics is the ﬁeld
of study that seeks to understand how these two
systems work together to produce behavior. Entries
in this section examine the neuromechanics of
postural control (Neuromechanics of Postural Con-
trol), characterizing how neural and musculoskele-
tal systems interact to stabilize the body to maintain
upright stance, and the neuromechanics of joint
coordination during locomotion (Neuromechanics
of Joint Coordination), characterizing how task
goals are accomplished through dynamic coordina-
tion of low level execution variables.
Cross-References
▶Coordinate Transformations, Role of Spinal
Circuitry in
▶Decision-Making, Motor Planning
▶General Overview of Spinal Anatomy and
Physiology Organization
▶Motoneurons and Neuromuscular Systems:
Overview
▶Neuromechanics of Joint Coordination
▶Neuromechanics of Postural Control
▶Neuromuscular Control Systems, Models of
▶Proprioception
▶Proprioceptor Models
▶Spinal Cord, Integrated (Non CPG) Models of
▶Vertebrate Pattern Generation: Overview
References
Burke
RE
(2009)
Motor
units.
In:
Binder
MD,
Hirokawa N, Windhorst U (eds) Encyclopedia of neu-
roscience. Springer, Berlin/Heidelberg, pp 2443–2446
Collins S, Ruina A, Tedrake R, Wisse M (2005) Efﬁcient
bipedal robots based on passive-dynamic walkers. Sci-
ence 307:1082–1085
Georgopoulos AP, Grillner S (1989) Visuomotor coordina-
tion
in
reaching
and
locomotion.
Science
245:
1209–1210
Grey MJ, Nielsen JB (2009) Integration of spinal reﬂexes.
In: Binder MD, Hirokawa N, Windhorst U (eds) Ency-
clopedia of neuroscience. Springer, Berlin/Heidelberg,
pp 1985–1988
Heckman CJ, Perreault E, Sandercock T, Maas H (2009)
Muscle. In: Binder MD, Hirokawa N, Windhorst
U (eds) Encyclopedia of neuroscience. Springer, Ber-
lin/Heidelberg, pp 2479–2487
Loeb GE, Brown IE, Cheng EJ (1999) A hierarchical foun-
dation for models of sensorimotor control. Exp Brain
Res 126:1–18
Nichols TR, Houk JC (1973) Reﬂex compensation for
variations in the mechanical properties of a muscle.
Science 181:182–184
Tresch MC, Jarc A (2009) The case for and against muscle
synergies. Curr Opin Neurobiol 19:601–607
Spinal and Neuromechanical Integration: Overview
119

Spinal Interfaces: Overview
Jacob G. McPherson1 and Michel Lemay2
1Program in Physical Therapy and Department of
Anesthesiology, Washington University School of
Medicine, St. Louis, MO, USA
2Department of Bioengineering, Temple
University, Philadelphia, PA, USA
Definition
Spinal interfaces have traditionally focused on
electrical stimulation of spinal cord tissue (i.e.,
white matter axonal tracts and/or gray matter neu-
rons) or associated spinal roots (see ▶“General
Overview of Spinal Anatomy and Physiology
Organization” in this Encyclopedia). Spinal inter-
face applications have primarily centered about
restoration of function lost to neurological impair-
ment
and
study
of
spinal
neural
circuitry
(organization, basic functions). More recently,
spinal interfaces are being developed to record
neural activity from large populations of spinal
neurons and to deliver pharmacological agents to
speciﬁc regions of the spinal cord. These inter-
faces may also be capable of simultaneously stim-
ulating spinal tissue. Emerging application areas
of spinal interfaces include chronic monitoring of
neural transmission to serve as a biomarker of
pathology/recovery and so-called closed-loop
stimulation protocols, in which delivery of spinal
stimulation is contingent upon real-time detection
of salient neural activity. This latter category is
particularly useful for studying and promoting
neural plasticity in spinal circuits.
Methodologies to record and/or modulate neu-
ral transmission in spinal tissue can be divided on
the basis of applications (bladder, movement res-
toration, pain reduction, etc.), electrode technol-
ogy employed (electrical, optical, etc.), electrode
position (e.g., epidural, intradural, intraspinal), or
a number of other characteristics. In this entry,
spinal interface techniques are presented based
on the degree of invasiveness, and the similar
technological methods used to record from or
stimulate the neural tissue are highlighted. In
increasing order of invasiveness, spinal interfaces
can be divided into (1) transcutaneous (recording
and stimulation), (3) epidural, (4) intradural or
intrathecal, and (5) intraspinal.
Detailed Description
High-Density Surface Electromyography
Spinal alpha motoneurons have long been the
only neuron in the human central nervous system
from which it is possible to obtain direct record-
ings. This is because motoneuron action poten-
tials occur in a one-to-one ratio with motor unit
action potentials (MUAPs), which are discrete
action potentials evident in muscle ﬁbers that
can be accessed via electromyography (i.e.,
recording
muscle
electrical
activity,
EMG).
Because each MUAP waveform has unique spa-
tiotemporal features and because each muscle
ﬁber is innervated by only one motoneuron
(together, a “motor unit”), neural ﬁring patterns
of individual motoneurons can be discriminated
from one another based on the shape of a given
MUAP. These ﬁring patterns are referred to as
spike trains. When EMG is used to discriminate
individual motor units and/or spike trains, it can
be considered a form of transcutaneous spinal
interface.
Traditional approaches for recording MUAPs,
and by extension for accessing spike trains of
discrete motoneurons, have relied on percutane-
ous needle-style or ﬁne-wire electrodes that pen-
etrate the skin and anchor into the muscle ﬁbers
below (i.e., intramuscular EMG). In addition to its
invasiveness,
this
technique
results
in
an
extremely low yield of detectable motor units
(~2–5/muscle) and provides reliable information
only during contractions of low force magnitude.
Over the last 5–10 years, however, a powerful
new technology has emerged for accessing moto-
neuron spike trains: high-density surface EMG
(HD-sEMG) decomposition (Del Vecchio et al.
2020). In this non-invasive technique, ﬂexible
skin-surface electrodes containing 10’s of electri-
cally and spatially isolated recording channels
(typically 64 or 128) are placed over a muscle of
interest.
Surface
EMG
activity
is
recorded
120
Spinal Interfaces: Overview

simultaneously from each channel. Subtle differ-
ences in the spatiotemporal and spectral charac-
teristics of each channel are then exploited by
advanced statistical signal processing techniques
(e.g., blind-source separation algorithms, inde-
pendent components analysis) to decompose the
multichannel surface EMG activity into individual
MUAP
waveforms
and
spike
trains.
This
approach enables access to entire populations of
discrete motor units (to date, ranging from ~10 to
40/muscle), providing a remarkably detailed win-
dow into descending neural drive and spinal motor
output and distinguishing HD-sEMG decomposi-
tion from the general overview of motor unit func-
tion afforded by traditional intramuscular EMG.
HD-sEMG decomposition is also substantially
more robust than intramuscular EMG during con-
tractions of increasing force magnitude, enabling
studies of human motor unit recruitment and rate
modulation that were previously either inferential
or, more often than not, infeasible.
While the potential applications of HD-sEMG
decomposition are near limitless, early examples
have included advanced myoelectric control of
prostheses (Kapelner et al. 2019), pathological
neural control of movement following neurologic
injury (Miller et al. 2014), and spinal motor output
in the decerebrate cat (Thompson et al. 2018).
Transcutaneous Stimulation
Transcutaneous spinal stimulation refers to the
activation of spinal tissue through the skin. It is
broadly categorized either as magnetic stimula-
tion or electrical stimulation. In the case of mag-
netic
transcutaneous
spinal
stimulation,
a
magnetic ﬁeld induces ionic currents within the
spinal tissue (see ▶“Paraspinal Magnetic and
Transcutaneous Electrical Stimulation” entry in
this
Encyclopedia).
The
basic
methodology
involves inducing depolarization via a time-
varying magnetic ﬁeld delivered transcutaneously
by a coil applied to the skin surface. This approach
is similar in concept to transcranial magnetic stim-
ulation (TMS) used to probe cortical circuits. The
principal applications of transcutaneous spinal
magnetic stimulation have been for the initiation
of locomotion following spinal cord injury or in
Parkinsonian patients and for the treatment of pain
when stimulation is delivered over the dorsal col-
umns (see ▶“Paraspinal Magnetic and Transcu-
taneous Electrical Stimulation” entry in this
Encyclopedia). Finite element model studies of
the spinal cord indicate that the greatest site of
activation is within the tracts due to the lower
threshold of activation of myelinated ﬁbers of
the white matter versus the cells of the gray matter
(see ▶“Finite Element Models of Transcutaneous
Spinal Cord Stimulation” entry in this Encyclope-
dia). Since activation is typically delivered dor-
sally, the largest site of depolarization typically
involves posterior roots and dorsal columns,
although the anterior motor roots are also hot
spots of depolarization.
Transcutaneous electrical stimulation can be
used to indirectly modulate neural transmission
in spinal circuits. In this approach, skin-surface
electrodes are placed dorsally over the relevant
vertebrae and traditional, pulsatile electrical stim-
ulation is delivered. Rather than passing current
directly into the spinal cord, the electrical stimu-
lation is used to modulate neural transmission in
primary afferent nerve ﬁbers providing sensory
inputs to spinal circuits damaged or weakened
by injury. Although rarely described as such,
some variants of classical transcutaneous electri-
cal nerve stimulation (TENS) for pain are exam-
ples of this approach. More recently, however,
transcutaneous electrical spinal stimulation has
been introduced as a possible means to aid loco-
motor and upper limb rehabilitation following
spinal cord injury (Inanici et al. 2018). The efﬁ-
cacy of this approach has not been demonstrated
in randomized controlled trials as yet, and efforts
to characterize the neural mechanisms by which it
exerts potential therapeutic effects are ongoing
but remain incomplete (Benavides et al. 2020).
Transcutaneous electrical spinal stimulation
can also be delivered as a form of direct current
stimulation, referred to as spinal cord direct cur-
rent stimulation or transcutaneous direct current
spinal stimulation (sDCS, tDCSS). Its use for
modulating neural transmission in spinal circuits
has paralleled that of similar approaches for mod-
ulating neural transmission in the brain. In tDCSS,
two adhesive skin surface electrodes are placed on
either side of the abdomen, one ventrally and one
Spinal Interfaces: Overview
121

dorsally. These electrodes are connected to the
anode and cathode of a constant current stimula-
tor, which drives current between the two sites.
Unlike traditional transcutaneous electrical stim-
ulation approaches (above), in which current is
delivered in discrete pulses or trains thereof, in
tDCSS the current is continuous (i.e., non-
pulsed). In this regard, it can be considered to
polarize the neural tissue for a period of time.
The potential applications of tDCSS and the
mechanisms by which it modulates neural trans-
mission remain a source of active research. How-
ever, tDCSS appears to modulate transmission in
motor and sensory regions of the spinal cord, as
well as primary motor and sensory cortices, in a
polarity-dependent manner (Cogiamanian et al.
2008, Ahmed 2011, Song and Martin 2017).
These effects are thought to be driven at least in
part by facilitation of certain forms of activity-
independent neural plasticity (Jankowska 2017).
Given its modulatory capacity and relative lack of
invasiveness, tDCSS may be useful for priming
spinal circuits during rehabilitation interventions.
Epidural Stimulation
Epidural stimulation refers to electrical stimula-
tion delivered via leads inserted within the verte-
bral canal but outside the dura. Surgery is
involved, but no stimulating electrodes are actu-
ally placed within the central nervous system or in
the cerebrospinal ﬂuid (CSF). Epidural stimula-
tion is currently employed in a number of appli-
cations. Sacral anterior root stimulation combined
with posterior sacral rhizotomy has been used
extensively to restore bladder and bowel conti-
nence and voiding following spinal cord injury
(Brindley 1988). Although most electrodes are
implanted intradurally around the sacral roots,
epidural electrode leads are sometimes used to
activate the anterior sacral roots (typically S2,
S3, and S4) (Egon et al. 1998). The anterior root
stimulator takes advantage of the different con-
traction and relaxation times of the sphincter and
bladder muscles. While the muscle ﬁbers that
constitute the sphincter are mainly “fast-twitch”
muscle ﬁbers that fatigue easily and relax very
quickly, the ﬁbers that compose the detrusor mus-
cles
are
“slow-twitch”
ﬁbers
that
contract
gradually and maintain force for a longer period
of time. With intermittent short bursts of high-
frequency electrical pulses to the sacral nerve
roots, bladder and sphincter muscles contract,
but while the sphincter relaxes rapidly at the end
of each stimulus burst, the pressure in the bladder
is still sufﬁciently high to produce urination. Acti-
vation of the S2–S4 roots with longer stimulus
trains is used to obtain defecation in a number of
patients. Variations of the basic principles are still
employed today, although advances in our knowl-
edge of urogenital anatomy and electrode design
have
led
to
more
sophisticated
stimulation
schemes (see ▶“Methodologies for the Restora-
tion of Bladder and Bowel Functions” in this
Encyclopedia).
Epidural stimulation has also been used exten-
sively for the restoration of locomotion following
spinal cord injury in various animal models and
humans (see ▶“Epidural Stimulation” entry in
this Encyclopedia). In all species, stimulation is
delivered over the dorsal columns with the most
efﬁcacious segments for initiating locomotion
varying between species. The stimulus train is typ-
ically a low-frequency (40 Hz) train of pulses deliv-
ered continuously over the dorsal columns at
lumbar levels to produce activation of the locomo-
tor circuitry. The exact mechanisms of activation or
even the neuronal composition of the locomotor
centers remains unknown, but the technique has
been used clinically to restore standing and some
locomotor movements in individuals with com-
plete motor paralysis below the level of injury
when coupled with highly intensive physical ther-
apy (Edgerton and Harkema 2011; Harkema et al.
2011, Wagner et al. 2018). In animal models, the
technique has also been combined with intrathecal
delivery of targeted therapeutics to further aid res-
toration of locomotion and for upper limb rehabil-
itation, although these areas have yet to be realized
clinically (Musienko et al. 2011, Barra et al. 2018).
Epidural stimulation has also been employed to
facilitate the initiation of locomotion in Parkinso-
nian patients (Fuentes et al. (2010), also see ▶“Spi-
nal Stimulation for Parkinson Treatment” in this
Encyclopedia).
Finally, epidural stimulation of the dorsal col-
umns has been applied for the treatment of chronic
122
Spinal Interfaces: Overview

and intractable pain arising from conditions such
as failed back surgery syndrome, phantom limb
pain, spinal cord injury, diabetic neuropathy, and
ischemic
limb
pain
(Kumar
et
al.
1998;
Ramasubbu et al. 2013, Chari et al. 2017). Stim-
ulation is delivered through electrode leads placed
over the dorsal columns in an attempt to provide
analgesia for a number of various chronic pain
conditions. Success rate in the long-term relief of
pain is variable, ranging from about 20% to 80%
(Kumar et al. 1998). Similar to epidural electrical
stimulation
for
locomotor
rehabilitation,
the
mechanisms of analgesia remain unclear. In
more recent implementations, which use stimula-
tion frequencies in the KHz range, it is thought
that the stimulation blocks neural transmission in
ascending pain pathways (Crosby et al. 2017,
Linderoth and Foreman 2017).
For both locomotor and analgesic applications,
epidural stimulation has faced two particular chal-
lenges in clinical translation: off-target effects and
the duration of therapeutic beneﬁts. Off-target
effects frequently take the form of parasthesias,
which range from mildly irritating to painful in
their own right. In some individuals, these effects
limit the time that stimulation can be tolerated.
A beneﬁt of newer KHz-range stimulation para-
digms is that they may evoke fewer or less intense
parasthesias, although the efﬁcacy of these para-
digms for locomotor rehabilitation has yet to be
investigated (Crosby et al. 2017). Regarding the
duration of effects, in studies to date the therapeu-
tic beneﬁts of epidural stimulation often diminish
rapidly upon cessation of stimulation. This phe-
nomenon is presumably related both to the rela-
tively nonspeciﬁc delivery of current to the spinal
cord and to the non-physiological and asynchro-
nous stimulus waveforms used to modulate neural
transmission. While more targeted stimulation
approaches and closed-loop stimulation para-
digms may address some of these drawbacks,
they are not without their own barriers to clinical
translation (see below and other entries in this
section).
Intradural Stimulation
Next in order of increasing invasiveness is
intradural or intrathecal stimulation. Here, leads
are placed within the dural sac itself. Surgical and
infection risks increase with this approach, as the
implant is now placed within the central nervous
system space and CSF and the blood-brain barrier
is breached. Dural closure remains a surgical dif-
ﬁculty. However, closer proximity to the spinal
cord provides greater access to speciﬁc spinal
structures, allowing stimulation to be delivered
to particular roots. Thus, it is no surprise that the
Brindley-Finetech (described in Epidural Stimu-
lation above) sacral anterior root electrodes are
mostly implanted intradurally over the divided
(anterior separated from posterior) sacral roots to
restore bladder and bowel functions.
Intraspinal Stimulation
Intraspinal microstimulation (ISMS) provides the
greatest access to the spinal cord but is also the
most invasive and thus riskiest stimulation tech-
nique in terms of the potential damage it may
cause to the nervous system. Stimulation is deliv-
ered within the spinal cord, typically via micro-
wires implanted within the white and gray matter,
although new high-density microelectrode array
and optogenetic techniques amenable to spinal
cord applications are being developed (Alilain
et al. 2008, Lu et al. 2017). A particular beneﬁt
of ISMS for rehabilitation purposes is that it pre-
serves orderly, physiological recruitment of motor
units based on Henneman’s Size Principle and
allows smooth grading of contraction magnitude.
This
is
contrasted
by
the
well-documented
reversed recruitment order and ballistic contrac-
tions common to transcutaneous neuromuscular
electrical stimulation (NMES) techniques. ISMS
of motor pools also readily recruits groups of
synergist
muscles,
which
has
fueled
many
advances in our basic science understand of spinal
physiology but also provides an opportunity to
cohesively restore grouped movements via a sin-
gle stimulation site (which is not possible with
transcutaneous NMES).
Applications of ISMS have included direct
activation of motor pools/motor units to cause
muscle
contractions
that
aid
locomotion
(Bamford and Mushahwar 2011), bladder and
bowel functions (Pikov et al. 2007), and respira-
tion (Sunshine et al. 2018) (see ▶“Intraspinal
Spinal Interfaces: Overview
123

Stimulation” in this Encyclopedia). Emerging
applications include the induction and study of
neural plasticity – particularly spike-timing-
dependent plasticity – for rehabilitation using
closed-loop neural computer interface approaches
(McPherson et al. 2015, and see ▶“Electrical
Conditioning for Spike-Timing-Dependent Plas-
ticity of Neural Circuits” in this Encyclopedia).
And while the invasiveness of ISMS has pre-
sented a conceptual barrier to widespread transla-
tion, multiple efforts are currently underway to
achieve regulatory approval for ISMS electrodes
in humans. One need look no farther than deep
brain stimulation electrodes as an example of the
potential clinical translational path for efﬁcacious
but invasive neuromodulatory therapies.
Electrode Technologies: Issues and
Potential Solutions
The electrodes used to deliver stimulation to the
spinal cord are for the most part similar to the ones
used to deliver electrical stimulation to peripheral
nerves and/or the brain. They include cuff elec-
trodes that use metalized contacts (typically plat-
inum) encapsulated within silicone or other
materials. These electrodes are typically used to
stimulate roots (Brindley-Finetech anterior root
stimulator electrode, and see Xiao et al. 2012 for
more advanced designs). Metal disk or lead wires
are
typically
used
to
deliver
stimulation
epidurally, and microwires are used to deliver
stimulation intraspinally. New electrodes being
developed and tested in animal models aim to
increase the number of contacts and stimulation
sites which would allow for greater selectivity in
the delivery of stimulation. These include elec-
trode designs for intraspinal microstimulation
(Snow et al. 2006) and multi-contact surface elec-
trodes for epidural stimulation (Gad et al. 2013).
Intraspinal microstimulation remains one of
the toughest applications as the stiff metal- or
silicone-based electrode presents a mechanical
impedance mismatch compared to the higher
compliance of the spinal tissue, leading to tissue
damage. In addition, the spinal cord glides sub-
stantially
within
the
vertebral
canal
during
movements (Ranger et al. 2008), and the tension
it produces in the cable attaching electrode to
associated connector may cause further damage
to the tissue unless the cable’s ﬂexibility is sufﬁ-
cient to dissociate the motions of the vertebral
canal and spinal cord. A promising approach to
minimizing mismatches in tissue and electrode
material impedances is the braided electrode
design of the Giszter group (see ▶“Braided Elec-
trodes” entry in this Encyclopedia). By braiding
extremely ﬁne wires, a very high overall compli-
ance electrode and cabling technology is pro-
duced that can be inserted within the spinal cord
with the help of a removable cannula guide and
produce minimal histological damage to the cen-
tral nervous tissue after long-term implantation
(Kim et al. 2013a). As part of recent translation
efforts, Mushahwar and colleagues have also
begun
development
of
ISMS
implants
for
humans, performing initial mechanical testing of
the implants in porcine models (Toossi et al.
2017). Wireless stimulators may also offer free-
dom from the cable tethering problem and poten-
tially dura resealing. Miniaturized stimulators can
be implanted close to the neural target and con-
trolled via wireless approaches (radio frequency,
optical, or acoustic waves, etc.). The approach can
have the beneﬁts of a cable-free approach like the
transcutaneous stimulators but will allow much
more precise targeting of the stimulation delivery
(see ▶“Wireless Microstimulators” entry in this
Encyclopedia).
Other stimulation techniques have been used
experimentally to activate the spinal cord, but to
date no clinical application of these techniques
has been pursued. Bizzi and colleagues have
used iontophoresis to deliver an excitatory neuro-
transmitter locally within the gray matter (Saltiel
et al. 1998), but since electrodes are pulled glass
micropipettes, applications are limited to cases
where
the
spinal
cord
can
be
securely
immobilized. More recently, cell soma-sized
microprobes have been developed to deliver elec-
trical stimulation while simultaneously recording
neurotransmitter release (Schwerdt et al. 2018).
Although their mechanical impedance is encour-
aging for spinal cord applications, they have yet to
be rigorously tested for this application.
124
Spinal Interfaces: Overview

Optogenetic methods have also been used to
activate spinal tissue in the rat animal model (see
▶“Intraspinal Stimulation” entry in this Encyclo-
pedia). Stimulation is usually achieved with deliv-
ery of light from the surface, but delivery of light
deep into tissue is possible through pulled optic
ﬁbers inserted into neural tissue (Gradinaru et al.
2009, Lu et al. 2017) and the development of new
miniaturized optoelectronics (Kim et al. 2013b,
Montgomery et al. 2015, Samineni et al. 2017).
While earlier generations of optical ﬁbers suffered
from the same mechanical impedance mismatch
issues as their metal electrode counterparts, these
newer optrodes represent a substantial improve-
ment in ﬂexibility and compliance.
Finally, infrared light from lasers has also been
used to elicit action potentials in peripheral nerve
and cochlea of small (rodents and guinea pig)
animal models (Izzo et al. 2006; Wells et al.
2005a, b; Xia et al. 2014). The methodology is
termed optical stimulation, and action potential
initiation appears to be caused by the induction
of thermal transient within the axons or neurons
(Wells et al. 2007). Depth of penetration of the
light is a limiting factor attempting to activate
deep tissue, and to date the technology has not
been used in the spinal cord. Optical stimulation
methods offer the advantage of producing no
electrical stimulation artifacts, which facilitates
neural recordings and closed-loop conﬁgurations
because the ampliﬁers and analog-to-digital con-
verters used in recordings are no longer saturated
by the large electrical signal generated by the
stimulus pulses.
References
Ahmed Z (2011) Trans-spinal direct current stimulation
modulates motor cortex-induced muscle contraction
in mice. J Appl Physiol (1985) 110(5):1414–1424.
https://doi.org/10.1152/japplphysiol.01390.2010
Alilain WJ, Li X, Horn KP, Dhingra R, Dick TE,
Herlitze S, Silver J (2008) Light-induced rescue of
breathing
after
spinal
cord
injury.
J
Neurosci
28:11862–11870
Bamford JA, Mushahwar VK (2011) Intraspinal micro-
stimulation for the recovery of function following spi-
nal cord injury. Prog Brain Res 194:227–239. https://
doi.org/10.1016/B978-0-444-53815-4.00004-2
Barra B, Roux C, Kaeser M et al (2018) Selective recruit-
ment of arm Motoneurons in nonhuman Primates using
epidural electrical stimulation of the cervical spinal cord.
Conf Proc IEEE Eng Med Biol Soc 2018:1424–1427.
https://doi.org/10.1109/EMBC.2018.8512554
Benavides
FD,
Jo
HJ,
Lundell
H,
Edgerton
VR,
Gerasimenko Y, Perez MA (2020) Cortical and
subcortical effects of transcutaneous spinal cord stim-
ulation in humans with tetraplegia. J Neurosci 40(13):
2633–2643. https://doi.org/10.1523/JNEUROSCI.
2374-19.2020
Brindley GS (1988) The Ferrier lecture, 1986. The actions
of parasympathetic and sympathetic nerves in human
micturition, erection and seminal emission, and their
restoration in paraplegic patients by implanted electri-
cal stimulators. Proc R Soc Lond B Biol Sci 235:
111–120
Chari A, Hentall ID, Papadopoulos MC, Pereira EA
(2017) Surgical Neurostimulation for spinal cord
injury. Brain Sci 7(2):18. https://doi.org/10.3390/
brainsci7020018. Published 10 Feb 2017
Cogiamanian F, Vergari M, Pulecchi F, Marceglia S, Priori
A (2008) Effect of spinal transcutaneous direct current
stimulation on somatosensory evoked potentials in
humans.
Clin
Neurophysiol
119(11):2636–2640.
https://doi.org/10.1016/j.clinph.2008.07.249
Crosby ND, Janik JJ, Grill WM (2017) Modulation of
activity and conduction in single dorsal column axons
by
kilohertz-frequency
spinal
cord
stimulation.
J Neurophysiol 117(1):136–147. https://doi.org/10.
1152/jn.00701.2016
Del Vecchio A, Holobar A, Falla D, Felici F, Enoka RM,
Farina D (2020) Tutorial: analysis of motor unit dis-
charge characteristics from high-density surface EMG
signals. J Electromyogr Kinesiol 53:102426. https://
doi.org/10.1016/j.jelekin.2020.102426
Edgerton VR, Harkema S (2011) Epidural stimulation of
the spinal cord in spinal cord injury: current status and
future challenges. Expert Rev Neurother 11:1351–1353
Egon G, Barat M, Colombel P, Visentin C, Isambert JL,
Guerin J (1998) Implantation of anterior sacral root
stimulators combined with posterior sacral rhizotomy
in spinal injury patients. World J Urol 16:342–349
Fuentes R, Petersson P, Nicolelis MA (2010) Restoration
of locomotive function in Parkinson’s disease by spinal
cord stimulation: mechanistic approach. Eur J Neurosci
32:1100–1108
Gad P, Choe J, Nandra MS, Zhong H, Roy RR, Tai YC,
Edgerton VR (2013) Development of a multi-electrode
array for spinal cord epidural stimulation to facilitate
stepping and standing after a complete spinal cord
injury in adult rats. J Neuroeng Rehabil 10:2
Gradinaru V, Mogri M, Thompson KR, Henderson JM,
Deisseroth K (2009) Optical deconstruction of parkin-
sonian neural circuitry. Science 324:354–359
Harkema S, Gerasimenko Y, Hodes J, Burdick J, Angeli C,
Chen Y, Ferreira C, Willhite A, Rejc E, Grossman RG,
Edgerton VR (2011) Effect of epidural stimulation of
the lumbosacral spinal cord on voluntary movement,
Spinal Interfaces: Overview
125

standing, and assisted stepping after motor complete
paraplegia: a case study. Lancet 377:1938–1947
Inanici F, Samejima S, Gad P, Edgerton VR, Hofstetter CP,
Moritz CT (2018) Transcutaneous electrical spinal
stimulation promotes long-term recovery of upper
extremity function in chronic tetraplegia. IEEE Trans
Neural Syst Rehabil Eng 26(6):1272–1278. https://doi.
org/10.1109/TNSRE.2018.2834339
Izzo AD, Richter CP, Jansen ED, Walsh JT Jr (2006) Laser
stimulation of the auditory nerve. Lasers Surg Med
38:745–753
Jankowska E (2017) Spinal control of motor outputs by
intrinsic and externally induced electric ﬁeld potentials.
J Neurophysiol 118(2):1221–1234. https://doi.org/10.
1152/jn.00169.2017
Kapelner T, Vujaklija I, Jiang N et al (2019) Predicting
wrist kinematics from motor unit discharge timings for
the control of active prostheses. J Neuroeng Rehabil
16(1):47. https://doi.org/10.1186/s12984-019-0516-x.
Published 5 Apr 2019
Kim T, Branner A, Gulati T, Giszter SF (2013a) Braided
multi-electrode probes: mechanical compliance charac-
teristics and recordings from spinal cords. J Neural Eng
10:045001
Kim TI, McCall JG, Jung YH, Huang X, Siuda ER, Li Y,
Song J, Song YM, Pao HA, Kim RH, Lu C, Lee SD,
Song IS, Shin G, Al-Hasani R, Kim S, Tan MP,
Huang Y, Omenetto FG, Rogers JA, Bruchas MR
(2013b) Injectable, cellular-scale optoelectronics with
applications
for
wireless
optogenetics.
Science
340:211–216
Kumar K, Toth C, Nath RK, Laing P (1998) Epidural spinal
cord stimulation for treatment of chronic pain-some
predictors of success. A 15-year experience. Surg
Neurol 50:110–120, discussion 120–111
Linderoth B, Foreman RD (2017) Conventional and novel
spinal stimulation algorithms: hypothetical mecha-
nisms
of
action
and
comments
on
outcomes.
Neuromodulation 20(6):525–533. https://doi.org/10.
1111/ner.12624
Lu C, Park S, Richner TJ et al (2017) Flexible and stretch-
able nanowire-coated ﬁbers for optoelectronic probing
of spinal cord circuits. Sci Adv 3(3):e1600955. https://
doi.org/10.1126/sciadv.1600955. Published 29 Mar
2017
Miller LC, Thompson CK, Negro F, Heckman CJ,
Farina D, Dewald JP (2014) High-density surface
EMG decomposition allows for recording of motor
unit discharge from proximal and distal ﬂexion synergy
muscles simultaneously in individuals with stroke.
Conf Proc IEEE Eng Med Biol Soc 2014:5340–5344.
https://doi.org/10.1109/EMBC.2014.6944832
Montgomery KL, Yeh AJ, Ho JS et al (2015) Wirelessly
powered, fully internal optogenetics for brain, spinal
and
peripheral
circuits
in
mice.
Nat
Methods
12(10):969–974. https://doi.org/10.1038/nmeth.3536
Musienko P, van den Brand R, Märzendorfer O et al
(2011)
Controlling
speciﬁc
locomotor
behaviors
through multidimensional monoaminergic modulation
of spinal circuitries. J Neurosci 31(25):9264–9278.
https://doi.org/10.1523/JNEUROSCI.5796-10.2011
Pikov V, Bullara L, McCreery DB (2007) Intraspinal stim-
ulation for bladder voiding in cats before and after
chronic spinal cord injury. J Neural Eng 4(4):
356–368. https://doi.org/10.1088/1741-2560/4/4/002
Ramasubbu C, Flagg A 2nd, Williams K (2013) Principles
of electrical stimulation and dorsal column mapping as
it relates to spinal cord stimulation: an overview. Curr
Pain Headache Rep 17:315
Ranger MR, Irwin GJ, Bunbury KM, Peutrell JM
(2008) Changing body position alters the location of
the spinal cord within the vertebral canal: a magnetic
resonance imaging study. Br J Anaesth 101:804–809
Saltiel P, Tresch MC, Bizzi E (1998) Spinal cord modular
organization and rhythm generation: an NMDA ionto-
phoretic study in the frog. J Neurophysiol 80:2323–2339
Samineni VK, Yoon J, Crawford KE et al (2017) Fully
implantable,
battery-free
wireless
optoelectronic
devices
for
spinal
optogenetics.
Pain
158(11):2108–2116. https://doi.org/10.1097/j.pain.
0000000000000968
Schwerdt HN, Zhang E, Kim MJ et al (2018) Cellular-scale
probes enable stable chronic subsecond monitoring of
dopamine neurochemicals in a rodent model. Commun
Biol 1:144. https://doi.org/10.1038/s42003-018-0147-
y. Published 12 Sep 2018
Snow S, Horch KW, Mushahwar VK (2006) Intraspinal
microstimulation using cylindrical multi-electrodes.
IEEE Trans Biomed Eng 53:311–319
Song W, Martin JH (2017) Spinal cord direct current stim-
ulation differentially modulates neuronal activity in the
dorsal
and
ventral
spinal
cord.
J
Neurophysiol
117(3):1143–1155. https://doi.org/10.1152/jn.00584.
2016
Sunshine MD, Ganji CN, Reier PJ, Fuller DD, Moritz CT
(2018) Intraspinal microstimulation for respiratory
muscle activation. Exp Neurol 302:93–103. https://
doi.org/10.1016/j.expneurol.2017.12.014
Thompson CK, Negro F, Johnson MD et al (2018) Robust
and accurate decoding of motoneuron behaviour and
prediction of the resulting force output. J Physiol
596(14):2643–2659. https://doi.org/10.1113/JP276153
Toossi A, Everaert DG, Azar A, Dennison CR, Mushahwar
VK (2017) Mechanically stable Intraspinal micro-
stimulation implants for human translation. Ann
Biomed Eng 45(3):681–694. https://doi.org/10.1007/
s10439-016-1709-0
Wagner FB, Mignardot JB, Le Goff-Mignardot CG et al
(2018) Targeted neurotechnology restores walking in
humans
with
spinal
cord
injury.
Nature
563(7729):65–71. https://doi.org/10.1038/s41586-
018-0649-2
Wells J, Kao C, Jansen ED, Konrad P, Mahadevan-Jansen
A (2005a) Application of infrared light for in vivo
neural stimulation. J Biomed Opt 10:064003
Wells J, Kao C, Mariappan K, Albea J, Jansen ED,
Konrad P, Mahadevan-Jansen A (2005b) Optical stim-
ulation of neural tissue in vivo. Opt Lett 30:504–506
126
Spinal Interfaces: Overview

Wells J, Kao C, Konrad P, Milner T, Kim J, Mahadevan-
Jansen A, Jansen ED (2007) Biophysical mechanisms
of transient optical stimulation of peripheral nerve.
Biophys J 93:2567–2580
Xia N, Wu XY, Wang X, Mou ZX, Wang MQ, Gu X,
Zheng XL, Hou WS (2014) Pulsed 808-nm infrared
laser stimulation of the auditory nerve in Guinea pig
cochlea. Lasers Med Sci 29:343–349
Xiao L, Demosthenous A, Vanhoestenberghe A, Dai J,
Donaldson N (2012) Active books: the design of an
implantable stimulator that minimizes cable count
using integrated circuits very close to electrodes.
IEEE Trans Biomed Circuits Syst 6:216–227
Synaptic Dynamics: Overview
Patrick D. Roberts
Department of Biomedical Engineering,
Oregon Health and Science University, Portland,
OR, USA
Definition
Synaptic dynamics describes the time-dependent
changes in synaptic currents that alter the strength
of coupling between neurons. Various mecha-
nisms, both pre- and postsynaptic, contribute to
ongoing changes of synaptic currents and modu-
late the overall network activity. These mecha-
nisms
operate
on
various
time
scales
(milliseconds to years) and can lead to immediate
changes in neuronal activity, ongoing adaptation
of neuronal responses to changing inputs, and
long-term learning and memory.
Detailed Description
Connections between neurons form network cir-
cuitry, and these connections are not static, but
change in amplitude and timing. When a spike
reaches a presynaptic terminal, processes are ini-
tiated that result in release of neurotransmitters
that diffuse across the synaptic cleft to reach the
postsynaptic receptors. Changes in these release
dynamics can lead to short-term or lasting
changes in the amount and timing of transmitter
released. In addition, surrounding cells such as
glia can inﬂuence the local synaptic dynamics in
tripartite synapse. Neurotransmitters bind to
postsynaptic receptors that may induce current
to ﬂow that changes the postsynaptic neurons
membrane potential or initiates signaling path-
ways that affect various properties of the post-
synaptic neuron.
Fast Dynamics
Changes of synaptic currents following a single
synaptic transmitter release event are usually on
the time scale of milliseconds to second and
controlled by the dynamics of postsynaptic
receptors. These fast dynamics follow a time
course determined by transmitter diffusion, con-
formational changes in postsynaptic receptors
that allows current to ﬂow though the receptor
pore, or changes that result in triggering second
messengers that modify membrane currents. The
resulting current ﬂow is characterized by a sharp
rise followed by a slower decay as the receptors
inactivate and neurotransmitters unbind. The
resulting currents can either excite or inhibit the
activity of the postsynaptic neuron, depending
on the selectivity of membrane channels for
speciﬁc ions.
Ionotropic receptors. Receptors that allow
direct current ﬂow though a pore are called
ionotropic receptors (Eccles and McGeer 1979)
and provide the fastest synaptic dynamics. Math-
ematical representations of a synaptic current,
Is(t), can be represented by a conductance gs(t)
that regulates current ﬂow across the membrane
(Gerstner and Kistler 2002; Koch 2004),
Is tð Þ ¼ gs tð Þ  V tð Þ  Es
ð
Þ,
ð1Þ
where V(t) is the membrane potential and Es is the
reversal potential of the current. If Es is greater
than the membrane potential, then the synaptic
excites the neuronal activity, and if Es is less
than the membrane potential, then the synapse
inhibits the neuronal activity.
Synaptic Dynamics: Overview
127

The time-dependent synaptic conductance may
be
based
on
double
exponential
functions
(or similarly shaped functions for t  0).
gs tð Þ ¼ gs et=t1 þ et=t2


ð2Þ
where t1 is the onset time constant and t2 is the
decay time constant. More detailed representa-
tions of changes in synaptic conductances are
developed in kinetic models of channels.
Two important types of excitatory currents are
mediated by AMPA glutamate receptor and
N-methyl-d-aspartate (NMDA) receptors that are
excitatory in most neurons and have time con-
stants for a risetime of a few milliseconds. They
differ in two respects, time constants and depen-
dence on postsynaptic voltage, but are often
colocalized at the same synaptic zones. AMPA
receptors usually have a larger peak conductance
and decay with a few milliseconds (Trussell et al.
1993). NMDA receptors have a longer decay
(Jahr and Stevens 1990), and the peak current is
dependent on the postsynaptic membrane poten-
tial. NMDA receptors pass calcium ions that can
initiate signaling cascades that may have a sec-
ondary effect on synaptic dynamics on a longer
time
scale
such
as
long-term
plasticity
(Collingridge and Bliss 1987; Lisman 1989).
Another
important
current
is
mediated
by
gamma-aminobutyric acid (GABA) receptors
and is usually inhibitory in adult neurons.
Metabotropic receptors. Receptors that trigger
second messengers to modify membrane currents
are called metabotropic receptors (Eccles and
McGeer 1979). These receptors are typically
slower in their effect on the membrane potential
than ionotropic receptors and can result in
secondary long-lasting effects on the excitability
of neurons. They can also help to initiate signaling
cascades that lead to long-term synaptic plasticity.
Their fast dynamics are mediated though changes
in membrane currents, such as potassium currents,
and the effects on membrane excitability can be
either excitatory or inhibitory depending on the
type of metabotropic receptor. In the simplest
mathematical representations of these receptors,
the conductance can be represented by a double
exponential function (Eq. 2) with a long onset
time constant (t1) of several milliseconds.
Short-Term Dynamics
The synaptic dynamics caused by a spike may be
affected by the history of previous spikes due to
short-term plasticity (Zucker and Regehr 2002;
Blitz et al. 2004). If the second of two spikes has
a larger postsynaptic current than the ﬁrst spikes,
then the plasticity is called facilitation, and if the
second spike has a smaller postsynaptic current,
then it is called short-term depression. The
amount of facilitation and depression on further
spikes in a series changes over time and may
saturate and last up to several seconds. The dura-
tion of facilitation and depression may occur on
different
time
scales
leading
to
complex
sequences of postsynaptic currents that could be
unique to the pattern of spikes.
Both pre- and postsynaptic mechanisms can be
responsible for short-term plasticity. An important
presynaptic mechanism is based on the presynap-
tic probability of neurotransmitter release for each
spikes combined with the rate of replenishment of
presynaptic vesicles containing neurotransmitters.
If the probability of release is high, then synaptic
zones may not yet be prepared for the next spike
leading to a reduced peak synaptic current
observed in short-term depression. In contrast, if
the release probability is low, then the ﬁrst spike
may have helped to prime the release so that the
release probability is higher for the second spike.
This increased release probability at each synaptic
zone leads to an increase in peak synaptic current
observed in facilitation.
Long-Term Dynamics
Changes in synaptic currents that last longer than
several minutes are called long-term plasticity.
Several mechanisms, both pre- and postsynaptic,
may be responsible for these lasting changes that
may increase or decrease the strength of synaptic
currents. Increases in synaptic currents are called
long-term potentiation (LTP) (Bliss and Lomo
128
Synaptic Dynamics: Overview

1973), and decreases are called long-term depres-
sion (LTD) (Dudek and Bear 1993). Long-term
plasticity may be dependent on the activity of pre-
and postsynaptic neurons, and when the relative
differences of timing of spikes between the neu-
rons determine whether the change is potentiation
or depression, then it is referred to as spike-
timing-dependent plasticity (STDP) (Abbott and
Nelson 2000). The processes of long-term depres-
sion and potentiation are balanced by synaptic
scaling that adjusts the global input to neurons to
maintain stable activity. Long-term plasticity is
believed to form the basis of learning and memory
as learned sensory cues, and behaviors are
encoded in the strengths of synapse in neural
circuitry (Hebb 1949).
Cross-References
▶AMPA Glutamate Receptor (AMPA Receptor),
Conductance Models
▶Anti-Hebbian Learning
▶Biochemical Signaling Pathways and Diffu-
sion: Overview
▶Bimolecular Reactions, Modeling of
▶Enzyme Kinetics, Modeling of
▶Facilitation, Biophysical Models
▶Gamma-Aminobutyric
Acid
Type-A
(GABA-A) Receptors, Kinetic Models
▶Gap Junctions in Small Networks
▶Gap Junctions, Neural Population Models and
▶Hebbian Learning
▶Ionotropic Receptors Dynamics, Conductance
Models
▶Kinetic Models of Postsynaptic Currents
▶Learning Rules: Overview
▶Long Term Depression in the Granule Cell-
Purkinje Cell Synapse
▶Long-Term Plasticity, Biophysical Models
▶Metabotropic Receptors (G Protein Coupled
Receptors)
▶Metabotropic Receptors Dynamics, Conduc-
tance Models
▶N-Methyl-D-Aspartate
(NMDA)
Receptors,
Conductance Models
▶Olfactory
Computation
in
Glomerular
Microcircuits
▶SenseLab:
Integration
of
Multidisciplinary
Neuroscience Data
▶Short-Term Plasticity, Biophysical Models
▶Short-Term Synaptic Plasticity in Central Pat-
tern Generators
▶Signaling Pathways, Modeling of
▶Spike-Timing Dependent Plasticity (STDP),
Biophysical Models
▶Spike-Timing Dependent Plasticity, Learning
Rules
▶Stability and Homeostasis in Small Network
Central Pattern Generators
▶Tripartite Synapse (Neuron–Astrocyte Interac-
tions), Conductance Models
▶Working Memory, Models of
References
Abbott LF, Nelson SB (2000) Synaptic plasticity: taming
the beast. Nat Neurosci 3(suppl):1178–1183
Bliss TV, Lomo T (1973) Long-lasting potentiation of
synaptic transmission in the dentate area of the
anaesthetized rabbit following stimulation of the
perforant path. J Physiol 232(2):331–356
Blitz DM, Foster KA, Regehr WG (2004) Short-term syn-
aptic plasticity: a comparison of two synapses. Nat Rev
Neurosci 5(8):630–640
Collingridge GL, Bliss TVP (1987) NMDA receptors-their
role in long-term potentiation. Trends Neurosci 10(7):
288–293
Dudek SM, Bear MF (1993) Bidirectional long-term mod-
iﬁcation of synaptic effectiveness in the adult and
immature hippocampus. J Neurosci 13:2910–2918
Eccles JC, McGeer PL (1979) Ionotropic and metabotropic
neurotransmission. Trends Neurosci 2:39–40
Gerstner W, Kistler WM (2002) Spiking neuron models:
single neurons, populations, plasticity. Cambridge Uni-
versity Press, Cambridge
Hebb DO (1949) The organization of behavior. Wiley,
New York
Jahr CE, Stevens CF (1990) A quantitative description of
NMDA receptor-channel kinetic behavior. J Neurosci
10(6):1830–1837
Koch C (2004) Biophysics of computation: information
processing in single neurons. Oxford University
Press, New York
Lisman J (1989) A mechanism for the Hebb and the anti-
Hebb processes underlying learning and memory. Proc
Natl Acad Sci U S A 86(23):9574–9578
Trussell LO, Zhang S, Ramant IM (1993) Desensitization
of AMPA receptors upon multiquantal neurotransmitter
release. Neuron 10(6):1185–1196
Zucker RS, Regehr WG (2002) Short-term synaptic plas-
ticity. Annu Rev Physiol 64(1):355–405
Synaptic Dynamics: Overview
129

Further Reading
Dayan P, Abbott LF, Abbott L (2001) Theoretical neuro-
science: computational and mathematical modeling of
neural systems. Taylor & Francis, Cambridge, MA
Vertebrate Pattern
Generation: Overview
Ilya A. Rybak and Jessica Ausborn
Department of Neurobiology and Anatomy,
Drexel University College of Medicine,
Philadelphia, PA, USA
Synonyms
Central pattern generator; CPG; Neural oscillator;
Rhythm generators
Definition
Central pattern generators (CPGs) are neural net-
works that can produce organized rhythmic pat-
terns in the absence of rhythmic sensory and
descending inputs from other parts of the nervous
system (Marder and Calabrese 1996).
Detailed Description
Vertebrate Central Pattern Generators
The central nervous systems of vertebrates con-
tain many types of central pattern generators
(CPGs) that generate and control various rhyth-
mic movements. These CPGs control important
functions, including different forms of locomo-
tion, such as swimming, walking, running, and
ﬂying, and non-locomotor processes and behav-
iors, such as breathing, swallowing, chewing,
mastication, scratching, whisking (in rodents),
singing (in birds), etc. This overview focuses on
computational models of locomotion and breath-
ing in mammals, which are brieﬂy discussed
below. Other known models of vertebrate CPGs
include CPGs controlling swimming in Xenopus
tadpoles (Wolf et al. 2009; ▶“Rhythm Genera-
tion in Young Xenopus Tadpoles”), zebra ﬁsh
(Hill et al. 2004; Knudsen et al. 2006), and lam-
prey (Grillner et al. 2007); CPGs controlling
swimming and walking in salamander (Ijspeert
2001; ▶“Control of Aquatic and Terrestrial
Gaits in Salamander”); CPGs controlling locomo-
tion and scratching in turtles (Hao et al. 2011;
▶“Control of Locomotion and Scratching in Tur-
tles”); CPGs responsible for rhythm generation in
embryonic chick spinal cord (Tabak et al. 2001;
▶“Rhythm Generation in Embryonic Chick Spi-
nal Cord”); and others.
Locomotor CPG in Mammals
The Half-Center Concept of the Locomotor CPG
The ability of the spinal cord to generate coordi-
nated locomotor-like rhythmic activity in the
absence of rhythmic supraspinal and afferent
inputs provides strong evidence for both the exis-
tence of locomotor CPGs in vertebrates including
mammals and their location in the spinal cord
(Grillner 1981; Rossignol 1986; Orlovsky et al.
1999; Stuart and Hultborn 2008; Guertin 2009).
The ﬁrst schematic of a locomotor CPG, called the
“half-center” model, was proposed by Graham
Brown (1914, see also Stuart and Hultborn
2008; Guertin 2009). This model consisted of
two (ﬂexor and extensor) half-centers that recip-
rocally inhibited each other. The mutual inhibitory
interactions between the half-centers were medi-
ated by corresponding inhibitory interneurons,
which ensured that only one half-center could be
active at a time. The activity of the currently active
half-center reduced gradually because of some
fatigue or adaptation mechanisms leading to
the activation of the antagonistic half-center,
inhibiting the currently active half-center, and
switching the locomotor phase. It was suggested
that the ﬂexor and extensor half-centers project to
and activate the ﬂexor and extensor motoneurons,
respectively. Studies on immobilized decerebrate
cats demonstrated that continuous electrical stim-
ulation of the midbrain locomotor region (MLR)
can produce “ﬁctive locomotion” – the rhythmic
pattern of motoneuron activity characterized by
alternating activation of ﬂexor and extensor
130
Vertebrate Pattern Generation: Overview

motoneurons similar to that observed during nor-
mal locomotion in the intact animal (Rossignol
1986). Similar patterns of locomotor activity
could be also produced by systemic administra-
tion of the noradrenergic precursor, L-DOPA
(e.g., Jankowska et al. 1967a, b). The demon-
strated possibility to produce either MLR-evoked
or drug-evoked ﬁctive locomotion in the cat spinal
cord has provided strong evidence for the exis-
tence of locomotor CPGs in the mammalian
spinal cord.
The Unit Burst Generators Concept
CPG half-center models can be theoretically ana-
lyzed and classiﬁed into categories such as intrin-
sic escape, intrinsic release, synaptic escape, and
synaptic release depending on which half-center
and which process associated with this half-center
control the transitions between activity states
(Wang and Rinzel 1992; Skinner et al. 1994;
Ausborn et al. 2018b; ▶“Comparative Analysis
of
Half-Center
Central
Pattern
Generators
(CPGs)”). At the same time, the classical half-
center concept can only represent a simpliﬁed
CPG organization and cannot reproduce and
explain many features of the locomotor pattern
observed in the mammalian spinal cord. Speciﬁ-
cally, biological locomotor activity does not
exhibit strictly alternating ﬂexor and extensor
activities (with all motoneurons clearly belonging
to one of these two groups) as suggested by the
classical half-center model. The real pattern is
more complex and includes motoneuron pools
(such as those controlling biarticular muscles)
generating bursts during both the ﬂexion and
extension phases of the step cycle or during only
a part of one phase. There are also noticeable
differences in the timing of burst onset and/or
offset between different ﬂexor and/or between
different extensor pools. To overcome these and
other limitations of the classical half-center
model, Grillner (1981) proposed a unit burst gen-
erator (UBG) concept suggesting the existence of
several separate rhythmogenic modules (or unit
burst generators) controlling each joint of the limb
and interacting with each other as coupled neural
oscillators. A simpliﬁed mathematical model of
the mammalian locomotor CPG based on the
UBG concept was proposed and analyzed by
Sherwood et al. (2011; ▶“Computational Analy-
sis of Rodent Spinal CPG”). Despite many obvi-
ous advantages, this model showed a very slow
recovery after applied perturbations. The analysis
of sub-networks of this CPG revealed that the
endogenous bursting properties of coupled neu-
rons dominated over the phasing and synchroni-
zation properties of the CPG network, leading to
the slow phase resetting and rhythm stabilization
in response to applied perturbations.
The Two-Level Model of the Locomotor CPG
An important extension of the classical half-center
model was suggested based on the analysis of
(a) spontaneous deletions (missing bursts) in the
rhythmic motoneuron activities and (b) effects of
afferent stimulations on the locomotor pattern
during ﬁctive locomotion in the cat (Rybak et al.
2006a, b; McCrea and Rybak 2007, 2008;
▶“Two-Level Model of Mammalian Locomotor
CPG”). This analysis led to the suggestion that the
locomotor CPG has a two-level architecture with
a separate top-level, half-center rhythm generator
(RG) and a pattern formation network (PF). While
the RG deﬁnes the locomotor frequency, the PF
network is controlled by the RG and deﬁnes and
coordinates the more complicated ﬁring patterns
of different motoneuron pools. The two-level
model was able to reproduce a number of features
of the locomotor pattern observed during cat
locomotion, including phase maintenance of
motoneuron
activities
following
spontaneous
deletions or brief stimulation of some afferents,
which would be difﬁcult or even impossible to
explain in the framework of the classical half-
center architecture (see ▶“Two-Level Model of
Mammalian Locomotor CPG”). Some aspects of
supraspinal and afferent control of the two-level
locomotor CPG were analyzed using computa-
tional modeling approaches (Rybak et al. 2006a;
Markin et al. 2010; Spardy et al. 2011a, b).
One advantage of the two-level model is a
possibility
to
perform
independent
control
(by supraspinal and afferent inputs) of the step
cycle duration (locomotor speed) at the RG level
and the activity of motoneurons and motor syner-
gies at the PF level. This CPG model was used in
Vertebrate Pattern Generation: Overview
131

several studies focused on the development of
CPG-controlled legged robots (Chen et al. 2007;
Maeda 2008; Amrollah and Henaff 2010).
Rhythmic Activity in Isolated Spinal Cord
Preparations of Rodents
The mammalian locomotor CPG was also studied
in vitro using isolated spinal cord preparations of
rodents (Smith and Feldman 1987; ▶“Locomotor
Pattern Generation in the Rodent Spinal Cord”).
The locomotor CPG in these preparations can be
activated in several ways, including pharmacolog-
ical manipulations [by using solutions with
speciﬁc combinations of N-methyl-D-aspartate
(NMDA) and serotonin], stimulations of sensory
afferents or dorsal roots, and brainstem stimula-
tion. The elicited motor output shows alternation
of activity in the left and right ventral roots (e.g.,
left L2 and right L2) combined with alternation of
ipsilateral ﬂexor (L2) and extensor (L5) activities
on each side of the cord. Signiﬁcant progress in
the understanding of functional and structural
organization of spinal circuits has been achieved
using special combinations of genetic, molecular,
and developmental approaches. Several classes of
spinal interneurons were deﬁned in the embryonic
and early postnatal spinal cord based on the
dynamic expression pattern of transcription fac-
tors (Goulding 2009; Whelan 2010; Gosgnach
2011; Kiehn 2011; Dougherty and Ha 2019;
▶“Locomotor Pattern Generation in the Rodent
Spinal Cord”). The new genetic techniques allow
visualization of identiﬁed neurons as well as
manipulation of their activity and their selective
elimination. Several classes of inhibitory (CINi)
and excitatory (CINe) commissural interneurons
(CINs) have been identiﬁed. The axons of these
cells cross the midline, project to the opposite side
of the cord, and mediate interactions between left
and right circuits. The role of different CINs in
locomotion has been identiﬁed by using trans-
genic mice with mutations in, or knockout
(KO) of, speciﬁc genes resulting in various abnor-
mal locomotor phenotypes (Kullander et al. 2003;
Lanuza et al. 2004; Akay et al. 2006; Lundfald
et al. 2007; Crone et al. 2008, 2009; Zhang et al.
2008; Zagoraiou et al. 2009; Rabe et al. 2009;
Restrepo et al. 2011; Talpalar et al. 2013).
A two-level computational model of neural cir-
cuits in the spinal cord with left and right rhythm-
generating populations interacting via inhibitory
and excitatory CINs has been developed based on
the analysis of spontaneous deletions in the iso-
lated spinal cord preparation with rhythmic activ-
ity evoked by administration of NMDA and
serotonin (Zhong et al. 2012; ▶“Locomotor Pat-
tern Generation in the Rodent Spinal Cord”) and
further reﬁned in subsequent studies (Rybak et al.
2013, 2015; Shevtsova et al. 2015; Shevtsova and
Rybak 2016; Danner et al. 2016, 2017).
Modeling the Effects of Genetic Transformations
on the Locomotor Pattern
A representative example of how genetic manip-
ulations can change the phase relationships
between ipsi- and contralateral neurons involved
in control of locomotion in mice is based on the
genetic ablation of special molecules involved in
guidance of CIN axons, such as Netrin-1 and
DCC. Also, the spinal cord contains glutamatergic
interneurons with ipsilateral projections, whose
axon guidance involves the EphA4 receptor. In
EphA4 knockout (KO) and Netrin-1 KO mice, the
normal left–right alternating pattern is replaced
with a synchronized hopping gait, and the spinal
cord of DCC KO mice exhibits uncoordinated left
and right oscillations (Rabe et al. 2009; Restrepo
et al. 2011; Rabe Bernhardt et al. 2012; Vallstedt
and Kullander 2013). To investigate the effects of
these genetic transformations, Rybak et al. (2013)
developed a computational model of the spinal
circuits containing the left and right rhythm-
generating neuron populations (RGs), each with
a subpopulation of EphA4-positive neurons, and
the CINi and CINe populations mediating, respec-
tively, mutual inhibition and mutual excitation
between the left and right RGs. In the EphA4
KO circuits, half of the axons of EphA4-positive
cells crossed the midline and excited the contra-
lateral RG neurons. In the Netrin-1 KO model, the
number of contralateral CINi projections was sig-
niﬁcantly reduced, while in the DCC KO model,
the numbers of connections from both CINi and
CINe were reduced. In the simulations performed,
the models of EphA4 and Netrin-1 KO circuits
showed switching from the left–right alternating
132
Vertebrate Pattern Generation: Overview

pattern to a synchronized hopping pattern, and the
DCC KO network exhibited uncoordinated left–
right activity. The model was able to reproduce
multiple experimental data on the effects of above
genetic transformations on the locomotor pattern,
providing important insights into the organization
of the spinal locomotor network and was later
build upon by subsequent studies (Shevtsova
et al. 2015; Danner et al. 2016, 2017; Ausborn
et al. 2019).
Respiratory CPG in Mammals
Respiratory Pattern and Spatial Organization of
the Respiratory CPG
The respiratory motor pattern observed during
quiet breathing in mammals (“eupnea”) consists
of three phases: inspiration (I), post-inspiration
(post-I or E1), and late expiration (E2), which
can be recognized in the integrated activity of
the phrenic and cranial nerves. This pattern orig-
inates within a bilateral column of neurons, called
the ventral respiratory column (VRC), located in
the ventrolateral medulla and controlled by inputs
from other medullary (retrotrapezoid nucleus,
RTN, raphé, etc.) and pontine regions. The VRC
includes several compartments arranged in the
rostro-caudal
direction:
Bötzinger
Complex
(BötC), pre-Bötzinger Complex (pre-BötC), and
rostral (rVRG) and caudal (cVRG) subregions of
the ventral respiratory group (VRG). Respiratory
neurons in these compartments are usually classi-
ﬁed
based
on
their
ﬁring
pattern
(e.g.,
decrementing, augmenting) and phase of activity
relative to the breathing cycle, such as early inspi-
ratory (early-I or I-DEC), i.e., inspiratory neurons
with a decrementing discharge pattern; ramp-
inspiratory (ramp-I or I-AUG), i.e., inspiratory
neurons with an augmenting ﬁring pattern; post-
inspiratory neurons (post-I or E-DEC), i.e.,
neurons with a decrementing activity during expi-
ration; augmenting or stage II expiratory (aug-E
or E-AUG or E2), i.e., expiratory neurons with
an
augmenting
pattern;
and
pre-inspiratory/
inspiratory neurons (pre-I/I), i.e., the neurons
whose activity starts before the onset of inspira-
tion and continues throughout inspiration. The
BötC, with predominately post-I and aug-E
neurons, is considered to be a major source of
expiratory activity. The adjacent, more caudal
pre-BötC contains neural circuits essential for
generating inspiratory activity (Cohen 1979;
Bianchi
et
al.
1995;
Richter
1996;
Smith
et al. 2013).
Computational models of the respiratory net-
work have been in development for several
decades. Early computational models of the respi-
ratory CPG focused on the network interactions
between different types of respiratory neurons
and
did
not
consider
intrinsic
biophysical
rhythmogenic properties of single respiratory
neurons
and
their
possible
contribution
to
rhythmogenesis. Generation of the respiratory
rhythm in these models was based on the half-
center concept suggesting that the respiratory
oscillations
result
from
sequential
phase
switching resulting from the reciprocal inhibitory
interactions between different types of respiratory
neurons (Botros and Brace 1990; Dufﬁn 1991;
Ogilvie et al. 1992; Balis et al. 1994; Rybak
et al. 1997; reviewed by Lindsey et al. 2012).
The Pre-Bo¨tzinger Complex and Rhythm
Generation In Vitro
A fundamentally distinct concept of respiratory
rhythm generation was derived from neonatal
in vitro studies. An important discovery has
been that a subregion of the VRC, called the
pre-Bötzinger Complex, contains a population of
excitatory interneurons that can intrinsically gen-
erate an inspiratory-like rhythm (Smith et al.
1991). This rhythm was shown to persist after
blockade of synaptic inhibition, indicating that
the pre-BötC may contain cells with intrinsic
bursting properties. Butera et al. (1999a, b) devel-
oped and analyzed a series of computational
models of bursting pacemaker neurons and
populations of these neurons with mutual excit-
atory connections. In these models, the intrinsic
bursting was based on a slowly inactivating per-
sistent sodium current (INaP) as the essential burst-
generating, inward cationic current. The rhythmic
bursting cycle in these models was controlled by
the slow kinetics of inactivation and recovery
from inactivation of INaP. Simulations performed
have
shown
that
the
excitatory
synaptic
Vertebrate Pattern Generation: Overview
133

interactions coupled with INaP activation can read-
ily synchronize cellular bursts and produce popu-
lation bursting. Importantly, generation of this
rhythm does not require inhibitory interactions,
which can explain the persistence of the in vitro
oscillations after inhibitory synaptic transmission
was blocked. It was also shown that even a small
fraction of intrinsically bursting cells (5–10%) can
produce a synchronized bursting activity of the
entire population. Moreover, synchronized popu-
lation activity may occur even if none of the cells
operate in the intrinsic bursting state. Elevation of
tonic drive to the population reduces burst dura-
tion, increases burst frequency, and, ﬁnally,
switches population activity from population
bursting to a regime of sustained asynchronous
activity (Butera et al. 1999a).
Network-Based Versus Pacemaker-Driven
Mechanisms for Respiratory Rhythmogenesis and
Hybrid Pacemaker–Network Models
The early network models were able to generate a
realistic respiratory motor pattern and simulate
various alterations of this pattern under different
conditions. However, these models failed to
reproduce some characteristic behaviors observed
in the reduced in vitro preparations and, speciﬁ-
cally, the maintenance of the respiratory rhythm
following inhibition blockade. Alternatively, the
pacemaker-based models, developed to ﬁt to
in vitro data, could not explain many respiratory
behaviors observed in vivo, such as the Hering–
Breuer and other respiratory reﬂexes as well as the
independent control of the duration of each respi-
ratory phase. Neither could these models repro-
duce apneusis, a well-known abnormal breathing
pattern characterized by a signiﬁcantly prolonged
inspiration (up to several seconds) alternating
with short expiratory intervals. Moreover, the
rhythmic
activity
generated
in
the
reduced
in vitro preparations and reproduced by the
pacemaker-driven models is characterized by a
decrementing shape of inspiratory discharges
that clearly differs from the augmenting shape of
phrenic discharges generated during eupneic
breathing in vivo; these discharges rather resem-
ble the decrementing bursts observed during
gasping.
The
above
contradictions
between
the
network-based and the pacemaker-based con-
cepts and models can be resolved by postulating
that (i) the pre-BötC, while capable of intrinsic
bursting when isolated and under some special
conditions, is normally embedded in the larger
brainstem respiratory network which makes its
behavior dependent on its interactions with
other brainstem structures and (ii) the respira-
tory rhythmogenesis is state dependent, and
therefore the respiratory oscillations may be
generated
by
either
a
network-based
or
pacemaker-driven mechanisms or their speciﬁc
combinations
depending
on
the
conditions
(Smith et al. 2000, 2007; Rybak et al. 2002,
2004, 2007; Lindsey et al. 2012; Smith et al.
2013; Koizumi et al. 2016; Ausborn et al.
2018a; ▶“Computational Models of Mamma-
lian Respiratory CPG”).
The Respiratory CPG and Spatial and Functional
Hierarchy of Multiple Rhythmogenic Mechanisms
Signiﬁcant progress in understanding of spatial
and functional organization of the respiratory
CPG in the mammalian brainstem has been
achieved by using an arterially perfused in situ
rat brainstem–spinal cord preparation (Paton
1996) and applying sequential rostral-to-caudal
microtransections through the brainstem while
recording cranial and spinal motor outﬂow to
observe transformations of network behavior
(Rybak et al. 2007; Smith et al. 2007). This
approach revealed the existence of a rostral-to-
caudal stacking of network building blocks serv-
ing distinct circuit functions, which are fully
integrated in the intact system, but can be
revealed
when
particular
compartments
are
removed or under special metabolic conditions
(hypoxia, hypercapnia). These studies revealed
that sequential reduction of the network progres-
sively reorganizes network dynamics, such that
new rhythmogenic mechanisms emerge. Specif-
ically, starting from a transection at the pontine–
medullary
junction, the normal three-phase
respiratory pattern is transformed to a two-
phase
rhythmic
pattern
lacking
the
post-I
phase. With more caudal transections made
close to or at the rostral boundary of the
134
Vertebrate Pattern Generation: Overview

pre-BötC, the respiratory activity transforms to
“one-phase” inspiratory oscillations originating
within the pre-BötC, without critical involve-
ment of phasic expiratory inhibition (Rybak
et al. 2007; Smith et al. 2007). These results
led to the conclusion that (i) generation of the
normal three-phase rhythmic pattern requires the
presence of the pons (i.e., excitatory drive from
pontine neurons to the VRC); (ii) generation of
the two-phase pattern is intrinsic to reciprocal
inhibitory synaptic interactions between the
BötC and the pre-BötC and may also involve
the RTN to provide excitatory drive to generate
stable behavior; and (iii) the one-phase inspira-
tory oscillations are generated within the pre--
BötC and rely on intrinsic cellular mechanisms
operating in the context of the pre-BötC excit-
atory network. These data allowed the conclu-
sion that there is a spatial and dynamical
hierarchy of interacting pontine, BötC and pre--
BötC circuits, each of which controls different
aspects of respiratory rhythm generation and
pattern formation, which can be revealed as the
network is progressively reduced. The expres-
sion of each rhythmogenic mechanism is state
dependent and produces speciﬁc motor patterns
likely to underpin distinct motor behaviors
(Smith et al. 2007, 2013; ▶“Computational
Models of Mammalian Respiratory CPG”).
A minimal network conﬁguration proposed to
represent the above multiple rhythmic states and
their transformations included (i) a mutually
inhibitory circuit with a ringlike architecture
composed of post-I and aug-E neurons of the
BötC compartment and early-I neurons within
the pre-BötC and (ii) a pre-BötC kernel of
excitatory pre-I/I neurons, with intrinsic INaP-
dependent bursting properties. The latter partici-
pate dynamically in the expiratory–inspiratory
phase transition and generation of the inspiratory
phase. The detailed description and computational
model of this core network and the circuit ele-
ments participating in each rhythmic state can be
found in a series of related publications (Rybak
et al. 2007; Smith et al. 2007; Rubin et al. 2009b;
for review see also Lindsey et al. 2012; Smith
et al. 2013; ▶“Computational Models of Mam-
malian Respiratory CPG”).
Respiratory Rhythm Generation and Coupled
Oscillators
As described above, the respiratory CPG is
considered
to
comprise
several
interacting
populations of respiratory neurons located in the
pre-BötC and BötC circuits within the VRC.
A distinct site of neural oscillations was identiﬁed
in vitro in the isolated neonatal rat brain stem–
spinal cord preparation (Onimaru and Homma
1987, 2003; Onimaru et al. 1988). This additional
oscillator, called the parafacial respiratory group
(pFRG), seems to reside within, or overlap with,
the RTN. It was also found that RTN/pFRG oscil-
lations in vivo drive abdominal motor activity,
expressing
late-expiratory
(late-E,
or
pre-
inspiratory) bursts or biphasic discharges (with
pre-I and post-I components) in the abdominal
motor output when the system operates in the
active (forced) expiration regime (Janczewski
et al. 2002; Janczewski and Feldman 2006;
Feldman and Del Negro 2006; Abdala et al.
2009). Several competing concepts concerning
the physiological role of RTN/pFRG oscillations
have been suggested, including the dual oscillator
concept that considers the RTN/pFRG as an inde-
pendent expiratory rhythm generator coupled
with a distinct inspiratory rhythm generator in
the pre-BötC (Janczewski and Feldman 2006).
However, the exact physiological role of pFRG
oscillations, the speciﬁc conditions for their emer-
gence, and the nature and mechanisms of the
interactions between the BötC/pre-BötC and
RTN/pFRG oscillators are not yet known.
Molkov et al. (2010; ▶“Coupled Oscillations
in Neural Control of Breathing”) extended the
previous respiratory CPG model (Rybak et al.
2007;
Smith
et
al.
2007)
to
include
the
RTN/pFRG oscillator and consider interactions
between the BötC/pre-BötC and RTN/pFRG
oscillators. The extended model incorporates an
additional late-E population in the RTN/pFRG
compartment, representing a source of late-E
oscillatory activity. In the proposed model, under
normal metabolic conditions, the RTN/pFRG
oscillator is inhibited by the BötC/pre-BötC cir-
cuits, and the late-E oscillations can be only
released by either hypercapnia-evoked activation
of
RTN/pFRG
or
by
hypoxia-dependent
Vertebrate Pattern Generation: Overview
135

suppression of RTN/pFRG inhibition by BötC/
pre-BötC. The proposed interactions between the
BötC/pre-BötC and RTN/pFRG oscillators allow
the model to reproduce several experimentally
observed behaviors, including quantal accelera-
tion of abdominal late-E oscillations with progres-
sive hypercapnia and quantal slowing of phrenic
activity with progressive suppression of pre-BötC
excitability, as well as to predict a release of late-E
oscillations by disinhibition of RTN/pFRG under
normal
conditions
(Molkov
et
al.
2010;
▶“Coupled Oscillations in Neural Control of
Breathing”). Rubin et al. (2011) performed thor-
ough analysis of the reduced model and explained
the regimes of quantal acceleration and quantal
slowing in terms of synchronization of the BötC/
pre-BötC and RTN/pFRG oscillators. They have
shown that the dynamics of each oscillator can be
represented by a stable limit cycle in some phase
space. The phase space of a system of two coupled
oscillators is a Cartesian product of the phase
spaces of each oscillator. The behavior of this
system can be represented by a trajectory on 2D
invariant torus. If the ratio of oscillation frequen-
cies of the two oscillators is rational (i.e., equal to
N/M, for some integers N and M), then this tra-
jectory is closed, indicating N/M synchronization
between oscillators, where N and M represent
numbers of rotations around two orthogonal cir-
cles that together span the torus (Rubin et al.
2011). These modeling studies provide mechanis-
tic explanations for the emergence of RTN/pFRG
oscillations and their interaction with the BötC/
pre-BötC circuits representing the core of the
respiratory CPG.
Intrinsic Neuronal Properties Involved in
Rhythmic Bursting in the Brainstem and
Spinal Cord
The mechanisms generating neural oscillations in
the mammalian brainstem, particularly in the pre--
Bötzinger Complex involved in control of respi-
ration, and in the spinal cord (locomotor CPG),
that persist after blockade of synaptic inhibition,
remain poorly understood. Experimental studies
in
medullary
slices
from
neonatal
rodents
containing the pre-BötC identiﬁed two mecha-
nisms
that
could
potentially
contribute
to
generation of rhythmic bursting: one based on
the persistent or slowly inactivating INaP (Butera
et al. 1999a, b; Koizumi and Smith 2008) and the
other involving the calcium-activated nonspeciﬁc
cation current (ICAN) activated by intracellular
Ca2+ accumulated from extracellular (e.g., cal-
cium currents, ICa) and/or intracellular sources
(Pace et al. 2007). The involvement and relative
roles of these mechanisms in rhythmic bursting
are still under debate. Several related models com-
bining the above two mechanism have been
developed (Rubin et al. 2009a; Toporikova and
Butera 2011; Dunmyre et al. 2011; Jasinski et al.
2013). Jasinski et al. (2013) investigated Na+-and
Ca2+-dependent bursting generated in single cells
and in a heterogeneous population of synaptically
interconnected excitatory neurons with INaP and
ICa randomly distributed within the population.
They analyzed the possible roles of network con-
nections, ionotropic and metabotropic synaptic
mechanisms, intracellular Ca2+ release, and the
Na+/K+ pump in rhythmic bursting activity gen-
erated under different conditions. They showed
that heterogeneous populations of excitatory neu-
rons with these properties can operate in different
oscillatory regimes with bursting dependent on
INaP and/or on ICAN, or independent of both. The
exact oscillatory regime and the operating burst-
ing mechanism may depend on neuronal excit-
ability,
synaptic
interactions,
and
relative
expression of particular ionic currents. The exis-
tence of multiple oscillatory regimes and their
state-dependency may provide explanations for
different rhythmic activities observed in the
brainstem and spinal cord under different experi-
mental conditions.
Interestingly, the Na+- and Ca2+-dependent
bursting
mechanisms
can
be
unexpectedly
connected. Brocard et al. (2013) have shown that
the evoked locomotor-like activity in the isolated
neonatal rodent spinal cord reduces the extracellu-
lar calcium concentration ([Ca2+]o]) to 0.9 mM and
increases the extracellular potassium concentration
([K+]o) to 6 mM. Such changes in [Ca2+]o and [K+]
o trigger a rhythmic bursting activity in spinal
interneurons that may represent elements of the
respiratory CPG. The performed experimental
studies and modeling have shown that the
136
Vertebrate Pattern Generation: Overview

emergence of this rhythmic activity critically
involves a [Ca2+]o-dependent activation of the per-
sistent sodium current (INaP). These results suggest
that the locomotor oscillations in the spinal cord
may relay on the INaP -dependent pacemaker prop-
erties in spinal interneurons, which can be turned
on and off by activity-dependent changes in [Ca2+]
o and [K+]o (Brocard et al. 2013).
References
Abdala
APL,
Rybak
IA,
Smith
JC,
Paton
JFR
(2009) Abdominal expiratory activity in the rat
brainstem–spinal cord in situ: patterns, origins and
implications
for
respiratory
rhythm
generation.
J Physiol 587:3539–3559. https://doi.org/10.1113/
jphysiol.2008.167502
Akay T, Acharya HJ, Fouad K, Pearson KG (2006) Behav-
ioral and electromyographic characterization of mice
lacking EphA4 receptors. J Neurophysiol 96:642–651.
https://doi.org/10.1152/jn.00174.2006
Amrollah E, Henaff P (2010) On the role of sensory feed-
backs in Rowat–Selverston CPG to improve robot leg-
ged locomotion. Front Neurorobot 4:1–9. https://doi.
org/10.3389/fnbot.2010.00113
Ausborn J, Koizumi H, Barnett WH, John TT, Zhang R,
Molkov YI, Smith JC, Rybak IA (2018a) Organization
of
the
core
respiratory
network:
insights
from
optogenetic and modeling studies. PLoS Comput Biol
14:e1006148. https://doi.org/10.1371/journal.pcbi.100
6148
Ausborn J, Snyder AC, Shevtsova NA, Rybak IA,
Rubin JE (2018b) State-dependent rhythmogenesis
and frequency control in a half-center locomotor
CPG. J Neurophysiol 119:96–117. https://doi.org/10.
1152/jn.00550.2017
Ausborn J, Shevtsova NA, Caggiano V, Danner SM,
Rybak
IA
(2019)
Computational
modeling
of
brainstem circuits controlling locomotor frequency
and gait. eLife 8:e43587. https://doi.org/10.7554/
eLife.43587
Balis UJ, Morris KF, Koleski J, Lindsey BG (1994) Simu-
lations of a ventrolateral medullary neural network for
respiratory rhythmogenesis inferred from spike train
cross-correlation. Biol Cybern 70:311–327. https://
doi.org/10.1007/BF00200329
Bianchi AL, Denavit-Saubie M, Champagnat J (1995)
Central control of breathing in mammals: neuronal
circuitry, membrane properties, and neurotransmitters.
Physiol Rev 75:1–45. https://doi.org/10.1152/physrev.
1995.75.1.1
Botros SM, Brace EN (1990) Neural network implemen-
tation of a three-phase model of respiratory rhythm
generation. Biol Cybern 63:143–153. https://doi.org/
10.1007/BF00203037
Brocard F, Shevtsova NA, Bouhadfane M, Tazerart S,
Heinemann U, Rybak IA, Vinay L (2013) Activity-
dependent changes in extracellular Ca2+ and K+ reveal
pacemakers in the spinal locomotor-related network.
Neuron 77:1047–1054. https://doi.org/10.1016/j.neu-
ron.2013.01.026
Butera RJ, Rinzel J, Smith JC (1999a) Models of respira-
tory rhythm generation in the pre-Bötzinger complex.
I.
Bursting
pacemaker
neurons.
J
Neurophysiol
82:382–397. https://doi.org/10.1152/jn.1999.82.1.382
Butera RJ, Rinzel J, Smith JC (1999b) Models of respira-
tory rhythm generation in the pre-Bötzinger complex.
II.
Populations
of
coupled
pacemaker
neurons.
J Neurophysiol 82:398–415. https://doi.org/10.1152/
jn.1999.82.1.398
Chen Y, Bauer C, Burmeister O, Rupp R, Mikut R (2007)
First steps to future applications of spinal neural circuit
models in neuroprostheses and humanoid robots. In:
Proceedings of 17th workshop computational intelli-
gence, Dortmund, pp 186–199
Cohen MI (1979) Neurogenesis of respiratory rhythm in
the mammal. Physiol Rev 59:1105–1173. https://doi.
org/10.1152/physrev.1979.59.4.1105
Crone
SA,
Quinlan
KA,
Zagoraiou
L,
Droho
S,
Restrepo
CE,
Lundfald
L,
Endo
T,
Setlak
J,
Jessell TM, Kiehn O, Sharma K (2008) Genetic abla-
tion of V2a ipsilateral interneurons disrupts left-right
locomotor coordination in mammalian spinal cord.
Neuron 60:70–83. https://doi.org/10.1016/j.neuron.
2008.08.009
Crone SA, Zhong G, Harris-Warrick R, Sharma K (2009)
In mice lacking V2a interneurons, gait depends on
speed of locomotion. J Neurosci 29:7098–7109.
https://doi.org/10.1523/JNEUROSCI.1206-09.2009
Danner SM, Wilshin SD, Shevtsova NA, Rybak IA
(2016) Central control of interlimb coordination and
speed-dependent
gait
expression
in
quadrupeds.
J Physiol 594:6947–6967. https://doi.org/10.1113/
JP272787
Danner SM, Shevtsova NA, Frigon A, Rybak IA
(2017) Computational modeling of spinal circuits con-
trolling limb coordination and gaits in quadrupeds.
eLife 6:e31050. https://doi.org/10.7554/eLife.31050
Dougherty KJ, Ha NT (2019) The rhythm section: an
update on spinal interneurons setting the beat for mam-
malian locomotion. Curr Opin Physiol 8:84–93. https://
doi.org/10.1016/j.cophys.2019.01.004
Dufﬁn J (1991) A model of respiratory rhythm generation.
Neuroreport 2:623–626. https://doi.org/10.1097/
00001756-199110000-00018
Dunmyre JR, Del Negro CA, Rubin JE (2011) Interactions
of persistent sodium and calcium-activated nonspeciﬁc
cationic currents yield dynamically distinct bursting
regimes in a model of respiratory neurons. J Comput
Neurosci 31:305–328. https://doi.org/10.1007/s10827-
010-0311-y
Feldman JL, Del Negro CA (2006) Looking for inspira-
tion: new perspectives on respiratory rhythm. Nat Rev
Neurosci 7:232. https://doi.org/10.1038/nrn1871
Vertebrate Pattern Generation: Overview
137

Gosgnach S (2011) The role of genetically-deﬁned inter-
neurons in generating the mammalian locomotor
rhythm. Integr Comp Biol 51:903–912. https://doi.
org/10.1093/icb/icr022
Goulding M (2009) Circuits controlling vertebrate loco-
motion: moving in a new direction. Nat Rev Neurosci
10:507–518. https://doi.org/10.1038/nrn2608
Grillner S (1981) Control of locomotion in bipeds, tetra-
pods, and ﬁsh. In: Brookhart JM, Mountcastle VB (eds)
Handbook
physiology.
The
nervous
system,
vol
II.
American
Physiology
Society,
Bethesda,
pp 1179–1236
Grillner S, Kozlov A, Dario P, Stefanini C, Menciassi A,
Lansner A, Hellgren Kotaleski J (2007) Modeling a
vertebrate motor system: pattern generation, steering
and control of body orientation. Prog Brain Res
165:221–234. https://doi.org/10.1016/S0079-6123(06)
65014-0
Guertin PA (2009) The mammalian central pattern gener-
ator for locomotion. Brain Res Rev 62:45–56. https://
doi.org/10.1016/j.brainresrev.2009.08.002
Hao
Z-Z,
Spardy
LE,
Nguyen
EBL,
Rubin
JE,
Berkowitz A (2011) Strong interactions between spinal
cord
networks
for
locomotion
and
scratching.
J Neurophysiol 106:1766–1781. https://doi.org/10.
1152/jn.00460. 2011
Hill SA, Liu X, Borla MA, José JV, O’Malley DM
(2004) Neurokinematic modeling of complex swim-
ming patterns of the larval zebraﬁsh. Neurocomputing
65–66:61–68. https://doi.org/10.1016/j.neucom.2004.
10.092
Ijspeert AJ (2001) A connectionist central pattern genera-
tor for the aquatic and terrestrial gaits of a simulated
salamander. Biol Cybern 84:331–348. https://doi.org/
10.1007/s004220000211
Janczewski WA, Feldman JL (2006) Distinct rhythm gen-
erators for inspiration and expiration in the juvenile rat.
J Physiol 570:407–420. https://doi.org/10.1113/
jphysiol.2005.098848
Janczewski WA, Onimaru H, Homma I, Feldman JL
(2002) Opioid-resistant respiratory pathway from the
preinspiratory neurones to abdominal muscles: in vivo
and in vitro study in the newborn rat. J Physiol Lond
545:1017–1026. https://doi.org/10.1113/jphysiol.
2002.023408
Jankowska E, Jukes MG, Lund S, Lundberg A (1967a) The
effect of DOPA on the spinal cord. 5. Reciprocal orga-
nization of pathways transmitting excitatory action to
alpha motoneurones of ﬂexors and extensors. Acta
Physiol Scand 70:369–388. https://doi.org/10.1111/j.
1748-1716.1967.tb03636.x
Jankowska E, Jukes MG, Lund S, Lundberg A (1967b) The
effect of DOPA on the spinal cord. 6. Half-centre orga-
nization of interneurones transmitting effects from the
ﬂexor reﬂex afferents. Acta Physiol Scand 70:389–402.
https://doi.org/10.1111/j.1748-1716.1967.tb03637.x
Jasinski PE, Molkov YI, Shevtsova NA, Smith JC,
Rybak IA (2013) Sodium and calcium mechanisms of
rhythmic bursting in excitatory neural networks of the
pre-Bötzinger complex: a computational modelling
study. Eur J Neurosci 37:212–230. https://doi.org/10.
1111/ejn.12042
Kiehn O (2011) Development and functional organization of
spinal locomotor circuits. Curr Opin Neurobiol 21:
100–109. https://doi.org/10.1016/j.conb.2010.09. 004
Knudsen DP, Arsenault JT, Hill SA, O’Malley DM,
José JV (2006) Locomotor network modeling based
on
identiﬁed
zebraﬁsh
neurons.
Neurocomputing
69:1169–1174. https://doi.org/10.1016/j.neucom.
2005. 12.068
Koizumi H, Smith JC (2008) Persistent Na+ and K+-
dominated leak currents contribute to respiratory
rhythm generation in the pre-Bötzinger complex
in vitro. J Neurosci 28:1773–1785. https://doi.org/10.
1523/JNEUROSCI.3916-07.2008
Koizumi H, Mosher B, Tariq MF, Zhang R, Koshiya N,
Smith JC (2016) Voltage-dependent rhythmogenic
property
of
respiratory
pre-Bötzinger
complex
glutamatergic,
dbx1-derived,
and
somatostatin-
expressing neuron populations revealed by graded
optogenetic
inhibition.
eNeuro
3:ENEURO.0081.
https://doi.org/10.1523/ENEURO.0081-16.2016
Kullander
K,
Butt
SJB,
Lebret
JM,
Lundfald
L,
Restrepo CE, Rydström A, Klein R, Kiehn O (2003)
Role of EphA4 and EphrinB3 in local neuronal circuits
that control walking. Science 299:1889–1892. https://
doi.org/10.1126/science.1079641
Lanuza GM, Gosgnach S, Pierani A, Jessell TM,
Goulding M (2004) Genetic identiﬁcation of spinal
interneurons
that
coordinate
left-right
locomotor
activity necessary for walking movements. Neuron
42:375–386. https://doi.org/10.1016/s0896-6273(04)
00249-1
Lindsey BG, Rybak IA, Smith JC (2012) Computational
models and emergent properties of respiratory neural
networks. Compr Physiol 2:1619–1670. https://doi.
org/10.1002/cphy.c110016
Lundfald L, Restrepo CE, Butt SJB, Peng C-Y, Droho S,
Endo T, Zeilhofer HU, Sharma K, Kiehn O (2007)
Phenotype of V2-derived interneurons and their rela-
tionship to the axon guidance molecule EphA4 in the
developing
mouse
spinal
cord.
Eur
J
Neurosci
26:2989–3002. https://doi.org/10.1111/j.1460-9568.
2007.05906.x
Maeda Y (2008) A hardware neuronal network model of
a two-level central pattern generator. Trans Jpn Soc
Med Biol Eng 46:496–504. https://doi.org/10.11239/
jsmbe.46.496
Marder E, Calabrese RL (1996) Principles of rhythmic
motor pattern generation. Physiol Rev 76:687–717.
https://doi.org/10.1152/physrev.1996.76.3.687
Markin SN, Klishko AN, Shevtsova NA, Lemay MA,
Prilutsky BI, Rybak IA (2010) Afferent control of loco-
motor CPG: insights from a simple neuromechanical
model. Ann N YAcad Sci 1198:21–34. https://doi.org/
10.1111/j.1749-6632.2010.05435.x
138
Vertebrate Pattern Generation: Overview

McCrea DA, Rybak IA (2007) Modeling the mammalian
locomotor CPG: insights from mistakes and perturba-
tions. Prog Brain Res 165:235–253. https://doi.org/10.
1016/S0079-6123(06)65015-2
McCrea DA, Rybak IA (2008) Organization of mammalian
locomotor rhythm and pattern generation. Brain Res
Rev 57:134–146. https://doi.org/10.1016/j.brainresrev.
2007.08.006
Molkov YI, Abdala APL, Bacak BJ, Smith JC, Paton JFR,
Rybak IA (2010) Late-expiratory activity: emergence
and
interactions
with
the
respiratory
cpg.
J Neurophysiol 104:2713–2729. https://doi.org/10.
1152/jn.00334.2010
Ogilvie MD, Gottschalk A, Anders K, Richter DW,
Pack AI (1992) A network model of respiratory
rhythmogenesis. Am J Phys 263:R962–R975. https://
doi.org/10.1152/ajpregu.1992.263.4.R962
Onimaru H, Homma I (1987) Respiratory rhythm genera-
tor neurons in medulla of brainstem-spinal cord prepa-
ration from newborn rat. Brain Res 403:380–384.
https://doi.org/10.1016/0006-8993(87)90080-1
Onimaru H, Homma I (2003) A novel functional neuron
group for respiratory rhythm generation in the ventral
medulla. J Neurosci 23:1478–1486
Onimaru H, Arata A, Homma I (1988) Primary respiratory
rhythm generator in the medulla of brainstem-spinal cord
preparation from newborn rat. Brain Res 445:314–324.
https://doi.org/10.1016/0006-8993(88)91194-8
Orlovsky G, Deliagina TG, Grillner S (1999) Neuronal
control of locomotion: from mollusc to man. Oxford
University Press, New York
Pace RW, Mackay DD, Feldman JL, Del Negro CA
(2007) Inspiratory bursts in the preBötzinger complex
depend on a calcium-activated non-speciﬁc cation cur-
rent linked to glutamate receptors in neonatal mice.
J Physiol Lond 582:113–125. https://doi.org/10.1113/
jphysiol.2007.133660
Paton JF (1996) A working heart-brainstem preparation of
the mouse. J Neurosci Methods 65:63–68. https://doi.
org/10.1016/0165-0270(95)00147-6
Rabe Bernhardt N, Memic F, Gezelius H, Thiebes A-L,
Vallstedt A, Kullander K (2012) DCC mediated axon
guidance of spinal interneurons is essential for normal
locomotor central pattern generator function. Dev Biol
366:279–289. https://doi.org/10.1016/j.ydbio.2012.
03.017
Rabe
N,
Gezelius
H,
Vallstedt
A,
Memic
F,
Kullander
K
(2009)
Netrin-1-dependent
spinal
interneuron subtypes are required for the formation of
left-right alternating locomotor circuitry. J Neurosci
29:15642–15649.
https://doi.org/10.1523/
JNEUROSCI.5096-09.2009
Restrepo CE, Margaryan G, Borgius L, Lundfald L,
Sargsyan D, Kiehn O (2011) Change in the balance of
excitatory and inhibitory midline ﬁber crossing as an
explanation for the hopping phenotype in EphA4
knockout mice. Eur J Neurosci 34:1102–1112. https://
doi.org/10.1111/j.1460-9568.2011.07838.x
Richter DW (1996) Neural regulation of respiration:
rhythmogenesis and afferent control. In: Comprehensive
human
physiology.
Springer,
Berlin/Heidelberg,
pp 2079–2095
Rossignol S (1986) Neural control of stereotypic limb
movements. In: Rowell LB, Sheperd J (eds) Handbook
of
physiology.
American
Physiology
Society,
Bethesda, pp 173–216
Rubin JE, Hayes JA, Mendenhall JL, Del Negro CA
(2009a) Calcium-activated nonspeciﬁc cation current
and synaptic depression promote network-dependent
burst oscillations. Proc Natl Acad Sci U S A 106:
2939–2944. https://doi.org/10.1073/pnas.080877 6106
Rubin JE, Shevtsova NA, Ermentrout GB, Smith JC,
Rybak IA (2009b) Multiple rhythmic states in a
model of the respiratory central pattern generator.
J Neurophysiol 101:2146–2165. https://doi.org/10.
1152/jn.90958.2008
Rubin JE, Bacak BJ, Molkov YI, Shevtsova NA, Smith JC,
Rybak IA (2011) Interacting oscillations in neural con-
trol of breathing: modeling and qualitative analysis.
J Comput Neurosci 30:607–632. https://doi.org/10.
1007/s10827-010-0281-0
Rybak IA, Paton JF, Schwaber JS (1997) Modeling neural
mechanisms for genesis of respiratory rhythm and pat-
tern. II. Network models of the central respiratory pat-
tern generator. J Neurophysiol 77:2007–2026. https://
doi.org/10.1152/jn.1997.77.4.2007
Rybak IA, Paton JFR, Rogers RF, St.-John WM (2002) Gen-
eration of the respiratory rhythm: state-dependency and
switching. Neurocomputing 44–46:605–614. https://doi.
org/10.1016/S0925-231 2(02)00447-2
Rybak
IA,
Shevtsova
NA,
Paton
JFR,
Dick
TE,
St-John WM, Mörschel M, Dutschmann M (2004)
Modeling the ponto-medullary respiratory network.
Respir Physiol Neurobiol 143:307–319. https://doi.
org/10.1016/j.resp.2004.03.020
Rybak
IA,
Shevtsova
NA,
Lafreniere-Roula
M,
McCrea
DA
(2006a)
Modelling
spinal
circuitry
involved in locomotor pattern generation: insights
from deletions during ﬁctive locomotion. J Physiol
Lond 577:617–639. https://doi.org/10.1113/jphysiol.
2006.118703
Rybak IA, Stecina K, Shevtsova NA, McCrea DA (2006b)
Modelling spinal circuitry involved in locomotor pat-
tern generation: insights from the effects of afferent
stimulation. J Physiol Lond 577:641–658. https://doi.
org/10.1113/jphysiol.2006.118711
Rybak IA, Abdala APL, Markin SN, Paton JFR, Smith JC
(2007) Spatial organization and state-dependent mech-
anisms for respiratory rhythm and pattern generation.
Prog Brain Res 165:201–220. https://doi.org/10.1016/
S0079-6123(06)65013-9
Rybak IA, Shevtsova NA, Kiehn O (2013) Modelling
genetic reorganization in the mouse spinal cord
affecting left-right coordination during locomotion.
J Physiol Lond 591:5491–5508. https://doi.org/10.
1113/jphysiol.2013.261115
Vertebrate Pattern Generation: Overview
139

Rybak IA, Dougherty KJ, Shevtsova NA (2015) Organiza-
tion of the mammalian locomotor CPG: review of
computational model and circuit architectures based
on genetically identiﬁed spinal interneurons. eNeuro
2:ENEURO.0069. https://doi.org/10.1523/ENEURO.
0069-15.2015
Sherwood WE, Harris-Warrick R, Guckenheimer J (2011)
Synaptic patterning of left-right alternation in a com-
putational model of the rodent hindlimb central pattern
generator. J Comput Neurosci 30:323–360. https://doi.
org/10.1007/s10827-010-0259-y
Shevtsova NA, Rybak IA (2016) Organization of ﬂexor-
extensor interactions in the mammalian spinal cord:
insights from computational modelling. J Physiol
594:6117–6131. https://doi.org/10.1113/JP272437
Shevtsova
NA,
Talpalar
AE,
Markin
SN,
Harris-
Warrick RM, Kiehn O, Rybak IA (2015) Organization
of left–right coordination of neuronal activity in the
mammalian spinal cord: insights from computational
modelling. J Physiol 593:2403–2426. https://doi.org/
10.1113/JP270121
Skinner FK, Kopell N, Marder E (1994) Mechanisms for
oscillation and frequency control in reciprocally inhib-
itory model neural networks. J Comput Neurosci
1:69–87
Smith JC, Feldman JL (1987) In vitro brainstem-spinal
cord preparations for study of motor systems for mam-
malian respiration and locomotion. J Neurosci Methods
21:321–333
Smith JC, Ellenberger HH, Ballanyi K, Richter DW,
Feldman
JL
(1991)
Pre-Bötzinger
complex:
a
brainstem region that may generate respiratory rhythm
in mammals. Science 254:726–729
Smith JC, Butera RJ, Koshiya N, Del Negro C, Wilson CG,
Johnson SM (2000) Respiratory rhythm generation in
neonatal and adult mammals: the hybrid pacemaker-
network model. Respir Physiol 122:131–147. https://
doi.org/10.1016/s0034-5687(00)00155-9
Smith JC, Abdala APL, Koizumi H, Rybak IA, Paton JFR
(2007) Spatial and functional architecture of the
mammalian brain stem respiratory network: a hier-
archy of three oscillatory mechanisms. J Neuro-
physiol 98:3370–3387. https://doi.org/10.1152/jn.
00985.2007
Smith JC, Abdala APL, Borgmann A, Rybak IA,
Paton JFR (2013) Brainstem respiratory networks:
building blocks and microcircuits. Trends Neurosci
36:152–162. https://doi.org/10.1016/j.tins.2012.11.
004
Spardy LE, Markin SN, Shevtsova NA, Prilutsky BI,
Rybak IA, Rubin JE (2011a) A dynamical systems
analysis of afferent control in a neuromechanical
model of locomotion: I. Rhythm generation. J Neural
Eng 8:065003. https://doi.org/10.1088/1741-2560/8/6/
065003
Spardy LE, Markin SN, Shevtsova NA, Prilutsky BI,
Rybak IA, Rubin JE (2011b) A dynamical systems
analysis of afferent control in a neuromechanical
model of locomotion: II. Phase asymmetry. J Neural
Eng 8:065004. https://doi.org/10.1088/1741-2560/8/6/
065004
Stuart DG, Hultborn H (2008) Thomas Graham Brown
(1882–1965), Anders Lundberg (1920–), and the neu-
ral control of stepping. Brain Res Rev 59:74–95.
https://doi.org/10.1016/j.brainresrev.2008.06.001
Tabak J, Rinzel J, O’Donovan MJ (2001) The role
of
activity-dependent
network
depression
in
the
expression and self-regulation of spontaneous activity
in
the
developing
spinal
cord.
J
Neurosci
21:
8966–8978
Talpalar AE, Bouvier J, Borgius L, Fortin G, Pierani A,
Kiehn O (2013) Dual-mode operation of neuronal
networks involved in left-right alternation. Nature
500:85–88. https://doi.org/10.1038/nature12286
Toporikova N, Butera RJ (2011) Two types of independent
bursting mechanisms in inspiratory neurons: an inte-
grative model. J Comput Neurosci 30:515–528. https://
doi.org/10.1007/s10827-010-0274-z
Vallstedt A, Kullander K (2013) Dorsally derived spinal
interneurons in locomotor circuits. Ann N Y Acad Sci
1279:32–42. https://doi.org/10.1111/j.1749-6632.
2012.06801.x
Wang X-J, Rinzel J (1992) Alternating and synchronous
rhythms in reciprocally inhibitory model neurons.
Neural Comput 4:84–97. https://doi.org/10.1162/neco.
1992.4.1.84
Whelan PJ (2010) Shining light into the black box of spinal
locomotor networks. Philos Trans R Soc Lond Ser
B Biol Sci 365:2383–2395. https://doi.org/10.1098/
rstb.2009.0322
Wolf E, Soffe SR, Roberts A (2009) Longitudinal neuro-
nal organization and coordination in a simple verte-
brate:
a
continuous,
semi-quantitative
computer
model of the central pattern generator for swimming
in
young
frog
tadpoles.
J
Comput
Neurosci
27:291–308. https://doi.org/10.1007/s10827-009-
0143-9
Zagoraiou L, Akay T, Martin JF, Brownstone RM,
Jessell TM, Miles GB (2009) A cluster of cholinergic
premotor interneurons modulates mouse locomotor
activity. Neuron 64:645–662. https://doi.org/10.1016/
j.neuron.2009.10.017
Zhang Y, Narayan S, Geiman E, Lanuza GM, Velasquez T,
Shanks B, Akay T, Dyck J, Pearson K, Gosgnach S,
Fan C-M, Goulding M (2008) V3 spinal neurons estab-
lish a robust and balanced locomotor rhythm during
walking. Neuron 60:84–96. https://doi.org/10.1016/j.
neuron.2008.09.027
Zhong G, Shevtsova N, Rybak I, Harris-Warrick R (2012)
Neuronal activity in the isolated mouse spinal cord
during spontaneous deletions in ﬁctive locomotion:
insights into locomotor CPG organization. J Physiol
590:4735–4759. https://doi.org/10.1113/jphysiol.
2012.240895
140
Vertebrate Pattern Generation: Overview

Vestibular System: Overview
Americo Migliaccio
Balance and Vision Laboratory, Neuroscience
Research Australia, University of New South
Wales, Sydney, NSW, Australia
Detailed Description
The vestibular system is our sixth sense. Not only
does it contribute to our sense of orientation and
movement in space but, via the vestibulo-ocular
reﬂex (VOR), it stabilizes our vision, especially
during rapid unpredictable head movements.
Without the VOR our visual world would appear
to move every time we undertook an activity that
involves head movement, such as during walking,
running, climbing, or driving on a bumpy road.
The vestibular organ is located in the inner ear
next to the cochlear. As our head movements
consist of rotations (nodding in afﬁrmation) and
translations (jumping up and down when happy),
the vestibular organ comprises two sensor sys-
tems. The semicircular canals sense angular head
rotations while the otolith organs sense linear
head translations (see ▶“Vestibular Otoliths,
Response to Vibration and Sound”). Both the
semicircular canals and otolith organs rely on
specialized hair cells and the inertia of ﬂuid
(canals) or a gelatinous mass (otoliths) to bend
the sensory hairs. Because there are two types of
hair cells and because individual vestibular affer-
ents receive input from different combinations of
these hair cells, the signals coming from the ves-
tibular periphery contain a wealth of sensory
information that includes head-rotation velocity
and acceleration, head linear acceleration, or tilt
angle (see ▶“Peripheral Vestibular Signal Pro-
cessing”). This information is encoded in the sig-
nals coming from the peripheral vestibular organs
but further processing occurs centrally (see
▶“Central Vestibular Signal Processing”) to gen-
erate
the
appropriate
three-dimensional
(horizontal,
vertical,
and
torsional)
eye
movements required to stabilize vision and the
muscle responses that equilibrate balance. Some
of this processing is very rapid. For example, the
direct pathway of the VOR consists of a three-
neuron arc that has a transmission delay of ~7 ms
in primates. In fact, the VOR is our fastest neural
reﬂex.
The central vestibular neural circuitry, located
in the vestibular nuclei of the brainstem, receives
input from the vestibulo-cerebellum to sustain,
modify, and train the vestibular response (see
▶“Vestibular Adaptation and Compensation”).
Thus, the vestibular system is an exquisitely
adaptable system during normal function. For
example, the VOR response increases almost
immediately when needed to view close objects
or scenes, as if that context had been pre-
programmed and can be trained to increase or
decrease when wearing magnifying or minifying
lenses. The vestibular system also has the ability
to recalibrate to compensate for permanent injury
to one (complete injury) or both (incomplete
injury) of the vestibular organs and their associ-
ated nerves. There are many causes of damage to
the vestibular end organ and associated nerves.
The most common of these include age-related
cell loss, tumors, head trauma, ototoxic injury
(e.g., by aminoglycoside drugs), Meniere’s dis-
ease, infection (meningitis and vestibular neuri-
tis), and cochlear implantation (see ▶“Vestibular
Function After Cochlear Implantation”). About
1% of adults under 65 and 25% over 65 will
have an injury to the vestibular organs. Other
patients will have normal functioning vestibular
organs and nerves but suffer from abnormalities of
the vestibular organs that result in conditions such
as benign paroxysmal positional vertigo or supe-
rior canal dehiscence syndrome. Because the ves-
tibular system operates over a large range of head-
rotation frequencies (0 to ~15 Hz) and comprises
the canals and otolith organs, a range of diagnostic
tests have been developed to test for the different
conditions above including the following: head
impulse testing (see ▶“Vestibular, Canal Testing:
The Head Impulse Test”), video (see ▶“Vestibu-
lar, Eye Movement Testing”), imaging, and
Vestibular System: Overview
141

vestibular evoked myogenic potential testing. For
example, the head impulse test is used to deter-
mine the function of each of the six semicircular
canals and their associated nerves during physio-
logically relevant
rapid head
rotations. For
patients with complete vestibular organ loss, ves-
tibular function can be restored by a vestibular
prosthesis that senses head movements and trans-
mits an appropriately encoded electrical signal to
drive the vestibular nerves (see ▶“Vestibular
Prosthesis, Interface”). For prosthesis recipients
and also patients with incomplete vestibular loss,
vestibular rehabilitation is essential for maximum
recovery (see ▶“Vestibular, Rehabilitation”). Ide-
ally, vestibular rehabilitation exercises are tailored
for each patient so that compensatory mechanisms
are best enabled. The accepted techniques devel-
oped to diagnose, cure, and rehabilitate vestibular
injury have helped many patients because they are
based on our fundamental understanding of ves-
tibular signals and their processing. This is the
focus of the articles in this section.
Cross-References
▶Central Vestibular Signal Processing
▶Peripheral Vestibular Signal Processing
▶Vestibular Adaptation and Compensation
▶Vestibular
Function
After
Cochlear
Implantation
▶Vestibular Otoliths, Response to Vibration and
Sound
▶Vestibular Prosthesis, Interface
▶Vestibular, Canal Testing: The Head Impulse
Test
▶Vestibular, Eye Movement Testing
▶Vestibular, Rehabilitation
142
Vestibular System: Overview

