
Structural Equation
Modeling
A Bayesian Approach
Sik-Yum Lee
Department of Statistics
Chinese University of Hong Kong


Structural Equation
Modeling

WILEY SERIES IN PROBABILITY AND STATISTICS
Established by WALTER A. SHEWHART and SAMUEL S. WILKS
Editors: David J. Balding, Peter Bloomfield, Noel A. C. Cressie,
Nicholas I. Fisher, Iain M. Johnstone, J. B. Kadane, Geert Molenberghs,
Louise M. Ryan, David W. Scott, Adrian F. M. Smith, Sanford Weisberg
Editors Emeriti Vic Barnett, J. Stuart Hunter, David G. Kendall
A complete list of the titles in this series appears at the end of this volume.

Structural Equation
Modeling
A Bayesian Approach
Sik-Yum Lee
Department of Statistics
Chinese University of Hong Kong

Copyright © 2007
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester,
West Sussex PO19 8SQ, England
Telephone
+44 1243 779777
Email (for orders and customer service enquiries): cs-books@wiley.co.uk
Visit our Home Page on www.wiley.com
All Rights Reserved. No part of this publication may be reproduced, stored in a retrieval system or
transmitted in any form or by any means, electronic, mechanical, photocopying, recording,
scanning or otherwise, except under the terms of the Copyright, Designs and Patents Act 1988 or
under the terms of a licence issued by the Copyright Licensing Agency Ltd, 90 Tottenham Court
Road, London W1T 4LP, UK, without the permission in writing of the Publisher. Requests to
the Publisher should be addressed to the Permissions Department, John Wiley & Sons Ltd, The
Atrium, Southern Gate, Chichester, West Sussex PO19 8SQ, England, or emailed to
permreq@wiley.co.uk, or faxed to (+44) 1243 770620.
Designations used by companies to distinguish their products are often claimed as trademarks. All
brand names and product names used in this book are trade names, service marks, trademarks or
registered trademarks of their respective owners. The Publisher is not associated with any product
or vendor mentioned in this book.
This publication is designed to provide accurate and authoritative information in regard to the
subject matter covered. It is sold on the understanding that the Publisher is not engaged in
rendering professional services. If professional advice or other expert assistance is required, the
services of a competent professional should be sought.
Other Wiley Editorial Offices
John Wiley & Sons Inc., 111 River Street, Hoboken, NJ 07030, USA
Jossey-Bass, 989 Market Street, San Francisco, CA 94103-1741, USA
Wiley-VCH Verlag GmbH, Boschstr. 12, D-69469 Weinheim, Germany
John Wiley & Sons Australia Ltd, 42 McDougall Street, Milton, Queensland 4064, Australia
John Wiley & Sons (Asia) Pte Ltd, 2 Clementi Loop #02-01, Jin Xing Distripark, Singapore
129809
John Wiley & Sons Canada Ltd, 22 Worcester Road, Etobicoke, Ontario, Canada M9W 1L1
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print
may not be available in electronic books.
Library of Congress Cataloging in Publication Data
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library
ISBN-13 978-0-470-02423-2(HB)
Typeset in 10/12pt Galliard by Integra Software Services Pvt. Ltd, Pondicherry, India
Printed and bound in Great Britain by TJ International, Padstow, Cornwall
This book is printed on acid-free paper responsibly manufactured from sustainable forestry in which
at least two trees are planted for each one used for paper production.

For Mable Lee and Timothy Lee


Contents
About the Author
xi
Preface
xiii
1
Introduction
1
1.1
Standard Structural Equation Models
1
1.2
Covariance Structure Analysis
2
1.3
Why a New Book?
3
1.4
Objectives of the Book
4
1.5
Data Sets and Notations
6
Appendix 1.1
7
References
10
2
Some Basic Structural Equation Models
13
2.1
Introduction
13
2.2
Exploratory Factor Analysis
15
2.3
Confirmatory and Higher-order Factor Analysis Models
18
2.4
The LISREL Model
22
2.5
The Bentler–Weeks Model
26
2.6
Discussion
27
References
28
3
Covariance Structure Analysis
31
3.1
Introduction
31
3.2
Definitions, Notations and Preliminary Results
33
3.3
GLS Analysis of Covariance Structure
36
3.4
ML Analysis of Covariance Structure
41
3.5
Asymptotically Distribution-free Methods
44
3.6
Some Iterative Procedures
47
Appendix 3.1: Matrix Calculus
53
Appendix 3.2: Some Basic Results in Probability Theory
57

viii
CONTENTS
Appendix 3.3: Proofs of Some Results
59
References
65
4
Bayesian Estimation of Structural Equation Models
67
4.1
Introduction
67
4.2
Basic Principles and Concepts of Bayesian Analysis of SEMs
70
4.3
Bayesian Estimation of the CFA Model
81
4.4
Bayesian Estimation of Standard SEMs
95
4.5
Bayesian Estimation via WinBUGS
98
Appendix 4.1: The Metropolis–Hastings Algorithm
104
Appendix 4.2: EPSR Value
105
Appendix 4.3: Derivations of Conditional Distributions
106
References
108
5
Model Comparison and Model Checking
111
5.1
Introduction
111
5.2
Bayes Factor
113
5.3
Path Sampling
115
5.4
An Application: Bayesian Analysis of SEMs with Fixed
Covariates
120
5.5
Other Methods
127
5.6
Discussion
130
Appendix 5.1: Another Proof of Equation (5.10)
131
Appendix 5.2: Conditional Distributions for Simulating Yt
133
Appendix 5.3: PP p-values for Model Assessment
136
References
136
6
Structural Equation Models with Continuous and Ordered
Categorical Variables
139
6.1
Introduction
139
6.2
The Basic Model
142
6.3
Bayesian Estimation and Goodness-of-fit
144
6.4
Bayesian Model Comparison
155
6.5
Application 1: Bayesian Selection of the Number of Factors in
EFA
159
6.6
Application 2: Bayesian Analysis of Quality of Life Data
164
References
172
7
Structural Equation Models with Dichotomous Variables
175
7.1
Introduction
175
7.2
Bayesian Analysis
177
7.3
Analysis of a Multivariate Probit Confirmatory Factor Analysis
Model
186

CONTENTS
ix
7.4
Discussion
190
Appendix 7.1: Questions Associated with the Manifest Variables
191
References
192
8
Nonlinear Structural Equation Models
195
8.1
Introduction
195
8.2
Bayesian Analysis of a Nonlinear SEM
197
8.3
Bayesian Estimation of Nonlinear SEMs with Mixed
Continuous and Ordered Categorical Variables
215
8.4
Bayesian Estimation of SEMs with Nonlinear Covariates
and Latent Variables
220
8.5
Bayesian Model Comparison
230
References
239
9
Two-level Nonlinear Structural Equation Models
243
9.1
Introduction
243
9.2
A Two-level Nonlinear SEM with Mixed Type Variables
244
9.3
Bayesian Estimation
247
9.4
Goodness-of-fit and Model Comparison
255
9.5
An Application: Filipina CSWs Study
259
9.6
Two-level Nonlinear SEMs with Cross-level Effects
267
9.7
Analysis of Two-level Nonlinear SEMs using WinBUGS
275
Appendix 9.1: Conditional Distributions: Two-level Nonlinear
SEM
279
Appendix 9.2: MH Algorithm: Two-level Nonlinear SEM
283
Appendix 9.3: PP p-value for Two-level NSEM with Mixed
Continuous and Ordered-categorical Variables
285
Appendix 9.4: Questions Associated with the Manifest Variables
286
Appendix 9.5: Conditional Distributions: SEMs with Cross-level
Effects
286
Appendix 9.6: The MH algorithm: SEMs with Cross-level Effects
289
References
290
10
Multisample Analysis of Structural Equation Models
293
10.1
Introduction
293
10.2
The Multisample Nonlinear Structural Equation Model
294
10.3
Bayesian Analysis of Multisample Nonlinear SEMs
297
10.4
Numerical Illustrations
302
Appendix 10.1: Conditional Distributions: Multisample SEMs
313
References
316
11
Finite Mixtures in Structural Equation Models
319
11.1
Introduction
319
11.2
Finite Mixtures in SEMs
321

x
CONTENTS
11.3
Bayesian Estimation and Classification
323
11.4
Examples and Simulation Study
330
11.5
Bayesian Model Comparison of Mixture SEMs
344
Appendix 11.1: The Permutation Sampler
351
Appendix 11.2: Searching for Identifiability Constraints
352
References
352
12
Structural Equation Models with Missing Data
355
12.1
Introduction
355
12.2
A General Framework for SEMs with Missing Data that are
MAR
357
12.3
Nonlinear SEM with Missing Continuous and Ordered
Categorical Data
359
12.4
Mixture of SEMs with Missing Data
370
12.5
Nonlinear SEMs with Nonignorable Missing Data
375
12.6
Analysis of SEMs with Missing Data via WinBUGS
386
Appendix 12.1: Implementation of the MH Algorithm
389
References
390
13
Structural Equation Models with Exponential Family of
Distributions
393
13.1
Introduction
393
13.2
The SEM Framework with Exponential Family of
Distributions
394
13.3
A Bayesian Approach
398
13.4
A Simulation Study
402
13.5
A Real Example: A Compliance Study of Patients
404
13.6
Bayesian Analysis of an Artificial Example using WinBUGS
411
13.7
Discussion
416
Appendix 13.1: Implementation of the MH Algorithms
417
Appendix 13.2
419
References
419
14
Conclusion
421
References
425
Index
427

About the Author
Sik-Yum Lee is a professor of statistics at the Chinese University of Hong
Kong. He earned his Ph.D. in biostatistics at the University of California, Los
Angeles, USA. He received a distinguished service award from the International
Chinese Statistical Association, is a former president of the Hong Kong Statis-
tical Society, and is an elected member of the International Statistical Institute
and a Fellow of the American Statistical Association. He serves as Associate
Editor for Psychometrika and Computational Statistics & Data Analysis, and as
a member of the Editorial Board of British Journal of Mathematical and Statis-
tical Psychology, Structural Equation Modeling, Handbook of Computing and
Statistics with Applications and Chinese Journal of Medicine. His research inter-
ests are in structural equation models, latent variable models, Bayesian methods
and statistical diagnostics. He is editor of Handbook of Latent Variable and
Related Models and author of over 140 papers.


Preface
Substantive theory usually involves observed and latent variables. An observed
variable can be directly measured through a single measurement, such as income
or systolic blood pressure. Latent variables are those that cannot be directly
measured. Usually, one has to use several observed variables to assess the char-
acteristic of a latent variable. It is very easy to give examples of latent variables in
behavioral, biological, educational, medical, psychological and social sciences.
For instance, in medical science, obesity is a latent variable that should be
assessed by body mass, waist and hip indexes; lipid is a latent variable that is
better assessed by non-high density lipoprotein, low density lipoprotein and
triglyceride; blood pressure is another latent variable that should be measured
by both systolic and diastolic blood pressures. Basically, structural equation
models (SEMs) are regression models with observed and latent variables. For
example, the influential LISREL model is composed of two simple regression
equations: the measurement equation that relates the observed variables to the
latent variables, and the structural equation that relates the endogenous latent
variables to other latent variables. Due to the contributions of many psychome-
tricians, including but not limited to Karl Jöreskog and Peter Bentler, and their
LISREL and EQS6 programs, SEMs have been extensively applied not only to
behavioral, educational and social science, but also to biological and medical
sciences in the last quarter of a century.
The excellent book of Bollen (1989) on standard SEMs was published more
than 15 years ago. Despite the widespread use of the standard SEMs, and the
rapid growth in the new developments of nonlinear SEMs, two-level SEMs,
and mixtures of SEMs, as well as SEMs for more complex data structures, such
as dichotomous, ordered categorical, binary and missing data, there are very
few new reference/textbooks in the field and there have been very few practical
applications of the aforementioned recent developments to substantive research.
This unexpected phenomenon is the motivation for writing this book, to provide
a reference for researchers in various disciplines, and a textbook for graduate
students in statistics, biostatistics and psychometrics. My main purpose is to
introduce a Bayesian approach for developing efficient and rigorous statistical

xiv
PREFACE
methodologies in SEMs and applying them to practical problems. Given the
importance of latent variables and the popularity of the regression models, I
hope that this book will help to promote SEMs to become a mainstream in
statistics and psychometrics, and stimulate more applications.
The theme of this book is on the Bayesian analysis of SEMs. An introduc-
tion is given in Chapter 1, and some standard models are briefly discussed in
Chapter 2. Chapter 3 presents the basic asymptotic theory for the traditional
maximum likelihood (ML) and generalized least squares (GLS) approaches in
analyzing the covariance structure of the model. The reason for including these
nonBayesian materials is to provide a rigorous technical background related
to the statistical results that are given in a large number of commercial soft-
ware packages. For example, I present detailed proof on the consistency and
asymptotic normality of the estimators, as well as the asymptotic chi-square
distribution of the goodness-of-fit statistic. As far as I know, these kind of
technical materials cannot be found in existing books on SEMs. Chapter 4
gives a detailed introduction to Bayesian estimation, whereas Chapter 5 treats
Bayesian model comparison through the Bayes factor. Materials provided in
these two core chapters can be applied to standard SEMs and their general-
izations. Chapters 6 to 13 present the descriptions of the Bayesian approach
as applied to SEMs with ordered categorical variables, SEMs with dichoto-
mous variables, nonlinear SEMs, two-level nonlinear SEMs, multisample SEMs,
mixtures of SEMs, SEMs with missing data and SEMs with variables from
an exponential family of distributions. The Bayesian methodologies are illus-
trated by analyses of real examples in various fields via our tailor-made
programs and/or the general software WinBUGS. The fundamental material
in these chapters is self-contained. However, for generality and for enhancing
wider applications, certain sections in the chapters discuss combinations of
models. These sections depend on some of the material presented in previous
chapters.
This book requires an understanding of some fundamental concepts in
statistics. In particular, the concept of conditional distributions is necessary to
understand the key ideas of the posterior distribution and the sampling-based
computational methods. It does not need a knowledge of factor analysis or
SEMs, but such a knowledge will enhance the appreciation of the Bayesian
methodologies. It requires some basic knowledge of convergence in proba-
bility and convergence in distribution to understand the materials in Chapter 3.
However, other chapters are completely independent of this chapter. Readers
who do not have an interest in the asymptotic theory of the traditional ML and
GLS approaches may skip Chapter 3. I am pleased to receive any comments
about the book via email at sylee@sta.cuhk.edu.hk.
I owe a great debt to organizations and individuals for the use of their data
sets. These include: World Value Study Group, World Values Survey, 1981–
1984 and 1990–1993, for allowing the use of the Inter-university Consor-
tium for Political and Social Research (ICPSR) data set; the Faculty of Educa-

PREFACE
xv
tion and Hong Kong Institute of Educational Research, the Chinese University
of Hong Kong for providing part of the data set in the Accelerated Schools for
Quality Education (ASQE) Project; D. E. Morisky, J. A. Stein and their colleagues
for providing the AIDS data set; and Juliana C. N. Chan for providing the dataset
about patient’s non-adherence. I am deeply grateful to the Medical Research
Council Biostatistics Unit (Cambridge, UK), and the Department of Epidemi-
ology and Public Health of the Imperial College School of Medicine at St. Mary’s
Hospital(London,UK)forprovidingthepowerfulsoftwareWinBUGSasanalter-
native program in obtaining the Bayesian solutions. Without the kind support
fromtheabovementionedorganizationsandindividuals,thisbookmightnothave
existed. The Research Grants Council of the Hong Kong Special Administration
RegionandtheChineseUniversityofHongKonghaveprovidedfinancialsupport
for my research and the writing of this book.
This book owes much to many people. It is certainly a great pleasure to thank
them individually for their help and influence in producing this book, which
represents most of my recent work in SEMs. My advisor, R. I. Jennrich, taught
me computational methods and deeply inspired me with his serious attitude
toward research. P. M. Bentler led me to the exciting field of SEM; he has
had a distinctive influence on me during my early work in this field. I am
grateful to colleagues in the Department of Statistics, the Chinese University
of Hong Kong. In particular, W.Y Poon has given me generous support in
research and administration work; W. H. Wong introduced the idea of data
augmentation and Markov chain Monte Carlo methods during his 3 year stay
in our department; and X. Y. Song gave numerous constructive suggestions in
many chapters and essentially wrote all the programs for analyzing the artificial
and real examples. I am very fortunate to have excellent students and research
assistants. Those to whom I am greatly indebted include S. J. Wang, J. Q. Shi,
W. Zhang, H. T. Zhu, X. Y. Song, J. S. Fu, L. Xu, B. Lu, Y. M. Xia, N. S. Tang,
Y. Li, F. Chen, J. H. Cai, C. T. Poon and Y. Zhou. Most of them read
the manuscript and gave helpful comments. WinBUGS results were mostly
obtained by B. Lu. A number of people tackled the tedious task of typing
the manuscript and drawing the path diagrams. For their excellent work and
their patience with my handwriting, I would like to thank K. H. Leung and
E. L. S. Tam. I am grateful to all the wonderful people on the John Wiley
editorial staff, particularly Kelly Board, Lucy Bryan, Wendy Hunter, Simon
Lightfoot, Kathryn Sharples, and Vidya Vijayan, and their design team for their
continued assistance, encouragement and support of my work. Last and most
important, I owe deepest thanks to Mable Lee and Timothy Lee for their
constant understanding, support, encouragement and love that greatly release
me from the pressure of the observed and latent variables.
SIK-YUM LEE
Department of Statistics
Chinese University of Hong Kong
Shatin, NT,
Hong Kong


1
Introduction
1.1
STANDARD STRUCTURAL EQUATION MODELS
In behavioural, educational, medical, and social sciences, substantive theory
usually involves two kinds of variables, namely manifest (observed) and latent
variables. Manifest variables are those that can be measured directly, such as
income, test scores, systolic blood pressure, diastolic blood pressure, weight or
heart rate. All data are records of measurements from observed data. Very often,
it is necessary to deal with latent variables that cannot be directly measured
by a single manifest variable. Examples are intelligence, personality, quantita-
tive ability, anxiety, buying behavior, blood pressure and health condition. In
practice, the characteristics of a latent variable can be partially measured by
a linear combination of some manifest variables. For example, the quantita-
tive ability of secondary school students can be reflected by their test scores
in mathematics, physics and chemistry; the blood pressure of a patient can be
measured by systolic and diastolic blood pressures. In most substantive research,
it is important to establish an appropriate model to evaluate a series of simulta-
neous hypotheses about the impacts of latent variables and manifest variables on
the other variables, and take the measurement errors into account. Structural
equation models (SEMs) are well recognized as the most important statis-
tical method to serve the above purpose. SEMs can be applied to many fields.
For example, they can be applied to market research for establishing interrela-
tionships between demand and supply, and the attitude and behaviour of the
customers; to environmental science for investigating how health is affected
by air and water pollution; to education for measuring the growth of intel-
ligence and its relationship to personality and school environment; and to
medicine for analyzing quality of life data and/or examining the impacts of
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

2
1
INTRODUCTION
physicians’ concern, social influence and cognition on patients’ adherence to
medication.
The standard SEM, in particular the LISREL model (Jöreskog and Sörbom,
1996), is composed of two components. The first component is a confirma-
tory factor analysis model which relates the latent variables to all their corre-
sponding manifest variables (indicators) and takes the measurement errors into
account. This component can be regarded as a regression model which regresses
the manifest variables with a small number of latent variables. The second
component is again a regression type structural equation which regresses the
endogenous (dependent) latent variables with the linear terms of some endoge-
nous and exogenous (independent) latent variables. As latent variables are
random, they cannot be directly analyzed by techniques in ordinary regression
that are based on raw observations. However, conceptually, SEMs are formu-
lated by the familiar regression type model, hence they are easy to apply in
practice.
1.2
COVARIANCE STRUCTURE ANALYSIS
For standard SEMs, the covariance matrix of the manifest random vector y
contains all the unknown parameters in the model. Hence, the classical methods
for analyzing standard SEMs focused on the sample covariance matrix S and
not the raw individual random vectors yi. This involves the formulation of the
covariance structure , which is a matrix function of the unknown parameters
vector ; estimation of  by minimizing (or maximizing) some objective func-
tions that measure the discrepancy between S and , such as the maximum
likelihood (ML) function or the generalized least square (GLS) function; and
the derivation of asymptotic goodness-of-fit statistics for assessing whether 
fits S. As this kind of analysis emphasizes the population covariance matrix
and the sample covariance matrix, it is often called covariance structure anal-
ysis. Today, more than a dozen user-friendly SEM software packages have been
developed on the basis of covariance structure analysis approach with the sample
covariance matrix. Typical examples are LISREL, EQS6 and AMOS. The covari-
ance structure analysis approach depends heavily on the asymptotic normality of
S, either in defining the objective function or in deriving the asymptotic prop-
erties for statistical inferences. When the distribution of the manifest random
vector yi is multivariate normal and the sample size is reasonably large, the
asymptotic distribution of S accurately approximates to the claimed multivariate
normal distribution, and as a result this approach works fine. However, under
slightly more complex situations that are common in substantive research, the
covariance structure analysis approach on the basis of S is not effective and may
encounter theoretical and computational problems.
It is well-recognized that estimating nonlinear terms (particularly the inter-
action term) among latent variables in the structural equation is an important

1.3
WHY A NEW BOOK?
3
issue in behavioral, social and psychological science (see Kenny and Judd,
1984; Bagozzi, Baumgartner and Yi, 1992). Due to the presence of the
nonlinear terms of latent variables, the endogenous latent variables and the
related manifest variables in yi are not normally distributed. Hence, the sample
covariance matrix of the raw sample observations is inadequate for modeling the
nonlinear relationships. The product indicator approach (Jöreskog and Yang,
1996; Marsh, Wen and Hau, 2004) that artifically added products of indi-
cators to yi and used the sample covariance matrix of the enlarged manifest
random vector for analysis, cannot provide a satisfactory method to cope with
the problem (see Lee, Song, and Poon, 2004). For dichotomous or ordered
categorical data, the sample covariance matrix of the raw sample data cannot
be used. The multistage estimation procedures in LISREL or EQS produced
estimates that are less optimal than the exact ML estimates and cannot be
applied to analyze nonlinear terms of latent variables. For missing data that have
a small number of observations within some missing patterns, the covariance
structure analysis approach would also encounter serious difficulties because the
sample covariance matrices corresponding to those patterns could be singular.
The degree of difficulty is further compounded with a large number of missing
patterns in the data set, or the missing data are missing with a nonignorable
missing mechanism. For hierarchical data, the individual observations are corre-
lated; this induces a problem in the covariance structure analysis with the sample
covariance matrix.
In view of the above discussion, it is clear that although the covariance
structure analysis approach based on the sample covariance matrix works well
for the standard SEMs under the normality assumption, it cannot be applied
to more complex models or data structures that are commonly encountered in
substantive research. To develop sound statistical methods for those complex
situations, it is necessary to develop better statistical methods which are based
on the individual observations and their basic model, rather than the sample
covariance matrix.
1.3
WHY A NEW BOOK?
In the past few years, the growth of SEM has been very rapid. New models and
statistical methods have been developed for better analyses of more complex
data structures in substantive research. These include but are not limited to: (i)
SEMs with dichotomous and/or ordered categorical data (Shi and Lee, 1998,
2000; Moustaki, 2003; Rabe-Hesketh, Skrondal and Pickles, 2004; among
others); (ii) nonlinear SEMs (Kenny and Judd, 1984; Klein and Moosbrugger,
2000; Lee and Song, 2003a; Wall and Amemiya, 2000; among others); (iii)
linear or nonlinear SEMs with covariates (Lee and Shi, 2000; Moustaki, 2003;
Song and Lee, 2006a; among others); (iv) two-level or multilevel SEMs (Lee
and Shi, 2001; Ansari and Jedidi, 2000; Rabe-Hesketh, Skrondal and Pickles,

4
1
INTRODUCTION
2004; Song and Lee, 2004; among others); (v) multisample SEMs (Song and
Lee, 2001, 2002b); (vi) mixtures of SEMs (Jedidi, Jagpal and DeSarbo, 1997;
Arminger, Stein and Wittenberg, 1999; Dolan and van der Maas, 1998; Zhu
and Lee, 2001; Lee and Song, 2003b; among others); (vii) SEMs with missing
data that are missing at random or with a nonignorable mechanism (Jamshidian
and Bentler, 1999; Song and Lee, 2002a; Lee and Song, 2004a; Lee and Tang,
2006a; Song and Lee, 2006b; among others); and (viii) SEMs with variables
from the exponential family distributions (Wedel and Kamakura, 2001; Rabe-
Hesketh, Skrondal and Pickles, 2004; Lee and Tang, 2006b; Song and Lee,
2006b; among others). The above articles not only provide theoretical results,
but also have significant practical value. For instance, it is very common in
practice to encounter ordered categorical data with missing data, hierarchical
data and/or heterogeneous data, and hence developments of sound statistical
methods to cope with such practical situations are useful.
The primary goal of all the existing commercial software packages in SEMs
is for analyzing the standard SEMs under the normal assumption. They cannot
effectively and efficiently analyze the more complex models and/or data struc-
tures mentioned above. At the moment, there are only a limited number
of reference/textbooks in SEM. Moreover, the emphasis of all the existing
books, for example Bollen (1989), was devoted to the standard SEMs, and
focused on the covariance structure analysis approach. Hence, despite the
widespread use of SEMs, and the importance of the aforementioned complex
models for sound and rigorous analyses of real data sets, there have been very
few practical applications of the recent developments of SEMs to substantive
research. The limited applications are not due to a lack of relevant substan-
tive applications that required such models and their associative statistical
methods. Rather, the main reasons are that the applied researchers are not
familiar with these models and methods, and the existing commercial SEM
software cannot produce satisfactory solutions for coping with the complex
situations.
Therefore, there is a need for a new reference/textbook for the second
generation of SEM which involves a much wider class of SEMs that include
the standard SEMs and their useful generations. This book should provide a
more appropriate approach than the covariance structure analysis approach in
analyzing the general class of SEMs, together with a dependable software for
obtaining reliable and rigorous results for statistical inference.
1.4
OBJECTIVES OF THE BOOK
One of the basic objectives of this book is to propose a Bayesian approach for
analyzing some useful structural equation models and/or data structures that

1.4
OBJECTIVES OF THE BOOK
5
are commonly encountered in substantive research. This includes the treatments
of the standard SEMs and their useful generalizations in various ways. More
specifically, the generalizations are SEMs with ordered categorical variables,
SEMs with dichotomous variables, nonlinear SEMs, two-level SEMs, multi-
sample SEMs, mixtures of SEMs, SEMs with ignorable and/or nonignorable
missing data, SEMs with variables from the exponential family distributions,
and some of their combinations. In formulating various SEMs, and in devel-
oping the Bayesian methods, the emphasis is placed on the raw individual
random observations rather than on the sample covariance matrix. This formu-
lation has several advantages. First, the development of statistical methods is
based on the first moment properties of the raw individual observations which
is simpler than the second moment properties of the sample covariance matrix.
Hence, it is easier to apply in more complex situations. Second, it leads to
a direct estimation of the latent variables which is better than the classical
regression method or the Bartlett’s method for obtaining the factor score esti-
mates. Third, as it directly models manifest variables with their latent variables
through the familiar regression equations, it gives a more direct interpreta-
tion and can utilize the common techniques in regression such as outlier and
residual analyses in conducting statistical analysis. The advantages of a Bayesian
approach are that it allows the use of genuine prior information in addition
to the information that is available in the observed data for producing better
results, provides useful statistics such as the mean and percentiles of the poste-
rior distribution of the unknown parameters, and gives more reliable results for
small samples (see Dunson, 2000; Lee and Song; 2004b; Scheines, Hoijtink and
Boomsma, 1999).
The next aim is to describe the technique of data augmentation (Tanner and
Wong, 1987), and introduce some efficient tools in statistical computing for
analyzing SEMs. The key idea of data augmentation is to augment the observed
data with the latent quantities, which could be the latent variables or the unob-
servable data (missing data or the unobservable continuous measurements that
underlie the dichotomous and/or ordered categorical data), so that the Bayesian
analysis is feasible with the complete-data set. The introduced tools in statistical
computing include some Markov chain Monte Carlo (MCMC) methods, such
as the Gibbs sampler (Geman and Geman, 1984) and the Metropolis – Hast-
ings algorithm (Metropolis et al., 1953; Hastings, 1970) and path sampling
(Gelman and Meng, 1998). The strategy of data augmentation followed by
MCMC methods will be used repeatedly to analyze complex SEMs and data
structures throughout this book. The Bayesian methodologies will be demon-
strated through real examples in the fields of education, management, medicine,
psychology and sociology.
One of the main goals is the introduction of the freely available software
WinBUGS (Spiegelhalter, Thomas, Best and Lunn, 2003) to the field of SEM.
This software is able to produce reliable Bayesian statistics including the Bayesian

6
1
INTRODUCTION
estimates and their standard error estimates for a wide range of statistical models
(Congdon, 2003). For most SEMs, it also provides the Deviance Informa-
tion Criterion (DIC, see Speigelhalter, Thomas, Best and Lunn, 2003) for
model comparison or goodness-of-fit assessment of the hypothesized model.
The applications of WinBUGS to various SEMs are illustrated by real and/or
artifical examples in Chapter 4 and Chapters 6 to 13. With the availability of the
dependable software WinBUGS, researchers can apply the Bayesian approach,
which has the aforementioned advantages and the same statistical optimality as
the maximum likelihood, without implementing their own computer programs.
Given its potential in analyzing various kinds of models, readers are recom-
mended to make some effort to get to know more about WinBUGS and its
power in solving practical problems.
1.5
DATA SETS AND NOTATIONS
Real examples require real data sets. The author owes a great debt to many
academic organizations and colleagues for allowing him to use their valuable
data in the real examples of this book. As it may not be appropriate to describe
every real data set, here is a brief description of the Inter-university Consortium
for Political and Social Research (ICPSR) data set which was collected by the
World Value Study Group (1994) in the project World Value Survey 1981–
1984 and 1990–1993 (World Value Study Group, ICPSR Version). The ICPSR
data set has been used in many of the illustrative examples. It was collected
in 45 societies around the world on very broad topics related to family life,
the meaning and purpose of life, work and contemporary political and social
issues (see the Summary of the ICPSR data set). It provides a rich source of real
data in relation to management, psychology and sociology. We have used the
data obtained in several countries, such as Canada, UK and USA. The variables
(or items in the questionnaire) that have been used in subsequent chapters are
collectively presented in Appendix 1.1.
There are not enough symbols for different types of observations in rela-
tion to observable manifest continuous and discrete variables, or covariates,
unobservable measurements in relation to missing data or continuous measure-
ments underlie the discrete data, latent variables, as well as different types of
parameters, such as thresholds, structural parameters in the model, and hyper-
parameters in the prior distributions. Hence, if the context is clear, some letters
of the Greek alphabet may be used to serve different purposes. For example,
 has been used to denote (i) an unknown threshold in defining an ordered
categorical variable, (ii) an intercept in the measurement equation, and (iii) a
hyperparameter in some prior distributions. Nevertheless, some general nota-
tion is given as in Table 1.1.

APPENDIX 1.1
7
Table
1.1
Typical notation.
Symbols
Meaning
u v x y
Manifest random vectors

Latent random vector in the measurement equation

Endogenous latent vector in the structural equation

Exogenous latent vector in the structural equation
 
Random vectors of measurement errors
	
Factor loading matrix in the measurement equation

 B 
Matrices of regression coefficients in the structural equation

Covariance matrix of latent variables
   
Diagonal covariance matrices of measurement errors with
diagonal elements k and k, respectively
ok 	ok ok	ok
Hyperparameters in the Gamma distributions of k and k,
respectively
Ro, o
Hyperparameters in the Wishart distribution related to the prior
distribution of 
	ok Hoyk
Hyperparameters in the multivariate normal distribution
related to the prior distribution of the kth row of 	 in the
measurement equation with y
	o
k Ho
k
Hyperparameters in the multivariate normal distribution related
to the prior distribution of the kth row of 	
k in the structural
equation
Iq
A q by q identity matrix. Sometimes just I is used to denote an
identity matrix if its dimension is clear
APPENDIX 1.1
This appendix provides the questions/items in the Data Collection Description,
World Values Study Group (1994), World Values Survey, 1981–1984 and
1990–1993 that have been used as manifest variables in the illustrative examples
of this book.
Thinking about your reasons for doing voluntary work, please use the
following five-point scale to indicate how important each of the reasons below
have been in your own case. (WHERE 1 IS UNIMPORTANT AND 5 IS
VERY IMPORTANT)
V 62
Religious beliefs
1
2
3
4
5
During the past few weeks, did you ever feel    (Yes: 1
No: 2)
V 89
Bored
1
2
V 91
Depressed or very unhappy
1
2
V 93
Upset because somebody criticized you
1
2

8
1
INTRODUCTION
V 96
All things considered, how satisfied are you with your life as a whole
these days?
1
2
3
4
5
6
7
8
9
10
Dissatisfied
Satisfied
Here are some aspects of a job that people say are important. Please look
at them and tell me which ones you personally think are important in a job.
(Mentioned: 1;
Not Mentioned: 2)
V 99
Good pay
1
2
V 100
Pleasant people to work with
1
2
V 102
Good job security
1
2
V 103
Good chances for promotion
1
2
V 111
A responsible job
1
2
V 115
How much pride, if any, do you take in the work that you do?
1
A great deal,
2
Some,
3
Little,
4
None
V 116
Overall, how satisfied or dissatisfied are you with your job?
1
2
3
4
5
6
7
8
9
10
Dissatisfied
Satisfied
V 117
How free are you to make decisions in your job?
1
2
3
4
5
6
7
8
9
10
Not at all
A great deal
V 129
When jobs are scarce, people should be forced to retire early,
Agree
1;
Neither
2;
Disagree
3
V 132
How satisfied are you with the financial situation of your household?
1
2
3
4
5
6
7
8
9
10
Dissatisfied
Satisfied
V 176
How important is God in your life? 10 means very important and 1
means not at all important.
1
2
3
4
5
6
7
8
9
10
V 179
How often do you pray to God outside of religious services? Would
you say   
1
Often
2
Sometimes
3
Hardly ever
4
Only in times of crisis
5
Never

APPENDIX 1.1
9
V 180
Overall, how satisfied or dissatisfied are you with your home life?
1
2
3
4
5
6
7
8
9
10
Dissatisfied
Satisfied
V 241
How interested would you say you are in politics?
1
Very interested
2
Somewhat interested
3
Not very interested
4
Not at all interested
Now I’d like you to tell me your views on various issues. How would you
place your views on this scale? 1 means you agree completely with the statement
on the left, 10 means you agree completely with the statement on the right, or
you can choose any number in between.
V 252
1
2
3
4
5
6
7
8
9
10
Individual should take more
responsibility for providing for
themselves
The state should take more respon-
sibility to ensure that everyone is
provided for
V 253
1
2
3
4
5
6
7
8
9
10
People who are unemployed should
have to take any job available or lose
their unemployment benefits
People who are unemployed should
have the right to refuse a job they
do not want
V 254
1
2
3
4
5
6
7
8
9
10
Competition is good. It stimulates
people to work hard and develop
new ideas
Competition is harmful. It brings
out the worst in perople
V 255
1
2
3
4
5
6
7
8
9
10
In the long run, hard work usually
brings a better life
Hard work doesn’t generally brings
success — it’s more a matter of luck
and connections
Please tell me for each of the following statements whether you think it can
always be justified, never be justified, or something in between.

10
1
INTRODUCTION
V 296
Claiming government benefits which you are not entitled to
1
2
3
4
5
6
7
8
9
10
Never
Always
V 297
Avoiding a fare on public transport
1
2
3
4
5
6
7
8
9
10
Never
Always
V 298
Cheating on tax if you have the chance
1
2
3
4
5
6
7
8
9
10
Never
Always
V 314
Failing to report damage you’ve done accidentally to a parked vehicle
1
2
3
4
5
6
7
8
9
10
Never
Always
I am going to read out some statements about the government and the
economy. For each one, could you tell me how much you agree or disagree?
V 336
Our government should be made much more open to the public
1
2
3
4
5
6
Agree Completely
Disagree Completely
V 337
We are more likely to have a healthy economy if the government allows
more freedom for individuals to do as they wish
1
2
3
4
5
6
Agree Completely
Disagree Completely
V 339
Political reform in this country is moving too rapidly
1
2
3
4
5
6
Agree Completely
Disagree Completely
REFERENCES
Ansari, A. and Jedidi, K. (2000) Bayesian factor analysis for multilevel binary observa-
tions. Psychometrika, 65, 475–497.
Arminger, G., Stein, P. and Wittenberg, J. (1999) Mixtures of conditional mean- and
covariance-structure models. Psychometrika, 64, 475–494.
Bagozzi, R. P., Baumgartner, H. and Yi, Y. (1992) State versus action orientation and
the theory of reasoned action: an application to coupon usage. Journal of Consumer
Research, 18, 505–517.
Bollen, K. A. (1989) Structural Equation Models with Latent Variables, NJ: John
Wiley & Sons, Inc.
Congdon, P. (2003) Applied Bayesian Modeling, Hoboken, NJ: John Wiley & Sons, Inc.

REFERENCES
11
Dolan, C. V. and van der Maas, J. J. L. (1998) Fitting multivariate normal mixtures
subject to structural equation modeling. Psychometrika, 63, 227–254.
Dunson, D. B. (2000) Bayesian latent variable models for clustered mixed outcomes.
Journal of the Royal Statistical Society, Series B, 62, 355–366.
Gelman, A. and Meng, X. L. (1998) Simulating normalizing constants: from importance
sampling to bridge sampling to path sampling. Statistical Science, 13, 163–185.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distribution, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Hastings, W. K. (1970) Monte Carlo sampling methods using Markov chains and their
application. Biometrika, 57, 97–109.
Jamshidian, M. and Bentler, P. M. (1999) ML estimation of mean and covariance
structures with missing data using complete data routines. Journal of Educational and
Behavioral Statistics, 24, 21–41.
Jedidi, K., Jagpal, H. S. and DeSarbo, W. S. (1997) STEMM: A general finite mixture
structural equation model. Journal of Classification, 14, 23–50.
Jöreskog, K. G. and Sörbom, D. (1996) LISREL 8: Structural Equation Modeling with
the SIMPLIS Command Language. Scientific Software International.
Jöreskog, K. G. and Yang, F. (1996) Nonlinear structural equation models: the Kenny–
Judd model with interaction effects. In G. A. Marcoulides and R. E. Schumacker (eds),
Advanced Structural Equation Modeling Techniques (pp. 57–88). Hillsdale, NJ: LEA.
Kenny, D. A. and Judd, C. M. (1984) Estimating the nonlinear and interactive effects
of latent variables. Psychological Bulletin, 96, 201–210.
Klein, A. and Moosbrugger, M. (2000) Maximum likelihood estimation of latent inter-
action effects with the LMS method. Psychometrika, 65, 457–474.
Lee, S. Y. and Shi, J. Q. (2000) Bayesian analysis of structural equation model with fixed
covariates. Structural Equation Modeling, 7, 411–430.
Lee, S. Y. and Shi, J. Q. (2001) Maximum likelihood estimation of two-level
latent variable models with mixed continuous and polytomous data. Biometrics, 57,
787–794.
Lee, S. Y. and Song, X. Y. (2003a) Model comparison of nonlinear structural equation
models with fixed covariates. Psychometrika, 68, 27–47.
Lee, S. Y. and Song, X. Y. (2003b) Bayesian model selection for mixtures of struc-
tural equation models with an unknown number of components. British Journal of
Mathematical and Statistical Psychology, 56, 145–165.
Lee, S. Y. and Song, X. Y. (2004a) Bayesian model comparison of nonlinear structural
equation models with missing continuous and ordinal categorical data. British Journal
of Mathematical and Statistical Psychology, 57, 131–150.
Lee, S. Y. and Song, X. Y. (2004b) Evaluation of Bayesian and maximum likelihood
approaches in analyzing structural equation models with small sample sizes. Multi-
variate Behavioral Research, 39, 653–686.
Lee, S. Y. and Tang, N. S. (2006a) Bayesian analysis of nonlinear structural equation
models with nonignorable missing data. Psychometrika, in press.
Lee, S. Y. and Tang, N. S. (2006b) Bayesian analysis of structural equation models with
mixed exponential family and ordered categorical data. British Journal of Mathematical
and Statistical Psychology, 59, 151–172.
Lee, S. Y., Song, X. Y. and Poon, W. Y. (2004) Comparison of approaches in estimating
interaction and quadratic effects of latent variables. Multivariate Behavioral Research,
39, 37–67.

12
1
INTRODUCTION
Marsh, H. W., Wen, Z. and Hau, K. T. (2004) Structural equation models of latent
interaction: evaluation of alternative estimation strategies and indicator construction.
Psychological Methods, 9, 275–300.
Metropolis, N. et al. (1953) Equations of state calculations by fast computing machine.
Journal of Chemical Physics, 21, 1087–1091.
Moustaki, I. (2003) A general class of latent variable methods for ordinal manifest
variables with covariate effects on the manifest and latent variables. British Journal of
Mathematical and Statistical Psychology, 56, 337–357.
Rabe-Hesketh S., Skrondal, A. and Pickles, A. (2004) Generalized multilevel structural
equation modeling. Psychometrika, 69, 167–190.
Scheines, R., Hoijtink, H. and Boomsma, A. (1999) Bayesian estimation and testing of
structural equation models. Psychometrika, 64, 37–52.
Shi, J. Q. and Lee, S. Y. (1998) Bayesian sampling-based approach for factor analysis
model with continuous and polytomous data. British Journal of Mathematical and
Statistical Psychology, 51, 233–252.
Shi, J. Q. and Lee, S. Y. (2000) Latent variable models with mixed continuous and
polytomous data. Journal of the Royal Statistical Society, Ser B, 62, 77–87.
Song, X. Y. and Lee, S. Y. (2001) Bayesian estimation and test for factor analysis
model with continuous and polytomous data in several populations. British Journal of
Mathematical and Statistical Psychology, 54, 237–263.
Song, X. Y. and Lee, S. Y. (2002a) Analysis of structural equation model with ignorable
missing continuous and polytomous data. Psychometrika, 67, 261–288.
Song, X. Y. and Lee, S. Y. (2002b) Bayesian estimation and testing for nonlinear factor
analysis in several populations. Structural Equation Modeling, 9, 523–553.
Song, X. Y. and Lee, S. Y. (2004) Bayesian analyses of two-level nonlinear structural
equation models with continuous and polytomous data. British Journal of Mathemat-
ical and Statistical Psychology, 57, 29–52.
Song, X. Y. and Lee, S. Y. (2006a) Bayesian analysis of structural equation models
with nonlinear covariates and latent variables. Multivariate Behavioral Research, 41,
337–365.
Song, X. Y. and Lee, S. Y. (2006b) Bayesian analysis of latent variable models with
nonignorable missing outcomes from exponential family. Statistics in Medicine, in
press.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. (2003) WinBugs User
Manual. Version 1.4. Cambridge, UK: MRC Biostatistics Unit.
Tanner, M. A. and Wong, W. H. (1987) The calculation of posterior distributions by
data augmentation(with discussion). Journal of the American Statistical Association,
82, 528–550.
Wall, M. M. and Amemiya, Y. (2000) Estimation for polynomial structural equation
models. Journal of the American Statistical Association, 95, 929–940.
Wedel, M. and Kamakura, W. (2001) Factor analysis with (mixed) observed and latent
variables in the exponential family. Psychometrika, 55, 515–530.
World Values, Study Group (1994) World Values Survey, 1981–1984 and 1990–
1993 (Computer file). ICPSR version. Ann Arbor, MI: Institute for Social Research
(producer), Ann Arbor, MI: Inter-university Consortium for Political and Social
Research (distributor).
Zhu, H. T. and Lee, S. Y. (2001) A Bayesian analysis of finite mixtures in the LISREL
model. Psychometrika, 66, 133–152.

2
Some Basic Structural
Equation Models
2.1
INTRODUCTION
The main objective of this chapter is to introduce some basic structural equation
models (SEMs). Definitions of the models will be given, their basic features
discussed and the possible improvements that can be achieved through the
Bayesian approach will be pointed out. The covariance structure approach for
analyzing these models will be discussed in Chapter 3. The Bayesian approach,
which has several key advantages over the covariance structure approach, will
be discussed in Chapters 4 to 13.
Historically, factor analysis is the most basic SEM which was developed by
psychometricians (e.g. Spearman, 1904; Thurstone, 1944) to study internal
relationships of a set of variables. Its primary concern was on hypotheses testing
about the organization of mental ability. Nowadays, the model has been applied
to a much wider range of situations, for example, analyzing sets of economics
quantities, sets of tests of attitudes and behaviors, and sets of physical measure-
ments, etc..
The basic motivation for developing exploratory factor analysis is that for a
given set of response variables one wants to search a fewer number of uncor-
related latent factors that will account for the intercorrelations of the response
variables so that when the latent factors are partialled out from the response
variables there no longer remain any correlations between them. It can be
regarded as an exploratory data analysis method at the initial stage of the devel-
opment of SEM. Starting with a given data set, the relating statistical problems
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

14
2
SOME BASIC STRUCTURAL EQUATION MODELS
are determining the number of factors for the structure, estimating the param-
eters and achieving meaningful interpretations of the factors via appropriate
rotations of the factor loading matrix. All these can be viewed as exploratory
data analysis for a better understanding of the given data. Development of
the confirmatory factor analysis was made by Jöreskog (1969). In this model,
the latent factors are allowed to be correlated. Moreover, any values may be
preassigned in advance for any number of specified factor loadings, factor corre-
lations and unique variances. Typical application of the model is on confirma-
tory studies, where the experimenter has already obtained a certain amount of
knowledge about the model from the substantive theory or exploratory anal-
ysis, and is in a position to formulate a more precise confirmatory factor model
with a given number of latent factors. Usually, the model is identified by the
fixed parameters, the resulting solution will be directly interpretable, and the
subsequent rotation of the factor loading matrix is not necessary. In the anal-
ysis, the unspecified parameters are estimated conditionally on the preassigned
values of the specified parameters. Goodness-of-fit of the proposed model to
the data is usually assessed via a discrepancy function that measures the differ-
ence between the estimated and the observed covariance matrices. It can be
used to ‘confirm’ the results obtained from exploratory analysis of the model.
Moreover, the flexibility of fixing and freeing of parameters at will provides a
powerful way for establishing plausible models for substantive theory in the real
world, and inspires the later stage development of SEMs. Shortly after solving
the fundamental problems of the confirmatory factor analysis model, Jöreskog
(1970) developed the second-order factor model. This model was further
generalized to higher-order factor models, and higher-order moment structures
by Bentler (1976, 1983).
As pointed out by Bentler (1983), the most exciting development in structural
equation modeling has been the integration of the confirmatory factor analysis
model and the simultaneous equation model, achieved by Ward Keesling, David
Wiley and Karl Jöreskog (Jöreskog, 1977; Wiley, 1973). The several genera-
tions of the computer program LISREL (Jöreskog and Sörbom, 1983–1996)
have had a tremendous impact on the early stage development of SEMs. The
LISREL model is defined by a structural equation which is essentially a set
of simultaneous linear equations with latent variables, and two measurement
models that relate the latent variables to the manifest variables via confirma-
tory factor analysis models. One of its key features is the distinction between
latent and manifest variables. A model on the basis of a generic linear struc-
tural matrix equation with no such distinction has been developed by Bentler
and Weeks (1980). The well-known EQS6 (Bentler and Wu, 2002) program
is based on this model. Nowadays, there are a number of other programs in
the field such as AMOS, CALIS, COSAN, LINCS, MECOSA, MPLUS, PLS,
RAM, RAMONA, SEPATH, STREAMS and TETRAD II, among others. In

2.2
EXPLORATORY FACTOR ANALYSIS
15
order to conserve space, the underlying models of these programs, and other
classical models in the field such as the multi-mode models (Tucker, 1966;
Bentler and Lee, 1979) and moment structure model (Bentler, 1983), will
not be discussed. Also the controversy criticisms about SEMs, such as those
related to causality, the use of latent variables, etc., are not specifically discussed
because they have been well addressed by Bollen (1989). Moreover, some of
the criticisms, for example about the falsifiability of the models and the distri-
butional assumptions, would be alleviated by the Bayesian model comparison,
and the generality of the more subtle SEMs that are less reliant on the normality
assumption of the manifest variables.
Owing to the strong demand for more general models to cope with the
complicated real life practical problems, a number of useful generalizations of the
basic SEMs have been proposed. The most important representatives are models
with ordered categorical variables, dichotomous variables, nonlinear models,
multilevel models, multisample models, mixtures models, models with missing
data and models with the variables from the exponential family of distribution.
These models will be discussed in subsequent chapters of this book.
In this chapter, the standard exploratory and confirmatory factor analysis
models are discussed, as are the higher-order factor analysis models, the LISREL
model, and the Bentler and Weeks (1980) model that are related to the well-
known LISREL program and the EQS program, respectively.
2.2
EXPLORATORY FACTOR ANALYSIS
Exploratory factor analysis (EFA) is a basic model and has received a lot of
attention in the field for many years. This section makes no attempt to give a
complete treatment of the subject. Comprehensive discussions may be found in
Lawley and Maxwell (1971) or Mulaik (1972).
2.2.1
Model Deﬁnition
The exploratory factor analysis model is defined by a p × 1 random vector x
that satisfies the following linear equation:
x =  +
(2.1)
where p × q  is a matrix of factor loadings, q × 1 is a random vector of
latent common factors and p ×1 is a random vector of error measurements
(sometimes called latent unique factors or residuals). It is assumed that  is
distributed as N 0I, and  is distributed as N 0, where  is a diagonal

16
2
SOME BASIC STRUCTURAL EQUATION MODELS
matrix, and  is uncorrelated with . Generally, q is much smaller than p. The
manifest random vector x is distributed as N 0, where
 = T +
(2.2)
Clearly, covx = . Hence, the correlations between the latent factors and
the manifest variables are given by the elements in the factor loading matrix .
The variance of the kth manifest variable is equal to
	kk = 
2
k1 +···+
2
kq +k
where 
kh and k are the (k,h)th element of  and the kth elements of ,
respectively. The quantity 
2
k1 +···+
2
kq is called the communality which repre-
sents the variance contributed by the latent factors.
In principal component analysis, a set of p variables is transformed linearly
and orthogonally to an equal number of new uncorrelated variables, such that
the total variance is unchanged. In contrast to principal component analysis,
the EFA model explains most of the dependence structure by a much smaller
number of common factors with the residuals accounted for by the unique
factors. It can also be viewed as a linear regression model in which a vector of
manifest variables is regressed on a smaller dimensional random vector of latent
factors. In practical applications, EFA has been widely used to group together
manifest variables that are related to a particular latent factor for the purpose of
data reduction.
2.2.2
Identiﬁcation and Analysis of the Model
Suppose that a random sample x1   xn of x has been obtained. We define
the sample covariance matrix by
S = n −1−1
n
i=1
xi −xxi −xT 
where x is the sample mean. The sample covariance matrix is an unbiased
estimator of , and n −1S is distributed as Wpn −1, a p-dimensional
Wishart distribution with n −1 degrees of freedom (see Anderson, 1984). In
practice, unknown elements in the parameter matrices  and  are estimated
from S. Before estimation, we have to consider the identification of the model.
If q = 1, then  is unique apart from a plausible change of sign of all its
elements, which corresponds to changing the sign of the factors. When q > 1,
there is an infinity of choices for , because Equations (2.1) and (2.2) are still

2.2
EXPLORATORY FACTOR ANALYSIS
17
satisfied if  is replaced by A and  by AT , where A is any orthogonal
matrix of rank q. This factor indeterminacy problem may be solved by imposing
identification conditions on the parameters (see Anderson and Rubin, 1956;
Jöreskog, 1967; and Jennrich and Robinson, 1969). Some examples of the
identification conditions are: (i) T −1 is diagonal; (ii) T  −1
  is diagonal;
(iii) T S−1 is diagonal; or (iv) 
kh = 0 for all h = 2   q and k < h. An
example of q = 3 corresponding to (iv) is given by
⎡
⎢⎢⎢⎢⎢⎣

11
0
0

21

22
0

31

32

33




p1

p2

p3
⎤
⎥⎥⎥⎥⎥⎦

See Anderson and Rubin (1956) for more theoretical discussion and examples
on the identification conditions.
For a proposed model with a specified fixed number of factors, estimates of
the parameters subject to an appropriate identification condition are obtained
by minimizing an objective function. Since the solution is usually not in closed
form, estimates are obtained via some iterative procedures. Procedures for
obtaining the ML estimates of  and  that subject to (ii) and (iii) were devel-
oped by Jöreskog (1967) and Jennrich and Robinson (1969) respectively. ML
estimates subject to (iv) can be obtained by the scoring algorithm or the Gauss
Newton algorithm that will be discussed in Chapter 3. It should be remem-
bered that ML estimates of the parameters that are obtained on the basis of
different identification conditions are equivalent. In practice, the estimate of
 is usually rotated by some methods such as the varimax rotation (see e.g.
Lawley and Maxwell, 1971) for better interpretations.
To apply the exploratory factor analysis model for finding the dependence
structure of the manifest variables, selecting an appropriate number of latent
factors is an important issue. Hence, a basic statistical problem is on testing
the hypothesis concerning q. Consider the following negative log-likelihood
function on the basis of the information provided by S:
L ˜ ˜ = log  ˜ ˜
T + ˜+trS ˜ ˜
T + ˜−1−log S−p
where  ˜ ˜ are the ML estimates of . The likelihood ratio criterion is
given by nL ˜ ˜. Under the null hypothesis that the true model has q latent
factors, the asymptotic distribution of this statistic is 2
df , where the ‘degrees
of freedom’ df is equal to the number of parameters in a general symmetric

18
2
SOME BASIC STRUCTURAL EQUATION MODELS
covariance matrix minus the number of unknown parameters in the proposed
model (see Chapter 3, Section 3.2). Hence,
df = pp +1
2
−
	
pq +p−qq −1
2

= 1
2

p −q2 −p +q


The proposed model with q latent factors is rejected at a chosen type I error
level if nL ˜ ˜ is larger than the corresponding critical value that is obtained
from the chi-square table.
In practice, a sequential procedure is usually applied to decide the number
of latent factors in fitting the data. Starting with a small q1, say q1 = 1, one can
estimate the parameters and perform the above goodness-of-fit test. If the test
criterion is not significant at the chosen level, the model with q1 latent factors
can be regarded as a plausible model for the data; otherwise the procedure
continues with q1 +1. However, the above procedure is open to the following
objections: (i) the significance level for the test criterion has not been adjusted
to take into account that a sequence of hypotheses is being tested, with each
one dependent on the rejection of all the previous tests; (ii) even if the null
hypothesis is not rejected, it does not provide supportive evidence for the model
specified by the null hypothesis; (iii) the null hypothesis is usually rejected in
situations with huge sample sizes. A better Bayesian approach for selecting q
is via the Bayes factor (see Berger, 1985), which will be discussed in detail in
Chapter 5, Section 5.2, and Chapter 6, Section 6.5.
More detailed discussions on the estimation, such as the asymptotic distribu-
tions of the estimators and the iterative procedures for achieving the solution,
are left until Chapter 3 under the general framework of the traditional covari-
ance structure analysis approach.
2.3
CONFIRMATORY AND HIGHER-ORDER FACTOR
ANALYSIS MODELS
2.3.1
Conﬁrmatory Factor Analysis Model
The confirmatory factor analysis (CFA) model is a natural extension of the EFA
model. It is defined by
x =  +
(2.3)
where the definitions of x, and , and the distributional assumption of 
are the same as in an EFA model. In a CFA model, the latent common factors
are allowed to be correlated so that  is distributed as N 0 with a positive

2.3
CONFIRMATORY AND HIGHER-ORDER FACTOR ANALYSIS
19
definite covariance matrix . Under the assumption that  is independent with
, the covariance matrix of x is
 = T + 
(2.4)
In this model, elements of  and   are allowed to be fixed at preassigned
values. The positions of the fixed parameters and their preassigned values repre-
sent part of the hypotheses that the investigator wishes to test about the model.
For example, 
kh = 0 means that the hth common factor does not enter into
the kth manifest (response) variable, and jh = 0 means the jth common factor
is uncorrelated with the hth factor. The proposed model involves specifications
of the number of latent factors, as well as the positions and preassigned values
of the fixed parameters. In almost all practical applications, the proposed CFA
model is identified by the fixed parameters in  and  at appropriate preas-
signed values. Estimates of the unknown parameters are unique, and hence the
factor rotation is not relevant in this situation.
In EFA analysis, we are given a data set corresponding to a number of manifest
variables, and we wish to group together some correlated manifest variables to a
latent factor which hopefully can be interpreted as a meaningful latent construct
(via factor rotation). This is an exploratory data-driven procedure that does not
depend on any prior knowledge of the manifest variables. On the other hand,
most CFA applications are confirmatory. From the subject-matter knowledge,
we already have in mind some correlated manifest variables that would be
grouped to a latent construct (factor) of interest. Typical examples of the latent
constructs are the job attitude and satisfaction in organization and management
research, consumer satisfaction and emotion in marketing research, physical and
mental health in quality of life research, etc.. More specifically, in the WHOQOL
quality of life instrument (Power, Bullingen, Hazper and WHOQOL Group,
1999), investigators used items such as ‘pain’, ‘medication’, ‘energy and fatigue’,
‘mobility’, ‘sleep and rest’, ‘daily activities’ and ‘work capacity’ to reflect the
latent construct about ‘physical health’. In general, as the latent constructs and
the corresponding items can be naturally regarded as latent factors and the
related manifest variables, the CFA is a logical confirmatory statistical model for
substantive research that involves manifest and latent constructs.
The CFA model has been applied widely to many substantive problems.
As an example to illustrate the flexibility of the CFA model, we now present
its application to analyze the multitrait–multimethod model that is useful for
assessing the convergent and discriminant validity of latent constructs. Consider
a situation with t≥3 traits which are measured by m≥3 methods. Let xjk
represent an observed variable that corresponds to the jth trait measured by the
kth method. The multitrait – multimethod model is given as
xjk = jkTj +jkMk +jk
j = 1   t  k = 1   m

20
2
SOME BASIC STRUCTURAL EQUATION MODELS
where Tj and Mk represent the jth trait and the kth method, respectively,
jk and jk are unknown coefficients and jk is the residual. This model can be
expressed as a CFA model as follows:
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x11

x1m
x21

x2m

xt1

xtm
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
11
0 ··· 0 11
 0




 

1m
0 ··· 0
0 ··· 1m
0
21 ··· 0 21 ··· 0




 

0 2m ··· 0
0 ··· 2m







0
0 ··· t1 t1 ··· 0







0
0 ··· tm 0 ··· tm
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
T 1

Tt
M 1

Mm
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
+
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
11

1m
21

2m

t1

tm
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦

where the zeros are fixed parameters. The unknown parameters in the loading
matrix are those unknown coefficients jk and jk. Correlations among traits,
methods, and traits and methods are represented by the elements in , the
correlation matrix of the vector of latent factors T 1   TtM 1   MmT .
Similar to many other applications of the CFA model, the clear associations
of the latent factors and their indicators suggest a specific form of the loading
matrix with appropriate elements fixed at zero.
In general, it can be seen from Equation (2.4) that all unknown parame-
ters in the model are involved in the covariance matrix . Hence, an approach
for developing statistical theory is based on the framework of covariance struc-
ture analysis. The covariance matrix 	 is regarded as a matrix function of
the parameter vector 	 which contain the unknown parameters 	, an objec-
tive function f	S which measures the discrepancy between the proposed
covariance matrix and the sample covariance matrix S is minimized. Commonly
used objective functions are the negative log-likelihood function and the gener-
alized least square function. The goodness-of-fit of the proposed model is
assessed by nf˜	S at the estimate ˜	 of 	 (see Chapter 3 for more detailed
discussions in the framework of covariance structure analysis).
2.3.2
Estimation of Factor Scores
In addition to the estimation of the structural parameters in the underlying
covariance matrix, a primary interest is on estimating the random vector  of
latent factor scores.

2.3
CONFIRMATORY AND HIGHER-ORDER FACTOR ANALYSIS
21
The regression method is based on the following joint distribution of x and :
x


D= N
	0
0


T +

T



Regressing  on x, we have
 = T T +−1x = T −1x
(2.5)
For EFA,  in Equation (2.5) is taken to be the identity matrix. Another
method is to minimize the sum of squares of the standardized residuals, that is
x −T  −1
 x −
For both the CFA and EFA models, the solution is
 = T  −1
 −1T  −1
 x
(2.6)
Estimates of  that are obtained by the above methods depend on the true
parameter matrices. Since these matrices are unknown in practice, they are
replaced by estimates that are obtained from some estimation procedures. From
Equations (2.5) and (2.6), the estimates of  corresponding to an individual x
are
˜ = ˆ ˆT ˆ−1x
(2.7)
or
˜ =  ˆT ˆ −1

ˆ−1 ˆT ˆ −1
 x
(2.8)
These methods are easy to apply. However, the factor score estimates obtained
are rarely used in practice, probably due to the following deficiencies of these
methods: (i) the sampling errors are ignored, and (ii) as ˜ is a nonlinear function
of the parameter estimates, its distribution could be very complicated. Hence, it
is rather difficult to use these estimates for further rigorous statistical analyses.
A better Bayesian method that does not have these deficiencies will be given in
Chapter 4.

22
2
SOME BASIC STRUCTURAL EQUATION MODELS
2.3.3
Higher-order Factor Analysis Model
The CFA model can be generalized to a higher-order factor analysis model in
a natural way. The second-order model is given by
x = B ++
 = B +B +

(2.9)
where B and  are factor loading matrices,  is a random vector of latent
common factors, and  and 
 are residuals. Random vectors  and 
 are
independently distributed as N 0N 0 and N 0, respectively,
where   and  are diagonal matrices. The covariance matrix of x is equal to
	 = BT +BT +
(2.10)
According to Jöreskog (1970), elements of the parameter vector 	 are of three
kinds: (i) fixed parameters that have been assigned given values, (ii) constrained
parameters that are unknown but equal to one or more other parameters, and
(iii) free unknown parameters. By utilizing the flexibility of fixing arbitrary
parameters at given values at will, Jöreskog (1970) showed this model can be
applied to a number of statistical models, such as test theory models, simplex
models and models for several sets of congeneric test scores.
Higher-order factor analysis model can be defined as:
x = 12 ···kk +···+122 +11
(2.11)
where 1   k are loading matrices, and 1   k are independently
distributed random vectors which may be latent factors or error measurements.
The covariance matrix of x is given by
 = 1 ···kkT
k ···T
1 +···+122T
2T
1 +11T
1
(2.12)
where 1   k are covariance matrices of 1   k, respectively. Param-
eters in 1 ,   k and 1   k are allowed to be fixed at any given
values.
2.4
THE LISREL MODEL
In the CFA model, correlations among latent variables can be assessed by their
covariance matrix; however, latent variables are never regressed on the other

2.4
THE LISREL MODEL
23
variables. The basic goal of the development of SEMs is to generalize the CFA
model for assessing how latent variables affect each other in various ways. A
typical representation of SEM is the LISREL model, which consists of two major
components, namely the measurement equations and the structural equation.
The measurement equations are defined by the following CFA models:
x1 = 1+1
(2.13)
x2 = 2 +2
(2.14)
where x1r ×1 and x2s ×1 are random vectors of the manifest variables which
are the respective indicators for  and , 1r ×q1 and 2s ×q2 are loading
matrices, 1r ×1 and 2s ×1 are random vectors of error measurements. It is
assumed that 1 and 2 are uncorrelated with  and 
, and the distributions
of these random vectors are normal with zero means. Given the observed data
in manifest random vectors x1 and x2, the measurement equations appropriately
group together the correlated manifest variables to form latent variables in  and
. This is done by assigning fixed parameters and defining unknown parameters
in 1 and 2.
The structural equation, which specifies relationships among the identified
latent variables, is defined by the equation:
 = + +

(2.15)
where q1×1 is an endogenous random vector of latent variables and q2×1
is an exogenous random vector of latent variables, q1 × q1 and q1 × q2
are unknown matrices of regression coefficients that represent the causal effects
among  and , and 
q1 × 1 is a random vector of error measurements or
residuals. It is assumed that I − is nonsingular,  is uncorrelated with 
,
and the means of these random vectors are zero.
Let   1 and  2 be the covariance matrices of 
1 and 2, respec-
tively, the covariance matrix of xT
1 xT
2  is
 =
⎡
⎣
1I−−1T +I−−T T
1 +1
1I−−1T
2
2T I−−T T
1
2T
2 + 2
⎤
⎦
(2.16)
The elements of  are functions of the parameter matrices 12
  1, and  2. The elements of these parameter matrices are of three kinds:
(i) fixed parameters that have been assigned at given values, (ii) constrained
unknown parameters that are equal to one or more other parameters, and (iii)
free unconstrained parameters. The flexibility of allowing arbitrary parameters

24
2
SOME BASIC STRUCTURAL EQUATION MODELS
at any given values gives high power for the LISREL model to subsume many
useful models. Clearly, the EFA and CFA models are its special cases. Let
 = 0 and substitute the resulting Equation (2.15) to (2.13), we see that the
second-order factor analysis model is also its special case. If we assume 1 = I
2 = I1 = 0, and 2 = 0, then Equation (2.15) becomes
x1 = x2 +

which is the simultaneous equation model in econometrics.
Like applications of many statistical models, applications of the LISREL
model or SEMs rely on substantive knowledge to build a model. It is a confir-
matory tool rather than an exploratory tool. In practical applications, we usually
have a clear objective of the study, and some basic background about the key
structure of the model that is obtained either from the subject knowledge or
from preliminary data analysis. This basic background will be used in both
model specification and interpretation.
The LISREL as defined in Equations (2.13), (2.14) and (2.15) specified the
key number q1, and q2. The choice of these numbers is important in formulating
the model and in interpretation of the statistical results. In substantive applica-
tions, the basic background of the given p manifest variables in y together with
the sample correlation matrix of these variables usually give good choices of q1
and q2. To give a simple illustrative example in medical research, suppose we
are interested in studying the effects of blood pressure and obesity on kidney
failure which is assessed by urinary albumin creatinine ratio (ACR) and plasma
creatinine (PCr). In addition to ACR and PCr, suppose the other manifest vari-
ables are: stystolic blood pressure (SBP), diastolic blood pressure (DBP), body
mass index (BMI), hip index (HIP), and waist index (WST). From the medical
knowledge of these variables, the following groupings are clear. (i) The manifest
variables ACR and PCr provide key information about kidney disease severity;
hence, they are grouped together to form the endogenous latent variable that
can be interpreted as ‘kidney disease severity, ’. (ii) Based on the medical
knowledge and/or information from sample correlation matrix, SBPDBP
are taken to be the manifest variables of the latent variable that can be inter-
preted as ‘blood pressure, 1’. (iii) Similarly, BMIHIPWST are grouped
together in a latent variable that can be interpreted as ‘obesity, 2’. Hence, it
is natural to group the seven manifest variables into three latent variables, and
take q1 = 1 and q2 = 2. Let
1 =
	10∗

121

and 2 =
⎡
⎢⎢⎢⎢⎣
10∗
0∗

221
0∗
0∗
10∗
0∗

242
0∗

252
⎤
⎥⎥⎥⎥⎦


2.4
THE LISREL MODEL
25
where parameters with asterisks are known parameters which are fixed at the
preassigned values. This notation will be used throughout this book. According
to the usual practice of factor analysis, the fixed value of 1.0 is used to specify
the ‘scale’ of the unknown parameters (factor loadings) in its column. The non-
overlapping factor loading matrix 2 is taken because SBP and DBP are clear
indicators of ‘blood pressure, 1’, whilst BMI, HIP and WST are not; and BMI,
HIP and WST are clear indicators of ‘obesity, 2’, whilst SBP and DBP are not.
Obviously, ACR and PCr are the manifest variables related to . Hence the
measurement equations are equal to:
x1 =
	10∗

121

+
	11
12

(2.17)
x2 =
⎡
⎢⎢⎢⎢⎣
10∗
0∗

221
0∗
0∗
10∗
0∗

242
0∗

252
⎤
⎥⎥⎥⎥⎦
	1
2

+
⎡
⎢⎢⎢⎢⎣
21
22
23
24
25
⎤
⎥⎥⎥⎥⎦

(2.18)
The latent variables are related by the following structural equation:
 = 11 +22 +
(2.19)
hence,  = 0 and  = 1 2. The covariance matrices of 12T , 1112T
and 21   25T are respectively equal to
 =
	11 12
21 22

 1 = diag1112and  2 = diag21   25
To simplify the notation, we consider the following equivalent formulation
of the LISREL model. Let v = xT
1 xT
2 T  = T T T and  = T
1 T
2 T , so
that Equations (2.13) and (2.14) can be expressed as
v =
x1
x2

=
1 0
0 2



+
1
2

= +
(2.20)
In all applications of SEMs, the form of  is given by Equation (2.20) with
the zero matrices at the specified positions. The parameter matrix 2 could be
non-overlapping. Clearly, Equation (2.20) is equivalent to Equations (2.13)
and (2.14). Hence, the LISREL model can be formulated as a CFA model as

26
2
SOME BASIC STRUCTURAL EQUATION MODELS
defined in Equation (2.20), and the following linear structural equation that is
equivalent to Equation (2.15):
 =   



+
 = +

(2.21)
where  =  .
2.5
THE BENTLER–WEEKS MODEL
The Bentler–Weeks model (Bentler and Weeks, 1980) is a combination of a
structural equation model and a selection model. The structural equation model
is defined as
 = +
(2.22)
where q1 × 1 is a random vector of dependent variables, q2 × 1 is a
random vector of independent variables, q1 ×q1 and q1 ×q2 are matrices
of regression coefficients. It is assumed that I− is nonsingular. In general,
the parameters involved in Equation (2.22) are of the three kinds that are similar
to those described in the LISREL model. This equation relates all the manifest
and latent variables under consideration. A variable is included in  if it is ever
considered to be a dependent variable in any structural equation; otherwise it is
included in  and considered as an independent variable in the model. Thus, a
key concept involves the designation of each and every variable in the system as
either a dependent or an independent variable. The random vector  consists
of all manifest dependent variables and certain latent variables such as common
factors of any level and residuals. The random vector  contains the manifest
and latent variables that are not structural functions of other manifest or latent
variables. Manifest variables in  and  are represented by x and y, which are
extracted via the selection model:
x = Gx y = Gy
(2.23)
where Gx and Gy are selection matrices with entries 1.0 or 0.0. Let  be the
covariance matrix of , the covariance matrix of xT yT T is equal to
 =
⎡
⎣
GxI−−1T I−−T Gx
T
GxI−−1GT
y
GyT I−−T GT
x
GyGT
y
⎤
⎦
(2.24)

2.6
DISCUSSION
27
This covariance matrix can be more simply represented as
 = GoI−o−1oT
o I−o−T GT
o 
(2.25)
where
Go =
	Gx
0
0
Gy

o =
	
0
0
0

and o =
	
I

There are only three parameter matrices in this simple representation, namely
Go, o and o.
Comparing covariance matrices given in Equations (2.16) and (2.24), it can
be seen that the Bentler–Weeks model is a special case of the LISREL model.
Conversely, let Equation (2.22) be expressed by the following block vectors
and block matrices.
⎡
⎣
x1
x2

⎤
⎦=
⎡
⎣
0
0
1
0
0
0
0
0

⎤
⎦
⎡
⎣
x1
x2

⎤
⎦+
⎡
⎣
0
I
0
0
2
0
I
0

0
0
I
⎤
⎦
⎡
⎢⎢⎣

1
2

⎤
⎥⎥⎦
Note that the covariance matrix of T T
1 T
2 
T  is
⎡
⎢⎢⎣

0
0
0
0
1
0
0
0
0
2
0
0
0
0

⎤
⎥⎥⎦
Using the LISREL notation as in Equations (2.13), (2.14) and (2.15), the
LISREL model can be regarded as a special case of the Bentler–Weeks model.
This shows that the Bentler–Weeks model is equivalent to the LISREL model.
Thus, it has the same power of practical applicability as the LISREL model. See
Bentler and Weeks (1980) for more discussion on the comparison with other
models on various theoretical and practical aspects. The EQS6 program (Bentler
and Wu, 2002) is written on the basis of this model.
2.6
DISCUSSION
This chapter presents the most basic SEMs, including the EFA, CFA, LISREL
and the Bentler–Weeks models. These SEMs have one common characteristic,

28
2
SOME BASIC STRUCTURAL EQUATION MODELS
namely the covariance matrices contain the underlying unknown parameters of
the models. Under the normality assumption, the ML or the generalized least
squares theory for analyzing these models can be derived through a covariance
structure analysis approach, on the basis of the sample covariance matrix. As
outputs of almost all common software in SEMs provide the ML and the GLS
solutions, the asymptotic properties of these two approaches are important and
will be discussed on Chapter 3.
In the real world, there are a large number of practical problems that cannot
be handled by these standard SEMs. Hence, there is a strong demand for gener-
alizations of the standard SEMs to cope with more complicated substantive
theory and complex data sets. A number of important generalizations will be
discussed in subsequent chapters, based on the Bayesian approach. For easier
understanding of the model, better interpretation of the results and conve-
niences in developing the Bayesian methodologies, these generalizations are
basically developed on the basis of LISREL type models with integral compo-
nents that are basically defined by Equations (2.20) and (2.21).
REFERENCES
Anderson, T. W. (1984) An Introduction to Multivariate Statistical Analysis (2nd edn).
New York: John Wiley & Sons, Inc.
Anderson, T. W. and Rubin, H. (1956) Statistical inference in factor analysis. Proceedings
of the Third Berkeley Symposium on Mathematical Statistics and Probability. 110–150.
Berkeley: University of California Press.
Bentler, P. M. (1976) Multistructural statistical models applied to factor analysis. Multi-
variate Behavioral Research, 11, 3–25.
Bentler, P. M. (1983) Some contributions to efficient statistics for structural models:
specification and estimation of moment structures. Psychometrika, 48, 493–517.
Bentler, P. M. and Lee, S. Y. (1979) A statistical development of three-mode factor
analysis. British Journal of Mathematical and Statistical Psychology, 32, 87–104.
Bentler, P. M. and Weeks, D. G. (1980) Linear structural equations with latent variables.
Psychometrika, 45, 289–308.
Bentler, P. M. and Wu, E. J. C (2002) EQS6 for Windows User Guide. Enciuo, CA.:
Multivariate Software, Inc.
Berger, J. O. (1985) Statistical Decision Theory and Bayesian Analysis. New York:
Springer-Verlag.
Bollen, K. A. (1989) Structural Equations with Latent Variables. New York: John Wiley
& Sons, Inc.
Jennrich R. I. and Robinson, S. M. (1969) A Newton–Raphson algorithm for maximum
likelihood. factor analysis. Psychometrika, 34, 111–123.
Jöreskog, K. G. (1967) Some contributions to maximum likelihood factor analysis.
Psychometrika, 32, 443–482.
Jöreskog, K. G. (1969) A general approach to confirmatory maximum likelihood factor
analysis. Psychometrika, 34, 183–202.
Jöreskog, K. G. (1970) A general method for analysis of covariance structures.
Biometrika, 57, 239–251.

REFERENCES
29
Jöreskog, K. G. (1977) Factor analysis by least-squares and maximum likelihood
methods. In K. Enslein, A. Ralston and H. S. Wilf (eds), Statistical Methods for Digital
Computers. New York: John Wiley & Sons, Inc..
Jöreskog, K. G. and Sörbom, D. (1994) LISREL VIII: A Guide to the Program and
Applications, Chicago, IL: SPSS, Inc..
Jöreskog, K. G. and Sörbom, D. (1996) LISREL 8: Structural Equation Modeling with the
SIMPLIS Command Language. Hove and London: Scientific Software International.
Lawley, D. N. and Maxwell, A. E. (1971) Factor Analysis as a Statistical Method (2nd
edn). New York: Elsevier.
Mulaik, S. A. (1972). The Foundations of Factor Analysis. New York: McGraw-Hill Book
Company.
Power, M., Bullingen, M., Hazper, A. and WHOQOL Group (1999) The World Health
Organization WHOQOL-100: tests of the universality of quality of life in 15 different
cultural groups worldwide. Health Psychology, 18, 495–505.
Spearman, C. (1904) General intelligence objectively determined and measured.
American Journal of Psychology, 15, 201–293.
Thurstone, L. L. (1944) A multiple group method of factoring the correlation matrix.
Psychometrika, 10, 73–38.
Tucker,
L.
R.
(1966)
Some
mathematical
notes
on
three-mode
factor
anal-
ysis.Psychometrika, 31, 279–311.
Wiley, D. (1973) The identification problem for structural equation models with unmea-
sured variables. In A. S. Goldberger and O. D. Duncan (eds), Structural Equation
Models in the Social Sciences pp. 69–83. New York: Academic Press.


3
Covariance Structure
Analysis
3.1
INTRODUCTION
Structural equation models (SEMs) are typically formulated with random
observed variables, latent variables and error measurements. Although the model
equations are very similar to regression equations, common statistical methods
in analyzing the regression model that are derived on the basis of the raw
observations cannot be applied to SEMs, mainly because of the presence of the
latent variables. For example, consider the following simple confirmatory factor
analysis (CFA) model:
xi = i +i
i = 1   n
where i is a vector of latent variables (factor scores) with a distribution
N 0 , and  and i are similarly defined as in Chapter 2, Section 2.3.
In this model, i is not observed as in a regression model. Consequently, the
generalized least square or maximum likelihood methods in regression cannot
be applied. However, if i is given, the model becomes a regression model and
the resulting analysis is much simpler.
It is noted that for all the standard SEMs presented in Chapter 2, the parame-
ters involved in the model are contained in the covariance matrix of the observed
variables. For example, in CFA,  = T +  and the unknown param-
eters are those free elements in  and  . Hence, the covariance matrix
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

32
3
COVARIANCE STRUCTURE ANALYSIS
plays an important role, and the basic statistical analysis can be regarded as the
covariance structure analysis (CSA).
In the general CSA, the covariance matrix of the observed random vector
of the hypothesized model is formulated as a matrix function of the unknown
parameters vector , say . Since the sample covariance matrix S is an
unbiased estimator of the true covariance matrix, one approach in developing
statistical methods to analyze  is based on S. Let f S be an objective
function which measures the discrepancy between  and S. A method for
estimating  is by finding the value of  that minimizes f S. Moreover,
if the final function value evaluated at the estimate of  is large, then one
may conclude that  does not agree with the data, and hence the proposed
model is rejected. Otherwise,  may be regarded as a plausible model for the
given data. The degree of the ‘largeness’ for rejecting a hypothesized model is
assessed by a goodness-of-fit statistic. After achieving a plausible model, further
statistical analyses are conducted through the asymptotic distribution of the
estimator.
The existing software in the field of SEM are mainly developed on the basis
of the LISREL model, the Bentler and Weeks model, or their equivalent forms.
Usually, the outputs give same basic statistics, such as the estimates of the
unknown parameters, their standard error estimates, and the p-value of a chi-
square goodness-of-fit test. The asymptotic properties of these statistics are
important in conducting statistical inferences and obtaining correct interpreta-
tions of the results. One objective of this chapter is to derive the asymptotic
properties that are associated with the special functions of f S that give
the well-known maximum likelihood (ML) and generalized least squares (GLS)
approaches.
Except for very special cases, the minimum of f S cannot be obtained
in closed form, hence some iterative procedure is required. Historically, the
search for an efficient algorithm for obtaining the ML estimate of parameters
in the EFA model was the focus of attention in the 1960s. The major break-
through came from the effort of Karl Jöreskog who successfully implemented
the Fletcher–Powell (FP) algorithm to get the ML solution. Eventually, this
algorithm became the major computing tool in the LISREL (Jöreskog and
Sörbom, 1996) program. Another program EQS6 (Bentler and Wu, 2002)
applies the Gauss–Newton (GN) algorithm for getting the generalized least
squares (GLS) estimate, and a GN-type algorithm on an iteratively reweighted
least squares mode for obtaining the ML estimate (see Lee and Jennrich, 1979).
These algorithms are discussed in Section 3.6.
Inspired by the nice work of Browne (1974), we will start with a GLS
approach. Asymptotic properties of the GLS estimate, such as consistency,
asymptotic normality and the asymptotic distribution of the goodness-of-fit
test statistics, will be derived on the basis of the normal assumption and some
mild conditions on the model. Other reasons for using a GLS approach are:

3.2
DEFINITIONS, NOTATIONS AND PRELIMINARY RESULTS
33
(i) it extends naturally and conveniently to the asymptotically distribution-free
method, and (ii) the best GLS estimator is asymptotically equivalent to the ML
estimator. Because of (ii), the ML estimator has the same asymptotic properties
as the GLS estimator. These properties provide the foundation for statistical
inferences of the standard SEMs.
To derive expressions for implementing the iterative procedures and achieving
the asymptotic properties of the estimators, we need to differentiate some matrix
functions with respect to some matrix of variables. A brief outline of a matrix
calculus method is given in Appendix 3.1. Preliminary results concerning the
convergence of sequences of random variables and distribution functions are
given in Appendix 3.2. Proofs of some theoretical results are presented in
Appendix 3.3.
3.2
DEFINITIONS, NOTATIONS AND PRELIMINARY RESULTS
Let Ap ×q = akh and Br ×s be any matrices; we define the right Kronecker
product of A and B by
A ⊗B =
⎡
⎢⎢⎢⎣
a11B
a12B
··· a1qB
a21B
a22B
··· a2qB
			
			
			
ap1B
ap2B
···
apqB
⎤
⎥⎥⎥⎦	
Therefore, A ⊗B is a pr by qs block matrix with a typical khth block akhB.
When A is a scalar, this product reduces to the ordinary product of a scalar and
a matrix. This Kronecker product has the following properties.
Lemma 1
(i)
A1 ⊗A2A3 ⊗A4 = A1A3 ⊗A2A4, where Aii = 1   4 are
matrices such that A1A3 and A2A4 are well defined.
(ii)
A1 ⊗A2T = AT
1 ⊗AT
2 .
(iii)
A1 ⊗A2−1 = A−1
1 ⊗A−1
2 , where A1 and A2 are nonsingular matrices.
(iv)
If A is a positive definite matrix, then A ⊗A is positive definite.
For any matrix Ap ×q and any symmetric matrix Bp ×p, we define
vecA = a11   a1qa21   a2q   ap1   apqT
vecsB = b11b21b22b31b32b33   bp1   bppT 	

34
3
COVARIANCE STRUCTURE ANALYSIS
Hence, vecA is a pq ×1 column vector which is formed by stacking rows of
A sequentially, and vecsB is a p∗= 2−1pp + 1 by 1 column vector which is
formed by stacking the lower triangular elements of B, row by row sequentially.
We have the following additional properties of Kronecker product and the vec
operator.
Lemma 2
(i)
A1 ⊗A2vecC = vecA1CAT
2 , where A1A2 and C are matrices such that
A1CAT
2 is well defined.
(ii)
vecC1T A1 ⊗A2vecC2 = tr CT
1 A1C2AT
2  where A1A2C1 and C2 are
matrices such that CT
1 A1C2AT
2 is well defined.
It can be shown that vecsB and vecB are related by (see Browne, 1974)
vecs B = KT
p vec B
(3.1)
where the typical element of the matrix Kpp2 ×p∗ is
Kpijkh = 2−1
ik
jh +
ih
jk i ≤p j ≤p k ≤h ≤p
and 
ik represents the Kronecker’s delta such that 
ik = 1 if i = k, and 
ik = 0 if
i ̸= k. Hence, KT
p reduces vecB to vecsB which only contains the non-redundant
elements in the lower-triangular part of the synmetirc matrix. An example Kp
with p = 3 is
K3 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1

0
0

0
0
0
0

1
2
0

0
0
0
0

0
0

1
2
0
0
−−−−−−−−−−−−−−
0

1
2
0

0
0
0
0

0
1

0
0
0
0

0
0

0
1
2
0
−−−−−−−−−−−−−−
0

0
0

1
2
0
0
0

0
0

0
1
2
0
0

0
0

0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
	
The rank of Kp is p∗and its left inverse is
K−
p = KT
p Kp−1KT
p 	
(3.2)

3.2
DEFINITIONS, NOTATIONS AND PRELIMINARY RESULTS
35
This left inverse can be used to express vecB in terms of vecsB as follows,
vecB = K−
p T vecsB	
(3.3)
Another important matrix Mpp2 ×p2 is defined by
Mp = KpK−
p = KpKT
p Kp−1KT
p 	
An example of Mp with p = 3 is given by
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0

0
0
0

0
0
0
0
1
2
0

1
2
0
0

0
0
0
0
0
1
2

0
0
0

1
2
0
0
−−−−−−−−−−−−−−−−−−
0
1
2
0

1
2
0
0

0
0
0
0
0
0

0
1
0

0
0
0
0
0
0

0
0
1
2

0
1
2
0
−−−−−−−−−−−−−−−−−−
0
0
1
2

0
0
0

1
2
0
0
0
0
0

0
0
1
2

0
1
2
0
0
0
0

0
0
0

0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
	
The matrix Mp has the following properties.
Lemma 3
(i)
Mp is a symmetric idempotent matrix of rank p∗, so MpMp = Mp.
(ii)
MpKp = Kp.
(iii)
Mp vecB = vecB, for any symmetric matrix B.
(iv)
MpA ⊗A = A ⊗AMq, for any matrix Ap ×q.
More properties of the matrices KpMp and the Kronecker product can be
found in Browne (1974), and Bentler and Lee (1975).
For simplicity, we use ‘·’ over a function to denote its derivatives. Let 
be a symmetric matrix of differentiable functions of , we have ˙ = ˙ =
/ and ¨ = ¨ = 2/. From the definition of ˙, its jth row
is
˙j = 11/j   1p/j   p1/j   pp/j

36
3
COVARIANCE STRUCTURE ANALYSIS
which is equal to
˙j = vec
⎡
⎢⎢⎢⎢⎢⎣
11
j
···
1p
j
			
			
			
p1
j
···
pp
j
⎤
⎥⎥⎥⎥⎥⎦
	
Hence, it follows from Lemma 3 (iii) that
Mp ˙
T
j = ˙
T
j
and
˙j = ˙jMp	
(3.4)
This is a useful result for deriving the asymptotic distributions.
3.3
GLS ANALYSIS OF COVARIANCE STRUCTURE
3.3.1
The GLS Approach
We consider a random vector xp ×1 which has a mean vector 0 and a covari-
ance matrix 0. Suppose 0 is a matrix function of a true though unknown
parameter vector 0q ×1, so that 0 = 0. To develop statistical theory for
analyzing the covariance structure, we regard q ×1 as a vector of mathemat-
ical variables which can take the value 0 and/or any estimate of 0. Moreover,
 will be regarded as a matrix function of . In the following text, if the
context is clear, we will denote  by  to simplify notation.
The following mild regularity conditions will be assumed throughout this
chapter.
(a)
The matrix 0 is positive definite.
(b)
The vector 0 is an interior point in the parameter space, that is there exists
a neighborhood of 0 that completely lies inside the parameter space.
(c)
The model is identified, that is 0 = ∗ implies 0 = ∗.
(d)
All partial derivatives of the first three orders of  with respect to elements
of  are continuous and bounded in a neighborhood of 0.
(e)
The q by p2 matrix of partial derivatives / is of full rank in a neigh-
borhood of 0.
Condition (a) implies that the distribution of x is not degenerate. Condition (d)
implies that all elements of  are continuous functions of ; this condition and
condition (b) are necessary in applying the ‘mean-value’ theorem (see Bartle,

3.3
GLS ANALYSIS OF COVARIANCE STRUCTURE
37
1964, p. 210) in the derivation of the asymptotic properties. Condition (c) is
essential for obtaining a unique estimator of 0; iterative estimation procedures
will diverge without this condition. Condition (e) is a natural assumption which
eliminates collinearity among the elements of . Hence for each of the models
discussed in Chapter 2, appropriate identification conditions have to be imposed
to guarantee that condition (c) is satisfied. In the following, we assume that all
the models and 0 in the discussion satisfy the above regularity conditions.
Let x1   xn be a random sample of x, such that all xi are identically
and independently distributed (iid) according to N 00. Throughout this
chapter, n is assumed to be significantly larger than p∗. The sample covariance
matrix is
S = n −1−1
n
i=1
xi −xxi −xT 
where x = n−1x1 +···+xn is the sample mean. (Rigorously, some asymptotic
results should be presented with n −1; however, as n is large, the difference
is very minor.) This matrix is positive definite with probability 1, it is an unbi-
ased estimate of 0, and it also converges to 0 in probability. Moreover, it
follows from the ‘Multivariate Central Limit Theorem’ that (see Anderson,
1984 Chapter 4)
n1/2vecsS−0
L
−→N 0 2KT
p 0 ⊗0Kp
(3.5)
where ‘
L
−→’ denotes convergence in distribution (see Appendix 3.2). A key
feature of Equation (3.5) is that the covariances/variances of elements of S,
which are quadratic functions of x1   xn, can be expressed by 0, the
central second moment of x. This is the most basic result for developing the
theory of CSA under the multivariate normal assumption. An analogous result
that is obtained via Equation (3.5) is
n1/2 vecS−0 = n1/2 K−T
p
vecsS−0
L
−→N 0 2K−T
p
KT
p 0 ⊗0KpK−
p 
= N 0 2Mp0 ⊗0Mp	
(3.6)
The asymptotic distribution of S as given by Equation (3.5) motivates the
consideration of the non-linear regression model
vecsS = vecs+	
(3.7)

38
3
COVARIANCE STRUCTURE ANALYSIS
where

is
the
residual
vector
which
has
an
asymptotic
distribution
N 02n−1KT
p 0 ⊗0Kp. The definition of the GLS function is motivated
from the residual quadratic form:
vecsS−T 2n−1KT
p 0 ⊗0Kp−1vecsS−	
(3.8)
The inverse of KT
p 0 ⊗0Kp is K−
p 0 ⊗0−1K−T
p
, because from the defini-
tion and properties of Mp,
KT
p 0 ⊗0KpK−
p 0 ⊗0−1K−T
p

=
KT
p 0 ⊗0Mp0 ⊗0−1K−T
p
=
KT
p Mp0 ⊗00 ⊗0−1K−T
p
=
KT
p K−T
p
= K−
p KpT = I	
Substituting this result in (3.8), it follows from Equation (3.3) and Lemma 1
that
n2−1vecsS−T K−
p 0 ⊗0−1K−T
p
vecsS−
=
n2−1vecS−T −1
0 ⊗−1
0 vecS−	
This residual quadratic form is a function of −1
0 . As 0 is unknown, we shall
replace −1
0
by a p by p positive definite matrix V, and consider the GLS
function:
G = 2−1vecS−T V ⊗VvecS−
= 2−1 trS−V2	
(3.9)
The matrix V can be a constant positive definite matrix or a stochastic matrix
that converges to a positive definite matrix V∗. When V equals to the identity
matrix, the GLS function reduces to a least squares function.
Definition 3.1
The GLS estimator ˜ of 0 is the vector that minimizes G.
On the basis of the techniques in matrix calculus (see Appendix 3.1) the
gradient vector ˙G and the Hessian matrix ¨G of G can be derived. The
results are given by the following lemma.

3.3
GLS ANALYSIS OF COVARIANCE STRUCTURE
39
Lemma 4
i
˙G = G

= −˙V ⊗VvecS−	
(3.10)
ii
¨G = 2G
 = ˙V ⊗V ˙T −¨Iq ⊗V ⊗VvecS−
(3.11)
where Iq is a q by q identity matrix.
The proof of Lemma 4 is given in Appendix 3.3. Since G˜ is the minimum of
G ˙G˜ = 0, and ¨G˜ is positive definite. As ¨G is a continuous function,
¨G is also positive definite within a sufficiently small neighborhood of ˜.
Moreover, since S
p
−→0, where ‘
p
−→’ denotes convergence in probability, it
follows from Equation (3.11) that
¨G0
p
−→	V∗ = ˙0V∗⊗V∗ ˙0T 	
(3.12)
Since ˙0 is full rank and V∗⊗V∗ is a positive definite; 	V∗, and the
Hessian matrix ¨G0 are positive definite. These matrices are important for
deriving the asymptotic distribution of ˜.
3.3.2
Asymptotic Properties of the GLS Estimator
Asymptotic properties presented in this section are useful for analyzing covari-
ance structures. The proof of these asymptotic results is given in Appendix 3.3.
Conditions (a) – (e) are assumed throughout.
Theorem 3.1
The GLS estimator is consistent, that is ˜
p
−→0.
It follows from Theorem 3.1 that if n is sufficient large, the GLS estimator
˜ would be very close to the true parameter vector 0. This property gives
confidence in the GLS estimator. The asymptotic distribution of ˜ is given by
the next theorem.
Theorem 3.2
Let C0 = 2	V∗−1	V∗0V∗	V∗−1, where 	V∗ =
˙0V∗⊗V∗ ˙0T , then
n1/2˜ −0
L
−→N 0C0	
(3.13)

40
3
COVARIANCE STRUCTURE ANALYSIS
The asymptotic covariance matrix of ˜ is rather complicated. However, if V
is chosen such that V∗= −1
0 , the asymptotic covariance matrix of ˜ is signifi-
cantly simplified. Moreover, the resulting GLS estimator will have the minimum
asymptotic variance, in the sense as given by the following corollary.
Corollary 3.1
If V
p
−→V∗= −1
0 , then
n1/2˜ −0
L
−→N 02	−1
0 −1	
(3.14)
Moreover, C0−2	−1
0 −1 is a positive definite matrix.
Browne (1974) called the GLS estimator with V∗= −1
0
the ‘best’ GLS
estimator. Clearly, if V = S−1, then V∗= −1
0 . Hence, S−1 is a logical choice
for the weight matrix V in G .
Let
˜ = ˜ and
˜	 ˜
−1 = ˙˜ ˜
−1 ⊗˜
−1 ˙˜T . Since
˜	 ˜
−1
converges in probability to 	−1
0 , standard error estimates of elements in ˜
can be obtained via the corresponding diagonal elements of 2n−1 ˜	 ˜
−1−1.
Other statistical inference of 0, for example the asymptotic t-test or confidence
interval of an individual parameter, can be obtained from standard multivariate
methods.
The following theorem provides an asymptotic test statistic for assessing the
goodness-of-fit of the hypothesized model.
Theorem 3.3
If V∗= −1
0 , then the asymptotic distribution of nG˜ is chi-
square with p∗−q degrees of freedom, that is,
nG˜
L
−→2
p∗−q	
(3.15)
On the basis of Theorem 3.3, the proposed model  is rejected if nG˜ is
larger than 2
p∗−q, the critical value given by the chi-square table with p∗−q
degrees of freedom, at the given type I error level . The proposed model 
is not rejected if nG˜ is less than 2
p∗−q. However, this does not mean that
 can be accepted as the correct model by the theory of hypothesis testing.
In the common practice of SEM, the proposed model is regarded as a plausible
model if  is not rejected.

3.4
ML ANALYSIS OF COVARIANCE STRUCTURE
41
3.4
ML ANALYSIS OF COVARIANCE STRUCTURE
Given that the random sample x1   xn is from a multivariate normal distri-
bution N 
00, the exact distribution of the sample covariance matrix S is a
Wishart distribution with a probability density function (see Anderson, 1984),
f S0 = Cexp−n −1/2tr S−1
0
0n−1/2

where C is an appropriate normalizing constant. Hence, the negative log-
likelihood function is equal to
−log C + n −1
2
log +tr S−1
where  is treated as a vector of mathematical variables as in the GLS estimation.
In CSA, it is common to work with the following discrepancy function (see
Jöreskog, 1978; Browne, 1984),
F = log +tr S−1 −log S−p	
(3.16)
Definition 3.2
The vector ˜M that minimizes F is defined as the ML
estimate of 0.
It follows from standard statistical theory that ˜M is a consistent estimator
of 0, that is, ˜M
p
−→0. The goodness-of-fit of the proposed model can be
tested via the asymptotic distribution of the likelihood ratio criterion.
Let the null hypothesis be H0: The covariance matrix of x is given by the
hypothesized model , and let the general hypothesis be H ∗: The covari-
ance matrix of x is any arbitrary positive definite matrix. The likelihood ratio
criterion is
LR =
f S˜M 
f Sn −1n−1S
(3.17)
where n−1n−1S is the ML estimate of the covariance matrix under the general
hypothesis. It follows from the definition of f ·· that
−2logLR =−2log
	
˜M −n−1/2 exp−n −1 tr S˜M −1/2
n −1n−1S−n−1/2 exp−n −1 tr Sn −1n−1S−1/2


42
3
COVARIANCE STRUCTURE ANALYSIS
p
−→nF˜M 	
From the standard ML theory, −2logLR
L
−→2
p∗−q, hence the following
theorem is valid.
Theorem 3.4
The asymptotic distribution of nF˜M  is chi-square with p∗−q
degrees of freedom, that is
nF˜M 
L
−→2
p∗−q	
On the basis of the techniques in matrix calculus (see Appendix 3.1) the
gradient vector ˙F and the Hessian matrix ¨F can be derived. Results are
given by the following lemma.
Lemma 5
˙F =−˙−1 ⊗−1vecS−	
(3.18)
¨F =2F
 = ˙−1 ⊗−1 ˙T −2 ˙−1 ⊗−1S−−1 ˙T
−¨Iq ⊗−1 ⊗−1vecS−	
(3.19)
Similar to ¨G in the GLS estimation, ¨F is also positive definite within
a sufficiently small neighborhood of ˜M . Moreover, it follows from Equa-
tion (3.19) that
¨F˜M 
p
−→˙0−1
0 ⊗−1
0  ˙0T = 	−1
0 
(3.20)
and
E¨F0 = 	−1
0 	
(3.21)
Hence, 	−1
0  is the information matrix of F  evaluated at 0.
It follows from Equation (3.18) and (3.20), and similar arguments to those
in the proof of Theorem 3.2 that
n1/2˜M −0
p
−→	−1
0 −1 ˙0−1
0 ⊗−1
0 n1/2vecS−0	
(3.22)

3.4
ML ANALYSIS OF COVARIANCE STRUCTURE
43
From (A3.5) in Appendix 3.3, and (3.22), n1/2˜ −˜M  converges in
probability to
	V ∗−1 ˙0V∗⊗V∗−	−1
0 −1 ˙0−1
0 ⊗−1
0 n1/2 vecS−0	
Since 	V ∗−1 ˙0V∗⊗V∗−	−1
0 −1 ˙0−1
0 ⊗−1
0  converges to
zero in probability and n1/2vecS−0 converges in distribution, n1/2˜ −˜M 
converges to zero in probability. Therefore for the ‘best’ GLS estimator ˜ with
V∗= −1
0 , the following result is established.
Theorem 3.5
The ‘best’ GLS estimator and the ML estimator are asymptoti-
cally equivalent in the sense that
n1/2˜ −˜M 
p
−→0	
(3.23)
Corollary 3.2
(i)
n1/2˜M −0
L
−→N 02	−1
0 −1	
(ii)
nG˜M 
L
−→2
p∗−q	
(iii)
nF˜
L
−→2
p∗−q	
The proof of Corollary 3.2 is given in Appendix 3.3. Results in this corollary
indicate that the ML estimator and the GLS estimator are closely related.
Theoretically, we may use either the ML or the GLS estimator and its asymptotic
properties in analyzing the model.
The derivations of the asymptotic properties of the GLS and ML approaches
heavily depend on the validity of the result about the asymptotic distribution of
the sample covariance matrix as given in (3.5). Thus, to apply these asymptotic
properties in practice, we have to check whether the following related condi-
tions are satisfied: (i) the data are iid, (ii) the data are coming from a normal
distribution, and (iii) the sample size is large enough. Clearly, these properties
are invalid for data that are not identically or not independently distributed.
For some non-normal distributions, the asymptotic covariance matrix of S may
heavily depend on the fourth-order cumulant (see Section 3.5), and hence
may be very different from 2KT
p 0 ⊗0Kp. Hence, as expected, the ML and
GLS approaches are not robust to the normal assumption. Moreover, the ML
approach (or GLS approach) is not robust to small sample sizes, as demon-
strated by many simulation studies (see for example, Lee and Song, 2004). The
demand for rigorous and efficient statistical methods for analyzing the complex
non-normal data motivates the rapid growth of SEM.

44
3
COVARIANCE STRUCTURE ANALYSIS
3.5
ASYMPTOTICALLY DISTRIBUTION-FREE METHODS
To address the problem on the violation of the assumption about multivariate
normality, Browne (1984) developed an asymptotically distribution-free (ADF)
approach that does not require this assumption. The main ideas of this approach
are briefly discussed in this section.
Suppose x1   xn are iid observations that are sampled from a distribution,
not necessarily multivariate normal, with a mean vector 
, a covariance matrix
0 = 0, and finite eighth-order moments. Analysis of the covariance struc-
ture is again on the basis of the sample covariance matrix S. The finite sample
distribution of n1/2vecsS−0 has a zero mean vector and a covariance matrix
with typical element (see Kendall and Stuart, 1969, Section 13.11),
Covn −11/2Sij −0ij n −11/2Skh−0kh
=
0ik0jh+0ih0jk+n −1n−1ijkh
where ijkh is a fourth-order cumulant which is given by
ijkh = 
ijkh −0ij 0kh−0ik0jh−0ih0jk
with

ijkh = Ex i−ix j −j x k−kx h−h	
According to the multivariate central limit theorem (see Anderson, 1984,
Chapter 3),
n1/2vecsS−0
L
−→N 0∗
(3.24)
where
∗ijkh = 0ik0jh+0ih0jk+ijkh
= 
ijkh −0ij0kh	
(3.25)
The asymptotic distribution of S as given in (3.24) is similar to that given
in (3.5). The difference is on their asymptotic covariance matrices. Under the
multivariate normality assumption, the fourth-order cumulants ijkh are equal
to zero, hence ∗ijkh reduces to 0ik0jh+0ih0jk (in matrix

3.5
ASYMPTOTICALLY DISTRIBUTION-FREE METHODS
45
form, ∗= 2KT
p 0 ⊗0Kp which has the same form as given by (3.5). In
general, the convergence in distribution that is based on the general case given
in (3.24) is much slower than that in (3.5) under the multivariate normal
distributions. Similar to the normal theory, the asymptotic distribution of S as
given by (3.24) and (3.25) is very important in the development of the statistical
properties of the ADF method.
The ADF estimator ˜A of 0 under the general situation is defined as the
vector that minimizes the following GLS function
GA = 2−1vecsS−T W−1vecsS−
(3.26)
where W is a positive definite stochastic weight matrix. Here, we just consider
the W which converges to ∗in probability. A natural candidate of W is the
matrix with typical element
Wijkh = sijkh −Sij Skh
(3.27)
where
sijkh = n−1
n
t=1
xti−xi xtj −xj xtk−xkxth−xh
which is the sample fourth-order moment about the mean. This matrix is
positive definite with probability one.
Using similar arguments to those used in Section 3.3 and Appendix 3.3, it
can be shown that the following asymptotic properties relating to ˜A are valid.
Theorem 3.6
The ADF estimator ˜A of 0 has the following asymptotic
properties:
(i)
˜A is consistent, that is, ˜A
p
−→0.
(ii)
n1/2˜A −0
L
−→N 0CA0, where CA0 =  ˙0∗−1 ˙0T 
−1.
(iii)
nGA˜A
L
−→2
p∗−q.
The goodness-of-fit of the hypothesized model can be tested through (iii)
of Theorem 3.6 in a standard manner as before. Standard error estimates of
elements of ˜A may be obtained from the square roots of the corresponding
diagonal elements of n−1˜A ˜
∗−1 ˙˜AT with a consistent estimator ˜
∗of
∗. Both the LISREL and EQS6 programs have options for analyzing standard

46
3
COVARIANCE STRUCTURE ANALYSIS
SEMs via the ADF method. Practically, as pointed out by Hu, Bentler and Kano
(1992), and Bentler and Dudgeon (1996), the ADF theory requires extremely
large sample sizes to attain its claimed asymptotic results.
The dimension of the weight matrix W in Equation (3.26) is p∗by p∗,
hence the computational burden for obtaining ˜A is rather heavy for large size
problems. Bentler (1983) and Browne (1984) noted that this computational
burden can be substantially alleviated if the sample observations are coming from
an elliptical distribution. Under an elliptical distribution, the general element
of the asymptotic covariance matrix ∗is
∗ijkh = +10ik0jh+0ij 0kh
which depends on a relative kurtosis coefficient  and elements of 0. In matrix
form,
∗= KT
p 2+10 ⊗0+ vec0vec0T Kp	
Let ˜ and ˜ be consistent estimates of  and 0, respectively, a consistent
estimate of ∗is
W = KT
p 2˜+1 ˜⊗˜+ ˜ vec ˜ vec ˜T Kp	
The corresponding GLS function is (see Bentler, 1983; Browne, 1984)
GA = 2−1˜+1−1 trS− ˜
−12 −˜trS− ˜
−12
4˜+1+2p ˜˜+1	
Clearly, a natural choice of ˜ is S. An estimate of  that is proposed by Browne
(1984) is
ˆ = np p +2−1
n
i=1
xi −xT S−1xi −x2 −1	
Other estimates of  are available in Bentler (1992). The above GA just
involves scalars and p by p symmetric matrices. Computational efforts and
computer storage are greatly reduced. The EQS6 (Bentler and Wu, 2002)
program has an option to compute GLS estimates under elliptical distributions.
As the elliptical distribution is symmetrical and has longer tails, the option
developed under this distribution is effective in handling data with heavy tails.
However, it may not be effective in handling skewed data. Compared with the
general ADF method, the method based on an elliptical distribution requires a
relatively smaller sample size to achieve the asymptotic results.

3.6
SOME ITERATIVE PROCEDURES
47
3.6
SOME ITERATIVE PROCEDURES
In general, there are many iterative procedures which could be considered
for minimizing an objective function. Here, we shall restrict our attention to
those related to the LISREL (Jöreskog and Sörbom, 1996) and EQS6 (Bentler
and Wu, 2002); namely the Newton–Raphson, Scoring, Fletcher–Powell, and
Gauss–Newton algorithms. The Newton–Raphson and the Fletcher–Powell
algorithms can be used for minimizing any arbitrary objective functions, the
Scoring algorithm is most suitable for minimizing the likelihood function F,
while the Gauss–Newton algorithm is most suitable for minimizing the GLS
function G.
In general, suppose  is a vector of unknown parameters and we want to
estimate  by minimizing a real-valued objective function Q. The main
purpose of nonlinear programming is to locate ˜, the value of  for which Q
is smallest. We start with an initial guess 1 and iteratively generate a sequence
23 … which hopefully will converge to ˜. We will call the computation
of i+1 the ith iteration, the vector i the ith iterate, and
△i = i+1 −i
(3.28)
the ith step. If Qi+1 is less than Qi, then the ith step is said to be
acceptable.
All the algorithms considered in this chapter will be of the following form:
(i)
Set i = 1, guess 1.
(ii)
At the ith iteration, determine a scalar i and a positive definite matrix
R i such that △i = −iR i ˙Qi is an acceptable step, where ˙Qi
is the gradient vector of Q at i.
(iii)
Check convergence. If the algorithm is not yet converged, go to step (ii)
and continue.
The Newton–Raphson algorithm is to choose the inverse of the Hessian
matrix, ¨Qi−1, for R i; hence its ith step is defined by
△i = −i ¨Qi−1 ˙Qi	
(3.29)
If Q is quadratic, this algorithm converges in one iteration from any starting
value. For general functions, this method gives quadratic convergence near the
minimum. However, for poor starting values, ¨Q may not be positive definite.
Greenstadt (1967) modified this algorithm by replacing the negative definitive
Hessian matrix with some positive definite matrix. In practice, we can use a

48
3
COVARIANCE STRUCTURE ANALYSIS
positive definite matrix for R i, such as the inverse of E ¨Q or the identity
matrix, at the first few iterations and then switch to ¨Q−1 near the minimum,
for achieving quicker convergence.
Another major disadvantage of the Newton–Raphson algorithm is that it
requires the second-order partial derivatives of Q. Sometimes these deriva-
tives are difficult to obtain and take a long time to compute. Some algorithms,
which use only first-order derivatives, will be considered below.
The well-known LISREL program uses the Fletcher–Powell algorithm
(Fletcher and Powell, 1963) to minimize the objective function. Steps of this
algorithm are:
(i)
Start with 1 and any positive definite matrix A1. Set i = 1.
(ii)
At the ith iteration, choose i, the value of  such that Qi −
Ai ˙Qi is minimized with respect to .
(iii)
Obtain
i+1 = i −iAi ˙Qi△i ˙Qi+1,
and
△˙Qi =
˙Qi+1−˙Qi.
(iv)
Set Ai+1 = Ai +△A
i
i
and i = i +1, where
△Ai =
1
△iT △˙Qi △i△iT −
1
△˙QiT Ai△˙Qi Ai△˙Qi△˙QiT Ai	
(v)
Check convergence. If the algorithm is not yet converged, go to step (ii)
and continue.
The initial matrix A1 can be chosen to be any positive definite symmetric
matrix that approximates the inverse of the Hessian matrix. If we do not have
such an approximation, the identity matrix may be used. Fletcher and Powell
(1963) showed that if Ai is positive definite, then Ai+1 is positive definite.
Hence, starting with a positive definite matrix Ai always gives an acceptable
step. Moreover, if Q is a quadratic function, then
Aq+1 = ¨Q−1	
(3.30)
It should be noted that the above results depend on the optional choice of
i. For minimizing Q, a highly accurate of i is not important. For
example, Jöreskog (1967) used the ‘extrapolation and interpolation’ technique
of Davidon (1959) for getting i.
Equation (3.30) motivates using 2n−1 ˜A to estimate the asymptotic covariance
matrix and standard errors of ˜ or ˜M , where ˜A denotes the converged value
of the matrix A in the Fletcher–Powell algorithm. However, Lee and Jennrich

3.6
SOME ITERATIVE PROCEDURES
49
(1979) showed that this method for approximating the asymptotic covariance
matrix or the standard errors fails to give answers consistent with the appropriate
Hessian matrix or the information matrix. In the LISREL program, standard
errors are computed via the inverse of the information matrix.
The scoring algorithm can be applied when Q  depends on observed values
of random variables. In this algorithm, the Hessian matrix is replaced by its
expectation. Hence
△i = −iE ¨Qi−1 ˙Qi	
(3.31)
This algorithm is usually more robust to bad starting values, because
E  ¨Qi is often positive definite. Moreover, as E  ¨Qi usually just
involves first partial derivatives, it requires less computing time per iteration
than the Newton–Raphson algorithm.
The Gauss–Newton algorithm is particularly attractive for minimizing objec-
tive functions of the form
Q = 2−1eT We
where e is a column vector of residuals and W is a weight matrix. From the
product rule of matrix calculus,
˙Q = 2−1˙e1⊗We+ ˙eWe⊗1 = ˙eWe
¨Q = 2−1 
˙e1⊗We+ ˙eWe⊗1
= ˙eW˙eT + ¨eWe	
Since e is usually small, particularly near the minimum, the last term in ¨Q
is neglected in the Gauss–Newton algorithm. Let U = ˙eW˙eT , the ith
step of the Gauss–Newton algorithm is defined by
△i = −iUi−1 ˙Qi	
(3.32)
In the scoring algorithm or the Gauss–Newton algorithm, the step size i is
commonly chosen via step-halving, that is, using the first value in the sequence
12−12−2    that provides an acceptable step.

50
3
COVARIANCE STRUCTURE ANALYSIS
When applying the Fletcher–Powell algorithm to the GLS and ML approaches
of the covariance structure analysis under the multivariate normal assumption,
˙Q = ˙G or ˙Q = ˙F. The scoring algorithm for the ML estimation
becomes (see Equations (3.18) and (3.19))
△i = i ˙−1 ⊗−1 ˙T −1 ˙−1 ⊗−1vecS−
=
i	
(3.33)
For the Gauss–Newton algorithm in the GLS estimation, e = vecsS− and
W = V ⊗V, see Equation (3.9). Hence, from Equations (3.10) and (3.11),
△i = i ˙V ⊗V ˙T −1 ˙V ⊗VvecS−
=
i	
(3.34)
Comparing Equations (3.33) and (3.34), we see that the scoring algorithm
is an iteratively reweighted Gauss–Newton algorithm, that is, one in which
the weight matrix V = −1 changes with  from iteration to iteration. This
means that the Gauss–Newton algorithm may be used for both GLS and ML
estimations. Standard errors for the estimates are obtained from E¨F˜−1 or
U˜−1, which are by-products of the scoring algorithm or the Gauss–Newton
algorithm. The well-known EQS program uses the Gauss–Newton algorithm
for obtaining the GLS and ML solutions.
To apply the Fletcher–Powell algorithm to the GLS approach under ADF
theory with objective function GA ˙Q = ˙GA = −˙W−1vecS−.
When
applying
the
Gauss–Newton
algorithm
to
ADF
estimation,
Equation (3.32) becomes
△i = −i ˙W−1 ˙T −1 ˙GA
=
i	
(3.35)
3.6.1
Numerical Examples
In this section, we give an example to illustrate the Fletcher–Powell (FP)
algorithm that is used in the LISREL program, and the Gauss–Newton (GN)

3.6
SOME ITERATIVE PROCEDURES
51
algorithm and its iteratively reweighted mode that are used in the EQS program
for obtaining the GLS and ML estimates. Although these algorithms can be
applied to more general models, for simplicity, only the exploratory factor anal-
ysis defined by Equation (2.2) is considered. Hence the parameter vector 
contains the unknown parameters in  and diag   (the vector that contains
the diagonal elements in  ).
Based on the matrix calculus given in Appendix 3.1, the gradient vector of
the objective functions are given by
˙F =
⎡
⎢⎢⎣
F

F
diag 
⎤
⎥⎥⎦= −
⎡
⎣
2 vec−1S−−1
Cp−1S−−1
⎤
⎦
˙Q =
⎡
⎢⎢⎣
Q

Q
diag 
⎤
⎥⎥⎦= −
⎡
⎣
2 vecVS−V
CpVS−V
⎤
⎦	
The implementation of the FP algorithm and the GN algorithm requires
these first derivatives. It can be shown that the matrix U required by the
GN algorithm (see Equation (3.32)) is given by
U =
4V ⊗T VMpI⊗
2V ⊗T VCT
p
2CpV ⊗VT 
V ∗V

where V ∗V is the Hadamard product of V which is a p × p matrix with the
general ijth entry v2
ij. For the fixed parameters in , the corresponding
rows in ˙F and ˙Q, and the corresponding rows and columns of U  are
removed in the implemention. In the illustration, we use V = S−1 in analyzing
the Emmett (1949) data which consist of nine variables on a sample of 211
subjects. Starting values are obtained by the principal component factor anal-
ysis (see e.g. Afifi and Azen, 1974). The data set is fitted with a three-factor
model, and the upper triangle elements of  are fixed at the starting values for
identifying the model. We say that an algorithm has been converged if the root
mean squares of the gradient vector or the root mean squares of △ is less than
0.0001.
In obtaining the GLS estimates, the GN algorithm and the FP algorithm
converge quickly; the root mean square of the gradient and the root mean
square of △ approach zero, and the function values converge to the same

52
3
COVARIANCE STRUCTURE ANALYSIS
Table
3.1
Convergence of the Gauss–Newton algorithm; on Emmett’s (1949) data
in the GLS estimation.
Iteration
Function value
RMS ˙Q
RMS △
11
92
2
0
0	88830
0	4293
—
0.749 −0	285
0.337
1
0	63940
0	5467
0	2822
0.499
0	238
0.417
2
0	07278
0	0919
0	1229
0.595
0	414
0.413
3
0	03414
0	0123
0	0499
0.656
0	339
0.414
4
0	03324
0	0010
0	0145
0.661
0	340
0.416
5
0	03322
0	0003
0	0023
0.661
0	339
0.416
6
0	03322
0	0001
0	0012
0.662
0	339
0.416
7
0	03322
0	0000
0	0003
0.662
0	339
0.416
Taken from Lee and Jennrich (1979).
Table
3.2
Convergence of the Fletcher–Powell algorithm; on Emmett’s (1949) data
in the GLS estimation.
Iteration
Function value
RMS ˙Q
RMS △
11
92
2
0
0	88830
0	4293
—
0	749
−0	285
0.337
5
0	20870
0	1028
0	0522
0	673
0.037
0.413
10
0	05665
0	0550
0	0143
0	697
0.260
0.407
15
0	03641
0	0207
0	0123
0	669
0.335
0.411
20
0	03363
0	0059
0	0032
0	660
0.342
0.414
25
0	03324
0	0021
0	0008
0	661
0.341
0.416
30
0	03322
0	0004
0	0002
0	662
0.341
0.416
35
0	03322
0	0001
0	0000
0	662
0.341
0.416
Taken from Lee and Jennrich (1979).
minimum value. To give some idea of the convergence, summaries of the algo-
rithms are presented in Tables 3.1 and 3.2, respectively. The final parameter
estimates obtained by these two algorithms are very close to each other; almost
all of them are exactly the same in the first three decimal places. Hence, only the
estimates and their standard errors estimates that are obtained via the GN algo-
rithm are presented in Table 3.3. In obtaining the ML estimates, the iteratively
reweighted GN algorithm and the FP algorithm converged in six and 32 itera-
tions. The similar convergence summaries are not presented. The ML estimates
obtained by the two algorithms are very close, hence only those obtained via
the GN algorithm are presented in Table 3.4, together with the standard errors
estimates. As expected, the asymptotically equivalent GLS and ML results are
very close.

APPENDIX 3.1: MATRIX CALCULUS
53
Table
3.3
The GLS solution by the GN algorithm; stan-
dard error estimates are in parentheses and parameters with an
asterisk are fixed.
ˆ
ˆ 
0.66(0.07)
0	32∗
−0	08∗
0.45(0.05)
0.62(0.20)
0.39(0.34)
0	19∗
0.42(0.05)
0.55(0.20)
0.20(0.37)
0	220	16
0.60(0.07)
0.29(0.18)
0.84(0.11)
0	040	83
0.21(0.04)
0.19(0.24)
0.75(0.23)
0	160	79
0.37(0.05)
0.21(0.12)
0.88(0.12)
0	110	94
0.17(0.05)
0.72(0.11)
0.27(0.21)
0	070	17
0.39(0.05)
0.51(0.30)
0.18(0.52)
−0	480	22
0.47(0.15)
0.81(0.09)
0.34(0.14)
0	000	15
0.23(0.04)
Table
3.4
The
ML
solution
obtained
by
iteratively
reweighted GN algorithm; standard error estimates are in
parentheses and parameters with an asterisk are fixed.
ˆ
ˆ 
0.66(0.07)
0	32∗
−0	08∗
0.45(0.05)
0.62(0.19)
0.39(0.31)
0	19∗
0.43(0.05)
0.54(0.18)
0.20(0.33)
0	220	15
0.62(0.07)
0.29(0.16)
0.84(0.09)
0	030	78
0.21(0.04)
0.19(0.21)
0.75(0.20)
0	150	74
0.38(0.05)
0.23(0.11)
0.88(0.12)
−0	110	88
0.18(0.05)
0.72(0.11)
0.27(0.20)
0	080	16
0.40(0.05)
0.52(0.29)
0.17(0.50)
−0	490	24
0.46(0.17)
0.81(0.09)
0.34(0.14)
0	010	15
0.23(0.04)
APPENDIX 3.1: MATRIX CALCULUS
There are many methods for obtaining derivatives of a matrix of function with
respect to a matrix of their variables. The method described here is an adaptation
of McDonald and Swaminathan (1973).
Definition 3.1.1
Given a p by q matrix Y = yij whose elements are differen-
tiable functions of elements of an m by n matrix X = xij, the matrix derivatives
of Y with respect to X is

54
3
COVARIANCE STRUCTURE ANALYSIS
Y
X =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
y11
x11
···
y1q
x11
···
yp1
x11
···
ypq
x11
			
			
			
			
y11
x1n
···
y1q
x1n
···
yp1
x1n
···
ypq
x1n
			
			
			
			
y11
xm1
···
y1q
xm1
···
yp1
xm1
···
ypq
xm1
			
			
			
			
y11
xmn
···
y1q
xmn
···
yp1
xmn
···
ypq
xmn
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
	
The order of Y/X is mn by pq. It is obtained by differentiation of all
elements of Y, row by row sequentially, with respect to all elements of X, also
following the manner of row by row sequentially. Recall
vecX = x11   x1nx21   x2n   xm1   xmnT 
it follows from the definition of Y/X that
Y
X =
Y
vecX =
Y
vecXT = vecY
X
= vecYT
X
	
Examples. (1)
Let X be a 3 by 2 matrix with distinct variables,
E32 = XT
X =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0

0
0
0
0
0
0

1
0
0
−−−−−−−−−−−−
0
1
0

0
0
0
0
0
0

0
1
0
−−−−−−−−−−−−
0
0
1

0
0
0
0
0
0

0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
The typical element of Emn corresponding to an m by n matrix X is
Emnnj −1+kmi −1+h = 1 if j = hk = i
= 0
otherwise
where 0 < jh ≤m, and 0 < ik ≤n.

APPENDIX 3.1: MATRIX CALCULUS
55
(2)
Let X be a 3 by 3 symmetric matrix with distinct lower-triangular variables,
T3 =
X
vecsX =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
0
0

0
0
0

0
0
0
−−−−−−−−−−−−−−−−−−
0
1
0

1
0
0

0
0
0
0
0
0

0
1
0

0
0
0
−−−−−−−−−−−−−−−−−−
0
0
1

0
0
0

1
0
0
0
0
0

0
0
1

0
1
0
0
0
0

0
0
0

0
0
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
	
The typical element of Tn corresponding to an n by n symmetric matrix X is
Tnjj −1+kni −1+h = 1 if j = ik = h or j = hi = k
= 0
otherwise	
(3)
Let X be a 3 by 3 diagonal matrix with distinct diagonal variables,
C3 =
X
diagX =
⎡
⎣
1
0
0

0
0
0

0
0
0
0
0
0

0
1
0

0
0
0
0
0
0

0
0
0

0
0
1
⎤
⎦
The general typical element of Cp corresponding to a p by p diagonal matrix
X is
Cpij  = 1 if j = p i −1+i
= 0
otherwise	
Matrices EmnTn and Cn are common and useful for implementing itera-
tive procedures for estimation of CSA. See Bentler and Lee (1975) for more
properties of these matrices among themselves, with the Kronecker product, Kp
and Mp.
Now, let us recall the ‘chain rule’ in scalar calculus. Let z = g y1   yp
be a differentiable function of y1   yp, and for each k = 1   p, let yk =
fkx1   xq be a differentiable function of x1   xq. From the theory of
partial differentiation,
z
xi
=
gy1   yp
xi

56
3
COVARIANCE STRUCTURE ANALYSIS
=
p
k=1
fkx1   xq
xi
gy1   yp
yk
=
p
k=1
yk
xi
z
yk
	
(A3.1)
This is the ‘chain rule’ in scalar calculus. To apply it correctly, y1   yp are
considered as mathematically independent variables in gy1   yp/yk. With
this fact in mind, we now present the ‘chain rule’ in matrix calculus.
Theorem 3.1.1
(The Chain Rule). If elements of Z are differentiable functions
of elements of Y, and elements of Y are differentiable functions of elements of
X, then
Z
X = Y
X
Z
Y 	
(A3.2)
The proof is straightforward. It should be noted that in applying this rule,
Z/Y in Equation (A3.2) is calculated with elements of Y being treated as
mathematically independent variables.
Theorem 3.1.2
(The Product Rule). If elements of Yp × r and Zr × q are
differentiable functions of elements of X, then
YZ
X = Y
X Ip ⊗Z+ Z
X YT ⊗Iq	
(A3.3)
The proof is straightforward. Some useful results are given below.
Theorem 3.1.3
Suppose elements of Yp ×p and Zr ×m are differentiable
functions of elements of X, then:
(i)
trY
X
= Y
X vec Ip.
(ii)
Y
X
= Y
X YvecYT −1, if Y is non-singular.
(iii)
 log Y
X
= Y
X vecYT −1, if Y is non-singular.
(iv)
Y−1
X
= −Y
X YT −1 ⊗Y−1, if Y is non-singular.

APPENDIX 3.2: SOME BASIC RESULTS IN PROBABILITY THEORY
57
(v)
AZB
X
= Z
X AT ⊗B, if A and B are constant matrices such that AZB is
well defined.
(vi)
ZAZT
X
= Z
X Ir ⊗AZT + ZT
X AZT ⊗Ir, if Am ×m is a constant
symmetric matrix.
Proof.
The proof of (i) is straightforward. To prove (ii) note that Y =

p
j=1 yijYij, where Yij is the cofactor of yij, which is also equal to the ijth
entry of YYT −1. Hence Y/yij = Yij and
Y
X = Y
X vecYYT −1 = Y
X YvecYT −1	
Assertion (iii) follows immediately from (ii). To prove (iv), it follows from
YY−1 = I that
0 = YY−1
X
= Y
X Ip ⊗Y−1+ Y−1
X YT ⊗Ip	
Hence,
Y−1
X
= −Y
X Ip ⊗Y−1YT ⊗Ip−1 = −Y
X YT −1 ⊗Y−1	
Assertions (v) and (vi) follow directly from the product rule.
APPENDIX 3.2: SOME BASIC RESULTS IN PROBABILITY
THEORY
This appendix contains the basic results in probability theory needed for deriving
the asymptotic results of the GLS, ML and ADF estimators. Proofs are not
presented, they can be found in other textbooks in probability theory or math-
ematical statistics such as Rao (1973, Chapter 2), among others.
Definition 3.2.1
Let Xn = Xnn = 12    be a sequence of random
variables.
(i)
Xn is said to converge to a constant c in probability if, for every  > 0,
lim
n→	 PXn −c >  = 0	

58
3
COVARIANCE STRUCTURE ANALYSIS
Such convergence is denoted by Xn
p
−→c.
(ii)
Xn is said to converge to a random variable X in probability if the
sequence of random variables Xn −Xn = 12    converges to zero
in probability. Such convergence is denoted by Xn
p
−→X.
(iii)
Let Fnn = 12    be the sequence of distribution functions corre-
sponding to Xn. Then Xn is said to converge in distribution to a
random variable X with distribution function F if
Fn −→F as n −→	
at all continuity points of F . Such convergence is denoted by Xn
L
−→X.
The approximating distribution is called the asymptotic distribution of
Xn.
In most of our applications, the random variable Xn stands for a statistic
computed from a sample of size n. Since the actual distribution of Xn is difficult
to find, it is necessary to obtain its asymptotic distribution for statistical infer-
ences. Some basic properties of the above defined convergences are presented
via the following theorem. These properties are essential for obtaining the
asymptotic distribution.
Theorem 3.2.1
Let XnYnn = 12   be a sequence of pairs of variables.
Then
(i)
Xn
L
−→XYn
p
−→0 =⇒XnYn
p
−→0.
(ii)
Xn
L
−→XYn
p
−→c =⇒Xn +Yn
L
−→X +c.
(iii)
Xn
p
−→XYn
p
−→Y =⇒XnYn
p
−→XY .
(iv)
Xn −Yn
p
−→0Xn
L
−→X =⇒Yn
L
−→X.
(v)
If Xn
p
−→X and g· is a continuous function, then
gXn
p
−→gX	
Since X
L
−→X is always true, so if Xn −X
p
−→0, then it follows from
(iv) that Xn
L
−→X . Hence, convergence in probability implies convergence in
distribution.

APPENDIX 3.3: PROOFS OF SOME RESULTS
59
For simplicity, the above definitions and theorem are given in terms of random
variables, but they are also valid for random vectors.
APPENDIX 3.3: PROOFS OF SOME RESULTS
PROOF OF LEMMA 4
˙G = G

= 1
2
 trS−V2

= 1
2
S−

S−V
S−
S−V2
S−V
trS−V2
S−V2
= −1
2
˙Ip ⊗VIp ⊗S−V+VS−⊗Ipvec Ip
= −1
2
˙Ip ⊗VS−V +VS−⊗Vvec Ip
= −1
2
˙2vecVS−V
= −˙vecVS−V
¨G = 2G
 = −
  ˙V ⊗VvecS−
= −¨Iq ⊗V ⊗VvecS−−V ⊗VvecS−

 ˙T ⊗1
= −¨Iq ⊗V ⊗VvecS−−S−

V ⊗V ˙T 	
PROOF OF THEOREM 3.1
Consider the quadratic form
G∗ = 2−1vec0 −T V∗⊗V∗vec0 −	
Since the model is identified and V∗⊗V∗ is positive definite, this quadratic
form has its unique minimum which is equal to zero at  = 0. Compare it with
G = 2−1vecS−T V ⊗VvecS−	

60
3
COVARIANCE STRUCTURE ANALYSIS
Since S and V converge respectively to 0 and V∗in probability, and  is
bounded in a neighborhood of 0G converges in probability to G∗
uniformly in a neighborhood of 0. Since G is continuous, its unique
minimum ˜ converges to the unique minimum 0 of G∗ in probability. This
proof is an adoption of a proof of Browne (1974).
PROOF OF THEOREM 3.2
It follows from the definition of ˜ that ˙G˜ = 0. Applying the ‘mean-value’
theorem (see Bartle, 1964, p. 210) to ˙G˜, we have
0 = ˙G˜ = ˙G0+ ¨G∗˜ −0
(A3.4)
where ∗is a vector which lies between ˜ and 0. From Equations (3.10) and
(A3.4),
¨G∗n1/2˜ −0 = −n1/2 ˙G0 = ˙0V ⊗Vn1/2vecS−0	
Thus,
n1/2˜ −0 = ¨G∗−1 ˙0V ⊗Vn1/2 vecS−0	
From Equations (3.6) and (3.12), and knowing that V converges to V∗in
probability, we have
n1/2˜ −0
p
−→	V∗−1 ˙0V∗⊗V∗n1/2 vecS−0
(A3.5)
L
−→N 0C0
where
C0 = 	V∗−1 ˙0V∗⊗V∗2Mp0 ⊗0MpV∗⊗V∗ ˙0T 	V∗−1
= 2	V∗−1 ˙0V∗⊗V∗0 ⊗0V∗⊗V∗ ˙0T 	V∗−1
= 2	V∗−1	V∗0V∗	V∗−1	
This completes the proof.
The idea of the above proof can be applied to other objective functions.
Roughly, asymptotic normality of the corresponding estimator can be achieved
if the gradient vector of the objective function is asymptotic normal.

APPENDIX 3.3: PROOFS OF SOME RESULTS
61
PROOF OF COROLLARY 3.1
If V∗= −1
0 , then
C0 = 2	−1
0 −1	−1
0 0−1
0 	−1
0 −1 = 2	−1
0 −1	
Now, consider the quadratic form
2	V∗−1 ˙0V∗⊗V∗−	−1
0 −1 ˙0−1
0 ⊗−1
0 0 ⊗0
	V∗−1 ˙0V∗⊗V∗−	−1
0 −1 ˙0−1
0 ⊗−1
0 T
=2	V∗−1 ˙0V∗⊗V∗0 ⊗0V∗⊗V∗ ˙0T 	V∗−1
−	V∗−1 ˙0V∗⊗V∗0 ⊗0−1
0 ⊗−1
0  ˙0T 	−1
0 −1
−	−1
0 −1 ˙0−1
0 ⊗−1
0 0 ⊗0V∗⊗V∗ ˙0T 	V∗−1
+	−1
0 −1 ˙0−1
0 ⊗−1
0 0 ⊗0−1
0 ⊗−1
0  ˙0T 	−1
0 −1
=2	V ∗−1 ˙0V∗0V∗⊗V∗0V∗ ˙0T 	V∗−1
−	−1
0 −1 −	−1
0 −1 +	−1
0 −1
=C0−2	−1
0 −1	
Since 0 ⊗0 is positive definite, the above quadratic form and hence C0−
2	−1
0 −1 is positive definite. This completes the proof of the corollary.
PROOF OF THEOREM 3.3
On the basis of the ‘mean-value’ theorem, there exists a ∗between ˜ and 0
such that
n1/2vecS−˜ = n1/2vecS−0− ˜−0
= n1/2vecS−0−˙∗T n1/2˜ −0	
From above and (A3.5)
n1/2vecS−˜
p
−→n1/2vecS−0−˙∗T 	−1
0 −1 ˙0−1
0 ⊗−1
0 n1/2vecS−0
p
−→0n1/2vecS−0
(A3.6)

62
3
COVARIANCE STRUCTURE ANALYSIS
where 0 = Ip2 −˙0T 	−1
0 −1 ˙0−1
0 ⊗−1
0 Ip2 is a p2 ×p2 identity
matrix.
Substitute (A3.6) to nG˜,
nG˜ = 2−1n1/2vecS−˜T V ⊗Vn1/2vecS−˜
p
−→2−1n1/2vecS−0T T
0 −1
0 ⊗−1
0 02−1n1/2vecS−0	
Now, 2−1n1/2vecS −0
L
−→N0Mp0 ⊗0Mp. Hence, from a stan-
dard theorem on quadratic form (see Graybill, 1961 p. 83) it suffices to show
T
0 −1
0 ⊗−1
0 0 Mp0 ⊗0Mp is an idempotent matrix with rank p∗−q.
First, we note that
T
0 −1
0 ⊗−1
0 0
=−1
0 ⊗−1
0 −−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0−1
0 ⊗−1
0 0
=−1
0 ⊗−1
0 −−1
0 ⊗−1
0  ˙0T 	−1
0  ˙0−1
0 ⊗−1
0 
−−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0−1
0 ⊗−1
0 
+−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0−1
0 ⊗−1
0 −1
× ˙0T 	−1
0 −1 ˙0−1
0 ⊗−1
0 
=Ip2 −−1
0 ⊗−1
0  ˙0T 	−1
0  ˙0−1
0 ⊗−1
0 	
Hence, it follows from Equation (3.4) and Lemma 3 that
T
0 −1
0 ⊗−1
0 0Mp0 ⊗0Mp
=Ip2 −−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0−1
0 ⊗−1
0 0 ⊗0Mp
=Mp −−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0	
To show this is an idempotent matrix, we note that
Mp −−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0
×Mp −−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0
= Mp −−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0
−−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0

APPENDIX 3.3: PROOFS OF SOME RESULTS
63
+−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0
= Mp −−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0	
Hence, T
0 −1
0 ⊗−1
0 0Mp0 ⊗0Mp is idempotent. Its rank is equal to
trMp−tr−1
0 ⊗−1
0  ˙0T 	−1
0 −1 ˙0
=pp +1/2−tr	−1
0 −1 ˙0−1
0 ⊗−1
0  ˙0T 
=p∗−tr Iq = p∗−q	
This completes the proof.
PROOF OF LEMMA 5
˙F = F

= 
log +trS−1
=  log 

+ ˙−1

S−1
−1
trS−1
S−1
= ˙vec−1+ ˙−−1 ⊗−1S⊗Ipvec Ip
= ˙vec−1−˙vec−1S−1
= −˙vec−1S−−1
= −˙−1 ⊗−1vecS−	
(A3.7)
Further,
−1S−−1

= −1
 Ip ⊗S−−1+ S−−1

−1 ⊗Ip
=−˙−1 ⊗−1Ip ⊗S−−1+S−

Ip ⊗−1
+ −1
 S−⊗Ip−1 ⊗Ip
=−˙−1 ⊗−1S−−1+−˙Ip ⊗−1
−˙−1 ⊗−1S−⊗Ip−1 ⊗Ip
=−˙−1 ⊗−1S−−1 +−1S−−1 ⊗−1−˙−1 ⊗−1	
(A3.8)

64
3
COVARIANCE STRUCTURE ANALYSIS
From Equations (3.21) and (3.22),
¨F = 2F
 = 


−˙vec−1S−−1


=−¨Iq ⊗vec−1S−−1−−1S−−1

 ˙T ⊗1
= ˙−1 ⊗−1 ˙T −¨Iq ⊗−1 ⊗−1vecS−
−˙−1 ⊗−1S−−1+−1S−−1 ⊗−1 ˙T
= ˙−1 ⊗−1 ˙T −2 ˙−1 ⊗−1S−−1 ˙T
−¨Iq ⊗−1 ⊗−1vecS−	
(A3.9)
PROOF OF COROLLARY 3.2
(i)
It follows from Corollary 3.1 and Equation (3.23) that
n1/2˜M −0 = n1/2˜M −˜+n1/2˜ −0
L
−→N02	−1
0 −1	
(ii)
From the ‘mean-value’ theorem, there exists a ∗in between ˜M and ˜
such that
nG˜M −nG˜ = n1/2 ˙G∗n1/2˜M −˜
= − ˙∗V ⊗Vn1/2vecS−∗n1/2˜M −˜	
(A3.10)
Similarly, there exists a ∗∗between ∗and ˜ that
n1/2 vecS−∗−n1/2 vecS−˜ = −n1/2 vec∗−˜
= −˙∗∗T n1/2∗−˜
which converges in probability to zero because n1/2∗−˜ converges in prob-
ability to zero and ˙∗∗ is bounded. Hence, n1/2vecS −∗ converges
in distribution to the same one as n1/2vecS −˜. It follows from Equa-
tion (A3.10) that nG˜M  converges to nG˜ in probability and hence they

REFERENCES
65
have the same asymptotic distribution, which is 2
p∗−q. This completes the proof
of (ii). The proof of (iii) is very similar.
REFERENCES
Afifi, A. A. and Azen, S. P. (1974) Statistical Analysis:A Computer Oriented Approach.
New York: Academic Press.
Anderson, T. W. (1984) An Introduction to Multivariate Statistical Analysis (2nd edn).
New York: John Wiley & Sons, Inc..
Bartle, R. G. (1964) The Elements of Real Analysis. New York: John Wiley & Sons, Inc..
Bentler, P. M. (1983) Some contributions to efficient statistics for structural models:
specification and estimation of moment structures. Psychometrika, 48, 493–517.
Bentler, P. M. (1992) EQS: Structural Equation Program Manual, Los Angeles, CA:
BMDP Statistical Software.
Bentler, P. M. and Dudgeon, P. (1996) Covariance structure analysis: statistical practice,
theory and directions. Annual Review of Psychology, 47, 563–592.
Bentler, P. M. and Lee, S. Y. (1975) Some extensions of matrix calculus. General Systems,
20, 145–150.
Bentler, P. M. and Wu, E. J. C. (2002) EQS 6 for Windows User’s Guide. Encino, CA:
Multivariate Software.
Browne, M. W. (1974) Generalized least squares estimators in the analysis of covariance
structures. South African Statistical Journal, 8, 1–24.
Browne, M. W. (1984). Asymptotic distribution free methods in analysis of covariance
structure. British Journal of Mathematical and Statistical Psychology, 37, 62–83.
Davidon, W. C. (1959). Variable metric method for minimization. U.S. Atomic Energy
Commission, Argonne National Laboratories, Research and Development Report ANL-
5990, p. 27.
Emmett, W. G. (1949) Factor analysis by Lawley’s method of maximum likelihood.
British Journal of Mathematical and Statistical Psychology, 2, 90–97.
Fletcher, R. and Powell, M. (1963) A rapidly convergent descent method for minimiza-
tion. Computer Journal, 6, 163–168.
Greenstadt, J. (1967) On the relative efficiencies of gradient method. Mathematical
Computing, 21, 360–367.
Graybill, F. A. (1961). An Introduction to Linear Statistical Models. New York: McGraw-
Hill.
Hu, L., Bentler, P. M. and Kano, Y. (1992) Can test statistics in covariance structure
analysis be trusted? Psychological Bulletin, 112, 351–362.
Jöreskog, K. G. (1967) Some contributions to maximum likelihood factor analysis.
Psychometrika, 32, 443–482.
Jöreskog, K. G. (1978) Structural analysis of covariance and correlation matrices.
Psychometrika, 43, 443–477.
Jöreskog, K. G. and Sörbom, D. (1996) LISREL 8: Structural Equation Modeling with the
SIMPLIS Command Language. Hove and London: Scientific Software International.
Kendall, M. G. and Stuart, A. (1969) The Advanced Theory of Statistics, vol. 1. London:
Charles Griffin.
Lee, S. Y. and Jennrich, R. I. (1979) A study of algorithms for covariance structure
analysis with specific comparisons using factor analysis. Psychometrika, 44, 99–113.

66
3
COVARIANCE STRUCTURE ANALYSIS
Lee, S. Y. and Song, X. Y. (2004) Evaluation of the Bayesian and maximum likeli-
hood approaches in analyzing structural equation models with small sample sizes.
Multivariate Behavioral Research, 39, 653–686.
McDonald, R. P. and Swaminathan, H. (1973) A simple matrix calculus with applications
to multivariate analysis. General System, XVIII, 37–54.
Rao, CR. (1973) Linear Statistical Inference and its Applications (2nd edn). New York:
John Wiley & Sons, Inc..

4
Bayesian Estimation of
Structural Equation
Models
4.1
INTRODUCTION
In Chapter 3, we discussed the GLS and ML approaches in a covariance struc-
ture analysis framework for analyzing the standard structural equation model.
The statistical theory that is associated with the GLS and ML approaches as
well as the computational algorithms are developed on the basis of the sample
covariance matrix S. For example, the form of the GLS function (see Equa-
tion (3.9)) and the derivation of the asymptotic properties of the GLS estimator
heavily depend on the asymptotic distribution of S (see Equation (3.5)). Hence,
these approaches work well under certain assumptions that ensure the validity
of Equation (3.5). Typically, random observations are assumed to be identically
and independently distributed according to a multivariate normal distribution.
If some of the assumptions are violated, S and/or its asymptotic properties
may be difficult to derive. Unfortunately, as the real world is complicated, the
required assumptions cannot be satisfied by a large number of substantive prob-
lems. Hence, there is a strong demand for new developments of new statistical
methods for handling more general models and complex data structures. This
strong demand produces the recent growth of SEMs.
The basic objective of this book is to introduce a Bayesian approach for
analyzing not only the standard SEMs but also their useful generalizations which
have been developed in recent years. In contrast to the existing covariance
structure analysis approach, we focus on the use of the raw observations rather
than the sample covariance matrix. To solve the difficulties that are induced
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

68
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
by the complexities of the model and the data, the following general strategy
has been emphasized and used repeatedly throughout this book. First, we treat
the latent variables in the model, and the latent measurements (such as the
real missing data, or the continuous measurements that are associated with the
discrete data) as missing data then we analyze the model on the basis of the
complete data set that contains the observed data and all the missing data by
applying some powerful tools in statistical computing. As the complete data set
is much easier to handle, the difficulties that are induced by the complexities of
the model and the data are alleviated. The choice of the Bayesian approach is
justified by the following points.
The basic attractive feature of a Bayesian approach is its flexibility to utilize
useful prior information for achieving better results. In many practical problems,
statisticians may have good prior information from some sources, for example
the knowledge of experts and analyses of similar data and/or past data. For
example, in research relating to organization and management that involves
latent variables about job performance and job satisfaction, we may have some
prior information about the correlation of these latent variables, say a relatively
large value is one that is larger than 0.4; we also may have some prior infor-
mation on the values of the factor loadings, say a relatively large loading that
corresponds to ‘salary’ and ‘job satisfaction’. For situations without accurate
prior information, some type of non-informative prior distributions can be used
in a Bayesian approach. In these cases, the accuracy of the Bayesian estimates is
close to that of the ML estimates.
It is well known that the statistical properties of the ML approach are
asymptotic. Hence, they are valid for situations with large sample sizes. In
the context of some basic SEMs, many studies (see, for example, Boomsma,
1982; Chou, Bentler and Satorra, 1991; Hu, Bentler and Kano, 1992;
Hoogland and Boomsma, 1998) have been devoted to study the behaviors of
the ML asymptotic properties with small sample sizes. It was concluded by
such research that the properties of the statistics are not robust for small sample
sizes. The reason for this phenomenon is that the derivation of the statistics
heavily depends on the important result that the sample covariance matrix S is
asymptotically normal. However, even if the given data are normal, the distribu-
tion of S approaches normal only if the corresponding sample size is large. On
the contrary, as pointed out by many important articles in Bayesian analyses of
structural equation models (Ansari and Jedidi, 2000; Ansari, Jedidi and Dube,
2002; Ansari, Jedidi and Jagpal, 2000; Dunson, 2000; Scheines, Hoijtink and
Boomsma, 1999; Lee and Song, 2004), the sampling-based Bayesian methods
depend less on asymptotic theory, and hence have the potential to produce
reliable results even with small samples.
The posterior distributions of parameters and latent variables can be esti-
mated by using a sufficiently large number of observations that are simulated
from the posterior distribution of the unknown parameters through efficient
tools in statistical computing such as the various Markov chain Monte Carlo

4.1
INTRODUCTION
69
(MCMC) methods. Means as well as quantiles of this posterior distribution
can be estimated from the simulated observations. These quantities are useful
in making statistical inferences. For example, the Bayesian estimates of the
unknown parameters and the latent variables can be obtained from the corre-
sponding sample means of the posterior distribution. From these estimates,
estimated residuals can be obtained. In some complex situations, these esti-
mated residuals are useful in assessing the goodness-of-fit of the proposed model
and detecting outliers. Finally, the Bayes factor that is closely related with the
Bayesian approach gives a more flexible and natural statistic for model compar-
ison than the classical likelihood ratio test (see Kass and Raftery, 1995). We
will give a detailed discussion on the model comparison with the Bayes factor
in Chapter 5.
Before the 20th century, the Bayesian approach received little attention in
SEM. Contributions are only limited to factor analysis (see, for example, Martin
and McDonald, 1975; Lee, 1981; Bartholomew, 1981). More recently, the
idea of data augmentation (Tanner and Wong, 1987) and the powerful tools
in statistical computing for simulating observations from posterior distributions
have greatly enhanced the applicability of the general Bayesian approach. A
number of generalizations of the standard SEM have been separately developed
by this approach. These include the developments of models with fixed covari-
ates, nonlinear models, multilevel models, multisample models, mixture models,
models with mixed continuous, dichotomous and/or ordered categorical vari-
ables, models with missing data, and models with data that are coming from
an exponential family of distributions. These developments will be described in
subsequent chapters. More importantly, we show that the freely available soft-
ware WinBUGS has the potential to produce various Bayesian statistics, such
as the Bayesian estimates, their standard error estimates and the estimates of
the latent variables. Given the availability of WinBUGS, it is convenient for
the applied researchers to use the Bayesian approach for applying either the
standard SEMs or the complex SEMs to substantive problems.
The objective of this chapter is to provide an introduction of the Bayesian
approach to SEMs. It is not intended to present a full coverage of the general
Bayesian theory. Readers may refer to other excellent books, for example Box
and Tiao (1973), and Gelman, Carlin, Stern and Rubin (1995) for more details
of this general statistical method. This chapter begins with a section on the
basic ideas of the Bayesian approach in estimation, including the discussion of
the prior distribution and the posterior analysis by some MCMC methods. A
Bayesian estimation of the CFA model is presented in Section 4.3, with a real
example to illustrate the concept, and some simulation results on the accu-
racy of the Bayesian estimates in small samples. Section 4.4 considers Bayesian
estimation of a LISREL type model. The final section demonstrates the use
of WinBUGS to obtain Bayesian estimates of the parameters, standard error
estimates and latent variable estimates. Some technical details are given in the
Appendices to the chapter.

70
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
4.2
BASIC PRINCIPLES AND CONCEPTS OF BAYESIAN
ANALYSIS OF SEMs
4.2.1
Bayesian Estimation
The Bayesian approach is well recognized in the statistics literature as an attrac-
tive approach to analyze a wide variety of models (Berger, 1985; Congdon,
2003). To introduce this approach for analyzing SEMs, we let M be an arbi-
trary SEM with a vector of unknown parameters , and let Y be the observed
data set of raw observations with a sample size n. In a non-Bayesian approach,
for example in an ML approach,  is not considered as random. In a Bayesian
approach,  is considered to be random with a distribution (called prior distri-
bution) and an associative (prior) density function, say, p M (see Berger,
1985, and the references therein for the theoretical and practical rationales for
treating  as random). For simplicity, we use p  to denote p M. Bayesian
inference is based on the observed data Y and the prior distribution of .
Let p YM be the probability density function of the joint distribution
of Y and  under M . The behavior of  under the given data Y is fully
described by the conditional distribution of  given Y. This conditional distri-
bution is called the posterior distribution of . Let p YM be the density
function of the posterior distribution, which is called the posterior density func-
tion. The posterior distribution of  or its density plays the most important
role in the Bayesian analysis of the model. Based on a well-known identity
in probability, we have p YM = p YMp  = p YMp YM. As
p YM does not depend on , and can be regarded as a constant with fixed Y,
we have
log p YM ∝log p YM+log p
(4.1)
Note that p YM can be regarded as the likelihood function, because it
is the probability density of y1   yn conditional on the parameter vector
. It follows from Equation (4.1) that the posterior density function incor-
porates the sample information and the prior information through the likeli-
hood function p YM and the prior density function p . Note also that
p YM depends on the sample size, whereas p  does not. When the
sample size becomes arbitrarily large, logYM could be very large and hence
log p YM dominates log p . In this situation, the prior distribution of 
plays a less important role, and the posterior density function log p YM is
close to the log-likelihood function log p YM. Hence, Bayesian and ML
approaches are asymptotically equivalent, and the Baysian estimates have the
same optimal properties as the ML estimates. When the sample sizes are small
or moderate, the prior distribution of  plays a significant role in the Bayesian
approach. Hence, in substantive research problems where the sample sizes are

4.2
BAYESIAN ANALYSIS OF SEMs
71
small or moderate, prior information about the parameter vector  can be
incorporated into the Bayesian analysis through the prior distribution of  in
order to achieve better results (see below for the utilization of useful prior infor-
mation in the analysis). For many practical problems, researchers may have good
prior information from experts, from analyses of similar or past data or from
some other sources. More accurate results can be achieved by incorporating the
appropriate prior information in the analysis through the prior distribution of
. Thus, the selection of the prior density is an important issue in Bayesian anal-
ysis. In the following sections and chapters, the symbol M will be omitted if
the context is clear. Moreover, ‘p y
D=’ will also be used to denote that ‘the
conditional distribution of y is distributed as’, if the context is clear.
4.2.2
Prior Distributions
Prior distribution of  represents the distribution of possible parameter values,
from which the parameter  has been drawn. Basically, there are two kinds
of prior distributions, namely the non-informative prior distributions and the
informative prior distributions. Non-informative prior distributions associate
with situations where the prior distributions have no population basis. They are
used when we have little prior information, and hence the prior distributions
play a minimal role in the posterior distribution. The associated prior density is
regarded as vague, diffuse, flat or non-informative, for example density that is
proportional to a constant or has an extremely huge variance. In this case, the
Bayesian estimation is unaffected by information external to the observed data.
For informative prior distribution, we may have prior knowledge about this
distribution, either from closed related data or from the subjective knowledge
of experts. Usually, an informative prior distribution has its own parameters,
which are called hyperparameters.
A commonly used informative prior distribution in the general Bayesian
approach to statistical problems is the conjugate prior distribution. Let us
consider an example with the univariate binomial model. Considered as a func-
tion of 	, the likelihood of an observation y is of the form
p y	 =
n
y

	y1−	n−y
If the prior density of 	 is of the same form, then it can be seen from Equa-
tion (4.1) that the posterior density will also be of this form. More specifically,
consider the prior density of 	:
p 	 ∝	
−11−	−1
(4.2)

72
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
which is a beta distribution with hyperparameters parameters 
 and . Then,
p 	y ∝p y	p 	
∝	y1−	n−y	
−11−	−1
= 	y+
−11−	n−y+−1
(4.3)
which is a beta distribution with parameters y + 
 and n −y + . We see that
p 	 and p 	y are of the same form. The property that the posterior distri-
bution follows the same parametric form as the prior distribution is called
conjugacy, and the prior distribution is called a conjugate prior distribution
(Gelman, Carlin, Stern and Rubin, 1995). In the above example, the beta prior
distribution is a conjugate family for the binomial likelihood.
Consider another example with a sample of independently and identically
distributed (iid) observations y1   yn from N 	2, where 2 is known. Let
Y = y1   yn, the likelihood function is
p Y	 =
1
2
n
2 n exp

−1
22
n
i=1
yi −	2


Considered as a function of 	, the likelihood is an exponential of a quadratic
form in 	. Thus, a conjugate prior distribution of 	 can be parameterized as
p 	 ∝exp

−1
22
0
	 −02


(4.4)
that is, 	
D= N 02
0, where 0 and 2
0 are hyperparameters. The posterior
density is
p 	Y ∝p 	p Y	
∝exp

−1
22
0
	 −02

p Y	
∝exp
	
−1
2

1
2
0
	 −02 + 1
2
n
i=1
yi −	2

(4.5)
which can be shown to belong to the same distribution family as the distribution
of 	.
If Y is from N 	2 with 	 known and 2 unknown, then
pY2 ∝2−n/2
n
i=1
exp
 1
22 yi −	2


4.2
BAYESIAN ANALYSIS OF SEMs
73
=2−n/2 exp

−n
22 v


where v = n−1n
i=1yi −	2. The corresponding conjugate prior density of 2 is
p2 ∝2−
0+1 exp−0/2
(4.6)
This is an inverted-gamma density function with hyperparameters 
0 and 0.
The posterior distribution of 2 is
p2Y ∝p2pY2
∝2−n/2+
0+1 exp

−1
22 nv +20


(4.7)
which belong to the inverted-gamma distribution family. Equivalently, we may
consider the conjugate prior distribution for −2 as gamma 
∗
0∗
0 with hyper-
parameters 
∗
0 and ∗
0. That is
p−2 ∝−2
∗
0−1 exp−∗
0−2
(4.8)
Again, the resulting posterior distribution belongs to the gamma distribution
family.
Now consider the situation with a sample of i.i.d observations y1   yn from
N 2, where  and 2 are unknown. To obtain some idea for finding the
conjugate prior distribution of  and 2, we note that
p 2Y ∝p Y2p 2
(4.9)
The likelihood p Y2 is proportional to
2−n/2 exp

−1
22
n
i=1
yi −2

=2−n/2 exp

−1
22 n −1s 2 +ny −2


(4.10)
where s 2 = n −1−1 n
i=1yi −y2 is the sample variance. Hence, if the prior
distribution of 2 has the same form as given in Equation (4.10), we expect
that the posterior density p 2Y will also have the same form. On the basis
of the following equality,
p 2 = p 2p 2

74
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
we need to specify p 2 and p 2. Motivated from the above discussion on
treating 2 in a normal distribution, see Equation (4.7), a natural choice for the
prior distribution of 2 is an inverted-gamma distribution, with hyperparameters

0 and 0, that is,
p 2 ∝2−
0+1 exp−0/2
(4.11)
A natural choice for the prior distribution of  given 2 is a normal distribution,
say N 022
0, with hyperparameters 0 and 2
0, that is
p 2 ∝2−1/2 exp−−02/222
0
(4.12)
Combining Equations (4.11) and (4.12), the prior distribution of 2 is of
the form
p 2 ∝2−
0+12−1/2 exp−0/2exp−−02/222
0
∝2−
0+12−1/2 exp

−1
22

20 + −02
2
0


which is equivalent to the so-called normal-inverted gamma distribution. On
the basis of this prior distribution, it follows from Equations (4.9) and (4.10)
that the posterior density p 2Y is proportional to
2−
0+1 exp−0/2×2−1/2 exp−−02/222
0
×2−n/2 exp−n −1s 2 +ny −02/22
which can be shown to be in the form of a normal-inverted gamma distribu-
tion. Hence, a conjugate type prior distribution of 2 is given by Equa-
tions (4.11) and (4.12).
The above discussion on scalar parameters motivates the selection of conju-
gate type prior distributions for the parameters in Bayesian analyses of SEMs.
For instance, considering the following factor analysis model corresponding to
a measurement equation:
yi = i +i
where iq × 1 is distributed as N 0, and i is distributed as N 0,
where  is diagonal with diagonal elements k, and i and i are independent.

4.2
BAYESIAN ANALYSIS OF SEMs
75
Let T
k be the kth row of . A conjugate type prior distribution of kk is
k
D= Inverted Gamma 
∗
0k∗
0k or equivalently−1
k
D= Gamma 
0k0k
and kk
D= N

0kkH0yk


(4.13)
where 
0k0k
∗
0k∗
0k, and elements in 0k and H0yk are hyperparameters,
and H0yk is a positive definite matrix. The conjugate prior distribution of −1
is a q-dimensional Wishart distribution,
−1 D= WqR00 or equivalently 
D= IWqR ∗
00
(4.14)
where WqR00 is a q-dimensional Wishart distribution with hyperparameters
0 and a positive definite matrix R0, and IWqR ∗
00 is an inverted Wishart
distribution with hyperparameters 0 and a positive definite matrix R ∗
0. This is
a multivariate extension of the prior distribution of −1
k in Equation (4.13). As
the forms of the structural equation and the factor analysis model are similar,
analogous conjugate prior distributions as given above are also used for the
parameters involved.
If the hyperparameters in the conjugate prior distributions are not known,
then they may be treated as unknown parameters and thus have their own prior
distributions in a full Bayesian analysis. These hyperprior distributions again
have their own hyperparameters. As a result, the problem will become tedious.
Hence, for convenience, we usually assign fixed known values to the hyperpa-
rameters in the conjugate prior distributions. In fact, existing works in Bayesian
analysis of SEMs, such as Shi and Lee (1998), Lee and Zhu (1999), and Song
and Lee (2001), used conjugate type prior distributions with given hyperpa-
rameters values. It has been shown that these distributions work well for many
SEMs. Therefore, in this book, we will use the conjugate prior distributions in
most of our Bayesian analyses.
In specifying conjugate prior distributions, we assign values to their hyper-
parameters. In practice, these preassigned values (prior inputs) represent the
available prior knowledge. In general, if we have confidence to have good prior
information about a parameter, then it is desirable to take a small variance
in the corresponding prior distribution; otherwise a larger variance should be
selected. For example, if we have confidence that the true k is not too far away
from the preassigned hyperparameter value 0k, then H0yk should be taken as
a matrix with small variances (such as 05I). The choice of 
0k and 0k is
based on the same general rationale and the nature of the parameter k in
the model. First we note that the distribution of the error measurement k is
N0k. Hence, if we think that the variation of k with its mean value 0 is
small (that is, T
k i is a good predictor for yik), then the prior distribution of

76
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
k should have a small mean value as well as a small variance. Otherwise, the
prior distribution k should have a large mean value and/or a large variance as
well. This gives some idea in choosing the hyperparameters 
0k and 0k in the
inverted Gamma distribution (see Equation (4.13)). Note that for the inverted
Gamma distribution, the mean is equal to 0k/
0k −1, and the variance is
equal to 2
0k/
0k −12
0k −2. Hence, we may take 
0k = 9 and 0k = 4 for
a situation where we have confidence that T
k i is a good predictor of yik in
the measurement equation. With this choice, the mean of k is 4/8 = 05, and
the variance of k is 42/9−129−2 = 1/28. For a situation with little confi-
dence, we may take 
0k = 6 and 0k = 10, then the mean of k is 2.0 and the
variance is 1.0. Now, consider the choice of R0 and 0 in the prior distribution
of , see Equation (4.14). It follows from Muirhead (1982, p. 97) that the
mean of  is R −1
0 /0 −q −1. Hence, if we have confidence that  is not too
far away from 0, we can choose R −1
0
and 0 such that R −1
0
= 0 −q −10.
Other values of R −1
0
and 0 may be considered for situations without good
prior information. Based on our experience in handling situations with nonac-
curate prior inputs, large values of 0k or 0 should be taken so that the variance
of the corresponding prior distribution is large enough.
Now, we discuss some methods to obtain 0k and R0. As we mentioned
before, these hyperparameter values may be obtained from the subjective knowl-
edge of field experts and/or analysis of past or closely related data. If this kind of
information is not available and the sample size is small, we may consider using
non-informative prior distributions. Based on the general Bayesian framework in
Zellner (1971), some common non-informative prior distributions in SEMs are
p ∝p1   p ∝
p
k=1
−1
k and p ∝−q+1/2
(4.15)
In Equation (4.15), the prior distribution of the unknown parameters in  is
implicitly taken to be proportional to a constant. Note that no hyperparameters
are involved in these non-informative prior distribution. Bayesian analysis on the
basis of the above non-informative prior distribution is basically equivalent to the
Bayesian analysis with conjugate prior distributions given by Equations (4.13)
and (4.14) that have very large variances. If the sample is large, one possible
method is to use a portion of the data, say one-half or less, to conduct an auxil-
iary Bayesian estimation with non-informative priors to obtain initial Bayesian
estimates. Then, use the remaining data to do the actual Bayesian analysis, by
utilizing the initial Bayesian estimates as hyperparameter values. For situations
with moderate sample sizes, the Bayesian analysis may be done by applying data-
dependent prior inputs that are obtained from the initial estimation of the whole
data set. Although the above methods are reasonable, we emphasize that we are
not routinely recommending them for every practical application. In general,
the issue of selecting prior inputs should be carefully approached on a problem-
by-problem basis. Moreover, it is desirable to conduct a sensitivity study to see
whether the results are robust for prior inputs. This can be done by perturbing
the given hyperparameter values or considering some ad hoc prior inputs.

4.2
BAYESIAN ANALYSIS OF SEMs
77
4.2.3
Posterior Analysis
The Bayesian estimate of  is usually defined as the mean or the mode of
p Y. Martin and McDonald (1975) and Lee (1981) respectively obtained
the Bayesian estimates of  in EFA and CFA models from the mode of
log p Y. They worked directly with Equation (4.1) by finding the maximum
of log p Y + log p , or its equivalent form by using iterative proce-
dures as described in Section 3.6. This procedure will encounter difficul-
ties if log p Y is intractable. A way of handling these difficulties is by
applying the EM algorithm (Dempster, Laird and Rubin, 1977) for finding
the maximum of log p Y	 + log p . In this book, we are mainly inter-
ested in estimating the mean of the posterior distribution of the unknown
parameters.
Theoretically, the mean of the posterior distribution Y could be obtained
via integration. For most situations, the integration does not have a closed form.
However, if we can simulate a sufficiently large number of observations from
Y (or from p Y), we can approximate the mean, and/or other useful
statistics, through the simulated observations. Hence, to solve the problem, it
is important to develop efficient methods for drawing observations from the
posterior distribution (or the posterior density). For most nonstandard SEMs,
the posterior distribution Y is complicated. It is difficult to derive this distri-
bution and simulate observations from it. A major break through for posterior
simulation is the idea of data augmentation that was proposed by Tanner and
Wong (1987). The strategy is to treat latent quantities as hypothetical missing
data and augment the observed data with them so that the posterior distribu-
tion (or the posterior density) based on the complete data set is relatively easy
to analyze. This strategy has been found to be extremely useful in statistics; see,
for example, Rubin (1991), Zeger and Karim (1991), Albert and Chib (1993),
among many others. It is particularly useful for SEMs which involve latent
variables. The feature that makes SEMs different from the common regression
model and the simultaneous equation model is the existence of latent random
variables. In fact, for many situations, the presence of latent variables causes the
difficulties in analyzing the model. However, if the latent random variables are
observed, the SEM will become the familiar regression model or simultaneous
equation model that can be handled without much difficulty. For example, if
the factor scores in a factor analytic model are not random but observed data,
then the model becomes the regression model.
Hence, the above-mentioned strategy based on data augmentation provides
an useful approach to solve the problem that is induced by latent variables.
Augmenting the observed variables in complicated SEMs with the latent vari-
ables that are treated as hypothetical missing data, we usually can obtain the
Bayesian solution on the basis of the complete-data set. More specifically,
instead of working with the intractable posterior density p Y, we will work
with p 	Y, where 	 is the set of latent variables in the model. For most
cases, p 	Y is still not in closed form and it is difficult to deal with it

78
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
directly. However, on the basis of the complete-data set 	Y, the conditional
distribution 	Y is usually standard; moreover, the conditional distribu-
tion 	Y can be derived from the definition of the model without much
difficulty. Consequently, we can apply some MCMC methods to simulate
observations from p 	Y by drawing observations iteratively from their full
conditional densities p 	Y and p 	Y. (Following the terminology in
MCMC methods, we may call p 	Y and p 	Y conditional distribu-
tions, if the context is clear.) A useful algorithm to do this is the following
Gibbs sampler (Geman and Geman, 1984).
In the model M , suppose the parameter vector  and the latent matrix
	 are respectively decomposed into the following components or subvectors
 = 1   a and 	 = 	1   	b. The Gibbs sampler is a Markov chain
algorithm which performs an alternating conditional sampling at each of its iter-
ations. It cycles through the components of  and 	, drawing each component
conditional on the values of all the other components. More specifically, at the
jth iteration with current values j = 
j
1    
j
a  and 	j = 	
j
1    	
j
b ,
it simulates in turn,

j+1
1
from
p 1
j
2    j
a 	jY

j+1
2
from
p 2
j+1
1
   j
a 	jY


j+1
a
from
p a
j+1
1
   
j+1
a−1 	jY
	
j+1
1
from
p 	1j+1	
j
2 ··· 	
j
b Y
	
j+1
2
from
p 	2j+1	
j+1
1
   	
j
b Y
(4.16)


	
j+1
b
from
p 	bj+1	
j+1
1
   	
j+1
b−1 Y
There are a +b steps in the jth iteration of the Gibbs sampler. At each step, each
component in  and 	 is updated conditional on the latest values of the other
components. We may simulate the components in 	 first, then the components
in , or vice versa. Most of the full conditional distributions in (4.16) are
the standard normal, gamma or Wishart distributions. Simulating observations
from them is rather straightforward. For nonstandard conditional distributions,
the Metropolis–Hastings (MH) algorithm (Metropolis et al., 1953; Hastings,
1970) may have to be used for efficient simulation. A brief description of the
MH algorithm is given in Appendix 4.1.
It has been shown (Geman & Geman, 1984; Geyer, 1992) that under mild
regularity conditions, the joint distribution of j	j converges at an expo-
nential rate to the desired posterior distribution 	Y, after a sufficiently

4.2
BAYESIAN ANALYSIS OF SEMs
79
large number of iterations, say J . If the iterations have not proceeded long
enough, the simulated observations may not be representative of the posterior
distribution. Moreover, after the algorithm has reached approximate conver-
gence, observations obtained at the early iterations should be discarded because
they are still not part of the target distribution. The required number of itera-
tions for achieving convergence of the Gibbs sampler, that is the burn-in itera-
tion J , can be determined by plots of the simulated sequences of the individual
parameters. At convergence, parallel sequences generated with different starting
values should be mixed well together. Examples of sequences for which conver-
gence looks reasonable, and sequences that have not reached convergence are
presented in Figure 4.1. Another method for monitoring convergence of the
Gibbs sampler is the following method as described in Gelman (1996). Based
on different starting values of the structural parameters and latent variables,
parallel sequences of observations are generated and the ‘estimated potential
scale reduction (EPSR)’ values corresponding to the parameters are calculated
sequentially as the runs proceed. As suggested by Gelman (1996), convergence
of these sequences has been achieved where the EPSR values are all less than
1.2. The computation of the EPSR values is presented in Appendix 4.2. A minor
problem with iterative simulation draws is their within-sequence correlation.
In general, inference from correlated observations is less precise than from the
same number of independent observations. To obtain a less correlated sample,
bb[5] chains 1:3
1
1000
2000
3000
4000
1.5
1.0
0.5
0.0
–0.5
1.0
0.0
–1.0
–2.0
iteration
(a)
1
1000
2000
3000
4000
iteration
(b)
bb[15] chains 1:3
Figure
4.1
Sample traces of chains: (a) for which convergence looks reasonable;
(b) which have not reached convergence.

80
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
observations may be collected in cycles with indices J +cJ +2c   J +T ∗c
for some spacing c (see Gelfand & Smith , 1990). However, in most practical
applications a small c will suffice for many statistical analyses such as obtaining
estimates of the parameters and standard errors (see Zeger and Karim (1991);
Albert and Chib (1993)). In the numerical illustrations of the remaining chap-
ters, we will use c = 1.
Statistical inference of the model can then be conducted on the basis of
a simulated sample of observations from p	Y, namely t	t  t =
1   T ∗. The Bayesian estimate of  as well as the numerical standard error
estimates can be obtained from
ˆ = T ∗−1
T ∗

t=1
t
(4.17)

VarY = T ∗−1−1
T ∗

t=1
t −ˆt −ˆT 
(4.18)
It can be shown that (Geyer, 1992) ˆ tends to EY as T ∗tends to infinity.
It can be seen from Equation (4.1) that ˆ has the same large sample properties
as the ML estimate. Other statistical inference on  can be carried out based
on the simulated sample, t  t = 1   T ∗. For instance, the 2.5 %, 50 %
and 97.5 % points of the sampled distribution of an individual parameter give
a 95 % posterior interval and convey skewness in its marginal posterior density.
The construction of the posterior interval does not depend on any asymptotic
results. The total number of draws, T ∗, that is required for statistical analysis
depends on the form of the posterior distribution. Clearly, different choices of
sufficiently large T ∗would produce close estimates, although they may not be
exactly equal.
As the posterior distribution of  given Y describes the distributional behav-
iors of  with the given data, the dispersion of  can be assessed through
varY, with an estimate given by Equation (4.18), based on the sample
covariance matrix of the simulated observations. Let 	k be the kth element of .
The positive square root of the kth diagonal element in 
VarY is commonly
taken as the standard error estimate of the Bayesian estimate ˆ	k. While this
estimate provides some information about the variation of ˆ	k, it may not be
appropriate to construct a ‘z-score’ in hypothesis testing for some complex
SEMs with non-standard data. In general Bayesian analysis, the issue of hypoth-
esis testing is formulated as a model comparison problem, and is handled by
some model comparison statistics such as the Bayes factor. (see Chapter 5 for
more detailed discussions).
For any individual yi, let i be the vector of latent variables, E iyi
and Var iyi be the posterior mean and the posterior covariance matrix.

4.3
BAYESIAN ESTIMATION OF THE CFA MODEL
81
A Bayesian estimate ˆi can be obtained through 	tt = 1   T ∗ as follows:
ˆi = T ∗−1
T ∗

t=1

t
i 
(4.19)
where 
t
i
is the ith column of 	t. This gives a direct Bayesian estimate
which is not expressed in terms of the structural parameter estimates. Hence,
in contrast to the classical methods in estimating latent variables, no sampling
errors of the estimates are involved in the Bayesian method. It can be shown
(Geyer, 1992) that ˆi is a consistent estimate of E iyi. A consistent estimate
of Var iyi can be obtained as

Variyi = T ∗−1−1
T ∗

t=1

t
i −ˆi
t
i −ˆiT 
(4.20)
In practice, numerical standard error estimates of elements in ˆi can be obtained
via square roots of diagonal elements in 
Variyi. Therefore, the proposed
estimation procedure based on the Gibbs sampler also produces Bayesian esti-
mates of the latent variables as well as their numerical standard errors estimates.
These estimates ˆi can be used for outlier and residual analysis, and for assessing
the goodness-of-fit of the measurement equation or the structural equation,
particularly in analyzing complicated SEMs (see more discussions on this issue
in Chapter 8, Section 8.4.3). It should be noted that as the data information
for estimating ˆi is only given by the single observation yi, we cannot expect
ˆi to be an accurate estimate of the true latent variable io (see the simula-
tion study reported in Lee and Shi (2000) on the estimation of factor scores
in a factor analysis model). However, the empirical distribution of the Bayesian
estimates  ˆ1    ˆn is close to the distribution of the true factor scores
 ˆ1o    ˆno (Shi and Lee (1998)).
Before ending this section, we emphasize the importance of the following
strategy in analyzing complex SEMs: (i) Apply the idea of data augmentation to
augment the observed data Y with the unknown quantities, which could be the
latent variables, missing data, etc., that caused the difficulties in the problem,
and then work with the joint posterior distribution in the posterior analysis. (ii)
Apply MCMC tools in statistical computing to draw observations from the full
conditional distributions of the joint posterior distribution. This strategy will
be repeatedly applied in subsequent chapters.
4.3
BAYESIAN ESTIMATION OF THE CFA MODEL
To illustrate the Bayesian estimation and the associative MCMC method
described in Section 4.2, we present a detailed application in the context of

82
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
CFA model. Consider the following CFA model which is equivalent to that as
defined in Equation (2.3) of Chapter 2. For i = 1   n
yi = i +i
(4.21)
where yi is a p×1 observed random vector,  is a p×q factor loading matrix, i
is a q ×1 vector of factor scores and i is a p×1 random vector of error measure-
ments which is independent of i. Suppose i is distributed as N 0, where
 is a diagonal matrix, and i is distributed as N 0 with some positive
definite covariance matrix .
Let Y = y1   yn be the observed data matrix, 	 = 1   n be the
matrix of latent factor scores, and  be the structural parameter vector that
contains the unknown elements of  and  in the model. It is assumed
that this CFA model is identified. One common method to achieve this assump-
tion is to set some appropriate elements in  to fixed known values. From a
Bayesian point of view, this is equivalent to assigning the fixed values to the
corresponding parameters with probability one. In the analysis, fixed parameters
are not estimated. In the Bayesian analysis, we will treat the latent factor scores
in 	 as hypothetical missing data, and augment the observed data set Y with
	 in the posterior analysis. A sufficiently large sample of 	 from the joint
posterior distribution 	Y is generated by the following Gibbs sampler
algorithm. At the j + 1th iteration with current values of 	j
j
 j and
j:
i
Generate 	j+1 from p 	 j
 jjY
(4.22)
ii
Generate  j+1

from p 	j+1jjY
iii
Generate j+1 from p 	j+1 j+1

jY
iv
Generate j+1 from p 	j+1 j+1

j+1Y
The conditional distributions involved in steps (4.22) are required in the imple-
mentation of the Gibbs sampler.
4.3.1
Conditional Distributions
The derivation of p 	Y = p 	Y is based on the definition
of the model and the distributional properties of the random vectors yi and
i. Note that for i = 1   ni are mutually independent, and yi are also
mutually independent given i. Hence, we have
p 	Y =
n
i=1
p iyi ∝
n
i=1
p i p yii
(4.23)

4.3
BAYESIAN ESTIMATION OF THE CFA MODEL
83
Moreover, since the conditional distributions of i given , and yi given i
are N0, and Ni respectively, it can be shown (Lindley and Smith
(1972), pp. 4–5) that the conditional distribution of i given yi is equal
to
iyi
D= N

−1 +T  −1
 −1T  −1
 yi−1 +T  −1
 −1

(4.24)
Hence, the conditional distribution of 	 given Y can be obtained from
Equations (4.23) and (4.24).
The
conditional
distribution
of

given
Y	
is
proportional
to
p p Y	. Hence, it is necessary to select the prior density function p 
that represents the prior information of . Based on the factor analysis model
as defined in Equation (4.21), we first note that with given 	 the underlying
CFA model becomes a regression model with parameters  and  only. On
the other hand, the parameter matrix  is only involved in the distribution of
i. Hence, it is reasonable to assume that the prior distributions of  and
 are independent. As a result, we specify the prior distribution of  as follows:
p  = p  = p  p 
(4.25)
Moreover, the conditional distribution of Y given 	 only depends on  and
, and the distribution of 	 only involves . Consequently, we assume that
p Y	 =p Y	 ∝p Y	p  = p Y	p 	p 
=p Y	p 	p p 
= p p Y	p 	p 
(4.26)
Since the first term of the product on the right-hand side of Equation (4.26)
depends only on  while the second term depends only on , the
marginal conditional densities pY	 and pY	 are proportional
to p p Y	 and p 	p , respectively. As a result, these
densities can be treated separately.
The following conjugate type prior distributions are considered as prior distri-
butions for  and . Let k and T
k be the kth diagonal elements of
 and the kth row of , respectively. For any k ̸= h, we assume that the prior
distribution of kk is independent of hh. Moreover, we take
−1
k
D= Gamma 
0k0k
kk
D= N 0k kH0yk and −1 D= Wq R00
(4.27)

84
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
where Gamma
 represents the gamma distribution with shape parameter

 > 0 and inverse scale parameter  > 0Wq·· denotes an q-dimensional
Wishart distribution, 
0k, 0k, 0k, 0 and the positive definite matrices H0yk
and R0 are hyperparameters whose values are assumed to be given from the
prior information of previous studies or other sources.
Let YT
k be the kth row of YAk = H−1
0yk +		T −1ak = AkH−1
0yk0k +	Yk,
and k = 0k + 2−1YT
k Yk −aT
k A−1
k ak + T
0kH−1
0yk0k, it can be shown as in
Appendix 4.3 that for k = 1   p, the conditional distribution of k−1
k 
given Y and 	 is independently distributed as the following normal–gamma
distribution (Broemeling, 1985):
−1
k Y	
D= Gamma n/2+
0k k and kY	−1
k 
D= N akkAk
(4.28)
Since p k−1
k Y	 = p −1
k Y	p kY	−1
k , the conditional distri-
bution of k−1
k  given Y	 can be obtained via Equations (4.28). In
the implementation of the Gibbs sampler, we just simulate an observation −1
k
from Equations (4.28), then we can obtain an observation k by taking the
inverse. This k will be used in the other parts of the program, for example,
for simulating k in Equation (4.28). This method will be used in simulating
the variances of the error measurements throughout this book.
To derive p Y	, we first note from Equation (4.26) that it is propor-
tional to p p 	. As i are independent, we have
p Y	 ∝p 
n
i=1
p i
(4.29)
From the prior distribution of −1 given in Equation (4.27), it implies that 
D=
IWqR −1
0 0, where IWqR −1
0 0 denotes an q-dimensional inverted Wishart
distribution. Moreover, since the distribution of i given  is N0, we have
pY	 ∝

−0+q+1/2 exp

−1
2trR −1
0 −1

×
	
−n/2 exp

−1
2
n
i=1
T
i −1i

=−n+0+q+1/2 exp

−1
2 tr−1		T +R −1
0 


(4.30)
Since the right-hand side of Equation (4.30) is proportional to the density
function of an inverted Wishart distribution (see Zellner, 1971), we have
Y	
D= IWq		T +R −1
0  n +0
(4.31)

4.3
BAYESIAN ESTIMATION OF THE CFA MODEL
85
The above derivation can be extended to handle the general situation with
fixed known elements in  as follows. Consider T
k , the kth row of  with
certain fixed parameters. Let ck be the corresponding 1×q row vector such that
ckj = 0 if kj is a fixed parameter; and ckj = 1 if kj is an unknown parameter,
for k = 1   pj = 1   q, and rk = ck1 +    + ckq. Moreover, let ∗T
k
be
the 1 by rk row vector that contains the unknown parameters in k and let
	∗
k be the rk by n submatrix of 	 such that for j = 1   rk, all the rows
corresponding to ckj = 0 are deleted. Let Y∗T
k
= y∗
1k   y∗
nk with
y∗
ik = yik −
q
j=1
kjij1−ckj
As an example, let T
k = 1k2k30, where 1 and 0 are fixed. Then ck =
0110rk = 2∗T
k
= k2k3y∗
ik = yik −i1, and
	∗
k =
21    2n
31    3n


which is obtained by deleting the first and the fourth rows of 	.
Suppose the conjugate prior distribution defined in Equation (4.27) about
the loading matrix is
∗
kk
D= N ∗
0k kH∗
0yk
for some hyperparameters ∗
0k and H∗
0yk. Let A∗
k = H∗−1
0yk + 	∗
k	∗T
k −1a∗
k =
A∗
kH∗−1
0yk ∗
0k + 	∗
kY∗
k, and k = 0k + 1
2Y∗T
k Y∗
k −a∗T
k A∗−1
k
a∗
k + ∗T
0k H∗−1
0yk ∗
0k.
Then, for k = 1   p, it can be shown from exactly the same reasoning as
given above and in Appendix 4.3 that the posterior distributions of ∗
k−1
k 
and  corresponding to the conjugate priors are respectively given by:
−1
k Y	
D= Gamma n/2+
0kk ∗
kY	−1
k 
D= N a∗
kkA∗
k
and Y	
D= IWq		T +R −1
0  n +0
(4.32)
It can be shown by a similar derivation, as in Appendix 4.3, that for k = 1   p
the posterior distributions of ∗
k	k obtaining from the non-informative
prior distributions given in (4.15) are:
−1
k Y	
D= Gamma n −rk/22−1Y∗T
k In −	∗T
k 	∗
k	∗T
k −1	∗
kY∗
k and
∗
kY	−1
k 
D= N 	∗
k	∗T
k −1	∗
kY∗
kk	∗
k	∗T
k −1
(4.33)

86
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
where In denotes the identity matrix of order n. Moreover,
Y	
D= IWq		T n
(4.34)
It can be shown that if H∗−1
0yk tends to the zero matrix, then A∗
k = 	∗
k	∗T
k −1,
a∗
k = 	∗
k	∗T
k −1	∗
kY∗
k, and k = 0k + 2−1Y∗T
k In −	∗T
k 	∗
k	∗T
k −1	∗
kY∗
k.
Under this situation, if 
0k tends to −rk/2 and 0k tends to zero, the condi-
tional distributions −1
k Y	 and ∗
kY	−1
k  given in Equations (4.32)
reduce to those given in Equations (4.33) with non-informative priors. More-
over, if 0 tends to zero, and R −1
0
tends to the zero matrix, the conditional
distribution Y	 given in Equation (4.32) reduces to that given in Equa-
tion (4.34). Note that the above hyperparameters are associated with prior
distributions having extremely huge variances. Hence, conditional distributions
that are obtained on the basis of non-informative priors are special cases of the
conditional distributions that are obtained from conjugate prior distributions
with very large variances.
Let t	tt = 1   T ∗ be the observations of 	 generated by
the Gibbs sampler from the joint posterior distribution of  and 	 given
YE Y and VarY are the posterior mean vector and the posterior covari-
ance matrix, respectively. The Bayesian estimate ˆ can easily be obtained from
the simulated random observations as in Equation (4.17). An estimate of the
posterior covariance matrix can be obtained easily via Equation (4.18).
Now consider the posterior analysis about the factor scores. For any given
individual yi, let E iyi and Variyi be the posterior mean and the poste-
rior covariance matrix, respectively, and let io be the true factor scores of
yi. A Bayesian estimate ˆi of io can be obtained on the basis of the simu-
lated sample from the posterior distribution as in Equation (4.19). This gives a
direct Bayesian estimate that does not express in terms of the structural param-
eter estimates, and is a consistent estimate of Eiyi. A consistent estimate of
Variyi can be similarly obtained as in Equation (4.20). In practice, standard
error estimates of elements in ˆi can be obtained via 
Variyi. It should be
noted that both Eiyi and Variyi are difficult to assess using the classical
theory of factor analysis (see Bartholomew (1981)).
4.3.2
A Numerical Example
In this example, a real data set (see Fuller, 1987, p. 154) from a study about
the writing skill of non-native speakers of English is considered. One hundred
faculty members were asked to read and give scores to two essays using 11 items.
The information on each item in the data set is the sum of scores on that item
for the two essays. A part of the data set that involved six items was analyzed
by Fuller (1987). To show aspects of the Gibbs sampler and the Bayesian
estimates, this part of the data set was analyzed based on the assumption that

4.3
BAYESIAN ESTIMATION OF THE CFA MODEL
87
the random observations are coming from a multivariate normal population
with a factor analysis model. Following the suggestion in Fuller (1987), the
following structure of  is considered in the analysis:
T =

11 21
0
0
1 0
12 22 32 42 0 1


where elements with ‘0’ and ‘1’ are treated as fixed known values. Hence, the
structural parameter vector  contains the unknown elements of , the upper
triangular elements of  and diagonal elements of . The total number of
unknown structural parameters is 15.
Bayesian estimates with conjugate prior distributions are first obtained. In
this example, we do not have any historical information about the values of
the hyperparameters. For illustration, ML estimates that are obtained from
the first 40 observations are used to provide values of the hyperparameters in
∗
0k, R0, 
0k and 0k, k = 1   p. Let ∗
kk and 
 be the corresponding
ML estimates. We select ∗
0k = ˜∗
k; and since E k = 0k/
0k −1 and
E = R −1
0 /0 −q −1, we take 
0k = 3, 0 = r +4, 0k = 
0k −1 ˜k and
R −1
0
= 0 −q −1
. Finally, for convenience, we take H0yk to be a rk by rk
identity matrix. The Bayesian estimates were obtained with the remaining 60
observations. The convergence of the Gibbs sampler is monitored by the ‘esti-
mated potential scale reduction (EPSR)’ values obtained from three parallel
sequences of the 15 structural parameters generated with different starting
values. As suggested by Gelman (1996), convergence of these sequences has
been achieved if the EPSR values are all less than 1.2. Figure 4.2 shows the plots
of the EPSR values against the iteration numbers. We observe from this figure
that the sequences converged rapidly. The values of EPSR for all parameters are
less than 1.1 after about 250 iterations.
A total of T ∗= 4000 observations are collected after 250 iterations with the
spacing c = 1. Then, the Bayesian estimates and their numerical standard errors
estimates are obtained via Equations (4.17) and (4.18), respectively. Moreover,
ML estimates of the structural parameters are also obtained from the given
data set Y and LISREL VIII (Jöreskog and Sörbom, 1996). Bayesian estimates
(BAY), ML estimates and estimates of the corresponding standard errors are
presented in Table 4.1. We observe that the ML estimate of 5, the unique
variance corresponding to the fifth item, is equal to 0.06. Hence, this estimate
is very close to a Heywood case. As pointed out by Lee (1980), Heywood
cases in the ML estimation can be avoided by imposing inequality constraints
on k with a penalty function. In the Bayesian approach, the conjugate prior
distribution of −1
k specified k in a region of positive values and hence has a
similar effect as adding a penalty function. Hence, no Heywood cases are found
in the Bayesian solution because of the penalty function induced by the prior
distribution on −1
k . This phenomenon agrees with the discussion in Martin
and McDonald (1975).

88
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
0
100
200
300
400
500
600
700
800
900
1000
0.5
1
1.5
2
2.5
3
iteration
EPSR
Figure
4.2
EPSR values in the analysis of the language data (this figure is taken from
Lee and Shi (2000)).
Table
4.1
Bayesian estimates (BAY) and ML estimates and their standard errors in
the language example
Estimates
Standard errors
Estimates
Standard errors
Parameters
ML
BAY
ML
BAY
Parameters ML
BAY
ML
BAY
11
0.77
1.04
0.18
0.18
1
0.97
0.79
0.21
0.18
12
0.28
0.06
0.18
0.20
2
1.03
1.02
0.17
0.16
21
0.53
0.65
0.14
0.14
3
0.97
1.03
0.20
0.21
22
0.50
0.42
0.16
0.17
4
0.97
1.01
0.21
0.21
32
1.19
1.25
0.14
0.19
5
0.06
0.40
0.26
0.13
42
1.27
1.34
0.15
0.19
6
0.89
0.93
0.17
0.17
11
2.35
1.92
0.43
0.35
12
1.15
1.09
0.25
0.24
22
1.41
1.31
0.32
0.31
Note: Table 4.1 is taken from Lee and Shi (2000).

4.3
BAYESIAN ESTIMATION OF THE CFA MODEL
89
Bayesian estimates of the parameters in this example have also been obtained
with spacings c = 50 and 100. We find that the estimates obtained with different
c are very close to each other. Using c = 1, Bayesian estimates have also been
obtained with T ∗= 2000 and 8000. We found that these estimates are very
close to the previous estimates with T ∗= 4000 (difference only at the third
decimal place). Hence, it seems that the choices of c and T ∗are not important
in this analysis.
4.3.3
Robustness for Small Sample Sizes
Statistical properties of the estimates and the goodness-of-fit test that are
obtained from the ML and GLS methods in the CSA approach or the ADF
approach are asymptotically true only. Hence, theoretically, large sample sizes
are required for making valid statistical inferences. In past years, a lot of studies
(see for example Boomsma, 1982; Chou, Bentler and Satorra, 1991; Hu,
Bentler and Kano, 1992; Yung and Bentler, 1994; Hoogland and Boomsma,
1998) have been devoted to study the behaviors of these approaches with small
sample sizes. From such research, it is concluded that the properties of the
ML approach are not robust for small sample sizes. In the Bayesian approach,
the sampling-based procedures generate a sufficiently large number of obser-
vations, say T ∗, from the augmented joint posterior distribution p 	Y by
some MCMC methods, and use the empirical distribution to approximate the
posterior distribution. Now, an interesting question is: for a sufficiently large
T ∗, is the empirical distribution of the generated sample able to give an accu-
rate approximation of the underlying posterior distribution, regardless of the
sample size? For instance, the more specific question in estimation is: is the
Bayesian estimate obtained via the sample mean of a sufficiently large sample
of observations drawn from the joint posterior distribution p 	Y close to
the parameter vector 0 even if the information provided by the given data Y is
limited with a small sample size? Recently, Lee and Song (2004) addressed this
question via a simulation study. Some of their basic results are discussed below.
We will present the results of their simulation study that is based on a two-factor
CFA model defined by Equation (4.21) with the following specifications:
T =
10∗21 31 41
0∗
0∗
0∗
0∗
0∗
0∗42 10∗62 72


 =
11 12
21 22


and
 = diag1   7
where parameters with asterisks are treated as fixed parameters. The number of
unknown parameters is 16. These parameters are classified into three groups:
the unknown coefficients 21, 31, 41, 42, 62 and 72; the covariance of the
latent factor 21; and the variances of the error measurements, 1   7. We

90
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
Table
4.2
Design with respect to parameters’ magnitudes.
Regression
Covariance of latent variables
Variance of errors
kh
21
k
V1
08
06
036
V2
08
06
072
V3
08
02
036
V4
08
02
072
V5
04
06
036
V6
04
06
072
V7
04
02
036
V8
04
02
072
Note: Tables 4.2 to 4.6 are taken from Lee and Song (2004).
consider comparatively small and large values for parameter(s) in each group.
More specifically, the combinations of different true parameter values are given
by V1   V8 in Table 4.2. For example, V1 and V8 corresponding to situa-
tions with the smallest (relative) and the largest error measurement variances,
respectively. The true values of 11 and 22 are 1.0. The most important factor
in the simulation study is on the sample sizes. We consider sample sizes given
by n = da, where a is the number of parameters in the model and d = 234
and 5. Hence, n = 32, 48, 64 and 80 are considered for each V1   V8.
For each of the 328×4 combinations, the accuracy of the Bayesian estimates
is assessed by means of 200 replications, with data-dependent prior inputs that
are obtained via an auxiliary estimation with non-informative priors. The bias
(BIAS) of the estimates and the following root mean squares (RMS) between
the true values and the corresponding estimates are computed:
RMS of ˆ	h =
	
1
200
200

r=1
ˆ	rh−	0h2

1/2

(4.35)
where ˆ	h and 	0h are the hth elements of ˆ	 and its true value, respectively.
To study the behavior of the numerical standard error estimates, let SD(	h)
be the sample standard deviation obtained from ˆ	rh  r = 1   200, and
SE(	h) be the mean of the numerical standard errors estimates of ˆ	h obtained
via Equation (4.18). If the standard errors estimates obtained from Equa-
tion (4.18) are close to the sample standard deviations, SE(	h) should be
close to SD(	h), and SE(	h)/SD(	h) should be close to 1.0. Hence the
ratio SE(	h)/SD(	h) is used to assess the behavior of the numerical standard
error estimates.
We only present the results of some randomly selected parameters corre-
sponding to regression coefficients and variances of the errors in the measure-
ment and the covariance of the latent variables. The ‘BIAS’, ‘RMS’ and ‘SE/SD’

4.3
BAYESIAN ESTIMATION OF THE CFA MODEL
91
Table
4.3
Performance of the Bayesian approach under V1  kh = 0821 = 06
k = 036, and V2  kh = 0821 = 06k = 072.
V1
V2
Par
32
48
64
80
Par
32
48
64
80
21
0032
0040
0025
0004
21
0039
0033
0038
0029
41
0032
0020
0006
0021
41
0054
0062
0061
0041
62
0050
0020
0004
0019
62
0035
0019
0012
0006
72
0048
0036
0018
0025
72
0009
0006
0012
0003
BIAS
11
0016
0018
0003
0006
11
0057
0003
0002
0018
21
0098
0061
0067
0039
21
0123
0108
0068
0096
22
0094
0077
0028
0063
22
0093
0068
0108
0012
1
0036
0028
0011
0007
1
0009
0004
0004
0010
3
0013
0014
0005
0003
3
0029
0026
0011
0025
7
0033
0023
0018
0011
7
0004
0015
0003
0011
21
0145
0121
0097
0090
21
0203
0159
0159
0149
41
0188
0157
0123
0122
41
0232
0236
0223
0203
62
0143
0102
0090
0093
62
0185
0147
0141
0141
72
0120
0108
0096
0101
72
0170
0166
0135
0133
RMS
11
0271
0233
0206
0195
11
0303
0237
0228
0215
21
0231
0189
0165
0157
21
0268
0211
0186
0179
22
0284
0238
0211
0202
22
0314
0274
0291
0225
1
0074
0071
0059
0059
1
0127
0127
0133
0117
3
0065
0065
0057
0048
3
0134
0124
0117
0113
7
0066
0066
0059
0054
7
0121
0114
0116
0110
21
1097
1102
1159
1063
21
1078
1175
1037
0998
41
1104
1095
1183
1090
41
1272
1111
1047
1029
62
1046
1156
1148
1002
62
1119
1167
1050
0989
72
1276
1132
1095
0926
72
1218
1019
1090
1036
SE/SD
11
1157
1133
1101
1051
11
1226
1257
1166
1126
21
1094
1069
1082
0986
21
1050
1130
1072
1067
22
1233
1204
1102
1116
22
1233
1171
1056
1079
1
1599
1384
1372
1236
1
1465
1297
1141
1213
3
1422
1257
1241
1308
3
1296
1223
1170
1119
7
1640
1303
1281
1216
7
1456
1319
1178
1134
under Vjj = 1   8 with different true parameter values are reported in
Tables 4.3 to 4.6, respectively. The following phenomena are observed from
the results on ‘BIAS’ and ‘RMS’ which reveal the accuracy of the estimates: (i)
Even under situations with small sample sizes (n = 2a or 3a, where a is the
number of unknown parameters), the ‘BIAS’ values are acceptable. For instance,
over 70 % of them are less than 0.05, and about 90 % of them are less than 0.1.
These results imply that the Bayesian estimates are close to the true values of
the parameters. (ii) The overall empirical performances in terms of RMS are

92
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
Table
4.4
Performance of the Bayesian approach under V3  kh = 0821 =
02k = 036, and V4  kh = 0821 = 02k = 072.
V3
V4
Par
32
48
64
80
Par
32
48
64
80
21
0036
0034
0028
0037
21
0087
0073
0065
0069
41
0061
0053
0041
0036
41
0086
0076
0044
0051
62
0015
0009
0001
0003
62
0006
0017
0003
0001
72
0031
0012
0003
0003
72
0003
0012
0004
0008
BIAS
11
0004
0039
0002
0016
11
0051
0072
0024
0044
21
0019
0029
0025
0022
21
0040
0035
0016
0018
22
0083
0049
0018
0017
22
0060
0026
0012
0030
1
0039
0031
0023
0024
1
0003
0015
0014
0018
3
0014
0007
0001
0004
3
0021
0032
0010
0032
7
0030
0026
0016
0015
7
0008
0010
0005
0010
21
0146
0121
0101
0098
21
0223
0193
0152
0158
41
0161
0136
0127
0115
41
0242
0225
0195
0172
62
0130
0116
0100
0078
62
0169
0160
0135
0131
72
0131
0108
0099
0093
72
0179
0165
0160
0137
RMS
11
0260
0219
0200
0199
11
0258
0238
0217
0227
21
0182
0141
0138
0123
21
0193
0157
0150
0116
22
0247
0259
0209
0185
22
0287
0258
0228
0214
1
0073
0075
0064
0061
1
0127
0136
0125
0120
3
0066
0061
0051
0055
3
0136
0126
0124
0116
7
0067
0067
0060
0055
7
0120
0137
0119
0121
21
1111
1133
1150
1110
21
1145
1109
1241
1106
41
1197
1160
1025
1012
41
1182
1049
1007
1031
62
1122
1043
1066
1218
62
1261
1134
1186
1098
72
1119
1112
1071
1021
72
1179
1102
1008
1053
SE/SD
11
1187
1177
1155
1034
11
1326
1262
1261
1090
21
1134
1185
1056
1058
21
1142
1148
1063
1244
22
1412
1053
1108
1127
22
1308
1191
1207
1202
1
1717
1355
1391
1385
1
1494
1269
1286
1244
3
1423
1319
1370
1188
3
1294
1236
1125
1122
7
1578
1322
1243
1238
7
1448
1144
1180
1073
acceptable. Together with the findings in (i), we can conclude that the Bayesian
estimates obtained from small samples are fairly accurate. (iii) Comparing V1
with V2, V3 with V4, V5 with V6 and V7 with V8, the overall performances under
V1V3V5 and V7 are better than those under V2V4V6 and V8, respectively.
That is, performances with models having small error measurement variances
are better. Based on the nature of a CFA model, small error variances implies
a better fit of the dependent variables in yi by the independent variables in i.
Hence, it is fairly logical to have better performances when working with better
models. (iv) Comparing V1 with V3, V2 with V4, V5 with V7 and V6 with V8,

4.3
BAYESIAN ESTIMATION OF THE CFA MODEL
93
Table
4.5
Performance of the Bayesian approach under V5  kh = 0421 = 06
k = 036, and V6  kh = 0421 = 06k = 072.
V5
V6
Par
32
48
64
80
Par
32
48
64
80
21
0014
0037
0013
0022
21
0024
0048
0040
0031
41
0021
0025
0007
0021
41
0044
0077
0056
0068
62
0001
0006
0002
0008
62
0021
0001
0020
0003
72
0012
0014
0008
0000
72
0002
0001
0000
0014
BIAS
11
0011
0005
0012
0002
11
0033
0033
0008
0015
21
0151
0100
0086
0070
21
0175
0185
0132
0126
22
0020
0008
0007
0016
22
0074
0020
0011
0026
1
0053
0047
0039
0032
1
0028
0034
0017
0034
3
0007
0009
0004
0006
3
0029
0029
0021
0028
7
0012
0009
0006
0007
7
0021
0010
0002
0026
21
0133
0108
0085
0084
21
0204
0165
0156
0138
41
0150
0145
0121
0121
41
0237
0212
0208
0192
62
0129
0096
0096
0074
62
0192
0155
0147
0131
72
0136
0104
0102
0091
72
0174
0165
0154
0117
RMS
11
0262
0214
0211
0178
11
0293
0248
0235
0222
21
0255
0197
0177
0168
21
0284
0272
0228
0209
22
0245
0216
0194
0189
22
0294
0244
0257
0211
1
0077
0074
0071
0068
1
0115
0142
0129
0130
3
0055
0055
0054
0049
3
0119
0107
0113
0100
7
0064
0053
0052
0048
7
0119
0114
0115
0099
21
1101
1141
1174
1084
21
1054
1168
1034
1045
41
1323
1140
1140
1053
41
1243
1271
1099
1142
62
1065
1166
1014
1180
62
1080
1134
1038
1024
72
1016
1084
0955
0949
72
1199
1061
0993
1150
SE/SD
11
1213
1282
1158
1221
11
1318
1289
1236
1218
21
1118
1143
1107
1015
21
1193
1083
1042
1066
22
1330
1243
1245
1163
22
1390
1351
1131
1311
1
2296
2011
1817
1684
1
2036
1509
1510
1490
3
1527
1339
1199
1213
3
1381
1348
1123
1163
7
1339
1383
1244
1245
7
1365
1228
1096
1173
the general performances of V3V4V7 and V8 are slightly better than those
of V1V2V5 and V6 respectively. Hence, performances of models having small
latent variables correlation are better. This is logical, as the performances are
expected to be better for models with small multicollinearity among indepen-
dent variables. (v) Consider two kinds of indicators (manifest variables) within
each Vj. The first kind involves indicators for just one latent factor 1 or 2,
such as the first three and the last three indicators in the model. The second
kind involves indicators for both 1 and 2, such as the fourth indicator in the
model. The empirical performances of the estimates associated with loadings

94
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
Table
4.6
Performance of the Bayesian approach under V7  kh = 0421 = 02
k = 036, and V8  kh = 0421 = 02k = 072.
V7
V8
Par
32
48
64
80
Par
32
48
64
80
21
0054
0040
0038
0039
21
0052
0063
0071
0057
41
0051
0029
0034
0043
41
0081
0112
0079
0073
62
0002
0027
0022
0028
62
0007
0034
0054
0018
72
0006
0016
0014
0003
72
0010
0037
0045
0033
BIAS
11
0053
0057
0079
0068
11
0067
0090
0112
0088
21
0045
0036
0049
0016
21
0074
0056
0053
0042
22
0011
0055
0035
0020
22
0051
0084
0057
0053
1
0072
0066
0057
0058
1
0054
0067
0081
0078
3
0011
0001
0002
0000
3
0050
0025
0016
0033
7
0013
0005
0014
0008
7
0034
0030
0029
0015
21
0146
0121
0117
0103
21
0200
0191
0175
0161
41
0161
0112
0110
0103
41
0253
0212
0175
0171
62
0116
0108
0095
0083
62
0202
0168
0159
0139
72
0132
0107
0098
0078
72
0190
0171
0163
0137
RMS
11
0243
0235
0214
0200
11
0261
0246
0226
0242
21
0181
0160
0149
0136
21
0186
0171
0166
0158
22
0257
0215
0198
0201
22
0225
0247
0231
0225
1
0099
0094
0088
0090
1
0141
0142
0162
0143
3
0067
0055
0049
0051
3
0126
0129
0106
0107
7
0061
0059
0056
0052
7
0123
0124
0106
0104
21
1120
1093
0989
1025
21
1213
1094
1115
1061
41
1115
1256
1133
1128
41
1098
1251
1269
1167
62
1248
1169
1138
1196
62
1125
1199
1115
1080
72
1106
1138
1083
1181
72
1224
1170
1063
1148
SE/SD
11
1317
1171
1173
1142
11
1419
1372
1450
1198
21
1178
1086
1053
1012
21
1327
1165
1074
1020
22
1251
1274
1247
1113
22
1643
1342
1315
1262
1
2013
1872
1715
1594
1
1799
1760
1512
1689
3
1292
1312
1324
1143
3
1359
1112
1225
1120
7
1436
1225
1221
1157
7
1333
1157
1223
1122
(here, 2162 and 72) that belong to the first kind indicators are better than
those belonging to the second kind (41 in this case). This indicates the expected
result that the complexity of the factor loading structure has some effect on the
accuracy of the estimates. (vi) The empirical performances of the estimates for
the variance and covariance (1112 and 22) of the latent factors are not as
good as the others.
The ratios SE/SD corresponding to most of the parameters having different
magnitudes in V1   V8 are over 1.0. Based on the definitions of SE(	h) and
SD(	h), the sample standard deviation of ˆ	rhr = 1   200 is smaller than

4.4
BAYESIAN ESTIMATION OF STANDARD SEMs
95
the mean of the numerical standard error estimates computed from the simulated
observations via Equation (4.18). That is, the variability of the Bayesian estimates
is relatively small. This may be regarded as a good property of the Bayesian esti-
mates from an estimation point of view. On the other hand, the numerical stan-
dard error estimates of the Bayesian approach produced by Equation (4.18) are
overestimated. However, as there are not too many ratios that are over 1.5, partic-
ularly with n = 4a or 5a, the induced impact is not very substantial. Moreover, it
shouldbenotedthattheuseofthenumericalstandarderrorestimatesinaBayesian
approach is not as important as in the ML approach. For instance, we do not use
them in model comparison or hypothesis testing.
Lee and Song (2004) also provided simulation results that were obtained by
the ML method in the context of the CSA approach. Based on their results,
they arrived at the same conclusion given by many previous studies in the
literature that the ML–CSA approach is not recommended for situations with
small sample sizes.
4.4
BAYESIAN ESTIMATION OF STANDARD SEMs
In this section, we further illustrate the Bayesian estimation by considering a
standard SEM that is equivalent to the most commonly used LISREL model.
It is composed of a measurement equation and a structural equation. The
measurement equation is basically equivalent to the following CFA model as
given in Equation (4.21):
yi = i +i
(4.36)
with the same definition and assumption for , i and i, except that the
covariance matrix of i is no longer . Let i = 
T
i T
i T be a partition of
i into an q1 ×1 dependent latent vector 
i, and an q2 ×1 independent latent
vector i. The structural equation for assessing the relationship between 
i and
i is given by

i = 
i +i +i
(4.37)
where q1 ×q1 and q1 ×q2 are unknown parameter matrices of regression
coefficients, and i is a q1 × 1 random vector of error measurements. It is
assumed that i is distributed as N 0, and i is distributed as N 0,
where  is a diagonal matrix and i and i are independent. For simplicity,
it is further assumed that 0 = I − is a positive constant independent with
elements in . This model is not identified, but it can be identified by restricting
appropriate elements in ,  and/or  at fixed known values.

96
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
Let Y = y1   yn and 	 = 1   n, and let  be the vector of
unknown parameters in , , , ,  and . In the posterior analysis,
we augment the observed data Y with the matrix of latent variable 	, and
consider the joint posterior distribution 	Y. Again, a sufficiently large
number of observations are generated from this posterior distribution by the
Gibbs sampler, which is implemented as follows. At the j +1th iteration with
current values of 	j and j:
i
Generate 	j+1 from p 	jY
ii
Generate j+1 from p 	j+1Y
(4.38)
Note that  involves components that correspond to , , , ,  and .
This Gibbs sampler is similar to the one for the CFA model, except that more
components corresponding to the additional parameters are involved.
The SEM defined by Equations (4.36) and (4.37) is a straightforward gener-
alization of the CFA model through an additional structural Equation (4.37).
The conditional distributions involved in the Gibbs sampler that correspond to
the measurement equation are very similar to those that are associated with the
CFA model. This is also true for the structural equation, because it is essentially
a regression model or a factor analysis model with latent variables. Hence, the
generalization of the conditional distributions in the Gibbs sampler for a CFA
model to an SEM does not involve too much difficulty.
Under the similar definition and assumption, p 	Y can be expressed
as in Equation (4.23), with the conditional distribution of i given yi is
similarly given as in Equation (4.24). However, in Equation (4.24)  should
be replaced by the following covariance matrix of , , which is derived on
the basis of the SEM defined in Equations (4.36) and (4.37):
 =
⎡
⎣
−1
0 T +−T
0
−1
0 
T −T
0

⎤
⎦
(4.39)
The
conditional
distribution
of

given
Y	
is
proportional
to
p p Y	. We note that when 	 is given, Equations (4.36) and (4.37) are
regression models. Let y be the unknown parameters in  and  associated
with the measurement equation; and  be the unknown parameters in  
and  associated with the structural model with the latent variables. It is natural
to assume that the prior distribution of y is independent of the prior distribu-
tion of , that is, p  = p yp . Moreover, p Y	 = p Y	y and
p 	 = p 	. Hence,
p yY	 ∝p Y	yp yp 	p 

4.4
BAYESIAN ESTIMATION OF STANDARD SEMs
97
Since the first term of the product on the right-hand side depends only on y,
whereas the second term depends only on , the marginal conditional densities
y and  are proportional to p Y	yp y and p 	p , respectively.
Consequently, these conditional densities can be treated separately.
The marginal conditional distribution of y is p Y	. Under the
conjugate prior distributions given in Equation (4.27), this conditional distri-
bution can be obtained from Equation (4.28).
Now, consider the conditional distribution of  that is proportional to
p 	p . Let 	1 = 
1   
n and 	2 = 1   n. Since the distri-
bution of i only involves , p 	2 = p 	2. Moreover, it is natural to
assume that the prior distribution of  is independent of the prior distributions
of ,  and . Hence,
p 	p  = p 	1	2p p 	2p 
and the marginal conditional densities of  and  can be treated
separately.
Consider a conjugate type prior distribution for  with −1 D= Wq2R00,
a Wishart distribution with hyperparameters 0 and a positive definite matrix
R0. It can be shown by reasoning similar to that used in Section 4.3.1 that
p 	2p  ∝−n+0+q2+1/2 exp

−1
2 tr−1	2	T
2 +R −1
0 


Thus, the conditional distribution of  given 	2 is given by
	2
D= IWq2	2	T
2 +R −1
0 n +0
(4.40)
Rewrite Equation (4.37) as 
i = i + i, where  = . This is very
similar to a factor analysis model, and when 	 is given this is a regression
model. Let k be the kth diagonal element of , and T
k be the kth row of
. The prior distributions of (k k) are similarly selected via the following
conjugate type distributions:
−1
k
D= Gamma
0k0k and
kk
D= N 0kkH0kk = 12   k1
(4.41)

98
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
where 
0k, 0k and H0k are given hyperparameters. Moreover, it is assumed
that for h ̸= k, kk and hh are independent. Then, following the
same reasoning as before, it can be shown that:
−1
k 	
D= Gamma n/2+
0kk and
k	−1
k 
D= N akkAk
(4.42)
where Ak = H−1
0k + 		T −1
ak = AkH−1
0k0k + 		1k and k =
0k + 1
2	T
1k	1k −aT
kA−1
k ak + T
0kH−1
0k0k, in which 	T
1k is the kth
row of 	1.
The situation with fixed parameters can be handled in a similar way to
Section 4.3.1. See also Appendix 5.1 for the treatment of fixed parameters in
an SEM with fixed covariates.
It should be emphasized that the Bayesian analysis of the CFA model and
the SEM are based on the basic model of the raw individual observations,
rather than the sample covariance matrix. Again, estimates of the latent variables
can be easily obtained through the observations of 	 simulated by the Gibbs
sampler.
4.5
BAYESIAN ESTIMATION VIA WINBUGS
The freely available software WinBUGS (Windows version of Bayesian infer-
ence Using Gibbs Sampling) is useful to produce reliable Bayesian statistics for
a very wide range of statistical models. The algorithm used in WinBUGS is
mainly developed using MCMC techniques, such as the Gibbs sampler (Geman
and Geman, 1984) and the MH algorithm (Metropolis et al., 1953; Hast-
ings, 1970). It has been shown that under broad conditions, this software can
provide simulated samples from the joint posterior distribution of the unknown
quantities, such as parameters and latent variables in the model. As discussed
in previous sections, Bayesian estimates of the unknown parameters and their
standard error estimates in the model can be obtained from these samples for
conducting statistical inferences.
The advanced version of the program is WinBUGS 1.4 running under
Windows, which is developed by the Medical Research Council (MRC)
Biostatistics Unit (Cambridge, UK) and the Department of Epidemiology
and Public Health of the Imperial College School of Medicine at St Mary’s
Hospital (London). It can be downloaded from the website http://www.mrc-
bsu.cam.ac.uk/bugs/. The free version of WinBUGS is a restricted version;
one needs to email the BUGS project for a key that will let the user use the
full version. The WinBUGS Manual (Spiegelhalter, Thomas, Best and Lunn,
2003), which gives brief instructions on WinBUGS, is available online. See

4.5
BAYESIAN ESTIMATION VIA WINBUGS
99
also Lawson, Browne and Vidal Rodeiro (2003, Chapter 4) for supplementary
descriptions.
The following LISREL type model is used to illustrate the use of WinBUGS
in conducting Bayesian analysis of SEMs. The measurement equation of the
model is defined by nine manifest variables in yi = yi1   yi9T and three
latent variables in i = ii1i2T as follows:
yi1 = 
1 +i +i1 yik = 
k +k1i +ik
k = 23
yi4 = 
4 +i1 +i4 yik = 
k +k2i1 +ik k = 56
yi7 = 
7 +i2 +i7 yik = 
k +k3i2 +ik k = 89
(4.43)
where ikk = 1   p, is independently distributed as N 0k, and is also
independent with i. The structural equation is defined by
i = 1i1 +2i2 +i = i +i
(4.44)
where i = i1i2T is distributed as N 0, and i is distributed as
N 0, i and i are independent. An artificial random sample of size n = 300
is simulated via the following true population values of the free parameters:

1 = ··· = 
9 = 00 21 = 52 = 83 = 09
31 = 62 = 93 = 07
1 = 2 = 3 = 03 4 = ··· = 7 = 058 = 9 = 04
1 = 062 = 04
11 = 22 = 1012 = 03 and  = 036
(4.45)
where 1121 and 22 are distinct elements in .
The following conjugate prior distributions are used in the Bayesian estima-
tion. For k = 1   p,

D= N 0I−1 D= W2I5
−1
k
D= Gamma 94
k
D= N 0kkI
−1

D= Gamma 94

D= N 0I
(4.46)
where elements in 0k and 0 are taken to be 0.8, and 0.5, respectively, and I′s
are the identity matrices of appropriate orders.

100
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
lambda[5,2] chains 1:3
iteration
1
1000
2000
3000
4000
–0.5
0.0
0.5
1.0
gamma[1] chains 1:3
iteration
1
1000
2000
3000
4000
–0.5
0.0
0.5
1.0
psi[6] chains 1:3
iteration
1
1000
2000
3000
4000
0.0
0.5
1.0
1.5
phi[1,2] chains 1:3
iteration
1
1000
2000
3000
4000
–1.0
–0.5
0.0
0.5
1.0
psi_delta chains 1:3
iteration
1
1000
2000
3000
4000
0.0
5.0
10.0
Figure
4.3
Three chains of observation corresponding to 52, 1, 6, 12 and 
generated by different initial values.

4.5
BAYESIAN ESTIMATION VIA WINBUGS
101
To specify the model in the WinBUGS language, the measurement Equa-
tion (4.43) is reformulated as: yik
D= N ikk, where
i1 = 
1 +i ik = 
k +k1i k = 23
i4 = 
4 +i1 ik = 
k +k2i1 k = 56
i7 = 
7 +i2 ik = 
k +k3i2 k = 89
and the structural Equation (4.44) is reformulated by defining the conditional
distribution of i given i1 and i2 as N  i, where
 i = 1i1 +2i2
(4.47)
WinBUGS was used to analyze the artificial data with n = 300 which were
simulated according to the model defined above, with true parameter values
given in Equation (4.46). The WinBUGS codes together with some instructions
are given in the following website: http://www.wiley.com /go/lee_structural.
The artificial data set is also given in this website. The hyperparameter values
are given in Equation (4.45). Plots of sequences of observations corresponding
to some parameters are displayed in Figure 4.3. These plots indicate that the
algorithm converged in less than 1000 iterations. We discard 2000 burn-in
Table
4.7
Bayesian estimates (EST) and their standard error (SE) of the artificial
example obtained through WinBUGS.
Par
True value
EST
SE
Par
True value
EST
SE

1
00
−0005
0068
1
03
0376
0041

2
00
0.007
0069
2
03
0331
0040

3
00
−0020
0052
3
03
0279
0028

4
00
0.120
0072
4
05
0462
0063

5
00
0.106
0063
5
05
0496
0053

6
00
0.019
0057
6
05
0479
0047

7
00
0.120
0070
7
05
0514
0067

8
00
0.047
0061
8
04
0471
0053

9
00
0.040
0052
9
04
0371
0039
21
09
1.042
0055
1
06
0501
0058
31
07
0.741
0045
2
04
0440
0060
52
09
0.794
0061
11
10
1089
0134
62
07
0.684
0055
21
03
0346
0077
83
09
0.841
0070
22
10
0964
0127
93
07
0.702
0056

036
0357
0047

102
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
30
20
10
0
60
40
20
0
50
60
40
–3
–2
–1
0
1
2
3
(a)
–3
–2
–1
0
1
2
3
(b)
Figure
4.4
Histograms of the latent variables: (a) ˆi1, and (b) ˆi2.
iterations, and use T ∗= 2000 observations to obtain the Bayesian results, see
Equation (4.17) to (4.20). The Bayesian estimates and their standard error
estimates are given in Table 4.7. We observe that the Bayesian estimates are
reasonably close to their true values and the standard error estimates are reason-
able. The sample variances and covariance of ˆi1 and ˆi2 are equal to 0.932,
0.818 and 0.354, respectively; they are reasonably close to the true values of
11 = 22 = 1012 = 03. Histograms that correspond to the set of latent
variable estimates ˆi1 and ˆi2 are shown in Figure 4.4. These histograms indicate
that the distributions of ˆi1, and ˆi2 are close to normal. The estimated residual
plots of ˆi1 ˆi5 and ˆi against the case number are displayed in Figure 4.5, and
plots of estimated residual ˆi1 versus ˆi1 ˆi2 and ˆi, ˆi versus ˆi1 and ˆi2 are

4.5
BAYESIAN ESTIMATION VIA WINBUGS
103
hat{epsilon}_1
(a)
0
50
100
150
200
250
300
(b)
0
50
100
150
200
250
300
(c)
0
50
100
150
200
250
300
–2
–1
0
2
1
–2
–1
0
2
1
hat{epsilon}_5
–2
–1
0
2
1
hat{delta}
Figure
4.5
Estimated residual plots: (a) ˆi1, (b) ˆi5 and (c) ˆi.
shown in Figures 4.6 and 4.7 respectively. Other residual plots are similar but
they are not presented to save space. These plots lie within two parallel hori-
zontal lines that are centered at zero, and no linear or quadratic trends are
detected. This indicates that the measurement model and the structural equa-
tion of the proposed model adequately fit the data. Basically, these residual
plots are interpreted in a similar way to a regression model and may be used for
outlier analysis.

104
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
(a)
–2
–1
0
1
2
3
(c)
–3
–2
–1
0
1
2
(b)
–2
–1
0
1
2
hat{epsilon}_1
3
2
1
0
–1
–2
–3
hat{epsilon}_1
3
2
1
0
–1
–2
–3
hat{epsilon}_1
3
2
1
0
–1
–2
–3
Figure
4.6
Plots of estimated residual ˆi1 versus: (a) ˆi1, (b) ˆi2 and (c) ˆi.
APPENDIX 4.1: THE METROPOLIS–HASTINGS ALGORITHM
Suppose we wish to simulate observations say Xjj = 12    from a
conditional distribution with target density ·. At the jth iteration of the
Metropolis–Hastings (MH) algorithm with a current Xj, the next Xj+1 is chosen
by first sampling a candidate point Y from a proposal distribution q·Xj which
is easy to sample. This candidate point Y is accepted as Xj+1 with probability
min

1
YqXjY
XjqY Xj



APPENDIX 4.2: EPSR VALUE
105
(a)
–2
–1
0
1
2
3
(b)
–2
–1
0
1
2
hat{delta} 
3
2
1
0
–1
–2
–3
hat{delta}
3
2
1
0
–1
–2
–3
Figure
4.7
Plots of estimated residual ˆi versus: (a) ˆi1 and (b) ˆi2.
If the candidate point Y is rejected, then Xj+1 = Xj and the chain does not
move.
The proposal distribution q·· can have any form and the stationary distri-
bution of the Markov chain will be the target distribution with density ·.
In most analyses of SEMs considered in this book, we will take q·X to be a
normal distribution with mean X and some covariance matrix.
APPENDIX 4.2: EPSR VALUE
Assessing convergence of a Monte Carlo simulation procedure should be on
the basis of several simulation sequences generated independently via different
starting values. The following approach (Gelman, 1996) involves monitoring
each scalar estimate (e.g. parameter) of interest separately. Let n be the length
of each sequence, after discarding the first part of the simulations. For each
scalar estimate, say , let jkj = 1   nk = 1   K be the draws from K
parallel sequences of length n. The between- and within-sequences variances
are computed as
B =
n
K −1
K
k=1
·k −··2 where ·k = n−1
n
j=1
jk ·· = K −1
K
k=1
·k
W = 1
K
K
k=1
s 2
k  where s 2
k = n −1−1
n
j=1
jk −·k2

106
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
The estimate of varY, the marginal posterior variance of the estimate, is
then obtained by a weighted average of B and W as follows

Var = n −1
n
W + 1
n B
The ‘estimated potential scale reduction (EPSR)’ is defined as
ˆR1/2 = 
Var/W 1/2
As the simulation converges, ˆR1/2 should be close to 1.0. In monitoring
convergence, all EPSR values for all scalar estimates are computed. The whole
simulation procedure is said to be converged if all the EPSR values are less
than 1.2.
APPENDIX 4.3: DERIVATIONS OF CONDITIONAL
DISTRIBUTIONS
To simplify notation in the derivation of pkkY	, we let  k = −1
k .
From Equations (4.28), the conjugate prior density of  k, and the conju-
gate prior density of k given  k, are proportional to  
0k−1
k
exp−0k k
and  
q/2
k
exp−1
2k −0kT H−1
0ykk −0yk k, respectively. Also, from Equa-
tion (4.21), it can be seen that the likelihood of Y is given by
pY	 ∝−n/2 exp

−1
2
n
i=1
yi −iT  −1
 yi −i


Let YT
k
be the kth row of Yyik be the ith component of YT
k A∗
k =
		T −1	Yk, and bk = YT
k Yk −YT
k 	T 		T −1	Yk = YT
k Yk −A∗T
k 		T A∗
k,
the exponential term in pY	 can be expressed as
−1
2
n
i=1
yi −iT  −1
 yi −i = −1
2
n
i=1
p
k=1
−1
k yik −T
k i2
=−1
2
p
k=1
	
 k

n
i=1
y2
ik −2T
k
n
i=1
ykii +trkT
k
n
i=1
iT
i 

=−1
2
p
k=1

 k

YT
k Yk −2T
k 	Yk +T
k 		T k


APPENDIX 4.3: DERIVATIONS OF CONDITIONAL DISTRIBUTIONS
107
=−1
2
p
k=1

 k

YT
k Yk −YT
k 	T 		T −1	Yk

+  k

k −		T −1	Yk
T 		T 

k −		T −1	Yk


=−1
2
p
k=1
 kbk +k −A∗
kT 		T k −A∗
k
Therefore, it follows from the likelihood of Y and the conjugate density of k
and vk that:
p 1    pY	 ∝
p
k=1

 
n/2+q/2+
0k−1
k
×exp

−1
2 kk −A∗
kT 		T k −A∗
k
+k −0kT H−1
0ykk −0k− k0k +bk/2

=
p
k=1
pk kY	
From the above equation, it can be seen that the conditional distributions of
k k given Y! are mutually independent for k = 1··· p. Hence, it is
sufficient to derive pk kY	.
Let Ak = H−1
0yk +		T −1 and ak = AkH−1
0yk0k +	Yk, it follows that
k −A∗
kT 		T k −A∗
k+k −0kT H−1
0ykk −0k
=k −akT A−1
k k −ak−aT
k A−1
k ak +A∗T
k 		T A∗
k +T
0kH−1
0yk0k
Hence
pk kY	 = p kY	 pkY	 k
∝

 n/2+
0k−1
k
exp−k k

·

 
q/2
k
exp −1
2k −akT A−1
k k −ak k

where k = 0k + 2−1YT
k Yk −aT
k A−1
k ak + T
0kH−1
0yk0k. Thus, the posterior
distribution of k k given Y and 	 is the following normal–gamma distri-
bution (Broemeling, 1985):
 kY	
D= Gamman/2+
0k k and kY	 k
D= Nak  −1
k Ak

108
4
BAYESIAN ESTIMATION OF STRUCTURAL EQUATION MODELS
REFERENCES
Albert, J. H. and Chib, S. (1993) Bayesian analysis of binary and polychotomous response
data. Journal of the American Statistical Association, 88, 669–679.
Ansari, A. and Jedidi, K. (2000) Bayesian factor analysis for multilevel binary observa-
tions. Psychometrika, 65, 475–498.
Ansari, A., Jedidi, K. and Dube, L. (2002) Heterogeneous factor analysis models: A
Bayesian approach. Psychometrika, 67, 49–78.
Ansari, A., Jedidi, K. and Jagpal, S. (2000) A hierarchical Bayesian methodology for
treating heterogeneity in structural equation models. Marketing Science, 19, 328–347.
Bartholomew, D. J. (1981) Posterior analysis of the factor model. British Journal of
Mathematics and Statistical Psychology, 34, 93–99.
Berger, J. O. (1985) Statistical Decision Theory and Bayesian Analysis. New York:
Springer-Verlag.
Boomsma, A. (1982) The robustness of LISREL against small sample sizes in factor
analysis model. In K. G. Jörkog and H. Wold (eds), System under Indirect Observation:
Causality, Structure, Prediction pp. 149–173. Amsterdam: North-Holland.
Box, G. E. P. and Tiao, G. C. (1973) Bayesian Inference in Statistical Analysis. Reading,
MA: Addison-Wesley.
Broemeling, L. D. (1985) Bayesian Analysis of Linear Models. New York: Marcel
Dekker Inc..
Chou, C. P., Bentler, P. M. and Satorra, A. (1991) Scaled test statistics and robust
standard errors for non-normal data in covariance structure analysis: A Monte Carlo
study. British Journal of Mathematical and Statistical Psychology, 44, 347–357.
Congdon, P. (2003) Applied Bayesian Modeling. Hoboken, New York: John Wiley &
Sons, Inc..
Dempster, A. P., Laird, N. M. and Rubin, D. B. (1977) Maximum likelihood from
incomplete data via the EM algorithm (with discussion). Journal of the Royal Statistical
Society, Series B, 39, 1–38.
Dunson, D. B. (2000) Bayesian latent variable models for clustered mixed outcomes.
Journal of the Royal Statistical Society, Series B, 62, 355–366.
Fuller, W. A. (1987) Measurement Error Models. New York: John Wiley & Sons, Inc..
Gelfand, A. E. and Smith, A. F. M. (1990) Sampling-based approaches to calculating
marginal densities. Journal of the American Statistical Association, 85, 398–409.
Gelman,
A.
(1996)
Inference
and
monitoring
convergence.
In
W.
R.
Gilks,
S. Richardson, and D. J. Spiegelhalter (eds), Markov Chain Monte Carlo in Practice,
pp. 131–144. London: Chapman and Hall.
Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. (1995) Bayesian Data Analysis.
London: Chapman & Hall Ltd.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distribution and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Geyer, C. J. (1992) Practical Markov chain Monte Carlo. Statistical Science, 7, 473–511.
Hastings, W. K. (1970) Monte Carlo sampling methods using Markov chains and their
application. Biometrika, 57, 97–109.
Hoogland, J. J. and Boomsma, A. (1998) Robustness studies in covariance structure
modeling: an overview and a meta analysis. Sociological Methods & Research, 26, 329–
368.

REFERENCES
109
Hu, L., Bentler, P. M. and Kano, Y. (1992) Can test statistics in covariance structure
analysis be trusted. Psychological Bulletin, 112, 351–362.
Jöreskog, K. G. and Sörbom, D. (1996) LISREL 8: Structural Equation Modeling with the
SIMPLIS Command Language. Hove and London: Scientific Software International.
Kass, R. E. and Raftery, A. E. (1995) Bayes factors. Journal of the American Statistical
Association, 90, 773–795.
Lawson, A. B., Browne, W. J. and Vidal Rodeiro, C. L. (2003) Disease Mapping with
WinBUGS and MLWIN. Cluchester: John Wiley & Sons, Ltd.
Lee, S. Y. (1980) Estimation of covariance structure models with parameters subject to
functional restraints. Psychometrika, 45, 309–324.
Lee, S. Y. (1981) A Bayesian approach to confirmatory factor analysis. Psychometrika,
46, 153–160.
Lee, S. Y. and Shi, J. Q. (2000) Joint Bayesian analysis of factor scores and structural
parameters in the factor analysis model. Annals of the Institute of Statistical mathe-
matics, 52, 722–736.
Lee, S. Y. and Song, X. Y. (2004) Evaluation of the Bayesian and maximum likeli-
hood approaches in analyzing structural equation models with small sample sizes.
Multivariate Behavioral Research. 39, 653–686.
Lee, S. Y. and Zhu, H. T. (1999) Statistical analysis of nonlinear factor analysis models.
British Journal of Mathematical and Statistical Psychology, 52, 225–242.
Lindley, D. V. and Smith, A. F. M. (1972) Bayes estimates for the linear model (with
discussion). Journal of the Royal Statistical Society, Series B, 1–42.
Martin, J. K. and McDonald, R. P. (1975) Bayesian estimation in unrestricted factor
analysis: a treatment for Heyword cases. Psychometrika, 40, 505–577.
Metropolis, N. et al. (1953) Equations of state calculations by fast computing machine.
Journal of Chemical Physics, 21, 1087–1091.
Muirhead, R. J. (1982) Aspects of Multivariate Statistical Theory. New York:
John Wiley & Sons, Inc..
Rubin, D. B. (1991) EM and beyond. Psychometrika, 56, 241–254.
Scheines, R., Hoijtink, H. and Boomsma, A. (1999) Bayesian estimation and testing of
structural equation models. Psychometrika, 64, 37–52.
Shi, J. Q. and Lee, S. Y. (1998) Bayesian sampling-based approach for factor analysis
model with continuous and polytomous data. British Journal of Mathematical and
Statistical Psychology, 51, 233–252.
Song, X. Y. and Lee, S. Y. (2001) Bayesian estimation and test for factor analysis
model with continuous and polytomous data in several populations. British Journal of
Mathematical and Statistical Psychology, 54, 237–263.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. (2003) WinBUGS User
Manual. Version 1.4. Cambridge, England: MRC Biostatistics Unit.
Tanner, M. A. and Wong, W. H. (1987) The calculation of posterior distributions by
data augmentation(with discussion). Journal of the American Statistical Association,
82, 528–550.
Yung, Y. F. and Bentler, P. M. (1994) Bootstrap-corrected ADF test statistics in covari-
ance structure analysis. British Journal of Mathematical and Statistical Psychology, 47,
63–84.
Zeger, S. L. and Karim, M. R. (1991) Generalized linear models with random effects: A
Gibbs sampling approach. Journal of the American Statistical Association, 86, 79–86.
Zellner, A. (1971) An Introduction to Bayesian Inference in Econometrics. New York:
John Wiley & Sons, Inc..


5
Model Comparison and
Model Checking
5.1
INTRODUCTION
One important statistical inference beyond estimation is on testing of various
hypotheses about the model. In the field of structural equation modeling, a
common approach for hypothesis testing is to use the significance tests on the
basis of p-values that are determined by some asymptotic distributions of the
test statistics. As pointed out in the statistics literature (see e.g. Berger and
Sellke, 1987; Berger and Dalampady, 1987; Kass and Raftery, 1995) there are
problems associated with such an approach. Those which are related to SEMs
are discussed as follows:
(i)
Tests on the basis of p-values tend to reject the null hypothesis too
frequently with large sample sizes. A dramatic example with a sample size
113 556 was given by Raftery (1986), where a substantively meaningful
model (associated with the null hypothesis) that explained 99.7 % of the
deviance was rejected by a standard chi-squared test with an extremely
small p-value. In the traditional analysis of SEMs, various descriptive
fit indexes, such as the well-known normed or non-normed fit indexes
(Bentler and Bonett, 1980) and the comparative fit index (Bentler, 1992)
have been proposed as complementary measures for the goodness-of-fit
of the model. Very often, the values of the fit indexes are over 0.95, but
the p-values of the 2-test are less than 0.01. Under these situations, the
conclusions drawn from these two testing methods seem contradictory.
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

112
5
MODEL COMPARISON AND MODEL CHECKING
(ii)
The p-value of a significance test in hypothesis testing is a measure of
evidence against the null model, not a means of supporting/proving the
model. Hence, the conclusion of a significance test can only be used to
reject the null hypothesis and cannot offer an assessment of the strength
of the evidence in favor of the null hypothesis. As a result, even the chi-
square goodness-of-fit test does not reject the null hypothesis, nor can it
be used to conclude that the posited model is better than the alternative
model, or to conclude that the given data support the posited model. On
the other hand, rejection of the null hypothesis by such a test does not
indicate the alternative model is better.
(iii)
The significance tests, as well as the descriptive fit indexes mentioned
above, cannot be applied to test non-nested hypotheses or to compare
non-nested models. Therefore only a hierarchy of nested hypotheses can
be assessed see, for example, Bollen (1989). However, we are very often
interested in assessing non-nested SEMs in practical applications.
In this chapter, we consider a Bayesian approach for hypothesis testing that
does not have the above problems. As we can associate a hypothesis with a
model, testing the null hypothesis H0 against its alternative H1 can be regarded
as comparing two models corresponding to H0 and H1. Hence we will use
the general term ‘model comparison’ to represent hypothesis testing, model
comparison and/or model selection. In the field of SEM, we are often inter-
ested in comparing a discrete set of competing models. Typical examples are
comparing an FA model with three factors with an FA model with four factors,
comparing an SEM with an interaction term of exogenous latent variables with
one without the interaction term, comparing a mixture SEM with two compo-
nents with one with three components, and so on. The well-known statistic
in Bayesian model comparison, namely the Bayes factor (Berger, 1985; Kass
and Raftery, 1995), will be emphasized and applied to the problem of model
comparison. Other methods that emphasize for comparing continuous fami-
lies of models (see Gelman, Carlin, Stern and Rubin 2003, and the references
therein) are not considered.
In general, the computation of the Bayes factor is difficult. Various compu-
tational methods have been proposed (Kass and Raftery, 1995). A simple but
rough approximation, namely the Bayesian Information Criterion (BIC) has
been used for model comparison of some SEMs. For example, Raftery (1993)
applied it to the LISREL model, Lee and Song (2001) applied it to a two-
level SEM, and Jedidi, Jagpal, and DeSarbo (1997) applied it to finite mixtures
of SEMs with a fixed number of components, among others. Other useful
methods for computing the Bayes factor have been established on the basis
of posterior simulation, using recently developed MCMC methods. DiCiccio,
Kass, Raftery and Wasserman (1997) provided a comparative study on a variety
of methods, from Laplace approximation to importance sampling and bridge
sampling, and concluded that bridge sampling is an attractive method. Gelman

5.2
BAYES FACTOR
113
and Meng (1998) showed that path sampling is a direct extension of the bridge
sampling. Naturally, it is expected that path sampling is even better. In this
book, we emphasize the application of path sampling (Gelman and Meng,
1998) to compute Bayes factors for model comparisons of various SEMs.
Differing from estimation, Bayesian model comparison using the Bayes factor
may be sensitive to prior distributions of the parameters. Hence, these distri-
butions should be selected with care, and sensitivity analysis of the prior inputs
should be conducted.
An introduction of the Bayes factor will be presented in Section 5.2, followed
by the discussion of path sampling for computing this statistic in Section 5.3.
Section 5.4 provides an application of the methodology to SEMs with fixed
covariates. Some other methods for model comparison and model checking are
given in Section 5.5, and a discussion is given in Section 5.6.
5.2
BAYES FACTOR
In this section, we introduce an important Bayesian statistic, the Bayes factor
(Berger, 1985; Kass and Raftery, 1995), for model comparison/selection. This
statistic has a solid logical foundation that offers great flexibility. It has been
extensively applied to a lot of statistical models, see the references given in
Kass and Raftery (1995). Its applicability is further enhanced by the powerful
MCMC methods that have been recently developed in statistical computing.
Suppose the given data Y with a sample size n have arisen under one of the
two competing models M1 and M0 according to a probability density pYM1
or pYM0, respectively. Let pM0 be the prior probability of M0 and pM1 =
1−pM0, and let pMkY be the posterior probability, for k = 01. From the
Bayes theorem, we obtain
pMkY =
pYMkpMk
pYM1pM1+pYM0pM0
k = 01
Hence,
pM1Y
pM0Y = pYM1 pM1
pYM0 pM0
(5.1)
The Bayes factor for comparing M1 and M0 is defined as
B10 = pYM1
pYM0
(5.2)

114
5
MODEL COMPARISON AND MODEL CHECKING
From Equation (5.1), we see that
posterior odds = Bayes factor ×prior odds
In the special case where the competitive models M1 and M0 are equally probable
a priori so that pM1 = pM0 = 05, the Bayes factor is equal to the posterior
odds in favor of M1. In general, it is a summary of evidence provided by the data
in favor of M1 as opposed to M0, or in favor of M0 as opposed to M1. It may
reject a null hypothesis associated with M0, or may equally provide evidence in
favor of the null hypothesis or the alternative hypothesis associated with M1.
Moreover, unlike the significance test approach that is based on the likelihood
ratio criterion and its asymptotic chi-square statistic, the comparison does not
depend on the assumption that either model is ‘true’. Moreover, it can be seen
from Equation (5.2) that the same data set is used in the comparison, hence,
it does not favor the alternative hypothesis (or M1) in extremely large samples.
Finally, it can be applied to compare non-nested models M0 and M1.
According to the suggestion given in Kass and Raftery (1995), the criterion
that is given in Table 5.1 is used for interpreting B10 and 2log B10. Kass and
Raftery (1995) pointed out that these categories furnish appropriate guidelines
for practical applications of the Bayes factor. Depending on the competing
models M0 and M1 for fitting a given data set, if the Bayes factor (or 2log
Bayes factor) rejects the null hypothesis H0 that is associated with M0, we can
conclude that the data give evidence to support the alternative hypothesis H1
that is associated with M1. Similarly if the Bayes factor rejects H1, a more definite
conclusion of supporting H0 can be attained.
The interpretation of evidence provided by Table 5.1 depends on the specific
context. For two non-nested competitive models, say M1 and M0, we should
select M0 if 2log B10 is negative. If 2log B10 is in (0, 2), we may interpret M1
is slightly better than M0 and hence it may be better to select M1. The choice
of M1 is more definite if 2log B10 is larger than 6. For two nested competitive
models, say M0 is nested in the more complicated model M1, if M1 is significantly
better than M0, it can be much larger than 6. Then the above criterion will
suggest a decisive conclusion to select M1. However, if 2log B10 is in (0, 2), then
the difference between M0 and M1 is ‘not worth more than a bare mention’.
Table
5.1
Interpretation of Bayes factor.
B10
2log B10
Evidence against H0 M0
< 1
< 0
Supports H0M0
1 to 3
0 to 2
Not worth more than a bare mention
3 to 20
2 to 6
Positive
20 to 150
6 to 10
Strong
> 150
> 10
Decisive

5.3
PATH SAMPLING
115
Under this situation, great caution should be taken in drawing a conclusion.
According to the ‘parsimonious’ guideline in practical applications, it may be
desirable to select M0 if it is simpler than M1. The criterion given in Table 5.1
is a suggestion, it is not necessary to regard it as a strict rule. Similarly in
frequentist hypothesis testing, one may take the type I error to be 0.05 or 0.10,
and the choice is decided with other factors in the substantive situation. Similar
to other data analyses, for conclusions drawn from the marginal cases, it is
always helpful to conduct other analysis, for example residual analysis, to cross-
validate the results. Generally speaking, model selection should be approached
on a problem-by-problem basis. It is also desirable to take the opinions from
experts into account if no clear conclusion can be drawn.
From Equation (5.2), we see that the density pYMk is involved in the Bayes
factor. This function is obtained by integrating pYkMkpkMk over the
parameter space, that is
pYMk =

pYkMkpkMkdk
(5.3)
wherek istheparametervectorinMkpkMkisitspriordensityandpYkMk
is the probability density of Y given k. The dimension of this integral is equal
to the dimension of k. This quantity can be interpreted as the marginal like-
lihood of the data, obtained by integrating the joint density of Yk over
k. It can also be interpreted as the predictive probability of the data; that is,
the probability of seeing the data that actually were observed, calculated before
any data became available. Sometimes, it is also called an integrated likelihood.
Note that, as in the computation of the likelihood ratio statistic but unlike in
some other applications of likelihood, all constants appearing in the definition
of the likelihood pYkMk must be retained when computing B10. In fact,
B10 is closely related to the likelihood ratio statistic, in which the parameters
k are eliminated by maximization rather than by integration. Very often, it is
extremely difficult to obtain B10 analytically, and various analytic and numer-
ical approximations have been proposed in the literature. For example, Chib
(1995), and Chib and Jeliazkov (2001) respectively developed efficient algo-
rithms for computing the marginal likelihood through MCMC chains produced
by the Gibbs sampler and the MH algorithm. Based on the results of DiCiccio,
Kass, Raftery and Wasserman (1997) and the recommendation of Gelman and
Meng (1998), we will apply path sampling to compute the Bayes factor for model
comparison.
5.3
PATH SAMPLING
A procedure based on path sampling (Gelman and Meng, 1998) is introduced
in this section for computing the Bayes factor. The key idea of path sampling

116
5
MODEL COMPARISON AND MODEL CHECKING
is to compute the ratio of normalizing constants of probability densities (or
equivalently difference of the logarithm of them). Hence, it can be applied to
compute the Bayes factor. Following Gelman and Meng (1998), we motivate
this computing tool from importance sampling (see for example Gelfand and
Dey, 1994) and bridge sampling (Meng and Wong, 1996).
In the context of SEMs, we consider two competitive models M0 and M1 with
the matrix  of latent variables. Let  be the vector of unknown parameters in
both M1 and M0. Usually, it is quite difficult to apply path sampling to evaluate
pYMk under complicated situations. Hence, we first use the idea of data
augmentation to include the latent variables in the computation, by considering
the complete-data set Y. Basically, we compute
zk = pYMk =

pYMkdd
k = 01
and obtain B10 as the ratio of z1/z0.
The key idea of importance sampling is the following equality:
z1 = pYM1 =
 pYM1
pYM1 pYM1dd
(5.4)
Viewing pYM1 as a probability density function, z1 is the expectation
of pYM1/pYM1 with respect to the joint distribution of  and
 under M1. On the basis of the simple idea that approximating the expectation
by the sample mean of observations in a sufficiently large sample, it can be
approximated as below,
z1  1
J
J
j=1
pYjjM1
pjjYM1

(5.5)
where jjj = 1··· J is a sample of observations drawn from the
target distribution pYM1 via some posterior simulation methods. Simi-
larly we can obtain z0. Finally, the Bayes factor can be estimated via z1/z0.
In this method, we need to simulate observations from both pYM0 and
pYM1. The following slight modification may be preferable. It follows
from an identity in Meng and Wong (1996, Equation 1.4) that the ratio of
normalizing constants can be expressed as
z1
z0 = E0
pYM1
pYM0


when
	1 ⊂	0
(5.6)

5.3
PATH SAMPLING
117
where 	k is the set containing  such that pYMk > 0 k = 01,
and Ek is the expectation taken with respect to pYMk. Then,
B10 = z1
z0  1
J
J
j=1
pYjjM1
pYjjM0

(5.7)
where jjj = 1··· J are sampled from pYM0. Hence, this
application of the importance sampling only requires posterior simulation of
pYM0. The application of importance sampling is fairly straightforward.
However, this method is only effective if M0 and M1 are close to each other.
Bridge sampling is a better method for computing the Bayes factor in the
case where M0 and M1 may not be close to each other. Its development is based
on following important identity whose general form has been studied in detail
by Meng and Wong (1996):
z1
z0 = E0
pYM1
E1
pYM0
(5.8)
where  is an arbitrary function satisfying
0 <


	0∩	1
pYM1pYM0dd
 < 
The existence of such  is guaranteed if the set of  that satis-
fies pYM1 > 0 and pYM0 > 0 is not empty; that is, there is
some overlap between M0 and M1. Taking  = 1/pYM0, Equa-
tion (5.8) reduces to Equation (5.6), which is the method based on the impor-
tance sampling. Now, we define
 =
pYM 1
2 
pYM0pYM1
where pYM 1
2  is a density in between pYM0 and pYM1
having support 	0 ∩	1 with a model M 1
2 in between M0 and M1. Substitute
this  into Equation (5.8), we have
z1
z0 =
E0
pYM 1
2 /pYM0
E1
pYM 1
2 /pYM1

J −1
0
J0
j=1
pY
j
0 
j
0 M 1
2 /pY
j
0 
j
0 M0
J −1
1
J1
j=1
pY
j
1 
j
1 M 1
2 /pY
j
1 
j
1 M1

(5.9)

118
5
MODEL COMPARISON AND MODEL CHECKING
where 
j
0 
j
0 j = 1··· J0 and 
j
1 
j
1 j = 1··· J1 are drawn from
pYM0 and pYM1, respectively. Hence, in contrast to the impor-
tance sampling which directly approximates z1/z0 via M0 and M1, the bridge
sampling makes use of a sensible choice of a bridge model M 1
2 so that M0
is closer to M 1
2 and M 1
2 is closer to M1, for improving the effectiveness and
accuracy.
The efficiency of bridge sampling depends on the overlap of M0 and M 1
2 , as
well as M 1
2 and M1. The more they overlap, the more efficient is the algorithm.
If pYM0 and pYM1 are very far apart, then the algorithm will be
unstable even with the optimal bridge. Hence, it is useful to construct a series of
L −1 intermediate models, from which we can make draws. Let pYtℓ,
where ℓ= 0··· L and tℓin 
01, represent a class of densities corresponding
to models Mℓin between M0 and M1. For example, pYt0 = 0 and
pYtL = 1 represent M0 and M1, respectively. For each pair of consecutive
functions pYtℓ and pYtℓ−1
2 , it follows from Equations (5.8) and
(5.9) that
z1
z0 =
L
ℓ=1
Eℓ−1
pYtℓ−1
2 /pYtℓ−1
Eℓ
pYtℓ−1
2 /pYtℓ

Let L tend to infinity and consider the index tℓas a parameter in 
01. Gelman
and Meng (1998) proved that
log z1
z0 = lim
L→log
L
ℓ=1
Eℓ−1
pYtℓ−1
2 /pYtℓ−1
Eℓ
pYtℓ−1
2 /pYtℓ
=
 1
0 E
UYtdt
(5.10)
where E denotes the expectation with respect to the distribution pYt,
and
UYt = d
dt log pYt
(5.11)
Following a procedure in Gelman and Meng (1998) to evaluate numerically
the above integral over t via the method in Ogata (1989, 1990), an estimate
of log B10 can be obtained. Specifically, we first order the values of S fixed
grids tsS
s=0 such that t0 = 0 < t1 < t2 < ··· < tS < tS+1 = 1, and estimate
log B10 by
	
log B10 = 1
2
S
s=0
ts+1 −tsU s+1 +U s
(5.12)

5.3
PATH SAMPLING
119
where U s is the average of the values of UYt on the basis of all
simulation draws for which t = ts, that is,
U s = J −1
J
j=1
UYjjts
(5.13)
in which jjj = 1··· J are simulated observations drawn from
pYts. This is the path sampling approach for estimating log B10. We
see that it is an extension of the bridge sampling from one bridge to an infi-
nite number of bridges. Based on the equality pY = pY/pY
and treating pY as a normalizing constant, we have another derivation of
Equation (5.10). The details are presented in Appendix 5.1.
To apply the path sampling procedure, we need to define a linked model Mt
to link M0 and M1, such that when t = 0Mt = M0 and when t = 1Mt = M1.
Then, we obtain UYt by differentiating the logarithm of the complete-
data likelihood function under Mt with respect to t, and finally estimate log B10
via Equation (5.12). The main computation is on simulating the sample of
observations jjj = 1··· J from pYts, for s = 0   S.
This task can be done via some efficient MCMC methods, such as the Gibbs
sampler and the MH algorithm as described in the Chapter 4. See also the
illustrative examples given in the next section and other chapters in this book.
For most SEMs, S = 20 and J = 1000 would provide results that are accurate
enough for most practical applications. Experience indicates that S = 10 is also
acceptable.
The path sampling approach has several attractive features. Its implementa-
tion is simple, the main programming task is on simulating observations from
pYts. In general, as pointed out by Gelman and Meng (1998), we
can always construct a continuous path to link two competing models. Hence
the method can be applied to a wide variety of models. Bayesian estimates of
the unknown parameters and other interesting statistics under M0 and M1 can
be obtained via the simulated observations at t = 0 and t = 1. Distinct from
most existing approaches in computing the Bayes factor, the prior density is
not directly involved in the evaluation. Furthermore, the logarithm scale of
Bayes factor is computed, which is generally more stable than the ratio scale.
In a comparative study on a variety of methods for computing the Bayes factor,
DiCiccio, Kass, Raftery and Wasserman (1997) concluded that bridge sampling
typically provides an order of magnitude of improvement on other methods
in computing the Bayes factor. As path sampling is a generalization of bridge
sampling, it has the potential to have even more improvement.

120
5
MODEL COMPARISON AND MODEL CHECKING
5.4
AN APPLICATION: BAYESIAN ANALYSIS OF SEMs
WITH FIXED COVARIATES
As noted by Sammel and Ryan (1996), very often it is useful to accommodate
fixed covariates in a model so as to give more ingredients and flexibility for
developing better models. Clearly, like other statistical models, residuals errors
in the measurement and structural equations of the model can be reduced by
additionally incorporating fixed covariates. For the measurement equation, fixed
covariates provide a more subtle structure, and hence enable us to assess a more
precise relationship between the latent variables and their manifest variables. For
the structural equation, fixed covariates provide more ingredients for accounting
the endogenous latent variables, in addition to the exogenous latent variables.
For a possible exogenous variable that can be accounted by a single manifest
variable, we can directly assess its causal effect on the endogenous variables by
treating it as a fixed covariate in the structural equation. In this way, it is not
necessary to include this manifest variable in the measurement equation, and
the formulation of the model is simplified. The main focus of this subsection is
to develop a Bayesian procedure for estimation and model comparison of SEMs
with fixed covariates on the measurement and structural equations.
Consider the following measurement equation for the p ×1 manifest random
vector yi measured on an individual i:
yi = Axi +i +i
i = 1··· n
(5.14)
in which A and  are unknown parameter matrices, xi is a r1 ×1 vector of fixed
covariates, i is an q ×1 latent random vector and i is a p ×1 random vector
of error measurements with distribution N 
0 , where   is diagonal and
i is independent with i. Furthermore, we model the latent subvectors i and
	i of i further with an additional r2 × 1 vector of fixed covariates zi via the
following structural equation:
i = Bzi +
i +	i +i
(5.15)
where i = T
i 	T
i T i and 	i are respectively q1×1 and q2×1 latent vectors;
B
 and  are unknown parameter matrices; 	i and i are independently
distributed as N 
0 and N 
0 , respectively, where   is a diagonal
covariance matrix. The measurement and structural equations defined in Equa-
tions (5.14) and (5.15) reduces to the ordinary simultaneous equation model
if 
 = 0,  = 0 and  = 0. Let  = 1 2 be a partition of , where 1
and 2 are p ×q1 and p ×q2 matrices, respectively; and 
0 = I−
. We further

5.4
BAYESIAN ANALYSIS OF SEMs WITH FIXED COVARIATES
121
assume that 
0 is a constant independent with elements in 
. The covariance
structure of i is given by
 =


−1
0 T + 
−T
0

−1
0 
T 
−T
0



The marginal model for the manifest random vector yi is given by
yiAB
  
D= N 
Axi +1
−1
0 Bzi T + 
where the mean structure is defined as a linear combination of various fixed
covariates, while the covariance structure is equal to the LISREL model
(Jöreskog and Sörbom, 1996). It should also be noted that the fixed covari-
ates xi and zi can be discrete, ordered categorical or continuous measurements.
The mean vector can be represented by a column in A corresponding to covari-
ates that are fixed at 1.0. Following a common practice in structural equation
modeling to identify the model, some appropriate elements in 
 and 
will be restricted at known values and not be estimated. Below, we assume that
the covariance structure of yi is identified.
Let Y = y1··· ynX = x1··· xn and Z = z1··· zn be the data
matrices; and let  = 1··· n be the matrix of latent vectors. Moreover,
let  be the structural parameter vector that contains all the unknown param-
eters in AB
  and  . One main objective is to illustrate the
path sampling procedure in computing the log Bayes factor for model compar-
ison. A sequence of random observations from the joint posterior distribution

Yt will be generated by the Gibbs sampler which is implemented as
below:
At the j +1th iteration with current value j and j.
Step (a):
Generate a random variate j+1 from the conditional distribution

Yjt.
Step (b):
Generate a random variate j+1 from the conditional distribution

Yj+1t, and return to ‘Step (a)’ if necessary.
As the model defined by Equations (5.14) and (5.15) are essentially regression
models with latent variables and similar to the standard SEM defined by Equa-
tions (4.36) and (4.37), the conditional distributions 
Y and 
Y
under the conjugate prior distribution are very similar to those presented in
Section 4.4.1. For completeness, expressions of these conditional distributions
are discussed in Appendix 5.2. For a given t, corresponding conditional distri-
butions can be obtained by incorporating these expressions with t.

122
5
MODEL COMPARISON AND MODEL CHECKING
To illustrate the application of path sampling for computing the Bayes factor
with a real example, a small portion of the Inter-university Consortium for
Political and Social Research (ICPSR) data set collected in the project World
Value Survey 1981–1984 and 1990–1993 (World Value Study Group, (1994))
is analyzed. In our illustration, only the data obtained from Canada are used.
Six variables in the full data set (variables 180, 96, 62, 176, 116 and 117;
see Appendix 1.1) that are related to the respondents’ job, religious belief and
homelife are taken as manifest variables in y. Intercepts with xi1 = 10 and a
variable xi2 (variable 353) about gender are taken as fixed covariates for the
manifest variables in the measurement equation. An additional variable (variable
364) about socio-economic status is taken as a fixed covariate zi in the structural
equation. For simplicity, cases with missing data are not used, and the remaining
sample size is 454.
Parameter matrices A and  in the measurement equation (5.14) are specified
as follows:
AT =
a11 ···a61
a12 ···a62

 and T =
⎡
⎢⎣
10
21
0
0
0
0
0
0
10
42
0
0
0
0
0
0
10
63
⎤
⎥⎦
where ones’s and zero’s in  were treated as fixed parameters to identify the
model. In this model, there are three latent factors: , 1 and 2, which can be
roughly interpreted as ‘life’, ‘religious belief’ and ‘job satisfaction’, respectively.
We first conduct an auxiliary Bayesian estimation with non-informative priors to
obtain some prior inputs for the conjugate prior distributions. Then, we do an
actual simulation from the conditional distributions to obtain some idea about
the convergence of the Gibbs sampler with the hyperparameter values that are
obtained from the auxiliary estimation. We observe that the Gibbs sampler
converged in less than 800 iterations. To be conservative, we take a burn-in
phase of 1000 iterations and for each ts an additional J = 2000 observations
are collected after convergence for computing the Bayes factor. The number of
grids is taken to be 20.
The following three models are compared:
M0  y = Ax ++ and  = bz +11 +22 +
M1  y = + and  = bz +11 +22 +
M2  y = Ax ++ and  = 11 +22 +
Note that M1 and M2 are non-nested, while both of them are nested in M0.
Models M0 and M1 only differ in the measurement equations, hence they can
be linked up by a parameter t in [0, 1] as
Mt01  y = 1−tAx ++

5.4
BAYESIAN ANALYSIS OF SEMs WITH FIXED COVARIATES
123
with the same structural equation. Clearly when t = 0Mt01 = M0, and when
t = 1Mt01 = M1. The parameter vector  in the linked model Mt01 contains
the unknown matrix A that is not involved in M1. Models M0 and M2 have
the same measurement equation. Their structural equations are linked up by a
parameter t in [0, 1] as
Mt02   = 1−tbz +11 +22 +
Clearly when t = 0Mt02 = M0, and when t = 1Mt02 = M2. Note that the
parameter vector  in Mt02 contains b that is not involved in M2. Finally, models
M1 and M2 can be linked up as
Mt12 y = tAx ++
 = 1−tbz +11 +22 +
where when t = 0 M t12 = M1, and when t = 1 Mt12 = M2.
The logarithm complete-data likelihood functions corresponding to linked
models Mt01Mt02 and Mt12 are respectively given by:
log p01Yt =−1
2

c∗+
n
i=1
	T
i −1	i
+
n
i=1

yi −1−tAxi −iT  −1
 
yi −1−tAxi −i
+
n
i=1
i −bzi −1i1 −2i2−1
 i −bzi −1i1 −2i2


log p02Yt =−1
2

c∗+
n
i=1
	T
i −1	i
+
n
i=1
yi −Axi −iT  −1
 yi −Axi −i
+
n
i=1

i −1−tbzi −1i1 −2i2−1

× 
i −1−tbzi −1i1 −2i2
log p12Yt =−1
2

c∗+
n
i=1
	T
i −1	i
+
n
i=1
yi −tAxi −iT  −1
 yi −tAxi −i

124
5
MODEL COMPARISON AND MODEL CHECKING
+
n
i=1

i −1−tbzi −1i1 −2i2−1

× 
i −1−tbzi −1i1 −2i2
where c∗is a constant that is equal to np +qlog2+n log  +n log +
n log , and  is the parameter vector in the linked model. Derivatives of these
functions with respect to t are given by
d log p01Yt
dt
=−
n
i=1
yi −1−tAxi −iT  −1
 Axi
d log p02Yt
dt
=−
n
i=1
i −1−tbzi −1i1 −2i2−1
 bzi
and
d log p12Yt
dt
=
n
i=1
yi −tAxi −iT  −1
 Axi
−i −1−tbzi −1i1 −2i2−1
 bzi
These derivatives give UYt under various situations. The log Bayes
factors are estimated via Equations (5.12) and (5.13).
To give some idea about the sensitivity of the Bayesian analysis with respect to
priors, we consider the model comparison with three types of prior inputs: prior
inputs obtained from an auxiliary estimation (type I), those equal to half (type II)
andtwice(typeIII)ofthetypeIpriorinputs.TheestimatedtwicelogBayesfactors
are reported in Table 5.2. Clearly, the conclusions drawn from these results are
not very sensitive to these prior inputs. From Table 5.2, we see that all 2 	
logB01
and 2 	
logB21 are significantly larger than 10, and hence M1 is rejected. All 2 	
logB02
are less than 2, that means the more complicated model M0 is not significantly
betterthanthesimplermodelM2.Hence,M2 maybeselected.Itcanbeconcluded
from the model comparison results that the effect of the fixed covariate ‘gender’
is important in relating the latent variables with their manifest variables, whilst the
Table
5.2
Estimated twice log Bayes factors.
Prior inputs
Type I
Type II
Type III
2log B01
162.2
158.8
163.4
2log B02
1.058
0.668
0.534
2log B21
165.0
166.8
171.4

5.5
BAYESIAN ANALYSIS OF SEMs WITH FIXED COVARIATES
125
fixed covariate about socio-economic status has little impact on the endogenous
latent variable on ‘life’.
To give some information about the convergence of the Gibbs sampler in the
context of the selected model M2, plots of several sequences of some randomly
selected parameters values generated from different starting points against the
iteration numbers are displayed in Figure 5.1. We observe that the sequences
mixed well in less than 800 iterations. The EPSR values against the iteration
numbers are plotted in Figure 5.2. We observe that these values are less than 1.2
after 800 iterations. For completeness, the Bayesian estimates and the standard
errors estimates of the unknown parameters in M2 under prior inputs type I
are presented in Table 5.3. Interpretations of the results given in this table are
straightforward.
(a)
0
500
1000
1500
2000
0
500
1000
1500
2000
12
10
8
6
4
(b)
2
1
0
–1
–2
0
500
1000
1500
2000
0
500
1000
1500
2000
(c)
2.0
1.5
1.0
0.5
0.0
(d)
1.5
0.5
–0.5
0
500
1000
1500
2000
0
500
1000
1500
2000
1.5
0.5
–0.5
(e)
5
4
3
2
1
0
(f)
Figure
5.1
(a)–(f) are plots of parallel sequences corresponding to different starting
values of a11, a12, 63, 1, 3 and 12 against iteration numbers with different starting
values.

126
5
MODEL COMPARISON AND MODEL CHECKING
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
500
1000
1500
2000
2500
3000
4000
3500
4500
5000
Figure
5.2
EPSR values against the number of iterations.
Table
5.3
Bayesian estimates and their standard errors.
Parameters
Est.
SE
Parameters
Est.
SE
a11
834
021
21
107
009
a21
774
021
42
278
026
a31
267
021
63
082
009
a41
566
033
11
074
013
a51
776
024
12
025
008
a61
757
032
22
191
019
a12
004
014
1
126
013
a22
016
013
2
074
012
a32
007
013
3
157
013
a42
088
020
4
156
034
a52
019
015
5
087
014
a62
−12
021
6
322
025
1
016
008

087
012
2
044
006

5.5
OTHER METHODS
127
5.5
OTHER METHODS
5.5.1
Bayesian Information Criterion and Akaike Information
Criterion
A simple approximation of 2log B10 which does not depend on the prior density
is the following Schwarz criterion S∗(Schwarz, 1978):
2log B10  2S∗= 2
log pY˜1M1−log pY˜0M0−d1 −d0log n
(5.16)
where ˜1 and ˜0 are ML estimates of 1 and 0 under M1 and M0, respectively,
d1 and d0 are the dimensions of 1 and 0 and n is the sample size. Minus 2S∗is
the following well-known Bayesian information criterion (BIC) for comparing
M1 and M0:
BIC10 = −2S∗ −2log B10 = 2log B01
(5.17)
The interpretation of BIC10 can be based on Table 5.1. For each Mkk = 01,
we define
BICk = −2log pY˜kMk+dk log n
(5.18)
Then 2log B10 = BIC0 −BIC1. Hence, it follows from Table 5.1 that the model
Mk with the smaller BICk value is selected.
As n tends to infinity, it has been shown (Schwarz, 1978) that
S∗−log B10
log B10
→0
thus S∗may be viewed as an approximation to log B10. This approximation is of
the order O1, hence it does not give the exact log B10 even for large samples.
However, as pointed out by Kass and Raftery (1995), it can be used for scientific
reporting as long as the number of degrees of freedom d1 −d0 involved in the
comparison is small relative to the sample size n. The BIC is appealing in that
it is relatively simple and can be applied even when the priors pkMk are hard
to set precisely. The ML estimates of 1 and 0 are involved in the computation
of BIC. Sometimes obtaining the ML estimates is difficult for some complex
SEMs. In practice, since the Bayesian estimates and the ML estimates are close
to each other in large samples, they can be used to replace the ML estimates in
computing the BIC. The order of approximation is not changed and the BIC
obtained can be interpreted using the rough criteria given in Table 5.1.

128
5
MODEL COMPARISON AND MODEL CHECKING
The Akaike information criterion (AIC, Akaike, 1973) associated with a
competing model Mk is given by
AICk = −2log pY ˜kMk+2dk
(5.19)
which does not involve the sample size n. The interpretation of AICk is similar to
BICk. Hence, Mk is selected if its AICk is smaller. Comparing Equations (5.18)
and (5.19), we see that BIC tends to favor simpler models more than those
selected by AIC.
5.5.2
Deviance Information Criterion
Another goodness-of-fit or model comparison statistic that takes into account
the number of unknown parameters in the model is the deviance information
criterion (DIC), see Spiegelhalter, Best, Carlin and van der Linde (2002). This
statistic can be viewed as a generalization of AIC. Under a competing model Mk
with a vector of unknown parameter k of dimension dk, let 
j
k  j = 1   J
be a sample of observations simulated from the posterior distribution. We define
DICk = −2
J
J
j=1
log pY
j
k Mk+2dk
(5.20)
In model comparison, the model with the smaller DIC value is selected.
The computational burden of DIC is similar to the computational burden
of BIC or AIC, and is less heavy than that of the Bayes factor. In analyzing
a hypothesized model, WinBUGS (Spiegelhalter, Best and Lunn, 2003) also
produces a DIC value. This DIC value can be used for model comparison.
However, as pointed out in the WinBUGS User Manual in practical applications
of DIC it is important to note the following points: (i) DIC assumes the
posterior mean to be a good estimate of the parameter. There are circumstances,
such as with mixture models, in which WinBUGS will not give the DIC values.
(ii) If the difference in DIC is small, for example less than 5, and the models
make very different inferences, then just reporting the model with the lowest
DIC could be misleading. (iii) DIC can be applied to non-nested models.
Moreover, similar to the Bayes factor, BIC and AIC, DIC gives clear conclusion
to support the null hypothesis or the alternative hypothesis. Detailed discussions
of DIC can be found in Spiegelhalter, Best, Carlin and van der Linde (2002),
and Celeux, Forbes, Roberts and Titterington (2003).
5.5.3
Posterior Predictive p-value
The Bayes factor can be used to assess the goodness-of-fit of the hypothe-
sized model by taking M0 or M1 to be the saturated model. However, when

5.5
OTHER METHODS
129
analyzing some complex SEMs it is rather difficult to define a saturated model.
For example, in analyzing nonlinear SEMs, the distribution associated with the
hypothesized model is not normal. Hence, the model having a normal distri-
bution with a general unstructured covariance matrix cannot be regarded as a
saturated model. Under these situations, it is not appropriate to apply model
comparison to access goodness-of-fit of the hypothesized model. A simple and
more convenient alternative without involving a basic saturated model is the
posterior predictive p-values (PP p-values) introduced by Meng (1994) on the
basis of the posterior assessment in Rubin (1984). It has been shown (see
Gelman, Meng and Stern, 1996 and the references therein) that this approach
is conceptually and computationally simple, and is useful in model-checking
for a wide variety of complicated situations. Moreover, the required compu-
tation is a by-product of the common Bayesian simulation procedures such as
the Gibbs sampler or its related algorithms. A brief outline of this method is
given in Appendix 5.3. The proposed model may be considered as plausible if
the PP p-value estimate is not far from 0.5. As pointed out by Meng (1994),
and Carlin and Louis (1996), the PP p-value is only useful for the goodness-
of-fit assessment of a single model, it is not suitable for comparing different
models. Moreover, it may show good fit for some inappropriate models, due
to the problem of using the data twice. Hence, the PP p-value should only
be used as a complementary statistic for the Bayes factor. Bayarri and Berger
(2000) proposed a partial posterior p-value to overcome the problem of using
the data twice. Recently, this statistic has been applied to a nonlinear SEM with
nonignorable missing data (see Lee and Tang, 2006).
5.5.4
Residual and Outliers Analysis
Many common model checking methods in data analysis, including residual
analysis and test for outliers, can be incorporated in the Bayesian analysis. An
advantage of the sampling-based Bayesian approach to SEMs is that we can
obtain the estimates of the latent variables through the posterior simulation
so that reliable estimates of the residuals in the measurement equation and
the structural equation can be obtained. The graphical interpretation of these
residuals is similar to those in other statistical models, for example, regression.
Consider the SEMs with fixed covariates as described in Section 5.4. Esti-
mates of the residuals in the measurement equation can be obtained from
Equation (5.14) as:
ˆi = yi −ˆAxi −ˆ ˆi
i = 1   n
(5.21)
where ˆA ˆ and ˆi are Bayesian estimates that are obtained from the corre-
sponding simulated observations through the MCMC methods. Plots of ˆi
with ˆi give useful information for the fit of the measurement equation. For

130
5
MODEL COMPARISON AND MODEL CHECKING
a reasonable good fit, the plots should lie within two parallel horizontal lines
that are not widely separated apart and centered at zero. Estimates of residuals
in the structural equation can be obtained from Equation (5.15) as:
ˆi = I−ˆ
 ˆi −ˆBzi −ˆ ˆ	i
i = 1··· n
(5.22)
where ˆ
 ˆB ˆ ˆi and ˆ	i are Bayesian estimates obtained via the MCMC
methods. The interpretation and the use of plots of ˆi are similar.
The residual estimates ˆi can also be used for outliers analysis. A particular
observation yi whose residual is far from the expected value (¯) may be infor-
mally regarded as an outlier. Moreover, the QQ plots (Johnson and Wichern,
1992) of ˆijj = 1··· p, and/or ˆikk = 1··· r2, can be used to check the
assumption of normality.
5.6
DISCUSSION
The main objective of this chapter is to consider Bayesian model comparison
of SEMs, with emphasis on the computation of the well-known Bayes factor
through path sampling. In applying path sampling, it is necessary to find a
path t in 
01 to join the competing models M1 and M2. In most cases, it is
fairly straightforward to find such a path or the corresponding linked model.
However, for some complicated situations that involve M1 and M2 being far
apart, it is difficult to find a path that directly links M1 and M2. In most
situations, this difficulty can be solved by using appropriate auxiliary models
MaMb···, in between M1 and M2. For example, suppose that Ma and Mb are
appropriate auxiliary models such that Ma can be linked with M1, and Mb, and
Mb can be linked with M2. Then
pYM1
pYM2 = pYM1/pYMa
pYM2/pYMa
and
pYM2
pYMa = pYM2/pYMb
pYMa/pYMb
Hence, log B12 = log B1a +log Bab −log B2b. Each logarithm Bayes factor can be
computed through path sampling (see an example in Chapter 10).
The prior distributions of  are either directly or indirectly involved in the
computation of the Bayes factor. The BIC defined in Equation (5.17) does not
depend directly on prior densities. However, if the ML estimates are replaced
by the Bayesian estimates in the computation, then the posterior simulations
in obtaining the Bayesian estimates will depend on the prior distributions of .
The importance sampling, bridge sampling and path sampling approaches are
indirectly affected by the priors during the simulation of observations from the
posterior distributions that depend on the prior distributions. Thus, a concern
is to choose prior distributions to represent the available information. Once this

APPENDIX 5.1: ANOTHER PROOF OF EQUATION (5.10)
131
is done, an important issue is the sensitivity of the Bayes factor to the choices
of priors.
Consider the problem of comparing M0 with M1, as pointed out by Kass
and Raftery (1995), using a prior with a very large spread on the parameters
under M1 so as to make it ‘noninformative’; this will force the Bayes factor
to favor the competitive model M0. This is known as the ‘Bartlett’s paradox’.
For example, in the context of linear and log-linear models, Spiegelhalter and
Smith (1982) showed that the Bayes factor may be problematic if the prior
distribution of the parameters involved in the comparison (or in the hypoth-
esis) are noninformative. To avoid this difficulty, priors on parameters under
the model comparison are generally taken to be proper and do not have too big
a spread. The conjugate families suggested in the previous section with reason-
able spread are good choices. Parameters not under the model comparison are
regarded as nuisance parameters. As pointed out by Kass and Raftery (1995),
under mild regularity conditions, choices of priors of the nuisance parame-
ters do not greatly affect the comparison results. Hence, noninformative priors
can be used for the nuisance parameters. However, more care should be taken
for model comparison involving parameters with non-informative prior. One
simple method suggested in Kass and Raftery (1995) on the basis of the idea in
Lempers (1971) is to set aside part of the data to use as a training sample which
is combined with the noninformative prior distribution to produce an informa-
tive prior distribution. The Bayes factor is then computed from the remainder
of the data. More advanced methods have been suggested (see, for example,
O’Hagan (1995) and Berger and Pericchi (1996), among others.
To study the sensitivity of the Bayes factor to the choices of the prior inputs
in terms of the hyperparameters’ values, a common method (see Kass and
Raftery, 1995; Lee and Song, 2003, among others) is to perturb the prior
inputs. For example, if the prior distribution is N 
02
0 in which the given
hyperparameters are 0 and 2
0, the hyperparameters may be perturbed by
changing 0 to 0 ± c and halving and doubling 2
0, and the Bayes factor
recomputed.
For most SEMs, the freely available software WinBUGS produces the DIC
value of the hypothesized model. In practice, this value conveniently gives an
alternative statistic for model comparison. The residual plots are also helpful
in assessing the goodness-of-fit of the measurement equation and structural
equation.
APPENDIX 5.1: ANOTHER PROOF OF EQUATION (5.10)
This approach derives Equation (5.10) on the basis of the equality pY =
pY/pY. The marginal density of pY can be treated as the normalizing

132
5
MODEL COMPARISON AND MODEL CHECKING
constant of pY. Consider the following class of densities with a contin-
uous parameter t ∈
01:
pYt =
1
ztpYt
(A5.1)
where
zt = pYt =

pYtdd =

pYtpdd
(A5.2)
with p be the prior density of  which is independent of t. In computing
the Bayes factor, we need to construct a path using a parameter t ∈
01
to link the two competing models M0 and M1 together, so that z1 =
pY1 = pYM1 z0 = pY0 = pYM0, and B10 = z1/z0. From Equa-
tions (A5.1) and (A5.2), we have
d logzt
dt
=

1
zt
d
dt pYtdd
=
 pYt
pYt
d
dt pYtdd
=
  d
dt log pYt

·pYtdd
= E
 d
dt log pYt


(A5.3)
where E is the expectation with respect to the distribution pYt.
Define:
UYt = d
dt log pYt = d
dt log pYt
(A5.4)
As p is independent of t, UYt does not involve the prior density
p. It follows from Equations (A5.3) and (A5.4) and integrating from 0 to
1, that
log z1
z0 =
 1
0 E
UYtdt

APPENDIX 5.2
133
APPENDIX 5.2: CONDITIONAL DISTRIBUTIONS FOR
SIMULATING [, |Y, T]
We first note that for i = 1··· ni are conditionally independent given 
and yi are also conditionally independent with given i. Hence,
pY ∝
n
i=1
pi pyii
(A5.5)
It implies that the conditional distribution of i given yi are mutually
independent for different i, and piyi ∝pipyii. From the fact
that

i
D= N

−1
0 Bzi
0




and 
yii
D= N
Axi +i , we have

iyi
D= N

∗−1T  −1
 yi −Axi+∗−1−1


−1
0 Bzi
0

∗−1

(A5.6)
where ∗= −1
 +T  −1
 . We see that the conditional distribution 
iyi
is a normal distribution, and it is similar to that given in Equation (4.25).
CONDITIONAL DISTRIBUTION [|Y, ]
The conditional distribution of  given Y is proportional to ppY.
We note that  is given, and Equations (5.14) and (5.15) are linear models
with fixed covariates. Let y be the unknown parameters in A and  
associated with the measurement equation and  be the unknown parameters
in B
 and   associated with the structural model. Again, it is assumed
that the prior distribution of y is independent of the prior distribution of ,
that is, p = pyp. Moreover, it follows from the reasoning given in
Section 4.4.1 that the marginal conditional densities y and  given Y are
proportional to pYypy and pp, respectively. Hence, these
conditional densities can be treated separately as before.
Consider first the marginal conditional distribution of y. Let y = A
with general elements ykj j = 1··· r1 +q k = 1··· p, and ui = xT
i T
i T .
It follows that yi = yui +i. This simple transformation reformulates the model
with fixed covariate xi to the original factor analysis model. The positions of

134
5
MODEL COMPARISON AND MODEL CHECKING
the fixed elements in y are identified via an index matrix Ly with the following
elements:
lykj =

0
if
ykj
fixed
1
if
ykj
free
for
j = 1··· r1 +q
and
k = 1··· p
Let k be the kth diagonal element of  , and T
yk be the row vector
that contains the unknown parameters in the kth row of y. The following
commonly used conjugate type prior distributions are used. For any k ̸= h, we
assume that the prior distribution of kyk is independent of hyh, and
−1
k
D= Gamma
0k 0k and 
ykk
D= N
0ykkH0yk k = 1··· p
(A5.7)
where 0k0kT
0yk = AT
0kT
0k and the positive definite matrix H0yk are
hyperparameters whose values are assumed to be given from the prior informa-
tion of previous studies or other sources.
Let U = u1··· un, and let Uk be the submatrix of U such that all the rows
corresponding to lykj = 0 are deleted; let YT
k be the kth row of Y with general
elements yik and Y∗T
k
= y∗
1k··· y∗
nk with
y∗
ik = yik −
r1+q

j=1
ykjuij1−lykj
where uij is jth element of ui. Then, for k = 1··· p, it can be shown (by
similar reasoning as in Appendix 4.3) that

−1
k Y
D= Gamma
n/2+0kk 
ykY−1
k 
D= N
aykkAyk
(A5.8)
where Ayk = H−1
0yk +UkUT
k −1ayk = AykH−1
0yk0yk +UkY∗
k and
k = 0k + 1
2Y∗T
k Y∗
k −aT
ykA−1
yk ayk +T
0ykH−1
0yk0yk
Since 
yk−1
k Y is equal to 
−1
k Y
ykY−1
k , it can be obtained
via (A5.8). This gives the conditional distribution in relation to y.
Now, consider the conditional distribution of  that is proportional to
pp. Let 1 = 1··· n and 2 = 	1··· 	n. Since the distri-
bution of 	i only involves p2 = p2. Under the assumption that
the prior distribution of  is independent of the prior distributions of B

and  ,we have
pp = 
p12B
 pB
 
p2p

APPENDIX 5.2
135
Hence,the marginal conditional densities of B
  and  can be treated
separately.
Consider a conjugate type prior distribution for  with −1 D= Wr2
R0 0,
with hyperparameters  0 and R0. It can be shown by reasoning similar to that
used in Section 4.4.1 that the conditional distribution of  given 2 is given by

2
D= IWq2
2T
2 +R −1
0 n + 0
(A5.9)
Rewrite Equation (5.15) as i = vi + i, where  = B
 with
general elements kj for k = 1   r1, and vi = zT
i T
i 	T
i T = zT
i T
i T be
a (r2 +q×1 vector. Let V = v1··· vnL be the index matrix with general
elements lkj that similarly defined as Ly to indicate the fixed known parameters
in ; let k be the kth diagonal element of   and T
k be the row vector
that contains the unknown parameters in the kth row of . The prior distri-
butions of k and −1
k are similarly selected via the following conjugate type
distributions:
−1
k
D= Gamma
0k0k and 
kk
D= N
0kkH0k
k = 1··· q1
(A5.10)
where 0k0kT
0k = BT
0k
T
0kT
0k and H0k are given hyperparameters.
Moreover, it is assumed that for h ̸= kkk and hh are indepen-
dent. Let Vk be the submatrix of V such that all the rows corresponding to
lkj = 0 are deleted, and let T
k = ∗
1k··· ∗
nk where
∗
ik = ik −
r1+q

j=1
kjvij1−lkj
Then, following the same reasoning as before, it can be shown that:

−1
k 
D=Gamma 
n/2+0kk and

k−1
k 
D=N
akkAk
(A5.11)
where Ak = H−1
0k +VkVk
T −1ak = AkH−1
0k0k +Vkk and
k = 0k + 1
2!T
k k −aT
kA−1
k ak +T
0kH−1
0k0k
The conditional distribution 
B
  can be obtained through Equa-
tions (A5.11).

136
5
MODEL COMPARISON AND MODEL CHECKING
APPENDIX 5.3: PP P-VALUES FOR MODEL ASSESSMENT
Based on the posterior predictive assessment as discussed in Rubin (1984),
Gelman, Meng and Stern (1996) introduced a Bayesian counterpart of the
classical p-value by defining a posterior predictive (PP) p-value for model-
checking. To apply the approach for establishing a goodness-of-fit assessment
of the posited model M0 with parameter vector , observed data Y and latent
data , one needs to specify a discrepancy variable DY for measuring
the discrepancy between Y and the generated hypothetical replicate data Yrep.
More specifically, the PP p-value is defined as
pBY = Pr
DYrep ≥DYYM0
=

I 
DYrep ≥DYpYrepYM0d Yrepdd
where I· is an indicator function. The probability is taken over the following
joint posterior distribution of Yrep given Y and M0:
pYrepYM0 = pYreppY
In almost all the applications to SEMs considered in this book, we take the chi-
square discrepancy variable such that DYrep has a chi-squared distribution
with pn degrees of freedom. Thus, the PP p-value is equal to

p
2pn ≥DYpYdd
A Rao–Blackwellized type estimate of this PP p-value is:
ˆpBY = J −1
J
j=1
Pr
2d∗ ≥DYjj
(A5.12)
where jjj = 1··· J are observations simulated during the estima-
tion. The computational burden for obtaining this ˆpBY is light.
REFERENCES
Akaike, H. (1973) Information theory and an extension of the maximum likelihood
principle. In B. N. Petrox and F. Caski (eds), Second International Symposium on
Information Theory, p. 267. Budapest, Hungary: Akademiai Kiado.

REFERENCES
137
Bayarri, M. J. and Berger, J. O. (2000) P values for composite null models. Journal of
the American Statistical Association, 95, 1127–1142.
Bentler, P. M. (1992) EQS: Structural Equation Program Manual. Los Angeles, CA:
BMDP Statistical Software.
Bentler, P. M. and Bonett, D. G. (1980) Significance tests and goodness of fit in the
analysis of covariance structures. Psychological Bulletin, 88, 588–606.
Berger, J. O. (1985) Statistical Decision Theory and Bayesian Analysis. New York:
Springer-Verlag.
Berger, J. O. and Dalampady, M. (1987) Testing precise hypotheses. Statistical Science,
3, 317–352.
Berger, J. O. and Pericchi, L. R. (1996) The intrinsic Bayes factor for model selection
and prediction. Journal of the American Statistical Association, 91, 109–122.
Berger, J. O. and Sellke, T. (1987) Testing a point null hypothesis: The irreconcilability
of P values and evidence. Journal of the American Statistical Association, 82, 112–122.
Bollen, K. A. (1989) A new incremental fit index for general structural equation models.
Sociological Methods and Research, 17, 303–316.
Carlin, B. P. and Louis, T. A. (1996) Bayes and Empirical Bayes Methods for Data
Analysis. New York: Chapman and Hall.
Celeux, G., Forbes, F., Roberts, C. P. and Titterington, D. M. (2003) Deviance infor-
mation criteria for missing data models. Cahires du Ceremade, 0325.
Chib, S. (1995) Marginal likelihood from the Gibbs output. Journal of the American
Statistical Association, 90, 1313–1321.
Chib, S. and Jeliazkov, I. (2001) Marginal likelihood from the Metropolis–Hastings
outputs. Journal of the American Statistical Association, 96, 270–281.
DiCiccio, T. J., Kass, R. E., Raftery, A. and Wasserman, L. (1997) Computing Bayes
factors by combining simulation and asymptotic approximations. Journal of the Amer-
ican Statistical Association, 92, 903–915.
Gelfand, A. E. and Dey, D. K. (1994) Bayesian model choice: Asymptotics and exact
calculations. Journal of the Royal Statistical Society, Series B, 56, 501–514.
Gelman, A. and Meng, X. L. (1998) Simulating normalizing constants: from importance
sampling to bridge sampling to path sampling. Statistical Science, 13, 163–185.
Gelman, A., Meng, X. L. and Stern, H. (1996) Posterior predictive assessment of model
fitness via realized discrepancies. Statistica Sinica, 6, 733–759.
Gelman, A., Carlin, J. B., Stern, H. and Rubin, D. B. (2003) Bayesian Data Analysis,
(2nd edn). London: Chapman and Hall / CRC.
Jedidi, K., Jagpal, H. S. and DeSarbo, W. S. (1997) Finite-mixture structural equation
models for response-based segmentation and unobserved heterogeneity. Marketing
Science, 16, 39–59.
Johnson, R. A. and Wichern, D. W. (1992) Applied Multivariate Statistical Analysis
(3rd edn). New York: Prentice-Hall International, Inc.
Jöreskog, K. G. and Sörbom, D. (1996). LISREL 8: Structural Equation Modeling with
the SIMPLIS Command Language. Hove and London: Scientific Software Interna-
tional.
Kass, R. E. and Raftery, A. E. (1995) Bayes factors. Journal of the American Statistical
Association, 90, 773–795.
Lee, S. Y. and Song, X. Y. (2001) Hypothesis testing and model comparison in two-level
structural equation models. Multivariate Behavioral Research, 36, 639–55.

138
5
MODEL COMPARISON AND MODEL CHECKING
Lee, S. Y. and Song, X. Y. (2003) Bayesian model selection for mixtures of structural
equation models with an unknown number of components. British Journal of Mathe-
matical and Statistical Psychology, 56, 145–165.
Lee, S. Y. and Tang, N. S. (2006) Bayesian analysis of nonlinear structural equation
models with nonignorable missing data. Psychometrika, in press.
Lempers, F. B. (1971) Posterior Probabilities of Alternative Linear Models. Rotterdam:
University Press.
Meng, X. L. (1994) Posterior predictive p-values. The Annals of Statistics, 22, 1142–
1160.
Meng, X. L. and Wong, W. H. (1996) Simulating ratios of normalizing constants via
simple identity, a theoretical exploration. Statistica Sinica. 6, 831–860.
Ogata, Y. (1989) A Monte Carlo method for high dimensional integration. Numerische
Mathematik, 55, 137–157.
Ogata, Y. (1990) A Monte Carlo method for an objective Bayesian procedure. Annals
of the Institute of Statistical Mathematics, 42, 403–433.
O’Hagan, A. (1995) Fractional Bayes factor for model comparison. Journal of the Royal
Statistical Society, Series B, 57, 99–138.
Raftery, A. E. (1986) Choosing models for cross-classifications. American Sociological
Review, 51, 145–146.
Raftery, A. E. (1993) Bayesian model selection in structural equation models. In K.
A. Bollen and J. S. Long (eds), Testing Structural Equation Models, pp. 163–180.
Beverly Hills, CA: Sage.
Rubin, D. B. (1984) Bayesianly justifiable and relevant frequency calculations for the
applied statistician. The Annals of Statistics, 12, 1151–1172.
Sammel, M.D. and Ryan, L. M. (1996) Latent variable models with fixed effects. Biomet-
rics, 52, 650–663.
Schwarz, G. (1978) Estimating the dimension of a model. The Annals of Statistics, 6,
461–464.
Spiegelhalter, D. J. and Smith, A. F. M. (1982) Bayes factor for linear and log-linear
models with vague prior information. Journal of the Royal Statistical Society, Series B,
44, 377–387.
Spiegelhalter,
D.
J.,
Best,
N.
G.
and
Lunn,
D.
(2003)
WinBUGS
Version
1.4 User Manual. Cambridge: MRC Biostatistics Unit, URL http://www.mrc-
bsu.cam.ac.uk/bugs/.
Spiegelhalter, D. J., Best, N. G., Carlin, B. P. and van der Linde, A. (2002) Bayesian
measure of model complexity and fit. Journal of the Royal Statistical Society, Series B,
64, 583–639.
World Values Study Group (1994) World Values Survey, 1981–1984 and 1990–1993.
ICPSR version. Ann Arbor, MI: Institute for Social Research (producer). Ann Arbon,
MI: Inter-university Consortium for Political and Social Research (distributor).

6
Structural Equation
Models with Continuous
and Ordered
Categorical Variables
6.1
INTRODUCTION
Due to the design of questionnaires and the nature of the problems in behav-
ioral, educational, medical and social sciences, data are often coming from
ordered categorical variables with observations in discrete form. Examples of
such variables are attitude items, Likert items, rating scales and the like. Typical
cases are when a subject is asked to report some attitude on scales like ‘disap-
prove’, ‘no opinion’, ‘approve’; to report the effect of a drug on scale like
‘getting worse’, ‘no change’, ‘getting better’; or to report the opinion of a
policy on a scale such as ‘strongly disagree’, ‘disagree’, ‘no opinion’, ‘agree’,
‘strongly agree’. Consider an ordered categorical variable with a five-point scale
12345 corresponding to the answer on the opinion of a policy. One
common approach is to treat the assigned integers as continuous data from
a normal distribution. This approach may not lead to serious problems if the
histogram of the observations is symmetrical and with the highest frequency at
the center. This is the situation where most subjects choose the category ‘no
opinion’. To claim multivariate normality of the observed variables, we need to
have most subjects choosing the middle category, for example ‘no opinion’ or
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

140
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
‘no change’, in all the corresponding items. However, for an interesting item in
the questionnaire, most subjects would be likely to select categories at both ends,
for example, ‘strongly agree (strongly disagree)’ or ‘agree (disagree)’. Hence,
in practice, histograms corresponding to most variables are either skewed or
bi-modal. Clearly, routinely treating ordered categorical variables as normal may
lead to erroneous conclusions (see Olsson, 1979a,b; Lee, Poon and Bentler,
1990a,b).
A better approach for assessing these kinds of discrete data is to treat them
as observations that are coming from a hidden continuous normal distribution
with a threshold specification. Suppose for a given data set, the proportions of
1, 2, 3, 4 are 0.05, 0.05, 0.40 and 0.5, respectively. From the histogram given
in Figure 6.1, we see that these discrete data are highly skewed to the right. The
threshold approach for analyzing this highly skewed discreted variable is to treat
the ordered categorical data as manifestations of an underlying normal variable
y. The exact continuous measurements of y are not available, but are related to
the observed ordered categorical variable z such as follows: for k = 1234
z = k
if k−1 < y ≤k
where − = 0 < 1 < 2 < 3 < 4 = , and 12, and 3 are thresholds.
Then, the ordered categorical observations that give the histogram in Figure 6.1
can be captured by N 01 with appropriate thresholds (see Figure 6.2). As 2−
1 can be different from 3 −2, unequal-interval scales are allowed. Hence,
this threshold approach allows flexible modeling. As it is related to a common
normal distribution, it also provides easy interpretation of the parameters. It
should be noted that the ad hoc integral values, here k = 1234, are solely
used to represent the category; only their frequencies are important in the
statistical analysis.
Analysis of SEMs with mixed continuous and ordered categorical data is not
straightforward, because we need to compute the multiple integrals associated
0.05
0.05
1
0.4
0.5
2
3
4
Figure
6.1
Histogram of a hypothetical ordinal categorical data set. This figure and
Figure 6.4 are taken from Lee, Song, Skevington and Hao (2005).

6.1
INTRODUCTION
141
0.05
0.05
0.5
0.40
N [0, 1]
–∞ = α0 α1 = –1.65 α2 = –1.28
α3 = 0.0
α4 =  ∞
Figure
6.2
The underlying normal distribution with a threshold specification.
with the cell probabilities that are induced by the ordered categorical outcomes.
Some multistage methods have been proposed to reduce the computational
burden in evaluating these integrals. The basic procedure of these multistage
methods is to estimate the polychoric and polyserial correlations, and the thresh-
olds at the first stage, derive the asymptotic distribution of the estimates at
the second stage, and analyze the SEM with a covariance structural analysis
approach through a generalized least square (GLS) minimization at the final
stage. Different methods employed at the first stage lead to the different proce-
dures that are given in PRELIS and LISREL (Jöreskog and Sörbom, 1996),
LISCOMP and Mplus (Muthen and Muthen, 2000) and Lee, Poon, and Bentler
(1995). However, the multistage estimators are not statistically optimal and
need to invert at each iteration of the GLS minimization a huge matrix whose
dimension increases very rapidly with the number of manifest variables. Besides
the multistage procedures, Reboussin and Liang (1998) proposed an estimating
equation approach, and Shi and Lee (2000) developed a Monte Carlo EM type
algorithm (Wei and Tanner, 1990) for ML analysis of a factor analysis model.
The main objective of this chapter is to introduce a Bayesian approach on the
basis of the framework given in Chapters 4 and 5 for analyzing SEMs with mixed
continuous and ordered categorical variables. The main idea in handling the
ordered categorical variables in the Bayesian analysis is to treat the underlying
latent continuous measurements as hypothetical missing data, and augment
them with the observed data in the posterior analysis. Using this data augmen-
tation strategy, the model that is based on the complete data set becomes one
with continuous variables. In the posterior analysis, sequences of observations
of the structural parameters, latent variables and thresholds, are simulated from
the joint posterior distribution via a hybrid algorithm that combines the Gibbs
sampler (Geman and Geman, 1984) and the MH algorithm (Metropolis et al.,
1953; Hastings, 1970). By means of the simulated observations, joint Bayesian
estimates of the unknown thresholds, structural parameters and latent variables
are produced together with their standard error estimates. In addition to these
point estimates, we also consider Bayesian model selection via the Bayes factor.

142
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
This chapter is organized as follows. For completeness, SEMs with continuous
and ordered categorical variables are described in Section 6.2. Identification
of an ordered categorical variable is discussed. Bayesian estimation, and the
goodness-of-fit test of a hypothesized model are addressed in Section 6.3.
Detailed discussions of the posterior analysis are presented here. These include
the derivation of the full conditional distributions that are required by the
Gibbs sampler for simulating observations from the joint posterior distribution,
and the implementation of the MH algorithm. The PP p-value for assessing
goodness-of-fit is outlined. Section 6.4 considers Bayesian model comparison.
A path sampling procedure for computing the Bayes factor is described. Two
applications are presented in Sections 6.5 and 6.6 to illustrate the Bayesian
methodologies. One is on the Bayesian selection of the number of factors
in EFA, the other is on Bayesian analysis of quality of life (QOL) data. An
application of the software WinBUGS (Spiegelhalter, Thomas, Best and Lunn,
2002) to obtain the Bayesian solution is outlined in Section 6.6.
6.2
THE BASIC MODEL
Consider the following measurement equation for a p × 1 manifest random
vector vi:
vi = +i +i i = 1	 	 	 n
(6.1)
where 
p ×1 is the vector of intercepts, 
p ×q is the factor loading matrix,
i
q × 1 is a latent random vector and i
p × 1 is a random vector of error
measurements with distribution N 0,  is diagonal and i is independent
with i. We let i
q1×1 and i
q2×1 be latent subvectors of i, and consider
the following structural equation:
i = 	i +
i +i
(6.2)
where 	
q1 ×q1 and 

q1 ×q2 are matrices of regression coefficients, i and i
are independently distributed as N 0 and N 0, where  is a diagonal
covariance matrix. It is assumed that 	0 = Iq1 −	 is a nonzero constant that
is independent with elements in 	. Let  = 
	
, then Equation (6.2) can
be written as i = i +i.
Let v = xy, where x = x1	 	 	 xr, is a subset of variables whose exact
continuous measurements are observable, while y = y1	 	 	 ys is the remaining
subset of variables such that p ≥s = p −r ≥0 and the corresponding continuous
measurements are unobservable. The information associated with y is given
by an observable ordered categorical vector z = 
z1	 	 	 zsT . For any latent
variable in  or , it may be related with manifest variables in either x or y.

6.2
THE BASIC MODEL
143
That is, any latent variable may have continuous and/or ordered categorical
manifest variables as its indicators. The relationship between y and z is defined
by a set of thresholds as follows:
z =
⎡
⎢⎣
z1

zs
⎤
⎥⎦
if
1z1 < y1 ≤1z1+1

szs < ys ≤szs +1

(6.3)
where for k = 1	 	 	 s, zk is an integral value in 01	 	 	 bk, and k0 < k1 <
··· < kbk < kbk+1. In general, we set k0 = −kbk+1 = . For the kth
variable, there are bk +1 categories which are defined by the unknown thresholds
kj. The integral values 01	 	 	 bk of zk are just used for specifying the
categories that contain the corresponding elements in yk. For example, let zk
be any component of z, ‘zk = 3’ provides only the information that yk is in
the interval 
k3k4 that is defined by the threshold parameters k3 < k4.
Here, ‘3’ is just used as a symbol and can be replaced by some other values.
However, in order to indicate the ‘ordered’ nature of the categorical values and
their thresholds, it is desirable to choose an ordered set of integers for z. In the
Bayesian analysis, these integral values are neither directly used in the posterior
simulation nor in any actual computation.
Suppose that the model in relation to the subvector yi = 
yi1	 	 	 yisT of vi
is given by:
yi = y +yi +yi
(6.4)
where y
s × 1 is a subvector of y
s × q is a submatrix of yi
s ×
1 is a subvector of i with diagonal covariance submatrix y of . Let
zi = 
zi1	 	 	 zisT be the ordered categorical observation corresponding to
yii = 1	 	 	 n. In this chapter, the SEM that is defined by Equations (6.1) and
(6.2) is analyzed with the following mixed continuous and ordered categorical
observations:
x1
z1
	
 	 	 	 
xn
zn
	

The model defined by Equations (6.1) and (6.2) is not identified without
imposing appropriate identification conditions. There are two kinds of inde-
terminacies involved in this model. One is the common indeterminacy coming
from the covariance structure of the model that can be solved by the common
method of fixing appropriate elements in 	 and/or 
 at preassigned values.
The other indeterminacy is induced by the ordered categorical variables.

144
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
To tackle the identification problem that is induced by an ordered categorical
variable, we have to keep in mind the following two issues. First, as it is difficult
to obtain a necessary and sufficient condition for identification, we are mainly
interested in finding a reasonable and convenient way to solve the problem.
Second, for an ordered categorical variable, the location and dispersion of its
underlying continuous normal variable are unknown. As we have completely
no idea about these latent values, it is desirable to take a unified scale to
every ordered categorical variable, and the obtained statistical results should be
interpreted in relative sense.
Now, let zk be the ordered categorical variable that is defined with a set
of thresholds and the underlying continuous variable yk whose distribution is
N 2. The indeterminacy is caused by the fact that the thresholds,  and
2 are not identified. One method to solve the problem is to fix 
2 at
some fixed values. However, as these parameters are usually of primary interest,
it is better not to impose restrictions on them. Hence, we propose to impose
the identification conditions on the thresholds, which are the less interesting
nuisance parameters. More specifically, we propose to fix the thresholds at
both ends, k1, and kbk, at preassigned values. This method implicitly picks
measures for the location and the dispersion of yk. For instance, the range
kbk–k1 provides a standard for measuring the dispersion. This method can
be applied to the multivariate case by imposing the above restrictions on the
appropriate thresholds for every component in z. If the model is scale invariant,
the choice of the preassigned values for the fixed thresholds only changes the
scale of the estimated covariance matrix (see Lee, Poon and Bentler, 1990a).
For better interpretation of the statistical results, it is advantageous to assign the
values of the fixed thresholds so that the scale of each variable is the same. One
common method is to use the observed frequencies and the standard normal
distribution, N 01. More specifically, for every k, we may fix k1 = ∗−1
f ∗
k1
and kbk = ∗−1
f ∗
kbk, where ∗
· is the distribution function of N 01, f ∗
k1
and f ∗
kbk are the frequency of the first category, and the cumulative frequencies
of categories with zk < bk, respectively. For linear SEMs, these restrictions imply
that the mean and the variance of the underlying continuous variable yk are
0 and 1, respectively. They have been frequently used in Bayesian analyses of
SEMs with ordered categorical variables, see for example Lee and Zhu (2000)
and Song and Lee (2001). Although there are other possible methods, for
convenience we will use the above method for solving the identification problem
of the ordered categorical variables in this book.
6.3
BAYESIAN ESTIMATION AND GOODNESS-OF-FIT
We will utilize the useful strategy of data augmentation described in Chapter 4
in the Bayesian estimation for the current SEM with continuous and ordered
categorical variables. Let X = 
x1	 	 	 xn and Z = 
z1	 	 	 zn be the observed

6.3
BAYESIAN ESTIMATION AND GOODNESS-OF-FIT
145
continuous and ordered categorical data matrices, respectively; and let Y =

y1	 	 	 yn and  = 
1	 	 	 n be the matrices of latent continuous
measurements and latent variables, respectively. The observed data XZ are
augmented with the latent data Y in the posterior analysis. Joint Bayesian
estimates of , unknown thresholds in  = 
1	 	 	 s and the structural
parameter vector  that contains all unknown parameters in 
and , will be obtained.
In the Bayesian approach, we need to evaluate the posterior distribution
XZ. This distribution is rather complicated. To capture its charac-
teristics, we will try to draw a sufficiently large number of observations from
it such that the empirical distribution of the generated observations is a close
approximation to the true distribution. A good candidate for simulating obser-
vations from the posterior distribution is the Gibbs sampler (Geman and Geman,
1984), which iteratively simulates  and  from the full conditional distri-
butions. However, owing to the presence of the ordered categorical variables,
these conditional distributions are rather complicated to derive and simulating
observations from them is difficult. This motivates the further augmentation
of the latent matrix Y in the posterior analysis, and the consideration of the
joint posterior distribution YXZ. To implement the Gibbs sampler
for generating observations of this posterior distribution, we start with initial
starting values 

0
0
0Y
0, then simulate 

1
1
1Y
1 and so
on according to the following procedure. At the mth iteration with current
values 
m
m
mY
m,
Step
a 
Generate 
m+1 from p

m
mY
mXZ
Step
b 
Generate 
m+1 from p

m+1
mY
mXZ
Step
c  Generate 

m+1Y
m+1 from p
Y
m+1
m+1XZ
The cycle defined above generates 

m+1
m+1
m+1Y
m+1 after
the mth iteration. As m
approaches infinity, the joint distribution of


m
m
mY
m can be shown to approach the joint posterior distribution
YXZ, see Geman and Geman (1984), and Geyer (1992). Conver-
gence of the Gibbs sampler can be monitored by the EPSR values (Gelman,
1996), or plots of several simulated sequences of the individual parameters with
different starting values. The sequences of the quantities simulated from the
joint posterior distribution will be used to calculate the Bayesian estimates and
other related statistics. Conditional distributions required by the Gibbs sampler
are presented below.
6.3.1
Conditional Distributions
We first consider the conditional distribution in Step (a) of the Gibbs sampler.
We note that as the underlying continuous measurements in Y are given, Z gives

146
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
no additional information to this conditional distribution. As vi are conditionally
independent, and i are also mutually independent among themselves and
independent with Z, we have
p
YXZ =
n
i=1
p
ivi
There are two representations for p
ivi. The first representation follows
from a similar derivation to that in Chapter 5 that
ivi
D= N ∗−1T  −1
 
vi −∗−1
(6.5)
in which ∗= 
−1
 +T  −1
 , where
 =
	−1
0 


T +	−T
0
	−1
0 


T 	−T
0



is the covariance matrix of i. For the second representation, we note that
p
ivi ∝p
viip
iip
i
Based on the definition of the model and assumptions,
p
vii ∝exp

−1
2
vi −−iT  −1
 
vi −−i


p
ii ∝exp

−1
2
i −iT  −1
 
i −i


p
i ∝exp

−1
2T
i −1i
	

Consequently, p
ivi is proportional to
exp−1
2T
i −1i +
vi −−iT  −1
 
vi −−i
(6.6)
+
i −iT  −1
 
i −i
Hence, p
YXZ is a product of p
ivi, whose distribution can be
obtained from either Equations (6.5) or (6.6). Based on the practical experience

6.3
BAYESIAN ESTIMATION AND GOODNESS-OF-FIT
147
available so far, simulating observations on the basis of Equations (6.5) or (6.6)
gives similar and acceptable results for statistical inference.
To derive the conditional distributions with respect to the structural param-
eters in Step (b), we note that as  and Y are given, the model defined by
Equations (6.1) and (6.2) reduces to the standard linear model with continuous
data. Hence, these conditional distributions are independent of  and Z. Let
v be the unknown parameters in  and  associated with Equation (6.1),
and let  be the unknown parameters in  and  associated with Equa-
tion (6.2). It is natural to take prior distributions such that p
 = p
vp
.
We first consider the conditional distributions corresponding to v 
p
Vp
V and p
V. In the same way
as before, the following commonly used conjugate type prior distributions are
used:

D= N00 −1
k
D= Gamma0k0k
kk
D= N0kkH0vk k = 1	 	 	 p
where k is the kth diagonal element of , T
k is an 1×rk row vector that only
contains the unknown parameters in the kth row of ; 0k0k00kH0vk
and 0 are hyperparameters whose values are assumed to be given. For k ̸= h,
it is assumed that 
kk and 
hh are independent.
To cope with the case with fixed known elements in , let C = 
ckj be the
index matrix such that ckj = 0 if kj is known and ckj = 1 if kj is unknown, and
rk = q
j=1 ckj. Let k
rk × n be a submatrix of  such that all the jth rows
with ckj = 0 are deleted, and let V∗T
k
= 
v∗
1k	 	 	 v∗
nk with
v∗
ik = vik −k −
q
j=1
kjij
1−ckj
where vik is the kth element of vi, and k is the kth element of . Let Ak =

H−1
0vk + kT
k −1ak = Ak
H−1
0vk0k + kV∗
k and k = 0k + 2−1
V∗T
k V∗
k −
aT
k A−1
k ak +T
0kH−1
0vk0k. Then it can be shown (by similar reasoning to that in
Appendix 4.3) that for k = 1	 	 	 p
p
−1
k V
D= Gamman/2+0kk p
k−1
k V
D= N akkAk
p
V
D= N 
−1
0 +n −1
 −1
n −1
 	V +−1
0 0 
−1
0 +n −1
 −1
(6.7)
where 	V = n
i=1
vi −i/n

148
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
Now, consider the conditional distribution of . As the parameters in 
are just involved in the structural equation, this conditional distribution is
proportional to p
p
, which is independent of V and Z. Let 1 =

1	 	 	 n and 2 = 
1	 	 	 n. Since the distribution of i only involves
, p
2 = p
2. Moreover, it is natural to take prior distribution of 
such that it is independent with the prior distributions of  and . It follows
that
p
p
 ∝p
12p
p
2p

Hence, the marginal conditional densities of 
 and  can again be
treated separately.
Consider a conjugate type prior distribution for  with −1 D= Wq2
R00,
where Wq2
·· denotes the Wishart distribution with q2 degrees of freedom,
0 and the positive definite matrix R0 are the given hyperparameters. It can be
shown (see Chapter 4, Section 4.3.1) that
p

2
D= IWq2

2T

2 +R −1
0 n +0
(6.8)
where IW ·· denotes the inverted Wishart distribution.
In a similar way to before, the prior distributions of elements in 
 are
taken as
−1
k
D= Gamma0k0k
kk
D= N 0kkH0k
where k = 1	 	 	 q1T
k is an 1 × rk row vector that contains the unknown
parameters in the kth row of ; 0k0k0k and H0k are given hyperpa-
rameters. For h ̸= k, 
kk and 
hh are assumed to be independent.
Let C = 
ckj be the index matrix associated with , here k = 1	 	 	 q1
and j = 1	 	 	 q. Let ∗
k be the submatrix of  such that all the jth rows
corresponding to ckj = 0 are deleted; and ∗T
k = 
∗
1k	 	 	 ∗
nk with
∗
ik = ik −
q
j=1
kjij
1−ckj
where ij is the jth element of i. Then, it can be shown that
p
−1
k 
D= Gamman/2+0kk p
k−1
k 
D= N akkAk (6.9)

6.3
BAYESIAN ESTIMATION AND GOODNESS-OF-FIT
149
where Ak = 
H−1
0k + ∗
k∗T
k −1ak = Ak
H−1
0k0k + ∗
k∗
k, and k =
0k +2−1
∗T
k ∗
k −aT
kA−1
k ak +T
0kH−1
0k0k.
Finally, we consider the joint conditional distribution of 
Y given X
and Z. Due to the ordinal nature of the thresholds, and for less complicated
derivation of the conditional distributions, it is natural to use the following
non-informative prior distribution for the unknown thresholds in k:
p
k2	 	 	 kbk−1 ∝c for k2 < ··· < kbk−1 k = 1	 	 	 s
where c is a constant. Given , and the fact that the covariance matrix  is
diagonal, the ordered categorical data Z and the thresholds corresponding to
different rows are also conditionally independent. For k = 1	 	 	 s, let Yk and
Zk be the kth rows of Y and Z, respectively, it follows from Equation (6.4) and
Cowles (1996) that
p
kYkZk = p
kZkp
YkkZk
(6.10)
with
p
kZk ∝
n
i=1

∗
−1/2
yk

kzik+1 −yk −T
yki

−∗
−1/2
yk

kzik −yk −T
yki


(6.11)
and p
YkkZk is a product of p
yikkZk, where
p
yikkZk
D= N 
yk +T
ykiykI
kzik kzik+1
yik
(6.12)
in which yk is the kth diagonal element of yyk is the kth element of yT
yk
is the kth row of y, IA
y is an indicator function which takes 1 if y ∈A and
0 otherwise, and ∗
· denotes the standard normal cumulative distribution
function. As a result,
p
kYkZk ∝
n
i=1


−1/2
yk

yik −yk −T
yki

I
kzik kzik+1
yik
(6.13)
where 
· is the standard normal density. It should be noted that the integral
values of zik are only used in specifying the thresholds in deciding the interval
I
kzik kzik+1
yik in Equation (6.13). As long as the integral values are ordered,
the intervals can be precisely defined and the exact integral values are not
important.

150
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
6.3.2
Implementation
The efficiency of the Gibbs sampler algorithm heavily depends on how easily one
can sample observations from the conditional distributions. It can be seen that
conditional distributions associated with Equations (6.5), (6.7)–(6.9) are the
familiar normal, Gamma and inverted Wishart distributions. Drawing observa-
tions from these distributions is straightforward and fast. From Equations (6.6),
(6.11), and (6.12), we see that it is not easy to sample from p
ivi and
p
kYkZk, which are nonstandard and complex. As the number of
latent continuous measurements Y is large, simulating observations in Step (c)
of the Gibbs sampler plays a more important role in the algorithm.
The Metropolis–Hastings (MH) algorithm ( Metropolis et al., 1953;
Hastings, 1970) is a well-known MCMC method that has been widely used to
simulate observations from a target density via the help of a proposal distribution
from which it is easy to sample. Inspired by the work of Zhu and Lee (1998),
this algorithm is applied here to generate observations from our target densities
p
ivi in Equation (6.6) and p
kYkZk in Equations (6.11) and
(6.12).
For p
ivi, we choose N 02∗−1 as the proposal distribution, where
∗= −1
 +T  −1
 , with
−1
 =
 	T
0  −1
 	0
−	T
0  −1
 
−
T  −1
 	0
−1 +
T  −1
 


where 	0 = Iq1 −	. Let p
·02∗−1 be the proposal density corresponding
to N 02∗−1. The MH algorithm is implemented as follows. At the lth
MH iteration with a current value 

l
i , a new candidate i is generated from
p
·

l
i 2∗−1, and accepting this new candidate with the probability
min

1
p
ivi
p


l
i vi


where p
ivi is given by Equation (6.6). The variance 2 can be chosen
such that the average acceptance rate is approximately 0.25 or more, see Gelman,
Carlin, Stern and Rubin (1995). Note that p
ivi could be simulated from
Equation (6.5) without using the MH algorithm.
In constructing a suitable joint proposal density for k, and Yk in the
MH algorithm for generating observations from the target distribution
p
kYkZk in Equation (6.13), we follow Cowles (1996) to consider
the same factorization of the target distribution as in Equation (6.10):
p
kYkZk = p
kzkp
YkkZk

6.3
BAYESIAN ESTIMATION AND GOODNESS-OF-FIT
151
However, in the proposal distribution, k is not simulated from Equa-
tion (6.11). Rather, at the lth MH iteration, we generate a vector of thresholds

k2	 	 	 kbk−1 from the following univariate truncated normal distribution:
kz ∼N


l
kz2
kI
kz−1
l
kz+1
kz
for z = 2	 	 	 bk −1
(6.14)
where 

l
kz is the current value of kz at the lth iteration of the Gibbs sampler,
and 2
k is an appropriate preassigned constant. Random observations from the
above univariate truncated normal are simulated via the algorithm of Roberts
(1995). It follows from the MH algorithm that the acceptance probability for

kYk as a new observation is min
1Rk, where
Rk = p
kYkZkp


l
k Y

l
k kYkZk
p


l
k Y

l
k Zkp
kYk

l
k Y

l
k Zk

It can be shown from Equations (6.13) and (6.14) that
Rk =
bk−1

z=2
∗



l
kz+1 −

l
kz/k

−∗

kz−1 −

l
kz/k

∗

kz+1 −kz/k

−∗




l
kz−1 −kz/k
 ×
n
i=1
∗
−1/2
yk

kzik+1 −yk −T
yki

−∗
−1/2
yk

kzik −yk −T
yki

∗

−1/2
yk



l
kzik+1 −yk −T
yki

−∗

−1/2
yk



l
kzik −yk −T
yki

(6.15)
As Rk only depends on the old and new values of k and not on the Yk, so it is
not necessary to generate a new Yk in any iteration in which the new value of k
is not accepted (see Cowles, 1996). For an accepted k, a new Yk is simulated
from Equation (6.12).
6.3.3
Bayesian Estimates
It has been shown (Geman and Geman, 1984; Geyer, 1992) that under
mild conditions and for sufficiently large m, the joint distribution of


m
mY
m
m converges at an exponential rate to the desired posterior
distribution YXZ. Hence, YXZ can be approximated
by the empirical distribution of a sufficiently large number of simulated obser-
vations collected after convergence of the algorithm. The convergence of the
algorithm is monitored by the ‘estimated potential scale reduction (EPSR)’

152
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
values suggested by Gelman and Rubin (1992), or by plots of the generated
parameters values from different starting points.
For brevity, let 

j
j
jY
jj = 1	 	 	 J be the observations of

Y generated from YXZ by the proposed hybrid algo-
rithm. The Bayesian estimates of  and , and a Bayesian estimate of  can be
obtained easily via the corresponding sample means of the generated observa-
tions as follows:
ˆ = J −1
J
j=1

j ˆ = J −1
J
j=1

j ˆ = J −1
J
j=1

j
(6.16)
Clearly, these Bayesian estimates are consistent estimates of the corresponding
posterior means, see Geyer (1992). Estimates of the latent variables scores can be
obtained directly from ˆi in ˆ. It is rather difficult to derive analytic forms for
the covariance matrices Var
XZ, Var
XZ and Var
iXZ. However,
estimates of these covariance matrices can be obtained as the corresponding
sample covariance matrices based on the simulated observations. For example,
consistent estimates of Var
XZ and Var
iXZ can be obtained as
follows:
Var


XZ = 
J −1−1
J
j=1


j −ˆ

j −ˆT 
Var


iXZ = 
J −1−1
J
j=1



j
i −ˆi


j
i −ˆiT 
Finally, estimated residual can be obtained through the estimates of the latent
variables and the latent continuous measurements of y, and the parameter
estimates. For example, it follows from Equation (6.4) that
ˆyi = ˆyi −ˆy −ˆy ˆi
6.3.4
Goodness-of-ﬁt of the Model
The goodness-of-fit of a posited model can be tested via the posterior predictive
(PP) p-value (Gelman, Meng and Stern, 1996) as introduced in Chapter 5. Let
H0 be the null hypothesis that the proposed model defined in Equations (6.1)
and (6.2) is plausible. Let V = 
v1	 	 	 vn and Vrep denotes a replication of V,
the PP p-value is defined by
pB = Pr D
VrepY ≥D
VYXZH0

6.3
BAYESIAN ESTIMATION AND GOODNESS-OF-FIT
153
where D
·· is a discrepancy variable. The probability is taken over the joint
posterior distribution of 
VrepY given H0X and Z, where
p
VrepYXZH0 = p
VrepYp
YXZ
For our model, we choose the following 2 discrepancy variable
D 
VrepY =
n
i=1

V
rep
i
−−iT  −1
 
V
rep
i
−−i
which is distributed as 2
pn, a chi-square distribution with pn degrees of
freedom. Here, implicitly, the partition 
T
i T
i T of i is required to satisfy the
model as defined in Equation (6.2). The PP p-value based on this discrepancy
variable is given by
pB
XZ =

Pr 
2
pn ≥D 
VYp
YXZ d d dY
A Rao–Blackwellized type estimate of pB
XZ is equal to
ˆpB
XZ = J −1
J
j=1
Pr 2
pn ≥D 
XY
j
j
j
where 2
pn is a chi-square distribution with pn degrees of freedom. The
computation of ˆpB
XZ is straightforward, since D 
XY
j
jZ
j can be
calculated in each iteration and the tail-area probability of the 2 distribution
can be obtained in any standard statistical software. H0 is not rejected if ˆpB
XZ
is not far away from 0.5.
6.3.5
An Illustrative Example
A small portion of the ICPSR data set collected in the project World Values
Survey 1981–1984 and 1990–1993 (World Value Study Group, 1994) is
analyzed for illustration. The data obtained from Canada are used to illustrate
our proposed method. Eight variables in the original data set (with corre-
sponding question numbers 180, 96, 116, 117, 62, 179, 252 and 254, see
Appendix 1.1) that are related with respondents’ job, religious belief and home-
life are taken as manifest variables in 
v
1	 	 	 v
8T . After deleting the cases
with missing entries, the sample size is 470. Among them, 
v
1v
2 are related
to life, 
v
3v
4 are related to job satisfaction, 
v
5v
6 are related to religious

154
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
belief and 
v
7v
8 are related to job attitude. Measurements associated with
v
5 and v
6 are based on a five-point scale, while all others are measured by a
10-point scale. For convenience, variables v
5 and v
6 are treated as ordered
categorical and the others are treated as continuous.
As an illustration, consider the structural equation model with the following
measurement equation:
v = ++
(6.17)
T =
⎡
⎢⎢⎣
1
21
0
0
0
0
0
0
0
0
1
42
0
0
0
0
0
0
0
0
1
63
0
0
0
0
0
0
0
0
1
84
⎤
⎥⎥⎦
where 1’s and 0’s are fixed parameters, and  = 
1212T . In this spec-
ification, the endogenous latent variables 1 and 2 in  can be interpreted
as factors about life and job satisfaction, and the exogenous latent variables
1 and 2 can be interpreted as factors about religious belief and job attitude,
respectively. The structural equation for the latent variables is given by
1
2
	
=
 0
0
21
0
	
1
2
	
+
11
0
0
22
	
1
2
	
+
1
2
	

here the 0’s are also fixed parameters. There are four thresholds for each ordered
categorical variable that is measured with a five-point scale. After fixing the
smallest and the largest thresholds for identification, each ordered categorical
variable associates with two unknown thresholds. Hence there are a total of
four unknown thresholds. Including the other structural parameters, there are
a total of 32 unknown parameters in this model.
The MCMC method described in previous subsections is used to obtain the
Bayesian estimates of the unknown parameters. Hyperparameter values in the
conjugate prior distributions are set equal to the estimates that are obtained
from an initial run with non-informative priors. The Gibbs sampler in the actual
estimation converged fairly quickly within 500 iterations. To give some idea
about the convergence of the MCMC method, plots of sequences of some
parameters’ values with different starting values are displayed in Figure 6.3. The
PP p-value is equal to 0.563, indicating that the proposed model fits the sample
data. The Bayesian estimates and their associated standard errors estimates are
reported in Table 6.1. As we fix the smallest and the largest thresholds of
the ordered categorical variables associated with v
5 and v
6 according to the
commutative distribution of N 01, the estimates of 5 and 6 are close
to zero and the estimates of 5 and 6 are less than the other estimates of
elements in .

6.4
BAYESIAN MODEL COMPARISON
155
(a)
2
1
0
–1
–2
(b)
1.0
0.5
–0.5
0.0
–1.0
(c)
5
4
3
2
1
0
(d)
0
.3
0
.2
0
.1
0
.0
(e)
5
4
3
2
1
0
(f)
0
.1
5
.0
0
.0
–0.5
0
500
1000
1500
2000
0
500
1000
1500
2000
0
500
1000
1500
2000
0
500
1000
1500
2000
0
500
1000
1500
2000
0
500
1000
1500
2000
Figure
6.3
(a), (b), (c), (d), (e) and (f) are plots of parallel sequences corresponding
to different starting values of 21, 21, 3, 11, 1 and 12 against iterations, in the
ICPSR example.
6.4
BAYESIAN MODEL COMPARISON
The Bayes factor introduced in Chapter 5 is applied to compare different
competing models that are defined in the context of Equations (6.1) and (6.2).
Let M0 and M1 be two competing models of interest. Given the observed data

XZ, the Bayes factor for comparing M1 with M0 is
B10 = p
XZM1
p
XZM0

156
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
Table
6.1
Bayesian estimates and standard error estimates.
Parameters
EST
SE
Parameters
EST
SE
1
8.414
0079
21
−0112
0094
2
7.999
0076
11
0.136
0132
3
3.871
0113
22
−0539
0141
4
3.204
0096
11
0.453
0077
5
0.007
0053
12
0.086
0052
6
−0014
0055
22
0.890
0147
7
8.049
0071
1
1.309
0171
8
7.446
0091
2
1.925
0511
21
1.006
0105
1
1.112
0144
42
1.015
0269
2
0.853
0149
63
−1004
0137
3
4.087
0668
84
1.584
0230
4
2.714
0665
12
−0166
0032
5
0.547
0072
13
0.332
0041
6
0.558
0079
22
0.479
0042
7
1.669
0174
23
0.733
0035
8
2.289
0351
A path sampling procedure is used here for computing logarithm B10. Similar
to the Bayesian estimation, the observed data 
XZ are augmented with the
latent data 
Y in the computation. Suppose Mt is a linked model that links
M1 and M0. Let p
XZYt be the complete-data likelihood function
under the linked model Mt, and
U 
XZYt = d
dt log p 
XZYt
(6.18)
Moreover, let t
ss = 0	 	 	 S + 1 be grids in [0,1] such that t
0 = 0 < t
1
< ··· < t
S = t
S+1 = 1. The logarithm Bayes factor is computed as
log B10 = 1
2
S
s=0

t
s+1 −t
s
U 
s+1 +U 
s
(6.19)
where
U 
s = J −1
J
j=1
U
XZ
j
jY
j
jt
s
(6.20)
in which 

j
jY
j
jj = 1	 	 	 J are simulated observations from
the posterior distribution p
YXZt
s. These observations are drawn
via the MCMC methods in the estimation described in Section 6.3.

6.4
BAYESIAN MODEL COMPARISON
157
The prior distributions of the thresholds and the structural parameters are
only involved in the posterior simulation in generating the 

j
jY
j
j,
but not in the direct computation of log B10. As the thresholds are nuisance
parameters that are not involved in the competing models, the choice of non-
informative priors is acceptable (see discussions at the end of Chapter 5). Infor-
mative prior distributions should be used for the structural parameters that are
involved in the competing models. The conjugate prior families used in the
estimation are convenient and good choices.
The choice of the linked model Mt depends on the competing models M0 and
M1. We have to approach this on a problem-by-problem basis. Usually, defining
a suitable Mt is straightforward. To give a more specific illustration, consider
the following models with different measurement and structural equations:
M0 
v = +0+
and
 = 	0+
0 +
M1 
v = +1+
and
 = 	1+
1 +
(6.21)
These two models can be linked up by the following Mt with t ∈01:
Mt 
v = +
1−t0 +t1+
and
 = 
1−t	0 +t	1+
1−t
0 +t
1 +
(6.22)
Clearly, when t = 0Mt = M0; when t = 1Mt = M1.
Based on the model defined in Equations (6.1) and (6.2), and the nature of
the ordered categorical data, the complete-data likelihood function is equal to
p
XZYt = p
ZXYtp 
XYtp
t
= 
2−np/2−n/2 exp

−1
2
n
i=1

vi −−tiT  −1
 
vi −−ti

IAi
yi
×
2−nq2/2−n/2 exp

−1
2
n
i=1
T
i −1i

×
2−nq1/2
Iq1 −	tn−n/2
×exp

−1
2
n
i=1

i −	ti −
tiT  −1
 
i −	ti −
ti


where t = 
1 −t0 + t1	t = 
1 −t	0 + t	1
t = 
1 −t
0 + t
1IA
y
is an indicator function which takes the value 1 if y ∈A and zero otherwise,
and
Ai = 
1zi11zi1+1×···×
szis szis +1

158
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
Note that for every vi, there exists one and only one Ai such that yi is in Ai and
IAi
yi = 1, hence the corresponding value of the density function is nonzero.
The logarithm of complete-data likelihood corresponding to Mt is equal to
log p
XZYt = log p
XZYt+logp
t
=−1
2


p +qn log
2+n log +n log −2n log Iq1 −	t+n log 
+
n
i=1

vi −−tiT  −1
 
vi −−ti
+
n
i=1

i −	ti −
tiT  −1
 
i −	ti −
ti
+
n
i=1
T
i −1i


Note that as IAi
yi = 1, it does not appear in the log-likelihood. By differenti-
ation with respect to t, we have
U
XZYt =
n
i=1


vi −−tiT  −1
 t0i
+
i −	ti −
tiT  −1
 
	t0i +
t0i


where t0 = 1 −0	t0 = 	1 −	0 and 
t0 = 
1 −
0. Observations from the
posterior distribution p
YXZt are then simulated by the MCMC
method as described in Section 6.3. The Bayes factor is estimated by using
Equations (6.19) and (6.20).
We use the real example given in Section 6.3.5 to illustrate path sampling in
computing the Bayes factor for model comparison. Suppose we are interested in
the following non-nested models whose measurement equation is the same as
given in Equation (6.17) but with the following different structural equations:
M0 
1
2
	
=
11
12
21
22
	1
2
	
+
1
2
	

M1 
1
2
	
=
 0
0
21
0
	1
2
	
+
11
0
0
22
	1
2
	
+
1
2
	

M2 
1
2
	
=
 0
0
21
0
	1
2
	
+
 0
12
21
0
	1
2
	
+
1
2
	

These competing models are special cases of the models given in Equa-
tion (6.21), hence the linked models can be easily obtained as special cases of

6.5
BAYESIAN SELECTION OF THE NUMBER OF FACTORS IN EFA
159
Mt given in Equation (6.22). In the application of path sampling, the number
of grids in [0, 1] is taken to be 20. Again we use the same prior inputs as
before and take 500 burn-in iterations in the MCMC method in simulating
the observations. After convergence, 2000 observations are collected for each
t
s for computing U 
s. We find that

log B01 = −0206 and

log B12 = 6588.
According to the criterion given in Table 5.1, these results suggest that M1
is better than M0 and is significantly better than M2. Hence, M1 is selected.
Bayesian estimates of the parameters in M1 can be obtained in the computa-
tion of

log B12. The data have been reanalyzed using different prior inputs. We
obtain close values of the logarithm of Bayes factors and the same conclusion
as above.
6.5
APPLICATION 1: BAYESIAN SELECTION OF THE
NUMBER OF FACTORS IN EFA
Factor analysis (Lawley and Maxwell, 1971) is the most basic structural equa-
tion model and it is still very useful in behavioral, social and psychological
research. One fundamental problem associated with factor analysis is to select
the appropriate number of factors for an observed data set. For continuous data,
this problem has been addressed by Akaike (1987) via the Akaike’s informa-
tion criterion (AIC) that is analogous to the final prediction error (see Akaike,
1970). Another related criterion is the BIC which is a rough approximation of
the Bayes factor. For factor analysis models with ordered categorical variables,
AIC and BIC depend on the observed-data likelihood function that involves
rather complicated multiple integrals. The computation of these criteria is not
straightforward. The main purpose of this section is to illustrate the application
of path sampling for computing the Bayes factor in solving this fundamental
problem.
For completeness, we present the following exploratory factor analysis model
with a p ×1 manifest random vector vi:
vi = i +i i = 1	 	 	 n
(6.23)
where 
p ×q is the factor loading matrix, i
q ×1 is a latent random vector
with distribution N 0I, and i
p × 1 is a random vector of error measure-
ments with distribution N 0, where  is diagonal and i is independent
of i. Let v = xy, where x is an r ×1 vector of variables whose exact contin-
uous measurements are observable, whereas y is an s ×1 vector of the remaining
variables such that the corresponding continuous measurements are unobserv-
able, and their information is given by an observable ordered categorical vector
z as described in Section 6.2. Let X = 
x1	 	 	 xn and Z = 
z1	 	 	 zn be the

160
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
observed data matrices. Let  = 
1	 	 	 n be the q × n matrix of latent
factors and let Y = 
y1	 	 	 yn be the latent continuous measurements under-
lying Z. Let  be the vector that contains all the unknown thresholds and  be
the vector that contains all the unknown parameters in  and .
Consider the application of the path sampling to select one of the following
two competing models M0 and M1:
M0 
v = 101 +···+q0q +
M1 
v = 111 +···+r1r +
where q < r, and h0 and h1 are the columns of the loading matrices under M0
and M1 respectively. Clearly, M0 corresponds to a model with q factors whilst
M1 corresponds to a model with r factors. These two competing models are
linked up by t in 01 as below:
Mt 
v =
1−t10 +t111 +···+
1−tq0 +tq1q +t
q+11q+1
+···+tr1r +
Clearly Mt reduces to M0 if t = 0, and it reduces to M1 if t = 1. Let
t = 
1−t10 +t11	 	 	 
1−tq0 +tq1t
q+11	 	 	 tr1, the corre-
sponding logarithm complete-data likelihood function is given by
log p
YXZt = −1
2pn log
2+n log +
n
i=1
T
i i +
n
i=1

vi −tiT  −1
 
vi −ti
where vi = xiyi and yi is in one and only one Ai = 
1zi11zi1+1 × ··· ×

szis szis +1. By differentiation with respect to t, we have
U
YXZt =
n
i=1

vi −tiT  −1
 t0i
(6.24)
where t0 = 
11 −10	 	 	 q1 −q0
q+11	 	 	 r1. The logarithm of
Bayes factor for selecting M0 or M1 can be estimated by using expressions
similar to those in Equations (6.19) and (6.20). The main computation

6.5
BAYESIAN SELECTION OF THE NUMBER OF FACTORS IN EFA
161
is on simulating a sample 

jY
j
jj = 1	 	 	 J from the poste-
rior distribution p
YXZt. The task can be done by the Gibbs
sampler which iteratively generates observations from conditional distributions
p
YXZp
YXZ and p
YXZ as before. In the
derivation of these conditional distributions, the following conjugate prior
distributions for the factor loadings and unique variance are used: for k =
1	 	 	 p
−1
k
D= Gamma
0k0k kk
D= N0kkH0vk
where T
k is the kth row of , k is the kth diagonal element of , and
0kH0vk0k and 0k are given hyperparameters. Non-informative prior is
used for the nuisance threshold parameters.
6.5.1
A Simulation Study
Results obtained from a simulation study are presented here to illustrate the
path sampling approach for computing the Bayes factor in selecting the number
of factors in the model. The true model is a two-factor model with the following
population values:
T =
08
00
08
08
00
00
08
06
00
08
00
00
08
08
00
06


 = diag036036036036036036036028
A data set vii = 1	 	 	 n is generated according to Equation (6.23). The
continuous measurements v
7 and v
8 are transformed to ordered categor-
ical observations via the following thresholds: 1 = 2 = 
−10−020210.
Here, the first six variables are continuous and the last two are ordered cate-
gorical.
To identify the ordered categorical variables, the first and the last elements of
1 and 2 are fixed at -1.0 and 1.0, respectively. To eliminate the indeterminacy
of rotation in a model with q factors, we fix ab = 00 with ab = 1	 	 	 q
and a < b. This condition is sufficient to identify the covariance structure of
the model but not enough to identify  completely because each column
of  can have a change of sign. Thus, we require to fix more elements in
 in order to remove this indeterminacy. We select these elements by the
information obtained from an initial estimation on the basis of non-informative
prior distributions and ab with fixed at 0.0 for ab = 1	 	 	 q and a < b.
Then, for each column, we pick the entry associated with the largest estimate

162
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
and fix it at some value. Using this method in the simulation study, we fix
12 = 00, 31 = 08 and 52 = 08 in assessing the two-factor model; and
additionally fix 13 = 00, 23 = 00 and 43 = 08 in assessing the three-factor
model.
Using a simulated sample of size n = 300, the path sampling procedure is
implemented using 20 grids in [0,1], and the following hyperparameter values
in the prior distributions: 0k fixed at the true values, 0k = 10 and 0k =
8, for all k = 1	 	 	 8. We observe that the hybrid algorithm for simulating
observations in the calculation of ¯U
s converges quickly within 200 iterations. At
each t
s, a total of J = 2000 observations are collected after a ‘burn-in’ phase of
200 iterations. The logarithm Bayes factors are computed via Equations (6.24)
and (6.20). We find that

log B21 = 19523 and

log B32 = −3145. These results
indicate that a two-factor model is better than either a one-factor model or a
three-factor model. Hence, the two-factor model is selected. This agrees with
the true situation.
In providing some insight into the sensitivity of the Bayes factor to the
prior inputs, we perturb the hyperparameters alternatively as follows: (I) For all
k = 1	 	 	 p, fix 0k = 10, and 0k = 8; hyperparameters in 0k are selected
to be (i) half of the true values and (ii) twice the true values. (II) For all k =
1	 	 	 p, fix 0k at the true values, 0k and 0k are selected as follows: (i) 0k =
8, 0k = 6, and (ii) 0k = 12, 0k = 10. The averages, minimum and maximum
of the estimated log B21 on the basis of 50 replications under (I, i), (I, ii), (II, i)
and (II, ii) are equal to 189051367624058188041377424108,
195081419024822 and 184201357523643 respectively. The
values corresponding to the estimated log B32 are −3393−4335−2468,
−3474−4440−2534, −3206−4120−2276 and −3575−4535
−2654, respectively. Hence, the Bayes factors estimated through the proposed
method give the same statistical conclusion under these prior inputs and n =
300. As expected, the Bayesian estimates of the unknown parameters under
different prior inputs are close to each other. To provide some idea about the
accuracy of the estimates, the mean of the Bayesian estimates and the root mean
squares (RMS) of the estimates and the true values of the selected two-factor
model with prior inputs 0k = 100k = 8 and 0k is equal to twice the true
values are reported in Table 6.2.
Using a SUN Enterprise 4500 Server, the computational time for calculating
log B21 and log B32 in each replication is roughly 6 and 8 minutes, respectively.
6.5.2
A Real Example
To provide a real example for illustrating the proposed method, a small portion
of the ICPSR data set collected in the project World Values Survey 1981–1984
and 1990–1993 (World Values Study Group, 1994) is analyzed. As an illustra-
tion, only the data obtained from USA were used. Nine variables in the original
data set (variables 132, 116, 117, 129, 115, 241, 336, 337 and 339) were taken

6.6
BAYESIAN SELECTION OF THE NUMBER OF FACTORS IN EFA
163
Table
6.2
Mean and RMS of Bayesian estimates corresponding to a two-factor
model: simulation study.
Par
Mean
RMS
Par
Mean
RMS
11 = 08
0.776
0047
1 = 036
0.404
0054
21 = 00
−0011
0063
2 = 036
0.412
0062
22 = 08
0.792
0047
3 = 036
0.390
0042
32 = 00
−0004
0041
4 = 036
0.409
0058
41 = 08
0.783
0051
5 = 036
0.396
0049
42 = 00
0.002
0048
6 = 036
0.409
0057
51 = 00
−0004
0070
7 = 036
0.443
0092
61 = 00
−0011
0075
8 = 028
0.381
0108
62 = 08
0.788
0045
72 = −02
−0207
0054
71 = 08
0.809
0059
73 = 02
0.221
0058
72 = 00
−0040
0062
82 = −02
−0193
0058
81 = 06
0.605
0057
83 = 02
0.214
0048
82 = 06
0.609
0066
This table and Table 6.3 are taken from Lee and Song (2002).
as manifest variables in v = 
v
1	 	 	 v
9T . Questions corresponding to these
variables are presented in Appendix 1.1. Variables v
1v
2v
3 were measured
via a 10-point scale and hence were treated as continuous for convenience;
variables v
4 and v
5 are measured via a three-point scale, variable v
6 via a
four-point scale and variables v
7v
8 and v
9 via a five-point scale; these vari-
ables are treated as ordered categorical. For brevity, observations with missing
entries are deleted and the remaining sample size is 1048.
The data set is first analyzed with a three-factor model using non-informative
prior distributions, in which 1213 and 23 are fixed at 0.0. On the basis of the
results in this initial analysis, we obtain hyperparameter values, and fixed 2172
and 93 at 1.0 to identify the model completely. After fixing these parameters,
there are a total of 37 unknown parameters which involve the unknown loadings,
unique variances and thresholds 627273828392 and 93. The
path sampling procedure is applied to estimate the Bayes factors for selecting
the appropriate number of factors, using 20 grids in [0, 1]. We observe that the
Gibbs sampler again converges very quickly within 200 iterations. Again, a total
of J = 2000 observations are collected after a ‘burn-in’ phase of 200 iterations.
We find that

log B21 = 1166

log B32 = 1970 and

log B43 = −1129. Hence,
the model with three factors is selected. The corresponding PP p-value of this
model is 0.558, which indicates that the three factor model fits the observed
data. The Bayesian estimates and their standard error estimates associated with
this model are reported in Table 6.3. To achieve better interpretation, the
estimated factor loading matrix can be rotated.

164
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
Table
6.3
Bayesian estimates of parameters and their standard error estimates for a
three-factor model: ICPSR data set.
Par
EST
SE
Par
EST
SE
Par
EST
SE
11
0.859
0075
62
0.086
0043
4
0.976
0088
22
−0304
0130
63
−0271
0043
5
0.616
0061
31
1.507
0099
71
0.269
0090
6
0.915
0057
32
−0443
0144
73
0.098
0058
7
0.366
0050
33
0.344
0087
81
0.077
0049
8
0.858
0051
41
0.122
0051
82
0.382
0044
9
0.535
0076
42
0.016
0056
83
0.055
0046
62
0.328
0034
43
0.230
0060
91
−0104
0052
72
1.116
0052
51
−0487
0059
92
0.036
0073
73
1.657
0062
52
0.316
0078
1
4.190
0195
82
−0107
0034
53
−0111
0059
2
1.798
0115
83
0.472
0036
61
−0048
0039
3
2.604
0224
92
−0782
0046
93
0.162
0046
6.6
APPLICATION 2: BAYESIAN ANALYSIS OF QUALITY OF
LIFE DATA
There is increasing recognition that measures of quality of life (QOL) and/or
health-related QOL have great value for clinical work and the planning and
evaluation of health care as well as for medical research. It has been generally
accepted that QOL is a multidimensional concept (Staquet, Hayes and Fayes,
1998)thatisbestevaluatedbyanumberofdifferentlatentconstructssuchasphys-
ical function, health status, mental status and social relationships. As these latent
constructs often cannot be measured objectively and directly, they are treated
as latent variables in QOL analysis. The most popular method that is used to
assess a latent construct is by a survey which incorporates a number of related
items that are intended to reflect the underlying latent construct of interest.
EFA has been used as a method for exploring the structure of a new QOL
instrument (The WHOQOL Group, 1998; Fayer and Machin, 1998), while
CFA has been used to confirm the factor structure of the instrument. SEMs
that are based on continuous observations with a normal distribution have also
been applied to QOL analyses (Power, Bullingen and Hazper, 1999).
Items in a QOL instrument are usually measured on an ordered categorical
scale, typically with three- to five-points. The discrete ordinal nature of the items
also attracts much attention in QOL analysis (Fayer and Machin, 1998; Fayer
and Hand, 1997). It has been pointed out that non-rigorous treatments of the
ordinal items as continuous can be subjected to criticism (Glonek and McCullagh,
1995), and models such as the item response model and ordinal regression
that take into account the ordinal nature are more appropriate (Olschewski and
Schumacker, 1990). The aim of this section is to apply the Bayesian methods
for analyzing a common QOL instrument with ordered categorical items.

6.6
BAYESIAN ANALYSIS OF QUALITY OF LIFE DATA
165
6.6.1
A Synthetic Illustrative Example
This instrument WHOQOL-100 (Power, Bullingen and Hazper, 1999) was
established to evaluate four latent constructs. The first seven items (Q3 to Q9)
are intended to address physical health, the next six items (Q10 to Q15) are
intended to address psychological health, the three items (Q16, Q17, Q18)
that follow are for social relationships, and the last eight items (Q19 to Q26)
are intended to address environment. In addition to the 24 ordered categorical
items, the instrument also includes two ordered categorical items for the overall
QOL (Q1) and general health (Q2), giving a total of 26 items. All of the
items are measured with a five-point scale (1 = ‘not at all/very dissatisfied’;
2 = ‘a little/dissatisfied’; 3 = ‘moderate/neither’; 4 = ‘very much/satisfied’; 5
= ‘extremely/very satisfied’). The sample size of the whole data set is extremely
large. To illustrate the Bayesian methods, we only analyze a synthetic data set
with sample size n = 338. The frequencies of all the ordered categorical items
are presented in Table 6.4. As can be seen from the table, many items are skewed
to the right. Treating these ordered categorical data as coming from normal is
not correct. Hence, our Bayesian approach that takes into account the discrete
nature of the data is applied to analyze this ordered categorical data set.
To illustrate the path sampling procedure, we compare an SEM with four
exogenous latent variables with another SEM with three exogenous latent vari-
ables, see Lee, Song, Skevington and Hao (2005). Let M1 be the SEM whose
measurement equation is defined by
yi = 11i +i
(6.25)
where 1i = 
ii1i2i3i4T , i is distributed according to N 01,
and
T
1 =
⎡
⎢⎢⎣
1
21
0
0
···
0
0
0
···
0
0
0
0
0
0
···
0
0
0
1
42
···
92
0
0
···
0
0
0
0
0
0
···
0
0
0
0
0
···
0
1
113
···
153
0
0
0
0
0
···
0
0
0
0
0
···
0
0
0
···
0
1
174
184
0
0
···
0
0
0
0
0
···
0
0
0
···
0
0
0
0
1
205
···
265
⎤
⎥⎥⎦
The structural equation of M1 is defined by
 = 11 +22 +33 +44 +
(6.26)
where the distributions of 
1234T and  are independently distributed as
N 01, and N 02
1, respectively. Let M2 be the SEM whose measurement
is defined by
yi = 22i +i
(6.27)

166
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
Table
6.4
Frequencies of the questions in the WHOQOL data set.
1
2
3
4
5
Q1 Overall QOL
2
34
75
160
67
Q2 Overall health
25
89
71
117
36
Q3 Pain and discomfort
16
49
78
111
84
Q4 Medical treatment dependence
17
48
65
83
125
Q5 Energy and fatigue
16
53
107
86
76
Q6 Mobility
13
33
62
95
135
Q7 Sleep and rest
23
62
73
116
64
Q8 Daily activities
9
55
63
158
53
Q9 Work capacity
19
71
79
116
53
Q10 Positive feeling
8
22
93
165
50
Q11 Spirituality/personal beliefs
8
29
99
137
65
Q12 Memory and concentration
4
22
148
133
31
Q13 Bodily image and appearance
3
30
106
112
87
Q14 Self-esteem
7
38
104
148
41
Q15 Negative feeling
4
35
89
171
39
Q16 Personal relationship
5
16
59
165
93
Q17 Sexual activity
25
48
112
100
53
Q18 Social support
7
6
73
164
88
Q19 Physical safety and security
4
20
147
129
38
Q20 Physical environment
7
20
142
126
43
Q21 Financial resources
15
34
140
87
62
Q22 Daily life information
4
22
102
154
56
Q23 Participation in leisure activity
15
76
102
108
37
Q24 Living condition
4
12
35
173
114
Q25 Health accessibility and quality
4
20
59
205
50
Q26 Transportation
5
16
43
188
86
Total
269
960
2326
3507
1726
where 2i = 
ii1i2i3T . i is distributed according to N 02, and
T
2 =
⎡
⎢⎣
1
21
0
0
···
0
0
0
···
0
0
0
···
0
0
0
1
42
···
92
0
0
···
0
0
0
···
0
0
0
0
0
···
0
1
113
···
153
0
0
···
0
0
0
0
0
···
0
0
0
···
0
1
174
···
264
⎤
⎥⎦
The structural equation of M2 is defined by
 = 11 +22 +33 +
(6.28)
where the distributions of 
123T and  are independently distributed as
N 02, and N 02
2, respectively. Bayesian analyses are conducted using

6.6
BAYESIAN ANALYSIS OF QUALITY OF LIFE DATA
167
the conjugate prior distributions. The hyperparameter values corresponding to
the prior distributions of the unknown loadings in 1 or 2 are all taken to
be 0.8; those corresponding to 1234 are {0.6, 0.6, 0.4, 0.4}; those
corresponding to 1 and 2 are 0 = 30, R−1
0 = 8I4 and R−1
0 = 8I3, respectively;
H0vk = 025I26H0k = 025I4, or H0k = 025I30k = 0k = 10, and 0k =
0k = 8. In the path sampling procedure in computing the Bayes factor, we
take S = 10 and J = 2000 after a ‘burn-in’ phase of 1000 iterations.
We first compare M1 with the following simple model M0:
M0  yi = i
where i
D= N0 and  is a diagonal matrix. The measurement equation of
the linked model is defined by Mt  yi = t1i +i. We obtain

log B10 = 8105.
Clearly, M1 is better than M0. Similarly, M2 and M0 can be compared via the
path sampling procedure. We find that

log B20 = 5765, which suggests that M2
is better than M0. From the above results, we can obtain an estimate of log B12,
which is equal to 23.40. Hence, M1, the SEM with one endogenous and four
exogenous latent variables is selected. Bayesian estimates of the unknown struc-
tural parameters in M1 are presented in Figure 6.4. The less interesting threshold
estimates are not presented. All the factor loading estimates, except ˆ174 that
associates with the indicator ‘sexual activity’, are high. This indicates a strong
association between each of the latent variables and their corresponding indi-
cators. From the meaning of the items, 123 and 4 can be interpreted
as the overall QOL, physical health, psychological health, social relationship
and environment, respectively. The estimates of correlations among the exoge-
nous latent variables are equal to 068004320632068507360698.
As expected, these correlations are high. The estimated structural equation that
addresses the relations of QOL with the latent constructs about physical and
psychological health, social relationship and environment is
 = 07241 +03192 +01693 −00414
Hence, physical health has the most important effect on QOL, followed in turn
by psychological health and social relationship, while the effect of environment
is not important.
6.6.2
Application of WinBUGS
For
SEMs
with
ordered
categorical
variables,
the
software
WinBUGS
(Spiegelhalter, Thomas, Best and Lunn, 2003) can produce Bayesian estimates
of the structural parameters and latent variable estimates in the model, as well

168
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
0.69
0.66
0.48
η
ξ1
ξ3
0.70
Q3
Q4
Q5
Q6
Q7
Q8
Q9
Q10
Q11
0.69
0.52
0.46
0.74
0.34
0.45
0.52
0.71
0.75
0.76
0.51
0.64
Q12
Q13
Q15
ξ2
ξ4
Q18
Q17
Q16
Q24
Q23
Q22
Q20
Q19
Q26
Q25
Q1
0.99
0.52
0.61
0.76
0.72
0.57
0.64
0.77
0.78
0.73
0.78
0.49
0.33
0.76
Q2
0.17
0.72
0.32
–.04
1.0∗
1.0∗
1.0∗
1.0∗
0.67
0.98
0.77
0.64
0.29
0.46
0.48
0.51
0.48
0.42
0.32
0.84
0.71
0.63
Q21
Q14
0.80
0.90
1.00
1.10
0.90
0.67
0.72
1.0∗
0.65
0.84
0.65
Figure
6.4
Path diagram and Bayesian estimates of parameters in the analysis of the
QOL data. Note that Bayesian estimates of 11, 22, 33 and 44 are 0.648, 0.706,
0.694 and 0.680, respectively.
as their standard error estimates, by means of a sufficiently large number of
observations simulated by MCMC methods. In our Bayesian treatment of the
ordered categorical variables, we fix the thresholds at both ends in order to
solve the identification problem, then the other unknown thresholds are simul-
taneously estimated with the structural parameters  and  in
the model. However, according to our understanding of WinBUGS, it is not
straightforward to apply this software to estimate the unknown thresholds and
structural parameters simultaneously. Hence, in applying WinBUGS, we first
estimate all the thresholds through the method as described in Section 6.2,
using the observed frequencies and the distribution function of N 01. Then,
the thresholds are fixed in the WinBUGS program in producing the Bayesian
solutions. Note that this procedure may underestimate the standard error esti-
mates. Hence, hypothesis testing should be conducted through DIC, rather
than the z-score that depends on the standard error estimate.

6.6
BAYESIAN ANALYSIS OF QUALITY OF LIFE DATA
169
0.42
Q4
Q5
Q6
Q8
Q7
Q9
Q10
Q11
Q3
0.62
0.62
0.46
Q12
Q13
Q15
ξ2
ξ4
ξ3
ξ1
Q18
Q17
Q16
Q24
Q23
Q22
Q20
Q19
Q26
Q25
Q1
0.96
0.46
0.52
0.70
0.67
0.53
0.57
0.73
0.71
0.66
0.71
0.39
0.25
0.85
Q2
0.15
0.76
0.35
–.02
1.0∗
1.0∗
1.0∗
1.0∗
0.79
1.14
0.87
0.72
0.22
0.36
0.38
0.39
0.39
0.31
0.27
0.95
0.81
0.84
0.73
Q21
0.57
0.45
0.69
0.70
0.65
0.47
0.39
0.28
0.71
0.40
Q14
η
0.91
1.05
1.14
1.25
1.0∗
0.79
0.75
1.09
0.78
0.76
0.98
0.77
Figure
6.5
Path diagram and Bayesian estimates of parameters in the analysis of the
QOL data obtained via WinBUGS. Note that Bayesian estimates of 11, 22, 33 and
44 are 0.493, 0.584, 0.600 and 0.530, respectively.
We apply WinBUGS to the synthetic QOL data set as discussed in the
previous subsection. Bayesian solutions are obtained under the selected model
M1 with the same conjugate prior distributions and hyperparameter inputs.
Note that it is desirable to give initial values for  and  in order to save
computer time. The DIC value corresponding to this model is 19 537.9.
Bayesian estimates of the unknown structural parameters are presented in
Figure 6.5. We observe that the estimates respectively presented in Figures 6.4
and 6.5 are not as close as expected. The possible reason may be that the treat-
ments of the thresholds by the two programs are different. The completely

170
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
(a)
(c)
0
100
200
300
(d)
0
100
200
300
(b)
0
100
200
300
hat{epsilon}_i18
2
1
0
–1
–2
1.0
0.5
0.0
–0.5
–1.0
hat{epsilon}_i8
2
1
0
–1
–2
0
100
200
300
hat{epsilon}_i1
2
1
0
–1
–2
hat{delta}_i
Figure
6.6
Estimated residual plots, (a) ˆi1, (b) ˆi8, (c) ˆi18 and (d) ˆi.
standardized solutions corresponding to these two sets of estimates are closer,
for example, the estimates of the correlations among the exogenous latent
variables are equal to 066404060610066107050679. Some esti-
mated residual plots, ˆi1, ˆi8, ˆi18 and ˆi versus the case number are displayed
in Figure 6.6, while plots of estimated residual ˆi1 and ˆi versus ˆi1, ˆi2, ˆi3,
ˆi4 and ˆi are presented in Figures 6.7 and 6.8 respectively. Other estimated

6.6
BAYESIAN ANALYSIS OF QUALITY OF LIFE DATA
171
hat{delta}_i
2
1
0
–1
–2
hat{delta}_i
2
1
0
–1
–2
hat{delta}_i
2
1
0
–1
–2
hat{delta}_i
2
1
0
–1
–2
(c)
(b)
–2
–1
0
1
2
(e)
–2
–1
0
1
2
(d)
–2
–1
0
1
2
–2
–1
0
1
(a)
hat{delta}_i
2
1
0
–1
–2
–1
0
1
Figure
6.7
Plots of estimated residuals ˆi1 versus (a) ˆi1, (b) ˆi2, (c) ˆi3, (d) ˆi4 and
(e) ˆi.
residual plots are similar. These estimated residual plots lie within two parallel
horizontal lines that are centered at zero, and no linear or quadratic trends are
detected. This indicates that the proposed measurement and structural equa-
tions are adequate. The WinBUGS codes and the data are given in the following
website: http://www.wiley.com/go/lee_structural.

172
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
hat{epsilon}_i18
2
1
0
–1
–2
hat{epsilon}_i18
2
1
0
–1
–2
hat{epsilon}_i18
2
1
0
–1
–2
hat{epsilon}_i18
2
1
0
–1
–2
hat{epsilon}_i18
2
1
0
–1
–2
(c)
(b)
–2
–1
0
1
2
(e)
–2
–1
0
1
2
(d)
–2
–1
0
1
2
–2
–1
0
1
(a)
–1
0
1
Figure
6.8
Plots of estimated residuals ˆi versus (a) ˆi1, (b) ˆi2, (c) ˆi3, (d) ˆi4 and
(e) ˆi.
REFERENCES
Akaike, H (1970). Statistical predictor identification. Annals of the Institute of Statistical
Mathematics, 22, 203–217.
Akaike, H. (1987) Factor analysis and AIC. Psychometrika, 52, 317–332.

REFERENCES
173
Cowles, M. K. (1996) Accelerating Monte Carlo Markov chain convergence for
cumulative-link generalized linear models. Statistics and Computing, 6, 101–111.
Fayer, P. M. and Hand, D. J. (1997) Factor analysis, causal indicators and quality of
life. Quality of Life Research, 8, 139–150.
Fayer, P. M. and Machin, D. (1998) Factor analysis. In M. J. Staquet, R. D. Hayes and
P. M. Fayer (eds), Quality of Life Assessment in Clinical Trials. New York: Oxford
University Press.
Gelman, A. (1996) Inference and monitoring convergence. In W. R. Gilks, S. Richardson
and D. J. Spiegelhalter (eds), Markov Chain Monte Carlo in Practice pp. 131–144.
London: Chapman and Hall.
Gelman, A. and Rubin, D. B. (1992) Inference from iterative simulation using multiple
sequences. Statistical Science, 7, 457–472.
Gelman, A., Meng, S. L. and Stern, H. (1996) Posterior predictive assessment of model
fitness via realized discrepancies. Statistica Sinica, 6, 733–759.
Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. (1995) Bayesian Data Analysis.
London: Chapman and Hall.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distribution and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Geyer, C. J. (1992) Practical Markov chain Monte Carlo. Statistical Science, 7, 473–511.
Glonek, G. F. V. and McCullagh, P. (1995) Multivariate logistic models. Journal of the
Royal Statistical Society, Series B, 57, 533–546.
Hastings, W. K. (1970) Monte Carlo sampling methods using Markov chains and their
application. Biometrika, 57, 97–109.
Jöreskog, K. G. and Sörbom, D. (1996) LISREL 8: Structural Equation Modeling with the
SIMPLIS Command Language. Hove and London: Scientific Software International.
Lawley, D. N. and Maxwell, A. E. (1971) Factor Analysis as a Statistical Method (2nd
edn). New York: Elsevier.
Lee, S. Y. and Song, X. Y. (2002) Bayesian selection on the number of factors in a factor
analysis model. Behaviormetrika, 29, 23–39.
Lee, S. Y. and Zhu, H. T. (2000) Statistical analysis of nonlinear structural equation
models with continuous and polytomous data. British Journal of Mathematical and
Statistical Psychology, 53, 209–232.
Lee, S. Y., Poon, W. Y. and Bentler, P. M. (1990a) Full maximum likelihood analysis
of structural equation models with polytomous variables. Statistics and Probability
Letters, 9, 91–97.
Lee, S. Y., Poon, W. Y. and Bentler, P. M. (1990b). A three-stage estimation procedure
for structural equation models with polytomous variables. Psychometrika, 55, 45–51.
Lee, S. Y., Poon, W. Y. and Bentler, P. M. (1995). A two-stage estimation of struc-
tural equation models with continuous and polytomous variables. British Journal of
Mathematical and Statistical Psychology, 48, 339–358.
Lee, S. Y., Song, X. Y., Skevington, S. and Hao, Y. T. (2005) Application of structural
equation models to quality of life. Structural Equation Modeling – A Multidisciplinary
Journal, 12, 435–453.
Metropolis, N. et al. (1953) Equations of state calculations by fast computing machine.
Journal of Chemical Physics, 21, 1087–1091.
Muthen, L. and Muthen, B. (2000) Mplus User’s Guide. Los Angeles, CA: Muthen and
Muthen.

174
6
SEMS WITH ORDERED CATEGORICAL VARIABLES
Olschewski, M. and Schumacker, M. (1990) Statistical analysis of quality of life data in
cancer clinical trials. Statistics in Medicine, 9, 749–763.
Olsson, U. (1979a) Maximum likelihood estimation of the polychoric correlation coef-
ficient. Psychometrika, 44, 443–460.
Olsson, U. (1979b) On the robustness of factor analysis against crude classification of
the observations. Multivariate Behavioral Research, 14, 485–500.
Power, M., Bullingen, M. and Hazper, A. (1999) The World Health Organization
WHOQOL-100: Tests of the universality of quality of life in 15 different cultural
groups worldwide. Health Psychology, 18, 495–505.
Reboussin, B. A. and Liang, K. Y. (1998) An estimating equation approach for the
LISCOMP model. Psychometrika, 63, 165–182.
Roberts, C. P. (1995) Simulation of truncated normal variables. Statistics and
Computing, 5, 121–125.
Shi, J. Q. and Lee, S. Y. (2000) Latent variable models with mixed continuous and
polytomous data. Journal of the Royal Statistical Society, Series B, 62, 77–87.
Song, X. Y. and Lee, S. Y. (2001) Bayesian estimation and test for factor analysis
model with continuous and polytomous data in several populations. British Journal of
Mathematical and Statistical Psychology, 54, 237–263.
Song, X. Y. and Lee, S. Y. (2002) Analysis of structural equation model with ignorable
missing continuous and polytomous data. Psychometrika, 67, 261–288.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. (2003) WinBUGS User
Manual. Version 1.4. Cambridge, England: MRC Biostatistics Unit.
Staquet, M. J., Hayes, R. D. and Fayes, P. M. (1998) Quality of Life Assessment in
Clinical Trials. New York: Oxford University Press.
The WHOQOL Group (1998) Development of the World Health Organization
WHOQOL-BREF quality of life assessment. Psychological Medicine, 28, 551–558.
Wei, G. C. G. and Tanner, M. A. (1990) A Monte Carlo implementation of the EM
algorithm and the poor man’s data augmentation algorithm. Journal of the American
Statistical Association, 85, 699–704.
World Value Study Group (1994) World Values Survey, 1981–1984 and 1990–1993.
ICPSR version. Ann Arbor, MI: Institute for Social Research (producer). Ann Arbon,
MI: Inter-university Consortium for Political and Social Research (distributor).
Zhu, H. T. and Lee, S. Y. (1998) Statistical analysis of nonlinear factor analysis models.
British Journal of Mathematical and Statistical Psychology, 52, 225–242.

7
Structural Equation
Models with
Dichotomous Variables
7.1
INTRODUCTION
A Bayesian approach for analyzing SEMs with ordered categorical variables that
are defined by more than two categories discussed in Chapter 6. Another kind
of discrete variable that is frequently encountered in behavioral, educational,
medical and social research is the dichotomous variable which only involves
two categories. In this chapter, we will focus on dichotomous variables that are
ordered binary and defined with one threshold. Dichotomous variables arise
when respondents are asked to select answers from ‘Yes or No’ about the
presence of a symptoms, ‘Feeling better or Feeling worse’ about the effect of a
drug, ‘Agree or Disagree’ about a policy, ‘Satisfactory or Unsatisfactory’ about
a public service, etc.. The usual numerical values assigned to these variables are
the ad hoc numbers with an ordering such as ‘0’ and ‘1’, or ‘1’ and ‘2’. For
example, if ‘Unsatisfactory’ and ‘Satisfactory’ are respectively coded by ‘a’ and
‘b’, then we have a natural ordering that a < b. In analyzing dichotomous data,
the basic assumption in SEM that the data come from a continuous normal
distribution is clearly violated, and rigorous analysis that takes into account the
dichotomous nature is necessary. As we will see, the analysis of SEMs with
dichotomous variables is similar to, but not exactly the same as, the analysis
with ordered categorical variables.
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

176
7
STRUCTURAL EQUATION MODELS
In many substantive research, particularly in education, it is important to
explore and determine a small number of intrinsic latent factors under a number
of test items. Item factor analysis is an important model that has been proposed
for explaining the underlying factor structures. An important direction of anal-
ysis is the ML full information item factor (FIIF) approach of Bock and Aitkin
(1981). Their strategy in solving the computational difficulties induced by the
dichotomous variables was to treat the latent factors as missing data, and then
to apply the EM algorithm (Dempster, Laird and Rubin, 1977), with the E-step
completed by numerical integration via fixed-point Gauss–Hermite quadra-
ture. More recently, Meng and Schilling (1996) improved their EM algorithm
by using a Monte Carlo EM (MCEM) algorithm (Wei and Tanner, 1990)
for achieving the ML estimates. A Bayesian method for analyzing the more
general two-level factor analysis model with dichotomous data has been devel-
oped by Ansari and Jedidi (2000), by using some MCMC methods such as the
Gibbs sampler (Geman and Geman, 1984) and the MH algorithm (Metropolis
et al., 1953; Hastings, 1970). More recently, Lee and Song (2003) devel-
oped a Bayesian approach for a general SEM with dichotomous variables that
is compose of a measurement equation and a structural equation, based on the
Gibbs sampler. In this type of analysis, the emphasis is on the relationships
between the manifest and latent variables, or the related covariance structure,
rather than the mean vector.
Another direction of analysis is motivated from the fact that correlated
dichotomous data frequently arise in many medical and biological studies,
ranging from measurements of random cross section subjects to repeated
measurements in longitudinal studies. The multivariate probit (MP) model is
a popular method in biostatistics for analyzing this kind of data. This model
is described in terms of a correlated multivariate normal distribution of the
underlying latent variables that are manifested as discrete variables through a
threshold specification, and hence allows the flexible modeling of the corre-
lation structure and easy interpretation of the parameters. The emphasis of
this approach is focused on the mean structure, and the main difficulty in the
analysis is in evaluating the multivariate normal orthant probabilities induced
by the dichotomous variables. One approach uses models with less restrictive
covariance structures to reduce the computational burden of evaluating the
probabilities. For example, Bock and Gibbons (1996), and Gibbons and Wilcox-
Gök (1998) considered the exploratory factor analysis model for the covariance
structure, fixed covariates for accounting the mean structure, and applied the
fixed point Gauss–Hermite quadrature to approximate the orthant probabilities.
Their model can be regarded as a generalization of the FIIF model that allows
fixed covariates. Chib and Greenberg (1998) developed a Bayesian approach
and an ML approach for an MP model with a generic residual covariance struc-
ture, and applied the method to various real data sets, including the canonical 4
year data set from the Six Cities Study of health effects. Both their Bayesian and
ML approaches require the simulation of observations from a multivariate trun-
cated normal distribution with an arbitrary covariance matrix. Even with the

7.2
BAYESIAN ANALYSIS
177
efficient methods of statistical computing, the underlying computational effort
is rather heavy. We will show in Section 7.3 that this computational burden can
be reduced by adopting an SEM approach.
This chapter is organized as follows. In Section 7.2, we discuss the Bayesian
approach proposed in Lee and Song (2003) for analyzing SEMs with dichoto-
mous variables. Bayesian estimation and model comparison will be discussed
and a real example will be given. An SEM with fixed covariates for analyzing
the MP model will be discussed in Section 7.3 and an illustrative example is
provided (WinBUGS codes for analyzing an example are given in a web site).
A discussion is given in Section 7.4.
7.2
BAYESIAN ANALYSIS
Again, we consider a commonly used SEM that is compose of a measurement
equation and a structural equation. The measurement equation is defined by
the following confirmatory factor analysis model:
yi = +i +i
i = 1   n
(7.1)
where yi is a p ×1 random vector of manifest variables,  is a p ×1 mean vector,
i is a q ×1 random vector of latent variables, i is a p ×1 random vector of
residuals, and  is a p ×q unknown factor loading matrix. It is assumed that for
i = 1   n, i is independently distributed as N 0, i is independently
distributed as N0 , where   is a diagonal matrix, and i and i are
uncorrelated. Let the latent vector i be partitioned into T
i 	T
i T , where i
and 	i are q1 ×1, and q2 ×1 vectors of latent variables respectively. Moreover,
suppose that these latent vectors satisfy the following structural equation:
i = 
i +	i +i
(7.2)
where 
 and  are q1 ×q1 and q1 ×q2 matrices of unknown parameters, and i
is distributed as N 0 	, where  	 is a diagonal matrix, and 	i and i are
independent. It is assumed that 
0 = I−
 is nonsingular, and its determinant
is a nonzero constant independent with elements of 
. For convenience, let

 = 
, then the structural Equation (7.2) can be written as:
i = 
i +i
(7.3)

178
7
STRUCTURAL EQUATION MODELS
Now suppose that the exact measurement of yi = yi1   yipT is not avail-
able and its information is given by an observed dichotomous vector zi =
zi1   zipT such that, for k = 1   p,
zik =

1
if yik > 0
0
otherwise
(7.4)
The available observed data set is zii = 1   n. As the density function
of zi involves a multidimensional integral, analysis of SEMs with dichotomous
variables is not straightforward.
Consider the relationship between the factor analysis model defined by Equa-
tion (7.1) with the dichotomous variables in z. Let T
k k and k be the
kth row of , the kth element of , and the kth diagonal element of  ,
respectively. It follows from Equation (7.4) that
Przik = 1ikkk = Pryik > 0ikkk
= ∗T
k /1/2
k i +k/1/2
k 
(7.5)
where ∗is the distribution function of N01. Note that kk and
k are not estimable, because Ck/C1/2
k  = k/1/2
k
and Ck/C1/2
k  =
k/1/2
k
for any positive constant C. There are many ways to solve this identifi-
cation problem. Meng and Schilling (1996) suggested fixing k implicitly and
only estimating k. Following this basic idea, we fix k = 10. Note that the
value 1.0 is chosen for convenience and any other value would give an equiv-
alent solution up to a change of scale. Again the measurement and structural
equations are not identified. We follow the common method in SEMs to solve
this problem by fixing the approximate elements of  and 
 at preassigned
values.
Now, we consider the interpretation of the factor analysis model in the
measurement equation by looking at the relation between  and the observed
dichotomous vector z. Let y∗
ik = T
k i +ik, that is y∗
ik +k = yik. Because yik > 0
if and only if y∗
ik ≥−k, we have zik = 1 if and only if y∗
ik > −k. Consequently,
−k is the threshold corresponding to the factor analysis model associated with
y∗
ik. A special case of the current model with i distributed as N 0I is the
FIIF model (see Bock and Aitkin, 1981) that has wide applications in education.
Under this special case, −k can be interpreted as the kth item difficulty, and
the elements in T
k are the item slopes of the latent factors. Using the above
natural formulation of the model and identification conditions, we can estimate
the thresholds via −k. Moreover, as k are not the parameters of main interest,
fixing them at 1.0 does not have a great influence on the interpretation of the
structural equation model. However, we emphasize that this is not the unique
way to formulate the model and/or to impose its identification conditions.

7.2
BAYESIAN ANALYSIS
179
Note that as there are at least two thresholds associated with an ordered
categorical variable, the relation between the thresholds and −k is not as
clear. Also, the identification conditions are slightly different. Thus, methods
for analyzing these two types of discrete variables are not exactly the same.
Let Z = z1   zn be the observed data set of the dichotomous variables,
and our objective is to develop a Bayesian procedure to estimate the unknown
parameter vector , which contains parameters in , 
,  and  	, based
on Z. As usual, this is done by analyzing the following log posterior density of
 given Z,
log pZ ∝log pZ+log p
(7.6)
where pZ is the likelihood function, and p is the prior density of . Due
to the complexity of the model and the nature of the data, pZ involves
complicated multidimensional integrals and the posterior distribution is very
complicated. It is both difficult and tedious to compute the posterior mean
directly from pZ. Hence, the technique of data augmentation (Tanner and
Wong, 1987) is again employed in the posterior analysis. Let  = 1   n
be the matrix of latent variables of the model, and Y = y1   yn be the
matrix of latent continuous measurements underlying the matrix of observed
dichotomous data Z. In the analysis, the observed data Z is augmented with 
and Y, which can be considered as hypothetical missing data, to form a complete
data set ZY. Again, a large sample of observations will be sampled from
pYZ by the Gibbs sampler (Geman and Geman, 1984). The statistics of
interest, for example the Bayesian estimates, will be obtained from the generated
sample via standard data analysis methods.
To implement the Gibbs sampler, we start with initial values 00Y0,
simulate 11Y1    and continue as follows. At the mth iteration
with current values m, m and Ym:
Step a  Generate m+1 from pmYmZ
Step b  Generate m+1 from pm+1YmZ
Step c  Generate Ym+1 from pYm+1m+1Z
(7.7)
Under mild conditions and after a sufficiently large number of iterations, the
joint distribution of mmYm converges at an exponential rate to the
desired posterior distribution YZ. Convergence of the Gibbs sampler
can be revealed by plots of the simulated sequences of the individual parameters,
and/or by the EPSR values (Gelman, 1996) corresponding to the parameters
that are calculated sequentially as the runs proceed.
In deriving the conditional distribution pYZ in Step (a), we first
note that given Y, the underlying model becomes one with continuous data

180
7
STRUCTURAL EQUATION MODELS
and is relatively easy to handle. Moreover, as ii = 1   n, are mutually
independent,
pYZ = pY =
n
i=1
piyi
(7.8)
It follows from derivations similar to these in Section 6.3.1 that one represen-
tation of the conditional distribution iyi is given by:
iyi
D= N ∗T  −1
 yi −∗
i = 1   n
(7.9)
where ∗= −1

 +T  −1
 −1, 
 is the covariance matrix of i (see Equa-
tion (6.5)), and   is fixed as the identity matrix for identifying the model.
The conditional distribution of  given , Y and Z can be obtained via Equa-
tions (7.8) and (7.9).
Consider the conditional distribution of  given Y and Z in Step (b).
It involves components that correspond to , 
,  and  	. Hence, it
is divided into the following substeps which generate m+1 from pm
m

  m 
m
	  m Ym Z m+1 from pm+1 m

  m 
m
	 
m YmZ, m+1

from p
m+1 m+1 m 
m
	  m, YmZ,
m+1 from pm+1 m+1 m+1

 
m
	  m YmZ, and 
m+1
	
from
p 	m+1 m+1 m+1

 m+1 m Ym Z. We first note that the SEM
defined in Equations (7.1) and (7.2) is equal to that given in Equations (6.1) and
(6.2). Moreover, as Y and  are given, we essentially obtain the same information
about the underlying continuous measurements and the latent variables as in the
conditionaldistributionof involvedinStep(b)oftheGibbssamplerdescribedin
Section 6.3. Hence, the model defined in Equations (7.1) and (7.2) with given Y
and  reduces to a similar linear model as before. Consequently, the conditional
distributions involved in Step (b) can be obtained from Equations (6.7), (6.8) and
(6.9), under the following conjugate prior distributions that are similar to those
given in Section 6.3.1:

D= N 00
k
D= N 0kH0yk
−1
	k
D= Gamma 0	k0	k

k	k
D= N 0
k	kH0
k
−1 D= WqR00
(7.10)
where 	k is the kth diagonal element of  	, T
k and T

k are the kth rows of 
and 
, respectively; and 0k, 0
k 0	k, 0	k, 0 and 0, and positive definite
matrices H0yk, H0
k and R0 are hyperparameters whose values are assumed to
be given by the prior information.

7.2
BAYESIAN ANALYSIS
181
We now consider pYZ. As the yi are mutually independent, it follows
that
pYZ =
n
i=1
pyiizi
Moreover, it follows from the definition of the model and Equation (7.4) that
pyikizi
D=

N k +T
k i1I−0yik
if zik = 0
N k +T
k i1I0yik
if zik = 1
(7.11)
where k is the kth element of , and IAy is an indicator function that takes
the value 1 if y is in A, and 0 otherwise.
As the conditional distributions that are involved in Steps (a), (b) and (c) are
familiar, drawing observations from them is straightforward and fast. Conse-
quently, the programming and computational burden involved in the Gibbs
sampler is not heavy.
Statistical inference of the model can be obtained on the basis of the simulated
sample of observations from pYZ, as before. For example, the Bayesian
estimates of elements in  as well as their numerical standard error estimates
can be obtained from the sample mean and the sample covariance matrix,
respectively. Note that for dichotomous data analysis, a vector of dichotomous
observation zi rather than yi is observed; hence a lot of information of yi is lost.
As a result, it requires a rather large sample size to achieve accurate estimates.
The PP p-value for assessing the goodness-of-fit of a posited model, and the
Bayes factor for model comparison can be obtained via similar developments
as given in Chapter 6. Theoretically, for each yi, we can estimate its associative
vector of latent variables i. Since the only data information available in the
estimation is yi, ˆi is not expected to be an accurate estimate of the true i.
This is particularly true in dealing with dichotomous data. As the estimate of
the residual ˆi depends on an estimate of yi, its accuracy is further affected.
7.2.1
Illustrative Example 1
It has been pointed out that patient adherence to prescribed medication is
crucial to the success of medical treatment, and that nonadherence can lead to
a misjudgment of the effectiveness of the medication, see Czajkowski, Chesney,
and Smith (1998), and Rand and Weeks (1998). In the promotion of adherence,
it is desirable to establish a statistical model to study the relationships between
nonadherence and its core factors such as a patient’s knowledge, attitudes and
beliefs concerning medication, the health condition of the patients, a physician’s
interaction and communication, and so on.

182
7
STRUCTURAL EQUATION MODELS
To enrich existing knowledge about patient nonadherence, the Department
of Medicine and Therapeutics, Community and Family Medicine, and Pharmacy
at the Chinese University of Hong Kong conducted a survey of ethnic Chinese
patients diagnosed as suffering from hypertension (Chan, 2002). The main
objectives were to measure and examine relationships among latent variables
such as physician advice and concern, patient knowledge and belief, social
cognition and social influence, and the subsequent study reported nonadherence
with reference to a structural equation model. Inspired by the fact that this study
involved many dichotomous variables, we apply the methodology developed
here to assess the effects of some latent variables on patients’ nonadherence to
medication.
For the sake of illustration, suppose that we are interested in studying how
‘nonadherence’ of patients is affected by their ‘knowledge of medication’ and
‘health condition’ by analyzing the related portion of the whole data set.
Six dichotomous manifest variables are selected as indicators for the first two
latent variables mentioned above. Three ordered categorical manifest variables
measured by a five-point scale are used as indicators for the last latent vari-
able about ‘health condition’. As these ordered categorical variables are heavily
skewed (see frequencies of the last three variables in Appendix 7.1), and in order
to provide an illustration of the proposed method for dichotomous variables,
we transform them to dichotomous by grouping the first four categories on the
left together. The information lost due to this grouping should not be substan-
tial. Translations of the corresponding questions from Chinese into English are
listed in Appendix 7.1, together with their frequencies. For brevity, we deleted
a small number of observations with missing entries, and the remaining sample
size is 837.
The resulting data set of dichotomous observations is analyzed using a model
as defined in Equations (7.1) and (7.2) (see also Lee and Song, 2003). Although
other structures for the loading matrix can be considered, we choose the struc-
ture that gives nonoverlapping latent factors for clear interpretation. Hence the
following specification of the loading matrix  is used:
T =
⎡
⎣
1 21 31 0
0
0
0
0
0
0
0
0
1 52 62 0
0
0
0
0
0
0
0
0
1 83 93
⎤
⎦
where ij’s are the unknown factor loading parameters, but 1’s and 0’s are fixed
in the estimation for achieving an identified model. From the meanings of the
questions, see Appendix 7.1, it is clear that this structure gives three nonoverlap-
ping factors (latent variables), which can be interpreted as the ‘nonadherence,
’, ‘knowledge of medication, 1’ and ‘health condition, 2’ of the patients. As
our main interest is to study the linear effects of ‘knowledge of medication’ and

7.2
BAYESIAN ANALYSIS
183
‘health condition’ on ‘nonadherence’, the structural equation is chosen to be a
linear regression model which regresses  on 1 and 2 as follows:
 = 11 +22 +	
(7.12)
where 1 and 2 are unknown parameters. Other unknown parameters include
the variances and covariance of 1 and 2: 11, 22 and 12, and the variance
of 	, 	. There are a total of 12 unknown parameters in this model.
The proposed Gibbs sampler algorithm is used to obtain Bayesian estimates
of the parameters. Estimates of the latent variables can also be obtained as
by-products. As an illustration for analyzing data sets with no good prior
information, we use the following data-dependent prior inputs. An auxiliary
Bayesian estimation with non-informative prior for obtaining prior inputs of the
hyperparameters values for 0k, 0
k and R0 in the conjugate prior distribu-
tions is first conducted. Consequently, the values of hyperparameters are given
by 0	k = 8, 0	k = 10, 0 = 8, H0yk and H0
k are identity matrices, 0k = k,
0
k = 
k and R0 = I, where k and 
k are estimates obtained from the
auxiliary estimation. With some arbitrary starting values, we observe that the
Gibbs sampler in the actual estimation converges in about 5000 iterations. To
3
2.5
2
1.5
1
0.5
0
1000
2000
3000
4000
5000
6000
7000
8000
9000 10000
Figure
7.1
EPSR values against the numbers of iterations. This figure and Figure 7.2
are taken from Lee and Song (2003).

184
7
STRUCTURAL EQUATION MODELS
reveal the convergence, plots of the EPSR values and some parameters’ values
against the iteration numbers are presented in Figures 7.1 and 7.2 respectively.
Comparing the numbers of iterations required in analyzing dichotomous and
ordered categorical data, we find that dichotomous data require more iterations.
This is probably due to the fact that dichotomous data carry less information
than ordered categorical data. To obtain the Bayesian estimates, 5000 obser-
vations are collected after discarding the first 5000 burn-in iterations. Results
are presented in Table 7.1. Based on these estimates, ˆ and ˆ, the estimated
covariance matrices of the manifest random vector y and the latent random
vector , can be computed. Sometimes, it is also desirable to transform these
estimates to a completely standardized (CS) solution such that both manifest
and latent variables are standardized, and ˆ and ˆ are correlation matrices
1.0
0.5
0.0
–0.5
6
5
4
3
2
1
0
2.0
1.5
1.0
–1.0
–1.0
–2.0
1.0
1.0
0.5
0.5
0.0
0.0
0.0
6
5
4
3
2
1
0
(a)
(c)
(d)
(e)
(f)
(b)
0
2000
4000
6000
8000
10000
0
2000
4000
6000
8000
10000
0
2000
4000
6000
8000
10000
0
2000
4000
6000
8000
10000
0
2000
4000
6000
8000
10000
0
2000
4000
6000
8000
10000
Figure
7.2
(a), (b), (c), (d), (e) and (f) are plots of parallel sequences corresponding
to different starting values of 31528312 and 12 against iterations.

7.2
BAYESIAN ANALYSIS
185
Table
7.1
Bayesian estimates and its completely standard-
ized solution.
Parameter
Bayesian estimates
CS solution
11
10∗
097
21
366
097
31
012
131
42
10∗
040
52
432
088
62
367
085
73
10∗
057
83
432
095
93
414
095
11
019
10
21
−018
−059
22
049
10
1
−121
−038
2
090
045
	
089
046
Note: 11 42 and 73 are fixed in the Bayesian estimation. This table
is taken from Lee and Song (2003).
(see Jöreskog and Sörbom, 1996). For completeness, the CS solution obtained
from our Bayesian estimates is also presented in Table 7.1. The PP p-value for
goodness-of-fit testing of the posited model is 0.414. On the basis of the result
given in Gelman, Meng and Stern (1996) which suggested that a PP p-value
close to 0.5 indicates good fit of the posited model, the PP p-value obtained
indicates that the proposed model fits the data.
The most important interpretations of the results are as follows. (i) From
the estimates of 1 and 2 in the structural equation, we see that 1 and 2
have significant effects on . Moreover, it is clear from the estimated structural
equation:  = −1211 +0902, that better ‘knowledge of medication, 1’ has
a negative effect on ‘nonadherence’, whilst weaker or worse ‘health condition,
2’ has a positive effect on ‘nonadherence’. Hence, it is desirable to better
educate patients about their illnesses and encourage them to pay more attention
to their health. (ii) From the completely standardized solution, we see that
the correlation estimate between 1 and 2 is −059. Hence, we arrive at the
expected conclusion that better ‘knowledge of medication, 1’ and weaker
‘health condition, 2’ are negatively correlated.
This data set has been reanalyzed by using WinBUGS. We find that the
WinBUGS program converged after 5000 iterations. The Bayesian estimates
that are obtained by using 5000 observations after convergence are reported
in Table 7.2. Under the expected MCMC sampling errors, we observe that
the Bayesian estimates obtained by WinBUGS are reasonable close to those
that are reported in Table 7.1. The DIC value corresponding to this model is

186
7
STRUCTURAL EQUATION MODELS
Table
7.2
Bayesian estimates and their standard error estimates obtained via
WinBUGS.
Parameter
Estimate
SE
Parameter
Estimate
SE
11
10∗
–
11
013
002
21
3.36
0.38
21
−014
002
31
0.12
0.04
22
042
006
42
10∗
–
52
4.69
0.37
1
−151
026
62
4.14
0.36
2
101
015
73
10∗
–
83
4.90
0.38
	
080
014
93
4.26
0.36
Note: 1142 and 73 are fixed in the Bayesian estimation.
7403.5. The codes and the data set are presented in the following website:
http://www.wiley.com/go/lee_structural.
7.3
ANALYSIS OF A MULTIVARIATE PROBIT
CONFIRMATORY FACTOR ANALYSIS MODEL
As the main objective of a multivariate probit (MP) model is on the mean
structure, we consider a model such that each subject has vectors of fixed
covariates that may come from any mixture of discrete and continuous variables.
Specifically, we assume that each subject produces p distinct quantal responses
or is classified with respect to p dichotomous categories. Let zi = zi1   zipT
denote the collection of observed dichotomous responses in p variables on the
ith subject, i = 1   n, xik be an rk ×1 vector of covariates, R = r1 +···+rp,
and let
Xi =
⎡
⎢⎢⎢⎣
xT
i1
0
···
0
0
xT
i2
···
0




0
0
···
xT
ip
⎤
⎥⎥⎥⎦
be a p × R matrix. Consider the following MP model that was formulated by
Chib and Greenberg (1998). Let yi = yi1   yipT denote a p-variate normal
vector of ‘response strengths’ such that
yi = XiB+i
i = 1   n
(7.13)

7.3
ANALYSIS OF A MULTIVARIATE PROBIT CFA
187
where BT = bT
1    bT
p , bk is an rk × 1 unknown parameter vector, i is a
p ×1 vector of residuals that is distributed as N0, and
zik =

1
if yik = xT
ikbk +ik = bT
k xik +ik > 0
k = 1   p
0
if otherwise
(7.14)
In this model, the exact measurement of ‘response strengths’ yi is not
observed, and its information is given by an observed dichotomous vector
zi = zi1   zipT with zik defined by Equation (7.14). Here, B is an R × 1
vector of regression coefficients of yi on Xi. If we take the first component of
xik to be 1.0 for all i and k, then the first component of bk can be interpreted
as the mean (or intercept) of yik. This MP model has quite wide applications.
Chib and Greenberg (1998) utilized some MCMC methods in developing a
Bayesian approach and an ML approach for analyzing this model. As the covari-
ance matrix, , of i is a general positive definite matrix, observations from a
multivariate truncated normal distribution are simulated in their method. The
computational burden for this task is fairly heavy. We will show below that this
burden can be reduced by incorporating a confirmatory factor analysis in the
MP model.
Consider the following extension of the MP model with a confirmatory factor
analysis model for the underlying ‘response strengths’ yi (see Song and Lee,
2005):
yi = XiB+i +i
i = 1   n
(7.15)
where  is a p × q loading matrix of parameters which may be unknown or
known, i is a q ×1 vector of latent factors, and i is a p ×1 vector of residuals.
For i = 1   n, we assume that i is independently distributed as N 0,
i is independently distributed as N 0 , where  is an arbitrary covariance
or correlation matrix,   is a diagonal covariance matrix with diagonal elements
k, and i and i are uncorrelated. For brevity, we call this the MPCFA model.
To identify the model, we fix   at a known diagonal matrix and appropriate
elements in  at preassigned values.
The MP model (Chib and Greenberg, 1998), as defined by Equations (7.13)
and (7.14) with an arbitrary covariance matrix , can be analyzed with the
MPCFA model by setting  = Ip, where Ip is a p×p identity matrix, i = i +i
and  = + . In practice, we can set   = cIp, where c is a small value, and
use  to capture covariances in .
If all xi1   xip equal to an r∗×1 vector xi, then XiB in Equation (7.15) can
be written as B∗xi, where B∗is a p ×r∗matrix with rows equal to bT
1    bT
p .
The form of Equation (7.15) then reduces to the model given by Gibbons and
Wilcox-Gök (1998). Hence, the covariates that are considered in the proposed

188
7
STRUCTURAL EQUATION MODELS
MPCFA model are more general. Another generalization is that the covariance
matrix of the latent factors in the MPCFA model is equal to a general covariance
or correlation matrix  rather than an identity matrix as in the Gibbons and
Wilcox-Gök (1998) model. This extension requires very little extra computing
effort. On the other hand, because  is an arbitrary positive definite matrix, it is
useful for capturing  as +  in analyzing models with arbitrary covariance
structures. Another special case of the MPCFA model is given by
yi = i +i
i = 1   n
(7.16)
without any covariates. This model can be viewed as a confirmatory factor
analysis model for dichotomous variables, or the FIIF model with correlated
factors.
Let Z = z1   zn be the observed data matrix of the dichotomous
outcomes, Y = y1   yn be the matrix of latent continuous measure-
ments underlying Z,  = 1   n be the matrix of latent variables, X =
X1   Xn be the observed values of the fixed covariates, and  be the param-
eter vector that contains unknown parameters in B,  and . The Bayesian
estimate of , and the latent vectors in  can be obtained via the following
Gibbs sampler: At the mth iteration with current values mm and Ym,
Step (a)  Generate m+1 from pmYmZ
Step (b)  Generate m+1 from pm+1YmZ
Step (c)  Generate Ym+1 from pYm+1m+1Z
The basic structure of this Gibbs sampler is similar to that given in (7.7). For
completeness, the conditional distributions are given as follows.
To derive the conditional distribution in Step (a), we let ∗= −1 +
T  −1
 −1. We have
pYZ = pY =
n
i=1
piyi
where iyi
D= N∗T  −1
 yi −XiB∗. To obtain the conditional distri-
bution in Step (c), it follows from the definition of the model in Equation (7.15)
that
pYZ =
n
i=1
pyiizi
=
n
i=1
p
k=1
pyikizik

7.3
ANALYSIS OF A MULTIVARIATE PROBIT CFA
189
where
yikizik
D=
⎧
⎨
⎩
N xT
ikbk +T
k ikI−0yik
if
zik = 0
N xT
ikbk +T
k ikI0yik
if
zik = 1
The following conjugate prior distributions that are similar to those given in
Equation (7.10) are used in obtaining the conditional distributions involved in
Step (b). For k = 1   p
bk
D= N b0kH0bk k
D= N 0kH0yk and −1 D= WqR00
(7.17)
where T
k is the kth row of , and b0kH0bk0kH0ykR0 and 0 are given
hyperparameters. The conditional distribution corresponding to bk is given by
pbkYZ
D= N akAk
(7.18)
where
Ak = H−1
0bk + X∗
kX∗T
k −1ak = AkH−1
0bkb0k + X∗
kY∗T
k 
with
X∗
k =
x1k   xnk and Y∗
k = yik −k
T ii = 1   n. The conditional distribu-
tions corresponding to k and  can be obtained from the results given in
Section 6.3.1 (see also Section 7.2).
7.3.1
Illustrative Example 2
The same data set used in the illustrative example in Section 7.2.1 is reanalyzed.
To demonstrate the MPCFA model, we include two fixed covariates, one about
patients’ education, xik1 (coded by 0, 1, 2, 3), and the other about the presence
of side effects of drugs, xik2 (coded by 0 and 1) in the analysis. The loading
matrix  for the three latent factors ‘nonadherence,  = 
1’, ‘knowledge of
medication 1 = 
2’ and ‘health condition, 2 = 
3’ is taken to be the same
nonoverlapping structure as given in Section 7.2.1. To identify the model,  
is fixed to be an identity matrix.
To illustrate the use of the Bayes factor for model comparison, we consider
the following models with or without fixed covariates:
M0  yik = bk1xik1 +bk2xik2 +T
k i +ik
M1  yik = bk1xik1 +T
k i +ik
M2  yik = bk2xik2 +T
k i +ik
M3  yik = ki +ik
where bT
k = bk1bk2 In the same way as before, the hyperparameters b0k and
0k are obtained on the basis of an auxiliary estimation with noninformative

190
7
STRUCTURAL EQUATION MODELS
Table
7.3
Bayesian estimates and their standard error estimates obtained
via WinBUGS for the MPCFA model.
Parameters
Estimate
SE
Parameters
Estimate
SE
b11
−0658
0061
21
2485
0628
b12
−1658
0314
31
0298
0081
b13
−0024
0034
52
4157
0637
b14
−0124
0034
62
4693
0676
b15
0838
0089
83
4145
0634
b16
0718
0090
93
4264
0678
b17
−0090
0039
b18
−1430
0204
11
0778
0216
b19
−1469
0224
21
−0101
0024
b21
0074
0147
31
0301
0057
b22
0170
0269
22
0094
0021
b23
0103
0112
32
−0087
0017
b24
0328
0115
33
0450
0088
b25
0285
0190
b26
−0054
0194
b27
0261
0133
b28
0633
0317
b29
0646
0320
priors, H0bk and H0yk are taken to be the identity matrices, R0 = 05I and 0 = 8.
In the path sampling procedure, we take 20 grids in [0, 1], 1000 burn-in iter-
ations, and further collect 1000 observations at each grid for computing the
Bayes factor. The estimated logarithm Bayes factors are

logB01 = 487

log B02 =
10556 and

log B03 = 10647. Based on the selection criterion of the Bayes
factor, M0 is selected. Hence, the model with two fixed covariates about educa-
tion and side effects is the best model among the competitive models.
The Bayesian estimates of the parameters in M0 obtained through WinBUGS
are presented in Table 7.3, together with the standard error estimates. Inter-
pretation of these estimates is obvious. The DIC value corresponding to
M0 is 7200.51. The WinBUGS codes and data are given in the website:
http://www.wiley.com/go/lee_structural.
7.4
DISCUSSION
Based on the definition of a dichotomous observation, see Equation (7.4), we
understand that the exact measurement of the underlying continuous variable y
in the whole range − is not available. The only information available is
whether y is less than zero (in that case z = 0), or y is larger than zero (in that
case z = 1). As the information carried by a dichotomous observation is very
rough, we need to pay special attention in analyzing SEMs or MPCFA models

APPENDIX 7.1
191
with dichotomous variables. We find from our experience that: (i) It requires
comparatively large sample sizes to achieve accurate estimates. Recall that in
Section 4.3.3, we reported results of simulation studies to show that for contin-
uous data, the sufficient sample sizes to achieve accurate Bayesian estimates in
analyzing a model with a unknown parameters is 5a. Roughly speaking, to
achieve reasonably accurate results in analyzing dichotomous data, the available
sample sizes should be larger than 30a. (ii) As the information given by the
data is rough, the prior information plays a more important role in the Bayesian
analysis. (iii) It requires more iterations for the MCMC algorithm to converge.
It is important to monitor convergence with more care. (iv) The standard error
estimates usually overestimate the true standard derivations. Thus, these esti-
mates should be interpreted with great caution, and they should not be used to
obtain the z-score for hypothesis testing. We emphasize that hypothesis testing
should be approached by means of model comparison through Bayes factor or
DIC, particularly in analyzing models with dichotomous variables.
In this chapter, we focused on the dichotomous variables that are basically
ordered binary variables. These kind of variables are defined by an underlying
normal distribution with a threshold at zero, and their coded values have a
natural ordering. It is important to note that an ordered binary variable is
different from an unordered binary variable which has a binomial distribution.
Unordered binary variables come from items which give unordered responses.
For example, they arise when subjects are asked to select one of the two different
products AB , etc.. As pointed out by Lee, Song and Cai (2006), treating
unordered binary variables as ordered binary variables will lead to biased results.
In applying the methodologies presented in this chapter, it is important to make
sure that the dichotomous variables are ordered binary. Analysis of unordered
binary variables will be discussed in Chapter 13, under the framework of the
exponential family of distributions.
APPENDIX 7.1: QUESTIONS ASSOCIATED WITH THE
MANIFEST VARIABLES
Frequencies of (Yes ‘1’/No ‘0’) are in parentheses.
y1:
Did you have any surplus in the previous prescribed drugs? (175/662)
y2:
Did you stop/reduce/increase the dosage? (69/768)
y3:
Did you forget to take medications? (391/446)
y4:
Do you feel you have hypertension? (363/474)
y5:
Do you know the reasons for taking drugs? (650/187)
y6:
Do you know the reasons for taking drugs long term? (605/232)
y7:
In the past 2 weeks, did you have emotional problems such as feeling
upset, bad tempered, etc.? (387/450)
y8:
In the past 2 weeks, did your health cause any difficulties in daily activities?
(181/656)

192
7
STRUCTURAL EQUATION MODELS
y9:
In the past 2 weeks, did your health cause any difficulties in social activities?
(177/660)
This Dataset was supplied by Professor Juliana C.N. Chan and Dr Grace
Chan of the Chinese University of Hong Kong, and was taken from Lee and
Song (2003).
REFERENCES
Ansari, A. and Jedidi, K. (2000) Bayesian factor analysis for multilevel binary observa-
tions. Psychometrika, 65, 475–498.
Bock, R. D. and Aitkin, M. (1981) Marginal maximum likelihood estimation of item
parameters: Application of an EM algorithm. sl Psychometrika, 46, 443–461.
Bock, R. D. and Gibbons, R. D. (1996) High dimensional multivariate probit analysis.
Biometrics, 52, 1183–1194.
Chan, G. M. C. (2002) The Effects of Treatment Compliance on Clinical Outcomes, in
Patients with Chronic Diseases. Ph.D. Thesis, Department of Medicine and Therapeu-
tics, the Chinese University of Hong Kong.
Chib, S. and Greenberg, E. (1998) Analysis of multivariate probit models. Biometrika,
85, 347–361.
Czajkowski, S. M., Chesney, M. A. and Smith, A. W. (1998) Adherence and the placebo
effect. In Shumaker, S. A., Schran, E. B., Ockene, J. K. and McBee, W. L. (eds), The
Handbook of Health Behavior Change (2nd edn), pp. 513–534. New York: Springer.
Dempster, A. P., Laird, N. M. and Rubin, D. B. (1977) Maximum likelihood from
incomplete data via the EM algorithm (with discussion), Journal of Royal Statistical
Society, Series B, 39, 1–38.
Gelman, A. (1996) Inference and monitoring convergence. In W. R. Gilks, S. Richardson
and D. J. Spiegelhalter (eds), Markov Chain Monte Carlo in Practice, pp. 131–144.
London: Chapman and Hall.
Gelman, A., Meng, X. L. and Stern, H. (1996) Posterior predictive assessment of model
fitness via realized discrepancies. Statistica Sinica, 6, 733–759.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distribution and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Gibbons, R. D. and Wilcox-Gök, V. (1998) Health service utilization and insurance
coverage: a multivariate probit model. Journal of the American Statistical Association,
93, 63–72.
Hastings, W. K. (1970) Monte Carlo sampling methods using Markov chains and their
application. Biometrika, 57, 97–109.
Jöreskog, K. G. and Sörbom, D. (1996) LISREL 8: Structural Equation Modeling with the
SIMPLIS Command Language. Hove and London: Scientific Software International.
Lee, S. Y. and Song X. Y. (2003) Bayesian analysis of structural equation models with
dichotomous variables. Statistics in Medicine, 22, 3073–3088.
Lee, S. Y., Song, X. Y. and Cai, J. H. (2006) A Bayesian approach for nonlinear struc-
tural equation models with ordered and unordered binary variables using WinBUGS.
Submitted.

REFERENCES
193
Meng, X. L. and Schilling, S. (1996) Fitting full-information item factor models and
an empirical investigation of bridge sampling. Journal of the American Statistical
Association, 91, 1254–1267.
Metropolis, N. et al. (1953) Equations of state calculations by fast computing machine.
Journal of Chemical Physics, 21, 1087–1091.
Rand, C. S. and Weeks, K. (1998) Measuring adherence with medication regiments in
clinical care and research, In Shumaker, S. A., Schron, E. B., Ockene, J. K. and McBee,
W. L. (eds), The Handbook of Health Behavior Change (2nd edn), pp. 71–103. New
York: Springer-Vertag.
Song, X. Y. and Lee, S. Y. (2005) A multivariate probit latent variable model for analyzing
dichotomous responses. Statistica Sinica, 15, 645–664.
Tanner, M. A. and Wong, W. H. (1987) The calculation of posterior distributions by
data augmentation(with discussion). Journal of the American statistical Association,
82, 528–550.
Wei, G. C. G. and Tanner, M. A. (1990) A Monte Carlo implementation of the EM
algorithm and the poor man’s data augmentation algorithm. Journal of the American
Statistical Association, 85, 699–704.


8
Nonlinear Structural
Equation Models
8.1
INTRODUCTION
Structural equation models considered in previous chapters, for example the
factor analysis model and the LISREL type models, are models in which the
latent variables are related by linear functions. Recently, it has been recognized
that nonlinear relations among the latent variables are important for establishing
more meaningful and correct models for some complex situations. For example,
see Busemeyer and Jones (1983), Jonsson (1998), Ping (1996), Kenny and Judd
(1984), Bagozzi, Baumgartner and Yi (1992), Schumacker and Marcoulides
(1998), and references therein on the importance of quadratic and interaction
effects of latent variables in various applied research.
Due to the complex distributions associated with the nonlinear latent vari-
ables, methods for analyzing nonlinear structural equation models are more
difficult. Nonlinear factor analysis models with polynomial relationships were
first explored by McDonald (1962), then followed by Etezadi-Amoli and
McDonald (1983) and Mooijaart and Bentler (1986). More recently, methods
that used the LISREL (Jöreskog and Sörbom, 1996) program have been
proposed to analyze some nonlinear structural equation models with interac-
tion terms of latent variables, see, for example, Kenny and Judd (1984), Ping
(1996), Jaccard and Wan (1995), Jöreskog and Yang (1996) and Marsh, Wen
and Hau (2004) among others. The basic approach of these contributions
is to include artificial nonlinear manifest variables in the analysis to account
for nonlinear relationships among variables. However, this approach has some
remaining issues that need to be addressed. For example, there is no sound
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

196
8
NONLINEAR STRUCTURAL EQUATION MODELS
theoretical basis for forming the products of the indicators, the conclusions that
are obtained using different indicators can be different, and the problems on
the robustness of the ML to violations of multivariate normality still exist (see
Lee, Song and Poon, 2004; Marsh, Wen and Han, 2004). The asymptotically
distribution-free (ADF) theory (Browne, 1984; Bentler, 1983, 1992) is an alter-
native for rigorous treatment for the non-normality induced by the nonlinear
latent variables. However, it is well known that (see, for example, Hu, Bentler
and Kano, 1992; Bentler and Dudgeon, 1996) ADF theory requires very large
sample sizes to attain its large sample properties. Other methodological devel-
opments include the moment-based approaches (Wall and Amemiya, 2000,
2003), the latent moderated structural equations (LMS) approach that utilizes
the mixture distributions (Klein and Moosbrugger, 2000), and the exact ML
approach (Lee and Zhu, 2002). Moreover, the Bayesian approach has also been
developed for nonlinear SEMs with general forms, see Zhu and Lee (1999),
Lee and Song (2003). Lee, Song and Poon (2004) conducted simulation
studies to compare the Bayesian approach with the product indicator approach.
Their simulation results indicate that the Bayesian approach is better. Recently,
Lee and Zhu (2000) and Lee and Song (2004) developed Bayesian methods
for analyzing nonlinear SEMs with mixed continuous and ordered categorical
variables.
Model comparison (or hypothesis testing) is a very important component
in analyzing SEMs, especially nonlinear SEMs. A fundamental problem is to
decide whether a nonlinear SEM is better than a linear one in fitting a particular
data set. It is also important to compare various non-nested nonlinear models
and to select the better model. As the covariance matrix and distribution of the
nonlinear latent variables are rather complex, the covariance structure and the
distribution of the manifest variables in the nonlinear model are complicated.
The traditional likelihood ratio test that requires the normality assumption of
the observed random vector cannot be used. Moreover, this traditional statistic
cannot be applied to compare non-nested models or hypotheses. Here, we again
use the Bayes factor to address the important issue of model comparison and
propose path sampling to compute this statistic.
The main objective of this chapter is to describe Bayesian estimation
and model comparison (and/or hypothesis testing) procedures for analyzing
nonlinear SEMs. The basic model is defined by a linear measurement equa-
tion and a nonlinear structural equation that involves differentiable functions
of the exogenous latent variables. Under this framework, nonlinear causal rela-
tionships among the latent variables in the model can be analyzed. Nonlinear
SEMs with continuous data will be discussed in Section 8.2. Bayesian esti-
mates are obtained through a hybrid algorithm which combines the Gibbs
sampler (Geman and Geman, 1984) and the MH algorithm (Metropolis et al.,
1953; Hastings, 1970). Joint Bayesian estimates of the random variables and
the unknown parameters are obtained together with their standard error esti-
mates. We also report simulation results in relation to the comparisons of a

8.2
BAYESIAN ANALYSIS OF A NONLINEAR SEM
197
Bayesian approach with a product indicator approach (Jaccard and Wan, 1995)
as well as the LMS (Klein and Moosbrugger, 2000) approach. Bayesian anal-
ysis of nonlinear SEMs with mixed type of continuous and ordered categorical
data is discussed in Section 8.3. Then, in Section 8.4, we consider a general
SEM which involves nonlinear terms among covariates and exogenous latent
variables. A detailed real example is presented. In addition, we discuss the appli-
cation of WinBUGS (Spiegelhalter, Thomas, Best and Lunn, 2003) in analyzing
an artificial example. A procedure based on the path sampling for computing
the Bayes factor for model comparison is described in Section 8.5.
8.2
BAYESIAN ANALYSIS OF A NONLINEAR SEM
8.2.1
The Model
Consider the following nonlinear structural equation model (NSEM) with a
p ×1 manifest random vector y = y1   ypT :
y = ++
(8.1)
where  is a vector of intercepts,  is a p × q matrix of factor loadings,  =
1   qT is a random vector of latent factors with q < p is a p×1 random
vector of error measurements with distribution N0 	, where   is diagonal
and  is independent of . To handle more complex situations, we partition
 as T T T and further model this latent vector by the following nonlinear
structural model:
 = 	+
H+
(8.2)
where  and  are q1 ×1 and q2 ×1 latent subvectors of  respectively; H =
h1   htT is a t × 1 nonzero vector-valued function with nonzero,
known, and linearly independent differential functions h1   ht, and t ≥q2;
	q1 × q1 and 
q1 × t are matrices of regression coefficients of  on  and
H, respectively. It is assumed that  and  are independently distributed as
N0	 and N0 
	 respectively, where  
 is a diagonal covariance matrix.
If some hj are nonlinear, the distribution of the manifest random vector y
is non-normal. Let 	0 = Iq1 −	, we assume that 	0 is a nonzero constant
independent of 	. The structural model in Equation (8.2) is linear in parameter
matrices 	 and 
, but may be nonlinear in the latent variables in . This allows
the assessment of the nonlinear causal effects of latent variables in  on latent
variables in . Let  = 	
 and G = T HT T , then Equation (8.2)
can be written as
 = G+
(8.3)

198
8
NONLINEAR STRUCTURAL EQUATION MODELS
The components of H are any differential functions, hence they are general
enough to deal with the common polynomial relationships among the latent
variables. However, the choice of H is not completely arbitrary. For example,
the following obvious cases are not allowed: H1 = 122
12
1 and
H2 = 12120. Clearly, H1 and H2 should be modified as
122
1 and 1212, respectively. An example of an identified structural
equation is:
1
2

=
0 
0 0
1
2

+
11 12 0
0
0
21 22 23 24 25

⎛
⎜⎜⎜⎜⎝
1
2
2
1
12
2
2
⎞
⎟⎟⎟⎟⎠
+

1

2


The proposed nonlinear model is over parameterized without appropriate
identification conditions. For instance, an equivalent form of model Equa-
tion (8.1) is y = +∗∗+, where ∗= R and ∗= R −1, for any nonsin-
gular matrix R. One common method to solve the identification problem in
structural equation modeling is to fix appropriate elements in  at some known
values so that the only possible choice of R is the identity matrix. Similarly,
appropriate elements in  may also be fixed at known values if necessary.
In the context of nonlinear SEMs, more care is needed to interpret the mean
vector of y. Let T
k be the kth row of . For k = 1   p, it follows from
Equation (8.1) that Eyk = k +T
k E. Although E = 0, it follows from
Equation (8.2) that E ̸= 0 if H is a nonlinear function of . Hence E ̸=
0 and Eyk ̸= k. Let T
k = T
kT
k be a partition of T
k that corresponds to
the partition of  = T T T . Because E = 0 and  = I −	−1
H	,
it follows from Equation (8.2) that
Eyk = k +T
kE+T
kE = k +T
kI−B−1
	EH
(8.4)
As H is usually not very complicated in most practical applications, E H
is not very complex and the computation of E yk is not difficult. See the
illustrative example presented in subsequent sections.
For a linear SEM, the saturated model is defined by y =  + ∗, where ∗
is distributed as N0	, in which  is a generic covariance matrix without
any structure. The above model cannot be regarded as a saturated model for
assessing the goodness-of-fit of the posited model in the context of nonlinear
SEMs, because in the presence of the nonlinear terms, the distribution of ∗
is not normal. Hence, the assessment about the significance of the nonlinear
terms should be approached with care. This issue will be discussed further in
Section 8.5.

8.2
BAYESIAN ANALYSIS OF A NONLINEAR SEM
199
8.2.2
The Gibbs Sampler For Posterior Simulation
Let Y = y1   yn be the observed data matrix and  be the parameter
vector that contains the unknown parameters in   and  
. Due
to the nonlinearity of H, the classical GLS and ML approaches would
encounter great difficulties in estimating . The main objective here is to obtain
a Bayesian estimate of  with the given observed data Y. To alleviate the diffi-
culty induced by H, Y is augmented with the matrix of latent variables,
 = 1   n in the posterior analysis. Then, the following Gibbs sampler
(Geman and Geman, 1984) is used to generate a sequence of observations from
the posterior distribution Y	: At the jth iteration with current values j
and j, it simulates in turn,
Step (a):
j+1 from pjY, and
Step (b):
j+1 from pj+1Y.
In Step (a),  is composed of   and  
. As  is given, the
nonlinear SEM defined by Equations (8.1) and (8.2) reduces to a simulta-
neous regression model and it is not too difficult to derive the full conditional
distributions.
8.2.3
Full Conditional Distributions
Let y be the unknown parameters in  and   that are associated with
the measurement equation; and let  be the unknown parameters in 
and  
 that are associated with the structural equation. For simplicity, we
assume no fixed parameters. Slight modification as given in Section 5.3.1 can
be incorporated for situations with fixed parameters. It is natural to assume that
the prior distribution of y is independent of the prior distribution of , i.e.
p = pyp. Moreover, as pY = pYyp, we have
pyY ∝pYypyp
= pYypy	pp	
As a result, the conditional densities of y and  can be treated separately.
According to the reasoning given in previous chapters, the following commonly
used conjugate type prior distributions are used. For k = 1   p

D= N 00	
−1
k
D= Gamma 0k0k	
kk	
D= N 0kkH0yk	
(8.5)

200
8
NONLINEAR STRUCTURAL EQUATION MODELS
where T
k is the kth row of 0k0k00kH0yk and 0 are hyperpa-
rameters whose values are assumed to be given. For k ̸= h, it is assumed that
kk and hh are independent. The conditional distribution of  is
given for the case where all its elements are unknown parameters. The condi-
tional distribution with fixed elements can be obtained with slight modification
as before.
Let Ak = H−1
0yk + T −1Y∗
k = y∗
1k   y∗
nkT with y∗
ik = yik −k, ak =
AkH−1
0yk0k + Y∗
k and k = 0k + 2−1Y∗T
k Y∗
k −aT
k A−1
k ak + T
0kH−1
0yk0k.
Then, it can be shown by similar reasoning to that in Section 4.3.1 that for
k = 1   p,
p−1
k Y
D= Gamman/2+0kk	 pkY−1
k 
D= N akkAk	
(8.6)
pY 
D=N −1
0 +n −1
 −1n −1

¯Y +−1
0 0−1
0 +n −1
 −1	
(8.7)
where ¯Y = n
i=1yi −i/n.
Now, consider the conditional distribution of  that is proportional
to
pp.
Let
1 = 1   n2 = 1··· n
and
G =
G1   Gn	. Since the distribution of i only involves p2 =
p2. Moreover, it is assumed that the prior distribution of  is indepen-
dent of the prior distribution of  and  
. It follows that
pp ∝p12 
p 
	p2p	
Hence, the marginal conditional densities of  
 and  can be treated
separately, as before.
Consider a similar conjugate type prior distribution for  with −1 D=
Wq2R00	, where 0 and the positive definite matrix R0 are the given hyper-
parameters. It can be shown by reasoning similar to that in Section 4.3.1 that
p2
D= IWq22T
2 +R −1
0 n +0	
(8.8)
Similarly as before, we use conjugate prior distributions for , so that for
k = 1··· q1,
−1

k
D= Gamma0
k0
k	
k
k	
D= N 0k
kH0k	
(8.9)
where T
k is the kth row of 0
k0
k0k and H0k are all given hyper
parameters. For h ̸= k
kk and 
hh are assumed to be independent.
Similar to k, we assume no fixed elements in k.

8.2
BAYESIAN ANALYSIS OF A NONLINEAR SEM
201
Let E∗
k = 1k   nkT where k = 1   q1kj and kl are elements in 	
and 
, respectively. Then, it can be shown that for k = 1   q1,
p−1

k 
D= Gamman/2+0
k
k	
pk−1

k 
D= Nak
kAk	
(8.10)
where Ak = H−1
0k +GGT −1 ak = AkH−1
0k0k +GE∗
k, and 
k = 0
k +
2−1E∗T
k E∗
k −aT
kA−1
k ak +T
0kH−1
0k0k.
Hence, the conditional distributions associated with Step (a) in Equa-
tion (8.3) are the familiar gamma, normal and inverted Wishart distributions.
Simulating observations from these distributions is straightforward and fast.
Now, consider the conditional distributions associated with Step (b) in Equa-
tion (8.3). It can be shown on the basis of the definition and assumptions
that
pY = 	n
i=1piyi ∝	n
i=1pyiipiipi
As i are mutually independent, and yi are also mutually independent,
piyi is proportional to
exp

−1
2T
i −1i −1
2yi −−iT  −1
 yi −−i−
1
2i −Gi	T  −1

 i −Gi	


(8.11)
Hence, the conditional distribution required in Step (b) is achieved. This distri-
bution is nonstandard and complex. The MH algorithm is used to generate
observations from the target density piyi as given in (8.11). In this
algorithm, we choose N02	 as the proposal distribution, where −1
 =
−1

 +T  −1
  and −1

is given by
−1

 =
⎡
⎣
	T
0  −1

 	0
−	T
0  −1

 

−T 
T  −1

 	0
−1 +T 
T  −1

 

⎤
⎦
(8.12)
where  = Hi/T
i i=0. Let p·2 be the proposal density corre-
sponding to N2	, the MH algorithm for our problem is implemented
as follows. At the rth iteration with a current value 
r
i , a new candidate i
is generated from p·
r
i 2, and accepting this new candidate with the
probability
min

1 piyi
p
r
i yi


(8.13)

202
8
NONLINEAR STRUCTURAL EQUATION MODELS
The variance 2 is chosen such that the acceptance rate is approximately 0.25
or more, see Gelman, Roberts and Gilks (1995).
It should be noted that the components of the conditional distribution
Y	 involved in the Gibbs sampler in analyzing the nonlinear SEMs are
very similar to those in analyzing the linear SEMs, whereas the conditional
distribution Y	 is slightly more complicated (see Equations (8.11), (4.25)
and (4.39)). Hence, the generalization of the Bayesian methods for the linear
SEMs to nonlinear SEMs is not very difficult. Indeed, after data augmenta-
tion and given the latent variables, both the linear and nonlinear SEMs are just
regression models.
8.2.4
Bayesian Estimates
For brevity, let ttt = 1   T ∗ be the random observations of
 generated by the Gibbs sampler and the MH algorithm from Y	.
As before, Bayesian estimates of  and i can easily be obtained via the
corresponding sample means of the generated observations as given in Equa-
tions (4.17) and (4.19) of Chapter 4. Clearly, these Bayesian estimates are
consistent estimates of the corresponding posterior means (see Geyer (1992)).
It is rather difficult to derive analytic forms for the covariance matrices VarY
and VariY. However, estimates of these covariance matrices can be obtained
via the corresponding sample covariance matrices based on the simulated obser-
vations (see Equations (4.18) and (4.20) of Chapter 4). Moreover, other statis-
tical inferences can be conducted on the basis of the generated observations,
see Besag, Green, Higdon and Mengersen (1995) and Gilks, Richardson and
Spiegehalter, (1996). For instance, estimated residuals can be obtained by means
of the parameter estimates and latent variable estimates (see Section 8.4.3).
8.2.5
Illustrative Example 1
To illustrate the Bayesian approach with a real example, a small portion of
the Inter-university Consortium for Political and Social Research (ICPSR) data
set collected in the project World Values Survey 1981–1984 and 1990–1993
(World Values Study Group, 1994) is analyzed. In our illustration, only the
data obtained from Canada were used. Six variables in the full data set (variables
180, 96, 62, 176, 116 and 117) that are related with the respondents’ job,
religious belief and homelife were taken as manifest variables in Y. Details of
these variables are given in Appendix 1.1. Variable 62 was measured by a five-
point scale, while all others were measured by a 10-point scale. As the purpose
of this example is for illustration, they were all treated as continuous for brevity.
Moreover, cases with missing data were not used, and the remaining sample
size was 451.

8.2
BAYESIAN ANALYSIS OF A NONLINEAR SEM
203
Parameters in  and  of the measurement Equation (8.1) were specified as
follows:
 = 1   6T  and T =
⎡
⎣
10
21
0
0
0
0
0
0
10
42
0
0
0
0
0
0
10
63
⎤
⎦ (8.14)
where 1’s and 0’s in  were treated as fixed parameters to identify the model. In
this model, there were three latent factors: , 1 and 2, which can be roughly
interpreted as ‘life’, ‘religious belief ’ and ‘job satisfaction’ respectively. These
latent variables were related by the following nonlinear structural equation:
 = 11 +22 +32
1 +412 +52
2 +
(8.15)
To obtain some prior inputs for the hyperparameters, we first conducted an
auxiliary Bayesian estimation with noninformative prior distributions. Then, we
carried out an actual estimation with the following hyperparameter values for
obtaining the conditional distribution pY  0k = 0
 = 100k = 0
 =
100 = 8H0yk = IH0k = I0k = k0k = k and R0 = 0 −q2 −1
,
where k k and 
 are estimates obtained from the auxiliary estimation. The
2 in the proposed distribution of the MH algorithm was equal to 1.0, giving
an approximately average acceptance rate of 0.46. The algorithm converged in
about 2000 iterations. We took a burn-in phase of 2000 iterations and collected
J = 2000 observations after convergence for obtaining the Bayesian estimates
and their standard errors estimates. Results are reported in Table 8.1. It can be
seen from ˆj that the linear effects and the quadratic effect of 1 are significant,
whilst the interaction effect and the quadratic effect that involve 2 are not
significant.
8.2.6
Comparison with a Product Indicator Method
In the past few years, a number of methods that used the key idea of Kenny
and Judd (1984) incorporating appropriate products of indicators in the model
have been proposed. Results were obtained via the LISREL program, based on
the ML option with nonlinear constraints. See, for example, Jaccard and Wan
(1995), Jöreskog and Yang (1996), Ping (1996), among others. Li et al. (1998)
provided a comparison of these methods and concluded that no substantial
discrepancy was found in parameters estimates across these methods. Owing to
this conclusion, the results obtained by the product indicator (PI) approach as
given in Jaccard and Wan (1995) are compared here with the Bayesian estimates
via a simulation study as reported in Lee, Song and Poon (2004).

204
8
NONLINEAR STRUCTURAL EQUATION MODELS
Table
8.1
Bayesian estimates (EST) and standard error esti-
mates (SE) of illustrative example 1.
Parameter
EST
SE
Parameter
EST
SE
1
8296
0112
21
0.971
0084
2
7867
0102
42
2.400
0194
3
2758
0073
63
0.822
0101
4
6951
0121
1
0.183
0081
5
8027
0076
2
0.476
0071
6
7389
0099
3
0.164
0082
1
1147
0137
4
0.033
0080
2
0872
0114
5
−0025
0041
3
1476
0124
11
0.882
0114
4
2242
0420
21
0.270
0087
5
0897
0192
22
1.829
0259
6
3257
0255

0.870
0124
The simulation study was conducted on the basis of a nonlinear LISREL type
model. According to slightly different notation in Jaccard and Wan (1995), let
y = y1y2x1x2x3x4T . The measurement equations are given by
y1
y2

=
 1
y

+
1
2


(8.16)
⎛
⎜⎜⎝
x1
x2
x3
x4
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
1
0
21 0
0
1
0 42
⎞
⎟⎟⎠
1
2

+
⎛
⎜⎜⎝
1
2
3
4
⎞
⎟⎟⎠
(8.17)
where the y’s and x’s are manifest variables; 1 and 2 are latent variables, ’s
and ’s are error measurements; y21 and 42 are unknown parameters, 1’s
and 0’s are fixed parameters. It is assumed that  = 12T is distributed as
N0	, where the covariance matrix  contains unknown parameters 1121
and 22 = 12T is distributed as N0 	, where   is a diagonal
covariance matrix with diagonal elements 1 and 2 = 1234T is
distributed as N0 	, where   is a diagonal covariance matrix with diag-
onal elements kk = 1   4. The latent vectors  and  are uncorrelated
to each other as usual.
The following structural equation is considered:
 = 11 +22 +312 +42
1 +52
2 +

(8.18)
where ′s are unknown regression coefficients that describe the relations between
the endogenous latent variable  with exogenous variables ′s
 is distributed

8.2
BAYESIAN ANALYSIS OF A NONLINEAR SEM
205
as N0
	 and is uncorrelated with the ′s. In this structural equation, both
interaction and quadratic effects are involved. It is linear in the regression coef-
ficients, but nonlinear in the latent variables. The estimates of the unknown
parameters obtained from the Bayesian and PI approaches are compared.
Consider the application of the PI approach for obtaining the estimates. It is
assumed that the data are in mean deviation form. This approach requires four
product indicators for 12, three product indicators for 2
1 and three product
indicators for 2
2. It can be shown via similar derivations to those in Jaccard
and Wan (1995) or Jöreskog and Yang (1996) that the equation relating the
exogenous latent variables and their indicators is given by:
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
x1
x2
x3
x4
x1x3
x1x4
x2x3
x2x4
x2
1
x1x2
x2
2
x2
3
x3x4
x2
4
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
=
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
0
0
0
0
21 0
0
0
0
0
1
0
0
0
0 42
0
0
0
0
0
1
0
0
0
0
42
0
0
0
0
21
0
0
0
0 2142 0
0
0
0
0
1
0
0
0
0
21 0
0
0
0
2
21 0
0
0
0
0
1
0
0
0
0 42
0
0
0
0 2
42
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎝
1
2
12
2
1
2
2
⎞
⎟⎟⎟⎟⎠
+
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
2
3
4
5
6
7
8
9
10
11
12
13
14
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠

The covariance matrix of 12122
12
2T and the variances of 5   14
can be derived by straightforward calculations of variances and covari-
ances (see Lee, Song and Poon (2004)). Moreover, following Jaccard and
Wan (1995), the following pairs of measurement errors associated with the
products of indicators must be permitted to be correlated. More specif-
ically, cov56cov57cov68cov78cov910cov1011,
cov1213 and cov1314 are not equal to zero. All these nonlinear
constraints and specifications are required to cooperate in the LISREL program;
see Lee, Song and Poon (2004) for the program setup.
In the simulation study, two sets of true population values of the unknown
parameters are considered:
(I):
y = 0621 = 0542 = 061 = 2 = 051 = 2 = 3 = 4 =
0511 = 22 = 1021 = 061 = 042 = 06, 3 = 4 = 5 = 05.
(II):
The unknown parameters are the same as the first set, except 21 = 02,
3 = 4 = 5 = 015.

206
8
NONLINEAR STRUCTURAL EQUATION MODELS
The first set of true values is corresponding to a situation with larger latent
factors correlation and more significant interaction and/or quadratic effects.
On the basis of these true values, random vectors i1i2T  i1i2T 
i1i2i3i4T , 
i  i = 1   n, with n = 150 and 300, are simulated from
the corresponding normal distributions. Then ii = 1   n are obtained
from i1i2 and 
i via Equation (8.18). Finally, for i = 1   nyi is
obtained from i and i1i2 via Equation (8.16), and xi is obtained from
i1i2 and i1i2i3i4 via Equation (8.17). Although the population
means of y and x are zero, the data are transformed to mean deviation form
in order to satisfy the assumption of the PI approach. The same data are
analyzed by the PI approach with the LISREL program and the Bayesian
approach.
In the simulation study, the hyperparameters in the conjugate prior distri-
bution are specified as follows. We first conduct a Bayesian estimation on
a simulated data set with non-information prior distribution which involves
ad hoc prior hyperparameters, then we choose the hyperparameter values from
the solution of this preliminary estimation. A few test runs have also been taken
to decide the required number of ‘burn-in’ iterations. We observe that the
Gibbs sampler with the MH algorithm converges at about 450 iterations. To be
conservative, we take 500 ‘burn-in’ iterations. Then, a total of 2000 additional
iterations are taken to collect the sample t  t = 1   T ∗ for computing
the Bayesian estimates and standard error estimates.
The empirical performances of the approaches are evaluated on the basis of
ˆr  r = 1   100 obtained in 100 replications. The means (MEAN) of the
estimates and the following root mean squares (RMS) between the true values
and the corresponding estimates are computed:
RMS of ˆ k =

100−1
100

r=1
ˆ rk− ok	2
1/2

where ˆ k and  ok are the kth element of the parameter vector and its
true value, respectively. To evaluate the accuracy of the standard errors esti-
mates, let SD k be the sample standard deviation obtained from ˆ rkr =
1   100, and SE k be the mean of standard errors estimates of ˆ k
obtained from the various approaches. If the standard errors estimates obtained
by the method are accurate, SE k should be close to SD k and
SE k/SD k should be close to 1.0. Hence the ratio SE k/SD k
is used to evaluate the accuracy of standard error estimates (see Lee, Poon and
Bentler, 1995).
Results obtained from the PI approach on the basis of the model with struc-
tural Equation (8.18) under true values 3 = 4 = 5 = 015 and 21 = 02
and 3 = 4 = 5 = 05 and 21 = 06 are presented in Tables 8.2 and 8.3
respectively. From the RMS columns, it is obvious that the estimates produced

8.2
BAYESIAN ANALYSIS OF A NONLINEAR SEM
207
Table
8.2
PI Estimates in model with true quadratic and interaction coefficients
3 = 4 = 5 = 015 and 21 = 02.
Parameter
True
value
n = 150
n = 300
MEAN
RMS
SE/SD
MEAN
RMS
SE/SD
y
06
0620
0111
0820
0587
0070
0.921
21
05
0550
0544
0179
0507
0158
0.409
42
06
0644
0253
0365
0619
0183
0.323
1
04
0385
0158
0747
0388
0128
0.646
2
06
0586
0144
0847
0589
0126
0.670
3
015
0103
0251
0653
0119
0122
0.852
4
015
0147
0162
0726
0162
0096
0.813
5
015
0169
0176
0663
0140
0081
0.847
11
10
1156
0868
0451
1076
0843
0.388
21
02
0251
0176
0367
0229
0100
0.384
22
10
1017
0587
0378
1037
0401
0.395
1
05
0492
0655
0898
0490
0613
0.984
2
05
0487
0248
0943
0503
0197
0.939
1
05
0327
0165
0461
0422
0113
0.402
2
05
0468
0081
0208
0490
0057
0.487
3
05
0462
0561
0405
0462
0371
0.382
4
05
0466
0286
0426
0488
0080
0.374

036
0372
0196
0789
0391
0174
0.728
Note: Tables 8.2 to 8.5 are Taken from Lee, Song and Poon (2004).
from the PI approach are inaccurate. Also, from the SE/SD columns, it is
obvious that the PI approach seriously underestimates the standard errors. For
this model, parameter estimates and standard error estimates are also obtained
via the Bayesian approach. To give some idea about the empirical performances,
Bayesian estimates with true values 3 = 4 = 5 = 015 and 21 = 02 and
3 = 4 = 5 = 05 and 21 = 06 are reported in Tables 8.4 and 8.5, respec-
tively. It can be observed from these tables that the parameter estimates and
the standard error estimates obtained from the Bayesian approach are accurate,
and clearly better than those that are obtained from the PI approach.
8.2.7
Comparison with the LMS Approach via the
Kenny–Judd Model
The objective of this simulation study is to investigate the empirical performances
of the Bayesian approach in analyzing the basic Kenny–Judd model (Kenny and
Judd,1984),andtocomparetheefficiencyoftheBayesianmethodwiththeresults
that were obtained by the LMS approach (Klein and Moosbrugger, 2000), and

208
8
NONLINEAR STRUCTURAL EQUATION MODELS
Table
8.3
PI estimates in model with quadratic and interaction effects coefficients
3 = 4 = 5 = 05 and 21 = 06.
Parameter
True
value
n = 150
n = 300
MEAN
RMS
SE/SD
MEAN
RMS
SE/SD
y
06
0608
0044
1003
0599
0037
0.836
21
05
0460
0126
0314
0470
0089
0.351
42
06
0580
0134
0350
0575
0098
0.334
1
04
0401
0261
0694
0372
0194
0.684
2
06
0580
0254
0746
0590
0179
0.714
3
05
0019
1051
0683
0107
0661
0.753
4
05
0557
0557
0649
0526
0367
0.617
5
05
0653
0577
0646
0561
0308
0.745
11
10
1145
0706
0398
1111
0599
0.394
21
06
0666
0560
0438
0652
0305
0.507
22
10
1081
0316
0359
1059
0233
0.381
1
05
0507
0639
0966
0490
0591
0.950
2
05
0486
0566
0934
0510
0548
1.050
1
05
0314
0194
0390
0353
0149
0.374
2
05
0508
0093
0355
0504
0062
0.370
3
05
0374
0291
0427
0378
0227
0.386
4
05
0488
0100
0434
0503
0074
0.417

036
0699
0335
0746
0872
0305
0.612
Table
8.4
Bayesian estimates in model with true quadratic and interaction coefficients
3 = 4 = 5 = 015 and 21 = 02.
Parameter
True
value
n = 150
n = 300
MEAN
RMS
SE/SD
MEAN
RMS
SE/SD
y
0.6
0.613
0093
0871
0593
0.059
0.963
21
0.5
0.582
0146
1102
0557
0.104
1.055
42
0.6
0.668
0125
1131
0629
0.089
0.941
1
0.4
0.447
0121
1241
0430
0.102
0.921
2
0.6
0.639
0114
1278
0616
0.087
1.042
3
0.15
0.230
0179
1086
0192
0.113
0.956
4
0.15
0.177
0108
1132
0186
0.079
1.028
5
0.15
0.191
0115
0976
0149
0.061
0.968
11
1.0
0.874
0254
1035
0905
0.194
0.992
21
0.2
0.187
0103
1057
0189
0.078
1.008

8.2
BAYESIAN ANALYSIS OF A NONLINEAR SEM
209
22
1.0
0.896
0237
0950
0978
0.152
1.019
1
0.5
0.511
0143
0969
0518
0.097
1.028
2
0.5
0.506
0079
0994
0511
0.054
1.003
1
0.5
0.650
0250
0951
0610
0.174
1.018
2
0.5
0.498
0078
1062
0486
0.055
1.090
3
0.5
0.617
0191
1010
0540
0.135
0.882
4
0.5
0.494
0073
1135
0501
0.059
0.998

0.36
0.345
0119
1053
0333
0.102
0.947
Table
8.5
Bayesian estimates in model with true quadratic and interaction coefficients
3 = 4 = 5 = 05 and 21 = 06.
Parameter
True
value
n = 150
n = 300
MEAN
RMS
SE/SD
MEAN
RMS
SE/SD
y
0.6
0.608
0037
0883
0604
0.027
1.004
21
0.5
0.510
0082
1047
0523
0.063
1.047
42
0.6
0.615
0081
1102
0615
0.063
1.004
1
0.4
0.375
0185
1113
0367
0.137
1.127
2
0.6
0.597
0182
1127
0628
0.128
1.207
3
0.5
0.519
0238
1275
0475
0.184
1.289
4
0.5
0.514
0176
1236
0565
0.168
1.099
5
0.5
0.582
0197
1269
0539
0.129
1.326
11
1.0
0.974
0194
0989
0915
0.164
0.941
21
0.6
0.617
0126
1000
0581
0.091
0.968
22
1.0
0.955
0204
0912
0933
0.147
0.972
1
0.5
0.539
0179
0856
0521
0.085
1.312
2
0.5
0.501
0087
0909
0506
0.052
1.067
1
0.5
0.562
0157
0886
0560
0.111
0.929
2
0.5
0.524
0076
0983
0504
0.050
0.946
3
0.5
0.560
0126
1027
0529
0.085
0.961
4
0.5
0.515
0065
1144
0509
0.052
0.964

0.36
0.353
0129
1085
0353
0.059
1.327
publishedinSchermelleh-Engel,KleinandMoosbrugger(1998).Usingournota-
tion that is slightly different from that in Schermelleh-Engel, Klein and Moos-
brugger (1998), the Kenny–Judd model is defined by the following structural
equation and measurement models:
y1 = 1 +1 y2 = 211 +2
y3 = 2 +3 y4 = 422 +4
y∗=  = +11 +22 +312 +
(8.19)

210
8
NONLINEAR STRUCTURAL EQUATION MODELS
Based on the Kenny–Judd model, Schermelleh-Engel, Klein and Moos-
brugger (1998) conducted a simulation study to examine the performances
of the LMS method. In their simulation study, three models that correspond
to the following true values of the latent interaction effect were considered:
3 = 03 (Model A), 3 = 07 (Model B) and 3 = 15 (Model C). The true
values of the other parameters in Models A, B and C were given by
1 = 02 2 = 04  = 10 
 = 02 11 = 049 21 = 02352
22 = 06421 = 06 42 = 07 1 = 051 2 = 064
3 = 0364 = 051
where 
 is the variance of 
ij are the variances and covariance of (12) and
i are the variances of i. (Note that in the notations of Schermelleh-Engel,
Klein and Moosbrugger (1998), 11 = 
 and  ii = i.) The sample sizes were
varied by 200, 400 and 800. For each case in the 3×3 design, they used 500
replications (except for nonconvergent replications) to obtain the results. The
means (MEAN) and standard deviation (MC-SD, same as our SD( k)) of the
parameter estimates, and the means of the estimated standard error (EST-SD,
same as our SE( k)) were computed. The simulated results were reported in
Appendix C of Schermelleh-Engel, Klein and Moosbrugger (1998).
To study the impact of the prior inputs of the hyperparameters (see Equa-
tions (8.8) and (8.9)) in the Bayesian method, we consider accurate prior inputs
(Type I) and inaccurate prior inputs (Type II). The accurate prior inputs are given
by the following: (i) in the prior distribution of ’s, the means of the normal
distributions are taken to be the true values and the covariance matrices are taken
to be the identity matrix; (ii) in the prior distribution of 12 and 3, the
elements in the mean vector of the normal distribution are taken to be the true
values and the covariance matrix is taken to be the identity matrix; (iii) in the prior
distribution of , 0 and R0 in the Wishart distribution are respectively taken
to be 4 and −1
0 , where 0 is the matrix with true values of 1121 and 22;
(iv) in the prior distributions of 
 and i, the hyperparameters in the inverted
Gamma distribution are taken to be 0k = 5, 0k = 2 and 0
k = 10, 0
k =
2. The inaccurate prior inputs are given by the following ad hoc values. In (i)
and (ii), the means of prior normal distributions are all zero, and the covari-
ance matrices are equal to twice the identity matrices; in (iii), 0 = 10, and R0
is the identity matrix; and in (iv), 0k = 8, 0k = 5 and 0
k = 20, 0
k = 5.
The other settings of the simulation study, for example the sample sizes, are
exactly equal to those in Schermelleh-Engel, Klein and Moosbrugger (1998). The
data sets for the simulation study were generated by using the true values and
the model defined by Equation (8.19). Although these data sets are not exactly
the same as those used in Schermelleh-Engel, Klein and Moosbrugger (1998),
they are simulated from the same distribution. Bayesian solutions were obtained
on the bases of 3000 observations after 3000 burn-in iterations.

8.2
BAYESIAN ANALYSIS OF A NONLINEAR SEM
211
Table
8.6
Results of Monte Carlo studies with 3 = 03 (Model A): mean and stan-
dard deviation of parameter estimates and estimated standard error.
N = 200
N = 400
N = 800
Par.
True
value
Mean MC-SD EST-SD Mean MC-SD EST-SD Mean MC-SD EST-SD
Prior input Type I

100
1002
040
039
1001
027
028
1001
019
.019
1
020
196
049
050
199
036
035
198
025
.025
2
040
396
045
044
399
030
031
400
022
.022
3
030
302
048
047
302
034
033
299
022
.023

020
198
018
019
200
013
014
200
010
.010
21
060
590
086
080
595
056
057
597
044
.040
42
070
696
062
063
697
044
045
698
032
.032
11
049
498
050
050
494
034
035
492
024
.025
21
0.2352
236
041
043
235
031
031
237
022
.022
22
064
644
063
065
646
047
046
643
031
.032
1
051
513
050
051
512
034
036
512
027
.025
2
064
633
062
062
636
043
045
639
032
.032
3
036
364
033
036
365
025
026
362
017
.018
4
051
511
051
050
513
037
036
512
025
.026
Prior input Type II

100
995
039
040
999
029
028
1000
018
.020
1
020
197
049
052
201
035
036
200
023
.025
2
040
396
043
045
397
031
031
398
022
.022
3
030
301
048
049
299
034
033
300
023
.023

020
212
017
020
207
012
014
204
009
.010
21
060
599
076
081
596
057
057
600
041
.041
42
070
698
063
064
697
044
045
698
031
.032
11
049
482
046
048
490
035
034
490
025
.024
21
0.2352
231
042
042
233
030
030
234
021
.021
22
064
625
061
062
636
044
045
638
032
.032
1
051
526
050
051
519
034
036
515
026
.026
2
064
647
061
063
646
044
045
644
032
.032
3
036
386
033
037
373
024
026
366
017
.018
4
051
527
050
051
519
034
036
514
027
.026
The simulation results obtained by the Bayesian approach under Models A,
B and C are reported in Tables 8.6, 8.7, and 8.8, respectively. Our focus
is on the comparison of the results in these tables with those reported in
Table 10.C1, Table 10.C2, and Table 10.C3 in Schermelleh-Engel, Klein and
Moosbrugger (1998), in terms of the bias of parameter estimates and standard
error estimates (with more emphasis on the interaction effect 3), and the
efficiency of parameter estimates.

212
8
NONLINEAR STRUCTURAL EQUATION MODELS
Table
8.7
Results of Monte Carlo studies with 3 = 07 (Model B): mean and stan-
dard deviation of parameter estimates and estimated standard error.
N = 200
N = 400
N = 800
Par.
True
value
Mean MC-SD EST-SD Mean MC-SD EST-SD Mean MC-SD EST-SD
Prior input Type I

100
1001
040
039
1002
028
028
1003
019
.019
1
020
200
050
050
199
035
035
199
025
.025
2
040
399
042
044
400
030
031
400
023
.022
3
070
695
048
047
699
033
033
698
023
.023

020
197
018
019
200
014
014
200
010
.010
21
060
594
081
081
596
056
057
600
039
.040
42
070
697
065
063
693
043
045
702
033
.032
11
049
497
051
050
493
034
035
493
025
.025
21
0.2352
237
043
044
236
030
031
237
021
.022
22
064
648
066
065
647
045
046
645
031
.032
1
051
507
049
050
511
037
036
511
027
.025
2
064
637
060
063
639
046
045
640
032
.032
3
036
363
034
036
364
025
026
363
018
.018
4
051
514
049
051
511
035
036
512
026
.026
Prior input Type II

100
997
041
040
1000
027
028
1001
021
.020
1
020
201
052
052
200
036
036
198
024
.025
2
040
395
044
045
400
032
031
400
023
.022
3
070
699
049
049
698
032
033
701
024
.023

020
214
015
020
207
013
014
205
010
.010
21
060
596
083
081
594
055
057
597
040
.040
42
070
698
063
064
695
044
045
699
033
.032
11
049
481
046
047
487
033
034
490
024
.024
21
0.2352
227
041
042
231
030
030
235
021
.021
22
064
626
060
062
632
045
045
638
032
.032
1
051
526
047
051
518
033
036
517
025
.026
2
064
647
062
063
645
044
045
644
033
.032
3
036
385
035
037
373
025
026
368
017
.018
4
051
521
046
051
518
036
036
516
025
.026
Bias of parameters and standard error estimates. The means of the param-
eter estimates as reported in the columns under ‘Mean’ in Tables 8.6, 8.7 and
8.8 indicate that there is no sign of substantial bias in the Bayesian estimates.
Schermelleh-Engel, Klein and Moosbrugger (1998) defined the following
relative bias of the estimated standard errors:
Rel-Biasˆ  = EST-SDˆ 	/MC-SDˆ 	−10	×100%

8.2
BAYESIAN ANALYSIS OF A NONLINEAR SEM
213
Table
8.8
Results of Monte Carlo studies with 3 = 15 (Model C): mean and
standard deviation of parameter estimates and estimated standard error.
N = 200
N = 400
N = 800
Par.
True
value
Mean MC-SD EST-SD Mean MC-SD EST-SD Mean MC-SD EST-SD
Prior input Type I

100
1006
038
039
1005
027
028
1003
021
.020
1
020
204
050
051
201
036
036
202
025
.025
2
040
398
044
044
399
032
031
398
023
.022
3
150
1488
048
048
1497
031
033
1497
023
.023

020
203
017
020
202
013
014
201
010
.010
21
060
597
078
080
600
059
057
602
041
.040
42
070
695
061
063
697
044
044
700
030
.031
11
049
499
047
050
494
034
035
492
025
.025
21
0.2352
239
042
044
235
029
031
238
021
.022
22
064
645
063
065
648
048
046
646
034
.032
1
051
507
050
050
502
033
035
510
024
.025
2
064
633
059
062
633
045
045
640
031
.032
3
036
363
034
036
360
025
025
363
017
.018
4
051
508
049
050
507
033
036
509
026
.025
Prior input Type II

100
1001
038
041
1002
027
028
1001
019
.020
1
020
205
050
053
201
036
036
200
025
.025
2
040
396
044
046
398
032
032
399
022
.022
3
150
1489
048
050
1497
031
034
1498
023
.023

020
221
016
020
212
012
014
207
010
.010
21
060
597
078
081
600
059
057
598
041
.040
42
070
695
061
064
697
044
045
697
031
.032
11
049
484
046
048
487
034
034
491
024
.025
21
0.2352
232
040
042
234
029
030
235
021
.021
22
064
626
062
062
638
047
045
638
033
.032
1
051
521
048
050
509
032
035
510
025
.025
2
064
643
058
062
638
045
045
638
032
.032
3
036
381
033
037
369
025
026
364
018
.018
4
051
522
047
051
514
032
036
514
026
.026
for comparing the standard error estimate with the ‘true’ standard error estimate
(MC-SD(ˆ )). From Tables 8.6, 8.7 and 8.8, we observe that the relative bias of
the estimated standard error of the other estimates is low for all models, sample
sizes and prior inputs Type I and Type II. Hence, the estimated standard errors
provided by the Bayesian method are comparable to those in Schermelleh-
Engel, Klein and Moosbrugger (1998) and can be effectively used for confidence
intervals and hypothesis testing.

214
8
NONLINEAR STRUCTURAL EQUATION MODELS
Table
8.9
Relative efficiency of Bayesian estimates ˆ3 and ˆ
 in relation to LMS in
Simulation Study I.
Model
Prior
N = 200
N = 400
N = 800
ˆ3
ˆ
ˆ3
ˆ
ˆ3
ˆ
A
Type I
23.5%
41.3%
22.9%
42.3%
20.2%
51.0%
Type II
23.5%
36.9%
22.9%
36.0%
22.0%
41.3%
B
Type I
13.2%
29.8%
12.3%
31.4%
12.5%
39.1%
Type II
13.8%
20.7%
11.6%
27.0%
13.6%
39.1%
C
Type I
4.1%
10.3%
3.7%
11.7%
4.4%
13.7%
Type II
4.1%
9.1%
3.7%
10.0%
4.4%
13.7%
Efficiency of parameter estimates. Schermelleh-Engel, Klein and Moosbrugger
(1998) compared the efficiency of different methods by means of the relative
efficiency. Based on their definition, the relative efficiency of the Bayesian
method versus LMS is calculated by
Rel-Effˆ  = MC-SDˆ BAY/MC-SDˆ LMS	×100%
The relative efficiency values of the Bayesian methods versus LMS for the
interesting estimators ˆ3 and ˆ
 are reported in Table 8.9. We observe that
under prior inputs Type I and II, the relative efficiency percentage of parameter
estimates ˆ3 and ˆ
 for the Bayesian method versus LMS is significantly smaller
than 100%. Comparing the other MC-SD(ˆ) values in Tables 8.6 to 8.9 with
those values in Tables 10.C1, 10.C2 and 10.C3 in Schermelleh-Engel, Klein
and Moosbrugger (1998), the above finding is also true for the other parameter
estimates. These results indicate that the Bayesian estimates are significantly
more efficient than the LMS estimates.
From our discussion on the Bayesian estimation presented in Chapter 4,
Section 4.2.1, particularly Equation (4.1), we know that the asymptotic stan-
dard derivations of the Bayesian estimate and the ML estimate should be close.
Here, we recall that the ‘true’ ML estimator is the one that maximizes the exact
likelihood function (or equivalently its logarithm), and this ‘true’ ML estimator
has the optimal (asymptotic) properties. However, the LMS estimator is not
the ‘true’ ML estimator for the Kenny–Judd model. Recall that the LMS proce-
dure at least involves the following rather complicated approximations. In Klein
and Moosbrugger’s notation, the density for the realization x = xy = y is
expressed as a high-dimensional integral, see their Equation (15). This high-
dimensional integral was approximated by the Hermite–Gaussian quadrature,
and then led to a finite mixture density of M= 24 components. Hence, the
exact likelihood function is not expressed in closed form. The LMS proce-
dure then applied the EM algorithm to find the solution of the ‘approximated’

8.3
BAYESIAN ESTIMATION OF NONLINEAR SEMs
215
likelihood function. As a result, the LMS method only gives an approximation
of the ‘true’ ML estimate, which may not be able to achieve the optimal asymp-
totic properties. Practically, the large variablity of the LMS estimate may be due
to the various complicated approximations.
8.3
BAYESIAN ESTIMATION OF NONLINEAR SEMs WITH
MIXED CONTINUOUS AND ORDERED CATEGORICAL
VARIABLES
In this section, NSEMs with mixed continuous and ordered categorical vari-
ables are analyzed. Using similar notations to those in Chapter 6, the p × 1
manifest random vector is denoted by v, which contains a subvector of vari-
ables x = x1   xrT whose exact continuous measurements are observable,
and a subvector of variables y = y1   ysT whose continuous measurements
are unobservable, but the underlying information is given by an observable
ordered categorical vector z = z1   zsT . The relationship between y and z
is similarly given by Equation (6.3) by a set of thresholds k = k1   kbk
defined as before. Without loss of generality, suppose v = xT yT T and this
manifest random vector satisfies the following measurement equation as in
Equation (8.1):
vi = +i +i
i = 1   n
(8.20)
where i and i are exactly defined as in Section 8.2.1. The nonlinear
structural equation is defined either by Equation (8.2) or (8.3). Again, the
same identification conditions are imposed and the variables in x and y can be
indicators for  and/or . Similarly, as before, suppose that the model relating
to the subvector yi = yi1   yisT of vi is given by:
yi = y +yi +yi
where ys ×1 is a subvector of ys ×q is a submatrix of , and yis ×1
is a subvector of i with diagonal covariance matrix  y. Let zi = zi1   zisT
be the ordered categorical observation corresponding to yii = 1   n. In
this section, the NSEM defined above is estimated with the data xizi
i = 1   n of mixed continuous and ordered categorical observations.
8.3.1
Posterior Analysis
Let X = x1   xn and Z = z1   zn be the observed continuous
and ordered categorical data matrices respectively; let Y = y1   yn and

216
8
NONLINEAR STRUCTURAL EQUATION MODELS
 = 1   n be the latent continuous data and latent variables respec-
tively. Inspired again by the technique of data augmentation, the observed
data XZ	 are augmented with the latent data Y	 in the posterior anal-
ysis. As we will see later, the difficulty induced by the ordered categor-
ical data can be solved. Joint Bayesian estimates of , the thresholds in
 = 1   s, and the structural parameter vector  that contains all
unknown parameters in  
 and  , will be obtained via the
following Gibbs sampler. At the jth iteration with current values jjj
and Yj:
Generate j+1 from pjjYjXZ
Generate j+1 from pj+1jYjXZ
Generate j+1Yj+1 from pYj+1j+1XZ
(8.21)
To derive the conditional distributions involved in Equation (8.21), we
need to impose some natural assumptions on the prior distributions of . Let
V = v1   vn with vi = xiyi; thus V = XY. Moreover, let  be the
unknown parameters in  and   that are associated with Equation (8.20),
and let  be the unknown parameters in  and  
 that are associated
with Equation (8.2). We assume that the prior distribution of  is indepen-
dent of the prior distribution of . Hence p = pp. Conditional on
the given X and Y and  are not relevant. Hence,
pYXZ = pXY = pV
(8.22)
For the conditional distribution corresponding to ,
pYXZ = pV = pV pV
(8.23)
Moreover, when  is given, X is conditionally independent of Y, hence
pYXZ = pYZ
(8.24)
Note that given V, the model with the ordered categorical data is the same
as the model with the continuous data. Hence, using the same conjugate prior
distributions, the conditional distribution in Equation (8.22) is exactly the same
as that given in Equation (8.11), and the conditional distributions involved
in Equation (8.23) are exactly the same as those given in Equations (8.6),
(8.7), (8.8), and (8.10). Moreover, as Y is associated with the measure-
ment Equation (8.20) which is exactly the same as that given in Equation (6.1),

8.3
BAYESIAN ESTIMATION OF NONLINEAR SEMs
217
pYZ can be derived by the same reasoning as before. Hence, under
the same noninformative prior distribution for the unknown threshold param-
eters as given before, that is, for k = 1   s,
pk2   kbk−1 ∝c for k2 < ··· < kbk−1
where c is a constant; the conditional distribution pYZ is given by
Equation (6.13). For completeness, it is reproduced here. For k = 1   s, let
Yk and Zk be the kth rows of Y and Z, respectively, it follows that
pkYkZk = pkZkpYkkZk
with
pkZk ∝
n
i=1

!∗
−1/2
yk
kzik+1 −yk −T
yki

−!∗
−1/2
yk
kzik −yk −T
yki


and pYkkZk is a product of pyikkZk, where
pyikkZk
D= Nyk +T
ykiykIkzik kzik+1 yik
in which yk is the kth diagonal element of  yyk is the kth element of yT
yk
is the kth row of yIAy is an index function which takes 1 if y ∈A and
0 otherwise, and !∗· denotes the standard normal cumulative distribution
function. As a result,
pkYkZk ∝
n
i=1
−1/2
yk
yik −yk −T
yki	Ikzik kzik+1 yik
where · is the standard normal density. Note that the above result is almost
identical to Equation (6.11), except that the underlying model is different.
In a similar way to before, because the conditional distributions involved in
pYXZ are the standard Gamma, normal and inverted Wishart distri-
butions, simulating observations from them is straightforward and fast. The
MH algorithm as described in Sections 8.2.3 and 6.3.2 can be applied in exactly
the same manner for simulating observations from pYXZ and
pYXZ, respectively. Bayesian estimates and other statistics for 

218
8
NONLINEAR STRUCTURAL EQUATION MODELS
and  can be obtained via a sufficiently large number of simulated observations
collected from the hybrid algorithm after convergence (see Equations (8.14)
and (8.15)).
We mentioned at the end of Section 8.2.3 that the generalization of the
Bayesian approach from a standard linear SEM to an NSEM is not difficult.
From the discussion in this section, we understand that the Bayesian approach
for analyzing an NSEM with continuous variables can be generalized to handle
NSEMs with mixed continuous and ordered categorical variables without much
difficulty. It is conceptually simple, and just involves one additional component
in simulating pkYkZk in the Gibbs sampler. In contrast, it is rather
difficult to apply either the covariance structure analysis approach, the product
indicator approach, or the multistage approach to analyze nonlinear SEMs with
ordered categorical variables.
8.3.2
Illustrative Example 2
A small portion of the ICPSR data set collected in the project World Values
Survey 1981–1984 and 1990–1993 (World Values Study Group, 1994) is
analyzed in this example. In this illustration, only the data obtained from the
UK were used. Six variables in the original data set (variables 180, 96, 62, 179,
116 and 117; see Appendix 1.1) that related to the respondents’ job, religious
belief and homelife were taken as manifest variables in v = v1··· v6T . After
deleting cases with missing data, the sample size was 197. Among them, v1v2
were related to life, v3v4 were related to religions belief and v5v6 were
related to job satisfaction. Variables v3v4 were ordered categorical with five
categories. Other variables were measured on a 10-point scale and for conve-
nience they were treated as continuous variables.
A nonlinear structural equation model with the latent factors  and  =
12T
was proposed with the following specifications: 	 = 0H =
1212T , and

T =
⎡
⎣
1
2
3
⎤
⎦
T =
⎡
⎣
10
21
0
0
0
0
0
0
10
42
0
0
0
0
0
0
10
63
⎤
⎦
where the 1’s and 0’s in  were treated as fixed parameters. Here, the nonlinear
structural equation is  = 11 + 22 + 312 + 
. From the structure of ,
latent factors , 1 and 2 can be roughly interpreted as ‘life’, ‘religious belief’
and ‘job satisfaction’, respectively, while 12 represents the interaction of ‘reli-
gious belief’ and ‘job satisfaction’. To identify the ordered categorical variables,
we set k1 = !∗−1nk1/n and k4 = !∗−14
i=1 nki/n, where nki denotes
the number of ordered categorical observations that are equal to i, and !∗
denotes the distribution function of the standard normal distribution. In this

8.3
BAYESIAN ESTIMATION OF NONLINEAR SEMs
219
nonlinear model, there were a total of 26 unknown parameters which are the
unknown elements in  and kik = 34i = 23, , variances and covari-
ance in  and the diagonal elements in   and  
. Bayesian estimates of
these structural parameters, and estimates of ii1 and i2 were obtained via
the hybrid algorithm that combines the Gibbs sampler and the MH algorithm.
The variances of the proposal distributions in the MH algorithm were chosen
such that the approximately average acceptance rate is 0.35. The following
hyperparameters were selected: 0k = 0
 = 100k = 0
 = 8H0yk and H0k
are diagonal matrices with diagonal elements 0.25, 0 = 200 = I6R −1
0
=
2 ˜, 0 = ˜ and 0k = ˜0k, where ˜ ˜ and ˜0k were the Bayesian esti-
mates obtained using noninformative prior distributions. Based on different
starting values of the parameters, three parallel sequences of observations were
generated. We found that the hybrid algorithm converged after about 2000
iterations. After convergence, a total of 2000 observations were collected to
obtain the results.
Bayesian estimates of the structural parameters and their standard error esti-
mates are reported in Table 8.10. The estimated nonlinear structural equation is
i = 0723i1 +0710i2 −0494i1i2
From the standard error estimates of ˆ1, ˆ2 and ˆ3, we see that all these
coefficients are significantly different from zero. Hence, ‘religious belief’ and
‘job satisfaction’ have significant positive linear effects to ‘life’. This finding
is reasonable. Moreover, ‘religious belief’ and ‘job satisfaction’ also have a
Table
8.10
The Bayesian estimates (EST) and their standard error estimates
(SE) of illustrative example 2.
Parameter
EST
SE
Parameter
EST
SE
32
0305
0057
1
8414
0121
33
0608
0060
2
7824
0123
42
−0140
0054
3
−0001
0081
43
0075
0058
4
0030
0083
1
0723
0183
5
7573
0142
2
0710
0142
6
7430
0167
3
−0494
0150
1
0697
0153
21
0869
0096
2
1287
0191
42
1050
0148
3
0447
0085
63
1151
0270
4
0528
0092
11
0519
0122
5
2439
0507
21
−0110
0108
6
3356
0543
22
1792
0587

0828
0168
This table is taken from Lee and Zhu (2000).

220
8
NONLINEAR STRUCTURAL EQUATION MODELS
significant negative interaction effect on ‘life’. This shows that ‘life’ cannot be
adequately accounted by the linear effects of ‘religious belief’ and ‘job satisfac-
tion’, and an interaction effect of these two independent latent variables has
to be added. Depending on the signs of 1i and 2i, the interaction term has
different impacts on i. For example, if both ‘religious belief’ and ‘job satis-
faction’ are positive, then their linear effects would have too strong a positive
effect on ‘life’ and need to be adjusted by a negative interaction term. If both
of them are negative, then their linear effect would have too weak a nega-
tive effect on ‘life’ and need to be further adjusted by a negative interaction
term. Finally, if one of them is negative and one of them is positive, then their
interaction would add a positive effect to ‘life’. Now, we consider the inter-
cepts and the means of the manifest variables. As an example, we consider the
mean of the second manifest variable, say Ey2. Since E1 = E2 = 0 ˆ2 =
7824 ˆ21 = 0869 ˆ3 = −0494 and ˆE12 = ˆ21 = −0110, it follows from
Equation (8.4) that

Ey2 = ˆ2 + ˆ21 ˆ3 ˆE12 = 7824+0869−0494−0110 = 7871
8.4
BAYESIAN ESTIMATION OF SEMs WITH NONLINEAR
COVARIATES AND LATENT VARIABLES
As we discussed in Section 5.4, a useful extension of the basic SEM is the
accommodation of covariates in the model for establishing better relationships,
or for prediction. In the measurement equation that is used for exploring
and/or identifying the latent variables, the inclusion of covariates is helpful in
providing a better relationship between the manifest and latent variables. As
the main purpose of the structural equation is to achieve accurate prediction or
relation among latent variables, the accommodation of covariates is even more
important. A covariate can be an explanatory variable such as age, gender, social
status, or other important variable that is defined by a single manifest variable
for relating or predicting the endogenous latent variables. It may come from
discrete and continuous distributions.
From the above discussion, we see that it is useful to incorporate linear
and nonlinear terms among exogenous latent variables and covariates in the
structural equation; especially for situations where some covariates that repre-
sent important predictors of the endogenous latent variables. In this section,
we formulate an NSEM which includes a measurement equation that is
defined with linear covariates and linear latent variables, and a structural
equation that is defined by a general vector-valued function that accom-
modates general nonlinear terms of both the covariates and exogenous
latent variables.

8.4
BAYESIAN ESTIMATION OF SEMs
221
8.4.1
A model with Ordered Categorical Variables and
Nonlinear Covariates and Latent Variables
The discussion in this section is based on the NSEM proposed in Song and
Lee (2006). In this model, the measurement Equation (8.1) is generalized as
follows:
yi = Aci +i +i
i = 1   n
(8.25)
where cim1 ×1 is a vector of covariates and Ap ×m1 is an unknown param-
eter matrix. Elements of ci could be constants or values that come from contin-
uous or discrete distributions. To assess the possible important causal effects of a
vector of covariates xim2×1 to i, the structural Equation (8.2) is generalized
as follows:
i = 	i +
Fxii+i
i = 1   n
(8.26)
where Fxii = f1xii   frxiiT is a nonzero vector-valued func-
tion with nonzero, known and differentiable functions f1   fr, and 
q1 ×r
is a matrix of unknown parameters. To cope with some situations in substan-
tive research, we allow that components in yi are either continuous or ordered
categorical. Identification of this model can be achieved via the methods given
in Sections 6.2 and 8.3.1.
Clearly, the structural equation with the nonlinear term 
Fxii is rather
general. A more concrete example of the general structural equation defined in
Equation (8.26) with i = i, i = i1i2T and xi = xi1xi2T is:
i =1xi1 +2xi2 +3xi1xi2 +1i1 +2i2 +3i1i2 +1xi1i1 +2xi1i2
+3xi2i1 +4xi2i2 +5xi1i1i2 +6xi2i1i2 +7xi12
i1 +8xi22
i2
+9xi1xi2i1i2 +
i
Here,

 = 123123123456789T
and
Fxii = xi1xi2xi1xi2i1i2i1i2xi1i1xi1i2xi2i1xi2i2xi1i1i2xi2
i1i2xi12
i1xi22
i2xi1xi2i1i2T . More complex product terms of elements
in xi and i can be assessed via appropriately defined structural equations. As
the covariates xi may come from any arbitrary distributions that give contin-
uous data, ordered or unordered categorical data, this NSEM can handle a
wide range of situations. Again, more care is needed to interpret the mean
vector of yi under the context of nonlinear SEM. Let AT
k
and T
k be the
kth rows of A and , respectively. For k = 1··· p, it follows from Equa-
tion (8.25) that Eyik = AT
k ci +T
k Ei. Although Ei = 0, it follows from
Equation (8.26) that Ei ̸= 0 if Fxii is a nonlinear function of i. This

222
8
NONLINEAR STRUCTURAL EQUATION MODELS
implies that Ei ̸= 0 and Eyik ̸= AT
k ci. Let T
k = T
kT
k be a partition
of T
k that corresponds to the partition of i = T
i T
i T . Because Ei = 0
and i = I−	−1
Fxii+i	, it follows from Equation (8.26) that
Eyik = AT
k ci +T
kEi+T
kEi = AT
k ci +T
kI−	−1
	EFxii	
As Fxii is usually not very complicated in most practical applications,
EFxii	 is not very complex, and the computation of Eyik is not difficult.
Treating a dichotomous variable as an ordered categorical variable with two
categories and a fixed threshold at zero, Bayesian estimates of the parameters
in the above NSEM can be obtained by a modified Gibbs sampler algorithm
as described in Section 7.3, with additional modifications to take care with the
covariates. Because the full conditional distributions have been presented in
Song and Lee (2006), or can be similarly derived as in previous sections, they
are not reported here.
8.4.2
Illustrative Example 3
A portion of the data that were obtained from the UK in the project World
Values Survey 1981–1984 and 1990–1993 was analyzed in this illustrative
example, see Song and Lee (2006). Seven variables related to respondents’
homelife, job satisfaction, and their attitude on job benefit and working envi-
ronment were taken as manifest variables. As covariates are less important in the
measurement equation, only the intercept A = a1··· a7T with all ci = 10 is
considered in defining Equation (8.25). For the structural equation, a binary
variable x that measures whether respondents can get comfort and strength
from religion is incorporated as a covariate. Details of these variables ( variables
180, 96, 116, 117, 99, 100 and 101) are given in Appendix 1.1. For brevity,
data points with missing entries were deleted, and the remaining sample size is
816. From the meaning of the corresponding questions, the first two manifest
variables are indicators for the latent variable ‘home life’, the next two mani-
fest variables are indicators for the latent variable ‘job satisfaction’, and the
last three manifest variables are indicators for the latent variable ‘ job benefit–
environment attitude’. The manifest variables for ‘homelife’ and ‘job satisfac-
tion’ were measured by a 10-point scale and they are treated as continuous.
The remaining three manifest variables were dichotomous. For clear interpre-
tation, we related these seven manifest variables with three latent variables via a
 with the following non-overlapping structure:
T =
⎡
⎣
1
21
0
0
0
0
0
0
0
1
42
0
0
0
0
0
0
0
1
63
73
⎤
⎦

8.4
BAYESIAN ESTIMATION OF SEMs
223
where 1’s and 0’s are fixed to identify the model. Parameters in  are
112122. The covariance matrix   is taken to be a diagonal matrix, with
diagonal elements 12 34111, in which 123 and 4 are
unknown parameters, and the 1’s are fixed to identify the dichotomous vari-
ables. As an illustration, we considered ‘homelife’ as the endogenous latent
variable , which is related with exogenous latent variables ‘job satisfaction,
1’, ‘job benefit–environment attitude, 2’, and the covariate x is about religion
(variable 177). The following nonlinear structural equation is used to assess the
interaction effects from the covariate x and the exogenous latent variables:
i = xi +1i1 +2i2 +1xii1 +2xii2 +3i1i2 +
i
(8.27)
In this model, there are 25 unknown parameters. The path diagram describing
the proposed NSEM is presented in Figure 8.1.
The Bayesian estimates of the parameters were obtained with conjugate prior
distributions. The hyperparameter values were produced by data-dependent
prior inputs as in Section 8.3.2. The convergence of the Gibbs sampler was
monitored by the EPSR values (see Gelman, 1996). To give some idea about the
convergence, plots of these values against the iteration number are displayed in
Figure 8.2. We observe that the sequence of observations converged after about
12 000 iterations. To obtain the Bayesian estimates and the numerical standard
errors, an additional T = 8000 observations were collected after convergence.
The Bayesian estimates, and the standard error estimates are presented in
Table 8.11. Some interesting interpretations are given below.
y3
y4
x
y5
y6
y7
y2
y1
3
4
5
6
7
ξ1
ξ2
η
x ξ1
x ξ2
ξ1ξ2
1
2
δ
α2
β3
1.0
γ
λ42
λ73
λ 21
λ63
1.0
1.0
β1
β2
α1
Figure
8.1
Path diagram of the SEM with interactions of covariates and latent vari-
ables. This figure and Figure 8.2 are taken from Song and Lee (2006).

224
8
NONLINEAR STRUCTURAL EQUATION MODELS
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
× 104
Figure
8.2
EPSR values against the number of iterations in the analysis of ICPSR
data.
Table
8.11
Bayesian estimates (EST) and the standard error
estimates (SE) of illustrative example 3.
Par
EST
SE
Par
EST
SE
a1
8120
006

−0197
010
a2
7457
006
1
0621
009
a3
7425
007
2
−0991
027
a4
6972
009
1
0064
008
a5
−0755
006
2
−0228
025
a6
−0462
005
3
0603
015
a7
0989
007
21
0908
008
11
2489
006
42
0768
009
12
0186
006
63
0644
017
22
0355
007
73
0894
022
1
1120
018

0718
019
2
1391
015
3
1625
026
4
4869
029
This table is taken from Song and Lee (2006).

8.4
BAYESIAN ESTIMATION OF SEMs
225
(i)
All of the non-fixed factor loading estimates are quite large. From the stan-
dard error estimates, it can be seen that these factor loadings are different
from zero. These results indicate a strong association between each of
the latent variables and their respective indicators. As expected, the stan-
dard errors of the loading estimates that correspond to the dichotomous
manifest variables are relatively large.
(ii)
The estimated nonlinear structural equation is given by
i =−0197xi +0621i1 −0991i2 +0064xii1 −0228xii2
+0603i1i2
(8.28)
From the standard error estimates, we observe that all the linear effects of
the covariate and the exogenous latent variables, and the interaction effects
corresponding to i1i2 and xii2 are quite different from zero, whilst the
interaction effect corresponding to xii1 is small. Hence, the interaction
causal effect of religion xi and ‘job satisfaction, i1’ is not significant when
given the other linear and interaction effects in the structural equation.
As i2 is related to the dichotomous variables, the standard error estimate
= 0250 corresponding to
ˆ2= −0228 is quite large. However, as
−0228 is quite different from zero in magnitude, we tend to believe that
the corresponding interaction effect is substantial.
(iii)
Before giving more detailed interpretations of the nonlinear interaction
terms, we note from the scales of the indicators in relation to ‘life, ’ and
‘job satisfaction, 1’ that a comparatively larger (positive) value of  or 1
implies that the individual has better ‘life’ or ‘job satisfaction’, respectively.
From the scale of the indicators in relation to ‘job benefit–environment
attitude, 2’, a comparatively smaller (negative) value of 2 implies that the
individual is more concerned about benefits and working environment.
With the above understanding of the exogenous latent variables, it follows
from ˆ1 = 0621 and ˆ2 = −0991 that job satisfaction and more concern
about benefits and working environment have a positive impact on ‘life,
’. From ˆ3 = 0603, ‘job satisfaction, 1’, and ‘job benefit–environment
attitude, 2’ have an interaction effect on ‘life, ’. The basic interpretation
is that the ‘additive’ effect of the linear latent variables of 1 and 2 in
the structural equation is inadequate to account for their relationships
with the latent variable ‘life, ’, and an interaction term of 1 and 2 has
to be added. Depending on various situations, this interaction term has
different effects. For example, it is interesting to observe that for employees
with good job satisfaction (positive i1), and more concern about benefits
and working environment (negative i2) would have an additive positive
effect on ‘life, ’ which is too strong; hence a negative adjustment via an
interaction effect (0.603 times a positive i1 and a negative i2 would be
negative) is necessary. For the covariate about religion, from the coding

226
8
NONLINEAR STRUCTURAL EQUATION MODELS
of the corresponding question, we note that an individual with a small
positive (or negative) value of xi has more comfort and strength from
religion, and vice versa. From ˆ= −0197, we observe the expected
finding that individuals who can gain comfort and strength from religion
tend to have a better life. The interpretation about the estimate of the
interaction term  ˆ2 = −0228 of xi and i2 is similar to the interpretation
of ˆ3.
(iv)
From ˆ11, ˆ12 and ˆ22, we note that the estimate of the correlation
between ‘job satisfaction, 1’ and ‘job benefit–environment attitude, 2’ is
0.198. It indicates that job satisfaction and less concern about the benefit
and environment are positively correlated.
(v)
Now, we consider the interpretation of the intercepts and the means of the
manifest variables. As an example, we consider 
Eyi2. Since E1 = E2 =
0 ˆa2 = 7457 ˆ21 = 0768 ˆ = −0197 ˆ3 = 0603 and ˆE12 = ˆ12 =
0186, we have

Eyi2 = ˆa2 + ˆ21ˆxi + ˆ3 ˆEi1i2	 = 7543−0151xi
(8.29)
This provides an estimate of the mean value of the second manifest variable
of the individual i, which depends on the value of the covariate xi. Based
on the meaning of y2 and x, the interpretation of Equation (8.29) is that
the mean life satisfaction is increased by perceived comfort and strength
from religion.
8.4.3
Analysis of an Artiﬁcial Example using WinBUGS
The software WinBUGS (Spiegelhalter, Thomas, Best and Lunn, 2003) can
produce Bayesian estimates of the parameters in an NSEM with covariates for
continuous, ordered categorical and/or dichotomous data. As we mentioned in
Section 6.6.2, for models with ordered categorical data, it is rather difficult to
simultaneously estimate the unknown thresholds and the unknown structural
parameters; and a convenient way is to fix the thresholds at some preassigned
values. For simplicity, we illustrate the use of WinBUGS in analyzing an artificial
example that is based on the following NSEM with a linear covariate (see Lee,
Song and Tang, 2006) Let yik
D= Nikik	, where
i1 =1 +i ik = k +k1i k = 23
i4 =4 +i1 ik = k +k2i1 k = 567 and
i8 =8 +i2 ik = k +k3i2 k = 910
(8.30)

8.4
BAYESIAN ESTIMATION OF SEMs
227
and the structural equation is reformulated by defining the conditional distri-
bution of i given i1 and i2 as N"i
	, where
"i = 1xi +1i1 +2i2 +3i1i2 +42
i1 +52
i2
(8.31)
The true population values of the free parameters in the model were taken to be:
1 =··· = 10 = 00 21 = 52 = 93 = 0931 = 62 = 103 = 07
72 =051 = 2 = 3 = 034 = ··· = 7 = 05
8 = 9 = 10 = 04
(8.32)
1 =05 1 = 2 = 04 3 = 03 4 = 02 5 = 05 and
11 =22 = 10 12 = 03 and 
 = 036
Based on the model formulation and these true parameter values, a random
sample of continuous observations yii = 1   500 was simulated, which
gave the observed data set Y. The following hyperparameter values were taken
for the conjugate prior distributions:
a0 = 00   00T  0 = I10 0k = 0
 = 9 0k = 0
 = 4
elements in 0k and 
0 are taken to be the true values,
H0yk = I10 H0k = I5 0 = 4 R0 = −1
0 
(8.33)
where 0 is the matrix with true values of 1122 and 12. These
hyperparameter values represent accurate prior inputs. We observe that the
WinBUGS program converged in less than 4000 iterations. The plots of some
of the simulated sequences of observations for monitoring convergence are
presented in Figure 8.3. Bayesian estimates of the parameters and their stan-
dard error estimates as obtained from 6000 iterations after the 4000 ‘burn-in’
iterations are presented in Table 8.12. We observe that the Bayesian esti-
mates are close to the true values, and that the standard error estimates are
reasonable. WinBUGS also produced estimates of the latent variables  ˆi =
ˆi ˆi1 ˆi2Ti = 1   n. Histograms that correspond to the sets of latent
variable estimates ˆi1 and ˆi2 are presented in Figure 8.4. The QQ plots are
presented in Figure 8.5. We observe from these histograms that the corre-
sponding empirical distributions are close to normal. The elements in the sample
covariance matrix of ii = 1   n are s11 = 0902s12 = 0311 and s22 =
0910, and thus this sample covariance matrix is close to the true covariance
matrix of i, see Equation (8.32).

228
8
NONLINEAR STRUCTURAL EQUATION MODELS
alpha[1] chains 1:3
1
2500
7500
10000
–0.5
0.0
0.5
1.0
1.5
5000
iteration
lambda[2,1] chains 1:3
iteration
1
2500
5000
7500
10000
–0.5
0.0
0.5
1.0
gamma[1] chains 1:3
iteration
1
2500
5000
7500
10000
–0.5
0.0
0.5
1.0
phi[1,2] chains 1:3
iteration
1
2500
5000
7500
10000
–2.0
–1.0
0.0
1.0
psiepsilon[4] chains 1:3
iteration
1
2500
5000
7500
10000
0.0
1.0
2.0
3.0
4.0
5.0
Figure
8.3
(a), (b), (c), (d) and (e) Three chains of observations corresponding to
121112 and 4, generated by different initial values.

8.4
BAYESIAN ESTIMATION OF SEMs
229
Table
8.12
Bayesian estimates of the artificial example obtained from WinBUGS.
Par
True value
EST
SE
Par
True value
EST
SE
1
00
0022
0069
1
03
0.324
0032
2
00
0065
0062
2
03
0.285
0027
3
00
0040
0052
3
03
0.284
0022
4
00
0003
0058
4
05
0.558
0050
5
00
0036
0056
5
05
0.480
0045
6
00
0002
0047
6
05
0.554
0041
7
00
0004
0042
7
05
0.509
0035
8
00
0092
0053
8
04
0.382
0035
9
00
0032
0050
9
04
0.430
0035
10
00
−0000
0044
10
04
0.371
0029
21
09
0889
0022
1
05
0.525
0075
31
07
0700
0019
1
04
0.438
0059
52
09
0987
0053
2
04
0.461
0034
62
07
0711
0046
3
03
0.304
0045
72
05
0556
0040
4
02
0.184
0060
93
09
0900
0042
5
05
0.580
0050
103
07
0766
0038
11
10
1.045
0120
12
03
0.302
0057
22
10
1.023
0089

036
0.376
0045
Inspired by the data analysis techniques in regression, we obtain the following
estimated residuals by means of the ˆ and ˆi = ˆi ˆi1 ˆi2T for i = 1   n
ˆi1 = yi1 −ˆ1 −ˆi ˆik = yik −ˆk −ˆk1 ˆi k = 23
ˆi4 = yi4 −ˆ4 −ˆi1 ˆik = yik −ˆk −ˆk2ˆi1 k = 567
ˆi8 = yi8 −ˆ8 −ˆi2 ˆik = yik −ˆk −ˆk3ˆi2 k = 910
ˆ
i = ˆi −ˆ1xi −ˆ1ˆi1 −ˆ2ˆi2 −ˆ3ˆi1ˆi2 −ˆ4ˆ2
i1 −ˆ5ˆ2
i2
Some estimated residual plots, ˆi2, ˆi3, ˆi8 and ˆ
i, against the case number are
displayed in Figure 8.6. And the plots of estimated residual ˆ
i versus ˆi1 and ˆi2;
and ˆi2 versus ˆi1, ˆi2 and ˆi are presented in Figures 8.7 and 8.8, respectively.
The other residual plots are similar. These plots lie within two parallel horizontal
lines that are centered at zero, and no linear or quadratic trends are detected.
This roughly indicates that the proposed measurement equation and structural
equation are adequate. The WinBUGS codes and data are given in the website:
http://www.wiley.com/go/lee_structural.

230
8
NONLINEAR STRUCTURAL EQUATION MODELS
–3
–2
–1
0
1
2
3
0
20
40
60
80
100
0
20
40
60
80
100
(a)
–3
–2
–1
0
1
2
3
(b)
Figure
8.4
Histograms of the latent variables (a) ˆi1 and (b) ˆi2.
8.5
BAYESIAN MODEL COMPARISON
In this section, we address the important issue about model comparison. For
NSEMs, the nonlinear terms of latent variables in the structural equation are
of particular interest. To assess the significance of the unknown coefficient
associated with a nonlinear term, the classical non-Bayesian approach uses the
z-score that is obtained by the estimate divided by its standard error estimate.
Due to the disadvantages of this approach as given in Chapter 5, Section 5.1,
we recommend that this issue should be addressed through Bayesian model
comparison using the Bayes factor. Basically, in assessing the significance of
one or more than one nonlinear terms, we compare a nonlinear SEM with the
nonlinear terms with an SEM without the nonlinear terms. The next section
describes the path sampling procedure for computing the Bayes factor.

8.5
BAYESIAN MODEL COMPARISON
231
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
3
(a)
Theoretical Quantiles
Sample Quantiles
−3
−2
−1
0
1
2
(b)
Theoretical Quantiles
Sample Quantiles
Figure
8.5
QQ plots of the latent variables (a) ˆi1 and (b) ˆi2
8.5.1
Path Sampling for Computing the Bayes Factor
We first consider the situation with continuous data. Let M0 and M1 be two
competing models under the framework of an NSEM defined in Equations (8.1)
and (8.2); and let B10 be the Bayes factor for comparing M0 and M1 as defined

232
8
NONLINEAR STRUCTURAL EQUATION MODELS
hat{epsilon}_i2
(a)
0
100
200
300
400
500
(b)
0
100
200
300
400
500
(c)
0
100
200
300
400
500
(d)
0
100
200
300
400
500
–2
–1
0
2
1
hat{epsilon}_i3
–2
–1
0
1
2
hat{epsilon}_i8
–2
–1
0
1
2
–2
–1
0
1
2
hat{delta}_i
Figure
8.6
Estimated residual plots (a) ˆi2, (b) ˆi3, (c) ˆi8 and (d) ˆ
i.
in Chapter 5. Let Y = y1   yn be the observed data matrix and  =
1   n be the matrix of latent variables. Let log pYt be the log
likelihood function with a continuous parameter t in [0, 1] and
UYt = d
dt log pYt
(8.34)

8.5
BAYESIAN MODEL COMPARISON
233
(a)
hat{delta}_i
–2
–1
0
1
2
3
–2
–1
0
1
2
(b)
hat{delta}_i
–3
–2
–1
0
1
2
–2
–1
0
1
2
Figure
8.7
Plots of estimated residuals ˆ
i versus (a) ˆi1 and (b) ˆi2.
(a)
hat{epsilon}_i2
–2
–1
0
1
2
3
–2
–1
0
1
2
–2
–1
0
1
2
–2
–1
0
1
2
hat{epsilon}_i2
(b)
–3
–2
–1
0
1
2
(c)
hat{epsilon}_i2
–2
0
2
4
6
8
Figure
8.8
Plots of estimated residuals ˆi2 versus (a) ˆi1, (b) ˆi2 and (c) ˆi.

234
8
NONLINEAR STRUCTURAL EQUATION MODELS
Further, let t0 = 0 < t1 < ··· < tS < tS+1 = 1 be fixed grids in [0, 1]. It
follows from the reasoning in Section 5.3 that an estimate of logarithm B10
obtained via path sampling is given by

log B10 = 1
2
S
s=0
ts+1 −tsU s+1 +U s
(8.35)
where U s is the average of the values of UYt based on all simulated
draws for which t = ts, that is,
U s = J −1
J
j=1
UYjjts
(8.36)
in which jjj = 1   J are simulated draws from pYts via
the algorithm developed in the estimation of the model.
To give a more specific illustration in applying the path sampling proce-
dure to NSEMs, consider the following models M1 and M0 which satisfy the
same measurement Equation (8.1), but with the following different nonlinear
structural equations for the latent variables:
M1   =	+
1H1+
M0   =	+
0H0+
where H1 and H0 may involve different numbers of distinct nonlinear functions
of , and 
1 and 
0 are the corresponding unknown coefficient matrices. In
this illustration, differences between M1 and M0 are on the more interesting
nonlinear relationships among the latent variables. The competing models M1
and M0 are linked up by a model Mt which is defined by the same measurement
equation, and the following structural equation with t in [0, 1]:
Mt   = 	+1−t
0H0+t
1H1+ = twG+
where t = 	1 −t
0t
1 and G = T H0T H1T T . It can
be shown that
logpYt =−1
2 p +qn log2+n log  +n log  
+n log 
−2n log 	0 +
n
i=1
T
i −1i

8.5
BAYESIAN MODEL COMPARISON
235
+
n
i=1
yi −−iT  −1
 yi −−i
+
n
i=1
i −tGi	T  −1

i −tGi	
Here,  is the parameter vector in the linked model. It contains all the common
and distinct unknown parameters in M0 and M1; that is, unknown parameters
in  	
0
1,  and  
. By differentiation with respect to t, we have
UYt =
n
i=1
i −tGi	T  −1

 0Gi
where 0 = 0−
0
1.
Applications of this procedure to other special cases of the general model
are similar. The main computation involved in the applications is on gener-
ating observations jj from pYts for evaluating U s, see Equa-
tions (8.35) and (8.36).
Now consider the situation with mixed continuous and ordered categor-
ical data. The basic NSEM is the same as above. However, to cope with the
ordered categorical variables the notation for the measurement equation is
slightly different, see Equation (8.20). Note that the nonlinear structural equa-
tion is again given by Equation (8.2) or (8.3). Let M0 and M1 be two competing
models with observed data matrices X = x1   xn of continuous measure-
ments and Z = z1   zn of ordered categorical measurements. Let  be the
matrix of latent variables, and let Y = y1   yn be the latent continuous data
matrix that underlies the ordered categorical data matrix Z. The observed data
XZ are augmented with the latent data matrix Y and the latent variables
matrix  in the path sampling procedure for computing the Bayes factor.
Let  be the vector which contains all the unknown thresholds, and
UXZYt = d
dt log pXZYt
(8.37)
The log B10 can be estimated by Equation (8.35) with
U s = J −1
J
j=1
UXZjjjYjts
(8.38)
where jjjYjj = 1   J are simulated observations drawn
from pYXZts via the hybrid algorithm presented in Section 8.3
for the estimation.

236
8
NONLINEAR STRUCTURAL EQUATION MODELS
8.5.2
Illustrative Example 4
The example given in Section 8.2.5 on the basis of an NSEM with continuous
variables is used to illustrate the path sampling procedure in computing the
logarithm Bayes factor for model comparison. Let M0 be the model with the
measurement equation and the nonlinear structural equation given in Equa-
tions (8.14) and (8.15), respectively. The competing models are defined by the
same measurement equation and the following different structural equations:
M0   =11 +22 +32
1 +412 +52
2 +

(8.39)
M1   =11 +22 +32
1 +

(8.40)
M2   =11 +22 +

(8.41)
The corresponding linked models are:
Mt01   =11 +22 +32
1 +1−t412 +52
2+

Mt02   =11 +22 +1−t32
1 +412 +52
2+

Mt12   =11 +22 +1−t32
1 +

Clearly, when t = 0Mt01 = M0Mt02 = M0 and Mt12 = M1; when t = 1Mt01 =
M1Mt02 = M2 and Mt12 = M2. The log-likelihood functions corresponding to
the linked models are:
log p01Yt =−1
2

c∗+
n
i=1
T
i −1i
+
n
i=1
yi −−iT  −1
 yi −−i
+
n
i=1
i −1i1 −2i2 −32
i1 −1−t4i1i2 +52
i2	
−1

 i −1i1 −2i2 −32
i1 −1−t4i1i2 +52
i2	

log p02Yt =−1
2

c∗+
n
i=1
T
i −1i
+
n
i=1
yi −−iT  −1
 yi −−i
+
n
i=1
i−1i1 −2i2 −1−t32
i1 +4i1i2 +52
i2	

8.5
BAYESIAN MODEL COMPARISON
237
×−1

 i−1i1−2i2−1−t32
i1 +4i1i2 +52
i2	

log p12Yt =−1
2

c∗+
n
i=1
T
i −1i
+
n
i=1
yi −−iT  −1
 yi −−i
+
n
i=1
i −1i1 −2i2 −1−t32
i1	−1

×i −1i1 −2i2 −1−t32
i1	


where c∗is a constant that is equal to np +qlog2+log  +log 
+
log 	. Derivatives of these functions with respect to t are equal to:
d log p01Yt
dt
=−
n
i=1

i −1i1 −2i2 −32
i1 −1−t4i1i2 +52
i2

×−1

 4i1i2 +52
i2
d log p02Yt
dt
=−
n
i=1

i −1i1 −2i2 −1−t32
i1 +4i1i2 +52
i2

×−1

 32
i1 +4i1i2 +52
i2
and
d log p12Yt
dt
= −
n
i=1

i −1i1 −2i2 −1−t32
i1

−1

 32
i1
These derivatives give UYt for computing the logarithm Bayes factors,
see Equations (8.35) and (8.36).
In the path sampling procedure, we take S = 20 grids in 01	. Based on
the previous analysis of this example, for each ts, we take a burn-in phase of
2000 iterations in the MCMC algorithm, and 200 observations are collected
after convergence for computing the logarithm Bayes factor. To give some idea
about the sensitivity of the procedure with respect to prior inputs, three types
of hyperparameters in the conjugate prior distributions are considered. The first
type (Type I) is given by the hyperparameter values specified in Section 8.2.5,
with some information provided by the auxiliary estimation. For Type II and
Type III prior inputs, we take 0k = 0
 = 100k = 0
 = 100 = 8H0yk = I

238
8
NONLINEAR STRUCTURAL EQUATION MODELS
Table
8.13
Estimated
log
Bayes
factors
under different prior inputs.
Prior inputs
I
II
III
log B01
030
037
0.43
log B02
215
193
1.84
log B12
185
155
1.40
and H0k = I, as before, but the prior inputs on 0yk0k and R0 in Type II
and Type III are respectively equal to half and twice of the prior inputs given
in Type I. The estimated logarithm Bayes factors are presented in Table 8.13.
On the basis of the criterion for interpreting logarithm Bayes factors, both
nonlinear models M0 and M1, are better than the linear model M2; while the
more complex model M0 is barely better than M1. As a result, either M0 or M1
is selected. The results also indicate that the estimated logarithm Bayes factors
are not very sensitive to Type I, II and III prior inputs.
8.5.3
Model Comparison using DIC and WinBUGS
We use the artificial example discussed in Section 8.4.3 to illustrate model
comparison of nonlinear SEMs using the DIC criterion that can be obtained
from WinBUGS. The same random sample of observations simulated according
to the settings described in Section 8.4.3 is considered. Let MA be the true
model that is defined by Equation (8.30) and (8.31), with the true values
of Equation (8.32). Let MB be a competing model, which is a linear SEM
defined by the same measurement Equation (8.30) and the following structural
equation:
vi = 1xi +2i2 +3i3
Using WinBUGS, we find that the DIC values corresponding to MA and MB
are respectively equal to DICA = 108489, and DICB = 110075. As DICA is
less than DICB, the true nonlinear SEM is selected. This model comparison
result can be used to reject the null hypothesis H0  3 = 4 = 5 = 0, where
34 and 5 are the coefficients corresponding to the nonlinear terms of latent
variables in Equation (8.31).
8.5.4
Remarks on Goodness-of-ﬁt Assessment
For assessing the goodness-of-fit of a hypothesized linear SEM, we can compare
it with the saturated model Ms which is defined by yi =  + i, where i is

REFERENCES
239
distributed as N0	 with a generic covariance matrix  without any structure.
However, for assessing NSEMs, the saturated model cannot be represented by
Ms that is defined by the normal assumption. As a result, it is difficult to apply
the classical likelihood ratio test. In the Bayesian approach, the goodness-of-fit
of a posited NSEM can be assessed by the PP p-value (see Appendix 5.2 and
Section 6.3.4). For continuous data, this statistic can be computed through
Equation (A5.8) with the following #2 discrepancy variable
DYrep =
n
i=1
Y
rep
i
−−iT  −1
 Y
rep
i
−−i
where Y
rep
i
is a replication of Yi. For mixed continuous and ordered categorical
data, this statistic can be computed by the method as described in Section 6.3.4.
Finally, the conclusion could be cross-validated with the residual plots.
REFERENCES
Bagozzi, R. P., Baumgartner, H. and Yi, Y. (1992) State versus action orientation and
the theory of reasoned action: an application to coupon usage. Journal of Consumer
Research, 18, 505–517.
Bentler, P. M. (1983) Some contributions to efficient statistics for structural models:
specification and estimation of moment structures. Psychometrika, 48, 493–517.
Benter, P. M. (1992) EQS: Structural Equation Program Manual. Los Angeles: BMDP
Statistical Software.
Bentler, P. M. and Dudgeon, P. (1996) Covariance structure analysis: statistical practice,
theory and directions. Annual Review of Psychology, 47, 541–570.
Besag, J., Green, P., Higdon, D. and Mengersen, K. (1995) Bayesian computation and
stochastic systems. Statistical Science, 10, 3–66.
Browne, M. W. (1984) Asymptotically distribution-free methods in the analysis of covari-
ance structures. British Journal of Mathematical and Statistical Psychology, 37, 62–83.
Busemeyer, J. R. and Jones, L. E. (1983) Analysis of multiplicative combination rules
when the causal variables are measured with error. Psychological Bulletin, 93, 549–562.
Etezadi-Amoli, J. and McDonald, R. P. (1983) A second generation nonlinear factor
analysis. Psychometrika, 48, 315–342.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distribution and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Gelman, A. (1996) Inference and monitoring convergence. In W. R. Gilks, S. Richardson
and D. J. Spiegelhalter (eds), Markov Chain Monte Carlo in Practice, pp. 131–144.
London: Chapman and Hall.
Gelman, A., Meng, X. L. and Stern, H. (1996) Posterior predictive assessment of model
fitness via realized discrepancies. Statistica Sinica, 6, 733–759.
Gelman, A., Roberts, G. O. and Gilks, W. R. (1995) Efficient Metropolis jumping rules.
In J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith (eds), Bayesian
Statistics 5, pp. 599–607. Oxford: Oxford University Press.

240
8
NONLINEAR STRUCTURAL EQUATION MODELS
Geyer, C. J. (1992) Practical Markov chain Monte Carlo. Statistical Science, 7, 473–511.
Gilks, W. R., Richardson, S. and Spiegelhalter, D. J. (1996) Introducing Markov chain
Monte Carlo. In W. R. Gilks, S. Richardson and D. J. Spiegelhalter (eds), Markov
Chain Monte Carlo in Practice, pp. 1–19. London: Chapman and Hall.
Hastings, W. K. (1970) Monte Carlo sampling methods using Markov chains and their
application. Biometrika, 57, 97–109.
Hu, L., Bentler, P. M. and Kano, Y. (1992) Can test statistics in covariance structure
analysis be trusted Psychological Bulletin, 112, 351–362.
Jaccard, J. and Wan, C. K. (1995) Measurement error in the analysis of interaction
effects between continuous predictors using multiple regression: multiple indicator
and structural equation approaches. Psychological Bulletin, 117, 348–357.
Jonsson, F. Y. (1998) Modeling interaction and non-linear effects: a step by step LISREL
example. In R. E. Schumacker and G. A. Marcoulides (eds), Interaction and Nonlinear
Effects in Structural Equation Models pp. 17–42. Mahwah, NJ: Lawrence Erlbaum
Associates, Publishers.
Jöreskog, K. G. and Sörbom, D. (1996) LISREL 8: Structural Equation Modeling with the
SIMPLIS Command Language. Hove and London: Scientific Software International.
Jöreskog, K. G. and Yang, F. (1996) Nonlinear structural equation models: the Kenny–
Judd model with interaction effects. In G. A. Marcoulides and R. E. Schumacker
(eds), Advanced Structural Equation Modeling Techniques, pp. 57–88. Hillsdale, NJ:
Lawrence Erlbaum Associates, Publishers.
Kenny, D. A. and Judd, C. M. (1984) Estimating the nonlinear and interactive effects
of latent variables. Psychological Bulletin, 96, 201–210.
Klein, A. and Moosbrugger, M. (2000) Maximum likelihood estimation of latent inter-
action effects with the LMS method. Psychometrika, 65, 457–474.
Lee, S. Y. and Song, X. Y. (2003) Estimation and model comparison for a nonlinear
latent variable model with fixed covariates. Psychometrika, 68, 27–47.
Lee, S. Y. and Song, X. Y. (2004) Bayesian model comparison of nonlinear structural
equation models with missing continuous and ordinal categorical data. British Journal
of Mathematical and Statistical Psychology, 57, 131–150.
Lee, S. Y. and Zhu, H. T. (2000) Statistical analysis of nonlinear structural equation
models with continuous and polytomous data. British Journal of Mathematical and
Statistical Psychology, 53, 209–232.
Lee, S. Y. and Zhu, H. T. (2002) Maximum likelihood estimation of nonlinear structural
equation models. sl Psychometrika, 67, 189–210.
Lee, S. Y., Poon, W. Y. and Bentler, P. M. (1995) A two-stage estimation of struc-
tural equation models with continuous and polytomous variables. British Journal of
Mathematical and Statistical Psychology, 48, 339–358.
Lee, S. Y., Song, X. Y. and Poon, W. Y. (2004) Comparison of approaches in estimating
interaction and quadratic effects of latent variables. Multivariate Behavioral Research,
39, 37–67.
Lee, S. Y., Song, X. Y. and Tang, N. S. (2006) Bayesian methods for analyzing structural
equation models with covariates, interactions and quadratic latent variables. Structural
Equation Modeling, in press.
Li, F. et al. (1998) Approaches to testing interaction effects using structural equation
modeling methodology. Multivariate Behavioral Research, 33, 1–39.
Marsh, H. W., Wen, Z. and Hau, K. T. (2004) Structural equation models of latent
interaction: evaluation of alternative estimation strategies and indicator construction.
Psychological Methods. 9, 275–300.

REFERENCES
241
McDonald, R. P. (1962) A general approach to nonlinear factor analysis. Psychometrika,
27, 123–157.
Metropolis, N. et al. (1953) Equations of state calculations by fast computing machine.
Journal of Chemical Physics, 21, 1087–1091.
Mooijaart, A. and Bentler, P. (1986) Random polynomial factor analysis. In Diday, E.
et al. (eds), Data Analysis and Informatics, IV, pp. 241–250. Amsterdam: Elsevier
Science Publishers B. V. (North-Holland).
Ping, R. A. (1996) Estimating latent variable interactions and quadratics: the state of
this art. Journal of Management, 22, 163–183.
Schermelleh-Engel, K., Klein, A. and Moosbrugger, H. (1998) Estimating nonlinear
effects using a latent moderated structural equations approach. In R. E. Schumacker
and G. A. Marcoulides (eds), Interaction and Nonlinear Effects in Structural Equation
Models, pp. 203–238. Mahwah, NJ: Lawrence Erlbaum Associates, Publishers.
Schumacker, R. E. and Marcoulides, G. A. (1998) Interaction and Nonlinear Effects in
Structural Equation Models. Mahwah, NJ: Lawrence Erlbaum Associates, Publishers.
Song, X. Y. and Lee, S. Y. (2006) Bayesian analysis of structural equation models
with nonlinear covariates and latent variables. Multivariate Behavioral Research, 41,
337–365.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. (2003) WinBUGS User
Manual. Version 1.4. Cambridge, England: MRC Biostatistics Unit.
Wall, M. M. and Amemiya, Y. (2000) Estimation for polynomial structural equation
models. Journal of the American Statistical Association, 95, 929–940.
Wall, M. M. and Amemiya, Y. (2003) A method of moments technique for fitting
interaction effects in structural equation models. British Journal of Mathematical and
Statistical Psychology, 56, 47–63.
World Values Study Group (1994) World Values Survey 1981–1984 and 1990–1993.
ICPSR version. Ann Arbor, MI: Institute for Social Research (producer). Ann Arbor,
MI: Inter-university Consortium for Political and Social Research (distributor).
Zhu, H. T. and Lee, S. Y. (1999) Statistical analysis of nonlinear factor analysis models.
British Journal of Mathematical and Statistical Psychology, 52, 225–242.


9
Two-level Nonlinear
Structural Equation
Models
9.1
INTRODUCTION
Statistical methods described in previous chapters for analyzing various SEMs
assume that the available data are obtained from a random sample from a single
population. However, in many applications, the data may exhibit two possible
kinds of heterogeneity. The first kind is mixture data, which involve indepen-
dent observations that come from one of the K populations with different
distributions, and no information is available on which of the K populations an
individual observation belongs to. Although K may be known or unknown, it
is usually quite small. Mixture models are used to analyze this kind of heteroge-
neous data. Analysis of a mixture of SEMs will be postponed to the Chapter 11.
The second kind of heterogeneous data are drawn from a number of different
groups (clusters) with a known hierarchical structure. Examples may well be:
the drawing of random samples of patients from within random samples of
clinics or hospitals; individuals from within random samples of families; or
students from within random samples of schools. In contrast to the mixture
data, these hierarchically structured data usually involve a large number of
G groups, and the group membership of each observation can be specified
accurately. However, we allow that individuals within a group share certain
common influential factors and hence lead to correlated observations. Hence,
the assumption of independence among observed observations is violated when
dealing with this kind of data. Clearly, ignoring the correlated structure of the
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

244
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
data and analyzing them as observations from a single random sample give erro-
neous results. Moreover, it is also desirable to establish a meaningful model
for the between-groups levels, and study the effects of the between-groups
latent variables to the within-groups latent variables. Consequently, the need
for developing two-level models that take into consideration the correlated
structure of the data is well recognized in structural equation modeling. Statis-
tical methods that are based on the ML or its related approaches have been
developed, see McDonald and Goldstein (1989), Zhang and Lee (2001), Lee
and Poon (1998), Lee and Tsang (1999), Rabe-Hesketh et al. (2004), Lee
and Shi (2001) and Lee and Song (2005). Using a Bayesian approach, Song
and Lee (2004) developed MCMC methods for analyzing two-level nonlinear
models with continuous and ordered categorical data, and Lee and Tang (2006)
consider two-level nonlinear SEMs with cross-level effects. For reasons stated
in previous chapters, we will describe Bayesian methods for analyzing various
two-level SEMs in this chapter.
To provide a comprehensive framework for analyzing two-level models,
nonlinear structural equations are incorporated in the SEMs that are associ-
ated with within-groups and between-groups models. Moreover, the model
can accommodate mixed type of continuous and ordered categorical data. In
addition to Bayesian estimation, we will present a path sampling procedure to
compute the Bayes factor for model comparison. The generality of the model
is important for providing a comprehensive framework for model comparison
of different kinds of SEMs. For example, we can compare a two-level nonlinear
model with a two-level linear model. Again, the idea of data augmentation
is utilized. Here, the observed data are augmented with various latent vari-
ables at both the levels, and the latent continuous random vectors that underly
the ordered categorical variables. An algorithm that is based on the Gibbs
sampler and the Metropolis–Hastings (MH) algorithm is described for estima-
tion. Observations generated by this algorithm will be used in the path sampling
procedure in computing Bayes factor. Although we concentrate on a two-level
SEM, the methodology proposed can be extended to higher level SEMs. Finally,
an application of WinBUGS to two-level nonlinear SEMs will be discussed.
9.2
A TWO-LEVEL NONLINEAR SEM WITH MIXED
TYPE VARIABLES
Consider a collection of p-variate random vectors ugii = 1   Ng, within
groups g = 1   G. The sample sizes Ng may differ from group to group so
that the data set is unbalanced. At the first level we assume that, conditional
on the group mean vg, random observations in each group satisfy the following
measurement equation:
ugi = vg +vgi = vg +1g1gi +1gi
g = 1   G i = 1   Ng
(9.1)

9.2
A TWO-LEVEL NONLINEAR SEM WITH MIXED TYPE VARIABLES
245
where 1g is a p ×q1 matrix of factor loadings, 1gi is a q1 ×1 random vector
of latent factors, and 1gi is a p ×1 random vector of error measurements which
is independent of 1gi and is distributed as N0 1g, where  1g is a diagonal
matrix. Note that due to the existence of vgugi and ugj are not independent.
Hence, in the two-level SEM, the usual assumption on the independence of the
observations is violated. This induces some difficulty in the analysis. To account
for the structure at the between-groups level, we assume that the group mean
vg satisfies the following factor analysis model:
vg = +22g +2g
g = 1   G
(9.2)
where  is the vector of intercepts, 2 is a p ×q2 matrix of factor loadings, 2g
is a q2 × 1 vector of latent variables and 2g is a p × 1 random vector of error
measurements which is independent of 2g and is distributed as N 0 2,
where  2 is a diagonal matrix. Moreover, the first- and second-level measure-
ment errors are assumed to be independent. It follows from Equations (9.1)
and (9.2) that
ugi = +22g +2g +1g1gi +1gi
(9.3)
For assessing the interrelationships among the latent variables, latent vectors
1gi and 2g are partitioned as 1gi = T
1giT
1giT and 2g = T
2gT
2gT ,
respectively; where 1giq11 ×11giq12 ×12gq21 ×1 and 2gq22 ×1 are
latent vectors, with qj1 +qj2 = qj, for j = 12. The distributions of 1gi and 2g
are N 0	1g and N 0	2, respectively. The following non-linear structural
equations are incorporated in the between-groups and within-groups models
of the proposed two-level model:
1gi = 
1g1gi +1gH11gi+1gi and
(9.4)
2g = 
22g +2H22g+2g
(9.5)
where H11gi = h111gi     h1a1giT and H22g = h212g    
h2b2gT are vector-valued functions with nonzero differentiable known func-
tions h1k and h2k, and usually a ≥q12 and b ≥q22, 
1gq11 ×q11
2q21 ×q21
1gq11 × a and 2q21 × b are unknown parameter matrices, 1gi is a vector
of error measurements which is distributed as N 0 1g	, 2g is a vector
of error measurements which is distributed as N 0 2	 and  1g	 and  2	
are diagonal matrices. Due to the nonlinearity induced by H1 and H2, the
underlying distribution of ugi is not normal. In the within-groups structural
equation, we assume as usual that 1gi and 1gi are independent. Similarly,
in the between-groups structural equation, we assume that 2g and 2g are
independent. Moreover, we assume that the within-groups latent vectors 1gi
and 1gi are independent of the between-groups latent vectors 2g and 2g.

246
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
Hence, it follows from Equation (9.4) that 1gi is independent of 2g and 2g.
That is, this two-level SEM does not accommodate the effects of the latent
vectors in the between-groups level on the latent vectors in the within-groups
level. However, in the within-groups model or in the between-groups model,
nonlinear effects of the exogenous latent variables to the endogenous latent
variables can be assessed through Equations (9.4) and (9.5), and the hier-
archical structure of the data has been taken into account. As the functions
h1k in H11gi and h2k in H22g are rather general, the common interac-
tion and quadratic effects are their special cases. Practically, allowing nonlinear
relationships such as interaction and quadratic terms among latent variables
leads to models that more accurately represent reality. Furthermore, we assume
that I1 −
1g and I2 −
2 are nonsingular, and their determinants are respec-
tively independent of the elements in 
1g and 
2. The proposed two-level
SEM is not identified without imposing the identification restrictions. The
common method of fixing appropriate elements in 1g
1g1g2
2 and
2 at preassigned known values can be used to achieve an identified model. In
a similar way to the analysis of single-level SEMs, there are no necessary and
sufficient conditions to achieve identifiability, and the problem is approached
on a problem-by-problem basis. In a similar way to the analysis of nonlinear
SEMs, the choices of H11gi and H22g are not arbitrary (see discussion in
Section 8.2).
To accommodate mixed ordered categorical and continuous variables,
without loss of generality, we suppose that ugi = xT
giyT
giT , where xgi =
xgi1··· xgirT
is an observable continuous random vector, and ygi =
ygi1··· ygis T an unobservable continuous random vector. In a similar way to
the previous chapters, a threshold specification is used to model the observ-
able ordered categorical vector z = z1··· zsT with its underlying continuous
vector y = y1··· ysT as follows:
z =
⎡
⎢⎣
z1

zs
⎤
⎥⎦
if

1z1 < y1 ≤
1z1+1


szs < ys ≤
szs +1
(9.6)
where zk is an integral value in 01··· bk. In general, we let 
k0 = −,

bk+1 = . For the kth variable, there are bk + 1 categories which are defined
by unknown thresholds 
kj. Dichotomous variables are treated as an ordered
categorical variable with a single threshold that is fixed at zero. The link between
a dichotomous variable with its underlying continous variable y is given by
d = 1 if y > 0 and d = 0 if y ≤0
(9.7)
In general, the thresholds, mean and variance of an ordered categorical variable
can be identified through the method given in Chapter 6; and the mean and

9.3
BAYESIAN ESTIMATION
247
variance of a dichotomous variable can be identified through the method given
in Chapter 7.
Let  be the parameter vector that contains all the unknown structural param-
eters in 1g 1g
1g1g	1g 1g	2 22	2 and  2	, and  be the
parameter vector that contains all the unknown thresholds. The total number
of unknown parameters in  and  is usually large. In the following discussion,
we assume that the two-level nonlinear model defined by  and  is identified.
The above model subsumes a number of important models in the recent
developments of SEMs. For instance, the models discussed in Shi and Lee
(1998, 2000) are generalized in two aspects: one from linear models to
nonlinear models and the other from single-level models to two-level models.
The nonlinear SEMs proposed by Lee and Zhu (2000) are extended from
single-level to two-level. Despite its generality, the proposed two-level SEM
is defined by measurement and structural equations that describe the relation-
ships among the observed and latent variables at both levels by conceptually
simple regression models. Consider the following three major components of
the proposed model and the data structure: (1) a two-level model for hierar-
chically structured data, (2) discrete natures of the data, and (3) a nonlinear
structural equation in the within-group model. The first two components are
important for achieving correct statistical results. The last component is essential
for analyzing more complicated situations, as nonlinear terms of latent variables
have been found to be useful in establishing a better model. The between-
groups model is also defined with a nonlinear structural equation for generality.
In practice, situations might not often arise that require all the aforementioned
major components to fit the data. Still, the development is useful to provide
a general framework for analyzing the large number of its submodels. This is
particularly true from a model comparison perspective. For example, even if a
linear model is better than a nonlinear model in fitting a data set, such a conclu-
sion cannot be reached without the model comparison statistic under the more
general nonlinear model framework. For practical situations where G is not
large, it may not be worthwhile or practical to consider a complicated between-
groups model. Moreover, most two-level SEMs in the literature assume that
the within-groups parameters are invariant over groups.
9.3
BAYESIAN ESTIMATION
In this section, we consider the statistical analysis of the general two-level
nonlinear SEM described in Section 9.2. We first note that due to the presence
of vg (see Equation (9.1)), ugi and ugj within the gth group are correlated. To
cope with this dependence structure and to obtain independent observations
in the traditional ML approach that focused on the observed data likelihood,
one has to work with u∗
g = u∗T
g1    u∗T
gNg T , where u∗
gi = xT
gizT
giT which is a
mixed continuous and ordered categorical random observation. Because of the

248
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
discrete nature of the ordered categorical data and the nonlinearities in the first-
and second-level structural equations, lgu∗
g, the probability density function of
the high dimensional random vector u∗
g, involves high dimensional intractable
integrals. The observed data likelihood is equal to l1u∗
1···lGu∗
G. Maximizing
this function through the classical numerical methods such as the Newton–
Raphson algorithm is extremely complicated. Moreover, because u∗
1   u∗
G
have different dimensions, they are not identically distributed. The ML theory
that depends on the assumption of identically distributed observations may
not be directly applicable to draw statistical conclusions. For example, the
observed data log-likelihood should be used with great caution in testing the
goodness-of-fit of the hypothesized model.
Motivated by its various advantages, we propose the Bayesian approach for
analyzing the current two-level nonlinear SEM with mixed continuous, dichoto-
mous and/or ordered categorical data. As we have discussed in previous chap-
ters, recent MCMC methods in statistical computing for posterior simulation
greatly enhance the applicability of the Bayesian inference. Our basic strategy
is to augment the observed data with the latent data that come from the latent
variables and/or latent measurements, then MCMC tools are applied to simu-
late observations in the posterior analysis.
9.3.1
Posterior Simulation and Bayesian Estimates
Let Xg = xg1   xgNg and X = X1   XG be the observed continuous
data, and Zg = zg1   zgNg and Z = Z1   ZG be the observed ordered
categorical data. Let Yg = yg1   ygNg and Y = Y1   YG be the latent
continuous measurements associated with Zg and Z, respectively. The observed
data will be augmented with Y in the posterior analysis. Once Y is given, all
the data are continuous and the problem will be easier to cope with. Let V =
v1   vG be the matrix of between-group latent variables. If V is observed,
the model is reduced to the single-level multi-sample model. Moreover, let 1g =
1g1   1gNg1 = 11   1G and 2 =
21   2G be the
matrices of latent variables at the within-groups and between-groups levels. If
these matrices are observed, the complicated nonlinear structural Equations (9.4)
and (9.5) reduce to the regular simultaneous regression models. Difficulties due
to the nonlinear relationships among the latent variables are greatly alleviated.
Hence, problems associated with the complicated components of the model,
such as the correlated structure of the observations induced by the two-level
data, the discrete nature of the ordered categorical variables and the nonlin-
earity of the latent variables at both levels, can be handled by data augmenta-
tion. In the posterior analysis, the observed data XZ will be augmented with
YV12, the hypothetically missing data matrices of latent measurements
and variables. More specifically, we will consider the joint posterior distribution
YV12XZ. The Gibbs sampler (Geman and Geman, 1984) will be

9.3
BAYESIAN ESTIMATION
249
used for generating a sequence of observations from this joint posterior distribu-
tion. Then the Bayesian solution is obtained by standard inferences on the basis of
thegeneratedsampleofobservations.InapplyingtheGibbssampler,weiteratively
sample from the following conditional distributions: VY12XZ,
1YV2XZ, 2YV1XZ, YV12XZ
and YV12 XZ.
For
the
proposed
two-level
model,
the
conditional
distribution
YV12XZ is further decomposed into components involving
various structural parameters in the between-groups and within-groups models.
These components are different under various special cases of the model. Some
typical examples are:
(a)
Models with different within-groups parameters across groups: in this case,
the within-groups structural parameters 1g = 1g  1g 
1g 1g 	1g
 1g	 and threshold parameters g associated with the gth group are
different from those associated with the hth group, for g ̸= h. Practically,
G and Ng should not be too small for drawing valid statistical conclusions
for the between-group model and the gth within-groups model.
(b)
Models with some invariant within-groups parameters: in this case, param-
eters 1g and/or g associated with the gth group are equal to those
associated with some other groups.
(c)
Models with all invariant within-groups parameters: under this situation,
11 = ··· = 1G, and 1 = ··· = G.
Conditional distributions under various special cases are similar but different.
Moreover, prior distributions of the parameters are also involved. On the basis
of the reasoning given in previous chapters, conjugate type prior distribu-
tions are used. The non-informative distribution is used for the prior distri-
bution of the thresholds. The conditional distributions of the components in
YV12XZ as well as other conditional distributions required
by the Gibbs sampler are briefly discussed in Appendix 9.1. As we can see
in this appendix, these conditional distributions are generalizations of those
that are associated with a single-level model, and most of them are stan-
dard distributions such as normal, univariate truncated normal, gamma and
inverted Wishart. Simulating observations from them requires little computing
time. The MH algorithm will be used for simulating observations efficiently
from the three complicated conditional distributions: 1YV2XZ,
2YV1XZ and YV12XZ. Some technical details
on the implementation of the MH algorithm are given in Appendix 9.2.
As in previous chapters, observations obtained from the posterior simulation
can be used for various statistical inferences via some data analysis methods.
Bayesian estimates of  and latent variables 2g and 1gi at both levels can be
obtainedeasilyviathecorrespondingsamplemeansofthegeneratedobservations.
Specifically, let tt
t
1 
t
2 t = 1   T ∗ be random observations

250
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
generated from the joint posterior distribution pYV12XZ,
then joint Bayes estimates of 2g and 1gi are obtained as follows:
ˆ = 1
T ∗
T ∗

t=1
t ˆ = 1
T ∗
T ∗

t=1
t ˆ2g = 1
T ∗
T ∗

t=1

t
2g ˆ1gi = 1
T ∗
T ∗

t=1

t
1gi
(9.8)
where 
t
2g and 
t
1gi are from 
t
2 and 
t
1g, respectively. These joint Bayesian
estimates tend to their corresponding posterior means in probability as T tends
to infinity. Since we have a large sample of  from its posterior distribution, an
estimate of Var can be obtained easily from the sample covariance matrix.
Moreover, estimated residuals ˆ1gi, ˆ2g, ˆ1gi and ˆ2g can be obtained by means
of the parameter estimates, the latent variable estimates, and the estimates of
the unobserved continuous measurements that underly the dichotomous or
ordered categorical data.
9.3.2
Simulation Studies
The objectives of the two simulation studies presented in this section are to
reveal the performance of the Bayesian approach and its associated algorithm
in recovering the true parameters, and roughly to examine the sensitivity of
the parameter estimates to different prior inputs of the hyperparameters in the
conjugate prior distributions. The first study is concerned with a model with
invariant within-groups parameters, while the second study is focused on a
different model with certain distinct parameters across some within-groups.
In the first simulation study, random vectors ugi satisfied the two-level model
defined by Equations (9.1) and (9.2) are considered with the following speci-
fications: 1g = 1 and  1g =  1, where
T
1 =
⎡
⎣
10∗
08
08
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
10∗
08
08
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
10∗
08
08
⎤
⎦
and  1 = 08I9 = 0   0T 2 has the same structure as 1 except the
true values are all 0.6 instead of 0.8, and  2 = 04I9. As usual, parameters with
an asterisk are fixed at the preassigned values. On the basis of the structures in
1 and 2, there are three latent variables in the within-groups and between-
groups models. These latent variables are denoted as 1gi1gi11gi2 and
2g2g12g2, respectively. The latent variables are respectively related by the
following structural equations:
1gi =111gi1 +121gi2 +132
1gi1 +141gi11gi2 +152
1gi2 +	1gi
(9.9)
2g =212g1 +222g2 +	2g
(9.10)

9.3
BAYESIAN ESTIMATION
251
and
the
following
specifications:
1g = 1 = 11   15 = 0505
080808 and 2 = 2122 = 0606. True values of parameters in
the covariance matrices 	1g = 	1,  1g	 = 1	, 	2 and 2	 are respectively
given by 111 = 122 = 10, 121 = 05, 1	 = 08, 211 = 222 = 10,
221 = 03 and 2	 = 07. The between-groups structural equation is linear,
and the within-groups structural equation is nonlinear with interaction and
quadratic terms of latent variables. Random observations ugi are simulated from
G = 180 groups according to Equations (9.1) and (9.2) with the following
unbalanced design: Ng = 10 for g = 1   60Ng = 15 for g = 61   120,
and Ng = 20 for g = 121   180. The last three continuous measure-
ments are transformed to ordered categorical data according to the following
true thresholds: g1 = g2 = g3 = 1 = 2 = 3 = −10∗−060610∗.
In the analysis, 27, 28 and 29 that correspond to the ordered categor-
ical variables are fixed at 0.4. There are a total of 57 parameters in this
model.
In the Bayesian estimation, prior distributions are selected from the conjugate
families as given in Appendix 9.1. As the within-groups parameters are invariant
over groups, their prior distributions are given by Equation (A9.8). To give a
sensitivity analysis about the prior inputs of the hyperparameters in the prior
distributions, we perturb the prior inputs as follows:
Type I: 0 = 02J9, where J9 is a 9×1 vector with elements 1001k, ∗
01k,
02k and ∗
02k are equal to half of the true population values, 
01k = 
02k =

01	k = 
02	k = 5, 01k = 02k = 01	k = 02	k = 4, 01 = 02 = 3, R −1
01 = R −1
02 =
50I2, H01k, H∗
01k, H02k, H∗
02k and 0 are equal to identity matrices of appro-
priate orders.
Type II: 0 = 20J9, 01k, ∗
01k, 02k and ∗
02k are equal to twice the true
population values, 
01k, 
02k, 
01	k, 
02	k, 01k, 02k, 01	k, 02	k, 01 and
02 are equal to four times the values given in Type I, while the other
hyperparameter values are the same as those given in Type I.
The Gibbs sampler coupled with the MH algorithm is used to produce
Bayesian estimates under the two prior inputs in 100 replications. In the MH
algorithm, we set 2 = 068 to give an approximate acceptance rate of 0.47.
The convergence of the algorithm is monitored by the ‘estimated potential scale
reduction (EPSR)’ values suggested by Gelman and Rubin (1992), and by plots
of simulated sequences of the individual parameters. At convergence, the EPSR
values should be less than 1.2 and the parallel sequences generated by different
starting values should be mixed well together. We initially conduct a few test
runs of the MCMC algorithm to decide the ‘burn-in’ phase and observe that it
converged in less than 1000 iterations. Hence, we take a burn-in-phase of 1000
iterations and collected T ∗= 4000 iterations after convergence to produce the

252
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
Bayesian estimates and the related statistics. The mean (MEAN) and the root
mean squares (RMS) are computed on the basis of the results obtained from
the 100 replications. Results are presented in Tables 9.1 and 9.2. These tables
show close agreement between the Bayesian estimates under the different prior
Table
9.1
Bayesian estimates of the structural parameters (Str. Par.) and
thresholds: first simulation study, Type I prior inputs.
Parameter
Within group
Parameter
Between group
MEAN
RMS
MEAN
RMS
Str. Par.
Str. Par.
121 = 08
0800
0007
221 = 06
0609
0048
131 = 08
0800
0007
231 = 06
0605
0048
152 = 08
0800
0024
252 = 06
0589
0066
162 = 08
0801
0024
262 = 06
0596
0064
183 = 08
0796
0038
283 = 06
0602
0071
193 = 08
0800
0035
293 = 06
0608
0077
11 = 05
0487
0064
21 = 06
0583
0104
12 = 05
0486
0058
22 = 06
0600
0101
13 = 08
0810
0057
21 = 04
0416
0080
14 = 08
0781
0099
22 = 04
0415
0057
15 = 08
0797
0089
23 = 04
0414
0048
11 = 06
0603
0028
24 = 04
0396
0065
12 = 06
0601
0025
25 = 04
0398
0050
13 = 06
0606
0023
26 = 04
0415
0051
14 = 06
0602
0024
	2 = 07
0706
0125
15 = 06
0603
0021
211 = 10
1067
0155
16 = 06
0607
0022
212 = 03
0322
0109
17 = 06
0612
0047
222 = 10
1094
0215
18 = 06
0606
0034
1 = 00
−0098
0160
19 = 06
0611
0033
2 = 00
−0064
0104
	1 = 08
0791
0032
3 = 00
−0064
0111
111 = 10
0995
0045
4 = 00
−0067
0115
112 = 05
0501
0037
5 = 00
−0039
0078
122 = 10
1015
0078
6 = 00
−0038
0084
7 = 00
−0053
0107
8 = 00
−0041
0079
9 = 00
−0037
0077
Thresholds

72 = −06
−0597
0023

73 = 06
0602
0023

82 = −06
−0600
0020

83 = 06
0601
0021

92 = −06
−0600
0021

93 = 06
0602
0021

9.3
BAYESIAN ESTIMATION
253
Table
9.2
Bayesian estimates of the structural parameters (Str. Par.) and
thresholds: first simulation study, Type II prior inputs.
Parameter
Within group
Parameter
Between group
MEAN
RMS
MEAN
RMS
Str. Par.
Str. Par.
121 = 08
0801
0007
221 = 06
0611
0048
131 = 08
0800
0007
231 = 06
0607
0046
152 = 08
0804
0025
252 = 06
0612
0068
162 = 08
0804
0024
262 = 06
0621
0066
183 = 08
0810
0039
283 = 06
0628
0083
193 = 08
0814
0038
293 = 06
0637
0090
11 = 05
0506
0065
21 = 06
0596
0106
12 = 05
0517
0061
22 = 06
0633
0113
13 = 08
0793
0064
21 = 04
0411
0046
14 = 08
0826
0100
22 = 04
0413
0044
15 = 08
0812
0084
23 = 04
0413
0037
11 = 06
0608
0028
24 = 04
0418
0047
12 = 06
0603
0024
25 = 04
0396
0038
13 = 06
0609
0023
26 = 04
0409
0038
14 = 06
0607
0025
	2 = 07
0736
0086
15 = 06
0604
0020
211 = 10
0943
0144
16 = 06
0610
0023
212 = 03
0289
0105
17 = 06
0620
0044
222 = 10
0944
0197
18 = 06
0612
0033
1 = 00
0099
0163
19 = 06
0618
0036
2 = 00
0068
0108
	1 = 08
0799
0061
3 = 00
0066
0117
111 = 10
0983
0048
4 = 00
0051
0107
112 = 05
0491
0038
5 = 00
0037
0078
122 = 10
0977
0077
6 = 00
0039
0084
7 = 00
0065
0115
8 = 00
0037
0079
9 = 00
0042
0081
Thresholds

72 = −06 -0.596
0023

73 = 06
0602
0022

82 = −06 -0.600
0020

83 = 06
0602
0021

92 = −06 -0.600
0022

93 = 06
0603
0021
inputs. Hence, under the given sample size, the proposed Bayesian estimation
is not sensitive to these two different prior inputs. We also observe the close
agreement between the true parameters and the means of Bayesian estimates,
and rather small RMS.

254
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
In the second simulation, a model with noninvariant within-group parameters
is considered. Here, G = 230. For g = 1   150, we took Ng = 8, and the
following true parameter values:
T
1g =
⎡
⎣
10∗
12
0∗
0∗
0∗
0∗
0∗
0∗
10∗
12
0∗
0∗
0∗
0∗
0∗
0∗
10∗
12
⎤
⎦ and  1g = 08I6
whilst for g = 151   230, we took Ng = 10,
T
1g =
⎡
⎣
10∗
06
0∗
0∗
0∗
0∗
0∗
0∗
10∗
06
0∗
0∗
0∗
0∗
0∗
0∗
10∗
06
⎤
⎦ and  1g = 036I6
Here, there are three latent variables: 1g1g1 and 1g2. These latent variables
are related by the following nonlinear structural equation
1gi = 1g11gi1 +1g21gi2 +1g31gi11gi2 +	1gi
(9.11)
where for g = 1   150, the true values of the parameters are 1g =
050506, 1	g = 08, 1g11 = 1g22 = 10 and 1g21 = 03; while for g =
151   230, 1g = 030304, 1	g = 036, 	1g is equal to the matrix
just given above. We consider a correlated factor analysis model with  = 0 in
the between-groups level. The structure of 2 is the same as the 1g specified
above except the true value of the free loading parameters is 0.8, the vari-
ances of the latent factors are all 0.7, the covariances among all pairs of factors
are 0.15, and the unique variances are 0.3. The fourth and sixth continuous
measurements of observations are transformed to ordered categorical through
the following thresholds: g1 = 1 = −10∗−050310∗ and g2 = 2 =
−10∗−030510∗. In the analysis, 24 and 26 are fixed at 0.3. There are
a total of 55 unknown parameters.
Based on the results obtained from the first simulation about the sensitivity of
prior inputs in Bayesian estimation, only one set of hyperparameter values was
considered. These values are given by: 0 = 00J6, 01gk, ∗
01gk, 02k and ∗
02k,
are the true population values, H01gk, H∗
01gk, H02k and H∗
02k are identity matrices
of appropriate orders, 
01gk = 
01g	k = 6, 01gk = 01g	k = 4, 
02k = 
02	k = 10,
02k = 02	k = 3, 01g = 02 = 6, R −1
01g = R −1
02 = 50I2. Again, 100 replications
are completed via the MCMC algorithm. The 2 in the MH algorithm is
taken to be 0.68, giving an approximate acceptance rate of 0.52. On the basis
of the results obtained from some test runs, we took a burn-in-phase of 500
iterations and collected 3000 observations after convergence to produce the
Bayesian solution. Results are presented in Table 9.3. Again, we observe close
agreement between the true parameters and the means of Bayesian estimates,
and small RMS.

9.4
GOODNESS-OF-FIT AND MODEL COMPARISON
255
Table
9.3
Bayesian estimates of the structural parameters (Str. Par.) and
thresholds: second simulation study.
Within-group 1, g = 1   150
Within-group 2, g = 151   230
Str. Par.
MEAN
RMS
Str. Par.
MEAN
RMS
1g21 = 12
1206
0.051
1g21 = 06
0.596
0.044
1g42 = 12
1214
0.089
1g42 = 06
0.628
0.051
1g63 = 12
1209
0.094
1g63 = 06
0.626
0.050
1g1 = 05
0500
0.053
1g1 = 03
0.305
0.038
1g2 = 05
0496
0.058
1g2 = 03
0.306
0.048
1g3 = 06
0608
0.056
1g3 = 04
0.420
0.056
1g1 = 08
0809
0.069
1g1 = 08
0.809
0.040
1g2 = 08
0783
0.098
1g2 = 08
0.783
0.024
1g3 = 08
0794
0.073
1g3 = 08
0.794
0.070
1g4 = 08
0814
0.104
1g4 = 08
0.814
0.035
1g5 = 08
0805
0.063
1g5 = 08
0.805
0.068
1g6 = 08
0803
0.098
1g6 = 08
0.803
0.031
	1g = 08
0777
0.086
	1g = 036
0.366
0.042
1g11 = 10
1004
0.087
1g11 = 10
0.956
0.094
1g12 = 03
0305
0.046
1g12 = 03
0.291
0.047
1g22 = 10
0998
0.089
1g22 = 10
0.965
0.091
Between-group
1 = 00
−0015
0.081
211 = 07
0.690
0.116
2 = 00
−0014
0.066
212 = 15
0.137
0.070
3 = 00
−0015
0.073
213 = 15
0.142
0.074
4 = 00
−0022
0.074
222 = 07
0.717
0.108
5 = 00
−0001
0.078
223 = 15
0.140
0.064
6 = 00
0000
0.068
233 = 07
0.722
0.111
221 = 08
0832
0.076
21 = 03
0.318
0.045
242 = 08
0812
0.092
22 = 03
0.296
0.037
263 = 08
0819
0.084
23 = 03
0.313
0.050
25 = 03
0.309
0.050
Thresholds

42 = −05
−0498
0.025

43 = 03
0.299
0.028

62 = −03
−0301
0.026

63 = 05
0.496
0.024
9.4
GOODNESS-OF-FIT AND MODEL COMPARISON
Assessing the goodness-of-fit of a hypothesized model is an important issue in
SEM. In the Bayesian approach, the posterior predictive (PP) p-value (Gelman,
Meng and Stern, 1996, see also Appendix 5.2) can be used as a goodness-
of-fit assessment for a hypothesized two-level nonlinear SEM with the mixed
type data. A brief description of the PP p-values in the context of the current

256
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
model is given in Appendix 9.3. As the PP p-value is not suitable for comparing
two competing models M0 and M1 (see Carlin and Louis, 1996), the following
Bayes factor (see Kass and Raftery, 1995) is used for comparing M0 and M1:
B10 = p XZM1
p XZM0
In general, it is well known that the computation of B10 is nontrivial. This
is particularly true for the current nonlinear two-level model which includes
a large number of parameters and latent measurements, and latent variables.
Inspired by the good features of path sampling (Gelman and Meng, 1998), we
will use it to compute B10.
In the application of path sampling in computing B10, we again use the
data augmentation idea to augment XZ with YV12 in the analysis.
Consider the following class of densities defined by a continuous parameter t
in [0,1]:
p YV12XZt = p YV12XZt
zt

where zt = pXZt. Let t in 01 be a parameter linking the competing
models M0 and M1 such that for a = 01, za = pXZt = a = pXZMa,
then B10 = z1/z0. Taking logarithms and differentiating zt with respect to
t, it can be shown by reasoning similar to that in Section 5.4 that:
log B10 = 1
2
S
s=0
ts+1 −ts ¯Us+1 + ¯Us
(9.12)
where t0 = 0 < t1 < ··· < tS < tS+1 = 1 are fixed grids in [0,1] and
¯Us = 1
J
J
j=1
UjjYjVj
j
1 
j
2 XZts
(9.13)
in which jjYjVj
j
1 
j
2   j=1, …, J } is a sample of observations
simulated from the joint posterior distribution YV12XZts,
and
U YV12XZt = d log pYV12XZt/dt
(9.14)

9.4
GOODNESS-OF-FIT AND MODEL COMPARISON
257
where pYV12XZt is the complete data likelihood. Note that
this complete data likelihood is not complicated and obtaining the function U
through differentiation is not difficult. Moreover, the program implemented
in estimation can be used for simulating observations in Equation (9.13),
hence there is little additional programming effort required. Usually, S = 10
grids is sufficient for providing a good approximation of the logarithm B10 for
competing models which are not far apart. More grids are required for very
different M1 and M0, and the issue should be approached on a problem-by-
problem basis. In Equation (9.13), a value of J = 2000 is usually enough for
most practical applications.
An important step in applying path sampling for computing logarithm B12
is to find a good path t in 01 to link the competing models M1 and M2.
Because the two-level nonlinear SEM is rather complex, M1 and M2 can be
quite different and finding a path to link them may require some insight. Two
illustrative examples are discussed as follows.
Example1 
Mk 
ugi = vg +k
11gi +1gi
k = 12
(9.15)
vg = +k
22g +2g
(9.16)
1gi = 
k
11gi +k
1Hk
11gi+1gi
(9.17)
2g = 
k
22g +k
2Hk
22g+2g
(9.18)
where the superscript k in the parameters and functions is used for denoting
the model Mkk = 12. M1 and M2 can be linked via the following Mt12 with t
in 01:
Mt12 
ugi = vg +t1
1 +1−t2
11gi +1gi
vg = +t1
2 +1−t2
22g +2g
1gi = t
1
1 +1−t
2
11gi +t1
1H1
11gi+1−t2
1H2
11gi+1gi
2g = t
1
2 +1−t
2
22g +t1
2H1
22g+1−t2
2H2
22g+2g
When t = 1, Mt12 reduces to M1; when t = 0, Mt12 reduces to M2. From
Equations (9.12) and (9.13), logarithm B12 can be computed. Hence, general
models involving different matrix coefficients in the measurement equa-
tions, and different forms of nonlinear structural equations, can be easily
compared.

258
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
Example 2: The competing models M1 and M2 have the following within-groups
measurement and structural equations which are defined in a similar way to
Equations (9.15) and (9.16):
ugi =vg +11gi +1gi
(9.19)
1gi =
11gi +1H11gi+1gi
(9.20)
The difference between M1 and M2 is on the between-groups models. Let
M1 
vg = +1
22g +2g
where 2g is distributed as N 0	2. Thus, the between-groups model in M1
is a factor analysis model. In M2, 2g = T
2gT
2gT , and the measurement and
structural equations in the between-groups model are given as follows:
M2 
vg =+2
22g +2g
(9.21)
2g =
2
22g +2
2H22g+2g
(9.22)
The between-groups model in M2 is an NSEM with a nonlinear structural
equation. Note that M1 and M2 are non-nested. As there are two different
models for 2g, it is rather difficult to directly link M1 and M2. This difficulty
can be solved via an auxiliary model Ma which can be linked with both M1
and M2. We first compute log B1a and log B2a, and then obtain log B12 via the
following equation:
log B12 = log pXZM1/pXZMa
pXZM2/pXZMa = log B1a −log B2a
(9.23)
For our current problem, one auxiliary model is Ma in which the measure-
ment and structural equations of the within-groups model are given by Equa-
tions (9.19) and (9.20), while the between-groups model is defined by vg =
 + 2g. The link model Mt1a is defined by Mt1a  ugi =  + t1
22g + 2g +
11gi +2gi, with within-groups structural equations given by Equation (9.20),
where 2g is distributed as N 0	 and without a between-groups structural
equation. Clearly, t = 1 and 0 corresponds to M1 and Ma, respectively. Hence,
log B1a can be computed under this setting via the path sampling procedure.
The link model Mt2a is defined by Mt2a  ugi = +t2
22g +2g +11gi +1gi,
with the within-groups and between-groups structural equations respectively
given by Equations (9.20) and (9.22). Clearly, t = 1 and 0 corresponds to M2

9.5
AN APPLICATION: FILIPINA CSWs STUDY
259
and Ma. Hence, log B2a can be obtained. Finally, log B12 can be obtained from
log B1a and log B2a via Equation (9.23).
In general, just one auxiliary model may not be adequate to link two very
different M1 and M2. However, based on the key idea of the above example,
the difficulty can be solved by using more than one appropriate auxiliary model
MaMb   in between M1 and M2. For example, suppose we use Ma and Mb
to link M1 and M2, with Ma closer to M1. Then
pXZM1
pXZM2 = pXZM1/pXZMa
pXZM2/pXZMa and
pXZM2
pXZMa = pXZM2/pXZMb
pXZMa/pXZMb
(9.24)
hence, log B12 = log B1a +log Bab −log B2b. Each logarithm Bayes factor can be
computed via path sampling.
Similar to the goodness-of-fit assessment in the context of single-level
nonlinear SEMs, it is rather difficult to find a saturated model for the two-
level nonlinear SEMs. However, the goodness-of-fit of a proposed model can
be assessed by means of the PP p-value, and the estimated residual plots.
9.5
AN APPLICATION: FILIPINA CSWs STUDY
As an illustration of the proposed methodology, we use a small portion of the
data set in the study of Morisky et al. (1998) on the effects of establishment
policies, knowledge and attitudes on condom use among Filipina commercial
sex workers (CSWs). It has been argued that the nature of commercial sex
work promotes the spread of AIDS and other sexually transmitted diseases;
thus promotion of safer sexual practice among CSWs is important. The study
of Morisky et al. (1998) concerned the development and preliminary findings
from an AIDS preventative intervention for Filipina CSWs. The data set was
collected from female CSWs in establishments (bars, night clubs, Karaoke TV
and massage parlours) in cities of the Philippines. The whole questionnaire
consisted of 134 items on areas of demographics knowledge, attitudes, beliefs,
behaviors, self-efficacy for condom use and social desirability. Latent psycho-
logical determinants such as CSWs’ risk behaviors, knowledge and attitudes
associated with AIDS and condom use are important issues to be assessed. For
instance, a basic concern is to explore whether linear relationships among these
latent variables are sufficient, or is it better to incorporate nonlinear relation-
ships in the model. The manifest variables that are used as indicators for latent
quantities are measured in terms of ordered categorical and continuous scales.
Moreover, as emphasized by Morisky et al. (1998), establishments’ policies on
their CSWs condom use practices exert a strong influence on CSWs. Hence,

260
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
it is interesting to study the influence of the establishment by incorporating a
between-group model for the data. As observations within each establishment
are correlated, the usual assumption of independence in the standard single-
level SEMs is violated. On the basis of the above considerations, it is desirable to
develop a two-level nonlinear SEM in the context of mixed ordered categorical
and continuous data.
Nine manifest variables, of which the seventh, eighth and ninth variables are
continuous and the remaining are ordered categorical with a five-point scale, are
selected. Questions corresponding to these variables are given in Appendix 9.4.
For brevity, we delete those observations with missing entries in the analysis,
and the remaining sample size is 755. There are 97 establishments. The numbers
of individuals in establishments varied from 1 to 58 which gives an unbalanced
data set. The sample means and standard deviations of the continuous vari-
ables are 244211800465 and 529922081590, respectively. The
cell frequencies of the ordered categorical variables are ranged from 12 to 348.
To unify scales of variables, the raw continuous data are standardized.
After some preliminary studies and based on the meanings of the questions
corresponding to the manifest variables (see Appendix 9.4), in the measurement
equations corresponding to the between-groups and within-groups models,
we use the first three, the next three, and the last three manifest variables as
indicators for latent factors that can be roughly interpreted as ‘worry about
AIDS’, ‘attitude to the risk of getting AIDS’ and ‘aggressiveness’. For the
between-groups model, we propose a factor analysis model with the following
specifications:
T
2 =
⎡
⎣
10∗
221
231
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
10∗
252
262
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
10∗
283
293
⎤
⎦
	2 =
⎡
⎣
211
sym
221
222
231
232
233
⎤
⎦
and  2 = diag030303030303272829, where the unique vari-
ances corresponding to the ordered categorical variables are fixed at 0.3.
Although other structures for 2 can be considered, we choose this common
form in confirmatory factor analysis (see, for example, Ansari, Jedidi and Dube,
2002; Lee and Zhu, 2000, among others) that gives nonoverlapping latent
factors for clear interpretation. These latent factors are allowed to be corre-
lated. For the within-groups model with the latent factors 1gi1gi11gi2,
we considered invariant within-groups parameters such that  1g =  1 =
diag11··· 19, and 1g = 1, where 1 has the same common structure
as 2 with unknown loadings 121131152162183193. However, as
the within-groups model is directly related to the CSWs, we wish to consider

9.5
AN APPLICATION: FILIPINA CSWs STUDY
261
a more subtle model with a structural equation that accounts for relationships
among the latent factors. To assess the interaction effect of the exogenous latent
factors, the following structural equation for the latent variables is taken:
1gi = 111gi1 +121gi2 +131gi11gi2 +	1gi
(9.25)
To identify the model with respect to ordered categorical variables via the
common method (see, Lee, Poon and Bentler, 1995), 
k1 and 
k4k = 1··· 6
are fixed at 
kj = ∗−1mk, where ∗is the distribution function of N 01,
and mk is the observed cumulative marginal proportion of the categories with
zgk < j. There are a total of 48 parameters in this two-level nonlinear SEM.
In the Bayesian analysis, we need to specify hyperparameter values in the
proper conjugate prior distributions of the unknown parameters. For situa-
tions where we have good prior information, for example from closely related
data or the knowledge of experts, subjective hyperparameter values should be
taken. Under the general situation without good prior information, alternative
methods have to be used to fix the hyperparameters. Some Bayesian anal-
yses of SEMs (see e.g. Ansari, Jedidi and Dube, 2002) used vague but proper
priors with ad hoc hyperparameter values. Many kinds of data-dependent priors
have appeared in Bayesian literature (see Raftery, 1996a,b; Richardson and
Green, 1997; Pauler, Wakefield and Kass, 1999; Song and Lee, 2001; Zhu
and Lee, 2001, among others). In this illustrative example, we use some data-
dependent prior inputs, and ad hoc prior inputs that give rather vague but
proper prior distributions. We emphasize that these prior inputs are used for
the purpose of illustration only, we are not routinely recommending them for
other substantive applications. The data-dependent prior inputs are obtained by
conducting an auxiliary Bayesian estimation with proper vague conjugate prior
distributions which gives estimates ˜01k, ˜
∗
01k, ˜02k and ˜
∗
02k for some hyper-
parameter values (according to the notation in Appendix 9.1). Then, results are
obtained and compared on the basis of the following types of hyperparameter
values:
(I):
Hyperparameters 01k, ∗
01k, 02k and ∗
02k are equal to ˜01k, ˜
∗
01k, ˜02k
and ˜
∗
02k, respectively; H01k, H∗
01k, H02k and H∗
02k are equal to identity
matrices of appropriate orders; 
01k = 
02k = 
01	k = 
02	k = 10, 01k =
02k = 01	k = 02	k = 8, 01 = 02 = 6, R −1
01 = 50I2 and R −1
02 = 50I3.
(II):
Hyperparameter values in 01k, ∗
01k, 02k and ∗
02k are equal to the
zeros, H01k, H∗
01k, H02k and H∗
02k are equal to 5.0 times the identity
matrices of appropriate orders. Other hyperparameter values are equal to
those given in (I). These prior inputs are not data-dependent.
Bayesian estimates are obtained by the proposed algorithm that involves the
Gibbs sampler and the MH algorithm. The convergence of this algorithm is

262
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Figure
9.1
EPSR values against the number of iterations in the analysis of AIDS
data. This figure is from Song and Lee (2004).
monitored by the (EPSR) values suggested by Gelman and Rubin (1992), and
by plots of generated observations obtained with different starting values. To
give some idea about the convergence of the MCMC method in analyzing this
complex SEM, the EPSR values corresponding to the analysis with type (I) prior
inputs are displayed in Figure 9.1. We observe that the algorithm converged in
less than 2000 iterations. Hence, we take a burn-in-phase of 2000 iterations,
and further collect 3000 observations to produce the Bayesian estimates and
their standard error estimates. Results obtained under prior inputs (I) and (II)
are reported in Tables 9.4 and 9.5. From these tables, we see that the estimates
obtained under these different prior inputs are reasonably close. The PP p-
values corresponding to these two sets of estimates are equal to 0.592 and
0.600, which indicate that the proposed model fits the sample data, and this
statistic is quite robust to the selected prior inputs under the given sample size
of 755.
In order to illustrate the proposed path sampling in computing the Bayes
factor for model comparison, we compare this two-level nonlinear model with
some non-nested models. Let M1 be the two-level nonlinear SEM with the
above specifications and the nonlinear structural Equation (9.25), and M2 and

9.5
AN APPLICATION: FILIPINA CSWs STUDY
263
Table
9.4
Bayesian estimates of the structural parameters (Str. Par.) and
thresholds under prior (I) for M1: AIDS data.
Within-group
Between-group
EST
SE
EST
SE
Str. Par.
Str. Par.
121
0238
0.081
221
1248
0218
131
0479
0.112
231
0839
0189
152
1102
0.213
252
0205
0218
162
0973
0.185
262
0434
0221
183
0842
0.182
283
0159
0209
193
0885
0.192
293
0094
0164
11
0454
0.147
211
0212
0042
12
−0159
0.159
212
−0032
0032
13
−0227
0.382
213
0008
0037
111
0216
0.035
222
0236
0054
112
−0031
0.017
223
0006
0041
122
0202
0.037
233
0257
0063
11
0558
0.087
27
0378
0070
12
0587
0.049
28
0349
0053
13
0725
0.063
29
0259
0039
14
0839
0.084
15
0691
0.085
16
0730
0.081
17
0723
0.056
18
0629
0.053
	1
0460
0.080
Thresholds

12
−1163
0.054

13
−0751
0045

22
−0083
0.033

23
0302
0035

32
−0985
0.045

33
−0589
0044

42
−0406
0.035

43
0241
0029

52
−1643
0.063

53
−0734
0027

62
−1038
0.043

63
−0118
0025
This table and Tables 9.5–9.6 are taken from Song and Lee (2004).
M3 be non-nested models with the same specifications except that the corre-
sponding nonlinear structural equations are given by
M2 
1gi =111gi1 +121gi2 +142
1gi1 +	1gi
(9.26)
M3 
1gi =111gi1 +121gi2 +152
1gi2 +	1gi
(9.27)

264
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
Table
9.5
Bayesian estimates of the structural parameters (Str. Par.) and
thresholds under prior (II) for M1: AIDS data.
Within-group
Between-group
EST
SE
EST
SE
Str. Par.
Str. Par.
121
0239
0080
221
1404
0283
131
0495
0119
231
0869
0228
152
1210
0284
252
0304
0293
162
1083
0250
262
0602
0264
183
0988
0215
283
0155
0230
193
0918
0197
293
0085
0176
11
0474
0157
211
0196
0043
12
−0232
0165
212
−0026
0030
13
−0353
0540
213
0010
0032
111
0198
0048
222
0219
0048
112
−0022
0015
223
0008
0037
122
0181
0035
233
0251
0060
11
0562
0100
27
0376
0068
12
0587
0049
28
0350
0055
13
0715
0066
29
0256
0039
14
0849
0091
15
0702
0093
16
0697
0079
17
0738
0055
18
0601
0056
19
0828
0064
	1
0460
0077
Thresholds

12
−1170
0060

13
−0755
0053

22
−0084
0029

23
0303
0033

32
−0986
0043

33
−0589
0042

42
−0406
0035

43
0242
0029

52
−1656
0066

53
−0738
0028

62
−1032
0044

63
−0119
0026
To apply the path sampling in computing the Bayes factor for comparing M1
and M2, we link up M1 and M2 by Mt with the following structural equation:
1gi = 111gi1 +121gi2 +t131gi11gi2 +1−t142
1gi1 +	1gi
(9.28)

9.5
AN APPLICATION: FILIPINA CSWs STUDY
265
Clearly, when t=1, Mt = M1; when t=0, Mt = M2. By differentiating the
complete-data likelihood pYV12XZt with respect to t, we
obtain
U YV12XZt =
n
i=1
	
1gi −111gi1 −121gi2
−t131gi11gi2 −1−t142
1gi1

−1
1	

131gi11gi2 −142
1gi1


(9.29)
Consequently, log B12 can be computed by Equations (9.12) and (9.13) with a
sample of observations simulated from the appropriate posterior distributions.
The above procedure can be similarly used for computing log B13 and log B23.
In this example, we take 20 grids in [0,1] and J = 1000 in computing loga-
rithm Bayes factors. The log B12, log B13 and log B23 under prior inputs (I, II)
are respectively equal to (0.317, 0.018), (0.176, 0.131) and −0138−0203.
Hence, the values of logarithm Bayes factors are reasonably close to the given
different prior inputs. According to the criterion given in Kass and Raftery
(1995) for comparing non-nested models, M2 is slightly better than M1 and
M3. To apply the procedure for comparing nested models, we further compare
M2 with a linear model M0, and a more comprehensive model M4. Competing
models M0 and M4 have the same specifications as M2, except the corresponding
structural equations are given by:
M0  1gi = 111gi1 +121gi2 +	1gi
M4  1gi = 111gi1 +121gi2 +131gi11gi2 +142
1gi1 +152
1gi2 +	1gi
Note that M0 is nested in M2, and M2 is nested in M4. Using the path sampling
procedure, estimates of log B40 and log B42 under prior inputs (I, II) are found
to be (1.181, 1.233) and (1.043, 1.071) respectively. Based on the criterion
given in Kass and Raftery (1995), M4 is slightly better than M0 and M2. The PP
p-values corresponding to M4 under prior inputs (I) and (II) are equal to 0.582
and 0.611, respectively. These two values are close and indicate the expected
result that the selected model also fits the data. Bayesian estimates under M4 and
their standard error estimates are reported in Table 9.6. Results obtained under
prior inputs (II) are similar. We also observe comparatively large variability
corresponding to estimates of parameters in 2 in the between-groups model,
and estimates of 1314 corresponding to the nonlinear terms of the latent
variables. This phenomenon may be due to the small sample size at the between-
groups level and the complicated nature of the parameters. Other straightfor-
ward interpretations are not discussed. Based on the proposed methodology,
more complicated or other combinations of nonlinear terms can be similarly
analyzed.

266
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
Table
9.6
Bayesian estimates of the structural parameters (Str. Par.) and
thresholds under prior (I) for M4: AIDS data.
Within-group
Between-group
EST
SE
EST
SE
Str. Par.
Str. Par.
121
0203
0.070
221
1261
0.233
131
0450
0.100
231
0842
0.193
152
0992
0.205
252
0189
0.227
162
0868
0.180
262
0461
0.209
183
0936
0.172
283
0157
0.230
193
0880
0.194
293
0074
0.167
11
0489
0.147
211
0211
0.040
12
−0026
0.217
212
−0029
0.033
13
−0212
0.265
213
0010
0.035
14
0383
0.442
222
0223
0.053
15
−0147
0.188
223
0013
0.038
111
0245
0.042
233
0243
0.059
112
−0029
0.020
27
0377
0.068
122
0186
0.031
28
0351
0.055
11
0546
0.093
29
0258
0.040
12
0591
0.047
13
0724
0.063
14
0826
0.081
15
0716
0.092
16
0731
0.077
17
0733
0.049
18
0610
0.048
19
0833
0.064
	1
0478
0.072
Thresholds

12
−1163
0.058

13
−0.753
0.048

22
−0088
0.032

23
0.299
0.036

32
−0980
0.046

33
−0.583
0.047

42
−0407
0.034

43
0.244
0.028

52
−1650
0.063

53
−0.734
0.027

62
−1034
0.043

63
−0.118
0.026
To summarize, we have establish a two-level nonlinear SEM with three non-
overlapping factors: ‘worry about AIDS’, ‘attitude to the risk of getting AIDS’
and ‘aggressiveness’ in the within-groups and between-groups covariance struc-
tures. The significance of the establishments’ influence is reflected by relatively
large estimates of some between-groups parameters.

9.6
TWO-LEVEL NONLINEAR SEMs WITH CROSS-LEVEL EFFECTS
267
9.6
TWO-LEVEL NONLINEAR SEMs WITH
CROSS-LEVEL EFFECTS
The within-groups nonlinear structural equation of the within-groups model of
the two-level nonlinear SEM (see Equation (9.4)) discussed in previous sections
is capable of assessing the nonlinear effects of the within-groups latent variables
in 1gi and 1gi to the within-groups endogenous latent variables in 1gi. It
does not accommodate the cross-level effects of between-groups latent variables
in 2g to 1gi. However, in practice, cross-level effects may be important, see
Rabe-Hesketh et al. (2004). For example, effects of the latent variables at the
school level can have great influence on some of latent variables at the teacher
level. Hence, it is desirable to develop two-level SEMs with cross-level effects.
The main purpose of this subsection is to introduce a two-level nonlinear
SEM with cross-level effects. For brevity, we consider data that are continuous
with the usual normality assumption and invariant parameter matrices over
groups. Moreover, we combine the within-groups and between-groups error
measurements at the measurement equation and focus on the structures of the
latent variables.
9.6.1
The Model
We consider the following measurement equation to relate the observed vari-
ables with the latent variables at the within-groups and between-groups models
(see Lee and Tang (2006)):
ugi = +22g +11gi +gi
g = 1   G
i = 1   Ng
(9.30)
where 2g is a q2 × 1 random vector of latent factors with distribution
N 0	2, gi is a p ×1 random vector with distribution N 0 , where  
is a diagonal matrix, and gi are independent of 2g and 1gi. The definitions
of the other quantities are the same as before. For brevity, we consider a factor
analysis model at the between-groups level. For the within-groups model, we
let 1gi = T
giT
giT be a partition of 1gi. To simplify notation, we omit the
subscript 1 in gi and gi. We consider the following structural equation in the
within-groups model:
gi = Hgi2g+gi
(9.31)
where giq11 × 1 and giq12 × 1 are latent subvectors of 1gi
and
Hgi2g = h1gi2g   hmgi2gT
is
an
m × 1
nonzero
vector-valued funtion with differential known functions h1   hm, and m ≥
maxq12q2q11 ×m is the matrix of unknown coefficients, gi and gi are

268
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
respectively distributed as N 0	1 and N0 	, where  	 is diagonal; and
gi is independent of gi and 2g. The generality of the vector-valued function
Hgi2g accommodates nonlinear terms of the exogenous latent variables in
2g and gi to predict the endogenous latent variables in gi. A concrete example
that is associated with gi = gigi = gi1gi2T and 2g = 2g12g22g3
is given below:
gi = 1gi1 +2gi2 +3gi1gi2 +42g1 +52g2 +62g3 +72g22g3 +	gi
(9.32)
where  = 1   7Hgi2g = gi1gi2gi1gi22g12g22g32g2
2g3T . Note that linear and interaction terms of the exogenous latent vari-
ables at the within-groups and between-groups model are involved in Equa-
tion (9.32). If necessary, we can easily include other nonlinear terms. Let  =
21 and gi = T
2gT
1giT , then Equation (9.30) can be rewritten as:
ugi = +gi +gi
(9.33)
It is assumed that for g ̸= h, ugi and uhj are independent, for any i and j.
However, due to the presence of 2g, the observed measurements ugi and ugj are
correlated. Moreover, due to the presence of 2g in the within-groups structural
Equation (9.31), for i ̸= j, gi and gj are dependent and hence 1gi and 1gj
are dependent. Similarly, the within-groups latent vector 1gi depends on the
between-groups latent vector 2g. Hence, the usual assumption in the common
two-level SEMs (Ansari and Jedidi, 2000; Lee and Shi, 2001; Song and Lee,
2004) about the independence of 2g and 1gi is violated. The covariances
among the observed and latent variables become more complicated because of
the various kinds of dependence, not only among the ugi and ugj, but also among
the 1gi1gj and 2g. For instance, due to the complexity of Hgi2g
in Equation (9.31), the covariance matrix of 1gi can be complicated; due
to the correlated structure of 2g and 1gi, their covariance is complicated;
and the covariance matrix of ugi can be very complicated. Moreover, as the
covariance of ugi and ugj can be very complicated, the covariance matrix of
ug = uT
g1   uT
gNg T can be very complicated. Therefore, the accommodation
of the between-groups latent variables’ effect to the within-groups endogenous
latent variables gi further compounds the difficulty in analyzing the two-level
nonlinear SEMs. As we can see, the difficulty can be solved by the technique of
data augmentation. In the next section, we assume that the model defined by
Equations (9.30) and (9.31) is identified.
9.6.2
Bayesian Analysis
Let U = u1··· uG be the overall observed data, n = N1 + ··· + NG, and
let 1g1 and 2 be data matrices defined as before. Further, let  be the

9.6
TWO-LEVEL NONLINEAR SEMs WITH CROSS-LEVEL EFFECTS
269
parameter vector that contains all unknown parameters in 12
	2	1  and  	. Utilizing the key idea of data augmentation (Tanner and
Wong, 1987), the joint posterior distribution of interest is 21U. The
Bayesian estimates of the parameters and latent variables and the PP p-value can
be obtained given a sufficiently large number of observations that are simulated
from 21U. Moreover, the Bayes factor for model comparison can be
similarly computed through Equations (9.12) and (9.13), with a slight modifi-
cation on the derivative of the complete-data likelihood (see Equation (9.14)).
Hence, the major task is to simulate observations from the joint posterior distri-
bution by the Gibbs sampler coupled with the MH algorithm. The Gibbs
sampler is implemented as follows. At the j + 1th iteration with current
value j
j
2 
j
1 , iteratively generate (a) 
j+1
2
from 2j
j
1 U,
(b) 
j+1
1
from 1j
j+1
2
U, and (c) j+1 from p
j+1
2

j+1
1
U.
Brief derivations of the conditional distributions involved in the Gibbs sampler
are presented in Appendix 9.5. The related MH algorithm is presented in
Appendix 9.6. Convergence of the algorithm is monitored by the ‘estimated
potential scale reduction (EPSR)’ values suggested by Gelman and Rubin
(1992), or by the plots of parallel sequences of observations simulated via
different starting values.
9.6.3
An Application
The Accelerated Schools for Quality Education (ASQE) Project is a huge project
which was conducted for helping schools to achieve an internal cultural change
in order to be self-reliant in attaining school-based goals in self-improvement.
In this section, we focus on the particular issue about the causal relation-
ships among the ‘school values inventory’, teachers ‘job satisfaction’, and their
‘empowerment’ in identifying and solving the schools’ problems. Relationships
among these latent variables at the school level and the teacher level are impor-
tant in the cultivation of their own and their peers’ skills in improving their
teaching skills and practice. Based on the proposed two-level SEM that incor-
porates the effects of the between-groups (school level) latent variables to the
within-groups (teacher level) latent variables, we can assess precise interrelation-
ships among the latent variables in both levels.
To save space, we only present our results based on analyses of the data
that were obtained in the year from September, 1998, to August, 1999. The
data set is hierarchically structured with n = 1555 teachers nested in G = 50
schools. The data set is unbalanced with values of Ng ranged from 14 to 47.
Three manifest variables (relating to questions: I proudly introduce my school
as a worth-while working place to my friends; I find that my attitude of value
is close to my school’s attitude of value; and I can fully utilize my potential
in my school work), ug1ug2 and ug3 that are related with respondents’ job
satisfaction are taken as indicators for the latent factor, ‘job satisfaction’. These
variables are measured via a seven-point scale. For brevity, they are treated

270
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
as continuous. The manifest variables ug4ug5 and ug6 for the latent variable,
‘school value inventory’ are: (1) participation and collaboration, (2) collegiality,
and (3) communication and consensus, which are respectively measured by the
averages of seven, six and ten items in the questionnaire. The manifest variables
ug7ug8 and ug9 for the latent factor, ‘teachers empowerment’ are: (1) decision
making, (2) self efficacy, and (3) self autonomy, which are measured by the
averages of four, four and five items in the questionnaire. The sample means and
standard deviations of the manifest variables are {4.139, 4.553, 4.487, 2.406,
3.171, 3.468, 0.534, 0.381, 0.601} and {1.371, 1.187, 1.181, 0.848, 0.763,
0.728, 0.499, 0.486, 0.490}, respectively.
To establish an appropriate SEM for the school and teacher levels in the above
data set, a two-level structural equation model with nine manifest variables and
three latent variables is considered with the following specifications. We consider
a factor analysis model for the between-groups model at the schools level.
Although other structures for the factor loading matrix 2 can be used, based
on the meaning of the questions that are associated with the manifest variables,
we choose the following common structure for the factor loading matrix (see
Jöreskog and Sörbom, 1996) that gives nonoverlapping latent factors with clear
interpretation. These latent factors are allowed to be correlated. Specifically,
we take
T
2 =
⎡
⎣
10 221 231 00 00
00 00 00
00
00 00
00 10 252 262 00 00
00
00 00
00 00 00
00 10 283 293
⎤
⎦ 	2 =
⎡
⎣
211 212 213
212 222 223
213 223 233
⎤
⎦
where 1’s and 0’s in 2 are fixed parameters. Based on the meaning of the
corresponding questions, the latent variables can be roughly interpreted as the
influence of the schools on ‘job satisfaction, 2g1’, ‘schools value inventory,
2g2’, and ‘teachers empowerment, 2g3’ of the teachers. For the within-groups
model at the teachers level, we also use the same factor loading structure of 2
for 1 (with unknown elements denoted by 1ij) to relate the latent factors to
the manifest variables. Again, there are three latent factors, gigi1 and gi2, in
the within-groups model. Similarly, based on the meaning of the corresponding
questions, interpretations of gigi1 and gi2 are ‘job satisfaction’, ‘schools
value inventory’, and ‘teachers empowerment’ that are directly related to the
teachers. The variances and covariance of gi1 and gi2 are given by 111122
and 112, respectively. As ‘job satisfaction’ of the teachers is an important factor
in education, it is important to investigate its relationships with the other latent
factors. Here, we study various within-groups structural equations with gi as
the endogenous variable.
To address the questions on the importance of the exogenous latent vari-
ables, we compare competing models which have the same between-groups
(school level) factor analysis model, and the same within-groups (teacher level)

9.6
TWO-LEVEL NONLINEAR SEMs WITH CROSS-LEVEL EFFECTS
271
measurement equation, but with the following different within-groups struc-
tural equations:
M0  gi = 1gi1 +2gi2 +42g1 +52g2 +62g3 +	gi
M1  gi = 	gi
M2  gi = 1gi1 +2gi2 +	gi
M3  gi = 1gi1 +2gi2 +3gi1gi2 +42g1 +52g2 +62g3 +	gi (9.34)
M4  gi = 1gi1 +2gi2 +3gi1gi2 +42g1 +52g2 +62g3
+72g22g3 +	gi
Model M0 involves linear effects of the exogenous latent variables at both levels.
Model M1 corresponds to a simple model without any exogenous latent vari-
ables in the structural equation, and M2 corresponds to a linear model without
the effects of school level latent variables. Models M3 and M4 are nonlinear
SEMs that involve both effects from the within-groups and between-groups
exogenous latent variables to the endogenous latent variable in the structural
equation. Model M3 involves an interaction effect 3gi1gi2 of the teacher
level latent variables, whereas M4 further involves an additional interaction
effect 72g22g3 of the school level latent variables. M1 and M2 are nested in
M0; while M0 is nested in M3 and M4. Defining a path t ∈01 to link any
two of the above models is straightforward. For example, M0 and M3 can be
linked by:
Mt03  gi = 1gi1 +2gi2 +t3gi1gi2 +42g1 +52g2 +62g3 +	gi
Note that Mt03 reduces to M0 when t = 0, and reduces to M3 when t = 1.
The logarithm Bayes factors for comparing the above models are computed
via the path sampling procedure with the following Type I hyperparameters in
the conjugate distributions: 
0k = 
0	k = 10, 0k = 0	k = 4, 01 = 02 = 8,
H0k = H0uk = 025I, R01 = 01 −q12 −1 ˜	1R02 = 02 −q2 −1 ˜	2, 0k =
 ˜2k ˜1k and 0k = ˜k, where ˜2k, ˜1k, ˜k, ˜	1 and ˜	2 are the Bayesian
estimates obtained by an auxiliary estimation with noninformative prior distri-
butions. In the MH algorithm, the variances in the proposal distributions are
chosen such that the approximately average acceptance rates are 0.276 and
0.283, respectively. To monitor convergence, three parallel sequences of obser-
vations are generated for each parameter via different starting values, and the
EPSR values of all parameters are computed. To give some idea about the
convergent behavior of the MCMC methods in analyzing these kinds of complex

272
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
0
1000
2000
3000
4000
5000
6000
7000
1
2
3
4
5
6
7
8
9
10
iteration
EPSR
Figure
9.2
EPSR values of all parameters against iteration numbers in the ASQE
example.
SEMs, plots of the EPSR values for all parameters against the iteration numbers
in analyzing the selected model M3 are presented in Figure 9.2. It can be seen
that the EPSR values are less than 1.2 within 4000 iterations. To compute the
Bayes factor, the number of grids is taken to be 10, and for each ts 6000 simu-
lated observations are used to compute ¯Us after 4000 burn-in iterations. The
estimated logarithm Bayes factors computed via the path sampling procedure
are equal to

log B01 = 88231

log B02 = 24059

log B03 = −4018

log B04 =
−2832 and

log B43 = −1187. Based on the criterion given in Table 5.1, M0
is significantly better than M1 and M2, but not as good as M4 and M3. From

log B54, M3 is selected. The estimated PP p-value corresponding to M3 is 0.512,
which indicates that M3 is a plausible model for fitting the data. As by-products,
Bayesian estimates of the unknown parameters and their standard error esti-
mates for the selected model M3 are obtained from the observations simulated
at t = 1 in the path sampling procedure. Results are reported in Table 9.7.
Path diagrams of the structural equation and the measurement equation in the
selected model M3 are displayed in Figures 9.3 and 9.4, respectively.

9.6
TWO-LEVEL NONLINEAR SEMs WITH CROSS-LEVEL EFFECTS
273
Table
9.7
The Bayesian estimates and their standard errors under Type I
prior inputs: ASQE example.
Para.
EST
SE
Para.
EST
SE
Para.
EST
SE
221
0518
0029
121
0.834
0007
1
0158
0006
231
0760
0024
131
0.818
0007
2
0206
0006
252
0828
0094
152
0.933
0019
3
0248
0007
262
2904
0128
162
0.705
0018
4
0315
0009
283
0839
0051
183
1.004
0024
5
0235
0007
293
0640
0054
193
0.825
0024
6
0267
0007
211
0292
0043
111
0.396
0014
7
0117
0003
212
0028
0005
112
0.157
0006
8
0111
0003
213
0044
0008
122
0.113
0005
9
0158
0004
222
0007
0001
1
0.533
0049
	
0647
0020
223
0005
0001
2
1.856
0101
1
4219
0033
233
0016
0002
3
−0652
0065
2
4644
0022
4
−1734
0059
3
4557
0026
5
1.561
0230
4
2394
0014
6
4.139
0209
5
3161
0012
6
3437
0020
7
0520
0012
8
0369
0011
9
0592
0010
δgi
ω2g1
ω2g 2
ω2g 3
ηgi
γ1
γ2
γ3
γ4
γ5
γ6
ξgi 1
ξgi 1ξgi 2
ξgi 2
Figure
9.3
The path diagram corresponding to the within-groups structural equa-
tion: ASQE example.

274
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
Between-groups
ω2g1
ω2g2
ω2g3
ugi 1
ugi 2
ugi 3
1.0
1.0
1.0
Within-groups
1.0
λ2,21
λ1,21
λ2,31
λ1,31
λ2,52
λ1,52
λ2,62
λ1,62
λ2,83
λ1,83
λ2,93
λ1,93
1.0
1.0
ugi 4
ugi 5
ugi 6
ugi 7
ugi 8
ugi 9
ηgi
ξgi 1
ξgi 2
Figure
9.4
The path diagram corresponding to the measurement equations of the
between-groups and within-groups models; for brevity, paths of error measurements are
not displayed: ASQE example.
Comparing M0 with M1, and M2, we conclude that a two-level model with
a teacher level structural equation that includes effects of school level latent
factors and the teacher level latent variables is significantly better than those
that only involve the teacher level latent variables. This indicates that the cross-
level effects are necessary for establishing an appropriate model. The model
M0 is further compared with more subtle models that contain nonlinear terms
of latent variables from both levels in the within-groups structural equation.
Finally, the two-level nonlinear SEM M3 is selected. In M3, a factor analysis
model with three correlated latent factors is proposed for the between-groups
model. An SEM is proposed for the within-groups model, which involves a
structural equation which contains the linear effects of the school level latent
factors 2g12g2 and 2g3, linear effects of the teacher level latent variables
gi1 and gi2, and an interaction effect of the teacher level latent variables,
gi1gi2 to gi. We note from the SEs in Table 9.7 that the standard errors
of some of the between-groups parameter estimates are slightly larger than
those in the within-groups model. As G is smaller than N1 + ··· + NG, this
phenomenon is reasonable. From the Bayesian estimates of 111, 112 and 122
given in Table 9.7, the estimated correlation between ‘school value inventory’
and ‘teacher empowerment’ at the teacher level is 0.742. As expected, these
latent variables are highly correlated. From the Bayesian estimates of 2ij, we
see that the corresponding correlation in the between-groups model is reduced

9.7
ANALYSIS OF TWO-LEVEL NONLINEAR SEMs USING WINBUGS
275
to 0.473. This indicates a difference between these variables at the school and
teacher levels. The estimated within-groups structural equation is:
gi =0533gi1 +1856gi2 −0652gi1gi2 −17342g1 +15612g2
+41392g3 +	gi
We can see from the standard errors that the regression coefficients are
significantly different from zero. Comparing the magnitudes of ˆ1, ˆ2 and ˆ3
with ˆ4, ˆ5 and ˆ6, we note that the impact of the effects from the latent
variables in the school level is stronger than that from latent variables in the
teacher level. Moreover, from ˆ1= 0533 and ˆ2= 1856, we see that the
latent variables about ‘school value inventory’ and ‘teacher empowerment’ at
the teacher level have positive effects on teachers’ ‘job satisfaction’. This is
a logical finding. From ˆ3= −0652, ‘school value inventory’ and ‘teacher
empowerment’ at the teacher level have a negative interaction on teachers’
‘job satisfaction’. The basic interpretation is that the additive effect of the
linear random variables (gi1, gi2) in the structural equation are inadequate to
account for their relationships with the latent variable ‘job satisfaction, gi’,
and a negative interaction term of gi1 and gi2 has to be added. Depending
on different situations, this negative interaction term has varying impact. For
example, it indicates that for teachers with a positive feeling on both ‘school
value inventory’ and ‘teacher empowerment’ (both gi1 and gi2 are positive), it
would have a too strong additive positive effect on ‘job satisfaction’, and hence
a negative adjustment by the interaction effect is necessary. From ˆ5= 1561
and ˆ6= 4139, the influences of the schools on teachers’ ‘empowerment’
and ‘school value inventory’ also have a positive effect on the ‘job satisfaction’
of the teachers. Interestingly, we observe from ˆ4= −1734 that the latent
factor on influence of schools on teachers’ ‘job satisfaction’ has rather strong
negative effects on ‘job satisfaction’ of the teachers. It may indicate the presence
of some conflicts between the school policy and the teachers’ job satisfaction.
This finding provides some insight to the administrators in deciding their school
policy and their relationship with their teachers.
9.7
ANALYSIS OF TWO-LEVEL NONLINEAR SEMs
USING WINBUGS
The software WinBUGS (Spiegelhalter, Thomas, Best and Lunn, 2003) can
produce Bayesian estimates of the parameters in some two-level nonlinear SEMs.
As we mentioned before, for models with ordered categorical data, it is hard
to estimate the unknown thresholds and the unknown parameters in the struc-
tural model simultaneously by WinBUGS. Here, we fix the unknown thresholds

276
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
lambda2_21 chains 1:3
iteration
1
2500
5000
7500
10000
–0.5
0.0
0.5
1.0
lambda1_93 chains 1:3
iteration
1
2500
5000
7500
10000
–0.5
0.0
0.5
1.0
gamma_2 chains 1:3
iteration
1
2500
5000
7500
10000
–1.0
0.0
1.0
2.0
3.0
phi2_11 chains 1:3
iteration
1
2500
5000
7500
10000
0.0
1.0
2.0
3.0
4.0
phi1_12 chains 1:3
iteration
1
2500
5000
7500
10000
–1.5
–1.0
–0.5
    0.0
    0.5
Figure
9.5
(a), (b), (c), (d) and (e) Plots of WinBUGS sequences corresponding to
2211932211 and 112.
at some preassigned values. Based on our experience in analyzing two-level
SEMs with cross-level effects through WinBUGS, we find that the convergence
of the MCMC chains corresponding to the parameters in 	2 is not good.
This phenonmenon appears even in analyzing two-level linear SEMs. However,
WinBUGS can be applied to analyze two-level nonlinear SEMs without cross-
level effects. To demonstrate this, we apply WinBUGS to analyzing the ASQE
data on the basis of the same two-level SEM and Type I prior inputs as

9.7
ANALYSIS OF TWO-LEVEL NONLINEAR SEMs USING WINBUGS
277
in Section 9.6.3, except that the within-groups structural equation is now
equal to
gi = 1gi1 +2gi2 +3gi1gi2 +	gi
(9.35)
This structural equation involves an interaction term of the latent variables
at the teacher level, but no cross-level effects. The DIC value corresponding
to this model is 19 703.80. This value can be used for model comparison.
Plots of three simulated sequences of observations obtained from WinBUGS
are presented in Figure 9.5. Bayesian estimates are reported in Table 9.8. From
estimates of the parameters and latent variables, Equations (9.30) and (9.31),
we can obtain estimated residuals ˆgi and ˆ	gi, for g = 1   Gi = 1   Ng.
These estimated residuals are useful for outlier analysis and for goodness-of-
fit assessment. Some estimated residual plots, ˆgi4, ˆgi6 and ˆ	gi versus the case
numbers, are displayed in Figure 9.6. Plots of estimated residual ˆ	gi versus ˆgi1
and ˆgi2 are presented in Figure 9.7, and plots of ˆgi4 versus ˆgi1, ˆgi2 and ˆgi
are presented in Figure 9.8. Other plots are similar. These plots indicate the
corresponding measurement and structural equations of the proposed two-level
model fit the data reasonably well. The WinBUGS codes and the data are given
in the following website: http://www.wiley.com/go/lee_structural.
Table
9.8
The Bayesian estimates and their standard errors obtained
through WinBUGS: ASQE example.
Para.
EST
SE
Para.
EST
SE
Para.
EST
SE
221
0738
0060
121
0.844
0015
1
0163
0011
231
0706
0041
131
0.836
0014
2
0225
0011
252
0688
0162
152
0.936
0037
3
0242
0012
262
2040
0247
162
0.694
0032
4
0315
0016
283
0754
0083
183
1.000
0042
5
0233
0013
293
0545
0095
193
0.824
0043
6
0285
0012
211
0484
0101
111
0.396
0026
7
0114
0006
212
0003
0016
112
0.160
0010
8
0111
0005
213
0048
0019
122
0.115
0008
9
0158
0006
222
0019
0006
1
0.508
0089
	
0631
0035
223
0010
0004
2
1.858
0176
1
4227
0106
233
0026
0006
3
−0644
0126
2
4629
0081
3
4562
0077
4
2395
0030
5
3164
0024
6
3445
0044
7
0523
0026
8
0373
0021
9
0595
0018

278
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
(a)
hat{epsilon}_gi4
0
500
1000
1500
(b)
hat{epsilon}_gi6
0
500
1000
1500
(c)
hat{delta}_gi
0
500
1000
1500
–2
–1
0
2
1
–2
–1
0
2
1
–2 –1 0
2
3
1
Figure
9.6
Estimated residual plots: (a) ˆgi4, (b) ˆgi6 and (c) ˆ	gi.
(a)
hat{delta}_gi
–2
–1
0
–4
–2
0
2
4
1
Figure
9.7
Plots of estimated residuals ˆ	gi versus: (a) ˆgi1 and (b) ˆgi2.

APPENDIX 9.1
279
(b)
hat{delta}_gi
–0.8
–0.6
–0.4
–0.2
0.0
0.2
0.4
0.6
–4
–2
0
4
2
Figure
9.7
(Continued)
(a)
hat{epsilon}_gi4
–2
–1
0
–4
–2
0
4
2
1
(b)
hat{epsilon}_gi4
–0.8
–0.6
–0.4
–0.2
0.0
0.2
0.4
0.6
–4
–2
0
4
(c)
hat{epsilon}_gi4
–4
–2
0
–4
–2
0
4
2
2
2
Figure
9.8
Plots of estimated residuals ˆgi4 versus: (a) ˆgi1, (b) ˆgi2, and (c) ˆgi.
APPENDIX 9.1: CONDITIONAL DISTRIBUTIONS: TWO-LEVEL
NONLINEAR SEM
Owing to the complexity of the model, it is very tedious to derive all the condi-
tional distributions required by the Gibbs sampler, hence only brief discussions
are given. For brevity, we will use p·· to denote the conditional distribution

280
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
if the context is clear. Moreover, we only consider the case that all parameters
in 1g2
1g1g
2 and 2 are not fixed. Conditional distributions for the
case with fixed parameters can be obtained by slight modifications as given in
previous chapters.
pVY12XZ: Since vgs are independent and not depending
on , this conditional distribution is equal to a product of pvgYg
1g2gXgZg with g = 1   G. For each gth term in this product,
pvgYg1g2gXgZg ∝pvg2g
Ng

i=1
pugivg1gi
(A9.1)
∝exp

−1
2

vT
g Ng −1
1g + −1
2 vg −2vT
g
×

 −1
1g
Ng

i=1
ugi −1g1gi+ −1
2 +22g
 

Hence, for each vg, its conditional distribution pvg· is N∗
g∗
g, where
∗
g = ∗
g

 −1
1g
Ng

i=1
ugi −1g1gi+ −1
2 +22g

 and
∗
g = Ng −1
1g + −1
2 −1
p1YV2XZ: Since 1gi are mutually independent, ugi is inde-
pendent of uhj for all h ̸= g, and they are not depending on  and Y, we
have
p1· =
G
g=1
Ng

i=1
p1givg2gugi
∝
G
g=1
Ng

i=1
pugivg1gip1gi1gip1gi
It follows that p1gi· is proportional to
exp

−1
2T
1gi	−1
1g 1gi +ugi −vg −1g1giT  −1
1g ugi −vg −1g1gi
+1gi −
1g1gi −1gH11giT  −1
1g	1gi −
1g1gi −1gH11gi


(A9.2)

APPENDIX 9.1
281
p2YV1XZ: This distribution has a very similar form to
p1· and Equation (A9.2), and hence it is not presented.
pYV12XZ: We only consider the case that all the thresholds
corresponding to each within-groups are different. The other cases can be
similarly derived. To deal with the situation with little or no information about
these parameters, the following noninformative prior distribution is used:
p
gk = p
gk2   
gkbk−1 ∝c g = 1   G k = 1   s
where c is a constant. Now, since gYg is independent of hYh for g ̸= h,
and  1g is diagonal,
pY· =
G
g=1
pgYg· =
G
g=1
s
k=1
pgkYgk·
(A9.3)
where Ygk = yg1k   ygNg k. Let 1gk be the kth diagonal element of  1gvgk
be the kth element of vg and T
1gk be the kth row of 1g, and IAy be an
indicator function with value 1 if y in A and zero otherwise, pY· can be
obtained from Equation (A9.3) and
pgkYgk· ∝
Ng

i=1
−1/2
1gk ygik −vgk −T
1gk1giI
gkzgik 
gkzgik+1ygik
(A9.4)
where  is the probability density function of N 01.
pYV12XZ: This conditional distribution is different under
different special cases as discussed in Section 9.3.1. We first consider the situ-
ation with distinct within-groups parameters, that is 11 ̸= ··· ̸= 1G. Let 2
be the vector of unknown parameters in 2 and  2; and 2 be the vector
of unknown parameters in 
22	2 and  2	. These between-groups param-
eters are the same for each g. For the within-group parameters, let 1g be
the vector of unknown parameters in 1g and  1g and 1g be the vector of
unknown parameters in 
1g1g	1g and  1g	. It is natural to assume the
prior distributions of these parameter vectors in different independent groups
are independent of each other, and hence they can be treated separately.
For 1g, the following commonly used conjugate type prior distributions are
used:
−1
1gk
D= Gamma
01gk01gk 1gk1gk
D= N01gk1gkH01gk k = 1   p
where T
1gk is the kth row of 1g and 
01gk01gk01gk and H01gk are given
hyperparameter values. For k ̸= h, it is assumed that 1gk1gk and 1gh1gh
are independent. Let U∗
g = ugi −vg i = 1   Ng and U∗T
gk
be the kth

282
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
row of U∗
g, 1gk = H−1
01gk + 1gT
1g−1m1gk = 1gkH−1
01gk01gk + 1gU∗
gk,
1g = 1g1   1gNg 
and
1gk = 01gk + 1
2U∗T
gk U∗
gk −mT
1gk−1
1gkm1gk +
T
01gkH−1
01gk01gk, then it can be shown that
p−1
1gk·
D= Gamma 2−1Ng +
01gk1gk and
p1gk−1
1gk·
D= N m1gk1gk1gk
(A9.5)
For 1g, it is assumed that 	1g is independent of ∗
1g 1g	, where ∗
1g =

1g1gT . Also, ∗
1gk1g	k and ∗
1gh1g	h are independent, where ∗T
1gk
and 1g	k are the kth row and diagonal element of ∗
1g and  1g	 respectively.
The associated prior distribution of 	1g is: p	−1
1g 
D= Wq12R01g01g, where
01g and the positive definite matrix R01g are given hyperparameters. Moreover,
the prior distribution of 1g	k and ∗
1gk are:
−1
1g	k
D= Gamma 
01g	k01g	k and ∗
1gk1g	k
D= N ∗
01gk1g	kH∗
01gk
where 
01g	k01g	k∗
01gk and H∗
01gk are given hyperparameters. Let ∗
1g =
1g1   1gNg , ∗T
1gk be the kth row of ∗
1g1g = 1g1   1gNg  and
H∗
1g = H∗
11g1   H∗
11gNg , in which H∗
11gi = T
1giH11giT T ,
i = 1   Ng, it can be shown that for k = 1   q11,
p1g	k·
D= Gamma 2−1Ng +
01g	k1g	k and
p∗
1gk−1
1g	k·
D= N m∗
1gk1g	k∗
1gk
(A9.6)
where ∗
1gk = H∗−1
01gk + H∗
1gH∗T
1g −1m∗
1gk = ∗
1gkH∗−1
01gk∗
01gk + H∗
1g∗
1gk and
1g	k = 01g	k + 1
2∗T
1gk∗
1gk −m∗T
1gk∗−1
1gk m∗
1gk + ∗T
01gkH∗−1
01gk∗
01gk. The condi-
tional distribution relating to 	1g is given by
p	1g1g
D= IWq121gT
1g +R −1
01gNg +01g
(A9.7)
Conditional distributions involved in 2 are derived in the same way on
the basis of the following independent conjugate type prior distributions: for
k = 1   p, and

D= N 00 −1
2k
D= Gamma 
02k02k
2k2k
D= N 02k2kH02k

APPENDIX 9.2: MH ALGORITHM: TWO-LEVEL NONLINEAR SEM
283
where T
2k is the kth row of 2
02k02k0, 002k and H02k are given
hyperparameters.
Similarly, conditional distributions involved in 2 are derived on the basis
of the following conjugate type distributions: for k = 1   q21,
	−1
2
D= Wq22R0202 −1
2	k
D= Gamma 
02	k02	kand
∗
2k2	k
D= N ∗
02k2	kH∗
02k
where ∗
2 = 
T
2 T
2 T and ∗T
2k is the vector that contains the unknown param-
eters in the kth row of ∗
2. As these conditional distributions are similar to those
in Equations (A9.5)–(A9.7), they are not presented here.
In the situation where 11 = ··· = 1G= 1, the prior distributions corre-
sponding to components of 1 are not depending on g, and all the data in
the within groups should be combined in deriving the conditional distributions
for the estimation. Conditional distributions can be derived with the following
conjugate type prior distributions: for k = 1   p and similar notations as
above,
−1
1k
D= Gamma
01k01k 1k−1
1k
D= N 01k1kH01k
	−1
1 
D= Wq22R0101
−1
1	k
D= Gamma
01	k01	k
and
∗
1k1	k
D= N ∗
01k1	kH∗
01k
(A9.8)
and the prior distributions and conditional distributions corresponding to struc-
tural parameters in the between-groups covariance matrix are the same as before.
APPENDIX 9.2: MH ALGORITHM: TWO-LEVEL NONLINEAR
SEM
Simulating observations from the gamma, normal and inverted Wishart distribu-
tions is straightforward and fast. However, the conditional distributions p1·,
p2· and pY· are complex and it is therefore necessary to implement the
MH algorithm for efficient simulation of observations from these conditional
distributions.
For the conditional distribution p1·, we need to simulate observations
from the target density p1gi· as given in Equation (A9.2). Similar to
the method of Zhu and Lee (1999) and Lee and Zhu (2000), we choose

284
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
N ·2
1C1g as the proposal distribution, where C−1
1g = C−1
1g +T
1g −1
1g 1g and
C−1
1g is given by
C−1
1g =


T
1g0 −1
1g	
1g0
−
T
1g0 −1
1g	1g1g
−T
1gT
1g −1
1g	
1g0
	−1
1g +T
1gT
1g −1
1g	1g1g


where 
1g0 = Iq11 −
1g with an identity matrix Iq11 of order q11 and 1g =
H11gi/1gi1gi=0. Let p·∗2
1C1g be the density function corresponding
to the proposal distribution N ∗2
1C1g, the MH algorithm is implemented
as follows. At the mth iteration with a current value 
m
1gi , a new candidate
∗
i is generated from p·
m
1gi 2
1C1g and accepting this new candidate with
probability min1p∗
i ·/p
m
1gi ·. The variance 2
1 can be chosen such that
the average acceptance rate is approximately 0.25 or more, see Gelman, Roberts,
and Gilks (1995).
Observations from the conditional distribution p2· with target density
similar to Equation (A9.2) can be simulated via a similar MH algorithm as
described above. To save space, details are not given.
An MH type algorithm that is similar to that described in Section 6.3.2 is
necessary for simulating observations from the complex distribution pY·.
Here, the target density is given in Equation (A9.4). According to the factor-
ization recommended by Cowles (1996) (see also Lee and Zhu (2000)) the
joint proposal density of gk and Ygk is constructed as
pgkYgk· = pgk ·pYgkgk ·
(A9.9)
Then, the algorithm is implemented as follows. At the mth iteration with

m
gk Y
m
gk , the acceptance probability for a gkYgk as a new observation

m+1
gk
Y
m+1
gk
 is min1Rgk, where
Rgk =
pgkYgkZgk1gp
m
gk Y
m
gk gkYgkZgk1g
p
m
gk Y
m
gk Zgk1gpgkYgk
m
gk Y
m
gk Zgk1g

with Zgk = zgik   zgng k. To search for a new observation via the proposal
density of Equation (A9.9), we first generate a vector of thresholds 
gk2   ,

gkbk−1 from the following truncated normal distribution

gkz
D= N 
m
gkz2

gkI
gkz−1
m
gkz+1
gkz z = 2   bk −1

APPENDIX 9.3
285
where 2

gk is a preassigned value to give an approximate acceptance rate 0.44
(see Cowles 1996). It follows from, Equation (A9.4) and the above result that
Rgk =
bk−1

z=2
∗
m
gkz+1 −
m
gkz/
gk−∗
gkz−1 −
m
gkz/
gk
∗
gkz+1 −
gkz/
gk−∗
m
gkz−1 −
gkz/
gk
Ng

i=1
∗−1/2
1gk 
gkzgik+1 −vgk −T
1gk1gi−∗−1/2
1gk 
gkzgik −vgk −T
1gk1gi
∗−1/2
1gk 
m
gkzgik+1 −vgk −T
1gk1gi−∗−1/2
1gk 
m
gkzgik −vgk −T
1gk1gi

where ∗is the distribution function of N 01. Since Rgk only depends on
the old and new values of gk but not on Ygk, it only needs to generate a new
Ygk for an accepted gk. This new Ygk is simulated from the truncated normal
distribution pYgkgk· via the algorithm given in Roberts (1995).
APPENDIX 9.3: PP P-VALUE FOR TWO-LEVEL NSEM WITH
MIXED CONTINUOUS AND ORDERED-CATEGORICAL
VARIABLES
Suppose null hypothesis H0 is that the proposed model defined in Equa-
tions (9.1) and (9.2) is plausible, the PP p-value is defined as
pBXZ = PrDUrepYV12 ≥DUYV12XZH0
where Urep denotes a replication of U = ugii = 1   Ngg = 1   G,
with ugi satisfying the model defined by Equation (9.3) that involves structural
parameters and latent variables satisfying equations (9.4) and (9.5) and D ··
is a discrepancy variable. Here, the following 2 discrepancy variable is used:
D UrepYV12 =
G
g=1
Ng

i=1
u
rep
gi −vg −1g1giT  −1
1g u
rep
gi −vg −1g1gi
which is distributed as 2pn, where n = N1 +···+NG, a 2 distribution with
pn degrees of freedom. The PP p-value on the basis of this discrepancy variable is
PBXZ =

Pr2pn ≥DUYV12×
pYV12XZdddYdVd1d2

286
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
A Rao–Blackwellized type estimate of the PP p-value is equal to
ˆPBXZ
= T −1
T
t=1
Pr2pn ≥DUttYtVt
t
1 
t
2 
where D UttYtVt
t
1 
t
2  is calculated at each iteration and
the tail-area of a 2 distribution which can be obtained via standard statis-
tical software. The hypothesized model is rejected if ˆPBXZ is not close
to 0.5.
APPENDIX 9.4: QUESTIONS ASSOCIATED WITH THE
MANIFEST VARIABLES
ug1:
How much of a threat do you think AIDS is to the health of people?
ug2:
What are the chances that you yourself might get AIDS?
ug3:
How worried are you about getting AIDS?
How great is the risk of getting AIDS or the AIDS virus from sexual
intercourse with someone:
ug4:
who has the AIDS virus using a condom?
ug5:
whom you don’t know very well without using a condom?
ug6:
who injects drugs?
ug7:
How often did you perform vaginal sex in the last 7 days?
ug8:
How often did you perform manual sex in the last 7 days?
ug9:
How often did you perform oral sex in the last 7 days?
APPENDIX 9.5: CONDITIONAL DISTRIBUTIONS: SEMs WITH
CROSS-LEVEL EFFECTS
Let 1 be the structural parameter vector that contains all unknown parameters
in  and  	 that are involved in the structural Equation (9.31); let 2 be the
structural parameter vector that contains all unknown parameters in 2, 1
and   that are involved in the measurement Equation (9.30); let 3 be the
structural parameter vector that contains all unknown parameters in covariance
matrices 	2 and 	1 of the latent vectors. Let p be some appropriate prior
distribution of . Based on the definition of the model and the nature of the
structural parameters, it is reasonable to consider prior distributions that satisfy
p = p1p2p3.

APPENDIX 9.5
287
In deriving the conditional distribution p21U, it can be shown that
p21U =
G
g=1
p2g1gug ∝
G
g=1

p2g3
Ng

i=1
pugi2g1gi2pgigi2g1


Based on the definition of the model and assumptions, p2g1gug is
proportional to
exp

−1
2
Ng

i=1
ugi −−22g −11giT  −1
 ugi −−22g −11gi
−1
2T
2g	−1
2 2g −1
2
Ng

i=1
gi −Hgi2gT  −1
	 gi −Hgi2g


(A9.10)
Consider the conditional distribution p12U. Note that given U
and 2, 1gi are mutually independent for g = 1   G and i = 1   Ng.
Hence, it can be shown that
p12U =
G
g=1
Ng

i=1
p1giugi2g ∝
G
g=1
Ng

i=1
pugi1gi2g2pgigi2g1pgi3
Note that given 1gi and 2g, ugi are mutually independent, and given gi
and 2g, gi are mutually independent. Hence, it follows from the definition
of the model and assumptions that p1giugi2g is proportional to
exp

−1
2ugi −−22g −11giT  −1
 ugi −−22g −11gi
−1
2T
gi	−1
1 gi −1
2gi −Hgi2gT  −1
	 gi −Hgi2g


(A9.11)

288
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
Hence, by utilizing the idea of data augmentation, and working with condi-
tional distributions on the basis of a complete data set, the problems that are
induced by the dependence of the observed and/or latent vectors can be solved.
Useful prior information of  can be incorporated in the conditional distri-
bution of p21U for achieving better results. Again, the commonly
used conjugate prior distributions are used. Specifically, we use the following
conjugate prior distribution for 1. For k = 1   r1,
−1
	k
D= Gamma
0	k0	k k	k
D= N 0k	kH0k
(A9.12)
where 	k is the kth diagonal element of  	, and T
k is the kth row vector
of , 
0	k0	k0k and the positive definite matrix H0k are hyperparameters
whose values are assumed to be given from the prior information of previous
studies or other sources. For h ̸= k, it is assumed that 	kk and 	hh
are independent. The following conjugate prior distributions for   and  =
21 in 2 are used. For k = 1   p

D= N 00 −1
k
D= Gamma
0k0k kk
D= N 0kkH0uk
(A9.13)
where k is the kth diagonal element of  , T
k is the kth row vector of ,

0k0k0k and H0uk are the given hyperparameters. For h ̸= k, it is assumed
that hh and kk are independent. The conjugate prior distributions
for components in 3 are:
	2
D= IWqR0202
	1
D= IWr2R0101
(A9.14)
where 02, 01, and the positive definite matrices R02 and R01 are the given
hyperparameters.
Consider the components of the conditional distributions in relation to
p121U. It follows from reasoning similar to that in Song and Lee
(2004) that
p−1
	k 21U
D= Gamma n
2 +
0	k	k and
pk21U−1
	k 
D= N 	k	k	k
(A9.15)
where
	k = H−1
0k + G
g=1
Ng
i=1 Hgi2gHgi2gT −1,
	k =
	kH−1
0k0k + G
g=1
Ng
i=1 gikHgi2g, gik is the kth element of gi and
	k = 0	k +G
g=1
Ng
i=1 2
gik −T
	k−1
	k 	k +T
0kH−1
0k0k/2.

APPENDIX 9.6
289
Consider the conditional distributions in relation to p221U. Let
k = H−1
0uk +G
g=1
Ng
i=1 giT
gi−1, k = kH−1
0uk0k +G
g=1
Ng
i=1 ugikgi and
k = 0k +G
g=1
Ng
i=1 u2
gik −T
k−1
k k +T
0kH−1
0uk0k/2, where gi is defined
as in Equation (9.33). Similarly, it can be shown that
p−1
k 21U
D= Gamma n
2 +
0kk
pk21U−1
k 
D= N kkk
pU 1
D= N −1
0 +n −1
 −1n −1

¯U∗+−1
0 0−1
0 +n −1
 −1
(A9.16)
where ¯U∗= G
g=1
Ng
i=1ugi −gi/n.
Finally, for 3, it can be shown that
p	221U
D= IWq

G
g=1
2gT
2g +R02G +02


(A9.17)
p	121U
D= IWr2

G
g=1
Ng

i=1
giT
gi +R01n +01


(A9.18)
APPENDIX 9.6: THE MH ALGORITHM: SEMs WITH
CROSS-LEVEL EFFECTS
This algorithm is implemented as follows. At the j + 1th iteration with a
current value 
j
1gi, a new candidate 1gi is generated from the proposal distri-
bution N 
j
1gi2, where −1
 = T
1  −1
 1 + with
 =

 −1
	
− −1
	 
−T T  −1
	
	−1
1 +T T  −1
	 


where  = Hgi2g/T
gigi=0 and 2 is chosen such that the average
acceptance rate is approximately 025 or more (see Gelman, Roberts and Gilks,
1995). The acceptance probability is
min

1
p1giugi2g
p
j
1giugi2g



290
9
TWO-LEVEL NONLINEAR STRUCTURAL EQUATION MODELS
Similarly, the MH algorithm for sampling 2g from p2g1gug is
as follows. At the j + 1th iteration with a current value 
j
2g, a new candi-
date 2g is generated from the proposal distribution N 
j
2g2, where
 = NgT
2  −1
 2 + 	−1
2 + Ng
i=1 T
2iT  −1
	 2i with 2i = Hgi2g/
T
2g2g =0 and the choice of 2 is similar. The acceptance probability is
min

1
p2g1gug
p
j
2g1gug


REFERENCES
Ansari, A. and Jedidi, K. (2000) Bayesian factor analysis for multilevel binary observa-
tions. Psychometrika, 65, 475–498.
Ansari, A., Jedidi, K. and Dube, L. (2002) Heterogeneous factor analysis models: a
Bayesian approach. Psychometrika, 67, 49–78.
Carlin, B. P. and Louis, T. A. (1996) Bayes and Empirical Bayes Methods for Data
Analysis. London: Chapman and Hall.
Cowles, M. K. (1996) Accelerating Monte Carlo Markov chain convergence for
cumulative-link generalized linear models. Statistics and Computing, 6, 101–111.
Gelman, A. and Meng, X. L. (1998) Simulating normalizing constants: from importance
sampling to bridge sampling to path sampling. Statistical Science, 13, 163–185.
Gelman, A. and Rubin, D. B. (1992) Inference from iterative simulation using multiple
sequences. Statistical Science, 7, 457–472.
Gelman, A., Meng, X. L. and Stern, H. (1996) Posterior predictive assessment of model
fitness via realized discrepancies. Statistica Sinica, 6, 733–759.
Gelman, A., Roberts, G. O. and Gilks, W. R. (1995) Efficient Metropolis jumping rules.
In J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith (eds), Bayesian
Statistics 5, pp. 599–607. Oxford: Oxford University Press.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distribution and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Jöreskog, K. G. and Sörborn, D. (1996) LISREL 8: Structural Equation Modeling with the
SIMPLIS Command Language. Hove and London: Scientific Software International.
Kass, R. E. and Raftery, A. E. (1995) Bayes factors. Journal of the American Statistical
Association, 90, 773–795.
Lee, S. Y. and Poon, W. Y. (1998) Analysis of two-level structural equation models via
EM type algorithms. Statistica Sinica, 8, 749–766.
Lee, S. Y. and Shi, J. Q. (2001) Maximum likelihood estimation of two-level
latent variable models with mixed continuous and polytomous data. Biometrics, 57,
787–794.
Lee, S. Y. and Song, X. Y. (2005) Maximum likelihood analysis of a two-level nonlinear
structural equation model with fixed covariates. Journal of Educational and Behavioral
Statistics, 30, 1–26.
Lee, S. Y. and Tang, N. S. (2006) Bayesian analysis of two-level nonlinear structural
equation models with cross-level effects. (Unpublished manuscript).

REFERENCES
291
Lee, S. Y. and Tsang, S. Y. (1999) Constrained maximum likelihood estimation of two-
level covariance structure model via EM type algorithms. Psychometrika, 64, 435–450.
Lee, S. Y. and Zhu, H. T. (2000) Statistical analysis of nonlinear structural equation
models with continuous and polytomous data. British Journal of Mathematical and
Statistical Psychology, 53, 209–232.
Lee, S. Y., Poon, W. Y. and Bentler, P. M. (1995) A two-stage estimation of struc-
tural equation models with continuous and polytomous variables. British Journal of
Mathematical and Statistical Psychology, 48, 339–358.
McDonald, R. P. and Goldstein, H. (1989) Balanced versus unbalanced designs for
linear structural relations in two-level data. The British Journal of Mathematical and
Statistical Psychology, 42, 215–232.
Morisky, D. E. et al. (1998) The effects of establishment practices, knowledge and
attitudes on condom use among Filipina sex workers. AIDS Care, 10, 213–220.
Pauler, D. A., Wakefield, J. C. and Kass, R. E. (1999) Bayes factor and approximations
for variance component models. Journal of the American Statistical Association, 94,
1242–1253.
Rabe-Hesketh, S., Skrondal, A. and Pickles, A. (2004) Generalized multilevel structural
equation modeling. Psychometrika, 69, 167–190.
Raftery, A. E. (1996a) Approximate Bayes factors and accounting for model uncertainty
in generalised linear models. Biometrika, 83, 251–266.
Raftery, A. E. (1996b) Hypothesis testing and model selection. In W. R . Gilks,
S. Richardson and D.J. Spieglhalter (eds), Practical Markov Chain Monte Carlo,
pp. 163–188. London: Chapman and Hall.
Richardson, S. and Green, P. J. (1997) On Bayesian analysis of mixtures with an unknown
number of components (with discussion). Journal of the Royal Statistical Society, Series
B, 59, 731–792.
Roberts, C. P. (1995) Simulation of truncated normal variables. Statistics and
Computing, 5, 121–125.
Shi, J. Q. and Lee, S. Y. (1998) Bayesian sampling-based approach for factor analysis
model with continuous and polytomous data. British Journal of Mathematical and
Statistical Psychology, 51, 233–252.
Shi, J. Q. and Lee, S. Y. (2000) Latent variable models with mixed continuous and
polytomous data. Journal of the Royal Statistical Society, Series B, 62, 77–87.
Song, X. Y. and Lee, S. Y. (2001) Bayesian estimation and test for factor analysis
model with continuous and polytomous data in several populations. British Journal of
Mathematical and Statistical Psychology, 54, 237–263.
Song, X. Y. and Lee, S. Y. (2004) Bayesian analysis of two-level nonlinear structural equa-
tion models with continuous and polytomous data. British Journal of Mathematical
and Statistical Psychology, 57, 29–52.
Spiegalhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. (2003) WinBUGS User
Manual, Version 1.4, Cambridge, England: MRC Biostatistics Unit.
Tanner, M. A. and Wong, W. H. (1987) The calculation of posterior distributions by
data augmentation(with discussion). Journal of the American Statistical Association,
82, 528–550.
Zhang, W. and Lee, S. Y. (2001) Asymptotic theory of two-level structural equation
models with constraints. Statistica Sinica, 11, 135–145.
Zhu, H. T. and Lee, S. Y. (1999) Statistical analysis of nonlinear factor analysis models.
British Journal of Mathematical and Statistical Psychology, 52, 225–242.
Zhu, H. T. and Lee, S. Y. (2001) A Bayesian analysis of finite mixtures in the LISREL
model. Psychometrika, 66, 133–152.


10
Multisample Analysis of
Structural Equation
Models
10.1
INTRODUCTION
The two-level SEMs are used to analyze data with hierarchical structures which
usually involve a large number of groups, and observations within each group
are correlated. The main purposes of two-level SEMs are to take into account
the correlated structure of the data, and to establish a between-groups model.
Multisample data are coming from a comparatively smaller number of groups
(populations). The number of observations within each group is usually large,
and it is assumed that observations within each group are independent. As the
within-group observations are independent, the multisample data do not have a
hierarchical structure. In multisample analysis of SEMs, each group is associated
with a hypothesized model of interest. One main objective is to investigate
the similarities or differences among the models in the different groups. As a
result, the statistical inferences emphasized in analyzing multisample SEMs are
different from those in analyzing two-level SEMs.
Analysis of multiple samples is a major topic in structural equation modeling.
This kind of analysis is useful for investigating the behaviors of different groups
of employees, different cultures, different treatment groups, etc.. The main
interest is the testing of hypotheses about the different kinds of invariances
among the models in different groups. The traditional approach for testing
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

294
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
invariance applied the likelihood ratio test on the basis of the asymptotic chi-
square distribution (see Bollen (1989)). For more complex SEMs, it is rather
difficult to evaluate the observed-data likelihood at the ML estimates to obtain
the test statistics; moreover, the asymptotic distribution of the likelihood ratio
test statistic is unknown and hard to derive. As we will see later in this chapter,
this important issue can be formulated as a model comparison problem, and
can be effectively addressed by the Bayes factor or DIC in a Bayesian approach.
Another advantage of the Bayesian model comparison through Bayes factor is
that non-nested models (hypotheses) can be compared, hence it is not necessary
to follow a hierarchy of hypotheses to assess the invariance for the SEMs in
different groups.
Bayesian methods for analyzing mutlisample SEMs are presented in this
chapter. We will emphasize nonlinear SEMs with ordered categorical (or
dichotomous) variables, although the general ideas can be applied to SEMs
with other settings. The rest of the chapter is organized as follows: Section 10.2
presents the model; the Bayesian approach for estimation and model compar-
ison (hypothesis testing) are discussed in Section 10.3; Section 10.4 presents
the results that are obtained from the analyses of two applications. One is about
job attitude, emotion and benefit attitude, while the other is about quality of
life, and analyzed via WinBUGS. Technical details are given in the appendix.
10.2
THE MULTISAMPLE NONLINEAR STRUCTURAL
EQUATION MODEL
We first extend the standard SEMs to a multisample nonlinear SEM. Consider
G independent groups of individuals that may represent different populations
of employees, different cultures, etc.. For g = 1   G, and i = 1   Ng, let
v
g
i
be the p × 1 random vector of manifest variables that correspond to the
ith observation (subject) in the gth group. In contrast to two-level SEMs, for
i = 1   Ng in the gth group, v
g
i
are assumed to be independent. For each
g = 1   Gv
g
i
is related to latent variables in a q ×1 random vector 
g
i
by
the following measurement equation:
v
g
i
= g +g
g
i +
g
i 
(10.1)
where g is the vector of the intercepts, g is the parameter matrix of
regression coefficients that reflect the relation of manifest variables in v
g
i
with
the latent variables in 
g
i , and 
g
i
is a random vector of the measurement
errors. It is assumed that 
g
i
and 
g
i
are independent, and the distribution
of 
g
i
is N0
g
 , where 
g

is a diagonal covariance matrix. Let 
g
i
=

g
i
T 
g
i
T T , where 
g
i
represents the q1 ×1 vector of endogenous latent

10.2
MULTISAMPLE NONLINEAR SEM
295
variables, and 
g
i
represents the q2 × 1 vector of the exogenous latent vari-
ables. Note that it is naturally assumed that q1 and q2 are independent of g,
that is they are the same for each group. To assess the effects of the nonlinear
terms of latent variables in 
g
i
to 
g
i , we consider a nonlinear SEM with the
following nonlinear structural equation:

g
i
= 	g
g
i +
gH
g
i +
g
i
= g
	 H∗
g
i +
g
i 
(10.2)
where 
g and 
gare unknown parameter matrices, and 
g
i
is the random
vector of error measurements that is independent of 
g
i 
g
	 = 	g
g,
H∗
g
i  = i
gT H
g
i T T and H
g
i  = h1
g
i    ha
g
i T , in which
the hs are nonzero and known differentiable functions that include polynomials.
It is assumed that the vector-valued functions H and H∗do not depend on g.
However, different groups can have different linear or nonlinear terms of 
g
i
by defining appropriate H (or H∗) and assigning zero values to appropriate
elements in 
g. For example, let 
g
i
= 
g
i 
g
i1 
g
i2 T , for g = 12. Suppose
that the structural equations in the first and second groups are given by

1
i
=
1
1 
1
i1 +
1
2 
1
i2 +
1
3 
1
i1 
1
i2 +
1
i 

2
i
=
2
1 
2
i1 +
2
2 
2
i2 +
2
4 
2
i1 
2
i1 +
2
i 
(10.3)
In defining these structural equations, we consider H
g
i  = 
g
i1  
g
i2  
g
i1 
g
i2 

g
i1 
g
i1 T  
1 = 
1
1  
1
2  
1
3 0 and 
2 = 
2
1  
2
2  0, 
2
4 . In practice,
models in different groups usually have the same linear and nonlinear terms of

g
i . For handling very complicated different nonlinear functions in the models,
this assumption can be relaxed with minor modifications. In the same way as
for models in previous chapters, it is assumed that I −	g is a nonsingular
matrix, the determinant of which is not dependent on elements in 	g. It is
further assumed that 
g
i
is distributed as N 0g, and 
g
i
is distributed
with N0
g
 , where 
g

is a diagonal covariance matrix.
To handle the ordered categorical outcomes, suppose that v
g
i
= xi
gT 
yi
gT T  where x
g
i
is an observable r ×1 subvector of continuous responses,
while y
g
i
is an s × 1 subvector of unobservable continuous responses, the
information of which is reflected by an observable ordered categorical vector
z
g
i . In a generic sense, an ordered categorical variable z
g
m is defined with its
underlying latent continuous random variable y
g
m by:
zg
m = a if g
mzm ≤yg
m < 
g
mzm+1 a = 1   bm m = 1   s
(10.4)
where − = 
g
m1 < 
g
m2 < ··· < 
g
mbm < 
g
mbm+1 =  is the set of threshold
parameters that define the categories, and bm is the number of categories for the

296
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
ordered categorical variable z
g
m . Note that for each ordered categorical variable,
the number of thresholds is the same for each group. However, the categories
can be unequally spaced under this formulation.
To tackle the identification problem, we have to pay attention to the following
issues. There are two kinds of indeterminacies in the multisample SEMs with
ordered categorical variables. First, the SEM that is defined by Equations (10.1)
and (10.2) is not identified. This indeterminacy can be solved by the common
method of fixing appropriate elements in g	g and/or 
g at preassigned
values. The other indeterminacy is induced by the ordered categorical vari-
ables. Consider an ordered categorical variable z
g
m that is defined by a set of
thresholds 
g
mk and an underlying latent continuous variable y
g
m with mean and
variance 
g
m  and 
2g
m . The indeterminacy is caused by the fact that 
g
mk
g
m
and 
2g
m
are not simultaneously estimable. For a given group g, a common
method to solve this identification problem with respect to each mth ordered
categorical variable that corresponds to the gth group is to fix 
g
m2 and 
g
mbm
at preassigned values (Lee, Poon and Bentler, 1995; Shi and Lee, 2000; Lee,
Song, Skevington and Hao, 2005). For example, we may fix 
g
m2 = ∗−1f
g
m2
and 
g
mbm = ∗−1f
g
mbm, where ∗is the cumulative function of N01 f
g
m2
and f
g
mbm are the frequencies of the first category and the cumulative frequen-
cies of categories with z
g
m < bm. For analyzing multisample models with interest
in group comparisons, it is important to impose conditions for identifying the
ordered categorical variables such that the underlying latent continuous vari-
ables have the same scale among the groups. To achieve this, we can choose
the first group as the reference group and identify its ordered categorical vari-
ables by fixing both end thresholds as above. Then, for any m, and g ̸= 1, we
impose the following restrictions (see Lee, Poon and Bentler,1989),

g
mk = 
1
mk
k = 1   bm
(10.5)
on the thresholds for every ordered categorical variable z
g
m . Under these iden-
tification conditions, the unknown parameters in the groups are interpreted
in a relative sense, compared over groups. Note that when a different refer-
ence group is used, relations over groups are unchanged. Hence, the statistical
inferences are unaffected by the choice of the reference group. Clearly, the
compatibility of the groups is reflected by the differences of the parameter esti-
mates. Note that for dichotomous variables the threshold in each group is fixed
at zero, and z
g
m is either equal to ‘0’ or ‘1’. In this case, it is not necessary to
set any constraints on the thresholds across groups.

10.3
BAYESIAN ANALYSIS OF MULTISAMPLE NONLINEAR SEMs
297
10.3
BAYESIAN ANALYSIS OF MULTISAMPLE
NONLINEAR SEMs
In this section, we will describe Bayesian estimation and model comparison in
the context of multisample nonlinear SEMs with ordered categorical variables.
Again, the general strategy that utilizes the idea of data augmentation and
MCMC tools is used. Theoretically, as a multisample SEM is a special case of the
two-level SEM, some conditional distributions required in the Gibbs sampler
can be obtained from the results in Chapter 9. However, as specific constraints
among the parameters in different groups are imposed, it is necessary to pay
more attention in specifying the corresponding prior distributions. In the same
way as the model comparison in two-level SEMs, it requires some insight in
applying the path sampling procedure.
10.3.1
Bayesian Estimation
Let g be the unknown parameter vector in the identified model and let g
be the vector of unknown thresholds that correspond to the gth group. In
multisample analysis, a certain type of parameter in g is often hypothesized
to be invariant over the group models. For example, restrictions on the thresh-
olds, and the following constraints 1 = ··· = G, 1 = ··· = G and/or

1 = ··· = 
G, are often imposed. Hence, in the analysis we allow common
parameters in 1   G. Let  be the vector that contains all unknown
distinct parameters in 1   G, and let  be the vector that contains all
the unknown thresholds, the Bayesian estimates of  and  are obtained by the
Gibbs sampler.
Let Xg = x
g
1    x
g
Ng  and X = X1   XG be the observed contin-
uous data, and Zg = z
g
1    z
g
Ng  and Z = Z1   ZG be the observed
ordered categorical data. Let Yg = y
g
1    y
g
Ng  and Y = Y1   YG be
the latent continuous measurement associated with Zg and Z, respectively.
The observed data will be augmented with Y in the posterior analysis. Once
Y is given, all the data are continuous and the problem will be easier to cope
with. Moreover, let g = 
g
1    
g
Ng  and  = 1   G be the
matrices of latent variables. Note that when  is observed, the measurement
equation and the nonlinear structural equation reduce to the regular simulta-
neous regression model. Difficulties due to the nonlinear relationships among
the latent variables are greatly alleviated. Hence, problems associated with the
complicated components of the model can be handled by data augmenta-
tion. In the posterior analysis, the observed data XZ will be augmented
with Y. More specifically, we will consider the joint posterior distribu-
tion YXZ. The Gibbs sampler (Geman and Geman, 1984) will be
used in generating a sequence of observations from this joint posterior distri-
bution. Then the Bayesian solution is obtained by standard inferences on the

298
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
basis of the generated sample of observations. In applying the Gibbs sampler,
we iteratively sample observations from the following conditional distributions:
YXZ, YXZ] and YXZ. As in our previous
treatments of the thresholds, we consider non-informative prior to , so that
the corresponding prior distribution is proportional to a constant. The first
two conditional distributions, which can be derived by similar reasoning in
Chapter 9, Appendix 9.1, are presented in Appendix 10.1. The conditional
distribution YXZ is further decomposed into components involving
various structural parameters in the different group models. These components
are different under various hypotheses of interest or various competing models.
Some examples of non-nested competing models (or hypotheses) are:
MA

No constraints
M1  1 = ··· = G
M2  1 = ··· = G
M3

1
	 = ··· = G
	 
M4  1 = ··· = G
(10.6)
M5

 1

= ··· =  G
 
M6  
1
 = ··· = 
G
 
The components in the conditional distribution YXZ and the spec-
ification of prior distributions are slightly different under different Mk defined
above. First, prior distributions for nonconstrained parameters in different
groups are naturally assumed to be independent. In estimating the uncon-
strained parameters, we need to specify its own prior distribution, and the
data in the corresponding group are used. For constrained parameters across
groups, only one prior distribution for these constrained parameters is neces-
sary, and all the data in the groups should be combined in the estimation
(see Song and Lee (2001)). In contrast to the Bayesian analysis of other
single group SEMs, we may not wish to take a joint prior distribution for
the factor loading matrix and the unique variance of the error measurement.
In the gth group model, let 
g
k and 
gT
k
be the kth diagonal element of
 g

and the kth row of g, respectively. In multisample analysis, if those
parameters are not invariant over groups, their joint prior distribution could
be taken as: 
g−1
k
D= Gamma
g
0k
g
0k
g
k 
g
k 
D= N
g
0k 
g
k H
g
0yk where

g
0k
g
0k, 
g
0k and H
g
0yk are hyperparameters. This kind of joint prior distribu-
tion may cause problems under the constrained situation where 1 = ··· =
G =  and  1

̸= ··· ̸=  G
 , because it is difficult to select pk· based
on a set of different 
g
k . Hence, for convenience, and following the sugges-
tion of Song and Lee (2001), we select independent prior distributions for
g and  g
  such that pg g
  = pgp g
 , for g = 12   G.
Under this kind of prior distribution, the prior distribution of  under the
constraint 1 = ··· = G =  is given by: k
D= N0kH0yk, which is inde-
pendent of 
g
k . Under the situation 1 ̸= ··· ̸= G, the prior distribu-
tion of each 
g
k
is given by N
g
0k H
g
0yk. The prior distribution of 
g−1
k

10.3
BAYESIAN ANALYSIS OF MULTISAMPLE NONLINEAR SEMs
299
is again Gamma
g
0k
g
0k. Similarly, we select the prior distributions for

g
	
and 
g

such that p
g
	 
g
  = p
g
	 p
g
 , for g = 1   G. Let
T
	k and 
g
	k
T be the kth rows of 	 and 
g
	 , respectively. The prior distribu-
tions of 	, and g are given by:
(i)
if 1
	 = ··· = G
	 = 	, 	k
D= N0	kH0	k,
(ii)
if 1
	 ̸= ··· ̸= G
	 ,

g
	k
D= N
g
0	kH
g
0	k,
with the hyperparameters 0	k, 
g
0	k, H0	k and H
g
0	k. The prior distribution
of 
g−1
k
is Gamma
g
0k
g
0k, with hyperparamters 
g
0k, and 
g
0k. The prior
distributions of  and  are given by:
g D= N
g
0 
g
0  g−1 D= Wq2R
g
0 
g
0 
g = 1   G
where 
g
0 , 
g
0 , R
g
0
and 
g
0
are hyperparameters. Similar adjustments are
taken under various combinations of constraints. Based on the above under-
standing, and the reasoning given in Chapter 9, the conditional distribution
YXZ under various competing models can be obtained. Some
results are given in Appendix 10.1.
10.3.2
Bayesian Model Comparison
In analyzing multisample SEMs, one important statistical inference beyond esti-
mation is on testing whether some type of parameters are invariant over the
groups. In Bayesian analysis, each hypothesis of interest is associated with a
model, and the problem is approached through model comparison. For instance,
the competing models of interest could be those given in (10.6), or any combi-
nations of those models, which specify certain kinds of constraints on some
parameters among groups. In contrast to the traditional approach using the
likelihood ratio test, it is not necessary to proceed Bayesian model comparison
with a sequence of hierarchical models. For example, depending on the interest
of a substantive problem, we can compare any two nonnested models Mk and
Mh, as given in (10.6); or compare any Mk with any combination of the models
given in (10.6).
Let Mh and Mk be any two competing multisample SEMs. The following
Bayes factor is used to compare Mh and Mk:
Bkh = pXZMk
pXZMh
(10.7)

300
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
where XZ is the observed data set. Again, this Bayes factor is computed by
a path sampling (Gelman and Meng, 1998) procedure. Let t be a path in 01
to link Mh and Mk, and t0 = 0 < t1 < ··· < tS < tS+1 = 1 be fixed grids in
01. Let pYXZt be the complete-data likelihood,  is the vector
of unknown parameters in the linked model and
UYXZt = d log pYXZt/dt
It can be shown by reasoning similar to that used in previous chapters that

log Bkh = 1
2
S
s=0
ts+1 −ts ¯Us+1 + ¯Us
(10.8)
where
¯Us = 1
J
J
j=1
UjjYjjXZts
(10.9)
in which jjYjj  j=1,2,…,J} is a sample of observations simulated
from the joint posterior distribution YXZts.
To find a good path to link the competing models Mh and Mk is a crucial step
in the path sampling procedure for computing log Bkh. In multisample analysis,
it may not be easy to find a path that directly links Mh and Mk. Under this
situation, the use of an auxiliary model Ma which can be linked with both Mh
and Mk is helpful. We have used this technique in analyzing two-level SEMs (see
Example 2 in Chapter 9, Section 9.4). An illustrative example for multisample
analysis is given below.
In this illustrative example, G = 2. For each g, the model is defined by
Equations (10.1) and (10.2) with 	g = 0. For g = 12i = 1   Ng, suppose
that the competing models M1 and M2 are defined as:
M1 
v
g
i
= g +
g
i +
g
i

g
i
= 
gH
g
i +
g
i 
M2 
v
g
i
= g +g
g
i +
g
i

g
i
= 
H
g
i +
g
i 
In M1,  is invariant over the two groups; whilst in M2, 
 is invariant over the
groups. Note that in both models, the form of the nonlinear terms is the same.

10.3
BAYESIAN ANALYSIS OF MULTISAMPLE NONLINEAR SEMs
301
Due to the constraints imposed on the parameters, it is quite difficult to find
a path t in 01 that directly links M1 and M2. This difficulty can be solved
through the use of the following auxiliary model Ma:
Ma 
v
g
i
= g +
g
i g = 12i = 1   Ng
The linked model Mta for linking M1 and Ma is defined as: for g = 12i =
1   Ng
Mta1 
v
g
i
= g +t
g
i +
g
i 

g
i
= 
gH
g
i +
g
i 
When
t = 1,
Mta1
reduces
to
M1;
and
when
t = 0,
Mta1
reduces
to
Ma.
The
parameter
vector

in
Mta1
contains
12 1
  2
 
1
212
1

and 
2
 . The linked model
Mta2 for linking M2 and Ma is defined as:
Mta2 
v
g
i
= g +tg
g
i +
g
i 

g
i
= 
H
g
i +
g
i 
Clearly, when t = 1 and 0, Mta2 reduces to M2 and Ma, respectively. The param-
eter vector in Mta2 contains 1212 1
  2
 
12
1

and 
2
 . We first compute log B1a and log B2a, and then obtain log B12 via the
following equation:
log B12 = log pXZM1/pXZMa
pXZM2/pXZMa = log B1a −log B2a
(10.10)
The above method can be applied to link other competing models. Moreover,
the approach presented in Chapter 9, Section 9.4, can be applied to situations
where just one auxiliary model is not adequate to link the two very different
competing models M1 and M2. Similarly, we use more than one auxiliary model
MaMb   to link M1 and M2. For example, suppose we use Ma and Mb to
link M1 and M2, with Ma closer to M1, and Mb closer to M2. Then
pXZM1
pXZM2 = pXZM1/pXZMa
pXZM2/pXZMa and
pXZM2
pXZMa = pXZM2/pXZMb
pXZMa/pXZMb

302
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
hence, log B12 = log B1a + log Bab −log B2b. Each logarithm Bayes factor is
computed via path sampling.
Model comparison can also be conducted with DIC that is available via
WinBUGS, see Section 10.4.2. In a similar way to the goodness-of-fit assess-
ment in the context of single group nonlinear SEMs, it is quite difficult to find a
saturated model for the multisample nonlinear SEMs. However, the goodness-
of-fit of a proposed multisample model can be assessed by the estimated residual
plots, or the PP p-value that can be obtained through straightforward modifi-
cation of the derivation presented in Chapter 9, Appendix 9.3.
10.4
NUMERICAL ILLUSTRATIONS
10.4.1
Analysis of Multisample Management Data
Assessing employee job attitudes, benefits and emotion is an interesting issue
that has received much attention in organizational and management research.
In this section, we use the proposed Bayesian approach to analyze a multisample
nonlinear SEM with these latent constructs. The aims are to study the nonlinear
effects of the exogenous latent variables of ‘benefit attitude’ and ‘emotion’ to
the endogenous latent variable of ‘job attitude’, and assess various hypotheses
for comparing the nonlinear SEMs between two populations.
A portion of the Inter-University Consortium for Political and Social Research
(ICPSR) data set collected in the project World Value Survey 1981–1984 and
1990–1993 (World Value Study Group, 1994) was analyzed. In this illustra-
tion, the data obtained from the UK g = 1 and Canada g = 2 with nine
manifest variables (variables 252, 253, 254, 89, 91, 93, 99, 102 and 103, see
Appendix 1.1) were used. After deleting observations with missing entries, the
sample sizes are N1 = 1412 and N2 = 1686. It is clear from Appendix 1.1 that
the first three manifest variables, which were measured via a 10-point Likert
scale and hence treated as continuous for convenience, are indicators of the
latent variable ‘job attitude’; the next three dichotomous manifest variables are
indicators of the latent variable ‘emotion’, and that the last three dichotomous
manifest variables are indicators of the latent variable ‘benefit attitude’. The
dichotomous variables are coded with 1 for ‘yes, or mentioned’, and 0 for ‘no,
or not mentioned’. On the basis of the clear meaning associated with the indi-
cators of the latent variables, the factor loading matrices are taken to be of the
following nonoverlapping structure for better interpretation:
gT =
⎡
⎢⎣
10∗

g
21

g
31
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
10∗

g
52

g
62
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
0∗
10∗

g
83

g
93
⎤
⎥⎦
g = 12
(10.11)

10.4
NUMERICAL ILLUSTRATIONS
303
The following nonlinear structural equation was considered for each group to
assess the interaction of ‘emotion, 
g
1 ’, and ‘benefit attitude, 
g
2 ’, with ‘job
attitude, g’:

g
i
= 
g
1 
g
i1 +
g
2 
g
i2 +
g
3 
g
i1 
g
i2 +
g
i 
(10.12)
The covariance matrix of the error measurements in the measurement equation
is set equal to 
g
 , with diagonal elements 
g
1 
g
2 
g
3 10∗10∗10∗10∗
10∗10∗. Here, the variances of the error measurements that correspond to the
dichotomous variables are fixed at 1.0 for achieving an identified model. Other
unknown parameters in this model are 
g
11
g
22
g
12, the variances and covari-
ances of the latent variables 
g
1 
g
2  in g, and 
1

and 
2
 , the variances
of 
g
i
in the structural Equation (10.12). In the following Bayesian analysis,
some prior inputs of the conjugate prior distributions, such as 
g
0 
g
0k R
g
0
and 
g
0	k, are taken from the Bayesian estimates of the parameters obtained
by an initial estimation, based on the noninformative prior distributions with
the following ad hoc prior inputs: 
g
0 = IH
g
0yk = IH
g
0	k = I
1
0k = 
1
01 =
10
1
0k = 
1
01 = 8
2
0k = 
2
01 = 20
2
0k = 
2
01 = 16
1
0 = 20 and 
2
0 = 4.
In this multisample analysis, we employed the Bayes factor to study the rela-
tions of the parameter matrices in the nonlinear SEMs of Canada and the
UK, and for finding a better model. We consider the following non-nested
models M1M2M3M4, M5 and M6, which are associated with the following
hypotheses: H1: no constraints; H2  1 = 2; H3  
1 = 
2; H4  1 =
2; H5   1

=  2
 ; and H6  
1

= 
2
 . The logarithm Bayes factors for
comparing these non-nested models are computed by the path sampling proce-
dure as described in Section 10.3.2. In the computation, we selected S = 15,
took 2000 burn-in iterations and further collected J = 2000 observations in
computing U s in Equation (10.9) . We obtained

log B21 = −1600. Note that
as the sample sizes N1 and N2 are large, the magnitude of the estimated loga-
rithm Bayes factor is large. This reveals the fact that large sample sizes provide
clear evidence of selecting the better model. Based on the interpretation of

log B21, M1 with no constraints is better than M2 with the constraint 1 = 2.
What is the interpretation of this result? From the structure of g (see Equa-
tion (10.11)) and the corresponding indicators (see Appendix 1.1), we have a
rather clear interpretation of the latent variables g
g
1
and 
g
2
as ‘job atti-
tude’, ‘emotion’ and ‘benefit attitude’. Based on Equation (10.11), we see
that the associations between the latent variables and their respective indica-
tors are clearly indicated by the corresponding elements in g. The rejection
of M2 reveals that the associations in group 1 (UK) are different from those
in group 2 (Canada). The common multisample analysis of structural equation
models via the likelihood ratio test based on p-value is to stop if the hypothesis
1 = 2 is rejected. The reason for ending further analysis may be due to
the belief that rejection of 1 = 2 implies that the scale of measurements of

304
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
the latent variables in group 1 are different from the scale of measurements
of the latent variables in group 2. However, based on the likelihood ratio test
with p-value, the conclusion that H2 is not rejected does not imply that H2 is
true, and it does not imply that the scales of these two groups are the same.
Hence, the justification that is just based on the result of hypothesis testing
for continuing the analysis is not very strong. Moreover, for dichotomous vari-
ables, as the underlying continuous responses are not observed, it is difficult
to draw a conclusion on the scales of the latent variables on the basis of the
hypothesis testing result. From the non-overlapping structure of g, these
latent variables can be clearly interpreted as the latent constructs in relation to
job attitude, emotion and benefit attitude, although the associations of these
latent constructs and their indicators are not the same in groups 1 and 2.
Hence, we think that it is desirable to conduct a further analysis to obtain a
deeper understanding of these latent constructs. This additional analysis involves
comparisons of M1 and M2 with models M3M4M5 and M6 that are associated
with H3H4H5 and H6. The logarithm Bayes factors computed by the path
sampling procedure for comparing these models are reported in Table 10.1.
We again found that the magnitudes of these logarithm Bayes factors are large.
It can be observed from Table 10.1 that M5 associated with the constraint

1

= 
2
 , and M6 associated with the constraint 
1

= 
2
 , are better than
M1 with no constraints, whilst M2M3 and M4 with the other constraints are
worse than M1. As

log B65 = 910M6 is better than M5.
The interpretations of the above model comparison results in the context of
the two SEMs for Canada and UK are: the factor loading matrices are different,
regression coefficients in the structural equation of the exogenous latent vari-
ables to the endogenous variables are different, the covariance matrices of the
exogenous latent variables are different and the covariance matrices of the error
measurements in the measurement equation are different; however, the variances
of the residuals in the structural equations are equal. Basically, we can conclude
that the measurement and structural equations of these two groups are different.
Table
10.1
Estimated log Bayes factors: log Bjk.
Model k
Model j
M1
M2
M3
M4
M5
M6
M1
–
M2
−160
–
M3
−516
−356
–
M4
−156
004
3.60
–
M5
011
171
5.27
1.67
–
M6
101
261
6.17
2.57
0.91
–
Note:

log Bjk = jk entry ×103; hence

log B21 = −1600 and

log B65 = 910.

10.4
NUMERICAL ILLUSTRATIONS
305
Table
10.2
Bayesian estimates and standard error estimates under M6.
UK
CANADA
UK
CANADA
PAR
EST
SE
EST
SE
PAR
EST
SE
EST
SE
1
5313
0068
3936
0067
1
0404
0104
0347
0116
2
5493
0074
4831
0069
2
0193
0114
0293
0092
3
3949
0063
3109
0056
3
−0277
0204
0035
0126
4
−0566
0051
−0761
0060
11
0827
0133
0747
0209
5
−1225
0096
−1444
0130
12
0052
0042
0077
0042
6
−1132
0048
−1054
0050
22
0605
0102
0885
0141
7
0627
0047
0944
0055
1
3814
0275
4268
0313
8
0259
0042
0601
0050
2
5753
0294
7136
0279
9
−0269
0027
−0021
0022
3
4269
0217
3886
0190
21
0879
0068
0633
0065

3077
0290
3077
0290
31
0747
0059
0648
0061
52
1260
0237
1374
0288
62
0420
0082
0681
0129
83
0945
0139
0968
0136
93
1197
0219
0972
0161
4.5
4
3.5
3
2.5
2
1.5
1
0.5
00
1000
2000
3000
4000
5000
1.2
Figure
10.1
EPSR values against the number of iterations in the ICPSR data
under M6.
The Bayesian estimates of the unknown parameters in M6 are presented in
Table 10.2. To reveal the convergence of the MCMC algorithm in analyzing
multisample SEMs, the EPSR values, and sequences of generated observa-
tions corresponding to some parameters in the two groups, are presented
in Figures 10.1, 10.2 and 10.3, respectively. We first interpret the Bayesian

306
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
0
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
0
2
4
6
8
μ1
(1)
0
0.5
1
λ21
(1)
2
4
6
8
10
ψε2
(1)
−1
0
1
γ 2
(1)
2
3
4
5
ψδ
−0.5
0
0.5
1
φ12
(1)
Figure
10.2
Plots of parallel sequences of 1
1 1
21 , 1
2 , 1
2 1

and 1
12 in the
ICPSR data under M6.
0
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
0
1000
2000
3000
4000
5000
0
2
4
6
8
μ 1
(2)
0
0.5
1
λ21
(2)
2
4
6
8
10
ψε2
(2)
γ 2
(2)
−1
0
1
2
3
4
5
ψδ
−0.5
0
0.5
1
φ12
(2)
Figure
10.3
Plots of parallel sequences of 2
1 2
21 , 2
2 , 2
2 2

and 2
12 in the
ICPSR data under M6.

10.4
NUMERICAL ILLUSTRATIONS
307
estimates in relation to UK. Note that for nonlinear SEMs, the intercept
g is not equal to the mean of vg. Equation (8.4) in Chapter 8 can be
used in the same way here to evaluate the mean of vg. From the specifica-
tions (see Equation (10.11)) of the nonlinear SEM in analyzing this example,
E
1
1  = E
1
2  = 0 and ˆE
1
1 
1
2  = ˆ
1
12 = 0053, so we can compute the
estimate of Evk for every k = 1   9. For k = 12 and 3 we have ˆEv
1
1  =
ˆ
1
1 + ˆ
1
3 ˆE
1
i1 
1
i2  = 5314 + 10 × −0277 × 0053 = 5299, and similarly
ˆEv
1
2  = 5481 and ˆEv
1
3  = 3938. For k = 4   9 ˆEv
1
k  = ˆk. Hence,
ˆEv
1
k  and ˆ
1
k
are close or equal to each other. The interpretation of the esti-
mated means ˆEv
1
1 , ˆEv
1
2  and ˆEv
1
3  that correspond to the indicators of
‘job attitude’ is standard, because it is based on a 10-point scale that is treated
as continuous. From ˆEv
1
k k = 456 that correspond to the dichotomous
indicators, we know that the mean scores of the items that served as indicators
for ‘emotion’ shift to the left. Hence, it seems that on the average, the British
do have good emotions. From ˆEv
1
k k = 789, it seems that they are more
concerned about ‘good pay’ and ‘good job security’, but less concerned about
‘good chances for promotion’. All of the factor loading estimates are quite large.
This indicates a strong association between each of the latent variables and their
respective indicators. The estimated nonlinear structural equation is given by:

1
i
= 0404 
1
i1 +0193 
1
i2 −0277 
1
i1 
1
i2 
(10.13)
Before giving the interpretations of this nonlinear structural equation, we note
again from the scales of the dichotomous indicators in relation to ‘emotion, 1’
and ‘benefit attitude, 2’ that a comparatively larger (positive) value of 1 implies
that an individual has worse emotions, and a comparatively larger (positive)
value of 2 implies that an individual is more concerned about benefits. More-
over, a comparatively large (positive) value of  implies a comparatively bad
job attitude. With the above understanding of the latent variables, it follows
from ˆ
1
1
= 0404 and ˆ
1
2
= 0193 that worse emotions, and more concern
about benefits, imply a worse job attitude. From ˆ
1
3
= −0277, ‘emotion, 
1
1 ’
and ‘benefit attitude, 
1
2 ’ have an interaction effect on ‘job attitude’. The
basic interpretation is that the ‘additive’ effect of the linear latent variables of
‘emotion, 
1
1 ’, and ‘benefit attitude, 
1
2 ’ in the structural equation is inade-
quate to account for their relationships with the latent variable ‘job attitude, 1’
and an interaction term of 
1
1
and 
1
2
has to be added. Depending on various
situations, this interaction term (with a negative sign) has different effects. For
example, it can indicate that: (i) for employees with good emotions (negative

1
i1 ), less concern about benefits (negative 
1
i2 ) would have a positive effect on
job attitude (−0277 
1
i1 
1
i2 would decrease the value of 
1
i ), whereas more
concern about benefits (positive 
1
i2 ) would have a negative effect on job atti-
tude (−0277 
1
i1 
1
i2 would increase the value of 
1
i ). These interpretations
may provide useful insights for administrators to improve the management of

308
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
their organizations. From ˆ
1
11  ˆ
1
12 and ˆ
1
22 , the estimate of the correlation of
‘emotion’ and ‘benefit attitude’ is equal to 0.075. As expected, the correla-
tion of these two latent variables is small. The interpretations of the Bayesian
estimates of unknown parameters in the nonlinear SEM that corresponds to
Canada are similar.
The following similarities and differences are observed in comparing the
Bayesian estimates of these two countries. (i) Although the magnitudes are
different, the mean behavior of manifest variables v1 to v9 (corresponding to
questions 1–9) is similar in the sense that 
1
k
and 
2
k
have the same sign.
(ii) The factor loading estimates in the Canada model are also quite large,
thus indicating a strong association between the indicators (manifest variables)
and their latent variables. (iii) The interaction effect of ‘emotion’ and ‘benefit
attitude’ in the Canada model is very minor. (iv) As shown by the model
comparison result, the variances of the residual in the structural equations of
the models in the two countries are equal. (v) In general, the standard error
estimates of the Bayesian estimates are quite small. This indicates a nice feature
of the Bayesian application to this real example.
10.4.2
Analysis of Multisample Quality of Life Data
via WinBUGS
Analysis of single group quality of life (QOL) data has been considered in
Chapter 6, Section 6.6. Here, we describe the Bayesian methods in analyzing
multisample QOL data. As the latent constructs of QOL can be naturally
regarded as latent variables that are reflected by the related items (observed
variables) in the questionnaire, multisample factor analysis and multisample
structural equation models have been used in analyzing QOL data. For instance,
Power et al. (1999) applied a multisample model to investigate whether the
WHOQOL instrument is structurally comparable in different cultures, and
Meuleners, Lee, Binns and Lower (2003) applied a LISREL model to analyze
the QOL for adolescents. However, the above cited work, as well as most
applications of the factor analysis model to QOL, are based on the assumption
that the data are continuous and normally distributed.
The WHOQOL-BREF (Power et al., 1999) instrument was taken from the
WHOQOL-100 instrument by selecting 26 ordered categorical items out of 100
original items. The observations were taken from 15 international field centers,
one of which is China, and the rest are Western countries, such as the UK, Italy
and Germany. The first two items are the overall QOL and general health, the
next seven items address physical health, the next six items address psychological
health, the three items that follow are for social relationships and the last eight
items address the environment. All of the items are measured with a five-point
scale (1 = ‘not at all/very dissatisfied’; 2 = ‘a little/dissatisfied’; 3 = ‘moderate/
neither’; 4 = ‘very much/satisfied’; and 5 = ‘extremely/very satisfied’).

10.4
NUMERICAL ILLUSTRATIONS
309
To illustrate the Bayesian methodology, we use synthetic two-sample data
that mimic the QOL study with the same items as mentioned above for each
sample, see also Chapter 6, Section 6.6. Hence, we consider a two-sample SEM
on the basis of a simulated data set of randomly drawn observations from two
populations. The sample sizes are N1 = 338 and N2 = 247. We note that several
items are seriously skew to the right. Treating these discrete data as coming
from a normal distribution may lead to a misleading conclusion.
We apply the multisample SEM as defined in Equations (10.1) and (10.2)
with G = 2 to analyze the data. In the Bayesian analysis, we identify the ordered
categorical variables by the method described in Section 10.2, using the first
group g = 1 as the reference group. Based on the meaning of the questions,
we use the following non-overlapping g for clear interpretation of latent
variables: For g = 12
gT =
⎡
⎢⎢⎣
1
g
21
0
0
···
0
0
0
···
0
0
0
0
0
0
···
0
0
0
1
g
42
···
g
92
0
0
···
0
0
0
0
0
0
···
0
0
0
0
0
···
0
1
g
113
···
g
153
0
0
0
0
0
···
0
0
0
0
0
···
0
0
0
···
0
1
g
174
g
184
0
0
···
0
0
0
0
0
···
0
0
0
···
0
0
0
0
1
g
205
···
g
265
⎤
⎥⎥⎦
(10.14)
where 1s and 0s are fixed parameters. In a similar way to the analysis presented
in Section 6.6, the latent variables in 
gT
i
= 
g
i 
g
i1 
g
i2 
g
i3 
g
i4  are inter-
preted as ‘health related QOL, ’, ‘physical health, 1’, ‘psychological health,
2’, ‘social relationship, 3’ and ‘environment, 4’. The measurement equation
in the model is given by
v
g
i
= g +g
g
i +
g
i 
g = 12
(10.15)
with g defined as above. The following structural equation is used to assess
the effects of the latent constructs in 
g
i
to the health related QOL, 
g
i :

g
i
= 
g
1 
g
i1 +
g
2 
g
i2 +
g
3 
g
i3 +
g
4 
g
i4 +
g
i 
(10.16)
In the Bayesian analysis, the prior inputs of the hyperparameters in the conjugate
prior distributions given at the end of Section 10.3.1 are taken as: 
g
0k = 
g
0k =
10, 
g
0k = 
g
0k = 8, elements in 
g
0k are taken as 0.8, elements in 
g
0	k are
taken as 0.6, H
g
0yk and H
g
0	k are diagonal matrices with diagonal elements 0.25,
R
g
0
= 8I4 and 
g
0 = 30. Note that the prior distributions and the prior inputs
are appropriately modified for handling situations with constraints.
Three multisample models M1, M2 and M3 that are respectively associ-
ated with following hypotheses are considered: H1  no constraints H2  1 =
2 and H3  1 = 21 = 2. The software WinBUGS was applied

310
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
to obtain the Bayesian results. In the analysis, the number of burn-in iter-
ations was 10 000 and 10 000 observations were collected after convergence
to produce the results. The DIC values corresponding to M1, M2 and M3
are equal to 32 302.6, 32 321.7 and 32 341.9, respectively. Hence, model M1
with the smallest DIC value is selected. Note that one may consider other
competing models if desirable. The Bayesian estimates and standard error esti-
mates produced by WinBUGS under M1 are presented in Table 10.3. The
Table
10.3
Bayesian estimates of unknown parameters in the two-group SEM with
no constraints
Group
Group
Group
PAR
g = 1
g = 2
PAR
g = 1
g = 2
PAR
g = 1
g = 2
1
0.021
−0519
21
0859
0804
1
0.847
0539
2
0.001
0.059
42
0952
0754
2
0.334
0139
3
0.002
−0240
52
1112
1016
3
0.167
0026
4
0.009
−0300
62
1212
0976
4
−0068
0241
5
−0004
−0188
72
0820
0805
1
0.400
0248
6
0.008
−0382
82
1333
1123
2
0.422
0268
7
−0002
−0030
92
1203
0961
3
0.616
0584
8
0.008
−0070
113
0799
0827
4
0.628
0445
9
0.003
0.108
123
0726
0987
5
0.462
0214
10
0.004
−0358
133
0755
0669
6
0.401
0184
11
0.003
−0286
143
1011
0762
7
0.709
0253
12
0.001
−0087
153
0874
0719
8
0.271
0202
13
0.004
−0373
174
0273
0627
9
0.393
0191
14
0.003
0.031
184
0954
0961
10
0.471
0288
15
0.002
0.079
205
0804
1108
11
0.654
0262
16
0.012
−0404
215
0772
0853
12
0.707
0428
17
0.000
0.037
225
0755
0815
13
0.698
0348
18
0.010
−0596
235
0723
0672
14
0.453
0269
19
0.005
−0183
245
0984
0647
15
0.575
1137
20
0.004
−0543
255
0770
0714
16
0.462
0267
21
0.003
−0571
265
0842
0761
17
0.962
0297
22
0.002
−0966
11
0450
0301
18
0.522
0301
23
0.001
−0220
12
0337
0279
19
0.530
0559
24
0.017
−1151
13
0211
0162
20
0.679
0565
25
−0001
−0837
14
0299
0207
21
0.708
0392
26
0.007
−0982
22
0579
0537
22
0.714
0386
23
0390
0251
23
0.736
0493
24
0393
0290
24
0.577
0451
33
0599
0301
25
0.719
0482
34
0386
0210
26
0.670
0408
44
0535
0386

0246
0234

10.4
NUMERICAL ILLUSTRATIONS
311
(a)
0
100
200
300
–2 –1
0
2
1
(b)
0
100
200
300
–2 –1
0
2
1
(c)
0
100
200
300
–2 –1 0
2
1
(d)
0
100
200
300
–1.0–0.5 0.0 0.5 1.0
Figure
10.4
Estimated residual plots versus case numbers (a) ˆ1
i1 , (b) ˆ1
i2 , (c) ˆ1
i3
and (d) ˆ1
i .
estimated residuals ˆ
g
i
and ˆ
g
i
under M1 can be obtained in the same way as in
previous chapters. For group one, some estimated residual plots, ˆ
1
i1 , ˆ
1
i2 , ˆ
1
i3
and/or ˆ
1
i
versus case numbers, are displayed in Figure 10.4. Some estimated
residual plots of ˆ
1
i1 and ˆ
1
i
versus ˆ
1
i1 , ˆ
1
i2 , ˆ
1
i3 , ˆ
1
i4 and/or ˆ
1
i
are presented
in Figures 10.5 and 10.6, respectively. Other plots are similar. Interpretations of
these plots are the same as before. The WinBUGS codes in analyzing models M1,
M2 and M3 are given in the website: http://www.wiley.com/go/lee_structural.

312
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
(a)
hat{epsilon}^(1)_i1
–1
0
1
–2 –1
0
1
2
–2 –1
0
1
2
hat{epsilon}^(1)_i1
–2
–1
0
1
2
(b)
–2 –1
0
1
2
hat{epsilon}^(1)_i1
–2
–1
0
1
(c)
–2 –1
0
1
2
hat{epsilon}^(1)_i1
–2
–1
0
1
2
(d)
–2 –1
0
1
2
(e)
hat{epsilon}^(1)_i1
–2
–1
0
1
2
Figure
10.5
Plots of estimated residuals ˆ1
i1 versus (a) ˆ1
i1 , (b) ˆ1
i2 , (c) ˆ1
i3 , (d) ˆ1
i4
and (e) ˆ1
i .

APPENDIX 10.1
313
2
(a)
hat{delta}^(1)_i
–1
0
1
–2 –1 0
1
2
(b)
–2 –1 0
1
2
hat{delta}^(1)_i
–2 –1 0
1
2
hat{delta}^(1)_i
–2 –1 0
1
2
hat{delta}^(1)_i
–2
–1
0
1
(c)
–2
–1
0
1
(d)
–2
–1
0
1
2
Figure
10.6
Plots of estimated residuals ˆ1
i
versus (a) ˆ1
i1 , (b) ˆ1
i2 , (c) ˆ1
i3 and
(d) ˆ1
i4 .
APPENDIX 10.1: CONDITIONAL DISTRIBUTIONS:
MULTISAMPLE SEMs
The
conditional
distributions
YXZ,
YXZ
and
YXZ that are required in the implementation of the Gibbs sampler
are presented in this appendix. Note that the results on the first two condi-
tional distributions are natural extensions of those given in Chapter 6, but they
can be regarded as the special cases of those given in Chapter 9. Also note
that we allow common parameters in  according to the constraints under the
competing models.

314
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
Conditional distribution of YXZ can be obtained as below:
pYXZ =
G
g=1
Ng

i=1
p
g
i v
g
i g
where
p
g
i v
g
i g ∝exp

−1
2

v
g
i −g−g
g
i T  
g−1v
g
i
−g −g
g
i 
+
g
i −g
	 H∗
g
i T 
g

−1
g
i −g
	 H∗
g
i  +
gT
i
g−1
g
i


(A10.1)
Since the conditional distribution of (A10.1) is not standard, the Metropolis–
Hasting (MH) algorithm can be used to draw random observations from this
distribution.
Under the multisample situation, the notation in the conditional distribution
Y, XZ is very tedious. The derivation is similar to the two-level
case as given in Chapter 9, Appendix 9.1, Equations (A9.3) and (A9.4). As
gYg is independent with hYh for g ̸= h, and 
g

is diagonal,
pY· =
G
g=1
pgYg· =
G
g=1
s
k=1
p
g
k Y
g
k ·
(A10.2)
where Y
g
k
= y
g
1k    y
g
Ng k. Let 
g
k be the kth diagonal element of  g
 , 
g
k
be the kth element of g, 
gT
k
be the kth row of g and IAy be an
indicator function with value 1 if y in A and zero otherwise; pY· can be
obtained from Equation (A10.2) and
p
g
k Y
g
k · ∝
Ng

i=1

g
k
−1/2y
g
ik −
g
k −
gT
k

g
i Ikzik kzik+1y
g
ik 
(A10.3)
where  is the probability density function of N01. Note that in Equa-
tion (A10.3) the superscript ‘g’ in the threshold is suppressed to simplify
notation.
For dichotomous data, no thresholds are involved in the model. The condi-
tional distribution of interest is pYXZ. Based on the definition and

APPENDIX 10.1
315
the properties of the data, let y
g
ik
be the kth element of y
g
i
and z
g
ik
be its
corresponding dichotomous observations, k = 1   s, so it can be shown that
pYXZ =
G
g=1
Ng

i=1
s
k=1
py
g
ik ·
py
g
ik ·
D=
⎧
⎨
⎩
N
g
k +
g
k
T 
g
i 
g
k I−0y
g
ik 
z
g
ik = 0
N
g
k +
g
k
T 
g
i 
g
k I0y
g
ik 
z
g
ik = 1
(A10.4)
Under the prior distributions of components in  as given in Section 10.3.1,
the conditional distribution YXZ is presented. Note that as Y is
given, the model is defined with continuous data; hence, the conditional distri-
bution is independent of  and Z.
The conditional distribution of some components in g g = 1   G
under the situation without any parameter constraints are given as follows. Let

g
k
T be the kth row of g, 
g
k be the kth diagonal element of 
g
 , V
∗gT
k
be
the kth row of V∗g = v
g
1 −g   v
g
Ng −g and 
g
2 = 
g
1    
g
Ng . It
can be shown that:
gg g
 YX
D= Nag
 Ag
 

g
k  g
 gYX
D= Na
g
k A
g
k 

g−1
k

g
k gYX
D= GammaNg/2+
g
0k
g
k 
(A10.5)
g
g
2 
D= IWq2R
g−1
0
+
g
2 
g
2
T Ng +
g
0 
in which
ag
 = Ag



g−1
0

g
0 +Ng g−1

¯vg −g ¯g

Ag
 =


g−1
0
+Ng g−1

−1

a
g
k
= A
g
k

H
g−1
0yk

g
0k +
g−1
k
gV
∗g
k

A
g
k
=


g−1
k
ggT +H
g−1
0yk
−1

g
k = 
g
0k + 1
2


gT
k

g
k 
gT
k

g
k −2
gT
k
gV
∗g
k
+V
∗gT
k
V
∗g
k


(A10.6)
with ¯vg =
Ng

i=1
v
g
i /Ng and ¯g =
Ng

i=1

g
i /Ng being the means of v
g
i
and 
g
i
within the gth group.

316
10
MULTISAMPLE ANALYSIS OF STRUCTURAL EQUATION MODELS
As we mentioned, slight modifications are required to handle models with
parameter constraints, see Section 10.3.1. Under the constraints 
1
k
= ··· =

G
k
= k, the conjugate prior distribution of k is N0kH0yk and the condi-
tional distribution is
k 
1
k    
G
k  1   G Y  X
D= NakAk
(A10.7)
where ak = AkH−1
0yk0k +
G
g=1

g−1
k
gV
∗g
k
, and Ak = 
G
g=1

g−1
k
ggT +
H−1
0yk−1. Under the constraints 
1
k = ··· = 
G
k = k, the conjugate prior
distribution of k is Gamma0k0k, and the conditional distribution is
−1
k  
1
k    
G
k 1   G Y  X
D= GammaN ∗/2+0kk
(A10.8)
where N ∗= N1 +··· +NG
k = 0k + 1
2
G
g=1


gT
k
ggT 
g
k −2
gT
k
g V
∗g
k
+V
∗gT
k
V
∗g
k


Under the constraints 1 = ··· = G = , the conjugate prior distribution
of −1 is Wq2R00 and the conditional distribution is
  
1
2 ··· 
G
2 
D= IWq2

R −1
0
+
G
g=1

g
2 
gT
2

 N ∗+0

 (A10.9)
The conditional distributions of 
g
k
and 
g
k
are similar, and hence not
presented.
As the conditional distributions involved in Equation (A10.5) or (A10.7)–
(A10.9) are standard distributions, drawing observations from them is straight-
forward. Simulating observations from the conditional distributions that are
given in Equation (A10.4) or (A10.3) involves the univariate truncated normal
distribution, and this is done by the inverse distribution method proposed by
Devroye (1985). A Metropolis–Hasting (MH) algorithm is used to simulate
observations from the complex conditional distributions of Equations (A10.1)
and (10.3).
REFERENCES
Bollen, K. A. (1989) Structural Equations with Latent Variables. New York: John
Wiley & Sons, Inc..
Devroye, L. (1985) Non-Uniform Random Variate Generation. New York: Springer
Verlag.

REFERENCES
317
Gelman, A. and Meng, X. L. (1998) Simulating normalizing constant: from importance
sampling to bridge sampling to path sampling. Statistical Science, 13, 163–185.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distributions and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Lee, S. Y., Poon, W. Y. and Bentler, P. M. (1989) Simultaneous analysis of multivariate
polychoric correlation model in several groups. Psychometrika, 54, 63–73.
Lee, S. Y., Poon, W. Y. and Bentler, P. M. (1995) A two-stage estimation of struc-
tural equation models with continuous and polytomous variables. British Journal of
Mathematical and Statistical Psychology, 48, 339–358.
Lee, S. Y., Song, X. Y., Skevington, S. and Hao, Y. T. (2005) Application of structural
equation models to quality of life. Structural Equation Modeling: A Multidisciplinary
Journal, 12, 435–453.
Meuleners, L. B., Lee, A. H., Binns, C. W. and Lower, A. (2003) Quality of life for
adolescents: assessing measurement properties using structural equation modeling.
Quality of Life Research, 12, 283–290.
Power, M., Bullingen, M., Hazper, A. and WHOQOL Group (1999) The World Health
Organization WHOQOL-100: tests of the universality of quality of life in 15 different
cultural groups worldwide. Health Psychology, 18(5), 495–505.
Shi, J. Q. and Lee, S. Y. (2000) Latent variable models with mixed continuous and
polytomous data. Journal of the Royal Statistical Society, Series B, 62, 77–87.
Song, X. Y. and Lee, S. Y. (2001) Bayesian estimation and test for factor analysis
model with continuous and polytomous data in several populations. British Journal of
Mathematical and Statistical Psychology, 54 237–263.
World Values Study Group (1994) World Values Survey, 1981–1984 and 1990–1993.
ICPSR version. Ann Arbor, MI: Institute for Social Research (producer). Ann Arbo,
MI: Inter-university Consortium for Political and Social Research (distribution).


11
Finite Mixtures in
Structural Equation
Models
11.1
INTRODUCTION
In behavioral science such as psychology, sociology, education, etc., hetero-
geneity of population is inevitable and is an important concern. The result would
be seriously distorted if analyzing a heterogeneous population as a homoge-
nous population. Ordinary multiple group methods should not be used, unless
the membership of each independent observation can be specified accurately.
Recently, finite mixtures have been applied to structural equation models in
order to deal with heterogeneous populations.
In general, a finite mixture model (see Redner and Walker, 1984;
Titterington, Smith and Markov, 1985) arises with a population which is
a mixture of K components with associative probability densities fkk =
1   K and mixing proportions kk = 1   K. Mixture models arise
in many fields, including behavioral, medical, economics and environmental
sciences. They have been used in modeling heterogeneity, handling outliers
(Pettit and Smith, 1985) and density estimation (Roeder and Wasserman,
1997). It is well recognized that statistical analysis of mixture models is
not straightforward. For estimation with a fixed number of components K,
numerous methods have been proposed. Examples are the method of moments
(Lindsay and Basak, 1993), Bayesian methods with MCMC techniques (Diebolt
and Robert, 1994; Robert, 1996), and the ML method (Hathaway, 1985).
For mixture models with K treated as random, Richardson and Green (1997)
developed a full Bayesian analysis on the basis of the reversible jump MCMC
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

320
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
method introduced by Green (1995). For the challenging problem of testing
the number of components, the classical likelihood-based inference encoun-
tered serious difficulties. Due to some non-regular problems, some standard
asymptotic properties associated with the ML estimation and the likelihood
ratio test are not valid. To deal with problems involved in the likelihood ratio
test, some bootstrap methods have been proposed (see Feng and McCulloch,
1996; McLachlan, 1987). However, as pointed out by Richardson and Green
(1997), the Bayes paradigm is particularly suitable for analyzing mixture models
with an unknown K.
In the field of SEM, Arminger and Stein (1997) used a two-stage method to
analyze finite mixtures of conditional distributions with covariance structures.
Jedidi, Jagpal and DeSarbo (1997a) analyzed the finite mixtures of multivariate
regression and simultaneous equation models, while Jedidi, Jagpal and DeSarbo
(1997b) considered the estimation of a general finite mixtures of structural
equation models and gave a brief discussion on the problem of model selection
via the Bayesian information criterion (BIC) with a fixed number of compo-
nents. Yung (1997) investigated finite mixtures of confirmatory factor analysis
models. He proposed an approximated scoring algorithm and an EM algo-
rithm to solve the likelihood equation. Dolan and van der Maas (1998) applied
a quasi-Newton algorithm to finite mixtures and inferred the estimation by
changing the degree of separation and the sample size. Arminger, Stein and
Wittenberg (1999) discussed ML analysis for mixtures of conditional mean-
and covariance-structure models, and three estimation strategies on the basis
of the EM algorithm were established. They also pointed out the difficulty of
the usual likelihood ratio test for testing the number of unknown components,
briefly discussed an ad hoc test and proposed a procedure that uses the para-
metric bootstrap to construct an estimate of the distribution of the likelihood
ratio test under the null hypothesis. The test is based on the unrestricted param-
eter estimates of the variances/covariances in the covariance matrices. Detailed
statistical properties and justifications associated with these tests have not been
established.
Zhu and Lee (2001) proposed a Bayesian analysis to finite mixtures in the
LISREL model, using the idea of augmenting the observed data with latent
variables and allocation variables. Joint Bayesian estimates of the mixing propor-
tions, structural parameters and latent variables were obtained via some MCMC
methods. Lee and Song (2003b) developed a path sampling procedure to
compute the observed data log-likelihood, for evaluating the BIC in selecting
the appropriate number of components for a mixture SEM with missing data.
A Bayesian approach to analyze mixtures of SEMs with an unknown number
of components has been developed by Lee and Song (2002). They formulated
the problem as a model selection problem for selecting one of the two mixture
SEMs with different numbers of components. Their approach is based on the
Bayes factor (Berger, 1985; Kass and Raftery, 1995), which is computed via a
path sampling (Gelman and Meng, 1998) procedure. Spiegelhalter, Thomas,

11.2
FINITE MIXTURES IN SEMS
321
Best and Lunn (2003) pointed out that DIC may not be appropriate for model
comparison in the context of mixture models. Hence, for mixture models,
WinBUGS does not give DIC results.
For a mixture SEM with K components, it is well-known that the model
is invariant with respect to permutation of the labels k = 1   K. Thus, the
model is not identified, and adoption of a unique labelling for identifiability is
important. In the literature, a common method is to use some constraints on
the components of the mean vector to force a unique labelling. In a Bayesian
approach, arbitrary constraints may not be able to solve the problem. In this
chapter, we will apply the permutation sampler (Frühwirth-Schnatter, 2001) to
find the appropriate identifiability constraints. We start with the description of
finite mixtures in structural equation models, then move to the Bayesian esti-
mation of the model and the Bayesian model comparison. The methodologies
will be illustrated with examples. The application of WinBUGS in obtaining
some Bayesian results is presented in Section 11.4.1.
11.2
FINITE MIXTURES IN SEMS
Let yi be a p × 1 random vector corresponding to the ith observation in a
random sample of size n, and suppose that its distribution is given by the
following probability density function:
fyi =
K
k=1
kfkyikk
i = 1   n
(11.1)
where K is a given integer, k is the unknown mixing proportion such that
k > 0 and 1 +···+K = 1	0fkykk is the multivariate normal density
function with an unknown mean vector k and a general covariance structure
k = kk that depends on an unknown parameter vector k, and  is the
parameter vector that contains all unknown parameters in kk and kk =
1   K. As the LISREL type model (Jöreskog and Sörbom, 1996) is a very
popular model, it will be used here as a representative model for the random
vector yi conditional on the kth component. For the kth component, the
measurement equation of the model is given by:
yi = k +kki +ki
(11.2)
where k is the mean vector, k is the p × q factor loading matrix, ki is a
random vector of latent variables and ki is a random vector of residuals which is
distributed according to N 
0 k, where  k is a diagonal matrix. It is assumed
that ki and ki are independent. Let ki = 	T
ki
T
kiT be a partition of ki

322
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
into an endogenous latent vector 	ki and an exogenous latent vector 
ki. The
structural equation of the model, which describes the relationships among the
latent variables, is defined as
	ki = k	ki +k
ki +ki
(11.3)
where 	ki and 
ki are q1 × 1 and q2 × 1 subvectors of ki respectively, ki is a
random vector of residuals that is independent of 
kik and k are unknown
parameter matrices such that −1
0k = I−k−1 exists and 0k is independent of
elements in k. Let the distributions of 
ki and k be N 
0k and N 
0 k,
respectively, where  k is a diagonal matrix. The parameter vector k in the kth
component contains the free unknown parameters in kkkk k and
 k. The covariance structure of ki is given by
k =
⎡
⎣
−1
0k kkT
k + k−1
0k T
−1
0k kk
kT
k −1
0k T
k
⎤
⎦
(11.4)
and kk = kkT
k + k. Any of these unknown parameter matrices can be
invariant across components. However, it is important to assign a different k
in the measurement Equation (11.2) of each component, in order to analyze
effectively data from the heterogenous populations that are different by their
mean vectors. We do not formulate mixtures of SEMs with different mean
vectors in the latent vector k (or 
k) and allow the same k =  for the manifest
vector y. The main reason is that for the kth component under such formulation,
Ey = +kEk; hence, Ey is not directly related to Ek, but affected by
k which may be greatly influenced by the variation of population through k.
As the mixture model defined in Equation (11.1) is invariant with respect
to the permutation of labels k = 1   K, adoption of a unique labelling for
identifiability is important. Roeder and Wasserman (1997) and Zhu and Lee
(2001) proposed imposing the ordering 11 < ··· < K1 for solving the label
switching problem (jumping between the various labelling subspace), where k1
is the first element of the mean vector k. This method works fine if 11 < ··· <
K1 are well separated. However, if 11 < ··· < K1 are close to each other, it
may not be able to eliminate the label switching and may give incorrect results.
Hence, it is important to find an appropriate identifiability constraint. Here, the
random permutation sampler that is developed by Frühwirth-Schnatter (2001)
will be applied for finding the suitable identifiability constraints.
Moreover, for each k = 1   K, structural parameters in the covariance
matrix k corresponding to the model defined by Equations (11.2) and (11.3)
are not identified. This problem is solved by the common method in struc-
tural equation modeling by fixing appropriate elements in kk and/or k
at preassigned values that are chosen on a problem-by-problem basis. For clear

11.3
BAYESIAN ESTIMATION AND CLASSIFICATION
323
presentation of the Bayesian method, we assume that all the unknown parame-
ters in the model are identified.
The next section discusses Bayesian estimation with the permutation sampler,
and Bayesian classification. A simulation study and some illustrative exam-
ples, including one that is analyzed through WinBUGS, are presented in
Section 11.4. Bayesian model comparison is addressed in Section 11.5. Tech-
nical details are given in Appendices 11.1 and 11.2.
11.3
BAYESIAN ESTIMATION AND CLASSIFICATION
Let yk be the vector of unknown parameters in k and  k, and let k be the
vector of unknown parameters in kk, k and  k. Let y and  be the
vectors that contain the unknown parameters in 1   K, 1   K,
y1   yK and 1   K, respectively; and let  = y be
the overall parameter vector. Inspired by Zhu and Lee (2001) and other works
in finite mixture models, we introduce a group label wi for the ith observation
yi as a latent allocation variable, and assume that it is independently drawn from
the following distribution:
pwi = k = k
for
k = 1   K	
(11.5)
Moreover, let Y = y1   yn be the observed data matrix,  = 1   n
with i = ki if it is in the kth component, and W = w1   wn be the matrix
of allocation variables.
In a standard Bayesian analysis, we need to evaluate the posterior distribution
of the unknown parameters, pY. Due to the nature of the mixture model, this
posterior distribution is complicated. However, if W is observed, the component
of every yi can be identified and the mixture model becomes the more familiar
multiple group model. In addition, if  is observed, the underlying SEM will
become the linear simultaneous equation model which is comparatively easy to
handle. Hence, the observed data Y are augmented with the latent quantities 
and W in the posterior analysis. In the following sections, we will concentrate on
analyzing pWY, the posterior distribution of W given Y.
The label switching problem has to be solved in the posterior analysis. For
general mixture models with K components, the unconstrained parameter space
contains K! subspaces, each one corresponding to a different way to label the
states. In the current mixture of SEM, the likelihood is invariant to relabelling
the states. If the prior distributions of  and other parameters in  are also
invariant, the unconstrained posterior is invariant to relabelling the states and
identical on all labelling subspaces. This induces a multimodal posterior, and a
serious problem in Bayesian estimation.
We will use the MCMC approach proposed by Frühwirth-Schnatter (2001)
to deal with the above label switching problem. In this approach, an uniden-
tified model is first estimated by sampling from the unconstrained posterior

324
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
using the random permutation sampler, where each sweep is concluded by a
random permutation of the current labelling of the components. The random
permutation sampler delivers a sample that explores the whole unconstrained
parameter space and jumps between the various labelling subspace in a balanced
fashion. As pointed out by Frühwirth-Schnatter (2001), although the model is
unidentified, the output of the random permutation sampler can be used to
estimate unknown parameters that are invariant to relabelling the states, and
can be explored to find suitable identifiability constraints. Then, the model
is reestimated by sampling from the posterior distribution under the imposed
identifiability constraints, again using the permutation sampler. The implemen-
tation of the permutation sampler in relation to the mixtures of SEMs, and
the method of selecting the identifiability constraint are briefly described in
Appendices 11.1 and 11.2, respectively.
The Bayesian estimates of  and  will be obtained by computing the
posterior means of  and  in the posterior distribution of 
WY. The
main task is to simulate a sufficient large sample of observations from this
posterior distribution, then the Bayesian estimates can be approximated by the
sample means. Similar to many Bayesian analyses of SEMs, the Gibbs sampler
(Geman and Geman, 1984) is used to generate a sequence of observations from
pWY. The basic algorithm of this sampler is briefly given as below. At
the rth iteration with current values rr and Wr:
Step (a): Generate Wr+1r+1 from pWYr;
Step (b): Generate r+1 from pYr+1Wr+1.
Step (c): Reorder the label through the permutation sampler to make the
identifiability fulfill.
As pWY = pWYpYW, Step (a) can be further decom-
posed into the following two steps:
Step (a1): Generate Wr+1 from pWYr;
Step (a2): Generate r+1 from pYrWr+1.
Simulating observations W through Steps (a1) and (a2) is more efficient
than using Step (a).
Conditional distributions required for implementing the Gibbs sampler are
discussed below. As we have mentioned before, the finite mixtures model
becomes a multisample model with given W. Thus the conditional distributions
associated with the Gibbs sampler can be derived without much difficulty.
We first consider the conditional distribution associated with Step (a1). As
wi are independent,
pWY =
n
i=1
pwiyi	
(11.6)

11.3
BAYESIAN ESTIMATION AND CLASSIFICATION
325
Moreover,
pwi = kyi = pwi = kyi
pyi
=pwi = kpyiwi = kkk
pyi
=kfkyikk
pyi

(11.7)
where fkyikk is the probability density function of N 
kkk. Hence,
the conditional distribution of W given Y and  can be derived from Equa-
tions (11.6) and (11.7). It can be seen from Equation (11.7) that drawing
observations from pWYr is not difficult.
Consider the conditional distribution involved in Step (a2). Because i are
mutually independent with given wi, we have
n
i=1
piyiwi =pYW ∝pYWypW
=
n
i=1
pyiiwiypiwi	
(11.8)
Let Ck = −1
k +T
k  −1
k k; where k is the covariance matrix of ki in the kth
component (see Equation (11.4)). Moreover, as the conditional distribution of
i given  and wi = k is N 
0k, and the conditional distribution of yi given
iy, and wi = k is N 
k +kki k, it can be shown (see Lindley and
Smith (1972), pp. 4 and 5) that
piyiwi = k
D= N 
C−1
k T
k  −1
k yi −kC−1
k 	
(11.9)
The conditional distribution of pYW can be obtained from Equa-
tions (11.8) and (11.9). Drawing observations from this familiar normal distri-
bution is fast. Hence Step (a2) of the Gibbs sampler can be completed.
We now consider the conditional distribution pYW that is required
in Step (b) of the Gibbs sampler. This conditional distribution is quite compli-
cated. However, the difficulty can be reduced by assuming the following mild
conditions on the prior distribution of . We assume that the prior distribu-
tion of the mixing proportion  is independent of the prior distributions of
y and . Like many Bayesian analyses in SEMs, the prior distribution of
the mean vector  can be taken to be independent of the prior distributions
of the parameters y and  in the covariance structures. Moreover, when 
is given, the parameters in yk = k k are the parameters involved in the
linear regression model with the manifest variables in y (see Equation (11.2)),

326
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
and the parameters in k = kkk k are the parameters involved in
the other simultaneous equation model with the latent variables (see Equa-
tion (11.3)). Hence, we assume that the prior distributions of y and  are
independent. As a result, p = py = pppyp. More-
over, from the definition of the model and the properties of W and ,
we have pW = pWpYW = pYWypW and
pW = p. Consequently, the joint conditional distribution of  =
y can be expressed as
pWY = pyWY
∝pppyppWY
∝pppyppWpYW
∝pppyppWppYWy
= 
ppW
ppypYWy
pp	
(11.10)
Using this result, the marginal densities p·, py· and p· can be
treated separately.
The prior distribution of  is taken to be the symmetric Dirichlet distribution,
that is,  ∼D    with the probability density function given by
p = K
K 
1 ···
K
where · is the Gamma function. Since pW ∝K
k=1 nk
k , it follows from
Equation (11.10) that the full conditional distribution for  remains Dirichlet
in the following form:
p· ∝ppW ∝
K
k=1
nk+
k

(11.11)
where nk is the total number of i such that wi = k. Thus p· is distributed
as D+n1   +nK.
Let Yk and k be the respective submatrices of Y and , such that all the
ith columns with wi ̸= k are deleted. It is natural to assume that for k ̸= h,
kykk and hyhh are independent. Hence, given W, we have
pyYW ∝
K
k=1
pkpykpkpYkkkykpkk
(11.12)

11.3
BAYESIAN ESTIMATION AND CLASSIFICATION
327
and we can treat the product in Equation (11.12) separately for each k. With
W given, the original complicated problem of finite mixtures reduces to a much
simpler multisample problem. Here, for brevity, we assume that there are no
cross-group constraints, and the analysis can be carried out separately with each
individual sample. Situations with cross-group constraints can be handled in a
similar way to multiple groups analysis.
For mixture models, Diebolt and Robert (1994), and Roeder and Wasserman
(1997) pointed out that using fully noninformative prior distributions may lead
to improper posterior distributions. Thus most existing Bayesian analyses on
mixtures of the normal distribution used the conjugate type prior distributions
(see Roeder and Wasserman, 1997). This type of prior distribution for various
components of  is adopted here. Let k = kk, for m = 1   p and
l = 1   q1, for convenience, we take

kmkm
D= N 
0kmkmH0ykm −1
km
D= Gamma
0k0k

klkl
D= N 
0klklH0kl −1
kl
D= Gamma
0k0k
k
D= N 00 −1
k
D= Wq2
R00
(11.13)
where km and kl are the mth diagonal element of  k and the lth diag-
onal element of  k, respectively; T
km and T
kl are vectors that contain
unknown parameters in the mth row of k and the lth row of k, respec-
tively; 00k0k0km0k0k0kl and 0, and positive definite matrices
0H0ykmH0kl and R0 are hyperparameters whose values are assumed to be
given, and Wq2
·· denotes the Wishart distribution of dimension q2. More-
over, we also assume that kmkm is independent of khkh for m ̸= h,
and klkl is independent of khkh for l ̸= h.
Let 1k and 2k be submatrices of k that respectively contain the q1 rows
of k and the remaining q2 rows of 
k. It can be shown by similar derivations
as in previous chapters that the conditional distributions of components of k
are the following familiar normal, Gamma and inverted Wishart distributions:

kYkkk k
D= N 
−1
0 +nk −1
k −1nk −1
k
¯Yk +−1
0 0
−1
0 +nk −1
k −1

kmYkkkm−1
km
D= N 
aykmkmAykm

−1
kmYkkkm
D= Gamma
nk/2+0kkm

klYkk−1
kl
D= N 
aklklAkl

−1
klYkk
D= Gamma
nk/2+0kkl

k2k
D= IWq2
2kT
2k +R −1
0 nk +0

328
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
where ¯Yk = 	
iwi=kyi −kki/nk, and 	
iwi=k denotes the summation with
respect to those i such that wi = k, and
aykm = AykmH−1
0ykm0km +k ˜YT
km Aykm = H−1
0ykm +kT
k −1
km = 0k +2−1
 ˜Ykm ˜YT
km −aT
ykmA−1
ykmaykm +T
0kmH−1
0ykm0km
akl = AklH−1
0kl0kl +kT
1klAkl = H−1
0kl +kT
k −1
kl = 0k +2−1
1klT
1kl −aT
klA−1
klakl +T
0klH−1
0kl0kl
in which ˜Ykm is the mth row of ˜Yk which is a matrix whose columns are equal to
the columns of Yk minus k, and 1kl is the lth row of 1k. The computational
burden in simulating observations from these conditional distributions is light.
As the situation with fixed parameters can be handled in the same way as in
Zhu and Lee (2001), details are not presented here.
The conditional distributions given above are familiar and simple distribu-
tions. The computational burden required in simulating observations from these
distributions is not heavy. For brevity, we do not introduce any model to the
mean vector. Extension of the method discussed here to models with mean
structures is straightforward.
Let ttWtt = 1   T ∗ be the observations of W gener-
ated by the Gibbs sampler from the posterior distribution 
WY. The
Bayesian estimates of  and  can easily be obtained via the corresponding
sample means of the generated observations. For example,
ˆ = T ∗−1
T ∗

t=1
t
and ˆ = T ∗−1
T ∗

t=1
t	
(11.14)
These Bayesian estimates are consistent estimates of the corresponding posterior
means. It is rather difficult to derive analytic forms for the covariance matrices
VarY and VariY. However, their consistent estimates can be obtained
as follows:

VarY = T ∗−1−1
T ∗

t=1
t −ˆt −ˆT 
(11.15)

VarkiY = T ∗−1−1
T ∗

t=1

t
ki −ˆki
t
ki −ˆkiT  i = 1   n	
Standard error estimates of  can be obtained through the square roots of

VarY. Other statistical inferences on  or ki can be achieved based on the
simulated observations as well (see, for example, Besag, Green, Higdon and

11.3
BAYESIAN ESTIMATION AND CLASSIFICATION
329
Mengersen, 1995; Gilks, Richardson and Spiegelhalter, 1996). For k =
12   K estimated residuals can be obtained from ˆki and ˆk through Equa-
tions (11.2) and (11.3) as follows:
ˆki = yi −ˆk −ˆk ˆki and
ˆki = ˆ	ki −ˆk ˆ	ki −ˆk ˆ
ki
where the index i belongs to the set i  ˆwi = k.
In addition to their role in facilitating estimation, the allocation variables in
W also form a coherent basis for Bayesian classification of the observations.
Classification can either be addressed on a within-sample basis or a predictive
basis. Using the ‘percentage correctly classified’ loss function (see Richardson
and Green, 1997; Zhu and Lee, 2001), Bayesian classification of an existing
observation yi and a new observation y∗are respectively given by
ˆwi = argmaxkPrwi = kY
and
ˆw∗= argmaxkPrw∗= kYy∗	
The posterior probabilities Prwi = kyik = 1   K can be approximated
via the sample mean of the observations generated by the Gibbs sampler:
Prwi = kY ≈T ∗−1
T ∗

t=1
Iw
t
i
= k
where I is an indicator function. Consider the predictive classification in classi-
fying a new observation y∗. Let the corresponding allocation variable be w∗, the
Bayes classification needs to evaluate Prw∗= kYy∗. Inclusion of the addi-
tional y∗theoretically changes the posterior distributions, and the simulation
process should be rerun for each new y∗. This is obviously not practical. Hence,
we use the following approximation
Prw∗= kYy∗ =

pw∗= ky∗pYy∗d ≈

pw∗= ky∗pYd
and estimate the last integral by the following sample average of the generated
observations from the Gibbs sampler procedure:
Prw∗= kYy∗ ≈T ∗−1
T ∗

t=1


t
k fky∗
t
k 
t
k /
K
h=1

t
h fhy∗
t
h 
t
h 

	
The associated computing burden is light.

330
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
11.4
EXAMPLES AND SIMULATION STUDY
11.4.1
Analysis of an Artiﬁcial Example
An important issue in analyzing mixture SEMs is the separation of the compo-
nents. Yung (1997) and Dolan and van der Maas (1998) pointed out that
some statistical results they achieved cannot be trusted when the separation
is poor. Yung (1997) considered the following measure of separation: dkh =
maxl∈khk −hT −1
l k −h1/2 and recommended that dkh should be
about 3.8 or over. In view of this, an objective of this artificial example is to
investigate the performance of the proposed Bayesian approach in analyzing a
mixture of SEMs with two components which are not well separated. Another
objective is to demonstrate the random permutation sampler for finding suitable
identifiability constraints. Random observations are simulated from a mixture
SEM with two components defined by Equations (11.1), (11.2) and (11.3).
The model for each k = 12 involves nine manifest variables which are indica-
tors for three latent variables k, k1 and k2. The structure of the factor loading
matrix in each component is
T
k =
⎡
⎣
1	0
k21
k31
0
0
0
0
0
0
0
0
0
1	0
k52
k62
0
0
0
0
0
0
0
0
0
1	0
k83
k93
⎤
⎦
where the 1s and 0s are fixed parameters for achieving an identified covari-
ance structure, whilst the others are distinct unknown parameters. In the kth
component, the structural equation is given by: ki = k1ki1 + k2ki2 + ki,
where k1 and k2 are unknown parameters. The true population values are
given by: 1 = 2 = 0	5, 1 = 0	00	00	00	00	01	0 1	01	01	0T , 2 =
0	00	00	00	51	50	01	01	01	0T ; 121 = 131 = 183 = 193 = 0	4,
152 = 162 = 0	8, 221 = 231 = 283 = 293 = 0	8, 252 = 262 = 0	4, 11 =
0	2, 12 = 0	7, 21 = 0	7, 22 = 0	2, 111 = 122 = 211 = 222 = 1	0, 112 =
212 = 0	3, 11 = ··· = 19 = 21 = ··· = 29 = 11 = 21 = 0	5. In this two-
component mixture SEM, the total number of unknown parameters is 62. The
separation d12 is equal to 2.56, which is less than the suggested value in Yung
(1997).
Based on the above settings, we simulate 400 observations from each compo-
nent and the total sample size is 800. We focus on 1 (or 2) in finding a suit-
able identifiability constraint. The first step is to apply the random permutation
sampler to produce an MCMC sample from the unconstrained posterior with
size 5000 after a burn-in phase of 500 simulations. This random permutation
sampler delivers a sample that explores a whole unconstrained parameter space
and jumps between the various labelling subspaces in a balanced fashion. For a
mixture of SEMs with two components, we just have 2! labelling subspaces. In
the random permutation sampler, after each sweep the 1s and 2s are permuted

11.4
EXAMPLES AND SIMULATION STUDY
331
mu11
–1.0
0.0
1.0
2.0
mu11
–1.0
0.0
1.0
2.0
mu11
–1.0
0.0
1.0
2.0
mu11
–1.0
0.0
1.0
2.0
mu11
–1.0
0.0
1.0
2.0
mu11
–1.0
0.0
1.0
2.0
mu11
–1.0
0.0
1.0
2.0
mu11
–1.0
0.0
1.0
2.0
mu12
2.0
1.5
1.0
0.5
0.0
–0.5
–1.0
mu13
2.0
1.5
1.0
0.5
0.0
–0.5
–1.0
mu14
2.0
1.5
1.0
0.5
0.0
–0.5
–1.0
mu15
2.0
1.5
1.0
0.5
0.0
–0.5
–1.0
mu16
2.0
1.5
1.0
0.5
0.0
–0.5
–1.0
mu17
2.0
1.5
1.0
0.5
0.0
–0.5
–1.0
mu18
2.0
1.5
1.0
0.5
0.0
–0.5
–1.0
mu19
2.0
1.5
1.0
0.5
0.0
–0.5
–1.0
Figure
11.1
Scatterplots of MCMC output for components of 1.
randomly; that is, with probability 0.5, the 1s stay as 1s, and with probability
0.5 they become 2s. The output can be explored to find a suitable identifi-
ability constraint. Based on the reasoning given in Appendix 11.2, it suffices
to consider the parameters in 1. To search for an appropriate identifiability
constraint, we look at scatterplots of 11 versus 1l, l = 2   9, for obtaining
information on aspects of the states that are most different. These scatterplots
are presented in Figure 11.1. These plots clearly indicate that the two most
significant differences between the two components are sampled values corre-
sponding to 15 and 16. If permutation sampling is based on the constraint
15 < 25 or 16 > 26, the label switching will not appear.
Bayesian estimates are obtained by using the permutation sampler with the
identifiability constraint 15 < 25. Values of hyperparameters in the conjugate
prior distributions (see Equation 11.13) are taken as: for m = 1   9, 0m is
equal to the sample mean ¯ym, 0 = 102I, elements in 0km and 0kl (which
only involves the s) are taken to be true parameter values, H0ykm and H0kl are
the identity matrices, 0k = 0k = 10, 0k = 0k = 8, 0 = 6 and R −1
0
= 5I.
The  in the Dirichlet distribution of  is taken as 1. We conduct a few test
runs and find that the algorithm converged in less than 500 iterations. Based
on this finding, Bayesian estimates are obtained using a burn-in phase of 500
iterations and a total of 2000 observations are collected after the burn-in phase.
Results are reported in Table 11.1. We observe that the Bayesian estimates are
pretty close to their true parameter values.

332
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
Table
11.1
Bayesian estimates in the artificial example.
Component 1
Component 2
Par
EST
SE
Par
EST
SE
1 = 0	5
0	504
0	021
2 = 0	5
0	496
0	028
11 = 0	0
0	070
0	082
21 = 0	0
0	030
0	084
12 = 0	0
0	056
0	046
22 = 0	0
−0	056
0	061
13 = 0	0
0	036
0	045
23 = 0	0
−0	014
0	061
14 = 0	0
0	108
0	075
24 = 0	5
0	430
0	071
15 = 0	0
0	209
0	061
25 = 1	5
1	576
0	057
16 = 1	0
1	101
0	062
27 = 0	0
−0	084
0	052
17 = 1	0
1	147
0	112
27 = 1	0
0	953
0	110
18 = 1	0
1	033
0	046
28 = 1	0
1	041
0	056
19 = 1	0
0	974
0	046
29 = 1	0
0	941
0	057
121 = 0	4
0	322
0	052
221 = 0	8
0	851
0	060
131 = 0	4
0	411
0	052
231 = 0	8
0	810
0	060
152 = 0	8
0	712
0	060
252 = 0	4
0	498
0	067
162 = 0	8
0	695
0	066
262 = 0	4
0	480
0	064
183 = 0	4
0	386
0	075
283 = 0	8
0	738
0	077
193 = 0	4
0	428
0	079
293 = 0	8
0	826
0	083
11 = 0	2
0	236
0	104
21 = 0	7
0	817
0	104
12 = 0	7
0	740
0	074
22 = 0	2
0	210
0	074
111 = 1	0
1	017
0	121
211 = 1	0
0	820
0	118
112 = 0	3
0	249
0	090
212 = 0	3
0	283
0	074
122 = 1	0
0	900
0	185
222 = 1	0
0	982
0	163
11 = 0	5
0	535
0	092
21 = 0	5
0	588
0	080
12 = 0	5
0	558
0	046
22 = 0	5
0	489
0	053
13 = 0	5
0	510
0	045
23 = 0	5
0	565
0	057
14 = 0	5
0	483
0	067
24 = 0	5
0	620
0	085
15 = 0	5
0	492
0	056
25 = 0	5
0	556
0	056
16 = 0	5
0	554
0	063
26 = 0	5
0	507
0	050
17 = 0	5
0	696
0	126
27 = 0	5
0	569
0	108
18 = 0	5
0	563
0	052
28 = 0	5
0	578
0	062
19 = 0	5
0	566
0	053
29 = 0	5
0	508
0	065
11 = 0	5
0	549
0	094
21 = 0	5
0	549
0	082
This artificial data set has also been analyzed by using WinBUGS
(Spiegelhalter, Thomas, Best and Lunn, 2003). We first conduct an initial
Bayesian estimation without the identifiability constraints to identify the appro-
priate identifiability constraint 15 < 25 from the output as before. The model
is then reestimated with this identifiability constraint, and three starting values
of the parameters that are obtained from the sample mean, the fifth, and
95th percentile of the corresponding simulated samples. Bayesian estimates are
obtained by using the permutation samples with the identifiability constraint,
and the hyperparameter values given above. Results are presented in Table 11.2.

11.4
EXAMPLES AND SIMULATION STUDY
333
Table
11.2
Bayesian estimates in the artificial example via WinBUGS.
Component 1
Component 2
Par
EST
SE
Par
EST
SE
1 = 0	5
0	503
0	027
2 = 0	5
0	498
0	027
11 = 0	0
0	035
0	068
21 = 0	0
0	028
0	070
12 = 0	0
0	050
0	046
22 = 0	0
−0	043
0	062
13 = 0	0
0	034
0	046
23 = 0	0
−0	008
0	062
14 = 0	0
0	124
0	067
24 = 0	5
0	435
0	066
15 = 0	0
0	192
0	060
25 = 1	5
1	590
0	056
16 = 1	0
1	089
0	061
27 = 0	0
−0	065
0	053
17 = 1	0
1	068
0	067
27 = 1	0
0	961
0	066
18 = 1	0
1	024
0	046
28 = 1	0
1	053
0	056
19 = 1	0
0	974
0	047
29 = 1	0
0	916
0	057
121 = 0	4
0	355
0	047
221 = 0	8
0	850
0	053
131 = 0	4
0	428
0	047
231 = 0	8
0	817
0	053
152 = 0	8
0	723
0	062
252 = 0	4
0	507
0	065
162 = 0	8
0	741
0	068
262 = 0	4
0	495
0	064
183 = 0	4
0	403
0	058
283 = 0	8
0	741
0	061
193 = 0	4
0	385
0	061
293 = 0	8
0	837
0	064
11 = 0	2
0	208
0	071
21 = 0	7
0	873
0	103
12 = 0	7
0	740
0	094
22 = 0	2
0	152
0	068
111 = 1	0
0	953
0	115
211 = 1	0
0	785
0	110
112 = 0	3
0	278
0	071
212 = 0	3
0	305
0	067
122 = 1	0
0	932
0	133
222 = 1	0
0	988
0	117
11 = 0	5
0	496
0	072
21 = 0	5
0	519
0	059
12 = 0	5
0	536
0	043
22 = 0	5
0	512
0	052
13 = 0	5
0	504
0	042
23 = 0	5
0	566
0	054
14 = 0	5
0	517
0	067
24 = 0	5
0	620
0	076
15 = 0	5
0	492
0	056
25 = 0	5
0	536
0	052
16 = 0	5
0	542
0	063
26 = 0	5
0	518
0	051
17 = 0	5
0	636
0	093
27 = 0	5
0	533
0	066
18 = 0	5
0	536
0	045
28 = 0	5
0	574
0	054
19 = 0	5
0	586
0	048
29 = 0	5
0	487
0	054
11 = 0	5
0	479
0	077
21 = 0	5
0	510
0	072
Some estimated residual plots for the first component, ˆ1i1, ˆ1i4, ˆ1i7 and ˆ1i
versus case numbers, are displayed in Figure 11.2. Estimated residual plots of
ˆ1i versus ˆ1i1 and ˆ1i2; and plots of ˆ1i1 versus ˆ1i1, ˆ1i2 and ˆ1i are presented in
Figures 11.3 and 11.4, respectively. The estimated residual plots associated with
the second component are similar. These plots lie within two parallel horizontal
lines that are centered at zero and no linear or quadratic trends are detected.
This roughly indicates that the proposed measurement and structural equa-
tions are adequate. As WinBUGS does not give the DIC value, these estimated
residual plots provide useful information for goodness-of-fit assessment. The

334
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
(a)
0
100
200
300
400
hat{epsilon}_1i4
–2
–1
0
2
1
(b)
0
100
200
300
400
hat{epsilon}_1i4
–2
–1
0
2
1
(c)
0
100
200
300
400
hat{epsilon}_1i7
–2
–1
0
2
1
(d)
0
100
200
300
400
hat{delta}_1i
–2
–1
0
2
1
Figure
11.2
Estimated residual plots (a) ˆ1i1, (b) ˆ1i4, (c) ˆ1i7 and (d) ˆ1i.
WinBUGS codes under the identifiability constraint 15 < 25, and the data
are given in the following website: http://www.wiley.com/go/lee_structural.
11.4.2
A Simulation Study
Results of a simulation study will be presented to give some ideas on the
performance of the proposed Bayesian approach for analyzing finite mixtures

11.4
EXAMPLES AND SIMULATION STUDY
335
(a)
–2
–1
0
2
1
(b)
–2
–1
0
2
1
hat{delta}_1i
–3 –2 –1 0
3
2
1
hat{delta}_1i
–3 –2 –1 0
3
2
1
Figure
11.3
Plots of estimated residuals ˆ1i versus (a) ˆ1i1 and (b) ˆ1i2.
(a)
–2
–1
0
hat{epsilon}_1i1
–3 –2 –1
0
3
2
1
2
1
(b)
–2
–1
0
2
hat{epsilon}_1i1
–3 –2 –1
0
3
2
1
1
(c)
–2
–1
0
2
hat{epsilon}_1i1
–3 –2 –1
0
3
2
1
1
Figure
11.4
Plots of estimated residuals ˆ1i1 versus (a) ˆ1i1, (b) ˆ1i2 and (c) ˆ1i.

336
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
of SEMs. The data set is generated from a mixture of two SEMs defined in
Equations (11.1), (11.2) and (11.3). Each model involves six manifest variables
which are related with three latent factors 	k = k1k2, and k for k = 12.
In these two components, the population values of the elements in 1, 2, 1
and 2 are taken as:
T
1 = T
2 =
⎡
⎣
1	0∗
0	8
0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
1	0∗
0	8
0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
1	0∗
0	8
⎤
⎦
1 =
⎡
⎣
0∗
0∗
0	5
0∗
⎤
⎦2 =
⎡
⎣
0∗
0∗
−0	5
0∗
⎤
⎦	
In the analysis, parameters with asterisks are treated as fixed known parame-
ters. The true population values of the other unknown parameters are given
by: 1 = 2 = 1	0, 1 = 0	0 × J6
2 = 2	0 × J6
1m = 2m = 0	8 for
all m = 1   6, 1l = 2l = 1	0 for all l = 12, 1 = 
0	60	6T 2 =

0	6−0	6T , where J6 is a 6 × 1 vector with all elements equal to 1. The
following two designs with different mixing proportions are considered: 1 =
0	52 = 0	5 and 1 = 0	32 = 0	7	 For each design, we have a mixture
of SEMs with two components and 40 unknown parameters. Sample sizes
n = 400 and 800 were selected and 100 replications were completed for each
combination.
Two sets of Bayesian estimates are obtained using the conjugate prior distri-
butions with different prior inputs: (I) Estimates based on conjugate priors
with hyperparameters 0km0kl fixed at the true values;  = 1, 0 = ¯y,
0 = Sy/2, 0 = 5, R −1
0
= 5	0, 0k = 0k = 100k = 0k = 8; H0ykm = I
and H0kl = I for all km and l, where ¯y and Sy are the sample mean and the
sample covariance matrix of the simulated data. This can be regarded as a situ-
ation with accurate prior information. (II) Estimates based on the following
values of hyperparameters: 0km0kl equal to 2.0 times the true values;
other hyperparameter values are fixed at the same values as in (I). Starting values
of the unknown parameters are given by: 1 = 2 = 0	51 = 2 = 2	01 =
0	0 × J62 = 3	0 × J61m = 2m = 1	2 for m = 1   61l = 2l = 1	2
for l = 121 = 1	01	0T 2 = 1	0−1	0T and 0	0 for all the unknown
parameters in 1 and 2.
Based on the exploration of the MCMC outputs of the random permutation
sampler with an unconstrained model, we observe that any element in k can
be used to define the identifiability constraint. In this simulation study we
use 11 < 21. A few test runs are conducted initially as a pilot study to
obtain some idea about the number of the Gibbs sampler iterations required in
achieving convergence. In all these runs, the Gibbs sampler converges in about
500 to 1000 iterations, where the ESPR values (Gelman and Rubin, 1992)

11.4
EXAMPLES AND SIMULATION STUDY
337
are less than 1.2. Hence, for the 100 replications in the simulation, random
observations are collected after 1000 iterations. Then a total of an additional
3000 observations are collected to produce the Bayesian estimates and their
standard error estimates via Equations (11.14) and (11.15). Based on the 100
replications, the mean and the standard deviations (SD) of the estimates, as
well as the mean of the standard error estimate (SE) are computed. Moreover,
the bias which is the difference between the true parameter and the mean of
the corresponding estimates, and the root mean squares (RMS) between the
estimates and the true values based on the 100 replications, are computed. The
results are reported in Tables 11.3 to 11.6. We have the following findings from
these tables. (i) Bayesian estimates are reasonably accurate. (ii) As expected,
increasing the sample size improves the accuracy of the estimates. (iii) Bayesian
estimates with more accurate priors are better, but the differences are not
significant. Hence, it seems that the requirement of accurate hyperparameter
values is not crucial in the Bayesian estimation under sample sizes n = 400 and
800. (iv) In most cases, the SE values are slightly smaller than the SD values.
As pointed out by Dolan and van der Maas (1998), this minor difference may
be due to the difference in the model that featured in the simulation study.
However, the SE and SD values are quite close to each other, this indicates that
the standard error estimates produced by the proposed procedure are close to
the standard deviation of the estimates.
All computations are performed using a Sun Enterprise 4000 server. For
n = 400, the average computing time over the four designs with different 
and prior inputs is about 292 minutes for 100 replications, while for n = 800,
the corresponding average computer time is about 640 minutes.
11.4.3
An Example on ‘Job’ and ‘Homelife’
A small portion of the ICPSR data set collected in the project World Values
Survey 1981–1984 and 1990–1993 (World Value Study Group, 1994) is
analyzed in this example. The whole data set was collected in 45 societies around
the world on broad topics such as work, the meaning and purpose of life, family
life and contemporary social issues. As an illustration of our proposed method,
only the data obtained from the UK with a sample size of 1484 are used. Eight
variables in the original data set (variables 116, 117, 180, 132, 96, 255, 254
and 252) that related with respondents’ job and homelife are taken as manifest
variables in y. A description of these variables in the questionnaire is given in
Appendix 1.1. After deleting the cases with missing data the sample size is 824.
The data set is analyzed with a mixture SEM with two components. For each
component, there are three latent variables which can be roughly interpreted
as ‘job satisfaction, ’, ‘homelife, 1’ and ‘job attitude, 2’. For k = 12, the

338
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
Table
11.3
Summary statistics for Bayesian estimates (I) for model with 1 = 0	5.
n = 400
n = 800
Par
Bias
SE
SD
RMS
Bias
SE
SD
RMS
1 = 0	5
0.01
0	05
0	04
0	05
0.01
0	03
0	03
0	03
2 = 0	5
0.01
0	04
0	04
0	05
0.01
0	03
0	03
0	03
1 = 1	0
0.09
0	24
0	25
0	25
−0	07
0	17
0	18
0	18
2 = 1	0
0.08
0	21
0	21
0	22
−0	06
0	15
0	16
0	16
11 = 0	0
0.03
0	16
0	15
0	16
0.01
0	11
0	11
0	11
12 = 0	0
0.05
0	14
0	13
0	15
0.01
0	10
0	10
0	10
13 = 0	0
0.05
0	19
0	18
0	20
0.01
0	14
0	13
0	14
14 = 0	0
0.06
0	17
0	16
0	18
0.01
0	13
0	12
0	13
15 = 0	0
0.02
0	14
0	13
0	14
−0	00
0	11
0	10
0	11
16 = 0	0
0.04
0	14
0	13
0	14
0.00
0	09
0	09
0	09
21 = 2	0
0.02
0	13
0	12
0	13
0.00
0	08
0	09
0	08
22 = 2	0
0.01
0	10
0	11
0	10
0.00
0	09
0	08
0	08
23 = 2	0
−0	02
0	14
0	14
0	14
0.00
0	09
0	09
0	09
24 = 2	0
−0	01
0	12
0	11
0	12
0.00
0	08
0	08
0	08
25 = 2	0
0.03
0	11
0	12
0	12
0.00
0	08
0	08
0	08
26 = 2	0
0.02
0	10
0	11
0	10
0.00
0	08
0	08
0	08
11 = 1	0
−0	09
0	13
0	18
0	16
−0	09
0	12
0	14
0	16
12 = 1	0
−0	09
0	13
0	18
0	16
−0	08
0	15
0	15
0	16
21 = 1	0
−0	11
0	14
0	18
0	18
−0	08
0	12
0	14
0	15
22 = 1	0
−0	12
0	15
0	18
0	19
−0	07
0	14
0	15
0	15
11 = 0	8
0.06
0	12
0	15
0	14
0.06
0	10
0	12
0	12
12 = 0	8
−0	03
0	09
0	12
0	10
−0	02
0	08
0	10
0	09
13 = 0	8
0.06
0	13
0	16
0	15
0.03
0	09
0	13
0	09
14 = 0	8
−0	01
0	10
0	13
0	10
−0	02
0	08
0	10
0	08
15 = 0	8
0.09
0	11
0	14
0	14
0.06
0	11
0	12
0	13
16 = 0	8
−0	03
0	09
0	11
0	09
−0	01
0	09
0	09
0	08
21 = 0	8
0.05
0	12
0	15
0	13
0.05
0	11
0	12
0	13
22 = 0	8
0.00
0	10
0	12
0	10
0.00
0	09
0	10
0	08
23 = 0	8
0.03
0	12
0	16
0	13
0.04
0	12
0	13
0	11
24 = 0	8
0.01
0	09
0	13
0	09
0.00
0	08
0	11
0	08
25 = 0	8
0.06
0	12
0	15
0	14
0.04
0	10
0	11
0	10
26 = 0	8
0.00
0	10
0	12
0	10
−0	02
0	08
0	09
0	08
121 = 0	8
0.07
0	09
0	12
0	11
0.06
0	08
0	09
0	10
142 = 0	8
0.04
0	08
0	10
0	08
0.02
0	06
0	07
0	07
163 = 0	8
0.11
0	15
0	16
0	18
0.08
0	13
0	12
0	15
221 = 0	8
0.03
0	09
0	11
0	09
0.04
0	08
0	08
0	09
242 = 0	8
0.01
0	07
0	08
0	07
0.02
0	05
0	06
0	06
263 = 0	8
0.08
0	12
0	14
0	14
0.07
0	09
0	10
0	12
121 = 0	5
0.01
0	16
0	16
0	16
0.04
0	12
0	11
0	12
111 = 0	6
0.06
0	19
0	18
0	20
0.03
0	12
0	12
0	12
121 = 0	6
0.06
0	24
0	23
0	24
0.04
0	15
0	15
0	16
221 = −0	5
−0	02
0	17
0	18
0	17
−0	02
0	12
0	11
0	12
211 = 0	6
0.10
0	15
0	15
0	17
0.03
0	11
0	10
0	11
221 = −0	6
−0	11
0	21
0	23
0	23
−0	04
0	14
0	14
0	15
This table and Table 11.4 are taken from Zhu and Lee (2001).

11.4
EXAMPLES AND SIMULATION STUDY
339
Table
11.4
Summary statistics for Bayesian estimates (II) with 1 = 0	5.
n = 400
n = 800
Par
Bias
SE
SD
RMS
Bias
SE
SD
RMS
1 = 0	5
0.01
0	05
0	04
0	05
0.01
0	04
0	03
0	04
2 = 0	5
−0	01
0	05
0	04
0	05
−0	01
0	04
0	03
0	04
1 = 1	0
−0	14
0	23
0	25
0	27
−0	08
0	20
0	19
0	21
2 = 1	0
−0	18
0	18
0	20
0	26
−0	12
0	16
0	15
0	20
11 = 0	0
0.04
0	13
0	15
0	14
0.01
0	13
0	11
0	13
12 = 0	0
0.05
0	14
0	14
0	15
0.02
0	12
0	10
0	12
13 = 0	0
0.06
0	20
0	19
0	21
0.03
0	15
0	13
0	15
14 = 0	0
0.06
0	18
0	17
0	19
0.04
0	13
0	12
0	13
15 = 0	0
−0	01
0	14
0	14
0	14
0.01
0	10
0	10
0	10
16 = 0	0
0.03
0	14
0	13
0	14
0.02
0	10
0	09
0	10
21 = 2	0
0.00
0	13
0	13
0	13
0.02
0	09
0	09
0	09
22 = 2	0
−0	00
0	11
0	12
0	11
0.01
0	08
0	08
0	08
23 = 2	0
−0	03
0	12
0	14
0	13
−0	02
0	10
0	09
0	10
24 = 2	0
−0	03
0	11
0	12
0	12
−0	02
0	07
0	08
0	07
25 = 2	0
0.05
0	13
0	12
0	14
0.02
0	08
0	08
0	08
26 = 2	0
0.01
0	10
0	11
0	10
0.03
0	08
0	08
0	09
11 = 1	0
−0	11
0	16
0	18
0	20
−0	07
0	12
0	15
0	14
12 = 1	0
−0	12
0	15
0	18
0	19
−0	06
0	13
0	15
0	14
21 = 1	0
−0	11
0	13
0	18
0	17
−0	09
0	11
0	14
0	14
22 = 1	0
−0	09
0	14
0	20
0	17
−0	09
0	14
0	16
0	17
11 = 0	8
0.08
0	12
0	15
0	15
0.06
0	10
0	13
0	12
12 = 0	8
−0	02
0	10
0	13
0	10
0.00
0	08
0	10
0	08
13 = 0	8
0.08
0	13
0	16
0	15
0.04
0	10
0	13
0	11
14 = 0	8
−0	03
0	10
0	13
0	10
−0	01
0	08
0	10
0	08
15 = 0	8
0.11
0	12
0	15
0	17
0.08
0	12
0	12
0	14
16 = 0	8
−0	04
0	10
0	12
0	11
−0	02
0	08
0	09
0	08
21 = 0	8
0.06
0	11
0	15
0	12
0.04
0	11
0	12
0	12
22 = 0	8
−0	03
0	09
0	13
0	10
−0	01
0	08
0	10
0	08
23 = 0	8
0.02
0	11
0	16
0	12
0.03
0	11
0	13
0	11
24 = 0	8
−0	00
0	09
0	13
0	09
−0	01
0	09
0	10
0	09
25 = 0	8
0.09
0	11
0	15
0	14
0.08
0	11
0	11
0	13
26 = 0	8
−0	02
0	11
0	12
0	12
−0	01
0	09
0	09
0	09
121 = 0	8
0.09
0	11
0	13
0	14
0.04
0	09
0	10
0	10
142 = 0	8
0.03
0	08
0	10
0	09
0.03
0	06
0	07
0	07
163 = 0	8
0.16
0	19
0	18
0	24
0.09
0	11
0	13
0	14
221 = 0	8
0.07
0	10
0	11
0	13
0.04
0	07
0	08
0	08
242 = 0	8
0.03
0	08
0	09
0	08
0.02
0	05
0	06
0	05
263 = 0	8
0.13
0	14
0	16
0	20
0.09
0	10
0	11
0	14
121 = 0	5
0.02
0	15
0	17
0	15
0.02
0	11
0	12
0	11
111 = 0	6
0.10
0	17
0	19
0	20
0.06
0	12
0	13
0	13
121 = 0	6
0.15
0	23
0	24
0	27
0.06
0	15
0	16
0	16
221 = −0	5
0.02
0	17
0	18
0	17
0.01
0	12
0	12
0	12
211 = 0	6
0.12
0	17
0	18
0	21
0.06
0	13
0	12
0	14
221 = −0	6
−0	14
0	20
0	26
0	25
−0	12
0	16
0	17
0	20

340
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
Table
11.5
Summary statistics for Bayesian estimates (I) for model with 1 = 0	3.
n = 400
n = 800
Par
Bias
SE
SD
RMS
Bias
SE
SD
RMS
1 = 0	3
0.00
0	04
0	04
0	04
0.01
0	03
0	03
0	03
2 = 0	7
0.00
0	04
0	04
0	04
0.01
0	03
0	03
0	03
1 = 1	0
−0	19
0	27
0	28
0	32
0.08
0	21
0	23
0	23
2 = 1	0
−0	08
0	17
0	18
0	19
0.03
0	14
0	13
0	14
11 = 0	0
−0	01
0	19
0	21
0	19
0.04
0	16
0	15
0	17
12 = 0	0
0.02
0	18
0	19
0	18
0.06
0	14
0	14
0	15
13 = 0	0
0.02
0	27
0	26
0	27
0.06
0	19
0	19
0	20
14 = 0	0
0.02
0	22
0	22
0	21
0.06
0	18
0	17
0	19
15 = 0	0
−0	05
0	17
0	19
0	18
0.03
0	14
0	14
0	14
16 = 0	0
−0	01
0	17
0	18
0	17
0.05
0	14
0	13
0	15
21 = 2	0
0.00
0	09
0	10
0	09
0.01
0	06
0	07
0	06
22 = 2	0
0.00
0	08
0	09
0	08
0.01
0	05
0	06
0	05
23 = 2	0
−0	01
0	10
0	11
0	10
−0	01
0	07
0	08
0	07
24 = 2	0
−0	01
0	11
0	09
0	11
−0	01
0	07
0	07
0	07
25 = 2	0
0.01
0	08
0	09
0	09
0.01
0	06
0	07
0	06
26 = 2	0
0.01
0	09
0	08
0	09
0.01
0	06
0	06
0	06
11 = 1	0
−0	12
0	14
0	21
0	18
−0	08
0	16
0	18
0	18
12 = 1	0
−0	13
0	15
0	21
0	20
−0	10
0	14
0	18
0	17
21 = 1	0
−0	08
0	15
0	16
0	17
−0	04
0	11
0	13
0	12
22 = 1	0
−0	05
0	16
0	18
0	17
−0	06
0	11
0	14
0	12
11 = 0	8
0.07
0	12
0	18
0	14
0.05
0	10
0	14
0	11
12 = 0	8
0.02
0	11
0	15
0	11
−0	03
0	11
0	12
0	11
13 = 0	8
0.05
0	11
0	18
0	12
0.06
0	12
0	15
0	13
14 = 0	8
0.00
0	11
0	15
0	11
−0	02
0	09
0	12
0	10
15 = 0	8
0.09
0	13
0	16
0	15
0.08
0	13
0	14
0	15
16 = 0	8
0.00
0	11
0	15
0	11
−0	01
0	09
0	11
0	09
21 = 0	8
0.05
0	11
0	14
0	12
0.03
0	09
0	11
0	10
22 = 0	8
0.01
0	09
0	11
0	09
0.00
0	08
0	08
0	08
23 = 0	8
0.03
0	10
0	14
0	11
0.02
0	10
0	11
0	10
24 = 0	8
0.01
0	10
0	12
0	10
−0	01
0	08
0	10
0	08
25 = 0	8
0.05
0	11
0	13
0	12
0.02
0	08
0	10
0	08
26 = 0	8
0.00
0	09
0	10
0	09
0.00
0	07
0	08
0	07
121 = 0	8
0.06
0	13
0	16
0	14
0.08
0	10
0	12
0	12
142 = 0	8
0.02
0	11
0	13
0	11
0.04
0	07
0	09
0	08
163 = 0	8
0.16
0	17
0	23
0	24
0.12
0	11
0	16
0	17
221 = 0	8
0.04
0	09
0	09
0	10
0.02
0	07
0	07
0	07
242 = 0	8
0.00
0	06
0	07
0	06
0.00
0	05
0	05
0	05
263 = 0	8
0.06
0	11
0	12
0	13
0.02
0	08
0	08
0	08
121 = 0	5
0.08
0	21
0	22
0	22
0.01
0	15
0	15
0	15
111 = 0	6
0.10
0	23
0	26
0	25
0.18
0	16
0	17
0	18
121 = 0	6
0.03
0	27
0	31
0	26
0.17
0	20
0	21
0	21
221 = −0	5
−0	01
0	14
0	14
0	14
0.01
0	09
0	09
0	09
211 = 0	6
0.05
0	14
0	13
0	15
0.03
0	09
0	09
0	09
221 = −0	6
−0	07
0	16
0	18
0	16
−0	04
0	12
0	12
0	13

11.4
EXAMPLES AND SIMULATION STUDY
341
Table
11.6
Summary statistics for Bayesian estimates (II) with 1 = 0	3.
n = 400
n = 800
Par
Bias
SE
SD
RMS
Bias
SE
SD
RMS
1 = 0	3
0.02
0	04
0	04
0	04
0.01
0	03
0	03
0	03
2 = 0	7
−0	02
0	04
0	04
0	04
−0	01
0	03
0	03
0	03
1 = 1	0
−0	17
0	26
0	31
0	30
−0	17
0	23
0	23
0	29
2 = 1	0
−0	12
0	16
0	18
0	20
−0	05
0	13
0	13
0	14
11 = 0	0
0.08
0	17
0	21
0	19
0.03
0	15
0	15
0	15
12 = 0	0
0.09
0	18
0	20
0	20
0.05
0	15
0	14
0	16
13 = 0	0
0.13
0	25
0	27
0	28
0.05
0	21
0	19
0	22
14 = 0	0
0.12
0	20
0	24
0	23
0.06
0	18
0	17
0	19
15 = 0	0
0.02
0	18
0	19
0	18
0.01
0	14
0	14
0	14
16 = 0	0
0.09
0	16
0	19
0	19
0.04
0	13
0	13
0	14
21 = 2	0
0.01
0	10
0	10
0	10
0.01
0	08
0	07
0	08
22 = 2	0
0.00
0	08
0	09
0	08
−0	01
0	06
0	06
0	07
23 = 2	0
−0	01
0	09
0	11
0	10
0.01
0	08
0	08
0	08
24 = 2	0
−0	01
0	09
0	10
0	09
0.00
0	07
0	07
0	07
25 = 2	0
0.02
0	10
0	09
0	10
0.02
0	07
0	07
0	07
26 = 2	0
0.01
0	08
0	09
0	08
0.00
0	06
0	06
0	06
11 = 1	0
−0	13
0	14
0	21
0	19
−0	10
0	12
0	18
0	16
12 = 1	0
−0	10
0	14
0	22
0	18
−0	09
0	16
0	18
0	18
21 = 1	0
−0	09
0	12
0	16
0	15
−0	04
0	11
0	13
0	12
22 = 1	0
−0	09
0	15
0	17
0	18
−0	04
0	12
0	14
0	13
11 = 0	8
0.08
0	12
0	18
0	15
0.08
0	10
0	15
0	13
12 = 0	8
−0	00
0	10
0	15
0	10
−0	01
0	11
0	12
0	11
13 = 0	8
0.05
0	11
0	18
0	12
0.05
0	13
0	15
0	14
14 = 0	8
−0	04
0	09
0	14
0	10
−0	01
0	10
0	12
0	10
15 = 0	8
0.13
0	13
0	17
0	18
0.10
0	11
0	14
0	15
16 = 0	8
−0	02
0	10
0	15
0	10
−0	02
0	10
0	12
0	10
21 = 0	8
0.05
0	11
0	14
0	12
0.02
0	10
0	11
0	10
22 = 0	8
−0	01
0	10
0	11
0	10
0.01
0	07
0	08
0	07
23 = 0	8
−0	00
0	09
0	14
0	09
0.02
0	09
0	12
0	09
24 = 0	8
0.01
0	10
0	11
0	10
−0	01
0	07
0	09
0	07
25 = 0	8
0.08
0	11
0	13
0	13
0.04
0	09
0	10
0	10
26 = 0	8
−0	02
0	08
0	11
0	09
0.01
0	08
0	08
0	08
121 = 0	8
0.09
0	11
0	16
0	14
0.07
0	10
0	13
0	12
142 = 0	8
0.04
0	10
0	11
0	11
0.03
0	08
0	09
0	08
163 = 0	8
0.23
0	19
0	25
0	30
0.20
0	15
0	19
0	25
221 = 0	8
0.05
0	09
0	10
0	11
0.02
0	06
0	07
0	07
242 = 0	8
0.01
0	06
0	07
0	06
0.01
0	04
0	05
0	04
263 = 0	8
0.10
0	11
0	13
0	15
0.04
0	08
0	09
0	09
121 = 0	5
0.21
0	22
0	27
0	30
0.09
0	18
0	18
0	20
111 = 0	6
0.03
0	21
0	23
0	21
0.02
0	14
0	16
0	14
121 = 0	6
0.22
0	37
0	35
0	43
0.12
0	23
0	23
0	26
221 = −0	5
0.06
0	13
0	14
0	15
0.04
0	10
0	09
0	11
211 = 0	6
0.01
0	13
0	14
0	13
−0	01
0	09
0	10
0	09
221 = −0	6
−0	11
0	17
0	19
0	21
−0	05
0	13
0	13
0	14

342
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
specification of the parameter matrices in the model formulation are given by:
k = 0 k = kk = k1k2,
T
k =
⎡
⎣
1	0∗k21 0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
1	0∗k42 k52 0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
0	0∗
1	0∗k73 k83
⎤
⎦ k =
⎡
⎣
k11 k12
k21 k22
⎤
⎦
(11.16)
and  k = diag(k1··· k8). To identify the model, elements in k with an
asterisk are fixed. The total number of unknown parameters is 62.
Bayesian estimates of the structural parameters and estimates of the factor
scores are obtained via the Gibbs sampler. The following hyperparameters are
selected:  = 1, 0 = ¯y, 0 = Sy/2	00 = 5 and R −1
0
= 5I2; 0k = 0k = 0k =
0k = 6 for all k; H0ykm = I and H0kl = I, 0km = ˜0km and 0kl = ˜0kl for
all k, m and l, where ˜0km and ˜0kl are obtained by some initial estimation
with noninformative prior distributions. We first use MCMC samples simulated
by the random permutation sampler to find an identifiability constraint. We find
that 11 < 21 is a suitable one. Under this constraint, Bayesian estimates are
obtained by the procedure described in Section 11.3. Based on different starting
values of the parameters, three parallel sequences of observations are generated
and the EPSR values are calculated. Figure 11.5 presents the plots of the EPSR
values against the iteration numbers. We observe that the EPSR values of the
5
10
15
EPRS
0
500
1000
1500
2000
iteration
Figure
11.5
EPSR values of all parameters from three parallel runs in the ICPSR
example. This table is taken from Zhu and Lee (2001).

11.4
EXAMPLES AND SIMULATION STUDY
343
parameters at the starting points were quite large, this indicates that the starting
values are far away from the solution. The Gibbs sampler algorithm converged
within 1000 iterations. After the convergence of the Gibbs sampler, a total of
1000 observations with a spacing of 10 are collected for analysis. The Bayesian
estimates of the structural parameters and their standard error estimates are
reported in Table 11.7. From this table, it can be seen that there are at least
two components which have different sets of Bayesian parameter estimates.
Table
11.7
Bayesian estimates and standard error estimates
of the ICPSR example.
Parameter
Component 1
Component 2
EST
SE
EST
SE
k
0	56
0	03
0	44
0	03
k1
6	91
0	11
8	09
0	09
k2
6	30
0	14
7	90
0	14
k3
5	87
0	14
7	83
0	11
k4
7	83
0	10
8	70
0	07
k5
7	10
0	11
8	07
0	09
k6
5	41
0	14
4	01
0	15
k7
4	06
0	13
3	61
0	14
k8
5	59
0	14
4	61
0	14
k11
1∗
–
1∗
–
k21
0	49
0	11
0	86
0	13
k32
1∗
–
1∗
–
k42
1	30
0	17
0	94
0	10
k52
1	58
0	20
1	02
0	11
k63
1∗
–
1∗
–
k73
2	05
0	44
0	98
0	07
k83
1	08
0	27
0	74
0	08
k1
0	68
0	14
0	77
0	11
k2
−0	02
0	15
−0	09
0	04
k11
1	18
0	26
0	90
0	18
k21
−0	12
0	09
−0	28
0	15
k22
0	92
0	30
4	30
0	52
k1
1	56
0	65
0	56
0	11
k2
6	92
0	50
2	80
0	34
k3
4	86
0	37
1	35
0	18
k4
2	51
0	27
0	45
0	07
k5
1	29
0	27
0	55
0	08
k6
6	31
0	50
1	25
0	35
k7
2	43
0	76
1	07
0	23
k8
6	39
0	57
3	15
0	40
k
3	38
0	72
0	70
0	12
This table is taken from Zhu and Lee (2001).

344
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
11.5
BAYESIAN MODEL COMPARISON OF MIXTURE SEMs
In previous sections, we discussed the Bayesian estimation for a finite mixture
of SEMs with a known number of components. In the analysis, K is given and
each k is greater than zero. The objective of this section is to consider the
Bayesian model selection problem for selecting one of the two mixtures of SEMs
with a different number of components. An approach based on the Bayes factor
(Berger, 1985) will be developed. For mixtures of SEMs that involve a large
number of unknown parameters and latent variables, an efficient procedure for
computing the Bayes factor is important. Here, an algorithm based on path
sampling (Gelman and Meng, 1998; Lee and Song, 2003a) is described. In
the implementation, random observations are needed to sample from some
appropriate conditional distributions. The Gibbs sampler (Geman and Geman,
1984) developed in previous sections in the estimation can be used for this
purpose.
The underlying finite mixtures of SEMs is defined again by Equations (11.1),
(11.2) and (11.3), except that K is not fixed and k are non-negative compo-
nent probabilities that sum to 1.0. Note that some k may be equal to zero.
When using the likelihood ratio test, some unknown parameters may be on the
boundary of the parameter space, and this causes serious difficulty in developing
the test statistics for testing the hypothesis about the number of components.
On the contrary, the Bayesian approach for model comparison with the Bayes
factor does not have this problem.
11.5.1
A Model Selection Procedure through Path Sampling
Let M1 be a mixture SEM with K components, and M0 be a mixture SEM with
c components, where c < K. The Bayes factor for selection between M0 and
M1 is defined by
B10 = PYM1
PYM0
(11.17)
based on the given data set Y. In computing the Bayes factor through path
sampling, the observed data set Y is augmented with the matrix of latent
variables  and W. Based on similar reasoning and derivation to that given in
previous chapters, log B10 can be estimated as follows. Let
UYt = d
dt logpYt

11.5
BAYESIAN MODEL COMPARISON OF MIXTURE SEMs
345
where pYt is the complete-data log-likelihood function. Then,

log B10 = 1
2
S
s=0
ts+1 −tsU s+1 +U s
(11.18)
where ts are fixed grids tsS
s=0 in [0, 1] such that t0 = 0 < t1 < t2 < ··· <
tS < tS+1 = 1, and U s is the average of the UYt on the basis of all
simulation draws for which t = ts. That is,
U s = J −1
J
j=1
UYjjts
(11.19)
in which jjj = 1   J are simulated observations drawn from
pWYts.
The implementation of path sampling is straightforward. Drawing observations
jj  j = 1   J from the posterior distribution pWYts is
the major task in the proposed procedure. This posterior distribution is compli-
cated and difficult to cope with. Similar to estimation, we further utilize the idea
of data augmentation to augment the observed data Y with the latent matrix
W of the allocation variables. As a consequence, the Gibbs sampler described
in Section 11.3 can be applied to simulate observations from pWYts.
However, note that in computing Bayes factors, the inclusion of an identifia-
bility constraint in simulating jjj = 1   J is not necessary. As the
likelihood is invariant to relabelling the states, the inclusion of such a constraint
will not change the values of UYt. As a result, the logarithm Bayes
factors estimated by Equations (11.18) and (11.19) will not be changed.
Finding a good path t in [0,1] to link competing models M1 and M0 is an
important step in applying path sampling. An illustrative example is given as
follows. Consider the following competing models:
M1 

y
D=
K
k=1
kfkykk
(11.20)
corresponding to a model of K components with positive component proba-
bilities k, and
M0 

y∗
D=
c
k=1
∗
kfkykk
(11.21)

346
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
corresponding to a model of c components with positive component probabili-
ties ∗
k, where 1 ≤c < K. To apply path sampling for computing log B10, these
competing models are linked up by a path t ∈
01 as follows:
Mt 

yt
D=
1 +1−ta1c+1 +···+Kf1y11+···
+
c +1−tacc+1 +···+Kfcycc
+tc+1fc+1yc+1c+1+···+tKfKyKK
(11.22)
where a1··· ac are given positive weights such that a1 +···+ac = 1. Clearly,
when t = 1, Mt reduces to M1; when t = 0, Mt reduces to M0 with ∗
k =
k + akc+1 + ··· + K, k = 1   c. The weights a1   ac represent the
increases of the corresponding component probabilities from a K component
SEM to a c component SEM. A natural and simple suggestion for practical
applications is to take ak = c−1.
The complete-data log-likelihood function can be written as
log pYt =
n
i=1
log

c
k=1

k +1−tak
K
h=c+1
h
(11.23)
×fkyikikk+
K
k=c+1
tkfkyikikk

	
By differentiation with respect to t, we have
UYt =
(11.24)
n
i=1
−
K	
h=c+1
h
c	
k=1
akfkyikikk+
K	
k=c+1
kfkyikikk
c	
k=1

k +1−tak
K	
h=c+1
h

fkyikikk+
K	
k=c+1
tkfkyikikk

where
fkyikikk = 2−p/2 k−1/2
×exp

−1
2yi −k −kkiT  −1
k yi −k −kki

×2−q1/2Iq1 −k k−1/2

11.5
BAYESIAN MODEL COMPARISON OF MIXTURE SEMs
347
×exp

−1
2	ki −kkiT  −1
k 	ki −kki

×2−q2/2k−1/2exp

−1
2
T
ki−1
k 
ki


and k = kk. Thus, the Bayes factor can be estimated via Equa-
tion (11.18) with
¯Us = J −1
J
j=1
UYjjts
(11.25)
where jj  j = 1   J are observations drawn from pYts.
11.5.2
A Simulation Study
In this section, results obtained from a simulation study will be presented to
illustrate the accuracy of the results obtained from the path sampling proce-
dure, and the sensitivity to prior inputs, for model selection in finite mixtures
of SEMs. The true model is a mixture SEM with two components defined in
Equations (11.2) and (11.3). In each component, there are six manifest vari-
ables, which are related to three latent factors in 	k = k1k2, and 
k = k.
The specifications and population values of the elements in 1, 2, 1 and 2
are taken as:
T
1 = T
2 =
⎛
⎝
1∗0	8 0∗
0∗
0∗
0∗
0∗
0∗
1∗0	8 0∗
0∗
0∗
0∗
0∗
0∗
1∗0	8
⎞
⎠1 =
 0∗
0∗
0	5 0∗

2 =
 0∗
0∗
−0	5 0∗


where parameters with an asterisk are fixed at the preassigned values. The
true population values of the other unknown parameters are given by: 1 =
1	0, 2 = 1	0, 1m = 0	0, 2m = 2	0, 1m = 2m = 0	64, for m = 1   6,
1l = 2l = 0	8 for l = 12, 1 = 
0	50	5T , 2 = 
0	5−0	5T and 1 =
2 = 0	5. Note that the mean vectors,  and , of these two components are
different. The separation d12 = maxk∈12
1 −2T −1
k 1 −2 of these two
components is equal to 4.128.
Using n = 400 and the above specifications, a random sample of observations
is generated to illustrate the sensitivity of the proposed procedure for computing
the logarithm Bayes factor with respect to prior distributions. Since there are
many parameters involved in the prior distributions, it would be very tedious
to provide a detailed analysis on the impact of each type of them. Hence,
we concentrate on the important hyperparameters and fix the less important

348
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
ones at the following given values: H0ykm = I, H0kl = I, for all k, m and
l, 0 = 8 and R0 = 5	0, where I denotes an identity matrix of appropriate
order. Based on the nature of the parameters, we divide the remaining more
important hyperparameters into three groups: , 000km0kl and
0k0k0k0k. In the sensitivity analysis, we follow the suggestion
of Kass and Raftery (1995) to perturb alternatively the hyperparameters in these
groups. We take  = 1 and  = 2 in the symmetric Dirichlet distribution, the
prior distribution of . For each value of , the following different types of
hyperparameter values in the other groups are considered.
Type I:
Fix 0k = 0k = 6 and 0k = 0k = 4 for each component. Hyper-
parameters in the remaining groups are given by:
(a)
For each k = 12m = 1   6l = 120km0kl are equal to the
true population values for all k, m and l; 0 = ¯y, 0 = Sy/2, where ¯y and
Sy are the sample mean and the sample covariance matrix of the simulated
data.
(b)
0km0kl, 0 and 0 are set equal to half the values given in (a).
(c)
0km0kl, 0 and 0 are set equal to twice the values given in (a).
Type II:
For each k = 12m = 1   6l = 12, fix 0km0kl at the
true values for all k, m and l; 0 = ¯y, and 0 = Sy/2. Hyperparameters in the
remaining group are taken as:
(a)
0k = 0k = 6, 0k = 0k = 4 for each component.
(b)
0k = 0k = 10, 0k = 0k = 8 for each component.
The path sampling procedure with 20 grids in [0,1] is applied to estimate
the logarithm Bayes factors for comparing M1M2 and M3, where Mk is a
mixture of SEM with k components. In simulating the required observations,
we first conduct a few test runs in a pilot study to obtain some idea about the
convergence of the Gibbs sampler. We find that in all cases, the Gibbs sampler
converged quickly within 100 iterations. For each ts in every replication, J =
1000 observations are collected after discarding 100 burn-in iterations. The
logarithm Bayes factors for different types of hyperparameters are estimated via
Equations (11.18) and (11.19). Results are presented in Table 11.8. All the
estimated log B21 are significantly larger than 3.0 and the estimated log B32 are
less than zero. These results lead to the conclusion that M2 is much better than
M1 and it is also better than M3. On the basis of the criterion for interpreting
the logarithm Bayes factor, it is concluded that a two-component SEM should
be selected. This conclusion is consistent with the true situation.
To obtain more understanding about the sensitivity of the Bayes factor to
prior inputs, we further conduct a study on the basis of some changes on
0k0k0k and 0k under the Type I hyperparameter values. Here, we take

11.5
BAYESIAN MODEL COMPARISON OF MIXTURE SEMs
349
Table
11.8
The estimated logarithms of Bayes factors.
Type
I
II
(a)
(b)
(c)
(a)
(b)
log B21
12.21
9.39
11.05
12.21
10.79
 = 1
log B32
−0	42
−0	37
−0	46
−0	42
−0	51
log B21
13.19
11.80
11.41
13.20
12.21
 = 2
log B32
−0	73
−0	66
−0	89
−0	73
−1	00
Table
11.9
The estimated logarithms of Bayes factors under different prior
inputs and S.
S = 20
S = 10
(a)
(b)
(c)
(a)
(b)
(c)
log B21
12.43
10.94
11.24
11.33
9.34
9.87
 = 1
log B32
−0	32
−0	32
−0	40
−0	33
−0	35
−0	43
log B21
13.35
14.19
12.60
11.86
10.21
12.30
 = 2
log B32
−0	60
−0	60
−0	71
−0	63
−0	64
−0	18
01 = 01 = 702 = 02 = 501 = 01 = 4 and 02 = 02 = 3, while the
other hyperparameter values under (a) and (b) are the same as before. Moreover,
to provide some idea about the Bayes factor estimates under different choices
on the number of grids in [0, 1], we consider S = 20 and S = 10. The estimated
logarithm of Bayes factors are reported in Table 11.9. We observe that the
estimates given in Table 11.9 are close to those given in columns under Type
I of Table 11.8. From the

log B21 and

log B32 values under various choices
of hyperparameters, it is observed that under the given sample size, the Bayes
factor for analyzing mixtures of SEMs is not very sensitive to these prior inputs.
11.5.3
An Illustrative Example
The same portion of the ICPSR data set as described in Section 11.4.3 is
reanalyzed to illustrate the path sampling procedure. We wish to find out
whether there are some mixture models that are better than the mixture SEM
with two components proposed in Section 11.4.3. For each component, the

350
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
specifications of the model are the same as before, see descriptions around
Equation (11.16). However, the number of components, K, is not fixed.
The hyperparameter values are selected as follows. First  = 1, 0 = ¯y, 0 =
Sy/2, 0 = 6 and R −1
0
= 5I, H0ykm = I and H0kl = I are selected for each
km = 1   p, and l = 1   q1. Moreover, 0k0k and 0k0k are
selected such that the means and standard deviations of the prior distributions
associated with km and kl are equal to 5.0. Finally, we take 0km = ˜0km
and 0kl = ˜0kl for all k, m and l, where ˜0km and ˜0kl are the corre-
sponding Bayesian estimates obtained through a single component model with
noninformative prior distributions. Again, for each ts, the convergence of the
Gibbs sampler algorithm is monitored by parallel sequences of the generated
observations from very different starting values. We observe that the algorithm
converged quickly within 200 iterations. Since the convergence behaviors are
similar to those in the simulation study, they are not presented. A total of
J = 1000 additional observations are collected after a burn-in phase of 200
iterations for computing ¯Us in Equation (11.19), and then the logarithms
of Bayes factors are estimated via Equation (11.18), using 20 fixed grids in
[0,1]. Let Mk denote the mixture model with k components, and the estimated
logarithms of Bayes factors are equal to:

log B21 = 75	055,

log B32 = 4	381,

log B43 = −0	824 and

log B53 = −1	395. According to the criterion of the loga-
rithm Bayes factor, the one-component model is significantly worse than the
two-component model which is significantly worse than the three-component
model; while the three-component model is better than the four-component
and five-component models. Hence, it can be concluded that a mixture model
with three components should be chosen. Although the two-component model
suggested in Section 11.5.2 is a plausible model, it does not give as strong a
support of evidence as the three-component model.
In estimation, based on the MCMC samples simulated by the random permu-
tation sampler, we find 11 < 21 is a suitable identifiability constraint. Bayesian
estimates of the selected three-component mixture model obtained under the
selected constraint are presented in Table 11.10, together with the corre-
sponding standard error estimates. For parameters directly associated with mani-
fest variables y1 to y5, we from this table that their Bayesian estimates under
component two are close to those under component three, but these estimates
are quite different from those under component one. In contrast, for parameters
directly associated with manifest variables y6 to y8, estimates under component
one are close to those under component three, but these estimates are quite
different from those under component two. Hence, it is reasonable to select
a three-component model for this data set. For completeness, estimate the
separations of these components and find that they are equal to d12 = 2	257,
d13 = 2	590 and d23 = 2	473. These results indicate that the proposed procedure
is able to select the appropriate three-component model whose components are
not well separated.

APPENDIX 11.1: THE PERMUTATION SAMPLER
351
Table
11.10
Bayesian estimates of parameters and standard errors for the selected
model with three components in analyzing the ICPSR data set.
Component 1
Component 2
Component 3
PAR
EST
SE
EST
SE
EST
SE
k
0.51
0	03
0.23
0	03
0.26
0	03
k1
6.75
0	13
8.05
0	15
8.25
0	15
k2
5.95
0	12
7.53
0	21
8.63
0	17
k3
5.76
0	18
7.67
0	18
7.87
0	14
k4
7.74
0	13
8.65
0	12
8.77
0	12
k5
7.05
0	12
8.06
0	12
8.04
0	11
k6
5.50
0	25
2.70
0	18
5.20
0	15
k7
4.12
0	23
2.60
0	16
4.35
0	14
k8
5.66
0	24
3.08
0	23
5.94
0	15
k21
0.31
0	12
1.10
0	21
0.66
0	08
k42
1.38
0	18
0.84
0	13
0.87
0	16
k52
1.67
0	19
0.92
0	15
1.10
0	18
k73
2.15
0	31
0.98
0	09
1.94
0	22
k83
0.88
0	22
0.97
0	11
0.64
0	20
k1
0.62
0	16
0.52
0	15
0.69
0	14
k2
0.01
0	11
-0.37
0	12
-0.12
0	14
k11
1.07
0	21
1.30
0	33
0.81
0	20
k21
-0.13
0	13
-0.59
0	20
0.07
0	07
k22
1.10
0	49
1.45
0	38
1.57
0	21
k1
1.08
0	12
0.87
0	19
0.56
0	35
k2
6.79
0	17
2.02
0	45
0.83
0	46
k3
4.75
0	38
1.71
0	40
1.17
0	37
k4
2.54
0	13
0.45
0	08
0.69
0	27
k5
1.11
0	16
0.55
0	09
0.71
0	23
k6
5.75
0	55
0.55
0	13
4.71
0	46
k7
1.36
0	35
0.60
0	11
1.13
0	47
k8
6.10
0	52
1.05
0	50
4.52
0	47
k
3.80
0	13
0.63
0	15
0.68
0	51
APPENDIX 11.1: THE PERMUTATION SAMPLER
Let  = W, the permutation sampler for generating  from the posterior
pY is implemented as follows.
(1) First generate ˜ from the unconstrained posterior pY using standard
Gibbs sampling steps.
(2) Select some permutation 1   K of the current labelling of
the states and define  =  ˜ from ˜ by reordering the labelling through
this permutation, 1   K = 1   K and W = w1   wn =
w1    wn.

352
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
One application of permutation sampling is the random permutation sampler,
where each sweep of the MCMC chain is concluded by relabelling the states
through a random permutation of 1··· K. This method delivers a sample
that explores the whole unconstrained parameter space and jumps between
the various labelling subspaces in a balanced fashion. Another application
of the permutation sampler is the permutation sampling under identifiability
constraints. A common way to include an identifiability constraint is to use a
permutation sampler, where the permutation is selected in such a way that the
identifiability constraint is fulfilled.
APPENDIX 11.2: SEARCHING FOR IDENTIFIABILITY
CONSTRAINTS
For k = 1   K, let k denote the parameter vector corresponding to the
kth component. According to the suggestion by Fr¨uhwirth-Schnatter (2001),
the MCMC output of the random permutation sampler can be explored to
find a suitable identifiability constraint. It is sufficient to consider only the
parameters in 1, because a balanced sample from the unconstrained posterior
will contain the same information for all parameters in k with k > 1. As the
random permutation sampler jumps between the various labelling subspaces,
part of the values sampled for 1 will belong to the first state, part will belong
to the second state, and so on. To differ for various states, it is most useful
to consider bivariate scatterplots of 1i versus 1l for possible combinations
of i and l. Jumping between the labelling subspaces produces groups in these
scatterplots that correspond to different states. By describing the difference
between the various groups geometrically, identification of a unique labelling
subspace through conditions on the state-specific parameters is attempted. If
the values sampled for a certain component of  differ markedly between the
groups when jumping between the labelling subspaces, then an order condition
on this component could be used to separate the labelling subspaces, while if
the values sampled for a certain component of  hardly differ between the states
when jumping between the labelling subspaces, then this component will be a
poor candidate for separating the labelling subspaces.
REFERENCES
Arminger, G. and Stein, P. (1997) Finite mixtures of covariance structure models with
regressors. Sociological Methods and Research, 26, 148–182.
Arminger, G., Stein, P. and Wittenberg (1999) Mixtures of conditional mean- and
covariance-structure models. Psychometrika, 64, 475–494.
Berger, J. O. (1985) Statistical Decision Theory and Bayesian Analysis. New York:
Springer-Verlag.

REFERENCES
353
Besag, J., Green, P., Higdon, D. and Mengersen, K. (1995) Bayesian computation and
stochastic systems. Statistical Science, 10, 3–66.
Diebolt, J. and Robert, C. P. (1994) Estimation of finite mixture distributions through
Bayesian sampling. Journal of the Royal Statistical Society, Series B, 56, 363–375.
Dolan, C. V. and van der Maas, J. J. L. (1998) Fitting multivariate normal mixtures
subject to structural equation modeling. Psychometrika, 63, 227–253.
Feng, Z. D. and McCulloch, C. E. (1996) Using bootstrap likelihood ratios in finite
mixture models. Journal of the Royal Statistical Society, Series B, 58, 609–617.
Fr¨uhwirth-Schnatter, S. (2001) Markov chain Monte Carlo estimation of classical and
dynamic switching and mixture models. Journal of the American Statistical Association,
96, 194–208.
Gelman, A. and Meng, X. L. (1998) Simulating normalizing constants: from importance
sampling to bridge sampling to path sampling. Statistical Science, 13, 163–185.
Gelman, A. and Rubin, D. B. (1992) Inference from iterative simulation using multiple
sequences. Statistical Science, 7, 457–472.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distribution and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Gilks, W. R., Richardson, S. and Spiegelhalter, D. J. (1996) Introducing Markov chain
Monte Carlo. In W.R. Gilks, S. Richardson and D. J. Spiegelhalter (eds), Markov
Chain Monte Carlo in Practice, pp. 1–19. London: Chapman and Hall.
Green, P. J. (1995) Reversible jump Markov chain Monte Carlo computation and
Bayesian model determination. Biometrika, 82, 711–732.
Hathaway, R. J. (1985) A constrained formulation of maximum-likelihood estimation
for normal mixture distributions. The Annals of Statistics, 13, 795–800.
Jedidi, K., Jagpal, H. S. and DeSarbo, W. S. (1997a) Finite-mixture structural equation
models for response-based segmentation and unobserved heterogeneity. Marketing
Science, 16, 39–59.
Jedidi, K., Jagpal, H. S. and DeSarbo, W. S. (1997b) STEMM: a general finite mixture
structural equation model. Journal of Classification, 14, 23–50.
Jöreskog, K. G. and Sörbom, D. (1996) LISREL 8: Structural Equation Modeling with the
SIMPLIS Command Language. Hove and London: Scientific Software International.
Kass, R. E. and Raftery, A. E. (1995) Bayes factors. Journal of the American Statistical
Association, 90, 773–795.
Lee, S. Y. and Song, X. Y. (2002) Bayesian selection on the number of factors in a factor
analysis model. Behaviormetrika, 29, 23–39.
Lee, S. Y. and Song, X. Y. (2003a) Bayesian model selection for mixtures of struc-
tural equation models with an unknown number of components. British Journal of
Mathematical and Statistical Psychology, 56, 145–65.
Lee, S. Y. and Song, X. Y. (2003b) Maximum likelihood estimation and model compar-
ison for mixtures of structural equation models with ignorable missing data. Journal
of Classification, 20, 221–255.
Lindley, D. V. and Smith, A. F. M. (1972) Bayes estimates for the linear model (with
discussion). Journal of the Royal Statistical Society, Series B, 34, 1–42.
Lindsay, B. G. and Basak, P. (1993) Multivariate normal mixtures: a fast consistent
method of moments. Journal of the American Statistical Association, 88, 468–476.
McLachlan, G. J. (1987) On bootstrapping the likelihood ratio test statistics for the
number of components in a normal mixture. Applied Statistics, 36, 318–324.

354
11
FINITE MIXTURES IN STRUCTURAL EQUATION MODELS
Pettit, L. I. and Smith, A. F. M. (1985) Outliers and influential observations in linear
models. In J. M. Bernardo et al. (eds), Bayesian Statistics, 2 pp. 473–494. Amsterdam:
North-Holland.
Redner, R. A. and Walker, H. F. (1984) Mixture densities, maximum likelihood and the
EM algorithm. SIAM Review, 26, 195–239.
Richardson, S. and Green, P. J. (1997) On Bayesian analysis of mixtures with an unknown
number of components (with discussion). Journal of the Royal Statistical Society, Series
B, 59, 731–792.
Robert, C. (1996) Mixtures of distributions: Inference and estimation. In W. R. Gilks,
S. Richardson and D. J. Spiegelhalter (eds), Practical Markov Chain Monte Carlo,
pp. 163–188. London: Chapman and Hall.
Roeder, K. and Wasserman, L. (1997) Practical Bayesian density estimation using
mixtures of normals. Journal of the American Statistical Association, 92, 894–902.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. (2003) WinBUGS User
Manual. Version 1.4. Cambridge, England: MRC Biostatistics Unit.
Titterington, D. M., Smith, A. F. M. and Markov, U. E. (1985) Statistical Analysis of
Finite Mixture Distributions. Chichester: John Wiley & Sons, Ltd.
World Values Study Group (1994) World Values Survey 1981–1984 and 1990–1993.
ICPSR version. Ann Arbor, MI: Institute for Social Research (producer). Ann Arbor,
MI: Inter-university Consortium for Political and Social Research (distributor).
Yung, Y. F. (1997) Finite mixtures in confirmatory factor-analysis models. Psychometrika,
62, 297–330.
Zhu, H. T. and Lee, S. Y. (2001) A Bayesian analysis of finite mixtures in the LISREL
model. Psychometrika, 66, 133–152.

12
Structural Equation
Models with Missing
Data
12.1
INTRODUCTION
Missing data are very common in substantive research. For example, respon-
dents in a household survey may refuse to report income, individuals in an
opinion survey may refuse to express their attitudes toward some sensitive or
embarrassing questions. Moreover, it is common to have missing individuals at
one or more time points in longitudinal studies, or some results are missing
because of mechanical breakdowns in a psychological experiment. Clearly, the
effect of missing data needs to be taken into account for better statistical infer-
ences. In statistics, analysis of missing data has a long history, it continuously
receives a lot of attention and is still an active area of research (see, for example,
Afifi and Elashoff (1969), Little and Rubin (1987), and Ibrahim, Chen and
Lipsitz (2001), among many others).
In structural equation modeling, much attention has been given to the anal-
ysis of models in the presence of missing data. Some historical approaches to
missing data problems – such as the listwise deletion, and filling in the missing
values by mean estimates or least square estimates – although not without
value, tend to have an ad hoc character and will encounter serious difficulty in
achieving theoretical properties. More statistically rigorous methods have been
proposed. The earlier contributions were mainly focused on standard SEMs
and were developed through the multigroup analysis of a covariance structure
analysis (CSA) approach. The basic technique treated observations that belong
to the same missing pattern as an independent group, obtained the sample
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

356
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
covariance matrix of each independent group, and then analyzed the sample
covariance matrices through the multigroup methods in the CSA approach.
This approach would encounter theoretical difficulty if some missing patterns
just have a small number of observations so that the corresponding sample
covariance matrices may be singular. It is computationally tedious if the number
of missing patterns is large. Moreover, it is difficult to extend this approach
to handle more complicated SEMs or missing data that are missing with a
nonignorable missing mechanism. Motivated by the deficiency of the multi-
group CSA approach, methods that are focused on the raw observations have
been developed. For example, in analyzing standard SEMs, Arbuckle (1996)
proposed the full information ML method which maximizes the case wise likeli-
hood of the observed continuous data. Recently, through the utilization of the
data augmentation idea and MCMC methods in statistical computing, Bayesian
methods for analyzing missing data in the context of more complex SEMs have
been developed. For instance, Song and Lee (2002) and Lee and Song (2004)
developed Bayesian methods for analyzing linear and nonlinear SEMs with
mixed continuous and ordered categorical variables. While the above mentioned
contributions in SEM are established on the basis of ignorable missing data
that are missing at random (MAR), Lee and Tang (2006) developed Bayesian
methods for analyzing nonlinear SEMs with nonignorable missing data.
The main objective of this chapter is to introduce the Bayesian approach for
analyzing SEMs with ignorable missing data that are missing at random (MAR),
and nonignorable missing data that are missing according to a nonignorable
missing mechanism. As described in Little and Rubin (1987), if the probability
of response depends on the fully observed data but not on the missing data,
the corresponding missing data can be regarded as MAR. For nonignorable
missing data, the probability of response depends not only on the observed
data but also on the missing data, according to a nonignorable missing model.
In the development of the Bayesian approach, we again emphasize the useful
strategy that combines the idea of data augmentation and application of MCMC
methods. We will show that Bayesian methods for analyzing complex SEMs
with fully observed data can be extended to handle missing data with a large
number of missing patterns without much theoretical and practical difficulty. In
this chapter, we regard observations with missing entries as partially observed
data.
We will present a general Bayesian framework for analyzing general SEMs
with missing data that are MAR, which includes Bayesian estimation, and model
comparison via the Bayes factor. This general framework will be applied to
nonlinear SEMs with missing continuous and ordered categorical data, and
to mixtures of SEMs. We assess the effect of missing data on estimation and
show that the proposed Bayesian approach that uses both the fully and partially
observed data produces accurate estimates. Moreover, we investigate the impact
of ignoring the partially observed data on model selection. In the analysis of
mixtures of SEMs, we give an example to illustrate that ignoring the partially

12.2
SEMS WITH MAR MISSING DATA
357
observed data will give different estimates and model comparison results. Then,
we present Bayesian methods to analyze nonlinear SEMs with missing data that
are missing with a nonignorable mechanism. Finally, we demonstrate the use
of the software WinBUGS to obtain the Bayesian solutions.
12.2
A GENERAL FRAMEWORK FOR SEMS WITH MISSING
DATA THAT ARE MAR
Consider a general SEM of interest, which could be the standard model,
a nonlinear model, a multilevel model or a finite mixture of SEMs. Let
V = v1   vn be a matrix of random vectors with vi = xT
i yT
i T , in which xi
and yi are vectors of manifest continuous variables whose exact measurements
are observable and unobservable, respectively. Let Z = z1   zn be observ-
able ordered categorical data (or dichotomous data ) that correspond to Y =
y1   yn. Suppose vi follows a SEM with real and/or augmented latent vari-
ables  = 12, where 1 = 11   1n contains latent variables in the
structural equation model such as the factor scores, and 2 = 21   2n
contains external latent quantities such as the allocation variables in a finite
mixture of SEMs.
To deal with the missing data that are MAR, let Xobs and Xmis be the observed
and missing data sets corresponding to the continuous data X = x1   xn;
Zobs and Zmis be the observed and missing data sets corresponding to the ordered
categorical data Z; Yobs and Ymis be the hypothetical observed and missing
data sets of Y corresponding Zobs and Zmis, respectively. Moreover, we let
Vobs = XobsYobs and Vmis = XmisYmis. The main goal is to develop Bayesian
methods for estimating the unknown parameter vector ∗of the model, and
comparing competitive models on the basis of the observed data Xobs, Zobs
We first consider the Bayesian estimation by investigating the following poste-
rior distribution of ∗with given Xobs and Zobs,
p∗XobsZobs ∝pXobsZobs∗p∗
where pXobsZobs∗ is the observed data likelihood and p∗ is the prior
density of ∗. Owing to the complexities of the model and the data,
p∗XobsZobs is usually very complicated. In a similar way to the Bayesian
analyses discussed in previous chapters in solving this problem, we utilize the
idea of data augmentation (Tanner and Wong, 1987), and then perform the
posterior simulation with the MCMC methods. For the current situation, it is
natural to augment the observed data XobsZobs with the latent and missing
quantities XmisYmisYobs = VmisYobs. As Ymis is included, it is not
necessary to augment with the corresponding Zmis. A sufficiently large number
of random observations will be simulated from the joint posterior distribu-
tion ∗VmisYobsXobsZobs	, so that its distribution can be approximated

358
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
adequately from the empirical distribution of the generated observations. This
task can be similarly completed by a hybrid algorithm that combines the Gibbs
sampler (Geman and Geman, 1984), and the MH algorithm (Metropolis et al.,
1953; Hastings, 1970) as before. Bayesian estimates of parameters in ∗and
the standard error estimates can be obtained through the sample of simulated
observations, ∗ttV
t
misY
t
obst = 1   T ∗, that are drawn from the
joint posterior distribution.
The
following
conditional
distributions:
p∗VmisYobsXobsZobs
p∗VmisYobsXobsZobspVmis∗YobsXobsZobs and pYobs∗,
VmisXobsZobs, are required in the implementation of the Gibbs sampler. With
given Y = YmisYobs and V = VmisVobs, the conditional distributions corre-
sponding to ∗and  can be derived in exactly the same way as in the situation
with fully observed data. Similarly, with VmisXobs and Zobs given, the condi-
tional distribution corresponding to Yobs can be derived as in previous chapters.
We only need to pay more attention to the conditional distribution corre-
sponding to Vmis. Under the usual assumption that v1   vn are mutually
independent, it follows that:
pVmis∗YobsXobsZobs =
n
i=1
pvmisi∗yobsixobsizobsi
(12.1)
where vmisi = xmisiymisi is the ith data point in the random sample of
size n. The individual vmisi can be separately simulated from the conditional
distribution in Equation (12.1), hence the simulation is simple and is not
affected by the missing patterns. Moreover, it will be seen in the subsec-
tions of this chapter that even for complex SEMs, the conditional distribution
pvmisi∗yobsixobsizobsi is usually simple. Consequently, the computa-
tional burden for simulating Vmis from its conditional distribution is light. Note
that the statistical properties of the Bayesian estimates obtained do not depend
on the sample sizes within the missing patterns.
To address the model comparison problem, we let M0 and M1 be two
competing models and consider the computation of the following Bayes factor
B10 = pXobsZobsM1
pXobsZobsM0
The log B10 can be similarly computed by path sampling (Gelman and Meng,
1998). Consider the following class of densities defined by a continuous param-
eter t in 01	:
p∗VmisYobsXobsZobst = p∗VmisYobsXobsZobst/zt

12.3
NONLINEAR SEM WITH MISSING MIXED DATA
359
where zt = pXobsZobst. Let t be a parameter linking the competing
models M0 and M1 such that z1 = pXobsZobst = 1 = pXobsZobsM1 and
z0 = pXobsZobst = 0 = pXobsZobsM0, then B10 = z1/z0. Based on
the reasoning given in Chapter 5, it can be shown in the same way that the
logarithm of Bayes factor can be estimated as follows:

log B10 = 1
2
S
s=0
ts+1 −tsU s+1 +U s
(12.2)
where t0 = 0 < t0   tS < tS+1 = 1, which are fixed grids in [0, 1], and
U s = J −1
J
j=1
U∗jjV
j
misY
j
obsXobsZobsts
(12.3)
in which ∗jjV
j
misY
j
obsj = 1   J is a sample of observations simu-
lated from p∗VmisYobsXobsZobsts and
U∗VmisYobsXobsZobst = d
dt log pVmisYobsXobsZobs∗t
where pVmisYobsXobsZobs∗t is the complete data likelihood. As we
have a program to simulate ∗VmisYobs in the Bayesian estimation, the
implementation of the path sampling procedure for computing the logarithm
Bayes factor is straightforward. Basically, by focusing on the raw observations,
and conducting the Bayesian approach with data augmentation and MCMC
methods, the methodologies in analyzing SEMs with fully observed data can be
generalized to handle missing data that are MAR without too many new deriva-
tions nor too much additional programming work. It just needs to deal with
one additional simple component in the Gibbs sampler (see Equation (12.1),
and concrete examples in the following sections).
12.3
NONLINEAR SEM WITH MISSING CONTINUOUS AND
ORDERED CATEGORICAL DATA
We first consider the analysis of a nonlinear SEM with missing continuous and
ordered categorical data. As this model is rather general, the results presented
here can be applied to many special cases, such as a linear SEM with missing
continuous and ordered categorical data, or a nonlinear SEM with missing
continuous data. This nonlinear SEM is the same as that defined in Section 8.3;
however, for completeness it is briefly described here.

360
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
Consider p × 1 random vectors v1   vn, which are identically and inde-
pendently distributed, and satisfy the following measurement equation:
vi = +i +i
i = 1   n
(12.4)
where  is a vector of intercepts,  is an unknown parameter matrix, i is
a q × 1 vector of latent variables, i is a vector of error measurements with
distribution N0 ,   = diag1   p and i and i are independent.
The latent vector i is partitioned into subvectors 	T
i 
T
i T which satisfy the
following nonlinear structural equation:
	i = 	i +H
i+i
(12.5)
where 	i and 
i are q1 × 1 and q2 × 1 latent random vectors, respectively,
H
 = h1
    hm
T is a vector-valued function with differentiable
functions h1   hm and m ≥q2, and  and  are unknown parameter matrices.
Moreover, it is assumed that Iq1 − is independent of elements in , 
i and
i are independently distributed as N0	 and N0 	, respectively; and
  = diag1   q1. Let  =  and Gi = 	T
i H
iT T , then
the above structural equation can be written as:
	i = Gi+i
(12.6)
Without loss of generality, suppose vi = xT
i yT
i T , where xi is an r ×1 vector
of observed measurements and yi is an s ×1 vector of unobserved measurements.
The information of y = y1   ysT is given by an observable ordinal categorical
vector z such that
z =
⎛
⎜⎝
z1



zs
⎞
⎟⎠
if
1z1 < y1 ≤1z1+1



szs < ys ≤szs +1

(12.7)
where zk is an integral value in the set 01   bk for k = 1   s, k0 = −
and kbk+1 = . Hence, for the kth variable, there are bk +1 categories which
are defined by unknown threshold parameters kj. Here, we assume the model
is identified after imposing appropriate conditions as describe in Chapter 6.
To deal with the missing data problem, let xi = xobsixmisi and zi =
zobsizmisi where xobsi and zobsi represent the observed data, while xmisi and
zmisi represent the missing data. For a fully observed xizi data point, xmisi and
zmisi are empty. Let yi = yobsiymisi represent the latent continuous measure-
ments, where yobsi and ymisi are respectively corresponding to zobsi and zmisi.

12.3
NONLINEAR SEM WITH MISSING MIXED DATA
361
Let vobsi = xobsiyobsi and vmisi = xmisiymisi, thus vi = vobsivmisi. If there
are no missing data in the whole data set, then it is not necessary to define
xmisizmisiymisi and vmisi, and the observed data set is xizii = 1   n
as in Section 8.3. Here, we require the additional notation due to the pres-
ence of missing data. In this situation, we keep in mind that each data point is
composed of its observed part and missing part, and the Bayesian methods are
developed on the basis of the observed data set xobsizobsii = 1   n.
We first consider Bayesian estimation of the unknown parameters in ∗=
, where  contains all the structural parameters of the model, whilst
 contains all the thresholds parameters. Let p be the prior density
of  and , Xobs = xobsii = 1   n and Zobs = zobsii = 1   n be
observed continuous and ordered categorical data, respectively. The joint poste-
rior density of  and  given Xobs and Zobs is pXobsZobs. We again
apply the idea of data augmentation to the posterior analysis. As before, we
have to augment the observed data with the matrix of latent variables  =
ii = 1   n, and unobservable continuous measurements that under-
line the observed and missing ordered categorical data. Due to the presence
of missing entries in the observable continuous measurements we additionally
need to augment with these missing entries in the posterior analysis. More
specifically, we let Xmis = xmisii = 1   n, Yobs = yobsii = 1   n and
Ymis = ymisii = 1   n be the latent data. Further, we let Vobs = vobsii =
1   n, Vmis = vmisii = 1   n, X = XobsXmis, Y = YobsYmis and
V = VobsVmis. The observed data XobsZobs are augmented with the missing
quantities XmisYmisYobs = VmisYobs in the posterior analysis. A suffi-
ciently large number of random observations will be simulated from the joint
posterior distribution VmisYobsXobsZobs	 so that its distribution can
be approximated adequately by the empirical distribution of the generated
observations. The following Gibbs sampler (Geman and Geman, 1984) will be
used. At the jth iteration with current values j, j, V
j
mis, j and Y
j
obs, we
iteratively generate:
j+1 from pXobsZobsjV
j
misjY
j
obs
j+1 from pXobsZobsj+1V
j
misjY
j
obs
V
j+1
mis
from pVmisXobsZobsj+1j+1jY
j
obs
j+1Y
j+1
obs
 from pYobsXobsZobsj+1j+1V
j+1
mis

(12.8)
Recall that once ymisi is given, it is not necessary to simulate zmisi. Thus, the
missing data set zmisii = 1   n is not involved. As before, convergence
of the Gibbs sampler is monitored by the ‘estimated potential scale reduction
(EPSR)’ values corresponding to the parameters, and/or by inspecting several
parallel sequences of observations generated from different starting values.

362
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
Conditional distribution corresponding to  and Yobs with given Vmis
can be derived in exactly the same way with fully observed data V = XY as
described in Chapter 8. More specifically, based on the same conjugate prior
distributions of  as given in Equations (8.5) and (8.9), the conditional distri-
butions corresponding to components in  are given by Equations (8.6), (8.7),
(8.8) and (8.10). The conditional distribution of  is given by Equation (8.11).
Compared with the Gibbs sampler (see Equation (8.21)) for analyzing the
model without missing data, the only additional task to implement the Gibbs
sampler of Equation (12.8) just involves the conditional distribution corre-
sponding to Vmis.
For i = 1   n, since vi are mutually independent, vmisi are also mutually
independent. Since   is diagonal, vmisi is independent with vobsi = xobsiyobsi.
Let pi be the dimension of vmisi, it follows from Equation (12.4) that
pVmisiXobsZobsYobs =
n
i=1
pvmisii and
vmisii	
D= Nmisi +misii misi	
(12.9)
where misi is a pi ×1 subvector of  with elements corresponding to observed
components deleted, misi is a pi ×q submatrix of  with rows corresponding
to observed components deleted and  misi is a pi ×pi submatrix of   with the
appropriate rows and columns deleted. Note that even though the form of Vmis
is complicated with many distinct missing patterns, its conditional distribution
only involves a product of very simple normal distributions. The computational
burden for simulating Vmis is light.
Simulating observations from pVmisYobsXobsZobs is straightfor-
ward, because they are the familiar normal, Gamma and inverted Wishart
distributions. In a similar way to the analysis of the model with fully
observed data, conditional densities involved in pVmisYobsXobsZobs
and pYobsZobs are nonstandard and complex. The well-known
Metropolis–Hastings (MH) algorithm (Metropolis et al., 1953; Hastings, 1970)
is again used to simulate observations from these conditional distributions. As
the implementation of the MH algorithm is very similar to that described in
Sections 8.2.3 and 6.3.2, the discussion is not repeated here. Bayesian estimates
and their standard error estimates are similarly obtained via the sample mean
and the sample covariance matrix as before. Moreover, estimated residuals ˆi
and ˆi can be obtained in a similar way to previous chapters.
We now consider the issue on model comparison of two competing nonlinear
SEMs M0 and M1 on the basis of the observed data XobsZobs. Again, the
Bayes factor B10 is used, and a procedure for computing this statistic is devel-
oped via path sampling (Gelman and Meng, 1998). Based on the general

12.3
NONLINEAR SEM WITH MISSING MIXED DATA
363
framework given in Section 12.2, the logarithm Bayes factor is estimated by
Equation (12.2) with
¯Us = J −1
J
j=1
UjjV
j
misjY
j
obsXobsZobsts
(12.10)
in which jjV
j
misjY
j
obsj = 1   J is a sample of observations
simulated from pVmisYobsXobsZobsts, and
UVmisYobsXobsZobst = d
dt log pVmisYobsXobsZobst
(12.11)
Defining a linked model with a good choice of continuous path to link the
competing models M0 and M1 is crucial in the path sampling procedure. This
is done on a problem-by-problem basis. As an illustration, we consider the
following nonlinear SEMs:
M0 
vi = 0 +0i +i 	i = 0G0i+i i = 1   n
M1 
vi = 1 +1i +i 	i = 1G1i+i i = 1   n
(12.12)
where 000G0 and 111G1 are two sets of parame-
ters and functions of  that associate with M0 and M1 respectively. In general,
some components in one set may be equal to or different from the corre-
sponding components in the other set. These models can be linked up by t in
[0,1] as below:
Mt 
vi = 1−t0 +0i	+t1 +1i	+i
	i = 1−t0G0i+t1G1i+i i = 1   n
Clearly, when t = 0, Mt = M0 and when t = 1, Mt = M1. It follows from the
definition of the model that log pVmisYobsXobsZobst is equal to
n
i=1

C −1
2itT  −1
 it+itT  −1
 it	


(12.13)
where
C
is
a
constant
independent
of
t,
it = vi −1 −t0 +
0i −t1 + 1i and it = 	i −1 −t0G0i −t1G1i. In
pVmisYobsXobsZobst,  is the parameter vector in the linked model.

364
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
It contains all the common and distinct parameters in 0, 1, 0, 1, 0
and 1, ,   and  . Differentiating the above log-likelihood function with
respect to t, we have
UVmisYobsXobsZobst =
itT  −1
 1 +1i −0 −0i	+itT  −1
 1G1i−0G0i	
(12.14)
The Bayes factor is computed via Equations (12.10) and (12.11) with a sample
jjV
j
misj, Y
j
obsj = 1   J simulated by the hybrid algorithm.
Note that in the Bayesian approach, the presence of missing data does not
induce much difficulty in computing the logarithm Bayes factor.
12.3.1
A Simulation Study
The main purpose is to illustrate the empirical performance of the proposed
Bayesian approach and to study the impact of missing data in estimation and
model comparison. Random observations are generated from an NSEM defined
by Equations (12.4) and (12.5) with six manifest variables that are related with
latent variables , 1 and 2. The specifications in Equation (12.4) are
 = 0   0T 
and
T =
⎡
⎣
1
21
0
0
0
0
0
0
1
42
0
0
0
0
0
0
1
63
⎤
⎦
(12.15)
where 1s and 0s in  are fixed, 21, 42 and 63 are unknown parameters with
true values 0.8, 0.7 and 0.8, respectively. The nonlinear structural equation is
given by
M1   = 11 +22 +32
1 +
with true values of 1, 2 and 3 equal to 0.6, 0.6 and 0.3, respectively. True
variances of ik and  are all equal to 0.5, 11 = 22 = 1
0 and 12 = 0
2. Contin-
uous measurements corresponding to the last two variables are transformed to
ordered categorical data via true thresholds 1 = 2 = −1
0−0
60
61
0,
where −1
0 and 1.0 are fixed.
In each replication, a complete data set with 500 random observations is
generated. Then MAR missing data are created as follows. (1) 100 fully observed
data points are randomly selected, and the sample mean of the first four variables,
¯x1, ¯x2, ¯x3 and ¯x4 are computed. (2) For each element, x1, x2, x3 and x4, and
in each and every of the remaining 400 observations, we generate randomly

12.3
NONLINEAR SEM WITH MISSING MIXED DATA
365
an observation u from the uniform distribution on 01	 to decide to whether
the element is missing or not. More specifically, we randomly generate four
independent observations u1, u2, u3 and u4 from the uniform distribution on
01	, then x1 is deleted if x1 + ¯x1 + ¯x2 > u1 −1
0, otherwise x2 is deleted if
x2 + ¯x1 + ¯x2 > u2 −1
0; also x3 is deleted if x3 + ¯x3 > u3, otherwise x4 is deleted
if x4 + ¯x4 > u4. In the created missing data sets, all entries relating to y1 and
y2 are retained and about two-thirds of the observations contain one or more
missing entries in x1   x4. The number of fully observed data points in each
replication can be different. For example, we see from Table 12.1 that the
number of fully observed data in replication 1 is 179, and that number in
replication 10 is 150.
We will compare the above NSEM (M1) to a linear SEM which is defined with
the same measurement equation as specified, and the following linear structural
equation:
M0   = 11 +22 +
To give some rough idea about the sensitivity of the results to prior inputs,
three types of hyperparameter values in the conjugate prior distributions of
 (see Equations (8.5) and (8.9)) are considered. For k = 1   6, we fixed
H0yk = I and H0k = I for all types of prior inputs. Then in prior inputs Type I,
we take 0 = 10, R0 = 4I, 0k = 0k = 8, 0k = 0k = 10 for all k; and other
values in 0k and 0k equal to the true parameter values. Hyperparameter
values in prior inputs Type II and Type III are respectively equal to half and
twice those given in Type I. On the basis of the fully observed data, and the
Table
12.1
Estimated log B10 in the 10 replications. ‘FOD’ stands for fully observed
data, and ‘FBA’ stands for the proposed Bayesian approach that uses all fully observed
and incomplete data, and ‘LDA’ stands for the listwise approach.
# of
FBA
LDA
Rep.#
FOD
II
I
III
II
I
III
1
179
8
201
6
805
5
809
0
283
0
279
0
349
2
156
5
777
5
234
4
439
0
205
0
251
0
173
3
172
7
427
6
350
4
739
0
849
0
998
1
129
4
144
3
005
2
868
2
163
0
190
0
172
0
188
5
165
5
280
4
694
3
660
0
351
0
292
0
312
6
161
5
035
4
121
3
378
0
093
0
129
0
246
7
175
3
880
3
543
2
816
0
300
0
316
0
252
8
193
6
730
5
530
4
611
0
692
0
733
0
862
9
184
8
550
7
241
6
303
0
484
0
450
0
383
10
150
8
280
7
365
5
773
0
325
0
287
0
231
Note: Tables 12.1–12.4 are taken from Lee and Song (2004).

366
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
fully and partially observed data with missing entries, estimates of the logarithm
Bayes factors for comparing M1 and M0 are obtained via the path sampling
procedure with S = 20 and J = 1000. Results obtained from 10 replications
are reported in Table 12.1. Based on the criterion for interpreting the Bayes
factor, in most cases, results produced by the Bayesian approach that uses all
the fully and partially observed data (FBA) clearly suggest the correct model is
M1; but results obtained from the listwise deletion approach (LDA) that only
uses fully observed data cannot provide a definite suggestion. It seems that the
conclusions are not changed with the above different prior inputs.
Results of simulation studies to investigate the accuracy of Bayesian estimates
for mixed continuous and order categorical data have been reported in Song
and Lee (2002), and Lee and Song (2004) in the context of linear and nonlinear
SEMs, respectively. The conclusions are that Bayesian estimates are accurate,
and are significantly better than the listwise deletion approach under some
missing patterns and sample sizes.
12.3.2
An Illustrative Example
To illustrate the methodology further, a portion of the data set obtained from
a study (Morisky et al., 1998) of the effects of establishment policies, knowl-
edge and attitudes on condom use among Filipino commercial sex workers
(CSWs) is analyzed. As the nature of commercial sex work promotes the spread
of AIDS and other sexually transmitted diseases, promotion of safer sexual
practice among CSWs is an important issue. The primary concerns are on the
developments and findings from an AIDS preventative intervention for Filipino
CSWs. The data set was collected from female CSWs in the cities of the Philip-
pines. The entire questionnaire consisted of 134 items, covering the areas of
demographics knowledge, attitudes, beliefs, behaviors, self-efficacy for condom
use and social desirability. In our illustrative example, only six manifest vari-
ables v1   v6 are selected. Variables v1 and v2 are related to the ‘worry
about getting AIDS’, v3 and v4 are related to the ‘number of times of vaginal
sex in the last seven days’ and the ‘average weekly money (in pesos) earned
as an entertainer’, while v5 and v6 are about the ‘attitudes of getting AIDS
from sexual intercourse using a condom’. Variables v3 and v4 are continuous,
while the others are ordered categorical measured with a five-point scale. Since
the respondents and nonrespondents with the same values of recorded vari-
ables do not differ systematically on the values of variables missing for the
nonrespondents, we assume that the missing values are missing at random
and the missing mechanism is ignorable. After deleting obvious outliers, the
data set contains 1080 observations, only 754 of them are fully observed. The
missing patterns are presented in Table 12.2. Note that some missing patterns
just have a very small number of observations. These missing patterns cannot
be analyzed by the multisample method with the covariance structure anal-
ysis approach. To unify scales of the continuous variables, the corresponding

12.3
NONLINEAR SEM WITH MISSING MIXED DATA
367
Table
12.2
Missing patterns and their sample sizes: AIDS data set, ‘×’ and ‘o’
indicate missing and observed entries, respectively.
Manifest variables
Manifest variables
Pattern
Sample
size
1
2
3
4
5
6
Pattern
Sample
size
1
2
3
4
5
6
1
784
o
o
o
o
o
o
11
7
×
o
o
o
×
o
2
100
×
o
o
o
o
o
12
7
×
o
o
o
o
×
3
57
o
×
o
o
o
o
13
9
o
×
o
o
×
o
4
6
o
o
×
o
o
o
14
3
o
×
o
o
o
×
5
4
o
o
o
×
o
o
15
1
o
×
o
×
o
o
6
25
o
o
o
o
×
o
16
1
o
×
×
o
o
o
7
26
×
o
o
o
o
×
17
4
o
×
o
o
×
×
8
17
×
×
o
o
o
o
18
2
×
o
o
o
×
×
9
23
o
o
o
o
×
×
19
1
o
o
×
o
×
×
10
2
×
o
×
o
o
o
20
1
×
×
o
o
×
×
raw continuous data are standardized. The sample means and standard devi-
ations of the continuous variables are 1
581203
74 and 1
841096
32,
respectively. The cell frequencies of each individual ordinal categorical vari-
able range from 21 to 709 (see Morisky et al. (1998) for other descriptive
statistics).
To identify parameters associated with the ordered categorical variables 11,
14, 21, 24, 31, 34, 41 and 44 are fixed at −0
478, 1.034, −1
420,
0.525, −0
868, 0.559, −2
130 and −0
547, respectively. These fixed values
are selected via kh = ∗−1fk, where ∗is the distribution function of N01	,
and fk are observed cumulative marginal proportions of the categories with
zk < h. Based on the meanings of the questions corresponding to the selected
manifest variables, the data set is analyzed by a model with three latent variables
, 1 and 2 and the measurement equation as specified in Equations (12.4)
and (12.15).
Competing models associated with the same measurement equation but the
following different structural equations are considered for illustration purposes:
M1   =11 +22 +
M2   =11 +22 +32
1 +
M3   =11 +22 +412 + and
M4   =11 +22 +52
2 +
Note that M1 is nested in M2, M3 and M4, whilst M2, M3 and M4 are nonnested.
Estimated logarithm Bayes factors are obtained by the proposed path sampling

368
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
procedure with S = 20 and J = 1000. Assuming that we have no prior infor-
mation from other sources, we conduct an initial Bayesian estimation based on
M1 with noninformative priors in order to obtain prior inputs of some hyper-
parameters. Again, three types of prior inputs are considered. Prior inputs in
Type I are the same as those given by Type I in the simulation study except
that the true values are replaced by the Bayesian estimates obtained from the
initial estimation, and the prior distribution of 34 and 5 is a normal distri-
bution with mean zero and a large variance. Prior inputs in Type II and Type
III are obtained the same way as in the simulation study.
We are interested in comparing the linear model M1 with the nonlinear
models. It is easy to construct a path to link the competing models. For example,
the linked model Mt for M1 and M2 is Mt   = 11 +22 +t32
1 +. Hence,
when t = 0, Mt = M0 and when t = 1, Mt = M1. Results obtained on the basis
of the FBA and LDA approaches are presented in the top part of Table 12.3.
The estimated log Bayes factors are not very sensitive to Type I, Type II and
Type III prior inputs. In comparing M2 and M1,

log B21 obtained from the FBA
approach clearly recommends the nonlinear model M2, while

log B21 obtained
from the LDA approach provides no suggestion. From

log B31 and

log B41, the
other nonlinear models are not significantly better than M1. Hence, M2 is the
best model from M1   M4.
To compare M2 with more complex models, we consider the following models
with more complicated structural equations:
M5   =11 +22 +32
1 +412 +
M6   =11 +22 +32
1 +52
2 +
Here, M2 is nested in M5 and M6. The estimated logarithm Bayes factors are
presented in the bottom part of Table 12.3. We see that the more complex
models are not significantly better than M2; hence the simpler model M2 is
selected. We compute and find that the PP p-value (Gelman, Meng and Stern,
Table
12.3
Estimated log Bayes factors under different prior inputs.
LDA
FBA
Priors
I
II
III
I
II
III
log B21
0
551
0
864
0
418
2
303
2
325
2
097
log B31
0
296
0
296
0
186
0
340
0
411
0
354
log B41
0
811
0
871
0
846
0
780
0
766
0
714
log B52
0
419
0
496
0
336
0
406
0
466
0
395
log B62
0
364
0
344
0
307
0
489
0
442
0
414

12.3
NONLINEAR SEM WITH MISSING MIXED DATA
369
1996) corresponding to M2 is 0.572. This indicates that M2 fits the data well.
In almost all cases, the EPSR values in monitoring the convergence of the
hybrid algorithm for drawing observations from the posterior distribution are
less than 1.2 after about 500 iterations. For completeness, estimates of unknown
parameters in M2 obtained by the FBA approach on the basis of different prior
inputs are reported in Table 12.4.
Based on the results obtained, an NSEM has been chosen. Its specifica-
tion about  in the measurement equation suggests that there are three non-
overlapping latent factors , 1 and 2, which can be roughly interpreted as
‘worry about AIDS’, ‘aggressiveness’ of CSWs and ‘attitude to the risk of getting
AIDS’. Based on the Type I prior inputs, these latent factors are related by the
following nonlinear structural equation:  = 0
5451 −0
0332 −0
2262
1 +,
estimated with Type I prior inputs. Thus, ‘aggressiveness’ of the CSWs has both
linear and quadratic effects on ‘worry about AIDS’. Plotting the quadratic curve
of  against 1, we find that the maximum of  is roughly at 1 = 1
2, and 
decreases as 1 moves away from both directions at 1.2. This indicates that the
‘more aggressive’ CSWs are not afraid of getting AIDS; on the other hand, the
‘less aggressive’ CSWs are not worried about getting AIDS. From the model
comparison results, the model with the quadratic term of ‘attitude to the risk
of getting AIDS’ or the corresponding interaction term with ‘aggressiveness’
is not as good. Thus, these nonlinear relationships are not important, and it is
not necessary to consider the more complicated model that involves both the
interaction and quadratic terms.
Table
12.4
Bayesian estimates of parameters in M2, under different prior inputs.
Bayesian estimate
Bayesian estimate
Par
I
II
III
Par
I
II
III
21
0.228
0.226
0.226
1
0.593
0.510
0.660
42
0.353
0.318
0.402
2
0.972
0.973
0.975
63
0.358
0.276
0.405
3
0.519
0.433
0.586
4
0.943
0.943
0.940
1
0.544
0.505
0.538
5
0.616
0.545
0.716
2
−0
033
−0
051
−0
076
6
1.056
1.086
1.048
3
−0
226
−0
153
−0
223
12
−0
030
−0
031
−0
015
11
0.508
0.592
0.442
13
0.340
0.345
0.359
12
−0
029
−0
028
−0
460
22
−0
961
−0
964
−0
966
22
0.394
0.479
0.305
23
−0
620
−0
623
−0
627
32
−0
394
−0
399
−0
399

0.663
0.668
0.659
33
0.257
0.255
0.256
42
−1
604
−1
613
−1
604
43
−0
734
−0
736
−0
738

370
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
12.4
MIXTURE OF SEMS WITH MISSING DATA
In this section, we consider the mixture of SEMs with missing data and an
unknown number of components. As one does not know the component
memberships of the observations, it is not possible to handle missing data in
mixtures of SEMs by replacement with the mean estimate or the least square
estimate. We will present a Bayesian approach to solve the problem, and give an
example to reveal the impact of ignoring the incomplete data on model compar-
ison. The mixture model has been discussed in Chapter 11, Section 11.2, hence,
it is just briefly described here for completeness.
A K component mixture SEM for a p × 1 random vector yi is defined as
follows:
f yi =
K
k=1
kfkyikk i = 1   n
(12.16)
For the kth component,
yi = k +kki +ki
(12.17)
	ki = k	ki +k
ki +ki
(12.18)
where k
D= N0 k	, 
k and k are independently distributed as N0k and
N0 k, respectively. The definitions of the random vectors and parameter
matrices, as well as the assumptions of this mixture SEM are exactly the same as
those given in Equations (11.1), (11.2) and (11.3). Recall that as the mixture
model is invariant with respect to permutation of labels k = 1   K, adoption
of a unique labelling for identifiability is important. The MCMC approach
proposed by Frühwirth-Schnatter (2001) as described in Chapter 11 will also
be used here to deal with the label switching problem. Moreover, for each k =
1   K, the covariance matrix k is identified by fixing appropriate elements
in k, k and/or k at preassigned values that are chosen on a problem-by-
problem basis. Let  = 1   K, and ∗be the parameter vector that
contains all unknown parameters in k and kk = 1   K. In the terminology
of Chapter 11,  = ∗.
To deal with the missing data problem, let yi = yobsiymisi, where yobsi
represents the observed elements of yi, whilst ymisi represents the missing entries.
Bayesian analysis of the current mixture of SEMs will be studied on the basis of
the observed data set Yobs = yobsii = 1   n. Let Ymis = ymisii = 1   n
be the collection of missing data, Y = YobsYmis, 1 = 1   n be the
matrix of latent variables and W = w1   wn be the matrix of allocation
variables (see Equation (11.5)). In the posterior analysis, the observed data Yobs
is augmented with Ymis, 1 and W. The Gibbs sampler (Geman and Geman,
1984) is applied to simulate observations from the joint posterior distribution

12.4
MIXTURE OF SEMS WITH MISSING DATA
371
pYmis1W∗Yobs as follows. At the j + 1th iteration with a current
∗jj, Y
j
misi, 
j
1 and Wj,
Step a  generate Y
j+1
mis

j+1
1
Wj+1 from pYmis1WYobs∗jj
Step b  generate ∗j+1j+1 from p∗YobsY
j+1
mis

j+1
1
Wj+1
(12.19)
Step (a) can be further broken down into the following three steps:
Step a1 
generate Wj+1 from pW∗jjY
j
misYobs
Step a2 
generate 
j+1
1
from p1∗jjY
j
misWj+1Yobs
Step a3 
generate Y
j+1
mis
from pYmis∗jj
j+1
1
Wj+1Yobs
Note that these steps are very similar to those given in Section 11.3, under
the situation without missing data. The prior distributions of the parameters
in ∗and  are taken from the conjugate prior distributions and the Dirichlet
distribution as given in Section 11.3. As Y = YmisYobs is given, the conditional
distributions corresponding to ∗and  in Step (b), and those corresponding
to W and 1, in Steps (a1) and (a2) can be obtained from the expressions given
as before.
The remaining conditional distribution pYmis∗1WYobs in Step
(a3) is derived below. For i = 1   n, as yi are independent given 1, ymisi
are also conditionally independent. As   is diagonal; and given wii, and
∗ymisi is independent of yobsi. Moreover, with given Wi, we know the compo-
nent membership of ymisi; hence,  is irrelevant. Let pi be the dimension of
ymisi, it follows that
pYmis∗WYobs1 =
n
i=1
pymisi∗iwi
and
ymisi∗iwi = k	
D=Nmisik +misikki misik	
where misik is a pi ×1 subvector of k with elements corresponding to observed
components deleted, misik is the corresponding pi × q submatrix of k, and
 misik is the corresponding pi ×pi submatrix of  k, with the appropriate rows
and/or columns deleted. Thus, even though the form of Ymis is complicated
with many distinct missing patterns, its conditional distribution only involves
a product of very simple normal distributions. The computational burden for
simulating Ymis is light.

372
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
Let M0 and M1 be two mixtures of SEMs with different members of compo-
nents. The choice between M0 and M1 is based on the Bayes factor that is
defined by
B10 = pYobsM1
pYobsM0
(12.20)
Path sampling can be used again to compute log B10, by utilizing the simu-
lated observations Y
j
mis
j
1 ∗jj from the joint posterior distribution,
see Equation (11.18). To explicitly demonstrate how to apply the above proce-
dure for Bayesian model selection in the context of a mixture of SEMs with
missing data, let
M1 
yobsi∗	 ∼
K
k=1
kfkyobsikk
(12.21)
corresponding to a model of K components, and
M0 
yobsi∗∗	 ∼
c
k=1
∗
kfkyobsikk
(12.22)
corresponding to a model of c components with component probabilities ∗
k,
where 1 ≤c < K. Note here the density in each component is based on the
observed data yobsi. To apply the proposed method for computing log B10, these
competing models are linked up by a path with t ∈01	:
Mt 
yobsi∗t	 ∼1 +1−ta1c+1 +···+K	f1yobsi11+···
+c +1−tacc+1 +···+K	fcyobsicc+tc+1fc+1yobsic+1c+1
+···+tKfKyobsiKK
(12.23)
where a1   ac are given non-negative weights such that a1 + ··· + ac = 1.
Clearly, when t = 1, Mt reduces to M1; when t = 0, Mt reduces to M0 with ∗
k =
k +akc+1 +···+K, k = 1   c. The complete data log-likelihood func-
tion log pYobsYmis1∗t is given by Equation (11.23), and its derivative
is given in Equation (11.24).
12.4.1
An Illustrative Example
To illustrate the path sampling procedure in model comparison, a real example
on the basis of a small portion of ICPSR data set collected in the project World

12.4
MIXTURE OF SEMS WITH MISSING DATA
373
Values Survey 1981–1984 and 1990–1993 (World Values Study Group, 1994)
is analyzed. Based on the data obtained from the UK, six variables (variables 180,
96, 62, 176, 116 and 117, see Appendix 1.1) that are related to respondents’
job and homelife are taken as manifest variables in y = y1   y6T . These
variables were measured via a 10-point scale; for convenience, they are treated
as continuous in this example. There are 1483 random observations, many of
them are with missing entries and some of the sample sizes within the missing
patterns are very small. The missing patterns are presented in Table 12.5.
We observe that there are only 196 fully observed data. From the questions
associated with the manifest variables, it is natural to consider a measurement
model Equation (12.17) with three latent variables: , 1 and 2, such that the
first two manifest variables are indicators for , the third and fourth manifest
variables are indicators for 1, and the remaining manifest variables are indicators
for 2. More specifically, the following specifications on the parameter matrices
of component are used:  = 0,  = 12,   = ,
T =
⎛
⎝
1
0∗
21
0∗
0∗
0∗
0∗
0∗
0∗
1
0∗
42
0∗
0∗
0∗
0∗
0∗
0∗
1
0∗
63
⎞
⎠
 =
11
12
21
22


(12.24)
and  = diag1   6. The latent variables can be roughly interpreted as
‘job satisfaction, ’, ‘homelife, 1’, and ‘job attitude, 2’.
This data set is analyzed on the basis of a mixture of SEMs with an unknown
number of components. The formulation of the model in every component is
taken to be the same as in Equation (12.24). The path sampling procedure is
used to select a model with the most appropriate number of components. In
Table
12.5
Missing patterns and their sample sizes: ICPSR data set, ‘×’ and ‘o’
indicate missing and observed entries, respectively.
Manifest variables
Manifest variables
Pattern
Sample
size
1
2
3
4
5
6
Pattern
Sample
size
1
2
3
4
5
6
1
196
o
o
o
o
o
o
9
3
o
×
×
o
×
×
2
630
o
o
×
o
o
o
10
2
×
o
×
o
o
o
3
515
o
o
×
o
×
×
11
2
×
o
×
o
×
×
4
106
o
o
o
o
×
×
12
2
o
o
o
o
×
×
5
10
o
o
×
×
o
o
13
1
o
o
o
o
o
×
6
7
o
o
×
×
×
×
14
1
o
×
o
o
×
×
7
4
o
×
×
o
o
o
15
1
×
o
o
o
×
×
8
3
o
o
×
o
o
×

374
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
the implementation of the Gibbs sampler, the following hyperparameter values
in the conjugate prior distributions are used. For m = 1   pl = 1   q,
0 = ¯ywhere ¯y is obtained on the basis of the 196 fully observed data
0 = 92I
0km = 00kl = 0H0ykm = IH0kl = I
0k0k = 0k0k = 230 and 0 = 20R −1
0
= 5I
(12.25)
For the prior distribution of , the value of the hyperparameter  in the
symmetric Dirichlet distribution is taken to be 1. Again, for each ts, the conver-
gence of the Gibbs sampler algorithm is monitored by ESPR values associated
with different starting values. We observe that the algorithm converged quickly
within 500 iterations. The logarithm Bayes factors were estimated by using 20
fixed grids in [0,1], and a total of J = 1000 additional observations were simu-
lated by the Gibbs sampler after a burn-in phase of 500 iterations. Let Mk denote
the mixture model with k components, the logarithm Bayes factor estimates
obtained via FBA are equal to

log B21 = 61
42 and

log B32 = −0
96, whilst the
estimates obtained via LDA are equal to

log B21 = −0
77 and

log B32 = −0
46,
respectively. According to the criterion for interpreting the log Bayes factor, a
two component model is selected by FBA, whilst a one component model is
selected by LDA. Clearly, the conclusions are different.
To cross-validate the selection results obtained via the logarithm Bayes factors
with respect to prior inputs, the following hyperparameter values in the conju-
gate distributions are considered:
Type A:
Same prior inputs as given in Equation (12.25), except 0k0k =
0k0k are perturbed as below. I: (2,30), II: (2,20) and III:
(2,40).
Type B:
Same prior inputs as given in Equation (12.25), except 0 is
perturbed as below. I: 0 = 92I, II: 0 = 32I and III: 0 = 9/22I.
Type C:
Same prior inputs as given in Equation (12.25), except 0 is perturbed
as below. I: 0 = 20, II: 0 = 10 and III: 0 = 30.
The estimated logarithm Bayes factors based on these prior inputs are reported in
Table 12.6. We observe that under all different prior inputs, the log B21 estimates
obtained from FBA are very large. These results clearly give the same conclusion
on selecting a two component mixture SEM. From the log B32 estimates, it is
obvious that a two component model is preferable to a three component model.
Finally, the log B21 estimates obtained by LDA suggest a misleading conclusion
to select a one component model. In Bayesian estimation, we find from the

12.5
NONLINEAR SEMS WITH NONIGNORABLE MISSING DATA
375
Table
12.6
The estimated logarithms of Bayes factors under different prior inputs.
Type
A
B
C
I
II
III
I
II
III
I
II
III
FBA
log B21
61
42
45
93
73
31
61
42
64
86
65
03
61
42
59
54
61
11
log B32 −0
96 −0
97 −0
94 −0
96 −0
93 −0
90 −0
96 −0
90 −0
94
LDA
log B21 −0
77 −0
94
0
28 −0
77 −0
08
0
24 −0
774 −0
72 −0
84
Table
12.7
Bayesian estimates of parameters in the two component model in
analyzing the ICPSR data set.
PAR
Estimate in
Component 1
Estimate in
Component 2
PAR
Estimate in
Component 1
Estimate in
Component 2

0
317
0
683
11
0
984
1
259
1
6
879
8
843
12
−0
124
0
185
2
5
975
8
177
22
0
603
0
816
3
1
913
2
520
1
2
812
0
620
4
5
157
5
448
2
3
937
0
965
5
5
770
8
151
3
2
423
1
352
6
5
146
7
735
4
4
820
2
535
21
0
535
0
996
5
5
929
1
342
42
2
349
2
375
6
6
305
3
185
63
2
308
1
082

2
603
0
764
1
0
268
0
182
2
−0
794
0
366
MCMC samples simulated by the random permutation sampler that 11 < 21
is a suitable identifiability constraint. By using the permutation sampler with
this identifiability constraint, Bayesian estimates of the selected mixture model
with two components are obtained. Results are presented in Table 12.7. We
observe that parameters of 1, 2, 3, 5, 6, 21, 63, 2, 12, 1, 2, 3, 4,
5, 6 and  under component 1 and component 2 are quite different. This
indicates that a two component model is more reasonable.
12.5
NONLINEAR SEMS WITH NONIGNORABLE
MISSING DATA
In addition to the ignorable missing data that are MAR, many missing data in
behavioral, medical, social and psychological research are nonignorable in the
sense that the reason for missing depends on the observed data and the missing
data themselves. For example, the side effects of the treatment may make the

376
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
patients worse and thereby affect patients’ participation. Nonignorable missing
data are more difficult to handle than the ignorable missing data, and have
received considerable attention in statistics (see Diggle and Kenward (1994) and
Ibrahim, Chen and Lipsitz (2001) among others). In the field of SEM, limited
work has been done on analyzing nonignorable missing data. In this section, we
present a Bayesian approach (see Lee and Tang, 2006) for analyzing a nonlinear
SEM with nonignorable missing data. For brevity, we focus on continuous
data. However, the Baysian development can be extended to other SEMs or to
dichotomous and/or ordered categorical data, based on the key ideas presented
in this section and previous chapters. Again, the idea of data augmentation and
the MCMC tools will be used in the Bayesian analysis. Bayesian estimates of
the unknown parameters, and the Bayes factor will be computed. Although
Ibrahim, Chen and Lipsitz (2001) pointed out that the parametric form of
the assumed missing mechanism itself is not ‘testable’ from the data, the Bayes
factor provides a useful statistic for comparing different missing data models.
Moreover, in the context of a given nonignorable missing data model, the Bayes
factor can be used to select a better NSEM for fitting the data.
12.5.1
The Model and the Nonignorable Missing Mechanism
For each p × 1 random vector vi = vi1   vipT
in the data set V =
v1   vn, we define a missing indicator r i = ri1   ripT such that rij = 1
if vij is missing, and rij = 0 if vij is observed. Let r = r 1   r n; and let
Vmis and Vobs be the missing and observed data, respectively. If the distribu-
tion of r is independent of Vmis, the missing mechanism is defined to be MAR;
otherwise the missing mechanism is nonignorable (see Little and Rubin, 1987).
For a nonignorable missing mechanism, it is necessary to investigate the condi-
tional distribution of r given V, rV, where  is a parameter vector. If this
distribution does not contain unknown parameters in , the missing mecha-
nism is called nonignorable and known, otherwise it is called nonignorable and
unknown. An example of a nonignorable and known mechanism is censored
data with a known censoring point. For analyzing missing data with a nonig-
norable and unknown mechanism, the basic issues are to specify a reasonable
model for r given V, and then develop statistical methods for analyzing this
model together with the model in relation to V.
Let v1   vn be independent random observations, and let 1   n
be the corresponding latent vectors in the hypothesized nonlinear SEM as
defined by Equations (12.4), (12.5) and (12.6). In this section, we consider
the situation where the manifest vector vi is incompletely observed with an
nonignorable mechanism. Let vi = vT
obsivT
misiT , where vobsi is a p1i ×1 vector
of observed manifest variables, vmisi is a p2i ×1 vector of missing components,
and p1i + p2i = p. Here, we assume an arbitrary pattern of missing data in vi,
and thus vi = vT
obsivT
misiT may represent some permutation of the indices of
the original vi. Let r ivii	 be the conditional distribution of r i given vi

12.5
NONLINEAR SEMS WITH NONIGNORABLE MISSING DATA
377
and i with a probability density function pr ivii that is related to the
nonignorable missing mechanism. Let  be the structural parameter vector that
contains all unknown distinct parameters in    and . Our main
interest is in the posterior inferences about  and  based on the missing data
indicator r and the observed data Vobs = vobs1   vobsn. According to the
definition of the model, the joint posterior density of  and  based on Vobs
and r is given by:
pVobsr ∝

n
i=1

ivmisi
pviipr iviipididvmisi

×p
(12.26)
where p denotes the joint prior distribution of  and . In general, the
integral in Equation (12.26) does not have a closed form and its dimension is
equal to the sum of the dimensions of i and vmisi.
We now consider the selection of a model for the nonignorable missing
mechanism. Theoretically, any general model can be taken. However, one must
be careful not to use too complicated or too large a model, since it can easily
become unidentifiable. Moreover, too complex a model will also induce diffi-
culty in deriving the corresponding conditional distributions of the missing
responses given the observed data, and inefficient sampling from those condi-
tional distributions. Since the observations are independent,
prV =
n
i=1
pr ivii
where  = 1   n. As the covariance matrix of the error measurement,
i, is diagonal, it follows that when i is given, the components of vi are
independent. Hence, for j ̸= ℓ, it is reasonable to assume that the conditional
distributions of r ij and r il given i are independent. Under this assumption, we
propose the following binomial model for the nonignorable missing mechanism
(see Ibrahim, Chen and Lipsitz, 2001; Lee and Tang, 2006):
prV =
n
i=1
p
j=1
prrij = 1vii	rij 1−prrij = 1vii	1−rij 
(12.27)
Ibrahim, Chen and Lipsitz (2001) pointed out that since rij is binary, one
can use a sequence of logistic regressions for modeling prrij = 1vii
in Equation (12.27). They also pointed out that this model has the potential
for reducing the number of parameters in the missing data mechanism, yields

378
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
correlation structures between the rijs, allows more flexibility in specifying
the missing data model, and facilitates efficient sampling from the conditional
distribution of the missing response given the observed data. Following the
suggestion given in Ibrahim, Chen and Lipsitz (2001), the following logistic
regression model is used:
mvii = logitprrij = 1vii	
= 0 +1vi1 +···+pvip +p+1i1 +···+p+qiq=T d i
(12.28)
where d i = 1vi1   vipi1   iqT and  = 01   p+qT . For the
normal random effects model: vi = X i + Z ibi + i, where X i and Z i are
covariates,  is an unknown parameter vector and bi is the vector of latent
random effects with distribution N0	; Ibrahim, Chen and Lipsitz (2001)
suggested the use of the logistic regression that depends on vi but not bi. They
pointed out this is a reasonable assumption in practice, since models for r ivi	
can be chosen to resemble and closely approximate a model for r ivibi	
that might include bi. Considering the similarity between the measurement
equation of our NSEM and the above normal random effect model, and based
on the above arguments of Ibrahim, Chen and Lipsitz (2001), it is desirable to
adopt the following special case of Equation (12.28) which does not depend
on i:
mvii = logitprr ij = 1vii	 = 0 +1vi1 +···+pvip
(12.29)
Moreover, it follows from the measurement equation and the basic concepts
of latent variables and their indicators in SEMs that the characteristics of i
are revealed by the manifest variables in vi. Hence, in practice, even if we
think the nonignorable missing data depend on i, the missing mechanism
can still be adequately accounted for by the model given in Equation (12.29)
that only depends on vi. However, for generality, the Bayesian approach will
be developed on the basis of the more general model of Equation (12.28).
As Equation (12.29) is equivalent to specifying p+1 = ··· = p+q = 0 in
Equation (12.28), the modifications of the general Bayesian development in
addressing this special case are straightforward. We do not recommend the
routine use of Equations (12.28) or (12.29) for modeling the nonignorable
missing mechanism in every practical application. Other missing mechanism
models may be preferable for situations where one is certain about the specific
form for the missing mechanism. However, the proposed model specified in
Equations (12.28) or (12.29) is a reasonable one, and is useful for sensitivity
analysis of the estimates with respect to missing data with different missing
mechanisms.

12.5
NONLINEAR SEMS WITH NONIGNORABLE MISSING DATA
379
12.5.2
Bayesian Analysis of the Model
In this and the following sections, let T
k and T
k be the kth row vectors
of  and , respectively; k and k be the kth diagonal elements of
  and  , respectively. Let Vmis = vmis1   vmisn be the set of missing
values associated with the manifest variables. Again, we use the useful strategy
that combines the idea of data augmentation and MCMC methods in the
Bayesian analysis. The observed data Vobs and the missing data indicator r
are augmented with the missing quantities Vmis in the posterior anal-
ysis. The Gibbs sampler (Geman and Geman, 1984) is used to generate
a sequence of random observations from the joint posterior distribution
VmisVobsr	, and then the Bayesian estimates are obtained from
the observations in this generated sequence. In this algorithm, observations
Vmis are sampled iteratively from the following conditional distri-
butions: pVobsVmisr = pVr, pVmisVobsr,
pVobsVmisr = pVr,
and
pVobsVmisr =
pV. Note that because of the data augmentation, the last conditional
distribution does not depend on r, and can be obtained from the expressions
in Chapter 8.
Consider the conditional distribution pVr. Note that when Vmis
is given, the underlying model with missing data reduces to an NSEM with fully
observed data. Thus, it follows from a derivation similar to that in Chapter 8 that
pVr =
n
i=1
pivir i
∝
n
i=1
pviip	i
ip
ipr ivii
where pivir i is proportional to
exp

−1
2
T
i −1
i −1
2vi −−iT  −1
 vi −−i−1
2	i−Gi	T
× −1
 	i −Gi	+
 p
j=1
rij

T d i −p log1+expT d i	


(12.30)
To derive pVmisVobsr, we note that r i is independent of .
Moreover, as   is diagonal, vmisi is independent of vobsi. It can be shown that:
pVmisVobsr =
n
i=1
pvmisivobsiir i
∝
n
i=1
pvmisiipr ivii

380
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
and pvmisivobsiir i is proportional to
exp

−1
2vmisi−misi−misiiT  −1
misivmisi−misi−misii+

p
j=1
rij

T d i

1+expT d i	p

(12.31)
where misi is a p2i × 1 subvector of  with its elements corresponding to
missing components of vi, misi is a p2i ×q submatrix of  with its rows corre-
sponding to missing mponents of vi, and  misi i is a p2i ×p2i submatrix of  
with the rows and columns corresponding to missing components of vi.
Finally, we consider the conditional distribution of  given V and r.
Let p be the prior density of , such that 
D= N0H0, where 0 and
H0 are the hyperparameters whose values are assumed to be given by the prior
information. Since the distribution of r only involves V,  and , and it is
assumed that the prior distribution of  is independent of the prior distribution
of , we have
pVr ∝prVp
Thus, it follows from Equations (12.27) and (12.28) that pVr is
proportional to
exp

n
i=1

p
j=1
rij

T d i −1
2−0T H−1
0 −0

n
i=11+expT d i	p

(12.32)
This completes the derivation of the full conditional distributions that are
required in the implementation of the Gibbs sampler. The conditional
distributions pivir ipvmisivobsiir i and pVr are
nonstandard. Some details of the MH algorithm for simulating observations
form these conditional distributions are presented in Appendix 12.1
We again propose the well-known Bayes factor for model comparisons. Let
M0 and M1 be two competing models, the Bayes factor (see Kass and Raftery,
1995) is defined as
B10 = pVobsrM1
pVobsrM0
(12.33)
where
pVobsrMk =

pVobsrkkpkkdkdk
k = 10
(12.34)

12.5
NONLINEAR SEMS WITH NONIGNORABLE MISSING DATA
381
is the marginal density of Mk with parameter vectors k and k, and pkk
is the prior density of k and k. As pVobsrMk is difficult to evaluate,
computation of the Bayes factor is a very challenging problem (see DiCiccio,
Kass, Raftery and Wasserman, 1997). In a similar way to the treatment of other
models, the following path sampling (Gelman and Meng, 1998) procedure is
used to evaluate the logarithm Bayes factor.
We consider the following class of densities defined with a continuous param-
eter t ∈01	 
pVmisVobsrt = pVmisVobsrt/zt
where
zt = pVobsrt =

pVmisVobsrtddddVmis
and t is in [0,1] for linking the competing models M0 and M1 such that for k =
01zk = pVobsrt = k = pVobsrMk. Let UVmisVobsrt =
d log pVmisVobsrt/dt,
where
pVmisVobsrt
is
the
complete data likelihood function at t. On the basis of a natural assumption
that the prior of ( is independent of t, it can be shown by similar reasoning
to that in the previous chapters that
log B10 = log z1
z0 =
 1
0 EVmisUVmisVobsrt	dt
where EVmis
is the expectation with respect to the distribution
pVmisVobsrt. Let t0 = 0 < t1 < t2 < ··· < tS < tS+1 = 1 be
fixed and ordered grids; then, log B10 is estimated by

log B10 = 1
2
S
s=0
ts+1 −ts ¯Us+1 + ¯Us
where
¯Us = J −1
J
j=1
UjjjV
j
misVobsrts
and jjjV
j
misj = 1   J are observations that are simulated from
pVmisVobsrts

382
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
12.5.3
An Illustrative Real Example
To give an illustration of the proposed methodology, a small portion of the
ICPSR data set collected by the World Values Survey 1981–1984 and 1990–
1993 (World Values Study Group, 1994) is analyzed in this example (see also
Lee and Tang (2006)). Here, eight variables of the original data set (variables
116, 117, 252, 253, 254, 296, 298 and 314, see Appendix 1.1) are taken
as manifest variables in v = v1   v8T . These variables are measured on a
10-point scale and for convenience they are treated as continuous. We choose
the data corresponding to females in Russia, who either answered question
116 or 117, or both. Under this choice, most of the data were obtained from
working females. There are 712 random observations in the data set in which
there are only 451 (63.34%) fully observed cases. The missing data are rather
complicated, with 69 different missing patterns. Considering that the questions
are either related to personal attitudes or related to personal morality, the
corresponding missing data are better treated as nonignorable. To roughly unify
the scales, the raw data are standardized using the sample mean and standard
deviation obtained from the fully observed data.
Based on the meanings of the questions corresponding to the manifest vari-
ables, we propose an NSEM with the following specifications. For the measure-
ment equation, we consider  = 1   8T , and
T =
⎡
⎣
1
0∗
21
0
0∗
0
0∗
0
0∗
0
0∗
0
0∗
0
0∗
0
0∗
0
0∗
1
0∗
42
52
0
0∗
0
0∗
0
0∗
0
0∗
0
0∗
0
0∗
0
0∗
0
0∗
1
0∗
73
83
⎤
⎦
which corresponds to latent variables , 1 and 2. As before, we choose the
structure that gives nonoverlapping latent factors on the basis of the meaning of
the questions, and for clear interpretation. The latent variable  can be roughly
interpreted as ‘job satisfaction’, and the latent variables 1 and 2 can be roughly
interpreted as ‘job attitude’ and ‘morality (in relation to money)’, respectively.
We first consider the following model M1, which involves an encompassing
structural equation with all second-order terms of i1 and i2:
M1  i = 1i1 +2i2 +3i1i2 +42
i1 +52
i2 +i
Although the Bayes factor can be applied to compare many other missing data
models, the following three models are considered for assessing the missing
data in this illustrative example:
Ma 
logitprrij = 1vii	 = 0 +1vi1 +···+8vi8
Mb 
logitprrij = 1vii	 = 0 +1i +2i1 +3i2
MM 
MAR

12.5
NONLINEAR SEMS WITH NONIGNORABLE MISSING DATA
383
Note that Ma involves all the manifest variables, while Mb involves all the latent
variables.
The logarithm Bayes factors for comparing the above models MaMb and MM
under the NSEM M1 are computed via the path sampling procedure. The prior
inputs in the conjugate prior distributions are selected as before via an auxilary
estimation. The number of grids in the path sampling procedure for computing
all the logarithm Bayes factors is taken to be 10, and for each ts 5000 simulated
observations collected after 5000 burn-in iterations are used to compute ¯Us. To
give some idea about the convergent behaviors in simulating the observations,
plots of EPSR values (Gelman, 1996) of all parameters against iterations in
analyzing the encompassing model M1 with Ma are displayed in Figure 12.1.
In addition, plots of three parallel sequences generated from different starting
values of some parameters (others are similar) are presented in Figure 12.2.
The estimated log Bayes factors are equal to

log B
1
ab = 47
34 and

log B
1
aM =
43
85, where the superscript of 
log B indicates the NSEM M1, and subscripts
of

log B indicate the competing models for the missing mechanism. Clearly,
from the estimated logarithm Bayes factors, the data give strong evidence to
support the missing data model Ma, which is in the form of the proposed model
Equation (12.29), for modeling the nonignorable missing data.
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10 000
1
2
3
4
5
6
7
8
9
10
Iteration
EPRS
Figure
12.1
EPSR values of all parameters against iteration numbers in the real
example with encompassing model M1 and missing data model Ma. This figure is taken
from Lee and Tang (2006).

384
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
−0.5
0.0
0.5
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10 000
(a)
iteration
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10 000
(b)
iteration
−0.5
0.0
0.5
1.0
1.5
2.0
2.5
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10 000
(c)
iteration
0.0
0.5
1.0
1.5
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10 000
(d)
iteration
−0.5
0.0
0.5
1.0
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10 000
(e)
iteration
−0.2
−0.1
0.0
0.1
0.2
0.3
0.4
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10 000
(f)
iteration
−3.2
−3.0
−2.8
−2.6
−2.4
−2.2
−2.0
−1.8
−1.6
−1.4
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10 000
(g)
iteration
−0.4
−0.2
0.0
0.2
0.4
0.6
0.8
Figure
12.2
(a), (b), (c), (d), (e), (f) and (g) are plots of three parallel sequences
corresponding to different starting values of 52122120 and 3 against iter-
ation numbers in the real example with the encompassing model M1, and missing data
model Ma. This figure is taken from Lee and Tang (2006).

12.5
NONLINEAR SEMS WITH NONIGNORABLE MISSING DATA
385
In addition to the encompassing NSEM M1, we also consider the following
NSEMs:
M2 
i =1i1 +2i2 +32
i1 +i
M3 
i =1i1 +2i2 +3i1i2 +i
M4 
i =1i1 +2i2 +32
i2 +i
Under each of the above models, we compare the missing mechanism models
MaMb and MM . The estimated logarithm Bayes factors are reported in
Table 12.8, for example, 
logB2
ab = 48
56 and 
logB3
aM = 44
04. It is clear from
the results in this table that for every M2M3 and M4, the data strongly support
Ma. For each case, Bayesian estimates obtained under Ma are different from
those obtained under MAR. To save space, these estimates are not reported.
Based on the above comparison results, the model Ma is used for comparing
NSEMs M1M2M3 and M4. In this model comparison, the estimated log Bayes
factors are equal to

log B12 = −1
29 
log B32 = −2
59 and

log B42 = −1
51.
These results give evidence that the data support the NSEM M2.
The
Bayesian
estimates
and
their
standard
error
estimates
of
the
unknown parameters in the selected model M2 are presented in the left
columns of Table 12.9. We observe that the estimates of the coefficients
023456 and 8 are significantly different from zero. This result
indicates that the nonignorable missing data model for accounting the nature
of the missing data is necessary. The factor loading estimates indicate significant
associations between the latent variables and their indicators. From ˆ11 ˆ12
and ˆ22, the estimate of the correlation between 1 and 2 is 0.163. This esti-
mate indicates that ‘job attitude, 1’, and ‘morality, 2’, is weakly correlated.
The estimated nonlinear structural equation is equal to
i = −0
103i1 +0
072i2 +0
3062
i1
Table
12.8
The estimated log Bayes factors: log Br
ab and
log Br
aM , r = 234.
SEM
log Br
ab
log Br
aM
M2
48
56
46
66
M3
50
40
44
04
M4
50
38
44
69
Note: Tables 12.8 and 12.9 are taken from Lee and Tang (2006).

386
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
Table
12.9
Bayesian estimates and their standard error estimates of M2 with Ma.
PAR
Our method
WinBUGS
PAR
Our method
WinBUGS
EST
SE
EST
SE
EST
SE
EST
SE
0
−2
791
0
043
−2
794
0
076
1
−0
135
0
038
−0
139
0
065
1
0.038
0
033
0.040
0
059
2
−0
136
0
032
−0
129
0
058
2
−0
280
0
037
−0
280
0
068
3
0.018
0
023
0.015
0
039
3
0.370
0
036
0.365
0
073
4
0.004
0
023
0.005
0
041
4
−0
265
0
041
−0
262
0
083
5
−0
129
0
026
−0
139
0
045
5
−0
455
0
070
−0
502
0
126
6
−0
046
0
023
−0
040
0
041
6
−0
405
0
073
−0
341
0
154
7
0.053
0
026
0.045
0
046
7
0.059
0
056
0.013
0
134
8
0.144
0
026
0.141
0
045
8
0.332
0
038
0.323
0
061
21
0.917
0
129
0.830
0
168
42
0.307
0
060
0.317
0
123
52
0.328
0
068
0.320
0
119
73
1.244
0
122
0.955
0
203
83
0.455
0
071
0.388
0
114
1
0.544
0
067
0.508
0
096
2
0.637
0
060
0.673
0
080
3
0.493
0
058
0.492
0
111
4
0.935
0
033
0.932
0
059
5
0.907
0
039
0.922
0
068
6
0.640
0
042
0.548
0
095
7
0.612
0
051
0.714
0
086
8
1.065
0
040
1.065
0
069
1
−0
103
0
047
−0
103
0
085
2
0.072
0
052
0.044
0
081
3
0.306
0
083
0.317
0
139
11
0.459
0
057
0.459
0
113
12
0.062
0
016
0.071
0
033
22
0.316
0
041
0.405
0
096

0.413
0
056
0.463
0
105
The interpretation of this equation in relation to the effect of the exogenous
latent variables i1 and i2 to the endogenous latent variable i is similar to the
interpretation of other nonlinear structural equations.
12.6
ANALYSIS OF SEMS WITH MISSING DATA VIA
WINBUGS
The software WinBUGS (Spiegelhalter, Thomas, Best and Lunn, 2003) can be
used to produce Bayesian solutions, such as Bayesian estimates, standard error
estimates and estimated residuals, for SEMs with missing data that are ignor-
ably missing with MAR or missing with a nonignorable missing mechanism.

12.6
ANALYSIS OF SEMS WITH MISSING DATA VIA WINBUGS
387
As nonignorable missing data subsume missing data that are MAR, we focus
on a discussion about nonignorable missing data. In the context of a specific
SEM, in addition to the WinBUGS codes that are required to specify that SEM,
we require codes that relate to the nonignorable missing data, which involve
the definition of the missing model, the prior distribution in relation to the
parameters in the missing model, etc..
The real example given in Section 12.5.3 has been reanalyzed by using
WinBUGS with the same settings – for example, the same model structure and
same prior inputs. Results obtained on the basis of M2 with Ma are reported in
the right-hand columns of Table 12.9. The DIC value corresponding to this
model is 16 961.2. We observe that the estimates obtained from WinBUGS
are close to our estimates that are presented in the left-hand columns of
Table 12.9. However, the numerical standard error (SE) estimates produced
by this general software are larger than those that are produced by our tailor-
made program for the specific SEM. Estimated residual plots, ˆi1, ˆi4 and ˆi
versus case numbers, are displayed in Figure 12.3. Some estimated residual
plots of ˆi versus ˆi1 and ˆi2, and plots of ˆi4 versus ˆi1, ˆi2 and ˆi, are
presented in Figures 12.4 and 12.5, respectively. These plots roughly indicate
(a)
0
200
400
600
(b)
0
200
400
600
(c)
0
200
400
600
hat{epsilon}_i1
–2
–1
2
1
0
hat{epsilon}_i4
–2
–1
2
1
0
hat{delta}_i
–2
–1
2
1
0
Figure
12.3
Estimated residual plots (a) ˆi1, (b) ˆi4 and (c) ˆi.

388
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
(a)
–1.0
–0.5
0.0
0.5
1.0
(b)
–0.5
0.0
0.5
1.0
1.5
2.0
hat{delta}_i
–3 –2 –1 0
3
2
1
hat{delta}_i
–3 –2 –1 0
3
2
1
Figure
12.4
Plots of estimated residuals ˆi versus (a) ˆi1 and (b) ˆi2.
(a)
–1.0
–0.5
0.0
0.5
1.0
hat{epsilon}_i4
–3 –2 –1 0
3
2
1
(b)
–0.5
0.0
0.5
1.0
1.5
2.0
hat{epsilon}_i4
–3 –2 –1 0
3
2
1
Figure
12.5
Plots of estimated residuals ˆi4 versus (a) ˆi1, (b) ˆi2 and (c) ˆi.

APPENDIX 12.1
389
(c)
–1.0
–0.5
0.0
0.5
1.0
hat{epsilon}_i4
–3 –2 –1 0
3
2
1
Figure
12.5
(Continued)
that the measurement equation and the structural equation are adequate. From
Figure 12.5(b) we observe that the empirical distribution of ˆi2 is skewed to
the left. For this complicated nonlinear SEM with nonignorable missing data,
estimates of latent variables should be used with great caution. The WinBUGS
codes and data are given at the following website: http://www.wiley.com/go/
lee_structural.
APPENDIX 12.1: IMPLEMENTATION OF THE MH ALGORITHM
Simulating observations from pivir i is based on the reasoning in
Roberts (1996). Specially, we choose N02
 as the proposal distribution,
where −1
 =  +T  −1
 ,
 =

T
0  −1
 0
−T
0  −1
 
−T T  −1
 0
−1 +T T  −1
 

+
p exp0 +T
I vi
1+exp0 +T
I vi2 II T
II 
with 0 = I −,  =  H 
i/ 
T
i

i=0, I = 1   pT , and II =
p+1   p+qT 
 The MH algorithm is implemented as follows. At the j +
1th iteration with a current value 
j
i , a new candidate i is generated from
N
j
i 2
 and accepted with probability:
min

1 pivir i
p
j
i vir i


The variance 2
 can be chosen such that the average acceptance rate is approx-
imately 0.25 or more (see Gelman, Roberts and Gilks, 1995).

390
12
STRUCTURAL EQUATION MODELS WITH MISSING DATA
To sample vmi and  from pvmivoiir i and pVr, we
respectively choose N02
v vmi and N02
 as their corresponding
proposal distributions, where
−1
vmi =  −1
mi +
p exp0 +
l∈¯D lvil +T
II i

1+exp

0 +
l∈¯D lvil +T
II i
 !2 mT
m
−1
 = p
4
n
i=1
d id T
i +H−1
0 
in which m is a vector that contains the elements of  corresponding to
vmi,  mi is a submatrix of   corresponding to vmi, ¯D is the set of indexes
corresponding to voi and 2
v and 2
 are chosen as before.
REFERENCES
Afifi, A. A. and Elashoff, R. M. (1969) Multivariate two sample tests with dichotomous
and continuous variables. I: The location model. The Annals of Mathematical Statistics,
40, 290–298.
Arbuckle, J. L. (1996) Full information estimation in the presence of incomplete data.
In G. A. Marcoulides and R. E. Schumacker (eds), Advanced Structural Equation
Modeling. Mahwah, NJ: Lawrence Erlbaum Publishers.
DiCiccio, T. J., Kass, R. E., Raftery, A. and Wasserman, L. (1997) Computing Bayes
factors by combining simulation and asymptotic approximations. Journal of the
American Statistical Association, 92, 903–915.
Diggle, P. and Kenward, M. G. (1994) Informative drop-out in longitudinal data analysis
(with discussion). Applied Statistics, 43, 49–93.
Frühwirth-Schnatter, S. (2001) Markov chain Monte Carlo estimation of classical and
dynamic switching and mixture models. Journal of the American Statistical Association,
96, 194–208.
Gelman, A. (1996). Inference and monitoring convergence. In W.R. Gilks, S. Richardson
and D.J. Spiegelhalter (eds), Markov Chain Monte Carlo in Practice, pp. 131–144.
London: Chapman and Hall.
Gelman, A. and Meng, X. L. (1998) Simulating normalizing constants: from importance
sampling to bridge sampling to path sampling. Statistical Science, 13, 163–185.
Gelman, A., Meng, X. L. and Stern, H. (1996) Posterior predictive assessment of model
fitness via realized discrepancies. Statistica Sinica, 6, 733–759.
Gelman, A., Roberts, G. and Gilks, W. (1995) Efficient Metropolis jumping rules . In
J.M. Bernardo, J. O. Berger, A. P. David and A. F. M. Smith (eds), Bayesian Statistics
(5th edn). Oxford: Oxford University Press.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distribution and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Hastings, W. K. (1970) Monte Carlo sampling methods using Markov chains and their
application. Biometrika, 57, 97–109.

REFERENCES
391
Ibrahim, J. G., Chen, M. H. and Lipsitz, S. R. (2001) Missing responses in generalised
linear mixed models when the missing data mechanism is nonignorable. Biometrika,
88, 551–564.
Kass, R. E. and Raftery, A. E. (1995) Bayes factors. Journal of the American Statistical
Association, 90, 773–795.
Lee, S. Y. and Song, X. Y. (2004) Bayesian model comparison of nonlinear latent
variable models with missing continuous and ordinal categorical data. British Journal
of Mathematical and Statistical Psychology, 57, 131–150.
Lee, S. Y. and Tang, N. S. (2006) Bayesian analysis of nonlinear structural equation
models with nonignorable missing data. Psychometrika, in press.
Little, R. J. A. and Rubin, D. B. (1987) Statistical Analysis with Missing Data. New York:
John Wiley & Sons, Inc.
Metropolis, N. et al. (1953) Equations of state calculations by fast computing machine.
Journal of Chemical Physics, 21, 1087–1091.
Morisky, D. E. et al. (1998) The effects of establishment practices, knowledge and
attitudes on condom use among Filipina sex workers. AIDS Care, 10, 213–220.
Roberts, G. O. (1996) Markov chain concepts related to sampling algorithms. In
W. R. Gilks, S. Richardson and D. J. Spiegelhalter (eds), Markov Chain Monte Carlo
in Practice, pp. 45–58. London: Chapman and Hall.
Song, X. Y. and Lee, S. Y. (2002) Analysis of structural equation model with ignorable
missing continuous and polytomous data. Psychometrika, 67, 261–288.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. (2003) WinBUGS User
Manual, Version 1.4. Cambridge, England: MRC Biostatistics Unit.
Tanner, M. A. and Wong, W. H. (1987) The calculation of posterier distribution by
data augmentation (with discussion). Journal of the American Statistical Association,
86, 79–86.
World Values Study Group (1994) World Values Survey, 1981–1984 and 1990–1993.
ICPSR version. Ann Arbor, MI: Institute for Social Research (producer). Ann Arbor,
MI: Inter-university Consortium for Political and Social Research (distributor).


13
Structural Equation
Models with Exponential
Family of Distributions
13.1
INTRODUCTION
The standard and the complex SEMs discussed in previous chapters are
developed under the crucial assumption that the conditional distribution of the
manifest variables, given the latent variables, is normal. For example, consider
the nonlinear SEM defined by Equations (8.1) and (8.2) in Chapter 8. Although
we do not assume that y is normal, we do assume that y is normal, or that
 is normal. In this chapter, we generalize the distribution of y from normal
to the exponential family of distributions (EFDs). This family is very general,
it includes discrete distributions such as binomial and Poisson, and contin-
uous distributions such as normal, exponential and gamma, as special cases.
Some common distributions in the univariate exponential family are presented
in Table 13.1, see also Wedel and Kamakura (2001). The generalization from
the normal distribution to EFDs is not only for theoretical interest, but also has
significant practical value, particularly in analyzing more general discrete data,
which include but are not limited to unordered binary data.
The problem of treating non-normal continuous data has received a consid-
erable amount of attention in the field of SEM. Based on the traditional covari-
ance structure analysis approach, general robust methods against the normal
assumption have been developed with the sample covariance matrix (see Browne
(1987), Shapiro and Browne (1987), Kano, Berkane and Bentler (1993) and
Yuan and Bentler (1997), among others). In this chapter, we present a Bayesian
approach that is developed on the basis of raw observations under a slightly
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

394
13
STRUCTURAL EQUATION MODELS
Table
13.1
Some common distributions in the univariate exponential family.
Distribution
Density function
Range
Discrete
Binomial, BKPr
K
y

Pry1−PrK−y
0K
Poisson, P
e−y/y!
0
Continuous
Normal, N 2
2	2−1
2 exp−y −2/22
−
Gamma, G

1/yy/
 exp−y/

0
more restrictive assumption that the non-normal data are coming from the
EFDs. As the methodologies are tailor-made for this family of distributions,
they are expected to be more effective for data that satisfy this distributional
assumption. Moveover, the Bayesian approach can be applied to more subtle
SEMs and complex data structures.
The exponential family of distributions has been extensively used in many
areas of statistics, particularly in relation to latent variable models, such as
the generalized linear models (McCullagh and Nelder, 1989) and generalized
linear mixed models (Booth and Hobert, 1999). In contrast to SEMs, the main
objective of these latent variable models is to assess the effects of covariates
on the manifest variables, and their latent variables are usually used to model
the random effects. They have been extensively applied to longitudinal studies
where nonignorable missing data are frequently encountered.
Motivated by the above consideration, in this chapter we will concentrate on a
nonlinear SEM that can accommodate covariates, variables from the exponential
family of distributions, ordered categorical variables and missing data with a
nonignorable mechanism. We will show that the strategy that combines the data
augmentation and MCMC methods is again useful in developing the Bayesian
methodologies. The model will be described in Section 13.2, Bayesian methods
are presented in Section 13.3, and results of a simulation study are reported in
Section 13.4. Section 13.5 demonstrates these methods with a real example.
The application of WinBUGS will be illustrated with an artificial example in
Section 13.6 and Section 13.7 concludes with a discussion.
13.2
THE SEM FRAMEWORK WITH EXPONENTIAL FAMILY
OF DISTRIBUTIONS
13.2.1
The Model
The aim of this section is to describe a nonlinear SEM with fixed covariates
on the basis of the exponential family of distributions (EFDs) (see Song and

13.2
THE SEM FRAMEWORK WITH EXPONENTIAL FAMILY
395
Lee (2006)). For i = 1   n, let yi = yi1   yipT be a vector of manifest
variables measured on each of the n independently distributed individuals. For
brevity, we assume that the dimension of yi is the same for every i, however,
this assumption can be relaxed without much difficulty. We wish to identify the
relationship between the manifest variables in yi and the related latent vector
i = i1   iqT with fixed covariates. For k = 1   p, we assume that
the conditional distribution of yik given i is independent and comes from the
following exponential family with a canonical parameter ik (Sammel, Ryan and
Legler, 1997):
pkyiki = expyikik −bik/k +ckyikk
(13.1)
Eyiki = ˙bik
and
Varyiki = k ¨bik
where b· and ck· are specific differentiable functions with the dots
denoting the derivatives, and ik = gkik with a link function gk. Let i =
i1   ipT xikmk ×1 be vectors of fixed covariates, and
Xi =
⎡
⎢⎢⎢⎣
xT
i1
0
···
0
0
xT
i2
···
0




0
0
0
xT
ip
⎤
⎥⎥⎥⎦
be a p ×m matrix, where m = m1 +···+mp. We propose the model i = XiA +
i for assessing the relation of i with Xi and i, where A = aT
1    aT
p T
is an m × 1 vector with ak being an mk × 1 vector of unknown parame-
ters, and  = 1   pT is a matrix of unknown parameters. Clearly, for
k = 1   p,
ik = xT
ikak +T
k i
(13.2)
Note that Equation (13.2) can accommodate an intercept k by taking a compo-
nent of xik as 1 and defining the corresponding component of ak as k. In
situations where i is distributed according to N 0∗, in which ∗is an
unknown covariance matrix, the model defined by Equation (13.2) with EFDs
subsumes some useful latent variable models in biostatistics and psychometrics.
For instance, the model can be regarded as a factor analysis model with fixed
covariates, and thus is a generalization of the normal factor analysis model with
covariates (Sammel and Ryan, 1996) to a factor analysis model with covariate
based on variables from EFDs. Moreover, it extends the factor analysis model
with EFDs (Wedel and Kamakura, 2001) to a factor analysis model with EFDs
and covariates.

396
13
STRUCTURAL EQUATION MODELS
Equation (13.2) can be viewed as a ‘measurement’ model. Its main purpose
is to identify the latent variables via the corresponding manifest variables in y,
with the help of the fixed covariates in Xi. Another basic goal of the SEM is to
assess how the latent variables affect each other, by incorporating a structural
model. Let i = T
i T
i T , where i (q1 ×1) is the endogenous latent vector
and i (q2 × 1) the exogenous latent vector. Inspired by the strong demand
for investigating the effects of fixed covariates (Sammel and Ryan, 1996; Lee
and Song, 2004) and the nonlinear effects of the latent variables in i to
i (Schumacker and Marcoulides, 1998), we propose the following nonlinear
structural model with fixed covariates:
i = Bci +	i +
Hi+i
(13.3)
where ci is a vector of fixed covariates, Hi = h1i··· hliT hji
is a nonzero differentiable function of ii is an error term and B	, and

 are unknown matrices. Fixed covariates in ci may or may not be equal
to those in xik. The distributions of i and i are N 0 and N 0 ,
respectively, and i and i are uncorrelated. For computing efficiency and
stability, the covariance matrix   is assumed to be diagonal. We assume that,
similar to many other nonlinear SEMs in previous chapters, I−	 is nonsingular
and its determinant is independent of elements in 	. Let  = B	
 and
Gi = cT
i T
i HiT T , then Equation (13.3) can be rewritten as i =
Gi+i.
In behavioral, medical and psychological research, a lot of data are ordered
categorical. To accommodate these kind of data, we allow any component y
of y to be unobservable, and its information is given by an observable ordered
categorical variable z as follows: z = k if 
k < y ≤
k+1, for k = 0··· b −1,
where − = 
0 < 
1 < ··· < 
b−1 < 
b =  is the set of thresholds that
defines the categories. As the distribution of y is defined with the EFDs, the
ordered categorical variables are incorporated into the exponential family of
distributions. Note that the underlying distribution of the ordered categorical
variable is from EFDs rather than the more restrictive normal distribution as
in the previous chapters. This model can be identified through the methods
given in the previous chapters. For instance, for each ordered categorical vari-
able which is not identifiable, we fix 
1 and 
b−1 at preassigned values; and
the SEM can be identified by the common practice of restricting the appro-
priate elements in  and  to fixed known values. Dichotomous variables
can be analyzed as a special case of ordered categorical variables with some
modifications.
13.2.2
The Nonignorable Missing Mechanism
Missing data are very common in substantive research. For generality, we
consider missing data with a nonignorable missing mechanism. Nonignorable

13.2
THE SEM FRAMEWORK WITH EXPONENTIAL FAMILY
397
missing data are more difficult to handle than ignorable missing data. As in
Chapter 12, nonignorable missing data are analyzed with a missing mechanism
using logistic regression.
To accommodate missing data, we define for each yi a missing indicator
vector ri, such that for k = 1   prik = 1 if yik is missing and rik = 0 if yik
is observed. Let yi = yT
obsiyT
misiT , where yobsi is a pi1 × 1 vector of observed
manifest variables and ymisi is a pi2 × 1 vector of the missing components in
yi  pi1 +pi2 = p. We assume that there is an arbitrary pattern ofs missing data in
yi, and thus that yT
obsiyT
misiT may represent the original yi with a permutation
of the indices. To account for the nonignorable missing mechanism, ri is treated
as random. In specifying a model in relation to the conditional distribution
riyii with a parameter vector , it is not worthwhile using too compli-
cated or large a model, because it can easily become unidentifiable. Moreover, a
model that is too complex will also make it difficult to derive the corresponding
conditional distribution of the missing responses given the observed data, and
will induce inefficient sampling of the conditional distributions in the MCMC
algorithm.
Let Y = y1   yn = 1   n and r = r1   rn. As the obser-
vations are conditionally independent, prY = pr1y11 × ··· ×
prnynn. Based on the suggestion of Ibrahim, Chen and Lipsitz (2001),
one possible model for the missing data mechanism is the following binomial
model, which assumes independence between rik:
priyii =
p
k=1
prrik = 1yiirik1−prrik = 1yii1−rik
(13.4)
The independent assumption can be relaxed by the more general multinomial
distribution that specifies the joint distribution of ri (see Ibrahim, Chen and
Lipsitz, 2001). In this chapter, the prrik = 1yii in Equation (13.4) is
modeled by the following logistic regression:
logitprrik =1yii = 0 +1yi1 +···+pyip +p+1i1
+···+p+qiq = T di
(13.5)
where  = 0··· p+qT and di = 1yi1   yipi1   iqT . As any
parameters in the sets 1   p or p+1   p+q can be fixed to zero, the
nonignorable missing mechanism defined in Equation (13.5) is rather flexible.
It can handle the special cases in which ri just depends on a subset of entries
in yi, or a subset of entries in i, or both. However, we do not recommend
using Equation (13.5) routinely for modeling the nonignorable mechanism in
every practical application. Other mechanisms may be preferable for situations

398
13
STRUCTURAL EQUATION MODELS
where one is certain about its specific form. Finally, Ibrahim, Chen and Lipsitz
(2001) mentioned that the parametric form of the missing mechanism itself is
not ‘testable’ from the data.
13.3
A BAYESIAN APPROACH
Again, the Bayesian methods are developed by the useful strategy that combines
data augmentation and some MCMC methods. In this section, we present the
full conditional distributions in the implementation of the Gibbs sampler for
simulating observations of the parameters and the latent variables from their
joint posterior distribution. These generated observations are used to obtain
the Bayesian estimates and their standard error estimates, and to compute the
Bayes factor for model comparison. Under the special case that the missing data
are MAR, we do not need to specify the missing mechanism model, and the
related components of ri and  are not involved.
13.3.1
Prior Distributions
Proper conjugate prior distributions are taken for various matrices of the
unknown regression coefficients, as well as the variance and covariance matrices
of the latent variables and the error term. More specifically, let   be the
diagonal covariance matrix of the error measurements that correspond to the
ordered categorical variables; for k = 1   p, or k = 1   q1
ak
D= N a0kH0k −1
k
D= Gamma
0k0k kk
D= N0kkH0yk
−1 D= Wq2R00 −1
k
D= Gamma
0k0k kk
D= N0kkH0k

D= N 0H0
(13.6)
where k and k are the kth diagonal elements of   and  , respectively;
k and k are the kth rows of  and , respectively; a0k, 
0k, 0k, 0k,
0, 
0k, 0k, 0k, 0, and positive definite matrices H0kH0H0ykH0k
and R0 are hyperparameters whose values are assumed to be given by the prior
information, and W ·· denotes Wishart distribution. For k ̸= l, it is assumed
that prior distributions of kk and llkk and ll, as
well as ak and al, are independent.
13.3.2
Full Conditional Distributions
We first consider the situation in which components of yobsi in yi are
neither dichotomous nor ordered categorical, but can be directly observed.

13.3
A BAYESIAN APPROACH
399
Let Y = YobsYmis, where Yobs = yobsii = 1   n is the observed data set
and Ymis = ymisii = 1   n is the missing data set. Based on the idea of data
augmentation, we focus on the joint posterior distribution YmisYobsr,
where  is the parameter vector that contains all the unknown parame-
ters in the model. The Gibbs sampler (Geman and Geman, 1984) is used
for simulating observations of the posterior distribution YmisYobsr.
The required full conditional distributions are given as follows.
It can be shown by the definition of the proposed SEM that the full condi-
tional distribution of  is given by
pYr =
n
i=1
piyiri
∝
n
i=1
pyiipii pipriyii
where piyiri is proportional to
exp
 p
k=1
yikik −bik/k +
 p
k=1
rik

T di −p log1+expT di
−1
2

T
i −1i
+i −Bci −	i −
HiT  −1
 i −Bci −	i −
Hi



(13.7)
For every i, once i is given, ymisi is independent of yobsi. It follows from the
definition of the model and Equation (13.5) that the full conditional distribu-
tion of Ym is given by
pYmisrYobs =
n
i=1
pymisiiriyobsi
where
pymisiiriyobsi ∝exp


krik=1
yikik −bik/k +ckyikk
+
 p
k=1
rik

T di −p log1+expT di

 (13.8)

400
13
STRUCTURAL EQUATION MODELS
Under the conjugate prior distributions given in Equation (13.6), it can be
shown that the full conditional distributions of the components of  are given by
pakYk−1
k  ∝exp

n
i=1
yikik −bik
k
−1
2ak −a0kT H−1
0k ak −a0k


(13.9)
pkYakk ∝
− n
2 +
0k−1
k
exp

n
i=1
yikik −bik
k
+ckyikk

−0k
k


(13.10)
pkYak−1
k  ∝exp

n
i=1
yikik −bik
k
−1
2−1
k k −0kT H−1
0ykk −0k


(13.11)
pkk
D= Gamman/2+
0kk
(13.12)
pk−1
k 
D= NakkAk
(13.13)
p
D= IWq22T
2 +R −1
0 n +0
(13.14)
where Ak = H−1
0k +GGT −1ak = AkH−1
0k0k +G1k and k = 0k+
T
1k1k −aT
kA−1
k ak +T
0kH−1
0k0k/2, in which G = G1    Gn
1 = 1   n2 = 1   n and T
1k is the kth row of 1.
It can be derived from Equation (13.5) that the full conditional distribution
of  is equal to
pYr ∝prYp ∝p
n
i=1
p
k=1
prikyiki
∝exp

n
i=1

p
k=1
rikT di −p
n
i=1
log1+expT di −1
2−0T H−1
0 −0


(13.15)
To handle the ordered categorical data, let Yk be the kth row of Y that is not
directly observable. Let zk be the corresponding ordered categorical vector that
includes nk observable components after discarding the missing entries, and let
k = 
k1   
kbk−1. It is natural to assume that the prior distribution of k

13.3
A BAYESIAN APPROACH
401
is independent of the prior distribution of . To deal with a general situation
in which there is little or no information about the thresholds, the following
noninformative prior distribution is used: pk = p
k1   
kbk−1 ∝c for

k1 < ··· < 
kbk−1, where c is a constant. Moreover, it is assumed that k and
l are independent for k ̸= l. It can be shown by a derivation similar to that in
Chapter 6, Section 6.3.1 that
pkykzk = pkzkpykkzk
∝
k
i=1
expyikik −bik/k +ckyikkI
kzik 
kzik+1yik
(13.16)
where IAy is an index function which takes 1 if y ∈A, and 0 otherwise.
The treatment of dichotomous variables is similar – see Chapter 7. Note that
for the missing entries, the underlying continuous values can be simulated by
Equation (13.8).
The Gibbs sampler algorithm proceeds by sampling iymisikyk and 
from conditional distributions of Equations (13.7) to (13.16), respectively. The
simulation of observations from the standard distributions involved in Equa-
tions (13.12), (13.13) and (13.14) is straightforward. However, various forms
of the Metropolis–Hastings (MH) algorithm (Metropolis et al., 1953; Hast-
ings, 1970) are required to simulate observations from the remaining complex
conditional distributions. Due to the complexity of the proposed model and
data structures, the implementations are not straightforward. Some details are
given in Appendix 13.1.
13.3.3
Model Comparison
In this section, we use Dobs to denote the observed data, which include the
directly observable data and the ordered categorical data, and use Dmis to
denote unobservable data, which include missing data, latent variables and
unobserved data that underlie the ordered categorical data. Moreover, let ∗=
 be the overall unknown parameter vector, where  is a vector that
includes all unknown thresholds. Suppose that Dobs has arisen under one of
the two competing models M0 and M1. For l = 01, let pDobsMl be the
probability density of Dobs under Ml. Recall that the Bayes factor is defined
by B10 = pDobsM1/pDobsM0. In a similar way to the previous chapters, a
path sampling procedure is presented to compute the Bayes factor for model
comparison.
Utilizing the idea of data augmentation, Dobs is augmented with Dmis in the
computation. Let t be a continuous parameter in [0, 1] to link the competing
models M0 and M1, let pDmisDobsr∗t be the complete data likelihood

402
13
STRUCTURAL EQUATION MODELS
and U∗DmisDobsrt = d log pDmisDobsr∗t/dt. It can be shown by
similar reasoning to that in the previous chapters that
log B10 =
 1
0 E∗DmisU∗DmisDobsrt dt
where
E∗Dmis
is
the
expectation
with
respect
to
the
distribution
p∗DmisDobsrt. This integral can be numerically evaluated with the trape-
zoidal rule. Specifically, let S different fixed grids tsS
s=0 be ordered as t0 =
0 < t1 < t2 < ··· < tS < tS+1 = 1. Then, log B10 can be computed as

log B10 = 1
2
S
s=0
ts+1 −ts ¯Us+1 + ¯Us
(13.17)
where ¯Us is the average of the U∗DmisDobsrt on the basis of all simu-
lation draws for which t = ts, that is, ¯Us = J −1 J
j=1 Uj
∗D
j
misDobsrts,
in which j
∗D
j
misj = 1   J are observations simulated from the joint
conditional distribution p∗Dmis Dobsrts.
13.4
A SIMULATION STUDY
A Bayesian approach for analyzing SEMs with dichotomous variables was
presented in Chapter 7. A dichotomous variable is an ordered categorical vari-
able that is defined by two categories with a threshold of zero. It is usually
coded with ‘0’ and ‘1’, and the probability of observing ‘0’ and ‘1’ is decided
by an underlying normal distribution with a fixed threshold. Another kind of
discrete variable which also has two categories and is usually coded with ‘0’
and ‘1’ is the binary variable. Although they have same coding, binary vari-
ables are different from dichotomous variables. Binary variables are unordered,
they do not associate with thresholds and the probabilities of observing ‘0’ and
‘1’ are decided by a binomial distribution rather than the normal distribution.
Hence, the methods for analyzing dichotomous variables should not be directly
applied to analyze binary variables. Given a variable with binary codings ‘0’
and ‘1’ it is important to decide whether it is an ordered dichotomous variable
or an unordered binary variable. Clearly, binary variables are also common in
behavioral, medical and social research.
Results obtained from a simulation study are presented here to illustrate the
empirical performance of the Bayesian approach in analyzing binary and ordered
categorical data. For brevity, we only consider ignorable missing data that are
MAR. A data set Y = yii = 1   n was generated with yi = yi1   yi9T 
For k = 123, the distribution of yik is binomial, B1pik, that is yik ∝

13.4
A SIMULATION STUDY
403
exp

yikik −log1+eik

with bik = log1+eik and ik = log pik/1−pik.
Note that for each of these binomial variables, k = 10 is treated as a fixed
parameter. For k = 456, the observations yi4yi5 and yi6 are simulated from
a multivariate normal distribution. For k = 789 the underlying latent contin-
uous measurements corresponding to yi7, yi8 and yi9 are first simulated from a
multivariate normal distribution; then they are transformed to ordered categorical
observationszi7,zi8 andzi9 viathesamethresholds−10∗−050510∗,where
−10∗and10∗withanasteriskaretreatedasfixedforidentification.The‘measure-
ment equation’ (see Equation (13.2)): ik = k + xT
ikek + T
k i, is defined by
an intercept k, a 3 × 1 vector of fixed covariates xik that is simulated from
N0I, and a 3 × 1 vector of latent variables i = ii1i2T . The specifi-
cations of the corresponding matrices of the unknown coefficients are given by:
 = 000000000000000000T ,
AT =
⎡
⎣
10
10
10
10
10
10
10
10
10
00∗
00∗
00∗
00∗
00∗
00∗
10
10
10
10
10
10
00∗
00∗
00∗
00∗
00∗
00∗
⎤
⎦
(13.18)
AT =
⎡
⎣
10∗
08
08
00∗
00∗
00∗
00∗
00∗
00∗
00∗
00∗
00∗
10∗
08
08
00∗
00∗
00∗
00∗
00∗
00∗
00∗
00∗
00∗
10∗
08
08
⎤
⎦
(13.19)
where the values given in A and  are the true population values and
as usual the entries with an asterisk denote parameters that are fixed at
that values. The true population values for the diagonal elements of   are
10∗10∗10∗101010101010. The structural equation (see Equa-
tion (13.3)) is defined by
i = b1ci1 +b2ci2 +1i1 +2i2 +3i1i2 +i
(13.20)
where ci1 and ci2 are covariates that are simulated from N01, the true values
of b1b212 and 3 are 10100606 and −04, respectively. The true
values of 1112 and 22 are 1.0, 0.3 and 1.0, respectively; while the true value
of  is 0.36. There are a total of 51 unknown parameters in this model.
A random sample of size n = 500 was simulated; 50 observations out of
the 500 observations contain missing entries that are missing at random.
Simulation results were obtained on the basis of 100 replications. The prior
inputs of the hyperparameters in the conjugate prior distributions are taken as:

0k = 90k = 8
01 = 901 = 30 = 6R0 = 6IH0k and H0yk are iden-
tity matrices, H01 = 10 and elements in 0k and 01 are taken to be the
true values. These prior inputs represent a situation with good prior informa-
tion. Based on some preliminary analyses for checking convergence, we took
3000 burn-in iterations and collected 3000 observations to obtain the Bayesian

404
13
STRUCTURAL EQUATION MODELS
Table
13.2
Performance of the Bayesian estimates with n = 500 in the simulation
study.
Par
BIAS
SD
RMS
Par
BIAS
SD
RMS
Par
BIAS
SD
RMS
1
0025
0143
0133
a11
0004
0150
0142
21
0034
0133
0127
2
0016
0135
0128
a13
0042
0147
0155
31
0047
0136
0142
3
0023
0134
0150
a21
0035
0150
0137
52
0011
0087
0094
4
0009
0065
0071
a23
0019
0147
0128
62
0004
0086
0087
5
0001
0059
0065
a31
0045
0150
0196
83
0037
0124
0123
6
0002
0059
0059
a33
0034
0148
0154
93
0015
0123
0109
7
0002
0078
0086
a41
0001
0065
0063
b1
0011
0118
0119
8
0003
0072
0072
a51
0001
0059
0059
b2
0006
0118
0140
9
0013
0071
0075
a61
0016
0059
0062
1
0009
0142
0137
4
0015
0112
0128
a71
0020
0089
0090
2
0031
0148
0172
5
0002
0091
0097
a72
0018
0089
0085
3
0036
0147
0167
6
0009
0090
0086
a81
0001
0083
0081

72
0004
0063
0060
7
0020
0165
0144
a82
0009
0083
0086

73
0002
0063
0069
8
0008
0140
0120
a91
0003
0084
0078

82
0007
0061
0059
9
0016
0141
0124
a92
0020
0084
0089

83
0004
0061
0067

0014
0107
0060
11
0019
0142
0143

92
0003
0061
0049
12
0003
0080
0083

93
0003
0061
0070
22
0023
0192
0195
solution. The absolute bias (BIAS) of the mean of Bayesian estimates and the
true values, the standard deviation (SD) of the Bayesian estimates and the root
mean squares (RMS) of the Bayesian estimates are reported in Table 13.2. It
can be seen that under the given sample size and prior inputs, the empirical
performance of the Bayesian approach is acceptable. For instance, all the ‘BIAS’
values are less than 0.05, and most of the ‘SD’ and ‘RMS’ values are less than
0.15. Note that the Bayesian estimates corresponding to 12321 and
31, and some elements in E that are associated with the binary manifest vari-
ables are not as good as others. Finally, we expect slightly worse performances
under situations with less accurate prior inputs.
13.5
A REAL EXAMPLE: A COMPLIANCE STUDY
OF PATIENTS
The Bayesian methodology is used to assess the effects of some exogenous
latent variables on patient nonadherence to medication in a study which was
conducted by the Department of Medicine and Therapeutics, the Department
of Community and Family Medicine, and the Department of Pharmacy at the
Chinese University of Hong Kong (Chan, 2001). See Chapter 7, Section 7.2.1
for a background of this study, and Song and Lee (2006) for a similar analysis. A
total of 849 ethnic Chinese patients diagnosed with hypertension were randomly

13.5
A REAL EXAMPLE: A COMPLIANCE STUDY OF PATIENTS
405
selected from hospitals and clinics to serve as subjects for the study. In this
example, we are interested in nine manifest variables, y1   y9. A translation
of the corresponding items in the questionnaires from Chinese to English is
presented in Appendix 13.2. The first three binary variables are indicators of the
latent variable of ‘patient nonadherence, ’, the next three binary variables are
indicators of the latent variable of ‘knowledge of medication, 1’, and the last
three ordered categorical variables with a five-point scale are indicators of the
latent variable of ‘(dis)satisfaction with the physician, 2’. As an illustration, three
continuous fixed covariates, ‘age of the patient’, ‘years of having hypertension’,
‘years of using antihypertensive drugs’, and a binary fixed covariate of whether
the patient ‘can read the instructions on the labels’, are incorporated in the
first six manifest variables in the measurement model Equation (13.2). In the
presence of some missing data, the frequency of ‘yes’ answers corresponding
to the binary variables ranged from 69 to 651, and the cell frequency of each
ordered categorical variable ranged from 15 to 508. In the analysis of this data
set, for each k = 1   6, the distribution of yik is naturally taken to be a
binomial distribution B1pik, such that
yik ∝exp

yikik −log1+eik


and
(13.21)
ik = log
pik
1−pik
= xT
ikak +T
k i
(13.22)
where i = ii1i2T and T
k is the kth row of the following :
T =
⎡
⎣
1
21
31
0
0
0
0
0
0
0
0
0
1
52
62
0
0
0
0
0
0
0
0
0
1
83
93
⎤
⎦
where ijs are the unknown factor loading parameters, and 1s and 0s are
fixed in the estimation to achieve an identified model. For k = 1   6k =
10, which are treated as fixed parameters. For k = 789, the elements in
ak are fixed at 0, and the unobservable variable yik that corresponds to the
observable ordered categorical variable is assumed to have a normal distribution
NT
k ik in the EFDs. To identify parameters that are associated with
ordered categorical variables, the thresholds 
71, 
74, 
81, 
84, 
91 and 
94 are
fixed via the same method presented in previous chapters in identifying ordered
categorical variables. The endogenous latent variable of patient nonadherence
() is regressed on the exogenous latent variables of knowledge of medication
(1) and the (dis)satisfaction with the physician (2), and two binary fixed
covariates that are related to whether the patient is taking other long-term
medication (c1) and the existence of side effects (c2). The following nonlinear
structural model with the fixed covariates is considered:
 = b1c1 +b2c2 +11 +22 +312 +
(13.23)

406
13
STRUCTURAL EQUATION MODELS
In this formulation of Equation (13.3), B = b1b2	 = 0 and 
 = 123.
Other unknown parameters are the variances and covariance of 1 and
21122 and 12), the variances of ordered categorical variables k for
k = 789, and the variance of . In this example, we use the logistic regres-
sion for the nonignorable missing data mechanism logit prrik = 1yii =
0 +1yi1 +···+9yi9. There are a total of 58 unknown parameters.
To specify the conjugate prior distributions, we use the following data-
dependent prior inputs. We first conducted an auxiliary Bayesian estimation
with a noninformative prior to obtain the prior inputs of the hyperparam-
eter values for a0k0k and 0k in the conjugate prior distributions in the
actual estimation. Consequently, the values of the hyperparameters are given
by 
0k = 100k = 5
0k = 100k = 50 = 8 and H0ykH0kH0k and H0
are assigned to be 025I, where I is an identity matrix with appropriate dimen-
sions, a0k = ˜ak0k = ˜k0k = ˜k and R0 = 2I, where ˜ak, ˜k and ˜k are
estimates obtained from the auxiliary estimation. The Gibbs sampler algorithm
with the MH algorithm is used to generate samples from the joint poste-
rior distribution of the parameters and the latent quantities. Three chains with
different initial values are run. To give some information about the convergence
of the algorithm, plots of these chains for some randomly chosen parameter
values against the iteration numbers are displayed in Figure 13.1. Moreover,
the ‘estimated potential scale reduction (EPSR)’ values (Gelman and Rubin,
1992) of the parameters on the basis of the different initial values are displayed
in Figure 13.2. We observe that the algorithm converged after about 5000
iterations. The Bayesian inferences are based on 10 000 observations that are
collected after convergence of the Gibbs sampler. The Bayesian estimates and
standard error estimates are obtained from the sample mean and the sample
covariance matrix of the simulated observations.
We first conduct a model comparison of the encompassing model (M0) as
defined above with the competing models M1 and M2, which are defined
by the same measurement model and the following structural models:
M1   = b1c1 + b2c2 + 11 + 22 +  and M2   = 11 + 22 + 312 + .
The interaction effect of the exogenous latent variables is not included in M1,
while the fixed covariates (c1 and c2) are not included in M2. To assess the
impact of the fixed covariates in the measurement equation, we consider an
additional competing model M3, which is defined by the structural model
given in Equation (13.23) and the measurement model that without any fixed
covariate, ik = T
k i. Models M1M2 and M3 are nested in M0. It is easy
to construct a path to link the competing models with the encompassing
model. For example, the linked model Mt for M0 and M1 is defined with
Equations (13.21) and (13.22), and the structural model  = b1c1 + b2c2 +
11 + 22 + 1 −t312 + . When t = 0Mt reduces to M0, and when
t = 1Mt reduces to M1. In the path sampling procedure, we take S = 20
and 1000 observations collected after convergence of the Gibbs sampler for
each ts. The computed logarithm Bayes factors, log B01log B02 and log B03,

13.5
A REAL EXAMPLE: A COMPLIANCE STUDY OF PATIENTS
407
(a)
0
2000
4000
6000
8000
10 000
–1.0
0.0
0.5
1.0
(b)
0
2000
4000
6000
8000
10 000
0.0
0.5
1.0
1.5
2.0
(c)
0
2000
4000
6000
8000
10 000
–4
–2
2
1
0
(d)
0
2000
4000
6000
8000
10 000
–1.0
0.0
0.5
1.0
(e)
0
2000
4000
6000
8000
10 000
0.0
0.5
1.0
1.5
2.0
(f)
0
2000
4000
6000
8000
10 000
–4
–2
0
1
2
Figure
13.1
(a), (b), (c), (d), (e) and (f) are plots of three parallel sequences corre-
sponding to different starting values of 
72, 93, 1, 12, 7 and a41 against iterations
in the analysis of the real data.
are equal to 1.23, 18.52 and 211.65, respectively. According to the crite-
rion that is based on two times the logarithm Bayes factor (Kass and Raftery,
1995), the models without fixed covariates perform significantly worse. More-
over, M0 is better than M1, which does not involve the interaction effect.
The PP p-value (see Gelman, Stern and Meng, 1996) for the goodness-of-
fit testing of M0 is 0.42, which indicates that M0 fits the data well. To reveal
the fit of the structural model Equation (13.23), the estimated residuals ˆi =
ˆi −ˆb1ci1 −ˆb2ci2 −ˆ1ˆi1 −ˆ2ˆi2 −ˆ3ˆi1ˆi2 were computed. Plots of ˆi versus the
case numbers ˆi1 and ˆi2 are displayed in Figure 13.3. These plots lie within
two parallel horizontal lines that are centered at zero, and no linear or quadratic

408
13
STRUCTURAL EQUATION MODELS
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10 000
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Figure
13.2
EPSR values against the number of iterations in the analysis of the real
data.
trends are observed. This roughly reveals that the proposed structural model is
adequate.
The Bayesian estimates of the parameters in M0 are presented in Table 13.3,
together with their standard error estimates. It can be seen from the estimates
of k that except for 1 and 2, all other s are significantly different from
zero. This confirms the demand for a nonignorable mechanism for the missing
data. As expected, some elements in ˆak ˆb1 and ˆb2 are significantly different from
zero, indicating the significant effects of the fixed covariates. Except for ˆ31,
the loading estimates in  are high, indicating a strong association between the
latent variables and their respective items. It can be calculated from ˆ11 ˆ12
and ˆ22 that the estimated correlation between 1 and 2 is −0337. This
indicates the expected phenomenon that better ‘knowledge of medication’ and
‘(dis)satisfaction with the physician’ are negatively correlated. The estimated
structural model is  = −089c1 +034c2 −0791 +0312 +06812. We have
the following interpretations. (i) From the estimates of the coefficients of the
covariates, it seems that ‘taking other long-term medication’ has a negative
effect on ‘nonadherence’, whereas ‘existence of side effects’ has a positive effect.
(ii) Better ‘knowledge of medication’ has a negative effect on ‘nonadherence’.
(iii) ‘(Dis)satisfaction with the physician’ has a positive effect on ‘nonadherence’.
The reasonable findings in (i), (ii) and (iii) provide concrete suggestions for
improving the medication scheme; for example, in allocating resources to give

13.5
A REAL EXAMPLE: A COMPLIANCE STUDY OF PATIENTS
409
5
4
3
2
1
0
–1
–2
–3
–4
–5
0
100
200
300
400
500
600
700
800
Case Number
(a)
4
3
2
1
0
–1
–2
–3
–4
–1.0
–0.8
–0.6
–0.4
–0.2
0.2
0.0
0.4
0.6
0.8
1.0
(b)
4
3
2
1
0
–1
–2
–3
–4
–2.0
–1.5
–1.0
–0.5
0.5
0.0
1.0
1.5
2.0
(c)
ξ1i
ξ2i
δi
δi
δi
Figure
13.3
Plots of residuals ˆi versus case numbers, ˆi1 and ˆi2.

410
13
STRUCTURAL EQUATION MODELS
Table
13.3
Bayesian estimates and standard error estimates of the real
example.
Par
EST
SE
Par
EST
SE
Par
EST
SE
a11
0.07
0100
21
2.60
0373

72
−015
0039
a12
−031
0171
31
0.17
0088

73
0.46
0039
a13
−030
0166
52
4.02
0393

82
−019
0039
a14
−013
0128
62
3.83
0389

83
0.27
0037
a15
−044
0186
83
0.92
0062

92
0.56
0045
a16
−026
0260
93
0.77
0059

93
1.14
0045
a21
−007
0286
7
0.32
0039
0
−478
0263
a22
0.04
0319
8
0.43
0041
1
−008
0292
a23
−012
0075
9
0.60
0047
2
0.11
0301
a24
−013
0136
b1
−089
0134
3
−123
0290
a25
0.22
0130
b2
0.34
0143
4
−094
0275
a26
0.55
0100
1
−079
0209
5
−111
0261
a31
−014
0078
2
0.31
0110
6
−064
0259
a32
−050
0143
3
0.68
0266
7
0.47
0251
a33
0.18
0143
11
0.29
0057
8
−046
0259
a34
0.27
0088
12
−014
0028
9
0.40
0231
a35
0.17
0139
22
0.69
0068
a36
0.08
0210

0.83
0222
a41
−115
0220
a42
−317
0222
a43
−005
0124
a44
−031
0221
a45
2.54
0206
a46
1.95
0184
better education and consultation to patients about the disease, and to maintain
a good ‘physician–patients’ relationship. (iv) The interaction term of ‘knowledge
of medication’ and ‘(dis)satisfaction with the physician’ has a significant effect
on ‘nonadherence’. Depending on the situation, this interaction has various
impacts. For example, it indicates that less knowledge of medication (negative
1) and a bad relationship with the doctor (positive 2) would have a very
strong additive effect on nonadherence, which would need to be reduced by the
interaction effect. Interpretations of the less important parameters in k
and  are not discussed.
The robustness of the above results to the choice of prior inputs has been
evaluated by repeating the analysis using reasonable alternative prior inputs.
Moreover, the logarithm Bayes factors have been recomputed using different
values of S and J . The same model comparison conclusion, and similar Bayesian
estimates and standard error estimates, are obtained.

13.6
BAYESIAN ANALYSIS USING WINBUGS
411
13.6
BAYESIAN ANALYSIS OF AN ARTIFICIAL EXAMPLE
USING WINBUGS
To illustrate the application of WinBUGS (Spiegelhalter, Thomas, Best and
Lunn, 2003) in analyzing the nonlinear SEMs with fixed covariates in the
context of unordered binary data, the following artificial example is used.
A data set Y = yii = 1   n was generated with yi = yi1   yi9T 
For k = 1   9, the distribution of yik is binomial, B5pik, that is yik ∝
exp

yikik −5log1+eik

bik = log1+eik and ik = log pik/1−pik =
k +T
k i. Note that for these binomial variables, k = 10 are treated as fixed
parameters. The nonlinear structural equation is given by i = bci + 1i1 +
2i2 +3i1i2 +i. The fixed covariates ci are generated from B105. The
specifications of the matrices  and  are:
T =
⎡
⎣
10∗
21
31
00∗00∗00∗00∗00∗00∗
00∗00∗00∗10∗
52
62
00∗00∗00∗
00∗00∗00∗00∗00∗00∗10∗
83
93
⎤
⎦and  =
11 12
12 22


where the 1s and 0s are treated as known parameters, whilst 21, 31, 52,
62, 83, 93, 1112 and 22 are unknown parameters. The true population
values of the unknown parameters are:  = 1··· 9T = 08··· 08T ,
21 = 31 = 83 = 93 = 06, 52 = 62 = 07, 
 = 123 = 050505,
b = 06,  = 07, 111222 = 100510.
Two independent data sets with n = 500 random observations are gener-
ated based on the above settings. The first data set contains fully observed
data without any missing data and the second data set contains some nonig-
norable missing data. We use the following logistic regression to generate the
nonignorable missing data in the second data set:
logitprrik = 1yii = 0 +1yi1 +···+9yi9
(13.24)
where the true values of parameter vector  are given by 0 = −40, 1 = ··· =
6 = 05, 7 = −15 and 8 = 9 = −10. In this created missing data set the
number of full observations is 397. That is, about 20.6 % of the total number
of observations have missing entries.
We use WinBUGS to analyze these two data sets. Hyperparameters of the
conjugate prior distributions are taken as: 
01 = 1001 = 8, 0 = 8, H0yk =
025I, H01 = 025, H0k = 025I, H0 = 025I, R0 = 50, where 0 is the true
value of ; 0, b0, 0k, 01 and 0 are all equal to the true values of the corre-
sponding parameters. In the analysis of the second data set, the missing model
specified in Equation (13.24) is used to cope with the nonignorable missing
data. Plots of sequences of observations that correspond to some parameters for
the first and second data sets are displayed in Figures 13.4 and 13.5, respectively.

412
13
STRUCTURAL EQUATION MODELS
u[5] chains 1:3
Iteration
1
10 000
20 000
30 000
40 000
    0.4
    0.6
    0.8
    1.0
    1.2
b chains 1:3
Iteration
1
10 000
20 000
30 000
40 000
    0.0
    0.5
    1.0
    1.5
lambda[8, 3] chains 1:3
Iteration
1
10 000
20 000
30 000
40 000
–0.5
    0.0
    0.5
    1.0
    1.5
gamma[2] chains 1:3
Iteration
1
10 000
20 000
30 000
40 000
–0.5
    0.0
    0.5
    1.0
    1.5
phi[2, 2] chains 1:3
Iteration
1
10 000
20 000
30 000
40 000
    0.0
    1.0
    2.0
    3.0
Figure
13.4
Three chains of observation corresponding to 5, b, 83, 2 and 22
generated by different initial values in the analysis of the first data set.

13.6
BAYESIAN ANALYSIS USING WINBUGS
413
u [2] chains 1:3
b chains 1:3
lambda[5, 2] chains 1:3
gamma[1] chains 1:3
psi_delta chains 1:3
1.50
1.25
1.00
0.75
0.50
0.25
1.5
1.0
0.5
0.0
–0.5
1.5
1.0
0.5
0.0
–0.5
1.5
1.0
0.5
0.0
–0.5
4.0
3.0
2.0
1.0
0.0
1
10 000
20 000
Iteration
30 000
40 000
1
10 000
20 000
Iteration
30 000
40 000
1
10 000
20 000
Iteration
30 000
40 000
1
10 000
20 000
Iteration
30 000
40 000
1
10 000
20 000
Iteration
30 000
40 000
Figure
13.5
Three chains of observation corresponding to 2, b, 52, 1 and 
generated by different initial values in the analysis of the second data set.

414
13
STRUCTURAL EQUATION MODELS
Table
13.4
Bayesian estimates and standard error estimates in the artificial example.
Without missing data
With nonignorable missing data
Par
True
value
EST
SE
Par
True
value
EST
SE
Par
True
value
EST
SE
1
080
079
010
1
080
082
011
0
−400
−396
040
2
080
076
007
2
080
083
007
1
0.50
0.40
012
3
080
077
007
3
080
078
007
2
0.50
0.53
013
4
080
073
006
4
080
082
007
3
0.50
0.47
012
5
080
081
006
5
080
087
006
4
0.50
0.41
011
6
080
082
006
6
080
086
006
5
0.50
0.42
013
7
080
085
007
7
080
097
007
6
0.50
0.47
013
8
080
089
005
8
080
085
005
7
−150
−121
012
9
080
085
005
9
080
093
005
8
−100
−107
012
b
060
069
012
b
060
056
013
9
−100
−082
011
21
060
059
006
21
060
052
006
31
060
060
006
31
060
057
006
52
070
082
009
52
070
076
008
62
070
085
010
62
070
077
008
83
060
067
008
83
060
064
008
93
060
059
007
93
060
051
008
1
050
068
013
1
050
067
012
2
050
058
012
2
050
051
014
3
050
055
013
3
050
065
013
11
100
073
011
11
100
108
015
12
050
043
007
12
050
059
009
22
100
094
015
22
100
091
015

070
075
012

070
080
014
These plots indicate that the algorithm converged in less than 5000 iterations.
To be conservative, we discard the first 20 000 burn-in iterations, and collect
20 000 further observations for obtaining the Bayesian results. The Bayesian
estimates and their standard error estimates are presented in Table 13.4. It
seems that the Bayesian estimates are acceptable, and the standard error esti-
mates are reasonable. For the first data set, the DIC value corresponding to
the model is 12 604.5. Plots of estimated residuals ˆi versus case numbers ˆi1
and ˆi2 in the analyses of this data set are presented in Figure 13.6. For the
second set with nonignorable missing data, WinBUGS does not give the DIC
value. To assess the goodness-of-fit roughly, we examine the estimated residual
plots of ˆi versus the case numbers ˆi1 and ˆi2. These plots are presented in
Figure 13.7. From these estimated residual plots, we see that the measure-
ment and structural equations fit the data reasonably well. The WinBUGS
codes and data are given in the following website: http://www.wiley.com/go/
lee_structural.

13.6
BAYESIAN ANALYSIS USING WINBUGS
415
(a)
0
100
200
300
400
500
hat{delta}_i
–2 –1
0
2
1
(b)
–2
–1
0
hat{delta}_i
–2 –1
0
2
1
1
(c)
–2
–1
0
hat{delta}_i
–2 –1
0
2
1
2
1
Figure
13.6
Plots of estimated residuals ˆi versus (a) case numbers, (b) ˆi1 and (c)
ˆi2 in the analysis of the first data set.
(b)
(a)
0
100
200
300
400
500
hat{delta}_i
–2 –1
0
2
1
–2
–1
0
hat{delta}_i
–2 –1
0
2
1
1
2
(c)
–2
–1
0
hat{delta}_i
–2 –1
0
2
1
2
1
Figure
13.7
Plots of estimated residuals ˆi versus (a) case numbers, (b) ˆi1 and (c)
ˆi2 in the analysis of the second data set.

416
13
STRUCTURAL EQUATION MODELS
13.7
DISCUSSION
Based on the Bayesian framework presented in this chapter, general mixed non-
normal and ordered categorical data that are common in practical applications
can be analyzed through a nonlinear SEM with fixed covariates and EFDs. After
the accommodation of missing data with a nonignorable missing mechanism, a
comprehensive model is provided for analyzing complex continuous and discrete
data in substantive research. We discuss the application of the Gibbs sampler
and the MH algorithm for obtaining the Bayesian estimates and their standard
error estimates. For model comparison, we present a path sampling procedure
for computing the Bayes factor, which can be used to compare different SEMs
or different models for the nonignorable missing mechanism. The freely avail-
able software WinBUGS is able to produce the Bayesian solution for most
special cases of the framework. However, for some complex situations, for
example the model considered in Section 13.6 with nonignorable missing data,
WinBUGS does not provide the DIC value. However, this software is able to
produce estimated residual plots which can be used to assess the goodness-of-fit
roughly.
As the binomial distribution belonged to EFDs, unordered binary data can be
analyzed through the methodology presented here. We mentioned in Chapter 7
that it is incorrect to treat unordered binary variables as ordered binary variables.
Here, we point out that it is also incorrect to treat ordered binary variables as
unordered binary variables. The methodologies presented in Chapter 7 should
be used to analyze ordered binary data.
The proposed model framework and the key formulae in the Bayesian
methods have a high potential for applications to the following useful models.
Consider the latent variable model developed in Sammel, Ryan and Legler
(1997) that concentrated on the mean structure of the manifest variables. An
equivalent representation is defined as: for i = 1   n, yi and i satisfy the
expressions given in Equation (13.1), the vector of canonical parameters i is
related to i and Xi by the model defined by Equation (13.2), and i is related
to fixed covariates ci by the linear model i = Bci +i. Comparing this linear
model with Equation (13.3), we see that it is a special case of our nonlinear struc-
tural equation (with i = i, 	 = 0 and 
 = 0), and relationships among the
latent variables in i are not assessed. The Bayesian framework in this chapter
provides methods to analyze these relationships. The binomial probit model
is defined by yi = XiA + i, where yi is a vector of binomial variables, Xi are
covariates and i is a vector of error measurement with distribution N0 .
This model, which is a special case of the model presented in this chapter, has
wide applications in medicine, for example to gene-variable selection (Lee et al.,
2003; Zhou, Wang and Dougherty, 2003). Moreover, the multivariate probit
model (Bock and Gibbons, 1996) as discussed in Chapter 7 is also a special
case of the presented model. Finally, to accommodate a pi ×q matrix of fixed
covariates, C∗
i = c∗T
i1    c∗T
ipi T , which links to i, we can allow the dimension

APPENDIX 13.1: IMPLEMENTATION OF THE MH ALGORITHMS
417
of yi to be pi, which varies with i, and modify the ‘measurement’ model of
Equation (13.2) to
ik = xT
ikek +c∗
iki
k = 1   pi
(13.25)
The model defined by Equations (13.1) and (13.25) is a generalized linear
mixed model (GLMM). Hence, the nonlinear SEM that is defined by Equa-
tions (13.1), (13.25) and (13.3) generalizes the GLMM to a model with a more
general covariance structure. Bayesian methods for analyzing this model can be
established by essentially the same procedure as that proposed in Section 13.3.
For example, the full conditional distributions can be obtained from those
presented in Section 13.3.1, with T
k be replaced by c∗
ik.
The Bayesian framework with EFDs presented in this chapter is a generaliza-
tion of two types of method. First, it generalizes SEMs, which can be regarded
as well-known methods in social–psychological research, to model with vari-
ables in EFDs and subtle mean structure with covariates. Secondly, it generalizes
some widely used latent variable models in biostatistics, such as the multivariate
probit model, and possibly GLMMs, to models with subtle covariance struc-
tures.
APPENDIX 13.1: IMPLEMENTATION OF THE MH
ALGORITHMS
In simulating observations from piyiri, we choose N·2
 as the
proposal distribution, where −1
 =  + +T  , in which
 =

	T
0  −1
 	0
−	T
0  −1
 

−T 
T  −1
 	0
−1 +T 
T  −1
 



 =
p exp0 +T
y yi
1+exp0 +T
y yi2 T

with
	0 = I −	,
 =  Hi/ T
i i=0,
y = 1   pT ,
 =
p+1   p+qT and   = diag¨bi1/1    ¨bip/pi=0.
In simulating observations from the conditional distributions pymisi
iriyobsi pakYkk, pkYakk, pkYakk
and pYr, the proposal distributions are N·2
y !ymi, N·2
aak,
N·2
k, N·2
k and N·2
, respectively, where
−1
ymi =

krik=1
 2ckyikk
 ymisi ymisi

yik=0
+
p exp0 +
ℓ ¯D ℓyiℓ+T
i
1+exp0 +
ℓ ¯D ℓyiℓ+T
i2 mT
m

418
13
STRUCTURAL EQUATION MODELS
where m is a vector that contains the elements of y corresponding to ymisi,
¯D is the set of indexes corresponding to y0i.
−1
ak =
n
i=1
¨bikxikxT
ik/k

ak=0
+H−1
0k 
!−1
k =1−n/2−
0k −2
n
i=1
yikik −bik−¨ckyikk

k=1
+20k
−1
k =
n
i=1
¨bikiT
i
k=0
+−1
k H−1
0yk
and
−1
 = p
4
n
i=1
didT
i +H−1
0 
To improve efficiency, we respectively use Nymiymi, N akak,
Nkk, N k k and N  as initial proposal distributions in
the first few iterations, where
ymi =

krik=1
 yikik/k +ckyikk
 ymisi

yik=0
+
 p
k=1
rik −p exp0 +
ℓ ¯D ℓyiℓ+T
i
1+exp0 +
ℓ ¯D ℓyiℓ+T
i

m
ak =
n
i=1

yik −˙bikak=0
 xik
k
+H−1
0k a0k
k =1−n/2−
0k −
n
i=1
yikik −bik+ ˙ckyikk

k=1
+0k
k =
n
i=1

yik −˙bikk=0
 i
k
+H−1
0yk0k
and
 =
n
i=1
 p
k=1
rik −p
2

di +H−1
0 0
A multivariate version of the MH algorithm is used to simulate obser-
vations from pky∗
k zk (see Equation (13.16)). Following Cowles
(1996), for the joint proposal distribution of k and y∗
k given zk,  and
 can be constructed according to the factorization pky∗
kzk =
pkzkpy∗
kkzk. At the jth iteration, we generate a candidate
vector of thresholds 
k1   
kbk−1 from the following univariate truncated
normal distribution

km ∼N
j
km2

kI
km−1
j
km+1
km
for
m = 1   bk −1

REFERENCES
419
where 
j
km is the current value of 
km and 2

k is chosen to obtain an average
acceptance rate of approximately 0.25 or greater. The acceptance probability for
a candidate vector ky∗
k as a new observation 
j+1
k
y∗j+1
k
 is min1Rk,
where
Rk =
pky∗
kzkp
j
k y∗j
k ky∗
kzk
p
j
k y∗j
k zkpky∗
k
j
k y∗j
k zk

For an accepted k, a new y∗
k is simulated from the following univariate trun-
cated distribution:
pyikkziki
D= expyikik −bik/k +ckyikkI
kzik 
kzik+1 yik
where yik and zik are the ith components of y∗
k and zk, respectively, and IAy is
an indicator function which takes 1 if y is in A and zero otherwise.
APPENDIX 13.2
y1: Did you have unused drugs remaining? (No, ‘0’/Yes, ‘1’)
y2: Did you stop, decrease or increase the dosage? (No, ‘0’/Yes, ‘1’)
y3: Did you forget to take drugs? (No, ‘0’/Yes, ‘1’)
y4: Do you know that you have hypertension? (No, ‘0’/Yes, ‘1’)
y5: Do you know the reason for taking drugs? (No, ‘0’/Yes, ‘1’)
y6: Do you know the reason for taking drugs for a long time? (No, ‘0’/Yes, ‘1’)
y7: Your physician consults your opinion in deciding the medication method.
(Strongly agree, ‘1’/agree, ‘2’/no opinion, ‘3’/disagree, ‘4’/strongly
disagree, ‘5’)
y8: You are free to discuss your important opinion with your physician. (Strongly
agree, ‘1’/agree, ‘2’/no opinion, ‘3’/disagree, ‘4’/strongly disagree, ‘5’)
y9: Your physician listens carefully to you. (Strongly agree, ‘1’/agree, ‘2’/no
opinion, ‘3’/disagree, ‘4’/strongly disagree, ‘5’)
REFERENCES
Bock, R. D. and Gibbons, R. D. (1996) High dimensional multivariate probit analysis.
Biometrics, 52, 1183–193.
Booth, J. G. and Hobert, J. P. (1999) Maximum generalized linear mixed model likeli-
hoods with an automated Monte Carlo EM algorithm. Journal of the Royal Statistical
Society, Series B, 6, 265–285.
Browne, M. W. (1987) Robustness of statistical inference in factor analysis and related
models. Biometrika, 74, 375–384.
Chan, G. M. C. (2001) The Effects of Treatment Compliance on Clinical Outcomes in
Patients with Chronic Diseases. Ph.D. Thesis, Department of Medicine and Thera-
peutics, the Chinese University of Hong Kong.

420
13
STRUCTURAL EQUATION MODELS
Cowles, M. K. (1996) Accelerating Markov chain Monte Carlo convergence for
cumulative-link generalized linear models. Statistics and Computing, 6, 101–111.
Gelman, A. and Rubin, D. B. (1992) Inference from iterative simulation using multiple
sequences. Statistical Science, 7, 457–472.
Gelman, A., Meng, X. L. and Stern, H. (1996) Posterior predictive assessment of model
fitness via realized discrepancies. Statistica Sinica, 6, 733–807.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distributions and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Hastings, W. K. (1970) Monte Carlo sampling methods using Markov chains and their
application. Biometrika, 57, 97–100.
Ibrahim, J. G., Chen, M. H. and Lipsitz, S. R. (2001) Missing responses in generalized
linear mixed models when the missing data mechanism is nonignorable. Biometrics,
88, 551–564.
Kano, Y., Berkane, M. and Bentler, P. M. (1993) Statistical inference based on pseudo-
maximum likelihood estimators in elliptical populations. Journal of the American
Statistical Association, 88, 135–143.
Kass, R. E. and Raftery, A. E. (1995) Bayes factors. Journal of the American Statistical
Association, 90, 773–795.
Lee, S. Y. and Song, X. Y. (2004) Maximum likelihood analysis of a general latent
variable model with hierarchically mixed data. Biometrics, 60, 624–636.
Lee, K. E. et al. (2003) Gene selection: a Bayesian variable selection approach. Bioin-
formatics, 19, 90–97.
McCullagh, P. and Nelder, J. A. (1989) Generalized Linear Models (2nd edn.). London:
Chapman and Hall.
Metropolis, N. et al. (1953) Equations of state calculations by fast computing machine.
Journal of Chemical Physics, 21, 1087–1091.
Sammel, M. D. and Ryan, L. M. (1996) Latent variables with fixed effects. Biometrics,
52, 220–43.
Sammel, M. D., Ryan, L. M. and Legler, J. M. (1997) Latent variable models for mixed
discrete and continuous outcomes. Journal of the Royal Statistical Society, Series B,
59, 667–678.
Schumacker, R. E. and Marcoulides, G. A. (1998) Interaction and Nonlinear Effects in
Structural Equation Models. Mahwah, N. J: Lawrence Erlbaum Associates.
Shapiro, A. and Browne, M. W. (1987) Analysis of covariance structures under elliptical
distributions. Journal of American Statistical Association, 82, 1092–1097.
Song, X. Y. and Lee, S. Y. (2006) Bayesian analysis of latent variable models with nonignor-
able missing outcomes from exponential family. Statistics in Medicine, in press.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. (2003). WinBUGS User
Manual, Version 1.4. Cambridge, England: MRC Biostatistics Unit.
Wedel, M. and Kamakura, W. (2001) Factor analysis with (mixed) observed and latent
variables in the exponential family. Psychometrika, 55, 515–530.
Yuan, K. H. and Bentler, P. M. (1997) Mean and covariance structures analysis: theoret-
ical and practical improvements. Journal of the American Statistical Association, 92,
767–774.
Zhou, X. B., Wang, X. D. and Dougherty, E. R. (2003) Missing-value estimation using
linear and non-linear regression with Bayesian gene selection. Bioinformatics, 19,
2302–2307.

14
Conclusion
In this book, we have introduced the Bayesian approach coupled with the
data augmentation algorithm and MCMC methods to analyze SEMs. We have
discussed the basic factor analysis model, the LISREL type models, as well
as more complex models such as models with ordered categorical variables,
and/or dichotomous variables, nonlinear models, multilevel models, multi-
sample models, mixture models, models with missing data, models with variables
from the exponential family of distributions, and some of their special cases and
combinations. We have concentrated on obtaining the Bayesian estimates of
the unknown parameters and their standard error estimates, and computing the
Bayes factor for model comparison. Moreover, we have shown that estimates
of latent variables and their standard error estimates can be produced as by-
products, and we have also demonstrated the use of the latent variable estimates
to conduct residual analysis through the estimated residual plots. The Bayesian
methodologies in analyzing the above complex SEMs have been demonstrated
by real examples from behavioral, educational, medical, social and psycholog-
ical research. Almost all of these examples cannot be effectively handled by the
existing commercial software in SEM.
As we have mentioned and shown throughout this book, the Bayesian
approach has the following advantages. (i) It can directly incorporate genuine
prior knowledge in the analysis for obtaining better results. (ii) Except possibly
for dichotomous or binary variables, the associated sampling-based Bayesian
methods give reliable estimates and standard error estimates even with small
sample sizes. (iii) Compared with the classical methods, it gives better esti-
mates of the latent variables. (iv) Samples that are simulated from the posterior
distributions of parameters and latent variables are not only useful in Bayesian
estimation and model comparison, but also in residual and outlier analyses.
Moreover, statistical analyses such as construction of confidence intervals based
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

422
14
CONCLUSION
on these samples do not rely on asymptotic theory. (v) The Bayes factor or DIC
that is closely related to a Bayesian approach gives a more flexible statistic for
model comparison/selection than the classical likelihood ratio test in the ML
approach (see Kass and Raftery, 1995).
So far, even in the context of standard SEMs with normal data, latent variable
estimates have not been used much in practice. One of the reasons may be
due to the problems of the Bartlett’s method or the regression type method
(Lawley and Maxwell, 1971). For example, the estimates produced by the above
methods depend on the parameter estimates without taking into account the
sampling errors, and the sampling distribution of the estimates is complicated.
As the latent variable estimates produced by the Bayesian approach do not have
these problems, we hope to see more applications of them to real research in the
future. Due to the nature of the data, latent variable estimates obtained from
dichotomous data or binary data may be inaccurate. These estimates should be
treated with great caution.
The Bayesian methodologies for the aforementioned complex SEMs are
developed through a commonly used strategy in the literature of statistics. That
is, we first utilize the idea of data augmentation (Tanner and Wong, 1987) to
augment the observed data with the latent quantities which cause the difficulties
in the analysis. The latent quantities may be the latent variables in the model,
the unobservable continuous measurements that underly the dichotomous or
the ordered categorical variables, the allocation variables in the mixture models,
and/or missing data. It has been shown that after data augmentation, the poste-
rior analysis based on the complete data set can be handled. Usually, a sufficiently
large number of observations are simulated from the joint posterior distribution
of the parameters and the augmented latent quantities to obtain the Bayesian
solutions. This task is done by applying tools in statistical computing, namely
the Gibbs sampler and the MH algorithm in estimation, and path sampling in
computing the Bayes factor for model comparison. These tools are well-known
in statistics, and conceptually simple, although the derivations of the full condi-
tional distributions and the implementations of the algorithms in the context
of various complex SEMs are not straightforward. Recently, we have demon-
strated that this useful strategy is also effective in developing Bayesian robust
methods (Lee and Xia, 2006b). Based on our experience with this strategy, we
strongly recommend it to readers for handling complicated problems.
Dichotomous, ordered categorical and/or missing data are often encoun-
tered in behavioral, educational, medical and social sciences; it is also well-
recognized that two-level models should be used to cope with the hierarchical
data, mixture models should be used to handle heterogeneity, and nonlinear
models are necessary to analyze the important interaction or quadratic effects of
the exogenous latent variables to the endogenous latent variables. In fact, the
developments of the rigorous methods through the Bayesian approach are moti-
vated by the need of these techniques for conducting correct statistical infer-
ences under the real complex situations. Unfortunately, applications of these

CONCLUSION
423
new techniques to substantive research are rather limited. In our opinion the
main reason for this phenonemon is that the common software in SEMs cannot
provide satisfactory solutions for the complex models or data structures, and
applied researchers encounter difficulty in implementing the technically involved
computer programs. Thanks to the development of WinBUGS (Speighalter
Thomas, Best and Lunn, 2003), this difficulty could be overcome. As we have
shown, most of the complex SEMs discussed in this book can be analyzed
through WinBUGS. Recently, Sturtz, Ligges and Gelman (2005) developed an
R2WinBUGS package, which provides convenient functions to call WinBUGS
from R (R Development Core Team, 2004) for further analyses. The Bayes
factor and the PP p-values could be obtained by saving the output in WinBUGS
and reading it into R. Given the advantages of the Bayesian approach, and the
feasibility of WinBUGS in analyzing complicated models and data, we strongly
recommend this general software to applied researchers. We think that it is
worthwhile to give time and effort to know more about this useful software,
despite its slower convergence than our tailor-made programs.
The developments of Bayesian statistical methods as well as Bayesian
computing tools has been very rapid in recent years. In the process of writing
this book, we realized that some of our content could be improved by these
recent developments. With regard to the Bayesian methods, the partial poste-
rior predictive (PPP) p-value (Bayarri and Berger, 2000) is an improvement
over the PP p-value (Gelman, Meng and Stern, 1996) for overcoming the
problem of ‘double use’ of the data. In the context of SEMs with nonignor-
able missing data, Lee and Tang (2006) proposed an algorithm for computing
the PPP p-value, and showed that the additional computation is not difficult.
Moreover, it may be desirable to present the highest posterior density (HPD)
interval (see Chen, Shao and Ibrahim, 2000) of a parameter to reveal the vari-
ability of its estimate, as was done in Lee and Song (2003) and Song and Lee
(2004). Computationally, the efficiency of the proposed data augmentation
scheme and the MCMC methods may be improved by the more recent devel-
opments in statistical computing, such as Meng and van Dyk (1999), van Dyk
and Meng (2001) and Gelman (2004). We may incorporate these and other
possible improvements in a revised edition.
There are a few common assumptions in formulating the SEMs in this book.
First, we usually assume that   is diagonal. This assumption can be partially
relaxed by extending   to be a diagonal block matrix of the form
  =
⎡
⎢⎣
 1
0

0
 K
⎤
⎥⎦
where  k is an unknown positive definite matrix. Based on the conjugate prior
distribution of  k, which is a similar Wishart distribution, the corresponding

424
14
CONCLUSION
conditional distribution p k· required by the Gibbs sampler can be derived.
For dichotomous or ordered categorical data, the conditional distribution of
latent continuous variables would be related to a multivariate truncated normal
distribution rather than a univariate one, and simulating observations is more
complicated. The idea given in Chapter 7, Section 7.3, can be used but details
have to be worked out. The extension of   to an arbitrary patterned matrix is
nontrivial mainly because it may not be possible to use the Wishart distribution
as its prior distribution, and the full conditional distribution of the whole
parameter matrix  is more complicated.
Secondly, we usually assume that the matrix of coefficients  in the structural
equation satisfies the condition that I − is independent of the elements in
. Let the conjugate prior distribution be NoHo	
, it follows from Lee
and Zhu (2000) that the required conditional distribution of  in the Gibbs
sampler is proportional to
exp

n log I−−1
2−oT H−1
o	 −o
−1
2
n	
i=1
i −i −iT  −1
 i −i −i


If I − is independent of elements in , this conditional distribution is
essentially a normal distribution. If I − depends on , this conditional
distribution is not normal. However, an MH type of algorithm can be applied
to simulate the required observations.
Thirdly, we assume the distributions of ii and i are multivariate normal
distributions; although for certain SEMs, we allow that the distribution of the
manifest variables is non-normal. An interesting topic for further research is
about the robustness of the Bayesian methods against this assumption. For
dichotomous and ordered categorical data with a threshold specification, we
expect the problem due to violation of this assumption may not be serious.
However, as latent variables play the most important role in both the measure-
ment equation and the structural equation, it is desirable to develop semipara-
metric Bayesian methods to relax the normality assumption of i.
Inspired by the pioneer work of Cook (1977, 1986) on case-deletion and
local influence measures, as well as their wide applications to various statistical
models, it is interesting to identify the influential observations in relation to
the Bayesian methods. In view of this, a further research topic is to develop
Bayesian case-deletion measures and/or Bayesian local influence measures.
Moreover, it is also desirable to investigate the impact of outliers on the Bayesian
methods. Recently, in the context of an ML approach and a nonlinear SEM
with missing data, Lee and Xia (2006a) proposed a robust method to handle
outliers, by incorporating stochastic weights in the covariance matrices of ii
and i. The effect of the stochastic weights is to reduce the influence of

REFERENCES
425
the outliers. Theoretically, the idea of stochastic weights can be applied to the
Bayesian approach. However, many details have to be worked out.
Unordered categorical variables with a multinomial distribution are involved
in much important substantive research. For instance, unordered categorical
observations are associated with genotype variables in genetic studies, and the
selection model in economics. Development of SEMs with this kind of discrete
data has great practical value. Recently, analysis of longitudinal data has received
a great deal of attention. With some appropriate specifications, the factor anal-
ysis model has been applied to analyze latent curve models (see, for example,
Blozis (2004)). An interesting research topic would be to develop more subtle
models and Bayesian methods for analyzing multivariate longitudinal data. More
generally, it is useful to establish dynamic SEMs for analyzing time-series data,
or functional data (see Ramsay and Silverman, 1997).
REFERENCES
Bayarri M. J. and Berger J. O. (2000) P values for composite null models. Journal of
American Statistical Association, 95, 1127–1142.
Blozis, S. A. (2004) Structured latent curve models for the study of change in multivariate
repeated measures. Psychologial Methods, 9, 334–353.
Chen, M. H., Shao, Q. M. and Ibrahim, J. G. (2000) Monte Carlo Methods in Bayesian
Computation. New York: Springer-Verlag.
Cook, R. D. (1977) Detection of influential observations in linear regression. Techno-
metrics, 19, 15–18.
Cook, R. D. (1986) Assessment of local influence (with discussion). Journal of the Royal
Statistical Society, Series B, 48, 133–169.
Gelman, A. (2004) Parameterization and Bayesian modeling. Journal of American Statis-
tical Association, 99, 537–545.
Gelman, A., Meng, X. L. and Stern, H. (1996) Posterior predictive assessment of model
fitness via realized discrepancies. Statistica Sinica, 6, 733–807.
Kass, R. E. and Raftery, A. E. (1995) Bayes factors. Journal of the American Statistical
Association, 90, 773–795.
Lawley, D. N. and Maxwell, A. E. (1971) Factor Analysis as a Statistical Method (2nd
edn). New York: American Elsevier.
Lee, S. Y. and Song, X. Y. (2003) Model comparison of nonlinear structural equation
models with fixed covariates. Psychometrika, 68, 27–47.
Lee, S. Y. and Tang, N. S. (2006) Bayesian analysis of nonlinear structural equation
models with nonignorable missing data. Psychometrika, in press.
Lee, S. Y. and Xia, Y. M. (2006a) Maximum likelihood methods in treating outliers and
symmetrically heavy-tailed distributions for nonlinear structural equation models with
missing data. Psychometrika, in press.
Lee, S. Y. and Xia, Y. M. (2006b) A Bayesian robust approach for latent variable models
with ignorable missing data. Submitted manuscript.
Lee, S. Y. and Zhu, H. T. (2000) Statistical analysis of nonlinear structural equation
models with continuous and polyfomous data. British Journal of Mathematical and
Statistical Psychology, 53, 209–232.

426
14
CONCLUSION
Meng, X. L. and van Dyk, D. A. (1999) Seeking efficient data augmentation scheme via
conditional and marginal augmentation. Biometrika, 86, 301–320.
R Development Core Team (2004) R: A Language and Environment for Statistical
Computing. Vienna, Austria: R Foundation for Statistical Computing.
Ramsay, J. O. and Silverman, B. W. (1997) Functional Data Analysis. New York:
Springer-Verlag.
Song, X. Y. and Lee, S. Y. (2004) Bayesian analysis of two-level nonlinear structural equa-
tion models with continuous and polytomous data. British Journal of Mathematical
and Statistical Psychology, 57, 29–52.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. (2003) WinBugs User
Manual. Version 1.4. Cambridge, England: MRC Biostatistics Unit.
Sturtz, S., Ligges, U. and Gelman, A. (2005) R2WinBUGS: a package for running
WinBUGS from R. Journal of Statistical Software, 12, 1–17.
Tanner, M. A. and Wong, W. H. (1987) The calculation of posterior distributions by
data augmentation(with discussion). Journal of the American Statistical Association,
82, 528–550.
Van Dyk, D. A. and Meng, X. L. (2001) The art of data augmentation (with discussion).
Journal of Computational and Graphical Statistics, 10, 1–111.

Index
Akaike information criterion (AIC),
127–128
Algorithm
Fletcher–Powel, 32, 47–48, 50
Gauss–Newton, 32, 47, 49–50
Metropolis–Hastings (MH), 5, 78, 98,
104, 141, 176, 196, 244, 319,
358, 362, 401
Newton–Raphson, 47–49, 248
scoring, 17, 47, 49–50, 320
Asymptotic distribution, 32, 39
Asymptotically distribution free method
(ADF), 33, 43–46, 68, 70, 89, 196
Average acceptance rate, 150, 203, 289,
389, 415
Bayes factor, 18, 69, 89, 112–121, 128,
142, 155, 158–160, 162, 181, 189,
196–197, 223, 230, 231–235, 256,
259, 262, 269, 272, 294, 300–302,
344, 347–349, 350, 356, 362, 364,
372, 374
Bayesian approach
method, 5, 21, 28, 81, 141, 164, 165,
176, 196, 203, 207, 210, 214,
244, 294, 308, 319, 323,
356–357, 361, 394, 398, 404,
416–417, 421–425
Bayesian classification, 323, 329
Bayesian information criterion (BIC),
127, 320
Between-groups model, 244, 246–247,
250, 258, 265, 267, 270, 274, 290
Bridge sampling, 117
Categorical variable, 5, 6, 15, 69, 139,
215, 221, 285
Chain rule in matrix calculus, 56
Chi-square
distribution, 136, 153, 248
variable, 136, 153, 248
Communality, 16
Complete-data likelihood, 123, 156–157,
160, 265, 269, 300
Conditional distribution, 70, 93–97, 101,
121, 133, 134–135, 144, 148, 179,
188, 200–203, 216, 249, 281, 287,
297, 314, 362
Confirmatory factor analysis (CFA), 2,
14, 15, 31, 177, 186–188, 260, 320
Conjugate prior distribution, 71–73, 87,
121, 206, 288, 316, 424, 428
Consistent estimator, 41, 45
Constraints
equality, 73, 116, 131
identification, 16–17, 143, 144, 154,
168, 172, 179, 198, 215, 246,
296, 352, 403
Structural Equation Modeling: A Bayesian Approach
S-Y. Lee
© 2007 John Wiley & Sons, Ltd

428
INDEX
Continuous variables, 141, 218, 246,
260, 357, 366, 390, 424
Convergence in distribution, 37, 45, 58
Convergence in probability, 39, 58
Convergence of the algorithm, 151, 251,
269, 406
Correlation
polychoric, 141
polyserial, 141
Covariance matrix, 2, 3, 5, 18–23,
27–28, 31–32, 36–37, 41–43, 46,
48, 67–68, 82, 86, 90, 95–96, 105,
120, 122, 129, 144, 146, 149,
180–181, 187–188, 196, 198, 204,
210, 223, 227, 250, 268, 283, 294,
322, 325, 336, 348, 356, 370, 393,
395–396, 398, 406
Covariance structure
analysis, 2, 31
approach, 13
Covariate, 120, 122, 124, 133, 220,
225–226, 232, 326, 405–406
Cross-level effects, 244, 267, 274,
277
Data
binary, 393, 411, 416, 422
dichotomous, 175–176, 179, 181,
184, 191, 226, 314, 357, 414
exponential family, 4, 5, 15, 69, 191,
393, 394–396, 421
hierarchical, 3, 4, 422
missing, 3–5, 15, 68–69, 77, 81–82,
129, 141, 153, 163, 176, 179,
182, 202, 248, 340, 355–390,
394–400, 403, 406, 408, 411,
414, 421–424
multilevel, 3, 15, 69, 357, 421
nonnormal, 416
ordered categorical, 3–5, 140, 145,
147, 165, 184, 197, 215–220,
221, 239, 244, 248–251, 275,
297, 356, 357, 359, 361, 364,
376, 400–402, 416, 424–425
Data augmentation, 5, 14, 69, 77, 81,
116, 141, 179, 202, 216, 244, 248,
268, 288, 297, 345, 356, 359, 361,
376, 379, 394, 398, 401, 421–423
Data-dependent prior, 90, 183,
223, 261
Derivatives
matrix, 53
partial, 36, 48–49
Deviance information criterion (DIC), 6,
128
Dichotomous, 175
data, 175–176, 179, 181, 184, 191,
226, 314, 357, 414
variable, 175, 222, 247, 402
Differentiable function, 55, 396
Distributions
binomial, 191, 402, 405, 416
chi-square, 153
conditional, 43, 47, 78, 85–88, 96,
107, 121, 161, 180, 189, 216,
249, 269, 283, 298, 313, 320,
324, 327, 362, 371, 377–378,
398–400, 401, 417, 421
conjugate prior, 71–78, 85–86, 98,
121, 154, 157, 180, 200, 237,
250, 288, 309, 366, 374, 383,
398, 400, 403, 406, 411, 424
elliptical, 46
exponential family (EFD), 4, 5, 15, 69,
78, 151, 171, 191, 394–396, 421
Gamma, 76, 210
inverted Gamma, 75–76, 210
inverted Wishart, 150, 327, 662
multivariate normal, 44
posterior, 5, 68–73, 77–81, 85–86, 98,
107, 121, 130, 136, 141, 142,
145, 151, 153, 156, 158, 161,
179, 199, 248–250, 265, 269,
297, 300, 323–324, 327–329,
345, 357, 361, 369, 370, 372,
377, 379, 398, 402, 406, 421
prior, 6, 68–76, 83–87, 96–99, 122,
130, 133–135, 147–149, 157,
161–163, 167, 169, 180, 189,
199, 200, 203, 206, 216–219,
223, 237, 249–251, 256, 261,
271, 281, 282, 286, 297–299,
303, 309, 315–316, 325–327,
331, 336, 347–350, 362, 368,
371, 374, 377, 380, 383, 387,
400–401, 406, 411, 423–424

INDEX
429
uniform, 365
Wishart, 16, 41, 75, 78, 84, 97, 148,
150, 201, 210, 216, 283, 321,
327, 362, 398, 423–424
Endogenous latent variable, 3, 24, 120,
125, 154, 204, 220, 223, 246, 267,
268, 271, 294, 302, 322, 386, 396,
405, 422
Errors of measurement, 1, 4, 90, 120,
205
Estimated potential scale reduction
(EPSR), 79, 87, 106, 125, 145, 151,
179, 184, 223, 251, 262, 269, 271,
305, 342, 361, 369, 406, 408
Estimates
Bayesian, 71, 76, 81, 95, 99, 122, 142,
144, 156, 177, 183, 186, 196,
203, 206, 214, 244, 251,
253–254, 261, 297, 321, 323,
332, 337, 344, 356, 359, 361,
374, 406, 421
generalized least squares (GLS), 32–33,
38, 39–40, 43, 46, 50, 52, 67
maximum likelihood (ML), 2, 6
Exogenous latent variable, 112, 120, 154,
165, 167, 196, 197, 205, 220, 223,
225, 246, 268, 270–271, 295, 302,
304, 405–406, 422
Exploratory factor analysis (EFA), 15–16,
21, 24, 27, 32, 77, 142, 164
Exponential family distribution (EFD), 4,
5, 15, 69, 339, 393–421
Factor analysis model
confirmatory (CFA), 2, 14–15, 18–19,
24, 31, 177, 187–188, 260, 320
exploratory (EFA), 13–15, 24, 51,
159, 176
higher-order, 14, 22
second-order, 15, 22
Factor loading, 14, 16, 22, 25, 94, 142,
159, 163, 167, 177, 182, 225, 270,
298, 302, 304, 307, 308, 321, 380,
385, 405
Factor score, 5, 21
Finite mixtures in SEMs, 112
Fixed covariates, 396
Generalized least squares (GLS), 28, 32
Gibbs sampler, 5, 78–79, 81–82, 84, 86–
87, 96, 98, 115, 119, 121, 125, 129,
141–142, 145, 150, 151, 154, 161,
176, 179–181, 188, 199, 202, 206,
216, 219, 223, 244, 251, 258, 269,
279, 313, 324, 325, 328–329, 336,
343–345, 350, 358, 361–362, 370,
374, 380, 398, 401, 406, 422, 424
Goodness-of-fit, 14
Heterogeneity, 243, 319, 422
Hierarchical structure, 243, 246, 293
Higher-order factor analysis, 15, 22
Histogram, 139–140
Hyperparameters, 261, 348, 411
Hypothesis testing, 40, 80, 95, 111–112,
115, 168, 191, 196, 213, 304
Identifiability constraints, 321–322, 324,
330, 332, 350
Identification condition, 17
Identification constraints, 144, 168, 178,
296
Ignorable missing data, 353, 356,
375–376, 397, 402
Importance sampling, 112, 116–118, 130
Indicator, 3, 93, 167, 203, 218,
376–377, 396, 419
Indicator function, 136, 157, 181, 281,
314, 419
Informative prior distribution, 71, 76,
131, 149
Interaction, 112, 181, 195, 203, 205,
210–211, 218, 220, 225–226, 246,
251, 261, 268, 274–275, 277, 303,
307, 308, 369, 403
Intercept, 6, 187, 222, 307, 395, 403
Kronecker product of matrices, 33
Kurtosis, 46
Latent allocation variable, 323
Latent factor, 16, 19–20, 82, 89, 93,
269–270, 275
Latent variable model, 271

430
INDEX
Latent variables, 1, 2, 5, 14, 22–29,
68–69, 77, 81–83, 93, 98, 102, 116,
121, 124, 141, 152, 164, 167, 170,
176, 179–183, 195–205, 220–227,
234, 244, 245–251, 259, 267,
268–271, 274–275, 294–296,
302–304, 320–322, 344, 357, 364,
378, 385, 393, 396, 401, 404–406,
416, 421–422, 424
Likelihood function, 17, 20, 47, 70, 72,
119, 157, 159, 179, 214, 232,
345–346, 364
Likelihood ratio, 17, 41, 69, 114, 115,
136, 299, 303, 320, 344, 422
Linear structural equation, 26, 245, 365
Linked model, 11, 19, 123, 130, 157,
167, 235, 300–301, 363, 368, 406
LISREL, 2, 3, 14, 15, 24, 25–27, 32, 45,
47, 48–50, 69, 87, 95, 112, 141,
195, 203–206, 308, 321, 421
Log likelihood function, 17, 20, 232,
346, 372
Logistic regression, 378, 397, 406, 411
Manifest variables, 1, 26
Markov chain Monte Carlo (MCMC)
methods, 5, 68–69, 78, 81, 89, 112,
113, 119, 129, 130, 151, 155, 157,
159, 160, 169, 177, 188, 245, 249,
263, 272, 321, 357, 358, 360, 380,
395, 399, 422, 424
Matrix
calculus, 38, 42, 49, 51, 55–56
correlation, 20, 24, 184, 187–188
covariance, 2–3, 14, 16, 18–20, 22–28,
32, 36, 40, 43, 67–68, 82, 86, 90,
152, 176, 188, 196, 202–204,
210, 223, 227, 268, 283, 303,
320, 322, 335, 362, 370, 393,
396, 406, 424
Hessian, 38–39, 42, 47–49
idempotent, 35, 62–63
information, 42, 49
positive definite, 33, 38, 40, 41,
47–48, 75, 97, 134, 145,
187–188, 200, 282–283, 423
Maximum likelihood (ML), 2, 32
Measurement equation, 6, 74, 76, 96,
99, 101, 120, 123, 129, 131, 133,
142, 154, 158, 164, 167, 176, 196,
199, 203, 215, 220, 222, 229, 234,
238, 244, 267, 271, 286, 294, 303,
309, 321–322, 360, 365, 369, 378,
382, 406, 424
Metropolis–Hastings (MH) algorithm,
78, 104, 150, 344, 362, 401
Missing data
ignorable (missing at random, MAR),
356, 375, 397, 402
nonignorable, 3–5, 129, 356–357,
375–377, 382, 383, 385–387,
394, 396–397, 408, 411, 416,
420, 423
Mixture of structural equation model,
320, 322–324, 330, 336, 344, 357,
370, 372
Model
Bentler and Weeks, 14, 15, 26, 27
LISREL, 22–26
mixture of SEMs, 323
multisample nonlinear SEMs, 297–302
multivariate probit, 186–190
nonlinear SEMs (NSEMs), 215, 267,
275, 297, 375
two-level SEMs, 5, 244, 247, 267,
268, 276, 293, 294, 297, 300
with dichotomous variables, 175
with exponential family variables, 393,
394–398
with missing data, 355, 357–359, 370,
386–389
with ordered categorical variables,
221–222
Model comparison (selection), 111, 155,
230, 238, 255, 299, 344, 401
Multisample analysis, 293
Multitrait–Multimethod, 19
Multivariate normal, 2, 37, 41, 44, 50,
67, 87, 139, 176, 196, 321, 403,
424
Multivariate probit confirmatory factor
analysis (MPCFA) model, 186–190
Nested models, 265, 299
Non-normal data, 43, 394

INDEX
431
Non-normal distribution, 43
Nonignorable missing data, 375
Noninformative prior distribution, 131,
203, 217, 219, 271, 281, 303, 327,
342, 350, 401
Nonlinear covariates and latent variables,
220–230
Nonlinear structural equation, 294
Nonlinear structural equation model
(NSEM), 195, 243
Nonnested models, 299
Normalizing constant, 41, 116, 119
Observed data likelihood, 159, 247, 248,
294, 357
Observed variables, 31, 77, 139, 267, 308
Ordered categorical variable, 139, 215,
221
Outlier analysis, 103, 277
Path diagram, 223, 272
Path sampling, 115–119
Permutation sampler, 321, 322, 323,
324, 330, 331, 336, 342
Posterior analysis, 77–81
Posterior density, 70, 71, 77
Posterior distribution, 69, 70, 73, 77, 89,
96, 145, 179, 323, 324, 345,
357, 421
Posterior mean, 80, 86, 128, 152, 159,
202, 250, 324, 328
Posterior predictive (PP) p-value,
128–129
Posterior simulation, 199, 248–250
Prior distribution
conjugate, 71, 72, 73, 75, 76, 261,
288, 316, 406
informative, 71–76
non-informative, 71–76
Product indicator, 203–207
Product rule in matrix calculus, 33, 38,
42, 49, 51
Proposal distribution, 150, 151, 201,
219, 271
Quadratic form, 38, 62, 72
Quality of life, 164, 308–313
Random variable, 33, 49, 57, 58, 59, 77,
196, 275, 295
Real data set
Accelerated Schools for Quality
Education (ASQE), 269, 276
AIDS, 259, 260, 266, 366, 369
Inter-university Consortium for
Political and Social Research
(ICPSR), 6, 122, 202, 302
patients’ nonadherence, 182
Regression model
logistic regression, 377, 378, 397, 406,
411
Residual analysis plots, 129–130
Robustness, 89, 410, 424
Root mean squares (RMS), 51, 90, 162,
206, 252, 337, 404
Sample covariance matrix, 2, 3, 5, 16, 20,
32, 37, 41, 43, 44, 67, 68, 80, 98,
181, 227, 250, 336, 348, 362,
393, 406
Sample size, 89–91
Sampling error, 21, 81, 185, 422
Sensitivity, 76, 113, 124, 131, 162, 237,
250, 251, 254, 347, 348, 365, 378
Significance test, 111, 112, 114
Simulation, 161–162, 199, 248, 250,
330, 334, 347, 364, 402
Skewness, 80
Standard deviation, 90, 94, 206, 210,
211, 260, 270, 337, 350, 367, 382,
404
Standard error estimates, 6, 32, 40, 45,
69, 80, 81, 86, 90, 95, 98, 102,
141, 159, 163, 168, 181, 206, 207,
211, 219, 223, 225, 227, 265, 272,
308, 310, 328, 337, 343, 350, 358,
362, 385, 386, 398, 406, 408, 410,
414, 416, 421
Standardized variable, 184, 260, 367, 382
Starting values, 47, 49, 51, 79, 87,
145, 154, 183, 219, 251, 262,
269, 271, 332, 336, 342, 350,
361, 374, 383
Structural equation, 1, 13, 67, 139, 175,
195, 243, 293, 294, 319, 355, 393

432
INDEX
Structural equation models (SEMs), 13,
67, 139, 175, 195, 243, 293, 294,
319, 355, 393
Threshold, 140, 143, 154, 161, 167,
171, 191, 246, 249, 295, 296, 402
Two-level nonlinear SEM, 244–247,
267–269, 275–276, 279
Unique labelling, 321, 322, 370
Variables, 175, 215, 220, 221, 244
binary, 175, 393, 402, 405, 416
continuous, 139, 215–220,
359–362
dichotomous, 175–192
latent, 220, 221
multinomial, 397
observed, 19, 31, 49, 77, 139, 188,
267, 308
ordered categorical (ordinal), 139, 215,
221, 285, 359
unobserved, 250, 360, 401
Weight matrix, 40, 45, 49, 50
WinBUGS, 98, 101, 128, 131, 167, 226,
238, 275, 308, 386, 411
Wishart distribution, 16, 41, 75, 78, 84,
94, 148, 150, 201, 210, 217, 327,
362, 398, 423, 424
Within-groups model, 245, 246, 249,
258, 267, 270, 274

WILEY SERIES IN PROBABILITY AND STATISTICS
ESTABLISHED BY WALTER A. SHEWHART AND SAMUEL S. WILKS
Editors
David J. Balding, Peter Bloomfield, Noel A. C. Cressie, Nicholas I. Fisher,
Iain M. Johnstone, J. B. Kadane, Geert Molenberghs, Louise M. Ryan,
David W. Scott, Adrian F. M. Smith, Sanford Weisberg
Editors Emeriti
Vic Barnett, J. Stuart Hunter, David G. Kendall
The Wiley Series in Probability and Statistics is well established and authoritative. It covers
many topics of current research interest in both pure and applied statistics and probability theory.
Written by leading statisticians and institutions, the titles span both state-of-the-art developments
in the field and classical methods.
Reflecting the wide range of current research in statistics, the series encompasses applied, method-
ological and theoretical statistics, ranging from applications and new techniques made possible by
advances in computerized practice to rigorous treatment of theoretical approaches.
This series provides essential and invaluable reading for all statisticians, whether in academia,
industry, government, or research.
ABRAHAM and LEDOLTER Statistical Methods for Forecasting
AGRESTI Analysis of Ordinal Categorical Data
AGRESTI An Introduction to Categorical Data Analysis
AGRESTI Categorical Data Analysis, Second Edition
ALTMAN,GILL,andMcDONALDNumericalIssuesinStatisticalComputingfortheSocialScientist
AMARATUNGA and CABRERA Exploration and Analysis of DNA Microarray and Protein Array
Data
AND ˇEL Mathematics of Chance
ANDERSON An Introduction to Multivariate Statistical Analysis, Third Edition
ANDERSON The Statistical Analysis of Time Series
ANDERSON, AUQUIER, HAUCK, OAKES, VANDAELE, and WEISBERG Statistical Methods
for Comparative Studies
ANDERSON and LOYNES The Teaching of Practical Statistics
ARMITAGE and DAVID (editors) Advances in Biometry
ARNOLD, BALAKRISHNAN, and NAGARAJA Records
ARTHANARI and DODGE Mathematical Programming in Statistics
BAILEY The Elements of Stochastic Processes with Applications to the Natural Sciences
BALAKRISHNAN and KOUTRAS Runs and Scans with Applications
BALAKRISHNAN and NG Precedence-Type Tests and Applications
BARNETT Comparative Statistical Inference, Third Edition
BARNETT Environmental Statistics: Methods & Applications
BARNETT and LEWIS Outliers in Statistical Data, Third Edition
BARTOSZYNSKI and NIEWIADOMSKA-BUGAJ Probability and Statistical Inference
BASILEVSKY Statistical Factor Analysis and Related Methods: Theory and Applications
BASU and RIGDON Statistical Methods for the Reliability of Repairable Systems
BATES and WATTS Nonlinear Regression Analysis and Its Applications
BECHHOFER, SANTNER, and GOLDSMAN Design and Analysis of Experiments for Statistical
Selection, Screening, and Multiple Comparisons
∗Now available in a lower priced paperback edition in the Wiley Classics Library.

BELSLEY Conditioning Diagnostics: Collinearity and Weak Data in Regression
BELSLEY, KUH, and WELSCH Regression Diagnostics: Identifying Influential Data and Sources
of Collinearity
BENDAT and PIERSOL Random Data: Analysis and Measurement Procedures, Third Edition
BERNARDO and SMITH Bayesian Theory
BERRY, CHALONER, and GEWEKE Bayesian Analysis in Statistics and Econometrics: Essays in
Honor of Arnold Zellner
BHAT and MILLER Elements of Applied Stochastic Processes, Third Edition
BHATTACHARYA and JOHNSON Statistical Concepts and Methods
BHATTACHARYA and WAYMIRE Stochastic Processes with Applications
BIEMER, GROVES, LYBERG, MATHIOWETZ, and SUDMAN Measurement Errors in Surveys
BILLINGSLEY Convergence of Probability Measures, Second Edition
BILLINGSLEY Probability and Measure, Third Edition
BIRKES and DODGE Alternative Methods of Regression
BLISCHKE and MURTHY (editors) Case Studies in Reliability and Maintenance
BLISCHKE and MURTHY Reliability: Modeling, Prediction, and Optimization
BLOOMFIELD Fourier Analysis of Time Series: An Introduction, Second Edition
BOLLEN Structural Equations with Latent Variables
BOLLEN and CURRAN Latent Curve Models: A Structural Equation Perspective
BOROVKOV Ergodicity and Stability of Stochastic Processes
BOULEAU Numerical Methods for Stochastic Processes
BOX Bayesian Inference in Statistical Analysis
BOX R. A. Fisher, the Life of a Scientist
BOX and DRAPER Empirical Model-Building and Response Surfaces
BOX and DRAPER Evolutionary Operation: A Statistical Method for Process Improvement
BOX, HUNTER, and HUNTER Statistics for Experimenters: An Introduction to Design, Data
Analysis, and Model Building
BOX, HUNTER, and HUNTER Statistics for Experimenters: Design, Innovation and Discovery,
Second Edition
BOX and LUCE∼NO Statistical Control by Monitoring and Feedback Adjustment
BRANDIMARTE Numerical Methods in Finance: A MATLAB-Based Introduction
BROWN and HOLLANDER Statistics: A Biomedical Introduction
BRUNNER, DOMHOF, and LANGER Nonparametric Analysis of Longitudinal Data in Factorial
Experiments
BUCKLEW Large Deviation Techniques in Decision, Simulation, and Estimation
CAIROLI and DALANG Sequential Stochastic Optimization
CASTILLO, HADI, BALAKRISHNAN and SARABIA Extreme Value and Related Models with
Applications in Engineering and Science
CHAN Time Series: Applications to Finance
CHATTERJEE and HADI Regression Analysis by Example, Fourth Edition
CHATTERJEE and HADI Sensitivity Analysis in Linear Regression
CHATTERJEE and PRICE Regression Analysis by Example, Third Edition
CHERNICK Bootstrap Methods: A Practitioner’s Guide
CHERNICK and FRIIS Introductory Biostatistics for the Health Sciences
CHILÈS and DELFINER Geostatistics: Modeling Spatial Uncertainty
CHOW and LIU Design and Analysis of Clinical Trials: Concepts and Methodologies, Second
Edition
CLARKE and DISNEY Probability and Random Processes: A First Course with Applications,
Second Edition
COCHRAN and COX Experimental Designs, Second Edition
∗Now available in a lower priced paperback edition in the Wiley Classics Library.

CONGDON Applied Bayesian Modelling
CONGDON Bayesian Models for Categorical Data
CONGDON Bayesian Statistical Modelling
CONGDON Bayesian Statistical Modelling, Second Edition
CONOVER Practical Nonparametric Statistics, Second Edition
COOK Regression Graphics
COOK and WEISBERG An Introduction to Regression Graphics
COOK and WEISBERG Applied Regression Including Computing and Graphics
CORNELL Experiments with Mixtures, Designs, Models, and the Analysis of Mixture Data, Third
Edition
COVER and THOMAS Elements of Information Theory
COX A Handbook of Introductory Statistical Methods
COX Planning of Experiments
CRESSIE Statistics for Spatial Data, Revised Edition
CSÖRGÖ and HORVÁTH Limit Theorems in Change Point Analysis
DANIEL Applications of Statistics to Industrial Experimentation
DANIEL Biostatistics: A Foundation for Analysis in the Health Sciences, Sixth Edition
DANIEL Fitting Equations to Data: Computer Analysis of Multifactor Data, Second Edition
DASU and JOHNSON Exploratory Data Mining and Data Cleaning
DAVID and NAGARAJA Order Statistics, Third Edition
DEGROOT, FIENBERG, and KADANE Statistics and the Law
DEL CASTILLO Statistical Process Adjustment for Quality Control
DEMARIS Regression with Social Data: Modeling Continuous and Limited Response Variables
DEMIDENKO Mixed Models: Theory and Applications
DENISON, HOLMES, MALLICK, and SMITH Bayesian Methods for Nonlinear Classification
and Regression
DETTE and STUDDEN The Theory of Canonical Moments with Applications in Statistics, Prob-
ability, and Analysis
DEY and MUKERJEE Fractional Factorial Plans
DILLON and GOLDSTEIN Multivariate Analysis: Methods and Applications
DODGE Alternative Methods of Regression
DODGE and ROMIG Sampling Inspection Tables, Second Edition
DOOB Stochastic Processes
DOWDY, WEARDEN, and CHILKO Statistics for Research, Third Edition
DRAPER and SMITH Applied Regression Analysis, Third Edition
DRYDEN and MARDIA Statistical Shape Analysis
DUDEWICZ and MISHRA Modern Mathematical Statistics
DUNN and CLARK Applied Statistics: Analysis of Variance and Regression, Second Edition
DUNN and CLARK Basic Statistics: A Primer for the Biomedical Sciences, Third Edition
DUPUIS and ELLIS A Weak Convergence Approach to the Theory of Large Deviations
EDLER and KITSOS (editors) Recent Advances in Quantitative Methods in Cancer and Human
Health Risk Assessment
ELANDT-JOHNSON and JOHNSON Survival Models and Data Analysis
ENDERS Applied Econometric Time Series
ETHIER and KURTZ Markov Processes: Characterization and Convergence
EVANS, HASTINGS, and PEACOCK Statistical Distribution, Third Edition
FELLER An Introduction to Probability Theory and Its Applications, Volume I, Third Edition,
Revised; Volume II, Second Edition
FISHER and VAN BELLE Biostatistics: A Methodology for the Health Sciences
∗Now available in a lower priced paperback edition in the Wiley Classics Library.

FITZMAURICE, LAIRD, and WARE Applied Longitudinal Analysis
FLEISS The Design and Analysis of Clinical Experiments
FLEISS Statistical Methods for Rates and Proportions, Second Edition
FLEMING and HARRINGTON Counting Processes and Survival Analysis
FULLER Introduction to Statistical Time Series, Second Edition
FULLER Measurement Error Models
GALLANT Nonlinear Statistical Models.
GEISSER Modes of Parametric Statistical Inference
GELMAN and MENG (editors) Applied Bayesian Modeling and Casual Inference from Incomplete-
data Perspectives
GEWEKE Contemporary Bayesian Econometrics and Statistics
GHOSH, MUKHOPADHYAY, and SEN Sequential Estimation
GIESBRECHT and GUMPERTZ Planning, Construction, and Statistical Analysis of Comparative
Experiments
GIFI Nonlinear Multivariate Analysis
GIVENS and HOETING Computational Statistics
GLASSERMAN and YAO Monotone Structure in Discrete-Event Systems
GNANADESIKAN Methods for Statistical Data Analysis of Multivariate Observations, Second
Edition
GOLDSTEIN and LEWIS Assessment: Problems, Development, and Statistical Issues
GREENWOOD and NIKULIN A Guide to Chi-Squared Testing
GROSS and HARRIS Fundamentals of Queueing Theory, Third Edition
HAHN and SHAPIRO Statistical Models in Engineering
HAHN and MEEKER Statistical Intervals: A Guide for Practitioners
HALD A History of Probability and Statistics and their Applications Before 1750
HALD A History of Mathematical Statistics from 1750 to 1930
HAMPEL Robust Statistics: The Approach Based on Influence Functions
HANNAN and DEISTLER The Statistical Theory of Linear Systems
HEIBERGER Computation for the Analysis of Designed Experiments
HEDAYAT and SINHA Design and Inference in Finite Population Sampling
HEDEKER and GIBBONS Longitudinal Data Analysis
HELLER MACSYMA for Statisticians
HINKELMANN and KEMPTHORNE Design and Analysis of Experiments, Volume 1: Introduc-
tion to Experimental Design
HINKELMANN and KEMPTHORNE Design and analysis of experiments, Volume 2: Advanced
Experimental Design
HOAGLIN, MOSTELLER, and TUKEY Exploratory Approach to Analysis of Variance
HOAGLIN, MOSTELLER, and TUKEY Exploring Data Tables, Trends and Shapes
HOAGLIN, MOSTELLER, and TUKEY Understanding Robust and Exploratory Data Analysis
HOCHBERG and TAMHANE Multiple Comparison Procedures
HOCKING Methods and Applications of Linear Models: Regression and the Analysis of Variance,
Second Edition
HOEL Introduction to Mathematical Statistics, Fifth Edition
HOGG and KLUGMAN Loss Distributions
HOLLANDER and WOLFE Nonparametric Statistical Methods, Second Edition
HOSMER and LEMESHOW Applied Logistic Regression, Second Edition
HOSMER and LEMESHOW Applied Survival Analysis: Regression Modeling of Time to Event
Data
HUBER Robust Statistics
HUBERTY Applied Discriminant Analysis
HUNT and KENNEDY Financial Derivatives in Theory and Practice, Revised Edition
HUSKOVA, BERAN, and DUPAC Collected Works of Jaroslav Hajek—with Commentary
HUZURBAZAR Flowgraph Models for Multistate Time-to-Event Data

IMAN and CONOVER A Modern Approach to Statistics
JACKSON A User’s Guide to Principle Components
JOHN Statistical Methods in Engineering and Quality Assurance
JOHNSON Multivariate Statistical Simulation
JOHNSON and BALAKRISHNAN Advances in the Theory and Practice of Statistics: A Volume
in Honor of Samuel Kotz
JOHNSON and BHATTACHARYYA Statistics: Principles and Methods, Fifth Edition
JUDGE, GRIFFITHS, HILL, LU TKEPOHL, and LEE The Theory and Practice of Econometrics,
Second Edition
JOHNSON and KOTZ Distributions in Statistics
JOHNSON and KOTZ (editors) Leading Personalities in Statistical Sciences: From the Seventeenth
Century to the Present
JOHNSON, KOTZ, and BALAKRISHNAN Continuous Univariate Distributions, Volume 1,
Second Edition
JOHNSON, KOTZ, and BALAKRISHNAN Continuous Univariate Distributions, Volume 2,
Second Edition
JOHNSON, KOTZ, and BALAKRISHNAN Discrete Multivariate Distributions
JOHNSON, KOTZ, and KEMP Univariate Discrete Distributions, Second Edition
JURECKOVA and SEN Robust Statistical Procedures: Asymptotics and Interrelations
JUREK and MASON Operator-Limit Distributions in Probability Theory
KADANE Bayesian Methods and Ethics in a Clinical Trial Design
KADANE and SCHUM A Probabilistic Analysis of the Sacco and Vanzetti Evidence
KALBFLEISCH and PRENTICE The Statistical Analysis of Failure Time Data, Second Edition
KARIYA and KURATA Generalized Least Squares
KASS and VOS Geometrical Foundations of Asymptotic Inference
KAUFMAN and ROUSSEEUW Finding Groups in Data: An Introduction to Cluster Analysis
KEDEM and FOKIANOS Regression Models for Time Series Analysis
KENDALL, BARDEN, CARNE, and LE Shape and Shape Theory
KHURI Advanced Calculus with Applications in Statistics, Second Edition
KHURI, MATHEW, and SINHA Statistical Tests for Mixed Linear Models
KISH Statistical Design for Research
KLEIBER and KOTZ Statistical Size Distributions in Economics and Actuarial Sciences
KLUGMAN, PANJER, and WILLMOT Loss Models: From Data to Decisions
KLUGMAN, PANJER, and WILLMOT Solutions Manual to Accompany Loss Models: >From
Data to Decisions
KOTZ, BALAKRISHNAN, and JOHNSON Continuous Multivariate Distributions, Volume 1,
Second Edition
KOTZ and JOHNSON (editors) Encyclopedia of Statistical Sciences: Volumes 1 to 9 with Index
KOTZ and JOHNSON (editors) Encyclopedia of Statistical Sciences: Supplement Volume
KOTZ, READ, and BANKS (editors) Encyclopedia of Statistical Sciences: Update Volume 1
KOTZ, READ, and BANKS (editors) Encyclopedia of Statistical Sciences: Update Volume 2
KOVALENKO, KUZNETZOV, and PEGG Mathematical Theory of Reliability of Time-
Dependent Systems with Practical Applications
KUROWICKA and COOKE Uncertainty Analysis with High Dimensional Dependence Modelling
LACHIN Biostatistical Methods: The Assessment of Relative Risks
LAD Operational Subjective Statistical Methods: A Mathematical, Philosophical, and Historical
Introduction
LAMPERTI Probability: A Survey of the Mathematical Theory, Second Edition
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
∗Now available in a lower priced paperback edition in the Wiley - Interscience Paperback Series.

LANGE, RYAN, BILLARD, BRILLINGER, CONQUEST, and GREENHOUSE Case Studies in
Biometry
LARSON Introduction to Probability Theory and Statistical Inference, Third Edition
LAWLESS Statistical Models and Methods for Lifetime Data, Second Edition
LAWSON Statistical Methods in Spatial Epidemiology, Second Edition
LE Applied Categorical Data Analysis
LE Applied Survival Analysis
LEE Structural Equation Modeling: A Bayesian Approach
LEE and WANG Statistical Methods for Survival Data Analysis, Third Edition
LEPAGE and BILLARD Exploring the Limits of Bootstrap
LEYLAND and GOLDSTEIN (editors) Multilevel Modelling of Health Statistics
LIAO Statistical Group Comparison
LINDVALL Lectures on the Coupling Method
LINHART and ZUCCHINI Model Selection
LITTLE and RUBIN Statistical Analysis with Missing Data, Second Edition
LLOYD The Statistical Analysis of Categorical Data
LOWEN and TEICH Fractal-Based Point Processes
MAGNUS and NEUDECKER Matrix Differential Calculus with Applications in Statistics and
Econometrics, Revised Edition
MALLER and ZHOU Survival Analysis with Long Term Survivors
MALLOWS Design, Data, and Analysis by Some Friends of Cuthbert Daniel
MANN, SCHAFER, and SINGPURWALLA Methods for Statistical Analysis of Reliability and Life
Data
MANTON, WOODBURY, and TOLLEY Statistical Applications Using Fuzzy Sets
MARCHETTE Random Graphs for Statistical Pattern Recognition
MARDIA and JUPP Directional Statistics
MARONNA, MARTIN, and YOHAI Robust Statistics: Theory and Methods
MASON, GUNST, and HESS Statistical Design and Analysis of Experiments with Applications to
Engineering and Science, Second Edition
MCCULLOCH and SERLE Generalized, Linear, and Mixed Models
MCFADDEN Management of Data in Clinical Trials
MCLACHLAN Discriminant Analysis and Statistical Pattern Recognition
MCLACHLAN, DO, and AMBROISE Analyzing Microarray Gene Expression Data
MCLACHLAN and KRISHNAN The EM Algorithm and Extensions
MCLACHLAN and PEEL Finite Mixture Models
MCNEIL Epidemiological Research Methods
MEEKER and ESCOBAR Statistical Methods for Reliability Data
MEERSCHAERT and SCHEFFLER Limit Distributions for Sums of Independent Random
Vectors: Heavy Tails in Theory and Practice
MICKEY, DUNN, and CLARK Applied Statistics: Analysis of Variance and Regression, Third
Edition
MILLER Survival Analysis, Second Edition
MONTGOMERY, PECK, and VINING Introduction to Linear Regression Analysis, Fourth
Edition
MORGENTHALER and TUKEY Configural Polysampling: A Route to Practical Robustness
MUIRHEAD Aspects of Multivariate Statistical Theory
MULLER and STEWART Linear Model Theory: Univariate, Multivariate, and Mixed Models
MURRAY X-STAT 2.0 Statistical Experimentation, Design Data Analysis, and Nonlinear Opti-
mization
MURTHY, XIE, and JIANG Weibull Models
MYERS and MONTGOMERY Response Surface Methodology: Process and Product Optimization
Using Designed Experiments, Second Edition

MYERS, MONTGOMERY, and VINING Generalized Linear Models. With Applications in Engi-
neering and the Sciences
NELSON Accelerated Testing, Statistical Models, Test Plans, and Data Analysis
NELSON Applied Life Data Analysis
NEWMAN Biostatistical Methods in Epidemiology
OCHI Applied Probability and Stochastic Processes in Engineering and Physical Sciences
OKABE, BOOTS, SUGIHARA, and CHIU Spatial Tesselations: Concepts and Applications of
Voronoi Diagrams, Second Edition
OLIVER and SMITH Influence Diagrams, Belief Nets and Decision Analysis
PALTA Quantitative Methods in Population Health: Extentions of Ordinary Regression
PANJER Operational Risks: Modeling Analytics
PANKRATZ Forecasting with Dynamic Regression Models
PANKRATZ Forecasting with Univariate Box-Jenkins Models: Concepts and Cases
PARZEN Modern Probability Theory and Its Applications
PENA, TIAO, and TSAY A Course in Time Series Analysis
PIANTADOSI Clinical Trials: A Methodologic Perspective
PORT Theoretical Probability for Applications
POURAHMADI Foundations of Time Series Analysis and Prediction Theory
PRESS Bayesian Statistics: Principles, Models, and Applications
PRESS Subjective and Objective Bayesian Statistics, Second Edition
PRESS and TANUR The Subjectivity of Scientists and the Bayesian Approach
PUKELSHEIM Optimal Experimental Design
PURI, VILAPLANA, and WERTZ New Perspectives in Theoretical and Applied Statistics
PUTERMAN Markov Decision Processes: Discrete Stochastic Dynamic Programming
QIU Image Processing and Jump Regression Analysis
RAO Linear Statistical Inference and its Applications, Second Edition
RAUSAND and HOYLAND System Reliability Theory: Models, Statistical Methods and Applica-
tions, Second Edition
RENCHER Linear Models in Statistics
RENCHER Methods of Multivariate Analysis, Second Edition
RENCHER Multivariate Statistical Inference with Applications
RIPLEY Spatial Statistics
RIPLEY Stochastic Simulation
ROBINSON Practical Strategies for Experimenting
ROHATGI and SALEH An Introduction to Probability and Statistics, Second Edition
ROLSKI, SCHMIDLI, SCHMIDT, and TEUGELS Stochastic Processes for Insurance and Finance
ROSENBERGER and LACHIN Randomization in Clinical Trials: Theory and Practice
ROSS Introduction to Probability and Statistics for Engineers and Scientists
ROSSI, ALLENBY, and MCCULLOCH Bayesian Statistics and Marketing
ROUSSEEUW and LEROY Robust Regression and Outline Detection
RUBIN Multiple Imputation for Nonresponse in Surveys
RUBINSTEIN Simulation and the Monte Carlo Method
RUBINSTEIN and MELAMED Modern Simulation and Modeling
RYAN Modern Regression Methods
RYAN Statistical Methods for Quality Improvement, Second Edition
SALEH Theory of Preliminary Test and Stein-Type Estimation with Applications
SALTELLI, CHAN, and SCOTT (editors) Sensitivity Analysis
SCHEFFE The Analysis of Variance
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
∗Now available in a lower priced paperback edition in the Wiley - Interscience Paperback Series.

SCHIMEK Smoothing and Regression: Approaches, Computation, and Application
SCHOTT Matrix Analysis for Statistics
SCHOUTENS Levy Processes in Finance: Pricing Financial Derivatives
SCHUSS Theory and Applications of Stochastic Differential Equations
SCOTT Multivariate Density Estimation: Theory, Practice, and Visualization
SEARLE Linear Models
SEARLE Linear Models for Unbalanced Data
SEARLE Matrix Algebra Useful for Statistics
SEARLE and WILLETT Matrix Algebra for Applied Economics
SEBER Multivariate Observations
SEBER and LEE Linear Regression Analysis, Second Edition
SEBER and WILD Nonlinear Regression
SENNOTT Stochastic Dynamic Programming and the Control of Queueing Systems
SERFLING Approximation Theorems of Mathematical Statistics
SHAFER and VOVK Probability and Finance: Its Only a Game!
SILVAPULLE and SEN Constrained Statistical Inference: Inequality, Order, and Shape Restrictions
SINGPURWALLA Reliability and Risk: A Bayesian Perspective
SMALL and MCLEISH Hilbert Space Methods in Probability and Statistical Inference
SRIVASTAVA Methods of Multivariate Statistics
STAPLETON Linear Statistical Models
STAUDTE and SHEATHER Robust Estimation and Testing
STOYAN, KENDALL, and MECKE Stochastic Geometry and Its Applications, Second Edition
STOYAN and STOYAN Fractals, Random and Point Fields: Methods of Geometrical Statistics
STYAN The Collected Papers of T. W. Anderson: 1943–1985
SUTTON, ABRAMS, JONES, SHELDON, and SONG Methods for Meta-Analysis in Medical
Research
TANAKA Time Series Analysis: Nonstationary and Noninvertible Distribution Theory
THOMPSON Empirical Model Building
THOMPSON Sampling, Second Edition
THOMPSON Simulation: A Modeler’s Approach
THOMPSON and SEBER Adaptive Sampling
THOMPSON, WILLIAMS, and FINDLAY Models for Investors in Real World Markets
TIAO, BISGAARD, HILL, PENA, and STIGLER (editors) Box on Quality and Discovery: with
Design, Control, and Robustness
TIERNEY LISP-STAT: An Object-Oriented Environment for Statistical Computing and Dynamic
Graphics
TSAY Analysis of Financial Time Series
UPTON and FINGLETON Spatial Data Analysis by Example, Volume II: Categorical and Direc-
tional Data
VAN BELLE Statistical Rules of Thumb
VAN BELLE, FISHER, HEAGERTY, and LUMLEY Biostatistics: A Methodology for the Health
Sciences, Second Edition
VESTRUP The Theory of Measures and Integration
VIDAKOVIC Statistical Modeling by Wavelets
VINOD and REAGLE Preparing for the Worst: Incorporating Downside Risk in Stock Market
Investments
WALLER and GOTWAY Applied Spatial Statistics for Public Health Data
WEERAHANDI Generalized Inference in Repeated Measures: Exact Methods in MANOVA and
Mixed Models
∗Now available in a lower priced paperback edition in the Wiley Classics Library.

WEISBERG Applied Linear Regression, Second Edition
WELISH Aspects of Statistical Inference
WESTFALL and YOUNG Resampling-Based Multiple Testing: Examples and Methods for p-Value
Adjustment
WHITTAKER Graphical Models in Applied Multivariate Statistics
WINKER Optimization Heuristics in Economics: Applications of Threshold Accepting
WONNACOTT and WONNACOTT Econometrics, Second Edition
WOODING Planning Pharmaceutical Clinical Trials: Basic Statistical Principles
WOOLSON and CLARKE Statistical Methods for the Analysis of Biomedical Data, Second Edition
WU and HAMADA Experiments: Planning, Analysis, and Parameter Design Optimization
WU and ZHANG Nonparametric Regression Methods for Longitudinal Data Analysis: Mixed-
Effects Modeling Approaches
YANG The Construction Theory of Denumerable Markov Processes
YOUNG, VALERO-MORA, and FRIENDLY Visual Statistics: Seeing Data with Dynamic Inter-
active Graphics
ZELLNER An Introduction to Bayesian Inference in Econometrics
ZELTERMAN Discrete Distributions: Applications in the Health Sciences
ZHOU, OBUCHOWSKI, and McCLISH Statistical Methods in Diagnostic Medicine
∗Now available in a lower priced paperback edition in the Wiley Classics Library.

