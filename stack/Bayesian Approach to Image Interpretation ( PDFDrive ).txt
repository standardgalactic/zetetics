Bayesian Approach to
Image Interpretation

THE KLUWER INTERNATIONAL SERIES 
IN ENGINEERING AND COMPUTER SCIENCE 

BAYESIAN APPROACH TO 
IMAGE INTERPRETATION
SUNIL K. KOPPARAPU 
Research and Development Group 
Aquila Technologies Private Limited 
Bangalore - 560 068. 
India
kopparapu @ bigfoot.com 
UDAY B. DESAI 
SPANN Lab. 
Department of Electrical Engineerin 
Indian Institute of Technology - Bombay
Powai, Mumbai 400 076.
India
ubdesai @ ee.iitb.ernet.in 
Kluwer Academic Publishers 
New York/Boston/Dordrecht/London/Moscow

eBook ISBN:
0-306-46996-0
Print ISBN:
0-792-37372-3
©2002 Kluwer Academic Publishers
New York, Boston, Dordrecht, London, Moscow
Print  ©2000 Kluwer Academic / Plenum Publishers
New York
All rights reserved
No part of this eBook may be reproduced or transmitted in any form or by any means, electronic,
mechanical, recording, or otherwise, without written consent from the Publisher
Created in the United States of America
Visit Kluwer Online at:  
http://kluweronline.com
and Kluwer's eBookstore at:
http://ebooks.kluweronline.com

Contents
List of Figures
ix
List of Tables
xi
Preface
xiii
Acknowledgments
xv
1. OVERVIEW
1
1. 
Introduction 
1
1.1.
Scope of ComputerVision
2
2. 
Image Interpretation 
2
3.
Literature Review
5
4. 
Approaches 
7 
5. 
Layout of the Monograph 
9 
2. BACKGROUND 
11
1. 
Introduction 
11
2. 
Markov Random Field Models 
13
2.1. 
MRF Model and Gibbs Distribution 
13
2.2. 
Maximum Entropy Interpretation 
17
2.3. 
Application: Estimating Image Attribute 
18 
2.4. 
Posterior Distribution 
19 
2.5. 
Detecting Discontinuities: Line Field Model 
22 
2.6. 
Minimizing posteriori energy 
23 
3. 
Multiresolution 
24 
3.1. 
Gaussian and Laplacian Pyramid 
26 
3.2. 
Wavelet Pyramid 
29 

vi
3. MRF FRAMEWORK FOR IMAGE INTERPRETATION 
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
35
1.
MRF on a Graph
35
1.1. 
MRF based Formulation for Interpretation 
37
1.2.
Selection of Cliques Functions
39
43
1.
Introduction
43
2. 
MRF model leading to Bayesian Network Formulation 
44 
3. 
Bayesian Networks and Probabilistic Inference 
45
3.2. 
Definition of a Bayesian Network 
47
3.3. 
Joint Probability Distribution 
47
3.4. 
Properties of Bayesian Networks 
48
4. 
Probability Updating in Bayesian Networks 
49
4.1. 
The Dependence Semantics of Bayesian Networks 
49 
4.2. 
Different Schemes for Updating Probabilities 
50
4.3. 
Evidential Reasoning using Stochastic Simulation 
50 
4.  BAYESIAN NET APPROACH TO INTERPRETATION 
3.1. 
Introduction 
46
4.4. 
Computational Considerations in Probability Updating 
5 1 
5. 
Bayesian Networks for Gibbsian Image Interpretation 
51
5.1. 
Construction of a Suitable Network 
52
5.2. 
Dependencies in the Interpretation Network 
53
5.3. 
Relaxation Scheme with Gibbsian Distributions 
53 
6. 
Experimental Results 
54
6.1. 
Simulation on Real Images 
54
6.2. 
Obtaining the Conditionals 
54
7. 
Conclusions 
55
59
1. 
Introduction 
59
2,
Image Interpretation using Integration
60
3. 
The Joint Segmentation and Image Interpretation Scheme 
64 
3.1. 
Image Interpretation Algorithm 
64
4. 
Experimental Results 
69
4.1. 
Features Used 
69
4.2. 
Road Images 
69
4.3. 
Building Image 
71 
4.4.
Computer Images 
72
5. 
Conclusions 
74
5. JOINT SEGMENTATION AND IMAGE INTERPRETATION 

Contents
vii
6.CONCLUSIONS
79
Appendices
81
Appendix A.
Bayesian Reconstruction
81
Appendix B.
Proof of Hammersley-Clifford Theorem
83
1.
Justification for the General form for U(x)
84
1.1. 
Proof of the Hammersley-Clifford Theorem 
86
Appendix C.
Simulated Annealing Algorithm- Selecting T0 in
91
1.
Experiments
92
practise
Appendix D.
Custom Made Pyramids
95
Appendix E.
Proof of Theorem 4.6
97
Appendix F. 
k-means clustering 
99
Appendix G. 
Features used in Image Interpretation 
101 
1.
Primary Features 
101 
2.
Secondary Features 
102
Appendix H. 
Knowledge Acquisition 
107
108
1.
How to merge regions using the XV color editor 
2. 
Acquired Knowledge 
110
3. 
Knowledge Pyramid 
111 
Appendix I.
HMM for Clique Functions
113
References
115
Index
123

List of Figures 
1.1
Scope of Computer Vision.
2
1.2
Sample image to be interpreted.
3
1.3
Overviewof image interpretation task .
3
1.4
Image interpretation schemes.
4
2.1
Clique neighborhood.
15
2.2
Gaussian and Laplacian pyramid construction.
27
2.3
Example of a Gaussian pyramid.
28
2.5
Multiresolution pyramid using wavelets. 
32
2.6
Image and its wavelet transform.
32
3.1
Segmented image and RAG.
36
3.2
1-node Basis function.
40
3.3
Basis function for average gray level .
40
3.4
Basis function - variation of sigmoidal.
41
4.1
A Simple Bayesian Network.
46
4.2
Network used for image interpretation.
52
4.4
Interpretation results using Bayes net (Example 1).
58
5.1
Macro-level joint segmentation and image interpreta-
tion scheme .
61
5.2
Wavelet Transform representation of Y Ω.
62
5.3
Micro-level joint segmentation and image interpreta - 
tion scheme.
65
5.4
To describe the process ofrefining segmentation.
66
5.5
Image interpretation (real image, Example 1).
70
5.6
Image interpretation (real image, Example 2).
71
5.7
Image interpretation (real image, Example 3).
72
5.9
Image interpretation (real image, Example 5).
76
2.4
Example of a Laplacian pyramid.
28
4.3
Histograms obtained for some Conditional Distributions .
57
5.8
Image interpretation (real image, Example 4).
74

x
BAYESIANAPPROACH TO IMAGE INTERPRETATION
5.10
Image interpretation (real image, Example 6).
77
5.11
Plot of a possible basis function.
77
C.3
Restored images.
93
C.1
A plot showing ∆E versus exp (– (∆E)/  T0).
92
C.2
Original and Noisy images.
92
D.1
Vector resampling.
96
G.1
Description ofAreaand Convex Area.
103
G.2
Features used in image interpretation.
105
H.1
Sample image for knowledge acquisition.
108
H.2
Wavelet transform based scheme for knowledge acquisition. 108
H.3
Manual segmentation for knowledge acquisition.
109
H.4
Image labelling.
109
H.5
Manual segment merging for knowledge acquisition.
110
H.6
Post manual merging.
110

List of Tables 
5.1
Image interpretation using integration. 
61
5.2 
Knowledge base used for road images. 
70 
5.3
Knowledge base used for outdoor building scene. 
73
5.4 
Knowledge base used for the indoor computer images. 
75
C.1
Restoration comparison.
93
G.1 
Assumptions imposed on the problem due to the choice 
of features. 
102
G.2
Definitions of commonly used features. 
105
H.1
Details of image used for knowledge acquisition. 
110
H.2
Knowledge associated with Figure H.1
111
H.3
Rules for constructing knowledge pyramid. 
111

Preface
Image interpretation is a high-level vision task which focuses on giving se-
mantic description to regions in an image. There exists vast literature on image 
interpretation because of its use in very many applications in varied fields. In 
general, the process of low-level vision task of segmentation precedes the pro- 
cess of interpretation. Interpretation of the segmented regions itself uses a 
priori domain knowledge in some form or another. In effect the process of 
interpretation depends on the domain knowledge and on the segmentation pro- 
cess.
In this monograph we concentrate on image interpretation schemes based 
on Bayesian approaches. The use of Bayesian approach results in encoding 
the domain knowledge in a probabilistic framework and hence reduces the de- 
pendence of the interpretation process on the domain knowledge. Further the 
monograph describes a scheme which reduces the dependence of the interpre- 
tation process on the segmentation process by synergitically integrating the 
segmentation and the interpretation processes. 
We first review the image interpretation schemes drawing heavily from the 
work of Modestino and Zhang and Kim and Yang. Next we present some of 
our work on image interpretation using a Bayesian network, and some new 
results on joint segmentation and image interpretation which exploits wavelet 
transform for the task. In all the approaches, the basic assumption will be 
that the probability of the interpretation given the domain knowledge and the 
feature measurements is a Markov random field (MRF). 
In Chapter 1 we introduce and review the high-level vision task of image 
interpretation and detail the various approaches that exist in literature and give 
the layout of the monograph. In Chapter 2 we introduce the Markov random 
fields (MRF) as applicable to problems in computer vision and further discuss 
concepts which are useful in later chapters. Chapter 3 describes the Markov 
random field approach to image interpretation and introduces the useful con- 
cept of cliques. In Chapter 4 we describe the use of Bayesian network for 

xiv
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
image interpretation. Chapter 5 presents a new approach in image interpreta- 
tion where synergism between the process of segmentation and interpretation 
in a multiresolution framework is explored and a joint segmentation and image 
interpretation algorithm is presented. We discuss and conclude in Chapter 6. 
We have tried to keep this monograph as updated as possible but as is usual 
in a field like image understanding or interpretation, progress continues even 
as we finalize the manuscript. 
SUNIL N UBD

Acknowledgments
We are indebted to a number of people who have contributed in different, 
but important ways in making this monograph a reality. Foremost, thanks are 
due to the Signal Processing and Artificial Neural Networks (SPANN) Labo- 
ratory at IIT - Bombay, where most of the work was carried out. In particular 
we would like to thank V. P. Kumar for his work on image interpretation using 
Bayesian networks. We would like to thank Dr. P. G. Poonacha and Dr. S. C. 
Chaudhuri for many valuable discussions. We would like to thank R. Duggad 
and N. Kamath who shared their thoughts on various aspects of image inter- 
pretation. Thanks are also due to members of the SPANN Lab. (too many to 
name here), at IIT-Bombay, for their cooperation and dedication. 
We would like to thank Carl Harris, Jennifer Evans and Anne Murray of 
Kluwer Academic for their support and cooperation in the quick publication of 
this manuscript. 
Sunil would like to thank his family who have been a rich source of inspira- 
tion in all walks of his life. 

Chapter1
OVERVIEW
1. Introduction 
Vision comes to human so naturally that the processing capability of the 
human visual system is often taken for granted and for this reason we seem to 
underplay the difficulty in automating it. Computer vision is the science that 
develops theoretical and algorithmic basis by which useful information about 
the world can be extracted and analyzed automatically from an observed image 
or a sequence of images, by computations made on special or general purpose 
computers. The information derived from the image or a sequence of images 
could be used to compute any attribute of interest of the unknown object. The 
arrtibute of interest could be the three dimensional description of an unknown 
object or the motion of the object or the spatial property of the object. 
Problems in computer vision essentially revolve around the concept of pro-
viding vision capability of the human eye to the computer, albeit in a very small 
way. Often, one is trying to mimic the functioning of the eye or the human vi-
sual system (HVS) in general. Most of the theoretical and algorithmic basis 
for computer vision is motivated by the way the HVS functions, the underly-
ing reason being that HVS is not only the best vision system but in addition is 
an excellent image processor. It is robust in the sense that it rarely gets fooled;
this is because the HVS makes use of other cues as source of information. This 
helps in disambiguating illusions and thus makes the human visual system ro- 
bust. Problems in computer vision become hard to solve, because, one cannot 
claim complete knowledge regarding the functioning of the human visual sys- 
tem, though, vision is one of the widely used senses by human. Studies in 
psychophysics, physiology, and experiments carried out on the visual system 
of primates help in understanding the HVS. These studies and experiments 
form a basis for solving vision problems. 

2
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Figure1.1.
Scope of Computer Vision.
1.1. 
Scope of Computer Vision 
Computer vision is the enterprise of automating and integrating a wide range 
of processes and representations useful for vision perception. Though there 
is no strict demarcation between vision problems in terms of classification, 
we can broadly categorize vision problems into (i) low-level vision problems 
and (ii) high-level vision problems. Low-level vision problems are essentially 
image-image vision tasks, for example edge detection, segmentation, optical 
flow estimation and depth estimation from stereo pair to name a few, and high- 
level vision problems are image-scene vision tasks, where the objective is 
essentially to develop an understanding of the 3-D scene contents contained in 
the 2-D image. The tasks of image recognition, scene interpretation or scene 
understanding, navigation can be categorized as high-level vision. One can get 
an idea of the scope of computer vision by considering Table 1.1. It can be 
observed that image interpretation or scene understanding is a high-level task, 
nevertheless, inputs to this high-level vision task come from various low-level 
vision modules. 
2. 
Image Interpretation 
Image interpretation, the theme of the monograph is part of scene under- 
standing; it can be viewed as the process of giving meaning to a 2-D image 
by identifying and labelling significant objects or segments in the image. For
example we may first segment an image and then interpret each segment as 
being a road, river, building, trees, vehicle, etc. Consider the image in Figure 

Overview
3
Figure1.3.
Overview of image interpretation task.
1.2. It is a rather simple task for the human visual system to discern that the 
image contains, a road, trees, sidewalk, and sky. The objective of an image 
interpretation system is to make the computer do the same. 
Image interpretation is a high-level description of the environment from 
which the image was taken. It is essentially an analysis problem where we 
try to understand the image by identifying some important features or objects 
in the image and analyze them depending on their spatial relationship. Figure 
1.3 shows the task of image interpretation. 
Image interpretation is a high-level description of the environment from 
which the image was taken. It is essentially an analysis problem where we 
try to understand the image by identifying some important features or objects 
and analyze them depending on their spatial relationship. Interpretation must 
be in the form that is suitable for planning such diverse activities as robot 
arm and hand motion, obstacle avoidance by vehicle, aircraft navigation, re-
mote sensing or in biomedical applications. Image interpretation is knowledge 
based processing, which requires the use of both low-level processing (image 
processing techniques of contrast enhancement, computer vision techniques of 
segmentation, feature extraction, region labelling) and high-level vision tasks 
involving processing a great amount of non-image related knowledge underly-
Figure 1.2. Sample images to be interpreted, see text for details

4
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
2-D data
2-D data
(b) Three block scheme. 
Figure 1.4. 
Basic image interpretation schemes. 
ing the scene representation, for example, knowledge about the world physical 
constraints influencing entities [1]. At low-level the basic processing unit be-
ing pixel, there is no simple computational transformation that will map arrays 
of pixels onto stored symbolic concepts represented in the high-level knowl-
edge base. It is generally accepted that many stages of processing must take 
place for reliable interpretation of a scene. 
A typical image interpretation scheme is shown in Figure 1.4(a) and con- 
sist of two blocks a low-level vision block (segmentation) which segments the 
observed 2D image and computes various features for each segment. These 
features (Appendix G gives a list of features that can find use in image inter- 
pretation) could be based on gray level of the segment (for example average 
gray level, texture, etc.) as well as the shape of the segment (perimeter, area, 
compactness, etc.). The next block (interpretation) is the one which provides 
the semantic description, namely, interpretation to each segment of the image. 
This block has as its inputs, the domain knowledge and the various features 
obtained from the low-level vision block. More recently the trend is to look 
at the image interpretation scheme as a three scheme (Figure 1.4(b)) where in 
addition to segmentation and interpretation blocks we have a knowledge ac- 

Overview
5
quisition block which updates the domain knowledge base and gets its input 
from the 2-D scene and the segmentation block. 
Domain knowledge is what represents a priori information regarding vari-
ous interpretations. For example, for a road we may have the domain knowl-
edge as: average gray level = 70, standard deviation of gray level = 7.32,
and so on. It is to be noted that the process of acquiring domain knowledge re-
quires a fair amount of work (Appendix H describes one possible methodology 
to extract useful knowledge): in brief, one needs a set of sample images and on 
each image one needs to perform manual segmentation. Next various features 
are computed on these segments, and then the domain knowledge would be the 
average of the feature value of all the segments in all the images having the 
sameinterpretation. Analternate way ofcharacterizing the domainknowledge
is using the histogram for the features given the interpretation, which is used 
for the interpretation scheme based on Bayesian networks as seen in Chapter 
4.
Information on acquiring domain knowledge can be obtained from the mono-
graph of Ohta [2] and [3]. In Appendix G we describe a simple procedure to 
extract domain knowledge corresponding to a human face image. 
3.
Literature Review
A major application of image interpretation is in remote sensing which is 
widely used in geographical surveys and military applications. Image inter-
pretation also plays a major role in biomedical science and particle physics, 
where many of the results are recorded in the form of photographs. The 
earliest available image interpretation literature dates back to 1969 [4, 5, 6]. 
Research in the area of image interpretation encompasses images related to 
biomedical applications [7, 8, 9, 10, 11], satellite images [12], aerial imagery 
[13, 14, 15, 16, 17, 18, 19, 20, 21], road scene images [22], range images 
[23, 24, 25, 26], natural scene images [27, 28], color images [2], infra red im-
agery [29, 30], remotely sensed data images[31, 32], seismic data images[33], 
SAR images [34], laser radar images [35], astronomical image [36], thermal 
images [30], ultra sound images [37, 10], geophysical images [38]. Inter-
pretation based on multiple images like stereo [39, 40], sequence of moving 
images [41,42], moving viewer [43] images have also been reported in litera-
ture.
Early work on image interpretation was based largely on isolated image 
features and these salient features were classified into a finite set of classes, 
namely, interpretation labels, presumably this scheme is not robust especially 
when the low-level vision tasks give out an erroneous output. More recent 
approaches adopt knowledge based systems for image interpretation. Here, a 
great amount of non-image related knowledge underlying the scene representa-
Image interpretation has been researched widely for the last couple of decades.

6
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
tion is used along with the spatial constraints. Thus, even an ambiguous object 
can be recognized based on the successful recognition of its neighborhood ob-
jects.
The early work in knowledge based image interpretation is summarized in 
Nago and Matsuyama [13], Binford [44], Ohata [2], Smyrniotis [45], Ballard 
[46], Draper [22], Mitiche [47] and more recently by Chu [35,48] and for man 
made objects like office buildings and houses in an aerial images by Schutte 
[21]. Rule based strategies are especially appropriate in view of lack of com- 
plete models and algorithmic strategies [33, 1,49,50]. 
Various other approaches have been successfully used for image interpre-
tation. For example Fourier domain has been used by Andrews for automatic 
interpretation and classification of images [5]. Heller and others [34] use pro-
jective invariants and deformable templates for 
interpretation of synthetic 
aperture radar (SAR) images. cellular automata to interpret patterns in human 
melanomas has been used by Smolle and others [51], morphometric and den- 
sitometric approach has been used by Evangelista and Salvetti [52] for image 
interpretation. 
Bayesian and probabilistic networks have been used for the 
purpose of image interpretation [53, 54,55, 56]. Wilhelmi [57] proposed an 
interpretation scheme based on algebraic topology. Of late, Markov Random 
Field (MRF) models are being used for image interpretation with the view to 
make the interpretation systematic and domain independent [58,59, 56]. Most 
of the interpretation schemes assume the availability of a good segmented im- 
age of the scene a priori. But in practice obtaining a good segmented image is 
difficult for the simple reason that segmentation itself depends on interpreta- 
tion and hence is a function of the output of interpretation. 
Experiments conducted by Tenenbaum and Barrow [60], where they ex-
periment on the use of interpretation to guide segmentation, indicate the first 
possible use of interaction between the interpretation and segmentation mod-
ules. Though their requirement was to segment an image, we see that it was a 
good step, till then though it was known that both segmentation and interpre- 
tation were related, the fact was not exploited. Later there was discussion in 
this regard by Bajcsy in [61]. Sonka et al [62] have integrated segmentation 
and interpretation into a single feedback process that incorporates contextual 
knowledge. They use genetic algorithm to produce an optimal image inter- 
pretation. More recently, Kim and Yang [59] integrate segmentation and inter- 
pretation by forming a combined weighted energy function; the segmentation 
block is weighted high initially and as the algorithm iterates the weights shift 
to the interpretation block. 
In Chapter 5, we propose a scheme for joint segmentation and image in- 
terpretation in a multiresolution framework. Unlike earlier work in multires-
olution interpretation [16] we do not assume a priori, the availability of the 
segmented image. In fact, in the proposed approach, segmentation and in-

Overview
7
terpretation are interleaved (modular integration) and the two operations are 
carried out at each resolution of the multiresolution pyramid, the idea being 
that the two operations while integrating, help each other to perform better.
The segmentation module helps the interpretation module which in turn helps 
the segmentation module. 
4. 
Approaches 
Image interpretation has been an active area of research for the past couple
of decades. By and large most approaches fall in one of the following cate-
gories:
1 Classification based Approach: This was the early and direct approach 
[4, 6, 63] where using isolated image features a region was accorded a 
semantic label. It neither exploited any interrelationship that existed be- 
tween neighboring regions nor did it have a hierarchical decision scheme 
of elimination and acceptance. 
2 Knowledge Based Approach: This has also been termed as an expert sys- 
tem or rule based system for image interpretation. By far the most popu- 
lar approach followed by a large number of researchers. The monograph 
by Ohta [2] provides an excellent exposition on this topic. We cite some 
early and some recent papers on knowledge based image interpretation: 
[13, 44, 15, 64, 45, 50, 21]. This is simply an illustrative list and by no 
means does justice to the large body of work that exist on knowledge based 
image interpretation. For a reasonably comprehensive list please see [65]. 
In this approach, besides using various features for a given region, relation- 
ships between regions are also exploited. For example, there could be adja- 
cency constraints, like the road can be adjacent to a side walk but the road 
need not be adjacent to the sky. Moreover, there would be nominal value 
for the ratio of the perimeter of the road and the perimeter common to the 
road and the sidewalk. More such features are described in Appendix G. 
Of course, the designer of the system has to develop rules for interpretation 
based on features for the region under consideration and spatial constraints. 
One of the main drawback of this approach is its strong dependence on 
domain knowledge and the rigidity of rules. Thus, rule based image in- 
terpretation systems tend to be non-robust. To an extent this problem can 
be overcome by incorporating uncertainty factors or one could use a fuzzy 
expert system. 
3 Probabilistic Approach: This is the approach that will be emphasized in 
this monograph. The main motivation for considering a probabilistic frame- 
work is to make the interpretation process less dependent on domain knowl- 

8
BAYESIAN APPROACH TO IMAGE INTERPRETATION
edge. Here too, features and spatial constraints are used. But now they are 
embedded in a probability distribution function, thus they do not appear as 
rigid parameters as in the knowledge based system. Consequently, depen- 
dence on domain knowledge is reduced. 
(a) MRF Framework: Modestino and Zhang [58] introduce the Markov 
random field (MRF) framework for the image interpretation problem. 
A fundamental assumption they make is that the conditional proba- 
bility of the interpretation variables given the domain knowledge and 
measurement on the observed image is a MRF. Later work using this 
framework also adhere to this assumption. The interpretation problem 
is then formulated as a MAP (maximum a posteriori) estimation prob- 
lem.
Modestino and Zhang select the basis function for the clique poten- 
tial in the MRF model, based on functions used in fuzzy set theory. 
In order to overcome the prior selection of the cliques potential func-
tion and characterization of domain knowledge in terms of certain fixed 
numbers, Kim and Yang [59] proposed the use of neural networks , in 
particular the multilayer perceptron (MLP) network, for cliques poten-
tial functions. The MLP networks are trained using the features from 
a set of training images. Thus the neural network learns the domain 
knowledge and is represented by the weights in the MLP network. 
(b) Bayesian Networks: Another path uses probabilistic reasoning – pi-
oneered by Pearl [66], [67]. Unfortunately there is no Uniformity in 
naming probabilistic reasoning networks; they have been referred to as 
Bayesian networks, causal networks, belief networks or independence 
networks. We shall stick to Bayesian networks, only because this is the 
more widely used name. It should be noted that Bayesian networks are 
distinctly different from probabilistic expert systems. The fundamental 
distinction being that Bayesian networks are directed from cause to ef- 
fect, while expert systems (probabilistic or otherwise) are directed from 
effect to cause. For more on this please see Pearl [68] and Neapolitan 
A few researchers (Jensen et al [54], Mann and Binford [55], Kumar 
and Desai [56]) have adopted this approach for image interpretation. 
In this monograph we present the approach of [56], since it deals with 
the image interpretation problem using the MRF model and the MAP 
estimation framework. Here, the MRF model is used to build a relative 
simple, singly connected, two layered, Bayesian network for image in-
terpretation. The domain knowledge is represented as the conditional 
probabilities of features given the interpretation, implying a soft depen- 
dence on domain knowledge. The converged state of the network rep-
[69].

Overview
9
resents the MAP estimate for the interpretation. The Bayesian network 
is made to converge to its equilibrium state using a novel modification 
in the stochastic simulation algorithm of Pearl [67]; the novelty is in 
introducing an annealing schedule. 
4  Dempster-Shafer Evidential Reasoning [70]: Dempster-Shafer theory of 
evidence has been advocated to overcome some of the limitations of prob- 
ability theory; some authors have referred to this as an extension of prob- 
ability theory [71]. Though, we have not come across any work using this 
approach for image interpretation, we mention it here because it certainly 
does look like a possible approach for investigation; something future re-
searchers in the field may want to consider. 
All of the above approaches assumes that a segmented image, namely, an 
image where regions to be interpreted are clearly demarcated, is available. This 
is a very strong assumption, because it is well known that segmentation is a 
difficult problem, and as yet a satisfactory solution which works for a variety of 
images is illusive. In any case the basic approach is sequential-segmentation
followed by interpretation (Figure 1.4(a)). It seems reasonable to expect that 
segmentation and interpretation should operate in an interactive manner-one
should be able to help improve the results of the other module. Some attempts 
have been made in this direction. The work of Tenenbaum and Barrow [60] is 
possibly the first attempt at tackling this problem. In recent times there have 
not been much work in this direction. We mention the work of Kim and Yang 
[72], [73], where they propose an energy function in the MRF framework, 
which consists of two parts: one corresponding to segmentation and the other 
corresponding to interpretation. The segmentation part has a higher weight 
initially and as the iteration progress the interpretation block receives higher 
weight.
We have developed an integrated segmentation and interpretation scheme 
[74] using the multiresolution framework coupled with the MRF framework 
(Chapter 5). The wavelet transform of the image is used to refine the segmen-
tation obtained from a standard algorithm, say, the k-means clustering algo- 
rithm. At each resolution segmentation and interpretation are carried out in 
an interactive manner; this information is then communicated to the next finer 
resolution. The process continues till we reach the finest resolution. 
5.
Layoutof theMonograph
Chapter 2 covers the necessary background that encompasses the mono- 
graph — in brief we explore Markov random fields in general for vision ap-
plications. We also sketch details and concepts of multiresolution. In Chapter 
3, the MRF formulation of the image interpretation problem is presented. This 

10
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
section also contains a discussion the selection of clique function based on the 
work of Modestino and Zhang [58]. In Section 1.2 we review the selection 
of clique functions based on the work of Kim and Yang [59]. In Chapter 4 
we present some our results based on image interpretation using the Bayesian 
network. In Chapter 5 we present some of our new results on joint image seg- 
mentation and interpretation. Finally some thought on the future direction for 
image interpretation are presented in Chapter 6. 

Chapter 2 
BACKGROUND
In this chapter we present some material and concepts that will be useful 
when reading other parts of this monograph. The idea in introducing this ma-
terial is to make the monograph self contained and to give the reader easy 
access to material information. 
1. Introduction 
The basic objective in machine vision is to give the machine a sense of 
vision, that is to use visual sensors for such tasks as detection and recogni-
tion of objects, tracking of objects, avoiding obstacles, navigation, and per-
haps reasoning! Since the work of Marr [75] a generally accepted view is 
to consider a vision system as an information processing system which has a 
two level hierarchical structure. The first level is referred to as early vision 
and the second level as high-level vision (see Table 1.1). Typical early vision 
modules are, edge detection, texture, segmentation, shape, surface discontinu-
ities, stereo disparity, depth, color, motion; while high-level vision modules 
are recognition, vision based navigation, tracking, scene understanding, image 
interpretation.
Early vision modules are assumed to operate relatively independently, and 
then the information provided by these modules is integrated to solve the high- 
level vision problem. More recently the relative independence of early vision 
modules is being questioned, and the scenario emerging is that there is consid- 
erable interaction between these modules (modular integration). 
Early vision problems can be viewed as problems of recovering 3-D surface 
properties (for example depth) form 2-D observations (for example intensity 
images); thus vision problems can be regarded as problems in inverse optics. 
Most inverse problems are ill posed, namely (a) solution may not exist, or (b) 
solution may not be unique, or (c) solution may not depend continuously on 

12
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
the data. Vision is no exception, all the early vision problems listed above are 
ill posed. 
Another observation has been that in case of images, unlike one dimen- 
sional signals, important information is in the discontinuities. Thus one needs 
to have a somewhat different perspective when dealing with images, since a 
fundamental attribute to be estimated would be discontinuities (for example 
edges or segments). 
We will focus on early vision problems; and that too from the perspective 
that early vision modules are essentially independent. Most early vision prob- 
lems are formulated as energy (cost function) minimization problems, (see for 
example Horn [76] and Shah and Mumford [77]) and are solved using a vari-
ational approach. An alternate approach is to use probabilistic methods based 
on Markov random field (MRF) models of the image; solution is then cast as 
a maximum a posteriori (MAP) estimation problem (Besag [78], Geman and 
Geman [79]). This approach also involves solving an energy minimization 
problem; except the MRF framework makes it relatively easy to incorporate 
terms in the energy function to account for image discontinuities [79]. 
In general, the energy function associated with early vision problems will be 
non-convex, and could have several local minima, and also the global minima 
need not be unique. In fact, under the digitally quantized condition for images, 
one could view this as a problem in combinatorics. Typically one uses the 
simulated annealing (Kirkpatrick et al [80]) algorithm (SAA) for non-convex
minimization problems. Under very mild conditions convergence with proba- 
bility one to the set of global minima is guaranteed (Geman and Geman [79], 
Hajek [81], Aarts and Korst [82]). The Boltzmann machine (Hinton and Se-
jnowski [83]) can be viewed as a parallel and distributed implementation of 
the SAA (Aarts and Korst [82]). This is where neural networks come into the 
picture. We shall see that the selection of parameters in the MRF model would 
be equivalent to the learning problem in the Boltzmann machine (BM). 
Thus the flow of relationship we attempt to describe in this section is to 
formulate an early vision problem using a MRF model, the emerging energy 
function minimization is then carried out using a neural network. 
Early Vision 
Markov Random Field 
Neural Networks 
It is essential to note that MRF based formulation is by no means the only 
one. A deterministic approach to the problem is indeed taken in literature 
(Koch et al [84], Yuille [85]). A direct formulation of the an energy function, 

Background
13
taking into account discontinuities, is given. This energy function could then
be minimized using a Hopfield network.
An alternate deterministic approach starting with the MRF formulation is
also possible (see for example Zerubia and Chellappa [86], and Geiger and
Girosi [87]).
This approach is referred to as the mean field approximation
or sometimes mean field annealing. This algorithm is inherently parallel and
can be implemented using neural networks. Moreover, through simulations it
has been observed that the mean field approximation algorithm is significantly
faster than the simulated annealing algorithm.
2.
MarkovRandomFieldModels
The material in this section is drawn heavily on the pioneering work of 
Geman and Geman [79], and Besag [78], and more recent work of Poggio, 
Yuille, Chellappa, Marroquin, and others. The other part presents some new 
results on robust [88] image restoration. The perspective is to model an image 
as a Markov random field and then use a Bayesian approach for estimating 
early vision attributes of the image. This then leads to a non-convex energy 
function minimization problem, which can be satisfactorily solved using the 
simulated annealing algorithm. 
2.1.
Markov Random Field Model and Gibbs Distribution 
Let X be an image (for example intensity or range) over an N × M lattice
of sites S = {( i, j ) : 1 ≤ i ≤ N, 1 ≤ j ≤ M }, namely 
Lexicographical ordering 
The second equation represents the conventional lexicographical ordering for 
images. We assume that each pixel Xi,j takes values from a finite set of P
levels. These P levels could be 256 intensity levels in a typical 8 bit inten-

14
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
sity image, or 512 range levels in a typical range image. Thus the number 
of possible configurations for X becomes finite, namely, PNM, but extremely
large.
The basic idea behind Markovianess is for X i,j to have local dependencies. 
In fact, among other properties, it is this locality property which makes MRF 
modeling an attractive proposition for problems in vision. Formally, we say 
that
DEFINITION 2.1
X is a Markov random field (MRF), if and only if 
where P[ ⋅  |⋅ ] is the conditional probability, and η  i,j is the neighborhood of the 
site (i, j). 
For example a first order neighborhood of ( i,j) would consists of sites 
( i - 1, j ), ( i + 1, j ), ( i,j - 1), and ( i,j + 1). Figure 2.1 illustrates first 
and second order neighborhood. The neighborhood structure is assumed to 
be translational invariant, except at the boundary, where the free boundary 
assumption is made, namely, the set of neighbors of a boundary site is the 
intersection of the translational invariant neighborhood with the finite lattice. 
For example, assuming a second order neighborhood, the boundary site x (1, j) 
will have neighbors { x ( 1 , j - 1), x ( 1 , j + 1), x (2 , j - 1), x (2, j + 1)). In 
essence, this amounts to appending rows and columns of zeros all around the 
image; the size of these rows and columns will depend on the size of the image 
and the neighborhood order. 
Besides locality another important aspect of MRF models is that their dis-
tribution can be explicitly stated, and this is what lends real power to the use 
of MRF models. 
THEOREM
2.1 ( HAMMERSLEY-CLIFFORD)
Assume that X has a finite con-
figuration over S and that P [ x = 0] > 0, then X is a MRF with respect to a 
neighborhood η if and only if X is Gibbs distributed, with U(x ) defined as
(2.1)
where, x is a realization of X, Z is a normalization constant commonly re-
ferred to as the partition function and is given by 
(2.2)
(2.3)
and U(x) is given by 

Background
15
(a) First order. 
(b)Second order. 
Figure 2.1.
Clique neighborhood. 

16
The general form for U(x) is 
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
(2.4)
where the functions G ., ...,.(⋅ , ⋅ ⋅ ⋅ , ⋅) arearbitrary,except that they are zero if the
arguments do not belong to a clique. For example Gi,j;m ,n  (xi ,j, xm ,n ) = 0 if 
xi,j and xm ,n  do not belong to the same clique. 
Proof: See Appendix B. 
NOTE 2.1 G... ( ⋅ ⋅ ⋅ ) need not be a continuous function; in fact when the line
process is introduced one would indeed have a discontinuous G..., . Appendix B 
also gives a justification for the general form of U (x).
NOTE 2.2 Note that, for a 8 bit gray image ofsize 256 ×  256 image there are
(256)(256)2 configurations of x. Thus, ∑ all config. is indeed not practical.
Now U(x), also referred to as simply the energy function is defined by 
(2.5)
where Vc(x ) is the potential function defined over cliques C. This bring us to 
the discussion on cliques. 
DEFINITION 2.2 A clique is a subset C ⊆ S such that every pair of distinct
sites in C are neighbors. 
This definition is best understood by considering some examples. For a first 
order neighborhood corresponding to the site ( i, j) the clique set is 

Background
17
For a second order neighborhood, corresponding to the site (i, j), the clique 
set is 
C =
Figure 2.1(a) and Figure 2.1(b) display cliques for a first order and sec- 
ond order neighborhood systems respectively. Note that the number of cliques 
quickly blows up by increasing the neighborhood order. Thus, in most appli-
cations one considers a first order or a second order neighborhood systems. 
Often one considers higher order neighborhood systems but restricts attention 
to pair cliques, or some such simplification, to ensure that the total number of 
cliques per pixel is not too large. 
To illustrate the above definitions we cite the example of the Ising model 
[89]. Here X is a binary image with xi,j = ±1 (+1 representing an up-spin 
and –1 representing a down-spin). Moreover the Ising model assumes a first 
order neighborhood, and the energy function in is given by 
Such models were studied in the context of ferro magnetism. β depends on the
property of the material, and β > 0 represents the attractive case while β < 0
the repulsive case. a represents the effect of the external force field. A typical 
problem would be to find that configuration of up and down spins x which
maximizes P[X = x]. We see that for β > 0 this will amount to aligning all
the spins in the same direction as the external force field. 
2.2. 
Maximum Entropy Interpretation 
Besides the Hammersley-Clifford Theorem there is another reason for se-
lecting the Gibbs distribution model; this is based on the solution to a con- 
strained maximum entropy problem. Let the entropy be defined as 
(2.6)
where

18
Now consider the maximization problem: 
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
max[S ] = max
P
P
subject to 
Then the solution to this problem (which can be easily obtained using the 
method of Lagrange multipliers) is 
(2.7)
x
namely, the Gibbs distribution. 
Thus we are choosing a probability measure (Gibbs) which has the largest 
uncertainty (with respect to entropy) among all possible measures with a spec-
ified expected energy. 
NOTE 2.3 A typical unconditional problem can be defined as: Find a config-
uration x such that P [X = x ] is maximized, equivalently U (x) is minimized. 
2.3.
Application: Estimating ImageAttribute
In this section we demonstrate how the MRF theory developed in Section 2 
can be utilized in practise. We specifically look at the the problem of estimating 
the original image attribute from a noisy image. We first consider an image 
observation model (in a lexicographical ordering)
(2.8)
where Y is the observed image of size MN × 1, X is of size MN × 1 and is 
the image to be estimated along with its attributes; φ , in general, is a nonlinear 
operator, N is the corrupting noise to be further qualified a bit later, and Ο is
.
an invertible nonlinear operation (for example addition or multiplication). 
We assume that X is a MRF and is statistically independent of N. Usu-
ally one considers φ to be a linear operator and O⋅to be an additive operator,
resulting in the observation model 
Y= H X + N 
and at pixel level 
where H is a circulant matrix corresponding to the point spread function (PSF) 
and often referred to as the PSF matrix, having dimension MN × MN. Often

Background
19
H is also referred to as the blurring matrix. Lower case letters represent appro-
priate element of the respective matrix or column vector, and w is the window
size over which the PSF operates; typically a 3 × 3 or 5 × 5 window. For the
specific structure of H see [88].
Let A be any early vision attribute of the image X (for example depth, edge,
segment, texture, restored image, etc.), then the problem one is interested in is
Estimate A ( or a combination of A ) given Y.
We shall employ the maximum a posteriori (MAP) estimation approach, namely
Estimate A
such that
P [ A | Y = y ]
is maximized
NOTE 2.4 If we are interested in restoring X from Y then we would compute
that x which maximizes P [ X = x | Y = y ].
2.4.
Posterior Distribution
Before we can tackle any of the early vision problems we need an expression
for the posterior distribution P [ X = x|Y = y ]; and examine whether the
posterior distribution also has the locality property.
Using
Bayes rule, we
have
(2.9)
Since Y is the observed image, P [
Y = y ] is fixed. Moreover, P [ X = x] is
given by the Gibbs distribution (2.7). Thus the problem now is to find P[Y =
y|X = x ]. Using the observation model (2.8) we have 
(2.10)
where O⋅-l is the inverse of the operator O⋅, and to obtain (2.10) we have made
use of the assumption that N is statistically independent of X.
In order to have the locality property for the posterior distribution we need 
to assume that N is also Gibbsian. We know that Gaussian distribution is 
a special case of Gibbsian distribution; thus for specificity we assume that 
elements of N are i.i.d Gaussian distributed with mean µ and variance σ 2. 
Consequently
(2.11)
where || ⋅ || is the usual Euclidean norm. Using (2.1) and (2.11) in the expression
for the posterior distribution P [ X = x|Y = y ] (2.9) we obtain

20
where the posterior energy function 
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
and
(2.12)
Thus we see that computing the MAP estimate is equivalent to minimizing the 
posterior energy function, namely, 
(2.13)
To estimate other early vision attributes, we need to appropriately modify 
the potential function V c ( x ), and in some cases incorporate an additional field 
corresponding to the early vision attribute like, edges, segments, etc. In fact 
the art lies in selecting a good potential function. 
NOTE 2.5 For the linear case with additive noise the posterior energy func-
tion simplifies to 
2.4.1
In order to determine the neighborhood for the posterior distribution we 
consider the conditional probability of a pixel x i,j,
1
≥
 ( i,j )
≥
 ( N, M ),
given all the remaining pixels xk,l, 1
≥
 ( k , l )
≥
 ( N, M ) and (k , l ) ≠ ( i , j ) ,
and the observed image Y .
Neighborhood for the Posterior Distribution 
Using (2.7) it is easily shown that 
(2.14)
possible
levels

Background
21
In (2.14), x is a vector whose components are fixed except for the ( i,j ) th
component xi,j in lexicographical ordering. The component xi,j can take any 
of the P possible levels (for example 256 gray levels). The summation is over 
these P levels.
Define the vector 
Consequently the posterior distribution can be written as 
(2.15)
(2.16)
We assume that operator φ has the locality property, that is, φ operating on
xi , j will involve x i,j and pixels in the neighborhood of xi,j. Let this neigh- 
borhood corresponding to site ( i, j ) be denoted by ς i,j, which we also assume
to be translationally invariant. Note that this neighborhood will, in general, be 
different from η i,j the neighborhood for the MRF model.
Keeping in mind that we are interested in finding the neighborhood for the 
conditional probability distribution of the pixel xi,j given all other pixels and 
the observation Y, we now decompose U P ( x ) as follows. 
Now substituting (2.17) in (2.14) and canceling common terms in the numera- 
tor and the denominator we obtain 
possible
levels
Clearly the posterior neighborhood structure corresponding to site ( i, j ) is 
determined by the the sites involved in the above expression, namely 

22
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
which shows that the posterior distribution also has local dependencies and 
consequently
LEMMA 2.1
The posterior probability distribution (2.14) is a Markov random
field with the neighborhood η 
p
i, j.
Thus a typical conditional problem is 
Find a configuration x such that P [ X = x|Y = y ] is maximized, equiv-
alentlyU P(x ) (2.17) is minimized. 
Though the above is in the context of image restoration, it generalizes to es- 
timating early vision attributes by incorporating extra fields besides X, and 
appropriately modifying the potential function Vc ( x ) . 
2.5. 
Detecting Discontinuities: Line Field Model 
It is well known that in case of images important information is conveyed 
at points where there is a significant change in the image irradiance. Such 
changes correspond to changes in the surface property; these could be due to 
changes in depth, texture, surface orientation, source orientation, changes due 
to motion, or some other surface attribute. Thus it is important to incorporate 
a component to account for the presence or absence of discontinuities. 
Geman and Geman [79] introduced the concept of line field located on the 
dual lattice to detect discontinuities. This dual lattice has two sites, each cor-
responding to the vertical and horizontal line fields. The horizontal line field 
lij connects site ( i ,j ) 
to ( i , j – 1); this will aid in detecting a horizontal edge. 
Similarly, vertical line field vi,j connects site ( i,j ) to ( i - 1, j ) and this will 
help in detecting a vertical edge. Note that li,j and vi,j are {0,1} variables, and 
the corresponding line fields, L and V, are binary. 
Now, within the Bayesian framework, we can define a Gibbs (MRF) prior 
distribution that combines the image X, the horizontal line field L and the 
vertical line field V;
and
where U(x,l,v) is given by 

Background
23
(2.18)
The term in the first bracket (multiplying µ) signifies the interaction between 
the neighboring pixels; if the gradient is high (determined by a preset thresh- 
old), then the corresponding line field will get activated to indicate a disconti- 
nuity. For example 
li, j  
=
    
1
 
if
              | x 
i
 
,
 
j - x 
i
 -1, j
 | > threshold 
The terms in the second bracket (multiplying γ ) provide a penalty for every
discontinuity created and also prevent spurious discontinuities. Such line fields 
have been used earlier by Marroquin et al [90] and Geiger and Girosi [87]. 
Note that Markovianess is still preserved. For the purpose of illustration we 
have specified line filed models using a first order neighborhood for the cliques. 
Nevertheless, more complex line field models can be used; often one uses a 
second order line field model (see Graffigne and Geman [91]). Using the line 
field model, the posterior distribution can be expressed as 
where the posterior energy function 
As seen, the solution of any vision task formulated in a MRF framework 
reduces in computing the MAP estimate of the attribute of interest. This is 
equivalent to minimizing the posteriori energy function (2.13). The posteriori 
energy function is generally non-convex in nature. 
2.6.
In general U P( x ) will be non-convex, mainly due to the non-convexity of the 
potential function Vc ( x ) (see for example (2.18)). Consequently, algorithms 
based on ideas of steepest descent would be prone to being trapped in a local 
minima. Also the presence of a binary line field precludes the possibility of 
using steepest descent type algorithms. At present a good option seems to be 
some kind of random search method, though such algorithms are extremely 
slow. One such algorithm which has gained a lot of popularity and has proved 
Minimizingposteriori energy function UP(x )
=
0
o therwise

24
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
very effective in many problems, is the simulated annealing (SA) algorithm 
of Kirkpatrick et al [80] (see also Aarts and Korst [82]). We would like to 
mention that another algorithm due to Solis and Wets [92] referred to as the 
random optimization (RO) algorithm could also be used. From preliminary 
experiments conducted by us, it seems that the SA algorithm is faster than the 
RO algorithm, though one has to take care of some extra nuances of the SA 
algorithm.
2.6.1 
Simulated Annealing Algorithm 
depend on a temperature T, 
First the unconditional or the conditional probability distribution is made to 
where,
all config. x
where, x is a realization of X. Similar modification applies to the conditional 
distribution.
Let Lk be the number of iterations at temperature Tk, then the simulated 
annealing algorithm follows Listing 1. Under the inverse log cooling schedule 
the above algorithm converges in probability to the set of globally minimum 
solution (see Geman and Geman [79], Hajek [81]). Another crucial aspect of 
the SAA is the choice of the initial temperature T0, we discuss this in Appendix 
C.
NOTE 2.6 The acceptance criterion used in the SA algorithm is referred to 
us the Metropolis acceptance criterion, based on the earliest work on the use
of simulated annealing in physics by Metropolis et al [93]. 
3.
Multiresolution
The idea of looking at signals and analyzing them at various scales or res- 
olutions has received enormous attention in the field of computer vision. Mul-
tiresolution is a mode of efficiently and effectively representing the data with 
an objective of reducing the computational complexity. Multiresolution can be 
thought of as a data structure which produces a successive condensed repre- 
sentation of the information in a given image. The data at each resolution is 
the output of a bandpass filter with some centre frequency, usually the centre 
frequency of the filters are octave apart. The most obvious advantage of this 
type of representation is that they provide a possibility for reducing compu-
tational cost of various image operations. The reduction in the computational 

Background
25
Listing 1 Simulated Annealing Algorithm (SAA)
1: Initialize: ( xstart, T0, L0 ) {T0 is an initial high temperature}
2: k := 0;
3:
xold
:=
xstart
4: while Stop Criterion do
6:
7:
8:
xold := xnew {accept}
5:
for l := 1 to Lk do
Generate xnew {using Uniform, Gibbsian, or Gaussian sampler}
if U ( xnew ) ≥ U (x old ) then
9:
else
) > rand[0,1) then
(–U(xold)–U(xnew)
if
exp
11:
xold := xnew
12:
endif
13:
endif
14:
k: = k + 1
15:
16:
17:
18:
end for 
19: end while {If no change in U ( x ) for M successive Tk, or a fixed number 
of k's}
Update Lk ; (fixed or variable) 
Calculate Tk (inverse log or linear) 
Tk = 
or Tk = α Tk –1
A
log (k+1)’ 
cost is due to the fact that when we operate in a multiresolution framework, 
we operate on data from the coarse to the fine resolution. The result of going 
through such a procedure is that, at the finest resolution we start off with a 
fairly good guess of the solution, the guess having come from the immediate 
coarse resolution and hence we need less time to reach the solution. One does 
indeed need to start with an arbitrary guess at the coarsest resolution, but at that 
resolution the amount of data one is operating on is many orders of magnitude 
less than that at the finest resolution. 
In multiresolution representation, the problem of solving the vision task V is 
reduced to the task of solving V at each resolution. Let, V Ω represent the vision 
task at the finest resolution V. In the multiresolution approach the vision task 
VW is not solved directly, but is solved by solving the vision tasks V Ω –1, V Ω –2,
⋅ ⋅ ⋅, V Ω –n(n ≥ Ω) at coarse resolutions and appropriately passing the variables
of interest from coarse to fine resolution. The motivation to use multiresolution 
in vision task comes from: 
1 Vision tasks are usually computationally intensive and there is a need to
reduce its computational complexity, 
Tk

26
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
2 Most of the vision tasks are motivated by the functioning of HVS and the 
HVS uses multiresolution [1]. Experimental results show that a multifre-
quency channel decomposition seems to be taking place in the human vi-
sual cortex [94]. Experiments based on adaptation techniques show that at 
some stage in the HVS, the visual information in different frequency bands 
is processed separately. Experiments also show that the retina image is 
decomposed into several frequency bands, each having approximately the 
same bandwidth on an octave scale. 
There exists two predominant ways of constructing multiscale representa- 
tions using multiscale filtering: (i) Gaussian and Laplacian pyramids proposed 
by Burt and Adelson [95] and (ii) the wavelet pyramids [94]. Both these as- 
sume that the image size be a power of 2 (see Appendix D for discussion 
on custom made pyramids). Gaussian pyramids are obtained by smoothening 
brightness values over larger areas producing a set of low pass filtered copy 
of the original image and Laplacian pyramids are obtained by differentiating 
smoothed brightness values producing a set of bandpass filtered copy of the 
original image (see Figure 2.2). The wavelet pyramids are constructed by low 
pass filtering and high pass filtering the image along the rows and then along 
the columns to produce four quadrants 
Figure 2.5) such that the first 
quadrant is low pass filtered along both the rows and columns of the image, 
the second quadrant is low pass filtered along the rows and high pass filtered 
along the columns, the third quadrant is high pass filtered along the rows and 
low pass filtered along the columns and finally the fourth quadrant is high pass 
filtered along both rows and columns. The filter coefficients are determined 
from a set of four linear equations obtained as a consequence of the constraints 
placed on the coefficients of the filter [96]. 
Nevertheless the wavelet representation of the image is more compact in the 
sense, the lattice size does not increase. The wavelet transform of an image of 
size 2Ω  × 2Ω is again an image of size 2Ω × 2Ω where as the Gaussian pyramid
when constructed as suggested in [95] would end up with a larger number of 
data points [94]. 
3.1. 
Gaussian and Laplacian Pyramid 
The basic approach of Burt and Adelson 
is depicted in Figure 2.2. Let, 
xk be the gray level image at resolution k and let xk–1 represent the image 
at resolution ( k –1 ) , which is obtained using the algorithm proposed in [95]. 
An image xk at a given resolution is low pass filtered so that the high spatial 
frequencies are removed, as a result we can sample it at a lower rate (typically 
one half) and hence we have an image of lower resolution and one half the size 
of the original image in each dimension. This process of low pass filtering and 

Background
27
Figure 2.2.
from xk.
Procedure for obtaining the Gaussian ( x k-1 ) and Laplacian (∆k) pyramid images 
sub-sampling results in images, which are at different resolutions. In addition, 
the difference images, ∆ k at different resolutions are obtained by up-sampling 
the coarse image by a factor of 2, interpolating it, and then subtracting it from 
the next fine resolution image. 
A suitable kernel for low pass filtering is used to obtain images at different 
resolutions. If we assume a 1-D signal and the size of the kernel to be 5, then as 
shown by Burt and Adelson [95] the weights of the kernel, denoted by w (–2),
w(-1) , w(0) , w(1) , w(2), should satisfy the following constraints,
1 ∑ ii==
2– 2w(i) = 1 (Normalization),
2 w (i) = w ( –i) for i = 0, 1, 2 (Symmetry) and
3 if w (0) = a , w (1) = b ,w (2) = c, then a + 2 c = 2 b must be satisfied
(Equal Contribution). 
A 5 × 5 kernel from [95]
Ker = 

28
BAYESIAN APPROACH TO IMAGE ITERPRETATION 
Figure 2.3.
Gaussian pyramid ( xk ) constructed using the procedure depicted in Figure 2.2. 
Figure 2.4.
Laplacian pyramid ( ∆ k ) constructed using procedure depicted in Figure 2.2.
with a = 0.4, b = 0.25 and c = 0.05, can be used to convolve with the high 
resolution image xk; the convolved image is then down-sampled (for example, 
selecting every alternate pixel along each row and column) to obtain the image 
at lower resolution xk – 1. The process of obtaining the low resolution image
and the difference image is depicted in Figure 2.2, where the block Ker rep-
resents convolution with the kernel Ker and ↓ 2 represents down-sampling by 
2 (namely, considering only every alternate sample of the signal or in other 
.words discarding every alternate sample of the signal), and 
↓ 
2 represents up- 
sampling by 2 (namely, introducing a zero between every sample of the sig- 
nal). Figure 2.3 depicts the Gaussian pyramid ( xk ) and Figure 2.4 depicts that 
Laplacian pyramid ( ∆ k) constructed using the procedure shown in Figure 2.2. 
The leftmost image in Figure 2.3 is the image of size 256 × 256 at the finest
resolution Ω = 8. 

Background
29
3.2. Wavelet Pyramid 
Multiresolution approach using wavelets is an efficient and effective way of 
representing data. According to Mallat, a primary advantage of the multires-
olution approach using wavelets is its spatial orientation selectivity [94]. A 
disadvantage of Gaussian pyramid [95], which is overcome by the wavlets is 
that the data at separate levels are correlated, where as in the wavelet domain 
the data at each resolution is uncorrelated, this reflects in the form of increased 
data size in the Gaussian pyramid compared to the wavelet pyramid. 
Fourier transform is a tool widely used for many scientific purposes, but it 
is well suited only to the study of stationary signals where all frequencies have 
an infinite coherence time. The Fourier analysis brings only global informa- 
tion about the signal which is not sufficient, especially when we need to detect 
compact patterns. Gabor[97] introduced a local Fourier analysis, taking into 
account a sliding window, leading to a time-frequency analysis. This method 
is only applicable to situations where the coherence time is independent of the 
frequency. This is the case for instance for singing signals which have their 
coherence time determined by the geometry of the oral cavity. Morlet intro- 
duced the wavelet transform in order to have a coherence time proportional to 
the period [98]. 
Wavelets are functions, suited for the study of non-stationary continuous 
signals. They form the kernel of the wavelet transform, and enable mapping 
the signals from the time-domain into the time-frequency domain. The advan- 
tage of using wavelets is that at different times and at different frequencies, a 
different resolution can be obtained. The wavelet transform provides a unified 
framework in decomposing the signal into a set of basis functions by varying 
the resolution ∆ t and ∆ f in a time frequency plane. The basis functions are 
called wavelets and are obtained from a single prototype wavelet ψ called the 
mother wavelet. For any function to be a mother wavelet, it must satisfy the 
admissibility criterion [96]. The wavelet transform of a signal x (t ) ∈ L2 (IR),
denoted by XW
a
T (b ), is
(2.19)
.
--
where, ( ⋅ , ⋅ ) is the inner product, ψ ( t ) is the wavelet and a, b ∈ IR. Here, a
is the dilation parameter which determines the scale and b is the translation 
parameter. The signal x ( t ) can be reconstructed from its wavelet transform 
XW
a
T ( b ) by an inverse transformation, 
(2.20)

30
BAYESIAN APPROACH TO IMAGE INTERPRETATION
where, Cψ  is a constant that satisfies the admissibility criterion:
..
The discrete wavelet transform (DWT) is defined as, 
(2.21)
where m , n ∈ II, and a0 and b0 are constants such that a 0 > 1, b0 ≠ 0. In 
the rest of this section we assume a0 = 2 (dyadic wavelet) and b0
= 1. The 
discretization on dyadic grid initiates the idea for multiresolution analysis. The 
wavelet transform can be viewed as constant Q filtering with a set of bandpass
filters followed by subsample by a factor of two. 
Assume { ψ m,n } to constitute an orthonormal basis in L
2 ( IR ). Then,
(2.22)
m n 
The signal x ( t ) at a fixed resolution m , can be obtained by summing over all 
translations n ,
Hence, xm ( t ) at m th resolution is obtained by linear combination of the span- 
ning set { ψ m , n}n∞  = – ∞.  Let, Wm define the space spanned by { ψ m ,n} n∞ = – ∞. 
Hence, xm ( t ) ∈ Wm. It can be shown that the signal xm ( t ) at m th resolution
is orthogonal to the signal xl ( t ) at the lth resolution. Therefore, we can write, 
(2.23)
where, ⊕ represents the direct sum. Now, let Vm the space spanned by {φ m,k } k 
be defined as ⋅ ⋅ ⋅ ⊕ Wm–2 ⊕ Wm–1, then it can be shown that Vm have the nested 
structure
and Vm
+1 = Vm ⊕ Wm.
Since Wm contains the details (variation of the signal), a given resolution 
can be obtained by adding the details. This is the basis for the construction of 
multiresolution signals using the wavelet transform. 

Background
31
In practice, wavelet decomposition of the signal at various resolutions is 
implemented using sub-band filtering [99]. The low pass, subsamples approx- 
imation of the signal x ( t ) is obtained by passing the signal through a low pass 
filter, L (.), followed by down-sampling by a factor of 2, the detailed or the 
difference image is obtained by passing the signal through a high pass filter, 
H (.), and then down-sampling by a factor of 2. In other words, if L (.) is an 
ideal half-band low pass filter, then an ideal half-band high pass filter H (.)
will lead to a perfect representation of the original signal into two subsamples 
versions.
Suppose x ( t ) ∈ V0
and the above mentioned ladder of spaces exist, then 
we have V 0 = V–1 ⊕ W–1 , where V0 , V–1
and W–1
are spanned by { φ 0, k } k,
{ φ –1, k } k and { φ –l , k } k respectively. Then, we can represent x ( t ) in terms of 
xV-1 ( t ) and x W -1( t ) which are one level coarse 
where,
(2.24)
(2.25)
Thus, the problem of multiresolution decomposition in essence is to compute 
c – J,k , d – J,k , d –J+ 1
,k, ⋅⋅⋅ , d –1, k which are the wavelet coefficients. One can 
show that [94] 
(2.26)
and
(2.27)
where, l* and h* are the interscale basis coefficients and can be looked upon 
as a low pass ( L in Figure 2.5) and high pass filter ( H in Figure 2.5) respec- 
tively. Hence, { c – J +1, m } ∀ m represents a signal at resolution –J + 1, namely, 
x – J +1 and similarly { c –J,k }∀ k represents x –J. Hence (2.26) implies that the 
coarse signal x –J at resolution -J is obtained by low pass filter ( l*) the fine 
resolution image or signal ( x – J +1 ).
The general idea behind wavelets as applied to image processing or com-
puter vision is simply to look at the wavelet coefficients as an alternative rep- 
resentation of the image. So, instead of performing operations on the pixels we 
work with the wavelet coefficients. This gives us an opportunity to take advan- 
tage of their multiresolution structure and their time-frequency localization. In 
practise, wavelet pyramid is constructed, using a set of quadrature mirror fil- 

32
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Figure 2.5. 
the low pass filter and H is the high pass filter, L and H form a quadrature mirror pair. 
Procedure for obtaining multiresolution images using the wavelet transform; L is
(a)
(b)
Figure 2.6. 
Daubechies 4 tap filter). 
ters L and H corresponding to the low pass and high pass filtering respectively. 
Initially the quadrature mirror filters act along the rows of the image and then 
again on the columns of the resulting image. This results in the original image 
being divided into four quads as shown in Figure 2.5. Figure 2.6(a) shows a 
256 x 256 image and Figure 2.6(b) is the wavelet transform of 2.6(a) generated 
using the procedure depicted in Figure 2.5 with Daubechies 4 tap filter [96]. 
3.2.1 
Where to Use Multiresolution? 
It is important to know where it is beneficial to use multiresolution. It can 
be said that multiresolution can be used in all vision tasks, the motivation com-
ing from the fact that HVS uses multiresolution [100]. In fact, in the words 
of Tanimoto (foreword of [ 100]) the human retina acquires visual information 
at different degrees of spatial resolution at the same time – high resolution 
in the fovea and low resolution in the periphery; not only does the density of 
the receptors vary in the visual field, but the varying ratio of the receptors to 
(a) Image of size 256 x 256 (Pentagon) and (b) its wavelet transform (using the 

Background
33
the bipolar and ganglion cells that carry their information forward in the vi-
sual pathway further stretches the range of resolutions of the data obtained by 
the eye. Data at different resolutions play different roles in vision. Informa-
tion from periphery typically controls the focus of attention while fovea infor-
mation provides the details needed for finer distinction and careful analysis. 
It is strongly believed that the human eye very much acts in a multiresolu- 
tion framework, initially we tend to infer information by looking at the scene 
coarsely and then trying to pinpoint on the object that is of interest [1]. 
The multiresolution representation can be used with effect if it is known a 
priori that the attribute of interest (i) varies smoothly with scale, and (ii) co- 
incide spatially across all scales. This knowledge can improve significantly 
the computational savings by designing coarse to fine operations[101]. For 
example, the attribute, disparity of a stereo pair of images has the properties 
mentioned above and hence a coarse to fine strategy can be used to compute 
the disparity map. This procedure implies that first we apply operations to the 
coarse image and then use the results to focus attention or refine results in fine 
image, a philosophy advocated earlier by Marr and Poggio [102]. The prob- 
lems that use this method do not try to combine the descriptions at different 
scales, except in the sense of constraining the computation at the previous fine 
scale. In Chapter 5 we propose a joint segmentation and image interpretation 
algorithm that uses multiresolution. 

Chapter 3 
MRF FRAMEWORK FOR 
IMAGE INTERPRETATION 
The MRF formulation for the image interpretation problem is as follows: 
Using the available segmented image, for the image to be interpreted, a region 
adjacency graph (RAG) is constructed. Each node of the RAG corresponds 
to an image segment, and two nodes have connectivity between them if the 
corresponding two segments share a common boundary. Next it is assumed 
that the node interpretation given the domain knowledge and the features ob- 
tained from the observed image obey a MRF model. Constructing this model 
involves defining appropriate clique functions and estimating the clique param- 
eters. The interpretation problem is then solved as a MAP estimation problem 
[581]
1. 
MRF on a Graph 
corresponding RAG is as shown in Figure 3.1. 
■G
be a RAG with nodes R = { R 1 , R 2 ,⋅⋅⋅ R n } and E denoting the set of
edges. There will be an edge between node Ri and Rj if the corresponding 
segments Ri and Rj share a common boundary. 
Consider a segmented image having n segments { R 1, R 2, ⋅ ⋅ ⋅ R n } and the 
Let
■
the neighborhood system on G be
Note, Ri ∈ η ( Rj ) if Ri ∉ η ( Ri ) and Rj ∈ η ( Ri ).
■
 X = { X 1, X 2, ⋅ ⋅ ⋅ X n } be the set of random variables defined on R. Each
Xi corresponds to Ri. Moreover we assume that Xi takes values form a 
finite sample space. 

36
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Figure 3.1. 
Segmented image and RAG. 
DEFINITION 3.1
X is called an MRF on G with respect to the neighborhood 
η if
1 P [ X ] > 0 for all realizations of X. 
2 P
[Xi| X j ∀ j ≠ i ] = P [ X | X j ∀ j s.t. Rj ∈ η ( Ri ) ]. 
One of the advantages of a MRF model is that under some very mild as-
sumption there exist a functional form for the probability distribution function, 
namely the Gibbs distribution. This is established by the Hammersley-Clifford
theorem.
DEFINITION 3.2 (CLIQUES)
A clique c is a subset of the nodes of G (namely 
R) such that every pair of distinct nodes in c are neighbors. 
THEOREM
3.1
( HAMMERSLEY-CLIFFORD)
Assume that X has finite con-
figurations over the sample space s, and that P [ X = 0 ] > 0, then X is a MRF 
with respect to a neighborhood η if and only if X is Gibbs distributed, namely, 
(3.1)
where, x is a realization of X, Z is a normalization constant commonly re-
ferred to as the partition function and is given by 
all config. x
U ( x ) is referred to as the Gibbs energy function with 
(3.2)
(a)
(b)

MRF Framework for Image Interpretation
37
where C ( G, h ) denotes the set of all cliques of the graph G under the neigh-
borhood η, and Vc ( xc ) is the clique potential, and xc is the value of the node
variables for those nodes appearing in clique c. 
Proof: See Appendix B.
1.0.2
Notion of Cliques
list below the cliques corresponding to two nodes. 
To get a feel for the notion of cliques consider the RAG in Figure 3.1. We
Node
1-node clique
2 -node clique 3 - node  clique 
R1
It should also be noted that corresponding to node R 1, { R 1, R 2, R 3, R 4}
cannot be a clique since R 2 and R 3 are not neighbors. Thus the clique function 
Vci will involve the nodes in the clique ci; moreover each cliques function, 
expresses the form and the degree of interaction that the node Ri has with its
neighbors. The clique function takes care of first order interaction (1-node
clique), second order interaction (2-node clique) and so on. 
1.1.
When the problem of image interpretation is restricted to labelling the re-
gions of a segmented image, we can assign a node in R to every segment. The
edge set E is such that node Ri is connected to Rj only if the corresponding 
segments are spatially adjacent. Then the segmented image of Figure 3.1 (a) 
takes the form of the adjacency graph in Figure 3.1 (b).
MRF based Formulation for Interpretation 
Let
■each node Ri have an interpretation from the set I = { I 1, I 2, ⋅ ⋅ ⋅ IM }.
Then the sample space for each Xi will be { I 1, I 2, ⋅ ⋅ ⋅ I M }. For example, 
the possible set of interpretation could be { I 1 = grassland, I2 = road, 
I 3 = sky, I 4 = buildings, I 5 = carr}, then Xi would take a value from the 
set { I 1, I 2, I 3,I 4, I 5} corresponding to the five possible interpretations. 
■K be the domain knowledge or a priori knowledge regarding the various
interpretation, obtained from the set of training images. For example, if 
we consider features like average gray level for sky the grassland, and the 
compactness for the grassland, then K would include the nominal values 
for these features. Thus, characterizing K would imply computing nominal 
values for all features which are considered important for deciding on an 
{R1}
R5
{R5}
{R1,R2}
{R1,R4}
{R1,R3}
{R5,R4}
{R 1,R 2,R4}
{R1,R3,R 4}
—

38
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
interpretation. These features could be based gray level or shape character- 
istics on 1-node cliques (example: average gray level, area), 2-node cliques 
(mutual contrast, common boundary length), or multiple node cliques. For 
more on different type of features please see Appendix G. 
In order to obtain the domain knowledge it is assumed that a segmented 
image for each training image is available. 
At this point we would like to mention that selection of features is in a 
sense a knowledge representation problem. What would like is a way of 
characterizing and represent knowledge such that it would be optimal for 
interpretation. As far as we know, in general, this problem is unsolved. 
Thus, in the absence of any other method, the approach has been to create 
a set of features and then validate its selection through empirical studies. 
The creation of this set is based on our understanding on what we use for 
interpretation and some studies in psychophysics of vision. 
■
the set of feature measurements on the observed image be
where
is the set of q features measured on the clique c. Once again the q features 
would be the same as those used in the domain knowledge K, and that 
segmentation on the observed image is already achieved. 
Next, we assume that the probability distribution of the interpretation ran- 
dom vector X defined on the RAG G given the domain knowledge K and the 
set of feature measurements on the observed image is a MRF, namely, 
(3.3)
Now the image interpretation problem is solved as a maximum a posteriori 
estimation (MAP) estimation problem, namely 
Or equivalently, 
The fundamental question at this point is 
How to select the clique functions? 
(3.4)

MRF Framework for Image Interpretation
1.2.
Selection of Cliques Functions
clique functions: 
39
Modestino and Zhang [58] propose the following rule for the selection of
If the interpretation of a regions in a clique tends to be consistent with
the feature measurements and domain knowledge, the clique function
decreases; otherwise the clique function increases.
It should be obvious that a decrease in clique function will result in an in-
crease in the probability (3.4), and correspondingly it aids in achieving the
MAP solution. They specify separate clique functions for 1-node cliques and
multiple node cliques. 
1.2.1
1-node Clique Functions
These are specified by 
(3.5)
where c is a 1-node clique, q are the number of 1-node features, Bic is the
basis function corresponding to the 1-node clique c and the feature i, and the
weight wic selected such that ∑ q
i =1 wic = 1. Most often wic is chosen to be 1 / q
implying that all features are equally important. 
To get an idea as how the basis function are constructed, consider an exam-
ple:
Interpretation:
Car
Feature:
Average gray level 
Domain Knowledge: 
Average gray level ~~150
Then we can have a basis function as shown Figure 3.2. Since the average 
gray level is approximately 150, we give a value of zero for B avg-gray-level
car
for average gray level for the car, in the observed image, to be in the range 
145 to 155. Outside this range the basis function gradually increases. Another 
possible form of the basis function is 

40
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
1.0
0.0
Bc (road ; KΩ= 70, avg gray level)
Figure 3.2. 
1-node Basis function. 
Figure 3.3. 
Table 5. 2 ,Chapter 5). Here, a = 60, b = 65, c = 75, d = 80 
Example of a basis function used for the average gray level feature for road (see 
and is characterized by four parameters, which are determined heuristically 
based on the domain knowledge K. It is to be noted that the basis function is 
very much like the logic function used in fuzzy systems [ 103]. 
An alternate form of the basis function is based on variation of the sigmoidal 
function,
In case the minimization algorithm for the MAP estimation problem re-
quires mathematical differentiation then the above form will be preferable. 
Note, that the above form is characterized by only one parameter β, this could 
also be another point in favor of sigmoidal form. 

MRF Framework for Image Interpretation 
41
Clique
Domain Knowledge 
{car,road}
acceptable combination 
{car,sky)
unacceptable combination 
Sigmoidal 
variation 
for beta=1, 
beta=0.01 
Sc
0
I
Bic(ξ 
Figure 3.4. 
Basis function - variation of sigmoidal. 
1.2.2 
Multiple Node Clique Function 
The construction of multiple node clique function is similar to single node 
clique function except that an additional term is added to take care of spatial 
constraints.
(3.6)
Here c represents multiple node cliques, for example c = { R k , Rl } for 
2-node cliques. qm is the number of multiple node features, for example i
could be mutual contrast. The basis function Bic has the same form as the 
basis function for a 1-node clique. The second term, Sc, takes care of spatial 
constraints.
Typically, the first term represents fuzzy constraints among nodes, while 
the second term represents hard spatial constraints among the nodes. As an 
illustration of this, consider the case of the following two 2-node cliques : 
Note, a zero value for clique function gives a higher probability and there- 
fore for an acceptable combination we have a value of 0 for Sc. Thus we can 
define the hard spatial constraint as 

42
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
if c is an acceptable combination 
according to K
if c is an unacceptable combination 
according K
1.2.3 
Minimization Algorithm 
interpretation problem can now be written as 
Having constructed the clique functions, the MAP (3.4) formulation of the 
(3.7)
In general the expression inside the square brackets will not be convex and 
therefore will have local minima and possibly multiple minima. Thus, a need 
for an algorithm which assures a global minimum. Now in a typical inter- 
pretation problem the sample space of the interpretation random variables is 
finite, and therefore the above problem reduces to a combinatoral minimiza-
tion problem. Consequently, the most commonly uses algorithm is the simu-
lated annealing (SA) algorithm ([80], [79]) because it guarantees convergence 
in probability to the set of globally minimal solutions. Nevertheless, one could 
use some other algorithm, for example the genetic algorithm [104]. 

Chapter 4 
BAYESIAN NET APPROACH TO 
IMAGE INTERPRETATION 
The problem of image interpretation is one of inference with the help of 
domain knowledge. As described in earlier chapters it is best formulated in a 
probabilistic sense. Hence, the solution to this problem can be described as 
probabilistic expert system. The problem of image interpretation has been for-
mulated as the Maximum a posteriori (MAP) estimate of a Markov Random 
Field (MRF). Bayesian networks have been used by researchers in decision 
analysis and artificial intelligence to build probabilistic expert systems. In this 
chapter we show how the MRF model allows the construction of a relatively 
simple, singly connected, two-layered Bayesian network for image interpreta-
tion. Although this problem can be solved deterministically, a stochastic relax-
ation scheme for finding the MAP estimate of the interpretations is described. 
1. Introduction 
Early work in image interpretation was an attempt at mere classification us-
ing features [2,105,106]. But recent image interpretation systems are knowledge-
based (expert) systems which make use of domain knowledge and spatial con-
straints for inference. A probabilistic approach to image interpretation that is 
domain knowledge independent was attempted by Modestino and Zhang [58]. 
Their approach was to model the segments of an image as the nodes of an 
adjacency graph and the labels associated with the nodes as a MRF over the 
graph (Section 1, Chapter 3). They used simulated annealing [82] to find the 
MAP estimate of the MRF which corresponds to the best interpretation given 
the features and the spatial constraints. 

44
BAYESIAN APPROACH TO IMAGE INTERPRETATION
Modestino and Zhang,s model suggests that we can formulate the image in-
terpretation problem as one of probabilistic reasoning. Researchers in the area
of artificial intelligence (AI) have used Bayesian networks for building expert
systems in fields like medical diagnosis where one is required to reason about
the probable diseases given the symptoms. In this chapter we present the use of
Bayesian networks for image interpretation. We show how a simple Bayesian
network can be used to model the interpretations and feature measurements.
Gibbs sampling followed by simulated annealing is used to relax the network
to an optimum set of interpretations.
This chapter is organized as follows.
In Section 2 a detailed description
of Bayesian networks is given.
In Section 4.1 we describe the dependence
semantics of a Bayesian network and also discuss schemes for the updating of
probabilities given some evidence. In Section 5.1 we discuss the construction
of a Bayesian network for the purpose of image interpretation. In Section 6 we
describe some experimental results on a real image.
2.
MRFmodel leading toBayesian Network Formulation
Let I = { I1, I 2, ⋅ ⋅ ⋅ , I n} represent the set of interpretations that can be
given to the nodes R = { R 1, R 2, ⋅ ⋅ ⋅ , R n} of the graph G.
Assume that
each of the random variable I takes a value for a finite sample space L =
{ L 1, L 2, ⋅ ⋅ ⋅ L M }. L represents the set of labels that we would like to give
to R.
Let the set of feature measurements made over the cliques of G be
F(R) = {FC|∀ c ∈ C(G, η )}, where FC = {F1
c, F c
2, ⋅ ⋅ ⋅ Fqc}. As seen earlier
Fic represents an individual feature measurement made on the clique c. Let
K denote the domain knowledge. The image labelling problem can then be
formulated as the following optimization problem : Given R, find i* such that 
where P [ . | ., .] is the aposteiori pdf of the interpretation (I) given the domain
knowledge ( K ) and the feature measurements ( F ). Using Theorem 3.1 due to
Hammeresley-Clifford, we have,
(4.1)
(4.2)
where
where ic = i | c, namely i restricted to the clique c. The set of cliques can
be partitioned into sets Cτ of cliques with τ nodes in each clique. For a planar
graph τ = 1,...,4 . If we set 

Bayesian Net approach to Interpretation
where Zc is a normalizing constant. Now we can express (4.1) as
45
where β1 is a constant. Bayes theorem allows us to write
(4.3)
(4.4)
where P [., .] denotes the apriori pdf.
It is reasonable to consider the individual feature measurements Fci to be 
independent given the interpretations and the domain knowledge. Hence, we
can write 
Also assuming that the a priori distributions of the interpretations and the fea-
ture measurements and the domain knowledge are constant we can write (4.4)
as
(4.5)
where α c is a constant. Substituting (4.5) in (4.3) we get
(4.6)
Since P [ I, F , K ] = P[ I | F ,K ] P [ F,K ] using (4.6) we can write the joint
distribution function of the interpretations and the feature measurements as the 
product of a number of conditionals. So 
(4.7)
where β is a constant which depends upon the a priori distributions of the in-
terpretations and the feature measurements. This aspect is utilized in bringing 
out the equivalence between a particular Bayesian network and the MRF. 
3.
Bayesian Networks and Probabilistic Inference 
From the discussion in the previous section it is clear that the random vari- 
ables involved in the interpretation problem are not merely those in the set 

46
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Burglary 
Earthquake 
Burglar Alarm 
Figure 4.1. 
A Simple Bayesian Network. 
of interpretation vector I but the feature measurements themselves are vari-
ables whose realization is necessary before any meaningful interpretation is 
attempted. Hence this problem is similar to the one in which based upon cer- 
tain evidence certain other inferences are made probabilistically. A Bayesian 
network is an efficient way of formulating such a problem. We now introduce 
the concept of Bayesian network. 
3.1.
Introduction
systems suffer from disadvantages, namely, 
1 Rules or premises are best suited for representing categorical knowledge. 
For representing uncertain knowledge there is hardly an alternative to the 
language of probability. 
The predominant form of expert systems are rule-based. But rule-based 
2 Rule-based systems can be highly unclear about the underlying dependence 
and independence of propositions. 
Consider the following example. A burglar alarm sounds whenever the fol- 
lowing two events occur: (i) a burglary and (ii) an earthquake. Hence, a bur- 
glary and an earthquake can be considered as the two causes of the event: 
sounding of the burglar alarm (Figure 4.1). Now, as long as we do not know 
whether the burglar alarm has rung or not, the occurrence of an earthquake is 
independent from that of a burglary. However, as soon as we learn that the 
burglar alarm has rung, its two causes become dependent on each other, in the 
sense that the occurrence of a burglary almost precludes the occurrence of an 
earthquake and vice-versa. 

Bayesian Net approach to Interpretation
47
It is precisely to capture such vagaries in the reasoning process that Pearl
[ 107] pioneered the concept of a Bayesian network and put it on a strong math-
ematical foundation. A Bayesian or causal network is a directed acyclic graph
(DAG) G whose nodes are propositional variables (V) and whose directed
edges (E) start from the causes and end on the consequences. A number of
graph theoretic results are necessary for a complete analysis of Bayesian net- 
works. These results are explained in Neapolitan[108]. 
3.2.
DEFINITION 4.1 (PROPOSITIONAL VARIABLE) Given theprobability space
( Ω, f, P ), a
finite propositional variable A is a
function from Ω to a subset of
f containing a finite number of mutually exclusive and exhaustive events.
This definition allows us to view propositional variables as we would view
random variables. Also, we can define the joint and marginal density functions
Let V = { V 1 , V 2, ⋅ ⋅ ⋅ , V m } be a finite set of finite propositional variables de-
fined over the probability space (Ω,  F, P) . Let G (V, E) be a directed acyclic
graph (DAG). For each Vi ∈ V, let V (Vi ) ⊆ V be the set of all parents of Vi
and D (Vi ) be the set of all descendants of Vi. Furthermore, let A (Vi ) ⊆ V be
V - (D (Vi ) U { Vi }), namely, the set of all propositional variables excluding 
V, and V i ’s descendants. Suppose for every W ⊆ A ( Vi ), W and Vi are con-
ditionally independent given U (Vi); that is if ui is a realization of Vi, ui is a
realization of U (Vi ) and w is a realization of W and P [U (Vi ) = ui ] > 0 then
Definition ofa Bayesian Network
of a set of propositional variables.
DEFINITION 4.2 (BAYESIAN NETWORK)
The triplet B = (V, E, P) is called
a Bayesian or causal network. 
The set U (Vi ) is called the set of causes of Vi
3.3.
Joint Probability Distribution on Bayesian Networks 
The following theorem holds for any Bayesian network. 
THEOREM 4.1 (PEARL [107])
Let B = ( V, E, P ) be a Bayesian network 
where V = { V 1, V 2, ⋅ ⋅ ⋅ Vm } and let u = { u1, u2, ⋅ ⋅ ⋅um, } be any realization of 
this network. Then we have, 

48
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
If P [U ( Vi ) = ui ] = 0 for any ui then the term corresponding to that Vi does
not figure in the product. We state without proof the following theorems. 
THEOREM 4.2
Let V be finite set of alternatives.
Let G (V, E ) be a DAG
in which ∀ Vi ∈ V let U (Vi ) ⊆ V be the set of all parents of Vi.
Let a
conditional distribution of Vi be given for every realization of U (Vi ), namely,
Pˆ [Vi = ui|U (Vi ) = ui] is specified for every possible value of ui and ui such
that
Let
Then ( V, E, P ) constitutes a Bayesian network. Furthermore, the specified 
conditionals are indeed conditional probabilities in this Bayesian network for 
all cases where P [ U ( V )] > 0. 
THEOREM 4.3 Let B = ( V, E, P ) be a Bayesian network Then there is a 
unique Bayesian network B' = (V, E', P ), where E' ⊆ E, with the same joint
probability distribution, in which the parent sets are minimal. 
THEOREM 4.4
Let P be the joint probability distribution of afinite set V of 
propositional variables. Then there is some Bayesian network B = ( V, E, P )
containing P. 
3.4. 
Properties of Bayesian Networks 
above theorems. 
We can deduce the following properties of Bayesian networks from the 
1 A Bayesian network by definition contains a unique DAG and a unique 
pdf on the variables of the DAG. Hence, it also determines the conditional 
probabilities of variables given their causes. As Theorem 4.1 states the joint 
distribution may be retrieved from the conditionals alone. 
2 As Theorem 4.2 states, every DAG whose vertices are propositional vari-
ables, together with a set of conditional probabilities of the variables given 
their causes, uniquely determine a Bayesian network. 
3 A joint distribution of a set of propositional variables does not uniquely 
determine a Bayesian network. However, Theorem 4.4 states that there is 
at least one network which contains the joint distribution. 
4 A DAG whose vertices are propositional variables, along with their joint 
distribution is not necessarily a Bayesian network. However, if we deter- 
~~
~

Bayesian Net approach to Interpretation
49
mine the conditionals from the joint distribution, then they form a Bayesian
network. But the joint distribution of this Bayesian network is not the same
as the original distribution.
4.
Before we consider
the updating of probability in Bayesian network, it
is worthwhile to see if the Bayesian networks are clear about the underlying
dependence and independence of the propositional variables that make up the
network.
4.1.
The Dependence Semantics ofBayesian Networks
The problem of determining exactly what independencies are implicit in the
structure of a Bayesian network was pursued by Pearl and Verma [108]. Their
fundamental result on D-separation is explained below. Let G (V, E ) be a DAG.
Let W ⊆ V and X and Y be vertices in V – W.
DEFINITION 4.3 A chain ρ , between X and Y is blocked by W if one of the
following is true
Probability Updating inBayesian Networks
1 ∃ a vertex Wi ∈ W such that the arcs which determine that Wi is on ρ, 
meet tail to tail (
→ 
o → ) at Wi.
2 ∃ a vertex Wi ∈ W such that the arcs which determine that Wi is on ρ, 
3 ∃ a vertex Z ∈ V, for which Z and none of Z's descendants are in W, on
the chain ρ such that the arcs which determine that Z is on ρ, meet head to
head ( → o → ) at Z.
meet head to tail (
→ 
o → ) at Wi.
DEFINITION 4.4
X and Y are D-separated by W if every chain between
them is blocked by W.
Let W ¹ , W 2, W 3 be disjoint subsets of V.
DEFINITION 4.5
W 1 and W 2 are D-separated by W 3 ∀ X ∈ W1 and
Y ∈ W 2, X and Y are D-separated by W 3.
We state without proof the following theorem due to Verma and Pearl [108].
THEOREM 4.5 (VERMA AND PEARL)
Let B = (v, E, P) be Bayesian net-
work. Let X, Y and Z be subsets of V such that X and Y are D-separated
by Z Then, 
That is X and Y are conditionally independent given Z. 
P [ X,Y | Z ] = P [ X | Z ] P [ Y | Z ] 

50
BAYESIAN APPROACH TO IMAGE INTERPRETATION
Consider the example of the burglar alarm mentioned in the last section. From
Figure 4.1, it may be seen that a chain from burglary to earthquake meet head
to head at burglar alarm. Hence, from Theorem 4.5 it is clear that the variables
associated with burglary and earthquake are independent only so long as it is
not known whether the burglar alarm has rung or not. This is exactly as we
would expect. This shows that the Bayesian network is capable of representing
dependencies that match with our intuitive notions.
4.2.
Since we intend to apply a Bayesian network for reasoning, we need to
compute the probabilities of certain propositions in a Bayesian network, given
some evidence. The evidence is specified in the form of instantiations of cer-
tain other propositions.
A Bayesian network is completely specified by specifying the DAG and the
conditional probabilities, namely, the a priori probabilities of the roots and the
conditional probabilities of other variables given their causes. With these the
probabilities of every propositional variable can be computed. With the arrival
of new evidence in the form of instantiations of certain nodes, the probabilities
of other nodes can be updated, thus increasing our capacity to reason about
them. This process is called evidential reasoning.
Probability updating in Bayesian networks has been approached differently
by different researchers. Pearl has developed a propagation scheme for finding
the a posteriori probabilities in a singly-connected Bayesian network
(Pearl
[ 107], Neapolitan [ 108]). For non-singly connected networks Lauritzen and
Spiegelhalter [ 109] have developed a method of probability propagation in
trees of cliques. However, for computational reasons it is imperative that ran-
domized algorithms for probability updating be developed.
One such tech-
nique proposed by Pearl [ 110] is that of stochastic simulation.
4.3.
THEOREM 4.6 (PEARL[110])
Let Vi be any variable in a Bayesian network
( V,E,P ). Let U ( Vi ) be the set of Vi’s causes. Let Y ( Vi ) = { Y 1( Vi ) , Y 2 ( Vi ),
⋅ ⋅ ⋅ YL ( Vi } be the set of Vi ’s children. Let Fj be the set ofparents of Yj. Let
Wvi be the set V - { Vi }. Then,
Different Schemes for Updating Probabilities
Evidential Reasoning using Stochastic Simulation
where α is a constant. (Proof in Appendix E). 
The technique used by Pearl is to instantiate the entire network such that it 
is consistent with the evidence provided. The pdf of any particular variable 

Bayesian Net approach to Interpretation 
51
Vi, given all the other variables is computed using the result of Theorem 4.6. 
Now, once Vi’s pdf is known, Vi’s value is changed using Gibbs sampling
(Geman and Geman [79]). This process is repeated for every variable that is 
not a part of the evidence. In successive visits to the same node, the new pdf 
is computed by averaging over all previously computed pdf’s. As this process 
is continued indefinitely the pdf of each variable tends to its actual pdf given 
the evidence. However, this technique has the disadvantage that it is efficient 
only for computing the marginal distributions of variables given the evidence. 
In order to compute the joint distribution Pearl suggests that we keep track 
of all the realizations and their relative frequency. This is clearly inefficient. 
However, as we shall see, there exists a simple method of getting over this 
problem using relaxation, in the case of a Gibbsian pdf. 
4.4. 
Computational Considerations in Probability 
Updating
As long as a Bayesian network is singly connected the probability updating 
scheme of Pearl [ 107] and Lauritzen and Spiegelhalter [ 109] gives polynomial 
time solutions to the problem of determining the a posteriori probabilities.
Cooper [ 111] has shown that the problem of determining the probabilities of 
the remaining variables given that certain variables are known is NP-hard for 
the class of non-singly connected Bayesian networks. Hence, it is unlikely 
that we would develop an efficient method for propagating probabilities in an 
arbitrary non-singly connected network. As a result, a number of random-
ized schemes for approximately computing the a posteriori probabilities have 
been proposed. Besides the scheme of Pearl [ 110], Chavez and Cooper [ 112] 
have suggested a randomized approximate scheme (ras). They compute a pri-
ori bounds on the running time by analyzing the structure and contents of 
the Bayesian networks. However, they also show that straight simulation re- 
quires exponential time when the conditionals approach 0 and 1. Dagum and 
Luby [ 113] have shown that even approximating probabilistic inference with 
absolute bounds is NP-hard for the general class of Bayesian networks. They 
suggest a restrictive topology of networks or control over the inference al-
gorithm as an alternative to overcoming the computational complexity of the 
Bayesian networks. 
5.
Bayesian Networks for Gibbsian Image Interpretation 
Equation (4.7) in Section 2, suggests that the joint probability distribution of 
the interpretations and feature measurements can be specified by the product of 
the conditional probabilities of the individual feature measurements given the 

52
BAYESIANAPPROACHTOIMAGEINTERPRETATION
Figure 4.2. 
Network used for image interpretation. 
interpretations. Also, Theorem 4.4 suggests that there exits a Bayesian network 
that contains the joint pdf of the interpretations and the feature measurements. 
In light of (4.7), a Bayesian network for containing this joint distribution sug-
gests itself. 
5.1. 
Construction of a Suitable Network 
In Figure 4.2 we have represented the interpretations as the roots and the 
features of the segments as the children of the interpretations. Clearly the 
nodes with only one parent are feature measurements over singleton cliques 
and those with two parents are feature measurements over cliques with two 
nodes and so on. For the sake of clarity we have not shown those features over 
set of interpretations of an MRF over the graph G and F = { F c| c ∈ C(G,η ) }, 
the set of feature measurements over the set of cliques C with the neighborhood 
system η where each F
C = { F1c,F c
2 ,⋅ ⋅ ⋅ F cq} is the set of individual feature 
measurements over the clique c. Then, from Theorem 4.1 the joint probability 
distribution of the interpretations and the feature measurements is given by 
(4.8)
where I ( F c
j ) is the set of parents of F c
j . Since the a priori probabilities of the 
interpretations can be considered as constants, then it is clear that Equation 4.8 
is essentially the same as (4.7). Hence, we have constructed a Bayesian net- 
work with the same pdf as the MRF considered by Modestino and Zhang [58]. 
We shall call this network as the interpretation network. 
cliques containing 
more than two nodes. If I={I1, Ι2,⋅⋅⋅ 
In} represents the 

Bayesian Net approach to Interpretation 
5.2.
It is clear that the Bayesian network constructed in the previous section sat-
isfies the assumptions made about the problem in Section 2, for any chain from 
one feature to another meets tail to tail at some interpretation variable. Hence, 
the feature variables are D-separated from each other by the interpretation vari- 
ables. Theorem 4.5 ensures that the feature measurements are independent 
given the interpretations. 
We also see that any chain from one interpretation variable to another meets 
head to head at features which correspond to those defined on cliques con- 
taining two or more nodes. Unless the segments corresponding to two inter- 
pretation variables are adjacent, any chain from one interpretation variable to 
another meets head to tail at another interpretation variable. This means that 
a priori all the interpretation variables are D-separated and hence independent 
from each other and once the features are all known, the interpretation variables 
are conditionally independent of all those interpretations that lie beyond a cer- 
tain neighborhood. This neighborhood η ( Ii ), of the interpretation Ii, consists
of all those interpretations with whom they share children or in other words 
are spatially adjacent in the adjacency graph of the interpretations. This can be 
expressed as follows: 
53
Dependencies in the Interpretation Network 
Hence a posteriori the interpretations form an MRF where the neighborhood 
is the same as the one on an adjacency graph of the segmented image as we 
would expect from (4.8). It is also seen that the Bayesian network in Figure 
4.2 not only incorporates the original MRF of the interpretations but is itself 
an MRF but with a neighborhood given by the set of its descendants, parents, 
and all the parents of its descendants which are not in the set of its descendants 
and parents. 
5.3. 
Relaxation Scheme with Gibbsian Distributions 
The procedure for image interpretation now boils down to instantiating the 
feature variables with the extracted feature values and finding the MAP esti-
mate of the interpretations. This can be done by the method detailed in the last 
section due to Pearl [110]. However, since it deals with keeping track of the 
relative frequency of all previous configurations of the interpretation variables, 
it requires a large amount of storage space and hence is inhibitive. Since we 
now know that the pdf we deal with is Gibbsian, we can do away with this pro- 
cedure and use simulated annealing to relax the network to a state of optimum 
set of interpretations. This method is explained below. 

54
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Let Pv be the pdf of a variable V computed at some stage in the process
detailed in the last section. Let 
.
Let the Gibbs sampler sample from the distribution Pvˆ
instead of Pv. If
Pv is Gibbsian and the temperature T is reduced sufficiently slowly, then the 
joint pdf of all the variables in the perturbation tends to the distribution that is 
uniform over those realizations that maximize P [ V ] and zero over all others. 
Hence, we would relax to the state of optimum interpretations. 
6. 
Experimental Results 
The efficacy of the Bayesian network based approach depends upon several 
factors, including validity of the model, the quality of the segmentation and 
feature extractions and how powerful the knowledge base is in bringing out the 
conditional dependencies of the features on the regions or objects in the image. 
6.1. 
Simulation on Real Images 
With real-life images it is necessary to have a good segmentation before a 
Bayesian network can be constructed for identifying regions. An aerial im- 
age was segmented manually, using an interactive image processing software. 
Based on the work of Kim and Yang [59] the following features were extracted 
1 Area of the segment As. This was specified by the number of pixels in the 
2 Average gray level of the segment. 
3 Average texture density of the segment. This was computed by the relation 
segment.
where fi is the pixel value of the ith pixel and N ( i) is the set of 8 pixels in 
the neighborhood of the ith pixel.
4 Contrast between two segments. This was specified as the difference be-
6.2. 
Obtaining the Conditionals 
The method requires the specification of the conditional probabilities of fea- 
tures given the interpretations. This was done by considering only the class 
tween the average gray levels of the two segments. 

Bayesian Net approach to Interpretation
55
of aerial images and the objects identifiable from the air. Hence, the condi-
tional probabilities were determined heuristically by studying an aerial image 
for the following objects: road, grassland, foliage, vehicle (car), building, and 
shadow. Note that the conditionals were extracted from a part of the image 
that was not subject to interpretation. After the image had been segmented 
manually, several blocks of size 8 × 8 were placed randomly all over the im-
age such that each block would lie completely in one segment. For each block 
the average gray level, average texture density and the number of pixels of the 
block that did not figure in any other previously visited block were computed. 
For each identifiable object (or each pair of objects in case of contrast) in the 
image an histogram was prepared with the feature level on the horizontal axis 
and the frequency of occurrence of the feature level (taken to be the number 
of blocks that showed up that feature value) for that object on the vertical axis. 
To represent the conditional distributions for average gray level the gray levels 
considered were discretized conveniently. It was found using the interactive 
image processing software that the images had gray levels only in certain nar- 
row regions. Consequently the range over which gray levels were to be found 
was discretized into a number of levels (16 for the case of the image in Figure 
4.4). This was also done for the case of texture and area levels (10 and 16 
levels respectively for the case of the image in Figure 4.4). This histogram was 
used as an approximation for our conditional distribution of features given the 
interpretations. Since several feature values threw up the value of zero (which 
is inadmissible in the conditionals of a Bayesian network) we pegged small 
values to the histogram for all these features and normalized the entire his- 
togram to obtain our operational conditional pdf. Some of the histograms that 
were obtained for the image in Figure 4.4 are shown in Figure 4.3. They are 
self explanatory. 
Only a small portion of the test image was actually subject to the interpre-
tation scheme. The image in Figure 4.4 (a) has all the objects except the river 
that we set out to identify. The image of Figure 4.4 (b) has 16 segments and 
it can be seen that the method has identified all the segments correctly (Figure 
4.4 (c)). 
7. 
Conclusions 
The technique for maximizing the a posteriori distribution of an MRF to 
find the optimum set of interpretations uses a heuristic design rule for obtaining 
the clique functions. One of the disadvantages of this approach is that these 
clique functions can not be identified with intuitive dependencies. This renders 
the heuristics suspect. It is much more desirable if we could specify the domain 
knowledge in terms of dependencies instead of clique functions. The technique 
presented in this chapter accomplishes exactly this. The domain knowledge 
can be reliably constructed from the intuitive notions only. 

56
BAYESIAN APPROACH TO IMAGE INTERPRETATION
0.3
0.25
0.2
P[x-Foliage]
0.15
0.1
0.05
0
Gray Level
0.25
0.2
P[x-Building]
0.15
0.1
0.05
0
2
4
6
8
10
12
14
16
Gray Level
(b)
0 
2 
4 
6 
8
10
12 
14 
16 

Bayesian Net approach to Interpretation 
57
0.3
0.25
0.2
P[x-Road]
0.15
0.1
0.05
0-
7
2 
4 
6 
8 
10 
12 
14 
16 
Area
(c)
0.3
0.25
0.2
P[x-Foliage]
0.15
0.1
0.05
0 1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
Texture
(d)
Figure 4.3.
Histograms obtained for some Conditional Distributions. 

58
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Figure 4.4. 
for the interpreted image. 
(a) Original Image, (b) Segmented Image, (c) Image after interpretation, (d) Key 

Chapter 5 
JOINT SEGMENTATION AND 
IMAGE INTERPRETATION 
1. 
Introduction 
Though considerable amount of work has been done in the area of image 
interpretation one is still on the lookout for a fully automated image interpre- 
tation scheme. Automatic scene interpretation requires the construction of at 
least a partial description of the original environment, rather than a description 
of the image itself. It involves, not only labelling certain regions in an image, 
or locating a single object in the viewed scene, but often requires a 3D model 
of the surroundings, with associated identification in the 2D image. 
For high-level interpretation, the principle unit of information is a symbolic 
description of an object, or a set of image events, sometimes referred to as 
symbolic tokens, extracted from the image. The description includes relation- 
ships to other 2D symbolic tokens extracted from the sensory data, such as 
lines, segments and other objects in the 3D scene being viewed. It also in- 
cludes pointers to elements of general knowledge that has been used to support 
the interpretation process. 
As shown earlier in Figure 1.3, the task of image interpretation would es- 
sentially involve the task of segmenting (not shown in Figure 1.3) the image to 
produce regions which have some relation to the objects in the scene and then 
using some a priori knowledge to interpret, regions in the segmented image. 
The image interpretation task is very much dependent on the a priori knowl-
edge, in the sense, knowledge acquired from an image should bear resemblance
to the image that is being interpreted. In other words, the knowledge must have 
been acquired from an image which belongs to the same class as the image that 
is being interpreted. 

60
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
NOTE 5.1 In literature, this aspect of image interpretation has either been 
taken for granted or not addressed at all; in this sense the task of image inter-
pretation is nowhere close to being called fully automated. 
The need for image interpretation can be found in many diverse fields of 
science and engineering (see Section 3, Chapter 1). Traditionally, the task 
of image interpretation is performed by experienced human experts. How- 
ever, analyzing a complex image is quite labor intensive, hence, much of the 
research is directed towards constructing automated image interpretation sys-
tems. Recent research in intelligent robots has created yet another need for 
automated image interpretation. In this case, the requirement is to understand 
what the robots see with the imaging sensors to be able to perform intelligent 
task in complex environments. Here, the robots have to rely entirely on auto- 
mated image interpretation. 
As seen in Section 4 of Chapter 1, the main approach in early research in 
image interpretation was that of classification, in which, isolated image primi- 
tives were classified into a finite set of object classes according to their feature 
measurements. However, since low-level processing often produces erroneous 
or incomplete primitives, and the noise in the image may cause measurement 
errors in the features, the performance of the image interpretation systems us-
ing the classification approach is quite limited and prone to mistakes. The 
main problem here is that the rich knowledge in the spatial relationships that 
the human expert use is not used in the process of image interpretation. 
In this chapter, we propose a scheme for joint segmentation and image in- 
terpretation in a multiresolution framework. Unlike earlier work in multires- 
olution interpretation [16] we do not assume a priori, the availability of the 
segmented image. In fact, in our approach, segmentation and interpretation 
processes or modules are interleaved (modular integration) as shown in Figure 
5.1 and the two operations are carried out at each resolution of a multiresolu-
tion pyramid the idea being that the two operations while synegetically inte-
grating, help each other to perform better. The segmentation module helps the 
interpretation module which in turn helps the segmentation module. 
2. 
Image Interpretation using Integration 
The problem of image interpretation would essentially involve the low-level
vision task of image segmentation to produce regions in the given image corre- 
sponding to some objects in the scene and then giving some labels or interpre-
tation to the segmented regions based on some a priori knowledge. Table 5.1, 
shows the task of image interpretation in the framework of modular integration 
[ 114]. In general, one is given an image which is a projection of a 3D scene 
onto the 2D plane and some knowledge about the 3D environment. From the 

Joint Segmentation and Image Interpretation
61
Figure 5.1.
Macro-level joint segmentation and image interpretation scheme.
(ii) image 
based features, and 
spatial constraints 
Table 5.1.
Image interpretation using integration.
2D image we need to segment the image and interpret the regions based on 
the segmented image. This is shown in Figure 5.1, except for the fact that the 
portions corresponding to wavelet transform and refine using difference image 
do not come into existence. The task of image interpretation in the framework 
of modular integration and multiresolution can be explicitly stated as: 
Given the image Y Ω which is a projection of a 3D scene onto the 2D
plane at thefinest resolution Ω defined over the 2D lattice of size 2Ω ×
2
Ω
 , and some knowledge K about the 3D environment. The problem of
interpretation involves 
1 segmenting the image Y
Ωto obtain sY Ωand
Modules
Feature
knowledge
Integrated
F
K
(i) segmentation 
gray 
level 
nominal values of 
scene based 
features
interpretation
gray level and shape 

62
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Figure 5.2,
Wavelet Transform representation of Y Ω.
2 interpreting the image Y
Ω , based on the segmented image
sY
Ω and
the domain knowledge K 
We now formulate the problem of image interpretation by synergistically 
integrating both the segmentation and the interpretation modules in a mul- 
tiresolution framework. We term this procedure of interleaving segmentation 
and interpretation procedures as joint segmentation and interpretation scheme.
The idea of integrating these two operations is two fold (i) both segmentation 
and interpretation modules by themselves do not work efficiently because a 
good segmented image helps the interpretation module perform better and to 
get a good segmentation, knowledge of the scene, or in other words the inter- 
pretation of the scene is essential, and (ii) we end up getting as a byproduct 
a better segmented image in addition to a correctly interpreted image. The
idea of formulating this problem in a multiresolution framework is to speed up 
computation as discussed in Section 3 of Chapter 2. 
NOTE 5.2 Experimental results show that we need not work on the whole 
image but could stop at one level coarser resolution while interpreting, namely 
ifwe need to interpret a 256 × 256
image it is enough if we interpret a 128 ×128
image.
We construct the wavelet transform of the image Y Ω [94] which results in 
Y Ω-1 =D
Ω-1
Y,LL,
the low pass filtered image and D
Ω-1
Y,HL,
DΩ-1
Y,LH,
D Ω-1 the
Y,HH
difference image, each of size 2
(Ω-1) × 2(Ω-1) . Figure 5.2 shows the wavelet 
transformed structure of Y
Ω, where D
Ω-1
Y,HL (D
Ω-1
Y,LH ) corresponds to the differ-
ence image obtained when Y
Ωis filtered by a high pass filter along the rows
(columns) and by a low pass filter along the columns (rows). The low pass 
filtered image Y
Ω–1 is segmented using any segmentation algorithm. In this
chapter for the purpose of simulations we have used the k-means clustering 
algorithm (see Appendix F) to produce a crude segmented image. The seg-

Joint Segmentation and Image Interpretation
63
mented image is refined using the difference image (D
Ω-1
Y,HL
D
Ω-1
Y,HL ,
D
Ω-1
Y HL,
as described in Section 3. 
The segmented image is subjected to interpretation. The problem of image 
interpretation is formulated in a MRF framework along the lines described in 
Section 1.1 of Chapter 3 (similar to the formulation of Modestino and Zhang
[58]) except that we have a provision for a no-interpretation label L0.
The
reason for having no-interpretation label, as a possible label, is to refine the 
segmented image before further interpretation can be carried out. The process 
of, interpretation, merging of the no-interpretation labels to produce a better
segmented image and again interpretation, is carried out until none of the re-
gions have label no-interpretation (see Figure 5.1). The resulting segmented 
image is assumed to be the final segmented image and final interpretation is 
carried out on it. 
At each resolution (say k ) let the segmented image (Figure 3.1) be repre-
sented as an undirected simple planar graph. The nodes (R
k
1, R
k
2 , ⋅ ⋅ ⋅Rkn )
rep-
resenting the n regions of the segmented image at resolution k. Let the inter-
pretation I
k
j be a random variable associated with the region Rkj
j = 1, ⋅ ⋅ ⋅ , n,
and Ijk takes a value from the label set Lk∆=
Lk
0
,
Lk
1
,
Lk
2 ⋅ ⋅ ⋅
Lkm where L k
0
Lk
1, Lk
2 , ⋅ ⋅, L km arethem + 1 possiblelabels.
NOTE 5.3 Lk
0 is the no-interpretation label, and {Lki }mi =1 represent the m 
possible interpretation labels
Let F k be the feature measurements made on the segmented image sY k and
K k be the domain knowledge at resolution k If we assume that the conditional
probability of I k, given Fk and K k is a Markov random field (MRF) , namely,
(5.1)
Now the image interpretation problem can be posed as an optimization problem 
and the optimal interpretation label vector I k
* is obtained by solving the MAP
estimation problem, namely, 
(5.2)
The problem of interpretation reduces to the problem of minimizing the en- 
ergy function U(Ik; Fk, Kk). The energy function is constructed such that it
takes a minimum value when the interpretation labels are consistent with the 
,
(
,
,
,
{
}
⋅

64
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
knowledge Kk and the feature measurements Fk derived from sYk . The min-
imization of the energy functional U(Ik; Fk, K k) results in interpretation of
the given scene. Now, 
where Vc (⋅; ⋅, ⋅)’s are the clique functions which need to be constructed (see
Section 1.2, Chapter 3). 
NOTE 5.4
The clique function should decrease when the interpretation labels 
are consistent with the domain knowledge and core variables, thus resulting in 
a decrease of the energy function. This means that the interpretation of the 
image that is most consistent with the domain knowledge and core variables 
will have minimum energy. 
3.
The Joint Segmentation and Image Interpretation
Scheme
We present an image interpretation scheme in a multiresolution frame work 
based on integration of the segmentation and the interpretation modules. We 
call the scheme joint segmentation and image interpretation as depicted in 
Figure 5.3. The term joint signifies the integration of the two modules needed 
for image interpretation. The description of the algorithm is based on Figure 
5.3.
NOTE 5.5 For the sake of clarity only two resolutions are pictorially depicted 
in Figure 5.3. 
3.1. 
Image 
Interpretation 
Algorithm 
■
Step 0: Initialization
1 Given: KΩthe a priori knowledge and Y Ω(Figure 5.3 component a),
the scene to be interpreted, defined on a lattice of size 2Ωx 2Ω
DΩ–1
Y,HL ,
DΩ–1
Y,HL
, DΩ–1
Y ,HL , using a wavelet filter (Figure 5.3 component b). 
■
Step I: Segmentation and refining: Obtain sY Ω– k the segmented image
of Y Ω– k =
d e f DΩ– k
Y , LL
usingk-meansclustering algorithm (Figure 5.3 com-
ponentc)andrefinesYΩ–k,using D Ω–N
Y ,H L
,DΩ–N
Y ,H L
,andD Ω–N
Y ,H L
,andusing
a predefined threshold to merge all segments whose area is less than the 
prespecified minimum area, to get sY Ω– k (Figure 5.3 component d).
2     Construct DΩ–1
Y,HL ,
DΩ–1
Y,HL,
DΩ–1
,HL, D Ω–2
Y,HH, DΩ–2
Y,HL
,
DΩ–2
Y,LH , ⋅⋅⋅,DΩ–N
Y,HH ,
1 At the coarsest resolution Ω– N .
Y

Joint Segmentation and Image Interpretation 
65
Figure 5.3. 
Micro-level joint segmentation and image interpretation scheme. 
(a) Construct histogram of Y Ω–N , choose dominant peaks - let there 
be B such peaks. These are the chosen values of the bins in the 
k-means clustering algorithm. 
(b) k-means clustering algorithm will produce, say, some M segments
using the optimality criterion, namely, that the pixels which are 
within a region are as close as possible to the centroid of the region. 

66
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Figure 5.4. 
To describe the process of refining segmentation. 
(c) Refine the regions or segments obtained from Step (I. 1.b) using the 
difference images DY,H H
,
 
D Y,H L
, and DΩ– N
Y,LH , and the minimum
area criteria.
Refinement using the difference image : The refinement procedure 
is best described by looking atFigure 5.4. For example if DΩ– NY
Y, HH is
not zero at the pixel location (i, j) it means that there is a diagonal
edge present at the pixel location (i, j). The presence of an edge
means that pixels (i - 1, j - 1) and (i,j) should not belong to 
the same segment. If the pixels (i - 1, j - 1) and (i, j) belong to 
different clusters they are not touched, else pixel (i - 1, j - 1) is 
assigned a new segment which is not the same as that occupied by 
pixel (i,j). The assignment of the segment label is based on the
nearest neighborhood scheme: pixel (i - 1, j - 1) is assigned to 
that segment whose centroid is closest to the gray level value of the 
pixel (i - 1, j - 1) and excluding the centroid of the segment to 
which pixel (i + 1, j + 1) belongs. In a similar manner refinement is 
done using the difference images DY,LH , and DY ,HL, corresponding 
to the vertical and horizontal edge fields respectively. 
Refinement using the minimum area criterion: Let k be a region 
with an area less than the minimum area and let l, m be regions 
adjacent to region k and having areas greater than the minimum
area. Then the refinement criterion is: 
Ω–N
Ω–N
Ω– N
Ω– N
region k is merged with region l if the difference between
the gray level of the centroids of regions k and l is less 
than the difference between the gray level of the centroids 
of regions k and m.
In other words, if Rq represents region q and G(Rq) represents
the average gray level value of region Rq , then the above criterion 
i-1-j-l
i-1,j
i-1,j+1
i,j-1
i,j
i,j+1
i+1,j-1
i+1,j
i+1,j+1

Joint Segmentation and Image Interpretation 
67
translates as: 
where, the notation Rl ←Ri ∪+
Rk means that region Ri and Rk,
are merged to form a new region Rl.
2 At any resolution Ω- k
(k ≠N) 
(a) Quadtree interpolate the segmented image at (Ω– k – 1) to (Ω – k)
resolution. If sY Ω– k –1
ij
is the (i, j)th pixel of the segmented image
at resolution (Ω– k –1), then
Use this information as the initial segmented image. In addition the 
centroids of each of the regions is also transfered to the next finer 
resolution. These centroids are then used to initialize the bins to be 
used in the k-means clustering algorithm at resolution (Ω– k).
(b) use the interpretation labels obtained from the previous resolution 
Ω– k – 1, to initialize the interpretation labels at this resolution.
(c) Repeat (I.1.b) and (I.1.c) for resolution (Ω – k).
Step II: Interpretation - Segmentation loop
1 Interpretation:
The segments are interpreted using the knowledge 
base K Ω– k, the core variables FΩ– k derived from the segmented im- 
age sY Ω– k and using the assumption that the conditional probability is 
a MRF (5.1). This conditional probability can be minimized using a re- 
laxation algorithm (in our simulations we used the simulated annealing 
algorithm). The energy to be minimized is 
where,
U
and Vc (IΩ– k ; FΩ– k , KΩ– k ) are the clique potentials.
criterion for moving from coarse to fine resolution is: 
2 Recall that {LΩ– k } contains the no-interpretation label .
Thus our
if none of the labels have the label no-interpretation then move
from the present coarse resolution
to the next fine resolution
(Step III). 

68
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
3 Segmentation: In case any segment has the label no-interpretation, we 
merge it with one of the interpreted segments which is adjacent to it, 
depending on a probability criterion. For example: If region j has label 
no-interpretation, that is, IΩ–
j
k
takes the label L0 and if l,m are the
regions adjacent to region j, then region j is assigned the interpretation 
label Ll corresponding to the region l if P[I Ω–
j
k
= LI | F Ω– k; K Ω– k]
> P[IΩ
j
– k = Lm | FΩ– k ; K Ω– k ]. In other words,
This step will output a better segmented image. 
4 Go back to Step 11 (Interpretation-Segmentation loop) 
Step III: Coarse to fine resolution 
1 If not working at Ω – 1 resolution
(a) Interpretation labels at this resolution are transfered to the next
(b) k →k – 1; go back to Step I (segmentation and refining)
2 At resolution Ω– 1:
(a) Output the interpretation labels, 
(b) Output the segmented image 
(c) Quadtree interpolate the segmented and interpreted images to ob-
tain the final segmented and interpreted images at the finest resolu- 
tion Ω(in all our experiments, we have found that it was sufficient 
to stop the algorithm at resolution Ω– 1 instead of segmenting and 
interpreting the image at resolution Ω).
finer resolution (IΩ– k →IΩ– k + 1)
NOTE 5.6
We have assumed the availability of knowledge base Kk at all res-
olution k = 0, 1,⋅⋅⋅Ω
NOTE 5.7 In principle it may be possible to generate the knowledge base 
separately at each resolution. But, knowledge base generation is a user in-
teractive process, which tends to get difficult at coarser resolutions. Thus, we 
prefer to use the rules described in Table H.3, Appendix H for constructing the 
knowledge pyramid. 

Joint Segmentation and Image Interpretation 
69
4. 
Experimental Results 
Experiments were carried out to validate the proposed scheme of joint seg- 
mentation and image interpretation in the framework of modular integration 
and multiresolution. Tests were conducted both on real outdoor (road images) 
and indoor (computer images) images1 of size 256 x 256 which were either
captured using the QuickTake100 digital camera (Figures in Section 4.2) or 
using an aim and shoot Kodak Pro 111 camera and then scanning using the HP 
Color Scanner (Figures in Section 4.3). 
4.1. 
Features Used 
Feature selection is an important aspect of image interpretation. The fea-
tures used in all our experimental work are (Table G.2, Appendix G): (i) Single
node cliques: area (A), perimeter (P), average gray level (G) , the mass center 
(mx , my ), variance (V), compactness (C) and (ii) Two node cliques: contrast
(CR), common perimeter ratio or boundary length (CPR) . 
4.2. 
Road Images 
Figures 5.5a and 5.6a are the original images of the scene to be interpreted 
and 5.5b and 5.6b are the wavelet transformed images of 5.5a and 5.6a respec- 
tively using the 4 tap Daubechies filter coefficients. The D Ω– 1
Y , LL is segmented 
using the k-means clustering algorithm and refined  using
D Ω– 1
Y ,H L DW – 1
Y, L H  and 
D Ω–1
Y ,HH as  described in Section 3 (See Figure 5.2). The resulting image is dis-
played in Figure 5.5c and 5.6c. Figure 5.5d, 5.6d and Figure 5.5e, 5.6e depict 
segmentation and interpretation of the scene at an intermediate stage. The final 
segmented image is shown in Figure 5.5f and 5.6f. The final interpreted image 
is shown in Figure 5.5g and 5.6g. The a priori knowledge (details regarding the 
aquization of knowledge can be found in [115]) that is used for interpreting 
the images in this section is tabulated in Table 5.2. 
Figure 5.5f the segmentation is not perfect, for example the road is in fact 
divided into 2 different segments, but the interpretation block interprets both 
the segments as road and this is seen in Figure 5.5g. This is an indication that 
the segmentation and the interpretation modules cannot work independently, 
they work best when they work synergistically. This aspect of modular inte- 
gration is also seen in Figure 5.6g, where the segments corresponding to sky 
(see Figure 5.6f ) are all merged into a single segment after interpretation. 
Figure 5.7a is the original image of a scene to be interpreted. The interpreta-
tion labels that one is looking for in this image are (i) sky, (ii) tree, (iii) sidewalk 
and (iv) road. Interpretation was done as described in Section 3. Figure 5.7b 
1Available from SPANN Laboratory, Indian Institute of Technology - Bombay, database. 

70
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Tree
Sidewalk
Road
Label 
A 
G 
P 
mx 
my 
Sky 
1052 
180 
204 
63 
12 
56.631790 
3087
64
312
18
38
22.751346
1858
99
496
54
66
18.838804
6193
70
411
66 
101 
7.3232887
Common Perimeter (CP)
Sky
Sky 
Tree 
Sidewalk 
Road 
9 3
 
–
  
–
 
 
–  
Tree
Sidewalk
Road
9 3
 
 
–
 
–
 
4 7 9
 
–
 4 7 9
 
–
 
5 0 8
 
–
 
 
–
 5 0 8
 
–
Table 5.2. 
Knowledge base used for the road images (Section 4.2). 
I,
F → 
→
 {Sc (I, F)}
sky
Sky 
Tree 
Sidewalk 
Road 
1
0
 
1
 
1
Tree
Sidewalk
Road
Figure 5.5. 
(a) Original image of size 256 x 256, (b) wavelet transformed image, (c) initial 
segmentation (after k-means clustering) of size 128 x 128, (d) segmentation at an intermediate 
stage, (e) interpretation at an intermediate stage, (f) final segmentation, and (g) final interpreta-
tion.
1 
0
1
 
0 
1 
0
 
1 
1
1
 
0 
v
1 
0

Joint Segmentation and Image Interpretation 
71
Figure 5.6. 
(a) Original image of size 256 x 256, (b) wavelet transformed image, (c) initial 
segmentation (after k-means clustering) of size 128 x 128, (d) segmentation at an intermediate 
stage, (e) interpretation at an intermediate stage, (f) final segmentation, and (g) final interpreta-
tion.
is the the wavelet transformed images of Figure 5.7a. Figure 5.7c depicts the 
output of Step I of the proposed joint segmentation and image interpretation 
scheme (see Section 3). Figure 5.7d and Figure 5.7e depict segmentation and 
interpretation of the scene at an intermediate stage. The final segmented image 
is shown in Figure 5.7f and the final interpreted image is shown in Figure 5.5g. 
Unlike the earlier experimental results, here the segmented image is perfect 
and so the interpretation module assigns different labels to different segments 
and hence there is no merging of segments. 
4.3. 
Building Image 
Figure 5.8a is the image to be interpreted using the knowledge acquired and 
tabulated in Table 5.3. Observe that we have two interpretation labels (Ta-
ble 5.3) corresponding to the Tree, namely, Tree (Left) and Tree (Right), in 
our knowledge base, this to take care of the fact that tree cover occurs in two 
different shapes and having different gray level variation. Nevertheless, we in- 
terpret either of the labels as Tree in our simulations and hence have a single 
legend (Figure 5.8). A similar observation is true for the interpretation label 

72
BAYESIAN APPROACH TO IMAGE INTERPRETATION
Figure 5.7. 
(a) Original image of size 256 x 256, (b) wavelet transformed image, (c) initial 
segmentation (after k-means clustering) of size 128 x 128, (d) segmentation at an intermediate 
stage, (e) interpretation at an intermediate stage, (f) final segmentation, and (g) final interpreta-
tion.
Build. Figure 5.8b is the wavelet transformed image and 5.8c is the output 
of the k-means segmentation algorithm and Figure 5.8f is the resultant image 
obtained after refining the k-means segmented image using the difference im- 
age information present in Figure 5.8b. Figure 5.8d depicts segmentation and 
Figure 5.8e depicts interpretation of the scene at an intermediate stage. Fig-
ure 5.8g gives the final interpreted image. In this example, the interpretation 
merges segments of sky by assigning them labels corresponding to sky. In fact, 
as seen from Figure 5.8g, there are three different segments from Figure 5.8f 
which have been correctly assigned the same label, sky. 
4.4. 
Computer Images 
Figures 5.9a and 5.10a are images captured in the laboratory using the Pul-
nix CCD camera with zoom. The interpretations that we are looking forward 
for are (i) background, (ii) screen, (iii) screen-frame, (iv) keyboard, (v) shoe-
box and (vi) table-top. Figure 5.9b and 5.10b are the wavelet transformed 
images of Figures 5.9a and 5.10a respectively. Figures 5.9c and 5.10c are the 
output of the k-means segmentation algorithm after refinement of Figures 5.9a 

Joint Segmentation and Image Interpretation 
73
Label
A
G
P
Road
4999
106
395
Sky
7730
208
443
Tree(Left)
1535
85
285
Tree(Right)
550
78
113
Build(Left)
879
137
204
Build(Right)
685
145
127
mx
my
V
64
107
18.334378
64
30
20.975156
37
80
17.957333
117
84
14.810193
24
63
19.207092
108
65
15.920010
Table 5.3. 
Knowledge base used for outdoor building scene (Section 4.3). 
and 5.10a respectively. Figures 5.9f and 5.10f are the final segmentation and 
5.9g and 5.10g are the finaIly interpreted images. An intermediate stage in the 
joint segmentation and interpretation scheme is shown in Figures 5.9d, 5.10d 
(segmentation) and Figures 5.9e, 5.10e (interpretation). The knowledge base 
that was used for the purpose of interpretation is shown in Table 5.4. 
As seen earlier, interpretation helps in refining segmentation and hence pro- 
ducing a better segmentation. 
Figure 5.9f shows the shoebox as three seg-
ments, while after interpretation the three segments of the shoebox get labeled 
as shoebox (see Figure 5.9g) and hence form a single segment. The spot above 
the shoebox looks like a rectangular strip and hence gets labeled as a shoebox, 
though it is because of a shadow formed by a black cloth kept behind the com- 
puter while capturing the image. A similar situation is seen in Figure 5.10g 
where the background (above the keyboard and right of the monitor screen- 
frame) gets labeled as screen because of the shape which looks more like a 
screen. But for these, the labelling is correct in both the examples. 
The three sets of experimental results show that the proposed framework 
of modular integration and multiresolution produces results which can only 
be better than that produced when there were to be no integration between 
the modules (just a feed forward interaction). The multiresolution aspect of 
Common 
Road 
Sky 
Tree 
Perimeter (CP)
(Left)
Road
– 
– 
192
– 
– 
82
Tree (Left) 
192 
82 
–
Sky
Tree (Right) 
89 
–
Build (Right) 
29 
100 
–
–
Build (Left) 
–
151 
94 
Tree
Build
Build
(Right)
(Left)
(Right)
89
–
29
151
100
–
94
–
– 
– 
74
74
–
–
–
–
–
–
I,  F → 
→
{ S c ( I,F )
Road
Sky
Tree (Left or right) 
Build (Left or right) 
Road
Sky
Tree (Left or Right) 
Build (Left orRight) 
1 
1 
1 
1 
0
0
0 
0 
1
0
0 
0 
0
1
0 
0 

74
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Figure 5.8. 
(a) Original image of size 256 x 256, (b) wavelet transformed image, (c) initial 
segmentation (after k-means clustering) of size 128 x 128, (d) segmentation at an intermediate 
stage, (e) interpretation at an intermediate stage, (f) final segmentation, and (g) find interpreta-
tion.
the proposed framework not only helps in refining the segmented image, but 
we find that we need not interpret the image at the finest resolution namely 
256 x 256; it is sufficient to stop interpretation at one level coarser namely at 
128 x 128 without affecting the interpretation results. This helps in reducing 
computational cost. 
5. 
Conclusions 
The applicability of the proposed modular integration and multiresolution 
for the high-level vision task of image interpretation has been demonstrated by 
proposing a joint segmentation and image interpretation scheme. The proposed 
scheme has been tested on both indoor and outdoor real images and it is found 
that the scheme is not only capable of interpreting the segments correctly but 
also is able to produce a good segmented image. The main reason for the good 
performance (in terms of correct interpretation) of the proposed scheme lies in 
the synergistic integration of the segmentation and the interpretation modules. 
This supports the belief that integration of different modules is very useful. 

Joint Segmentation and Image Interpretation 
75
Label
A
G 
P 
Background
8334
36 
785 
Screen
1510
38 
158 
Screen-Frame 
1537 
39 
439 
Keyboard
1067
74 
203 
Shoebox
2401
35 
212 
Table-top 
1529 
77 
323 
mx
my 
V 
66 
36 
6.749894
80 
75 
2.292660
86 
78 
3.056080
82 
111 
34.836517 
21 
82 
7.556810
56 
121 
30.613126 
Common 
Background 
Screen 
Perimeter
(CP)
Background 
– 
– 
Screen 
– 
– 
Screen-Frame 
426 
296 
Keyboard 
432 
Shoebox 
422 
Table- top 
– 
– 
–
–
Table 5.4. 
Knowledge base used for the indoor computer images(Section 4.4). 
The use of multiresolution for joint segmentation and image interpretation 
has the following advantages: 
■
the difference image which results as an outcome of multiresolution pyra-
mid, is used to refine the k-means segmented image (See Figure 5.3 part 
d),
■it reduces the computational time
–
by using the interpretation labels estimated at coarse resolution to ini-
tialize the interpretation labels at fine resolution and thus reducing the 
number of iteration for the interpretation module to converge, and 
by actually requiring the interpretation to be carried out on 1/4 of the 
total number of pixels of the image to be interpreted. In all our experi-
ments we found that stopping the interpretation at one resolution below 
the finest resolution, namely stopping atresolution Ω– 1, did not effect
the interpretation results. 
–
Screen-
Keyboard 
Shoebox 
Table-top
Frame
426 
432 
422 
– 
296 
– 
– 
– 
– 
– 
– 
– 
– 
180 
– 
79 
– 
180 
79 
– 
–
– 
– 
I,
F → 
Background 
Screen 
Screen-
→ 
{ Sc(I, F)}
Frame
Background 
1 
1 
0 
Screen 
1 
1 
1 
Screen-Frame 
0 
0 
1 
Keyboard 
0 
1 
1 
Shoebox 
0 
1 
1 
Table-too 
1 
1 
1 
Keyboard 
Shoebox 
Table-top
0 
0 
1 
0 
1 
1 
1 
1 
1 
1 
1 
0 
1 
1 
0 
0 
0 
1

76
BAYESIAN APPROACH TO
IMAGE INTERPRETATION
Figure 5.9
(a) Original image or size 256 x 256, (b) wavelet transtormed image, (c) initial
segmentation (after k-means clustering) of size 128 x 128, (d) segmentation at an intermediate
stage, (e) interpretation at an intermediate stage, (f) final segmentation, and (g) final interpreta-
tion.
NOTE 5.8 The fact that we have worked on images of size which are mul-
tiples of 2 (256 x 256) is no constraint, Appendix D gives an insight on how
pyramids of non-octave scale can be constructed [59, 116]. 
■
Building a knowledge base is an important aspect of image interpretation
and needs to be explored. One could possibly generate knowledge by con- 
structing conditional probabilities from the test images and use the resultant 
density functions instead of what is shown in Figures 3.3 or 3.4 as the ba- 
sis function. Figure 5.11 gives a sample plot of one such basis function. 
The x - axis is the feature value (in this case it is the gray level value) and 
y - axis shows the normalized frequency of occurrence of gray level sub- 
tracted from 1. This would be more realistic than the presently used linear 
basis function (Figure 3.3). The other possibility is to design a neural net- 
work to derive knowledge. One could also think in terms of integrating the 
knowledge base module into the interpretation system. 

Joint Segmentation and Image Interpretation 
77
Figure 5.10. 
(a) Original image of size 256 x 256, (b) wavelet transformed image, (c) initial 
segmentation (after k-means clustering) of size 128 x 128, (d) segmentation at an intermediate 
stage, (e) interpretation at an intermediate stage, (f) find Segmentation, and (g) final interpreta-
tion.
Figure 5.11. 
used basis function in Figure 3.3. 
A possible basis function which could be used in the cost function instead of the 

78
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
■Build some amount of user interactibility into the interpretation scheme.
The interactibility may prove useful in the segmentation stage, since it is 
well known that no known segmentation scheme exist that can give a good
segmentation in all parts of the image. User interactibility would enhance 
the overall performance of the proposed joint segmentation and image in- 
terpretation scheme. 
■Some study is called for to decide on how the features should be chosen.
This study would come in handy and would provide guidelines as to the 
choice of the features. The choice of the features would invariably depend 
on some a priori knowledge that the user has about the scene to be inter- 
preted.
■One could think of using an additional cue in the form of color to enhance
the interpretation scheme [2]. The other effort could go towards construc- 
tion of a single energy function which would do both segmentation and 
image interpretation similar to [72], but in a multiresolution framework. 
■One major drawback of the image interpretation scheme is the inherent as-
sumption that one makes when choosing the features. The strongest one 
being that the details of the scene are known even before the proposed al- 
gorithm is put to test, meaning we are a priori aware as to what to expect 
from the scene. This is in some sense constraining the algorithm by telling 
it what to expect in the scene. One should think of schemes which either 
integrate these constraints into the knowledge base or possibly take care by 
some other means. Even a small breakthrough in this path will be a good 
contribution.

Chapter 6 
CONCLUSIONS
Image interpretation is a high-level description of the environment from 
which the 2D image was taken. It is essentially an analysis problem where 
one tries to describe the objects in a 3D environment by identifying, analysing 
and understanding the 2D image corresponding to the 3D scene. The task of 
image interpretation is a two step process and involves firstly segmenting the 
image to isolate regions of significance and then secondly giving interpretation 
lables to the segmented regions aided by a priori accumulated knowledge base. 
Image interpretation is an important problem and has several applications. 
Amongst others, image interpretation finds use in fields like medical engineer-
ing where it is used for identifying structures within the human body; it can be 
used for automatic target recognition; for navigation; remote sensing and the 
entire gamut of activities involving robots. 
Image interpretation is a challenging task to high level vision researchers 
especially because the humans seems to be able to do a good job of it. The 
objective of an image interpretation system is to make the computer interpret 
the image as well as or better than what the human visual system is capable 
of doing. While there have been good and healthy progress in this area, one 
can not claim the problem of image interpretation to be completely solved.
The problem of image interpretation depends largely on the robustness of the 
knowledge base to minimize false alarms and increase interpretation accuracy. 
The monograph emphasizes the fact that the use of probabilistic framework 
to represent knowledge base is the right step towards making interpretation 
schemes more robust and less dependent on the knowledge base. 
The monograph describes ways and methods of making the interpretation 
schemes less depend on the knowledge base. Recently we have been explor- 
ing the possibility of further reducing the dependence on domain knowledge, 
by using a hidden Markov model (HMM) for the features [117]. The overall 

80
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
framework for image interpretation still remains MRF; the cliques functions 
now use the HMM model for various features rather than the features them- 
selves. The domain knowledge is, now, represented by the parameters of the 
HMM. The image interpretation problem can be formulated as 
where, H(K) represents the HMMs corresponding to the reference set (do- 
main knowledge) of interpretation labels.
H(R) =
{Hj(Ri)}
F,N
j,i=1,1 repre-
sents the set of HMMs for the given image, where Hj(Ri) represents the 
HMM for region Ri and feature j; F denotes the number of features. This 
set contains HMMs and joint HMMs for single and multiple node cliques. 
As seen earlier, assuming P(I | H(K), H (R)) to be a MRF, we can express
P(I | H(K), H(R)) as a Gibbs distribution (Theorem 2.1) we have
where, C representing the collection of all the cliques with respect to some 
neighborhood system η Vc (I; H(K), H(R)) are the clique functions and the 
Z is the partition function. Appendix I gives details of how to construct a 
clique function based on HMM. 
NOTE 6.1 There is a two level probabilistic structure. At one level, the inter-
pretation is modeled as a MRF and at the other level the clique functions for 
the features are based on HMMs. It is for this reason the interpretation process 
is less dependent on the domain knowledge. 
In the image interpretation schemes implemented in earlier chapters, only 
a few features as described in Appendix G have been used. In actual practise 
several more features and color information can be incorporated to make the 
techniques more robust and powerful. As a general rule when interpreting 
complex scenes, it is advisable to attempt unsupervised learning in parallel 
with interpretation. This allows the system not only to learn the dependencies 
but also allows one to add new objects to its list of identifiable objects. 
One major drawback of the image interpretation scheme as it is reported in 
literature is the inherent assumption that one makes when choosing the fea- 
tures. The strongest one being that the details of the scene are known even
before the interpretation scheme is put to test, meaning that it is a priori known 
as to what to expect from the scene. This is in some sense constraining the 
algorithm by telling it what to expect in the scene. It is preferable to think of 
schemes which either integrate these constraints into the knowledge base or 
possibly take care by some other means. Even a small breakthrough along this 
direction will be a significant contribution to the image interpretation literature. 

Appendix A 
Bayesian Reconstruction 
Let g = f + w, the general problem of reconstruction involves taking a given image g and,
from it, attempting to deduce what the original scene f looked like, here w corresponds to noise 
of known statistics. One form of reconstruction involves considering the space of all possible 
original scenes and selecting that scene which is the most probable. 
This probability can be written as the conditional probability
P(f |g). According to Bayes
Theorem,
(A.1)
This conditional probability P(f |g) is called the posterior probability. The first factor in the
numerator of (A.l), P(g| f) , is the probability of obtaining image g as a result of imaging the
scene f. We can model this and is usually dictated by the task in hand, for example in case 
of image restoration this is dependent on the choice of the degradation model. Irrespective of 
modeling, the probability should be high when the image and the scene are similar one does not 
expect to get an image that is too different from the original scene. Furthermore, it should model 
the known properties of the imaging system. One such function of modeling this probability is 
to use an inverse exponential function where the probability decreases as the difference between 
f and g increases. If the image has Gaussian uncorrelated noise with variance σ 2, we have:
where,
s
also notice, that the scene f that maximizes this probability is one identical to the image g. In 
other words, the best reconstruction is the original image. Obviously, this is not very useful. 
This is where the other factor, P(f) , in the numerator comes in: the probability of any scene 
f occurring at all, P(f) is called the a priori probability : the probability of f being the scene 
without ever taking a picture g of it. This is called also called a prior. In the prior, we take 
into account the desired properties that we want in the restored image f. For example, we may 
assume that most scenes have patches of uniform intensity with sharp discontinuities between 
them. We can again describe the probability using an inverse exponential: 

82
BAYESIAN APPROACH TO IMAGE INTERPRETATION
where, Hp(f) = ∑  i,j∈N (i) (fi-fi)
1–2
measures the total of the differences between a pixels
and their neighbors. We can design Hp(f) to be whatever we want (in fact the MRF assumption 
on f helps in designing Hp(f)), so long as it embodies what we want in our reconstruction, λ 
lets us control the relative priority of the prior. 
The denominator P(g) measures the likelihood of any given image being produced by the 
imaging device, regardless of the input. If we assume that our device is equally capable of 
imaging any scene, this probability is constant for all g. Since our goal is simply to maximize 
the posterior probability, we do not really care what the exact value is, so long as we select the 
largest. For this reason, we can simply ignore the constant denominator. If, however, we wanted 
to model tendencies for our device to produce images with certain properties, we could model 
it here. 
To get the best Scene f, we find the Scene that maximizes the numerator: 
(A.2)
The f
∧
that maximizes (A.2) is the same one that maximizes the logarithm of the numerator:
Or, we could just as well minimize the negative of this quantity: 
This is simply the weighted sum of a data-fit term Hn (f, g) and a prior term Hp (f) where the
weight λ controls the relative importance.

Appendix B 
Proof of Hammersley-Clifford Theorem 
For completeness, so that one does not have to go back and forth, we repeat the statement of 
the theorem. 
Assume that X has a finite configuration over S and that P [x = 0] > 0, then X is a 
MRF with respect to a neighborhood η iff X is Gibbs distributed, with U(x) as defined
below; namely 
(B.1)
where x is a realization of X, Z is a normalization constant commonly referred to as 
the partition function and is given by 
(B.2)
(B.3)
and U(x) is given by
The general form for U(x) is

84
BAYESIAN APPROACH TO IMAGE INTERPRETATION
(B.4)
where the functions G.,...,⋅(⋅, ⋅⋅⋅, ⋅) are arbitrary, except that they are zero if the argu-
ments do not belong to a clique. For example Gi,jim ,n (x i,j, xm ,n) = 0 if xi,j and
xm ,n do not belong to the same clique. 
1.
Justification for the General form for U(x) 
To get an idea as to why U(x) as defined earlier (B.4) is the most general form, we first start 
with a function f(x) for a scalar variable x, and f(x) analytic at 0. Consider the Maclaurian 
series for x : 
(B.5)
we use the notation fx  ... x
i–time s
y ... y
j–times (xo, yo) to denote the ith order partial deriva-
tive with respective to x and j th order partial derivative with respect to y of f (⋅, ⋅) at (xo, yo).
Next consider the expansion of a function of two variables, namely f(x, y) which is analytic 
at (0,0). 

APPENDIX B: Proof of Hammersley-Clifford Theorem 
85
(B.6)
In case of functions over a finite discrete domain, the extension of the Taylor series (Maclau-
Let f(x) be a function of a binary (0,1) variable x, then we have the expansion 
rian series for multivariable case) is the Reed-Muller expansion. 
Note, f (x) takes only two values corresponding to the two values of x; and the above identity
Next consider f (x, y) as a function of two binary variables x and y, then f (x, y) can be
is indeed satisfied (f (1) = f (0) (0) + f (1)1 = f (1)).
represented as 
This idea can be extended to functions of finite multivalued variables. For example, for 
the case of a function of two variables f (x, y), where x and y are multivalued variables (say
m-valued variables), we can represent f (x, y) as
(B.7)
where G... (⋅⋅ ⋅) can be found from f (. , .) evaluated at appropriate points. To evaluate G1 (x)
consider
This implies 
Next to evaluate G2 (y) consider
This implies 
Finally, using the above, G1,2 (x, y) is obtained
Note, any finite number works, because it will come into picture only for x = 0, or y = 0 or 
both x = 0 and y = 0. Now, any finite number multiplied by zero will give us zero. 
Now we look upon U (x) as a multivariable function of finite multivalued variables. In case
of modeling images, the number of finite values taken by each variable is equal to the number of 
gray levels under consideration. Thus, the general form proposed by Besag [78] can be looked 
upon as Reed-Muller expansion of a multivalued-multivariable function. 

86
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
1.1.
Proof of the Hammersley-Clifford Theorem 
The proof follows [78], with perhaps, some pedagogical elaborations. 
Notation:
1 c : (i,j) ∈c will cover all cliques associated with (i,j). Note, this will cover all sites
(k, 1) ∈ηi,j
2
c : (i, j) ∉c will cover all cliques associated with all sites (m,n) ≠(i, j).
3 ∑e – ∑c vc(x) represents summation of the term e – ∑ c vc(x) over all possible values of
xi,j , Note, there will be cliques which will contain the pixel xi,j , and it is the contribution 
from this pixel which is summed out. 
xi,j
if part: X is Gibbs distributed as defined in the theorem then X is a MRF. 
Consider,
(B.8)
Next, through similar manipulations, we show that P[Xi,j = xi,j | Xk ,i = x k ,i, (k, l) ∈η i,j]
equals (B.8). 

APPENDIX B: Proof of Hammersley-Cliflord Theorem 
87
(B-9)
Canceling similar terms from the numerator and denominator, and comparing with (B.8) we 
see that 
Note that P[X = 0] = 
1
Z– e -
U (0) ≠ 0 as long as U(0) ≠∞  ; and U (0) will not be infinity 
Thus we have shown that X is a MRF with P[X = 0] > 0.
because x i,j takes countable finite values.
theorem.
The proof is in two parts: 
(a) First we need to show that there exist a U(x) such that the probability distribution of X can 
be expressed as P[X = x] =
1
Z– e – U (x).
(b) Next we need to consider the general form for U (x), and establish that the MRF condition
implies that G...(⋅⋅⋅) will be zero, if its argument do not belong to the same clique. This will 
then establish that U (x) can be expressed as U (x) = ∑ c V c(x).
(a) Since P[X = 0] ≠0, we define U (x) as
obviously U(0) = 0 and
only if part: MRF⇒ Gibbs distribution with the general form for U(x) as specified in the 

88
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
(b) In order to prove that G... (⋅⋅⋅) becomes zero if its argument do not belong the same clique, 
consider the following: 
Let X- m,n be the state defined by
Let -x m, n be a realization of -X m, n Now consider 
consequently
(B.10)
In the above, the common term P[X k ,l
= xk ,l, ∀(k, l) ∈S, (k, l) ≠(m,n)] has been
canceled.
Now, by, the Markovian assumption, the right hand side depends upon xm ,n  and the neigh-
borhood of (m, n) , namely, 
(B. 11)
Using the general form for U(x) and U( -xm ,n ) (Reed-Muller expansion) we need to show that 
G... (⋅⋅⋅) will be zero if it does not have all its arguments as neighbors of one another. Rewriting 
(B.4),

APPENDIX B: Proof of Hammersley-Cliflord Theorem 
89
Without loss of generality we can consider (m, n) = (1,1). Then in U(x) - U( -x1, 1 ):
1 Since x1, 1  = 0, in U( -x1, 1 ), all terms corresponding to x1 ,1  will give a zero contribution. 
2 All terms not involving x1 ,1  in U(x) will cancel with all terms not involving x1 ,1  in 
3
Finally we will be left with terms involving only x1 ,1 , and these terms come from U(x).
U(x- 1, 1 ).
Based on the above observations we have 
(B.12)
Now suppose site (p, q) is not a neighbor of (1,1). Then by (B.11), the right hand side of 
(B.12) must be independent of xp ,q . This can be used to show that all G...(⋅  ⋅) which involve 
x1 ,1  and xp,q will be zero. This, in turn, can be achieved by proper selection of x.
Choose x such that xk ,l = 0, ∀ (k, 1) ≠(1,1) and (k, l) ≠ (p, q)
then
In order for this to be independent of xp,q we have to have 
(B.13)
⋅

90
By considering all sites (p, q) which are not neighbors of (1,1) we will then obtain
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
(B.14)
Next,for three node case,choose x such that {xk , l, = 0, ∀(k, l) ≠(1,1) , (k, l) ≠(p, q), and (k,l) ≠
(t, u)}. Also ((1,1) ≠ (p,q) ≠ (t,u)}. Here, (t, u) is some arbitrary site. Then, following an 
approach analogous to deriving (B.13) we obtain 
From (B.14), we have G1 ,1;p ,q (x1 ,1 , xp ,q ) = 0. Next, whether (t, u) is a neighbor of (1,1) or
not, in order for the right hand side to be independent of xp,q , we need that
Doing this for all sites (p, q) which are not neighbors of (1,1) we get 
Continuing in this fashion we can show that all G...(⋅⋅⋅) whose arguments are not neighbors, 
or equivalently whose arguments do not belong to a clique, have to be zero. Consequently, we 
can express U(x) as
where U c will be determined by non-zero G...(⋅⋅⋅) (or G...(⋅
⋅) whose arguments form a
clique).
This completes the proof of the Hammersley-Clifford Theorem. 
⋅

Appendix C 
Simulated Annealing Algorithm – Selecting T0 in
practise
SAA is effective if the temperature is chosen correctly.
It is well known that the tem-
perature should be high initially and should reduce slowly with time (iterations) using some 
cooling schedule.
The reason being that a large value of T0
returns a value close to 1 for
exp (-(∆E)/T0) which in turn means that it is more likely that a new configuration will be 
accepted even though the energy contributed by this new configuration is high (remember we 
are looking for a smaller energy configuration). But with iterations T0 decreases and the value
of exp( - (∆E)/T0) tends to 0, making it conceptually a gradient decent situation - accepting
only configuration which have lesser energy. Suppose we have a known cooling schedule and 
what we are required to do is choose a starting T0 so that the algorithm is capable of reaching
every solution - now what does this depend on? A look at the literature gives no information on 
the choice instead the choice are ad hoc or based on trial and error and it is not uncommon to 
find — we chose the starting temperature as xyz. 
It can be seen that the temperature choice depends on ∆E which is dependent on the dif-
ference in the (∆E) energy between the old configuration and new configuration. Clearly ∆E is 
application dependent and hence the choice of T0 . Figure C. 1 plots the value of exp( – (∆E)/T0 )
for ∆E varying between 1 and 100 for different values of T0 . Suppose for a particular applica-
tion the range of ∆E is 100; we are now required to choose a suitable T0 . For example if we 
choose a starting T0 to be 100, then clearly as seen from the plot in Figure C. 1 exp(– (∆E)/T0 )
would have a value between 0.3 and 0.4, implying that there is more probability that a high en-
ergy will not be accepted as a possible configuration from the beginning, which is contrary 
to what we require — more probability of a high energy state being accepted and hence the 
algorithm being able to span the whole solution space. But if T0  is chosen as 1000, there 
exp(– (∆E)/T 0 ) is greater than 0.9 and hence there is a good probability of the algorithm 
being able to span the solution space. This clearly leads to the belief that T0  should be cho-
sen such that it has some bearing on ∆E. In fact, in practise it is best to choose T0  such that 
∆E/T0 ~~  0.1 so that there is a good probability of choosing a higher energy state and hence 
being able to span whole of the solution space. 
Now, let us look at a case where ∆E is say of the order 100, then we know from the 
above discussion that we should choose T0  to start at say 1000. But now the T0  should be 
reduced from 1000 (using some cooling schedule) such that the exp(– (∆E)/T0 ) value be-
comes small; meaning less probability of a high energy state being accepted. Suppose we agree 

92
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Figure C.I.
∆E versus exp (– (∆E)/T 0).
Figure C.2. 
Original and Noisy images. 
that exp(– (∆E)/T0 ) = 0.1 is good enough probability of the high energy states not being 
accepted then it is easy to see that we would have to reduce T0 to 50 – this implies that a log 
cooling schedule would need n iterations (to reduce the T0  from 1000 to 50) where n is given 
by
1000/ loge(n) = 50
⇒n = 4.8 x 108
(C. 1)
If an application has ∆E in the range 10 then we would have to choose T0 =
100 and log
reduce it to 5 taking the same number of iterations. This shows that a higher T0  does not affect 
the number of iterations. In practise, one can use a different cooling schedule and reduce the 
number of iterations. T0 the best of our knowledge we have not come across similar discussion
about choice of T0  in literature. We believe this discussion could prove to be of use especially 
when using SAA in practise. 
1. 
Experiments 
Assume a simple degradation model Y  = X + W, where Y , X, W be the observed, original 
and noise images respectively. The aim of image restoration is to recover X from the observa- 

APPENDIX C: Simulated Annealing Algorithm — Selecting T 0
in practise 
93
TO (choice)
T0 (initial)
25
using information in this appendix 
0.2 dB 
Figure C.3(a) 
SNR improvement 
0.8 dB 
arbitrary
250
Table C. 1.
Restoration comparison.
Figure C.3. 
Restored images. 
tion Y. We formulate the problem of image restoration as an optimization problem as 
Simulated annealing algorithm is used to find the optimal solution Xˆ.
Figure C.3(a) and C.3(b)
shows restored images after 100 iterations using the cooling schedule of the form Tk +1 = 
0.9 x Tk  (for computational reasons we do not use the inverse log cooling schedule). An 
arbitrary T0  = 250 was chosen for restoring Figure C.3(a) while the temperature variable was 
chosen as 25 which satisfies the discussion in this appendix to restore image (Figure C.3(b)). 
The SNR improvement is for both the cases are tabulated in Table C.1. Clearly, the right choice 
of the start temperature is useful in the restoration process. 
Figure C.3(b) 

Appendix D 
Custom Made Pyramids 
Pyramids are data structures used to store and process images at multiple levels of resolu-
tions. Use of pyramids with dimensions given by power of 2 is an unnecessary restriction on 
the construction of the pyramids [116]. The question of what size levels are most appropriate 
are not known but the size depends on the application and empirical experiences. Here we show 
how pyramids of arbitrary size reductions between levels can be constructed. The reduction 
level can be different in each direction and could also differ between levels. The basic idea is 
the use of spatial resampling technique used in graphics which is related to anti-sampling.
For simplicity, we look at the case of one dimensional resampling. The problem of con-
structing pyramids of arbitrary size reductions can be defined ,as follows 
Given N, M ∈II, reduce a given vector [V0 , V1 ,
,VN - 1 ] of size N pixels to a
vector [W0 , W1 ,
⋅, WM - 1 ] of size M pixels.
Observe that there is no restriction on both N, M, except that N, M ∈II. The three possibilities 
that arise are (i) M > N (interpolation), (ii) M < N (sub-sampling) and (iii) M = N (trivial). 
For the purpose of discussion we restrict ourselves to M < N.
The vector [W0 , W1 ,⋅⋅⋅, WM - 1 ] can be constructed by uniformly sampling the vector
[V0 ,V1 ,⋅⋅⋅,VN - 1 ] asfollows:
where,
(D.1)
[a] represents the ceil of a, [a] represents the floor of a and ρ = M/N.
Interpreting (D.1).
:
Assume M,N so that ρ is small. Then nearly all τ ij
for j in
the appropriate range will have the value ρ , with the exception of the two extreme τ ij. More
generally, we can regard the output range [0, n] split up into M subintervals [i, i + 1] with 
i = 0,1,2, ⋅⋅. , M – 1. The same region is also split up into N subintervals [ρ   (j), ρ (j + 1)]
with j = 0, 1, 2, ⋅⋅⋅, N – 1. The coefficient τ  ij is the total length of the partition of the interval
[ρ  (j), ρ (j + 1)] that intersects the interval [i, i + 1].
⋅⋅
⋅⋅⋅

96
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Figure D.1. 
uniform sampling. 
Example.
: Let N = 4 and M = 3. Using (D.1) we can write (also see Figure D.1):
A vector of length N = 4 being converted into a vector of length M = 3 usin
Properties.
:
m Total contribution of Vj for all W is ρ , namely, 
H Total contribution of Wi for all V is 1, namely,
w0 
w1
w2
v0
v1
v2
v3

Appendix E 
Proof of Theorem 4.6 
We index the variables of the Bayesian network by an ordering consistent with the orientation 
of the Bayesian network as V = {V1 , V2 , ⋅⋅⋅VN }, namely, Vj  is a descendant of Vi  only if 
j > i. Then the joint pdf of the network can be written as 
(E.1)
Equation (E.1) holds because the parents of a variable Vi  are a subset of the set of its pre-
decessors that, once known, renders Vi  independent of all its other predecessors (Theorem 
4.1). Now consider a typical variable Vi
∈ V having parent set Uvi, and L children Y vi =
{Y1 , Y2 ,⋅⋅⋅Y m}. Vi appears exactly in L + 1 factors of the product in (E.1); once in P[Vi =
uilUvi
= uvi] and once each in P[Yj
= yj | Fj
= fj] corresponding to the jth child of Vi .
Thus, we can write 
where K = {k | Vk ∈Wvi – Y vi }. Since Vi does not figure in the last product it can be treated
as a constant α' with respect to Vi . So, we can write

98
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Since P [Wvi = w vi] = ∑  vi P[V i = ui, Wvi = wvi], it is also a constant with respect to vi.
So, we can write 
This proves the Theorem. 

Appendix F 
k-means clustering 
A technique similar to vector quantization with squared error cost function developed my 
MacQueen in 1967 goes by the name k-means clustering algorithm [118]. The basic idea of 
k-means clustering algorithm is minimum distortion (i) partitions and (ii) centroids. 
The goal of k-means clustering algorithm is to produce a partition S = {S0 , ⋅ ⋅ ⋅
 SN - 1 }
of the training alphabet A = {xi, i = ,0, ⋅⋅⋅ , n - 1} consisting of all vectors in the training 
sequence. The corresponding alphabet A will then be the collection of the Euclidean centroids 
of the set Si, that is, the final reproduction alphabet will be optimal for the final partition (but the 
final partition may not be optimal for that final reproduction alphabet except when n →∞          ).
To obtain S, we can think of each Si as a bin in which to place the training sequence vector
until all are placed. Initially we need to decide on the bins (say N), then we start by placing the 
first of the N vectors (from A) in separate bins, xi
∈S1 , i = 0, . . ⋅N - 1. We then proceed
as follows: at each iteration, a new training vector xm  is observed, and the Si for which the 
distortion between xm  and the centroid x( ˆSi) is minimized and then the new vector xm  is 
added to the bin Si. Thus, at each iteration, the new vector is added to the bin with the closest 
centroid, so that the next time, this bin will have a new centroid. This operation is continued till 
all sample vectors are exhausted. 
The Algorithm. : 
Step 0: Initialize: the number of bins N, distortion threshold ∈≥ 0, an initial N level repro-
duction alphabet Aˆ 0 and a training sequence {xj; j = 0,1, ⋅⋅⋅n – 1}. Set m = 0 and 
D1  = ∞
Step1: Given Am
ˆ
 = {yi, i = 0, ⋅⋅⋅ , N} find the minimum distortion partition P(Aˆ m ) = 
{Si, i = 0, ⋅⋅⋅,N) of the training sequence xj
∈ Si if d(xj,yi) ≤d(xj,yl) for all l
Compute the average distortion 
Step 2:
If (Dm - 1 - Dm)/Dm
≤∈ , halt with A m as the final reproduction alphabet, else
Step 3:
Find the optimal reproduction alphabet
ˆx(P(A m )) = {xˆ (Si); i = 1,⋅. ⋅, N} for
continue with Step 3
P( ˆA m ). Set A m +1 = xˆ (PAˆ m ), replace m
→
m + 1 and go to Step 1
ˆ
ˆ
ˆ

100
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Initial assignment of bin values. : There are a number of ways in which the bins can 
be initialized. 
First N vectors : choose the first N vectors of the training vector and assign the values taken
Histogram
: calculate the histogram of the image and select the N local maxima and assign
Splitting :
by the vectors to the bins 
them as the initial bin values. 
Step 0 Initialization: Set M = 1 and define A 0ˆ
(1) = 
ˆx(A), the centre of the entire 
training set. 
Step 1 Given the reproduction alphabet or the bins 
ˆA 0 (M) containing M vectors {yi, i =
1,2, ⋅⋅⋅ , M} split each vector yi into 2 close vectors y ± ∈ where ∈ is a fixed pertur-
bation vector. Now -A has 2M vectors ( {yi ±
       ∈  ;   i = 1, ⋅⋅⋅, M}). Replace M
→
2M.
Step 2
Is M = N(the required number of bins)? If yes, set
ˆA 0 =
-A(M) and halt.
-Ao is 
then the initial reproduction vector for the N initial bins. If not run the algorithm for 
an M level quantizer on
~A(M) to produce a good reproduction alphabet ˆA(M) and
then return to Step 1. 

Appendix G 
Features used in Image Interpretation 
Though there is usually not much discussion initiated on the selection of features as applica-
ble to image interpretation, feature selection forms an important aspect of image interpretation. 
The choice of features impose assumptions on the problem of image interpretation. Table G.1 
illustrates the assumptions that are imposed on the problem when that feature alone is used for 
the purpose of image interpretation. For example, if average gray level is selected as the only 
feature for image interpretation, then one is assuming the fact that the scenes that are being 
interpreted have been captured under the same lighting condition (else the use of this feature for 
image interpretation does not makes much sense). The choice of features and the assumptions 
that they explicitly impose on the problem are tabulated in Table G.1. The a priori knowledge 
K: is also very crucial, in fact this imposes a very strong assumption that we are aware of what
to expect from the scene even before we allow the algorithm to do the interpretation. In other 
words, we cannot have the knowledge associated with a scene which is completely different 
from the scene to be interpreted, for example we cannot use the knowledge obtained for an 
indoor scene to interpret outdoor scenes. 
Features that are useful for image interpretation can be broadly classified as (i) primary 
features, namely the features that are obtained from the scene through direct measurement and 
(ii) secondary features, namely the features that are derived from primary features and hence 
are not directly measured from the scene. 
1. 
Primary Features 
Primary features are features obtained from the scene through direct measurement. A few 
primary features are enumerated below:
Area
A = the number of pixels in the region R
Perimeter P = the number of pixels on the boundary of the region R
Maximum Diameter Dmax
Minimum Diameter Dmin
Average gray value 

102
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
average gray value 
area, perimeter 
mass center 
variance,
Feature
Imposed inherent assumption (when that feature alone is used) 
the scenes have the same lighting conditions 
same object at different scales are not present 
objects are in the same position in the scene 
objects have same structure (for example, a tree in a scene with 
compactness,
aspect
scatter matrix 
leaves and without leaves do not have the same structure) 
objects having the same form cannot coexist in the scene, 
(meaning if there are two different objects corresponding to a big 
and a small square - both of them will be labeled the same) 
Table G.1. 
Assumptions imposed on the problem due to the choice of features. 
where x(i, j) are the gray levels of the pixel at the location (i, j) in the region R.
Variance of gray levels 
Mass Center 
Scatter Matrix represents the elliptical area which approximates the shape of the region. In 
other words it quantifies what can be termed as the shape variance. 
Common Perimeter feature, common perimeter PIJ = The boundary common to region I and 
J
2. 
Secondary Features 
The secondary features are nothing but shape descriptors and are simply combinations of 
primary features or size parameters, arranged so that their dimensions cancel out. This helps in 
retaining the numerical value of the secondary feature when the size of the feature is changed. 
There are many dimensionless expressions (formed from the combinations of the size parame- 
ters), but only a few are relatively common combinations [ 119]. 
Compactness
Orientation

APPENDIX G: Features used in Image Interpretation 
103
Figure G.1. 
Area (A), convex area (A + B + C + D + E + F + G + H).
Boundry length The length of the boundary common to two adjoint regions I and J is given 
by
PI  – PJ 
BIJ =
2PI J 
where, PI is the boundary of region I, PJ is the boundary of region J and PIJ is the 
boundary common to regions I and J 
Contrast
CIJ = |GI
– GJ|
where, GI is the average gray value of the region I, GJ is the average gray value of the 
region J 
Roundness
4A  
πD 2m a x
Aspect Ratio 
Dm a x
Dm in
Convex area and area are best described in Figure G.1. 
The region A gives a measure 
of the area of a segment while the convex area is obtained by summing A, B, C, D, E, F, G, 
and H. Convex perimeter is defined as the length of the hashed line. The secondary features 
associated with these features are convexity and solidity are as defined below 

104
Convexity
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Convex Perimeter
P
Solidity
A
Convex Area
Extent
A
Area of the minimum Bounding Rectangle 
Length and width make more sense when the body is rigid, on the other hand, if the object 
is really a worm or a noodle that is flexible, and the overall shape is an accident of placement, it 
would be much more meaningful to measure the length along the fibre axis and the width across 
it. To distinguish from length and width these are sometimes called fibre length and fibre width. 
Curl and Elongation are the secondary features that are associated with these features. 
Curl
Length
Fibre Length 
Elongation
Fibre Length 
Fibre Width 
There is a definite advantage of using features which are invariant under certain operations 
compared to those that change. For example, the secondary features that are obtained as ratio 
of other primary features may prove to be more useful even when the two images (image from 
which knowledge was acquired and image to be interpreted) are scaled versions of each other 
or we are working in a multiresolution framework. Another example which would work under 
different lighting conditions is the contrast in the gray levels between two adjacent regions. 
Nevertheless, one can still use these non-ratio features provided one is aware as to how the 
features change over scales (see Table H.3). This knowledge can be appropriately used to solve 
the vision task of image interpretation in the proposed multiresolution framework. 
Features commnly used in image interpretation are shown in Figure G.2 and defined in Table 
G.2.

APPENDIX H: Knowledge Acquisition 
105
1 -node features 
area (A)
perimeter (P)
average gray value (G)
Figure G.2. 
tion.
Features used in image interpretation. Table G.2 gives their mathematical defini-
total number of pixels in the region R
total number of pixels on the boundary of segment 
1
A ∑i, j ∈R x(i, j), where x(i, j) are the 
features
definition
mass center (mx , my)
variance (V)
compactness (C)
2-node features
contrast (CR)
common perimeter ratio (CPR)
gray levels of the pixel at the location (i, j)
M={mx ,my } 
=  {
1–
A ∑i, j∈R i,
1–A ∑i , j
R j}
{1–A ∑i, j ∈R  (x(i, j) — G)2}
1 /2
P2/4πA
|Gi - Gj |, regions i, j are adjacent,
and Gi represents the average gray value of region i
Pij
Pi + pi,
where Pi j is the common perimeter (CP)
(CPR)
to region i and j
Table G.2. 
example).
Definitions of commonly used features (Figure G.2 shows these features with an 
∈
–

Appendix H 
Knowledge Acquisition 
This section, though written keeping the problem of image interpretation in view, is useful 
in any problem where knowledge base is an essential prerequisite. 
Here we present an user interactive scheme for knowledge acquisition which has also worked 
well for our problem. The user interaction is through the popular public domain image display 
softwareXV 1 .
The steps involved in acquiring knowledge can be enumerated as 
1 use a known segmentation algorithm to obtain a crude segmentation, 
2 merge regions that could have possibly been classified as different segments manually 
3 assume the segmented image obtained after manually merging as the required segmented 
4 extract the required knowledge from the segments thus obtained. 
image and 
The essential idea is to segment the given image using one of the crude segmentation al-
gorithm. We demonstrate the procedure involved in acquiring knowledge with the help of an 
example face image (Figure H.1) as the image to be segmented. We use the k-means segmen-
tation algorithm to segment Figure H.1, please note that one could use any other segmentation 
algorithm and there is nothing special about using the k-means except that it is simple to im-
plement and does a good segmentation. We start by constructing the wavelet transform of the 
image (see Figure H.2(a)). 
The first quad of the wavelet transformed image is segmented. This is done to keep in tune 
with the scheme described in [74] where we have the segmented image being refined using the 
difference images (the other 3 quads). One could have as well applied the k-means clustering 
algorithm on Figure H.1. The result of k-means segmentation using 3 bins is shown in Figure 
H.2(b).
The regions resulting from the k-means segmented (Figure H.2(b)) are labelled using any 
labelling scheme, in this report we have used a labelling algorithm2 that was available in the 
laboratory.
1 Author:John Bradley, available from ftp.cis.upenn.edu:/pub/xv as a shareware 
2 labelling algorithm developed in the SPANN Laboratory by Ms Namita Maiti [120] 

BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Figure H.1. 
Image from which knowledge is to be acquired. 
Figure H.2. 
to the first quad of (a). 
(a) Wavelet transformed image of Figure H. 1 and (b) k-mean output when applied 
There are in all 21 regions as seen in the XV’s color editor (Figure H.3). Figure H.4 shows 
the output of the labelling module. 
Now, we need to manually merge the segments to get the desired segmented image (un-
doubtedly the dirtiest part to do - one has to go through this, especially because segmentation 
is a process that depends on interpretation and which is the problem that is being addressed!), 
which can be used for acquiring knowledge. 
1.
How to merge regions using the XV color editor 
The color editor is shown in Figure H.3. Initially the given labelled image (Figure H.4) 
is assigned a random color to each of the regions by clicking on the button
. This
operation will assign a different color to each of the regions. Now we describe how to merge 
two regions - in the language of the XV color editor this means - how to assign the same color 
to two regions in order to merge them. Now look at the hand portion of the image (Figure H.4, 
lower left hand comer), the hand is actually segmented into two regions, we require that they 
be merged. Assumed that you are viewing Figure H.4 using XV, now take the mouse arrow to 
the lighter color region in the hand and click the middle button. You will observe that one of 
the colors in the color editor gets activated (activation means a square is formed on the color 
Random

APPENDIX H: Knowledge Acquisition 
109
Figure H.3. 
all the labelled regions - 21 in all. 
XV’s color editor showing all the details. The color map editing (top left) shows 
a 
b 
Figure H.4. 
color editor which shows the various colors. 
(a) The labelled image with 21 regions (see Figure H.3) and (b) portion of the 
corresponding to the color on which one clicks the middle button). See Figure H.4b, the color 
block 18 is activated. 
You need to remember it - the block 18. Now click on the color (the darker part of the hand) 
that you want to assign to the the presently activated color (block 18). Now a separate block is 
selected in this case the block number 15. Now place the mouse on block 18 and press the right 
mouse button. You will observe that now both the blocks (15 and 18) have the same color and 
also the two regions of the hand have merged (See Figure H5b and Figure H.5a respectively). 
The procedure described in this section is carried out until all the segments that belong to the 
same segment (you are the judge!) are merged. The final merged image is shown in Figure H.6a 
and the corresponding color editor map is shown in H.6b. It can be seen that the 21 segments as 
seen in Figure H.4 have been finally merged into 9 segments (Figure H.6). 

110
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
a 
b 
Figure H.5. 
compare it with Figure H.4 and (b) The color editor after merging 
(a) The image showing two regions merged - observe the hand portion and 
__
a 
b 
Figure H.6. 
ually merging regions) and (b) portion of the color editor which shows the various colors 
(a) The final labelled image (see Figure H.3 from where we started before man- 
Table H.1. 
Details of image used for knowledge acquisition. 
2. 
Acquired Knowledge 
Table H. 1 gives the final labels of the segmented images and Table H.2 gives the details of 
the acquired knowledge. The details of the knowledge given are basically the primary features 
and the other features like the form factor, compactness can be estimated using these primary 
features and are hence rightly termed as the secondary features. The features that have been 
given in Table H.2 are : (a) area of the segment, (b) average gray level of the region, (c) perimeter 
of the region, (d) and (e) the region position in terms of mass centre (f) the variance of the gray 
levels in the region. 

APPENDIX H: Knowledge Acquisition 
111
features
area
perimeter
average gray value 
mass center 
variance
compactness
contrast
common perimeter ratio 
Segment
1 (Face) 
A
A/4
P 
P/2 
G 
G 
M 
M/2 
V 
V 
C 
C 
CR
CR
CPR
CPR
2 (Hair)
3(-)
4 (Eye)
5 (Brow)
6 (Eye)
7 (Hand)
8 (Lips)
9( -)
Area
5646
7774
440
85
58
133
1837
56
346
Avg Gray
156
32
125
79
82
65
169
78
49
Perimeter
634
899
114
42
48
65
254
56
187
(XPos, Y Pos)
(67,65)
(69,52)
(119,11)
(83,50)
(35,54)
(43,59)
(24, 104)
( 73,96)
(26,121)
Variance
36.593968
18.806145
23.286994
42.275149
22.789138
29.439884
35.033595
15.466554
47.435231
TableH.2.
Knowledge associated with Figure H.1
Table H.3.
Rules for constructing knowledge pyramid. 
3. 
Knowledge Pyramid 
It is often required to be able to acquire knowledge at different resolutions. Thought it 
is possible to acquire knowledge in the way described in this appendix for all resolutions, in 
practise the knowledge acquizition becomes difficult at coarser resolution. It is often useful to 
construct the knowledge at coarser resolution from the knowledge aquired at a fine resolution 
using the rules tabulated in Table H.3. 
KΩ– k
K Ω– k – 1

Appendix I 
HMM for Clique Functions 
HMM for Single Node Clique to represent shape feature Multiresolution signal decompo-
sition based on the discrete wavelet transform (DWT) is utilized to represent the shape of the 
object at different scales. The shape of the object is encoded by HMM by first obtaining the 
contour of the object. The contour is represented as a sequence S =
S 1, . . . , S ζ = {si}ζ
i =1
where ζ is the total number of boundary points. Each S i is represented in the polar coordinate 
system with centroid of the object as the origin, namely, 
where,
S is the distance along the contour in the direction of traversal. To make the representa-
tion scale invariant, all the components of R = {τi} and Θ = {θ i} are normalized, namely,
The DWT of the sequence R is found from scales 0, . . . , nτ. Similarly the DWT of the se-
quence
is found from scales 0, . . . , n . Now the feature vector used for training the HMM is
constructed as follows 
where τk
i  is the ith component in the DWT of the sequence R at scale k, and similarly θ
k
i  is the 
ith component in the DWT of  at scale k. This set of feature vectors provide a representation 
of the object at different scales. Thus, the shape model obtained is robust enough to withstand 
distortion, data perturbation or noise in the data. The sequence {X(i)}
ζ
i = 1  will not be rota-
tionally invariant. In order to achieve rotation invariance we consider all rotated versions of the 
sequence {X(i)}
ζ
i = 1  . Consequently, all the rotated sequences 
for j = 1, . . . , ζ, constitute an equivalent training sequence for the shape based HMM. 
Θ
θ
Θ

114
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
HMM for Multiple Node Cliques - Joint HMM: Spatial relationships between adjacent re-
gions in an image can be modeled using multiple node cliques. A new method for modeling 
adjacency relationships using joint HMM is described below. 
Consider a two node clique and let R1  and R2  represent the two adjacent regions in the 
segmented image. We build HMMs H1 , H2
1  for a specific feature (say shape) corresponding 
to regions R1  and R2  respectively. Since the regions are adjacent, there exists what we term 
as joint HMM which gives information about the spatial relationship between the two regions 
which is different from the information given by H1  and H2  individually using the following 
scheme:
■
Let O1 and O 2  be two observation sequences generated using H1  and H2  respectively. 
Construct a vector 
. 
. 
, 
. 
where I2 * denotes the optimal state sequence corresponding to the observation O1 , suppose 
that O1  was generated by H2 . and similarly, I1 * denotes the optimal state sequence corre- 
sponding to the observation O2 , suppose that O2  was generated by H1 . This can also be 
generalized to the case of a n – node clique where the training vector becomes: 
where, O1  , . . . , On are the observation sequences generated using the HMMs H1  , . . . , Hn
for an n-node clique. Thus the vector x will be of dimension n * (n - 1) for an n-node
clique.
■
This vector x is assumed to model the spatial relationship between the two sequences O1
and O2 . The generated vector x is then used as the observation sequence for training the 
two node clique HMM. The number of states in this HMM is taken as the number of nodes 
in the clique. 
The spatial relations can thus be modeled using HMMs, and the training observation se- 
quences depend only on the HMMs forming the clique. 
1 note that H1∈{H 1(R1), ⋅⋅⋅, HF(R1)} and H2 ∈}H1(R2), ⋅⋅⋅, HF(R2)}

References
[1] R. J. Schalkoff, Digital image processing and computer vision. Singapore: John Wiley 
and Sons, 1989. 
[2] Y. Ohta, Knowledge Based Interpretation of Outdoor Natural Color Scenes. Boston:
Pitman, 1985. 
[3] V. P. Kumar and U. B. Desai, “Image interpretation using a Bayesian network,” Tech. 
Rep. SPANN.96.3, SPANN Lab, Dept. of Elect. Engrg., Indian Institute of Technology 
- Bombay, May 1996. 
[4] A. Grasselli, Automatic interpretation and classification of images. New York Aca-
demic Press, 1969. 
[5] H. Andrews, Automatic Interpretation and Classification of Images by Use of the 
Fourier Domain. New York Academic Press, 1969. 
[6] R. Narasimham, On the Description, Generation, and Recognition of Classes of Pic-
tures. New York Academic Press, 1969. 
[7] I. Hofmann, H. Niemann, and G. Sagerer, “Model based interpretation of image se-
quences from the heart,” in Proceedings of an international workshop held in Amster-
dam, Holland, 1985.
[8] C. Sagerer, “Automatic interpretation of medical image sequences,” Pattern Recogni-
tion Letters, pp. 87–102, 1988. 
[9] N. Karssemeijer, Interpretation of medical images by model guided analysis. PhD the-
sis, Katholieke Universiteit Leuven, 1989. 
[ 10] R. Baldock, “Trainable models for the interpretation of biomedical images,” Image and 
Vision Computing, pp. 444–450, 1992. 
[ 11] T. Cootes, A. Hill, C. Taylor, and J. Haslam, “Use of active shape models for locating 
structure in medical images,” Image and Vision Computing, pp. 355–365, 1994. 
[ 12] 
J. Desachy, “A knowledge-based system for satellite image interpretation,” in Proceed-
ings 11th IAPR International Conference on Pattern Recognition, pp. 198–200, 1992. 

116
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
[13] M. Nagao and T. Matsuyama, A Structural Analysis of Complex Aerial Photographs.
New York Plenum, 1980. 
[14] J. McKendrick and M. Lybanon, “Knowledge-based interpretation aids to the navy 
oceanographic image analyst,” in Proceedings: Image Understanding Workshop, 
pp. 61–63, 1985. 
[15] D. M. Jr. and W. Harvey, “Automating knowledge acquisition for aerial image interpre-
tation:” in Image Understanding Workshop, 1987.
[ 16] T. Silberberg, “Multiresolution aerial image interpretation,” in Proceedings Image Un-
derstanding Workshop, pp. 505–511, 1988. 
[17] D. Kuan, H. Shariat, K. Dutta, and P. Ransil, “A constraint-based system for interpreta-
tion of aerial imagery,” in Second International Conference on Computer Vision, 1988.
[18] D. M. Jr, W. Harvey, and L. Wixson, “Automating knowledge acquisition for aerial 
image interpretation:” CVGIP: Image Understanding, pp. 37–81, 1989. 
[ 19] P. Garnesson, G. Giraudon, and P. Montesinos. “An image analysis system, application 
for aerial imagery interpretation,” in Tenth International Conference on Pattern Recog-
nition, 1990.
[20] V. Venkateswar and R. Chellappa, “A framework for interpretation of aerial images,” in 
Tenth International Conference on Pattern Recognition, 1990.
[21] K. Schutte, Knowledge Based Recognition of Man-Made Objects. PhD thesis, Univer-
sity of Twente, P.O. Box 217 7500 AE Enschede The Netherlands, February 1994. 
[22] B. Draper, R. Collins, J. Brolio, J. Griffith, A. Hanson, and E. Riseman, “Tools and 
experiments in the knowledge-directed interpretation of road scenes,” in Image Under-
standing Workshop, 1987.
[23] Y. Ozaki, K. Sato, and S. Inokuchi, “Rule-driven processing and recognition from range 
image,” in Intel: Conf. on Pattern Recog., pp. 804–807, 1988. 
[24] D. Chelberg, “Uncertainty in interpretation of range imagery,” in Third International 
Conference on Computer Vision, 1990.
[25] J. Aggarwal and N. Nandhakumar, Multisensor Fusion for Automatic Scene Interpreta-
tion. Ramesh C. Jain and Anil K. Jain, Analysis and Interpretation of Range Images: 
Springer-Verlag, 1990. 
[26] R. C. Jain and A. K. Jain, Analysis and Interpretation of Range Images. Springer-
Verlag, 1990. 
[27] T. Strat and M. Fischler, “A context-based recognition system for natural scenes and 
complex domains,” in Image Understanding Workshop, pp. 456–472,1990. 
[28] M. Hild and Y. Shirai, “Interpretation of natural scenes using multi-parameter default 
models and qualitative constraints,” in International Conference on Computer Vision, 
pp. 497–501, 1993. 
[29] T. Silberberg, “Infrared image interpretation using spatial and temporal knowledge,” in 
Workshop on Computer Vision, pp. 264–267, 1987. 

REFERENCES
117
[30] N. Nandhakumar and J. Aggarwal, “Integrated analysis of thermal and visua images for 
scene interpretation,” IEEE Trans. on Patt. Anal. and Mach. Intell., pp. 469–431, 1988. 
[31] A. Taylor, A. Gross, D. Hogg, and D. Mason, “Knowledge-based interpretation of re-
motely sensed images,” IVC, pp. 67–83, 1986. 
[32] V. Clement, G. Giraudon, and S. Houzelle, “Interpretation of remotely sensed images in 
a context of multisensor fusion,’’ in Second European Conference on Compute Vision, 
1992.
[33] Z. Zhang and M. Simaan, “A rule-based interpretation system for segmentation of seis-
mic images,” Pattern Recognition, pp. 45–53, 1987. 
[34] A. Heller, D. LaRocque, and J. Mundy, “The interpretation of synthetic aperture radar 
images using projective invariants and deformable templates,” in DARPA Image Under-
standing Workshop, pp. 831–837, 1992. 
[35] C.-C. Chu and J. Aggarwal, “The interpretation of laser radar images by a knowledge-
based system,” Machine Vision and Applications, pp. 145–163, 1991. 
[36] M. Kurtz, P. Mussio, and P. Ossorio, “A cognitive system for astronomical image inter-
pretation,” Pattern Recognition Letters, pp. 507–515, 1990. 
[37] S. Towers and R. Baldock, “Application of a knowledge-based system to the interpreta-
tion of ultrasound images,” in Ninth International Conference on Pattern Recognition, 
1988.
[38] V. Roberto, A. Peron, and P. Fumis, “Low-level processing techniques in geophysical 
image interpretation,” Pattern Recognition Letters, pp. 111–122, 1989. 
[39] K. Sugimoto, M. Takahashi, and F Tomita, “Scene interpretation based on boundary 
representations of stereo images,” in Ninth International Conference on Pattern Recog-
nition, 1988.
[40] T. Pridmore, J. Mayhew, and J. Frisby, “Exploiting image-plane data in the interpre-
tation of edge-based binocular disparity,’’ Computer Vision, Graphics, and Image Pro-
cessing, pp. 1–25, 1990. 
[41] Y. L. Guilloux, “Automatic computation of motion in an image sequence, interest for 
interpretation,” Signal Processing, pp. 377–, 1985. 
[42] A. Milano, F Perotti, S. Serpico, and G. Vemazza, “A system for the interpretation of 
3-d moving scenes from 2-d image sequences,” International Journal of Pattern Recog-
nition and Artificial Intell., pp. 765–796, 1991. 
[43] S. Tsuji, “Continuous image interpretation by a moving viewer,” in Ninth International 
Conference on Pattern Recognition, pp. 514–519, 1988. 
[44] T. Binford, “Survey of model based image analysis systems,” Int. J. Roborics Res., 
pp. 587–633, 1982. 
[45] C. Smyrmiotis and K. Dutta, “A knowledge-based system for recognizing man-made
objects in aerial images,” in Proc. Comp. Vis. and Patt. Recog., pp. 111–117, 1988. 

118
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
[46] D. Ballard, C. Brown, and J. Feldman, “An approach to knowledge-directed scene anal-
ysis:’ in CVS, pp. 271–281, 19xx. 
[47] A. Mitiche, A. Mansouri, and C. Meubus, “A knowledge based image interpretation 
system,” in Ninth International Conference on Pattern Recognition, 1988.
[48] C.-C. Chu and J. Aggarwal, “Image interpretation using multiple sensing modalities,” 
IEEE Trans. on Patt. Anal. and Mach. Intell., pp. 840–847, 1992. 
[49] V. Roberto, “Knowledge-based understanding of signals: An introduction,” Signal Pro-
cessing, pp. 29–56, 1993. 
[50] P. Puliti and G. Tascini, “Knowledge-based approach to image interpretation,” Image
and Vision Computing, pp. 122–128, 1993. 
[5 13 J. Smolle, R. Hofmann-Wellenhof, and H. Kerl, “Pattern interpretation by cellular au-
tomata (pica)- evaluation of tumour cell adhesion in human melanomas,” Analytical
Cellular Pathology, pp. 91–106, 1994. 
[52] R. Evangelista and 0. Salvetti, “A morphometric and densitometric approach to image 
interpretation,” Pattern Recognition and Image Analysis, pp. 305–3 10, 1993. 
[53] W. Dickson, “Feature grouping in a hierarchical probabilistic network,” Image and Vi-
sion Computing, pp. 51–57, 1991. 
[54] F. V. Jensen, H. I. Christensen, and J. Nielsen, “Bayesian methods for interpretation 
and control in multi-agent vision systems,” Applications of Artificial Intelligence X: 
Machine Vision and Robotics, SPIE Proceedings Series, vol. 1708, 1992. 
[55] W. B. Mann and T. O. Binford, “An example of 3-D interpretation of images using 
Bayesian networks,” in Proceedings DARPA Image Understanding Workshop,, 1992.
[56] V. P. Kumar and U. B. Desai, “Image interpretation using Bayesian networks,” IEEE
Trans. on Pattern Anal. and Machine Intell., pp. 74–77, 1996. 
[57] W. Wilhelmi, “Image interpretation by algebraic topology,” Pattern Recognition and 
Image Analysis, pp. 126–134, 1992. 
[58] J. A. Modestino and J. Zhang, “A Markov random field model based approach to image 
interpretation,” IEEE Trans. on Patt. Anal. and Mach. Intell., pp. 606–615, 1992. 
[59] I. Y. Kim and H. S. Yang, “Efficient image labeling based on Markov random field and 
error backpropagation network,” Pattern Recog., pp. 1695–1707, 1993. 
[60] J. M. Tenenbaum and H. G. Barrow, “Experiments in interpretation guided segmenta-
tion,” Artificial Intelligence, pp. 241–274, 1977. 
[61] R. Bajcsy, F Solina, and A. Gupta, Segmentation versus Object Representation-Are
They Separable? Springer-Verlag, 1990. 
[62] M. Sonka, S. K. Tadikonda, and S. M. Collins, “Genetic algorithms in hypothesize-and-
verify image interpretation,” Proc. SPIE - Sensor Fusion VI, pp. 236–247,1993. 
[63] R. A. Schowengerdt, Techniques for Image Processing and Classification in Remote 
Sensing. New York: Academic Press, 1983. 

REFERENCES
119
[64] B. Draper, J. Brolio, R. Collins, A. Hanson, and E. Riseman, “Image interpretation by 
distributed cooperative processes,” in Proc. Comp. Vis. and Patt. Recog., 1988.
[65] K. S. Kumar and U. B. Desai, “Joint segmentation and image interpretation,” Tech. 
Rep. SPANN.96.2, SPANN Lab, Dept. of Elect. Engrg., Indian Institute of Technology 
- Bombay, May 1996. 
[66] J. Pearl, “Fusion, propagation and structuring in belief networks,” Artificial Intelligence, 
pp. 241–288, 1986. 
[67] J. Pearl, “Evidential reasoning using stochastic simulation of causal models:’ Artificial
Intelligence, pp. 245–257, 1987. 
[68] J. Pearl, Probabilistic Reasoning in Intelligent Systems. New York: Morgan Kaufmann, 
1988.
[69] R. E. Neapolitan, Probabilistic Reasoning in Expert Systems. New York: John Wiley, 
1988.
[70] G. Shafer, A Mathemetical Theory of Evidence. Princeton, New Jersey: Princeton Uni-
versity Press, 1976. 
[71] E. J. Horovitz, D. E. Heckerman, and C. P. Langlotz, “A framework for comparing 
alternative formalism for plausible reasoning,” in Proc. of the Fifth National . on 
AI, (Philadelphia, Pennsylvania), 1986. 
[72] I. Y. Kim and H. S. Yang, “An integrated approach for scene understanding based on 
Markov random field,” Pattern Recog., pp. 1887–1897, 1995. 
[73] I. Y. Kim and H. S. Yang, “An integration scheme for image segmentation and labeling 
based on Markov random fields,” IEEE Trans. on Patt. Anal. and Mach. Intell., pp. 69– 
73, 1996. 
[74] K. S. Kumar and U. B. Desai, “Joint segmentation and image interpretation,” Pattern
Recognition, pp. 557–589, April 1999. 
[75] D. Marr, Vision. San Francisco: W. H. Freeman and Co., 1982. 
[76] B. K. P. Horn, Robot Vision. Cambridge: MIT Press, 1986. 
[77] D. Mumford and J. Shah, “Optimal approximations by piecewise smooth functions 
and variational problems,” Communication of Pure and Applied Variational Problems, 
vol. XLII, no. 5, pp. 577–685, 1988. 
[78] J. Besag, “Spatial interaction and the statistical analysis of lattice systems,” J. Royal 
Statistical Society, pp. 192–236, 1974. 
[79] S. Geman and D. Geman, “Stochastic relaxation, Gibbs distribution, and Bayesian 
restoration of images,’’ IEEE Trans. on Patt. Anal. and Mach. Intell., pp. 721–741, 1984. 
[80] S. Kirkpatrick, C. S. Gelatt, and M. P. Vecchi, “Optimization by simulated annealing,” 
Science, pp. 671–680, 1983. 
[81] B. Hajek, “Cooling schedules for optimal annealing,’’ Mathematics of Operations Re-
search, vol. 134, pp. 311–329, 1989. 

120
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
[82] E. Aarts and J. Korst, Simulated annealing and Boltzmann machines. John Wiley, 1989. 
[83] G. E. Hinton and T. J. Sejnowski, “Learning and relearning in Boltzmann machines,” 
in Parallel and Distributed Processing (D. E. Rumelhart, L. McClelland, and the PDP 
Research Group, eds.), MIT Press, 1988. 
[84] C. Koch, J. Marroquin, and A. Yuille, “Analog neuronal networks in early vision,” Proc.
National Academic Sciences, pp. 4263–4267, 1986. 
[85] A. L. Yuille, “Energy functions for early vision and analog networks,” Biological Cy-
bernetics, vol. 61, pp. 115-123, 1989. 
[86] J. Zerubia and R. Chellappa, “Mean Field Annealing for edge detection and image 
restoration,” in European Conference on Computer Vision, 1990.
[87] D. Geiger and E Girosi, “Parallel and deterministic algorithms from MRF’s : Surface 
reconstruction,” IEEE Tran. on Pattern Analysis and Machine Intelligence, vol. 13, 
pp. 401–412, 1991. 
[88] M. R. Bhatt and U. B. Desai, “Robust image restoration algorithm using Markov ran-
dom field model,’’ Graphical Models and Image Proc., vol. 56, pp. 61–74, January 
1994.
[89] E. king, “Beitag sur theorie des ferromagnetismus,” Zeit. fur Physik, vol. 31, pp. 253– 
258,1925.
[90] J. Marroquin, S. Mitter, and T. Poggio, “Probabilistic solution of ill-posed problems in 
computational vision,” ASAJ, vol. 82, pp. 76–89, March 1987. 
[91] S. Geman and C. Graffigne, “Markov random fields image models and their applications 
to computer vision,” in Proc. Int. Congr. Math., 1987.
[92] F. J. Solis and J. B. Wets, “Minimization by random search techniques,” Math. of Op-
eration Research, vol. 6, pp. 19–30, 1991. 
[93] N. Metropolis, A. Rosenbluth, M.Rosenbluth, and E. Teller, “Equation of state calcula-
tions by fast computing machines,” J. of Chem. Physics. vol. 21, pp. 1087–1092, 1953. 
[94] S. Mallat, “A theory for multiresolution signal decomposition: the wavelet representa-
tion:’ IEEE Trans. on Pattern Anal.and Machine Intell., pp. 674–693, 1989. 
[95] P. J. Burt and E. H. Adelson, “The Laplacian pyramid as a compact image code,’’ IEEE
Tran on Comm., pp. 532–540, 1983. 
[96] I. Daubechies, Ten Lectures on Wavelets. Philadelphia, Pennsylvania: SIAM, 1992. 
[97] D. Gabor, “Theory of communication,” Journal of I.E.E., vol. 93, pp. 429–441, 1946. 
[98] Y. Meyer, Wavelets. Berlin,: Springer Verlag, 1989. 
[99] P. P. Vaidyanathan, Multirate systems and filter banks. Englewoods Cliff, New Jersey: 
Prentice Hall, 1993. 
[ 100] J. M. Jolion and A. Rosenfeld, A pyramid framework for early vision. The Netherlands: 
Kluwer Academic Publishers, 1994. 

REFERENCES
121
[101] S. K. Kopparapu, U. B. Desai, and P. I. Corke, “Behaviour of image degradation model 
in multiresolution,” Signal Processing, vol. 80, pp. 2407–2420, 2000. 
[ 102] D. Marr and T. Poggio, “A computational theory of human stereo vision,” in Procedings
Royal Society London, 1979.
[103] B. Kosko, Neural Networks and Fuzzy Systems. India: Prentice Hall, 1992. 
[ 104] L. Davis, Genetic Algorithms and Simulated Annealing. London: Pitman Publishing, 
1987.
[105] R. Prasannappa, L. Davis, and V. S. S. Hwang, “A knowledge-based vision system for 
aerial image understanding,” CS-TR, p. 1785, 1987. 
[106] D. M. McKeown, W. A. Harvey, and J. McDermott, “Rule-based interpretation of aerial 
imagery,” IEEE Tran. on Pattern Analysis and Machine Intelligence, vol. 7, pp. 570– 
585, 1985. 
[107] J. Pearl, “Fusion, propagation and structuring in belief networks,” Artificial Intelligence, 
vol.. 29, pp. 241–288, 1986. 
[108] R. E. Neapolitan, Probabilistic Reasoning in Expert Systems. John Wiley, 1990. 
[109] S. L. Lauritzen and D. J. Spiegelhalter, “Local computation with probabilities in graph-
ical structures and their applications to expert systems,” Journal of the Royal Statistical 
Society series B, vol. 50, 1988. 
[ 110] J. Pearl, “Evidential reasoning using stochastic simulation of causal models,” Artificial
Intelligence, vol. 32, pp. 245-257, 1987. 
[ 11 1] G. F, Cooper, “The computational complexity of probabilistic inference using Bayesian 
belief networks,” Artificial Intelligence, vol. 42, pp. 393–405, 1990. 
[112] R. M. Chavez and G. F Cooper, “A randomized approximation algorithm for proba-
bilistic inference on the Bayesian belief networks,” Networks, vol. 20, pp. 661–685, 
1990.
[113] P. Dagum and M. Luby, “Approximating probabilistic inference in Bayesian belief net-
works is NP-hard,” Artificial Intelligence, vol. 60, pp. 141–153, 1993. 
[114] K. S. Kumar, Modular Integration for Low-level and High-level Vision Problems in 
a Multiresolution Framework. PhD thesis, Indian Institute of Technology - Bombay,
1997.
[ 115] K. S. Kumar and U. B. Desai, “Joint segmentation and image interpretation,” Tech. Rep. 
SPANN.96.2, Indian Institute of Technology - Bombay, May 1996. 
[116] S. Peleg, 0. Federbusch, and R. Hummel, “Custom made pyramids,” in Parallel Com-
puter vision (L. Uhr, ed.), pp. 125-146, Academic Press, 1987. 
[117] N. Kamath, K. S. Kumar, U. B. Desai, and R. Duggad, “Joint segmentation and image 
interpretation using HMM,” in Proceedings of the International Conference on Pattern 
Recognition, (Brisbane, Austalia), August 1998. 

122
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
[118] Y. Linde, A. Buzo, and R. M. Gray, “An algorithm for vector quantization design,” 
IEEE Trans. Communictions, vol. 28, pp. 84–95, 1980. 
[ 119] J. C. Russ, The image processing handbook. Boca Raton, Florida: CRC Press, 1994. 
[ 120] E. R. Davies, Machine vision: theory, algorithms, practicalities. London: Academic 
Press, 1990. 

Index
Adjacency constraints, 7 
Adjacency graph, 37, 43, 53 
Adjacent regions, 114 
Admissibility criterion, 29-30
Algebraic topology, 6
Clique,
1–17, 36, 38, 44, 84, 87–88,114 
Algorithm 
n-node, 114 
joint segmentation and image interpretation, 64
1-node, 37
segmentation, 62 
2-node, 37 
simulated annealing, 91
1-node, 39, 41 
2-node, 41 
k-means, 99 
segmentation, 107 
3-node, 37 
multiple node, 39 
Cliques functions, 80 
Cliques, 8, 23, 37, 44, 50, 52–53, 86 
Coarse to fine, 33 
Color editor. 108 
Combinatorial, 42 
Combinatorics, 12 
Compactness, 4.37,110 
Conditional distribution, 48, 55 
Conditional distributions, 55 
Conditional pdf, 55 
Conditional probability, 8-
9, 14, 20–21, 24, 63, 67, 
Clique functions. 35, 38, 42, 55 
Clique parameters, 35 
Clique potential, 8, 37 
Clique potentials, 67 
Analytic function, 84 
Annealing schedule, 9 
Anti-sampling, 95 
Automatic target recognition, 79
Bandpass filter, 24 
Basis function, 8, 41, 76 
linear, 76 
Bayes rule, 19 
Bayes theorem, 19, 45 
Bayes Theorem, 81 
Bayesian approach, 13 
Bayesian network, 10, 8, 43–55, 97 
Bayesian networks, 5–6, 43–44, 47, 51 
81
Bayesian
Convolve, 28 
Belief networks, 8 
Biomedical science, 5 
Blurring matrix, 19
D-separated, 49, 53 
Boltzmann machine, 12 
D-separation, 49 
Causal network, 47 
Data perturbation, 113 
Causal networks, 8 
Data structure., 24 
Cellular automata, 6 
Data structures, 95 
Centre frequency, 24 
Daubechies filter, 69 
Centroid, 113 
Degradation model, 81, 92 
Centroids, 99 
Dempster-Shafer theory, 9 
Clique function, 10,37,39, 41, 64 
Densitometric, 6
singly-connected, 50-51
Conditional probabilities, 48, 50, 54, 76 
updating, 49 
properties, 48
Conditionally independent, 53
Reconstruction, 81
Cooling schedule, 24,91
Cost function, 12,99 
Custom made pyramids, 26 

124
Density functions, 76 
Fuzzy set, 8 
Depth estimation, 2 
Fuzzy systems, 40 
Detection, 11 
Ganglion cells, 33 
Difference image, 63 
Gaussian distribution, 19 
Dilation parameter, 29 
Gaussian pyramid, 26, 29 
Directed acyclic graph, 47 
Gaussian, 81 
Discontinuities, 81 
General purpose computers, 1 
Discrete wavelet transform, 30 
Genetic algorithm, 42 
Distortion threshold, 99 
Gibbs distributed, 36 
Domain independent, 6 
Gibbs Distribution, 13 
Domain knowledge, 4–5, 7–8, 35,37–40 
Gibbs distribution, 18–19, 36, 83, 86 
Domain Knowledge, 41 
Gibbs energy, 36 
Domain knowledge, 43–45, 55, 63–64
Gibbs sampling, 44, 51 
Down sample, 28 
Gibbsian distribution, 19 
Down-sampled, 28 
Gibbsian pdf, 51, 53 
Dyadic wavelet, 30 
Gradient decent, 91 
Early vision, 11, 19, 22 
Graph, 47 
Edge detection, 2,11 
Hammersley-Clifford Theorem, 90 
Edge fields, 66 
Hierarchical decision, 7 
Energy function, 9,13, 16–17, 20, 63–64, 78 
High level vision, 79
Energy minimization, 12 
High pass filter, 26, 31, 62 
Equal contribution, 27 
High-level vision, 2, 11, 74 
Evidential reasoning, 50 
Histogram, 5,55, 100 
Expert system, 7 
HMM, 113 
Expert systems, 44, 46 
Hopfield network, 13
Feature selection, 101 
Horizontal edge, 22 
Feature vector, 113 
Human visual cortex, 26 
Feature
Human visual system, 1, 3 
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
area, 101, 103
HVS, 1, 25 
aspect ratio, 103 
I.i.d. 19 
boundary length, 103 
111-posed, 11 
common perimeter, 102 
compactness, 102
Image interpretation, 3, 8, 35, 37–38, 43–44, 53, 
contrast,
103
59–61, 63–64, 69, 74, 76, 78, 80, 101, 107 
convex area, 103 
convexity, 104
Image reconstruction, 81
curl, 104 
elongation, 104 
Image-image task, 2 
extent, 104 
Image-scene task, 2 
gray value, 102 
Imaging device, 82 
mass center, 102 
Independence networks, 8 
maximum diameter, 101 
Inner product, 29 
minimum diameter, 101 
Interpolate, 68 
orientation, 102 
Interpret, 2 
perimeter, 101
Interpretation block, 6, 69 
roundness, 103
Interpretation labels, 5, 64, 69 
scatter matrix, 102 
solidity, 104 
Interpretation network, 52 
variance, 102 
Interpretation scheme, 78 
Image discontinuities, 12 
Image recognition, 2 
Image restoration, 13, 22, 81,92 
Interpretation module, 60, 62, 69, 71 
Features, 4,110
Interpretation, 5, 9, 37, 42, 46, 73, 80
Feedback process, 6 
approaches, 7 
Ferro magnetism, 17 
Finite configuration, 83 
Form factor, 110 
Fourier transform, 29 
Fovea, 32 
knowledge based, 7 
Fuzzy expert system, 7 
astronomical images, 5 
deformable template, 6 
Fourier domain, 6 
geophysical images, 5 
laser radar images, 5 

INDEX
125
literature, 5 
Markov random field, 6, 8 
neural networks, 8 
probabilistic approach, 7 
remote sense images, 5 
rules for, 7 
SAR images, 5 
scheme, 4 
Machine vision, 11 
seismic images, 5 
thermal images, 5 
ultra sound images, 5 
aerial images, 5 
Bayesian network, 5 
biomedical, 5 
Markovian, 88 
Interpretation
Markovianess, 14,23 
block
Maximum entropy, 17 
cellular automata, 6 
color images, 5 
Metropolis, 24 
genetic algorithm, 6 
MLP, 8 
infra red images, 5 
knowledge based, 6 
natural scene, 5 
Morphometric, 6 
range image, 5 
three blocks, 5 
Lexicographical ordering, 13, 18, 21 
Line field, 22 
Linear basis function, 76 
Local maxima, 100 
Locality property, 14, 19, 21 
Low pass filter, 26, 31, 62 
Low-level vision,
2, 4–5, 60 
Maclaurian series, 84-85
Manual segmentation, 5 
Marginal density function, 47 
Marginal distributions, 51 
Markov random field, 13 
two
block,
4
MAP,
8–9, 12, 19–20, 35, 38–40, 42–43, 53, 63 
knowledge acquisition, 5 
Mean field annealing, 13 
Merge regions, 107 
Modeling images, 85 
Modular integration,
7,
11, 60–61, 69, 73–74 
Mother wavelet, 29 
satellite
images,
5
MRE 8–9, 12–14, 18, 21–22, 35–36, 38, 43, 45, 
52–53, 55, 63, 67, 82–83, 86–87 
Interpretation. projective invariants, 6 
Multifrequency channel, 25 
Interpretations, 44-45
Multilayer perceptron, 8 
Inverse exponential, 81 
Multiple node cliques, 114 
Inverse optics, 11 
Multiresolution decomposition, 31 
Ising model, 17 
Multiresolution framework, 60,62,104 
Isolated features, 5,7 
Multiresolution, 6,9,24–25 
Joint density function, 47 
Multiresolution, 29 
Joint
distribution,
48–49, 51
Multiresolution,
30–31, 60–61, 69, 73–74, 78 
Joint HMM, 114 
Multiscale representations, 26 
Joint pdf. 51,54, 97 
Multivalued variables, 85 
Joint probability distribution, 48, 51-52
Navigation, 2, 11,79 
Joint segmentation and integration, 62 
Neighborhood
K-means clustering, 9, 62, 64–65, 67, 69, 99, 107 
first order, 14, 17 
K-means segmentation, 72, 107 
second order, 14, 17 
K-means segmented, 107 
Neural network, 76 
Neural networks, 8, 12–13 
K-means, 75,107 
Knowledge acquisition, 5, 107 
Noise, 81, 113 
Knowledge aquization, 69 
Non-convex, 12,23 
Knowledge base, 67–68, 71, 73, 76 
Non-stationary signal, 29 
Knowledge pyramid, 68 
Normalization constant, 14 
Knowledge-based, 43 
Normalization, 27 
Label, 66 
NP-hard, 5 1 
no-interpretation, 63.67 
Obstacle avoidance, 3 
Labelled image, 108
Octave, 24, 26 
Labelling, 2, 37, 44,107
Optimal interpretation, 6
Labels, 44, 60,67,71,75
Partial derivative,
84
Lagrange multipliers, 18 
Laplacian pyramid, 26 
Lattice, 14,22,26 
Partition, 99
Labelling algorithm, 107 
Optical flow, 2
Particle physics, 5 
Partition function,
14, 36, 83 

126
Partitions, 99 
manual, 5 
Pdf,
8, 44–45, 48,51–53,97 
Physiology, 1
Segmenting, 59
Planar graph, 44, 63 
Segments, 2, 4 
Point spread function, 18 
Semantic description, 4 
Polar coordinate, 113 
Semantic label, 7 
Posterior distribution, 19 ,21–23
Sensory data, 59
Posterior probability, 81–82 
Sequential approach, 9 
Potential function,16, 20, 22–23
Shape descriptors, 102
Primary features, 101 
Shape variance, 102 
Prioriprobability, 81
Sigmoidal function, 40
Probabilistic expect systems, 8, 43 
Signal decomposition. 113
Probabilistic reasoning, 8,44
Simulated annealing algorithm, 13, 24 
Probability distribution, 8, 38, 87 
Simulated annealing algorithm, 93 
Probability, 91
Simulated annealing, 12, 24, 42–44, 53 
Single node cliques, 69 
BAYESIAN APPROACH TO IMAGE INTERPRETATION 
Segmented image, 6, 9, 35, 38, 53, 64, 107, 114
joint, 5 1
Segmented, 108, 110 
priori, 81 
conditional, 81 
Singleton cliques, 52 
posterior, 81 
Spatial constraints, 6–7,43 
PSF, 18 
Spatial relationship, 3,114 
Psychophysics, 1,38
Spatial relationships, 60
Pyramid, 60,75 
Spatial resampling, 95 
Pyramids, 95 
Statistics, 81 
Pyramids, 95 
Steepest descent, 23 
Quadrature mirror filters, 31 
stereo, 2,11
RAG, 35,37–38
Stochastic relaxation, 43
Random optimization, 24 
Stochastic simulation algorithm, 9 
Random variable, 45
Stochastic simulation, 50
Random variables, 47 
Sub-band filtering, 31 
Randomized algorithms, 50 
Sub-sampling, 27 
Randomized schemes, 51 
Symmetry, 27 
Range image, 14 
Recognition, 11 
Taylor series, 85 
Reconstruction, 81 
Texture, 4 
Reed-Muller expansion, 85, 88 
Theorem 
Region adjacency graph, 35 
Relative frequency, 53
Hammersley-Clifford, 14, 17, 36, 83 
Relaxation algorithm, 67 
Time-domain, 29 
Remote sensing, 3,5, 79 
Resolution, 25, 30,63
Time-frequency, 29, 31 
fine, 67 
Training sequence, 99 
coarse, 67 
Training vector, 99 
Resolutions, 24,27,95 
Translational invariant, 14 
Retina image, 26 
Uncorrelated noise, 8 1 
Rotation invariant, 113 
Unsupervised learning, 80 
Rule based, 7 
Up sampling, 28 
Scale invariant, 113 
Upsampling, 27 
Scene interpretation, 2 
Variational approach, 12 
Scene understanding, 2 
Vector quantization, 99 
Secondary features, 101-102
Vertical edge, 22 
Segment, 61 
Vision modules, 2, 11 
Segmentation block, 6 
Vision perception, 2 
Segmentation module, 60,62,69 
Vision system, 11 
Segmentation, 2,5–6 , 9,11,38,54,60,64 
Vision
Segmentation, 68 
high-level, 2 
Segmentation, 69,72–74, 78,107 
low-level, 2 
non-octave, 76 
Statistically independent, 18 
Synergistic integration, 74 
Bayes, 81 
Time-frequency analysis, 29 

INDEX
127
modules, 2 
Wavelet filter, 64 
scope of, 2
Wavelet pyramid, 29, 31, 95 
Wavelet pyramids, 26 
Wavelet representation, 26 
Wavelet transform, 9, 26, 29, 61–62, 69, 72, 107, 
Visual sensors, 11 
Wavelet coefficient, 31 
Wavelet coefficients, 31 
Wavelet decomposition, 3 1 
113 
Wavelet domain, 29 
Wavelet, 107 

