Gerson J. Ferreira
Nanosciences Group www.inﬁs.ufu.br/gnano
Introduction to Computational Physics
with examples in Julia
DOI: 10.13140/RG.2.2.33138.30401
This is a draft!
Please check for an updated version on
http://www.infis.ufu.br/gerson
Currently, I’m not working on this text. I’ll start to review and
improve/ﬁnish some sections next semester.
Institute of Physics, Federal University of Uberlândia
October 5, 2016


Contents
Introduction
1
1
Introduction to the Julia language
3
1.1
Installing Julia (needs update) . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.1
JuliaBox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.2
Juno IDE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.1.3
iJulia
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2
Trying Julia for the ﬁrst time . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.1
The command line interface
. . . . . . . . . . . . . . . . . . . . .
7
1.2.2
Using scripts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.2.3
Other interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.3
Constants and variable types . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.3.1
Assertion and Function Overloading
. . . . . . . . . . . . . . . .
11
1.3.2
Composite Types (struct)
. . . . . . . . . . . . . . . . . . . . . . .
12
1.3.3
Tuples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.3.4
Arrays: vectors and matrices
. . . . . . . . . . . . . . . . . . . . .
14
1.3.5
Scope of a variable . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.4
Control Flow: if, for, while and comprehensions
. . . . . . . . . . . . . .
18
1.4.1
Conditional evaluations: if
. . . . . . . . . . . . . . . . . . . . . .
19
1.4.2
Ternary operator ?:
. . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.4.3
Numeric Comparisons . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.4.4
For and While Loops . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.4.5
Comprehensions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
1.5
Input and Output
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.6
Other relevant topics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.6.1
Passing parameters by reference or by copy (not ﬁnished) . . . .
23
1.6.2
Operator Precedence . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.7
Questions from the students . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.8
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
i

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
2
Differential and Integral Calculus
27
2.1
Interpolation / Discretization . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.2
Numerical Integration - Quadratures . . . . . . . . . . . . . . . . . . . . .
30
2.2.1
Polynomial Interpolations . . . . . . . . . . . . . . . . . . . . . . .
31
2.2.2
Adaptive and multi-dimensional integration . . . . . . . . . . . .
33
2.2.3
Monte Carlo integration . . . . . . . . . . . . . . . . . . . . . . . .
34
2.3
Numerical Derivatives
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.3.1
Finite differences and Taylor series
. . . . . . . . . . . . . . . . .
35
2.3.2
Matrix Representation . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.3.3
Derivatives via convolution with a kernel . . . . . . . . . . . . . .
38
2.3.4
Other methods, Julia commands and packages for derivatives
.
39
2.4
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3
Ordinary Differential Equations
43
3.1
Initial value problem: time-evolution . . . . . . . . . . . . . . . . . . . . .
44
3.1.1
The Euler method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.1.2
Runge-Kutta Methods . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.1.3
Stiff equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.1.4
Julia’s ODE package . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.2
Boundary-value problems . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
3.2.1
Boundary conditions: Dirichlet and Neumann . . . . . . . . . . .
53
3.2.2
The Sturm-Liouville problems . . . . . . . . . . . . . . . . . . . .
55
3.2.3
The Wronskian method . . . . . . . . . . . . . . . . . . . . . . . .
57
3.2.4
Schroedinger equations: transmission across a barrier . . . . . .
59
3.2.5
Non-linear differential equations
. . . . . . . . . . . . . . . . . .
61
3.3
The eigenvalue problem
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
3.3.1
Oscillations on a string . . . . . . . . . . . . . . . . . . . . . . . . .
63
3.3.2
Electron in a box
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
3.3.3
Method of ﬁnite differences . . . . . . . . . . . . . . . . . . . . . .
64
3.4
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4
Fourier Series and Transforms
69
4.1
General properties of the Fourier transform . . . . . . . . . . . . . . . . .
71
4.2
Numerical implementations . . . . . . . . . . . . . . . . . . . . . . . . . .
72
4.2.1
Discrete Fourier Transform (DFT) . . . . . . . . . . . . . . . . . .
72
4.2.2
Fast Fourier Transform (FFT) . . . . . . . . . . . . . . . . . . . . .
72
4.2.3
Julia’s native FFT
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
4.3
Applications of the FFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.3.1
Spectral analysis and frequency ﬁlters . . . . . . . . . . . . . . . .
75
ii

Contents
4.3.2
Solving ordinary and partial differential equations . . . . . . . .
76
4.4
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
5
Statistics (TO DO)
81
5.1
Random numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
5.2
Random walk and diffusion
. . . . . . . . . . . . . . . . . . . . . . . . . .
81
5.3
The Monte Carlo method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
6
Partial Differential Equations (TO DO)
83
6.1
Separation of variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
6.2
Discretization in multiple dimensions . . . . . . . . . . . . . . . . . . . .
83
6.3
Iterative methods, relaxation . . . . . . . . . . . . . . . . . . . . . . . . . .
83
6.4
Fourier transform (see §4.3.2) . . . . . . . . . . . . . . . . . . . . . . . . .
83
7
Plotting (not ﬁnished)
85
7.1
PyPlot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
7.1.1
Installing PyPlot . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
7.1.2
Using PyPlot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
7.1.3
Most useful features and examples . . . . . . . . . . . . . . . . . .
86
8
Other topics (TO DO)
91
8.1
Linear algebra
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
8.2
Root ﬁnding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
8.3
Linear systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
8.4
Least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
Bibliography
93
iii


Introduction
T
HESE ARE THE CLASS NOTES for the course “Computational Physics”, lectured to the
students of Bacharelado em Física de Materiais, Física Médica, and Licenciatura em
Física from the Instituto de Física, Universidade Federal de Uberlândia (UFU). Since
this is an optional class intended for students with diverse knowledge and interest in
Computational Physics and computer languages, I choose to discuss selected topics
superﬁcially with practical examples that may interest the students. Advanced topics
are proposed as home projects to the students accordingly to their interest.
The ideal environment to develop the proposed lessons is the Linux operational
system, preferably Debian-based distributions1. However, for the lessons and examples
of this notes we use the recently developed Julia2 language, which is open-source and
available to any OS (GNU/Linux, Apple OS X, and MS Windows). This language has a
syntax similar to MATLAB, but with many improvements. Julia was explicitly developed
for numerical calculus focusing in vector and matrix operations, linear algebra, and
distributed parallel execution. Moreover, a large community of developers bring extra
functionalities to Julia via packages, e.g.: the PyPlot package is plotting environment
based on matplotlib, and the ODE package provides efﬁcient implementation of adap-
tive Runge-Kutta algorithms for ordinary differential equations. Complementing Julia,
we may discuss other efﬁcient plotting tools like gnuplot3 and Asymptote4.
Home projects. When it comes to Computational Physics, each problem has a
certain degree of difﬁculty, computational cost (processing time, memory, ...) and chal-
lenges to overcome. Therefore, the home projects can be taken by group of students
accordingly to the difﬁculty of the project. It is desirable for each group to choose a sec-
ond programming language (C/C++, Python, ...) for the development, complementing
their studies. The projects shall be presented as a short report describing the problem,
computational approach, code developed, results and conclusions. Preferably, the text
should be written in LATEX to be easily attached to this notes.
Text-books. Complementing these notes, complete discussions on the proposed
topics can be found on the books available at UFU’s library[1, 2, 3, 4, 5, 6], and other
references presented throughout the notes.
Please check for an updated version of these notes at www.infis.ufu.br/gerson.
1Debian: www.debian.org, Ubuntu: www.ubuntu.com.
2Julia: www.julialang.com.
3gnuplot: www.gnuplot.info.
4Asymptote: asymptote.sourceforge.net.
1


CHAPTER 1
Introduction to the Julia language
Hello world!
J
ULIA is a recently developed (∼2012) high-level computer language designed for high-
performance numerical and scientiﬁc computing. The language has many advanced
features that we will not cover on this introductory class. Please check the ofﬁcial web-
site1 and the Wikipedia2 for more information. Here we will focus numerical calculus,
Fourier transforms, systems of equations, eigenvalue problems, random number gener-
ators and statistics, and plotting ﬁgures, which constitutes the core of an introduction
to scientiﬁc computing.
Keep in mind that Julia is a high-level language. Its performance is typically worse
than low-level languages like C by a factor of two. Still, this is an impressive performance
for a high-level language. Therefore Julia combines the easy-to-use features of high-
level languages like Matlab with the performance of low-level languages. Add to this the
fact that Julia is open-source and has a variety of available and useful packages (e.g.:
ODE, PyPlot) for beginners and experts, and Julia becomes the ideal language for an
introductory class.
In this Chapter we will cover only the fundamental aspects of the Julia language as a
introduction for the students with zero or little experience with computer languages.
For the more experienced student, I suggest you check the Tutorials on the Learning
section of Julia’s webpage.
1.1
Installing Julia (needs update)
1.1.1
JuliaBox
You don’t even need to install Julia! JuliaBox3 is an online server that allows you to
run your Julia codes on a remote machine hosted by the the Amazon WebServices. To
access this server you’ll need a Google account. Once you are in, there will be an iJulia
1Julia: www.julialang.org
2Wikipedia: https://en.wikipedia.org/wiki/Julia_(programming_language)
3JuliaBox: www.juliabox.org
3

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
notebook interface running on Jupiter (iPython). You can easily upload, download and
edit your codes. Moreover, you can sync your ﬁles with Google Drive or GIT.
In JuliaBox you can run your code on the Amazon server for free up to six hours.
After that your virtual machine will be shut down and you have to login again.
Unfortunately, my personal experience with JuliaBox is that it is not stable yet. It’s
still beta. The kernel seems to die often, but it could be a problem with my network
(proxy?). Despite this, I encourage you to try it. Once this is stable, it will be a great tool.
MS Windows and Apple OS X
I highly recommend you to use Linux. However, if you are stubborn enough I allow
you to waste time with another OS. For MS Windows and Apple OS X (at least this
one is UNIX-based) the installation follows usual procedures. Please visit the Down-
load section on Julia’s webpage to ﬁnd the newest version of the installation ﬁles and
instructions.
Debian/Ubuntu/Fedora Linux
You can also ﬁnd the installation ﬁles for Linux distributions on the Downloads section
of Julia’s webpage. However, one of the great advantages of using Linux is the advanced
packaging system that helps you keep your software always up to date. Many third-party
applications will have PPA repositories for Ubuntu Linux. So here I’ll quick teach you
how to use them.
If you are new to Linux. Try Ubuntu. Their PPA repositories are great.
Ubuntu Linux
Julia’s PPA repository 4 is ppa:staticfloat/juliareleases. To add it to your list of
repositories and install Julia, please open the Terminal and follow the commands below.
Please read the output of each command as it may request you to accept the changes.
Example 1.1: Installing Julia in Ubuntu Linux
sudo apt-add-repository ppa:staticfloat/juliareleases
sudo apt-get update # this command updates the package list
sudo apt-get install julia
Depending on the Ubuntu version you are using, you may also need to add the PPA
for Julia’s dependent packages: ppa:staticfloat/julia-deps.
That is it! If all goes well, Julia is already installed. If there is a new version and you
want to upgrade, go again to the terminal and run:
4Julia’s PPA: https://launchpad.net/~staticfloat/+archive/ubuntu/juliareleases
4

Introduction to the Julia language
Example 1.2: Upgrading your installed software
$ sudo apt-get update
$ sudo apt-get upgrade
As a suggestion for the new Linux users, there are a few graphical interfaces for the
APT packaging system. I recommend you to use Synaptic. If it is not already installed,
simply update your package list as in the examples above, and run sudo apt-get
install synaptic.
If you are new to Linux, get ready to be comfortable using the Terminal as it makes
your life much easier. For instance, many of the commands above are probably new to
you. To learn more about them, you can check the man-pages on the Terminal. Please
check and read the content of the commands below (to exit press q).
Example 1.3: Using the man-pages
$ man sudo # execute a command as another user
$ man apt-get # APT package handling utility -- command-line interface
$ man julia # launch a Julia session
$ man synaptic # graphical management of software packages
$ man man # an interface to the on-line reference manuals
$ man bash # GNU Bourne-Again SHell
Debian Linux
Unfortunately you cannot ﬁnd Debian packages of Julia in their main webpage. In
Debian Jessie (stable) repository you will ﬁnd an old version of Julia (0.3.2 the last
time I’ve checked). To get the newest version of Julia, you will need Debian Stretch
(testing) or Sid (unstable). It could be tricky to install a Stretch package in Jessie, so I
don’t recommend this. If you use Debian Jessie, the best choice is to use the Generic
Linux binaries from Julia’s webpage Download section.
Here’s my suggestion on how to install the Generic Linux Binaries:
First go to Julia’s webpage Download section and get the ﬁle for the 64bit or 32bit
version, depending on your computer architecture. The ﬁle name will be something like
julia-0.4.3-linux-x86_64.tar.gz. The .tar.gz extension indicates a compressed
ﬁle (like a zip ﬁle). You may save it to your Downloads folder.
Open the Terminal. Next we will extract the ﬁles from the .tar.gz ﬁle and move it
to a better location. Follow these commands:
5

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Example 1.4: Julia: manual installation, part 1
# Change Directiory to where you have saved the .tar.gz file:
$ cd Downloads
# extract the contents:
$ tar -xzf julia-0.4.3-linux-x86_64.tar.gz
# list the files within the current directory:
$ ls
The last command, ls, will list the ﬁles on the directory. Among other personal
ﬁles of yours, you’ll see the .tar.gz ﬁle and a new folder with a name similar to
julia-a2f713dea5. Let’s now move it to a better location, back to the Terminal:
Example 1.5: Julia: manual installation, part 2
# let’s move it to a different place. The dot makes it a hidden folder
$ mv julia-a2f713dea5 ∼/.julia
# open the .bashrc with your favorite text editor
$ gedit ∼/.bashrc
# to add Julia’s binary to the path where the system looks for binaries,
# add the following line at the end of the file
export PATH=$PATH:$HOME/.julia/bin
# save the file and close the text editor
Great! If all goes well, close your Terminal and open it again. Every time the Terminal
(BASH) starts, it runs the content of .bashrc. The line we have added to this ﬁle tell the
system to search Julia’s folder for binaries. Therefore, now you can start Julia in bash
running $ julia.
1.1.2
Juno IDE
Juno5 is a powerful IDE (Integrated Development Environment) built speciﬁcally for
Julia. Please check its webpage for installation instructions for MS Windows, Apple OS
X, and Linux. Unfortunately, up to now there’s no ofﬁcial repository for Juno on Linux,
so you have to install it manually.
1.1.3
iJulia
iJulia6 provides the Jupiter interactive environment for Julia (similar to iPhyton). This is
equivalent to the interface of JuliaBox, but this one runs locally on your computer. To
install it, go to the Terminal, start Julia’s command line interface and run:
5Juno: junolab.org
6iJulia: github.com/JuliaLang/IJulia.jl
6

Introduction to the Julia language
Example 1.6: Installing iJulia
First, open the Terminal and start Julia
$ julia
Next, install iJulia with the Pkg.add command
julia> Pkg.add("IJulia")
To start and use iJulia, follow these commands:
Example 1.7: Using iJulia
First, open the Terminal and start Julia
$ julia
Next, start iJulia
julia> using IJulia; notebook();
The commands above will start the iJulia server, which you have to access via you
Internet browser at 127.0.0.1:8888.
1.2
Trying Julia for the ﬁrst time
First of all, I’m old fashion. So I prefer to run my codes on the command line interface,
while I edit my code using Emacs7. Emacs can be complicated for a beginner, while
it becomes a incredible tool as you get experience. For now, you can use simpler text
editors like gEdit or Kate. Of course, if you are using Juno, JuliaBox, or iJulia interface,
you won’t need a text editor. But ﬁrst let’s learn how to get around the command line
interface, and later on we’ll start with the codes and scripts.
1.2.1
The command line interface
Open the Terminal and start Julia.
Example 1.8: Julia’s initial screen
_
_ _(_)_
|
A fresh approach to technical computing
(_)
| (_) (_)
|
Documentation: http://docs.julialang.org
_ _
_| |_
__ _
|
Type "?help" for help.
| | | | | | |/ _’ |
|
| | |_| | | | (_| |
|
Version 0.4.2 (2015-12-06 21:47 UTC)
_/ |\__’_|_|_|\__’_|
|
Official http://julialang.org release
|__/
|
x86_64-linux-gnu
julia>
7Emacs: http://www.gnu.org/software/emacs/tour/.
To install Emacs on Ubuntu: sudo apt-get install emacs.
7

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
The ﬁrst thing you see is a message on how to ask for help. Very encouraging! Since
we don’t know anything, let’s ask for help. Follow the instruction on the screen. Type
“?help” for help. Great! From the command line there’s always an easy access to a short
documentation that you can use to check what an speciﬁc Julia command does and
what parameters does it take.
Try some simple commands:
Example 1.9: Simple commands
julia> 2+2
julia> sin(30)
julia> sin(30*pi/180)
julia> sin(30pi/180)
julia> pi
julia> a = 2^4;
julia> a
julia> println("The value of a is ", a);
julia> ?println
Above you can see that Julia (as any other computing language) takes angles in radi-
ans, there’s a pre-deﬁned constant for π, the exponentiation is done with the circumﬂex
symbol (xy = x^y), you can assign value to variables with =, print text and values on
the screen with println, and if you ﬁnish a line with a semicolon (;), it suppresses the
output. That’s a good start.
In the examples above, note that you don’t need to type the multiplication symbol *
between numbers and variables. Julia understands the implied multiplication. This is a
nice trick to keep your expressions short and easy to read.
Let’s try another basic set of commands:
Example 1.10: Functions and memory use
julia> a = 0; # defines and attributes value to the variable
julia> f(x,a) = exp(-x/a)sin(x); # defines f (x,a) = e−x/a sin(x)
julia> f(1,1) # calculates the function for the specific values
julia> whos() # prints the memory use
julia> workspace(); # cleans the user-defined varibles
julia> f(1,1) # gives an error, since f(x,a) is not defined anymore...
julia> whos() # ... as you can see here.
Here we are deﬁning a function f(x,a) that takes two parameters. Note that even
tough we have set the variable a=0 on the previous line, the function deﬁnition does
not use this value, since it belongs to a different scope.
The commands whos() and workspace() are very helpful to keep track of memory
use, and to clean all user-deﬁned values and start again when needed.
8

Introduction to the Julia language
Essential commands, auto-complete, and command history
Here’s a list of other essential commands to keep in mind:
• Everything that follows # is a comment and it is ignored;
• To exit Julia’s command line interface: quit(), or exit(), or press CTRL+D (= ^D).
As usual, let’s use the circumﬂex symbol ^ to refer to CTRL key for short notation.
• To interrupt a computation, press ^C;
• Press ; at an empty line to go back to BASH temporally.
• To run a script ﬁle, run include("file.jl"). Always use the .jl extension so
your text editor recognizes that your are writing a Julia script;
• Use the TAB key to complete what you are typing (or a list of suggestions). Try to
type "in" and press TAB twice, you’ll see a list of possible commands that start
with "in". Continue the command pressing c to form "inc" and press TAB again.
It’s magic!
• You can go through your commands history pressing the up and down arrows of
your keyboard. Also, you can search the command history pressing ^R and typing
any part of the command you want to recall.
1.2.2
Using scripts
The command line interface is great for quick calculations, test codes, plot ﬁgures, etc.
However, in practice we should always have our code stored in a ﬁle somewhere. This is
the script. While in low-level compiled languages we refer to the codes as the “source
code”, for high-level interpreted languages, it is called a “script”.
There’s no secret here. Through the classes we will learn Julia commands and struc-
tures, try some codes and solve some problems. A script ﬁle is just a text ﬁle with the
sequence of commands you want to run. Julia scripts should end with a .jl extension,
so that your script editor recognizes it is a Julia script. In Linux, I recommend the be-
ginners to use one of these editors: Kate or gEdit. The ﬁrst is part of the KDE ensemble,
while the second belongs to Gnome. For those with more experience, try to learn Emacs.
In the next section we will start to write more complicated functions, so I invite you
to code them in a script ﬁle instead of the command line as we were doing so far. Simply
open your favorite editor, type the code, and save it with the .jl extension. Let’s say
your ﬁle is script.jl. There’s two ways to run the script:
1. In the Terminal, start Julia and run: julia> include("script.jl");
2. In the Terminal, do not start Julia! From the shell, run: $ julia script.jl
What’s the difference between these two cases?
9

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
1.2.3
Other interfaces
During the classes we will use mostly the command line interface and scripts. Therefore
I will not cover here a guide to use the other possible interfaces: JuliaBox, Juno IDE, and
iJulia. These are easy-to-use interfaces, so after learning about the command line and
scripts, you will probably have no difﬁcult using the others.
1.3
Constants and variable types
Julia comes with a few predeﬁned mathematical constants:
• π = pi = 3.1415926535897...;
• e = e or eu = 2.7182818284590..., Euler’s number, base for the natural logarithm;
• For complex numbers, use im for the imaginary part. For example: z = 5 + 3im;
• True and false constants are true=1, and false=0;
The variable types in Julia are dynamic. This means that you don’t have to declare it
beforehand. For instance:
Example 1.11: Variable types
julia> a = 8
julia> typeof(a) # will tell you that "a" is 64bit integer
julia> sizeof(a) # used memory in bits
julia> a = 8.0
julia> typeof(a) # now "a" is a 64bit floating-point number
julia> sizeof(a)
julia> a = 8.0 + 3.0im
julia> typeof(a) # is a complex floating-point number
julia> sizeof(a)
julia> a = 8 + 3im
julia> typeof(a) # is an integer floating-point number
julia> sizeof(a)
julia> a = true
julia> typeof(a) # is a Boolean
julia> sizeof(a)
The difference between the types is how the variable is stored in memory. A ﬂoating-
point number is the computer version of the Real numbers8. Float64 means that Julia
is using the double precision format (64 bits), which uses one bit for the sign, 11 bits for
the exponent, and 52 bits for the fraction. Since 1 byte = 8 bits, each Float64 variable
will need 8 bytes memory. For instance, if you have a 100×100 real matrix, there will
8For more details, check Wikipedia’s entry on the double precision (64bits) ﬂoating-point num-
bers: https://en.wikipedia.org/wiki/Double-precision_floating-point_format, and a gen-
eral discussion on ﬂoating-point numbers https://en.wikipedia.org/wiki/Floating_point.
10

Introduction to the Julia language
be 1002 Float64 numbers to store, consuming 80.000 bytes = 80 kB. You can check the
memory use of any variable with the command sizeof(...). Evidently, a 64 bit integer,
e.g. Julia’s Int64, also needs 8 bytes of memory. So, what’s the advantage of having both
Float64 and Int64 types of variables? I’ll leave this to you as a problem.
Note that the complex variable takes twice the memory of a real or integer variable.
It has to store both the real and imaginary part. Boolean variables uses only a single bit!
Since it can be only true or false, a single bit sufﬁces.
1.3.1
Assertion and Function Overloading
Type assertion is particularly useful when deﬁning functions. It can be used to assure
that the functions receives an argument of a certain type. For instance, let’s deﬁne a
function that calculates the factorial using a loop:
Example 1.12: Factorial function with assertion
function fact(n::Int64)
res = 1; # initialize the result as 1
for i=n:-1:1 # loop the variable i from N to 1 in steps of -1
res = res*i; # since n! = n ·(n −1)·(n −2)···(1)
end # end loop
return res; # return the result
end
In the ﬁrst line the argument of the function fact(n::Int64) is declared as an Int64.
If you try to call this function as fact(4), you will get the correct result, 24. But if you
call fact(4.0), it fails. The function is not deﬁned for an arbitrary argument.
If you have a function that for different types of parameters may take different
forms, you can use function overloading. This is an extremely useful feature of modern
computer languages. For instance, the factorial of a real number can be deﬁned via the
Riemann’s Γ function,
Γ(n) =(n −1)!, for n ∈Z+,
(1.1)
Γ(t) =
Z ∞
0
xt−1e−xdx, for t ∈R.
(1.2)
We, humans, use the same letter Γ for these functions. But the expression used to
calculate the result depends on the parameter being integer or real. Function overload-
ing is the computer language way of making the decision between the two forms of Γ
above. Check the next example.
11

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Example 1.13: Function Overloading
function Gamma(n::Int64)
return fact(n-1);
end
function Gamma(t::Float64)
println("We’ll learn how to calculate numerical integrals later...");
end
Try to deﬁne all these functions in Julia. Simply copy and paste the code from here.
Now, if you call Gamma(5) the result will be 24 again. But for Gamma(5.0) you will get the
message above from println. Simply call Gamma with no parameters or parenthesis, and
you see the message “Gamma (generic function with 2 methods)”. The different
methods are the different implementations of Gamma with distinct type or number of
parameters.
Please note that this is a simple illustrative example. In practice Julia has built-in
functions to calculate both the factorial and Riemann’s Γ. Built-in are always more
efﬁcient.
1.3.2
Composite Types (struct)
Composite types are equivalent to the structs in C language. It is usually useful when
you want to store different information about an object in a single variable.
Since we are taking a Computation Physics class... let’s exemplify the composite
types as a way to store the position and momenta of particles.
Example 1.14: Composite Types: particle position and momenta
Say that you need to store the position (x, y) and momenta (px,py) of all
habitants of ﬂatland[7]. We can deﬁne a type of variable that we choose to
name “Particle”:
type Particle
x::Float64
y::Float64
px::Float64
py::Float64
end
Now we can create many particles calling
q1 = Particle(0.0, 0.0, 0.0, 0.0); # resting at origin
q2 = Particle(10.0, 10.0, -1.0, -1.0); # in a collision course with q1
To avoid the collision, let’s update the position of particle q1:
q1.x = 1; # now q1 is still at rest, but now on (x,y) = (1,0)
12

Introduction to the Julia language
In the example above we have deﬁned a new type of variable (Particle), and we
create two variables of this type representing particles q1 and q2. The parameters (posi-
tion and momenta) are the “ﬁelds” of this new variable type. The parameters passed to
these variables when deﬁning q1 and q2 are called the “initialization”. Once the variables
exist, you can access and modify its contents using the construct variable.field, as
in the example where we update the position of particle q1.
This is a silly example, but eventually we will use this to help us solve the motion of
planets using Newtonian dynamics.
Instead of the declaring the composite type using the type keyword, you could
also work with the immutable, i.e. immutable Particle ··· end. Immutable types work
as constants, they cannot be modiﬁed. The main difference between “mutable” and
immutable types is that the ﬁrst is passed by reference to a function, while the second is
passed by copy9.
1.3.3
Tuples
In mathematics, a tuple is a ordered list of elements. For instance, a vector r can
be written as 3-tuple: (x, y,z). Julia’s documentation describes this type of data as an
abstraction of the arguments of a function.
Example 1.15: Tuples: square root
function squareroot(x::Number)
return (+sqrt(x), -sqrt(x)) # returns a tuple
end
sols = squareroot(9);
println("The square roots are ", sols[1], " and ", sols[2]);
println("The number of square roots is ", length(sols));
sols[1] = 0; # will return an error
In the example above, while the sqrt(···) native command returns only the positive
square root, our example function returns a tuple with all square roots. The tuple is
stored in the variable sols, and each value can be accessed via the construct sols[1],
sols[2], ··· sols[i] ··· sols[n], where the number of elements in the tuple can be
checked with the length command. In the simple example above, we will always have
two entries in the tuple. However, some codes may return a tuple of an arbitrary number
of entries. In this case you may use the command length to check the number of
elements.
In Julia, a tuple is an immutable type: you cannot change its contents, and it will be
passed to a function by copy. Therefore tuples are appropriated when you need to pass
9If you are a beginner, you probably don’t know what “passing by reference/copy” means. Don’t
worry, we’ll get to that.
13

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
vector or matrix parameters by copy, instead of by reference. Apart from this, they are
equivalent to arrays.
1.3.4
Arrays: vectors and matrices
As we have seen above, mathematically and array is a tuple. In Julia an array is a mutable
tuple. To distinguish these two, tuples are deﬁned by parenthesis as pos = (x,y), while
arrays are set by square brackets as pos = [x, y]. Essentially, both tuples and arrays
could be used to represent vectors and matrices. However, when we are working with
vectors and matrices, we will probably need to update their values along the calculation.
Therefore we need a mutable type: the array.
Let’s deﬁne a vector and a matrices and try some common operations.
Example 1.16: Arrays: vectors and matrices
veclin = [1 2 3] # this is a line vector (1×3)
veccol = [1; 2; 3] # this is a column vector (3×1)
matA = [4 5 6; 7 8 9; 10 11 12] # a 3×3 matrix
matB = [3 2 1; 6 5 4; 9 8 7] # another 3×3 matrix
matA*veccol # (3×3)·(3×1) = (3×1)
veclin.’ # transpose the vector
veccol’ # conjugate transpose the vector
matA’ # conjugate transpose the matrix
inv(matA) # calculates the inverse
matA*matB # mathematical product of the matrices
matA.*matB # direct element by element product
Run the example above line by line and check the outputs to understand what
each command does. You will see that there’s no difference between the conjugate and
the transpose conjugate here, because we are dealing with real values. Try deﬁning a
complex matrix or vector and check again the difference between these two commands.
There’s a lot more you can do with vectors and matrices of course. We’ll learn
more tricks as we go along. For instance, a nice trick to construct matrices is the
comprehension construct that we present in Section 1.4.5. Other relevant commands
that we may use later on are:
• zeros(n): creates an array of zeros with n elements;
• zeros(n,p): creates a n × p matrix with all elements set to zero;
• ones(n), and ones(n,p): equivalent to zeros, but all elements are set to 1;
14

Introduction to the Julia language
• eye(n): creates the identity matrix of order n. This is different than ones(n,n);
• rand(n), and rand(n,p): array or matrix of random values uniformly distributed
between [0,1);
• randn(n), and randn(n,p): equivalent to rand, but with a standard normal
distribution (mean value = 0, and standard deviation = 1);
• linspace(start, stop, n): creates a range from start to stop with n ele-
ments. To convert the range into an array, use the collect command;
• norm(v): returns the norm of vector v;
• cross(x,y): calculates the cross product x × y;
• diagm(v[, k]): constructs a matrix with the k-th diagonal set to the vector v
and all other elements to zero;
• trace(M): returns the trace of the matrix M;
• det(M): returns the determinant of the matrix M;
• inv(M): inverse of the matrix M;
• eig(M): calculate the eigenvalues and eigenvectors of the matrix M;
Indexing an array or matrix
To access or change the value of a matrix or array element, use matA[i,j] and veclin[i],
respectively. The ﬁrst access the element (i, j) of matrix matA, the second access the
element i of vector veclin from the example above. You may also use ranges to access
multiple elements. Check the example:
Example 1.17: Indexing arrays and matrices
# creates a random 6×6 matrix. Check the output
M = randn(6,6)
M[2,3] # returns element of the 2nd row, 3rd column
M[2:4, 5] # returns rows 2 to 4 in column 5
M[:, 2] # returns column 2
M[3, :] # returns row 3
M[1, end] # returns element in the last column of row 1
15

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Concatenation
Array concatenation in Julia follow the vector and matrix constructions as shown in the
example below. Additionally, one may also explicitly call the *cat-functions: cat, vcat,
hcat, hvcat.
In the next example we start with a vector x and add more elements to it, either
increasing its size, or transforming it into a matrix. Please check carefully the outputs of
each line to understand the different types of concatenations.
Example 1.18: Concatenation
x = rand(3); # initialized as a column vector 3x1
# repets x and adds two more elements, making it 5x1
y = [x; 10; 20];
# the line above is equivalent to the vcat: vertical concatenation
y = vcat(x, [10; 20]);
# creates another 5x1 vector
z = rand(5);
# creates a 5x2 matrix setting y as the first column and z as the second
m = [y z];
# the line above is equivalent to hcat: horizontal concatenation
m = hcat(y, z);
Keep in mind that concatenation can be useful to store results in a list of initially
unknown size. You can concatenate new results of some calculation into the list as
needed. However, this is not efﬁcient for large vectors, in which case it is better to
known the size of the vector and initialize it beforehand.
In the next example we calculate the average and standard deviation of a list of
random numbers as a function of the number of random values sampled. This is not
the most efﬁcient way to calculate and store the results, but exempliﬁes an interesting
way of using concatenation.
16

Introduction to the Julia language
Example 1.19: Concatenation to store results on demand
# using the PyPlot package that will be introduced in Section 7.1
using PyPlot
# initialize empty arrays of Float64 to store the resuts
average = Float64[];
stddev
= Float64[];
n = 1; # initialize counter of sampled random numbers
# the average of a uniform random distribution should be 0.5
# let’s run the loop until we reach this limit
while length(average)==0
abs(average[end]-0.5) > 1e-6
n += 1; # increment the counter
list = rand(n); # sample a new list
average = [average; mean(list)]; # concatente new average to the list
stddev
= [stddev;
std(list)]; # and the same for the std. deviation
end
# plot the average and standard deviation as a function of
# the number of sampled points
subplot(211)
plot(average)
subplot(212)
plot(stddev)
1.3.5
Scope of a variable
The scope of a variable refers to the region of the code where the variable is accessible.
You may, for instance, deﬁne two functions of x, say f (x) and g(x). Here x is suppose
to be a free variable, being assigned to a value only when you call the functions; e.g. if
you call f (3) the code inside the function f (x) uses x = 3. From a mathematical point of
view this is quite obvious, right?
What about for a computer? Is it obvious? Remember that a computer does whatever
you code him to do. Therefore the deﬁnition of the scope of a variable is extremely
important. Otherwise you and the computer could have some misunderstandings.
Check the example:
17

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Example 1.20: Scope of a variable
x = 3; # global definition of x
# definition of f(x): here x is local. The scope is the function.
f(x) = exp(-x)sin(x);
# definition of g(y): here x takes the global value, y is local.
g(y) = sin(x*y);
# definition of h(y): x, dx, and ans are local variables
function h(y)
x = 0;
dx = 0.01;
res = 0;
while x <= y
res = res + x*dx;
x = x + dx;
end
return res;
end
x # check the value of x
f(0) # will return 0
f(1) # will return 0.3095598756531122
f(x) # will return 0.00702595148935012
h(2) # will return 1.99
x # still has the same value. Calling h(2) didn’t change it.
g(10*pi/180) # will return 0.49999999999999994
x = pi/180;
f(x) # now returns 0.017150447238797207
g(30) # returns 0.49999999999999994
h(2) # still returns 1.99
Do you understand all outputs of the example above? Please try to follow what
happens at each line, and what value of x is used. The variable x is ﬁrst initialized in the
global scope (x = 3). However, when we deﬁne a function, its arguments are threated
as local variables. Therefore the variable x that appears in f (x) is not the same as the
global x. It has the same name, but refers to a different scope. Check Problem 1.4.
Besides the functions, new scopes are initiated every time you start a new code
block; e.g. for and while loops.
1.4
Control Flow: if, for, while and comprehensions
In a vector oriented language one should always try to avoid using loops (e.g.: for,
while). Also, it’s always better to avoid conditional evaluations (if). But it is not always
18

Introduction to the Julia language
possible. So let’s check how to use them in Julia.
1.4.1
Conditional evaluations: if
The deﬁnition of the if construction follows the common if-elseif-else syntax:
if x < y
println("x is less than y")
elseif x > y
println("x is greater than y")
else
println("x is equal to y")
end
As usual, it checks if the ﬁrst statement is true (x < y), if so, it runs the ﬁrst block.
Otherwise it goes to the second condition (elseif). If all conditions are false, it runs
the else block. You may have many elseif conditions if you need it... or none. Both
elseif and else blocks are optional.
One can also use Short-Circuits (AND: &&, OR: ||). The && is the boolean AND
operator. For instance, (testA && testB) will return TRUE only if both testA and
testB are TRUE, and it returns FALSE otherwise. For efﬁciency it ﬁrst evaluate testA, if
it returns FALSE, Julia does not have to evaluate testB. Do you agree?
What about the OR operator ||? What is the outcome of (testA || testB) for
different results of testA and textB?
Write and run the following code. It uses a comprehension to create a matrix. We’ll
learn about comprehensions later, but you can probably guess what it does just by
reading the code. Try to understand the content of the matrices Tand and Tor.
Example 1.21: Truth tables
tf = [true; false];
Tand = [ x && y for x in tf, y in tf ]
Tor = [ x || y for x in tf, y in tf ]
1.4.2
Ternary operator ?:
The ternary operator (a ? b : c) is simply a short syntax for if statements that can
be put in a single line. Here a is the boolean test, b is the code to run if a is TRUE,
and c is the code if a is FALSE. For instance, the next example shows two different
implementation of a function that calculates the absolute value of x.
19

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Example 1.22: Ternary operator: absolute value
function absolute(x)
if x >= 0
return x
else
return -x
end
end
ternaryabsolute(x) = (x>=0) ? x : -x;
1.4.3
Numeric Comparisons
Comparison operators follow standard notation:
• Equality a == b, tests if a is equal to b;
• Inequality a != b, tests if a and b are different;
• Less than: a < b;
• Less than or equal to: a <= b;
• Greater than: a > b;
• Greater than or equal to: a >= b;
It’s possible to compare numbers to inﬁnity (Inf = ∞), or undeﬁned results (NaN
= not a number). For instance, (1/0 == Inf) returns TRUE, and (0/0 == NaN) also
returns TRUE. Other comparisons can be made with the help of predeﬁned functions:
• iseven(a), returns TRUE if a is even;
• isodd(a), returns FALSE if a is odd;
• Other useful tests: isinteger(a), isnumber(a), isprime(a), isreal(a),
isfinite(a), ...
Essentially, every predeﬁned function that starts with is... is a boolean test. To
ﬁnd others, open the command line interface, type ?is and press TAB twice.
1.4.4
For and While Loops
In Julia you can create repeated evaluations using for and while block codes. They are
both quite intuitive, but let’s take a chance to introduce some new commands frequently
used together with loops.
The while loop simply runs the block while the test remains true. For instance:
20

Introduction to the Julia language
Example 1.23: While loop
n = 0; # initialize n
while n <= 10
println("Running for n=", n);
n += 2; # update n, incrasing by 2
end
When we use while, usually a variable used for the stop condition must be initial-
ized outside, and updated inside the block. Here we initialize with n=0, and update it
incrementing as n += 2, which is a short notation for n = n + 2.
The code above could be better written with a for statement:
Example 1.24: For loop
for n=0:2:10
println("Running for n=", n);
end
Here we use the colon operator (start:step:end) to initialize n = 0, update its
value in steps of 2 until it reaches 10. We have used this form in the implementation of
the factorial function in Example 1.12.
You may also use for to run through lists. The example is self-explanatory:
Example 1.25: For each in list
# using an array to store a list of values
numericlist = [ 1, 10, 2, 8, 20, 0 ];
for each in numericlist
println("each = ", each);
end
stringlist = [ "this", "may", "also", "be text" ];
for txt in stringlist
println(txt);
end
1.4.5
Comprehensions
Comprehensions use the structure of the for loop to easily create arrays. For instance,
Example 1.21 uses comprehensions to create the Truth Tables. The general form is
A = [ f(x,y) for x=rangeX, y=rangeY ];
A = [ f(x,y) for x in listX, y in listY ];
21

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
The ﬁrst variable after the for refers to the line, and the second to the column. It is
possible to go further into multi-dimensional arrays (tensors), but let’s keep it simple
for now. The extension to n-dimensions is evident.
1.5
Input and Output
Input and Output (I/O) is essential for any code. You will almost always need to save
your data on a ﬁle, and sometime you need to read some data from somewhere. The
basic commands to read ﬁles are: readdlm and readcsv. To write a ﬁle: writedlm and
writecsv. Check their help-description in Julia.
Here ...dlm stands for “delimited”, while ...csv is “comma-separated values”.
Let’s ﬁrst assume you have your data stored in the ﬁle “foo.dat” as a matrix, such
each line of the ﬁles correspond to a line of the matrix, and the matrix columns are
separated by white spaces. You can read it running: julia> a = readdlm("foo.dat"). The
data will be stored in the matrix (array) a.
Example 1.26: Reading and writing files
Using your favorite text editor, create a data ﬁle “foo.dat” and ﬁll with a
5×5 numerical matrix of your choice. This matrix may have integers or real
numbers. Separate the numbers in each line using spaces. Now, in Julia’s
command line interface, run:
julia> data = readdlm("foo.dat")
You should see something like this:
julia> data = readdlm("foo.dat")
5x5 Array{Float64,2}:
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
8.0
9.0
10.0
11.0
12.3
13.0
14.0
15.0
16.0
17.3
18.2
19.0
20.1
21.0
22.3
24.0
25.0
Now the data is stored in the variable data. Let’s multiply this data by 2, and
save it in a different ﬁle. But now the columns will be separated by commas.
julia> writedlm("bar.dat", 2*data, ’,’);
Open the ﬁle “bar.dat” in the text editor to see the result. This is the format
of a CSV ﬁle. You would get the same result running:
julia> writecsv("bar.dat", 2*data)
22

Introduction to the Julia language
1.6
Other relevant topics
1.6.1
Passing parameters by reference or by copy (not ﬁnished)
1.6.2
Operator Precedence
The operator precedence rules deﬁne the order that operations are evaluated. For
instance, what is the result of 2+3×4? You are probably getting 14 and not 20, right?
You know that you should multiply 3×4 ﬁrst, and them sum 2. How do you know that?
Someone has told you that multiplication takes precedence over addition. If we want to
add 2 and 3 ﬁrst, and then multiply the result by 4, you should write (2+3)×4. This one
give us 20.
Sometimes it might be difﬁcult to remember the precedence rules. For instance,
what’s the result of (10/2×5)? Is it 1 or 25? To avoid problems, it is always better to use
parenthesis: (10/(2×5)) or ((10/2)×5).
In Julia, the most common operations follow this order of precedence
1. Exponentiation ^ or the elementwise equivalent .^
2. Multiplication ∗and division / or the elementwise equivalents .∗and ./
3. Addition + and −
4. Comparisons: > < >= <= == ! =
For more details and the full list of operations, please check Julia’s documentation.
1.7
Questions from the students
Here I’ll list some questions from the students that I couldn’t answer during the class.
Question [1]
How to read a data ﬁle that has complex numbers?
Answer:
The readdlm command will return a string "3+1im" instead of the complex
number 3+1im. In this case the data ﬁle will be read as type Any. Below I wrote two
codes to overcome this.
In the ﬁrst code I read the data as ASCIIString and use parse and eval to evaluate
the expression in Julia as if it were a code. For instance, if one entry of the data ﬁle is
"2+2", it will actually be evaluated to 4. Here’s the code:
data = map(x->eval(parse(x)), readdlm("data.dat", ASCIIString));
data = convert(Array{Complex{Float64},2}, data);
The problem with the code above is that it ﬁrst reads everything as Strings. A second
choice would be:
23

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
myparse(x) = (typeof(x) == SubString{ASCIIString}) ? eval(parse(x)) : x;
data = map(x->myparse(x), readdlm("data.dat"));
data = convert(Array{Complex{Float64},2}, data);
This version reads the data as type Any and uses the user-deﬁned function myparse(x)
to convert to a number only the Strings.
However, a better choice would be to save the real and imaginary parts separated
in the ﬁrst place. If your data is a n-columns ﬁle, save real part in one column and the
imaginary in the next. If you are saving matrices, save the real and imaginary in different
ﬁles. This way is probably more efﬁcient, since for large ﬁles the parse(x) command
will probably be too slow.
Question [2]
How to choose the line color in PyPlot?
Answer:
Please check the Python and Matplotlib installation. The simple code tested
in class should work:
using PyPlot
x = linspace(0,2pi,50);
plot(x, sin(x), color="red", linewidth=2.0, linestyle="-")
plot(x, cos(x), color="blue", linewidth=2.0, linestyle="--")
Please check the updated PyPlot installation section above.
Question [3]
What’s the difference between Tuples and Arrays?
Answer:
Tuples are immutable and array are mutable. The example in the text and
problem proposed below do not explore this difference. I’ll replace them with a better
discussion soon. The main difference is that a Tuple will be passed to functions by copy,
while arrays are passed by reference. But I still have to conﬁrm this with some example.
1.8
Problems
Problem 1.1: Int64 and Float64
If both formats Int64 and Float64 use the same amount of memory, why having
both deﬁned instead of always using Float64? Check, for instance, Ref. [2] and the
Wikipedia pages mentioned earlier.
Problem 1.2: Bhaskara
Write a function named bhaskara that receives three parameters a, b and c repre-
senting the coefﬁcients of the quadratic polynomial equation ax2+bx +c = 0. Calculate
the roots r1 and r2, returning them as a Tuple. Try the code below to test your imple-
mentation:
24

Introduction to the Julia language
julia> root1, root2 = bhaskara(2.0, -2.0, -12.0);
julia> println("The first root is ", root1)
The first root is 3.0
julia> println("The second root is ", root2)
The second root is -2.0
Problem 1.3: Scripts
Write a code of your choice as a script (could be one of the examples or problems
here). Try to run it using both methods described in Section 1.2.2. What is the difference
between these two methods?
Problem 1.4: Scope of a variable
a) Go back to Example 1.20. Save it as a script ﬁle. Run it and check that the values
match the numbers in the example. Can you explain the outputs?
b) In the ﬁrst line of the deﬁnition of the function h(y)
replace: x = 0
with: global x = 0
Run the script again. Explain the new output of each line.
c) Still on h(y), what happens if you also initialize the variable res as global?
Problem 1.5: Conditional evaluations
a) Write a code to represent the following function:
ψ(x) =







1
if x < −a
(x/a)2
if −a ≤x ≤+a
1
if x > +a
(1.3)
b) Use linspace to create a range of 100 values for x between -2a and 2a. Calculate
ψ(x) over this range using your function. Use PyPlot or any other plotting package to
plot your function.
Problem 1.6: While and For Loops
Rewrite the factorial code of Example 1.12 using a while loop instead of a for.
Problem 1.7: Input and Output
a) Go back to Example 1.26. Write a code to read the CSV data in “bar.dat”, and save
it as “semicolon.dat” using semicolon (;) as a separator.
b) Write a code to read the “semicolon.dat”, and save it now as “text.dat” with the
data type set to String.
Problem 1.8: Random numbers and histograms
25

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Write a code to generate a large set of random numbers (start with 1000) and use the
hist(...) command to generate a histogram, and use PyPlot to plot it. The output of
hist(...) is tricky. Pay attention to it and check Julia’s documentation.
a) Use the rand(...) function to get an uniform distribution between [0,1);
b) Use the randn(...) function to get a standard normal distribution.
Problem 1.9: Cross and dot products
Given a parallelepiped deﬁned by the vectors a = (1,0,0), b = (0,1,0) and c = (5,5,1),
write a code to calculate its volume V = a·(b×c) using Julia’s deﬁnitions of dot and
cross products.
Problem 1.10: Calculate π using random numbers
a) Consider a square of side 1 centered at origin, and a circle of radius 1 inscribed
into the square. Draw two random numbers x and y from an uniform distribution
between [0,1) and calculate its norm. Count the number of pairs (x, y) that falls inside
and outside the circle. You’ll see that a good approximation for π is achieve after you
draw many pairs (x, y) using the expression pi = 4*inside/total.
b) Explain why this code gives you π.
c) Use PyPlot or any other plotting package to show the random numbers ﬁlling
the circle and square (use different colors if inside or outside the circle) as you draw
random pairs (x, y).
26

CHAPTER 2
Differential and Integral Calculus
Step by step.
I
N A TRADITIONAL introductory class of calculus, your teacher has probably started
the discussion deﬁning a function x(t) and a pair of points ti and t f . If you are a
physicist, x(t) was the trajectory deﬁned by the position at time t. Within the ﬁnite
interval ∆t = t f −ti, the distance traveled is ∆x = x(t f )−x(ti). From these we can deﬁne
the mean velocity within this interval as 〈v〉= ∆x/∆t. Moreover, in the limit where t f
and ti are inﬁnitesimally close, we get the instantaneous velocity:
v(t) = ∂
∂t x(t) = lim
∆t→0
∆x
∆t .
(2.1)
Using similar considerations your teacher has shown you that the integral is a sum
of inﬁnitesimal contributions:
x(t) = x(0)+
Z t
0
v(t′)dt′ = x(0)+ lim
∆t′→0
N
X
n=0
v
¡
t′
n
¢
∆t′,
(2.2)
where the sum runs over a discrete set labeled by the integer 0 ≤n ≤N that maps the
time axes 0 ≤t′
n ≤t.
If you remember all that, great! The main idea behind the numerical differential
and integral calculus is to drop the inﬁnitesimal limit and simply work with the ﬁnite
differences. Of course this is too simple. It would be a crude approximation1. We can do
better! But let’s take it step by step... discrete steps.
2.1
Interpolation / Discretization
A numerical calculation usually require us to represent Real (continuous) axis, planes or
volumes on the computer. This is impossible, right? Between 0.0 and 1.0 there’s already
∞numbers. We have to represent them in a discrete set, as introduced above.
1Before continuing to the next section, please check Problem 2.1 for some motivation.
27

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
The interpolation then deals with the unknown data in between discrete points.
Let’s start plotting sin(x) between 0 ≤x ≤2π with a few points only.
Example 2.1: Discrete plot of sin(x)
using PyPlot
x = linspace(0, 2pi, 7);
plot(x,
sin(x), "o")
plot(x,
sin(x), "-")
In the example above, the ﬁrst plot draws the circles, while the second draws the
lines connecting them, equivalent to the green dots in Fig. 2.1(a). The lines are a linear
interpolation of the points. Try again with more or less points to see how it goes.
Figure 2.1: Starting with the sin(x) deﬁned at only 7 points (blue dotes), we calculate the
(a) constant (rectangle) (b) linear (trapezoidal) and (c) quadratic interpolation points
(green dots). In both panels the yellow lines show the exact sin(x) for comparison.
Linear interpolation
Say you have a ﬁnite set of points {xn} labeled by the integer 0 ≤n ≤N, and you known
the function f (x) at these points. You may use the data from the example above with
f (x) = sin(x), or any other function of your choice.
Now, assume you want the value of the function at an general point x that does
not belong to your discrete set {xn}, but lies within xa ≤x ≤xb. Here b = a +1 labeling
consecutive points of the set. The most general expression for a linear interpolation
connecting the points {xa, f (xa)} and {xb, f (xb)} is ˜f (x) = C1x +C0. We’ll use the ∼
symbol to refer to the interpolated function. We want ˜f (x) to match f (x) at xa and xb,
therefore we have
˜f (xa) = C1xa +C0 = f (xa),
(2.3)
˜f (xb) = C1xb +C0 = f (xb),
(2.4)
28

Differential and Integral Calculus
which deﬁnes a system of two equations and two unknowns (C1 and C0). This set of
equations can be cast in a matrix form M ·C = F, where the matrix M, coefﬁcient vector
C and function vector F are show explicitly below:
µxa
1
xb
1
¶µC1
C0
¶
=
µf (xa)
f (xb)
¶
.
(2.5)
This equation can be easily solved as C = M−1 ·F. Evidently, you are able to express
the result with paper & pencil in this case. However, the matrix form makes it easy
to generalize to higher order interpolation. The next example shows a code for linear
interpolation, see Fig. 2.1(a). In Problem 2.2 I ask you to generalize this code for a
quadratic interpolation that should result in Fig. 2.1(b).
As you can see, the quadratic interpolation is already very close to the exact sin(x)
function. The interpolations usually work well because we are almost always deal-
ing with analytical functions, smooth functions. Complicated functions will probably
require advanced methods. For instance, functions with singularities.
29

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Example 2.2: Code for Linear Interpolation
using PyPlot
xlst = linspace(0, 2pi, 7); # create data for the example
flst = sin(xlst);
# receives number of points n (odd) to interpolate
# and the original data via x and f
function interp1(n, x, f)
xnew = linspace(x[1], x[end], n); # creates new axes
fnew = zeros(n); # and initialize new data as zero
i = 1; # label new sites
for j=1:(length(x)-1) # runs over old sites
xa = x[j]; # known points
xb = x[j+1];
fa = f[j]; # known data
fb = f[j+1];
# matrix form to find the coefficients
M = [xa 1.0; xb 1.0];
C1, C0 = inv(M)*[fa; fb];
# calculate the new data within every two points interval
while i <= n && xnew[i] <= xb
fnew[i] = C1*xnew[i] + C0;
i += 1;
end
end
return xnew, fnew; # return interpolated data
end
# calls function to interpolate
xnew, fnew = interp1(51, xlst, flst);
# plot old data, new data, and exact function
plot(xlst, flst, "o");
plot(xnew, fnew, "g.");
plot(xnew, sin(xnew), "y-");
2.2
Numerical Integration - Quadratures
Common numerical integration schemes rely on simple interpolations (Newton–Cotes
rules). Let’s discuss these methods ﬁrst. Later we give an overview of adaptive inte-
gration schemes, which is natively implemented in Julia as the function quadgk, and
more sophisticated implementations for multi-dimensional integrals can be found on
the Cubature package. We ﬁnish this section discussing the Monte Carlo integration
technique.
30

Differential and Integral Calculus
2.2.1
Polynomial Interpolations
We have brieﬂy discussed polynomial interpolations in the previous section. Now we’ll
use different interpolations to deﬁne common quadrature schemes. To establish a
notation, we’ll refer to the discrete points as xn, where the integer n labels the points as
in Fig. ?. We’ll assume that the set of points {xn} is equally spaced, so we can deﬁne a
constant step ∆x = xn+1 −xn.
Rectangle rule
The simplest quadrature method is the rectangle rule. Here the function is assumed
to be constant and equal to f (xn) within the interval xn −∆x/2 ≤x ≤xn +∆x/2. The
interpolation resulting from this rule can be seen in Fig. 2.1(a). Consequently, the
integral over this range is
Z xn+ ∆x
2
xn−∆x
2
f (x)dx ≈f (xn)∆x +O(∆x).
(2.6)
Trapezoidal rule
The trapezoidal rule uses the linear interpolation shown in Fig. 2.1(b). It is easy to see
(Prob. 2.3) that this leads to
Z xn+1
xn
f (x)dx ≈f (xn+1)+ f (xn)
2
∆x +O(∆x2).
(2.7)
Simpson’s rule
If we use the parabolic interpolation, we get Simpson’s rule:
Z xn+2
xn
f (x)dx ≈
h
f (xn)+4f (xn+1)+ f (xn+2)
i∆x
3 +O(∆x4).
(2.8)
From Fig. 2.1 one can already expect that the Simpson’s rule to give better results
and the rectangular or trapezoidal rules.
General remarks on the rules above
Note that the trapezoidal rule is deﬁned over a range set by two consecutive points,
while the Simpson’s rule runs over three points, from xn to xn+2. As a consequence the
Simpson’s rule can only be used if you have an odd number of known points.
Higher order polynomials leads to more precise quadrature rules. Cubic interpola-
tion leads to the Simpson’s 3/8 rule, which uses 4 points. For polynomial interpolations
of degree 4 we get Boole’s rule, which uses 5 points.
In the next example I assume we have a discrete set of points given by xlst and flst
that correspond to the output of some previous calculation. The function trapezoidal
31

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
is then used to integrate the function. Problem 2.1 asks you to implement other rules.
Try to change the number of points in the example, as well as other more complicated
functions.
Example 2.3: Numerical quadrature of a discrete function
# receives two vectors for the axis x and function f
function trapezoidal(x, f)
delta = x[2]-x[1]; # assuming constant step
res = 0.0; # initializes the sum
for n=1:(length(x)-1)
res += 0.5*(f[n+1]+f[n])*delta; # traditional rule
end
return res; # returns the result
end
# create data for the example
xlst = linspace(0, 2pi, 7); # x axis
flst = sin(xlst); # discretized function f(x)
trapezoidal(xlst, flst) # calls the function and shows the result
We may also need to integrate an exact function f (x). This is done in the next
example. Now the trapezoildal function receives the function to be integrated, the
limits of integration, and the number of points to consider. Here the function is never
written as a vector. Instead, it is calculated as needed. This will be more efﬁcient for
integration with a large number of points, as it doesn’t store the function as the vector
flst as above.
Example 2.4: Numerical quadrature of a function f(x)
# receives the function g(x) to be integraded
# from x=a to x=b with n points
function trapezoildal(g, a, b, n)
dx = (b-a)/(n-1.0);
xi(i) = a + (i-1.0)*dx;
res = 0.0;
for i=1:(n-1)
res += 0.5*(g(xi(i+1))+g(xi(i)))*dx
end
return res;
end
f(x) = sin(x); # chosen function
trapezoildal(f, 0, pi, 3) # test with 3 points
trapezoildal(f, 0, pi, 20) # 20 points
trapezoildal(f, 0, pi, 100) # 100 points
32

Differential and Integral Calculus
Note that these examples illustrate two distinct cases. In Example 2.3 we assume
that some previous calculation has given us the axis discretized into the vector xlst as
well as the function f(x) calculated at these points and stored in the vector flst. This
means that we are assuming that we don’t have direct access to an exact function f(x).
Example 2.4 presents the opposite case. Here we do have access to the exact function
f(x). In this case one may use more sophisticated quadrature schemes. We present
some of them in the next section.
2.2.2
Adaptive and multi-dimensional integration
Julia already has a native quadrature code implemented (quadgk). It uses an adaptive
Gauss-Kronrod integration technique. Since this is an introductory class, I will not go
into details of this method2.
But to get an idea of how the code works, imagine you have two quadrature of
different order implemented, say the trapezoidal and Simpson’s rules, and you want to
integrate f(x) from x=a to x=b. First you split your integration into subintervals, say
from x=a to x=c, and from x=c to x=b, i.e.
Z b
a
f (x)dx =
Z c
a
f (x)dx +
Z b
c
f (x)dx.
(2.9)
Then you evaluate each integral on the right hand side independently. You can estimate
the error of each subinterval comparing the result of the trapezoidal and Simpson’s
rules. If the error of a subinterval is too big, you split it into a new pair of subintervals
and repeat the test until you converge to the desired error.
An efﬁcient implementation of such methods can be difﬁcult, and I leave it as a
challenge for the experienced programmers in Problem 2.4.
Luckily, Julia has already the function quadgk. Check its documentation for details
on the parameters. The next example shows a basic usage.
Example 2.5: Numerical quadrature with quadgk
# let’s start with the same function from the previous example
f(x) = sin(x);
# quadgk receives the function and the limits of integration
quadgk(f, 0, pi)
# you may also try functions with integrable singularities...
f(x) = 1/sqrt(x);
quadgk(f, 0, 16) # exact result is 8
# and integrate all the way to infinity (Inf)
f(x) = exp(-(x^2)/2)/sqrt(2);
res = quadgk(f, -Inf, Inf) # exact result is pπ
pi-res[1]^2 # compare to π to check the error
2Those who are interested in mode details, please check the Wikipedia page for Adaptive quadrature
and references within: https://en.wikipedia.org/wiki/Adaptive_quadrature
33

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
For more advanced routines and multi-dimensional integration, please check Julia’s
Cubature package3.
2.2.3
Monte Carlo integration
While the previous methods integrate functions sampling them along a regular grid,
the Monte Carlo integration scheme samples the integration interval using random
numbers. We have used this method already to calculate π back in Problem 1.10, which
can be recast as a two-dimensional integral:
f (x, y) =
(
4
if x2 + y2 ≤1,
0
otherwise,
(2.10)
π =
Z
Ω
f (x, y)dxd y,
(2.11)
where the integration area Ωis set by the ranges 0 ≤x ≤1 and 0 ≤y ≤1.
The Monte Carlo implementation of this integral reads
RN = 1
N
N
X
i=1
f (xi, yi).
(2.12)
The next example shows an implementation of this integral
Example 2.6: Monte Carlo integral to calculate π
function Rn(f, n) # Monte Carlo integration of f(x,y)
res=0.0;
for i=1:n
res += f(rand(), rand());
end
return res/n;
end
# define the function using the ternary operator
f(x,y) = (x^2+y^2 <= 1.0)?4.0:0.0;
Rn(f, 10) # run the integral with 10 points
Rn(f, 1000) # run the integral with 100 points
The main advantage of the Monte Carlo integral is that its error decays with 1/
p
(n)
independently of the dimensions of the integral. Therefore this method becomes very
interesting for high-dimensional integrals.
3Cubature package: https://github.com/stevengj/Cubature.jl
34

Differential and Integral Calculus
2.3
Numerical Derivatives
At the beginning of this Chapter, our overview of the numerical calculus introduced
the idea of a numerical derivative simply as dropping the inﬁnitesimal limit on the
deﬁnition of a derivative. Indeed this leads to a familiar expression:
∂f (x)
∂x
= lim
∆x→0
f (x +∆x)−f (x)
∆x
≈f (x +∆x)−f (x)
∆x
.
(2.13)
As far as ∆x is small, this should give us a good estimate of the derivative. This formula
is actually known as the forward derivative, as it gives the derivative of f (x) at x using
two points: x itself, and the one step forward x +∆x.
2.3.1
Finite differences and Taylor series
The ﬁnite differences scheme is the most common method for numerical differentiation.
Its arises from the Taylor expansion of a function f (x):
f (xi +h) = f (xi)+h ∂f
∂x
¯¯¯¯
xi
+ h2
2!
∂2 f
∂x2
¯¯¯¯
xi
+ h3
3!
∂3 f
∂x3
¯¯¯¯
xi
+···
(2.14)
Here xi is an arbitrary point of our discrete grid labeled by the integer i, and h is the
step size between discrete points.
Let’s use the Taylor expansion to get the 1st and 2nd derivatives of f (x) using two or
three points of the discrete axis x →xi.
Forward 1st derivative
If we truncate the Taylor expansion on the h2 term, we can rearrange the remaining
terms to read:
∂f
∂x
¯¯¯¯
xi
≈f (xi +h)−f (xi)
h
+O(h)
(2.15)
Backward 1st derivative
Another choice is to start with the Taylor expansion for a negative step
f (xi −h) = f (xi)−h ∂f
∂x
¯¯¯¯
xi
+ h2
2!
∂2 f
∂x2
¯¯¯¯
xi
−h3
3!
∂3 f
∂x3
¯¯¯¯
xi
+···
(2.16)
and once again truncate the expansion on the h2 term to get
∂f
∂x
¯¯¯¯
xi
≈f (xi)−f (xi −h)
h
+O(h)
(2.17)
35

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Symmetric (or central) 1st derivative
Subtracting Eq. (2.14) from Eq. (2.16) we eliminate the h2 term
f (xi +h)−f (xi −h) = 2h ∂f
∂x
¯¯¯¯
xi
+2h3
3!
∂3 f
∂x3
¯¯¯¯
xi
+···
(2.18)
and now we can truncate the sum on the h3 term to get
∂f
∂x
¯¯¯¯
xi
≈f (xi +h)−f (xi −h)
2h
+O(h2)
(2.19)
Symmetric (or central) 2nd derivative
This time let’s sum Eqs. (2.14) and (2.16) to eliminate the ﬁrst derivative:
f (xi +h)+ f (xi −h) = 2f (xi)+h2 ∂2 f
∂x2
¯¯¯¯
xi
+···
(2.20)
Rearranging the expansion truncated on the h4 term give us
∂2 f
∂x2
¯¯¯¯
xi
≈f (xi +h)−2f (xi)+ f (xi −h)
h2
+O(h2)
(2.21)
Run the next example to get a plot comparing the ﬁrst derivatives with the exact
result. Try changing the step size from h=pi/5 to pi/50 and pi/500.
Example 2.7: Derivative using finite differences
# simple implementation of the finite differences
diff1_forward(f, x, h) = (f(x+h)-f(x))/h;
diff1_backward(f, x, h) = (f(x)-f(x-h))/h;
diff1_symmetric(f, x, h) = (f(x+h)-f(x-h))/(2h);
f(x) = sin(x); # chosen function to test
xgrid = 0:(pi/50):pi; # discrete x axis
h = pi/5; # step for the derivatives
# using comprehensions to calculate the derivatives along xgrid
fwd = [diff1_forward(f, x, h) for x=xgrid]
bwd = [diff1_backward(f, x, h) for x=xgrid]
sym = [diff1_symmetric(f, x, h) for x=xgrid]
# plot the exact result and the approximate derivatives for comparison
clf();
plot(xgrid, cos(xgrid); label="cos(x)")
plot(xgrid, fwd; label="forward")
plot(xgrid, bwd; label="backward")
plot(xgrid, sym; label="symmetric")
legend()
36

Differential and Integral Calculus
In the example above we are assuming that we have access to the exact function f (x)
at any point. This is similar to what we saw in Example 2.4. What happens if we consider
a situation similar to the one in Example 2.3? There we assume that we only known the
function f (x) via the discrete points set by xlst and flst. Check Problem 2.7.
2.3.2
Matrix Representation
Sometimes it is useful to represent the derivatives in matrix forms, such that it becomes
an operator. Say that we have a discrete x axis labeled by xi for 1 ≤i ≤N, and a function
deﬁned at these points by fi = f (xi). These are exactly the xlst and flst vectors from
the previous Examples of this Chapter. Let’s assume that fi = 0 for i ≤0 and i ≥N +1.
Let’s write the expressions for the symmetric ﬁnite differences derivative f ′
i = ∂f (x)
∂x
¯¯¯
xi
at
each point xi:
for i=1,
f ′
1 = f2 −0
2h ,
(2.22)
i=2,
f ′
2 = f3 −f1
2h
,
(2.23)
i=3,
f ′
3 = f4 −f2
2h
,
(2.24)
i=4,
f ′
4 = f5 −f3
2h
,
(2.25)
···
f ′
i = fi+1 −fi−1
2h
,
(2.26)
i=N-1,
f ′
N−1 = fN −fN−2
2h
,
(2.27)
i=N,
f ′
N = 0−fN−1
2h
.
(2.28)
The zero in the ﬁrst and last lines refer to f0 = 0 and fN+1 = 0, respectively.
The set of equations above can be put in a matrix form as


f ′
1
f ′
2
f ′
3
f ′
4
···
f ′
i
···
f ′
N−1
f ′
N


= 1
2h


0
1
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
0
0
−1
0




f1
f2
f3
f4
···
fi
···
fN−1
fN


(2.29)
In Julia this matrix can be implemented using the diagm function:
37

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Example 2.8: Matrix representation of the first derivative
x = linspace(-10, 10, 101); # discrete x axis
f = exp(-(x.^2)/2.0); # function sampled at x
n = length(x); # number of points
h = x[2] - x[1]; # step size
matdiff = (diagm(ones(n-1), 1) - diagm(ones(n-1), -1))/(2h);
# calculates the derivative of f as a product of matrix and vector
dfdx = matdiff*f;
plot(x, dfdx);
2.3.3
Derivatives via convolution with a kernel
A one-dimensional (1D) discrete convolution of the vectors f and g read
(g ∗f )[n] =
N
X
m=1
g[m]f [n + N −1−m],
(2.30)
where the index of our arrays g[m] and f [m] start at 1 and N is the length of the array g.
Let’s refer to the vector g as the kernel, and f as our function. If the kernel is
g = 1
2h(1,0,−1), the convolution above becomes
(g ∗f )[n] = 1
2h
³
f [n +1]−f [n −1]
´
,
(2.31)
which is exactly our deﬁnition of the symmetric ﬁrst derivative.
In Julia we can use the conv function for 1D convolutions and the conv2 function
for two-dimensional (2D) convolutions. The next example can be compared with the
previous one. Note that after the convolution we remove end points to keep dfdx with
the same size of x.
Example 2.9: Derivatives via convolution with a kernel 1D
x = linspace(-10, 10, 101); # discrete x axis
f = exp(-(x.^2)/2.0); # function sampled at x
n = length(x); # number of points
h = x[2] - x[1]; # step size
# kernel for the symmetric first derivative 1D
kernel = [1.0; 0.0; -1.0]/(2h);
dfdx = conv(kernel, f); # convolution
dfdx = dfdx[2:end-1]; # remove end points
plot(x, dfdx);
38

Differential and Integral Calculus
In 2D the kernel becomes a matrix. For instance, the kernel for
∂2
∂x∂y represented by
the symmetric differences with step sizes hx and hy in each direction is
g =
1
4hxhy


0
1
0
1
0
−1
0
−1
0

.
(2.32)
Example 2.10: Derivatives via convolution with a kernel 2D
# x and y axes
# transpose y to get matrix mesh for f later on
x = linspace(-5, 5, 101);
y = transpose(linspace(-5, 5, 101));
hx = x[2]-x[1]; # step sizes
hy = y[2]-y[1];
# 2D function as a matrix (lines are x, columns are y)
f = exp(-(x.^2)/2).*exp(-(y.^2)/2);
# 2D kernel for
∂2
∂x∂y
kernel = (1.0/(4*hx*hy))*[0.0 1.0 0.0; 1.0 0.0 -1.0; 0.0 -1.0 0.0];
df = conv2(kernel, f); # 2D convolution
df = df[2:end-1, 2:end-1]; # remove end points
surf(x, y’, df) # surface plot
# requires y to be transposed back into a column vector
2.3.4
Other methods, Julia commands and packages for derivatives
Later on we’ll see how to take derivatives using the properties of Fourier transforms.
This will be useful to solve differential equations, for instance with the split-operator or
split-step methods.
Julia has a native command for the ﬁrst order derivatives: gradient. This command
uses the forward rule to evaluate the derivative at the ﬁrst point, the backward rule at
the last point, and the symmetric rule at the internal points.
There exists also packages, like the ForwardDiff4. However there’s a warning on
the webpage of ForwardDiff and at this moment I’m not familiarized with the issues
they are facing.
2.4
Problems
Problem 2.1: Checking the simplest numerical integration
Consider the functions f (x) = sin(x), g(x) = cos(x), and h(x) = ex.
4ForwardDiff package: https://github.com/JuliaDiff/ForwardDiff.jl
39

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
a) What is the exact result for the integration of f (x) and g(x) on the interval 0 ≤x ≤
2π? Try also the interval 0 ≤x ≤π. What about the integration of h(x) over 0 ≤x ≤1?
b) Try to numerically integrate these functions as initially described at the introduc-
tion of this chapter. Simply dropping the inﬁnitesimal limit. Compare the deviation
from the exact result as a function of the number of points used in the integration in a
log-plot. Useful Julia commands: linspace, sum.
c) Complement item (b) as you learn new methods: compare the precision of dif-
ferent methods (trapezoidal, Simpson’s, Boole’s, Monte Carlo, ...) as a function of the
number of points being used.
Problem 2.2: Interpolation
Go back to Example 2.2, which shows a code for a linear interpolation ˜f (x) = C1x+C0.
Following the example, implement (a) a code to get a quadratic interpolation ˜f (x) =
C2x2 +C1x +C0, (b) a code for the rectangle interpolation. You must reproduce Fig. 2.1.
Problem 2.3: Quadrature equations
(a) Show that the linear interpolation leads to the trapezoidal quadrature rule.
(b) Show that the quadratic interpolation leads to the Simpson’s rule.
Problem 2.4: Adaptive Integration
This problem might not be easy. I’ll leave it here as a challenge for the more expe-
rienced programmers among the students: Try to implement an adaptive integration
code using the trapezoidal and Simpson’s rule discussed in the text.
Problem 2.5: Monte Carlo integration and π
Edit the Monte Carlo Example 2.6 or your code from Problem 1.10 to calculate π as
a function of the number n of randomly sampled points. Let’s say that Rn is the result of
the calculation with n points. The relative error is En = (Rn −π)/π. Do a log-log plot of
En vs n to see that the error goes with 1/
p
(n), which is the typical behavior of Monte
Carlo integrals.
Problem 2.6: Derivatives via ﬁnite differences with an exact function
(a) Following Example 2.7, write a code to calculate the second derivative of a
function f (x) and compare with the exact result.
(b) Run the Example 2.7 and the code you wrote on item (a) with the following
functions
• f (x) = e−x2, for −5 < x < 5;
• f (x) = px, for 0 < x < 10;
• f (x) = 4x4 +3x3 +2x2 + x, for −10 < x < 10.
40

Differential and Integral Calculus
Problem 2.7: Derivatives via ﬁnite differences with a discrete axis
Let’s assume that we only have the function f (x) sampled at discrete points set by
xlst and stored at the vector flst:
h = pi/5; # grid step
xlst = 0:h:pi; # discrete x axis
flst = sin(xlst); # function evaluated at discrete points
(a) Write a code to calculate the ﬁrst and second derivatives using this discrete data.
Here the derivative step size has to be equal to the grid step size. You will face a difﬁculty
at the end points. What can you do?
(b) Test your code with the functions of the previous Problem. Try to vary the grid
step to see how the precision changes.
Problem 2.8: Matrix representation of the ﬁnite differences
Following Example 2.8, implement the matrix representation for the ﬁnite differ-
ences derivatives:
(a) First derivative: forward, backward and symmetric. Change the step size trying
to visualize the difference between these implementations.
(b) Second derivative: symmetric.
Always test your implementations with well known functions that you can differen-
tiate analytically for comparison.
Problem 2.9: Derivatives via convolution with a kernel
Implement the forward and backward ﬁrst derivatives, and the second symmetric
derivative in 1D using convolution with a kernel following the Example 2.9.
41


CHAPTER 3
Ordinary Differential Equations
ENIAC, since 1946
E
NIAC, Electronic Numerical Integrator And Computer[8], the ﬁrst electronic general-
purpose computer was designed by the US Army and ﬁnished in 19461. The goal was
to numerically calculate artillery ﬁring tables by solving sets of differential equations.
Essentially, F = ma with quite a few complications.
Indeed we are surrounded by differential equations...
m ∂2r
∂t2 = F ,
(3.1)
iħ∂ψ(r,t)
∂t
= Hψ(r,t),
(3.2)
∇·D = ρ,
∇·B = 0,
∇×E = −∂B
∂t ,
∇×H = ∂D
∂t +J,
(3.3)
∂u
∂t −α2∇2u = 0,
(3.4)
∂2u
∂t2 −c2∇2u = 0,
(3.5)
... to list a few. We also have Navier–Stokes, general relativity, etc.
The unknown quantity in a differential equation can be a function of a single vari-
able, like the position that depends on time on Newton’s second law above (r ≡r(t)).
These cases are labeled ordinary differential equations (ODE). If your function
depends upon two or more variables and your differential equation has derivatives
on these variables, it becomes a partial differential equation (PDE). This is the
case of all other examples above. PDEs and ODEs are intrinsically different. Important
theorems of ODEs do not apply for PDEs.
1ENIAC: https://en.wikipedia.org/wiki/ENIAC
43

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Let us compare two simple cases, the Laplace equation in one and two dimensions:
∂2 f (x)
∂x2
= 0,
(3.6)
µ ∂2
∂x2 + ∂2
∂y2
¶
f (x, y) = 0.
(3.7)
The 1D equation is a second order ODE. This means that its most general solution
is deﬁned up to two unknown constants. These are set by two initial or boundary
conditions. In this case the general solution is a straight line,
f (x) = c1x +c0,
(3.8)
where c1 and c0 are the unknown coefﬁcients.
Contrasting the simplicity of the 1D Laplace equation, the 2D version, which is a
PDE, has an inﬁnite number of solutions. Introducing complex variables, it is easy to
verify that the most general solution is
f (x, y) = p(z)+ q(¯z),
(3.9)
where p(z), q(¯z) are arbitrary analytical functions of the complex variables z = x +i y,
¯z = x −i y.
For now, we will focus on ordinary differential equations (ODE). These can
be classiﬁed into three main categories: (i) initial value problems; (ii) boundary-value
problems; and (iii) eigenvalue problems. Here we shall restrict ourselves to introductory
discussion on the methods to solve ODEs, for more details please check Ref. [4, 6].
3.1
Initial value problem: time-evolution
Initial value problems usually deal with the dynamics of a system, where the derivatives
are taken with respect to time. Newton’s second law is the paradigmatic example of an
initial value ODE,
d2r
dt2 = F
m ,
(3.10)
requiring the initial position r(0) and velocity v(0) to be solved.
Let’s consider the one dimensional case to guide our discussion. Just keep in mind
that a generalization to more dimensions is immediate. First we write Newton’s second
law in a more general form of an arbitrary second order initial value problem ODE,
d2x
dt2 = g(x,t).
(3.11)
44

Ordinary Differential Equations
This second order ODE can be split into a pair of coupled ﬁrst order ODEs using
v = dx
dt , yielding
dv
dt = g(x,t),
and
dx
dt = v.
(3.12)
Finally, we can deﬁne vectors y = [v(t),x(t)] and f = [g(x,t),v(t)], such that the
equations above can be cast as
dy
dt = f.
(3.13)
Example: the pendulum
Any physics text-book will tell you that the equation of motion for the pendulum is
d2θ(t)
dt2
= −ω2 sin[θ(t)],
(3.14)
where θ(t) is the angular displacement, ω2 = g/ℓ, g ≈9.8 m/ss is the gravity, and ℓis
the pendulum length.
For small oscillations we can use Taylor’s expansion to write sinθ ≈θ. This leads to
the usual harmonic solution θ(t) = Acos(ωt +φ), where the amplitude A and the phase
constant φ are set by initial conditions.
Let us try to go beyond the small oscillations approximation and numerically solve
Eq. (3.14) with the content of the next sections. To do this, ﬁrst we have to rewrite
Eq. (3.14) in the form of a set of coupled ﬁrst order ODEs as we generically did above.
With this purpose, deﬁne2 x(t) = θ(t) and v(t) = ˙θ(t). The pendulum equation of motion
becomes
dv
dt = −ω2 sin[θ(t)],
and
dx
dt = v,
(3.15)
which can be cast as Eq. (3.13) setting y = [˙θ(t),θ(t)] and f = [−ω2 sin[θ(t)], ˙θ(t)]. To
solve these equations we will have to specify the initial position x(0) and velocity v(0).
In the next sections we will learn methods to solve differential equations and apply
them to this pendulum example. Note that replacing sinθ →θ in Eq. (3.15) allows you
to compare the results with the exact solution for small oscillations. Once you have
conﬁdence that your code works, put back the full sinθ dependence and compare the
small oscillations solution with the numerical solution for large amplitudes.
2Within the text I’ll use the dot notation to refer to time derivatives, i.e. ˙θ = dθ
dt
45

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
3.1.1
The Euler method
Go back to the previous Chapter and check the expression for the forward ﬁrst derivative.
Applying it to Eq. (3.13) gives
y(t +τ)−y(t)
τ
= f(t),
(3.16)
where τ is the discrete time step. Labeling the discrete time tn = t0 +nτ with integers n,
we can deﬁne y(tn) = yn and f(yn,tn) = fn. Rewriting the equation above give us
yn+1 = yn +τfn.
(3.17)
Since in principle we known y0 and f0 (initial values), the equation above can be used
to iterate the solution from n = 0 to all n. The global error of the Euler method is O(τ).
Example 3.1: Euler method - Pendulum
using PyPlot
w = 2pi; # frequency, period for small oscillations T = 2pi/w
g(x,t) = -(w^2)*sin(x); # r.h.s. of Newton’s 2nd law
f(x,v,t) = [g(x,t); v]; # r.h.s. of linearized ODE
x0 = 0.5pi; # initial angular displacement
v0 = 0.0; # inicial angular velocity
y0 = [v0; x0]; # initial vector for linearized ODE
tau = 1e-4; # time step
tspan = 0:tau:5; # time range
yt = y0; # we will store the data for each t in yt
y = y0; # y at current t for the calculation
for t=tspan[1:end-1]
y = y + tau*f(y[2], y[1], t); # Euler iteration
yt = [ yt y ]; # store solution for each time step
end
# data stored in lines, transpose to use with PyPlot
v = transpose(yt[1,:]); # first line is v(t)
x = transpose(yt[2,:]); # second line is x(t)
small = x0*cos(w*tspan); # exact solution for small oscillations
# plot numerical and exact solutions for comparison
clf();
plot(tspan, x; label="numerical");
plot(tspan, small; label="small oscillations");
legend();
In the example above, the initial displacement of the pendulum is set by −π < x0 <
π. For |x0| ≪π you should see a good agreement with the exact solution for small
oscillations. For large |x0| ≲π the numerical simulation will show plateaus as the
46

Ordinary Differential Equations
pendulum slows down near x0 = ±π. However, to see this you will probably have to
reduce τ to recover stability. Try to play with the parameters.
3.1.2
Runge-Kutta Methods
The Runge-Kutta methods are the most popular methods for solving ODE due to its
high-precision and stability. These methods can be classiﬁed as predictor-corrector
methods, see Ref. [6]. The most used version is the 4th order Runge-Kutta method, or
simply RK4. But let’s start with the RK2 for simplicity.
2nd order Runge-Kutta method (RK2)
Our prototype for a differential equation, Eq. (3.13), can be written in a integral form as
y(t +τ) = y(t)+
Z t+τ
t
f
³
y(t′),t′´
dt′.
(3.18)
If we approximate the integrand by a constant value evaluated at the lower limit of the
integral, i.e. f(y(t′),t′) = f(y(t),t), we get the Euler method again: yn+1 = yn +τfn.
Instead, if we approximate the integrand by the midpoint, i.e. f(y(t′),t′) = f(y(t +
τ/2),t +τ/2), we get
yn+1 = yn +τf
³
yn+ 1
2 ,tn + τ
2
´
.
(3.19)
But we don’t known yn+1/2. To proceed, there’s two possibilities:
(i) The explicit RK2
is obtained using the Euler method to express yn+1/2 = yn + 1
2τfn,
yielding
yn+1 = yn +τf
³
yn + τ
2f(yn,tn), tn + τ
2
´
.
(3.20)
(ii) The implicit RK2
is obtained if we use the midpoint rule to express yn+1/2 =
1
2(yn +yn+1),
yn+1 = yn +τf
³yn +yn+1
2
, tn + τ
2
´
.
(3.21)
Both version result in global errors O(τ2).
Notice that the explicit RK2 is slightly more complicated than the Euler method,
but still very similar: on the left hand side we have the unknown yn+1 at the next time
step, and on the right hand side we have all quantities evaluated at the current time
step. Therefore it is easy to generalize the Euler code from Example 3.1 to implement
the explicit RK2.
In contrast, the implicit RK2 has the unknown yn+1 on both sides of the equation.
To solve Eq. (3.20) one can use the ﬁxed-point iteration method. Start with an initial
guess for yn+1, which could be from the Euler method: y[0]
n+1 = yn +τfn. For now on the
47

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
superscript [k] in y[k]
n+1 refers to the level of the iteration. Then iterate the solution until
convergence,
y[0]
n+1 = yn +τfn,
(3.22)
y[1]
n+1 = yn +τf
³yn +y[0]
n+1
2
, tn + τ
2
´
,
(3.23)
y[2]
n+1 = yn +τf
³yn +y[1]
n+1
2
, tn + τ
2
´
,
(3.24)
...
(3.25)
y[k+1]
n+1 = yn +τf
³yn +y[k]
n+1
2
, tn + τ
2
´
.
(3.26)
Convergence is achieved for large k when y[k+1]
n+1 −y[k]
n+1 is sufﬁciently small.
It is also possible to solve Eq. (3.20) using root-ﬁnding algorithms (e.g. Newton’s
method). Notice that the only unknown quantity is yn+1, so let’s deﬁne an auxiliary
function R(yn+1),
R(yn+1) = yn+1 −yn −τf
³yn +yn+1
2
, tn + τ
2
´
,
(3.27)
such that the possible solutions of Eq. (3.20) are the roots of R(yn+1) = 0.
Explicit 4th order Runge-Kutta method (RK4)
The RK4 method is by far the most popular method to solve ODEs. Its implementation
is not much more complicated than the simples Euler method, but its precision is far
superior with a global error O(τ4). The iteration rule for the RK4 is
k1 = f(yn,tn),
(3.28)
k2 = f
³
yn + τ
2k1,tn + τ
2
´
,
(3.29)
k3 = f
³
yn + τ
2k2,tn + τ
2
´
,
(3.30)
k4 = f
³
yn +τk3,tn +τ
´
,
(3.31)
yn+1 = yn + τ
6
³
k1 +2k2 +2k3 +k4
´
.
(3.32)
Since this is the most used method for differential equations, I will not show an
implementation example here. Instead, I’ll leave its implementation as a problem for
the students.
48

Ordinary Differential Equations
3.1.3
Stiff equations
Stiff differential equations are those where the numerical solution require a step size
excessively small when compared with the actual smoothness of the solution. Typically,
a stiff equation solved with a large step size show spurious oscillations.
A stiff differential equation can be as simple as
d y(t)
dt
= −15y(t),
(3.33)
which has an exact solution y(t) = e−15t for the initial condition y(0) = 1. Note that
y(t) > 0 for any t > 0. However, if you apply the Euler method with f (y) = −15y, you will
get yn+1 = yn −15τyn. For a large τ, the Euler method may give negative values for yn+1.
Indeed, if you solve this equation with the Euler method with a large τ, you will get the
spurious oscillations.
A better choice is to use the implicit RK2 method. In this particular case you can
actually solve Eq. (3.21) analytically for yn+1 to obtain
yn+1 =
1−15
2 τ
1+ 15
2 τ
yn.
(3.34)
The next example compares the numerical solutions of this stiff equation obtained
with the explicit and implicit RK2 methods. Try running it with different step sizes τ and
compare the results.
49

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Example 3.2: Stiff ODE, implicit vs explicit RK2 methods
# explicit RK2 solver receives the right hand side function,
# initial (t0) and final (t1) times, time-step (tau),
# and initial condition (y0)
function explicitRK2(rhs, t0, t1, tau, y0)
tspan = t0:tau:t1; # sets the time span
y = y0; # stores solutions in y
yt = y0; # yt is the auxiliary for the iterations
for t=tspan[1:end-1]
# explicit RK2 rule:
yt = yt + tau*rhs(t+tau/2, yt+0.5*tau*rhs(t, yt));
y = [ y; yt ]; # stores solutions
end
return tspan, y;
end
# does not need the rhs as it is implemented specifically
# for the ODE: dy/dt = -15y
function implicitRK2(t0, t1, tau, y0)
tspan = t0:tau:t1;
y = y0;
yt = y0;
for t=tspan[1:end-1]
# explicit solution of the implicit rule for this particular ODE
yt = yt*(1.0-15*tau/2.0)/(1.0+15*tau/2.0);
y = [ y; yt ];
end
return tspan, y;
end
rhs(t,y) = -15*y; # defines the right hand side of the ODE
y0 = 1.0; # initial condition
tau = 1.0/10; # same tau for both methods
te, ye = explicitRK2(rhs, 0.0, 1.0, tau, y0); # calls explicit RK2
ti, yi = implicitRK2(0.0, 1.0, tau, y0); # calls implicit RK2
texact = 0:0.01:1;
exact = exp(-15*texact); # exact solution for comparison
clf(); # plot the results
plot(texact, exact; label="Exact");
plot(te, ye; label="Explicit");
plot(ti, yi; label="Implicit");
legend();
axis([0.0, 1.0, -1.5, 1.5]);
A more complicated stiff ODE is
d y(t)
dt
= y2(t)−y3(t).
(3.35)
50

Ordinary Differential Equations
If you put f (y) = y2−y3 into the implicit RK2 Eq. (3.21), you will have three roots. Which
one should you use? In this case it is better to use the ﬁxed-point iteration method.
In the next example implement the implicit RK2 method using the ﬁxed-point
iteration for an initial condition y(0) = y0 within a time range 0 ≤t ≤2/y0. For small y0
the ODE become stiff and the solution will require a very small τ to converge. You may
use the implementation of the explicit RK2 from the previous example to compare with
the implicit RK2 again.
Example 3.3: Stiff ODE, implicit vs explicit RK2 methods
# implicit RK2 solver receives the right hand side function,
# initial (t0) and final (t1) times, time-step (tau),
# initial condition (y0), and relative tolerance (reltol)
function implicitRK2(rhs, t0, t1, tau, y0, reltol)
tspan = t0:tau:t1;
y = y0;
yt = y0;
for t=tspan[1:end-1]
yold = yt; # previous value
ynext = yt + tau*rhs(t, yt); # next value
k = 0; # loop counter
# check convergence, while loops
while abs(ynext-yold) > reltol*ynext && k < 50000
yold = ynext; # update old
ynext = yt + tau*rhs(t+tau/2.0, (yt+ynext)/2.0); # update new
k += 1;
end
yt = ynext; # final result
y = [ y; yt ];
end
return tspan, y;
end
rhs(t,y) = y^2 - y^3;
y0 = 0.1;
tau = (2.0/y0)/5;
te, ye = explicitRK2(rhs, 0.0, 2/y0, tau, y0, 1e-6);
ti, yi = implicitRK2(rhs, 0.0, 2/y0, tau, y0, 1e-6);
clf();
plot(te, ye; label="Explicit");
plot(ti, yi; label="Implicit");
legend();
axis([0.0, 2/y0, -0.1, 1.5]);
After checking the results of the code above, try reducing the step size to tau =
(2.0/y0)/10 and tau = (2.0/y0)/100 to see how the solution improves. Next, reduce
y0 to y0 = 0.01 and the solutions will split again. Reduce τ until they match. Now try
for y0 = 0.0001. As you try different parameters, zoom in into the y = 1 plateau to see
the oscillations on the solutions.
51

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
3.1.4
Julia’s ODE package
The ODE package3 provides efﬁcient implementations of adaptive Runge-Kutta meth-
ods, including a method for stiff ODEs. To install it, simply call Pkg.add("ODE"). The
ODE solvers are labeled odeXY, where X is the main order of the method, and Y is the
order of the error control. Let’s focus on the ode45 solver.
All methods of the ODE package are implemented to solve the differential equation
dy
dt = F (t,y),
(3.36)
and the methods obey the function prototype:
tout, yout = odeXX(F, y0, tspan; keywords...)
Input: F is a function that receives the current time t and the corresponding vector
y, and returns a vector as deﬁned by the right hand side of Eq. (3.36). Additionally,
odeXX receives a vector with the initial conditions y0 (= y(0)), and the range of time
over which the solution is desired (tspan). The last parameter, keywords, are allow
you to set extra conﬁgurations, like the error tolerance. I suggest you use the key-
word points=:specified, so that the output is returned only for each time instant in
tspan = 0:dt:tmax.
Output: the package returns tout and yout. The ﬁrst, tout, is the list of time in-
stants t at which the solution y(t) was calculated. If the keyword points=:specified
was passed to odeXX, then tout = tspan, otherwise it may vary. The solutions y(t) are
returned in yout.
Next I adapt Example 3.1 to run with the ODE package.
3ODE: https://github.com/JuliaLang/ODE.jl
52

Ordinary Differential Equations
Example 3.4: ODE Package - Pendulum
using PyPlot
using ODE
w = 2pi; # frequency, period for small oscillations T = 2pi/w
g(x,t) = -(w^2)*sin(x); # r.h.s. of Newton’s 2nd law
f(x,v,t) = [g(x,t); v]; # r.h.s. of linearized ODE
x0 = 0.5pi; # initial angular displacement
v0 = 0.0; # inicial angular velocity
y0 = [v0; x0]; # initial vector for linearized ODE
tau = 1e-4; # time step
tspan = 0:tau:5; # time range
# the rhs function is our implementation of F (t,y) from Eq. (3.36)
rhs(t, y) = f(y[2], y[1], t);
tout, yout = ode45(rhs, y0, tspan; points=:specified);
x = map(k-> k[2], yout); # extract x from yout
small = x0*cos(w*tspan); # exact solution for small oscillations
# plot numerical and exact solutions for comparison
clf();
plot(tspan, x; label="numerical");
plot(tspan, small; label="small oscillations");
legend();
3.2
Boundary-value problems
While the differential equations of initial value problems require initial conditions set
on a single point (e.g.: position and velocity at t = 0), the boundary-value problem (BVP)
applies when the constraints are speciﬁed at two or more points (e.g.: the potential
of the source and drain electrodes on a sample). Therefore, typical boundary-value
problems involve second oder differential equations.
We can split the boundary-value problems into two categories. First we will discuss
differential equations in the Sturm-Liouville form, including both homogeneous and
inhomogeneous cases. These are linear equations and we shall use its well known
properties to optimize our numerical implementations. Later we discuss non-linear
differential equations for which the properties of the Sturm-Liouville case do not apply.
The methods discussed here are also valid for the initial-value problems of the
preceding section, since these are simply a particular case of the class of boundary-
value problems.
3.2.1
Boundary conditions: Dirichlet and Neumann
The boundary conditions of a differential equation reﬂect an external constraint that
must be imposed on the general solution in order to ﬁnd the speciﬁc solution of the
53

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
problem at hand. For instance, on an oscillating string one might have the ends of
the string ﬁxed. In this case the string oscillation obey the wave equation, while ﬁxed
ends conditions must be imposed on the solution, constraining it to have vanishing
oscillation at these end points. The solution of an differential equation that satisfy the
speciﬁed boundary conditions is unique. For ordinary differential equations, there’s
two types of possible cases: (i) Dirichlet; and (ii) Neumann boundary conditions.
To guide our discussion, let’s consider a general second order differential equation
of the form
y′′(x) = f [y′(x), y(x),x],
(3.37)
where we use the prime notation for the derivatives (i.e. y′(x) = d y/dx, and y′′(x) =
d2y(x)/dx2), and f (y′, y,x) is a function to be speciﬁed by the problem at hand.
A boundary condition is said to be of the Dirichlet type when it constrains the value
of the solution at speciﬁc points. For instances, it requires that at the points x = x0 and
x = x1 the solution y(x) must satisfy y(x0) = y0, and y(x1) = y1, where y0 and y1 are
constants. If we are dealing with the heat equation, these boundary conditions could
be specifying the constant temperatures at two reservoirs. On a electrostatic problem
(see the Poisson equation example below), the Dirichlet boundary condition is used to
impose the the potential difference between two distant electrodes.
A Neumann boundary condition constrains the derivative of the solution at speciﬁc
points. For instance, it may require y′(x0) = v0 and y′(x1) = v1, where v0 and v1 are
constants. On an electrostatic problem, these could be specifying the electric ﬁelds at
the end points. On the wave equation the boundary condition y′(L) = 0 is used when
the point L is not ﬁxed.
One can always mix these types of boundary conditions. These are called mixed
boundary conditions. For instance, we may require y(x0) = y0 and y′(x1) = v1. Notice
that these are speciﬁed at distinct points x0 and x1. If they were both speciﬁed at the
same point, i.e. y(x0) = y0 and y′(x0) = v0, we would have the initial-value problem
discussed in the previous section. Therefore, one may see the initial-value problem
as a particular case of the boundary-value problem. Nonetheless, it is important to
distinguish these cases because the techniques used to solve them are distinct.
For ordinary differential equations, the boundary condition that leads us back to the
initial-value problem is called Cauchy boundary condition. However, this nomenclature
is irrelevant in one dimension (which is always the case for ordinary differential condi-
tions). Later on we will discuss partial differential equations, where we’ll come back to
this discussion to properly present the Cauchy and the Robin boundary conditions.
Example: the Poisson equation
As a paradigmatic example of the boundary-value problem, consider the Poisson equa-
tion for the electrostatic potential in one-dimension. Let us start from the electrostatic
Gauss law, ∇·E(r) = ρ(r)/ϵ0, where ϵ0 is vacuum dielectric constant, ρ(r) is the charge
density at r, and E(r) is the electric ﬁeld at r. In electrostatics the electric ﬁeld can be
written in terms of the scalar potential φ(r) as E(r) = −∇φ(r), such that the Gauss law
54

Ordinary Differential Equations
takes the form of the Poisson equation, ∇2φ(r) = −ρ(r)/ϵ0. In three-dimensions the
Poisson equation is a partial differential equation (PDE), as it has partial derivatives in
x, y and z. In this Chapter we are interested in ordinary differential equations (ODE)
only, therefore we will consider the one-dimensional case of the Poisson equation,
∂2φ(z)
∂z2
= −ρ(z)
ε0
.
(3.38)
Let’s say that we have metallic contacts setting the electrostatic potential at z = ±L
as φ(±L) = ±φ0, and there’s a narrow charge distribution at z = 0 set as ρ(z) = qδ(z).
You can solve this problem analytically to get
φ(z) = −q
2ϵ0
(|z|−1)+ φ0
L z.
(3.39)
This solution is shown in Fig. 3.1.
Figure 3.1: Illustration of the solution of the Poisson equation given by Eq. (3.39) for
φ0 = L = q = ε0 = 1.
In this section we will learn how to solve this equation numerically for an arbitrary
charge distribution ρ(z). Since it might be difﬁcult to deal with δ distributions numeri-
cally, we shall consider, for instance, a Gaussian charge distribution ρ(z) = qe−(z−z0)2
2Γ2 ,
where z0 is the center of the distribution and Γ is the broadening. For z0 = 0 and small Γ
you should be able to reproduce the analytical solution above.
3.2.2
The Sturm-Liouville problems
In physics, many problems fall into differential equations that take the Sturm-Liouville
form,
½ d
dx
·
p(x) d
dx
¸
+ q(x)
¾
y(x) = λw(x)y(x)+r(x),
(3.40)
where p(x), q(x), w(x) and r(x) are functions speciﬁed by the problem at hand, while
y(x) is the unknown function that we want to ﬁnd. For instance, the Poisson equation
55

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
in Eq. (3.38) is set by using p(x) = 1, q(x) = w(x) = 0, and r(x) = −ρ(x)/ϵ0. If w(x) ̸= 0, λ
is the eigenvalue of the equation. We will discuss the class of eigenvalue problems later
on in this Chapter, therefore, for now we shall take w(x) = 0. Moreover, let’s assume that
p(x), q(x) and r(x) are analytical functions over the domain of interest.
Let’s ﬁrst present useful properties of the Sturm-Liouville equation without properly
deriving them. For the derivations and more details please check specialized books on
Mathematical Physics [refs] and Problem 3.4. Next, we use these properties to establish
the Wronskian method to solve Sturm-Liouville problems.
The homogeneous Sturm-Liouville equation
If r(x) = 0, Eq. (3.40) is said homogeneous as it takes the form L y(x) = 0, where L is
the Sturm-Liouville operator, which is given by the terms between curly brackets, {···},
in Eq. (3.40). It follows the properties:
(1) Principle of superposition. Since this equation is linear, if ya(x) and yb(x) are solu-
tions, then the linear combination yc(x) = aya(x)+byb(x) also satisﬁes L yc = 0. Here
a and b are arbitrary constants that shall be deﬁned by the boundary conditions, i.e.
the linear combination coefﬁcients.
(2) Linear independence. Two solutions ya(x) and yb(x) are linearly independent if
their Wronskian W (x) ̸= 0 over the domain of x. The Wronskian of a pair of solutions
ya(x) and yb(x) is W (x) = y′
a(x)yb(x) −y′
b(x)ya(x). For the class of Sturm-Liouville
homogeneous equations it is easy to show that
W (x) = W (x0)exp
½
−
Z x
x0
p1(x′)
p(x′) dx′
¾
,
(3.41)
where p1(x) = d
dx p(x), and x0 belongs to the domain of x. Therefore, if the Wronskian
W (x0) ̸= 0 in a speciﬁc point x = x0, it will be non-zero over the whole domain.
(3) Uniqueness of the solution. Since the Sturm-Liouville equation is a second-order
ordinary differential equation, its solution is unique if it satisﬁes the ODE and two
boundary (or initial) conditions. As a consequence, its most general solution can be
written as a linear combination of two linearly independent solutions.
The properties above form the basis need to solve the homogeneous Sturm-Liouville
problem using the Wronskian method. But before discussing this method, let’s check
the inhomogeneous Sturm-Liouville problem.
The inhomogeneous Sturm-Liouville equation
For r(x) ̸= 0 the Sturm-Liouville problem takes the form L y(x) = r(x), and is said to
be inhomogeneous as it has a term independent of y(x). In this case, the principle of
superposition as stated above is not valid (see Problem 3.4). Instead, the generalized
principle of superposition of inhomogeneous equations states that the most general
solution can be written as
y(x) = aya(x)+byb(x)+ yp(x),
(3.42)
56

Ordinary Differential Equations
where ya(x) and yb(x) are linearly independent solutions of the homogeneous equation
L ya/b(x) = 0, and yp(x) is the particular solution of the inhomogeneous equation
L yp(x) = r(x). Here again, a and b are arbitrary constants to be set by the boundary
conditions.
Since ya(x) and yb(x) are solutions of the homogeneous case, property (2) above
follows and the Wronskian criteria can be used to verify or assure that ya(x) and yb(x)
are linearly independent solutions. Property (3) also follows, and the solution y(x)
is unique if and only if it satisﬁes the ODE L y(x) = r(x) and the required boundary
conditions.
3.2.3
The Wronskian method
The Wronskian method applies to both homogeneous and inhomogeneous cases of the
Sturm-Liouville equation. Essentially, it provides a method to obtain a pair of linear in-
dependent solutions ya(x) and yb(x). We will state the problem for the inhomogeneous
case, since the homogeneous case is simply a particular case in which r(x) = 0.
To guide our description let’s consider Dirichlet boundary conditions, i.e. y(x0) = y0
and y(x1) = y1. Generalizations to Neumann or mixed boundary conditions will be
simple enough and we leave it to the reader as an exercise. We want to solve the Sturm-
Liouville equation L y(x) = r(x) in the domain x0 ≤x ≤x1.
First step: ﬁnd two linearly independent solutions of the homogeneous equation
Our ﬁrst step is to ﬁnd a pair of linearly independent solutions ya(x) and yb(x) that
satisfy the homogeneous equation L ya/b(x) = 0. Accordingly to property (2) above,
ya(x) and yb(x) will be linearly independent if their Wronskian W (x) ̸= 0 at any point x.
For practical purposes, we chose to analyze the Wronskian at x = x0, the left end point
of the domain. Namely, we want
W (x0) = y′
a(x0)yb(x0)−ya(x0)y′
b(x0) ̸= 0.
(3.43)
Notice that the ﬁnal solution will be given by y(x) in Eq. (3.42). Therefore the bound-
ary conditions must apply to y(x), and not to the auxiliary functions ya(x), yb(x) or
yp(x). Therefore we can attribute auxiliary boundary conditions to ya(x) and yb(x) in
order to assure that the condition above is satisﬁed and the pair [ya(x), yb(x)] forms a
set of linearly independent solutions. For instance, we may choose
ya(x0) = 0, and y′
a(x0) = A1,
(3.44)
yb(x0) = A2, and y′
b(x0) = 0,
(3.45)
such that W (x0) = A1A2, where A1 ̸= 0 and A2 ̸= 0 are arbitrary nonzero constants.
The auxiliary boundary conditions above actually transforms the problem of ﬁnding
ya/b(x) into a pair of independent initial-value problems, with the initial values for
ya(x), yb(x) (and their derivatives) set above in terms of the arbitrary A1 and A2. We can
57

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
use any initial-value problem method to obtain ya/b(x) satisfying the homogeneous
equation L ya/b(x) = 0.
Second step: ﬁnd the particular solution of the inhomogeneous equation
Now we need to ﬁnd yp(x). Since this is also an auxiliary function, its boundary or initial
conditions are arbitrary. Let’s use yp(x0) = B1 and y′
p(x0) = B2, where B1 and B2 are
arbitrary constants. We can use any method of the initial-value problems to ﬁnd yp(x).
Third step: impose the physical boundary conditions
In this ﬁnal step we must impose the physical boundary conditions stated by our
problem. Namely, we want y(x0) = y0 and y(x1) = y1. From Eq. (3.42) we have
y(x0) = aya(x0)+byb(x0)+ yp(x0) = bA2 +B1,
(3.46)
y(x1) = aya(x1)+byb(x1)+ yp(x1),
(3.47)
where ya(x0) = 0, yb(x0) = A2, and yp(x0) = B1 where the auxiliary initial conditions
set above. The quantities ya(x1), yb(x1), and yp(x1) are known from the solution of the
auxiliary initial-value problems. To satisfy the boundary conditions, the coefﬁcients a
and b must be
b = y0 −B1
A2
,
(3.48)
a =
y1 −byb(x1)−yp(x1)
ya(x1)
.
(3.49)
Combining these quantities, we have the all terms of Eq. (3.42) to compose our ﬁnal
solution y(x).
Example: the Poisson equation via the Wronskian method
Let’s apply the Wronskian method to the Poisson equation to reproduce numerically the
example of the beginning of this section; see Fig. 3.1. Let’s write the Poisson equation,
Eq. (3.38), using the notation of the Sturm-Liouville problem above. It reads
d2
dx2 y(x) = −ρ(x).
(3.50)
Next we use the ODE package to numerically ﬁnd the auxiliary functions ya(x), yb(x)
and yp(x) with appropriate initial conditions, and combine them at the end to satisfy
the physical boundary condition.
The homogeneous version of the 1D Poisson’s equation (ρ(x) = 0) is actually Laplace’s
equation in one dimension. Therefore it would be quite easy to ﬁnd the solutions ya(x)
and yb(x) of the homogeneous equation. It’s simply ya(x) = (x −x0)A1 and yb(x) = A2.
However, in the numerical code below we choose to ﬁnd these simple solutions numeri-
cally just to exemplify how to proceed in a more difﬁcult scenario.
58

Ordinary Differential Equations
Example 3.5: Poisson via Wronskian method
using ODE
using PyPlot
z = linspace(-1.0, 1.0, 1000); # z axes
eps0 = 1.0; # ε0
# charge density ρ(z)
g = 0.01; # broadening Γ
rho(z) = exp(-(z.^2)/(2*g^2))/(sqrt(2pi)*g);
# physical boundary conditions: y0 and y1
y0 = -1.0;
y1 = +1.0;
# find ya(z)
A1 = 1.0;
ic = [0.0; A1]; # = (0, A1)
rhs(z, y) = [y[2]; 0.0];
za, ya = ode45(rhs, ic, z; points=:specified);
ya = map(k->k[1], ya);
# find yb(z)
A2 = 1.0;
ic = [A2; 0.0]; # = (A2, 0)
rhs(z, y) = [y[2]; 0.0];
zb, yb = ode45(rhs, ic, z; points=:specified);
yb = map(k->k[1], yb);
# find yp(z)
B1 = 0.0;
B2 = 0.0;
ic = [B1; B2]; # = (B1, B2)
rhs(z, y) = [y[2]; -rho(z)/eps0];
zp, yp = ode45(rhs, ic, z; points=:specified);
yp = map(k->k[1], yp);
# coefficients and final solution
b = (y0 - B1)/A2;
a = (y1 - b*yb[end] - yp[end])/ya[end];
y = a*ya + b*yb + yp;
plot(z, y);
3.2.4
Schroedinger equations: transmission across a barrier
Let’s consider a free electron in one dimension colliding with a general shaped barrier
set by a potential V (x). The dynamics of this electron is tunnel or scatter back as it
collides with the barrier. Here we want to calculate the transmission T and reﬂection R
probabilities.
59

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
The Hamiltonian of the system is
H = −1
2
∂2
∂x2 +V (x).
(3.51)
Here we use atomic units (ħ = 1, m = 1), and V (x) is a general potential within the
scattering region 0 ≤x ≤L, and V (x) = 0 outside these limits.
Referring to the region x < 0 as I, let’s assume that the state of the electron is a linear
combination of the injected and reﬂected waves,
ψI(x) = eikx +re−ikx,
(3.52)
where r the reﬂection coefﬁcient, and k =
p
2ε and ε is the electron energy.
We’ll refer to the region x > L as III. There the electron state is composed solely by
the transmitted wave,
ψI I I(x) = teikx,
(3.53)
where t is the transmission coefﬁcient.
At the central region (II), where 0 ≤x ≤L, we need to ﬁnd two linear independent
solutions, ψa(x) and ψb(x), of the Schroedinger equation Hψ(x) = εψ(x). I’ll leave
this as a Problem for the reader. These solutions can be combined in a general linear
combination
ψI I(x) = aψa(x)+bψb(x),
(3.54)
where a and b are the linear combination coefﬁcients.
The ﬁnal solution and its derivative must be continuous at the interfaces x = 0 and
x = L. Therefore, we impose the conditions: (i) ψI(0) = ψI I(0); (ii) ψ′
I(0) = ψ′
I I(0); (iii)
ψI I(L) = ψI I I(L); (iv) ψ′
I I(L) = ψ′
I I I(L). These give us the following equation for the
coefﬁcients a, b, r and t:


ψa(0)
ψb(0)
−1
0
ψ′
a(0)
ψ′
b(0)
ik
0
ψa(L)
ψb(L)
0
eikL
ψ′
a(L)
ψ′
b(L)
0
ikeikL




a
b
r
t

=


1
ik
0
0

.
(3.55)
From the equation above one can easily extract t. The transmission probability is
then T = |t|2, and the reﬂection probability is R = |r|2. You can check that T +R = 1.
In Fig. 3.2 we consider the transmission over a Gaussian barrier
V (x) = e−1
2(x−x0)2,
(3.56)
where x0 = L/2 is the center of the barrier, and L = 20.
60

Ordinary Differential Equations
Figure 3.2: (left) Illustration of the Gaussian barrier with the injected, reﬂected and
transmitted waves at outer regions. (right) Transmission probability T as a function of
the energy ε.
3.2.5
Non-linear differential equations
For non-linear differential equations, the properties of Sturm-Liouville operator dis-
cussed in the previous sections does not hold. Particularly, there is no superposition
principle for non-linear ODEs. As a consequence, the task to solve a boundary value
problem for a non-linear ODE can be quite difﬁcult.
The non-linearity implies that a ODE with a speciﬁc set of boundary conditions may
have more than one solution. This is again in direct contrast with the Sturm-Liouville
case, where the uniqueness theorem (by Picard–Lindelöf) states that given the boundary
or initial conditions, the ODE has only one solution.
Let’s illustrate this with a very simple differential equation:
d2y(x)
dx2
+|y(x)| = 0,
(3.57)
which is non-linear due to the absolute value in the second term. Consider that the
boundary conditions are y(0) = 0 and y(4) = −2.
The shooting method
One way of solving this problem is the shooting method. First, convert the second order
differential equations into a pair of coupled ﬁrst order differential equations. This is
the same procedure used at the begging of this chapter. We know how to solve initial
value problems easily. Therefore, consider the auxiliary initial conditions y(0) = y0 and
y′(0) = v0. Clearly, it is useful to set y0 = 0 to automatically satisfy our desired boundary
condition. The problem is v0. What is the appropriate value of v0 that satisﬁes our
boundary conditions? Namely, we want v0 set such the evolution of the initial value
problem yields y(4) = −2.
In the general there’s no direct approach to ﬁnd the appropriate value of v0. In our
example there’s actually two possible values of v0 that yield y(4) = −2. To see this, I
invite the reader to write a code using the Runge-Kutta method, or Julia’s ODE package
61

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
to solve the initial value problem for the example above for different values of the initial
condition y′(0) = v0. Evolving the solution from x = 0 to x = 4 we can extract y(4) for
each value of v0 and plot y(4)× v0. This is shown in Fig. 3.3.
Figure 3.3: (top) Values of y(4) obtained propagating the non-linear differential equation
from x = 0 to x = 4 with the initial condition set to y′(0) = v0, as a function of v0. There
are two values of v0 that satisfy y(4) = −2, as indicated by the circles. (bottom) The two
possible solutions y(x)× x for the values of v0 found to satisfy the boundary conditions.
3.3
The eigenvalue problem
Here we’ll consider only eigenvalue problems of linear and homogeneous differential
equations. Particularly, we are only interest again in the Sturm-Liouville problems. In
the Sturm-Liouville Eq. (3.40), the differential equation is an eigenvalue problem if
w(x) ̸= 0 and r(x) = 0. Therefore we have
½ d
dx
·
p(x) d
dx
¸
+ q(x)
¾
y(x) = λw(x)y(x),
(3.58)
where p(x), q(x) and w(x) are given functions set by the problem at hand, while λ and
y(x) are the unknowns that we want to ﬁnd. For a set of boundary conditions, this
differential equation have solutions only for particular values of λ, which are called the
eigenvalues. The solutions y(x) associated with each particular λ are the eigenfunctions.
If we vectorize this equation using ﬁnite differences, the function y(x) becomes a vector:
the eigenvector.
There are many ways of solving an eigenvalue problem numerically. But let’s ﬁrst
check two analytical cases to serve as examples where we can try our numerical ap-
proaches.
62

Ordinary Differential Equations
3.3.1
Oscillations on a string
A string ﬁxed at its end points separated by a distance ℓsatisfy the wave equation,
∂2y(x,t)
∂x2
−1
v2
∂2y(x,t)
∂t2
= 0,
(3.59)
where y(x,t) is the string proﬁle as a function of space x and time t, and v is the wave
velocity on the string. This is a partial differential equation, but in this chapter we are
dealing with ordinary differential equations only. Since for now we are only interested
in the normal modes of oscillation, we can Fourier transform the equation from time t
to frequency w, yielding
∂2y(x,ω)
∂x2
+ ω2
v2 y(x,ω) = 0.
(3.60)
This equation takes the form of eigenvalue Sturm-Liouville problem if we set p(x) = 1,
q(x) = 0, w(x) = 1, and ω2 = v2λ.
If the string is ﬁxed at its end points, the boundary conditions are y(0,t) = 0, and
y(ℓ,t) = 0 for any t. Equivalently, the Fourier transformed y(x,ω) also satisfy the same
boundary conditions: y(0,ω) = 0 and y(ℓ,ω) = 0 for any ω.
You have probably seen already in a theoretical physics class that the solution to
this problem is
y(x,ωn) = Asin
³ωn
v x
´
,
(3.61)
where kn = ωn/v are the quantized wave-numbers, ωn = nπv/ℓare the quantized
frequencies, and the integer n labels the normal modes of oscillation. The amplitude A
depends on the initial condition, which we will not consider yet.
3.3.2
Electron in a box
An electron in a box is described by the Schroedinger equation. It’s dynamics is set
by the time-dependent Schroedinger equation. But similarly to the string oscillations,
its normal modes are given by a static equation, the time-independent Schroedinger
equation,
−1
2
∂2
∂x2 ψ(x)+V (x)ψ(x) = εψ(x),
(3.62)
where we use atomic units (ħ = 1, m = 1) for simplicity. Here V (x) is the electrostatic
potential that conﬁnes the electron, ψ(x) is the wave-function and ε is the energy. This
is indeed a Sturm-Liouville eigenvalue problem where the eigenvalue is the energy ε
and the eigenfunction is the wave-function ψ(x).
If the electron is trapped in a box of width ℓwith hard walls, such that V (x) = 0
for 0 < x < ℓ, and V (x) = ∞outside, the wave-function must satisfy the boundary-
conditions ψ(0) = 0 and ψ(ℓ) = 0.
63

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Since V (x) = 0 inside the box, the problem is very similar to the oscillations on a
string. The solutions are
ψn(x) = Asin(knx),
(3.63)
where the wave-numbers kn = nπ/ℓ, the eigenenergies εn = 1
2k2
n, and the integer n
labels the quantized eigenstates of the electron.
3.3.3
Method of ﬁnite differences
For simplicity let’s consider the Sturm-Liouville eigenvalue problem with p(x) = 1 and
w(x) = 1,
½ d2
dx2 + q(x)
¾
y(x) = λy(x).
(3.64)
We have seen in Chapter 2.3.2 that we can represent the derivative operators as
matrices. Particularly, the second derivative becomes
d2
dx2 y(x) =


y′′
1
y′′
2
y′′
3
y′′
4
···
y′′
i
···
y′′
N−1
y′′
N


= 1
h2


−2
1
0
0
0
0
0
0
0
1
−2
1
0
0
0
0
0
0
0
1
−2
1
0
0
0
0
0
0
0
1
−2
1
0
0
0
0
0
0
0
1
−2
1
0
0
0
0
0
0
0
1
−2
1
0
0
0
0
0
0
0
1
−2
1
0
0
0
0
0
0
0
1
−2
1
0
0
0
0
0
0
0
1
−2




y1
y2
y3
y4
···
yi
···
yN−1
yN


, (3.65)
where yi = y(xi) and xi are the discretized coordinates. I leave as an exercise to the
reader to check that this representation is compatible with the boundary conditions
y(0) = 0 and y(ℓ) = 0. Here the discretization of x is such that x0 = 0 and xN+1 = ℓ, and
h is the discrete step.
The q(x) term is local, therefore its matrix representation is diagonal,
q(x)y(x) =


q1
0
0
0
0
0
0
0
0
0
q2
0
0
0
0
0
0
0
0
0
q3
0
0
0
0
0
0
0
0
0
q4
0
0
0
0
0
0
0
0
0
...
0
0
0
0
0
0
0
0
0
qi
0
0
0
0
0
0
0
0
0
...
0
0
0
0
0
0
0
0
0
qN−1
0
0
0
0
0
0
0
0
0
qN




y1
y2
y3
y4
···
yi
···
yN−1
yN


.
(3.66)
where qi = q(xi).
64

Ordinary Differential Equations
With these matrix representations, the differential equation above can be put in
a matrix form Hy = λy. The matrix H is the sum of matrices above, y is the vector
composed by yi, and λ is the eigenvalue.
The most used numerical package to solve eigenvalue problems in a matrix form is
Lapack4. The language Julia has this package natively implemented in the command
eig. Try to run the next example.
Example 3.6: Eigenvalues and eigenvectors
using PyPlot
# creates the matrix H = −∂2
x
H = 2*diagm(ones(10)) - diagm(ones(9),1) - diagm(ones(9),-1);
# calculates the eigenvalues and eigenvetors
evals, evecs = eig(H);
subplot(311)
plot(evecs[:,1]) # plot the first eigenvector
subplot(312)
plot(evecs[:,2]) # plot the second eigenvector
subplot(313)
plot(evecs[:,3]) # plot the third eigenvector
# print the first three eigenvalues
println("E1 = ", evals[1]);
println("E2 = ", evals[2]);
println("E3 = ", evals[3]);
As you can see in the example, the eig command receives the matrix H and returns
its eigenvalues in a vector and the eigenvetors as a matrix. In this eigenvector matrix,
each eigenvector is set as a column. Therefore evecs[:,n] returns the eigenvetor n.
3.4
Problems
Problem 3.1: Damped harmonic oscillator
Implement a code to solve the damped harmonic oscillator problem,
d2x(t)
dt2
= −ω2
0x(t)−γdx(t)
dt
.
(3.67)
Fix ω0 = 2π and solve the equation for different values of γ to show the solutions for the
three possible regimes: (i) subcritical; (ii) critical; and (iii) supercritical. You will ﬁnd the
exact solutions on the volume 2 of Ref. [9].
4Lapack: http://www.netlib.org/lapack/
65

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Problem 3.2: Compare Euler and RK for the pendulum
Compare the results of Example 3.1 and Example 3.4 for small and large time steps
tau, and for small and large oscillations (amplitude x0).
Problem 3.3: Runge Kutta RK4
Implement your own version of the RK4 code in Julia to solve the pendulum problem
or any other different equation you may prefer. Your code won’t be more efﬁcient
than the adaptive code in the ODE package. However it is a good practice for you to
implement the RK4 by yourself, since it is the most well known and used method for
differential equations.
Problem 3.4: Sturm-Liouville equation
Tip: Check Moysés Nussenzveig’s book, volume 2, chapter on oscillations [9].
(a) Derive the three properties listed for the homogeneous Sturm-Liouville problem:
(1) principle of superposition; (2) linear independence; and (3) Uniqueness of the
solution.
(b) Derive the generalization of the principle of superposition for inhomogeneous
equations.
Problem 3.5: Wronskian
(a) Show that for an homogeneous Sturm-Liouville differential equation the Wron-
skian W (x) = y′
a(x)yb(x)−ya(x)y′
b(x) can be written as
W (x) = W (x0)exp
½
−
Z x
x0
p1(x′)
p(x′) dx′
¾
,
(3.68)
(b) Show that if the Wronskian is nonzero over the whole domain, the solutions
ya(x) and yb(x) are linearly independent.
Problem 3.6: Quantum transmission probability for electrons
(a) Why do we identify the positive exponentials in ψI(x) and ψI I I(x), Eqs. (3.52)-
(3.53), as forward moving, and the negative one as a backwards motion?
(b) Show that matching the continuity conditions at the interfaces x = 0 and x = L
leads to Eq. (3.55).
(c) Implement a code using the Wronskian method to obtain ψa(x) and ψb(x) and
calculate t.
(d) Transform the code of item (c) into a function that receives the energy ε and re-
turns the transmission T = |t|2. Use this code to reproduce the transmission probability
plot shown in Fig. 3.2.
(e) Try now with two Gaussian barriers, and run for a thin energy grid. You will see
the Fabry–Pérot resonances at the quasi-conﬁned states of a resonant-tunneling diode.
66

Ordinary Differential Equations
Problem 3.7: The Schroedinger equation and the Sturm-Liouville parameters
(a) What are the expressions for the Sturm-Liouville parameters p(x), q(x), w(x)
and r(x) that represents the time-independent Schroedinger equation, Eq. (3.62)?
(b) Show that the solution of electron in a box problem is given by Eq. (3.63) with
eigenenergies εn = 1
2(nπ/ℓ)2.
Problem 3.8: Matrix representation of the derivative operator
(a) The matrix representation of the derivative operator using ﬁnite differences faces
a problem at the edges. Show that this is not a problem when we consider hard wall
boundary conditions: y(0) = 0 and y(ℓ) = 0.
(b) What changes in the matrix representation if we consider periodic boundary
conditions? Namely y(ℓ) = y(0).
Problem 3.9: The potential well
(a) Implement a code to solve the time-independent Schroedinger equation for a
Gaussian potential well,
V (x) = V0e−1
2(x−c)2,
(3.69)
with amplitude V0 = −10, centered at c = ℓ/2, hard-wall boundary conditions ψ(0) =
ψ(ℓ) = 0, and ℓ= 10.
(b) Make a plot of the potential well and the ﬁrst three eigenstates.
(c) Compare the solution with a discretization of x with 100 points and with 500
points. The result (eigenvalues and eigenvectors) must not change much if the imple-
mentation is correct.
67


CHAPTER 4
Fourier Series and Transforms
2π? Or not 2π?
The Fourier series and transform are among the most essential mathematical tools
for physicists. TO DO: Finish introduction...
Hereafter we assume that we are dealing with well behaved functions on their rele-
vant domains. We shall restrict ourselves to the one-dimensinal cases. Generalizations
are imediate.
Every periodic function f (x) can be expanded as a sum of trigonometric functions.
If the period of the function is λ, i.e. f (x +λ) = f (x), the Fourier series reads
f (x) =
∞
X
n=−∞
cne−iknx,
(4.1)
where kn = 2πn/λ, n ∈Z, and eiθ = cosθ +i sinθ. The coefﬁcients cn are obtained from
the orthogonality of the trigonometric functions, yielding
cn = 1
λ
Z x0+λ
x0
f (x)eiknxdx.
(4.2)
Here x0 is arbitrary and the integral runs over one period of the function f (x).
If the function f (x) is not periodic, but we are interested in a ﬁnite domain x ∈
[xi,x f ], one can extend consider a periodic extension of f (x) outside the domain, such
that the Fourier series applies with the replacements x0 →xi and λ →x f −xi. The
periodic extension of f (x) shall match the original function on the desired domain.
Consider now that the function f (x) lives in the symmetric domain x ∈[−λ
2 , λ
2 ],
and we take the limit λ →∞. The discrete kn now become inﬁnitesimally close, ∆k =
kn+1 −kn = 2π
λ →0, and become a continuous variable kn →k. Converting the sum
in Eq. (4.1) into an integral with P
n →λ
2π
R
dk, and replacing cn →(
p
2π/λ) ˜f (k), the
Fourier series, Eq. (4.1), becomes
69

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
f (x) =
1
p
2π
Z ∞
−∞
˜f (k)e−ikxdk,
(4.3)
˜f (k) =
1
p
2π
Z ∞
−∞
f (x)eikxdx,
(4.4)
where ˜f (k) = F[f (x)] is the Fourier transform of f (x), and reciprocally, f (x) = F −1[ ˜f (k)]
is the inverse Fourier transform of ˜f (k).
The next example shows the Fourier series. Play with the function and parameters.
Example 4.1: Fourier Series
using PyPlot
x = linspace(0.0, 30.0, 300); # domain
f(x) = sin(x)./x; # example function
lambda = x[end]-x[1]; # period
k(n) = 2pi*n/lambda; # wave number
# define a function to caculate the coefficients cn
c(n) = quadgk(x->f(x)*exp(1im*k(n)*x), x[1], x[end])[1]/lambda;
N = 10; # the sum will be
N
X
n=−N
Ni = -N:N; # array of values of n
Ci = [c(n) for n=Ni]; # calculate the coefficients and store in an array
# combine the arrays to recover the approximate function
fa=sum(i-> Ci[i]*exp(-1im*k(Ni[i])*x), 1:(2*N+1))
clf(); # the plot is shown in Fig. 4.1
subplot2grid((2,2),(0,0),colspan=2)
plot(x,f(x)); # plot the original function
plot(x, real(fa)) # and the approximated one
xlabel(L"$ x $");
ylabel(L"$ f(x) $");
subplot2grid((2,2),(1,0))
vlines(Ni, 0, real(Ci)); # real part of cn
scatter(Ni, real(Ci));
xlabel(L"$ n $")
ylabel(L"$ Re\{c_n\} $")
subplot2grid((2,2),(1,1))
vlines(Ni, 0, imag(Ci)); # imaginary part of cn
scatter(Ni, imag(Ci));
xlabel(L"$ n $")
ylabel(L"$ Im\{c_n\} $")
tight_layout();
70

Fourier Series and Transforms
Figure 4.1: Output of Example 4.1.
Typically, when we work with a function f (x), where x is a space coordinate, the
Fourier transform is set in terms of the wave-number k as above. If our signal is a
function of time, i.e. f (t), the Fourier transform is usually set in terms of the angular
frequency ω as ˜f (ω) = F[f (t)].
4.1
General properties of the Fourier transform
There are many important properties of the Fourier transform that can be applied to
solve physics problems. Here I’ll list (without demonstrations) a few properties that will
be useful for us in this Chapter.
Linearity
Let f (x) = Ag(x)+Bh(x), the Fourier transform of f (x) is ˜f (k) = A ˜g(k)+B ˜h(k).
Translation
For a constant shift x0 in f (x) = g(x−x0), we get ˜f (k) = e−ix0k ˜g(k). Equivalently, a phase
set by k0 as f (x) = eik0xg(x) leads to a Fourier transform shifted in the reciprocal space
˜f (k) = g(k −k0).
71

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Scaling
For a real number A in f (x) = g(Ax), the Fourier transform scales as ˜f (k) = 1
|A| ˜g(k/A).
Derivatives
Let fn(x) = dn
dxn g(x) denote the n-th derivative of g(x), the transform is ˜fn(k) = (ik)n ˜g(k).
Equivalently, for f (x) = xng(x), the transform is ˜f (k) = i n dn
dkn ˜g(k).
Convolution
The convolution of the functions g(x) and h(x) is
f (x) = (g ∗h)(x) =
Z ∞
−∞
g(x′)h(x −x′)dx′.
(4.5)
The Fourier transform of a convolution is ˜f (k) =
p
2π ˜g(k) ˜h(k), which is simply the
product of Fourier transformed functions. Conversely, if the function is the product of
g(x) and h(x), i.e. f (x) = g(x)h(x), the Fourier transform is ˜f (k) = ( ˜g ∗˜h)/
p
2π.
4.2
Numerical implementations
4.2.1
Discrete Fourier Transform (DFT)
The Discrete Fourier Transform is equivalent to the Fourier series applied on a discrete
x lattice. It can be obtained from Eqs. (4.1)-(4.2) for x0 = 0, dx = λ/N, x →xn = nλ/N,
and converting the integral in Eq. (4.2) into a sum.
Let f be an array with N elements representing the function f (x). The Fourier
transform ˜f (k) = F[f (x)] is represented by an array ˜f whose elements are
˜fm =
N
X
n=1
fn exp
·
−i 2π(n −1)(m −1)
N
¸
.
(4.6)
Conversely, given a vector ˜f representing a function ˜f (k) in k-space, the inverse
Fourier transform f (x) = F[ ˜f (k)] is represented by the array f with elements
fn = 1
N
N
X
m=1
˜fm exp
·
+i 2π(m −1)(n −1)
N
¸
.
(4.7)
Since we have to calculate all elements to compose the full array ˜f or f , the DFT or
the inverse DFT takes N 2 operations. In contrast, the Fast Fourier Transform (FFT)
algorithm takes N log2 N operations, which is much smaller than N 2 for large N.
4.2.2
Fast Fourier Transform (FFT)
(TO DO) Here I’ll follow the Radix-2 code of Cooley-Tukey: https://en.wikipedia.
org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm
72

Fourier Series and Transforms
4.2.3
Julia’s native FFT
Julia comes with a native implementation of FFT via the efﬁcient FFTW library1. We have
implicitly used the FFT before when using the command conv to perform convolutions.
The main commands to remember are fft, ifft, fftshift, and ifftshift.
These functions act on arrays. Therefore, hereafter consider a discrete domain
(linspace) x with spacing ∆x and the array f (x) →f set on this domain.
Figure 4.2: Output of Example 4.2.
First, the command fft acts on an array f and returns its Fourier transform ˜f .
Reciprocally, the command ifft acts on the array ˜f and returns its inverse Fourier
transform f .
Notice that the array f represents the function f (x) over the discrete domain set
by the array x. Conversely, the array ˜f represents the Fourier transformed function
˜f (k). The FFT implementation deﬁnes the domain of reciprocal space as k ∈[0, 2π
∆x ).
However, due to the periodicity of the Fourier transforms, the region
π
∆x ≤k < 2π
∆x is
actually equivalent to the region −π
∆x ≤k < 0. Typically, it is more interesting to work on
the reciprocal space in the symmetric domain k ∈[−π
∆x , π
∆x ).
One can use the commands fftshift and ifftshift to switch between these
choices of domains above. The fftshit converts the domain from k ∈[0, 2π
∆x ) into
k ∈[−π
∆x , π
∆x ), while ifftshift performs the inverse operation.
1FFTW: http://www.fftw.org/
73

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
The next example uses the translation property of the Fourier transform to shift a
function by x0. Check also Problem 4.2.
Example 4.2: Fourier transform using the FFT
using PyPlot
n = 50; # using a small number of points for better visualization
x = linspace(-5.0, 15.0, n); # x domain
dx = x[2]-x[1]; # spacing ∆x
k = linspace(-pi/dx, pi/dx, n); # k domain (symmetric)
# function in x-space
f = exp(-x.^2); # example function
# function in k-space
# and using fftshift to put ft in the symmetric domain
ft = fftshift(fft(f));
x0 = 10; # let’s shift the function
gk = ft.* exp(-1im*x0*k); # see the Fourier transform properties
# goes back to the [0, 2pi
dx ) k-domain and apply ifft
g = ifft(ifftshift(gk));
# g(x) = F −1£
e−ix0k ˜f (k)
¤
clf(); # the plot is shown in Fig. 4.2
subplot2grid((2,2),(0,0),colspan=2)
plot(x,f); # plot the original function
plot(x, real(g)) # and shifted one
xlabel(L"$x$");
ylabel(L"$f(x)$ and $g(x)$")
# plot the real part of the Fourier transform
# shifting to the symmetric k-domain
subplot2grid((2,2),(1,0))
plot(k, real(ft));
xlabel(L"$k$")
ylabel(L"$Re\{\tilde{f}(k)\} $")
# plot the imaginary part of the Fourier transform
# shifting to the symmetric k-domain
subplot2grid((2,2),(1,1))
plot(k, imag(ft));
xlabel(L"$k$")
ylabel(L"$Im\{\tilde{f}(k)\} $")
tight_layout();
74

Fourier Series and Transforms
4.3
Applications of the FFT
4.3.1
Spectral analysis and frequency ﬁlters
Let ˜f (ω) = F[f (t)] be the Fourier transform of f (t). The power spectrum of f (t) is
S(ω) = | ˜f (ω)|2. The plot of S(ω) vs ω shows us which components (frequencies ω) of the
Fourier transform contribute to the signal f (t). The power spectrum characterizes the
color of a light beam, and the timbre of a musical instrument.
If our signal is simply f (t) = Asin(ω0t), its Fourier transform is
˜f (ω) = i A
rπ
2
h
δ(ω−ω0)−δ(ω+ω0)
i
,
(4.8)
and the power spectrum will show two δ-peaks at ω = ±ω0. This is the expected solution
of the simple pendulum in the harmonic regime. Check Problem 4.3.
Noise and frequency ﬁlters
What if our signal was noisy? There are many types of noise depending on their source2.
Musical instruments may suffer from high frequency noises due to electronic systems,
bad cables, etc. The type of noise is deﬁned by its power spectrum. For instance, a white
noise shows a constant power spectrum will all frequencies contributing equally. Hence
the name white noise as a reference to white light. The power spectrum of a pink noise
decays with 1/ω and appears in most natural phenomena. The types of noise got their
names as colors due to the initial analogy of white noise and white light.
In the next example, let’s artiﬁcially generate and eliminate a white noise. The result
is shown if Fig. 4.3. The white noise in this example have a power spectrum S(ω) ≈10,
and two signal peaks show at ω = ±2π. To ﬁlter the white noise we simply collect only
the fft components that are above the white noise power spectrum.
2The colors of noise (Wikipedia)
75

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Example 4.3: White noise
using PyPlot
N=300000; # generate signal with large number of points
t=linspace(0, 30, N); # time domain
dt=t[2]-t[1];
w=linspace(0, 2pi/dt, N); # frequency domain
w0 = 2pi; # original frequency of the signal
# signal + random fluctations:
f=(1+0.5*randn(N)).*sin((1+0.001*randn(N)).*w0.*t);
g = fft(f); # fft of the signal
s = log(abs2(g)); # power spectrum of the signal
g2 = g.*(s .> 15); # filters the spectrum above the white noise (~10)
s2 = log(1e-10+abs2(g2)); # spectrum of the filtered signal
f2 = ifft(g2); # recovers filtered signal with inverse FFT
clf();
subplot(211)
# first half shows original signal with noise
# second half shows filtered signal, now clean
n2 = Int(N/2);
plot(t[1:n2], f[1:n2])
plot(t[n2:end], f2[n2:end]);
xlabel(L"t")
ylabel(L"f(t)");
subplot(212)
# plot the power spectra
plot(w-pi/dt, fftshift(s));
plot(w-pi/dt, fftshift(s2));
xlim([-3w0, 3w0]);
xlabel(L"\omega")
ylabel(L"\logS(\omega)");
tight_layout();
In practice, each noise color requires a different method to be eliminated. Typical
cases are the high-pass and low-pass ﬁlters, that apply frequency cuts to allow only high
and low frequencies, respectively.
4.3.2
Solving ordinary and partial differential equations
There are many different approaches to solve partial differential equations (PDE), which
we will discuss in Chapter 6. For now, we shall discuss only the a method using the
Fourier transform. As paradigmatic examples, we will use the diffusion equation for
the homogeneous case, and the Schrödinger equation for the inhomogeneous case.
However, keep in mind that the method used in the Schrödinger equation is also valid
for an inhomogeneous diffusion equation.
76

Fourier Series and Transforms
Figure 4.3: Output of Example 4.3.
Diffusion equation
Let’s consider the homogeneous diffusion equation in one dimension,
∂
∂t u(x,t)−D ∂2
∂x2 u(x,t)+ v ∂
∂x u(x,t) = 0,
(4.9)
where D is the diffusion coefﬁcient, v is the drift velocity and u(x,t) is density of the
diffusing material (mass, temperature, ...).
In this very simple homogeneous case, the diffusion equation has an analytical
solution. For an initial condition u(x,0) = δ(x),
u(x,t) =
1
2
p
πDt
exp
·
−(x −vt)2
4Dt
¸
,
(4.10)
which is a Gaussian packet with a center that moves as x = vt, and broadens with time.
To ﬁnd this solution, we ﬁrst take the Fourier transform x →k of the diffusion
equation,
∂
∂t ˜u(k,t)+Dk2 ˜u(k,t)+ivk ˜u(k,t) = 0.
(4.11)
It is easy to show that the solution in k-space is ˜u(k,t) = e(−ivk−dk2)t ˜u(k,0), where
˜u(k,0) is the Fourier transform of the initial condition u(x,0). If the initial condition
77

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
is u(x,0) = δ(x), then ˜u(k,0) = 1/
p
2π. The inverse Fourier transform can be easily
calculated analytically to obtain the solution u(x,t) above.
In this simple case we don’t really need any numerical calculation. However, you
could eventually face a more general diffusion equation (inhomogeneous, vectorial,
coupled, ...) for which numerical methods will be necessary. Therefore, let’s illustrate
the numerical method assuming that you are able to obtain the solution in k-space
˜u(k,t) analytically, but you are not able to calculate the Fourier transform of the initial
condition ˜u(k,0) = F[u(x,0)] and the inverse Fourier transform to recover u(x,t) =
F −1[ ˜u(k,t)]. The next example solves this problem.
Example 4.4: Diffusion equation
using PyPlot
D = 1; # diffusion coefficient
v = 1; # drift velocity
N = 100; # number of points
L = -10;
x = linspace(-L, L, N); # x domain
dx = x[2]-x[1]; # x step
k = linspace(-pi/dx, pi/dx, N); # k domain (symmetric)
ux0 = exp(-x.^2); # initial condition u(x,0)
uk0 = fftshift(fft(ux0)); # initial condition in k-space ˜u(k,0)
# analytical solution in k-space written with the numerical k vector
ukt(t) = exp((-1im*v*k-D*k.^2)*t).*uk0;
# the solution u(x,t) as a function that uses the inverse FFT
uxt(t) = ifft(ifftshift(ukt(t)));
# plot the resuts for different times t = 0,1,2,3
clf();
plot(x, ux0);
plot(x, uxt(1));
plot(x, uxt(2));
plot(x, uxt(3));
xlabel(L"$x$");
ylabel(L"$u(x,t)$"];
Quantum operators in k-space, the split-step method
Trying to ﬁnd an easy way to prove the O(τ3)
Let H(x,p,t) = T (p,t)+V (x,t) be a general Hamiltonian that can be written as a
sum of two terms, where the ﬁrst, T (p,t), is a function of the momentum operator
p = −iħ∂x and the time t, while the second, V (x,t), is a function of the position x and
time t. In general the commutator [T,V ] ̸= 0. We want to solve the time-dependent
78

Fourier Series and Transforms
Schrödinger equation
iħ ∂
∂t ψ(x,t) = H(x,p,t)ψ(x,t).
(4.12)
For a small time step τ, an approximate numerical solution can be computed as
ψ(x,t +τ) ≈e−i V τ
2ħ e−i T τ
ħ e−i V τ
2ħ ψ(x,t).
(4.13)
Here we start with half of a time step, τ/2 using V, followed by a full time step τ using T ,
and another τ/2 to complete the evolution from t to t +τ. This spitting of the operators
generates an error of order O(τ3).
More interestingly, this approach can be extremely efﬁcient if we notice that T is a
function of momentum p, and, therefore, can be better represented in Fourier space
(see the derivative property of the Fourier transform). The trick is to carry all operations
using V in coordinates space, while those involving T are done in k-space. In terms of
Fourier transforms, the evolution reads
ψ(x,t +τ) ≈exp
µ
−i V τ
2ħ
¶
F −1h
exp
µ
−i
˜T τ
ħ
¶
F
h
exp
µ
−i V τ
2ħ
¶
ψ(x,t)
ii
,
(4.14)
where ˜T ≡˜T (k,t) is the Fourier transform p →ħk of the T (p,t) operator.
Notice that V (x,t) becomes a diagonal matrix when we discretize x, but T (p,t) is
non-diagonal, as it contains derivatives in the momentum operator. However, in k-space,
˜T (k,t) is diagonal. Therefore, using the Fourier transform as in the equations above, all
exponentials will be deﬁned in terms of diagonal matrices, which is easy to calculate.
In fact, calculating the exponential of a general matrix is much less efﬁcient than the
FFT. Hence the high efﬁciency of the split-step method. The next example illustrates
the difference between the coordinate- and k-space calculations of φ = exp
¡
−i T τ
ħ
¢
φ0,
where T = −1
2p2.
4.4
Problems
Problem 4.1: Discrete Fourier Transform
Implement your own version of the Discrete Fourier Transform (DFT) in Julia and
compare the results with Example 4.2. Your implementation will not be efﬁcient and
you shall use Julia’s native fft function always. However, implementing the DFT will
help you learn more about coding.
Problem 4.2: Derivatives with the FFT package
Check again Example 4.2 and adapt it to use the derivative property of the Fourier
transforms to calculate the ﬁrst, second, and third derivatives of a function f (x) set on
a discrete domain. Choose a simple function f (x), so that you can ﬁnd its derivatives
analytically and compare with the numeric results.
79

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
Problem 4.3: Pendulum: small vs large amplitudes
In the previous chapter, you have written a code to solve the differential equation of
a pendulum,
d2
dt2 x(t) = −ω2
0 sin[x(t)]−γvx(t).
(4.15)
For now, let’s neglect the damping term, γ = 0. Consider ω0 = 2π and the initial
conditions x(0) = x0, and v(0) = 0. Prepare your code to run over 0 ≤t ≤100.
(a) Find x(t) for small amplitudes set by x0 = 0.01π and use the fft command to
obtain ˜x(ω). Plot the spectral function S(ω) = | ˜x(ω)|2 for −2ω0 ≤ω ≤2ω0.
(b) Do the same for large amplitudes set by x0 = 0.99π.
(c) Discuss the differences between the power spectra of cases (a) and (b).
Problem 4.4: Pendulum: large amplitudes and damping
Consider the same equation an parameters from the previous problem. But now
let’s start with a large amplitude x0 = 0.99π, and a ﬁnite damping γ = 0.001. Solve the
differential equation for 0 ≤t ≤6000 with a large number of points (∼105). As a function
of t you shall see the oscillations loosing amplitude due to the damping.
(a) Plot the spectral function S(ω) using only the numerical data of x(t) for small t,
say in a window 0 ≤t ≤500.
(b) Plot S(ω) for the data at large t, say in a window 5500 ≤t ≤6000.
(c) Explain the difference between the spectral functions.
Problem 4.5: Driven pendulum
Let’s analyze the power spectrum of a driven pendulum[6]. Consider a driven pen-
dulum set by
d2
dt2 x(t) = −ω2
0 sin[x(t)]−γvx(t)+ f0 cos(ω1t).
(4.16)
(a) Solve this differential equation for x(0) = 0, vx(0) = 2, and 0 < t < 3000, using the
parameters ω0 = 1, ω1 = 2/3, γ = 1/2, and f0 = 0.9.
(b) Now try it with the same parameters, except for f0 = 1.15.
(c) The power spectrum is S(ω) = | ˜x(ω)|2, where ˜x(ω) is the Fourier transform (t →ω)
of x(t). Compare the power spectra of cases (a) and (b). The ﬁrst is periodic, and
will show narrow peaks at the motion frequencies. The second is chaotic, almost all
frequencies contribute and the power spectrum is fractal.
80

CHAPTER 5
Statistics (TO DO)
What are the odds of having a silly quote here?
5.1
Random numbers
5.2
Random walk and diffusion
5.3
The Monte Carlo method
81


CHAPTER 6
Partial Differential Equations (TO DO)
6.1
Separation of variables
6.2
Discretization in multiple dimensions
6.3
Iterative methods, relaxation
6.4
Fourier transform (see §4.3.2)
83


CHAPTER 7
Plotting (not ﬁnished)
... is worth a thousand words
J
ULIA has a few interesting packages for plotting:
• PyPlot1: provides an interface to Python’s MatPlotLib;
• Gaston2: provides a gnuplot interface to Julia;
• Gadfly3: “The Grammar of Graphics”;
• Winston4: brings the MATLAB syntax to Julia plots.
Here we will discuss PyPlot to take advantage of its extensive documentation and
large number of users, making it easy to ﬁnd examples online.
7.1
PyPlot
7.1.1
Installing PyPlot
To install PyPlot, ﬁrst you need Python and Matplotlib. If you use Ubuntu Linux, run
$ sudo apt-get update
$ sudo apt-get install python python-matplotlib
Now you can install PyPlot. Start Julia and run: julia> Pkg.add("PyPlot").
1PyPlot: https://github.com/stevengj/PyPlot.jl
2Gaston: https://github.com/mbaz/Gaston.jl
3Gadﬂy: http://dcjones.github.io/Gadfly.jl/
4Winston: https://github.com/nolta/Winston.jl
85

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
7.1.2
Using PyPlot
To use PyPlot, ﬁrst you have to initialize it by calling: julia> using PyPlot.
Example 7.1: Using PyPlot
using PyPlot
x = linspace(-3pi,3pi,200);
plot(x,
sin(x)./x, color="red", linewidth=2.0, linestyle="-");
plot(x, -sin(x)./x, color="blue", linewidth=2.0, linestyle="--");
xlabel("x");
ylabel("f(x)");
Since the PyPlot package is an interface to Python’s Matplotlib, one may use its
extensive documentation5.
7.1.3
Most useful features and examples
Calling non-ported functions from Matplotlib
PyPlot’s documentation says that only the documented Python’s matplotlib.pyplot
API is exported to Julia. Nonetheless, other functions from Python’s PyPlot can be
accessed as
matplotlib.pyplot.foo(...) −→plt[:foo](...),
where on the left we have Python’s notation, and on the right Julia’s.
For instance, you can generate an histogram using:
Example 7.2: Using non-ported functions from Matplotlib
using PyPlot
x = randn(100000);
plt[:hist](x);
xlabel("Random number");
ylabel("Number of occurrences");
Latex
It is very easy to use Latex with Julia’s PyPlot package. Implicitly, PyPlot uses the La-
TexStrings package6. Therefore on can use Latex commands on a string simply con-
structing it prepending with a L. For instance:
5Matplotlib’s documentation: http://matplotlib.org/
6LaTeXStrings: https://github.com/stevengj/LaTeXStrings.jl
86

Plotting (not ﬁnished)
Example 7.3: Using Latex
using PyPlot
x = linspace(0, 2pi, 100);
f = sin(x);
plot(x, f);
xlabel(L"$\theta$ [rads]");
ylabel(L"$\sin\theta$ [a.u.]");
This Latex notation can be used in all text elements of PyPlot.
Subplot
The subplot command allows you to break the plot window into a grid. Say that the
desired grid has N lines and M columns, the command subplot(N, M, j) speciﬁes that
you want to plot the next ﬁgure into the j-th element of the grid, with the elements
sorted in a “Z” shape. Try the next example.
Example 7.4: Using subplot
using PyPlot
x = linspace(-5pi, 5pi, 1000);
clf();
subplot(2,2,1);
plot(x, sin(x));
xlabel(L"$x$");
ylabel(L"$\sin(x)$");
subplot(2,2,2);
plot(x, cos(x));
xlabel(L"$x$");
ylabel(L"$\cos(x)$");
subplot(2,2,3);
plot(x, exp(-x.^2));
xlabel(L"$x$");
ylabel(L"Gaussian");
subplot(2,2,4);
plot(x, sin(x)./x);
xlabel(L"$x$");
ylabel(L"$sinc(x)$");
tight_layout(); # adjust the subplots to better fit the figure
87

Introduction to Computational Physics
Gerson J. Ferreira - INFIS - UFU - Brazil
In the example above we are using Latex notation for the labels. The ﬁnal command
tight_layout(...) allows you to adjust the subplots spacings to better adjust them within
the ﬁgure. This avoids overlapping of the labels and plots.
Even more control can be achieved with the command subplot2grid(...), which
we use in Fig. 4.1, for instance. Here the shape is passed as a tuple (N,M) to the ﬁrst
parameter, the second parameter takes the location of the desired plot an ordered pair
(i, j), where 0 ≤i ≤(N −1) and 0 ≤j ≤(M −1). The top-left corner grid element is
(i, j) = (0,0), and the bottom-right is (i, j) = (N −1,M −1). The subplot2grid becomes
more interesting as the next parameters allow you to set a rowspan and a colspan to
span over cells. Check again Example 4.1.
Labels
Relevant commands: text
Legends
Relevant commands: legend
Other plot elements
Relevant commands: annotate, arrow, axes, axis, axhline, axvline, contour,
Saving into ﬁles (PNG, PDF, SVG, EPS, ...)
Figures are shown by default on screen as PNG. However, you can save it to ﬁles with
many different formats using the savefig(...) command. Check the example:
Example 7.5: Using subplot
using PyPlot
x = linspace(-5pi, 5pi, 1000);
f = sin(x);
plot(x, f);
xlabel(L"$\theta$ [rads]");
ylabel(L"$f(\theta) = \sin\theta$");
savefig("myplot.png");
savefig("myplot.pdf");
savefig("myplot.svg");
savefig("myplot.eps");
Please check the full documentation of the matplotlib for more details. You can
choose the paper size, dpi, etc.
88

Plotting (not ﬁnished)
Animations
Here’s an example of using Julia and matplotlib to create animations7. The PyCall
package is used to import the animation library from Python, since it is not natively
implemented in PyPlot.
Example 7.6: Animated plot
using PyCall # package used to call Python functions
using PyPlot
# import the animation library
@pyimport matplotlib.animation as anim
# this function must plot a frame of the animation
# the parameter t will be iterated by the matplotlib
function animate(t)
# in this example we willl plot a Gaussian moving around
c = 3*sin(2pi*t);
x = linspace(-10, 10, 200);
y = exp(-(x-c).^2/2);
# clear the figure before ploting the new frame
clf();
# plot the new frame
plot(x,y)
# for this basic usage, the returned value is not important
return 0
end
# matplotlib’s FuncAnimation require an explicit figure object
fig = figure();
tspan = linspace(0, 5, 100); # will be used to iterate the frames
# animation with a interval of 20 miliseconds between frames
video = anim.FuncAnimation(fig, animate, frames=tspan, interval=20)
# save the animation to a MP4 file using ffmpeg and the x264 codec
video[:save]("anim.mp4", extra_args=["-vcodec", "libx264"])
7Adapted from Andee Kaplan’s presentation at http://heike.github.io/stat590f/gadfly/
andee-graphics/
89


CHAPTER 8
Other topics (TO DO)
... and yet another silly quote
8.1
Linear algebra
8.2
Root ﬁnding
8.3
Linear systems
8.4
Least squares
91


Bibliography
[1] Cláudio Scherer. Métodos computacionais da Física. Editora Livraria da Física, São
Paulo, 2010.
[2] Neide Bertoldi Franco. Cálculo numérico. Pearson Prentice Hall, São Paulo, 2006.
[3] Selma Arenales and Artur Darezzo. Cálculo numérico: aprendizagem com apoio de
software. Thomson Learning, São Paulo, 2008.
[4] J. C. Butcher. Numerical methods for ordinary differential equations. John Wiley &
Sons Ltd., Chichester, England, 2008.
[5] Jos Thijssen. Computational physics. Cambridge University Press, 2007.
[6] Tao Pang. An introduction to computational physics. Cambridge University Press,
2006.
[7] Edwin Abbott Abbott. Flatland: A romance of many dimensions. Princeton Univer-
sity Press, 2015.
[8] Thomas Haigh, Mark Priestley, and Crispin Rope. Eniac in Action: Making and
Remaking the Modern Computer. Mit Press, 2016.
[9] Herch Moysés Nussenzveig. Curso de física básica. Edgard Blücher, 2002, 2007.
93

