Explaining Graph Neural Networks for Node Similarity on Graphs
Daniel Daza∗
Vrije Universiteit Amsterdam
University of Amsterdam
The Netherlands
dfdazac@gmail.com
Cuong Xuan Chu
Trung-Kien Tran
Daria Stepanova
Bosch Center for Artificial Intelligence
Germany
firstname.lastname@de.bosch.com
Michael Cochez
Vrije Universiteit Amsterdam
The Netherlands
m.cochez@vu.nl
Paul Groth
University of Amsterdam
The Netherlands
p.groth@uva.nl
ABSTRACT
Similarity search is a fundamental task for exploiting information
in various applications dealing with graph data, such as citation
networks or knowledge graphs. While this task has been inten-
sively approached from heuristics to graph embeddings and graph
neural networks (GNNs), providing explanations for similarity has
received less attention. In this work we are concerned with ex-
plainable similarity search over graphs, by investigating how GNN-
based methods for computing node similarities can be augmented
with explanations. Specifically, we evaluate the performance of two
prominent approaches towards explanations in GNNs, based on
the concepts of mutual information (MI), and gradient-based expla-
nations (GB). We discuss their suitability and empirically validate
the properties of their explanations over different popular graph
benchmarks. We find that unlike MI explanations, gradient-based
explanations have three desirable properties. First, they are action-
able: selecting inputs depending on them results in predictable
changes in similarity scores. Second, they are consistent: the effect
of selecting certain inputs overlaps very little with the effect of
discarding them. Third, they can be pruned significantly to obtain
sparse explanations that retain the effect on similarity scores.
KEYWORDS
Similarity search, graph neural networks, explainability
1
INTRODUCTION
While large parts of Web data are still unstructured, both the re-
search community and industry have made great efforts to create
structured or semi-structured data such as graphs [13, 35], which
form the cornerstone for various applications [44]. In such appli-
cations, similarity search has evolved into a major topic [61]. For
example, similarity search can be used in recommendation systems
to recommend content to users based on the similarity of the con-
tent with user preferences. In information retrieval, as used for web
search similarity search provides results that are similar to a query.
We are concerned with similarity search over graphs, where
given a query node, the goal is to retrieve a list of similar nodes
ranked by a certain score. Several methods to solve this problem
have been proposed in the literature, ranging from heuristic-based
∗Work done during internship at the Bosch Center for AI.
methods to data-driven, machine learning methods. Heuristics for
similarity search on graphs exploit various graph statistics, or tech-
niques based on hashing to solve the problem [60, 61]. Machine
learning methods, on the other hand, avert the need to design hand-
engineering heuristics or features and instead they seek to exploit
domain-specific patterns in the graph to learn node representations,
or embeddings, so that similarities are captured via functions such
as cosine similarity. Graph neural networks (GNNs), in particu-
lar, have become a standard in machine learning approaches that
process graph-structured data [19, 28, 56].
While GNNs offer several advantages due to their capacity to
adapt to specific properties of the graph at hand, these benefits may
be compromised when interpretability becomes a necessity [1, 6].
Given their demonstrated effectiveness on different tasks, there are
compelling motivations to explore methods for explaining their
predictions [87], which would enable applications that require ac-
countable decision-making to leverage their predictive power.
While extensive works on explaining GNNs exist, the majority
of the methods focus on supervised learning problems, where the
predicted target is well-defined based on some ground-truth data,
as in the case of node classification [33, 34, 39, 86]. The applicability
of such methods to the problem of explaining node similarities,
often done via unsupervised learning in GNNs, is an open question.
In this work, we are interested in the problem of explaining node
similarities computed by GNN-based approaches. Fig. 1 illustrates
this problem, where an unsupervised learning algorithm is used
to train a GNN to obtain the embeddings for nodes 1 and 2. The
embeddings are used to compute the cosine similarity that we want
to explain. The explanation consists of an attribution of values to
edges depending on their influence on the similarity score, where
blue edges result in increasing similarity scores and red edge results
in decreasing the score. Depending on the explanation method used,
the effect of attribution values on similarity scores can be different.
We investigate the properties of two prominent methods for
explaining GNNs, based on the mutual information (MI) between
the graph and the prediction, and gradient-based (GB) explana-
tions. We discuss their properties, contrast them with desirable
explanations in the context of node similarity, and find that their
applicability changes, in comparison with other problems such as
node classification. We empirically evaluate the performance of
explanations by measuring the effect of intervening on the graph,
arXiv:2407.07639v1  [cs.LG]  10 Jul 2024

Trovato and Tobin, et al.
GNN
Explainer
Node Embeddings
Node Similarity
Explanation
Unsupervised Learning
Similarity =
Figure 1: Illustration of the problem we investigate in our work. Given nodes 1 and 2 in a graph, unsupervised learning methods
can be used to train a GNN to learn node embeddings, where a score of similarity can be estimated by cosine similarity. We are
interested in computing explanations for such scores, that assign values of attributions to edges in the graph. In this example,
we show with blue a positive influence in the similarity score, and with red a negative influence.
given the knowledge provided by an explanation. We conclude that
gradient-based methods are better suited for explaining similarities,
by providing explanations with a predictable and consistent effect
of increasing or decreasing similarity scores.
Our salient contributions are summarized as follows:
• We analyze the properties of two prominent approaches
for explaining GNNs in the context of node similarities
learned via unsupervised learning, which to the best of our
knowledge, has not been considered in previous work.
• We contrast these properties with general requirements
of explainable artificial intelligence systems, proposing a
series of desirable properties for explanations of node simi-
larities on graphs.
• We find that unlike MI explanations, gradient-based expla-
nations meet these properties. First, they are actionable:
selecting inputs depending on them results in predictable
changes in similarity scores. Second, they are consistent:
the effect of selecting certain inputs overlaps very little with
the effect of discarding them. Third, they can be pruned
significantly, resulting in sparse explanations that retain
the effects on similarity scores.
Our results provide practical insights for systems requiring ex-
planations for node similarities learned via GNNs.
2
RELATED WORK
Similarity learning. The problem of computing node similarities
on graphs has been addressed in previous methods that rely on
heuristics, rather than representations learned from the data. Some
examples of such methods rely on statistics of connectivity [5, 23],
co-occurrence statistics [24], meta-paths in heterogeneous net-
works [66], and metrics for measuring structural similarities [81].
Other methods employ ideas from hashing techniques to compute
vector representations useful for similarity search [20, 61, 89]. Such
heuristics are useful when they are broad enough to be applicable
to different graphs. Graph neural networks, on the other hand, are
able to adapt to specific signals present in the data, such as domain-
specific topological properties and rich multi-modal features like
text and images [17, 36]. Their demonstrated effectiveness for differ-
ent tasks thus warrants an investigation on how explanations can
be provided for them, in the event of applications where rationales
for predictions of GNNs are valuable.
Unsupervised learning on graphs. In contrast with tasks like node
classification or regression where labeled data is available, simi-
larity learning is rarely accompanied with ground truth data. An
alternative consists of learning representations that capture pat-
terns already present in the graph [30, 31, 79]. In the absence of
labels that could be used for training, learning in this setting relies
on optimization algorithms that produce representations useful
for a pretext task. Examples of pretext tasks are maximizing the
mutual information between different views of a graph [45, 65, 70],
embedding shortest path distances [4, 14], reconstructing parts of
the input [27, 71], or maintaining invariance with respect to small
changes in the input [69, 78]. The resulting representations can
then be employed in tasks such as clustering and similarity search.
Most of the research in this area has focused on studying different
ways of designing pretext tasks. However, the area of explainability
in unsupervised learning on graphs is underexplored [31, 79]. A
recently proposed method is Task-Agnostic Graph Explanations
(TAGE) [77], which proposes explaining specific dimensions of
embeddings obtained via unsupervised learning. The motivation
for explaining embedding dimensions is transferring the explainer
module of TAGE to supervised learning tasks. The performance of
TAGE for generating explanations for problems where labeled data
is not available, such as similarity computations, is not explored.
Explaining graph neural networks. Graph neural networks (GNNs)
are neural networks tailored to the irregular structure of graphs,
that are able to learn representations of a node in a graph taking into
consideration arbitrary subgraphs around it [74, 85, 92]. A growing
number of methods have been proposed in the literature that pro-
vide explanations to predictions computed by GNNs, in the form
of edges and features responsible for a prediction [87]. Existing
methods assume a trained GNN and provide post hoc mechanisms
for explaining their predictions [34, 86, 88], or propose methods
that are explainable a priori [29, 39]. Fundamentally, these methods
are focused on explaining GNNs for supervised learning. In this
work, we are interested in providing explanations for predictions
of similarity without access to labeled data. We further elaborate

Explaining Graph Neural Networks for Node Similarity on Graphs
on the implications of methods for explainable GNNs on the task
of similarity learning in the next section.
Knowledge graph embeddings and entity similarity. Knowledge
graph embeddings are representations of entities and relation types,
which are commonly trained for the link prediction task [43, 72]:
Given a query entity and a relation, the embeddings are used to
predict a target entity that is likely to form a valid triple with
the query entity and relation. KG embeddings have been applied
in similarity computations via functions like cosine similarity or
the dot product [11, 18, 26, 32, 82], which are not designed to be
explainable.
Prior work has explored the problem of explainability for KG
embeddings. Some methods have proposed learning embeddings
with a predefined structure, such as a set of interpretable con-
cepts [7, 76, 91], or via sparsity constraints [94]. The result is an
embedding space, where it is possible to identify distinct semantic
regions, e.g., “professions” or “cities”. This differs from the problem
of grounding similarities computed between pairs of entities on
known attributes of the entities, which is the focus of our work.
Other works focus on providing explanations given an existing
set of KG embeddings trained for link prediction, with explanations
in the form of a subset of supporting triples [3, 48, 53, 90], paths [21],
and Horn rules [15]. While there is empirical evidence for KG
embeddings being able to capture notions of similarity [15], some
works have suggested that the link prediction objective is sub-
optimal for this task [8, 51, 52]. This motivates our use of GNNs
that operate directly on node features and subgraphs that can serve
as explanations for predicted similarity scores.
Another line of work [46, 47] focused on identifying reasons
behind the similarity of two given entities by extracting SPARQL
queries, which have both of the entities as answers. However, unlike
in our proposal, in [46, 47] the authors did not aim at explaining
the similarity scores computed by a machine learning method, but
rather exclusively relied on the graph structure.
3
LEARNING AND EXPLAINING
SIMILARITIES
Let 𝐺= (A, X) be a graph with 𝑛nodes, where A is an 𝑛× 𝑛
adjacency matrix with 𝐴𝑖𝑗= 1 if nodes 𝑖and 𝑗are connected,
and 0 otherwise, and X ∈R𝑛×𝑚is a feature matrix, where the
𝑖-th row x𝑖contains the 𝑚-dimensional feature vector of the node
𝑖. In the following sections, we discuss the problems of learning
representations of nodes for the similarity task, and our proposals
on how similarity scores can be explained.
3.1
Learning representations for similarity
Graph neural networks have become a standard architecture for
processing graph-structured data, due to their ability to incorporate
arbitrary neighborhoods around a node [9, 19, 28, 37, 80]. They can
easily be extended to graphs with rich edge features and multimodal
data [12, 16, 55, 56]. Furthermore, the fact that GNNs implement
an explicit function that maps node neighborhoods and features to
an embedding offers the opportunity for determining which parts
of the input are responsible for a certain output. This is a desirable
property when explaining computations such as similarity scores.
A prominent example of a graph neural network is the Graph
Convolutional Network (GCN) [28]. A single layer of the GCN
implements the following propagation rule:
GCN(X, A) = 𝜎

˜AXΘ

,
(1)
where ˜A is the normalized adjacency matrix, ˜A = ˆD−1
2 ˆA ˆD−1
2 . Let
I𝑛be the 𝑛× 𝑛identity matrix. Then ˆA = A + I𝑛is the adjacency
matrix, adding self-loops, and ˆD is the degree matrix after adding
self loops, such that ˆ𝐷𝑖𝑖= Í
𝑗ˆ𝐴𝑖𝑗.
The weight matrix Θ in Eq. 1 contains the parameters of the layer
to be learned during training. When composing together multiple
GCN layers, we obtain a function 𝑓𝜃(X, A) = Z ∈R𝑛×𝑑that maps
each node and its features to an embedding, conditioned on the
features of nodes in its neighborhood.
We approach the problem of training a GNN to learn node em-
beddings from the perspective of unsupervised learning: In the
absence of labeled data containing ground-truth similarity informa-
tion, we resort to methods that learn node embeddings by capturing
patterns existing in the graph, such as communities or structural
roles [22]. The resulting node embeddings are vectors z𝑖∈R𝑑, with
𝑖= 1, . . .𝑛, where such patterns are preserved by the geometry of
the space. This allows us to address the problem of similarity search
for a given query node 𝑖, by ranking the rest of the nodes in the
graph according to a function such as cosine similarity:
𝑦(𝑖, 𝑗) =
z⊤
𝑖z𝑗
∥z𝑖∥∥z𝑗∥,
(2)
where 𝑗= 1, . . . ,𝑛and ∥z𝑖∥is the ℓ2-norm of z𝑖.
Several methods are available in the literature for unsupervised
learning on graphs [22, 25, 31]. Examples include Graph Autoen-
coders and Variational Graph Autoencoders [27], which optimize
node embeddings so that they are able to reconstruct the adjacency
matrix; Deep Graph Infomax [70], that learns node embeddings by
maximizing the mutual information between them and a summa-
rized representation of the graph; and Graph Contrastive Represen-
tation Learning [93], which compares different views of a node by
perturbing its neighborhood and features.
3.2
Explaining GNNs
The success of GNNs at various tasks has been accompanied by
increased interest in explaining the predictions they provide [87].
Informally, methods for explaining GNNs aim to determine i) which
parts of the input graph 𝐺= (X, A) are responsible for a particular
prediction, and ii) how they are responsible. The mechanisms used
to answer these questions vary with each method.
A recent survey [87] classifies methods for explaining GNNs
into two main groups: instance-level and model-level methods.
Instance-level methods produce a distinct explanation for a par-
ticular prediction (such as the label predicted for a specific node
in the graph), while model-level methods aim to understand the
behavior of the GNN under different inputs. Since we are interested
in explaining similarity scores computed for specific pairs of nodes,
we focus on the class of instance-level explanations.
Two important classes of instance-level methods are perturbation
methods and gradient-based methods. They represent an explana-
tion as an assignment of values to parts of the input (for example,

Trovato and Tobin, et al.
edges in the graph or node features), where the values indicate a
degree of importance for computing the output of the GNN, as we
illustrate in Fig. 1. In this work, the parts of the inputs to the GNN
that we consider for explanations are edges between nodes, but our
discussion can be easily extended to consider node features.
Formally, we assume that we have access to an already trained
GNN. The output 𝑓𝜃(X, A) of the GNN is used to compute a predic-
tion 𝑦= 𝑔(𝑓𝜃(X, A)), and we wish to compute an explanation for
it that describes the degree of influence of an edge in a prediction.
For similarity search the prediction is the cosine similarity between
two specific node embeddings as defined in Eq. 2.
Explanations over edges in the graph can be defined as a function
that maps a prediction to a matrix M ∈R𝑛×𝑛containing explanation
values for each of the (non-zero) entries of the adjacency matrix.
For the majority of perturbation methods, the explanation values
in M lie in the interval [0, 1], and they can be interpreted as a mask,
where values of 1 indicate relevant edges and 0 otherwise. Gradient-
based methods, on the other hand, are unconstrained, providing
explanation values over the real numbers that not only carry the
magnitude with which an edge influences a prediction, but also its
direction (positive or negative) via the sign of the gradient.
Given a matrix M of explanation values, a subset of the edges
in the graph can be selected by defining an explanation threshold 𝑡.
The subset is defined by the entries in the adjacency matrix 𝐴𝑖𝑗such
that 𝑀𝑖𝑗> 𝑡. The meaning of the selected edges for an explanation
of the similarity score depends on whether the matrix is interpreted
as a mask, or as a gradient.
We thus turn our attention to the question: is any of these two
interpretations of explanation values better suited for explaining
similarities of nodes in a graph? To investigate this question, we
analyze the properties of methods based on mutual information,
which are representative of the class of perturbation methods, and
gradient-based methods for explaining similarities.
3.2.1
Mutual information methods. A common approach for iden-
tifying explanations for GNNs consists of determining what edges
are relevant for computing a prediction, by relying on the concept
of Mutual Information (MI)[34, 39, 73, 86].
Given two random variables 𝑈,𝑉, the mutual information (MI)
between them is defined as
𝐼(𝑈;𝑉) =
∫∫
𝑝(𝑢, 𝑣) log 𝑝(𝑢, 𝑣)
𝑝(𝑢)𝑝(𝑣)𝑑𝑢𝑑𝑣,
(3)
where 𝑝(𝑢, 𝑣) is the joint probability distribution of 𝑈and 𝑉, and
𝑝(𝑢) and 𝑝(𝑣) are the marginal distributions of 𝑈and 𝑉, respec-
tively. Intuitively, the mutual information measures the reduction
in the uncertainty of 𝑈given the knowledge of 𝑉. For two inde-
pendent random variables, the mutual information is 0 [10].
In the context of explaining GNNs, existing works have proposed
explaining a prediction 𝑦= 𝑔(𝑓𝜃(X, A)) by finding a subgraph from
the original graph that has high mutual information with the pre-
diction. This implies that only a region of the graph is relevant for
computing a prediction, whereas the rest can be discarded with-
out affecting it. This mechanism for finding an explanation can be
formalized by assuming that the matrix M of explanation values
is a sample of a random variable 𝑀with values in {0, 1}, and then
maximizing the mutual information between the original prediction
(now a random variable 𝑌) and the prediction after “masking” the
adjacency matrix with the values in 𝑀:
max
𝑀𝐼(𝑔(𝑓𝜃(X, A);𝑔(𝑓𝜃(X, A ⊙𝑀)),
(4)
where ⊙indicates element-wise multiplication.
In practice, the problem in Eq. 4 is not tractable. Instead, an ap-
proximation leads to the problem of finding a matrix that minimizes
the cross-entropy loss [86]:
MMI B argmin
M
−E𝑌[log𝑝(𝑌|X, A ⊙M)]
(5)
This problem is solved by randomly initializing MMI and up-
dating it via gradient descent in the direction that minimizes the
cross-entropy loss [34, 39, 86].
Interpreting the explanation matrix. Given the formulation of MI-
based methods for explaining GNNs, entries of MMI with a value of
1 indicate edges that are relevant for the prediction, and 0 if they
are irrelevant. When the matrix contains values in the continuous
interval [0, 1], an appropriate threshold for selecting or discarding
edges is then 𝑡= 0.5.
We note that while this interpretation might be useful for prob-
lems like node classification (on which the the majority of works
on explaining GNNs have focused), the case of similarity is more
nuanced. In the case of node classification, a subset of a node neigh-
borhood might be enough for a node to be labeled with a class from a
pre-defined set. The rest of the subgraph could be discarded without
affecting the prediction. Node similarity, in particular when con-
sidering metrics like cosine similarity, is in contrast a fine-grained
prediction with no pre-defined values that can increase or decrease
with small changes in the neighborhoods of the compared nodes.
We thus argue that for the problem of node similarity, all edges
are relevant for computing the prediction, hence explanations based
on relevance (such as those provided by MI methods), are not suffi-
cient for understanding the relationship between the data associ-
ated with a pair of nodes and the corresponding cosine similarity
computed by a GNN.
3.2.2
Gradient-based methods. We now describe an alternative
approach for computing explanations for node similarities, which
we refer to as gradient-based (GB) methods.
An early approach for identifying parts of the inputs relevant
for a prediction computed by a neural network is to compute the
gradient of the output with respect to the input [57, 62, 63, 67]. This
is motivated by the fact that the gradient indicates the direction
and rate with which the outputs change with respect to the inputs.
The extension of this approach to explaining GNNs is natural:
the explanation matrix is equal to the gradient of the prediction
with respect to the adjacency matrix,
MGB B ∇A𝑔(𝑓𝜃(X, A)).
(6)
Relying on the gradient alone might become problematic in deep
neural networks using non-linearities like the ReLU activation
function, whose derivative is zero over half of its domain. To address
this issue, more advanced methods based on the gradient have been
proposed, such as Guided Backpropagation [64], which ignores zero
gradients, or Integrated Gradients [67], which computes the total
change from different values of the gradient, rather than relying
on a single gradient.

Explaining Graph Neural Networks for Node Similarity on Graphs
Interpreting the explanation matrix. The values in the explana-
tion matrix MGB are unconstrained, and they can take positive
or negative values, depending on the sign of the gradient. This
means that for each edge in the graph, GB explanations provide a
magnitude and direction of influence. In this case, an appropriate
threshold for selecting or discarding edges is 𝑡= 0.
When explaining predictions of node similarity, the (𝑖, 𝑗) entry
of the explanation matrix indicates i) how much the presence of
an edge between nodes 𝑖and 𝑗influences the similarity score, via
the magnitude of the gradient, and ii) the direction of influence
–positive or negative– via the sign. Unlike explanations from MI
methods, we note that GB explanations are therefore more fine-
grained, by providing additional information about how inputs
affect changes in similarity scores.
3.2.3
Desiderata for explanations of similarity. Several works in
the literature have highlighted the importance of explainability in
artificial intelligence systems, particularly when they face human
users that could benefit from an understanding of their predic-
tions [1, 40, 41, 49, 83]. These works define a series of properties
that explanations should have. For example, they should “produce
details or reasons to make its functioning clear or easy to under-
stand’’ [1], they should be useful for debugging algorithms [83],
they should provide answers to why questions [40] –e.g. why is
this the similarity score?–; and they should have properties such as
fidelity (how much the explanation agrees with the input-output
map of the prediction under explanation), low ambiguity, and low
complexity, among others [49].
The properties defined in such works are applicable to a broad
class of explanation methods, and they can serve as a guide for defin-
ing desirable properties of explanations of GNNs in the context of
node similarity. Given that the explanation methods we have con-
sidered provide an explanation value for each edge involved in the
computation of a prediction, we propose the following properties
that such explanations should meet:
• Explanations are actionable: We can use the edges whose
explanation value is above or below the threshold 𝑡to make
interventions in the graph, resulting in a predictable effect
on the original similarity score. This would facilitate an
understanding of the specific effect of some edges on the
similarity score, and follows desiderata on understanding
model decisions [1, 40], interactivity via interventions [1],
model debugging [83], and fidelity [49].
• Explanations are consistent: The effect of keeping edges
above the threshold is distinct from the effect of discard-
ing them. This would imply that the explanations capture
specific behaviors of the similarity under explanation, thus
indicating fidelity and low ambiguity [49].
• Explanations are sparse. Rather than presenting the com-
plete set of explanation values, a subset can be selected that
preserves the original effects of keeping or discarding edges
on the similarity score. The result is an explanation that
remains actionable and consistent, while enabling simpler,
parsimonious explanations [49] that might be preferable in
certain situations [40].
Our previous discussion on the interpretation of explanation
matrices provided by MI and GB methods suggests that the latter
Table 1: Statistics of graphs used in our experiments.
Dataset
Nodes
Edges
Features
Cora
2,708
5,429
1,433
Citeseer
3,327
4,732
3,703
Pubmed
19,717
44,338
500
Chameleon
2,277
36,101
2,325
Actor
7,600
33,544
931
Squirrel
5,201
217,073
2,089
DBpedia50k
30,449
57,161
N/A
are more effective at meeting this list of properties. Our experiments
are designed to test this hypothesis.
4
EXPERIMENTS
We are interested in answering the following research question: Do
mutual information and gradient-based methods provide explana-
tions of similarities learned by GNNs that are actionable, consistent,
and sparse? To answer it, we implement different methods for un-
supervised learning on graphs and then analyze the properties of
explanations provided by MI and GB methods quantitatively and
qualitatively.
4.1
Datasets
We study the problem of learning and explaining similarities by
considering six graph datasets of different sizes and domains: Cora,
Citeseer, and Pubmed [42, 58, 84] are citation networks from the
computer science and medical domains, where each node corre-
sponds to a scientific publication and an edge indicates that there
is a citation from one publication to another. These graphs are
known to exhibit high homophily: similar nodes (such as publica-
tions within the same field) are very likely to be connected [38].
To consider graphs with different structural properties, we also
carry out experiments with heterophilic graphs where connected
nodes are not necessarily similar. Chameleon and Squirrel are
graphs obtained from Wikipedia, where each node is a web page
and an edge denotes a hyperlink between pages [54]. Actor is a
graph where each node is an actor, and an edge indicates that two
actors co-occur on a Wikipedia page [68]. Furthermore, we also
experiment with the DBpedia50k knowledge graph [59], a subset
of the DBpedia knowledge graph [2]. The DBpedia50k graph does
not contain node features, therefore for this dataset we also train
input node embeddings for the GNN.
In all graphs, each node is associated with a feature vector. Sta-
tistics of all datasets is presented in Table 1.
4.2
Learning node embeddings for similarity
We implement the following unsupervised learning methods: Graph
Autoencoders (GAE) and Variational Graph Autoencoders (VGAE) [27],
Deep Graph Infomax (DGI) [70], and Graph Contrastive Represen-
tation Learning (GRACE) [93]. We use them to train a 2-layer GNN
as defined in Eq. 1. We tune the hyperparameters of the GNN and
specific hyperparameters of each unsupervised learning method
via grid search, selecting the values with the lowest training loss.

Trovato and Tobin, et al.
Table 2: Results of fidelity metrics (Fid𝑎and Fid𝑏) and effect overlap (EO, lower is better) when applying different explanation
methods to multiple unsupervised learning methods and graphs. As explanation methods we consider GNNExplainer [86] (MI),
and two gradient-based methods based on direct computation of the gradient (GB1), and Integrated Gradients [67] (GB2).
Cora
Citeseer
Pubmed
Chameleon
Actor
Squirrel
Method
Fid𝑎
Fid𝑏
EO
Fid𝑎
Fid𝑏
EO
Fid𝑎
Fid𝑏
EO
Fid𝑎
Fid𝑏
EO
Fid𝑎
Fid𝑏
EO
Fid𝑎
Fid𝑏
EO
GAE
MI
0.133
0.019
0.451
0.130
0.029
0.406
0.136
0.202
0.532
0.292
0.353
0.531
0.134
0.209
0.521
0.386
0.357
0.411
GB1
0.118
-0.076
0.033
0.114
-0.026
0.129
0.236
-0.064
0.141
0.355
-0.107
0.125
0.442
-0.146
0.120
0.520
-0.126
0.160
GB2
0.279
-0.067
0.013
0.366
-0.025
0.098
0.443
-0.144
0.011
0.718
-0.180
0.030
0.555
-0.392
0.008
0.755
-0.317
0.038
VGAE
MI
0.103
0.039
0.504
0.156
0.004
0.397
0.140
0.149
0.502
0.311
0.403
0.540
0.142
0.176
0.506
0.363
0.399
0.450
GB1
0.149
-0.087
0.045
0.078
-0.054
0.049
0.250
-0.121
0.098
0.412
-0.156
0.105
0.423
-0.203
0.081
0.577
-0.172
0.150
GB2
0.392
-0.075
0.007
0.185
-0.045
0.023
0.418
-0.180
0.017
0.781
-0.218
0.030
0.522
-0.386
0.009
0.766
-0.400
0.042
DGI
MI
0.015
0.032
0.546
0.039
0.029
0.568
0.061
0.008
0.452
0.322
0.441
0.539
-0.009
-0.000
0.552
0.142
0.162
0.561
GB1
0.218
-0.118
0.060
0.105
-0.084
0.082
0.023
-0.055
0.254
0.515
-0.196
0.326
-0.009
-0.012
0.511
0.119
-0.400
0.277
GB2
0.283
-0.161
0.053
0.149
-0.122
0.056
0.029
-0.043
0.182
0.399
-0.299
0.288
-0.087
-0.373
0.491
0.216
-0.449
0.273
GRACE
MI
0.076
0.007
0.536
0.102
0.010
0.475
0.222
0.096
0.513
0.254
0.132
0.535
0.016
-0.185
0.511
0.112
0.020
0.594
GB1
0.142
-0.057
0.016
0.113
-0.030
0.062
0.182
-0.016
0.158
0.338
-0.149
0.022
0.124
-0.262
0.155
0.253
-0.276
0.046
GB2
0.155
-0.071
0.017
0.140
-0.028
0.063
0.235
-0.041
0.052
0.382
-0.154
0.055
0.012
-0.443
0.217
0.133
-0.382
0.151
4.3
Evaluating explanations
Given a trained GNN 𝑓𝜃, we evaluate the properties of explanations
for node similarities by measuring quantities that assess changes in
the similarity score, after performing interventions in the graph on
the basis of the explanation. More concretely, let (𝑖, 𝑗) be a pair of
nodes in the graph. Given the set of node embeddings Z = 𝑓𝜃(X, A),
we select the embeddings of 𝑖and 𝑗from it and compute the cosine
similarity𝑦(𝑖, 𝑗) as defined in Eq. 2. The explanation method is then
executed on this value, which results in an explanation matrix M.
In our experiments, we employ GNNExplainer [86] as an instance
of MI methods. For GB methods, we consider directly using the
gradient with respect to the adjacency matrix (as defined in Eq. 6),
and Integrated Gradients [67].
Given M, we compute two matrices M𝑎and M𝑏that select values
above or below a threshold 𝑡, respectively, such that
𝑀𝑎,𝑖𝑗= 𝑀𝑖𝑗
if 𝑀𝑖𝑗≥𝑡else 0
(7)
𝑀𝑏,𝑖𝑗= 𝑀𝑖𝑗
if 𝑀𝑖𝑗< 𝑡else 0,
(8)
where the threshold for GNNexplainer is 0.5 and 0 for GB methods.
We use these matrices to intervene in the graph, by computing
the element-wise multiplication of these matrices with the adja-
cency matrix, and re-computing the node embeddings, which yields
Z𝑎= 𝑓𝜃(X, A ⊙M𝑎)
(9)
Z𝑏= 𝑓𝜃(X, A ⊙M𝑏).
(10)
Given these embeddings, we then re-compute the similarity scores,
which for each case we denote as 𝑦𝑎(𝑖, 𝑗) and 𝑦𝑏(𝑖, 𝑗) respectively.
Based on these new similarity scores, we first compute a fidelity
metric [50], which measures the change in the similarity score after
the intervention with respect to the original similarity score:
Fid𝑎= 𝑦𝑎(𝑖, 𝑗) −𝑦(𝑖, 𝑗)
(11)
Fid𝑏= 𝑦𝑏(𝑖, 𝑗) −𝑦(𝑖, 𝑗)
(12)
With fidelity metrics, we aim to determine whether the explanations
are actionable, since they measure the effect on similarity scores
after intervening on the graph with explanations that are either
above or below the threshold. We compute the average values of
Fid𝑎and Fid𝑏over a sample of 1,000 randomly selected pairs of
nodes from the graph (without replacement).
Eqs.11 and 12 imply that the effect on the similarity score can
be to increase it (in which case fidelity is positive) or to decrease it
(when fidelity is negative). To evaluate the property of consistency,
we obtain a tuple (𝑎1,𝑎2) where 𝑎1 is the number of times Fid𝑎is
positive over the 1,000 pairs of nodes, and 𝑎2 is the number of times
it is negative. We obtain another tuple (𝑏1,𝑏2) in the same way
based on the values of Fid𝑏. We then measure the effect overlap
(EO) between Fid𝑎and Fid𝑏by computing the generalized Jaccard
similarity:
EO =
Í2
𝑖=1 min(𝑎𝑖,𝑏𝑖)
Í2
𝑖=1 max(𝑎𝑖,𝑏𝑖)
.
(13)
An explanation method with an EO of zero indicates that the
effect observed in Fid𝑎is always positive, and always negative in
Fid𝑏(or viceversa). This indicates that the effects are distinct and
thus the explanations are consistent. The maximum value of EO is
1 and it occurs if the effect is always positive or always negative,
or if the counts of effects are the same.
4.4
Results
We present the results of the fidelity and effect overlap metrics in
Tables 2 for the homophilic and heterophilic graphs, and Table 3
for DBpedia50k. We denote GNNExplainer as MI, directly using the
gradient as GB1, and Integrated Gradients as GB2.
GB explanations are actionable. The values of Fid𝑎and Fid𝑏for
GB methods show that across all unsupervised learning methods
and datasets, keeping edges above the explanation threshold always
results in an increase of the similarity score, while keeping the edges
below the threshold always results in a lower score. This means
that GB explanations are actionable, as they allow interventions
that result in a predictable effect on the similarity score. Relying on
these explanations would allow to determine what edges contribute
to increase (or decrease) the score, and to interact with them by
re-computing the similarity score with the knowledge provided by
the explanation. This property is not observed with GNNExplainer,

Explaining Graph Neural Networks for Node Similarity on Graphs
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
1.0
0.5
0.0
0.5
1.0
Cora
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
1.0
0.5
0.0
0.5
1.0
Citeseer
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
1.0
0.5
0.0
0.5
1.0
Pubmed
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
1.0
0.5
0.0
0.5
1.0
Chameleon
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
1.0
0.5
0.0
0.5
1.0
Actor
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Sparsity
1.0
0.5
0.0
0.5
1.0
Squirrel
Fida
Fidb
EO
Figure 2: Influence of sparse explanations on fidelity metrics (Fid𝑎and Fid𝑏) and effect overlap (EO), evaluated with GAE
embeddings across different datasets. At zero sparsity, all edges above (or below) the explanation threshold are kept and used to
compute the change in similarity scores Fid𝑎(or F𝑏), as well as the effect overlap (EO). Larger values of sparsity indicate the
fraction of edges discarded before computing the change in similarity scores. Confidence intervals are shown indicating two
standard deviations over 10 runs.
where the effect of keeping edges above the threshold is not clear,
and certain patterns seem to depend on factors such as the model
used to learn the embeddings, and the dataset. For example, for
GAE and VGAE embeddings, keeping the edges above the threshold
increases the similarity score more than keeping the edges below
the threshold on Cora and Citeseer, but the opposite happens in
the remaining datasets.
GB explanations are consistent. GB methods result in the lowest
effect overlap across all learning methods and datasets. In the ma-
jority of cases the overlap is around 0.1 or lower, indicating that the
effect of keeping edges above the threshold is distinct from the ef-
fect of keeping the edges below, thus showing that GB explanations
are consistent. Interestingly, this behavior is not as clear when
using DGI embeddings on the heterophilic datasets (Chameleon,
Actor, and Squirrel), where the overlap increases. This could be
an effect of how the performance of DGI degrades in heterophilic
graphs [75], lowering the quality of its embeddings in graphs with
these properties and thus becoming sensitive to the interventions
required to compute the fidelity and effect overlap metrics. In the
case of GNNExplainer, in the majority of cases the effect overlap is
around 0.4 or even larger than 0.5, indicating that in almost half of
the cases keeping the edges above the threshold increases the score,
and in the other half the score decreases. We thus cannot rely on
its explanations for a consistent effect on similarity scores.
Sparse GB explanations preserve effects. Our previous experi-
ments have taken into account all explanation values assigned
to edges in the graph to compute the effect on similarity scores.
A third desirable property of explanations is that of sparsity. We
limit this investigation to explanations computed with Integrated
Gradients, since we have already observed that its explanations are
actionable and consistent, and we are interested in determining if
this property holds under different levels of sparsity.
To carry out this study, instead of taking all values of the expla-
nation matrix above the threshold (as outlined in Eqs. 7 and 8), we
drop a fraction 𝑠of the smallest values in M𝑎, and a fraction 𝑠of the
largest values in M𝑏, where 𝑠is the sparsity level taking values in
the interval [0, 1]. When 𝑠= 0 all values in the explanation matrix
are used, and we obtain the results previously described in Table 2.
As 𝑠increases, only the edges with the largest or the smallest values
are kept in M𝑎and M𝑏.
We compute the fidelity and effect overlap metrics for different
values of sparsity from 0 up to 0.9 with increments of 0.1, when
using GAE to learn embeddings. The results are shown in Fig. 2.
We observe that the actionable and consistent properties of GB

Trovato and Tobin, et al.
Table 3: Results of fidelity metrics (Fid𝑎and Fid𝑏) and effect
overlap (EO, lower is better) when applying different expla-
nation methods to multiple unsupervised learning methods
on the DBpedia50k knowledge graph.
DBpedia50k
Method
Fid𝑎
Fid𝑏
EO
GAE
MI
0.057
-0.073
0.564
GB1
0.148
-0.190
0.050
GB2
0.149
-0.213
0.028
VGAE
MI
0.059
-0.054
0.614
GB1
0.149
-0.185
0.059
GB2
0.182
-0.187
0.037
DGI
MI
-0.035
-0.044
0.618
GB1
0.107
-0.189
0.065
GB2
0.121
-0.215
0.030
GRACE
MI
-0.120
-0.002
0.541
GB1
0.055
-0.071
0.043
GB2
0.033
-0.081
0.046
explanations remain almost constant across all datasets. This im-
plies that when obtaining GB explanations, we can further reduce
the set of edges in the explanation by up to 90%, and the different
effects on the similarity scores will be preserved. This is beneficial
for applications in which a more compact explanation is desired.
Examples. We present concrete examples of the explanations
obtained by GNNExplainer and Integrated Gradients in Fig. 3. For
this case study, we train node embeddings using GAE on the DB-
pedia50k knowledge graph [59]. We then select the most relevant
edges according to the explanation values assigned by each method.
We consider two entities in the graph: Lilium and Dendrobium,
which are two genera of flowering plants. Their similarity is re-
flected in a cosine similarity value of 0.705. We denote the effect
attributed to each edge with colors, with blue indicating an increase
in the similarity, red a decrease, and gray indicating little or no
effect. When we obtain explanations with GNNExplainer, we ob-
serve that a few edges increase the similarity score, and none of
them are in the 1-hop neighborhood of the entities, where their
similarities are apparent. Both entities belong to the Plant kingdom
and the Flowering Plant division. With gradient-based explanations,
we observe that edges containing this information contribute to
increase the similarity score, with the highest contributions (illus-
trated with the thickness of the edges) assigned to the relationships
with Plant and Flowering Plant. Overall, we note that that gradient-
based explanations are intuitive, by indicating both the magnitude
and direction in which inputs affect similarity scores.
5
CONCLUSION
We have investigated the problem of explaining node similarities
learned by graph neural networks. We discuss the properties of two
prominent methods for explainability on GNNs, based on the idea
of mutual information, which selects parts of the input relevant
Lilium
Dendrobium
Flowering
Plant
Hoodia
Gordonii
Plant
Cucurbitaceae
Chayote
(a) GNNExplainer explanation.
Lilium
Dendrobium
Flowering
Plant
Popowia
Hoodia
Gordonii
Plant
Ormosia
Hippomane
Peniocereus
Scutellaria
Cirsium
Pitcheri
(b) Gradient-based explanation.
Figure 3: Example of explanations provided by GNNEx-
plainer (3a) and Integrated Gradients (3b) for the similarity
computed between two entities in the DBpedia50k knowl-
edge graph: Lilium and Dendrobium, two genera of flowering
plants. Edge thickness indicate magnitude, and blue indicates
edges that result in an increase of the score, red edges result
in a decrease, and gray edges have little effect.
for a prediction; and gradients, which measure changes in the pre-
diction with respect to the inputs. By contrasting their properties
with desirable explanations in the context of node similarity, we
find that the applicability changes, in comparison with other prob-
lems in which they have been applied, such as node classification.
We conclude that gradient-based methods are better suited for ex-
plaining similarities, by providing explanations with a predictable
and consistent effect of increasing or decreasing similarity scores.
Furthermore, we observe that the complexity of the explanations
can be reduced while maintaining their desirable properties.
The properties we present in our work can be extended to the
general problem of explaining similarities on graphs via methods
other than GNNs, as well as the design of methods for similarity
search on graphs that are explainable a priori, which we plan to
explore in future work.

Explaining Graph Neural Networks for Node Similarity on Graphs
REFERENCES
[1] Alejandro Barredo Arrieta, Natalia Díaz Rodríguez, Javier Del Ser, Adrien Ben-
netot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-Lopez, Daniel
Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Ex-
plainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities
and challenges toward responsible AI. Inf. Fusion 58 (2020), 82–115.
https:
//doi.org/10.1016/j.inffus.2019.12.012
[2] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyga-
niak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In
international semantic web conference. Springer, 722–735.
[3] Patrick Betz, Christian Meilicke, and Heiner Stuckenschmidt. 2022. Adversarial
Explanations for Knowledge Graph Embeddings. In Proceedings of the Thirty-
First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna,
Austria, 23-29 July 2022, Luc De Raedt (Ed.). ijcai.org, 2820–2826. https://doi.
org/10.24963/ijcai.2022/391
[4] Aleksandar Bojchevski and Stephan Günnemann. 2018. Deep Gaussian Em-
bedding of Graphs: Unsupervised Inductive Learning via Ranking. In 6th In-
ternational Conference on Learning Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
https://openreview.net/forum?id=r1ZdKJ-0W
[5] Sergey Brin. 1998. The PageRank citation ranking: bringing order to the web.
Proceedings of ASIS, 1998 98 (1998), 161–172.
[6] Nadia Burkart and Marco F. Huber. 2021. A Survey on the Explainability of
Supervised Machine Learning. J. Artif. Intell. Res. 70 (2021), 245–317.
https:
//doi.org/10.1613/jair.1.12228
[7] . Chandrahas, Tathagata Sengupta, Cibi Pragadeesh, and Partha Talukdar. 2020.
Inducing Interpretability in Knowledge Graph Embeddings. In Proceedings of
the 17th International Conference on Natural Language Processing (ICON). NLP
Association of India (NLPAI), Indian Institute of Technology Patna, Patna, India,
70–75. https://aclanthology.org/2020.icon-main.9
[8] Michael Cochez, Petar Ristoski, Simone Paolo Ponzetto, and Heiko Paulheim.
2017.
Global RDF Vector Space Embeddings. In The Semantic Web - ISWC
2017 - 16th International Semantic Web Conference, Vienna, Austria, October
21-25, 2017, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 10587),
Claudia d’Amato, Miriam Fernández, Valentina A. M. Tamma, Freddy Lécué,
Philippe Cudré-Mauroux, Juan F. Sequeda, Christoph Lange, and Jeff Heflin (Eds.).
Springer, 190–207. https://doi.org/10.1007/978-3-319-68288-4_12
[9] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Velick-
ovic. 2020. Principal Neighbourhood Aggregation for Graph Nets. In Advances
in Neural Information Processing Systems 33: Annual Conference on Neural In-
formation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (Eds.).
https://proceedings.neurips.cc/paper/2020/hash/
99cad265a1768cc2dd013f0e740300ae-Abstract.html
[10] Thomas M Cover. 1999. Elements of information theory. John Wiley & Sons.
[11] Daniel Daza, Michael Cochez, and Paul Groth. 2021. Inductive Entity Repre-
sentations from Text via Link Prediction. In WWW ’21: The Web Conference
2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021, Jure Leskovec, Marko
Grobelnik, Marc Najork, Jie Tang, and Leila Zia (Eds.). ACM / IW3C2, 798–808.
https://doi.org/10.1145/3442381.3450141
[12] Yasha Ektefaie, George Dasoulas, Ayush Noori, Maha Farhat, and Marinka Zitnik.
2023. Multimodal learning with graphs. Nat. Mac. Intell. 5, 4 (2023), 340–350.
https://doi.org/10.1038/s42256-023-00624-6
[13] Fredo Erxleben, Michael Günther, Markus Krötzsch, Julian Mendez, and Denny
Vrandecic. 2014. Introducing Wikidata to the Linked Data Web. In ISWC. 50–65.
[14] Charlie Frogner, Farzaneh Mirzazadeh, and Justin Solomon. 2019. Learning
Embeddings into Entropic Wasserstein Spaces. In 7th International Conference
on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net. https://openreview.net/forum?id=rJg4J3CqFm
[15] Mohamed H. Gad-Elrab, Daria Stepanova, Trung-Kien Tran, Heike Adel, and
Gerhard Weikum. 2020. ExCut: Explainable Embedding-Based Clustering over
Knowledge Graphs. In The Semantic Web - ISWC 2020 - 19th International Semantic
Web Conference, Athens, Greece, November 2-6, 2020, Proceedings, Part I (Lecture
Notes in Computer Science, Vol. 12506), Jeff Z. Pan, Valentina A. M. Tamma,
Claudia d’Amato, Krzysztof Janowicz, Bo Fu, Axel Polleres, Oshani Seneviratne,
and Lalana Kagal (Eds.). Springer, 218–237. https://doi.org/10.1007/978-3-030-
62419-4_13
[16] Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, and Jens
Lehmann. 2020. Message Passing for Hyper-Relational Knowledge Graphs. In
Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor
Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics,
7346–7359. https://doi.org/10.18653/v1/2020.emnlp-main.596
[17] Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, and Xilin Chen. 2020. Multi-
Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text.
In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR
2020, Seattle, WA, USA, June 13-19, 2020. Computer Vision Foundation / IEEE,
12743–12753. https://doi.org/10.1109/CVPR42600.2020.01276
[18] Emma J. Gerritse, Faegheh Hasibi, and Arjen P. de Vries. 2020. Graph-Embedding
Empowered Entity Retrieval. In Advances in Information Retrieval - 42nd European
Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14-17, 2020, Proceed-
ings, Part I (Lecture Notes in Computer Science, Vol. 12035), Joemon M. Jose, Emine
Yilmaz, João Magalhães, Pablo Castells, Nicola Ferro, Mário J. Silva, and Flávio
Martins (Eds.). Springer, 97–110. https://doi.org/10.1007/978-3-030-45439-5_7
[19] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E.
Dahl. 2017. Neural Message Passing for Quantum Chemistry. In Proceedings of
the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,
Australia, 6-11 August 2017 (Proceedings of Machine Learning Research, Vol. 70),
Doina Precup and Yee Whye Teh (Eds.). PMLR, 1263–1272. http://proceedings.
mlr.press/v70/gilmer17a.html
[20] Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 1999. Similarity Search
in High Dimensions via Hashing. In VLDB’99, Proceedings of 25th Interna-
tional Conference on Very Large Data Bases, September 7-10, 1999, Edinburgh,
Scotland, UK, Malcolm P. Atkinson, Maria E. Orlowska, Patrick Valduriez,
Stanley B. Zdonik, and Michael L. Brodie (Eds.). Morgan Kaufmann, 518–529.
http://www.vldb.org/conf/1999/P49.pdf
[21] Arthur Colombini Gusmão, Alvaro Henrique Chaim Correia, Glauber De Bona,
and Fabio Gagliardi Cozman. 2018. Interpreting embedding models of knowledge
bases: a pedagogical approach. arXiv preprint arXiv:1806.09504 (2018).
[22] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation Learning
on Graphs: Methods and Applications. IEEE Data Eng. Bull. 40, 3 (2017), 52–74.
http://sites.computer.org/debull/A17sept/p52.pdf
[23] Taher H. Haveliwala. 2002. Topic-sensitive PageRank. In Proceedings of the
Eleventh International World Wide Web Conference, WWW 2002, May 7-11, 2002,
Honolulu, Hawaii, USA, David Lassner, David De Roure, and Arun Iyengar (Eds.).
ACM, 517–526. https://doi.org/10.1145/511446.511513
[24] Glen Jeh and Jennifer Widom. 2002. SimRank: a measure of structural-context
similarity. In Proceedings of the Eighth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, July 23-26, 2002, Edmonton, Alberta,
Canada. ACM, 538–543. https://doi.org/10.1145/775047.775126
[25] Wei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang
Qin, Jianhao Shen, Fang Sun, Zhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng
Zhao, Xiao Luo, and Ming Zhang. 2023. A Comprehensive Survey on Deep Graph
Representation Learning. CoRR abs/2304.05055 (2023). https://doi.org/10.48550/
arXiv.2304.05055 arXiv:2304.05055
[26] Nasrullah Khan, Zongmin Ma, Aman Ullah, and Kemal Polat. 2022. Similarity
attributed knowledge graph embedding enhancement for item recommendation.
Inf. Sci. 613 (2022), 69–95. https://doi.org/10.1016/j.ins.2022.08.124
[27] Thomas N. Kipf and Max Welling. 2016. Variational Graph Auto-Encoders. CoRR
abs/1611.07308 (2016). arXiv:1611.07308 http://arxiv.org/abs/1611.07308
[28] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net. https://openreview.net/forum?id=SJU4ayYgl
[29] Namkyeong Lee, Dongmin Hyun, Gyoung S. Na, Sungwon Kim, Junseok Lee, and
Chanyoung Park. 2023. Conditional Graph Information Bottleneck for Molecular
Relational Learning. CoRR abs/2305.01520 (2023). https://doi.org/10.48550/arXiv.
2305.01520 arXiv:2305.01520
[30] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and
Jie Tang. 2023. Self-Supervised Learning: Generative or Contrastive. IEEE Trans.
Knowl. Data Eng. 35, 1 (2023), 857–876.
https://doi.org/10.1109/TKDE.2021.
3090866
[31] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and Philip S.
Yu. 2023. Graph Self-Supervised Learning: A Survey. IEEE Trans. Knowl. Data
Eng. 35, 6 (2023), 5879–5900. https://doi.org/10.1109/TKDE.2022.3172903
[32] Zhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. 2019. Explore
Entity Embedding Effectiveness in Entity Retrieval. In Chinese Computational
Linguistics - 18th China National Conference, CCL 2019, Kunming, China, October
18-20, 2019, Proceedings (Lecture Notes in Computer Science, Vol. 11856), Maosong
Sun, Xuanjing Huang, Heng Ji, Zhiyuan Liu, and Yang Liu (Eds.). Springer, 105–
116. https://doi.org/10.1007/978-3-030-32381-3_9
[33] Ana Lucic, Maartje A. ter Hoeve, Gabriele Tolomei, Maarten de Rijke, and Fabrizio
Silvestri. 2022. CF-GNNExplainer: Counterfactual Explanations for Graph Neural
Networks. In International Conference on Artificial Intelligence and Statistics,
AISTATS 2022, 28-30 March 2022, Virtual Event (Proceedings of Machine Learning
Research, Vol. 151), Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera
(Eds.). PMLR, 4499–4511. https://proceedings.mlr.press/v151/lucic22a.html
[34] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen,
and Xiang Zhang. 2020. Parameterized Explainer for Graph Neural Network.
In Advances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/
hash/e37b08dd3015330dcbb5d6663667b8b8-Abstract.html

Trovato and Tobin, et al.
[35] Farzaneh Mahdisoltani, Joanna Biega, and Fabian M. Suchanek. 2015. YAGO3: A
Knowledge Base from Multilingual Wikipedias. In CIDR.
[36] Elan Markowitz, Keshav Balasubramanian, Mehrnoosh Mirtaheri, Murali An-
navaram, Aram Galstyan, and Greg Ver Steeg. 2022. StATIK: Structure and
Text for Inductive Knowledge Graph Completion. In Findings of the Associ-
ation for Computational Linguistics: NAACL 2022, Seattle, WA, United States,
July 10-15, 2022, Marine Carpuat, Marie-Catherine de Marneffe, and Iván
Vladimir Meza Ruíz (Eds.). Association for Computational Linguistics, 604–615.
https://doi.org/10.18653/v1/2022.findings-naacl.46
[37] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. 2019.
Provably Powerful Graph Networks. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and
Roman Garnett (Eds.). 2153–2164. https://proceedings.neurips.cc/paper/2019/
hash/bb04af0f7ecaee4aae62035497da1387-Abstract.html
[38] Miller McPherson, Lynn Smith-Lovin, and James M Cook. 2001. Birds of a feather:
Homophily in social networks. Annual review of sociology 27, 1 (2001), 415–444.
[39] Siqi Miao, Mia Liu, and Pan Li. 2022. Interpretable and Generalizable Graph
Learning via Stochastic Attention Mechanism. In International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Pro-
ceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.). PMLR,
15524–15543. https://proceedings.mlr.press/v162/miao22a.html
[40] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social
sciences. Artif. Intell. 267 (2019), 1–38. https://doi.org/10.1016/j.artint.2018.07.007
[41] Shane T Mueller, Robert R Hoffman, William Clancey, Abigail Emrey, and Gary
Klein. 2019. Explanation in human-AI systems: A literature meta-review, synopsis
of key ideas and publications, and bibliography for explainable AI. arXiv preprint
arXiv:1902.01876 (2019).
[42] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. 2012. Query-
driven active surveying for collective classification. In 10th international workshop
on mining and learning with graphs, Vol. 8. 1.
[43] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2016.
A Review of Relational Machine Learning for Knowledge Graphs. Proc. IEEE 104,
1 (2016), 11–33. https://doi.org/10.1109/JPROC.2015.2483592
[44] Natalya Fridman Noy, Yuqing Gao, Anshu Jain, Anant Narayanan, Alan Patterson,
and Jamie Taylor. 2019. Industry-scale knowledge graphs: lessons and challenges.
Commun. ACM 62, 8 (2019), 36–43. https://doi.org/10.1145/3331166
[45] Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang
Xu, and Junzhou Huang. 2020. Graph Representation Learning via Graphical
Mutual Information Maximization. In WWW ’20: The Web Conference 2020, Taipei,
Taiwan, April 20-24, 2020, Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten
van Steen (Eds.). ACM / IW3C2, 259–270.
https://doi.org/10.1145/3366423.
3380112
[46] Alina Petrova, Egor V. Kostylev, Bernardo Cuenca Grau, and Ian Horrocks. 2019.
Query-Based Entity Comparison in Knowledge Graphs Revisited. In The Seman-
tic Web - ISWC 2019 - 18th International Semantic Web Conference, Vol. 11778.
Springer, 558–575.
[47] Alina Petrova, Evgeny Sherkhonov, Bernardo Cuenca Grau, and Ian Horrocks.
2017. Entity Comparison in RDF Graphs. In The Semantic Web - ISWC 2017,
Vol. 10587. Springer, 526–541.
[48] Pouya Pezeshkpour, Yifan Tian, and Sameer Singh. 2019. Investigating Robust-
ness and Interpretability of Link Prediction via Adversarial Modifications. In
Proceedings of the 2019 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill
Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-
tional Linguistics, 3336–3347. https://doi.org/10.18653/v1/n19-1337
[49] Gabriëlle Ras, Marcel van Gerven, and Pim Haselager. 2018. Explanation meth-
ods in deep learning: Users, values, concerns and challenges. Explainable and
interpretable models in computer vision and machine learning (2018), 19–36.
[50] Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I
Trust You?": Explaining the Predictions of Any Classifier. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, San Francisco, CA, USA, August 13-17, 2016, Balaji Krishnapuram, Mohak
Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi
(Eds.). ACM, 1135–1144. https://doi.org/10.1145/2939672.2939778
[51] Petar Ristoski and Heiko Paulheim. 2016. RDF2Vec: RDF Graph Embeddings for
Data Mining. In The Semantic Web - ISWC 2016 - 15th International Semantic Web
Conference, Kobe, Japan, October 17-21, 2016, Proceedings, Part I (Lecture Notes
in Computer Science, Vol. 9981), Paul Groth, Elena Simperl, Alasdair J. G. Gray,
Marta Sabou, Markus Krötzsch, Freddy Lécué, Fabian Flöck, and Yolanda Gil
(Eds.). 498–514. https://doi.org/10.1007/978-3-319-46523-4_30
[52] Petar Ristoski, Jessica Rosati, Tommaso Di Noia, Renato De Leone, and Heiko
Paulheim. 2019. RDF2Vec: RDF graph embeddings and their applications. Se-
mantic Web 10, 4 (2019), 721–752. https://doi.org/10.3233/SW-180317
[53] Andrea Rossi, Donatella Firmani, Paolo Merialdo, and Tommaso Teofili. 2022.
Explaining Link Prediction Systems based on Knowledge Graph Embeddings. In
SIGMOD ’22: International Conference on Management of Data, Philadelphia, PA,
USA, June 12 - 17, 2022, Zachary G. Ives, Angela Bonifati, and Amr El Abbadi
(Eds.). ACM, 2062–2075. https://doi.org/10.1145/3514221.3517887
[54] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. 2021. Multi-Scale attributed
node embedding. J. Complex Networks 9, 2 (2021).
https://doi.org/10.1093/
comnet/cnab014
[55] Raeid Saqur and Karthik Narasimhan. 2020.
Multimodal Graph Networks
for Compositional Generalization in Visual Question Answering. In Advances
in Neural Information Processing Systems 33: Annual Conference on Neural In-
formation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (Eds.).
https://proceedings.neurips.cc/paper/2020/hash/
1fd6c4e41e2c6a6b092eb13ee72bce95-Abstract.html
[56] Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg,
Ivan Titov, and Max Welling. 2018. Modeling Relational Data with Graph Con-
volutional Networks. In The Semantic Web - 15th International Conference, ESWC
2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings (Lecture Notes in Com-
puter Science, Vol. 10843), Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal,
Pascal Hitzler, Raphaël Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam
(Eds.). Springer, 593–607. https://doi.org/10.1007/978-3-319-93417-4_38
[57] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-
tam, Devi Parikh, and Dhruv Batra. 2017. Grad-CAM: Visual Explanations from
Deep Networks via Gradient-Based Localization. In IEEE International Conference
on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. IEEE Computer
Society, 618–626. https://doi.org/10.1109/ICCV.2017.74
[58] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and
Tina Eliassi-Rad. 2008. Collective Classification in Network Data. AI Mag. 29, 3
(2008), 93–106. https://doi.org/10.1609/aimag.v29i3.2157
[59] Baoxu Shi and Tim Weninger. 2018. Open-World Knowledge Graph Completion.
In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,
(AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18),
and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith
and Kilian Q. Weinberger (Eds.). AAAI Press, 1957–1964.
https://doi.org/10.
1609/aaai.v32i1.11535
[60] Yuxuan Shi, Gong Cheng, Trung-Kien Tran, Jie Tang, and Evgeny Kharlamov.
2021. Keyword-Based Knowledge Graph Exploration Based on Quadratic Group
Steiner Trees. In Proceedings of the Thirtieth International Joint Conference on
Artificial Intelligence, IJCAI. 1555–1562.
[61] Larissa Capobianco Shimomura, Rafael Seidi Oyamada, Marcos R. Vieira, and
Daniel S. Kaster. 2021. A survey on graph-based methods for similarity searches in
metric spaces. Inf. Syst. 95 (2021), 101507. https://doi.org/10.1016/j.is.2020.101507
[62] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning
Important Features Through Propagating Activation Differences. In Proceedings
of the 34th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, 3145–
3153. https://proceedings.mlr.press/v70/shrikumar17a.html
[63] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep Inside
Convolutional Networks: Visualising Image Classification Models and Saliency
Maps. In 2nd International Conference on Learning Representations, ICLR 2014,
Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings, Yoshua Bengio
and Yann LeCun (Eds.). http://arxiv.org/abs/1312.6034
[64] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A.
Riedmiller. 2015. Striving for Simplicity: The All Convolutional Net. In 3rd
International Conference on Learning Representations, ICLR 2015, San Diego, CA,
USA, May 7-9, 2015, Workshop Track Proceedings, Yoshua Bengio and Yann LeCun
(Eds.). http://arxiv.org/abs/1412.6806
[65] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. 2020. InfoGraph:
Unsupervised and Semi-supervised Graph-Level Representation Learning via
Mutual Information Maximization. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net. https://openreview.net/forum?id=r1lfF2NYvH
[66] Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. 2011. PathSim:
Meta Path-Based Top-K Similarity Search in Heterogeneous Information Net-
works. Proc. VLDB Endow. 4, 11 (2011), 992–1003. http://www.vldb.org/pvldb/
vol4/p992-sun.pdf
[67] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution
for Deep Networks. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 (Proceedings of
Machine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.).
PMLR, 3319–3328. http://proceedings.mlr.press/v70/sundararajan17a.html
[68] Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. 2009. Social influence analysis
in large-scale networks. In Proceedings of the 15th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, Paris, France, June 28 -
July 1, 2009, John F. Elder IV, Françoise Fogelman-Soulié, Peter A. Flach, and
Mohammed Javeed Zaki (Eds.). ACM, 807–816. https://doi.org/10.1145/1557019.

Explaining Graph Neural Networks for Node Similarity on Graphs
1557108
[69] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou,
Eva L. Dyer, Rémi Munos, Petar Velickovic, and Michal Valko. 2022. Large-Scale
Representation Learning on Graphs via Bootstrapping. In The Tenth International
Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.
OpenReview.net. https://openreview.net/forum?id=0UXT6PpRpW
[70] Petar Velickovic, William Fedus, William L. Hamilton, Pietro Liò, Yoshua Bengio,
and R. Devon Hjelm. 2019. Deep Graph Infomax. In 7th International Conference
on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net. https://openreview.net/forum?id=rklz9iAcKQ
[71] Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. 2017.
MGAE: Marginalized Graph Autoencoder for Graph Clustering. In Proceedings of
the 2017 ACM on Conference on Information and Knowledge Management, CIKM
2017, Singapore, November 06 - 10, 2017, Ee-Peng Lim, Marianne Winslett, Mark
Sanderson, Ada Wai-Chee Fu, Jimeng Sun, J. Shane Culpepper, Eric Lo, Joyce C.
Ho, Debora Donato, Rakesh Agrawal, Yu Zheng, Carlos Castillo, Aixin Sun,
Vincent S. Tseng, and Chenliang Li (Eds.). ACM, 889–898. https://doi.org/10.
1145/3132847.3132967
[72] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge Graph
Embedding: A Survey of Approaches and Applications. IEEE Trans. Knowl. Data
Eng. 29, 12 (2017), 2724–2743. https://doi.org/10.1109/TKDE.2017.2754499
[73] Xiang Wang, Ying-Xin Wu, An Zhang, Xiangnan He, and Tat-Seng Chua. 2021.
Towards Multi-Grained Explainability for Graph Neural Networks. In Advances
in Neural Information Processing Systems 34: Annual Conference on Neural In-
formation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual,
Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and
Jennifer Wortman Vaughan (Eds.). 18446–18458. https://proceedings.neurips.cc/
paper/2021/hash/99bcfcd754a98ce89cb86f73acc04645-Abstract.html
[74] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
Philip S. Yu. 2021. A Comprehensive Survey on Graph Neural Networks. IEEE
Trans. Neural Networks Learn. Syst. 32, 1 (2021), 4–24. https://doi.org/10.1109/
TNNLS.2020.2978386
[75] Teng Xiao, Zhengyu Chen, Zhimeng Guo, Zeyang Zhuang, and Suhang Wang.
2022. Decoupled Self-supervised Learning for Graphs. In NeurIPS. http://papers.
nips.cc/paper_files/paper/2022/hash/040c816286b3844fd78f2124eec75f2e-
Abstract-Conference.html
[76] Qizhe Xie, Xuezhe Ma, Zihang Dai, and Eduard Hovy. 2017. An Interpretable
Knowledge Transfer Model for Knowledge Base Completion. In Proceedings of
the 55th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational Linguistics, Vancouver, Canada,
950–962. https://doi.org/10.18653/v1/P17-1088
[77] Yaochen Xie, Sumeet Katariya, Xianfeng Tang, Edward W. Huang, Nikhil
Rao, Karthik Subbian, and Shuiwang Ji. 2022.
Task-Agnostic Graph Ex-
planations. In NeurIPS.
http://papers.nips.cc/paper_files/paper/2022/hash/
4eb7f0abf16d08e50ed42beb1e22e782-Abstract-Conference.html
[78] Yaochen Xie, Zhao Xu, and Shuiwang Ji. 2022. Self-Supervised Representation
Learning via Latent Graph Prediction. In International Conference on Machine
Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings
of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka,
Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.). PMLR, 24460–
24477. https://proceedings.mlr.press/v162/xie22e.html
[79] Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. 2023.
Self-Supervised Learning of Graph Neural Networks: A Unified Review. IEEE
Trans. Pattern Anal. Mach. Intell. 45, 2 (2023), 2412–2429. https://doi.org/10.1109/
TPAMI.2022.3170559
[80] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful
are Graph Neural Networks?. In 7th International Conference on Learning Rep-
resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.
https://openreview.net/forum?id=ryGs6iA5Km
[81] Xiaowei Xu, Nurcan Yuruk, Zhidan Feng, and Thomas A. J. Schweiger. 2007.
SCAN: a structural clustering algorithm for networks. In Proceedings of the 13th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
San Jose, California, USA, August 12-15, 2007, Pavel Berkhin, Rich Caruana, and
Xindong Wu (Eds.). ACM, 824–833. https://doi.org/10.1145/1281192.1281280
[82] Ikuya Yamada, Akari Asai, Jin Sakuma, Hiroyuki Shindo, Hideaki Takeda,
Yoshiyasu Takefuji, and Yuji Matsumoto. 2020. Wikipedia2Vec: An Efficient
Toolkit for Learning and Visualizing the Embeddings of Words and Entities from
Wikipedia. In Proceedings of the 2020 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online,
November 16-20, 2020, Qun Liu and David Schlangen (Eds.). Association for Com-
putational Linguistics, 23–30. https://doi.org/10.18653/v1/2020.emnlp-demos.4
[83] Wenli Yang, Yuchen Wei, Hanyu Wei, Yanyu Chen, Guan Huang, Xiang Li,
Renjie Li, Naimeng Yao, Xinyi Wang, Xiaotong Gu, Muhammad Bilal Amin, and
Byeong Kang. 2023. Survey on Explainable AI: From Approaches, Limitations
and Applications Aspects. Hum. Centric Intell. Syst. 3, 3 (2023), 161–188. https:
//doi.org/10.1007/s44230-023-00038-y
[84] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. 2016. Revisiting
Semi-Supervised Learning with Graph Embeddings. In Proceedings of the 33nd
International Conference on Machine Learning, ICML 2016, New York City, NY,
USA, June 19-24, 2016 (JMLR Workshop and Conference Proceedings, Vol. 48),
Maria-Florina Balcan and Kilian Q. Weinberger (Eds.). JMLR.org, 40–48. http:
//proceedings.mlr.press/v48/yanga16.html
[85] Zi Ye, Yogan Jaya Kumar, Goh Ong Sing, Fengyan Song, and Junsong Wang. 2022.
A Comprehensive Survey of Graph Neural Networks for Knowledge Graphs. IEEE
Access 10 (2022), 75729–75741. https://doi.org/10.1109/ACCESS.2022.3191784
[86] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
2019. GNNExplainer: Generating Explanations for Graph Neural Networks.
In Advances in Neural Information Processing Systems 32: Annual Confer-
ence on Neural Information Processing Systems 2019, NeurIPS 2019, Decem-
ber 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle,
Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Gar-
nett (Eds.). 9240–9251.
https://proceedings.neurips.cc/paper/2019/hash/
d80b7040b773199015de6d3b4293c8ff-Abstract.html
[87] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. 2023. Explainability in
Graph Neural Networks: A Taxonomic Survey. IEEE Trans. Pattern Anal. Mach.
Intell. 45, 5 (2023), 5782–5799. https://doi.org/10.1109/TPAMI.2022.3204236
[88] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. 2021. On Explain-
ability of Graph Neural Networks via Subgraph Explorations. In Proceedings of
the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139), Marina
Meila and Tong Zhang (Eds.). PMLR, 12241–12252. http://proceedings.mlr.press/
v139/yuan21c.html
[89] Reza Bosagh Zadeh and Ashish Goel. 2013. Dimension independent similarity
computation. J. Mach. Learn. Res. 14, 1 (2013), 1605–1626. https://doi.org/10.
5555/2567709.2567715
[90] Hengtong Zhang, Tianhang Zheng, Jing Gao, Chenglin Miao, Lu Su, Yaliang Li,
and Kui Ren. 2019. Data Poisoning Attack against Knowledge Graph Embedding.
In Proceedings of the Twenty-Eighth International Joint Conference on Artificial
Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, Sarit Kraus (Ed.).
ijcai.org, 4853–4859. https://doi.org/10.24963/ijcai.2019/674
[91] Zhao Zhang, Fuzhen Zhuang, Meng Qu, Zheng-Yu Niu, Hui Xiong, and Qing He.
2021. Knowledge graph embedding with shared latent semantic units. Neural
Networks 139 (2021), 140–148. https://doi.org/10.1016/j.neunet.2021.02.013
[92] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,
Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks:
A review of methods and applications. AI Open 1 (2020), 57–81. https://doi.org/
10.1016/j.aiopen.2021.01.001
[93] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020.
Deep Graph Contrastive Representation Learning. CoRR abs/2006.04131 (2020).
arXiv:2006.04131 https://arxiv.org/abs/2006.04131
[94] Unai Zulaika, Aitor Almeida, and Diego López-de Ipiña. 2022. Regularized Online
Tensor Factorization for Sparse Knowledge Graph Embeddings. Neural Comput.
Appl. 35, 1 (sep 2022), 787–797. https://doi.org/10.1007/s00521-022-07796-z

