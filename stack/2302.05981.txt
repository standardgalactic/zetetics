MarioGPT: Open-Ended Text2Level Generation through Large
Language Models
Shyam Sudhakaran1, Miguel González-Duque1, Claire Glanois1,
Matthias Freiberger1, Elias Najarro1, Sebastian Risi1,2
1IT University of Copenhagen
2modl.ai, Copenhagen
Denmark
{shsu,migd,clgl,matfr,enaj,sebr}@itu.dk
ABSTRACT
Procedural Content Generation (PCG) algorithms provide a tech-
nique to generate complex and diverse environments in an auto-
mated way. However, while generating content with PCG methods
is often straightforward, generating meaningful content that reflects
specific intentions and constraints remains challenging. Further-
more, many PCG algorithms lack the ability to generate content in
an open-ended manner. Recently, Large Language Models (LLMs)
have shown to be incredibly effective in many diverse domains.
These trained LLMs can be fine-tuned, re-using information and
accelerating training for new tasks. In this work, we introduce Mari-
oGPT, a fine-tuned GPT2 model trained to generate tile-based game
levels, in our case Super Mario Bros levels. We show that MarioGPT
can not only generate diverse levels, but can be text-prompted for
controllable level generation, addressing one of the key challenges
of current PCG techniques. As far as we know, MarioGPT is the
first text-to-level model. We also combine MarioGPT with novelty
search, enabling it to generate diverse levels with varying play-
style dynamics (i.e. player paths). This combination allows for the
open-ended generation of an increasingly diverse range of content.
CCS CONCEPTS
• Computing methodologies →Artificial intelligence.
1
INTRODUCTION
Procedural Content Generation (PCG) refers to techniques that
can automatically create game content, such as levels, maps, or
characters [39]. Some of the benefits of PCG are an increase in the
replayability of a game and reduced production costs.
Recently, developments in PCG and machine learning have started
to influence each other in different ways. On the one hand, there
is now a consensus that procedurally generated environments [8]
provide a better way to evaluate the generalisation capabilities
of trained agents [33] over non-PCG benchmarks such as Arcade
Learning Environment [2]. On the other hand, PCG researchers
have started to incorporate machine learning-based approaches
into their systems. For example, recent work has shown that mod-
els such as Generative Adversarial Networks (GANs) [16] can be
trained to generate levels for games as diverse as Doom [13] or
Super Mario Bros, using a level from the Video Game Level Corpus
[47]. However, current approaches in this field of Procedural Con-
tent Generation via Machine Learning (PCGML) [42] often rely on
costly searching inside of the latent space of the underlying neural
(a) "many pipes, many enemies, little blocks, low elevation"
(b) "little pipes, little enemies, many blocks, high elevation"
(c) "many pipes, some enemies"
(d) "no pipes, no enemies, many blocks"
Figure 1: Prompt-conditioned generations from a single
seed block. MarioGPT is able to create diverse levels solely based
on a text prompt in natural language.
networks. It would be more desirable to being able to directly con-
dition a generator to create levels with certain properties, ideally
in natural language.
To address these challenges, we propose MarioGPT (Figure 2),
a fine-tuned GPT-2 model trained to generate Mario levels. Our
model demonstrates how LLMs can be combined with PCG tech-
niques enabling the effective creation of new and diverse levels
through natural language prompts. Large language models (LLMs)
trained on a diverse corpus such as the GPT-n family model [32],
capture the statistical correlations of the human experience in the
form of language correlations. Through this process, GPT acquires
knowledge of how to represent and predict intricate sequences. We
utilize this knowledge to provide our model with the ability to
generate levels that incorporate simple artefacts as well as more
complex relational properties. Surprisingly, a high percentage (88%)
of MarioGPT generated levels are in fact playable.
Furthermore, we combine MarioGPT with novelty search [25], a
diversity-seeking algorithm, to continually generate diverse levels
in an open-ended manner. The combination of LLMs with algo-
rithms such as novelty search opens up many interesting new
directions for future research. We hope our work opens the door
to more flexible and controllable PCG methods that can generate
infinite content that is complex, diverse, and functional. To facilitate
arXiv:2302.05981v1  [cs.AI]  12 Feb 2023

,
Sudhakaran et al.
Prediction
56, 13, 14, 88, ...
Initial Tokenized
level
2, 14, 26, 33, 13, 88, ...
Prompt
3 pipes, 1 enemy,
some blocks, 
low elevation
Frozen Text
Encoder
Generated level
Cross
attention
GPT Layers
MarioGPT
Figure 2: MarioGPT prediction pipeline. Our MarioGPT model is a finetuned version of the distilled GPT2 language model. Like GPT2, MarioGPT is
trained to predict next token sequences. Levels are represented as strings, which are tokenized by a Byte-Pair Encoding, similar to the original GPT2 model.
The level is then split by columns and flattened into a single vector (or batch of vectors for multiple levels). To incorporate prompt information, we utilize a
frozen text encoder in the form of a pretrained bidirectional LLM (BART), and output the average hidden states of the model’s forward pass. This average
hidden state is then used in the cross attention layers of the GPT2 architecture in combination with the actual level sequence being passed into the model.
this, we will make our code available soon. In summary, our main
contributions are the following:
(1) Introducing MarioGPT, a text-to-level model that can gener-
ate Mario levels based on natural language prompts.
(2) Showing how MarioGPT can be combined with novelty
search to produce diverse levels in an open-ended fashion.
2
BACKGROUND AND RELATED WORK
2.1
Procedural Content Generation
Procedural Content Generation (PCG) algorithms [39] deals with
the automatic creation of game content (e.g. for level design, charac-
ter generation, environment modeling, etc.). As reviewed in [39, 48],
earlier works often focused on evolutionary computation [5], solver-
based methods [40] or constructive generation methods (such as
cellular automata, grammar-based methods), etc. More recently,
research on deep learning for PCG [30, 42] has proposed a promis-
ing approach to generate high-quality game content –not only
aesthetically pleasing but also functional and challenging– in a
data-driven manner, while reducing the manual effort required
from game developers. Diversity, originality and playability of the
generated content, but also the controllability of such generation,
remain major challenges [42]. Our work aims to show how condi-
tioned language models, paired with novelty-driven approaches to
content generation [3, 29], could help tackle these shortcomings.
2.2
Neural Network-based Level Generation
Recent works in the space of video game level generation, par-
ticularly for Super Mario [3, 11, 36–38, 47], also leveraged neural
network architectures to create levels. Beukman et al. [3] evolved
neural networks in order to generate levels, while [11, 15, 37, 47]
performed evolution / search in the latent space of a trained genera-
tive model. These works showed that guided sampling of the latent
space of the learned generative model could result in a diverse set
of levels.
However, because the employed generative networks did not
also output play paths through the level, they relied on actual en-
vironment interactions using an A* agent to measure whether the
generated levels were playable. This requirement can be computa-
tionally expensive, especially when there are large levels. As we’ll
show in the following sections, MarioGPT has an advantage com-
pared to these algorithms because it can not only generate diverse
levels, but also predict accurate paths that an agent would take
in the level. This ability, when combined with a diversity-driven
objective, results in a computationally efficient method of gener-
ating diverse and playable levels. In addition, because MarioGPT
utilizes the GPT2 architecture, it can handle much longer sequences
compared to its convolutional and LSTM [18] counterparts, leading
to more diverse and longer levels.
The previous works that utilize a trained generative model [11,
15, 37, 47] also explored the abilities to control characteristics in gen-
erated levels. However, to do so these methods relied on searching
the latent space (e.g. through quality diversity algorithms [12, 31]
and evolutionary strategies [17]) for levels with specific target char-
acteristics (e.g. a level with many pipes). This is a limitation in
that even though the generative models may represent a rich set
of content, one has to search through its latent space for content
with specific characteristics. MarioGPT is able to improve upon this
by incorporating text prompts into the actual generative process,
allowing for easily controllable level generation. In other words, in-
stead of searching for a level with specific characteristics, MarioGPT
allows us to just ask for it.
2.3
Open-Endedness and Genetic Algorithms
The open-endedness paradigm focuses on algorithms that can pro-
duce infinite innovation [27]. These open-ended algorithms are
popular in the field of PCG, where designers and players both can
benefit from diverse and never-ending content. However, PCG must
balance the hard task of generating content with diversity as well
as playability. Genetic algorithms (GA), a family of optimization al-
gorithms that are inspired by the principles of natural selection, are
commonly used as the backbone for more open-ended search meth-
ods. Because GAs allow the integration of multiple objectives, they
are particularly suitable for achieving a balance between fitness
and diversity.
In that regard, novelty search approaches [26] aim at finding the
most novel solutions at each generation, in comparison to what

MarioGPT: Open-Ended Text2Level Generation through Large Language Models
,
has been seen (i.e. an archive of previous solutions). What makes
novelty-search powerful, and motivated its use in this paper, is
that it guides generation towards increasingly diverse solutions in
an open-ended fashion. Novelty search keeps track of solutions in
an archive and measures diversity by the distance between their
behavior characteristics (BCs) compared to that of their 𝑘closest
neighbors. This makes novelty search very flexible, allowing for
the use of many different behavior characteristic types.
2.4
Sequence Modelling and Transformers
Classic approaches to sequence modelling using recurrent neural
networks (RNNs) [34] and Long Short Term Memory (LSTM) net-
works [18] have traditionally been constrained by fading memory
of the networks state vector, as well as limited scalability due to
temporal interdependency of the operations. Transformers [46]
address both challenges by applying associative attention [1] to
learned reprojections of the windowed input sequence, which is
commonly referred to as self-attention. Through the paradigm of
self-attention, transformers can learn to extract the most relevant
information from a currently given time window of the input se-
quence, to predict the next element of the output sequence. Since
this input window is presented in its entirety to the transformer,
many operations within a single self-attention layer can be paral-
lelized and the architecture tends to scale well.
These architectural innovations have enabled Large Language
Models (LLMs) to learn from massive datasets. Additionally, such
models have also shown to be effective in accelerated learning
of down-stream tasks. Fine-tuning LLMs [9] involves using pre-
trained model weights as a weight initialization for new tasks.
Fine-tuning bidirectional LLMs [9, 35] and unimodal LLMs [32]
have shown impressive results on mask prediction and sequence
modelling tasks, respectively.
One particularly relevant use of pretrained / fine-tuned LLMs
comes from the method Evolution through Large Models (ELM), pro-
posed in Lehman et al. [24]. ELM utilizes an LLM diff model [4],
which is trained on code diffs obtained by Github data, giving the
model the ability to modify a code snippet based on a particular
commit message. This diff model is used as a "mutation operator",
for a GA that evolves a population of programs. The wide genera-
tive capabilities of the LLM produce incredibly diverse mutations,
resulting in novel individuals that vary increasingly over the course
of the GA.
3
OPEN-ENDED LEVEL GENERATION
THROUGH LLMS
Here we present our complete approach to open-ended level gener-
ation through LLMs, which is composed of two parts. First, we in-
troduce our prompt-conditioned model MarioGPT (Figure 2) in Sec-
tion 3.2, which generates levels –encoded as text– given a natural-
language prompt. Second, we detail how MarioGPT can be used in a
novelty-search evolutionary loop (Figure 3) in Section 3.3, allowing
the approach to produce a continual stream of diverse levels.
Before going into detail about the MarioGPT model, we introduce
the Mario level representation in the next section.
Table 1: Unique Mario tiles
Tile Type
Symbol
Visualization
Empty
-
Unbreakable
X
Breakable
S
Question Block
? / Q
Coin
o
Enemy
E
Left pipe top
<
Right pipe top
>
Left pipe lower
[
Right pipe lower
]
Cannon Top
B
Cannon Body
b
Path
x
3.1
Level Representation
Mario levels are represented similarly to previous works [11, 15, 36–
38, 47], using the levels provided in the Video Game Level Corpus
(VGLC) [43]. We utilize a relatively small set of path-annotated
levels, taken from Super Mario Bros. and Super Mario Bros.: The
Lost Levels (in total 37 levels). These levels are stitched together,
to essentially make one giant level, allowing us to sample freely
without worrying about the ends of the levels. The levels are repre-
sented as tiles of strings, which encode certain game objects shown
in Table 1. The string representation and characters are tokenized
into discrete values using a Byte Pair Encoding tokenizer used in
the original GPT2 model [32]. One limitation from the dataset is the
simplified representation of enemies. Even though levels contain
many different enemies, each with different behaviors and features,
the dataset represents them all as the same token.
3.2
MarioGPT Model
Our model, MarioGPT, is a prompt-conditioned unidirectional lan-
guage model, optimized for long sequence level prediction. More
precisely, MarioGPT’s architecture relies on a distilled, lightweight
version of GPT2 [32] transformer architecture called DistilGPT2
[35]. Encoding slices of Mario levels as strings, similar to the ap-
proach taken in [41], we can fine-tune this distilled version of GPT2
on predicting next tokens in Mario levels. To generate levels, we
concatenate a window of previous 50 columns into a single vector
and feed them into MarioGPT. Leveraging the wider attention-span
of transformer architectures (compared to LSTM or other mod-
els previously in use for procedural content generation [41, 47]),
generated levels are of improved quality and longer length.
Architecture: MarioGPT’s architecture is the same as the Dis-
tilGPT2 architecture, except the cross attention weights are utilized
for prompting. Even though DistilGPT2 supports context lengths
up to size 1024, we limit our context lengths to 700, as we found
increasing it did little to increase performance. In total, MarioGPT
has 96 million parameters (86 million of the original DistilGPT2
parameters and 10 million from cross attention weights). We train
MarioGPT for 50,000 steps, sampling 4 random slices of levels at
each iteration and optimize the model using the Adam optimizer

,
Sudhakaran et al.
Figure 3: Novelty search setup and MarioGPT mutation operators. A level is sampled from a set of top elites in the archive, mutated, and, if novel
enough, selected to join the archive. The mutation process involves two main steps: (1) Pick a random slice from the level and replace it with a new MarioGPT
sample, using a random prompt. (2) Inpaint the border region with MarioBert to preserve path consistency.
[23]. In total, MarioGPT sees (200,000) training samples. Because
the model is relatively small, it can be trained using a single Nvidia
2080ti GPU.
Prompting details: In order to incorporate prompt informa-
tion, we fine-tune the attention layers’ cross attention weights, as
illustrated in Figure 2. Prompts are encoded through a frozen pre-
trained language model –BART [28]. Prompts are passed through
the frozen language model and the hidden states from the forward
pass are averaged into a single vector. This average hidden state is
then used in the cross attention layers of the GPT2 architecture in
combination with the actual level sequence being passed into the
model.
Prompts are represented as combination of specific features (e.g.
pipes, enemies, blocks, elevation) alongside quantitative keywords:
• { no, little, some, many, [0-1000]} pipes
• { no, little, some, many, [0-1000] } enemies
• { little, some, many, [0-1000]} blocks
• { low, high} elevation
As an example, "no pipes, many enemies, low elevation" or "many
pipes, many enemies, many blocks" are both possible prompts. The
keywords "no", "little", "some", "many" are calculated from quantiles
of the corresponding count (within a 50 column window), as detailed
in Table 2. The "low" and "high" elevation are determined from the
height of the highest unbreakable blocks in a segment of the level.
Table 2: Prompt Quantiles and corresponding counts within
a 50 column window
tile
no
little
some
many
pipes
0
1
2
5
enemies
0
1
3
7
blocks
0
50
75
176
3.3
Open-Ended Mario Level Generation with
Novelty Search
In the realm of PCG, it is important to not only generate levels
with diverse physical features, but also levels that elicit a wide
range of player behavior. Wen it comes to creating Mario levels,
the focus is on the different paths a player can take to complete
the level. This is often a challenge for many algorithms (such as
[11, 47]) and requires the use of an external agent for evaluation.
However, with MarioGPT, it is possible to generate diverse and con-
trollable levels that approximate a realistic player path, reducing
the need for an external agent and producing levels that are directly
playable. To encourage diversity in generated levels, we integrate
MarioGPT within a novelty search augmented genetic algorithm
(NS-MarioGPT), where language-models play the role of mutation
operators. As illustrated in Figure 3, NS-MarioGPT iteratively sam-
ples and mutates elite levels from an archive of generated levels,
which are initialized by randomly prompted MarioGPT samples of
size 1400 (100 columns).
Novelty Search: Mutated levels are only stored in the archive
if they achieve a higher novelty score compared to the previous
elites. The novelty score is measured as the mean distance between
the behavioral characteristic vector of the levels and the behavioral
characteristic vector of the 𝐾closest elements from the archive (𝐾-
means). Our goal in level generation is to create paths that result in
diverse player behavior, so we use predicted player paths as our
basis for these behavior characteristics. More specifically, we are
interested in the relative patterns of predicted paths. For instance, if
a player character moves in a straight line on high elevated blocks,
we want the path’s representation to be close in behavior space to
a path that moves straight in lower elevation. To achieve this, we
represent the behavior characteristic as the normalized average of
the predicted path’s coordinates, allowing a smooth representation
of paths (Figure 4). Thus the significance of a single block difference
is reduced, making it harder for mutated levels to be added to the

MarioGPT: Open-Ended Text2Level Generation through Large Language Models
,
archive. This is desired because we don’t want the archive to fill
up with levels that only vary slightly from the existing levels in
the archive. For all our novelty search experiments, we use a small
neighborhood of size 4, which results in a behavioral characteristic
of dimension 100. We initialize our archive with a small number
of levels (30), as we found mutations are significant enough to
generate a diverse set of levels without a big starting population.
(a) Generated level
0
20
40
60
80
100
2
4
6
8
(b) Extracted path from level
0
20
40
60
80
100
0.00
0.25
0.50
0.75
1.00
(c) Behavior characteristic of level: smoothed path using a moving
average with a window of 5 coordinates
Figure 4: Novelty search behavior characteristic. The behavior
characteristic of a level (a) is represented as the moving average,
with a window of 5, of predicted path coordinates (b). Its smoother
compared to the normal path (b)
Mutations: The LLM-based mutation operation introduced in
this paper (Figure 3) transforms a randomly picked slice of a level
(a slice between 40 −80 columns) with a new MarioGPT prediction,
guided by a random prompt. By itself, MarioGPT is able, through
mutations, to produce a variety of levels with varying agent paths.
However, because MarioGPT is a unidirectional model, we cannot
guarantee that the new generated path is consistent with the rest
of the level. To further improve path consistency, we incorporate
a fine-tuned mask prediction model (which we call MarioBert),
based on the Bert architecture. The BERT language model [10] is a
bidirectional LLM that shows impressive performance in the task
of mask prediction, which is analogous to image in-painting. This
ability is ideal for our use case, where MarioBert is used to inpaint
its border region after the newly sampled slice, smoothly joining
the mutated slice and the rest of level. This can be obeserved in the
second step of the "Mutation process" part of Figure 3.
4
EXPERIMENTS AND RESULTS
4.1
Reconstruction and Sampling Quality
In order to evaluate MarioGPT’s ability to generate diverse and
playable levels, we are interested in the following questions:
(1) How does MarioGPT compare to other baselines in validation
set tile prediction accuracy
(2) Are MarioGPT’s generated levels solveable?
(3) Is MarioGPT memorizing?
(1) Tile Prediction Accuracy: To measure how proficient Mar-
ioGPT is in generating levels, we compare prediction accuracy com-
pared to baselines: LSTM, as proposed in [41] and MarioGPT that
Table 3: Training Reconstruction Accuracy – Validation Set
Model
Tile Acc.
Path Acc.
Promptable?
LSTM
46%
39%
NO
from-scratch-
MarioGPT
31%
23%
YES
adapter-
MarioGPT
21%
11%
YES
MarioGPT
93%
91%
YES
is trained from scratch (without using pretrained GPT2 weights),
with results reported in Table 3. For all our baselines, we train for
the same amount (200,000 samples). Its clear that MarioGPT (using
a pretrained GPT2 model) outperforms all other baselines with re-
gards to tile prediction. In addition, training MarioGPT from scratch
and training an adapter layer (a small multi layer network on top of
the original prediction layer) results in models that performs worse
than even the LSTM baseline (given the 200,000 training samples).
Table 4: Mean average error (MAE) between paths suggested
by model and Baumgarten’s A* agent. Results are averaged over
5 runs per level to account for minor stochastic variation in agent
simulation. MAEs are computed between 𝑦coordinates of path
trajectories for every point on the 𝑥axis (which goes across the
level) both trajectories have visited.
Playable
Not Playable
All
1.14
4.55
1.55
(2) Measuring Playability of Levels:
To test for playability, we deploy Robin Baumgarten’s A* agent
[22, 45] in 250 generated levels1. We find that 88.33% of all MarioGPT-
generated levels can be completed by the agent, and are therefore
considered playable. Moreover, we find that only one of the suc-
cessful levels needed a retry with the A* agent. We further test
whether the path generated by the model matches that of the A*
agent to assess their feasibility. Table 4 shows the mean absolute
error (MAE) between suggested and actual agent path for playable
and not playable levels respectively. We see that for playable levels,
the MAE between the path generated by the model and the actually
taken path by the agent is 1.14 tiles, i.e. paths are on average about
1 tile apart. For the non-playable levels, this average difference of
taken paths is significantly higher with 4.55 tiles. Thus, we can
conclude that in playable levels, the agent mostly takes a similar
path as the one generated by the model. The significantly higher
MAE of 4.55 in non-playable levels on the other hand indicates that
the path generated by the models in these cases may not be feasible
for the agent.
In general though, considering the MAE of 1.55 tiles for paths in
all levels, we can conclude that in the majority of the cases, the path
generated by the model is similar to the path taken by an actual
agent, and having the model generate a path through the level
1Since the agent’s performance depends on the available compute, we test for playa-
bility by running each level 5 times.

,
Sudhakaran et al.
(a) "many pipes, many enemies, little blocks, low elevation"
(b) "no pipes, some enemies, many blocks, high elevation"
(c) "many pipes, many enemies"
(d) "no pipes, no enemies, many blocks"
(e) Prompt that does not exist in the dataset: "many pipes, no enemies, many
blocks"
(f) Failure case: "Many pipes, no enemies, some blocks"
Figure 5: Multiple prompt generations from a single seed block. For the majority of cases, MarioGPT is able to successfully generate
levels that follow the text prompt (a–e). Rarely failure cases happen; for example in (f) the model manages to generate many pipes and some
blocks, but it still generates enemies even though it was prompted with "no enemies".
jointly with the level is an effective approach to obtain high-quality
levels in terms of playability.
To investigate the quality of the generated paths further, we
visualize the paths with the most, least and median overlap (i.e. the
levels corresponding to the maximum, minimum and median values
for the mean absolute error in height) as well as two interesting
handpicked examples in Figure 6.
One can see in Figures 6d and 6e that paths generated by Mari-
oGPT tend to have more airtime than Baumgarten’s agent in the
sense that they only weakly take into account "gravity". This may
be attributed to the nature of the path annotations in the models
training set. In Summerville et al. [43], the authors use an A* path
solver to find a path through the level, while an actual agent, such
as the one we used for comparison here, is more strongly bound
by game physics (especially "gravity") and has to avoid enemies
in the level. A second reason for non-playable levels can be seen
in Figure 6c: Baumgarten’s agent is spawned in a tight space from
which it can not escape, while the model has generated a path that
traverses beyond the actual level, again a path that would likely be
suggested by a solver.
We argue that these issues can in part be attributed to the paths
in the training data stemming from a solver rather than an actual
agent, and could be alleviated in future work by annotating the
training data with the trajectories of actual agents.
(3) Is MarioGPT memorizing?: Memorization dynamics in
LLMs remain an open problem when training transformer architec-
tures, and have been explored by many works ([6, 19, 44]). While
LLMs are incredibly powerful, they can sometimes overfit extremely
and end up regurgitating training data. One popular way to alle-
viate this issue is to add some randomness in predictions in the
form a tunable "temperature" parameter [20]. To evaluate whether
MarioGPT is generating levels that are identical to the training
set, we sample with different temperature parameters and compare
them the closest level in the training dataset. From Figure 7, we can
see that increasing temperature results in samples that are more
diverse, but lack quality. In our case, when generating levels we use
a temperature of 2.4-2.7, as it can generate diverse samples while
still retaining some quality. There are many possible improvements
to explore in the future. One common way is to simply increase the
richness of the dataset. The more samples the model has access to,
the less likely it is to overfit. We could also improve MarioGPT’s
sampling abilities by introducing different search methods other
than sampling with temperature, such as constrained beam search
[7] and dataset augmented search [19], to increase diversity while
preserving more quality.

MarioGPT: Open-Ended Text2Level Generation through Large Language Models
,
(a) Level with minimum MAE between paths
(b) Level with median MAE between paths
(c) Level with maximum MAE between paths
(d) Playable level with interesting path
trajectories
(e) Non-playable level with interesting path
trajectories
Figure 6: A* vs. MarioGPT generated paths. Fig. 6a - Fig. 6c:
Levels with minimum (0.02), median (0.89) and maximum (11.0)
mean absolute error (MAE) between trajectory of actual A* agent
(denoted as A), and model suggestion (denoted as P), as well as
interesting hand-picked examples. Positions where both trajectories
overlap are marked with *. As Fig. 6d and 6e show, paths suggested
by the model generally tend to have more airtime than the A* agent,
likely due to game physics not being accounted for in the original
path annotations of the training data.
4.2
Guided Level Generation via Prompting
Through simple prompting, we are able to guide MarioGPT towards
controllable and diverse level generation. We empirically evaluate
the prompting ability of MarioGPT by generating 1000 samples
with various combinations of prompts, and check how accurate
the generated levels are to the prompt descriptions. The results
suggest that MarioGPT can generate levels that match their given
prompts most of the time (Table 5). MarioGPT is the most accurate
with blocks and the least accurate with enemies. This is expected
because there are fewer total tiles of enemies, while there are many
more block tiles observed during training.
Table 5: Prompt vs actual description accuracy
pipes
enemies
blocks
elevation
81%
68%
92%
76%
(a) Sample with temp 1.0
(b) Closest dataset sample to (a)
(c) Sample with temp 1.7
(d) Closest dataset sample to (c)
(e) Sample with temp 2.8
(f) Closest dataset sample to (e)
Figure 7: Generated levels vs closest in dataset. Temperature
of 1.0 ends up spitting out almost exactly what is in the dataset,
while increasing temperature improves sample diversity.
We also visually evaluated the system, displaying selected prompt-
conditioned generations in Figure 5. In fact, MarioGPT is actually
able to generate levels from text descriptions that are not repre-
sented in the dataset. For instance, Figure 5 (e) shows a success-
ful approximation of the prompt, "many pipes, no enemies, many
blocks", with a slight innacuracy in that it has 1 less pipe (5 pipes
is considered "many", while 4 are present). However, this is not
always the case, as can be seen in Figure 5 (f), where the model,
prompted by "many pipes, no enemies, some blocks", generates a
level with the correct number of pipes and blocks but generates too
many enemies. In future work, we hope to explore more ways to
incorporate prompt importance, such as editing levels with tiles to
create more samples or prompt tuning [21].
4.3
Generating Diverse Levels with Novelty
Search
Through the combination of an LLM (Section 3.2) and novelty
search (Section 3.3), we are able to continuously generate diverse
levels in an open-ended fashion. Specifically, NS-MarioGPT is able
to generate a collection of levels with a diverse set of predicted
agent paths. We project the archive as a set of 2D embeddings in
Figure 10 and darken the embedding points that are added later in
the process. We can see that the levels are increasingly filling up
empty spots in the embedding space.
Figure 8 displays all the overlayed predicted paths (in a level grid)
as more and more levels get added to the archive during novelty
search. We can see that over time, the space of possible predicted
agent paths gets filled, as increasingly diverse levels are mutated
and added to the archive. As more levels are added to the archive,
more and more of the tiles / empty space in the grid are being

,
Sudhakaran et al.
0
20
40
60
80
100
0
2
4
6
8
10
12
14
(a) Archive after 10 levels
0
20
40
60
80
100
0
2
4
6
8
10
12
14
(b) 50 levels
0
20
40
60
80
100
0
2
4
6
8
10
12
14
(c) 150 levels
0
20
40
60
80
100
0
2
4
6
8
10
12
14
(d) 300 levels
Figure 8: Generated path populations during novelty search. Each line is a path through a level. Over time, NS-MarioGPT fills more
and more of the space of all possible paths.
(a) Level with lowest novelty score
(b) Level with highest novelty score
(c) Level with 2nd lowest novelty score
(d) Level with 2nd highest novelty score
Figure 9: Comparison of most and least novel levels in the archive. The two least novel levels are very similar to each other, while
the most novel levels have more distinct path patterns.
Figure 10: tSNE of the Archive Elements. tSNE embeddings are
computed from the behavioral characteristic. Darker points indicate
more recently added elements. Although novelty search is using
the behavioral characteristics of the player paths, the levels also
demonstrate visual novelty.
filled up, indicating that NS-MarioGPT is discovering a variety of
levels that produce diverse paths. Concretely, we found that after
300 levels are added to the archive, around 78% of the possible
coordinates are filled up. However, there are still many overlapping
paths in the archive, meaning that similar paths are still being added
to the archive. This is an issue that could be improved by using
more related time series distance metrics that account for patterns
in a path [14].
Levels with the highest and lowest novelty score from the archive
are also shown in Figure 9. The level with the lowest novelty, shown
in Figure 9 (a), has a path that is much more common in the archive,
which can be seen by its almost identical look compared to the 2nd
lowest in Figure 9 (c). The levels with higher novelty, Figure 9 (b)
and Figure 9 (d), have more unique patterns, but share a similar
pattern towards the end. This indicates that one was created by
mutating the other.
While NS-MarioGPT is still able to discover many diverse levels
through its simple mutation process, more complex functions could
also be explored. For instance, crossover, a common mutation uti-
lized in many genetic algorithms, would increase mutation diversity
which can lead to more diverse levels.
5
CONCLUSION
Here we introduced MarioGPT, a fine-tuned GPT2 LLM that can
not only generate diverse levels, but can guide its generation via
a language prompt. This ability is useful in the field of Procedural
Content Generation, where balancing controllable and diverse gen-
eration is a difficult task. We showed that MarioGPT is also able to
(1) predict player interaction in generated levels, (2) generate diverse
and playable environments, and (3) reduce the need for expensive
external agent interactions (MarioGPT can generate playable levels
approximately 88% of the time). Additionally, when combined with
a diversity-driven algorithm like novelty search, MarioGPT can
generate open-ended and functional content.
One major benefit of re-using an existing LLM is that we can
take advantage of all the research and improvements that have
gone into these architectures. We plan to leverage the scalability
of LLMs and train MarioGPT on bigger, more detailed annotated
levels. Also, we are particularly excited about incorporating human
feedback into the level generation process through reinforcement
learning from human feedback (RLHF) [49]. The ability to fine-tune
these models on human feedback allows users to continually tune
their generated levels towards desired characteristics. Ultimately,
we hope that MarioGPT opens the door to more controllable and
diverse PCG systems.

MarioGPT: Open-Ended Text2Level Generation through Large Language Models
,
ACKNOWLEDGEMENTS
This project was supported by a DFF-Research Project1 grant (9131-
00042B).
REFERENCES
[1] Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. 2014. Neural ma-
chine translation by jointly learning to align and translate.
arXiv preprint
arXiv:1409.0473 (2014).
[2] Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. 2012. The
Arcade Learning Environment: An Evaluation Platform for General Agents. arXiv
(July 2012). https://doi.org/10.1613/jair.3912 arXiv:1207.4708
[3] Michael Beukman, Christopher W Cleghorn, and Steven James. 2022. Procedural
Content Generation Using Neuroevolution and Novelty Search for Diverse Video
Game Levels. In Proceedings of the Genetic and Evolutionary Computation Confer-
ence (Boston, Massachusetts) (GECCO ’22). Association for Computing Machinery,
New York, NY, USA, 1028–1037. https://doi.org/10.1145/3512290.3528701
[4] Herbie Bradley, Honglu Fan, Harry Saini, Reshinth Adithyan, Shivanshu Purohit,
and Joel Lehman. 2023. Diff Models - A New Way to Edit Code. CarperAI Blog
(Jan 2023). https://carper.ai/diff-model/
[5] Cameron Browne and Frederic Maire. 2010. Evolutionary game design. IEEE
Transactions on Computational Intelligence and AI in Games 2, 1 (2010), 1–16.
[6] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
Tramer, and Chiyuan Zhang. 2022. Quantifying Memorization Across Neural
Language Models. https://doi.org/10.48550/ARXIV.2202.07646
[7] Katsuki Chousa and Makoto Morishita. 2021. Input Augmentation Improves
Constrained Beam Search for Neural Machine Translation: NTT at WAT 2021.
https://doi.org/10.48550/ARXIV.2106.05450
[8] Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. 2019. Leverag-
ing Procedural Generation to Benchmark Reinforcement Learning. arXiv (Dec.
2019). https://doi.org/10.48550/arXiv.1912.01588 arXiv:1912.01588
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
https://doi.org/10.48550/ARXIV.1810.04805
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
https://doi.org/10.48550/ARXIV.1810.04805
[11] Matthew C. Fontaine, Ruilin Liu, Ahmed Khalifa, Jignesh Modi, Julian Togelius,
Amy K. Hoover, and Stefanos Nikolaidis. 2020. Illuminating Mario Scenes in the
Latent Space of a Generative Adversarial Network.
https://doi.org/10.48550/
ARXIV.2007.05674
[12] Matthew C. Fontaine, Julian Togelius, Stefanos Nikolaidis, and Amy K. Hoover.
2020. Covariance matrix adaptation for the rapid illumination of behavior space.
In Proceedings of the 2020 Genetic and Evolutionary Computation Conference. ACM.
https://doi.org/10.1145/3377930.3390232
[13] Edoardo Giacomello, Pier Luca Lanzi, and Daniele Loiacono. 2018. Doom level
generation using generative adversarial networks. In 2018 IEEE Games, Entertain-
ment, Media Conference (GEM). IEEE, 316–323.
[14] Omer Gold and Micha Sharir. 2016. Dynamic Time Warping and Geometric Edit
Distance: Breaking the Quadratic Barrier. https://doi.org/10.48550/ARXIV.1607.
05994
[15] Miguel González-Duque, Rasmus Berg Palm, Søren Hauberg, and Sebastian Risi.
2022. Mario Plays on a Manifold: Generating Functional Content in Latent Space
through Differential Geometry. https://doi.org/10.48550/ARXIV.2206.00106
[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial
networks. Commun. ACM 63, 11 (2020), 139–144.
[17] Nikolaus Hansen. 2016. The CMA Evolution Strategy: A Tutorial.
https:
//doi.org/10.48550/ARXIV.1604.00772
[18] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[19] Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagiel-
ski, Katherine Lee, Christopher A. Choquette-Choo, and Nicholas Carlini. 2022.
Preventing Verbatim Memorization in Language Models Gives a False Sense of
Privacy. https://doi.org/10.48550/ARXIV.2210.17546
[20] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical Reparameterization
with Gumbel-Softmax. https://doi.org/10.48550/ARXIV.1611.01144
[21] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2019. How Can
We Know What Language Models Know? https://doi.org/10.48550/ARXIV.1911.
12543
[22] Ahmed. Khalifa. 2009. The Mario AI Framework. https://github.com/amidos2006/
Mario-AI-Framework.
[23] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti-
mization. https://doi.org/10.48550/ARXIV.1412.6980
[24] Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and
Kenneth O. Stanley. 2022. Evolution through Large Models. https://doi.org/10.
48550/ARXIV.2206.08896
[25] Joel
Lehman
and
Kenneth
O.
Stanley.
2011.
Abandoning
Objec-
tives:
Evolution
Through
the
Search
for
Novelty
Alone.
Evolu-
tionary
Computation
19,
2
(06
2011),
189–223.
https://doi.org/
10.1162/EVCO_a_00025
arXiv:https://direct.mit.edu/evco/article-
pdf/19/2/189/1494066/evco_a_00025.pdf
[26] Joel Lehman, Kenneth O Stanley, et al. 2008. Exploiting open-endedness to solve
problems through the search for novelty.. In ALIFE. 329–336.
[27] Joel Lehman, Kenneth O. Stanley, and Lisa Soros. [n. d.]. Open-endedness: The
last grand challenge you’ve never heard of. https://www.oreilly.com/radar/open-
endedness-the-last-grand-challenge-youve-never-heard-of/. Accessed: 2017-12-
19.
[28] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. BART: De-
noising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. https://doi.org/10.48550/ARXIV.1910.13461
[29] Antonios Liapis, Georgios N Yannakakis, and Julian Togelius. 2015. Constrained
novelty search: A study on game content generation. Evolutionary computation
23, 1 (2015), 101–129.
[30] Jialin Liu, Sam Snodgrass, Ahmed Khalifa, Sebastian Risi, Georgios N Yannakakis,
and Julian Togelius. 2021. Deep learning for procedural content generation.
Neural Computing and Applications 33, 1 (2021), 19–37.
[31] Jean-Baptiste Mouret and Jeff Clune. 2015. Illuminating search spaces by mapping
elites. https://doi.org/10.48550/ARXIV.1504.04909
[32] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
[33] Sebastian Risi and Julian Togelius. 2020. Increasing generality in machine learning
through procedural content generation. Nature Machine Intelligence 2, 8 (2020),
428–436.
[34] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1985. Learning
internal representations by error propagation. Technical Report. California Univ
San Diego La Jolla Inst for Cognitive Science.
[35] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-
tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. https:
//doi.org/10.48550/ARXIV.1910.01108
[36] Anurag Sarkar and Seth Cooper. 2021. Dungeon and Platformer Level Blending
and Generation using Conditional VAEs. In Proceedings of the IEEE Conference on
Games (CoG).
[37] Anurag Sarkar and Seth Cooper. 2021. Generating and Blending Game Levels via
Quality-Diversity in the Latent Space of a Variational Autoencoder. In Proceedings
of the Foundations of Digital Games.
[38] Anurag Sarkar, Zhihan Yang, and Seth Cooper. 2020. Conditional Level Genera-
tion and Game Blending. In Proceedings of the Experimental AI in Games (EXAG)
Workshop at AIIDE.
[39] Noor Shaker, Julian Togelius, and Mark J Nelson. 2016. Procedural content
generation in games. (2016).
[40] Adam M Smith and Michael Mateas. 2011. Answer set programming for pro-
cedural content generation: A design space approach. IEEE Transactions on
Computational Intelligence and AI in Games 3, 3 (2011), 187–200.
[41] Adam Summerville and Michael Mateas. 2016. Super Mario as a String: Platformer
Level Generation Via LSTMs. https://doi.org/10.48550/ARXIV.1603.00930
[42] Adam Summerville, Sam Snodgrass, Matthew Guzdial, Christoffer Holmgård,
Amy K Hoover, Aaron Isaksen, Andy Nealen, and Julian Togelius. 2018. Proce-
dural content generation via machine learning (PCGML). IEEE Transactions on
Games 10, 3 (2018), 257–270.
[43] Adam James Summerville, Sam Snodgrass, Michael Mateas, and Santiago On-
tañón. 2016. The VGLC: The Video Game Level Corpus.
https://doi.org/10.
48550/ARXIV.1606.07487
[44] Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Agha-
janyan. 2022. Memorization Without Overfitting: Analyzing the Training Dy-
namics of Large Language Models. https://doi.org/10.48550/ARXIV.2205.10770
[45] Julian Togelius, Sergey Karakovskiy, and Robin Baumgarten. 2010. The 2009
Mario AI Competition. In IEEE Congress on Evolutionary Computation. 1–8. https:
//doi.org/10.1109/CEC.2010.5586133
[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[47] Vanessa Volz, Jacob Schrum, Jialin Liu, Simon M. Lucas, Adam Smith, and Sebas-
tian Risi. 2018. Evolving Mario Levels in the Latent Space of a Deep Convolutional
Generative Adversarial Network. https://doi.org/10.48550/ARXIV.1805.00728
[48] Georgios N Yannakakis and Julian Togelius. 2018. Artificial intelligence and games.
Vol. 2. Springer.
[49] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford,
Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-Tuning Language
Models from Human Preferences. https://doi.org/10.48550/ARXIV.1909.08593

