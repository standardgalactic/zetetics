Information-Theoretic Foundations for Machine Learning
Hong Jun Jeon1∗and Benjamin Van Roy2,3
1Department of Computer Science, Stanford University
2Department of Electrical Engineering, Stanford University
3Department of Management Science and Engineering, Stanford University
Abstract
The staggering progress of machine learning over the past decade has been a sight to behold. In retrospect,
it is both remarkable and unsettling that these milestones were achievable with little to no rigorous theory to
guide experimentation. Despite this fact, practitioners have been able to guide their future experimentation
via observations from previous large-scale empirical investigations. However, alluding to Plato’s Allegory of
the cave, it is likely that the observations which form the field’s notion of reality are but shadows representing
fragments of that reality. In this work, we propose a theoretical framework which attempts to answer what exists
outside of the cave. To the theorist, we provide a framework which is mathematically rigorous and leaves open
many interesting ideas for future exploration. To the practitioner, we provide a framework whose results are
very intuitive, general, and which will help form principles to guide future investigations. Concretely, we provide
a theoretical framework rooted in Bayesian statistics and Shannon’s information theory which is general enough
to unify the analysis of many phenomena in machine learning. Our framework characterizes the performance
of an optimal Bayesian learner, which considers the fundamental limits of information. Unlike existing analyses
that weaken with increasing data complexity, our theoretical tools provide accurate insights across diverse
machine learning settings. Throughout this work, we derive very general theoretical results and apply them to
derive insights specific to settings ranging from data which is independently and identically distributed under
an unknown distribution, to data which is sequential, to data which exhibits hierarchical structure amenable
to meta-learning.
We conclude with a section dedicated to characterizing the performance of misspecified
algorithms. These results are exciting and particularly relevant as we strive to overcome increasingly difficult
machine learning challenges in this endlessly complex world.
∗Correspondence to hjjeon@stanford.edu.
1
arXiv:2407.12288v2  [stat.ML]  18 Jul 2024

Contents
1
Introduction
4
2
Related Works
5
2.1
Frequentist and Bayesian Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.2
PAC Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.3
Information Theory
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3
A Framework for Learning
7
3.1
Probabilistic Framework and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3.2
Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
3.3
Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
3.4
Achievable Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
4
Requisite Information Theory
11
4.1
Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
4.2
Conditional Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
4.3
Mutual Information
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
4.4
Differential Entropy
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
4.5
Requisite Results from Information Theory
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
5
Connecting Learning and Information Theory
18
5.1
Error is Information
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
5.2
Characterizing Error via Rate-Distortion Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
6
Learning from iid Data
22
6.1
Theoretical Results Tailored for iid Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
6.2
Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
6.2.1
Data Generating Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
6.2.2
Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
6.2.3
Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
6.3
Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
6.3.1
Data Generating Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
6.3.2
Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
6.3.3
Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
6.4
Deep Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
6.4.1
Data Generating Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
6.4.2
Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
6.4.3
Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
6.5
Nonparametric Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
6.5.1
Data Generating Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
6.5.2
Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
6.5.3
Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
7
Learning from Sequences
41
7.1
Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
7.2
Binary AR(K) Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
7.2.1
Data Generating Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
7.2.2
Preliminary Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
7.2.3
Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
7.3
Transformer Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
7.3.1
Data Generating Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2

7.3.2
Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
7.3.3
Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
8
Meta-Learning
52
8.1
Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
8.2
Meta-Learning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
8.3
Achievable Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
8.4
General Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
8.5
Linear Representation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
8.5.1
Data Generating Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
8.5.2
Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
8.5.3
Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
8.6
In-Context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
8.6.1
Data Generating Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
8.6.2
Theoretical Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
8.6.3
Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
9
Misspecification
66
9.1
General Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
9.2
Linear Regression with Misspecification
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
9.2.1
Data Generating Process
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
9.2.2
Misspecified Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
9.3
Neural Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
10 Conclusion
73
Appendix
75
A Lower Bounds for Linear Regression
75
B Theoretical Results for Dirichlet-Multinomial
77
B.1
General Combinatorics Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
B.2
Lemmas pertaining to Dirichlet Multinomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
3

1
Introduction
In the past decade, the staggering progress of machine learning has been a sight to behold. The research community
has conquered games such as go, which were thought to require human-level learning and abstraction capabilities
[Silver et al., 2016]. We have produced systems which are capable of displaying common-sense and holding coher-
ent dialogues with humans around the globe [Achiam et al., 2023]. It is undeniable that these artifacts will be
remembered throughout the future of humanity’s pursuit of discovering and understanding intelligence.
In retrospect, it is both remarkable and unsettling that these milestones were achievable with little to no rigorous
theory to guide experimentation. While theorists have attempted to repurpose existing statistical tools to analyze
modern machine learning, the conclusions have largely contradicted the observations of practitioners. Zhang et al.
[2021] aptly demonstrated this point via a series of simple experiments which elucidated the fundamental incom-
patibility of empirically observed phenomena with existing notions of generalization. Despite this incoherence,
practitioners have been able to guide their future experimentation based on prior large-scale empirical investiga-
tions. However, without a clear idea of how these phenomena slot into a larger picture, many research efforts will
continue to be led astray. Alluding to Plato’s Allegory of the cave, it is likely that the observations which form the
field’s notion of reality are but shadows representing fragments of that reality.
In this work, we propose a theoretical framework which attempts to answer what exists outside of the cave. To
the theorist, we provide a framework which is mathematically rigorous and leaves open many interesting ideas for
future exploration. To the practitioner, we provide a framework whose results are very intuitive, general, and which
will help form principles to guide future investigations. Concretely, we provide a theoretical framework rooted in
Bayesian statistics which is general enough to unify the analysis of many phenomena in machine learning.
Our theoretical framework draws inspiration from both Shannon’s theory of information and his maxim of “infor-
mation first, then computation”. The turn of the twentieth century brought a wave of interest in communications
research; work that would enable the transmission of signals across long distances. Much of the work in encod-
ing/decoding was approached heuristically, similarly to how deep learning is today. Shannon’s theory and maxim
redirected attention to characterizing what was fundamentally possible or impossible, in the absence of compu-
tational constraints. His theory guided the discovery of algorithms which achieved these fundamental limits and
eventually practical implementations as well.
The aforementioned staggering feats of machine learning and artificial intelligence have fueled optimism that any-
thing is learnable with sufficient data and computation. However, research directions have largely been informed by
informal reasoning supported by a plethora of empirical studies. While work in statistics provides some guidance,
the results for the most part lack the generality required to explain the continuing onslaught of novel empirical
findings. This monograph aims to provide a general framework to elucidate what is possible by studying how the
limits of performance in machine learning depend on the informational content of the data. Our framework is based
on Shannon’s information theory and characterizes the dependence of performance on information in the absence
of computational constraints. Concretely, we characterize the performance of an optimal Bayesian learner that
observes data generated by a suite of data generating processes of increasing complexity. By expressing what is
fundamentally possible in machine learning, we aim to develop intuition that can guide fruitful investigation.
Our framework characterizes the performance of an optimal Bayesian learner, which considers the fundamental
limits of information. Unlike existing analyses that weaken with increasing data complexity, our theoretical tools
provide accurate insights across diverse machine learning settings. For example, previous theories about learning
from sequential data rely on specific and rigid mixing time assumptions. However, Jeon et al. [2024] leverage our
framework to arrive at more general results which characterize the sample complexity of learning from sequences
autoregressively generated by transformers [Vaswani et al., 2017]. They further extend the techniques to analyze
hierarchical data generating processes which resemble meta-learning and in-context learning in large language
models (LLMs). That these analytic techniques remarkably apply whether data is exchangeable or exhibits more
complex structure points to the fundamental nature of our findings.
In recent years, we have observed that training larger machine learning models on more data continues to produce
significantly better performance. This continual improvement indicates that the data generating processes that
we study are more complex than the machine learning models which we fit.
We refer to this phenomenon as
“misspecification” and it is prominently observed in natural language processing (NLP), where researchers have
tried to mathematically characterize this improvement in performance [Kaplan et al., 2020, Hoffmann et al., 2022].
The “neural scaling laws” of Kaplan et al. [2020] and Hoffmann et al. [2022] characterize the rate at which out-of-
4

sample log-loss decreased with respect to more available compute and data. While these works provide extensive
empirical experimentation, their cursory mathematical analysis leave open many questions regarding how scaling
laws change depending on the complexity of the data-generating process. Jeon and Roy [2024] once again use the
theoretical tools from this monograph to rigorously characterize the error incurred by a misspecified algorithm.
They study a data-generating process which is identified by an single hidden-layer network of infinite width and
characterize how an algorithm with finite budget should optimally allocate between parameter count and dataset
size. These results notably are consistent (up to logarithmic factors) with the the findings of Hoffmann et al. [2022],
where the optimal dataset size and parameter count exhibit a linear relationship.
Despite the fact that our theory does not address computational constraints, empirical studies with neural networks
suggest that practical stochastic gradient algorithms suffice to attain the tradeoffs that our theory establishes
between information and performance [Zhu et al., 2022]. Throughout this work, we derive very general theoretical
results and apply them to derive insights specific to settings ranging from data which is iid under an unknown
distribution to data which is sequential to data which exhibits hierarchical structure amenable to meta-learning.
We conclude with a section dedicated to characterizing the performance of suboptimal algorithms that are based
on misspecified models, an exciting and relevant direction for future work.
2
Related Works
2.1
Frequentist and Bayesian Statistics
We begin with a discussion of frequentist statistics, the predominant framework which encompasses existing theo-
retical results. As its name would suggest, in frequentist statistics, probability describes how often an event occurs
if a procedure is repeated many times. For instance, suppose there exists an unknown parameter µ ∈ℜand a sam-
ple of size T: (X1, X2, . . . , XT ) which is drawn iid N(µ, 1). After observing the sample, the frequentist statistician
may construct a 95% confidence interval for the unknown µ. However, recall that in frequentism, probability is
assigned to how often an event occurs if a procedure is repeated many times. For our example, this entails that
if random samples of size T were drawn repeatedly and their corresponding confidence intervals constructed, then
95% of those confidence intervals would contain µ. Note that the unknown parameter µ is fixed and hence not
a random variable in the frequentist framework. As a result, the frequentist framework does not use the tools of
probability to model uncertainty pertaining to µ.
In contrast, Bayesian statistics treats all unknown quantities as random variables. A consequence is that these
quantities must be assigned subjective probabilities which reflect one’s prior beliefs pertaining to their values.
Returning to our example, the Bayesian may assign a prior distribution P(µ ∈·) = N(0, 1) which reflects their
beliefs prior to observing the sample. After observing the sample (x1, x2, . . . , xT ), they may construct a 95% credible
interval for µ, an interval (a, b) for which P(µ ∈(a, b)|X1 = x1, X2 = x2, . . . , XT = xT ) ≥0.95. The posterior
distribution P(µ ∈·|X1 = x1, X2 = x2, . . . , XT = xT ) is computed via Bayes rule. Note that unlike the frequentist
confidence interval which pertains to repeated experimentation, the Bayesian credible interval states that for this
particular sample, with 95% probability, µ ∈(a, b). However, we note that this probability is subjective as it relies
upon the prior subjective probability P(θ ∈·). While this subjectivism has been a topic of constant philosophical
debate, we note that in the realm of decision theory, it is well known that the choices of a decision maker which
abides by axioms of rationality can be explained as the result of a utility function and subjective probabilities
assigned to events [Savage, 1972].
While the debate surrounding these two schools of thought have continued for almost a century, it is prudent to
consider the purpose for such theory. We are reminded of Laplace’s prudent remark that “Probability theory is
nothing but common sense reduced to calculation”. Theory’s merit ought to stem from the results it can provide
for specific problems [Jaynes and Kempthorne, 1976]. Jaynes and Kempthorne [1976] espoused this viewpoint as
their background in physics led to their interest in the use of probability to describe and predict phenomena of
the physical world. Machine learning too is is rooted in predictions based on data produced by the physical world.
Therefore, we argue that the merits of machine learning theory also ought to stem from its ability to describe and
predict phenomena of data generated by the physical world. To this end, we believe that the results which we
derive via our framework both better reflect what is observed empirically and are also general enough to unify
many disparate areas of the field.
5

2.2
PAC Learning
The majority of existing theoretical results for the analysis of modern machine learning are set in the probably
approximately correct (PAC) learning framework [Valiant, 1984]. In PAC learning, an algorithm is presented with
a sample of data and is tasked with returning a hypothesis from a hypothesis set which can accurately perform
predictions out-of-sample. The probably approximately correct come from the detail that these results are phrased
as follows: “For any data distribution, with probability at least 1 −δ over the randomness of an iid sample, the
excess out-of-sample error is ≤ϵ”. “Probably” refers to the 1 −δ probability and “approximately correct” the ϵ
tolerance of out-of-sample error. While this framework has facilitated the development of influential theoretical
concepts such as VC dimension [Vapnik, 2013] and Rademacher complexity [Bartlett and Mendelson, 2002], Zhang
et al. [2021] have demonstrated that these tools are inherently insufficient to explain modern empirical phenomena.
Namely, they demonstrate empirically that while the Rademacher complexity of a deep neural networks leads to
vacuous theoretical results, the observed out-of-sample error of these deep neural networks is actually small.
We posit that the looseness of these theoretical results stems from the fact that they hold for any data distribution
and uniformly over the hypothesis set. While it is true that Rademacher complexity depends on the distribution of
the inputs, it does not depend on the joint distribution of the input and outputs. Meanwhile, data which we observe
from the real world clearly contains inherent structure between input and output which facilitate sample-efficient
learning. Suppose we perform binary classification with input X ∼N(0, Id). In case 1, consider a data generating
process in which the corresponding class Y only depends on the first element of X. In case 2, consider a data
generating process in which Y depends on all d elements of X. Common sense would dictate that if we observed
an equal number of samples from each data generating process and considered the same hypothesis spaces, the
generalization error of case 1 ought to be lower than that of case 2. However, an analysis via VC dimension or
Rademacher complexity would result in the same generalization bound for case 1 and 2. This problem exacerbated
by high dimensional input distributions and overparameterized hypothesis classes, both of which are prevalent
qualities of deep learning.
Several lines of analysis have attempted to ameliorate this via data dependent bounds. While these results also
hold for any data distribution, the choice of data distribution will impact the resulting error bound. Therefore,
such a result will be vacuous (as expected) for a problem instance with unstructured data, but potentially much
tighter for one which exhibits structure. The main frameworks for data dependent PAC results involve PAC Bayes
[McAllester, 1998] and the information-theoretic framework of Xu and Raginsky [2017]. Both frameworks involve
an algorithm which produces a predictive distribution of the hypothesis conditioned on the observed data (hence
they analyze a Bayesian algorithm under the PAC framework). The two only differ in that PAC Bayes results hold
with high probability over random draws of the data while the information-theoretic results hold in expectation
over random draws of the data. Therefore, each result upper bounds generalization error via the KL divergence or
the mutual information between the observed data and the hypothesis.
These data dependent results mark a significant step in understanding the puzzling empirical success of deep
learning. Notably, Dziugaite and Roy [2017] establish PAC-Bayes results for deep neural networks which result in
bounds that dramatically improve upon those based on parameter count or VC dimension. These results reflect
the importance that the data generating process has on the generalization error.
However, a limitation is the
lack of theoretical tools which facilitate analytic derivations. Namely, the aforementioned KL divergence/mutual
information which bound generalization cannot be computed analytically outside of simple problem instances.
This is because these quantities involve the posterior distribution of the hypothesis conditioned on the data, which
cannot be expressed analytically outside of simple instances which exhibit a conjugate distribution. In contrast,
our results analyze these quantities in a Bayesian setting, allowing us to develop general tools to bound mutual
information analytically without needing to write down these complicated posterior distributions. The conciseness
and generality of these results lead us to believe that they are fundamental.
2.3
Information Theory
The results of this work elucidate the tight relation between error in learning and information measures of Shannon’s
theory [Shannon, 1948]. Prior work establishes connections between information theory and parameter estimation
notably through information-theoretic lower bounds. Namely, the global Fano method resembles a portion of the
techniques which we will cover in this monograph.
A widely known framework involving information theory and machine learning is the information bottleneck method
[Tishby et al., 2000]. On the surface, this work exhibits many similarities to ours as it draws a connection between
6

information theory, rate-distortion theory and machine learning. However, the two works diverge in their purpose.
The information bottleneck framework describes a learning objective rooted in information theory and prescribes a
learning algorithm to solve this optimization problem. While they leveraged their framework to produce early work
on generalization in deep neural networks [Tishby and Zaslavsky, 2015], the results remained very abstract. While
they devised metrics which they approximate empirically, they do not provide theoretical tools to analyze these
metrics analytically. In contrast, we present our framework with a collection of theoretical tools which facilitate
analytic solutions. This is an important property of a theoretical framework as it allows the researcher to forecast
what ought to be possible in practice.
As alluded to above, there exist notable information-theoretic generalization results in the PAC learning framework
introduced by Russo and Zou [2019] and expanded upon by Xu and Raginsky [2017]. While our work shares similar
analytic techniques, we are able to strengthen the results and provide more streamlined theoretical tools by framing
results in a Bayesian setting. In particular, this framing allows us to upper and lower bound the mutual information
between the data and a latent parameter via the rate-distortion function, which we can characterize analytically
for even complex data generating processes. We find the results of our framing to be fundamental and we hope
that the readers share this sentiment. We also hope that provides the reader with a new perspective on machine
learning.
The recent advances in LLMs have incited an interest in the connection between learning and compression. In
particular, researchers have posited that models which are better able to compress the observed data will achieve
lower out-of-sample error. This point is conveyed in [Deletang et al., 2024]. However, they do not provide a math-
ematically rigorous connection between learning and compression. In this work we establish a rigorous connection
between learning and optimal lossy compression. The loss incurred by an optimal Bayesian learner is upper and
lower bounded by appropriate expressions containing the rate-distortion function (characterization of optimal lossy
compression).
For this community, we hope that our work provides clarity to this matter and informs future
experimentation and algorithm design.
3
A Framework for Learning
3.1
Probabilistic Framework and Notation
In our work, we define all random variables with respect to a common probability space (Ω, F, P). Recall that a
random variable θ is a measurable function Ω7→Θ from the sample space Ωto a set Θ.
The probability measure P : F 7→[0, 1] assigns probabilities to events in the σ −algebra F. In particular, for any
event F ∈F, P(F) denotes the probability of the event. For events F, G ∈F for which P(G) > 0, P(F|G) denotes
the probability of event F conditioned on event G.
For each realization z of a random variable Z, P(Z = z) is hence a function of z. We denote the value of this
function evaluated at Z by P(Z). Therefore, P(Z) is a random variable (since it takes realizations in [0, 1] depending
on the value of Z). Likewise for realizations (y, z) of random variables Y, Z, P(Z = z|Y = y) is a function of (y, z)
and P(Z|Y ) is a random variable which denotes the value of this function evaluated at (Y, Z).
If random variable Z : Ω7→ℜK has density pZ w.r.t the Lebesgue measure, the conditional probability P(F|Z = z)
is well-defined despite the fact that for all z, P(Z = z) = 0. If function f(z) = P(F|Z = z) and Y : Ω7→ℜK is
a random variable whose range is a subset of Z’s, then we use the ←symbol with P(F|Z ←Y ) to denote f(Y ).
Note that this is different from P(F|Z = Y ) since this conditions on the event Z = Y while P(F|Z ←Y ) indicates
a change of measure.
For any random variable θ, we use the notation P(θ ∈·) to denote the distribution of that random variable i.e.
P(θ ∈·) = P(θ−1(·)), where θ−1(·) denotes the pre-image of · (the pre-image must be ∈F due to measurability). We
make a clear distinction between a random variable and its distribution in this way to provide accurate definitions
of information-theoretic quantities later in this work. As mentioned in the introduction, our framework is Bayesian,
and hence uses the tools of probability to model uncertainty about the unknown value of a variable of interest (for
instance θ). This involves treating θ as a random variable with a prior distribution P(θ ∈·) which encodes the
designer’s prior information about the value of this variable. The designer will often never directly observe θ,
but rather a stream of data which will contain information about θ. Machine Learning is therefore the process of
reducing uncertainty about θ in ways that are relevant for making better predictions about the future of this data
stream.
7

3.2
Data Generating Process
In machine learning, we are interested in discerning the relationship between input and output pairs (X, Y ). Most
frameworks of machine learning focus on a static dataset of fixed size and hope to characterize the performance
of a predictive algorithm which leverages the information from the dataset for future predictions. However, any
practical system will continually have access to additional observations as it interacts with the environment. As a
result, it is prudent to consider a framework in which the data arrives in an online fashion and the objective is to
perform well across all time.
We consider a stochastic process which generates a sequence ((Xt, Yt+1) : t ∈Z+) of data pairs. For all t, we let
Ht denote the history (X0, Y1, . . . , Xt−1, Yt, Xt) of experience. We assume that there exists an underlying latent
variable θ such that (X0, X1, . . .) ⊥θ and θ prescribes a conditional probability measure θ(·|Ht) to the next label
Yt+1. In the case of an iid data generating process, this conditional probability measure would only depend on
Ht via Xt. Furthermore, such a latent variable θ must exist under an infinite exchangability assumption on the
sequence ((Xt, Yt+1) : t ∈Z+) by de Finetti’s Theorem. While we will first focus on this iid setting, we will also
study learning setting in which the future data may be arbitrarily dependent on Ht even when conditioned on θ.
As our framework is Bayesian, we represent our uncertainty about θ by modeling it as a random variable with prior
distribution P(θ ∈·).
3.3
Error
Our framework focuses on a particular notion of error which facilitates analysis via Shannon-information theory.
For all t ∈Z+, our algorithm is tasked with providing a predictive distribution Pt of Yt+1 which may be derived
from the history of data Ht which has already been observed. We denote such algorithm as π for which for all t,
Pt = π(Ht). As aforementioned, an effective learning system ought to leverage data as it becomes available. As a
result, for any time horizon T ∈Z+, we are interested in quantifying the cumulative expected log-loss:
LT,π = 1
T
T −1
X
t=0
Eπ [−ln Pt(Yt+1)] .
As outlined in section 3.1, we take all random variables to be defined with respect to a common probability space.
As a result, the expectation E integrates over all random variables which we do not condition on. We use the
subscript π in Eπ to specify that it is a function of π since for all t, π produces Pt.
As Yt+1 is the random
variable which represents the next label that is generated by the underlying stochastic process, Pt(Yt+1) denotes
the probability that our prediction Pt assigned to label Yt+1.
This loss function is commonly referred to in the literature as “log-loss” or “negative log-likelihood” and has
become a cornerstone of classification methods via neural networks. However, it is important to note that even for
an optimal algorithm, the minimum achievable log-loss is not 0. For instance, consider an omniscient algorithm
in the classification setting which produces for all t the prediction Pt = P(Yt+1 ∈·|θ, Ht). Even this agent incurs a
loss of:
1
T
T −1
X
t=0
Eπ [−ln P(Yt+1|θ, Ht)] = 1
T
T −1
X
t=0
H(Yt+1|θ, Ht)
= H(Y1:T |θ, X0:T −1)
T
,
where our point follows from the fact that the conditional entropy of a discrete random variable Y1:T is non-negative.
As a result, we define the estimation error as:
LT,π = LT,π −H(Y1:T |θ, X0:T −1)
T
.
Estimation error represents the error which is reducible via learning. As such, for a competent learning algorithm
tasked with a learnable task, LT,π should decay to 0 as T goes to ∞.
3.4
Achievable Error
Since we are interested in characterizing the limits of what is possible via machine learning, a natural question
which arises is: For all T, which π minimizes LT,π? Since log-loss is a proper scoring rule, the optimal algorithm
8

π is one such that for all t, Pt = P(Yt+1 ∈·|Ht). This predictive distribution is often referred to as the Bayesian
posterior and going forward we will denote it by ˆPt. The following result establishes optimality of ˆPt.
Lemma 1. (Bayesian posterior is optimal) For all t ∈Z+,
E
h
−ln ˆPt(Yt+1) | Ht
i
a.s.
=
min
π
Eπ [−ln Pt(Yt+1) | Ht] .
Proof.
E [−ln Pt(Yt+1)|Ht]
a.s.
= E
"
−ln ˆPt(Yt+1) + ln
ˆPt(Yt+1)
Pt(Yt+1)
Ht
#
a.s.
= E
h
−ln ˆPt(Yt+1)|Ht
i
+ dKL

ˆPt∥Pt

.
The result follows from the fact that KL-divergence is non-negative and the tower property.
This result is rather convenient since it prescribes that across all problem instances, the optimal prediction is ˆPt.
This is widely considered an advantage of the Bayesianism as opposed to the frequentism; the Bayesian need not
specify an ad-hoc algorithm to analyze/solve a problem. While in practice it may be intractable to compute ˆPt
exactly, for the purposes of characterizing achievable error, it provides immense utility. Going forward, we will
restrict our attention to the optimal achievable estimation error which we denote by:
LT = 1
T
T −1
X
t=0
E
h
−ln ˆPt(Yt+1)
i
−H(Y1:T |θ, X0:T −1)
T
.
The process of learning should result in LT vanishing to 0 as T increase to ∞. While we have established that
the Bayesian posterior provides optimal predictions at each timestep, in its current form, it is unclear how to
characterize LT for problems in which the posterior distribution does not exhibit an analytic form. In section 5, we
will establish the connection between our learning framework and information theory. This connection will facilitate
the analysis of arbitrary learning problems, even those for which ˆPt cannot be expressed analytically.
In the following section, we overview requisite definitions and tools from information theory to establish the con-
nection between learning and information theory. For readers who are new to information theory, we provide the
following section for completeness. Even for readers who are familiar with information theory, there may be details
or results is the following section which may be worth revisiting.
Summary
• The algorithm’s observations through time t form a history Ht = (X0, Y1, . . . Xt−1, Yt, Xt).
• The algorithm π produces for all t a predictive distribution Pt of Yt+1 given the history Ht.
• For any horizon T ∈Z+, we define estimation error of an algorithm π as
LT,π = 1
T
T −1
X
t=0
Eπ [−ln Pt(Yt+1)]
|
{z
}
log−loss
−H(Y1:T |θ, X0:T −1)
T
|
{z
}
irreducible error
.
• The optimal algorithm assigns for all t,
Pt ←ˆPt = P(Yt+1 ∈·|Ht).
9

• We denote the estimation error incurred by the optimal algorithm by
LT = 1
T
T −1
X
t=0
Eπ
h
−ln ˆPt(Yt+1)
i
−H(Y1:T |θ, X0:T −1)
T
.
10

4
Requisite Information Theory
In this section we outline definitions and results from information theory which we will refer to in later sections of
this monograph. For a comprehensive overview of the topic, we point the reader to [Cover and Thomas, 2012].
4.1
Entropy
In this text, H(X) denotes the entropy of a discrete random variable X : Ω7→X. H is defined as follows:
H(X) =
X
x∈X
P(X = x) ln
1
P(X = x).
Throughout this monograph, we use the convention that 0 ln 0 = 0. While there are many colloquial interpretations
of entropy which describe it as the expected “surprise” associated with outcomes of a random variable, we provide
a concrete motivation for entropy based on coding theory.
We begin our exposition of entropy with an introduction to coding theory. We first define a code for a random
variable.
Definition 2. A code C for random variable X : Ω7→X is a function which maps X 7→{0, 1}∗, where {0, 1}∗
denotes the set of all binary strings.
When we send a text message to our friend, the characters that comprise of our message can be thought of as the
realizations of random variables. In many applications involving digital data transfer, a message (outcome of a
random variable) is encoded into a binary string which is passed through a communication channel, and decoded
at an endpoint. Since the binary string arrives in a stream, it would be convenient if the message could be uniquely
decoded as the data is arriving. A necessary and sufficient condition for this is online decodability is to design a
code C which is prefix-free i.e. no element in the image of C is a prefix of another element in the image of C. We
use CX to denote the set of prefix-free codes for a random variable X
Since these codes are stored, transmitted, and decoded, the memory footprint becomes a significant design consid-
eration. A natural question which arises is: “how do we devise optimal prefix-free codes?” The notion of optimality
which gives rise to Shannon entropy is the following:
arg min
C∈CX
X
x∈X
P(X = x) · len (C(x))
log2(e)
,
where len(c) denotes the length of binary string c. A prefix-free code which minimizes this objective will on-average
require the fewest number of bits to store/transmit information. A naive prefix-free code is one which maps each of
the |X| outcomes of X to a unique binary string of length ⌈log2 |X|⌉. While such a code would be reasonable if all
outcomes of X were equally likely, such a code would be highly suboptimal if some outcomes are much more/less
likely than others. A competent coding scheme ought to map more likely outcomes to shorter strings and less likely
outcomes to longer strings.
11

0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
e t a o i n s h r d l
c u m w f g y p b v k
j
x q z
Figure 1: The English alphabet consists of characters of varying frequency across common text corpuses. Notably,
the vowels appear with greater frequency. A coding scheme ought to map these more frequently appearing characters
to shorter strings.
The following result establishes the tight connection between the entropy of X and its optimal prefix-free code.
Theorem 3. (entropy characterizes optimal prefix-free code length) For all discrete random variables
X : Ω7→X,
H(X) ≤
min
C∈CX
X
x∈X
P(X = x) · len(C(x))
log2(e)
≤H(X) +
1
log2(e).
This result demonstrates that the entropy of a random variable tightly characterizes the fundamental limit to which
it can be losslessly compressed. As a result, the entropy reflects the inherent complexity of a random variable. This
connection is useful to keep in mind as there exist the following analogies between our Bayesian and the frequentist
frameworks:
frequentist 7→Bayesian
Θ 7→P(θ ∈·)
|Θ| 7→H(θ),
where θ : Ω7→Θ and |Θ| denotes the cardinality of Θ.
4.2
Conditional Entropy
H(X|Y ) denotes the conditional entropy of a discrete random variable X : Ω7→X conditioned on another discrete
random variable Y : Ω7→Y. The conditional entropy is defined as follows:
H(X|Y ) =
X
x∈X,y∈Y
P(X = x, Y = y) ln
1
P(X = x|Y = y).
Note that unlike conditional expectation, conditional entropy is a number (and not a random variable). Upon closer
inspection it is clear that conditional entropy is also always non-negative and is 0 only when Y fully determines X.
On the other hand, when X ⊥Y , we have that H(X|Y ) = H(X). We provide the following result which facilitates
mathematical manipulations involving the information-theoretic quantities outlined thus far.
Lemma 4. (chain rule of conditional entropy) For all discrete random variables X : Ω7→X, Y : Ω7→Y,
H(X, Y ) = H(X) + H(Y |X) = H(Y ) + H(X|Y ).
12

Proof.
H(X) + H(Y |X) =
X
x∈X
P(X = x) ln
1
P(X = x) +
X
x∈X,y∈Y
P(X = x, Y = y) ln
1
P(Y = y|X = x)
=
X
x∈X,y∈Y
P(X = x, Y = y) ln
1
P(X = x) +
X
x∈X,y∈Y
P(X = x, Y = y) ln
P(X = x)
P(X = x, Y = y)
=
X
x∈X,y∈Y
P(X = x, Y = y) ln
1
P(X = x, Y = y)
= H(X, Y ).
The second equality in the lemma statement follows from the same technique shown above.
If H(X, Y ) denotes the average length of a prefix-free code for (X, Y ) jointly, Lemma 4 establishes that H(X|Y )
reflects the average length of a prefix-free code for X after Y is already observed. Evidently if X ⊥Y , then
observing Y does not provide any information which enables a shorter code for X (hence, H(X|Y ) = H(X)).
However, in the other extreme in which X
a.s.
= Y , observing Y means that X is also known. As a result, a trivial
code which maps every outcome of X to the null string ∅will suffice (hence, H(X|Y ) = 0).
4.3
Mutual Information
I(X; Y ) denotes the mutual information between two random variables X : Ω7→X and Y : Ω7→Y.
Con-
cretely,
I(X; Y ) = dKL (P((X, Y ) ∈·)∥P(X ∈·) ⊗P(Y ∈·)) ,
where P(X ∈·) ⊗P(Y ∈·) denotes the outer product distribution. Note that KL divergence is a non-symmetric
function which maps two probability distributions to ℜ+∪{∞}. For discrete random variables, we have the following
equivalence between mutual information and differences of (conditional) entropies:
I(X; Y ) = H(X) −H(X|Y ) = H(Y ) −H(Y |X).
Note that mutual information is symmetric i.e. I(X; Y ) = I(Y ; X) and it is also always non-negative (follows directly
as a consequence of Lemma 10). Intuitively, the mutual information I(X; Y ) represents the amount of information
that X conveys about Y and vice versa.
As such, if X fully determines Y , then I(X; Y ) = H(X) = H(Y ).
Meanwhile if X ⊥Y , then I(X; Y ) = 0 as the two random variables convey no information about each other. As
with conditional entropy, we provide the following result which facilitates mathematical analyses involving mutual
information:
Lemma 5. (chain rule of mutual information) For all random variables X : Ω7→X, Y : Ω7→Y, Z : Ω7→Z,
I(Y ; Z|X) + I(X; Z) = I(X, Y ; Z).
Proof.
I(Y ; Z|X) + I(X; Z) = H(Z|X) −H(Z|X, Y ) + H(Z) −H(Z|X)
= H(Z) −H(Z|X, Y )
= I(X, Y ; Z).
4.4
Differential Entropy
While we have defined information-theoretic quantities for discrete random variables, outside of mutual information
we have not broached a notion of information regarding continuous random variables. For a continuous random
variable X : Ω7→X with density pX (w.r.t the Lebesgue measure), we denote the differential entropy of X by
h(X) =
Z
x∈X
pX(x) ln
1
pX(x)dµ(x),
13

Figure 2: This venn diagram illustrates the relationships between the introduced information-theoretic quantities.
where µ(·) denotes the Lebesgue measure. While differential entropy ostensibly resembles discrete entropy, the two
are different in almost all regards. For instance, while the discrete entropy of a random variable is always non-
negative, the differential entropy can often be negative. Look no further than X = Uniform([0, 1/2]). In this case,
h(X) = −ln(2). Furthermore, while discrete entropy is invariant under one-to-one transformations, the differential
entropy is not. For instance, h(2X) = h(X) + ln 2. A measure of information should not be negative nor should
it be dependent on units used. For these reasons, unlike discrete entropy, differential entropy is not a meaningful
informational quantity by itself. The correct extension of discrete entropy to continuous random variables exists
via rate-distortion theory, which we will present in the following section.
While differential entropy itself is not a meaningful measure of information, differences in (conditional) differential
entropies are still equal to mutual information.
Concretely, for continuous random variables X, Y with finite
(conditional) differential entropies,
I(X; Y ) = h(X) −h(X|Y ) = h(Y ) −h(Y |X).
4.5
Requisite Results from Information Theory
We now present an amalgamation of widely known and requisite results from information theory. Various proofs
throughout this monograph will refer to the results of this section.
Lemma 6. (log-sum inequality) For all n ∈Z, if a1, . . . , an ≥0, b1, . . . , bn ≥0, and a = Pn
i=1 ai, b = Pn
i=1 bi,
then
n
X
i=1
ai ln ai
bi
≥a ln a
b .
Proof.
n
X
i=1
ai ln ai
bi
=
n
X
i=1
bi · ai
bi
ln ai
bi
= b
n
X
i=1
bi
b · ai
bi
ln ai
bi
(a)
≥b
 n
X
i=1
bi
b · ai
bi
!
ln
 n
X
i=1
bi
b · ai
bi
!
= b · a
b ln a
b
= a ln a
b ,
where (a) follows from Jensen’s inequality applied to the function x ln x.
Lemma 7. (conditioning reduces entropy) For all discrete random variables X : Ω7→X, Y : Ω7→Y,
H(X) ≥H(X|Y ).
14

Proof.
H(X) =
X
x∈X
P(X = x) ln
1
P(X = x)
=
X
x∈X
X
y∈Y
P(X = x, Y = y) ln
1
P(X = x)
(a)
≥
X
x∈X
X
y∈Y
P(X = x, Y = y) ln
P(Y = y)
P(X = x, Y = y)
=
X
x∈X
X
y∈Y
P(X = x, Y = y) ln
1
P(X = x|Y = y)
= H(X|Y ),
where (a) follows from negating the log-sum inequality and setting ai = P(X = x, Y = y), bi = P(Y = y) and
a = P(X = x), b = 1.
Lemma 8. (conditioning reduces differential entropy) For all continuous random variables X : Ω7→X, Y :
Ω7→Y, if h(X), h(X|Y ) exist and are both finite, then
h(X) ≥h(X|Y ).
Proof. The proof follows from the same reasoning as in Lemma 7 by constructing a sequence of partitions of X, Y
and taking the associated limits.
Lemma 9. (equivalence of mutual information and KL-divergence) For all random variables X : Ω7→
X, Y : Ω7→Y,
I(X; Y ) = E [dKL (P(Y ∈·|X) ∥P(Y ∈·))] .
Proof. We prove the result for discrete random variables X, Y . With appropriate technical assumptions, the result
can also be extended to continual random variables which exhibit density functions.
I(X; Y ) =
X
x∈X
X
y∈Y
P(X = x, Y = y) ln
P(X = x, Y = y)
P(X = x) · P(Y = y)
=
X
x∈X
P(X = x)
X
y∈Y
P(Y = y|X = x) ln P(Y = y|X = x)
P(Y = y)
= E [dKL (P(Y ∈·|X) ∥P(Y ∈·))] .
Lemma 10. (non-negativity of KL-divergence) For all probability distributions P(·) : F 7→[0, 1], Q(·)F 7→
[0, 1],
dKL (P(·) ∥Q(·)) ≥0.
Furthermore,
dKL((P(·) ∥Q(·)) = 0 ⇐⇒
for all ν ∈F, (P(ν) > 0 =⇒P(ν) = Q(ν)) .
15

Proof.
dKL (P(·) ∥Q(·)) =
X
ν∈F
P(ν) ln P(ν)
Q(ν)
=
X
ν∈F
−P(ν) ln Q(ν)
P(ν)
(a)
≥−ln
X
ν∈F
P(ν) · Q(ν)
P(ν)
= −ln
X
ν∈F
Q(ν)
= 0.
where (a) follows from Jensen’s inequality. To prove the second result, consider the case in which Jensen’s inequality
holds with equality. This occurs iff Q(ν)/P(ν) = P
ν∈F P(ν) · Q(ν)/P(ν) = 1. This occurs only when Q(ν) = P(ν)
for all ν ∈F for which P(ν) > 0.
Lemma 11. (maximum differential entropy) For all density functions q : ℜd 7→ℜ+, for all i, j ∈[d], let
Σi,j =
R
x∈ℜd xixjq(x)dx. If X ∼N(0, Σ), then
h(X) ≥
Z
x∈ℜd q(x) ln
1
q(x)dx.
Proof. Let p denote the probability density function of X.
h(X) =
Z
x∈ℜd −p(x)
1
2 ln(2πe|Σ|) −x⊤Σ−1x
2

dx
= −1
2 ln (2πe|Σ|) +
Z
x∈ℜd p(x) · x⊤Σ−1x
2
dx
(a)
= −1
2 ln (2πe|Σ|) +
Z
x∈ℜd q(x) · x⊤Σ−1x
2
dx
=
Z
x∈ℜd q(x) ln
1
p(x)dx
=
Z
x∈ℜd q(x) ln
1
q(x)dx + dKL (q(·) ∥p(·))
(b)
≥
Z
x∈ℜd q(x) ln
1
q(x)dx
where (a) follows from the equivalence of covariances assumption and (b) follows from Lemma 10.
Lemma 12. (data processing inequality) Let X, Y, Z be random variables for which X ⊥Z|Y , then
I(X; Z) ≤I(Y ; Z), I(X; Z) ≤I(X; Y ).
I(X; Z) ≤I(X, Y ; Z)
= I(Y ; Z) + I(X; Z|Y )
(a)
= I(Y ; Z),
where (a) follows from the independence assumption. Similarly,
I(X; Z) ≤I(X; Y, Z)
= I(X; Y ) + I(X; Z|Y )
= I(X; Y ).
16

Summary
• The entropy of a discrete random variable X : Ω7→X is
H(X) =
X
x∈X
P(X = x) ln
1
P(X = x).
• The conditional entropy of a discrete random variable X : Ω7→X conditioned on another discrete
random variable Y : Ω7→Y is
H(X|Y ) =
X
x∈X,y∈Y
P(X = x, Y = y) ln
1
P(X = x|Y = y).
• The mutual information between two random variables X : Ω7→X and Y : Ω7→Y is
I(X; Y ) = dKL (P((X, Y ) ∈·)∥P(X ∈·) ⊗P(Y ∈·)) ,
where P(X ∈·) ⊗P(Y ∈·) denotes the outer product distribution. For discrete random variables,
I(X; Y ) = H(X) −H(X|Y ) = H(Y ) −H(Y |X).
• (chain rule of mutual information) For all random variables X : Ω7→X, Y : Ω7→Y, Z : Ω7→Z,
I(Y ; Z|X) + I(X; Z) = I(X, Y ; Z).
• The differential entropy of a continuous random variable X : Ω7→X with density pX (w.r.t the Lebesgue
measure µ(·)) is
h(X) =
Z
x∈X
pX(x) ln
1
pX(x)dµ(x).
• Differential entropy can be negative and is unit-dependent.
• For continuous random variables X : Ω7→X, Y : Ω7→Y,
I(X; Y ) = h(X) −h(X|Y ) = h(Y ) −h(Y |X).
17

5
Connecting Learning and Information Theory
In this section, we will leverage the requisite information-theoretic results of section 4 to derive general upper and
lower bounds for the estimation error of the Bayesian posterior (LT ). The results of this section will facilitate the
analysis of concrete problem instances in the following sections.
5.1
Error is Information
We now establish the elegant connection between LT and mutual information.
Theorem 13. (optimal error equals total information) For all T ∈Z+,
LT = I(HT ; θ)
T
.
Proof.
LT = 1
T
T −1
X
t=0
E
h
−ln ˆPt(Yt+1)
i
−H(Y1:T |θ, X0:T −1)
T
= 1
T
T −1
X
t=0
E
"
ln P(Yt+1|θ, Ht)
ˆPt(Yt+1)
#
= 1
T
T −1
X
t=0
E [dKL (P(Yt+1 ∈·|θ, Ht) ∥P(Yt+1 ∈·|Ht))]
= 1
T
T −1
X
t=0
I(Yt+1; θ|Ht)
= 1
T
T −1
X
t=0
I(Yt+1; θ|Ht) + I(Xt; θ|Ht−1, Yt)
= I(HT ; θ)
T
.
The estimation error incurred by an optimal algorithm over horizon T is exactly equal to the total information
acquired about θ from the observing the data HT . Every nat of information about θ can only be acquired via
incurring error on a prediction which relied on that information. Evidently, an optimal algorithm never makes
the same mistake twice, hence resulting in the equality between total loss incurred and total information acquired.
Results of a similar flavor appear in the global Fano’s method from the frequentist analysis of minimax lower
bounds. However, the following connections to rate-distortion theory are novel.
A natural question which may arise when inspecting Theorem 13 is: “Does LT decay to 0 in T and if so, at what
rate?”. Ostensibly, the numerator I(θ; HT ) appears to be growing in T, so it is not immediately obvious that even
an optimal learner will experience vanishing error. A simple instance to initially consider is θ which is a discrete
random variable. In such an instance, we can always provide the upper bound:
LT ≤H(θ)
T
.
Therefore, for any θ for which H(θ) < ∞, we have that LT = O(1/T).
However, what about a more realistic scenario in which θ is a continuous random variable? A naive idea would
be to simply supplant the discrete entropy H(θ) with the differential entropy h(θ). However, while differences
in conditional differential entropy equal mutual information (just as with discrete entropy), differential entropy
does not upper bound mutual information. This is because differential entropy can be negative (as discussed in
section 4.4). The appropriate extension of discrete entropy to continuous random variables can be establish via
rate-distortion theory.
18

5.2
Characterizing Error via Rate-Distortion Theory
We begin with the definition of the rate-distortion function.
Definition 14. (rate-distortion function) Let ϵ ≥0, θ : Ω7→Θ be a random variable, and ρ a distortion
function which maps θ and another random variable ˜θ to ℜ+. The rate-distortion function evaluated for random
variable θ at tolerance ϵ is defined as:
inf
˜θ∈˜Θϵ
I(θ; ˜θ)
| {z }
rate
,
where
˜Θϵ =







˜θ : E
h
ρ(θ, ˜θ)
i
|
{z
}
distortion
≤ϵ







.
Intuitively, one can think of ˜θ as the result of passing θ through a noisy channel, resulting in a lossy compression.
The objective I(θ; ˜θ), referred to as the rate, characterizes the number of nats that ˜θ retains about θ. meanwhile, the
distortion function ρ characterizes how lossy the compression is. The rate-distortion function returns the minimum
number of nats necessary to achieve distortion at most ϵ. In many ways this generalizes the concept of an ϵ-cover
in frequentist statistics. For the application of rate-distortion theory to the analysis of estimation error in machine
learning, we consider the following distortion function:
E
h
ρt(θ, ˜θ)
i
= E
h
dKL

P(Yt+1 ∈·|θ, Ht) ∥P(Yt+1 ∈·|˜θ, Ht)
i
= I(Yt+1; θ|˜θ, Ht),
where the second equality follows from the fact that ˜θ ⊥Yt+1|(θ, Ht). This restriction ensure that ˜θ does not
contain exogenous information about Yt+1 such as aleatoric noise which cannot be determined from (θ, Ht). We
use the notation Hϵ,T (θ) to denote the following rate-distortion function:
Hϵ,T (θ) =
inf
˜θ∈˜Θϵ,T
I(θ, ˜θ),
where
˜Θϵ,T =
(
˜θ : ˜θ ⊥HT |θ; I(HT ; θ|˜θ)
T
≤ϵ
)
.
The following result upper and lower bounds the optimal estimation error in terms of the rate-distortion func-
tion:
Theorem 15. (rate-distortion estimation error bounds) For all T ∈Z+,
sup
ϵ≥0
min
Hϵ,T (θ)
T
, ϵ

≤LT ≤inf
ϵ≥0
Hϵ,T (θ)
T
+ ϵ.
19

Proof. We begin with a proof of the upper bound.
LT = I(HT ; θ)
T
= 1
T
T −1
X
t=0
I(Yt+1; θ|Ht)
= 1
T
T −1
X
t=0
I(Yt+1; θ, ˜θ|Ht)
= 1
T
T −1
X
t=0
I(Yt+1; ˜θ|Ht) + I(Yt+1; θ|˜θ, Ht)
= I(HT ; ˜θ)
T
+ 1
T
T −1
X
t=0
I(Yt+1; θ|˜θ, Ht)
(a)
≤inf
ϵ≥0
inf
˜θ∈˜Θϵ,T
I(θ; ˜θ)
T
+ 1
T
T −1
X
t=0
I(Yt+1; θ|˜θ, Ht)
= inf
ϵ≥0
inf
˜θ∈˜Θϵ,T
I(θ; ˜θ)
T
+ I(HT ; θ|˜θ)
T
≤inf
ϵ≥0
Hϵ,T (θ)
T
+ ϵ
where (a) follows from the data processing inequality applied to the markov chain ˜θ ⊥HT |θ.
We now proceed with the lower bound. Suppose that I(HT ; θ) < Hϵ,T (θ). Let ˜θ = ˜HT /∈˜Θϵ,T where ˜HT is another
history sampled in the same manner as HT .
I(HT ; θ) =
T −1
X
t=0
I(Yt+1; θ|Ht)
(a)
≥
T −1
X
t=0
I(Yt+1; θ| ˜Ht, Ht)
=
T −1
X
t=0
I(Yt+1; θ|˜θ, Ht)
(b)
≥ϵT,
where (a) follows from the fact that conditioning reduces entropy and that Yt+1 ⊥˜Ht|(θ, Ht) and (b) follows from
the fact that ˜θ /∈˜Θϵ,T . Therefore, for all ϵ ≥0, I(HT ; θ) ≥min{Hϵ,T , ϵT}. The result follows.
Theorem 15 establishes the tight relation between estimation error and the rate-distortion function. The result
is very general and facilitates the analysis of concrete problem instances in supervised learning. In the following
section, we will study 3 concrete instances of increasing complexity to demonstrate how Theorem 15 facilitates
analysis.
We also note the qualitative similarity between Theorem 15 and classical results from PAC learning which all
characterize estimation error as a fraction involving a complexity function of the hypothesis space and the dataset
size (VC-dimension, Rademacher complexity, log-covering number). In our framework, the rate-distortion function
serves as the “complexity” function which characterizes the difficulty of learning θ for the purposes of prediction.
In the following section, we will use this general result to derive concrete error bounds for a suite of problems
involving data pairs which are iid when conditioned on θ.
20

Summary
• (optimal estimation error equals total information) For horizon T ∈Z+, the estimation error of
the optimal algorithm is denoted as LT and is
LT = I(θ; HT )
T
.
• For all T ∈Z+,
LT ≤H(θ)
T
.
Therefore, for a discrete random variable θ with finite entropy, LT = O(1/T).
• The extension of this result to continuous random variables can be made via rate-distortion theory.
• (rate-distortion function) Let ϵ ≥0, θ : Ω7→Θ be a random variable, and ρ a distortion function
which maps θ and another random variable ˜θ to ℜ+. The rate-distortion function evaluated for random
variable θ at tolerance ϵ is defined as:
inf
˜θ∈˜Θϵ
I(θ; ˜θ),
where
˜Θϵ =
n
˜θ : E
h
ρ(θ, ˜θ)
i
≤ϵ
o
.
• We use the notation Hϵ,T (θ) to denote the following rate-distortion function:
Hϵ,T (θ) =
inf
˜θ∈˜Θϵ,T
I(θ, ˜θ),
where
˜Θϵ,T =
(
˜θ : ˜θ ⊥HT |θ; I(HT ; θ|˜θ)
T
≤ϵ
)
.
• (rate-distortion estimation error bounds) For all T ∈Z+,
sup
ϵ≥0
min
Hϵ,T (θ)
T
, ϵ

≤LT ≤inf
ϵ≥0
Hϵ,T (θ)
T
+ ϵ.
21

6
Learning from iid Data
In this section, we restrict our attention to the analysis of learning under independently and identically distributed
(iid) data. Concretely, we assume that the random process representing the inputs: (X0, X1, . . .) is iid. Each input
Xt is associated with a label Yt+1 and we assume infinite exchangeability of the sequence ((X0, Y1), (X1, Y1), . . .).
By de Finetti’s theorem, there exists a latent random variable, which we denote by θ, for which conditioned on θ,
the above sequence is iid. We further make the standard assumption that the sequence of inputs (X0, X1, . . .) is
independent of θ. As a result, θ is a random variable which represents for all t the conditional probability measure
θ(·|Xt) of Yt+1. The process of learning involves the reduction of uncertainty about θ in ways which are relevant
for future predictions.
In the following section, we provide several results which follow as a consequence of the above iid assumption.
We dedicate the remaining sections to studying 4 concrete problem instances of increasing complexity: 1) linear
regression, 2) logistic regression, 3) deep neural networks, 4) non-parametric learning. We hope that this suite of
examples provide the reader with enough intuition and tools to analyze their own problems of interest.
6.1
Theoretical Results Tailored for iid Data
We begin with several intuitive theoretical results which follow as a result of the iid assumptions on the data. The
first result establishes monotonicity of estimation error:
Lemma 16. (monotonicity of per-timestep estimation error) If ((Xt, Yt+1 : t ∈Z+) is an iid stochastic
process when conditioned on θ, then for all t ∈Z+,
I(Yt+2; θ|Ht+1) ≤I(Yt+1; θ|Ht).
Proof. We have
I(θ; Yt+2|Ht+1) = h(Yt+2|Ht+1) −h(Yt+2|θ, Ht+1)
(a)
= h(Yt+2|Ht+1) −h(Yt+2|θ, Ht−1, Yt, Xt+1)
(b)
≤h(Yt+2|Ht−1, Yt, Xt+1) −h(Yt+2|θ, Ht−1, Yt, Xt+1)
= I(Yt+2; θ|Ht−1, Yt, Xt+1)
(c)
= I(Yt+1; θ|Ht−1, Yt, Xt)
= I(Yt+1; θ|Ht),
where (a) follows since Yt+2 ⊥(Xt, Yt+1)|(θ, Xt+1), (b) follows from the fact that conditioning reduces differential
entropy, and (c) follows from the fact that (Xt, Yt+1) and (Xt+1, Yt+2) are identically distributed conditioned on
(Ht−1, Yt).
The above result establishes that when the data generating process is iid conditioned on θ, the optimal per-
timestep estimation error is monotonically non-increasing. This is intuitive, if the data is iid, the future sequence
is exchangeable and hence, conditioning on additional information (Ht+1 as opposed to Ht) should only improve
our ability to make predictions. A corollary of this result is that for iid data, we can upper bound distortion via
an expression which is much simpler to analyze.
Corollary 17. (distortion upper bound) If ((Xt, Yt+1 : t ∈Z+) is an iid stochastic process when conditioned
on θ and for all t ∈Z+, Yt+1 ⊥˜θ|(θ, Xt), then for all T ∈Z++,
I(HT ; θ|˜θ)
T
≤I(Y1; θ|˜θ, X0).
22

Proof.
I(HT ; θ|˜θ)
T
= 1
T
T −1
X
t=0
I(Xt, Yt+1; θ|˜θ, Ht−1, Yt)
(a)
= 1
T
T −1
X
t=0
I(Yt+1; θ|˜θ, Ht)
(b)
≤I(Y1; θ|˜θ, X0)
where (a) follows from the assumption that Xt ⊥θ and (b) follows from the same proof technique as in Lemma
16.
In general, the expression I(HT ; θ|˜θ) is much more cumbersome to deal with in comparison to I(Y1; θ|˜θ, X0). Corol-
lary 17 established a suitable relationship between the two quantities. However, to facilitate the analysis of rate-
distortion lower bounds, it is fruitful to consider analysis via a modified rate-distortion function tailored for iid
data generating processes. For all ϵ ≥0, we let
Hϵ(θ) = inf
˜θ∈˜Θϵ
I(θ; ˜θ),
where
˜Θϵ = {˜θ : ˜θ ⊥H∞|θ, I(Y1; θ|˜θ, X0) ≤ϵ}.
Note that in contrast to the general rate-distortion formulation of section 5, this modified rate-distortion does not
depend on the time horizon T. We now provide an estimation error lower bound in terms of Hϵ(θ) which applies
for learning from iid data.
Lemma 18. (estimation error lower bound for iid data) If ((Xt, Yt+1 : t ∈Z+) is an iid stochastic process
when conditioned on θ, then for all T ∈Z++,
sup
ϵ≥0
min
Hϵ(θ)
T
, ϵ

≤LT .
Proof. Fix T ∈Z+. Let ˜θ = ( ˜HT −2, ˜YT −1) be independent from but distributed identically with (HT −2, YT −1)
when conditioned on θ.
Fix ϵ ≥0. If LT < Hϵ(θ)/T then
LT < Hϵ(θ)/T
=⇒
I(HT ; θ) < Hϵ(θ)
=⇒
I(˜θ; θ) < Hϵ(θ).
Since the rate of ˜θ is lower than the rate-distortion function Hϵ(θ), ˜θ /∈˜Θϵ. As a result,
LT
(a)
= I(HT ; θ)
T
(b)
= 1
T
T −1
X
t=0
I(Yt+1; θ|Ht)
(c)
≥I(YT ; θ|HT −1)
= I(YT ; θ|˜θ, XT −1)
(d)
> ϵ,
where (a) follows from Lemma 13, (b) follows from the chain rule of mutual information, (c) follows from Lemma
16, and (d) follows from the fact that ˜θ /∈˜Θϵ. Therefore,
LT ≥min{Hϵ(θ)/T, ϵ}.
Since this holds for any ϵ ≥0, the result follows.
In the following 4 sections we will apply our general results to concrete problem instances. We begin with linear
regression.
23

6.2
Linear Regression
Figure 3: We depict our linear regression data generating process above.
It consists of an input vector X of
dimension d, an unknown weight vector θ of dimension d, and a final output Y which is the sum of θ⊤X and
independent Gaussian noise Z.
6.2.1
Data Generating Process
In linear regression, the variable that we are interested in estimating is a random vector θ ∈ℜd. As our analytical
tools are developed in a Bayesian framework, we assume a known prior distribution P(θ ∈·) to model our uncertainty
over the value of θ. For simplicity and concreteness, in this example, we assume that P(θ ∈·) = N(0, Id/d). For
all t ∈Z+, inputs and outputs are generated according to a random vector Xt
iid
∼N(0, Id) and
Yt+1 = θ⊤Xt + Wt+1,
where Wt
iid
∼N(0, σ2) for known variance σ2. We note that the results and techniques developed in this section
certainly extend to input and prior distributions which are not Gaussian with slight modifications. We study the
Gaussian case as it minimizes ancillary clutter without compromising generality. In the following section, we will
establish a series of smaller results which will allow us to streamline our error analysis using the tools established
in section 5.
6.2.2
Theoretical Building Blocks
In this section we will establish 3 relatively simple results which will enable us to directly apply Theorem 15 and
arrive at estimation error bounds for linear regression.
As such, the results will all involve characterizing the
rate-distortion function for this data generating process. We begin our analysis with a result which establishes the
threshold ϵ at which the rate-distortion function is trivially 0.
Lemma 19. (linear regression 0 rate-distortion threshold) For all d ∈Z++, σ2 ∈ℜ++, if for all t ∈Z+,
(Xt, Yt+1) are generated according to the linear regression process and ϵ ≥1
2 ln(1 + 1/σ2), then for all T ∈Z++,
Hϵ,T (θ) = 0.
24

Proof. Let ˜θ = ∅. Then,
I(HT ; θ|˜θ)
T
(a)
≤I(Y1; θ|˜θ, X0)
= h(Y1|˜θ, X0) −h(Y1|θ, ˜θ, X0)
= h(Y1|X0) −h(W)
(b)
≤E
1
2 ln
 2πe
 σ2 + Var[θ⊤X|X]

−1
2 ln
 2πeσ2
(c)
= E
1
2 ln

2πe

σ2 + ∥X∥2
2
d

−1
2 ln
 2πeσ2
(d)
≤1
2 ln
 
2πe
 
σ2 + E

∥X∥2
2

d
!!
−1
2 ln
 2πeσ2
= 1
2 ln

1 + 1
σ2

≤ϵ,
where (a) follows from Corollary 17, (b) follows from Lemma 11, (c) follows from the fact that θ has covariance
matrix Id/d, and (d) follows from Jensen’s inequality. The above establishes that ˜θ ∈˜Θϵ,T for all T. The result
follows from the fact that I(θ; ˜θ) = 0.
Intuitively, there should be a threshold of ϵ for which values greater than ϵ result in a rate-distortion value of
0. Evidently, a 0-bit quantization that solely relies on the prior distribution will result in a suitable distortion
value when the threshold ϵ is large enough. This result characterizes the edge-case condition for the rate-distortion
function. The following result will provide an upper bound on the rate-distortion function for the more interesting
scenario in which the threshold ϵ < 1/2 ln(1 + 1/σ2).
Lemma 20. (linear regression rate-distortion upper bound) For all d ∈Z++, σ2 ∈ℜ++, and ϵ ∈ℜ++, if
for all t ∈Z+, (Xt, Yt+1) are generated according to the linear regression process, then for all T,
Hϵ,T (θ) ≤
d
2 ln

1
σ2(e2ϵ −1)

+
.
Proof. Fix σ2 ∈ℜ++. Let ˜θ = θ + V , where V ∼N(0, (δ2/d)Id) for δ2 =
σ2(e2ϵ−1)
1−σ2(e2ϵ−1) and V ⊥θ. Note that δ2 ≥0
25

for all 0 < ϵ < 1
2 ln
 1 +
1
σ2

. We begin by showing that ˜θ ∈˜Θϵ,T for all T.
I(HT ; θ|˜θ)
T
≤I(Y1; θ|˜θ, X0)
= h(Y1|˜θ, X0) −h(Y1|θ, ˜θ, X0)
= h(W1 + θ⊤X|˜θ, X0) −h(W1)
= h
 
W1 + θ⊤X0 −
˜θ⊤X0
1 + δ2
˜θ, X0
!
−h(W1)
= h
 
W1 +

δ2
1 + δ2 θ +
1
1 + δ2 V
⊤
X0
˜θ, X0
!
−h(W1)
≤h
 
W1 +

δ2
1 + δ2 θ +
1
1 + δ2 V
⊤
X0
X0
!
−h(W1)
(a)
≤E
1
2 ln

2πe

σ2 +

δ4
d(1 + δ2)2 +
δ2
d(1 + δ2)2

∥X0∥2
2

−1
2 ln
 2πeσ2
= E
1
2 ln

1 +
δ2∥X0∥2
2
d(1 + δ2)σ2

(b)
≤1
2 ln

1 +
δ2
(1 + δ2)σ2

= 1
2 ln
 e2ϵ
= ϵ,
where (a) follows from Lemma 11 and (b) follows from Jensen’s inequality. Next, we upper bound the rate of ˜θ:
I(θ; ˜θ) = h(˜θ) −h(˜θ|θ)
= h(˜θ) −h(V )
(a)
≤d
2 ln

2πe
δ2 + 1
d

−d
2 ln

2πe
δ2
d

= d
2 ln

1 + 1
δ2

= d
2 ln

1
σ2 (e2ϵ −1)

,
where (a) follows from Lemma 11. The result follows from Lemma 19 and the fact that ˜θ ∈˜Θϵ,T for all T ∈Z++
and ϵ < 1/2 ln(1 + 1/σ2).
Note that above, we never explicitly relied on the assumption that P(θ ∈·) = N(0, Id/d). We simply needed the
fact that the elements of θ are independent with variances which sum to 1 (for inequality (a) above). To establish
a lower bound on the rate-distortion function in the linear regression setting, we rely more on the Gaussian prior
assumption. Concrete details of the most general assumptions required to arrive at such lower bounds can be found
in Appendix A.
Lemma 21. (linear regression rate-distortion lower bound) For all d ∈Z++ s.t. d > 2, σ2 ∈ℜ++, and
ϵ ∈ℜ++, if for all t ∈Z+, (Xt, Yt+1) are generated according to the linear regression process, then
Hϵ(θ) ≥
 
d
2 ln
 
1
(8 +
d
d−2σ2)ϵ
!!
+
.
26

Proof. Fix σ2 ∈ℜ++, ϵ ∈Z+ T ∈Z++, and ˜θ ∈˜Θϵ,T . Then,
ϵ
(a)
≥I(Y ; θ|˜θ, X)
(b)
≥E

∥X∥2
2
2(4∥X∥2
2 + dσ2)

E
h
∥θ −E[θ|˜θ]∥2
2
i
= E


1
2(4∥X∥2
2+dσ2)
∥X∥2
2

E
h
∥θ −E[θ|˜θ]∥2
2
i
= E
"
1
8 +
dσ2
∥X∥2
2
#
E
h
∥θ −E[θ|˜θ]∥2
2
i
(c)
≥
1
E
h
8 +
dσ2
∥X∥2
2
iE
h
∥θ −E[θ|˜θ]∥2
2
i
(d)
=
1
8 + dσ2
d−2
E
h
∥θ −E[θ|˜θ]∥2
2
i
≥
1
8 +
d
d−2σ2 E
h
∥θ −E[θ|˜θ]∥2
2
i
where (a) follows from the fact that ˜θ ∈˜Θϵ, (b) follows from Lemma 63, (c) follows from Jensen’s inequality, and
(d) follows from the fact that E[1/χ2(d)] = 1/(d −2).
Since the above condition is an implication that holds for arbitrary ˜θ ∈˜Θϵ, minimizing the rate I(θ; ˜θ) over the
set of proxies that satisfy E[∥θ −E[θ|˜θ]∥2
2] ≤(8 +
d
d−2σ2)ϵ will provide a lower bound. However, this is simply the
rate-distortion problem for a multivariate source under squared error distortion. For this problem there exists a
well known lower bound (Theorem 10.3.3 of [Cover and Thomas, 2006]). The lower bound follows as a result.
6.2.3
Main Result
The rate-distortion bounds which we established in the previous section allow us to directly apply Theorem 15 to
arrive at a bound for estimation error.
Theorem 22. (linear regression estimation error bounds) For all d ∈Z++, σ2 ≥0, if for all t, (Xt, Yt+1)
are generated according to the linear regression processes, then for all T,
d
2T W
 
2T
d(8 +
d
d−2σ2)
!
≤LT ≤
 d
2T ln
 T
σ2d

+
+ 1
2T ln

1 + d
T

,
where W is the Lambert W function.
Proof.
LT
(a)
≤inf
ϵ≥0
Hϵ,T (θ)
T
+ ϵ
(b)
≤inf
ϵ≥0

d ln

1
σ2(e2ϵ−1)

+
2T
+ ϵ
(c)
≤

d ln

1
σ2

(1+ d
T )
T −1


+
2T
+ 1
2T ln

1 + d
T

≤
 d
2T ln
 T
σ2d

+
+ 1
2T ln

1 + d
T

,
27

where (a) follows from Theorem 15, (b) follows from Lemma 20, and (c) follows from setting ϵ =
1
2T ln
 1 + d
T

.
LT ≥sup
ϵ≥0
min
Hϵ(θ)
T
, ϵ

≥sup
ϵ≥0
min
( 
d
2T ln
 
1
(8 +
d
d−2σ2)ϵ
!!
+
, ϵ
)
= ϵ s.t.
 
d
2T ln
 
1
(8 +
d
d−2σ2)ϵ
!!
+
= ϵ
(a)
= e
−W
 
8+
d
d−2 σ2
2
!
2T
d(8+
d
d−2 σ2)
(b)
≥e−ln(8+
d
d−2 σ2)
2T
d(8+
d
d−2 σ2)
= d
2T ,
where in (a), W denotes the Lambert W function and (b) follows from the fact that for all x ≥e, W(x) ≤ln(2x).
Notably, this bound is consistent with classical results in statistics which dictate that the estimation error grows
linearly in the problem dimension d and decays linearly in the number of samples observed T. In the following
section, we will observe that qualitatively similar results also hold for logistic regression.
6.3
Logistic Regression
Figure 4: We depict our logistic regression data generating process above. It consists of an input vector X of
dimension d, an unknown weight vector θ of dimension d, and a final binary output Y ∈{0, 1}. Y is sampled
according to probabilities generated by the sigmoid function applied to θ⊤X.
6.3.1
Data Generating Process
We next study logistic regression to demonstrate the application of our tools to classification. Just as in linear
regression, in logistic regression, the variable that we are interested in estimating is a random vector θ ∈ℜd. Again
for simplicity and concreteness, we assume that P(θ ∈·) = N(0, Id/d). For all t ∈Z+, inputs and outputs are
generated according to a random vector Xt
iid
∼N(0, Id) and
Yt+1 =
(
1
w.p.
1
1+e−θ⊤Xt
0
otherwise
.
28

6.3.2
Theoretical Building Blocks
We begin with the following result which upper bounds the binary KL-divergence of a sigmoidal output via the
squared difference between the logits.
Lemma 23. (squared error upper bounds binary KL-divergence) For all x, y ∈ℜ,
1
1 + e−x ln 1 + e−y
1 + e−x +
1
1 + ex ln 1 + ey
1 + ex ≤(x −y)2
8
Proof.
This upper bound facilitates the following upper bound on the rate-distortion function.
Lemma 24. (logistic regression rate-distortion upper bound) For all d ∈Z++ and ϵ ∈ℜ++, if for all
t ∈Z+, (Xt, Yt+1) are generated according to the logistic regression process, then for all T,
Hϵ,T ≤d
2 ln

1 + 1
8ϵ

.
Proof. Let ˜θ = θ + Z where Z ⊥θ and Z ∼N(0, 8ϵ/d). Then,
I(HT ; θ|˜θ)
T
≤I(Y1; θ|˜θ, X0)
= E
h
dKL

P(Y1 ∈·|θ, X0) ∥P(Y1 ∈·|˜θ, X0)
i
(a)
≤E
h
dKL

P(Y1 ∈·|θ, X0) ∥P(Y1 ∈·|θ ←˜θ, X0)
i
= E


ln

1+e−˜
θ⊤X0
1+e−θ⊤X0

1 + e−θ⊤X0
+
ln

1+e
˜
θ⊤X0
1+eθ⊤X0

1 + eθ⊤X0


(b)
≤
E

θ⊤X0 −˜θ⊤X0
2
8
=
E
h
∥θ −˜θ∥2
2
i
8
= ϵ,
where (a) follows from Lemma 57 and (b) follows from Lemma 23.
We now upper bound the rate.
I(θ; ˜θ) = h(˜θ) −h(˜θ|θ)
(a)
≤d
2 ln

2πe
1
d + 8ϵ
d

−d
2 ln

2πe8ϵ
d

= d
2 ln

1 + 1
8ϵ

,
where (a) follows from Lemma 11. The result follows.
6.3.3
Main Result
With the rate-distortion upper bound in place, we establish the following upper bound on estimation error.
Theorem 25. (logistic regression estimation error upper bound) For all d ∈Z++, if for all t, (Xt, Yt+1)
is generated according to the logistic regression process, then for all T,
LT ≤
d
2T

1 + ln

1 + T
4d

.
29

Proof.
LT
(a)
≤inf
ϵ≥0
Hϵ,T (θ)
T
+ ϵ
(b)
≤inf
ϵ≥0
d
2 ln
 1 + d
8ϵ

T
+ ϵ
(c)
≤
d
2T ln

1 + T
4d

+ d
2T ,
where (a) follows from Theorem 15, (b) follows from Lemma 24, and (c) follows from setting ϵ =
d
2T .
We observe that just as in linear regression, we observe an error bound which is ˜O(d/T). In the following section,
we consider a much more complex deep neural network data generating process.
6.4
Deep Neural Networks
Figure 5: We depict our deep neural network data generating process above. It consists of input dimension d,
width N, depth L, with output dimension 1, and ReLU activation units. We denote the weights at layer ℓby A(ℓ)
and the output of layer ℓby U (ℓ). We assume that the final output Y is the sum of the final network output U (L)
and independent Gaussian noise Z.
6.4.1
Data Generating Process
For a deep neural network, we are interested in estimating a collection of random matrices (A(1), . . . , A(L)) which
represent the weights of the network. We assume a known prior distribution P(A(1:L) ∈·) to model our uncertainty
over the network weights. For simplicity, we assume that the width of every hidden layer is identically set to a
positive integer N and that the network has input dimension d and output dimension 1. We further assume the
following prior distribution on the weights of the network.
P(A(ℓ)) =





N(0, IN×d
d
)
if ℓ= 1
N(0, IN×N
N
)
if 2 ≤ℓ≤L −1
N(0, IN
N )
if ℓ= L
.
We further make the assumption that the weights across layers are independent. These Gaussian assumptions are
again not necessary for the following results (only the specified covariance structure is required), but we provide
the above instance for concreteness.
30

For all t ∈Z+, inputs and outputs are generated according to a random vector Xt
iid
∼N(0, Id) and
U (0)
t
= Xt
U (ℓ+1)
t
= ReLU

A(ℓ+1)U (ℓ)
t

Yt+1 = U (L)
t
+ Wt+1,
where Wt
iid
∼N(0, σ2) for known variance σ2.
6.4.2
Theoretical Building Blocks
In this section, we will cover several helpful lemmas which will allow us to streamline the error analysis for neural
networks. Lemma 29 establishes an upper bound on the distortion function in the deep neural network setting.
This result allows us to easily derive upper bounds for the rate-distortion function in Theorem 30. However, we
begin with the following three Lemmas (26, 27, 28) which facilitate the derivation of distortion upper bound in
Lemma 29. We begin with an initial result which decomposes the total distortion I(Y ; A(1:L)| ˜A(1:L), X) into a sum
of simpler terms.
Lemma 26. (distortion decomposition) If (X, Y ) are generated by the deep neural network process and
˜A1, . . . , ˜AL are independent random variables such that, Y ⊥˜A(1:L)|A(1:L), X, then
I(Y ; A(1:L)| ˜A(1:L), X) =
L
X
ℓ=1
I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(1:ℓ), X).
Proof.
I(Y ; A(1:L)| ˜A(1:L), X)
(a)
=
L
X
ℓ=1
I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(1:L), X)
(a)
=
L
X
ℓ=1
I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(1:ℓ), X),
where (a) follows from the chain rule and (b) follows from conditional independence assumptions.
With this decomposition in mind, we can derive a suitable upper bound for the total distortion by deriving an
upper bound for each individual term of the decomposition. The following two results establish such an upper
bound for the individual terms.
Lemma 27. (more information with the true input) If (X, Y ) are generated by the deep neural network process
and ˜A1, . . . , ˜AL are independent random variables such that, Y ⊥˜A(1:L)|A(1:L), X, then for all ℓ∈{1, . . . , L},
I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(1:ℓ), X) ≤I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(ℓ), U (ℓ−1)).
Proof.
I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(1:ℓ), X) = I(Y, X, ˜A(1:ℓ−1); A(ℓ)|A(ℓ+1:L), ˜A(ℓ)) −I(X, ˜A(1:ℓ−1); A(ℓ)|A(ℓ+1:L), ˜A(ℓ))
= I(Y, X, ˜A(ℓ−1:1); A(ℓ)|A(ℓ+1:L), ˜A(ℓ))
= I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(ℓ)) + I(X, ˜A(1:ℓ−1); A(ℓ)|Y, A(ℓ+1:L), ˜A(ℓ))
(a)
≤I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(ℓ)) + I(X, A(1:ℓ−1); A(ℓ)|Y, A(ℓ+1:L), ˜A(ℓ))
= I(Y, X, A(1:ℓ−1); A(ℓ)|A(ℓ+1:L), ˜A(ℓ))
(b)
= I(Y ; A(ℓ)|X, A(1:ℓ−1), A(ℓ+1:L), ˜A(ℓ))
(c)
= I(Y ; A(ℓ)|U (ℓ−1), A(ℓ+1:L), ˜A(ℓ)),
where (a) follows from the fact that A(ℓ) ⊥˜A(1:ℓ−1)|(X, Y, A(1:ℓ−1)) and the data processing inequality, (b) follows
from the fact that I(A(ℓ); A(1:ℓ−1), X|A(ℓ+1:L), ˜A(ℓ)) = 0, and (c) follows from the fact that Y ⊥(A(1:ℓ−1), X)|U (ℓ−1).
31

This result states that more information is extracted about A(ℓ) when we condition on the true input to layer ℓ
(U (ℓ−1)) as opposed to the approximate input based on (A(1:ℓ−1), X). This is a rather intuitive result as having
access to the true input U (ℓ−1) which generated label Y should allow us to extract more information about the
parameters A(ℓ). This allows us to simplify our analysis since the RHS of Lemma 27 only consists of one approximate
term ˜A(ℓ) as opposed to ℓof them in the LHS. The following result leverages this simplified form to derive an upper
bound for each individual term in the decomposition of Lemma 26.
Lemma 28. For all d, N, L ∈Z++, σ2 ∈ℜ++, if (X, Y ) are generated according to the deep neural network process
and ˜A1, . . . , ˜AL are independent random variables such that for all Y ⊥˜A(1:L)|(A(1:L), X), then for all ℓ∈[L],
I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(1:ℓ), X) ≤1
2 ln



1 +
E
(A(ℓ) −˜A(ℓ))U (ℓ−1)
2
2

σ2N



.
Proof. In the proof below, we use the notation fA(ℓ:L) to denote the depth L + 1 −ℓMLP with ReLU activation
units and weights parameterized by A(ℓ:L).
I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(1:ℓ), X)
(a)
≤I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(ℓ), U (ℓ−1))
= h(Y |A(ℓ+1:L), ˜A(ℓ), U (ℓ−1)) −h(Y |A(ℓ+1:L), U (ℓ−1))
= h(Y |A(ℓ+1:L), ˜A(ℓ), U (ℓ−1)) −1
2 ln
 2πeσ2
(b)
≤E


1
2 ln




E

Y −f ˜
A(ℓ),A(ℓ+1:L)
 U (ℓ−1)2 A(ℓ+1:L), ˜A(ℓ), U (ℓ−1)

σ2






≤E

1
2 ln


1 +

fA(ℓ:L)
 U (ℓ−1)
−f ˜
A(ℓ),A(ℓ+1:L)
 U (ℓ−1)2
σ2





(c)
≤E

1
2 ln


1 +

A(L)A(L−1) . . . A(ℓ+1)(A(ℓ) −˜A(ℓ))U (ℓ−1)2
σ2





(d)
≤1
2 ln



1 +
E

A(L)A(L−1) . . . A(ℓ+1)(A(ℓ) −˜A(ℓ))U (ℓ−1)2
σ2




(e)
= 1
2 ln



1 +
E
(A(ℓ) −˜A(ℓ))U (ℓ−1)
2
2

σ2N




where (a) follows from Lemma 27, (b) follows from Lemma 11, (c) comes from the fact that for all n and x, y ∈ℜn,
∥x −y∥2 ≥∥ReLU(x) −ReLU(y)∥2
2, (d) follows from Jensen’s inequality, and (e) follows from the independence
and variance assumptions of the deep neural network data generating process.
With this upper bound in place, it becomes trivial to derive an upper bound for the total distortion. We present
this result now whose proof follows as a direct application of the above established lemmas.
Lemma 29. (distortion upper bound) For all d, N, L ∈Z++, σ2 ∈ℜ++, if (X, Y ) are generated accord-
ing to the deep neural network process and ˜A1, . . . , ˜AL are independent random variables such that for all Y ⊥
32

˜A(1:L)|(A(1:L), X), then
I(Y ; A(1:L)| ˜A(1:L), X) ≤
L
X
ℓ=1
1
2 ln



1 +
E


A(ℓ) −˜A(ℓ)
U (ℓ−1)
2
2

σ2N



.
Proof.
I(Y ; A(1:L)| ˜A(1:L), X)
(a)
=
L
X
ℓ=1
I(Y ; A(ℓ)|A(ℓ+1:L), ˜A(1:ℓ), X)
(b)
≤
L
X
ℓ=1
1
2 ln



1 +
E


A(ℓ) −˜A(ℓ)
U (ℓ−1)
2
2

σ2N



,
where (a) follows from Lemma 26 and (b) follows from Lemma 28.
We note that remarkably, the error in each term of the sum in the RHS does not depend on L. This is a clear
improvement upon the results based on VC dimension [Bartlett et al., 1998, 2019] for which the term within the
log would contain the product of the operator norms of the matrices A(ℓ+1:L). The average-case framework allows
us to not incur such a penalty since for all ℓ, E[A(ℓ)⊤A(ℓ)] ∝I. Therefore, the resulting rate-distortion bound will
depend only linearly on the parameter count of the network (as opposed to linearly in the product of parameter
count and depth). With an upper bound on the total distortion in place, we can easily derive an upper bound for
the rate-distortion function for the deep neural network data generating process.
Theorem 30. (deep neural network rate-distortion upper bound) For all d, N, L ∈Z++, σ2, ϵ ∈ℜ++, if
(X, Y ) are generated according to the deep neural network process, then
Hϵ(A(1:L)) ≤
(L −2)N 2 + N + dN
2

ln

1 +
1
σ2

e
2ϵ
L −1


.
Proof. For all ℓ∈[L], let ˜A(ℓ) = A(ℓ) + Z(ℓ) where Z(ℓ) ⊥A(ℓ) and each element of Z(ℓ) is
iid
∼N(0, δ2), where
δ2 = σ2(e2ϵ/L −1)/d(ℓ−1) and d(ℓ−1) denotes the dimension of U (ℓ−1). Then,
I(Y ; A(1:L)| ˜A(1:L), X) ≤
L
X
ℓ=1
1
2 ln



1 +
E


A(ℓ) −˜A(ℓ)
U (ℓ−1)
2
2

σ2N




=
L
X
ℓ=1
1
2 ln

1 +
E
hZ(ℓ)U (ℓ−1)2
2
i
σ2N


=
L
X
ℓ=1
1
2 ln
 
1 + σ2N(e
2ϵ
L −1)E

∥U (ℓ−1)∥2
2

σ2Nd(ℓ−1)
!
= ϵ,
33

where (a) follows from Lemma 29, and (b) follows from the fact that E[Z(ℓ)⊤Z(ℓ)] = σ2N(e2ϵ/L−1)
d(ℓ−1)
Id(ℓ−1).
I(A(1:L); ˜A(1:L)) =
L
X
ℓ=1
I(A(ℓ); ˜A(ℓ))
=
L
X
ℓ=1
h( ˜A(ℓ)) −h( ˜A(ℓ)|A(ℓ))
(a)
≤
L
X
ℓ=1
d(ℓ−1)d(ℓ)
2
ln

2πe

δ2 +
1
d(ℓ−1)

−d(ℓ−1)d(ℓ)
2
ln
 2πeδ2
=
L
X
ℓ=1
d(ℓ−1)d(ℓ)
2
ln

1 +
1
δ2d(ℓ−1)

=
L
X
ℓ=1
d(ℓ−1)d(ℓ)
2
ln

1 +
1
σ2(e
2ϵ
L −1)

=
(L −2)N 2 + N + dN
2

ln

1 +
1
σ2

e
2ϵ
L −1


,
where (a) follows from Lemma 11. The result follows from the definiton of the rate-distortion function.
Note that the bound in Theorem 30 is only linear in the parameter count of the network. In a setting in which
we assume an independent prior on the weights of the network, such a result is the best that one could expect. In
the following subsection, we will leverage this rate-distortion upper bound and Theorem 15 to arrive at an upper
bound for the estimation error for the deep neural network setting.
6.4.3
Main Result
With the theoretical tools developed in the previous section, we now establish the main result, which upper bounds
the estimation error of an optimal agent learning from data generated by the deep neural network process.
Theorem 31. (deep neural network estimation error upper bound) For all d, N, L ∈Z++, σ2 ∈ℜ++, if
for all t, (Xt, Yt+1) are generated according to the deep neural network process, then for all T,
LT ≤
P
2T

1 + ln

1 + 2LT
σ2P

,
where P = (L −2)N 2 + N + dN denotes the total parameter count of the network.
Proof.
LT
(a)
≤inf
ϵ≥0
Hϵ(A(1:L))
T
+ ϵ
(b)
≤inf
ϵ≥0
P ln
 
1 +
1
σ2

e
2ϵ
L −1

!
2T
+ ϵ
(c)
≤
P

1 + ln

1 +
1
σ2

e
P
2LT −1





2T
(d)
≤P
2T

1 + ln

1 + 2LT
σ2P

,
34

where (a) follows from Theorem 15, (b) follows from Theorem 30, (c) follows by setting ϵ = P/2T, and (d) follows
from the fact that for all x ∈ℜ+,
ln

1 +
1
σ2(ex −1)

≤ln

1 +
1
σ2x

.
Notably, Theorem 31 establishes an upper bound which is only linear in the total parameter count of the network
(O(P)). This notably improves upon existing results from the frequentist line of analysis [Bartlett et al., 1998,
Harvey et al., 2017] which derive an upper bound which is ˜O(PL). As mentioned in the previous section, we
are able to arrive at these stronger results by leveraging an expectation with respect to the prior distribution as
opposed to a worst-case assumption over the hypotheses in a set.
Since we observe empirically that deep neural networks are able to effectively learn even in the presence of limited
data, our error analysis in the Bayesian framework provides results which are closer to qualitative observations of
empirical studies. However, the results of this section are not sufficient to explain how learning may be possible
when the dataset size is smaller than the parameter count of the model which generated the data. Such results will
require stronger assumptions surrounding the dependence between weights in the neural network. In the following
section, we explore this phenomenon in a nonparametric setting in which the neural network which generated the
data may consist of infinitely many parameters, but good performance can be obtained with relatively modest
amounts of data.
6.5
Nonparametric Learning
Figure 6: We depict our nonparametric data generating process above. It consists of input dimension d, an infinite
number of hidden units with ReLU activations, and output dimension 1. We denote the weights of the first layer via
an infinite dimensional matrix A and the weights of the output layer by an infinite dimensional vector θ. To enforce
structure which enables learning, we assume an appropriate prior distribution which results in a concentration of
the weights of θ as depicted by the solid (as opposed to dotted) lines in the above diagram. We assume that the
final output Y is the sum of the final network output and independent Gaussian noise Z.
In this section, we study a nonparameteric data generating process which can be parameterized by a two-layer
neural network of infinite width. While the results we present in this section are limited to a two-layer example,
the techniques derived in this section are general enough to be applied to appropriate instances of nonparameteric
deep neural networks as well.
6.5.1
Data Generating Process
For a nonparametric neural network, we are interested in estimating a function which can be uniquely identified
by an infinite-dimensional matrix A ∈ℜ∞×d which represents the first-layer weights, and an infinite-dimensional
vector θ ∈ℜ∞which denotes the output-layer weights. As in prior examples, we assume a known prior distribution
35

P((A, θ) ∈·). The neural network has input dimension d and output dimension 1. In this analysis, we restrict our
attention to a particular prior distribution on the weights of the network. This prior distribution is describe by a
Dirichlet process which detail next.
A Dirichlet process is a stochastic process whose realizations are probability mass functions over a countable
(often infinite) support. The Dirichlet process takes as input a scale parameter K, and a base distribution P. Its
realizations are hence probability mass functions with support on a countable subset of the set defined by the base
distribution P. In our problem, we will take this base distribution to be Unif(Sd−1), the uniform distribution over
the unit sphere of dimension d. As a result, realizations of the Dirichlet process will be probability mass functions
over a countable subset of Sd−1.
As such, a realization of a Dirichlet process exhibits a parameterization via a vector in ¯θ ∈ℜ∞which denotes
the frequencies of each outcome along with the collection of vectors in An ∈Sd−1 for n ∈Z++, which comprise
of the support. With this parameterization, we can construct the following data generating process. For all t, let
Xt
iid
∼N(0, Id) and
Yt+1 = Wt+1 +
√
K ·
∞
X
n=1
θnReLU(A⊤
n Xt),
θn =
(¯θn
w.p. 0.5
−¯θn
w.p. 0.5 ,
where ¯θn denotes the frequency associated with the outcome denoted by An and for all t, Wt
iid
∼N(0, σ2) denotes
independent additive Gaussian noise of known variance σ2.
We now speak about the scale parameter K. Evidently, ¯θ, though in ℜ∞, is limited in complexity by the fact that for
all n, ¯θn ≥0 and P∞
n=1 ¯θn = 1. The scale parameter K induces additional structure in the form of concentration.
The Dirichlet process inherently assumes a degree of concentration from the fact that its realizations are in a
countable subset of Sd−1. However, the scale parameter induces further concentration to create a sparsity-like
structure in the outcomes. Without such structure, effective learning in the presence of finite compute and memory
may be infeasible. Therefore, despite the fact that this neural network is parameterized by an infinite number of
parameters, the structure induced by Dirichlet prior and finite scale parameter K will allow us to reason about the
achievable performance of an optimal learning algorithm.
6.5.2
Theoretical Building Blocks
In prior sections, we often derived the appropriate rate-distortion result by simply devising a compression which
adds independent Gaussian noise to the parameters. This will no longer suffice as the parameters of interest are
infinite-dimensional and would hence result in an infinite rate. We introduce a proof technique derived from early
results by Barron [1993] in his seminal work on the approximation rates of neural networks. The key insight is that
if one samples m times from the categorical distribution induced by ¯θ, A and simply construct a function which
averages the observed outcomes, the approximation error (in our case, distortion) would decay linearly in m. The
following theoretical result concretely establishes this idea.
Lemma 32. (approximation via multinomial) For all K, m ∈Z++, let (A, ¯θ) ∼DP(K, Unif(Sd−1)) and for
all n ∈Z++,
θn =
(¯θn
w.p. 0.5
−¯θn
w.p. 0.5 .
If for all i ∈[m], ci
iid
∼Categorical(¯θ) and ˜Ai = Aci, then
E








√
K ·
∞
X
n=1
θnReLU(A⊤
n Xt)
|
{z
}
true function
−
√
K
m
m
X
i=1
sign(θci) · ReLU( ˜A⊤
i Xt)
|
{z
}
approximation






2

≤K
m.
36

Proof.
E


 
√
K ·
∞
X
n=1
θnReLU(A⊤
n Xt) −
√
K
m
m
X
i=1
sign(θci) · ReLU( ˜A⊤
i Xt)
!2

(a)
= E


 √
K
m
m
X
i=1
sign(θci) · ReLU( ˜A⊤
i Xt)
!2
−
 
√
K ·
∞
X
n=1
θnReLU(A⊤
n Xt)
!2

≤E


 √
K
m
m
X
i=1
sign(θci) · ReLU( ˜A⊤
i Xt)
!2

(b)
= K
m2
m
X
i=1
E
h
(ReLU( ˜A⊤
i Xt))2i
≤K
m2
m
X
i=1
E
h
( ˜A⊤
i Xt)2i
= K
m,
where (a) follows from the fact that the two expressions in the difference have the same expectation and (b) follows
from the fact that the ci’s are independent and E[sign(θci)] = 0.
This result establishes that despite the fact that this data generating process ostensibly has infinite complexity,
there exist good approximations which only require finite information about the data generating process. The
above result will allow us to establish bounds on the distortion of a suitable compression. The following result will
allow us to establish favorable bounds on the rate of the compression.
Lemma 33. (Dirichlet-multinomial concentration) For all K, m ∈Z++, let (A, ¯θ) ∼DP(K, Unif(Sd−1)). If
for all i ∈[m], ci
iid
∼Categorical(¯θ) and Nm denotes the random variable which represents the number of unique
categories draw in (c1, . . . , cm), then
E [Nm] ≤K ln

1 + n
K

.
Proof. As the proof of this result requires significant mathematical machinery, we refer the reader to Appendix B
for the result.
Lemma 33 establishes that the approximation studied above also has favorable informational complexity. Suppose
we construct a compression of (θ, A) as in the statement of Lemma 32, however, instead of setting ˜Ai = Aci, we
instead use a quantization of Sd−1. Let Aϵ denote an ϵ-cover of Sd−1 with respect to ∥· ∥2
2. Furthermore, let
¯Ai,ϵ = arg min
a∈Aϵ
∥Ai −a∥2
2.
Suppose we construct a compression for which ˜Ai = ¯Aci,ϵ.
Then, the collection ( ˜A1, . . . , ˜Am) will have finite
entropy since it consists of a finite collection of discrete random variables defined on the finite set Aϵ. However, a
naive calculation of the entropy will result in a sub-optimal bound on the rate. We arrive at a tighter bound by
considering a loss-less compression of ( ˜A1, . . . , ˜Am) and leveraging the insight of Theorem 3 that entropy is upper
bounded by the average length of an optimal prefix-free code.
Theorem 34. (Dirichlet process rate-distortion upper bound) For all d, K ∈Z++ and σ2, ϵ ∈ℜ++, if
(X, Y ) are generated according to the Dirichlet neural network process with parameters θ, A, then
Hϵ(θ, A) ≤K ln

1 +
2
σ2ϵ

ln
2K
σ2ϵ

+ 2dK ln

1 +
2
σ2ϵ

ln

1 + 4
ϵ

.
Proof. Suppose we set ϵ′ = mσ2(e2K/(mσ2) −1)/2K −1 ≥0 and m = ⌈K
σ2ϵ⌉. For all i ∈[m], let ci
iid
∼Categorical(¯θ).
Let Aϵ denote an ϵ′-cover of Sd−1 with respect to ∥· ∥2
2 and for all i ∈Z++, let
¯Ai,ϵ = arg min
a∈Aϵ
∥Ai −a∥2
2.
37

Recall that Nm denotes the number of unique outcomes in (c1, . . . , cm). Let I = (I1, . . . , INm) denote an ordered
set which consists of the unique outcomes. Let (˜θ, ˜A) = (˜θ1, ˜A1, . . . , ˜θNm, ˜ANm) be a collection of random variable
such that
˜Ai = ¯AIi,ϵ;
˜θi =
m
X
j=1
sign(θIi) · 1[cj = Ii].
Therefore, ˜θ consists of Nm integers in ±[m] and ˜A consists of Nm outcomes from the set Aϵ. We begin by upper
bounding the distortion of (˜θ, ˜A).
I(Y ; θ, A|˜θ, ˜A, X) = h(Y |˜θ, ˜A, X) −h(Y |θ, A)
= h
 
Y −
√
K
m
X
i∈I
˜θi · ReLU( ˜A⊤
i X)
˜θ, ˜A
!
−h(W)
(a)
≤E

1
2 ln




Y −
√
K
m
P
i∈I ˜θi · ReLU( ˜A⊤
i X)
2
σ2





(b)
≤1
2 ln




E

Y −
√
K
m
P
i∈I ˜θi · ReLU( ˜A⊤
i X)
2
σ2




= 1
2 ln


1 +
√
K · P∞
n=1 θnReLU(A⊤
n X) −
√
K
m
Pm
i=1 sign(θci) · ReLU( ˜A⊤
i X)
2
σ2



(c)
≤1
2 ln



1 +
2K
m + 2K
m2 E
Pm
i=1 sign(θci)ReLU(A⊤
i X) −Pm
i=1 sign(θci) · ReLU( ˜A⊤
i X)
2
σ2




= 1
2 ln

1 +
2K
m + 2K
m2
Pm
i=1 E
h
(ReLU(A⊤
i X) −ReLU( ˜A⊤
i X))2i
σ2


(d)
≤1
2 ln

1 +
2K
m + 2K
m E
h
(A⊤
i X −˜A⊤
i X)2i
σ2


(e)
≤1
2 ln

1 + 2K(1 + ϵ′)
σ2m

=
K
σ2m
≤ϵ,
where (a) follows from Theorem 11 and the fact that conditioning reduces differential entropy, (b) follows from
Jensen’s inequality, (c) follows from Lemma 32 and the fact that for all x, y, z ∈ℜ, (x−y)2 ≤2(x−z)2 +2(z −y)2,
(d) follows from the fact that for all x, y ∈ℜ, (x−y)2 ≥(ReLU(x) −ReLU(y))2, and (e) follows from the fact that
˜Ai is the closest element in an ϵ′ cover of Sd−1.
38

We now upper bound the rate of (˜θ, ˜A)
I(θ, A; ˜θ, ˜A) ≤H(˜θ, ˜A)
(a)
≤E [Nm · (ln(2m) + ln(|Aϵ′|))]
(b)
≤K ln

1 + m
K
 
ln 2m + d ln 3
ϵ′2

(c)
≤K ln

1 +
2
σ2ϵ
  
ln 2K
σ2ϵ + 2d ln
 
ϵ
√
3
eϵ −1 −ϵ
!!
(d)
≤K ln

1 +
2
σ2ϵ

ln
2K
σ2ϵ

+ 2dK ln

1 +
2
σ2ϵ

ln

1 + 4
ϵ

,
where (a) follows from Theorem 3 and the fact that a realization (˜θ, ˜A) can be mapped to a codeword of length
Nm(ln(2m) + ln(|Aϵ′|)) by using ln(2m) nats to encode each of ˜θ1, . . . , ˜θNm, and ln(|Aϵ′|) to encode each of
˜A1, . . . , ˜ANm, (b) follows from Lemma 33 and the fact that |Aϵ′| ≤(3/ϵ′2)d, (c) follows by upper bounding
the quantity via m = 2K/(σ2ϵ) as opposed to ⌈K/(σ2ϵ)⌉, and (d) follows from the fact that for all ϵ ≥0,
ln(ϵ
√
3/(eϵ −1 −ϵ)) ≤ln(1 + 4/ϵ).
6.5.3
Main Result
With the theoretical tools derived in the previous section, we now establish the following upper bound on the
estimation error of an optimal agent learning from data generated by the Dirichlet process.
Theorem 35. (Dirichlet process estimation error upper bound) For all d, K ∈Z++, if for all t, (Xt, Yt+1)
are generated according to the Dirichlet neural network process, then for all T,
LT ≤K
T ln

1 + T
σ2d

ln
 T
σ2d

+ 2dK
T

1 + ln

1 + T
σ2d

ln

1 + T
dK

.
Proof. The result follows from Theorems 15 and 34 and setting ϵ = 2dk/T.
Notably, this result is ˜O(dK/T) despite the fact that the Dirichlet neural network process is described by a neural
network with infinite width. The scale parameter K determines the degree of concentration which occurs in the
output-layer weights of the network and hence controls the difficult of learning. This example of learning under
data generated by a nonparametric process further demonstrates the flexibility and ingenuity of proof techniques
which are encompassed in our framework. We hope that the suite of examples provided in this section enables the
reader to analyze whichever complex processes they may have in mind.
Summary
• (monotonicity of per-timestep estimation error) If for all s ∈Z+, (Xs, Ys+1) is sampled iid from
some distribution P(·|θ), then for all t ∈Z+,
I(Yt+2; θ|Ht+1) ≤I(Yt+1; θ|Ht).
• For all ϵ ≥0, we let
Hϵ(θ) = inf
˜θ∈˜Θϵ
I(θ; ˜θ),
where
˜Θϵ = {˜θ : ˜θ ⊥H∞|θ, I(Y1; θ|˜θ, X0) ≤ϵ}.
Note that in contrast to the general rate-distortion formulation of section 5, this modified rate-distortion
does not depend on the time horizon T.
39

• (linear regression estimation error bounds) For all input dimensions d ∈Z++ and noise variance
σ2 ≥0, if for all t, (Xt, Yt+1) are generated according to the linear regression processes, then for all T,
d
2T W
 
2T
d(8 +
d
d−2σ2)
!
≤LT ≤
 d
2T ln
 T
σ2d

+
+ 1
2T ln

1 + d
T

,
where W is the Lambert W function.
• (logistic regression estimation error upper bound) For all input dimensions d ∈Z++, if for all t,
(Xt, Yt+1) is generated according to the logistic regression process, then for all T,
LT ≤
d
2T

1 + ln

1 + T
4d

.
• (deep neural network estimation error upper bound) For all input dimensions d, widths N, and
depths L each ∈Z++, and noise variances σ2 ∈ℜ++, if for all t, (Xt, Yt+1) are generated according to
the deep neural network process, then for all T,
LT ≤
P
2T

1 + ln

1 + 2LT
σ2P

,
where P = (L −2)N 2 + N + dN denotes the total parameter count of the network.
• (Dirichlet process estimation error upper bound) For all input dimensions d, scale parameters
K, each ∈Z++, if for all t, (Xt, Yt+1) are generated according to the Dirichlet neural network process,
then for all T,
LT ≤K
T ln

1 + T
σ2d

ln
 T
σ2d

+ 2dK
T

1 + ln

1 + T
σ2d

ln

1 + T
dK

.
40

7
Learning from Sequences
In the previous section, we focused on the special case of learning from an (infintely) exchangeable sequence of
data (iid when conditioned on θ). However, in general, machine learning systems will have to reason about data
which does not obey this rigid structure. For instance, suppose that X0, X1, X2, . . . describes a sequence of text
tokens from a book. It’s clear that such a sequence would not exhibit exchangeability as the order of the tokens
plays a critical role in deriving meaning. Existing frameworks for analyzing machine learning can only reason about
learning from sequential data under rigid and contrived notions of mixing time for the data generating process.
However, in our framework, Theorem 15 does not make any explicit assumptions on the structure of the data.
In this section, we will demonstrate that Theorem 15 seamlessly extends to the analysis of learning from data
generated by an auto-regressive process.
7.1
Data Generating Process
Let X0, X1, X2, be a sequence of random variables representing observations. We assume that this sequence is
generated by an autoregressive model which is parameterized by a random variable θ. As a result, for all t, Xt+1 is
drawn according to a probability distribution which depends on θ and the history Ht = (X0, X1, . . . , Xt). In this
section, we will analyze learning under two concrete autoregressive data generating processes. The first involves a
binary AR(K) process and the second a transformer model with context length K.
7.2
Binary AR(K) Process
7.2.1
Data Generating Process
We begin with a simple AR(K) process over a binary alphabet. We aim to demonstrate that our framework can
easily adapt to machine learning from sequential data. In this problem, we are interested in estimating random
vectors θ1, . . . , θK ∈ℜd from data which is generated in the following way. Let (X0, X1 . . . , XK−1) ∼Unif({0, 1}K).
Furthermore, consider known vectors Φ0, Φ1 ∈ℜd of L2 norm equal to 1 which correspond to vector embedding of
the binary outcomes 0 and 1 respectively. For brevity of notation, we use ϕt to denote ΦXt.
For all t ≥K, let
Xt+1 =
(
1
w.p. σ
PK
k=1 θ⊤
k ϕt−k+1

0
otherwise
,
where σ denotes the sigmoid function. For all k, we assume the independent prior distributions P(θk) = N(0, Id/(K)).
7.2.2
Preliminary Theoretical Results
As in all prior examples, our strategy is to leverage rate-distortion theory to arrive at an estimation error upper
bound. We begin with the following result which upper bounds the rate-distortion function in the binary AR(K)
problem setting.
Lemma 36. (binary AR(K) rate-distortion upper bound) For all d, K ∈Z++ and ϵ ∈ℜ++, if for all
t ∈Z+, Xt is generated according to the binary AR(K) process, then for all T,
Hϵ,T (θ1:K) ≤dK
2 ln

1 + 1
8ϵ

.
Proof. For k ∈{0, 1, . . . , K −1}, let ˜θk = θk + Zk where Zk ⊥θk and Zk ∼N(0, Id · 8ϵ/K). For all t, let Ht denote
41

(X0, X1, . . . , Xt). Then,
I(HT ; θ1:K|˜θ1:K)
T
= 1
T
T −1
X
t=0
I(Xt+1; θ1:K|˜θ1:K, Ht)
≤1
T
T −1
X
t=0
I(Xt+1; θ1:K|˜θ1:K, Xt:t−K+1)
≤1
T
T −1
X
t=0
E
h
dKL

P(Xt+1 ∈·|θ1:K, Xt:t−K+1) ∥P(Xt+1 ∈·|˜θ1:K, Xt:t−K+1)
i
(a)
≤1
T
T −1
X
t=0
E
h
dKL

P(Xt+1 ∈·|θ1:K, Xt:t−K+1) ∥P(Xt+1 ∈·|θ1:K ←˜θ1:K, Xt:t−K+1)
i
= 1
T
T −1
X
t=0
E


ln

1+e−PK
k=1 θ⊤
k ϕt−k+1
1+e−PK
k=1 θ⊤
k ϕt−k+1

1 + e−PK
k=1 θ⊤
k ϕt−k+1
+
ln

1+e
PK
k=1 θ⊤
k ϕt−k+1
1+e
PK
k=1 θ⊤
k ϕt−k+1

1 + e
PK
k=1 θ⊤
k ϕt−k+1


(b)
≤1
T
T −1
X
t=0
E
PK
k=1 θ⊤
k ϕt−k+1 −˜θ⊤
k ϕt−k+1
2
8
= 1
T
T −1
X
t=0
E
hPK
k=1 ϕ⊤
t:t−K+1(θk −˜θk)(θk −˜θk)⊤ϕt:t−K+1
i
8
=
ϵ · E
hPK
k=1 ϕ⊤
t−kϕt−k+1
i
K
= ϵ,
where (a) follows from Lemma 57 and (b) follows from Lemma 23.
We now upper bound the rate.
I(θ1:K; ˜θ1:K) = h(˜θ1:K) −h(˜θ1:K|θ1:K)
(a)
≤dK
2 ln

2πe
 1
K + 8ϵ
K

−dK
2 ln

2πe8ϵ
K

= dK
2 ln

1 + 1
8ϵ

,
where (a) follows from Lemma 11. The result follows.
7.2.3
Main Result
We now present the main result of this section which upper bounds the estimation error of learning under the
binary AR(K) data generating process. The result follows directly as a result of Lemma 36.
Theorem 37. (binary AR(K) estimation error upper bound) For all d, K ∈Z++, if for all t ∈Z+, Xt is
generated according to the binary AR(K) process, then for all T,
LT ≤dK
2T

1 + ln

1 +
T
4dK

.
42

Proof.
LT
(a)
≤inf
ϵ≥0
Hϵ,T (θ1:K)
T
+ ϵ
(b)
≤inf
ϵ≥0
dK ln
 1 + 1
8ϵ

2T
+ ϵ
(c)
≤dK
2T

1 + ln

1 +
T
4dK

,
where (a) follows from Theorem 15, (b) follows from Lemma 36, and (c) follows by setting ϵ = (dK)/(2T).
An interesting aspect of this result is that both the proof techniques and final result are hardly impacted by the
fact that the sequence is not iid when conditioned on θ1:K. Existing tools for statistical analysis can struggle in the
setting without the appropriate averages of iid quantities. However, our analytical tools involving rate-distortion
theory allow us to handle such problem instances with relative ease and produce reasonable upper bounds on
estimation error in learning settings involving sequences of data. In the following section, we extend these tools to
analyze a more complex transformer data generating process.
7.3
Transformer Process
7.3.1
Data Generating Process
Let X0, X2, . . . be a sequence over a finite vocabulary {1, . . . , d}. Each of the d outcomes is associated with a known
embedding vector denoted as Φj for j ∈{1, . . . , d}. We assume that for all j, ∥Φj∥2 = 1. For brevity of notation,
we let ϕt = ΦXt i.e. the embedding associated with token Xt.
Let K denote the context length of the transformer, L denote it’s depth, and r denote the attention dimension. We
assume that the first token X0 is sampled from an arbitrary pmf on {1, . . . , d} but subsequent tokens are sampled
based on the previous K tokens within the context window and the weights of a depth L transformer model.
Just as in the deep neural networks section, we use U (ℓ)
t
to denote the output of layer ℓat time t. As a result, for
ℓ= 0, Ut,0 = ϕt−K+1:t (the embeddings associated with the past K tokens). For all t ≤T, ℓ< L, let
Attn(ℓ) 
U (ℓ−1)
t

= Softmax
 
U (ℓ−1)⊤
t
A(ℓ)U (ℓ−1)
t
√r
!
denote the attention matrix of layer (ℓ) where Softmax denotes the softmax function applied elementwise along the
columns. The matrix A(ℓ) ∈ℜr×r can be interpreted as the product of the key and query matrices and without loss
of generality, we assume that the elements of the matrices A(ℓ) are distributed iid N(0, 1) (Gaussian assumption is
not crucial but unit variance is).
Subsequently, we let
U (ℓ)
t
= Clip

V (ℓ)U (ℓ−1)
t
Attn(ℓ) 
U (ℓ−1)
t

,
where Clip ensures that each column of the matrix input has L2 norm at most 1. The matrix V (ℓ) resembles
the value matrix and we assume that the rows of V (ℓ) are distributed iid Unif(Sd−1). For ℓ< L, we have that
V (ℓ) ∈ℜr×r, whereas V (L) ∈ℜd×r.
Finally, the next token is generated via sampling from the softmax of the final layer:
Xt+1 ∼Softmax

U (L)
t
[−1]

,
where U (L)
t
[−1] denotes the right-most column of U (L)
t
. At each layer ℓ, the parameters θ(ℓ) consist of the matrices
A(ℓ), V (ℓ).
43

7.3.2
Theoretical Building Blocks
As with deep neural networks, our strategy is to drive a rate-distortion bound by deriving a suitable bound for the
distortion function. Recall that by the chain rule, for all t,
I(Xt+1; θ(1:L)|˜θ(1:L), Ht) =
L
X
ℓ=1
I(Xt+1; θ(ℓ)|θ(ℓ+1:L), ˜θ(1:ℓ), Ht).
In the deep neural network setting, the above RHS could be further simplified via an upper bound which replaces
the conditioning on ˜θ(1:ℓ) to (˜θ(ℓ), θ(1:ℓ−1)). However, this result relied heavily on the iid structure of the data. In
the sequential data setting, we have the following weaker result which still facilitates our analysis.
Lemma 38. For all t, L ∈Z++ and ℓ∈{1, . . . , L}, if θ(i) ⊥θ(j), ˜θ(i) ⊥˜θ(j), and θ(i) ⊥˜θ(j) for i ̸= j, then
I(Xt+1; θ(ℓ)|θ(ℓ+1:L), ˜θ(1:ℓ), Ht) ≤I(Ht+1; θ(ℓ)|θ(ℓ+1:L), θ(1:ℓ−1), ˜θ(ℓ), X0).
Proof.
I(Xt+1; θ(ℓ)|θ(ℓ+1:L), ˜θ(1:ℓ), Ht)
(a)
= I(Ht+1, ˜θ(1:ℓ−1); θ(ℓ)|θ(ℓ+1:L), ˜θ(ℓ)) −I(Ht, ˜θ(1:ℓ−1); θ(ℓ)|θ(ℓ+1:L), ˜θ(ℓ))
(b)
≤I(Ht+1, ˜θ(1:ℓ−1); θ(ℓ)|θ(ℓ+1:L), ˜θ(ℓ), X0)
(c)
≤I(Ht+1, θ(1:ℓ−1); θ(ℓ)|θ(ℓ+1:L), ˜θ(ℓ), X0)
(d)
= I(Ht+1, θ(1:ℓ−1); θ(ℓ)|θ(ℓ+1:L), ˜θ(ℓ), X0) −I(θ(1:ℓ−1); θ(ℓ)|θ(ℓ+1:L), ˜θ(ℓ), X0)
(e)
= I(Ht+1; θ(ℓ)|θ(ℓ+1:L), θ(1:ℓ−1), ˜θ(ℓ), X0)
where (a) follows from the chain rule of mutual information, (b) follows from the independence assumptions, (c)
follows from the data processing inequality applied to the markov chain θi ⊥˜θ(1:ℓ−1)|(Ht+1, θ(ℓ+1:L), θ(1:ℓ−1), XK
0 ),
(d) follows from the fact that I(θ(1:ℓ−1); θi|θ(ℓ+1:L), ˜θi, X0) = 0, and (e) follows from the chain rule of mutual
information.
Note that in the RHS of Lemma 38 is Ht+1 as opposed to Xt+1 which we would have desired. Nonetheless, this
factor of t will eventually only appear as a logarithmic factor in the final bound. To account for the influence of
conditioning on the future layers θ(ℓ+1:L), we establish the following result on the squared Lipschitz constant. Note
that we use ∥· ∥σ to denote the operator norm of a matrix and ∥· ∥F to denote the frobenius norm.
Lemma 39. (transformer layer Lipschitz) For all d, r, K, ℓ∈Z++, if
˜U (ℓ) = Clip

V (ℓ) ˜U (ℓ−1)Attn(ℓ) 
˜U (ℓ−1)
,
then
U (ℓ) −˜U (ℓ)
2
F ≤2K∥V (ℓ)∥2
σ

1 + 4K∥A(ℓ)∥2
σ
r

·
U (ℓ−1) −˜U (ℓ−1)
2
F .
44

Proof.
U (ℓ) −˜U (ℓ)
2
F
=
Clip

V (ℓ)U (ℓ−1)σ
U (ℓ−1)⊤A(ℓ)U (ℓ−1)
√r

−Clip
 
V (ℓ) ˜U (ℓ−1)σ
 ˜U (ℓ−1)⊤A(ℓ) ˜U (ℓ−1)
√r
!!
2
F
(a)
≤
V (ℓ)U (ℓ−1)σ
U (ℓ−1)⊤A(ℓ)U (ℓ−1)
√r

−V (ℓ) ˜U (ℓ−1)σ
 ˜U (ℓ−1)⊤A(ℓ) ˜U (ℓ−1)
√r
!
2
F
(b)
=
K
X
k=1
∥V (ℓ)∥2
σ
U (ℓ−1)σ
 
U (ℓ−1)⊤A(ℓ)U (ℓ−1)
k
√r
!
−˜U (ℓ−1)σ
 ˜U (ℓ−1)⊤A(ℓ) ˜U (ℓ−1)
k
√r
!
2
2
≤
K
X
k=1
2∥V (ℓ)∥2
σ
U (ℓ−1)σ
 
U (ℓ−1)⊤A(ℓ)U (ℓ−1)
k
√r
!
−˜U (ℓ−1)σ
 
U (ℓ−1)⊤A(ℓ)U (ℓ−1)
k
√r
!
2
2
+
K
X
k=1
2∥V (ℓ)∥2
σ

˜U (ℓ−1)σ
 
U (ℓ−1)⊤A(ℓ)U (ℓ−1)
k
√r
!
−˜U (ℓ−1)σ
 ˜U (ℓ−1)⊤A(ℓ) ˜U (ℓ−1)
k
√r
!
2
2
(c)
≤
K
X
k=1
2∥V (ℓ)∥2
σ∥U (ℓ−1) −˜U (ℓ−1)∥2
F
+
K
X
k=1
2K
r ∥V (ℓ)∥2
σ
U (ℓ−1)⊤A(ℓ)U (ℓ−1)
k
−˜U (ℓ−1)⊤A(ℓ) ˜U (ℓ−1)
k

2
2
(d)
≤2K∥V (ℓ)∥2
σ
U (ℓ−1) −˜U (ℓ−1)
2
F + 4K
r
K
X
k=1
∥V (ℓ)∥2
σ
U (ℓ−1)⊤A(ℓ)U (ℓ−1)
k
−U (ℓ−1)⊤A(ℓ) ˜U (ℓ−1)
k

2
2
+ 4K
r
K
X
k=1
∥V (ℓ)∥2
σ
U (ℓ−1)⊤A(ℓ) ˜U (ℓ−1)
k
−˜U (ℓ−1)⊤A(ℓ) ˜U (ℓ−1)
k

2
2
(e)
≤2K∥V (ℓ)∥2
σ
U (ℓ−1) −˜U (ℓ−1)
2
F + 4K2
r
K
X
k=1
∥V (ℓ)∥2
σ∥A(ℓ)∥2
σ
U (ℓ−1)
k
−˜U (ℓ−1)
k

2
2
+ 4K
r
K
X
k=1
∥V (ℓ)∥2
σ∥A(ℓ)∥2
σ
U (ℓ−1) −˜U (ℓ−1)
2
F
= 2K∥V (ℓ)∥2
σ
U (ℓ−1) −˜U (ℓ−1)
2
F + 8K2
r
∥V (ℓ)∥2
σ∥A(ℓ)∥2
σ
U (ℓ−1) −˜U (ℓ−1)
2
F
= 2K∥V (ℓ)∥2
σ

1 + 4K∥A(ℓ)∥2
σ
r

·
U (ℓ−1) −˜U (ℓ−1)
2
F ,
where (a) follows from the fact that Clip is a contraction mapping, where in (b), U (ℓ−1)
k
denotes the kth column of
U (ℓ−1) ∈ℜd×K, (c) follows from the fact ∥˜U (ℓ−1)∥2
σ ≤K and the fact that softmax is 1-Lipschitz, (d) follows from
the fact that (x + z)2 ≤2(x −y)2 + 2(y + z)2, and (e) follows from the fact that ∥U (ℓ−1)U (ℓ−1)⊤∥σ ≤K.
note that again, unlike the standard deep neural network setting, the Lipschitz constant is not 1, but rather scales
with K, the context length. The intricacies of the softmax self-attention mechanism complicate the process of
arriving at a tight characterization. However, for the purposes of deriving an estimation error bound, this result
will suffice.
With this result in place, we now upper bound the expected squared difference between an output generated by a
transformer layer with the correct weights A, V and an output generated by slightly perturbed weights ˜A, ˜V .
Lemma 40. For all d, r, K ∈Z++ and ϵ ≥0, if V ∈ℜr×r consists of elements distributed iid N(0, 1/r), A ∈ℜr×r
consists of elements distributed N(0, 1), for all ℓ∈[L],
˜U (ℓ) = Clip

˜V (ℓ)U (ℓ−1)
˜
Attn
(ℓ) 
U (ℓ−1)
,
45

where for all ℓ< L, E[∥V (ℓ) −˜V (ℓ)∥2
F ] ≤ϵ, E[∥A(ℓ) −˜A(ℓ)∥2
F ] ≤ϵ, and E[∥V (L) −˜V (L)∥2
F ] ≤ϵ, E[∥A(L) −˜A(L)∥2
F ] ≤
rϵ/d, then
E
h
∥U (ℓ) −˜U (ℓ)∥2
F
i
≤
(
2K2 (1 + K) ϵ
if ℓ< L
2K (1 + K) ϵ
if ℓ= L .
Proof.
E
U (ℓ) −˜U (ℓ)
2
F

≤E

sup
u∈U
V (ℓ)uAttn(ℓ)(u) −˜V (ℓ)u ˜
Attn
(ℓ)(u)

2
F

= E

sup
u∈U
V uσ
u⊤Au
√r

−˜V uσ
 
u⊤˜Au
√r
!
2
F


(a)
≤2E

sup
u∈U


V −˜V

uσ
 
u⊤˜Au
√r
!
2
F

+ 2E

sup
u∈U
V u
 
σ
u⊤Au
√r

−σ
 
u⊤˜Au
√r
!!
2
F


(b)
≤2E

sup
u∈U
V −˜V

2
F
uσ
 
u⊤˜Au
√r
!
2
F

+ 2E

sup
u∈U
∥V ∥2
F
uσ
u⊤Au
√r

−uσ
 
u⊤˜Au
√r
!
2
F


(c)
≤2ϵ · sup
u∈U
∥u∥2
F ·
σ
 
u⊤˜Au
√r
!
2
F
+ 2E

∥V ∥2
F · sup
u∈U
∥u∥2
F
σ
u⊤Au
√r

−σ
 
u⊤˜Au
√r
!
2
F


(d)
≤2ϵK2 + 2E

Kr · sup
u∈U

u⊤Au
√r
−u⊤˜Au
√r

2
F


(e)
= 2ϵK2 + 2K · E

sup
u∈U
K
X
i=1
K
X
j=1

u⊤
i (A −˜A)uj
2


≤2ϵK2 + 2K · E


K
X
i=1
K
X
j=1
A −˜A

2
F


= 2K2ϵ + 2K3ϵ,
where (a) follows from the fact that ∥a + b∥2
F ≤2∥a∥2
F + 2∥b∥2
F for all matrices a, b, (b) follows from the fact that
∥ab∥2
F ≤∥a∥2
σ∥b∥2
F and ∥a∥2
σ ≤∥a∥2
F for all matrices a, b, (c) follows from the fact that E
h
∥V −˜V ∥2
F
i
= ϵ, (d)
follows from the fact that E[∥V ∥2
F ] = r, and the fact that softmax is 1-Lipschitz, and where in (e), xi denotes the
ith column of matrix x.
46

For ℓ= L,
E
U (L) −˜U (L)
2
F

≤E

sup
u∈U
V (L)uAttn(L)(u)[−1] −˜V (L)u ˜
Attn
(L)(u)[−1]

2
F

= E

sup
u∈U
V (L)uσ
u⊤A(L)u[−1]
√r

−˜V (L)uσ
 
u⊤˜A(L)u[−1]
√r
!
2
F


(a)
≤2E

sup
u∈U


V (L) −˜V (L)
uσ
 
u⊤˜A(L)u[−1]
√r
!
2
F


+ 2E

sup
u∈U
V (L)u
 
σ
u⊤A(L)u[−1]
√r

−σ
 
u⊤˜A(L)u[−1]
√r
!!
2
F


(b)
≤2E

sup
u∈U
V (L) −˜V (L)
2
F
uσ
 
u⊤˜A(L)u[−1]
√r
!
2
F


+ 2E

sup
u∈U
V (L)
2
F
uσ
u⊤A(L)u[−1]
√r

−uσ
 
u⊤˜A(L)u[−1]
√r
!
2
F


(c)
≤2ϵ · sup
u∈U
∥u∥2
F ·
σ
 
u⊤˜A(L)u[−1]
√r
!
2
F
+ 2E

∥V (L)∥2
F · sup
u∈U
∥u∥2
F
σ
u⊤A(L)u[−1]
√r

−σ
 
u⊤˜A(L)u[−1]
√r
!
2
F


(d)
≤2ϵK + 2E

Kd · sup
u∈U

u⊤A(L)u[−1]
√r
−u⊤˜A(L)u[−1]
√r

2
F


(e)
= 2ϵK + 2Kd
r
· E
"
sup
u∈U
K
X
i=1

u⊤
i (A(L) −˜A(L))u[−1]
2
#
≤2ϵK + 2Kd
r
· E
" K
X
i=1
A(L) −˜A(L)
2
F
#
= 2Kϵ + 2K2ϵ,
where (a) follows from the fact that ∥a + b∥2
F ≤2∥a∥2
F + 2∥b∥2
F for all matrices a, b, (b) follows from the fact that
∥ab∥2
F ≤∥a∥2
σ∥b∥2
F and ∥a∥2
σ ≤∥a∥2
F for all matrices a, b, (c) follows from the fact that E
h
∥V −˜V ∥2
F
i
= ϵ, (d)
follows from the fact that E[∥V ∥2
F ] = d, and the fact that softmax is 1-Lipschitz, and where in (e), ui denotes the
ith column of matrix u.
With these preliminary results in place, we establish the following upper bound on the distortion of a single layer
ℓof our transformer.
Lemma 41. (transformer layer distortion upper bound) For all d, r, t, K, L ∈Z++, 0 ≤ϵ, and ℓ∈[L], if
˜θ(ℓ) = ( ˜V (ℓ), ˜A(ℓ)) = (V (ℓ) + Z(ℓ), A(ℓ) + B(ℓ)),
where
(Z(ℓ), B(ℓ)) ⊥(V (ℓ), A(ℓ)),
Z(ℓ) iid
∼
(
N(0, ϵ/r2)
if ℓ< L
N(0, ϵ/(rd)
if ℓ= L ;
B(ℓ) iid
∼N(0, ϵ/r2),
then
I(Ht+1; θ(ℓ)|θ(ℓ+1:L), θ(1:ℓ−1), ˜θ(ℓ), X0) ≤ϵ(t + 1)K (8K(1 + 16K))L−ℓ+1 .
47

Proof. We begin with the base cases ℓ= L
I(Xt+1; θ(L)|θ(1:L−1), ˜θ(L), Ht) = E
h
dKL

P(Xt+1 ∈·|θ(1:L), Ht) ∥P(Xt+1 ∈·|θ(1:L−1), ˜θ(L), Ht)
i
≤E
h
dKL

P(Xt+1 ∈·|θ(1:L), Ht) ∥P(Xt+1 ∈·|θ(1:L−1), θ(L) ←˜θ(L), Ht)
i
≤E
hfθ(1:L)(Xt:t−K+1) −f˜θ(L) (fθ(1:L−1)(Xt:t−K+1))
2
2
i
≤2K(1 + K)ϵ,
For ℓ< L, we have:
I(Xt+1; θ(ℓ)|θ(1:ℓ−1), θ(ℓ+1:L), ˜θ(ℓ), Ht)
= E
h
dKL

P

Xt+1 ∈·|θ(1:L), Ht

∥P

Xt+1 ∈·|θ(1:ℓ−1), θ(ℓ+1:L), ˜θ(ℓ), Ht
i
(a)
≤E
h
dKL

P

Xt+1 ∈·|θ(1:L), Ht

∥P

Xt+1 ∈·|θ(1:ℓ−1), θ(ℓ+1:L), θ(ℓ) ←˜θ(ℓ), Ht
i
(b)
≤E
hfθ(1:L)(Xt:t−K+1) −
 fθ(ℓ+1:L) ◦f˜θ(ℓ) ◦fθ(1:ℓ−1)

(Xt:t−K+1)
2
2
i
(c)
≤E

2K∥V (L)∥2
σ

1 + 4K∥A(L)∥2
σ
r

·
fθ(1:L−1)(Xt:t−K+1) −
 fθ(ℓ+1:L−1) ◦f˜θ(ℓ) ◦fθ(1:ℓ−1)

(Xt:t−K+1)
2
2

(d)
≤E
" 
L
Y
l=ℓ+1
2K∥V (l)∥2
σ

1 + 4K∥A(l)∥2
σ
r
!
·
fθ(1:ℓ)(Xt:t−K+1) −
 f˜θ(ℓ) ◦fθ(1:ℓ−1)

(Xt:t−K+1)
2
2
#
(e)
≤E
" 
L
Y
l=ℓ+1
2K∥V (l)∥2
σ

1 + 4K∥A(l)∥2
σ
r
!
· 2K2(1 + K)ϵ
#
(f)
≤(8K(1 + 16K))L−ℓ· 2K2(1 + K)ϵ
≤ϵK (8K(1 + 16K))L−ℓ+1
where (a) follows from Lemma 57, (b) follows from Lemma 23, (c) and (d) follow from Lemma 39, (e) follows from
Lemma 40 and the fact that (V (i), A(i) ⊥(V (j), A(j))) and (V (i), A(i) ⊥( ˜V (j), ˜A(j))) for i ̸= j, and (f) follows from
the fact that A(L) ⊥V (L) and E

∥A(L)∥2
σ

≤4r and E

∥V (L)∥2
σ

≤4.
With this inequality in place, we have the following.
I(Ht+1; θ(ℓ)|θ(ℓ+1:L), θ(1:ℓ−1), ˜θ(ℓ), X0) =
t
X
k=0
I(Xk+1; θ(ℓ)|θ(ℓ+1:L), θ(1:ℓ−1), ˜θ(ℓ), Hk)
(a)
≤
t
X
k=0
ϵK (8K(1 + 16K))L−ℓ+1
= ϵ(t + 1)K (8K(1 + 16K))L−ℓ+1 ,
where (a) follows from the above result.
With Lemma 41 in place, we establish the final upper bound on the distortion for the full deep transformer.
Lemma 42. (transformer distortion upper bound) For all d, r, t, K, L ∈Z++, and ℓ∈[L], if
˜θ(ℓ) = ( ˜V (ℓ), ˜A(ℓ)) = (V (ℓ) + Z(ℓ), A(ℓ) + B(ℓ)),
where
(Z(ℓ), B(ℓ)) ⊥(V (ℓ), A(ℓ)),
Z(ℓ) iid
∼
(
N(0, ϵ/r2)
if ℓ< L
N(0, ϵ/(rd)
if ℓ= L ;
B(ℓ) iid
∼N(0, ϵ/r2),
then
I(Xt+1; θ(1:L)|˜θ(1:L), Ht) ≤ϵKL(t + 1) (8K(1 + 16K))L .
48

Proof.
I(Xt+1; θ(1:L)|˜θ(1:L), Ht) =
L
X
ℓ=1
I(Xt+1; θ(ℓ)|˜θ(1:L), θ(ℓ+1:L)Ht)
(a)
≤
L
X
ℓ=1
I(Ht+1; θ(ℓ)|θ(ℓ+1:L), θ(1:ℓ−1), ˜θ(ℓ), X0)
(b)
≤ϵKL(t + 1) (8K(1 + 16K))L ,
where (a) follows from Lemma 38, and (b) follows from Lemma 41.
Lemma 43. (transformer rate-distortion upper bound) For all d, r, t, K, L ∈Z++, if for all t, Xt is generated
by the transformer process, then
Hϵ,T (θ(1:L)) ≤r · max{r, d}L ln

1 + r · max{r, d}KLT(8K(1 + 16K))L
ϵ

.
Proof. For all ℓ∈[L], let
˜θ(ℓ) = ( ˜V (ℓ), ˜A(ℓ)) = (V (ℓ) + Z(ℓ), A(ℓ) + B(ℓ)),
where
(Z(ℓ), B(ℓ)) ⊥(V (ℓ), A(ℓ)),
Z(ℓ) iid
∼
(
N(0, ϵ′/r2)
if ℓ< L
N(0, ϵ′/(rd)
if ℓ= L ;
B(ℓ) iid
∼N(0, ϵ′/r2).
Let ϵ′ =
ϵ
KLT (8K(1+16K))L . Then,
I(θ(1:L); ˜θ(1:L)) = h(˜θ(1:L)) −h(˜θ(1:L)|θ(1:L))
=
L
X
ℓ=1
h(˜θ(ℓ)) −h(˜θ(ℓ)|θ(ℓ))
=
L
X
ℓ=1

h( ˜V (ℓ)) −h( ˜V (ℓ)|V (ℓ)) + h( ˜A(ℓ)) −h( ˜A(ℓ)|A(ℓ))

≤
L−1
X
ℓ=1
r2
2 ln

2πe
1
r + ϵ′
r2

−r2
2 ln

2πe
 ϵ′
r2

+
rd
2 ln

2πe
1
r + ϵ′
rd

−rd
2 ln

2πe
 ϵ′
rd

+
L
X
ℓ=1
r2
2 ln

2πe

1 + ϵ′
r2

−r2
2 ln

2πe
 ϵ′
r2

≤Lr · max{r, d}
2
ln

1 + max{r, d}KLT(8K(1 + 16K))L)
ϵ

+ Lr2
2
ln

1 + r2KLT(8K(1 + 16K)))L
ϵ

≤Lr · max{r, d} ln

1 + r · max{r, d}KLT(8K(1 + 16K))L
ϵ

,
We now verify that the distortion is ≤ϵ.
I(HT ; θ(1:L)|˜θ(1:L))
T
= 1
T
T −1
X
t=0
I(Xt+1; θ(1:L)|˜θ(1:L), Ht)
≤1
T
T −1
X
t=0
ϵ′KL(t + 1) (8K(1 + 16K))L
≤ϵ′KLT (8K(1 + 16K))L
= ϵ.
49

7.3.3
Main Result
With the results established in the previous section, we now present the following result which upper bounds the
estimation error of an optimal agent which learns from data generated by the transformer process.
Theorem 44. (transformer estimation error upper bound) For all d, r, L, K, T ∈Z++, if for all t ∈Z+, Xt
is generated according to the transformer process, then
LT ≤r · max{r, d}L2 ln(8eK(1 + 16K))
T
+
r · max{r, d}L ln

2KT 2
L

T
.
Proof.
LT
(a)
≤inf
ϵ≥0
Hϵ,T (θ(1:L))
T
+ ϵ
(b)
≤inf
ϵ≥0
r · max{r, d}L ln

1 + r·max{r,d}KLT (8K(1+16K))L
ϵ

T
+ ϵ
(c)
≤
r · max{r, d}L ln

1 + r·max{r,d}KLT (8K(1+16K))L
r·max{r,d}L2
T

T
+ r · max{r, d}L2
T
≤
r · max{r, d}L ln

2KT 2(8K(1+16K))L
L

T
+ r · max{r, d}L2
T
= r · max{r, d}L2 ln (8eK(1 + 16K))
T
+
r · max{r, d}L ln

2KT 2
L

T
,
where (a) follows from Theorem 15, (b) follows from Lemma 43, (c) follows by setting ϵ = r · max{r, d}L2/T
Notably, the result scales linearly in the product of the parameter count of the transformer and the depth of the
transformer. Unlike in the standard feed-forward network setting, we are unable to eliminate the quadratic depth
dependence. This is due to fact that it is unknown whether softmax attention obeys the condition that the expected
squared lipschitz constant is ≤1. In this work we upper bounded this lipschitz value by (2K + 8K2) which results
in the additional dependence on depth.
Summary
• In this section, we demonstrate that the analytic tools which we developed seamlessly transfer to the
setting in which data is no longer iid under some unknown distribution. To demonstrate concrete use
of our tools, we derive estimation error upper bounds for 2 settings involving learning from sequential
data: 1) a simple binary AR(K) process, 2) a transformer process.
• (binary AR(K) estimation error upper bound) For all embedding dimensions d and context lengths
K, if for all t ∈Z+, Xt is generated according to the binary AR(K) process, then for all T,
LT ≤dK
2T

1 + ln

1 +
T
4dK

.
• Consider a sequence which is generated by a transformer with vocabulary size d, embedding dimension
r, depth L, and context length K. The following result holds:
• (transformer estimation error upper bound) For all vocabulary sizes d, embedding dimensions r,
depths L, and context lengths K, if for all t ∈Z+, Xt is generated according to the transformer process,
50

then for all T,
LT ≤r · max{r, d}L2 ln(8eK(1 + 16K))
T
+
r · max{r, d}L ln

2KT 2
L

T
.
51

8
Meta-Learning
Abstract, Recent, work, has, 
demonstrated, substantial, gains, …
Article, From, Wikipedia, the, free, 
encyclopedia, …
import, torch, import, torch, .nn, as, nn, 
import, torch, .nn, .functional, as, F …
Pile-CC
OpenWebText2
Stack 
Exchange
Wikipedia
Bibliotik
PG-19
BC2
Github
DM Math
Subtitles
IRC
EP
HN
YT
PubMed Central
Arxiv
FreeLaw
USPTO
PMA
Phil
NIH
Figure 7: The above diagram depicts the pre-training of large language models as a meta-learning problem. The
meta-parameters ψ specify a distribution over document types (tasks). Each document type (task) θm specifies
an autoregressive random process. Each document (X(m)
0
, X(m)
1
, X(m)
2
, . . .), is represented by a sequence of tokens
which is distributed according to the random process designated by the document type.
8.1
Data Generating Process
In meta-learning assume a further underlying structure in the data that we observe. Concretely, let the random
process (X(m)
t
: m, t ∈Z++) denote observations. For any fixed m, (X(m)
0
, X(m)
1
, . . .) represents the data associated
with a particular task indexed by m. In meta-learning, we assume that there exists a shared structure across the
tasks. Learning the structure will often accelerate learning on subsequent tasks. We assume that the sequence
(X(1)
0:∞, X(2)
0:∞, . . .) is exchangeable i.e. any finite permutation of the sequence does not alter the joint distribution.
De Finetti’s theorem then states that there exists a latent random variable, which we will denote by ψ such that
conditioned on ψ, the above sequence is iid. Note that this iid assumption does not pertain to the observations
within a given task, but rather to the entire collections X(m)
0:∞across tasks.
This latent variable ψ, which we refer to as the meta-parameters, represents the aforementioned information which
is shared across tasks. However, typically in meta-learning, for any particular task, there exists information beyond
ψ which must be learned to produce accurate predictions. As a result, we let (θ1, θ2, . . .) be a sequence which is
also iid when conditioned on ψ and we assume that for all m, X(m)
0:∞⊥ψ|θm. For each task m, θm may subsume the
information contained in ψ and contains all other relevant information which can be learned from the intra-task
observations.
We again outline that, we do not make the assumption that the intra-task observations are iid when conditioned
on θm. This allows us to characterize more realistic data generating processes such as natural language for which
the “tasks” may represent document types and the “observations” the sequence of tokens which comprise of a
document of said type (Figure 7).
8.2
Meta-Learning Error
We define notions of error for meta-learning which directly parallel the definitions of section 3.3. For purposes
of analysis, we consider learning under a setting for which there exist M tasks and T observations per task. A
52

learning algorithm produces for each (m, t) ∈[M] × [T], a predictive distribution Pm,t of X(m)
t+1 after observing the
concatenated history which we denote by
Hm,t =

X(1)
0:T , X(2)
0:T , . . . , X(m−1)
0:T
, X(m)
0:t

.
Hm,t consists of all observations from tasks 1, . . . , m −1 and up to the t-th observation of task m. We express our
meta-learning algorithm in terms of a function π for which Pm,t = π(Hm,t). For all M, T ∈Z++, we measure the
error realized by our predictions Pm,t in terms of the average cumulative expected log-loss:
LM,T,π =
1
MT
M
X
m=1
T −1
X
t=0
Eπ
h
−ln Pm,t

X(m)
t+1
i
.
Since this error is not baselined at 0, we elect to study the following estimation error:
LM,T,π = LM,T,π −H (HM,T |ψ, θ1:M)
MT
.
Note that while we introduce this new notation to facilitate exposition of meta-learning, the above notions of error
are subsumed by those established in section 3.3.
8.3
Achievable Error
We are again interested in characterizing the limits of performance via machine learning. As a result, it is prudent
to answer: For all (M, T), what π minimizes LM,T,π? As one would expect, the optimal algorithm assigns for all
(m, t), Pm,t = P(Xt+1 ∈·|Hm,t). We will denote this Bayesian posterior which we will denote as ˆPm,t.
Lemma 45. (Bayesian posterior is optimal) For all m, t ∈Z+,
E
h
−ln ˆPm,t

X(m)
t+1
i
= min
π
Eπ
h
−ln Pm,t

X(m)
t+1
i
.
Proof.
E
h
−ln Pm,t

X(m)
t+1

|Hm,t
i a.s.
= E

−ln ˆPm,t

X(m)
t+1

+ ln
ˆPm,t

X(m)
t+1

Pm,t(X(m)
t+1)
Hm,t


a.s.
= E
h
−ln ˆPm,t(X(m)
t+1)|Hm,t
i
+ dKL

ˆPm,t∥Pm,t

.
The result follows from the fact that KL-divergence is non-negative and the tower property.
Going forward, we will restrict our attention to the optimal achievable estimation error which we denote by:
LM,T =
1
MT
T −1
X
t=0
M
X
m=1
E
h
−ln ˆPm,t

X(m)
t+1
i
−H(HM,T |ψ, θ1, . . . , θM)
MT
.
The process of learning should result in LM,T vanishing to 0 as MT increases to ∞. We are interested in char-
acterizing the rate at which the estimation error decays to 0. In previous sections, we established that even if
the observations are not iid, we can observe error which decays linearly in the number of observations. In the
following section, we will establish general results which bound the estimation error of meta-learning and facilitate
the analysis of concrete problem instances.
8.4
General Theoretical Results
We begin with a general result which expresses the estimation error of meta-learning in terms of two very intuitive
quantities:
53

Theorem 46. (estimation error) For all M, T ∈Z+,
LM,T = I(HM,T ; ψ)
MT
|
{z
}
meta
estimation
error
+
M
X
m=1
I(X(m)
0:T ; θm|ψ)
MT
|
{z
}
intra-task
estimation
error
.
Proof.
LM,T =
1
MT
M
X
m=1
T −1
X
t=0
E
h
−ln ˆPm,t(X(m)
t+1)
i
−H(HM,T |ψ, θ1, . . . , θM)
MT
=
1
MT
M
X
m=1
T −1
X
t=0
I(X(m)
t+1; ψ, θm|Hm,t)
(a)
=
1
MT
M
X
m=1
I(X(m)
0:T ; ψ, θm|Hm−1,T )
(b)
=
1
MT
M
X
m=1
I(X(m)
0:T ; ψ|Hm−1,T ) +
1
MT
M
X
m=1
I(X(m)
0:T ; θm|ψ, Hm−1,T )
(c)
=
1
MT
M
X
m=1
I(X(m)
0:T ; ψ|Hm−1,T ) +
1
MT
M
X
m=1
I(X(m)
0:T ; θm|ψ)
(d)
= I(HM,T ; ψ)
MT
+
M
X
m=1
I(X(m)
0:T ; θm|ψ)
MT
,
where (a), (b), and (d) follow from the chain rule of mutual information, and (c) follows from the meta-learning
conditional independence assumptions.
The meta-estimation error term refers to the error which is attributed to learning about the meta-parameters ψ.
Since all of the observations contain information about ψ, it is intuitive that the numerator would include the
mutual information between the full history HM,T and the meta-parameters ψ. Likewise, it is also to be expected
that the error decays linearly in the product MT since each observation contributes information about ψ. The
second term consists of the error which is incurred from learning about the task specific parameters θ1, . . . , θM
conditioned on ψ.
Note that due to the conditional iid assumptions, this term simplifies to I(X(m)
0:T ; θm|ψ)/T.
Notably, this term decays linearly in T but not M since the data from other tasks is independent of θ(m) when
conditioned on ψ. Intuitively, this error captures the remaining information to be extracted for a particular task
beyond what is present in the meta-parameters.
While this result is useful for conceptual understanding, we require further tools to streamline theoretical anal-
ysis. We again leverage rate-distortion theory to establish suitable upper and lower bounds for the estimation
error of meta-learning.
Concretely, we define two rate-distortion functions.
The first characterizes the meta-
parameters:
Hϵ,M,T (ψ) =
inf
˜
ψ∈˜Ψϵ,M,T
I(ψ; ˜ψ),
where
Ψϵ,M,T =
(
˜ψ : ˜ψ ⊥HM,T ;
I(HM,T ; ψ| ˜ψ)
MT
≤ϵ
)
.
The second characterizes the intra-task parameters conditioned on ψ:
Hϵ,T (θm|ψ) =
inf
˜θm∈˜Θϵ,T
I(θm; ˜θm|ψ),
where
˜Θϵ,T =
(
˜θ : ˜θ ⊥HM,T
θm;
I(X(m)
0:T ; θm|˜θ, ψ)
T
≤ϵ
)
.
54

With these definitions in place, we establish upper and lower bounds on the estimation error in terms of the above
rate-distortion functions.
Theorem 47. (rate-distortion estimation error bound) For all M, T ∈Z+, and m ∈{1, . . . , M},
LM,T ≤inf
ϵ≥0
Hϵ,M,T (ψ)
MT
+ ϵ
+
inf
ϵ′≥0
Hϵ′,T (θm|ψ)
T
+ ϵ′,
and
LM,T ≥sup
ϵ≥0
min
Hϵ,M,T (ψ)
MT
, ϵ

+
sup
ϵ′≥0
min
Hϵ′,T (θm|ψ)
T
, ϵ′

.
Proof. We begin by showing the upper bound:
LM,T = I(HM,T ; ψ)
MT
+ I(X(m)
0:T ; θm|ψ)
T
= I(HM,T ; ψ, ˜ψ)
MT
+ I(X(m)
0:T ; θm, ˜θm|ψ)
T
= I(HM,T ; ˜ψ)
MT
+ I(HM,T ; ψ| ˜ψ)
MT
+ I(X(m)
0:T ; θm, ˜θm|ψ)
T
(a)
≤I(ψ; ˜ψ)
MT
+ I(HM,T ; ψ| ˜ψ)
MT
+ I(X(m)
0:T ; θm, ˜θm|ψ)
T
= I(ψ; ˜ψ)
MT
+ I(HM,T ; ψ| ˜ψ)
MT
+ I(X(m)
0:T ; ˜θm|ψ)
T
+ I(X(m)
0:T ; θm|˜θm, ψ)
T
(b)
≤I(ψ; ˜ψ)
MT
+ I(HM,T ; ψ| ˜ψ)
MT
+ I(θm; ˜θm|ψ)
T
+ I(X(m)
0:T ; θm|˜θm, ψ)
T
(c)
≤Hϵ,M,T (ψ)
MT
+ ϵ + Hϵ′,M,T (˜θm|ψ)
T
+ ϵ′,
where (a) and (b) follow from the data processing inequality and (c) follows from the definition of the rate-distortion
functions. The upper bound follows from the fact that inequality (c) holds for all ϵ ≥0.
We now prove the lower bound. Suppose that I(HM,T ; ψ) < Hϵ,M,T (ψ) Let ˜ψ = ˜HM,T /∈˜Ψϵ,M,T where ˜HM,T is
another history sampled in the same manner as HM,T .
I(HM,T ; ψ) =
M
X
m=1
T −1
X
t=0
I(X(m)
t+1; ψ|Hm,t)
(a)
≥
M
X
m=1
T −1
X
t=0
I(X(m)
t+1; ψ| ˜HM,T , Hm,t)
=
M
X
m=1
T −1
X
t=0
I(X(m)
t+1; ψ| ˜ψ, X(m)
1
, . . . , X(m)
t
)
(b)
≥ϵMT,
where (a) follows from the fact that conditioning reduces entropy and that X(m)
t+1 ⊥˜HM,T |(ψ, Hm,t) and (b) follows
from the fact that ˜ψ /∈˜Ψϵ,M,T . Therefore, for all ϵ ≥0, I(HM,T ; ψ) ≥min{Hϵ,M,T (ψ), ϵMT}.
Suppose that I(H(m)
T
; θm|ψ) < Hϵ,T (θm|ψ). Let ˜θm = ˜Dm /∈˜Θϵ,T where ˜Dm is another history sampled in the same
55

manner as X(m)
0:T .
I(X(m)
0:T ; θm|ψ) =
T −1
X
t=0
I(X(m)
t+1; θm|X(m)
1
, . . . , X(m)
t
, ψ)
(a)
≥
T −1
X
t=0
I(X(m)
t+1; θm| ˜Dm, X(m)
1
, . . . , X(m)
t
, ψ)
=
T −1
X
t=0
I(X(m)
t+1; θm|˜θm, Hm,t, ψ)
(b)
≥ϵT,
where (a) follows from the fact that conditioning reduces entropy and that X(m)
t+1 ⊥˜Dm|(ψ, X(m)
1
, . . . , X(m)
t
) and
(b) follows from the fact that ˜θm /∈˜Θϵ,T . Therefore, for all ϵ ≥0, I(X(m)
0:T ; θm|ψ) ≥min{Hϵ,M,T (θm), ϵT}. The
lower bound follows as a result.
Theorem 47 establishes an intimate connection between the estimation error of meta learning and the aforemen-
tioned rate-distortion functions via upper and lower bounds. This result will allow us to streamline the analysis
of learning under various meta-learning problems. In the subsequent sections, we will study two such instances.
The first is a simple linear representation learning setting which is provided as exposition to acclimate the reader
to the styles of analysis required for meta-learning. The second setting involves a mixture of transformers which
resembles and provides insights about pre-training and in-context learning in LLMs.
8.5
Linear Representation Learning
Figure 8: The above diagram depicts the linear representation learning problem. The matrix ψ ∈ℜd×r represents
the information which is shared across the tasks.
The intra-task information θm is the product ψξ(m), where
ξ(m) ∈ℜr. The resulting vector θm is passed through a softmax to produce a categorical distribution over d classes.
Each corresponding sequence X(m)
0
, X(m)
1
, X(m)
2
, . . . is produce via iid sampling of the aforementioned categorical
distribution.
8.5.1
Data Generating Process
We introduce a simple linear representation learning problem as a concrete example of meta-learning to demonstrate
our method of analysis. In this example, the intra-task observations are iid, but we begin with such an example
for simplicity and to demonstrate this as a special case of general meta-learning under our framework.
56

For all d, r ∈Z++, we let ψ : Ω7→ℜd×r be distributed uniformly over the set of d × r matrices with orthonormal
columns. We assume that d ≫r. For all m, let ξ(m) : Ω7→ℜr be distributed iid N(0, Ir/r). We let θm = ψξ(m).
As for the observable data, for each (m, t), let X(m)
t+1 be drawn as according to the following probability law:
X(m)
t+1 =









1
w.p. σ(θm)1
2
w.p. σ(θm)2
. . .
d
w.p. σ(θm)d
,
where σ(θm)j = eθm,j/ Pd
k=1 eθm,k. For each task m, the algorithm is tasked with estimating a vector θm from
sampled observations (X(m)
1
, X(m)
2
, . . .). By reasoning about data from previous tasks, the algorithm can estimate
ψ, which reduces the burden of estimating θm to just estimating ξ(m) for each task. This is significant given the
assumption that d ≫r.
8.5.2
Theoretical Building Blocks
We begin by establishing several lemmas which streamline our analysis. We begin with the following upper bound
on the meta-estimation error term:
Lemma 48. (meta rate-distortion upper bound) For all d, r, M, T ∈Z++, if for all (m, t) ∈[M] × [T], X(m)
t
is generated according to the linear representation learning process, then
HM,T,ϵ(ψ) ≤
dr
2MT ln

1 +
d
r

e
2ϵT
r
−1


.
Proof. Let ˜ψ = ψ + Z where Z ∈ℜd×r is independent of ψ and consists of elements which are distributed iid
N(0, ϵ′) where ϵ′ = (e
2ϵT
r
−1)/d We begin by upper bounding the rate.
I(ψ; ˜ψ)
MT
= h( ˜ψ) −h( ˜ψ|ψ)
MT
(a)
≤
dr
2 ln
 2πe
 ϵ′ + 1
r

−dr
2 ln (2πeϵ′)
MT
= dr ln
 1 +
1
rϵ′

2MT
=
dr
2MT ln

1 +
d
r

e
2ϵT
r
−1


,
where (a) follows from the maximum differential entropy of a random variable of fixed variance being upper bounded
by a Gaussian random variable.
57

We now upper bound the distortion.
I(HM,T ; ψ| ˜ψ)
MT
= H(HM,T | ˜ψ) −H(HM,T |ψ)
MT
=
PM
m=1 H(X(m)
0:T | ˜ψ, Hm−1,T ) −H(X(m)
0:T |ψ, Hm−1,T )
MT
≤H(X(1)
0:T | ˜ψ) −H(X(1)
0:T |ψ)
T
= I(X(1)
0:T ; ψ| ˜ψ)
T
≤I(θ1; ψ| ˜ψ)
T
=
E
h
dKL(P(θ1 ∈·|ψ)∥P(θ1 ∈·| ˜ψ))
i
n
(a)
≤
E
h
dKL(P(θ1 ∈·|ψ)∥P(θ∈· |ψ ←˜ψ))
i
T
≤
E
h
dKL(limδ→0 P(θδ ∈·|ψ)∥limδ→0 P(θδ|ψ ←˜ψ))
i
T
(b)
= 1
T E

lim
δ→0
1
2 ln


δId +
˜
ψ ˜
ψ⊤
r

δId + ψψ⊤
r


−d + Tr


 
δId +
˜ψ ˜ψ⊤
r
!−1 
δId + ψψ⊤
r




(c)
≤1
T E

lim
δ→0
1
2 ln


δId + ˜π ˜
ψ⊤
r

δId + ψψ⊤
r





(d)
= 1
T E

lim
δ→0
1
2 ln


|δId| ·
Ir +
˜
ψ⊤˜
ψ
rδ

|δId| ·
Ir + ψ⊤ψ
rδ





= 1
T E

lim
δ→0
1
2 ln


Ir +
˜
ψ⊤˜
ψ
rδ

Ir + Ir
rδ





(e)
≤lim
δ→0
1
2T ln




Ir +
E[ ˜
ψ⊤˜
ψ]
rδ

Ir + Ir
rδ





= lim
δ→0
1
2T ln




Ir +
E[Ir+dϵ′Ir]
rδ

Ir + Ir
rδ





= lim
δ→0
r
2T ln
 
1 + 1+dϵ′
rδ
1 + 1
rδ
!
= r
2T ln (1 + dϵ)
= ϵ,
where (a), (b) follows from continuity of the KL-divergence between two multivariate normal distributions w.r.t
the covariance matrix, (c) follows from the fact that the trace term is upper bounded by d, (d) follows from the
matrix determinant lemma, ϵ = 1
m, and (e) follows from Jensen’s inequality. The result follows.
Recall that for meta-learning, the total estimation error is upper bounded by the sum of the meta rate-distortion
58

function and the intra-task rate-distortion function. The following result establishes an upper bound on the intra-
task rate-distortion function.
Lemma 49. (intra-task rate-distortion upper bound) For all r, T ∈Z++,
HT,ϵ(θm|ψ) ≤r
2 ln

1 + 1
ϵ

.
Proof. Let ˜ξ = ξ(m) + Z where Z ⊥ξ(m) and Z ∼N(0, ϵIr/r). Let ˜θ = ψ˜ξ. We begin by upper bounding the rate.
I(θm; ˜θ|ψ) ≤I(ξ; ˜ξ|ψ)
= h(˜ξ|ψ) −h(˜ξ|ψ, ξ)
= h(˜ξ) −h(˜ξ|ξ)
= h(˜ξ) −h(Z)
= r
2 ln

2πe( ϵ
r + 1
r )

−r
2 ln
2πeϵ
r

= r
2 ln

1 + 1
ϵ

.
We now upper bound the distortion:
I(X(m)
0:T ; θm|˜θ, ψ)
T
≤I(X(m)
0:T ; θm|˜θ, ψ)
T
≤I(X(m)
1
; θm|˜θ)
= E
h
dKL

P

X(m)
1
∈·|θm

∥P

X(m)
1
∈·|˜θ
i
(a)
≤E
h
dKL

P

X(m)
1
∈·|θm

∥P

X(m)
1
∈·|θm ←˜θ
i
(b
≤E
h
∥˜θ −θm∥2
2
i
= E

ξ(m) −˜ξ
⊤
ψ⊤ψ

ξ(m) −˜ξ

= E

ξ(m) −˜ξ
⊤
ξ(m) −˜ξ

= E

Z⊤Z

= ϵ
where (a) follows from Lemma 1, and (b) follows from Lemma 23.
With these two results in place, we present the main result which upper bounds the estimation error of linear
representation learning.
8.5.3
Main Result
With the rate-distortion upper bounds established from the previous section, we present the following reslt which
upper bounds the estimation error of linear representation learning.
Theorem 50. (linear representation learning estimation error bound) For all d, r, M, T ∈Z++, if for all
(m, t), X(m)
t
is generated according to the linear representation learning process, then
LM,T ≤dr ln
 e
 1 + M
r

2MT
+ r ln
 e
 1 + 2T
r

2T
.
59

Proof.
LM,T
(a)
≤inf
ϵ≥0
HM,T ϵ(ψ)
MT
+ ϵ + inf
ϵ′≥0
HT,ϵ′(θm|ψ)
T
+ ϵ′
(b)
≤inf
ϵ≥0
dr
2MT ln

1 +
d
r

e
2ϵT
r
−1


+ ϵ + inf
ϵ′≥0
r
2n ln

1 + 1
rϵ′

+ ϵ′
(c)
≤
dr
2MT ln

1 +
d
r

e
dϵ
M −1


+
dr
2MT + r ln(1 + 2T
r )
2T
+ r
2T
≤dr ln
 1 + M
r

2MT
+
dr
2MT + r ln(1 + 2T
r )
2T
+ r
2T
= dr ln
 e
 1 + M
r

2MT
+ r ln
 e
 1 + 2T
r

2T
,
where (a) follows from Theorem 46, (b) follows directly from Lemmas 48 and 49, and (c) follows from setting
ϵ =
dr
2MT and ϵ′ =
r
2T .
The first term describes the error which is incurred in the process of estimating the meta-parameters ψ. Notably,
this term is linear in dr, the parameter count of ψ and decays linearly in MT, the total number of observations.
This is both intuitive and desirable since each observation ought to provide information about ψ. Meanwhile,
the second term describes the error which is incurred in the process of estimating the intra-task parameters θm
conditioned on ψ. In our problem instance, θ = ψξ for ξ ∈ℜr, so the remaining uncertainty in θ when conditioning
on ψ is simply ξ. Therefore, the intra-task error is linear in r, the parameter count of ξ and decays linearly in T,
the number of observations pertinent to task m.
In the following section, we will characterize the learning performance under a much more complex process which
resembles pre-training and in-context learning in LLMs.
8.6
In-Context Learning
Abstract, Recent, work, has
Article, From, Wikipedia, the
import, torch, import, torch
demonstrated
free
.nn
Figure 9: The above diagram depicts the in-context learning problem. ψ designates a mixture of many transformer
models, each which is responsible for producing documents of a particular type.
Each document is generated
by sampling a transformer model θm from the mixture ψ, and generating a sequence (X(m)
0
, X(m)
1
, X(m)
2
, . . .)
autoregressively.
60

8.6.1
Data Generating Process
In the ICL process, for all documents m, we let its tokens (X(m)
1
, X(m)
2
, . . .) be a sequence in {1, . . . , d}, where d
denotes the size of the vocabulary. Each of the d outcomes is associated with a known embedding vector which we
denote as Φj for j ∈{1, . . . , d}. We assume that for all j, ∥Φj∥2 = 1. For brevity of notation, we let ϕ(m)
t
= ΦX(m)
t
i.e. the embedding associated with token X(m)
t
.
Each document is generated by a transformer model which is sampled iid from a mixture.
We assume that
sampling is performed according to a categorical distribution parameterized by α with prior distribution P(α ∈
·) = Dirichlet(N, [R/N, . . . , R/N]) for a scale parameter R ≪N. Realizations of this prior distributions are hence
probability mass function on the classes {1, 2, . . . , N}. If we sample M times from this pmf, we recover a Dirichlet-
multinomial for which the expected number of unique classes grows linearly in R and only logarithmically in M.
This may be representative of the documents which comprise of an internet corpus. The vast majority will belong
to one of several prominent classes such as academic literature, articles, question-answer, etc. However, there may
be several documents which are rather spurious in relation to the remainder of the corpus. Therefore, we permit
the number of document types N to potentially be exponentially large, but we assume that the scale parameter R
remains modest.
Each of the N elements of the mixture corresponds to a transformer network as outlined in Section 7.3. Let K denote
the context lengths of the transformers, L denote their depths, and r their attention dimensions. Let ψ1, . . . , ψN
denote the weights of the transformer mixture elements. As a result, the meta-parameters ψ = (α, ψ1, . . . , ψN) i.e.
the mixture sampling weights and the mixture model weights.
For all t, the generation of token X(m)
t+1 will depend on the sampled mixture index im ∈{1, . . . , N}, the corresponding
transformer weights ψim and the previous K tokens X(m)
t−K+1, . . . , X(m)
t
. We use U (m)
t,ℓ
to denote the output of layer
ℓat time t for document m. For all m, t, we let (U (m)
t,0
= ϕt−K+1:t) be the embeddings associated with the past
K tokens.
For ℓ> 0, we let U (m)
t,ℓ
denote the output of layer ℓof the transformer with input U (m)
t,0 .
For all
t ≤T, i < L, m ≤M, let
Attn(ℓ) 
U (m)
t,ℓ−1

= Softmax
 
U (m)⊤
t,ℓ−1 A(m)
ℓ
U (m)
t,ℓ−1
√r
!
denote the attention matrix of layer ℓfor document m where the softmax function is applied elementwise along the
columns. The matrix A(m)
ℓ
∈ℜr×r can be interpreted as the product of the key and query matrices and we assume
that the elements of the matrices A(m)
ℓ
are distributed iid N(0, 1).
Subsequently, we let
U (m)
t,ℓ
= Clip

V (m)
ℓ
U (m)
t,ℓ−1Attn(ℓ) 
U (m)
t,ℓ−1

,
where Clip ensures that each column of the matrix input has L2 norm at most 1. The matrix V (m)
ℓ
resembles the
value matrix and we assume that the elements of V (m)
ℓ
are distributed iid N(0, 1/d).
Finally, the next token is generated via sampling from the softmax of the final layer:
X(m)
t+1 ∼Softmax

U (m)
t,L [−1]

,
where U (m)
t,L [−1] denotes the right-most column of U (m)
t,L . At each layer ℓ, the parameters consist of the matrices
A(m)
ℓ
, V (m)
ℓ
. As a result, the intra-task parameters consist of θm = (im, ψim) i.e. the mixture index and the weights
of the corresponding transformer model.
8.6.2
Theoretical Building Blocks
To obtain the tightest bounds for this problem instance, instead of consider two separate rate-distortion functions
as in Theorem 47, we must instead analyze the combined rate-distortion function defined below:
HM,T,ϵ(ψ, θ1:M) =
inf
˜θ∈˜ΘM,T,ϵ
I(ψ, θ1:M; ˜θ),
61

where
˜ΘM,T,ϵ =
(
˜θ : ˜θ ⊥HM,T |θ1:M;
I(HM,T ; ψ, θ1:M|˜θ)
MT
≤ϵ
)
.
Under this rate-distortion function, we obtain the following bounds on estimation error:
Theorem 51. (rate-distortion estimation error bound 2) For all M, T ∈Z+,
LM,T ≤inf
ϵ≥0
HM,T,ϵ(ψ, θ1:M)
MT
+ ϵ,
and
LM,T ≥sup
ϵ≥0
min
HM,T,ϵ(ψ, θ1:M)
MT
, ϵ

.
Proof. We begin by establishing the upper bound.
LM,T = I(HM,T ; ψ, θ1:M)
MT
=
inf
˜θ∈˜ΘM,T,ϵ
I(HM,T ; ψ, θ1:M, ˜θ)
MT
=
inf
˜θ∈˜ΘM,T,ϵ
I(HM,T ; ˜θ)
MT
+ I(HM,T ; θ1:M|˜θ)
MT
≤HM,T,ϵ(ψ, θ1:M)
MT
+ ϵ.
We now establish the lower bound. Suppose that I(HM,T ; ψ, θ1:M) < HM,T,ϵ(ψ, θ1:M). Let ˜θ = ˜HM,T /∈˜ΘM,T,ϵ
where ˜HM,T is another independent history sampled in the same manner as HM,T .
I(HM,T ; ψ, θ1:M) = H(HM,T ) −H(HM,T |ψ, θ1:M)
(a)
= H(HM,T ) −H(HM,T |ψ, θ1:M, ˜θ)
(b)
≥H(HM,T |˜θ) −H(HM,T |ψ, θ1:M, ˜θ)
= I(HM,T ; ψ, θ1:M|˜θ)
(c)
≥ϵMT
where (a) follows from conditional independence assumptions, (b) follows from the fact that conditioning re-
duces entropy, and c) follows from the fact that ˜θ /∈˜ΘM,T,ϵ.
Therefore, for all ϵ ≥0, I(HM,T ; ψ, θ1:M) ≥
min{HM,T,ϵ(ψ, θ1:M), ϵMT}. The result follows.
With this rate-distortion bound in place, we established that in order to bound the estimation error of ICL, we
simply must bound the rate-distortion function.
In the following result, we establish an upper bound on the
rate-distortion function for the ICL problem.
Lemma 52. (in-context learning rate-distortion upper bound) For all d, r, K, L, M, T, if for all (m, t) ∈
[M] × [T], X(m)
t
is generated by the ICL process, then
HM,T,ϵ(ψ, θ1:M) ≤M ln(N) + R ln

1 + M
R

· r · max{r, d}L ln

1 + r · max{r, d}KLT(8K(1 + 16K))L
ϵ

.
Proof. For the ICL data generating process, we will construct a compression which incrementally builds up elements
of the mixture. Concretely, let ˜θ = (i1, i2, . . . , iM, ˜ψ) where ψ = { ˜ψj : j ∈{i1, i2, . . . , iM}} and ˜ψj represents a
noisy version of ψj. We will use IM to denote the set of unique indices from {i1, i2, . . . , iM}.
Let ˜ψj = ( ˜Vj,ℓ, ˜Aj,ℓ: ℓ∈[L]) where

˜Vj,ℓ, ˜Aj,ℓ

= (Vj,ℓ+ Zj,ℓ, Aj,ℓ+ Bj,ℓ)
62

(Zj,ℓ, Bj,ℓ) ⊥(Vj,ℓ, Aj,ℓ),
Zj,ℓ
iid
∼
(
N(0, ϵ′/r2)
if ℓ< L
N(0, ϵ′/(rd)
if ℓ= L ;
Bj,ℓ
iid
∼N(0, ϵ′/r2),
and ϵ′ =
ϵ
KLT (8K(1+16K))L .
We begin by upper bounding the rate.
I(ψ, θ1:M; ˜θ) = I(ψ, θ1:M; (i1, . . . , iM), ˜ψ)
= I(ψ, (i1, . . . , iM); (i1, . . . , iM), ˜ψ)
= H(i1, . . . , iM) + I(ψ; ˜ψ|(i1, . . . , iM))
≤M ln(N) + E
" N
X
i=1
1[i∈IM] · I(ψi; ˜ψi)
#
(a)
≤M ln(N) + R ln

1 + M
R

· r · max{r, d}L ln

1 + r · max{r, d}KLT(8K(1 + 16K))L
ϵ

,
where (a) follows from Lemmas 33 and 43.
We now upper bound the distortion:
I(HM,T ; ψ, θ1:M|˜θ)
MT
= I(HM,T ; θ1:M|˜θ)
MT
=
M
X
m=1
I(X(m)
0:T ; θ1:M|˜θ, Hm−1,T )
MT
≤I(X(1)
0:T ; θ1:M|˜θ)
T
= I(X(1)
0:T ; θ1| ˜ψi1)
T
(a)
≤ϵ,
where (a) follows from Lemma 43.
8.6.3
Main Result
With the rate-distortion results established from the previous section, we establish the following upper bound on
the estimation error of in-context learning.
Theorem 53. (in-context learning estimation error upper bound) For all d, r, L, K, R, N, M, T ∈Z++, if
for all (m, t) ∈[M] × [T], X(m)
t
is generated according to the ICL process, then
LM,T ≤r max{r, d}RL2 ln
 1 + M
R

ln (8Ke(1 + 16K))
MT
+
r max{r, d}RL ln
 1 + M
R

ln

2KMT 2
L

MT
+ ln(N)
T
.
Proof.
LM,T ≤inf
ϵ≥0
HM,T,ϵ(ψ, θ1:M)
MT
+ ϵ
≤inf
ϵ≥0
ln(N)
T
+
r max{r, d}RL ln
 1 + M
R

ln

1 + r max{r,d}KLT (8K(1+16K))L
ϵ

MT
+ ϵ
≤ln(N)
T
+
r max{r, d}RL ln
 1 + M
R

ln

1 + KMT 2(8K(1+16K))L
L

MT
+ r max{r, d}RL2
MT
≤ln(N)
T
+
r max{r, d}RL ln
 1 + M
R

ln

2KMT 2(8K(1+16K))L
L

MT
+ r max{r, d}RL2
MT
≤ln(N)
T
+ r max{r, d}RL2 ln
 1 + M
R

ln (8Ke(1 + 16K))
MT
+
r max{r, d}RL ln
 1 + M
R

ln

2KMT 2
L

MT
63

Notably, the term which decays linearly in T indicates that once the mixture is learned, the only information which
must be deduced from each document is its mixture index. As a result, the error grows logarithmically in N (the
size of the mixture) and decays linearly in T (the length of the intra-task sequence). Notably, the first term scales
linearly in the product of the parameter count and depth of the transformer model. Furthermore it also scales
linearly in R ln(1 + M/R), the expected number of unique mixture elements from M documents. Notably, this
component of the error decays linearly in MT, so it will decay both as the number of documents and the number
of tokens per document grow. As a result, for a pre-training dataset with sufficiently large M, this portion of the
error could become negligible, which would indicate that the loss incurred by ICL would eventually be ≈ln(N)/T.
This is intuitive as once the relevant mixture elements have been learned, the algorithm just has to disambiguate
which index the next document belongs to.
Summary
• Meta-learning consists of a random process (X(m)
t
: m, t ∈Z++) of observations.
For any fixed m,
(X(m)
0
, X(m)
1
, . . .) represents the data associated with a particular task indexed by m.
• The meta-parameters ψ, represent information which is shared across tasks.
• The intra-task parameters (θ1, θ2, . . .) are iid when conditioned on ψ and for all m, X(m)
0:∞⊥ψ|θm.
• A meta-learning algorithm produces for each (m, t) ∈[M] × [T], a predictive distribution Pm,t of X(m)
t+1
after observing the concatenated history which we denote by
Hm,t =

X(1)
0:T , X(2)
0:T , . . . , X(m−1)
0:T
, X(m)
0:t

.
• For all M, T ∈Z++, we measure the error realized by our predictions Pm,t in terms of the average
cumulative expected log-loss:
LM,T,π =
1
MT
M
X
m=1
T −1
X
t=0
Eπ
h
−ln Pm,t

X(m)
t+1
i
.
Since this error is not baselined at 0, we elect to study the following estimation error:
LM,T,π = LM,T,π −H (HM,T |ψ, θ1:M)
MT
.
• (Bayesian posterior is optimal) For all m, t ∈Z+,
E
h
−ln ˆPm,t

X(m)
t+1
i
= min
π
Eπ
h
−ln Pm,t

X(m)
t+1
i
.
• (estimation error) For all M, T ∈Z+,
LM,T = I(HM,T ; ψ)
MT
|
{z
}
meta
estimation
error
+
M
X
m=1
I(X(m)
0:T ; θm|ψ)
MT
|
{z
}
intra-task
estimation
error
.
• We define the rate-distortion function for meta-parameters ψ as
Hϵ,M,T (ψ) =
inf
˜
ψ∈˜Ψϵ,M,T
I(ψ; ˜ψ),
64

where
Ψϵ,M,T =
(
˜ψ : ˜ψ ⊥HM,T ;
I(HM,T ; ψ| ˜ψ)
MT
≤ϵ
)
.
• We define the rate-distortion function for intra-task parameters θm conditioned on ψ as
Hϵ,T (θm|ψ) =
inf
˜θm∈˜Θϵ,T
I(θm; ˜θm|ψ),
where
˜Θϵ,T =
(
˜θ : ˜θ ⊥HM,T
θm;
I(X(m)
0:T ; θm|˜θ, ψ)
T
≤ϵ
)
.
• (rate-distortion estimation error bound) For all M, T ∈Z+, and m ∈{1, . . . , M},
LM,T ≤inf
ϵ≥0
Hϵ,M,T (ψ)
MT
+ ϵ
+
inf
ϵ′≥0
Hϵ′,T (θm|ψ)
T
+ ϵ′,
and
LM,T ≥sup
ϵ≥0
min
Hϵ,M,T (ψ)
MT
, ϵ

+
sup
ϵ′≥0
min
Hϵ′,T (θm|ψ)
T
, ϵ′

.
• (linear representation learning estimation error bound) For all d, r, M, T ∈Z++, if for all (m, t),
X(m)
t
is generated according to the linear representation learning process, then
LM,T ≤dr ln
 e
 1 + M
r

2MT
+ r ln
 e
 1 + 2T
r

2T
.
• We define the combined rate-distortion function of (ϕ, θ1:M) as
HM,T,ϵ(ψ, θ1:M) =
inf
˜θ∈˜ΘM,T,ϵ
I(ψ, θ1:M; ˜θ),
where
˜ΘM,T,ϵ =
(
˜θ : ˜θ ⊥HM,T |θ1:M;
I(HM,T ; ψ, θ1:M|˜θ)
MT
≤ϵ
)
.
• (rate-distortion estimation error bound) For all M, T ∈Z+,
LM,T ≤inf
ϵ≥0
HM,T,ϵ(ψ, θ1:M)
MT
+ ϵ,
and
LM,T ≥sup
ϵ≥0
min
HM,T,ϵ(ψ, θ1:M)
MT
, ϵ

.
• (in-context learning estimation error upper bound) For all vocabulary sizes d, embedding dimen-
sions r, depths L, context lengths K, scale parameters R, mixture size N, and M, T ∈Z++, if for all
(m, t) ∈[M] × [T], X(m)
t
is generated according to the ICL process, then
LM,T ≤r max{r, d}RL2 ln
 1 + M
R

ln (8Ke(1 + 16K))
MT
+
r max{r, d}RL ln
 1 + M
R

ln

2KMT 2
L

MT
+ ln(N)
T
.
65

9
Misspecification
All of the results presented in this text have involved bounding the estimation error of an optimal agent with a
correctly specified prior distribution. While in general little can be said about the performance of an arbitrarily
suboptimal agent, in this section, we first provide some general theoretical results which may allow one to study the
error of a suboptimal agent under certain problem settings. Afterwards, we apply the results to study the impact
of misspecification in the Bayesian linear regression setting.
9.1
General Theoretical Results
We begin with the following general result which exactly characterizes the loss incurred by an arbitrary prediction
˜Pt which may depend on the previous data Ht. Recall that P ∗
t (·) denotes P(Yt+1 ∈·|θ, Ht) and ˆPt(·) denotes
P(Yt+1 ∈·|Ht).
Theorem 54. (loss of an arbitrary algorithm) For all T ∈Z+, if for all t ∈[T], ˜Pt is a predictive distribution
which may depend on the previous data Ht, then
1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥˜Pt(·)
i
= I(HT ; θ)
T
+ 1
T
T −1
X
t=0
E
h
dKL

ˆPt(·)
 ˜Pt(·)
i
|
{z
}
misspecification error
.
Proof.
1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥˜Pt(·)
i
= 1
T
T −1
X
t=0
E

ln P ∗
t (Yt+1)
˜Pt(Yt+1)

= 1
T
T −1
X
t=0
E
"
ln P ∗
t (Yt+1)
ˆPt(Yt+1)
#
+ E
"
ln
ˆPt(Yt+1)
˜Pt(Yt+1)
#
= I(HT ; θ)
T
+ 1
T
T −1
X
t=0
E
h
dKL

ˆPt(·)
 ˜Pt(·)
i
.
This result states that for a suboptimal prediction ˜Pt, the additional error incurred (misspecification error) is
characterized by the KL divergence between the optimal prediction ˜Pt = P(Yt+1 ∈·|Ht) and ˜Pt. We note that
under many circumstances, this KL divergence may be difficult to study. However, if we restrict our attention to
predictions which are suboptimal due to a misspecified prior distribution ˜P(θ ∈·), we have the following upper
bound:
Theorem 55. (misspecified prior error bound) For all T ∈Z++, if for all t ∈[T], ˜Pt(·) is the Bayesian
posterior of Yt+1 conditioned on Ht under the prior ˜P(θ ∈·), then
1
T
T −1
X
t=0
E
h
dKL

ˆPt(·)
 ˜Pt(·)
i
≤
dKL

P(θ ∈·)∥˜P(θ ∈·)

T
.
Proof.
1
T
T −1
X
t=0
E
h
dKL

ˆPt(·)
 ˜Pt(·)
i (a)
= 1
T dKL

P(HT ∈·)
 ˜P (HT ∈·)

(b)
≤
dKL

P(θ ∈·) ∥˜P(θ ∈·)

T
,
where (a) follows from the chain rule of KL divergence and (b) follows from the data processing inequality of KL
Divergence.
66

We note that in instances for which the numerator of the RHS is finite, this provides strong guarantees. This
stipulates that if ˜P never assigns probability 0 to an event A for which P(θ ∈A) > 0, then the misspecification
error will decay linearly in T. However, it may often be the case ˜P(θ ∈·) may only have support on a subset of
the support of P(θ ∈·). In such instances, our upper bound is vacuous as the KL divergence will be ∞.
For such instances with mismatched supports, it is prudent to reframe the behavior of the agent as learning and
making inferences with respect to a misspecified learning target.
Such agents learn about a random variable
˜θ : Ω7→˜Θ for which ˜Θ ⊆Θ and behave as if θ = ˜θ. This may reflect machine learning algorithms in the non-
realizable setting which is becoming more and more relevant with the increasing complexity of problems (natural
language, robotics, etc). Concretely, for all t, we assume that this agent produces a prediction
˜Pt(·) =
X
ν∈˜Θ
P

˜θ = ν|Ht

· P (Yt+1 ∈·|θ = ν, Ht) .
Note that this prediction integrates over realizations of ˜θ and posits a predictive distribution for Yt+1 as if θ were
the realization. While we present the above for discrete random variable ˜θ, natural extensions exist by replacing
the sum with an integral and the probability measures with their Radon-Nikodymn derivatives w.r.t the Lebesgue
measure. If ˜Θ ⊂Θ, then it’s evident that the error incurred by ˜Pt will not vanish to 0 if P(θ ∈Θ \ ˜Θ) > 0. The
following result provides a very intuitive upper bound on the error incurred by ˜Pt.
Theorem 56. (misspecified learner error bound) For all T ∈Z++ and random variables ˜θ : Ω7→˜Θ for which
˜Θ ⊆Θ, if
˜Pt(·) =
X
ν∈˜Θ
P

˜θ = ν|Ht

· P (Yt+1 ∈·|θ = ν, Ht) ,
then
1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥˜Pt(·)
i
≤
I(HT ; ˜θ)
T
|
{z
}
estimation error
+ 1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥P

Yt+1 ∈·|θ ←˜θ, Ht
i
|
{z
}
misspecification error
.
67

Proof.
1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥˜Pt(·)
i
= 1
T
T
X
t=0
E

dKL

P ∗
t (·)

X
ν∈˜Θ
P(Yt+1 ∈·|θ = ν, Ht) · P(˜θ = ν|Ht)




= 1
T
T −1
X
t=0
I(Xt+1; θ|Ht) + E

dKL

P(Yt+1 ∈·|Ht)

X
ν∈˜Θ
P(Yt+1 ∈·|θ = ν, Ht) · P(˜θ = ν|Ht)




= 1
T
T −1
X
t=0
I(Xt+1; θ|Ht)
+ 1
T
T −1
X
t=0
E

X
ν∈˜Θ
X
y∈Y
P(Yt+1 = y|Ht) · P(˜θ = ν|Yt+1 = y, Ht) ln
P(Yt+1 = y|Ht)
P
ν∈˜Θ P(Yt+1 = y|θ = ν, Ht) · P(˜θ = ν|Ht)


= 1
T
T −1
X
t=0
I(Xt+1; θ|Ht)
+ 1
T
T −1
X
t=0
E

X
ν∈˜Θ
X
y∈Y
P(Yt+1 = y|Ht) · P(˜θ = ν|Yt+1 = y, Ht) ln
P
ν∈˜Θ P(Yt+1 = y|Ht) · P(˜θ = ν|Yt+1 = y, Ht)
P
ν∈˜Θ P(Yt+1 = y|θ = ν, Ht) · P(˜θ = ν|Ht)


(a)
≤1
T
T −1
X
t=0
I(Xt+1; θ|Ht)
+ 1
T
T −1
X
t=0
E

X
y∈Y
X
ν∈˜Θ
P(Yt+1 = y|Ht) · P(˜θ = ν|Yt+1 = y, Ht) ln P(Yt+1 = y|Ht) · P(˜θ = ν|Yt+1 = y, Ht)
P(Yt+1 = y|θ = ν, Ht) · P(˜θ = ν|Ht)


= 1
T
T −1
X
t=0
I(Xt+1; θ|Ht)
+ 1
T
T −1
X
t=0
E
h
dKL

P(˜θ ∈·|Yt+1, Ht)∥P(˜θ ∈·|Ht)
i
+ E
h
dKL

P(Yt+1 ∈·|Ht)∥P(Yt+1 ∈·|θ ←˜θ, Ht)
i
= 1
T
T −1
X
t=0
E [dKL (P(Yt+1 ∈·|θ, Ht)∥P(Yt+1 ∈·|Ht))]
+ 1
T
T −1
X
t=0
I(Yt+1; ˜θ|Ht) + E
h
dKL

P(Yt+1 ∈·|Ht)∥P(Yt+1 ∈·|θ ←˜θ, Ht)
i
= 1
T
T −1
X
t=0
I(Yt+1; ˜θ|Ht) + E
h
dKL

P(Yt+1 ∈·|θ, Ht)∥P(Yt+1 ∈·|θ ←˜θ, Ht)
i
= I(HT ; ˜θ)
T
+ 1
T
T −1
X
t=0
E
h
dKL

P(Yt+1 ∈·|θ, Ht)∥P(Yt+1 ∈·|θ ←˜θ, Ht)
i
,
where (a) follows from the log-sum inequality.
This result is rather intuitive as it involves an estimation error term which grows with the number of nats that ˜θ
contains about HT . Note that the estimation error will always decay to 0 as T →∞so long as I(HT ; θ)/T also
decays to 0 as T →∞. Meanwhile, the misspecification error signifies the error which persists even with access to
infinite data. Note that when ((Xt, Yt+1) : t ∈Z+) is an iid process conditioned on θ, each term in the sum will be
equal and hence reduce to E[dKL(P(Yt+1 ∈·|θ, Xt)∥P(Yt+1 ∈·|θ ←˜θ, Xt))]. We note that often this quantity can
be simple to bound for concrete problem instances and was used as an intermediate step in the derivation of many
earlier proofs in this paper.
68

In regards to this quantity, we note the following result which states that the prediction P(Yt+1 ∈·|˜θ, Ht) experiences
less error than the prediction which results from the change of measure: P(Yt+1 ∈·|θ ←˜θ, Ht).
Lemma 57. If ˜θ : Ω7→˜Θ is a random variable such that for all t ∈Z+, Yt+1 ⊥˜θ|θ, Ht and ˜Θ ⊆Θ, then
E
h
dKL(P(Yt+1 ∈·|θ, Ht)∥P(Yt+1 ∈·|˜θ, Ht))
i
≤E
h
dKL(P(Yt+1 ∈·|θ, Ht)∥P(Yt+1 ∈·|θ ←˜θ, Ht))
i
.
Proof.
E
h
dKL(P(Yt+1 ∈·|θ, Ht)∥P(Yt+1 ∈·|θ ←˜θ, Ht))
i
= E

ln
P(Yt+1|θ, Ht)
P(Yt+1|θ ←˜θ, Ht)

= E

ln P(Yt+1|θ, Ht)
P(Yt+1|˜θ, Ht)

+ E
"
ln
P(Yt+1|˜θ, Ht)
P(Yt+1|θ ←˜θ, Ht)
#
= E
h
dKL(P(Yt+1 ∈·|θ, Ht)∥P(Yt+1 ∈·|˜θ, Ht))
i
+ E
"
E
"
ln
P(Yt+1|˜θ, Ht)
P(Yt+1|θ ←˜θ, Ht)
˜θ, Ht
##
= E
h
dKL(P(Yt+1 ∈·|θ, Ht)∥P(Yt+1 ∈·|˜θ, Ht))
i
+ E
h
dKL

P(Yt+1 ∈·|˜θ, Ht)∥P(Yt+1 ∈·|θ ←˜θ, Ht)
i
(a)
≥E
h
dKL(P(Yt+1 ∈·|θ, Ht)∥P(Yt+1 ∈·|˜θ, Ht))
i
,
where (a) follows from the fact that KL-divergence is non-negative.
In the section to follow, we run through two concrete examples in the linear regression setting which demonstrate
use of Theorems 55 and 56.
9.2
Linear Regression with Misspecification
In this section, we will apply Theorems 54 and 55 to study the impact of misspecification in the linear regression
setting from earlier in the text. We select this setting as the existence of a conjugate prior allows us to draw
additional insights about the misspecification error.
9.2.1
Data Generating Process
We restate the linear regression setting here for the reader’s convenience. In linear regression, the variable that we
are interested in estimating is a random vector θ ∈ℜd. We assume a prior distribution P(θ ∈·) = N(0, Id/d). For
all t ∈Z+, inputs and outputs are generated according to a random vector Xt
iid
∼N(0, Id) and
Yt+1 = θ⊤Xt + Wt+1,
where Wt
iid
∼N(0, σ2) for known variance σ2.
9.2.2
Misspecified Algorithms
In this section, we will study the estimation error of two misspecified linear regression algorithms.
The first
algorithm will be misspecified in that it will assume a prior distribution ˜P(θ ∈·) = N(µ, Id), where µ ̸= 0. The
second algorithm will make predictions while failing to model the impact of one of the features.
The following result upper bounds the misspecification error incurred by an algorithm which performs Bayesian
inference with respect to a misspecified prior distribution ˜P(θ ∈·) = N(µ, Id).
Theorem 58. (mean misspecification error upper bound) For all d, T ∈Z++, if for all t ∈[T], ˜Pt returns
a predictive distribution of Yt+1 conditioned on Ht under prior distribution ˜P(θ ∈·) = N(µ, I), then
1
T
T
X
t=0
E
h
dKL

ˆPt∥˜Pt
i
≤∥µ∥2
2
2T
69

Proof.
1
T
T
X
t=0
E
h
dKL

ˆPt∥˜Pt
i (a)
≤
dKL

P(θ ∈·) ∥˜P(θ ∈·)

T
(b)
= ∥µ∥2
2
2T ,
where (a) follows from Theorem 55 and (b) follows from the formula of KL divergence between two multivariate
normal distributions.
We notice that in this setting, the misspecification error decays to 0 as T →∞. This is intuitive as in this setting,
the algorithm, though suboptimal, does not assign 0 probability mass to any events with truly have non-zero
probability. In the following example, we will study what occurs when this is not the case and rather the algorithm
is blind to certain aspects of the underlying data generating process.
We now study the misspecification error of an algorithm which ignores the influence of the final dimension of θ.
Conceretely, if θi denotes the ith dimension of θ ∈ℜd, then ˜θ = (θ1, θ2, . . . , θd−1, 0). We assume that for all t, the
algorithm produces a prediction
˜Pt (·) =
Z
ν∈ℜd−1 P(Yt+1 ∈·|θ = ν, Xt) · p˜θ(ν|Ht) dµ(ν),
where p˜θ(·|Ht) is the Radon-Nikodym derivative of P(˜θ ∈·|Ht) w.r.t. the Lebesgue measure µ. We now derive an
upper bound for the error incurred by this algorithm.
Theorem 59. (misspecification error of missing feature agent) For all d, T ∈Z++, if for all t ∈{0, 1, . . . , T−
1}, (Xt, Yt+1) is generated by the linear regression process and
˜Pt(·) =
Z
ν∈ℜd−1 P(Yt+1 ∈·|θ = ν, Xt) · p˜θ(ν|Ht) dµ(ν),
then,
1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥˜Pt(·)
i
≤d −1
2T
 1
σ2 + ln

1 + T
d

+
1
dσ2 .
70

Proof. Let ¯θ = ˜θ + Z where Z ⊥˜θ and Z1:d−1 ∼N(0, ϵId−1), Zd = 0 for ϵ = 1/T. Then,
I(HT ; ˜θ)
T
= 1
T
T −1
X
t=0
I(Yt+1; ˜θ|Ht)
= 1
T
T −1
X
t=0
I(Yt+1; ¯θ|Ht) + I(Yt+1; ˜θ|¯θ, Ht)
(a)
≤1
T
T −1
X
t=0
I(Yt+1; ¯θ|Ht) + I(Yt+1; θ|¯θ, Ht)
= I(¯θ; HT )
T
+ 1
T
T −1
X
t=0
I(Yt+1; θ|¯θ, Ht)
≤I(¯θ; θ)
T
+ 1
2E
"
ln
 
1 +
 θ⊤Xt −¯θ⊤Xt
2
σ2
!#
(b)
≤d −1
2T
ln

1 + 1
dϵ

+ 1
2 ln

1 +
E
h Z⊤Xt,1:d−1
2i
+ 1
d
σ2


= d −1
2T
ln

1 + 1
dϵ

+ 1
2 ln

1 + (d −1)ϵ + 1
d
σ2

= d −1
2T
ln

1 + T
d

+ 1
2 ln
 
1 +
(d−1)
T
+ 1
d
σ2
!
≤d −1
2T
ln

1 + T
d

+ d −1
2Tσ2 +
1
2dσ2 ,
where (a) follows from the data processing inequality and (b) follows from Jensen’s inequality.
1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥P

Yt+1 ∈·|θ ←˜θ, Ht
i
= E
h
dKL

P ∗
t (·)∥P

Yt+1 ∈·|θ ←˜θ, Xt
i
=

θ⊤Xt −˜θ⊤Xt
2
2σ2
=
1
2dσ2 .
Therefore, the result follows:
1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥˜Pt(·)
i
≤I(HT ; ˜θ)
T
+ 1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥P

Yt+1 ∈·|θ ←˜θ, Ht
i
≤d −1
2T
ln

1 + T
d

+ d −1
2Tσ2 +
1
2dσ2 +
1
2dσ2
= d −1
2T
 1
σ2 + ln

1 + T
d

+
1
dσ2 ,
where (a) follows from Theorem 56.
Note that the first term describes the error which reduces to 0 as T →∞. Meanwhile, the second term represents
irreducible error which comes from the fact that ˜θ ignores the influence of the final feature.
9.3
Neural Scaling Laws
More broadly, results such as Theorem 56 can allow us to more carefully consider the trade-offs which occur in the
process of machine learning. This is already becoming apparent in the phenomenon of neural scaling laws [Hoffmann
71

et al., 2022, Kaplan et al., 2020] which aim to optimally balance the error due to misspecification and estimation
for a fixed FLOP budget. The FLOP count is roughly the product of the parameter count of the trained model
and the size of the training dataset. Evidently, increasing the parameter count would reduce misspecification error,
potentially at the cost of greater estimation error. Meanwhile, increasing the dataset will reduce the estimation
error. As a result, for a fixed FLOP count, it is not immediately clear how much should be allocated to parameter
count vs dataset size to achieve the smallest loss. Jeon and Roy [2024] provide a theoretical study of how compute-
bound algorithms ought to effectively trade-off these two sources of error in learning by leveraging the tools in this
section. We hope that this work provides the foundational groundwork for future mathematical analyses of neural
scaling laws.
Summary
• Recall that P ∗
t (·) = P(Yt+1 ∈·|θ, Ht), ˆPt(·) = P(Yt+1 ∈·|Ht).
• (loss of an arbitrary algorithm) For all T ∈Z+, if for all t ∈[T], ˜Pt is a predictive distribution
which may depend on the previous data Ht, then
1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥˜Pt(·)
i
= I(HT ; θ)
T
+ 1
T
T −1
X
t=0
E
h
dKL

ˆPt(·)
 ˜Pt(·)
i
|
{z
}
misspecification error
.
• (misspecified prior error bound) For all T ∈Z++, if for all t ∈[T], ˜Pt(·) is the Bayesian posterior
of Yt+1 conditioned on Ht under the prior ˜P(θ ∈·), then
1
T
T −1
X
t=0
E
h
dKL

ˆPt(·)
 ˜Pt(·)
i
≤
dKL

P(θ ∈·)∥˜P(θ ∈·)

T
.
• For instances in which ˜P(θ ∈·) assigns 0 probability mass to a set A for which P(θ ∈A) > 0, the above
bound is vacuous.
• In such instances, it is prudent to instead consider the result of the following learner which performs
inference on a separate learning target ˜θ and performs predictions as if θ = ˜θ:
• (misspecified learner error bound) For all T ∈Z++ and random variables ˜θ : Ω7→˜Θ for which
˜Θ ⊆Θ, if
˜Pt(·) =
X
ν∈˜Θ
P

˜θ = ν|Ht

· P (Yt+1 ∈·|θ = ν, Ht) ,
then
1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥˜Pt(·)
i
≤
I(HT ; ˜θ)
T
|
{z
}
estimation error
+ 1
T
T −1
X
t=0
E
h
dKL

P ∗
t (·)∥P

Yt+1 ∈·|θ ←˜θ, Ht
i
|
{z
}
misspecification error
.
• Results like the one above can provide theoretical insights into how estimation error and misspecification
error ought to be traded off with access to limited data or compute.
72

10
Conclusion
Summary. In this paper, we provided a rigorous mathematical framework to analyze achievable performance
in machine learning. By taking a Bayesian framing, we were able to discover and leverage connections to Shan-
non’s information theory. In doing so, we established the intimate connection between learning and optimal lossy
compression via rate-distortion theory, providing a streamlined process for deriving error bounds for a plethora of
settings ranging from simple to complex. Regardless of whether the data is iid under an unknown distribution,
exhibits sequential structure, or even hierarchical structure, our general results apply and provide accurate insights.
We concluded with a section which extends our framework to the study of misspecified algorithms.
Future Research. Our framework leaves open many directions of future research. One exciting direction involves
expanding upon the neural scaling laws developed by Jeon and Roy [2024] to provide actionable insights to prac-
titioners. Rate-distortion lower bounds are also another interesting topic as we largely ignore the lower bounds
in this work outside of linear regression. To what extent tools such as the Donsker-Varadhan lower bound can
be leveraged to derive general rate-distortion lower bounds is left as future work. As new puzzling and complex
phenomena continue to emerge in the field of machine learning, we hope that the reader can leverage the tools of
this monograph to achieve clarity.
Acknowledgements
Financial support from the NSF GRFP fellowship and the Army Research Office (ARO) Grant W911NF2010055
is gratefully acknowledged.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint
arXiv:2303.08774, 2023.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions
on Information Theory, 39(3):930–945, 1993.
Peter L Bartlett and Shahar Mendelson.
Rademacher and gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.
Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear VC-dimension bounds for piecewise polynomial
networks. Neural computation, 10(8):2159–2173, 1998.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and pseu-
dodimension bounds for piecewise linear neural networks. Journal of Machine Learning Research, 20(63):1–17,
2019.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and
Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954.
Thomas M Cover and Joy A Thomas. Elements of Information Theory. John Wiley & Sons, 2012.
Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern,
Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness.
Language modeling is compression. In The Twelfth International Conference on Learning Representations, 2024.
URL https://openreview.net/forum?id=jznbgiynus.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic)
neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.
Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear
neural networks. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on Learning
Theory, volume 65 of Proceedings of Machine Learning Research, pages 1064–1068. PMLR, 07–10 Jul 2017. URL
https://proceedings.mlr.press/v65/harvey17a.html.
73

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego
de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican,
George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W.
Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.
Edwin T Jaynes and Oscar Kempthorne. Confidence intervals vs Bayesian intervals. In Foundations of Probability
Theory, Statistical Inference, and Statistical Theories of Science: Proceedings of an International Research Col-
loquium held at the University of Western Ontario, London, Canada, 10–13 May 1973 Volume II Foundations
and Philosophy of Statistical Inference, pages 175–257. Springer, 1976.
Hong Jun Jeon and Benjamin Van Roy. Information-theoretic foundations for neural scaling laws, 2024. URL
https://arxiv.org/abs/2407.01456.
Hong Jun Jeon, Jason D. Lee, Qi Lei, and Benjamin Van Roy. An information-theoretic analysis of in-context
learning, 2024.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei.
Scaling laws for neural language models.
arXiv preprint
arXiv:2001.08361, 2020.
David A McAllester. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual conference on Compu-
tational learning theory, pages 230–234, 1998.
Daniel Russo and James Zou. How much does your data exploration overfit? controlling bias via information usage.
IEEE Transactions on Information Theory, 66(1):302–323, 2019.
Leonard J Savage. The foundations of statistics. Courier Corporation, 1972.
Claude E Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423,
1948.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with
deep neural networks and tree search. Nature, 529(7587):484–489, 2016.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 ieee information
theory workshop (itw), pages 1–5. IEEE, 2015.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint
physics/0004057, 2000.
Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and
Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms.
Advances in neural information processing systems, 30, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
(still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.
Yifan Zhu, Hong Jun Jeon, and Benjamin Van Roy. Is stochastic gradient descent near optimal?, 2022.
74

Appendix
A
Lower Bounds for Linear Regression
We now introduce a lemma relating expected KL divergence to mean-squared error, a distortion measure that is
prevalent in the literature. This relation will allow us to derive a lower bound for the rate-distortion function in
the Gaussian linear regression setting.
Lemma 60. For all d ∈Z++ and σ2 ≥0, if θ : Ω7→ℜd has iid components that are each ν2-subgaussian and
symmetric, X ∼N(0, Id), and Y ∼N(θ⊤X, σ2), then for all proxies ˜θ ∈˜Θ, Y −E[Y |˜θ, X] is 4ν2∥X∥2
2 + σ2-
subgaussian conditioned on X.
Proof.
E
h
eλ(Y −E[Y |˜θ,X])X
i (a)
= E
h
eλ(Y −E[Y |θ,X])X
i
· E
h
eλ(E[Y |θ,X]−E[Y |˜θ,X])X
i
= e
λ2σ2
2
· E
h
eλ((θ−E[θ|˜θ])⊤X)X
i
(b)
≤e
λ2σ2
2
· E
h
e−λ(θ⊤X) · E
h
e−λ(θ⊤X)|˜θ, X
i X
i
(c)
≤e
λ2σ2
2
E
h
E[e−λ(θ⊤X)|˜θ, X]2X
i
≤e
λ2σ2
2
E
h
E[e−2λ(θ⊤X)|˜θ, X]
X
i
= e
λ2σ2
2
E
h
e−2λ(θ⊤X)X
i
(d)
= e
λ2σ2
2
e2λ2ν2∥X∥2
2
= e
λ2(σ2+4ν2∥X∥2
2)
2
,
where (a) follows from Y −E[Y |θ, X] = W which is independent from E[Y |θ, X] −E[Y |˜θ, X], (b) follows from
the fact that θ⊤X is symmetric conditioned on X and Jensen’s inequality, (c) follows from the fact that eθ⊤X =
E[eθ⊤X|θ, ˜θ, X], and (d) follows from the fact that the components of θ are ν2-subgaussian.
Lemma 61. If Y −E[Y |˜θ, X] is ν2-subgaussian conditional on X, then for all α > 1, Y −E[Y |˜θ, X] is αν2-
subgaussian conditional on (˜θ, X).
Proof. Assume that for some α > 1, there exists an event S s.t. P(˜θ ∈S) > 0 and ˜θ ∈S implies that Y −E[Y |˜θ, X]
is not αν2-subgaussian conditioned on (˜θ, X). We have that
E
h
eλ(Y −E[Y |˜θ,X])X
i
≥P(˜θ ∈S) · E
h
eλ(Y −E[Y |˜θ,X])|˜θ ∈S, X
i
(a)
> eln P(˜θ∈S) · e
αλ2ν2
2
= e
λ2(αν2+ 2
λ2 ln P(˜
θ∈S))
2
= e
λ2(ν2+(α−1)ν2+ 2
λ2 ln P(˜
θ∈S))
2
,
where (a) holds for all λ s.t. |λ| ≥|λ∗| for some λ∗. Such λ∗exists because of the fact that ˜θ ∈S implies that
Y −E[Y |˜θ, X] is not αν2-subgaussian conditioned on (˜θ, X). As a result, for λ such that λ2 > max
n
2 ln P(˜θ∈S)
(1−α)ν2 , λ2
∗
o
,
we have that
E
h
eλ(Y −E[Y |˜θ,X])X
i
> e
λ2ν2
2 ,
which is a contradiction since Y −E[Y |˜θ, X] is ν2-subgaussian conditional on X. Therefore the assumption that
there exists α > 1 and ˜θ s.t. is not αν2-subgaussian conditional on (X, ˜θ) cannot be true. The result follows.
75

Lemma 62. If Y −E[Y |˜θ, X] is ν2-subgaussian conditioned on (˜θ, X), then
E



E[Y |θ, X] −E[Y |˜θ, X]
2
2ν2

≤E
h
dKL(P(Y ∈·|θ, X)∥P(Y ∈·|˜θ, X))
i
.
Proof. We begin by stating the Donsker-Varadhan variational form of the KL-divergence.
For all probability
distributions P(·) and Q(·) over ℜsuch that P is absolutely continuous with respect to Q, for densities dP, dQ
w.r.t the Lebesgue measure,
dKL(P(·)∥Q(·)) =
sup
g:ℜ→ℜ
Z
y∈ℜ
g(y)dP(y) −ln
Z
y∈ℜ
eg(y)dQ(y)

,
where the supremum is taken over measurable functions for which
R
y∈ℜg(y)dP(y) is well-defined and the expression
on the right
R
y∈ℜeg(y)dQ(y) is finite.
Let P(·) = P(Y ∈·|θ, Xt), Q(·) = P(Y ∈·|˜θ, X), and Z = Y −E[Y |˜θ, X]. Then, for arbitrary λ ∈R, applying the
variational form of KL-divergence with g(Y ) = λZ gives us
dKL(P(Y ∈·|θ, X)∥P(Y ∈·|˜θ, X))
(a)
= dKL(P(Y ∈·|θ, ˜θ, X)∥P(Y ∈·|˜θ, X))
≥λE
h
Z|θ, ˜θ, X
i
−ln E
h
eλZ|˜θ, X
i
(b)
≥λ

E[Y |θ, X] −E[Y |˜θ, X]

−λ2ν2
2
,
where (a) follows from Y ⊥˜θ|(θ, X) and (b) follows from Z being ν2-subgaussian conditioned on (˜θ, X). Since the
above holds for arbitrary λ, maximizing the RHS w.r.t λ give us:
dKL(P(Y ∈·|θ, X)∥P(Y ∈·|˜θ, X)) ≥

E[Y |θ, X] −E[Y |˜θ, X]
2
2ν2
.
The result follows from taking an expectation on both sides.
We establish a lower bound by first finding a suitable lower bound for the distortion function. For subgaussian
random vectors, the following lemma allows us to lower bound the expected KL-divergence distortion by a multiple
of the mean squared error.
Lemma 63. For all ˜θ ∈˜Θ, d ∈Z++ and σ2 ∈ℜ++, if θ : Ω7→ℜd consists of iid components each of which are
1/d-subgaussian and symmetric, P(X ∈·) ∼N(0, Id), and if Y ∼N(θ⊤X, σ2), then
E

∥X∥2
2
2(4∥X∥2
2 + dσ2)

E
h
∥θ −E[θ|˜θ]∥2
2
i
≤I(Y ; θ|˜θ, X).
Proof. θ⊤X is ∥X∥2
2/d-subgaussian conditioned on X and by Lemma 60, Y −E[Y |˜θ, X] is 4∥X∥2
2/d+σ2-subgaussian
conditioned on ∥X∥2
2. Lemma 61 then states that Y −E[Y |˜θ, X] is is α(4∥X∥2
2/d + σ2)-subgaussian conditioned on
76

(˜θ, X) for all α > 1. Therefore,
E
h
dKL(P(Y ∈·|θ, X)∥P(Y ∈·|˜θ, X))
i (a)
≥lim
α↓1 E



(θ −E[θ|˜θ])⊤X
2
2α

4∥X∥2
2
d
+ σ2



= E



(θ −E[θ|˜θ])⊤X
2
2

4∥X∥2
2
d
+ σ2



= E

E



(θ −E[θ|˜θ])⊤X
2
2

4∥X∥2
2
d
+ σ2

∥X∥2
2




= E


∥X∥2
2
d
E
h
∥(θ −E[θ|˜θ]∥2
2
i
2

4∥X∥2
2
d
+ σ2



(b)
= E

∥X∥2
2
2(4∥X∥2
2 + dσ2)

E
h
∥θ −E[θ|˜θ]∥2
2
i
where (a) follows from Lemma 62 and (b) follows from the fact that X ∼N(0, Id).
B
Theoretical Results for Dirichlet-Multinomial
B.1
General Combinatorics Results
We begin with the folloiwing general combinatorics results that we will eventually apply to bound the estimation
error of the finite N Dirichlet-Multinomial process.
Lemma 64. For all m, j, n, K, N ∈Z++ s.t. j < n, if
Cm(j) =
n−1
X
i=j
K
N
K + i ·
n−1
X
i=j+1
K
N
K + i · . . . ·
n−1
X
i>j+m−1
K
N
K + i
|
{z
}
m
,
then
1
m!
Km
N m lnm

K + n
K + m −1 + j

≤Cm(j) ≤1
m!
Km
N m lnm

K + n
K −1 + j

.
Proof. We first prove the upper bound via induction. Base Case: n = 1
C1(j) =
n−1
X
i=j
K
N
K + i
≤K
N
Z n
j−1
1
K + xdx
= K
N ln

K + n
K −1 + j

77

Assume inductive hypothesis is true for m = k.
Ck+1(j) =
n−1
X
i=j
K
N
K + i · Ck(i + 1)
≤K
N
Z n
j−1
1
K + x · Ck(x + 1)
≤Kk+1
N k+1
1
k!
Z n
j−1
1
K + x · lnk
K + n
K + x

dx
≤−Kk+1
N k+1
1
(k + 1)! · lnk+1
K + n
K + x
 
n
j−1
= Kk+1
N k+1
1
(k + 1)! lnk+1

K + n
K −1 + j

.
We now prove the lower bound also via induction. Base Case: m = 1
C1(j) =
n−1
X
i=j
K
N
K + i
≥K
N
Z n
j
1
K + xdx
= K
N ln
K + n
K + j

Assume inductive hypothesis is true for m = k.
Ck+1(j) =
n−1
X
i=j
K
N
K + i · Ck(i + 1)
≥K
N
Z n
j
1
K + x · Ck(x + 1)
≥Kk+1
N k+1
1
k!
Z n
j
1
K + x · lnk

K + n
K + k + x

dx
≥−Kk+1
N k+1
1
(k + 1)! · lnk+1

K + n
K + k + x
 
n
j
= Kk+1
N k+1
1
(k + 1)! lnk+1
K + k + n
K + k + j

≥Kk+1
N k+1
1
(k + 1)! lnk+1

K + n
K + k + j

.
The result follows.
Lemma 65. For all i, n, K, N ∈Z++, if 2 ≤K ≤
√
N and n ≤N, then
1
(2i)!
K2i
N 2i ln2i
K + n
K −1

−
1
(2i + 1)!
K2i+1
N 2i+1 ln2i+1
 K + n
K + 2i

≥0.
78

Proof.
0
(a)
≤1 −
1
2i + 1
K
N ln
K + n
K −1

= ln2i
K + n
K −1

−
1
2i + 1
K
N ln2i+1
K + n
K −1

≤ln2i
K + n
K −1

−
1
2i + 1
K
N ln2i+1
 K + n
K + 2i

=
1
(2i)!
K2i
N 2i ln2i
K + n
K −1

−
1
(2i + 1)!
K2i+1
N 2i+1 ln2i+1
 K + n
K + 2i

where (a) follows for all n ≤(K −1) e
3N
K −K which is implied by n ≤N for K ≥2.
Lemma 66. For all n, K, N ∈Z++, if 2 ≤K ≤
√
N and n ≤N, then
n−1
Y
i=0
 
1 −
K
N
K + i
!
≥1 −K
N ln

1 + n
K

.
Proof.
n−1
Y
i=0
 
1 −
K
N
K + i
!
(a)
≥1 −
⌊n−1
2
⌋
X
i=0
1
(2i + 1)!
K2i+1
N 2i+1 ln2i+1
 K + r
K + 2i

+
⌈n−1
2
⌉
X
i=1
1
(2i)!
K2i
N 2i ln2i
K + r
K −1

(b)
≥1 −K
N ln

1 + n
K

+
⌊n−1
2
⌋
X
i=1
1
(2i)!
K2i
N 2i ln2i
K + n
K −1

−
1
(2i + 1)!
K2i+1
N 2i+1 ln2i+1
 K + n
K + 2i

(c)
≥1 −K
N ln

1 + n
K

,
where (a) follows from Lemma 64, and (b) follows from Lemma 65
B.2
Lemmas pertaining to Dirichlet Multinomial
The following result upper bounds the expected number of unique classes drawn from a Dirichlet-multinomial
distribution with n draws and α = [K/N, . . . , K/N] ∈ℜN.
Lemma 67. For all n, K, N ∈Z++ s.t. K ≤
√
N and n ≤N, if ˜θ′ ∼DirMult(n, α) for α = [K/N, . . . , K/N] ∈ℜN,
then
E
" N
X
i=1
1[˜θ′
i>0]
#
≤K ln

1 + n
K

.
79

Proof.
E
" N
X
i=1
1[˜θ′
i>0]
#
= N · P(˜θ′
i > 0)
(a)
= N ·

1 −P(˜θ′
1 = 0, ˜θ′
2 + · · · + ˜θ′
N = n)

= N ·
 
1 −Γ(K)Γ(n + 1)
Γ(n + K)
·
Γ
  K
N

Γ
  K
N

Γ (1) ·
Γ
 n + K
N (N −1)

Γ
  K
N (N −1)

Γ (n + 1)
!
= N ·
 
1 −
Γ(K)
Γ(n + K) · Γ
 n + K −K
N

Γ
 K −K
N

!
= N ·
 
1 −
Qn−1
i=0
 K −K
N + i

Qn−1
i=0 K + i
!
= N ·
 
1 −
n−1
Y
i=0
 
1 −
K
N
K + i
!!
(b)
≤N ·

1 −

1 −K
N ln

1 + n
K

= K ln

1 + n
K

.
where (a) follows from the aggregation property of the Dirichlet-multinomial distribution and (b) follows from
Lemma 66.
Note that this upper bound is independent of N. In the next section, we will apply the dominated convegence
theorem to bound the number of unique basis functions drawn by our learning model.
The extention to a Dirichlet Process (N →∞) follows trivially.
Lemma 68. For all n, K ∈Z++, if θ is distributed according to a Dirichlet process with base distribution
Uniform(Sd−1) with scale parameter K and ˜θ ∼Multinomial(θ), then
E
" X
w∈W
1[˜θw>0]
#
≤K ln

1 + n
K

.
Proof.
E
" X
w∈W
1[˜θw>0]
#
= E
"
E
" X
w∈W
1[˜θw>0]
W
##
(a)
= E

E

lim
N→∞
X
w∈˜
W
1[˜θ′w>0]
W




(b)
=
lim
N→∞E

E

X
w∈˜
W
1[˜θ′
w>0]
W




(c)
≤
lim
N→∞K ln

1 + n
K

= K ln

1 + n
K

,
where in (a), WN is a subset of the first N elements of W and ˜XN is DirMult(n, αN) where αN = [K/N, . . . , K/N] ∈
ℜN and WN is the set of classes, (b) follows from the dominated convergence theorem since | P
w∈˜
W 1[˜θ′w>0]| ≤n,
and (c) follows from Lemma 67.
80

