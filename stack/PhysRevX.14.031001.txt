How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model
Francesco Cagnetta ,1,*,† Leonardo Petrini ,1,* Umberto M. Tomasini ,1
Alessandro Favero ,1,2 and Matthieu Wyart1,‡
1Institute of Physics, EPFL, Lausanne, Switzerland
2Institute of Electrical Engineering, EPFL, Lausanne, Switzerland
(Received 19 December 2023; revised 15 April 2024; accepted 31 May 2024; published 1 July 2024)
Deep learning algorithms demonstrate a surprising ability to learn high-dimensional tasks from limited
examples. This is commonly attributed to the depth of neural networks, enabling them to build a hierarchy
of abstract, low-dimensional data representations. However, how many training examples are required to
learn such representations remains unknown. To quantitatively study this question, we introduce the
random hierarchy model: a family of synthetic tasks inspired by the hierarchical structure of language and
images. The model is a classification task where each class corresponds to a group of high-level features,
chosen among several equivalent groups associated with the same class. In turn, each feature corresponds to
a group of subfeatures chosen among several equivalent groups and so on, following a hierarchy of
composition rules. We find that deep networks learn the task by developing internal representations
invariant to exchanging equivalent groups. Moreover, the number of data required corresponds to the point
where correlations between low-level features and classes become detectable. Overall, our results indicate
how deep networks overcome the curse of dimensionality by building invariant representations and provide
an estimate of the number of data required to learn a hierarchical task.
DOI: 10.1103/PhysRevX.14.031001
Subject Areas: Complex Systems,
Interdisciplinary Physics,
Statistical Physics
I. INTRODUCTION
Deep learning methods exhibit superhuman perfor-
mances in areas ranging from image recognition [1] to
Go playing [2]. However, despite these accomplishments,
we still lack a fundamental understanding of their working
principles. Indeed, Go configurations and images lie in
high-dimensional spaces, which are hard to sample due
to the curse of dimensionality: The distance δ between
neighboring data points decreases very slowly with their
number P, as δ ¼ OðP−1=dÞ, where d is the space dimen-
sion. Solving a generic task such as regression of a
continuous function [3] requires a small δ, implying
that P must be exponential in the dimension d. Such a
number of data is unrealistically large: For example, the
benchmark dataset ImageNet [4], whose effective dimen-
sion is estimated to be ≈50 [5], consists of only ≈107 data,
significantly smaller than e50 ≈1020. This immense differ-
ence implies that learnable tasks are not generic, but highly
structured. What is then the nature of this structure, and
why are deep learning methods able to exploit it?
A popular idea attributes the efficacy of these methods
to their ability to build a useful representation of the
data, which becomes increasingly complex across the
layers [6]. Interestingly, a similar increase in complexity
is also found in the visual cortex of the primate brain
[7,8]. In simple terms, neurons closer to the input learn to
detect simple features like edges in a picture, whereas
those deeper in the network learn to recognize more
abstract features, such as faces [9,10]. Intuitively, if these
representations are also invariant to aspects of the data
unrelated to the task, such as the exact position of an
object in a frame for image classification [11], they may
effectively reduce the dimensionality of the problem and
make it tractable. This view is supported by several
empirical studies of the hidden representations of trained
networks. In particular, measures such as the mutual
information between such representations and the input
[12,13], their intrinsic dimensionality [14,15], and their
sensitivity toward transformations that do not affect the
task (e.g., smooth deformations for image classification
[16,17]), all eventually decay with the layer depth.
However, none of these studies addresses the sample
*These authors contributed equally to this work.
†Corresponding author: francesco.cagnetta@epfl.ch
‡Corresponding author: matthieu.wyart@epfl.ch
Published by the American Physical Society under the terms of
the Creative Commons Attribution 4.0 International license.
Further distribution of this work must maintain attribution to
the author(s) and the published article’s title, journal citation,
and DOI.
PHYSICAL REVIEW X 14, 031001 (2024)
2160-3308=24=14(3)=031001(24)
031001-1
Published by the American Physical Society

complexity, i.e., the number of training data necessary for
learning such representations, and thus the task.
In this paper, we study the relationship between sample
complexity, depth of the learning method, and structure of
the data by focusing on tasks with a hierarchically composi-
tional structure—arguably a key property for the learn-
ability of real data [18–25]. To provide a concrete example,
consider a picture that consists of several high-level
features like face, body, and background. Each feature is
composed of subfeatures like ears, mouth, eyes, and nose
for the face, which can be further thought of as combina-
tions of low-level features such as edges [26]. Recent
studies have revealed that deep networks can represent
hierarchically compositional functions with far fewer
parameters than shallow networks [21], implying an
information-theoretic lower bound on the sample complex-
ity which is only polynomial in the input dimension [24].
While these works offer important insights, they do not
characterize the performance of deep neural networks
trained with gradient descent (GD).
We investigate this question by adopting the physicist’s
approach [27–32] of introducing a model of synthetic data,
which is inspired by the structure of natural problems,
yet simple enough to be investigated systematically. This
model (Sec. II) belongs to a family of hierarchical classi-
fication problems where the class labels generate the input
data via a hierarchy of composition rules. These problems
were introduced to highlight the importance of input-to-
label correlations for learnability [19] and were found to be
learnable via an iterative clustering algorithm [22]. Under
the assumption of randomness of the composition rules,
we show empirically that shallow networks suffer from
the curse of dimensionality (Sec. III), whereas the sample
complexity P of deep networks (both convolutional net-
works and multilayer perceptrons) is only polynomial in
the size of the input. More specifically, with nc classes
and L composition rules that associate m equivalent low-
level representations to each class or high-level feature,
P ≃ncmL asymptotically in m (Sec. III).
Furthermore, we find that P coincides with both (a) the
number of data that allows for learning a representation
that is invariant to exchanging the m semantically equiv-
alent low-level features (Sec. III A) and (b) the size of the
training set for which the correlations between low-level
features and class label become detectable (Sec. IV). We
prove for a simplified architecture trained with gradient
descent that (a) and (b) must indeed coincide. Via (b), P
can be derived analytically under our assumption of
randomness of the composition rules.
A. Relationship to other models of data structure
Characterizing
the
properties
that
make
high-
dimensional data learnable is a classical problem in
statistics. Typical assumptions that allow for avoiding
the curse of dimensionality include (i) data lying on a
low-dimensional
manifold
and
(ii)
the
task
being
smooth [33]. For instance, in the context of regression,
the sample complexity is not controlled by the bare input
dimensionality d, but by the ratio dM=s [34–36], where dM
is the dimension of the data manifold and s the number of
bounded derivatives of the target function. However, dM is
also large in practice [5]; thus, keeping dM=s low requires
an unrealistically large number of bounded derivatives.
Moreover, properties (i) and (ii) can already be leveraged
by isotropic kernel methods, and thus cannot account for
the significant advantage of deep learning methods in many
benchmark datasets [37]. Alternatively, learnability can be
achieved when (iii) the task depends on a small number of
linear projections of the input variables, such as regression
of a target function fðxÞ ¼ gðxtÞ, where x ∈Rd and
xt ∈Rt [38–41]. Methods capable of learning features
from the data can leverage this property to achieve a
sample complexity that depends on t instead of d [42].
However, one-hidden-layer networks are sufficient for that;
hence, this property does not explain the need for deep
architectures.
In the context of statistical physics, the quest for a model
of data structure has been pursued within the framework
of teacher-student models [43–45], where a teacher uses
some ground truth knowledge to generate data, while a
student tries to infer the ground truth from the data. The
structural properties (i)–(iii) can be incorporated into this
approach [28,46]. In addition, using a shallow convolu-
tional network as a teacher allows for modeling (iv) the
locality of imagelike datasets [31,47,48]. In the context of
regression, this property can be modeled with a function
fðxÞ ¼ P
i f
i ðxiÞ where the sum is on all patches xi of t
adjacent pixels. Convolutional networks learn local tasks
with a sample complexity controlled by the patch dimen-
sion t [47], even in the “lazy” regime [49,50] where they do
not learn features. However, locality does not allow for
long-range nonlinear dependencies in the task. It might
be tempting to include these dependencies by considering
a deep convolutional teacher network, but then the
sample complexity would be exponential in the input
dimension d [25].
The present analysis based on hierarchical generative
models shows that properties (i)–(iii) are not necessary to
beat the curse of dimensionality. Indeed, for some choices
of the parameters, the model generates all possible
d-dimensional sequences of input features, which violates
(i). Additionally, changing a single input feature has a finite
probability of changing the label, violating the smoothness
assumption (ii). Finally, the label depends on all of the d
input variables of the input, violating (iii). Yet, we find that
the sample complexity of deep neural networks is only
polynomial in d. Since locality is incorporated hierarchi-
cally in the generative process, it generates long-range
dependencies in the task, but it can still be leveraged by
building a hierarchical representation of the data.
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-2

II. RANDOM HIERARCHY MODEL
In this section, we introduce our generative model, which
can be thought of as an L-level context-free grammar—
a generative model of language from formal language
theory [51]. The model consists of a set of class labels
C ≡f1; …; ncg
and
L
disjoint
vocabularies
Vl ≡
fal
1; …; alvlg of low- and high-level features. As illustrated
in Fig. 1, left-hand panel, data are generated from the class
labels. Specifically, each label generates m distinct high-
level representations via m composition rules of the form
α ↦μðLÞ
1 ; …; μðLÞ
s
for α ∈C
and
μðLÞ
i
∈VL;
ð1Þ
having size s > 1. The s elements of these representations
are high-level features μðLÞ
i
such as background, face, and
body for a picture. Each high-level feature generates in turn
m lower-level representations via other m rules,
μðlÞ ↦μðl−1Þ
1
;…;μðl−1Þ
s
for μðlÞ∈Vl;
μðl−1Þ
i
∈Vl−1;
ð2Þ
from l ¼ L down to l ¼ 1. The input features μð1Þ
represent low-level features such as the edges in an image.
Because of the hierarchical structure of the generative
process, each datum can be represented as a tree of
branching factor s and depth L, where the root is the class
label, the leaves are the input features, and the hidden nodes
are the level-l features with l ¼ 2; …; L.
In addition, for each level l, there are m distinct rules
emanating from the same higher-level feature μðlÞ; i.e.,
there are m equivalent lower-level representations of μðlÞ
(see Fig. 1, right-hand panel, for an example with m ¼ 3).
Following the analogy with language, we refer to these
equivalent representations as synonyms. We assume that a
single low-level representation there is only one high-level
feature that generates it, i.e., that there are no ambiguities.
Since the number of distinct s-tuples at level l is bounded
by vs
l, this assumption requires mvlþ1 ≤vs
l for all
l ¼ 1; …; L (with vLþ1 ≡nc). If m ¼ 1, each label gen-
erates only a single datum and the model is trivial. For
m > 1, the number of data per class grows exponentially
with the input dimension d ¼ sL:
m × ms ×    × msL−1 ¼ m
PL−1
i¼0 si ¼ mðd−1Þ=ðs−1Þ:
ð3Þ
In particular, in the case where mvlþ1 ¼ vs
l, the model
generates all the possible data made of d features in V1.
Instead, for mvlþ1 < vs
l, the set of available input data is
given by the application of the composition rules; therefore,
it inherits the hierarchical structure of the model.
Let us remark that, due to the nonambiguity assumption,
each set of composition rules can be summarized with a
function gl that associates s-tuples of level-l features to the
corresponding level-(l þ 1) feature. The domain of gl is a
subset of Vs
l consisting of the mvlþ1 s-tuples generated by
the features at level (l þ 1). Using these functions, the label
α ≡μðLþ1Þ of an input datum μð1Þ ¼ ðμð1Þ
1 ; …; μð1Þ
d Þ can be
written as a hierarchical composition of L local functions
of s variables [20,21]:
μðlþ1Þ
i
¼ gl

μðlÞ
ði−1Þsþ1; …; μðlÞ
ði−1Þsþ1

;
ð4Þ
for i ¼ 1; …; sL−l and l ¼ 1; …; L.
FIG. 1.
The random hierarchy model. Left: structure of the generative model. The class label α ¼ 1; …; nc generates a set of m
equivalent (i.e., synonymic) high-level representations with elements taken from a vocabulary of high-level features VL. Similarly,
high-level features generate m equivalent lower-level representations, taken from a vocabulary VL−1. Repeating this procedure L −2
times yields all the input data with label α, consisting of low-level features taken from V1. Right: example of random hierarchy
model with nc ¼ 2 classes, L ¼ 3, s ¼ 2, m ¼ 3, and homogeneous vocabulary size v1 ¼ v2 ¼ v3 ¼ 3. The three sets of rules are
listed at the top, while two examples of data generation are shown at the bottom. The first example is obtained by following the rules
in the colored boxes.
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-3

Note that, while we keep s and m constant throughout
the levels for ease of exposition, our results can be
generalized without additional effort. Likewise, we will
set the vocabulary size to v for all levels. To sum up, a
single classification task is specified by the parameters nc,
v, m, and s and by the L composition rules. In the random
hierarchy model (RHM) the composition rules are chosen
uniformly at random over all the possible assignments of m
representations of s low-level features to each of the v
high-level features. An example of binary classification
task (nc ¼ 2), with s ¼ 2, L ¼ 3, and v ¼ m ¼ 3, is shown
in Fig. 1, right-hand panel, together with two examples of
label-input pairs. Notice that the random choice induces
correlations between low- and high-level features. In
simple terms, each of the high-level features, e.g., the
level-2 features d, e, or f in the figure, is more likely to be
represented with a certain low-level feature in a given
position, e.g., i on the right for d, g on the right for e, and h
on the right for f. These correlations are crucial for our
predictions and are analyzed in detail in Appendix C.
III. SAMPLE COMPLEXITY
OF DEEP NEURAL NETWORKS
The main focus of our work is the answer to the
following question.
How much data is required to learn a typical instance of
the random hierarchy model with a deep neural network?
Thus, after generating an instance of the RHM with fixed
parameters nc, s, m, v, and L, we train neural networks of
varying depth with stochastic gradient descent (SGD) on a
set of P training points. The training points are sampled
uniformly at random without replacement from the set of
available RHM data; hence, they are all distinct. We adopt a
one-hot encoding of the input features, so that each input
point x is a (d × v)-dimensional sequence where, for
i ¼ 1; …; d and ν ∈V1,
xi;ν ¼
(
1
if μð1Þ
i
¼ ν
0
otherwise:
ð5Þ
All our experiments consider overparametrized networks,
which we achieve in practice by choosing the width H of
the network’s hidden layers such that (i) training loss
reaches 0 and (ii) test accuracy does not improve by
increasing H. To guarantee representation learning as H
grows, we consider the maximal update parametrization
[52], equivalent to having the standard H−1=2 scaling of the
hidden layer weights plus an extra factor of H−1=2 at the last
layer. Further details of the machine learning methods can
be found in Appendix A.
(a) Shallow networks are cursed. Let us begin with
the sample complexity of two-layer fully connected
networks. As shown in Fig. 2, in the maximal case
nc ¼ v, m ¼ vs−1 these networks learn the task only if
trained on a significant fraction of the total number of
data Pmax. From Eq. (3),
Pmax ¼ ncmðd−1Þ=ðs−1Þ;
ð6Þ
which equals vsL in the maximal case. The bottom
panel of Fig. 2, in particular, highlights that the
number of training data required for having a test
error ϵ ≤0.7ϵrand, with ϵrand ¼ 1 −n−1
c
denoting the
error of a random guess of the label, is proportional to
Pmax. Since Pmax is exponential in d, this is an instance
of the curse of dimensionality.
(b) Deep networks break the curse. For networks having a
depth larger than that of the RHM L, the test error
displays a sigmoidal behavior as a function of the
training set size. This finding is illustrated in the top
panels of Figs. 3 and 4 (and Fig. 12 of Appendix F for
varying nc) for convolutional neural networks (CNNs)
of depth L þ 1 (details in Appendix A). Similar results
are obtained for multilayer perceptions of depth > L,
as shown in Appendix F. All these results suggest the
existence of a well-defined number of training data at
which the task is learned. Mathematically, we define
the sample complexity P as the smallest training set
size P such that the test error ϵðPÞ is smaller than
ϵrand=10. The bottom panels of Figs. 3 and 4 (and
Figs. 12 and 13) show that
P ≃ncmL ⇔P
nc
≃dlnðmÞ= lnðsÞ;
ð7Þ
FIG. 2.
Sample complexity of two-layer fully connected net-
works, for L ¼ s ¼ 2 and v ¼ nc ¼ m. Top: test error versus the
number of training data. Different colors correspond to different
vocabulary sizes v. Bottom: number of training data resulting in
test error ¯ϵ ¼ 0.7 as a function of Pmax, with the black dashed line
indicating a linear relationship.
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-4

independently of the vocabulary size v. Since P is a
power of the input dimension d ¼ sL, the curse
of dimensionality is beaten, which evidences the
ability of deep networks to harness the hierarchical
compositionality of the task. It is crucial to note,
however, that this ability manifests only in feature
learning regimes, e.g., under the maximal update
parametrization considered in this work. Conversely,
as shown in Fig. 14 of Appendix F for the maximal
case nc ¼ v, m ¼ vs−1, deep networks trained in
the “lazy” regime [49]—where they do not learn
features—suffer from the curse of dimensionality,
even when their architecture is matched to the struc-
ture of the RHM.
We now turn to study the internal representations of
trained networks and the mechanism that they employ to
solve the task.
A. Emergence of synonymic invariance
in deep CNNs
A natural approach to learning the RHM would be to
identify the sets of s-tuples of input features that correspond
to the same higher-level feature, i.e., synonyms. Identifying
synonyms at the first level would allow for replacing each
s-dimensional patch of the input with a single symbol,
reducing the dimensionality of the problem from sL to sL−1.
Repeating this procedure L times would lead to the class
labels and, consequently, to the solution of the task.
To test if deep networks trained on the RHM resort to a
similar solution, we introduce the synonymic sensitivity,
which is a measure of the invariance of a function with
respect to the exchange of synonymic low-level features.
Mathematically, we define Sk;l as the sensitivity of the
kth layer representation of a deep network with respect to
exchanges of synonymous s-tuples of level-l features.
Namely,
Sk;l ¼ hkfkðxÞ −fkðPlxÞk2ix;Pl
hkfkðxÞ −fkðyÞk2ix;y
;
ð8Þ
where fk is the sequence of activations of the kth layer in
the network, Pl is an operator that replaces all the level-l
tuples with one of their m −1 synonyms chosen uniformly
at random, h·i with subscripts x, y denotes average over
pairs of input data of an instance of the RHM, and the
subscript Pl denotes average over all the exchanges of
synonyms.
Figure 5 reports S2;1, which measures the sensitivity to
exchanges of synonymic tuples of input features, as a
function of the training set size P for deep CNNs trained on
RHMs with different parameters. We focused on S2;1—the
sensitivity of the second layer of the network—since a
single linear transformation of the input cannot produce an
invariant representation in general [53]. Note that all the
curves display a sigmoidal shape, signaling the existence of
a characteristic sample size which marks the emergence
of synonymic sensitivity in the learned representations.
Remarkably, by rescaling the x axis by the sample
FIG. 3.
Sample complexity of depth-(L þ 1) CNNs, for s ¼ 2
and m ¼ nc ¼ v. Top: test error versus number of training points.
Different colors correspond to different vocabulary sizes v while
the markers indicate the hierarchy depth L. Bottom: sample
complexity P corresponding to a test error ϵ ¼ 0.1ϵrand. The
empirical points show remarkable agreement with the law
P ¼ ncmL, shown as a black dashed line.
FIG. 4.
Sample complexity of depth-(L þ 1) CNNs, for s ¼ 2,
nc ¼ v and varying m ≤v. Top: test error versus number of
training points, with different colors corresponding to different
vocabulary sizes v and markers indicating the hierarchy depth L.
Bottom: sample complexity P, with the law P ¼ ncmL shown
as a black dashed line.
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-5

complexity of Eq. (7) (bottom panel of Fig. 5), curves
corresponding to different parameters collapse. We con-
clude that the generalization ability of a network relies on
the synonymic invariance of its hidden representations.
Measures of the synonymic sensitivity Sk;1 for different
layers k are reported in Fig. 6 (blue lines), showing indeed
that the layers k ≥2 become insensitive to exchanging
level-1 synonyms. Figure 6 also shows the sensitivities to
exchanges of higher-level synonyms: All levels are learned
together as P increases, and invariance to level-l exchanges
is achieved from layer k ¼ l þ 1. The test error is also
shown (gray dashed lines) to further emphasize its corre-
lation with synonymic invariance.
(a) Synonymic invariance and effective dimension. Note
that the collapse of the representations of synonymic
tuples to the same value implies a progressive reduc-
tion of the effective dimensionality of the hidden
representations, as reported in Fig. 11 of Appendix E.
IV. CORRELATIONS GOVERN SYNONYMIC
INVARIANCE
We now provide a theoretical argument for understand-
ing the scaling of P of Eq. (7) with the parameters of the
RHM. First, we compute a third characteristic sample size
Pc, defined as the size of the training set for which the local
correlations between any of the input patches and the label
become detectable. Remarkably, Pc coincides with P of
Eq. (7). Second, we demonstrate how a shallow (two-layer)
neural network acting on a single patch can use such
correlations to build a synonymic invariant representation
in a single step of gradient descent so that Pc and P also
correspond to the emergence of an invariant representation.
Last, we show empirically that removing such correlations
leads again to the curse of dimensionality, even if the
network architecture is matched to the structure of the RHM.
A. Identify synonyms by counting
Groups of input patches forming synonyms can be
inferred by counting, at any given location, the occurrences
of such patches in all the data corresponding to a given
class α. Indeed, tuples of features that appear with identical
frequencies are likely synonyms. More specifically, let us
denote xj an s-dimensional input patch for j in 1; …; sL−1,
an s-tuple of input features with μ ¼ ðμ1; …; μsÞ, and the
number of data in class α having xj ¼ μ with Njðμ; αÞ [54].
Normalizing this number by NjðμÞ ¼ P
α Njðμ; αÞ yields
the conditional probability fjðαjμÞ for a datum to belong
to class α conditioned on displaying the s-tuple μ in the
jth input patch:
fjðαjμÞ ≔Prfx ∈αjxj ¼ μg ¼ Njðμ; αÞ
NjðμÞ :
ð9Þ
If the low-level features are homogeneously spread across
classes, then f ¼ n−1
c for all α, μ, and j. In contrast, due to
the aforementioned correlations, the probabilities of the
RHM are all different from n−1
c —we refer to this difference
as signal. Distinct level-1 tuples μ and ν yield a different f
(and thus a different signal) with high probability unless μ
and ν are synonyms, i.e., they share the same level-2
FIG. 5.
Synonymic sensitivity S2;1 for a depth-(L þ 1) CNN
trained on the RHM with s ¼ 2, nc ¼ m ¼ v as a function of the
training set size (L and v as in the key). The collapse achieved
after rescaling by P ¼ ncmL highlights that the sample complex-
ity coincides with the number of training points required to build
internal representations invariant to exchanging synonyms.
FIG. 6.
Synonymic sensitivities Sk;l of the layers of a depth-
(L þ 1) CNN trained on an RHM with L ¼ 3, s ¼ 2, nc ¼ m ¼
v ¼ 8, as a function of the training set size P. The colors denote
the level of the exchanged synonyms (as in the key), whereas
different panels correspond to the sensitivity of the activations
of different layers (layer index in the gray box). Synonymic
invariance is learned at the same training set size for all layers,
and invariance to level-l exchanges is obtained from layer
k ¼ l þ 1.
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-6

representation. Therefore, this signal can be used to identify
synonymous level-1 tuples.
B. Signal versus sampling noise
When measuring the conditional class probabilities with
only P training data, the occurrences in the right-hand side
of Eq. (9) are replaced with empirical occurrences, which
induce a sampling noise on the f’s. For the identification of
synonyms to be possible, this noise must be smaller in
magnitude than the aforementioned signal—a visual rep-
resentation of the comparison between signal and noise is
depicted in Fig. 7.
The magnitude of the signal can be computed as the ratio
between the standard deviation and mean of fjðαjμÞ over
realizations of the RHM. The full calculation is presented in
Appendix C: Here we present a simplified argument based
on an additional independence assumption. Given a class α,
the tuple μ appearing in the jth input patch is determined
by a sequence of L choices—one choice per level of the
hierarchy—of one among m possible lower-level repre-
sentations. These mL possibilities lead to all the mv distinct
input s-tuples. Njðμ; αÞ is proportional to how often the
tuple μ is chosen—mL=ðmvÞ times on average. Under
the assumption of independence of the mL choices, the
fluctuations of Njðμ; αÞ relative to its mean are given by the
central limit theorem and read ½mL=ðmvÞ−1=2 in the limit of
large m. If nc is sufficiently large, the fluctuations of NjðμÞ
are negligible in comparison. Therefore, the relative fluc-
tuations of fj are the same as those of Njðμ; αÞ, and the size
of the signal is ½mL=ðmvÞ−1=2.
The magnitude of the noise is given by the ratio between
the standard deviation and mean, over independent sam-
plings of a training set of fixed size P, of the empirical
conditional probabilities ˆfjðαjμÞ. Only P=ðncmvÞ of the
training points will, on average, belong to class α while
displaying feature μ in the jth patch. Therefore, by the
convergence of the empirical measure to the true proba-
bility, the sampling fluctuations of ˆf relative to the mean
are of order ½P=ðncmvÞ−1=2—see Appendix C for a
detailed derivation. Balancing signal and noise yields
the characteristic Pc for the emergence of correlations.
For large m, nc, and P,
Pc ¼ ncmL;
ð10Þ
which coincides with the empirical sample complexity of
deep networks discussed in Sec. III.
C. Learning level-1 synonyms with one step
of gradient descent
To complete the argument, we consider a simplified
one-step gradient descent setting [55,56], where Pc marks
the number of training examples required to learn a
synonymic invariant representation. In particular, we
focus on the s-dimensional patches of the data and study
how a two-layer network acting on one of such patches
learns the first composition rule of the RHM by building
a
representation
invariant
to
exchanges
of
level-1
synonyms.
Let us then sample an instance of the RHM and P input-
label pairs ðxk;1; αkÞ with αk ≔αðxkÞ for all k ¼ 1; …; P
and xk;1 denoting the first s patch of the datum xk. The
network output reads
F NNðx1Þ ¼ 1
H
X
H
h¼1
ahσðwh · x1Þ;
ð11Þ
where the inner-layer weights wh’s have the same dimen-
sion as x1, the top-layer weights ah’s are nc dimensional,
and σðxÞ ¼ max ð0; xÞ is the rectified linear unit (ReLU)
activation function. To further simplify the problem, we
represent x1 as a vs-dimensional one-hot encoding of the
corresponding s-tuple of features. This representation is
equivalent to an orthogonalization of the input points.
In addition, the top-layer weights are initialized as i.i.d.
Gaussian with zero mean and unit variance and fixed,
whereas the wh’s are initialized with all their elements set to
1 and trained by gradient descent on the empirical cross-
entropy loss:
L ¼ 1
P
X
P
k¼1

−log
 e½F NNðxk;1ÞαðxkÞ
Pnc
β¼1 e½F NNðxk;1Þβ

:
ð12Þ
Finally, we consider the mean-field limit W →∞, so that,
at initialization, F ð0Þ
NN ¼ 0 identically.
Let us denote with μðx1Þ the s-tuple of features encoded
in x1. Because of the one-hot encoding, fhðx1Þ ≔wh · x1
coincides with the μðx1Þth component of the weight wh.
FIG. 7.
Signal versus noise illustration. The dashed function
represents the distribution of fðαjμÞ resulting from the random
sampling of the RHM rules. The solid dots illustrate the true
frequencies fðαjμÞ sampled from this distribution, with different
colors corresponding to different groups of synonyms. The
typical spacing between the solid dots, given by the width of
the distribution, represents the signal. Transparent dots represent
the empirical frequencies ˆfjðαjμÞ, with dots of the same color
corresponding to synonymous features. The spread of transparent
dots of the same color, which is due to the finiteness of the
training set, represents the noise.
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-7

This component, which is set to 1 at initialization, is
updated by (minus) the corresponding component of the
gradient of the loss in Eq. (12). Recalling also that the
predictor is 0 at initialization, we get
Δfhðx1Þ ¼ −∇ðwhÞμðx1ÞL
¼ 1
P
X
P
k¼1
X
nc
α¼1
ah;αδμðx1Þ;μðxk;1Þ

δα;αðxkÞ −1
nc

¼
X
nc
α¼1
ah;α
 ˆN1ðμðx1Þ; αÞ
P
−1
nc
ˆN1ðμÞ
P

;
ð13Þ
where ˆN1ðμÞ is the empirical occurrence of the s-tuple μ in
the first patch of the P training points and ˆN1ðμ; αÞ is the
(empirical) joint occurrence of the s-tuple μ and the class
label α. As P increases, the empirical occurrences ˆN
converge to the true occurrences N, which are invariant
for the exchange of synonym s-tuples μ. Hence, the hidden
representation is also invariant for the exchange of syno-
nym s-tuples in this limit.
This prediction is confirmed empirically in Fig. 8, which
shows the sensitivity S1;1 of the hidden representation [57]
of shallow fully connected networks trained in the setting
of this section, as a function of the number P of training
data for different combinations of the model parameters.
The bottom panel, in particular, highlights that the sensi-
tivity is close to 1 for P ≪Pc and close to 0 for P ≫Pc. In
addition, notice that the collapse of the preactivations of
synonymic tuples onto the same, synonymic invariant
value, implies that the rank of the hidden weights matrix
tends to v—the vocabulary size of higher-level features.
This low-rank structure is typical in the weights of deep
networks trained on image classification [58–61].
(a) Including all patches via weight sharing. Let us
remark that one can easily extend the one-step setting
to include the information from all the input patches,
for instance, by replacing the network in Eq. (11) with
a one-hidden-layer convolutional network with filter
size s and nonoverlapping patches. Consequently, the
empirical occurrences on the right-hand side of
Eq. (13) would be replaced with average occurrences
over the patches. However, this average results in a
reduction of both the signal and the sampling noise
contributions to the empirical occurrences by the same
factor
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
sL−1
p
. Therefore, weight sharing does not
affect the sample size required for synonymic invari-
ance in the one-step setting.
(b) Improved sample complexity via clustering. A distance-
based clustering method acting on the representations
of
Eq.
(13)
can
actually
identify synonyms at
P ≃
ﬃﬃﬃﬃﬃ
nc
p
mL ¼ Pc=
ﬃﬃﬃﬃﬃ
nc
p
, which is much smaller than
Pc in the large-nc limit. Intuitively, using a sequence
instead of a scalar amplifies the signal by a factor nc and
the sampling noise by a factor
ﬃﬃﬃﬃﬃ
nc
p
, improving the
signal-to-noise ratio. We show that this is indeed
the case in Appendix D for the maximal dataset case
nc ¼ v and m ¼ vs−1. Previous theoretical studies have
considered the possibility of intercalating clustering
steps in standard gradient descent methods [22,62], but
the question of whether deep learning methods can
achieve a similar sample complexity with standard end-
to-end training remains open.
D. Curse of dimensionality without correlations
To support the argument that learning is possible because
of the detection of local input-label correlations, we show
that their removal in the RHM leads to a sample complexity
exponential in d, even for deep networks. Removing such
correlations implies that, at any level, features are uni-
formly distributed among classes. This is achieved enforc-
ing that a tuple μ in the jth patch at level l belongs to a
class α with probability n−1
c , independently on μ, j, l, and
α, as discussed in Sec. IVA. Such procedure produces an
uncorrelated version of the RHM, which generalizes
the parity problem (realized for m ¼ v ¼ nc ¼ 2), a task
that cannot be learned efficiently with gradient-based
FIG. 8.
Synonymic sensitivity of the hidden representation
versus P for a two-layer fully connected network trained on the
first patch of the inputs of an RHM with s ¼ 2 and m ¼ v, for
varying L, v, and nc. The top panel shows the bare curves
whereas, in the bottom panel, the x axis is rescaled by
Pc ¼ ncmL. The collapse of the rescaled curves highlights that
Pc coincides with the number of training data for building a
synonymic invariant representation.
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-8

methods [63]. Indeed, deep CNNs with depth L þ 1,
trained on this uncorrelated RHM, are cursed by dimen-
sionality, as shown in Fig. 9. The CNN test error is close to
ϵrand, given by randomly guessing the label, even for
P=Pmax > 0.9, particularly for v > 2.
V. CONCLUSION
What makes real-world tasks learnable? This question
extends from machine learning to brain science [64]. To
start thinking quantitatively about it, we introduced the
random hierarchy model: a family of tasks that captures the
compositional structure of natural data. We showed that
neural networks can learn such tasks with a limited training
set, by developing a hierarchical representation of the data.
Overall, these results rationalize several phenomena asso-
ciated with deep learning.
First, our finding that for hierarchical tasks the sample
complexity is polynomial in the input dimension (and not
exponential) leads to a plausible explanation for the
learnability of real-world tasks. Moreover, our results
provide a rule of thumb for estimating the order of
magnitude of the sample complexity of benchmark data-
sets. In the case of CIFAR10 [65], for instance, having
10 classes, taking reasonable values for task parameters
such as m ∈½5; 15 and L ¼ 3, yields P ∈½103; 3 × 104,
comparable with the sample complexity of modern archi-
tectures (see Fig. 15).
Second, our results quantify the intuition that depth is
crucial to building a hierarchical representation that effec-
tively lowers the dimension of the problem, and allows for
avoiding the curse of dimensionality. On the one hand, this
result gives a foundation to the claim that deep is better than
shallow, beyond previous analyses that focused on expres-
sivity [21,24] rather than learning. On the other hand, our
result that the internal representations of trained networks
mirror the hierarchical structure of the task explains why
these representations become increasingly complex with
depth in real-world applications [9,10].
Furthermore, we provided a characterization of the
internal representations based on their sensitivity toward
transformations of the low-level features that leave the class
label unchanged. This viewpoint complements existing
ones that focus instead on the input features that maximize
the response of hidden neurons, thus enhancing the
interpretability of neural nets. In addition, our approach
bypasses several issues of previous characterizations. For
example, approaches based on mutual information [12] are
ill defined when the network representations are determin-
istic functions of the input [13], whereas those based on
intrinsic dimension [14,15] can display counterintuitive
results—see Appendix E for a deeper discussion of the
intrinsic dimension and on how it behaves in our
framework.
Finally, our study predicts a fundamental relationship
between sample complexity, correlations between low-level
features and labels, and the emergence of invariant repre-
sentations. This prediction can be tested beyond the context
of our model, for instance, by studying invariance to
exchanging synonyms in language modeling tasks.
Looking forward, the random hierarchy model is a
suitable candidate for the clarification of other open
questions in the theory of deep learning. For instance,
a formidable challenge is to obtain a detailed description
of the gradient descent dynamics of deep networks.
Indeed, dynamics may be significantly easier to analyze
in this model, since quantities characterizing the network
success, such as sensitivity to synonyms, can be delin-
eated. In addition, the model could be generalized to
describe additional properties of data, e.g., noise in the
form of errors in the composition rules or inhomogene-
ities in the frequencies at which high-level features
generate low-level representations. The latter, in particu-
lar, would generate data where certain input features are
more abundant than others and, possibly, to a richer
learning
scenario with
several
characteristic
training
set sizes.
Beyond supervised learning, in the random hierarchy
model the set of available input data inherits the hierar-
chical structure of the generative process. Thus, this model
offers a new way to study the effect of compositionality
on self-supervised learning or probabilistic generative
models—extremely powerful techniques whose under-
standing is still in its infancy.
The codes that support the findings of this paper are
openly available [66–68].
FIG. 9.
Test error of depth-(L þ 1) CNNs trained on uncorre-
lated RHM versus number P of training points rescaled with
Pmax, with s ¼ 2 and m ¼ nc ¼ v with different v (different
colors), for L ¼ 2 (top) and L ¼ 3 (bottom). Horizontal dashed
lines stand for ϵrand, given by guessing the label uniformly at
random.
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-9

ACKNOWLEDGMENTS
The authors thank Antonio Sclocchi for fruitful discus-
sions and helpful feedback on the manuscript. This work
was supported by a grant from the Simons Foundation
(No. 454953 for M. W.).
APPENDIX A: METHODS
1. RHM implementation
The code implementing the RHM is available online [66].
The inputs sampled from the RHM are represented as a
one-hot encoding of low-level features so that each input
consists of sL pixels and v channels (size sL × v). The input
pixels are whitened over channels; i.e., each pixel has zero
mean and unit variance over the channels.
2. Machine learning models
We consider both generic deep neural networks and deep
convolutional networks tailored to the structure of the RHM.
Generic deep neural networks are made by stacking fully
connected layers, i.e., linear transformations of the kind
x ∈Rdin →d−1=2
in
W · x þ b ∈Rdout;
ðA1Þ
whereW isadout × din matrixofweights,bisadout sequence
of biases, and the factor d−1=2
in
guarantees that the outputs
remain of order 1 when din is varied. Convolutional layers,
instead,act on imagelikeinputs that havea spatial dimension
d and cin channels and compute the convolution of the input
with a filter of spatial size f. This operation is equivalent to
applying the linear transformation of Eq. (A1) to input
patches of spatial size f, i.e., groups of f adjacent pixels
[dimension din ¼ ðf × cinÞ]. The output has an imagelike
structure analogous to that of the input, with spatial dimen-
sion depending on how many patches are considered. In the
nonoverlappingpatchescase,forinstance,thespatialdimen-
sion of the output is d=f.
For all layers but the last, the linear transformation is
followed by an elementwise nonlinear activation function
σ. We resort to the popular ReLU σðxÞ ¼ max ð0; xÞ. The
output dimension is always fixed to the number of classes
nc, while the input dimension of the first layer is the same
as the input data: spatial dimension sL and v channels,
flattened into a single sL × v sequence when using a fully
connected layer. The dimensionalities of the other hidden
layers are set to the same constant H throughout the
network. Following the maximal update parametrization
[69], the weights of the last layer are multiplied by an
additional factor H−1. This factor causes the output at
initialization to vanish as H grows, which induces
representation learning even in the H →∞limit. In
practice, we set H ¼ ð4–8Þ × vs. Increasing this number
further does not affect any of the results presented in
the paper.
To tailor deep CNNs to the structure of the RHM, we set
f ¼ s so that, in the nonoverlapping patches setting, each
convolutional filter acts on a group of s low-level features
that correspond to the same higher-level feature. Since the
spatial dimensionality of the input is sL and each layer
reduces it by s, the number of nonlinear layers in a tailored
CNN is fixed to the depth of the RHM L, so that the
network depth is L þ 1. Fully connected networks, instead,
can have any depth. The code for the implementation of
both architectures is available online [67].
3. Training procedure
Training is performed within the PYTORCH deep learning
framework [70]. Neural networks are trained on P training
points sampled uniformly at random from the RHM data,
using stochastic gradient descent on the cross-entropy loss.
The batch size is 128 for P ≥128 and P otherwise, the
learning rate is initialized to 10−1 and follows a cosine
annealing schedule which reduces it to 10−2 over 100
epochs. Training stops when the training loss reaches 10−3.
The corresponding code is available online [68].
The performance of the trained models is measured as
the classification error on a test set. The size of the test set is
set to minðPmax −P; 20 000Þ. Synonymic sensitivity, as
defined in Eq. (8), is measured on a test set of size
minðPmax −P; 1000Þ. Reported results for a given value
of RHM parameters are averaged over 10 jointly different
instances of the RHM and network initialization.
APPENDIX B: STATISTICS OF THE
COMPOSITION RULES
In this appendix, we consider a single composition rule,
that is the assignment of m s-tuples of low-level features to
each of the v high-level features. In the RHM these rules are
chosen uniformly at random over all the possible rules; thus
their statistics are crucial in determining the correlations
between the input features and the class label.
1. Statistics of a single rule
For each rule, we call Niðμ1; μ2Þ the number of occur-
rences of the low-level feature μ1 in position i of the
s-tuples generated by the higher-level feature μ2. The
probability of Niðμ1; μ2Þ is that of the number of successes
when drawing m (number of s-tuples associated with the
high-level feature μ2) times without replacement from a
pool of vs (total number of s-tuples with vocabulary size v)
objects where only vs−1 satisfy a certain condition (number
of s-tuples displaying feature μ1 in position i):
PrfNiðμ0;μ1Þ¼kg¼
vs−1
k
vs −vs−1
m−k
	vs
m

;
ðB1Þ
which is a hypergeometric distribution Hvs;vs−1;m, with mean
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-10

hNi ¼ m vs−1
vs ¼ m
v ;
ðB2Þ
and variance
σ2
N ≔

ðN −hNiÞ2
¼ m vs−1
vs
vs −vs−1
vs
vs −m
vs −1
¼ m
v
v −1
v
vs −m
vs −1 ⟶
m≫1 m
v ;
ðB3Þ
independently of the position i and the specific low- and
high-level features. Note that, since m ≤vs−1 with s fixed,
large m implies also large v.
2. Joint statistics of a single rule
(a) Shared high-level feature. For a fixed high-level
feature μ2, the joint probability of the occurrences
of two different low-level features μ1 and ν1 is a
multivariate hypergeometric distribution,
PrfNiðμ1; μ2Þ ¼ k; Niðν1; μ2Þ ¼ lg
¼
vs−1
k
vs−1
l
vs −2vs−1
m −k −l
	vs
m

;
ðB4Þ
giving the following covariance:
cN ≔hðNiðμ1; μ2Þ −hNiÞðNiðν1; μ2Þ −hNiÞi
¼ −m
v2
vs −m
vs −1 ⟶
m≫1 −
m
v
2 1
m :
ðB5Þ
The covariance can also be obtained via the constraint
P
μ1 Niðμ1; μ2Þ ¼ m. For any finite sequence of
identically distributed random variables Xμ with a
constraint on the sum P
μ Xμ ¼ m:
X
v
μ¼1
Xμ ¼ m ⇒
X
v
μ¼1
ðXμ −hXμiÞ ¼ 0
⇒ðXν −hXνiÞ
X
v
μ¼1
ðXμ −hXμiÞ ¼ 0
⇒
X
v
μ¼1
hðXν −hXνiÞðXμ −hXμiÞi ¼ 0
⇒Var½Xμ þ ðv −1ÞCov½Xμ; Xν ¼ 0:
ðB6Þ
In the last line, we used the identically distributed
variables hypothesis to replace the sum over μ ≠ν
with the factor (v −1). Therefore,
cN ¼ Cov½Niðμ1; μ2Þ; Niðν1; μ2Þ
¼ −Var½Niðμ1; μ2Þ
v −1
¼ −σ2
N
v −1 :
ðB7Þ
(b) Shared low-level feature. The joint probability of the
occurrences of the same low-level feature μ1 starting
from different high-level features μ2 ≠ν2 can be
written as follows,
PrfNðμ1; μ2Þ ¼ k; Nðμ1; ν2Þ ¼ lg
¼ PrfNðμ1; μ2Þ ¼ kjNðμ1; ν2Þ ¼ lg
× PrfNðμ1; ν2Þ ¼ lg
¼ Hvs−m;vs−1−l;mðkÞ × Hvs;vs−1;mðlÞ;
ðB8Þ
resulting in the following “interfeature” covariance:
cif ≔Cov½Niðμ1; μ2Þ; Niðμ1; ν2Þ ¼ −
m
v
2 v −1
vs −1 :
ðB9Þ
(c) No shared features. Finally, by multiplying both sides
of P
μ1 Nðμ1; μ2Þ ¼ m with Nðν1; ν2Þ and averaging,
we get
cg ≔Cov½Niðμ1; μ2Þ; Niðν1; ν2Þ
¼ −Cov½Niðμ1; μ2Þ; Niðμ1; ν2Þ
v −1
¼
m
v
2
1
vs −1 :
ðB10Þ
APPENDIX C: EMERGENCE OF INPUT-OUT
CORRELATIONS (Pc)
As discussed in the main text, the random hierarchy
model presents a characteristic sample size Pc correspond-
ing to the emergence of the input-output correlations. This
sample size predicts the sample complexity of deep CNNs,
as we also discuss in the main text. In this appendix,
we prove that
Pc ⟶
nc;m→∞ncmL:
ðC1Þ
1. Estimating the signal
The correlations between input features and the class
label can be quantified via the conditional probability
(over realizations of the RHM) of a data point belonging
to class α conditioned on displaying the s-tuple μ in the
jth input patch,
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-11

fjðαjμÞ ≔Prfx ∈αjxj ¼ μg;
ðC2Þ
where the notation xj ¼ μ means that the elements of
the patch xj encode the tuple of features μ. We say that the
low-level features are correlated with the output if
fjðαjμÞ ≠1
nc
;
ðC3Þ
and define a “signal” as the difference fjðαjμÞ −n−1
c . In the
following, we compute the statistics of the signal over
realizations of the RHM.
a. Occurrence of low-level features
Let us begin by defining the joint occurrences of a
class label α and a low-level feature μ1 in a given position
of the input. Using the tree representation of the model, we
will identify an input position with a set of L indices
il ¼ 1; …; s, each indicating which branch to follow when
descending from the root (class label) to a given leaf (low-
level feature). These joint occurrences can be computed by
combining the occurrences of the single rules introduced in
Appendix B. With L ¼ 2, for instance,
Nð1→2Þ
i1i2
ðμ1; αÞ ¼
X
v
μ2¼1

ms−1Nð1Þ
i1 ðμ1; μ2Þ

× Nð2Þ
i2 ðμ2; αÞ;
ðC4Þ
where
(i) Nð2Þ
i2 ðμ2; αÞ counts the occurrences of μ2 in position
i2 of the level-2 representations of α, i.e., the s-tuples
generated from α according to the second-layer
composition rule;
(ii) Nð1Þ
i1 ðμ1; μ2Þ counts the occurrences of μ1 in position
i1 of the level-1 representations of μ2, i.e., s-tuples
generated by μ2 according to the composition rule of
the first layer;
(iii) the factor ms−1 counts the descendants of the
remaining s −1 elements of the level-2 representa-
tion (m descendants per element);
(iv) the sum over μ2 counts all the possible paths of
features that lead to μ1 from α across 2 generations.
The generalization of Eq. (C4) is immediate once one takes
into account that the multiplicity factor accounting for
the descendants of the remaining positions at the lth
generation is equal to msl−1=m (sl−1 is the size of the
representation at the previous level). Hence, the overall
multiplicity factor after L generations is
1 × ms
m × ms2
m ×    × msL−1
m
¼ mðsL−1Þ=ðs−1Þ−L;
ðC5Þ
so that the number of occurrences of feature μ1 in position
i1…iL of the inputs belonging to class α is
Nð1→LÞ
i1→L ðμ1;αÞ
¼ mðsL−1Þ=ðs−1Þ−L
X
v
μ2;…;μL¼1
Nð1Þ
i1 ðμ1;μ2Þ××NðLÞ
iL ðμL;αÞ;
ðC6Þ
where we used i1→L as a shorthand notation for the tuple of
indices i1; i2; …; iL.
The same construction allows us to compute the
number of occurrences of up to s −1 features within
the s-dimensional patch of the input corresponding to the
path i2→L. The number of occurrences of a whole s-tuple,
instead, follows a slightly different rule, since there is only
one level-2 feature μ2 which generates the whole s-tuple of
level-1 features μ1 ¼ ðμ1;1; …; μ1;sÞ—we call this feature
g1ðμ1Þ, with g1 denoting the first-layer composition rule.
As a result, the sum over μ2 in the right-hand side of
Eq. (C6) disappears and we are left with
Nð1→LÞ
i2→L ðμ1; αÞ ¼ mðsL−1Þ=ðs−1Þ−L
×
X
v
μ3;…;μL¼1
Nð2Þ
i2 ½g1ðμ1Þ; μ3 ×    × NðLÞ
iL ðμL; αÞ:
ðC7Þ
Coincidentally, Eq. (C7) shows that the joint occurrences
of an s-tuple of low-level features μ1 depend on the level-2
feature corresponding to μ1. Hence, Nð1→LÞ
i2→L ðμ1; αÞ is
invariant for the exchange of μ1 with one of its synonyms,
i.e., level-1 tuples ν1 corresponding to the same level-2
feature.
b. Class probability conditioned
on low-level observations
We can turn these numbers into probabilities by normal-
izing them appropriately. Upon dividing by the total
occurrences of a low-level feature μ1 independently of
the class, for instance, we obtain the conditional probability
of the class of a given input, conditioned on the feature in
position i1…iL being μ1:
fð1→LÞ
i1→L ðαjμ1Þ ≔
Nð1→LÞ
i1→L ðμ1;αÞ
Pnc
α0¼1Nð1→LÞ
i1→L ðμ1;α0Þ
¼
Pv
μ2;…;μL¼1Nð1Þ
i1 ðμ1;μ2Þ××NðLÞ
iL ðμL;αÞ
Pv
μ2;…;μL¼1
Pnc
μLþ1¼1Nð1Þ
i1 ðμ1;μ2Þ××NðLÞ
iL ðμL;μLþ1Þ
:
ðC8Þ
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-12

Let us also introduce, for convenience, the numerator and denominator of the right-hand side of Eq. (C8):
Uð1→LÞ
i1→L ðμ1αÞ ¼
X
v
μ2;…;μL¼1
Nð1Þ
i1 ðμ1; μ2Þ ×    × NðLÞ
iL ðμL; αÞ;
Dð1→LÞ
i1→L ðμ1Þ ¼
X
nc
α¼1
Uð1→LÞ
i1→L ðμ1; αÞ:
ðC9Þ
c. Statistics of the numerator U
We now determine the first and second moments of the numerator of fð1→LÞ
i1→L ðμ1; αÞ. Let us first recall the definition for
clarity:
Uð1→LÞ
i1→L ðμ1; αÞ ¼
X
v
μ2;…;μL¼1
Nð1Þ
i1 ðμ1; μ2Þ ×    × NðLÞ
iL ðμL; αÞ:
ðC10Þ
(a) Level 1: L ¼ 1. For L ¼ 1, U is simply the occurrence of a single production rule Niðμ1; αÞ,

Uð1Þ
¼ m
v ;
ðC11Þ
σ2
Uð1Þ ≔Var

Uð1Þ
¼ m
v
v −1
v
vs −m
vs −1 ⟶
v≫1 m
v ;
ðC12Þ
cUð1Þ ≔Cov

Uð1Þðμ1; αÞ; Uð1Þðν1; αÞ

¼ −Var½Uð1Þ
ðv −1Þ ¼ −
m
v
2 vs −m
vs −1
1
m ⟶
v≫1m
v
2 1
m ;
ðC13Þ
where the relationship between variance and covariance is due to the constraint on the sum of Uð1Þ over μ1;
see Eq. (B6).
(b) Level 2: L ¼ 2. For L ¼ 2,
Uð1→2Þ
i1→2 ðμ1; αÞ ¼
X
v
μ2¼1
Nð1Þ
i1 ðμ1; μ2Þ × Nð2Þ
i2 ðμ2; αÞ ¼
X
v
μ2¼1
Nð1Þ
i1 ðμ1; μ2ÞUð2Þ
i2 ðμ2; αÞ:
ðC14Þ
Therefore,

Uð1→2Þ
¼ v
m
v

×

Uð1Þ
¼ v
m
v
2
;
ðC15Þ
σ2
Uð2Þ ≔Var

Uð1→2Þ
¼
X
v
μ2;ν2¼1

Nð1Þðμ1; μ2ÞNð1Þðμ1; ν2Þ

Uð2Þðμ2; αÞUð2Þðν2; αÞ

−hNi2
Uð1Þ2
¼
X
μ2;ν2¼μ2
   þ
X
μ2
X
ν2≠μ2
  
¼ v

σ2
Nσ2
Uð1Þ þ σ2
N

Uð1Þ2 þ σ2
Uð1ÞhNi2
þ vðv −1Þ

cifcUð1Þ þ cif

Uð1Þ2 þ cUð1ÞhNi2
¼ vðσ2
Nσ2
Uð1Þ þ ðv −1ÞcifcUð1ÞÞ þ v

Uð1Þ2ðσ2
N þ ðv −1ÞcifÞ þ vhNi2ðσ2
Uð1Þ þ ðv −1ÞcUð1ÞÞ
¼ vσ2
Uð1Þðσ2
N −cifÞ þ v

Uð1Þ2ðσ2
N þ ðv −1ÞcifÞ;
ðC16Þ
cUð2Þ ¼ −
σ2
Uð2Þ
ðv −1Þ :
ðC17Þ
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-13

(c) Level L. In general,
Uð1→LÞ
i1→L ðμ1; αÞ ¼
X
v
μ2¼1
Nð1Þ
i1 ðμ1; μ2ÞUð2→LÞ
i2→L ðμ2; αÞ:
ðC18Þ
Therefore,

UðLÞ
¼ v
m
v

×

UðL−1Þ
¼ vL−1
m
v
L
;
ðC19Þ
σ2
UðLÞ ¼
X
v
μ2;ν1¼1

Nð1Þðμ1; μ2ÞNð1Þðμ1; ν2Þ

Uð2→LÞðμ2; αÞUð2→LÞðν1; αÞ

−hNi2
Uð1→ðL−1ÞÞ2
¼
X
μ2;ν2¼μ2
   þ
X
μ2
X
ν2≠μ2
  
¼ v

σ2
Nσ2
UðL−1Þ þ σ2
N

UðL−1Þ2 þ σ2
UðL−1ÞhNi2
þ vðv −1Þ

σ2
ifcUðL−1Þ þ cif

UðL−1Þ2 þ cUðL−1ÞhNi2
¼ vσ2
UðL−1Þðσ2
N −cifÞ þ v

UðL−1Þ2ðσ2
N þ ðv −1ÞcifÞ;
ðC20Þ
cUðLÞ ¼ −
σ2
UðLÞ
ðv −1Þ :
ðC21Þ
(d) Concentration for large m. In the large multiplicity limit m ≫1, the U’s concentrate around their mean value. Because
of m ≤vs−1, large m implies large v; thus, we can proceed by setting m ¼ qvs−1, with q ∈ð0; 1 and studying the
v ≫1 limit. From Eq. (C19),

UðLÞ
¼ qLvLðs−1Þ−1:
ðC22Þ
In addition,
σ2
N⟶
v≫1 m
v ¼ qvðs−1Þ−1;
cif⟶
v≫1 −
m
v
2
1
vs−1 ¼ −q2vðs−1Þ−2;
ðC23Þ
so that
σ2
UðLÞ ¼ vσ2
UðL−1Þðσ2
N −σ2
ifÞ þ v

UðL−1Þ2ðσ2
N þ ðv −1Þσ2
ifÞ
⟶
v≫1σ2
UðL−1Þqvðs−1Þ þ σ2
UðL−1Þq2vðs−1Þ−1 þ q2L−1ð1 −qÞvð2L−1Þðs−1Þ−2:
ðC24Þ
The second of the three terms is always subleading with respect to the first, so we can discard it for now. It remains to
compare the first and the third terms. For L ¼ 2, since σ2
Uð1Þ ¼ σ2
N, the first term depends on v as v2ðs−1Þ−1, whereas the
third is proportional to v3ðs−1Þ−2. For L ≥3, the dominant scaling is that of the third term only: for L ¼ 3 it can be
shown by simply plugging the L ¼ 2 result into the recursion, and for larger L it follows from the fact that replacing
σ2
UðL−1Þ in the first term with the third term of the precious step always yields a subdominant contribution. Therefore,
σ2
UðLÞ⟶
v≫1 q2v2ðs−1Þ−1 þ q3ð1 −qÞv3ðs−1Þ−2
for L ¼ 2
q2L−1ð1 −qÞvð2L−1Þðs−1Þ−2
for L ≥3:
ðC25Þ
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-14

Upon dividing the variance by the squared mean, we get
σ2
UðLÞ
hUðLÞi2 ⟶
v≫1 1
q2
1
v2ðs−1Þ−1 þ 1−q
q
1
vðs−1Þ
for L ¼ 2
1−q
q
1
vðs−1Þ
for L ≥3;
ðC26Þ
whose convergence to 0 guarantees the concentration of the U’s around the average over all instances of the RHM.
d. Statistics of the denominator D
Here we compute the first and second moments of the denominator of fð1→LÞ
i1→L ðμ1; αÞ:
Dð1→LÞ
i1→L ðμ1Þ ¼
X
v
μ2;…;μL¼1
X
nc
μLþ1¼1
Nð1Þ
i1 ðμ1; μ2Þ ×    × NðLÞ
iL ðμL; μLþ1Þ:
ðC27Þ
(a) Level 1: L ¼ 1. For L ¼ 1, D is simply the sum over classes of the occurrences of a single production rule,
Dð1Þ ¼ P
α Niðμ1; αÞ,

Dð1Þ
¼ nc
m
v ;
ðC28Þ
σ2
Dð1Þ ≔Var

Dð1Þ
¼ ncσ2
N þ ncðnc −1Þcif ¼ nc
m
v
2 v −1
vs −1
vs
m −nc

⟶
v≫1nc
m
v
2v
m −nc
vs−1

;
ðC29Þ
cDð1Þ ≔Cov

Dð1Þðμ1Þ; Dð1Þðν0Þ

¼ −Var½Dð1Þ
ðv −1Þ ¼ nccN þ ncðnc −1Þcg;
ðC30Þ
where, in the last line, we used the identities σ2
N þ ðv −1ÞcN ¼ 0 from Eq. (B5) and cif þ ðv −1Þcg ¼ 0
from Eq. (B10).
(b) Level 2: L ¼ 2. For L ¼ 2,
Dð1→2Þ
i1→2 ðμ1Þ ¼
X
v
μ2
X
nc
μ3¼1
Nð1Þ
i1 ðμ1; μ2Þ × Nð2Þ
i2 ðμ2; μ3Þ ¼
X
v
μ2¼1
Nð1Þ
i1 ðμ1; μ2ÞDð2Þ
i2 ðμ2Þ:
ðC31Þ
Therefore,

Dð1→2Þ
¼ v
m
v

×

Dð1Þ
¼ nc
v m2;
ðC32Þ
σ2
Dð2Þ ≔Var

Dð1→2Þ
¼
X
v
μ2;ν1¼1

Nð1Þðμ1; μ2ÞNð1Þðμ1; ν1Þ

Dð2Þðμ2ÞDð2Þðν1Þ

−hNi2
Dð1Þ2
¼
X
μ2;ν1¼μ2
   þ
X
μ2
X
ν1≠μ2
  
¼ v

σ2
Nσ2
Dð1Þ þ σ2
N

Dð1Þ2 þ σ2
Dð1ÞhNi2
þ vðv −1Þ

cifcDð1Þ þ cif

Dð1Þ2 þ cDð1ÞhNi2
¼ vðσ2
Nσ2
Dð1Þ þ ðv −1ÞcifcDð1ÞÞ þ v

Dð1Þ2ðσ2
N þ ðv −1ÞcifÞ þ vhNi2ðσ2
Dð1Þ þ ðv −1ÞcDð1ÞÞ
¼ vσ2
Dð1Þðσ2
N −cifÞ þ v

Dð1Þ2ðσ2
N þ ðv −1ÞcifÞ;
ðC33Þ
cDð2Þ ¼ −
σ2
Dð2Þ
ðv −1Þ :
ðC34Þ
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-15

(c) Level L. In general,
Dð1→LÞ
i1→L ðμ1Þ ¼
X
v
μ2¼1
Nð1Þ
i1 ðμ1; μ2ÞDð2→LÞ
i2→L ðμ2Þ:
ðC35Þ
Therefore,

DðLÞ
¼ v
m
v

×

DðL−1Þ
¼ nc
v mL;
ðC36Þ
σ2
DðLÞ ¼
X
v
μ2;ν1¼1

Nð1Þðμ1; μ2ÞNð1Þðμ1; ν1Þ

Dð2→LÞðμ2; αÞDð2→LÞðν1; αÞ

−hNi2
Dð1→ðL−1ÞÞ2
¼
X
μ2;ν1¼μ2
   þ
X
μ2
X
ν1≠μ2
  
¼ v

σ2
Nσ2
DðL−1Þ þ σ2
N

DðL−1Þ2 þ σ2
DðL−1ÞhNi2
þ vðv −1Þ

cifcDðL−1Þ þ cif

DðL−1Þ2 þ cDðL−1ÞhN
E2
¼ vσ2
DðL−1Þðσ2
N −cifÞ þ v

DðL−1Þ2ðσ2
N þ ðv −1ÞcifÞ;
ðC37Þ
cDðLÞ ¼ −
σ2
DðLÞ
ðv −1Þ :
ðC38Þ
(d) Concentration for large m. Since the D’s can be expressed as a sum of different U’s, their concentration for m ≫1
follows directly from that of the U’s.
e. Estimate of the conditional class probability
We can now turn back to the original problem of estimating
fð1→LÞ
i1→L ðαjμ1Þ ¼
Pv
μ2;…;μL¼1 Nð1Þ
i1 ðμ1; μ2Þ ×    × NðLÞ
iL ðμL; αÞ
Pv
μ2;…;μL¼1
Pnc
μLþ1¼1 Nð1Þ
i1 ðμ1; μ2Þ ×    × NðLÞ
iL ðμL; μLþ1Þ
¼ Uð1→LÞ
i1→L ðμ1; αÞ
Dð1→LÞ
i1→L ðμ1Þ
:
ðC39Þ
Having shown that both numerator and denominator converge to their average for large m, we can expand for small
fluctuations around these averages and write
fð1→LÞ
i1→L ðαjμ1Þ ¼
v−1mL
1 þ
Uð1→LÞ
i1→L ðμ1;αÞ−mL=v
mL=v

ncv−1mL
1 þ
Dð1→LÞ
i1→L ðμ1Þ−ncmL=v
mL

ðC40Þ
¼ 1
nc
þ 1
nc
Uð1→LÞ
i1→L ðμ1; αÞ −mL=v
mL=v
−1
nc
Dð1→LÞ
i1→L ðμ1Þ −ncmL=v
mL=v
¼ 1
nc
þ
v
ncmL

Uð1→LÞ
i1→L ðμ1; αÞ −1
nc
Dð1→LÞ
i1→L ðμ1Þ

:
ðC41Þ
Since the conditional frequencies average to n−1
c , the term in brackets averages to zero. We can then estimate the size of the
fluctuations of the conditional frequencies (i.e., the signal) with the standard deviation of the term in brackets.
It is important to notice that, for each L and position i1→L, D is the sum over α of U, and the U with different α at fixed
low-level feature μ1 are identically distributed. In general, for a sequence of identically distributed variables ðXαÞα¼1;…;nc,
 1
nc
X
v
β¼1
Xβ
2
¼ 1
n2c
X
nc
β¼1

hXβi2 þ
X
β0≠β
hXβXβ0i

¼ 1
nc

hXβi2 þ
X
β0≠β
hXβXβ0i

:
ðC42Þ
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-16

Hence,

Xα −1
nc
X
nc
β¼1
Xβ
2
¼ hX2αi þ n−2
c
X
nc
β;γ¼1
hXβXγi −2n−1
c
X
nc
β¼1
hXαXβi
¼ hX2αi −n−1
c

hXαi2 þ
X
β≠α
hXαXβi

¼ hX2αi −n−2
c
X
nc
β¼1
Xβ
2
:
ðC43Þ
In our case,

Uð1→LÞ
i1→L ðμ1; αÞ −1
nc
Dð1→LÞ
i1→L ðμ1Þ
2
¼

Uð1→LÞ
i1→L ðμ1; αÞ
2
−nc−2

Dð1→LÞ
i1→L ðμ1Þ
2
¼ σ2
UðLÞ −n−2
c σ2
DðLÞ;
ðC44Þ
where, in the second line, we have used that hUðLÞi ¼ hDðLÞi=nc to convert the difference of second moments into a
difference of variances. By Eqs. (C19) and (C36),
σ2
UðLÞ −n−2
c σ2
DðLÞ ¼ vσ2
UðL−1Þðσ2
N −σ2
ifÞ þ v

UðL−1Þ2ðσ2
N þ ðv −1Þσ2
ifÞ
−v
n2c
σ2
DðL−1Þðσ2
N −σ2
ifÞ −v
n2c

DðL−1Þ2ðσ2
N þ ðv −1Þσ2
ifÞ
¼ vðσ2
N −σ2
ifÞðσ2
UðL−1Þ −n−2
c σ2
DðL−1ÞÞ;
ðC45Þ
having used again that hUðLÞi ¼ hDðLÞi=nc. Iterating,
σ2
UðLÞ −n−2
c σ2
DðLÞ ¼ ½vðσ2
N −σ2
ifÞL−1
ðσ2
Uð1Þ −n−2
c σ2
Dð1ÞÞ

:
ðC46Þ
Since
σ2
Uð1Þ ¼ m
v
v −1
v
vs −m
vs −1 ⟶
v≫1 m
v ;
n−2
c σ2
Dð1Þ ¼ n−1
c σ2
N þ n−1
c ðnc −1Þσ2
if⟶
v≫1n−1
c
m
v
2v
m −nc
vs−1

¼ 1
nc
m
v

1 −mnc
vs

;
ðC47Þ
one has
σ2
UðLÞ −n−2
c σ2
DðLÞ⟶
v≫1 mL
v

1 −1 −ncm=vs
nc

;
ðC48Þ
so that
Var
h
fð1→LÞ
i1→L ðαjμ1Þ
i
¼ v2

ðUð1→LÞ
i1→L ðμ1; αÞ −1
nc Dð1→LÞ
i1→L ðμ1ÞÞ2
n2cm2L
⟶
v;nc≫1 v
nc
1
ncmL :
ðC49Þ
2. Introducing sampling noise due to the finite training set
In a supervised learning setting where only P of the total data are available, the occurrences N are replaced with their
empirical counterparts ˆN. In particular, the empirical joint occurrence ˆNðμ; αÞ (where we dropped level and positional
indices to ease notation) coincides with the number of successes when sampling P points without replacement from a
population of Pmax where only Nðμ; αÞ belong to class α and display feature μ in position j. Thus, ˆNðμ; αÞ obeys a
hypergeometric distribution where P plays the role of the number of trials, Pmax the population size, and the true occurrence
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-17

Nðμ; αÞ the number of favorable cases. If P is large and
Pmax, Nðμ; αÞ are both larger than P, then
ˆNðμ;αÞ→N

PNðμ;αÞ
Pmax
;PNðμ;αÞ
Pmax

1−Nðμ;αÞ
Pmax

;
ðC50Þ
where the convergence is meant as a convergence in
probability and N ða; bÞ denotes a Gaussian distribution
with mean a and variance b. The statement above holds
when the ratio Nðμ; αÞ=Pmax is away from 0 and 1, which is
true with probability 1 for large v due to the concentration
of fðαjμÞ. In complete analogy, the empirical occurrence
ˆNðμÞ obeys
ˆNðμÞ →N

P NðμÞ
Pmax
; P NðμÞ
Pmax

1 −NðμÞ
Pmax

:
ðC51Þ
We obtain the empirical conditional frequency by the ratio
of Eqs. (C50) and (C51). Since NðμÞ ¼ Pmax=v and
fðαjμÞ ¼ Nðμ; αÞ=NðμÞ, we have
ˆfðαjμÞ ¼
fðαjμÞ
v
þ ξP
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
P
fðαjμÞ
v

1 −fðαjμÞ
v

r
1
v þ ζP
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
P
1
v ð1 −1
vÞ
q
;
ðC52Þ
where ξP and ζP are correlated zero-mean and unit-variance
Gaussian random variables over independent drawings of
the P training points. By expanding the denominator of the
right-hand side for large P we get, after some algebra,
ˆfðαjμÞ ≃fðαjμÞ þ ξP
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
vfðαjμÞ
P

1 −fðαjμÞ
v

s
−ζPfðαjμÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
v
P

1 −1
v

s
:
ðC53Þ
Recall that, in the limit of large nc and m, fðαjμÞ ¼
n−1
c ð1 þ σfξRHMÞ, where ξRHM is a zero-mean and unit-
variance Gaussian variable over the realizations of the
RHM, while σf is the signal, σ2
f ¼ v=mL by Eq. (C49). As
a result,
ˆfðαjμÞ ⟶
nc;m;P≫1 1
nc

1 þ
ﬃﬃﬃﬃﬃﬃﬃ
v
mL
r
ξRHM þ
ﬃﬃﬃﬃﬃﬃﬃ
vnc
P
r
ξP

:
ðC54Þ
3. Sample complexity
From Eq. (C54) it is clear that for the signal ˆf, the
fluctuations due to noise must be smaller than those due to
the random choice of the composition rules. Therefore, the
crossover takes place when the two noise terms have the
same size, occurring at P ¼ Pc such that
ﬃﬃﬃﬃﬃﬃﬃ
v
mL
r
¼
ﬃﬃﬃﬃﬃﬃﬃ
vnc
Pc
r
⇒Pc ¼ ncmL:
ðC55Þ
APPENDIX D: IMPROVED SAMPLE
COMPLEXITY VIA CLUSTERING
In this appendix, we consider the maximal dataset case
nc ¼ v and m ¼ vs−1, and show that a distance-based
clustering method acting on the hidden representations of
Eq. (13) wouldidentifysynonyms at P ≃
ﬃﬃﬃﬃﬃ
nc
p
mL. Let usthen
imagine feeding the representations updates ΔfhðμÞ of
Eq. (13) to a clustering algorithm aimed at identifying
synonyms. This algorithm is based on the distance between
therepresentationsofdifferenttuplesofinputfeaturesμandν,
kΔfðμÞ −ΔfðνÞk2 ≔1
H
X
H
h¼1
ðΔfhðμÞ −ΔfhðνÞÞ2;
ðD1Þ
where H is the number of hidden neurons. By defining
ˆgαðμÞ ≔
ˆN1ðμ; αÞ
P
−1
nc
ˆN1ðμÞ
P
;
ðD2Þ
and denoting with ˆgðμÞ the nc-dimensional sequence having
the ˆgα’s as components, we have
kΔfðμÞ −ΔfðνÞk2 ¼
X
nc
α;β¼1
 1
H
X
H
h
ah;αah;β

ðˆgαðμÞ −ˆgαðνÞÞðˆgβðμÞ −ˆgβðνÞÞ
⟶
H→∞X
nc
α¼1
ðˆgαðμÞ −ˆgαðνÞÞ2 ¼ kˆgðμÞ −ˆgðνÞk2;
ðD3Þ
where we used the i.i.d. Gaussian initialization of the readout weights to replace the sum over neurons with δα;β.
Because of the sampling noise, from Eqs. (C50) and (C51), when 1 ≪P ≪Pmax,
ˆgαðμÞ ¼ gαðμÞ þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
ncmvP
s
ηαðμÞ;
ðD4Þ
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-18

where ηαðμÞ is a zero-mean and unit-variance Gaussian
noise and g without hat denotes the P →Pmax limit of ˆg.
In the limit 1 ≪P ≪Pmax, the noises with different α and
μ are independent of each other. Thus,
kˆgðμÞ −ˆgðνÞk2
¼ kgðμÞ −gðνÞk2 þ
1
ncmvP kηðμÞ −ηðνÞk2
þ
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ncmvP
p
ðgðμÞ −gðνÞÞ · ðηðμÞ −ηðνÞÞ:
ðD5Þ
If μ and ν are synonyms, then gðμÞ ¼ gðνÞ and only the
noise term contributes to the right-hand side of Eq. (D5). If
this noise is sufficiently small, then the distance above can
be used to cluster tuples into synonymic groups.
By the independence of the noises and the central limit
theorem, for nc ≫1,
kηðμÞ −ηðνÞk2 ∼N ð2nc; Oð
ﬃﬃﬃﬃﬃ
nc
p
ÞÞ;
ðD6Þ
over independent samplings of the P training points. The
g’s are also random variables over independent realizations
of the RHM with zero mean and variance proportional to
the variance of the conditional probabilities fðαjμÞ [see
Eqs. (C40) and (C49)]:
Var½gαðμÞ ¼
1
ncmvncmL ¼
1
ncmvPc
:
ðD7Þ
To estimate the size of kgðμÞ −gðνÞk2 we must take into
account the correlations (over RHM realizations) between
g’s with different class label and tuples. However, in the
maximal dataset case nc ¼ v and m ¼ vs−1, both the sum
over classes and the sum over tuples of input features of the
joint occurrences Nðμ; αÞ are fixed deterministically. The
constraints on the sums allow us to control the correlations
between occurrences of the same tuple within different
classes and of different tuples within the same class, so that
the size of the term kgðμÞ −gðνÞk2 for nc ¼ v ≫1 can be
estimated via the central limit theorem:
kgðμÞ −gðνÞk2 ∼N

2nc
ncmvPc
; Oð
ﬃﬃﬃﬃﬃ
nc
p
Þ
ncmvPc

:
ðD8Þ
The mixed term ðgðμÞ −gðνÞÞ · ðηðμÞ −ηðνÞÞ has zero
average (both with respect to training set sampling and
RHM realizations) and can also be shown to lead to relative
fluctuations of order Oð
ﬃﬃﬃﬃﬃ
nc
p
Þ in the maximal dataset case.
To summarize, we have that, for synonyms,
kˆgðμÞ −ˆgðνÞk2 ¼ kηðμÞ −ηðνÞk2
∼
1
mvP

1 þ
1ﬃﬃﬃﬃﬃ
nc
p
ξP

;
ðD9Þ
where ξP is some Oð1Þ noise dependent on the training set
sampling. If μ and ν are not synonyms, instead,
kˆgðμÞ −ˆgðνÞk2 ∼
1
mvP

1 þ
1ﬃﬃﬃﬃﬃ
nc
p
ξP

þ
1
mvPc

1 þ
1ﬃﬃﬃﬃﬃ
nc
p
ξRHM

;
ðD10Þ
where ξRHM is some Oð1Þ noise dependent on the RHM
realization. In this setting, the signal is the deterministic
part of the difference between representations of non-
synonymic tuples. Because of the sum over class labels,
the signal is scaled up by a factor nc, whereas the
fluctuations (stemming from both sampling and model)
are only increased by Oð
ﬃﬃﬃﬃﬃ
nc
p
Þ. Therefore, the signal
required for clustering emerges from the sampling noise
at P ¼ Pc=
ﬃﬃﬃﬃﬃ
nc
p
¼
ﬃﬃﬃﬃﬃ
nc
p
mL, equal to v1=2þLðs−1Þ in the
maximal dataset case. This prediction is tested for s ¼ 2
in Fig. 10, which shows the error achieved by a layerwise
FIG. 10.
Sample complexity for layerwise training, m ¼ nc ¼ v, L ¼ 3, s ¼ 2. Training of an L-layers network is performed
layerwise by alternating one-step GD as described in Sec. IV C and clustering of the hidden representations. Clustering of the mv ¼ v2
representations for the different one-hot-encoded input patches is performed with the k-means algorithms. Clustered representations are
then orthogonalized and the result is given to the next one-step GD procedure. Left: test error versus number of training points. Different
colors correspond to different values of v. Center: collapse of the test error curves when rescaling the x axis by vLþ1=2. Right: analogous,
when rescaling the x axis by vLþ1. The curves show a better collapse when rescaling by vLþ1=2, suggesting that these layerwise
algorithms have an advantage of a factor
ﬃﬃﬃv
p
over end-to-end training with deep CNNs, for which P ¼ vLþ1.
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-19

algorithm which alternates single GD steps to clustering of
the resulting representations [22,62]. More specifically, the
weights of the first hidden layer are updated with a single
GD step while keeping all the other weights frozen. The
resulting representations are then clustered, so as to identify
groups of synonymic level-1 tuples. The centroids of the
ensuing clusters, which correspond to level-2 features, are
orthogonalized and used as inputs of another one-step GD
protocol, which aims at identifying synonymic tuples of
level-2 features. The procedure is iterated L times.
APPENDIX E: INTRINSIC DIMENSIONALITY
OF DATA REPRESENTATIONS
In deep learning, the representation of data at each layer
of a network can be thought of as lying on a manifold in the
layer’s activation space. Measures of the intrinsic dimen-
sionality of these manifolds can provide insights into how
the networks lower the dimensionality of the problem layer
by layer. However, such measurements have challenges.
One key challenge is that it assumes that real data exist on a
smooth manifold, while in practice, the dimensionality is
estimated based on a discrete set of points. This leads to
counterintuitive results such as an increase in the intrinsic
dimensionality with depth, especially near the input. An
effect that is impossible for continuous smooth manifolds.
We resort to an example to illustrate how this increase with
depth can result from spurious effects. Consider a manifold
of a given intrinsic dimension that undergoes a trans-
formation where one of the coordinates is multiplied by a
large factor. This operation would result in an elongated
manifold that appears one dimensional. The measured
intrinsic dimensionality would consequently be one,
despite the higher dimensionality of the manifold. In the
context of neural networks, a network that operates on such
an elongated manifold could effectively “reduce” this extra,
spurious dimension. This could result in an increase in the
observed intrinsic dimensionality as a function of network
depth, even though the actual dimensionality of the mani-
fold did not change.
In the specific case of our data, the intrinsic dimension-
ality of the internal representations of deep CNNs mono-
tonically decreases with depth, see Fig. 11, consistently
with the idea proposed in the main text that the CNNs solve
the problem by reducing the effective dimensionality of
data layer by layer. We attribute this monotonicity to the
absence of spurious or noisy directions that might lead to
the counterintuitive effect described above.
APPENDIX F: ADDITIONAL RESULTS ON
SAMPLE COMPLEXITY
This appendix collects additional results on the sample
complexity of deep networks trained on the RHM (Figs. 12
and 13), on the learning curves for “lazy” neural networks
(Fig. 14), and for a ResNet18 trained on different sub-
samples of the benchmark dataset CIFAR10 (Fig. 15).
Figure 12 shows the behavior of the sample complexity
with varying number of classes nc when all the other
parameters of the RHM are fixed, confirming the linear
scaling discussed in the main text.
Figure 13 shows the behavior of the sample complexity
for deep fully connected networks having depth larger than
L þ 1, which are not tailored to the structure of the RHM.
Notice that changing architecture seems to induce an
additional factor of 2L to the sample complexity, indepen-
dent of v, nc, and m. This factor is also polynomial in the
input dimension.
Figure 14 presents the learning curves for deep CNNs
tailored to the structure of the model and trained in the lazy
regime on the maximal case, i.e., nc ¼ v and m ¼ vs. In
particular, we consider the infinite-width limit of CNNs
with all layers scaled by a factor H−1=2, including the
last. In this limit, CNNs become equivalent to a kernel
method [49], with an architecture-dependent kernel known
as the neural tangent kernel. In our experiments, we use the
FIG. 11.
Effective dimension of the internal representation of a CNN trained on one instance of the RHM with m ¼ nc ¼ v; L ¼ 3
resulting in Pmax ¼ 6232. Left: average nearest neighbor distance of input or network activations when probing them with a dataset of
size P. The value reported on the y axis is normalized by δ0 ¼ δðP ¼ 10Þ. The slope of δðPÞ is used as an estimate of the effective
dimension. Right: effective dimension as a function of depth. We observe a monotonic decrease, consistent with the idea that the
dimensionality of the problem is reduced by deep neural networks with depth.
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-20

FIG. 12.
Sample complexity of deep CNNs, for L ¼ s ¼ 2, v ¼ 256, m ¼ 23 and different values of nc. Left: test error versus number
of training points with the color indicating the number of classes (see key). Right: sample complexity P (crosses) and law P ¼ ncmL
(black dashed).
FIG. 13.
Sample complexity of deep fully connected networks with different depth, for s ¼ 2 and m ¼ nc ¼ v. Left: test error versus
number of training points. The color denotes the value of m ¼ nc ¼ v, the marker the hierarchy depth of the RHM L. Solid lines
represent networks having depth L, while dashed lines correspond to networks with depth 6 > L. Note that, in all cases, the behavior of
the test error is roughly independent of the network depth. Right: sample complexity P (crosses and circles). With respect to the case of
deep CNNs tailored to the structure of the RHM, the sample complexity of generic deep networks seems to display an additional factor
of sL independently of nc, m, and v.
FIG. 15.
Test error versus number of training points for a
ResNet18 trained on subsamples of the CIFAR10 dataset. Results
are the average of 10 jointly different initializations of the
networks and dataset sampling.
FIG. 14.
Learning curves of depth-(L þ 1) CNNs, for L ¼ 2,
s ¼ 2 and m ¼ nc ¼ v trained in the “lazy” regime (full
lines)—where they are equivalent to a kernel method [49]—
and in the “feature” learning regime (dashed lines). Different
colors correspond to different vocabulary sizes v. Vertical
lines signal Pmax ¼ vsL.
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-21

analytical form of this kernel (see, e.g., Ref. [25]) and train
a kernel logistic regression classifier up to convergence.
Our main result is that, in the lazy regime, the generali-
zation error stays finite even when P ≈Pmax; thus, kernels
suffer from the curse of dimensionality.
Notice that the learning curves of the lazy regime follow
those of the feature learning regime for P ≪P. This is
because the CNN kernel can also exploit local correlations
between the label and input patches [25] to improve its
performance. However, unlike in the feature regime,
kernels cannot build a hierarchical representation, and thus
their test error does not converge to zero.
[1] A. Voulodimos, N. Doulamis, A. Doulamis, and E.
Protopapadakis, Deep learning for computer vision: A brief
review, Comput. Intell. Neurosci. 2018, 1 (2018).
[2] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,
A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton
et al., Mastering the game of Go without human knowledge,
Nature (London) 550, 354 (2017).
[3] U. v. Luxburg and O. Bousquet, Distance-based classifica-
tion with Lipschitz functions, J. Mach. Learn. Res. 5, 669
(2004).
[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
ImageNet: A large-scale hierarchical image database, in
Proceedings of the 2009 IEEE Conference on Computer
Vision and Pattern Recognition (IEEE, New York, 2009),
pp. 248–255, https://ieeexplore.ieee.org/document/5206848.
[5] P. Pope, C. Zhu, A. Abdelkader, M. Goldblum, and T.
Goldstein, The intrinsic dimension of images and its impact
on learning, in Proceedings of the International Conference
on
Learning
Representations
(ICLR,
2021),
https://
openreview.net/forum?id=XJk19XzGq2J.
[6] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature
(London) 521, 436 (2015).
[7] D. C. Van Essen and J. H. Maunsell, Hierarchical organi-
zation and functional streams in the visual cortex, Trends
Neurosci. 6, 370 (1983).
[8] K. Grill-Spector and R. Malach, The human visual cortex,
Annu. Rev. Neurosci. 27, 649 (2004).
[9] M. D. Zeiler and R. Fergus, Visualizing and understanding
convolutional networks, in Proceedings of the 13th Euro-
pean Conference Computer Vision—-ECCV 2014, Zurich
(Springer, Cham, 2014), pp. 818–833, 10.1007/978-3-319-
10590-1_53.
[10] D. Doimo, A. Glielmo, A. Ansuini, and A. Laio, Hierar-
chical nucleation in deep neural networks, Adv. Neural Inf.
Process. Syst. 33, 7526 (2020), https://proceedings.neurips
.cc/paper/2020/hash/54f3bc04830d762a3b56a789b6ff62df-
Abstract.html.
[11] J. Bruna and S. Mallat, Invariant scattering convolution
networks, IEEE Trans. Pattern Anal. Mach. Intell. 35, 1872
(2013).
[12] R. Shwartz-Ziv and N. Tishby, Opening the black box of
deep neural networks via information, arXiv:1703.00810.
[13] A. M. Saxe, Y. Bansal, J. Dapello, M. Advani, A.
Kolchinsky, B. D. Tracey, and D. D. Cox, On the informa-
tion bottleneck theory of deep learning, J. Stat. Mech.
(2019) 124020.
[14] A. Ansuini, A. Laio, J. H. Macke, and D. Zoccolan, Intrinsic
dimension of data representations in deep neural networks,
Adv. Neural Inf. Process. Syst. 32, 6111 (2019).
[15] S. Recanatesi, M. Farrell, M. Advani, T. Moore, G. Lajoie,
and E. Shea-Brown, Dimensionality compression and ex-
pansion in deep neural networks, arXiv:1906.00443.
[16] L. Petrini, A. Favero, M. Geiger, and M. Wyart, Relative
stability toward diffeomorphisms indicates performance in
deep nets, Adv. Neural Inf. Process. Syst. 34, 8727 (2021).
[17] U. M. Tomasini, L. Petrini, F. Cagnetta, and M. Wyart, How
deep convolutional neural networks lose spatial informa-
tion with training, Mach. Learn. Sci. Technl. 4, 045026
(2023).
[18] A. B. Patel, T. Nguyen, and R. G. Baraniuk, A probabilistic
theory of deep learning, arXiv:1504.00641.
[19] E. Mossel, Deep learning and hierarchal generative mod-
els, arXiv:18612.09057.
[20] H. Mhaskar, Q. Liao, and T. Poggio, When and why are
deep networks better than shallow ones?, in Proceedings
of the AAAI Conference on Artificial Intelligence, 2017
(AAAI Press, San Francisco, 2017), Vol. 31, 10.1609/
aaai.v31i1.10913.
[21] T. Poggio, H. Mhaskar, L. Rosasco, B. Miranda, and Q.
Liao, Why and when can deep—but not shallow—networks
avoid the curse of dimensionality: A review, Int. J. Autom.
Comput. 14, 503 (2017).
[22] E. Malach and S. Shalev-Shwartz, A provably correct
algorithm for deep learning that actually works, arXiv:
1803.09522.
[23] J. Zazo, B. Tolooshams, D. Ba, and H. J. A. Paulson,
Convolutional dictionary learning in hierarchical networks,
in Proceedings of the IEEE 8th International Workshop
on Computational Advances in Multi-Sensor Adaptive
Processing, 2019 (IEEE, New York, 2019), Vol. 131,
10.1109/CAMSAP45676.2019.9022440.
[24] J. Schmidt-Hieber, Nonparametric regression using deep
neural networks with ReLU activation function, Ann.
Stat. 48, 1875 (2020), https://proceedings.mlr.press/v202/
cagnetta23a.html.
[25] F. Cagnetta, A. Favero, and M. Wyart, What can be learnt
with wide convolutional neural networks?, in Proceedings
of the International Conference on Machine Learning,
PMLR, 2023 (PMLR, 2023), pp. 3347–3379.
[26] U. Grenander, Elements of Pattern Theory (Johns Hopkins
University Press, Baltimore and London, 1996), pp. xiii +
222, 10.1002/bimj.4710390707.
[27] M. M´ezard, Mean-field message-passing equations in the
Hopfield model and its generalizations, Phys. Rev. E 95,
022117 (2017).
[28] S. Goldt, M. M´ezard, F. Krzakala, and L. Zdeborová,
Modeling the influence of data structure on learning in
neural networks: The hidden manifold model, Phys. Rev. X
10, 041044 (2020).
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-22

[29] A. M. Saxe, J. L. McClelland, and S. Ganguli, A math-
ematical theory of semantic development in deep neural
networks, Proc. Natl. Acad. Sci. U.S.A. 116, 11537 (2019).
[30] Y. Bahri, J. Kadmon, J. Pennington, S. S. Schoenholz, J.
Sohl-Dickstein, and S. Ganguli, Statistical mechanics of
deep learning, Annu. Rev. Condens. Matter Phys. 11, 501
(2020).
[31] A. Ingrosso and S. Goldt, Data-driven emergence of
convolutional structure in neural networks, Proc. Natl.
Acad. Sci. U.S.A. 119, e2201854119 (2022).
[32] E. DeGiuli, Random language model, Phys. Rev. Lett. 122,
128301 (2019).
[33] F. Bach, The quest for adaptivity, Machine Learning
Research Blog (2021), https://francisbach.com/quest-for-
adaptivity/.
[34] L. Györfi, M. Kohler, A. Krzyzak, H. Walk et al., A
Distribution-Free Theory of Nonparametric Regression,
Vol. 1 (Springer, New York, 2002), 10.1007/b97848.
[35] S. Kpotufe, k-NN regression adapts to local intrinsic
dimension, Adv. Neural Inf. Process. Syst. 24, 729
(2011),
https://proceedings.neurips.cc/paper_files/paper/
2011/file/05f971b5ec196b8c65b75d2ef8267331-Paper.pdf.
[36] T. Hamm and I. Steinwart, Adaptive learning rates for
support vector machines working on data with low intrinsic
dimension, Ann. Stat. 49, 3153 (2021).
[37] M. Geiger, L. Petrini, and M. Wyart, Landscape and
training regimes in deep learning, Phys. Rep. 924, 1
(2021).
[38] J. Paccolat, L. Petrini, M. Geiger, K. Tyloo, and M. Wyart,
Geometric compression of invariant manifolds in neural
networks, J. Stat. Mech. (2021) 044001.
[39] E. Abbe, E. Boix-Adsera, M. S. Brennan, G. Bresler, and D.
Nagaraj, The staircase property: How hierarchical struc-
ture can guide deep learning, in Advances in Neural
Information Processing Systems, Vol. 34, edited by M.
Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W.
Vaughan (Curran Associates, Inc., 2021), pp. 26989–27002,
https://proceedings.neurips.cc/paper_files/paper/2021/file/
e2db7186375992e729165726762cb4c1-Paper.pdf.
[40] B. Barak, B. Edelman, S. Goel, S. Kakade, E. Malach, and C.
Zhang, Hidden progress in deep learning: SGD learns parities
near the computational limit, Adv. Neural Inf. Process. Syst.
35, 21750 (2022), https://proceedings.neurips.cc/paper_files/
paper/2022/file/884baf65392170763b27c914087bde01-
Paper-Conference.pdf.
[41] Y. Dandi, F. Krzakala, B. Loureiro, L. Pesce, and L.
Stephan, Learning two-layer neural networks, one (giant)
step at a time, arXiv:2305.18270.
[42] F. Bach, Breaking the curse of dimensionality with convex
neural networks, J. Mach. Learn. Res. 18, 629 (2017), http://
jmlr.org/papers/v18/14-546.html.
[43] E. Gardner and B. Derrida, Three unfinished works on the
optimal storage capacity of networks, J. Phys. A 22, 1983
(1989).
[44] L. Zdeborová and F. Krzakala, Statistical physics of
inference: Thresholds and algorithms, Adv. Phys. 65,
453 (2016).
[45] M. M´ezard, Spin glass theory and its new challenge:
Structured disorder, Indian J. Phys., 1 (2023).
[46] S. Spigler, M. Geiger, and M. Wyart, Asymptotic learning
curves of kernel methods: Empirical data versus teacher–
student paradigm, J. Stat. Mech. (2020) 124001.
[47] A. Favero, F. Cagnetta, and M. Wyart, Locality defeats the
curse of dimensionality in convolutional teacher-student
scenarios, Adv. Neural Inf. Process. Syst. 34, 9456 (2021),
https://proceedings.neurips.cc/paper_files/paper/2021/file/
4e8eaf897c638d519710b1691121f8cb-Paper.pdf.
[48] R. Aiudi, R. Pacelli, A. Vezzani, R. Burioni, and P. Rotondo,
Local kernel renormalization as a mechanism for feature
learning in overparametrized convolutional neural net-
works, arXiv:2307.11807.
[49] A. Jacot, F. Gabriel, and C. Hongler, Neural tangent
kernel: Convergence and generalization in neural networks,
Adv.
Neural
Inf.
Process.
Syst.
31,
8580
(2018),
https://proceedings.neurips.cc/paper_files/paper/2018/file/
5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf.
[50] L. Chizat, E. Oyallon, and F. Bach, On lazy training in
differentiable programming, in Advances in Neural Informa-
tion Processing Systems (Curran Associates, Inc., 2019),
pp.
2937–2947,
https://proceedings.neurips.cc/paper_files/
paper/2019/file/ae614c557843b1df326cb29c57225459-Paper
.pdf.
[51] G. Rozenberg and A. Salomaa, Handbook of Formal
Languages (Springer Berlin, Heidelberg, 1997), 10.1007/
978-3-642-59136-5.
[52] G. Yang and E. J. Hu, Feature learning in infinite-width
neural networks, arXiv:2011.14522.
[53] Let us focus on the first s-dimensional patch of the input x1,
which can take mv distinct values—m for each of the v
level-2 features. For a linear transformation, insensitivity is
equivalent to the following set of constraints: For each level-
2 feature μ, and x1;i encoding for one of the m level-1
representations generated by μ, w · x1;i ¼ cμ. Since cμ is an
arbitrary constant, there are v × ðm −1Þ constraints for the
v × s components of w, which cannot be satisfied in general
unless m ≤ðs þ 1Þ.
[54] The notation xj ¼ μ means that the elements of the patch xj
encode the tuple of features μ.
[55] A. Damian, J. Lee, and M. Soltanolkotabi, Neural networks
can learn representations with gradient descent, in Pro-
ceedings of Thirty-Fifth Conference on Learning Theory,
2022 (PMLR, 2022), Vol. 178, p. 5413, https://proceedings
.mlr.press/v178/damian22a.html.
[56] J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G.
Yang, High-dimensional asymptotics of feature learning:
How one gradient step improves the representation,
Adv.
Neural
Inf.
Process.
Syst.
35,
37932
(2022),
https://proceedings.neurips.cc/paper_files/paper/2022/file/
f7e7fabd73b3df96c54a320862afcb78-Paper-Conference.pdf.
[57] Here invariance to exchange of level-1 synonyms can
already be achieved at the first hidden layer due to the
orthogonalization of the s-dimensional patches of the input,
which makes them linearly separable.
[58] M. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de
Freitas, Predicting parameters in deep learning, in Advances
in Neural Information Processing Systems, Vol. 26 (2013),
https://proceedings.neurips.cc/paper_files/paper/2013/file/
7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf.
HOW DEEP NEURAL NETWORKS LEARN COMPOSITIONAL …
PHYS. REV. X 14, 031001 (2024)
031001-23

[59] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R.
Fergus, Exploiting linear structure within convolutional
networks for efficient evaluation, in Advances in Neural
Information Processing Systems, Vol. 27 (Curran Associ-
ates, Inc., 2014), https://proceedings.neurips.cc/paper_files/
paper/2014/file/2afe4567e1bf64d32a5527244d104cea-Paper
.pdf.
[60] X. Yu, T. Liu, X. Wang, and D. Tao, On compressing deep
models by low rank and sparse decomposition, in Proceed-
ings of the 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (IEEE, New York, 2017),
Vol. 67, https://openaccess.thecvf.com/content_cvpr_2017/
html/Yu_On_Compressing_Deep_CVPR_2017_paper.html.
[61] F. Guth, B. M´enard, G. Rochette, and S. Mallat, A rainbow
in deep network black boxes, arXiv:2305.18512.
[62] E. Malach and S. Shalev-Shwartz, The implications of
local
correlation
on
learning
some
deep
functions,
Adv.
Neural
Inf.
Process.
Syst.
33,
1322
(2020),
https://proceedings.neurips.cc/paper_files/paper/2020/file/
0e4ceef65add6cf21c0f3f9da53b71c0-Paper.pdf.
[63] S. Shalev-Shwartz, O. Shamir, and S. Shammah, Failures of
gradient-based deep learning, in Proceedings of the
International Conference on Machine Learning (PMLR,
2017), pp. 3067–3075, https://proceedings.mlr.press/v70/
shalev-shwartz17a.html.
[64] N. Kruger, P. Janssen, S. Kalkan, M. Lappe, A. Leonardis, J.
Piater, A. J. Rodriguez-Sanchez, and L. Wiskott, Deep
hierarchies in the primate visual cortex: What can we learn
for computer vision?, IEEE Trans. Pattern Anal. Mach.
Intell. 35, 1847 (2012).
[65] A. Krizhevsky, Learning multiple layers of features from
tiny
images
(2009),
https://www.cs.toronto.edu/~kriz/
learning-features-2009-TR.pdf.
[66] L. Petrini and F. Cagnetta, Random hierarchy model (2023),
10.5281/zenodo.12509435;
https://github.com/pcsl-epfl/
hierarchy-learning/blob/master/datasets/hierarchical.py.
[67] https://github.com/pcsl-epfl/ hierarchy-learning/blob/master/
models.
[68] https://github.com/pcsl-epfl/hierarchy-learning/blob/master.
[69] G. Yang, Scaling limits of wide neural networks with weight
sharing: Gaussian process behavior, gradient independence,
and neural tangent kernel derivation, arXiv:1902.04760.
[70] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G.
Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al.,
PYTORCH: An imperative style, high-performance deep
learning library, Adv. Neural Inf. Process. Syst. 32, 8026
(2019),
https://proceedings.neurips.cc/paper_files/paper/
2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.
FRANCESCO CAGNETTA et al.
PHYS. REV. X 14, 031001 (2024)
031001-24

