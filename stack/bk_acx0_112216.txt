1 
Free  
PDF Bonus 
 
Install and import NLTK data 
 
sudo easy _ install pip 
sudo pip install - U nltk 
 
Run your Python shell 
 
import nltk  nltk . download( )  
 
Noise removal in Python 
 
Noise _ list = [ " an " , " a " , " 
the " , " ... " ]  
def _ remove _ noise ( input _ text ) 
: 
words = input _ text . split ( )  

2 
noise _ free_words = [w ord for word 
in words if word not in noise _ list ]  
noise _ free _ text = " " . join ( 
noise _ free _ words )  
return noise _ free _ text 
_ remove _ noise ( " your sample text 
" ) 
>>> " your sample text " 
 
Removing regex patterns 
 
import re  
def _ remove _ regex ( input _ text , 
regex _ pattern ): 
urls = re . finditer ( regex _ pattern 
, input _ text )  
for i in urls :  
input _ text = re . sub ( i . group ( 
) . strip ( ) , ' ' , input _ text ) 
return input_text 
regex _ pattern = "#[ \w ]*"   
_ remove _ regex ( " remove this 
#hashtag from sample text " , regex _ 
pattern ) 
>>> " remove this  from sample text " 

3 
Performing stemming and lemmatization 
 
from nltk . stem . wordnet import 
WordNetLemmatizer  
lem = WordNetLemmatizer ( ) 
from nltk . stem . porter import 
PorterStemmer  
stem = PorterStemmer ( ) 
word = " swimming "  
lem . lemmatize ( word , " v " ) 
" swim "  
stem . stem ( word ) 
" swim " 
 
Using dictionary lookup methods 
 
Lookup _ dict = { ' rt ' : ' Retweet 
', ' dm ':' direct message ', " luv " 
: " awsm ", " love " : " awesome ", " 
... " } 
def _ lookup _ words ( input _ text ) 
: 
words = input _ text . split ( )  
new _ words = [ ]  

4 
for word in words : 
if word . lower ( ) in lookup _ dict : 
word = lookup _ dict [ word . lower ( 
)] 
new _ words . append ( word ) new _ 
text = " " . join ( new _ words )  
return new _ text 
_ lookup _ words ( " RT this is a 
retweeted tweet " ) 
" Retweet this is a retweeted tweet " 
 
Performing pos tagging annotation 
 
from nltk import word _ tokenize , pos 
_ tag 
text = " I am learning Natural 
Language Processing " 
tokens = word _ tokenize ( text ) 
print pos _ tag ( tokens ) 
[ (' I ', ' PRP ' ), ( ' am ', ' VBP ' 
), ( ' learning ' , ' VBG ' ), ( ' 
Natural ' , ' NNP ' ) , ( ' Language ' 
, ' NNP ' ) , ( ' Processing ' , ' NNP 
' ] 
 

5 
Implementing LDA in Python 
 
doc1 = " Sample Text 1 "  
doc2 = " Sample Text 2 " 
doc3 = " Sample Text 3 " 
doc _ complete = [ doc1 , doc2 , doc3 
] 
doc _ clean = [ doc . split ( ) for 
doc in doc _ complete ] 
import gensim from gensim 
import corpora 
 
Creating dictionary of your text corpus 
 
dictionary = corpora . Dictionary ( 
doc _ clean ) 
doc _ term_matrix = [ dictionary . 
doc2bow ( doc ) for doc in doc _ clean 
] 
 
Creating your LDA model object 
 
Lda = gensim . models . ldamodel . 
LdaModel 

6 
Run and train your LDA model 
 
ldamodel = Lda ( doc _ term _ matrix , 
num _ topics = 3, id2word = dictionary 
, passes = 50 ) 
print ( ldamodel . print _ topics ( ) 
) 
Creating biograms 
 
def generate _ ngrams ( text , n ) : 
words = text . split ( ) 
output = [ ]   
for i in range ( len ( words ) - n+1 ) 
: 
output . append ( words [ i:i +n ] ) 
return output 
generate _ ngrams ( ' your sample text 
' , 2 ) 
[ [ ' your ' , ' sample ' ] , , [ ' 
sample ' , ' text ' ] ]  
 
Using scikit-learn Python package 
 
from sklearn . feature _ extraction . 
text import TfidfVectorizer 

7 
obj = TfidfVectorizer ( ) 
corpus = [ ' This is your sample 
document . ' , ' another random sample 
document . ' , ' your third sample 
document text ' ] 
X = obj . fit _ transform ( corpus ) 
print X 
( 0, 1 ) 0.345205016865 
( 0, 4 ) ... 0.444514311537 
( 2, 1 ) 0.345205016865 
( 2, 4 ) 0.444514311537 
 
Using genism package for word embedding 
 
from gensim . models import Word2Vec 
sentences = [ [ ' natural ' , ' 
langauge ' , ' processing ' , ] ] 
 
Train your model 
 
model = Word2Vec ( sentences , min _ 
count = 1 ) 
print model . similarity ( ' natural ' 
, ' language ' ' processing ' ) 

8 
0.11222489293 
print model [ ' learning ' ]   
array ( [ 0.00459356  0.00303564 -
0.00467622  0.00209638, ... ] ) 
 
Using TextBlob 
 
from textblob . classifiers import 
NaiveBayesClassifier as NBC 
from textblob import TextBlob 
training _ corpus = [ 
( ' Sample Text 1 ' , ' Class _ B ' ), 
( " Sample Text 2 " , ' Class _ B ' ), 
( 'Sample Text 3 ' , ' Class _ B ' ), 
( ' Sample Text 4 ' , ' Class _ B ' ), 
( ' Sample Text 5 ' , ' Class _ A ' ) 
, 
( ' Sample Text 6 ' , ' Class _ A ' ) 
, 
( ' Sample Text 7 ' , ' Class _ A ' ) 
, 
( ' Sample Text 8 ' , ' Class _ A ' ) 
, 
( " Sample Text 9 " , ' Class _ A ' ) 
, 

9 
( ' Sample Text 9 , ' Class _B ' ) ] 
Test _ corpus = [ 
( " Sample Text " , ' Class _ B ' ) ,  
( " Sample Text " , ' Class _ A ' ) ,  
( ' Sample Text ' , ' Class _ A ' ),  
( " Sample Text " , ' Class _ B ' ) ,  
( ' Sample Text ' , ' Class _ A ' ) ,  
( ' Sample Text ' , ' Class _ B ' ) ] 
model = NBC( training _ corpus )  
print ( model . classify ( " Sample 
Text" ) ) 
" Class _ A "  
Print ( model . classify ( " Sample 
Text " ) ) 
" Class _ B " 
print( model . accuracy ( test _ 
corpus ) ) 
0.83  
 
Using natural language processing framework 
 
from sklearn . feature _ extraction . 
text 

10 
import TfidfVectorizer from sklearn . 
metrics 
import classification _ report 
from sklearn import svm  
 
Prepare your data for SMV model 
 
( bayes example ) 
train _ data = [ ] 
train _ labels = [ ] 
for row in training_corpus: 
train _ data . append ( row [ 0 ] ) 
train _ labels . append ( row [ 1 ] ) 
test _ data = [ ]  
test _ labels = [ ]  
for row in test _ corpus : 
test _ data . append ( row [ 0 ] )  
test _ labels . append ( row [ 1 ] ) 
 
Training your feature networks 
 

11 
vectorizer = TfidfVectorizer ( min _ 
df = 4, max _ df = 0.9 ) 
train _ vectors = vectorizer . fit _ 
transform ( train _ data ) 
 
Performing classification with SMV 
 
Test _ vectors = vectorizer . 
transform ( test _ data ) 
model = svm . SVC ( kernel = ' linear 
' )  
model . fit ( train _ vectors , train 
_ labels )  
prediction = model . predict ( test _ 
vectors ) 
[ ' Class _ A ' ' Class _ A ' ' Class 
_ B ' ' Class _ B ' ' Class _ A ' ' 
Class _ A ' ] 
print ( classification _ report( test 
_ labels , prediction ) ) 
 
Implementing levenshtein distance 
 
def levenshtein ( s1, s2 ) :  
if len ( s1 ) > len ( s2 ) : 

12 
s1 , s2 = s2 , s1  
distances = range ( len ( s1 ) + 1 )  
for index2 , char2 in enumerate ( s2 
): 
newDistances = [ index2 + 1 ] 
for index1 , char1 in enumerate ( s1 ) 
: 
if char1 = = char2 : 
           
newDistances . append ( distances [ 
index1 ] )  
or: 
newDistances . append ( 1 + min ( ( 
distances [ index1 ], distances [ 
index1 + 1 ] , newDistances [ -1 ] ) ) 
)  
distances = newDistances  
return distances [ -1 ] 
print ( levenshtein ( " analyze " , " 
analyse " ) ) 
 
Using Python Model Fuzzy  
 
import fuzzy  
soundex = fuzzy . Soundex ( 4 )  

13 
print soundex ( ' ankit ' ) 
“ A523 ” 
print soundex ( ' aunkit ' ) 
“ A523 ”  
 
Convert your text to vectors 
 
import math 
from collections import Counter 
def get _ cosine ( vec1 , vec2 ) : 
common = set ( vec1 . keys ( ) ) & set 
( vec2 . keys ( ) ) 
numerator = sum ( [ vec1 [ x ] * vec2 
[ x ] for x in common ] ) 
sum1 = sum ( [ vec1 [ x ] **2 for x in 
vec1 . keys ( ) ] )  
sum2 = sum ( [ vec2 [ x ] **2 for x in 
vec2 . keys( ) ] )  
denominator = math . sqrt ( sum1 ) * 
math . sqrt( sum2 ) 
if not denominator: 
return 0.0  
or: 

14 
return float ( numerator ) / 
denominator 
def text _ to _ vector ( text ):  
words = text . split ( )  
return Counter(words) 
text1 = ' Learning Natural Language 
Processing '  
text2 = ' Natural Languge Processing 
in Python Guide ' 
vector1 = text _ to _ vector ( text1 )  
vector2 = text _ to _ vector ( text2 )  
cosine = get _ cosine ( vector1 , 
vector2 ) 
0.62  

