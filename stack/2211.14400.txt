arXiv:2211.14400v6  [stat.ML]  8 Apr 2024
OPTIMAL APPROXIMATION RATES FOR DEEP RELU NEURAL
NETWORKS ON SOBOLEV AND BESOV SPACES
Jonathan W. Siegel
Department of Mathematics
Texas A&M University
College Station, TX 77843
jwsiegel@tamu.edu
April 9, 2024
ABSTRACT
Let Ω= [0,1]d be the unit cube in Rd. We study the problem of how efﬁciently, in terms of the
number of parameters, deep neural networks with the ReLU activation function can approximate
functions in the Sobolev spaces W s(Lq(Ω)) and Besov spaces Bs
r(Lq(Ω)), with error measured in
the Lp(Ω) norm. This problem is important when studying the application of neural networks in
a variety of ﬁelds, including scientiﬁc computing and signal processing, and has previously been
solved only when p = q = ∞. Our contribution is to provide a complete solution for all 1 ≤p,q ≤∞
and s > 0 for which the corresponding Sobolev or Besov space compactly embeds into Lp. The
key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse
vectors. This enables us to obtain sharp upper bounds in the non-linear regime where p > q. We
also provide a novel method for deriving Lp-approximation lower bounds based upon VC-dimension
when p < ∞. Our results show that very deep ReLU networks signiﬁcantly outperform classical
methods of approximation in terms of the number of parameters, but that this comes at the cost of
parameters which are not encodable.
1
Introduction
Deep neural networks have achieved remarkable success in both machine learning [37] and scientiﬁc computing [31,
46]. However, a precise theoretical understanding of why deep neural networks are so powerful has not been attained
and is an active area of research. An important part of this theory is the study of the approximation properties of deep
neural networks, i.e. to understand how efﬁciently a given class of functions can be approximated using deep neural
networks. In this work, we solve this problem for the class of deep ReLU neural networks [42] when approximating
functions lying in a Sobolev or Besov space with error measured in the Lp-norm. We remark that the ReLU activation
functions is very widely used and is a major driver of many recent breakthroughs in deep learning [29,37,42].
Let us begin by giving a description of the Sobolev function classes, which are widely used in the theory of solutions
to partial differential equations (PDEs) [27], and the Besov function classes, which are widely used in approximation
theory [19], statistics [24,25], and signal processing [18].
Let Ω⊂Rd be a bounded domain, which we take to be the unit cube Ω= [0,1]d in the following. Due to a variety of
extension theorems for Sobolev and Besov spaces (see for instance [19,23,27,59]), this is not a signiﬁcant restriction
and our results will apply to many other sufﬁciently well-behaved domains. We denote by Lp(Ω) the set of functions
f for which the Lp-norm on Ωis ﬁnite, i.e.
∥f∥Lp(Ω) =
Z
Ω|f(x)|pdx
1/p
< ∞.
When p = ∞, this becomes ∥f∥L∞(Ω) = esssupx∈Ω|f(x)|. Suppose that s > 0 is a positive integer. Then f ∈W s(Lq(Ω))
is in the Sobolev space (see [15], Chapter 2 for instance) with s derivatives in Lq if f has weak derivatives of order s

A PREPRINT - APRIL 9, 2024
and
∥f∥q
Ws(Lq(Ω)) := ∥f∥q
Lq(Ω) + ∑
|α|=k
∥Dα f∥q
Lq(Ω) < ∞.
Here α = (αi)d
i=1 with αi ∈Z≥0 is a multi-index and |α| = ∑d
i=1 αi is the total degree. The W s(Lq(Ω)) semi-norm is
deﬁned by
|f|W s(Lq(Ω)) :=
 
∑
|α|=k
∥Dα f∥q
Lq(Ω)
!1/q
,
(1.1)
and the standard modiﬁcations are made when q = ∞.
When s > 0 is not an integer, we write s = k + θ with k ≥0 an integer and θ ∈(0,1). The Sobolev semi-norm is
deﬁned by (see [15] Chapter 4 or [23] Chapter 1 for instance)
|f|q
W s(Lq(Ω)) :=
Z
Ω×Ω
|Dα f(x)−Dα f(y)|q
|x−y|d+θq
dxdy
(1.2)
when 1 ≤q < ∞and
|f|W s(L∞(Ω)) := sup
|α|=k
sup
x,y∈Ω
|Dα f(x)−Dα f(y)|
|x−y|θ
.
We deﬁne the Sobolev norm by
∥f∥q
Ws(Lq(Ω)) := ∥f∥q
Lq(Ω) + |f|q
W s(Lq(Ω)),
with the usual modiﬁcation when q = ∞. We remark that in the case of non-integral s these spaces are also called
Sobolev-Slobodeckij spaces. Sobolev spaces are widely used in PDE theory and a priori estimates for PDE solutions
are often given in terms of Sobolev norms [27]. For applications of neural networks to scientiﬁc computing it is thus
important to understand how efﬁciently neural networks can approximate functions from W s(Lq(Ω)).
Next, we consider the Besov spaces, which we deﬁne in terms of moduli of smoothness. Given a function f ∈Lq(Ω)
and an integer k, the k-th order modulus of smoothness of f is given by
ωk(f,t)q = sup
|h|≤t
∥∆k
h f∥Lq(Ωkh),
(1.3)
where h ∈Rd, the k-th order ﬁnite difference ∆k
h is deﬁned by
∆k
h f(x) =
k
∑
j=0
(−1)j
k
j

f(x+ jh),
and the Lq norm is taken over the set Ωkh := {x ∈Ω, x + kh ∈Ω} to guarantee that all terms of the ﬁnite difference
are contained in the domain Ω. Fix an integer k > s. The Besov space Bs
r(Lq(Ω)) is deﬁned via the norm
∥f∥Bsr(Lq(Ω)) := ∥f∥Lq(Ω) + |f|Bsr(Lq(Ω)),
with Besov semi-norm given by
|f|Bsr(Lq(Ω)) :=
Z ∞
0
ωk(f,t)r
q
tsr+1
dt
1/r
when r < ∞and by
|f|Bs∞(Lq(Ω)) := sup
t>0
t−sωk(f,t)q,
when r = ∞. It can be shown that different choices of k > s result in equivalent norms [19]. One can think of the Besov
space Bs
r(Lq(Ω)) roughly as being a space of functions with s derivatives lying in Lq, similar to the Sobolev space
W s(Lq(Ω)), with the additional index r providing a ﬁner gradation. Indeed, a variety of embedding and interpolation
results relating Besov spaces and Sobolev spaces are known (see for instance [20,21,36,63]).
Besov spaces are central objects in approximation theory due to their close connection with approximation by trigono-
metric polynomials (on the circle) and splines [19,45]. In fact, there are equivalent deﬁnitions of the Besov semi-norms
in terms of approximation error by trigonometric polynomials and splines. They are also closely connected to the the-
ory of wavelets [12], and one can give equivalent deﬁnitions of the Besov norms in terms of the wavelet coefﬁcients
of f as well [18]. For this reason, Besov spaces play an important role in signal processing [11, 26] and statistical
recovery of functions from point samples [24,25], for instance.
2

A PREPRINT - APRIL 9, 2024
Our goal is to study the approximation of Sobolev and Besov functions by neural networks. One of the most important
classes of neural networks are deep ReLU neural networks, which we deﬁne as follows. We use the notation AW,b to
denote the afﬁne map with weight matrix W and offset, or bias, b, i.e.
AW,b(x) = Wx+ b.
When the weight matrix W is an k×n and the bias b ∈Rk, the function AW,b : Rn →Rk maps Rn to Rk. Let σ denote
the ReLU activation function [42], speciﬁcally
σ(x) =
0
x < 0
x
x ≥0.
The ReLU activation function σ has become ubiquitous in deep learning in the last decade and is used in most state-
of-the-art architectures. Since σ is continuous and piecewise linear, it also has the nice theoretical property that neural
networks with ReLU activation function represent continuous piecewise linear functions. This property has been
extensively studied in the computer science literature [3, 33, 48, 57] and has been connected with traditional linear
ﬁnite element methods [34].
When x ∈Rn, we write σ(x) to denote the application of the activation function σ to each component of x separately,
i.e. σ(x)i = σ(xi). The set of deep ReLU neural networks with width W and depth L mapping Rd to Rk is given by
ϒW,L(Rd,Rk) := {AWL,bL ◦σ ◦AWL−1,bL−1 ◦σ ◦···◦σ ◦AW1,b1 ◦σ ◦AW0,b0},
where the weight matrices satisfy WL ∈Rk×W, W0 ∈RW×d, and W1,...,WL−1 ∈RW×W, and the biases satisfy
b0,...,bL−1 ∈RW and bL ∈Rk. Notice that our deﬁnition of width does not include the input and output dimen-
sions and only includes the intermediate layers. When the depth L = 0, i.e. when the network is an afﬁne function,
there are no intermediate layers and the width is undeﬁned, in this case we write ϒ0(Rd,Rk). We also use the notation
ϒW,L(Rd) := ϒW,L(Rd,R)
to denote the set of ReLU deep neural networks with width W, depth L which represent scalar functions. We note that
our notation only allows neural networks with ﬁxed width. We do this to avoid excessively cumbersome notation. We
remark that the dimension of any hidden layer can naturally be expanded and thus any fully connected network can be
made to have a ﬁxed width.
The problem we study in this work is to determine optimal Lp-approximation rates
sup
∥f∥Ws(Lq(Ω))≤1

inf
fL∈ϒW,L(Rd)∥f −fL∥Lp(Ω)

and
sup
∥f∥Bsr(Lq(Ω))≤1

inf
fL∈ϒW,L(Rd)∥f −fL∥Lp(Ω)

(1.4)
for the class of Sobolev and Besov functions using very deep ReLU networks, i.e. using networks with a ﬁxed (large
enough) width W and depth L →∞. We will prove that this gives the best possible approximation rate in terms of the
number of parameters. One can more generally consider approximation error in terms of both the width W and depth
L simultaneously [50], but we leave this more general analysis as future work.
This problem has been previously solved (up to logarithmic factors) in the case where p = q = ∞, where the optimal
rate is given by
inf
fL∈ϒW,L(Rd)∥f −fL∥L∞(Ω) ≤C∥f∥Ws(L∞(Ω))L−2s/d
(1.5)
for a sufﬁciently large but ﬁxed width W. Speciﬁcally, this result was obtained for 0 < s ≤1 in [61] and for all s > 0
(up to logarithmic factors) in [40]. An analogous result also holds for Bs
r(L∞(Ω)) for 1 ≤r ≤∞. Further, the best rate
when both the width and depth vary (which generalizes (1.5)) has been obtained in [50].
The method of proof in these cases uses the bit-extraction technique introduced in [5] and developed further in [6]
to represent piecewise polynomial functions on a ﬁxed regular grid with N cells using only O(
√
N) parameters. This
enables an approximation rate of CN−2s/d in terms of the number of parameters N, which is signiﬁcantly faster than
traditional methods of approximation. This phenomenon has been called the super-convergence of deep ReLU net-
works [14,16,50,61]. The super-convergence has a limit, however, and the rate (1.5) is shown to be optimal using the
VC-dimension of deep ReLU neural networks [6,50,61].
In this work, we generalize this analysis to determine the optimal approximation rates (1.4) for all 1 ≤p,q ≤∞and
s > 0, i.e. to the approximation of any Sobolev or Besov class in Lp(Ω), with the exception of the Sobolev embedding
endpoint (described below). This was posed as a signiﬁcant open problem in [16]. We remark that the existing upper
3

A PREPRINT - APRIL 9, 2024
bounds in L∞clearly imply corresponding upper bounds in Lp for p < ∞. The key problem lies in extending the upper
bounds to that case where q < ∞, in which case we must approximate a larger function class. A further problem is the
extension of the lower bounds to the case p < ∞, in which we are measuring error in a weaker norm.
A necessary condition that we have any approximation rate in (1.4) at all is for the Sobolev space W s(Lq(Ω)) or Besov
space Bs
r(Lq(Ω) to be contained in Lp, i.e. W s(Lq(Ω)),Bs
r(Lq(Ω) ⊂Lp(Ω). Indeed, any deep ReLU neural network
represents a continuous function and so if f /∈Lp(Ω) it cannot be approximated at all by deep ReLU networks. We
will in fact consider the case where we have a compact embeddingW s(Lq(Ω)),Bs
r(Lq(Ω) ⊂⊂Lp(Ω). Here the symbol
A ⊂⊂B for two Banach spaces A and B means that A is contained in B and the unit ball of A is a compact subset of B.
This compact embedding is guaranteed for both Besov and Sobolev spaces by the strict Sobolev embedding condition
1
q −1
p −s
d < 0.
(1.6)
We determine the optimal rates in (1.4) under this condition. Speciﬁcally, we prove the following Theorems. The ﬁrst
two give an upper bound on the approximation rate by deep ReLU networks on Sobolev and Besov spaces, respectively.
Theorem 1. Let Ω= [0,1]d be the unit cube in Rd and let 0 < s < ∞and 1 ≤p,q ≤∞. Assume that 1
q −1
p < s
d, which
guarantees that we have the compact embedding
W s(Lq(Ω)) ⊂⊂Lp(Ω).
Then we have that
inf
fL∈ϒ25d+31,L(Rd)∥f −fL∥Lp(Ω) ≤C∥f∥W s(Lq(Ω))L−2s/d
for a constant C := C(s,q, p,d) < ∞.
Theorem 2. Let Ω= [0,1]d be the unit cube in Rd and let 0 < s < ∞and 1 ≤r, p,q ≤∞. Assume that 1
q −1
p < s
d ,
which guarantees that we have the compact embedding
Bs
r(Lq(Ω)) ⊂⊂Lp(Ω).
Then we have that
inf
fL∈ϒ25d+31,L(Rd)∥f −fL∥Lp(Ω) ≤C∥f∥Bsr(Lq(Ω))L−2s/d
for a constant C := C(s,r,q, p,d) < ∞.
We remark that the constant in Theorem 2 can be chosen uniformly in r. Note that the width W = 25d + 31 of our
networks are ﬁxed as L →∞, but scale linearly with the input dimension d. We remark that a linear scaling with
the input dimension is necessary since if d ≥W, then the set of deep ReLU networks is known to not be dense in
C(Ω) [32]. The next Theorem gives a lower bound which shows that the rates in Theorems 1 and 2 are sharp in terms
of the number of parameters.
Theorem 3. Let r, p,q ≥1 and s > 0, Ω= [0,1]d be the unit cube, and W,L ≥1 be integers. Then there exists an f
with ∥f∥Ws(Lq(Ω)) ≤1 and ∥f∥Bsr(Lq(Ω)) ≤1 such that
inf
fW,L∈ϒW,L(Rd)∥f −fW,L∥Lp(Ω) ≥C(p,d,s)min{W 2L2 log(WL),W 3L2}−s/d.
We remark that if the embedding condition (1.6) strictly fails, then a simply scaling argument shows that
W s(Lq(Ω)),Bs
r(Lq(Ω)) ⊈Lp(Ω) and we cannot get any approximation rate. On the boundary where the embedding
condition (1.6) holds with equality it is not a priori clear whether one has an embedding or not (this depends on the
precise values of s, p,q and r). Consequently this boundary case is much more subtle and we leave this for future
work.
The key technical difﬁculty in proving Theorem 1 is to deal with the case when p > q, i.e. when the target function’s
(weak) derivatives are in a weaker norm than the error. Classical methods of approximation using piecewise polynomi-
als or wavelets can attain an approximation rate of CN−s/d with N wavelet coefﬁcients or piecewise polynomials with
N pieces. When p ≤q this rate can be achieved by linear methods, while for p > q nonlinear, i.e. adaptive, methods
are required. For the precise details of this theory, see for instance [17,19,39].
Thus, in the linear regime where p ≤q we can use piecewise polynomials on a ﬁxed uniform grid to approximate
f, while in the non-linear regime we need to use piecewise polynomials on an adaptive (i.e. depending upon f)
non-uniform grid. This greatly complicates the bit-extraction technique used to obtain super-convergence, since the
methods in [50, 51, 61] are only applicable to regular grids. The tool that we develop to overcome this difﬁculty is
4

A PREPRINT - APRIL 9, 2024
a novel bit-extraction technique, presented in Theorem 4, which optimally encodes sparse vectors using deep ReLU
networks. Speciﬁcally, suppose that x ∈ZN is an N-dimensional integer vector with ℓ1-norm bounded by
∥x∥ℓ1 ≤M.
In Theorem 4 we give (depending upon N and M) a deep ReLU neural network construction which optimally encodes
x.
We remark, however, that super-convergence comes at the cost of parameters which are non-encodable, i.e. cannot be
encoded using a ﬁxed number of bits, and this makes the numerical realization of this approximation rate inherently
unstable. In order to better understand this, we recall the notion of metric entropy ﬁrst introduced by Kolmogorov.
The metric entropy numbers εN(A) of a set A ⊂X in a Banach space X are given by (see for instance [39], Chapter 15)
εN(A)H = inf{ε > 0 : A is covered by 2N balls of radius ε}.
An encodable approximation method consists of two maps, an encoding map E : A →{0,1}N mapping the class A to
a bit-string of length N, and a decoding map D : {0,1}N →X which maps each bit-string to an element of X. This
reﬂects the fact that any method which is implemented on a classical computer must ultimately encode all parameters
using some number of bits. The metric entropy numbers give the minimal reconstruction error of the best possible
encoding scheme.
Let Us(Lq(Ω)) := { f : ∥f∥W s(Lq(Ω)) ≤1} denote the unit ball of the Sobolev space W s(Lq(Ω)). The metric entropy
of this function class is given by
εN(Us(Lq(Ω)))Lp(Ω) ≂N−s/d
whenever the Sobolev embedding condition (1.6) is strictly satisﬁed. This is known as the Birman-Solomyak Theo-
rem [8]. The same asymptotics for the metric entropy also hold for the unit balls in the Besov spaces Bs
r(Lq(Ω)) if
the compact embedding condition (4.10) is satisﬁed. So the approximation rates in Theorems 1 and 2 are signiﬁcantly
smaller than the metric entropy of the Sobolev and Besov classes. This manifests itself in the fact that in the construc-
tion of the upper bounds in Theorems 1 and 2 the parameters of the neural network cannot be speciﬁed using a ﬁxed
number of bits, but rather need to be speciﬁed to higher and higher accuracy as the network grows [62], which is a
direct consequence of the bit-extraction technique.
Concerning the lower bounds, the key difﬁculty in proving Theorem 3 is to extend the VC-dimension arguments used
to obtain lower bounds when the error is measured in L∞to the case when the error is measured in the weaker norm
Lp for p < ∞. We do this by proving Theorem 5, which gives a general lower bound for Lp-approximation of Sobolev
spaces by classes with bounded VC dimension. We have recently learned of a different approach to obtaining Lp lower
bounds using VC-dimension [1], which is more generally applicable but introduces additional logarithmic factors in
the lower bound.
We remark that there are other results in the literature which obtain approximation rates for deep ReLU networks on
Sobolev spaces, but which do not achieve superconvergence, i.e. for which the approximation rate is only CN−s/d (up
to logarithmic factors), where N is the number of parameters [30, 60]. In addition, the approximation of other novel
function classes (other than Sobolev spaces, which suffer the curse of dimensionality) by neural networks has been
extensively studied recently, see for instance [4,13,14,35,44,52–54].
Finally, we remark that although we focus on the ReLU activation function due to its popularity and to simplify the
presentation, our results also apply to more general activation functions as well. Speciﬁcally, the lower bounds in
Theorem 3 based upon VC-dimension hold for any piecewise polynomial activation function. The upper bounds in
Theorems 1 and 2 hold as long as we can approximate the ReLU to arbitrary accuracy on compact subsets (i.e. ﬁnite
intervals) using a network with a ﬁxed size. Using ﬁnite differences this can be done for the ReLUk activation functions
deﬁned by
σk(x) =
0
x < 0
xk
x ≥0
when k ≥1 for instance. In fact, a similar construction using ﬁnite differences can approximate the ReLU as long as
the activation function is a continuous piecewise polynomial which is not a polynomial.
The rest of the paper is organized as follows. First, in Section 2 we describe a variety of deep ReLU neural network
constructions which will be used to prove Theorem 1. Many of these constructions are trivial or well-known, but
we collect them for use in the following Sections. Then, in Section 3 we prove Theorem 4 which gives an optimal
representation of sparse vectors using deep ReLU networks and will be key to proving superconvergence in the non-
linear regime p > q. In Section 4 we give the proof of the upper bounds in Theorems 1 and 2. Finally, in Section 5 we
prove the lower bound Theorem 3 and also prove the optimality of Theorem 4. We remark that throughout the paper,
unless otherwise speciﬁed, C will represent a constant which may change from line to line, as is standard in analysis.
The constant C may depend upon some parameters and this dependence will be made clear in the presentation.
5

A PREPRINT - APRIL 9, 2024
2
Basic Neural Network Constructions
In this section, we collect some important deep ReLU neural network constructions which will be fundamental in our
construction of approximations to Sobolev and Besov functions. Many of these constructions are well-known and will
be used repeatedly to construct more complex networks later on, so we collect them here for the reader’s convenience.
We being by making some fundamental observations and constructing some basic networks. Much of these are trivial
consequences of the deﬁnitions, but we collect them here for future reference. We begin by noting that by deﬁnition
we can compose two networks by summing their depths.
Lemma 1 (Composing Networks). Suppose L1,L2 ≥1 and that f ∈ϒW,L1(Rd,Rk) and g ∈ϒW,L2(Rk,Rl). Then the
composition satisﬁes
g(f(x)) ∈ϒW,L1+L2(Rd,Rl).
Further, if f is afﬁne, i.e. f ∈ϒ0(Rd,Rk), then
g(f(x)) ∈ϒW,L2(Rd,Rl).
Finally, if instead g is afﬁne, i.e. g ∈ϒ0(Rk,Rl) then
g(f(x)) ∈ϒW,L1(Rd,Rl)
We remark that combining this with the simple fact that we can always increase the width of a network, we can apply
Lemma 1 to networks with different widths and the width of the resulting network will be the maximum of the two
widths. We will use this extension without comment in the following.
Next, we give a simple construction allowing us to apply two networks networks in parallel.
Lemma 2 (Concatenating Networks). Let d = d1+d2 and k = k1+k2 with di,ki ≥1. Suppose that f1 ∈ϒW1,L(Rd1,Rk1)
and f2 ∈ϒW2,L(Rd2,Rk2). We view Rd = Rd1 ⊕Rd2 and Rk = Rk1 ⊕Rk2. Then the function f = f1 ⊕f2 : Rd →Rk
deﬁned by
(f1 ⊕f2)(x1 ⊕x2) = f1(x1)⊕f2(x2)
satisﬁes f1 ⊕f2 ∈ϒW1+W2,L(Rd,Rk).
Proof. This follows by setting the weight matrices Wi = W1
i ⊕W2
i and bi = b1
i ⊕b2
i , where W1
i ,b1
i and W2
i ,b2
i represent
the parameters deﬁning f1 and f2 respectively. Recally that the direct sum of matrices is simply given by
A⊕B =

A
0
0
B

.
Note that this result can be applied recursively to concatenate multiple networks. Combining this with the trivial fact
that the identity map is in ϒ2,1(R,R) we see that a network can be applied to only a few components of its input.
Lemma 3. Let m ≥0 and suppose that f ∈ϒW,L(Rd,Rk). Then the function f ⊕I on Rd+m deﬁned by
(f ⊕I)(x1 ⊕x2) = f(x1)⊕x2
satisﬁes f ⊕I ∈ϒW+2m,L(Rd+m,Rk+m).
Using these basic lemmas we obtain the well-known construction of a deep network which represents the sum of a
collection of smaller networks.
Proposition 1 (Summing Networks). Let fi ∈ϒW,Li(Rd,Rk) for i = i,...,n. Then we have
n
∑
i=1
fi ∈ϒW+2d+2k,L(Rd,Rk),
where L = ∑n
i=1 Li.
For completeness we give a detailed proof in Appendix A. An important application of this is the following well-known
result showing how piecewise linear continuous functions can be represented using deep networks.
Proposition 2. Suppose that f : R →R is a continuous piecewise linear function with k pieces. Then f ∈ϒ5,k−1(R).
6

A PREPRINT - APRIL 9, 2024
For the readers convenience, we give the proof in Appendix A.
Next we describe how to approximate products using deep ReLU networks. This will be necessary in the following to
approximate piecewise polynomial functions. The method for doing this is based upon a construction of Telgarsky [55]
and was ﬁrst applied to approximating smooth functions using neural networks by Yarotsky [60]. This construction
has since become an important tool in the analysis of deep ReLU networks and has been used by many different
authors [16,40,44]. For the readers convenience, we reproduce a complete description of the construction in Appendix
B.
Proposition 3 (Product Network, Proposition 3 in [60]). Let k ≥1. Then there exists a network fk ∈ϒ13,6k+3(R2)
such that for all x,y ∈[−1,1] we have
|fk(x,y)−xy| ≤6 ·4−k.
The key to obtaining superconvergence for deep ReLU networks is the bit extraction technique, which was ﬁrst intro-
duced in [5] with the goal of lower bounding the VC dimension of the class of neural networks with polynomial activa-
tion function. This technique as also been used to obtain sharp approximation results for deep ReLU networks [50,61].
In the following Proposition, which is a minor modiﬁcation of Lemma 11 in [6], we construct the bit extraction net-
works that we will need in our approximation of Sobolev and Besov functions. For the readers convenience, we give
the complete proof in Appendix C.
Proposition 4 (Bit Extraction Network). Let n ≥m ≥0 be an integer. Then there exists a network fn,m ∈ϒ9,4m(R,R2)
such that for any input x ∈[0,1] with at most n non-zero bits, i.e.
x = 0.x1x2 ···xn
(2.1)
with bits xi ∈{0,1}, we have
fn,m(x) =

0.xm+1 ···xn
x1x2 ···xm.0

.
Finally, in order to deal with the case when the error is measured in L∞, we will need the following technical construc-
tion. We construct a ReLU network which takes an input in Rd and returns the k-th largest entry. The ﬁrst step is the
following simple Lemma, whose proof can be found in Appendix A.
Lemma 4 (Max-Min Networks). There exists a network p ∈ϒ4,1(R2,R2) such that
p

x
y

=

max(x,y)
min(x,y)

.
Using these networks as building blocks, we can implement a sorting network using deep ReLU neural networks.
Proposition 5. Let k ≥1 and d = 2k be a power of 2. Then there exists a network s ∈ϒ4d,L(Rd,Rd) where L =
 k+1
2

which sorts the input components.
Note that the power of 2 assumption is for simplicity and is not really necessary. It is also known that the depth
 k+1
2

can be replaced by a multiple Ck where C is a very large constant [2,43], but this will not be important in our argument.
Proof. Suppose that (i1, j1),...,(i2k−1, j2k−1) is a pairing of the indices of Rd. By Lemma 4 and Lemma 2, there exists
a network g ∈ϒ4d,1(Rd,Rd) which satisﬁes for all l = 1,...,k −1
g(x)il = max(xil,xjl), g(x)jl = min(xil,xjl),
i.e. which sorts the entries in each pair. By a well-known construction of sorting networks (for instance bitonic
sort [7]), composing
 k+1
2

such functions can be used to sort the input.
Finally, we note that by selecting a single output (which is an afﬁne map), we can obtain a network which outputs any
order statistic.
Corollary 1. Let 1 ≤τ ≤d and d = 2k is a power of 2. Then there exists a network gτ ∈ϒ4d,L(Rd) with L =
 k+1
2

such that
gτ(x) = x(τ),
where x(τ) is the τ-th largest entry of x.
7

A PREPRINT - APRIL 9, 2024
3
Optimal Representation of Sparse Vectors using Deep ReLU Networks
In this section, we prove the main technical result which enables the efﬁcient approximation of Sobolev and Besov
functions in the non-linear regime when q < p. Speciﬁcally, we have the following Theorem showing how to optimally
represent sparse integer vectors using deep ReLU neural networks.
Theorem 4. Let M ≥1 and N ≥1 and x ∈ZN be an N-dimensional vector satisfying
∥x∥ℓ1 ≤M.
(3.1)
Then if N ≥M, there exists a neural network g ∈ϒ17,L(R,R) with depth L ≤C
p
M(1 + log(N/M)) which satisﬁes
g(n) = xn for n = 1,...,N.
Further, if N < M, then there exists a neural network g ∈ϒ17,L(R,R) with depth L ≤C
p
N(1 + log(M/N)) which
satisﬁes g(n) = xn for n = 1,...,N.
Before proving this theorem, we explain the meaning of the result and give some intuition. We let
SN,M = {x ∈ZN, ∥x∥ℓ1 ≤M}
(3.2)
denote the set of integer vectors which we wish to encode. We can estimate the cardinality of this set as follows. Using
a stars and bars argument we see that
|{x ∈ZN
≥0, ∥x∥ℓ1 ≤M}| =
N + M
N

=
N + M
M

.
Further, the signs of each non-zero entry of the above set can be chosen arbitrarily. The number of such choices of
sign is equal to the number of non-zero entries and is at most min{M,N}. This gives the bound
|SN,M| ≤
2M N+M
M

N ≥M
2N N+M
N

N < M.
Taking logarithms and utilizing the bound from Lemma 9 (proved later), we estimate
log|SN,M| ≤C
M(1 + log(N/M))
N ≥M
N(1 + log(M/N))
N < M,
and this controls the number of bits required to encode the set SN,M. Theorem 4 implies that using deep ReLU neural
networks, the number of parameters required is the square root of the number of bits required for such an encoding.
This is analogous to the original application of bit extraction [5] and underlies the superconvergence phenomenon.
Finally, we note that in Theorem 6 from Section 5 we prove that Theorem 4 itself is optimal as long as M is not
exponentially small or exponentially large relative to N.
Proof of Theorem 4. Let M ≥1 and N ≥1 be ﬁxed. There are two cases to consider, when N ≥M and when N < M.
The key to the construction in both cases will be an explicit length k binary encoding of the set SN,M deﬁned in equation
(3.2).
By a length k binary encoding we mean a pair of maps:
• E : SN,M →{0,1}≤k (an encoding map which maps SN,M to a bit-string of length at most k)
• D : {0,1}≤k →SN,M (a decoding map which recovers x ∈SN,M from a bit-string of length at most k)
which satisfy
D(E(x)) = x.
Note that the bound in equation 3 implies that there exists such an encoding as long as
k ≥C
M(1 + log(N/M))
N ≥M
N(1 + log(M/N))
N < M.
However, in order to construct deep ReLU networks which prove Theorem 4, we will need to construct encoding and
decoding maps E and D which are given by an explicit, simple algorithm. These will then be used to construct the
neural network g.
8

A PREPRINT - APRIL 9, 2024
Let us begin with the ﬁrst case, when N ≥M. In this case, we set k = 2M(3 + ⌈log(N/M)⌉) (note that all logarithms
are taken with base 2). The encoding map E is deﬁned as
E(x) = f1t1 f2t2 ··· fRtR,
the concatenation of R ≤2M blocks consisting of fi ∈{0,1}1+⌈log(N/M)⌉and ti ∈{0,1}2. The fi-bits encode an offset
in {0,1,...,⌈N/M⌉} (via binary expansion), and the ti-bits encode a value in {0,±1} (via 0 = 00, 1 = 10, and −1 = 01).
The fi and ti are determined from the input x ∈SN,M by Algorithm 1.
It is clear that the number of blocks R produced by Algorithm 1 is at most 2M since in each round of the while loop
either fi = ⌈N/M⌉(which can happen at most M times before the index j reaches the end of the vector) or the entry
rj is decremented (which can happen at most M times since ∥x∥ℓ1 ≤M).
Algorithm 1 Small ℓ1-norm Encoding Algorithm
Input: x ∈ZN, ∥x∥ℓ1 ≤M
1: Set j = 0, r = x {Set pointer right before the beginning of the input x and the residual to x}
2: Set i = 1
3: while r ̸= 0 do
4:
l = min{i : ri ̸= 0} {Find the ﬁrst non-zero index in the residual}
5:
if l −j ≤⌈N/M⌉then {If we can make it to the next non-zero index, do so}
6:
fi = l −j
7:
j = l
8:
else {Otherwise go as far as we can}
9:
fi = ⌈N/M⌉
10:
j = j + ⌈N/M⌉
11:
end if
12:
if j = l then {If we are at the next non-zero index, ti captures its sign}
13:
ti = sgn(rj)
14:
rj = rj −ti {This decrements ∥r∥ℓ1 which can happen at most M times}
15:
else {This can only happen if fi = ⌈N/M⌉, which can occur at most M times}
16:
ti = 0
17:
end if
18:
i = i+ 1
19: end while
Next, we consider the case N < M. In this case we set k = 2N(3 + ⌈log(M/N)⌉), and deﬁne the encoding map E via
E(x) = t1 f1t2 f2 ···tR fR,
i.e. E(x) is the concatenation of R ≤2N blocks consisting of ti ∈{0,1}2+⌈log(M/N)⌉and fi ∈{0,1}. The fi-bits encode
an offset in {0,1}, and the ti-bits encode a value in {−⌈M/N⌉,...,⌈M/N⌉}. Here the ﬁrst bit of each ti determines
its sign, while the remaining 1 + ⌈log(M/N)⌉bits consist of the binary expansion of its magnitude (which lies in
{0,...,⌈M/N⌉}). The ti and fi are determined from the input x ∈SN,M by Algorithm 2.
It is clear that the number of blocks R produced by Algorithm 2 is at most 2N since in each round of the while loop
either the entry rj is decremented by at least ⌈M/N⌉(which can happen at most N times since ∥x∥ℓ1 ≤M), or the entry
rj is zeroed out (which can happen at most N times before r = 0 since there are only N entries).
In both cases, the decoding map D is given by algorithm 3. It is easy to verify that algorithm 3 reconstructs the input
x from the output of either algorithm 1 or 2.
We now show how to use these algorithms to construct an appropriate deep ReLU neural network g. Let S be a
threshold parameter, to be chosen later.
Given a vector x ∈ZN, we decompose it into two pieces x = xB +xs (here xB represents the ‘big’ part and xs the ‘small’
part). We deﬁne
xB
i =
|xi|
xi ≥S
0
xi < S
and
xs
i =
0
xi ≥S
|xi|
xi < S
9

A PREPRINT - APRIL 9, 2024
Algorithm 2 Large ℓ1-norm Encoding Algorithm
Input: x ∈ZN, ∥x∥ℓ1 ≤M
1: Set j = 0, r = x {Set pointer right before the beginning of the input x and the residual to x}
2: Set i = 1
3: while r ̸= 0 do
4:
if j = 0 or rj = 0 then {If the value at the current index is 0, then shift the index}
5:
fi = 1
6:
j = j + 1
7:
else
8:
fi = 0
9:
end if
10:
if |rj| ≤⌈M/N⌉then {If we can fully capture the current value, do so}
11:
ti = rj
12:
rj = 0 {This zeros out an entry, which can happen at most N times}
13:
else {Otherwise capture as much as we can}
14:
ti = sgn(rj)⌈M/N⌉
15:
rj = rj −ti {This reduces ∥r∥ℓ1 by at least ⌈M/N⌉which can happen at most N times}
16:
end if
17:
i = i+ 1
18: end while
Algorithm 3 Decoding Algorithm
Input: A bit string f1t1 ··· fRtR
1: Set x = 0 and j = 0 {Start with the 0 vector}
2: for i = 1,...,R do
3:
j = j + fi {Shift index by fi}
4:
x j = x j +ti {Increment value by ti}
5: end for
The large part xB has small support and so can be efﬁciently encoded as a piecewise linear function. Speciﬁcally, the
ℓ1-norm bound (3.1) on x implies that the support of xB is at most of size
|{n : xB
n ̸= 0}| ≤∥xB∥ℓ1
S
≤∥x∥ℓ1
S
≤M
S .
This means that there is a piecewise linear function with at most 3M/S pieces which matches the values of xB, so by
Proposition 2 there exists a network
gB ∈ϒ5,L(R)
with depth bounded by L ≤3M/S such that gB(n) = xB
n for n = 1,...,N.
The heart of the proof is an efﬁcient encoding of the small part xs. This requires the encoding and decoding algorithms
1, 2 and 3. We consider ﬁrst the case M ≤N, which is captured in the following Proposition.
Proposition 6. Let M ≤N and suppose that x ∈ZN and satisﬁes ∥x∥ℓl ≤M and ∥x∥ℓ∞< S. Then there exists a
g ∈ϒ15,L(R) such that g(n) = xn for n = 1,...,N with
L ≤8M/S + 8S(5 + ⌈log(N/M)⌉)+ 4.
The proof is quite technical, so let us give a high-level description of the ideas ﬁrst. The idea is to take the execution
of the decoding algorithm 3 which reconstructs x and to divide it into blocks of length on the order of S. Each block
will start at a point i in the algorithm at which x j = 0 before step 4 of the loop in 3. During the execution of this block,
the index j increases and reaches a larger value at the end of the block. All of the entries xn for n between these values
is reconstructed during the given block of the reconstruction algorithm.
We now construct three networks. Given an input index n, ﬁnd the block during which the value xn is reconstructed.
On the input index n, one network outputs the value of j at the beginning of the block, and another network outputs a
real number whose binary expansion contains the bits consumed during this block. Both of these can be implemented
using piecewise linear functions whose number of pieces is proportional to the number of blocks. The ﬁnal network
10

A PREPRINT - APRIL 9, 2024
extracts the bits from the output of the second network and implements the execution of algorithm 3 in this block to
reconstruct the value of x.
Before giving the detailed proof of Proposition 6, let us complete the proof of Theorem 4 in the case M ≤N. We apply
Proposition 6 to the small part xs to get a network gs. Then we use Proposition 1 to add this network to the network
gB representing the large part xB to get a network g ∈ϒ17,L(R) with
L ≤11M/S + 8S(5 + ⌈log(N/M)⌉)+ 4
such that g(n) = xn for n = 1,...,N. Finally, we choose S optimally, namely
S =
s
M
5 + ⌈log(N/M)⌉,
to get L ≤C
p
M(1 + log(N/M)) as desired.
Proof of Proposition 6. Let f1t1 ··· fRtR be the output of the encoding Algorithm 1 run on input xs. Let
F := {i ∈{1,...,R} : fi = 0}.
We decompose the set F into intervals, i.e.
F =
T[
m=1
[Bm,Um],
where [I,J] := {I,I + 1,...,J} for I ≤J and Bm+1 > Um + 1.
Note that since ∥x∥ℓ∞< S, the length of these intervals is strictly less than S, i.e. Um−Bm+1 < S for m = 1,...,T . This
holds since the encoding algorithm 1 stays at the same index for all steps i ∈[Bm,Um]. Hence this index is decremented
Um −Bm + 1 times and so this quantity must be smaller than S.
Let ρ = ⌈R/S⌉and consider steps i0,...,iρ deﬁned by iρ = R+ 1 (the end of the algorithm) and
ik =
1 + kS
1 + kS /∈F
Bm −1
1 + kS ∈[Bm,Um],
for k = 0,...,ρ −1. (Note that f1 ̸= 0 since the index j starts at 0 in algorithm 1. Thus 1 /∈F and so i0 = 1.)
The bound Um −Bm + 1 < S on the length of the interval [Bm,Um] implies that ik > 1 −(k −1)S. This implies that
ik−1 < ik and also that the gaps satisfy ik −ik−1 < 2S for all k = 1,...,ρ.
Next, let indices jk for k = 0,...,ρ −1 be the values of the index j at the beginning of step ik in the decoding algorithm
3. We also set jρ = N. Since by construction the intervals are not consecutive, i.e. Bm+1 > Um + 1, the steps ik /∈F,
i.e. fik > 0. This means that jk−1 < jk for all k = 1,...,ρ.
Observe that the steps ik and indices jk have been constructed such that for an integer n in the interval jk < n ≤jk+1,
the value xn is only affected during the steps ik,...,ik+1 −1 in the reconstruction algorithm 3. Further, the length of
each block satisﬁes ik −ik−1 < 2S.
Next, we construct two piecewise linear functions J and R as follows. For integers n = 1,...,N, we set
J(n) = jk for jk < n ≤jk+1,
and
R(n) = rk for jk < n ≤jk+1,
where
rk = 0.fiktik ··· fik+1−1tik+1−1
is the real number whose binary expansion contains the encoding of x from step ik to ik+1 −1 (followed by zeros).
Both J and R take at most ρ + 1 different values and hence can be implemented by piecewise linear functions with at
most 2ρ + 1 pieces. Thus, by Proposition 2 we have J,R ∈ϒ5,2ρ(R).
We being our network construction as follows. We begin with the afﬁne map
x →
 x
x
x
!
∈ϒ0(R,R3),
11

A PREPRINT - APRIL 9, 2024
and use Lemmas 1 and 3 to apply J to the ﬁrst component and then apply R to the second component to get
x →
 J(x)
x
x
!
→
 J(x)
R(x)
x
!
∈ϒ9,4ρ(R,R3).
Composing with the afﬁne map
 x
y
z
!
→
 z−x
y
0
!
∈ϒ0(R3,R3),
and using Lemma 1 again we get that
x →
 x−J(x)
R(x)
0
!
∈ϒ9,4ρ(R,R3).
(3.3)
Applied to an integer jk < n ≤jk+1, this network maps
n →
 
n −jk
0.fiktik ··· fik+1−1tik+1−1
0
!
.
Thus the ﬁrst entry is the gap between n and the index j at the beginning of step ik and the last entry is the value of xn
at the beginning of step ik, while the middle entry contains the bits used by the algorithm between steps ik,...,ik+1 −1.
The proof will now be completed by constructing a network which applies a single step of the decoding algorithm 3
to each of these entries, this is collected in the following technical Lemma.
Lemma 5. Given positive integers α and β there exists a network g ∈ϒ15,4α+16(R3,R3) such that
g :
 
x
0.f1t1 ··· fktk
Σ
!
→
 
x−f1
0.f2t2 ··· fktk
Σ+t1δ(x−f1)
!
whenever x ∈Z, k ≤β and len(fi) = α. Here the fi denote integers encoded via binary expansion and len(fi) is the
length of this expansion, ti ∈{±1,0} are encoded using two bits (speciﬁcally via 0 = 00, 1 = 10 and −1 = 01), Σ
denotes a running sum, and δ is the integer Dirac delta deﬁned by
δ(z) =
1
z = 0
0
z ̸= 0
for integer inputs z.
Before proving this Lemma, let us complete the proof of Proposition 6. We set α = 1+⌈log(N/M)⌉and β = 2S, and
compose the map in (3.3) with 2S copies of the network given by Lemma 5. Then we ﬁnally compose with an afﬁne
map which selects the last coordinate. This gives a g ∈ϒ15,L(R) with
L = 4ρ + 2S(4α + 16) = 4⌈R/S⌉+ 8S(5 +⌈log(N/M)⌉)
≤8M/S + 8S(5 + ⌈log(N/M)⌉)+ 4,
since R ≤2M.
When applied to an integer n ∈{1,...,N} with jk < n ≤jk+1, the map in (3.3) sets the offset between n and the index
jk and the start of step ik, outputs a number whose binary expansion contains the bits used from step ik to step ik+1 −1,
and sets a running sum to 0.
Then the 2S copies of the network from Lemma 5 implement algorithm 3 from step ik to step ik+1 −1. Note that if
the number of steps is less than 2S, the network pads with zero blocks (fi,ti) = 0, and these additional steps have no
effect. Since by construction the entry xn is only modiﬁed during these steps, the running sum will now be equal to xn.
Finally, we select the last coordinate, which guarantees that g(n) = xn.
Proof of Lemma 5. We construct the desired network as follows. We use Lemma 3 to apply the bit extractor network
fn,α from Proposition 4 to the second component. Here we choose n ≥β(α +2), which is guaranteed to be larger than
the length of the bit-string in the second component. This results in the map
 
x
0.f1t1 ··· fktk
Σ
!
→



x
f1
0.t1 f2t2 ··· fktk
Σ


∈ϒ13,4α(R3,R4).
12

A PREPRINT - APRIL 9, 2024
Subtracting the second component from the ﬁrst, this gives
 
x
0.f1t1 ··· fktk
Σ
!
→
 
x−f1
0.t1 f2t2 ··· fktk
Σ
!
∈ϒ13,4α(R3,R3),
(3.4)
and completes the ﬁrst part of the construction.
Next, we implement a network which extracts the two bits corresponding to t and then adds t to the third component
iff the ﬁrst component is 0. Let h(z) denote the continuous piecewise linear function
h(z) =







0
z ≤−1
z+ 1
−1 < z ≤0
1 −z
0 < z ≤1
0
z > 1.
(3.5)
For integer inputs, h is simply the delta function, i.e. h(z) = δ(z) for z ∈Z, and by Proposition 2 we have h ∈ϒ5,3(R).
We ﬁrst apply an afﬁne map which duplicates the ﬁrst coordinate
 z1
z2
z3
!
→



z1
z1
z2
z3


∈ϒ0(R3,R4).
Then, we use Lemma 3 to apply h to the second coordinate and apply the bit extractor network fn,1 from Proposition
4 to the third component. As before, we choose n ≥β(α + 2) which is guaranteed to be larger than the length of the
bit-string in the second component. This gives (note that we write b1b2 for the two bits corresponding to t1)
 
z1
0.b1b2 f2t2...fktk
z3
!
→





z1
h(z1)
b1
0.b2 f2t2...fktk
z3




∈ϒ15,7(R3,R5).
Now we compose this using Lemma 1 with the map





z1
z2
z3
z4
z5




→



z1
z2 + z3 −1
z4
z5


→



z1
σ(z2 + z3 −1)
z4
z5


→
 
z1
z4
z5 + σ(z2 + z3 −1)
!
∈ϒ7,1(R5,R3).
Here the ﬁrst and last maps in the composition are afﬁne and the middle map is in ϒ7,1(R4,R4) by Lemma 3. This
gives
 
z1
0.b1b2 f2t2...fktk
z3
!
→
 
z1
0.b2 f2t2...fktk
z3 + σ(h(z1)+ b1 −1)
!
∈ϒ15,8(R3,R3).
(3.6)
Notice that σ(h(z1)+ b1 −1) equals 1 precisely when z1 = 0 and b1 = 1 and equals zero otherwise (for integral z1).
In an analogous manner, we get
 
z1
0.b2 f2t2...fktk
z3
!
→
 
z1
0.f2t2...fktk
z3 −σ(h(z1)+ b2 −1)
!
∈ϒ15,8(R3,R3).
(3.7)
Composing the networks in (3.6) and (3.7) will extract t1 ∈{0,±1} (recall the encoding 0 = 00, 1 = 10 and −1 = 01)
and add t1 to the last coordinate iff the ﬁrst coordinate is 0. Composing this with the network in (3.4) gives a network
g ∈ϒ15,4α+16(R3,R3) as stated in the Lemma.
Next, we consider the case M > N, which is somewhat complicated by the fact that the threshold parameter S and the
spacing of the blocks are no longer equal in this case. The key construction is contained in the following Proposition.
13

A PREPRINT - APRIL 9, 2024
Proposition 7. Let M > N and suppose that x ∈ZN and satisﬁes ∥x∥ℓl ≤M and ∥x∥ℓ∞< S. Then there exists a
g ∈ϒ15,L(R) such that g(n) = xn for n = 1,...,N with
L ≤8M/S + 8(SN/M+ 1)(4 + ⌈log(M/N)⌉)+ 4.
Utilizing this Proposition, we complete the proof of Theorem 4 in the case M > N. We apply Proposition 7 to xs and
use Proposition 1 to add the network to the network representing xB to get a network g ∈ϒ17,L(R) representing x with
L ≤11M/S + 8(SN/M+ 1)(4 + ⌈log(M/N)⌉)+ 4.
Finally, we optimize in S, resulting in a value
S = M
p
4 + ⌈log(M/N)⌉
√
N
to get L ≤C
p
N(1 + log(M/N) as desired.
Proof of Proposition 7. The proof proceeds in a very similar manner to the proof of Proposition 6 and we only indicate
the differences here.
We begin with the same set F and its decomposition into intervals [Bm,Um], except that f1t1 ··· fRtR is now the output
of the encoding algorithm 2.
Our bound on the block length becomes Um −Bm + 1 ≤SN/M. This holds since the encoding algorithm stays at the
same index for all steps i ∈[Bm,Um], and thus this index in decremented by an amount ⌈M/N⌉a total of Um −Bm + 1
times. The bound on the ℓ∞-norm implies that (M/N)(Um −Bm + 1) < S, which gives the desired bound.
Thus, in this case we set T = ⌈SN/M⌉and ρ = ⌈R/T⌉and consider steps i0,...,iρ deﬁned by iρ = R + 1 (the end of
the algorithm) and
ik =
1 + kT
1 + kT /∈F
Bm −1
1 + kT ∈[Bm,Um],
for k = 0,...,ρ −1.
We now proceed with the same argument as in Proposition 6, except that the bound on Um −Bm + 1 < T implies that
all block lengths are bounded by 2T. The proof is ﬁnally completed with the following variant of Lemma 5, which
implements a step of the decoding algorithm 3 with the values fi and ti encoded as they are for M > N.
Lemma 6. Given positive integers α and β there exists a network g ∈ϒ15,4α+8(R3,R3) such that
g :
 
x
0.f1t1 ··· fktk
Σ
!
→
 
x−f1
0.f2t2 ··· fktk
Σ+t1δ(x−f1)
!
whenever x ∈Z, k ≤β and len(ti) = α. Here fi ∈{0,1} are single bits, ti ∈Z is encoded via binary expansion with
a single bit giving its sign and len(ti) is the length of this expansion, Σ denotes a running sum, and δ is the integer
Dirac delta deﬁned by
δ(z) =
1
z = 0
0
z ̸= 0
for integer inputs z.
Given this lemma, we complete the proof as before, setting α = 2 + ⌈log(M/N)⌉and β = 2T and composing the
network implementing the maps J and R with 2T copies of the network from Lemma 6. This gives a network g ∈
ϒ15,L(R) with
L ≤4ρ + 2T(4α + 8) = 4⌈R/T⌉+ 8T(4 + ⌈log(M/N)⌉)
≤8N/T + 8T(4 + ⌈log(M/N)⌉)+ 4
≤8M/S + 8(SN/M+ 1)(4 + ⌈log(M/N)⌉)+ 4,
since R ≤2N and T = ⌈SN/M⌉.
14

A PREPRINT - APRIL 9, 2024
Proof of Lemma 6. We use Lemma 3 to apply the bit extractor network fn,1 from Proposition 4 to the second compo-
nent. Here we choose n ≥β(α + 1), which is guaranteed to be larger than the length of the bit-string in the second
component. Then we subtract the second component from the ﬁrst. This results in the map
 
x
0.f1t1 ··· fktk
Σ
!
→
 
x−f1
0.t1 f2t2 ··· fktk
Σ
!
∈ϒ13,4(R3,R3),
and completes the ﬁrst part of the construction.
Now we wish to extract the integer t1 and add it to Σ iff the ﬁrst coordinate (which is an integer) is 0. We do this
by using Lemma 3 to apply the bit extractor network fn,1 to the second coordinate and then apply fn,α−1 to the third
coordinate of the result to get
 
z1
0.b1b2...bα f2t2...fktk
z3
!
→





z1
b1
b2...bα
0.f2t2...fktk
z3




∈ϒ15,4α(R3,R5),
(3.8)
where we have written b1b2...bα for the bits of t1.
Next, consider the following sequence of compositions, where h is the function deﬁned in (3.5),





z1
z2
z3
z4
z5




→







z1
z1
z2
z3
z4
z5







→







z1
h(z1)
z2
z3
z4
z5







→







z1
z2
z3
z3 −2α(1 −h(z1)+ z2)
z4
z5







→
→







z1
z2
z3
σ(z3 −2α(1 −h(z1)+ z2))
z4
z5







→
→





z1
z2
z3
z4
z5 + σ(z3 −2α(1 −h(z1)+ z2))




∈ϒ15,4(R5,R5).
(3.9)
Using a sequence of applications of Lemmas 3 and 1, we obtain that this map can be implemented by a network in
ϒ15,4(R5,R5).
Note that when z1 ∈Z and z2 ∈{0,1}, we have that (recall that h(z) = δ(z) for integer z)
(1 −h(z1)+ z2) =







0
z1 = 0 and z2 = 0
1
z1 ̸= 0 and z2 = 0
1
z1 = 0 and z2 = 1
2
z1 ̸= 0 and z2 = 1.
If we also have that z3 ∈{0,...,2α}, then it follows that
σ(z3 −2α(1 −h(z1)+ z2)) =
z3
z1 = 0 and z2 = 0
0
otherwise.
Thus, if we compose the network in (3.8) with the network in (3.9), we will add the number b2...bα (which is less than
2α) to the last coordinate iff z1 = b1 = 0. As b1 = 0 to indicate that t1 is positive and b2...bα contain the value of t1,
his handles the case where t1 is positive and has no effect when t1 is negative.
15

A PREPRINT - APRIL 9, 2024
Next, we construct a network which handles the negative part of t1. This is given by the following composition





z1
z2
z3
z4
z5




→







z1
z1
z2
z3
z4
z5







→







z1
h(z1)
z2
z3
z4
z5







→







z1
z2
z3
z3 −2α(2 −h(z1)−z2)
z4
z5







→
→







z1
z2
z3
σ(z3 −2α(2 −h(z1)−z2))
z4
z5







→
→





z1
z2
z3
z4
z5 −σ(z3 −2α(2 −h(z1)−z2))




∈ϒ15,4(R5,R5).
(3.10)
When z1 ∈Z and z2 ∈{0,1}, we have that
(2 −h(z1)−z2) =







1
z1 = 0 and z2 = 0
2
z1 ̸= 0 and z2 = 0
0
z1 = 0 and z2 = 1
1
z1 ̸= 0 and z2 = 1.
So, if we compose the network in (3.8) with the network in (3.10), we will subtract the number b2...bα (which is less
than 2α) from the last coordinate iff z1 = 0 and b1 = 1. As b1 = 1 to indicate that t1 is negative and b2...bα contain the
value of t1, this handles the case where t1 is negative and has no effect when t1 is positive.
We obtain the ﬁnal network g by successively composing the network in (3.8) with the networks in (3.9) and (3.10)
and then dropping the second and third components.
4
Optimal Approximation of Sobolev Functions Using Deep ReLU Networks
In this section, we give the main construction and the proof of Theorems 1 and 2. A key component of the proof is
the approximation of piecewise polynomial functions using deep ReLU neural networks. To describe this, we ﬁrst
introduce some notation.
Throughout this section, unless otherwise speciﬁed, let b ≥2 be a ﬁxed integer. To avoid excessively cumbersome
notation, we suppress the dependence on b in the following notation. Let l ≥0 be an integer and consider the b-adic
decomposition of the cube Ω= [0,1)d (note that by removing a zero-measure set it sufﬁces to consider this half-open
cube in the proof) at level l given by
Ω=
[
i∈Il
Ωl
i,
(4.1)
where the index i lies in the index set Il := {0,...,bl −1}d, and Ωl
i is deﬁned by
Ωl
i =
d
∏
j=1
[b−lij,b−l(ij + 1)).
(4.2)
Note that for each l, the bdl subcubes Ωl
i form a partition of the original cube Ω. For an integer k ≥0, we let Pk denote
the space of polynomials of degree at most k and consider the space
Pl
k =
n
f : Ω→R, fΩl
i ∈Pk for all i ∈Il
o
16

A PREPRINT - APRIL 9, 2024
of (non-conforming) piecewise polynomials subordinate to the partition (4.1). The space Pl
k has dimension
 d+k
k

bdl
and a natural (L∞-normalized) basis
ρα
l,i(x) =

∏d
j=1(blxj −ij)αj
x ∈Ωl
i
0
x /∈Ωl
i
indexed by i ∈Il and α a d-dimensional multi-index with |α| ≤k.
In our construction, we will approximate piecewise polynomial functions from Pl
k by deep ReLU neural networks.
However, since a deep ReLU network can only represent a piecewise continuous function, this approximation will not
be over the full cube Ω. Rather, we will need to remove an arbitrarily small region from Ω. This idea is from the
method in [50], where this region was called the triﬂing region. Given ε > 0 we deﬁne sets
Ωl
i,ε =
d
∏
j=1
[b−lij,b−l(ij + 1)−ε)
ij < bl −1
[b−lij,b−l(ij + 1))
ij = bl −1,
(4.3)
which are slightly shrunk sub-cubes (except at one edge) from (4.2). We then deﬁne the good region to be
Ωl,ε :=
[
i∈Il
Ωl
i,ε.
Next, we will show how to approximate piecewise polynomials from Pl
k on the set Ωl,ε. For this, we begin with the
following Lemma, which ﬁrst appears in [50]. This Lemma is essentially a minor modiﬁcation of the bit-extraction
technique used to prove Proposition 4. We give a detailed proof for the reader’s convenience in Appendix C.
Lemma 7. Let l ≥0 be an integer and 0 < ε < b−l. Then there exists a deep ReLU neural network qd ∈ϒ9d,2(b−1)l(Rd)
such that
qd(Ωl
i,ε) = ind(i) :=
d
∑
j=1
bl(j−1)i j.
Note that here ind(i) ∈{0,...,bdl −1} is just an integer index corresponding to the sub-cube position i.
Using this Lemma we prove the following key technical Proposition, which shows how to efﬁciently approximate
piecewise polynomial functions on the good set Ωl,ε.
Proposition 8. Let l ≥0 be an integer and ε > 0. Suppose that f ∈Pl
k is expanded in terms of the bases ρα
l,i,
f(x) =
∑
|α|≤k, i∈Il
aα
i ρα
l,i(x).
Let 1 ≤q ≤p ≤∞and choose a parameter δ > 0 and an integer m ≥1. Then there exists a deep ReLU network
fδ,m ∈ϒ22d+18,L(Rd) such that
∥f −fδ,m∥Lp(Ωl,ε) ≤C

δ min
n
1,b−dlδ −qo1/p
+ 4−m
 
∑
|α|≤k, i∈Il
|aα
i |q
!1/q
(with the standard modiﬁcation when q = ∞), and whose depth satisﬁes
L ≤C
m+ l + δ −q/2p
1 + dl log(b)+ qlog(δ)
δ −q ≤bdl
m+ l + bdl/2p
1 −logδ −(dl/q)log(b)
δ −q > bdl.
Here the constants C := C(p,q,d,k,b) only depend upon p,q,d,k and the base b, but not on f, δ, l, ε, or m.
Before we prove this Proposition, let us explain the intuition behind it and the meaning of the parameter δ. The
parameter δ represents a discretization level for the coefﬁcients aα
i . Speciﬁcally, we will round each coefﬁcient
down (in absolute value) to the nearest multiple of δ to produce an approximation to f. Then, we will represent this
approximation by encoding these discretized coefﬁcients using deep ReLU networks. This reduces to encoding an
integer vector which can be done optimally using Theorem 4. The two regimes δ −q ≤bdl and δ −q > bdl correspond
to the case of dense and sparse coefﬁcients, which are handled differently in Theorem 4.
Proof of Proposition 8. We begin by decomposing f = ∑|α|≤k fα where
fα(x) = ∑
i∈Id
aα
i ρα
l,i(x).
17

A PREPRINT - APRIL 9, 2024
By Proposition 1 and the triangle inequality, it sufﬁces to prove the result for each fα individually with width W =
20d + 17 (at the expense of larger constants). So in the following we assume that f = fα and write ai := aα
i . By
normalizing f we may assume also without loss of generality that
 
∑
i∈Il
|ai|q
!1/q
≤1.
(4.4)
We construct the following network. First, duplicate the input x ∈Rd three times using an afﬁne map
x →
 x
x
x
!
∈ϒ0(Rd,R3d).
Next, we use Lemmas 3 and 1 to apply the network qd from Lemma 7 to the last coordinate and apply q1 from Lemma
7 to each entry of the ﬁrst coordinate to get
x →






q1(x1)
...
q1(xd)
x
qd(x)






∈ϒ20d,2(b−1)l(Rd,R2d+1).
We now compose with the afﬁne map
 x
y
r
!
→

bly−x
r

∈ϒ0(R2d+1,Rd+1),
where x,y ∈Rd and r ∈R, to get
x →





blx1 −q(x1)
...
blxd −q(xd)
qd(x)




∈ϒ20d,2(b−1)l(Rd,Rd+1).
(4.5)
On the set Ωl
i,ε from (4.3) this map becomes
x →





blx1 −i1
...
blxd −id
ind(i)




.
The next step in the construction will be to approximate the coefﬁcients ai. To do this we round the ai down to the
nearest multiple of δ (in absolute value) to get approximate coefﬁcients
˜ai := δ sgn(ai)
|ai|
δ

.
We estimate the ℓp-norm of the error this incurs as follows. Write
∥a −˜a∥ℓp =
 
∑
i∈Il
|ai −˜ai|p
!1/p
with the standard modiﬁcation when p = ∞. Note that
∥a −˜a∥ℓq ≤∥a∥ℓq ≤1
by (4.4). In addition, it is clear from the rounding procedure that ∥a −˜a∥ℓ∞≤δ. Hölder’s inequality thus implies that
(since p ≥q)
∥a −˜a∥ℓp ≤∥a −˜a∥q/p
ℓq ∥a −˜a∥1−q/p
ℓ∞
≤δ 1−q/p.
18

A PREPRINT - APRIL 9, 2024
On the other hand, using that |Il| = bdl, we can use the bound ∥a −˜a∥ℓ∞≤δ to get
∥a −˜a∥ℓp ≤bdl/pδ.
Putting these together, we get
∥a −˜a∥ℓp ≤δ min{bdl,δ −q}1/p.
(4.6)
Next we construct a ReLU neural network which maps the index ind(i) to the rounded coefﬁcients ˜ai. For this Theorem
4 will be key. We set N = bdl and write ˜ai = δxind(i) for a vector x ∈ZN deﬁned by
xind(i) = sgn(ai)
|ai|
δ

.
We proceed to estimate ∥x∥ℓ1. We observe that by (4.4)
N
∑
i=1
|xi|q ≤∑
i∈Il
|ai|
δ
q
≤δ −q.
(4.7)
Thus ∥x∥ℓq ≤δ −1. Moreover, since x ∈ZN, (4.7) implies that the number of non-zero entries in x satisﬁes
|{i : xi ̸= 0}| ≤min{δ −q,N}.
We can thus use Hölder’s inequality to get the bound
∥x∥ℓ1 ≤|{i : xi ̸= 0}|1−1/q∥x∥ℓq ≤δ −1 min{δ −q,N}1−1/q.
Using this we apply Theorem 4 with M = δ −1 min{δ −q,N}1−1/q to the vector x. We calculate that if δ −q ≤N, then
M = δ −1δ −q(1−1/q) = δ −q ≤N,
while if δ q > N, then
M = δ −1N(1−1/q) = N(δ −qN−1)1/q ≥N.
Thus, Theorem 4 (combined with a scaling by δ) gives a network g ∈ϒ17,L(R) such that g(ind(i)) = ˜ai, whose depth
is bounded by
L ≤C
δ −q/2p
1 + dl log(b)+ qlog(δ)
δ −q ≤bdl
bdl/2p
1 −logδ −(dl/q)log(b)
δ −q > bdl.
Using Lemma 3 to apply g to the last coordinate of the output in (4.5) gives a network ˜fδ ∈ϒ20d+17,L with depth
bounded by
L ≤2(b −1)l +C
δ −q/2p
1 + dl log(b)+ qlog(δ)
δ −q ≤bdl
bdl/2p
1 −logδ −(dl/q)log(b)
δ −q > bdl,
such that for x ∈Ωl
i,ε we have
˜fδ (x) =





blx1 −i1
...
blxd −id
˜ai




.
(4.8)
Finally, to obtain the network fδ,m we use Lemma 1 to compose ˜fδ with a network Pm which approximates the product




z1
...
zd
zd+1



→zd+1
d
∏
j=1
z
αj
j
on the set where |zj| ≤1 for all j = 1,...,d + 1. Note from the bound (4.4) we see that | ˜ai| ≤|ai| ≤∥a∥ℓq ≤1. In
addition, it is easy to see that for x ∈Ωl
i,ε we have |blxj −ij| ≤1 for j = 1,...,d. Thus the output of ˜fδ satisﬁes these
assumptions for any x ∈Ωl,ε.
19

A PREPRINT - APRIL 9, 2024
We construct the network Pm using Proposition 3 as follows. Choose a parameter m ≥1. We ﬁrst approximate a
function which multiplies the last entry zd+1 by the i-th entry zi. We do this by duplicating the i-th entry using an
afﬁne map and then applying Lemma 3 to apply the network fm from Proposition 3 to the i-th and last entries




z1
...
zd
zd+1



→






z1
...
zd
zd+1
zi






→



z1
...
fm(zd+1,zi)


∈ϒ2d+13,6m+3(Rd+1,Rd+1).
In order to ensure that the resulting approximate product is still bounded in magnitude by 1 (so that we can recursively
apply these products), we apply the map z →max(min(z,−1),1) ∈ϒ5,2(R) to the last component. This gives a network
Pm
i ∈ϒ2d+13,6m+5(Rd+1,Rd+1), which maps
Pm
i :




z1
...
zd
zd+1



→






z1
...
zd
zd+1
zi






→



z1
...
˜fm(zd+1,zi)


,
where ˜fm(zd+1,zi) = max(min(fm(zd+1,zi),−1),1). Observe that since the true product zd+1zi ∈[−1,1] the truncation
cannot increase the error, so that Proposition 3 implies
| ˜fm(zd+1,zi)−zizd+1| ≤|fm(zd+1,zi)−zizd+1| ≤6 ·4−m.
We construct Pm by composing (using Lemma 1) αj copies of Pm
j and then applying an afﬁne map which selects the
last coordinate. Thus Pm ∈ϒ2d+13,L(Rd+1) with L ≤k(6m + 5). Moreover, since all entries zi are bounded by 1, we
calculate that
Pm(z)−zd+1
d
∏
j=1
z
αj
j
 ≤
d
∑
j=1
αj| ˜fm(zd+1,zj)−zjzd+1| ≤6k ·4−m.
(4.9)
We obtain the network fδ,m ∈ϒ20d+17,L(Rd,R) by composing ˜fδ and Pm using Lemma 1. Its depth is bounded by
L ≤2(b −1)l + k(6m+ 5)+C
δ −q/2p
1 + dl log(b)+ qlog(δ)
δ −q ≤bdl
bdl/2p
1 −logδ −(dl/q)log(b)
δ −q > bdl,
and we note that k(6m+ 5) ≤Cm for integers m ≥1 and a constant C := C(k) which depends upon k.
We bound the error using equations (4.6), (4.8), (4.9), and the fact that the basis ρα
l,i is normalized in L∞and has
disjoint support for ﬁxed α to get
∥f −fδ,m∥p
Lp(Ωl,ε) ≤2−ld ∑
i∈Il
|ai −˜ai|p + (6k ·4−m)p,
so that
∥f −fδ,m∥Lp(Ωl,ε) ≤2−ld/p∥a −˜a∥ℓp + 6k ·4−m ≤C

δ min
n
1,2−dlδ −qo1/p
+ 4−m

,
which completes the proof.
Next, we use the construction in Proposition 8 to approximate a target function f ∈W s(Lq(Ω)) in Lp(Ω) using deep
ReLU neural networks, again removing an arbitrarily small triﬂing set in the spirit of [50].
Proposition 9. Let Ω= [0,1)d, 1 ≤q ≤p ≤∞and f ∈W s(Lq(Ω)) with ∥f∥Ws(Lq(Ω)) ≤1 for s > 0. Suppose that the
Sobolev embedding condition is strictly satisﬁed, i.e.
1
q −1
p −s
d < 0,
(4.10)
which guarantees the compact embedding W s(Lq(Ω)) ⊂⊂Lp(Ω) holds. Let ε > 0 and l0 ≥1 be an integer and set
l∗= ⌊κl0⌋with
κ :=
s
s+ d/p −d/q.
20

A PREPRINT - APRIL 9, 2024
Note that 1 ≤κ < ∞by the Sobolev embedding condition. Then there exists a network fl0,ε ∈ϒ24d+20,L(Rd) such that
∥f −fl0,ε∥Lp(Ωl∗,ε) ≤Cb−sl0
and whose depth is bounded by
L ≤Cbdl0/2.
Here the constants C := C(s, p,q,d,b) do not depend upon l0, f or ε.
Before giving the detailed proof, let us comment on the intuition and the meaning of κ and l∗. The idea is to decompose
the function f into different scales which consist of piecewise polynomial functions. We then appoximate these
piecewise polynomial functions using neural networks via Proposition 8 to varying degrees of accuracy dependent on
the parameter δ used at each level. The parameter l0 gives the ﬁnest level at which we approximate the coefﬁcients
in the dense regime δ −q > bdl, while the level l∗is the ﬁnest level which appears in the approximation. All levels
between l0 and l∗are approximated in the sparse regime δ −q ≤bdl. The parameter κ controls the gap between l0 and
l∗are essentially measures how adaptive the approximation must be. The proof is completed by choosing δ optimally
at each level, analogous to the proof of the Birman-Solomyak Theorem [8] which calculates the metric entropy of the
Sobolev unit ball.
Proof of Proposition 9. For a function f ∈Lq(Ω), we write
Πl
k(f) = arg min
p∈Pl
k
∥f −p∥Lq(Ω)
for the Lq-projection of f onto the space of piecewise polynomials of degree k. We will utilize the following well-
known multiscale dyadic decomposition of the function f, which is a common tool in harmonic analysis [8, 38, 41]
and the analysis of multigrid methods [10],
f =
∞
∑
l=0
fl,
where the components at level l are deﬁned by f0 = Π0
k(f) and fl = Πl
k(f) −Πl−1
k
(f) for l ≥1. Expanding the
components fl in the basis ρα
l,i, we write
fl(x) =
∑
|α|≤k, i∈Il
aα
l,iρα
l,i(x).
(4.11)
The key estimate in the proof is to establish the following coefﬁcient bound
|aα
l,i| ≤Cb(d/q−s)l|f|W s(Lq(Ωl−1
i−)),
(4.12)
where Ωl−1
i−⊃Ωl
i is the parent domain of Ωl
i when l ≥1. When l = 0, we have the simple modiﬁcation
|aα
0,0| ≤C∥f∥Ws(Lq(Ω)).
We prove (4.12) by utilizing the Bramble-Hilbert lemma [9] and a well-known scaling argument. For l ≥1 consider
the scaling map Sl,i which scales the small domain Ωl−1
i−
up to the large domain Ω, deﬁned by
Sl,i(f)(x) = f(bl−1x−i−) ∈Lq(Ω)
for f ∈Lq(Ωl−1
i−). We verify the following simple facts
|Sl,i(f)|W s(Lq(Ω)) = b(d/q−s)(l−1)|f|W s(Lq(Ωi−))
Sl,i(fl) = Sl,i(Πl
k(f)−Πl−1
k
(f)) = Π1
k(Sl,i(f))−Π0
k(Sl,i(f))
Sl,i(ρα
l,i) = ρα
1,j,
(4.13)
where j ∈{0,1,...,b}d is the index of Ωl
i in Ωl−1
i−, i.e. j ≡i (mod b). From the last two facts we deduce that
Π1
k(Sl,i(f))−Π0
k(Sl,i(f)) = ∑
j∈I1
aα
l,(bi−+j)ρα
1,j,
where the aα
l,(bi−+j) are the coefﬁcients from the expansion (4.11) of fl. Combining this with the ﬁrst fact from (4.13),
it sufﬁces to prove (4.12) when l = 1 and apply this to Sl,i(f).
21

A PREPRINT - APRIL 9, 2024
To prove (4.12) when l = 1, we use the Bramble-Hilbert lemma [9]. We calculate using the Bramble-Hilbert lemma
that
∥Π0
k(f)−f∥Lq(Ω1
i ) ≤∥Π0
k(f)−f∥Lq(Ω) ≤C|f|W s(Lq(Ω))
∥Π1
k(f)−f∥Lq(Ω1
i ) ≤C|f|W s(Lq(Ω1
i )) ≤C|f|W s(Lq(Ω)).
Combining these two estimates, we get
∥Π0
k(f)−Π1
k(f)∥Lq(Ω1
i ) ≤C|f|W s(Lq(Ω)).
When l = 0 we make the modiﬁcation
∥Π0
k(f)∥Lq(Ω) ≤∥f∥Lq(Ω) + ∥Π0
k(f)−f∥Lq(Ω) ≤C∥f∥W s(Lq(Ω)).
Now we use the fact that all norms on the ﬁnite dimensional space of polynomials of degree at most k are equivalent
to transfer the Lq bound to a bound on the coefﬁcients. This implies (4.12).
From (4.12), we deduce the following bound on the ℓq-norm of the coefﬁcients of fl:
 
∑
|α|≤k, i∈Il
|aα
l,i|q
!1/q
≤Cb(d/q−s)l
 
∑
|α|≤k, i∈Il
|f|q
W s(Lq(Ωl−1
i−))
!1/q
≤Cb(d/q−s)l,
(4.14)
since ∥f∥W s(Lq(Ω)) ≤1. This follows from the sub-additivity of the Sobolev norm,
∑
i∈Il−1
|f|q
W s(Lq(Ωl−1
i
)) ≤|f|q
W s(Lq(Ω)),
(4.15)
since each Ωl−1
i−
appears a ﬁnite number of times in the sum (4.14) (namely
 k+d
d

bd which is independent of l).
We remark that the Sobolev sub-additivity (4.15) immediately follows from the deﬁnitions (1.1) and (1.2). Note also
that the bound (4.14) also easily follows when the standard modiﬁcations are made for q = ∞.
Next, we derive the following bound, which follows from (4.14), the L∞-normalization of the basis functions ρα
l,i, the
fact that for ﬁxed α the functions ρα
l,i have disjoint support, and the assumption that p ≥q:
∥fl∥Lp(Ω) ≤∑
|α|≤k
b−dl/p
 
∑
i∈Il
|aα
l,i|p
!1/p
≤b−dl/p
k + d
d
1−1/p 
∑
|α|≤k, i∈Il
|aα
l,i|p
!1/p
≤Cb(d/q−d/p−s)l.
(4.16)
We now complete the proof by using Proposition 8 to approximate each fl for l = 1,...,l∗, for which we must choose
appropriate parameters. First, we choose τ > 0 such that
d
q −d
p −s+

1 −q
p

τ < 0.
Note that this condition can be satisﬁed since q ≤p and the Sobolev embedding condition (4.10) holds. For each level
l we choose parameters
δ = δ(l) =
b−dl0/q+τ(l−l0)
l ≥l0
b−dl/q+(s+1)(l−l0)
l < l0
(4.17)
and
m = m(l) = K1l0 + K2l
(4.18)
in Proposition 8, where K1,K2 > 0 are parameters to be chosen later. Note that δ(l)−q ≤bdl when l ≥l0 and δ(l)−q >
bdl when l < l0. This means that the coarser levels are discretized ﬁnely and the coefﬁcients are dense, while the ﬁner
levels are discretized coarsely so that the coefﬁcients are sparse.
So, we deﬁne the network fl0,ε using Proposition 1 to be
fl0,ε =
l∗
∑
l=0
fδ(l),m(l),
where fδ(l),m(l) is constructed using Proposition 8 applied to fl with parameters δ = δ(l) and m = m(l).
22

A PREPRINT - APRIL 9, 2024
Propositions 1 and 8 imply that fl0,ε ∈ϒ24d+20,L(Rd) with
L ≤
l∗
∑
l=0
Ll,
where the depths Ll for each level are bounded using Proposition 8 by
Ll ≤C
m(l)+ l + δ(l)−q/2p
1 + dl log(b)+ qlog(δ(l))
δ(l)−q ≤bdl
m(l)+ l + bdl/2p
1 −logδ(l)−(dl/q)log(b)
δ(l)−q > bdl.
Plugging in the expressions for δ(l) and m(l) given in (4.17) and (4.18), and using that δ(l)−q ≤bdl when l ≥l0 and
δ(l)−q > bdl when l < l0, we get the bound
L ≤C
 
l∗
∑
l=0
K1l0 + (K2 + 1)l + bdl0/2
l0−1
∑
l=0
bd(l−l0)/2p
1 + log(b)(s+ 1)(l −l0)
+ bdl0/2
l∗
∑
l=l0
b−τ(l−l0)/2p
1 + log(b)(d/q + τ)(l −l0)
!
.
Summing the series above (and noting that the latter two are bounded by convergent geometric series), we get
L ≤C((l∗)2 + bdl0/2).
Note that here the constant C depends upon the choice of parameters K1 and K2. Since l∗≤κl0 is a linear function
of l0, the quadratic term (l∗)2 ≤(κl0)2 is dominated by the exponential second term. Thus we get L ≤Cbdl0/2 (for a
potentially larger constant C).
Finally, we bound the error. For this we use Proposition 8 to bound
∥fδ(l),m(l) −fl∥Lp(Ωl,ε) ≤C

δ(l)min
n
1,b−dlδ(l)−qo1/p
+ 4−m(l)
 
∑
|α|≤k, i∈Il
|aα
l,i|q
!1/q
.
Combining this with the bound (4.14), plugging in the choices (4.17) and (4.18) (here again we have b−dlδ(l)−q ≤1
when l ≥l0 and b−dlδ(l)−q > 1 when l < l0), and noting that Ωl,ε ⊃Ωl∗,ε if l ≤l∗, we get
l∗
∑
l=0
∥fδ(l),m(l) −fl∥Lp(Ωl∗,ε) ≤C
 l0−1
∑
l=0
b(d/q−s)l h
δ(l)+ 4−m(l)i
+
l∗
∑
l=l0
b(d/q−s)l h
δ(l)1−q/pb−(d/p)l + 4−m(l)i!
.
Plugging in our choices for δ(l) and m(l), we calculate
l∗
∑
l=0
∥fδ(l),m(l) −fl∥Lp(Ωl∗,ε) ≤C
 
b−sl0
l0−1
∑
l=0
bl−l0
+ b−sl0
l∗
∑
l=l0
b(d/q−d/p−s+τ(1−q/p))(l−l0)
+
l∗
∑
l=0
b(d/q−s)l4−K1l0−K2l
!
.
(4.19)
The ﬁrst sum above is a convergent geometric series and is bounded by Cb−sl0. Due to the choice of τ, the second
sum is also a convergent gemoetric series, and is also bounded by Cb−sl0. Choosing K1 and K2 large enough so that
4−K1 ≤b−s and 4−K2 < b(s−d/q), the ﬁnal sum is also a convergent geometric series which is bounded by Cb−sl0. Thus,
we obtain
l∗
∑
l=0
∥fδ(l),m(l) −fl∥Lp(Ωl∗,ε) ≤Cb−sl0
23

A PREPRINT - APRIL 9, 2024
for an appropriate constant C. Finally, we estimate
∥f −fl0,ε∥Lp(Ωl∗,ε) ≤
l∗
∑
l=0
∥fδ(l),m(l) −fl∥Lp(Ωl∗,ε) +
∞
∑
l=l∗+1
∥fl∥Lp(Ωl∗,ε).
Utilizing (4.19) and (4.16) we get
∥f −fl0,ε∥Lp(Ωl∗,ε) ≤Cb−sl0 +C
∞
∑
l=l∗+1
b(d/q−d/p−s)l.
The compact Sobolev embedding condition implies that the second sum is a convergent geometric series, bounded by
a multiple of its ﬁrst term. This gives
∥f −fl0,ε∥Lp(Ωl∗,ε) ≤C(b−sl0 + b(d/q−d/p−s)l∗).
Finally, we use the deﬁnition of l∗and κ to see that
b(d/q−d/p−s)l∗≤Cb−sl0,
which completes the proof.
We note that a completely analogous Proposition holds for the Besov spaces Bs
r(Lq(Ω)), i.e. Proposition 9 holds with
the Sobolev space W s(Lq(Ω)) replaced by Bs
r(Lq(Ω)). The proof is exactly the same, utilizing a piecewise polynomial
approximation, with the main difference being that the Bramble-Hilbert lemma is replaced by the following bound
on piecewise polynomial approximation of Besov functions, known as Whitney’s theorem (see [20], Section 3 for
instance)
∥Π0
k−1(f)−f∥Lq(Ω) ≤Cωk(f,1)q,
where Ω= [0,1]d is the unit cube, ωk is the modulus of smoothness introduced in (1.3), and the constant depends
upon d,q and k. The proof proceeds via the same scaling argument, with the sub-additivity (4.15) replaced by the
corresponding result for the modulus of smoothness
∑
i∈Il−1
ωk(f,t,Ωl−1
i
)q
q ≤Cωk(f,t)q
q.
Note that here the modulus of smoothness on the left hand side is taken relative to subdomain Ωl−1
i
, while on the right
the modulus is taken relative to the whole domain Ω. This inequality holds for a constant C depending on d,q and k
(see [22]). Finally, we use the bound
ωk(f,t)q ≤Cts∥f∥Bsr(Lq(Ω)),
which holds as long as k > s, to obtain a bound on the error in terms of the Besov norm from a bound in terms of the
modulus of smoothness.
Finally, we show how to remove the triﬂing region to give a proof of Theorem 1. This is a technical construction similar
to the method in [40, 50, 51], but we signiﬁcantly reduce the size of the required network (in particular the width no
longer depends exponentially on the input dimension) by using the sorting network construction from Corollary 1.
In addition to using sorting network, in our approach we use different bases bi to create minimally overlapping triﬂing
regions. This is somewhat different than the aforementioned approaches [40, 50, 51], which shift the grid to achieve
the same effect. The reason we did this is to avoid the use of Sobolev and Besov extension theorems, as our method
allows everything to stay within the unit cube. Although such extension theorems could be used, they become quite
technical in full generality, and so we have found our approach simpler.
The proof of Theorem 2 is completely analogous using Proposition 9 with the Sobolev spaces replaced by Besov
spaces, and is omitted.
Proof of Theorem 1. We assume without loss of generality that f ∈W s(Lq(Ω)) has been normalized, i.e. so that
∥f∥Ws(Lq(Ω)) ≤1.
In order to remove the triﬂing region from the preceding construction we will make use of different bases b. Let r be
the smallest integer such that 2r ≥2d +2 (so that 2r ≤4d +4), set m = 2r, and set bi = πi (the i-th prime number) for
i = 1,...,m.
Let n ≥bm be an integer. We will construct a network fL ∈ϒ30d+24,L(Rd) such that
∥f −fL∥Lp(Ω) ≤Cn−s
24

A PREPRINT - APRIL 9, 2024
with depth L ≤Cnd/2, which will complete the proof.
For i = 1,...,m, set li = ⌊log(n)/log(bi)⌋to be the largest power of bi which is at most n, and write l∗
i = ⌊κli⌋where
κ is deﬁned as in Proposition 9. Note that since the πi are all pairwise relatively prime, the numbers
S :=
(
1
π
l∗
1
1
,..., π
l∗
1
1 −1
π
l∗
1
1
, 1
π
l∗
2
2
,..., π
l∗
2
2 −1
π
l∗
2
2
,..., 1
πl∗m
m
,..., πl∗m
m −1
πl∗m
m
)
are all distinct. Choose an ε > 0 which satisﬁes
ε < min
x̸=y∈S|x−y|,
(4.20)
i.e. which is smaller than the distance between the two closest elements of S. This ε has the property that any x ∈[0,1]
is contained in at most one of the sets
[jπ−l∗
i
i
−ε, jπ−l∗
i
i
) for i = 1,...,m and j = 1,...,πl∗
i
i −1.
(4.21)
This means that for any x ∈Ω, we have x /∈Ωl∗
i ,ε for at most d different values i. Here Ωl∗
i ,ε is the good region at level
l∗
i with base bi. This holds since x has d coordinates and each coordinate can be contained in at most one bad set from
(4.21).
We now use Proposition 9, setting l0 = li and using an ε satisfying (4.20), to construct fi ∈ϒ24d+20,L(Rd) which
satisﬁes
∥f −fi∥Lp(Ωl∗
i ,ε) ≤Cπ−sli
i
≤Cn−s
and has depth bounded by
L ≤Cπdli/2
i
≤Cnd/2.
Finally, we construct the following network. We sequentially duplicate the input and apply the network fi to the new
copy using Lemma 3 to get
x →

x
x

→

x
f1(x)

→
 
x
x
f1(x)
!
→
 
x
f2(x)
f1(x)
!
→··· →




fm(x)
...
f2(x)
f1(x)



∈ϒ30d+24,L(Rd,Rm)
(4.22)
with L ≤C∑m
i=1 πdli/2
i
≤Cnd/2.
We construct the network fL ∈ϒ30d+24,L(Rd) by composing the network from (4.22) with the order statistic network
which selects the median, i.e. the m/2-largest value. By construction the network depth of fL satisﬁes
L ≤Cnd/2 +
m+ 1
2

≤Cnd/2,
since
 m+1
2

is a constant independent of n.
To bound the approximation error of fL we introduce the following notation. Given x ∈[0,1)d, we write
K (x) = {i : x ∈Ωl∗
i ,ε}
for the set of indices such that x is contained in the good region for the base bi decomposition. Since x fails to be in
Ωl∗
i ,ε for at most d values of i, we get
|K (x)| ≥m−d ≥m/2 + 1
since m ≥2d + 2. Thus the m/2-largest element among the f1(x),..., fm(x) is both smaller and larger than some
element of { fi(x), i ∈K (x)}, which implies
min
i∈K (x) fi(x) ≤fL(x) ≤max
i∈K (x) fi(x),
so that
|fL(x)−f(x)| ≤max
i∈K (x)|fi(x)−f(x)|.
This completes the proof when p = ∞, since if i ∈K (x) then |fi(x)−f(x)| ≤Cn−s by Proposition 9 and the deﬁnition
of K (x).
25

A PREPRINT - APRIL 9, 2024
For p < ∞, we note that
Z
Ω|fL(x)−f(x)|pdx ≤
Z
Ωmax
i∈K (x)|fi(x)−f(x)|pdx ≤
Z
Ω∑
i∈K (x)
|fi(x)−f(x)|pdx
≤
m
∑
i=1
∥fi −f∥p
Lp(Ωl∗
i ,ε)
≤Cn−sp.
Taking p-th roots completes the proof.
5
Lower Bounds
In this section, we study lower bounds on the approximation rates that deep ReLU neural networks can achieve on
Sobolev spaces. Our main result is to prove Theorem 3, which shows that the construction of Theorem 1 is optimal in
terms of the number of parameters. In addition, we show that the representation of sparse vectors proved in Theorem
4 is optimal.
The key concept is the notion of VC dimension, which was used in [50,61] to prove lower bounds for approximation
in the L∞-norm. We generalize these results to obtain sharp lower bounds on the approximation in Lp as well. Let K
be a class of functions deﬁned on Rd. The VC-dimension [56] of K is deﬁned to be the largest number n such that
there exists a set of points x1,...,xn ∈Ωsuch that
|{(sgn(g(x1)),...,sgn(g(xn))), g ∈K}| = 2n,
i.e. such that every sign pattern at the points x1,...,xn can be matched by a function from K. Such a set of points is
said to be shattered by K.
The VC dimension of classes of functions deﬁned by neural networks has been extensively studied and the most precise
results are available for piecewise polynomial activation functions. We will discuss two main results concerning the
VC dimension of ϒW,L(Rd). The ﬁrst bound is most useful when the depth L is ﬁxed and the width W is large and is
given by
VC-dim(ϒW,L(Rd)) ≤C(W 2L2 log(WL)).
(5.1)
This was proved in Theorem 6 of [6]. The second bound, which is most informative when the width W is ﬁxed and the
depth L is large is
VC-dim(ϒW,L(Rd)) ≤C(W 3L2).
(5.2)
This was proved in Theorem 8 of [6] using a technique developed in [28]. In either case, the VC-dimension of a deep
ReLU neural network with P = O(W 2L) parameters is bounded by CP2, with this bound achieved up to a constant only
in the case where the width W is ﬁxed and the depth L grows. This bound on the VC-dimension was used in [50,61]
to prove Theorem 3 in the case p = ∞. However, in order to extend the lower bound to p < ∞a more sophisticated
analysis is required. The key argument is captured in the following Proposition.
Theorem 5. Let p > 0, Ω= [0,1]d and suppose that K is a translation invariant class of functions whose VC-dimension
is at most n. By translation invariant we mean that f ∈K implies that f(·−v) ∈K for any ﬁxed vector v ∈Rd. Then
there exists an f ∈W s(L∞(Ω))∩Bs
1(L∞(Ω)) such that
inf
g∈K∥f −g∥Lp(Ω) ≥C(p,d,s)n−s
d max
n
∥f∥Ws(L∞(Ω)),∥f∥Bs
1(L∞(Ω))
o
.
Although the translation invariance holds for many function classes of interest, it is an interesting problem whether it
can be removed. Before proving this result, we ﬁrst show how Theorem 3 follows from this.
Proof of Theorem 3. Note that the class of deep ReLU networks ϒW,L(Rd) is translation invariant. Combining this
with the VC-dimension bounds (5.1) and (5.2), Theorem 5 implies Theorem 3 in the case q = ∞and r = 1. The
general case follows trivially since W s(L∞(Ω)) ⊂W s(Lq(Ω)) for any q ≤∞, and Bs
1(L∞(Ω) ⊂Bs
r(Lq(Ω)) for r ≥1
and q ≤∞.
Let us turn to the proof of Theorem 5. A key ingredient is the well-known Sauer-Shelah lemma [47,49].
26

A PREPRINT - APRIL 9, 2024
Lemma 8 (Sauer-Shelah Lemma). Suppose that K has VC-dimension at most n. Given any collection of N points
x1,...,xN ∈Ω, we have
|{(sgn(g(x1)),...,sgn(g(xN))), g ∈K}| ≤
n
∑
i=0
N
i

.
We will also utilize the following elementary bound on the size of a Hamming ball.
Lemma 9. Suppose that N ≥2n, then
n
∑
i=0
N
i

≤2NH(n/N),
where H(p) is the entropy function
H(p) = −plog(p)−(1 −p)log(1 −p).
(Note that all logarithms here are taken base 2.)
Proof. Observe that since N −n ≥n, we have
N −n
N
N−n  n
N
n n
∑
i=0
N
i

≤
n
∑
i=0
N
i
N −n
N
N−i  n
N
i
<
N
∑
i=0
N
i
N −n
N
N−i  n
N
i
= 1.
This means that
n
∑
i=0
N
i

≤
"N −n
N
N−n  n
N
n
#−1
.
Taking logarithms, we obtain
log
 
n
∑
i=0
N
i
!
≤−N

(N −n)log
N −n
N

+ nlog
 n
N

= NH(n/N)
as desired.
Utilizing these lemmas, we give the proof of Theorem 5.
Proof of Theorem 5. Let c < 1/2 be chosen so that H(c) < 1/2 (for instance c = 0.1 will work) and ﬁx
k := ⌈dp
n/c⌉≤C(d)n1/d,
and ε := k−1. Next, we consider shifts of an equally spaced grid with side length ε. Speciﬁcally, for each λ ∈[0,ε)d,
deﬁne the point set
Xλ =
n
λ + εz, z ∈[k]do
,
where we have written [k] := {0,...,k −1} for the set of integers from 0 to k −1.
Let us now investigate the set of sign patterns which the class K can match on Xλ. To do this, we will introduce some
notation. For a function g ∈K, we write
sgn(g|Xλ ) ∈{±1}[k]d, sgn(g|Xλ )(z) = sgn(g(λ + εz))
for the set of signs which g takes at the (shifted) grid points Xλ. Here the vector sgn(g|Xλ ) is indexed by the coordinate
z ∈[k]d which speciﬁes the location of a point in the shifted grid Xλ.
We write
sgn(K|Xλ ) :=

sgn(g|Xλ ), g ∈K
	
⊂{±1}[k]d
for the set of sign patterns attained by the class K on Xλ. Observe that since K is assumed to be translation invariant,
the set sgn(K|Xλ ) is independent of the shift λ. To see this, let λ,µ ∈[0,ε)d be two different shifts and let g ∈K. By
the translation invariance, we ﬁnd that the function g′ deﬁned by
g′(x) = g(x+ λ −µ)
27

A PREPRINT - APRIL 9, 2024
is also in K. We easily calculate that
sgn(g|Xλ ) = sgn(g′|Xµ ),
which implies that sgn(K|Xλ ) = sgn(K|Xµ). In the following we simplify notation and write sgn(K) ⊂{±1}[k]d for
this set.
Next, we will show that there exists a choice of signs α ∈{±1}[k]d which differs from every element of sgn(K) in a
constant fraction of its entries. To do this, it is convenient to use the notion of the Hamming distance between two sign
patterns, which is deﬁned as the number of indices in which they differ, i.e.
dH(α,β) := |{z ∈[k]d : α(z) ̸= β(z)}|.
We also use the notion of the Hamming ball of radius m around a sign pattern α ∈{±1}[k]d, which is deﬁned to be the
set of sign patterns which differ from α by at most m entries, i.e.
BH(α,m) = {β ∈{±1}[k]d, dH(α,β) ≤m}.
We note that Lemma 9 implies the following estimate on the size of BH(α,m) when 2m < kd:
|BH(α,m)| =
m
∑
i=0
kd
i

≤2kdH(m/kd).
Further, our assumption on the VC-dimension of K combined with Lemmas 8 and 9 implies that
|sgn(K)| ≤2kdH(n/kd) ≤2kdH(c) < 2kd/2
from our choice of c. If we choose m := ⌊ckd⌋≤ckd, it follows that

[
β∈sgn(K)
BH(β,m)

< 2kd/22kdH(m/kd) < 2kd/22kdH(c) < 2kd,
so that there must exist an α ∈{±1}[k]d such that
α /∈
[
β∈sgn(K)
BH(β,m),
and hence
inf
β∈sgn(K)dH(α,β) ≥m+ 1 ≥ckd.
(5.3)
Finally, we choose a compactly supported smooth positive bump function φ whose support is strictly contained in the
unit cube Ωand consider the function
f(x) = ∑
z∈[k]d
α(z)φ(kx−z).
Since the supports of the functions φ(kx−z) are all disjoint, we calculate
∥f∥Ws(L∞(Ω)) = ∥φ(kx−z)∥Ws(L∞(Ω)) ≤ks∥φ∥Ws(L∞(Ω)),
(5.4)
and also
∥f∥Bs
1(L∞(Ω)) ≤Cks∥φ∥Bs
1(L∞(Ω)),
(5.5)
for an appropriate constant C = C(d,s). Next, let g ∈K be arbitrary. We calculate
Z
Ω|f(x)−g(x)|pdx =
Z
[0,ε)d ∑
z∈[k]d
|f(λ + εz)−g(λ + εz)|pdλ
=
Z
[0,ε)d ∑
z∈[k]d
|α(z)φ(kλ)−g(λ + εz)|pdλ.
From equation (5.3) and the fact that sgn(g|Xλ ) ∈sgn(K), we see that
|{z ∈[k]d, α(z) ̸= sgn(g(λ + εz))}| ≥ckd.
28

A PREPRINT - APRIL 9, 2024
Further, if α(z) ̸= sgn(g(λ + εz)), then we have the lower bound
|α(z)φ(kλ)−g(λ + εz)| ≥φ(kλ)
since φ ≥0. This implies that for every λ ∈[0,ε)d we have the lower bound
∑
z∈[k]d
|α(z)φ(kλ)−g(λ + εz)|p ≥ckdφ(kλ)p.
We thus obtain
Z
Ω|f(x)−g(x)|pdx ≥ckd
Z
[0,ε)d φ(kλ)pdλ = c
Z
Ωφ(x)pdx,
from which we deduce
∥f −g∥Lp(Ω) ≥c
1
p ∥φ∥Lp(Ω).
Combining this with the bounds (5.4) and (5.5), using that k ≤C(d)n1/d, that φ is a ﬁxed function, and that g ∈K was
arbitrary, we get
inf
g∈K∥f −g∥Lp(Ω) ≥C(d, p)k−s∥f∥Ws(L∞(Ω)) ≥C(d,k)n−s/d max
n
∥f∥Ws(L∞(Ω)),∥f∥Bs
1(L∞(Ω))
o
,
as desired.
We conclude this section by proving that Theorem 4 is optimal up to a constant as long as the ℓ1-norm M is not too
large and not too small. Speciﬁcally, we have the following.
Theorem 6. Let M,N ≥1 be integers and deﬁne
SN,M = {x ∈ZN, ∥x∥ℓ1 ≤M}
as in the proof of Theorem 4. Suppose that W,L ≥1 are integers and that for any x ∈SN,M there exists an f ∈ϒW,L(R)
such that f(i) = xi for i = 1,..,N. Then there exists a constant C < ∞such that if Clog(N) < M ≤N, then
W 4L2 ≥C−1M(1 + log(N/M)),
and if N ≤M < exp(N/C), then
W 4L2 ≥C−1N(1 + log(M/N)).
This result implies that if ϒW,L(R) can match the values of any vector in SN,M for M in the range (Clog(N),exp(N/C)),
then the number of parameters must be larger than a constant multiple of the upper bound proved in Theorem 4. Thus
Theorem 4 is sharp in this range. If M < Clog(N) then piecewise linear functions with O(M) pieces can ﬁt SN,M, and
if M > exp(N/C) then piecewise linear functions with O(N) pieces can ﬁt SN,M. This implies that Theorem 4 is no
longer sharp outside this range.
Proof. Suppose ﬁrst that N/2 < M ≤2N, i.e. that M is of the same order as N. For any subset S ⊂{1,...,N/2} it is
easy to construct an x ∈SN,M such that xi > 0 iff i ∈S. Thus the class ϒW,L(R) must shatter a set of size at least N/2
and the VC-dimension bound (5.2) implies the result.
In the case where M << N or M >> N, the proof proceeds in a similar manner as the VC-dimension bounds from
[6,28] although the VC-dimension cannot directly be used.
We begin with the case where M ≤N/2. We will bound the total number of sign patterns that ϒW,L(R) can match
on the input set X = {1,...,N}. For i = 1,...,L, let εi ∈{0,1}W be a sign pattern. Given an input x ∈X and a neural
network with parameters Wi and bi, consider the signs of the following quantities
(AW0,b0(x))j, j = 1,...,W
(AW1,b1 ◦ε1 ◦AW0,b0(x))j, j = 1,...,W
(AW2,b2 ◦ε2 ◦AW1,b1 ◦ε1 ◦AW0,b0(x))j, j = 1,...,W
...
(AWL−1,bL−1 ◦εL−1 ◦···◦ε2 ◦AW1,b1 ◦ε1 ◦AW0,b0(x))j, j = 1,..,W
AWL,bL ◦εL ◦AWL−1,bL−1 ◦εL−1 ◦···◦ε2 ◦AW1,b1 ◦ε1 ◦AW0,b0(x).
(5.6)
29

A PREPRINT - APRIL 9, 2024
Here εi represents pointwise multiplication by the sign vector εi. For any input x ∈R the deﬁnition of the ReLU
activation function implies that if we recursively set
εi = sgn(AWi−1,bi−1 ◦εi−1 ◦···◦ε2 ◦AW1,b1 ◦ε1 ◦AW0,b0(x)),
(5.7)
then we will have
AWL,bL ◦εL ◦···◦ε2 ◦AW1,b1 ◦ε1 ◦AW0,b0(x) = AWL,bL ◦σ ◦···◦σ ◦AW1,b1 ◦σ ◦AW0,b0(x).
This implies that the signs of the quantities in (5.6) ranging over all sign vectors ε1,...,εL ∈{0,1}W uniquely determine
the value of the network at x. Thus the number of sign patterns achieved on the set X is bounded by the number of
sign patterns achieved in (5.6) as x ranges over the input set X, the εi range over the sign vectors {0,1}W, and the
parameters Wi, bi range of the set of all real numbers. As the εi range over the sign vectors {0,1}W and x ranges
over X, the quantities in (5.6) range over N(WL+ 1)2WL polynomials in the P ≤CW 2L parameter variables Wi, bi of
degree at most L. We can thus use Warren’s Theorem ( [58], Theorem 3) to bound the total number of sign patterns by
4eLN(WL+ 1)2WL
P
P
≤(4eLN(WL+ 1)2WL)CW 2L.
Suppose that ϒW,L(R) can match the values of any element in SN,M. Since the set SN,M contains the indicator function
of every subset of {1,...,N} of size M, we get that
N
M

≤(4eLN(WL+ 1)2WL)CW 2L.
Taking logarithms, we get
M log(N/M) ≤CW 3L2 +CW2Llog(N)+CW2Llog(4eL(WL+ 1))
≤CW 3L2 +CW2Llog(N) ≤CW 4L2 +CW2Llog(N).
Since M ≤N/2, we conclude that
M(1 + log(N/M)) ≤CM log(N/M) ≤Cmax{W 4L2,W 2Llog(N)}.
(5.8)
In the next few equations, let C denote the constant in (5.8). Suppose that W 4L2 < C−1M(1 + log(N/M)). Then
equation (5.8) implies that
W 2L ≥M 1 + log(N/M)
Clog(N)
.
But this would mean that
M 1 + log(N/M)
Clog(N)
≤W 2L =
√
W 4L2 <
q
C−1M(1 + log(N/M)).
Rearranging this, we get the inequality
√
M <
√
Clog(N)
p
1 + log(N/M)
,
from which we deduce that M ≤Clog(N) for a (potentially larger) new constant C.
Next, we consider the case where M > 2N. In this case we consider the following modiﬁcation of (5.6)
(AW0,b0(x))j, j = 1,...,W
(AW1,b1 ◦ε1 ◦AW0,b0(x))j, j = 1,...,W
(AW2,b2 ◦ε2 ◦AW1,b1 ◦ε1 ◦AW0,b0(x))j, j = 1,...,W
...
(AWL−1,bL−1 ◦εL−1 ◦···◦ε2 ◦AW1,b1 ◦ε1 ◦AW0,b0(x))j, j = 1,..,W
AWL,bL ◦εL ◦AWL−1,bL−1 ◦εL−1 ◦···◦ε2 ◦AW1,b1 ◦ε1 ◦AW0,b0(x)−k, k = 0,...⌊M/N⌋−1.
(5.9)
The number of sign patterns that can be obtained as x ranges over X, the εi range over {0,1}W, and the parameters
range over the set of real numbers is bounded (using Warren’s Theorem [58]) by
4eLN(WL+ M)2WL
P
P
≤(4eLN(WL+ M)2WL)CW 2L.
30

A PREPRINT - APRIL 9, 2024
The only difference to the previous bound is that for each choice of x ∈X and each choice of signs εi ∈{0,1}W, the
number of equations in (5.9) is WL+ ⌊M/N⌋≤WL+ M.
However, the set SN,M contains all (⌊M/N⌋+ 1)N−1 vectors whose ﬁrst N −1 coordinates are arbitrary integers in
{0,1,...,⌊M/N⌋} and whose last coordinate is chosen to make the ℓ1-norm equal to M. Thus, setting εi recursively
according to (5.7), we see that if every vector in SN,M can be represented by an element of ϒW,L(R), then
(⌊M/N⌋+ 1)N−1 ≤(4eLN(WL+ M)2WL)CW 2L.
Taking logarithms and calculating in a similar manner as before, we get
N(1 + log(M/N)) ≤CW 4L2 +CW2Llog(M).
As before we now deduce that if W 4L2 < C−1N(1 + log(M/N)), then N ≤Clog(M).
6
Acknowledgements
We would like to thank Ron DeVore for suggesting this problem, and Andrea Bonito, Geurgana Petrova, Zuowei
Shen, George Karniadakis, Jinchao Xu, and Qihang Zhou for helpful comments while preparing this manuscript. We
would also like to thank the anonymous reviewers for their careful reading and helpful comments. This work was
supported by the National Science Foundation (DMS-2111387 and CCF-2205004) as well as a MURI ONR grant
N00014-20-1-2787.
References
[1] El Mehdi Achour, Armand Foucault, Sébastien Gerchinovitz, and François Malgouyres, A general approximation
lower bound in Lp norm, with applications to feed-forward neural networks, arXiv preprint arXiv:2206.04360
(2022).
[2] Miklós Ajtai, János Komlós, and Endre Szemerédi, An O(nlogn) sorting network, Proceedings of the Fifteenth
Annual ACM Symposium on Theory of computing, 1983, pp. 1–9.
[3] Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee, Understanding deep neural networks with
rectiﬁed linear units, International Conference on Learning Representations, 2018.
[4] Francis Bach, Breaking the curse of dimensionality with convex neural networks, The Journal of Machine Learn-
ing Research 18 (2017), no. 1, 629–681.
[5] Peter Bartlett, Vitaly Maiorov, and Ron Meir, Almost linear VC dimension bounds for piecewise polynomial
networks, Advances in Neural Information Processing Systems 11 (1998).
[6] Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian, Nearly-tight VC-dimension and pseu-
dodimension bounds for piecewise linear neural networks, The Journal of Machine Learning Research 20 (2019),
no. 1, 2285–2301.
[7] Kenneth E Batcher, Sorting networks and their applications, Proceedings of the April 30–May 2, 1968, Spring
Joint Computer Conference, 1968, pp. 307–314.
[8] Mikhail Shlemovich Birman and Mikhail Zakharovich Solomyak, Piecewise-polynomial approximations of func-
tions of the classes W α
p , Matematicheskii Sbornik 115 (1967), no. 3, 331–355.
[9] James H Bramble and SR Hilbert, Estimation of linear functionals on Sobolev spaces with application to Fourier
transforms and spline interpolation, SIAM Journal on Numerical Analysis 7 (1970), no. 1, 112–124.
[10] James H Bramble, Joseph E Pasciak, and Jinchao Xu, Parallel multilevel preconditioners, Mathematics of Com-
putation 55 (1990), no. 191, 1–22.
[11] Antonin Chambolle, Ronald A DeVore, Nam-Yong Lee, and Bradley J Lucier, Nonlinear wavelet image process-
ing: variational problems, compression, and noise removal through wavelet shrinkage, IEEE Transactions on
Image Processing 7 (1998), no. 3, 319–335.
[12] Ingrid Daubechies, Ten Lectures on Wavelets, SIAM, 1992.
[13] Ingrid Daubechies, Ronald DeVore, Nadav Dym, Shira Faigenbaum-Golovin, Shahar Z Kovalsky, Kung-Chin
Lin, Josiah Park, Guergana Petrova, and Barak Sober, Neural network approximation of reﬁnable functions,
IEEE Transactions on Information Theory (2022).
31

A PREPRINT - APRIL 9, 2024
[14] Ingrid Daubechies, Ronald DeVore, Simon Foucart, Boris Hanin, and Guergana Petrova, Nonlinear approxima-
tion and (deep) ReLU networks, Constructive Approximation 55 (2022), no. 1, 127–172.
[15] Françoise Demengel, Gilbert Demengel, and Reinie Erné, Functional Spaces for the Theory of Elliptic Partial
Differential Equations, Springer, 2012.
[16] Ronald DeVore, Boris Hanin, and Guergana Petrova, Neural network approximation, Acta Numerica 30 (2021),
327–444.
[17] Ronald A DeVore, Nonlinear approximation, Acta Numerica 7 (1998), 51–150.
[18] Ronald A DeVore, Björn Jawerth, and Bradley J Lucier, Image compression through wavelet transform coding,
IEEE Transactions on Information Theory 38 (1992), no. 2, 719–746.
[19] Ronald A DeVore and George G Lorentz, Constructive Approximation, vol. 303, Springer Science & Business
Media, 1993.
[20] Ronald A DeVore and Vasil A Popov, Interpolation of besov spaces, Transactions of the American Mathematical
Society 305 (1988), no. 1, 397–414.
[21] Ronald A DeVore and Robert C Sharpley, Maximal functions measuring smoothness, vol. 293, American Mathe-
matical Soc., 1984.
[22]
, Besov spaces on domains in Rd, Transactions of the American Mathematical Society 335 (1993), no. 2,
843–864.
[23] Eleonora Di Nezza, Giampiero Palatucci, and Enrico Valdinoci, Hitchhikers guide to the fractional Sobolev
spaces, Bulletin des Sciences Mathématiques 136 (2012), no. 5, 521–573.
[24] David L Donoho and Iain M Johnstone, Adapting to unknown smoothness via wavelet shrinkage, Journal of the
American Statistical Association 90 (1995), no. 432, 1200–1224.
[25]
, Minimax estimation via wavelet shrinkage, The Annals of Statistics 26 (1998), no. 3, 879–921.
[26] David L. Donoho, Martin Vetterli, Ronald A. DeVore, and Ingrid Daubechies, Data compression and harmonic
analysis, IEEE Transactions on Information Theory 44 (1998), no. 6, 2435–2476.
[27] Lawrence C Evans, Partial Differential Equations, vol. 19, American Mathematical Soc., 2010.
[28] Paul Goldberg and Mark Jerrum, Bounding the Vapnik-Chervonenkis dimension of concept classes parameter-
ized by real numbers, Proceedings of the Sixth Annual Conference on Computational Learning Theory, 1993,
pp. 361–369.
[29] Ian Goodfellow, Yoshua Bengio, and Aaron Courville, Deep Learning, MIT press, 2016.
[30] Ingo Gühring, Gitta Kutyniok, and Philipp Petersen, Error bounds for approximations with deep ReLU neural
networks in W s,p norms, Analysis and Applications 18 (2020), no. 05, 803–859.
[31] Jiequn Han, Arnulf Jentzen, and Weinan E, Solving high-dimensional partial differential equations using deep
learning, Proceedings of the National Academy of Sciences 115 (2018), no. 34, 8505–8510.
[32] Boris Hanin, Universal function approximation by deep neural nets with bounded width and ReLU activations,
Mathematics 7 (2019), no. 10, 992.
[33] Boris Hanin and David Rolnick, Complexity of linear regions in deep networks, International Conference on
Machine Learning, PMLR, 2019, pp. 2596–2604.
[34] Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng, ReLU deep neural networks and linear ﬁnite elements,
Journal of Computational Mathematics 38 (2020), no. 3, 502–527.
[35] Jason M Klusowski and Andrew R Barron, Approximation by combinations of ReLU and squared ReLU ridge
functions with ℓ1 and ℓ0 controls, IEEE Transactions on Information Theory 64 (2018), no. 12, 7649–7656.
[36] Alois Kufner, Oldrich John, and Svatopluk Fucik, Function Spaces, vol. 3, Springer Science & Business Media,
1977.
[37] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, Deep learning, Nature 521 (2015), no. 7553, 436–444.
[38] John E Littlewood and Raymond EAC Paley, Theorems on Fourier series and power series, Journal of the
London Mathematical Society 1 (1931), no. 3, 230–233.
[39] George G Lorentz, Manfred v Golitschek, and Yuly Makovoz, Constructive Approximation: Advanced Problems,
vol. 304, Springer, 1996.
[40] Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang, Deep network approximation for smooth functions,
SIAM Journal on Mathematical Analysis 53 (2021), no. 5, 5465–5506.
32

A PREPRINT - APRIL 9, 2024
[41] Stéphane Mallat, A Wavelet Tour of Signal Processing, Elsevier, 1999.
[42] Vinod Nair and Geoffrey E Hinton, Rectiﬁed linear units improve restricted boltzmann machines, International
Conference on Machine Learning, 2010.
[43] Michael S Paterson, Improved sorting networks with O(logN) depth, Algorithmica 5 (1990), no. 1, 75–92.
[44] Philipp Petersen and Felix Voigtlaender, Optimal approximation of piecewise smooth functions using deep ReLU
neural networks, Neural Networks 108 (2018), 296–330.
[45] Pencho P Petrushev, Direct and converse theorems for spline and rational approximation and Besov spaces,
Function Spaces and Applications: Proceedings of the US-Swedish Seminar held in Lund, Sweden, June 15–21,
1986, Springer, 1988, pp. 363–377.
[46] Maziar Raissi, Paris Perdikaris, and George E Karniadakis, Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equations, Journal
of Computational physics 378 (2019), 686–707.
[47] Norbert Sauer, On the density of families of sets, Journal of Combinatorial Theory, Series A 13 (1972), no. 1,
145–147.
[48] Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam, Bounding and counting linear regions of
deep neural networks, International Conference on Machine Learning, PMLR, 2018, pp. 4558–4566.
[49] Saharon Shelah, A combinatorial problem; stability and order for models and theories in inﬁnitary languages,
Paciﬁc Journal of Mathematics 41 (1972), no. 1, 247–261.
[50] Zuowei Shen, Haizhao Yang, and Shijun Zhang, Optimal approximation rate of ReLU networks in terms of width
and depth, Journal de Mathématiques Pures et Appliquées 157 (2022), 101–135.
[51] Zhang Shijun, Deep neural network approximation via function compositions, Ph.D. thesis, National University
of Singapore (Singapore), 2021.
[52] Jonathan W Siegel and Jinchao Xu, Approximation rates for neural networks with general activation functions,
Neural Networks 128 (2020), 313–321.
[53]
, High-order approximation rates for shallow neural networks with cosine and ReLUk activation func-
tions, Applied and Computational Harmonic Analysis 58 (2022), 1–26.
[54]
, Sharp bounds on the approximation rates, metric entropy, and n-widths of shallow neural networks,
Foundations of Computational Mathematics (2022), 1–57.
[55] Matus Telgarsky, Beneﬁts of depth in neural networks, Conference on Learning Theory, PMLR, 2016, pp. 1517–
1539.
[56] Vladimir N Vapnik and A Ya Chervonenkis, On the uniform convergence of relative frequencies of events to their
probabilities, Measures of Complexity, Springer, 2015, pp. 11–30.
[57] Shuning Wang and Xusheng Sun, Generalization of hinging hyperplanes, IEEE Transactions on Information
Theory 51 (2005), no. 12, 4425–4431.
[58] Hugh E Warren, Lower bounds for approximation by nonlinear manifolds, Transactions of the American Mathe-
matical Society 133 (1968), no. 1, 167–178.
[59] Hassler Whitney, Analytic extensions of differentiable functions deﬁned in closed sets, Transactions of the Amer-
ican Mathematical Society 36 (1934), no. 1, 63–89.
[60] Dmitry Yarotsky, Error bounds for approximations with deep ReLU networks, Neural Networks 94 (2017), 103–
114.
[61]
, Optimal approximation of continuous functions by very deep ReLU networks, Conference on Learning
Theory, PMLR, 2018, pp. 639–649.
[62] Dmitry Yarotsky and Anton Zhevnerchuk, The phase diagram of approximation rates for deep neural networks,
Advances in Neural Information Processing Pystems 33 (2020), 13005–13015.
[63] Wen Yuan, Winfried Sickel, and Dachun Yang, Morrey and Campanato Meet Besov, Lizorkin and Triebel,
Springer, 2010.
33

A PREPRINT - APRIL 9, 2024
A
Elementary Constructions
Here we give the constructions of sums and of piecewise linear continuous functions using deep ReLU networks
references in Section 2.
Proof of Proposition 1. We will show by induction on j that

x
0

→

x
∑j
i=1 fi(x)

∈ϒW+2d+2k,L(Rd+k,Rd+k)
for L = ∑j
i=1 Li. The base case j = 0 is trivial since the identity map is afﬁne. Suppose we have shown this for j −1,
i.e.

x
0

→

x
∑j−1
i=1 fi(x)

∈ϒW+2d+2k,L(Rd+k,Rd+k),
where L = ∑j−1
i=1 Li. Compose this map with an afﬁne map which duplicates the ﬁrst entry to get

x
0

→


x
x
∑j−1
i=1 fi(x)

∈ϒW+2d+2k,L(Rd+k,R2d+k).
Now, we use Lemma 3 to apply f j to the middle entry. This gives

x
0

→


x
f j(x)
∑j−1
i=1 fi(x)

∈ϒW+2d+2k,L+Lj(Rd+k,Rd+2k).
We ﬁnally compose with the afﬁne map
 x
y
z
!
→

x
y+ z

∈ϒ0(Rd+2k,Rd+k),
and apply Lemma 1 to obtain

x
0

→

x
∑j
i=1 fi(x)

∈ϒW+2d+2k,L+Lj(Rd+k,Rd+k),
which completes the inductive step.
Applying the induction up to j = n, we have that
x →

x
0

→

x
∑n
i=1 fi(x)

→
n
∑
i=1
fi(x) ∈ϒW+2k+2,L(Rd,Rk),
where L = ∑n
i=1 Li, since the ﬁrst and last maps above are afﬁne (applying Lemma 1).
Proof of Proposition 2. First observe that any piecewise linear function f with k pieces can be written as
f(x) = a0x+ c+
k−1
∑
i=1
aiσ(x−bi)
(A.1)
for appropriate weights a0,...,ak−1 and b1,...,bk−1. Speciﬁcally, the bi are simply equal to the breakpoints points at
which the derivative of f is discontinuous, while the ai give the jump in derivative at those points. a0 is set equal to
the derivative in the left-most component and c is set to match the value at 0.
Now we apply Proposition 1 to the sum (A.1) to get the desired result, since we easily see that
x →a0x+ c ∈ϒ0(R)
and
x →aiσ(x−bi) ∈ϒ1,1(R).
34

A PREPRINT - APRIL 9, 2024
Proof of Lemma 4. We observe the basic formulas:
max(x,y) = x+ σ(y−x), min(x,y) = x−σ(x−y).
We begin with the afﬁne map

x
y

→
 x
y−x
x−y
!
∈ϒ0(R2,R3).
Next, we use the fact that σ ∈ϒ1,1(R) and Lemmas 1 and 3 to apply σ to the last two coordinates. We get

x
y

→
 
x
σ(y−x)
σ(x−y)
!
∈ϒ4,1(R2,R3).
Finally, we use Lemma 1 to compose with the afﬁne map
 x
y
z
!
→

x+ y
x−z

.
B
Product Network Construction
Proof of Proposition 3. Observe that the piecewise linear hat function
f(x) =
2x
x ≤1/2
2(1 −x)
x > 1/2
satisﬁes f ∈ϒ5,1(R) by Proposition 2. On the interval [0,1], f composed with itself n times is the sawtooth function
f ◦n(x) := (f ◦···◦f)(x) = f(2n−1x−⌊2n−1x⌋),
and one can calculate that (see [60])
x2 = x−
∞
∑
n=1
4−n f ◦n(x)
(B.1)
for x ∈[0,1].
Using this, we construct a network gk ∈ϒ7,k(R) such that
sup
x∈[0,1]
|x2 −gk(x)| ≤4−k.
(B.2)
To do this, we ﬁrst apply the afﬁne map which duplicates the input
x →

x
x

∈ϒ0(R,R2).
(B.3)
Next, we show by induction on k that the map
x →

x−∑k
n=14−n f ◦n(x)
f ◦k(x)

∈ϒ7,k(R,R2).
(B.4)
The base case k = 0 is simply (B.3).
For the inductive step suppose that (B.4) holds for k ≥0. We use Lemma 3 to apply f ∈ϒ5,1(R) to the second
coordinate, showing that

x
y

→

x
f(y)

∈ϒ7,1(R2,R2).
Using the inductive assumption and Lemma 1, we compose this with the map in (B.4) to get
x →

x−∑k
n=1 4−n f ◦n(x)
f ◦(k+1)(x)

∈ϒ7,k+1(R,R2).
35

A PREPRINT - APRIL 9, 2024
We again use Lemma 1 and compose with the afﬁne map

x
y

→

x+ y
y

∈ϒ0(R2,R2)
to complete the inductive step.
To construct gk we then simply compose the map in (B.4) with the afﬁne map

x
y

→x ∈ϒ0(R2,R)
which forgets the second coordinate. Then for x ∈[0,1] we have
gk(x) = x−
k
∑
n=1
4−n f ◦n(x)
and by (B.1) we get the bound (B.2). This gives us a network which approximates x2 on the interval [0,1].
In order to obtain a network which approximates x2 on [−1,1] we observe that if x ∈[−1,1], then σ(x),σ(−x) ∈[0,1],
and
x2 = σ(x)2 + σ(−x)2.
We begin with the single layer network
x →

σ(x)
σ(−x)

∈ϒ2,1(R,R2).
(B.5)
Further, applying Lemma 3, we see that

x
y

→

gk(x)
y

∈ϒ9,k(R2,R2)
and also

x
y

→

x
gk(y)

∈ϒ9,k(R2,R2)
Finally, composing all of these and then applying the afﬁne summation map

x
y

→x+ y ∈ϒ0(R2),
we get, using Lemma 1 (note that we can expand the width of the network in (B.5)), a function hk ∈ϒ9,2k+1(R) such
that on [−1,1], we have
|x2 −hk(x)| ≤|σ(x)2 −hk(σ(x))|+ |σ(−x)2 −hk(σ(−x))| ≤4−k.
(Since one of σ(x) and σ(−x) is 0.)
Finally, to construct a network which approximates products, we use the formula
xy = 2
 x+ y
2
2
−
x
2
2
−
y
2
2
!
If x,y ∈[−1,1], then all of the terms which are squared in the previous equation are also in [−1,1], so that we can
approximate these squares using the network hk. Applying the afﬁne map

x
y

→
 (x+ y)/2
x/2
y/2
!
∈ϒ0(R2,R3),
then successively applying hk to the ﬁrst, second, and third coordinates using Lemmas 3 and 1, and ﬁnally applying
the afﬁne map
 x
y
z
!
→2(x−y−z) ∈ϒ0(R3),
we obtain a network fk ∈ϒ13,6k+3(R2) such that for x,y ∈[−1,1] we have
|fk(x,y)−xy| ≤6 ·4−k,
as desired.
36

A PREPRINT - APRIL 9, 2024
C
Bit Extraction Network Construction
Proof of Proposition 4. We begin by noting that for any ε > 0 the piecewise linear maps
bε(x) =



0
x ≤1/2 −ε
ε−1(x−1/2 + ε)
1/2 −ε < x ≤1/2
1
x > 1/2
and
gε(x) =



x
x ≤1 −ε
1−ε
ε (1 −x)
1 −ε < x ≤1
x−1
x > 1
satisfy bε,gε ∈ϒ5,2(R) by Proposition 2. In addition, these functions have been designed so that if ε < 2−n, we have
for any x of the form (2.1) that
bε(x) = x1, gε(2x) = 0.x2x3 ···xn.
We now construct the network fn,m by induction on m. In what follows, we assume that all of our inputs x are of the
form (2.1). The base case when m = 0 is simply the afﬁne map
x →

x
0

∈ϒ0(R,R2).
For the inductive step, we suppose that we have constructed a map
fn,m−1(x) =

0.xmxm+1 ···xn
x1x2 ···xm−1.0

∈ϒ9,4(m−1)(R,R2)
We then compose this network with an afﬁne map which doubles and duplicates the ﬁrst component

x
y

→
 2x
x
y
!
∈ϒ0(R2,R3)
to get the map
x →
 xm.xm+1 ···xn
0.xmxm+1 ···xn
x1x2 ···xm−1.0
!
∈ϒ9,4(m−1)(R,R3).
Next we choose ε < 2−n and use Lemmas 1 and 3 to apply gε to the ﬁrst component and then bε to the second
component. This gives a map
x →
 0.xm+1 ···xn
xm
x1x2 ···xm−1.0
!
∈ϒ9,4m(R,R3).
Finally, we complete the inductive step by composing with the afﬁne map
 x
y
z
!
→

x
2z+ y

∈ϒ0(R3,R2).
Proof of Lemma 7. Start with the following piecewise linear function
gε(x) :=







0
x ≤0
(j + ε−1(x−j/b))
j/b −ε < x ≤j/b, for j = 1,...,b −1
j
j/b < x ≤(j + 1)/b −ε, for j = 0,...,b −1
b −1
x > 1.
Note that this function has 2b −1 pieces and so by Proposition 2 we have gε ∈ϒ5,2(b−1)(R).
We set x0 = x and q0 = 0 and consider the following recursion
xn+1 = bxn −gε(xn), qn+1 = bqn + gε(xn).
37

A PREPRINT - APRIL 9, 2024
It is easy to verify that if x0 = x ∈[jb−l,(j+1)b−l −ε), then ql = j, since in this case all iterates xn /∈∪b
j=1(j/b−ε, j/b)
so that gε extracts the ﬁrst bit in the b-ary expansion of xn
gε(xn) = j if j/b ≤xn < (j + 1)/b.
In addition, when x0 = x ∈[1 −b−l,1], then ql = bl −1 since gε(xn) = b −1 for all n.
We implement this recursion using a deep ReLU network as follows. We begin with the afﬁne map
x →
 x
x
0
!
=
 x0
x0
q0
!
∈ϒ0(R,R3).
Now, we use induction. Suppose that the map
x →
 xn
xn
qn
!
∈ϒ9,2(b−1)n(R,R3)
has already been implemented. Then we use Lemmas 1 and 3 to apply gε to only the second coordinate. This gives
the map
x →
 
xn
gε(xn)
qn
!
∈ϒ9,2(b−1)(n+1)(R,R3).
Finally, we compose with the afﬁne map
 x
y
z
!
→
 2(x−y)
2(x−y)
2z+ y
!
∈ϒ0(R3,R3)
to complete the inductive step. After l steps of induction, we then compose with the afﬁne map which selects the last
coordinate to get the network q1 ∈ϒ9,2(b−1)l(R).
For higher dimensional cubes Ω= [0,1)d, we construct an indexing network qd ∈ϒ9d,2(b−1)l(Rd). We use Lemma 2
to apply q1 to each coordinate of the input. Then, we compose with the afﬁne map
x →
d
∑
j=1
bl(j−1)xj
to get qd ∈ϒ9d,2(b−1)l(Rd) with
qd(Ωl
i,ε) = ind(i) :=
d
∑
j=1
bl(j−1)i j.
38

