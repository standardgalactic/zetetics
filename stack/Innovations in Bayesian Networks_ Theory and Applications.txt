Dawn E. Holmes and Lakhmi C. Jain (Eds.)
Innovations in Bayesian Networks

Studies in Computational Intelligence,Volume 156
Editor-in-Chief
Prof. Janusz Kacprzyk
Systems Research Institute
Polish Academy of Sciences
ul. Newelska 6
01-447 Warsaw
Poland
E-mail: kacprzyk@ibspan.waw.pl
Further volumes of this series can be found on our homepage:
springer.com
Vol. 134. Ngoc Thanh Nguyen and Radoslaw Katarzyniak (Eds.)
New Challenges in Applied Intelligence Technologies, 2008
ISBN 978-3-540-79354-0
Vol. 135. Hsinchun Chen and Christopher C.Yang (Eds.)
Intelligence and Security Informatics, 2008
ISBN 978-3-540-69207-2
Vol. 136. Carlos Cotta, Marc Sevaux
and Kenneth S¨orensen (Eds.)
Adaptive and Multilevel Metaheuristics, 2008
ISBN 978-3-540-79437-0
Vol. 137. Lakhmi C. Jain, Mika Sato-Ilic, Maria Virvou,
George A. Tsihrintzis,Valentina Emilia Balas
and Canicious Abeynayake (Eds.)
Computational Intelligence Paradigms, 2008
ISBN 978-3-540-79473-8
Vol. 138. Bruno Apolloni,Witold Pedrycz, Simone Bassis
and Dario Malchiodi
The Puzzle of Granular Computing, 2008
ISBN 978-3-540-79863-7
Vol. 139. Jan Drugowitsch
Design and Analysis of Learning Classiﬁer Systems, 2008
ISBN 978-3-540-79865-1
Vol. 140. Nadia Magnenat-Thalmann, Lakhmi C. Jain
and N. Ichalkaranje (Eds.)
New Advances in Virtual Humans, 2008
ISBN 978-3-540-79867-5
Vol. 141. Christa Sommerer, Lakhmi C. Jain
and Laurent Mignonneau (Eds.)
The Art and Science of Interface and Interaction Design (Vol. 1),
2008
ISBN 978-3-540-79869-9
Vol. 142. George A. Tsihrintzis, Maria Virvou, Robert J. Howlett
and Lakhmi C. Jain (Eds.)
New Directions in Intelligent Interactive Multimedia,2008
ISBN 978-3-540-68126-7
Vol. 143. Uday K. Chakraborty (Ed.)
Advances in Differential Evolution, 2008
ISBN 978-3-540-68827-3
Vol. 144.Andreas Fink and Franz Rothlauf (Eds.)
Advances in Computational Intelligence in Transport, Logistics,
and Supply Chain Management, 2008
ISBN 978-3-540-69024-5
Vol. 145. Mikhail Ju. Moshkov, Marcin Piliszczuk
and Beata Zielosko
Partial Covers, Reducts and Decision Rules in Rough Sets, 2008
ISBN 978-3-540-69027-6
Vol. 146. Fatos Xhafa and Ajith Abraham (Eds.)
Metaheuristics for Schedulingin Distributed Computing
Environments, 2008
ISBN 978-3-540-69260-7
Vol. 147. Oliver Kramer
Self-Adaptive Heuristics for Evolutionary Computation, 2008
ISBN 978-3-540-69280-5
Vol. 148. Philipp Limbourg
Dependability Modelling under Uncertainty, 2008
ISBN 978-3-540-69286-7
Vol. 149. Roger Lee (Ed.)
Software Engineering, Artiﬁcial Intelligence, Networking and
Parallel/Distributed Computing, 2008
ISBN 978-3-540-70559-8
Vol. 150. Roger Lee (Ed.)
Software Engineering Research, Management and
Applications, 2008
ISBN 978-3-540-70774-5
Vol. 151. Tomasz G. Smolinski, Mariofanna G. Milanova
and Aboul-Ella Hassanien (Eds.)
Computational Intelligence in Biomedicine and Bioinformatics,
2008
ISBN 978-3-540-70776-9
Vol. 152. Jaroslaw Stepaniuk
Rough – Granular Computing in Knowledge Discovery and Data
Mining, 2008
ISBN 978-3-540-70800-1
Vol. 153. Carlos Cotta and Jano van Hemert (Eds.)
Recent Advances in Evolutionary Computation for
Combinatorial Optimization, 2008
ISBN 978-3-540-70806-3
Vol. 154. Oscar Castillo, Patricia Melin, Janusz Kacprzyk and
Witold Pedrycz (Eds.)
Soft Computing for Hybrid Intelligent Systems, 2008
ISBN 978-3-540-70811-7
Vol. 155. Hamid R. Tizhoosh and M.Ventresca (Eds.)
Oppositional Concepts in Computational Intelligence, 2008
ISBN 978-3-540-70826-1
Vol. 156. Dawn E. Holmes and Lakhmi C. Jain (Eds.)
Innovations in Bayesian Networks, 2008
ISBN 978-3-540-85065-6

Dawn E. Holmes
Lakhmi C. Jain
(Eds.)
Innovations in Bayesian Networks
Theory and Applications
123

Prof. Dawn E. Holmes
Department of Statistics and Applied Probability
University of California
Santa Barbara, CA 93106
USA
Email: holmes@pstat.ucsb.edu
Prof. Lakhmi C. Jain
Professor of Knowledge-Based Engineering
University of South Australia
Adelaide
Mawson Lakes, SA 5095
Australia
Email: Lakhmi.jain@unisa.edu.au
ISBN 978-3-540-85065-6
e-ISBN 978-3-540-85066-3
DOI 10.1007/978-3-540-85066-3
Studies in Computational Intelligence
ISSN 1860949X
Library of Congress Control Number: 2008931424
c⃝2008 Springer-Verlag Berlin Heidelberg
This work is subject to copyright. All rights are reserved, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlm or in any other way, and storage in data
banks.Duplication of this publication or parts thereof is permitted only under the provisions of
the German Copyright Law of September 9, 1965, in its current version, and permission for use
must always be obtained from Springer.Violations are liable to prosecution under the German
Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
Typeset & Cover Design: Scientiﬁc Publishing Services Pvt. Ltd., Chennai, India.
Printed in acid-free paper
9 8 7 6 5 4 3 2 1
springer.com

 
 
 
 
 
 
For: 
 
 
 
 
COLIN   and   HELEN 
 
 
 
 
 

Preface 
There are many invaluable books available on Bayesian networks. However, in com-
piling a volume titled “Innovations in Bayesian networks” we wish to introduce some 
of the latest developments to a broad audience of both specialists and non-specialists 
in this field. 
So, what are Bayesian networks? Bayesian networks utilize the probability calculus 
together with an underlying graphical structure to provide a theoretical framework for 
modeling uncertainty. Although the philosophical roots of the subject may be traced 
back to Bayes and the foundations of probability, Bayesian networks as such are a 
modern device, first appearing in Pearl (1988), and growing out of the research in 
expert or intelligent systems. In Pearl’s ground-breaking Probabilistic Reasoning in 
Intelligent Systems: Networks of Plausible Inference, (1988), he presents Bayesian 
networks for the first time.  Since then, Bayesian networks have become the vehicle of 
choice in many AI applications. 
In compiling this volume we have sought to present innovative research from pres-
tigious contributors in the field of Bayesian networks. Each chapter is self-contained 
and is described below. 
Chapter 1 by Holmes presents a brief introduction to Bayesian networks for readers 
entirely new to the field. 
Chapter 2 by Neapolitan, a self-proclaimed convert to Bayesianism, discusses the 
modern revival of Bayesian statistics research, due in particular to the advent of 
Bayesian networks. Bayesian and frequentist approaches are compared, with an em-
phasis on interval estimation and hypothesis testing. 
For Chapter 3 we are indebted to MIT Press and Kluwer books for permission to 
reprint Heckerman’s famous tutorial on learning with Bayesian networks, in which the 
reader is introduced to many of the techniques fundamental to the success of this  
formalism. 
In Chapter 4, Korb and Nicholson discuss some of the philosophical problems as-
sociated with Bayesian networks. In particular, they introduce a causal interpretation 
of Bayesian networks thus providing a valuable addition to the currently lively and 
productive research in causal modeling. 
Chapter 5 by Niedermayer explores how Bayesian Network theory affects issues 
of advanced computability. Some interesting applications together with a brief discus-
sion of the appropriateness and limitations of Bayesian Networks for human-computer 
interaction and automated learning make this a strong contribution to the field.  

      Preface 
VIII 
In Chapter 6 Nagl, Williams and Williamson introduce and discuss Objective 
Bayesian nets and their role in knowledge integration.  Their importance for medical 
informatics is emphasized and a scheme for systems modeling and prognosis in breast 
cancer is presented. 
Chapter 7 by Jiang, Wagner, and Cooper considers epidemic modeling.  Epidemic 
curves are defined and a Bayesian network model for real-time estimation of these 
curves is give. An evaluation of the experimental results and their accuracy rounds off 
this valuable contribution. 
Chapter 8 by Lauría begins with an excellent introduction to structure learning in 
Bayesian Networks. The paper continues by exploring the feasibility of applying an 
information-geometric approach to the task of learning the topology of Bayesian net-
work and also provides an introduction to information geometry. 
Chapter 9 by Maes, Leray and Meganck discusses causal graphical models for dis-
crete variables that can handle latent variables without explicitly modeling them quan-
titatively. The techniques introduced in this chapter have been partially implementing 
into the structure learning package (SLP) of the Bayesian networks toolbox (BNT) for 
MATLAB. 
Chapter 10 by Flores, Gamez, and Moral presents a new approach to the problem 
of obtaining the most probable explanations given a set of observations in a Bayesian 
network. Examples are given and a set of experiments to make a comparison with 
other existing abductive techniques that were designed with goals similar to those we 
pursue is described. 
Chapter 11 by Holmes describes a maximum entropy approach to inference in 
Bayesian networks. We are indebted to the American Institute of Physics for kind 
permission to reproduce this paper. 
Chapter 12 by de Salvo Braz, Amir, and Roth presents a survey of first-order prob-
abilistic models. The decision on using directed or undirected models is discussed, as 
are infinite models. The authors conclude that such algorithms would benefit from 
choosing to process first the parts of the model that yield greater amounts of informa-
tion about the query. 
This book will prove valuable to theoreticians as well as application scien-
tists/engineers in the area of Bayesian networks. Postgraduate students will also find 
this a useful sourcebook since it shows the direction of current research. 
We have been fortunate in attracting top class researchers as contributors and wish 
to offer our thanks for their support in this project. We also acknowledge the expertise 
and time of the reviewers. Finally, we also wish to thank Springer for their support. 
 
Dr Dawn E. Holmes 
Dr Lakhmi C. Jain 
University of California 
University of South Australia 
Santa Barbara, USA 
Adelaide, Australia 
 

Contents
1
Introduction to Bayesian Networks
Dawn E. Holmes, Lakhmi C. Jain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
2
A Polemic for Bayesian Statistics
Richard E. Neapolitan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3
A Tutorial on Learning with Bayesian Networks
David Heckerman . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
4
The Causal Interpretation of Bayesian Networks
Kevin B. Korb, Ann E. Nicholson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
5
An Introduction to Bayesian Networks and Their
Contemporary Applications
Daryle Niedermayer, I.S.P. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
6
Objective Bayesian Nets for Systems Modelling and
Prognosis in Breast Cancer
Sylvia Nagl, Matt Williams, Jon Williamson . . . . . . . . . . . . . . . . . . . . . . . . .
131
7
Modeling the Temporal Trend of the Daily Severity of an
Outbreak Using Bayesian Networks
Xia Jiang, Michael M. Wagner, Gregory F. Cooper. . . . . . . . . . . . . . . . . . . .
169
8
An Information-Geometric Approach to Learning Bayesian
Network Topologies from Data
Eitel J.M. Laur´ıa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
9
Causal Graphical Models with Latent Variables: Learning
and Inference
Sam Maes, Philippe Leray, Stijn Meganck . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219

X
Contents
10
Use of Explanation Trees to Describe the State Space of
a Probabilistic-Based Abduction Problem
M. Julia Flores, Jos´e A. G´amez, Seraf´ın Moral . . . . . . . . . . . . . . . . . . . . . . .
251
11
Toward a Generalized Bayesian Network
Dawn E. Holmes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
12
A Survey of First-Order Probabilistic Models
Rodrigo de Salvo Braz, Eyal Amir, Dan Roth . . . . . . . . . . . . . . . . . . . . . . . . .
289
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
319

D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 1–5, 2008. 
springerlink.com                                                        © Springer-Verlag Berlin Heidelberg 2008 
1 
Introduction to Bayesian Networks 
Dawn E. Holmes1 and Lakhmi C. Jain2 
1 Department of Statistics and Applied Probability, University of California,  
  Santa Barbara, USA 
 holmes@pstat.ucsb.edu 
2 Knowledge-Based Intelligent Engineering Systems (KES) Centre,  
  University of South Australia, Adelaide, Mawson Lakes, S.A. 5095, Australia 
 lakhmi.jain@unisa.edu.au 
Abstract. Reasoning with incomplete and unreliable information is a central characteristic of 
decision making, for example in industry, medicine and finance. Bayesian networks provide a 
theoretical framework for dealing with this uncertainty using an underlying graphical structure 
and the probability calculus. Bayesian networks have been successfully implemented in areas 
as diverse as medical diagnosis and finance. We present a brief introduction to Bayesian net-
works for those readers new to them and give some pointers to the literature. 
1.1   Introduction 
Reasoning under uncertainty has a long history and is a major issue in artificial intelli-
gence.  Several formalisms have been developed for dealing with uncertainty including 
fuzzy logic, non-monotonic reasoning, Dempster-Shafer theory, possibilistic logic and 
probability theory. The choice of approach is usually governed by the nature of the 
problem, rather than by any commitment to a particular formal approach. However, 
since probability theory is mathematically sound, it provides the ideal formalism.  
The theory of Bayesian networks derives, ultimately from the work of Thomas 
Bayes. In 1764 Bayes ‘An essay towards solving a problem in the doctrine of 
chances’, was published posthumously in the Philosophical Transactions of the Royal 
Society of London, 53:370- 418; this landmark paper contains a special case of Bayes’ 
Theorem, which is concerned with conditional probabilities. 
1.2   Reasoning under Uncertainty 
The original AI problem in computer science was to produce a computer program that 
displayed aspects of intelligent human behavior.  There was general agreement that 
one of the most important intelligent activities was problem solving, which seemed a 
computationally feasible activity. Initial attempts to produce general-purpose problem 
solving programs had limited success, due to problems with knowledge representation 
and complex algorithms requiring exponential processing time. Perhaps the most  
successful of these was GPS (Newell and Simon. 1972) which used a state transfor-
mation method. It was soon realized that experts worked in closed domains and  
that knowledge outside the relevant domain could be ignored. The importance of do-
main specific knowledge became apparent, and the expert or intelligent system was  

2 
D.E. Holmes and L.C. Jain 
developed. There still remained the problem of finding a computationally feasible 
algorithm and propagation seemed to provide the answer. In addition, the underlying 
philosophy of replacing the expert by something better was replaced by the more ac-
ceptable notion of expert system supported decision-making.  
It has been argued to the contrary that the idea of using classical probability theory 
in expert systems should be abandoned due to the computational overload and knowl-
edge elicitation problems (Gorry G. and Barnett G. 1968). In an attempt to overcome 
these problems, Prospector, (Duda R.O., et al 1976) another early system loosely 
based on probability, was developed. Although Prospector oversimplified the mathe-
matics involved, it was sufficiently successful to motivate further research into the use 
of probability in expert systems and subsequently Pearl found a computa-tionally trac-
table method using Bayesian networks where probabilities are propagated through the 
graphical structure (Pearl 1988). The application of Pearl’s work resulted in the expert 
system MUNIN (Muscle and Nerve Inference Network) (Andreasson S et al 1987) 
and from this, the shell HUGIN (Handling Uncertainty in General Inference Net-
works) (Andreasson S.K et al 1989) followed, demonstaring one of the main strengths 
of Bayesian networks; their ability to cope with large joint distributions. 
1.3   Bayesian Networks 
Informally, Bayesian networks are graphical models of causal relationships in a given 
domain. Formally, a Bayesian network is defined as follows.  
Let V be a finite set of vertices and B a set of directed edges between vertices with 
no feedback loops, the vertices together with the directed edges form a directed 
acyclic graph 
=
G
V,B . A set of events is depicted by the vertices of G and hence 
also represented by V. Let each event have a finite set of mutually exclusive out-
comes, where Ei is a variable which can take any of the outcomes 
j
ie of the event i, 
1,...,
i
j
n
=
. Let P be a probability distribution over the combinations of events. Let C 
be the following set of constraints:  
(i) 
the requirement that a probability distribution sums to unity. 
(ii) 
for each event i with a set of parents Mi there are associated conditional 
probabilities (
)
|
i
i
j
j M
P E
E
∈∧
for each possible outcome that can be as-
signed to Ei and the Ej. 
(iii) 
those independence relationships implied by d-separation in the directed 
acyclic graph 
 
Then N
G,P,C
=
 is a causal network if P has to satisfy C. 
 
D-separation, so-called in contrast to the analogous separation property of undirected 
graphs, constitutes a set of rules devised by Pearl, which provide a means for deter-
mining the independence or dependence of any pair of variables in a Bayesian  
 

 
1   Introduction to Bayesian Networks 
3 
network. The theorems arising from d-separation, taken together with the chain rule, 
are used by Pearl to determine the joint probability distribution of the system being 
considered.  
1.3.1   Propagation through Local Computation 
There are several methods of updating probabilities in Bayesian networks. Pearl de-
vised a method for propagating probabilities through a singly connected causal  
network (Pearl 1988) and this method, where probabilities are updated locally, was 
implemented in the expert system MUNIN. Pearl argues that this approach models 
some of the necessary building blocks of human reasoning, but philosophical specula-
tion aside; there are technical reasons in favor of adopting local updating. These are 
largely to do with implementation; for example, the separation of inference engine 
from knowledge-base make it possible to build re-usable shells.  Pearl has achieved 
his objective for singly connected networks; propagation can be performed in linear 
time by Pearl’s algorithm and so it is a very useful method. However, it must be em-
phasized that it only works on networks that are singly connected. It has been shown 
that for multiply-connected networks the problem of determining the probability  
distribution is NP-hard (Cooper G.F. 1990).  
Following on from Pearl’s work, theoretical refinements resulted in the algorithm 
developed by Lauritzen and Spiegelhalter, which has been successfully used in com-
mercially available expert systems, e.g. from the HUGIN shell (Lauritzen S.L. and 
Spiegelhalter D.J. 1988). In contrast to Pearls approach, Lauritzen and Spiegelhalter 
treated the problem of propagating probabilities in Bayesian networks as wholly 
mathematical. Their method, based on graph theory, works for any Bayesian network. 
No attempt was made to model human reasoning but since there is no reason to  
suppose that machine reasoning mimics human reasoning this cannot be considered a 
drawback. 
The main problem in constructing an algorithm for updating the conditional prob-
abilities in a Bayesian network is that the number of terms involved in a global calcu-
lation grows exponentially with the number of nodes in the network. Lauritzen and 
Spiegelhalter’s method overcomes this problem by using a technique involving local 
computations only and provides a standard method of updating which can be used 
once the priors have all been found. The requirement of complete knowledge regard-
ing the prior distribution highlights the limitation of causal networks as a vehicle for 
modeling problem domains. Jensen points out that there is often no theoretical way of 
determining all the required probabilities and gives examples of how they are ascer-
tained in practice Jensen (1996). Sometimes they are guessed, sometimes a complex 
and subjective procedure is gone through in order to produce an approximate and nec-
essarily biased value. When multivalued events are to be modeled, the situation  
becomes complex. In some situations, there are inevitably too many conditional prob-
abilities for an expert to reliably estimate. Thus, the need for a theoretically sound 
technique for estimating them in a minimally prejudiced fashion becomes apparent. 
The maximum entropy formalism provides just such a technique. However, Maung 
and Paris have shown that the general problem of finding the maximum entropy solu-
tion in probabilistic systems is NP-complete; generally there are 2n variables to  
consider and it is thus clearly infeasible to find the probability distribution of all the 

4 
D.E. Holmes and L.C. Jain 
state probabilities (Maung I. and Paris J.B. 1990).Therefore, if we are to apply this 
methodology to causal networks, we must show that the estimates for missing infor-
mation can be found efficiently.  
1.3.2   Inference in Bayesian Networks 
There are several well-known methods of exact inference in Bayesian networks: 
variable elimination and clique tree propagation being particularly popular. The 
methods of approximation most used are stochastic MCMC simulation and bucket 
elimination. 
1.3.3   Learning in Bayesian Networks 
A very useful and freely available resource on learning in Bayesian networks is An-
drew Moore’s website: 
http://www.cs.cmu.edu/~awm/tutorials. 
See also Heckerman, this volume. 
1.4   Applications 
Practically, graphical models are appealing since they provide a bridge between the 
user and the knowledge engineer. It is particularly important from the user’s point of 
view that they can be constructed gradually and so complex models can be built over 
a period of time. For the knowledge engineer their appeal lies, at least in part, in that 
the data structures are already recognizable. Among  the more well-known applica-
tions, The Pathfinder Project (Heckerman, Horvitz, Nathwani 1992), resulted in a 
system for diagnosing diseases of the lymph nodes that outperforms expert diagnosis. 
The AutoClass project is an unsupervised Bayesian classification system. Microsoft’s 
Office Assistant, a result of the Lumiere  Project, is also the result of a Bayesian net-
work. See, for example, Niedermayer, this volume. 
1.5   Selective Bibliography 
Below we mention just a few of the books and papers that  a newcomer to the field of 
Bayesian networks may find useful. 
 
Pearl, J.: Probabilistic Reasoning in Intelligent Systems. In: Networks of Plausible Inference. 
Morgan Kaufmann Publishers, San Francisco (1988) 
Lauritzen, S.L., Spiegelhalter, D.J.: Local Computations with Probabilities on Graphical Struc-
tures and their Applications to Expert Systems. J. Royal Statist.Soc. B 50(2), 154–227 
(1988) 
Andreasson, S., Woldbye, M., Falck, B., Andersen, S.K.: MUNIN - A Causal Probabilistic 
Network for Interpretation of Electromyographic Findings. In: Proc. of the 10th Int. Joint 
Conference on AI., Milan, pp. 366–372 (1987) 
Andreasson, S.K., Oleson, K.G., Jensen, F.V., Jensen, F.: HUGIN- A Shell for Building Bayes-
ian Belief Universes for Expert Systems. In: Proc. of the 11th Int. Joint Conference on AI., 
Detroit (1989) 

 
1   Introduction to Bayesian Networks 
5 
Klir, G.J., Folger, T.A.: Fuzzy Sets. In: Uncertainty and Information. Prentice-Hall, Englewood 
Cliffs (1995) 
Shannon, C.E., Weaver, W.: The Mathematical Theory of Communication. University of Illi-
nois Press (1948) 
Neapolitan, R.E.: Probabilistic Reasoning in Expert Systems. John Wiley, Chichester (1990) 
Jensen Finn, V.: An Introduction to Bayesian Networks. UCL Press (1996) 
Cooper, G.F.: The Computational Complexity of Probabilistic Inference Using Bayesian Belief 
Networks. Artificial Intelligence 42, 393–405 (1990) 

2
A Polemic for Bayesian Statistics
Richard E. Neapolitan
Northeastern Illinois University, Chicago, Il 60625
RE-Neapolitan@neiu.edu
Abstract. In the early part of the 20th century, forefathers of current statistical
methodology were largely Bayesians. However, by the mid-1930’s the Bayesian method
fell into disfavor for many, and frequentist statistics became popular. Seventy years
later the frequentist method continues to dominate. My purpose here is to compare
the Bayesian and frequentist approaches. I argue for Bayesian statistics claiming the
following: 1) Bayesian methods solve a wider variety of problems; and 2) It is sometimes
diﬃcult to interpret frequentist results.
2.1
Introduction
In the early part of the 20th century, forefathers of current statistical method-
ology were largely Bayesians (e.g. R.A. Fisher and Karl Pearson). However, by
the mid-1930’s the Bayesian method fell into disfavor for many, and frequentist
statistics, in particular the use of conﬁdence intervals and the rejection of null
hypotheses using p-values, became popular. Seventy years later the frequentist
method dominates, and most university statistics courses present this approach
without even a reference to Bayesian methods. However, there has recently been
a resurgence of interest in Bayesian statistics, partly due to the use of Bayesian
methods in Bayesian networks ([Pearl, 1988], [Neapolitan, 1990]) and in machine
learning. Curiously, sometimes systems that learn using Bayesian methods are
evaluated using frequentist statistics (e.g. in [Buntine, 1992]). This attests to the
dominance of frequentist statistics. That is, it seems researchers in Bayesian ma-
chine learning learned frequentist evaluation methods ﬁrst and therefore adhere
to them.
My purpose here is to compare the Bayesian and frequentist approaches fo-
cusing on interval estimation and hypothesis testing. I make no eﬀort to cover
the frequentist or Bayesian method in any detail; rather I show only suﬃcient
mathematics and examples to contrast the two approaches. I argue for Bayesian
statistics claiming the following: 1) Bayesian methods solve a wider variety of
problems; and 2) It is sometimes diﬃcult to interpret frequentist results, partic-
ularly in the case of hypothesis testing, whereas the interpretation of Bayesian
results is straightforward and meaningful. Bolstad [2004] solves many more prob-
lems using the two approaches, while Zabell [1992] presents some of the his-
tory of statistics over the 20th century, focusing on the life of R.A. Fisher. I
D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 7–32, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

8
R.E. Neapolitan
assume the reader is familiar with both the Bayesian and frequentist approaches
to probability. See [Neapolitan, 2004] for a brief, facile discussion of the two.
After methods for interval estimation are compared, hypothesis testing is
discussed.
2.2
Interval Estimation
A random sample consists of independent, identically distributed random vari-
ables X1, X2, . . . , Xn . In practice a random sample is obtained by selecting
individuals (or items) at random from some population.
Example 1. Suppose we select 100 American males at random from the popu-
lation of all American males. Let Xi be a random variable whose value is the
height of the ith individual. If, for example, the 10th individual is 71 inches,
then the value of X10 is 71.
Mathematically, the population is really the collection of all values of the vari-
able of interest. For example, if our set of entities is {George, Sam, Dave, Clyde},
their heights are 68 inches, 70 inches, 68 inches, and 72 inches, and the variable of
interest is height, then the population is the collection [68, 70, 68, 72]. However,
it is common to refer to the collection of all entities as the population.
A common task is to assume that the random variables are normally dis-
tributed, obtain values x1, x2, . . . , xn of X1, X2, . . . , Xn, estimate the true pop-
ulation mean (expected value) μ by
¯x =
n

i=1
xi,
and obtain some measure of conﬁdence as to how good this estimate is.
Example 2. Suppose we sample 100 American males and the average value of the
height turns out to be 70 inches. Then ¯x = 70, and our goal is to obtain some
measure of conﬁdence as to how close the true average height μ is to 70.
Ordinarily, we don’t know the variance either. However, for the sake of sim-
plicity, in the discussion here we will assume the variance is known. Although
it is elementary statistics, we will review the classical statistical technique for
obtaining a conﬁdence interval so that we can compare the method to Fisher’s
ﬁducial probability interval, and the Bayesian probability interval.
2.2.1
Conﬁdence Intervals and Fiducial Probability Intervals
First denote the normal density function and normal distribution as follows:
NormalDen(x; μ, σ2) ≡
1
2πσ e−(x−μ)2
2σ2 .

2 A Polemic for Bayesian Statistics
9
NormalDist(ξ; μ, σ2) ≡
 ξ
−∞
NormalDen(x; μ, σ2)dx.
Next we have the following lemma, whose proof can be found in most classical
statistics texts.
Lemma 1. Suppose we have a random sample X1, X2, . . . , Xn, where each Xi
has density function N(xi : μ, σ2). Then the random variable ¯X, which is the
average of the n random variables, has density function
NormalDen(n; μ, σ2/n).
Intuitively, the average has the same mean as the variables in the sample, but
a smaller variance. For example, if we are sampling one individual there will
be a lot of variance in the height obtained. However, if we are sampling 1000
individuals and taking the average of the heights, there will be much less variance
in the average obtained. Due to the result in this lemma, we have that
P( ¯X < ξ) = NormalDist(ξ; μ, σ2/n)
= NormalDist( ξ −μ
σ/√n; 0, 1).
Recall NormalDist(x; 0, 1) is called the standard normal distribution. Now
suppose we want to know the value of ξ such that P( ¯X < ξ) = 1 −α. We set
NormalDist( ξ −μ
σ/√n; 0, 1) = 1 −α,
which means
ξ −μ
σ/√n = zα,
where zα ≡NormalDist−1(1 −α). We then have
ξ = μ + zα
σ
√n.
We therefore conclude that
P( ¯X −zα
σ
√n < μ) = 1 −α.
Proceeding in the same way we can show that
P( ¯X −zα/2
σ
√n < μ < ¯X + zα/2
σ
√n) = 1 −α.
(2.1)
Example 3. Suppose in a sample of size n = 100 the value ¯x of ¯X is 70, and we
assume σ = 4. Let α = .05. Then
¯x −zα/2
σ
√n = 70 −1.96 4
√
10 = 67.52
¯x + zα/2
σ
√n = 70 + 1.96 4
√
10 = 72.48.

10
R.E. Neapolitan
Our purpose was not to review elementary statistics. Rather we wanted to
show the mathematics involved in obtaining the interval (67.52, 72.48) so we
could discuss the meaning we could attach to the interval. In 1937 Jerzy Neyman
noted that, if we take a relatively frequency approach to probability, owing to
Equality 2.1, in 95% of trials the interval obtained will contain the mean μ,
and in only 5% of trials it will not (By ‘trial’ I mean a single occurrence of
sampling n individuals and taking the average their heights.). So Neyman called
(67.52, 72.48) a 95% conﬁdence interval. Speciﬁcally, he said the following:
The functions θ(E) and θ(E) satisfying the above conditions will be
called the lower and upper conﬁdence limits of θ1. The value α of the
probability will be called the conﬁdence coeﬃcient, and the interval, say
δ(E), from θ(E) to θ(E), the conﬁdence interval corresponding to the
conﬁdence coeﬃcient α.
- Jerzy Neyman [1937, p. 348]
In general,

¯x −zα/2
σ
√n, ¯x + zα/2
σ
√n

(2.2)
is called a 100(1 −α)% conﬁdence interval for the unknown mean μ.
Note that Neyman was not saying that the probability that μ is in (67.52,
72.48) is .95. That is, μ is a parameter (constant) and not a random variable
in the probability space, and therefore, according to Neyman’s theory of prob-
ability, we cannot make probabilistic statements about μ. This is why Neyman
coined the new term ‘conﬁdence’. Furthermore, Neyman was not making the
statement that we can be conﬁdent that the particular interval obtained in this
trial contains the unknown parameter, but rather that over all trials over all dis-
tributions, we can have 95% conﬁdence that we will obtain an interval containing
the parameter. The following statement makes this clear:
During a period of time the statistician may deal with a thousand
problems of estimation and in each case the parameter θ1 to be estimated
and the probability law of the X’s may be diﬀerent. As far as in each case
the functions θ(E) to θ(E) are properly calculated and correspond to the
same value of α, his steps (a), (b), and (c), though diﬀerent in details
of sampling and arithmetic, will have this in common - the probability
of their resulting in a correct statement will be the same, α. Hence the
frequency of actually correct statements will approach α. ...
... Can we say that in this particular case the probability of the true
value of θ1 falling between 1 and 2 is equal to α?
The answer is obviously negative. The parameter θ1 is an unknown
constant and no probability statement concerning its value may be made.
- Jerzy Neyman [1937, p. 349]
Yet many modern frequentist texts apply the concept to a single trial. For ex-
ample, Anderson et al. [2005] state ‘Because 95% of all the intervals constructed

2 A Polemic for Bayesian Statistics
11
using ¯x±3.92 will contain the population mean, we say that we are 95% conﬁdent
that the interval 78.08 to 85.92 includes the population mean μ.’
R.A. Fisher was at one time a Bayesian, but he abandoned the approach in
favor of the frequentist approach. However, he felt we should still make a prob-
abilistic statement about μ, but do so without making any prior assumptions,
even ones about prior ignorance. He noted that if we say the probability that μ
is in the interval is .95, this is a true probability in the relative frequency sense
because 95% of the time μ will be in the interval. Like Neyman, he is not saying
that the probability that μ is in a particular interval obtained in a given trial is
.95, but rather over all trials it will be in the interval generated 95% of the time.
He called this a ﬁducial probability. Speciﬁcally, he said the following
We may express this relationship by saying that the true value of
θ will be less than the ﬁducial 5 per cent value corresponding to the
observed value of T in exactly 5 trials in 100. ... This then is a deﬁnite
probability statement about the unknown parameter θ which is true
irrespective of any assumption as to its a priori distribution.
- R.A. Fisher [1930, p. 533]
It’s not clear (at least to me) whether Fisher meant over all trials for a partic-
ular θ, or over all trials over all possible θ (that is, all normal distributions). Both
statements are true. If he meant the latter it seems he is just stating the same
thing as Neyman in a slightly diﬀerent way, which critics noted. If he meant the
former, the distinction between his interpretation and Neyman’s is more clear.
In any case, Neyman’s conﬁdence interval won out and is the current interpre-
tation and terminology used in frequentist statistics. Zabell [1992] provides an
excellent discussion of Fisher’s ﬁduciary probabilities, how Fisher’s description
of them changed over time from the one we gave above, and why they failed to
gain favor.
2.2.2
Probability Intervals
The Bayesian approach to representing uncertainty in the value of an un-
known parameter is somewhat diﬀerent. The fundamental diﬀerence between
the Bayesian and frequentist approach is that the Bayesian represents uncer-
tainty in an outcome of a single experiment or in a variable having a particular
value with a probability of that outcome or that value, whereas the frequen-
tist reserves the term probability for the relative frequency of occurrence of an
outcome in repeated trials of an experiment. Bayesians call probabilities in the
frequentist’s sense relative frequencies, and probabilities in their sense be-
liefs. I have reasoned elsewhere for the Bayeisan interpretation of probability
and the use of probability to represent belief (See [Neapolitan, 1990, 2004]).
The Bayesian, therefore, represents uncertainty concerning the value of an
unknown parameter with a higher order probability distribution in which
the parameter is a random variable. For example, if we are going to repeatedly
do the experiment of tossing a thumb tack, we can assume there is some relative

12
R.E. Neapolitan
X2
M
X1
Xn
U(P) = N(P;a,V2
M)
U(x1|P) = N(x1;P,V2)
U(x2|P) = N(x2;P,V2)
U(xn|P) = N(xn;P,V2)
Fig. 2.1. A Bayesian network representing the assumption that the Xis are indepen-
dent conditional on M. We have simply used N rather than NormalDen to denote the
normal density function.
frequency p (frequentist probability) with which it will land ‘heads’ (on its ﬂat
end rather than with the edge of the ﬂat end and the point touching the ground).
So P(Heads) = p is an unknown parameter in the probability space determined
by this experiment. We can represent our uncertainty as to this parameter’s
value by creating a probability distribution of p. If we thought all values of p
were equally like, we could use the uniform prior density function. That is, we
set ρ(p) = 1 over the interval [0, 1]. Before we toss the thumb tack, our proba-
bility distribution of p is called our prior probability distribution of p. After
tossing it a number of times, the updated probability distribution, based on the
prior distribution and results of the tosses, is called our posterior probability
distribution of p.
In the case of the unknown mean μ of a normal distribution, we can represent
our uncertainty as to the value of μ using a higher order normal distribution.
That is, we represent the unknown mean μ by a random variable M, whose space
consists of the possible values of μ, and which has density function
NormalDen(μ; a, σ2
M).
Note that for this distribution the expected value of M is a and the vari-
ance of M is σ2
M. Given a random sample X1, X2, . . . , Xn, we assume the
Xi are independent conditional on the value of M. The Bayesian network in
Figure 2.1 represents this assumption. This assumption is more readily explained
with the example of tossing a thumbtack. If we knew the relative frequency of
heads was, for example, .3, our belief in heads for the outcome of any particular
toss would be .3 regardless of the outcomes of the other tosses. So the outcomes
are independent conditional on the value of P(Heads). However, if we did not
know P(Heads), and the ﬁrst 8 tosses were heads, we would consider it more

2 A Polemic for Bayesian Statistics
13
likely that the thumbtack has a propensity to land head often, and increase our
belief in heads for the 9th toss.
The theorem that follows gives us the posterior distribution of M given a
random sample. In this theorem, we will ﬁnd it more convenient to reference the
precision of a normal distribution rather than the variance. If σ2 is the variance
of a normal distribution than the precision r is given by
r = 1
σ2 .
Theorem 1. Suppose we have a normal distribution with unknown mean μ,
known precision r > 0, and we represent our prior belief concerning the mean
with a random variable M that has density function
NormalDen(μ; a, 1/v),
where v > 0. Then, given a random sample of size n, and a value ¯x of the mean
of the sample, the posterior density function of M is
NormalDen(μ; a∗, 1/v∗),
where
a∗= va + nr¯x
v + nr
and
v∗= v + nr.
(2.3)
Proof. Let d = {x1, x2, . . . , xn} be the set of values of the random sample. It is
not hard to show that
ρ(d|μ) ⋍exp

−r
2
n

i=1
(xi −μ)2

,
(2.4)
where exp(y) denotes ey and ⋍means ‘proportionate to’, and that
n

i=1
(xi −μ)2 = n(μ −x)2 +
n

i=1
(xi −x)2.
(2.5)
Since the far right term in Equality 2.5 does not contain μ, we may rewrite
Relation 2.4 as follows:
ρ(d|μ) ⋍exp

−nr
2 (μ −x)2	
.
(2.6)
The prior density function of M satisﬁes the following:
ρ(μ) ⋍exp

−v
2(μ −a)2	
.
(2.7)
We have
ρ(μ|d) ⋍ρ(d|μ)ρ(μ)
(2.8)
≃exp

−v
2(μ −a)2	
exp

−nr
2 (μ −x)2	
.

14
R.E. Neapolitan
The ﬁrst proportionality in Relation 2.8 is due to Bayes’ Theorem, and the second
is due to Relations 2.6 and 2.7. It is not hard to show
v(μ −a)2 + nr(μ −x)2 = (v + nr)(μ −a∗)2 + vnr(x −a)2
v + nr
.
(2.9)
Since the ﬁnal term in Equality 2.9 does not contain μ, it can also be included
in the proportionality factor. So we can rewrite Relation 2.8 as
ρ(μ|d) ⋍exp

−v + nr
2
(μ −a∗)2

⋍exp

−(μ −a∗)2
2 (1/v∗)

⋍NormalDen(μ; a∗, 1/v∗).
(2.10)
Since ρ(μ|d) and NormalDen(μ; a∗, 1/v∗) are both density functions, their inte-
grals over the real line must both equal 1. Therefore, owing to Relation 2.10, they
must be the same function.
Example 4. Suppose we assume the heights of American males are normally dis-
tributed with known variance 4, and we represent our prior belief concerning the
average height of American men with the density function
NormalDen(μ; 72, 32),
and in a sample of size n = 100 the value ¯x of is 70. Then a = 72, v = 1/9, and
r = 1/4. We therefore have that
a∗= va + nr¯x
v + nr
= (1/9) 72 + 100(1/4)70
1/9 + 100(1/4)
= 70.009.
v∗= v + nr = (1/9) + 100(1/4) = 25.111.
Suppose instead that n = 10. Then
a∗= va + nr¯x
v + nr
= (1/9) 72 + 10(1/4)70
1/9 + 10(1/4)
= 70.09.
v∗= v + nr = (1/9) + 10(1/4) = 2.61.
A 100(1 −α)% probability interval for the unknown mean is an inter-
val centered around the expected value of the unknown mean that contains
100(1 −α)% of the mass in the posterior probability distribution of the mean.
That is, the probability that μ is in this interval is .95. It is not hard to see that
this interval is given by

a∗−zα/2
1
√
v∗, a∗+ zα/2
1
√
v∗

.

2 A Polemic for Bayesian Statistics
15
Notice that the precision v in our prior distribution of μ must be positive.
That is, the variance cannot be inﬁnite. To represent prior ignorance as to the
mean we take the variance to be inﬁnite and therefore the precision to be 0. This
would constitute an improper prior distribution. Neapolitan [2004] shows how
to compute a posterior distribution using this improper prior. We can obtain
the same result by simply taking the limit of the expressions in Equality 2.3 as
v goes to 0. To that end, we have
lim
v→0 a∗= lim
v→0
va + nr¯x
v + nr
= ¯x
and
lim
v→0 v∗= lim
v→0 (v + nr) = nr.
(2.11)
In this case, our 100(1 −α)% probability interval is equal to

¯x −zα/2
1
√nr, ¯x + zα/2
1
√nr

=

¯x −zα/2
σ
√n, ¯x + zα/2
σ
√n

.
We see that this interval is identical to the conﬁdence interval shown in
Equality 2.2.
Example 5. Suppose we want to obtain an estimate of how many pounds of coﬀee
a coﬀee manufacturer currently puts, on the average, into its 3 pound coﬀee cans.
Suppose further that, from years of previous data, we know that σ = .3, and in a
sample of size n = 2 we obtain the result that ¯x = 2.92. Then for the frequentist
a 95% conﬁdence interval for the unknown average μ is given by

¯x −z.05/2
σ
√n, ¯x + z.05/2
σ
√n

=

2.92 −1.96 .3
√
2, 2.92 + 1.96 .3
√
2

(2.12)
= (2.504, 3.336).
This is also a 95% probability interval for the Bayesian if we assume prior
ignorance.
I stress that although the conﬁdence interval and the probability interval as-
suming prior ignorance are the same interval, the frequentist and the Bayesian
attach diﬀerent meaning to them. If we take an empirical Bayesian stance and
acknowledge the existence of relative frequencies in nature, and if we assume the
means μ of normal distributions are distributed uniformly in nature, then a 95%
posterior probability distribution can be described as follows. Let a trial consist
of selecting a population from the set of all normally distributed populations in
nature, taking a sample of size n from this population, and obtaining a particular
value of ¯x. In 95% of these trails, the population selected will have a mean μ that
is in the particular interval determined by ¯x and n. This is a probabilistic state-
ment about μ. On the other hand, a 95% conﬁdence interval can be described
like this. Suppose we have a particular population with unknown mean μ. Let a
trial consist of taking a sample of size n from this population. In 95% of these
trials, the conﬁdence interval obtained will contain the mean μ of this popula-
tion. This is a probabilistic statement about the intervals obtained in repeated
trials. Furthermore, if a trial consists of selecting a population from the set of

16
R.E. Neapolitan
all normally distributed populations in nature, and then taking a sample size n
from the population selected, the conﬁdence interval obtained will still contain
the population mean 95% of the time (This is Neyman’s explanation). The con-
ﬁdence interval approach (and its related ﬁduciary probability approach) does
not say that the mean of the selected population is in the particular resultant
interval 95% of the time we have this particular resultant value of ¯x. Rather it
says the interval obtained will contain the population mean 95% of the time.
Since the approach does not entail any assumption about the prior distribution
of means in nature, it cannot lead to a posterior distribution of means, and so
it cannot yield a probability statement that the mean is in a particular interval.
You might say “What if means are not uniformly distributed in nature? Then
the Bayesian will get the ‘wrong’ answer.” My statement about the uniform
distribution of means was just a frequentist allegory. That is, it is the frequentist
who models reality by claiming that there is an objective probability distribution
out there, and we can only make probabilistic statements that are true relative
to this distribution. The Bayesian feels probability is about coherent belief and
updating that belief in the light of new evidence. So all that matters to the
Bayesian is that for this particular distribution there is no reason to believe one
mean is more likely than another. Whose interpretation of probability is more
cogent - the frequentist’s or the Bayesian’s? Let’s answer that with another
question: Is there a relative frequency, accurate to an arbitrary number of digits,
with which the coin in my pocket has been somehow predetermined to land
heads if I toss it indeﬁnitely? Or rather do we say the probability of heads is .5
because we have no reason to prefer heads to tails, and would we change that
belief if suﬃcient tosses indicate otherwise?
Actually, it does not seem reasonable to assume prior ignorance in Example
5 since we know the average amount of coﬀee cannot be negative. Rather than
assuming ignorance, we should at least assume knowledge upon which everyone
would agree. So minimally we should assume μ > 0. If we did this we would
obtain a diﬀerent probability interval. Instead of showing an example that makes
this assumption (which would result in more complex mathematics to compute
the posterior probability), we show a simpler case.
Example 6. Let’s assume we can get everyone to agree the prior expected value
of the mean is 3 and it is very unlikely to be less than 2 or greater than 4. In
this case a standard technique is to use 1/6 of the diﬀerence between the high
and low possible values of the mean as our prior standard deviation. So in this
case our prior standard deviation is (1/6) (4 −2) = 1/3, which means our prior
precision is v = 1/(1/3)2 = 9. So our prior density function for the mean is
NormalDen(μ; a, 1/v),
where a = 3 and v = 9. If we obtain the results in the previous example, we
have then that
a∗= va + nr¯x
v + nr
= (9) 3 + 2
 1
.3
2 2.92
9 + 2
 1
.3
2
= 2.943.

2 A Polemic for Bayesian Statistics
17
v∗= v + nr = 9 + 2
 1
.3
2
= 31.222.
So our posterior 95% probability interval is given by

a∗−z.05/2
1
√
v∗, a∗+ z.05/2
1
√
v∗

=

2.943 −1.96
1
√
31.222, 2.943 + 1.96
1
√
31.222

= (2.592, 3.294).
Note that the 95% probability interval obtained in the preceding example is
substantially diﬀerent from the conﬁdence interval obtained in Example 5. I
purposely used a small sample size to show the diﬀerence can be substantial. As
mentioned before, a given individual may not agree with the prior distribution
we used for the mean in this example, but no individual would have the prior
belief that the mean could be negative. Which means no one would obtain the
posterior probability interval in Equality 2.12.
Even in the case of an unknown variance a 100(1−α)% conﬁdence interval
for the unknown mean μ of a normal distribution is identical to a 100(1 −α)%
probability interval for μ under the assumption of prior ignorance. Neapolitan
[2004] shows this result. However, in general the two are usually close but not
identical.
2.2.3
Conﬁdence Intervals or Probability Intervals?
We presented two methods for obtaining our belief in the location of the mean
of a normal distribution from a random sample, namely the conﬁdence interval
and the probability interval. Which method is more compelling? Next we review
arguments in favor of each.
Arguments for the Conﬁdence Interval
As noted previously, R.A. Fisher was originally a Bayesian. However, by 1921
he said that the Bayesian approach “depends upon an arbitrary assumption,
so the whole method has been widely discredited,” and in 1922 he denounced
“inverse probability, which like an impenetrable jungle arrests progress towards
precision of statistical concepts.” It was this displeasure with Bayesian methods
that led statisticians to embrace the notion of ‘complete objectivity’ and the
conﬁdence interval interpretation. They wanted a technique that did not require
an individual’s subjective belief; their aim was to assure that everyone must agree
on statistical results. But why couldn’t the ‘objective’ statistician be content with
a uniform prior distribution for the unknown parameter, since it represents prior
ignorance? Fisher also noted in 1922 that the uniformity of a prior distribution is
not invariant under a parametric transformation. For example, let p be a random
variable representing the unknown relative frequency (probability) with which

18
R.E. Neapolitan
a thumbtack lands heads. We can assume prior ignorance as to the value of p
using the uniform density function. That is, ρ(p) = 1 for all p ∈[0, 1]. However,
if we let q = p2, then q is not uniformly distributed in the interval [0, 1]. It is
more probable that q takes small values. For example, the probability that q is
in [0, .25] is equal to the probability that p is in [0, .5], which is .5. So our claim
as to ignorance about p is a claim as to knowledge about q.
We see that the argument for using the conﬁdence interval methodology is
actually that the Bayesian approach is not reasonable. The ﬁrst objection about
‘arbitrary assumptions,’ however, is not very compelling because the Bayesian
method does not require an individual’s prior belief; rather it just allows them.
We can always use a uniform prior. But, as noted above, Fisher also objected to
this because a uniform prior on one random variable can result in a nonuniform
prior on another. So he concludes that a uniform prior does not really represent
prior ignorance. This is, however, exactly the point. That is, uniformity is relative
to a particular choice of scale, and therefore represents knowledge, not ignorance.
As Sandy Zabell (private correspondence) puts it, “ Before you impose a uniform
prior, you have to choose the units or scale. Once you do that, you can write
down the uniform prior. But if you had chosen another scale, you would have
gotten a diﬀerent prior, uniform with respect to the second scale. So the question
becomes: what is the justiﬁcation for the particular scale chosen? If you claim
not to know anything at all, then obviously you can’t justify the choice of any
particular scale.” So when we say that p is uniformly distributed in [0, 1] our
knowledge is that we are completely ignorant as to the value of p. This entails
that we are not completely ignorant as to the value of p2.
Arguments for the Probability Interval
The standard argument for the Bayesian approach is that it allows us to combine
information and evidence from diﬀerent sources. For example, when developing
an expert system we may want to combine expert information with informa-
tion obtained from data in order to estimate a probability value. When using a
medical expert system we may want to compute the probability of lung cancer
given the evidence that the patient smokes and has a positive chest X-ray. When
trying to determine how likely it is that a husband killed his wife, we may want
to combine the evidence that the husband’s gun was used to shoot the wife and
that the husband’s blood was found at the scene of the crime. We will not be-
labor this matter further as it is discussed in detail in Bayesian statistics books
such as [Berry, 1996].
Rather we present the more subtle argument that the use of the conﬁdence
interval can lead one to a ‘Dutch book,’ which is a bet one cannot win. The Dutch
book argument in probability is based on the assumption that if one claims the
probability of an event is p, than that person would deem it fair to give $p for
the promise to receive $1 if the event occurs (or is true). For example, if Clyde
says that the probability that the Chicago Cubs will beat the Detroit Tigers in
an upcoming American baseball game is .6, then Clyde should consider it fair
to give $.6 for the promise to receive $1 if the Cubs win. Since he deems it a

2 A Polemic for Bayesian Statistics
19
fair bet, Clyde should be willing to take either side of the bet. It is possible to
show (See [Finetti, 1964] or [Neapolitan, 1990].) that, if an individual assigns
numbers (representing the individual’s beliefs) between 0 and 1 to uncertain
events, then unless the individual agrees to assign and combine those numbers
using the rules of probability, the individual can be forced into a Dutch book.
Here is a simple example. Suppose Clyde maintains that the probability of the
Cubs winning is .6 and the probability of the Tigers winning is .6. Then we
can require that he give $.6 for the promise to receive $1 if the Cubs win, and
also require that he give $.6 for the promise to receive $1 if Detroit wins. Since
precisely one team will win (There are no ties in American baseball), Clyde will
deﬁnitely give $1.20 and receive $1. Poor Clyde is sure to lose $.2. So he should
have set P(Cubs win) + P(T igers win) = 1 as the rules of probability require.
Consider now the conﬁdence assigned to a conﬁdence interval. Regardless of
the physical interpretation we give to it, is it not a number between 0 and 1
that represents its user’s belief as to the location of the unknown parameter? If
so, it satisﬁes the requirements to be called a subjective probability. Consider
again Example 5. If Clyde says he is 95% conﬁdent the mean is in (2.504, 3.336),
wouldn’t Clyde say it is fair to give up $.95 for the promise to receive $1 if
the mean is in that interval? If Clyde acknowledges he would do this, then the
conﬁdence measure is indeed a subjective probability for Clyde.
Suppose now that Clyde accepts Bayesian methodology. As noted following
Example 5, everyone, including Clyde, knows that the mean cannot be nega-
tive. So, whatever Clyde’s prior belief is concerning the value of the mean, it is
diﬀerent from the improper uniform prior over the real line. Therefore, Clyde’s
posterior 95% probability interval, obtained using Bayesian updating, will not
be (2.504, 3.336) (Recall that this is also the 95% probability interval obtained
when we assume a uniform prior over the real line.). For the sake of illustration,
let’s say Clyde’s posterior 95% probability interval is the interval (2.592, 3.294)
obtained in Example 6. Since this interval was obtained using the rules of prob-
ability, Clyde must use it to represent his posterior belief to avoid a Dutch
book. However, from his conﬁdence interval analysis, he also has the interval
(2.504, 3.336) representing his posterior belief. This means we can get him to
bet on both intervals. We have him give $.95 for the promise to receive $1 if
the mean is in (2.592, 3.294), and we have him receive $.95 for the promise to
give $1 if the mean is in (2.504, 3.336) (Recall that he would take either side of
the bet). If it turns out the mean is in (2.592, 3.294) or outside (2.504, 3.336),
he breaks even. However, if it is in (2.504, 2.592) or in (3.294, 2.336), he loses
$.95+$.05 = $1.00. Clyde can’t win money, but can lose some. Football gamblers
call this situation a ‘middle’, and they actively look for two diﬀerent spreads on
the same game to be on the advantageous end of the middle. They would like to
ﬁnd Clyde.
So if Clyde (or any Bayesian) accepts the argument concerning Dutch books,
he cannot, in general, use a conﬁdence interval to represent his belief. He can of
course use it for its literal meaning as the probability of an interval that contains

20
R.E. Neapolitan
the mean. There is no disputing this. But its literal meaning has limited value
as he can’t bet on it (literally or ﬁguratively).
Where does all this leave the frequentist? The frequentist can escape from
this conundrum by refusing to play the Bayesian’s game. Suppose Ann is a
frequentist. Ann could argue that her conﬁdence in a conﬁdence integral does
not represent what she consider’s a fair bet. So we can’t force her to bet according
to it. Or, Ann could maintain that, even though she knows the mean cannot be
negative, she cannot assign a numerical prior probability distribution to the
value of the mean. So she has no posterior probability interval for the mean, and
therefore there is no bet we could force concerning such an interval. Regardless,
it should be somewhat disconcerting to one who would use a conﬁdence interval
that, in some cases, the conﬁdence interval obtained is identical to the probability
interval obtained assuming a mean could be negative when we know it cannot.
2.3
Hypothesis Testing
After discussing frequentist hypothesis testing, we show the Bayesian method.
2.3.1
Frequentist Hypothesis Testing
We illustrate the method with an example of one of the most common applica-
tions of hypothesis testing.
Example 7. Suppose a coﬀee manufacturer claims that on the average it puts 3
pounds of coﬀee in its 3 pound coﬀee cans. However, consumers have complained
that they have not been receiving that much. So we decide to investigate the
claim by obtaining a random sample of size n = 40 coﬀee cans. Suppose further
that the result of that sample is that ¯x = 2.92, and that, from years of previous
data, we know that σ = .3. We are interested in whether the true mean μ is ≥3
because, if so, the manufacturer is meeting its claim or doing better. We call this
event the null hypothesis and denote it H0. We call the event that μ is < 3 the
alternate hypothesis HA. If this event is true the manufacturer is cheating on the
quantity of coﬀee supplied. Our goal is to see if we can reject the null hypothesis,
and thereby suspect the coﬀee manufacturer of cheating. In summary, we have
H0 : μ ≥3
HA : μ < 3.
Now owing to Lemma 1, if μ = 3 then ¯X has density function
NormalDen(¯x; 3, (.3)2 /40).
This density function appears in Figure 2.2. Recall that our sample mean had
¯x = 2.92. We have that
2.92

−∞
NormalDen(¯x; 3, (.3)2 /40)d¯x = .046.

2 A Polemic for Bayesian Statistics
21
2.80
2.85
2.90
2.95
3.00
3.05
3.10
3.15
3.20
0
1
2
3
4
5
6
7
8
9
10
_
x
    _
U(x)
Area = .046
Fig. 2.2. The p-value at 2.92 is .046
This means that .046 of the area under the density function falls to the left of
2.92. This is illustrated in Figure 2.2. Frequentists call .046 the p-value for the
sample result, and they say we can reject H0 at the .046 signiﬁcance level. Their
explanation is that if we reject H0 whenever the p-value lies at .046 or to the
left of it, if H0 is true, we will reject it at most .046 fraction of the time. We say
‘at most’ because H0 is true if μ ≥3. We computed the p-value relative to 3.
The p-values relative to numbers greater than 3 will be smaller.
Often frequentists set a signiﬁcance level α before doing the study, and then
reject H0 at the α signiﬁcance level if the result falls to the left of the point ¯xα
determined by α. For example, if α = .05, ¯xα = 2.988. In the previous example
our result is ¯x = 2.92, which is to the left of 2.988. So we reject H0 at the
.05 signiﬁcance level. Regarding this practice, Brownlee [1965] says “The ﬁrst
criterion is that only in a small fraction α of the time shall we commit a Type
I error or an error of the ﬁrst kind, which is to reject the null hypothesis when
in fact it is true. Clearly we wish to commit such an error only rarely: common
choices for α are 0.05 and 0.01.” It has become credo among many practitioners
of frequentist statistics to deem a result signiﬁcant whenever the p-value is ≤.05.
We postpone further discussion of frequentist hypothesis testing until after
we show the Bayesian method.
2.3.2
Bayesian Hypothesis Testing
Recall that the Bayesian represents uncertainty concerning the unknown mean
with a random variable M that has density function
NormalDen(μ; a, 1/v).

22
R.E. Neapolitan
Then, if r = 1/σ2 where σ2 is the know variance, given a random sample of size n
and a value ¯x of the mean of the sample, the posterior density function of M is
ρ(μ|¯x) = NormalDen(μ; a∗, 1/v∗),
where
a∗= va + nr¯x
v + nr
and
v∗= v + nr.
Now suppose we have the following hypothesis:
H0 : μ ≥ξ
HA : μ < ξ.
Let d denote the information on which we are conditioning. That is, d = {n, ¯x}.
Then, due to the law of total probability, the posterior probability of the null
hypothesis H0 given d is given by
P(H0|d) =
∞

−∞
P(H0|d, μ) × ρ(μ|d)dμ
=
ξ

−∞
0 × ρ(μ|d)dμ +
∞

ξ
1 × ρ(μ|d)dμ
=
∞

ξ
NormalDen(μ; a∗, 1/v∗)dμ
Similarly,
P(HA|d) =
ξ

−∞
NormalDen(μ; a∗, 1/v∗)dμ.
Example 8. Suppose we have the situation described in Example 7. Then σ = .3,
n = 40, ¯x = 2.92, and
H0 : μ ≥3
HA : μ < 3.
If we assume prior ignorance as to the value of μ, then owing to Equality 2.11,
ρ(μ|d) = NormalDen(μ; ¯x, 1/nr)
= NormalDen(μ; ¯x, σ2/n)
= NormalDen(μ; 2.92, (.3)2 /40).
Therefore,
P(H0|d) =
∞

3
NormalDen(μ; 2.92, (.3)2 /40))dμ = .046

2 A Polemic for Bayesian Statistics
23
2.70
2.75
2.80
2.85
2.90
2.95
3.00
3.05
3.10
0
1
2
3
4
5
6
7
8
9
10
P
P(HA|d) = .954
P(H0|d) = .046
U(P)
Fig. 2.3. The posterior probability of H0 is the area to the right of the dotted line,
while that of HA is the area to the left
P(HA|d) =
3

−∞
NormalDen(μ; 2.92, (.3)2 /40))dμ = .954.
Notice in the previous example that P(H0|d) is equal to the p-value obtained
in Example 7. It is not hard to see that this is a general result. That is, if
we assume prior ignorance then the Bayesian’s posterior probability of H0 is
equal to the frequentist’s p-value. Furthermore, this result holds for the normal
distribution even if we do not assume we know the variance. However, in general
the two are usually close but not identical. The following example illustrates this.
Example 9. Suppose the new drug xhair is being introduced to prevent hair loss,
but it seems that it may have the unpleasant side eﬀect of acne. The drug
company wishes to investigate this possibility. Let
p1 = P(acne|xhair)
p2 = P(acne|⌝xhair),
where xhair denotes the event that one uses xhair, and ⌝xhair denotes the
event that one does not use it. To investigate whether xhair may cause acne, the
company formulates these hypotheses:
H0 : p1 ≤p2
HA : p1 > p2.
They wish to see if they need to reject H0 and admit a problem with their drug.
Suppose in a study consisting of 900 xhair users and 900 control subjects they
obtain these results:

24
R.E. Neapolitan
acne ⌝acne
xhair
34
866
⌝xhair 19
881
Then Fisher’s exact test for 2 × 2 tables yields this result:
p-value =
1
900+900
34+19

19

k=0
900
k

900
34 + 19 −k

= .0250.
To assume prior ignorance as to the values of p1 and p2, we assign the uniform
prior density functions as follows:
ρ(p1) = 1
for p1 ∈[0, 1]
ρ(p2) = 1
for p2 ∈[0, 1].
Given the data d in the above table and these priors, it is possible to show (See
[Neapolitan, 2004]) that the posterior density functions are given by
ρ(p1|d) = BetaDen(p1; 1 + 34, 1 + 866)
ρ(p2|d) = BetaDen(p2; 1 + 19, 1 + 881),
where BetaDen is the beta density function. We then have that
P(H0|d) = P(p1 ≤p2|d)
=
1

0
p2

0
BetaDen(p1, 35, 867) BetaDen(p2, 20, 882)dp1dp2
= .0186.
We see that the p-value and posterior probability are close but not equal.
2.3.3
Comparing the Two Approaches
Next we compare the two approaches to hypothesis testing by citing diﬃculties
with the frequentist approach and showing the Bayesian approach does not have
these diﬃculties.
Frequentist Results are Diﬃcult to Understand
Recall that Brownlee [1965] explained signiﬁcance level as follows: “The ﬁrst
criterion is that only in a small fraction α of the time shall we commit a Type
I error or an error of the ﬁrst kind, which is to reject the null hypothesis when
in fact it is true. Clearly we wish to commit such an error only rarely: common
choices for α are 0.05 and 0.01.” A Question immediately come to bear. Why do
we choose the far left region of the curve as our rejection interval? In Example 7
if we choose a region just to the right of 3 with area .05, Brownlee’s criterion

2 A Polemic for Bayesian Statistics
25
will still be satisﬁed. However, this makes no sense at all since, if the result
falls in this region, it seems to support H0 rather than discredit it. So further
explanation is required. Now we must consider errors of the second kind. Again
quoting [Brownlee, 1965] “To accept H0 when it is false is an error, known as an
error of the second kind or Type II error. The probability of the complementary
event, namely of correctly rejecting the null hypothesis, is know as the power
of the test, say π. The power of the test is thus equal to the probability of x
falling in the region of rejection when it is has the alternative distribution. ...
We would like to have a large power.” In Example 7 the alternative hypothesis
is composite consisting of all values < 3. So the situation is somewhat more
complex than than just choosing a region that attempts to minimize Type I
errors while maximizing power for a particular value. Further development is
necessary to justify the interval choice, and a theoretical frequentist text such as
[Brownlee, 1965] provides that justiﬁcation, although many of the practitioner
frequentist texts do not. Brieﬂy, if we let ρ(x : μ) denote the density function if
the value of the unknown mean is μ, then the Neyman-Pearson Lemma [Neyman
and Pearson, 1933] state that the rejection region should include those x for
which ρ(x : μ) is as large as possible for every μ in HA compared to ρ(x : μ′)
where μ′ is the endmost value in H0. The lemma states that such a region will
result in maximum power. The region is then found by maximizing a likelihood
ratio. In the case of Example 7, the region turns out to be the far left region
shown in Figure 2.2 (as our intuition tells us it should be). For every x in that
region and for every μ ∈HA, ρ(x : μ) > ρ(x : 3).
If the preceding explanation seems confusing, I suspect it is because the un-
derlying methodology is confusing. It is no wonder that many practitioners have
simply chosen to consider a result signiﬁcant if the p-value is below the enigmatic
.05 level.
The Bayesian’s life is so much easier. In Example 8, we conditioned on the
fact that n = 40 and ¯x = 2.92, and learned that the posterior probability of H0
was .046 and that of HA was .954. This result tells the whole story. There is
no need to talk about rejection regions, errors of the ﬁrst, errors of the second
kind, etc. When I ﬁrst encountered hypothesis testing as a college student, I was
somewhat ﬂummoxed by the concern for a region when in fact the result was a
point value. The Bayesian only conditions on this point value.
Frequentist Results Do Not Give a Conﬁdence Measure
The fact that frequentist methods are harder to understand should not in itself
preclude them. A more serious problem is that the p-value is not a measure of our
conﬁdence in the falsity of H0. That is, we may know that the rejection region is
unlikely given any value in H0 and it is more likely given any value in HA, but in
general we don’t have a measure of how much more likely it is. That is, a small
p-value does not rule out that the result could also be unlikely given HA. The
frequentist is not able to give us a numeric measure of how conﬁdent we should
be that H0 is false. The Bayesian does not have this problem. In Example 8,

26
R.E. Neapolitan
the posterior probability of H0 was .046 and that of HA was .954. We know
exactly how likely H0 is.
The situation is a little more complex when H0 consists of a point value μ0.
For example, H0 may be that the mean of a normal distribution is 3, while HA
is that the mean is not 3. Then the probability of H0 is 0 since the probability
of every point value is 0. In this case, we calculate a 100(1 −α)% posterior
probability interval for the unknown mean, and see if μ0 is in the interval. If
μ0 is not in the interval we reject H0 at the .05 probability level. This is the
standard Bayesian method for handling this situation. However, it seems it may
be a carryover of frequentist methodology. That is, in most (if not all) situations
it seems that no reasonable hypothesis could be that a parameter has a precise
value. In such situations the hypothesis could just be that the mean is close to
3. Then we can investigate the posterior probability of intervals centered at 3.
How small these intervals need be and how large the probabilities need be (to
not reject H0) would depend on the application.
2.3.4
The Multiple Hypothesis Paradox
Another diﬃculty with frequentist hypothesis testing is that it can lead to a prob-
lem which I call the multiple hypothesis paradox. I ﬁrst describe the paradox with
a classic example. Then I show a practical situation in which the problem arises,
and a frequentist eﬀort to solve the problem that leads to questionable results.
The Case of the Positive Blood Test
Suppose a woman is murdered and her husband is the primary suspect. Sup-
pose further that a very accurate blood test can be run to see if the husband’s

2 A Polemic for Bayesian Statistics
27
blood matches that found at the scene of the crime. If the test comes back posi-
tive, the jury will become almost certain the husband is the murderer because, if
the blood at the crime scene is not the husband’s blood, a very rare event would
have occurred. On the other hand, if we select 1, 000, 000 people in the city and
test their blood, it is probable that at least one’s blood will match that found
at the crime scene (even if the blood is not from any of them). So if one of their
blood matches that found at the crime scene, we will not become conﬁdent that
the person is the murderer. The husband, realizing this, can insist that they test
him along with 1, 000, 000 other people so that, if his test is positive, the jury
cannot be conﬁdent he is the murderer. I call this the multiple hypothesis
paradox for frequentist statistics.
Let’s formalize the previous discussion with two experiments:
Experiment 1
Assume the accuracy of the blood test is as follows:
P(match|blood) = 1
P(match|⌝blood) = .00001,
where blood is the event that the blood at the scene of the crime is blood from
the individual being tested, ⌝blood is the event that the blood at the scene of the
crime is not from the given individual, and match is the event that the blood at
the scene of the crime matches blood taken from the given individual. Let our
hypotheses be as follows:
H0 : The blood at the crime scene is not from the given individual
HA : The blood at the crime scene is from the given individual.
Then
P(match|H0) = .00001
P(match|HA) = 1.
The region of rejection is match, and the region of acceptance is ⌝match. So
the signiﬁcance level of the test is .00001, while the power is 1. If the husband’s
test comes back positive (event match occurs), we reject H0 at a very signiﬁcant
level and believe the husband is the murderer.
Experiment 2
We test the blood of 1, 000, 000 people (including the husband) to see if any of
their blood matches that found at the crime scene. Let our hypotheses be as
follows:
H0 : The blood at the crime scene is not from any of them
HA : The blood at the crime scene is from one of them.

28
R.E. Neapolitan
Then
P(any match|H0) = 1 −(1 −.00001)1000000 = .99995.
P(any match|HA) = 1,
where any match is the event that at least one person’s blood matches that at
the crime scene. The test has little signiﬁcance. So if the husband’s blood matches
we cannot reject H0 with any signiﬁcance, which means we cannot believe he is
the murderer.
How can the frequentist resolve this matter? Perhaps the frequentist could
make claims about the experimental setup. That is, our only purpose was to in-
vestigate the husband. So we should run an experiment that involves only him.
We should not involve individuals we do not suspect. But isn’t this implicitly
updating using the prior belief that the husband is the murderer, which frequen-
tists emphatically claim they do not do. The Bayesian makes this prior belief
explicit.
The Bayesian Method
The Bayesian can assign a prior belief to the hypothesis that the husband is
the murderer based on evidence so far analyzed. Let’s say we assign P(husband
murderer) = .1. For simplicity let’s also assume someone is the murderer if
and only if it is their blood that is at the crime scene. Then for the husband
P(blood) = .1. We then have
P(blood|match) =
P(match|blood)P(blood)
P(match|blood)P(blood) + P(match|⌝blood)P(⌝blood)
=
1(.1)
1(.1) + .00001(.9) = .9999.
If the blood matches the husband’s blood, we will become almost certain it is his
blood at the crime scene and therefore he is the murderer. You might ask how
we could obtain this prior probability of .1. Alternatively, we could determine
the posterior probability needed to believe the husband is guilty “beyond a
reasonable doubt.” Let’s say that probability is .999. Then we solve
1(p)
1(p) + .00001(1 −p) = .999
for p. The solution is p = .0099. So if the prior evidence entails that the chances
are at least 1 in 100 that he is the murderer, a match should be suﬃcient addi-
tional evidence to convict him.
Now what about an arbitrary person whose blood matches the crime scene
blood when that person is one of 1, 000, 000 people we test. Again it all depends
on the prior probability that this person is the murderer. If, for example, we
were checking all 1, 000, 000 in the city and we were certain that one of them is
the murderer, then this prior probability would be .000001, and we would have

2 A Polemic for Bayesian Statistics
29
P(blood|match) =
P(match|blood)P(blood)
P(match|blood)P(blood) + P(match|⌝blood)P(⌝blood)
=
1(.000001)
1(.000001) + .00001(.999999) = .091.
Note that this is the probability of the person being the murderer in the absence
of information about the test results of others. For example, in the scenario we
described, if all other results were negative, we would be certain this person is
the murderer.
Spatial Cluster Detection
Spatial cluster detection consists of detecting clusters of some event in subre-
gions of a region being analyzed. Practitioners in spatial cluster detection are
troubled by the multiple hypothesis paradox. Next I discuss their problem, and
a frequentist eﬀort to solve it. On page 16, Neill [2006] describes the problem in
the context of disease outbreak detection as follows:
Let us consider the example of disease surveillance, assuming that
we are given the count (number of disease cases) ci, as well as the ex-
pected count (mean μi and standard deviation σi), for each zip code si.
How can we tell whether any zip code has a number of cases that is
signiﬁcantly higher than expected? One simple possibility would be to
perform a separate statistical test for each zip code, and report all zip
codes that are signiﬁcant at some level α.. . . A second, and somewhat
more subtle, problem is that of multiple hypothesis testing. We typically
perform statistical tests to determine if an area is signiﬁcant at some
level α, such as α = .05, which means that if there is no abnormality
in that area (i.e. the “null hypothesis” of no clusters is true) our prob-
ability of a false alarm is at most α. A lower value of α results in less
false alarms, but also reduces our chance of detecting a true cluster. Now
let us imagine that we are searching for disease clusters in a large area
containing 1000 zip codes, and that there happen to be no outbreaks
today, so any areas we detect are false alarms. If we perform a separate
signiﬁcance test for each zip code, we expect to trigger an alarm with
probability α = 0.05. But because we are doing 1000 separate tests, our
expected number of false alarms is 1000 × 0.05 = 50. Moreover, if these
1000 tests were independent, we would expect to get at least one false
alarm with probability 1 −(1 −.05)1000 ≈1.. . . The main point here,
though, is that we are almost certain to get false alarms every day, and
the number of such false alarms is proportional to the number of tests
performed.
Later on page 20, Neill [2006] oﬀers the following frequentist solution to this
problem:
Once we have found the regions with the highest scores F(S), we
must still determine which of these “potential clusters” are likely to be

30
R.E. Neapolitan
“true clusters” resulting from a disease outbreak, and which are likely
to be due to be due to chance.. . . Because of the multiple hypothesis
testing problem discussed above, we cannot simply compute separately
whether each region score F(S) is signiﬁcant, because we would obtain
a large number of false positives, proportional to the number of regions
searched. Instead for each region S, we ask the question, “If this data
were generated under the null hypothesis H0, how likely would we be to
ﬁnd any regions with scores higher than F(S)?” To answer this question,
we use the method known as randomization testing: we randomly gener-
ate a large number of “replicas” under the null hypothesis, and compute
the maximum score F ∗= maxS F(S) of each replica.. . .
Once we have obtained F ∗for each replica, we can compute the
statistical signiﬁcance of any region S by comparing F(S) to these replica
values of F ∗.. . . The p-value of region S can be computed as Rbeat+1
R+1
,
where R is the total number of replicas created, and Rbeat is the number
of replicas with F ∗greater than F(S). If the p-value is less than our
signiﬁcance level α, we conclude that the region is not signiﬁcant (likely
to be due to chance).
Suppose epidemiologists decide to use this strategy to monitor for disease
outbreaks in Westchester, Il., a suburb of Chicago, and it works quite well.
That is, outbreaks are detected early enough that preventative measures can be
taken, while false positives are kept at an acceptable level. It works so well that
they decide to extend the system to monitor the entire Chicagoland area. Since
we now have a much larger region and we are committed to keeping the total
false positive rate (over the entire monitored region) under α, we will report
outbreaks in Westchester far less often. If we extend the monitored region to
all of Illinois, or even all of the United States, we will almost never report an
outbreak in Westchester. By the mere fact that we wanted to monitor a larger
region, we have messed up a system that was working in Westchester! The point
is that the threshold at which we report an outbreak in a given region should
be based on how early we feel outbreaks must be detected in the region verses
the number of false positives we would tolerate for the region. However, the
randomization testing technique discussed above entails that outbreak detection
in a given region depends on how many other regions we decide to monitor along
with the region, all due to our need to satisfy some arbitrary signiﬁcance level!
Neill et al. [2005] developed a Bayesian spatial scan statistic which does not do
randomization testing.
A Final Example
These considerations apply to many domains. For example, several individuals
have argued the following to me: “There is no reason to believe any one mu-
tual fund manager is better than another one. Sure, there are quite a few with
good records over the past 20 years; even ones who did not lose during the years

2 A Polemic for Bayesian Statistics
31
2000-2003. However, with all the fund managers out there by chance there should
be some with good records.” I personally have a prior belief that some managers
could be better than others. This seems true in sports, science, and other do-
mains. So I have no reason to believe it is not true in ﬁnance. Therefore, I have
the posterior belief that those with good track records are better and are likely
to do better in the future. Curiously, there are some who believe those with good
records are likely to do worse in the future because they are due for a fall. But
this is a another topic!
Acknowledgement. I would like to thank Sandy Zabell, Piotr Gmytrasiewicz, and
Scott Morris for reading this paper and providing useful criticisms and sugges-
tions. As a personal historical note, I was a frequentist up until the late 1980’s
as that was the only probability and statistics I knew. However, I was never
comfortable with the methods. At that time I fortuitously met Sandy Zabell
at the University of Chicago, and we subsequently spent many hours at a local
campus diner drinking coﬀee and discussing the foundations of probability and
statistics. I was eventually ‘converted’ to being a Bayesian. I say ‘converted’
because, as Sandy used to say, one cannot think clearly about probability un-
til one purges oneself of frequentist dogma and realizes probability is about
belief.
References
1. Anderson, D., Sweeny, D., Williams, T.: Statistics for Business and Economics,
South-Western, Mason, Ohio (2005)
2. Berry, D.: Statistics: A Bayesian Perspective, Duxbury, Belmont, CA (1996)
3. Bolstad, W.: Introduction to Bayesian Statistics. Wiley, NY (2004)
4. Brownlee, K.: Statistical Theory and Methodology. Wiley, NY (1965)
5. Buntine, W.: Learning Classiﬁcation Trees. Statistics and Computing 2 (1992)
6. de Finetti, B.: Foresight: Its Logical Laws, Its Subjective Source. In: Kyburg Jr.,
H.E., Smokler, H.E. (eds.) Studies in Subjective Probability. Wiley, NY (1964)
7. Fisher, R.A.: On the Probable Error of a Coeﬃcient of Correlation Deduced from
a Small Sample. Metron 1 (1921)
8. Fisher, R.A.: On the Mathematical Foundations of Theoretical Statistics. Philos.
Trans. Roy. Soc. London, Ser. A 222 (1922)
9. Fisher, R.E.: Inverse Probability. Proceedings of the Cambridge Philosophical So-
ciety 26 (1930)
10. Neapolitan, R.E.: Probabilistic Reasoning in Expert Systems. Wiley, NY (1990)
11. Neapolitan, R.E.: Learning Bayesian Networks. Prentice Hall, Upper Saddle River
(2004)
12. Neill, D.B.: Detection of Spatial and Spatio-Temporal Clusters, Ph.D. thesis, De-
partment of Computer Science, Carnegie Mellon University, Technical Report
CMU-CS-06-142 (2006)
13. Neill, D.B., Moore, A.W., Cooper, G.F.: A Bayesian Spatial Scan Statistic. Ad-
vances in Neural Information Processing Systems (NIPS) 18 (2005)
14. Neyman, J.: Outline of a Theory of Statistical Estimation Based on the Classical
Theory of Probability. Philos. Trans. Roy. Soc. London, Ser. A 236 (1937)

32
R.E. Neapolitan
15. Neyman, J., Pearson, E.S.: On the Problem of the Most Eﬃcient Type of Statistical
Hypotheses. Philos. Trans. Roy. Soc. London, Ser. A 231 (1933)
16. Pearl, J.: Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San
Mateo (1988)
17. Zabell, S.: R.A.Fisher and the Fiducial Argument. Statistical Sciences 7(3) (1992)

3
A Tutorial on Learning with Bayesian Networks⋆
David Heckerman
Microsoft Research
Advanced Technology Division
Microsoft Corporation
One Microsoft Way
Redmond WA, 98052
heckerma@microsoft.com
Abstract. A Bayesian network is a graphical model that encodes probabilistic rela-
tionships among variables of interest. When used in conjunction with statistical tech-
niques, the graphical model has several advantages for data analysis. One, because the
model encodes dependencies among all variables, it readily handles situations where
some data entries are missing. Two, a Bayesian network can be used to learn causal
relationships, and hence can be used to gain understanding about a problem domain
and to predict the consequences of intervention. Three, because the model has both
a causal and probabilistic semantics, it is an ideal representation for combining prior
knowledge (which often comes in causal form) and data. Four, Bayesian statistical
methods in conjunction with Bayesian networks oﬀer an eﬃcient and principled ap-
proach for avoiding the overﬁtting of data. In this paper, we discuss methods for con-
structing Bayesian networks from prior knowledge and summarize Bayesian statistical
methods for using data to improve these models. With regard to the latter task, we
describe methods for learning both the parameters and structure of a Bayesian net-
work, including techniques for learning with incomplete data. In addition, we relate
Bayesian-network methods for learning to techniques for supervised and unsupervised
learning. We illustrate the graphical-modeling approach using a real-world case study.
3.1
Introduction
A Bayesian network is a graphical model for probabilistic relationships among a
set of variables. Over the last decade, the Bayesian network has become a pop-
ular representation for encoding uncertain expert knowledge in expert systems
(Heckerman et al., 1995a). More recently, researchers have developed methods
for learning Bayesian networks from data. The techniques that have been de-
veloped are new and still evolving, but they have been shown to be remarkably
eﬀective for some data-analysis problems.
In this paper, we provide a tutorial on Bayesian networks and associated
Bayesian techniques for extracting and encoding knowledge from data. There
are numerous representations available for data analysis, including rule bases,
decision trees, and artiﬁcial neural networks; and there are many techniques for
⋆Re-printed with kind permission of MIT Press and Kluwer books.
D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 33–82, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

34
D. Heckerman
data analysis such as density estimation, classiﬁcation, regression, and clustering.
So what do Bayesian networks and Bayesian methods have to oﬀer? There are
at least four answers.
One, Bayesian networks can readily handle incomplete data sets. For example,
consider a classiﬁcation or regression problem where two of the explanatory or in-
putvariablesarestronglyanti-correlated.Thiscorrelationisnota problemforstan-
dardsupervisedlearning techniques,providedallinputsaremeasuredineverycase.
When one of the inputs is not observed, however, most models will produce an in-
accurate prediction, because they do not encode the correlation between the input
variables. Bayesian networks oﬀer a natural way to encode such dependencies.
Two, Bayesian networks allow one to learn about causal relationships. Learn-
ing about causal relationships are important for at least two reasons. The process
is useful when we are trying to gain understanding about a problem domain, for
example, during exploratory data analysis. In addition, knowledge of causal re-
lationships allows us to make predictions in the presence of interventions. For
example, a marketing analyst may want to know whether or not it is worthwhile
to increase exposure of a particular advertisement in order to increase the sales
of a product. To answer this question, the analyst can determine whether or not
the advertisement is a cause for increased sales, and to what degree. The use
of Bayesian networks helps to answer such questions even when no experiment
about the eﬀects of increased exposure is available.
Three, Bayesian networks in conjunction with Bayesian statistical techniques
facilitate the combination of domain knowledge and data. Anyone who has per-
formed a real-world analysis knows the importance of prior or domain knowledge,
especially when data is scarce or expensive. The fact that some commercial sys-
tems (i.e., expert systems) can be built from prior knowledge alone is a testament
to the power of prior knowledge. Bayesian networks have a causal semantics that
makes the encoding of causal prior knowledge particularly straightforward. In
addition, Bayesian networks encode the strength of causal relationships with
probabilities. Consequently, prior knowledge and data can be combined with
well-studied techniques from Bayesian statistics.
Four, Bayesian methods in conjunction with Bayesian networks and other
types of models oﬀers an eﬃcient and principled approach for avoiding the over
ﬁtting of data. As we shall see, there is no need to hold out some of the available
data for testing. Using the Bayesian approach, models can be “smoothed” in
such a way that all available data can be used for training.
This tutorial is organized as follows. In Section 3.2, we discuss the Bayesian
interpretation of probability and review methods from Bayesian statistics for
combining prior knowledge with data. In Section 3.3, we describe Bayesian net-
works and discuss how they can be constructed from prior knowledge alone.
In Section 3.4, we discuss algorithms for probabilistic inference in a Bayesian
network. In Sections 3.5 and 3.6, we show how to learn the probabilities in a
ﬁxed Bayesian-network structure, and describe techniques for handling incom-
plete data including Monte-Carlo methods and the Gaussian approximation.
In Sections 3.7 through 3.12, we show how to learn both the probabilities and

3 A Tutorial on Learning with Bayesian Networks
35
structure of a Bayesian network. Topics discussed include methods for assessing
priors for Bayesian-network structure and parameters, and methods for avoid-
ing the overﬁtting of data including Monte-Carlo, Laplace, BIC, and MDL
approximations. In Sections 3.13 and 3.14, we describe the relationships be-
tween Bayesian-network techniques and methods for supervised and unsuper-
vised learning. In Section 3.15, we show how Bayesian networks facilitate the
learning of causal relationships. In Section 3.16, we illustrate techniques dis-
cussed in the tutorial using a real-world case study. In Section 3.17, we give
pointers to software and additional literature.
3.2
The Bayesian Approach to Probability and Statistics
To understand Bayesian networks and associated learning techniques, it is impor-
tant to understand the Bayesian approach to probability and statistics. In this
section, we provide an introduction to the Bayesian approach for those readers
familiar only with the classical view.
In a nutshell, the Bayesian probability of an event x is a person’s degree of
belief in that event. Whereas a classical probability is a physical property of the
world (e.g., the probability that a coin will land heads), a Bayesian probability
is a property of the person who assigns the probability (e.g., your degree of
belief that the coin will land heads). To keep these two concepts of probability
distinct, we refer to the classical probability of an event as the true or physical
probability of that event, and refer to a degree of belief in an event as a Bayesian
or personal probability. Alternatively, when the meaning is clear, we refer to a
Bayesian probability simply as a probability.
One important diﬀerence between physical probability and personal probabil-
ity is that, to measure the latter, we do not need repeated trials. For example,
imagine the repeated tosses of a sugar cube onto a wet surface. Every time the
cube is tossed, its dimensions will change slightly. Thus, although the classical
statistician has a hard time measuring the probability that the cube will land
with a particular face up, the Bayesian simply restricts his or her attention to the
next toss, and assigns a probability. As another example, consider the question:
What is the probability that the Chicago Bulls will win the championship in
2001? Here, the classical statistician must remain silent, whereas the Bayesian
can assign a probability (and perhaps make a bit of money in the process).
One common criticism of the Bayesian deﬁnition of probability is that prob-
abilities seem arbitrary. Why should degrees of belief satisfy the rules of proba-
bility? On what scale should probabilities be measured? In particular, it makes
sense to assign a probability of one (zero) to an event that will (not) occur, but
what probabilities do we assign to beliefs that are not at the extremes? Not
surprisingly, these questions have been studied intensely.
With regards to the ﬁrst question, many researchers have suggested diﬀerent
sets of properties that should be satisﬁed by degrees of belief (e.g., Ramsey 1931,
Cox 1946, Good 1950, Savage 1954, DeFinetti 1970). It turns out that each set
of properties leads to the same rules: the rules of probability. Although each set

36
D. Heckerman
Fig. 3.1. The probability wheel: a tool for assessing probabilities
of properties is in itself compelling, the fact that diﬀerent sets all lead to the
rules of probability provides a particularly strong argument for using probability
to measure beliefs.
The answer to the question of scale follows from a simple observation: people
ﬁnd it fairly easy to say that two events are equally likely. For example, imagine
a simpliﬁed wheel of fortune having only two regions (shaded and not shaded),
such as the one illustrated in Figure 3.1. Assuming everything about the wheel
as symmetric (except for shading), you should conclude that it is equally likely
for the wheel to stop in any one position. From this judgment and the sum rule of
probability (probabilities of mutually exclusive and collectively exhaustive sum
to one), it follows that your probability that the wheel will stop in the shaded
region is the percent area of the wheel that is shaded (in this case, 0.3).
This probability wheel now provides a reference for measuring your probabili-
ties of other events. For example, what is your probability that Al Gore will run
on the Democratic ticket in 2000? First, ask yourself the question: Is it more
likely that Gore will run or that the wheel when spun will stop in the shaded
region? If you think that it is more likely that Gore will run, then imagine an-
other wheel where the shaded region is larger. If you think that it is more likely
that the wheel will stop in the shaded region, then imagine another wheel where
the shaded region is smaller. Now, repeat this process until you think that Gore
running and the wheel stopping in the shaded region are equally likely. At this
point, your probability that Gore will run is just the percent surface area of the
shaded area on the wheel.
In general, the process of measuring a degree of belief is commonly referred to
as a probability assessment. The technique for assessment that we have just de-
scribed is one of many available techniques discussed in the Management Science,
Operations Research, and Psychology literature. One problem with probability
assessment that is addressed in this literature is that of precision. Can one re-
ally say that his or her probability for event x is 0.601 and not 0.599? In most
cases, no. Nonetheless, in most cases, probabilities are used to make decisions,
and these decisions are not sensitive to small variations in probabilities. Well-
established practices of sensitivity analysis help one to know when additional
precision is unnecessary (e.g., Howard and Matheson, 1983). Another problem
with probability assessment is that of accuracy. For example, recent experiences
or the way a question is phrased can lead to assessments that do not reﬂect
a person’s true beliefs (Tversky and Kahneman, 1974). Methods for improving

3 A Tutorial on Learning with Bayesian Networks
37
accuracy can be found in the decision-analysis literature (e.g, Spetzler et al.
(1975)).
Now let us turn to the issue of learning with data. To illustrate the Bayesian
approach, consider a common thumbtack—one with a round, ﬂat head that can
be found in most supermarkets. If we throw the thumbtack up in the air, it
will come to rest either on its point (heads) or on its head (tails).1 Suppose we
ﬂip the thumbtack N + 1 times, making sure that the physical properties of the
thumbtack and the conditions under which it is ﬂipped remain stable over time.
From the ﬁrst N observations, we want to determine the probability of heads on
the N + 1th toss.
In the classical analysis of this problem, we assert that there is some physical
probability of heads, which is unknown. We estimate this physical probability
from the N observations using criteria such as low bias and low variance. We then
use this estimate as our probability for heads on the N +1th toss. In the Bayesian
approach, we also assert that there is some physical probability of heads, but we
encode our uncertainty about this physical probability using (Bayesian) proba-
bilities, and use the rules of probability to compute our probability of heads on
the N + 1th toss.2
To examine the Bayesian analysis of this problem, we need some notation. We
denote a variable by an upper-case letter (e.g., X, Y, Xi, Θ), and the state or value
of a corresponding variable by that same letter in lower case (e.g., x, y, xi, θ).
We denote a set of variables by a bold-face upper-case letter (e.g., X, Y, Xi).
We use a corresponding bold-face lower-case letter (e.g., x, y, xi) to denote an
assignment of state or value to each variable in a given set. We say that variable
set X is in conﬁguration x. We use p(X = x|ξ) (or p(x|ξ) as a shorthand) to
denote the probability that X = x of a person with state of information ξ. We
also use p(x|ξ) to denote the probability distribution for X (both mass functions
and density functions). Whether p(x|ξ) refers to a probability, a probability
density, or a probability distribution will be clear from context. We use this
notation for probability throughout the paper. A summary of all notation is
given at the end of the chapter.
Returning to the thumbtack problem, we deﬁne Θ to be a variable3 whose
values θ correspond to the possible true values of the physical probability. We
sometimes refer to θ as a parameter. We express the uncertainty about Θ using
the probability density function p(θ|ξ). In addition, we use Xl to denote the
variable representing the outcome of the lth ﬂip, l = 1, . . . , N+1, and D = {X1 =
x1, . . . , XN = xN} to denote the set of our observations. Thus, in Bayesian terms,
the thumbtack problem reduces to computing p(xN+1|D, ξ) from p(θ|ξ).
1 This example is taken from Howard (1970).
2 Strictly speaking, a probability belongs to a single person, not a collection of peo-
ple. Nonetheless, in parts of this discussion, we refer to “our” probability to avoid
awkward English.
3 Bayesians typically refer to Θ as an uncertain variable, because the value of Θ is
uncertain. In contrast, classical statisticians often refer to Θ as a random variable.
In this text, we refer to Θ and all uncertain/random variables simply as variables.

38
D. Heckerman
To do so, we ﬁrst use Bayes’ rule to obtain the probability distribution for Θ
given D and background knowledge ξ:
p(θ|D, ξ) = p(θ|ξ) p(D|θ, ξ)
p(D|ξ)
(3.1)
where
p(D|ξ) =

p(D|θ, ξ) p(θ|ξ) dθ
(3.2)
Next, we expand the term p(D|θ, ξ). Both Bayesians and classical statisticians
agree on this term: it is the likelihood function for binomial sampling. In partic-
ular, given the value of Θ, the observations in D are mutually independent, and
the probability of heads (tails) on any one observation is θ (1−θ). Consequently,
Equation 3.1 becomes
p(θ|D, ξ) = p(θ|ξ) θh (1 −θ)t
p(D|ξ)
(3.3)
where h and t are the number of heads and tails observed in D, respectively.
The probability distributions p(θ|ξ) and p(θ|D, ξ) are commonly referred to as
the prior and posterior for Θ, respectively. The quantities h and t are said to be
suﬃcient statistics for binomial sampling, because they provide a summarization
of the data that is suﬃcient to compute the posterior from the prior. Finally, we
average over the possible values of Θ (using the expansion rule of probability)
to determine the probability that the N + 1th toss of the thumbtack will come
up heads:
p(XN+1 = heads|D, ξ) =

p(XN+1 = heads|θ, ξ) p(θ|D, ξ) dθ
=

θ p(θ|D, ξ) dθ ≡Ep(θ|D,ξ)(θ)
(3.4)
where Ep(θ|D,ξ)(θ) denotes the expectation of θ with respect to the distribution
p(θ|D, ξ).
To complete the Bayesian story for this example, we need a method to assess
the prior distribution for Θ. A common approach, usually adopted for conve-
nience, is to assume that this distribution is a beta distribution:
p(θ|ξ) = Beta(θ|αh, αt) ≡
Γ(α)
Γ(αh)Γ(αt)θαh−1(1 −θ)αt−1
(3.5)
where αh > 0 and αt > 0 are the parameters of the beta distribution, α =
αh + αt, and Γ(·) is the Gamma function which satisﬁes Γ(x + 1) = xΓ(x) and
Γ(1) = 1. The quantities αh and αt are often referred to as hyperparameters to
distinguish them from the parameter θ. The hyperparameters αh and αt must
be greater than zero so that the distribution can be normalized. Examples of
beta distributions are shown in Figure 3.2.

3 A Tutorial on Learning with Bayesian Networks
39
Beta(3,2)
Beta(2,2)
Beta(1,1)
Beta(19,39)
Fig. 3.2. Several beta distributions
The beta prior is convenient for several reasons. By Equation 3.3, the posterior
distribution will also be a beta distribution:
p(θ|D, ξ) =
Γ(α + N)
Γ(αh + h)Γ(αt + t)θαh+h−1(1 −θ)αt+t−1 = Beta(θ|αh + h, αt + t)
(3.6)
We say that the set of beta distributions is a conjugate family of distributions for
binomial sampling. Also, the expectation of θ with respect to this distribution
has a simple form:

θ Beta(θ|αh, αt) dθ = αh
α
(3.7)
Hence, given a beta prior, we have a simple expression for the probability of
heads in the N + 1th toss:
p(XN+1 = heads|D, ξ) = αh + h
α + N
(3.8)
Assuming p(θ|ξ) is a beta distribution, it can be assessed in a number of
ways. For example, we can assess our probability for heads in the ﬁrst toss of
the thumbtack (e.g., using a probability wheel). Next, we can imagine having
seen the outcomes of k ﬂips, and reassess our probability for heads in the next
toss. From Equation 3.8, we have (for k = 1)
p(X1 = heads|ξ) =
αh
αh + αt
p(X2 = heads|X1 = heads, ξ) =
αh + 1
αh + αt + 1
Given these probabilities, we can solve for αh and αt. This assessment technique
is known as the method of imagined future data.
Another assessment method is based on Equation 3.6. This equation says that,
if we start with a Beta(0, 0) prior4 and observe αh heads and αt tails, then our
posterior (i.e., new prior) will be a Beta(αh, αt) distribution. Recognizing that
a Beta(0, 0) prior encodes a state of minimum information, we can assess αh
and αt by determining the (possibly fractional) number of observations of heads
and tails that is equivalent to our actual knowledge about ﬂipping thumbtacks.
Alternatively, we can assess p(X1 = heads|ξ) and α, which can be regarded as an
4 Technically, the hyperparameters of this prior should be small positive numbers so
that p(θ|ξ) can be normalized.

40
D. Heckerman
equivalent sample size for our current knowledge. This technique is known as the
method of equivalent samples. Other techniques for assessing beta distributions
are discussed by Winkler (1967) and Chaloner and Duncan (1983).
Although the beta prior is convenient, it is not accurate for some problems.
For example, suppose we think that the thumbtack may have been purchased at
a magic shop. In this case, a more appropriate prior may be a mixture of beta
distributions—for example,
p(θ|ξ) = 0.4 Beta(20, 1) + 0.4 Beta(1, 20) + 0.2 Beta(2, 2)
where 0.4 is our probability that the thumbtack is heavily weighted toward heads
(tails). In eﬀect, we have introduced an additional hidden or unobserved variable
H, whose states correspond to the three possibilities: (1) thumbtack is biased
toward heads, (2) thumbtack is biased toward tails, and (3) thumbtack is normal;
and we have asserted that θ conditioned on each state of H is a beta distribution.
In general, there are simple methods (e.g., the method of imagined future data)
for determining whether or not a beta prior is an accurate reﬂection of one’s
beliefs. In those cases where the beta prior is inaccurate, an accurate prior can
often be assessed by introducing additional hidden variables, as in this example.
So far, we have only considered observations drawn from a binomial distri-
bution. In general, observations may be drawn from any physical probability
distribution:
p(x|θ, ξ) = f(x, θ)
where f(x, θ) is the likelihood function with parameters θ. For purposes of this
discussion, we assume that the number of parameters is ﬁnite. As an exam-
ple, X may be a continuous variable and have a Gaussian physical probability
distribution with mean μ and variance v:
p(x|θ, ξ) = (2πv)−1/2 e−(x−μ)2/2v
where θ = {μ, v}.
Regardless of the functional form, we can learn about the parameters given
data using the Bayesian approach. As we have done in the binomial case, we
deﬁne variables corresponding to the unknown parameters, assign priors to these
variables, and use Bayes’ rule to update our beliefs about these parameters given
data:
p(θ|D, ξ) = p(D|θ, ξ) p(θ|ξ)
p(D|ξ)
(3.9)
We then average over the possible values of Θ to make predictions. For example,
p(xN+1|D, ξ) =

p(xN+1|θ, ξ) p(θ|D, ξ) dθ
(3.10)
For a class of distributions known as the exponential family, these computations
can be done eﬃciently and in closed form.5 Members of this class include the
5 Recent advances in Monte-Carlo methods have made it possible to work eﬃciently
with many distributions outside the exponential family. See, for example, Gilks
et al. (1996).

3 A Tutorial on Learning with Bayesian Networks
41
binomial, multinomial, normal, Gamma, Poisson, and multivariate-normal dis-
tributions. Each member of this family has suﬃcient statistics that are of ﬁxed
dimension for any random sample, and a simple conjugate prior.6 Bernardo and
Smith (pp. 436–442, 1994) have compiled the important quantities and Bayesian
computations for commonly used members of the exponential family. Here, we
summarize these items for multinomial sampling, which we use to illustrate many
of the ideas in this paper.
In multinomial sampling, the observed variable X is discrete, having r possible
states x1, . . . , xr. The likelihood function is given by
p(X = xk|θ, ξ) = θk,
k = 1, . . . , r
where θ = {θ2, . . . , θr} are the parameters. (The parameter θ1 is given by 1 −
r
k=2 θk.) In this case, as in the case of binomial sampling, the parameters
correspond to physical probabilities. The suﬃcient statistics for data set D =
{X1 = x1, . . . , XN = xN} are {N1, . . . , Nr}, where Ni is the number of times
X = xk in D. The simple conjugate prior used with multinomial sampling is the
Dirichlet distribution:
p(θ|ξ) = Dir(θ|α1, . . . , αr) ≡
Γ(α)
r
k=1 Γ(αk)
r

k=1
θαk−1
k
(3.11)
where α = r
i=1 αk, and αk > 0, k = 1, . . . , r. The posterior distribution
p(θ|D, ξ) = Dir(θ|α1+N1, . . . , αr+Nr). Techniques for assessing the beta distri-
bution, including the methods of imagined future data and equivalent samples,
can also be used to assess Dirichlet distributions. Given this conjugate prior and
data set D, the probability distribution for the next observation is given by
p(XN+1 = xk|D, ξ) =

θk Dir(θ|α1 + N1, . . . , αr + Nr) dθ = αk + Nk
α + N
(3.12)
As we shall see, another important quantity in Bayesian analysis is the marginal
likelihood or evidence p(D|ξ). In this case, we have
p(D|ξ) =
Γ(α)
Γ(α + N) ·
r

k=1
Γ(αk + Nk)
Γ(αk)
(3.13)
We note that the explicit mention of the state of knowledge ξ is useful, because
it reinforces the notion that probabilities are subjective. Nonetheless, once this
concept is ﬁrmly in place, the notation simply adds clutter. In the remainder of
this tutorial, we shall not mention ξ explicitly.
In closing this section, we emphasize that, although the Bayesian and classical
approaches may sometimes yield the same prediction, they are fundamentally
diﬀerent methods for learning from data. As an illustration, let us revisit the
6 In fact, except for a few, well-characterized exceptions, the exponential family is the
only class of distributions that have suﬃcient statistics of ﬁxed dimension (Koop-
man, 1936; Pitman, 1936).

42
D. Heckerman
thumbtack problem. Here, the Bayesian “estimate” for the physical probability
of heads is obtained in a manner that is essentially the opposite of the classical
approach.
Namely, in the classical approach, θ is ﬁxed (albeit unknown), and we imag-
ine all data sets of size N that may be generated by sampling from the binomial
distribution determined by θ. Each data set D will occur with some probability
p(D|θ) and will produce an estimate θ∗(D). To evaluate an estimator, we com-
pute the expectation and variance of the estimate with respect to all such data
sets:
Ep(D|θ)(θ∗) =

D
p(D|θ) θ∗(D)
Varp(D|θ)(θ∗) =

D
p(D|θ) (θ∗(D) −Ep(D|θ)(θ∗))2
(3.14)
We then choose an estimator that somehow balances the bias (θ −Ep(D|θ)(θ∗))
and variance of these estimates over the possible values for θ.7 Finally, we apply
this estimator to the data set that we actually observe. A commonly-used esti-
mator is the maximum-likelihood (ML) estimator, which selects the value of θ
that maximizes the likelihood p(D|θ). For binomial sampling, we have
θ∗
ML(D) =
Nk
r
k=1 Nk
For this (and other types) of sampling, the ML estimator is unbiased. That is,
for all values of θ, the ML estimator has zero bias. In addition, for all values of θ,
the variance of the ML estimator is no greater than that of any other unbiased
estimator (see, e.g., Schervish, 1995).
In contrast, in the Bayesian approach, D is ﬁxed, and we imagine all possible
values of θ from which this data set could have been generated. Given θ, the
“estimate” of the physical probability of heads is just θ itself. Nonetheless, we
are uncertain about θ, and so our ﬁnal estimate is the expectation of θ with
respect to our posterior beliefs about its value:
Ep(θ|D,ξ)(θ) =

θ p(θ|D, ξ) dθ
(3.15)
The expectations in Equations 3.14 and 3.15 are diﬀerent and, in many cases,
lead to diﬀerent “estimates”. One way to frame this diﬀerence is to say that the
classical and Bayesian approaches have diﬀerent deﬁnitions for what it means to
be a good estimator. Both solutions are “correct” in that they are self consistent.
Unfortunately, both methods have their drawbacks, which has lead to endless
debates about the merit of each approach. For example, Bayesians argue that
it does not make sense to consider the expectations in Equation 3.14, because
we only see a single data set. If we saw more than one data set, we should
7 Low bias and variance are not the only desirable properties of an estimator. Other
desirable properties include consistency and robustness.

3 A Tutorial on Learning with Bayesian Networks
43
combine them into one larger data set. In contrast, classical statisticians argue
that suﬃciently accurate priors can not be assessed in many situations. The
common view that seems to be emerging is that one should use whatever method
that is most sensible for the task at hand. We share this view, although we also
believe that the Bayesian approach has been under used, especially in light of its
advantages mentioned in the introduction (points three and four). Consequently,
in this paper, we concentrate on the Bayesian approach.
3.3
Bayesian Networks
So far, we have considered only simple problems with one or a few variables.
In real learning problems, however, we are typically interested in looking for
relationships among a large number of variables. The Bayesian network is a rep-
resentation suited to this task. It is a graphical model that eﬃciently encodes
the joint probability distribution (physical or Bayesian) for a large set of vari-
ables. In this section, we deﬁne a Bayesian network and show how one can be
constructed from prior knowledge.
A Bayesian network for a set of variables X = {X1, . . . , Xn} consists of (1)
a network structure S that encodes a set of conditional independence assertions
about variables in X, and (2) a set P of local probability distributions associ-
ated with each variable. Together, these components deﬁne the joint probability
distribution for X. The network structure S is a directed acyclic graph. The
nodes in S are in one-to-one correspondence with the variables X. We use Xi
to denote both the variable and its corresponding node, and Pai to denote the
parents of node Xi in S as well as the variables corresponding to those parents.
The lack of possible arcs in S encode conditional independencies. In particular,
given structure S, the joint probability distribution for X is given by
p(x) =
n

i=1
p(xi|pai)
(3.16)
The local probability distributions P are the distributions corresponding to the
terms in the product of Equation 3.16. Consequently, the pair (S, P) encodes
the joint distribution p(x).
The probabilities encoded by a Bayesian network may be Bayesian or physical.
When building Bayesian networks from prior knowledge alone, the probabilities
will be Bayesian. When learning these networks from data, the probabilities
will be physical (and their values may be uncertain). In subsequent sections,
we describe how we can learn the structure and probabilities of a Bayesian
network from data. In the remainder of this section, we explore the construction
of Bayesian networks from prior knowledge. As we shall see in Section 3.10, this
procedure can be useful in learning Bayesian networks as well.
To illustrate the process of building a Bayesian network, consider the problem
of detecting credit-card fraud. We begin by determining the variables to model.
One possible choice of variables for our problem is Fraud (F), Gas (G), Jewelry

44
D. Heckerman
(J), Age (A), and Sex (S), representing whether or not the current purchase
is fraudulent, whether or not there was a gas purchase in the last 24 hours,
whether or not there was a jewelry purchase in the last 24 hours, and the age
and sex of the card holder, respectively. The states of these variables are shown
in Figure 3.3. Of course, in a realistic problem, we would include many more
variables. Also, we could model the states of one or more of these variables at a
ﬁner level of detail. For example, we could let Age be a continuous variable.
This initial task is not always straightforward. As part of this task we must
(1) correctly identify the goals of modeling (e.g., prediction versus explanation
versus exploration), (2) identify many possible observations that may be relevant
to the problem, (3) determine what subset of those observations is worthwhile to
model, and (4) organize the observations into variables having mutually exclusive
and collectively exhaustive states. Diﬃculties here are not unique to modeling
with Bayesian networks, but rather are common to most approaches. Although
there are no clean solutions, some guidance is oﬀered by decision analysts (e.g.,
Howard and Matheson, 1983) and (when data are available) statisticians (e.g.,
Tukey, 1977).
In the next phase of Bayesian-network construction, we build a directed acyclic
graph that encodes assertions of conditional independence. One approach for do-
ing so is based on the following observations. From the chain rule of probability,
we have
p(x) =
n

i=1
p(xi|x1, . . . , xi−1)
(3.17)
Now, for every Xi, there will be some subset Πi ⊆{X1, . . . , Xi−1} such that
Xi and {X1, . . . , Xi−1} \ Πi are conditionally independent given Πi. That is, for
any x,
p(xi|x1, . . . , xi−1) = p(xi|πi)
(3.18)
Combining Equations 3.17 and 3.18, we obtain
p(x) =
n

i=1
p(xi|πi)
(3.19)
Comparing Equations 3.16 and 3.19, we see that the variables sets (Π1, . . . , Πn)
correspond to the Bayesian-network parents (Pa1, . . . , Pan), which in turn fully
specify the arcs in the network structure S.
Consequently, to determine the structure of a Bayesian network we (1) order
the variables somehow, and (2) determine the variables sets that satisfy Equa-
tion 3.18 for i = 1, . . . , n. In our example, using the ordering (F, A, S, G, J), we
have the conditional independencies
p(a|f) = p(a)
p(s|f, a) = p(s)
p(g|f, a, s) = p(g|f)
p(j|f, a, s, g) = p(j|f, a, s)
(3.20)

3 A Tutorial on Learning with Bayesian Networks
45
Fraud
Age
Gas
p(f=yes) = 0..00001
p(a=<30) = 0.25
p(a=30-50) = 0.40
p(j=yes|f=yes,a=*,s=*) = 0.05
p(j=yes|f=no,a=<30,s=male) = 0..0001
p(j=yes|f=no,a=30-50,s=male) = 0.0004
p(j=yes|f=no,a=>50,s=male) = 0.0002
p(j=yes|f=no,a=<30,s=female) = 0..0005
p(j=yes|f=no,a=30-50,s=female) = 0.002
p(j=yes|f=no,a=>50,s=female) = 0.001
p(g=yes|f=yes) = 0.2
p(g=yes|f=no) = 0.01
Sex
Jewelry
p(s=male) = 0.5
Fig. 3.3. A Bayesian-network for detecting credit-card fraud. Arcs are drawn from
cause to eﬀect. The local probability distribution(s) associated with a node are shown
adjacent to the node. An asterisk is a shorthand for “any state.”
Thus, we obtain the structure shown in Figure 3.3.
This approach has a serious drawback. If we choose the variable order care-
lessly, the resulting network structure may fail to reveal many conditional inde-
pendencies among the variables. For example, if we construct a Bayesian network
for the fraud problem using the ordering (J, G, S, A, F), we obtain a fully con-
nected network structure. Thus, in the worst case, we have to explore n! variable
orderings to ﬁnd the best one. Fortunately, there is another technique for con-
structing Bayesian networks that does not require an ordering. The approach
is based on two observations: (1) people can often readily assert causal rela-
tionships among variables, and (2) causal relationships typically correspond to
assertions of conditional dependence. In particular, to construct a Bayesian net-
work for a given set of variables, we simply draw arcs from cause variables to
their immediate eﬀects. In almost all cases, doing so results in a network struc-
ture that satisﬁes the deﬁnition Equation 3.16. For example, given the assertions
that Fraud is a direct cause of Gas, and Fraud, Age, and Sex are direct causes
of Jewelry, we obtain the network structure in Figure 3.3. The causal semantics
of Bayesian networks are in large part responsible for the success of Bayesian
networks as a representation for expert systems (Heckerman et al., 1995a). In
Section 3.15, we will see how to learn causal relationships from data using these
causal semantics.
In the ﬁnal step of constructing a Bayesian network, we assess the local
probability distribution(s) p(xi|pai). In our fraud example, where all variables
are discrete, we assess one distribution for Xi for every conﬁguration of Pai.
Example distributions are shown in Figure 3.3.

46
D. Heckerman
Note that, although we have described these construction steps as a simple
sequence, they are often intermingled in practice. For example, judgments of
conditional independence and/or cause and eﬀect can inﬂuence problem for-
mulation. Also, assessments of probability can lead to changes in the network
structure. Exercises that help one gain familiarity with the practice of building
Bayesian networks can be found in Jensen (1996).
3.4
Inference in a Bayesian Network
Once we have constructed a Bayesian network (from prior knowledge, data, or a
combination), we usually need to determine various probabilities of interest from
the model. For example, in our problem concerning fraud detection, we want to
know the probability of fraud given observations of the other variables. This
probability is not stored directly in the model, and hence needs to be computed.
In general, the computation of a probability of interest given a model is known
as probabilistic inference. In this section we describe probabilistic inference in
Bayesian networks.
Because a Bayesian network for X determines a joint probability distribution
for X, we can—in principle—use the Bayesian network to compute any prob-
ability of interest. For example, from the Bayesian network in Figure 3.3, the
probability of fraud given observations of the other variables can be computed
as follows:
p(f|a, s, g, j) = p(f, a, s, g, j)
p(a, s, g, j)
=
p(f, a, s, g, j)

f ′ p(f ′, a, s, g, j)
(3.21)
For problems with many variables, however, this direct approach is not prac-
tical. Fortunately, at least when all variables are discrete, we can exploit the
conditional independencies encoded in a Bayesian network to make this compu-
tation more eﬃcient. In our example, given the conditional independencies in
Equation 3.20, Equation 3.21 becomes
p(f|a, s, g, j) =
p(f)p(a)p(s)p(g|f)p(j|f, a, s)

f ′ p(f ′)p(a)p(s)p(g|f ′)p(j|f ′, a, s)
(3.22)
=
p(f)p(g|f)p(j|f, a, s)

f ′ p(f ′)p(g|f ′)p(j|f ′, a, s)
Several researchers have developed probabilistic inference algorithms for
Bayesian networks with discrete variables that exploit conditional independence
roughly as we have described, although with diﬀerent twists. For example,
Howard and Matheson (1981), Olmsted (1983), and Shachter (1988) developed
an algorithm that reverses arcs in the network structure until the answer to the
given probabilistic query can be read directly from the graph. In this algorithm,
each arc reversal corresponds to an application of Bayes’ theorem. Pearl (1986)
developed a message-passing scheme that updates the probability distributions
for each node in a Bayesian network in response to observations of one or more

3 A Tutorial on Learning with Bayesian Networks
47
variables. Lauritzen and Spiegelhalter (1988), Jensen et al. (1990), and Dawid
(1992) created an algorithm that ﬁrst transforms the Bayesian network into a
tree where each node in the tree corresponds to a subset of variables in X. The
algorithm then exploits several mathematical properties of this tree to perform
probabilistic inference. Most recently, D’Ambrosio (1991) developed an inference
algorithm that simpliﬁes sums and products symbolically, as in the transforma-
tion from Equation 3.21 to 3.22. The most commonly used algorithm for discrete
variables is that of Lauritzen and Spiegelhalter (1988), Jensen et al (1990), and
Dawid (1992).
Methods for exact inference in Bayesian networks that encode multivariate-
Gaussian or Gaussian-mixture distributions have been developed by Shachter
and Kenley (1989) and Lauritzen (1992), respectively. These methods also use
assertions of conditional independence to simplify inference. Approximate meth-
ods for inference in Bayesian networks with other distributions, such as the
generalized linear-regression model, have also been developed (Saul et al., 1996;
Jaakkola and Jordan, 1996).
Although we use conditional independence to simplify probabilistic inference,
exact inference in an arbitrary Bayesian network for discrete variables is NP-
hard (Cooper, 1990). Even approximate inference (for example, Monte-Carlo
methods) is NP-hard (Dagum and Luby, 1993). The source of the diﬃculty lies
in undirected cycles in the Bayesian-network structure—cycles in the structure
where we ignore the directionality of the arcs. (If we add an arc from Age to
Gas in the network structure of Figure 3.3, then we obtain a structure with one
undirected cycle: F −G−A−J−F.) When a Bayesian-network structure contains
many undirected cycles, inference is intractable. For many applications, however,
structures are simple enough (or can be simpliﬁed suﬃciently without sacriﬁcing
much accuracy) so that inference is eﬃcient. For those applications where generic
inference methods are impractical, researchers are developing techniques that are
custom tailored to particular network topologies (Heckerman 1989; Suermondt
and Cooper, 1991; Saul et al., 1996; Jaakkola and Jordan, 1996) or to particular
inference queries (Ramamurthi and Agogino, 1988; Shachter et al., 1990; Jensen
and Andersen, 1990; Darwiche and Provan, 1996).
3.5
Learning Probabilities in a Bayesian Network
In the next several sections, we show how to reﬁne the structure and local prob-
ability distributions of a Bayesian network given data. The result is set of tech-
niques for data analysis that combines prior knowledge with data to produce
improved knowledge. In this section, we consider the simplest version of this
problem: using data to update the probabilities of a given Bayesian network
structure.
Recall that, in the thumbtack problem, we do not learn the probability of
heads. Instead, we update our posterior distribution for the variable that repre-
sents the physical probability of heads. We follow the same approach for prob-
abilities in a Bayesian network. In particular, we assume—perhaps from causal

48
D. Heckerman
knowledge about the problem—that the physical joint probability distribution
for X can be encoded in some network structure S. We write
p(x|θs, Sh) =
n

i=1
p(xi|pai, θi, Sh)
(3.23)
where θi is the vector of parameters for the distribution p(xi|pai, θi, Sh), θs is
the vector of parameters (θ1, . . . , θn), and Sh denotes the event (or “hypothesis”
in statistics nomenclature) that the physical joint probability distribution can
be factored according to S.8 In addition, we assume that we have a random
sample D = {x1, . . . , xN} from the physical joint probability distribution of
X. We refer to an element xl of D as a case. As in Section 3.2, we encode
our uncertainty about the parameters θs by deﬁning a (vector-valued) variable
Θs, and assessing a prior probability density function p(θs|Sh). The problem of
learning probabilities in a Bayesian network can now be stated simply: Given a
random sample D, compute the posterior distribution p(θs|D, Sh).
We refer to the distribution p(xi|pai, θi, Sh), viewed as a function of θi, as a
local distribution function. Readers familiar with methods for supervised learning
will recognize that a local distribution function is nothing more than a proba-
bilistic classiﬁcation or regression function. Thus, a Bayesian network can be
viewed as a collection of probabilistic classiﬁcation/regression models, organized
by conditional-independence relationships. Examples of classiﬁcation/regression
models that produce probabilistic outputs include linear regression, generalized
linear regression, probabilistic neural networks (e.g., MacKay, 1992a, 1992b),
probabilistic decision trees (e.g., Buntine, 1993; Friedman and Goldszmidt,
1996), kernel density estimation methods (Book, 1994), and dictionary meth-
ods (Friedman, 1995). In principle, any of these forms can be used to learn
probabilities in a Bayesian network; and, in most cases, Bayesian techniques for
learning are available. Nonetheless, the most studied models include the unre-
stricted multinomial distribution (e.g., Cooper and Herskovits, 1992), linear re-
gression with Gaussian noise (e.g., Buntine, 1994; Heckerman and Geiger, 1996),
and generalized linear regression (e.g., MacKay, 1992a and 1992b; Neal, 1993;
and Saul et al., 1996).
In this tutorial, we illustrate the basic ideas for learning probabilities (and
structure) using the unrestricted multinomial distribution. In this case, each
variable Xi ∈X is discrete, having ri possible values x1
i , . . . , xri
i , and each local
distribution function is collection of multinomial distributions, one distribution
for each conﬁguration of Pai. Namely, we assume
p(xk
i |paj
i, θi, Sh) = θijk > 0
(3.24)
8 As deﬁned here, network-structure hypotheses overlap. For example, given X =
{X1, X2}, any joint distribution for X that can be factored according the network
structure containing no arc, can also be factored according to the network structure
X1 −→X2. Such overlap presents problems for model averaging, described in Sec-
tion 3.7. Therefore, we should add conditions to the deﬁnition to insure no overlap.
Heckerman and Geiger (1996) describe one such set of conditions.

3 A Tutorial on Learning with Bayesian Networks
49
where pa1
i , . . . , paqi
i
(qi = 
Xi∈Pai ri) denote the conﬁgurations of Pai, and
θi = ((θijk)ri
k=2)qi
j=1 are the parameters. (The parameter θij1 is given by 1 −
ri
k=2 θijk.) For convenience, we deﬁne the vector of parameters
θij = (θij2, . . . , θijri)
for all i and j. We use the term “unrestricted” to contrast this distribution
with multinomial distributions that are low-dimensional functions of Pai—for
example, the generalized linear-regression model.
Given this class of local distribution functions, we can compute the posterior
distribution p(θs|D, Sh) eﬃciently and in closed form under two assumptions.
The ﬁrst assumption is that there are no missing data in the random sample D.
We say that the random sample D is complete. The second assumption is that
the parameter vectors θij are mutually independent.9 That is,
p(θs|Sh) =
n

i=1
qi

j=1
p(θij|Sh)
We refer to this assumption, which was introduced by Spiegelhalter and Lau-
ritzen (1990), as parameter independence.
Given that the joint physical probability distribution factors according to
some network structure S, the assumption of parameter independence can itself
be represented by a larger Bayesian-network structure. For example, the network
structure in Figure 3.4 represents the assumption of parameter independence
for X = {X, Y } (X, Y binary) and the hypothesis that the network structure
X →Y encodes the physical joint probability distribution for X.
Under the assumptions of complete data and parameter independence, the
parameters remain independent given a random sample:
p(θs|D, Sh) =
n

i=1
qi

j=1
p(θij|D, Sh)
(3.25)
Thus, we can update each vector of parameters θij independently, just as
in the one-variable case. Assuming each vector θij has the prior distribution
Dir(θij|αij1, . . . , αijri), we obtain the posterior distribution
p(θij|D, Sh) = Dir(θij|αij1 + Nij1, . . . , αijri + Nijri)
(3.26)
where Nijk is the number of cases in D in which Xi = xk
i and Pai = paj
i.
As in the thumbtack example, we can average over the possible conﬁgu-
rations of θs to obtain predictions of interest. For example, let us compute
9 The computation is also straightforward if two or more parameters are equal. For
details, see Thiesson (1995).

50
D. Heckerman
sample  1
sample  2
Θy|x
Θx
Θy|x
X
Y
X
Y

Fig. 3.4. A Bayesian-network structure depicting the assumption of parameter inde-
pendence for learning the parameters of the network structure X →Y . Both variables
X and Y are binary. We use x and ¯x to denote the two states of X, and y and ¯y to
denote the two states of Y .
p(xN+1|D, Sh), where xN+1 is the next case to be seen after D. Suppose that,
in case xN+1, Xi = xk
i and Pai = paj
i, where k and j depend on i. Thus,
p(xN+1|D, Sh) = Ep(θs|D,Sh)
 n

i=1
θijk

To compute this expectation, we ﬁrst use the fact that the parameters remain
independent given D:
p(xN+1|D, Sh) =

n

i=1
θijk p(θs|D, Sh) dθs =
n

i=1

θijk p(θij|D, Sh) dθij
Then, we use Equation 3.12 to obtain
p(xN+1|D, Sh) =
n

i=1
αijk + Nijk
αij + Nij
(3.27)
where αij = ri
k=1 αijk and Nij = ri
k=1 Nijk.
These computations are simple because the unrestricted multinomial distribu-
tions are in the exponential family. Computations for linear regression with Gaus-
sian noise are equally straightforward (Buntine, 1994; Heckerman and Geiger,
1996).
3.6
Methods for Incomplete Data
Let us now discuss methods for learning about parameters when the random
sample is incomplete (i.e., some variables in some cases are not observed). An

3 A Tutorial on Learning with Bayesian Networks
51
important distinction concerning missing data is whether or not the absence of
an observation is dependent on the actual states of the variables. For example,
a missing datum in a drug study may indicate that a patient became too sick—
perhaps due to the side eﬀects of the drug—to continue in the study. In contrast,
if a variable is hidden (i.e., never observed in any case), then the absence of this
data is independent of state. Although Bayesian methods and graphical models
are suited to the analysis of both situations, methods for handling missing data
where absence is independent of state are simpler than those where absence and
state are dependent. In this tutorial, we concentrate on the simpler situation
only. Readers interested in the more complicated case should see Rubin (1978),
Robins (1986), and Pearl (1995).
Continuing with our example using unrestricted multinomial distributions,
suppose we observe a single incomplete case. Let Y ⊂X and Z ⊂X denote the
observed and unobserved variables in the case, respectively. Under the assump-
tion of parameter independence, we can compute the posterior distribution of
θij for network structure S as follows:
p(θij|y, Sh) =

z
p(z|y, Sh) p(θij|y, z, Sh)
(3.28)
= (1 −p(paj
i|y, Sh))
	
p(θij|Sh)

+
ri

k=1
p(xk
i , paj
i|y, Sh)

p(θij|xk
i , paj
i, Sh)

(See Spiegelhalter and Lauritzen (1990) for a derivation.) Each term in curly
brackets in Equation 3.28 is a Dirichlet distribution. Thus, unless both Xi and all
the variables in Pai are observed in case y, the posterior distribution of θij will
be a linear combination of Dirichlet distributions—that is, a Dirichlet mixture
with mixing coeﬃcients (1 −p(paj
i|y, Sh)) and p(xk
i , paj
i|y, Sh), k = 1, . . . , ri.
When we observe a second incomplete case, some or all of the Dirichlet com-
ponents in Equation 3.28 will again split into Dirichlet mixtures. That is, the
posterior distribution for θij we become a mixture of Dirichlet mixtures. As we
continue to observe incomplete cases, each missing values for Z, the posterior
distribution for θij will contain a number of components that is exponential
in the number of cases. In general, for any interesting set of local likelihoods
and priors, the exact computation of the posterior distribution for θs will be
intractable. Thus, we require an approximation for incomplete data.
3.6.1
Monte-Carlo Methods
One class of approximations is based on Monte-Carlo or sampling methods.
These approximations can be extremely accurate, provided one is willing to wait
long enough for the computations to converge.
In this section, we discuss one of many Monte-Carlo methods known as
Gibbs sampling, introduced by Geman and Geman (1984). Given variables
X = {X1, . . . , Xn} with some joint distribution p(x), we can use a Gibbs sam-
pler to approximate the expectation of a function f(x) with respect to p(x) as

52
D. Heckerman
follows. First, we choose an initial state for each of the variables in X somehow
(e.g., at random). Next, we pick some variable Xi, unassign its current state,
and compute its probability distribution given the states of the other n −1 vari-
ables. Then, we sample a state for Xi based on this probability distribution, and
compute f(x). Finally, we iterate the previous two steps, keeping track of the
average value of f(x). In the limit, as the number of cases approach inﬁnity,
this average is equal to Ep(x)(f(x)) provided two conditions are met. First, the
Gibbs sampler must be irreducible: The probability distribution p(x) must be
such that we can eventually sample any possible conﬁguration of X given any
possible initial conﬁguration of X. For example, if p(x) contains no zero prob-
abilities, then the Gibbs sampler will be irreducible. Second, each Xi must be
chosen inﬁnitely often. In practice, an algorithm for deterministically rotating
through the variables is typically used. Introductions to Gibbs sampling and
other Monte-Carlo methods—including methods for initialization and a discus-
sion of convergence—are given by Neal (1993) and Madigan and York (1995).
To illustrate Gibbs sampling, let us approximate the probability density
p(θs|D, Sh) for some particular conﬁguration of θs, given an incomplete data set
D = {y1, . . . , yN} and a Bayesian network for discrete variables with indepen-
dent Dirichlet priors. To approximate p(θs|D, Sh), we ﬁrst initialize the states of
the unobserved variables in each case somehow. As a result, we have a complete
random sample Dc. Second, we choose some variable Xil (variable Xi in case
l) that is not observed in the original random sample D, and reassign its state
according to the probability distribution
p(x′
il|Dc \ xil, Sh) =
p(x′
il, Dc \ xil|Sh)

x′′
il p(x′′
il, Dc \ xil|Sh)
where Dc \ xil denotes the data set Dc with observation xil removed, and the
sum in the denominator runs over all states of variable Xil. As we shall see
in Section 3.7, the terms in the numerator and denominator can be computed
eﬃciently (see Equation 3.35). Third, we repeat this reassignment for all unob-
served variables in D, producing a new complete random sample D′
c. Fourth,
we compute the posterior density p(θs|D′
c, Sh) as described in Equations 3.25
and 3.26. Finally, we iterate the previous three steps, and use the average of
p(θs|D′
c, Sh) as our approximation.
3.6.2
The Gaussian Approximation
Monte-Carlo methods yield accurate results, but they are often intractable—for
example, when the sample size is large. Another approximation that is more eﬃ-
cient than Monte-Carlo methods and often accurate for relatively large samples
is the Gaussian approximation (e.g., Kass et al., 1988; Kass and Raftery, 1995).
The idea behind this approximation is that, for large amounts of data,
p(θs|D, Sh) ∝p(D|θs, Sh)·p(θs|Sh) can often be approximated as a multivariate-
Gaussian distribution.

3 A Tutorial on Learning with Bayesian Networks
53
In particular, let
g(θs) ≡log(p(D|θs, Sh) · p(θs|Sh))
(3.29)
Also, deﬁne ˜θs to be the conﬁguration of θs that maximizes g(θs). This conﬁg-
uration also maximizes p(θs|D, Sh), and is known as the maximum a posteriori
(MAP) conﬁguration of θs. Using a second degree Taylor polynomial of g(θs)
about the ˜θs to approximate g(θs), we obtain
g(θs) ≈g( ˜θs) −1
2(θs −˜θs)A(θs −˜θs)t
(3.30)
where (θs −˜θs)t is the transpose of row vector (θs −˜θs), and A is the negative
Hessian of g(θs) evaluated at ˜θs. Raising g(θs) to the power of e and using
Equation 3.29, we obtain
p(θs|D, Sh) ∝p(D|θs, Sh) p(θs|Sh)
(3.31)
≈p(D| ˜θs, Sh) p( ˜θs|Sh) exp{−1
2(θs −˜θs)A(θs −˜θs)t}
Hence, p(θs|D, Sh) is approximately Gaussian.
To compute the Gaussian approximation, we must compute ˜θs as well as the
negative Hessian of g(θs) evaluated at ˜θs. In the following section, we discuss
methods for ﬁnding ˜θs. Meng and Rubin (1991) describe a numerical technique
for computing the second derivatives. Raftery (1995) shows how to approxi-
mate the Hessian using likelihood-ratio tests that are available in many statis-
tical packages. Thiesson (1995) demonstrates that, for unrestricted multinomial
distributions, the second derivatives can be computed using Bayesian-network
inference.
3.6.3
The MAP and ML Approximations and the EM Algorithm
As the sample size of the data increases, the Gaussian peak will become sharper,
tending to a delta function at the MAP conﬁguration ˜θs. In this limit, we do not
need to compute averages or expectations. Instead, we simply make predictions
based on the MAP conﬁguration.
A further approximation is based on the observation that, as the sample size
increases, the eﬀect of the prior p(θs|Sh) diminishes. Thus, we can approximate
˜θs by the maximum maximum likelihood (ML) conﬁguration of θs:
ˆθs = arg max
θs
	
p(D|θs, Sh)

One class of techniques for ﬁnding a ML or MAP is gradient-based optimiza-
tion. For example, we can use gradient ascent, where we follow the derivatives of
g(θs) or the likelihood p(D|θs, Sh) to a local maximum. Russell et al. (1995)
and Thiesson (1995) show how to compute the derivatives of the likelihood
for a Bayesian network with unrestricted multinomial distributions. Buntine

54
D. Heckerman
(1994) discusses the more general case where the likelihood function comes from
the exponential family. Of course, these gradient-based methods ﬁnd only local
maxima.
Another technique for ﬁnding a local ML or MAP is the expectation–
maximization (EM) algorithm (Dempster et al., 1977). To ﬁnd a local MAP
or ML, we begin by assigning a conﬁguration to θs somehow (e.g., at random).
Next, we compute the expected suﬃcient statistics for a complete data set, where
expectation is taken with respect to the joint distribution for X conditioned on
the assigned conﬁguration of θs and the known data D. In our discrete example,
we compute
Ep(x|D,θs,Sh)(Nijk) =
N

l=1
p(xk
i , paj
i|yl, θs, Sh)
(3.32)
where yl is the possibly incomplete lth case in D. When Xi and all the vari-
ables in Pai are observed in case xl, the term for this case requires a trivial
computation: it is either zero or one. Otherwise, we can use any Bayesian net-
work inference algorithm to evaluate the term. This computation is called the
expectation step of the EM algorithm.
Next, we use the expected suﬃcient statistics as if they were actual suﬃcient
statistics from a complete random sample Dc. If we are doing an ML calculation,
then we determine the conﬁguration of θs that maximize p(Dc|θs, Sh). In our
discrete example, we have
θijk =
Ep(x|D,θs,Sh)(Nijk)
ri
k=1 Ep(x|D,θs,Sh)(Nijk)
If we are doing a MAP calculation, then we determine the conﬁguration of θs
that maximizes p(θs|Dc, Sh). In our discrete example, we have10
θijk =
αijk + Ep(x|D,θs,Sh)(Nijk)
ri
k=1(αijk + Ep(x|D,θs,Sh)(Nijk))
This assignment is called the maximization step of the EM algorithm. Dempster
et al. (1977) showed that, under certain regularity conditions, iteration of the
expectation and maximization steps will converge to a local maximum. The EM
10 The MAP conﬁguration ˜θs depends on the coordinate system in which the pa-
rameter variables are expressed. The expression for the MAP conﬁguration given
here is obtained by the following procedure. First, we transform each variable set
θij = (θij2, . . . , θijri) to the new coordinate system φij = (φij2, . . . , φijri), where
φijk = log(θijk/θij1), k = 2, . . . , ri. This coordinate system, which we denote by φs,
is sometimes referred to as the canonical coordinate system for the multinomial dis-
tribution (see, e.g., Bernardo and Smith, 1994, pp. 199–202). Next, we determine the
conﬁguration of φs that maximizes p(φs|Dc, Sh). Finally, we transform this MAP
conﬁguration to the original coordinate system. Using the MAP conﬁguration cor-
responding to the coordinate system φs has several advantages, which are discussed
in Thiesson (1995b) and MacKay (1996).

3 A Tutorial on Learning with Bayesian Networks
55
algorithm is typically applied when suﬃcient statistics exist (i.e., when local
distribution functions are in the exponential family), although generalizations of
the EM algroithm have been used for more complicated local distributions (see,
e.g., Saul et al. 1996).
3.7
Learning Parameters and Structure
Now we consider the problem of learning about both the structure and proba-
bilities of a Bayesian network given data.
Assuming we think structure can be improved, we must be uncertain about
the network structure that encodes the physical joint probability distribution
for X. Following the Bayesian approach, we encode this uncertainty by deﬁning
a (discrete) variable whose states correspond to the possible network-structure
hypotheses Sh, and assessing the probabilities p(Sh). Then, given a random
sample D from the physical probability distribution for X, we compute the
posterior distribution p(Sh|D) and the posterior distributions p(θs|D, Sh), and
use these distributions in turn to compute expectations of interest. For example,
to predict the next case after seeing D, we compute
p(xN+1|D) =

Sh
p(Sh|D)

p(xN+1|θs, Sh) p(θs|D, Sh) dθs
(3.33)
In performing the sum, we assume that the network-structure hypotheses are
mutually exclusive. We return to this point in Section 3.9.
The computation of p(θs|D, Sh) is as we have described in the previous two
sections. The computation of p(Sh|D) is also straightforward, at least in princi-
ple. From Bayes’ theorem, we have
p(Sh|D) = p(Sh) p(D|Sh)/p(D)
(3.34)
where p(D) is a normalization constant that does not depend upon structure.
Thus, to determine the posterior distribution for network structures, we need to
compute the marginal likelihood of the data (p(D|Sh)) for each possible structure.
We discuss the computation of marginal likelihoods in detail in Section 3.9. As
an introduction, consider our example with unrestricted multinomial distribu-
tions, parameter independence, Dirichlet priors, and complete data. As we have
discussed, when there are no missing data, each parameter vector θij is updated
independently. In eﬀect, we have a separate multi-sided thumbtack problem for
every i and j. Consequently, the marginal likelihood of the data is the just the
product of the marginal likelihoods for each i–j pair (given by Equation 3.13):
p(D|Sh) =
n

i=1
qi

j=1
Γ(αij)
Γ(αij + Nij) ·
ri

k=1
Γ(αijk + Nijk)
Γ(αijk)
(3.35)
This formula was ﬁrst derived by Cooper and Herskovits (1992).

56
D. Heckerman
Unfortunately, the full Bayesian approach that we have described is often
impractical. One important computation bottleneck is produced by the average
over models in Equation 3.33. If we consider Bayesian-network models with n
variables, the number of possible structure hypotheses is more than exponential
in n. Consequently, in situations where the user can not exclude almost all of
these hypotheses, the approach is intractable.
Statisticians, who have been confronted by this problem for decades in the
context of other types of models, use two approaches to address this problem:
model selection and selective model averaging. The former approach is to select
a “good” model (i.e., structure hypothesis) from among all possible models, and
use it as if it were the correct model. The latter approach is to select a manage-
able number of good models from among all possible models and pretend that
these models are exhaustive. These related approaches raise several important
questions. In particular, do these approaches yield accurate results when applied
to Bayesian-network structures? If so, how do we search for good models? And
how do we decide whether or not a model is “good”?
The question of accuracy is diﬃcult to answer in theory. Nonetheless, sev-
eral researchers have shown experimentally that the selection of a single good
hypothesis often yields accurate predictions (Cooper and Herskovits 1992; Alif-
eris and Cooper 1994; Heckerman et al., 1995b) and that model averaging using
Monte-Carlo methods can sometimes be eﬃcient and yield even better predic-
tions (Madigan et al., 1996). These results are somewhat surprising, and are
largely responsible for the great deal of recent interest in learning with Bayesian
networks. In Sections 3.8 through 3.10, we consider diﬀerent deﬁnitions of what
is means for a model to be “good”, and discuss the computations entailed by
some of these deﬁnitions. In Section 3.11, we discuss model search.
We note that model averaging and model selection lead to models that gener-
alize well to new data. That is, these techniques help us to avoid the overﬁtting
of data. As is suggested by Equation 3.33, Bayesian methods for model averaging
and model selection are eﬃcient in the sense that all cases in D can be used to
both smooth and train the model. As we shall see in the following two sections,
this advantage holds true for the Bayesian approach in general.
3.8
Criteria for Model Selection
Most of the literature on learning with Bayesian networks is concerned with
model selection. In these approaches, some criterion is used to measure the de-
gree to which a network structure (equivalence class) ﬁts the prior knowledge
and data. A search algorithm is then used to ﬁnd an equivalence class that re-
ceives a high score by this criterion. Selective model averaging is more complex,
because it is often advantageous to identify network structures that are signif-
icantly diﬀerent. In many cases, a single criterion is unlikely to identify such
complementary network structures. In this section, we discuss criteria for the
simpler problem of model selection. For a discussion of selective model averag-
ing, see Madigan and Raftery (1994).

3 A Tutorial on Learning with Bayesian Networks
57
3.8.1
Relative Posterior Probability
A criterion that is often used for model selection is the log of the relative posterior
probability log p(D, Sh) = log p(Sh) + log p(D|Sh).11 The logarithm is used for
numerical convenience. This criterion has two components: the log prior and
the log marginal likelihood. In Section 3.9, we examine the computation of the
log marginal likelihood. In Section 3.10.2, we discuss the assessment of network-
structure priors. Note that our comments about these terms are also relevant to
the full Bayesian approach.
The log marginal likelihood has the following interesting interpretation de-
scribed by Dawid (1984). From the chain rule of probability, we have
log p(D|Sh) =
N

l=1
log p(xl|x1, . . . , xl−1, Sh)
(3.36)
The term p(xl|x1, . . . , xl−1, Sh) is the prediction for xl made by model Sh after
averaging over its parameters. The log of this term can be thought of as the utility
or reward for this prediction under the utility function log p(x).12 Thus, a model
with the highest log marginal likelihood (or the highest posterior probability,
assuming equal priors on structure) is also a model that is the best sequential
predictor of the data D under the log utility function.
Dawid (1984) also notes the relationship between this criterion and cross
validation. When using one form of cross validation, known as leave-one-out
cross validation, we ﬁrst train a model on all but one of the cases in the random
sample—say, Vl = {x1, . . . , xl−1, xl+1, . . . , xN}. Then, we predict the omitted
case, and reward this prediction under some utility function. Finally, we repeat
this procedure for every case in the random sample, and sum the rewards for each
prediction. If the prediction is probabilistic and the utility function is log p(x),
we obtain the cross-validation criterion
CV(Sh, D) =
N

l=1
log p(xl|Vl, Sh)
(3.37)
which is similar to Equation 3.36. One problem with this criterion is that training
and test cases are interchanged. For example, when we compute p(x1|V1, Sh) in
Equation 3.37, we use x2 for training and x1 for testing. Whereas, when we com-
pute p(x2|V2, Sh), we use x1 for training and x2 for testing. Such interchanges
can lead to the selection of a model that over ﬁts the data (Dawid, 1984). Var-
ious approaches for attenuating this problem have been described, but we see
from Equation 3.36 that the log-marginal-likelihood criterion avoids the problem
11 An
equivalent
criterion
that
is
often
used
is
log(p(Sh|D)/p(Sh
0 |D))
=
log(p(Sh)/p(Sh
0 )) + log(p(D|Sh)/p(D|Sh
0 )). The ratio p(D|Sh)/p(D|Sh
0 ) is known
as a Bayes’ factor.
12 This utility function is known as a proper scoring rule, because its use encourages
people to assess their true probabilities. For a characterization of proper scoring
rules and this rule in particular, see Bernardo (1979).

58
D. Heckerman
altogether. Namely, when using this criterion, we never interchange training and
test cases.
3.8.2
Local Criteria
Consider the problem of diagnosing an ailment given the observation of a set
of ﬁndings. Suppose that the set of ailments under consideration are mutually
exclusive and collectively exhaustive, so that we may represent these ailments
using a single variable A. A possible Bayesian network for this classiﬁcation
problem is shown in Figure 3.5.
The posterior-probability criterion is global in the sense that it is equally
sensitive to all possible dependencies. In the diagnosis problem, the posterior-
probability criterion is just as sensitive to dependencies among the ﬁnding vari-
ables as it is to dependencies between ailment and ﬁndings. Assuming that we
observe all (or perhaps all but a few) of the ﬁndings in D, a more reasonable
criterion would be local in the sense that it ignores dependencies among ﬁndings
and is sensitive only to the dependencies among the ailment and ﬁndings. This
observation applies to all classiﬁcation and regression problems with complete
data.
One such local criterion, suggested by Spiegelhalter et al. (1993), is a variation
on the sequential log-marginal-likelihood criterion:
LC(Sh, D) =
N

l=1
log p(al|Fl, Dl, Sh)
(3.38)
where al and Fl denote the observation of the ailment A and ﬁndings F in the
lth case, respectively. In other words, to compute the lth term in the product,
we train our model S with the ﬁrst l −1 cases, and then determine how well it
predicts the ailment given the ﬁndings in the lth case. We can view this criterion,
like the log-marginal-likelihood, as a form of cross validation where training and
test cases are never interchanged.
The log utility function has interesting theoretical properties, but it is some-
times inaccurate for real-world problems. In general, an appropriate reward or
utility function will depend on the decision-making problem or problems to which
Finding 1
Ailment
Finding 2
Finding n
. . .
Fig. 3.5. A Bayesian-network structure for medical diagnosis

3 A Tutorial on Learning with Bayesian Networks
59
the probabilistic models are applied. Howard and Matheson (1983) have collected
a series of articles describing how to construct utility models for speciﬁc decision
problems. Once we construct such utility models, we can use suitably modiﬁed
forms of Equation 3.38 for model selection.
3.9
Computation of the Marginal Likelihood
As mentioned, an often-used criterion for model selection is the log relative
posterior probability log p(D, Sh) = log p(Sh) + log p(D|Sh). In this section,
we discuss the computation of the second component of this criterion: the log
marginal likelihood.
Given (1) local distribution functions in the exponential family, (2) mutual in-
dependence of the parameters θi, (3) conjugate priors for these parameters, and
(4) complete data, the log marginal likelihood can be computed eﬃciently and in
closed form. Equation 3.35 is an example for unrestricted multinomial distribu-
tions. Buntine (1994) and Heckerman and Geiger (1996) discuss the computation
for other local distribution functions. Here, we concentrate on approximations
for incomplete data.
The Monte-Carlo and Gaussian approximations for learning about parameters
that we discussed in Section 3.6 are also useful for computing the marginal
likelihood given incomplete data. One Monte-Carlo approach, described by Chib
(1995) and Raftery (1996), uses Bayes’ theorem:
p(D|Sh) = p(θs|Sh) p(D|θs, Sh)
p(θs|D, Sh)
(3.39)
For any conﬁguration of θs, the prior term in the numerator can be evaluated
directly. In addition, the likelihood term in the numerator can be computed using
Bayesian-network inference. Finally, the posterior term in the denominator can
be computed using Gibbs sampling, as we described in Section 3.6.1. Other, more
sophisticated Monte-Carlo methods are described by DiCiccio et al. (1995).
As we have discussed, Monte-Carlo methods are accurate but computationally
ineﬃcient, especially for large databases. In contrast, methods based on the
Gaussian approximation are more eﬃcient, and can be as accurate as Monte-
Carlo methods on large data sets.
Recall that, for large amounts of data, p(D|θs, Sh) · p(θs|Sh) can often be
approximated as a multivariate-Gaussian distribution. Consequently,
p(D|Sh) =

p(D|θs, Sh) p(θs|Sh) dθs
(3.40)
can be evaluated in closed form. In particular, substituting Equation 3.31 into
Equation 3.40, integrating, and taking the logarithm of the result, we obtain the
approximation:
log p(D|Sh) ≈log p(D| ˜θs, Sh) + log p( ˜θs|Sh) + d
2 log(2π) −1
2 log |A|
(3.41)

60
D. Heckerman
where d is the dimension of g(θs). For a Bayesian network with unrestricted
multinomial distributions, this dimension is typically given by n
i=1 qi(ri −1).
Sometimes, when there are hidden variables, this dimension is lower. See Geiger
et al. (1996) for a discussion of this point.
This approximation technique for integration is known as Laplace’s method,
and we refer to Equation 3.41 as the Laplace approximation. Kass et al. (1988)
have shown that, under certain regularity conditions, relative errors in this ap-
proximation are O(1/N), where N is the number of cases in D. Thus, the Laplace
approximation can be extremely accurate. For more detailed discussions of this
approximation, see—for example—Kass et al. (1988) and Kass and Raftery
(1995).
Although Laplace’s approximation is eﬃcient relative to Monte-Carlo ap-
proaches, the computation of |A| is nevertheless intensive for large-dimension
models. One simpliﬁcation is to approximate |A| using only the diagonal ele-
ments of the Hessian A. Although in so doing, we incorrectly impose indepen-
dencies among the parameters, researchers have shown that the approximation
can be accurate in some circumstances (see, e.g., Becker and Le Cun, 1989,
and Chickering and Heckerman, 1996). Another eﬃcient variant of Laplace’s ap-
proximation is described by Cheeseman and Stutz (1995), who use the approx-
imation in the AutoClass program for data clustering (see also Chickering and
Heckerman, 1996.)
We obtain a very eﬃcient (but less accurate) approximation by retaining
only those terms in Equation 3.41 that increase with N: log p(D| ˜θs, Sh), which
increases linearly with N, and log |A|, which increases as d log N. Also, for large
N, ˜θs can be approximated by the ML conﬁguration of θs. Thus, we obtain
log p(D|Sh) ≈log p(D| ˆθs, Sh) −d
2 log N
(3.42)
This approximation is called the Bayesian information criterion (BIC), and was
ﬁrst derived by Schwarz (1978).
The BIC approximation is interesting in several respects. First, it does not de-
pend on the prior. Consequently, we can use the approximation without assessing
a prior.13 Second, the approximation is quite intuitive. Namely, it contains a term
measuring how well the parameterized model predicts the data (log p(D| ˆθs, Sh))
and a term that punishes the complexity of the model (d/2 logN). Third,
the BIC approximation is exactly minus the Minimum Description Length
(MDL) criterion described by Rissanen (1987). Thus, recalling the discussion in
Section 3.9, we see that the marginal likelihood provides a connection between
cross validation and MDL.
3.10
Priors
To compute the relative posterior probability of a network structure, we must
assess the structure prior p(Sh) and the parameter priors p(θs|Sh) (unless we
13 One of the technical assumptions used to derive this approximation is that the prior
is non-zero around ˆθs.

3 A Tutorial on Learning with Bayesian Networks
61
are using large-sample approximations such as BIC/MDL). The parameter pri-
ors p(θs|Sh) are also required for the alternative scoring functions discussed in
Section 3.8. Unfortunately, when many network structures are possible, these
assessments will be intractable. Nonetheless, under certain assumptions, we can
derive the structure and parameter priors for many network structures from a
manageable number of direct assessments. Several authors have discussed such
assumptions and corresponding methods for deriving priors (Cooper and Her-
skovits, 1991, 1992; Buntine, 1991; Spiegelhalter et al., 1993; Heckerman et al.,
1995b; Heckerman and Geiger, 1996). In this section, we examine some of these
approaches.
3.10.1
Priors on Network Parameters
First, let us consider the assessment of priors for the model parameters. We
consider the approach of Heckerman et al. (1995b) who address the case where
the local distribution functions are unrestricted multinomial distributions and
the assumption of parameter independence holds.
Their approach is based on two key concepts: independence equivalence
and distribution equivalence. We say that two Bayesian-network structures for
X are independence equivalent if they represent the same set of conditional-
independence assertions for X (Verma and Pearl, 1990). For example, given
X = {X, Y, Z}, the network structures X →Y →Z, X ←Y →Z, and
X ←Y ←Z represent only the independence assertion that X and Z are
conditionally independent given Y . Consequently, these network structures are
equivalent. As another example, a complete network structure is one that has
no missing edge—that is, it encodes no assertion of conditional independence.
When X contains n variables, there are n! possible complete network structures:
one network structure for each possible ordering of the variables. All complete
network structures for p(x) are independence equivalent. In general, two net-
work structures are independence equivalent if and only if they have the same
structure ignoring arc directions and the same v-structures (Verma and Pearl,
1990). A v-structure is an ordered tuple (X, Y, Z) such that there is an arc from
X to Y and from Z to Y , but no arc between X and Z.
The concept of distribution equivalence is closely related to that of indepen-
dence equivalence. Suppose that all Bayesian networks for X under consideration
have local distribution functions in the family F. This is not a restriction, per
se, because F can be a large family. We say that two Bayesian-network struc-
tures S1 and S2 for X are distribution equivalent with respect to (wrt) F if they
represent the same joint probability distributions for X—that is, if, for every
θs1, there exists a θs2 such that p(x|θs1, Sh
1 ) = p(x|θs2, Sh
2 ), and vice versa.
Distribution equivalence wrt some F implies independence equivalence, but
the converse does not hold. For example, when F is the family of generalized
linear-regression models, the complete network structures for n ≥3 variables
do not represent the same sets of distributions. Nonetheless, there are fami-
lies F—for example, unrestricted multinomial distributions and linear-regression

62
D. Heckerman
models with Gaussian noise—where independence equivalence implies distribu-
tion equivalence wrt F (Heckerman and Geiger, 1996).
The notion of distribution equivalence is important, because if two network
structures S1 and S2 are distribution equivalent wrt to a given F, then the hy-
potheses associated with these two structures are identical—that is, Sh
1 = Sh
2 .
Thus, for example, if S1 and S2 are distribution equivalent, then their probabil-
ities must be equal in any state of information. Heckerman et al. (1995b) call
this property hypothesis equivalence.
In light of this property, we should associate each hypothesis with an equiv-
alence class of structures rather than a single network structure, and our meth-
ods for learning network structure should actually be interpreted as methods
for learning equivalence classes of network structures (although, for the sake of
brevity, we often blur this distinction). Thus, for example, the sum over network-
structure hypotheses in Equation 3.33 should be replaced with a sum over
equivalence-class hypotheses. An eﬃcient algorithm for identifying the equiv-
alence class of a given network structure can be found in Chickering (1995).
We note that hypothesis equivalence holds provided we interpret Bayesian-
network structure simply as a representation of conditional independence.
Nonetheless, stronger deﬁnitions of Bayesian networks exist where arcs have a
causal interpretation (see Section 3.15). Heckerman et al. (1995b) and Heckerman
(1995) argue that, although it is unreasonable to assume hypothesis equivalence
when working with causal Bayesian networks, it is often reasonable to adopt a
weaker assumption of likelihood equivalence, which says that the observations in a
database can not help to discriminate two equivalent network structures.
Now let us return to the main issue of this section: the derivation of priors from
a manageable number of assessments. Geiger and Heckerman (1995) show that
the assumptions of parameter independence and likelihood equivalence imply
that the parameters for any complete network structure Sc must have a Dirichlet
distribution with constraints on the hyperparameters given by
αijk = α p(xk
i , paj
i|Sh
c )
(3.43)
where α is the user’s equivalent sample size,14, and p(xk
i , paj
i|Sh
c ) is computed
from the user’s joint probability distribution p(x|Sh
c ). This result is rather re-
markable, as the two assumptions leading to the constrained Dirichlet solution
are qualitative.
To determine the priors for parameters of incomplete network structures,
Heckerman et al. (1995b) use the assumption of parameter modularity, which
says that if Xi has the same parents in network structures S1 and S2, then
p(θij|Sh
1 ) = p(θij|Sh
2 )
for j = 1, . . . , qi. They call this property parameter modularity, because it says
that the distributions for parameters θij depend only on the structure of the
network that is local to variable Xi—namely, Xi and its parents.
14 Recall the method of equivalent samples for assessing beta and Dirichlet distributions
discussed in Section 3.2.

3 A Tutorial on Learning with Bayesian Networks
63
Given the assumptions of parameter modularity and parameter indepen-
dence,15 it is a simple matter to construct priors for the parameters of an arbitrary
network structure given the priors on complete network structures. In particular,
given parameter independence, we construct the priors for the parameters of each
node separately. Furthermore, if node Xi has parents Pai in the given network
structure, we identify a complete network structure where Xi has these parents,
and use Equation 3.43 and parameter modularity to determine the priors for this
node. The result is that all terms αijk for all network structures are determined by
Equation 3.43. Thus, from the assessments α and p(x|Sh
c ), we can derive the pa-
rameter priors for all possible network structures. Combining Equation 3.43 with
Equation 3.35, we obtain a model-selection criterion that assigns equal marginal
likelihoods to independence equivalent network structures.
We can assess p(x|Sh
c ) by constructing a Bayesian network, called a prior
network, that encodes this joint distribution. Heckerman et al. (1995b) discuss
the construction of this network.
3.10.2
Priors on Structures
Now, let us consider the assessment of priors on network-structure hypotheses.
Note that the alternative criteria described in Section 3.8 can incorporate prior
biases on network-structure hypotheses. Methods similar to those discussed in
this section can be used to assess such biases.
The simplest approach for assigning priors to network-structure hypotheses is
to assume that every hypothesis is equally likely. Of course, this assumption is
typically inaccurate and used only for the sake of convenience. A simple reﬁne-
ment of this approach is to ask the user to exclude various hypotheses (perhaps
based on judgments of of cause and eﬀect), and then impose a uniform prior on
the remaining hypotheses. We illustrate this approach in Section 3.12.
Buntine (1991) describes a set of assumptions that leads to a richer yet eﬃcient
approach for assigning priors. The ﬁrst assumption is that the variables can be
ordered (e.g., through a knowledge of time precedence). The second assumption
is that the presence or absence of possible arcs are mutually independent. Given
these assumptions, n(n −1)/2 probability assessments (one for each possible
arc in an ordering) determines the prior probability of every possible network-
structure hypothesis. One extension to this approach is to allow for multiple
possible orderings. One simpliﬁcation is to assume that the probability that an
arc is absent or present is independent of the speciﬁc arc in question. In this
case, only one probability assessment is required.
An alternative approach, described by Heckerman et al. (1995b) uses a prior
network. The basic idea is to penalize the prior probability of any structure
according to some measure of deviation between that structure and the prior
network. Heckerman et al. (1995b) suggest one reasonable measure of deviation.
Madigan et al. (1995) give yet another approach that makes use of imaginary
data from a domain expert. In their approach, a computer program helps the user
15 This construction procedure also assumes that every structure has a non-zero prior
probability.

64
D. Heckerman
create a hypothetical set of complete data. Then, using techniques such as those
in Section 3.7, they compute the posterior probabilities of network-structure
hypotheses given this data, assuming the prior probabilities of hypotheses are
uniform. Finally, they use these posterior probabilities as priors for the analysis
of the real data.
3.11
Search Methods
In this section, we examine search methods for identifying network structures
with high scores by some criterion. Consider the problem of ﬁnding the best
network from the set of all networks in which each node has no more than k
parents. Unfortunately, the problem for k > 1 is NP-hard even when we use
the restrictive prior given by Equation 3.43 (Chickering et al. 1995). Thus, re-
searchers have used heuristic search algorithms, including greedy search, greedy
search with restarts, best-ﬁrst search, and Monte-Carlo methods.
One consolation is that these search methods can be made more eﬃcient when
the model-selection criterion is separable. Given a network structure for domain
X, we say that a criterion for that structure is separable if it can be written as
a product of variable-speciﬁc criteria:
C(Sh, D) =
n

i=1
c(Xi, Pai, Di)
(3.44)
where Di is the data restricted to the variables Xi and Pai. An example of a sep-
arable criterion is the BD criterion (Equations 3.34 and 3.35) used in conjunction
with any of the methods for assessing structure priors described in Section 3.10.
Most of the commonly used search methods for Bayesian networks make suc-
cessive arc changes to the network, and employ the property of separability to
evaluate the merit of each change. The possible changes that can be made are
easy to identify. For any pair of variables, if there is an arc connecting them,
then this arc can either be reversed or removed. If there is no arc connecting
them, then an arc can be added in either direction. All changes are subject to
the constraint that the resulting network contains no directed cycles. We use E
to denote the set of eligible changes to a graph, and Δ(e) to denote the change
in log score of the network resulting from the modiﬁcation e ∈E. Given a sep-
arable criterion, if an arc to Xi is added or deleted, only c(Xi, Pai, Di) need be
evaluated to determine Δ(e). If an arc between Xi and Xj is reversed, then only
c(Xi, Pai, Di) and c(Xj, Πj, Dj) need be evaluated.
One simple heuristic search algorithm is greedy search. First, we choose a
network structure. Then, we evaluate Δ(e) for all e ∈E, and make the change e
for which Δ(e) is a maximum, provided it is positive. We terminate search when
there is no e with a positive value for Δ(e). When the criterion is separable,
we can avoid recomputing all terms Δ(e) after every change. In particular, if
neither Xi, Xj, nor their parents are changed, then Δ(e) remains unchanged for
all changes e involving these nodes as long as the resulting network is acyclic.

3 A Tutorial on Learning with Bayesian Networks
65
Candidates for the initial graph include the empty graph, a random graph, and
a prior network.
A potential problem with any local-search method is getting stuck at a local
maximum. One method for escaping local maxima is greedy search with random
restarts. In this approach, we apply greedy search until we hit a local maximum.
Then, we randomly perturb the network structure, and repeat the process for
some manageable number of iterations.
Another method for escaping local maxima is simulated annealing. In this
approach, we initialize the system at some temperature T0. Then, we pick some
eligible change e at random, and evaluate the expression p = exp(Δ(e)/T0).
If p > 1, then we make the change e; otherwise, we make the change with
probability p. We repeat this selection and evaluation process α times or until we
make β changes. If we make no changes in α repetitions, then we stop searching.
Otherwise, we lower the temperature by multiplying the current temperature T0
by a decay factor 0 < γ < 1, and continue the search process. We stop searching
if we have lowered the temperature more than δ times. Thus, this algorithm
is controlled by ﬁve parameters: T0, α, β, γ and δ. To initialize this algorithm,
we can start with the empty graph, and make T0 large enough so that almost
every eligible change is made, thus creating a random graph. Alternatively, we
may start with a lower temperature, and use one of the initialization methods
described for local search.
Another method for escaping local maxima is best-ﬁrst search (e.g., Korf,
1993). In this approach, the space of all network structures is searched system-
atically using a heuristic measure that determines the next best structure to
examine. Chickering (1996b) has shown that, for a ﬁxed amount of computa-
tion time, greedy search with random restarts produces better models than does
best-ﬁrst search.
One important consideration for any search algorithm is the search space. The
methods that we have described search through the space of Bayesian-network
structures. Nonetheless, when the assumption of hypothesis equivalence holds,
one can search through the space of network-structure equivalence classes. One
beneﬁt of the latter approach is that the search space is smaller. One drawback
of the latter approach is that it takes longer to move from one element in the
search space to another. Work by Spirtes and Meek (1995) and Chickering (1996)
conﬁrm these observations experimentally. Unfortunately, no comparisons are
yet available that determine whether the beneﬁts of equivalence-class search
outweigh the costs.
3.12
A Simple Example
Before we move on to other issues, let us step back and look at our overall
approach. In a nutshell, we can construct both structure and parameter priors
by constructing a Bayesian network (the prior network) along with additional
assessments such as an equivalent sample size and causal constraints. We then
use either Bayesian model selection, selective model averaging, or full model

66
D. Heckerman
Table 3.1. An imagined database for the fraud problem
averaging to obtain one or more networks for prediction and/or explanation. In
eﬀect, we have a procedure for using data to improve the structure and proba-
bilities of an initial Bayesian network.
Here, we present two artiﬁcial examples to illustrate this process. Consider
again the problem of fraud detection from Section 3.3. Suppose we are given the
database D in Table 3.1, and we want to predict the next case—that is, com-
pute p(xN+1|D). Let us assert that only two network-structure hypotheses have
appreciable probability: the hypothesis corresponding to the network structure
in Figure 3.3 (S1), and the hypothesis corresponding to the same structure with
an arc added from Age to Gas (S2). Furthermore, let us assert that these two
hypotheses are equally likely—that is, p(Sh
1 ) = p(Sh
2 ) = 0.5. In addition, let us
use the parameter priors given by Equation 3.43, where α = 10 and p(x|Sh
c )
is given by the prior network in Figure 3.3. Using Equations 3.34 and 3.35, we
obtain p(Sh
1 |D) = 0.26 and p(Sh
2 |D) = 0.74. Because we have only two models
to consider, we can model average according to Equation 3.33:
p(xN+1|D) = 0.26 p(xN+1|D, Sh
1 ) + 0.74 p(xN+1|D, Sh
2 )
where p(xN+1|D, Sh) is given by Equation 3.27. (We don’t display these proba-
bility distributions.) If we had to choose one model, we would choose S2, assum-
ing the posterior-probability criterion is appropriate. Note that the data favors
the presence of the arc from Age to Gas by a factor of three. This is not surpris-
ing, because in the two cases in the database where fraud is absent and gas was
purchased recently, the card holder was less than 30 years old.
An application of model selection, described by Spirtes and Meek (1995), is il-
lustrated in Figure 3.6. Figure 3.6a is a hand-constructed Bayesian network for the
domain of ICU ventilator management, called the Alarm network (Beinlich et al.,

3 A Tutorial on Learning with Bayesian Networks
67
case #   x1
x2
x3
   x37
1
2
3
4
10,000
3
2
1
3
2
3
2
3
2
2
2
2
3
3
2
4
3
3
1
3




17
25
6
5
4
19
27
20
10
21
37
31
11
32
33
22
15
14
23
13
16
29
8
9
28
12
34
35
36
24
30
7
26
18
3
2
1
(a)
(b)
17
25
18
26
3
6
5
4
19
27
20
10
21
35
37
36
31
11
32
34
12
24
33
22
15
14
23
13
16
29
30
7
8
9
28
2
1
(c)
17
25
6
5
4
19
27
20
10
21
37
31
11
32
33
22
15
14
23
13
16
29
8
9
28
12
34
35
36
24
30
7
26
18
3
2
1
(d)
deleted
Fig. 3.6.
(a) The Alarm network structure. (b) A prior network encoding a user’s
beliefs about the Alarm domain. (c) A random sample of size 10,000 generated from
the Alarm network. (d) The network learned from the prior network and the random
sample. The only diﬀerence between the learned and true structure is an arc deletion
as noted in (d). Network probabilities are not shown.
1989). Figure 3.6c is a random sample from the Alarm network of size 10,000.
Figure 3.6b is a simple prior network for the domain. This network encodes
mutual independence among the variables, and (not shown) uniform probability
distributions for each variable.
Figure 3.6d shows the most likely network structure found by a two-pass
greedy search in equivalence-class space. In the ﬁrst pass, arcs were added until
the model score did not improve. In the second pass, arcs were deleted until
the model score did not improve. Structure priors were uniform; and parameter
priors were computed from the prior network using Equation 3.43 with α = 10.

68
D. Heckerman
The network structure learned from this procedure diﬀers from the true net-
work structure only by a single arc deletion. In eﬀect, we have used the data to
improve dramatically the original model of the user.
3.13
Bayesian Networks for Supervised Learning
As we discussed in Section 3.5, the local distribution functions p(xi|pai, θi, Sh)
are essentially classiﬁcation/regression models. Therefore, if we are doing su-
pervised learning where the explanatory (input) variables cause the outcome
(target) variable and data is complete, then the Bayesian-network and classiﬁ-
cation/regression approaches are identical.
When data is complete but input/target variables do not have a simple
cause/eﬀect relationship, tradeoﬀs emerge between the Bayesian-network ap-
proach and other methods. For example, consider the classiﬁcation problem in
Figure 3.5. Here, the Bayesian network encodes dependencies between ﬁndings
and ailments as well as among the ﬁndings, whereas another classiﬁcation model
such as a decision tree encodes only the relationships between ﬁndings and ail-
ment. Thus, the decision tree may produce more accurate classiﬁcations, because
it can encode the necessary relationships with fewer parameters. Nonetheless, the
use of local criteria for Bayesian-network model selection mitigates this advan-
tage. Furthermore, the Bayesian network provides a more natural representation
in which to encode prior knowledge, thus giving this model a possible advantage
for suﬃciently small sample sizes. Another argument, based on bias–variance
analysis, suggests that neither approach will dramatically outperform the other
(Friedman, 1996).
Singh and Provan (1995) compare the classiﬁcation accuracy of Bayesian net-
works and decision trees using complete data sets from the University of Califor-
nia, Irvine Repository of Machine Learning databases. Speciﬁcally, they compare
C4.5 with an algorithm that learns the structure and probabilities of a Bayesian
network using a variation of the Bayesian methods we have described. The latter
algorithm includes a model-selection phase that discards some input variables.
They show that, overall, Bayesian networks and decisions trees have about the
same classiﬁcation error. These results support the argument of Friedman (1996).
When the input variables cause the target variable and data is incomplete,
the dependencies between input variables becomes important, as we discussed
in the introduction. Bayesian networks provide a natural framework for learning
about and encoding these dependencies. Unfortunately, no studies have been
done comparing these approaches with other methods for handling missing data.
3.14
Bayesian Networks for Unsupervised Learning
The techniques described in this paper can be used for unsupervised learning.
A simple example is the AutoClass program of Cheeseman and Stutz (1995),
which performs data clustering. The idea behind AutoClass is that there is a
single hidden (i.e., never observed) variable that causes the observations. This

3 A Tutorial on Learning with Bayesian Networks
69
D3
H
D2
D1
C1
C2
C3
C4
C5
Fig. 3.7. A Bayesian-network structure for AutoClass. The variable H is hidden. Its
possible states correspond to the underlying classes in the data.
(a)
(b)
Fig. 3.8.
(a) A Bayesian-network structure for observed variables. (b) A Bayesian-
network structure with hidden variables (shaded) suggested by the network structure
in (a).
hidden variable is discrete, and its possible states correspond to the underlying
classes in the data. Thus, AutoClass can be described by a Bayesian network such
as the one in Figure 3.7. For reasons of computational eﬃciency, Cheeseman and
Stutz (1995) assume that the discrete variables (e.g., D1, D2, D3 in the ﬁgure)
and user-deﬁned sets of continuous variables (e.g., {C1, C2, C3} and {C4, C5})
are mutually independent given H. Given a data set D, AutoClass searches over
variants of this model (including the number of states of the hidden variable) and
selects a variant whose (approximate) posterior probability is a local maximum.
AutoClass is an example where the user presupposes the existence of a hidden
variable. In other situations, we may be unsure about the presence of a hidden
variable. In such cases, we can score models with and without hidden variables to
reduce our uncertainty. We illustrate this approach on a real-world case study in
Section 3.16. Alternatively, we may have little idea about what hidden variables
to model. The search algorithms of Spirtes et al. (1993) provide one method

70
D. Heckerman
for identifying possible hidden variables in such situations. Martin and VanLehn
(1995) suggest another method.
Their approach is based on the observation that if a set of variables are mu-
tually dependent, then a simple explanation is that these variables have a single
hidden common cause rendering them mutually independent. Thus, to identify
possible hidden variables, we ﬁrst apply some learning technique to select a model
containing no hidden variables. Then, we look for sets of mutually dependent
variables in this learned model. For each such set of variables (and combinations
thereof), we create a new model containing a hidden variable that renders that
set of variables conditionally independent. We then score the new models, pos-
sibly ﬁnding one better than the original. For example, the model in Figure 3.8a
has two sets of mutually dependent variables. Figure 3.8b shows another model
containing hidden variables suggested by this model.
3.15
Learning Causal Relationships
As we have mentioned, the causal semantics of a Bayesian network provide a
means by which we can learn causal relationships. In this section, we examine
these semantics, and provide a basic discussion on how causal relationships can
be learned. We note that these methods are new and controversial. For critical
discussions on both sides of the issue, see Spirtes et al. (1993), Pearl (1995), and
Humphreys and Freedman (1995).
For purposes of illustration, suppose we are marketing analysts who want to
know whether or not we should increase, decrease, or leave alone the exposure
of a particular advertisement in order to maximize our proﬁt from the sales of a
product. Let variables Ad (A) and Buy (B) represent whether or not an individ-
ual has seen the advertisement and has purchased the product, respectively. In
one component of our analysis, we would like to learn the physical probability
that B = true given that we force A to be true, and the physical probability
that B = true given that we force A to be false.16 We denote these probabilities
p(b|ˆa) and p(b|ˆ¯a), respectively. One method that we can use to learn these proba-
bilities is to perform a randomized experiment: select two similar populations at
random, force A to be true in one population and false in the other, and observe
B. This method is conceptually simple, but it may be diﬃcult or expensive to
ﬁnd two similar populations that are suitable for the study.
An alternative method follows from causal knowledge. In particular, suppose
A causes B. Then, whether we force A to be true or simply observe that A is
true in the current population, the advertisement should have the same causal
inﬂuence on the individual’s purchase. Consequently, p(b|ˆa) = p(b|a), where
p(b|a) is the physical probability that B = true given that we observe A = true
in the current population. Similarly, p(b|ˆ¯a) = p(b|¯a). In contrast, if B causes
A, forcing A to some state should not inﬂuence B at all. Therefore, we have
p(b|ˆa) = p(b|ˆ¯a) = p(b). In general, knowledge that X causes Y allows us to
16 It is important that these interventions do not interfere with the normal eﬀect of A
on B. See Heckerman and Shachter (1995) for a discussion of this point.

3 A Tutorial on Learning with Bayesian Networks
71
Buy
Ad
(a)
(b)
Ad
Buy
Ad
H
Buy
Ad
S
Buy
Income
Location
Ad
Buy
Fig. 3.9.
(a) Causal graphs showing for explanations for an observed dependence
between Ad and Buy. The node H corresponds to a hidden common cause of Ad and
Buy. The shaded node S indicates that the case has been included in the database.
(b) A Bayesian network for which A causes B is the only causal explanation, given the
causal Markov condition.
equate p(y|x) with p(y|ˆx), where ˆx denotes the intervention that forces X to be
x. For purposes of discussion, we use this rule as an operational deﬁnition for
cause. Pearl (1995) and Heckerman and Shachter (1995) discuss versions of this
deﬁnition that are more complete and more precise.
In our example, knowledge that A causes B allows us to learn p(b|ˆa) and p(b|ˆ¯a)
from observations alone—no randomized experiment is needed. But how are we
to determine whether or not A causes B? The answer lies in an assumption
about the connection between causal and probabilistic dependence known as
the causal Markov condition, described by Spirtes et al. (1993). We say that a
directed acyclic graph C is a causal graph for variables X if the nodes in C are in
a one-to-one correspondence with X, and there is an arc from node X to node
Y in C if and only if X is a direct cause of Y . The causal Markov condition says
that if C is a causal graph for X, then C is also a Bayesian-network structure
for the joint physical probability distribution of X. In Section 3.3, we described
a method based on this condition for constructing Bayesian-network structure
from causal assertions. Several researchers (e.g., Spirtes et al., 1993) have found
that this condition holds in many applications.
Given the causal Markov condition, we can infer causal relationships from
conditional-independence and conditional-dependence relationships that we learn
from the data.17 Let us illustrate this process for the marketing example. Suppose
we have learned (with high Bayesian probability) that the physical probabilities
p(b|a) and p(b|¯a) are not equal. Given the causal Markov condition, there are
four simple causal explanations for this dependence: (1) A is a cause for B, (2)
17 Spirtes et al. (1993) also require an assumption known as faithfulness. We do not
need to make this assumption explicit, because it follows from our assumption that
p(θs|Sh) is a probability density function.

72
D. Heckerman
B is a cause for A, (3) there is a hidden common cause of A and B (e.g., the
person’s income), and (4) A and B are causes for data selection. This last expla-
nation is known as selection bias. Selection bias would occur, for example, if our
database failed to include instances where A and B are false. These four causal
explanations for the presence of the arcs are illustrated in Figure 3.9a. Of course,
more complicated explanations—such as the presence of a hidden common cause
and selection bias—are possible.
So far, the causal Markov condition has not told us whether or not A causes
B. Suppose, however, that we observe two additional variables: Income (I) and
Location (L), which represent the income and geographic location of the possible
purchaser, respectively. Furthermore, suppose we learn (with high probability)
the Bayesian network shown in Figure 3.9b. Given the causal Markov condition,
the only causal explanation for the conditional-independence and conditional-
dependence relationships encoded in this Bayesian network is that Ad is a cause
for Buy. That is, none of the other explanations described in the previous para-
graph, or combinations thereof, produce the probabilistic relationships encoded
in Figure 3.9b. Based on this observation, Pearl and Verma (1991) and Spirtes
et al. (1993) have created algorithms for inferring causal relationships from de-
pendence relationships for more complicated situations.
3.16
A Case Study: College Plans
Real-world applications of techniques that we have discussed can be found in
Madigan and Raftery (1994), Lauritzen et al. (1994), Singh and Provan (1995),
and Friedman and Goldszmidt (1996). Here, we consider an application that
comes from a study by Sewell and Shah (1968), who investigated factors that
inﬂuence the intention of high school students to attend college. The data have
been analyzed by several groups of statisticians, including Whittaker (1990) and
Spirtes et al. (1993), all of whom have used non-Bayesian techniques.
Sewell and Shah (1968) measured the following variables for 10,318 Wisconsin
high school seniors: Sex (SEX): male, female; Socioeconomic Status (SES): low,
lower middle, upper middle, high; Intelligence Quotient (IQ): low, lower middle,
upper middle, high; Parental Encouragement (PE): low, high; and College Plans
(CP): yes, no. Our goal here is to understand the (possibly causal) relationships
among these variables.
The data are described by the suﬃcient statistics in Table 3.2. Each entry
denotes the number of cases in which the ﬁve variables take on some particu-
lar conﬁguration. The ﬁrst entry corresponds to the conﬁguration SEX=male,
SES=low, IQ=low, PE=low, and CP=yes. The remaining entries correspond to
conﬁgurations obtained by cycling through the states of each variable such that
the last variable (CP) varies most quickly. Thus, for example, the upper (lower)
half of the table corresponds to male (female) students.
As a ﬁrst pass, we analyzed the data assuming no hidden variables. To
generate priors for network parameters, we used the method described in
Section 3.10.1 with an equivalent sample size of 5 and a prior network where

3 A Tutorial on Learning with Bayesian Networks
73
Table 3.2. Suﬃcient statistics for the Sewall and Shah (1968) study
Reproduced by permission from the University of Chicago Press. c⃝1968 by The
University of Chicago. All rights reserved.
SES
SEX
PE
IQ
CP
log
(
|
)
(
|
)
.
p D S
p S
D
h
h
1
1
45653
10
= −
=
SES
SEX
PE
IQ
CP
log
(
|
)
(
|
)
.
p D S
p S
D
h
h
2
2
10
45699
12
10
= −
=
×
−
Fig. 3.10. The a posteriori most likely network structures without hidden variables
p(x|Sh
c ) is uniform. (The results were not sensitive to the choice of parameter
priors. For example, none of the results reported in this section changed quali-
tatively for equivalent sample sizes ranging from 3 to 40.) For structure priors,
we assumed that all network structures were equally likely, except we excluded
structures where SEX and/or SES had parents, and/or CP had children. Be-
cause the data set was complete, we used Equations 3.34 and 3.35 to compute
the posterior probabilities of network structures. The two most likely network
structures that we found after an exhaustive search over all structures are shown
in Figure 3.10. Note that the most likely graph has a posterior probability that
is extremely close to one.
If we adopt the causal Markov assumption and also assume that there are no
hidden variables, then the arcs in both graphs can be interpreted causally. Some
results are not surprising—for example the causal inﬂuence of socioeconomic
status and IQ on college plans. Other results are more interesting. For example,
from either graph we conclude that sex inﬂuences college plans only indirectly
through parental inﬂuence. Also, the two graphs diﬀer only by the orientation

74
D. Heckerman
p(male) = 0.48
SES
H
SEX
PE
IQ
CP
p(H=0) = 0.63
p(H=1) = 0.37
H
0
1
0
1
PE
low
low
high
high
p(IQ=high|PE,H)
0.098
0.22
0.21
0.49
H
low
high
p(SES=high|H)
0.088
0.51
SEX
male
female
male
female
SES
low
low
high
high
p(PE=high|SES,SEX)
0.32
0.166
0.86
0.81
SES
low
low
low
low
high
high
high
high
PE
low
high
low
high
low
high
low
high
IQ
low
low
high
high
low
low
high
high
p(CP=yes|SES,IQ,PE)
0.011
0.170
0.124
0.53
0.093
0.39
0.24
0.84
log
(
|
)
p S
D
h
≅−45629
Fig. 3.11. The a posteriori most likely network structure with a hidden variable.
Probabilities shown are MAP values. Some probabilities are omitted for lack of space.
of the arc between PE and IQ. Either causal relationship is plausible. We note
that the second most likely graph was selected by Spirtes et al. (1993), who used
a non-Bayesian approach with slightly diﬀerent assumptions.
The most suspicious result is the suggestion that socioeconomic status has
a direct inﬂuence on IQ. To question this result, we considered new models
obtained from the models in Figure 3.10 by replacing this direct inﬂuence with
a hidden variable pointing to both SES and IQ. We also considered models
where the hidden variable pointed to SES, IQ, and PE, and none, one, or both
of the connections SES—PE and PE—IQ were removed. For each structure,
we varied the number of states of the hidden variable from two to six.
We computed the posterior probability of these models using the Cheeseman-
Stutz (1995) variant of the Laplace approximation. To ﬁnd the MAP ˜θs, we used
the EM algorithm, taking the largest local maximum from among 100 runs with
diﬀerent random initializations of θs. Among the models we considered, the one
with the highest posterior probability is shown in Figure 3.11. This model is
2·1010 times more likely that the best model containing no hidden variable. The
next most likely model containing a hidden variable, which has one additional
arc from the hidden variable to PE, is 5 · 10−9 times less likely than the best
model. Thus, if we again adopt the causal Markov assumption and also assume
that we have not omitted a reasonable model from consideration, then we have
strong evidence that a hidden variable is inﬂuencing both socioeconomic status
and IQ in this population—a sensible result. An examination of the probabilities
in Figure 3.11 suggests that the hidden variable corresponds to some measure of
“parent quality”.
3.17
Pointers to Literature and Software
Like all tutorials, this one is incomplete. For those readers interested in learn-
ing more about graphical models and methods for learning them, we oﬀer the

3 A Tutorial on Learning with Bayesian Networks
75
following additional references and pointers to software. Buntine (1996) provides
another guide to the literature.
Spirtes et al. (1993) and Pearl (1995) use methods based on large-sample
approximations to learn Bayesian networks. In addition, as we have discussed,
they describe methods for learning causal relationships from observational data.
In addition to directed models, researchers have explored network structures
containing undirected edges as a knowledge representation. These representa-
tions are discussed (e.g.) in Lauritzen (1982), Verma and Pearl (1990), Fryden-
berg (1990), Whittaker (1990), and Richardson (1997). Bayesian methods for
learning such models from data are described by Dawid and Lauritzen (1993)
and Buntine (1994).
Finally, several research groups have developed software systems for learning
graphical models. For example, Scheines et al. (1994) have developed a soft-
ware program called TETRAD II for learning about cause and eﬀect. Badsberg
(1992) and Højsgaard et al. (1994) have built systems that can learn with mixed
graphical models using a variety of criteria for model selection. Thomas, Spiegel-
halter, and Gilks (1992) have created a system called BUGS that takes a learning
problem speciﬁed as a Bayesian network and compiles this problem into a Gibbs-
sampler computer program.
Acknowledgments
I thank Max Chickering, Usama Fayyad, Eric Horvitz, Chris Meek, Koos Rom-
melse, and Padhraic Smyth for their comments on earlier versions of this
manuscript. I also thank Max Chickering for implementing the software used
to analyze the Sewall and Shah (1968) data, and Chris Meek for bringing this
data set to my attention.

76
D. Heckerman
Notation
X, Y, Z, . . . Variables or their corresponding nodes in a Bayesian
network
X, Y, Z, . . . Sets of variables or corresponding sets of nodes
X = x Variable X is in state x
X = x The set of variables X is in conﬁguration x
x, y, z Typically refer to a complete case, an incomplete
case, and missing data in a case, respectively
X \ Y The variables in X that are not in Y
D A data set: a set of cases
Dl The ﬁrst l −1 cases in D
p(x|y) The probability that X = x given Y = y
(also used to describe a probability density,
probability distribution, and probability density)
Ep(·)(x) The expectation of x with respect to p(·)
S A Bayesian network structure (a directed acyclic graph)
Pai The variable or node corresponding to the parents
of node Xi in a Bayesian network structure
pai A conﬁguration of the variables Pai
ri The number of states of discrete variable Xi
qi The number of conﬁgurations of Pai
Sc A complete network structure
Sh The hypothesis corresponding to network structure S
θijk The multinomial parameter corresponding to the
probability p(Xi = xk
i |Pai = paj
i)
θij = (θij2, . . . , θijri)
θi = (θi1, . . . , θiqi)
θs = (θ1, . . . , θn)
α An equivalent sample size
αijk The Dirichlet hyperparameter corresponding to θijk
αij = ri
k=1 αijk
Nijk The number of cases in data set D where Xi = xk
i and Pai = paj
i
Nij = ri
k=1 Nijk
References
[Aliferis and Cooper, 1994] Aliferis, C., Cooper, G.: An evaluation of an algorithm
for inductive learning of Bayesian belief networks using simulated data sets. In:
Proceedings of Tenth Conference on Uncertainty in Artiﬁcial Intelligence, Seattle,
WA, pp. 8–14. Morgan Kaufmann, San Francisco (1994)
[Badsberg, 1992] Badsberg, J.: Model search in contingency tables by CoCo. In: Dodge,
Y., Whittaker, J. (eds.) Computational Statistics, pp. 251–256. Physica Verlag, Hei-
delberg (1992)

3 A Tutorial on Learning with Bayesian Networks
77
[Becker and LeCun, 1989] Becker, S., LeCun, Y.: Improving the convergence of back-
propagation learning with second order methods. In: Proceedings of the 1988 Con-
nectionist Models Summer School, pp. 29–37. Morgan Kaufmann, San Francisco
(1989)
[Beinlich et al., 1989] Beinlich, I., Suermondt, H., Chavez, R., Cooper, G.: The
ALARM monitoring system: A case study with two probabilistic inference tech-
niques for belief networks. In: Proceedings of the Second European Conference on
Artiﬁcial Intelligence in Medicine, London, pp. 247–256. Springer, Berlin (1989)
[Bernardo, 1979] Bernardo, J.: Expected information as expected utility. Annals of
Statistics 7, 686–690 (1979)
[Bernardo and Smith, 1994] Bernardo, J., Smith, A.: Bayesian Theory. John Wiley
and Sons, New York (1994)
[Buntine, 1991] Buntine, W.: Theory reﬁnement on Bayesian networks. In: Proceedings
of Seventh Conference on Uncertainty in Artiﬁcial Intelligence, Los Angeles, CA, pp.
52–60. Morgan Kaufmann, San Francisco (1991)
[Buntine, 1993] Buntine, W.: Learning classiﬁcation trees. In: Artiﬁcial Intelligence
Frontiers in Statistics: AI and statistics III. Chapman and Hall, New York (1993)
[Buntine, 1996] Buntine, W.: A guide to the literature on learning graphical models.
IEEE Transactions on Knowledge and Data Engineering 8, 195–210 (1996)
[Chaloner and Duncan, 1983] Chaloner, K., Duncan, G.: Assessment of a beta prior
distribution: PM elicitation. The Statistician 32, 174–180 (1983)
[Cheeseman and Stutz, 1995] Cheeseman, P., Stutz, J.: Bayesian classiﬁcation (Auto-
Class): Theory and results. In: Fayyad, U., Piatesky-Shapiro, G., Smyth, P., Uthu-
rusamy, R. (eds.) Advances in Knowledge Discovery and Data Mining, pp. 153–180.
AAAI Press, Menlo Park (1995)
[Chib, 1995] Chib, S.: Marginal likelihood from the Gibbs output. Journal of the Amer-
ican Statistical Association 90, 1313–1321 (1995)
[Chickering, 1995] Chickering, D.: A transformational characterization of equivalent
Bayesian network structures. In: Proceedings of Eleventh Conference on Uncertainty
in Artiﬁcial Intelligence, Montreal, QU, pp. 87–98. Morgan Kaufmann, San Francisco
(1995)
[Chickering, 1996b] Chickering, D.: Learning equivalence classes of Bayesian-network
structures. In: Proceedings of Twelfth Conference on Uncertainty in Artiﬁcial Intel-
ligence, Portland, OR. Morgan Kaufmann, San Francisco (1996)
[Chickering et al., 1995] Chickering, D., Geiger, D., Heckerman, D.: Learning Bayesian
networks: Search methods and experimental results. In: Proceedings of Fifth Confer-
ence on Artiﬁcial Intelligence and Statistics, Ft. Lauderdale, FL. Society for Artiﬁcial
Intelligence in Statistics, pp. 112–128 (1995)
[Chickering and Heckerman, 1996] Chickering, D., Heckerman, D.: Eﬃcient approxi-
mations for the marginal likelihood of incomplete data given a Bayesian network.
Technical Report MSR-TR-96-08, Microsoft Research, Redmond, WA (Revised,
November 1996)
[Cooper, 1990] Cooper, G.: Computational complexity of probabilistic inference using
Bayesian belief networks (Research note). Artiﬁcial Intelligence 42, 393–405 (1990)
[Cooper and Herskovits, 1992] Cooper, G., Herskovits, E.: A Bayesian method for the
induction of probabilistic networks from data. Machine Learning 9, 309–347 (1992)
[Cooper and Herskovits, 1991] Cooper, G., Herskovits, E.: A Bayesian method for the
induction of probabilistic networks from data. Technical Report SMI-91-1, Section
on Medical Informatics, Stanford University (January 1991)
[Cox, 1946] Cox, R.: Probability, frequency and reasonable expectation. American
Journal of Physics 14, 1–13 (1946)

78
D. Heckerman
[Dagum and Luby, 1993] Dagum, P., Luby, M.: Approximating probabilistic inference
in bayesian belief networks is np-hard. Artiﬁcial Intelligence 60, 141–153 (1993)
[D’Ambrosio, 1991] D’Ambrosio, B.: Local expression languages for probabilistic de-
pendence. In: Proceedings of Seventh Conference on Uncertainty in Artiﬁcial Intel-
ligence, Los Angeles, CA, pp. 95–102. Morgan Kaufmann, San Francisco (1991)
[Darwiche and Provan, 1996] Darwiche, A., Provan, G.: Query DAGs: A practical
paradigm for implementing belief-network inference. In: Proceedings of Twelfth Con-
ference on Uncertainty in Artiﬁcial Intelligence, Portland, OR, pp. 203–210. Morgan
Kaufmann, San Francisco (1996)
[Dawid, 1984] Dawid, P.: Statistical theory. The prequential approach (with discus-
sion). Journal of the Royal Statistical Society A 147, 178–292 (1984)
[Dawid, 1992] Dawid, P.: Applications of a general propagation algorithm for proba-
bilistic expert systmes. Statistics and Computing 2, 25–36 (1992)
[de Finetti, 1970] de Finetti, B.: Theory of Probability. Wiley and Sons, New York
(1970)
[Dempster et al., 1977] Dempster, A., Laird, N., Rubin, D.: Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B 39,
1–38 (1977)
[DiCiccio et al., 1995] DiCiccio, T., Kass, R., Raftery, A., Wasserman, L.: Computing
Bayes factors by combining simulation and asymptotic approximations. Technical
Report 630, Department of Statistics, Carnegie Mellon University, PA (July 1995)
[Friedman, 1995] Friedman, J.: Introduction to computational learning and statistical
prediction. Technical report, Department of Statistics, Stanford University (1995)
[Friedman, 1996] Friedman, J.: On bias, variance, 0/1-loss, and the curse of dimen-
sionality. Data Mining and Knowledge Discovery, 1 (1996)
[Friedman and Goldszmidt, 1996] Friedman, N., Goldszmidt, M.: Building classiﬁers
using Bayesian networks. In: Proceedings AAAI 1996 Thirteenth National Confer-
ence on Artiﬁcial Intelligence, Portland, OR, pp. 1277–1284. AAAI Press, Menlo
Park (1996)
[Frydenberg, 1990] Frydenberg, M.: The chain graph Markov property. Scandinavian
Journal of Statistics 17, 333–353 (1990)
[Geiger and Heckerman, 1995] Geiger, D., Heckerman, D.: A characterization of the
Dirichlet distribution applicable to learning Bayesian networks. Technical Report
MSR-TR-94-16, Microsoft Research, Redmond, WA (Revised, February 1995)
[Geiger et al., 1996] Geiger, D., Heckerman, D., Meek, C.: Asymptotic model selection
for directed networks with hidden variables. In: Proceedings of Twelfth Conference
on Uncertainty in Artiﬁcial Intelligence, Portland, OR, pp. 283–290. Morgan Kauf-
mann, San Francisco (1996)
[Geman and Geman, 1984] Geman, S., Geman, D.: Stochastic relaxation, Gibbs dis-
tributions and the Bayesian restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence 6, 721–742 (1984)
[Gilks et al., 1996] Gilks, W., Richardson, S., Spiegelhalter, D.: Markov Chain Monte
Carlo in Practice. Chapman and Hall, Boca Raton (1996)
[Good, 1950] Good, I.: Probability and the Weighing of Evidence. Hafners, New York
(1950)
[Heckerman, 1989] Heckerman, D.: A tractable algorithm for diagnosing multiple dis-
eases. In: Proceedings of the Fifth Workshop on Uncertainty in Artiﬁcial Intelligence,
Windsor, ON, pp. 174–181. Association for Uncertainty in Artiﬁcial Intelligence,
Mountain View, CA (1989); Also In: Henrion, M., Shachter, R., Kanal, L., Lemmer,
J. (eds.) Uncertainty in Artiﬁcial Intelligence 5, pp. 163–171. North-Holland, New
York (1990)

3 A Tutorial on Learning with Bayesian Networks
79
[Heckerman, 1995] Heckerman, D.: A Bayesian approach for learning causal networks.
In: Proceedings of Eleventh Conference on Uncertainty in Artiﬁcial Intelligence,
Montreal, QU, pp. 285–295. Morgan Kaufmann, San Francisco (1995)
[Heckerman and Geiger, 1996] Heckerman, D., Geiger, D.: Likelihoods and priors for
Bayesian networks. Technical Report MSR-TR-95-54, Microsoft Research, Redmond,
WA (Revised, November 1996)
[Heckerman et al., 1995a] Heckerman,
D.,
Geiger,
D.,
Chickering,
D.:
Learning
Bayesian networks: The combination of knowledge and statistical data. Machine
Learning 20, 197–243 (1995a)
[Heckerman et al., 1995b] Heckerman, D., Mamdani, A., Wellman, M.: Real-world ap-
plications of Bayesian networks. Communications of the ACM 38 (1995b)
[Heckerman and Shachter, 1995] Heckerman,
D.,
Shachter,
R.:
Decision-theoretic
foundations for causal reasoning. Journal of Artiﬁcial Intelligence Research 3, 405–
430 (1995)
[Højsgaard et al., 1994] Højsgaard, S., Skjøth, F., Thiesson, B.: User’s guide to
BIOFROST. Technical report, Department of Mathematics and Computer Science,
Aalborg, Denmark (1994)
[Howard, 1970] Howard, R.: Decision analysis: Perspectives on inference, decision, and
experimentation. Proceedings of the IEEE 58, 632–643 (1970)
[Howard and Matheson, 1981] Howard, R., Matheson, J.: Inﬂuence diagrams. In:
Howard, R., Matheson, J. (eds.) Readings on the Principles and Applications of
Decision Analysis, Strategic Decisions Group, Menlo Park, CA, vol. II, pp. 721–762
(1981)
[Howard and Matheson, 1983] Howard, R., Matheson, J. (eds.): The Principles and
Applications of Decision Analysis, Strategic Decisions Group, Menlo Park, CA (1983)
[Humphreys and Freedman, 1996] Humphreys, P., Freedman, D.: The grand leap.
British Journal for the Philosphy of Science 47, 113–118 (1996)
[Jaakkola and Jordan, 1996] Jaakkola, T., Jordan, M.: Computing upper and lower
bounds on likelihoods in intractable networks. In: Proceedings of Twelfth Confer-
ence on Uncertainty in Artiﬁcial Intelligence, Portland, OR, pp. 340–348. Morgan
Kaufmann, San Francisco (1996)
[Jensen, 1996] Jensen, F.: An Introduction to Bayesian Networks. Springer, Heidelberg
(1996)
[Jensen and Andersen, 1990] Jensen, F., Andersen, S.: Approximations in Bayesian
belief universes for knowledge based systems. Technical report, Institute of Electronic
Systems, Aalborg University, Aalborg, Denmark (1990)
[Jensen et al., 1990] Jensen, F., Lauritzen, S., Olesen, K.: Bayesian updating in recur-
sive graphical models by local computations. Computational Statisticals Quarterly 4,
269–282 (1990)
[Kass and Raftery, 1995] Kass, R., Raftery, A.: Bayes factors. Journal of the American
Statistical Association 90, 773–795 (1995)
[Kass et al., 1988] Kass, R., Tierney, L., Kadane, J.: Asymptotics in Bayesian com-
putation. In: Bernardo, J., DeGroot, M., Lindley, D., Smith, A. (eds.) Bayesian
Statistics, vol. 3, pp. 261–278. Oxford University Press, Oxford (1988)
[Koopman, 1936] Koopman, B.: On distributions admitting a suﬃcient statistic.
Transactions of the American Mathematical Society 39, 399–409 (1936)
[Korf, 1993] Korf, R.: Linear-space best-ﬁrst search. Artiﬁcial Intelligence 62, 41–78
(1993)
[Lauritzen, 1982] Lauritzen, S.: Lectures on Contingency Tables. University of Aalborg
Press, Aalborg (1982)

80
D. Heckerman
[Lauritzen, 1992] Lauritzen, S.: Propagation of probabilities, means, and variances in
mixed graphical association models. Journal of the American Statistical Associa-
tion 87, 1098–1108 (1992)
[Lauritzen and Spiegelhalter, 1988] Lauritzen, S., Spiegelhalter, D.: Local computa-
tions with probabilities on graphical structures and their application to expert sys-
tems. J. Royal Statistical Society B 50, 157–224 (1988)
[Lauritzen et al., 1994] Lauritzen, S., Thiesson, B., Spiegelhalter, D.: Diagnostic sys-
tems created by model selection methods: A case study. In: Cheeseman, P., Oldford,
R. (eds.) AI and Statistics IV. Lecture Notes in Statistics, vol. 89, pp. 143–152.
Springer, New York (1994)
[MacKay, 1992a] MacKay, D.: Bayesian interpolation. Neural Computation 4, 415–447
(1992a)
[MacKay, 1992b] MacKay, D.: A practical Bayesian framework for backpropagation
networks. Neural Computation 4, 448–472 (1992b)
[MacKay, 1996] MacKay, D.: Choice of basis for the Laplace approximation. Technical
report, Cavendish Laboratory, Cambridge, UK (1996)
[Madigan et al., 1995] Madigan, D., Garvin, J., Raftery, A.: Eliciting prior information
to enhance the predictive performance of Bayesian graphical models. Communica-
tions in Statistics: Theory and Methods 24, 2271–2292 (1995)
[Madigan and Raftery, 1994] Madigan, D., Raftery, A.: Model selection and accounting
for model uncertainty in graphical models using Occam’s window. Journal of the
American Statistical Association 89, 1535–1546 (1994)
[Madigan et al., 1996] Madigan, D., Raftery, A., Volinsky, C., Hoeting, J.: Bayesian
model averaging. In: Proceedings of the AAAI Workshop on Integrating Multiple
Learned Models, Portland, OR (1996)
[Madigan and York, 1995] Madigan, D., York, J.: Bayesian graphical models for dis-
crete data. International Statistical Review 63, 215–232 (1995)
[Martin and VanLehn, 1995] Martin, J., VanLehn, K.: Discrete factor analysis: Learn-
ing hidden variables in bayesian networks. Technical report, Department of Com-
puter Science, University of Pittsburgh, PA. (1995),
http://bert.cs.pitt.edu/vanlehn
[Meng and Rubin, 1991] Meng, X., Rubin, D.: Using EM to obtain asymptotic
variance-covariance matrices: The SEM algorithm. Journal of the American Sta-
tistical Association 86, 899–909 (1991)
[Neal, 1993] Neal, R.: Probabilistic inference using Markov chain Monte Carlo meth-
ods. Technical Report CRG-TR-93-1, Department of Computer Science, University
of Toronto (1993)
[Olmsted, 1983] Olmsted, S.: On representing and solving decision problems. PhD the-
sis, Department of Engineering-Economic Systems, Stanford University (1983)
[Pearl, 1986] Pearl, J.: Fusion, propagation, and structuring in belief networks. Artiﬁ-
cial Intelligence 29, 241–288 (1986)
[Pearl, 1995] Pearl, J.: Causal diagrams for empirical research. Biometrika 82, 669–710
(1995)
[Pearl and Verma, 1991] Pearl, J., Verma, T.: A theory of inferred causation. In: Allen,
J., Fikes, R., Sandewall, E. (eds.) Knowledge Representation and Reasoning: Pro-
ceedings of the Second International Conference, pp. 441–452. Morgan Kaufmann,
New York (1991)
[Pitman, 1936] Pitman, E.: Suﬃcient statistics and intrinsic accuracy. Proceedings of
the Cambridge Philosophy Society 32, 567–579 (1936)
[Raftery, 1995] Raftery, A.: Bayesian model selection in social research. In: Marsden,
P. (ed.) Sociological Methodology. Blackwells, Cambridge (1995)

3 A Tutorial on Learning with Bayesian Networks
81
[Raftery, 1996] Raftery, A.: Hypothesis testing and model selection, ch. 10. Chapman
and Hall, Boca Raton (1996)
[Ramamurthi and Agogino, 1988] Ramamurthi, K., Agogino, A.: Real time expert sys-
tem for fault tolerant supervisory control. In: Tipnis, V., Patton, E. (eds.) Computers
in Engineering, American Society of Mechanical Engineers, Corte Madera, CA, pp.
333–339 (1988)
[Ramsey, 1931] Ramsey, F.: Truth and probability. In: Braithwaite, R. (ed.) The Foun-
dations of Mathematics and other Logical Essays. Humanities Press, London (1931);
(Reprinted in Kyburg and Smokler, 1964)
[Richardson, 1997] Richardson, T.: Extensions of undirected and acyclic, directed
graphical models. In: Proceedings of Sixth Conference on Artiﬁcial Intelligence and
Statistics, Ft. Lauderdale, FL, pp. 407–419. Society for Artiﬁcial Intelligence in
Statistics (1997)
[Rissanen, 1987] Rissanen, J.: Stochastic complexity (with discussion). Journal of the
Royal Statistical Society, Series B 49, 223–239, 253–265 (1987)
[Robins, 1986] Robins, J.: A new approach to causal interence in mortality studies
with sustained exposure results. Mathematical Modelling 7, 1393–1512 (1986)
[Rubin, 1978] Rubin, D.: Bayesian inference for causal eﬀects: The role of randomiza-
tion. Annals of Statistics 6, 34–58 (1978)
[Russell et al., 1995] Russell, S., Binder, J., Koller, D., Kanazawa, K.: Local learning
in probabilistic networks with hidden variables. In: Proceedings of the Fourteenth
International Joint Conference on Artiﬁcial Intelligence, Montreal, QU, pp. 1146–
1152. Morgan Kaufmann, San Mateo (1995)
[Saul et al., 1996] Saul, L., Jaakkola, T., Jordan, M.: Mean ﬁeld theory for sigmoid
belief networks. Journal of Artiﬁcial Intelligence Research 4, 61–76 (1996)
[Savage, 1954] Savage, L.: The Foundations of Statistics. Dover, New York (1954)
[Schervish, 1995] Schervish, M.: Theory of Statistics. Springer, Heidelberg (1995)
[Schwarz, 1978] Schwarz, G.: Estimating the dimension of a model. Annals of Statis-
tics 6, 461–464 (1978)
[Sewell and Shah, 1968] Sewell, W., Shah, V.: Social class, parental encouragement,
and educational aspirations. American Journal of Sociology 73, 559–572 (1968)
[Shachter, 1988] Shachter, R.: Probabilistic inference and inﬂuence diagrams. Opera-
tions Research 36, 589–604 (1988)
[Shachter et al., 1990] Shachter, R., Andersen, S., Poh, K.: Directed reduction algo-
rithms and decomposable graphs. In: Proceedings of the Sixth Conference on Un-
certainty in Artiﬁcial Intelligence, Boston, MA, pp. 237–244. Association for Uncer-
tainty in Artiﬁcial Intelligence, Mountain View, CA (1990)
[Shachter and Kenley, 1989] Shachter, R., Kenley, C.: Gaussian inﬂuence diagrams.
Management Science 35, 527–550 (1989)
[Silverman, 1986] Silverman, B.: Density Estimation for Statistics and Data Analysis.
Chapman and Hall, New York (1986)
[Singh and Provan, 1995] Singh,
M.,
Provan,
G.:
Eﬃcient
learning
of
selective
Bayesian network classiﬁers. Technical Report MS-CIS-95-36, Computer and Infor-
mation Science Department, University of Pennsylvania, Philadelphia, PA (Novem-
ber 1995)
[Spetzler and Stael von Holstein, 1975] Spetzler, C., Stael von Holstein, C.: Probabil-
ity encoding in decision analysis. Management Science 22, 340–358 (1975)
[Spiegelhalter et al., 1993] Spiegelhalter, D., Dawid, A., Lauritzen, S., Cowell, R.:
Bayesian analysis in expert systems. Statistical Science 8, 219–282 (1993)

82
D. Heckerman
[Spiegelhalter and Lauritzen, 1990] Spiegelhalter, D., Lauritzen, S.: Sequential updat-
ing of conditional probabilities on directed graphical structures. Networks 20, 579–
605 (1990)
[Spirtes et al., 1993] Spirtes, P., Glymour, C., Scheines, R.: Causation, Prediction, and
Search. Springer, New York (1993)
[Spirtes and Meek, 1995] Spirtes, P., Meek, C.: Learning Bayesian networks with dis-
crete variables from data. In: Proceedings of First International Conference on
Knowledge Discovery and Data Mining, Montreal, QU. Morgan Kaufmann, San
Francisco (1995)
[Suermondt and Cooper, 1991] Suermondt, H., Cooper, G.: A combination of exact
algorithms for inference on Bayesian belief networks. International Journal of Ap-
proximate Reasoning 5, 521–542 (1991)
[Thiesson, 1995a] Thiesson, B.: Accelerated quantiﬁcation of Bayesian networks with
incomplete data. In: Proceedings of First International Conference on Knowledge
Discovery and Data Mining, Montreal, QU, pp. 306–311. Morgan Kaufmann, San
Francisco (1995a)
[Thiesson, 1995b] Thiesson, B.: Score and information for recursive exponential models
with incomplete data. Technical report, Institute of Electronic Systems, Aalborg
University, Aalborg, Denmark (1995b)
[Thomas et al., 1992] Thomas, A., Spiegelhalter, D., Gilks, W.: Bugs: A program to
perform Bayesian inference using Gibbs sampling. In: Bernardo, J., Berger, J.,
Dawid, A., Smith, A. (eds.) Bayesian Statistics, vol. 4, pp. 837–842. Oxford Uni-
versity Press, Oxford (1992)
[Tukey, 1977] Tukey, J.: Exploratory Data Analysis. Addison-Wesley, Reading (1977)
[Tversky and Kahneman, 1974] Tversky, A., Kahneman, D.: Judgment under uncer-
tainty: Heuristics and biases. Science 185, 1124–1131 (1974)
[Verma and Pearl, 1990] Verma, T., Pearl, J.: Equivalence and synthesis of causal mod-
els. In: Proceedings of Sixth Conference on Uncertainty in Artiﬁcial Intelligence,
Boston, MA, pp. 220–227. Morgan Kaufmann, San Francisco (1990)
[Whittaker, 1990] Whittaker, J.: Graphical Models in Applied Multivariate Statistics.
John Wiley and Sons, Chichester (1990)
[Winkler, 1967] Winkler, R.: The assessment of prior distributions in Bayesian analy-
sis. American Statistical Association Journal 62, 776–800 (1967)

4
The Causal Interpretation of Bayesian Networks
Kevin B. Korb and Ann E. Nicholson
Clayton School of Information Technology
Monash University
Clayton, Victoria 3800, Australia
{kevin.korb,ann.nicholson}@infotech.monash.edu.au
Summary. The common interpretation of Bayesian networks is that they are vehicles
for representing probability distributions, in a graphical form supportive of human un-
derstanding and with computational mechanisms supportive of probabilistic reasoning
(updating). But the interpretation of Bayesian networks assumed by causal discov-
ery algorithms is causal: the links in the graphs speciﬁcally represent direct causal
connections between variables. However, there is some tension between these two in-
terpretations. The philosophy of probabilistic causation posits a particular connection
between the two, namely that causal relations of certain kinds give rise to probabilistic
relations of certain kinds. Causal discovery algorithms take advantage of this kind of
connection by ruling out some Bayesian networks given observational data not sup-
ported by the posited probability-causality relation. But the discovered (remaining)
Bayesian networks are then speciﬁcally causal, and not simply arbitrary representa-
tions of probability.
There are multiple contentious issues underlying any causal interpretation of Bayes-
ian networks. We will address the following questions:
•
Since Bayesian net construction rules allow the construction of multiple distinct
networks to represent the very same probability distribution, how can we come to
prefer any speciﬁc one as “the” causal network?
•
Since Bayesian nets within a Verma-Pearl pattern are strongly indistinguishable,
how can causal discovery ever come to select exactly one network as “the” causal
network?
•
Causal discovery assumes faithfulness (that d-connections in the model are ac-
companied by probabilistic dependency in the system modeled). However, some
physical systems cannot be modeled faithfully under a causal interpretation. How
can causal discovery cope with that?
Here we introduce a causal interpretation of Bayesian networks by way of answering
these questions and then apply this interpretation to answering further questions about
causal power, explanation and responsibility.
Keywords: causal discovery, faithfulness, Bayesian networks, probabilistic causality,
intervention, causal power, causal responsibility.
4.1
Introduction
In the last decade Bayesian networks have risen to prominence as the pre-
ferred technology for probabilistic reasoning in artiﬁcial intelligence, with a
D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 83–116, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

84
K.B. Korb and A.E. Nicholson
proliferation of techniques for fast and approximate updating and also for their
automated discovery from data (causal discovery, or “data mining” of Bayes-
ian networks). Philosophers of science have also begun to adopt the technology
for reasoning about causality and methodology (e.g., [2]; [23]). Both the causal
discovery and the philosophical analysis depend upon the propriety of a causal
interpretation of the Bayesian nets in use. However, the standard semantics for
Bayesian networks are purely probabilistic (see, e.g., [39]), and various of their
properties — such as the statistical indistinguishability of distinct Bayesian net-
works [53, 7] — seem to undermine any causal interpretation. As a result, skeptics
of causal interpretation are growing along with the technology itself.
4.2
Bayesian Networks
We begin with a perfectly orthodox (we hope) introduction to the concepts and
notation used in Bayesian networks (for a more detailed introduction see [28,
Chap 2]); readers familiar with these are advised to skip to the next section. A
Bayesian network is a directed acyclic graph (dag), M, over a set of variables
with associated conditional probabilities θ which together represent a proba-
bility distribution over the joint states of the variables. The fully parameterized
model will be designated M(θ). For a simple (unparameterized) example, see
Figure 4.1. Rain and Sprinkler are the root nodes (equivalently, the exogenous
variables), which report whether there is rain overnight and whether the au-
tomatic sprinkler system comes on. The endogenous (non-root) variables are
Lawn, which describes whether or not the lawn is wet, and Newspaper and Car-
pet, which respectively describe the resultant sogginess and muddiness when the
dog retrieves the newspaper. Note that we shall vary between talk of variables
and their values and talk of event types (e.g., rain) and their corresponding
token events (e.g., last night’s rain) without much ado.
Probabilistic reasoning is computationally intractable (speciﬁcally, NP-hard;
[10]). The substantial advantage Bayesian networks oﬀer for probabilistic rea-
soning is that, if the probability distribution can be represented with a sparse
Rain
Sprinkler
Newspaper
Lawn
Carpet
Fig. 4.1. A simple Bayesian network

4 The Causal Interpretation of Bayesian Networks
85
network (i.e., with few arcs), the computations become practicable. And, most
networks of actual interest to us are sparse. Given a sparse network, the prob-
ablistic implications of observations of a subset of the variables can be readily
computed using any of a large number of available algorithms [28]. In order for
the computational savings aﬀorded by low arc density to be realized, the lack of
an arc between two variables must be reﬂected in a probabilistic independence in
the system being modeled. Thus, in a simple two-variable model with nodes X
and Y , a missing arc implies that X and Y are probabilistically independent. If
they are not, then the Bayesian network simply fails to be an appropriate model.
Thus, in our example, Rain and Sprinkler must be independent of each other; if
the sprinkler system is turned oﬀin rainy weather, then Figure 4.1 is simply the
wrong model (which could be made right by then adding Rain −−▶Sprinkler).
X and Y being probabilistically independent just means that P(X = xi|Y =
yj) = P(X = xi) for any two states xi and yj. Conditional indepen-
dence generalizes this to cases where observations of a third variable may
induce an independence between the ﬁrst two variables, which may otherwise
be dependent. Philosophers, following [43], have tended to call this relation-
ship screening oﬀ. For example, Rain and Newspaper are presumably depen-
dent in Figure 4.1; however, if we hold ﬁxed the state of the lawn — say,
we already know it is wet — then they are no longer probabilistically re-
lated: P(Newspaper|Lawn, Rain) = P(Newspaper|Lawn), which we will com-
monly abbreviate as Newspaper
|=
Rain|Lawn. Given these and like facts for other
variables, Figure 4.1 is said to have the Markov property: that is, all of the
conditional independencies implied by the Bayesian network are true of the ac-
tual system (or, equivalently, it is said to be an independence map (I-map)
of the system). We shall often assume that our Bayesian networks are Markov,
since, as we have indicated, when they are not, this is simply because we have
chosen an incorrect model for our problem.
In the opposite condition, where all apparent dependencies in the network
are realized in the system, the network is said to be faithful to the system (or,
the network is called a dependence-map (D-map) of the system). A network
which both satisﬁes the Markov property and is faithful is said to be a perfect
map of the system.1 There is no general requirement for a Bayesian network to
be faithful in order to be considered an adequate probabilistic model. In particu-
lar, arcs can always be added which do nothing — they can be parameterized so
that no additional probabilistic inﬂuence between variables is implied. Of course,
there is a computational cost to doing so, but there is no misrepresentation of
the probability distribution. What we are normally interested in, however, are
I-maps that are minimal: i.e., I-maps such that if any arc is deleted, the model
is no longer an I-map for the system of interest. A minimal I-map need not
necessarily also be a perfect map, although they typically are; in particular,
there are some systems which have multiple distinct minimal I-maps.
1 The concept of “faithfulness” comes from the “faithfulness condition” of Spirtes,
et al. [47]; they also, somewhat confusingly, talk about graphs and distributions
being “faithful to each other”, when we prefer to talk about perfect maps.

86
K.B. Korb and A.E. Nicholson
We shall often be interested in probabilistic dependencies carried by particular
paths across a network. A path is a sequence of nodes which can be visited by
traversing arcs in the model (disregarding their directions) in which no node is
visited twice. A directed path proceeds entirely in the direction of the arcs
traversed. A fundamental graph-theoretic concept is that of two nodes X and
Y being d-separated by a set of nodes Z, which we will abbreviate X ⊥Y |Z.
Formally,
Deﬁnition 1 (D-separation)
X and Y are d-separated given Z (for any subset Z of variables not including
X or Y ) if and only if each distinct path Φ between them is cut by one of the
graph-theoretic conditions:
1. Φ contains a chain X1 −−▶X2 −−▶X3 and X2 ∈Z.
2. Φ contains a common causal structure X1 ◀−−X2 −−▶X3 and X2 ∈Z.
3. Φ contains a common eﬀect structure X1 −−▶X2 ◀−−X3 (i.e., an uncovered
collision with X1 and X3 not directly connected) and neither X2 nor any
descendant of X2 is in Z.
This is readily generalized to sets of variables X and Y.
The idea of d-separation is simply that dependencies can be cut by observing
intermediate variables (1) or common causes (2), on the one hand, and induced
by observing common eﬀects (or their descendants), on the other (3). As for
the latter, if we assume as above that the automated sprinkler and rain are
independent in Figure 4.1, they will not remain so if we presuppose knowledge
of the state of the lawn. For example, if the lawn is wet, something must explain
that, so if we learn that there was no rain overnight, we must increase our belief
that the sprinkler system came on.
A more formal version of the Markov property is then: a model has the
Markov property relative to a system if the system has a conditional indepen-
dence corresponding to every d-separation in the model. I.e.,
∀X, Y, Z (X ⊥Y |Z ⇒X
|=
Y |Z)
The opposite condition to d-separation is d-connection, when some path
between X and Y is not blocked by Z, which we will write X ̸⊥Y |Z. Faithfulness
of a graph is equivalent to
∀X, Y, Z (X ̸⊥Y |Z ⇒X ̸
|=
Y |Z)
A concept related to d-connection, but not the same, is that of an active path.
We use active paths to consider the probabilistic impact of observations of some
variables upon others: a path between X and Y is an active path in case an
observation of X can induce a change in the probability distribution of Y . The
concepts of d-connected paths and active paths are not equivalent because a
d-connected path may be inactive due only to its parameterization.

4 The Causal Interpretation of Bayesian Networks
87
4.3
Are Bayesian Networks Causal Models?
Such are Bayesian networks and some of their associated concepts. The illus-
trative network of Figure 4.1 is itself clearly a causal model: its arcs identify
direct causal relationships between event types corresponding to its nodes —
rain causes lawns to get wet, etc. But it is clear that there is no necessity in
this. We could represent the very same probabilistic facts with a very diﬀer-
ent network, one which does not respect the causal story, reversing some of the
arcs. Indeed, Chickering [7] introduced a transformation rule which allows us to
reverse any number of the arcs in a Bayesian network:
Rule 1 (Chickering’s Transformation Rule). The transformation of M to
M ′, where M = M ′ except that, for some variables C and E, C −−▶E ∈M
and C
◀−−E ∈M ′ (and excepting any arc introduced below), will allow the
probability distribution induced by M to be represented via M ′ so long as:
if any uncovered collision is introduced or eliminated, then a covering
arc is added (e.g., if A −−▶C −−▶E ∈M then A and E must be
directly connected in M ′).
Given any probability distribution, and any causal Bayesian network represent-
ing it, we can use Chickering’s rule to ﬁnd other, anti-causal, Bayesian networks.
Clearly, this rule only introduces arcs and never eliminates any. Thus, when start-
ing with a causal model and applying a sequence of Chickering transformations
to ﬁnd additional models capable of representing the original probability dis-
tribution, we can only start from a simpler model and reach (monotonically)
ever more complex models. For example, we can apply this rule to our Sprin-
kler model in order to ﬁnd that the alternative of Figure 4.2 can represent the
probabilities just as well. Except that, this model clearly does not represent the
probabilities just as well — it is far more, needlessly, complex. Since, in general,
parameter complexity is exponential in the number of arcs, this complexiﬁcation
is highly undesirable computationally. And the new complexity introduced by
Chickering transformations fail to represent the very same causal structure, and
under a causal interpretation they will imply falsehoods about the consequences
of interventions on its variables. The model arrived at in Figure 4.2, for example,
has a wet carpet watering the lawn.
The models in a sequence of Chickering transformations progress mono-
tonically from simpler to more complex; the set of probability distributions
representable by those models (by re-parameterizing them) form a sequence of
monotonically increasing supersets, with the ﬁnal model capable of representing
all of (and usually more than) the distributions representable by its predecessors.
Intuitively, we might come to a preference for causal models then on sim-
plicity grounds: causal models are the simplest of Bayesian networks capable
of representing the probabilistic facts (such nets we will call admissible mod-
els), whereas the transformed, non-causal networks are inherently more complex.
While we believe that this is largely true, and a suﬃcient motivation to prefer
causal over non-causal models, it is not universally true. There are true causal

88
K.B. Korb and A.E. Nicholson
Rain
Newspaper
Sprinkler
Carpet
Lawn
 
Fig. 4.2. A less simple Bayesian network
models which are more complex than alternative admissible models, although
the latter cannot be arrived at by Chickering’s rule. We shall see an example of
that below. Our real reason for preferring causal models to their probabilistic
imitators is that most of our interest in Bayesian networks arises from an in-
terest in understanding the stochastic universe around us, which comes in the
form of numerous (partially) independent causal systems. We generate models
to explain, predict and manipulate these systems, which models map directly
onto those systems — that is what it means to have a true (causal) theory of
any such system. Alternative models which arise from arbitrary redirections of
causal arcs may have some computational interest, but they have little or no
interest in scientiﬁc explanation or engineering application. The causal models
are metaphysically and epistemologically primary; the alternative models arise
from mathematical games.
So, to answer the question heading this section: most Bayesian networks are
not causal models; however, most interesting Bayesian networks are.
4.4
Causal Discovery and Statistical Indistinguishability
Causal discovery algorithms aim to ﬁnd the causal model responsible for gen-
erating some available data, usually in the form of a set of joint observations
over the variables being modeled. Since 1990, when the ﬁrst general discovery
algorithms were invented, many more have been developed. Causal discovery
algorithms include the original IC [53], PC [47], K2 [11], BDe/BGe [21], GES
[8], and CaMML [54].
Automated causal discovery is analogous to scientiﬁc discovery. It is typical
for scientiﬁc discovery to aim at ﬁnding some unique theory to explain some
data, only to ﬁnd instead some larger set of theories, all of which can equally
well explain the available data. Philosophers of science call this the problem of
underdetermination: any ﬁnite set of data can be explained by some large

4 The Causal Interpretation of Bayesian Networks
89
group of theories; adding some new data may well rule out a subset of the
theories, while the set of theories remaining consistent with the expanded data is
still large, perhaps inﬁnitely large. Karl Popper [41] based his theory of scientiﬁc
method on this observation, calling it Falsiﬁcationism: rather than verifying
theories as true, he suggested that science progresses by falsifying theories that
fail to accommodate new data; science progresses in a way vaguely similar to
evolution, with unﬁt theories dying out and multiple ﬁt theories remaining.2
Causal discovery can proceed in much the same way. We can begin with some
hypothetical set of dags (the model space) that could provide causal explana-
tions of reality and the real probability distribution, PR, to be explained.3 The
discovery process then repeatedly ﬁnds probabilistic dependencies in PR, cross-
ing oﬀall of those dags which cannot model the dependencies. In a simpliﬁed
form, the original Verma-Pearl IC algorithm can be expressed as:
1. Step I. Put an undirected link between any two variables X and Y if and
only if
for every set of variables S s.t. X, Y ̸∈S
X ̸
|=
Y |S
I.e., X and Y are directly connected if and only if they are always
conditionally dependent.
2. Step II. For every undirected structure X −Z −Y (where X and Y are
not themselves directly connected) orient the arcs X−−▶Z ◀−−Y if and only if
X ̸
|=
Y |S
for every S s.t. X, Y ̸∈S and Z ∈S.
I.e., we have an uncovered collision if and only if the ends are always
conditionally dependent upon the middle.
Following Steps I and II, a Step III checks for arc directions forced by fur-
ther considerations, such as avoiding the introduction of cycles and uncovered
collisions not revealed by PR. Collectively, these steps suﬃce to determine what
Verma and Pearl called a pattern, namely, a set of dags all of which share the
same skeleton (arc structure, disregarding orientations) and uncovered collisions.
The IC algorithm is both the ﬁrst causal discovery algorithm and the simplest
to understand; it is not, however, practical in that it relies upon direct access to
PR via some oracle. Spirtes et al. [47] redesigned it to be more practical, in their
PC algorithm, by ﬁnding some algorithmic eﬃciencies and, more importantly, by
replacing the oracle with statistical signiﬁcance tests for dependencies. Because
2 In later years Popper made this analogy more explicit, developing what he called
evolutionary epistemology [42].
3 Of course, we usually have access to PR only through statistical samples; we are
simplifying the description here.

90
K.B. Korb and A.E. Nicholson
of its simplicity, PC is now widely available, for example, in the machine learn-
ing toolbox, Weka [55]. An alternative approach to the discovery problem is to
generate a global Bayesian or information-theoretic score for candidate models
and then to search the model space attempting to optimize the score. Examples
of this are the K2, BDe/BGe and CaMML algorithms cited above. The main
diﬀerence between these two approaches, generally dubbed constraint-based and
metric discovery respectively, is that the constraint learners test for dependen-
cies in isolation, whereas the metric learners score the models based upon their
overall ability to represent a pattern of dependencies.
Considerations arising from the Verma-Pearl algorithm led directly to a the-
ory of the observational equivalence, or statistical indistinguishability, of models.
It turns out that, given the assumption of a dependency oracle, the IC algorithm
is in some sense optimal: Verma and Pearl proved that, if we restrict ourselves
to observational data, no algorithm can improve upon its power to use the ora-
cle’s answers to determine causal structure. We can formalize (strong) statistical
indistinguishability so:
Deﬁnition 2 (Strong Indistinguishability)
∀θ1∃θ2[PM1(θ1) = PM2(θ2)]
and vice versa
That is, any probability distribution representable by M1, via some parameteri-
zation θ1, is also representable by M2 via some other parameterization, and vice
versa. What Verma and Pearl [53] proved then is that two models are strongly
indistinguishable if and only if they are in the same pattern. The set of patterns
over the model space form a partition, with patterns being equivalence classes
of dags. In consequence of these results, most researchers have focused upon the
learning of patterns (or, equivalently, Markov equivalence classes), dismissing the
idea of discovering the causal models themselves as quixotic. Thus, for example,
the GES [8] stands for “Greedy Equivalence Search”; it explicitly eschews any
attempt to determine which dag within the best equivalence class of dags might
be the true causal model.
By strong indistinguishability, the dags within each equivalence class exactly
share the set of probability distributions which they are capable of representing.
So, in other words, in the hands of most researchers, causal discovery as been
turned from the search for causal models into, once again, the weaker and easier
search for probability distributions. Of course, not attempting the impossible is
generally good advice, so perhaps these researchers are simply to be commended
for their good sense. If the problem of underdetermination is unsolvable, there
is no point in attempting to solve it.
On the other hand, we might consider what human scientists do when obser-
vational data alone fail to help us, when multiple theories clash, but not over
the observational data. Suppose, for example, we were confronted with a largely

4 The Causal Interpretation of Bayesian Networks
91
isolated system that could equally well be one of these two causal models (which
are in a common pattern):
Smoking −−▶Cancer
Smoking ◀−−Cancer
Underdetermination may halt scientiﬁc discovery, but not so near to the starting
point as this! Commonly, of course, we would have more to go on than joint
observations of these two variables. For example, we would know that Smoking
generally precedes Cancer rather than the other way around. Or, we would
expand our observations, looking, say, for mutagenic mechanisms articulating
smoking and cancer. But, what would we do if we had no such background
knowledge and our technology allowed for no such mechanistic search? We would
not simply throw up our hands in despair and describe both hypotheses as equally
good. We would experiment by intervening upon Smoking (disregarding ethical
or practical issues).
Merely ﬁnding representations for our probability distributions does not ex-
haust our scientiﬁc ambitions, not by a long way. Probabilities suﬃce for predic-
tion in the light of evidence (probabilistic updating); but if we wish to understand
and explain our physical systems, or predict the impact of interventions upon
them, nothing short of the causal model itself will do.
4.5
A Loss of Faith
Before exploring the value of interventional data for the discovery process, we
shall consider another diﬃculty arising for causal modeling and for causal dis-
covery in particular, namely a lack of faith. This has become one of the central
arguments against the causal discovery research program.
The probabilistic causality research program in philosophy of science aims to
ﬁnd probabilistic criteria for causal claims [50, 46, 52]. The underlying intuition
is that, contrary to much of the philosophical tradition, causal processes are
fundamentally probabilistic, rather than deterministic. The probabilistic depen-
dencies we measure by collecting sample observations over related variables are
produced not merely by our ignorance but also, in some cases, directly by the
causal processes under study. A variety of powerful arguments have been brought
in support of this approach to understanding causality. Perhaps the most ba-
sic point is that our philosophy of science must at least allow the world to be
indeterministic. The question is, after all, synthetic, rather than analytic: inde-
terminism is logically possible, so deciding whether our world is indeterministic
requires empirical inquiry beyond any philosophizing we may do. Purely ana-
lytic philosophical argument cannot establish the truth of determinism. Turning
to empirical means to resolve the question, we will immediately notice that our
best fundamental theory of physics, quantum physics, is indeterministic, at least
in its most direct interpretation.
If, then, causal processes give rise to probabilistic structure, we should be
able to learn what causal processes are active in the world by an inverse in-
ference from the probabilistic structures to the underlying causal structure. In

92
K.B. Korb and A.E. Nicholson
other words, the probabilistic causality theory underwrites the causal discovery
research program. This is one reason why the causal discovery of Bayesian net-
works has attracted the attention of philosophers of science, both supporters and
detractors of probabilistic causality.
Patrick Suppes’ account of probabilistic causation begins with what he calls
prima facie causation [50]. C is a prima facie cause of E if the event type of C is
positively related to the event type of E (ignoring an additional temporal prece-
dence requirement); i.e., those C such that P(C|E) −P(C) > 0. We are then
to ﬁlter out spurious causes, which are any that can be screened oﬀby a com-
mon ancestor of the prima facie cause and the purported eﬀect. What remains
are the genuine causes of E. The essence of this is the identiﬁcation of potential
causes by way of probabilistic dependence.4 Of course, this is also how the IC algo-
rithm operates: both Step I and Step II posit causal structure to explain observed
dependencies. And every other method of automated causal discovery also takes
probabilistic dependence as its starting point; and they all assume that once all
probabilistic dependencies have been causally explained, then there is no more
work for causality to do. In other words, they assume the model to be discovered
is faithful: corresponding to every d-connection in a causal model there must be a
probabilistic dependence. This assumption is precisely what is wrong with causal
discovery, according to Nancy Cartwright [4] and other skeptics.
In the simplest case, where C −−▶E is the only causal process posited by
a model, it is hard to see how a presupposition of faithfulness can be contested.
Such a simple causal process which left no probabilistic reﬂection would be as
supernatural as a probabilistic reﬂection produced by nothing at all. In any case,
insisting upon the possibility of an unfaithful structure between understanding
and reality here leaves inexplicable the ability to infer the causal structure in
any circumstance.
But there are many situations where we should and must prefer an unfaithful
model. One kind of unfaithfulness is where transitivity of probabilistic depen-
dence fails. If causality is somehow based upon the kind of causal processes
investigated by Salmon [45] and Dowe [12], processes which are capable of carry-
ing information from one space-time region to another (Salmon-Dowe processes
for short), then it seems somehow causality ought to be transitive. Salmon-Dowe
processes are clearly composable: when ball A strikes B and B strikes C, the
subprocesses composed form a larger process from A to C. No doubt this kind of
Newtonian example lies behind the widespread intuition that causality must be
transitive. We share the intuition that causal processes are somehow foundational
for causal structure, and that they can be composed transitively; unfortunately
for any simple analysis, causal structure itself is not transitive. One of many
4 Suppes, as do many other advocates of probabilistic causality, directs attention to
causes which raise the probability of their eﬀects and away from those which lower
that probability — or, as Hausman [20] puts it, to the positive (promoting) causal
role of the candidate cause. We are not much concerned with causal roles (whether
promotion or inhibition) here, as such matters are primarily aimed at accounting for
ordinary language behavior.

4 The Causal Interpretation of Bayesian Networks
93
 
Hiker Survives
Hiker Ducks
Boulder Falls
Fig. 4.3. The hiker surviving
examples from Hitchcock [23] will make this clear: suppose there is a hiker on a
mountain side and at just the wrong time a boulder dislodges and comes ﬂying
towards her; however, observing the boulder, she ducks at the right moment,
and the boulder sails harmlessly past; the hiker survives. This is represented
graphically in Figure 4.3. We are to suppose that if the boulder dislodges, the
hiker will duck and survive, and that if the boulder doesn’t dislodge, she will
again survive. In this case, there is no sense in which the boulder makes a dif-
ference to survival. It would be perverse to say that the boulder has caused the
hiker to survive, or to generalize and assert that in this and relevantly similar
cases falling boulders cause hikers to survive. While causal processes, and their
inherent transitivity, are one part of the story of causality, making a diﬀerence,
probabilistic dependence, is equally part of that story, a part which here fails
dramatically.
Hiddleston [22] claims that intransitivity in all cases can be attributed to the
fact that there are multiple causal paths; when we look at component causal
eﬀects in isolation, such things cannot happen. He is mistaken. An example of
Richard Neapolitan [36] makes this clear: ﬁnesteride reduces DHT (a kind of
testosterone) levels in rats; and low DHT can cause erectile dysfunction. How-
ever, ﬁnesteride doesn’t reduce DHT levels suﬃciently for erectile dysfunction
to ensue (in at least one study); in other words, there is a threshold above which
variations in DHT have no eﬀect on dysfunction. Graphically, this is simply
represented by:
Finesteride −−▶DHT −−▶Dysfunction
Since, there is no probabilistic dependency between ﬁnesteride and erectile dys-
function, we have a failure of transitivity in a simple chain. We can equally have
failures of transitivity in simple collisions [38].5
5 Hiddleston’s mistake lies in an overly-simple account of causal power, for in linear
models, and the small generalization thereof that Hiddleston addresses, causality is
indeed transitive. Such models are incapable of representing threshold eﬀects, as is
required for the ﬁnesteride case.
We note also that some would claim that all cases of intransitive causation will be
eliminated in some future state of scientiﬁc understanding: by further investigation
of the causal mechanisms, and consequent increase in detail in the causal model, all
apparently intransitive causal chains will turn into (sets of) transitive causal chains.
This may or may not be so. Regardless, it is an a posteriori claim, and one which a
theory of causality should not presuppose.

94
K.B. Korb and A.E. Nicholson
Thrombosis
Pill
Pregnancy
+
−
+
 
Fig. 4.4. Neutral Hesslow
Pregnancy
−
+
Thrombosis
Pill
 
Fig. 4.5. Faithful Hesslow
Another kind of unfaithfulness does indeed arise by multiple paths, where
individual arcs can fail the faithfulness test. These include “Simpson’s paradox”
type cases, where two variables are directly related, but also indirectly related
through a third variable. The diﬃculty is most easily seen in linear models, but
generalizes to discrete models. Take a linear version of Hesslow’s example of the
relation between the Pill, Pregnancy and Thrombosis (Figure 4.4). In particular,
suppose that the causal strengths along the two paths from Pill to Thrombosis
exactly balance, so that there is no net correlation between Pill and Thrombosis.
And yet, the above model, by stipulation, is the true causal model. Well, in that
case we have a failure of faithfulness, since we have a direct causal arc from Pill
to Thrombosis without any correlation wanting to be explained by it. In fact,
causal discovery algorithms in this case will generally not return Figure 4.4, but
rather the simpler model (assuming no temporal information is provided!) of
Figure 4.5. This simpler model has all and only the probabilistic dependencies
of the original, given the scenario described.
A plausible response to this kind of example, the response of Spirtes et al. [48],
is to point out that it depends upon a precise parameterization of the model. If
the parameters were even slightly diﬀerent, a non-zero correlation would result,
and faithfulness would be saved. In measure theory (which provides the set-
theoretic foundations for probability theory) such a circumstance is described
as having measure zero — with the implication that the probability of this
circumstance arising randomly is zero (see [48], Theorem 3.2). Accepting the
possibility of zero-probability Simpson-type cases implies simply that we can go
awry, that causal discovery is fallible. But no advocate of causal discovery can

4 The Causal Interpretation of Bayesian Networks
95
reasonably be construed as having claimed infallibility.6 The question at issue
cannot be the metaphysical claim that faithfulness must always be maintained —
otherwise, how could we ever come to admit that it had been violated? Rather,
it is a methodological proposal that, until we have good reason to come to
doubt that we can ﬁnd a faithful model, we should assume that we can. In the
thrombosis case, with precisely counterbalancing paths, we begin with knowledge
that the true model is not faithful, so we are obliged to abandon faithfulness.
Cartwright objects to this kind of saving maneuver. She claims that the “mea-
sure zero” cases are far more common than this argument suggests. In particular,
she points out that many systems we wish to understand are artiﬁcial, rather
than natural, and that in many of these we speciﬁcally want to cancel out dele-
terious eﬀects. In such cases we can anticipate that the canceling out will be
done by introducing third variables associated with both cause and eﬀect, and
so introducing by design a “measure-zero” case. In addition, there are many nat-
ural cases involving negative feedback where we might expect an equilibrium to
be reached in which an approximate probabilistic independency is achieved. For
example, suppose that in some community the use of sun screen is observed to be
unrelated to skin cancer. Yet the possible causal explanation that sun screen is
simply ineﬀective may be implausible. A more likely causal explanation could be
that there is a feedback process such that the people using the sun screen expose
themselves to more sunlight, since their skin takes longer to burn. If the people
modulate their use of sun screen according to their exposure to the sun, then
their total UV exposure would remain the same. Again, Steel [49] has pointed
out that there are many cases of biological redundancy in DNA, such that if the
allele at one locus is mutated, the genetic character will still be expressed due to
a backup allele; in all such cases the mutation and the genetic expression will fail
the faithfulness test. As Steel emphasizes, the point of all these cases is that the
measure-zero premise fails to imply the probability zero conclusion: the system
parameters have not been generated “at random” but as a result of intelligent
or evolutionary design, leading to unfaithfulness.
If these cases posed some insurmountable burden for causal discovery algo-
rithms, this would surely justify skepticism about automating causal discovery,
for clearly we humans have no insurmountable diﬃculties in learning that the
sun causes skin cancer, etc., even if these relations are also not easy to learn.
However, it turns out there are equally clear, if again practically diﬃcult, means
available for machines to discover these same facts.
4.6
Intervention
So, it is time to see what interventions can do for causal discovery.
The concept of causal intervention and its uses in making sense of causal
models have been receiving more attention recently, as, for example, in [56].
6 Cartwright notwithstanding: “Bayes-net methods. . . will bootstrap from facts about
dependencies and independencies to causal hypotheses—and, claim the advocates,
never get it wrong” [4, p. 254] (italics ours). Here, Cartwright’s straw-man has it wrong.

96
K.B. Korb and A.E. Nicholson
Indeed, both Pearl [40] and Spirtes et al. [48] treat intervention in some detail
and provide a foundation for much of our work, and yet they have not applied
interventions to resolve the problems raised by faithlessness and statistical in-
distinguishability. We now indicate how these are resolvable using interventions,
ﬁrst dealing with loss of faith. In empirical science, when observational data
fail to diﬀerentiate between competing hypotheses, a likely response is to go be-
yond observation and experimentally intervene in nature: if we hold ﬁxed known
alternative causes of cancer and apply and withold a candidate carcinogen to
experimental and control groups respectively, we can be in a position to resolve
the issue of what the true causal model is, whether or not it is faithful to observ-
able dependency structures. Intervention and experiment would seem to have
the power to resolve our conﬂict between truth, on the one hand, and simpler,
faithless models, on the other.
In order to explore the epistemological power of intervention, we will consider
a particular kind of intervention, with some ideal features. Much of the literature
idealizes every possible feature (e.g., the do-calculus [40], and the manipulation
theorem of [48]): interventions are themselves uncaused (they are root nodes in
a causal model), and so multiple interventions are uncorrelated; interventions
impact upon exactly one variable in the original model; interventions are deter-
ministic, deﬁnitely resulting in the intervened upon variable adopting a unique
state. Such extreme idealization is not a promising starting point for a general
theory of causal modeling, and the actual interventions available to us often fall
well short of the ideal (cf. [27]). For our purposes here, however, we shall adopt
all of them except the last: by default our interventions inﬂuence their target
variables, but do not to the extreme of cutting oﬀall inﬂuence of their prior par-
ents (but, should they do so, we shall indicate this by calling the interventions
perfect).7 The extreme, perfect interventions can be represented in Bayesian net-
works simply by setting the target variable to the desired state and cutting all of
its inbound arcs. Our less perfect interventions cannot be so represented; existing
parents retain their arcs to the target variable. So, the natural representation of
our interventions is by augmenting our dags with new intervention variables.
It is worth noting, as an aside, that this approach makes clear the diﬀerence
between intervention and observation. Since most existing tools only provide ex-
plicit means for probabilistic updating under observation,8 some have attempted
to understand the eﬀect of causal interventions upon a variable X simply by set-
ting X to some desired value in a Bayesian network and updating. Such updates,
however, can radically mis-estimate the eﬀects of interventions. For example,
consider Figure 4.6. HT40 describes blood pressure (hypertension) at age 40,
HT50 the same at age 50 and CHD50+ the coronary status of a subject from
ages 50 to 60. We may be contemplating an intervention to reduce blood pressure
at age 50, which may well reduce the chances of a heart attack over then next ten
years. If, however, we were hypothetically to observe the same reduced level of
7 We will assume that our interventions have some positive probability of aﬀecting
the target variable; indeed, we shall assume the same for every parent variable.
8 For a description of a tool, The Causal Reckoner, that does more, see [27].

4 The Causal Interpretation of Bayesian Networks
97
HT40
HT50
CHD50+
Fig. 4.6. Hypertension and coronary heart disease
blood pressure at age 50 as we might achieve by intervention, the resulting risk
of heart attack would be even lower. Technically, that is because there are two
d-connecting paths from HT50 to CHD50+, whereas with a perfect intervention
the link from HT40 to HT50 is cut, so there is only one path from HT50 to
CHD50+.9 What this means non-technically is that should we observe someone
at 50 with lower blood pressure, that implies the she or he also had lower blood
pressure at age 40, which, entirely independently of blood pressure at 50, has
an eﬀect on CHD50+; whereas if we intervene at 50 we are not thereby gaining
any new information about HT40. In short, in order to determine the eﬀects
of intervention, simply using observations and updating with standard Bayesian
net tools is radically wrong.
So, we prefer to represent interventions quite explicitly, by putting new in-
tervention variables into our graphs. Furthermore, in order to test the limits
of what we can learn from intervention we consider fully augmented models,
meaning those where every original variable X gets a new intervention parent
IX, doubling the number of variables. In the case of the two Hesslow models,
faithless (true) and faithful (false), full augmentation results in Figure 4.7.
What we suggest, then, is that the argument over whether faithfulness is an
acceptable assumption for causal discovery is simply misdirected, ignoring the
power of interventions. Faithfulness is not the issue; the real issue is admissibil-
ity under augmentation: A causal model M is admissible under augmentation
if and only if its fully augmented model M ′ is capable of representing the sys-
tem’s fully augmented probability distribution.
Spirtes, Glymour and Scheines (SGS) [48] proved a theorem about augmenta-
tion which already suggests the value of interventions for distinguishing between
causal models:
SGS Theorem 4.6. No two distinct causal models that are strongly indistin-
guishable remain so under intervention.10
This has a trivial corollary:
Corollary 1. Under intervention, no two distinct causal models are strongly
indistinguishable.
9 And with an imperfect intervention, although the link from HT40 to HT50 is not
cut, the inﬂuence of HT40 on CHD50+ is reduced.
10 Note that where we write of augmenting models with intervention variables, Sprites
et al. [48] talk about “rigid indistinguishability”, which amounts to the same thing.

98
K.B. Korb and A.E. Nicholson
Pill
Pregnancy
Thrombosis
+
+
−
 
I
I
P
Pr
T
I
Thrombosis
Pregnancy
Pill
+
−
IT
IP
IPr
 
Fig. 4.7. Augmented Hesslow models: the faithless, but true, causal model (top); the
faithful, but false, model (bottom)
These results tell us that when we augment models, even strongly indistinguish-
able models, we can then ﬁnd some probability distribution that will distinguish
them. Unfortunately, this does not provide much practical guidance. In particu-
lar, it tells us nothing about the value of intervention when we are dealing with
a speciﬁc physical system and its actual probability distribution, as we typically
are in science. But we can also apply intervention and augmentation to answer-
ing questions about particular physical systems rather than the set of all possible
physical systems.
First, we can show that the neutral Hesslow system M1 of Figure 4.4 can be
distinguished from its faithless imposter M2 of Figure 4.5. Under these circum-
stances, the probability distributions asserted by these models are identical; i.e.,
PM1(θ1) = PM2(θ2). Then:
Theorem 2 (Distinguishability under Imperfect Intervention; [30])
If PM1(θ1) = PM2(θ2), then under imperfect interventions PM′
1(θ1) ̸= PM′
2(θ2)
(where M1 and M2 are the respective structures of Figures 4.4 and 4.5).
The proof is in [30] and is related to that of SGS Theorem 4.6 (we also proved
there the same result for perfect interventions). The notable diﬀerence is that we
begin with a particular probability distribution, that of the true Hesslow model,

4 The Causal Interpretation of Bayesian Networks
99
and a particular inability to distinguish two models using that distribution.11
Using Wright’s path modeling rules [57] we are able to ﬁnd the needed diﬀer-
ence between the augmented models, when there is none in their unaugmented
originals. This diﬀerence arises from the introduction of new uncovered colli-
sions under augmentation: every intervention variable induces a collision with
its target variable and any pre-existing parent variable. As the Verma-Pearl algo-
rithm is sensitive to such dependency structures (and likewise every other causal
discovery algorithm), the dependency structures of M1 and M2, while param-
eterized to be identical under observation, cannot be so parameterized under
intervention.
Although this theorem is particular to cases isomorphic to the neutral (and
linear) Hesslow case, the result is of much wider interest than that suggests, since
the neutral Hesslow structure is the only way in which a true linear causal model
can be unfaithful to the probability distribution which it generates, through
balancing multiple causal paths of inﬂuence. Perhaps of more interest will be
interventions upon discrete causal models, which are more commonly the center
of attention in causal discovery and which certainly introduce more complexity
than do linear models. In particular, in non-linear cases there are many more
opportunities for intransitivities to arise, both across multiple paths and, unlike
linear models, across isolated paths. Nevertheless, similar theoretical results are
available for discrete causal models, although, as their description and proof are
more complicated, we skip them here (see instead [38]).
But the power of intervention to reveal causal structure goes well beyond these
preliminary theorems [29]. It turns out that a comprehensive regime of interven-
tions has the power to eliminate all but the true causal model from consideration.
In other words, despite the fact that strong indistinguishability can leave a very
large number of causal models equally capable of accommodating observational
data, interventional data can guarantee the elimination of all models but one,
the truth (this is the combined eﬀect of Theorems 5 and 6 of [29]). Again, these
results are generalizable to discrete models, with some additional restrictions
upon the interventions needed to cope with the more complex environment [38].
Current research, at both Carnegie Mellon University and Monash University,
is aimed at determining optimal approaches to gathering interventional data to
aid the causal discovery process. None of these results are really surprising: they
are, after all, inspired by the observation that human scientists dig beneath the
readily available observations by similar means.
Neither the presumption of faithfulness by causal discovery algorithms nor
their inability to penetrate beneath the surface of strong statistical indistin-
guishability oﬀer any reason to dismiss the causal interpretation of Bayesian
networks nor their automated discovery. The common view to the contrary is
plausible only when ignoring the possibility of extending causal modeling and
causal discovery by intervening in physical systems. Even if such interventions
11 In SGS terminology, this is an application of the idea of rigid distinguishability to
models which are weakly indistinguishable, that is, having some probability distri-
bution which they can both represent.

100
K.B. Korb and A.E. Nicholson
are expensive, or even presently technologically impossible, that is no principled
reason for rejecting causal interpretation. The history of human science is replete
with examples of competing theories remaining indistinguishable for centuries,
or millenia, before technological advances have decisively found in favor of one
over another. Witness Copernican and Ptolemaic astronomy, continental drift
versus static geological theory, evolution theory versus static species theory. All
of these are decisively settled today. In no case was the truth of no interest
prior to the technological advances which allowed their settlement: on the con-
trary, interest in settling the issues has typically driven those very technological
advances.
4.7
Causal Explanation
So far, we ﬁnd that the causal interpretation of Bayesian nets is a reasonable
one. At least, the arguments thrown up against it have failed to show otherwise,
which is less than a clear justiﬁcation for adopting a causal interpretation, but
more than nothing. A principled justiﬁcation would perhaps rely upon a serious
exploration of the metaphysics of causation. We shall not attempt that, although
in the next sections we will direct interested readers to some of the relevant
current discussions in the philosophy of causation. A pragmatic justiﬁcation is
more to our taste: having dispelled the known objections, the success of causal
modeling and causal discovery in developing an application technology for AI
that is usable and used, and which presupposes the causal interpretation, should
settle the argument. Norsys Corp., developer of the Bayesian network tool Netica,
lists a considerable variety of real-world applications:
•
Agricultural Yield for predicting the results of agricultural interventions [3]
•
PROCAM model for predicting coronary heart disease risk [51]
•
Wildlife Viability for predicting species viability in response to resource man-
agement decisions [34]
•
Risk Assessment Fusion combining multiple expert opinions into one risk
assessment [1]
There is much more work that can be done in making sense of Bayesian
networks in causal terms. One such eﬀort is to provide an account of the ex-
planatory power of a cause for some eﬀect. For example, to what extent can we
attribute the current prevalence of lung cancer to smoking? This question about
type causality can also be particularized in a question about token causality,
as in: Was Rolah McCabe’s terminal lung cancer due to her smoking cigarettes
produced by British American Tobacco? We defer such questions about token
causation to the section §4.9.
The type causality question is an old issue in the philosophy of science; what
makes it also a new issue is the prospect of tying such an account to Bayesian
networks, so that we might have computer assisted causal reasoning. Such an
attempt is already on oﬀer in the work of Cheng and Glymour [6, 15]. They
deﬁne a concept of causal power for binomial variables, which, in the case of

4 The Causal Interpretation of Bayesian Networks
101
causes which promote their eﬀects (rather than inhibit them, which takes a
diﬀerent equation), is:
pc = P(e|c) −P(e|¬c)
1 −P(e|¬c)
The numerator corresponds to the probabilistic dependence used as a criterion
by Suppes of prima facie causation. Cheng and Glymour get the eﬀect of Suppes’
ﬁltering out of spurious cases of causation by imposing structural restrictions on
the Bayesian networks which source the probabilities required to deﬁne pc. So,
the result is a causal power measure for genuine causation which is proportional
to positive probabilistic dependence, relative to the background frequency of ¬e
when the cause is absent.
This is a plausible start on explanatory power in Bayesian networks; unfortu-
nately there is no ﬁnish. Cheng and Glymour’s deﬁnition is restricted to binomial
networks with parent variables which fail to interact (as, for example, an XOR
interacts).12 Furthermore, as Glymour [14] notes, the restrictions they impose
on allowable Bayesian networks are equivalent to requiring them to be noisy-OR
networks. The result is that for these, and also for the extension of Hiddleston
[22], causal power is transitive, whereas we have already seen in Section §4.5 that
probabilistic dependence over causal links (and, hence, a key ingredient for any
useful deﬁnition of causal power) is intransitive. A diﬀerent account is wanted.
We prefer to identify causal power with an information-theoretic measure re-
lated to mutual information [25]. The mutual information of C for E (or vice
versa) is a measure of how much you learn about one variable when the other
variable is observed. Hope and Korb’s causal power diﬀers from mutual infor-
mation per se in a number of ways. First, it is asymmetric; it is required that
it be attributed to the cause, rather than the eﬀect, according to the ancestral
relations indicated by the causal model. Also, the relevant probabilities are rel-
ativized to a context of interest. That is, any causal question is raised in some
context and a measure of causal power needs to be sensitive to that context. For
example, the causal power of smoking for lung cancer may be fairly high in some
populations, but it is non-existent amongst those who already suﬀer from lung
cancer. Finally, the causal power of C is measured according to a hypothetical
perfect intervention upon C and not based upon mutual information computed
by observations of C. Thus, if Sir Ronald Fisher’s [13] speculative defence of
smoking were actually true, i.e., that the true model were
Smoking ◀−−Gene −−▶Cancer
rather than
Smoking −−▶Cancer
then the causal power of Smoking for Cancer would be nil, whereas the mutual
information between the two is unaﬀected by the change of structure.
12 However, Novick and Cheng [37] relax this restriction to some extent by considering
pairwise parental interactions.

102
K.B. Korb and A.E. Nicholson
This measure of causal power takes full advantage of Bayesian networks: all
relevant interactions between causal variables are automatically taken into ac-
count, since the computation of the information-theoretic measure depends upon
the underlying computations of the Bayesian network. As a result, for example,
all cases of intransitivity found by Hitchcock and others yield end-to-end causal
powers of zero, as is desirable.
This causal power theory also shows considerable promise for answering a
pressing practical need in the application of Bayesian network technology, namely
making the networks easier to interpret. By providing a means to query any such
network about the implications of proposed interventions (of any type) it is no
longer necessary for users themselves to follow and account for causal inﬂuences
across multiple paths. Furthermore, by separating questions of causal power from
those of probabilistic updating we can reduce the temptation to attempt to ﬁnd
causal explanations using tools which only answer questions about probabilistic
updating, confusing any causal story rather than illuminating it.
The concept of causal power is a necessary ingredient for a full account of type
causality, i.e., causal relations between types of events, rather than particular
evens (token causality, treated in section §4.9). In some sense, causal Bayesian
networks without any variables instantiated provide as full an account of the
type causal relations between its variables as could be imagined (assuming they
are true models of reality, of course). However, there remains analytical work
to do beyond such an observation. In Hesslow’s neutral model, for example,
does the Pill cause Thrombosis? The net eﬀect — the net causal power — is,
as we have noted, nil. However, type causal questions are typically aimed at
determining whether there is some way (consistent with any explicitly provided
context) for the cause to have an impact on the eﬀect. In order to answer such a
question, (type) causal paths need to be considered in isolation, for example the
Pill −−▶Thrombosis path isolated from the path through Pregnancy, by ﬁxing
the value of the latter variable [23].13 The type causal question can be answered
aﬃrmatively if any such isolated path has a non-zero causal power. Of course,
we are commonly interested also in knowing how important a cause may be: for
that we need the non-zero causal power itself.
4.8
Causal Processes
Now we will consider how Bayesian networks can help us make sense of token
causality: claims about the particular responsibility of particular causal happen-
ings for particular outcomes. We begin by sketching some relevant philosophical
background, for we believe the philosophy of causal processes is now needed.
There are two distinct approaches which in recent times have dominated at-
tempts to come to grips with the notion of causality. One is the probabilistic
13 In addition to that, type causal questions need to be relativized to an objectively
homogeneous context, so that the causal power being computed is not an average of
disparate powers in distinct contexts, as Cartwright [5] has eﬀectively argued (see
also [52]).

4 The Causal Interpretation of Bayesian Networks
103
causality research program, already introduced. The other is the attempt to lo-
cate a supervenience base for causal relationships in an underlying metaphysics
of process, initiated by Salmon [45] and furthered by Dowe [12]. Processes are
contiguous regions of space extended through some time interval — i.e., space-
time “worms”. Of course, they can’t be just any such slice of spacetime; most
such slices are causal junk [26]. The Salmon-Dowe research program is largely
aimed at coming up with clear criteria that rule out junk, but rule in pro-
cesses which can sustain causal relationships. Intuitively, we can say legitimate
processes are those which can carry information from one spacetime region to
another (“mark transmission” is what Salmon called this; Dowe calls it “con-
serving physical quantities”). Examples are ordinary objects (balls carry around
their scratches) and ordinary processes (recipes carry their mistakes through to
the end). Non-examples are pseudo-processes and pseudo-objects (e.g., Platonic
objects, shadows, the Void of Lewis [33]). Hitchcock [24] rightly points out that,
thus far, this account leaves the metaphysics of causal processes unclear. The
Salmon-Dowe research program is incomplete. But we know of no reason to be-
lieve it is not completable, so for the purposes of our discussion we shall describe
as “Salmon-Dowe processes” those which fall under some future completed anal-
ysis of this type.
If it is causal processes which ground the probabilistic dependencies between
variables, then it must be possible to put the variables within a single model into
relation with one another via such processes. This suggests a natural criterion
of relevance to require of variables within a single causal model: namely, if two
variables appear in a causal model, there must be a sequence of possible or
actual causal processes connecting them. This makes precise Hitchcock [23], who
vaguely requires that pairs of variables not be “too remote” from each other. Note
that we do not demand a possible sequence of causal processes between any two
variables, but a sequence of possible processes: it may be, for example, that two
events are spacewise separated, yet mediated by a common third event. Nor, of
course, do we demand actual processes between any event types in the model.
Probabilistic dependency is founded upon possibilities, realized and unrealized.14
The two approaches to understanding causality, dependency and process, have
disparate strengths and weaknesses. This disparity has led many to suggest that
there is no one concept of causality and that attempts to provide a uniﬁed ac-
count are confused.15 While we agree that there may well be various distinct con-
cepts of causality, we are unconvinced that the particular disparity between the
dependence and process analyses argues for two concepts of causality. Instead,
14 That there are causal processes behind the arcs of causal models suggests the an-
swer to one of the concerns about causal modeling that Nancy Cartwright raises,
namely that causal reality may not be made up of discrete token events, but per-
haps continuous processes instead [4]. Well, we expect that reality is made up of
token processes, whether discrete or continuous. Discrete Bayesian networks are a
convenient way of modeling them, and the variables we choose are convenient and
useful abstractions. They need to be tied to the underlying reality in certain ways,
but they certainly do not need to be exhaustive descriptions of that reality.
15 Hitchcock has suggested this, e.g., in Hitchcock (2004a, b); see also [16].

104
K.B. Korb and A.E. Nicholson
Tee
Tree
Hole−in−One
 
Fig. 4.8. Rosen’s hole-in-one
we propose a causal uniﬁcation program: that we develop a unifying account
that uses the strengths of the one to combat the weaknesses of the other.
Dependency accounts characteristically have diﬃculties dealing with negative
relevance, that is, causes (promoters, in role language) which in some token
cases are negatively relevant to the eﬀect (i.e., prevent it), or vice versa. Deb-
orah Rosen [44] produced a nice example of this in response to Suppes [50]. In
Figure 4.8 Rosen has struck a hole-in-one, but in an abnormal way. In particular,
by hooking into the tree, she has lowered her chance of holing the ball, and yet
this very chance-lowering event is the proximal cause of her getting the hole-
in-one. The only hope of salvaging probability-raising here, something which all
of the dependency accounts mentioned above wanted, is to reﬁne the reference
class from that of simply striking the tree to something like striking the tree
with a particular spin, momentum, with the tree surface at some exact angle,
with such-and-such wind conditions, etc. But the idea that we can always reﬁne
this reference class in enough detail to recover a chance-raising reference class is
far-fetched. It is what Salmon [46] described as pseudo-deterministic faith.16 In
any case, as Salmon also pointed out, we can always generate chance-lowering
causes in games, or ﬁnd them in quantum-mechanical scenarios, where there
is no option for reﬁnement. Take Salmon’s “cascade” [46], where a quantum-
mechanical system undergoes state changes with the probabilities indicated in
Figure 4.9. Every time such a system reaches state d via state c rather than state
b, it has done so through a chance-lowering event of transition into state c. By
construction (in case this is a game, by nature otherwise) there is no reﬁnement
of the intermediate state which would make the ﬁnal transition to state d more
probable than the untaken alternative path through b; hence, probability-raising
alone cannot account for causality here.
16 Note that the escape by contrasting striking the tree with missing it fails on at
least two counts. Of course, missing the tree, given the hook, is a contrast class
with a lower probability of success than hitting the tree. But we are attempting to
understand causality relative to a given causal model. And this maneuver introduces
a new variable, namely how the ball is hit (or, perhaps, its general direction), so the
maneuver is strictly evasive. Secondly, if we are going to countenance new variables,
we can just introduce a local rule for this hole: just behind the tree is a large net;
landing in the net also counts as holing the ball.

4 The Causal Interpretation of Bayesian Networks
105
a
e
c
b
1/4
1/4
3/4
3/4
1
d
 
Fig. 4.9. Salmon’s quantum-mechanical cascade
Salmon’s way out was to bite the bullet: he asserted that the best we can do
in such cases is to locate the event we wish to explain causally (transition to
d) in the causal nexus.17 If the transition to d occurs through b, then everyone
is happy to make the causal attribution; but if it has occurred through c, then
it is no less caused, it is simply caused in a less probable way. Insisting on the
universal availability of promoting causes (probability raising) is tantamount to
the pseudo-deterministic faith he denounced [45, chapter 4]. Instead of reliance
upon the universal availability of promoters, Salmon asked us to rely upon the
universal availability of Salmon-Dowe causal processes leading from each state
to the next. This seems the only available move for retaining irreducible state
transitions within the causal order.
Assuming, per above, that the metaphysics of process has been completed,
the problem remains for Salmon’s move that it is insuﬃcient. For one thing, as
we saw above, causal processes are composable: if we can carry information from
one end to the other along two processes, then if we connect the processes, we can
carry the (or, at any rate, some) information from the composite beginning to
the composite end. But the many cases of end-to-end probabilistic independency
need to be accommodated; the possibility of causal intransitivity needs to be
compatible with our criteria of causality. Hence, invoking causal processes cannot
suﬃce.
A pure causal process account has other problems as well. Whereas prob-
ability raising clearly itself is too strong a criterion, missing minimally every
least probable outcome within a non-trivial range of outcomes, simply invoking
causal process is clearly too weak a criterion. In some sense the Holists are right
that everything is connected to everything else; at any rate, everything within a
lightcone of something else is likely to have a causal process or potential process
relating the two. But while it makes sense to assert that the sun causes skin
cancer, it makes little sense to say that the sun causes recovery from skin can-
cer. Yet from the sun stream causal processes to all such events, indeed to every
event on earth. Salmon’s account of 1984 lacked distinction.
It is only in adding back probabilistic dependencies that we can ﬁnd the
lacking distinction. Positive dependencies, of course, have diﬃculties dealing with
negative relevance; processes do not. Processes alone cannot distinguish relevant
from irrelevant connections; probabilistic dependencies can. Plausibly what is
17 In Salmon’s terms, within an objectively homogeneous reference class.

106
K.B. Korb and A.E. Nicholson
wanted is an account of causal relevance in terms of processes-which-make-a-
relevant-probabilistic-difference. Two accounts which provide this are those of
Menzies [35] and Twardy and Korb [52]. What we will do here, however, is apply
these two concepts of causal process and diﬀerence making to making sense of
causal responsibility.
4.9
Causal Responsibility
Many of the more pressing questions that arise about causality concern questions
of responsibility, legal or moral. These are questions about particular facts, that
is, particular events and their particular causal relationships. Or, in other words,
questions about token causality (“actual causality”) rather than type causality.18
Incidentally, much of the philosophical literature on causality focuses on token
causality and stories told about token causality; we shall treat some of them
here.
It has always been clear that type and token causality, while distinct, are
related, but the relationship is itself not clear. Causal modeling with Bayesian
networks provides an opportunity for getting that relationship clear, by providing
an opportunity to establish criteria for both based upon the same model. Our
analysis aims at putting type-token causality into a kind of general-to-particular
relationship. And what we will do here is to outline a plausible account of token
causality, one that needs work to be complete, but appears to us to be aimed in
the right direction.
Our treatment is based upon a presumption that we have in hand the right
causal model. That is, what our analysis asserts is, or is not, a cause of some
particular event, depends upon the causal model assumed to be true; the token
causality analysis itself does not provide guidance in ﬁnding that true model. We
will instead rely upon certain principles of model building which arise from the
prior discussion, although we do not defend them explicitly (for a more complete
defence see [30]):
Principle 1 (Intervention): Variables in a causal model must be intervenable.
Principle 2 (Distinction): Every pair of variables in a causal model must have
a physically possible intervention which, in some physically possible context,
aﬀects the distribution of one variable without aﬀecting that of the other.
Principle 3 (Process): If two variables appear in a causal model, there must be
a sequence of possible or actual causal processes connecting them.
The ﬁrst eﬀorts to provide a Bayesian-net based analysis of token causality
were those of Hitchcock [23] and Halpern and Pearl [17, 18]. Hitchcock’s is a
simpliﬁcation of [17], which is arguably superior in some ways but more complex
18 To be sure, any satisfying account of either legal or moral responsibility goes beyond
a satisfying account of token causality alone, since it will have to incorporate a
treatment of legal or moral principles and their application to causal questions. We
will not enter into such matters here.

4 The Causal Interpretation of Bayesian Networks
107
than we care to deal with here. The diﬃculties we will point out with Hitchcock’s
treatment carry through transitively to that of Halpern and Pearl.
Consider again the case of Hitchcock’s hiker (Figure 4.3). Clearly, what we want
to say is that boulders do cause death in such circumstances, if only because human
responses are fallible, so the type relations are right in that model — each arc corre-
sponds to a type causal relation that manifests itself in a probabilistic dependency
under the right circumstances.19 But in the particular case — to be sure, idealisti-
cally (deterministically) described — the boulder’s fall does not aﬀect survival in
any way, because there is no probabilistic dependency between the two.
Hitchcock [23] describes two plausible criteria for token (actual) causality.
Both of them look at component eﬀects, by isolating some causal path of interest.
The ﬁrst is very simple. Let’s call it H1 (following [22]).
H1: C = c actually caused E = e if and only if both C = c and
E = e occurred and when we iterate through all Φi ∈Paths(C, E), for
some such Φi if we block all the alternative paths by ﬁxing them at their
actually observed values, there is a probabilistic dependence between C
and E.
In application to Hitchcock’s hiker’s survival, this works perfectly. Considering
the direct path Boulder −−▶Survival, we must ﬁx Duck at true, when there is
no probabilistic dependency. The second path (through Duck) doesn’t need to
be considered, since there is no variable mediating the path alternative to it, so
there is no question of blocking it.
 
Hiker Ducks
Trajectory
Hiker Survives
Boulder Falls
Fig. 4.10. The hiker surviving some more
The second path could, of course, be considered if we embed the model of
Figure 4.3 in a larger model with a variable that mediates Boulder and Survival.
We could model the trajectory of the boulder, giving us Figure 4.10. In this
case, H1 gets the wrong answer, since we are now obliged to ﬁx Trajectory and
19 Of course, we would never say “Boulders falling cause survival.” But that’s because in
our speech acts causal role ordinarily leaks into causal attributions. We are not here
interested in a theory of ordinary language utterances about causality.

108
K.B. Korb and A.E. Nicholson
discover that there is now a probabilistic dependency between Boulder and Sur-
vival. In particular, if the boulder doesn’t fall, but somehow regardless achieves
its original trajectory, then the hiker won’t have ducked and will end up dead.
Hitchcock’s response to this possibility is to say that the introduction of Tra-
jectory requires a “sophisticated philosophical imagination” — we have to be
able to imagine the boulder miraculously appearing on collision course without
any of the usual preliminaries — and so an account of actual causation for the
ordinary world needn’t be concerned with it. Hiddleston objects to this as an
ad hoc maneuver: he suspects that variables will be called miraculous when and
only when they cause trouble for our analysis. However, he is mistaken. Our In-
tervention Principle makes perfectly good sense of Hitchcock’s response. Either
Trajectory is intervenable (independently of Boulder) or it is not. If it is not,
then modeling it is a mistake, and H1’s verdict in that case is irrelevant. If it is
intervenable, then there must be a possible causal process for manipulating its
value. A possible example would be: build a shunt aimed at the hiker through
which we can let ﬂy another boulder. For the purposes of the story, we can keep
it camouﬂaged, so the hiker has no chance to react to it. All of this is possible, or
near enough. But in order to introduce this variable, and render it intervenable,
we have to make the original story unrecognizable. Hitchcock’s criterion, just as
much as ours to follow, is model-relative. The fact that it gives diﬀerent answers
to diﬀerent models is unsurprising; the only relevant question is what answer it
gives to the right model.
This reveals some of the useful work our model-building principles do in ac-
counting for actual causation, even before considering the details of any explicit
criterion.
H1 handles a variety of examples without diﬃculty. For example, it copes with
the ordinary cases of pre-emption which cause problems for dependency theo-
ries. Thus, in Figure 4.11 if a supervisor assassin ﬁres at the victim if and only
if the trainee assassin doesn’t ﬁre and, idealistically again, neither the trainee
nor supervisor can miss, then an account requiring end-to-end dependency, such
as Lewis’s original counterfactual analysis of causation [31], fails. In particular,
should the trainee ﬁre, this action will not be considered the cause of the vic-
tim’s death, since there is no dependency. In the face of such counterexamples,
Lewis adopted a step-wise dependency of states of the bullet as it traverses the
distance to the victim. Although there is no end-to-end dependency, if we take
the transitive closure of step-by-step dependencies, we ﬁnd end-to-end causa-
tion. We ﬁnd this objectionable on two counts: ﬁrst, as we have seen, causation
is not transitive; second, ﬁnding the intermediate dependencies requires generat-
ing intermediate variables, and so altering the causal story in unacceptable ways.
Hitchcock’s H1, on the other hand, has it easy here: we simply observe that the
supervisor did not ﬁre and that under this circumstance there is a dependency
between the trainee’s action and the victim’s health.
This is an example of pre-emption by “early cutting”. Pre-emption can also
occur through late cutting. If Billy and Suzy are each throwing a rock at a bottle
(and, as usual, they cannot miss) and if Suzy throws slightly earlier than Billy,

4 The Causal Interpretation of Bayesian Networks
109
Trainee
Supervisor
Victim
 
Fig. 4.11. Pre-emptive assassination
Bottle Shatters
Billy Throws
Suzy Throws
 
Fig. 4.12. Pre-emptive bottle smashing
then Suzy causes the bottle to shatter and Billy does not (see Figure 4.12), again
despite the fact that there is no end-to-end dependency. In this case, however,
there is also no step-wise dependency for Lewis to draw upon: at the very last
step, where the bottle shatters, the dependency will always fail, because Billy’s
rock is on its way.
Hitchcock’s H1 fails to accommodate Suzy’s throw, because the end-to-end
dependency fails under the actual circumstances. So, Hitchcock resorts to coun-
terfactual contexts to cope (in this he is following the account of Halpern and
Pearl [17]).20 For these contexts Hitchcock only allows counterfactual circum-
stances which would not change the values of any of the variables on the causal
path under consideration. Any variable oﬀthat path will have a range of values
which have no impact on the causal path, minimally that value which it actually
took. Such values are said to be in the redundancy range (RR) for that path.
Then the new criterion, H2, is:
H2: C = c actually caused E = e if and only if both C = c and E = e
occurred and when we iterate through all Φi ∈Paths(C, E), for some
such Φi there is a set of variables W s.t., when ﬁxed at values in their
redundancy ranges relative to Φi, there is a probabilistic dependence
between C and E.
Since actual values are trivially within the RR, the prior (positive) successes of
H1 remain successes for H2. With Suzy and Billy, it’s clear that Billy’s throwing
or not are both within the redundancy range, and the dependency upon Suzy’s
20 Lewis [32] also resorted to counterfactuality, replacing sequences of dependencies
with sequences of hypothetical dependencies (“quasi-dependencies”). Incidentally,
Halpern and Pearl [17] analyse this case using a temporally expanded (dynamic)
network; however, the complexities involved would not reward our treating it here.

110
K.B. Korb and A.E. Nicholson
throw reappears when we consider what happens when Billy’s throw is absent.
This seems a very tidy solution.
However, Hiddleston [22] oﬀers an example which H2 cannot handle, as usual
an example concerning potential violent death. Suppose the king’s guard, fearing
an attack, pours an antidote to poison in the king’s coﬀee. The assassin, however,
fails to make an appearance; there is no poison in the coﬀee. The king drinks
his coﬀee and survives. Did the antidote cause the king to survive? That is
no more plausible than the claim that the boulder falling has caused the hiker
to survive; however, H2 makes this claim, since Poison being true is in the
redundancy range.21 Interestingly, H1 gets this story right, since the poison is
then forced to be absent, when the dependency of survival on antidote goes
away. Hiddleston concludes that H1 was just the right criterion all along, but
needed to be supplemented with Patricia Cheng’s (and Clark Glymour’s) theory
of causal models and process theory [6, 15, 14]. We agree with his general idea:
examining dependencies under actual instantiations of context variables is the
right way to approach actual causality. Cheng’s causal model theory, however,
is far too restrictive, as we noted above.
4.9.1
An Algorithm for Assessing Token Causation
So, we now present our alternative account of actual causation in the form of an
“algorithm” for assessing whether C = c actually caused E = e, given that both
events occurred. Our steps are hardly computationally primitive, but opportu-
nities for reﬁning them and making them clearer are surely available.
Step 1
Build the right causal model M.
Of course, this is a complicated step, possibly involving causal discovery, expert
consultation, advancing the science of relevant domains, and so forth. All of our
model-building rules apply. This (hopefully) leads to the right causal model,
describing the right type causal relationships. So, this account of token causality
starts from the type causal model and gets more speciﬁc from there.
This step, applying the model-building principles, circumvents a number of
problems that have arisen in the literature. For example, we know that Time
should not be invoked as a variable (it violates the Intervention Principle) and
that problem-deﬁning constraints should likewise be excluded (because of the
Distinction Principle). We also know that the imaginative introduction of in-
termediate events to save some kind of step-wise dependency across a causal
chain is (normally) illegitimate. So, despite being open-ended, this “step” is not
vacuous.
21 It might be pointed out that the model here is incomplete, and an intermediate node
which registers the combined state of poison and antidote would push Poison=true
out of the redundancy range. But that’s an ineﬀective response, unless in fact no
model of the structure oﬀered by Hiddleston is possible. However, we can always
construct such a model, as a game, for example.

4 The Causal Interpretation of Bayesian Networks
111
Fig. 4.13. Backup ﬁelders, imperfect and perfect
Step 2
Select and instantiate an actual context O (we will designate the model M in
context O by M/O).
Typically, this involves selecting a set O of variables in M and ﬁxing them at their
observed values. Often the causal question itself sets the context for us, when
selecting the context is trivial. For example, someone might ask, “Given that
no poison was added to the coﬀee, did the antidote cause the king’s survival?”
Indeed, that appears to be exactly what Hiddleston was asking in the above
example.
A striking example of context-setting is this (from [9]). Suzy and Billy are
playing left and center ﬁeld in a baseball game. The batter hits a long drive
between the two, directly at a kitchen window. Suzy races for the ball. Billy
races for the ball. Suzy’s in front and Billy’s behind. Suzy catches the ball. Did
Suzy prevent the window from being smashed? Not a hard question. However,
now suppose we replace Billy with a movable metal wall, the size of a house. We
position the wall in front of the window. The ball is hit, Suzy races and catches
it in front of the wall. Did Suzy prevent the window from being smashed? This
is no harder than the ﬁrst question, but produces the opposite answer. The
question, of course, is how can our criterion for actual causation reproduce this
switch, when the two cases are such close parallels of each other. What changes
is the context in which the causal question gets asked. Here the changed context
is not reﬂected in the values of the variables which get observed — neither the
wall nor Billy are allowed to stop the ball; rather, the changed context is in

112
K.B. Korb and A.E. Nicholson
the probability structure.22 Billy is a fallible outﬁelder; the wall, for practical
purposes, is an infallible outﬁelder. The infallibility of the wall leaves no possible
connecting process between Suzy’s ﬁelding and the window.23
The idea of a connecting process arose already in the Process Principle, in
that there must be a sequence of possible connecting processes between any
two variables in the model. Speciﬁcally, by connecting process we mean any
Salmon-Dowe process such that under the relevant circumstances (e.g., O) C
makes a probabilistic diﬀerence to E. We suggest beyond the Process Principle
a somewhat more demanding model building principle for individual arcs is in
order:
Principle 4 (Connecting Process): For every arc C −−▶E in M/O there must
be a possible value for C such that there is a connecting process between C
and E.
The baseball example is a case where an individual arc fails to have a corre-
sponding connecting process for any value of its causal variable. Such arcs we
call wounded and remove them:24
Step 3
Delete wounded arcs, producing M ∗.
In the second baseball case the arc Suzy Catches −−▶Window Shatters starts
out wounded: it’s an arc that should never have been added and which no causal
discovery algorithm would add. But many cases of wounding arise only when a
speciﬁc context is instantiated. For example, in bottle smashing (Figure 4.12), if
Suzy doesn’t throw, there’s nothing wrong with the causal process carrying in-
ﬂuence from Billy’s throw to the bottle. If we ask about Billy’s throw speciﬁcally
in the context of Suzy having thrown ﬁrst, however, then there is no connecting
process. The arc Billy Throws −−▶Bottle Shatters is vulnerable to that speciﬁc
context, and, given the context, is wounded.
Until now, in Bayesian network modeling two kinds of relationship between
pairs of variables < C, E > have been acknowledged: those for which there is
always a probabilistic dependency regardless of context set, when a direct arc
22 The causal question being relative to an observational context presupposes that it is
also relative to the causal model, including parameters, in which the context is set.
23 An alternative analysis of this would be to say that since the wall is infallible, it
eﬀectively has only one state, and so is not a variable at all. Whether we really
should remove the variable depends upon whether or not we wish to take seriously
the possibility of it altering state, even if only by an explicit intervention. Regardless
of whether we deal with the wall in parameters or structure, there will remain no
possible dependency between Suzy’s catch and the window.
24 For a more careful discussion of the metaphysics of process and wounding we refer
the reader to [19].

4 The Causal Interpretation of Bayesian Networks
113
must be added between them;25 and those pairs which are screened oﬀfrom each
other by some context set (possibly empty). But vulnerable arcs are those which
are sometimes needed, they connect pairs of the ﬁrst type above, but also they are
sometimes not needed; when they are wounded, the arcs can mediate no possible
probabilistic dependency, since they cannot participate in any active path. We
might say, the arcs ﬂicker on and oﬀ, depending upon the actual context.
Strictly speaking, deleting wounded arcs is not necessary for the assessment
of token causality, since the next step applies a dependency test which is already
sensitive to wounding. Removing the wounded arc simply makes the indepen-
dency graphic, as faithful models do.
Step 4
Determine whether intervening upon C can make a probabilistic diﬀerence to E
in the given circumstances M ∗/O.
Note that we make no attempt here to isolate any path connecting C and E
(in contrast with our treatment of type causality above). In token causation
our interest is in identifying whether C actually does make a diﬀerence within
context O; to answer this question we must allow that alternative strands of
inﬂuence may nullify the aﬀect. Thus, in questions of token causation we should
allow for neutralizing alternatives. In neutral Hesslow, for example, we would
not attribute token causality to the pill, whether or not thrombosis ensued —
unless, of course, the woman’s state of pregnancy were ﬁxed as part of the
context. Allowing neutralization for a type question is not an option, however,
since the type question indicates a desire to know whether there is some possible
extension to context which yields a diﬀerence-making intervention, which there is
in neutral Hesslow. So, in this way, token causality diﬀers from a straightforward
particularization of type causality, by being bound to the speciﬁc context M ∗/O.
Since we are not focused here on causal role, the probabilistic diﬀerence iden-
tiﬁed in Step 4 might well be to reduce the probability of E; hence, rather than
saying that C actually caused E, it might be better simply to say that C is
actually causally relevant to E. As the context in which the token causal ques-
tion gets raised, O, is enlarged, this criterion becomes more particular to the
historical circumstances; as the context O shrinks, this criterion more closely
resembles type causal relevance, with the exception noted above.
4.10
Conclusion
Bayesian network technology and its philosophy have reached an important junc-
ture. The representational and inferential methods have proven themselves in
25 This means, for every possible set of context variables O there is some instantiation
of the context O = o such that P(E|C, O = o) ̸= P(E|O = o). This is not to be con-
fused with requiring that for all possible context sets, and all possible instantiations
thereof, C and E are probabilistically dependent, which is a mistake Cartwright [4]
makes.

114
K.B. Korb and A.E. Nicholson
academic practice and are beginning to be taken up widely in industry and sci-
entiﬁc research. This, and the diﬃculty in building them by expert elicitation,
has fueled considerable work in the automation of causal discovery, which in
turn has prompted a reconsideration of the causal interpretation of the mod-
els discovered, by both supporters and skeptics. The friction between the two
subcommunities has sparked a variety of ideas, both philosophical disputes and
initiatives potentially valuable in application, such as the measures of causal
power introduced above. This is a lively and productive time for research in
causal modeling, both theoretical and applied.
Acknowledgements
We thank Erik Nyberg, Charles Twardy, Toby Handﬁeld, Graham Oppy and
Luke Hope for contributing to work and discussions that we have drawn upon
here.
References
1. Bouissou, M., Thuy, N.: Decision making based on expert assessments: Use of belief
networks to take into account uncertainty, bias, and weak signals. In: Decision-
making aid and control of the risks: Lambda-Mu 13/ESREL 2002 Conference,
Lyon, France (2002)
2. Bovens, L., Hartmann, S.: Bayesian networks and the problem of unreliable instru-
ments. Philosophy of Science 69, 29–72 (2002)
3. Cain, J.: Planning improvements in natural resources management. Technical re-
port, Centre for Ecology and Hydrology (2001)
4. Cartwright, N.: What is wrong with Bayes nets? The Monist 84, 242–264 (2001)
5. Cartwright, N.: Nature’s Capacities and their Measurement. Clarendon Press, Ox-
ford (1989)
6. Cheng, P.W.: From covariation to causation: A causal power theory. Psychological
Review 104, 367–405 (1997)
7. Chickering, D.M.: A tranformational characterization of equivalent Bayesian net-
work structures. In: Besnard, P., Hanks, S. (eds.) Proc. of the 11th Conf. on Un-
certainty in AI, San Francisco, pp. 87–98 (1995)
8. Max Chickering, D.: Optimal structure identiﬁcation with greedy search. Machine
Leaning Research 3, 507–559 (2002)
9. Collins, J.: Preemptive prevention. Journal of Philosophy 97, 223–234 (2000)
10. Cooper, G.F.: The computational complexity of probabilistic inference using Bayes-
ian belief networks. Artiﬁcial Intelligence 42, 393–405 (1990)
11. Cooper, G.F., Herskovits, E.: A Bayesian method for constructing Bayesian belief
networks from databases. In: D’Ambrosio, S., Bonissone (eds.) uai1991, pp. 86–94
(1991)
12. Dowe, P.: Physical Causation. Cambridge University, New York (2000)
13. Fisher, R.A.: Letter. British Medical Journal, 297–298 (August 3, 1957)
14. Glymour, C.: The Mind’s Arrows: Bayes Nets and Graphical Causal Models in
Psychology. MIT, Cambridge (2001)

4 The Causal Interpretation of Bayesian Networks
115
15. Glymour, C., Cheng, P.W.: Causal mechanism and probability: A normative ap-
proach. In: Oaksford, M., Chater, N. (eds.) Rational models of cognition, Oxford
(1998)
16. Hall, N.: Two concepts of causation. In: Collins, J., Hall, N., Paul, L.A. (eds.)
Causation and Counterfactuals, pp. 225–276. MIT Press, Cambridge (2004)
17. Halpern, J.Y., Pearl, J.: Causes and explanations, part I. British Journal for the
Philosophy of Science 56, 843–887 (2005)
18. Halpern, J.Y., Pearl, J.: Causes and explanations, part II. British Journal for the
Philosophy of Science 56, 889–911 (2005)
19. Handﬁeld, T., Oppy, G., Twardy, C., Korb, K.B.: Probabilistic process causality
(2005); Under Submission
20. Hausman, D.M.: Probabilistic causality and causal generalizations. In: Eells, E.,
Fetzer, J.H. (eds.) The Place of Probability in Science, Open Court (2005)
21. Heckerman, D., Geiger, D., Chickering, D.M.: Learning Bayesian networks: the
combination of knowledge and statistical data. In: de Mantras, L., Poole, D. (eds.)
Proc. of the 10th Conf. on Uncertainty in AI, San Francisco, pp. 293–301 (1994)
22. Hiddleston, E.: Causal powers. British Journal for the Philosophy of Science 56,
27–59 (2005)
23. Hitchcock, C.R.: The intransitivity of causation revealed in equations and graphs.
Journal of Philosophy 158(6), 273–299 (2001)
24. Hitchcock, C.R.: Routes, processes and chance-lowering causes. In: Dowe, P., No-
ordhof (eds.) Cause and Chance, Routledge, pp. 138–151 (2004)
25. Hope, L.R., Korb, K.B.: An information-theoretic causal power theory. In: Proc. of
the 18th Australian Joint Conference on AI, Sydney, NSW, pp. 805–811. Springer,
Heidelberg (2005)
26. Kitcher, P.: Explanatory uniﬁcation and the causal structure of the world. In:
Kitcher, P., Salmon, W.C. (eds.) Minnesota Studies in the Philosophy of Science,
Univ. of Minnesota, vol. XIII, pp. 410–505 (1989)
27. Korb, K.B., Hope, L.R., Nicholson, A.E., Axnick, K.: Varieties of causal interven-
tion. In: Paciﬁc Rim International Conference on AI, pp. 322–231 (2004)
28. Korb, K.B., Nicholson, A.E.: Bayesian Artiﬁcial Intelligence. CRC/Chapman and
Hall, Boca Raton,Fl (2004)
29. Korb, K.B., Nyberg, E.: The power of intervention. Minds and Machines 16, 289–
302 (2006)
30. Korb, K.B., Twardy, C.R., Handﬁeld, T., Oppy, G.: Causal reasoning with causal
models. Technical Report 2005/183, School of Computer Science and Software
Engineering, Monash University (2005)
31. Lewis, D.: Causation. Journal of Philosophy 70, 556–567 (1973)
32. Lewis, D.: Philosophical Papers, vol. II. Oxford Univ., Oxford (1986)
33. Lewis, D.: Void and object. In: Collins, J., Hall, N., Paul, L.A. (eds.) Causation
and Counterfactuals, pp. 277–290. MIT Press, Cambridge (2004)
34. Marcot, B.G., Holthausen, R.S., Raphael, M.G., Rowland, M.M., Wisdom, M.J.:
Using Bayesian belief networks to evaluate ﬁsh and wildlife population viability
under land management alternatives from an environmental impact statement.
Forest Ecology and Management 153, 29–42 (2001)
35. Menzies, P.: Diﬀerence making in context. In: Collins, J., Hall, N., Paul, L. (eds.)
Counterfactuals and Causation, pp. 139–180. MIT Press, Cambridge (2004)
36. Neapolitan, R.E.: Learning Bayesian Networks. Prentice Hall, Englewood Cliﬀs
(2003)
37. Novick, L.R., Cheng, P.W.: Assessing interactive causal inﬂuence. Psychological
Review 111, 455–485 (2004)

116
K.B. Korb and A.E. Nicholson
38. Nyberg, E.P., Korb, K.B.: Informative interventions. Technical Report 2006/204,
School of Information Technology, Monash University (2006)
39. Pearl, J.: Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San
Mateo (1988)
40. Pearl, J.: Causality: Models, reasoning and inference, Cambridge (2000)
41. Popper, K.R.: The Logic of Scientiﬁc Discovery. Basic Books, New York (1959);
Translation, with new appendices, of Logik der Forschung Vienna (1934)
42. Popper, K.R.: Objective Knowledge: An Evolutionary Approach. Oxford Univer-
sity, Oxford (1972)
43. Reichenbach, H.: The Direction of Time. Univ of California, Berkeley (1956)
44. Rosen, D.: In defense of a probabilistic theory of causality. Philosophy of Science 45,
604–613 (1978)
45. Salmon, W.C.: Scientiﬁc Explanation and the Causal Structure of the World,
Princeton (1984)
46. Salmon, W.: Probabilistic causality. Paciﬁc Philosophical Quarterly, 50–74 (1980)
47. Spirtes, P., Glymour, C., Scheines, R.: Causation, Prediction and Search. Lecture
Notes in Statistics, vol. 81. Springer, Heidelberg (1993)
48. Spirtes, P., Glymour, C., Scheines, R.: Causation, Prediction and Search, 2nd edn.
MIT Press, Cambridge (2000)
49. Steel, D.: Homogeneity, selection and the faithfulness condition. Minds and Ma-
chines 16, 303–317 (2006)
50. Suppes, P.: A Probabilistic Theory of Causality, North Holland, Amsterdam (1970)
51. Twardy, C.R., Nicholson, A.E., Korb, K.B.: Knowledge engineering cardiovascular
Bayesian networks from the literature. Technical Report 2005/170, Clayton School
of IT, Monash University (2005)
52. Twardy, C., Korb, K.B.: A criterion of probabilistic causation. Philosophy of Sci-
ence 71, 241–262 (2004)
53. Verma, T.S., Pearl, J.: Equivalence and synthesis of causal models. In: D’Ambrosio,
S., Bonissone (eds.) Proc. of the Sixth Conference on Uncertainty in AI, pp. 255–
268 (1991)
54. Wallace, C.S., Korb, K.B.: Learning linear causal models by MML sampling. In:
Gammerman, A. (ed.) Causal Models and Intelligent Data Management. Springer,
Heidelberg (1999)
55. Witten, I.H., Frank, E.: Data mining: Practical machine learning tools and tech-
niques, 2nd edn. Morgan Kaufmann, San Francisco (2005)
56. Woodward, J.: Making Things Happen. Oxford Univ., Oxford (2003)
57. Wright, S.: The method of path coeﬃcients. Annals of Mathematical Statistics 5(3),
161–215 (1934)

D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 117–130, 2008. 
springerlink.com                                                                © Springer-Verlag Berlin Heidelberg 2008 
5 
An Introduction to Bayesian Networks and Their 
Contemporary Applications 
Daryle Niedermayer, I.S.P.* 
College of the North Atlantic-Qatar, Doha, Qatar 
Abstract. Bayesian Networks are an important area of research and application within the  
domain of Artificial Intelligence. This paper explores the nature and implications for Bayesian 
Networks beginning with an overview and comparison of inferential statistics with Bayes’ 
Theorem. The nature, relevance and applicability of Bayesian Network theory for issues of 
advanced computability form the core of the current discussion. A number of current applica-
tions using Bayesian networks are examined. The paper concludes with a brief discussion of the 
appropriateness and limitations of Bayesian Networks for human-computer interaction and 
automated learning. 
5.1   Introduction 
Inferential statistics is a branch of statistics that attempts to make valid predictions 
based on only a sample of all possible observations. For example, imagine a bag of 
10,000 marbles. Some are black and some white, but the exact proportion of these 
colours is unknown. It is unnecessary to count all the marbles in order to make some 
statement about this proportion. A randomly acquired sample of 1,000 marbles may 
be sufficient to make an inference about the proportion of black and white marbles in 
the entire population. If 40% of our sample is white, then we may be able to infer that 
about 40% of the population is also white. 
To the layperson, this process seems rather straight forward. In fact, it might seem 
that there is no need to even acquire a sample of 1,000 marbles. A sample of 100 or 
even 10 marbles might do. 
This assumption is not necessarily correct. As the sample size becomes smaller, the 
potential for error grows. For this reason, inferential statistics has developed numer-
ous techniques for stating the level of confidence that can be placed on these  
inferences. 
If we took ten samples of 100 marbles each, we might find the following results: 
                                                           
* This paper is revised from an earlier work dated December 1, 1998. ©1998, 2008 by Daryle 
Niedermayer. All Rights Reserved. 

118 
I.S.P. Daryle Niedermayer 
Table 5.1. Relative proportions of 10 samples from a population of 10,000 
Sample  
Number 
(n) 
Number of 
White  
Marbles 
(X) 
Number of 
Black  
Marbles 
1 
40 
60 
2 
35 
65 
3 
47 
53 
4 
50 
50 
5 
31 
69 
6 
25 
75 
7 
36 
64 
8 
20 
80 
9 
45 
55 
10 
55 
45 
We are then in a position to calculate the mean and standard deviation of these 
samples. Our sample mean of white marbles from our sample is denoted by X  to 
distinguish it from the actual proportion of white and black marbles in the bag which 
we refer to as μ. The standard deviation denoted by the symbol sigma (σ) is found 
using equations 1 and 2: 
1
2
−
= ∑
n
x
σ
                                                 (eq. 5.1) 
where x2 is the sum of the squares and n is the number of samples taken, so that the 
equation is expanded to: 
1
)
...(
)
(
)
(
2
2
2
2
1
−
−
−
+
−
=
n
X
X
X
X
X
X
n
σ
                      (eq. 5.2) 
The value of X  derived from our 10 samples is 38.4. We might be tempted to say 
that about 40% of the marbles are white, but how sure are we that our samples are 
representative of the actual population of black and white marbles in the bag, μ? De-
termining this level of confidence requires some analysis concerning the variability in 
our samples. 
Using equation 2 above, we determine that σ=11.15. We must then determine the 
“Sample Error of the Mean” (denoted by ς): 
n
X
σ
ς
=
                                                (eq. 5.3) 
In our example, 
X
ς
 is 3.53. 
The confidence we can put on our hypothesis that 40% of the marbles are white is 
then found using a standard statistical test called a “z-test”: 

 
5   An Introduction to Bayesian Networks and Their Contemporary Applications 
119 
X
X
z
ς
μ
−
=
                                                 (eq. 5.4) 
Using our hypothesized value of 40% for μ, and entering our values for X  and 
X
ς
, we obtain a z-value of 0.4535. Looking this value up in a z-test table or using a 
z-test computer algorithm, we would find that 32% of the area of the normal curve 
falls below this “z” value.i 
Based on the inference that the more we sample our bag of marbles, the more we 
will find the number of white marbles in our samples ( X ) clustering around the ac-
tual number of white marbles (μ), the z-test helps us determine how confident we can 
be in our hypothesis. 
Figure 5.1a) illustrates this confidence. If μ actually is 40%, then only 32% of any 
samples we draw (represented by the shaded area) should have less than 38.4% white 
marbles. 
However, we are assuming that our sample mean is less than our actual mean, μ. 
This isn’t necessarily the case. Because the z-curve is symmetrical, we can see that 
there is also a 32% chance that a sample will have more than 41.6% white balls if μ is 
40%. This possibility is shown in Figure 5.1b). For this reason, we say that our sam-
ple distribution is two-sided or “two-tailed”. 
In summary, our series of ten samples can only give us a 36% confidence level that 
the actual percentage of white marbles in the bag is 40%±1.6%. Clearly, the confi-
dence we can place in our conclusion is not as good as it was on first glance. This lack 
of confidence is due to the high variability among the samples. If we took more sam-
ples or larger samples, our confidence in our conclusion might increase. 
X
μ
a)
X
μ
b)
 
Fig. 5.1. Areas of the distribution curve used in z-tests 
                                                           
i An example of such a table can be obtained at: http://instruct.uwo.ca/geog/500/z_test.htm 
(accessed November 2007). 

120 
I.S.P. Daryle Niedermayer 
5.2   An Introduction to Bayesian Inference 
Classical inferential models do not permit the introduction of prior knowledge into the 
calculations. For the rigours of the scientific method, this is an appropriate response to 
prevent the introduction of extraneous data that might skew the experimental results. 
However, there are times when the use of prior knowledge would be a useful contri-
bution to the evaluation process. 
Assume a situation where an investor is considering purchasing some sort of exclu-
sive franchise for a given geographic territory. Her business plan suggests that she 
must achieve 25% market saturation for the enterprise to be profitable. Using some of 
her investment funds, she hires a polling company to conduct a randomized survey. 
The results conclude that from a random sample of 20 consumers, 25% of the popula-
tion would indeed be prepared to purchase her services. Is this sufficient evidence to 
proceed with the investment? 
If this is all the investor has to go on, she could find herself on the break-even point 
or could just as easily turn a loss instead of a profit. On the basis of this data alone, 
she may not have enough confidence to proceed with her plan. 
Fortunately, the franchising company has a wealth of experience in exploiting new 
markets. Their results show that in 20% of cases, new franchises only achieve 25% 
market saturation, while in 40% of cases, new franchises achieve 30% market satura-
tion. The entire table of their findings appears below: 
Table 5.2. Percent of New Franchises achieving a given Market Saturation 
Market Saturation 
(Proportion)  
(p) 
Percent of  
Franchises (Relative 
Frequency) 
0.10 
0.05 
0.15 
0.05 
0.20 
0.20 
0.25 
0.20 
0.30 
0.40 
0.35 
0.10 
  
Total = 1.00 
 
Our investor's question is simple: “What is the probability that my population will 
achieve a market saturation of greater than 25% given the poll conducted and the re-
sults found in other places?” In effect, she needs to determine the probability that her 
population will one of the 70% of cases where market saturation is greater than or 
equal to 25%. She now has the information she needs to make a Bayesian inference of 
her situation. 
5.2.1   Bayes Theorem 
Bayes' Theorem, developed by the Rev. Thomas Bayes, an 18th century mathemati-
cian and theologian, was first published in 17631. Mathematically it is expressed as: 

 
5   An Introduction to Bayesian Networks and Their Contemporary Applications 
121 
)
|
(
)
,
|
(
)
|
(
)
,
|
(
c
E
P
c
H
E
P
c
H
P
c
E
H
P
×
=
                           (eq. 5.5) 
Essentially, we can update our belief in hypothesis H given the additional evidence 
E and the background context c. The left-hand term, P(H|E,c) is known as the “poste-
rior probability”, or the probability of H after considering the effect of E on c. The 
term P(H|c) is called the “prior probability of H given c alone.” The term P(E|H,c) is 
called the “likelihood” and gives the probability of the evidence assuming the hy-
pothesis H and the background information c is true. Finally, the last term P(E|c) is 
independent of H and can be regarded as a normalizing or scaling factor. 
In the case of our hypothetical investor, the franchiser already knows that 50% of 
prior franchisees are profitable, so P(H|c) is already known: 
)
|
(
)
,
|
(
50
.0
)
,
|
(
c
E
P
c
H
E
P
c
E
H
P
×
=
                            (eq. 5.6) 
It is important to note that all of these probabilities are conditional. They specify 
the degree of belief in some proposition or propositions based on the assumption that 
some other propositions are true. As such, the theory has no meaning without prior 
resolution of the probability of these antecedent propositions. 
5.2.2   Bayes Theorem Applied 
Let us return the example of the investor. From theory of binomial distributions, if the 
probability of some event occurring on any one trial is p, then the probability of x 
such events occurring out of n trials is expressed as: 
x
n
x
p
p
x
n
x
n
x
P
−
−
−
=
)
1(
)!
(!
!
)
(
                              (eq. 5.7)2 
For example, the likelihood that 5 out of 20 people will support her enterprise 
should her location actually fall into the category where 20% of franchises actually 
achieve 25% saturation is: 
20233
.0
)
75
.0
(
)
25
.0
(
)!
5
20
(!5
!
20
)
|
5
(
15
5
20
.
=
−
=
=
p
x
P
               (eq. 5.8) 
The likelihood of the other situations can also be determined: 
The sum of all the Joint Probabilities provides the scaling factor found in the de-
nominator of Bayes Theorem and is ultimately related to the size of the sample. Had 
the sample been greater than 20, the relative weighting between prior knowledge and 
current evidence would be weighted more heavily in favour of the latter. The Poste-
rior Probability column of Table 3 shows the results of the Bayesian theorem for this 
case. By adding up the relative posterior probabilities for market shares ≥25% and 
those <25%, our investor will see that there is a 75% probability that her franchise 
will make money—definitely a more attractive situation on which to base an invest-
ment decision. 

122 
I.S.P. Daryle Niedermayer 
Table 5.3. Likelihood of An Investor Finding herself in each situation given x=5 and n=20 ii 
Event (Market 
Saturation) 
(pi)
Prior 
Probability
P0(pi)
Likelihood 
of Situation 
P(x=5|pi)
Joint Probability 
of Situation 
P(x=5|pi)P0(pi)
Posterior 
Probabilityii
0.10 
0.05
0.03192
0.001596 
0.00959
0.15 
0.05
0.10285
0.005142 
0.00309
0.20 
0.20
0.17456
0.034912 
0.20983
0.25 
0.20
0.20233
0.040466 
0.24321
0.30 
0.40
0.17886
0.071544 
0.43000
0.35 
0.10
0.12720
0.012720 
0.07645
 
1.00
0.81772
0.166381 
0.99997  
5.3   Bayesian Networks 
5.3.1   Introduction 
The concept of conditional probability is a useful one. There are countless real world 
examples where the probability of one event is conditional on the probability of a 
previous one. While the sum and product rules of probability theory can anticipate 
this factor of conditionality, in many cases such calculations are “NP-hard” meaning 
that they are computationally difficult and cannot be determined in polynomial time. 
The prospect of managing a scenario with 5 discrete random variables (25-1=31 dis-
crete parameters) might be manageable. An expert system for monitoring patients 
with 37 variables resulting in a joint distribution of over 237 parameters would not be 
manageable.3 
5.3.2   Definition 
Consider a domain U of n variables, x1,...xn. Each variable may be discrete having a 
finite or countable number of states, or continuous. Given a subset X of variables xi 
where xi∈U, if one can observe the state of every variable in X, then this observation 
is called an instance of X and is denoted as: 
X
i
i
i
i
k
x
p
x
x
p
X
)
,
|
(
)
,
,...
|
(
1
1
ξ
ξ
Π
=
=
−
 for the observations 
X
x
k
x
i
i
i
∈
=
,
  
     (eq. 9) 
The 
“joint 
space” 
of 
U 
is 
the 
set 
of 
all 
instances 
of 
U. 
)
,
|
(
ξ
Y
X
k
Y
k
X
p
=
=
denotes 
the 
“generalized 
probability 
density” 
that 
X
i
i
i
i
k
x
p
x
x
x
p
X
)
,
|
(
)
,
,...
|
(
1
1
ξ
ξ
Π
=
=
−
 given 
Y
k
Y =
 for a person with 
                                                           
ii  The Posterior Probability can be expressed as: 
)
5
(
)
(
)
|
5
(
)
5
|
(
)
(
0
1
=
=
=
=
=
x
P
p
P
p
x
P
x
p
P
p
P
i
i
i
i
 

 
5   An Introduction to Bayesian Networks and Their Contemporary Applications 
123 
current state information ξ. p(X|Y, ) then denotes the “Generalized Probability Den-
sity Function” (gpdf) for X, given all possible observations of Y. The joint gpdf over 
U is the gpdf for U. 
A Bayesian network for domain U represents a joint gpdf over U. This representa-
tion consists of a set of local conditional gpdfs combined with a set of conditional 
independence assertions that allow the construction of a global gpdf from the local 
gpdfs. As shown previously, the chain rule of probability can be used to ascertain 
these values: 
∏
=
−
=
n
i
i
i
n
x
x
x
p
x
x
p
1
1
1
1
)
,
,...
|
(
)
|
,...
(
ξ
ξ
                           (eq. 5.10) 
One assumption imposed by Bayesian Network theory (and indirectly by the Prod-
uct Rule of probability theory) is that each variable xi, Πi ⊆ {x1,…xi-1} must be a set of 
variables that renders xi and {x1,...xi-1} conditionally independent. In this way: 
)
,
|
(
)
,
,...
|
(
1
1
ξ
ξ
i
i
i
i
x
p
x
x
x
p
Π
=
−
                (eq. 5.11)4  
A Bayesian Network Structure then encodes the assertions of conditional inde-
pendence in equation 10 above. Essentially then, a Bayesian Network Structure Bs “is 
a directed acyclic graph such that (1) each variable in U corresponds to a node in Bs, 
and (2) the parents of the node corresponding to xi are the nodes corresponding to the 
variables in ∏i.”5 
A Bayesian-network gpdf set Bp is the collection of local gpdfs: p(xi|∏i,ξ) for each 
node in the domain. 
5.3.3   Bayesian Networks Illustrated 
Given a situation where it might rain today, and might rain tomorrow, what is the 
probability that it will rain on both days? Rain on two consecutive days are not inde-
pendent events with isolated probabilities. If it rains on one day, it is more likely to 
rain the next. Solving such a problem involves determining the chances that it will 
rain today, and then determining the chance that it will rain tomorrow conditional on 
the probability that it will rain today. These are known as “joint probabilities.” Sup-
pose that P(rain today) = 0.20 and P(rain tomorrow given that it rains today) = 0.70. 
The probability of such joint events is determined by: 
)
|
(
)
(
)
,
(
1
2
1
2
1
E
E
P
E
P
E
E
P
=
                         (eq. 5.12) 
This can also be expressed as: 
)
(
)
,
(
)
|
(
1
2
1
1
2
E
P
E
E
P
E
E
P
=
                                 (eq. 5.13)6 
Working out the joint probabilities for all eventualities, the results can be ex-
pressed in a table format. 
From the table, it is evident that the joint probability of rain over both days is 0.14, 
but there is a great deal of other information that had to be brought into the calcula-
tions before such a determination was possible. With only two discrete, binary vari-
ables, four calculations were required. 

124 
I.S.P. Daryle Niedermayer 
Table 5.4. Marginal and Joint Probabilities for rain both today and tomorrow 
 
Rain Tomorrow 
No Rain 
Tomorrow 
Marginal 
Probability of 
Rain Today 
Rain Today 
0.14 
0.06 
0.20 
No Rain Today 
0.16 
0.64 
0.80 
Marginal Probability of Rain 
Tomorrow 
0.30 
0.70 
  
 
This same scenario can be expressed using a Bayesian Network Diagram as shown 
in Figure 5.2. 
P(E1)=0.20
P(!E1)=0.80
P(E2|!E1)=0.20
P(!E2|!E1)=0.80
P(E2|E1)=0.70
P(!E2|E1)=0.30
P(E1,E2)=0.14
P(!E1,!E2)=0.64
P(!E1,E2)=0.16
P(E1,!E2)=0.06
 
Fig. 5.2. A Bayesian Network showing the probability of rain 
One attraction of Bayesian Networks are their efficiency owing to the fact that only 
one branch of the tree needs to be traversed. We are really only concerned with P(E1), 
P(E2|E1) and P(E1,E2). 
We can also utilize the graph both visually and algorithmically to determine which 
parameters are independent of each other. Instead of calculating four joint probabili-
ties, we can use the independence of the parameters to limit our calculations to two. It 
is self-evident that the probabilities of rain on the second day having rained on the 
first are completely autonomous from the probabilities of rain on the second day hav-
ing not rained on the first. 
At the same time as emphasizing parametric indifference, Bayesian Networks also 
provide a parsimonious representation of conditionality among parametric relation-
ships. While the probability of rain today and the probability of rain tomorrow are two 
discrete events (it cannot rain both today and tomorrow at the same time), there is a 
conditional relationship between them (if it rains today, the lingering weather systems 
and residual moisture are more likely to result in rain tomorrow). For this reason, the 
directed edges of the graph are connected to show this dependency. 
Friedman and Goldszmidt suggest looking at Bayesian Networks as a story. They 
offer the example of a story containing five random variables: “Burglary”, “Earth-
quake”, “Alarm”, “Neighbour Call”, and “Radio Announcement”.7 In such a story, 
“Burglary” and “Earthquake” are independent, and “Burglary” and “Radio An-
nouncement” are independent given “Earthquake.” This is to say that there is no event 

 
5   An Introduction to Bayesian Networks and Their Contemporary Applications 
125 
which effects both burglaries and earthquakes. As well, “Burglary” and “Radio An-
nouncements” are independent given “Earthquake”—meaning that while a radio an-
nouncement might result from an earthquake, it will not result as a repercussion from 
a burglary. 
Because of the independence among these variables, the probability of P(A,R,E,B) 
(The joint probability of an alarm, radio announcement, earthquake and burglary) can 
be reduced from: 
P(A,R,E,B) = P(A|R,E,B) × P(R|E,B) × P(E|B) × P(B) 
involving 15 parameters to 8: 
P(A,R,E,B) = P(A|E,B) × P(R|E) × P(E) × P(B) 
This significantly reduced the number of joint probabilities involved. This can be 
represented as a Bayesian Network: 
Earthquake
Neighbour Call
Alarm
Radio Announcement
Burglary
 
Fig. 5.3. The conditional probabilities of an alarm given the independent events of a burglary 
and earthquake 
Using a Bayesian Network offers many advantages over traditional methods of de-
termining causal relationships. Independence among variables is easy to recognize 
and isolate while conditional relationships are clearly delimited by a directed graph 
edge: two variables are independent if all the paths between them are blocked (given 
that edges are directional). Not all the joint probabilities need to be calculated to make 
a decision; extraneous branches and relationships can be ignored (One can make a 
prediction of a radio announcement regardless of whether an alarm sounds). By opti-
mizing the graph, every node can be shown to have at most k parents. The algorithmic 
routines required can then be run in O(2kn) instead of O(2n) time. In essence, the algo-
rithm can run in linear time (based on the number of edges) instead of exponential 
time (based on the number of parameters). 
Associated with each node is a set of conditional probability distributions. For ex-
ample, the “Alarm” node might have the following probability distribution: 

126 
I.S.P. Daryle Niedermayer 
Table 5.5. Probability Distribution for the Alarm Node given the events of “Earthquakes” (E) 
and “Burglaries” (B) 
Earthquakes 
Burglaries 
P(A|E,B) 
P(!A|E,B) 
Earthquake 
Burglary 
0.90 
0.10 
Earthquake 
No Burglary 
0.20 
0.80 
No Earthquake 
Burglary 
0.90 
0.10 
No Earthquake 
No Burglary 
0.01 
0.99 
 
For example, should there be both an earthquake and a burglary, the alarm has a 
90% chance of sounding. With only an earthquake and no burglary, it would only 
sound in 20% of the cases. A burglary unaccompanied by an earthquake would set off 
the alarm 90% of the time, and the chance of a false alarm given no antecedent event 
should only have a probability of 0.1% of the time. Obviously, these values would 
have to be determined through observation. 
5.4   Algorithmic Implications of Bayesian Networks 
Bayesian networks are useful for both inferential exploration of previously undeter-
mined relationships among variables as well as descriptions of these relationships 
upon discovery. In the former case, raw computational power can be brought to bear 
upon a problem. In the case of determining the likelihood of rain the next day follow-
ing a rainy day, raw meteorological data can be input into the computer and the com-
puter can determine the resultant probability network. This process of network  
discovery is discussed by Friedman & Goldszmidt. Such a process is computationally 
intensive and NP-hard in its algorithmic implications. The benefit of such a process is 
evident in the ability to describe the discovered network in the future. The calculation 
of any probability branch of the network can then be computed in linear time. 
5.5   Practical Uses for Bayesian Networks 
5.5.1   AutoClass 
The National Aeronautic and Space Administration has a large investment in Bayes-
ian research. NASA's Ames Research Center is interested in deep-space exploration 
and knowledge acquisition. In gathering data from deep-space observatories and 
planetary probes, an a priori imposition of structure or pattern expectations is inap-
propriate. Researchers do not always know what to expect or even have hypotheses to 
test when gathering such data. Bayesian inference is useful because it allows the in-
ference system to construct its own potential systems of meaning upon the data. Once 
any implicit network is discovered within the data, the juxtaposition of this network 
against other data sets allows for quick and efficient testing of new theories and  
hypotheses. 
The AutoClass project is an attempt to create Bayesian applications that can auto-
matically interpolate raw data from interplanetary probes, and deep space explora-
tions.8 A graphical example of AutoClass's capabilities is displayed in Figure 5.4.  
 

 
5   An Introduction to Bayesian Networks and Their Contemporary Applications 
127 
 
Fig. 5.4. An AutoClass interpolation of raw data with no predefined categories. Sorted data is 
grouped by colour and shape. The top area is sorted into green-blue shapes, the middle into 
blues, and the bottom into red-orange-yellow shapes. (Image courtesy of NASA, used with 
permission). 
Incidentally, the source code for AutoClass is available in both LISP and C on an 
Open Source basis. 
An applied example of AutoClass's capabilities was the input of infrared spectra. 
Although no differences among these spectra were initially suspected, AutoClass suc-
cessfully distinguished two subgroups of stars.9 
5.5.2   Introduction of Search Heuristics 
Searching for a solution to a problem is usually an NP-hard problem resulting in a 
combinatorial explosion of possible solutions to investigate. This problem is often 
ameliorated through the use of heuristics, or sub-routines to make “intelligent” 
choices along the decision tree. An appropriately defined heuristic can quicken the 
search by eliminating obviously unsuccessful paths from the search tree. However, an 
inappropriately defined heuristic might eliminate the successful solutions and result in 
no evident solution. 
Bayesian networks can replace heuristic methods by introducing a method where 
the probabilities are updated continually during search. 
One class of search algorithms called “Stochastic searching” utilizes what are 
known as “Monte-Carlo” procedures. These procedures are non-deterministic and do 
not guarantee a solution to a problem. As such they are very fast, and repeated use of 
these algorithms will add evidence that a solution does not exist even though they 
never prove that such a solution is non-existent. 
Combining procedures with knowledge of properties of the distribution from which 
problem instances are drawn can help extend the utility of these algorithms by focus-
ing in on areas of the search tree not previously studied. 

128 
I.S.P. Daryle Niedermayer 
5.5.3   Lumiere 
Microsoft began work in 1993 on Lumiere, its project to create software that could 
automatically and intelligently interact with software users by anticipating the goals 
and needs of these users. 
This research was in turn based on earlier research on pilot-aircraft interaction.10 
The concern of this investigation was the ability of a system to supply a pilot with 
information congruent with the pilot's current focus of attention. Extraneous informa-
tion or information not related to the pilot's current task list was suppressed. 
“This ability to identify a pilot's focus of attention at any moment during a flight 
can provide an essential link to the provision of effective decision support. In particu-
lar, understanding the current goals of a pilot decision maker can be applied to select 
the presentation of alternative systems and displays.”11 
The Lumiere project at Microsoft eventually resulted in the “Office Assistant” with 
the introduction of the Office 95 suite of desktop products.12 
5.6   Limitations of Bayesian Networks 
In spite of their remarkable power and potential to address inferential processes, there 
are some inherent limitations and liabilities to Bayesian networks. 
In reviewing the Lumiere project, one potential problem that is seldom recognized 
is the remote possibility that a system user might wish to violate the distribution of 
probabilities upon which the system is built. While an automated help desk system 
that is unable to embrace unusual or unanticipated requests is merely frustrating, an 
automated navigation system that is unable to respond to some previously unforeseen 
event might put an aircraft and its occupants in mortal peril. While these systems can 
update their goals and objectives based on prior distributions of goals and objectives 
among sample groups, the possibility that a user will make a novel request for infor-
mation in a previously unanticipated way must be accommodated. 
Two other problems also warrant consideration. The first is the computational dif-
ficulty of exploring a previously unknown network. To calculate the probability of 
any branch of the network, all branches must be calculated. While the resulting ability 
to describe the network can be performed in linear time, this process of network dis-
covery is an NP-hard task which might either be too costly to perform, or impossible 
given the number and combination of variables. 
The second problem centers on the quality and extent of the prior beliefs used in 
Bayesian inference processing. A Bayesian network is only as useful as this prior 
knowledge is reliable. Either an excessively optimistic or pessimistic expectation of 
the quality of these prior beliefs will distort the entire network and invalidate the re-
sults. Related to this concern is the selection of the statistical distribution induced in 
modeling the data. Selecting the proper distribution model to describe the data has a 
notable effect on the quality of the resulting network. 
5.7   Conclusion 
These concerns aside, Bayesian networks have incredible power to offer assistance in 
a wide range of endeavours. They support the use of probabilistic inference to update 
and revise belief values. Bayesian networks readily permit qualitative inferences 

 
5   An Introduction to Bayesian Networks and Their Contemporary Applications 
129 
without the computational inefficiencies of traditional joint probability determina-
tions. In doing so, they support complex inference modeling including rational deci-
sion making systems, value of information and sensitivity analysis. As such, they are 
useful for causality analysis and through statistical induction they support a form of 
automated learning. This learning can involve parametric discovery, network discov-
ery, and causal relationship discovery. 
In this paper, we discussed the premises of Bayesian networks from Bayes' Theo-
rem and how such Bayesian inference differs from classical treatments of statistical 
inference. The reasons, implications and emerging potential of Bayesian networks in 
the area of Artificial Intelligence were then explored with an applied focus profiling 
some current areas where Bayesian networks and models are being employed to ad-
dress real-life problems. Finally, we examined some of the limitations of Bayesian 
networks. 
At best, such a paper can only be a snapshot of the state of Bayesian research at a 
given time and place. The breadth and eclectic foci of the many individuals, groups 
and corporations researching this topic makes it one of the truly dynamic areas within 
the discipline of Artificial Intelligence. 
References 
1. Stutz, J., Cheeseman, P.: A Short Exposition on Bayesian Inference and Probability.  
National Aeronautic and Space Administration Ames Research Centre: Computational Sci-
ences Division, Data Learning Group (June 1994)  
2. Morgan, B.W.: An Introduction to Bayesian Statistical Decision Processes, p. 15. Prentice-
Hall Inc., Englewood Cliffs (1968) 
3. Friedman, N., Goldszmidt, M.: Learning Bayesian Networks from Data, http:// 
robotics.stanford.edu/~nir/tutorial/Tutorial.ps.gz (accessed 
   November 2007) 
4. Heckerman, D., Geiger, D., Chickering, D.: Learning Bayesian Networks: The Combina-
tion of Knowledge and Statistical Data. Machine Learning 20, 197–243 (1995), 
http://research.microsoft.com/research/pubs/view.aspx?tr_id=
81 (accessed November 2007). This online version will be used for citation references 
throughout this paper 
5. Heckerman, D., Geiger, D., Chickering, D., p. 6  
6. Winkler, R.L.: An Introduction to Bayesian Inference and Decision. Holt, Rinehart and 
Winston. Toronto (1972) 
7. Friedman, N., Goldszmidt, M. 
8. Stutz, J., Taylor, W., Cheeseman, P.: AutoClass C - General Information. NASA, Ames Re-
search Center (1998), http://ic-www.arc.nasa.gov/ic/projects/bayes-
group/autoclass/autoclass-c-program.html#AutoClassC (accessed  
  November 2007)  
9. http://ic-www.arc.nasa.gov/ic/projects/bayes-group/autoclass/ 
index.html (accessed November 2007)  
10. Cooper, G., Horvitz, E., Curry R.: Conceptual Design of Goal Understanding Systems: In-
vestigation of Temporal Reasoning Under Uncertainty. Decision Theory & Adaptive Sys-
tems Group, Microsoft Research. Microsoft Corp. Redmond, WA (1998), 
  http://research.microsoft.com/research/dtg/horvitz/goal.htm 
 (accessed November 2007)  

130 
I.S.P. Daryle Niedermayer 
11. Horvitz, E.: Lumiere Project: Bayesian Reasoning for Automated Assistance. Decision The-
ory & Adaptive Systems Group, Microsoft Research. Microsoft Corp., Redmond, WA 
(1998), 
http://research.microsoft.com/research/dtg/horvitz/lum. 
htm (accessed November 2007)  
12. Heckerman, D., Horvitz, E.: Inferring Informational Goals from Free-Text Queries: A 
Bayesian Approach. Decision Theory & Adaptive Systems Group, Microsoft Research. 
Microsoft Corp., Redmond, WA (1998), 
http://research.microsoft.com/research/dtg/horvitz/aw.htm  
(accessed November 2007)  

6
Objective Bayesian Nets for Systems
Modelling and Prognosis in Breast Cancer
Sylvia Nagl1, Matt Williams2, and Jon Williamson3
1 Department of Oncology, University College London
s.nagl@medsch.ucl.ac.uk
2 Advanced Computation Laboratory, Cancer Research UK, and Computer Science,
University College London
m.williams@cs.ucl.ac.uk
3 Department of Philosophy, University of Kent
j.williamson@kent.ac.uk
Summary. Cancer treatment decisions should be based on all available evidence. But
this evidence is complex and varied: it includes not only the patient’s symptoms and
expert knowledge of the relevant causal processes, but also clinical databases relating
to past patients, databases of observations made at the molecular level, and evidence
encapsulated in scientiﬁc papers and medical informatics systems. Objective Bayesian
nets oﬀer a principled path to knowledge integration, and we show in this chapter how
they can be applied to integrate various kinds of evidence in the cancer domain. This
is important from the systems biology perspective, which needs to integrate data that
concern diﬀerent levels of analysis, and is also important from the point of view of
medical informatics.
In this chapter we propose the use of objective Bayesian nets for knowledge
integration in the context of cancer systems biology. In Part I we discuss this
context in some detail. Part II introduces the machinery that is to be applied,
objective Bayesian nets. Then a proof-of-principle application is presented in
Part III. Finally, in Part IV, we discuss avenues for further research.
Part I: Cancer Systems Biology and Knowledge
Integration
6.1
Cancer Systems Biology
Cancer systems biology seeks to elucidate complex cell and tumour behaviour
through the integration of many diﬀerent types of knowledge. Information is
obtained from scientiﬁc and clinical measurements made across biological scale,
ranging from molecular components to systems, and from the genome to the
whole patient. Integration of this information into predictive computational mod-
els, and their use in research and clinical settings, is expected to improve pre-
vention, diagnostic and prognostic prediction, and treatment of cancer.
D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 131–167, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

132
S. Nagl, M. Williams, and J. Williamson
Systems biology addresses the complexity of cancer by drawing on a concep-
tual framework based on the current understanding of complex adaptive sys-
tems.1 Complex systems are composed of a huge number of components that
can interact simultaneously in a suﬃciently rich number of parallel ways so
that the system shows spontaneous self-organisation and produces global, emer-
gent structures and behaviours.2 Self-organisation concerns the emergence of
higher-level order from the local interactions of system components in the ab-
sence of external forces or a pre-programmed plan embedded in any individual
component.3
The challenges posed by the complex-systems properties of cancer are several-
fold and can be thought about in terms of a taxonomy of complexity put forward
by Mitchell:4
•
Structural complexity;
•
Dynamic complexity—complexity in functional processes;
•
Evolved complexity—complex systems can generate alternative evolutionary
solutions to adaptive problems; these are historically contingent.
Decisions need to be made in the face of great uncertainty regarding all three
aspects of the complexity that is exhibited by the cancer systems in which one
seeks to intervene.5 This is true both for therapeutic decisions for individual
patients and also for design strategies leading to new anti-cancer therapies. Al-
though our ability to collect ever more detailed quantitative molecular data on
cells and cell populations in tumours is growing exponentially, and clinical in-
vestigations are becoming more and more sophisticated, our understanding of
system complexity advances more slowly for the following reasons.
It is very diﬃcult to directly observe and measure dynamic processes in com-
plex systems, and this is particularly challenging in biomedicine where human
subjects are involved. Research relies on data generated from tissue samples by
high throughput technologies mainly directed at the ‘omic’ levels of the genome,
transcriptome (gene transcripts) and proteome (proteins).6 However, data sam-
pling is highly uneven and incomplete, and the data themselves are noisy and
often hard to replicate. The molecular data that are being gathered typically only
illuminate ‘single-plane’ omic slices ‘dissected’ out of entire systems which are
characterized by highly integrated multi-scale organization and non-linear be-
haviour (Fig. 6.1). Furthermore, due to technological and economic constraints,
current techniques can only capture a few time points out of the continuous
systems dynamics, and are not yet able to address the ‘complexity explosion’ of
control at the proteomic level. This situation is likely to persist for some time
to come.
1 (Nagl, 2006.)
2 (Holland, 1995; Depew and Weber, 1996.)
3 (Holland, 1995, 1998; Mitchell, 2003.)
4 (Mitchell, 2003, pp. 4–7.)
5 (Nagl, 2005).
6 (Abramovitz and Leyland-Jones, 2006).

6 Objective Bayesian Nets for Systems Modelling
133
Fig. 6.1. Multi-scale integrated cell systems
A further challenge is posed by the need to relate molecular data to clin-
ical measurements, notably those obtained in clinical trials, for identiﬁcation
of molecular parameters underlying physiological (dys)function. This integra-
tion task spans spatial and temporal scales of several orders of magnitude and
complexity. Ultimately, cancer systems analysis needs to cut across all biologi-
cal levels—genome, transcriptome, proteome, cell, and beyond to tissue, organ
and patient (and the environment, but this is outside the present discussion).
Toyoda and Wada (2004) have coined the term omic space and presented a hier-
archical conceptual model linking diﬀerent omic planes. They showed that this
structuring of omic space helps to integrate biological ﬁndings and data com-
prehensively into hypotheses or models combining higher-order phenomena and
lower-order mechanisms through a comprehensive ranking of correspondences
among interactions in omic space. The key idea behind the concept of omic
space may also serve as a general organising principle for multi-scale systems,
and may be extended beyond cells to the tissue, organ and patient level. Below,
we discuss how objective Bayesian nets can make signiﬁcant contributions to the
elucidation of multi-scale relationships in cancer systems (see §§6.4, 6.20).
6.2
Unstable Genomes and Complexity in Cancer
Tumours maintain their survival and proliferative potential against a wide range
of anticancer therapies and immunological responses of the patient. Their robust-
ness is seen as an emergent property arising through the interplay of genomic
instability and selective pressure driven by host-tumour dynamics.7
7 (Kitano, 2004).

134
S. Nagl, M. Williams, and J. Williamson
Progression from normal tissue to malignancy is associated with the evolution
of neoplastic cell lineages with multiple genomic lesions (abnormal karyotypes).8
Most cancer cells do not have a signiﬁcantly higher mutation rate at the nu-
cleotide level compared to their normal counterparts, whereas extensive gross
chromosomal changes are observed in liquid, and nearly all, solid tumours. The
most common mutation class among the known cancer genes is chromosomal.
In cancer, dynamic large-scale changes of genome structure occur at dramat-
ically increased frequency and tumour cell–microenvironment interactions drive
selection of abnormal karyotypes. Copy-number changes, such as gene ampliﬁca-
tion and deletion, can aﬀect several megabases of DNA and include many genes.
These extensive changes in genome content can be advantageous to the cancer
cell by simultaneous activation of oncogenes and elimination of tumour suppres-
sors. Due to the magnitude of the observed genomic rearrangements, it is not
always clear which gene, or set of genes, is the crucial target of the rearrangement
on the basis of genetic evidence alone.
These changes encompass both directly cancer-causing and epiphenomenal
changes (bystander mutations) which can nevertheless contribute signiﬁcantly
to the malignant phenotype and can modulate treatment resistance in a com-
plex fashion. The ability of abnormal karyotypes to change autocatalytically
in response to challenge, the masking of speciﬁc cancer-promoting constella-
tions by collateral variation (any chromosomal combination that is speciﬁc for
a selected function is also speciﬁc for many unselected functions), and the com-
mon phenomenon of several alternative cell pathways able to generate the same
phenotype, limits the usefulness of context-independent predictions from kary-
otypic data. Interestingly, several researchers have put forward the theory that
the control of phenotype is distributed to various extents among all the genetic
components of a complex system.9
Mathematical modelling, adapted from Metabolic Control Analysis, suggests
that it may in fact be the large fraction of the genome undergoing diﬀerential
expression as a result of changes in gene dose (due to chromosomal rearrange-
ments) that leads to highly non-linear changes in the physiology of cancer cells.
6.3
A Systems View of Cancer Genomes and Bayesian
Networks
Genomes are dynamic molecular systems, and selection acts on cancer karyotypes
as integrated wholes, not just on individual oncogenes or tumour suppressors.
Given the irreversible nature of evolutionary processes, the randomness of mu-
tations and rearrangements relative to those processes, and the modularity and
redundancy of complex systems, there potentially exists a multitude of ways to
‘solve’ the problems of achieving a survival advantage in cancer cells.10 Since each
patient’s cancer cells evolve through an independent set of genomic lesions and
8 (Nygren and Larsson, 2003; Vogelstein and Kinzler, 2004).
9 (Rasnick and Duesberg, 1999, and references therein).
10 (Mitchell, 2003, p. 7).

6 Objective Bayesian Nets for Systems Modelling
135
selective environments, the resulting heterogeneity of cell populations within the
same tumour, and of tumours from diﬀerent patients, is a fundamental reason
for diﬀerences in survival and treatment response.
Since the discovery of oncogenes and tumour suppressors, a reductionist focus
on single, or a small number of, mutations has resulted in cancer being conceptu-
alized as a ‘genetic’ disease. More recently, cancer has been recast as a ‘genomic’
or ‘systems’ disease.11 In the work presented in this chapter, we apply a systems
framework to karyotype evolution and employ Bayesian networks to generate
models of non-independent rearrangements at chromosomal locations from com-
parative genome hybridisation (CGH) data.12 Furthermore, we present a method
for integration of genomic Bayesian network models with nets learnt from clini-
cal data. The method enables the construction of multi-scale nets from Bayesian
nets learnt from independent datasets, with each of the nets representing the
joint probability distributions of parameter values obtained from diﬀerent levels
of the biological hierarchy, i.e., the genomic and tumour level in the application
presented here (together with treatment and outcome data). Bayesian network
integration allows one to capture ‘more of the physiological system’ and to study
dependency relationships across scales.13
Some of the questions one may address by application of our approach include
•
utilising genomic (karyotype) data from patients:
–
Can we identify probabilistic dependency networks in large sample sets of
karyotypes from individual tumours? If so, under which conditions may
these be interpreted as causal networks?
–
Can we discover key features of the ‘evolutionary logic’ embodied in gene
copy number changes of individual karyotypes?
–
Can we characterise the evolutionary ‘solution space’ explored by unstable
cancer genomes? Is there a discernible dependence on cancer types?
•
utilising omic and other molecular data together with clinical measurements:
–
Can we identify probabilistic dependency networks involving molecular
and clinical levels?
–
How may such probabilistic dependencies aid diagnostic and prognostic
prediction and design of personalised therapies?
6.4
Objective Bayesianism and Knowledge Integration
Bayesian networks are well suited to problems that require integration of data
from various sources or data with diﬀerent temporal or spatial resolutions. They
can model complex non-linear relationships, and are also very robust to miss-
ing information. Bayesian network learning has already been successfully ap-
plied to data gathered at the transcriptomic and proteomic level for predictions
regarding structure and function of gene regulatory, metabolic and signalling
11 (Khalil and Hill, 2005; Lupski and Stankiewicz, 2006).
12 (Reis-Filho et al., 2005).
13 (Nagl et al., 2006).

136
S. Nagl, M. Williams, and J. Williamson
networks.14 Bulashevska and colleagues have applied Bayesian network analysis
to allelotyping data in urothelial cancer.15 However, these studies also demon-
strate persistent limits—only very partial answers have so far been obtained
concerning the organization and dynamic function of whole biological systems
which are by deﬁnition multi-scale and integrated by multiple feedback loops
(see §6.1).
Employing objective Bayesianism as our methodology, we present a multi-
scale approach to knowledge integration which utilises more fully the very con-
siderable scope of available data. Our method enables integration of ‘omic’ data
types and quantitative physiological and clinical measurements. These data com-
bined oﬀer rich, and as yet largely unexplored, opportunities for the discovery of
probabilistic dependencies involving system features situated at multiple levels
of biological organisation.
The technique supports progressive integration of Bayesian networks learnt
from independently conducted studies and diverse data types, e.g., mRNA or
proteomic expression, SNP, epigenetic, tissue microarray, and clinical data. New
knowledge and new data types can be integrated as they become available over
time. The application of our knowledge discovery method is envisaged to be
valuable in the clinical trials arena which is undergoing far-reaching changes
with steadily increasing incorporation of molecular proﬁling. It is our aim to
assess the potential of our technique for integrating diﬀerent types of clinical
trial datasets (with and without molecular data). The methods described here
are highly complementary to ongoing research initiatives, such as the Cancergrid
project (www.cancergrid.org) and caBIG (cabig.nci.nih.gov) which are already
addressing pressing informatics requirements that result from these changes in
clinical study design.
6.5
Complementary Data Integration Initiatives
We are currently not in a position to make maximal use of existing data sets for
Bayesian network analysis, since data have not yet been standardised in terms of
experimental and clinical data capture (protocols, annotation, data reproducibil-
ity and quality), and computational data management (data formats, vocabular-
ies, ontologies, metadata, exchange standards). Basic requirements are the
generation of validated high-quality datasets and the existence of the various data
sources in a form that is suitable for computational analysis and data integration.
This has been well recognised, as is amply demonstrated by the aims and activities
of a collaborative network of several large initiatives for data integration within
the cancer domain which work towards shared aims in a coordinated fashion (the
initiatives mentioned below are meant to serve as example projects and do not
represent the sum total of these eﬀorts on an international scale).
The National Cancer Institute Center for Bioinformatics (NCICB) in the
United States has developed caCORE which provides an open-source suite of
14 (Xia et al., 2004).
15 (Bulashevska et al., 2004).

6 Objective Bayesian Nets for Systems Modelling
137
common resources for cancer vocabulary, metadata and data management needs
(biological and clinical), and, from Version 3.0, achieves semantic interoperabil-
ity across disparate biomedical information systems (for detailed information
and access to the caCORE components, see ncicb.nci.nih.gov/ NCICB/infra-
structure/cacore overview). caCORE plays an essential integrative role for the
cancer Biomedical Informatics Grid (caBIG), a voluntary network connecting
individuals and institutions to enable the sharing of data and tools, creat-
ing a ‘World Wide Web of cancer research’ whose goal is to speed up the
delivery of innovative approaches for the prevention and treatment of cancer
(cabig.nci.nih.gov/).
In the United Kingdom, the National Cancer Research Institute (NCRI) is
developing the NCRI Strategic Framework for the Development of Cancer Re-
search Informatics in the UK (www.cancerinformatics.org.uk). The ultimate aim
is the creation of an internationally compatible informatics platform that would
facilitate data access and analysis. CancerGRID develops open standards and
information management systems (XML, ontologies and data objects, web ser-
vices, GRID technology) for clinical cancer informatics, clinical trials, integration
of molecular proﬁles with clinical data, and eﬀective translation of clinical trials
data to bioinformatics and genomics research (www.cancergrid.org).
Part II: Objective Bayesian Nets
6.6
Integrating Evidence Via Belief
In this information-rich age we are bombarded with evidence from a multiplic-
ity of sources. This is evidence in a defeasible sense: items of evidence may not
be true—indeed diﬀerent items of evidence often contradict each other—but we
take such evidence on trust until we learn that it is ﬂawed or until something
better comes along. In the case of breast cancer prognosis we have databases of
molecular and clinical observations of varying reliability, current causal knowl-
edge about the domain, knowledge encapsulated in medical informatics systems
(e.g. argumentation systems, medical ontologies), and knowledge about the pa-
tient’s symptoms, treatment, and medical history. The key question is how we
represent this eclectic body of evidence and render it coherent.
Knowledge impinges on belief, and one way in which we try to make sense
of conﬂicting evidence is by ﬁnding a coherent set of beliefs that best ﬁts this
knowledge. We try to ﬁnd beliefs that are consistent with undefeated items
of evidence where we can, and where two items conﬂict we try to ﬁnd some
compromise beliefs. But this is vaguely put, and in this Part we shall describe a
way of making this idea more precise.
Objective Bayesianism oﬀers a formalism for determining the beliefs that best
ﬁt evidence; §6.7 oﬀers a brief introduction to this theory. While this provides
a useful theoretical framework, further machinery is required in order to ﬁnd
these beliefs and reason with them in practice—this is the machinery of objective
Bayesian nets outlined in §6.8. In §6.9 we sketch a general procedure for construct-
ing these nets, then in §6.10 we see how objective Bayesian nets can be used to

138
S. Nagl, M. Williams, and J. Williamson
integrate qualitative evidence with quantitative evidence. Finally §6.11 discusses
objective Bayesian nets in the context of the problem at hand, breast cancer.16
6.7
Objective Bayesianism
According to Bayesian theory, an agent’s degrees of belief should behave like
probabilities. Thus you should believe that a particular patient’s cancer will
recur to some degree representable by a real number x between 0 and 1 inclu-
sive; you should believe that the patient’s cancer will not recur to degree 1 −x.
Many Bayesians go further by adopting empirical constraints on degrees of be-
lief. Arguably, for instance, degrees of belief should be calibrated with known
frequencies: if you know just that 40% of similar patients have cancers that re-
cur then you should believe that this patient’s cancer will recur to degree 0.4.
Objective Bayesians go further still, accepting not only empirical constraints on
degrees of belief but also logical constraints: in the absence of empirical evidence
concerning cancer recurrence you should equivocate on the question of this pa-
tient’s cancer recurring—i.e. you should believe the cancer will recur to the same
degree that you should belief it will not recur, 0.5.17
From a formal point of view the objective Bayesian position can be summed
up as follows.18 Applying Bayesian theory, the agent’s degrees of belief should
be representable by a probability function p. Suppose that the agent has em-
pirical evidence that takes the form of a set of quantitative constraints on p.
Then she should adopt the probability function p, from all those that sat-
isfy these constraints, that is maximally equivocal, i.e. that maximises entropy
H = −
v p(v) log p(v), where the sum is taken over all assignments v = v1 · · · vn
to the variables V1, . . . , Vn in the domain. This is known as the maximum entropy
principle.19
Note that two items of empirical evidence may conﬂict—for example, the agent
might be told that the frequency of recurrence is 0.4, but might also be told on
another occasion that the frequency of recurrence is 0.3, with neither of the two
reports known to be more reliable than the other and neither more pertinent to
the patient in question. Arguably, the agent’s degree of belief that the patient’s
cancer will recur will be constrained to lie within the closed interval [0.3, 0.4].
More generally, empirical constraints will constrain an agent’s belief function to
lie within a closed convex set of probability functions, and consequently there
will be a unique function p that maximises entropy.20 Thus the agent’s rational
belief function p is objectively determined by her evidence (hence the name
objective Bayesianism).21
16 See Williamson (2002); Williamson (2005a, §5.5–5.8) and Williamson (2005b) for
more detailed descriptions of the theory behind objective Bayesian nets.
17 (Russo and Williamson, 2007).
18 (Williamson, 2005a, Chapter 5).
19 (Jaynes, 1957).
20 (Williamson, 2005a, §5.3).
21 See Williamson (2007b) for a general justiﬁcation of objective Bayesianism, and
Russo and Williamson (2007) for a justiﬁcation within the cancer context.

6 Objective Bayesian Nets for Systems Modelling
139
We see then that objective Bayesianism provides a way of integrating evidence.
The maximum entropy probability function p commits to the extent warranted
by evidence: it satisﬁes constraints imposed by evidence but is non-committal
where there is insuﬃcient evidence. In that respect, objective Bayesian degrees
of belief can be thought of as representative of evidence.
6.8
Obnets
Finding a maximum entropy probability function by searching for the param-
eters p(v) that maximise the entropy equation is a computationally complex
process and impractical for most real applications. This is because for a do-
main of n two-valued variables there are 2n parameters p(v) to calculate; as
n increases the calculation gets out of hand. But more eﬃcient methods are
available. Bayesian nets, in particular, can be used to reduce the complexity of
representing a probability function and drawing inferences from it.
A Bayesian net is a graphical representation of a probability function. The
variables in the domain form the nodes of the graph. The graph also contains
arrows between nodes, but must contain no cycles. Moreover, to each node is
attached a probability table, containing the probability distribution of that vari-
able conditional on its parents in the graph. As long as the Markov condition
holds—i.e. each variable is probabilistically independent of its non-descendants
in the graph conditional on its parents, written Vi ⊥⊥NDi | Pari—the net suf-
ﬁces to determine a probability function over the whole domain, via the identity
p(v)
df= p(v1 · · · vn) =
n

i=1
p(vi|par i).
Thus the probability of an assignment to all the variables in the domain is the
product of the probabilities of the variables conditional on their parents. These
latter probabilities can be found in the probability tables. Depending on the
sparsity of the graph, a Bayesian net can oﬀer a much smaller representation of
a probability function than that obtained by listing all the 2n probability values
p(v). Furthermore, the Bayesian net can be used to eﬃciently draw inferences
from the probability function and there is a wide variety of software available
for handling these nets.22 The other chapters in this volume are testament to
the importance of Bayesian nets for probabilistic reasoning.
An objective Bayesian net, or obnet, is a Bayesian net that represents objec-
tive Bayesian degrees of belief, i.e. that represents an agent’s entropy-maximising
probability function. Because the objective Bayesian belief function is deter-
mined in a special way (via the maximum entropy principle) there are spe-
cial methods for constructing an objective Bayesian net, detailed in §6.9. These
methods are more eﬃcient to carry out than the more direct maximisation of
the parameters p(v) in the entropy equation.
22 (Neapolitan, 1990; Korb and Nicholson, 2003).

140
S. Nagl, M. Williams, and J. Williamson
Given an objective Bayesian net, standard Bayesian net algorithms can be
used to calculate probabilities, e.g. the probability of cancer recurrence given
the characteristics of a particular patient. Thus an obnet can help with the task
in hand, breast cancer prognosis. But an obnet can address other tasks too,
for example the problem of knowledge discovery. An objective Bayesian net can
suggest new relationships between variables: for instance if two variables are
found to be strongly dependent in the obnet but there is no known connection
between the variables that accounts for this dependence, then one might posit
a causal connection to explain that link (§§6.16, 6.20). An obnet can also be
used to determine new arguments to add to an argumentation framework: if
one variable signiﬁcantly raises the probability of another then the former is an
argument for the latter (§6.18). Thus an obnet is a versatile beast that can assist
with a range of tasks.
6.9
Constructing Obnets
One can build an objective Bayesian net by following a 3-step procedure. Given
evidence, ﬁrst determine conditional independencies that the entropy maximising
probability function will satisfy. With this information about independencies
one can then construct a directed acyclic graph for which the Markov condition
holds. Finally, add the probability tables by ﬁnding the probability parameter
p(vi|par i) that maximise entropy.
The ﬁrst step—ﬁnding the conditional independencies that p must satisfy—
can be performed as follows. As before, we suppose that background knowledge
imposes a set of quantitative constraints on p. Build an undirected constraint
graph by taking variables as nodes and linking two variables if they occur to-
gether in some constraint. We can then read oﬀprobabilistic independencies
from this graph: for sets of variables X, Y, Z, if Z separates X from Y in the con-
straint graph then X and Y will be probabilistically independent conditional on
Z, X ⊥⊥Y | Z, for the entropy maximising probability function p (Williamson,
2005a, Theorem 5.1).
The second step—determining the directed acyclic graph to go in the objec-
tive Bayesian net—is equally straightforward. One can transform the constraint
graph into a directed acyclic graph G that satisﬁes the Markov Condition via
the following algorithm:23
•
triangulate the constraint graph,
•
re-order V according to maximum cardinality search,
•
let D1, . . . , Dl be the cliques of the triangulated constraint graph ordered
according to highest labelled node,
•
set Ej = Dj ∩(j−1
i=1 Di) for j = 1, . . . , l,
•
set Fj = Dj\Ej for j = 1, . . . , l,
•
take variables in V as the nodes of G,
23 See Williamson (2005a, §5.7) for an explanation of the graph-theoretic terminology.

6 Objective Bayesian Nets for Systems Modelling
141
•
add an arrow from each vertex in Ej to each vertex in Fj (j = 1, . . . , l),
• ensure that there is an arrow between each pair of vertices in Dj (j = 1, . . . , l).
The ﬁnal step—determining the probability tables to go in the objective
Bayesian net—requires some number crunching. One needs to ﬁnd the pa-
rameters p(vi|par i) that maximise the entropy equation, which can be written
as H = n
i=1 Hi where Hi = −
v1···vn

Vj∈Anci p(vj|par j)

log p(vi|par i),
(Anci being the set of ancestors of Vi in G). This optimisation task can be car-
ried out in a number of ways. For instance, one can use numerical techniques or
Lagrange multiplier methods to ﬁnd the parameters.
This gives the general method for constructing an obnet. In §6.11 we shall
tailor this method to our particular problem domain, that of breast cancer. But
ﬁrst we shall see how the method can be extended to handle qualitative evidence.
6.10
Qualitative Evidence
In the breast cancer domain, as elsewhere, evidence can take qualitative form.
As well as quantitative evidence gleaned from clinical and molecular databases,
there is qualitative causal knowledge and also qualitative evidence gleaned from
medical ontologies and argumentation systems. In order to apply the maximum
entropy principle in this type of domain, qualitative evidence must ﬁrst be con-
verted into a set of quantitative constraints on degrees of belief. Here we shall
describe how this is possible.
Consider the following qualitative relationships: A is a cause of B; A is a
sub-type of B; A is an argument in favour of B. These causal, ontological and
evidential relations are all examples of what might be called inﬂuence relations.
Intuitively A inﬂuences B if bringing about A brings about B but bringing about
B does not bring about A. More precisely, a relation is an inﬂuence relation if
it satisﬁes the following property: learning of the existence of new variables that
are not inﬂuences of the other variables should not change degrees of belief
concerning those other variables.24
Qualitative knowledge of inﬂuence relationships can be converted into quan-
titative constraints on degrees of belief as follows. Suppose V ⊇U is a set of
variables containing variables in U together with other variables that are known
not to be inﬂuences of variables in U. As long as any other knowledge concern-
ing variables in V \U does not itself warrant a change in degrees of belief on
U, then pV
β⇂U = pU
βU , i.e., one’s belief function on the whole domain V formed
on the basis of all one’s background knowledge β, when restricted to U, should
match the belief function one would have adopted on domain U given just the
part βU of one’s knowledge involving U. These equality constraints can be used
to constrain degrees of belief so that the maximum entropy principle can be
applied. The equality constraints can also be fed into the procedure for con-
structing objective Bayesian nets: build the constraint graph as before from the
24 (Williamson, 2005a, §11.4).

142
S. Nagl, M. Williams, and J. Williamson
non-qualitative constraints; transform that graph into a directed acyclic graph
as before; but take the qualitative constraints, converted to quantitative equality
constraints, into account when determining the probability tables of the obnet
(Williamson, 2005a, Theorem 5.6).
6.11
Obnets and Cancer
In the context of our project we have several sources of general (i.e., not patient-
speciﬁc) evidence: databases of clinical data; databases of molecular data; a med-
ical ontology; arguments from an argumentation framework; evidence of causal
relationships from experts and also from published clinical trials. The study dis-
cussed in Part III focusses on databases of clinical and molecular data, but in this
section we shall show how all these varied evidence sources might be integrated.
These sources impose a variety of constraints on a rational belief function p.
Let C be the set of variables measured in a database of clinical data. Then p⇂C =
freqC, the rational probability function when restricted to the variables in the
clinical dataset should match the frequency distribution induced by that dataset.
Similarly, if M is the set of variables measured in a molecular dataset, then p⇂M =
freqM. A medical ontology determines inﬂuence relationships amongst variables.
For example, knowledge that assignment a is a type (or sub-classiﬁcation) of
assignment b imposes the constraint p(b|a) = 1, as well as the inﬂuence constraint
(§6.10) p{A,B}
⇂A
= p{A}. An argumentation framework also determines inﬂuence
relationships. An argument from a to b indicates that a and b are probabilistically
dependent. This yields the constraint p(b|a) ≥p(b)+τ where τ is some threshold
(which measures the minimum strength of arguments within the argumentation
framework), as well as the inﬂuence constraint p{A,B}
⇂A
= p{A}. Finally, causal
evidence yields inﬂuence constraints of the form p{A,B}
⇂A
= p{A}, and, if gleaned
from a clinical trial, quantitative constraints of the form p(b|a) ≥p(b) + τ.
In order to construct an objective Bayesian net from these sources, we can
follow the three-step procedure outlined in §6.9.
The ﬁrst task is to construct an undirected constraint graph. Following the
recipe, we link all the variables in C (the clinical variables), link all the variables
in M (the molecular / genomic variables), and link pairs of variables that are
connected by an argument or by a clause in the ontology or by a causal rela-
tion. But we can reduce the complexity of the resulting obnet, which is roughly
proportional to the density of the graph, still further as follows. One can use
standard algorithms to induce a Bayesian net that represents the frequency dis-
tribution of the clinical database. Similarly, one can induce a Bayesian net from
the clinical database. Then one can incorporate the independencies of these nets
in the constraint graph, to render the constraint graph more sparse. This can be
done as follows. Rather than linking every pair of variables in C with an edge
in the constraint graph, include a link only if one is the parent of the other in
frequency net, or if the two have a child in common in that net. Similarly for
the variables in M. This yields a constraint graph with fewer edges, and thus a
smaller obnet as a result.

6 Objective Bayesian Nets for Systems Modelling
143
The next two steps—converting the constraint graph into a directed acyclic
graph, and adding the probability tables—can be carried out as detailed in §6.9.
Note that there is a particularly simple special case. If the evidence consists
only of two databases which have just one variable in common, then one can
construct the directed acyclic graph of the obnet thus: for each database learn a
frequency net, ensuring that the variable in common is a root variable (i.e. has
no parents); then just join the frequency nets at the root variable.
Having discussed the theoretical aspects of objective Bayesian nets, we now
turn to a detailed description of the breast cancer application.
Part III: The Application
6.12
Obnets and Prediction in the Cancer Domain
We have applied objective Bayesian nets to the domain of breast cancer using
three sources of data: one clinical, and two genomic, as well as a published study.
The use of two genomic data sets was necessary as the more substantial genomic
Bayesian net did not have a node in common with the clinical network, and so we
used a smaller network to link the larger genomic network and the clinical one.
We start by reviewing the data used (§§6.13, 6.14), and then in §6.15 describe
how we constructed and merged the three separate networks. We then present
some initial data on the performance of the network, and conclude the Part with
a discussion of the uses of such networks in §6.16.
6.13
Breast Cancer
Breast Cancer is one of the commonest cancers in the Western World. It is
the commonest non-skin cancer in women in the UK and US, and accounts for
approximately a third of cancers in women, with lifetime rates of 1 in 10. Some
36000 cases are diagnosed each year in the UK, of whom about a third will
die from the disease.25 Consequently there has been a considerable amount of
research focused on breast cancer, and death rates have fallen over the last 10
years.26
The mainstay of treatment for breast cancer remains surgery and radiother-
apy,27 with hormonal and chemotherapeutic agents often used to treat presumed
micro-metastatic disease. One of the advantages of surgery is that, as well as re-
moving any local disease, a sample can also be taken of the axillary lymph nodes.
These are a common site of metastatic spread for the cancer, and their removal
not only removes any spread that may have occurred, but also allows analysis
of the nodes to describe the degree of spread. The two main aims of treatment
are to provide local control of, and to prevent premature death from, disease.
25 (McPherson et al., 2000).
26 (Quinn and Allen, 1995).
27 (Richards et al., 1994).

144
S. Nagl, M. Williams, and J. Williamson
Examination of the primary tumour and lymph nodes lets us deﬁne certain
characteristics of the disease that make local recurrence and death more likely.
These characteristics are primarily the grade of the tumour, (which represents
the degree of abnormality displayed by the cells, scored 1-3), the size of the
tumour (as its maximum diameter, in mm) and the number of involved nodes.28
There are also newer tests for the presence or absence of certain proteins on the
cell surface that may predict tumour behaviour or response to certain drugs.29
The central aim of therapy planning is to match treatment with the risk of
further disease. Thus those at high risk should be treated aggressively while
those at low risk should be treated less aggressively. This allows more eﬃcient
use of resources, and restricts the (often considerable) side eﬀects of intensive
treatment to those patients who would beneﬁt most.
Current Prognostic Techniques
These prognostic characteristics are currently modelled using statistical tech-
niques to provide an estimate of the probability of survival and local recurrence.
Two commonly used systems are the Nottingham Prognostic Index (NPI),30
which uses data from large UK studies, and results derived from the Ameri-
can Surveillance, Epidemiology and End Results (SEER) database,31 which are
used by systems such as Adjuvant Online.32 Both techniques rely on multivari-
ate analyses of large volumes of data (based on over 3 million people for SEER)
to calculate prognostic formulae.
These tools, and others like them, are eﬀective at providing estimates of risk of
death and local recurrence. However, they have two major weaknesses. Whilst
eﬀective, they lack explanatory power in a human-readable form. Therefore,
extra knowledge that has not been captured by the statistical analysis (such
as the presence and impact of other co-existing conditions) cannot be easily
incorporated. Secondly, knowledge that post-dates the formation of the formulae
(such as the discovery of Her-2neu, a cell-surface protein that is a marker for
more aggressive disease) is very diﬃcult to incorporate. Therefore, while they
excel at providing an accurate assessment of population-based risk, they have
weaknesses in the individualisation of that risk.
Humans are often poor at manipulating explicit probabilities;33 however, clin-
icians have the ability to process additional knowledge that statistically-based
systems often either ignore or treat on a perfunctory level. We would like to
support clinical decision making by providing explicit probabilistic estimates of
risk based on an integration of the variety of our knowledge sources.
28 (Richards et al., 1994).
29 (Veer et al., 2005; Cristofanilli et al., 2005).
30 (Galea et al., 1992).
31 (Ries et al., 2004).
32 (Ravdin et al., 2001).
33 (Kahneman and Tversky, 1973; Borak and Veilleux, 1982).

6 Objective Bayesian Nets for Systems Modelling
145
6.14
Our Knowledge Sources
Clinical Data
We used clinical data from a subset of the American Surveillance, Epidemiology
and End Results (SEER) study. The total study is very large (over 3 million
patients) and presents summary results on cancer diagnosis and survival in the
USA between 1975 and 2003,34 and subsets of the data are available for public
use. We used a subset that initially consisted of 4878 individuals with breast
cancer, which, once cases with incomplete data were removed, was reduced to
4731. The dataset consists of details of patient age (in 5 year bands, from 15–19
to 85+), the tumour size and histological grade, Oestrogen and Progesterone re-
ceptor status, the number of positive lymph nodes (if any), surgical type (mas-
tectomy vs breast conserving), whether radiotherapy was given, the patients’
survival from diagnosis (in months) and whether they had survived up until 5
years post-diagnosis. Patients in this subset were only followed up for 5 years,
and so there is no data available on longer survival times.
Initial inspection of the clinical data was carried out using standard spread-
sheet software (OpenOﬃce.org 2, 2005). Initial work concentrated on regrouping
some of the data as follows. Oestrogen receptors are produced as part of the same
intra-cellular pathway as Progesterone receptors, and as a result there is a very
close correlation between ER & PR status. Since they are regarded as being
one entity for most clinical purposes, we combined them into a single ‘Hormone
Receptor’ variable. The Lymph Node status was converted from a number of
positive lymph nodes (from 0–15) into a binary variable (True/ False), patient
age was converted from 5 year age bands into 15–50, 50–70, 70–90, and Tumour
size was converted from size in millimetres to sizes 0–20, 20–50, and 50–150
(these corresponding to clinical T Stages 1, 2, and 3+4). Patients with incom-
plete data (for example missing number of involved lymph nodes) were deleted
from the dataset. A sample of the dataset is depicted in Table 6.1.
Table 6.1. A sample of the clinical dataset
Age T Size Grade HR status Positive LN Surgery Radiotherapy Survival Status
70-74
22
2
1
1
1
1
37
1
45-49
8
1
1
0
2
1
41
1
The variables of the clinical database—i.e., the column headings of Table 6.1
are as follows:
Age: Age in years;
T Size: size of the primary tumour, in millimetres;
Grade: Histological grade of the tumour, from 1–3; 3 being most abnormal;
34 (Ries et al., 2004).

146
S. Nagl, M. Williams, and J. Williamson
HR Status: Positive if the sample was positive for either Oestrogen or Proges-
terone receptors;
Positive LN: 1 if the patient had any lymph nodes involved by tumour, 0 oth-
erwise;
Surgery: 1 if the patient had surgery for the tumour;
Radiotherapy: 1 if the patient received radiotherapy for their tumour;
Survival: Recorded survival in months;
Status: Status at ﬁnal follow-up, 1 = alive, 0 = died.
Genomic Data
We used two karyotype datasets from the progenetix database (www.progen-
etix. de).35 Progenetix contains discretised data on band-speciﬁc chromosomal
rearrangements of cancer and leukemia cases (loss -1, gain +1, no change 0). It
consists of a compilation of published data from comparative genome hybridisa-
tion (CGH), array CGH, and matrix CGH experiments, as well as some studies
using metaphase analysis. Progenetix is, with 12320 CGH experiments, by far
the largest public CGH database. We had available:
(i) a breast cancer CGH dataset of 502 cases which lacked consistent clinical
annotations, which we used to learn the genomic Bayesian net from band
data only,
(ii) a second CGH data set of 119 cases with clinical annotation, including lymph
node status (an additional 12 individual cases with clinical annotation were
set aside as a validation set), and
(iii)a recent study, Fridlyand et al. (2006), which contains quantitative informa-
tion concerning the probabilistic dependence between the variables HR status
and 22q12—this provided a further bridge between clinical and genomic
variables.
From the total number of chromosomal bands in the human genome, we se-
lected 28 bands in this proof-of-principle application. The chosen bands were
hypothesised to be closely associated with tumour characteristics, progression
and outcome (as represented by variables in the clinical net) based on genes
with known function present on the bands. Genes were evaluated according to
the biological processes they participate in, using their Gene Ontology annota-
tions, e.g., cell cycle regulation, DNA damage repair and cancer-related signal
pathways. An additional selection criterion was the presence of at least 3 relevant
genes on the band.
The larger dataset consisted of 116 separate data ﬁelds. For reasons of space,
12 sample ﬁelds are reproduced in Table 6.2. The code of the form Np/qn indi-
cates:
•
N: which chromosome (1–22, X or Y );
•
p/q: the short (p)/ long (q) arm of the chromosome;
•
n: the band on the chromosome arm (0–40, depending on chromosome).
35 (Baudis and Cleary, 2001).

6 Objective Bayesian Nets for Systems Modelling
147
Table 6.2. A sample of the larger of the genomic datasets
1p31 1p32 1p34 2q32 3q26 4q35 5q14 7p11 8q23 20p13 Xp11 Xq13
0
0
0
1
-1
0
0
1
0
0
0
-1
0
0
1
1
0
0
0
-1
-1
0
0
0
Table 6.3. A sample of the smaller of the genomic datasets
Lymph Nodes 1q22 1q25 1q32 1q42 7q36 8p21 8p23 8q13 8q21 8q24
0
1
1
1
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
The level of each band could either be unchanged (0) decreased (-1) or increased
(1).
The smaller dataset was similar to the larger one, except for the fact that it
included data on whether the patient’s lymph nodes were also involved. 11 of
the 26 data ﬁelds are reproduced in Table 6.3.
6.15
Constructing the Network
Our knowledge takes the form of a clinical database, two molecular databases,
and information about the relationship between two variables derived from a
research paper (§6.14). The clinical database determines a probability distribu-
tion freqc over the clinical variables and imposes the constraint p⇂C = freqc, i.e.,
the agent’s probability function, when restricted to the clinical variables, should
match the distribution determined by the clinical dataset. Similarly the ﬁrst
molecular database imposes the constraint p⇂M = freqm. The additional molec-
ular dataset and the paper contain the observations that deﬁne the probability
distribution freqs of three variables S = { HR status, Positive LN, 22q12}, the
ﬁrst two of which occur in the clinical dataset and the other of which occurs in
the molecular dataset; it imposes the constraint p⇂S = freqs.36
Given constraints of this form, an obnet on the variables in C, M and S can
be constructed in the following way. First use standard methods, such as Hugin
software, to learn a Bayesian net from the clinical dataset that represents freqc,
subject to the condition that the linking variables (positive LN, HR status) are
root variables (Fig. 6.2). Similarly learn a Bayesian net from the larger genomic
36 Fridlyand et al. (2006) report frequency data on gain and loss of 22q12 in breast
cancer dependent on oestrogen hormone receptor status. Interestingly, loss of 22q12
is far more frequent in ER positive tumours; in their study, 45% of ER positive cases
showed loss of 22q12, and 5% exhibited gain. In contrast, in ER negative tumours,
loss or gain occurs with equal frequency of 20%. This information was used to add
an additional arrow between HR status and 22q12, and the conditional probability
table for 22q12 was amended to reﬂect this dependence using the frequency data.

148
S. Nagl, M. Williams, and J. Williamson
Fig. 6.2. The graph of the Bayesian net constructed from the clinical dataset
Fig. 6.3. The graph of the Bayesian net constructed from the large genomic dataset
dataset that represents freqm, ensuring that the linked variable (22q12) is a
root of the net (Fig. 6.3). Finally learn a bridging network, Fig. 6.4, from the
smaller genomic dataset and the study, merge the three graphs to form one graph
by merging identical variables, and integrate the conditional probability tables.
Fig. 6.5 shows the graph of the resulting integrated obnet.
Here a conﬂict arose between the probability distribution determined by the
clinical dataset and the probability distribution determined by the genomic
dataset used to bridge the genomic and clinical variables: these gave diﬀerent
values to the probability of Positive LN. In §6.7, we pointed out that if nei-
ther dataset were to be preferred over the other, then the conﬂicting datasets

6 Objective Bayesian Nets for Systems Modelling
149


LN HHHH
j


22


HR 
*
Fig. 6.4. The graph of the Bayesian net constructed from the smaller genomic dataset
and the published study. The variables are Positive LN, HR status and 22q12.
Fig. 6.5. The objective Bayesian net. Probability for positive lymph node status is
set to 1 (black bar), and the calculated probability distributions for selected nodes are
shown (HR status: 0 negative, 1 positive; chromosomal bands: -1 loss, +1 gain, 0 no
rearrangement; RT radiotherapy).
constrain the probability of the assignment in question to lie within the closed
interval bounded by the conﬂicting values; one should then take the least com-
mittal value in that interval. But in this case there are reasons to prefer one
dataset over the other. First, the clinical dataset is based on a much larger sam-
ple than the bridging genomic dataset—for example, the clinical dataset has
2935 people with LN = 0, while the bridging genomic dataset has 56. Second,
the molecular dataset has a clear sample bias: overall a frequency bias in favour
of loss of 22q12 in breast cancer has been observed (20% loss vs. 7% gain in 800
cases in the progenetix database; accessed on the 14th of June 2006); further-
more, one may hypothesise that the presence of the KREMEN1 gene on 22q12
suggests that band loss, rather than gain, is more likely to be correlated with
positive lymph node status, at least in certain karyotype contexts (§6.16). Thus
the clinical data should trump the genomic data over the conﬂict on LN , i.e.,

150
S. Nagl, M. Williams, and J. Williamson
Table 6.4. Probability table for Positive LN in the obnet
Positive LN
p
0
0.62
1
0.38
Table 6.5. Conditional probability table for 22q12 in the obnet
Positive LN
0
0
1
1
22q12 HR Status
0
1
0
1
-1
0.082 0.205 0.266 0.567
0
0.835 0.772 0.468 0.370
1
0.082 0.023 0.266 0.063
the probability table of LN in the obnet, Table 6.4, is simply inherited from the
clinical net. The conditional probability table for 22q12 is depicted in Table 6.5.
Validation
Validation of our merged network is diﬃcult; almost by deﬁnition, a suitable
dataset involving the whole domain does not exist (if it did, we would not need
to use this technique; we would simply use the dataset to learn a Bayesian net).
Because of this, the best we were able to do was to use a small validation set with
11 test cases; validation showed reasonable agreement between the test cases and
the obnet, but we must be careful not to over-interpret the results.
We approached the validation from two sides; the ﬁrst was to set the 22q12
status and observe the eﬀect on lymph node (LN) status; the second was to set
the status of the lymph node variable, and observe the eﬀect on 22q12 status.
Unfortunately, the test set is both small and contains few cases of of 22q12
alteration.
Table 6.6. Setting 22q12 and observing LN status
22q12 LN status: Predicted from net Actual No.
1
0.62
0
1
0
0.5
0.55
9
-1
1
1
1
As can be seen from Table 6.6, only the linkage between no change of 22q12
and LN status predicted by the net is reﬂected in the validation set. It is diﬃcult
to interpret the results for the other values of 22q12.

6 Objective Bayesian Nets for Systems Modelling
151
Table 6.7. Setting LN status and observing 22q12
LN status 22q12:
Predicted from net
Actual
No.
+
0: 0.84, 1: 0.08, -1:0.08
0: 1.00
5
-
0: 0.95, 1: 0.05
0: 0.66, 1: 0.16, -1:0.16
6
Entering evidence in LN status, we have the results of Table 6.7. As we can
see, both cases of LN status agree reasonably well with the observed cases, but
again we must be careful not to over-interpret this relationship.
6.16
Interpretation of the Integrated Obnet
We have presented a general method for merging Bayesian networks which model
knowledge in diﬀerent areas. This method was applied to an example application
linking knowledge about breast cancer genomics and clinical data. As a result,
we have been able to examine the inﬂuence of karyotype pattern on clinical
parameters (e.g., tumour size, grade, receptor status, likelihood of lymph node
involvement) and vice versa (Fig. 6.5).
In the post-genome era, prognostic prediction and prediction of targets for
new anti-cancer treatments from omic and clinical data are becoming ever more
closely related—both need to relate molecular parameters to disease type and
outcome. This correspondence is very clearly reﬂected in the uses to which the
integrated obnet may be put. Obnet analysis may facilitate (i) discovery of ge-
nomic markers and signatures, and (ii) translation of clinical data to genomic
research and discovery of novel therapeutic targets.
Discovery of Genomic Markers
Since hormone receptor status and lymph node involvement are well-known prog-
nostic factors for survival and disease recurrence in patients with breast cancer,
the ability to link karyotype patterns to this is clearly of great potential sig-
niﬁcance. Previous tumour genotyping in breast cancer has already shown the
usefulness of genomic rearrangements as prognostic indicators.37
For clinical decision making, this technique may also be useful when applied
to integrate karyotype or other molecular data with parameters that cannot be
observed in routine clinical practice, but are of clinical signiﬁcance. An example
might be the presence of distant metastasis on PET-CT, an imaging modality
that may be present in the research setting but is not widely available in the
clinic, but which may have prognostic signiﬁcance for breast cancer recurrence.
The use of such a net would then allow practitioners, where PET-CT is not
available, to use genomic data to estimate the likelihood of a positive scan.
There are of course, many diﬀerent possible options for such networks, and it
37 See, e.g., Al-Kuraya et al. (2004).

152
S. Nagl, M. Williams, and J. Williamson
remains an open question as to which will, in clinical terms, prove to be the most
useful.
Large clinical datasets are extremely expensive and diﬃcult to collect. This is
particularly true in diseases such as breast cancer, where the risk of recurrence
extends up to at least 10 years, and hence requires long-term follow-up for ac-
curate estimation. However, the generation of potential new predictive markers,
such as genomic information or cell surface proteins, for exploration is currently
a signiﬁcant area of research. The correlation of such markers with better known
clinical markers is (relatively) simple, in that it does not require long-term follow-
up, and can be estimated following standard surgical treatment. However, for
such information to be useful, it must be integrated with the existing databases
on long-term outcomes, and it is this that we have demonstrated here.
Translation of Clinical Data to Genomic Research
The probabilistic dependence between 22q12 status and lymph node involve-
ment was followed up by analysis of the genes with known function on this
chromosomal band. This strongly suggested a causal interpretation of the de-
pendency relationship based on knowledge of cellular pathways which regulate
biological processes (mechanistic causation). KREMEN1 encodes a high-aﬃnity
dickkopf homolog 1 (DKK1) transmembrane receptor that functionally cooper-
ates with DKK1 to block wingless (WNT)/beta-catenin signalling, a pathway
which promotes cell motility.38 Loss of 22q12 may therefore contribute to cancer
cell migration through loss of the inhibiting KREMEN1 protein. The probability
distribution for 22q12 is consistent with this hypothesis (Fig. 6.5).
In total, twelve genes implicated in cell migration and metastatic potential
were identiﬁed on 22q12 and the other bands shown in Fig. 6.5. Like KREMEN1,
the protein products of the other eleven genes can also be placed in the context
of the metastatic pathways they participate in. Provided that appropriate ki-
netic interaction data are available, computational pathway modelling39 can be
employed to predict changes in pathway function resulting from the probabilis-
tically dependent band gains and losses and concomitant changes in gene copy
number. Molecularly targeted intervention strategies aimed at bringing about a
therapeutic response in the cells so aﬀected can be explored by running simu-
lations using such pathway models. Simulation may be seen as being motivated
by an agency-oriented notion of causation (see also §6.20).
Part IV: Further Development of the Method
6.17
Qualitative Knowledge and Hypotheses
There are various ways in which we intend to develop the method presented
here.
38 (Mao et al., 2002).
39 (Alves et al., 2006).

6 Objective Bayesian Nets for Systems Modelling
153
First, as discussed in §6.11, there are a variety of knowledge sources that we
hope to integrate. These include argumentation systems, medical ontologies, and
causal relationships, as well as the clinical and molecular datasets which have
been the focus of this chapter. In this Part, we shall discuss some of these other
knowledge sources.
Second, as indicated in §6.16, we intend to exploit the objective Bayesian net
that integrates these knowledge sources by using it not only for prognosis but also
as a device for hypothesising new qualitative relationships amongst the variables
under consideration. If the obnet reveals that two variables are probabilistically
dependent, and that dependence is not explained by background knowledge, then
we may hypothesise some new connection between the variables that accounts for
their dependence. For example, we may hypothesise that the variables are causally
(§6.20) or ontologically (§6.19) related. Furthermore, any such dependence can be
used to generate qualitative arguments (§6.18): each variable will be an argument
for or against the other, according to the direction of the dependence.
Third, we can increase the complexity of the formalism, in order to model
temporal change or diﬀerent levels of interaction, for instance. We shall discuss
such extensions in §6.21.
These are avenues for future research. In this Part it will suﬃce to make some
remarks on the likely directions that such research will take.
6.18
Argumentation
So far we have described how the network was developed, and analysed its per-
formance as a Bayesian network. However, as suggested in §6.10 above, we are
interested in more than just the probabilistic interpretation of the network—we
are also interested in what the new network says about the world. Whereas in
that section we suggested moving from qualitative to quantitative knowledge,
here we shall discuss the opposite.
Bayesian Networks as Arguments
Bayesian networks are a useful tool for providing estimates of the probability for
a variable of interest based on the evidence. Of course, Bayesian networks are
not the only method of doing so, and there has been much work over the years
on diﬀerent formal methods to support decision making (rule-based systems,
support vector machines, regression models, etc.). More generally, humans often
use the notion of weighing up ‘the arguments’ for a belief or action to help
them come to a conclusion. The argumentative method goes back at least 2500
years, and extends beyond the Graeco-Roman tradition.40 Arguments have the
advantage that they can present not only a conclusion, but also its justiﬁcation.
The idea of trying to base decision-making on arguments has a long history.
The ﬁrst clear example of an (informal) procedure for doing so was described by
Benjamin Franklin.41
40 (Gard, 1961).
41 (Franklin, 1887).

154
S. Nagl, M. Williams, and J. Williamson
Of course, this informal notion of an argument can be neither implemented
nor assessed in any rigorous fashion, but over the last 10-15 years there has been
some work on developing competing formal (and computable) models of argu-
ment.42 More recent work has drawn together developments in non-monotonic
logic and insights from cognitive science to produce a number of diﬀerent argu-
mentation frameworks.43 We do not intend to present a review of the ﬁeld here,
but suﬃce to say that there are two general themes, argument formation and
argument resolution. Each competing formalism deﬁnes these slightly diﬀerently,
but in general an argument is a set of premises that allow one to deduce, via a
set of rules, some set of conclusions. Resolution of competing arguments varies
considerably between formalisms, is diﬃcult to summarise in general terms, and
matters less for our discussion here. However, what interests us is how one can
interpret our new Bayesian network in terms of arguments. In other words, given
a Bayesian network, what can we say about the arguments for and against a set
of propositions, and given a new Bayesian network (formed from two or more ex-
isting ones) what new arguments can we make? Two of the authors of this paper
have previously presented a simple technique for developing arguments from a
Bayesian network,44 basing the arguments on a relatively simple argumentation
formalism,45 and we use the method outlined there to develop our arguments
from the Bayesian network. For reasons of space, we do not present the details
of our method here; they can be found in Williams and Williamson (2006). In-
stead, let us consider what it might mean, in general terms, for a probabilistic
statement to be interpreted as an argument. Firstly, therefore, let us consider
what we mean by an argument.
Rules, Arguments and Probability
Intuitively, an argument is a line of reasoning that proceeds from some premise(s)
via a set of deductions to some conclusion. As we all know, arguments are
defeasible—that is, their conclusions may at some point be challenged and what
was at some point held to be ‘true’ by argument may later be found to be untrue.
We can formalise this ‘method of argument’ in various diﬀerent ways (as men-
tioned above) but in general we have a quartet of premises, rules (for deduction),
conclusions, and conﬂict between arguments. In order to map probabilistic state-
ments into an argumentative framework, therefore, we need to consider how dif-
ferent aspects of a Bayesian system map into the quartet of argumentation, and
what eﬀects this has. We shall do this below, but ﬁrst we need to establish a (fairly
trivial) mapping between Bayesian notation and argumentative notation. We do
this by considering all variables to be binary in nature, and each node in the net-
work to represent a single binary-valued variable. The mapping between such a
network and a truth-valued logic (say, propositional logic) should be clear: for any
variable X, p(X = 1) is interpreted as x and p(X = 0) is interpreted as ¬x.
42 (Fox and Parsons, 1997).
43 (Amgoud et al., 2004; Hunter and Besnard, 2001; Krause et al., 1995).
44 (Williams and Williamson, 2006).
45 (Prakken and Sartor, 1996).

6 Objective Bayesian Nets for Systems Modelling
155
Given the correspondence above, mapping the premises is fairly simple: they
are the inputs to the network, i.e., the variables in the net that are instantiated.
Similarly, the conclusions are also relatively simple—they are the values of other
nodes in the network. Given any two nodes in a Bayesian network, the absence of
any connection between them implies probabilistic independence, and therefore
precludes one being an argument for the other. The presence of a connection
suggests that there may be a relationship between them. It is this relationship
that we interpret as forming the ‘rules’46 for an argumentation system, and in
its most basic form, if the truth of one variable A increases the probability of
another variable B being true, then we might write a ⇒b. An argument is
the association of a set of premises and rules that lead to a conclusion47—e.g.,
< {a, a ⇒b}, b >, where <> denotes the argument, the ﬁrst element {a, a ⇒b}
is the support and b is the conclusion. Arguments are in conﬂict if they argue
for conclusions which are mutually exclusive—so if we had another argument
< {c, c ⇒¬b}, ¬b >, this would be in conﬂict with our ﬁrst argument.
Our Network as Arguments
We are now in a position to return to the ﬁrst question we posed above—‘what
can we say about the arguments for and against a set of propositions?’ The ﬁrst
thing to observe is that our approach will only allow us to develop arguments
about literals that correspond to nodes in the network. Secondly, we can only
develop rules between literals that are linked in the network. Thus, while we
might know of some connection, that connection will not appear unless it also
appears as a conditional dependence (and hence a link) in our network. This is
why the procedure outlined in §6.10 is so important: all background knowledge
must be taken into account in the construction of the objective Bayesian net.
Of course, in general we might add some additional rules (from other sources),
but the rules (and hence the arguments) developed from the network will only
be concerned with those literals that appear and are associated in the network.
Thirdly, given the dichotomised nature of the variables, we have a tendency
to develop arguments both for and against literals. We can see this from the
following example. The CPT for the Tumour size node from our clinical network
is shown in Table 6.8. From these values we can calculate that p(T Size = 0–
20) is 0.657, whilst p(T Size = 0–20 |LN = 0) is 0.753, and p(T Size = 0–20
|LN = 1) is 0.5. Therefore, following the method outlined above, we can see that
we should develop the following rules:
•
(LN = 0) ⇒(T Size = (0–20))
•
(LN = 1) ⇒¬(T Size = (0–20))
On the one hand, this may seem to problematic—the generation of pairs of
opposing rules (and hence arguments) might lead us to some sort of deadlock.
46 A ‘rule’ in this context is a defeasible piece of knowledge that allows one to infer the
value of one variable from another.
47 Most systems, including ours, also impose further restrictions to ensure arguments
that are consistent and non-circular.

156
S. Nagl, M. Williams, and J. Williamson
Table 6.8. Conditional probability table for Tumour Size
Grade
1
1
2
2
3
3
T Size(mm) Positive LN
0
1
0
1
0
1
0—20
0.85 0.66 0.76 0.5 0.62 0.42
20—50
0.14 0.33 0.21 0.5 0.35 0.58
50—150
0.005
0
0.02
0
0.03
0
No.
689
271 1668 990 578 535
However, this is not a bad as it may seem. Firstly, it seems intuitively correct
to develop rules for both options—after all, the whole point of the Bayesian
net is that it contains information about both options. Secondly, the ‘deadlock’
between the rules can be resolved in a variety of ways (for example, we could
encode the likelihood of the diﬀerent options as ‘weights’ given to the rules).
Thirdly, the point of the rules, and arguments, is to allow us to help integrate
human decision-making with our Bayesian techniques, and to allow us to do
this we need to display arguments for both options, even if they have diﬀerent
weights. Finally, in this case it would of course be impossible to have a measure-
ment for both LN = 0 and LN = 1 at the same time (although we might have
other arguments for both at the same time, as we shall see below).
New Arguments from New Networks
The second question we asked above was ‘given a new Bayesian network. . .what
new arguments can we make?’ In a sense, the answer is (almost) ‘none’. After
all, as we said above, all the rules are developed from existing literals and rela-
tionships in a Bayesian network. Since our new network is only a combination of
the existing networks, there should not be anything new. However, this answer
misses one of the aspects that is crucial to the diﬀerence between argumentation
and Bayesian networks.


A
-


B
-


C
Fig. 6.6. The graph of a Bayesian network
One of the key features of Bayesian networks, as mentioned in §6.8 is that
each variable is probabilistically independent of its non-descendants conditional
on its parents, and as is noted above, this has some very desirable properties from
a computational aspect. However, once we consider developing arguments from
our network, we see that this relationship comes out diﬀerently when applied to
arguments. For example, consider a (very simple) Bayesian net, whose graph is
shown in Fig. 6.6. Depending on the probabilities in the net, we may develop
the rules:
•
(A = 1) ⇒(B = 1) which we write as a ⇒b
•
(B = 1) ⇒(C = 1) which we write as b ⇒c

6 Objective Bayesian Nets for Systems Modelling
157
•
(A = 0) ⇒(B = 0) which we write as ¬a ⇒¬b
•
(B = 0) ⇒(C = 0) which we write as ¬b ⇒¬c
Now, according to the Bayesian network, B screens C oﬀfrom A. However, under
our argumentation formalism, we might have the following arguments:
•
A1: < {a, a ⇒b, b ⇒c}, c > as an argument for c
•
A2: < {¬a, ¬a ⇒¬b, ¬b ⇒¬c}, ¬c > as an argument for ¬c
which seems to make explicit the dependence of c on a. Now, if we do not know
the status of b, then in both formalisms, we understand that in fact the best guide
to the state of c is the state of a, and so the two approaches are in agreement.
If we know both a and b, and they are ‘concordant’ (e.g. a and b or ¬a and ¬b)
then we will ﬁnd that indeed a is ‘redundant’, and c is entirely determined by b.
However, our work is motivated by the fact that our knowledge is often partial
and conﬂicting. For example, we might have one piece of information about a
and another about b, and they may conﬂict—for example, we may believe both
¬a and b. In such a situation, the Bayesian net approach would typically discard
the information about a, as it would be over-ridden by the information about b.
Under an argumentative approach, however, we are able to construct arguments
for both c and ¬c, as shown below:
•
A1: < {b, b ⇒c}, c >
•
A2: < {¬a, ¬a ⇒¬b, ¬b ⇒¬c}, ¬c >
Thus the argumentation diﬀers from the Bayesian net in that it does not
follow the probabilistic independencies of the net. Obviously at some point we
will need to resolve this disagreement, but we can at least start by considering
both cases. The Bayesian net approach retains probabilistic validity, but only
by enforcing a set of strict rules, one of which is committing to a particular
value of certain variables (b in our example); the argumentative approach loses
this precision, but has the advantage that it can handle conﬂicting premises and
generate arguments based on them, which it then resolves, rather than losing
this information (b and ¬a in our example). Such diﬀerences are not unique to
our particular brand of argumentation (for example, they are seen in Parsons’
qualitative probability framework,48 which devotes a considerable amount of
space to discussing the problem).
We are now ﬁnally in a position to answer our second question. When we add
a new network, we can still only develop the same rules that we developed in
each network. However, because we can use the rules to form arguments, we can
form arguments that are ‘bigger’ than those formed in either network alone. For
example, consider our merged network, Fig. 6.5. In this case, we can see that
22q12 is connected to lymph node status, and we would have been able to form
an argument linking the two from the genomic net alone. However, given the
links in the network, we can form an argument (but not a valid probabilistic
relationship) which would link lymph node status and 19p13 status, even if we
48 (Parsons, 2003, 2004).

158
S. Nagl, M. Williams, and J. Williamson
know 22q12 status. Such an argument is not interpreted probabilistically but
may still be useful for explaining the links to human users.
6.19
Ontologies
In §6.11 we mentioned some of the diﬀerent types of knowledge that we might
try and integrate into our Bayesian network; one important category is onto-
logical knowledge. Ontologies (formal representations of vocabularies and con-
cept relationships) and common data elements support automated reasoning
including artiﬁcial intelligence methods such as Bayesian networks. Various
standard vocabularies and object models have already been developed for ge-
nomics, molecular proﬁles, certain molecular targeted agents, mouse models of
human cancer, clinical trials and oncology-relevant medical terms and concepts
(SNOMED-RT/CT, ICD-O-3, MeSH, CDISC, NCI Health Thesaurus, caCORE,
HUGO). There are also existing ontologies describing histopathology (standards
and minimum datasets for reporting cancers, Royal College of Pathologists;
caIMAGE, NCI). The European Bioinformatics Institute (EBI) is developing
standards for the representation of biological function (Gene Ontology) and
the Microarray Gene Expression Data (MGED) Society is developing MIAME,
MAGE, and the MAGE ontology, a suite of standards for microarrays (tran-
scriptomic, array CGH, proteomic, SNP). However, signiﬁcant gaps still exist
and eventually, all cancer-relevant data types (see the NCRI Planning Matrix,
www.cancerinformatics.org.uk/planning matrix.htm) will need to be formalised
in ontologies. These eﬀorts are ongoing and pursued by a large community of
researchers (see above, and ftp1.nci.nih.gov/pub/cacore/ ExternalStds/ for fur-
ther details on available standards). Clearly, it would be desirable to incorporate
the fruits of these eﬀorts in our scheme for knowledge integration; the potential
for using ontologies as a knowledge source will increase with the maturation of
these other initiatives.
While we would like to base our objective Bayesian net on ontological knowl-
edge as well as our other knowledge sources, we also believe that we could es-
tablish some possible ontological relationships from our Bayesian network. The
most obvious of these is in establishing a sub-/ super-class relationship between
two variables. For example, imagine a dataset which recorded both whether
someone had had breast cancer and if they had had each individual subtype
of breast cancer (and also included those without breast cancer). Such a net-
work would contain several nodes, but in each case, if any subtype of cancer
was positive, then the ‘has cancer’ variable would also be positive. Such pat-
terns of conditional dependence may be complex—in our example, there would
be several diﬀerent nodes linking to the ‘has cancer’ node, but in general are in-
dicative of the presence of a sub-/super-class relationship (where the dependent
variable is the superclass). We may take this idea further by suggesting that if
there are certain combinations of variables that (together) are highly predictive
of another variable, we might regard those individuals as acting as a ‘deﬁnition’
of the outcome variable. For example, consider a dataset which records whether

6 Objective Bayesian Nets for Systems Modelling
159
individuals (of diﬀerent species) are human or not, whether they are women or
men, and if they have XX or XY chromosomes. Now, those individuals with XX
chromosomes are of course women, and so all individuals who are human and
have XX chromosomes will also be women. From this, we might deduce that in
fact, the two are equivalent, and thus being human and having XX chromosomes
is the same as being a woman. This approach is, to say the least, prone to error,
but provides a way to start learning such deﬁnitions from data, something that
currently has to be done by hand.
In an ideal world, we would expect these relationships to be absolute, but
in reality, we should allow for there being some ‘noise’ in the data, and may
well be willing to accept a near-absolute (say 95% or 98%) as being suggestive
of such a link. However, this is not the same as saying that the ontological
relationship is probabilistic, as some authors do. Instead, it is based upon a
supposition that the ontological relationship is absolute, but that the data may
imperfectly reﬂect this. Interestingly, however, we seem to rarely see this sort
of relationship in our networks. The reason for this is that the resolution of
ontological relationships is one of the things that we tend to do at either the data
collection or pre-processing stage. For example, as a part of our pre-processing
of the data we combined the Oestrogen and Progesterone receptor status into
a new variable, Hormone Receptor status, where the class of Oestrogen and
Progesterone receptors are subclasses of Hormone Receptors. However, this is
not to say that such relationships will never be important. One of the aims of
the semantic web, and science on the semantic web, is to enable large amounts of
data to be shared; such sharing will necessitate automatic handling of data (as
manual processing of larger and larger databases becomes harder and harder),
and tools for the handling of data may be able to use such strong probabilistic
relationships to highlight potential ontological issues to the user.
6.20
Causal Relationships
Concepts of Causation in Complex Systems
Since each patient’s cells evolve through an independent set of mutations and
selective environments, the resulting population of cancer cells in each patient is
likely to be unique in terms of the sum total of the mutational changes they have
undergone. Inter-personal variability has given rise to the new ﬁeld of ‘pharma-
cogenomics’ (in cancer and other diseases) which has as its ultimate aim diag-
nostic and prognostic prediction, and design of individualised treatments based
on patient-speciﬁc molecular characteristics. Given the prevailing high degree of
uncertainty in the face of biological complexity, pharmacogenomics oﬀers great
promise, but is also ‘high risk’. Risk has, for example, been highlighted by re-
cent ﬁndings of the nonreproducibility of classiﬁcation based on gene expression
proﬁling (expression microarrays, transcriptomics).49 In this situation, diagnosis
and prognosis based on biomarkers or proﬁles of multiple molecular indicators
49 See, for example, Michielsa et al. (2005).

160
S. Nagl, M. Williams, and J. Williamson
may lead to mis-classiﬁcation, and may identify patients as likely non-responders
to a given treatment when in fact they would derive beneﬁt or, conversely, may
falsely predict eﬃcacy in patients for whom none can be achieved. One may
argue that this uncertainty, at least in part, is compounded by prevailing no-
tions of biological causality which is still preoccupied with the search for single
(or a small number of) physical causes, and a failure to take into account the
characteristics of complex systems.
Diﬀerent views on the nature of causality lead to diﬀerent suggestions for
discovering causal relationships.50 Medicine has been, due to its very nature,
particularly focused on an agency-oriented account of causality which seeks to
analyse causal relations in terms of the ability of agents (doctors, health pro-
fessionals and scientists) to achieve goals (cures, amelioration of symptoms) by
manipulating their causes. According to this conception of causality, C causes
E if and only if bringing about C would be an eﬀective way of bringing about
E. Or conversely, for example, in the context of therapeutic intervention, C is
seen as a cause of E if by inhibiting C one can stop E from happening. In this
intervention-oriented stance, the agent would also seek to ground this view of
causality in a mechanistic account of physical processes, as, for example, in the
mechanistic mode of action of a drug. In diagnostic and prognostic prediction
from patient data, a causal framework is also implied; here, causation may be
conceptualised as agency-based, mechanistic or in terms of a probabilistic re-
lationship between variables. However, the extensive literature on the subject
reveals a number of problems associated with all three approaches.51
An alternative view of causality, termed epistemic causality by Williamson
(2005a), overcomes the strict compartmentalisation of current theories of cau-
sation, and focuses on causal beliefs and the role that all of these indicators
(mechanistic, probabilistic, agency-based) have in forming them. It takes causal-
ity as an objective notion yet primarily a mental construct, and oﬀers a formal
account of how we ought to determine causal beliefs.52 This approach will be
applied to glean causal hypotheses from an obnet, as outlined below.
We are faced with a profound challenge regarding causation in complex bio-
logical systems. In her discussion of developmental systems and evolution, Susan
Oyama observes ‘what a cause causes is contingent and is thus itself caused’.53
The inﬂuence of a gene, or a genetic mutation, depends on the context, such as
availability of other molecular agents and the state of the biological system, in-
cluding the rest of the genome. Oyama argues for a view of causality which gives
weight to all operative inﬂuences, since no single inﬂuence is suﬃcient for a bio-
logical phenomenon or for any of its properties. Variation in any one inﬂuence,
or many of them, may or may not bring about variation in the result, depending
on the conﬁguration of the whole. The mutual dependence of (physical) causes
leads to a situation where an entire ensemble of factors contribute to any given
50 (Williamson, 2007a).
51 (Williamson, 2005a; Williamson, 2007a, and references therein).
52 (Williamson, 2007a).
53 (Oyama, 2000, pp. 17–18 and references therein).

6 Objective Bayesian Nets for Systems Modelling
161
phenomenon, and the eﬀect of any one factor depends both on its own proper-
ties and on those of the others, often in complex combinations. This gives rise
to the concept of organised causal networks and is a central insight of systems
thinking. The biological relevance of any factor, and therefore the information
it conveys, is jointly determined, typically in a statistically interactive fashion,
by that factor and the entire system’s state.54
Whilst ‘systems thinking’ is likely to be fundamental for biomedicine and for
cancer, in particular, due to its overwhelming complexity, we still lack a princi-
pled methodology for addressing these questions. Methodology development is
a pressing need, and it is with this major objective in mind that our research
is undertaken. The work presented here combines a multidisciplinary framework
of biological systems theory and objective Bayesian network modelling; our next
step will be to integrate epistemic causality into this framework.
Here, it may be helpful, or even necessary, to draw a distinction between
fundamental science and applied biomedical research. In systems biology, the
ultimate goal may be to gain a complete mechanistic explanation of the system
complexity underlying causal networks. The achievement of this aim still lies in
the, possibly far distant, future. In contrast, in biomedical research and clinical
practice, we tend to be more immediately interested in discovering molecular
predictors for diagnostic and prognostic purposes, and in developing eﬀective
strategies for intervention in malfunctioning body systems and disease processes.
Perhaps surprisingly, an applied focus of this kind may work in our favour
vis-`a-vis biological complexity, as progress is not as severely constrained by a
requirement for an exhaustive mechanistic elucidation of the complete system.
In this chapter we sketch a discovery strategy which is based on the epistemic
view of causality (see below and §6.16). The strategy integrates probabilistic
dependency networks (Bayesian networks) with expert knowledge of biological
mechanisms, where available, to hypothesise causal networks inherent in the sys-
tem. This approach enables one to predict probabilistic biomarker proﬁles and
targets for intervention based on the identiﬁed dependencies between system
components. Interventions may then be tested by computational modelling and
experimental validation which may be seen as foregrounding ‘agency-based’ cau-
sation. This is a pragmatic strategy which can yield insights into system function
which are attainable now and are valuable from a biomedical point of view.
Gleaning Causal Relationships from an Obnet
The epistemic theory of causality maintains the following.55 Just as, under the
objective Bayesian account, an agent’s rational degrees of belief should take the
form of a probability function objectively determined by her evidence, so too
her rational causal beliefs, represented by a directed acyclic graph (dag), are
objectively determined by her evidence. An agent should adopt, as her causal
belief graph, the most non-committal graph (i.e., the dag with fewest arrows)
that satisﬁes constraints imposed by her evidence.
54 (Oyama, 2000, p. 38).
55 (Williamson, 2005a, Chapter 9).

162
S. Nagl, M. Williams, and J. Williamson
Now, evidence imposes constraints in several ways. (i) The agent may already
know of causal connections, in which case her causal graph should contain the
corresponding arrows. (ii) She may know that A only occurs after B, in which
case her causal graph should not contain an arrow from A to B. (iii) Or a causal
connection from A to B may be incompatible with her scientiﬁc knowledge inas-
much as her scientiﬁc knowledge implies that there is no physical mechanism
from A to B and hence no possible physical explanation of B that involves A;
then there should be no arrow from A to B in her causal belief graph. (iv) Or
there may be a strategic dependence from A to B (i.e., A and B may be prob-
abilistically dependent when intervening to ﬁx A and controlling for B’s other
causes) for which the agent has no explanation in her background knowledge;
she should then have an arrow from A to B in her causal graph to explain the
dependence, as long as other knowledge does not rule out such an arrow.
One can determine the agent’s causal belief graph by running through all
dags and taking a minimal graph that satisﬁes the constraints (i–iv) imposed by
background knowledge; but such a method is clearly computationally intractable.
Two other, more feasible methods are worth investigating. The agent’s causal
belief graph can be approximated by a minimal dag that satisﬁes the Markov
condition and constraints of type (i–iii); standard Bayesian net software can
be used to construct such a graph. Or one can generate an approximation to
the causal belief graph by constructing a graph that satisﬁes constraints (i–iii)
and incrementally adding further arrows (also satisfying these constraints) that
correspond to strategic dependences in the obnet. These extra arrows are causal
hypotheses generated by the objective Bayesian net.
6.21
Object-Oriented, Recursive and Dynamic Obnets
Another avenue for future research concerns extensions of the objective Bayesian
net framework to cope with object orientation, recursion and temporal change.
Object-oriented and dynamic Bayesian networks possess certain advantages
for the modelling of complex biological systems. For example, Dawid, Mortera
and Vicard have applied OOBNs to the domain of genetics and complex forensic
DNA proﬁling56 and Bangsø and Olesen showed how OOBNs can be adapted to
dynamically model processes over time, such as glucose metabolism in humans.57
Object-oriented Bayesian networks (OOBNs) allow one to represent complex
probabilistic models.58 Objects can be modelled as composed of lower-level ob-
jects, and an OOBN can have nodes that are themselves instances of other
networks, in addition to regular nodes. In an OOBN, the internal parts of an
object can be encapsulated within the object. Probabilistically, this implies that
the encapsulated attributes are d-separated from the rest of the network by the
object’s inputs and outputs. This separation property can be utilised to locally
56 (Dawid et al., 2007).
57 (Bangsøand Olesen, 2003).
58 (Koller and Pfeﬀer, 1997; Laskey and Mahoney, 1997).

6 Objective Bayesian Nets for Systems Modelling
163
constrain probabilistic computation within objects, with only limited interaction
between them.
By representing a hierarchy of inter-related objects, an OOBN makes organ-
isational structure explicit. OOBN ‘is-a’ and ‘part-of’ hierarchical structuring
mirrors the organisation of ontologies in the biomedical knowledge domain (see
§6.19), and ontologies can therefore be used as background knowledge to struc-
ture an OOBN.
Recursive Bayesian networks oﬀer a means of modelling a diﬀerent kind of
hierarchical structure—the case in which variables may themselves take Bayesian
networks as values.59 This extra structure is required, for instance, to cope with
situations in which causal relationships themselves act as causes and eﬀects. This
is often the case with policy decisions: e.g., the fact that smoking causes cancer
causes governments to restrict tobacco advertising.
The timing of observations (e.g., symptoms, measurements, tests, events)
plays a major role in diagnosis, prognosis and prediction. Temporal modelling
can be performed by a formalism called Temporal Bayesian Network of Events
(TBNE).60 In a TBNE each node represents an event or state change of a vari-
able, and an arc corresponds to a causal-temporal relationship. A temporal node
represents the time that a variable changes state, including an option of no-
change. The temporal intervals can diﬀer in number and size for each temporal
node, so this allows multiple granularity. The formalism of dynamic Bayesian
nets can also be applied.61
OOBN properties allow one to exploit the modular organisation of biological
systems for the generation of complex models. To our knowledge, OOBNs have
not been applied to systems-oriented cancer modelling. We aim to assess the
usefulness of OOBN methods for multi-scale models of cancer systems, especially
to represent variables associated with heterogeneity in tumours. Our research
will also evaluate the uses of the TBNE formalism and dynamic Bayesian nets
for temporal models of karyotype evolution (§§6.2, 6.3) and evolving therapeutic
systems (patient/tumour-therapy-response).
In sum, then, there are a variety of situations which call for a richer formalism.
Since an obnet is a Bayesian net, one can enrich an obnet using all the techniques
available for enriching Bayesian nets: one can render an obnet object-oriented,
recursive or dynamic. The details of these extensions are questions for further
work.
6.22
Conclusion
In this chapter we have presented a scheme for systems modelling and prognosis
in breast cancer. A multiplicity of knowledge sources can be integrated by form-
ing the objective Bayesian net generated by this evidence. This obnet represents
the probabilistic beliefs that should adopted by an agent with that evidence;
59 (Williamson and Gabbay, 2005).
60 (Arroyo-Figueroa and Sucar, 2005).
61 (Neapolitan, 2003).

164
S. Nagl, M. Williams, and J. Williamson
Table 6.9. The interplay between evidence and belief
Evidence
←→
Belief
Clinical data
Probabilistic (obnet)
Genomic data
Argumentative
Published studies
Ontological
Argumentation systems
Causal
Medical ontologies
Causal knowledge
Biological theory
it can be used to assist prognosis of cancer patients. The obnet together with
evidence can, in turn, be used to generate sets of argumentative, ontological and
causal beliefs. These are just hypotheses and require testing; more data must be
collected to conﬁrm or disconﬁrm these hypotheses. These new data increase the
base of evidence and consequently new beliefs (probabilistic, causal and so on)
must be formulated. We thus have a dialectical back-and-forth between evidence
and belief, as depicted in Table 6.9.
This iterative approach to knowledge discovery facilitates novel insights and
hypotheses regarding the organisation and dynamic functioning of complex bio-
logical systems, and can lead to fruitful discovery from limited data. Objective
Bayesian nets thus provide a principled and practical way of integrating domain
knowledge, and of using it for inference and discovery.
Acknowledgements
This research was carried out as part of the caOBNET project (www.kent.ac.uk/
secl/philosophy/jw/2006/caOBNET.htm). We are very grateful to Nadjet El-
Mehidi and Vivek Patkar for their assistance with this work. For ﬁnancial support
we are grateful to Cancer Research UK, the Colyer-Fergusson Awards of the Kent
Institute for Advanced Study in the Humanities and the Leverhulme Trust.
References
Abramovitz, M., Leyland-Jones, B.: A systems approach to clinical oncology: Focus on
breast cancer. BMC Proteome Science 4, 5 (2006)
Al-Kuraya, K., Schraml, P., Torhorst, J., Tapia, C., Zaharieva, B., Novotny, H.,
Spichtin, H., Maurer, R., Mirlacher, M., Kochl, O., Zuber, M., Dieterich, H., Mross,
F., Wilber, K., Simon, R., Sauter, G.: Prognostic relevance of gene ampliﬁcations
and coampliﬁcations in breast cancer. Cancer Research 64, 8534–8540 (2004)
Alves, R., Antunes, F., Salvador, A.: Tools for kinetic modeling of biochemical net-
works. Nature Biotechnology 24, 667–672 (2006)
Amgoud, L., Cayrol, C., Lagasquie-Schiex, M.-C.: On bipolarity in argumentation
frameworks. In: NMR, pp. 1–9 (2004)
Arroyo-Figueroa, G., Sucar, L.: Temporal Bayesian network of events for diagnosis and
prediction in dynamic domains. Applied Intelligence 23, 77–86 (2005)

6 Objective Bayesian Nets for Systems Modelling
165
Bangsø, O., Olesen, K.: Applying object oriented Bayesian networks to large (medical)
decision support systems. In: Proceedings of the Eighth Scandinavian Conference on
Artiﬁcial Intelligence. IOS Press, Amsterdam (2003)
Baudis, M., Cleary, M.: Progenetix.net: an online repository for molecular cytogenetic
aberration data. Bioinformatics 17, 1228–1229 (2001)
Borak, J., Veilleux, S.: Errors of intuitive logic among physicians. Soc. Sci. Med. 16,
1939–1947 (1982)
Bulashevska, S., Szakacs, O., Brors, B., Eils, R., Kovacs, G.: Pathways of urothelial
cancer progression suggested by Bayesian network analysis of allelotyping data. In-
ternational Journal of Cancer 110, 850–856 (2004)
Cristofanilli, M., Hayes, D., Budd, G., Ellis, M., Stopeck, A., Reuben, J., Doyle, G.,
Matera, J., Allard, W., Miller, M., Fritsche, H., Hortobagyi, G., Terstappen, L.:
Circulating tumor cells: A novel prognostic factor for newly diagnosed metastatic
breast cancer. J. Clin. Oncol. 23, 1420–1430 (2005)
Dawid, A., Mortera, J., Vicard, P.: Object-oriented Bayesian networks for complex
forensic DNA proﬁling problems. Forensic Science International 169(256), 195–205
(2007)
Depew, D., Weber, B.: Darwinism evolving: systems dynamics and the genealogy of
natural selection. MIT Press, Cambridge (1996)
Fox, J., Parsons, S.: On using arguments for reasoning about actions and values. In:
Proc. AAAI Spring Symposium on Qualitative Preferences in Deliberation and Prac-
tical Reasoning, Stanford (1997)
Franklin, B.: Collected Letters, Putnam, New York (1887)
Fridlyand, J., Snijders, A., Ylstra, B., Li, H., Olshen, A., Segraves, R., Dairkee, S.,
Tokuyasu, T., Ljung, B., Jain, A., McLennan, J., Ziegler, J., Chin, K., Devries, S.,
Feiler, H., Gray, J., Waldman, F., Pinkel, D., Albertson, D.: Breast tumor copy
number aberration phenotypes and genomic instability. BMC Cancer 6, 96 (2006)
Galea, M., Blamey, R., Elston, C., Ellis, I.: The Nottingham Prognostic Index in pri-
mary breast cancer. Breast Cancer Research and Treatment 3, 207–219 (1992)
Gard, R.: Buddhism. George Braziller Inc., New York (1961)
Holland, J.: Hidden order: how adaptation builds complexity. Helix Books, New York
(1995)
Holland, J.: Emergence: from chaos to order. Addison-Wesley, Redwood City (1998)
Hunter, A., Besnard, P.: A logic-based theory of deductive arguments. Artiﬁcial Intel-
ligence 128, 203–235 (2001)
Jaynes, E.T.: Information theory and statistical mechanics. The Physical Re-
view 106(4), 620–630 (1957)
Kahneman, D., Tversky, A.: On the psychology of prediction. Psychol. Rev. 80, 237–251
(1973)
Khalil, I., Hill, C.: Systems biology for cancer. Curr. Opin. Oncol. 17, 44–48 (2005)
Kitano, H.: Biological robustness. Nat. Rev. Genet. 5, 826–837 (2004)
Koller, D., Pfeﬀer, A.: Object-oriented Bayesian networks. In: Geiger, D., Shenoy, P.
(eds.) Proceedings of the 13th Annual Conference on Uncertainty in Artiﬁcial Intel-
ligence, pp. 302–313. Morgan Kaufmann Publishers, San Francisco (1997)
Korb, K.B., Nicholson, A.E.: Bayesian artiﬁcial intelligence. Chapman and Hall / CRC
Press, London (2003)
Krause, P., Ambler, S., Elvang-Goranssan, M., Fox, J.: A logic of argumentation for
reasoning under uncertainty. Computational Intelligence 11, 113–131 (1995)

166
S. Nagl, M. Williams, and J. Williamson
Laskey, K., Mahoney, S.: Network fragments: Representing knowledge for constructing
probabilistic models. In: Geiger, D., Shenoy, P. (eds.) Proceedings of the 13th Annual
Conference on Uncertainty in Artiﬁcial Intelligence, pp. 334–341. Morgan Kaufmann
Publishers, San Francisco (1997)
Lupski, J., Stankiewicz, P.: Genomic disorders: The genomic basis of disease. Humana
Press, Totowa (2006)
Mao, B., Wu, W., Davidson, G., Marhold, J., Li, M., Mechler, B., Delius, H., Hoppe,
D., Stannek, P., Walter, C., Glinka, A., Niehrs, C.: Kremen proteins are Dickkopf
receptors that regulate Wnt/beta-catenin signalling. Nature 417, 664–667 (2002)
McPherson, K., Steel, C., Dixon, J.: Breast cancer: Epidemiology, risk factors and
genetics. BMJ 321, 624–628 (2000)
Michielsa, S., Koscielnya, S., Hill, C.: Prediction of cancer outcome with microarrays:
a multiple random validation strategy. The Lancet 365(9458), 488–492 (2005)
Mitchell, S.: Biological complexity and integrative pluralism. Cambrige University
Press, Cambridge (2003)
Nagl, S.: Objective Bayesian approaches to biological complexity in cancer. In:
Williamson, J. (eds.) Proceedings of the Second Workshop on Combining Proba-
bility and Logic. (2005),
www.kent.ac.uk/secl/philosophy/jw/2005/progic/
Nagl, S.: A path to knowledge: from data to complex systems models of cancer. In:
Nagl, S. (ed.) Cancer Bioinformatics, pp. 3–27. John Wiley & Sons, London (2006)
Nagl, S., Williams, M., El-Mehidi, N., Patkar, V., Williamson, J.: Objective Bayesian
nets for integrating cancer knowledge: a systems biology approach. In: Rouso, J.,
Kaski, S., Ukkonen, E. (eds.) Proceedings of the Workshop on Probabilistic Mod-
elling and Machine Learning in Structural and Systems Biology, Tuusula, June 17-18
2006, vol. B-2006-4, pp. 44–49. Helsinki University Printing House, Finland (2006)
Neapolitan, R.E.: Probabilistic reasoning in expert systems: theory and algorithms.
Wiley, New York (1990)
Neapolitan, R.E.: Learning Bayesian networks. Pearson / Prentice Hall, Upper Saddle
River (2003)
Nygren, P., Larsson, R.: Overview of the clinical eﬃcacy of investigational anticancer
drugs. Journal of Internal Medicine 253, 46–75 (2003)
Oyama, S.: The ontogeny of information: developmental systems and evolution, 2nd
edn. Duke University Press, Durham (2000)
Parsons, S.: Order of magnitude reasoning and qualitative probability. International
Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 11(3), 373–390
(2003)
Parsons, S.: On precise and correct qualitative probabilistic reasoning. International
Journal of Approximate Reasoning 35, 111–135 (2004)
Prakken, H., Sartor, G.: Argument-based extended logic programming with defeasible
priorities. In: Schobbens, P.-Y. (ed.) Working Notes of 3rd Model Age Workshop:
Formal Models of Agents, Sesimbra, Portugal (1996)
Quinn, M., Allen, E.: Changes in incidence of and mortality from breast cancer in
England and Wales since introduction of screening. BMJ 311, 1391–1395 (1995)
Rasnick, D., Duesberg, P.: How aneuploidy aﬀects metabolic control and causes cancer.
Biochemical Journal 340, 621–630 (1999)
Ravdin, Siminoﬀ, Davis.: A computer program to assist in making decisions about
adjuvant therapy for women with early breast cancer. J. Clin. Oncol. 19, 980–991
(2001)

6 Objective Bayesian Nets for Systems Modelling
167
Reis-Filho, J., Simpson, P., Gale, T., Lakhan, S.: The molecular genetics of breast can-
cer: the contribution of comparative genomic hybridization. Pathol. Res. Pract. 201,
713–725 (2005)
Richards, M., Smith, I., Dixon, J.: Role of systemic treatment for primary operable
breast cancer. BMJ 309, 1263–1366 (1994)
Ries, L., Eisner, M., Kosary, C., Hankey, B., Miller, B., Clegg, L., Mariotto, A., Feuer,
E., Edwards, B.: SEER Cancer Statistics Review 1975-2001. National Cancer Insti-
tute (2004)
Russo, F., Williamson, J.: Interpreting probability in causal models for cancer. In:
Russo, F., Williamson, J. (eds.) Causality and probability in the sciences. Texts in
Philosophy, pp. 217–241. College Publications, London (2007)
Toyoda, T., Wada, A.: ‘omic space’: coordinate-based integration and analysis of ge-
nomic phenomic interactions. Bioinformatics 20, 1759–1765 (2004)
Veer, L., Paik, S., Hayes, D.: Gene expression proﬁling of breast cancer: a new tumor
marker. J. Clin. Oncol. 23, 1631–1635 (2005)
Vogelstein, B., Kinzler, K.: Cancer genes and the pathways they control. Nature
Medicine 10, 789–799 (2004)
Williams, M., Williamson, J.: Combining argumentation and Bayesian nets for breast
cancer prognosis. Journal of Logic, Language and Information 15, 155–178 (2006)
Williamson, J.: Maximising entropy eﬃciently. Electronic Transactions in Artiﬁcial
Intelligence Journal, 6 (2002), www.etaij.org
Williamson, J.: Bayesian nets and causality: philosophical and computational founda-
tions. Oxford University Press, Oxford (2005a)
Williamson, J.: Objective Bayesian nets. In: Artemov, S., Barringer, H., d’Avila Garcez,
A.S., Lamb, L.C., Woods, J. (eds.) We Will Show Them! Essays in Honour of Dov
Gabbay, vol. 2, pp. 713–730. College Publications, London (2005b)
Williamson, J.: Causality. In: Gabbay, D., Guenthner, F. (eds.) Handbook of Philo-
sophical Logic, vol. 14, pp. 89–120. Springer, Heidelberg (2007a)
Williamson, J.: Motivating objective Bayesianism: from empirical constraints to objec-
tive probabilities. In: Harper, W.L., Wheeler, G.R. (eds.) Probability and Inference:
Essays in Honour of Henry E. Kyburg Jr., pp. 151–179. College Publications, London
(2007b)
Williamson, J., Gabbay, D.: Recursive causality in Bayesian networks and self-ﬁbring
networks. In: Gillies, D. (ed.) Laws and models in the sciences, pp. 173–221. With
comments, pp. 223–245. King’s College Publications, London (2005)
Xia, Y., Yu, H., Jansen, R., Seringhaus, M., Baxter, S., Greenbaum, D., Zhao, H.,
Gerstein, M.: Analyzing cellular biochemistry in terms of molecular networks. Annu.
Rev. Biochem. 73, 1051–1087 (2004)

7
Modeling the Temporal Trend of the Daily
Severity of an Outbreak Using Bayesian
Networks
Xia Jiang, Michael M. Wagner, and Gregory F. Cooper
Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA
xjiang@cbmi.pitt.edu, mmw1@pitt.edu, gfc@cbmi.pitt.edu
Summary. A disease outbreak is an epidemic limited to localized increase, e.g., in a
village, town, or institution. An epidemic curve is a graphical depiction of the number
of outbreak cases by date of onset of illness. If we could estimate the epidemic curve
early in an outbreak, this estimate could guide the investigation of other outbreak
characteristics. Furthermore, a good estimate of the epidemic curves tells us how soon
the outbreak will reach a given level of severity if it goes uncontrolled. Previously,
methods for doing real-time estimation and prediction of the severity of an outbreak
were very limited. As far as predicting future cases, ordinarily epidemiologists simply
made an educated guess as to how many people might become aﬀected. We develop a
Bayesian network model for real-time estimation of an epidemic curve, and we show
results of experiments testing its accuracy.
7.1
Introduction
Le Strat and Carrat [1999] deﬁne an epidemic as “the occurrence of a number of
cases of a disease, in a given period of time in a given population, that exceed the
expected number.” Last [2000] deﬁnes an outbreak as “an epidemic limited to
localized increase, e.g., in a village, town, or institution.” An epidemic curve
is a graphical depiction of the number of outbreak cases by date of onset of
illness [Wagner et al., 2006]. An epidemic curve is one of the most important
characteristics of an outbreak. If we could estimate the epidemic curve early
in an outbreak, this estimate could guide the investigation of other outbreak
characteristics such as the disease-causing biological agent, source, and route of
transmission. Furthermore, a good estimate of the epidemic curves tells us how
soon the outbreak will reach a given level of severity if it goes uncontrolled. This
information is crucial to public health oﬃcials when they are making decisions
concerning preparatory measures. That is, once the epidemic curve estimate
informs them as to when the epidemic may reach a given level of severity, they
become aware of the immediacy with which they must obtain suﬃcient resources
and supplies to handle disease treatment.
As an example, consider the epidemic curve in Figure 7.1. This curve was
constructed (after the outbreak was over) from clinically deﬁned and laboratory-
conﬁrmed cases of a foodborne Cryptosporidium outbreak that occurred on a
D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 169–185, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

170
X. Jiang, M.M. Wagner, and G.F. Cooper
Fig. 7.1. An Epidemic Curve for the Washington D.C. Cryptosporidium outbreak
Washington, DC university campus. The curve indicates a possible food contam-
ination through a tight clustering of cases in three days. If health care oﬃcials
had been able to estimate this curve early during the outbreak, they could pos-
sibly have determined the biological agent and the route of transmission early
enough so that appropriate control measures could have been taken to prevent
additional cases.
The epidemic curve for an outbreak is often correlated with the daily counts
of some observable event. For example, Figure 7.2 (a) shows an epidemic curve
constructed from a sample of the population aﬀected by a Cryptosporidium out-
break in North Battleford, Saskatchewan in spring, 2001. The outbreak was
caused by a contamination of public drinking water. Cryptosporidium infection
causes diarrhea. Figure 7.2 (b) shows the weekly over-the-counter (OTC) sales of
antidiarrheal drugs at one pharmacy in North Battleford during the time period
aﬀected by the outbreak. The correlation between these two curves suggests that
by monitoring OTC sales of such drugs we can possibly detect a Cryptosporidium
outbreak at an early stage, and perhaps even estimate the epidemic curve.
Previously, methods for doing real-time estimation and prediction of the sever-
ity of an outbreak were very limited. For the most part, investigators simply did
their best to intensify surveillance in an eﬀort to identify all cases so that the
observed number of cases was as close to the real number of cases as possi-
ble [Wagner et al., 2006]. However, as far as predicting future cases, ordinarily
epidemiologists simply made an educated guess as to how many people might
become aﬀected. Recently, some strides have been made in characterizing out-
breaks and predicting their severity. PANDA [Cooper et al., 2004] and BARD

7 Modeling the Temporal Trend of the Daily Severity
171
(a)
0
5
10
15
20
3/4
3/11
3/18
3/25
4/1
4/8
4/15
4/22
4/29
Date of Onset
Number of New Cases
0
20
40
60
80
3/4
3/11
3/18
3/25
4/1
4/8
4/15
4/22
4/29
5/6
Date Week Starts
Weekly Number of Units 
Sold
(b)
Fig. 7.2. An epidemic curve for a Cryptosporidium outbreak in North Battleford,
Saskatchewan is in (a), while weekly OTC sales of antidiarrheal drugs at one pharmacy
in North Battleford is in (b)
[Hogan et al., 2007], which are two state of the art outbreak detection algo-
rithms, can provide estimates of some outbreak characteristics such as source
and/or route of transmission of the outbreak. However, neither of them predicts
the future characteristics of the outbreak. Jiang and Wallstrom [2006] describe
a Bayesian network model for outbreak detection and estimation. That model
not only detects an outbreak but also estimates important characteristics of the
outbreak. Namely, it estimates the outbreak’s size and duration, and how far
into the outbreak we are. They note that knowledge of the probable values of
these variables can help guide us to a decision that maximizes expected utility.
A shortcoming of their model is that it only estimates the size and duration of
the outbreak; it does not estimate the epidemic curve. However, as discussed

172
X. Jiang, M.M. Wagner, and G.F. Cooper
above, an estimate of the epidemic curve itself would be more valuable to public
health care oﬃcials.
Next we develop a Bayesian network model that estimates an epidemic curve
from the daily counts of some observable event. After presenting the model,
we show the results of evaluating its performance using four actual inﬂuenza
outbreaks.
7.2
A Model for Epidemic Curve Estimation
The Bayesian network model presented here is applicable to estimating the epi-
demic curve for any type of outbreak from the daily1 counts of some observable
event, whose daily count is positively correlated with the daily count of outbreak
cases. For example, it could be used to estimate the epidemic curve for an out-
break of inﬂuenza from the daily counts of patients presenting in the ED with
respiratory symptoms; in addition, it could be used to estimate the epidemic
curve for a Cryptosporidium outbreak from the daily counts of units of OTC
antidiarrheal drugs sold. We will describe the model in general terms, without
referring to any particular type of outbreak. First, we present the structure of
the Bayesian network; then we discuss the parameters. See [Neapolitan, 2004]
for an introduction to Bayesian networks and dynamic Bayesian networks, which
are networks speciﬁcally designed to model times series.
7.2.1
Network Structure
The general Bayesian network structure for the model is shown in Figure 7.3. It
is assumed that we already know (or at least suspect) an outbreak is ongoing,
for example, by way of clinical case ﬁndings or epidemiology surveillance. So the
system does not try to detect whether there is an outbreak. Rather based on
daily counts (The variables labeled C[i]) of the observable event up until today,
the system estimates the epidemic curve values (The variables labeled E[i] where
i ≤0) on today and days preceding today, and predicts the epidemic curve values
(The variables labeled E[i] where i > 0) on days following today. The set of values
of all the variables labeled E[i] constitute the epidemic curve. Our estimate of
them constitutes our estimate of the epidemic curve. The darkly-shaded
nodes in the networks are constants, while the lightly-shaded nodes are the
ones that are instantiated when we use the network to make inference. Next we
describe the nature of each node. First, we describe the nodes that are constants.
1. N: This constant is the number of people in the jurisdiction being monitored.
2. A: This constant is the probability that an individual, who is sick with a
disease that could produce an occurrence of the observable event, actually
does produce an occurrence. For example, if the observable event is the sale of
one thermometer, it is the probability that an individual with a temperature
buys a thermometer. We call this probability the action tendency.
1 Although the unit of time is usually one day, it need not be. For example, we could
use weekly counts.

7 Modeling the Temporal Trend of the Daily Severity
173
Outbreak
Started
S
D
O
Outbreak
Size
Outbreak
Duration
E[1]
E[0]
E[-1]
...
# New Sick
 1  Day Forward
# New Sick
Today
# New Sick
 1 Day Ago
E[-n]
# New Sick
n Days Ago
E[m]
# New Sick
m  Days Forward
...
P
Outbreak
Peak
C[0]
Observable
Count
Today
C[-1]
Observable
Count
1 Day Ago
C[-n]
Observable
Count
n Days Ago
N
A
K
L
# People in Population
Action Tendency
Background Count of
Outbreak Disease
Background Count of
Observable
J
Data Completeness
Fig. 7.3. Bayesian network for estimating an epidemic curve
3. J : This constant is the data completeness. It is the fraction of all occurrences
of the observable event that, on the average, occur in the entities being mon-
itored. For example, if the observable event is the sale of one thermometer,
it is the fraction of all thermometers sales (in the monitored jurisdiction)
that occur in the stores whose thermometer sales are included in our count.
4. K: This constant is the average daily count of occurrences of the outbreak
disease when no outbreak is occurring. It is called the background count. For
example, if the outbreak disease is inﬂuenza, it is the average daily count of
sporadic inﬂuenza cases in non-ﬂu season.
5. L: This constant is the average daily count of occurrences of the observable
event that are not due to the outbreak disease. It is assumed to be the
same regardless of whether an outbreak is taking place. For example, if the
outbreak disease is inﬂuenza and the observable event is the sale of one
thermometer, it is the average daily count of thermometer sales that are not
due to inﬂuenza.
Next we discuss the nodes that are variables.
1. S: This is the size of the outbreak. The size is the percent of the population
that eventually becomes ill due to the outbreak. Note that this is the percent
if no measures are taken to control the outbreak. We hope to make the size
smaller by taking appropriate measures when the potential epidemic curve is
estimated. Its domain includes the integers between minsize and maxsize,

174
X. Jiang, M.M. Wagner, and G.F. Cooper
where minsize and maxsize are the minimum and maximum sizes of an
outbreak of this type.
2. D: The variable D represents the duration of an outbreak that started some
time in the previous durmax days. Its domain includes the integers between
durmin and durmax inclusive, where durmin and durmax are the minimum
and maximum durations of an outbreak of this type.
3. O: This variable represents the number of days ago an ongoing outbreak
started. Its domain includes the integers between 1 and durmax inclusive. It
depends on D because the outbreak could not have started longer ago than
the duration of the outbreak.
4. P: This variable represents the day the outbreak reaches its peak. Its do-
main includes the integers between peakmin and peakmax inclusive, where
peakmin and peakmax are the ﬁrst and last days on which an outbreak of
this type could reach its peak. It depends on D because the day on which
the peak is reached must come before the last day of the outbreak.
5. E[i]: This variable is the number of individuals becoming sick with the moni-
tored (outbreak) disease on day i. For example, if the disease being monitored
is inﬂuenza, it is the number of individuals becoming sick with inﬂuenza on
day i. E[0] is the number today, E[−i] is the number i days before today,
and E[i] is the number i days after today. By today we mean the current day
on which we are trying to estimate the epidemic curve. The domain of these
variables includes the integers between 0 and maxsick, where maxsick is the
maximum number of people becoming sick on a given day in the jurisdiction
being monitored. They depend on S and D because larger outbreaks result
in more individuals becoming sick. They depend on O and P because, during
an outbreak, the number of sick individuals increases, as the outbreak pro-
gresses, to the peak, and then decreases. In the next subsection, we illustrate
how these dependencies can be modeled.
6. C[i]: This variable is the count of the observable event on day i. For example,
if the observable event is the sale of one unit of some OTC drug, it would be
the count of number of units sold. OC[0] is the count today, and OC[−i] is
the count i days before today. By today we mean the current day on which
we are trying to estimate the epidemic curve. We can look back as many
days as deemed appropriate. Their domain includes the integers between 0
and maxotc, where maxotc is the maximum count of the observable event in
the jurisdiction being monitored. C[i] depends on E[i] because when more
individuals are sick the count is higher. In the next subsection, we illustrate
how this dependency can be modeled.
7.2.2
Network Parameters
Although the parameters in diﬀerent applications will be similar, it is diﬃcult
to describe them in the abstract. Therefore, we will describe the parameters and
their values speciﬁc to the inﬂuenza outbreak detection system that is evaluated
in the next section. Some of the parameter values were obtained from domain
knowledge of inﬂuenza outbreaks. We will not repeatedly state this when it is

7 Modeling the Temporal Trend of the Daily Severity
175
the case. First, we give the values of the constants and show how they were
obtained.
1. The population size N was obtained from census data for each jurisdiction.
2. The data completeness J was obtained from data for each jurisdiction.
3. The background inﬂuenza count K was obtained from data for each
jurisdiction.
4. For a particular jurisdiction, let
T1 = Average daily count of thermometers sold in inﬂuenza season
T2 = Average daily count of thermometers sold in non-ﬂu season
F = Average daily number of inﬂuenza cases in inﬂuenza season
K = Average daily number of inﬂuenza cases in non-ﬂu season
L = Average daily number of thermometer sold not due to inﬂuenza
A = Probability of buying a thermometer if one has inﬂuenza
J = Data completeness for this jurisdiction.
Then
T1 = J(A × F + L)
(7.1)
and
T2 = J(A × K + L).
(7.2)
We obtained the values of T1, T2, F, K, and J from data for a given juris-
diction. We then simultaneously solved Equations 7.1 and 7.2 to obtain the
values of A and L.
Next we discuss the parameters. We assumed little prior knowledge concern-
ing the probability distributions of S, D, O, and P. Our purpose was to see
how much could be learned without imposing too much of our prior belief con-
cerning inﬂuenza outbreaks. Speciﬁcally, we assumed the following probability
distributions:
1. S is uniformly distributed over all integers between 2% and 11%.
2. D is uniformly distributed over all integers between 8 and 80.
3. For a given value of D, O is uniformly distributed over all integers between
1 and D.
4. For a given value of D, it is assumed the peak occurs in the ﬁrst D/2 days,
and is more likely to be near D/2 than 0. Speciﬁcally, P has the beta(P; 3, 2)
density function discretized and adjusted to the interval [0, D/2]. We chose
the beta density function because its parameters allow us to readily model
our prior belief as to where the peak might be. The least presumptive as-
sumption would be to use the beta(P; 1, 1) density function, which is the
uniform density function. However, this assumption is completely unrealis-
tic since the peak of the outbreak would not occur on the ﬁrst day.

176
X. Jiang, M.M. Wagner, and G.F. Cooper
O
P
M
H
K
D
Fig. 7.4. The area of this region is about equal to the total number of inﬂuenza cases
during the outbreak
5. When an outbreak is ongoing, all cases of the outbreak disease are included
in the epidemic curve. This is illustrated in Figure 7.4. In that ﬁgure, M
represents the number of new cases due to the outbreak today (the day we
are making our estimate), and H represents the number of new cases due
to the outbreak on the day the outbreak reaches its peak. The remaining
variables in the ﬁgure are the ones we previously deﬁned. Given values of K,
P, and D, the total number of inﬂuenza cases during the outbreak is given
approximately by the area of the region in Figure 7.4. We say approximately,
because the total number cases is the sum of discrete values on each day.
The area of the region in Figure 7.4 approximates the sum of the columns in
the bar graph that represents these values (See e.g. Figure 7.1). We therefore
have that
S × N
100
= KD + D × H
2
.
Furthermore, for O ≤P
M
O = H
P .
Solving for M, we have
M = O
P
S × N
50D
−2K

Finally, the height E[0] of the ﬁgure at point O, which is the number of
outbreak cases on day O of the outbreak, is given by
E[0] = K + M
= K + O
P
S × N
50D
−2K

.

7 Modeling the Temporal Trend of the Daily Severity
177
Similarly, if O ≥P
E[0] = K +
D −O
D −P
 S × N
50D
−2K

.
This would be the value of E[0] if the number of daily cases was determinis-
tically determined by the other variables. We inserted a random component
by making E[0] a random variable that has the Poisson distribution with
mean E. Similarly, E[i] for i ̸= 0 is assumed to have the Poisson distribution
with mean given by the previous equalities except O is replaced by O + i.
6. We have that the expected value μi of C[i] is given by
μi = J (A × E[i] + L) .
(7.3)
We made the assumption that the day on which an individual becomes sick
with the outbreak disease is the same as the day on which the individual takes
action A. Although this would not absolutely be the case, the assumption
should be a good ﬁrst order approximation as some individuals sick from
previous days should take the action on day i, while some individuals sick
on day i should take the action on future days. So the discrepancies should
approximately oﬀset. We made C[i] a random variable that has the Poisson
distribution with mean given by Equation 7.3. In the inﬂuenza outbreak
detection system, C[i] is the count of daily thermometer sales.
When the network is used to estimate an epidemic curve, the variables C[i]
for all i are instantiated to the OTC counts of thermometer sales for today
and all days preceding today (for as many days as we choose to look back).
Inference is then done to determine the conditional probability distributions of
the variables E[i] for all i. The expected values of these variables constitute our
estimate of the epidemic curve. We used the Bayesian network package Netica
(http://www.norsys.com/) to develop the network and perform the inference.
Note that for a given i, the conditional probability distribution of E[i] depends
not only on the instantiated value of C[i] but on the instantiated values of
C[j] for all j ̸= i. This is because C[i] does not d-separate E[i] from C[j].
So the inference is much more complex than simply doing a calculation using
Equality 7.3.
7.3
Experiments
We evaluated the model by determining how well it estimates the epidemic curves
of actual inﬂuenza outbreaks.
7.3.1
Method
The data necessary to obtain the parameter values for inﬂuenza outbreak de-
tection systems (as discussed in the previous section) were available for four

178
X. Jiang, M.M. Wagner, and G.F. Cooper
jurisdictions in the United States. Furthermore, all four jurisdictions had signif-
icant inﬂuenza outbreaks starting in fall, 2003 and lasting 66-68 days. For the
sake of maintaining anonymity, we labeled the jurisdictions A, B , C, and D. We
obtained the data necessary to the outbreak detection systems and information
concerning the outbreaks from the Centers for Disease Control and Prevention
(http://www.cdc.gov) and the National Retail Data Monitor system managed
by RODS laboratory (http://rods.health.pitt.edu). The outbreak information
did not include epidemic curves themselves. However, using the durations of
the outbreaks, the inﬂuenza-like illness (ILI) curves, counts of deaths due to
inﬂuenza during the outbreaks (which were also available), and national ﬁgures
concerning inﬂuenza and inﬂuenza deaths, we were able to estimate weekly epi-
demic curves for these outbreaks. We evaluated our model using these estimates
as gold standards. However, they too are really only estimates taken from all
known data concerning the outbreaks, and are not true gold standards.
Table 7.1. Values of the constants for each jurisdiction
Jurisdiction
N
A
J
K
L
A
10, 047, 239 .143 .24 322 114
B
698, 000
.161 .57 65 9.5
C
3, 001, 146 .175 .29 97
25
D
1, 261, 303
.1
.5 130 25
Table 7.1 shows the values of the constants for each of the four jurisdictions.
7.3.2
Results
First, we show the results of our study using the uninformative probability dis-
tributions of S, D, O, and P discussed above. Then we investigate the sensitivity
of the results to these distributions.
Results Using Uninformative Probability Distributions
Figures 7.5, 7.6, 7.7, and 7.8 show the gold standard epidemic curves for the
four jurisdictions along with the system’s epidemic curve estimations on the
20th day, 25th day, and 30th day of the outbreaks. The weekly totals of the
system’s posterior expected values of the number of inﬂuenza cases were used as
the estimates. The outbreaks lasted between 66 and 68 days.
If we let n be the number of weeks the outbreak took place, xi be the sequence
of weekly values in the gold standard epidemic curve, and yi be the sequence of
weekly values in the estimated epidemic curve, we set
Error = 100% ×
n
i=1 |yi −xi|
n
i=1 xi
.

7 Modeling the Temporal Trend of the Daily Severity
179
Week
New Flu Cases
9
8
7
6
5
4
3
2
1
100000
80000
60000
40000
20000
0
Variable
LA 25 Day Est.
LA 30 Day Est.
LA Weekly Flu Count
LA 20 Day Est.
Fig. 7.5. Epidemic curve estimates for jurisdiction A
Wee k
New Flu Cases
10
9
8
7
6
5
4
3
2
1
6000
5000
4000
3000
2000
1000
0
Varia ble
Jefferson 25  Day E st.
Jefferson 30  Day E st.
Jefferson Weekly Flu Count
Jefferson 20  Day E st.
Fig. 7.6. Epidemic curve estimates for jurisdiction B

180
X. Jiang, M.M. Wagner, and G.F. Cooper
Week
New Flu Cases
10
9
8
7
6
5
4
3
2
1
25000
20000
15000
10000
5000
0
Variable
Orange 25 Day Est.
Orange 30 Day Est.
Orange Weekly Flu Count
Orange 20 Day Est.
Fig. 7.7. Epidemic curve estimates for jurisdiction C
Week
New Flu Cases
10
9
8
7
6
5
4
3
2
1
16000
14000
12000
10000
8000
6000
4000
2000
0
Variable
Allegheny 25 Day Est.
Allegheny 30 Day Est.
Allegheny Weekly Flu Count
Allegheny 20 Day Est.
Fig. 7.8. Epidemic curve estimates for jurisdiction D

7 Modeling the Temporal Trend of the Daily Severity
181
Table 7.2. Errors in estimated epidemic curves
Jurisdiction
20 Day
Error
25 Day
Error
30 Day
Error
A
69.1%
32.9%
27.6%
B
59.6%
26.4%
11.8%
C
63.2%
20.9%
37.8%
D
66.9%
32.7%
15.1%
Average
64.7%
28.2%
23.1%
If the sequences were the same, this error would be 0. The values of the error on
the 20th day, 25th day, and 30th day are shown in Table 7.2. As expected, the
average error decreases as we proceed into the outbreak. Notice, however, that in
the case of jurisdiction C, the error is higher on the 30th day than on the 25 day.
Looking at Figure 7.7, we see that the epidemic curve estimate on the 30th day
looks much like the gold standard, but is shifted to the right. Given the way we
computed the error, this accounts for the large error. If we were only concerned
with the error in the total number of inﬂuenza cases, the error would be 1.8%. In
the same way, for jurisdiction A the 30-day estimate is shifted to the right, but
not so much as to make its error greater than that of the 25-day estimate. These
shifts may be occurring because individuals might often buy thermometers on
days following the onset of illness.
Results Using Informative Probability Distributions
Other than formulating a weak belief concerning when the outbreak reaches
its peak, in the studies just discussed we assumed uninformative probability
distributions of the size, duration, peak, and number of days since the outbreak
started. Our purpose was to investigate the quality of the estimates obtained
when we do not make additional assumptions about the nature of the outbreaks.
Next we analyze the sensitivity of our results to these probability distributions.
We repeated the study for jurisdiction A four times, each time using one of
the following informative probability distributions:
1. The duration has the beta(D; 10, 10) density function adjusted to the interval
[1, 80].
2. The size has the beta(S; 10, 10) density function adjusted to the interval
[1, 11].
3. The peak has the beta(P; 12, 8) density function adjusted to the interval
[1, D/2].
4. The number of days since the outbreak started has the beta(O; 10, 10) density
function adjusted to the interval [1, D/2].
Our purpose was to model what we consider reasonable prior beliefs about the
domain. For example, we believed that the duration was more likely to be in the
mid-range than at either end point; however, we did not have a strong belief as

182
X. Jiang, M.M. Wagner, and G.F. Cooper
(a) Duration has an informative probability distribution.
(b) Size has an informative probability distribution.
(c) Peak has an informative probability distribution.
(d) # days has an informative probability distribution.
Week
New Flu Cases
9
8
7
6
5
4
3
2
1
90000
80000
70000
60000
50000
40000
30000
20000
10000
0
Variable
LA 25 Day Est.
LA 30 Day Est.
LA Weekly Flu Co unt
LA 20 Day Est.
Week
New Flu Cases
9
8
7
6
5
4
3
2
1
100000
80000
60000
40000
20000
0
Variable
LA 25 Day Est.
LA 30 Day Est.
LA Weekly Flu Count
LA 20 Day Est.
Week
New Flu Cases
9
8
7
6
5
4
3
2
1
100000
80000
60000
40000
20000
0
Variable
LA 25 Day Est.
LA 30 Day Est.
LA Weekly Flu Co unt
LA 20 Day Est.
Week
New Flu Cases
9
8
7
6
5
4
3
2
1
100000
80000
60000
40000
20000
0
Variable
LA 25 Day Est.
LA 30 Day Est.
LA Weekly Flu Count
LA 20 Day Est.
Fig. 7.9. Epidemic curve estimates for the inﬂuenza outbreak in jurisdiction A using
informative probability distributions
to where it was actually located. So we used the beta(D; 10, 10) density function.
Similarly, we believed that the size was more likely to be in the mid-range. As
to the peak, we simply made our belief stronger that it was located about 2/3
of the way into the ﬁrst half of the outbreak (Recall that previously we used the
beta(P; 3, 2) density function). As far as the number of days since the outbreak
started, we assumed that we were investigating the outbreak during the ﬁrst
half of the outbreak. We also assumed that we were more likely to be in the
mid-range of the ﬁrst half of the outbreak. However, we obtained similar results
when we assumed a uniform distribution over the ﬁrst half. Note that, of the four
informative probability distributions, the only ones that favored the outbreaks
we were investigating were the ones concerning size and the peak. That is, the
size and the peak of the four outbreaks were all at about the points that we
considered most probable. On the other hand, the durations of the outbreaks
were 66-68 days. which is not near the midpoint. Furthermore, the assumption
concerning the number of days since the outbreak started has little to do with
a particular outbreak.
Figure 7.9 shows the results. Interestingly, the informative probability dis-
tributions that improved the accuracy most were the ones that did not favor
the outbreaks we were investigating. That is, using an informative probability

7 Modeling the Temporal Trend of the Daily Severity
183
(b) All variables have informative probability distributions.
(a) No variable has an informative probability distribution.
Week
New Flu Cases
9
8
7
6
5
4
3
2
1
100000
80000
60000
40000
20000
0
Variable
LA 25 Day Est.
LA 30 Day Est.
LA Weekly Flu Count
LA 20 Day Est.
Week
New Flu Cases
9
8
7
6
5
4
3
2
1
90000
80000
70000
60000
50000
40000
30000
20000
10000
0
Variable
LA 25 Day Est.
LA 30 Day Est.
LA Weekly Flu Count
LA 20 Day Est.
Fig. 7.10. Epidemic curve estimates for the inﬂuenza outbreak in jurisdiction A using
uninformative probability distributions are in (a), while estimates using informative
probability distributions of all four variables are in (b)

184
X. Jiang, M.M. Wagner, and G.F. Cooper
Table 7.3. Errors obtained using informative probability distributions are on the left,
while those obtained using uninformative probability distributions are on the right
Jurisdiction
20 Day
Error
25 Day
Error
30 Day
Error
A
50.3%
69.1% 30.1%
32.9% 17.2%
27.6%
B
50.4%
59.6% 27.3%
26.4% 11.9%
11.8%
C
41.6%
63.2% 18.6%
20.9% 22.3%
37.8%
D
57.3%
66.9% 33.1%
32.7% 15.8%
15.1%
Average
49.9%
64.7% 27.3%
28.2% 16.8%
23.1%
distribution for the duration improved the estimates at both the 25th and 30th
days the most, and using an informative probability distribution for the number
of days since the outbreak started improved the estimate at the 20th day the
most.
We repeated the study for jurisdiction A with all four variables having the in-
formative probability distributions listed above. Figure 7.10 (b) shows the results,
while Figure 7.10 (a) shows the estimates when we do not use any of the informa-
tive distributions (These are the same estimates as those shown in Figure 7.5).
Notice that the estimates in Figure 7.10 (b) combine the improvements that we
see in Figures 7.9 (a) and (d).
Finally, we repeated the study for the other three jurisdictions with all four
variables having informative probability distributions. Table 7.3 compares the
errors obtained using informative and uninformative probability distributions.
On the average, we obtain signiﬁcantly better estimates using informative prob-
ability distributions on the 20th and 30th day of the outbreak, but not on the
25th day. It seems prior information would be more useful when we are earlier
in the outbreak and do not have as much information concerning the speciﬁc
outbreak. So it may simply be an anomaly that we obtain greater improvement
on the 30th day than on the 25th day. Regardless, in general prior information
does seem to improve our results.
7.4
Discussion
Our study indicates that a system, which uses uninformative probability distri-
butions of size, duration, peak, and number of days since the outbreak started,
may be capable of doing a good job of estimating an inﬂuenza epidemic curve
as early as the 25th day of 66-68 day outbreaks, but it cannot closely estimate
the curve on the 20th day. We repeated the study using informative probability
distributions of these variables and obtained signiﬁcantly better results, with
even the 20-day estimate having an average error under 50%. These results in-
dicate that modeling our probability distributions of these variables accurately
can have a major impact on a system that estimates epidemic curves. Additional
investigation is needed to determine how to best model these distributions. For
the past several years, data on observable events related to inﬂuenza outbreaks

7 Modeling the Temporal Trend of the Daily Severity
185
have been collected by the RODS Laboratory at the University of Pittsburgh.
Once more data is collected, we can learn better as to how to represent our prior
beliefs.
The system we evaluated assumed that everyone who becomes sick on a given
day buys thermometers on that day (if they buy at all). In reality, there may be
some lag in the number of days in which a sick individual buys a thermometer.
A possible improvement would be to investigate what this lag is, and incorpo-
rate it into the system. Furthermore, the system could be improved by using
a multivariate times series (several observable events) rather than only a uni-
variate time series (a single observable event). Our model can readily handle
multivariate times series. We merely need to include a set of count nodes for
each observable event. By incorporating all these modiﬁcations, we may be able
to obtain better estimates earlier.
Acknowledgements. This research was supported by the National Science
Foundation Grant No. 0325581.
References
1. Cooper, G.F., Dash, D.H., Levander, J.D., Wong, W.K., Hogan, W.R., Wagner,
M.M.: Bayesian Biosurveillance of Disease Outbreaks. In: Proceedings of the 20th
Conference on Uncertainty in Artiﬁcial Intelligence, BanﬀPark Lodge, Banﬀ,
Canada, July 7-11 (2004)
2. Hogan, W.R., Cooper, G.F., Wallstrom, G.L., Wagner, M.M., Dipinay, J.M.: The
Bayesian aerosol release detector: An algorithm for detecting and characterizing out-
breaks caused by an atmospheric release of Bacillus anthracis. Statistics in Medicine
(October 22, 2007)
3. Jiang, X., Wagner, M.M., Wallstrom, G.L.: A Bayesian Network for Outbreak De-
tection and Prediction. In: Proceedings of AAAI 2006, Boston, MA (2006)
4. Last, J.M.: A Dictionary of Epidemiology. Oxford University Press, New York (2000)
5. Le Strat, Y., Carrat, F.: Monitoring Epidemiological Surveillance Data using Hidden
Markov Models. Statistics in Medicine 18 (1999)
6. Neapolitan, R.E.: Learning Bayesian Networks. Prentice Hall, Upper Saddle River
(2004)
7. Wagner, M.M., Gresham, L.S., Dato, V.: Case Detection, Outbreak Detection, and
Outbreak Characterization. In: Wagner, M.M. (ed.) Handbook of Biosurveillance.
Elsevier, NY (2006)

D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 187–217, 2008. 
springerlink.com                                                                © Springer-Verlag Berlin Heidelberg 2008 
8 
An Information-Geometric Approach to Learning 
Bayesian Network Topologies from Data 
Eitel J.M. Lauría 
School of Computer Science and Mathematics 
Marist College 
3399 North Road 
Poughkeepsie, NY 12601, USA 
Eitel.Lauria@marist.edu 
Abstract. This work provides a general overview of structure learning of Bayesian networks 
(BNs), and goes on to explore the feasibility of applying an information-geometric approach to 
the task of learning the topology of a BN from data. An information-geometric scoring function 
based on the Minimum Description Length Principle is described. The info-geometric score 
takes into account the effects of complexity due to both the number of parameters in the BN, 
and the geometry of the statistical manifold on which the parametric family of probability dis-
tributions of the BN is mapped. The paper provides an introduction to information geometry, 
and lays out a theoretical framework supported by empirical evidence that shows that this  
info-geometric scoring function is at least as efficient as applying BIC (Bayesian information 
criterion); and that, for certain BN topologies, it can drastically increase the accuracy in the  
selection of the best possible BN. 
8.1   Introduction 
A Bayesian network (BN) with topology G (represented by a directed acyclic graph, 
or DAG) and vector parameter 
1
2
{ ,
,..,
}
n
=
θ
θ θ
θ
, encodes the joint probability dis-
tribution of a set of random variables
{
}
1
2
n
X ,X ,...,X
=
X
. Recalling the product 
probability rule, the joint probability distribution of 
1
2
n
X ,X ,...,X  can be written as: 
( )
1
1
p( | , )
p(
|
,
, )
( )
n
n
i
pa i
i
ik
i
i
x
x
j
θ
=
=
=
=
∏
∏
x θ
θ
G
G
 
(8.1) 
The expression p( | , )
x θ G  denotes the probability of a conjunction of particular as-
signments 
1
2
,
,...,
n
x x
x  to the set of variables
i
X ∈X , and 
pa( )i
x
 identifies a given 
configuration of the list of direct parents of 
i
X ,  linked to 
i
X  through the arcs in 
DAG G . For a discrete BN, the conditional probability tables at each node 
i
X ∈X  
are represented by the parameters 
[
]
{
}
1
2
1
1
{ ,
,..,
}
( )
i
i
q
r
n
ik
k
j
j
θ
=
=
=
=
θ
θ θ
θ
 , where i=1..n 

188 
E.J.M. Lauría 
identifies each variable 
i
X ∈X ; 
1.. i
k
r
=
 identifies each of the 
ir   states of  variable 
i
X ∈X  ; j=1..qi identifies the set of 
iq  valid configurations  of values of the parent 
variables of
i
X ∈X  (
( )
pa i
x
). 
8.2   Learning the Topology of Bayesian Networks from Data 
The task of learning a BN’s topology from data can be seen as a model selection 
problem. Standard scientific practice, summarized by the inductive bias known as Oc-
cam’s razor, prescribes that it is usually preferable to choose the least complex net-
work that equally fits the data. Given a training data set D of N cases 
(1)
(2)
(
)
{
,
,..,
}
N
D = x
x
x
 where each 
( )
k
x
 is a row vector representing a conjunction of 
observed states 
( )
( )
( )
1
2
,
,...,
k
k
k
n
x
x
x
 of the vector variable
{
}
1
2
n
X ,X ,...,X
=
X
, the goal 
is to find a BN <G ;θ > with topology G   and vector parameter θ  that best describe 
the joint probability distribution over the training data D. The literature portrays two 
general approaches applied to the task of learning the topology of a Bayesian network 
from data: constraint-based methods and score-based methods.  In this work we focus 
on score-based methods, as they are among the most common and powerful methods 
currently in use. 
8.2.1   Score-Based Methods 
These methods pose learning structure as an optimization problem, where the goal is 
to find the BN with DAG G  in the space of network topologies that best matches the 
training data set D. In general, a search algorithm uses a scoring metric to evaluate 
each BN with respect to D, looking for the BN topology that optimizes this score. The 
optimal BN with DAG 
*
G  is then used as the model for the topology of the domain1. 
An alternative outcome is a sample of the models found, which approximates a 
Bayesian posterior probability.  
Two elements need to be specified when considering this approach: the search pro-
cedure and the scoring metric.  
Clearly, exhaustive search procedures are not feasible for most problems, as it is 
not possible to enumerate all the possible BNs for even a small number of nodes. The 
number of DAGs is super-exponential in the number of nodes: for just 10 variables 
there are 
18
4.2 10
⋅
 possible networks. Hence, exhaustive search has to be replaced by 
heuristic search techniques. 
A local search algorithm that changes one single arc at each step can efficiently as-
sess the gains made by adding, removing, or reversing the arc. A greedy hill-climbing 
algorithm, for example, performs at each step the local change that maximizes the 
gain, until it reaches a local maximum. The algorithm takes a starting network 
(empty, full, chosen at random, or elicited from previous knowledge) and iteratively 
applies single-arc addition, removal, or reversal, keeping the resulting network that 
                                                           
1 Note that the topology of a BN can only be learnt up to its Markov equivalent class. 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
189 
maximizes the score as the most current candidate.  According to [1], although there 
is no guarantee that the procedure will find a global maximum, it does perform well in 
practice. 
Learning the topology of a BN using greedy hill-climbing, simulated annealing and 
other variants is analyzed in [2]. Cooper and Herskovits’ K2 algorithm [3] chooses an 
order over the nodes of the BN. Although the ordering reduces considerably the total 
number of topologies, the number is still too big to allow for an exhaustive search. K2 
uses a greedy heuristic algorithm that, for each node 
i
X   in the ordering, successively 
adds as a parent of 
i
X  a variable from the list (
)
1
2
1
,
,..,
i
X
X
X −
 that contributes the 
maximum increase in the score (i.e. the highest increase in probability of the resulting 
topology). This procedure is repeated independently for each node 
i
X  until no node 
increases the score, or the number of parents of 
i
X  exceeds a given threshold. 
An alternative solution that performs in a quite efficient manner is to make use of 
simulated annealing. Simulated annealing is a Markov Chain Monte Carlo approach 
to optimization of multivariate functions. The term derives from the physical process 
of heating and slow cooling of a solid material (i.e. metal) in order to increase its 
toughness and reduce brittleness. During annealing, a melted solid in a heat bath, ini-
tially hot and therefore disordered, is slowly cooled so that the system remains in 
thermodynamic equilibrium at any given time, gradually moving towards an equilib-
rium state of minimal energy. In terms of statistical physics, the Boltzman distribution 
describes the probability distribution of a state G  with energy E(G ) for a system in a 
heat bath at temperature T:  
( )
( )
exp
E
T
π
⎛
⎞
∝
−
⎜
⎟
⎝
⎠
G
G
 
(8.2) 
The original Metropolis algorithm [4] simulates the evolution of a thermodynamic 
system from one configuration to another. Given an initial state 
0
G  of the system at 
energy 
(
)
E
0
G
 and temperature T, the initial configuration is perturbed (
*
→
0
G
G ) 
keeping T constant, and the change in energy 
E
Δ
= 
*
(
)
E G
-
(
)
E
0
G
is calculated.  If 
0
E
Δ
<
, the new configuration 
*
G  with lower energy is accepted. Otherwise,
*
G is 
accepted with probability given by the Boltzman distribution 
(
)
exp
E T
−Δ
.  
The cooling stage of the annealing process is simulated by the Metropolis algo-
rithm taking a sequence of slowly decreasing temperatures converging to 0. The  
Metropolis algorithm is run with each value of a sequence of decreasing tempera-
tures
0
1
,
,..,
N
T T
T , resulting in a sequence of annealing states 
0
1
,
,..,
N
G G
G  with  
decreasing energies
0
1
(
),
(
),..,
(
)
N
E
E
E
G
G
G
. In the limit, when the temperature ap-
proaches 0, the system evolves towards an equilibrium state with minimum energy. 
This can be seen as a combinatorial optimization process where a sequence of feasible 
solutions gradually approach an optimal solution (global minimum) using the energy 
equation for the thermodynamic system as the objective function. It has been shown 
[5] that simulated annealing converges asymptotically to the optimal solution. The 

190 
E.J.M. Lauría 
temperature T and number of steps of each Metropolis run at each state 
iG  control the 
optimization process.  
The algorithm, as applied to BNs, is described in Figure 8.1.  Its starts with an ini-
tial topology 
0
G  and is controlled by five parameters: 
0
T , the initial temperature; α , 
the number of iterations within the Metropolis subroutine; β , the number of effective 
changes of structure within Metropolis; γ , the temperature decay factor ( 0
1
γ
<
<
); 
and δ , the number of temperature decrements in the simulated annealing procedure 
(Figure 8.1). The energy function E(G ) is given by the scoring function , and the per-
turbation on the BN is any of three randomly eligible operations: arc addition, arc de-
letion or arc reversal.  
 
 
(
)
[
]
[ ]
←
←
←
←
←
←
0
0
0
0
Procedure Sim_Anneal(G ,T,α,β,γ,δ)
i
0
T
T
G
 G
while i < δ  
  G,k
Metropolis(G,T,α)
  if (k = 0)then exit
  else
     i
i + 1
    T
γ × T
  endif
endwhile
return G
 
 
(
)
(
)
[
]
←
←
←
←
←
←
←
new
new
-ΔE/T
new
Function Metropolis(G,T,α)
 j
0
 k
0
 while j < α  and k < β  
    j
j+ 1
  G
Perturb(G)
  ΔE
E(G
)- E(G)
  if (ΔE < 0) or (e
> Unif(0,1)) then
    G
G
     j
j+ 1
  endif
 endwhile
return G,k
 
Fig. 8.1. Simulated Annealing Algorithm 
Approximate model averaging can be useful when the amount of sample data is not 
large. When the number of possible topologies is large, it is usually not possible to 
average over all topologies. For such cases a search can be made that identifies highly 
probable topologies and then an average can be computed over them.  Madigan and 
York [6] proposed an approach based on Markov Chain Monte Carlo (MCMC) meth-
ods.  Friedman and Koller [7] have suggested using MCMC, but over orderings rather 
than over network topologies, adducing that the space of orderings is much smaller 
and more regular than the space of network topologies.  

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
191 
8.2.2   Incomplete Data 
If the BN’s topology is not known and not all of the variables are observable in the 
training data set (hidden variables or missing data), the problem is particularly chal-
lenging. Friedman [8] introduced the Structural EM algorithm, combining the Expec-
tation Maximization (EM) algorithm [9], used to estimate network parameters with 
incomplete data, with model searching using penalized scoring functions. The algo-
rithm is quite expensive, given that at each step the EM algorithm has to be executed 
in order to compute the maximum likelihood estimator, which is needed to compute 
the score of each network structure. 
8.2.3   Network Scoring Metrics 
Search-and-score methods applied to the task of learning structure from data require 
the definition of a scoring metric to assess at each step of the search process how well 
the current model approximates the data.  The Bayesian approach gives a clear-cut 
answer to the problem of model selection. A scoring function can be associated 
with p(
|
)
D
G
 
, the posterior probability of the model given the data, where G  is a to-
pology in the space of network topologies 
G
S . 
Score( ,
)
p(
|
),
D
D
∝
∈
G
G
G
 
G
S
 
(8.3)
It is usually convenient to apply logarithms in order to simplify computations by 
replacing products by sums. The score can be therefore expressed in terms of the log 
posterior probability of the topology G  given the data set D.  In other words,   
[
]
*
 
Best 
 
argmax  log p(
|
)
D
∈
≡
G
G
 
G
G
 
S
 
(8.4)
It should be stressed though that a fundamental feature of a score function is its  
decomposability. This means that the score function can be rewritten as a sum of con-
tributions associated with each node 
i
X  in the BN so that each contributing term de-
pends only on the value 
ix  and the configurations of its parents 
( )
pa i
x
, given the data 
D and a DAG G in the space of topologies 
G
S . 
( )
1
Score( ,
)
LocalScoreTerm( ,
; ,
)
n
i
pa i
i
D
x x
D
=
=∑
G
G

 
(8.5)
Cooper and Herskovits [5] make use of a Bayesian scoring metric and describe a 
method for computing the probability of BN topologies given a data set. The method 
considers a set of n discrete variables 
1
2
(
,
,..,
)
n
X
X
X
=
X
of a BN with  DAG G , 
where each node 
i
X ∈X  in the network  has ri possible states,  and has a set of par-
ents 
( )
pa i
X
 for which  there are 
iq  valid configurations  of values 
( )
pa i
x
. The joint 
probability  p( , )
D
G
 is taken over θ , the vector parameter given by the conditional 

192 
E.J.M. Lauría 
probabilities 
( )
p(
|
, )
i
pa i
x
x
G   in the BN < θ ,G >.  Recall that, according to  Bayes’ 
theorem, p(
|
)
p(
|
) p( )
D
D
∝
⋅
G
 
G
G , where 
p(
|
)
p(
| , ) p( |
) 
D
D
d
=
⋅
∫Θ
θ
θ
θ
G
G
G
 
(8.6) 
is the marginal likelihood  of the BN G. Assuming that G  is uniformly distributed in 
the space of topologies 
G
S , so that p( )
1
=
G
G
S
, then in order to select the best 
network (the one with highest p(
|
)
D
G
 
), it is sufficient to find the maximum mar-
ginal likelihood p(
|
)
D G , given by expression (8.6). 
 
Consider the following assumptions:  
(i) 
The variables in the BN are discrete, constituting a multinomial BN 
(ii) 
Records in the data set occur independently, meaning that 
( )
1
p(
| , )
p(
|
, )
N
i
pa i
i
D
x
x
=
=∏
θ G
G ; 
(iii) 
 There are no missing values in the data set. 
(iv) 
The distribution of the parameters follows a Dirichlet distribution (con-
jugate prior for a multinomial). We use the notation: 
1
1
( )
(
,
,...,
)
i
i
ij
ij
ijr
j
Di α
α
α
θ
∼
 
(8.7) 
to identify each vector of parameters 
( )
i j
θ
 following a Dirichlet distri-
bution with hyperparameters
1
1
,
,...,
i
ij
ij
ijr
α
α
α
. 
Given this set of assumptions, Cooper and Herskovits derive the marginal likelihood 
p(
|
)
D G  in closed form as: 
1
1
1
(
)
(
( ))
(
|
)
(
( ))
(
)
i
i
q
r
n
ij
ijk
ik
i
j
k
ij
i
ijk
N
j
p D
N
j
α
α
α
α
=
=
=
Γ
Γ
+
=
Γ
+
Γ
∏∏
∏
G
 
(8.8) 
where 
( )
Γ ⋅ is the Gamma function2, 
( )
ik
N
j  is number of cases in D in which 
i
X ∈X  has the value 
ix
k
≡
 for a given configuration of its parents 
( )
pa i
x
j
≡
, 
and
1
( )
( )
ir
i
ik
k
N
j
N
j
=
=∑
. 
This approach of using 
(
|
)
p D G  as a scoring function is useful for searching 
among a reduced number of network topologies, but when the space of topologies 
G
S  
 
                                                           
2 The Gamma Function is defined as 
( )
1
0
x
x
e dx
α
α
∞
−
−
Γ
= ∫
, where the expressions 
( )
(
)
( )
1
1; 
1
α
α
α
Γ
=
Γ
+
=
⋅Γ
 apply for all α  positive integers. 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
193 
is large, searching 
G
S exhaustively to find the DAG which maximizes the Bayesian 
scoring criterion becomes computationally impractical. For such reason, Cooper and 
Herskovits’ K2 heuristic algorithm restricts the search by forcing an ordering of the 
nodes (see section 8.2.1). 
8.2.3.1   Laplace’s Approximation and Bayesian Information Criterion 
Recalling the model selection criterion of choosing the network that renders the high-
est posterior probability p(
|
)
p(
|
) p( )
D
D
∝
⋅
G
 
G
G, the Gaussian (also called 
Laplace’s) approximation is an efficient estimate of the marginal likelihood 
p(
|
)
D
G
 
. Laplace’s approximation takes into account that: a) p(
|
)
D
G
 
 is averaged 
over the BN’s vector parameter θ  such that p(
|
)
p(
| , ) p( |
) 
D
D
d
=
⋅
∫Θ
θ
θ
θ
 G
G
G
; 
b) the product p(
| , ) p( |
)
D
⋅
θ
θ
G
G  approximates a multivariate Gaussian distribu-
tion. This can be shown by expanding 
[
]
( )
log p(
| , ) p( |
)
D
ϕ
=
⋅
θ
θ
θ
G
G
 in Taylor 
series around the maximum posterior (MAP) estimator θ  (note that the first order 
term vanishes when expanding around θ ): 
(
)
] (
)
t
1
( )
( )
( )
   
2
ϕ
≅ϕ
−
−
∇∇ϕ
−
θ
θ
θ
θ
θ
θ
θ
θ




 
(8.9) 
The expression 
ˆ
(
)t
−
θ
θ
 is the transpose of 
ˆ
(
)
−
θ
θ ,  and 
]
( )
∇∇ϕ
θ
θ
  is the Hes-
sian matrix of second derivatives, whose (ij)th element  is 
2 ( )
i
j
θ θ
∂ϕ
∂∂
θ , evaluated at 
=
θ
θ . Taking the exponential of ( )
ϕ θ : 
(
)
(
)
t
( )
1
e
p(
| , ) p( |
)exp
2
D
ϕ
⎧
⎫
≅
⋅
−
−
−
⎨
⎬
⎩
⎭
θ
θ
θ
θ
θ
A θ
θ




G
G
 
(8.10)
Note that 
(
)
(
)
t
1
exp
2
⎧
⎫
−
−
−
⎨
⎬
⎩
⎭
θ
θ
A θ
θ


  is proportional to a multivariate Gaussian 
density with  
=
μ
θ  and   covariance matrix 
=
Σ
A , where A  is equal to minus  
the inverse of the Hessian matrix evaluated at 
=
θ
θ . Therefore, replacing and  
integrating: 
(
)
(
)
(
)
(
)
t
| |
2
1
p(
|
)
p(
| , ) p( |
)
exp
 
2
2
            
p(
| , ) p( |
)
det
D
D
d
D
π
⎧
⎫
≅
⋅
⋅
−
−
−
⎨
⎬
⎩
⎭
≅
⋅
⋅
∫Θ
θ
θ
θ
θ
θ
A θ
θ
θ
θ
θ
A






 G
G
G
G
G
(8.11)
where |
|
θ  is the number of dimensions of vector parameter θ . 

194 
E.J.M. Lauría 
Taking logarithms we obtain the so called Laplace’s approximation of the mar-
ginal likelihood: 
(
)
(
)
|
|
1
log p(
|
)
log p(
| , )
p( |
)
log 2
log det
2
2
D
D
π
≅
+
+
−
θ
θ
θ
A


G
G
G
 
(8.12)
As shown by [10], Laplace’s approximation is quite accurate for many problems 
with moderate sizes of data, exhibiting a relative error of order 
(1/
)
O
N  A different 
version of expression (8.12) replaces A by 
ˆ ( )
θI θ , the Fisher’s information matrix 
evaluated at the  maximum likelihood estimator ˆθ .  
ˆ ( )
θI θ  is easier to calculate. As 
reported by [11], this approximation yields a larger relative error of order 
(
)
1/
O
N
. 
The Bayesian Information Criterion (BIC), first described by Schwartz [12], uses 
the fact that, as the size of the sample increases, the prior information is negligible 
relative to the likelihood of the data. Therefore, as N increases, it is possible to discard 
the use of parameter priors in expression  (8.12).  BIC’s formulation results in: 
(
)
|
|
ˆ
log p(
|
)
log p(
| , )
log
2
D
D
N
≅
−θ
θ
G
G
 
(8.13)
As described by [13], BIC’s formula has several interesting features: a) it elimi-
nates the dependence of a prior choice; b) it is easier to calculate; c) it is intuitively at-
tractive, as its expression characterizes the likelihood of the data penalized by the 
BN’s complexity (given by the dimensionality of vector parameter θ ). 
8.2.3.2   Stochastic Complexity and the Minimum Description Length Principle 
Occam’s razor, the inductive bias of science applied to model selection, can be sum-
marized as “choose the shortest hypothesis that fits the data” [14, pp 65]. The  
compromise between simplicity and complexity of a model is the trade-off between 
goodness of fit and generalization:  if the model is too simple (description length re-
quired to explain data is too short) it may fail to encode the data accurately; if instead, 
the model is too complex (description length too long) it may be excessively tailored 
around the features of the sample data, failing to generalize for new observations, and 
therefore limiting its predictive power on future data. This criterion is easily related to 
the stochastic complexity of a model introduced by Rissanen [15] and the information 
theoretical principle generally referred to as Minimum Description Length Principle 
(MDL). Applying Bayes theorem to expression (8.4), taking logarithms and changing 
signs, we get: 
 
[
]
*
Best 
 
argmin
log p(
| )
logp( )
D
∈
=
−
−
G
G
G

G
G
S
                  (8.14) 
Expression (8.14) can be analyzed from the perspective of information theory. If 
we think of a model in terms of a code capable of encoding a message (i.e. the data), a 
better model can be defined as the one that does a better job at eliminating the redun-
dancies of the data, attaining a shorter description of the message (note that we use 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
195 
“message” in a generic sense to mean the data to be compressed). Shannon’s noiseless 
coding theorem [16] showed that for any code encoding data 
{
}
(1)
(2)
(
)
,
,..,
N
D = x
x
x
  
for which the Kraft inequality holds, the optimal code3 assigns a length of 
( )
2
log p(
)
i
−
x
 to encode message
( )i
x
4 5 . This clearly establishes a one-to-one rela-
tionship between the probability of a message and the description length of its optimal 
code: the expression implies that simpler hypotheses (i.e. short description lengths) 
correspond to large probability values and vice versa.  
It follows that expression (8.14) can be interpreted in the following way: Select the 
BN with topology 
*
G   which minimizes the sum of the length of the description of 
the BN and the length of the description of the data when encoded by the BN [17].  
Rewriting (8.14): 
*
|
Best 
argmin
D
L
L
∈
⎡
⎤
=
+
⎣
⎦
G
G
G
G
G
S
 
(8.15)
LG  is the description length of the BN with DAG G  under optimal encoding, and 
|
D
L
G is  the description length of the data 
{
}
(1)
(2)
(
)
,
,..,
N
D = x
x
x
 given the BN with 
topology G . This shortest description length is what Rissanen [15] called stochastic 
complexity, rooted in the work on algorithmic complexity by Solomonoff [18],  
Kolmogorov [19], and Chaitin [20], that Rissanen applied  to those descriptions repre-
senting probability models, therefore equating the description of the data by a statisti-
cal model to the encoding of a message. Thus the purpose of model selection is to find 
models that can encode the data into short descriptions, and therefore compress the 
data. 
8.2.3.3   Two-Part Coding and Minimum Description Length 
The two-part coding scheme addresses the problem of encoding data with a chosen 
probability model so that the data strings can be transmitted with the shortest possible 
description length. To illustrate this let us consider the simple case6 of a set of obser-
vations 
{
}
(1)
(2)
(
)
,
,..,
N
D = x
x
x
 and a model 
∈
G
G
 
S  consisting of a parametric fam-
ily of distributions f( | ), 
k
⋅
∈
θ
θ
\  and maximum likelihood (ML) estimator ˆθ  for the 
                                                           
3 The one with a mean code length equal to the lower bound given by the entropy. 
4 It should be noted that Shannon’s theorem refers to log base 2, measuring the de-
scription length in bits. If natural logarithms are considered, the description length is 
measured in nats. 
5 In general, we can think of 
2
log p( )
−
x  as the description length for ideal codes re-
lated to a probability density function.  
6 It should be stressed that one of the strengths of the Minimum Descriptive Length Principle is 
that it can be generalized to far less restrictive settings [21]. 

196 
E.J.M. Lauría 
data D. Recall that best codes are closely related to maximum likelihood estimators 
through: 
ˆ
argmax p(
| )
argmin  
log p(
| )
argmin
|
D
D
LD
=
≡
−
=
∈
∈
∈
θ
θ
θ
θ
θ
Θ
θ
Θ
θ
Θ
(8.16)
Part one of the scheme takes care of encoding the ML estimator ˆθ  whose prefix 
code of length 
ˆLθ  is built after the probability model 
ˆ
f( | )
⋅θ .  
Part two takes care of encoding the data D using the distribution 
ˆ
f( | )
⋅θ  such that 
its description length amounts to: 
ˆ
log p(
| )
ˆ|
L
D
D
= −
θ
θ
 
(8.17)
Rissanen [15, 22] showed that choosing a discretization7 of the parameter space 
with precision
1/
N
ε =
 (optimal for regular parametric families) leads to an encod-
ing of the ML estimator ˆθ  with a minimum description length
ˆ
|
|log
2
L
N
≅
θ
θ
, where 
|
|
θ  is the dimensionality of θ , and N is the data sample size. Putting both parts to-
gether we get a total minimum description length of: 
|
|
ˆ
MDL =
log p(
| )
log
2
D
N
+
−
θ
θ
 
(8.18)
We can see that expression (8.18) takes the form of a penalized likelihood, where the 
term  |
|log
2
N
θ
 is the price paid to encode the complexity of the model, besides  
encoding the data.  It should also be noted that this version of MDL is equal to minus 
the Bayesian Information Criterion (BIC), as can be verified by comparing (8.18)  
with (8.13). 
8.3   Geometric Complexity of a Bayesian Network 
To properly interpret the meaning of the inherent complexity of a BN, we need to  
resort to geometry. This section describes some basic concepts of information geome-
try, a relatively new discipline that studies statistical inference from the point of view 
of modern differential geometry. The relevant differential geometry will be summa-
rized here, together with its link to inferential statistics. 
                                                           
7 Real numbers cannot be encoded with codes of finite length. Therefore we need to resort to 
discretizing the parameter space. 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
197 
8.3.1   Differential Geometry and Statistics 
We start with some definitions taken from [23, 24, 25, 26, 27]: 
A metric is a real-valued function d(x,y) describing the distance  between two  points 
for a given set, that satisfy the following conditions:  
(i) 
d( , )
0, with equality iff  
x y
x
y
≥
=
 
(ii) 
Triangular inequality: d( , )
d( , )
d( , )
x z
z y
x y
+
≥
 
(iii) 
Symmetry: d( , )
d( , )
x y
y x
=
 
An n-dimensional manifold M  is defined as a topological, usually Hausdorff 8 space 
with the property that for each point P  in M  there exists an open neighborhood 
UP  in M  and a mapping u  which puts UP  into a one-to-one correspondence with 
a neighborhood in
n
\
9. This mapping of each point P  to an n-tuple of real numbers 
1
2
(
,
,..,
)
n
u u
u
 is called a local coordinate system
1
2
(
,
,..,
)
n
u u
u
P
 on M : 
1
2
: 
 (
,
,..,
)
n
u
u u
u
→rG
P
 
(8.19)
For any point P in M , the pair (
, )
U
u
P
 is called a coordinate  chart , and the 
collection of such pairs for which the set of neighborhoods UP  cover M  is known 
as an atlas of P  ( see Figure 8.2) . 
If (
, )
U
u
P
 and (
, )
V
u
P
 are two coordinate charts of M  so that U
V
∩
≠∅
P
P
,  
there is a one-to-one correspondence  between the local coordinate systems 
1
2
(
,
,..,
)
n
u u
u
P
 and 
1
2
(
,
,..,
)
n
u u
u
P
,  such that  
1
2
(
,
,..,
), 
1..
i
i
n
u
u u u
u
i
n
=
=
 
with inverse transformation given by 
1
2
(
,
,..,
), 
1..
i
i
n
u
u u u
u
i
n
=
=
 
For 
a 
differentiable 
manifold 
we 
assume 
that 
the 
transformations 
1
2
(
,
,..,
)
i
i
n
u
u u u
u
=
 and their inverses 
1
2
(
,
,..,
)
i
i
n
u
u u u
u
=
 have continuous  
derivatives 
i
j
u
u
∂
∂
 and 
i
j
u
u
∂
∂
 ,  and Jacobians  satisfying det
0
i
j
u
u
⎛
⎞
∂
≠
⎜
⎟
∂
⎝
⎠
  and  
det
0
i
j
u
u
⎛
⎞
∂
≠
⎜
⎟
∂
⎝
⎠
, ,
1..
i j
n
=
. 
A differentiable curve (or trajectory) C  in an n-dimensional manifold M is a con-
tinuous mapping 
( )t
C
 of an interval a
b  in 
t
< <
\  into M . In terms of the local 
coordinates of a coordinate chart (
, )
U
u
P
, the curve can be expressed as 
1
2
(
( ),
( ),..,
( ))
n
u t u
t
u
t
rG
. 
                                                           
8 A topological space Ω  is Hausdorff if for all ,x y ∈Ω , with x
y
≠
, there exist open 
neighborhoods 
,
x
y
U
U  in Ω  such that 
x
y
U
U
∩
= ∅. 
9 It is useful to picture a manifold as an extension of a surface embedded in high  
dimensional spaces. 

198 
E.J.M. Lauría 
 
Fig. 8.2. Smooth Manifold 
The tangent or velocity vector of a curve C
1
2
(
( ),
( ),..,
( ))
n
u t u
t
u
t
≡rG

 at time t 
is10: 
,   
1..
i
i
i
i
d
du
du
i
n
dt
u
dt
dt
∂
=
=
⋅
=
⋅
=
∂
r
r
v
r
G
G
G
G


 
(8.20)
where 
i
iu
∂
= ∂
r
r
G
G
 is a tangent vector of a curve 
( ,.. ( ),..,
)
i
i
i
n
u
u t
u
C
where only the 
value of the coordinate 
( )
iu t  changes, while the rest remain constant. The n tangent 
vectors 
irG form a basis for a vector space 
(
)
TP M  known as the tangent space of a 
differentiable manifold M  at a point P . The tangent space is then the vector space 
whose elements are velocities of curves in M  that pass through P  (see Figure 8.3) 
Under a reparameterization 
1
2
(
,
,..,
)
i
i
n
u
u u u
u
=
, the tangent vectors 
irG  follow 
the transformation rule: 
i
i
j
i
j
i
j
j
u
u
u
u
u
u
∂
∂
∂
∂
=
=
=
∂
∂
∂
∂
r
r
r
r
G
G
G
G
 
(8.21)
                                                           
10 For notational convenience, in expression (8.20) and the sequel in this section, we use Ein-
stein’s summation convention by which, if an index occurs twice in a term, one as a sub-
script and one as a superscript summation over that index is thereby assumed. 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
199 
 
Fig. 8.3. Tangent space (adapted from [27]) 
The element of arc ds  of a curve C  in an n-dimensional manifold M  can be ex-
pressed as 
|
|
d
ds
dt
dt
dt
=
⋅
=
⋅
r
v
G
G
 such that the length of an arc of C  is equal  to 
| ( ) | 
b
a
s
t
dt
= ∫
vG
, where | ( ) |
t
vG
, the norm of 
( )t
vG
,  is the square root of the inner 
product 
( ), ( )
t
t
v
v
G
G
. 
Note that by application of (8.20), the element of arc ds  satisfies: 
(
)
2
2
,
(
)
                                             
g
  ,
1..
i
i
j
i
i
j
i
j
ij
ds
d
d
du
du du
du du
i j
n
=
=
⋅
=
⋅
=
=
r
r
r
r r
G
G
G
G G
 
(8.22)
This bilinear11 form 
 = g
i
j
ijdu du
Φ
given by the inner product 
( ), ( )
t
t
v
v
G
G
 is 
known as the first fundamental form, invariant under reparameterization (shown be-
low), which induces a metric on the manifold, enabling us to measure lengths, angles 
and volumes on the manifold. 
The 
resulting 
symmetric, 
positive 
definite 
matrix12 
of 
n2 
quantities 
1
2
g (
,
,..,
)
n
ij u u
u
 forms a geometrical object called the metric tensor. Under a pa-
rameter transformation 
1
2
(
,
,..,
)
i
i
n
u
u u u
u
=
, we obtain: 
                                                           
11 Given two vectors spaces V and W, for all vectors 
,
V
W
∈
∈
v
w
G
G
, a real valued function 
(
)
ψ v, w
G G
 is called a bilinear form,  if  for every fixed vG , it is a linear form on W, and for 
every fixed , it is a linear form on V. If 
(
)
ψ v, w
G G
is a bilinear form on V  and W=V, then be-
cause 
of 
its 
linearity 
properties 
it 
can 
be 
written 
in 
the 
form  
(
)
(
)
(
)
i
j
i
j
i
j
i
j
i
j
ij
v
w
v w
g v w
ψ
ψ
ψ
=
=
=
v, w
e ,
e
e , e
G
G
G G
G G
 , where 
1
(
)
..
n
e
e
G
G
 is a basis for V. 
12  Given 2 vectors  and 
a
b
G
G
, (i)
,
,
=
a b
b a
G
G
G
G ; (ii) if 
0,
,
0
≠
>
a
a a
G
G G
. 

200 
E.J.M. Lauría 
g
g( ,
)
g(
,
)
g( , )
g
k
l
k
l
k
l
ij
i
j
k
l
k
l
kl
i
j
i
j
i
j
u
u
u
u
u
u
u
u
u
u
u
u
∂
∂
∂
∂
∂
∂
=
=
=
=
∂
∂
∂
∂
∂
∂
r r
r
r
r r
G G
G
G
 
(8.23)
From (8.21) and (8.23) it follows that the first fundamental form is invariant: 
2
g
g
     
g
g
k
l
i
j
i
j
m
n
ij
kl
i
j
m
n
k
l
i
j
i
j
kl
i
j
ij
u
u
u
u
ds
du du
du du
u
u
u
u
du du
du du
δ δ
∂
∂
∂
∂
=
=
∂
∂
∂
∂
=
=
 
(8.24)
In expression (8.24), 
k
iδ =1 if i=j , and  
0 if 
j
i
i
j
δ
=
≠
. 
A manifold M  on which there is defined a symmetric positive definite bilinear 
form  
(
)
i
j
ij
g v w
ψ
=
v,w
G G
 is called a Riemannian manifold, and 
(
)
ψ v,w
G G
 is called a 
Riemannian metric. We shall assume that ψ  is infinitely differentiable. The simplest 
example is the inner product 
(
)
α
β
α
β
ψ
=
v ,v
v ,v
G
G
G
G
  of two tangent vectors 
(
)
T
α
β ∈
v ,v
G
G
P M . 
The length of an arc of a curve C  in M  bounded by  a
t
b
< <
 is calculated by 
integrating expression (8.22) as: 
( ), ( )  dt =
g
 
    ,
1..
i
j
b
b
ij
a
a
du du
s
t
t
dt
i j
n
dt
dt
=
=
∫
∫
v
v
G
G
 
(8.25)
The distance between two points 
1
2
and 
P
P can be expressed as: 
( )
[
]
{
}
1
2
1
2
(
,
)
inf  
: : a,b
, with (a)
 and (b)
dist
s
=
→
=
=
P P
C
C
C
P
C
P
M
 
(8.26)
Consider now the measurement of angles in the manifold. For that purpose, con-
sider two curves 
 and 
α
β
C
C  intersecting at point P . Their tangent vectors
α
vG  and 
β
vG  at point P  can be expressed in terms of the tangent space basis at  P  so that  
i
i
v
α
α
=
⋅
v
rG
G
 and 
i
i
v
β
β
=
⋅
v
rG
G
, 
1..
i
n
=
. 
If we denote by γ  the angle between vectors 
α
vG  and 
β
vG , then: 
g
cos
|
| |
|
g
g
i
j
ij
k
l
p
q
kl
pq
v v
v v
v v
α
β
α
β
α
β
α
β
α
β
γ
⋅
=
=
⋅
v
v
v
v
G
G
G
G
 
(8.27)
The volume element of an n-dimensional manifold is defined by: 
1
2
det
 
..
n
d
du du
du
=
G
V
 
(8.28)

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
201 
 
Fig. 8.4. Volume element of the manifold 
where 
(g )
ij
=
G
 is the metric tensor. This can be geometrically interpreted by viewing 
the manifold 
1
2
 
 (
,
,..,
)
n
u u
u
→rG
P
M, 
as an n-dimensional surface in a space of  
dimension N
n
>
 13. Taking for example the simple case of a two-dimensional Rieman-
nian manifold embedded in
N
\
, expression (8.28) can be seen as the area of an ‘infini-
tesimal parallelogram’ whose sides are vectors 
1
1 du
⋅
rG
  and 
2
2 du
⋅
rG
 (see Figure 8.4). 
This quantity can be calculated as the cross product: 
[
]
1
2
1
2
1
2
1
2
2
1
2
1
1
2
2
1
2
1
2
det (
)
(
)
det
    
(
)(
)
(
)
    
det
d
du
du
du du
du du
du du
⎡
⎤
=
⋅
×
⋅
=
×
⎣
⎦
=
⋅
⋅
−
⋅
=
r
r
r
r
r r
r
r
r r
G
G
G
G
G
G G
G
G
G G
V
 
(8.29) 
From this, the result follows14. 
Let us now try to make the link between differential geometry and statistics. For 
that purpose we will consider a family of probability distributions 
{
}
p( | )
x θ
M =
 of 
                                                           
13 This poses the question of whether every Riemannian metric can be realized in terms of an n-
dimensional manifold embedded isometrically in an Euclidean space 
N
\
. The initial proofs 
by Cartan [28] and Janet [29], showed that every sufficiently differentiable n-dimensional 
Riemannian manifold can be locally embedded in
N
\
, with 
(1 2)
(
1)
N
n
n
=
⋅
⋅
+
, having 
the nice derived  feature that a Riemannian manifold of n=2 can be locally embedded as a 
surface in 
3
\ . Nash’s Imbedding Theorem [30] proved that every compact smooth Rie-
mannian manifold can be globally embedded in 
N
\
. 
14 This simplified analysis, convenient to derive an intuitive explanation of basic differential 
geometry concepts, only makes sense for a space of dimension 3 or less. 

202 
E.J.M. Lauría 
a statistical model, indexed by a parameter 
n
∈
θ
\ , and the following regularity con-
ditions [26, 27]: 
- 
Given the log likelihood ( )
logp( | )
x
=
θ
θ
A
, for every fixed θ ,  the n partial de-
rivatives 
( ),  
1..
i
i
n
∂
=
θ
A
 are linearly independent  ( we use  
i
i
θ
∂
∂≡∂
). 
- 
The moments of 
( ),  
1..
i
i
n
∂
=
θ
A
 exist up to the necessary orders. 
- 
The expression 
f ( , ) 
 f( , ) 
i
i
x
dx
x
dx
∂
= ∂
∫
∫
θ
θ
 is valid for any function 
f( , )
x θ  considered in this analysis. 
- 
The Fisher Information matrix is given by  [
]
( )
E
( )
i
j
ij
⎡
⎤
=
∂∂
⎣
⎦
θ
I θ
θ
A
 
 
Fig. 8.5. Statistical Manifold 
Given these conditions, if we consider a mapping 
:
n
ϕ
→\
M
  such that 
(
)
p( | )
ϕ
=
x θ
θ , the vector θ  is used as a coordinate system  for the family 
{
}
p( | )
x θ
, forming a manifold M  embedded in the space of probability distribu-
tions. The parameter θ  labels each point P of M  (see Figure 8.5).  
Having established the connection between a parametric family of distributions and a 
manifold, we can try to identify the objects in the language of differential geometry 
that map those objects in the statistical domain: 
The 
tangent 
space 
(
)
TP M  
at 
the 
point 
P  
with 
coordinates   
1
2
(
,
,..,
)
n
n
θ θ
θ
=
∈
θ
rG
\  
corresponds 
to 
the 
vector 
space 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
203 
{
}
(1)
( ) |
( )
( )
i
i
T
V x
V x
V
=
=
∂
θ
θ
A
 spanned by the functions in x  
( ),  
1..
i
i
n
∂
=
θ
A
, 
such that: 
(1)
(
)
( )
i
i
T
T
∈
⇔∂
∈
θ
r
θ
G
A
P M
 
(8.30)
The space 
(1)
Tθ
 is called by Amari [26] the 1-representation of the tangent 
space
(
)
TP M .  
The choice of 
( )
i∂
θ
A
  as the basis for the vector space 
(1)
Tθ
, renders a 1-
representation of a vector 
(1)
( )
V x
T
∈
θ
 with  vanishing expected value: 
[
]
[
]
( )
log p( | ) p( | ) 
p( | ) 
0
i
i
x
i
x
E
x
x
dx
x
dx
∂
=
∂
=∂
=
∫
∫
θ
θ
θ
θ
A
 
(8.31)
Given two points 
q
P  and 
p
P defining probability densities 
p( | )
q
x
=
θ  and 
p( |
)
p
p
x
=
θ
, with coordinates given by 
1
2
(
,
,..,
)
n
θ θ
θ
=
θ
rG
 and  
p
d
=
+
θ
r
r
G
G , 
where drG  is a tangent vector at 
q
P , the Kullback number (relative entropy) 
(
: )
p q
I
 between p  and q  is a nonnegative function with a minimum equal to zero 
at 
p =
θ
θ . If we apply a Taylor expansion to second order on 
(
: )
p q
I
 at the mini-
mum 
p =
θ
θ , the first two terms of the expansion vanish, yielding: 
]
1
(
: )
(
: )
2
t
p q
d
p q
d
≅
∇∇
θ
r
r
G
G
I
I
 
(8.32)
where 
]
(
: ) p
p q
∇∇I
 is the matrix of second derivatives 
2 (
: )
i
j
p q
θ θ
⎛
⎞
∂
⎜
⎟
⎜
⎟
∂∂
⎝
⎠
I
 evaluated at 
θ . By applying straight forward computation on (32) we obtain the following quad-
ratic form: 
1
1
1
p( | ) 
p( | )  p( | ) 
 
2
p( | )
p( | )
(
: )
1
             
E
( )
( )
 
2
i
j
i
j
x
i
j
i
j
x
x
x
dx d
d
x
x
p q
d
d
∂
∂
⎧
⎫
⎛
⎞
≅
⎨
⎬
⎜
⎟
⎝
⎠
⎩
⎭
⎡
⎤
≅
∂
⋅∂
⎣
⎦
∫
θ
θ
θ
r
r
θ
θ
θ
θ
r
r
G
G
G
G
A
A
I
 
(8.33)
If we define  the inner product of the  basis vectors 
( )
i∂
θ
A
  and 
( )
i∂
θ
A
  as: 
g ( )
,
ij
i
j
d
d
=
θ
r
r
G
G
= E
( )
( )
i
j
⎡
⎤
∂
⋅∂
⎣
⎦
θ
θ
A
A
 
(8.34)

204 
E.J.M. Lauría 
This uniquely determines a Riemannian metric gij , invariant under reparameteriza-
tion, where the matrix (g )
ij  is the Fisher information matrix ( )
I θ . This can be shown 
as follows: 
[
]
E
( )
logp( | ) p( | )
1
         
  
p( | ) p( | )
p( | )
1
         
  
p( | )
p( | )
p( | )
p( | )
1
1
         
  
0
p( | )
p( | ) p( |
p( | )
p( | )
i
j
i
j
x
i
j
x
i
j
i
j
x
i
j
x
x
x
dx
x
x
dx
x
x
x
x
dx
x
x
x
x
x
x
⎡
⎤
∂∂
=
∂∂
⎣
⎦
⎡
⎤
=
∂
∂
⎢
⎥
⎣
⎦
⎡
⎤
=
∂∂
−
∂
∂
⎢
⎥
⎣
⎦
⎛
⎞⎛
⎞
=
−
∂
∂
⎜
⎟⎜
⎟
⎝
⎠⎝
⎠
∫
∫
∫
∫
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
A
)
         
  
E
( )
( )
i
j
dx
⎡
⎤
= −
∂
∂
⎣
⎦
θ
θ
θ
A
A
 
(8.35)
which coincides with expression (8.34), so that: 
( | )
(g )
E
( )
( )
E
( ) .
ij
i
j
i
j
x
⎡
⎤
⎡
⎤
≡
=
∂
⋅∂
= −
∂∂
⎣
⎦
⎣
⎦
I θ
θ
θ
θ
A
A
A
 
(8.36)
This means that by assuming that the Kullback number is the natural measure of 
separation between two probabilities in a parameter manifold, the induced local dis-
tance is equal to: 
1
(
: )
g ( )
 
2
ij
i
j
p q
d
d
≅
θ
r
r
G
G
I
 
(8.37)
A more general derivation of these expressions can be found in [31]. 
The element of area in the manifold of probability distributions is given by: 
det ( ) 
d
d
=
I θ
θ
V
 
(8.38)
Normalizing this measure by dividing the volume of the model family 
det ( )d
∫
I θ
θ  gives the so called Jeffreys’ prior15. 
                                                           
15 Jeffreys [32] proposed to use the square root of the determinant of the Fisher information ma-
trix as a non-informative prior. Roughly speaking, a prior distribution is non-informative if 
the prior is “flat” relative to the likelihood function; this means that the prior has small 
impact on the posterior distribution. Total ignorance seems to be best represented by using a 
uniform prior. But this condition is not sufficient: a uniform prior should also be uniform 
(i.e. remain invariant) under reparametrization. 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
205 
Let  
( ),
( )
V
x V
x
α
β
 be the 1-representations of 
,
(
)
T
α
β ∈
v
v
G
G
P M . Then their inner 
product is the covariance of the random variables 
( ),
( )
V
x V
x
α
β
: 
,
E
( )
( )
Covariance
( ),
( )
V
x V
x
V
x V
x
α
β
α
β
α
β
⎡
⎤
⎡
⎤
=
⋅
=
⎣
⎦
⎣
⎦
v
v
G
G
 
(8.39)
given the fact that 
[
]
E
( )
E
( )
0
V
x
V
x
α
β
⎡
⎤
=
=
⎣
⎦
. The length |
|
α
vG
of a tangent vector 
α
vG  is given by 
(
)
2
2
|
|
,
E
( )
V
x
α
αα
α
α
⎡
⎤
=
=
⎣
⎦
v
v
v
G
G
G
, which is the variance of the ran-
dom variable  Vα . 
8.3.2   The Info-geometric Version of the Minimum Description Principle 
The formulation of the Minimum Description Length Principle has gone through sev-
eral iterations. The two-part code MDL version described before has the same form as 
the Bayesian Information Criterion (BIC)16 which shows that the model  attaining: 
,| |
|
|
ˆ
min
log p(
| )
log
2
D
N
+
⎧
⎫
−
⎨
⎬
⎩
⎭
θ θ
θ
θ
 
(8.40)
provides the most efficient encoding of the data D. 
Rissanen [33] has proposed the following version of the MDL score: 
|
|
ˆ
log p(
| )
log
log
det ( ) 
o(1)
2
2
N
MDL
D
d
π
+
= −
+
+
∫
θ
θ
I θ
θ
 
(8.41)
The first term is minus the logarithm of the maximum likelihood, the second term 
measures the complexity given by the number of parameters in the model and the last 
term measures the complexity given by geometrical properties of the model (recall 
that 
det ( ) d
∫
I θ
θ  is the volume of the model manifold).  As can be seen, the ex-
pression of the MDL is given by a log likelihood penalized by factors associated to 
the complexity of the model, and in that sense reinforces the bias towards selecting 
simpler models. The merit of this version of MDL is to quantify complexity by con-
sidering both the number of parameters and the geometry of the hypothesis space.  
The complexity penalty measure can be explained heuristically [34] by considering 
the Kullback distance between the true distribution 
0
0
p
p(
|
)
D
≡
θ
θ
 and the paramet-
ric family of probability distributions 
{
}
,
p
p(
| , ),
D
D
=
∈
θ
θ
θ
Θ
G
, with a prior 
p( |
) on 
θ
Θ
G
, which describes a data set D of N independent  and identically  
distributed samples. Recalling that the marginal likelihood of the data can be  
                                                           
16 This has spurred some confusion, leading some people to think that MDL and BIC are the 
same measure. 

206 
E.J.M. Lauría 
expressed as p(
|
)
p(
| , ) p( |
) 
D
D
d
=
⋅
∫Θ
θ
θ
θ
 G
G
G
, and assuming  that the Central 
Limit Theorem holds for the maximum likelihood estimators  of the family 
,
pD θ , it 
turns out that the Kullback number 
0
,
(p
:p
)
D
θ
θ
I
 can be expressed as: 
0
0
0
,
p(
|
)
(p
:p
)
E
log p(
| , )
|
|
1
1
                   
log
logdet ( )
log
2
2
2
p( | )
D
D
D
N
π
⎡
⎤
=
⎢
⎥
⎣
⎦
≅−
−
+
θ
θ
θ
θ
θ
θ
I θ
θ
I
G
G
 
(8.42)
If we let the prior p( | )
θ G be the normalized Jeffreys’ prior given by: 
det ( )
p( | )
det ( )d
=
∫
I θ
θ
I θ
θ
G
 
(8.43)
then the Kullback distance 
0
,
(p
:p
)
D
θ
θ
I
 is equal to minus the model complexity 
penalty of the Minimum Description Length: 
0
,
|
|
(p
:p
)
log
log
det ( ) 
2
2
D
N
d
π
≅−
−
∫
θ
θ
θ
I θ
θ
I
 
(8.44)
Balasubramanian [34] introduces a measure of model complexity closely con-
nected to expression (8.44), which he calls the “razor” of a model. Balasubramanian’s 
razor takes the form:  
,
0
(p
:p
)
det ( ) 
( )
det ( ) 
D
N
N
e
d
R
d
−
= ∫
∫
θ
θ
I θ
θ
I θ
θ
I
G
 
(8.45)
where, as before, 
*
0
,
(p
:p
)
D
θ
θ
I
is the Kullback number between the true distribution 
,
pD θ  and the distribution 
*
,
pD θ   indexed by 
*
θ  that minimizes 
0
,
(p
:p
)
D
θ
θ
I
. His 
Taylor expansion of the razor renders the following expression, given 
by
( )
log
( )
N
R
Ν
χ
= −
G
G : 
0
|
|
ˆ
( )
log p(
| )
(p )
log
2
2
1
1
              
log
log
det (
) 
  + higher order terms
2
N
D
N h
d
Ν
π
χ
= −
−
⋅
+
+
+
∫
θ
θ
θ
I θ
θ
G
V
K
 
(8.46)
where 
0
(p )
h
θ
 is the differential entropy calculated on the true distribution, and 
V
K  
is the fraction of the volume  of the model manifold  occupied  by distinguishable 
probability distributions lying close to the truth (two probability distributions are  

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
207 
indistinguishable if it cannot be established which of them generated a given data set 
when the sample data size growths indefinitely)17.   
It is easily seen that if we discard 
0
(p )
N h
⋅
θ
, which depends on the true distribu-
tion, and we disregard 
1
1
 
log
2
⎛
⎞
⎜
⎟
⎝
⎠
V
K
, we obtain the same score as the one provided 
by Rissanen’s info-geometric MDL,  as shown in expression (8.41). As such, the 
MDL score can be seen in a certain sense as an approximation of the razor of a model.  
According to the info-geometric version of MDL we would therefore prefer mod-
els that: 
- 
Are accurate in describing the data, measured in terms of a larger likelihood of 
the data, or a smaller relative entropy between the true distribution and the distri-
bution on the model manifold.  
- 
Are simpler (fewer parameters), given the fact that smaller models are computa-
tionally easier to handle, are less prone to overfitting the data and to the conse-
quent generalization error, and will generally have fewer local maxima in which 
an estimation procedure can get stuck. 
- 
Hold a smaller volume in the space of distributions and are therefore more  
constrained. 
8.3.3   The Info-geo Scoring Metric 
We propose the use of a scoring metric based on the info-geometric version of the 
Minimum Description Length Principle, taking into account the volume of the BN’s 
manifold. The info-geo score is a maximum likelihood function, penalized by the di-
mensionality ofθ  and the volume of the statistical manifold. With complete, discrete 
data, the likelihood term is reduced to a sum of local counts that consider the number 
of times a given node takes a certain value for a given configuration of its parent val-
ues; these values are easily computed.  The dimensionality |
|
θ  is obtained in a 
straightforward manner by adding up the product of rows and columns of the condi-
tional probability tables at each node. Therefore our efforts should concentrate in 
finding a proper way of computing the volume 
log
det ( ) d
=
∫
I θ
θ
V
 at each step 
of the search procedure.  
In order to exemplify the computation of the volume  V  for a given DAG, let us 
consider the case of the discrete, binary BN depicted in Figure 8.6. 
The BN can be expressed in terms of its joint probability as: 
2
1
1
2
2
1
1
2
1
1
p(
,
)
p(
|
) p(
)
             
( )
x
x
x x
x
x
x
x
θ
θ
=
⋅
=
⋅
 
(8.47)
                                                           
17 For more details as to how to estimate 
V
K  see [34]. 

208 
E.J.M. Lauría 
 
Fig. 8.6. Computing the volume V of a binary BN of two nodes 
or equivalently: 
[
]
[
]
[
]
[
]
2
2
1
1
1
1
1
2
20
1
21
1
10
11
p( ,
)
( )
( )
x
x
x
x
x x
x
x
θ
θ
θ
θ
−
−
=
⋅
⋅
⋅
 
(8.48)
Acknowledging that
11
10
1
θ
θ
= −
, and 
21
1
20
1
( )
1
( )
x
x
θ
θ
= −
, this means that: 
1
2
11
21
1
2
11
21
1
2
11
21
1
2
11
21
p(
0,
0| )
(1
) (1
(0))
p(
0,
1| )
(1
)
(0)
p(
1,
0 | )
(1
(1))
p(
1,
1| )
(1)
x
x
x
x
x
x
x
x
θ
θ
θ
θ
θ
θ
θ
θ
=
=
=
−
⋅
−
⎧
⎪
=
=
=
−
⋅
⎪⎨
=
=
=
⋅
−
⎪
⎪
=
=
=
⋅
⎩
θ
θ
θ
θ
 
(8.49)
The Fisher information matrix ( )
I θ , is in this case a 3 x 3 matrix with elements 
,
ix
jx
i
j
Iθ
θ
: 
2
,
11
21
21
( )
( )
( )
E
,  for 
,
(0),
(1)
(
)
(
)
ix
jx
i
j
i
j
ix
pa j
jx
pa j
I
x
x
θ
θ
θ
θ
θ
θ
θ
⎡
⎤
∂
= −
⎢
⎥
∂
∂
⎢
⎥
⎣
⎦
θ
θ
A
 
(8.50)
Taking logarithms on (8.49), we compute the log likelihood ( )
θ
A
 as: 
11
21
1
2
11
21
1
2
11
21
1
2
11
21
1
2
log(1
)
log(1
(0))
if ( ,
)
(0,0)
log(1
)
log
(0)       
if ( ,
)
(0,1)
( )
      
log
log(1
(1))       
if ( ,
)
(1,0)
log
log
(1)              
if ( ,
)
(1,1)
x x
x x
x x
x x
θ
θ
θ
θ
θ
θ
θ
θ
−
+
−
=
⎧
⎪
−
+
=
⎪
= ⎨
+
−
=
⎪
⎪
+
=
⎩
θ
A
 
(8.51)
Using (8.51), we calculate the expected value of the second derivative to obtain the 
component 
11, 11
Iθ
θ of the Fisher information matrix ( )
I θ : 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
209 
11
21
2
2
11
11
11
11
21
2
2
11
11
11
11
21
2
2
11
11
11
2
11
11
11, 11
2
2
11
11
1
1
1
(1
) (1
(0))
1
(1
)
(1
)
1
1
1
(1
)
(0)
       
1
(1
)
(1
)
1
1
1
(1
(1))
  
1
1
( )
( )
Iθ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
θ
−
−
−
⋅
−
⋅
−
+
−
−
−
−
−
−
⋅
−
⋅
+
−
−
−
−
−
⋅
⋅
−
+
−
⎧
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
∂
∂
⎪
⎪
=
⇒
=
⇒
=
⎨
⎨
∂
∂
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
⎩
θ
θ
A
A
11
21
2
11
            
1
(1)                      
θ
θ
θ
−
⋅
⋅
⎧
⎪
⎪
⎪
⎪⎪⎨
⎪
⎪
⎪
⎪
⎪⎩
 
(8.52)
This renders: 
11, 11
11, 21(0)
11, 21(1)
11
11
1
(1
)
0              
0              
I
I
I
θ
θ
θ
θ
θ
θ
θ
θ
⎧
=
⎪
−
⎪⎪
=
⎨
⎪
=
⎪
⎪⎩
 
(8.53)
We calculate the other matrix elements in a similar fashion: 
21(0), 21(0)
21(0), 11
21(0), 21(1)
11
21
21
(1
)
(0) (1
(0))
0                               
0                            
I
I
I
θ
θ
θ
θ
θ
θ
θ
θ
θ
−
⎧
=
⎪
⋅
−
⎪⎪
=
⎨
⎪
=
⎪
⎪⎩
 
(8.54) 
21(1), 21(1)
21(1), 11
21(1), 21(0)
11
21
21
(1) (1
(1))
0                             
0                          
I
I
I
θ
θ
θ
θ
θ
θ
θ
θ
θ
⎧
=
⎪
⋅
−
⎪⎪
=
⎨
⎪
=
⎪
⎪⎩
 
(8.55) 
Being that all the 
,
ix
jx
i
j
Iθ
θ
elements outside the principal diagonal are equal to zero, 
the determinant of ( )
I θ  is equal to
11, 11
21(0), 21(0)
21(1), 21(1)
I
I
I
θ
θ
θ
θ
θ
θ
×
×
: 
11
11
11
21
21
11
21
21
1
0
0
(1
)
(1
)
( )
0
0
(0) (1
(0))
0
0
(1) (1
(1))
θ
θ
θ
θ
θ
θ
θ
θ
⎡
⎤
⎢
⎥
−
⎢
⎥
⎢
⎥
−
= ⎢
⎥
⋅
−
⎢
⎥
⎢
⎥
⎢
⎥
⋅
−
⎢
⎥
⎣
⎦
I θ
 
(8.56)

210 
E.J.M. Lauría 
This means that the volume of the manifold, calculated by integrating the square 
root of the determinant of ( )
I θ over the vector parameter θ , is equal to: 
12
21
21
21
21
1
(0) (1
(0))
(1) (1
(1))
d
θ
θ
θ
θ
⎡
⎤
=
⎢
⎥
⋅
−
⋅
⋅
−
⎣
⎦
∫Θ
θ
V
 
 
(8.57)
The multiple integral in expression (8.57) can be simplified as follows: 
{
}
{
}
1
(1 2 1)
(1 2 1)
21
21
21
0
1
(1 2 1)
(1 2 1)
21
21
21
0
2
(0)
1
(0)
(0)  
      
(1)
1
(1)
(1)
  
 Beta(1 2) Beta(1 2) = 
d
d
θ
θ
θ
θ
θ
θ
π
−
−
−
−
⎡
⎤
⎡
⎤
=
−
×
⎣
⎦
⎣
⎦
⎡
⎤
⎡
⎤
−
⎣
⎦
⎣
⎦
=
⋅
∫
∫
V
 
 
(8.58)
Rodríguez [35] derives the following exact formula for the volume element of a 
Bayesian Net of n discrete nodes: 
(
)
(
)
(
)
( )
( )
1 2
( )
( )
1
1 2
1
( )
0
det ( ) 
     
 
 
i
j
ap i
i
pa i
r
jx
pa j
n
x
j an j
r
i
x
ik
pa i
k
d
d
x
d
x
θ
θ
−
∈
−
=
=
=
⎧
⎫
⎪
⎪
⎨
⎬
⎪
⎪
⎩
⎭
=
⎡
⎤
⎣
⎦
∑∏
∏∏
∏
I θ
θ
θ
V  
 
(8.59)
The following convention is used: 
- 
There are n  variables in the network, 
,
1..
i
X i
n
=
 
- 
( )
pa i denotes the set of parents  of each node 
i
i
X
x
=
  
- 
( )
an i denotes the set of ancestors (including parents) of  each node 
ix   
- 
( )
ap i denotes the set of ancestors not parents of  each node 
ix  such that  
( )
( ) \
( )
ap i
an i
pa i
=
. 
- 
ir  denotes the number of possible states 
0,1,..,
1
i
i
x
r
=
−of variable 
i
X  
- 
( )
(
)
ik
pa i
x
θ
 is an entry in the conditional probability table for node 
i
X  taking 
value 
ix
k
=
 for  each of the 
iq  configurations of its parents 
( )
pa i
x
 
- 
( )
...
pa i
s
r
t
x
x
x
x
≡
∑∑∑∑
denotes the multiple sum over all possible values  of 
,
( )
{
,..,
)
pa
r
s
t
x
i
x x
x
=
 
Expression (8.59) can be obtained by calculating the Fisher Information matrix as 
minus the expectation  
2
( )
( )
( )
E
(
)
(
)
i
j
ix
pa j
jx
pa j
x
x
θ
θ
⎡
⎤
∂
⎢
⎥
∂
∂
⎢
⎥
⎣
⎦
θ
θ
A
 , where: 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
211 
( )
( )
1
1
( )
log
(
|
)
log
(
)
n
n
i
pa i
ik
pa i
i
i
p x
x
x
θ
=
=
=
=∑
∏
θ
A
 
(8.60)
Taking first derivatives over the multinomial expression of the likelihood, we verify that: 
0
( )
( )
( )
( )
( )
1
(
)
if 
0 and a given 
       
( )
1
(
)
if 
0, and a given 
(
)
0
otherwise                               
i
pa i
i
pa i
ik
pa i
i
pa i
ik
pa i
x
x
x
x
x
k
x
x
θ
θ
θ
−
=
⎧
∂
⎪
=
=
≠
⎨
∂
⎪⎩
θ
A
(8.61)
As the first derivative in only dependent on
( )
(
)
ik
pa i
x
θ
, the second derivative van-
ishes in all cases except
2
2
( )
( )
(
)
ik
pa i
x
θ
∂
∂
θ
A
. For the second derivatives we get the follow-
ing expression: 
2
0
( )
( )
2
2
( )
( )
2
( )
1
(
)
if 
0 and a given 
       
( )
1
(
)
if 
0, and a given 
(
)
0
otherwise                               
i
pa i
i
pa i
ik
pa i
i
pa i
ik
pa i
x
x
x
x
x
k
x
x
θ
θ
θ
⎧−
=
∂
⎪
= −
=
≠
⎨
∂
⎪⎩
θ
A
 
(8.62)
Taking expectations on (8.62): 
2
( )
2
( )
0
( )
( )
( )
( )
0
( )
( )
( )
1
E
p(
0,
| )
(
)
(
)
1
                                   
p(
,
| )
(
)
1
1
                           
p(
| )
(
)
(
)
i
pa i
ik
pa i
i
pa i
i
pa i
ik
pa i
pa i
i
pa i
ik
pa i
x
x
x
x
x
k x
x
x
x
x
θ
θ
θ
θ
θ
⎡
⎤
∂
= −
⋅
=
−
⎢
⎥
∂
⎢
⎥
⎣
⎦
⋅
=
⎛
⎞
= −
+
⋅
⎜
⎟
⎜
⎟
⎝
⎠
θ
θ
θ
θ
θ
A
 
(8.63)
and computing the determinant of the Fisher information as the product of the main 
diagonal we get: 
( )
( )
1
( )
1
1
0
( )
( )
( )
1
1
( )
0
1
1
det ( )
p(
| )
(
)
(
)
p(
| )
                
(
)
i
pa i
i
pa i
r
n
pa i
i
x
k
i
pa i
ik
pa i
n
pa i
r
i
x
ik
pa i
k
x
x
x
x
x
θ
θ
θ
−
=
=
−
=
=
⎧
⎫
⎡
⎤
⎪
⎪
=
+
⋅
⎢
⎥
⎨
⎬
⎢
⎥
⎪
⎪
⎣
⎦
⎩
⎭
=
∏∏∏
∏∏
∏
I θ
θ
θ
 
(8.64)
Note that 
( )
p(
| )
pa i
x
θ  can be expressed as: 

212 
E.J.M. Lauría 
( )
( )
( )
( )
( )
( )
( )
p(
| )
p(
,
)
                
(
)
ap i
j
ap i
pa i
pa i
ap i
x
jx
pa j
x
j an i
x
x
x
x
θ
∈
=
⎡
⎤
=
⎢
⎥
⎣
⎦
∑
∑∏
θ
θ
 
(8.65)
Replacing (8.65) in (8.64) we obtain: 
( )
( )
( )
( )
1
1
( )
0
(
)
det ( )
(
)
j
ap i
i
pa i
jx
pa j
n
x
j an i
r
i
x
ik
pa i
k
x
x
θ
θ
∈
−
=
=
⎡
⎤
⎢
⎥
⎣
⎦
=
∑∏
∏∏
∏
I θ
 
(8.66)
from where (8.59) follows.  
To compute the volume
log
det ( ) d
=
∫
I θ
θ
V
, we need to calculate the integral 
of expression (8.59) which, in most cases, does not have a closed form solution. We 
therefore need to resort to computing an estimate by simulation, applying Markov 
chain Monte Carlo for example. This solution may be too expensive if the number of 
iterations in each MCMC run is significant.  An alternative solution consists in calcu-
lating an approximate value of the volume based on computing high and low bounds 
of the integral. An algorithm by Rodríguez [36] takes as input parameter the adja-
cency matrix of a discrete BN with binary nodes, and returns a low and high bound of 
the volume together with the geometric mean of both values. The geometric mean is 
subsequently used to approximate the value of the volume. The algorithm takes into 
account the fact that the analytical expression of the volume of a discrete binary DAG 
can be assimilated to a factorization of Beta integrals (see the result of the example at 
the beginning of this section). The algorithm was benchmarked by comparing its out-
come with exact solutions obtained by integrating expression (8.59), and with  
estimates drawn from MCMC simulations, and has proven to return adequate  
approximations of the true volume of the BN under analysis [37]. 
8.3.4   Assessing the Importance of the Geometric Term in the Info-geo Score 
Function 
As we see from expression (8.59), the volume of the manifold is a geometric charac-
teristic associated with the BN’s topology. Each BN produces a different magnitude 
of the volume based on the BN’s DAG. This means that if the geometric term is rele-
vant enough compared to the other two terms of the MDL expression, it may very 
well be a critical factor in providing an accurate discrimination of competing BNs in 
the model selection process. Let us consider the general expression of the info-geo 
MDL score: 
|
|
ˆ
log p(
| )
log
log
det ( ) 
2
2
N
info - geo MDL
D
d
π
+
= −
+
∫
θ
θ
I θ
θ  
(8.67)

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
213 
For large enough sample sizes, the log likelihood term which increases linearly 
with N, dominates the score. The O(log N) term , |
|log
2
2
N
π
θ
tells us that the value of 
the score increases linearly in the dimension of the parameter space. But what hap-
pens with the geometrical term? The formulation of the geometric term suggests a 
preference for BNs with a smaller manifold volume. Very small volumes (close 
enough to zero) would imply a geometric term that adds substantially to the log like-
lihood instead of penalizing it. In the limit, a volume equal to zero would imply a 
negative infinite geometrical term that would evidently control the score [37]. 
 
Fig. 8.7. Bayesian Networks with varying topologies (DAGs) and number of nodes 
The implication of this analysis is quite straightforward: if the topology of the BN 
that we are trying to learn from sample data is such that the volume of the statistical 
manifold is small enough, for moderate sample sizes, the geometric term may provide 
a major contribution to the info-geo scoring function, therefore enhancing the dis-
crimination accuracy of well tested scoring functions such as BIC. 
To clarify this matter, let us consider the set of binary BN structures depicted in 
Figure 8.7. The group of DAGs has been organized in categories (collapse, line, star, 
pseudoloop, diamond, rhombus) according to their topologies. The set is by no means 
an exhaustive list of possible DAGs (the number of DAGs as a function of the number 
of nodes n is super-exponential in n), but it does represent a characteristic set of to-
pologies: DAGs with all the edges converging to one node (collapse), DAGs will all 
the edges diverging from one node (star), DAGs with nodes linked sequentially (line), 
DAGs with nodes organized in a structure close to a cycle  (pseudoloop), edges di-
verging form one node into a set of nodes and then converging to one last node  
(diamond), DAGS with two nodes feeding the rest  of  the nodes (rhombus). 

214 
E.J.M. Lauría 
Table 8.1. Approximate values of log(volume) for BNs with varying topologies and number of 
nodes 
 
3 nodes 
4 nodes 
6 nodes 
8 nodes 
Topology 
Volume 
LogVol 
Volume 
LogVol 
Volume 
LogVol 
Volume 
LogVol 
COLLAPSE 
15.02 
2.71
3.79
1.33
5.99E-10
-21.24
1.71E-75
-172.16 
LINE 
37.15 
3.61
139.83
4.94
1981.21
7.59
28070.16
10.24 
STAR 
38.25 
3.64
160.23
5.08
3121.60
8.05
65158.37
11.08 
PSEUDOLOOP 
16.23 
2.79
95.99
4.56
1360.01
7.22
19268.85
9.87 
DIAMOND 
- 
-
94.38
4.55
0.29
-1.24
8.51E-27
-60.03 
RHOMBUS 
- 
-
51.44
3.94
1013.76
6.92
25574.13
10.15 
 
Table 8.1 displays the approximate volume of each DAG for each of these catego-
ries and for a varying number of nodes (3, 4, 6 and 8). A chart tracing the values of 
the log(volume)  as a function of the number of nodes  is shown in Figure 8.8. 
It is quite evident from Table 8.1 that those topologies (namely collapse and dia-
mond) with a large maximum indegree (number of parents feeding a given node) have 
volumes which decrease in a super-exponential manner with the number of nodes.  
As shown in Figure 8.8, if the number of nodes is big enough, for a moderate value 
of indegree the geometrical term could provide a substantial contribution of to the to-
tal score, which could in turn enhance the accuracy of the model searching process. 
The inclusion of the log(volume) as part of the score may be the difference, in such 
cases, between succeeding or failing in finding the underlying structure of the model. 
In all the cases where the maximum number of fan-in nodes is low, the volume re-
mains within a bounded interval, suggesting that the contribution to the score, given 
by the log(volume), is close to null.   This means that in these cases it should be rea-
sonable to imagine a smaller difference in the performance of the info-geo scoring 
function relative to the BIC score.  
 
Fig. 8.8. Log(volume) as a function of the number of nodes (extracted from [37]) 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
215 
Lauria [37] checked the practical validity of the info-geo scoring function, bench-
marking it against BIC on a series of experiments using binary Bayesian networks.  The 
experiments tested the relative accuracy of the scoring metrics when varying the size 
and complexity of the BN. The general setup for the experiments was the following:  
- 
Given a binary BN with a predefined topology and random network parameters 
(conditional probability tables), the BN was sampled to generate a fairly large 
collection of simulated data sets.  
- 
The sampled data was used repeatedly to heuristically search the space of net-
work topologies. The goal was to recover the DAG that best fitted the sampled 
data. 
The tests showed that the info-geo score surpassed BIC for all data sets of moder-
ate size (100 – 500) samples. As predicted, for larger data sets, the log-likelihood term 
dominated the score, dwarfing the contribution of the geometric term and therefore 
eliminating the difference between both scores. The tests confirmed also that in the 
case of the collapse and diamond structures, the info-geo score radically outperforms 
BIC as the number of fan-in nodes increase, while presenting a similar performance to 
BIC in the rest of the topologies. Complete details of these simulation experiments 
can be found in [37]. 
8.4   Conclusion 
Learning a Bayesian network from data implies two stages: a qualitative stage that 
deals with the task of learning the dependency among the variables of the network 
(given by the topology of its directed acyclic graph); and a quantitative stage that de-
termines the conditional probabilities of each variable, given its parents. In this work 
we describe an information-geometric scoring (info-geo) metric that enhances the 
Bayesian Information Criterion (BIC) score and is based on the Minimum Description 
Length Principle. While MDL has been applied before as a scoring metric in model 
searching tasks, the underlying geometric properties of this approach applied to  
learning the topology of BNs has not been formally established in the literature. In 
particular, the inclusion of an additional term that gauges the information-geometric 
properties of the underlying statistical manifold may be instrumental in selecting ‘bet-
ter’ models from limited sets of data. We conclude that the info-geo score is as least 
as efficient as the BIC score and, under certain circumstances, can drastically improve 
the accuracy of the model searching process when learning BN topologies from data. 
References 
1. Friedman, N., Nachman, I., Peer, D.: Learning Bayesian Network Structures from Massive 
Datasets: The Sparse Candidate Algorithm. In: Proceedings of the Fifteenth Conference on 
Uncertainty in Articial Intelligence (UAI 1999), pp. 206–215 (1999) 
2. Neapolitan, R.: Learning Bayesian Networks. Artificial Intelligence. Prentice-Hall, Engle-
wood Cliffs (2003) 
3. Cooper, G., Herskovits, E.: A Bayesian Method for the Induction of Probabilistic Net-
works from Data. Machine Learning 9(4), 309–347 (1992) 

216 
E.J.M. Lauría 
4. Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., Teller, E.: Equation of state 
calculations by fast computing machines. Journal of Chemical Physics 21, 1087–1092 
(1953) 
5. Kirkpatrick, S., Gelatt, D., Vecchi, M.: Optimization by simulated annealing. Science 220, 
671–680 (1983) 
6. Madigan, D., York, J.: Bayesian Graphical Methods for Discrete Data. International Statis-
tical Review 63(2) (1995) 
7. Friedman, N., Koller, D.: Being Bayesian About Network Structure: A Bayesian Approach 
to Structure Discovery in Bayesian Networks. In: Proceedings of the 16th Conference on 
Uncertainty in Artificial Intelligence (UAI) (2000) 
8. Friedman, N.: Learning Bayesian networks in the presence of missing values and hidden 
variables. In: Proceedings of the 13th Confenrence on Uncertainty in Artificial Intelligence 
(UAI) (1997) 
9. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete data via 
the EM algorithm (with discussion). J.R. Statist. Soc. B 39, 1–38 (1977) 
10. Kass, R.E., Tierney, L., Kadane, J.B.: The validity of posterior asymptotic expansions 
based on Laplace’s method. In: Geisser, S., Hodges, J.S., Press, S.J., Zellner, A. (eds.) 
Bayesian and Likelihood Methods in Statistics and Econometrics. North Holland, New 
York (1990) 
11. Kass, R., Raftery, A.E.: Bayes factors and model uncertainty. Journal of the American Sta-
tistical Association 90, 773–795 (1995) 
12. Schwarz, G.: Estimating the dimension of a model. Annals of Statistics 6, 461–464 (1978) 
13. Heckermann, D.: A tutorial on learning with Bayesian Networks. In: Jordan, M. (ed.) 
Learning in graphical models. MIT Press, Cambridge (1999) 
14. Mitchell, T.: Machine Learning. McGraw-Hill, New York (1997) 
15. Rissanen, J.: Modeling by the shortest data description. Automatica J. IFAC 14, 465–471 
(1978) 
16. Shannon, C.: A Mathematical Theory of Communication. The Bell System Technical 
Journal 27, 379–423, 623–656 (1948) 
17. Vitányi, P., Ming, L.: Minimum Description Length Induction, Bayesianism, and Kolmo-
gorov Complexity. IEEE Transactions on Information Theory 46(2) (2000) 
18. Solomonoff, R.J.: A formal theory of inductive inference. Inform. Contr. pt. 1, 2, 7, 224–
254 (1964) 
19. Kolmogorov, A.N.: Three approaches to the quantitative definition of information. Probl. 
Inform. Transm. 1(1), 1–7 (1965) 
20. Chaitin, G.J.: A theory of program size formally identical to information theory. J. 
ACM 22, 329–340 (1975) 
21. Hansen, M., Yu, B.: Model Selection and the Principle of Minimum Description Length. 
JASA 96(454), 746–774 (2001) 
22. Rissanen, J.: Stochastic Complexity and Modeling. Annals of Statistics 14(3), 1080–1100 
(1986) 
23. Lipschultz, M.: Differential Geometry. Schaum Series. McGraw-Hill, New York (1969) 
24. Kreyszig, E.: Differential Geometry. Dover Publications (1991) 
25. Rodríguez, C.: Entropic priors, Tech. Report, SUNY Albany, Department of Mathematics 
and Statistics (1991) 
26. Amari, S.I.: Differential Geometrical Methods in Statistics. Springer, Heidelberg (1985) 
27. Amari, S.I., Nagaoka, H.: Methods of Information Geometry. Oxford University Press, 
Oxford (2000) 

 
8   An Information-Geometric Approach to Learning BN Topologies from Data 
217 
28. Cartan, E.: Sur la possibilite de plonger un espace riemannian donne un espace Euclidean. 
Ann. Soc. Pol. Math. 6, 1–7 (1927) 
29. Janet, M.: Sur la possibilite de plonger un espace riemannian donne das un espace Euclid-
ean. Ann. Soc. Math. Pol. 5, 74–85 (1931) 
30. Nash, J.: The imbedding problem for Riemannian manifolds. Annals of Mathematics 63, 
20–63 (1956) 
31. Rodríguez, C.: The Metrics Induced by the Kullback Number. In: Skilling, J. (ed.) Maxi-
mum Entropy and Bayesian Methods, Kluwer, Dordrecht (1989) 
32. Jeffreys, H.: The Theory of Probability. Oxford University Press, Oxford (1961) 
33. Rissanen, J.: Fisher Information and Stochastic Complexity. IEEE Transaction on Informa-
tion Theory 42, 40–47 (1996) 
34. Balasubramanian, V.: A Geometric Formulation of Occam’s Razor for Inference of Para-
metric Distributions. Princeton physics preprint PUPT-1588, Princeton (1996) 
35. Rodríguez, C.: Entropic priors for discrete probabilistic networks and for mixtures of 
Gaussian models. In: Proceedings of the 21st International Worskhop on Bayesian Infer-
ence and Maximum Entropy Methods in Science and Engineering, APL Johns Hopkins 
University, August 4-9 (2001) 
36. Rodríguez, C.: The Volume of Bitnets. In: Proceedings of the 24th International Worskhop 
on Bayesian Inference and Maximum Entropy Methods in Science and Engineering. AIP 
Conference Proceedings, Garching, Germany, vol. 735(1), pp. 555–564 (2004) 
37. Lauría, E.: Learning the Structure of a Bayesian Network: An Application of Information 
Geometry and the Minimum Description Length Principle. In: Knuth, K.H., Abbas, A.E., 
Morris, R.D., Castle, J.P. (eds.) Proceedings of the 25th International Workshop on Bayes-
ian Inference and Maximum Entropy Methods in Science and Engineering, San José State 
University, USA, pp. 293–301 (2005) 

9
Causal Graphical Models with Latent Variables:
Learning and Inference
Philippe Leray1, Stijn Meganck2, Sam Maes3, and Bernard Manderick2
1 LINA Computer Science Lab UMR6241, Knowledge and Decision Team,
Universit´e de Nantes, France
philippe.leray@univ-nantes.fr
2 Computational Modeling Lab, Vrije Universiteit Brussel, Belgium
3 LITIS Computer Science, Information Processing and Systems Lab EA4108,
INSA Rouen, France
9.1
Introduction
This chapter discusses causal graphical models for discrete variables that can
handle latent variables without explicitly modeling them quantitatively. In the
uncertainty in artiﬁcial intelligence area there exist several paradigms for such
problem domains. Two of them are semi-Markovian causal models and maxi-
mal ancestral graphs. Applying these techniques to a problem domain consists
of several steps, typically: structure learning from observational and experimen-
tal data, parameter learning, probabilistic inference, and, quantitative causal
inference.
We will start this chapter by introducing causal graphical models without
latent variables and then move on to models with latent variables.
We will discuss the problem that each of the existing approaches for causal
modeling with latent variables only focuses on one or a few of all the steps
involved in a generic knowledge discovery approach. The goal of this chapter
is to investigate the integral process from observational and experimental data
unto diﬀerent types of eﬃcient inference.
Semi-Markovian causal models (SMCMs) are an approach developed by
(Pearl, 2000; Tian and Pearl, 2002a). They are speciﬁcally suited for perform-
ing quantitative causal inference in the presence of latent variables. However, at
this time no eﬃcient parametrisation of such models is provided and there are
no techniques for performing eﬃcient probabilistic inference. Furthermore there
are no techniques to learn these models from data issued from observations,
experiments or both.
Maximal
ancestral
graphs
(MAGs)
are
an
approach
developed
by
(Richardson and Spirtes, 2002). They are speciﬁcally suited for structure learn-
ing in the presence of latent variables from observational data. However, the
techniques only learn up to Markov equivalence and provide no clues on which
additional experiments to perform in order to obtain the fully oriented causal
graph. See Eberhardt et al. (2005); Meganck et al. (2006) for that type of
D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 219–249, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

220
P. Leray et al.
results for Bayesian networks without latent variables. Furthermore, as of yet no
parametrisation for discrete variables is provided for MAGs and no techniques
for probabilistic inference have been developed. There is some work on algo-
rithms for causal inference, but it is restricted to causal inference quantities that
are the same for an entire Markov equivalence class of MAGs (Spirtes et al.,
2000; Zhang, 2006).
We have chosen to use SMCMs as a ﬁnal representation in our work, because
they are the only formalism that allows to perform causal inference while fully
taking into account the inﬂuence of latent variables. However, we will combine
existing techniques to learn MAGs with newly developed methods to provide an
integral approach that uses both observational data and experiments in order to
learn fully oriented semi-Markovian causal models.
Furthermore, we have developed an alternative representation for the proba-
bility distribution represented by a SMCM, together with a parametrisation for
this representation, where the parameters can be learned from data with clas-
sical techniques. Finally, we discuss how probabilistic and quantitative causal
inference can be performed in these models with the help of the alternative
representation and its associated parametrisation1.
The next section introduces the simplest causal models and their importance.
Then we discuss causal models with latent variables. In section 9.4, we discuss
structure learning for those models and in the next section we introduce tech-
niques for learning a SMCM with the help of experiments. Then we propose a
new representation for SMCMs that can easily be parametrised. We also show
how both probabilistic and causal inference can be performed with the help of
this new representation.
9.2
Importance of Causal Models
We start this section by introducing basic notations necessary for the understand-
ing of the rest of this chapter. Then we will discuss classical probabilistic Bayesian
networks followed by causal Bayesian networks. Finally we handle the diﬀerence
between probabilistic and causal inference, or observation vs. manipulation.
9.2.1
Notations
In this work, uppercase letters are used to represent variables or sets of variables,
i.e. V = {V1, . . . , Vn}, while corresponding lowercase letters are used to represent
their instantiations, i.e. v1, v2 and v is an instantiation of all Vi. P(Vi) is used to
denote the probability distribution over all possible values of variable Vi, while
P(Vi = vi) is used to denote the probability of the instantiation of variable Vi
to value vi. Usually, P(vi) is used as an abbreviation of P(Vi = vi).
1 By the term parametrisation we understand the deﬁnition of a complete set of pa-
rameters that describes the joint probability distribution which can be eﬃciently
used in computer implementations of probabilistic inference, causal inference and
learning algorithms.

9 Causal Graphical Models with Latent Variables: Learning and Inference
221
The operators Pa(Vi), Anc(Vi), Ne(Vi) denote the observable parents, ances-
tors and neighbors respectively of variable Vi in a graph and Pa(vi) represents
the values of the parents of Vi. If Vi ↔Vj appears in a graph then we say that
they are spouses, i.e. Vi ∈Sp(Vj) and vice versa.
When two variables Vi, Vj are independent we denote it by (Vi⊥⊥Vj), when
they are dependent by (Vi
⊭
Vj).
9.2.2
Probabilistic Bayesian Networks
Here we brieﬂy discuss classical probabilistic Bayesian networks.
See Figure 9.1 for a famous example adopted from Pearl (1988) representing an
alarm system. The alarm can be triggered either by a burglary, by an earthquake,
or by both. The alarm going of might cause John and/or Mary to call the house
owner at his oﬃce.
Mary Calls
John Calls
Alarm
Burglary
Earthquake
P(M=T)
0.01
M
T
F
P(J=T|M)
0.17
0.05
M
J
P(A=T|M,J)
T
T
T
F
F
T
F
F
0.76
0.01
0.02
0.00008
P(B=T|A)
A
T
F
0.37
0.00006
A
B
P(E=T|A,B)
T
T
T
F
F
T
F
F
0.002
0.37
0.002
0.001
Fig. 9.1. Example of a Bayesian network representing an alarm system
In Pearl (1988); Russell and Norvig (1995) probabilistic Bayesian networks
are deﬁned as follows:
Deﬁnition 1. A Bayesian network is a triple ⟨V, G, P(vi|Pa(vi))⟩, with:
•
V = {V1, . . . , Vn}, a set of observable discrete random variables
•
a directed acyclic graph (DAG) G, where each node represents a variable from
V
•
parameters: conditional probability distributions (CPD) P(vi|Pa(vi)) of each
variable Vi from V conditional on its parents in the graph G.
The CPDs of a BN represent a factorization of the joint probability distribu-
tion as a product of conditional probability distributions of each variable given
its parents in the graph:

222
P. Leray et al.
P(v) =

Vi∈V
P(vi|Pa(vi))
(9.1)
Inference
A BN also allows to eﬃciently answer probabilistic queries such as
P(burglary = true|Johncalls = true, Marycalls = false),
in the alarm example of Figure 9.1. It is the probability that there was a burglary,
given that we know John called and Mary did not.
Methods have been developed for eﬃcient exact probabilistic inference when
the networks are sparse (Pearl, 1988). For networks that are more complex this
is not tractable, and approximate inference algorithms have been formulated
(Jordan, 1998), such as variational methods (Jordan et al., 1999) and Monte
Carlo methods (Mackay, 1999).
Structure Learning
There are two main approaches for learning the structure of a BN from data: score-
based learning (Heckerman, 1995) and constraint-based learning (Spirtes et al.,
2000; Pearl, 2000).
For score-based learning, the goal is to ﬁnd the graph that best matches the
data by introducing a scoring function that evaluates each network with respect
to the data, and then to search for the best network according to this score.
Constraint-based methods are based on matching the conditional indepen-
dence relations observed between variables in the data with those entailed by a
graph.
However, in general a particular set of data can be represented by more than
one BN. Therefore the above techniques have in common that they can only
learn upto the Markov equivalence class. Such a class contains all the DAGs
that correctly represent the data and for performing probabilistic inference any
DAG of the class can be chosen.
9.2.3
Causal Bayesian Networks
Now we will introduce a category of Bayesian networks where the edges have a
causal meaning.
We have previously seen that in general there is more than one probabilis-
tic BN that can be used to represent the same JPD. More speciﬁcally, all the
members of a given Markov equivalence class can be used to represent the same
JPD.
Opposed to that, in the case of a causal Bayesian network (CBN) we assume
that in reality there is a single underlying causal Bayesian network that gener-
ates the JPD. In Figure 9.2 we see a conceptual sketch: the box represents the
real world where a causal Bayesian network generates the data in the form of a

9 Causal Graphical Models with Latent Variables: Learning and Inference
223
CAUSAL BAYESIAN NETWORK
With autonomous data-generation processes
from direct causes to effects.
BLACK BOX
REAL WORLD
DATA = JPD
Fig. 9.2. Conceptual sketch of how a CBN generates a JPD, that in its turn can be
represented by several probabilistic BNs of which one is a CBN
joint probability distribution. Below we see the BNs that represent all the inde-
pendence relations present in the JPD. Only one of them is the causal Bayesian
network, in this case the rightmost.
The deﬁnition of causal Bayesian networks is as follows:
Deﬁnition 2. A causal Bayesian network is a triple ⟨V, G, P(vi|Pa(vi))⟩,
with:
•
V = {V1, . . . , Vn}, a set of observable discrete random variables
•
a directed acyclic graph (DAG) G, where each node represents a variable from
V
•
parameters: conditional probability distributions (CPD) P(vi|Pa(vi)) of each
variable Vi from V conditional on its parents in the graph G.
•
Furthermore, the directed edges in G represent an autonomous causal relation
between the corresponding variables.
We see that it is exactly the same as Deﬁnition 1 for probabilistic Bayesian
networks, with the extra addition of the last item.
This is diﬀerent from a classical BN, where the arrows only represent a prob-
abilistic dependency, and not necessarily a causal one.
Our operational deﬁnition of causality is as follows: a relation from variable
C to variable E is causal in a certain context, when a manipulation in the form
of a randomised controlled experiment on variable C, induces a change in the
probability distribution of variable E, in that speciﬁc context (Neapolitan, 2003).

224
P. Leray et al.
cloudy
rain
sprinkler
wet  lawn
cloudy
rain
wet  lawn
sprinkler
(a)
(b)
Fig. 9.3. (a) A BN where not all the edges have a causal meaning. (b) A CBN that
can represent the same JPD as (a).
This means that in a CBN, each CPD P(vi|Pa(vi)) represents a stochastic
assignment process by which the values of Vi are chosen in response to the values
of Pa(Vi) in the underlying domain. This is an approximation of how events are
physically related with their eﬀects in the domain that is being modeled. For
such an assignment process to be autonomous means that it must stay invariant
under variations in the processes governing other variables Pearl (2000).
In the BN of Figure 9.3(a), these assumptions clearly do not hold for all edges
and nodes, since in the underlying physical domain, whether or not it is cloudy is
not caused by the state of the variable sprinkler, i.e. whether or not the sprinkler
is on.
Moreover, one could want to manipulate the system, for example by changing
the way in which the state of the sprinkler is determined by its causes. More speciﬁ-
cally, by changing how the sprinkler reacts to the cloudiness. In order to incorporate
the eﬀect of such a manipulation of the system into the model, some of the CPDs
have to be changed. However, in a non-causal BN, it is not immediately clear which
CPDs have to be changed and exactly how this must be done.
In contrast, in Figure 9.3(b), we see a causal BN that can represent the same
JPD as the BN in (a). Here the extra assumptions do hold. For example, if in the
system the state of the sprinkler is caused by the cloudiness, and thus the CPD
P(sprinkler|cloudy) represents an assignment process that is an approximation
of how the sprinkler is physically related to the cloudiness. Moreover, if the
sensitivity of the sprinkler is changed, this will only imply a change in the CPD
P(sprinkler|cloudy), but not in the processes governing other variables such as
P(rain|cloudy).
Note that CBNs are a subclass of BNs and therefore they allow probabilistic
inference. In the next section we will discuss what additional type of inference
can be performed with them, but ﬁrst we treat how CBNs can be learned.
Structure Learning
As CBNs are a subset of all BNs, the same techniques as for learning the structure
of BNs can be used to learn upto the Markov equivalence class. As mentioned
before, for BNs any member of the equivalence can be used.

9 Causal Graphical Models with Latent Variables: Learning and Inference
225
For CBNs this is not the case, as we look for the orientation of the unique
network that can both represent the JPD and the underlying causal inﬂuences
between the variables. In general, in order to obtain the causal orientation of all
the edges, experiments have to be performed, where some variables in the domain
are experimentally manipulated and the potential eﬀects on other variables are
observed.
Eberhardt et al. (2005) discuss theoretical bounds on the amount of experi-
ments that have to be performed to obtain the full oriented CBN. Meganck et al.
(2006) have proposed a solution to learning CBNs from experiments and obser-
vations, where the total cost of the experiments is minimised by using elements
from decision theory.
Other related approaches include Cooper and Yoo (1999) who derived a
Bayesian method for learning from an arbitrary mixture of observational and
experimental data.
Tong and Koller (2001) provide an algorithm that actively chooses the ex-
periments to perform based on the model learned so far. In this setting they
assume there are a number of query variables Q that can be experimented on
and then measure the inﬂuence on all other variables V \Q. In order to choose
the optimal experiment they introduce a loss-function, based on the uncertainty
of the direction of an edge, to help indicate which experiment gives the most
information. Using the results of their experiments they update the distribution
over the possible networks and network parameters. Murphy (2001) introduces
a slightly diﬀerent algorithm of the same approach.
9.2.4
Causal Inference
Here we will brieﬂy introduce causal inference, we start by pointing out the
diﬀerence with probabilistic inference, and then move on to discuss an important
theorem related to causal inference.
Observation vs. Manipulation
An important issue in reasoning under uncertainty is to distinguish between
diﬀerent types of conditioning, each of which modify a given probability distri-
bution in response to information obtained.
Deﬁnition 3. Conditioning by observation refers to the way in which a
probability distribution of Y should be modiﬁed when a modeler passively observes
the information X = x.
This is represented by conditional probabilities that are deﬁned as follows:
P(Y = y|X = x) = P(y|x) = P(Y = y, X = x)
P(X = x)
.
(9.2)
This type of conditioning is referred to as probabilistic inference. It is used when
the modeler wants to predict the behavior of some variables that have not been

226
P. Leray et al.
observed, based on the state of some other variables. E.g. will the patients’
infection cause him to have a fever ?
This can be very useful in a lot of situations, but in some cases the modeler
does not merely want to predict the future behavior of some variables, but has to
decide which action to perform, i.e. which variable to manipulate in which way.
For example, will administering a dose of 10mg of antibiotics cure the patients’
infection ?
In that case probabilistic inference is not the right technique to use, because
in general it will return the level of association between the variables instead of
the causal inﬂuence. In the antibiotics example: if observing the administration
of a dose of 10mg of antibiotics returns a high probability of curing the infection,
this can be due to (a mix of) several reasons:
•
the causal inﬂuence of antibiotics on curing the infection,
•
the causal inﬂuence of curing the infection on antibiotics,
•
the causal inﬂuence of another variable on both antibiotics and curing the
infection, or,
•
the causal inﬂuence of both antibiotics and curing the infection on another
variable that we inadvertently condition on (i.e. selection bias).
Without extra information we cannot make the diﬀerence between these reasons.
On the other hand if we want to know whether administering a dose of 10mg
of antibiotics will cure the patients’ infection, we will need to isolate the causal
inﬂuence of antibiotics on curing the infection and this process is denoted by
causal inference.
Deﬁnition 4. Causal inference is the process of calculating the eﬀect of
manipulating some variables X on the probability distribution of some other
variables Y .
Deﬁnition 5. Conditioning by intervention or manipulation2 refers to
the way the distribution Y should be modiﬁed if we intervene externally and
force the value of X to be equal to x.
To make the distinction clear, Pearl has introduced the do-operator (Pearl,
2000)3:
P(Y = y|do(X = x))
(9.3)
The manipulations we are treating here are surgical in the sense that they only
directly change the variable of interest (X in the case of X = do(x)).
To reiterate, it is important to realize that conditioning by observation is
typically not the way the distribution of Y should be modiﬁed if we intervene
externally and force the value of X to be equal to x, as can be seen next:
2 Throughout this chapter the terms intervention and manipulation are used inter-
changeably.
3 In the literature other notations such as P(Y = y||X = x), PX=x(Y = y), or
P(Y = y|X = ˆx) are abundant.

9 Causal Graphical Models with Latent Variables: Learning and Inference
227
P(Y = y|do(X = x)) ̸= P(Y = y|X = x)
(9.4)
and the quantity on the left-hand side cannot be calculated from the joint prob-
ability distribution P(v) alone, without additional assumptions imposed on the
graph, i.e. that a directed edge represents an autonomous causal relation as in
CBNs.
Consider the simple CBNs of Figure 9.4 in the left graph
P(y|do(x)) = P(y|x)
as X is the only immediate cause of Y , but
P(x|do(y)) = P(x) ̸= P(x|y)
as there is no direct or indirect causal relation going from Y to X. The equalities
above are reversed in the graph to the right, i.e. there it holds that P(y|do(x)) =
P(y) ̸= P(y|x) and P(x|do(y)) = P(x|y).
X
Y
X
Y
Fig. 9.4. Two simple causal Bayesian networks
Next we introduce a theorem that speciﬁes how a manipulation modiﬁes the
JPD associated with a CBN.
Manipulation Theorem
Performing a manipulation in a domain that is modeled by a CBN, does modify
that domain and the JPD that is used to model it. Before introducing a theorem
that speciﬁes how a CBN and the JPD that is associated with it must be changed
to incorporate the change induced by a manipulation, we will oﬀer an intuitive
example.
Example 1. Imagine we want to disable the alarm in the system represented by
the CBN of Figure 9.5(a) by performing the manipulation do(alarm=oﬀ).
This CBN represents an alarm system against burglars, it can be triggered
by a burglary, an earthquake or both. Furthermore, the alarm going oﬀmight
cause the neighbors to call the owner at his work.
Such a manipulation changes the way in which the value of alarm is being
produced in the real world. Originally, the value of alarm was being decided by
its immediate causes in the model of Figure 9.5(a): burglary and earthquake.
After manipulating the alarm by disabling it, burglary and earthquake are no
longer the causes of the alarm, but have been replaced by the manipulation.
In Figure 9.5(b) the graph of the post-manipulation CBN is shown. There we
can see that the links between the original causes of alarm have been severed
and that the value of alarm has been instantiated to oﬀ.

228
P. Leray et al.
alarm
burglary
earthquake
John
calls
Mary
calls
do(alarm=off)
burglary
earthquake
John
calls
Mary
calls
(a)
(b)
Fig. 9.5. (a) A CBN of an alarm system. (b) The CBN of the alarm system of (a)
after disabling the alarm via an external manipulation: do(alarm=oﬀ).
To obtain the post-manipulation distribution after ﬁxing a set of variables
M ⊆V to ﬁxed values M = m, the factors with the variables in M conditional
on their parents in the graph (i.e. their causes in the pre-intervention distribu-
tion), have to be removed from the JPD. Formally these are : P(mi|Pa(mi))
for all variables Mi ∈M. This is because after the intervention, it is this in-
tervention rather than the parent variables in the graph that cause the values
of the variables in M. Furthermore the remaining occurrences of M in the JPD
have to be instantiated to M = m.
A manipulation of this type only has a local inﬂuence in the sense that only
the incoming links of a manipulated variable have to be removed from the model,
no factors representing other links have to be modiﬁed, except for instantiating
the occurrences of the manipulated variables M to m. This is a consequence
of the assumption of CBNs that the factors of the JPD represent assignment
processes that must stay invariant under variations in the processes governing
other variables. Formally, we get from (Spirtes et al., 2000):
Theorem 1. Given a CBN with variables V = V1, . . . , Vn and we perform the
manipulation M = m for a subset of variables M ⊆V , the post-manipulation
distribution becomes:
P(v|do(m)) =

Vi∈V \M
P(vi|Pa(vi))

M=m
(9.5)
Where |M=m stands for instantiating all the occurrences of the variables M to
values m in the equation that precedes it.
9.3
Causal Models with Latent Variables
In all the above we made the assumption of causal suﬃciency, i.e. that for
every variable of the domain that is a common cause, observational data can

9 Causal Graphical Models with Latent Variables: Learning and Inference
229
be obtained in order to learn the structure of the graph and the CPDs. Often
this assumption is not realistic, as it is not uncommon that a subset of all the
variables in the domain is never observed. We refer to such a variable as a latent
variable.
We start this section by brieﬂy discussing diﬀerent approaches to modeling
latent variables. After that we introduce two speciﬁc models for modeling latent
variables and the causal inﬂuences between the observed variables. These will
be the two main formalisms used in the rest of this chapter so we will discuss
their semantics and speciﬁcally their diﬀerences in a lot of detail.
9.3.1
Modeling Latent Variables
Consider the model in Figure 9.6(a), it is a problem with observable variables
V1, . . . , V6 and latent variables L1, L2 and it is represented by a directed acyclic
graph (DAG). As this DAG represents the actual problem henceforth we will
refer to it as the underlying DAG.
One way to represent such a problem is by using this DAG representation
and modeling the latent variables explicitly. Quantities for the observable vari-
ables can then be obtained from the data in the usual way. Quantities involving
latent variables however will have to be estimated. This involves estimating the
cardinality of the latent variables and this whole process can be diﬃcult and
lengthy. One of the techniques to learn models in such a way is the structural
EM algorithm (Friedman, 1997).
Another method to take into account latent variables in a model is by repre-
senting them implicitly. With that approach, no values have to be estimated for
the latent variables, instead their inﬂuence is absorbed in the distributions of the
observable variables. In this methodology, we only keep track of the position of
the latent variable in the graph if it would be modeled, without estimating values
for it. Both the modeling techniques that we will use in this chapter belong to
that approach, they will be described in the next two sections.
9.3.2
Semi-Markovian Causal Models
The central graphical modeling representation that we use are the semi-Markovian
causal models. They were ﬁrst used by Pearl (2000), and Tian and Pearl (2002a)
have developed causal inference algorithms for them.
Deﬁnitions
Deﬁnition 6. A semi-Markovian causal model (SMCM) is an acyclic causal
graph G with both directed and bi-directed edges. The nodes in the graph repre-
sent observable variables V = {V1, . . . , Vn} and the bi-directed edges implicitly
represent latent variables L = {L1, . . . , Ln′}.
See Figure 9.6(b) for an example SMCM representing the underlying DAG
in (a).

230
P. Leray et al.
V3
V1
V2
V5
V4
V6
L1
L2
V3
V1
V2
V5
V4
V6
(a)
(b)
V3
V1
V2
V5
V4
V6
(c)
Fig. 9.6. (a) A problem domain represented by a causal DAG model with observable
and latent variables. (b) A semi-Markovian causal model representation of (a). (c) A
maximal ancestral graph representation of (a).
The fact that a bi-directed edge represents a latent variable, implies that the
only latent variables that can be modeled by a SMCM can not have any parents
(i.e. is a root node) and has exactly two children that are both observed. This
seems very restrictive, however it has been shown that models with arbitrary
latent variables can be converted into SMCMs, while preserving the same inde-
pendence relations between the observable variables (Tian and Pearl, 2002b).
Semantics
In a SMCM, each directed edge represents an immediate autonomous causal
relation between the corresponding variables, just as was the case for causal
Bayesian networks.
In a SMCM, a bi-directed edge between two variables represents a latent
variable that is a common cause of these two variables.
The semantics of both directed and bi-directed edges imply that SMCMs are
not maximal, meaning that not all dependencies between variables are repre-
sented by an edge between the corresponding variables. This is because in a
SMCM an edge either represents an immediate causal relation or a latent com-
mon cause, and therefore dependencies due to a so called inducing path, will not
be represented by an edge.
Deﬁnition 7. An inducing path is a path in a graph such that each observable
non-endpoint node is a collider, and an ancestor of at least one of the endpoints.
Inducing paths have the property that their endpoints can not be separated by
conditioning on any subset of the observable variables. For instance, in Figure
9.6(a), the path V1 →V2 ←L1 →V6 is inducing.
Parametrisation
SMCMs cannot be parametrised in the same way as classical Bayesian networks
(i.e. by the set of CPTs P(Vi|Pa(Vi))), since variables that are connected via a
bi-directed edge have a latent variable as a parent.

9 Causal Graphical Models with Latent Variables: Learning and Inference
231
For example in Figure 9.6(b), choosing P(V5|V4) as a parameter to be asso-
ciated with variable V5 would only lead to erroneous results, as the dependence
with variable V6 via the latent variable L2 in the underlying DAG is ignored. As
mentioned before, using P(V5|V4, L2) as a parametrisation and estimating the
cardinality and the values for latent variable L2 would be a possible solution.
However we choose not to do this as we want to leave the latent variables implicit
for reasons of eﬃciency.
In (Tian and Pearl, 2002a), a factorisation of the joint probability distribution
over the observable variables of an SMCM was introduced. Later in this chapter
we will derive a representation for the probability distribution represented by a
SMCM based on that result.
Learning
In the literature no algorithm for learning the structure of an SMCM exists, in
this chapter we introduce techniques to perform that task, given some simplifying
assumptions, and with the help of experiments.
Probabilistic Inference
Since as of yet no eﬃcient parametrisation for SMCMs is provided in the liter-
ature, no algorithm for performing probabilistic inference exists. We will show
how existing probabilistic inference algorithms for Bayesian networks can be
used together with our parametrisation to perform that task.
Causal Inference
SMCMs are speciﬁcally suited for another type of inference, i.e. causal inference.
An example causal inference query in the SMCM of Figure 9.6(a) is P(V6 =
v6|do(V2 = v2)).
As seen before, causal inference queries are calculated via the Manipulation
Theorem, which speciﬁes how to change a joint probability distribution (JPD)
over observable variables in order to obtain the post-manipulation JPD. In-
formally, it says that when a variable X is manipulated to a ﬁxed value x, the
parents of variables X have to be removed by dividing the JPD by P(X|Pa(X)),
and by instantiating the remaining occurrences of X to the value x.
When all the parents of a manipulated variable are observable, this can always
be done. However, in a SMCM some of the parents of a manipulated variable
can be latent and then the Manipulation Theorem cannot be directly used to
calculate causal inference queries. Some of these causal quantities can be calcu-
lated in other ways but some cannot be calculated at all, because the SMCM
does not contain enough information.
When a causal query can be unambiguously calculated from a SMCM, we say
that it is identiﬁable. More formally:
Deﬁnition 8. The causal eﬀect of variable X on a variable Y is identiﬁable
from a SMCM with graph G if PM1(y|do(x)) = PM2(y|do(x)) for every pair of

232
P. Leray et al.
SMCMs M1 and M2 with PM1(v) = PM2(v) > 0 and GM1 = GM2, where PMi
and GMi respectively denote the probability distribution and graph associated with
the SMCM Mi.
In Pearl (2000), Pearl describes the do-calculus, a set of inference rules and
an algorithm that can be used to perform causal inference. More speciﬁcally,
the goal of do-calculus is to transform a mathematical expression including ma-
nipulated variables related to a SMCM into an equivalent expression involving
only standard probabilities of observed quantities. Recent work has shown that
do-calculus is complete (Huang and Valtorta, 2006; Shpitser and Pearl, 2006).
Tian and Pearl have introduced theoretical causal inference algorithms to per-
form causal inference in SMCMs (Pearl, 2000; Tian and Pearl, 2002a). However,
these algorithms assume the availability of a subset of all the conditional distri-
butions that can be obtained from the JPD over the observable variables. We
will show that with our representation these conditional distributions can be
obtained in an eﬃcient way in order to apply this algorithm.
9.3.3
Maximal Ancestral Graphs
Maximal ancestral graphs are another approach to modeling with latent variables
developed by Richardson and Spirtes (2002). The main research focus in that
area lies on learning the structure of these models and on representing exactly
all the independences between the observable variables of the underlying DAG.
Deﬁnitions
Ancestral graphs (AGs) are graphs that are complete under marginalisation and
conditioning. We will only discuss AGs without conditioning as is commonly
done in recent work (Zhang and Spirtes, 2005b; Tian, 2005; Ali et al., 2005).
Deﬁnition 9. An ancestral graph without conditioning is a graph with no
directed cycle containing directed →and bi-directed ↔edges, such that there is
no bi-directed edge between two variables that are connected by a directed path.
Deﬁnition 10. An ancestral graph is said to be a maximal ancestral graph
if, for every pair of non-adjacent nodes Vi, Vj there exists a set Z such that Vi
and Vj are d-separated given Z.
A non-maximal AG can be transformed into a unique MAG by adding some
bi-directed edges (indicating confounding) to the model. See Figure 9.6(c) for an
example MAG representing the same model as the underlying DAG in (a).
Semantics
In this setting a directed edge represents an ancestral relation in the underlying
DAG with latent variables. I.e. an edge from variable A to B represents that
in the underlying causal DAG with latent variables, there is a directed path
between A and B.

9 Causal Graphical Models with Latent Variables: Learning and Inference
233
Bi-directed edges represent a latent common cause between the variables. How-
ever, if there is a latent common cause between two variables A and B, and there
is also a directed path between A and B in the underlying DAG, then in the MAG
the ancestral relation takes precedence and a directed edge will be found between
the variables. V2 →V6 in Figure 9.6(c) is an example of such an edge.
Furthermore, as MAGs are maximal, there will also be edges between vari-
ables that have no immediate connection in the underlying DAG, but that are
connected via an inducing path. The edge V1 →V6 in Figure 9.6(c) is an example
of such an edge.
These semantics of edges make some causal inferences in MAGs impossible.
As we have discussed before the Manipulation Theorem states that in order to
calculate the causal eﬀect of a variable A on another variable B, the immediate
parents (i.e. the old causes) of A have to be removed from the model. However,
as opposed to SMCMs, in MAGs an edge does not necessarily represent an
immediate causal relationship, but rather an ancestral relationship and hence
in general the modeler does not know which are the real immediate causes of a
manipulated variable.
An additional problem for ﬁnding the original causes of a variable in MAGs
is that when there is an ancestral relation and a latent common cause between
variables, that the ancestral relation takes precedence and that the confounding
is absorbed in the ancestral relation.
Learning
There is a lot of recent research on learning the structure of MAGs from obser-
vational data. The Fast Causal Inference (FCI) algorithm (Spirtes et al., 1999),
is a constraint based learning algorithm. Together with the rules discussed in
Zhang and Spirtes (2005a), the result is a representation of the Markov equiva-
lence class of MAGs. This representative is referred to as a complete partial an-
cestral graph (CPAG) and in Zhang and Spirtes (2005a) it is deﬁned as follows:
Deﬁnition 11. Let [G] be the Markov equivalence class for an arbitrary MAG
G. The complete partial ancestral graph (CPAG) for [G], PG, is a graph
with possibly the following edges →, ↔, o−o, o→, such that
1. PG has the same adjacencies as G (and hence any member of [G]) does;
2. A mark of arrowhead (>) is in PG if and only if it is invariant in [G]; and
3. A mark of tail (−) is in PG if and only if it is invariant in [G].
4. A mark of (o) is in PG if not all members in [G] have the same mark.
In the next section we will discuss learning the structure in somewhat more
detail.
Parametrisation and Inference
At this time no parametrisation for MAGs with discrete variables exists that repre-
sents all the properties of a joint probability distribution, (Richardson and Spirtes,
2002), neither are there algorithms fo probabilistic inference.

234
P. Leray et al.
As mentioned above, due to the semantics of the edges in MAGs, not all causal
inferences can be performed. However, there is an algorithm due to Spirtes et al.
(2000) and reﬁned by Zhang (2006), for performing causal inference in some
restricted cases. More speciﬁcally, they consider a causal eﬀect to be identiﬁable
if it can be calculated from all the MAGs in the Markov equivalence class that
is represented by the CPAG and that quantity is equal for all those MAGs. This
severely restricts the causal inferences that can be made, especially if more than
conditional independence relations are taken into account during the learning
process, as is the case when experiments can be performed. In the context of
this causal inference algorithm, Spirtes et al. (2000) also discuss how to derive
a DAG that is a minimal I -map of the probability distribution represented by a
MAG.
In this chapter we introduce a similar procedure, but for a single SMCM
instead of for an entire equivalence class of MAGs. In that way a larger class of
causal inferences can be calculated, as the quantities do not have to be equal in
all the models of the equivalence class.
9.4
Structure Learning with Latent Variables
Just as learning a graphical model in general, learning a model with latent vari-
ables consists of two parts: structure learning and parameter learning. Both can
be done using data, expert knowledge and/or experiments. In this section we dis-
cuss structure learning and we diﬀerentiate between learning from observational
and experimental data.
9.4.1
From Observational Data
In order to learn graphical models with latent variables from observational data a
constraint based learning algorithm has been developed by Spirtes et al. (1999).
It is called the Fast Causal Inference (FCI) algorithm and it uses conditional
independence relations found between observable variables to learn a structure.
Recently this result has been extended with the complete tail augmentation
rules introduced in Zhang and Spirtes (2005a). The results of this algorithm is a
CPAG, representing the Markov equivalence class of MAGs consistent with the
data.
Recent work in the area consists of characterising the equivalence class of
CPAGs
and
ﬁnding
single-edge
operators
to
create
equivalent
MAGs
(Ali and Richardson, 2002; Zhang and Spirtes, 2005a,b). One of the goals of
these advances is to create methods that search in the space of Markov equiv-
alent models (CPAGs) instead of the space of all models (MAGs), mimicking
results in the case without latent variables (Chickering, 2002).
As mentioned before for MAGs, in a CPAG the directed edges have to be
interpreted as representing ancestral relations instead of immediate causal rela-
tions. More precisely, this means that there is a directed edge from Vi to Vj if Vi
is an ancestor of Vj in the underlying DAG and there is no subset of observable

9 Causal Graphical Models with Latent Variables: Learning and Inference
235
variables D such that (Vi⊥⊥Vj|D). This does not necessarily mean that Vi has
an immediate causal inﬂuence on Vj, it may also be a result of an inducing path
between Vi and Vj. For instance in Figure 9.6(c), the link between V1 and V6 is
present due to the inducing path V1, V2, L1, V6 shown in Figure 9.6(a).
Inducing paths may also introduce ↔, →, o→or o−o between two variables,
although there is no immediate inﬂuence in the form of an immediate causal
inﬂuence or latent common cause between the two variables. An example of
such a link is V3o−oV4 in Figure 9.7.
A consequence of these properties of MAGs and CPAGs is that they are
not very suited for general causal inference, since the immediate causal parents
of each observable variable are not available as is necessary according to the
manipulation theorem. As we want to learn models that can perform causal
inference, we will discuss how to transform a CPAG into a SMCM next.
9.4.2
From Experimental Data
As mentioned above, the result of current state-of-the-art techniques that learn
models with implicit latent variables from observational data is a CPAG. This
is a representative of the Markov equivalence class of MAGs. Any MAG in that
class will be able to represent the same JPD over the observable variables, but
not all those MAGs will have all edges with a correct causal orientation.
Furthermore as mentioned in the above, in MAGs the directed edges do not
necessarily have an immediate causal meaning as in CBNs or SMCMs, instead
they have an ancestral meaning. If it is your goal to perform causal inference, you
will need to know the immediate parents to be able to reason about all causal
queries. However, edges that are completely oriented but that do not have a
causal meaning will not occur in the CPAG, there they will always be of the
types o→or o−o, so orienting them in correct causal way way suﬃces.
Finally, MAGs are maximal, thus every missing edge must represent a con-
ditional independence. In the case that there is an inducing path between two
variables and no edge in the underlying DAG, the result of the current learn-
ing algorithms will be to add an edge between the variables. Again, although
these type of edges give the only correct representation of the conditional inde-
pendence relations in the domain, they do not represent an immediate causal
relation (if the inducing edge is directed) or a real latent common cause (if the
inducing edge is bi-directed). Because of this they could interfere with causal
inference algorithms, therefore we would like to identify and remove these type
of edges.
To recapitulate, the goal of techniques aiming at transforming a CPAG must
be twofold:
•
ﬁnding the correct causal orientation of edges that are not completely speci-
ﬁed by the CPAG (o→or o−o), and,
•
removing edges due to inducing paths.
In the next section we discuss how these goals can be obtained by performing
experiments.

236
P. Leray et al.
9.5
From CPAG to SMCM
Our goal is to transform a given CPAG in order to obtain a SMCM that corre-
sponds to the underlying DAG. Remember that in general there are four types
of edges in a CPAG: ↔, →, o→, o−o, in which o means either a tail mark −or a
directed mark >. As mentioned before, one of the tasks to obtain a valid SMCM
is to disambiguate those edges with at least one o as an endpoint. A second task
will be to identify and remove the edges that are created due to an inducing
path.
In the next section we will introduced some simplfying assumptions we have
to use in our work. Then we will discuss exactly which information is obtained
from performing an experiment. After that, we will discuss the two possible
incomplete edges: o→and o−o. Finally, we will discuss how we can ﬁnd edges
that are created due to inducing paths and how to remove them to obtain the
correct SMCM.
9.5.1
Assumptions
As is customary in the graphical modeling research area, the SMCMs we take
into account in this chapter are subject to some simplifying assumptions:
1. Stability, i.e. the independencies in the underlying CBN with observed and
latent variables that generates the data are structural and not due to several
inﬂuences exactly cancelling each other out (Pearl, 2000).
2. Only a single immediate connection per two variables in the underlying DAG.
I.e. we do not take into account problems where two variables that are con-
nected by an immediate causal edge are also confounded by a latent variable
causing both variables. Constraint based learning techniques such as IC*
(Pearl, 2000) and FCI (Spirtes et al., 2000) also do not explicitly recognise
multiple edges between variables. However, Tian and Pearl (2002a) presents
an algorithm for performing causal inference where such relations between
variables are taken into account.
3. No selection bias. Mimicking recent work, we do not take into account latent
variables that are conditioned upon, as can be the consequence of selection
eﬀects.
4. Discrete variables. All the variables in our models are discrete.
5. Correctness. The CPAG is correctly learned from data with the FCI algo-
rithm and the extended tail augmentation rules, i.e. each result that is found
is not due to a sampling error or insuﬃcient sample size.
9.5.2
Performing Experiments
The experiments discussed here play the role of the manipulations discussed in
Section 9.2.3 that deﬁne a causal relation. An experiment on a variable Vi, i.e. a
randomised controlled experiment, removes the inﬂuence of other variables in the
system on Vi. The experiment forces a distribution on Vi, and thereby changes the

9 Causal Graphical Models with Latent Variables: Learning and Inference
237
joint distribution of all variables in the system that depend directly or indirectly
on Vi but does not change the conditional distribution of other variables given
values of Vi. After the randomisation, the associations of the remaining variables
with Vi provide information about which variables Vi inﬂuences (Neapolitan,
2003). To perform the actual experiment we have to cut all inﬂuence of other
variables on Vi. Graphically this corresponds to removing all incoming arrows
into Vi from the underlying DAG.
We then measure the inﬂuence of the manipulation on variables of interest by
obtaining samples from their post-experimental distributions.
More precisely, to analyse the results of an experiment on a variable Vexp, we
compare for each variable of interest Vj the original observational sample data
Dobs with the post-experimental sample data Dexp. The experiment consists of
manipulating the variable Vexp to each of its values vexp a suﬃcient amount of
times in order to obtain sample data sets that are large enough to analyse in a
statistically sound way. The result of an experiment will be a data set of samples
for the variables of interest for each value i of variable Vexp = i, we will denote
such a data set by Dexp,i.
In order to see whether an experiment on Vexp made an inﬂuence on another
variable Vj, we compare each post-experimental data set Dexp,i with the original
observational data set Dobs (with a statistical test like χ2). Only if at least one of
the data sets is statistically signiﬁcantly diﬀerent, we can conclude that variable
Vexp causally inﬂuences variable Vj.
However, this inﬂuence does not necessarily have to be immediate between
the variables Vexp and Vj, but can be mediated by other variables, such as in
the underlying DAG: Vexp →Vmed →Vj.
In order to make the diﬀerence between a direct inﬂuence and a potentially
mediated inﬂuence via Vmed, we will no longer compare the complete data sets
Dexp,i and Dobs. Instead, we will divide both data sets in subsets based on the
values of Vmed, or in other words condition on variable Vmed. Then we compare
each of the smaller data sets Dexp,i|vmed and Dobs|vmed with each other and this
for all values of Vmed. By conditioning on a potentially mediating variable, we
block the causal inﬂuence that might go through that variable and we obtain
the immediate relation between Vexp and Vj.
Note that it might seem that if the mediating variable is a collider, this ap-
proach will fail, because conditioning on a collider on a path between two vari-
ables creates a dependence between those two variables. However, this approach
will still be valid and this is best understood with an example: imagine the un-
derlying DAG is of the form Vexp · · · →Vmed ←. . . Vj. In this case, when we
compare each Dexp,i and Dobs conditional on Vmed, we will ﬁnd no signiﬁcant
diﬀerence between both data sets, and this for all the values of Vmed. This is
because the dependence that is created between Vexp and Vj by conditioning
on the collider Vmed is present in both the original underlying DAG and in the
post-experimental DAG, and thus this is also reﬂected in the data sets Dexp,i
and Dobs.

238
P. Leray et al.
Table 9.1. An overview of how to complete edges of type o→
Ao→B Type 1(a)
Type 1(b)
Type 1(c)
Exper. exp(A) ̸⇝B exp(A) ⇝B
exp(A) ⇝B
result
̸ ∃p.d. path A  B ∃p.d. path A  B
(length ≥2)
(length ≥2)
Orient. A ↔B
A →B
Block all p.d. paths by
result
conditioning on block-
ing set Z:
exp(A)|Z ⇝B: A →B
exp(A)|Z ̸⇝B: A ↔B
In order not to overload that what follows with unnecessary complicated nota-
tion we will denote performing an experiment at variable Vi or a set of variables
W by exp(Vi) or exp(W) respectively, and if we have to condition on some other
set of variables Z on the data obtained by performing the experiment, we denote
it as exp(Vi)|Z and exp(W)|Z.
In general if a variable Vi is experimented on and another variable Vj is
aﬀected by this experiment, i.e. has another distribution after the experiment
than before, we say that Vj varies with exp(Vi), denoted by exp(Vi) ⇝Vj. If
there is no variation in Vj we note exp(Vi) ̸⇝Vj.
Before going to the actual solutions we have to introduce the notion of po-
tentially directed paths:
Deﬁnition 12. A potentially directed path (p.d. path) in a CPAG is a path
made only of edges of types o→and →, with all arrowheads in the same direction.
A p.d. path from Vi to Vj is denoted as Vi  Vj.
9.5.3
Solving o→
An overview of the diﬀerent rules for solving o→is given in Table 9.1.
For any edge Vio →Vj, there is no need to perform an experiment at Vj
because we know that there can be no immediate inﬂuence of Vj on Vi, so we
will only perform an experiment on Vi.
If exp(Vi) ̸⇝Vj, then there is no inﬂuence of Vi on Vj so we know that
there can be no directed edge between Vi and Vj and thus the only remaining
possibility is Vi ↔Vj (Type 1(a)).
If exp(Vi) ⇝Vj, then we know for sure that there is an inﬂuence of Vi on Vj,
we now need to discover whether this inﬂuence is immediate or via some inter-
mediate variables. Therefore we make a diﬀerence whether there is a potentially
directed (p.d.) path between Vi and Vj of length ≥2, or not. If no such path
exists, then the inﬂuence has to be immediate and the edge is found Vi →Vj
(Type 1(b)).
If at least one p.d. path Vi  Vj exists, we need to block the inﬂuence of those
paths on Vj while performing the experiment, so we try to ﬁnd a blocking set Z
for all these paths. If exp(Vi)|Z ⇝Vj, then the inﬂuence has to be immediate,

9 Causal Graphical Models with Latent Variables: Learning and Inference
239
Table 9.2. An overview of how to complete edges of type o−o
Ao−oB Type 2(a)
Type 2(b)
Type 2(c)
Exper. exp(A) ̸⇝B exp(A) ⇝B
exp(A) ⇝B
result
̸ ∃p.d. path A  B ∃p.d. path A  B
(length ≥2)
(length ≥2)
Orient. A ←oB
A →B
Block all p.d. paths by
result
(⇒Type 1)
conditioning on block-
ing set Z:
exp(A)|Z ⇝B: A →B
exp(A)|Z ̸⇝B: A ←oB
(⇒Type 1)
because all paths of length ≥2 are blocked, so Vi →Vj. On the other hand if
exp(Vi)|Z ̸⇝Vj, there is no immediate inﬂuence and the edge is Vi ↔Vj (Type
1(c)).
A blocking set Z consists of one variable for each p.d. path. This variable can
be chosen arbitrarily as we have explained before that conditioning on a collider
does not invalidate our experimental approach.
9.5.4
Solving o−o
An overview of the diﬀerent rules for solving o−o is given in Table 9.2.
For any edge Vio−oVj, we have no information at all, so we might need to
perform experiments on both variables.
If exp(Vi) ̸⇝Vj, then there is no inﬂuence of Vi on Vj so we know that there
can be no directed edge between Vi and Vj and thus the edge is of the following
form: Vi ←oVj, which then becomes a problem of Type 1.
If exp(Vi) ⇝Vj, then we know for sure that there is an inﬂuence of Vi on
Vj, and like with Type 1(b) we make a diﬀerence whether there is a potentially
directed path between Vi and Vj of length ≥2, or not. If no such path exists,
then the inﬂuence has to be immediate and the edge becomes Vi →Vj.
If at least one p.d. path Vi  Vj exists, we need to block the inﬂuence of
those paths on Vj while performing the experiment, so we ﬁnd a blocking set Z
like with Type 1(c). If exp(Vi)|Z ⇝Vj, then the inﬂuence has to be immediate,
because all paths of length ≥2 are blocked, so Vi →Vj. On the other hand if
exp(Vi)|Z ̸⇝Vj, there is no immediate inﬂuence and the edge is of the following
form: Vi ←oVj, which again becomes a problem of Type 1.
9.5.5
Removing Inducing Path Edges
In the previous phase only o-parts of edges of a CPAG have been oriented. The
graph that is obtained in this way can contain both directed and bi-directed
edges, each of which can be of two types. For the directed edges:
•
an immediate causal edge that is also present in the underlying DAG
•
an edge that is due to an inducing path in the underlying DAG.

240
P. Leray et al.
V1
V2
V3
V4
V1
V2
V3
V4
(a)
(b)
Fig. 9.7. (a) A SMCM. (b) Result of FCI, with an i-false edge V3o−oV4.
For the bi-directed edges:
•
an edge that represents a latent variable in the underlying DAG
•
an edge that is due to an inducing path in the underlying DAG.
When representing the same underlying DAG, a SMCM and the graph obtained
after orienting all unknown endpoints of the CPAG have the same connections
except for edges due to inducing paths in the underlying DAG, these edges are
only represented in the experimentally oriented graph.
Deﬁnition 13. We will call an edge between two variables Vi and Vj i-false if it
was created due to an inducing path, i.e. because the two variables are dependent
conditional on any subset of observable variables.
For instance in Figure 9.6(a), the path V1, V2, L1, V6 is an inducing path,
which causes the FCI algorithm to ﬁnd an i-false edge between V1 and V6, see
Figure 9.6(c). Another example is given in Figure 9.7 where the SMCM is given
in (a) and the result of FCI in (b). The edge between V3 and V4 in (b) is a
consequence of the inducing path through the observable variables V3, V1, V2, V4.
In order to be able to apply a causal inference algorithm we need to remove
all i-false edges from the learned structure. The substructures that can indicate
this type of edges can be identiﬁed by looking at any two variables that a) are
connected by an edge, and, b) have at least one inducing path between them.
To check whether the immediate connection needs to be present we have to
block all inducing paths by performing one or more experiments on an inducing
path blocking set (i-blocking set) Zip and block all other open paths by con-
ditioning on a blocking set Z. Note that the set of variables Zip are the set of
variables which get an assigned value during the experiments, the set of vari-
ables Z are used when looking for independences in the interventional data. If
Vi and Vj are dependent, i.e. (Vi
⊭
Vj), under these circumstances then the edge
is correct and otherwise it can be removed.
In the example of Figure 9.6(c), we can block the inducing path by performing
an experiment on V2, and hence can check that V1 and V6 do not covary with
each other in these circumstances, so the edge can be removed.
An i-blocking set consists of a collider on each of the inducing paths connecting
the two variables of interest. Here a blocking set Z is a set of variables that blocks
each of the other open paths between the two variables of interest.
Table 9.3 gives an overview of the actions to resolve i-false edges.

9 Causal Graphical Models with Latent Variables: Learning and Inference
241
Table 9.3. Removing i-false edges
Given A MAG with a pair of connected variables Vi, Vj,
and a set of inducing paths Vi, . . . , Vj
Action Block all inducing paths Vi, . . . , Vj by performing exper-
iments on i-blocking set Zip.
Block all other open paths between Vi and Vj by condition-
ing on blocking set Z.
When performing all exp(Zip)|Z:
if (Vi
⊭
Vj): - confounding is real
- else remove edge between Vi, Vj
9.5.6
Example
We will demonstrate a number of steps to discover the completely oriented
SMCM (Figure 9.6(b)) based on the result of the FCI algorithm applied on
observational data generated from the underlying DAG in Figure 9.6(a). The
result of the FCI algorithm can be seen in Figure 9.8(a). We will ﬁrst resolve
problems of Type 1 and 2, and then remove i-false edges. The result of each step
is explained in Table 9.4 and indicated in Figure 9.8.
V3
V1
V2
V5
V4
V6
V3
V1
V2
V5
V4
V6
V3
V1
V2
V5
V4
V6
(b)
(c)
(d)
(e)
V3
V1
V2
V5
V4
V6
V3
V1
V2
V5
V4
V6
(f)
V3
V1
V2
V5
V4
V6
(a)
Fig. 9.8. (a) The result of FCI on data of the underlying DAG of Figure 9.6(a).
(b) Result of an experiment at V5. (c) Result after experiment at V4. (d) Result after
experiment at V3. (e) Result after experiment at V2 while conditioning on V3. (f) Result
of resolving all problems of Type 1 and 2.

242
P. Leray et al.
Table 9.4. Example steps in disambiguating edges by performing experiments
Exper. Edge
Experiment
Edge
Type
before
result
after
exp(V5) V5o−oV4 exp(V5) ̸⇝V4
V5 ←oV4 Type 2(a)
V5o→V6 exp(V5) ̸⇝V6
V5 ↔V6 Type 1(a)
exp(V4) V4o−oV2 exp(V4) ̸⇝V2
V4 ←oV2 Type 2(a)
V4o−oV3 exp(V4) ̸⇝V3
V4 ←oV3 Type 2(a)
V4o→V5 exp(V4) ⇝V5
V4 →V5 Type 1(b)
V4o→V6 exp(V4) ⇝V6
V4 →V6 Type 1(b)
exp(V3) V3o−oV2 exp(V3) ̸⇝V2
V3 ←oV2 Type 2(a)
V3o→V4 exp(V3) ⇝V4
V3 →V4 Type 1(b)
exp(V2) V2o−oV1 exp(V2) ̸⇝V1
V2 ←oV1 Type 2(a)
V2o→V3 exp(V2) ⇝V3
V2 →V3 Type 1(b)
V2o→V4 exp(V2)|V3 ⇝V4 V2 →V4 Type 1(c)
After resolving all problems of Type 1 and 2 we end up with the structure
shown in Figure 9.8(f), this representation is no longer consistent with the MAG
representation since there are bi-directed edges between two variables on a di-
rected path, i.e. V2, V6. However, this structure is not necessarily a SMCM yet,
as there is a potentially i-false edge V1 ↔V6 in the structure with inducing path
V1, V2, V6, so we need to perform an experiment on V2, blocking all other paths
between V1 and V6 (this is also done by exp(V2) in this case). Given that the
original structure is as in Figure 9.6(a), performing exp(V2) shows that V1 and
V6 are independent, i.e. exp(V2) : (V1⊥⊥V6). Thus the bi-directed edge between
V1 and V6 is removed, giving us the SMCM of Figure 9.6(b).
9.6
Parametrisation of SMCMs
As mentioned before, in his work on causal inference, Tian provides an algorithm
for performing causal inference given knowledge of the structure of an SMCM and
the joint probability distribution (JPD) over the observable variables. However, a
parametrisation to eﬃciently store the JPD over the observables is not provided.
We start this section by discussing the factorisation for SMCMs introduced in
Tian and Pearl (2002a). From that result we derive an additional representation
for SMCMs and a parametrisation of that representation that facilitates proba-
bilistic and causal inference. We will also discuss how these parameters can be
learned from data.
9.6.1
Factorising with Latent Variables
Consider an underlying DAG with observable variables V = {V1, . . . , Vn} and
latent variables L = {L1, . . . , Ln′}. Then the joint probability distribution can
be written as the following mixture of products:

9 Causal Graphical Models with Latent Variables: Learning and Inference
243
P(v) =

{lk|Lk∈L}

Vi∈V
P(vi|Pa(vi), LPa(vi))

Lj∈L
P(lj),
(9.6)
where LPa(vi) are the latent parents of variable Vi and Pa(vi) are the observable
parents of Vi.
Remember that in a SMCM the latent variables are implicitly represented by
bi-directed edges, then consider the following deﬁnition.
Deﬁnition 14. In a SMCM, the set of observable variables can be partitioned
into disjoint groups by assigning two variables to the same group iﬀthey are
connected by a bi-directed path. We call such a group a c-component (from
”confounded component”) (Tian and Pearl, 2002a).
E.g. in Figure 9.6(b) variables V2, V5, V6 belong to the same c-component. Then it
can be readily seen that c-components and their associated latent variables form
respective partitions of the observable and latent variables. Let Q[Si] denote the
contribution of a c-component with observable variables Si ⊂V to the mixture
of products in equation 9.6. Then we can rewrite the JPD as follows:
P(v) =

i∈{1,...,k}
Q[Si]
(9.7)
Finally, Tian and Pearl (2002a) proved that each Q[S] could be calculated
as follows. Let Vo1 < . . . < Von be a topological order over V , and let V (i) =
{Vo1, . . . , Voi}, i = 1, . . . , n and V (0) = ∅.
Q[S] =

Vi∈S
P(vi|(Ti ∪Pa(Ti))\{Vi})
(9.8)
where Ti is the c-component of the SMCM G reduced to variables V (i), that
contains Vi. The SMCM G reduced to a set of variables V ′ ⊂V is the graph
obtained by removing all variables V \V ′ from the graph and the edges that are
connected to them.
In the rest of this section we will develop a method for deriving a DAG from a
SMCM. We will show that the classical factorisation  P(vi|Pa(vi)) associated
with this DAG, is the same as the one that is associated with the SMCM as
above.
9.6.2
Parametrised Representation
Here we ﬁrst introduce an additional representation for SMCMs, then we show
how it can be parametrised and ﬁnally, we discuss how this new representation
could be optimised.
PR-representation
Consider Vo1 < . . . < Von to be a topological order O over the observable vari-
ables V , and let V (i) = {Vo1, . . . , Voi}, i = 1, . . . , n and V (0) = ∅. Then Table 9.5

244
P. Leray et al.
Table 9.5. Obtaining the parametrised representation from a SMCM
Given a SMCM G and a topological order O,
the PR-representation has these properties:
1. The nodes are V , the observable variables of the SMCM.
2. The directed edges that are present in the SMCM are also
present in the PR-representation.
3. The bi-directed edges in the SMCM are replaced by a number
of directed edges in the following way:
Add an edge from node Vi to node Vj iﬀ:
a) Vi ∈(Tj ∪Pa(Tj)), where Tj is the c-component of G
reduced to variables V (j) that contains Vj,
b) except if there was already an edge between nodes Vi and Vj.
V3
V1
V2
V5
V4
V6
V1,V2,V4,V5,V6
V2,V3,V4
V 2,V4
(a)
(b)
Fig. 9.9. (a) The PR-representation applied to the SMCM of Figure 9.6(b). (b) Junc-
tion tree representation of the DAG in (a).
shows how the parametrised (PR-) representation can be obtained from the
original SMCM structure.
What happens is that each variable becomes a child of the variables it would
condition on in the calculation of the contribution of its c-component as in
Equation (9.8).
In Figure 9.9(a), the PR-representation of the SMCM in Figure 9.6(a) can be
seen. The topological order that was used here is V1 < V2 < V3 < V4 < V5 < V6
and the directed edges that have been added are V1 →V5, V2 →V5, V1 →V6,
V2 →V6, and, V5 →V6.
The resulting DAG is an I -map (Pearl, 1988), over the observable variables
of the independence model represented by the SMCM. This means that all the
independencies that can be derived from the new graph must also be present
in the JPD over the observable variables. This property can be more formally
stated as the following theorem.
Theorem 2. The PR-representation PR derived from a SMCM S is an I-map
of that SMCM.

9 Causal Graphical Models with Latent Variables: Learning and Inference
245
Proof. Proving that PR is an I -map of S amounts to proving that all indepen-
dences represented in PR (A) imply an independence in S (B), or A ⇒B. We
will prove that assuming both A and ¬B leads to a contradiction.
Assumption ¬B: consider that two observable variables X and Y are depen-
dent in the SMCM S conditional on some (possible empty) set of observable
variables Z: X
⊭
SY |Z.
Assumpion A: consider that X and Y are independent in PR conditional on
Z: X⊥⊥P RY |Z.
Then based on X
⊭
SY |Z we can discriminate two general cases:
1. ∃a path C in S connecting variables X and Y that contains no colliders and
no elements of Z.
2. ∃a path C in S connecting variables X and Y that contains at least one
collider Zi that is an element of Z. For the collider there are three possibil-
ities:
a) X . . . Ci →Zi ←Cj . . . Y
b) X . . . Ci ↔Zi ←Cj . . . Y
c) X . . . Ci ↔Zi ↔Cj . . . Y
Now we will show that each case implies ¬A:
1. Transforming S into PR only adds edges and transforms double-headed
edges into single headed edges, hence the path C is still present in S and it
still contains no collider. This implies that X⊥⊥P RY |Z is false.
2.
a) The path C is still present in S together with the collider in Zi, as it has
single headed incoming edges. This implies that X⊥⊥P RY |Z is false.
b) The path C is still present in S. However, the double-headed edge is
transformed into a single headed edge. Depending on the topological
order there are two possibilities:
•
Ci →Zi ←Cj: in this case the collider is still present in PR, this
implies that X
⊭
P RY |Z
•
Ci ←Zi ←Cj: in this case the collider is no longer present, but in
PR there is the new edge Ci ←Cj and hence X
⊭
P RY |Z
c) The path C is still present in S. However, both double-headed edges
are transformed into single headed edges. Depending on the topological
order there are several possibilities. For the sake of brievity we will only
treat a single order here, for the others it can easily be checked that the
same holds.
If the order is Ci < Zi < Cj, the graph becomes Ci →Zi →Cj, but there
are also edges from Ci and Zi to Cj and its parents Pa(Cj). Thus the
collider is no longer present, but the extra edges ensure that X
⊭
P RY |Z.
This implies that X⊥⊥P RY |Z is false and therefore we can conclude that PR is
always an I -map of S under our assumptions.
⊓⊔
Parametrisation
For this DAG we can use the same parametrisation as for classical BNs, i.e.
learning P(vi|Pa(vi)) for each variable, where Pa(vi) denotes the parents in the

246
P. Leray et al.
new DAG. In this way the JPD over the observable variables factorises as in
a classical BN, i.e. P(v) =  P(vi|Pa(vi)). This follows immediately from the
deﬁnition of a c-component and from Equation (9.8).
Optimising the Parametrisation
Remark that the number of edges added during the creation of the PR-
representation depends on the topological order of the SMCM.
As this order is not unique, giving precedence to variables with a lesser amount
of parents, will cause less edges to be added to the DAG. This is because added
edges go from parents of c-component members to c-component members that
are topological descendants.
By choosing an optimal topological order, we can conserve more conditional
independence relations of the SMCM and thus make the graph more sparse,
leading to a more eﬃcient parametrisation.
Note that the choice of the topological order does not inﬂuence the correctness
of the representation, Theorem 2 shows that it will always be an I -map.
Learning Parameters
As the PR-representation of SMCMs is a DAG as in the classical Bayesian
network formalism, the parameters that have to be learned are P(vi|Pa(vi)).
Therefore, techniques such as ML and MAP estimation (Heckerman, 1995) can
be applied to perform this task.
9.6.3
Probabilistic Inference
Two of the most famous existing probabilistic inference algorithms for models
without latent variables are the λ−π algorithm (Pearl, 1988) for tree-structured
BNs, and the junction tree algorithm (Lauritzen and Spiegelhalter, 1988) for
arbitrary BNs.
These techniques cannot immediately be applied to SMCMs for two reasons.
First of all until now no eﬃcient parametrisation for this type of models was
available, and secondly, it is not clear how to handle the bi-directed edges that
are present in SMCMs.
We have solved this problem by ﬁrst transforming the SMCM to its PR-
representation which allows us to apply the junction tree (JT) inference al-
gorithm. This is a consequence of the fact that, as previously mentioned, the
PR-representation is an I -map over the observable variables. And as the JT al-
gorithm only uses independencies in the DAG, applying it to an I -map of the
problem gives correct results. See Figure 9.9(b) for the junction tree obtained
from the parametrised representation in Figure 9.9(a).
Note that any other classical probabilistic inference technique that only
uses conditional independencies between variables could also be applied to the
PR-representation.

9 Causal Graphical Models with Latent Variables: Learning and Inference
247
9.6.4
Causal Inference
In Tian and Pearl (2002a), an algorithm for performing causal inference was
developed, however as mentioned before they have not provided an eﬃcient
parametrisation.
In Spirtes et al. (2000); Zhang (2006), a procedure is discussed that can identify
a limited amount of causal inference queries. More precisely only those whose result
is equal for all the members of a Markov equivalence class represented by a CPAG.
In Richardson and Spirtes (2003), causal inference in AGs is shown on an
example, but a detailed approach is not provided and the problem of what to
do when some of the parents of a variable are latent is not solved.
By deﬁnition in the PR-representation, the parents of each variable are exactly
those variables that have to be conditioned on in order to obtain the factor of that
variable in the calculation of the c-component, see Table 9.5 and Tian and Pearl
(2002a). Thus, if we want to apply Tian’s causal inference algorithm, the PR-
representation provides all the necessary quantitative information, while the orig-
inal structure of the SMCM provides the necessary structural information.
9.7
Conclusions and Perspectives
In this chapter we have introduced techniques for causal graphical modeling
with latent variables. We have discussed all classical steps in a modeling process
such as learning the structure from observational and experimental data, model
parametrisation, probabilistic and causal inference.
More precisely we showed that there is a big gap between the models that
can be learned from data alone and the models that are used in causal inference
theory. We showed that it is important to retrieve the fully oriented structure of
a SMCM, and discussed how to obtain this from a given CPAG by performing
experiments.
As the experimental learning approach relies on randomized controlled exper-
iments, in general it is not scalable to problems with a large number of variables,
due to the associated large number of experiments. Furthermore, it cannot be
applied in application areas where such experiments are not feasible due to prac-
tical or ethical reasons.
For future work we would like to relax the assumptions made in this chapter.
First of all we want to study the implications of allowing two types of edges between
two variables, i.e. confounding as well as a immediate causal relationship. Another
direction for possible future work would be to study the eﬀect of allowing multiple
joint experiments in other cases than when removing inducing path edges.
Furthermore, we believe that applying the orientation and tail augmentation
rules of Zhang and Spirtes (2005a) after each experiment, might help to reduce
the number of experiments needed to fully orient the structure. In this way we
could extend our previous results (Meganck et al., 2006) on minimising the total
number of experiments in causal models without latent variables, to SMCMs.
This allows to compare practical results with the theoretical bounds developed
in Eberhardt et al. (2005).

248
P. Leray et al.
SMCMs have not been parametrised in another way than by the entire joint
probability distribution, we showed that using an alternative representation,
we can parametrise SMCMs in order to perform probabilistic as well as causal
inference. Furthermore this new representation allows to learn the parameters
using classical methods.
We have informally pointed out that the choice of a topological order, when
creating the PR-representation, inﬂuences the size and thus the eﬃciency of the
PR-representation. We would like to investigate this property in a more formal
manner. Finally, we have started implementing the techniques introduced in this
chapter into the structure learning package (SLP)4 of the Bayesian networks
toolbox (BNT)5 for MATLAB.
Acknowledgements
This work was partially funded by a IWT-scholarship. This work was partially
supported by the IST Programme of the European Community, under the PAS-
CAL network of Excellence, IST-2002-506778. This publication only reﬂects the
authors’ views.
References
Ali, A., Richardson, T.: Markov equivalence classes for maximal ancestral graphs. In:
Proc. of the 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 1–9
(2002)
Ali, A.R., Richardson, T., Spirtes, P., Zhang, J.: Orientation rules for constructing
markov equivalence classes of maximal ancestral graphs. Technical Report 476, Dept.
of Statistics, University of Washington (2005)
Chickering, D.: Learning equivalence classes of Bayesian-network structures. Journal
of Machine Learning Research 2, 445–498 (2002)
Cooper, G.F., Yoo, C.: Causal discovery from a mixture of experimental and obser-
vational data. In: Proceedings of Uncertainty in Artiﬁcial Intelligence, pp. 116–125
(1999)
Eberhardt, F., Glymour, C., Scheines, R.: On the number of experiments suﬃcient
and in the worst case necessary to identify all causal relations among n variables.
In: Proc. of the 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
178–183 (2005)
Friedman, N.: Learning belief networks in the presence of missing values and hidden
variables. In: Proc. of the 14th International Conference on Machine Learning, pp.
125–133 (1997)
Heckerman, D.: A tutorial on learning with bayesian networks. Technical report, Mi-
crosoft Research (1995)
Huang, Y., Valtorta, M.: Pearl’s calculus of intervention is complete. In: Proc. of the
22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 217–224 (2006)
Jordan, M.I. (ed.): Learning in Graphical Models. MIT Press, Cambridge (1998)
4 http://banquiseasi.insa-rouen.fr/projects/bnt-slp/
5 http://www.cs.ubc.ca/˜murphyk/Software/BNT/bnt.html

9 Causal Graphical Models with Latent Variables: Learning and Inference
249
Jordan, M.I., Ghahramani, Z., Jaakkola, T., Saul, L.K.: An introduction to variational
methods for graphical models. Machine Learning 37(2), 183–233 (1999)
Lauritzen, S.L., Spiegelhalter, D.J.: Local computations with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical
Society, series B 50, 157–244 (1988)
Mackay, D.: Introduction to monte carlo methods. In: Jordan, M.I. (ed.) Learning in
Graphical Models, pp. 175–204. MIT Press, Cambridge (1999)
Meganck, S., Leray, P., Manderick, B.: Learning causal bayesian networks from obser-
vations and experiments: A decision theoretic approach. In: Modeling Decisions in
Artiﬁcial Intelligence. LNCS, pp. 58–69 (2006)
Murphy, K.P.: Active learning of causal bayes net structure. Technical report, Depart-
ment of Computer Science, UC Berkeley (2001)
Neapolitan, R.: Learning Bayesian Networks. Prentice Hall, Englewood Cliﬀs (2003)
Pearl, J.: Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San Fran-
cisco (1988)
Pearl, J.: Causality: Models, Reasoning and Inference. MIT Press, Cambridge (2000)
Richardson, T., Spirtes, P.: Ancestral graph markov models. Technical Report 375,
Dept. of Statistics, University of Washington (2002)
Richardson, T., Spirtes, P.: Causal inference via Ancestral graph models. In: Highly
Structured Stochastic Systems. Oxford Statistical Science Series, ch. 3. Oxford Uni-
versity Press, Oxford (2003)
Russell, S.J., Norvig, P. (eds.): Artiﬁcial Intelligence: A Modern Approach. Prentice
Hall, Englewood Cliﬀs (1995)
Shpitser, I., Pearl, J.: Identiﬁcation of conditional interventional distributions. In: Proc.
of the 22nd Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 437–444
(2006)
Spirtes, P., Glymour, C., Scheines, R.: Causation, Prediction and Search. MIT Press,
Cambridge (2000)
Spirtes, P., Meek, C., Richardson, T.: An algorithm for causal inference in the presence
of latent variables and selection bias. In: Computation, Causation, and Discovery,
pp. 211–252. AAAI Press, Menlo Park (1999)
Tian, J.: Generating markov equivalent maximal ancestral graphs by single edge re-
placement. In: Proc. of the 21st Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pp. 591–598 (2005)
Tian, J., Pearl, J.: On the identiﬁcation of causal eﬀects. Technical Report (R-290-L),
UCLA C.S. Lab (2002a)
Tian, J., Pearl, J.: On the testable implications of causal models with hidden variables.
In: Proc. of the 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
519–527 (2002b)
Tong, S., Koller, D.: Active learning for structure in bayesian networks. In: Seventeenth
International Joint Conference on Artiﬁcial Intelligence (2001)
Zhang, J.: Causal Inference and Reasoning in Causally Insuﬃcient Systems. PhD thesis,
Carnegie Mellon University (2006)
Zhang, J., Spirtes, P.: A characterization of markov equivalence classes for ancestral
graphical models. Technical Report 168, Dept. of Philosophy, Carnegie-Mellon Uni-
versity (2005a)
Zhang, J., Spirtes, P.: A transformational characterization of markov equivalence for
directed acyclic graphs with latent variables. In: Proc. of the 21st Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 667–674 (2005b)

10
Use of Explanation Trees to Describe
the State Space of a Probabilistic-Based
Abduction Problem⋆
M. Julia Flores1, Jos´e A. G´amez1, and Seraf´ın Moral2
1 Computing Systems Department & SIMD - i3A
University of Castilla-La Mancha
Campus Universitario s/n. Albacete. 02071
{julia,jgamez}@dsi.uclm.es
2 Departamento de Ciencias de la Computaci´on e I. A.
Universidad de Granada
Granada, 18071, Spain
smc@decsai.ugr.es
Abstract. This chapter presents a new approach to the problem of obtaining the most
probable explanations given a set of observations in a Bayesian network. The method
provides a set of possibilities ranked by their probabilities. The main novelties are that
the level of detail of each one of the explanations is not uniform (with the idea of
being as simple as possible in each case), the explanations are mutually exclusive, and
the number of required explanations is not ﬁxed (it depends on the particular case
we are solving). Our goals are achieved by means of the construction of the so called
explanation tree which can have asymmetric branching and that will determine the
diﬀerent possibilities. This chapter describes the procedure for its computation based
on information theory criteria and shows its behaviour in some examples.
To test the procedure we have used a couple of examples that can be intuitively
interpreted and understood. Moreover, we have carried out a set of experiments to
make a comparison with other existing abductive techniques that were designed with
goals similar to those we pursue.
10.1
Abductive Inference in Bayesian Networks. Total
and Partial Abduction Techniques
When dealing with inference in Bayesian networks, the most typical and classical
approach is to compute the posterior probability distributions of the variables
when some evidence has been observed. This is indeed the most common proba-
bilistic inference task in Bayesian networks (BNs) for which normally probability
or evidence propagation [1, 2, 3] is used. In this case, the computation of the
posterior probability for each non-observed variables given a set of observations
(XO = xO) (the evidence) is the main goal:
P(Xi|XO = xO) ∀Xi ∈XI
(10.1)
⋆This chapter is an extended version of [27].
D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 251–280, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

252
M.J. Flores, J.A. G´amez, and S. Moral
This is normally known as evidence propagation or probability propagation. We
call evidence those facts which are observed and therefore the involved variables
are ﬁxed to a certain value each one.
But this is not the only existing problem we can solve with BNs computation,
and there are also other interesting inference tasks. In this category we can ﬁnd
abductive reasoning, which represents another type of propagation and has also
a great relevance within this BNs ﬁeld. Abduction is a method for inference used
for ﬁnding/generating explanations to some observed facts. Generating explana-
tions in Bayesian networks can be understood in two (main) diﬀerent ways:
1. Explaining the reasoning process (see [4] for a review). That is, trying to
justify how a conclusion was obtained, why new information was asked, etc.
2. Diagnostic explanations or abductive inference (see [5] for a review). In this
case the explanation reduces to factual information about the state of the
world, and the best explanation for a given evidence is the state of the world
(conﬁguration) that is the most probable given the evidence [1].
In this case we will focus on the second approach. Therefore, given a set of
observations or evidence (XO = xO or xO in short) known as the explanandum,
we aim to obtain the best conﬁguration of values for the explanatory variables
(the explanation) which is consistent with the explanandum and which could be
assumed to predict it.
As the abductive task is mostly considered as the search of explanations its
major application has been done for diagnosis and analysis problems [6, 7], where
medical diagnosis stands out especially. In the last years various authors have
directed their research endeavours to the study of performing abductive inference
for the formalism of Bayesian networks. Two main abductive tasks in BNs are
identiﬁed:
–
Most Probable Explanation (MPE) or total abduction. In this case all the un-
observed variables (XU) are included in the explanation [1]. The best explana-
tion is the assignment XU = x∗
U which has maximum a posteriori probability
given the explanandum, i.e.,
x∗
U = arg
max
xU ∈ΩXU
P(xU|xO).
(10.2)
Searching for the best explanation has the same complexity (NP-hard [8])
as probability propagation, in fact the best MPE can be obtained by using
probability propagation algorithms but replacing summation by maximum
in the marginalisation operator [9]. However, as it is expected to have several
competing hypothesis accounting for the explanandum, our goal usually is to
get the K best MPEs. Nilsson [10] showed that using algorithm in [9] only the
ﬁrst three MPEs can be correctly identiﬁed, and proposed a clever method
to identify the remaining (4, . . . , K) explanations. This kind of problems has
been studied by several authors who developed exact algorithms [11, 12] and
there are also works using approximate methods [13] in order to solve them.

10 Use of Explanation Trees to Describe the State Space
253
Nilsson proved that under certain assumptions, his algorithm to obtain K-
MPEs has a similar complexity to the computation of posterior probabilities
by Hugin algorithm [10, 14]
One of the main drawbacks of the MPE deﬁnition is that as it produces
complete assignments, the explanations obtained can exhibit the overspeci-
ﬁcation problem [15] because some non-relevant variables have been used as
explanatory.
–
Maximum a Posteriori Assignment (MAP) or partial abduction [16, 15]. The
goal of this task is to alleviate the overspeciﬁcation problem by considering
as target variables only a subset of the unobserved variables called the ex-
planation set (XE). Sometimes certain variables clearly have no explanatory
value. This could be the case in a network that represents a car, where for the
observation xO ={”car does not start“} one conﬁguration such as ”radio does
not work“ is not explanatory, since the status of the radio will not probably
be an answer by itself to the observed fault. Others will be just intermediate
nodes, let us imagine a network modelling the ﬁnal mark of a student. We
could have an intermediate node, again not explanatory by itself, which is
a combination of (=whose parents are) the really observable variables that
express Theory and Practice marks. In general there could be variables use-
less in an explanation because they simply not give any information from an
abductive point of view or we are just not interested in them.
Then, we look for the maximum a posteriori assignment only for those
variables given the explanandum, i.e.,
x∗
E = arg max
xE P(xE|xO) = arg max
xE

xR
P(xE, xR|xO),
(10.3)
where XR = XU \ XE.
In principle it could seem that the solution for this problem would arise
by means of an adaptation of those methods developed for total abduction.
However, a deeper study of the problem reveals that in general this does not
hold, and moreover those resolution methods for partial abduction will be
generally less eﬃcient than total abduction. Therefore, algorithms are almost
always approximate (based on search methods). This problem is therefore
more complex than the MPE problem, because it can be NP-hard even for
cases in which MPE is polynomial (e.g., polytrees) [17, 18], although recently
Park and Darwiche [19, 17] have proposed exact and approximate algorithms
to enlarge the class of eﬃciently solved cases. With respect to looking for
the K best explanations, exact and approximate algorithms which combine
Nilsson algorithm [10] with probability trees [20] have been proposed in [21].
In general, we could say that even being a more interesting‘problem partial
abduction has been less studied than total abduction [22].
The problem in both cases is set out as the search of the instantiation val-
ues for all (total) or a subset of (partial) non-observed variables in such a way
that the joint probability of the conﬁguration is maximum. Besides, since it is

254
M.J. Flores, J.A. G´amez, and S. Moral
not generally enough by ﬁnding the best explanation, but we need the K best
explanations, this K number is usually added to the problem statement.
The question now is which variables should be included in the explanation
set. Many algorithms avoid this problem by assuming that the explanation set is
provided as an input, e.g., given by the experts or users. Many others interpret
the BN as a causal one and only ancestors of the explanandum are allowed to
be included in the explanation set (sometimes only root nodes are considered)
[11]. However, including all the ancestors in the explanation set does not seem
to avoid the overspeciﬁcation problem and even so, what happens if the network
does not have a causal interpretation?, e.g., it has been learnt from a data base
or it represents an agent’s beliefs [23]. Shimony [15, 24] goes one step further
and describes a method which tries to identify the relevant variables (among
the explanandum ancestors) by using independence and relevance based crite-
ria. However, as pointed out in [23] the explanation set identiﬁed by Shimony’s
method is not as concise as expected, because for each variable in the explanan-
dum all the variables in at least one path from it to a root variable are included in
the explanation set. Henrion and Druzdzel [25] proposed a model called scenario-
based explanation. In this model a tree of propositions is assumed, where a path
from the root to a leaf represents a scenario, and they look for the scenario with
highest probability. In this model, partial explanations are allowed, but they are
restricted to come from a set of predeﬁned explanations.
As stated in [23] conciseness is a desirable feature in an explanation, that is,
the user usually wants to know only the most inﬂuential elements of the complete
explanation, and does not want to be burdened with unnecessary detail. This
follows the logical principle known as Occam’s razor 1 which can be stated as one
should not increase, beyond what is necessary, the number of entities required to
explain anything. This criterium for deciding among scientiﬁc theories or expla-
nations is also said to be parsimonious. Because of this conception of choosing
the simplest explanation for a phenomenon, a diﬀerent approach is taken in
[26]. The idea is that even when only the relevant variables to the explanandum
are included in the explanation set, the explanations can be simpliﬁed due to
context-speciﬁc irrelevance. This idea is even more interesting when we look for
the K MPEs, because it allows us to obtain explanations with diﬀerent number
of literals. In [26] the process is divided into two stages: (1) the K MPEs are
obtained for a given prespeciﬁed explanation set, and (2) then they are simpliﬁed
by using diﬀerent independence and relevance based criteria.
In our work we try to obtain simpliﬁed explanations directly. The reason is
that the second stage in [26] requires to carry out several probabilistic propaga-
tions and so its computational cost is high (and notice that this process is carried
out after -a complex- MAP computation). Another drawback of the procedure
in [26] is that it is possible, that after simpliﬁcation, the explanations are not
mutually exclusive, we can have even the case of two explanations such that
one is a subset of the other. Here, our basic idea is to start with a predeﬁned
explanation set XE, and them we build a tree in which variables (from XE)
1 Attributed to the medieval philosopher William of Occam (or Ockham).

10 Use of Explanation Trees to Describe the State Space
255
are added in function of their explanatory power with respect to the explanan-
dum but taken into account the current context, that is, the partial assignment
represented by the path obtained from the root to the node currently analysed.
Variables are selected based on the idea of stability, that is, we can suppose that
our system is (more or less) stable, and that it becomes unstable when some
(unexpected) observations are entered into the system. The instability of a vari-
able will be measured by its entropy or by means of its (im)purity (GINI index).
Therefore, we ﬁrst select those variables that reduce most the uncertainty of the
non-observed variables of the explanation set, i.e., the variables better determin-
ing the value of the explanation variables. Of course, the tree does not have to
be symmetric and we can decide to stop the growing of a branch even if not all
the variables in XE have been included. In any case, our set of explanations will
be mutually exclusive, and will have the additional property of being exhaustive,
i.e., we will construct a full partition of the set of possible conﬁgurations or sce-
narios of the values of the variables in the explanation set. We will see how this
complete partition of the space can lead to some conﬁgurations that should not
be understood as real explanations, on the contrary they may be just regarded
as conﬁgurations compatible with the evidence.
Along the subsequent sections we will describe our method in detail
(section 10.2) and afterwards illustrate it: ﬁrst (section 10.3), by using some
(toy) study cases. These two ﬁrst points will be a slightly extended version of
the published paper [27]. Section 10.4 will do a further analysis on the obtained
trees and another example (used in [26]) is added to our experiments in order
to compare both techniques.
10.2
On the Search for Minimal Explanations: The
Explanation Tree
Our method aims to ﬁnd the best explanation(s) for the observed variables that
do not necessarily have a ﬁxed number of literals and we want to achieve that
directly. The provided explanations will adapt to the current circumstances.
Sometimes the fact that a variable X takes a particular value is an explanation
by itself (Occam’s razor) and including other variables to this explanation will
not add any new signiﬁcant information.
We have then decided to represent our solutions by a tree, the Explanation
Tree (ET). In the ET, every inner node will denote a variable of the explanation
set and every branch from this variable will indicate the instantiation of this
variable to one of its possible states. Each node of the tree will determine an
assignment for the variables in the path from the root to it: each variable is equal
to the value on the edge followed by the path. This assignment will be called
the conﬁguration of values associated to the node. In the explanation tree, we
will store for each leaf the probability of its associated conﬁguration given the
evidence. The set of explanations will be the set of conﬁgurations associated to
the leaves of the explanation tree ordered by their posterior probability given
the evidence.

256
M.J. Flores, J.A. G´amez, and S. Moral
Y
Z
X
x1
x2
y1
y2
z1
z2
Explan_1:  P(X=x1|e)
Explan_2:  
 P(X=x2,Y=y1,Z=z1|e)
Explan_3: 
Explan_4:  P(X=x2,Y=y2|e)
P(X=x2,Y=y1,Z=z2|e)
Fig. 10.1. An example of explanation tree
For example, in Fig. 10.1 we can see three variables X,Y and Z that belong
to the explanation set, since they are nodes in the tree. In this particular ex-
ample there are four leaves nodes, that is, four possible explanations, each of
them labelled as Explan i. What this ET indicates is that, given the observed
evidence, X has the value x1 is a valid explanation for such situation (with its
probability/weight associated). But if it is not the case then we should look into
other factors, in this case Y . For example, we can see that adding Y = y2 to the
explanation will be enough. Otherwise, when Y takes value y1 the node needs
to be expanded and we have to look for other involved factors in order to ﬁnd a
valid explanation (in this example, variable Z).
Although the underlying idea is simple, how to obtain this tree is not so
evident. There are two major points that have to be answered:
–
As the ET is created in a top-down way, given a branch of the tree, how to
select the next variable?
–
Given our goals, i.e. allow asymmetry and get concise explanations, how to
decide when to stop branching?
To solve the two previous questions we have used information measures. For
the ﬁrst one, we look for the variable that once instantiated the uncertainty of
the rest explanation variables is reduced at maximum. In other words, given the
context provided by the current branch, we identify the most explicative as the
one that helps to determine the values of the other variables as much as possible.
Algorithm 1 (Create-New-Node) recursively creates our ET. In this
algorithm we assume the existence of an inference engine that provides us with
the probabilities needed during tree growing. We comment on such engine in
Section 10.2.1. The algorithm is called with the following parameters:

10 Use of Explanation Trees to Describe the State Space
257
1. The evidence/observations to be explained xO.
2. The path corresponding to the branch we are growing. In the ﬁrst call to this
algorithm, i.e. when deciding the root node, this parameter will be null.
3. The current explanation set (XE). That is, the set of explanatory variables
already available given the context (path). In the ﬁrst call XE is the original
explanation set. Notice also that if XE = XU in the ﬁrst call, i.e., all non-
observed variables belong to the explanation set, then the method has to
select those variables relevant to the explanation without prior information.
4. Two real numbers α and β used as thresholds (on information and probability
respectively) to stop growing.
5. The ﬁnal explanation tree that will be recursively and incrementally con-
structed as an accumulation of branches (paths). Empty in the initial call.
Algorithm 1. Creates a new node for the explanation tree
1: procedure Create new node(xO,path,XE,α,β,ET)
2:
for all Xj, Xk ∈XE do
3:
Info[Xj, Xk]
= Inf (Xj, Xk|xO, path)
4:
end for
5:
X∗
j
=
arg maxXj∈XE
P
Xk Info[Xj, Xk]
6:
if continue(Info[],X∗
j ,α) and P(path|xO) > β then
7:
for all state xj of X∗
j do
8:
new path ←path + X∗
j = xj
9:
Create new node(xO,new path,XE \ X∗
j ,α,β,ET)
10:
end for
11:
else
12:
ET ←ET ∪<path,P(path|xO) >
▷update the ET adding path
13:
end if
14: end procedure
In algorithm 1, for each variable in the explanation set, Xj, we compute the
sum of the amount of information that this variable provides about all the current
explanation variables conditioned to the current observations x∗
O = (xO, path).
We are interested in the variable that maximises this value. In our study we have
considered two classical measures:
–
mutual information:
Inf (Xj, Xk|x∗
O) = I(Xj, Xk|x∗
O) =

xj,xk P(xj, xk|x∗
O) log

P (xj,xk|x∗
O)
P (xj|x∗
O).P (xk|x∗
O)

–
GINI index:
Inf (Xj, Xk|x∗
O) = GINI(Xj, Xk|x∗
O) = 1 −
xj,xk P(xj, xk|x∗
O)2.
Thus, there are diﬀerent instances of the algorithm depending on the criterion
used as Inf.
Once we have selected the next variable to be placed in a branch, we have to
decide whether or not to expand this node. Again, we will use the measure Inf.

258
M.J. Flores, J.A. G´amez, and S. Moral
The procedure continue is the responsible to take this decision by considering
the vector Info[]. This procedure considers the list of values Info[X∗
j , Xk] for
Xk ̸= X∗
j , then it computes the maximum, minimum, or average of them, de-
pending on the particular criterion we are using. If this value is greater than α it
decides to continue. Of course the three criteria give rise to diﬀerent behaviours,
being minimum the most restrictive, maximum the most permissive and having
average and intermediate behaviour.
Notice that when only two variables remain in the explanation set, the one
selected in line 5 is in fact that having greater entropy (I(X, X) = H(X)) if
mutual information (or MI) is used. Also, when only one variable is left, it is of
course the selected one, but it is necessary to decide whether or not it should
be expanded. For that purpose, we use the same information measure, that is,
I(X, X) or GINI(X, X), and only expand this variable if it is at least as uncertain
(unstable) as the distribution [1/3, 2/3] (normalising in the case of more than
two states2). That is, we only add a variable if it has got more uncertainty than
a given threshold.
10.2.1
Computation
Our inference engine is (mainly) based on Shenoy Shafer propagation algorithm
running over a binary join tree [28]. Furthermore, we have forced the existence
of a single cluster (being a leaf) for each variable in XE, i.e. a clique which
contains only a variable. We use these clusters to enter as evidence the value to
which an explanatory variable is instantiated, as well as to compute its posterior
probability.
Here we comment on the computation of the probabilities needed to carry out
the construction of the explanation tree. Let us assume that we are considering
to expand a new node in the tree which is identiﬁed by the conﬁguration (path)
C = c. Let x∗
O be the conﬁguration obtained by joining the observations XO =
xO and C = c. Then, we need to calculate the following probabilities:
–
P(Xi, Xj|x∗
O) for Xi, Xj ∈XE \ C. To do this we use a two stage procedure:
1. Run a full propagation over the join tree with x∗
O entered as evidence.
In fact, many times only the second stage (i.e., DistributeEvidence) of
Shenoy-Shafer propagation is needed. This is due to the single cliques
included in the join tree, because if only one evidence item (say X) has
changed3 from the last propagation, we locate the clique containing X,
modify the evidence entered over it and run DistributeEvidence by using
it as root.
2. For each pair (Xi, Xj) whose joint probability is required, locate the two
closest cliques (Ci and Cj) containing Xi and Xj. Pick all the potentials
2 This normalisation has been done by using
H(Xi)
log|ΩXi | for entropy (MI) and GINI(Xi)
|ΩXi |−1
|ΩXi |
for GINI index.
3 Which happens frequently because we build the tree in depth, and (obviously) the
create-node algorithm and the probabilistic inference engine are synchronised.

10 Use of Explanation Trees to Describe the State Space
259
in the path between Ci and Cj and obtain the joint probability by using
variable elimination [29]. In this process, we can take as basis the deletion
sequence implicit in the joint tree (but without deleting the required
variables) and then the complexity is not greater than the complexity of
sending a series of messages along the path connecting Ci with Cj for each
possible value of Xi. But, the implicit triangulation has been optimised
to compute marginal distributions for single variables, and it is possible
to improve it to compute the marginal of two variables as in our case.
The complexity of this phase is also decreased by using caching/hashing
techniques, because some sub-paths can be shared between diﬀerent pairs,
or even a required potential can be directly obtained by marginalisation
from one previously cached.
–
P(C = c|xO) =
P (C=c,xO)
P (xO)
. This probability can be easily obtained from
previously described computations. We just use P(xO) that is computed in
the ﬁrst propagation (when selecting the variable to be placed in the root of
our explanation tree) and P(x∗
O) = P(C = c, xO) which is computed in the
current step (full propagation with x∗
O as evidence).
Though this method requires multiple propagations, all of them are carried out
over a join tree obtained without constraining the triangulation sequence, and so
it (generally) has a size considerably smaller than the join tree used for partial
abductive inference over the same explanation set [17, 18]. Besides, the join tree
can be pruned before starting the propagations [18].
10.3
Initial Testing: Preliminary Study Cases
In order to show how it works and the features of the provided explanations, we
found interesting to use some (toy) networks having a familiar meaning for us,
to test whether the outputs are reasonable.
We used the following two cases:
1. academe network: it represents the evaluation for a subject in an academic
environment, let us say, university, for example. This simple network has
got seven variables, as Fig. 10.2 shows. Some of them are intermediate or
auxiliary variables. What this network tries to model is the ﬁnal mark for a
student, depending on her practical assignments, her mark in a theoretical
exam, on some possible extra tasks carried out by this student, and on other
factors such as behaviour, participation, attendance... We have chosen this
particular topic because the explanations are easily understandable from an
intuitive point of view.
In this network we consider as evidence that a student has failed the
subject, i.e., xO ≡{ﬁnalMark=failed}, and we look for the best explanations
that could lead to this fact. We use {Theory, Practice, Extra, OtherFactors}
as the explanation set. In this ﬁrst approach we run our ET-based algorithm

260
M.J. Flores, J.A. G´amez, and S. Moral
theory
practice
markTP
Extra
globalMark
otherFactors
ﬁnalMark
1.0 (g,g)
0.85 (g,a)
0.0 (g,b)
0.9 (a,g)
0.2 (a,a)
0.0 (a,b)
0.0 (b,g)
0.0 (b,a)
0.0 (b,b)
= pass
markTP (M)   (T, P)
1.0  (y,p)
0.25  (y,f)
1.0  (n,p)
0.0  (n,f)
(g, a, b)
(0.4, 0.3, 0.3)
(g, a, b)
(0.6, 0.25, 0.15)
(y, n)
(0.3, 0.7)
(+, −)
(0.8, 0.2)
Theory (T)
Practice (P)
Extra (E)
Others (O)
= pass
finalMark (F)   (O, G)
1.0  (+,p)
0.7  (−,p)
0.0  (−,f)
0.05  (+,f)
= pass
globalMark (G)   (E, M))
Fig. 10.2. Case of study 1: academe network
with β = 0.0 (i.e. the growing of the tree is not limited when the explana-
tions have very little probability), α=0.05|0.07 and criterion = max|min|avg.
Fig. 10.4 summarises the obtained results (variables are represented by using
their initials).
2. gates network: this second net represents a logical circuit (Fig. 10.3.a). The
network (Fig. 10.3.b) is obtained from the circuit by applying the method
described in [30]. The network has a node for every input, output, gate and
intermediate output. Again, we use an example easy to follow, since the
original circuit only has got seven gates (two not-gates, two or-gates and
three and-gates) and the resulting network has 19 nodes.
In this case, we consider as evidence one possible input for the circuit
(ABCDE = 01010) plus an erroneous output (given such input), KL=00. Notice
that the correct output for this case is KL=01, and also notice that from the
transformation carried out to build the network, even when some gates are
E
D
C
B
A
L
K
H
G
F
I
J
N1
A3
A2
A1
N2
O1
O2
A1
A
B
C
D
A2
N1
H
O2
E
J
A3
F
O1
I
N2
K
G
L
(a)
(b)
Fig. 10.3. (a) Original logic circuit (b) Network gates obtained from (a) by using the
transformation described in [30]

10 Use of Explanation Trees to Describe the State Space
261
P
T
P
0.00540
a
b
O
E
E
0.03034
0.11283
+
−
y
n y
n
0.00479
0.01681
bad
0.56418
average
good
g
g
a
b
0.01195
0.00648
y
0.02018
O
E
E
0.01965
+
−
n y
n
0.00802
0.11473
0.08463
0.03895
0.05433
T
P
O
E
E
0.03034
0.00540
0.01195
0.11283
0.25369
0.56418
good
bad
a
g
b
average
+
−
y
n y
n
0.00479
0.01681
0.03895
(a)
(b)
Fig. 10.4. Results for academe: (a) is the obtained tree for all MI cases except
(MI,α=0.05,min) which produces tree (b) together with all (gini,α=0.05) cases and
(gini,α=0.07,max). Finally it is necessary to remark that (gini,α=0.07,min|avg) leads
to an empty tree, ∅, that is no node is expanded. β is 0.0.
A2
N1
N2
A1
A2
A1
A2
O1
ok
ok
f
0.21082
ok
ok
f
0.32775
f
fault
ok
f
0.32775
0.01510
min: 0.10809
0.00373
f
f
ok
f
ok
ok
0.105930.00216
0.00333
0.00343
A2
N1
N2
A1
A2
A1
ok
ok
f
0.21082
ok
ok
f
0.32775
f
fault
ok
f
0.32775
0.01510
f
ok
0.11141
0.00343
min: 0.11484
0.00373
(a)
(b)
Fig. 10.5. Results for gates and MI: (a) is the obtained ET for (MI,α=0.05,max|avg)
and also (MI,α=0.07,max); (b) is for (MI,α=0.07,avg). In both cases min prunes more
the tree than avg, so the dotted area would not be expanded. β is 0.05.
wrong the output could be correct (see [30]). So our evidence is ABCDEKL
= 0101000 and we consider XE = {A1, A2, A3, O1, O2, N1, N2} as the
explanation set with the purpose of detecting which gate(s) is(are) faulty.
Figures 10.5 and 10.6 show the trees obtained for mutual information (I)
and GINI respectively. The same parameters as in the previous study case
are used but β = 0.05.

262
M.J. Flores, J.A. G´amez, and S. Moral
A1
N2
A2
fault
ok
fault
ok
0.21456
ok
f
0.32775
0.11141
0.34628
A1
N2
A2
A2
fault
ok
fault
ok
0.21456
ok
f
0.32775
0.11141
ok
0.01520
0.33107
f
(a)
(b)
Fig. 10.6. Results for gates and GINI: (a) represents the tree for all gini cases, except
(gini,α=0.05,max) which produces tree in part (b). β is 0.05.
10.3.1
Analysis of the Obtained Trees
The ﬁrst thing we can appreciate from the obtained trees is that they are rea-
sonable, i.e., the produced explanations are those that could be expected.
Regarding the academe network, when a student is failed, it seems reasonable
that the most explicative variable is theory because of the probability tables
introduced in the network. Thus, in all the cases Theory is the root node, and
also in all the cases {theory=bad} constitutes an explanation by itself, being in
fact the most probable explanation (0.56).
The other common point for the obtained ETs is that the branch with theory
as good is always expanded. It is clear that being theory ok another reason must
explain the failure. On the other hand, the main diﬀerence between the two ETs
is that 10.4.(a) expands the branch {theory=average} and (b) does not. It is
obvious that a bigger α makes the tree more restrictive. If this tree is expanded,
as α=0.05 does, is because when theory is average it can be interesting to explore
what happens with the practical part of the subject.
In some cases, it can be useful to add to the explanation tree those vari-
ables that are not part of an explanation, but that change their ’a priori’ usual
value or that have an important change in its ’a priori’ probability distribution
could be added to the explanation as this could be useful to the ﬁnal user to
fully understand some situations. An example can be the case of academe net-
work with {theory = good, practice = good}. This branch is not expanded. The
reason is that in this situation, the other variables have small entropy: Extra
should be ’no’ and OtherFactors ’-’, with high probability. This implies an im-
portant change with respect to ’a priori’ probabilities for these values, and then
these variables with their respective values could be added to the explanation
{theory = good, practice = good}, making its meaning more evident.
We also used this case to show the inﬂuence of β. As β = 0.0 was used, we
can see that some branches represent explanations with a very low posterior
probability (those in the dashed area in Fig. 10.4), and so they will not be

10 Use of Explanation Trees to Describe the State Space
263
useful. The dashed areas in Fig. 10.4 represent the parts of the tree that are
not constructed if we use β ≃0.05, which apart of producing a simpler and
more understandable tree is also of advantage to reduce the computational eﬀort
(probabilistic propagations) required to construct the tree.
With respect to the resulting trees for the gates case, we can appreciate two
clear diﬀerences: (1) GINI produces simpler trees than MI, and (2) the most ex-
plicative variable is diﬀerent depending on the used measure. Regarding this last
situation, we can observe in the circuit that there are many independent causes4
(faults) that can account for the erroneous output. Choosing the and gate A1
as GINI does is reasonable (as well as choosing A2) because and gates have (in
our network) greater a priori fault probability. On the other hand, choosing N2
as MI does is also reasonable (and perhaps closer to human behaviour) because
its physical proximity to the wrong output. If we were a technician this would
probably be the ﬁrst gate to test. In this way, it seems that MI is more sensitive
to the fact that the impact a node has in the value of the remaining nodes is
attenuated with the distance in the graph.
Once the ﬁrst variable has been decided, the algorithm tries to grow the
branches until they constitute a good explanation. In some cases, it seems that
some branches could be stopped early (i.e. once we know that N2=fault), but
these situations depend on the thresholds used and it is clear that studying how
to ﬁx them is one of the major research lines for this work.
Perhaps an interesting point is to think about why O1 is not selected by MI
when N2=ok as could be expected given the distance-based preference previously
noticed. But, in this case and gates have more prior of failing than or gates.
Of course, we get diﬀerent explanations depending on the used measure, the
value of α or the criterion, but in general we can say that all the generated
explanations encode reasonable descriptions of the possible scenarios. Finally, in
all the trees there is a branch, and so an explanation which indicates that a set
of gates are ok. Perhaps this cannot be understood as an explanation to a fault,
but we leave it in the tree in order to provide a full partitioning. Some advice
about these explanations can be given to the user by indicating for example if
such explanations raise or not the probability of the fault with respect to its
prior probability.
10.4
Further Experimentation
From the previous study and the examined examples we can extract that the
explanations given by our ET-method are quite reasonable. Anyway, this rea-
sonability term might be a quite vague concept and we wish to reinforce the
belief on the goodness of the generated explanation tree. Hence, we have ﬁg-
ured out a more systematic way to contrast the obtained results to an already
4 However, it is interesting to observe that applying probability propagation, the pos-
terior probability of each gate given the evidence, e.g. P(A1|xO), indicates that that
for all the gates it is more probable to be ok.

264
M.J. Flores, J.A. G´amez, and S. Moral
existing technique with the same purpose: K Most Probable Explanations search.
In subsection 10.4.1 we will apply both techniques to the academe and gates
problem in order to compare the quality of explanations, and the diﬀerences in
the format of giving them (always using global conﬁgurations or not). With the
aim of going into greater detail, the next step will be a comparison with the
already cited method using simpliﬁcation of explanations [26]. To do so, we will
use again the gates network and another of the networks employed in [26] which
models the start mechanism for a car (subsection 10.4.2).
10.4.1
Explanation Tree vs. Partial Abduction
In this part of the chapter we intend to see the behaviour of our method and set
the given solution against the K-best explanations generated by partial abduc-
tion. To be fair, and since the output of both methods is diﬀerent, we will try
to make a kind of translation from one to the other, in such a way that:
1. ET →K-best explanations: From an Explanation Tree we will indicate
the corresponding ranking of explanations, ordered by probability. In our
method every leaf node represented one explanation, that one from the root
until this leaf. Thus, this is almost immediate to be done, but that will make
easier the search of similarities/diﬀerences when it is contrasted with the
K-best explanations. With this, we can see if the ET is able of representing
these K-best explanations and in which form.
2. K-best explanations →ET : The K-best explanations provided by the ab-
duction task will be reﬂected in a tree structure similar to the resulting ET.
So we will annotate, how many explanations are included in each branch
(and which ones). In this case we will measure the distributions of explana-
tions along the tree, and see that, as expected, in partial abduction there
are sets of conﬁgurations that could be aggregated in only one solution for
the sake of simplicity (Principle of parsimony).
In this case, we do not deem necessary to go through all examples, covering the
whole bunch of versions for ET provided before. We have decided to perform
this illustrative proof with the most representative examples.
Academical Example
For the academe network, we will look at the tree in ﬁgure 10.4.(a). We assume
β-pruning has been performed, so the dotted area is removed as ﬁgure 10.7
shows. We have chosen this tree (the largest one) on purpose, to avoid starting
from an advantageous situation. To get the K-best explanations, we have ﬁxed
the K value to 20. We think it is a big enough number to guarantee that we
allow partial abduction to have quite a lot explanations, more than the number
of leaves, by far. In this academe problem MI and GINI happened to behave
quite similarly, so we have not considered relevant to distinguish both cases.

10 Use of Explanation Trees to Describe the State Space
265
P
T
P
a
b
bad
0.56418
average
good
g
g
a
b
0.11473
0.08463
0.03895 0.11283
0.03034
0.05433
Fig. 10.7. ET for academe and MI used for comparison and which is α,β-pruned
#
Theory Practice Other Extra Prob.
1
bad
good
+
no
0.201776
2
bad
avg
+
no
0.084076
3
avg
avg
+
no
0.067259
4
good
bad
+
no
0.067259
5
bad
good
+
yes
0.064857
6
bad
good
-
no
0.053099
7
avg
bad
+
no
0.050044
8
bad
bad
+
no
0.050044
9
bad
avg
+
yes
0.027024
10
avg
avg
+
yes
0.027024
11
bad
avg
-
no
0.022124
12
good
bad
+
yes
0.021619
13
good
good
-
no
0.021240
14
avg
good
+
no
0.020178
15
bad
good
-
yes
0.019877
16
avg
good
-
no
0.019647
17
avg
avg
-
no
0.019027
18
good
bad
-
no
0.017699
19
good
avg
+
no
0.016815
20
avg
bad
+
yes
0.016214
Fig. 10.8. 20-best explanations for the academe model when XO = {ﬁnalMark =
failed} and XE = {theory, practice, otherFactors, Extra}
In Fig. 10.8 we reproduce the output of running partial abduction (Elvira soft-
ware [31]) on academe network with K = 20, just for the same example we went
through in the previous section. On the other hand, Fig. 10.9 has transformed
the corresponding tree (ﬁg. 10.7) into another ordered list of explanations to
have the same arrangement style. In the second ﬁgure we have preferred not to
add a column for every variable, since most of them will not be instantiated to
a particular value.
First, if we just watch the ﬁrst explanation in both cases, they coincide on
the value of Theory which is equal to bad. This is a good sign in the sense
that the most probable explanation is the same with the two techniques, but is

266
M.J. Flores, J.A. G´amez, and S. Moral
#
Conﬁguration
Prob.
1
{theory = bad}
0.56418
2 {theory = average, practice = average} 0.11473
3
{theory = good, practice = bad}
0.11283
4
{theory = average, practice = bad}
0.08463
5
{theory = average, practice = good}
0.05433
6
{theory = good, practice = average}
0.03895
7
{theory = good, practice = good}
0.03034
Fig. 10.9. Explanation ranking corresponding to ET in Fig. 10.7
this really exactly the same? Since the initial problem solving assumptions are
diﬀerent, these explanations are diﬀerent too. In MAP the ﬁrst explanation is
{Theory = bad, Practice = good, OtherFactors = +, Extra = no}. It seems quite
logical that a student having a bad theoretical part had failed, but why should
we assume he has done good practical assignments as this explanations says?
That could lead to think that having bad practical assignments would not be a
so nice explanation for this fact, and actually that would give even more reasons
to think that the student has failed. The same happens with the otherFactors
value, it is positive, but what if otherFactors would have been negative? Even
worse, that will increase the possibilities of the student to fail the subject. That
leads us to think that ET-explanations are more appropriate than K-MPE ones.
Once the explanation content has been regarded, we should also look at the
associated strength. In MAP the most probable explanation has a probability
of 0.2. But in our ET the ﬁrst explanation “Theory is bad” covers more than
the half (56%) of the explanation space. We ﬁnd this second number much more
accurate: in most of the cases where a student has failed, a bad theoretical exam
seems a good enough explanation. If we observe the prior probability tables
Theory was precisely the most inﬂuential factor. This conﬁrms our believe that
when abduction requires conﬁgurations always with |XE| elements, there are
some unnecessary variables Xunnecessay ⊂XE5 that change their states in the
conﬁgurations in a counter-intuitive way (they are in fact being tuned), since
this variable instantiation is not really signiﬁcant for the explanation.
To understand better this important point, we jump to the other related il-
lustration (translation K-best expl. →ET) in Fig. 10.10. Here we tried to make
a picture on how the K-best explanations would be distributed in the ET. So,
the structure is identical to the ET, i.e. a tree node is a variable, a branch indi-
cates the state associated to this variable and leaf nodes represent explanations.
Precisely at this lower level, where explanations are implicitly depicted, we have
included the K-best explanations data:
5 Notice that the set of unnecessary variables can vary depending on the given
conﬁguration, that is, on the other variables and values in the current explana-
tion/conﬁguration. Apart from this, it could happen that some of them are in all
cases unnecessary.

10 Use of Explanation Trees to Describe the State Space
267
–
Together with the last branch conﬁguration, and between brackets [ ], we
annotate the explanations that would fall in that branch/explanation. The
number indicates the ranking, as in column # of Fig. 10.8.
–
Just on the leaves a triangle indicates the number M of explanations that
are included in this position (Mexpl.) and below it we write the sum of their
probabilities in parentheses, that is:
Σ
xE∈Explpath
P(xE|xO)
where Explpath is the set of explanations in path going from root to the leaf
and |Explpath| = M.
For example explanation #14: {Theory=avg, Practice=good, OtherFac-
tors=+, Extra=yes} belongs to path <Theory=avg, Practice=good>.
–
Finally, in braces we indicate the branch/path probability as usual in the ET
representation.
Then, in Fig. 10.10 we can see how {Theory = bad} piles up the biggest
number of explanations (8 expl.), even if we look at all the branches at the
same detail level {Theory = good} has 5 and {Theory = avg } presents 7.
However it is not so a question of quantity, but mainly quality and what is
clear is that branch {Theory = bad} includes 7 of the 10-best explanations!!!
Notice that in Fig. 10.8 if we apply a similar criterion to our β-pruning, with
β = 0.05 we could have just taken the eighth ﬁrst explanations, the other could
be considered as too little relevant. Anyhow, as we have remarked above, we
show them because we wanted to take a big mass of explanation in order to
make the comparison advantageous to partial abduction in the way that many
explanations are regarded. In a practical case, the user would not normally need
to have up to 20 diﬀerent reasons.
The main conclusion from the previous discussed point is that when we per-
form the search of the K-best explanations, they are not necessarily K. With
practice
avg
avg
bad
practice
bad
avg
theory
bad
8 expl.
[1,2,5,6,8,9,10,17]
good
(0.4693)
(0.1079)
3expl.
good
[7,20]
[3,11,16]
[14,15]
(0.03982)
2expl.
[13]
good
1expl.
[10]
[4,12,18]
(0.1066)
3expl.
(0.0168)
1expl.
(0.0666)
2expl.
{0.03034}
{0.03895}
{0.11283}
{0.05433}
{0.11473}
{0.08463}
{0.56418}
(0.0216)
Fig. 10.10. ET structure for academe including 20-best explanations

268
M.J. Flores, J.A. G´amez, and S. Moral
the resulting ET we can see that for example 7 out of 10 explanations could be
(and should be in our opinion) amalgamate in only one.
Even if we think that our method manages to outperform partial abduction
in both eﬃciency and results, there are still doubtful issues in its own nature.
For example, with the same principle as the previously explained, we could think
that having Theory as average could also be an explanation by itself. Observe
that in Fig. 10.9, ET-explanations #2,#4 and #5 present that conﬁguration
and they diﬀer in the practical part value. But here there could be nuances
to take into account. For example having theory and practice ordinary (avg)
is the second explanation, very near from having bad practice and good theory
whereas average theory and good practice decrease considerably the probability
when we know that the student has failed. This instability is even more clear when
theory is good, we need to distinguish the possible states for Practice to reach
a valid explanation. Look at the diﬀerent probabilities for these three branches:
{Practice = bad} is 4 times more probable than the other two. Nevertheless,
in the 20-best explanations this one would appear in the forth place without
standing out specially from the rest, and later this explanation reappears in
#12 and #18.
Circuit Example
Unlike the ﬁrst example, the provided ETs diﬀer a lot from using MI or GINI
as the Info measure both in size and also in the nodes situation along the tree.
So, in this second case, we are going to divide this comparative study into two
parts: one for tree in Fig. 10.5.(a) and the other for tree in ﬁg. 10.6.(a).
•When Info is Mutual Information
The notation for ﬁgures is exactly the same as for the previous example. So, we
can ﬁnd the 20-best explanations in ﬁg. 10.11, the ranking for ET in ﬁg. 10.13
and the integrated tree in ﬁg. 10.14.
Looking at the two rankings, again we can detect that not only the ﬁrst ex-
planation, but the two ﬁrst (which are equiprobable) are alike in the sense that
the extracted anomaly is that either A1 does not work properly or the failure is
in A2. But again, since the K-MPEs need a value for every variable, this method
burdens this explanation with values that are not relevant, all the other gates
are set to ok. Here the overspeciﬁcation problem is again quite clear. And, from
the quantitative point of view (probability ordering) the same situation repeats:
as the signiﬁcant data is that gate A1 (or A2) fails, the rest of conﬁgurations
when A1 = fault are regarded here, considering only one of these conﬁgurations
instead of integrating them as when using the ET model. Luckily, in this exam-
ple, this diﬀerence is slightly greater than 0.015 (0.32775 - 0.311406), because
the accumulation of two gates faults is quite improbable and only one of the
conﬁgurations has most of the probability mass. To check that, we should just
look explanations from #6 to #20, where the total probability barely reaches

10 Use of Explanation Trees to Describe the State Space
269
#
N1
N2
A1
A2
A3
O1
O2
Prob.
1
ok
ok
ok
fault
ok
ok
ok
0.311406
2
ok
ok
fault
ok
ok
ok
ok
0.311406
3
ok
ok
ok
ok
ok
fault
ok
0.205486
4
ok
fault
ok
ok
ok
ok
ok
0.101705
5
ok
ok
fault fault
ok
ok
ok
0.014447
6
ok
ok
ok
fault
ok
fault
ok
0.006355
7
ok
ok
fault
ok
ok
fault
ok
0.006355
8
ok
ok
ok
fault fault
ok
ok
0.004815
9
ok
ok
fault
ok
fault
ok
ok
0.004815
10
ok
ok
ok
fault
ok
ok
fault 0.003178
11
ok
ok
fault
ok
ok
ok
fault 0.003178
12
ok
ok
ok
ok
fault fault
ok
0.003178
13
ok
fault
ok
fault
ok
ok
ok
0.003145
14
ok
fault fault
ok
ok
ok
ok
0.003145
15
ok
ok
ok
ok
ok
fault fault 0.002097
16
ok
fault
ok
ok
ok
ok
fault 0.002076
17
ok
fault
ok
ok
ok
fault
ok
0.002076
18 fault
ok
ok
fault
ok
ok
ok
0.001573
19 fault
ok
fault
ok
ok
ok
ok
0.001573
20 fault
ok
ok
ok
fault
ok
ok
0.001573
Fig. 10.11. 20-best explanations for gates with XO = {ABCDEKL = 0101000} and XE =
{A1, A2, A3, O1, O2, N1, N2}
0.07. Also, it is curious to see how three faults are not even considered within
the 20 best explanations.
In ET depending on the side of the tree, there could be three or four gates
included in the explanation. This gives us the hint that this designed algorithm
is also able of discerning those variables within the explanation set which are in
fact relevant to explain the given observations. A1 and A2 (with a symmetrical
behaviour for this example) together with N2 are clearly the three involved
gates that could have caused the given error. In this particular example, even
without considering the threshold β(=0.0), gates O2 and A3 never appear in the
explanation tree. We can see that the erroneous output is in signal K and these
two gates do not play a role for that value. In certain cases O1 (depending on the
thresholds’ values) could appear. As we already commented, this gate, even if it
is related to the bad output K, increases its belief of working properly (or, the
other way round, decreases its belief of being faulty) because it also participates
in producing signal L, which is correct. In K-best explanations this happens to
be the third possible faulty gate, so we ﬁnd that this implication (L signal valid
then O1 is likely to work properly) is not caught there.
The nice feature about ET selecting the explanatory variables has also been
shown in the academe example where Theory and Practice stood out as the
signiﬁcant variables.

270
M.J. Flores, J.A. G´amez, and S. Moral
If everything works ok
"0"
"0"
"0"
"0"
"0"
"1"
"1"
"0"
"0"
"0"
"0"
"0"
K should be "1"
A1
A2
O2
C
B
N1
O1
N2
A3
K
I
F
D
E
H
G
L
J
A
Fig. 10.12. Circuit with the evidence (ABCDEKL=0101000) incorporated and the as-
sumed signal values when if everything works ok annotated
#
Conﬁguration
Prob.
1
{N2 = ok, A1 = ok, A2 = fault}
0.32775
2
{N2 = ok, A1 = fault, A2 = ok}
0.32775
3
{N2 = ok, A1 = ok, A2 = ok, N1 = ok}
0.21082
4
{N2 = fault, A1 = ok, A2 = ok}
0.10809
5
{N2 = ok, A1 = fault, A2 = fault}
0.01510
6 {N2 = ok, A1 = ok, A2 = ok, N1 = fault} 0.00373
7
{N2 = fault, A1 = fault}
0.00343
8
{N2 = fault, A1 = ok, A2 = fault}
0.00333
Fig. 10.13. Explanation ranking corresponding to ET in ﬁgure 10.5.(a)
Also, let us comment that the set of paired explanations (for instance #1 and
#2, #8 and #9, #10 and #11, · · ·) comes from the symmetrical inﬂuence of
gates A1 and A2. From ﬁgure 10.12 this symmetry can easily observed.
A glance at the combined tree in ﬁg. 10.14 reinforces the conjectures previ-
ously remarked. The two branches that accumulate a larger number of (MPE)
explanations (5 of them each) are exactly the two ﬁrst candidates: A1 is faulty
or A2 is faulty. Next, unexpectedly maybe, we have that gates N2,A1,A2,N1 are
ok. But in this case, we have that O1 is faulty with probability close to 1.0. So,
we have again an example in which it seems reasonable to expand some given
explanations with the variables that has experiment an important modiﬁcation
of their prior probabilities.
Again, notice that if we had taken for example the 10-best explanations, our
tree would have given even a deeper level of detail just with 8 leaves/explanations
because some leaves will not correspond to any of the 10-best explanations, ET
adds then more information.
Finally, we would like to add again that the small probabilities of explanations
from #6 to #20 prove that they were unnecessary. Even though, with a few

10 Use of Explanation Trees to Describe the State Space
271
N2
A1
fault
fault
fault
fault
fault
N1
A2
A1
A2
A2
fault
1expl.
(0.00314)
{0.1081}
3expl.
(0.1058)
{0.32775}
(0.3273)
5expl.
{0.01510}
1expl.
ok
ok
(0.01445)
1expl.
ok
[3,12,15]
ok
(0.2108)
3expl.
{0.32775}
fault
[20]
{0.2108}
{0.00373}
(0.00314)
1expl.
(0.00157)
{0.00343}
ok
[14]
5expl.
[5]
(0.3273)
[13]
[4,16,17]
ok
[2,7,9,11,19]
ok
[1,6,8,10,18]
{0.00333}
Fig. 10.14. ET structure for gates with MI including 20-best explanations
#
Conﬁguration
Prob.
1
{A1 = fault}
0.34628
2 {A1 = ok, N2 = ok, A2 = fault} 0.32775
3
{A1 = ok, N2 = ok, A2 = ok}
0.21456
4 {A1 = ok, N2 = fault, A2 = ok} 0.11141
Fig. 10.15. Explanation ranking corresponding to ET in Fig. 10.6.(a)
propagation steps with the explanation tree we are able of capturing all of them.
Besides, these explanations are presented to the user in a more intuitive and
simpler manner.
•When Info is GINI Index
Using GINI index instead of MI has changed the order of selected variables in
the tree, as reviewed in section 10.3. Also, see that in Fig. 10.6.(b) the second
level variable is distinct depending on the state taken by the root variable A1.
Here we are going to examine tree 10.6.(a) and when incorporating the 20-
best explanations on it we obtain the one depicted in ﬁg. 10.16. As this tree
shows and as the corresponding ranking (ﬁg. 10.15) does too, again the two
main explanations are that either A1 or A2 does not work. In this case A1 has
been ﬁrst selected, but we think is a question of tie breaks. Apart from the tree
level of the involved gates and the slight diﬀerence between ET-explanations #1
and #2 the same conditions as with MI accomplish. But this time we have the
advantage of presenting a more compact tree, where the system determines that
the relevant gates to be tested are A1, A2 and N2 in this order (according to
the probability ranking). This is inﬂuenced by the thresholds α and β 6, as the
6 And by the min, avg or max criteria as well.

272
M.J. Flores, J.A. G´amez, and S. Moral
N2
fault
[2,5,7,9,11,14,19]
[4,13,16,17]
{0.21456}
fault
[1,6,8,10,18]
fault
{0.32775}
ok
[3,12,15,20]
{0.1141}
ok
A1
A2
ok
{0.34628}
4expl.
(0.34492)
7expl.
5expl.
(0.3273)
(0.21233)
4expl.
(0.1090)
Fig. 10.16. ET structure for gates with GINI including 20-best explanations
other technique was inﬂuenced by the K number. The user can make a choice
on these values depending the level of detail s/he requests. But for a general
case these explanations seem good enough and they have been reached with a
reasonably low eﬀort, just a few propagations on the join tree. Once again, we
can also say, that the method has made a reduction of the explanation set from
seven variables down to only three, which also simpliﬁes quite a lot the problem.
10.4.2
Looking into Simpliﬁcation Methods. A New Example:
Car-Start
To ﬁnish in this evaluation of the Explanation Tree technique, we ﬁnd inter-
esting to add some detail about the already mentioned method of simplifying
explanations [22, 26]. We just would like to set out the basic ideas in order to
see similarities and diﬀerences of this method with our proposal. In these two
works, the objective was to simplify the explanations given after applying MPE
as ﬁgure 10.17 illustrates. They also state the process as follows: “Let expl(xO)
= {x1
E, x1
E, . . . , xk
E} be the K MPEs obtained for evidence XO = xO. Then, for
all xE ∈expl(xO) we are looking for a sub-conﬁguration x′
E [X′
E ⊂XE], so that
x′
E is still accounting for the observed evidence.”
So, the process is diﬀerentiated into two main steps:
1. Generation of complete explanations (conﬁgurations of XE with |XE| liter-
als), ordered by their posterior probabilities given the observations.
2. Simpliﬁcation of these explanations by removing unimportant literals.
To design a mechanism for this reduction of explanations, the authors de-
ﬁne and propose two important criteria: Independence (I∼simpliﬁcation) and
Relevance (R∼simpliﬁcation)7. We avoid theoretical details and formulas,
7 In this work the authors develop other simpliﬁcation techniques such as those in-
duced by the graph, that we will not touch on here.

10 Use of Explanation Trees to Describe the State Space
273
Observed
    facts
inference
K MPEs
simplification
explanations
simplified
Fig. 10.17. Process followed in the simpliﬁcation of explanations in [26]
aiming the conception of both criteria. The ﬁrst one will try to detect those
variables that are useless in the explanation since the values these variables take
do not aﬀect the evidence. In the case of relevance-based criteria they attempted
to remove those literals that could be considered as insigniﬁcant in the sense that
they are (almost) irrelevant for the observed evidence.
There is a notable diﬀerence between ET-based simpliﬁcations and this two
stage process: in ET the root variable will always be in all the explanations
whereas when simplifying explanations it could happen that one explanation is
{A = a1, B = b1} and another one {C = c1, D = d1}.
It is clear that both approaches come up from the same concern: avoid the
overspeciﬁcation problem when dealing with abductive inference in BNs. In fact,
the current work was inspired on the previous one, but attempting to skip its two
main drawbacks: (1)little eﬃciency.- since it requires a two-step process and the
ﬁrst one includes K-MPEs and (2)being this simpliﬁcation K-MPE guided the
obtained solutions are somehow inﬂuenced by them. As we have just veriﬁed,
in many occasions the K-best explanations do not correspond to K diﬀerent
scenarios, because some explanations should have been aggregated into one.
Remember the previous example, where 7 of the 10th best explanations could
be summarised in only one. We observe that these 7 explanations are just the
combination of the simple explanation extended with diﬀerent conﬁgurations of
other variables. We also believe this happens very frequently when performing
partial abduction. Actually, the experiments executed in [22] discovered that
after applying simpliﬁcation to the K-best explanations, most of the times the
same subset of simpliﬁed explanations were repeated following a certain pattern.
To conﬁrm our feeling that ET succeeds not only in giving explanations with
diﬀerent number of literals, but also in improving the simpliﬁcation procedure
in [26] we are going to take a couple of examples in this work and apply the
ET-algorithm on them.
To start with, we take again the gates network, but this time we ﬁx the
evidence to XO = {D = 0, K = 1, L = 1} and the explanation set to
XE = {F, G, H, I, J, A1, A2, A3, O1, O2, N1, N2}. To have a visualisation of this
situation we have depicted ﬁg. 10.18. Weq have selected the explanation set as
those variables that are not observable in the network.
For this example in [22] it is applied a technique called successive explanations
search that after several stages gives rise to explanation X5
S = {A3 = ok}.
If we execute the ET-algorithm on this same data and some standard thresh-
olds (for example α = 0.05, β = 0.05) with MI the given tree is as simple as the
one depicted in ﬁgure 10.19.
Well, we could say that both explanations are not inconsistent, A3 can work
properly at the same time as N2 presents a fault. Besides, if A3 works, a failure in

274
M.J. Flores, J.A. G´amez, and S. Moral
"1"
"1"
"0"
"1"
"1"
"1"
A1
A2
O2
C
B
A
N1
O1
N2
A3
K
D
E
L
H
G
F
J
I
Fig. 10.18. Circuit with the evidence (DKL=011) incorporated and the assumed signal
values when everything works ok annotated. Notice that here there is a conﬂict: if
output L is 1, and being H = 1 an input of the or-gate O1, then I should be 1, but
that will imply K = 0 which is inconsistent with the evidence.
N2
0.9614
0.03856
ok
fault
Fig. 10.19. ET obtained for the gates with (MI,0.05,max) and β = 0.05 in the situation
of ﬁg. 10.18
N2 is the fact that better would explain this circumstance, because the output of
A3 should be correct. Thus, if L = 1 that means that I = 1, but this is an input
for the NOT-gate N2 that should have been the opposite signal, 0, according to
the introduced evidence. We think that the second explanation ({N2 = fault}
with 96% of strength) is quite more informative in a diagnosis task. So we really
ﬁnd that our answer is of better quality than the other one.
Another Example: Car-Start Problem
As a second example we have taken the car-start network from [26]8. We are
going to ﬁrstly introduce a simple and intuitive example, where the evidence
is that the car does not start (Starts = No), and the explanation set is {XE
= Alternator, FanBelt, Leak, Charge, BatteryAge, BatteryState, BatteryPower,
GasInTank, Starter, EngineCranks, Leak2, FuelPump, Distributor, SparkPlugs}.
8 They indicate that this network has been originally found in JavaBayes package.
http://www.cs.cmu.edu/∼javabayes is the web site.

10 Use of Explanation Trees to Describe the State Space
275
Radio
Starter
Alternator
FanBelt
Leak
Charge
BatteryAge
Battery
State
Battery
Power
Lights
GasGauge
GasInTank
Leak2
Engine
Cranks
FuelPump
Distributor
SparkPlugs
Starts
Fig. 10.20. Network modelling the car-start problem
With an ET -execution of kind [Info,criterion,α,β] =[MI,min,0.07,0.05] the result-
ing tree is the one in ﬁg. 10.21.
Even with real probability tables unknown, any person with a little knowledge
about cars could try to interpret this answer which just says that the two most
probable explanations are that the battery state is weak (0.77) or as a second
and less probable explanation (0.13) that the started could be faulted.
Let us now show a more complex case studied in [22] where evidence is
XO = {GasGauge = Gas, Lights = Work, Radio = Works, Starts = No} and
the explanation set is the same of the previous example. With the simpliﬁcation
method the ﬁnal obtained explanation is {GasInTank = Yes, Starter = Faulted}.
We have performed an execution of the Explanation Tree algorithm of kind
(MI,min,0.07,0.05) as before, and the given tree is drawn in ﬁg. 10.22. In this
case we ﬁnd that both simpliﬁed explanations are quite close. It is clear that
the starter does not work properly, which is probably the main explanation.
But both simpliﬁcation and ET go further adding also that there is no problem
with the gas status. In simpliﬁcation it says that there is gas in the tank while
ET has checked the possibility of presenting a leak of kind “2”, but this is quite

276
M.J. Flores, J.A. G´amez, and S. Moral
BatteryState
0.0325
0.13423
FanBelt
0.3425
faulted
ok
0.7774
weak
broken
slipping
ok
Starter
ok
0.03247
Fig. 10.21. ET for the car-start problem, when {Starts = No} and (MI,min,0.07,0.05)
0.815
faulted
ok
0.008
false
0.177
Starter
Leak2
true
Fig. 10.22. ET for the car-start problem, when {GasGauge
=
Gas, Lights
=
Work, Radio = Works, Starts = No} and (MI,min,0.07,0.05)
improbable. Since the evidence included that the gas gauge indicates enough
gas, and this device is not determined as faulted, we could have supposed that
it works properly and therefore {GasInTank = Yes}.
We have performed a ﬁnal comparison “simpliﬁcation vs. ET” taking
randomly selected examples from [26]. In this study they just indicate the expla-
nation set and the observed variables and, using diﬀerent types of algorithms for
simpliﬁcation, they show the obtained number of literals. We have then repro-
duced these tests with all the possible conﬁgurations for the observed variables

10 Use of Explanation Trees to Describe the State Space
277
with (MI,max,0.07,0.05) and in all cases either the tree was empty or it presented
one variable. In simpliﬁcation experiments the number of literals could vary from
1.5 until 6. In the cases where an empty tree was obtained, it could indicate that
all variables have a very low entropy and they present a ﬁxed value. Then, there
is only one possible scenario that will be the most probable explanation. On the
other hand, in the cases with one variable the distribution was quite clear giving
probability numbers such as 0.933 or 0.8146.
10.5
Discussion and Main Conclusions
This chapter has proposed a new approach to the problem of obtaining the most
probable explanations given a set of observations in a Bayesian network. The
method provides a set of possibilities ordered by their probabilities. The main
novelties are three:
1. The level of detail of each one of the explanations is not uniform (with the
idea of being as simple as possible in each case).
2. The generated explanations are mutually exclusive.
3. The number of required explanations is not ﬁxed (it depends on the partic-
ular case we are solving).
Our goals are achieved by means of the construction of the so called explanation
tree which can have asymmetric branching and that will determine the diﬀerent
possibilities.
We have described the procedure for its computation based on information
theoretic criteria. To show its behaviour some simple examples have been proved
and we have presented a comparison with the K-best explanations and with the
simpliﬁcation of explanations. From this experimental analysis, we can conclude
that our method outperforms the K-best explanations search by far both in
quality and eﬃciency, since our method minimises the number of propagations
and manage to do them in a very quick form. We ﬁnd that the description
given by an ET is more useful for a user than the one given by K complete
explanations because in the former case the possibilities are given in a more
compact and structured way.
With respect to eﬃciency we have that ET construction (β > 0) is polynomic9
when probabilistic propagation is polynomic as in the case of polytrees, whereas
K-MPEs are NP-hard in polytrees.
Regarding the simpliﬁcation of explanations, we could say that the obtained
explanations are diﬀerent, but not always of diﬀerent quality. Nevertheless, ET
is able of giving simpler explanations and what is more important it achieves
the task in a direct way, avoiding the cost of performing traditional abduction
ﬁrst.
9 Each expansion of a node in the tree implies a polynomial number of probability
propagations and the number of nodes in the tree is bounded by a function of β.

278
M.J. Flores, J.A. G´amez, and S. Moral
10.6
Further Work
Our main conclusion is that this technique behaves in a very satisfactory way,
but we would like to run more sophisticated experiments in order to evaluate it
in depth. We believe it is quite promising, even though we are conscious that
a better reﬁnement should be done for some issues such as the determination
of α and β thresholds or the characterisation of those explanations given for
partitioning the explanation space, but not relevant as an explanation.
As further work we plan also to study the possibility of using Kullback-Leibler
distance as the Info measure. In addition, very useful comments have suggested us
to explore the connection about this method and the so-called Hitting set which
is quite broadly studied by researchers on the logical ﬁeld together with theories
of diagnosis [6]. Finally, we would like to analyse if our diagnosis method could
be beneﬁtted from the conﬂict analysis theory, as long as conﬂicts are applicable
to a particular problem.
We also consider that clustering is an important ﬁeld of application/extension
for this method. So, we have already started a new research line oriented to that
speciﬁc task where the Explanation Tree has constituted an inspiration. The
idea and preliminaries about this technique and its structure, the Independency
Tree, can be found in [32].
References
1. Pearl, J.: Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San
Mateo (1988)
2. Castillo, E., Guti´errez, J.M., Hadi, A.S.: Expert Systems and Probabilistic Network
Models. Springer, Heidelberg (1997)
3. Jensen, F.V.: Bayesian Networks and Decision Graphs. Springer, Heidelberg (2001)
4. Lacave, C., D´ıez, F.J.: A review of explanation methods for Bayesian networks.
The Knowledge Engineering Review 17, 107–127 (2002)
5. G´amez, J.A.: Abductive inference in Bayesian networks: A review. In: G´amez,
J.A., Moral, S., Salmer´on, A. (eds.) Advances in Bayesian Networks, pp. 101–120.
Springer, Heidelberg (2004)
6. Reiter, R.: A theory of diagnosis from ﬁrst principles. Artiﬁcial Intelligence 32(1),
57–95 (1987)
7. Peng, Y., Reggia, J.A.: A probabilistic causal model for diagnostic problem solving.
IEEE Transactions on Systems, Man, and Cybernetics 17(2), 146–162 (1987)
8. Shimony, S.E.: Finding MAPs for belief networks is NP-hard. Artiﬁcial Intelli-
gence 68, 399–410 (1994)
9. Dawid, A.P.: Applications of a general propagation algorithm for probabilistic ex-
pert systems. Statistics and Computing 2, 25–36 (1992)
10. Nilsson, D.: An eﬃcient algorithm for ﬁnding the M most probable conﬁgurations
in Bayesian networks. Statistics and Computing 8, 159–173 (1998)
11. Li, Z., D’Ambrosio, B.: An eﬃcient approach for ﬁnding the MPE in belief net-
works. In: Proceedings of the 9th Conference on Uncertainty in Artiﬁcial Intelli-
gence, pp. 342–349. Morgan Kaufmann, San Francisco (1993)

10 Use of Explanation Trees to Describe the State Space
279
12. Seroussi, B., Goldmard, J.L.: An algorithm directly ﬁnding the k most probable
conﬁgurations in Bayesian networks. International Journal of Approximate Rea-
soning 11, 205–233 (1994)
13. Gelsema, E.S.: Abductive reasoning in Bayesian belief networks using a genetic
algorithm. Pattern Recognition Letters 16, 865–871 (1995)
14. Nilsson, D.: An algorithm for ﬁnding the most probable conﬁgurations of discrete
variables that are speciﬁed in probabilistic expert systems. MSc.Thesis, University
of Copenhagen, Copenhagen, Denmark (1994)
15. Shimony, S.E.: Explanation, irrelevance and statistical independence. In: Proc. of
the National Conf. in Artiﬁcial Intelligence, pp. 482–487 (1991)
16. Neapolitan, R.E.: Probabilistic Reasoning in Expert Systems. Theory and Algo-
rithms. Wiley Interscience, New York (1990)
17. Park, J.D., Darwiche, A.: Complexity results and approximation strategies for
MAP explanations. Journal of Artiﬁcial Intelligence Research 21, 101–133 (2004)
18. de Campos, L.M., G´amez, J.A., Moral, S.: On the problem of performing exact
partial abductive inference in Bayesian belief networks using junction trees. In:
Bouchon-Meunier, B., Gutierrez, J., Magdalena, L., Yager, R.R. (eds.) Technologies
for Constructing Intelligent Systems 2: Tools, pp. 289–302. Springer, Heidelberg
(2002)
19. Park, J.D., Darwiche, A.: Solving MAP exactly using systematic search. In: Pro-
ceedings of the 19th Conference on Uncertainty in Artiﬁcial Intelligene (UAI 2003),
pp. 459–468 (2003)
20. Salmer´on, A., Cano, A., Moral, S.: Importance sampling in Bayesian networks
using probability trees. Computational Statistics and Data Analysis 34, 387–413
(2000)
21. de Campos, L.M., G´amez, J.A., Moral, S.: Partial abductive inference in Bayesian
networks by using probability trees. In: Camp, O., Filipe, J., Hammoudi, S., Piat-
tini, M. (eds.) Enterprise Information Systems, vol. V, pp. 146–154. Kluwer Aca-
demic Publishers, Dordrecht (2004)
22. G´amez, J.A.: Inferencia abductiva en redes causales (Abductive inference in casual
networks). Doctoral thesis, Dpto. de Ciencias de la Computaci´on e I.A. Universidad
de Granada (June 1998)
23. Chajewska, U., Halpern, J.Y.: Deﬁning explanation in probabilistic systems. In:
Proc. of 13th Conf. on Uncertainty in Artiﬁcial Intelligence (UAI 1997), pp. 62–71
(1997)
24. Shimony, S.E.: The role of relevance in explanation I: Irrelevance as statistical
independence. International Journal of Approximate Reasoning 8, 281–324 (1993)
25. Henrion, M., Druzdzel, M.J.: Qualitative propagation and scenario-based schemes
for explaining probabilistic reasoning. In: Bonissone, P.P., Henrion, M., Kanal,
L.N., Lemmer, J.F. (eds.) Uncertainty in Artiﬁcial Intelligence, vol. 6, pp. 17–32.
Elsevier Science, Amsterdam (1991)
26. de Campos, L.M., G´amez, J.A., Moral, S.: Simplifying explanations in Bayesian
belief networks. International Journal of Uncertainty, Fuzziness and Knowledge-
based Systems 9, 461–489 (2001)
27. Flores, M.J., G´amez, J.A., Moral, S.: Abductive inference in Bayesian networks:
ﬁnding a partition of the explanation space. In: Godo, L. (ed.) ECSQARU 2005.
LNCS (LNAI), vol. 3571, pp. 63–75. Springer, Heidelberg (2005)
28. Shenoy, P.P.: Binary join trees for computing marginals in the Shenoy-Shafer archi-
tecture. International Journal of Approximate Reasoning 17(2-3), 239–263 (1997)

280
M.J. Flores, J.A. G´amez, and S. Moral
29. Dechter, R.: Bucket elimination: A unifying framework for probabilistic inference.
In: Proceedings of the Twelth Conference on Uncertainty in Artiﬁcial Intelligence
(UAI 1996), pp. 211–219 (1996)
30. deKleer, J., Williams, B.C.: Diagnosing multiple faults. Artiﬁcial Intelligence 32(1),
97–130 (1987)
31. Consortium, E.: Elvira: An Environment for Probabilistic Graphical Models. In:
G´amez, J.A., Salmer´on, A. (eds.) Proceedings of the 1st European Workshop on
Probabilistic Graphical Models, pp. 222–230 (2002)
32. Flores, M.J., G´amez, J.A., Moral, S.: The Independency tree model: a new ap-
proach for clustering and factorisation. In: Proceedings of the Third European
Workshop on Probabilistic Graphical Models, PGM 2006, pp. 83–90 (2006)

D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 281–288, 2008. 
springerlink.com                                                                © Springer-Verlag Berlin Heidelberg 2008 
11 
Toward a Generalized Bayesian Network* 
Dawn E. Holmes 
Department of Statistics and Applied Probability, South Hall,  
University of California, Santa Barbara, 
CA 93106, USA 
Abstract. The author’s past work in this area has shown that the probability of a state of a 
Bayesian network, found using the standard Bayesian techniques, could be equated to the Maxi-
mum Entropy solution and that this result enabled us to find minimally prejudiced estimates of 
missing information in Bayesian networks. In this paper we show that in the class of Bayesian 
networks known as Bayesian trees, we are able to determine missing constraint values optimally 
using only the maximum entropy formalism. Bayesian networks that are specified entirely within 
the maximum entropy formalism, whether or not information is missing, are called generalized 
Bayesian networks. It is expected that further work will fully generalize this result. 
Keywords: Bayesian networks, maximum entropy, d-separation. 
PACS: 02.50.Cw, 89.70.+c, 05.70.–a, 65.40.Gr. 
11.1   Introduction 
One of the major drawbacks of using Bayesian networks is that complete information, 
in the form of marginal and conditional probabilities must be specified before the 
usual updating algorithms are applied. Holmes [1] has shown that when all or some of 
this information is missing, it is possible to determine unbiased estimates using 
maximum entropy. The techniques thus developed depend on the property that the 
probability of a state of a fully-specified Bayesian network, found using the standard 
Bayesian techniques, can be equated to the maximum entropy solution. A fully-
constrained Bayesian network is clearly a special case, both theoretically and practi-
cally, and a general theory has yet to be provided. As a first step toward a general 
theory a generalized Bayesian network is defined as one in which some, all or none of 
the essential information is missing. It is then shown that missing information can be 
estimated using the maximum entropy formalism (MaxEnt) alone, thus divorcing 
these results from their dependence on Bayesian techniques.   
The techniques required for the current problem are substantially different to those 
used previously in that, although we still use the method of undetermined multipliers, 
we no longer equate the joint probability distributions given by the Bayesian and 
maximum entropy models in order to determine the Lagrange multipliers. Two pre-
liminary results are described here. Firstly, we extend the 2-valued work of Holmes 
[2] and of Markham and Rhodes [3] by developing an iterative algorithm for updating 
                                                           
* Re-printed with kind permission of the American Institute of Physics. 

282 
D.E. Holmes 
probabilities in a multivalued multiway tree, Secondly, we use the Lagrange multi-
plier technique to find the probability of an arbitrary state in a Bayesian tree using 
only MaxEnt. We begin by defining a Bayesian network. 
11.2   Bayesian Networks 
A Bayesian network is essentially a system of constraints; those constraints being 
determined by d-separation. Formally, a Bayesian network is defined as follows.  Let: 
 
(i)  
V be a finite set of vertices 
(ii)  
B be a set of directed edges between vertices with no feedback loops. The 
vertices together with the directed edges form a directed acyclic graph 
G = V,B   
(iii)  
a set of events be depicted by the vertices of G and hence also represented by 
V, each event having a finite set of mutually exclusive outcomes 
(iv)  
Ei be a variable which can take any of the outcomes 
j
ie of the event 
              , 
1...
i
i j
n
=
 
(v)  
P be a probability distribution over the combinations of events, i.e. P  
consists of all possible
i
i
P
E
∈
⎛
⎞
⎜
⎟
⎝
⎠
VI
. 
 
Let C be the following set of constraints: 
 
(2i) 
the elements of P sum to unity. 
(2ii) 
for each event i with a set of parents Mi  there  are associated conditional 
 
probabilities 
|
i
i
j
j M
P E
E
∈
⎛
⎞
⎜
⎟
⎜
⎟
⎝
⎠
I
 for each possible outcome that can 
be assigned to 
iE and 
j
E . 
(2iii) 
those independence relationships implied by d-separation in the directed 
 
acyclic graph. 
 
Then 
=
N
G,P,C  is a causal network if P satisfies C. 
In a Bayesian network the property of d-separation identifies all the constraints as 
independencies and dependencies. In classical Bayesian network theory a prior distri-
bution must be specified in order to apply the updating algorithms developed, for ex-
ample, by Pearl [4] or Lauritzen and Spiegalhalter [5].   By working with the same set 
of constraints as those implied by d-separation, the MaxEnt formalism provides a 
means of determining the prior distribution when information is missing. The author 
has previously shown that the MaxEnt model with complete information is identical 
to the Bayesian model and has used this property to estimate the optimal prior  

 
11   Toward a Generalized Bayesian Network 
283 
distribution when information is missing. We now show that the MaxEnt model is not 
dependent on the Bayesian model for a class of Bayesian networks.  
11.3   A Generalized Bayesian Network with Maximum Entropy 
Consider the knowledge domain represented by a set, K, of multivalued events
ia .  
Associated with each event is a variable
v
E . The general state S of the causal tree is 
the conjunction 
v
v
E
∈VI
. A particular state is obtained by assigning some 
j
ve  to 
each
v
E . It is assumed that the probability of a state is non-zero. The number of states 
NS in the tree is given by: 
S
i
i
N
n
∈
=∏
V
 
where ni is the number of values possessed by the ith event. States are numbered from 
1,...,
S
N  and denoted by 
:
1,...,
i
S
S
i
N
=
, and the probability of a state is denoted 
by
(
)
i
P S
. To determine a minimally prejudiced probability distribution P, using the 
maximum entropy formalism, we maximize 
1
(
)ln
(
)
s
N
i
i
i
H
P S
P S
=
= −∑
                                            (11.1) 
in accordance with the constraints implied by d-separation. These constraints are 
given in the form of marginal or conditional probabilities that represent the current 
state of knowledge of the domain.  
Let a sufficient set of constraints be denoted by C, where each constraint 
j
C ∈C . 
Each constraint is assigned a unique Lagrange multiplier
j
λ , where j represents the 
subscripts corresponding to the events on the associated edge. For the edge 
1
1
,
a b
, 
the Lagrange multipliers are (
)
(
)
(
)
1
1
1
2
1
1
1
1
1
1
,
, 
,
,...,
,
 
m
p
b a
b a
b
a
λ
λ
λ
where event 
1a  
has p outcomes and event 
1b has m outcomes. Without loss of generality we consider 
the constraints arising from a typical edge 
1
1
,
a b
thus:  
(
)
(
)
|
,
          
1,...,
;    
1,...,
j
i
b
a
j
i
S
P e
e
b a
i
N
j
m
β
=
=
=
                    (11.2) 
Since P is a probability distribution we also require the normalization constraint: 
1
(
)
1
s
N
i
i
P S
=
=
∑
                                                           (11.3) 

284 
D.E. Holmes 
The Lagrange multiplier
0
λ is associated with the sum to unity. Applying the the-
ory of Lagrange multipliers transforms the problem into that of maximizing: 
 
j
j
all j
F
H
C
λ
=
−∑
                                                     (11.4) 
By partially differentiating (11.4) with respect to 
(
)
i
P S
 and
j
λ , we see that the 
contribution to the expression for a maximum from H is given by: 
(
)
1
ln
(
)     
1,...,
i
S
P S
i
N
−
+
=
                                                 (11.5) 
Similarly, the contribution made by each causal constraint and the sum to unity to 
the expression for a maximum is given by   
1,...,
0
(
)
j
s
j
j
C
C
i
i
N
C
P S
λ
∈
=
∂
−
=
∂
∑
                                                      (11.6) 
resulting in a combined expression: 
(
)
1,...,
1
ln
(
)
0
(
)
j
s
j
i
j
C
C
i
i
N
C
P S
P S
λ
∈
=
∂
−
+
−
=
∂
∑
                                       (11.7) 
and hence 
(
)
1
1,...,
(
)
exp
(
)
j
s
j
i
j
C
C
i
i
N
C
P S
e
P S
λ
−
∈
=
∂
⎛
⎞
=
−
⎜
⎟
∂
⎝
⎠
∏
                                       (11.8) 
In order to further consider the probability of a state, as given in (11.8), we first need 
to transform the given constraints into expressions containing the sums of probabili-
ties of states. These causal constraints given in (11.2) are thus expressed in the form: 
(
)
(
)
(
)
(
)
(
)
1
1
1
1
1
,
,
0
j
i
j
i
x
y
x X
y Y
b a
P S
b a
P S
β
β
∈
∈
−
−
=
∑
∑
                        (11.9)   
where 
(
)
(
)
1
1
|
i
j
x
a
b
x
X
x
P S
P e e
⎧
⎫
=
=
⎨
⎬
⎩
⎭
∑
 and 
           
(
)
(
)
1
1
1
|
k
m
i
k
y
a
b
y
k
k
j
Y
y
P S
P e e
=
=
≠
⎧
⎫
⎪
⎪
=
=
⎨
⎬
⎪
⎪
⎩
⎭
∑
∑
 
 

 
11   Toward a Generalized Bayesian Network 
285 
This defines a family of constraint equations for the arbitrary edge
1
1
,
a b
. The root 
node is a special case of equations (11.2) since the information is given in the form of 
marginal probabilities and hence they need not be considered separately.  
Substituting (11.8) into (11.9) gives: 
(
)
(
)
(
)
(
)
(
)
1
1
1
1
1
1
1
1
,
,
1
,
exp
,
exp
0
(
)
(
)
j
i
j
i
j
j
j
j
j
i
j
i
b a
b a
x X
y Y
C
C
C
C
x
y
C
C
b a
b a
P S
P S
β
λ
β
λ
∈
∈
∈
∈
⎛
⎞
∂
∂
⎛
⎞
−
−
−
−
=
⎜
⎟
⎜
⎟
⎜
⎟
∂
∂
⎝
⎠
⎝
⎠
∑
∑
∏
∏
 
(11.10) 
 
Now consider the probability of the state with event 
1a  instantiated with its ith 
outcome and event 
1b with its jth outcome, denoted by (
)
1
1 ,
j
i
b
a
P S
. We see that 
when x
X
∈
, (
)
1
1 ,
j
i
b
a
P S
 contains the expression: 
(
)
(
)
(
)
(
)
(
)
1
1
1
1
exp
,
1
,
j
i
j
i
b a
b a
λ
β
−
−
 
Similarly, when y
Y
∈
,  (
)
1
1 ,
j
i
b
a
P S
  contains the terms: 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
1
1
1
1
1
1
1
1
exp
,
exp
,
,
k
m
j
i
k
i
k
i
k
b a
b a
b a
λ
λ
β
=
−
=
−
−
−
∏
 
Hence (
)
1
1 ,
j
i
b
a
P S
 contains the terms 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
1
1
1
1
1
1
1
1
1
1
exp
,
1
,
exp
,
,
k
m
j
i
j
i
k
i
k
i
k
k
j
b a
b a
b a
b a
λ
β
λ
β
=
−
=
≠
−
−
−
−
∏
 
arising from the edge 
1
1
,
a b
. Re-arranging gives 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
1
1
1
1
1
1
1
1
exp
,
exp
,
,
k
m
j
i
k
i
k
i
k
b a
b a
b a
λ
λ
β
=
−
=
−
−
−
∏
 
Since this constraint is typical we see that for all states belonging to X
x
∈
: 
(
)
(
)
(
)
(
)
(
)
1
1
1
1
exp
,
1
,
j
i
j
i
b a
b a
λ
β
−
−
                               (11.11) 
and for all states belonging toY
y
∈
: 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
1
1
1
1
1
1
1
1
exp
,
exp
,
,
k
m
j
i
k
i
k
i
k
b a
b a
b a
λ
λ
β
=
−
=
−
−
−
∏
         (11.12) 

286 
D.E. Holmes 
From equations (11.11) and (11.12) we see that (11.10) becomes: 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
1
1
,
1
1
1
1
,
1
1
1
1
1
1
,
1
1
,
exp
,
1
,
exp
(
)
,
exp
0
(
)
j
i
j
j
i
b
a
j
i
j
j
i
b
a
j
j
i
j
i
b
a
x X C
C
x
j
j
i
b
a
y Y C
C
y
C
b a
b a
P S
C
b a
P S
λ
β
λ
β
λ
⎛
⎞
⎜
⎟
⎝
⎠
⎛
⎞
⎜
⎟
⎝
⎠
∈
∈−
∈
∈−
∂
⎛
⎞
−
−
−
−
⎜
⎟
∂
⎝
⎠
⎛
⎞
∂
−
=
⎜
⎟
⎜
⎟
∂
⎝
⎠
∑
∏
∑
∏
C
C
 
and hence 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
1
1
,
1
1
1
1
,
1
1
1
1
,
1
1
1
1
,
,
exp
(
)
exp
,
1
,
exp
(
)
j
i
j
j
i
b
a
j
i
j
j
i
b
a
j
j
i
b
a
y Y C
C
y
j
i
j
j
i
b
a
x X C
C
x
C
b a
P S
b a
C
b a
P S
β
λ
λ
β
λ
⎛
⎞
⎜
⎟
⎝
⎠
⎛
⎞
⎜
⎟
⎝
⎠
∈
∈−
∈
∈−
⎛
⎞
∂
−
⎜
⎟
⎜
⎟
∂
⎝
⎠
−
=
∂
⎛
⎞
−
−
⎜
⎟
∂
⎝
⎠
∑
∏
∑
∏
C
C
 
(11.13) 
This expression enables us to update Lagrange multipliers using an iterative algo-
rithm. However, as we show in the next section, we can solve for the Lagrange multi-
pliers algebraically, thus producing a solution identical to that given in earlier papers, 
using techniques outside of the MaxEnt formalism. See for example, Holmes [6] 
11.4   Solving for the Lagrange Multipliers: Example 
For the purposes of illustration we consider a three valued causal binary tree with 
three nodes A, B and C.  Let 
{
}
{
}
{
}
1
2
3
1
2
3
1
2
3
, 
, 
a
a a
a
b
b b
b
c
c c
c
E
e e e
E
e e e
E
e e e
=
=
=
 
denote the outcomes of events a, b and c respectively, which are mutually exclusive 
and collectively exhaustive. The required information, given by conditional probabili-
ties associated with each outcome, is as follows: 
 
26
0
(
)
1   (constraint 0)
(
)
( ); 
1,2;  (constraints 1 and 2)
(
|
)
(
), (
|
)
(
);  
1,2,3; 
1,2; (constraints 3 - 8)
(
|
)
(
), (
|
)
(
);  
1,2,3; 
i
i
i
a
i
j
i
j
i
b
a
j
i
b
a
j
i
j
i
j
i
c
a
j
i
c
a
j
i
P S
P e
a
i
P e
e
b a
P e
e
b a
i
j
P e
e
c a
P e
e
c a
i
j
α
β
β
β
β
=
=
=
=
=
=
=
=
=
=
=
∑
1,2; (constraints 9 - 14)
=
 
(11.14) 

 
11   Toward a Generalized Bayesian Network 
287 
This system can be in any of 27 states, labeled 0-26, as follows: 
 
1 1 1
0
a
:
b c
S
e e e
1 1
2
1
a
:
b c
S
e e e
1 1
3
2
a
:
b c
S
e e e
1
2 1
3
a
:
b
c
S
e e e
1
2
2
4
a
:
b
c
S
e e e
1
2
3
5
a
:
b
c
S
e e e
1
3 1
6
a
:
b
c
S
e e e
1
3
2
7
a
:
b
c
S
e e e
1
3
3
8
a
:
b
c
S
e e e
 
 
 
 
 
The remaining states are similarly defined but with 
2
a
a
E
e
=
 for states 9 – 17 and 
3
a
a
E
e
=
 for states 18 – 26.  Each constraint in (11.14) can be expressed in terms of 
state probabilities, as in (11.9). For example, constraint 3 gives: 
(
)
1 1
1 1
0,1,2
3,4,5
6,7,8
1
(
)
(
)
(
)
(
)
(
)
0
i
i
i
i
i
i
b a
P S
b a
P S
P S
β
β
=
=
=
⎛
⎞
−
−
+
=
⎜
⎟
⎝
⎠
∑
∑
∑
    (11.15) 
In (11.14), sets X and Y as defined in (11.9), contain states 3,4,5 and 6,7,8 respec-
tively. Using the equation for probability of a state given by (11.6) together with 
(11.15) enables us to find the values of all the Lagrange multipliers. Expanding 
(11.15) and simplifying gives an expression for 
3
exp(
)
λ
−
in terms of known infor-
mation, together with certain unknown Lagrange multipliers thus: 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
1 1
3
1 1
4
5
6
4
5
4
6
5
6
(
)
exp(
)
1
(
)
1
exp
exp
exp
exp
exp
exp
exp
1
exp
exp
b a
b a
β
λ
β
λ
λ
λ
λ
λ
λ
λ
λ
λ
⎛
⎞
−
=
×
⎜
⎟
−
⎝
⎠
⎛
⎞
+
−
+
−
+
−
+
−
−
+
−
−
⎜
⎟
⎜
⎟
+
−
+
−
⎝
⎠
 
(11.16) 
Following the same procedure but with 
(
)
2
1
2
1
3,4,5
0,1,2
6,7,8
1
(
)
(
)
(
)
(
)
(
)
0
i
i
i
i
i
i
b a
P S
b a
P S
P S
β
β
=
=
=
⎛
⎞
−
−
+
=
⎜
⎟
⎝
⎠
∑
∑
∑
     (11.17)              
leads to 
 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
2
1
4
2
1
3
5
6
3
5
3
6
5
6
(
)
exp(
)
1
(
)
1
exp
exp
exp
exp
exp
exp
exp
1
exp
exp
b a
b a
β
λ
β
λ
λ
λ
λ
λ
λ
λ
λ
λ
⎛
⎞
−
=
×
⎜
⎟
−
⎝
⎠
⎛
⎞
+
−
+
−
+
−
+
−
−
+
−
−
⎜
⎟
⎜
⎟
+
−
+
−
⎝
⎠
       
(11.18) 
 
Using equations (1.16) and (1.17) we find, by factorization and substitution, that: 
 
(
)
1 1
2
1
3
3
1 1
2
1
(
)
(
)
exp(
)
1
1
exp(
)
1
(
)
1
(
)
b a
b a
b a
b a
β
β
λ
λ
β
β
⎛
⎞⎛
⎞
−
=
+
+
−
⎜
⎟⎜
⎟
−
−
⎝
⎠⎝
⎠
          (11.19) 

288 
D.E. Holmes 
hence 
(
)(
)
1 1
3
1 1
2
1
2
1
1 1
(
)
exp(
)
(
) (
)
1
(
) 1
(
)
b a
b a
b a
b a
b a
β
λ
β
β
β
β
−
=
+
−
−
                      (11.20) 
and so 
1 1
3
3 1
(
)
exp(
)
(
)
b a
b a
β
λ
β
−
=
                                                 (11.21) 
The remaining Lagrange multipliers are found similarly, and so the probability of 
each state can be determined. 
11.5   Remarks 
For the class of Bayesian networks discussed here, the non-linear independence con-
straints implied by d-separation are preserved by the maximum entropy formalism 
and do not need to be explicitly stated.  
Having shown how to find the Lagrange multipliers and thus the probability of 
each state, methods previously developed by Holmes and Rhodes [1] can be used to 
determine missing information since these depend only on the maximum entropy 
formalism. We have seen in this paper how to derive expressions for estimating miss-
ing information in tree-like Bayesian networks without equating the maximum en-
tropy and Bayesian models. 
The next step in the current project will be to develop the theory required to deal 
with the non-linear constraints inherent in singly connected networks without  
recourse to methods outside of the maximum entropy formalism. 
References 
1. Holmes, D.E., Rhodes, P.C.: Reasoning with Incomplete Information in a Multivalued Mul-
tiway Causal Tree Using the Maximum Entropy Formalism. International Journal of Intelli-
gent Systems 13(9), 841–859 (1998) 
2. Holmes, D.E.: Maximizing Entropy for Inference in a Class of Multiply Connected Net-
works. In: The 24th Conference on Maximum Entropy and Bayesian methods, American 
Institute of Physics (2004) 
3. Markham, M.J., Rhodes, P.C.: Maximizing Entropy to deduce an Initial Probability Distri-
bution for a Causal Network. International Journal of Uncertainty, Fuzziness and Knowl-
edge-based Systems 7(1), 63–80 (1999) 
4. Pearl, J.: Probabilistic Reasoning in Intelligent Systems. In: Networks of Plausible Infer-
ence. Morgan Kaufmann Publishers, San Francisco (1988) 
5. Lauritzen, S.L., Spiegelhalter, D.J.: Local Computations with Probabilities on Graphical 
Structures and their Applications to Expert Systems. J. Royal Statist. Soc. B 50(2), 154–227 
(1988) 
6. Holmes, D.E.: Efficient Estimation of Missing Information in Multivalued Singly Con-
nected Networks Using Maximum Entropy. In: von der Linden, W., Dose, V., Fischer, R., 
Preuss, R. (eds.) Maximum Entropy and Bayesian Methods, pp. 289–300. Kluwer Aca-
demic, Dordrecht (1999) 

12
A Survey of First-Order Probabilistic Models
Rodrigo de Salvo Braz⋆, Eyal Amir, and Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
Summary. There has been a long standing division in Artiﬁcial Intelligence between
logical and probabilistic reasoning approaches. While probabilistic models can deal
well with inherent uncertainty in many real-world domains, they operate on a mostly
propositional level. Logic systems, on the other hand, can deal with much richer rep-
resentations, especially ﬁrst-order ones, but treat uncertainty only in limited ways.
Therefore, an integration of these types of inference is highly desirable, and many ap-
proaches have been proposed, especially from the 1990s on. These solutions come from
many diﬀerent subﬁelds and vary greatly in language, features and (when available at
all) inference algorithms. Therefore their relation to each other is not always clear, as
well as their semantics. In this survey, we present the main aspects of the solutions
proposed and group them according to language, semantics and inference algorithm. In
doing so, we draw relations between them and discuss particularly important choices
and tradeoﬀs.
For decades after the ﬁeld of Artiﬁcial Intelligence (AI) was established, its most
prevalent form of representation and inference was logical, or at least symbolic
representations that were in a deeper sense equivalent to a fragment of logic.
While highly expressive, this type of model lacked a sophisticated treatment of
degrees of uncertainty, which permeates real-world domains, especially the ones
usually associated with intelligence, such as language, perception and common
sense reasoning.
In time, probabilistic models became an important part of the ﬁeld, incor-
porating probability theory into reasoning and learning AI models. Since the
1980s the ﬁeld has seen a surge of successful solutions involving large amounts
of data processed from a probabilistic point of view, applied especially to Natural
Language Processing and Pattern Recognition.1
⋆Currently at the Computer Science Division of University of California, Berkeley.
1 Strictly speaking, this tendency has not been only probabilistic, including machine
learning methods such as neural networks that did not claim to be modeling prob-
abilities. However, a link to probabilities can usually be found and the methods are
used in similar ways.
D.E. Holmes and L.C. Jain (Eds.): Innovations in Bayesian Networks, SCI 156, pp. 289–317, 2008.
springerlink.com
c
⃝Springer-Verlag Berlin Heidelberg 2008

290
R. de Salvo Braz, E. Amir, and D. Roth
This success, however, came with a price. Typically, probabilistic models
are less expressive and ﬂexible than logical or symbolic systems. Usually, they
involve propositional, rather than ﬁrst-order representations. When required,
more expressive, higher level representations are obtained by ad hoc manipula-
tions of lower level, propositional systems.
Starting in the 1970s but having greatly increased from the 1990s on, a line
of research sought to integrate those two important modes of reasoning. In this
chapter we give a survey of this research, and try to show some general lines
separating diﬀerent approaches.
We have roughly divided this research in diﬀerent stages. The 1970s and 1980s
saw great interest in expert systems [1, 2]. As these systems were applied to real-
world domains, coping with uncertainty became more desirable, giving rise to the
certainty factors approach, which uses rules with attached numbers (representing
degrees of certainty) that get propagated to conclusions during inference.
Certainty factors systems did not have clear semantics, and often produced
surprising and nonintuitive results [3]. The search for clearer semantics for rules
with varying certainty gave rise, among other things, to approaches such as
Bayesian Networks. These however were essentially propositional, and thus had
much less expressivity than logic systems.
The search for clear semantics of probabilities in logic systems resulted in
works such as Nilsson [4], Bacchus [5] and Halpern [6], which laid out the basic
theoretic principles supporting probabilistic logic. These works, however, did not
include eﬃcient inference algorithms.
Works aiming at eﬃcient inference algorithms for ﬁrst-order probabilistic in-
ference (FOPI) can be divided in two groups, which Pearl [3] calls extensional
and intensional systems. In the ﬁrst one, statements in the language are more
procedural in nature, standing for licenses for propagating truth values that
have been generalized from true or false to a gray scale of varying degrees of
certainty. In the second group, statements place restrictions on a probability
distribution on possible worlds. They do not directly correspond to computing
operations, nor can they typically be taken into account without regard to other
rules (that is, inference is not completely modular). Eﬃcient algorithms have to
be devised for these languages that preserve their semantics while doing better
than considering the entire model at every step.
Among intensional models, there are further divisions regarding the type of
algorithm proposed. One group proposes inference rules similar to the ones used
in ﬁrst-order logic inference (for example, modus ponens). A second one com-
putes, in more or less eﬃcient manners, the possible derivations of a query given a
model. A third one uses sampling to answer queries about a model. A fourth and
more prevalent group constructs a (propositional) graphical model (Bayesian or
Markov networks, for example) that answers queries, and uses general graphical
model inference algorithms for solving them. Finally, a ﬁfth one proposes lifted
algorithms that directly operate on ﬁrst-order representations in order to derive
answers to queries.
We now present these stages in more detail.

12 A Survey of First-Order Probabilistic Models
291
12.1
Expert Systems and Certainty Factors
Expert systems are based on rules meant to be applied to existing facts, produc-
ing new facts as conclusions [1]. Typically, the context is a deterministic one in
which facts and rules are assumed to be certain. Uncertainties from real-world
applications are dealt with during the modeling stage where necessary (and often
heavy-handed) simpliﬁcations are performed.
Certainty factors were introduced for the purpose of allowing uncertain rules
and facts, making for more direct and accurate modeling. A rule (A ←B) : c1,
with c1 ∈[0, 1], indicates that we can conclude A with a degree of certainty of
c1 × c2, if B is known to be true with a degree of certainty c2 ∈[0, 1]. Given a
collection of rules and facts, inference is performed by propagating certainties in
this fashion. There are also combination rules for the cases when more than one
rule provide certainty factors for the same literal.
A paradigmatic application of certainty factors is the system MYCIN [7], an
expert system dedicated to diagnosing diseases based on observed symptoms.
Clark & McCabe [8] describe how to use Prolog with predicates containing
an extra argument representing its certainty and being propagated accordingly.
Shapiro [9] describes a Prolog interpreter that does the same but in a way im-
plicit in the interpreter and language, rather than as an extra argument.
One can see that certainty factors have a probabilistic ﬂavor to them, but
formally they are not taken to be probabilistic. This is for good reason: should
we interpret them as probabilities, results would be inconsistent with probabil-
ity theory. Heckerman [10] and Lucas [11] discuss situations in which certainty
factor computations can and cannot be correctly interpreted probabilistically.
One reason they cannot is the incorrect treatment of bidirectional inference: two
certainty factor rules (A ←B) : c1 and B : c2 imply nothing about inference
from A to B, while P(A|B) and P(B) do place constraints on P(B|A). These
problems are further discussed in Pearl [3].
12.2
Probabilistic Logic Semantics
The semantic limitations of certainty factors is one of the motivations for deﬁning
precise semantics for probabilistic logics, but such investigations date from at
least as far back as Carnap [12].
One of the most inﬂuential AI works in this regard is Nilsson [4] (a similar
approach is given by Hailperin [13]). Nilsson establishes a systematic way of
determining the probabilities of logic sentences in a query set, given the proba-
bilities of logical sentences in an evidence set. To be more precise, the method
determines intervals of probabilities to the query sentences, since in principle
the evidence set may be consistent with an entire range of point probabilities
for them. For example, knowing that A is true with probability 0.2 and B with
probability 0.6 means that A ∧B is true with probability in [0, 0.2], depending
on whether A and B are mutually exclusive, or A →B, or anything in between.

292
R. de Salvo Braz, E. Amir, and D. Roth
Given a set of sentences L, Nilsson considers the equivalence classes of possible
worlds that assign the same truth values to the sentences in L (that is, as far
as L is concerned, all possible worlds in the same class are the same). Formally,
Nilsson’s system is based on the following linear problem:
Π = V P
0 ≤Πj ≤1
0 ≤Pi ≤1

i
Pi = 1
where Π is the vector of probabilities of sentences in both query and evidence
sets, P the vector of probabilities of possible worlds equivalence classes, and V
is a matrix with Vij = 1 if sentence j is true in possible world set i, and 0
otherwise. The probabilities of sentences in the knowledge base are incorporated
as constraints in this system as well, and linear programming techniques can be
used to determine the probability of novel sentences. However, as Nilsson points
out, the problem becomes intractable even with a modest number of sentences,
since all possible worlds equivalence classes need to be enumerated and this is
an intractable problem. Therefore this framework cannot be directly used in
practice.
Placing the probabilities on the possible worlds, as does Nilsson, makes it
easy to express subjective probabilities such as “Tweety ﬂies with probability
0.9” (that is, the sum of probabilities of all possible worlds in which Tweety
ﬂies is 0.9). However, probabilistic knowledge can also express statistical facts
about the domain such as “90% of birds ﬂy” (which says that, in each possible
world, 90% of birds ﬂy). Bacchus [5] provides an elaborate probabilistic logic
semantics that includes both types of probabilistic knowledge, making it possible
to use both statements above, as well as statements mixing them, such as “There
is a probability of 0.8 that 90% of birds ﬂy.” He also discusses the interplay
between the two types, namely the question of when it is correct to use the fact
that “90% of birds ﬂy” in order to assume that “a randomly chosen bird ﬂies
with probability 0.9,” a topic that has both formal and philosophical aspects.
Halpern [6] elaborates on the axiomatization of Bacchus, taking probabilities
to be real numbers (Bacchus did not), and is often cited as a reference for this
semantics with two types of probabilities. In subsequent work, the subjective
type probability has been much more developed and used, and is also the type
involved in propositional graphical models.
Fagin, Halpern, Meggido [14] present a logic to reason about probabilities,
including their addition and multiplication by scalars. Other works discussing
the semantics of probabilities on ﬁrst-order structures are [15, 16, 17].
12.3
Extensional Approaches
Somewhat parallel to the works on the semantics of probabilistic logic, a diﬀerent
line of research proposed logic reasoning systems incorporating uncertainty in

12 A Survey of First-Order Probabilistic Models
293
the explicit form of probabilities (as opposed to certainty factors). These systems
often stem from the ﬁelds of logic programming and deductive databases, and
ﬁt into the category described by [3] as extensional systems, that is, systems in
which rules work as “procedural licenses” for a computation step instead of a
constraint on possible probability distributions. Most of these systems operate on
a collection of rules or clauses that propagate generalized truth values (typically,
a value or interval in [0, 1]).
Kiefer and Li [18] provide a probabilistic interpretation and a ﬁxpoint seman-
tics to Shapiro [9]. W¨uthrich [19] elaborates on their work, taking into account
partial dependencies between clauses. For example, if each of them atoms a, b
and c has a prior probability of 0.5 and we have two rules p ←a ∧b and
p ←b ∧c, Kiefer and Li will assume the rules independent and assign a prob-
ability 0.25 + 0.25 −0.25 ∗0.25 = 0.4375 to p. W¨uthrich’s system, however,
takes into account the fact that b is shared by the clauses and computes instead
0.25 + 0.25 −0.53 = 0.375 (that is, it avoids double counting of the case where
the two rules ﬁre at the same time, which occurs only when the three atoms are
true at once).
One of the most inﬂuential works within the extensional approach is Ng and
Subrahmanian [20]. Here, a logic programming system uses generalized truth
values in the form of intervals of probabilities. They deﬁne probabilistic logic
program as sets of p-clauses of the form
A : μ ←F1 : μ1 ∧. . . Fn : μn,
where A is an atom, F1, . . . , Fn are basic formulas (conjunctions or disjunctions)
and μ, μ1, . . . , μn are probability intervals. A clause states that if the probability
of each formula Fi is in μi, then the probability of A is in μ. For example, the
clause
path(X, Y ) : [0.8, 0.95] ←a(X, Z) : [1, 1] ∧path(Z, Y ) : [0.85, 1]
states that, if a(X, Z) is certain (probability in interval [1, 1] and therefore 1)
and path(Z, Y ) has probability in [0.85, 1], then path(X, Y ) has probability in
[0.8, 0.95]. Probabilities of basic formulas Fi are determined from the probabil-
ity intervals of their conjuncts (or disjuncts) by taking into account the possible
correlations between them (similarly to what Nilsson does). The authors present
a ﬁxpoint semantics where clauses are repeatedly applied and probability inter-
vals successively narrowed up to convergence. They also develop a model theory
determining what models (sets of distributions on possible worlds) satisfy a
probabilistic logic program, and a refutation procedure for querying a program.
Lakshmanan and Sadri [21] propose a system similar to Ngo and Subrahma-
nian, while keeping track of both the probability of each atom as well of its
negation. Additionally, it uses conﬁgurable independence assumptions for dif-
ferent clauses, allowing the user to declare whether two atoms are independent,
mutually exclusive, or even the lack of an assumption (as in Nilsson). Laksh-
manan [22] separates the qualitative and quantitative aspects of probabilistic
logic. Dependencies between atoms are declared in terms of the boolean truth

294
R. de Salvo Braz, E. Amir, and D. Roth
values of a set of support atoms. Only later is a distribution assigned to the sup-
port atoms, consequently deﬁning distributions on the remaining atoms as well.
The main advantage of the approach is the possibility of investigating diﬀerent
total distributions, based on distributions on the support set, without having
to recalculate the relationship between atoms and support set. The algorithms
works in ways similar to Ngo and Haddawy [23] and Lakshmanan and Sadri [21],
but propagates support set conditions rather than probabilities. Support sets are
also a concept very similar to the hypotheses used in Probabilistic Abduction by
Poole [24] (see next section).
12.4
Intensional Approaches
We now discuss intensional approaches to probabilistic logic languages, where
statements (often in the form of rules) are interpreted as restrictions on a globally
deﬁned probability distribution. This probability distribution is over all possible
worlds or, in other words, on assignments to the set of all possible random
variables in the language. Statements typically pose constraints in the form of
conditional probabilities, and often also as conditional independence relations.
(As mentioned in Sect. 12.2, another possibility would be statistical constraints,
but this has not been explored in any works to our knowledge.)
The algorithms in intensional approaches, when available, are arguably more
complex than extensional approaches, since their steps do not directly correspond
to the application of rules in the language and need to be consistent with the
global distribution while being as local as possible (for eﬃciency reasons).
We cover ﬁve diﬀerent types of intensional approaches: deduction rules, ex-
haustive computation of derivations, sampling, Knowledge Based Model Con-
struction (KBMC) and Lifted inference.
12.4.1
Deduction Rules
Classical logic deduction systems often work by receiving a model speciﬁed in a
particular language and using deduction rules to derive new statements (guar-
anteed to be true) from subsets of previous statements. Some work has been
devoted to devising similar systems when the language is that of probabilistic
logic.
This method is particularly challenging in probabilistic systems because prob-
abilistic inference is not as modular as classical logical inference. For example,
while the logical knowledge of A →B allows us to deduce B given that A ∧ϕ is
true for any formula ϕ, knowing P(B|A) in itself does not tell us anything about
P(B|A ∧ϕ). In principle, one needs to consider all available knowledge when
establishing the conditional probability of B. Classical logic reasoning shows a
modularity that is harder to achieve in a probabilistic setting.
One way of making probabilistic inference more modular is to use knowledge
about conditional independencies between random variables. If we know that B is
independent of any other random variable given A, then we know that P(B|A∧ϕ)

12 A Survey of First-Order Probabilistic Models
295
is equal to P(B|A) for any ϕ. This has been the approach of graphical models
such as Bayesian and Markov networks [3], where independencies are represented
by the structure of a graph over the set of random variables.
The computation steps of speciﬁc inference algorithms for graphical models
(such as Variable Elimination [25]) could be cast as deduction rules, much like in
classical logic. However this is not traditionally done, mostly because inference
rules are typically described in a logic-like language and graphical models are
not. When dealing with a ﬁrst-order probabilistic logic language, however, this
approach becomes more natural.
Luckasiewicz [26] uses inference rules for solving trees of probabilistic con-
ditional constraints over basic events. These trees are similar to Bayesian net-
works, with each node being a random variable and each edge being labeled by
a conditional probability table. However, these trees are not meant to encode in-
dependence assumptions. Besides, conditional probabilities can also be speciﬁed
in intervals.
Frisch and Haddawy [27] present a set of inference rules for probabilistic
propositional logic with interval probabilities. They characterize it as an anytime
system since inference rules will increasingly narrow those intervals. They also
provide more modular inference by allowing statements on conditional indepen-
dencies of random variables, which are used by certain rules to derive statements
based on local information.
Koller and Halpern [28] investigate the use of independence information for
FOPI based on inference rules. They use this notion to discuss the issue of sub-
stitution in probabilistic inference. While substitution is fundamental to classical
logic inference, it is not sound in general in a probabilistic context. For example,
inferring P(q(A)) = 1
3 given ∀P(q(X)) = 1
3 is not sound. Consider three pos-
sible worlds w1, w2, w3 containing the three objects o1, o2, o3 each, where q(oi)
is 1 in wi and 0 otherwise. If each possible world has a probability 1
3 of being
the actual world, then ∀P(q(X)) = 1
3 holds. However, if A refers to oi in each
wi, then P(q(A)) = 1. While this problem can be solved by requiring constants
to be rigid designators (that is, each of them refers to the same object in all
worlds), the authors argue that this is too restrictive. Their solution is to use in-
formation on independence. They show that when the statements ∀P(q(X)) = 1
3
and x = A are independent, one can derive P(q(A)) = 1
3. Finally, they discuss
the topic of using statistical probabilities as a basis for subjective ones (the two
types discussed by Bacchus [5] and Halpern [6]) based on independencies.
12.4.2
Exhaustive Computation of Derivations
Another type of intensional system is the one in which the available algorithms
exhaustively compute the set of derivations or proofs for a query, in the same
way proofs are found for queries in logic programming. However, while in logic
programming it is often only necessary to ﬁnd one proof for a certain query,
in probabilistic models all proos will typically inﬂuence the query’s result, and
therefore need to be computed.

296
R. de Salvo Braz, E. Amir, and D. Roth
Riezler [29] presents a probabilistic account of Constraint Logic Programs
(CLPs) [30]. In regular logic programming, the only constraints over logical vari-
ables are equational constraints coming from uniﬁcation. CLPs generalize this by
allowing other constraints to be stated over those variables. These constraints are
managed by special-purpose constraint solvers as the derivation proceeds, and
failure in satisfying a constraint determines failure of the derivation. Probabilis-
tic Constraint Logic Programs (PCLPs) are a stochastic generalization of CLPs,
where clauses are annotated with a probability and chosen for the expansion of a
literal according to that probability, among the available clauses with matching
heads. The probability of a derivation is determined by the product of probabil-
ities associated to the stochastic choices. In fact, PCLPs are a generalization of
Stochastic Context-Free Grammars (SCFGs) [31], the diﬀerence between them
being that PCLP symbols have arguments in the form of logical variables with
associated constraints while grammar symbols do not. For this reason, PCLP
derivations can fail while SCFGs will always succeed. This presents a compli-
cation for PCLP algorithms because the probability has to be normalized with
respect to the sum of successful derivations only. It also makes the use of ef-
ﬁcient dynamic programming techniques such as the inside-outside algorithm
[32] not adequate for PCLPs, forcing us to compute all possible derivations of a
query. Riezler focuses on presenting an algorithm for learning the parameters of
a PCLP from incomplete data, in what is a generalization of the Baum-Welch
algorithm for HMMs [33].
Stochastic Logic Programs [34, 35] are very similar to PCLPs, restricting
themselves to regular logic programming (e.g., Prolog). This line of work is more
focused on the development of an actual system on top of a Prolog interpreter
and to be used with Inductive Logic Programming techniques such as Progol
[36]. Like Riezler, in [35] Cussens develops methods for learning parameters of
SLPs using Improved Interative Scaling [37] and the EM algorithm [38].
Luckasiewicz [39] presents a form of Probabilistic Logic Programming that
complements Nilsson’s [4] approach. Nilsson considers all equivalence classes of
possible worlds with respect to the given knowledge and builds a linear pro-
gram in order to assign probabilities to sentences. Luckasiewicz essentially does
the same by using logic programming for both determining the the equivalence
classes and the linear program.
Baral et al. [40] use answer set logic programming to implement a power-
ful probabilistic logic language. Its distinguishing feature is the possibility of
specifying observations and actions, with their corresponding implications with
respect to causality, as studied by Pearl [41]. However, the implementation, using
answer set Prolog, depends on determining all answer sets.
12.4.3
Sampling Approaches
Because building all drivations of a query given a program is very expensive,
approximation solutions become an attractive alternative.
Sato [42] presents PRISM, a full-featured Prolog interpreter extended with
probabilistic switches that can be used to encode probabilistic rules and facts.

12 A Survey of First-Order Probabilistic Models
297
These switches are special built-in predicates that randomly succeed or not,
following a speciﬁc probability distribution. They can be placed in the body of a
clause, which in consequence will succeed or not with the same distribution (when
the rest of the body succeeds). Therefore, multiple executions of the program
will yield diﬀerent random results that can be used as samples. A query can
then be answered by multiple executions which sample its possible outcomes.
Sato also provides a way of learning the parameters of the switches by using a
form of the EM algorithm [43].
BLOG [44] is a ﬁrst-order language allowing the speciﬁcation of a generative
model on a ﬁrst-order structure. It is similar in form to BUGS [45], a speciﬁcation
language for propositional generative models. The main distinction of BLOG is
its open world assumption; it does not require that the number of objects in
the world be set a priori, using instead a prior on this number and also keeping
track of identities of objects with diﬀerent names. BLOG computes queries by
sampling over possible worlds.
12.4.4
Knowledge Based Model Construction
We now present the most prominent family of models in the ﬁeld of FOPI mod-
els, Knowledge Based Model Construction (KBMC). These approaches work by
generating a propositional graphical model from a ﬁrst-order language speciﬁca-
tion that answers the query at hand. This construction is usually done in a way
speciﬁc to the query, ruling out irrelevant portions of the graph so as to increase
eﬃciency.
Many KBMC approaches use a ﬁrst-order logic-like speciﬁcation language, but
some use diﬀerent languages such as frame systems, parameterized fragments of
Bayesian networks, and description logics. Some build Bayesian networks while
others prefer Markov networks (and in one case, Dependency Networks [46]).
While KBMC approaches try to prune sections of underlying graphical models
which are irrelevant to the current query, there is still potentially much wasted
computation because they may replicate portions of the graph which require es-
sentially identical computations. For example, a problem may involve many em-
ployees in a company, and the underlying graphical model will contain a distinct
section with its own set of random variables for each of them (representing their
properties), even though all these sections have essentially the same structure.
Often the same computation will be repeated for each of those sections, while
it is possible to perform it only once in a generalized form. Avoiding this waste
is the object of Lifted First-Order Probabilistic Inference [47, 48], discussed in
Sect. 12.4.5.
The most commonly referenced KBMC approach is that of Breese [49, 50],
although Horsch and Poole [51] had presented a similar solution a year before.
[49] deﬁnes a probabilistic logic programming language, with Horn clauses anno-
tated by probabilistic dependencies between the clause’s head and body. Once
a query is presented, clauses are applied to it in order to determine the prob-
abilistic dependencies relevant to it. These dependencies are then used to form
a Bayesian network. Backward inference will generate the causal portion of the

298
R. de Salvo Braz, E. Amir, and D. Roth
network relative to the query; forward inference creates the diagnostic part. The
construction algorithm uses the evidence in order to decide when to stop expand-
ing the network – there is no need to generate portions that are d-separated from
the query by the evidence. In fact, this work covers not only Bayesian networks,
but inﬂuence diagrams as well, including decision and utility value nodes.
There are many works similar in spirit to [49] and diﬀering only in some
details; for example, the already mentioned Horsch and Poole [51], which also
uses mostly Horn clauses (it does allow for universal and existential quantiﬁers
over the entire clause body though) as the ﬁrst-order language. One distinction
in this work, however, is the more explicit treatment of the issue of combination
functions, used to combine distributions coming from distinct clauses with the
same head. One example of a combination function is noisy-or [3], which assumes
that the probability provided by a single clause is the probability of it making
the consequent true regardless of the other clauses. Suppose we have clauses
A ←B and A ←C in the knowledge base, the ﬁrst one dictating a probability
0.8 for A when B is true and the second one dictating a probability 0.7 for
A when C is true. Then the combination function noisy-or builds a Conditional
Probability Table (CPT) with B and C as parents of A, with entries P(A|B, C) =
{1 −0.2 × 0.3, 1 −0.8 × 0.3, 1 −0.2 × 0.7, 1 −0.8 × 0.7} = {0.94, 0.76, 0.86, 0.47}
for {(B = ⊤, C = ⊤), (B = ⊥, C = ⊤), (B = ⊤, C = ⊥), (B = ⊥, C = ⊥)},
respectively.
In ﬁrst-order models, noisy-or and other combination functions are especially
useful when a random variable has a varying number of parents, which makes
its CPT impossible to represent by a ﬁxed-dimensions table. A clause p ←
q(X), for example, determines that p depends on all instantiations of q(X), that
is, all instantiations of q(X) are parents of p. However, the number of such
instantiations depends on how many values X can take. Without knowing this
number, the only way of having a general speciﬁcation of p’s CPT is to have
a combination function on the instantiations of q(X). In fact, even when this
number is known it may be convenient to represent the CPT with a combination
function for compactness sake.
Charniak and Goldman [52] expand a deductive database and truth main-
tenance system (TMS) in order to deﬁne a language for constructing Bayesian
networks. The Bayesian networks come from the data-dependency network main-
tained by the TMS system, which is annotated with probabilities. There is also
a notion of combination functions. The authors choose not to expand logical
languages, justifying this choice by arguing that logic and probability do not
correspond perfectly, the ﬁrst being based on implication while the second on
conditioning.
Poole [24] deﬁnes Probabilistic Abduction, a probabilistic logic language
aimed at performing abduction reasoning. Probabilities are deﬁned only for a
set of predicates, called hypotheses (which is reminiscent of the support set in
[22]), while the clauses themselves are deterministic. When a problem has nat-
urally dependent hypotheses, one can redeﬁne them as regular predicates and
invent a new hypothesis to explain that dependence. While deterministic clauses

12 A Survey of First-Order Probabilistic Models
299
can seem too restrictive, one can always get the eﬀect of probabilistic rules by
using hypotheses as a condition of the rule (like switches in Sato’s PRISM [42]).
The language also assumes that the bodies of clauses with the same head are
mutually exclusive, and again this is not as restrictive as it might seem since
clauses with non-mutually exclusive bodies can be rewritten as a diﬀerent set of
clauses satisfying this. As in other works in this section, the actual computation
of probabilities is based on the construction of a Bayesian network. In [53], Poole
extends Probabilistic Abduction for decision theory, including both utility and
decision variables, as well as negation as failure.
Glesner and Koller [54] present a Prolog-like language that allows the dec-
laration of facts about a Bayesian network to be constructed by the inference
process. The computing mechanisms of Prolog are used to deﬁne the CPTs as
well, so they are not restricted to tables, but can be computed on the ﬂy. This
allows CPTs to be deﬁned as decision trees, for example, which provides a means
of doing automatic pruning of the resulting Bayesian network – if the evidence
provides information enough to make a CPT decision at a certain tree node,
the descendants of that node, along with parts of the network relevant to those
descendants only, do not need to be considered or built. The authors focus on
ﬂexible dynamic Bayesian networks that do not necessarily have the same struc-
ture at every time slice.
Haddawy [55] presents a language and construction method very similar to [51,
49]. However, he focuses on deﬁning the semantics of the ﬁrst-order probabilistic
logic language directly, and independently of the Bayesian network construction,
and proceeds to use it to prove the correctness of the construction method. Breese
[49] had done something similar by deﬁning the semantics of the knowledge base
as an abstract Bayesian network which does not usually get built itself in the
presence of evidence, and by showing that the Bayesian network actually built
will give the same result as the abstract one.
Koller and Pfeﬀer [56] present an algorithm for learning the probabilities of
noisy ﬁrst-order rules used for KBMC. They use the EM algorithm applied to the
Bayesian networks generated by the model, using incomplete data. This works
in the same way as the regular Bayesian network parameter learning with EM,
with the diﬀerence that many of the parameters in the generated networks are in
fact instances of the same parameter in a ﬁrst-order rule. Therefore, all updates
on these parameters must be accumulated in the original parameter.
Jaeger [57] deﬁnes a language for specifying a Bayesian network whose nodes
are the extensions of ﬁrst-order predicates. In other words, each node is the as-
signment to the set all atoms of a certain predicate. Needless to say, inference
in such a network would be extremely ineﬃcient since each node would have
an extremely large number of values. However, it oﬀers the advantage of mak-
ing the semantics of the language very clear (it is just the usual propositional
Bayesian network semantics – the extension of a predicate is just a propositional
variable with a very large number of values). The author proposes, like other
approaches here, to build a regular Bayesian network (with a random variable
per ground atom) for the purpose of answering speciﬁc queries. He also presents

300
R. de Salvo Braz, E. Amir, and D. Roth
a sophisticated scheme for combination functions, including the possibility of
their nesting.
Koller et al. [58, 59] deﬁne Probabilistic Relational Models (PRMs), a sharp
depart from the logical-probabilistic models that had been proposed until then
as solutions for FOPI models. Instead of adding probabilities to some logic-
like language, the authors use the formalism of Frame Systems [60] as a starting
point. The language of frames, similar also to relational databases, is less expres-
sive than ﬁrst-order logic, which is to the authors one of its main advantages
since ﬁrst-order logic inference is known to be intractable (which only gets worse
when probabilities are added to the mix). By using a language that limits its ex-
pressivity to what is most needed in practical applications, one hopes to obtain
more tractable inference, an argument commonly held in the Knowledge Rep-
resentation community [61]. In fact, Pfeﬀer and Koller had already investigated
adding probabilities to restricted languages in [62]. In that case, the language in
question was that of description logics.
The language of Frame Systems consists of deﬁning a set of objects described
by attributes – binary predicates relating an object to a simple scalar value
– or relations – binary predicates relating an object to another (or even self)
object. PRMs add probabilities to frame systems by establishing distributions on
attributes conditioned on other attributes (in the same object, or related object).
In order to avoid declaring these dependencies for each object, this is done at
a scheme level where classes, or template objects, stand for all instances of a
class. This scheme describes the attributes of classes and the relations between
them. Conditional probabilities are deﬁned for attributes and can name the
conditioning attributes via the relations needed to reach them.
As in the previous approaches, queries to PRMs are computed by generat-
ing an underlying Bayesian network. Given a collection of objects (a database
skeleton) and the relationships between them, a Bayesian network is built with a
random variable for each attribute in each object. The parents of these random
variables in the network are the ones determined by the relations in the particu-
lar database, and the CPT ﬁlled with the values speciﬁed at the template level.
An example of this process is shown in Fig. 12.1.
Note that the set of ancestors of attributes in the underlying network is de-
termined by the relations from one object to another. One could imagine an
attribute rating of an object representing a restaurant that depends on the at-
tribute training of the object representing its chef (related to it by the relation-
ship chef). In approaches following ﬁrst-order representations, chef would be a
binary predicate, and each of its instances a random variable. As a result, the
ancestors of rating would be the attributes training of all objects potentially
linked to the restaurant by the relationship chef, plus the random variables
standing for possible pairs in the relationship cook itself, resulting in a large
(and thus expensive) CPT. PRMs avoid this when they take data with a de-
ﬁned structure where the assignment to relations such as cook is known; in this
case, the random variables in the relationship chef would not even be included
in the network, and the attribute rating of each object would have a single

12 A Survey of First-Order Probabilistic Models
301
chef
Restaurant
rating
style
Worker
training
Place
zip code
Customer
preferences
patron
location
timpone’s.ratings
courier.ratings
joe.training
mary.training
timpone’s.style
courier.style
chef
Restaurant: 
Timpone’s
Worker: Joe
Place: Urbana
Customer: Jill
patron
location
chef
Restaurant: 
Courier
Worker: 
Mary
patron
location
jill.preferences
urbana.zip code
(a)
(b)
(c)
Fig. 12.1. (a) A PRM scheme showing classes of objects (rectangles), probabilistic
dependencies between their attributes (full arrows) and relationships (dashed arrows).
(b) A database skeleton showing a collection of objects, their classes and relationships.
(c) The corresponding generated Bayesian network.
ancestor. When relationships are not ﬁxed in advance, we have structural uncer-
tainty, which was addressed by the authors in [63]. These papers have presented
PRM learning of both parameters and structure (that is, the learning of the
scheme level).
PRMs make use of Bayesian networks, a directed graphical model that brings
a notion of causality. In relational domains it is often the case that random
variables depend on each other without a clear notion of causality. Take for
example a network of people linked by friendship relationships, with the at-
tribute smoker for each person. We might want to state the ﬁrst-order causal
relationship P(smoker(X)|friends(X, Y ), smoker(Y )) in such a model, but it
would create cycles in the underlying Bayesian network (between each pair of
smoker attributes such as smoker(john) and smoker(mary)). For this reason,
Relational Markov Networks (RMNs) [64] recast PRMs so they generate undi-
rected graphical models (Markov networks) instead of Bayesian networks. In
RMNs, dependencies are stated as ﬁrst-order features that get instantiated into
potential function on cliques of random variables, without a notion of causality
or conditional probabilities. The disadvantage of it, however, is that learning
in undirected graphical models is harder than in directed ones, involving a full
inference step at each expectation step of the EM algorithm.
Relational Dependency Networks (RDNs) [65] provide yet another alterna-
tive to this problem. They are the ﬁrst-order version of Dependency Networks

302
R. de Salvo Braz, E. Amir, and D. Roth
(DNs) (Heckerman, [46]), which use conditional probabilities but do not require
acyclicity. Using directed conditional probabilities avoids the expensive learning
of undirected models. However, DNs have the downside of conditional proba-
bilities being no longer guaranteed consistent with the joint probability deﬁned
by their normalized product. Heckerman shows that, as the amount of training
data increases, conditional probabilities in a DN will asymptotically converge
to consistency. RDNs are sets of ﬁrst-order conditional probabilities which are
used to generate an underlying regular dependency network. These ﬁrst-order
conditional probabilities are typically learned from data by relational learners
(Sect. 12.5). RDNs are implemented in Proximity, a well-developed, publicly
available software package.
Kersting and DeRaedt [66] introduce Bayesian Logic Programs. This work’s
motivation is to provide a language which is as syntactically and conceptually
simple as possible while preserving the expressive power of works such as Ngo
and Haddawy [23], Jaeger [57] and PRMs [58]. According to the authors, this is
necessary so one understands the relationship between all these approaches, and
also the fundamental aspects of FOPI models.
Fierens at al [67] deﬁne Logical Bayesian Networks (LBNs). LBNs are very
similar to Bayesian Logic Programs, with the diﬀerence of having both random
variables and deterministic logical literals in their language. A logic programming
inference process is run for the construction of the Bayesian network, during
which logical literals are used, but since they are not random variables, they
are not included in the Bayesian network. This addresses the same issue of
ﬁxed relationships discussed in the presentation of PRMs, that is, when a set of
relationships is deterministically known, we can create random variable nodes in
the Bayesian network with signiﬁcantly fewer ancestors. In the BLPs and LBNs
framework, this is exempliﬁed by a rule such as:
rating(X) ←cook(X, Y ), training(Y ) .
which has an associated probability, declaring that a restaurant X’s rating
depends on their cook Y ’s training. In Bayesian Logic Programs, the instan-
tiations of cook(X, Y ) are random variables (just like the instantiations of
rating(X) and training(Y )). Therefore, since we do not know a priori which
Y makes cook(timpone, Y ) true, rating(timpone) depends on all instantiations
of cook(timpone, Y ) and training(Y ) and has all of them as parents in the un-
derlying Bayesian network. If in the domain at hand the information of cook
is deterministic, then this would be wasteful. We could instead determine Y
such that cook(timpone, Y ), say Y = joe, and build the Bayesian network with
only the relevant random variable training(joe) as parent of rating(timpone).
This is precisely what LBNs do. In LBNs, one would deﬁne cook as a deter-
ministic literal that would be reasoned about, but not included in the Bayesian
network as a random variable. This in fact is even more powerful than the PRMs

12 A Survey of First-Order Probabilistic Models
303
approach since it deals even with the situation where relationships are not di-
rectly given as data, but have to be reasoned about in a deterministic manner.
Santos Costa et al. [68] propose an elegant KBMC approach that smoothly
leverages an already existing framework, Constraint Logic Programming (CLP).
In regular logic programming, the only constraint over logical variables are equa-
tional constraints coming from uniﬁcation. As explained in Sect. 12.4.2, CLP
programs generalize this by allowing other constraints to be stated over those
variables. These constraints are managed by special-purpose constraint solvers as
the derivation proceeds, and failure in satisfying a constraint determines failure
of the derivation. The authors leverage CLP by developing a constraint solver on
probabilistic constraints expressed as CPTs, and simply plug it into an already
existing CLP system. The resulting system can also use available logic program-
ming mechanisms in the CPT speciﬁcation, making it possible to calculate it
dynamically, based on the context, rather than by ﬁxed tables. The probabilistic
constraint solver uses a Bayesian network internally in order to solve the posed
constraints, so this system is also using an underlying propositional Bayesian
network for answering queries. Santos Costa et al. indicate [69] as the closest
approach to theirs, with the diﬀerence that the latter keeps hard constraints
on Bayesian variables separate from probabilistic constraints. This allows hard
constraints to be solved separately. It is also diﬀerent in that it does not use con-
ditional independencies (like Bayesian networks do), and therefore its inference
is exponential on the number of random variables.
Markov Logic Networks (MLNs) [70] is a recent and rapidly evolving frame-
work for probabilistic logic. Its main distinctions are that it is based on undi-
rected models and has a very simple semantics while keeping the expressive
1.5
¬ Smokes(X) Ç Cancer(X)
“Smoking causes cancer”
∀X Smokes(X)  ⇒Cancer(X)
Weight
Clausal form
English / First-Order Logic
1.1
¬ Friends(X,Y) Ç Smokes(X) Ç Smokes(Y)
“If two people are friends either both 
smoke or neither does”
∀X ∀Y Fr(X,Y) ⇒(Sm(X) ⇔Sm(Y))
Friends(A,A)
Smokes(A)
Smokes(B)
Friends(B,B)
Friends(A,B)
Friends(B,A)
Cancer(A)
Cancer(B)
Fig. 12.2. A ground Markov network generated from a Markov Logic Network for
objects Anna (A) and Bob (B) (example presented in [70])

304
R. de Salvo Braz, E. Amir, and D. Roth
Fig. 12.3. An example of a MEBN, as shown in [72]
power of ﬁrst-order logic. The downside to this is that its inference can become
quite slow if complex constructs are present.
MLNs consist of a set of weighted ﬁrst-order formulas and a universe of ob-
jects. Its semantics is simply that of a Markov network whose features are the
instantiations of all these formulas given the universe of objects. The potential of
a feature is deﬁned as the exponential of its weight in case it is true. Figure 12.2
shows an example.
Formulas can be arbitrary ﬁrst-order logic formulas, which are converted to
clausal form for inference. Converting existentially quantiﬁed formulas to clausal
form usually involves Skolemization, which requires uninterpreted functions in
the language. Since MLNs do not include such functions, existentially quantiﬁed
formulas are replaced by the disjunction of their groundings (this is possible
because the domain is ﬁnite). The great expressivity of MLNs allows them to
easily subsume other proposed FOPI languages. They are also a generalization
of ﬁrst-order logic, to which they reduce when weights are inﬁnite.
Learning algorithms for MLNs have been presented from the beginning. Because
learning in undirected models is hard, MLNs use the notion of pseudo-likelihood
[71], an approximate but eﬃcient method. When data is incomplete, EM is used.

12 A Survey of First-Order Probabilistic Models
305
MLNs are a powerful language and framework accompanied by well-supported
software (called Alchemy) and which has been applied to real domains. The
drawback of its expressivity is potentially very large underlying networks (for
example, when existential quantiﬁcation is used).
Laskey [72] presents multi-entity Bayesian networks (MEBNs), a ﬁrst-order
version of Bayesian networks, which rely on generalizing typical Bayesian net-
work representations rather than a logic-like language. A MEBN is a collection
of Bayesian network fragments involving parameterized random variables. As in
the other approaches, the semantics of the model is the Bayesian network re-
sulting from instantiating these fragments. Once they are instantiated, they are
put together according to the random variables they share. A MEBN is shown
in Fig. 12.3.
Laskey’s language is indeed quite rich, allowing inﬁnite models, function sym-
bols and distributions on the parameters of random variables themselves. The
work focus on deﬁning this language rather than on the actual implementation,
which is based on instantiating a Bayesian network containing the parts rele-
vant to the query at hand. It does not provide a detailed account of this process,
which can be especially tricky in the case of inﬁnite models.
12.4.5
Lifted Inference
One of the major diﬃculties in KBMC approaches is that they must proposi-
tionalize the model in order to perform inference. This does not preserve the rich
ﬁrst-order structure present in the original model; the propositionalized version
does not indicate anymore that CPTs are instantiations of the same original one,
or that random variables are instantiations of an original parameterized random
variable. In other words, it creates a potentially large propositional model with
a great amount of redundancy that cannot be readily exploited.
Recent research on lifted inference [73, 47, 48] has addressed this point. A
lifted inference algorithm receives a ﬁrst-order speciﬁcation of a probabilistic
model and performs inference directly on it, without propositionalization. This
can potentially yield an enormous gain in eﬃciency.
For example, a possible model can be formed by parameterized factors
(or parfactors) φ1(epidemic(D)) and φ2(sick(P, D)) and a set of typed ob-
jects flu, rubella, and john, mary etc. The model is equivalent to a propo-
sitional graphical model formed by all possible instantiations of parfactors
by the given objects, which is the set of regular factors φ1(epidemic(flu)),
φ1(epidemic(rubella)), . . . , and φ2(sick(john, flu)), φ2(sick(john, rubella)),
φ2(sick(mary, flu)),
φ2(sick(mary, rubella)), etc.
What lifted inference does, instead of actually generating these instantiations,
is to operate directly on the parfactors and obtain the same answer as the one
obtained by instantiating and solving by a propositional algorithm. By operat-
ing directly on parfactors, the lifted algorithm can potentially be much more
eﬃcient, since the ﬁrst-order structure is explicitly available to it. For example,
suppose we want to compute the marginal of P(epidemic(flu)). Then we have

306
R. de Salvo Braz, E. Amir, and D. Roth
to sum out all the other random variables in the model. While a regular KBMC
algorithm would instantiate them and then sum them out, a lifted inference al-
gorithm will directly sum out the parameterized epidemic(D), for D ̸= flu, and
sick(P, D). The lifted elimination operation may not depend on the number of
objects in the domain at all, greatly speeding up the process. The step in which
an entire class of random variables is eliminated at once is possible because they
all share the same structure, and this structure is explicitly available to the
algorithm.
Figure 12.4 presents a simpliﬁed diagram of a lifted inference operation.
Poole [73] proposes a lifted algorithm that generalizes propositional Variable
Elimination [25] but covers only some speciﬁc cases. de Salvo Braz et al. [47, 48]
present a broader algorithm, called First-Order Variable Elimination (FOVE).
FOVE includes Poole’s operation (which this work calls Inversion Elimination)
a generalized version of it, called simply Inversion, and a second elimination
operation called Counting Elimination. While Inversion does not depend on the
domain size, Counting Elimination does, but still only exponentially less than
propositionalization. The work also presents rigorous proofs of the correctness of
these operations and shows how to solve the lifted version of the Most Probable
Explanation (MPE) problem. While more general, FOVE still does not cover
all possible cases, when it too must resort to propositionalization. When this
sick(P, D)
epidemic(D)
D ≠measles
D ≠measles
Lifted VE
…
sick(john, flu)
epidemic(flu)
sick(mary, rubella)
epidemic(rubella)
…
Instantiation
…
sick(john, flu)
epidemic(flu)
sick(mary, rubella)
epidemic(rubella)
…
VE
Abstraction
epidemic(D)
D ≠measles
D ≠measles
sick(P, D)
Fig. 12.4. A diagram of several possible operations involving ﬁrst-order and propo-
sitional probabilistic models. The ﬁgure uses the notation of factor graphs, which ex-
plictly shows potential functions as squares connected to their arguments. Parameter-
ized factors are shown as piled up squares, since they compactly stand for multiple
factors. Lifted inference operates solely on the ﬁrst-order representation and can be
much faster than propositional inference, while producing the same results.

12 A Survey of First-Order Probabilistic Models
307
happens, this propositionalization will be localized to the parfactors involved in
the uncovered case.
Lifted FOPI is a further step towards closing the gap between logic and prob-
abilistic inference, bringing to the latter the type of inference that does not
require binding of parameters (which would be the logical variables in atoms, in
logical terms), often seen in the former. However, it has its own disadvantages.
It is relatively more complicated to implement, and requires a normalizing pre-
processing of the model (called shattering) that can be very expensive. Further
methods are being developed to circumvent these diﬃculties.
12.5
Relational Learning
In this section we discuss some ﬁrst-order models developed from a machine
learning perspective.
Machine learning algorithms have traditionally been deﬁned as classiﬁcation
of attribute-value vectors [74]. In many applications, it is more natural and con-
venient to represent data as graphs, where each vertex represents an object and
each edge represents a relation between objects. Vertices can be labeled with at-
tributes of its corresponding object (unary predicates or binary predicates where
the second argument is a simple value; this is similar to PRMs in Sect. 12.4.4),
and edges can be labeled (the label can be interpreted as a binary predicate
holding between the objects). This provides the typical data structure represen-
tations such as trees, lists and collections of objects in general. When learning
from graphs, we usually want to form hypotheses that explain one or more of
the attributes and (or) relations (the targets) of objects in terms of its neigh-
bors. Machine learning algorithms which were developed to beneﬁt from this
type of representation have often been called relational. This is closely associ-
ated to probabilistic ﬁrst-order models, since graph data can be interpreted as a
set of ground literals using unary and binary predicates. Because the hypothe-
ses explaining target attributes and relations apply to several objects, it is also
convenient to represent the learned hypotheses as quantiﬁed (ﬁrst-order) rules.
And because most learners involve probabilities or at least some measure of un-
certainty, probabilistic ﬁrst-order rules provide a natural representation option.
Figure 12.5 illustrates these concepts.
We now discuss three forms of relational learning: propositionalization (ﬂat-
tening), Inductive Logic Programming (ILP), and FOPI learning, which can be
seen as a synthesis of the two.
12.5.1
Propositionalization
A possible approach to relational machine learning is that of using a relational
structure for generating propositional attribute-value vectors for each of its ob-
jects. For this reason, the approach has been called propositionalization. Because
it transforms graph-like data into vector-like data, it is also often called ﬂattening.
Cumby & Roth [75] provide a language for transforming relational data into
attribute-value vectors. Their concern is not forming a ﬁrst-order hypothesis,

308
R. de Salvo Braz, E. Amir, and D. Roth
joe
male
interest(baseball)
interest(computers)
school(jfk)
mary
female
interest(baskteball)
interest(computers)
larry
male
interest(baskteball)
school(fdr)
paul
male
interest(music)
school(fdr)
lissa
female
interest(music)
school(jfk)
0.9: acqt(X,Y) ⇐school(X,Z) ÆÆÆÆ school(Y,Z)
0.7: friends(X,Y) ⇐interest(X,Z) ÆÆÆÆ interest(Y,Z)
0.6: friends(X,Y) ⇐school(X,Z) ÆÆÆÆ school(Y,Z)
0.55: male(X) ⇐interest(X,baseball)
acqt
friends
friends
acqt
friends
acqt
...
male(joe), 
interest(joe,baseball),
interest(joe,computers),
school(joe,jfk),
female(lissa),
interest(lissa,music),
school(lissa,jfk),
...
acqt(joe,lissa),
acqt(lissa,joe),
friends(lissa,paul),
friends(paul,lissa),
...
...
acqt
friends
...
...
Fig. 12.5. A fragment of a graph structure used as input for relational learning. The
same information can be represented as a set of ground literals (right). The hypotheses
learned to explain either relations or attributes can be represented as weighted ﬁrst-
order clauses over those literals (below).
however. They instead keep the attribute-value hypothesis and transform novel
data to that representation in order to classify it with propositional learners
such as Perceptron. For example, in the case of Fig. 12.5, a classiﬁer seeking
to learn the relation acqt would go through the instances of that predicate and
generate suitable attribute-value vectors. The literal acqt(paul, larry) would gen-
erate an example with label acqt(X, Y ) and features male(paul), male(larry),
interest(paul, music), school(paul, fdr), interest(larry, basketball),
school(larry, fdr) etc, as well as non-ground ones such as male(X), male(Y ),
school(X, fdr), school(Y, fdr), school(X, Z), school(Y, Z), interest(X, music)
etc. The literal acqt(joe, lissa) would generate an example with label acqt(X, Y )
and features male(joe), female(lissa), interest(joe, baseball),
interest(joe, computers), school(joe, jfk),etc, as well as non-ground ones such
as male(X), female(Y ), school(X, jfk), school(Y, jfk), school(X, Z),
school(Y, Z) etc. Note how this reveals abstractions – the examples above share
the features school(X, Z) and school(Y, Z), which may be one reason for people
being acquaintances in this domain. Should a target depend on speciﬁc objects
(say, it is much more likely for people at the FDR school to be acquainted to
each other) not completely abstracted features such as school(X, fdr) would be
preferred by the classiﬁer.

12 A Survey of First-Order Probabilistic Models
309
There are many diﬀerent ways of transforming a graph or set of literals into
attribute-value vectors for propositional learners. Each of them will represent
diﬀerent learning biases. Some works in this line are LINUS [76], which uses the
concept of ij-determinacy (a way of restricting the generalizations of literals)
in order to construct hypothesis, 1BC [77] and 1BC2 [78], which diﬀerentiate
between predicates representing attributes or relations and construct multiset
attributes (for example, the set of interests among one’s friends), and Relational
Bayesian Classiﬁer (RBC) [79], which also uses multiset attribute values with a
conditional independence assumption similar to the one used in the Naive Bayes
classiﬁer.
One disadvantage of propositionalization is that it performs classiﬁcation of
an object at a time. This prevents the use of possible dependencies between
object labels and introduces a bias. These dependencies can be used by FOPI
algorithms, which perform joint inference over several objects at once. Three
of FOPI models which have been speciﬁcally developed with this in mind are
RDNs [65], RMNs [64] and MLNs [70].
12.5.2
Inductive Logic Programming
Inductive Logic Programming (ILP) stemmed from the logic programming com-
munity with the goal of learning logic programs from data rather than writing
them. The choice of logic programming as a hypothesis language initially re-
stricted the ﬁeld to deterministic hypotheses; the later incorporation of proba-
bilities to this framework is one of the origins of FOPI models. However, the fact
that these algorithms learn from data give them a statistical ﬂavor even in the
deterministic case. For example, Progol [36] uses information-theoretic measures
for evaluating deterministic hypotheses.
A typical ILP algorithm works by forming hypotheses one clause at a time.
For example, if such an algorithm is trying to learn the concept mother(X, Y ),
it might generate the clause mother(X, Y ) : −female(X), since female(X)
does add some predictive power to whether X is a mother, and may at a latter
step reﬁne it to mother(X, Y ) : −female(X), child(Y, X). This is a top-down
approach since the most general clause is successively made more speciﬁc ac-
cording to examples. Two examples of work in this line are [80, 81]. Another
approach is bottom-up, best exempliﬁed by Progol [36], where sets of ground
literals are successively generalized in order to increase accuracy.
Some ILP algorithms use propositionalization as a subroutine that learns
one rule at a time. LINUS [76] transforms its data into attribute-value vectors,
applies regular attribute-value learners to it, and then transforms the attribute-
value hypothesis into a clause.
Probabilistic ILP (PILP) algorithms (a speciﬁc survey can be found at [82])
also often grow clauses and then estimate the probabilities (or parameters) asso-
ciated with those clauses. Some learn an entire logic program at ﬁrst, and then
the parameters, while others learn the parameters as soon as the clauses are
learned. In fact, PILP is simply FOPI with learning done with ILP techniques,
and many of these works have been mentioned in previous sections. For example,

310
R. de Salvo Braz, E. Amir, and D. Roth
MLNs [70], PRMs [58, 59] and BLPs [83] learn the structure of their models (as
opposed to their parameters) through techniques similar to those of ILP. RDNs
[65] use relational classiﬁers developed in the ILP community as a subroutine to
its model learning.
12.6
Restricted First-Order Probabilistic Models
The models presented so far intended to provide a level of expressivity similar
to ﬁrst-order logic. At a minimum, they provide unary and binary predicates
arbitrarily applied to a collection of objects. However, there are some proba-
bilistic models exhibiting a restricted subset of ﬁrst-order aspects, targeted for
particular tasks.
One such model is Hidden Markov Models (HMMs) [84], in which a sequence of
pairs of random variables represent a state and an observation. The model relates
observations to the state in the same time slice, as well as states in successive time
slides. While essentially propositional, HMMs exhibit the sharing of parameters
commonly observed in ﬁrst-order models, since its parameters equally apply to
all transitions from one step to the next. The time indices of random variables
of an HMM are a restricted form of treating them as ﬁrst-order.
The same sharing of parameters can be observed in related models. Stochas-
tic Context-Free Grammars [31] consist of set of production rules were a non-
terminal symbol is stochastically replaced by a number of possible sequences
of symbols. Again, the rules can be applied at diﬀerent points and parameters
are reused. Dynamic Bayesian Networks [85] generalize HMMs in that each step
is represented by a full Bayesian network rather than just a pair of state and
observations.
sick
epidemics
hospital
P: Person
D: Disease
Fig. 12.6. Repeated structure of a graphical model can be indicated by plates. Random
variables are implicitly indexed by integer variables associated with the plates inside
which they reside. The same CPT is used for all of them.

12 A Survey of First-Order Probabilistic Models
311
Other generalizations of these restricted models are Hidden Tree Markov Mod-
els [86], where possible states form a tree structure, Logical Hidden Markov
Models [66], where each state is represented by a set of logical literals, and Re-
lational Markov Models [87], where each state is also represented by a set of
logical literals, but possible arguments follow a taxonomy and induce a lattice
on possible literals.
Another simple generalization of propositional models is the plate notation
[88], which describes graphical models with parts replicated by a set of indices,
indicated by a rectangle involving that part in a diagram. An example is shown
in Fig. 12.6. The parameters into these parts are then also replicated. Mjolsness
[89] proposes many reﬁnements to this type of notation. The plate notation is
commonly used as a tool for descriptions in the literature but has not been
typically meant as an input to algorithms.
12.7
Conclusion
First-order probabilistic inference has made much progress in the last twenty
years. We believe the main accomplishments have been the clariﬁcation the se-
mantics of such languages, as well as a greater understanding on how several
diﬀerent language options relate to each other. In analyzing the works in the
area of FOPI, we can distinguish a few main options along which they seem to
organize themselves. We now make these aspects more explicit.
12.7.1
Directed vs. Undirected
The decision on using directed or undirected models carries over the the ﬁrst-
order case. While the intelligibility of directed models has favored their use in
ﬁrst-order proposals at ﬁrst, we can observe a current tendency to use undirected
models [70, 90], or at least directed models without the acyclicity requirement
[65]. The reason for this shift is that cycles are even more naturally occurring
in ﬁrst-order domains than in propositional ones. A “natural” conditional prob-
ability such as P(smoker(X)|friend(X, Y ), smoker(Y )) creates an underlying
network with cycles. The use of undirected models seems to be further justiﬁed
by the fact that they do not rule out directed models. If the given factors en-
code conditional probabilities that do not involve cycles, they will still represent
the correct distribution even if interpreted as an undirected factor (this however
loses structure that could be used to improve eﬃciency).
12.7.2
The Trade-Oﬀbetween Language and Algorithm
Some FOPI proposals focus on rich languages that allow the user to indicate do-
main restrictions which can be exploited for eﬃciency. Examples of such systems
are Ngo and Haddawy’s Probabilistic Logic Programming [23], PRMs [58, 59],
LBNs [67]. One example is the treatment of determinism (especially of relations)

312
R. de Salvo Braz, E. Amir, and D. Roth
being expressed explicitly by the language, as in the case of PRMs and LBNs,
as discussed in Sect. 12.4.4.
Other solutions propose simple languages, relying on inference algorithms to
exploit domain structure (sometimes guided by extra-language directives). The
most typical examples are BLPs [66] and MLNs [70].
This choice reﬂects a trade-oﬀbetween language and inference algorithm com-
plexities. Complex languagesbring built-in optimization hints; for example, PRMs
have attributes (with a value) and relations, even though both could be regarded
as binary predicates. By indicating that a binary predicate is an attribute, the user
implicitly indicates the most eﬃcient ways of using it, which are distinct from the
way a relation is used. To obtain the same eﬀect, an algorithm using only a single
general notion of binary predicates would have to either ﬁnd out or be told (by
extra-language directives) how to use each of them in the most eﬃcient manner.
In this case, compilation and learning can play important roles.
Our particular view is that FOPI languages will tend to become simpler,
leaving the complexities to be dealt with by compilation, learning and directives.
This reﬂects the evolution of programming languages, that have increasingly left
eﬃciency details to be dealt with by compilers and directives rather than by
the language itself, leaving the latter at a higher level that can be more easily
understood and theoretically related to other approaches. On the other hand,
this simplicity should not be such as to prevent the development of specialized
libraries and knowledge representations. These are useful but best included on
top of simple primitives instead of as primitives themselves.
12.7.3
Inﬁnite Models
First-order models may have a ﬁnite description while involving an inﬁnite num-
ber of random variables, if the number of objects in the domain is inﬁnite.
This poses problems to algorithms presented in this survey, since they may not
stop in that case. As pointed out by [91], models with inﬁnite number of ran-
dom variables can be dealt with by approximate, anytime algorithms. These
algorithms will provide arbitrarily precise approximations after a suﬃcient (but
ﬁnite) amount of computation is performed. Such an approach would also make
sense for processing models that, while involving a ﬁnite number of random vari-
ables, are too complex for exact inference. Such algorithms would also beneﬁt
from guided evaluation, that is, by choosing to process ﬁrst the parts of the
model that yield greater amounts of information about the query.
References
1. Russell, S., Norvig, P.: Artiﬁcial Intelligence: A Modern Approach, 2nd edn.
Prentice-Hall, Englewood Cliﬀs (2003)
2. Buchanan, B., Shortliﬀe, E.: Rule-Based Expert Systems: The MYCIN experiments
of the Stanford Heuristic Programming Project. Addison-Wesley, Reading (1984)
3. Pearl, J.: Probabilistic reasoning in intelligent systems: networks of plausible in-
ference. Morgan Kaufmann, San Mateo (Calif.) (1988)

12 A Survey of First-Order Probabilistic Models
313
4. Nilsson, N.J.: Probabilistic logic. Artiﬁcial Intelligence 28(1), 71–88 (1986)
5. Bacchus, F.: Representing and reasoning with probabilistic knowledge: a logical
approach to probabilities. MIT Press, Cambridge (1990)
6. Halpern, J.Y.: An analysis of ﬁrst-order logics of probability. In: Proceedings of
IJCAI 1989, 11th International Joint Conference on Artiﬁcial Intelligence, Detroit,
US, pp. 1375–1381 (1990)
7. Shortliﬀe, E.H.: Mycin: a rule-based computer program for advising physicians
regarding antimicrobial therapy selection. PhD thesis, Stanford University (1975)
8. Clark, K.L., McCabe, F.G.: Prolog: A language for implementing expert systems.
In: Hayes, J.E., Michie, D., Pao, Y.H. (eds.) Machine Intelligence, Ellis Horwood,
Chichester, vol. 10, pp. 455–470 (1982)
9. Shapiro, E.: Logic programs with uncertainties: A tool for implementing expert
systems. In: Proc. IJCAI 1983, pp. 529–532. William Kaufmann, San Francisco
(1983)
10. Heckerman, D.: Probabilistic interpretation for MYCIN’s certainty factors. In:
Kanal, L.N., Lemmer, J. (eds.) Uncertainty in Artiﬁcial Intelligence, pp. 167–196.
Kluwer Science Publishers, Dordrecht (1986)
11. Lucas, P.: Certainty-factor-like structures in bayesian belief networks. Knowledge-
Based Systems 14, 325–327 (2001)
12. Carnap, R.: The Logical Foundations of Probability. University of Chicago Press,
Chicago (1950)
13. Hailperin, T.: Probabilistic logic. Notre Dame Journal of Formal Logic 25(3), 198–
212 (1984)
14. Fagin, R., Halpern, J.Y., Megiddo, N.: A logic for reasoning about probabilities.
Information and Computation 87(1/2), 78–128 (1990)
15. Fenstad, J.E.: The structure of probabilities deﬁned on ﬁrst-order languages. Stud-
ies in Inductive Logic and Probabilities, pp. 251–262. California Press (1980)
16. Gaifman, H.: Concerning measures in ﬁrst-order calculi. Israel Journal of Mathe-
matics 2, 1–18 (1964)
17. Gaifman, H., Snir, M.: Probabilities over rich languages, testing and randomness.
Journal of Symbolic Logic 47(3), 495–548 (1982)
18. Kifer, M., Li, A.: On the semantics of rule-based expert systems with uncertainty.
In: Gyssens, M., Van Gucht, D., Paredaens, J. (eds.) ICDT 1988. LNCS, vol. 326,
pp. 102–117. Springer, Heidelberg (1988)
19. W¨uthrich, B.: Probabilistic knowledge bases. IEEE Trans. Knowl. Data Eng. 7(5),
691–698 (1995)
20. Ng, R.T., Subrahmanian, V.S.: Probabilistic logic programming. Information and
Computation 101(2), 150–201 (1992)
21. Lakshmanan, L.V.S., Sadri, F.: Probabilistic deductive databases. In: Symposium
on Logic Programming, pp. 254–268 (1994)
22. Lakshmanan, L.V.S.: An epistemic foundation for logic programming with uncer-
tainty. In: Foundations of Software Technology and Theoretical Computer Science,
pp. 89–100 (1994)
23. Ngo, L., Haddawy, P.: Probabilistic logic programming and Bayesian networks. In:
Asian Computing Science Conference, pp. 286–300 (1995)
24. Poole, D.: Probabilistic Horn abduction and Bayesian networks. Artiﬁcial Intelli-
gence 64(1), 81–129 (1993)
25. Zhang, N.L., Poole, D.: A simple approach to Bayesian network computations.
In: Proceedings of the Tenth Biennial Canadian Artiﬁcial Intelligence Conference
(1994)

314
R. de Salvo Braz, E. Amir, and D. Roth
26. Lukasiewicz, T.: Probabilistic deduction with conditional constraints over basic
events. J. Artif. Intell. Res (JAIR) 10, 199–241 (1999)
27. Frisch, A.M., Haddawy, P.: Anytime deduction for probabilistic logic. Artiﬁcial
Intelligence 69(1–2), 93–122 (1994)
28. Koller, D., Halpern, J.Y.: Irrelevance and conditioning in ﬁrst-order probabilistic
logic. In: Shrobe, H., Senator, T. (eds.) Proceedings of the Thirteenth National
Conference on Artiﬁcial Intelligence and the Eighth Innovative Applications of
Artiﬁcial Intelligence Conference, vol. 2, pp. 569–576. AAAI Press, Menlo Park
(1996)
29. Riezler, S.: Probabilistic constraint logic programming (1997)
30. Jaﬀar, J., Lassez, J.L.: Constraint logic programming. In: POPL 1987: Proceedings
of the 14th ACM SIGACT-SIGPLAN symposium on Principles of programming
languages, pp. 111–119. ACM Press, New York (1987)
31. Lari, K., Young, S.: The estimation of stochastic context-free grammars using the
inside-outside algorithm. Computer Speech and Language 4, 35–56 (1990)
32. Baker, J.: Trainable grammars for speech recognition. In: Speech communication
papers presented at the 97th Meeting of the Acoustical Society, pp. 547–550 (1979)
33. Baum, L.E.: An inequality and associated maximization technique in statistical es-
timation for probabilistic functions of markov processes. Inequalities 3, 1–8 (1972)
34. Muggleton, S.: Stochastic logic programs. In: De Raedt, L. (ed.) Proceedings of
the 5th International Workshop on Inductive Logic Programming, Department of
Computer Science, Katholieke Universiteit Leuven, vol. 29 (1995)
35. Cussens, J.: Loglinear models for ﬁrst-order probabilistic reasoning. In: Laskey,
K.B., Prade, H. (eds.) Proceedings of the 15th Annual Conference on Uncertainty
in AI (UAI 1999), pp. 126–133. Morgan Kaufmann, San Francisco (1999)
36. Muggleton, S.: Inverse entailment and Progol. New Generation Computing, Special
issue on Inductive Logic Programming 13(3-4), 245–286 (1995)
37. Della Pietra, S., Della Pietra, V.J., Laﬀerty, J.D.: Inducing features of random
ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence 19(4),
380–393 (1997)
38. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Statistical Society. Series B
(Methodological) 39(1), 1–38 (1977)
39. Lukasiewicz, T.: Probabilistic logic programming. In: European Conference on Ar-
tiﬁcial Intelligence, pp. 388–392 (1998)
40. Baral, C., Gelfond, M., Rushton, J.N.: Probabilistic reasoning with answer sets.
In: LPNMR, pp. 21–33 (2004)
41. Pearl, J.: Causality: Models, Reasoning and Inference. Cambridge University Press,
Cambridge (2000)
42. Sato, T., Kameya, Y.: Prism: A language for symbolic-statistical modeling. In:
IJCAI, pp. 1330–1339 (1997)
43. Sato, T., Kameya, Y.: A viterbi-like algorithm and em learning for statistical ab-
duction (2000)
44. Milch, B., Marthi, B., Russell, S., Sontag, D., Ong, D.L., Kolobov, A.: BLOG:
Probabilistic models with unknown objects. In: Proc. IJCAI (2005)
45. Spiegelhalter, D., Thomas, A., Best, N., Gilks, W.: Bugs: Bayesian inference using
gibbs sampling, version 0.30. Technical report, MRC Biostatistics Unit, University
of Cambridge (1994)
46. Heckerman, D., Chickering, D.M., Meek, C., Rounthwaite, R., Kadie, C.M.: De-
pendency networks for inference, collaborative ﬁltering, and data visualization.
Journal of Machine Learning Research 1, 49–75 (2000)

12 A Survey of First-Order Probabilistic Models
315
47. de Salvo Braz, R.: Lifted First-Order Probabilistic Inference. PhD thesis, Univer-
sity of Illinois at Urbana-Champaign (2007)
48. de Salvo Braz, R., Amir, E., Roth, D.: Lifted ﬁrst-order probabilistic inference. In:
Getoor, L., Taskar, B. (eds.) An Introduction to Statistical Relational Learning,
pp. 433–451. MIT Press, Cambridge (2007)
49. Breese, J.S.: Construction of belief and decision networks. Computational Intelli-
gence 8, 624–647 (1991)
50. Wellman, M.P., Breese, J.S., Goldman, R.P.: From knowledge bases to decision
models. Knowledge Engineering Review 7, 35–53 (1992)
51. Horsch, M., Poole, D.: A dynamic approach to probabilistic inference using
bayesian networks. In: Proceedings of the 6th Conference of Uncertainty in Ar-
tiﬁcial Intelligence, pp. 155–161. Morgan Kaufmann, San Francisco (1990)
52. Goldman, R.P., Cherniak, E.: A language for construction of belief networks. IEEE
Trans. Pattern Anal. Mach. Intell. 15(3), 196–208 (1993)
53. Poole, D.: The independent choice logic for modelling multiple agents under un-
certainty. Artiﬁcial Intelligence 94(1-2), 7–56 (1997)
54. Glesner, S., Koller, D.: Constructing ﬂexible dynamic belief networks from ﬁrst-
order probalistic knowledge bases. In: Symbolic and Quantitative Approaches to
Reasoning and Uncertainty, pp. 217–226 (1995)
55. Haddawy, P.: Generating bayesian networks from probability logic knowledge
bases. In: de Mantaras, R.L., Poole, D. (eds.) Uncertainty In Artiﬁcial Intelligence
10 (UAI 1994), pp. 262–269. Morgan Kaufmann, San Francisco (1994)
56. Koller, D., Pfeﬀer, A.: Learning probabilities for noisy ﬁrst-order rules. In: IJCAI,
pp. 1316–1323 (1997)
57. Jaeger, M.: Relational Bayesian networks. In: Kaufmann, M. (ed.) Proceedings of
the 13th Conference on Uncertainty in Artiﬁcial Intelligence, pp. 266–273 (1997)
58. Getoor, L., Friedman, N., Koller, D., Pfeﬀer, A.: Learning probabilistic relational
models. In: Dˇzeroski, S., Lavrac, N. (eds.) Relational Data Mining, pp. 307–335.
Springer, Heidelberg (2001)
59. Koller, D., Pfeﬀer, A.: Probabilistic frame-based systems. In: Proceedings of the
15th National Conference on Artiﬁcial Intelligence (AAAI), pp. 580–587 (1998)
60. Minsky, M.: A framework for representing knowledge. In: Computation & intelli-
gence: collected readings, pp. 163–189. American Association for Artiﬁcial Intelli-
gence, Menlo Park (1995)
61. Levesque, H.J., Brachman, R.J.: Expressiveness and tractability in knowledge rep-
resentation and reasoning. Computational Intelligence 3, 78–93 (1987)
62. Koller, D., Levy, A.Y., Pfeﬀer, A.: P-CLASSIC: A tractable probablistic description
logic. In: AAAI/IAAI, pp. 390–397 (1997)
63. Getoor, L.: Learning probabilistic relational models with structural uncertainty.
In: Choueiry, B.Y., Walsh, T. (eds.) SARA 2000. LNCS (LNAI), vol. 1864, pp.
322–329. Springer, Heidelberg (2000)
64. Taskar, B., Abbeel, P., Koller, D.: Discriminative probabilistic models for rela-
tional data. In: Proc. Eighteenth Conference on Uncertainty in Artiﬁcial Intelli-
gence (UAI), Edmonton, Canada (2002)
65. Neville, J.: Statistical models and analysis techniques for learning in relational
data. PhD thesis, University of Massachusetts Amherst (2006)
66. Kersting, K., De Raedt, L.: Bayesian logic programs. In: Cussens, J., Frisch, A.
(eds.) Proceedings of the Work-in-Progress Track at the 10th International Con-
ference on Inductive Logic Programming, pp. 138–155 (2000)
67. Fierens, D., Blockeel, H., Bruynooghe, M., Ramon, J.: Logical bayesian networks
and their relation to other probabilistic logical models. In: ILP, pp. 121–135 (2005)

316
R. de Salvo Braz, E. Amir, and D. Roth
68. Santos Costa, V., Page, D., Qazi, M., Cussens, J.: Clp(bn): Constraint logic pro-
gramming for probabilistic knowledge. In: Proceedings of the 19th Annual Con-
ference on Uncertainty in Artiﬁcial Intelligence (UAI 2003), pp. 517–552. Morgan
Kaufmann, San Francisco (2003)
69. Angelopoulos, N.: Probabilistic ﬁnite domains: A brief overview. In: Stuckey, P.J.
(ed.) ICLP 2002. LNCS, vol. 2401, p. 475. Springer, Heidelberg (2002)
70. Richardson, M., Domingos, P.: Markov logic networks. Technical report, Depart-
ment of Computer Science, University of Washington (2004)
71. Besag, J.: Statistical analysis of non-lattice data. The Statistician 24(3), 179–195
(1975)
72. Laskey, K.B.: First-order Bayesian logic. Technical report, George Mason Univer-
sity Department of Systems Engineering and Operations Research (2005)
73. Poole, D.: First-order probabilistic inference. In: Proceedings of the 18th Interna-
tional Joint Conference on Artiﬁcial Intelligence, pp. 985–991 (2003)
74. Mitchell, T.M.: Machine Learning. McGraw-Hill Higher Education, New York
(1997)
75. Cumby, C., Roth, D.: Relational representations that facilitate learning. In: Cohn,
A.G., Giunchiglia, F., Selman, B. (eds.) KR 2000: Principles of Knowledge Repre-
sentation and Reasoning, pp. 425–434. Morgan Kaufmann, San Francisco (2000)
76. Lavrac, N., Dzeroski, S.: Inductive Logic Programming: Techniques and Applica-
tions. Routledge, New York, 10001 (1993)
77. Flach, P., Lachiche, N.: 1BC: A ﬁrst-order Bayesian classiﬁer. In: Dˇzeroski, S.,
Flach, P. (eds.) ILP 1999. LNCS (LNAI), vol. 1634, pp. 92–103. Springer, Heidel-
berg (1999)
78. Lachiche, N., Flach, P.A.: 1BC2: a true ﬁrst-order Bayesian classiﬁer. In: Matwin,
S., Sammut, C. (eds.) ILP 2002. LNCS (LNAI), vol. 2583, pp. 133–148. Springer,
Heidelberg (2003)
79. Neville, J., Jensen, D., Gallagher, B.: Simple estimators for relational bayesian
classiﬁers. In: ICDM 2003: Proceedings of the Third IEEE International Conference
on Data Mining. IEEE Computer Society Press, Washington (2003)
80. Quinlan, J.: Learning logical deﬁnitions from relations. Machine Learning 5, 239–
266 (1990)
81. Raedt, L.D., Dehaspe, L.: Clausal discovery. Machine Learning 26(2-3), 99–146
(1997)
82. Raedt, L.D., Kersting, K.: Probabilistic inductive logic programming. In: ALT, pp.
19–36 (2004)
83. Kersting, K., De Raedt, L.: Towards combining inductive logic programming with
Bayesian networks. In: Rouveirol, C., Sebag, M. (eds.) ILP 2001. LNCS (LNAI),
vol. 2157, pp. 104–117. Springer, Heidelberg (2001)
84. Rabiner, L.R.: A tutorial on hidden markov models and selected applications in
speech recognition. In: Readings in speech recognition, pp. 267–296. Morgan Kauf-
mann Publishers Inc., San Francisco (1990)
85. Murphy, K.P.: Dynamic bayesian networks: representation, inference and learning.
PhD thesis, University of California Berkeley, Chair-Stuart Russell (2002)
86. Diligenti, M., Frasconi, P., Gori, M.: Hidden tree markov models for document
image classiﬁcation. IEEE Trans. Pattern Anal. Mach. Intell. 25(4), 519–523 (2003)
87. Anderson, C.R., Domingos, P., Weld, D.S.: Relational Markov models and their
application to adaptive web navigation. In: KDD 2002: Proceedings of the eighth
ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 143–152. ACM Press, New York (2002)

12 A Survey of First-Order Probabilistic Models
317
88. Buntine, W.L.: Operations for learning with graphical models. Journal of Artiﬁcial
Intelligence Research 2, 159–225 (1994)
89. Mjolsness, E.: Labeled graph notations for graphical models: Extended report.
Technical report, University of California Irvine – Information and Computer Sci-
ences (2004)
90. de Salvo Braz, R., Amir, E., Roth, D.: Lifted ﬁrst-order probabilistic inference.
In: Proceedings of IJCAI 2005, 19th International Joint Conference on Artiﬁcial
Intelligence (2005)
91. Pfeﬀer, A., Koller, D.: Semantics and inference for recursive probability models.
In: AAAI/IAAI, pp. 538–544 (2000)

Author Index
Amir, Eyal
289
Cooper, Gregory F.
169
Daryle Niedermayer, I.S.P.
117
de Salvo Braz, Rodrigo
289
Flores, M. Julia
251
G´amez, Jos´e A.
251
Heckerman, David
33
Holmes, Dawn E.
1, 281
Jain, Lakhmi C.
1
Jiang, Xia
169
Korb, Kevin B.
83
Laur´ıa, Eitel J.M.
187
Leray, Philippe
219
Maes, Sam
219
Meganck, Stijn
219
Moral, Seraf´ın
251
Nagl, Sylvia
131
Neapolitan, Richard E.
7
Nicholson, Ann E.
83
Roth, Dan
289
Wagner, Michael M.
169
Williams, Matt
131
Williamson, Jon
131

Editors 
 
 
 
 
 
 
 
 
 
 
Originally from the UK, Professor D. E. Holmes 
is now a permanent member of Faculty in the  
Department 
of 
Applied 
Probability 
and 
Statistics at UCSB, California.  Her research 
interests include Artificial Intelligence (the use 
of the maximum entropy formalism in Bayesian 
networks), Intuitionist Mathematics (Brouwers 
programme, intuitionist Markov chains) and 
some philosophical aspects of Bayesianism. 
 
 
 
Professor L.C. Jain is a Director/Founder of the 
Knowledge-Based Intelligent Engineering Systems (KES) 
Centre, located in the University of South Australia. 
His interests focus on the applications of novel 
techniques such as knowledge-based systems, virtual 
intelligent systems, defence systems, intelligence-based 
medical systems, e-Education and intelligent agents. 
 
 

