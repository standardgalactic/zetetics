Artiﬁcial Intelligence 314 (2023) 103806
Contents lists available at ScienceDirect
Artiﬁcial Intelligence
journal homepage: www.elsevier.com/locate/artint
Cooperative concurrent games ✩
Julian Gutierrez a,∗, Szymon Kowara c, Sarit Kraus b, Thomas Steeples c, 
Michael Wooldridge c
a Faculty of Information Technology, Monash University, Australia
b Department of Computer Science, Bar-Ilan University, Israel
c Department of Computer Science, University of Oxford, United Kingdom
a r t i c l e 
i n f o
a b s t r a c t
Article history:
Received 24 October 2021
Received in revised form 24 August 2022
Accepted 11 October 2022
Available online 28 October 2022
Keywords:
Concurrent games
Cooperative games
Multi-agent systems
Logic
Formal veriﬁcation
In rational veriﬁcation, the aim is to verify which temporal logic properties will obtain 
in a multi-agent system, under the assumption that agents (“players”) in the system 
choose strategies for acting that form a game theoretic equilibrium. Preferences are 
typically deﬁned by assuming that agents act in pursuit of individual goals, speciﬁed 
as temporal logic formulae. To date, rational veriﬁcation has been studied using non-
cooperative solution concepts—Nash equilibrium and reﬁnements thereof. Such non-
cooperative solution concepts assume that there is no possibility of agents forming binding 
agreements to cooperate, and as such they are restricted in their applicability. In this 
article, we extend rational veriﬁcation to cooperative solution concepts, as studied in the 
ﬁeld of cooperative game theory. We focus on the core, as this is the most fundamental 
(and most widely studied) cooperative solution concept. We begin by presenting a variant 
of the core that seems well-suited to the concurrent game setting, and we show that this 
version of the core can be characterised using ATL∗. We then study the computational 
complexity of key decision problems associated with the core, which range from problems 
in PSpace to problems in 3ExpTime. We also investigate conditions that are suﬃcient to 
ensure that the core is non-empty, and explore when it is invariant under bisimilarity. 
We then introduce and study a number of variants of the main deﬁnition of the core, 
leading to the issue of credible deviations, and to stronger notions of collective stable 
behaviour. Finally, we study cooperative rational veriﬁcation using an alternative model 
of preferences, in which players seek to maximise the mean-payoff they obtain over an 
inﬁnite play in games where quantitative information is allowed.
© 2022 Elsevier B.V. All rights reserved.
1. Introduction
Intelligent agents such as Siri, Alexa, and Cortana are now used by millions of people every day. Originally proposed 
in the early 1990s, the vision for such agents was that they would be pro-active assistants, working in pursuit of goals 
delegated to them by their human users. Current intelligent agents make use of an ever-increasing array of AI technologies, 
starting with natural language understanding in their interface, used to communicate with human users. The next step in 
✩This paper is an extended and revised version of [1].
* Corresponding author.
E-mail addresses: julian.gutierrez@monash.edu (J. Gutierrez), sarit@cs.biu.ac.il (S. Kraus), thomas.steeples@cs.ox.ac.uk (T. Steeples), 
michael.wooldridge@cs.ox.ac.uk (M. Wooldridge).
https://doi.org/10.1016/j.artint.2022.103806
0004-3702/© 2022 Elsevier B.V. All rights reserved.

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
the development of such agents is expected to be the ability of such agents to interact not just with human users but with 
each other in pursuit of their delegated goals. Such multi-agent systems raise a raft of research challenges, which have been 
the subject of considerable research over the past thirty years. The issue to which we address ourselves in this article is 
that of understanding the dynamics of such systems. In particular, collections of interacting agents may exhibit undesirable 
and unpredictable dynamics. The question of verifying the possible behaviours of multi-agent systems thus naturally arises.
One increasingly popular approach to the problem of verifying multi-agent systems involves viewing a system as a game 
(in the sense of game theory), in which agents act rationally and strategically in pursuit of delegated goals. Given this, it is 
natural to ask what behaviours a system might exhibit under the assumption that agents act rationally—in accordance with 
game-theoretic solution concepts. This is the key idea that underpins the rational veriﬁcation paradigm [2–5].
Previous work on rational veriﬁcation has used concurrent games as a semantic model of multi-agent systems. Since they 
were ﬁrst introduced [6], concurrent games have been very widely adopted in both the AI community and the veriﬁca-
tion/computer science community [2–4]. A concurrent game [6] is a ﬁnite-state environment, populated by a collection of 
independent, self-interested agents. A game takes place over an inﬁnite sequence of rounds, where at each round, each 
agent chooses an action to perform. Preferences in concurrent games are typically modelled by assuming that each agent is 
associated with a temporal logic goal formula [7], which it desires to see satisﬁed. The inﬁnite plays generated by a game 
will either satisfy or fail to satisfy each player’s goal, and players act rationally in an attempt to achieve their goal. Since 
the satisfaction of a player’s goal may be dependent on the choices made by other players, then players must make choices 
strategically in order to play optimally.
Now, in all previous studies that we are aware of, concurrent games are assumed to be non-cooperative: players act 
alone, and binding agreements between players are ruled out. The game theoretic solution concepts used in previous stud-
ies of concurrent games have therefore been those studied in non-cooperative game theory—primarily Nash equilibrium 
and reﬁnements thereof. In such a non-cooperative setting, the basic questions that we ask of a concurrent game are, for 
example, whether a particular temporal logic property holds on some computation of the system that could arise through 
players selecting strategies that form a Nash equilibrium (the E-Nash problem) or whether a property holds on all such 
computations (the A-Nash problem). These problems can be understood as game-theoretic counterparts of the conventional 
model checking problem [8]: in model checking, we are typically interested in whether a particular property could hold on 
some or all possible computations of a system, whereas in rational veriﬁcation, we are interested in whether a property 
holds on some or all computations that could arise through rational choices on the part of the players. The complexity of the 
corresponding decision problems in concurrent games has been extensively studied, and there now exist a small number of 
software tools that support rational veriﬁcation [9–12].
Non-cooperative game theory, however, represents just one branch of game theory. Cooperative game theory is a widely 
studied, albeit less well-known branch of game theory, which distinguishes itself from its non-cooperative counterpart in 
that it allows for the possibility that agents can make binding agreements with each other. The possibility of binding agreements 
makes it possible for agents to work in teams, and to enjoy the beneﬁts of cooperation. In the real world, we use contracts 
and other mechanisms (both formal and informal) to enable binding agreements.
The aim of the present paper, therefore, is to extend the study of rational veriﬁcation to include cooperative solution 
concepts [13–15]. Thus, we assume there is some (exogenous) mechanism through which players in a concurrent game 
can reach binding agreements and form teams (“coalitions”) in order to collectively achieve goals (although we emphasise 
that the nature of such a mechanism is beyond the scope of the present work). The possibility of binding cooperation 
and coalition formation eliminates some undesirable equilibria that arise in non-cooperative settings, and makes available 
a range of outcomes that cannot be achieved without cooperation. We focus on the core as our key solution concept. The 
basic idea behind the core is that it consists of those strategy proﬁles from which no subset of players could beneﬁt by 
collectively deviating. Now, in conventional cooperative games (characteristic function games with transferable utility [15]), 
this intuition can be given a simple and natural formal deﬁnition, and as a consequence, the core is probably the most 
widely-studied solution concept for cooperative games. However, the conventional deﬁnition of the core does not naturally 
map into our concurrent game setting, because in our games, coalitions are subject to externalities: whether a coalition has 
a beneﬁcial deviation depends not just on the makeup of that coalition, but also on the behaviour of the remaining players.
We begin by introducing the framework of concurrent games, and then proceed to deﬁne two variations of the core for 
such settings. In the ﬁrst, a coalition of players is assumed to have a beneﬁcial deviation if they have some course of action 
available to them which they would beneﬁt from no matter what the remaining players did (cf., the concept of the α-core in 
the classic game theory literature [16]). However, this “worst case” (maximin) formulation of the core requires a deviation 
to be beneﬁcial against all courses of action by the remaining players—even those that the remaining agents would not 
rationally choose. This motivates a second deﬁnition, where a deviation is only required to be beneﬁcial against all courses 
of action by remaining players that are credible, in the sense that those players would then be no worse off than they were 
originally. We also consider games where the agents have quantitative preferences, modelled as some kind of reward in every 
round of the game. In each case, we formally deﬁne the relevant solution concept, identify some of its key computational 
properties, give logical characterisations, and where possible, provide complexity results, which range from properties that 
can be checked in PSpace to properties that can be checked in 3ExpTime. We also study model theoretic properties related 
to the core: in particular, whether it is guaranteed to be non-empty, and whether temporal logic properties hold across 
bisimilar systems over plays (computation runs) induced by elements in the core of the game (a highly desirable property 
from a formal veriﬁcation perspective).
2

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
Structure of the paper
The remainder of this article is organised as follows:
• In the following section, we summarise the key concepts from logic and concurrent games that are used throughout 
the paper.
• In Section 3, we deﬁne the core and the main computational properties associated with it. In Section 3.4 we study the 
issue of credible coalition formations, with associated complexity results.
• In Section 4, we study the core in the quantitative setting of concurrent mean-payoff games.
• Concluding remarks and related work are given in Section 5, including a discussion around the implementation of our 
concepts using model checkers.
2. Preliminaries
2.1. Set and sequences
Given any set S, we use S∗, Sω, and S+ for, respectively, the sets of ﬁnite, inﬁnite, and non-empty ﬁnite sequences 
of elements in S. If w1 ∈S∗and w2 is any other (ﬁnite or inﬁnite) sequence, we write w1w2 for their concatenation. For 
Q ⊆S, we write S−Q for S \ Q and S−i if Q = {i}. We extend this notation to tuples u = (s1, ..., sk, ..., sn) in S1 ×···× Sn, 
and write u−k for (s1, ..., sk−1, sk+1, ..., sn), and similarly for sets of elements, that is, by u−Q we mean u without each 
sk, for k ∈Q . Given a sequence w, we write w[t] for the element in position t + 1 in the sequence; for instance, w[0]
is the ﬁrst element of w. We also use slice notation: we write w[l ...m] for the sequence w[l] ... w[m −1], w[l ...] for 
w[l]w[l + 1] ..., and w[...m] for w[0] ... w[m −1]; if m = 0, we let w[l ...m] be the empty sequence, denoted ϵ.
2.2. Games
We begin by introducing the model of multi-agent systems that we use throughout the remainder of the paper: concur-
rent game structures [6]. Informally, a concurrent game consists of a set of players, a set of actions for each of those players, 
a set of system states, and a transition function which describes how the state of the game changes, given a current state 
and an action for each of the players.
Formally, a concurrent game structure, M, is given by a tuple,
M = (Ag, St,{Aci}i∈Ag, s0, tr),
where:
• Ag and St are ﬁnite, non-empty sets of agents and states, respectively—we usually identify Ag with the set {1, ..., n};
• For each i ∈Ag, Aci is a ﬁnite, non-empty set of actions available to agent i. We associate each state s with a set of 
actions available at that state, Aci(s) ⊆Aci, and we write Ac for Ac1 × ··· × Acn;
• s0 ∈St is the initial/start state; and ﬁnally,
• tr : St × Ac →St is the transition function of the game.
As all the other components of the game can be derived from the transition function (the start state can be speciﬁed by 
listing its transitions ﬁrst, say), the size of M is the size of its transition function, given by |St| × |Ac||Ag|.
Given a concurrent game structure M, we can play a game on it as follows: the game starts in state s0, and each player 
i ∈Ag chooses an action available to them, ac0
i ∈Aci(s0). The game then moves to a new state,
s1 = tr(s0, ac0
1,..., ac0
n).
This process is then repeated. We typically write si for the ith state in the sequence, and aci = (aci
1, ..., aci
n) for the ith
vector of actions played in the sequence. Thus for all t ∈N, we have
st+1 = tr(st, act)
A run, ρ is a inﬁnite sequence ρ = s0s1s2 ... such that for every t ∈N, there exists some ac ∈Ac such that st+1 =
tr(st, ac). A path, π is a ﬁnite preﬁx of a run.
A strategy for a player i deﬁnes how player i chooses actions at each round of the game, as a function of the previous 
history of the game. Formally, a strategy is a function σi : St+ →Aci such that σi(πs) ∈Aci(s) for every π ∈St∗and s ∈St. 
Thus, for every path π, a strategy for a player i gives an action available to i from the last state of that path. The set of 
strategies for player i is denoted by i.1
A strategy proﬁle ⃗σ is a tuple of strategies, one for each player: ⃗σ = (σ1, ..., σn) ∈1 ×···×n. Observe that a strategy 
proﬁle, ⃗σ , together with a state s, induces a unique run, ρ(⃗σ, s), where:
1 Here, we deﬁne strategies with respect to ﬁnite sequences of states. One can also deﬁne them in terms of ﬁnite sequences of action proﬁles. As the 
sequence of states can be derived from the action proﬁles, but not the other way around, these strategies are more powerful than the ones we use, and 
3

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
• ρ(⃗σ, s)[0] = s; and
• ρ(⃗σ, s)[t + 1] = tr(ρ(⃗σ, s)[t], σ1(ρ(⃗σ, s)[...t]), ..., σn(ρ(⃗σ, s)[...t])), for all t ∈N.
We write ρ(⃗σ) if s = s0.
Note that viewing strategies as functions σi : St+ →Aci is problematic with respect to computational analysis, because 
the domain of such a function is inﬁnite. To be able to answer questions relating to (for example) computational complexity, 
we need a ﬁnite representation for strategies that must operate over an inﬁnite number of rounds. For this purpose, we 
follow standard practice in the concurrent game’s literature, and model strategies as ﬁnite state machines with output
(transducers) [2,3]. We note that, for players with LTL goals, such strategies are suﬃcient: no more powerful model of 
strategies is necessary [2,3]. Formally, a strategy for player i is a structure
σi = (Q i,q0
i ,δi,τi)
where:
• Q i is a ﬁnite, non-empty set of strategy states;
• q0
i ∈Q i is the initial strategy state;
• δi : Q i × St →Q i is a transition function; and
• τi : Q i →Aci is an output function.
A machine strategy works as follows: it begins in the initial state, q0
i , and chooses an action based on this, τ(q0
i ). The 
state of the game then follows the transition function into a new state. Based on this new (game) state, and the state of 
the strategy, the strategy then also moves into a new state, based on the strategy transition function. This process repeats, 
yielding a new action at each timestep. It is easy to see that a strategy proﬁle consisting solely of machine strategies will 
be eventually periodic (i.e., it will eventually enter a conﬁguration that it was in previously, at which point it will start to 
repeat its behaviour).
Sometimes, we ﬁnd we can use an even simpler model of strategies: memoryless strategies. A memoryless strategy 
σi : St →Aci simply chooses an action based on the current state of the environment. Whilst memoryless strategies are not 
nearly as expressive as ﬁnite-memory strategies, they are still of great importance, owing to their conceptual simplicity, 
their ease of implementation, and the fact that in many types of game (such as two-player mean-payoff games [18], and 
parity games [19]), memoryless strategies are suﬃcient to act optimally.2
2.3. Logics
Broadly, we need to appeal to certain logics for two main reasons: expressing the preferences of agents, and reasoning 
about those agents and their preferences. For this, we will use three logics throughout this paper: Linear Temporal Logic
(LTL) [22], Alternating-Time Temporal Logic (ATL*) [6], and Strategy Logic (SL) [23]. We use LTL for modelling agent preferences, 
and ATL*and SL for forming logical characterisations of the game-theoretic concepts we will study. Whilst it can be seen 
more formally from the syntax and semantics presented below, we note here that LTL can be seen as a subset of ATL*, 
which can be seen as a subset of SL.
2.3.1. Linear temporal logic
LTL is a widely used logic for reasoning about the behaviours of concurrent systems, and while we present the key 
concepts here, we refer the reader to any standard temporal logic textbook for details (e.g., [24]).
Let AP be a set of propositional variables. Then the syntax of an LTL formula ϕ is given by the following grammar:
ϕ := p | ¬ϕ | ϕ ∨ϕ | Xϕ | ϕUϕ,
where p ∈AP. The set of all LTL formulae over a set of propositional variables AP is denoted L(AP). If AP is clear from 
the context, we may instead just write L. We also introduce the traditional propositional abbreviations, · ∧·, · →·, · ↔·, 
deﬁned in the usual way, as well as the abbreviations Fϕ for ⊤Uϕ, and Gϕ for ¬F¬ϕ.
Typically, in the context of model checking, the semantics of LTL formulae are deﬁned relative to labelled transition 
systems [25], or Kripke structures [26], but for our purposes, we deﬁne them with respect to the inﬁnite runs generated by 
concurrent game structures. Formally, let M be a concurrent game structure, and let λ : St →P (AP) be a labelling function, 
mapping states to sets of propositional variables. Then given an inﬁnite run ρ ∈Stω and an LTL formula ϕ, we say that 
(M, λ, ρ) models ϕ and write (M, λ, ρ) |= ϕ according to the following inductive deﬁnition:
indeed, provide desirable properties such as the invariance of Nash equilibria under bisimilarity [17]. However, strategies deﬁned relative to states are 
standard in the concurrent game structures [6] and rational veriﬁcation [3] literature, so we use this model for consistency. For more details and further 
discussion, refer to [17].
2 For a characterisation of the types of games where memoryless strategies are suﬃcient for one or both players to play optimally, refer to [20,21].
4

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
• For p ∈AP, we have (M, λ, ρ) |= p if and only if p ∈λ(ρ[0]);
• For ϕ ∈L(AP), we have (M, λ, ρ) |= ¬ϕ if and only if it is not the case that (M, λ, ρ) |= ϕ;
• For ϕ, ψ ∈L(AP), we have (M, λ, ρ) |= ϕ ∧ψ if and only if we have both (M, λ, ρ) |= ϕ and (M, λ, ρ) |= ψ;
• For ϕ ∈L(AP), we have (M, λ, ρ) |= Xϕ if and only if (M, λ, ρ[1 ...]) |= ϕ;
• For ϕ, ψ ∈L(AP), we have (M, λ, ρ) |= ϕUψ if and only if there exists some j ≥0 such that (M, λ, ρ[ j ...]) |= ψ and 
for all i < j we have (M, λ, ρ[i ...]) |= ϕ.
For notational convenience, we will write (G, ρ) |= ϕ as shorthand for (M, λ, ρ) |= ϕ, and if G is apparent from the context, 
we will just write ρ |= ϕ.
2.3.2. Alternating-time temporal logic
ATL*is an extension of the branching-time temporal logic CTL* [27] — we shall use this logic in our proofs to reason 
about coalitions of agents more effectively. The key operator in ATL*is the cooperation modality ⟨⟨C⟩⟩φ, which asserts that 
the coalition C has the power to enforce the temporal property φ; more speciﬁcally, that there is a collection of strategies 
for C such that if C follow these strategies, then no matter what other agents do, φ is guaranteed to be made true. Given a 
set of atomic propositions AP and a set of agents Ag, the language of ATL* formulae is deﬁned by the following grammar:
ϕ ::=p | ¬ϕ | ϕ ∨ϕ | ⟨⟨C⟩⟩ψ
ψ ::=ϕ | ¬ψ | ψ ∨ψ | Xψ | ψUψ
where p ∈AP and C ⊆Ag. We call the formulae produced by ϕ in the above grammar ATL*state formulae, and denote 
the set that contains them by Ls(AP, Ag) and those generated by ψ in the above grammar ATL*path formulae, denoted 
by Lp(AP, Ag). (They are given these names as their semantics are deﬁned relative to states and paths respectively.) We 
emphasise that only ATL*state formulae are well-formed ATL*formulae, and thus, we write L(AP, Ag) as shorthand for 
Ls(AP, Ag). When either AP or Ag, or both, are known, we may omit them. With AP′ ⊆AP, we may write ϕ|AP′ if ϕ ∈
L(AP′, Ag) for some set of agents Ag.
In addition to the abbreviations used for LTL formulae as described above, we also use the shorthands Eϕ for ⟨⟨Ag⟩⟩ϕ, 
Aϕ for ⟨⟨∅⟩⟩ϕ, and Cϕ for ¬ ⟨⟨C⟩⟩¬ϕ. Finally, we deﬁne the size of an ATL*formulae ϕ as its number of subformulae.
To deﬁne the semantics of ATL*formulae, we actually need to deﬁne two semantic relations, |=s (for state formulae) 
and |=p (for path formulae). So, let M be some concurrent game structure, along with a labelling function λ : St →P (AP). 
Then given a state, s ∈St and an ATL*formula ϕ, we say that (M, λ, s) models ϕ and write (M, λ, s) |= ϕ according to the 
following inductive deﬁnition:
• For ϕ ∈Ls(AP, Ag), we have (M, λ, s) |= ϕ if and only if (M, λ, s) |=s ϕ;
• For operators that lie in LTL, their inductive semantics are the same as in LTL;
• For ϕ ∈Lp(AP, Ag), we have (M, λ, s) |=s ⟨⟨C⟩⟩ϕ if and only if there is some strategy vector ⃗σC for the coalition C, such 
that for all complementary strategy proﬁles, ⃗σAg\C , it is the case that (M, λ, ρ((⃗σAg\C, ⃗σC), s)) |=p ϕ holds;
• For ϕ ∈Ls(AP, Ag), we have (M, λ, ρ) |=p ϕ if and only if (M, λ, ρ[0]) |=s ϕ;
Where the concurrent game and labelling function are clear from context, we will simply write s |= φ. Given a concurrent 
game structure M and a labelling function λ : St →P (AP), we say that ϕ is satisﬁable if there exists some state s ∈St such 
that (M, λ, s) |= ϕ. Moreover, we say that ϕ is equivalent to ψ if for all states s ∈St we have (M, λ, s) |= ϕ if and only if 
(M, λ, s) |= ψ.
Note that LTL can be seen as the sublogic of ATL*given by all formulae Aϕ, where the formula ϕ does not contain 
the “coalition” quantiﬁers ⟨⟨C⟩⟩or C. Thus ATL*is a particularly effective tool for reasoning about the LTL properties that 
coalitions can achieve, and this is exactly how we will use it when we come to prove the complexity bounds of our decision 
problems. Speciﬁcally, if we have an LTL game G, we can write (G, s) |= ϕ as shorthand for (M, λ, s) |= ϕ, and furthermore, 
if the game G is apparent from the context, we shall drop it and simply write s |= ϕ instead.
The relevant decision problem here is the model checking problem for ATL*, which we utilise heavily in the following 
section:
ATL*Model Checking:
Given: Concurrent game M, labelling function λ, state s ∈St, and ATL* formula ϕ.
Question: Is it the case that (M, λ, s) |= ϕ?
This problem is 2ExpTime-complete [6] for games with two or more players, and PSpace-complete for one-player games 
(as then, the problem reduces to LTL model checking) [28].
2.3.3. Strategy logic
Whilst ATL* is a powerful tool for reasoning about coalitions, it famously cannot represent certain game-theoretic con-
cepts, such as the Nash equilibrium [23] — explicit quantiﬁcation over strategies is required to do this. For our purposes, 
5

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
we will be able to make most of the necessary logical characterisations using ATL*, but when we come to the strong core in 
Section 3.4, we too will also need the ability to reason explicitly about strategies. SL extends LTL with two strategy quanti-
ﬁers, ⟨⟨x⟩⟩and [[x]], and an agent binding operator (i, x), where i is an agent and x is a variable. These operators can be 
read as “there exists a strategy x”, “for every strategy x”, and “bind agent i to the strategy associated with variable x”, respectively. 
Formally, SL formulae are inductively built from a set of propositions AP, variables Var, and agents Ag, using the following 
grammar, where p ∈AP, x ∈Var, and i ∈Ag:
ϕ ::= p | ¬ϕ | ϕ ∧ϕ | Xϕ | ϕUϕ | ⟨⟨x⟩⟩ϕ | [[x]]ϕ | (i, x)ϕ.
We can now present the semantics of SL. First, denoting the set of all strategies by Str, we deﬁne an assignment to 
be a partial function that maps variables and agents to strategies, χ ∈Asg = (Var ∪Ag) ⇀Str. We then use the notation 
χ[i →f ]/χ[x →f ] to refer to the assignment which equals f on i/x, and agrees with χ on every other input on which it 
is deﬁned. Then, given a concurrent game structure M, for all SL formulae ϕ, states s ∈St in M, and assignments χ ∈Asg, 
the relation M, χ, s |= ϕ is deﬁned as follows:
1. For LTL formulae embedded in SL, their inductive semantics are the same as in LTL;
2. For all formulae ϕ and variables x ∈Var we have:
(a) M, χ, s |= ⟨⟨x⟩⟩ϕ if and only if there exists some f ∈Str such that M, χ[x →f ], s |= ϕ;
(b) M, χ, s |= [[x]]ϕ if and only if for all f ∈Str we have M, χ[x →f ], s |= ϕ;
3. For every agent i ∈Ag and variable x ∈Var, we have M, χ, s |= (i, x)ϕ if and only if M, χ[i →χ(x)], s |= ϕ.
For a sentence ϕ, that is, a formula with no free variables and agents [23], we say that M satisﬁes ϕ, and write M |= ϕ
in that case, if M, ∅, s0 |= ϕ, where ∅is the empty assignment. We use the following abbreviations: ⟨i⟩ϕ for ⟨⟨x⟩⟩(i, x)ϕ
and [i]ϕ for [[x]](i, x)ϕ, which can be intuitively understood as “there is a strategy for agent i such that ϕ holds” and “ϕ
holds, for all strategies of agent i”, respectively. We extend this notation to sets of players and write, for instance, ⟨C⟩ϕ
instead of ⟨i⟩ ...⟨j⟩ϕ, where C = {i,..., j}, and similarly for the universal quantiﬁer operator. Then, with ⟨C⟩ϕ we mean 
that “coalition C has a joint strategy such that ϕ holds.”
2.4. LTL games
We can now deﬁne LTL games [2,29,30]. The key idea in an LTL game is that each player i is associated with an LTL goal 
formula γi, which it desires to see satisﬁed. Formally, an LTL game, G, is given by a structure
G = (M, AP,λ,(γi)i∈Ag),
where M is a concurrent game structure, AP is a set of atomic propositions, λ : St →P (AP) is a labelling function, and for 
each i ∈Ag, γi is an LTL formula over AP that deﬁnes that player’s preference relation over runs.
We now describe how temporal goal formulae γi induce preference relations ⪰i over runs. First, given that under a 
provided run, LTL goals are either satisﬁed or not, we can identify a set of “winners” and a set of “losers” for that run. 
Formally, let W(ρ) denote the set of players that get their goal achieved under ρ, and let L(ρ) denote the set of players 
that do not:
W(ρ) = {i ∈Ag | ρ |= γi}
L(ρ) = Ag \ W(ρ).
We can now use winners and losers to deﬁne the preference relations of our agents. Intuitively, agent i will always 
strictly prefer a run that satisﬁes its goal γi over one that does not, but is indifferent between two runs that satisfy its goal, 
and is indifferent between two runs that fail to satisfy its goal. Formally, for two runs ρ, ρ′, and a player i, we have ρ ⪰i ρ′
if and only if i is a winner under ρ (note the lack of dependence of ρ′) or i is a loser under both ρ and ρ′. Put alternatively, 
we have ρ ⪰i ρ′ if and only if (M, λ, ρ′) |= γi implies that (M, λ, ρ) |= γi. Strict preference relations ≻i are deﬁned in the 
standard way: ρ ≻i ρ′ if and only if ρ ⪰i ρ′ but not ρ′ ⪰i ρ. We leave the reader to verify, ﬁrst, that this deﬁnition matches 
our informal explanation of preferences above, and second, that the relations ⪰i so deﬁned are indeed preference relations 
(i.e., the binary relation ⪰i is reﬂexive, complete, and transitive). Finally, we note that we use L in two different senses in 
this paper: to denote the languages of various logics, and to denote the set of losers of a run. As these generally appear in 
different contexts, and take different inputs, there should be no ambiguity as to what interpretation should be taken.
With preference relations deﬁned, we can introduce game-theoretic solution concepts. For now, we will stick with non-
cooperative solution concepts. First, a strategy proﬁle ⃗σ is said to be a Nash equilibrium if there is no player i ∈Ag and 
strategy σ ′
i for i such that we have (⃗σ−i, σ ′
i ) ≻i ⃗σ . That is, ⃗σ is a Nash equilibrium if no player can beneﬁt by unilater-
ally changing its strategy (assuming all other players leave their strategies unchanged). Let NE(G) denote the set of Nash 
equilibria of the game G.
We emphasise that Nash equilibrium only considers unilateral deviations, i.e., deviations by individual players. Compare 
this to the notion of a strong Nash equilibrium [31,32]: a strategy proﬁle ⃗σ is a strong Nash equilibrium if there is no 
6

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
coalition C and no strategy σ ′
C such that for all i ∈C we have (⃗σ−i, σ ′
i ) ≻i ⃗σ . Thus, strong Nash equilibria are those strategy 
proﬁles that are immune to multilateral deviations.
Finally, we mention in passing the notion of the coalition-proof Nash equilibrium [33,34]. Whilst we refrain from giving 
a formal deﬁnition here, informally, this can be thought of as capturing the outcomes that would arise when players have 
unlimited, but non-binding, pre-play communication.
3. Cooperative rational veriﬁcation
3.1. Deﬁning the core
We want to deﬁne counterparts of the rational veriﬁcation problems E-Nash and A-Nash, as studied in [2,3], but for 
cooperative settings. For this, we need a version of the core for our concurrent game setting. The core is probably the best-
known solution concept in cooperative game theory. Like Nash equilibrium in the non-cooperative setting, the core deﬁnes 
a notion of stability for games, but whereas Nash equilibrium only requires that an outcome is stable in the sense that it 
admits no individual beneﬁcial deviations, the core requires that an outcome admits no beneﬁcial deviations by coalitions. In 
the “standard” model of cooperative games, (cooperative games with transferable utility in characteristic function form [15]), 
this intuition is easily formalised, but in concurrent games, there is an important diﬃculty, as follows.
Suppose a coalition of players C ⊆Ag is contemplating participating in a strategy proﬁle ⃗σ , and in particular, are attempt-
ing to determine whether they have a cooperative beneﬁcial deviation from ⃗σ . Now, as they consider possible beneﬁcial 
deviations — collective strategies ⃗σC ∈
i∈C i — what assumptions should C make about the behaviour of the remaining players 
Ag \ C? In particular, assuming that the remaining players will not respond, by potentially altering their strategy, is im-
plausible in a cooperative setting.3 Rational players who can cooperate will respond to the deviation rationally and in a 
cooperative way against the players in C. And, crucially, whether or not C’s putative deviation is in fact beneﬁcial may well 
depend upon the behaviour of the remaining players. In game theoretic terms, our concurrent game setting is subject to 
externalities: the performance of the coalition C depends not just on the makeup of the coalition C, but on the behaviour of 
the remaining players.
It is well-known that cooperative solution concepts are diﬃcult to deﬁne in the presence of externalities [15]. In par-
ticular, there is no universally accepted deﬁnition of the core for games with externalities. Our ﬁrst deﬁnition of the core 
for concurrent games, therefore, captures worst case (maximin) reasoning. Thus, when coalition C is contemplating a devi-
ation, it requires that this deviation will be beneﬁcial no matter what the remaining players do. This idea has been explored 
in the concept of the α-core in cooperative games [16]. To make it formal, we need to formalise the notion of a beneﬁcial 
deviation. Let ⃗σ be a strategy proﬁle and let C be a coalition; then we say that ⃗σ ′
C is a beneﬁcial deviation from ⃗σ if both:
1. C ⊆L(ρ(⃗σ)); and
2. For all ⃗σ ′−C , we have C ⊆W(ρ(⃗σ ′
C, ⃗σ ′
−C)).
In other words, ⃗σ ′
C is said to be a beneﬁcial deviation from ⃗σ if the players in C would be better off playing ⃗σ ′
C , rather than 
their respective strategies in ⃗σ , no matter what strategies the players outside C play. The core of a game G, denoted core(G), is 
then deﬁned to be the set of outcomes of G that no coalition has a beneﬁcial deviation from.
Example 1. Consider the following game, which illustrates how cooperative and non-cooperative solution concepts differ: it 
contains a poor quality Nash equilibrium that is not in the core. The ability to cooperate makes it possible for agents to 
avoid the undesirable equilibrium. The game contains two players, Ag = {1, 2} and two variables AP = {p,q}, with player 1’s 
action set being Ac1 = {pt, pf } and player 2’s action set being Ac2 = {qt,qf }, satisfying that, for every reachable state, if 
player 1/2 plays pt/qt then p/q will hold, and will not hold if pf /qf is played instead (i.e., player 1 “controls” the value of 
p and player 2 the value of q). Their goals are identical (and so the game is a coordination game): γ1 = γ2 = G(p ∧q). Now, 
consider the strategy proﬁle ⃗σ in which both players simply ﬁx their respective variables to be false forever (i.e., play pf
and qf forever). Neither player will have their goal achieved by such a strategy proﬁle. However, the strategy proﬁle forms a 
Nash equilibrium, because unilateral deviation cannot improve the situation: neither player has an alternative strategy which 
would make them better off. In fact, there are inﬁnitely many such poor-quality Nash equilibria in this game, where neither 
player gets their goal achieved. However, this strategy proﬁle is not in the core, because there is a cooperative beneﬁcial 
deviation to the strategy proﬁle in which both players ﬁx their variables to be true forever (i.e., play pt and qt forever). And, 
in fact, in every strategy proﬁle which lies in the core, both players get their goal achieved. Thus, using the core instead of 
Nash equilibrium eliminates poor quality equilibria from the game, leading to socially more desirable outcomes.
Before proceeding, we note that additional variations of the core have been proposed in the game theory literature 
on cooperative games with externalities, in particular, the β-core and γ -core. The β-core assumes that players outside a 
deviating coalition will choose strategies to maximally punish the deviating coalition, and that members of a deviating 
3 This is the kind of behaviour that one has to assume to deﬁne strong Nash equilibrium, a non-cooperative solution concept.
7

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
coalition will then collectively best respond to these punishment strategies. Thus, the β-core assumes a minimax choice 
on behalf of the deviating coalition. In contrast, the γ -core assumes that players outside a deviating coalition will form 
singleton coalitions, with each non-deviator choosing individual strategies in a best response to the deviating coalition 
strategies. We note that these solution concepts are much less studied than the α-core, and are arguably less natural and 
less well-motivated. We, therefore, leave them for future study (see also comments in the Related Work Section 5).
3.2. Decision problems
In rational veriﬁcation [2–4] we are mainly interested in checking which temporal logic properties are satisﬁed in a given 
solution concept of a game; typically, in the non-cooperative setting, we study what LTL formulae hold in the Nash equilibria 
NE(G) of a game G. In the cooperative setting, as introduced here, we are instead interested in what properties hold in the 
core of the game. The two main decision problems in rational veriﬁcation are checking whether a temporal logic formula is 
satisﬁed by some/every stable strategy proﬁle of the game. For the core, these problems are deﬁned as follows—cf. [2–4].
E-Core:
Given: Game G, LTL formula ϕ.
Question: Is it the case that ∃⃗σ ∈core(G) : ρ(⃗σ) |= ϕ?
A-Core:
Given: Game G, LTL formula ϕ.
Question: Is it the case that ∀⃗σ ∈core(G) : ρ(⃗σ) |= ϕ?
One decision problem considered in the non-cooperative setting is the Non-Emptiness problem, which asks if the set 
of Nash equilibria of a given game is non-empty. However, as we shall show momentarily, the core of the game is always 
non-empty, so it does not make sense to consider the corresponding problem in the cooperative setting.
We will also be interested in two additional decision problems: checking whether a given strategy proﬁle is in the core 
(Core Membership), and checking whether a given strategy vector for a coalition is a beneﬁcial deviation with respect to a 
strategy proﬁle (Beneﬁcial Deviation). Formally:
Core Membership:
Given: Game G, strategy proﬁle ⃗σ .
Question: Is it the case that ⃗σ ∈core(G)?
Beneﬁcial Deviation:
Given: Game G, strategy proﬁle ⃗σ , coalition C, and deviation ⃗σ ′
C .
Question: Is ⃗σ ′
C a beneﬁcial deviation from ⃗σ ?
In what follows it is helpful to use the concept of a fulﬁlled coalition: a coalition is fulﬁlled if they are able to achieve 
their goals irrespective of what other players do; that is, they have a collective strategy that will guarantee their goals are 
achieved. Formally, we say that a coalition of players C is fulﬁlled if there is a joint strategy ⃗σC for C ⊆Ag such that for all 
joint strategies ⃗σ−C for Ag \ C we have
ρ((⃗σC, ⃗σ−C)) |=

i∈C
γi.
Thus, a fulﬁlled coalition has a winning strategy to collectively achieve their goals. Since we are considering cooperative 
games, the question is whether such a coalition will form. We have the following:
Lemma 1.
1. There are games G, with strategy proﬁles ⃗σ ∈core(G), containing fulﬁlled coalitions C ⊆Ag such that C ⊈W(ρ(⃗σ));
2. For every game G, strategy proﬁle ⃗σ ∈core(G), and fulﬁlled coalition C, we have that C ∩W(ρ(⃗σ)) ̸= ∅;
3. For every game G and fulﬁlled coalition C, then there is ⃗σ ∈core(G) such that C ⊆W(ρ(⃗σ)).
Informally, the ﬁrst part of the lemma says that the fact that a coalition is fulﬁlled does not mean that every player in 
such a coalition is guaranteed to get its goal achieved under an arbitrary member of the core. However, the second part of 
the lemma says that in any member of the core, some agents of every fulﬁlled coalition must get their goals achieved. And, 
the third part of the lemma says that for every fulﬁlled coalition the core contains a strategy proﬁle in which every player 
of this coalition gets its goal achieved.
Proof of Lemma 1.1. Consider a game with three states, start, up, down, and three players, {1, 2, 3}. Let start be the start 
state of the game, and assume that players 2 and 3 have only one action available to them, and player 1 has two actions, H
8

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
and T. As such, the transition function can be deﬁned solely in terms of player 1’s action, and we let tr(start, H) = up and 
tr(start, T) = down. The remaining two states are sink states, with tr(up, H) = tr(up, T) = up and tr(down, H) = tr(down, T) =
down. Further suppose we have AP = {p, q}, and that λ(start) = ∅, λ(up) = {p} and λ(down) = {q}. Finally, let the goals of the 
players be as follows: γ1 = ⊤, γ2 = X G p, and γ3 = X Gq. As the game will always end in one of two sink states after the 
ﬁrst action, we can identify strategies simply by the ﬁrst action of player 1: H or T.
Now, note that the coalition {1, 3} is a fulﬁlled coalition: if player 1 plays T, then the game will end up in the state 
down, and will permanently remain there, satisfying player 1’s and player 3’s goals. However, note that the strategy proﬁle 
where player 1 plays H is a member of the core: player 1’s and player 2’s goals are satisﬁed, and as player 3 only has one 
action, they cannot deviate to get their goal achieved. Thus, we have a fulﬁlled coalition and a member of the core, such 
that the fulﬁlled coalition is not a subset of the winners of the member of the core. Note the same argument could have 
been applied the other way around — {1, 2} is a fulﬁlled coalition, and T is a member of the core whose winners are not a 
superset of {1, 2}.

Proof of Lemma 1.2. Let G be a game, ⃗σ be a strategy proﬁle that lies in the core, and C a fulﬁlled coalition. For the sake 
of a contradiction, further suppose that the intersection of C and W(ρ(⃗σ)) is empty. This means that for each i ∈C, we 
have ρ(⃗σ) ̸|= γi. But as C is a fulﬁlled coalition, there exists a strategy ⃗σ ′
C , such that for all counterstrategies ⃗σ−C , we 
have ρ((⃗σC, ⃗σ−C)) |= 
i∈C γi. Naturally, this implies that ρ((⃗σC, ⃗σ−C)) |= γi for all i ∈C. But then this implies that ⃗σ ′
C is a 
beneﬁcial deviation for C, contradicting the fact that ⃗σ is a member of the core. Thus, we conclude that C and W(ρ(⃗σ))
have non-empty intersection.

Proof of Lemma 1.3. Let G be a game, and C be a fulﬁlled coalition. Let ⃗σ 1 = (⃗σC, ⃗σ−C) be any strategy proﬁle under which 
C are fulﬁlled (that is, every member of C achieves their goal regardless of what the countercoalition does). If ⃗σ 1 is a 
member of the core, then we are done. If not, then there exists some coalition D ⊆Ag \ C with a beneﬁcial deviation, ⃗σ ′
D. 
Then under the strategy ⃗σ 2 = (⃗σC, ⃗σ ′
D, ⃗σ−(C∪D)), every player in C gets their goal achieved. As before, if ⃗σ 2 is a member of 
the core, then we are done. If not, we can repeat this process, yielding a sequence of strategies, ⃗σ 1, ⃗σ 2, ⃗σ 3 .... As the losers 
of the successive strategy proﬁles get strictly smaller each time, this process can only continue a ﬁnite number of times. 
By construction, the ﬁnal member of the sequence will model the goal of every player in C, and will have no beneﬁcial 
deviations, making it a member of the core. Thus, we see that there exists some strategy proﬁle in the core such that C is 
a subset of the winners of the strategy proﬁle.

In fact, our proof of 1.3 can be modiﬁed to give us a stronger result, namely that the core is always non-empty, a 
highly desirable game-theoretic property, as it ensures the existence of stable strategy proﬁles for every game, making them 
rationally implementable in practice. This contrasts with the conventional formulation of the core in transferable utility 
games [15].
Theorem 1. For every game G, we have core(G) ̸= ∅.
Proof. Identical to the proof of Lemma 1.3, but instead of setting ⃗σ 1 to be a strategy proﬁle under which some coalition 
can be fulﬁlled, let it be any arbitrary strategy proﬁle, and continue as before.

As fulﬁlled coalitions can help us understand the coalition formation power in a game, we will also be interested in the 
following decision problem about coalitions.
Fulﬁlled Coalition:
Given: Game G, coalition C ⊆Ag.
Question: Is C a fulﬁlled coalition of G?
In the next section, we will investigate these decision problems, as well as some model-theoretic properties of the core.
3.3. Characterising the core
In this section we will study the complexity of the decision problems introduced in the previous section, and will estab-
lish some other properties of the core. We have already seen in Theorem 1 that the core is always non-empty, and we shall 
go on to prove that the satisfaction of an LTL property on some/every outcome in the core is bisimulation-invariant [35]. 
These two results sharply contrast with the rational veriﬁcation problem in the non-cooperative setting: in these, the set 
of Nash equilibria of a game is not guaranteed to always be non-empty [2], nor does bisimulation-invariance hold in the 
general case [17].
The ﬁrst of our own decision problems we will consider is Fulﬁlled Coalition, which we solve in the general case 
through a logical characterisation using ATL*.
9

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
Theorem 2. Fulﬁlled Coalition is PSpace-complete for one-player games, and it is 2ExpTime-complete for games with more than 
one player.
Proof. For membership we observe that given a game G = (M, γ1, ..., γn) and a coalition C ⊆Ag, it is the case that C is 
fulﬁlled if and only if s0 |= ⟨⟨C⟩⟩
i∈C γi holds. By appealing to ATL∗Model Checking, the two upper bounds immediately 
follow. For the lower bounds, we can reduce the problem of checking for the existence of a winning strategy in a two-player 
game with LTL goals as deﬁned in [36] for 2ExpTime-hardness and existential LTL model checking for PSpace-hardness [28]. 
Note that, as such, Fulﬁlled Coalition with more than one player is 2ExpTime-hard even for two-player zero-sum games, 
i.e., for games with γ1 = ¬γ2.

Fulﬁlled coalitions give an indication of which stable coalitions may form, but are insuﬃcient to characterise the core, 
and therefore, to check E-Core and A-Core properties of a multi-agent system. To do this, we adopt a different strategy and 
show that these two decision problems are, in general, also 2ExpTime-complete.
Theorem 3. E-Core and A-Core are PSpace-complete for one-player games and 2ExpTime-complete for games with more than one 
player.
Proof. Let us consider E-Core ﬁrst. For membership we observe that given a game G and an LTL formula ϕ, it is the case 
that (G, ϕ) ∈E-Core if and only if s0 |= ϕE-Core(G, ϕ) holds, where ϕE-Core(G, ϕ) is the following ATL* formula:

W ⊆Ag
⎛
⎝⟨⟨Ag⟩⟩
⎛
⎝ϕ ∧

i∈W
γi ∧

j∈Ag\W
¬γ j
⎞
⎠∧

L⊆Ag\W
L

j∈L
¬γ j
⎞
⎠
which states that there is a path in G that satisﬁes ϕ as well as the goals of a set of players W (the “winners”), and that 
for every subset of players L that do not get their goals achieved in such a path (the “losers”), it is not the case that those 
players have a beneﬁcial deviation from the path. As before, we appeal to ATL∗Model Checking to obtain the upper bounds; 
here we need to call a 2ExpTime algorithm an exponential number of times—one for each coalition of winners, and accept 
if any single one of them accepts.
For the lower bounds, as for Fulﬁlled Coalition, we can reduce the problem of checking for the existence of a winning 
strategy in a two-player game with LTL goals as deﬁned in [36] for 2ExpTime-hardness and existential LTL model checking for
PSpace-hardness [28]. In fact, note that for one-player games, E-Core and Fulﬁlled Coalition are equivalent when φ = γ1, 
as well as when γ1 = φ, γ2 = ¬φ, and C = {1}, which are the cases that arise in the two reductions above mentioned.
Finally, for A-Core, note that (G, ϕ) /∈A-Core if and only if (G, ¬ϕ) ∈E-Core: (G, ϕ) /∈A-Core means it is not the case 
that for all members of the core ⃗σ we have ρ(⃗σ) |= ϕ. Pushing the negation through the quantiﬁer, this implies that 
there exists a member of the core such that ρ(⃗σ) ̸|= ϕ, or ρ(⃗σ) |= ¬ϕ. The same argument can then be applied for the 
reverse direction. Then, since both PSpace and 2ExpTime are deterministic complexity classes, we can conclude that A-Core
is PSpace-complete if |Ag| = 1 and 2ExpTime-complete if |Ag| > 1, as it is for E-Core.

We now study Core Membership and Beneﬁcial Deviation. For these two problems we ﬁrst need to deﬁne how we will 
represent strategy proﬁles, as at present, strategies are deﬁned as inﬁnitary structures, which map ﬁnite histories to players’ 
actions.
Given this ﬁnite representation for strategies, we can establish the complexity of Core Membership and Beneﬁcial Devi-
ation.
Theorem 4. Core Membership is PSpace-complete for one-player games and 2ExpTime-complete for games with more than one 
player.
Proof. For membership, we ﬁrst compute the winners and losers with respect to ⃗σ = (σ1, ..., σn), the outcome of the game. 
This can be done in PSpace (it is equivalent to LTL model checking over a “product automata” or “concurrent program” [37]). 
Once we have computed W , we can check, for every L ⊆Ag \ W , whether L has a beneﬁcial deviation. This is true if 
and only if L is a fulﬁlled coalition. Because this can be checked in PSpace for one-player games and in 2ExpTime for 
games with more than one player, the two upper bounds immediately follow. For the lower bounds, we use Lemma 1 and 
Theorem 2 again. Consider the following game. Let ϕ be a satisﬁable LTL formula and ⃗σ an outcome that does not satisfy 
ϕ. Then, (G, ⃗σ) ∈Core Membership if and only if (G, {1}) /∈Fulﬁlled Coalition, whenever γ1 = ϕ and γ j = ¬ϕ, for every 
player j ∈Ag \ {1}.

Let us now consider Beneﬁcial Deviation. This is the only “easy” problem for multi-player games: it can be solved 
in PSpace. To show this, we again need to ﬁnd a different proof strategy. Consider any input instance (G, ⃗σ , ⃗σ ′
C) of the 
problem. We observe that, because ⃗σ ′
C is ﬁxed, we can make it part of the arena where the game is played, and then check 
10

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
if players not in C have a joint strategy for 
j∈C ¬γ j. Due to the deﬁnition of beneﬁcial deviation, we also need to check if 
ρ(⃗σ) |= 
j∈¬γ j holds or not.
In other words, the reason why this problem can be solved in PSpace for multi-player games, unlike all other decision 
problems we have studied so far (which, in general, can be solved in doubly exponential time), is that this decision problem 
can be reduced to a one-player game (given by coalition Ag \ C) with an LTL goal (given by γAg\C = 
j∈C ¬γ j) over a 
“product arena” (denoted by MC ) built from a concurrent game structure M and the joint strategy ⃗σ ′
C that we want to 
check.
Theorem 5. Beneﬁcial Deviation is PSpace-complete, even for one-player games.
Proof. Checking that ρ(⃗σ) |= 
j∈C ¬γ j holds can be done in PSpace. Again, this is equivalent to model checking LTL for-
mulae over a “product automata” or “concurrent program” [37]. If the statement does not hold, then, by deﬁnition, ⃗σ ′
C is 
not a beneﬁcial deviation, as at least one player in C already has its goal satisﬁed by ⃗σ . If the statement holds, then we 
check that ρ(⃗σ ′
−C, ⃗σ ′
C) |= 
j∈C γ j holds, for all joint strategies ⃗σ ′
−C for players not in C. We do this in PSpace by checking 
whether it is not the case that (MC, λ′, s0′) |= 
j∈C ¬γ j holds, where MC = (Ag′, Ac′, St′, s0′, tr′) is the concurrent game 
structure deﬁned as follows:
• Ag′ = {0}, Ac′ = i∈Ag\CAci;
• St′ = St ×  j∈C Q j;
• s0′ = (s0, q0
x, ..., q0
y), such that σz = (Q z, q0
z, δz, τz), ⃗σ ′
C = (σx, ..., σy), and z ∈{x,..., y};
• tr′((s, qx, ..., qy), (a, ..., b)) = (s′, q′
x, ..., q′
y) such that
– s′ = tr(s, τ(qx), ..., τ(qy), a, ..., b), and
– q′
z = δ(qz, s), with z ∈{x,..., y};
and λ′ is deﬁned as,
λ′(s,qx,...,qy) = λ(s).
In other words, MC transitions just like M save that it is restricted to the behaviour already deﬁned by ⃗σ ′
C .
For the lower bound we use LTL model checking.

In addition to the above complexity results, we also have a model-theoretic result. Before we can state it, however, we 
need to deﬁne the notion of bisimilarity [35,38]. Let M and M′ be two concurrent game structures with the same agents Ag, 
actions Ac and atomic propositions AP. Moreover, let their respective sets of states and transition functions be denoted by 
St/St′ and tr/tr′ respectively. Finally, let λ be a labelling function on St and λ′ a labelling function of St′. Then a bisimulation
between s∗∈St and t∗∈St′ is a non-empty binary relation, ∼⊆St × St′, such that,
• We have s∗∼t∗;
• For all s ∈St and for all t ∈St′, if s ∼t then λ(s) = λ′(t);
• For all s1, s2 ∈St and for all t1 ∈St′, if s1 ∼t1 and tr(s1, ac) = s2 for some ac ∈Ac, then tr′(t1, ac) = t2 for some t2 ∈St′
with s2 ∼t2;
• For all s1 ∈St and for all t1, t2 ∈St′, if s1 ∼t1 and tr(t1, ac) = t2 for some ac ∈Ac, then tr′(s1, ac) = s2 for some s2 ∈St′
with s2 ∼t2.
We say two concurrent game structures are bisimilar if their start states are bisimilar. For further details of bisimilarity 
over concurrent game structures, refer to [17]. We then say that a property P is bisimulation-invariant when if M ∼N, then 
P holds on M if and only if P holds on N.
We are now in a position to state our result: informally, we have that checking whether an LTL formula is satisﬁed by 
some outcome in the core is a bisimulation-invariant property. This result is easy, and follows directly from the membership 
proof of E-Core.
Corollary 1. Let G = (M, γ1, ..., γn) be a game, ϕ be an LTL formula, and M′ be a concurrent game structure that is bisimilar to M. 
Then, (G, ϕ) ∈E-Core if and only if (G′, ϕ) ∈E-Core, where G′ = (M′, γ1, ..., γn).
Proof. Follows from the fact that ATL* is bisimulation-invariant, and that the core can be characterised in ATL* using ϕE-Core, 
as deﬁned in the proof of Theorem 3. More speciﬁcally, it follows from the fact that (M, λ, s0) |= ϕE-Core(G, ϕ) if and only if 
(M′, λ′, s0′) |= ϕE-Core(G′, ϕ).

11

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
s0
s1
s2
s3
(a,a)
(a,b)
(b,b)
(b,a)
∗
∗
∗
Fig. 1. A game with a non-credible strategy.
3.4. On credible coalition formation: the strong core
As we noted above, our deﬁnition of the core assumes worst-case reasoning: a deviation must be beneﬁcial against all
counter-responses. This deﬁnition is robust in the sense that any core-stable outcome is stable in a very strong sense, but 
one could argue that in some cases it is too strong. In particular, when a coalition C is contemplating a deviation ⃗σC , it can 
surely assume that the remaining players will not act against their own interests. Thus, one could argue that a deviation 
need not be beneﬁcial for all behaviours of the remaining players, but only those behaviours that are credible, in the sense 
that the remaining players might rationally choose them. To make this discussion concrete, consider the following example.
Example 2. Suppose we have a two-player game G, with a start state, s0 and three sink states, s1, s2, and s3. Each player 
has two actions available to them, a and b, and the transition function from the start state is deﬁned as follows:
tr(s0,(a,a)) = s1,
tr(s0,(a,b)) = s2,
tr(s0,(b,a)) = s3,
tr(s0,(b,b)) = s3
Thus the game is as illustrated in Fig. 1. Additionally, suppose that the inﬁnite run that ends up in s1 is the one run which 
satisﬁes player one’s goal, and the run that ends up in s2 is the one which satisﬁes player two’s goal. Now, in this game, the 
run which ends up in s1 lies in the core, but with the use of a non-credible (punishing) strategy by player 1. Notice that the 
only possible deviation from (a, a) for player 2 is to play b, to which player 1 could respond by also playing b. Although this 
behaviour would prevent player 2 from achieving its goal, such a way of playing can be regarded as not rational for player 
1 given their preference relation: player 1 certainly prefers the run which ends in s1 over the other two possible runs, but 
is indifferent otherwise.
Motivated by this phenomenon, we now present a stronger deﬁnition for the core. More speciﬁcally, with this new 
deﬁnition we require that if a coalition C wants to deviate from a given strategy proﬁle, then the remaining players can 
only credibly threaten C when they have a counter-response in which both at least one player in C does not get its goal 
achieved and every winner in the original strategy proﬁle remains a winner in the new one, i.e., the counter-coalition act 
in accordance with their preference relations. In this solution concept, we are thus capturing the idea of remaining players 
being willing to punish deviators, but only up to a point: those left behind would prefer not to do worse than they were 
doing originally. We do not claim that this solution concept is always appropriate, as is the case with all solution concepts 
in cooperative games with externalities. But it captures another interesting variation of how a non-deviating coalition might 
respond to a deviation.
We then reformulate the deﬁnition of a beneﬁcial deviation, and now say that a deviation ⃗σ ′C is a strong beneﬁcial 
deviation from ⃗σ if the following conditions hold:
1. C ⊆L(⃗σ);
2. C ⊆W(⃗σ−C, ⃗σ ′C);
3. For every joint strategy ⃗σ ′
−C for Ag \ C, we have that if W(⃗σ) ⊆W(⃗σ ′
−C, ⃗σ ′
C), then C ⊆W(⃗σ ′
−C, ⃗σ ′
C).
12

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
With this deﬁnition in place we say that the strong core of a game, denoted core+(G), is the set of outcomes of G that 
admit no strong beneﬁcial deviation. Then, with respect to Example 2, we see that while ⃗σa1a1 is in core(G), it is not the 
case that ⃗σa1a1 is in core+(G), since player 2 can now strongly beneﬁcially deviate from ⃗σa1a1 to ⃗σa1b1.
It is worth pausing to reﬂect for a moment on the issue of formulating the core in the presence of externalities. The 
game theory literature on this topic is very large. The reason is that the existence of externalities leads to many different 
deﬁnitions of stable behaviour (see, e.g., [13,16,39,40] for many variants of the core). Here, we propose one deﬁnition but 
by no means do we claim it is the only possibility. Essentially, with our deﬁnition, we require that for a punishing joint 
strategy to be credible, winners must remain winners after the presenting the threat.
We will now study the complexity of the decision problems deﬁned in previous sections, but with respect to the strong 
core. There are four decision problems whose deﬁnition depends on the nature of the core: E-Core, A-Core, Core Mem-
bership, and Beneﬁcial Deviation. We will use the same names for these problems, with the understanding that results in 
this section are with respect to the strong core. As we will show next, these four problems have the same complexities as 
with core, but require a more complex logical characterisation, which we provide here using the two-alternation fragment 
of Strategy Logic (SL) [23].4
Theorem 6. Given a game G = (M, γ1, ..., γn) and LTL formula ϕ, we have (G, ϕ) ∈E-Core if and only if M |= ϕ+
E-Core(G, ϕ), where 
ϕ+
E-Core(G, ϕ) is the SL formula:
ϕ+
E-Core(G,ϕ) =

W ⊆Ag
⟨Ag⟩
⎛
⎝ϕ ∧

i∈W
γi ∧

j∈Ag\W
¬γ j ∧

C⊆Ag\W
ϕNoBD(G, W , C)
⎞
⎠
and the formula ϕNoSBD(G, W , C) is deﬁned as follows:
ϕNoSBD(G, W , C) = [C]
⎛
⎝
j∈C
γ j →⟨Ag \ C⟩
⎛
⎝
i∈W
γi ∧

j∈C
¬γ j
⎞
⎠
⎞
⎠
Proof. This SL formula expresses that in the concurrent game structure, there exists a path ⟨Ag⟩ (...) under which
1. The formula ϕ holds;
2. Some players get their goals achieved: 
i∈W γi;
3. The remaining players do not: 
j∈Ag\W ¬γ j;
4. No coalition of losers has a strong beneﬁcial deviation: 
C⊆Ag\W ϕNoSBD(G, W , C).
We express the condition of a coalition of losers C not having a strong beneﬁcial deviation with the SL formula 
ϕNoSBD(G, W , C); this is broken down as follows: for every joint strategy of C, if every player in C is better off 

j∈C γ j

, 
then the coalition of players outside C has a joint strategy (⟨Ag \ C⟩...) such that both the winners in the original outcome 
remain winners after the threat is presented 

i∈W γi

, and at least one player in the deviating coalition, C, does not get 
its goal achieved 

j∈C ¬γ j

.

At this point, we would like to make a couple of observations. First, that the complexity of checking SL formulae is non-
elementary and depends on the alternation-depth of the formula ([23]): SL formulae of alternation-depth n can be checked 
in (n + 1)-ExpTime, and in PSpace for formulae that are semantically equivalent to CTL∗formulae. Since ϕ+
E-Core(G, ϕ) is an 
SL formula with two alternations, it can be checked in 3ExpTime (and in PSpace if |Ag| = 1). Second, we also would like to 
recall that ﬁnite-state machine strategies, as those we use here, can be characterised in LTL using the technique presented 
in [2,3]. Using these logical characterisations, we can obtain the following complexity results.
Theorem 7. For multi-player games, while E-Core and A-Core are in 3ExpTime, Core Membership is 2ExpTime-complete and Ben-
eﬁcial Deviation is PSpace-complete. For one-player games, all problems are PSpace-complete.
Because we characterised the strong core using SL (which, in contrast to ATL*, is not a bisimulation-invariant logic), we 
cannot conclude that the satisfaction of LTL properties by outcomes in the strong core is a bisimulation-invariant property. 
We believe that this is not the case.
In our ﬁrst formulation of the core, we saw that the core is always non-empty. Thus, a natural question is to ask whether 
the strong core is always non-empty. We begin by showing that this is the case for games with three or fewer players:
4 We were unable to ﬁnd a logical characterisation of the strong core using ATL*. In fact, we believe that such a logical characterisation in ATL* is not 
possible for multi-player games.
13

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
Theorem 8. For every two-player and three-player game, G, we have core+(G) ̸= ∅.
Proof. First, consider games with only two players. For a contradiction, let us suppose that for some game G, the set of 
outcomes core+(G) is empty. This means that for every outcome either player 1 or player 2 or both have a strong beneﬁcial 
deviation. Then, we know that no outcome can satisfy both goals, γ1 and γ2. Let us then consider the three remaining 
possible cases: outcomes that only satisfy γ1 (case 1), outcomes that only satisfy γ2 (case 2), and outcomes that satisfy 
neither γ1 nor γ2 (case 3). Let ⃗f = ( f1, f2) be an outcome, f ′
1 be a deviation by player 1, and f ′
2 be a deviation by player 2, 
and consider the three cases above. In case 1, only player 2 would deviate. Then, outcome ( f1, f ′
2) only satisﬁes γ2. Because 
( f1, f ′
2) is not in the core either, from this outcome only player 1 would deviate, to another outcome ( f ′
1, f ′
2). Then, outcome 
( f ′
1, f ′
2) only satisﬁes γ1. But, then, we have a contradiction, since this means that ( f1, f2) would be in core+(G). We can 
reason symmetrically to show that case 2 is not possible either. For case 3 we note that only single deviations would be 
possible. But any such deviations would be to an outcome that either only satisﬁes γ1 or only satisﬁes γ2, which are no 
longer possible. Since no other cases are possible, we have to reject our assumption and conclude that, for two-player 
games, core+(G) is not empty.
For three-player games, the proof is similar, but requires a careful case-by-case analysis. For a full proof, see the ap-
pendix.

In contrast, for games with four or more players, we ﬁnd that the strong core may be empty, as the following example 
illustrates.
Example 3. Consider the following 4 player game, with a start state, s0, and six sink states, s1, ..., s6. Let each player have 
two actions each, {0, 1}, and writing a1a2a3a4 for the action (a1, a2, a3, a4), our transition function from the start state looks 
like the following:
tr(0000) = s1,
tr(0001) = s1,
tr(0010) = s2,
tr(0011) = s2,
tr(0100) = s1,
tr(0101) = s3,
tr(0110) = s2,
tr(0111) = s5,
tr(1000) = s6,
tr(1001) = s4,
tr(1010) = s4,
tr(1011) = s4,
tr(1100) = s1,
tr(1101) = s3,
tr(1110) = s4,
tr(1111) = s3.
Moreover, suppose player one prefers the runs that end up in the states s1, s2, s3, player two prefers those that end up in 
s1, s4, s5, player three s2, s4, s6, and player four s3, s5, s6. The game is illustrated in Fig. 2 — we label states with the players 
that are winners in that state, and edges with the possible action tuples.5 With a careful case-by-case analysis, one can 
verify that for every state, there exists some coalition with a strong beneﬁcial deviation. Thus, the strong core of the game 
is empty. For a complete analysis of the deviations that each coalition can make, please refer to the appendix where the full 
case-by-case analysis is presented.
4. Mean-payoff games
Thus far, we have considered games with qualitative preferences—each player has a goal given by some temporal logic 
formula, which, under a given run, is either satisﬁed or unsatisﬁed. But this is a very coarse-grained approach to specifying 
preferences; it does not offer a way of expressing the intensity of the individual player’s preferences. One possible way to 
obtain a richer model of preferences would be to introduce multiple LTL goals for each player, and deﬁne some mapping 
from the set of satisﬁed formulae to the real numbers [41–43]. However, an alternative approach, which has been widely 
studied in the literature, is to sidestep temporal logics entirely and assign weights to states, rather than atomic propositions. 
We then compute the mean-payoff of runs, with the idea that agents prefer runs which maximise their mean-payoff [18,
44,45]. In this section, we will revisit the formulation of the core in this mean-payoff setting.
Formally, a mean-payoff game, G, is a tuple,
G = (M,{wi}i∈Ag),
5 We would like to note that this (counter-)example was automatically generated using bounded exhaustive search of games of different size, which may 
explain why the game is so counter-intuitive, and most importantly why a game like this one is so hard to ﬁnd, or be produced, by hand.
14

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
∅
{1, 2}
{2, 4}
{1, 3}
{1, 4}
{3, 4}
{2, 3}
0000
0001
0100
1100
0010
0011
0110
0101
1101
1111
1001
1010
1011
1110
0111
1000
∗
∗
∗
∗
∗
∗
Fig. 2. A game with an empty core+.
where M is a concurrent game structure, and for each i ∈Ag, wi : St →Z is a weight function, mapping states to integers. 
Games are played in an identical way to the LTL setting, but the agents’ preference relations are deﬁned differently here. 
Let β ∈Rω be an inﬁnite sequence of real numbers. Then the mean-payoff of β, denoted by mp(β), is deﬁned as follows:
mp(β) = lim inf
n→∞
1
n
n−1

i=0
βi.
In a mean-payoff game, a run ρ = s0s1 ... induces an inﬁnite sequence of weights for each player, wi(s0)wi(s1) ... — we 
denote this sequence by wi(ρ) and for notational convenience, we will write payi(ρ) for mp(wi(ρ)). With this, we can 
deﬁne the preference relation for each player: given two runs, ρ and ρ′, we have ρ ⪰i ρ′ if payi(ρ) ≥payi(ρ′); the strict 
relation ≻i is deﬁned in the usual way.
Now, recall that our deﬁnition of the core in the setting of LTL games relies on the notion of winners and losers of a 
game. In the mean-payoff setting, it clearly makes no sense to classify players as winners or losers—they can receive a wide 
spectrum of payoffs. Thus, we need to revisit the concept of a beneﬁcial deviation. In the mean-payoff setting, we say that 
given a strategy proﬁle ⃗σ , a beneﬁcial deviation by a coalition C is a strategy vector ⃗σ ′
C such that for all complementary 
strategy proﬁles ⃗σ ′
Ag\C , we have ρ(⃗σ ′
C, ⃗σ ′
Ag\C) ≻i ρ(⃗σ) for all i ∈C. We then say that ⃗σ is a member of the core if there 
exists no coalition C which has a beneﬁcial deviation from ⃗σ .
4.1. Non-emptiness of the core
We begin by asking whether mean-payoff games always have a non-empty core. Recall that in the LTL games case, we 
found the core was guaranteed to be non-empty: we ﬁnd that this does not hold in general for mean-payoff games. Before 
proving this theorem, we need a small lemma showing that. . .
Lemma 2. Let {⃗σ j} j∈N be a sequence of strategy proﬁles, and suppose that for some player i, we have lim j→∞payi(ρ(⃗σ j)) = x for 
some x ∈R. Then there exists some strategy proﬁle ⃗σ x such that payi(ρ(⃗σ x)) = x.
Proof. First note that the game will end up in a strongly-connected component. So let C be the set of all simple cycles of 
the game graph of said strongly-connected component. Consider linear program with solution x — this gives proportion of 
cycles

15

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
Theorem 9. In mean-payoff games, if |Ag| ≤2, then the core is non-empty. For |Ag| > 2, there exist games with an empty core.
Proof. If |Ag| = 1, it is straightforward to see that the core is always non-empty; we use Karp’s algorithm for determining 
the maximum cycle in a weighted graph [46] to determine the maximum payoff that one player can achieve. For two-player 
games, let ⃗σ = (σ1, σ2) be any strategy proﬁle. If ⃗σ is not in the core, then either Player 1, or Player 2, or the coalition 
consisting of both players has a beneﬁcial deviation. If the latter is true, then there is a strategy proﬁle, ⃗σ ′ = (σ ′
1, σ ′
2) such 
that ⃗σ ′ ≻i ⃗σ for both i ∈{1, 2}. We repeat this process until the coalition of both players does not have a beneﬁcial deviation. 
This must eventually be the case as 1) each player’s payoff is capped by their maximum weight and 2) by Theorem 4 of [47], 
we see that the set of payoffs that a coalition can achieve is a closed set, so any limit point can be attained.6 So there must 
come a point when they cannot beneﬁcially deviate together. At this point, we must either be in the core, or either player 1 
or player 2 has a beneﬁcial deviation. If player j ∈{1, 2} has a beneﬁcial deviation, say σ j, then any strategy proﬁle (σ j, σi), 
with i ̸= j, that maximises Player i’s mean-payoff is in the core. Thus, for every two-player game, there exists some strategy 
proﬁle that lies in the core.
However, for mean-payoff games with three or more players, the core of a game may be empty. The following example 
illustrates this case.
Example 4. Consider the following three-player game G, where each player has two actions, H, T, and there are four states, 
P, R, B, Y . The states are weighted for each player as follows:
wi(s)
1
2
3
P
−1
−1
−1
R
2
1
0
B
0
2
1
Y
1
0
2
If the game is in any state other than P, then no matter what set of actions is taken, the game will remain in that state. 
Thus, we only specify the transitions for the state P:
Ac
St
(H, H, H)
R
(H, H, T)
R
(H, T, H)
B
(H, T, T)
P
(T, H, H)
P
(T, H, T)
Y
(T, T, H)
B
(T, T, T)
Y
Fig. 3 illustrates the structure of the game.
Note that strategies are characterised by the state that the game eventually ends up in. If the players stay in P forever, 
then they can all collectively change strategy to move to one of R, B, Y , and each get a better payoff. Now, if the game ends 
up in R, then players 2 and 3 can deviate by playing (T, H), and no matter what player 1 plays, the game will be in state B, 
leaving the two deviating players better off. But similarly, if the game is in B, then players 1 and 3 can deviate by playing 
(T, T) to enter state Y , in which they both will be better off, regardless of what player 2 does. And ﬁnally, if in Y , then 
players 1 and 2 can deviate by playing (H, H) to enter R and will be better off regardless of what player 3 plays. Thus, no 
strategy proﬁle lies in the core.

4.2. Decision problems
We now turn our attention to decision problems relating to the core in the mean-payoff setting. However, from a com-
putational perspective, there is an immediate concern here—given a potential beneﬁcial deviation, how can we verify that 
it is preferable to the status quo under all possible counter-responses? Fortunately, as we show in the following lemma, we 
can restrict our attention to memoryless strategies when thinking about potential counter-responses to players’ deviations 
(see Section 2 for the deﬁnition of memoryless strategies):
6 The result of [47] actually refers to two-player, multi-mean-payoff games, whilst we are working with multi-player mean-payoff games. We will cover 
why this is not an issue momentarily.
16

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
P
B
R
Y
(H, H,∗)
(∗, T, H)
(T,∗, T)
(H, T, T)
(T, H, H)
∗
∗
∗
Fig. 3. A game with an empty core.
Lemma 3. Let G be a game, C ⊆Ag be a coalition and ⃗σ be a strategy proﬁle. Further suppose that ⃗σ ′
C is a strategy vector such that 
for all memoryless strategy vectors ⃗σ ′
Ag\C , we have,
ρ(⃗σ ′
C, ⃗σ ′
Ag\C) ≻i ρ(⃗σ).
Then, for all strategy vectors, ⃗σ ′
Ag\C , not necessarily memoryless, we have,
ρ(⃗σ ′
C, ⃗σ ′
Ag\C) ≻i ρ(⃗σ).
Before we prove this, we need to introduce an auxiliary concept of two-player, turn-based, zero-sum, multi-mean-payoff 
games [48] (we will simply refer to these as multi-mean-payoff games moving forward). Informally, these are similar to 
two-player, turn-based, zero-sum mean-payoff games, except player 1 has k weight functions associated with the edges, 
and they are trying to ensure the resulting k-vector of mean-payoffs is component-wise greater than a vector threshold. 
Formally, a multi-mean-payoff game is given by a structure
G = (V 1, V 2, v0, E, w, zk)
where V 1, V 2 are sets of states controlled by players 1 and 2 respectively, with V = V 1 ∪V 2 the state space, v0 ∈V the 
start state, E ⊆V × V a set of edges, w : E →Zk a weight function, assigning to each edge a vector of weights, and zk ∈Qk
is a threshold vector.
The game is played by starting in the start state, v0 ∈V i, and player i choosing an edge (v0, v1), and traversing it to the 
next state. From this new state, v1 ∈S j, player j chooses an edge and so on, repeating this process forever. Runs are deﬁned 
in the usual way and the payoff of a run ρ, pay(ρ), is simply the vector (mp(w1(ρ)), ..., mp(wk(ρ))). Player 1 wins if the 
payi(ρ) ≥zi for all i ∈{1, ..., k}, and loses otherwise. The basic question associated with these games is whether player 1 
can force a win:
Multi-Mean-Payoff-Threshold:
Given: Multi-mean-payoff game G.
Question: Is it the case that player 1 has a winning strategy?
As shown in [48], this problem is co-NP-complete. Whilst we do not need to use this complexity result right now, we 
shall use this fact later. It is also worth noting that in our multi-player mean-payoff games, the weights are attached to 
states, whilst in multi-mean-payoff games, the weights are attached to edges. For our purposes, this difference is purely 
superﬁcial—the former can be mapped into the latter simply by pushing the weights onto the outgoing edges, whilst the 
latter can be mapped into the former by adding more states, one for each edge. If you do this in the correct way, this gives 
you only a polynomial overhead. As we proceed, we shall use this mapping implicitly (and we in fact have already used it 
in the proof of Theorem 9).
With this decision problem introduced, we are now in a position to prove Lemma 3.
17

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
y0
y0
y∗
x1
x′
1
x2
x′
2
···
xn−1
x′
n−1
xn
x′
n
yn
F
∗
T
T
F
T
F
T
F
T
F
T
F
∗
∗
∗
Fig. 4. Concurrent game structure for the reduction from 3SAT.
Proof of Lemma 3. Let ⃗σ ′
Ag\C be an arbitrary strategy and let i ∈C be an arbitrary agent. Suppose it is not the case that 
ρ(⃗σ ′
C, ⃗σ ′
Ag\C) ≻i ρ(⃗σ). Thus, we have ρ(⃗σ) ⪰i ρ(⃗σ ′
C, ⃗σ ′
Ag\C). Considering this as a two-player multi-mean-payoff game, where 
player 1’s strategy is ﬁxed and encoded into the game structure (i.e., player 1 follows ⃗σ ′
C , but has no say in the matter), and 
the payoff threshold is mp(ρ(⃗σ)), then ⃗σ ′
Ag\C is a winning strategy for player 2 in this game. Now, by [21,48], if player 2 
has a winning strategy, then they have a memoryless winning strategy. Thus, there is a memoryless strategy ⃗σ ′′
Ag\C such that 
ρ(⃗σ) ⪰i ρ(⃗σ ′
C, ⃗σ ′′
Ag\C). But this contradicts the assumptions of the lemma, and thus we must have ρ(⃗σ ′
C , ⃗σ ′
Ag\C) ≻i ρ(⃗σ).

We are now in a position to look at some complexity bounds for mean-payoff games in the cooperative setting. Let us 
begin by considering the following decision problem relating to beneﬁcial deviations:
Beneﬁcial Deviation:
Given: Game G and strategy proﬁle ⃗σ .
Question: Does some coalition have a beneﬁcial deviation from ⃗σ ? That is, does there exist C ⊆Ag and ⃗σ ′
C ∈C such 
that for all ⃗σ ′
Ag\C ∈Ag\C and for all i ∈C, we have:
ρ(⃗σ ′
C, ⃗σ ′
Ag\C) ≻i ρ(⃗σ)?
We have:
Theorem 10. If the provided strategy proﬁle ⃗σ is memoryless, then Beneﬁcial Deviation is NP-complete.
Proof. First correctly guess a deviating coalition C and a strategy proﬁle ⃗σ ′
C for such a coalition of players. Then, use the 
following three-step algorithm. First, compute the mean-payoffs that players in C get on ρ(⃗σ), that is, a set of values 
z∗
j = pay j(ρ(⃗σ)) for every j ∈C — this can be done in polynomial time simply by ‘running’ the strategy proﬁle ⃗σ . Then 
compute the graph G[⃗σ ′
C], which contains all possible behaviours (i.e., strategy proﬁles) for Ag \ C with respect to ⃗σ — this 
construction is similar to the one used in the proof of Theorem 5, that is, the game when we ﬁx ⃗σ ′
C , and can be done in 
polynomial time. Finally, we ask whether every path ρ in G[⃗σ ′
C] satisﬁes pay j(ρ) > z∗
j , for every j ∈C — for this step, we 
can use Karp’s algorithm [46] to answer the question in polynomial time for every j ∈C. If every path in G[⃗σ ′
C] has this 
property, then we accept; otherwise, we reject.
For hardness, we reduce from 3SAT, using a small variation of the construction in [28]. Let P = {x1, ..., xn} be a set 
of atomic propositions. Given a Boolean formula ϕ = 
1≤c≤m Cc (in conjunctive normal form) over P — where each Cc =
lc1 ∨lc2 ∨lc3, and each literal lck = x j or ¬x j, with 1 ≤k ≤3, for some 1 ≤j ≤n — we construct M =

Ag, St, s0,(Aci)i∈Ag, tr

, 
an m-player concurrent game structure deﬁned as follows, and illustrated in Fig. 4:
• Ag = {1, ..., m};
• St = {xv | 1 ≤v ≤n} ∪{x′
v | 1 ≤v ≤n} ∪{y0, yn, y0, y∗};
• s0 = y0;
18

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
• Aci = {t, f }, for every i ∈Ag, and Ac = Ac1 × ··· × Acm;
• For tr, refer to the Fig. 4, such that T = {(t1, ..., tm)} and F = Ac \ T .
With M at hand, we build a mean-payoff game using the following weight function:
• wi(xv) = 1 if xv is a literal in Ci and wi(xv) = 0 otherwise, for all i ∈Ag and 1 ≤v ≤n
• wi(x′
v) = 1 if ¬xv is a literal in Ci and wi(x′
v) = 0 otherwise, for all i ∈Ag and 1 ≤v ≤n
• wi(y0) = wi(yn) = wi(y0) = wi(y∗) = 0, for all i ∈Ag
Then, we consider the game G over M and any strategy proﬁle (in memoryless strategies) such that ⃗σ (s0) = y∗. For any 
of such strategy proﬁles the mean-payoff of every player is 0. However, if ϕ is satisﬁable, then there is a path in M, from 
y0 to yn, such that in such a path, for every player, there is a state in which its payoff is not 0. Thus, the grand coalition 
Ag has an incentive to deviate since traversing that path inﬁnitely often will give each player a mean-payoff strictly greater 
than 0. Observe two things. Firstly, that only if the grand coalition Ag agrees, the game can visit y0 after y0. Otherwise, 
the game will necessarily end up in y∗forever after. Secondly, because we are considering memoryless strategies, the path 
from y0 to yn followed at the beginning is the same path that will be followed thereafter, inﬁnitely often. Then, we can 
conclude that there is a beneﬁcial deviation (necessarily for Ag) if and only if ϕ is satisﬁable, as otherwise at least one of 
the players in the game will not have an incentive to deviate (because its mean-payoff would continue to be 0). We then 
conclude that (G, σ) ∈Beneﬁcial Deviation if and only if ϕ is satisﬁable.

From Theorem 10 it follows that checking if no coalition of players has a beneﬁcial deviation with respect to a given 
strategy proﬁle is co-NP complete, and thus we have:
Corollary 2. If the provided strategy proﬁle ⃗σ is memoryless, then Core Membership is co-NP-complete.
We can also leverage Beneﬁcial Deviation to obtain the following:
Theorem 11. If we restrict the existential quantiﬁcation over strategies to consider only memoryless strategies, then E-Core is in P
2 .
Proof. Given a game G, we guess a strategy proﬁle ⃗σ and check that (G, ⃗σ) is not an instance of Beneﬁcial Deviation. 
While the former can be done in polynomial time, the latter can be solved in co-NP using an oracle for Beneﬁcial Deviation. 
Thus, we have a procedure that runs in NPco-NP = P
2 .

Owing to the alternations present in the deﬁnition of the core and beneﬁcial deviations, we believe the E-Core problem 
to be complete for P
2 , but we have been unable to ﬁnd a proof of this. Note that suggests a contrast with the corresponding 
problem for Nash equilibrium in mean-payoff games, which lies in NP [45]. More importantly, the result also shows that 
the (complexity) dependence on the type of coalitional deviation is only weak, in the sense that different types of beneﬁcial 
deviations may be considered within the same complexity class, as long as such deviations can be checked with an NP or 
co-NP oracle.
4.3. Fulﬁlled coalitions
We now generalise the idea of a fulﬁlled coalition. To do this, we introduce the notion of a lower bound. Let C ⊆Ag be 
a coalition in a game G and let ⃗zC ∈QC . We say that ⃗zC is a lower bound for C if there is a joint strategy ⃗σC for C such 
that for all strategies ⃗σ−C for Ag \ C, we have payi(ρ(⃗σC, ⃗σ−C)) ≥zi, for every i ∈C. Based on this deﬁnition, we can prove 
the following, which characterises the core in terms of lower bounds:
Lemma 4. Let ρ be a run in G. There is ⃗σ ∈core(G) such that ρ = ρ(⃗σ) if and only if for every coalition C ⊆Ag and lower bound 
⃗zC ∈QC for C, there is some i ∈C such that zi ≤payi(ρ).
Proof. To show the left-to-right direction, suppose that there exists a member of the core ⃗σ ∈core(G) and suppose further 
that there is some coalition C ⊆Ag and lower bound ⃗zC ∈QC for C, such that for every i ∈C we have zi > payi(ρ). Because 
⃗zC is a lower bound for C, and zi > payi(ρ), for every i ∈C, then there is a joint strategy ⃗σC for C such that for all strategies 
⃗σ−C for Ag \ C, we have payi(ρ(⃗σC, ⃗σ−C)) ≥zi > payi(ρ), for every i ∈C. Then, it follows that (G, ⃗σ ) ∈Beneﬁcial Deviation, 
which further implies that ⃗σ cannot be in the core of G — a contradiction to our initial hypothesis.
For the right-to-left direction, suppose that there is ρ in G such that for every coalition C ⊆Ag and lower bound 
⃗zC ∈QC for C, there is i ∈C such that zi ≤payi(ρ). We then simply let ⃗σ be any strategy proﬁle such that ρ =
ρ(⃗σ). Now, let C = { j, ..., k} ⊆Ag be any coalition and ⃗σ ′
C
be any possible deviation of C from ⃗σ . Either ⃗z′C =
(pay j(ρ(⃗σ−C, ⃗σ ′
C)), ..., payk(ρ(⃗σ−C, ⃗σ ′
C))) is a lower bound for C or it is not.
19

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
If we have the former, by hypothesis, we know that there is i ∈C such that payi(ρ(⃗σ−C, ⃗σ ′
C)) ≤payi(ρ). Therefore, i will 
not have an incentive to deviate along with C \ {i} from ⃗σ , and as a consequence coalition C will not be able to beneﬁcially 
deviate from ⃗σ .
If, on the other hand, ⃗z′C is not a lower bound for C, then, by the deﬁnition of lower bounds, we know that it is not the 
case that ⃗σ ′
C is a joint strategy for C such that for all strategies ⃗σ ′
−C for Ag\C, we have payi(ρ(⃗σ ′
C, ⃗σ ′
−C)) ≥payi(ρ(⃗σ−C, ⃗σ ′
C)), 
for every i ∈C. That is, there exists i ∈C and ⃗σ ′
−C for Ag \ C such that payi(ρ(⃗σ ′
C, ⃗σ ′
−C)) < payi(ρ(⃗σ−C, ⃗σ ′
C)). We will now 
choose ⃗σ ′
−C so that, in addition, payi(ρ) ≥payi(ρ(⃗σ ′
C, ⃗σ ′
−C)) for some i.
Let ⃗z′′C = (pay j(ρ(⃗σ j
−C, ⃗σ ′
C)), ..., payk(ρ(⃗σ k
−C, ⃗σ ′
C))) where payi(ρ(⃗σ i
−C, ⃗σ ′
C)) is deﬁned to be min⃗σ ′
−C ∈−C payi(ρ((⃗σ ′
−C,
⃗σ ′
C))). That is, ⃗σ i
−C is a strategy for Ag \ C which ensures the lowest mean-payoff for i assuming that C is playing the joint 
strategy ⃗σ ′
C . By construction ⃗z′′C is a lower bound for C — since each z′′
i = payi(ρ(⃗σ i
−C, ⃗σ ′
C)) is the greatest mean-payoff 
value that i can ensure for itself when C is playing ⃗σ ′
C , no matter what coalition Ag \ C does—and therefore, by hypothesis 
we know that for some i ∈C we have payi(ρ(⃗σ i
−C, ⃗σ ′
C)) ≤payi(ρ). As a consequence, as before, i will not have an incentive 
to deviate along with C \{i} from ⃗σ , and therefore coalition C will not be able to beneﬁcially deviate from ⃗σ . Because C and 
⃗σ ′
C where arbitrarily chosen, we conclude that ⃗σ ∈core(G), proving the right-to-left direction and ﬁnishing the proof.

With this lemma in mind, we want to determine if a given vector, ⃗zC , is in fact a lower bound and importantly, how 
eﬃciently we can do this. That is, to understand the following decision problem:
Lower Bound:
Given: Game G, coalition C ⊆Ag, and vector ⃗zC ∈QAg.
Question: Is ⃗zC a lower bound for C in G?
Using the Multi-Mean-Payoff-Threshold decision problem introduced earlier, we can prove the following theorem:
Theorem 12. Lower Bound is co-NP-complete.
Proof. We prove membership as well as hardness by reducing to and from Multi-Mean-Payoff-Threshold in the obvious 
way. First, we show that Lower Bound lies in co-NP by reducing it to Multi-Mean-Payoff-Threshold. Suppose we have 
an instance, (G, C, ⃗zC), and we want to determine if it is in Lower-Bound. We can do this by forming a two-player, multi-
mean-payoff game, G′ = (V 1, V 2, v0, E, w′, zk). Here we have V 1 = St, V 2 = St × AcC and v0 = s0. Additionally, the set of 
edges of G′, E, is deﬁned as,
E = {(s,(s, acC)) | s ∈St, acC ∈AcC}
∪{((s, acC), tr(s,(acC, acAg\C))) | acAg\C ∈AcAg\C},
and the weight function, w′ : E →Z|C|, is deﬁned by the following two patterns:
w′
i(s,(s, acC)) = wi(s);
w′
i((s, acC), tr(s,(acC, acAg\C))) = wi(s).
Finally, we set z|C| to be ⃗zc.
Informally, the two players of the game are C and Ag \ C, the vector weight function is given by aggregating the weight 
functions of C and the threshold is ⃗zC . Now, if in this game, player 1 has a winning strategy, then there exists some strategy 
⃗σC such that for all strategies of player 2, ⃗σAg\C , we have that ρ(⃗σC, ⃗σAg\C) is a winning run for player 1. But this means that 
payi(ρ(⃗σC, ⃗σAg\C)) ≥zi for all i ∈C. But it is easy to verify that this implies that ⃗zC is a lower bound for C in G. Conversely, 
if player 1 has no winning strategy, then for all strategies, ⃗σC , there exists some strategy ⃗σAg\C such that ρ(⃗σC, ⃗σAg\C) is 
not a winning run. This in turn implies that for some j ∈C, we have that pay j(ρ(⃗σC, ⃗σAg\C)) < z j, which means that ⃗zC is 
not a lower bound for C in G. Also note that this construction can be performed in polynomial time, giving us the co-NP 
upper bound.
For the lower bound, we go the other way and reduce from Multi-Mean-Payoff-Threshold. Suppose we would like to 
determine if an instance G is in Multi-Mean-Payoff-Threshold. Then we form a concurrent mean-payoff game, G′, with 
k + 1 players, where the states of G′ coincide exactly with the states of G. In this game, only the 1st and (k + 1)th player 
have any inﬂuence on the strategic nature of the game. If the game is in a state in V 1, player one can decide which state to 
move into next. Otherwise, if the game is in a state within V 2, then the (k + 1)th player makes a move. Note we only allow 
moves that agree with moves allowed within G.
Now, in G′, the ﬁrst k players have weight functions corresponding correspond to the k weight functions of player 1 in 
G. The last player can have any arbitrary weight function. With this machinery in place, we ask if zk is a lower bound for 
{1, ..., k}. In a similar manner of reasoning to the above, it is easy to verify that G is an instance of Multi-Mean-Payoff-
Threshold if and only if zk is a lower bound for {1, ..., k} in the constructed concurrent mean-payoff game. Moreover, this 
reduction can be done in polynomial time and we can conclude that Lower Bound is co-NP-complete.

20

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
A notable omission from this section is that we have not presented any bounds for the complexity of E-Core in the 
general case. One reason for the upper bounds remaining elusive to us is due to the fact that whilst in a multi-mean-payoff 
game, player 2 can act optimally with memoryless strategies, player 1 may require inﬁnite memory [21,48]. Given the close 
connection between the core in our concurrent, multi-agent setting and winning strategies in multi-mean-payoff games, 
this raises computational concerns for the E-Core problem. Additionally, in [47], the authors study the Pareto frontier of 
multi-mean-payoff games, and provide an algorithm for constructing a representation of the achievable values of a given 
game, but this procedure takes an exponential amount of time in the size of the input. The same paper also establishes 
p
2-completeness for the polyhedron value problem. Both of these problems appear to be intimately related to the core, and 
we hope we might be able to use these results to gain more insight into the E-Core in the future.
5. Discussion and related work
In this section, we present some conclusions, brieﬂy discuss related work, discuss some issues related to the core, and 
speculate about how to implement our approach using model checking techniques.
5.1. Coalition formation in cooperative games
Coalition formation with externalities has been studied in the cooperative game-theory literature [16,39,40]. These works 
considered several possible formulations of the core. For instance, the α-core takes the pessimistic approach that requires 
that all members of a deviating coalition, S, will beneﬁt from the deviation regardless of the behaviour of the other coali-
tions that may be formed. Our ﬁrst deﬁnition of the core follows this approach. In contrast, β-core takes an optimistic 
approach, and requires that the members of a deviating coalition S will beneﬁt from at least one possibility of coalition 
formation of the rest of the players. In addition, γ -core [49,50] assumes that the coalition structure that will be created 
after a deviation will include the deviating coalition S and the rest of the coalition structure will consist of all singletons. 
The “worth” of S is now deﬁned as equal to its payoff in the Nash equilibrium between S and the other players acting indi-
vidually, in which the members of S play their joint best response strategy against the individually best response strategies 
of the remaining players. It is well-known that α- and β-characteristic functions lead to large cores [51], which is consistent 
with our observation that, with respect to our ﬁrst deﬁnition, the core is never empty. Coalition formation is important in 
multi-agent system [52]. However, even though coalition formation with externalities is a widely-studied problem in multi-
agent systems, not much work has studied the concept of stability in multi-agent coalition formation with externalities [53]. 
Instead, in artiﬁcial intelligence and multi-agent systems, most research has focused on the structure formation itself [54]. 
Notice that, from the point of view of cooperative game theory, our games are games of non-transferable utility (i.e., NTU 
games) [15, p.71]. For certain types of NTU games, there are other possible approaches to deﬁning the core, for example 
through a translation into a conventional (transferable utility) game (see, e.g. [55, p.238]). This approach might conceivably 
be used in our setting.
5.2. Rational veriﬁcation of concurrent games
The formal veriﬁcation of temporal logic properties of multi-agent systems, while assuming rational behaviour of the 
agents in such a system, has been studied for almost a decade now; see, for instance, [2–4,43,56]. However, to the best of 
our knowledge, all these studies have considered a non-cooperative setting, even if coalitional power is allowed, for instance, 
as in a strong Nash equilibrium. Nonetheless, also in such non-cooperative settings, the complexity of checking whether a 
temporal logic property is satisﬁed in a stable outcome of the game is a 2ExpTime-complete problem, even for two-player 
zero-sum games where only trivial coalitions can be formed. On the positive side, cooperative games seem to have better 
model-theoretic properties in the rational veriﬁcation framework: with respect to our ﬁrst deﬁnition of the core (which 
corresponds to the concept of α-core in the literature of cooperative games), a witness in the core is always guaranteed 
(since the core is never empty), preserved across bisimilar systems, and can be checked in practice using ATL* model 
checking techniques, which are supported by, e.g., MCMAS [10], an automated formal veriﬁcation tool that can perform ATL*
model checking (via an implementation of SL[1G], which subsumes ATL* [57]) and allows speciﬁcations of concurrent game 
structures in ISPL (Interpreted Systems Programming Language, the modelling language of MCMAS, based on the interpreted 
systems formalism [58]).
Declaration of competing interest
The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have 
appeared to inﬂuence the work reported in this paper.
Acknowledgement
This research has been supported by: a UKRI AI World Leading Researcher Fellowship, grant number EP/W002949/1 
(Wooldridge); the EU Horizon 2020 programme under project 952215 “TAILOR” (Kraus and Woodridge); the EPSRC Centre 
21

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
for Doctoral Training in Autonomous Intelligent Machines and Systems EP/L015897/1 (Steeples); the Ian Palmer Memorial 
Scholarship at the University of Oxford (Steeples); and, the Israel Science Foundation under grant 1958/20 (Kraus).
Appendix A. Proof that Example 3 has an empty strong core
We claimed earlier that there exist four players games with an empty strong core, and gave an example of such a game 
in Example 3. Below, we present a table giving, for every action proﬁle, a strong beneﬁcial deviation for some set of players. 
The notation {2 →0, 4 →1} means that the coalition {2, 4} has a strong beneﬁcial deviation where player 2 plays 0 and 
player 4 player 1. Note that a strong beneﬁcial deviation for a coalition does not require all players to change their action 
from the status quo.
Action proﬁle
Set of winning players
Strong beneﬁcial deviation
0000
{1, 2}
{3 →1}
0001
{1, 2}
{3 →1}
0010
{1, 3}
{2 →1, 4 →1}
0011
{1, 3}
{2 →1, 4 →1}
0100
{1, 2}
{3 →1}
0101
{1, 4}
{2 →0}
0110
{1, 3}
{2 →1, 4 →1}
0111
{2, 4}
{1 →1}
1000
{3, 4}
{1 →0}
1001
{2, 3}
{1 →0}
1010
{2, 3}
{1 →0}
1011
{2, 3}
{1 →0}
1100
{1, 2}
{3 →1}
1101
{1, 4}
{2 →0}
1110
{2, 3}
{1 →0}
1111
{1, 4}
{2 →0}
This table illustrates that the provided game has an empty strong core. It is an easy, but somewhat tedious, task to verify 
that the deviating players win under all counter-responses under which the original winning players win.
Appendix B. Proof of Theorem 8
We will prove this theorem by assuming the existence of a three player game G with an empty strong core, and then 
proving two contradictory facts, which immediately implies the result. The two claims are:
1. There is some strategy ⃗σ on G that models exactly two of γ1, γ2, γ3.
2. There is no strategy ⃗σ on G that models exactly two of γ1, γ2, γ3.
We use the following lemma throughout:
Lemma 5. Let G be a three-player game with an empty strong core. Then there does not exist a strategy proﬁle which satisﬁes the goals 
of all three players.
Proof. If G had a strategy satisfying all of the players’ goals, such a strategy would be in the strong core, which is a 
contradiction.

The ﬁrst claim we need to prove is the more straightforward of the two:
Lemma 6. Let G be a three-player game with an empty strong core. Then there exists some strategy ⃗σ on G that models exactly two of 
γ1, γ2, γ3.
Proof. Given claim 1, we know that every strategy on G must model exactly zero, one or two goals, but not three. Suppose, 
for contradiction, that G only has strategies modelling none or one of γ1, γ2, γ3. First, note that if G only had strategies 
satisfying zero of γ1, γ2, γ3, none of those strategies could admit a strong beneﬁcial deviation, and so would all be in the 
core+. Hence, G must have a strategy satisfying exactly one of γ1, γ2, γ3. Let such a strategy be denoted ⃗σ = (σ1, σ2, σ3), 
and without loss of generality, we may assume that it satisﬁes only γ1. Now, as by assumption ⃗σ does not lie in core+(G), 
there must be a strong beneﬁcial deviation from this strategy for some coalition C ⊆{2, 3}. Since under a strong beneﬁcial 
deviation, every member of the coalition is winning, and since we assumed that G only has strategies satisfying less than 
two of the goals, the coalition must be a singleton. Again, without loss of generality, we may assume that it is player 2 who 
has a deviation σ ′
2. By assumption, we have that σ1, σ ′
2, σ3 only models γ2.
22

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
But again, (σ1, σ ′
2, σ3) must have a strong beneﬁcial deviation for 1 or 3 to a strategy satisfying only γ1, or only γ3, 
respectively. If the former case were true, then this would contradict the fact that σ ′
2 was a strong beneﬁcial deviation from 
⃗σ . Hence, we must be in the latter case, and have a strong beneﬁcial deviation σ ′
3 for player 3.
The new strategy, (σ1, σ ′
2, σ ′
3) must once again admit a strong beneﬁcial deviation. If it admitted a strong beneﬁcial 
deviation σ ′
1 for 1 satisfying γ1 only, then the joint strategy (σ ′
1, σ ′
3) would be a punishment for σ ′
2. Otherwise, if a strong 
beneﬁcial deviation σ ′′
2 for 2 to a strategy satisﬁed γ2 only, then (σ1, σ ′′
2 ) would constitute a punishment for σ ′
3. Either way, 
we reach a contradiction, concluding our proof.

Before proving the second claim, we will need to appeal to two additional technical lemmas. The ﬁrst is the following:
Lemma 7. Let {a, b, c} be a permutation of {1, 2, 3}. Then there do not exist strategies (σa, σb, σc), (σa, σb, σ ′
c), (σ ′
a, σb, σ ′
c) of G such 
that all of the following hold:
• (σa, σb, σc) satisﬁes γa and γb but not γc
• (σa, σb, σ ′
c) satisﬁes γc only
• (σ ′
a, σb, σ ′
c) satisﬁes γa only
• σ ′
c is a beneﬁcial deviation for c from (σa, σb, σc) to (σa, σb, σ ′
c)
• σ ′
a is a beneﬁcial deviation for a from (σa, σb, σ ′
c) to (σ ′
a, σb, σ ′
c)
Proof. For a contradiction, we assume the statement of the lemma is not the case, and all of the statements hold. Without 
loss of generality, we can also assume that a = 1, b = 2, and c = 3.
Now, the third strategy, (σ ′
1, σ2, σ ′
3) must have a strong beneﬁcial deviation for 2, 3, or both. If it has a deviation (σ ′
2, σ ′′
3 )
(with possibly σ ′
2 = σ2) to a strategy that satisﬁes γ3 but not γ1, then it would be a punishment for σ ′
1. Therefore, the only 
possible sets of winners in the new strategy are {2}, {1, 2} and {1, 3}. We consider each of these cases in turn.
If player 2 is the only winner under the new strategy, the strong beneﬁcial deviation must be σ ′
2 by 2 only, to the 
strategy (σ ′
1, σ ′
2, σ ′
3).
Again, this strategy has to have a beneﬁcial deviation for 1, 3, or both. Let that deviation be (σ ′′
1 , σ ′′
3 ) with possibly σ ′′
1
= σ ′
1 or σ ′′
3 = σ ′
3 but not both. Consider the following cases for the resulting strategy, (σ ′′
1 , σ ′
2, σ ′′
3 ):
• If only γ1, or exactly γ1 and γ3 are satisﬁed, then (σ ′′
1 , σ ′′
3 ) is a punishment for σ ′
2; a contradiction.
• If exactly γ1 and γ2 are satisﬁed, then since 3 loses, we must necessarily have σ ′′
3 = σ ′
3 (the deviation is just by 1). 
In this case, the joint strategy (σ ′′
1 , σ ′
2) can take us from (σ1, σ2, σ ′
3) to (σ ′′
1 , σ ′
2, σ ′′
3 ) and form a punishment for σ ′
3; a 
contradiction.
• If only γ3, or exactly γ2 and γ3 are satisﬁed, then since either way 1 loses, we must have σ ′′
1 = σ ′
1 and the joint strategy 
(σ ′
2, σ ′′
3 ) can take us from (σ ′
1, σ2, σ ′
3) to (σ ′′
1 , σ ′
2, σ ′′
3 ) and form a punishment for σ ′
1; a contradiction.
Now suppose that the winners in the new strategy are 1 and 2. Since 1 was a winner in (σ ′
1, σ2, σ ′
3) already, the strong 
beneﬁcial deviation must once again be σ ′
2 by 2 only, to the strategy (σ ′
1, σ ′
2, σ ′
3).
But now, we can see that the joint strategy (σ ′
1, σ ′
2) is a punishment for σ ′
3 (both 1 and 2 remain winners); a contradic-
tion.
Finally, suppose the winners in the new strategy are 1 and 3. Since 1 was a winner in (σ ′
1, σ2, σ ′
3) already, the strong 
beneﬁcial deviation must this time be σ ′′
3 by 3 only, to the strategy (σ ′
1, σ2, σ ′′
3 ).
The only loser, 2, must have a strong beneﬁcial deviation σ ′
2 from this strategy. Now, see that:
• If exactly γ1 and γ2 were satisﬁed in the resulting strategy — (σ ′
1, σ ′
2, σ ′′
3 ), then the joint strategy (σ ′
1, σ ′
2) would be a 
punishment for σ ′
3.
• If exactly γ2 and γ3 were satisﬁed in (σ ′
1, σ ′
2, σ ′′
3 ) then the joint strategy (σ ′
2, σ ′′
3 ) would be a punishment for σ ′
1.
Hence, 2 must be the only winner in (σ ′
1, σ ′
2, σ ′′
3 ).
Once again, this strategy must admit a strong beneﬁcial deviation for 1, 3, or both. Let that deviation be (σ ′′
1 , σ ′′′
3 ) with 
possibly σ ′′
1 = σ ′
1 or σ ′′′
3
= σ ′′
3 but not both. Consider the following cases for the resulting strategy, (σ ′′
1 , σ ′
2, σ ′′′
3 ):
• If only γ1, or exactly γ1 and γ2 were satisﬁed then we would necessarily have σ ′′′
3 = σ ′′
3 and the joint strategy (σ ′′
1 , σ ′
2)
would be a punishment for σ ′′
3 , contradiction.
• If exactly γ1 and γ3 were satisﬁed then (σ ′′
1 , σ ′′′
3 ) would be a punishment for σ ′
2, contradiction.
• If only γ3, or exactly γ2 and γ3 were satisﬁed then we would necessarily have σ ′′
1 = σ ′
1 and the joint strategy (σ ′
2, σ ′′′
3 )
would be a punishment for σ ′
1, contradiction.
Thus all three possible sets of winners lead to a contradiction, yielding our result.

The second of the two technical lemmas is then as follows:
23

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
Lemma 8. Let {a, b, c} = {1, 2, 3} be the set Ag. There cannot be strategies (σa, σb, σc), (σa, σb, σ ′
c) of G such that all of the following 
hold:
• (σa, σb, σc) satisﬁes γa and γb but not γc
• (σa, σb, σ ′
c) satisﬁes γc only
• σ ′
c is a strong beneﬁcial deviation for c from (σa, σb, σc) to (σa, σb, σ ′
c)
Proof. For contradiction, assume otherwise. Without loss of generality, we can again assume that a = 1, b = 2, and c = 3.
The second strategy, (σa, σb, σ ′
c) must admit a strong beneﬁcial deviation for 1, 2, or both. However, see that a deviation 
to a strategy satisfying only γ1 or only γ2 would contradict Lemma 7. Hence, the new strategy must have exactly two 
winners. Moreover, if the winners in the new strategy were exactly 1 and 2, the deviation would provide a punishment 
for σ ′
3. Therefore, without loss of generality, we can assume that (σa, σb, σ ′
c) has a strong beneﬁcial deviation σ ′
1 for 1 to a 
strategy satisfying exactly γ1 and γ3.
Now, the only loser in (σ ′
1, σ2, σ ′
3), 2, must have a strong beneﬁcial deviation σ ′
2. If however, exactly γ1 and γ2 were 
satisﬁed in (σ ′
1, σ ′
2, σ ′
3), then (σ ′
1, σ ′
2) would be a punishment for σ ′
3. If exactly γ2 and γ3 were satisﬁed, (σ ′
2, σ ′
3) would 
provide a punishment for σ ′
1. Hence, 2 must be the only winner.
Once again a strong beneﬁcial deviation from (σ ′
1, σ ′
2, σ ′
3) to a strategy satisfying only γ1 or only γ3 would contradict 
Lemma 7. Therefore, there must be a strong beneﬁcial deviation by 1, 3, or both, to a strategy with exactly two winners. 
Now:
• If the winners were 1 and 3, the deviation would provide a punishment for σ ′
2.
• If the winners were 1 and 2 then the deviation must be σ ′′
1 by 1 only (as 3 loses). But then, the joint strategy (σ ′′
1 , σ ′
2)
would provide a punishment for the initial σ ′
3.
• If the winners were 2 and 3 then the deviation must be σ ′′
3 by 3 only. The joint strategy (σ ′
2, σ ′′
3 ) would be a punish-
ment for σ ′
1.
This exhausts all the cases, each of which lead to a contradiction, and ﬁnishes the proof of Lemma 8.

With these two technical lemmas in place, we are ready to prove the second part of our claim.
Lemma 9. Let G be a three-player game with an empty strong core. Then there does not exist a strategy proﬁle ⃗σ on G that models 
exactly two of γ1, γ2, γ3.
Proof. For a contradiction, assume otherwise. Without a loss of generality, let (σ1, σ2, σ3) be a strategy satisfying exactly 
γ1 and γ2. It must admit a strong beneﬁcial deviation σ ′
3 for the lone loser, 3. Moreover, by Lemma 8, he cannot win alone 
in the new strategy. Therefore, without loss of generality, we can assume that 1 and 3 are winners in (σ1, σ2, σ ′
3).
Again, by Lemma 8, the new strategy must have a strong beneﬁcial deviation σ ′
2 by 2 to a strategy where exactly two 
players win. However, if 1 and 2 were winners, then (σ1, σ ′
2) would constitute a punishment for σ ′
3. Therefore, exactly γ2
and γ3 must be satisﬁed in (σ1, σ ′
2, σ ′
3).
But now, again by Lemma 8, there must be a strong beneﬁcial deviation σ ′
1 for 1 to another strategy with exactly two 
winners. However, if those were 1 and 2, (σ ′
1, σ ′
2) would be a punishment for σ ′
3. Else, if the winners were 1 and 3 then 
(σ ′
1, σ ′
3) would form a punishment for σ ′
2. This concludes our proof.

Finally, the required result simply follows by combining the two lemmas.
Theorem 13. If G is a three-player game, then it has a non-empty strong core.
Proof. Result follows by combining Lemma 6 and Lemma 9.

References
[1] J. Gutierrez, S. Kraus, M.J. Wooldridge, Cooperative concurrent games, in: AAMAS, IFAAMAS, 2019, pp. 1198–1206.
[2] J. Gutierrez, P. Harrenstein, M. Wooldridge, Iterated boolean games, Inf. Comput. 242 (2015) 53–79, https://doi .org /10 .1016 /j .ic .2015 .03 .011.
[3] J. Gutierrez, P. Harrenstein, M. Wooldridge, From model checking to equilibrium checking: reactive modules for rational veriﬁcation, Artif. Intell. 248 
(2017) 123–157, https://doi .org /10 .1016 /j .artint .2017.04 .003.
[4] M. Wooldridge, J. Gutierrez, P. Harrenstein, E. Marchioni, G. Perelli, A. Toumi, Rational veriﬁcation: from model checking to equilibrium checking, in: 
Proc. of AAAI, AAAI Press, 2016, pp. 4184–4191.
[5] A. Abate, J. Gutierrez, L. Hammond, P. Harrenstein, M. Kwiatkowska, M. Najib, G. Perelli, T. Steeples, M.J. Wooldridge, Rational veriﬁcation: game-
theoretic veriﬁcation of multi-agent systems, Appl. Intell. 51 (9) (2021) 6569–6584, https://doi .org /10 .1007 /s10489 -021 -02658 -y.
[6] R. Alur, T.A. Henzinger, O. Kupferman, Alternating-time temporal logic, J. ACM 49 (5) (2002) 672–713, https://doi .org /10 .1145 /585265 .585270.
[7] E.A. Emerson, Temporal and modal logic, in: Formal Models and Semantics, Elsevier, 1990, pp. 995–1072.
[8] E.M. Clarke, O. Grumberg, D.A. Peled, Model Checking, MIT Press, 2002.
24

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
[9] J. Gutierrez, M. Najib, G. Perelli, M. Wooldridge, EVE: a tool for temporal equilibrium analysis, in: ATVA, in: Lecture Notes in Computer Science, 
vol. 11138, Springer, 2018, pp. 551–557.
[10] A. Lomuscio, H. Qu, F. Raimondi, MCMAS: an open-source model checker for the veriﬁcation of multi-agent systems, Int. J. Softw. Tools Technol. Transf. 
19 (1) (2017) 9–30, https://doi .org /10 .1007 /s10009 -015 -0378 -x.
[11] R. Brenguier, PRALINE: a tool for computing Nash equilibria in concurrent games, in: CAV, in: Lecture Notes in Computer Science, vol. 8044, Springer, 
2013, pp. 890–895.
[12] M. Kwiatkowska, G. Norman, D. Parker, G. Santos, {PRISM}-games 3.0: stochastic game veriﬁcation with concurrency, equilibria and time, in: S.K. Lahiri, 
C. Wang (Eds.), Computer Aided Veriﬁcation - 32nd International Conference, CAV 2020, Proceedings, Part II, Los Angeles, CA, USA, July 21-24, 2020, 
in: Lecture Notes in Computer Science, vol. 12225, Springer, 2020, pp. 475–487.
[13] M.J. Osborne, A. Rubinstein, A Course in Game Theory, MIT Press, 1994.
[14] M. Maschler, E. Solan, S. Zamir, Game Theory, Cambridge University Press, 2013.
[15] G. Chalkiadakis, E. Elkind, M. Wooldridge, Computational Aspects of Cooperative Game Theory, Morgan-Claypool, 2011.
[16] M. Uyanık, On the nonemptiness of the α-core of discontinuous games: transferable and nontransferable utilities, J. Econ. Theory 158 (2015) 213–231, 
https://doi .org /10 .1016 /j .jet .2015 .04 .005.
[17] J. Gutierrez, P. Harrenstein, G. Perelli, M. Wooldridge, Nash equilibrium and bisimulation invariance, in: CONCUR, in: LIPIcs, vol. 85, Schloss Dagstuhl, 
2017, 17.
[18] A. Ehrenfeucht, J. Mycielski, Positional strategies for mean payoff games, Int. J. Game Theory 8 (2) (1979) 109–113, https://doi .org /10 .1007 /bf01768705.
[19] E.A. Emerson, C.S. Jutla, Tree automata, mu-calculus and determinacy (extended abstract), in: 32nd Annual Symposium on Foundations of Computer 
Science, San Juan, Puerto Rico, 1–4 October 1991, IEEE Computer Society, 1991, pp. 368–377.
[20] H. Gimbert, W. Zielonka, Games where you can play optimally without any memory, in: M. Abadi, L. de Alfaro (Eds.), CONCUR 2005 - Concurrency 
Theory, 16th International Conference, Proceedings, CONCUR 2005, San Francisco, CA, USA, August 23–26, 2005, in: Lecture Notes in Computer Science, 
vol. 3653, Springer, 2005, pp. 428–442.
[21] E. Kopczynski, Half-positional determinacy of inﬁnite games, in: M. Bugliesi, B. Preneel, V. Sassone, I. Wegener (Eds.), Automata, Languages and Pro-
gramming, 33rd International Colloquium, Proceedings, Part II, ICALP 2006, Venice, Italy, July 10–14, 2006, in: Lecture Notes in Computer Science, 
vol. 4052, Springer, 2006, pp. 336–347.
[22] A. Pnueli, The temporal logic of programs, in: 18th Annual Symposium on Foundations of Computer Science (sfcs 1977), IEEE, 1977, pp. 46–57.
[23] F. Mogavero, A. Murano, G. Perelli, M.Y. Vardi, Reasoning about strategies, ACM Trans. Comput. Log. 15 (4) (2014) 1–47, https://doi .org /10 .1145 /2631917.
[24] C. Baier, J.-P. Katoen, Principles of Model Checking, MIT Press, 2008.
[25] R.M. Keller, Formal veriﬁcation of parallel programs, Commun. ACM 19 (7) (1976) 371–384, https://doi .org /10 .1145 /360248 .360251, Association for 
Computing Machinery, New York, NY, USA.
[26] S.A. Kripke, Semantical analysis of modal logic i normal modal propositional calculi, Z. Math. Log. Grundl. Math. 9 (5–6) (1963) 67–96, https://
doi .org /10 .1002 /malq .19630090502.
[27] E.A. Emerson, J.Y. Halpern, “Sometimes” and “not never” revisited, J. ACM 33 (1) (1986) 151–178, https://doi .org /10 .1145 /4904 .4999.
[28] A.P. Sistla, E.M. Clarke, The complexity of propositional linear temporal logics, J. ACM 32 (3) (1985) 733–749, https://doi .org /10 .1145 /3828 .3837.
[29] A. Pnueli, R. Rosner, On the synthesis of a reactive module, in: Conference Record of the Sixteenth Annual ACM Symposium on Principles of Program-
ming Languages, Austin, Texas, USA, January 11–13, 1989, ACM Press, 1989, pp. 179–190.
[30] D. Fisman, O. Kupferman, Y. Lustig, Rational synthesis, in: J. Esparza, R. Majumdar (Eds.), Tools and Algorithms for the Construction and Analysis of 
Systems, 16th International Conference, TACAS 2010, Held as Part of the Joint European Conferences on Theory and Practice of Software, Proceedings, 
ETAPS 2010, Paphos, Cyprus, March 20–28, 2010, in: Lecture Notes in Computer Science, vol. 6015, Springer, 2010, pp. 190–204.
[31] R.J. Aumann, Acceptable points in general cooperative n-person games, in: Contributions to the Theory of Games, Vol. IV, in: Annals of Mathematics 
Studies, vol. 40, Princeton University Press, Princeton, N.J., 1959, pp. 287–324.
[32] R.J. Aumann, Acceptable points in games of perfect information, Pac. J. Math. 10 (1960) 381–417.
[33] B.D. Bernheim, B. Peleg, M.D. Whinston, Coalition-proof Nash equilibria I. Concepts, J. Econ. Theory 42 (1) (1987) 1–12, https://doi .org /10 .1016 /0022 -
0531(87 )90099 -8, https://www.sciencedirect .com /science /article /pii /0022053187900998.
[34] B.D. Bernheim, M.D. Whinston, Coalition-proof Nash equilibria II. Applications, J. Econ. Theory 42 (1) (1987) 13–29, https://doi .org /10 .1016 /0022 -
0531(87 )90100 -1, https://www.sciencedirect .com /science /article /pii /0022053187901001.
[35] M. Hennessy, R. Milner, Algebraic laws for nondeterminism and concurrency, J. ACM 32 (1) (1985) 137–161, https://doi .org /10 .1145 /2455 .2460.
[36] R. Alur, S. La Torre, Deterministic generators and games for Ltl fragments, ACM Trans. Comput. Log. 5 (1) (2004) 1–25, https://doi .org /10 .1145 /963927.
963928.
[37] O. Kupferman, M.Y. Vardi, P. Wolper, An automata-theoretic approach to branching-time model checking, J. ACM 47 (2) (2000) 312–360, https://
doi .org /10 .1145 /333979 .333987.
[38] R. Milner, Communication and Concurrency, Prentice-Hall, Inc., USA, 1989.
[39] S.-S. Yi, Stable coalition structures with externalities, Games Econ. Behav. 20 (2) (1997) 201–237, https://doi .org /10 .1006 /game .1997.0567.
[40] M. Finus, B. Rundshagen, A non-cooperative foundation of core-stability in positive externality {NTU}-coalition games, SSRN Electron. J. (2003), https://
doi .org /10 .2139 /ssrn .406400.
[41] M. Mavronicolas, B. Monien, K.W. Wagner, Weighted boolean formula games, in: X. Deng, F.C. Graham (Eds.), Internet and Network Economics, Third 
International Workshop, Proceedings, WINE 2007, San Diego, CA, USA, December 12–14, 2007, in: Lecture Notes in Computer Science, vol. 4858, 
Springer, 2007, pp. 469–481.
[42] S. Almagor, O. Kupferman, G. Perelli, Synthesis of controllable Nash equilibria in quantitative objective game, in: Proceedings of the Twenty-Seventh 
International Joint Conference on Artiﬁcial Intelligence, IJCAI’18, International Joint Conferences on Artiﬁcial Intelligence Organization, event-place: 
Stockholm, Sweden, 2018, pp. 35–41.
[43] O. Kupferman, G. Perelli, M.Y. Vardi, Synthesis with rational environments, Ann. Math. Artif. Intell. 78 (1) (2016) 3–20, https://doi .org /10 .1007 /s10472 -
016 -9508 -8.
[44] U. Zwick, M. Paterson, The complexity of mean payoff games on graphs, Theor. Comput. Sci. 158 (1–2) (1996) 343–359, https://doi .org /10 .1016 /0304 -
3975(95 )00188 -3.
[45] M. Ummels, D. Wojtczak, The complexity of Nash equilibria in stochastic multiplayer games, Log. Methods Comput. Sci. 7 (3) (Sep. 2011), https://
doi .org /10 .2168 /lmcs -7(3 :20 )2011.
[46] R.M. Karp, A characterization of the minimum cycle mean in a digraph, Universitext 23 (3) (1978) 309–311, https://doi .org /10 .1016 /0012 -365x(78 )
90011 -0.
[47] R. Brenguier, J.-F. Raskin, Pareto curves of multidimensional mean-payoff games, in: D. Kroening, C.S. Pasareanu (Eds.), Computer Aided Veriﬁcation 
- 27th International Conference, Proceedings, Part II, CAV 2015, San Francisco, CA, USA, July 18–24, 2015, in: Lecture Notes in Computer Science, 
vol. 9207, Springer, 2015, pp. 251–267.
[48] Y. Velner, K. Chatterjee, L. Doyen, T.A. Henzinger, A. Rabinovich, J.-F. Raskin, The complexity of multi-mean-payoff and multi-energy games, Inf. Comput. 
241 (2015) 177–196, https://doi .org /10 .1016 /j .ic .2015 .03 .001.
25

J. Gutierrez, S. Kowara, S. Kraus et al.
Artiﬁcial Intelligence 314 (2023) 103806
[49] P. Chander, et al., Cores of games with positive externalities, CORE DP 2010/4, 2010.
[50] P. Chander, The gamma-core and coalition formation, Int. J. Game Theory 35 (4) (2007) 539–556, https://doi .org /10 .1007 /s00182 -006 -0067 -9.
[51] D. Ray, R. Vohra, Equilibrium binding agreements, J. Econ. Theory 73 (1) (1997) 30–78, https://doi .org /10 .1006 /jeth .1996 .2236.
[52] O. Shehory, S. Kraus, Methods for task allocation via agent coalition formation, Artif. Intell. 101 (1–2) (1998) 165–200, https://doi .org /10 .1016 /s0004 -
3702(98 )00045 -9.
[53] D. Mutzari, J. Gan, S. Kraus, Coalition formation in multi-defender security games, Proc. AAAI Conf. Artif. Intell. 35 (6) (2021) 5603–5610.
[54] T. Rahwan, T. Michalak, M. Wooldridge, N.R. Jennings, Anytime coalition structure generation in multi-agent systems with positive or negative exter-
nalities, Artif. Intell. 186 (2012) 95–122, https://doi .org /10 .1016 /j .artint .2012 .03 .007.
[55] P. Borm, Y. Ju, D. Wettstein, Rational bargaining in games with coalitional externalities, J. Econ. Theory 157 (2015) 236–254, https://doi .org /10 .1016 /j .
jet .2015 .01.011.
[56] O. Kupferman, Y. Lustig, What triggers a behavior?, in: Formal Methods in Computer Aided Design (FMCAD’07), in: Lecture Notes in Computer Science, 
vol. 6015, IEEE, 2007, pp. 190–204.
[57] P. Cermák, A. Lomuscio, A. Murano, Verifying and synthesising multi-agent systems against one-goal strategy logic speciﬁcations, in: B. Bonet, S. 
Koenig (Eds.), Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, January 25-30, 2015, Austin, Texas, USA, AAAI Press, 2015, 
pp. 2038–2044.
[58] R. Fagin, J.Y. Halpern, Y. Moses, M.Y. Vardi, Reasoning About Knowledge, MIT Press, 1995.
26

