Ordering-Based Causal Discovery with Reinforcement Learning
Xiaoqiang Wang1‡∗, Yali Du2 , Shengyu Zhu3† ,
Liangjun Ke1†‡ , Zhitang Chen3 , Jianye Hao3,4 and Jun Wang2
1Xi’an Jiaotong University
2University College London
3Huawei Noah’s Ark Lab
4College of Intelligence and Computing, Tianjin University
wangxq5127@stu.xjtu.edu.cn, yali.dux@gmail.com, zhushengyu@huawei.com,
keljxjtu@xjtu.edu.cn, {chenzhitang2, haojianye}@huawei.com, jun.wang@cs.ucl.ac.uk
Abstract
It is a long-standing question to discover causal rela-
tions among a set of variables in many empirical sci-
ences. Recently, Reinforcement Learning (RL) has
achieved promising results in causal discovery from
observational data. However, searching the space of
directed graphs and enforcing acyclicity by implicit
penalties tend to be inefﬁcient and restrict the exist-
ing RL-based method to small scale problems. In
this work, we propose a novel RL-based approach
for causal discovery, by incorporating RL into the
ordering-based paradigm. Speciﬁcally, we formu-
late the ordering search problem as a multi-step
Markov decision process, implement the ordering
generating process with an encoder-decoder archi-
tecture, and ﬁnally use RL to optimize the proposed
model based on the reward mechanisms designed
for each ordering. A generated ordering would then
be processed using variable selection to obtain the
ﬁnal causal graph. We analyze the consistency and
computational complexity of the proposed method,
and empirically show that a pretrained model can
be exploited to accelerate training. Experimental
results on both synthetic and real data sets shows
that the proposed method achieves a much improved
performance over existing RL-based method.
1
Introduction
Identifying causal structure from observational data is an im-
portant but also challenging task in many practical applica-
tions. This task can be formulated as that of ﬁnding a Directed
Acyclic Graph (DAG) that minimizes a score function de-
ﬁned w.r.t. the observed data. However, searching over the
space of DAGs for the best DAG is known to be NP-hard,
even if each node has at most two parents [Chickering, 1996].
Consequently, traditional methods mostly rely on local heuris-
tics to perform the search, including greedy hill-climbing and
∗Work was done during an internship at Huawei Noah’s Ark Lab.
†Corresponding author.
‡Xiaoqiang Wang and Liangjun Ke were supported by the Na-
tional Natural Science Foundation of China under Grant 61973244.
greedy equivalence search that explores the Markov equiva-
lence classes [Chickering, 2002].
Along with various search strategies, existing methods have
also cast causal structure learning problem as that of learn-
ing an optimal variable ordering, considering that the order-
ing space is signiﬁcantly smaller than that of directed graphs
and searching over the ordering space can avoid dealing with
the acyclicity constraint [Teyssier and Koller, 2005]. Many
methods, such as genetic algorithm [Larranaga et al., 1996],
Markov chain Monte Carlo [Friedman and Koller, 2003] and
greedy local hill-climbing [Teyssier and Koller, 2005], have
been exploited as the search strategies to ﬁnd desired orderings.
In practice, however, these methods often cannot effectively
ﬁnd a globally optimal ordering for their heuristic nature.
Recently, with smooth score functions, several gradient-
based methods have been proposed by exploiting a smooth
characterization of acyclicity, including NOTEARS [Zheng
et al., 2018] for linear causal models and several subse-
quent works, e.g., [Yu et al., 2019; Lachapelle et al., 2020;
Ng et al., 2019b; Ng et al., 2019a; Zheng et al., 2020], which
use neural networks for modelling non-linear causal relation-
ships. As another attempt, [Zhu et al., 2020] utilize Rein-
forcement Learning (RL) to ﬁnd the underlying DAG from
the graph space without the need of smooth score functions.
Unfortunately, [Zhu et al., 2020] achieved good performance
only with up to 30 variables, for at least two reasons: 1) the
action space, consisting of directed graphs, is tremendous for
large scale problems and is hard to be explored efﬁciently;
and 2) it has to compute scores for many non-DAGs generated
during training but computing scores w.r.t. data is generally
time-consuming. It appears that the RL-based approach may
not be able to achieve a close performance to other gradient-
based methods that directly optimize the same score function
for large causal discovery problems, due to its search nature.
By taking advantage of the reduced space of variable order-
ings and the strong search ability of modern RL methods, we
propose Causal discovery with Ordering-based Reinforcement
Learning (CORL), which incorporates RL into the ordering-
based paradigm and is shown to achieve a promising empirical
performance. In particular, CORL outperforms NOTEARS,
a state-of-the-art gradient-based method for linear data, even
with 150-node graphs. Meanwhile, CORL is also competitive
with a strong baseline, Causal Additive Model (CAM) method
[Bühlmann et al., 2014], on non-linear data models.
arXiv:2105.06631v2  [cs.LG]  17 May 2021

Contributions. We make the following contributions in
this work: 1) We formulate the ordering search problem as
a multi-step Markov Decision Process (MDP) and propose
to implement the ordering generating process in an effective
encoder-decoder architecture, followed by applying RL to op-
timizing the proposed model based on speciﬁcally designed
reward mechanisms. We also incorporate a pretrained model
into CORL to accelerate training. 2) We analyze the consis-
tency and computational complexity of the proposed method.
3) We conduct comparative experiments on synthetic and real
data sets to validate the performance of the proposed meth-
ods. 4) An implementation has been made available at https://
github.com/huawei-noah/trustworthyAI/tree/master/gcastle.
2
Related Works
Besides the aforementioned heuristic ordering search algo-
rithms, [Schmidt et al., 2007] proposed L1OBS to conduct
variable selection using ℓ1-regularization paths based on the
method from [Teyssier and Koller, 2005]. [Scanagatta et al.,
2015] further proposed an ordering exploration method on
the basis of an approximated score function so as to scale
to thousands of variables. The CAM method [Bühlmann et
al., 2014] was speciﬁcally designed for non-linear additive
models. Some recent ordering-based methods such as sparsest
permutation [Raskutti and Uhler, 2018] and greedy sparsest
permutation [Solus et al., 2017] can guarantee consistency of
Markov equivalence class, relying on some conditional inde-
pendence relations and certain assumptions like faithfulness.
A variant of greedy sparest permutation was further proposed
in [Bernstein et al., 2020] for the setting with latent variables.
In the present work, we mainly work on identiﬁable cases
which have different assumptions from theirs.
In addition, exact algorithms such as dynamic programming
[Xiang and Kim, 2013] and integer or linear programming
[Bartlett and Cussens, 2017] are also used for causal discovery
problem. However, these algorithms usually can only work on
small graphs [De Campos and Ji, 2011], and to handle larger
problems with hundreds of variables, they usually need to
incorporate heuristics search [Xiang and Kim, 2013] or limit
the maximum number of parents of each node.
Recently, RL has been used to tackle several combinatorial
problems such as the maximum cut and traveling salesman
problem [Bello et al., 2016; Khalil et al., 2017; Kool et al.,
2019]. These works aim to learn a policy as a solver based
on the particular type of combinatorial problems. However,
causal discovery tasks generally have different relationships,
data types, graph structures, etc., and moreover, are typically
off-line with focus on a or a class of causal graph(s). As such,
we use RL as a search strategy, similar to [Zoph and Le, 2017;
Zhu et al., 2020]. Nevertheless, a pretrained model or policy
can offer a good starting point to speed up training, as shown
in our evaluation results (cf. Figure 3).
3
Background
3.1
Causal Structure Learning
Let G = (d, V, E) denotes a DAG, with d the number of nodes,
V = {v1, · · · , vd} the set of nodes, and E = {(vi, vj)|i, j =
1, . . . , d} the set of directed edges from vi to vj. Each node
vj is associated with a random variable Xj. The probabil-
ity model associated with G factorizes as p(X1, · · · , Xd) =
Qd
j=1 p(Xj|Pa(Xj)), where p(Xj|Pa(Xj)) is the conditional
probability distribution for Xj given its parents Pa(Xj) :=
{Xk|(vk, vj) ∈E}. We assume that the observed data xj is
obtained by the Structural Equation Model (SEM) with ad-
ditive noises: Xj := fj(Pa(Xj)) + ϵj, j = 1, . . . , d, where
fj represents the functional relationship between Xj and its
parents, and ϵj’s denote jointly independent additive noise
variables. We assume causal minimality, which is equivalent
to that each fj is not a constant for any Xk ∈Pa(Xj) in this
SEM [Peters et al., 2014].
Given a sample X = [x1, · · · , xd] ∈Rm×d where xj is a
vector of m observations for random variable Xj. The goal
is to ﬁnd a DAG G that optimizes the Bayesian Information
Criterion (BIC) (or equivalently, minimum description length)
score, deﬁned as
SBIC(G)=
d
X
j=1
" m
X
k=1
log p(xk
j |Pa(xk
j ); θj)−|θj|
2 log m
#
, (1)
where xk
j is the k-th observation of Xj, θj is the parameter
associated with each likelihood, and |θj| denotes the parameter
dimension. For linear-Gaussian models, p(xk
j |Pa(xk
j ); θj) =
N(xj|θT
j Pa(xj), σ2) and σ2 can be estimated from the data.
Figure 1: An ex-
ample of the cor-
respondence be-
tween an order-
ing and a fully-
connected DAG.
The problem of ﬁnding a directed graph
that satisﬁes the ayclicity constraint can
be cast as that of ﬁnding a variable order-
ing [Teyssier and Koller, 2005; Schmidt
et al., 2007]. Speciﬁcally, let Π denote
an ordering of the nodes in V , where the
length of the ordering |Π| = |V | and Π is
indexed from 1. If node vj ∈V lies in the
p-th position, then Π(p) = vj. Notation
Π≺vj denotes the set of nodes that precede
node vj in Π. One can easily establish a
canonical correspondence between an or-
dering Π and a fully-connected DAG GΠ;
an example is presented in Figure 1. A
DAG G can be consistent with more than
one orderings and the set of these orderings is denoted by
Φ(Π)={Π : fully-connected DAG GΠ is a super-DAG of G},
where a super-DAG of G is a DAG whose edge set is a superset
of that of G. The the search for the true DAG G∗can be
decomposed to two phases: ﬁnding the correct ordering and
performing variable selection; the latter is to ﬁnd the optimal
DAG that is consistent with the ordering found in the ﬁrst step.
3.2
Reinforcement Learning
Standard RL is usually formulated as an MDP over the environ-
ment state s ∈S and agent action a ∈A, under an (unknown)
environmental dynamics deﬁned by a transition probability
T (s′|s, a). Let πφ(a|s) denote the policy, parameterized by φ,
which outputs a distribution used to select an action from ac-
tion space A based on state s. For episodic tasks, a trajectory
τ = {st, at}T
t=0, where T is the ﬁnite time horizon, can be
collected by executing the policy repeatedly. In many cases,

an immediate reward r(s, a) can be received when agent exe-
cutes an action. The objective of RL is to learn a policy which
can maximize the expected cumulative reward along a trajec-
tory, i.e., J(φ) = Eπφ[R0] with R0 = PT
t=0 γtrt(st, at) and
γ ∈(0, 1] being a discount factor. For some scenarios, the
reward is only earned at the terminal time (also called episodic
reward), and J(φ) = Eπφ[R(τ)] with R(τ) = rT (sT , aT ).
4
Method
In this section, we ﬁrst formulate the ordering search problem
as an MDP and then describe the proposed approach. We also
discuss the variable selection methods to obtain DAGs from
variable orderings, as well as the consistency and computa-
tional complexity regarding the proposed method.
4.1
Ordering Search as Markov Decision Process
To incorporate RL into the ordering-based paradigm, we for-
mulate the variable ordering search problem as a multi-step
decision process with a variable as an action at each decision
step, and the order of the selected actions (or variables) is
treated as the searched ordering. The decision-making process
is Markovian, and its elements are described as follows.
State
One can directly take the sample data xj as the
state. However, preliminary experiments (see Appendix A.1)
show that it is difﬁcult for feed-forward neural network models
to capture the underlying causal relationships directly using
observed data as states, and that the data pre-processed by an
encoder module is helpful to ﬁnd better orderings. The encoder
module embeds each xj to state sj and all the embedded states
constitute the state space S := {s1, · · · , sd}. In our case, we
also need an initial state, denoted by s0 (detailed choice is
given in Section 4.2), to select the ﬁrst action. The complete
state space would be ˆS := S ∪{s0}. We will use ˆst to denote
the actual state encountered at the t-th decision step when
generating a variable ordering.
Action
We select an action (variable) from the action
space A := {v1, · · · , vd} consisting of all the variables at
each decision step, and the action space size is equal to the
number of variables, i.e., |A| = d. Compared to the previous
RL-based method that searches over the graph space with
size O(2d×d) [Zhu et al., 2020], the resulting action space
becomes much smaller.
State transition
The state transition is related to the ac-
tion selected at the current decision step. If the selected vari-
able is vj at the t-th decision step, then the state is transferred
to the state sj ∈S which corresponds to xj embedded by the
encoder, i.e., ˆst+1 = sj.
Reward
In ordering-based methods, only the variables
selected in previous decision steps can be the potential par-
ents of the currently selected variable. Hence, we design the
rewards in the following cases: episodic reward and dense re-
ward. In the former case, we calculate the score for a variable
ordering Π with d variables as the episodic reward, i.e.,
R(τ) = rT (ˆsT , aT ) = SBIC(GΠ)
(2)
where T = d −1 and SBIC has been deﬁned in Equation (1),
with Pa(Xj) replaced by the potential parent variable set
U(Xj); here U(Xj) denotes the variables associated with
Figure 2: Illustration of the policy model. The encoder embeds the
observed data xj into the state sj. An action at can be selected by the
decoder according to the given state ˆst and the pointer mechanism at
each time step t. Note that T = d −1. See Appendix A for details.
the nodes in Π≺vj. If the score function is decomposable
(e.g., the BIC score), we can calculate an immediate reward by
exploiting the decomposability for the current decision step.
That is, for vj selected at time step t, the immediate reward is
rt =
m
X
k=1
log p(xk
j |U(xk
j ); θj) −|θj|
2 log m.
(3)
This belongs the second case with dense rewards. Here we
keep −|θj|/2 log m to make Equation (3) consistent with the
form of BIC score.
4.2
Implementation and Optimization with
Reinforcement Learning
We brieﬂy describe the neural network architectures imple-
mented in our method, as shown in Figure 2. More details can
be found in Appendix A.
Encoder
f enc
φe : ˜X 7→S is used to map the observed data
to the embedding space S = {s1, · · · , sd}. Similar to [Zhu et
al., 2020], we adopt mini-batch training and randomly draw n
samples from m samples of the data set X to construct ˜X ∈
Rn×d at each episode. We also set the embedding sj to be in
the same dimension, i.e., sj ∈Rn. For encoder choice, we
conduct an empirical comparison among several representative
structures such as MLP, LSTM and the self-attention based
encoder [Vaswani et al., 2017]. Empirically, we validate that
the self-attention based encoder in the Transformer structure
performs the best (see Appendix A.1).
Decoder
f dec
φd : ˆS 7→A maps the state space ˆS to the
action space A. Among several decoder choices (see also Ap-
pendix A.1 for an empirical comparison), we pick an LSTM
based structure that proves effective in our experiments. Al-
though the initial state is generated randomly in many appli-
cations, we pick it as s0 = 1
d
Pd
i=1 si, considering that the
source node is ﬁxed in a correct ordering. We restrict each
node only be selected once by masking the selected nodes, in
order to generate a valid ordering [Vinyals et al., 2015].
Optimization
The optimization objective is to learn
a policy maximizing J(φ), where φ = {φe, φd} with φe
and φd being parameters associated with encoder f enc and
decoder f dec, respectively. Based on the above deﬁnition,
policy gradient [Sutton and Barto, 2018] is used to opti-
mize the ordering generation model parameters.
For the
episodic reward case, we have the following policy gradi-
ent ∇J(φ) = Eπφ
h
R(τ) PT
t=0 ∇φ log πφ (at|ˆst)
i
, and the
algorithm in this case is denoted as CORL-1. For the dense

reward case, policy gradient can be calculated as ∇J(φ) =
Eπφ
hPT
t=0 Rt∇φ log πφ (at|ˆst)
i
, where Rt = PT −t
l=0 γlrt+l
denotes the return at time step t. We denote the algorithm in
this case as CORL-2. Using a parametric baseline to estimate
the expected score typically improves learning [Sutton and
Barto, 2018]. Therefore, we introduce a critic network Vφv(ˆst)
parameterized by φv, which learns the expected return given
state ˆst and is trained with stochastic gradient descent using
Adam optimizer on a mean squared error objective between
its predicted value and the actual return. More details about
the critic network are described in Appendix A.2.
Inspired by the beneﬁts from pretrained models [Bello et
al., 2016], we also consider to incorporate pretraining to our
method to accelerate training. In practice, one can usually
obtain some observed data with known causal graphs or correct
orderings, e.g., by simulation or real data with labeled graphs.
Hence, we can pretrain a policy model with such data in a
supervised way and use the pretrained model as initialization
for new tasks. Meanwhile, a sufﬁcient generalization ability is
desired and we hence include diverse data sets with different
numbers of nodes, noise types, causal relationships, etc.
4.3
Variable Selection
One can obtain the causal graph from an ordering by con-
ducting variable selection methods, such as sparse candidate
[Teyssier and Koller, 2005], signiﬁcance testing of covariates
[Bühlmann et al., 2014], and group Lasso [Schmidt et al.,
2007]. In this work, for linear data models, we apply linear
regression to the obtained fully-connected DAG and then use
thresholding to prune edges with small weights, as similarly
used by [Zheng et al., 2018]. For the non-linear model, we
adopt the CAM pruning used by [Lachapelle et al., 2020]. For
each variable Xj, one can ﬁt a generalized additive model
against the current parents of Xj and then apply signiﬁcance
testing of covariates, declaring signiﬁcance if the reported
p-values are lower or equal to 0.001.
4.4
Consistency Analysis
So far we have presented CORL in a general manner without
specifying explicitly the distribution family for calculating the
scores or rewards. In principle, any distribution family could
be employed as long as its log-likelihood can be computed.
However, whether the maximization of the accumulated re-
ward recovers the correct ordering, i.e., whether consistency
of the score function holds, depends on both the modelling
choice of reward and the underlying SEM. If the SEM is iden-
tiﬁable, then the following proposition shows that it is possible
to ﬁnd the correct ordering with high probability in the large
sample limit.
Proposition 1. Suppose that an identiﬁable SEM with true
causal DAG G∗on X = {Xj}d
j=1 induces distribution P(X).
Let GΠ be the fully-connected DAG that corresponds to an
ordering Π. If there is an SEM with GΠ inducing the same
distribution P(X), then GΠ must be a super-graph of G∗, i.e.,
every edge in G∗is covered in GΠ.
Proof. The SEM with GΠ may not be causally minimal but
can be reduced to an SEM satisfying the causal minimality
Algorithm 1 Causal discovery with Ordering-based RL.
Require: observed data X, initial parameters φe, φd and φv, two
empty buffers D and Dscore, initial value (negative inﬁnite)
BestScore and a random ordering BestOrdeing.
1: while not terminated do
2:
draw a batch of samples from X, encode them to S and calcu-
late the initial state ˆs0
3:
for t = 0, 1, . . . , T do
4:
collect a batch of data ⟨ˆst, at, rt⟩with πφ: D = D ∪
{⟨ˆst, at, rt⟩}
5:
if ⟨vt, Π≺vt, rt⟩is not in Dscore then
6:
store ⟨vt, Π≺vt, rt⟩in Dscore
7:
end if
8:
end for
9:
update φe, φd, and φv as described in Section 4.2
10:
if PT
t=0 rt > BestScore then
11:
update the BestScore and BestOrdering
12:
end if
13: end while
14: get the ﬁnal DAG by pruning the BestOrdering
condition [Peters et al., 2014]. Let ˜GΠ denotes the causal
graph in the reduced SEM with the same distribution P(X).
Since we have assumed that original SEM is identiﬁable, i.e.,
the distribution P(X) corresponds to a unique true graph, ˜GΠ
is then identical to G∗. The proof is complete by noticing that
GΠ is a super-graph of ˜GΠ.
Thus, if the causal relationships fall into the chosen model
functions and a right distribution family is assumed, then given
inﬁnite samples the optimal accumulated reward (e.g., the
optimal BIC score) must be achieved by a super-DAG of the
underlying graph. However, ﬁnding the optimal accumulated
reward may be hard, because policy gradient methods only
guarantee local convergence [Sutton and Barto, 2018], and we
can only apply approximate model functions and also need to
assume a certain distribution family for calculating the reward.
Nevertheless, the experimental results in Section 5 show that
the proposed method can achieve a better performance than
those with consistency guarantee in the ﬁnite sample regime,
thanks to the improved search ability of modern RL methods.
4.5
Computational Complexity
Our method is summarized in Algorithm 1. In contrast
with typical RL applications, we treat RL here as a search
strategy, aiming to ﬁnd an ordering that achieves the best
score. CORL requires the evaluation of the rewards at each
episode with O(dm2 + d3) computational cost if linear func-
tions are adopted to model the causal relations, which is same
to RL-BIC2 [Zhu et al., 2020]. Fortunately, CORL does not
need to compute the matrix exponential term with O(d3) cost
due to the use of ordering search. We observe that CORL
performs fewer episodes than RL-BIC2 before the episode
reward converges (see Appendix C). The evaluation of Trans-
former encoder and LSTM decoder in CORL take O(nd2)
and O(dn2), respectively. However, we ﬁnd that computing
rewards is dominating in the total running time (e.g., around
95% and 87% for 30- and 100-node linear data models). Thus,
we record the decomposed scores for each variable vj with

Table 1: Empirical results for ER and SF graphs of 30 and 100 nodes with LG data.
RANDOM
NOTEARS
DAG-GNN
RL-BIC2
L1OBS
A* Lasso
CORL-1
CORL-2
30 nodes
ER2
TPR
0.41 (0.04)
0.95 (0.03)
0.91 (0.05)
0.94 (0.05)
0.78 (0.06)
0.88 (0.04)
0.99 (0.02)
0.99 (0.01)
SHD
140.4 (36.7)
14.2 (9.4)
26.5 (12.4)
17.8 (22.5)
85.2 (23.8)
35.3 (14.3)
5.2 (7.4)
4.4 (3.5)
ER5
TPR
0.43 (0.03)
0.93 (0.01)
0.85 (0.11)
0.91 (0.03)
0.74 (0.04)
0.84 (0.05)
0.94 (0.03)
0.95 (0.03)
SHD
210.2 (43.5)
35.4 (7.3)
68.0 (39.8)
45.6 (13.3)
98.6 (32.7)
71.2 (21.5)
37.4 (16.9)
37.6 (14.5)
SF2
TPR
0.58 (0.02)
0.98 (0.02)
0.92 (0.09)
0.99 (0.02)
0.83 (0.04)
0.93 (0.02)
1.0 (0.01)
1.0 (0.01)
SHD
118.4 (12.3)
6.1 (2.3)
36.8 (33.1)
3.2 (1.7)
49.7 (28.1)
27.3 (18.4)
0.0 (0.0)
0.0 (0.0)
SF5
TPR
0.44 (0.03)
0.94 (0.03)
0.89 (0.09)
0.96 (0.03)
0.79 (0.04)
0.88 (0.03)
1.00 (0.00)
1.00 (0.00)
SHD
165.4 (10.6)
23.3 (6.9)
47.8 (35.2)
11.3 (5.2)
89.3 (25.7)
40.5 (19,8)
0.0 (0.0)
0.0 (0.0)
100 nodes
ER2
TPR
0.33 (0.05)
0.93 (0.02)
0.93 (0.03)
0.02 (0.01)
0.54 (0.02)
0.86 (0.04)
0.98 (0.02)
0.98 (0.01)
SHD
491.4 (17.6)
72.6 (23.5)
66.2 (19.2)
270.8 (13.5)
481.2 (49.9)
128.5 (38.4)
24.8 (10.1)
18.6 (5.7)
ER5
TPR
0.34 (0.04)
0.91 (0.01)
0.86 (0.16)
0.08 (0.03)
0.53 (0.02)
0.82 (0.05)
0.93 (0.02)
0.94 (0.03)
SHD
984.4 (35.7)
170.3 (34.2)
236.4 (36.8)
421.2 (46.2)
547.9 (63.4)
244.0 (42.3)
175.3 (18.9)
164.8 (17.1)
SF2
TPR
0.48 (0.03)
0.98 (0.01)
0.89 (0.14)
0.04 (0.02)
0.57 (0.03)
0.92 (0.03)
1.00 (0.00)
1.00 (0.00)
SHD
503.4 (23.8)
2.3 (1.3)
156.8 (21.2)
281.2 (17.4)
377.3 (53.4)
54.0 (22.3)
0.0 (0.0)
0.0 (0.0)
SF5
TPR
0.47 (0.04)
0.95 (0.01)
0.87 (0.15)
0.05 (0.03)
0.55 (0.04)
0.89 (0.03)
0.97 (0.02)
0.98 (0.01)
SHD
891.3 (19.4)
90.2 (34.5)
165.2 (22.0)
405.2 (77.4)
503.7 (56.4)
114.0 (36.4)
19.4 (5.2)
10.8 (6.1)
Figure 3: Learning curves of CORL-1, CORL-2 and CORL-2-
pretrain on 100-node LG data sets.
different parental sets Π≺vj to avoid repeated computations.
5
Experiments
In this section, we conduct experiments on synthetic data sets
with linear and non-linear causal relationships as well as a
real data set. The baselines are ICA-LiNGAM [Shimizu et al.,
2006], three ordering-based approaches L1OBS [Schmidt et
al., 2007], CAM [Bühlmann et al., 2014] and A* Lasso [Xi-
ang and Kim, 2013], some recent gradient-based approaches
NOTEARS [Zheng et al., 2018], DAG-GNN [Yu et al., 2019]
and GraN-DAG [Lachapelle et al., 2020], and the RL-based
approach RL-BIC2 [Zhu et al., 2020]. We use the original
implementations (see Appendix B.1 for details) and pick the
recommended hyper-parameters unless otherwise stated.
We generate different types of synthetic data sets which
vary along: level of edge sparsity, graph type, number of
nodes, causal functions and sample size. Two types of graph
sampling schemes, Erdös–Rényi (ER) and Scale-free (SF),
are considered here. We denote d-node ER and SF graphs
with on average hd edges as ERh and SFh, respectively. Two
common metrics are considered: True Positive Rate (TPR) and
Structural Hamming Distance (SHD). The former indicates
the probability of correctly ﬁnding the positive edges among
the discoveries. Hence, it can be used to measure the quality
of an ordering, and the higher the better. The latter counts the
total number of missing, falsely detected or reversed edges,
and the smaller the better.
5.1
Linear Models with Gaussian and
Non-Gaussian Noise
We evaluate the proposed methods on Linear Gaussian (LG)
with equal variance Gaussian noise and LiNGAM data models,
and the true DAGs in both cases are known to be identiﬁable
[Peters and Bühlmann, 2014; Shimizu et al., 2006]. We set h ∈
{2, 5} and d ∈{30, 50, 100} to generate observed data (see
Appendix B.2 for details). For variable selection, we set the
thresholding as 0.3 and apply it to the estimated coefﬁcients,
as similarly used by [Zheng et al., 2018; Zhu et al., 2020].
Table 1 presents the results for 30- and 100-node LG data
models; the conclusions do not change with 50-node graphs,
which are given in Appendix D. The performances of ICA-
LiNGAM, GraN-DAG and CAM is also given in Appendix D,
and they are almost never on par with the best methods pre-
sented in this section. CORL-1 and CORL-2 achieve consis-
tently good results on LiNGAM data sets which are reported
in Appendix E due to the space limit.
We now examine Table 1 (the values in parentheses repre-
sent the standard deviation across data sets per task). Across
all settings, CORL-1 and CORL-2 are the best performing
methods in terms of both TPR and SHD, while NOTEARS
and DAG-GNN are not too far behind. In Figure 3, we further
show the training reward curves of CORL-1 and CORL-2 on
100-node LG data sets, where CORL-2 converges faster to a
better ordering than CORL-1. We conjecture that this is be-
cause dense rewards can provide more guidance information
for the training process than episodic rewards, which is bene-
ﬁcial to the learning of RL model and improves the training
performance. Hence, CORL-2 is preferred in practice if the
score function is decomposable for each variable.
As dis-
cussed previously, RL-BIC2 only achieves satisfactory results
on graphs with 30 nodes. The TPR of L1OBS is lower than
that of A* Lasso, which indicates that L1OBS using greedy
hill-climbing with tabu lists may not ﬁnd a good ordering.
Note that the SHD of L1OBS and A* Lasso reported here are
the results after applying the introduced pruning method. We
observe that the SHDs are greatly improved after pruning. For
example, the SHDs of L1OBS decrease from 171.6 (29.5),
588.0 (66.2) and 1964.5 (136.6) to 85.2 (23.8), 215.4 (26.3)
and 481.2 (49.9) for ER2 graphs with 30, 50 and 100 nodes,

(a) TPR on GP10
(b) SHD on GP10
(c) TPR on GP30
(d) SHD on GP30
Figure 4: The empirical results on GP data models with 10 and 30 nodes.
respectively, while the TPRs almost keep the same.
We have also evaluated our method on 150-node LG data
models on ER2 graphs. CORL-1 has TPR and SHD being 0.95
(0.01) and 63.7 (9.1), while CORL-2 has 0.97 (0.01) and 38.3
(14.3), respectively. CORL-2 outperforms NOTEARS that
achieves 0.94 (0.02) and 50.8 (21.8).
Pretraining
We show the training reward curve of
CORL-2-pretrain in Figure 3, where the model parameters
are pretrained in a supervised manner. The data sets used
for pretraining contain 30-node ER2 and SF2 graphs with
different causal relationships. Note that the data sets used
for evaluation are different from those used for pretraining.
Compared to that of CORL-2 using random initialization, a
pretrained model can accelerate the model learning process.
Although pretraining requires additional time, it is only carried
out once and when ﬁnished, the pretrained model can be used
for multiple causal discovery tasks. Similar conclusion can be
drawn in terms of CORL-1, which is shown in Appendix G.
Running time
We also report the running time of all
the methods on 30- and 100-node linear data models: CORL-
1, CORL-2, GraN-DAG and DAG-GNN ≈15 minutes for
30-node graphs; CORL-1 and CORL-2 ≈7 hours against
GraN-DAG and DAG-GNN ≈4 hours for 100-node graphs;
CAM ≈15 minutes for both 30- and 100-node graphs, while
L1OBS and A* Lasso ≈2 minutes for that tasks; NOTEARS
≈5 minutes and ≈1 hour for the two tasks respectively; RL-
BIC2 ≈3 hours for 30-node graphs. We set the maximal
running time up to 15 hours, but RL-BIC2 did not converge on
100-node graphs, hence we did not report its results. Note that
the running time can be signiﬁcantly reduced by paralleling
the evaluation of reward. The neural network based learning
methods generally take longer time, and the proposed method
achieves the best performance among these methods.
5.2
Non-Linear Model with Gaussian Process
In this experiment, we consider causal relationships with fj
being a function sampled from a Gaussian Process (GP) with
radial basis function kernel of bandwidth one. The additive
noise follows standard Gaussian distribution, which is known
to be identiﬁable [Peters et al., 2014]. We consider ER1 and
ER4 graphs with different sample numbers (see Appendix B.2
for the generation of data sets), and we only report the results
with m = 500 samples due to the space limit (the remaining
results are given in Appendix F). For comparison, only the
methods that have been shown competitive for this non-linear
data model in existing works [Zhu et al., 2020; Lachapelle et
al., 2020] are included. For a given ordering, we follow [Zhu
et al., 2020] to use GP regression to ﬁt the causal relationships.
We also set a maximum time limit of 15 hours for all the
methods for fair comparison and only graphs with up to 30
nodes are considered here, as using GP regression to calculate
the scores is time-consuming. The variable selection method
used here is the CAM pruning from [Bühlmann et al., 2014].
The results on 10- and 30-node data sets with ER1 and ER4
graphs are shown in Figure 4. Overall, both GraN-DAG and
DAG-GNN perform worse than CAM. We conjecture that this
is because the number of samples are not sufﬁcient for GraN-
DAG and DAG-GNN to ﬁt neural networks well, as also shown
by [Lachapelle et al., 2020]. CAM, CORL-1, and CORL-2
have similar results, with CORL-2 performing the best on
10-node graphs and being slightly worse than CAM on 30-
node graphs. All of these methods have better results on ER1
graphs than on ER4 graphs, especially with 30 nodes. We also
notice that CORL-2 only runs about 700 iterations on 30-node
graphs and about 5000 iterations on 10-node graphs within
the time limit, due to the increased time from GP regression.
Nonetheless, the proposed method achieves a much improved
performance compared with the existing RL-based method.
5.3
Real Data
The Sachs data set [Sachs et al., 2005], with 11-node and
17-edge true graph, is widely used for research on graphical
models. The expression levels of protein and phospholipid in
the data set can be used to discover the implicit protein signal
network. The observational data set has m = 853 samples
and is used to discover the causal structure. We similary use
Gaussian Process regression to model the causal relationships
in calculating the score. In this experiment, CORL-1, CORL-2
and RL-BIC2 achieve the best SHD 11. CAM, GraN-DAG,
and ICA-LiNGAM achieve SHDs 12, 13 and 14, respectively.
Particularly, DAG-GNN and NOTEARS result in SHDs 16
and 19, respectively, whereas an empty graph has SHD 17.
6
Conclusion
In this work, we have incorporated RL into the ordering-based
paradigm for causal discovery, where a generated ordering
can be pruned by variable selection to obtain the causal DAG.
Two methods are developed based on the MDP formulation
and an encoder-decoder framework. We further analyze the
consistency and computational complexity for the proposed ap-
proach. Empirical results validate the improved performance
over existing RL-based causal discovery approach.

References
[Bartlett and Cussens, 2017] Mark Bartlett and James Cussens. In-
teger linear programming for the bayesian network structure learn-
ing problem. Artiﬁcial Intelligence, 244:258–271, 2017.
[Bello et al., 2016] Irwan Bello, Hieu Pham, Quoc V Le, Mo-
hammad Norouzi, and Samy Bengio.
Neural combinato-
rial optimization with reinforcement learning. arXiv preprint
arXiv:1611.09940, 2016.
[Bernstein et al., 2020] Daniel Bernstein, Basil Saeed, Chandler
Squires, and Caroline Uhler. Ordering-based causal structure
learning in the presence of latent variables. In International Con-
ference on Artiﬁcial Intelligence and Statistics (AISTATS), pages
4098–4108. PMLR, 2020.
[Bühlmann et al., 2014] Peter Bühlmann, Jonas Peters, Jan Ernest,
et al.
CAM: Causal additive models, high-dimensional or-
der search and penalized regression. The Annals of Statistics,
42(6):2526–2556, 2014.
[Chickering, 1996] David Maxwell Chickering. Learning Bayesian
networks is NP-complete.
In Learning from Data: Artiﬁcial
Intelligence and Statistics V. Springer, 1996.
[Chickering, 2002] David Maxwell Chickering. Optimal structure
identiﬁcation with greedy search. Journal of Machine Learning
Research, 3(Nov):507–554, 2002.
[De Campos and Ji, 2011] Cassio P De Campos and Qiang Ji. Ef-
ﬁcient structure learning of bayesian networks using constraints.
The Journal of Machine Learning Research, 12:663–689, 2011.
[Friedman and Koller, 2003] Nir Friedman and Daphne Koller. Be-
ing Bayesian about network structure. A Bayesian approach to
structure discovery in Bayesian networks. Machine learning,
50(1-2):95–125, 2003.
[Khalil et al., 2017] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra
Dilkina, and Le Song. Learning combinatorial optimization algo-
rithms over graphs. In Advances in Neural Information Processing
Systems (NeurIPS), 2017.
[Kool et al., 2019] Wouter Kool, Herke Van Hoof, and Max Welling.
Attention, learn to solve routing problems!
In International
Conference on Learning Representations (ICLR), 2019.
[Lachapelle et al., 2020] Sébastien Lachapelle, Philippe Brouillard,
Tristan Deleu, and Simon Lacoste-Julien. Gradient-based neural
DAG learning. In International Conference on Learning Repre-
sentations (ICLR), 2020.
[Larranaga et al., 1996] Pedro Larranaga, Cindy MH Kuijpers,
Roberto H Murga, and Yosu Yurramendi. Learning bayesian
network structures by searching for the best ordering with genetic
algorithms. IEEE Transactions on Systems, Man, and Cybernetics-
Part A: Systems and Humans, 26(4):487–493, 1996.
[Ng et al., 2019a] Ignavier Ng, Zhuangyan Fang, Shengyu Zhu, Zhi-
tang Chen, and Jun Wang. Masked gradient-based causal structure
learning. arXiv preprint arXiv:1910.08527, 2019.
[Ng et al., 2019b] Ignavier Ng, Shengyu Zhu, Zhitang Chen, and
Zhuangyan Fang. A graph autoencoder approach to causal struc-
ture learning. arXiv preprint arXiv:1911.07420, 2019.
[Peters and Bühlmann, 2014] Jonas Peters and Peter Bühlmann.
Identiﬁability of gaussian structural equation models with equal
error variances. Biometrika, 101(1):219–228, 2014.
[Peters et al., 2014] Jonas Peters, Joris M. Mooij, Dominik Janz-
ing, and Bernhard Schölkopf. Causal discovery with continuous
additive noise models. Journal of Machine Learning Research,
15(1):2009–2053, January 2014.
[Raskutti and Uhler, 2018] Garvesh Raskutti and Caroline Uhler.
Learning directed acyclic graph models based on sparsest per-
mutations. Stat, 7(1):e183, 2018.
[Sachs et al., 2005] Karen Sachs, Omar Perez, Dana Pe’er, Dou-
glas A Lauffenburger, and Garry P Nolan.
Causal protein-
signaling networks derived from multiparameter single-cell data.
Science, 308(5721):523–529, 2005.
[Scanagatta et al., 2015] Mauro Scanagatta, Cassio P de Campos,
Giorgio Corani, and Marco Zaffalon. Learning bayesian networks
with thousands of variables. In NeurIPS, 2015.
[Schmidt et al., 2007] Mark Schmidt, Alexandru Niculescu-Mizil,
Kevin Murphy, et al. Learning graphical model structure using
L1-regularization paths. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence (AAAI), 2007.
[Shimizu et al., 2006] Shohei Shimizu, Patrik O Hoyer, Aapo
Hyvärinen, and Antti Kerminen. A linear non-Gaussian acyclic
model for causal discovery. Journal of Machine Learning Re-
search, 7(Oct):2003–2030, 2006.
[Solus et al., 2017] Liam Solus, Yuhao Wang, and Caroline Uhler.
Consistency guarantees for greedy permutation-based causal in-
ference algorithms. arXiv preprint arXiv:1702.03530, 2017.
[Sutton and Barto, 2018] Richard S Sutton and Andrew G Barto.
Reinforcement Learning: An Introduction. MIT press, 2018.
[Teyssier and Koller, 2005] Marc Teyssier and Daphne Koller.
Ordering-based search: A simple and effective algorithm for learn-
ing bayesian networks. In Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), 2005.
[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in
Neural Information Processing Systems (NeurIPS), 2017.
[Vinyals et al., 2015] Oriol Vinyals, Meire Fortunato, and Navdeep
Jaitly. Pointer networks. In Advances in Neural Information
Processing Systems (NeurIPS), 2015.
[Xiang and Kim, 2013] Jing Xiang and Seyoung Kim. A* lasso
for learning a sparse bayesian network structure for continuous
variables. In Advances in Neural Information Processing Systems
(NeurIPS), 2013.
[Yu et al., 2019] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-
GNN: DAG structure learning with graph neural networks. In
International Conference on Machine Learning (ICML), 2019.
[Zheng et al., 2018] Xun Zheng, Bryon Aragam, Pradeep K Raviku-
mar, and Eric P Xing. DAGs with NO TEARS: Continuous opti-
mization for structure learning. In Advances in Neural Information
Processing Systems (NeurIPS), 2018.
[Zheng et al., 2020] Xun Zheng, Chen Dan, Bryon Aragam, Pradeep
Ravikumar, and Eric P Xing. Learning sparse nonparametric
DAGs. In International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), 2020.
[Zhu et al., 2020] Shengyu Zhu, Ignavier Ng, and Zhitang Chen.
Causal discovery with reinforcement learning. In International
Conference on Learning Representations (ICLR), 2020.
[Zoph and Le, 2017] Barret Zoph and Quoc V Le. Neural architec-
ture search with reinforcement learning. In International Confer-
ence on Learning Representations (ICLR), 2017.

Appendix to “Ordering-Based Causal Discovery with Reinforcement Learning”
A
Architectures and Hyper-Parameters
A.1
Encoder and Decoder Architectures
There are a variety of neural network modules that can be
used for encoder and decoder architectures. Here we con-
sider some representative modules, including: Multi Layer
Perceptrons (MLP) module, an LSTM based recurrent neural
network module, and the self-attention based encoder from
the Transformer structure. In addition, we use the original ob-
servational data as the state directly, i.e., no encoder module
is used, which is denoted as Null. More details regarding the
architectures and associated hyper-parameters choices will be
presented in Appendix A.2.
Table 1 reports the empirical results of CORL-2 on 30-
node LG ER2 data sets where the noise variances are equal
(see Appendix B.2 for details about data generation). We
observe that the LSTM based decoder achieves a better per-
formance than that of MLP based decoder, which indicates
that LSTM is more effective than MLP in sequential deci-
sion tasks. The overall performance of neural network en-
coders is better than that of Null, which shows that the data
pre-processed by an encoder module is necessary. Among
all these encoders, Transformer encoder achieves the best re-
sults. Similar conclusion was drawn in [?], and we hypoth-
esize that the performance of Transformer encoder beneﬁts
from the self-attention scheme that provide sufﬁcient interac-
tions amongst variables.
A.2
Model Architectures and Hyper-Parameters
The neural network structure of the Transformer encoder used
in our experiments is given in Figure 1. It consists of a feed-
forward layer with 256 units and three blocks. Each block
is composed of a multi-head attention network with 8 heads
and 2-layer feed-forward neural networks with 1024 and 256
units, and each feed-forward layer is followed by a normal-
ization layer. Given a batch of observed samples with shape
b × d × n, with b denoting the batch size, d the node num-
ber and n the number of observed data in a batch, the ﬁnal
output of the encoder is a batch of embedded state with shape
b × d × 256.
We illustrate the neural network structure of the LSTM
based decoder in Figure 2, which is similar to the decoder
proposed by [?]. The LSTM takes a state as input and outputs
an embedding. The embedding is mapped to the action space
Figure 1: Illustration of the Transformer encoder. The encoder em-
beds the observed data xj of each variable Xj into state sj. Notation
block@3 denotes three blocks.
A by using some feed-forward neural networks, a soft-max
module and the pointer mechanism [?]. The LSTM module
with 256 hidden units is used here. The outputs of encoder
are processed as the initial hidden state h0 for the decoder.
All of the feed-forward neural networks used in the decoder
have 256 units. For LSTM encoder, only the standard LSTM
module with 256 hidden units is used and its output is treated
as the embedded state.
The MLP module consists of 3-layer feed-forward neural
networks with 256, 512 and 256 units. For MLP encoder,
only the standard MLP module is used. For MLP decoder,
its structure is similar to Figure 2, in which LSTM module is
replaced by the MLP module.
Both CORL-1 and CORL-2 use the actor-critic algorithm
to train the model parameters. We use the Adam optimizer
with learning rate 1e−4 and 1e−3 for the actor and critic,
respectively. The discount factor γ is set to 0.98. The actor
consists of an encoder and a decoder whose choices have been
described above. The critic uses 3-layer feed-forward neural
networks with 512, 256, and 1 units, which takes a state ˆs
as input and outputs a predicted value for the current policy
given state ˆs. For CORL-1, the critic needs to predict the
score for each state ˆst, while for CORL-2, the critic takes the
initial state ˆs0 as input and outputs a predicted value directly
for a complete ordering.
arXiv:2105.06631v2  [cs.LG]  17 May 2021

Table 1: Empirical results of CORL-2 with different encoder and decoder architectures on 30-node LG ER2 data sets. True Positive Rate
(TPR) indicates the probability of correctly ﬁnding the positive edges among the discoveries, and the higher the better. Structural Hamming
Distance (SHD) counts the total number of missing, falsely detected or reversed edges, and the smaller the better.
Encoder
Null
LSTM
MLP
Transformer
MLP Decoder
TPR
0.81 (0.07)
0.86 (0.10)
0.96 (0.02)
0.98 (0.02)
SHD
54.2 (25.1)
36.0 (26.7)
11.0 (5.3)
5.0 (3.3)
LSTM Decoder
TPR
0.94 (0.04)
0.88 (0.09)
0.97 (0.01)
0.99 (0.01)
SHD
20.6 (20.0)
29.0 (17.8)
8.6 (4.3)
4.4 (3.5)
Figure 2: Illustration of LSTM decoder. At each time step, it maps
the state ˆst to a distribution over action space A = {a1, · · · , ad}
and then selects an action (variable) according to the distribution.
B
Baselines and Date Sets
B.1
Baselines
The baselines considered in our experiments are listed as fol-
lows:
• ICA-LiNGAM assumes linear non-Gaussian additive
model for data generating procedure and applies inde-
pendent component analysis to recover the weighted ad-
jacency matrix. This method usually achieves good per-
formance on LiNGAM data sets. However, it does not
provide guarantee for linear Gaussian data sets.1
• NOTEARS recovers the causal graph by estimating the
weighted adjacency matrix with the least squares loss
and a smooth characterization for acyclicity constraint.2
• DAG-GNN formulates causal discovery in the frame-
work of variational autoencoder.
It uses a modiﬁed
smooth characterization for acyclicity and optimizes
a weighted adjacency matrix with the evidence lower
bound as loss function.3
• GraN-DAG models the conditional distribution of each
variable given its parents with feed-forward neural net-
works.
It also uses the smooth acyclicity constraint
from NOTEARS to ﬁnd a DAG that maximizes the log-
likelihood of the observed samples.4
• RL-BIC2 formulates the causal discovery as a one-step
decision making process, and combines the score func-
1https://sites.google.com/site/sshimizu06/lingam
2https://github.com/xunzheng/notears
3https://github.com/ﬁshmoon1234/DAG-GNN
4https://github.com/kurowasan/GraN-DAG
tion and acyclicity constraint from NOTEARS as the re-
ward for a directed graph.5
• CAM conducts a greedy estimation procedure that starts
with an empty DAG and adds at each iteration the edge
(vk, vj) between nodes vk and vj that corresponds to the
largest gain in log-likelihood. For a searched ordering,
CAM prunes it to the ﬁnal DAG by applying signiﬁcance
testing of covariates. CAM also performs preliminary
neighborhood selection to reduce the ordering space.6
• L1OBS performs heuristic search (greedy hill-climbing
with tabu lists) through the space of topological order-
ings to see an ordering with the best score. It uses ℓ1
variable selection to prune the searched ordering (fully-
connected DAG) to the ﬁnal DAG.7
• A* Lasso with a limited queue size incorporates a
heuristic scheme into a dynamic programming based
method. The queue size usually needs to be adjusted
to balance the time cost and the quality of the solution.8
B.2
Data Generation
We generate synthetic data sets which vary along ﬁve dimen-
sions: level of edge sparsity, graph type, number of nodes,
causal functions and sample size.
We sample 5 data sets
with a required number of samples for each task: a ground
truth DAG G∗is ﬁrstly drawn randomly from either the
Erd¨os–R´enyi (ER) or Scale-free (SF) graph model, and the
data are then generated according to different given SEMs.
Speciﬁcally, for Linear Gaussian (LG) models, we set h ∈
{2, 5} and d ∈{30, 50, 100} to obtain the ER and SF graphs
with different levels of edge sparsity and different numbers
of nodes. Then we generate 3, 000 samples for each task fol-
lowing the linear SEM: X = W T X + ϵ, where W ∈Rd×d
denotes the weighted adjacency matrix obtained by assigning
edge weights independently from Unif([−2, −0.5]∪[0.5, 2]).
Here ϵ ∈Rd denote standard Gaussian noises with equal vari-
ances for each variable, which makes G∗identiﬁable [?].
For LiNGAM data model, the data sets are generated simi-
larly to LG data models but the noise variables follow non-
Gaussian distributions which pass the noise samples from
Gaussian distribution through a power nonlinearity to make
5https://github.com/huawei-noah/trustworthyAI/tree/master/
Causal Structure Learning/Causal Discovery RL
6https://cran.r-project.org/web/packages/CAM.
7https://www.cs.ubc.ca/∼murphyk/Software/DAGlearn/
8http://www.cs.cmu.edu/∼jingx/software/AstarLasso.zip

them non-Gaussian [?].
LiNGAM is also identiﬁable, as
shown in [?].
The GP data sets with different sample sizes are generated
following Xj = fj(Pa(Xj)) + ϵj, where the function fj is a
function sampled from a GP with radial basis function kernel
of bandwidth one and ϵj follows standard Gaussian distribu-
tion. This setting is also identiﬁable according to [?]. Due
to the efﬁciency of the reward calculation, we only conduct
experiments with up to 30 variables.
C
Number of Episodes Before Convergence
Table 2 reports the total number of episodes required for
CORL-2 and RL-BIC2 to converge, averaged over ﬁve seeds.
CORL-2 performs fewer episodes than RL-BIC2 to reach a
convergence, which we believe is due to reducing the size of
search space and avoiding dealing with acyclicity.
D
Additional Results on LG Data Sets
The results for 50-node LG data models are presented in
Table 3.
The conclusion is similar to that of the 30- and
100-node experiments. The results of ICA-LiNGAM, GraN-
DAG and CAM on LG data models are presented in Table 4.
Their performances do not compare favorably to CORL-1
nor CORL-2 on LG data sets. It is not surprising that ICA-
LiNGAM does not perform well because the algorithm is
speciﬁcally designed for non-Gaussian noise and does not
provide guarantee for LG data models. We hypothesize that
CAM’s poor performance on LG data models is because it
uses nonlinear regression instead of linear regression. As for
GraN-DAG, it uses 2-layer feed-forward neural networks to
model the causal relationships, which may not be able to learn
a good linear relationship in this experiment.
E
Results on LiNGAM Data Sets
We report the empirical results on 30-, 50- and 100-node
LiNGAM data sets in Table 5. For L1OBS, we increased the
recommended number of evaluations, from 2, 500 to 10, 000.
For A* Lasso, we pick the queue size from {10, 500, 1000},
and report the best result out of these parameter settings. The
results of L1OBS and A* Lasso reported here are those after
pruning with the same method as used by CORL-2. For other
baselines, we pick the recommended hyper-parameters.
Among all these algorithms, ICA-LiNGAM can recover
the true graph on most of the LiNGAM data sets.
This
is because ICA-LiNGAM is speciﬁcally designed for non-
Gaussian noises. CORL-1 and CORL-2 achieve consistently
good results, compared with other baselines.
F
Results on 20-Node GP Data Sets with
Different Sample Sizes
We take the 20-node GP data models as an example to show
the performance of our method w.r.t. different sample num-
bers. The data generated based on ER4 graphs. We illustrate
the empirical results in Table 6. Since previous experiments
have shown that CORL-2 is slightly better than CORL-1, we
only report the results of CORL-2 here. We also report the
results of CAM on these data sets, as it is the most compet-
itive baseline. TPR reported here is calculated based on the
variable ordering (i.e., w.r.t. its correponding fully-connected
DAG). As the sample size decreases, CORL-2 tend to per-
form better than CAM, and we believe this is because CORL-
2 beneﬁts from the exploration ability of RL.
G
CORL-1 with a Pretrained Model
Figure 3: Learning curves of CORL-1 and CORL-1-pretrain on 100-
node LG data sets.
Figure 3 shows the training reward curves of CORL-1 and
CORL-1-pretrain; the latter stands for CORL-1 with a pre-
trained model. The data sets used for pretraining are 30-node
ER2 and SF2 graphs with different causal relationships. We
can observe that the pretrained model, which serves as a good
initialization, again accelerates training.

Table 2: Total number of iterations (×103) to reach convergence.
30 nodes
50 nodes
100 nodes
ER2
ER5
ER2
ER5
ER2
ER5
CORL-2
1.0 (0.3)
1.1 (0.4)
1.9 (0.3)
2.4 (0.3)
2.3 (0.5)
2.9 (0.4)
RL-BIC2
3.9 (0.5)
4.1 (0.6)
3.4 (0.4)
3.5 (0.5)
×
×
Table 3: Empirical results for ER and SF graphs of 50 nodes with LG data. The higher TPR the better, the smaller SHD the better.
RANDOM
NOTEARS
DAG-GNN
RL-BIC2
L1OBS
A* Lasso
CORL-1
CORL-2
ER2
TPR
0.31 (0.03)
0.94 (0.02)
0.94 (0.04)
0.79 (0.10)
0.56 (0.02)
0.88 (0.03)
0.97 (0.04)
0.97 (0.02)
SHD
295.4 (28.5)
38.6 (10.8)
30.6 (8.3)
88.5 (49.3)
288.0 (66.2)
154.3 (27.6)
24.0 (32.3
17.9 (10.6)
ER5
TPR
0.32 (0.02)
0.90 (0.01)
0.87 (0.14)
0.74 (0.03)
0.57 (0.03)
0.82 (0.03)
0.90 (0.02)
0.92 (0.02)
SHD
378.4 (24.2)
67.8 (7.5)
93.2 (109.4)
128.9 (40.4)
299.4 (53.6)
104.0 (28.3)
68.3 (10.2)
64.8 (13.1)
SF2
TPR
0.49 (0.04)
0.99 (0.01)
0.90 (0.13)
0.84 (0.05)
0.67 (0.02)
0.89 (0.03)
1.00 (0.00)
1.00 (0.00)
SHD
215.6 (14.7)
3.5 (1.6)
79.3 (93.2)
115.2 (57.4)
182.3 (33.4)
124.0 (35.2)
0.0 (0.0)
0.0 (0.0)
SF5
TPR
0.51 (0.03)
0.94 (0.12)
0.88 (0.12)
0.75 (0.05)
0.61 (0.03)
0.81 (0.02)
0.94 (0.03)
0.95 (0.02)
SHD
345.6 (24.3)
20.1 (14.3)
89.2 (99.2)
115.2 (57.4)
217.3 (36.4)
131.0 (25.3)
24.3 (11.2)
20.8 (10.1)
Table 4: Empirical results of ICA-LiNGAM, GraN-DAG and CAM (against CORL-2 for reference) for ER and SF graphs with LG data.
ICA-LiNGAM
GraN-DAG
CAM
CORL-2
30 nodes
ER2
TPR
0.75 (0.03)
0.51 (0.17)
0.47 (0.05)
0.99 (0.01)
SHD
112.3 (12.8)
96.0 (11.3)
110.8 (10.3)
4.4 (3.5))
ER5
TPR
0.57 (0.03)
0.52 (0.03)
0.46 (0.02)
0.95 (0.03)
SHD
161.8 (9.2)
175.2 (27.4)
191.3 (32.5)
37.6 (14.5)
SF2
TPR
0.58 (0.05)
0.61 (0.04)
0.63 (0.02)
1.0 (0.0)
SHD
149.0 (19,8)
136.4 (21.2)
115.2 (26.7)
0.0 (0.0)
SF5
TPR
0.56 (0.04)
0.58 (0.02)
0.60 (0.03)
1.0 (0.0)
SHD
160.5 (8.9)
142.4 (24.3)
122.2 (17.4)
0.0 (0.0)
50 nodes
ER2
TPR
0.73 (0.03)
0.11 (0.04)
0.55 (0.06)
0.97 (0.02)
SHD
108.8 (11.3)
173.0 (22.9)
140.8 (35.4)
17.9 (10.6)
ER5
TPR
0.57 (0.01)
0.64 (0.03)
0.61 (0.02)
0.92 (0.02)
SHD
199.8 (90.7)
154.2 (36.4)
178.3 (34.8)
64.8 (13.1)
SF2
TPR
0.59 (0.04)
0.44 (0.05)
0.57 (0.02)
1.00 (0.00)
SHD
208.5 (83.2)
158.6 (34.5)
131.2 (24.4)
0.0 (0.0)
SF5
TPR
0.57 (0.01)
0.49 (0.04)
0.53 (0.03)
0.95 (0.02)
SHD
216.6 (88.4)
243.9 (27.2)
235.2 (34.2)
20.8 (10.1)
100 nodes
ER2
TPR
0.73 (0.02)
0.38 (0.02)
0.43 (0.02)
0.98 (0.01)
SHD
268.4 (28.5)
191.3 (31.9)
126.4 (27.8)
18.6 (5.7)
ER5
TPR
0.57 (0.05)
0.42 (0.03)
0.47 (0.02)
0.94 (0.03)
SHD
311.1 (63.7)
208.2 (54.4)
182.3 (34.9)
164.8 (17.1)
SF2
TPR
0.69 (0.03)
0.40 (0.03)
0.44 (0.02)
1.00 (0.00)
SHD
367.6 (67.5)
239.9 (43.2)
35.2 (37.4)
0.0 (0.0)
SF5
TPR
0.57 (0.05)
0.39 (0.03)
0.48 (0.04)
0.98 (0.01)
SHD
362.3 (82.8)
219.3 (32.2)
125.2 (24.7)
10.8 (6.1)

Table 5: Empirical results on 30-, 50- and 100-node LiNGAM ER2 data sets.
30 nodes ER2
50 nodes ER2
100 nodes ER2
Method
TPR
SHD
TPR
SHD
TPR
SHD
ICA-LiNGAM
1.00 (0.00)
0.0 (0.0)
1.00 (0.00)
0.0 (0.0)
1.00 (0.00)
1.0 (0.9)
NOTEARS
0.94 (0.04)
17.2 (13.2)
0.95 (0.02)
33.2 (16.5)
0.94 (0.03)
69.2 (23.2)
DAG-GNN
0.94 (0.03)
19.6 (10.5)
0.96 (0.01)
24.6 (2.9)
0.93 (0.03)
66.2 (19.2)
GraN-DAG
0.28 (0.09)
100.8 (14.6)
0.20 (0.01)
177.0 (25.9)
0.16 (0.04)
312.8 (25.2)
RL-BIC2
0.94 (0.07)
19.8 (23.0)
0.80 (0.08)
86.0 (51.9)
0.13 (0.12)
291.3 (24.1)
CAM
0.60 (0.11)
310.0 (34.0)
0.33 (0.07)
178.0 (31.9)
0.53 (0.05)
247.2 (32.1)
L1OBS
0.72 (0.04)
85.3 (23.3)
0.47 (0.02)
212.6 (24.6)
0.41 (0.03)
470.5 (48.1)
A* Lasso
0.87 (0.03)
42.3 (16.3)
0.88 (0.03)
82.6 (17.6)
0.85 (0.04)
102.5 (22.6)
CORL-1
0.99 (0.01)
3.8 (6.4)
0.96 (0.06)
24.6 (37.7)
0.98 (0.01)
20.0 (7.9)
CORL-2
0.99 (0.01)
3.9 (5.6)
0.96 (0.08)
20.2 (11.3)
0.99 (0.01)
13.8 (7.2)
Table 6: Empirical results on 20-node GP ER1 data sets with different sample sizes.
Sample size
Name
TPR
SHD
1000
CAM
0.91 (0.03)
30.0 (3.7)
CORL-2
0.87 (0.03)
36.5 (3.1)
500
CAM
0.86 (0.03)
45.0 (2.5)
CORL-2
0.85 (0.03)
46.3 (2.3)
400
CAM
0.83 (0.02)
51.0 (2.7)
CORL-2
0.84 (0.03)
50.5 (3.0)
200
CAM
0.60 (0.03)
66.3 (1.9)
CORL-2
0.75 (0.02)
63.1 (1.5)

