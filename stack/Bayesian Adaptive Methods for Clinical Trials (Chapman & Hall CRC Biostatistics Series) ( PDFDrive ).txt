
Bayesian  
Adaptive Methods  
for Clinical Trials

Editor-in-Chief
Shein-Chung Chow, Ph.D.
Professor
Department of Biostatistics and Bioinformatics
Duke University School of Medicine
Durham, North Carolina, U.S.A.
Series Editors
Byron Jones
Senior Director
Statistical Research and Consulting Centre 
(IPC 193)
Pfizer Global Research and Development
Sandwich, Kent, U.K.
Jen-pei Liu
Professor
Division of Biometry
Department of Agronomy
National Taiwan University
Taipei, Taiwan
Karl E. Peace
Georgia Cancer Coalition  
Distinguished Cancer Scholar
Senior Research Scientist and  
Professor of Biostatistics
Jiann-Ping Hsu College of Public Health
Georgia Southern University 
Statesboro, Georgia 
Bruce W. Turnbull
Professor
School of Operations Research  
and Industrial Engineering
Cornell University
Ithaca, New York

Published Titles
 
1. Design and Analysis of Animal Studies in 
Pharmaceutical Development, 
Shein-Chung Chow and Jen-pei Liu
 
2. Basic Statistics and Pharmaceutical Statistical 
Applications, James E. De Muth
 
3. Design and Analysis of Bioavailability and 
Bioequivalence Studies, Second Edition, Revised 
and Expanded, Shein-Chung Chow and 
Jen-pei Liu
 
4. Meta-Analysis in Medicine and Health Policy, 
Dalene K. Stangl and Donald A. Berry
 
5. Generalized Linear Models: A Bayesian 
Perspective, Dipak K. Dey, Sujit K. Ghosh, 
and Bani K. Mallick
 
6. Difference Equations with Public Health 
Applications, Lemuel A. Moyé and 
Asha Seth Kapadia
 
7. Medical Biostatistics, Abhaya Indrayan and 
Sanjeev B. Sarmukaddam
 
8. Statistical Methods for Clinical Trials, 
 Mark X. Norleans
 
9. Causal Analysis in Biomedicine and 
Epidemiology: Based on Minimal Sufficient 
Causation, Mikel Aickin
 10. Statistics in Drug Research: Methodologies and 
Recent Developments, Shein-Chung Chow 
and Jun Shao
 11. Sample Size Calculations in Clinical Research, 
Shein-Chung Chow, Jun Shao, and  
Hansheng Wang
 12. Applied Statistical Design for the Researcher, 
Daryl S. Paulson
 13. Advances in Clinical Trial Biostatistics, 
Nancy L. Geller
 14. Statistics in the Pharmaceutical Industry, 
Third Edition, Ralph Buncher and Jia-Yeong Tsay
 15. DNA Microarrays and Related Genomics 
Techniques: Design, Analysis, and Interpretation 
of Experiments, David B. Allsion, Grier P. Page, 
T. Mark Beasley, and Jode W. Edwards
 16. Basic Statistics and Pharmaceutical Statistical 
Applications, Second Edition, James E. De Muth
 17. Adaptive Design Methods in Clinical Trials, 
Shein-Chung Chow and Mark Chang
 18. Handbook of Regression and Modeling: 
Applications for the Clinical and Pharmaceutical 
Industries, Daryl S. Paulson
 
 19. Statistical Design and Analysis of Stability 
Studies, Shein-Chung Chow
 20. Sample Size Calculations in Clinical Research, 
Second Edition, Shein-Chung Chow, 
Jun Shao, and Hansheng Wang
 21. Elementary Bayesian Biostatistics, 
Lemuel A. Moyé
 22. Adaptive Design Theory and Implementation 
Using SAS and R, Mark Chang
 23. Computational Pharmacokinetics, Anders Källén
 24. Computational Methods in Biomedical Research, 
Ravindra Khattree and Dayanand N. Naik
 25. Medical Biostatistics, Second Edition, 
A. Indrayan
 26. DNA Methylation Microarrays: Experimental 
Design and Statistical Analysis, 
Sun-Chong Wang and Arturas Petronis
 27. Design and Analysis of Bioavailability and 
Bioequivalence Studies, Third Edition, 
Shein-Chung Chow and Jen-pei Liu
 28. Translational Medicine: Strategies and 
Statistical Methods, Dennis Cosmatos and 
Shein-Chung Chow
 29. Bayesian Methods for Measures of Agreement, 
Lyle D. Broemeling
 30. Data and Safety Monitoring Committees in 
Clinical Trials, Jay Herson
 31. Design and Analysis of Clinical Trials with Time-
to-Event Endpoints, Karl E. Peace 
 32. Bayesian Missing Data Problems: EM, Data 
Augmentation and Noniterative Computation,  
Ming T. Tan, Guo-Liang Tian, and Kai Wang Ng
 33. Multiple Testing Problems in Pharmaceutical 
Statistics, Alex Dmitrienko, Ajit C. Tamhane, 
and Frank Bretz 
 34. Bayesian Modeling in Bioinformatics, 
Dipak K. Dey, Samiran Ghosh, and  
Bani K. Mallick
 35. Clinical Trial Methodology, Karl E. Peace 
and Ding-Geng (Din) Chen
 36. Monte Carlo Simulation for the Pharmaceutical 
Industry: Concepts, Algorithms, and Case 
Studies, Mark Chang
 37. Frailty Models in Survival Analysis, 
Andreas Wienke
 38. Bayesian Adaptive Methods for Clinical Trials, 
Scott M. Berry, Bradley P. Carlin, J. Jack Lee, 
and Peter Muller

Scott M. Berry
Berry Consultants
College Station, Texas
Bradley P. Carlin
University of Minnesota
Minneapolis, Minnesota
J. Jack Lee
The University of Texas
MD Anderson Cancer Center
Houston, Texas
Peter Müller
The University of Texas
MD Anderson Cancer Center
Houston, Texas 
Bayesian  
Adaptive Methods  
for Clinical Trials

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2011 by Taylor and Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed in the United States of America on acid-free paper
10 9 8 7 6 5 4 3 2 1
International Standard Book Number: 978-1-4398-2548-8 (Hardback)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been made 
to publish reliable data and information, but the author and publisher cannot assume responsibility for the validity of all 
materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all 
material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not 
been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any 
future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized in 
any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, micro-
filming, and recording, or in any information storage or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://www.
copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-
8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For organizations that 
have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for identi-
fication and explanation without intent to infringe.
Library of Congress Cataloging‑in‑Publication Data
Bayesian adaptive methods for clinical trials / Scott M. Berry ... [et al.].
p. ; cm. --  (Chapman & Hall/CRC biostatistics series ; 38)
Includes bibliographical references and indexes.
Summary: “As has been well-discussed, the explosion of interest in Bayesian methods over the 
last 10 to 20 years has been the result of the convergence of modern computing power and elcient 
Markov chain Monte Carlo (MCMC) algorithms for sampling from and summarizing posterior 
distributions. Practitioners trained in traditional, frequentist statistical methods appear to have been 
drawn to Bayesian approaches for three reasons. One is that Bayesian approaches implemented with 
the majority of their informative content coming from the current data, and not any external prior 
information, typically have good frequentist properties (e.g., low mean squared error in repeated use). 
Second, these methods as now readily implemented in WinBUGS and other MCMC-driven software 
packages now offer the simplest approach to hierarchical (random effects) modeling, as routinely 
needed in longitudinal, frailty, spatial, time series, and a wide variety of other settings featuring 
interdependent data. Third, practitioners are attracted by the greater flexibility and adaptivity of 
the Bayesian approach, which permits stopping for elcacy, toxicity, and futility, as well as facilitates 
a straightforward solution to a great many other specialized problems such as dosing, adaptive 
randomization, equivalence testing, and others we shall describe. This book presents the Bayesian 
adaptive approach to the design and analysis of clinical trials”--Provided by publisher.
 ISBN 978-1-4398-2548-8 (hardcover : alk. paper)
1.  Clinical trials--Statistical methods. 2.  Bayesian statistical decision theory. I. Berry, Scott M. II. 
Series: Chapman & Hall/CRC biostatistics series ; 38. 
[DNLM: 1.  Clinical Trials as Topic. 2.  Bayes Theorem.  QV 771 B357 2011]
R853.C55B385 2011
615.5072’4--dc22 
2010022618
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com 

To
Our families

Contents
Foreword
xi
Preface
xiii
1
Statistical approaches for clinical trials
1
1.1
Introduction
1
1.2
Comparisons between Bayesian and frequentist approaches
4
1.3
Adaptivity in clinical trials
6
1.4
Features and use of the Bayesian adaptive approach
8
1.4.1
The fully Bayesian approach
8
1.4.2
Bayes as a frequentist tool
10
1.4.3
Examples of the Bayesian approach to drug and
medical device development
12
2
Basics of Bayesian inference
19
2.1
Introduction to Bayes’ Theorem
19
2.2
Bayesian inference
26
2.2.1
Point estimation
26
2.2.2
Interval estimation
27
2.2.3
Hypothesis testing and model choice
29
2.2.4
Prediction
34
2.2.5
Eﬀect of the prior: sensitivity analysis
37
2.2.6
Role of randomization
38
2.2.7
Handling multiplicities
40
2.3
Bayesian computation
42
2.3.1
The Gibbs sampler
44
2.3.2
The Metropolis-Hastings algorithm
45
2.3.3
Convergence diagnosis
48
2.3.4
Variance estimation
49
2.4
Hierarchical modeling and metaanalysis
51
2.5
Principles of Bayesian clinical trial design
63
2.5.1
Bayesian predictive probability methods
64

viii
CONTENTS
2.5.2
Bayesian indiﬀerence zone methods
66
2.5.3
Prior determination
68
2.5.4
Operating characteristics
70
2.5.5
Incorporating costs
78
2.5.6
Delayed response
81
2.5.7
Noncompliance and causal modeling
82
2.6
Appendix: R Macros
86
3
Phase I studies
87
3.1
Rule-based designs for determining the MTD
88
3.1.1
Traditional 3+3 design
88
3.1.2
Pharmacologically guided dose escalation
91
3.1.3
Accelerated titration designs
92
3.1.4
Other rule-based designs
92
3.1.5
Summary of rule-based designs
92
3.2
Model-based designs for determining the MTD
93
3.2.1
Continual reassessment method (CRM)
94
3.2.2
Escalation with overdose control (EWOC)
102
3.2.3
Time-to-event (TITE) monitoring
105
3.2.4
Toxicity intervals
109
3.2.5
Ordinal toxicity intervals
113
3.3
Eﬃcacy versus toxicity
116
3.3.1
Trial parameters
117
3.3.2
Joint probability model for eﬃcacy and toxicity
117
3.3.3
Deﬁning the acceptable dose levels
118
3.3.4
Eﬃcacy-toxicity trade-oﬀcontours
118
3.4
Combination therapy
121
3.4.1
Basic Gumbel model
122
3.4.2
Bivariate CRM
126
3.4.3
Combination therapy with bivariate response
127
3.4.4
Dose escalation with two agents
129
3.5
Appendix: R Macros
134
4
Phase II studies
137
4.1
Standard designs
137
4.1.1
Phase IIA designs
138
4.1.2
Phase IIB designs
140
4.1.3
Limitations of traditional frequentist designs
142
4.2
Predictive probability
142
4.2.1
Deﬁnition and basic calculations for binary data
143
4.2.2
Derivation of the predictive process design
146
4.3
Sequential stopping
150
4.3.1
Binary stopping for futility and eﬃcacy
150
4.3.2
Binary stopping for futility, eﬃcacy, and toxicity
151

CONTENTS
ix
4.3.3
Monitoring event times
154
4.4
Adaptive randomization and dose allocation
155
4.4.1
Principles of adaptive randomization
155
4.4.2
Dose ranging and optimal biologic dosing
163
4.4.3
Adaptive randomization in dose ﬁnding
167
4.4.4
Outcome adaptive randomization with delayed
survival response
168
4.5
Hierarchical models for phase II designs
173
4.6
Decision theoretic designs
176
4.6.1
Utility functions and their speciﬁcation
176
4.6.2
Screening designs for drug development
179
4.7
Case studies in phase II adaptive design
183
4.7.1
The BATTLE trial
183
4.7.2
The I-SPY 2 trial
189
4.8
Appendix: R Macros
191
5
Phase III studies
193
5.1
Introduction to conﬁrmatory studies
193
5.2
Bayesian adaptive conﬁrmatory trials
195
5.2.1
Adaptive sample size using posterior probabilities
196
5.2.2
Futility analyses using predictive probabilities
200
5.2.3
Handling delayed outcomes
204
5.3
Arm dropping
208
5.4
Modeling and prediction
211
5.5
Prior distributions and the paradigm clash
218
5.6
Phase III cancer trials
221
5.7
Phase II/III seamless trials
228
5.7.1
Example phase II/III trial
230
5.7.2
Adaptive design
231
5.7.3
Statistical modeling
232
5.7.4
Calculation
233
5.7.5
Simulations
235
5.8
Case study: Ablation device to treat atrial ﬁbrillation
241
5.9
Appendix: R Macros
247
6
Special topics
249
6.1
Incorporating historical data
249
6.1.1
Standard hierarchical models
250
6.1.2
Hierarchical power prior models
252
6.2
Equivalence studies
260
6.2.1
Statistical issues in bioequivalence
261
6.2.2
Binomial response design
263
6.2.3
2 × 2 crossover design
265
6.3
Multiplicity
268

x
CONTENTS
6.3.1
Assessing drug safety
269
6.3.2
Multiplicities and false discovery rate (FDR)
275
6.4
Subgroup analysis
276
6.4.1
Bayesian approach
276
6.4.2
Bayesian decision theoretic approach
277
6.5
Appendix: R Macros
280
References
281
Author index
297
Index
303

Foreword
It’s traditional to get a foreword written by an ´eminence grise, generally
an aging researcher who has seen better days. I can provide plenty of grise
although I am possibly a bit short on ´eminence. Perhaps I best qualify
through sheer long-service in trying to promote Bayesian clinical trials,
having started my small contribution to this epic eﬀort nearly 30 years ago
with Laurence Freedman, eliciting prior opinions from oncologists about
the plausible beneﬁts of new cancer therapies.
This ﬁne book represents the most recent and exciting developments
in this area, and gives ample justiﬁcation for the power and elegance of
Bayesian trial design and analysis. But it is still a struggle to get these
ideas accepted. Why is this? I can think of four main reasons: ideological,
bureaucratic, practical and pragmatic.
By ideological, I mean the challenge facing the “new” idea of using prob-
ability theory to express our uncertainty about a parameter or existing
state of the world – our epistemic uncertainty. Of course “new” is ironic,
given it is nearly 250 years since Bayes formalized the idea, but the idea
is still unfamiliar and disturbing to those brought up on classical ideas of
probability as long-run frequency. One can only sympathize with all that
eﬀort to master the correct deﬁnition of a p-value and a conﬁdence interval,
only to be told that the intuitive meanings can be right after all.
I really enjoy introducing students to this beautiful idea, but tend to
leave Bayes’ theorem to subsequent lectures. In fact I sometimes feel the
role of Bayes’ theorem in Bayesian analysis is overemphasized: the crucial
element is being willing to put a distribution over a parameter, and it is not
always necessary even to mention the “B-word.” Natural examples include
models for informative dropout in clinical trials, and the size of possible
biases in historical studies: in these situations there may be no information
in the data about the parameter, and so Bayes’ theorem is not used.
But of course there are bureaucratic obstacles: as the authors of this
book make clear, regulatory agencies perform a gate-keeping role where the
Neyman-Pearson framework of decision-making without a loss function still
has merits. Although the posterior distribution tells us what it is reasonable
to believe given the evidence in a speciﬁc study, the regulators do need to

xii
FOREWORD
consider a continuous sequence of drug approval decisions. So quantifying
Type I and Type II error can still be a valuable element of trial design, and
one that is excellently covered in this book.
Then there are practical problems: can we actually do the analysis, or is
the mathematics too tricky and there’s no software to help us along? The
authors have done a great job in discussing computation and providing
software, but I am sure would still admit that there’s some way to go
before all these wonderful techniques are easily available to the average
trial designer. But it will happen.
Finally, the crucial pragmatic test. Do these techniques help us do things
we could not do before? This has been the factor that has led to increasingly
widespread penetration of Bayesian methods into subject domains over the
last 20 years or so: people can ﬁt models and make inferences that were
previously impossible or very cumbersome. And this is where this book
wins hands down, since adaptive trials are so natural, ethical and eﬃcient,
that everyone wants to do them.
This book, based on the many years of cumulative experience of the
authors, manages to deal with all these diﬃculties. Adaptive studies are a
perfect application for a Bayesian approach, and I am conﬁdent that this
book will be a major contribution to the science and practice of clinical
trials.
David J. Spiegelhalter
MRC Biostatistics Unit and University of Cambridge
April 2010

Preface
As has been well discussed, the explosion of interest in Bayesian methods
over the last 10 to 20 years has been the result of the convergence of modern
computing power and eﬃcient Markov chain Monte Carlo (MCMC) algo-
rithms for sampling from and summarizing posterior distributions. Prac-
titioners trained in traditional, frequentist statistical methods appear to
have been drawn to Bayesian approaches for three reasons. One is that
Bayesian approaches implemented with the majority of their informative
content coming from the current data, and not any external prior informa-
tion, typically have good frequentist properties (e.g., low mean squared er-
ror in repeated use). Second, these methods as now readily implemented in
WinBUGS and other MCMC-driven software packages now oﬀer the simplest
approach to hierarchical (random eﬀects) modeling, as routinely needed
in longitudinal, frailty, spatial, time series, and a wide variety of other
settings featuring interdependent data. Third, practitioners are attracted
by the greater ﬂexibility and adaptivity of the Bayesian approach, which
permits stopping for eﬃcacy, toxicity, and futility, as well as facilitates a
straightforward solution to a great many other specialized problems such
as dose-ﬁnding, adaptive randomization, equivalence testing, and others we
shall describe.
This book presents the Bayesian adaptive approach to the design and
analysis of clinical trials. The ethics and eﬃciency of such trials can beneﬁt
from Bayesian thinking; indeed the Food and Drug Administration (FDA)
Center for Devices and Radiological Health (CDRH) has been encourag-
ing this through its document Guidance for the Use of Bayesian Statis-
tics; see www.fda.gov/MedicalDevices/DeviceRegulationandGuidance/
GuidanceDocuments/ucm071072.htm. The FDA Center for Drug Evalua-
tion and Research (CDER) and Center for Biologics Evaluation and Re-
search (CBER) has issued its own Guidance for Industry: Adaptive Design
Clinical Trials for Drugs and Biologics: www.fda.gov/downloads/Drugs/
GuidanceComplianceRegulatoryInformation/Guidances/UCM201790
.pdf. This document also mentions Bayes, albeit far less prominently. The
recent series of winter Bayesian biostatistics conferences at the University

xiv
PREFACE
of Texas M.D. Anderson Cancer Center in Houston are also testament to
the growing role Bayesian thinking plays in this ﬁeld.
The outline of the book is as follows. In Chapter 1 we summarize the
current state of clinical trial design and analysis, present the main ideas be-
hind the Bayesian alternative, and describe the potential beneﬁts of such an
alternative. We also describe what we mean by the word “adaptive” in the
book’s title. Chapter 2 then gives an overview of the basic Bayesian method-
ological and computational tools one needs to get started as a Bayesian
clinical trialist. While this whirlwind tour is not a substitute for a full
course in Bayesian methods (as from Gelman et al., 2004, or Carlin and
Louis, 2009), it should enable those with a basic understanding of classical
statistics to get “up and running” on the material. This chapter also in-
cludes overviews of hierarchical modeling (with special emphasis on its role
in Bayesian metaanalysis) and the basics of Bayesian clinical trial design
and analysis. The idea here is to establish the basic principles that will be
expanded and made phase- and endpoint-speciﬁc in subsequent chapters.
The next two chapters of the book (Chapters 3–4) follow standard clinical
trials practice by giving Bayesian tools useful in “early” and “middle” phase
clinical trials, roughly corresponding to phases I and II of the U.S. drug
regulatory process, respectively. While our own professional aﬃliations have
led us to focus primarily on oncology trials, the techniques we describe are
readily adapted to other disease areas. We also place primary emphasis on
“partially Bayesian” designs that concentrate on probability calculations
utilizing prior information and Bayesian updating while still maintaining
good frequentist properties (power and Type I error). An exception to this
general rule is Section 4.6, where we discuss “fully Bayesian” designs that
incorporate a utility function (and often more informative priors) within
a more formal decision-theoretic framework. Chapter 4 also contains brief
reviews of two recent trials utilizing Bayesian adaptive designs, BATTLE
and I-SPY 2.
Chapter 5 deals with late (phase III) studies, an important area and
the one of potentially greatest interest to statisticians seeking ﬁnal regu-
latory approval for their compounds. Here we emphasize modern adaptive
methods, seamless phase II–III trials for maximizing information usage and
minimizing trial duration, and describe in detail a case study of a recently
approved medical device. Finally, Chapter 6 deals with several important
special topics that ﬁt into various phases of the process, including the use of
historical data, equivalence studies, multiplicity and multiple comparisons,
and the related problem of subgroup analysis. The historical data material
is particularly relevant for trials of medical devices, where large historical
databases often exist, and where the product being evaluated (say, a car-
diac pacemaker) is evolving slowly enough over time that worries about the
exchangeability of the historical and current data are relatively low.
Since this is not a “textbook” per se, we do not include homework prob-

PREFACE
xv
lems at the end of every chapter. Rather, we view this book as a handbook
enabling those engaged in clinical trials research to update and expand their
toolkit of available techniques, so that Bayesian methods may be used when
appropriate. See http://www.biostat.umn.edu/~brad/data3.html and
http://biostatistics.mdanderson.org/SoftwareDownload/ on the web
for many of our datasets, software programs, and other supporting infor-
mation. The ﬁnal sections of Chapters 2–6 link to these software sites and
provide programming notes on the R and WinBUGS code we recommend.
We owe a debt of gratitude to those who helped in our writing process.
In particular, the second author is very grateful to Prof. Donald Berry
and the Division of Quantitative Sciences at the University of Texas M.D.
Anderson Cancer Center for allowing him to spend his fall 2008 sabbatic
time in the same U.S. state as the other three authors. Key staﬀmem-
bers worthy of special mention are Martha Belmares and the incomparable
Lydia Davis. Sections 1.1, 1.2, 1.4, and 2.4 are based on Prof. Berry’s pre-
vious work in their respective areas. Indeed, many sections of the book
owe much to the hard work of our research colleagues, including Lee Ann
Chastain, Nan Chen, Jason Connor, Laura Hatﬁeld, Brian Hobbs, Haijun
Ma, Ashish Sanil, and Amy Xia. We also thank the 2010 spring semester
“Topics in Clinical Trials” class at Rice University and the University of
Texas Graduate School of Biomedical Sciences,” taught by the third au-
thor, for commenting on the text and testing the supporting software. Rob
Calver and David Grubbs at Chapman and Hall/CRC/Taylor & Francis
Group were pillars of strength and patience, as usual. Finally, we thank
our families, whose ongoing love and support made all of this possible.
Scott M. Berry
College Station, Texas
Bradley P. Carlin
Minneapolis, Minnesota
J. Jack Lee
Houston, Texas
Peter M¨uller
Houston, Texas
March 2010

CHAPTER 1
Statistical approaches for clinical trials
1.1 Introduction
Clinical trials are prospective studies to evaluate the eﬀect of interventions
in humans under prespeciﬁed conditions. They have become a standard
and an integral part of modern medicine. A properly planned and executed
clinical trial is the most deﬁnitive tool for evaluating the eﬀect and applica-
bility of new treatment modalities (Pocock, 1983; Piantadosi, 2005; Cook
and Demets, 2008).
The standard statistical approach to designing and analyzing clinical tri-
als and other medical experiments is frequentist. A primary purpose of this
book is to describe an alternative approach called the Bayesian approach.
The eponym originates from a mathematical theorem derived by Thomas
Bayes (1763), an English clergyman who lived from 1702 to 1761. Bayes’
theorem plays a fundamental role in the inferential and calculational as-
pects of the Bayesian approach. The Bayesian approach can be applied sep-
arately from frequentist methodology, as a supplement to it, or as a tool for
designing eﬃcient clinical trials that have good frequentist properties. The
two approaches have rather diﬀerent philosophies, although both deal with
empirical evidence and both use probability. Because of the similarities, the
distinction between them is often poorly understood by nonstatisticians.
A major diﬀerence is ﬂexibility, in both design and analysis. In the
Bayesian approach, experiments can be altered in midcourse, disparate
sources of information can be combined, and expert opinion can play a role
in inferences. This is not to say that “anything goes.” For example, even
though nonrandomized trials can be used in a Bayesian analysis, biases that
can creep into some such trials can, in eﬀect, make legitimate conclusions
impossible. Another major diﬀerence is that the Bayesian approach can be
decision-oriented, with experimental designs tailored to maximize objective
functions, such as company proﬁts or overall public health beneﬁt.
Much of the material in this book is accessible to nonstatisticians. How-
ever, to ensure that statisticians can follow the arguments and reproduce
the results, we also include technical details. Not all of this technical devel-
opment will be accessible to all readers. Readers who are not interested in

2
STATISTICAL APPROACHES FOR CLINICAL TRIALS
technicalities may skim or skip the mathematics and still proﬁtably focus
on the ideas.
Certain subjects presented in this book are treated in a rather cursory
fashion. References written from the same perspective as the current re-
port but that are somewhat more comprehensive in certain regards include
(Berry, 1991; 1993). The text by Berry (1996) and its companion com-
puting supplement by Albert (1996) explain and illustrate Bayesian statis-
tics in very elementary terms and may be helpful to readers who are not
statisticians. Other readers may ﬁnd more advanced Bayesian texts acces-
sible. These texts include Box and Tiao (1973), Berger (1985), DeGroot
(1970), Bernardo and Smith (1994), Lee (1997), Robert (2001), Gelman,
Carlin, Stern, and Rubin (2004), and Carlin and Louis (2009). Berry and
Stangl (1996) is a collection of case studies in Bayesian biostatistics; it
gives applications of modern Bayesian methodology. Finally, the lovely text
by Spiegelhalter et al. (2004) is an outstanding introduction to Bayesian
thinking in many problems important to biostatisticians and medical pro-
fessionals generally, one of which is clinical trials.
Turning to the area of computing, Gilks, Richardson, and Spiegelhalter
(1996) is a collection of papers dealing with modern Bayesian computer sim-
ulation methodology that remains relevant since it was so many years ahead
of its time at publication. Two other recent Bayesian computing books by
Albert (2007) and Marin and Robert (2007) are also important. Both books
adopt the R language as their sole computing platform; indeed, both include
R tutorials in their ﬁrst chapters. Albert (2007) aims at North American
ﬁrst-year graduate or perhaps advanced undergraduate students, building
carefully from ﬁrst principles and including an R package, LearnBayes, for
implementing many standard methods. By contrast, the level of formal-
ity and mathematical rigor in Marin and Robert (2007) is at least that of
its fairly mature stated audience of second-year master’s students. In the
present book, we also use R as our “base” computing platform, consistent
with its high and accelerating popularity among statisticians. However, we
also take advantage of other, mostly freely available packages when they
oﬀer the most sensible solutions. In particular, we rely on WinBUGS, both
by itself and as called from R by the BRugs library. This popular software
has emerged as the closest thing to an “industry standard” that exists in
the applied Bayesian statistical community.
We now oﬀer a simple example to help show some of the primary features
of the frequentist perspective. We will return to this setting in Example 2.2
to show the corresponding Bayesian solution and its features.
Example 1.1 Suppose an experiment is conducted in which a device is
used to treat n = 100 patients, and a particular outcome measurement
is made on each. The design value for the device is a measurement of 0,
but as is usual, there is variability from the design value even under ideal

INTRODUCTION
3
−3
−2
−1
0
1
2
3
0.0
0.1
0.2
0.3
0.4
x−bar
dnorm(x)
Figure 1.1 Frequentist sampling distribution of the test statistic, ¯x, when µ = 0.
Each of the tails (left of –1.96 and right of 1.96) has area under the curve equal
to 0.025, so that the two-sided p-value is 0.05.
conditions. The goal of the experiment is to assess whether the mean µ
of the measurements in some population of devices is in fact 0: The null
hypothesis is that µ = µ0 = 0. Suppose that the average ¯x of the 100
measurements is 1.96 and the standard deviation σ is 10. In a frequentist
analysis, one calculates a z-score,
z = ¯x −µ0
σ/√n =
¯x −0
10/
√
100 = ¯x = 1.96 .
Since 1.96 is the 97.5 percentile of the standard normal distribution, the
null hypothesis is rejected at the (two-sided) 5% level of signiﬁcance. Put
another way, the results are statistically signiﬁcant with a p-value of 0.05.
This p-value statement is poorly understood by most nonstatisticians. Its
interpretation is as follows. If the population mean µ is indeed 0, observing
a value of ¯x as extreme as that observed or more so (that is, either larger
than or equal to 1.96, or smaller than or equal to –1.96) has probability
0.05 when the null hypothesis is true. The p-value of 0.05 is the sum of the
areas of the two tail regions indicated in Figure 1.1. The density shown in
this ﬁgure is for ¯x, conditioning on the null hypothesis being true. Because
p-values are tail areas, they include probabilities of observations that are
possible, but that were not actually observed.

4
STATISTICAL APPROACHES FOR CLINICAL TRIALS
1.2 Comparisons between Bayesian and frequentist approaches
This section addresses some of the diﬀerences between the Bayesian and
frequentist approaches. Later sections will discuss other diﬀerences and give
details of the comparisons made here. Listing diﬀerences gives a one-sided
view; there are many similarities between the two approaches. For exam-
ple, both recognize the need for controls when evaluating an experimental
therapy. Still, with this caveat, here are some of the key diﬀerences between
the two approaches:
1. Probabilities of parameters: All unknowns have probability distributions
in the Bayesian approach. In the frequentist approach, probabilities are
deﬁned only on the data space. In particular, Bayesians associate prob-
abilities distributions with parameters while frequentists do not. These
distributions are called the prior and posterior distributions. The former
summarizes information on the parameters before the data are collected,
while the latter conditions on the data once observed.
2. Using all available evidence: The fundamental inferential quantity in
the Bayesian approach is the posterior distribution of the various un-
known parameters. This distribution depends on all information cur-
rently available about these parameters. In contrast, frequentist mea-
sures are speciﬁc to a particular experiment. This diﬀerence makes the
Bayesian approach more appealing in a sense, but assembling, assess-
ing, and quantifying information from outside the trial makes for more
work. One approach to combining data is hierarchical modeling. This is
especially easy to implement from a Bayesian point of view, and leads to
borrowing of estimative strength across similar but independent experi-
ments. The use of hierarchical models for combining information across
studies is a Bayesian approach to metaanalysis; see Example 2.7.
3. Conditioning on results actually observed: Bayesian inferences depend
on the current study only through the data actually observed, while
frequentist measures involve probabilities of data (calculated by condi-
tioning on particular values of unknown parameters) that were possible
given the design of the trial, but were not actually observed. For ex-
ample, in Example 1.1, the value of ¯x that was observed was precisely
1.96, yet the p-value included the probability of ¯x > 1.96 and also of
¯x ≤−1.96 (assuming the null hypothesis). On the other hand, in the
Bayesian approach all probabilities condition only on ¯x = 1.96, the ac-
tual observed data value. See discussions of the Likelihood Principle in
Berger and Wolpert (1984), Berger and Berry (1988), Carlin and Louis
(2009, pp. 8, 51), as well as Subsection 2.2.3.
4. Flexibility: Bayesian inferences are ﬂexible in that they can be updated
continually as data accumulate. For example, the reason for stopping a
trial aﬀects frequentist measures but not Bayesian inferences. (See dis-

COMPARISONS BETWEEN BAYESIAN AND FREQUENTIST APPROACHES
5
cussions of the likelihood principle referred to in item 3 above.) Frequen-
tist measures require a complete experiment, one carried out according
to the prespeciﬁed design. Some frequentists are not hampered by such
restrictions, and reasonably so, but the resulting conclusions do not have
clear inferential interpretations. In a Bayesian approach, a sample size
need not be chosen in advance; before a trial, the only decision required
is whether or not to start it. This decision depends on the associated
costs and beneﬁts, recognizing when information will become available
should the trial start. Once a trial or development program has begun,
decisions can be made (at any time) as to whether to continue. Certain
types of deviations from the original plan are possible: the sample size
projection can be changed, the drugs or devices involved can be modiﬁed,
the deﬁnition of the patient population can change, etc. Such changes
can weaken some conclusions (unless they are prespeciﬁed, which we ad-
vocate), but Bayesian analyses may still be possible in situations where
frequentist analyses are not.
5. Role of randomization: Randomized controlled trials are the gold stan-
dard of medical research. This is true irrespective of statistical approach.
Randomization minimizes the possibility of selection bias, and it tends
to balance the treatment groups over covariates, both known and un-
known. There are diﬀerences, however, in the Bayesian and frequentist
views of randomization. In the latter, randomization serves as the basis
for inference, whereas the basis for inference in the Bayesian approach
is subjective probability, which does not require randomization.
6. Predictive probabilities: A Bayesian approach allows for calculating pre-
dictive probabilities, such as the probability that Ms. Smith will respond
to a new therapy. Probabilities of future observations are possible in a
formal frequentist approach only by conditioning on particular values of
the parameters. Bayesians average these conditional probabilities over
unknown parameters, using the fact that an unconditional probability
is the expected value of conditional probabilities.
7. Decision making: The Bayesian approach is ideal for and indeed is tai-
lored to decision making. Designing a clinical trial is a decision problem.
Drawing a conclusion from a trial, such as recommending a therapy
for Ms. Smith, is a decision problem. Allocating resources among R&D
projects is a decision problem. When to stop device development is a
decision problem. There are costs and beneﬁts involved in every such
problem. In the Bayesian approach these costs and beneﬁts can be as-
sessed for each possible sequence of future observations. Consider a par-
ticular decision. It will give rise to one among a set of possible future
observations, each having costs and beneﬁts. These can be weighed by
their corresponding predictive probabilities. The inability of the frequen-

6
STATISTICAL APPROACHES FOR CLINICAL TRIALS
tist approach to ﬁnd predictive probabilities makes it poorly suited to
decision making; see Section 4.6.
All of this is not to say that the frequentist approach to clinical trials is
totally without merit. Frequentism ﬁts naturally with the regulatory “gate-
keeping” role, through its insistence on procedures that perform well in the
long run regardless of the true state of nature. And indeed frequentist op-
erating characteristics (Type I and II error, power) are still very important
to the FDA and other regulators; see Subsections 1.4.2 and 2.5.4.
1.3 Adaptivity in clinical trials
The bulk of this chapter (and indeed the entire book) is devoted to de-
scribing the intricacies of the Bayesian approach, and its distinction from
corresponding frequentist approaches. However, we pause brieﬂy here to de-
scribe what we mean by the word “adaptive” in the book’s title. Certainly
there are a large number of recent clinical trial innovations that go under
this name, both frequentist and Bayesian. But perhaps it won’t come as a
surprise at this point that the two camps view the term rather diﬀerently.
Concerned as they must be with overall Type I error, frequentists have
sometimes referred to any procedure that changes its stopping boundaries
over time while still protecting overall Type I error rate as “adaptive.”
More recently, both frequentists and Bayesians mean a procedure that al-
ters something based on the results of the trial so far. But of course this is a
serious shift in the experimental design, and thus one that must be reﬂected
in the Type I error calculation. By contrast, freedom from design-based in-
ference means Bayesians are free to enter a trial with nothing more than
a stopping rule and a (possibly minimally informative) prior distribution.
In particular, note we need not select the trial’s sample size in advance
(although a maximum sample size is often given). Any procedure we de-
velop can be simulated and checked for frequentist soundness, but this is
not required for the Bayesian procedure to be sensibly implemented.
But all this raises the questions of what sorts of adaptation do we envision
in our trials, and in what sorts of settings (e.g., early versus late phase).
Of course, these two questions are related, since the task at hand depends
on the phase. But certainly it is true that as of the current writing, a great
many non-ﬁxed-sample-size trials are running across a variety of phases
of the regulatory process. In early phase studies it seems ethically most
important to be adaptive, since the patients are often quite ill, making
sudden treatment changes both more needed and possibly more frequent.
Phase I drug studies are typically about safety and dose-ﬁnding, meaning
in the latter case that the dose a patient receives is not ﬁxed in advance,
but rather determined by the outcomes seen in the patients treated to date.
The traditional approach for doing this, the so-called “3 + 3” design (see
Subsection 3.1.1) is constructed from sensible rules, but turns out to be

ADAPTIVITY IN CLINICAL TRIALS
7
a bit simpleminded with respect to its learning; model-based procedures
(described in Subsection 3.2) use the information in the data to better
advantage. We might also be interested in trading oﬀeﬃcacy and toxicity
where both are explicitly observed; here the EﬀTox approach and software
of Thall and Cook (2004) oﬀers an excellent example of an adaptive dose-
ﬁnding trial (see Subsection 3.3). The problems created by combination
therapies, where we seek to estimate the joint eﬀect of two concurrently
given treatments (which may well interact in the body) is another setting
in which adaptivity is paramount; see Subsection 3.4.
In phase II, we typically seek to establish eﬃcacy while still possibly
guarding against excess toxicity, and also against futility, i.e., continuing a
trial that is unlikely to ever produce a signiﬁcant result even if all avail-
able patients are enrolled. In such settings, we again wish to be adaptive,
stopping the trial early if any of the three conclusions (eﬃcacy, toxicity, or
futility) can be reached early; see Section 4.3. We may also wish to drop
unproductive or unpromising study arms, again a signiﬁcant alteration of
the design space but one that in principle creates no diﬃculties within the
Bayesian model-based paradigm.
Another form of adaptivity often encountered in phase II is that of adap-
tive randomization. For example, our trial’s goal may be to maintain the
advantages of randomizing patients to treatment assignment while allowing
the assignment of more patients to the treatments that do better in the trial.
Note that this sort of adaptive dose allocation is distinct from determin-
istic adaptive treatment assignment, such as so-called “play-the-winner”
rules (see e.g. Ware, 1989). In Section 4.4 we focus on outcome-adaptive (or
response-adaptive) designs, as opposed to covariate-adaptive designs that
seek to balance covariates across treatments. In particular, Subsection 4.4.4
oﬀers a challenging example where we wish to adapt in this way while also
facing the issue of delayed response (where some patients’ observations are
either totally or partially unknown at the time of analysis).
In phase III and beyond, the need for adaptivity may be reduced but
ethical treatment of the patients and eﬃcient use of their data requires as
much ﬂexibility as possible. For an adaptive trial featuring all of the afore-
mentioned complications including delayed response, see Subsection 5.2.3.
Indeed, a particular trial might start with multiple doses of a particular
drug, and with the intention that it consist of two consecutive phases: the
ﬁrst to determine the appropriate dose, and the second to compare its eﬃ-
cacy to a reference standard. Such seamless phase II-III trials are adaptive
in a variety of ways. For one thing, a decision may be made to abandon
the drug at any time, possibly eliminating phase III entirely. This type of
conﬁrmatory trial is sometimes referred to as a “learn and conﬁrm trial”;
see Section 5.7.
Finally, in some settings the need for adaptivity outstrips even the above
designs’ abilities to adapt the dose, randomization fraction, total sample

8
STATISTICAL APPROACHES FOR CLINICAL TRIALS
size, number of arms, and so on. Here we are imagining settings where a
speciﬁc decision must be made upon the trial’s conclusion. Of course, every
clinical trial is run so that a decision (say, the choice of best treatment)
may be made, and so in this sense the ﬁeld of statistical decision theory
would appear to have much to oﬀer. But to do this, we must agree on the
unit of analysis (say, research dollars, or patient quality-adjusted life years
(QALYs)), as well as the cost-beneﬁt function we wish to consider. For
instance, we may wish to choose the treatment that maximizes the QALYs
saved subject to some ﬁxed cost per patient, where this can be agreed upon
via a combination of economic and moral grounds. An immediate complica-
tion here is the question of whose lives we are valuing: just those enrolled in
the trial, or those of every potential recipient of the study treatment. Still,
in settings where these ground rules can be established, Bayesian decision
theoretic approaches seem very natural. Inference for sequentially arriving
data can be complex, since at every stage a decision must be made whether
to enroll more patients (thus incurring their ﬁnancial and ethical costs), or
to stop the trial and make a decision. Sadly, the backward induction method
needed to solve such a problem in full generality is complex, but feasible
given appropriate computing methods and equipment (see e.g. Carlin et al.,
1998; Brockwell and Kadane, 2003). In some settings, relatively straightfor-
ward algorithms and code are possible; the case of constructing screening
designs for drug development (see Subsection 4.6.2) oﬀers an example.
Throughout the book we will attempt to be clear on just what aspect(s)
of the trial are being adapted, and how they diﬀer from each other. This
task is larger than it might have initially seemed, since virtually every trial
we advocate is adaptive in some way.
1.4 Features and use of the Bayesian adaptive approach
Researchers at the University of Texas M.D. Anderson Cancer Center are
increasingly applying Bayesian statistical methods in laboratory experi-
ments and clinical trials. More than 200 trials at M.D. Anderson have been
designed from the Bayesian perspective (Biswas et al., 2009). In addition,
the pharmaceutical and medical device industries are increasingly using
the Bayesian approach. Many applications in all these settings use adap-
tive methods, which will be a primary focus of this text. The remainder
of this section outlines several features that make the Bayesian approach
attractive for clinical trial design and analysis.
1.4.1 The fully Bayesian approach
There are two overarching strategies for implementing Bayesian statistics
in drug and medical device development: a fully Bayesian approach, and a
hybrid approach that uses Bayes’ rule as a tool to expand the frequentist

FEATURES AND USE OF THE BAYESIAN ADAPTIVE APPROACH
9
envelope. Choosing the appropriate approach depends on the context in
which it will be used. Is the context that of company decision making, or
does it involve only the design and analysis of registration studies? Phar-
maceutical company decisions involve questions such as whether to move
on to phase III (full-scale evaluation of eﬃcacy), and if so, how many doses
and which doses to include, whether to incorporate a pilot aspect of phase
III, how many phase III trials should be conducted, and how many centers
should be involved. Other decision-oriented examples are easy to imagine.
An investment capitalist might wonder whether or not to fund a particular
trial. A small biotechnology company might need to decide whether to sell
itself to a larger ﬁrm that has the resources to run a bigger trial.
These questions suggest a decision analysis using what we call a fully
Bayesian approach, using the likelihood function, the prior distribution,
and a utility structure to arrive at a decision. The prior distribution sum-
marizes available information on the model parameters before the data are
observed; it is combined with the likelihood using Bayes’ Theorem (2.1) to
obtain the posterior distribution. A utility function assigns numerical val-
ues to the various gains and losses that would obtain for various true states
of nature (i.e., the unknown parameters). It is equivalent to a loss func-
tion, and essentially determines how to weigh outcomes and procedures.
Bayesian statistical decision theory suggests choosing procedures that have
high utility (low loss) when averaged with respect to the posterior.
Fully Bayesian analysis is the kind envisioned by the great masters De-
Finetti (reprinted 1992), Savage (1972), and Lindley (1972), and continues
to be popular in business contexts, where there is often a lone decision-
maker whose prior opinions and utility function can be reliably assessed.
In a drug or device evaluation, a decisionmaker may initially prefer a cer-
tain action a. After assessing the decisionmaker’s prior distribution and
utilities, we may discover that the optimal action is in fact b, perhaps by
quite a margin. This can then lead to an exploration of what changes to
the prior and utility structure are required in order for a to actually emerge
as optimal. Such a process can be quite revealing to the decisionmaker!
Still, in the everyday practice of clinical trials, the fully Bayesian ap-
proach can be awkward. First, except in the case of internal, company-
sponsored trials, there are often multiple decisionmakers, all of whom arrive
at the trial with their own prior opinions and tolerances for risk. Second,
when data arrive sequentially over time (as they typically do in clinical tri-
als), calculations in the fully Bayesian vein require a complex bookkeeping
system known as backward induction, in which the decision as to whether
to stop or continue the trial at each monitoring point must account for both
the informational value of the next observations, and the cost of obtaining
them (though again, see Carlin et al., 1998, and Brockwell and Kadane,
2003 for approaches that avoid backward induction in a class of clinical
trials). Third, the process of eliciting costs and beneﬁts can be a diﬃcult

10
STATISTICAL APPROACHES FOR CLINICAL TRIALS
process, even for seasoned experts trained in probabilistic thinking. More-
over, the appropriate scales for the losses (monetary units, patient lives,
etc.) are often diﬃcult to work with and lead to decision rules that seem
somewhat arbitrary.
For these and other reasons, fully Bayesian approaches have largely failed
to gain a foothold in regulatory and other later-phase clinical trial settings.
As such, with the notable exception of Sections 4.6 and 6.4.2, we will mostly
focus on the less controversial and easier-to-implement “probability only”
approach, where we use Bayesian techniques to summarize all available
information, but do not take the further step of specifying utility functions.
1.4.2 Bayes as a frequentist tool
In the context of designing and analyzing registration studies, the Bayesian
approach can be a tool to build good frequentist designs. For example,
we can use the Bayesian paradigm to build a clinical trial that requires a
smaller expected sample size regardless of the actual parameter values. The
design may be complicated, but we can always ﬁnd its frequentist operating
characteristics using simulation. In particular, we can ensure that the false-
positive rate is within the range acceptable to regulatory agencies.
Bayesian methods support sequential learning, allowing updating one’s
posterior probability as the data accrue. They also allow for ﬁnding predic-
tive distributions of future results, and enable borrowing of strength across
studies. Regarding the ﬁrst of these, we make an observation, update the
probability distributions of the various parameters, make another obser-
vation, update the distributions again, and so on. At any point we can
ask which observation we want to make next; e.g., which dose we want to
use for the next patient. Finding predictive distributions (the probabili-
ties that the next set of observations will be of a speciﬁc type) is uniquely
Bayesian. Frequentist methods allow for calculations that are conditional
on particular values of parameters, so they are able to address the question
of prediction only in a limited sense. In particular, frequentist predictive
probabilities that change as the available data change are not possible.
The Bayesian paradigm allows for using historical information and results
of other trials, whether they involve the same drug, similar drugs, or pos-
sibly the same drug but with diﬀerent patient populations. The Bayesian
approach is ideal for borrowing strength across patient and disease groups
within the same trial and across trials. Still, we caution that historical infor-
mation typically cannot simply be regarded as exchangeable with current
information; see Section 6.1.
Some trials that are proposed by pharmaceutical and device companies
are deﬁcient in ways that can be improved by taking a Bayesian approach.
For example, a company may regard its drug to be most appropriate for a
particular disease, but be unsure just which subtypes of the disease will be

FEATURES AND USE OF THE BAYESIAN ADAPTIVE APPROACH
11
most responsive. So they propose separate trials for the diﬀerent subtypes.
To be speciﬁc, consider advanced ovarian cancer, a particularly diﬃcult
disease to achieve tumor responses. In exploring the possible eﬀects of its
drug, suppose a company was trying to detect a tumor response rate of
10%. It proposed to treat 30 patients in one group and 30 patients in
the complementary group, but to run two separate trials. All 60 patients
would be accrued with the goal of achieving at least one tumor response.
Suppose there were 0 responses out of the 30 patients accrued in Trial 1
and 0 responses out of 25 patients accrued so far in Trial 2. By design, they
would still add 5 more patients in Trial 2. But this would be folly, since
so far, we would have learned two things: ﬁrst, the drug is not very active,
and second, the two patient subgroups respond similarly. It makes sense to
incorporate what has been learned from Trial 1 into Trial 2. A Bayesian
hierarchical modeling analysis (see Section 2.4) would enable this, and a
reasonable such analysis would show that with high probability it is futile
(and ethically questionable) to add the remaining 5 patients in Trial 2.
Bayesian designs incorporate sequential learning whenever logistically
possible, use predictive probabilities of future results, and borrow strength
across studies and patient subgroups. These three Bayesian characteris-
tics have implications for analysis as well as for design. All three involve
modeling in building likelihood functions.
Bayesian goals include faster learning via more eﬃcient designs of trials
and more eﬃcient drug and medical device development, while at the same
time providing better treatment of patients who participate in clinical tri-
als. In our experience, physician researchers and patients are particularly
attracted by Bayesian trial designs’ potential to provide eﬀective care while
not sacriﬁcing scientiﬁc integrity.
Traditional drug development is slow, in part because of several char-
acteristics of conventional clinical trials. Such trials usually have inﬂexible
designs, focus on single therapeutic strategies, are partitioned into discrete
phases, restrict to early endpoints in early phases but employ diﬀerent
long-term endpoints in later phases, and restrict statistical inferences to
information in the current trial. The rigidity of the traditional approach
inhibits progress, and can often lead to clinical trials that are too large
or too small. The adaptivity of the Bayesian approach allows for deter-
mining a trial’s sample size while it is in progress. For example, suppose
a pharmaceutical company runs a trial with a predetermined sample size
and balanced randomization to several doses to learn the appropriate dose
for its drug. That is like saying to a student, “Study statistics for N hours
and you will be a statistician.” Perhaps the student will become a statisti-
cian long before N. Or there may be no N for which this particular student
could become a statistician. The traditional approach is to pretend that the
right dose for an experimental drug is known after completing the canoni-

12
STATISTICAL APPROACHES FOR CLINICAL TRIALS
cal clinical trial(s) designed to answer that question. More realistically, we
never “know” the right dose.
A clinical trial should be like life: experiment until you achieve your ob-
jective, or until you learn that your objective is not worth pursuing. Better
methods for drug and device development are based on decision analyses,
ﬂexible designs, assessing multiple experimental therapies, using seamless
trial phases, modeling the relationships among early and late endpoints,
and synthesizing the available information. Flexible designs allow the data
that are accruing to guide the trial, including determining when to stop or
extend accrual.
We advocate broadening the range of possibilities for learning in the
early phases of drug and device development. For example, we might use
multiple experimental oncology drugs in a single trial. If we are going to
defeat cancer with drugs, it is likely to be with selections from lists of
many drugs and their combinations, not with any single drug. We will
also have to learn in clinical trials which patients (based on clinical and
biological characteristics) beneﬁt from which combinations of drugs. So
we need to be able to study many drugs in clinical trials. We might use,
say, 100 drugs in a partial factorial fashion, while running longitudinal
genomic and proteomic experiments. The goal would be to determine the
characteristics of the patients who respond to the various combinations of
drugs – perhaps an average of 10 drugs per patient – and then to validate
these observations in the same trial. We cannot learn about the potential
beneﬁts of combinations of therapies unless we use them in clinical trials.
Considering only one experimental drug at a time in clinical trials is an
ineﬃcient way to make therapeutic advances.
Regarding the process of learning, in the Bayesian paradigm it is nat-
ural to move beyond the notion of discrete phases of drug development.
An approach that is consistent with the Bayesian paradigm is to view
drug development as a continuous process. For example, seamless trials
allow for moving from one phase of development to the next without stop-
ping patient accrual. Another possibility is allowing for the possibility of
ramping up accrual if the accumulating data warrant it. Modeling relation-
ships among clinical and early endpoints will enable early decisionmaking
in trials, increasing their eﬃciency. Synthesizing the available information
involves using data from related trials, from historical databases, and from
other, related diseases, such as other types of cancer.
1.4.3 Examples of the Bayesian approach to drug and medical device
development
Here we oﬀer some case studies to illustrate the Bayesian design charac-
teristics of predictive probabilities, adaptive randomization, and seamless
phase II/III trials.

FEATURES AND USE OF THE BAYESIAN ADAPTIVE APPROACH
13
Predictive probability
Predictive probability plays a critical role in the design of a trial and also
in monitoring trials. For example, conditioning on what is known about
patient covariates and outcomes at any time during a trial allows for ﬁnd-
ing the probability of achieving statistical signiﬁcance at the end of the
trial. If that probability is suﬃciently small, the researchers may deem
that continuing is futile and decide to end the trial. Assessing such pre-
dictive probabilities is especially appropriate for data safety monitoring
boards (DSMBs) quite apart from the protocol, but it is something that
can and should be explicitly incorporated into the design of a trial.
A drug trial at M.D. Anderson for patients with HER2-positive neoad-
juvant breast cancer serves as an example of using predictive probability
while monitoring a trial (Buzdar et al., 2005). The original design called for
balanced randomization of 164 patients to receive standard chemotherapy
either in combination with the drug trastuzumab or not (controls). The
endpoint was pathologic complete tumor response (pCR). The protocol
speciﬁed no interim analyses. At one of its regular meetings, the insti-
tution’s DSMB considered the results after the outcomes of 34 patients
were available. Among 16 control patients there were 4 (25%) pCRs. Of
18 patients receiving trastuzumab, there were 12 (67%) pCRs. The DSMB
calculated the predictive probability of statistical signiﬁcance if the trial
were to continue to randomize and treat the targeted sample size of 164
patients, which turned out to be 95%. They also considered that the trial’s
accrual rate had dropped to less than 2 patients per month. They stopped
the trial and made the results available to the research and clinical com-
munities. This was many years sooner than if the trial had continued to
the targeted sample size of 164. The researchers presented the trial results
at the next annual meeting of the American Society of Clinical Oncology.
That presentation and the related publication had an important impact on
clinical practice, as well as on subsequent research. See Sections 2.5.1, 4.2,
and 5.2 for much more detail on predictive probability methods.
Adaptive randomization and early stopping for futility
An M.D. Anderson trial in the treatment of acute myelogenous leukemia
(AML) serves as an example of adaptive randomization (Giles et al., 2003).
That trial compared the experimental drug troxacitabine to the institu-
tion’s standard therapy for AML, which was idarubicin in combination
with cytarabine, also known as ara-C. It compared three treatment strate-
gies: idarubicin plus ara-C (IA), troxacitabine plus ara-C (TA), and troxac-
itabine plus idarubicin (TI). The maximum trial size was set in advance at
75. The endpoint was complete remission (CR); early CR is important in
AML. The trialists modeled time to CR within the ﬁrst 50 days. The study
design called for randomizing based on the currently available trial results.

14
STATISTICAL APPROACHES FOR CLINICAL TRIALS
In particular, when a patient entered the trial they calculated the proba-
bilities that TI and TA were better than IA, and the probability that TA
was better than TI, and used those current probabilities to assign the pa-
tient’s therapy. If one of the treatment arms performed suﬃciently poorly,
its assignment probability would decrease, with better performing thera-
pies getting higher probabilities. An arm doing suﬃciently poorly would be
dropped.
In the actual trial, the TI arm was dropped after 24 patients. Arm TA
was dropped (and the trial ended) after 34 patients, with these ﬁnal results
for CR within 50 days: 10 of 18 patients receiving IA (56%, a rate consistent
with historical results); 3 of 11 patients on TA (27%) and 0 of 5 patients
on TI (0%).
These results and the design used have been controversial. Some cancer
researchers feel that having 0 successes out of only 5 patients is not reason
enough to abandon a treatment. For some settings we would agree, but not
when there is an alternative that produces on the order of 56% complete
remissions. In view of the trial results, the Bayesian probability that either
TA or TI is better than IA is small. Moreover, if either has a CR rate that
is greater than that of IA, it is not much greater.
The principal investigator of this trial, Dr. Francis Giles, MD, was quoted
in Cure magazine (McCarthy, 2009) as follows:
“I see no rationale to further delay moving to these designs,” says Dr. Giles,
who is currently involved in eight Bayesian-based leukemia studies. “They
are more ethical, more patient-friendly, more conserving of resources, more
statistically desirable. I think the next big issue is to get the FDA to accept
them as the basis for new drug approvals.”
Adaptive randomization: screening phase II cancer agents
The traditional approach in drug development is to study one drug at a
time. Direct comparisons of experimental drugs with either standard thera-
pies or other experimental drugs are unusual in early phases; combinations
of experimental drugs are often frowned upon. Focusing on one drug means
that hundreds of others are waiting their turns in the research queue. Sim-
ply because of its size, the queue is likely to contain better drugs than the
one now being studied. A better approach is to investigate many drugs and
their combinations at the same time. One might screen drugs in phase II in
a fashion similar to screening in a preclinical setting. The goal is to learn
about safety and eﬃcacy of the candidate drugs as rapidly as possible. An-
other goal is to treat patients eﬀectively, promising them in the informed
consent process that if a therapy is performing better, then they are more
likely to receive it.
Consider a one-drug-at-a-time example in phase II cancer trials. Suppose
the historical tumor response rate is 20%. A standard design for a clinical

FEATURES AND USE OF THE BAYESIAN ADAPTIVE APPROACH
15
trial has two stages. The ﬁrst stage consists of 20 patients. The trial ends
after the ﬁrst stage if 4 or fewer tumor responses are observed, and also
if 9 or more tumor responses are observed. Otherwise, we proceed to the
second stage of another 20 patients. A positive result moves the drug into
phase III, or to some intermediate phase of further investigation. Progress
is slow.
Now consider an alternative adaptive design with many drugs and drug
combinations. We assign patients to a treatment in proportion to the prob-
ability that its response rate is greater than 20%:
r = P(rate > 20% | current data) .
We add drugs as they become available, and drop them if their probability
of having a response rate greater than 20% is not very high. Drugs that
have suﬃciently large r move on to phase III.
As an illustration, consider 10 experimental drugs with a total sample
size of 200 patients: 9 of the drugs have a mix of response rates 20% and
40%, and one is a “nugget,” a drug with a 60% response rate. The standard
trial design ﬁnds the nugget with probability less than 0.70. This is because
the nugget may not be among the ﬁrst seven or so drugs in the queue, and
that is all that can be investigated in 200 patients. On the other hand, the
adaptive design has better than a 0.99 probability of ﬁnding the nugget.
That is because all drugs have some chance of being used early in the
trial. Randomizing according to the results means that the high probability
of observing a response when using the nugget boosts its probability of
being assigned to later patients. So we identify the nugget with very high
probability and we ﬁnd the nugget much sooner: after 50 of 200 patients
for an adaptive design, as opposed to 110 of the 200 in the standard design
(conditioning on ﬁnding it at all). Adaptive randomization is also a better
method for ﬁnding the drugs that have response rates of 40%.
If we have many more drugs (say, 100) and proportionally more patients
(say, 2000), then the relative comparisons are unchanged from the earlier
case. We ﬁnd the 1-in-100 nugget drug essentially with certainty, and we
ﬁnd it much more quickly using adaptive randomization. The consequences
of using adaptive randomization are that we treat patients in the trial more
eﬀectively, we learn more quickly, and we are also able to identify the better
drug sooner, which allows it to move through the process more rapidly.
Beneﬁts accrue to both the patient and the drug developer.
These comparisons apply qualitatively for other endpoints, such as pro-
gression-free survival, and when randomization includes a control therapy.
See Sections 4.4 and 5.2 for full details on adaptive randomization in phase
II and III trials.

16
STATISTICAL APPROACHES FOR CLINICAL TRIALS
Seamless phase II and III trial designs
Consider a trial for which there is pharmacologic or pathophysiologic infor-
mation about a patient’s outcomes. In such a trial, clinicians may require
biologic justiﬁcation of an early endpoint. If the early endpoint is to serve
as a surrogate for the clinical endpoint in the sense that it replaces the
clinical endpoint, then we agree. But early endpoints can be used whether
or not the biology is understood: all that is required is some evidence that
it may be correlated with the clinical endpoint. The possibility of such cor-
relation can be modeled statistically. If the data in the trial point to the
existence of correlation (depending on treatment), then the early endpoint
is implicitly exploited through the modeling process. If the data suggest a
lack of correlation, then the early endpoint plays no role, and little is lost
by having considered the possibility.
In one study, we modeled the possible correlation between the success of a
spinal implant at 12 months and at 24 months. We didn’t assume that those
endpoints were correlated, but instead let the data dictate the extent to
which the 12-month result was predictive of the 24-month endpoint. The
primary endpoint was success at 24 months. The earlier endpoint at 12
months was not a “surrogate endpoint,” but rather an auxiliary endpoint.
In another study, we modeled the possible relationship among scores on
a stroke scale at early time points, weeks 1 through 12, but the primary
endpoint was the week-13 score on the stroke scale. We did not employ
anything so crude as “last observation carried forward,” but instead built a
longitudinal model and updated the model as evidence about relationships
between endpoints accumulated in the trial.
An early endpoint in cancer trials is tumor response. Early information
from tumor response can be used to construct a seamless phase II/III trial.
In conventional cancer drug development, phase II addresses tumor re-
sponse. Suﬃcient activity in phase II leads to phase III, which is designed
to determine if the drug provides a survival advantage. A conventional
phase II process generally requires more than 18 months, after which phase
III generally requires at least another 2 years. In contrast, a comparably
powered seamless phase II/III trial with modeling the relationship between
tumor response and survival can take less than two years in total.
In a seamless trial, we start out with a small number of centers. We accrue
a modest number of patients per month, randomizing to experimental and
control arms. If the predictive probability of eventual success is suﬃciently
promising, we expand into phase III, and all the while, the initial centers
continue to accrue patients. It is especially important to use the “phase II”
data because the patients enrolled in the trial early have longer follow-up
time and thus provide the best information about survival.
Our seamless design involves frequent analyses and uses early stopping
determinations based on predictive probabilities of eventually achieving

FEATURES AND USE OF THE BAYESIAN ADAPTIVE APPROACH
17
statistical signiﬁcance. Speciﬁcally, we look at the data every month (or
even every week), and use predictive probabilities to determine when to
switch to phase III, to stop accrual for futility if the drug’s performance is
suﬃciently bad, or to stop for eﬃcacy if the drug is performing suﬃciently
well.
Inoue et al. (2002) compared the seamless design with more conventional
designs having the same operating characteristics (Type I error rate and
power) and found reductions in average sample size ranging from 30% to
50%, in both the null and alternative hypothesis cases. In addition, the
total time of the trial was similarly reduced. We return to this subject in
detail in Section 5.7.
Summary
The Bayesian method is by its nature more ﬂexible and adaptive, even
when the conduct of a study deviates from the original design. It is possi-
ble to incorporate all available information into the prior distribution for
designing a trial, while recognizing that regulators and other reviewers may
well have a diﬀerent prior. Indeed, they may not have a prior at all, but will
want to use statistical signiﬁcance in the ﬁnal analysis. The Bayesian ap-
proach addresses this with aplomb, since predictive probabilities can look
forward to a frequentist analysis when all the data become available.
We note that a deviation in the conduct of a study from the original de-
sign causes the frequentist properties to change, whereas Bayesian proper-
ties (which always condition on whatever data emerge) remain unchanged.
Bayesian methods are better able to handle complex hierarchical model
structures, such as random eﬀects models used in metaanalysis to borrow
strength across diﬀerent disease subgroups or similar treatments (see Ex-
ample 2.7). Bayesian methods also facilitate the development of innovative
trials such as seamless phase II/III trials and outcome-based adaptive ran-
domization designs (Inoue et al., 2002; Thall et al., 2003; Berry, 2005; Berry,
2006; Zhou et al., 2008). In the next chapter we develop and illustrate the
requisite Bayesian machinery, before proceeding on to its use in speciﬁc
phase I-III trials in Chapters 3–5, respectively.

CHAPTER 2
Basics of Bayesian inference
In this chapter we provide a brief overview of hierarchical Bayesian mod-
eling and computing for readers not already familiar with these topics. Of
course, in one chapter we can only scratch the surface of this rapidly ex-
panding ﬁeld, and readers may well wish to consult one of the many recent
textbooks on the subject, either as preliminary work or on an as-needed
basis. By contrast, readers already familiar with the basics of Bayesian
methods and computing may wish to skip ahead to Section 2.5, where we
outline the principles of Bayesian clinical trial design and analysis.
It should come as little surprise that the Bayesian book we most highly
recommend is the one by Carlin and Louis (2009); the Bayesian method-
ology and computing material below roughly follows Chapters 2 and 3, re-
spectively, in that text. However, a great many other good Bayesian books
are available, and we list a few of them and their characteristics. First we
must mention texts stressing Bayesian theory, including DeGroot (1970),
Berger (1985), Bernardo and Smith (1994), and Robert (2001). These books
tend to focus on foundations and decision theory, rather than computation
or data analysis. On the more methodological side, a nice introductory
book is that of Lee (1997), with O’Hagan and Forster (2004) and Gelman,
Carlin, Stern, and Rubin (2004) oﬀering more general Bayesian modeling
treatments.
2.1 Introduction to Bayes’ Theorem
As discussed in Chapter 1, by modeling both the observed data and any un-
knowns as random variables, the Bayesian approach to statistical analysis
provides a cohesive framework for combining complex data models with ex-
ternal knowledge, expert opinion, or both. We now introduce the technical
details of the Bayesian approach.
In addition to specifying the distributional model f(y|θ) for the ob-
served data y = (y1, . . . , yn) given a vector of unknown parameters θ =
(θ1, . . . , θk), suppose that θ is a random quantity sampled from a prior
distribution π(θ|λ), where λ is a vector of hyperparameters. For instance,
yi might be the empirical drug response rate in a sample of women aged 40

20
BASICS OF BAYESIAN INFERENCE
and over from clinical center i, θi the underlying true response rate for all
such women in this center, and λ a parameter controlling how these true
rates vary across centers. If λ is known, inference concerning θ is based on
its posterior distribution,
p(θ|y, λ) = p(y, θ|λ)
p(y|λ)
=
p(y, θ|λ)
R
p(y, θ|λ) dθ =
f(y|θ)π(θ|λ)
R
f(y|θ)π(θ|λ) dθ .
(2.1)
Notice the contribution of both the data (in the form of the likelihood f)
and the previous knowledge or expert opinion (in the form of the prior π)
to the posterior. Since, in practice, λ will not be known, a second stage
(or hyperprior) distribution h(λ) will often be required, and (2.1) will be
replaced with
p(θ|y) = p(y, θ)
p(y)
=
R
f(y|θ)π(θ|λ)h(λ) dλ
R R
f(y|θ)π(θ|λ)h(λ) dθdλ .
This multi-stage approach is often called hierarchical modeling, a subject
to which we return in Section 2.4. Alternatively, we might replace λ by
an estimate ˆλ obtained as the maximizer of the marginal distribution
p(y|λ) =
R
f(y|θ)π(θ|λ)dθ, viewed as a function of λ. Inference could
then proceed based on the estimated posterior distribution p(θ|y, ˆλ), ob-
tained by plugging ˆλ into equation (2.1). This approach is referred to as
empirical Bayes analysis; see Carlin and Louis (2009, Chapter 5) for details
regarding empirical Bayes methodology and applications.
The Bayesian inferential paradigm oﬀers attractive advantages over the
classical, frequentist statistical approach through its more philosophically
sound foundation, its uniﬁed approach to data analysis, and its ability to
formally incorporate prior opinion or external empirical evidence into the
results via the prior distribution π. Modeling the θi as random (instead
of ﬁxed) eﬀects allows us to induce speciﬁc correlation structures among
them, hence among the observations yi as well.
A computational challenge in applying Bayesian methods is that for most
realistic problems, the integrations required to do inference under (2.1) are
often not tractable in closed form, and thus must be approximated nu-
merically. Forms for π and h (called conjugate priors) that enable at least
partial analytic evaluation of these integrals may often be found, but in
the presense of nuisance parameters (typically unknown variances), some
intractable integrations remain. Here the emergence of inexpensive, high-
speed computing equipment and software comes to the rescue, enabling the
application of recently developed Markov chain Monte Carlo (MCMC) in-
tegration methods, such as the Metropolis-Hastings algorithm (Metropolis
et al., 1953; Hastings, 1970) and the Gibbs sampler (Geman and Geman,
1984; Gelfand and Smith, 1990). Details of these algorithms will be pre-
sented in Section 2.3.

INTRODUCTION TO BAYES’ THEOREM
21
Illustrations of Bayes’ Theorem
Equation (2.1) is a generic version of what is referred to as Bayes’ Theorem
or Bayes’ Rule. It is attributed to Reverend Thomas Bayes, an 18th-century
nonconformist minister and part-time mathematician; a version of the re-
sult was published (posthumously) in Bayes (1763). In this subsection we
consider a few basic examples of its use.
Example 2.1 (basic normal/normal model). Suppose we have observed a
single normal (Gaussian) observation Y ∼N
¡
θ, σ2¢
with σ2 known, so that
the likelihood f (y|θ) = N
¡
y|θ, σ2¢
≡
1
σ
√
2π exp(−(y−θ)2
2σ2 ), y ∈ℜ, θ ∈ℜ,
and σ > 0. If we specify the prior distribution as π (θ) = N
³
θ µ, τ 2´
with
λ = (µ, τ 2)′ ﬁxed, then from (2.1) we can compute the posterior as
p (θ|y)
=
N
¡
θ|µ, τ 2¢
N
¡
y|θ, σ2¢
p (y)
∝
N
¡
θ|µ, τ 2¢
N
¡
y|θ, σ2¢
=
N
µ
θ
σ2
σ2 + τ 2 µ +
τ 2
σ2 + τ 2 y ,
σ2τ 2
σ2 + τ 2
¶
.
(2.2)
That is, the posterior distribution of θ given y is also normal with mean and
variance as given. The proportionality in the second row arises since the
marginal distribution p(y) does not depend on θ, and is thus constant with
respect to the Bayes’ Theorem calculation. The ﬁnal equality in the third
row results from collecting like (θ2 and θ) terms in the two exponential
components of the previous line, and then completing the square.
Note that the posterior mean E(θ|y) is a weighted average of the prior
mean µ and the data value y, with the weights depending on our relative
uncertainty with respect to the prior and the likelihood. Also, the posterior
precision (reciprocal of the variance) is equal to 1/σ2 + 1/τ 2, which is the
sum of the likelihood and prior precisions. Thus, thinking of precision as
“information,” we see that in the normal/normal model, the information in
the posterior is the total of the information in the prior and the likelihood.
Suppose next that instead of a single datum we have a set of n ob-
servations y = (y1, y2, . . . , yn)′. From basic normal theory we know that
f(¯y|θ) = N(θ, σ2/n). Since y is suﬃcient for θ, from (2.2) we have
p(θ|y) = p (θ|¯y) = N
µ
θ
(σ2/n)
(σ2/n) + τ 2 µ +
τ 2
(σ2/n) + τ 2 y ,
(σ2/n)τ 2
(σ2/n) + τ 2
¶
= N
µ
θ
σ2
σ2 + nτ 2 µ +
nτ 2
σ2 + nτ 2 y ,
σ2τ 2
σ2 + nτ 2
¶
.
Again we obtain a posterior mean that is a weighted average of the prior
(µ) and data-supported (¯y) values.

22
BASICS OF BAYESIAN INFERENCE
−4
−2
0
2
4
0.0
0.1
0.2
0.3
0.4
0.5
θ
density
posterior
prior
likelihood
Figure 2.1 Graphical version of Bayes’ Rule in the normal/normal example. The
vertical line marks the posterior mean, 1.31.
Example 2.2 (normal/normal model applied to a simple eﬃcacy trial).
Recall that in Example 1.1, a device is used to treat 100 patients and a
particular outcome measurement is made on each. The average ¯y of the
100 measurements is 1.96 and the standard deviation σ is 10. Suppose the
prior distribution is normal with mean 0 and variance 2 (standard deviation
√
2). This prior density and the likelihood function of θ (taken from above)
are shown in Figure 2.1 as dashed and dotted lines, respectively. As seen in
Example 2.1, the posterior density by Bayes’ Theorem is the product of the
prior and likelihood, restandardized to integrate to 1. This (also normal)
posterior density is shown in Figure 2.1 as a solid line. For ease of com-
parison, the three curves are shown as having the same area, although the
area under the likelihood function is irrelevant since it is not a probability
density in θ. Note the location of the posterior is a compromise between
that of the prior and the likelihood, and it is also more concentrated than
either of these two building blocks, since it reﬂects more information, i.e.,
the total information in both the prior and the data.
As seen in the previous example, there is a general formula for the pos-
terior distribution of θ when both the sampling distribution and the prior
distribution are normal. Figure 2.1 is representative of the typical case in
that the posterior distribution is more concentrated than both the prior

INTRODUCTION TO BAYES’ THEOREM
23
distribution and the likelihood. Also, the posterior mean is always between
the prior mean and the maximum likelihood estimate. Suppose again the
mean of the prior distribution for θ is µ and its variance is τ 2 = 1/h0;
h0 is the precision. If the sample size and population standard deviation
are again n and σ, then the sample precision hs = n/σ2. Since precisions
add in this normal model, the posterior precision is hpost = h0 + hs. The
posterior mean, E(θ|¯y), is a weighted average of the prior mean and sample
mean (called shrinkage), with the weights proportional to the precisions:
E(θ|¯y) =
h0
h0 + hs
µ +
hs
h0 + hs
¯y = h0µ + hs¯y
h0 + hs
.
In our case, we have µ = 0, h0 = 1/2, ¯y = 1.96, σ = 10, n = 100, hs =
100/(102) = 1, hpost = h0+hs = 3/2 (so the posterior standard deviation is
p
2/3 = 0.816), and E(θ|¯y) = 1.96/(3/2) = 1.31, as indicated in Figure 2.1.
The sample is twice as informative as the prior in this example, in the
sense that hs = 2h0. Relative to the experiment in question, the prior
information is worth the same as 50 observations (with the mean of these
hypothetical observations being 0). In general, hs/h0 is proportional to n,
and so for suﬃciently large sample size, the sample information overwhelms
the prior information. While this fact is comforting, the limiting case, n →
∞, is not very interesting. Usually, unknown parameters become known in
the limit and there is no need for statistics when there is no uncertainty.
In practice, sampling has costs and there is a trade-oﬀbetween increasing
n and making an unwise decision based on insuﬃcient information about
a parameter. When the sample size is small or moderate, the ability to
exploit prior information in a formal way is an important advantage of the
Bayesian approach.
The posterior distribution of the parameters of interest is the culmina-
tion of the Bayesian approach. With the posterior distribution in hand,
probabilities of hypotheses can be calculated, decisions can be evaluated,
and predictive probabilities can be derived. As an example of the ﬁrst of
these, consider the hypothesis θ > 0. Because our posterior distribution is
normal with mean 1.31 and standard deviation 0.816, the probability of
this hypothesis is 0.945, which is the area of the shaded region under the
curve shown in Figure 2.2.
In these two examples, the prior chosen leads to a posterior distribution
for θ that is available in closed form, and is a member of the same distribu-
tional family as the prior. Such a prior is referred to as a conjugate prior.
We will often use such priors in our work, since, when they are available,
conjugate families are convenient and still allow a variety of shapes wide
enough to capture our prior beliefs.
Note that setting τ 2 = ∞in the previous examples corresponds to a
prior that is arbitrarily vague, or noninformative. This then leads to a
posterior of p (θ|y) = N
¡
θ|y, σ2/n
¢
, exactly the same as the likelihood for

24
BASICS OF BAYESIAN INFERENCE
−4
−2
0
2
4
0.0
0.1
0.2
0.3
0.4
0.5
θ
density
Figure 2.2 Shaded area is the posterior probability that θ is positive.
this problem. This arises since the limit of the conjugate (normal) prior
here is actually a uniform, or “ﬂat” prior, and thus the posterior is nothing
but the likelihood, possibly renormalized so it integrates to 1 as a function
of θ. Of course, the ﬂat prior is improper here, since the uniform does not
integrate to anything ﬁnite over the entire real line. However, the posterior
is still well deﬁned since the likelihood can be integrated with respect to θ.
Bayesians use ﬂat or otherwise improper noninformative priors in situations
where prior knowledge is vague relative to the information in the likelihood,
or in settings where we want the data (and not the prior) to dominate the
determination of the posterior.
Example 2.3 (normal/normal model with unknown sampling variance).
Consider the extension of the normal/normal model in Examples 2.1 and
2.2 to the more realistic case where the sample variance σ2 is unknown.
Transforming again to the precision h = 1/σ2, it turns out the gamma
distribution oﬀers a conjugate prior. To see this, let h have a Gamma(α, β)
prior with pdf
p(h) =
βα
Γ(α)hα−1e−hβ, h > 0 .
(2.3)

INTRODUCTION TO BAYES’ THEOREM
25
Since the likelihood for any one observation yi is still
f(yi|θ, h) = h1/2
√
2π e−h
2 (yi−θ)2 ,
the posterior of h is proportional to the product of the full likelihood and
the prior,
p(h|y, θ)
∝
" n
Y
i=1
f(yi|θ, h)
#
× p(h)
∝
hn/2e−h
2
Pn
i=1(yi−θ)2 × hα−1e−hβ
∝
hn/2+α−1e−h[β+ 1
2
Pn
i=1(yi−θ)2] ,
where in all three steps we have absorbed any multiplicative terms that do
not involve h into the unknown normalizing constant. Looking again at the
form of the gamma prior in (2.3), we recognize this form as proportional
to another gamma distribution, namely a
Gamma
Ã
n/2 + α , β + 1
2
n
X
i=1
(yi −θ)2
!
.
(2.4)
Thus the posterior for h is available via conjugacy.
Note that (2.4) is only a conditional posterior distribution, since it de-
pends on the mean parameter θ, which is itself unknown. However, the
conditional posterior for θ, p(θ|y, h), is exactly the same as that previously
found in Example 2.2, since the steps we went through then to get p(θ|y) are
exactly those we would go through now; in both calculations, h is assumed
ﬁxed and known. Armed with these two full conditional distributions, it
turns out to be easy to obtain Monte Carlo samples from the joint pos-
terior p(θ, h|y), and hence the two marginal posteriors p(θ|y) and p(h|y),
using the Gibbs sampler; we return to this subject in Subsection 2.3.1.
Finally, regarding the precise choice of α and β, many authors (and even
the WinBUGS software manual) use α = β = ϵ for some small positive
constant ϵ as a sort of “default” setting. This prior has mean α/β = 1
but variance α/β2 = 1/ϵ, making it progressively more diﬀuse as ϵ →0. It
is also a “minimally informative” prior in the sense that choosing a very
small ϵ will have minimal impact on the full conditional in (2.4), forcing the
data and θ to provide virtually all the input to this distribution. However,
this prior becomes improper as ϵ →0, and its shape also becomes more
and more spiked, with an inﬁnite peak at 0 and a very heavy right tail (to
create the larger and larger variance). Gelman (2006) suggests placing a
uniform prior on σ, simply bounding the prior away from 0 and ∞in some
sensible way – say, via a Uniform(ϵ, 1/ϵ). We will experiment with both
of these priors in subsequent examples.
As a side comment, Spiegelhalter et al. (2004) recommend a Jeﬀreys

26
BASICS OF BAYESIAN INFERENCE
noninformative prior (see e.g. Carlin and Louis, 2009, Sec. 2.2.3) for the
sampling standard deviation σ, i.e., π(σ) = 1/σ. This is in some sense a
limiting version of the conjugate gamma prior above. However, Spiegelhal-
ter et al. (2004) express a preference for the Gelman prior for standard
deviations arising in random eﬀects distributions.
2.2 Bayesian inference
While the computing associated with Bayesian methods can be daunting,
the subsequent inference is relatively straightforward, especially in the case
of estimation. This is because once we have computed (or obtained an
estimate of) the posterior, inference comes down merely to summarizing
this distribution, since by Bayes’ Rule the posterior summarizes everything
we know about the model parameters in light of the data. In the remainder
of this section, we shall assume for simplicity that the posterior p(θ|y) itself
(and not merely an estimate of it) is available for summarization.
Bayesian methods for estimation are also reminiscent of corresponding
maximum likelihood methods. This should not be surprising, since likeli-
hoods form an important part of the Bayesian calculation; we have even
seen that a normalized (i.e., standardized) likelihood can be thought of
a posterior when this is possible. However, when we turn to hypothesis
testing, the approaches have little in common. Bayesians have a profound
dislike for p-values, for a long list of reasons we shall not go into here; the
interested reader may consult Berger (1985, Sec. 4.3.3), Kass and Raftery
(1995, Sec. 8.2), or Carlin and Louis (2009, Sec. 2.3.3).
2.2.1 Point estimation
To keep things simple, suppose for the moment that θ is univariate. Given
the posterior p(θ|y), a sensible Bayesian point estimate of θ would be some
measure of centrality. Three familiar choices are the posterior mean,
ˆθ = E(θ|y) ,
the posterior median,
ˆθ :
Z ˆθ
−∞
p(θ|y)dθ = 0.5 ,
and the posterior mode,
ˆθ : p(ˆθ|y) = sup
θ
p(θ|y) .
The lattermost estimate has historically been thought of as easiest to com-
pute, since it does not require any integration: we can replace p(θ|y) by
its unstandardized form, f(y|θ)p(θ), and get the same answer (since these

BAYESIAN INFERENCE
27
two diﬀer only by a multiplicative factor of the marginal distribution p(y),
which does not depend on θ). Existing code to ﬁnd maximum likelihood
estimates (MLEs) can be readily used with the product of the likelihood
and the prior (instead of the likelihood alone) to produce posterior modes.
Indeed, if the posterior exists under a ﬂat prior p(θ) = 1, then the posterior
mode is nothing but the MLE itself.
Note that for symmetric unimodal posteriors (e.g., a normal distribu-
tion), the posterior mean, median, and mode will all be equal. However,
for multimodal or otherwise nonnormal posteriors, the mode will often be
the poorest choice of centrality measure. Consider for example the case of
a steadily decreasing, one-tailed posterior: the mode will be the very ﬁrst
value in the support of the distribution — hardly central! By contrast, the
posterior mean will sometimes be overly inﬂuenced by heavy tails (just as
the sample mean ¯y is often nonrobust against outlying observations). As a
result, the posterior median will often be the best and safest point estimate.
In the days prior to MCMC integration, it was also the most diﬃcult to
compute, but this diﬃculty has now been mitigated; see Section 2.3.
2.2.2 Interval estimation
The posterior allows us to make direct probability statements about not
just its median, but any quantile. For example, suppose we can ﬁnd the
α/2- and (1 −α/2)-quantiles of p(θ|y), that is, the points qL and qU such
that
Z qL
−∞
p(θ|y)dθ = α/2 and
Z ∞
qU
p(θ|y)dθ = α/2 .
Then clearly P(qL < θ < qU|y) = 1 −α; our conﬁdence that θ lies in
(qL, qU) is 100 × (1 −α)%. Thus this interval is a 100 × (1 −α)% credible
set (or simply Bayesian conﬁdence interval) for θ. This interval is relatively
easy to compute, and enjoys a direct interpretation (“the probability that
θ lies in (qL, qU) is (1 −α)”) that the usual frequentist interval does not.
The interval just described is often called the equal tail credible set, for
the obvious reason that it is obtained by chopping an equal amount of
support (α/2) oﬀthe top and bottom of p(θ|y). Note that for symmetric
unimodal posteriors, this equal tail interval will be symmetric about this
mode (which we recall equals the mean and median in this case). It will
also be optimal in the sense that it will have shortest length among sets C
satisfying
1 −α ≤P(C|y) =
Z
C
p(θ|y)dθ .
(2.5)
Note that any such set C could be thought of as a 100 × (1 −α)% credible
set for θ. For posteriors that are not symmetric and unimodal, a better
(shorter) credible set can be obtained by taking only those values of θ
having posterior density greater than some cutoﬀk(α), where this cutoﬀis

28
BASICS OF BAYESIAN INFERENCE
chosen to be as large as possible while C still satisﬁes equation (2.5). This
highest posterior density (HPD) conﬁdence set will always be of optimal
length, but will typically be signiﬁcantly more diﬃcult to compute. The
equal tail interval emerges as HPD in the symmetric unimodal case since
there too it captures the “most likely” values of θ. Fortunately, many of
the posteriors we will be interested in will be (at least approximately)
symmetric unimodal, so the much simpler equal tail interval will often
suﬃce. This is due to the following theorem:
Theorem 2.1 (the “Bayesian Central Limit Theorem”). Suppose that the
data X1, . . . , Xn
iid
∼fi(xi|θ), and thus f(x|θ) = Qn
i=1 fi(xi|θ). Suppose the
prior π(θ) and f(x|θ) are positive and twice diﬀerentiable near ˆθ
π, the
posterior mode (or “generalized MLE”) of θ, assumed to exist. Then under
suitable regularity conditions, the posterior distribution p(θ|x) for large n
can be approximated by a normal distribution having mean equal to the
posterior mode, and covariance matrix equal to minus the inverse Hessian
(second derivative matrix) of the log posterior evaluated at the mode. This
matrix is sometimes notated as [Iπ(x)]−1, since it is the “generalized” ob-
served Fisher information matrix for θ. More speciﬁcally,
Iπ
ij(x) = −
·
∂2
∂θi∂θj
log (f(x|θ)π(θ))
¸
θ=ˆθ
π .
Other forms of the normal approximation are occasionally used. For in-
stance, if the prior is reasonably ﬂat, we might ignore it in the above cal-
culations. This in eﬀect replaces the posterior mode ˆθπ by the MLE ˆθ, and
the generalized observed Fisher information matrix by the usual observed
Fisher information matrix, bI(x), where
bIij(x)
=
−
·
∂2
∂θi∂θj
log f(x|θ)
¸
θ=bθ
=
−
n
X
l=1
·
∂2
∂θi∂θj
log f(xl|θ)
¸
θ=bθ
.
The moniker “Bayesian Central Limit Theorem” appears to come from
the fact that the theorem shows the posterior to be approximately normal
for large sample sizes, just as the “regular” Central Limit Theorem pro-
vides approximate normality for frequentist test statistics in large samples.
A general proof of the theorem requires only multivariate Taylor expan-
sions; an outline in the unidimensional case is provided by Carlin and Louis
(2009, p.109). The use of this theorem in Bayesian practice has diminished
in the past few years, due to concerns about the quality of the approxima-
tion combined with the increasing ease of exact solutions implemented via
MCMC. Still, the theorem provides a justiﬁcation for the use of equal-tail

BAYESIAN INFERENCE
29
credible intervals in most standard problems, and may also provide good
starting values for MCMC algorithms in challenging settings.
2.2.3 Hypothesis testing and model choice
Hypothesis testing is perhaps the bedrock statistical technique in the anal-
ysis of clinical trials, and dates back to the celebrated foundational 1930s
work of Jerzy Neyman and Egon Pearson (son of Karl, the founder of the
journal Biometrika and developer of the chi-squared test and other fun-
damental methods for the analysis of 2 × 2 tables). As everyone reading
this far into this text is no doubt aware, the basic setup compares two
hypotheses,
H0 : θ = θ0 versus HA : θ ̸= θ0 .
Here, note that H0 is the null hypothesis, and is the one our trial typically
hopes to reject (since θ0 is usually taken as 0, the case of no treatment
eﬃcacy). As already illustrated in Example 1.1, the frequentist approach
is to compute a test statistic, and check to see if it is “extreme” enough
relative to a reference distribution determined by the statistical model un-
der the null hypothesis. Despite his famous and lifelong disagreement with
Neyman, Fisher himself contributed to the eﬀort by developing the p-value,
p = P{T(Y) more “extreme” than T(yobs) | θ, H0} ,
(2.6)
where “extremeness” is in the direction of the alternative hypothesis. If the
p-value is less than some prespeciﬁed Type I error rate α, H0 is rejected;
otherwise, it is not.
While Bayesians generally embrace Fisher’s concept of likelihood, they
also abhor p-values for a variety of reasons. Some of these are purely prac-
tical. For one thing, note the setup is asymmetric in the sense that H0 can
never be accepted, only rejected. While this is the usual goal, in the case
of equivalence testing it is the alternative we actually hope to reject. This
forces the frequentist into an awkward restructuring of the entire decision
problem; see Section 6.2. Another practical problem with frequentist hy-
pothesis testing is that the null hypothesis must be a “reduction” (special
case) of the alternative; the hypotheses must be nested. But it is easy to
imagine our interest lying in a comparison of nonnested hypotheses like
H0 : θ < θ0 versus HA : θ ≥θ0 ,
or perhaps even nonnested models (quadratic versus exponential) or dis-
tributional (normal versus logistic) alternatives. Yet another practical dif-
ﬁculty (and one already mentioned in Example 1.1) is the common misin-
terpretation of the p-value by nonstatisticians (and even by many statisti-
cians!) as the “probability that the null hypothesis is true,” a misinterpreta-
tion reinforced by the fact that null hypotheses are rejected when p-values

30
BASICS OF BAYESIAN INFERENCE
are small. But in fact only Bayesians can make claims about the probabil-
ity that any hypothesis is true (or false), since only Bayesians admit that
unknown model characteristics have distributions!
While these practical diﬃculties are enough to make one rethink the use
of p-values in science, a much more fundamental diﬃculty is their viola-
tion of something called the Likelihood Principle. Originally postulated by
Birnbaum (1962), its brief statement is as follows:
The Likelihood Principle: In making inferences or decisions about θ after y
is observed, all relevant experimental information is contained in the likelihood
function for the observed y.
By taking into account not only the observed data y, but also the unob-
served (but more extreme) values of Y, classical hypothesis testing violates
the Likelihood Principle. This has great relevance for the practice of clini-
cal trials since it eﬀectively precludes the frequentist from “peeking” at the
data as it accumulates. This is because additional looks at the data change
the deﬁnition of “as extreme as the observed value or more so” in (2.6), and
thus the p-value. But altering our decision regarding the eﬃcacy of a drug
or device simply because we decided to peek at the data an extra time is
a clear violation of the Likelihood Principle; surely only the accumulating
data itself should drive our decision here, not how many times we look at it
or what might have happened had we stopped the experiment in a diﬀerent
way. The debate over the proper handling of this multiplicity problem is
ongoing; c.f. Example 2.6 in Subsection 2.2.7, as well as the much fuller
discussion and possible Bayesian remedies in Section 6.3.
We have seen that Bayesian inference (point or interval) is quite straight-
forward given the posterior distribution, or an estimate thereof. By con-
trast, hypothesis testing is less straightforward, for two reasons. First, there
is less agreement among Bayesians as to the proper approach to the prob-
lem. For years, posterior probabilities and Bayes factors were considered
the only appropriate method. But these methods are only suitable with
fully proper priors, and for relatively low-dimensional models. With the
recent proliferation of very complex models with at least partly improper
priors, other methods have come to the fore. Second, solutions to hypoth-
esis testing questions often involve not just the posterior p(θ|y), but also
the marginal distribution, p(y). Unlike the case of posterior and the predic-
tive distributions, samples from the marginal distribution do not naturally
emerge from most MCMC algorithms. Thus, the sampler must often be
“tricked” into producing the necessary samples.
Recently, an approximate yet very easy-to-use model choice tool known
as the Deviance Information Criterion (DIC) has gained popularity, as well
as implementation in the WinBUGS software package. We will limit our at-
tention in this subsection to Bayes factors and the DIC. The reader is
referred to Carlin and Louis (2009, Sections 2.3.3 and 4.4–4.6) for further

BAYESIAN INFERENCE
31
techniques and information, as well as a related posterior predictive crite-
rion proposed by Gelfand and Ghosh (1998).
Bayes factors
We begin by setting up the hypothesis testing problem as a model choice
problem, replacing the customary two hypotheses H0 and HA by two can-
didate parametric models M1 and M2 having respective parameter vectors
θ1 and θ2. Under prior densities πi(θi), i = 1, 2, the marginal distributions
of Y are found by integrating out the parameters,
p(y|Mi) =
Z
f(y|θi, Mi)πi(θi)dθi , i = 1, 2 .
(2.7)
Bayes’ Theorem (2.1) may then be applied to obtain the posterior prob-
abilities P(M1|y) and P(M2|y) = 1 −P(M1|y) for the two models. The
quantity commonly used to summarize these results is the Bayes factor,
BF, which is the ratio of the posterior odds of M1 to the prior odds of M1,
given by Bayes’ Theorem as
BF
=
P(M1|y)/P(M2|y)
P(M1)/P(M2)
(2.8)
=
h
p(y|M1)P (M1)
p(y)
i
/
h
p(y|M2)P (M2)
p(y)
i
P(M1)/P(M2)
=
p(y | M1)
p(y | M2) ,
(2.9)
the ratio of the observed marginal densities for the two models. Assuming
the two models are a priori equally probable (i.e., P(M1) = P(M2) = 0.5),
we have that BF = P(M1|y)/P(M2|y), the posterior odds of M1.
Consider the case where both models share the same parametrization
(i.e., θ1 = θ2 = θ), and both hypotheses are simple (i.e., M1 : θ = θ(1)
and M2 : θ = θ(2)). Then πi(θ) consists of a point mass at θ(i) for i = 1, 2,
and so from (2.7) and (2.9) we have
BF = f(y|θ(1))
f(y|θ(2))
,
which is nothing but the likelihood ratio between the two models. Hence,
in the simple-versus-simple setting, the Bayes factor is precisely the odds
in favor of M1 over M2 given solely by the data.
A popular “shortcut” method is the Bayesian Information Criterion
(BIC), also known as the Schwarz criterion, the change in which across
the two models is given by
∆BIC = W −(p2 −p1) log n ,
(2.10)

32
BASICS OF BAYESIAN INFERENCE
where pi is the number of parameters in model Mi, i = 1, 2, and
W = −2 log
·supM1 f(y|θ)
supM2 f(y|θ)
¸
,
the usual likelihood ratio test statistic. Schwarz (1978) showed that for
nonhierarchical (two-stage) models and large sample sizes n, BIC approx-
imates −2 log BF. An alternative to BIC is the Akaike Information Crite-
rion (AIC), which alters (2.10) slightly to
∆AIC = W −2(p2 −p1) .
(2.11)
Both AIC and BIC are penalized likelihood ratio model choice criteria, since
both have second terms that act as a penalty, correcting for diﬀerences in
size between the models (to see this, think of M2 as the “full” model and
M1 as the “reduced” model).
The more serious (and aforementioned) limitation in using Bayes factors
or their approximations is that they are not appropriate under noninfor-
mative priors. To see this, note that if πi(θi) is improper, then p(y|Mi) =
R
f(y|θi, Mi)πi(θi)dθi necessarily is as well, and so BF as given in (2.9)
is not well deﬁned. While several authors (see, e.g., Berger and Pericchi,
1996; O’Hagan, 1995) have attempted to modify the deﬁnition of BF to
repair this deﬁciency, we prefer the more informal yet still general approach
we now describe.
The DIC criterion
Spiegelhalter et al. (2002) propose a generalization of the AIC, whose
asymptotic justiﬁcation is not appropriate for hierarchical (3 or more level)
models. The generalization is based on the posterior distribution of the de-
viance statistic,
D(θ) = −2 log f(y|θ) + 2 log h(y) ,
(2.12)
where f(y|θ) is the likelihood function and h(y) is some standardizing
function of the data alone. These authors suggest summarizing the ﬁt of a
model by the posterior expectation of the deviance, D = Eθ|y[D], and the
complexity of a model by the eﬀective number of parameters pD (which may
well be less than the total number of model parameters, due to the bor-
rowing of strength across random eﬀects). In the case of Gaussian models,
one can show that a reasonable deﬁnition of pD is the expected deviance
minus the deviance evaluated at the posterior expectations,
pD = Eθ|y[D] −D(Eθ|y[θ]) = D −D(¯θ) .
(2.13)
The Deviance Information Criterion (DIC) is then deﬁned as
DIC = D + pD = 2D −D(¯θ) ,
(2.14)

BAYESIAN INFERENCE
33
with smaller values of DIC indicating a better-ﬁtting model. Both building
blocks of DIC and pD, Eθ|y[D] and D(Eθ|y[θ]), are easily estimated via
MCMC methods (see below), enhancing the approach’s appeal. Indeed,
DIC may be computed automatically for any model in WinBUGS.
Although the pD portion of this expression has meaning in its own right
as an eﬀective model size, DIC itself does not, since it has no absolute scale
(due to the arbitrariness of the scaling constant h(y), which is often simply
set equal to zero). Thus only diﬀerences in DIC across models are mean-
ingful. Relatedly, when DIC is used to compare nested models in standard
exponential family settings, the unnormalized likelihood L(θ; y) is often
used in place of the normalized form f(y|θ) in (2.12), since in this case the
normalizing function m(θ) =
R
L(θ; y)dy will be free of θ and constant
across models, hence contribute equally to the DIC scores of each (and
thus have no impact on model selection). However, in settings where we
require comparisons across diﬀerent likelihood distributional forms, gener-
ally we must be careful to use the properly scaled joint density f(y|θ) for
each model.
Identiﬁcation of what constitutes a signiﬁcant diﬀerence is also somewhat
subjective. Spiegelhalter et al. (2002) state that a DIC diﬀerence of 5 or 10
is typically thought of as “the smallest worth mentioning.” Regarding the
Monte Carlo variance of DIC, an informal approach is simply to recompute
DIC a few times using diﬀerent random number seeds, to get a rough idea
of the variability in the estimates. With a large number of independent DIC
replicates {DICl, l = 1, . . . , N}, one could of course estimate V ar(DIC)
by its sample variance,
d
V ar(DIC) =
1
N −1
N
X
l=1
(DICl −DIC)2 .
But in any case, DIC is not intended for formal identiﬁcation of the “cor-
rect” model, but rather merely as a method of comparing a collection of
alternative formulations (all of which may be incorrect). This informal out-
look (and DIC’s approximate nature in markedly nonnormal models) sug-
gests informal measures of its variability will often be suﬃcient. The pD
statistic is also helpful in its own right, since how close it is to the actual
parameter count provides information about how many parameters are ac-
tually “needed” to adequately explain the data. For instance, a relatively
low pD in a random eﬀects model indicates the random eﬀects are greatly
“shrunk” back toward their grand mean, possibly so much so that they are
not really needed in the model.
DIC is remarkably general, and trivially computed as part of an MCMC
run without any need for extra sampling, reprogramming, or complicated
loss function determination. Moreover, experience with DIC to date sug-
gests it works remarkably well, despite the fact that no formal justiﬁcation

34
BASICS OF BAYESIAN INFERENCE
for it is yet available outside of posteriors that can be well approximated
by a Gaussian distribution (a condition that typically occurs asymptoti-
cally, but perhaps not without a moderate to large sample size for many
models). Still, DIC is by no means universally accepted by Bayesians as
a suitable all-purpose model choice tool, as the discussion to Spiegelhalter
et al. (2002) almost immediately indicates. Model comparison using DIC
is not invariant to parametrization, so (as with prior elicitation) the most
sensible parametrization must be carefully chosen beforehand. Unknown
scale parameters and other innocuous restructuring of the model can also
lead to subtle changes in the computed DIC value.
Finally, DIC will obviously depend on what part of the model speciﬁ-
cation is considered to be part of the likelihood, and what is not. Spiegel-
halter et al. (2002) refer to this as the focus issue, i.e., determining which
parameters are of primary interest, and which should “count” in pD. For
instance, in a hierarchical model with data distribution f(y|θ), prior p(θ|η)
and hyperprior p(η), one might choose as the likelihood either the obvious
conditional expression f(y|θ), or the marginal expression,
p(y|η) =
Z
f(y|θ)p(θ|η)dθ .
(2.15)
We refer to the former case as “focused on θ,” and the latter case as
“focused on η.” Spiegelhalter et al. (2002) defend the dependence of pD
and DIC on the choice of focus as perfectly natural, since while the two
foci give rise to the same marginal density p(y), the integration in (2.15)
clearly suggests a diﬀerent model complexity than the unintegrated version
(having been integrated out, the θ parameters no longer “count” in the
total). They thus argue that it is up to the user to think carefully about
which parameters ought to be in focus before using DIC. Perhaps the one
diﬃculty with this advice is that in cases where the integration in (2.15)
is not possible in closed form, the unintegrated version is really the only
feasible choice. Indeed, the DIC tool in WinBUGS always focuses on the
lowest level parameters in a model (in order to sidestep the integration
issue), even when the user intends otherwise. Our view is that this is really
“not a bug but a feature” of WinBUGS, since we would nearly always want
the eﬀective parameter count to be relative to the total parameter burden,
as this provides an idea of how much the random eﬀects have shrunk back
toward their (posterior) grand means.
2.2.4 Prediction
An advantage of the Bayesian approach is the ability to ﬁnd probability
distributions of as yet unobserved results. Consider the following example:
Example 2.4 A study reported by Freireich et al. (1963) was designed
to evaluate the eﬀectiveness of chemotherapeutic agent 6-mercaptopurine

BAYESIAN INFERENCE
35
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
θ
density
Figure 2.3 Posterior density of θ, a Beta(19, 4).
(6-MP) for the treatment of acute leukemia. Patients were randomized to
therapy in pairs. Let θ be the population proportion of pairs in which
the 6-MP patient stays in remission longer than the placebo patient. (To
distinguish a probability θ from a probability distribution concerning θ, we
will call it a population proportion or a propensity.) The null hypothesis
is H0 : θ = 1/2, i.e., no eﬀect of 6-MP. Let H1 stand for the alternative,
H1 : θ ̸= 1/2. There were 21 pairs of patients in the study, and 18 of them
favored 6-MP.
Suppose that the prior distribution is uniform on the interval (0,1). The
Uniform(0, 1) distribution is also the Beta(1, 1) distribution. Updating the
Beta(a, b) distribution after y successes and n −y failures is easy because
the beta prior is conjugate with the likelihood. As mentioned above, this
means that the posterior distribution emerges as a member of the same
distributional family as the prior. To see this in the beta-binomial case,
from Bayes’ Rule (2.1) we have
p(θ|y)
∝
f(y|θ)π(θ)
=
µn
y
¶
θy(1 −θ)n−y × Γ(a + b)
Γ(a)Γ(b)θa−1(1 −θ)b−1
∝
θy+a−1(1 −θ)n−y+b−1 .

36
BASICS OF BAYESIAN INFERENCE
Note in this last expression we have absorbed all multiplicative terms that
do not involve θ into the unknown constant of proportionality. But now,
we recognize this form as being proportional to a Beta(y + a, n −y + b)
density. Since this is the only density proportional to our form that still
integrates to 1, this must be the posterior we are looking for. Thus, in
our case (y = 18 and n −y = 3), the posterior distribution under H1 is
Beta(19, 4), as plotted in Figure 2.3. Note this distribution is unimodal
with mean 19/(19 + 4) = .8261.
In the predictive context, suppose it is possible to take 5 additional ob-
servations. How useful would this be? A way to answer is to assess the
consequences of getting k successes in the next 5 pairs of patients (for a
total of 26 pairs in all) for k = 0, 1, . . . , 5, and then to weigh these conse-
quences by the predictive probabilities of the possible values of k.
To calculate the probabilities of future observations, we ﬁrst ﬁnd these
probabilities assuming that the parameters are known, and also ﬁnd the
posterior (or current) distribution of the parameters. We then average the
conditional distribution with respect to the posterior distribution of the
parameters. This gives the unconditional predictive distribution of interest.
To illustrate, ﬁrst consider a single additional, 22nd pair of patients,
having binary outcome variable x22. Assume that this pair is exchangeable
with the ﬁrst 21 pairs, x = (x1, . . . , x21)′, for which y = P21
i=1 xi = 18.
One member of the new pair is assigned to 6-MP, and the other is assigned
to placebo. The predictive probability that the 6-MP patient will stay in
remission longer is the mean of the posterior distribution of θ. For the
Beta(19, 4) distribution considered separately, the predictive probability
of success is given by Laplace’s Rule of Succession: 19/(19+4) = 0.8261
(Berry, 1996, Sec. 7.2). To prove this, note that the predictive distribution
of x22 is
f(x22|x)
=
Z
f(x22|θ)p(θ|x)dθ
=
Z
θx22(1 −θ)1−x22
Γ(23)
Γ(19)Γ(4)θ18(1 −θ)3dθ .
So the chance that the 22nd observation is a success (x22 = 1) is just
P(x22 = 1|x)
=
Z
Γ(23)
Γ(19)Γ(4)θ19(1 −θ)3dθ
=
Γ(23)
Γ(19)Γ(4)
Γ(4)Γ(20)
Γ(24)
Z
Γ(24)
Γ(20)Γ(4)θ19(1 −θ)3dθ
=
Γ(23)Γ(20)
Γ(19)Γ(24)
=
19
23 = .8261 ,

BAYESIAN INFERENCE
37
the third equality arising since the “ﬁxed up” integral equals 1, and the
fourth arising since Γ(z) = (z −1)Γ(z −1) for any z.
Now suppose that two more pairs of patients (both exchangeable with
the ﬁrst 21) are treated, one member of each pair with 6-MP and the other
with placebo. The predictive probability of both being successes for 6-MP
is not the square of 0.8261, but rather the probability of the ﬁrst pair being
a success times the probability of the second being a success given the result
of the ﬁrst. Using Laplace’s Rule of Succession twice, we have
P(x22 = 1 and x23 = 1|x)
=
P(x22 = 1|x)P(x23 = 1|x22 = 1, x)
=
µ19
23
¶ µ20
24
¶
= 0.6884 .
For 5 additional pairs, the predictive probability of 4 successes for 6-MP is
5
µ 22 · 21 · 20 · 19 · 4
27 · 26 · 25 · 24 · 23
¶
= 0.3624 ,
the leading “5” arising since there are 5 possible orders in which the 4 suc-
cesses could have arrived (SSSSF, SSSFS, . . ., FSSSS), all of which lead to
the same overall probability. Note that we could easily repeat this calcula-
tion for any number of successes and use the resulting table of probabilities
in our decision regarding whether to stop the trial; see Chapters 3 and 4.
2.2.5 Eﬀect of the prior: sensitivity analysis
Bayes’ Theorem makes it clear that the prior distribution inﬂuences the
posterior distribution, and therefore it inﬂuences conclusions. This aspect of
Bayesian analysis is regarded as negative by some. But the eﬀect is positive
in that it allows researchers and others to formally account for information
that is available separate from the current experiment. Consider two people
who come to markedly diﬀerent conclusions from an experiment. Either
they have been exposed to diﬀerent additional information or they have
processed the available evidence diﬀerently. In any case, we can infer that
the experiment is not very informative. Edwards, Lindman, and Savage
(1963) show that two reasonably open-minded people will eventually come
to agree if both are exposed to the same data and both use Bayes’ Theorem.
The condition “reasonably open-minded” is meant to exclude people who
“know” the answer in advance; if the two observers assign probability 1
to diﬀerent values of a parameter, no amount of information will dissuade
them from their original (and discordant) “knowledge.”
Example 2.5 As an example showing that opinions tend to converge, con-
sider control data presented by Smith, Spiegelhalter, and Parmar (1996):
of 1934 patients in an intensive care unit, 566 developed respiratory tract
infections. As seen in Example 2.4, for a Beta(a, b) prior distribution, the

38
BASICS OF BAYESIAN INFERENCE
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
θ
density
Beta(1,19)
Beta(19,1)
Figure 2.4 Two rather diﬀerent beta prior densities.
posterior distribution for the probability θ of developing an infection is
Beta(a + 566, b + 1368). With such a large sample size, two people with
prior opinions that are rather disparate would come to agree reasonably
well. For example, suppose one person’s (a, b) is (1,19) while the other’s is
(19,1) (as shown in Figure 2.4). Then the ﬁrst’s prior probability of infec-
tion is just 0.05, while the second’s is a whopping 0.95. The ﬁrst’s posterior
is Beta(567, 1387) and the second’s is Beta(585, 1369), both of which are
plotted in Figure 2.5. The corresponding predictive mean probabilities of
infection are remarkably similar: 0.290 and 0.299. The prior probability that
these two independent assessors are within 0.1 of each other is nearly 0,
but the corresponding posterior probability is nearly 1. This demonstrates
another feature of Bayesian methods: even investigators with wildly dis-
similar prior beliefs can ultimately come to agreement once suﬃcient data
have accumulated.
2.2.6 Role of randomization
The random assignment of patients to either the treatment or control group
in clinical trials is among the most important advances in the history of
medical research. No other design gives a comparably high level of conﬁ-
dence in the trial’s results. Randomization ensures that treatment assign-

BAYESIAN INFERENCE
39
0.0
0.2
0.4
0.6
0.8
1.0
0
10
20
30
40
θ
density
Beta(567,1387)
Beta(585,1369)
Figure 2.5 Beta posterior distributions for prior distributions in Figure 2.4.
ment is unbiased. In particular, randomization helps account for shifts in
the patient population, changes in the standard of care, and competing
treatment options over time. Without randomization, patient prognosis
may be correlated with therapy assigned. While adjusting for covariates
may help in parsing out treatment eﬀect in a nonrandomized trial, the im-
portant covariates may not be known or assessable. A distinctly secondary
advantage of randomization is that it serves to balance assignment among
the various treatments, including within patient subgroups.
However, while randomization is a basis for frequentist inference, it plays
no role in calculating posterior probabilities (except that a Bayesian may
choose to discount data from nonrandomized trials as compared with ran-
domized trials). This has led some Bayesians to suggest that randomization
is not essential. We believe the ability to eliminate or minimize assignment
bias makes randomization of utmost importance. If we could measure all
the important covariates then randomization would not be necessary, but
we cannot, or at least we cannot be sure that we know all the important
covariates ahead of time.
That said, randomization in clinical trials has at least three disadvan-
tages. First, physicians face ethical dilemmas in recommending a random-
ized trial to patients when their evaluation of the evidence is that the treat-
ments are not equally eﬀective and safe. Second, physicians worry that their

40
BASICS OF BAYESIAN INFERENCE
relationship with patients will be adversely aﬀected if they propose treat-
ment by coin toss. Third, since not all patients and physicians are willing
to participate in randomized trials, patients treated in a randomized trial
may diﬀer in important ways from those who might eventually be treated
in a clinical setting with one of the competing treatments.
Examining clinical databases to compare therapeutic eﬀects is wrought
with bias. The major bias is the one solved by randomization: assignment
to therapy may depend on prognosis. The Bayesian approach is ideal for
analyzing such data because characteristics of the database – including the
degree to which it is exchangeable with other databases and with data
from clinical trials – can be assessed subjectively. Examples are discussed
hereafter. To say that a Bayesian analysis is possible is not to say that
the Bayesian approach makes up for imperfections in design. For some
circumstances of poor data collection, the results can give no information
about therapeutic comparisons regardless of the statistical approach.
2.2.7 Handling multiplicities
Multiplicities are among the most diﬃcult of problems faced by statisticians
and other researchers, and several types of multiplicities are discussed in
this text, including metaanalysis, interim analysis, multiple comparisons,
and subgroup analysis. Handling multiplicities is controversial in statistics
and science. The standard frequentist approach is to condition on the null
hypotheses of no eﬀect in any of the comparisons or analyses, and this is
the subject of much criticism (Carmer and Walker, 1982; O’Brien, 1983;
Rothman, 1990). Some types of multiplicities (including metaanalysis, anal-
ysis of multicenter trials, and interim analysis) are not problematic for
Bayesians. Other forms of multiplicities (including multiple comparisons,
variable selection in regression, selecting transformations in regression, sub-
set analyses, data dredging, and publication biases) are problematic for
Bayesians as well as for frequentists.
Multiplicities are present in virtually every application of statistics. Most
frequentist statisticians subscribe to the principle that adjustments in infer-
ences must be made to account for multiplicities. A basic complaint against
the frequentist attitude toward multiple comparisons is that it is inconsis-
tent with what is perceived to be the scientiﬁc method, as the following
simple example illustrates.
Example 2.6 Consider a scientist who collects data concerning the eﬀec-
tiveness of treatments A and B, and ﬁnds that the diﬀerence in treatment
means is statistically signiﬁcant, say based on a p-value of 0.03. Consider a
second scientist who runs the same experiment except for the inclusion of a
third treatment, treatment C, and on treatments A and B she obtains data
identical to that of the ﬁrst scientist. After adjusting for multiple com-
parisons the second scientist cannot claim statistical signiﬁcance for the

BAYESIAN INFERENCE
41
diﬀerence between treatments A and B. The second scientist ran a more
informative experiment, yet is penalized for doing so by the increase in
Type I error. This seems unscientiﬁc. To make matters worse, the second
scientist may say that she had no intention of using the treatment C data
for any inferential purpose; it was simply a control to ensure that the exper-
iment was properly run. In this event, now she too can claim signiﬁcance
for treatment A vs. treatment B. Having conclusions depend on the mere
intentions of the experimenter also seems unscientiﬁc.
Conditioning on a null hypothesis is typically anathema to Bayesians.
Therefore, the Bayesian approach seems to reject adjustments for multi-
ple comparisons, and indeed this is the view of many Bayesians. However,
Bayesian adjustments that are similar to frequentist adjustments are le-
gitimate and appropriate in many applications; see e.g. Berry (1989), Du-
Mouchel (1990), Gopalan and Berry (1997), or Berry and Hochberg (1999).
Bayesian adjustments of posterior probabilities are similar to shrinkage ad-
justments mentioned above and will be discussed in Example 2.7. A distinc-
tion between the frequentist and Bayesian approaches is that, in the latter,
the mere existence of a third treatment is irrelevant. Bayesian adjustments
for comparing treatments A and B depend on the results that were actually
observed on treatment C, as well as on A and B. An important caveat of
posterior adjustment for multiplicities is that it is exactly that: only an
adjustment of probabilities. But computation of posterior probabilities is
only half the inference; the other half is making an actual decision (about
rejecting H0, etc.). For the latter, Bayesian inference is every bit as vul-
nerable to multiplicities as frequentist inference; see, for example, Berry
and Berry (2004), as well as Section 6.3. In particular, Subsection 6.3.1 de-
scribes an extension of Berry and Berry (2004) and subsequent application
to a real data set, while Subsection 6.3.2 further extends the thinking to
the selection of signiﬁcance thresholds in false discovery rate (FDR) esti-
mation settings. Statistical adjustments necessary for realistic assessment
of subgroup eﬀects, discussed in Section 6.4, are also related to the multi-
plicity problem since they are often initially uncovered by “data dredging”
after a trial fails to show a signiﬁcant treatment eﬀect in the overall patient
population.
Rothman (1990) refers to the adoption of adjustments for multiple com-
parisons in the biomedical and social sciences as “half-hearted,” and indeed
it is. Rothman then concludes that such adjustments are never needed.
As is clear from our foregoing comments, that is not our view. Science
is subjective, and a standard adjustment for multiplicities is not possible.
Sometimes adjustments are appropriate and sometimes they are not. When
they are appropriate, the amount of adjustment depends on the available
evidence, from both within and outside of the experiment in question. The
lack of a consistent policy regarding adjustments in science may seem “half-

42
BASICS OF BAYESIAN INFERENCE
hearted,” but no routine policy is possible or desirable. Two scientists who
analyze data from the same experiment may have diﬀerent knowledge bases
or they may interpret the available evidence diﬀerently. Statistical inference
is not like calculating an amount of income tax owed: in our view, diﬀer-
ences in conclusions from experimental results do not mean that one or
both of the scientists’ conclusions are wrong.
2.3 Bayesian computation
As mentioned above, in this section we provide a brief introduction to
Bayesian computing, following the development in Chapter 3 of Carlin and
Louis (2009). The explosion in Bayesian activity and computing power over
the last decade or so has led to a similar explosion in the number of books
in this area. The earliest comprehensive treatment was by Tanner (1998),
with books by Gilks et al. (1996), Gamerman and Lopes (2006), and Chen
et al. (2000) oﬀering updated and expanded discussions that are primarily
Bayesian in focus. Also signiﬁcant are the computing books by Robert and
Casella (2005) and Liu (2008), which, while not speciﬁcally Bayesian, still
emphasize Markov chain Monte Carlo methods typically used in modern
Bayesian analysis.
The presence of the integral in the denominator of Bayes’ Theorem (2.1)
means that, with a few exceptions, the history of real Bayesian data anal-
ysis goes back only as far as our ability to numerically evaluate integrals of
dimension higher than 5 or 10 – that is, to the 1960s and 70s. In those days,
most numerical integration routines did not use Monte Carlo methods, but
rather more traditional quadrature methods. The most basic of these are
the so-called Newton-Cotes rules that use a weighted sum of function val-
ues along a ﬁxed partition in each dimension of the integrand. Examples
of these rules include the trapezoidal and Simpson rules, familiar from ele-
mentary calculus textbooks since they are straightforward generalizations
of the usual Riemann approximation to the integral. More complex rules,
such as Gaussian quadrature, use fewer but irregularly spaced grid points,
and can improve eﬃciency by strategically placing more function evalua-
tions where the function is changing most dramatically.
While such approaches can be fast computationally, Bayesians largely
abandoned them in the 1990s in favor of Monte Carlo integration for two
reasons. First, quadrature methods suﬀer greatly from what numerical an-
alysts call the curse of dimensionality, which essentially is the fact that the
computational burden increases exponentially with the dimension of the
integral. Since modern Bayesian data analysis often requires models with
hundreds or even thousands of parameters, the curse often renders quadra-
ture infeasible. Second, the tuning required to implement good quadrature
methods is often rather high, compared to the Gibbs sampler and other
Monte Carlo methods which (like the jackknife and the bootstrap) are

BAYESIAN COMPUTATION
43
often implemented straightforwardly by statisticians with relatively mod-
est computing skills. Since the most prominent Bayesian software package,
WinBUGS, also uses Monte Carlo methods, we focus on them throughout
the rest of this section. However, we caution that quadrature methods still
have a place in Bayesian clinical trials analysis, at least for those having
relatively low-dimensional parameter spaces. In these settings, quadrature
will be much more eﬃcient, and thus possibly the best choice when simulat-
ing the operating characteristics of our procedures (see Subsection 2.5.4), a
task that requires repeated integral evaluation over a large number of sim-
ulated data sets. Indeed, a simple normal approximation to the posterior
(as provided by Theorem 2.1) will often suﬃce for such simulation studies;
we provide examples in Chapters 3–5.
The current popularity of Markov chain Monte Carlo (MCMC) methods
is due to their ability (in principle) to enable inference from posterior distri-
butions of arbitrarily large dimension, essentially by reducing the problem
to one of recursively solving a series of lower-dimensional (often unidimen-
sional) problems. Like traditional Monte Carlo methods, MCMC methods
work by producing not a closed form for the posterior in (2.1), but a sample
of values {θ(g), g = 1, . . . , G} from this distribution. While this obviously
does not carry as much information as the closed form itself, a histogram
or kernel density estimate based on such a sample is typically suﬃcient for
reliable inference; moreover such an estimate can be made arbitrarily accu-
rate merely by increasing the Monte Carlo sample size G. However, unlike
traditional Monte Carlo methods, MCMC algorithms produce correlated
samples from this posterior, since they arise from recursive draws from a
particular Markov chain, the stationary distribution of which is the same
as the posterior.
The convergence of the Markov chain to the correct stationary distri-
bution can be guaranteed for an enormously broad class of posteriors, ex-
plaining MCMC’s popularity. But this convergence is also the source of
most of the diﬃculty in actually implementing MCMC procedures, for two
reasons. First, it forces us to make a decision about when it is safe to stop
the sampling algorithm and summarize its output, an area known in the
business as convergence diagnosis. Second, it clouds the determination of
the quality of the estimates produced (since they are based not on i.i.d.
draws from the posterior, but on correlated samples). This is sometimes
called the variance estimation problem, since a common goal here is to es-
timate the Monte Carlo variances (equivalently standard errors) associated
with our MCMC-based posterior estimates.
A great many useful MCMC algorithms have appeared in the last twenty
or so years, many of which can oﬀer signiﬁcant advantages in certain spe-
cialized situations or model settings. For example, WinBUGS uses slice sam-
pling (Neal, 2003) for nonconjugate settings over bounded parameter do-

44
BASICS OF BAYESIAN INFERENCE
mains, partly because it turns out to be fairly natural here, and partly
because these domains do not lend themselves to ordinary Metropolis sam-
pling without transformation of the parameter space. Even more recent
research has focused on adaptive MCMC methods (Haario et al., 2001;
Roberts and Rosenthal, 2007) that attempt to accelerate convergence by
using the early output of an MCMC chain to reﬁne and improve the sam-
pling as it progresses. In the remainder of this section, however, we restrict
our attention to the two most popular and broadly applicable MCMC al-
gorithms, the Gibbs sampler and the Metropolis-Hastings algorithm. We
then return to the convergence diagnosis and variance estimation problems.
2.3.1 The Gibbs sampler
Suppose our model features k parameters, θ = (θ1, . . . , θk)′. To implement
the Gibbs sampler, we must assume that samples can be generated from
each of the full or complete conditional distributions {p(θi | θj̸=i, y), i =
1, . . . , k} in the model. Such samples might be available directly (say, if the
full conditionals were familiar forms, like normals and gammas) or indi-
rectly (say, via a rejection sampling approach). In this latter case two pop-
ular alternatives are the adaptive rejection sampling (ARS) algorithm of
Gilks and Wild (1992), and the Metropolis algorithm described in the next
subsection. In either case, under mild conditions, the collection of full con-
ditional distributions uniquely determines the joint posterior distribution,
p(θ|y), and hence all marginal posterior distributions p(θi|y), i = 1, . . . , k.
Given an arbitrary set of starting values {θ(0)
2 , . . . , θ(0)
k }, the algorithm
proceeds as follows:
Algorithm 2.1 (Gibbs Sampler).
For (t ∈1 : T), repeat:
Step 1: Draw θ(t)
1
from p
³
θ1 | θ(t−1)
2
, θ(t−1)
3
, . . . , θ(t−1)
k
, y
´
Step 2: Draw θ(t)
2
from p
³
θ2 | θ(t)
1 , θ(t−1)
3
, . . . , θ(t−1)
k
, y
´
...
Step k: Draw θ(t)
k
from p
³
θk | θ(t)
1 , θ(t)
2 , . . . , θ(t)
k−1, y
´
Then for t suﬃciently large, (θ(t)
1 , . . . , θ(t)
k )
approx
∼
p(θ1, . . . , θk|y).
The convergence of the k-tuple obtained at iteration t, (θ(t)
1 , . . . , θ(t)
k ), to a
draw from the true joint posterior distribution p(θ1, . . . , θk|y) occurs under
mild regulatory conditions that are generally satisiﬁed for most statistical
models (see, e.g., Geman and Geman, 1984, or Roberts and Smith, 1993).
This means that for t suﬃciently large (say, bigger than t0), {θ(t), t = t0 +
1, . . . , T} is a (correlated) sample from the true posterior, from which any

BAYESIAN COMPUTATION
45
posterior quantities of interest may be estimated. For example, a histogram
of the {θ(t)
i , t = t0 + 1, . . . , T} themselves provides a simulation-consistent
estimator of the marginal posterior distribution for θi, p(θi | y). We might
also use a sample mean to estimate the posterior mean, i.e.,
bE(θi|y) =
1
T −t0
T
X
t=t0+1
θ(t)
i
.
(2.16)
The time from t = 0 to t = t0 is commonly known as the burn-in period;
popular methods for selection of an appropriate t0 are discussed below.
In practice, we may actually run m parallel Gibbs sampling chains, in-
stead of only 1, for some modest m (say, m = 3). Parallel chains may be
useful in assessing sampler convergence, and anyway can be produced with
no extra time on a multiprocessor computer. In this case, we would again
discard all samples from the burn-in period, obtaining the posterior mean
estimate,
bE(θi|y) =
1
m(T −t0)
m
X
j=1
T
X
t=t0+1
θ(t)
i,j ,
(2.17)
where now the second subscript on θi,j indicates chain number. Again we
defer comment on the issues of how to choose t0 and how to assess the
quality of (2.17) and related estimators to subsequent subsections.
2.3.2 The Metropolis-Hastings algorithm
The Gibbs sampler is easy to understand and implement, but requires the
ability to readily sample from each of the full conditional distributions,
p(θi |θj̸=i, y). Unfortunately, when the prior distribution p(θ) and the like-
lihood f(y|θ) are not a conjugate pair, one or more of these full condi-
tionals may not be available in closed form. Even in this setting, however,
p(θi | θj̸=i, y) will be available up to a proportionality constant, since it is
proportional to the portion of f(y|θ) × p(θ) that involves θi.
The Metropolis algorithm (or Metropolis-Hastings algorithm) is a rejec-
tion algorithm that attacks precisely this problem, since it requires only a
function proportional to the distribution to be sampled, at the cost of re-
quiring a rejection step from a particular candidate density. Like the Gibbs
sampler, this algorithm was not developed by statistical data analysts for
this purpose; the primary authors on the Metropolis et al. (1953) paper
were computer scientists working on the Manhattan Project at Los Alamos
National Laboratory in the 1940s.
While as mentioned above our main interest in the algorithm is for gen-
eration from (typically univariate) full conditionals, it is easily described
(and theoretically supported) for the full multivariate θ vector. Thus, sup-
pose for now that we wish to generate from a joint posterior distribution

46
BASICS OF BAYESIAN INFERENCE
p(θ|y) ∝h(θ) ≡f(y|θ)p(θ). We begin by specifying a candidate density
q(θ∗|θ(t−1)) that is a valid density function for every possible value of the
conditioning variable θ(t−1), and satisﬁes q(θ∗|θ(t−1)) = q(θ(t−1)|θ∗), i.e.,
q is symmetric in its arguments. Given a starting value θ(0) at iteration
t = 0, the algorithm proceeds as follows:
Algorithm 2.2 (Metropolis Algorithm).
For (t ∈1 : T), repeat:
Step 1: Draw θ∗from q(·|θ(t−1))
Step 2: Compute the ratio r = h(θ∗)/h(θ(t−1)) = exp[log h(θ∗) −
log h(θ(t−1))]
Step 3: If r ≥1, set θ(t) = θ∗;
If r < 1, set θ(t) =
½ θ∗with probability r
θ(t−1) with probability 1 −r
.
Then under generally the same mild conditions as those supporting the
Gibbs sampler, θ(t) approx
∼
p(θ|y).
Note that when the Metropolis algorithm (or the Metropolis-Hastings
algorithm below) is used to update within a Gibbs sampler, it never samples
from the full conditional distribution. Convergence using Metropolis steps,
then, would be expected to be slower than that for a regular Gibbs sampler.
Recall that the steps of the Gibbs sampler were fully determined by the
statistical model under consideration (since full conditional distributions
for well-deﬁned models are unique). By contrast, the Metropolis algorithm
aﬀords substantial ﬂexibility through the selection of the candidate density
q. This ﬂexibility can be a blessing and a curse: while theoretically we are
free to pick almost anything, in practice only a “good” choice will result in
suﬃciently many candidate acceptances. The usual approach (after θ has
been transformed to have support ℜk, if necessary) is to set
q(θ∗|θ(t−1)) = N(θ∗|θ(t−1), eΣ) ,
(2.18)
since this distribution obviously satisﬁes the symmetry property, and is
“self-correcting” (candidates are always centered around the current value
of the chain). Speciﬁcation of q then comes down to speciﬁcation of eΣ.
Here we might try to mimic the posterior variance by setting eΣ equal to an
empirical estimate of the true posterior variance, derived from a preliminary
sampling run.
The reader might well imagine an optimal choice of q would produce
an empirical acceptance ratio of 1, the same as the Gibbs sampler (and
with no apparent “waste” of candidates). However, the issue is rather more
subtle than this: accepting all or nearly all of the candidates is often the
result of an overly narrow candidate density. Such a density will “baby-
step” around the parameter space, leading to high acceptance but also high

BAYESIAN COMPUTATION
47
autocorrelation in the sampled chain. An overly wide candidate density will
also struggle, proposing leaps to places far from the bulk of the posterior’s
support, leading to high rejection and, again, high autocorrelation. Thus
the “folklore” here is to choose eΣ so that roughly 50% of the candidates are
accepted. Subsequent theoretical work (e.g., Gelman et al., 1996) indicates
even lower acceptance rates (25 to 40%) are optimal, but this result varies
with the dimension and true posterior correlation structure of θ.
As a result, the choice of eΣ is often done adaptively. For instance, in one
dimension (setting eΣ = eσ, and thus avoiding the issue of correlations among
the elements of θ), a common trick is to simply pick some initial value of
eσ, and then keep track of the empirical proportion of candidates that are
accepted. If this fraction is too high (75 to 100%), we simply increase eσ; if
it is too low (0 to 20%), we decrease it. Since certain kinds of adaptation
can actually disturb the chain’s convergence to its stationary distribution,
the simplest approach is to allow this adaptation only during the burn-in
period, a practice sometimes referred to as pilot adaptation. This is in fact
the approach currently used by WinBUGS, where the default pilot period is
4000 iterations.
As mentioned above, in practice the Metropolis algorithm is often found
as a substep in a larger Gibbs sampling algorithm, used to generate from
awkward full conditionals. Such hybrid Gibbs-Metropolis applications were
once known as “Metropolis within Gibbs” or “Metropolis substeps,” and
users would worry about how many such substeps should be used. Fortu-
nately, it was soon realized that a single substep was suﬃcient to ensure
convergence of the overall algorithm, and so this is now standard practice:
when we encounter an awkward full conditional (say, for θi), we simply
draw one Metropolis candidate, accept or reject it, and move on to θi+1.
Further discussion of convergence properties and implementation of hybrid
MCMC algorithms can be found in Tierney (1994) and Carlin and Louis
(2009, Sec. 3.4.4).
We end this subsection with the important generalization of the Metropo-
lis algorithm devised by Hastings (1970). In this variant we drop the re-
quirement that q be symmetric in its arguments, which is often useful for
bounded parameter spaces (say, θ > 0) where Gaussian proposals as in
(2.18) are not natural.
Algorithm 2.3 (Metropolis-Hastings Algorithm).
In Step 2 of the Metropolis algorithm, replace the acceptance ratio r by
r =
h(θ∗)q(θ(t−1) | θ∗)
h(θ(t−1))q(θ∗| θ(t−1))
.
(2.19)
Then again under mild conditions, θ(t) approx
∼
p(θ|y)
In practice we sometimes set q(θ∗| θ(t−1)) = q(θ∗), i.e., we use a pro-
posal density that ignores the current value of the variable. This algorithm

48
BASICS OF BAYESIAN INFERENCE
is sometimes referred to as a Hastings independence chain, so named be-
cause the proposals (though not the ﬁnal θ(t) values) form an independent
sequence. While easy to implement, this algorithm can be diﬃcult to tune
since it will converge slowly unless the chosen q is rather close to the true
posterior (which is of course unknown in advance).
2.3.3 Convergence diagnosis
The most problematic part of MCMC computation is deciding when it is
safe to stop the algorithm and summarize the output. This means we must
make a guess as to the iteration t0 after which all output may be thought
of as coming from the true stationary distribution of the Markov chain
(i.e., the true posterior distribution). The most common approach here is
to run a few (say, m = 3 or 5) parallel sampling chains, initialized at widely
disparate starting locations that are overdispersed with respect to the true
posterior. These chains are then plotted on a common set of axes, and these
trace plots are then viewed to see if there is an identiﬁable point t0 after
which all m chains seem to be “overlapping” (traversing the same part of
the θ-space).
Sadly, there are obvious problems with this approach. First, since the
posterior is unknown at the outset, there is no reliable way to ensure that
the m chains are “initially overdispersed,” as required for a convincing
diagnostic. We might use extreme quantiles of the prior p(θ) and rely on
the fact that the support of the posterior is typically a subset of that of the
prior, but this requires a proper prior and in any event is perhaps doubtful
in high-dimensional or otherwise diﬃcult problems. Second, it is hard to see
how to automate such a diagnosis procedure, since it requires a subjective
judgment call by a human viewer. A great many papers have been written
on various convergence diagnostic statistics that summarize MCMC output
from one or many chains that may be useful when associated with various
stopping rules; see Cowles and Carlin (1996) and Mengersen et al. (1999)
for reviews of many such diagnostics.
Among the most popular diagnostic is that of Gelman and Rubin (1992).
Here, we run a small number (m) of parallel chains with diﬀerent starting
points thought to be initially overdispersed with respect to the true pos-
terior. (Of course, before beginning there is technically no way to ensure
this; still, the rough location of the bulk of the posterior may be discernible
from known ranges, the support of the (proper) prior, or perhaps a prelim-
inary posterior mode-ﬁnding algorithm.) The diagnostic is then based on a
comparison of the variance between the m chains and the variance within
them, two quantities that should be comparable if the chains are in rough
agreement regarding the location of the posterior. This approach is fairly
intuitive and is applicable to output from any MCMC algorithm. However,
it focuses only on detecting bias in the MCMC estimator; no information

BAYESIAN COMPUTATION
49
about the accuracy of the resulting posterior estimate is produced. It is
also an inherently univariate quantity, meaning it must be applied to each
parameter (or parametric function) of interest in turn, although Brooks
and Gelman (1998) extend the Gelman and Rubin approach in three im-
portant ways, one of which is a multivariate generalization for simultaneous
convergence diagnosis of every parameter in a model.
While the Gelman-Rubin-Brooks and other formal diagnostic approaches
remain popular, in practice very simple checks often work just as well and
may even be more robust against “pathologies” (e.g., multiple modes) in
the posterior surface that may easily fool some diagnostics. For instance,
sample autocorrelations in any of the observed chains can inform about
whether slow traversing of the posterior surface is likely to impede con-
vergence. Sample cross-correlations (i.e., correlations between two diﬀerent
parameters in the model) may identify ridges in the surface (say, due to
collinearity between two predictors) that will again slow convergence; such
parameters may need to be updated in multivariate blocks, or one of the
parameters may need to be dropped from the model altogether. Combined
with a visual inspection of a few sample trace plots, the user can at least
get a good feeling for whether posterior estimates produced by the sampler
are likely to be reliable.
2.3.4 Variance estimation
An obvious criticism of Monte Carlo methods generally is that no two an-
alysts will obtain the same answer, since the components of the estimator
are random. This makes assessment of the variance of these estimators cru-
cial. Combined with a central limit theorem, the result would be an ability
to test whether two Monte Carlo estimates were signiﬁcantly diﬀerent. For
example, suppose we have a single chain of N post-burn-in samples of a
parameter of interest λ, so that our basic posterior mean estimator (2.16)
becomes ˆE(λ|y) = ˆλN =
1
N
PN
t=1 λ(t). Assuming the samples comprising
this estimator are independent, a variance estimate for it would be given
by
d
V ariid(ˆλN) = s2
λ/N =
1
N(N −1)
N
X
t=1
(λ(t) −ˆλN)2 ,
(2.20)
i.e., the sample variance, s2
λ =
1
N−1
PN
t=1(λ(t) −ˆλN)2, divided by N. But
while this estimate is easy to compute, it would very likely be an under-
estimate due to positive autocorrelation in the MCMC samples. One can
resort to thinning, which is simply retaining only every kth sampled value,
where k is the approximate lag at which the autocorrelations in the chain
become insigniﬁcant. However, MacEachern and Berliner (1994) show that
such thinning from a stationary Markov chain always increases the vari-
ance of sample mean estimators, and is thus suboptimal. This is intuitively

50
BASICS OF BAYESIAN INFERENCE
reminiscent of Fisher’s view of suﬃciency: it is never a good idea to throw
away information (in this case, (k −1)/k of our MCMC samples) just to
achieve approximate independence among those that remain.
A better alternative is to use all the samples, but in a more sophisticated
way. One such alternative uses the notion of eﬀective sample size, or ESS
(Kass et al. 1998, p. 99). ESS is deﬁned as
ESS = N/κ(λ) ,
where κ(λ) is the autocorrelation time for λ, given by
κ(λ) = 1 + 2
∞
X
k=1
ρk(λ) ,
(2.21)
where ρk(λ) is the autocorrelation at lag k for the parameter of interest λ.
We may estimate κ(λ) using sample autocorrelations estimated from the
MCMC chain. The variance estimate for ˆλN is then
d
V arESS(ˆλN) = s2
λ/ESS(λ) =
κ(λ)
N(N −1)
N
X
t=1
(λ(t) −ˆλN)2 .
Note that unless the λ(t) are uncorrelated, κ(λ) > 1 and ESS(λ) < N, so
that d
V arESS(ˆλN) > d
V ariid(ˆλN), in concert with intuition. That is, since
we have fewer than N eﬀective samples, we expect some inﬂation in the
variance of our estimate.
In practice, the autocorrelation time κ(λ) in (2.21) is often estimated
simply by cutting oﬀthe summation when the magnitude of the terms ﬁrst
drops below some “small” value (say, 0.1). This procedure is simple but may
lead to a biased estimate of κ(λ). Gilks et al. (1996, pp. 50–51) recommend
an initial convex sequence estimator mentioned by Geyer (1992) which,
while still output-dependent and slightly more complicated, actually yields
a consistent (asymptotically unbiased) estimate here.
A ﬁnal and somewhat simpler (though also more naive) method of es-
timating V ar(ˆλN) is through batching. Here we divide our single long run
of length N into m successive batches of length k (i.e., N = mk), with
batch means B1, . . . , Bm. Clearly ˆλN = ¯B = 1
m
Pm
i=1 Bi. We then have the
variance estimate
d
V arbatch(ˆλN) =
1
m(m −1)
m
X
i=1
(Bi −ˆλN)2 ,
(2.22)
provided that k is large enough so that the correlation between batches
is negligible, and m is large enough to reliably estimate V ar(Bi). It is
important to verify that the batch means are indeed roughly independent,
say, by checking whether the lag 1 autocorrelation of the Bi is less than
0.1. If this is not the case, we must increase k (hence N, unless the current
m is already quite large), and repeat the procedure.

HIERARCHICAL MODELING AND METAANALYSIS
51
Regardless of which of these estimates ˆV is used to approximate V ar(ˆλN),
a 95% conﬁdence interval for E(λ|y) is then given by
ˆλN ± z.025
p
ˆV ,
where z.025 = 1.96, the upper .025 point of a standard normal distribution.
If the batching method is used with fewer than 30 batches, it is a good idea
to replace z.025 by tm−1,.025, the upper .025 point of a t distribution with
m −1 degrees of freedom. WinBUGS oﬀers both naive (2.20) and batched
(2.22) variance estimates; this software is illustrated in the next section.
2.4 Hierarchical modeling and metaanalysis
This section has two purposes. One is to introduce the notions and method-
ologies of hierarchical modeling. Another is to give a rather detailed devel-
opment of metaanalysis (Berry and Stangl, 2000) as an application of this
methodology. The larger ideas of hierarchical modeling are developed in
the context of this application. Other applications include the analysis of
multicenter trials, multiple comparisons, variable selection in regression,
subgroup or subset analyses, and pharmacokinetic modeling; see e.g. Sec-
tion 4.5.
Metaanalysis is a single analysis of multiple studies. Diﬀerent studies
have diﬀerent results. One reason is random variation. Other diﬀerences are
inherent: diﬀerent studies deal with diﬀerent types of patients and diﬀerent
types of controls, they take place at diﬀerent times and locations, etc. For
example, some institutions may study healthier populations than others.
But even when patient eligibility criteria are identical in two studies, the
respective investigators may apply the criteria diﬀerently. For example, for
two studies involving a particular device, both sets of eligibility criteria may
include patients who have a left ventricular ejection fraction (LVEF) as low
as 25%. However, one investigator may admit essentially every candidate
patient who meets this criterion, while another worries that some patients
with LVEF lower than 35% may be at unusual or unnecessary risk if treated
with the experimental device. Therefore, the patients in the ﬁrst study will
tend to have a higher degree of heart failure, and the overall results in
the ﬁrst study may suggest that the device is less eﬀective than in the
second. Since LVEF is a rather obvious covariate, this circumstance is easy
to address by accounting for LVEF in the analysis (Bayesian or frequentist).
But there may be important covariates that diﬀer in the two studies that
have not been measured.
It is not uncommon for diﬀerent studies to evince diﬀerent treatment
eﬀects. Sometimes it is possible to account for patient diﬀerences using
measurable covariates and sometimes it is not. In a Bayesian hierarchical
approach, the study is one level of experimental unit, and patients within
studies represent a second level of experimental unit. (Levels higher than

52
BASICS OF BAYESIAN INFERENCE
study, such as country or study type, can also be included in a hierarchi-
cal model, but for the moment we do not consider more than two levels.)
Characteristics of studies are unknown. In the Bayesian approach, all un-
knowns have probability distributions, so a Bayesian metaanalysis employs
a random eﬀects model.
Think of a study in a metaanalysis as having a distribution of patient
responses that is speciﬁc to the study. Selecting a study means selecting
one of these distributions. If the distribution of the selected study were
to be revealed, this would give direct information about the study distri-
butions, and result in a standard statistics problem. But since each study
contributes only a ﬁnite number of patients, the selected study distribu-
tions are not completely revealed; instead, one can observe only a sample
from each study’s distribution. This gives indirect information about the
distribution of study distributions. While it may seem strange to say “dis-
tribution of distributions,” not only is this correct, it is an essential aspect
of the approach.
Consider a simple analogy. A bag contains several thousand coins. These
coins may have diﬀerent propensities for producing heads. To get informa-
tion about the distribution of these propensities among the coins in the
bag, we select 10 coins at random, and toss each of them a total of 30
times. The data consist of 10 sample proportions of heads. If the observed
proportions of heads are wildly diﬀerent then it would seem that not all
the coins in the bag have the same propensity for heads. However, because
of sampling variability the sample proportions among the 10 coins tends
to overestimate the dispersion in the population of coins. Further, if the
sample proportions are quite similar then the coins in the bag may have
similar propensities for heads. In any case, the sample proportions give in-
formation about the distribution of propensities of heads among the coins
in the bag. Moreover, the results for one coin contain information about
the propensity of heads for the other coins. This is because one coin’s sam-
ple proportion gives information about the coins in the bag, and the other
coins are themselves selected from the bag.
Example 2.7 (metaanalysis for a single success proportion). Consider the
case of a single treatment. Table 2.1 gives numbers of successes (xi) and
numbers of patients (ni) for nine studies. A “success” is a response to an
antidepressant drug (Janicak et al., 1988), but one could just as easily think
of these as 9 studies (or 9 experiments) concerning the eﬀectiveness of a
medical device. Suppose that within study i the experimental units (pa-
tients, say) are exchangeable in the sense that all have the same propensity
pi of success. (For an example of a Bayesian analysis in the presence of
diﬀering prognoses, see Berry, 1989.)

HIERARCHICAL MODELING AND METAANALYSIS
53
study
xi
ni
ˆpi = xi/ni
1
20
20
1.00
2
4
10
0.40
3
11
16
0.69
4
10
19
0.53
5
5
14
0.36
6
36
46
0.78
7
9
10
0.90
8
7
9
0.78
9
4
6
0.67
total
106
150
0.71
Table 2.1 Successes and total numbers of patients in 9 studies.
The likelihood function is
L(p1, p2, . . . , p9) ∝
9
Y
i=1
pxi
i (1 −pi)ni−xi .
A combined analysis would assume that all 150 patients are exchange-
able, so that the nine pi are equal – say, with common value p. The likeli-
hood function of p would then be
L(p) ∝p106(1 −p)44 ,
which is shown in Figure 2.6. (The nine vertical bars in this ﬁgure corre-
spond to the observed proportions for the nine studies, with bar heights
proportional to sample sizes.) This ﬁgure shows that p is very likely to be
between 0.6 and 0.8. This conclusion is somewhat curious since, as shown
by the vertical bars on the p-axis in Figure 2.6, the observed success propor-
tions in 5 of the 9 studies are outside this range. While sampling variability
accounts for some of the diﬀerences among the sample proportions, the vari-
ability in Table 2.1 is greater than would be expected from sampling alone;
this variability suggests that the pi may not be equal.
Separate analysis of the 9 studies is even less satisfactory than combining
all 9 studies. The eﬀect of an experimental treatment is not well addressed
by giving nine diﬀerent likelihood functions, or by giving nine diﬀerent
conﬁdence intervals. Consider the probability of success if the treatment
were used with another patient, say one in a tenth study. How should the
results from these 9 studies be weighed? Or, suppose that the focus is on a
particular study, say study 9. How should the other 8 studies be weighed
in estimating p9?

54
BASICS OF BAYESIAN INFERENCE
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
p
density
Figure 2.6 Likelihood function L(p) assuming that pi = p for all i. This likeli-
hood, p106(1 −p)44, is proportional to a Beta(107, 45) density. The nine vertical
bars correspond to the observed proportions for the nine studies, with bar heights
proportional to sample sizes.
From a hierarchical Bayesian perspective, each study’s success propensity
is viewed as having been selected from some larger population. Therefore,
to use Bayes’ Theorem requires a probability distribution of population
distributions. Suppose p1, . . . , p9 is a random sample from population dis-
tribution F which is itself random. Assume that F is a beta distribution
with parameters a and b, where a and b are unknown. That is, assume that
an observation p from F has beta density,
B(a, b) pa−1(1 −p)b−1,
where a > 0 and b > 0, and where
(B(a, b))−1 =
Z 1
0
pa−1(1 −p)b−1dp .
That is, B(a, b) is the normalizing constant of the beta density. Referred
to as the beta function, it can be constructed from the gamma function as
B(a, b) =
Γ(a, b)
Γ(a)Γ(b) .
Assuming that F is a beta distribution is a restriction. This assumption

HIERARCHICAL MODELING AND METAANALYSIS
55
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 1 , 1 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 1 , 2 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 1 , 3 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 1 , 4 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 2 , 1 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 2 , 2 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 2 , 3 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 2 , 4 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 3 , 1 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 3 , 2 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 3 , 3 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 3 , 4 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 4 , 1 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 4 , 2 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 4 , 3 )
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
p
density
Beta( 4 , 4 )
Figure 2.7 Beta(a, b) densities for a, b = 1, 2, 3, 4.
means a small number of parameters (two for the beta) index the distribu-
tion. Any other two-dimensional family would work as well computation-
ally. The beta family has the pleasant characteristic that it represents a
variety of types and shapes of distributions. However, beta distributions
are either unimodal or bimodal, with the latter applying only if the two
modes are 0 and 1 (the case with a and b both less than 1).
Another type of restriction made in this section is the assumption that
p1, . . . , p9 arise from the same distribution. More generally, studies may be
more closely related to each other within subsets. For example, perhaps
p1, . . . , p5 might be viewed as arising from one distribution but p6, . . . , p9
viewed as taken from a second distribution, with the relationship between
the distributions being modeled.
Figure 2.7 shows several densities in the larger family of beta densities.
The layout of the ﬁgure has increasing a from top to bottom and increasing

56
BASICS OF BAYESIAN INFERENCE
b from left to right. Beta densities not shown in this ﬁgure include those
having a or b larger than 4, and those having fractional values of a and b.
The mean of p for given a and b is a/(a + b), and the variance is
ab
(a + b)2(a + b + 1) .
This tends to 0 as a + b tends to inﬁnity. So in a sense, a + b measures
homogeneity among studies. If a + b is large then the distribution of the pi
will be highly concentrated (near the mean, a/(a + b)), and consequently
the diﬀerences among studies will be slight. On the other hand, if a + b is
small then the pi will vary from study to study, and there will be a large
study eﬀect.
As is the case for any unknown in the Bayesian approach, the user chooses
a prior probability distribution for a and b; call it π(a, b). If available infor-
mation suggests homogeneity among studies then much of the probability
under p can be concentrated on large values of a and b, whereas if this
information allows for the possibility of heterogeneity then much of the
probability under π can be placed on small values of a and b. If there is
little information to suggest one or the other then both large and small
values of a + b should be assigned some prior probability, and indeed this
will be the typical case.
Consider a generic observation, say p, from F. While it is not possible
to observe p, temporarily suppose that this is possible. Call π′(a, b|p) the
posterior distribution of (a, b) given p. From Bayes’ Theorem (2.1),
π′(a, b|p) ∝B(a, b)pa−1(1 −p)b−1π(a, b) .
Extending this to the observation of a sample p1, . . . , p9 from F:
π′(a, b|p1, . . . , p9) ∝
9
Y
i=1
©
B(a, b)pa−1
i
(1 −pi)b−1ª
π(a, b) .
The more realistic case is that indirect information about p1, . . . , p9 is avail-
able by observing x1, . . . , x9, where the xi are binomial variables with pa-
rameters ni and pi, respectively. Consider a single observation x having
parameters n and p. Such an observation contains only indirect informa-
tion about F. Call π∗(a, b|x) the posterior distribution of a and b given x
and n. From Bayes’ Theorem,
π∗(a, b|x) ∝f(x|a, b)π(a, b),
where
f(x|a, b)
=
Z 1
0
µn
x
¶
px(1 −p)n−xB(a, b)pa−1(1 −p)b−1dp
=
µn
x
¶
B(a, b)
B(a + x, b + n −x) .

HIERARCHICAL MODELING AND METAANALYSIS
57
Therefore,
π∗(a, b|x) ∝
B(a, b)
B(a + x, b + n −x) π(a, b) .
Upon observing a sample x1, . . . , x9, where p1, . . . , p9 is a random sample
from F, the joint posterior density of a and b is
π∗(a, b|x1, . . . , x9) ∝
9
Y
i=1
½
B(a, b)
B(a + xi, b + ni −xi)
¾
π(a, b) .
As each ni →∞the limit of this expression is π′, setting pi equal to xi/ni.
This limiting equivalence is a law-of-large-numbers phenomenon and cor-
responds to the intuitive notion that an inﬁnite amount of sample evidence
about pi is equivalent to observing pi.
Now consider the response of an as yet untreated patient. First suppose
the patient is treated at one of the 9 studies considered in Table 2.1. Given
the results in that table, the probability of success for a patient treated in
study i, for i = 1, . . . , 9, is the posterior expected value of pi,
E(pi|x1, . . . , x9) = E
µ
a + xi
a + b + ni
x1, . . . , x9
¶
.
This expectation is with respect to distribution π∗. That is, (a + xi)/(a +
b + ni) is calculated for the various possible values of a and b and weighed
by the posterior distribution of a and b given the data. This formula applies
as well for a patient treated in a new study, say study 10, by taking i = 10:
the patient’s probability of success is the expected posterior mean of p10,
E(p10|x1, . . . , x9) = E
µ
a
a + b x1, . . . , x9
¶
.
(2.23)
To implement our Bayesian solution, we require a speciﬁc choice of the
prior density π(a, b). Applied Bayesians often attempt to choose the least
informative prior available, at least in the initial phases of an analysis. This
may be because it will typically produce answers that are not wildly incon-
sistent with those from traditional frequentist methods; note for instance
that the posterior mode under a “ﬂat” (uniform) prior is the same as the
maximum likelihood estimate (MLE). Unfortunately, such priors are often
improper, meaning that they do not themselves deﬁne a valid probability
speciﬁcation. An obvious example in our case is the bivariate uniform prior,
π(a, b) ∝1 for a, b > 0 .
(2.24)
This prior is “noninformative” in the sense that it does not favor any single
pair of (a, b) values over any other; all receive the same a priori credibility.
But this prior clearly does not integrate to 1 (or any ﬁnite number) over
the entire domain for a and b. As mentioned above, such priors may still be
acceptable if the resulting posterior distribution remains proper, but unfor-
tunately that is not the case here: Hadjicostas (1998) shows that the joint

58
BASICS OF BAYESIAN INFERENCE
posterior for (a, b) is improper under the unbounded ﬂat prior (2.24). (See
further discussion on this point below, as well as Natarajan and McCul-
loch, 1995, for similar diﬃculties with improper hyperpriors when the beta
prior is replaced by a normal on the logit(pi).) A more sensible alternative
is given by Gelman et al. (2004, p.128), who recommend reparametrizing
from (a, b) to (µ, η) where µ = a/(a+b), the prior mean, and η = 1/
√
a + b,
approximately the prior standard deviation, and placing independent uni-
form priors on both quantities.
In what follows, we simply truncate the range of the joint uniform prior
(2.24) so it can be restandardized to a proper joint uniform distribution.
That is, we suppose
π(a, b) ∝1 for 0 ≤a, b ≤10 .
(2.25)
This distribution associates some probability with a + b large and some
with a + b small, and it gives a moderate amount of probability to nearly
equal a and b (meaning that there is a moderate amount of probability on
p’s near 1/2).
This model is straightforwardly implemented via the WinBUGS package.
WinBUGS is a freely available program developed by statisticians and prob-
abilistic expert systems researchers at the Medical Research Council Bio-
statistics Unit at the University of Cambridge, England. In a nutshell,
it allows us to draw samples from any posterior distribution, freeing us
from having to worry overmuch about the integral in (2.1). This allows
us instead to focus on the statistical modeling, which is after all our pri-
mary interest. WinBUGS uses syntax very similar to that of R, and in fact
can now be called from R using the BRugs package, a subject to which
we return below. As of the current writing, the latest version of WinBUGS
may be downloaded from www.mrc-bsu.cam.ac.uk/bugs/welcome.shtml.
Once installed, a good way to learn the basics of the language is to fol-
low the tutorial: click on Help, pull down to User Manual, and then click
on Tutorial. Perhaps even more easily, one can watch “WinBUGS – The
Movie,” a delightful Flash introduction to running the software available at
www.statslab.cam.ac.uk/∼krice/winbugsthemovie.html. To gain prac-
tice with the language, the reader may wish to turn to the ample col-
lection of worked examples available within WinBUGS by clicking on Help
and pulling down to Examples Vol I or Examples Vol II. See Carlin and
Louis (2009, Chapter 2) for other step-by-step illustrations of various com-
mon statistical models implemented in WinBUGS.
WinBUGS solutions to Bayesian hierarchical modeling problems require
three basic elements: (1) some BUGS code to specify the statistical model,
(2) the data, and (3) initial values for the MCMC sampling algorithm. For
our binomial-beta-uniform model, these three components can be speciﬁed
in WinBUGS as follows:
BUGS code
model{

HIERARCHICAL MODELING AND METAANALYSIS
59
for( i in 1:I) {
x[i] ~ dbin(p[i] , n[i])
p[i] ~ dbeta(a,b)
}
a ~ dunif(0,10)
b ~ dunif(0,10)
#
a ~ dgamma(2,2)
#
b ~ dgamma(2,2)
}
#
end of BUGS code
# Data:
list(x = c(20, 4, 11, 10, 5, 36, 9, 7, 4, NA),
n = c(20, 10, 16, 19, 14, 46, 10, 9, 6, 1), I=10)
# Inits:
list(a=4, b=2, p = c(.5, .5, .5, .5, .5, .5, .5, .5, .5, .5))
Everything after a # sign in WinBUGS is interpreted as a comment, so the
Gamma(2, 2) priors for a and b are not operative in this code. The data
and inits are both being read in using list format (as also found in R),
but this is not necessary; traditional columnar data stored in .txt ﬁles are
also perfectly acceptable to WinBUGS. Note we deliberately expand the x
vector with an extra, tenth entry that is set equal to NA (missing value).
WinBUGS then treats this “missing data” value like another unknown in the
model, and samples it according to its full conditional distribution, which
in this case is simply the binomial likelihood given n10 (set to 1) and the
also-imputed value of p10. Finally, the initial values (“inits”) are chosen
to be “in the ballpark” but not really provide a convergence challenge
for the model. Were this a more complicated and/or higher-dimensional
model with correspondingly slower MCMC convergence, we would likely
experiment with more extreme initial values (say, pi closer to 0 and 1) that
would provide a greater challenge to the algorithm.
WinBUGS features an intuitive point-and-click interface that is quite user
friendly and perhaps best for initial runs and code testing. However, it
becomes tedious once the model and its convergence are well understood,
and we wish to investigate a signiﬁcant number of diﬀerent priors, models,
etc. A useful tool in this regard is the BRugs function, which enables one
to write a “script” of commands that are easily stored and then called
(perhaps repeatedly) from R. We now oﬀer a collection of BRugs commands
to implement the WinBUGS analysis just described. We begin by loading the
package, reading in the data, and writing it to disk for subsequent use by
BUGS:
BRugs code
install.packages("BRugs")
library(BRugs)
x <- c(20, 4, 11, 10, 5, 36, 9, 7, 4, NA)

60
BASICS OF BAYESIAN INFERENCE
n <- c(20, 10, 16, 19, 14, 46, 10, 9, 6, 1)
I <- 10
dput(pairlist(x=x,n=n,I=I),"betabinHM_data.txt")
dput(pairlist(a=4,b=2,p=c(.5,.5,.5,.5,.5,.5,.5,.5,.5,.5)),
"betabinHM_inits.txt")
Note the use of dput command to write out the data and inits ﬁles; these
ﬁles can of course be created externally to R as well. Next we issue four
BRugs commands corresponding to the Model - Specification menu in
WinBUGS:
BRugs code
modelCheck("betabinHM_BUGS.txt")
modelData(paste("betabinHM_data.txt",sep=""))
modelCompile(numChains=1)
modelInits("betabinHM_inits.txt")
modelGenInits()
# generates an initial value for the missing x
Finally, we run a 1000-iteration burn-in period, set the parameters of in-
terest into memory, run 10,000 more production iterations, and request
histories, density estimates, and summary statistics for each:
BRugs code
modelUpdate(1000)
samplesSet(c("a","b","p"))
modelUpdate(10000)
samplesHistory("*", mfrow = c(3, 2))
samplesDensity("*")
samplesStats(c("a","b","p"))
The posterior samples of p[10] will be those from the predictive distribu-
tion of interest, π(p10|x).
Estimating the joint posterior of a and b is easily done via a plot of their
matched Gibbs pairs:
BRugs code
par(mfrow=c(1,1))
plot(samplesSample("a"),samplesSample("b"),xlab="a",ylab="b")
Figure 2.8 shows the resulting plot of the 10,000 posterior sampled (a, b)
pairs, providing a reasonably good idea of the joint posterior distribution.
The posterior is “wedge-shaped” with increasing variability for larger a
and b. The apparent truncation at a = 10 suggests that the data could
support values of a even larger than 10. However, rerunning this model after
expanding the upper bounds of the a and b domains to 100 or 1000 in prior
(2.25) produces the same “truncated wedge” posterior. The implication is
that the posterior would indeed be improper in the limiting case of the
improper joint uniform prior (2.24). In what follows we carry on with the
prior in (2.25), but emphasize that other proper priors (say, a and b assumed
to be independent Gamma(2, 2) variables, or the aforementioned Gelman
et al. prior) may produce more defensible results. We also note that the
posterior draws are centered near the line a/(a+b) = 0.68, corresponding to

HIERARCHICAL MODELING AND METAANALYSIS
61
2
4
6
8
10
0
2
4
6
8
10
a
b
Figure 2.8 Samples from the joint posterior distribution π∗(a, b|x) for the data
shown in Table 2.1.
the posterior mean of the predictive probability (see Table 2.2). Densities
with maximal posterior probabilities are essentially dispersed about this
line.
The solid line in Figure 2.9 shows the mean posterior density of a generic
p; the likelihood under the “all pi’s equal” assumption (dashed line) and
study-speciﬁc estimates (vertical bars) from Figure 2.6 are repeated for
easy comparison. BRugs code to draw this ﬁgure is as follows:
BRugs code
p <- seq(0,1,length=401)
phat <- c(1, .4, .69, .53, .36, .783, .9, .778, .67)
n <- n[1:9]
samp <- samplesSample("p[10]")
plot(p,dbeta(p,107,45),type="l",lty=2,lwd=2,ylab="density")
lines(density(samp,bw=.04,from=0,to=1),lty=1,lwd=2)
lines(phat,n/12, type = "h", col = "red", lwd=5)
legend(.1,10,c("posterior","likelihood (p_i’s equal)"),lty=1:2)
Note the all-pi’s-equal likelihood is exactly a beta density, drawn using the
dbeta command, but the posterior is a kernel density smooth (via density)
of the p10 posterior samples produced earlier and stored in samp. Mathe-
matically, the posterior is an average of beta densities, where the weights
are π∗(a, b). The mean of p for this density is 0.68, which can be found using

62
BASICS OF BAYESIAN INFERENCE
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
p
density
posterior
likelihood (all p_i’s equal)
Figure 2.9 Mean posterior density of p for the data shown in Table 2.1 (likelihood
function from Figure 2.6 shown as dotted line).
the formula for E(p10|x) as given in (2.23), or simply estimated by aver-
aging all 10,000 Gibbs samples for p10. Clearly, the variability suggested
by this density estimate is greater than that in the all-pi’s-equal likelihood
(dashed line; solid line in Figure 2.6). Again, we caution that restricting a
and b to be no greater than 10 slightly overemphasizes heterogeneity, but
replacing our truncated uniform hyperpriors with, say, gamma hyperpriors
does not free us from having to think carefully a priori about the likely
range and relative magnitudes of a and b.
Table 2.2 repeats Table 2.1, but with an extra column: the probability
of success for the next patient in the corresponding study – including in a
new, tenth study – which is the overall mean and is shown as the column
total. The individual study probabilities are in eﬀect shrunk toward the
overall mean (“borrowing strength”), with greater shrinkage for smaller
studies. For example, the estimated success probability for study 1 (0.90)
is less than the observed success proportion (1.00) because the latter is
relatively large. In the other direction, the estimated success proportion
for study 5 (0.48) is larger than the observed proportion (0.36) because the
latter is relatively small. In both cases the shrinkage toward the middle is
an instance of the “regression eﬀect” or “regression to the mean” (Berry,
1996, Sec. 14.3). The amount of shrinkage depends on the distances from

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
63
study
xi
ni
ˆpi = xi/ni
pred. prob.
1
20
20
1.00
0.90
2
4
10
0.40
0.53
3
11
16
0.69
0.69
4
10
19
0.53
0.57
5
5
14
0.36
0.48
6
36
46
0.78
0.77
7
9
10
0.90
0.80
8
7
9
0.78
0.73
9
4
6
0.67
0.68
total
106
150
0.71
0.68
Table 2.2 Predictive probability of success by study.
the observed proportions to the mean and also on the sizes of the studies,
with greater shrinkage for smaller studies. (The dependence on study size
is not very clear in Table 2.2 because the study sample sizes are quite
comparable.)
The predictive probability for study 10 (0.68 for “total” in Table 2.2) is
diﬀerent from the overall success proportion (0.71). The prior distribution
is symmetric in a and b and so the prior probability of success is 0.50 (this
being the average of a/(a + b) over the various distributions considered in
the prior). The overall predictive probability of success is an average of
0.71 and 0.50, although calculating this average is rather complicated. Just
as for the case of normal sampling in Example 2.2 for which the posterior
mean is an average of the prior and sample means, this shrinkage toward
the prior mean wears oﬀfor larger numbers of studies.
2.5 Principles of Bayesian clinical trial design
The use of Bayesian statistical methods is somewhat less controversial when
matters turn to experimental design. This is because in order to carry out
a sample size calculation, a trial designer must have and use some pre-
existing knowledge (or what Bayesians would call prior opinion) regarding
the likely eﬀect of the treatment and its variability. More formally, all
evaluations at the design stage are preposterior; i.e., they involve integrating
over uncertainty in both the as-yet-unobserved data (a frequentist act)
and the unobservable parameters (a Bayesian act). In the terminology of
Rubin (1984), this double integration is a “Bayesianly justiﬁable frequentist
calculation.”

64
BASICS OF BAYESIAN INFERENCE
While the Bayesian advantages of ﬂexibility and borrowing of strength
(both from previous data and across subgroups) have been well-known to
clinical trialists for some time, they have proven elusive to obtain in practice
due to the diﬃculty in converting historical information into priors, and in
computing the necessary posterior summaries. Still, pressure to minimize
the ﬁnancial and ethical cost of clinical trials encourages greater devel-
opment and use of Bayesian thinking in their design and analysis. In the
case of medical device trials, where data are often scanty and expensive,
Bayesian methods already make up roughly 10% of new device approvals
(Berry, 2006). While the area of drug trials has been slower to embrace
the methods, even here they are gaining traction thanks to their ability
to readily incorporate early stopping for safety or futility, as well as easily
handle complications such as multiple endpoints or random eﬀects. Bayes
is also an especially natural approach for incorporating historical controls
into the analysis (Section 6.1), an area for which the classical frequentist
literature is very limited (though see Pocock, 1976; Prentice et al., 2006;
and Neaton et al., 2007, for notable exceptions).
In this section, we outline the basics of Bayesian clinical trial design and
analysis, and illustrate a general method for Bayesian sample size calcula-
tions using BRugs. This function’s ability to call BUGS from R (as already
seen in Example 2.7) allows us to repeatedly estimate the posterior given
various artiﬁcial data samples, and hence simulate the Bayesian and fre-
quentist operating characteristics (power and Type I error rates) of our
Bayesian designs.
2.5.1 Bayesian predictive probability methods
The ﬁrst two authors of this book have been strong and consistent advo-
cates for the use of predictive probabilities in making decisions based on
accumulating clinical trial data. Such an outlook is helpful in cases where,
perhaps due to especially acute ethical concerns, we are under pressure
to terminate trials of ineﬀective treatments early (say, because the treat-
ment is especially toxic or expensive). The basic idea is to compute the
probability that a treatment will ever emerge as superior given the patient
recruitment outlook and the data accumulated so far; if this probability
is too small, the trial is stopped. In the past, frequentists have sometimes
referred to this as stochastic curtailment; applied Bayesians have instead
tended to use the phrase stopping for futility. In this subsection we provide
only the briefest outline of the main ideas; see Sections 4.2 and 5.2 for full
details and illustrations.
To ﬁx ideas, consider again the simple binomial case where each patient
i is either a success on the study treatment (Yi = 1) or a failure (Yi = 0).
Assuming the patients are independent with common success probability
p, we obtain the familiar binomial likelihood for X = P
i Yi. Now suppose

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
65
No AE
AE
Total
Count
110
7
117
(%)
(94)
(6)
Table 2.3 Historical AE data, Safety Study A.
we have observed n1 patients to date, of which X1 have been successes,
so that X1 ∼Bin(n1, p). Under a conjugate Beta(a, b) prior for p, we of
course obtain a Beta(X1 + a, n1 −X1 + b) posterior for p. Inference and
decision making would now arise via the usual posterior summaries.
Now suppose that the trial has yet to reach a deﬁnitive conclusion, and
we wish to decide whether or not to randomize an additional n2 statisti-
cally independent patients into the protocol. Because we know Bayes’ Rule
may be used sequentially in this case, the current Beta(X1 +a, n1 −X1 +b)
posterior now serves as the prior for p, to be combined with a Bin(n2, p)
likelihood for X2. Posterior inference would now focus on the resulting
Beta(X1 + X2 + a, n1 + n2 −X1 −X2 + b) updated posterior. The pre-
dictive point of view argues that the appropriate calculation at this point
is to sample values p∗
j from the “prior” (actually, the interim posterior)
Beta(X1 + a, n1 −X1 + b), followed by fake data values X∗
2j repeatedly
from the Bin(n2, p∗
j) likelihood. Repeating this process for j = 1, . . . , Nrep
produces the collection of posterior predictive distributions
p(θ∗|X1, X∗
2j) = Beta(X1 + X∗
2j + a, n1 + n2 −X1 −X∗
2j + b) .
Inference is now based on an appropriate summary of these distributions.
Example 2.8 Suppose a medical device company wishes to run a safety
study on one of its new cardiac pacemakers. Speciﬁcally, the company
wishes to show that men receiving its new product will be very likely to
be free from adverse events (AEs) during the three months immediately
following implantation of the device. (Here, “adverse events” are limited
to those for which the device is directly responsible, and which require ad-
ditional action by the implanting physician.) Letting p be the probability
a patient does not experience an AE in the ﬁrst three months, we seek a
95% equal-tail Bayesian conﬁdence interval for p, (p.025, p.975). Suppose our
trial protocol uses the following decision rule:
Device is safe from AEs at 3 months ⇐⇒p.025 > 0.85 .
That is, if the lower conﬁdence bound for the chance of freedom from AEs
is at least 85%, the trial succeeds; otherwise it fails.
Now suppose we already have a preliminary study, Study A, whose re-
sults are given in Table 2.3. In our above notation, we have X1 = 110 and

66
BASICS OF BAYESIAN INFERENCE
n1 = 117. Our task is now to evaluate whether it is worth running a second
study, Study B, which would enroll an additional n2 patients. If we begin
with a Uniform(0, 1) = Beta(1, 1) prior for p, the interim posterior is then
Beta(X1 + a, n1 −X1 + b) = Beta(111, 8). Sampling p∗
j values from this
prior followed by potential Study B values X∗
2j from the Bin(n2, p∗
j) likeli-
hood produces the necessary Beta(111+X∗
2j, 8+n2 −X∗
2j) posteriors and,
hence, simulated lower conﬁdence limits p∗
.025,j from the posterior predic-
tive distribution for j = 1, . . . , Nrep. The empirical predictive probability
of trial success is then
bP(p.025 > 0.85) = number of p∗
.025,j > 0.85
Nrep
.
(2.26)
If this number is less than some prespeciﬁed cutoﬀ(say, 0.70), the trial
would be declared futile at this point, and it would be abandoned without
randomizing the additional n2 patients.
2.5.2 Bayesian indiﬀerence zone methods
Bayesian monitoring of clinical trials dates to the landmark but woefully
underutilized work of Cornﬁeld (1966a,b, 1969). These papers contained the
basic framework for clinical trial decisionmaking based on posterior distri-
butions, but in their era were regarded primarily as academic exercises, no
doubt in part because they were so far ahead of their time (and ahead of
the MCMC revolution in Bayesian statistics). Bayesian clinical trial meth-
ods did not begin to gain practical application until the 1980s, when the
work of Freedman and Spiegelhalter (1983) saw implementation in a few
large trials in the United Kingdom (see also Freedman and Spiegelhalter,
1989, 1992; Freedman et al., 1984; or the lovely review in Spiegelhalter
et al., 2004, Ch. 6). These authors suggested implementation of Bayesian
methodology through the use of an indiﬀerence zone (or range of equiv-
alence) for a treatment eﬀect parameter ∆. The basic idea is to replace
the traditional but unrealistic point null hypothesis, H0 : ∆= 0, with a
range of null ∆’s, say [δL, δU], over which we are indiﬀerent between the
intervention and the control. The upper bound, δU, represents the amount
of improvement required by the intervention to suggest clinical superiority
over control, while δL denotes the threshold below which the intervention
would be considered clinically inferior.
Suppose positive values of ∆are indicative of an eﬃcacious treatment.
Then we might for example set δU = K > 0 and δL = 0, an additional
beneﬁt perhaps being required of the treatment in order to justify its higher
cost in terms of resources, clinical eﬀort, or potential toxicity. Bayesian
stopping rules are then naturally based on the posterior probability of the
tail areas determined by the indiﬀerence zone endpoints. For instance, we

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
67
no decision
(∆L
∆U)
accept treatment
(∆L
∆U)
reject control
(∆L
∆U)
equivalence
(∆L
∆U)
reject treatment
(∆L
∆U)
accept control
(∆L
∆U)
δL
δU
control
better
treatment
better
Figure 2.10 Indiﬀerence zone (δL, δU) and corresponding conclusions for a clini-
cal trial based on the location of the 95% posterior credible interval for ∆.
might terminate the trial when
P(∆> δU|data)
(2.27)
is suﬃciently small (deciding in favor of the control), or when
P(∆< δL|data)
(2.28)
is suﬃciently small (deciding in favor of the treatment). Another rule would
be to stop when one region’s posterior probability is suﬃciently large, or,
failing this, when a predetermined total sample size is reached. Such a rule
might be appropriately applied to the lower tail in a drug-placebo study,
since clinicians would likely have very low prior belief in the placebo’s
superiority.
Trial stopping rules might also be based upon the location of the 95%
posterior credible interval for ∆, (∆L, ∆U), with respect to the indiﬀerence
zone [δL, δU], as demonstrated in Figure 2.10. Exactly six cases are possible,
with stronger evidence required to “accept” one hypothesis than merely
reject the other. Note that to conclude “equivalence,” the 95% interval
must lie entirely within the indiﬀerence zone; if the interval straddles both
ends of the zone, posterior evidence is too weak to make a decision of any
kind.
Consider a trial where increased ∆implies increased beneﬁt associated
with intervention. We sometimes take δL = 0 and δU > 0, but we might
also center the indiﬀerence zone around 0, i.e., δL = −ξ and δU = ξ. In
the latter case, for a ﬁxed n and under a proper prior on ∆, expanding the
indiﬀerence zone by increasing ξ corresponds to a decrease in Type I error

68
BASICS OF BAYESIAN INFERENCE
(since rejection of the control becomes more diﬃcult) but also a decrease
in power. On the other hand, decreasing ξ powers the trial for a more de-
sirable eﬀect diﬀerence, yet corresponds to an increase in Type I error. One
often strives for an indiﬀerence zone with various appealing “symmetry”
properties; however, note that symmetry within the indiﬀerence zone for
regression coeﬃcients in a logistic or Cox proportional hazards model does
not yield corresponding symmetries on the (nonlinearly transformed) odds
or hazard ratio scales.
Once the indiﬀerence zone is speciﬁed, the likelihood for the observed
trial outcomes must be formulated. Consider again the case of a binary
trial endpoint; say, whether the patient experiences progression of disease
or not during the trial, for i = 1, . . . , N. Then Yi ∼Bernoulli(pi), where
pi is the probability of disease progression for the ith patient. Now let xi
be an indicator variable for the intervention group. One possible model for
pi assumes
logit(pi) = log
µ
pi
1 −pi
¶
= λ0 + λ1xi ,
(2.29)
where λ0 and λ1 are random hyperparameters. If xi = 0 for control and 1 for
treatment, then λ1 captures the intervention eﬀect. Inference is typically
based on eλ1, the ratio of odds for disease progression between the two
groups. Therefore, eλ1 plays the role of ∆above.
Evaluating the posterior distribution of eλ1 requires us to specify priors
for the regression parameters. Because λk ∈ℜfor k = 0, 1, normal priors
could be used, with informative content added as indicated in the next
subsection. Any important prognostic factor zi may be added to logit(pi)
as a λ2zi term, although eλ1 would now need to be interpreted as the odds
ratio of disease progression for individuals with identical zi.
If instead of binary outcomes, we have continuous measurements (e.g.,
blood pressure, weight, etc.), then a normal likelihood may be more ap-
propriate. Now ∆would be expressed as the diﬀerence in group means,
prior knowledge on the likelihood mean would likely be incorporated us-
ing a normal prior, and the likelihood variance might use the standard
inverse gamma prior. Time-to-event outcomes may also be of interest, and
employ Weibull, gamma, or Cox partial likelihoods; Example 2.9 oﬀers an
illustration of the ﬁrst case.
2.5.3 Prior determination
When determining the priors for crucial parameters such as λ0 and λ1 in
(2.29), note that information may be plentiful for the former (since it is
determined by the rate in the control group), but not the latter (since this
parameter captures the improvement of the new therapy over control). Still,
every prior determination strategy should begin with a review of all avail-
able historical evidence on both the treatment and control groups. This

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
69
review helps to determine our analysis priors, the prior distributions that
will be used when the data are ultimately collected and the posterior distri-
bution computed. Even when not directed to do so by a regulatory agency,
we will often wish to compare the results obtained under an informative
analysis prior (i.e., one that incorporates available historical information)
with those from a noninformative one, to see the impact the historical data
have on the posterior.
We remark brieﬂy that we may well be in need of two prior speciﬁcations:
the aforementioned analysis prior, and also a design prior. This latter prior
is the one we use when designing the trial, and would therefore typically
use the full range of information gleaned from past data and literature
review. The analysis prior, by contrast, might be somewhat less informative,
especially if the goal of our trial is to win over skeptics whose faith in our
review of the evidence is lower than ours, or who might simply want the
results of the current trial to stand on their own. This is admittedly a
confusing (and seemingly “illegal”) distinction, and one we return to in
greater detail later in this section.
Community of priors
Often, there will be a wide range of prior beliefs that could plausibly be
derived from alternate readings of the available pre-trial information. In
addition, subject matter experts consulted by trial designers may well con-
tribute their own divergent opinions, based on their own clinical experience
or other expertise. Since trial results may well be sensitive to the choice
of prior (especially on the eﬃcacy parameters that drive the outcome),
Spiegelhalter et al. (1994) recommend using a community of several priors
(c.f. Kass and Greenhouse, 1989) in order to represent the broadest possible
audience. These priors might be broadly categorized as skeptical, enthusias-
tic, and reference (or noninformative). A skeptical prior is one that believes
the treatment is likely no better than control (as might be believed by a
regulatory agency). Such a prior might be centered around the clinical in-
feriority boundary, δL, which is often equal to 0. The spread of this prior
will then determine the a priori chance of clinical superiority, P(∆> δU).
An enthusiastic prior is one that believes the treatment will succeed. Since
this viewpoint is typical of the one held by the clinicians running the trial,
such a prior is sometimes known as a clinical prior. Here we might center
the prior around the clinical superiority boundary, δU, and again deter-
mine the variance based on tail area considerations, or perhaps simply by
matching to the skeptical prior variance. Finally, as already mentioned in
Section 2.1, a reference prior is one that attempts to express no particular
opinion about the treatment’s merit. Since ∆is a mean parameter that is
typically well-estimated by the data, an improper uniform (“ﬂat”) prior is
often permissible.

70
BASICS OF BAYESIAN INFERENCE
Note that it may be sensible to match the prior to the decision one hopes
to reach; the prior should represent “an adversary who will need to be disil-
lusioned by the data to stop further experimentation” (Spiegelhalter et al.,
1994). Thus, to conclude a treatment diﬀerence, we should use the skeptical
prior, while to conclude no diﬀerence, we should use the enthusiastic prior.
Figure 2.11 illustrates the use of a community of priors in interim mon-
itoring by looking at the marginal posterior probabilities of the two tail
areas (2.27) and (2.28) for a particular trial with four monitoring points.
This model is parametrized so that negative values of the treatment eﬀect,
β1, indicate clinical superiority. The graph shows results for three priors:
an enthusiastic, clinical prior (marked by “C” in the graphs), a skeptical
prior (“S”), and a noninformative, ﬂat prior (“L”, indicating that only the
likelihood is driving these results). Here the clinical inferiority boundary,
β1,U, is set to 0, while the clinical superiority boundary, β1,L, is set to
log(0.75) = −.288, which in this model corresponds to a 25% reduction
in hazard relative to control. In this particular example (see Carlin and
Louis, 2009, Sec. 8.2 for full details), the accumulating data actually favor
the placebo, with an excess of deaths gradually accumulating in the treat-
ment group. Thus, in the upper panel we see the posterior probability of
superiority steadily dropping over time, while the lower panel reveals the
opposite trend in the posterior probability of inferiority. Notice from the
upper panel that the skeptical and ﬂat prior analyses are ready to stop
and abandon the treatment by the third monitoring point (as its posterior
probability has dropped below 0.1), but the enthusiastic clinical prior is
not won over to this point of view until the fourth and ﬁnal interim look.
However, note also that the clinical prior is “unethical” in the sense that
it is ready to stop for superiority at the very ﬁrst monitoring point, when
no data have yet accumulated. This illustrates a clear risk when using an
overly optimistic prior, and one we shall adjust for by considering power
and Type I error in Subsection 2.5.4.
2.5.4 Operating characteristics
Without doubt the most commonly asked question of biostatisticians work-
ing in the practice of clinical trials is, “How big a sample size do I need for
this trial?” This sample size question is one that Bayesianism must have a
ready answer for if it is to play a signiﬁcant role in the practice of clinical
trials. Fortunately, the Bayesian paradigm is quite natural for experimental
design, and sample size calculation is a standard design problem. That is,
we determine the sample size by ﬁnding the smallest number of patients
that, for our chosen statistical model, will lead to a trial we know will have
good operating characteristics, such as low Type I error and good power
at “likely” alternatives. But “likely” here means “a priori,” since at the
design stage, no data are yet available (at least on the treatment group;

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
71
C
C
C
C
calendar date (cumulative # of events)
probability
0.0
0.2
0.4
0.6
0.8
1.0
1/15/91 (0)
12/31/91 (38)
3/30/92 (60)
L
L
L
L
S
S
S
S
a) P{beta_1 < log(.75) | R}
C
C
C
C
calendar date (cumulative # of events)
probability
0.0
0.2
0.4
0.6
0.8
1.0
1/15/91 (0)
12/31/91 (38)
3/30/92 (60)
L
L
L
L
S
S
S
S
b) P{beta_1 > 0 | R}
Posterior monitoring plot for beta_1;  Covariate =  Baseline CD4 Count
(C = clinical posterior, L = likelihood, S = skeptical posterior)
Figure 2.11 Example monitoring plot: posterior tail probabilities for the treat-
ment eﬀect at four interim monitoring points. Top panel: probability of treatment
superiority; bottom panel, probability of treatment inferiority.
we may have historical controls, a subject to which we will return). So it
makes sense that Bayesian methods might have something to oﬀer here.
As has already been mentioned, the CDRH (Center for Devices and Ra-
diological Health) branch of the FDA has been interested in Bayesian meth-
ods for quite some time. The initial impetus was to utilize prior information
from previously approved medical devices in order to enhance data on new
medical devices to be approved. Later on, the ﬂexibility of Bayesian adap-
tive designs proved to be even more appealing to device companies. At
present, the vast majority of Bayesian medical device clinical trials sub-
mitted to the FDA makes use of adaptive designs, even in the absence of
prior information. Still, the FDA remains a regulatory agency whose funda-
mental mission is to protect the public from harmful products, and ensure
that products billed as eﬀective really are eﬀective. Perhaps most signiﬁ-
cantly, they must do this “over the long haul;” when a product is judged
“signiﬁcantly better,” it must mean averaging over all products that are
tested over time. This is an inherently frequentist (not Bayesian) outlook.

72
BASICS OF BAYESIAN INFERENCE
So it is perhaps not surprising that FDA approval for Bayesian designs
continues to depend on demonstration of controlled Type I and Type II
error rates and acceptable frequentist power.
While it may seem odd to be adopting Bayesian methods if frequentist
operating characteristics continue to be so important, keep in mind that the
inherent advantages of the Bayesian paradigm (borrowing strength across
similar but independent units, utilizing reliable historical information to
supplement information in the data, etc.) will often permit well-designed
Bayesian trials to have excellent frequentist properties. And of course, even
if satisfying the FDA were not the primary goal, Bayesians still care about
long-run behavior of their procedures; they would simply prefer a prepos-
terior analysis – i.e., one that averaged over the variability in both the
unknown parameters and the as-yet unobserved data. Adding in the aver-
aging over the prior leads to obvious preposterior Bayesian analogs of Type
I error and power, a subject on which we will elaborate.
To ﬁx ideas, consider again the binary data setting, and the logistic re-
sponse model of equation (2.29). Suppose we wish to power a study to
deliver any of the six outcomes illustrated in Figure 2.10 (or combinations
thereof) with a given probability. For any ﬁxed, “true” values of the param-
eter vector λ0 and λ1 and proposed treatment allocation {xi}N
i=1, we can
simulate the frequentist power of our Bayesian procedure by computing the
pi from equation (2.29), and then generating fake data values Y ∗
ij repeat-
edly from the binomial likelihood for j = 1, . . . , Nrep. Each fake data vector
Y∗
j = (Y ∗
1j, . . . , Y ∗
Nj)′ leads to a 95% posterior interval for λ1, and hence
one of the six decisions in Figure 2.10. Repeating this for each of the Nrep
datasets, we can compute the empirical probability of each of the six out-
comes, and thus estimate any power we desire (a Type I error calculation
arises by setting λ1 = 0, the null value) in conjunction with the appropri-
ate superiority hypothesis. Thus, our Bayesian sample size problem comes
down to choosing a design (i.e., a sample size N and an indiﬀerence zone)
that delivers some prespeciﬁed acceptable frequentist properties. The use
of an informative ﬁtting prior is likely to pay dividends in cases where the
“truth” is congruent with this prior.
As alluded to above, a fully Bayesian version of this procedure would re-
place the ﬁxed, true values (λ0, λ1) by draws {(λ∗
0j, λ∗
1j), j = 1, . . . , Nrep}
from their prior distributions. This would acknowledge the uncertainty in
these parameters; we will never know “the truth” at the design stage. How-
ever, note that the design prior (i.e., the prior used to generate the fake
λ∗
0j and λ∗
1j) need not be the same as the analysis prior, with the latter
typically being the vaguer of the two. That is, we would wish to use all
available information at the design stage, but might prefer a vaguer, less
risky prior once the data have accumulated, in order to improve our shot
at good operating characteristics. Thus, in this text we will use the terms
design prior and analysis prior (or ﬁtting prior), and endeavor to clarify

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
73
any diﬀerences between the two when they exist. Incidentally, these ideas
were also appreciated by several previous authors, including O’Hagan and
Stevens (2001); see Subsection 2.5.5.
Having selected design and analysis priors, all that remains is to sum-
marize the (frequentist or Bayesian) power and Type I error, and select a
sample size N that delivers satisfactory levels of each. Note that the pos-
terior calculation for each fake data vector Y∗
j may be available in closed
form, as in the beta-binomial setting of Example 2.8. However, many mod-
els in clinical trial design and analysis (especially survival models, using say
the Weibull, gamma, or Cox partial likelihood) will require MCMC sam-
pling, perhaps through Nrep calls to the BUGS software. A feasible solution
here (and the one we recommend at least initially) is to write the outer,
fake data-generating loop in R, and call BUGS repeatedly using commands
from the BRugs package. Example 2.9 oﬀers an illustration.
Example 2.9 (simulating power and Type I error for a Weibull survival
model). Let ti be the time until death for subject i in a clinical trial, with
corresponding treatment indicator xi (set equal to 0 for control and 1 for
treatment). Suppose the survival time ti follows a Weibull(r, µi) distribu-
tion, where r > 0 and µi > 0. Adopting BUGS’ parametrization, this assumes
a pdf of f(ti|r, µi) = µirtr−1
i
exp(−µitr
i ). To incorporate the treatment in-
dicator into the model, we further parametrize µi = µi(xi) = e−(β0+β1xi).
Then the baseline hazard function is h0(ti) = rtr−1
i
, and the median sur-
vival time for subject i is
mi = [(log 2)eβ0+β1xi]1/r .
Thus the relative change in median survival time in the treatment group
is exp(β1/r), and so β1 > 0 indicates improved survival in the treatment
group. Moreover, the value of β1 corresponding to a 15% increase in median
survival in the treatment group satisﬁes
eβ1/r = 1.15 ⇐⇒β1 = r log(1.15) .
For humans older than age 1, we normally expect r > 1 (i.e., increasing
baseline hazard over time). For the purpose of illustration, in this example
we somewhat arbitrarily set r = 2, so our Weibull is equivalent to a Rayleigh
distribution. This then helps us specify an indiﬀerence zone (δL, δU) as
follows. First, we take δL, the clinical inferiority boundary, equal to 0,
since we would never prefer a harmful treatment. However, in order to
require a “clinically signiﬁcant” improvement under the treatment (due
to its cost, toxicity, and so on) we would prefer δU > 0. Since we have
selected r = 2, taking δU = 2 log(1.15) ≈0.28 corresponds to requiring a
15% improvement in median survival. The outcome of the trial can then be
based on the location of the 95% posterior conﬁdence interval for β1, say
(β1L, β1U), relative to this indiﬀerence zone. The six possible outcomes and

74
BASICS OF BAYESIAN INFERENCE
−1.0
−0.5
0.0
0.5
0.0
0.5
1.0
1.5
2.0
β1
prior density
skeptical prior
enthusiastic prior
clinical inferiority boundary
clinical superiority boundary
Figure 2.12 Skeptical (solid line) and enthusiastic (dashed line) priors, Weibull
survival model. Also shown (vertical lines) are the clinical inferiority boundary,
δL = 0, and the clinical superiority boundary, δU = 0.28.
decisions were previously shown in Figure 2.10; recall that a novel feature
of this setup is that both “acceptance” and “rejection” are possible.
Next we need to select an appropriate prior distribution. Following the
aforementioned “community of priors” idea, we select skeptical, enthusias-
tic, and reference priors for the treatment eﬀect parameter, β1. Beginning
with the skeptical case, we simply set the prior mean equal to 0 (implying
no change in survival in the treatment group relative to control) and then
choose the variance so that P(β1 > δU) = ϵ, for some small but positive
probability ϵ. In our setting ϵ = 0.05 delivers the N(0, (0.17)2) prior dis-
tribution, shown as a solid line in Figure 2.12. Turning to the enthusiastic
prior, we raise the mean up to the clinical superiority boundary (0.28), but
use the same standard deviation as the skeptical prior (0.17). This prior is
shown as the dashed line in Figure 2.12. Finally, for our reference (nonin-
formative) prior, we simply use an improper uniform (“ﬂat”) prior, since
it will still lead to a proper posterior here.
In all three cases we use a N(7.53, 0.2) prior for the intercept parame-
ter, β0. This prior is somewhat informative and centered near values that
emerge as plausible for our data generation mechanism, explained below.
We could certainly be less prescriptive regarding β0, but since it is merely

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
75
a “nuisance” parameter we prefer a fairly precise prior that will encourage
the data’s estimative power to focus on the parameter of interest, β1.
As already mentioned, we simulate power or other operating character-
istics within R, here also calling BUGS for each simulated data set since the
marginal posterior for β1 is not available in closed form. For example, to
simulate Bayesian operating characteristics, we begin by sampling a “true”
β from its design prior. Here this means sampling a β0 value from its prior,
followed by a β1 value from one of our community of priors (here, skeptical
or enthusiastic; the ﬂat prior is improper and hence cannot be sampled).
Given these, we then sample fake survival times ti (say, N from each study
group) from the Weibull likelihood. To add a bit of realism, we may also
wish to sample fake censoring times ci from a particular distribution (e.g., a
normal distribution truncated below 0). Then for all individuals i for whom
ti > ci, we replace ti by “NA” (missing value), corresponding to the indi-
viduals who were still alive when the study ended. Next, we call BUGS to get
the 95% equal-tail credible interval for β1, which it obtains from a (sorted)
collection of 1000 MCMC samples. We then determine the simulated trial’s
outcome based either on a posterior tail area (say, P(β1 < δL|t, x, c) or
P(β1 > δU|t, x, c)), or perhaps on the location of this interval relative to
the indiﬀerence zone (0, 0.28); once again, see Figure 2.10. After repeating
this entire process some large number of times Nrep, we can report the
appropriate summaries (e.g., the empirical frequencies of the six possible
outcomes) as simulation-consistent estimates of the desired Bayesian trial
operating characteristic.
A computer program to implement this solution is available online as
“Example 3” at http://www.biostat.umn.edu/∼brad/software/BRugs/.
As in Example 2.7, we require two pieces of code: a piece of BUGS code to
specify the Weibull model for any given dataset, and a piece of BRugs code
to repeatedly generate the fake data values and send them to BUGS for
analysis. Using our reference analysis prior, the BUGS code looks like this:
BUGS code
model {
for (i in 1:n) {
t[i] ~ dweib(2, mu[i]) I(t.cens[i], )
mu[i] <- exp(-beta0 - beta1*x[i])
}
beta0 ~ dnorm( 7.53, 25)
beta1 ~ dnorm(0,.0001)
# reference (noninformative) prior
}
#
end of BUGS code
This code is very simple, since (following the “mice” example in the
WinBUGS manual) all that is necessary is the (censored) Weibull likelihood,
an expression of the scale parameter µi in terms of the parameters and
the treatment indicator, and priors for β0 and β1. The censoring aspect is
handled rather ingeniously in BUGS using the I (indicator) function, which
here speciﬁes that the failure time t is restricted to being larger than the

76
BASICS OF BAYESIAN INFERENCE
analysis prior
N
Skeptical
Reference
Enthusiastic
25
.001
.053
.178
50
.009
.069
.213
75
.017
.110
.209
100
.034
.070
.214
Table 2.4 Probability of rejecting the control under a skeptical design prior for
four sample sizes N and three analysis priors, Weibull survival model.
censoring time t.cens. Censored individuals will have t = NA and t.cens
equal to their censoring times; individuals observed to fail will instead
have t equal to these failure times and t.cens = 0 (i.e., no additional
restriction on the usual Weibull distribution). BUGS then helpfully generates
any missing ti’s, along with the unknown β parameters, ending up after
convergence with the correct marginal posterior for β1.
By contrast, the outer, BRugs code is fairly lengthy, and is relegated to
the aforementioned website. This version of the code assumes a sample size
of N = 50 in each group, a median survival of 36 days, and a N(80, 202)
censoring distribution. We also assume a (very optimistic) 50% improve-
ment in the treatment group, and take the enthusiastic prior as the “truth”
(i.e., our design prior is enthusiastic). However, we use the reference prior
in our BUGS calls (i.e., our analysis prior is noninformative). For an ad-
mittedly too-small run of just Nrep = 100 replications, we obtained the
following output:
Here are simulated outcome frequencies for N= 50
accept control:
0
reject treatment:
0.07
equivalence:
0
reject control:
0.87
accept treatment:
0.06
no decision:
0
End of BRugs power simulation
As expected in this optimistic design, we are able to reach the “reject
control” decision 87% of the time, and in fact draw the even stonger “accept
treatment” decision an additional 6% of the time. Grouping these two cases
together, the estimated “Bayesian power” of our procedure is 93%. By
contrast, the estimated “Bayesian Type II error rate” is 7%, since this is
the empirical proportion of datasets for which the treatment was rejected
despite its superiority.
For given design and analysis priors, we would repeat this process (i.e.,

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
77
rerun the BRugs program) for several diﬀerent sample sizes N. We would
then choose the smallest sample size that still delivers power and Type
I error behavior we deem acceptable. Consider for instance the case of a
skeptical design prior (i.e, the “truth” now is that the treatment is very
similar to the control). Increasing Nrep to 1000 (but still using 100 burn-in
and 1000 production MCMC iterations per BUGS call), Table 2.4 shows the
simulated probabilities of rejecting the control when the skeptical prior is
true. The entries in this table are essentially Bayesian Type I error rates,
since the “truth” here is that nothing much is going on. Unsurprisingly,
error is lowest for the skeptical analysis prior, which has correctly guessed
the truth in this case. The enthusiastic prior has what might be considered
unacceptably high error rates, though they appear to have stabilized (up
to simulation error) near 20% for the larger sample sizes.
In the previous example, “true” parameter values were generated from a
design prior, leading to Bayesian analogs of power and Type I and II error.
But as mentioned previously, a fully frequentist Type I error calculation
is also possible using our approach. For example, the frequentist Type I
error of our Bayesian procedure could be simulated simply by ﬁxing β1 = 0
(rather than sampling it from a design prior), and generating only the ti
and ci for each of the Nrep iterations. Frequentist power and Type II error
can be simulated similarly by ﬁxing β1 at nonzero values.
Finally, we remark that we have only considered stopping for a single
endpoint (eﬃcacy). But handling multiple endpoints is also straightfor-
ward within the Bayesian framework. For example, suppose we wish to
power a study to evaluate both safety and long-term eﬃcacy of a particu-
lar treatment. If the responses on both endpoints can be reasonably treated
as discrete, a sensible and relatively simple Bayesian approach might use
a Dirichlet-multinomial model, a straightforward extension of the beta-
binomial model for binary responses. Speciﬁcally, once the joint distribution
of the multiple endpoints is speciﬁed via cross-classiﬁcation, the multino-
mial becomes the natural likelihood model. The Dirichlet distribution then
oﬀers a convenient conjugate prior whose speciﬁcation is similar to that of
the beta distribution and which is also available in both R and BUGS. More
complex (e.g., survival) models would likely require MCMC sampling and
the associated BRugs calls to BUGS. Early stopping for futility based on
predictive distributions (“Bayesian stochastic curtailment”) may also be of
interest; see Berry and Berry (2004) and Section 4.3 below.
Interim analysis
The subject of interim analysis (or “multiple looks [at the data]”) is a sub-
ject of constant worry in frequentist clinical trial analysis. The reason is
clear: since frequentists make decisions based on p-values (i.e., Type I error
levels) and other design-based summaries, if we decide to look at the accu-

78
BASICS OF BAYESIAN INFERENCE
mulating data many times over the course of the study, we must account
for this in the procedure or risk inﬂating its Type I error. Early solutions
to this problem by Pocock (1977) and O’Brien and Fleming (1979) involve
prespecifying the number of interim analyses and computing a stopping
boundary for each that restricts overall Type I error. Lan and DeMets
(1983) avoid prespecifying the number of interim looks by utilizing a Type
I error “spending function.” While an extremely clever tool, the spend-
ing function seems somewhat arbitrary and largely serves to rescue the
frequentist from a tight mathematical spot without addressing the funda-
mental problems with p-values; see e.g. Subsection 2.2.7 and Carlin and
Louis (2009, Sec. 2.3.3).
In principle, Bayesians do not face the “multiple looks problem” at all:
their decisions are based on posterior summaries that do not depend on
how the experiment was stopped; the posterior simply evolves (typically
narrowing) as data accumulate. This is why Bayesians are free to “peek
at their data” any time they wish, provided they really have no interest in
the long-run frequency properties of their procedures. Of course, this is not
the case in clinical trials, where we’ve already established the importance
of the long run. And in any case, government regulators’ ongoing interest
in the subject forces the Bayesian’s hand here.
Fortunately, simulation methods we have outlined once again come to
the rescue. We simply incorporate whatever pre-ordained schedule of looks
(say, every N = 20 patients, up to a maximum of 100) into the simula-
tion program, and modify our empirical stopping proportions accordingly.
Bookkeeping does become somewhat more complicated, since we are likely
interested in the proportions of trials stopped for each reason at each mon-
itoring point. At present this may begin to push the envelope of what is
possible computationally with BRugs calling BUGS; at some point a normal
or other approximation to the posterior may be necessary to avoid a BUGS
call for every simulated fake dataset.
Finally, a potential concern when using sequential stopping extensively is
that of “sampling to a foregone conclusion,” the mathematical result that
repeated calculation of Bayesian tail probabilities will ultimately lead to
rejection of any null hypothesis. While the number of interim looks typically
used in practice suggests this will rarely be a concern, see Spiegelhalter et
al. (2004, Section 6.6.5) and references therein for more on this issue.
2.5.5 Incorporating costs
The methods discussed so far might be described as “probability only”
methods, in that they base their decisions entirely on tail areas or conﬁ-
dence intervals derived from the posterior distributions of the model param-
eters of interest. No attempt is made to quantify any of the costs inherent
in the process, or to have this information inﬂuence the decisionmaking.

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
79
Yet clearly there are many such costs inherent in the process: the monetary
cost of enrolling and following each new patient, the cost to a company of
continuing to develop a new drug (rather than abandon it and focus re-
sources elsewhere), and of course the human cost of delaying a decision
while patients in the trial (half of whom are by deﬁnition receiving an
inferior treatment) are at risk for the trial’s endpoint(s).
If they can be reliably quantiﬁed, Bayesian methods are well-suited to
incorporating such costs, through the ﬁeld of Bayesian decision theory.
Whole textbooks (e.g. DeGroot, 1970; Berger, 1985) have been devoted
to this topic, and whole journals (e.g., Medical Decision Making) are de-
voted to the use of Bayesian and non-Bayesian quantitative methods in
medical cost-eﬀectiveness studies. Still, statisticians have historically been
somewhat reticent to use these methods, on the grounds that their results
depend crucially on the precise costs selected, whose values are often eas-
ily criticized by potential readers of the analysis. The implementation of
Bayesian decision-theoretic methods can also be complex, especially in the
case where MCMC algorithms must be used to estimate posteriors. For our
purposes, then, the primary use of these methods may be in internal studies
where appropriate costs can be agreed upon, and where the ﬁnal decision-
makers are also “in house,” rather than being some external reader whose
opinions regarding costs and beneﬁts cannot be known in advance. See Sec-
tion 4.6 for more discussion, and Subsection 4.6.2 for a speciﬁc application
of decision theoretic design to drug development.
A basic and computationally feasible framework for incorporating cost
eﬀectiveness was recently provided by O’Hagan and Stevens (2001). These
authors laid out a Bayesian formulation of the sample size determination
problem that generalizes a traditional frequentist sample size calculation
based on hypothesis testing. They did this in the context of assessing the
cost eﬀectiveness of a particular treatment relative to control. Speciﬁcally,
they let eij be the observed eﬃcacy and cij be the cost of treatment i for
patient j, j = 1, . . . , ni and i = 1, 2, where i = 1 denotes control and i = 2
denotes treatment. These authors then assume the bivariate normal model
µeij
cij
¶
∼N2
µµµi
γi
¶
, Σ
¶
,
where Σ11 = σ2
i , Σ22 = τ 2
i , and Σ12 = Σ21 = ρiσiτi. Given the scale in
which the costs cij are expressed (dollars, patient lives, etc.), suppose K is
the maximum amount we are prepared to pay to obtain one unit of increase
in eﬃcacy eij. Then our cost eﬀectiveness assessment must be based on the
net beneﬁt
β = K(µ2 −µ1) −(γ2 −γ1) .
The treatment (i = 2) is cost eﬀective if β > 0. Denoting all the data
{eij, cij} as y, suppose we require P(β > 0|y) > ω. Then this Bayesian
analysis objective is analogous to rejecting H0 : β = 0 in favor of the one-

80
BASICS OF BAYESIAN INFERENCE
sided alternative Ha : β > 0 at a p-value of α = 1−ω. O’Hagan and Stevens
(2001) refer to this as the analysis objective, and the prior used to calculate
the posterior probability as the analysis prior.
Now, the sample sizes ni in each group must be such that, averaging over
all datasets we might see, the probability of a positive result is at least δ.
That is, using subscripts to more clearly indicate the random variable with
respect to which an expectation is taken, we require
PY
h
Pξ(a′ξ > 0|y) > ω
i
> δ ,
(2.30)
where ξ = (µ1, γ1, µ2, γ2)′ and a = (−K, 1, K, −1)′, so that β = a′ξ.
The authors refer to the left-hand side of (2.30) as the Bayesian assur-
ance; note it is the Bayesian analogue of power, averaged with respect to the
prior distribution used to calculate the marginal distribution of the data
Y. Like our treatment in Subsection 2.5.4, O’Hagan and Stevens (2001)
observed that this prior distribution need not be the same as the one used
to calculate the inner, posterior probability in (2.30). That is, they too
recognized the need to allow for diﬀerent priors at the design and analysis
stages. In their notation, we would have
design prior:
ξ ∼N(md, Vd) ,
analysis prior:
ξ ∼N(ma, Va) ,
where md, Vd, ma, and Va are all assumed known. Note that V −1
a
= 0
produces a vague (zero precision) analysis prior, while Vd = 0 produces a
point design prior. Under these two conditions, Bayesian assurance is equal
to frequentist power at the proposed true ma value. These fully speciﬁed,
conjugate forms enable closed form posterior and marginal distributions for
ξ and Y, respectively, that in turn facilitate calculation of the assurance.
For example, consider the frequentist setting of a ﬂat analysis prior (V −1
a
=
0 in the limit) and a point design prior (Vd = 0). The former condition
implies V ∗= S, while the latter implies a precise speciﬁcation of ξ = md.
If we require n = n1 = n2, equal sample sizes in the treatment and control
groups, then the common sample size in the frequentist case turns out to
satisfy
n ≥(z1−ω + z1−δ)2a′S1a
(a′md)2
,
(2.31)
where S1 is the single-observation variance matrix obtained by setting n1 =
n2 = 1 in the previous expression for S.
Obviously the results of this subsection depend heavily on the speciﬁc
model used, which is somewhat artiﬁcial and specialized (normal distri-
butions with known variance matrices). However, the principles involved,
namely choosing design and analysis priors and using them to determine
sample sizes through ﬁxed deﬁnitions of trial success (analysis objective)
and Bayesian power (design objective), are quite general. Given the increas-

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
81
ing pressure to deliver therapies quickly and at low relative cost, along with
the slowly but steadily emerging consensus on appropriate cost metrics (say,
via quality adjusted life years, or QALYs), methods like these should ﬁnd
greater and greater application in clinical trials practice.
An important concept in cost eﬀectiveness studies is the incremental cost
eﬀectiveness ratio (ICER), the incremental cost over incremental beneﬁt
for an experimental therapy compared to the standard of care. When the
ICER for a new therapy is above a certain threshold, the therapy is judged
to be cost eﬀective. O’Hagan, Stevens, and Montmartin (2000) show that
this rule can be justiﬁed as a decision theoretic optimal Bayes rule under
a certain utility function.
Software note: In subsequent work (O’Hagan et al., 2001), these same three
authors oﬀer exempliﬁcation of the approach with a real dataset, with sup-
porting WinBUGS code available at
www.tonyohagan.co.uk/academic/Astradat.txt.
2.5.6 Delayed response
Clinical studies often involve delayed responses, i.e., outcomes that are
observed with a substantial time delay after assigning a treatment or en-
rolling a patient. Such lagged responses create challenges for clinical trial
designs when a stopping decision or treatment allocation requires outcomes
from earlier enrolled patients. Typical examples are phase II trials where
the outcome might be an indicator for a certain change in tumor volume
by a certain time, for example tumor response within 5 weeks after treat-
ment. Another typical example is the occurrence of graft versus host disease
(GVHD) within 100 days. In either case the outcome is observed with a
substantial delay after treatment allocation. When the next patient is re-
cruited, there would usually be several already enrolled patients who have
been assigned treatments, but are still awaiting the ﬁnal response. Such
delays complicate clinical trial design when a decision for the next patient
or patient cohort depends on responses from earlier patients.
For Bayesian designs, dependence on earlier outcomes is formalized by
basing current decisions on the posterior distribution conditional on all
previous outcomes. The principled nature of Bayesian inference oﬀers an
easy solution to the problem of delayed responses. The relevant posterior
distribution simply includes the partial responses from already enrolled
patients with missing ﬁnal response. For example, when the response is
an event time, this simply amounts to censoring. In general, the posterior
conditional on the partially observed response is the expected posterior
conditional on a hypothetical ﬁnal response. The expectation is deﬁned
with respect to the posterior predictive distribution for the ﬁnal response
(see Subsection 2.5.1).

82
BASICS OF BAYESIAN INFERENCE
In many studies with delayed responses, it is possible to record early out-
comes. For example, when the ﬁnal response is progression-free survival,
one could record shrinkage of tumor volume as an early outcome. Shrink-
age of tumor volume is usually considered to be indicative of an improved
survival outcome. Using the posterior predictive distribution conditional
on such early responses can greatly improve eﬃciency of the clinical trial
design. We defer further, more technical discussion of this issue to Subsec-
tion 4.4.4.
2.5.7 Noncompliance and causal modeling
While somewhat embarrassing to mention, everything we do in this book
depends on the subjects’ willingness to comply with the treatment they
are (randomly) assigned by the trial protocol. But such compliance (or
adherence) is far from given in most clinical trials. Acknowledging this
reality forces a major reassessment of our theory: to what extent does
failure to comply with the assigned treatment in a clinical trial alter the
trial’s fundamental ﬁndings?
Such considerations take us into the realm of causal inference, which
attempts to estimate not the eﬀect of being assigned to a particular treat-
ment, but the “causal” eﬀect of actually receiving such a treatment. It is
often argued that this quantity is the real target of interest in a clinical
trial, since we wish to estimate the actual eﬀect of receiving the treatment,
not merely the eﬀect of being assigned to the group that was supposed to
receive the treatment.
But even this modest intellectual leap is controversial. Many clinical tri-
alists maintain that the eﬀect of treatment assignment is what is relevant
in every trial; after all, if the drug is approved and ends up being used by
the general population, many patients assigned to receive the treatment by
their physicians will not actually receive it, perhaps due to cost, unpleasant
side eﬀects, or any number of any other reasons. Such trialists would likely
argue that since these problems are just as likely to appear in the trial
as in post-trial practice, it is more appropriate to simply ignore the non-
compliance problem at the trial stage and estimate the eﬀect of treatment
assignment. This viewpoint is extremely widespread and is captured by
the phrase “intention to treat” (ITT): we attempt to estimate the eﬀect of
treatment assignment (rather than actual treatment received) since this is
the eﬀect we can expect if the drug is approved and utilized by the general
population.
Still, the validity of the ITT approach clearly rests on the assumption
that the nature and amount of noncompliance in the clinical trial will be
the same as that emerging in the population at large. Clearly this may not
be the case. For one thing, persons enrolling in a trial may be more likely
to be concerned with their own well-being than an average person, and as

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
83
such be more likely to comply with their treatment assignment. Second,
the distribution of good compliers may not be the same in the treatment
and control groups: if the treatment has unpleasant side eﬀects, we might
expect poorer compliance in that group. In any case, an informed person
contemplating entering a drug regimen is more likely to wonder, “What
is the likely beneﬁt of this treatment given that I actually take it?” as
opposed to, “What is the average treatment beneﬁt for persons assigned to
take this treatment in a clinical trial?” We face a classic case of needing to
answer the right question, rather than the question that is easiest to pose
and answer. In short, we have to think about causality.
The problem of noncompliance and its impact on causality has plagued
Bayesians and frequentists alike. Both camps require extra model assump-
tions to advance beyond standard ITT approaches while still ensuring all
model parameters are identiﬁable. A signiﬁcant amount of the frequentist
statistical literature in this area has arisen from the work of J.M. Robins
and colleagues. For instance, Robins and Tsiatis (1991) extended the usual
accelerated failure time model with time-dependent covariates to a class of
semiparametric failure time models called structural failure time models,
and proposed a rank-based estimation method. Robins (1998) broadened
the set of models, focusing attention on several classes of what he termed
structural nested models. This work assumes the decision whether or not
to comply with treatment assignment is random conditional on the history
of a collection of prognostic factors. Greenland, Lanes, and Jara (2008)
explore the use of structural nested models and advocate what they call
g-estimation, a form of test-based estimation adhering to the ITT princi-
ple and accommodating a semiparametric Cox partial likelihood. In these
authors’ data illustration, g-estimation does produce a slightly larger esti-
mated treatment eﬀect than ITT, but also a signiﬁcantly wider conﬁdence
interval, reﬂecting what they argue is the true, higher level of uncertainty,
the “statistical price” of noncompliance.
For the most part, the work is highly theoretical and notoriously dif-
ﬁcult, though certainly not bereft of good data analysis; see e.g. Robins
and Greenland (1994). A forthcoming textbook (Hern´an and Robins, to
appear) ﬁgures to shed substantial light on frequentist causal inference,
both model-based and model-free. Of course, many good textbooks on the
subject already exist; of these, Pearl (2000) is worthy of special mention.
On the Bayesian side, the literature is dominated by the work of D.B.
Rubin and colleagues, and especially the “Rubin causal model” (Holland,
1986; Rubin, 2005). This framework, which is reminiscent of instrumen-
tal variables approaches in econometrics, also makes certain assumptions
about the underlying state of nature in order to ensure identiﬁability. The
usual reference is Imbens and Rubin (1997); here we follow the less techni-
cal summary in Mealli and Rubin (2002). These authors describe the basic
two-arm randomized trial model, where we set Zi = 1 if subject i is as-

84
BASICS OF BAYESIAN INFERENCE
signed to the active treatment arm, and Zi = 0 if assigned to the control
arm. The model imagines the existence of two potential outcomes Yi(1) and
Yi(0), only one of which is observed for each individual, depending on their
treatment assignment. In this basic model, compliance is assumed to be
“all or nothing”; i.e., some subjects assigned to the new treatment will not
take it, while some assigned to the control will take the new treatment.
The latter case is possibly rare in many carefully controlled trials, but does
occur in settings where randomization is not to a particular experimental
drug but to “encouragement” of some sort, where patients in the treatment
group are merely encouraged to take a treatment (say, a vaccine injection)
to which patients assigned to control would also have access.
Next let Di(z) indicate the treatment actually received (again, 1 for
treatment, 0 for control). These two indicators then partition the popula-
tion into four groups:
• always takers (ATs), for whom Di(z) = 1 regardless of z,
• never takers (NTs), for whom Di(z) = 0 regardless of z,
• compliers (Cs), for whom Di(z) = z, and
• deﬁers (Ds), for whom Di(z) = 1 −z .
The model does not assume that exact group membership is observed; a
subject assigned to treatment who actually receives control could be a deﬁer
or a never taker. However, thanks to randomization, the distribution across
the four groups is at least roughly the same in each treatment arm.
As mentioned above, the problem with ITT analysis is that its interpre-
tation is diﬃcult when compliance diﬀers across treatment group. In the
language used above, we can write the ITT eﬀect as
ITT = ηCITTC + ηNT ITTNT + ηAT ITTAT + ηDITTD ,
(2.32)
where the η’s give the proportion of subjects in each of the four classes,
and the ITTs give the eﬀect of treatment assignment on subjects of that
type. Since our data cannot identify all these ITT parameters, certain as-
sumptions must be made. The ﬁrst is the so-called exclusion restriction,
which essentially argues that since treatment assignment does not alter the
compliance behavior for ATs and NTs, it should not alter their outcomes
either, and therefore we may set ITTAT = ITTNT = 0. Note there is no
classical statistical estimate for either of these two quantities (since we
never see ATs in the control group, nor NTs in the treatment group), so
assuming them equal to 0 is certainly a convenient escape. A second com-
mon assumption is the so-called monotonicity assumption, which in this
context is equivalent to assuming that there are no deﬁers (e.g., ηD = 0).
Adding this to the exclusion restriction means that the eﬀect of treatment
assignment on compilers, ITTC, can be consistently estimated via (2.32)
provided both the Yi(Zi) and Di(Zi) are observed. In the presence of non-
compliance (ηC < 1), it’s easy to see that ITT < ITTC, i.e., the usual ITT

PRINCIPLES OF BAYESIAN CLINICAL TRIAL DESIGN
85
estimate is a conservative estimate of ITTC. However, relaxing the exclu-
sion restriction for NTs or ATs, this intuitive result may not manifest.
The Bayesian paradigm pays dividends in this setting by using proper pri-
ors. Combined with the (often weak) information in the data regarding the
size of the eﬀect in the four groups, we can often obtain improved estimates
that better reﬂect the true eﬀect of treatment assignment for noncompliers.
Hirano et al. (2000) oﬀer an example from a trial where one group of physi-
cians were encouraged to remind their patients to receive ﬂu shots, while
another group received no such special encouragement. A standard ITT
analysis suggests a 1.4% decrease in hospitalization rate in the encour-
agement group, suggesting a modest beneﬁt arising from the treatment.
However, in a reanalysis assuming monotonicity and ﬁxing ITTNT = 0
but allowing ITTAT ̸= 0, these authors ﬁnd ITTC ≈ITTAT ; the beneﬁt
arising from encouragement was roughly the same for the compliers as for
subjects who would have gotten the ﬂu shot no matter what. This in turn
suggests the shot itself is not very eﬀective, a counterintuitive result that
the authors suggest may be due to some encouraged ATs getting their ﬂu
shots a bit earlier than normal, which in turn provided as much beneﬁt as
the ﬂu shot itself did for the compliers.
Mealli and Rubin (2002) go on to outline further enhancements to the
model to accommodate missing outcomes in the presence of noncompliance.
The most recent work in this area builds on (and uses the dataset of) Efron
and Feldman (1991): Jin and Rubin (2008) use principal stratiﬁcation to
extend to the case of partial compliance, where each patient may only
take some portion of the assigned dose. This paper also allows diﬀerential
compliance levels in the treatment and control groups. Again, the work
is technical, but guided by Bayesian principles that at least permit a fair
comparison across models and informative priors.
While a full description of the technical issues involved in Bayesian causal
inference is well beyond the scope of this book, in the remainder of this
subsection we do provide some ﬂavor for the complexity of the modeling as
practiced today by describing the approach of Chib and Jacobi (2008). This
work is technically challenging, even for a model slightly simpler than that
of Imbens and Rubin (1997), which to us only reemphasizes the necessity of
being Bayesian, at least formally, just to make sense of such causal models
and provide a framework for judging performance. Chib and Jacobi (2008)
consider the case of an eligibility trial, where
Di(Zi) =
½
0
if Zi = 0
0 or 1
if Zi = 1 .
That is, persons assigned to control do not have access to the drug and must
therefore comply; there are no ATs. Chib and Jacobi (2008) assume that
xi ≡Di(zi) is observed for every subject, as is wi, a p-vector of observed
confounders that simultaneously aﬀect both the outcome and the intake

86
BASICS OF BAYESIAN INFERENCE
in the treatment arm. Writing the two potential outcomes as Y0i and Y1i,
these authors go on to model the joint density of each with drug intake
given the treatment assignment and the confounders,
p(yi, xi = 0|wi, zi = ℓ)
=
p(y0i, xi = 0|wi, zi = ℓ)
and p(yi, xi = 1|wi, zi = ℓ)
=
p(y1i, xi = 1|wi, zi = ℓ) ,
for ℓ= 0, 1. To specify these joint distributions, we ﬁrst deﬁne Iℓj = {i :
zi = ℓand xi = j}, and note that only I00, I10, and I11 are non-empty
(I01 = ∅since in this trial, those assigned the control cannot take the
treatment). If we further deﬁne si = 0 or 1 for never-takers and compliers,
respectively, then we can write
p(yi, xi = j|wi, zi = ℓ) =







(1 −qi)p0(yi|wi, si = 0)
+ qip0(yi|wi, si = 1)
if i ∈I00
(1 −qi)p0(yi|wi, si = 0)
if i ∈I10
qip1(yi|wi, si = 1)
if i ∈I11
,
where qi = P(si = 1|vi, α) = Φ(v′
iα), and Φ(·) is the standard normal cdf.
Chib and Jacobi (2008) go on to specify all the components in the above
expression, choosing Student t densities with means that involve linear
regressions on the wi for the conditional densities of the yi. Normal and
inverse gamma hyperpriors complete the model speciﬁcation. The authors
then provide a Gibbs-Metropolis algorithm for estimating the posterior
distributions of the model parameters, which in turn enables predictive
inference for compliers, and hence a natural estimate of the causal eﬀect.
This approach, while fairly involved, clariﬁes some previous literature by
making more explicit assumptions that can be debated and checked. Future
work in this area likely involves extending the approach to partial com-
pliance, clustered outcomes, binary response, and other more challenging
model settings. From a practical viewpoint, adoption of all of these strate-
gies awaits user-friendly software, and perhaps more importantly, greater
agreement among trialists that making the sorts of subjective assumptions
about compliance required by these models is preferable to simply living
with an ITT analysis that, while undoubtedly imperfect, at least oﬀers an
interpretable and typically conservative option.
2.6 Appendix: R Macros
The online supplement to this chapter
www.biostat.umn.edu/~brad/software/BCLM_ch2.html
provides the R and BUGS code that was used in this chapter, including
that for the basic clinical trial operating characteristic simulation program
described in Example 2.9 of Subsection 2.5.4.

CHAPTER 3
Phase I studies
In this chapter we tackle “early phase” problems, speciﬁcally those associ-
ated with phase I trials for safety and appropriate dosing of a new treat-
ment. Representing the ﬁrst application of a new drug to humans, early
phase trials are typically small – say, 20 to 50 patients. The main goal in
the early phases is to establish the safety of a proposed drug, and to study
what the body does to the drug as it moves through the body (pharmacoki-
netics), and what the drug in turn does to the body (pharmacodynamics).
Determining an appropriate dosing schedule for a drug, or dose-ﬁnding,
is a major component of phase I studies. For relatively nontoxic agents,
phase I trials may start with healthy volunteers. For agents with known
toxicity, such as cytotoxic agents in cancer therapy, phase I trials are con-
ducted among cancer patients for whom standard therapies have failed.
We will use drug development in cancer therapy as our main example in
this and subsequent chapters to demonstrate the application of adaptive
Bayesian methods, but stress that the methods are equally applicable in a
wide variety of non-cancer drug and device settings.
For developing a cytotoxic agent, the highest possible dose is sought,
since the beneﬁt of the new treatment is believed to increase with dose.
Unfortunately, the severity of toxicity is also expected to increase with dose,
so the challenge is to increase the dose without causing an unacceptable
amount of toxicity in the patients. Thus the primary goal of a phase I
study is to identify this dose, the maximum tolerated dose (MTD) in a
dose-escalation fashion.
Key elements of phase I studies include (a) deﬁning the starting dose, (b)
deﬁning the toxicity proﬁle and dose-limiting toxicity (DLT), (c) deﬁning an
acceptable level of toxicity, the target toxicity level (TTL), and (d) deﬁning
a dose escalation scheme. For the ﬁrst study in humans, the starting dose is
often chosen as one tenth of the LD10 (a lethal dose for 10% of the animals)
in mice, or one third of the lowest toxic dose in dogs, as these doses have
been shown to be safe in humans for cytotoxic agents (Collins et al., 1986).
While starting with a safe and low dose is important, investigators must
balance the risk of toxicity with the risk of treating patients with drugs
at ineﬀective doses. For most drugs, we assume that as the dose increases,

88
PHASE I STUDIES
the probability of toxicity and the probability of eﬃcacy will both increase.
Hence, the goal is to deﬁne the MTD or the recommended phase II dose
(RP2D) which yields an acceptable TTL – typically between 20% and 33%.
The dose escalation scheme contains three components: (i) a dose in-
crement, (ii) a dose assignment, and (iii) a cohort size. Many studies use
pre-determined dose increments at ﬁxed doses, such as 10 mg, 20 mg, 30
mg, and so on. Alternatively, we may specify a general scheme for setting
the doses, such as doubling the current dose when no toxicities are ob-
served, reducing to a 50% dose increment when non-dose-limiting toxicities
are observed, and reducing to a 25% dose increment when a DLT is ob-
served. In the examples of this chapter, we assume that all the dose levels
are speciﬁed in advance. We also generally assume that new patients are
treated in cohorts of a prespeciﬁed size (say, 1, 3, or 6).
Dose assignment refers to how to new patients enrolled in the trial are
assigned to dose levels. Based on dose assignment, phase I trials can be
classiﬁed into rule-based methods and model-based methods. The next two
sections consider each of these broad areas in turn.
3.1 Rule-based designs for determining the MTD
Standard rule-based designs assign new patients to dose levels according
to prespeciﬁed rules and without stipulating any assumption regarding the
dose-toxicity curve. These designs belong to the class of “up-and-down”
designs (Dixon and Mood, 1948; Storer, 1989), as they allow dose esca-
lation and de-escalation based on the absence or presence of toxicity in
the previous cohort. The simple up-and-down design converges to a dose
corresponding to a probability of DLT around 50%. The traditional 3+3
design is a rule-based design which remains widely used in clinical practice.
Variations of the 3+3 design, such as the pharmacologically guided dose
escalation method (Collins et al., 1990) and accelerated titration designs
(Simon et al., 1997), have also been applied in clinical trials.
3.1.1 Traditional 3+3 design
The traditional 3+3 design involves no modeling of the dose-toxicity curve
beyond assuming that toxicity increases with dose. The design proceeds in
cohorts of three patients, the ﬁrst cohort being treated at a starting dose,
and the next cohorts being treated at increasing dose levels that have been
ﬁxed in advance. Dose levels have historically been chosen according to
some variation of a Fibonacci sequence. A Fibonacci sequence is a sequence
of numbers where each number is the sum of the two previous numbers
in the sequence; an example is {1,1,2,3,5,8,...}. The doses are increased
according to the percentage increase between successive numbers in the
Fibonacci sequence; for this example {100,50,67,60,...}. Often, a modiﬁed

RULE-BASED DESIGNS FOR DETERMINING THE MTD
89
1.
Dose Level
2.
Dose Level
Cohort
1
2
3
4
5
Cohort
1
2
3
4
5
1
0/3
1
0/3
2
0/3
2
0/3
3
1/3
3
0/3
4
0/3
4
2/3
5
2/3
5
0/3
MTD
***
MTD
***
Figure 3.1 Example of the traditional 3+3 design; entries are (number of
DLTs/number of patients treated) by cohort and dose level.
sequence such as {100,67,50,40,33} is used so that the increments decrease
as the dose level increases.
There are slight variations on the traditional 3+3 design, but a com-
monly used version is as follows. If none of the three patients in a cohort
experiences a DLT, another three patients will be treated at the next higher
dose level. If one of the ﬁrst three patients experiences a DLT, three more
patients will be treated at the same dose level. The dose escalation con-
tinues but stops as soon as at least two patients experience DLTs, among
a total of up to six patients (i.e. probability of DLT at the dose≥33%).
The MTD is typically deﬁned as the highest dose level in which at least 6
patients are treated and where no more than 33% of the patients experi-
ence DLT. Thus, a summary of one common version of the approach is as
follows:
Algorithm 3.1 (3+3 design)
Step 1: Enter 3 patients at the lowest dose level
Step 2: Observe the toxicity outcome
0/3 DLT ⇒Treat next 3 patients at next higher dose
1/3 DLT ⇒Treat next 3 patients at the same dose
1/3 + 0/3 DLT ⇒Treat next 3 patients at next higher dose
1/3 + 1/3 DLT ⇒Deﬁne this dose as MTD
1/3 + 2/3 or 3/3 DLT ⇒dose exceeds MTD
2/3 or 3/3 DLT ⇒dose exceeds MTD
Step 3: Repeat Step 2 until MTD is reached. If the last dose exceeds
MTD, deﬁne the previous dose level as MTD if 6 or more patients were
treated at that level. Otherwise, treat more patients at the previous dose
level.
Step 4: MTD is deﬁned as a dose with ≤2/6 DLT
Figure 3.1 depicts two simple idealized illustrations of the 3+3 design. In
the ﬁrst panel of the ﬁgure, ﬁve cohorts of patients were treated sequentially

90
PHASE I STUDIES
Figure 3.2 Result of the taxotere trial applying the traditional 3+3 design; entries
are (number of DLTs/number of patients treated) by cohort and dose level. Notes:
(a) the 2 mg/m2/day x 5 days dose was skipped when another study reported no
toxicities at a total dose level greater than 10 mg/m2 just after this trial began;
(b) after observing DLTs, the study was expanded to include a heavily pretreated
group and a non-heavily pretreated group; (c) these intermediate doses were added
after the trial began.
in four dose levels. Dose level 3 is chosen as the MTD with an estimated
DLT rate of 16.7%. Similarly, in the second panel of Figure 3.1, dose 3 was
chosen as the MTD but the estimated DLT rate is 0%. Had one or two
DLTs been observed in the three patients in Cohort 5, dose 3 would still
have been chosen as the MTD in this case, with a DLT rate of 33%. The
examples show that the choice of the MTD in the traditional 3+3 design
is ad hoc and fairly imprecise.
We now give two examples showing how the traditional 3+3 design is
used in practice.
Example 3.1 Figure 3.2 shows the results from the taxotere trial reported
by Pazdur et al. (1992). In this trial, a total of 39 patients were treated. The
ﬁrst DLT was observed at the dose level of 16 mg/m2. The initial toxicities
were seen in heavily pretreated patients and the investigators decided to
expand the cohort to include non-heavily pretreated patients as well. The
dose was eventually shown to be too toxic; hence, two intermediate doses
were added to the trial. The MTD was deﬁned as the 14 mg/m2/day level,
for which the estimated DLT rate was 0.25.
Example 3.2 Figure 3.3 shows the results of a second trial employing a
3+3 design, the gemcitabine trial reported by Abbruzzese et al. (1991). The
starting dose level, 10 mg/m2 was chosen as 1/20 of the rat LD10. This trial
took 12 dose escalations to determine the MTD as 790 mg/m2, a dose level
79 times the starting dose. This trial illustrates that the traditional 3+3

RULE-BASED DESIGNS FOR DETERMINING THE MTD
91
Figure 3.3 Result of the gemcitabine trial applying the traditional 3+3 design;
entries are (number of DLTs/number of patients treated) by cohort and dose
level.
design can be very ineﬃcient when the starting dose is too low and the dose
increment is moderate. In a recent report (Le Tourneau et al., 2009), 19
anticancer agents were approved by the US Food and Drug Administration
(FDA) in solid tumors using the traditional 3+3 design. Among them, more
than half involved six or more dose levels.
3.1.2 Pharmacologically guided dose escalation
To more eﬃciently identify the dose region containing the MTD, the phar-
macologically guided dose escalation (PGDE) method assumes that DLTs
can be predicted by drug plasma concentrations, based on animal data
(Collins et al., 1990). The PGDE method is carried out in two stages. In the
ﬁrst stage, pharmacokinetic data are measured for each patient in real time
to determine the subsequent dose level. As long as a pre-speciﬁed plasma
exposure deﬁned by the area under the concentration-time curve (AUC),
extrapolated from preclinical data, is not reached, dose escalation proceeds
with one patient per dose level, typically at 100% dose increments. Once
the target AUC is reached or if DLTs occur, dose escalation switches to the
traditional 3+3 design with smaller dose increments (usually around 40%).
In clinical practice, the PGDE method has achieved good results with some
cytotoxic agents such as certain anthracyclines and platinum compounds,
while the method has been found to be inappropriate for other classes of

92
PHASE I STUDIES
agents such as antifolates that display a high interpatient pharmacokinetic
heterogeneity. The logistical diﬃculties in obtaining real-time pharmacoki-
netic results and in extrapolating preclinical pharmacokinetic data to phase
I studies also impedes the success of the PGDE method.
3.1.3 Accelerated titration designs
The accelerated titration design (ATD; Simon et al., 1997) is a commonly
used variation of the traditional 3+3 design in which intrapatient dose es-
calation is allowed in multiple cycles of the same patient. Although the
dose escalation scheme is rule-based, all the observed data can be used
to provide further modeling of the dose-toxicity curves. Two-stage designs
with an accelerated phase used in ATD theoretically help to reduce the
number of patients treated at subtherapeutic doses. Permitting intrapa-
tient dose escalation is also appealing because it gives some patients the
opportunity to be treated at higher, presumably more eﬀective doses. The
main drawback of intrapatient dose escalation is that it may mask any cu-
mulative eﬀects of treatment, and would certainly make them less obvious
and diﬃcult to diﬀerentiate from chronic or delayed toxicity.
3.1.4 Other rule-based designs
Alternative rule-based designs besides the 3+3 have been proposed, includ-
ing the “2+4”, “3+3+3” and “3+1+1” (also referred to as “best of ﬁve”
rule) designs; see Storer (2001). In the 2+4 design, an additional cohort of
four patients is added if one DLT is observed in a ﬁrst cohort of two pa-
tients. The stopping rule is the same as with the traditional “3+3” design.
In the 3+3+3 design, a third cohort of three patients is added if two of six
patients experienced a DLT at a certain dose level. The trial terminates if
at least three of nine patients experience a DLT. Finally, the “best of ﬁve”
design is more aggressive, as one additional patient can be added if one or
two DLTs are observed among the ﬁrst three patients. Another patient will
be added if two DLTs are observed among the four treated patients. Dose
escalation is allowed if 0/3, 1/4 or 2/5 DLTs are observed, while the trial
will terminate if three or more DLTs are observed.
3.1.5 Summary of rule-based designs
The advantages of the rule-based methods are that they are easy to imple-
ment and do not require specialized software. Their performance (operating
characteristics), however, may not be particularly attractive. For example,
their target toxicity levels are implicit, and ﬁxed after the rule is speciﬁed.
The methods may also be ineﬃcient in getting to a drug’s “action zone.”
As a result, in a review of studies at M.D. Anderson Cancer Center, only

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
93
Figure 3.4 Illustration of a dose-toxicity curve for a model-based design. If the
target toxicity level (TTL) is 33%, dose level 4 is the MTD, since it comes closest
to yielding the desired TTL.
about 3% of the patients responded to the assigned treatment in phase I
trials (Smith et al., 1996). The decision on dose allocation for future pa-
tients, as well as the deﬁnition of MTD or RP2D, is “memoryless” in that
it relies on information from the current dose level only, and thus does not
use all available information. As such, the MTD is then selected from the
pre-speciﬁed dose levels depending on which one best ﬁts the deﬁnition set
a priori. In addition, the target toxicity level is ﬁxed and implicitly speci-
ﬁed once the rule is set. Although the implicit TTL can be calculated (Lin
and Shih, 2001), the design is rigid and the rule often needs to be “bent”
to target a particular TTL.
3.2 Model-based designs for determining the MTD
An alternative to the rule-based methods for ﬁnding the MTD is to assume
that there is a monotonic dose-response relationship between the dose and
the probability of DLT for patients treated at that dose; see Figure 3.4.
In this approach, a dose-toxicity curve as well as the TTL are explicitly
deﬁned. The goal for the phase I clinical trial is, through treating patients
in a dose escalation fashion, to seek a suitable quantile of the dose-toxicity
curve; speciﬁcally, a dose that will induce a probability of DLT at a speciﬁed
target toxicity level. This method is most conveniently carried out under the
Bayesian framework. Simple one- or two- parameter parametric models are
often used to characterize the dose-toxicity relationship, with the Bayesian

94
PHASE I STUDIES
Figure 3.5 Typical CRM dose-toxicity response curves: left, hyperbolic tangent;
right, power.
posterior distribution used to estimate the parameters. These designs use
all the data to model the dose-toxicity curve, and provide a credible interval
for the MTD at the end of the trial.
3.2.1 Continual reassessment method (CRM)
The continual reassessment method (CRM) seems to have been the ﬁrst
Bayesian model-based phase I design introduced in the literature (O’Quigley
et al., 1990). In its most basic form, this method characterizes the dose-
toxicity relationship by simple one-parameter parametric models, such as
the hyperbolic tangent model, logistic model, or the power model. Specif-
ically, letting p(d) be the probability of DLT at dose d, these three para-
metric models are given by
Hyperbolic tangent: p(d)
=
[(tanh(d) + 1)/2]a =
·
exp(d)
exp(d) + exp(−d)
¸a
Logistic: p(d)
=
exp(3 + ad)
1 + exp(3 + ad)
Power: p(d)
=
dexp(a)
Figure 3.5 shows the diﬀerent shapes of the dose-toxicity curves for two of
these models for varying values of the parameter a.
The original CRM is carried out using the following algorithm:
Algorithm 3.2 (CRM design)
Step 1: Assume a vague or fully non-informative prior for a.

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
95
Step 2: Treat 1 patient at the level closest to the current estimate of
the MTD.
Step 3: Observe the toxicity outcome.
Step 4: Update a by computing its posterior distribution. This is of
course obtained by multiplying the prior chosen in Step 1 by the likeli-
hood, which after treating n patients is given by
L(a; d, y) ∝
n
Y
i=1
p(di)yi[1 −p(di)]1−yi ,
where di and yi are the dose level and toxicity outcome for patient i, and
where yi = 1 if a DLT is observed and yi = 0 if not. In this simple one-
parameter setting, the posterior arising from (2.1) might be most easily
computed by a standard numerical quadrature method (e.g., trapezoidal
rule), but of course MCMC methods (say, as implemented in WinBUGS)
can also be used.
Step 5: Treat the next patient at the level closest to the updated esti-
mate of MTD based on the posterior distribution of a.
Step 6: Repeat Steps 1–5 until a suﬃciently precise estimate of a is
achieved or the maximum sample size is reached.
The choice of the dose-toxicity curve and the initial estimate of a will gen-
erally be elicited from experts familiar with drug development. Although
this initial estimate may be inaccurate, it should provide an adequate start-
ing point for dose escalation. As the trial moves along, a more accurate
estimate of a is obtained; hence, more patients are treated at the dose
thought to be closest to the MTD, which corresponds to the dose at the
target toxicity level that maximizes the treatment eﬀect while controlling
toxicity. The original CRM allows jumping over multiple dose levels if so
indicated in Step 5 above.
The CRM was not well-accepted in its original format due to safety
considerations, as it could expose patients to unacceptably toxic doses if
the prespeciﬁed model were incorrect. Consequently, modiﬁcations to the
CRM were proposed to add additional safety measures which include (1)
treating the ﬁrst patient at the lowest starting dose level based on animal
toxicology and conventional criteria, (2) increasing the dose by only one pre-
speciﬁed level at a time, (3) not allowing dose escalation for the immediate
next patient if a patient experienced a DLT, and (4) treating more than
one patient at the same dose level, especially at higher dose levels. For
more on these and other CRM modiﬁcations, see Korn et al. (1994), Faries
(1994), Goodman et al. (1995), Piantadosi et al. (1998), and Heyd and
Carlin (1999).

96
PHASE I STUDIES
Dose Level
1
2
3
4
5
6
Dose (mg/m2)
10
20
40
60
75
90
Prob(toxicity)
0.05
0.10
0.20
0.30
0.50
0.70
Standardized dose
–1.47
–1.1
–0.69
–0.42
0
0.42
Table 3.1 Dose levels, standardized doses, and prior probabilities of toxicity, CRM
example.
Figure 3.6 Plots of the posterior distribution of a and the corresponding dose-
toxicity curve for a CRM trial, Step 0: based only on the prior information;
current MTD = dose 3.
We now oﬀer a simple example to illustrate how CRM is implemented
in practice.
Example 3.3 Suppose that in developing a new agent, six dose levels are
to be studied. We assume a hyperbolic tangent dose-toxicity curve with
target toxicity level set at 20%. Note that the actual dose level is not
important in the calculation. For convenience, the standardized dose can
be calculated as tanh−1(2p −1) assuming a = 1. This is legitimate since,
due to the one-to-one correspondence between a and d when p is given, we
can set a at any ﬁxed value to calculate the standardized dose. We assume
the prior distribution for a follows an exponential distribution with mean
1. As a result, the prior probability of DLT at each dose will center around
the initial estimate of p with a fairly wide credible interval. The dose levels
and our prior beliefs regarding the probability of toxicity at each dose level
are given in Table 3.1.
Figures 3.6–3.11 show a realization of one such trial applying the original

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
97
Figure 3.7 Plots of the posterior distribution of a and the corresponding dose-
toxicity curve for a CRM trial after Step 1: treat ﬁrst patient at dose 3; result =
no DLT; updated MTD = dose 4.
Figure 3.8 Plots of the posterior distribution of a and the corresponding dose-
toxicity curve for a CRM trial after Step 2: treat second patient at dose 4; result
= no DLT; updated MTD = dose 4.
CRM. The left panel of Figure 3.6 shows the prior distribution of a following
the unit exponential distribution. The right panel shows the dose toxicity
curve at the posterior mean of a. From the dose-toxicity curve, we ﬁnd
that dose level 3 is the dose closest to the current estimate of the MTD
which yields a TTL of 0.2. Figure 3.7 shows the result (no DLT) after
the ﬁrst patient is treated at dose 3. With this information, the posterior
distribution of a is calculated. The resulting dose-toxicity curve shows that
the updated estimate of MTD is now dose level 4.

98
PHASE I STUDIES
Figure 3.9 Plots of the posterior distribution of a and the corresponding dose-
toxicity curve for a CRM trial after Step 3: treat third patient at dose 4; result =
no DLT; updated MTD = dose 5.
Figure 3.10 Plots of the posterior distribution of a and the corresponding dose-
toxicity curve for a CRM trial after Step 4: treat fourth patient at dose 5; result
= DLT; updated MTD = dose 4.
Suppose the second patient treated is treated at dose level 4 and results
in no DLT. Figure 3.8 shows that the dose-toxicity curve continues to move
downward because no DLT is found. At this point, the current estimate of
MTD remains at dose level 4. The third patient is treated at dose level 4
and again no DLT is observed. Figure 3.9 shows that the new estimated
MTD moves up to dose level 5. Patient 4 is treated at dose 5 and develops
the ﬁrst DLT. The posterior of a is updated again (Figure 3.10, left panel),
and the resulting MTD now moves back to dose level 4.
To bring this example to a close, suppose the trial terminates at this

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
99
Figure 3.11 CRM trial, Step 5: stop the trial now; the ﬁnal posterior cdf of a and
corresponding dose-toxicity curve (with 80% conﬁdence bounds) are as shown.
point. Then the ﬁnal cumulative distribution of a is shown in the left panel
of Figure 3.11, with the corresponding dose-toxicity curve and its 80%
credible interval boundaries shown in the right panel. At the end of the
trial, the posterior mean of a is 1.33. Thus dose level 4 is chosen as the
ﬁnal MTD because it is the dose which yields a probability of DLT closest
to the targeted toxicity level.
In the previous example, all necessary calculations were carried out in R.
This code can be nested within a simulation program in order to evaluate
and compare the operating characteristics of the 3+3 and CRM designs.
Speciﬁcally, Table 3.2 compares the performance of the 3+3 and two CRM
designs using 10,000 simulated trials over three scenarios, each of which
operate over ﬁve dose levels. In all cases, we set the target toxicity level
to 0.30. In Scenario 1, the true probabilities of DLT at dose levels 1 to
5 are 0.05, 0.15, 0.30, 0.45, and 0.60, respectively. Therefore, dose level
3 is the true MTD. The percentages of patients treated at the ﬁve dose
levels using the 3+3 design are 26.0, 32.5, 27.2, 12.1, and 2.3, respectively.
Alternatively, using the CRM design with a cohort size of 1 (CRM 1), the
corresponding percentages are 15.6, 24.1, 34.7, 19.0, and 6.7. Thus more
patients are treated at the true MTD level, but more are also treated at
dose levels above the MTD. The overall percent of DLT for the 3+3 and
CRM 1 designs are 21.1 and 27.0, respectively. However, by increasing the
CRM cohort size from 1 to 3, this new design (CRM 3) treats fewer patients
at levels above the MTD. At the end of the trial, the percentages of trials
recommending dose level 3 as the MTD are 27.5, 52.4, and 49.8 for the
3+3, CRM 1, and CRM 3, respectively. Thus, compared to the 3+3 design,
CRM designs are much more likely to identify the correct MTD level.
Scenario 2 depicts the case when the probabilities of DLT at dose levels

100
PHASE I STUDIES
Dose
Ave
%
1
2
3
4
5
N
DLT
Scenario 1
P(DLT):
0.05
0.15
0.30
0.45
0.60
3+3
% patients
26.0
32.5
27.2
12.1
2.3
15.2
21.1
% MTD
20.5
42.7
27.5
5.7
0
CRM 1
% patients
15.6
24.1
34.7
19.0
6.7
18.5
27.0
% MTD
1.0
21.4
52.4
23.0
2.2
CRM 3
% patients
21.3
31.4
29.1
15.8
2.5
19.0
23.3
% MTD
1.5
22.6
49.8
23.7
2.4
Scenario 2
P(DLT):
0.05
0.10
0.20
0.30
0.50
3+3
% patients
21.6
25.7
26.4
18.9
7.3
16.9
18.3
% MTD
9.5
28.5
33
21.1
0
CRM 1
% patients
13.0
13.2
23.3
30.4
20.2
18.6
25.7
% MTD
0.1
6.4
25.6
49.4
18.5
CRM 3
% patients
19.3
19.8
25.2
25.2
10.5
19.1
20.8
% MTD
0.2
5.5
25.4
48.3
20.5
Scenario 3
P(DLT):
0.15
0.30
0.45
0.60
0.85
3+3
% patients
43.9
36.4
16.3
3.2
0.2
11.6
27.0
% MTD
65.4
27.9
6.3
0.4
0.0
CRM 1
% patients
40.5
35.4
17.7
6.1
0.3
18.5
28.7
% MTD
24.5
52.8
19.9
2.8
0.1
CRM 3
% patients
41.5
39.0
15.3
4.1
0.1
18.5
27.3
% MTD
23.6
53.7
19.6
3.0
0.1
Table 3.2 Simulation studies for comparing the operating characteristics of the
3+3 design versus the CRM designs. Abbreviations: CRM 1, CRM with cohort
size 1; CRM 3, CRM with cohort size 3; P(DLT), probability of DLT at each dose
level; % patients, % of patients treated at each dose level; % MTD, % of dose level
selected as the MTD; Ave N, average sample size in the trial; % DLT: percentage
of patients developing DLT. Values corresponding to the true MTD (0.30) are
shown in boldface type; value in slanted bold includes a 21.3% chance that
the recommended MTD is below Dose 1.
1 to 5 are 0.05, 0.10, 0.20, 0.30, and 0.50, respectively, a case where the
assigned doses are somewhat less toxic than anticipated. Again targeting
30% DLT, this means that dose level 4 is the true MTD in this scenario. For
the 3+3, CRM 1, and CRM 3 designs, the percentages of patients treated at
dose 4 are 18.9, 30.4, and 25.2, respectively. The corresponding percentages
for choosing dose 4 as the MTD are 21.1, 49.4, and 48.3, respectively.

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
101
Again, the CRM designs are much more likely to identify the correct MTD
compared to the 3+3 design. The overall percentages of DLT for each of
the three designs are 18.3, 25.7, and 20.8, respectively, showing that CRM
1 has the highest proportion of patients developing DLT, but one that is
still below the 30% target level. CRM 3 again oﬀers some protection in this
regard relative to CRM 1, and for only a fractionally higher average sample
size (“Ave N” in the table).
Scenario 3 illustrates a high-toxcity case, with DLT probabilities at dose
levels 1 to 5 of 0.15, 0.30, 0.45, 0.60, and 0.85. In this scenario, the second
dose is now the true MTD. The percentages of patients treated at dose
level 2 for the 3+3, CRM 1, and CRM 3 designs are 36.4, 35.4, and 39.0,
respectively. The percentages correctly identifying dose 2 as the MTD are
27.9, 52.8, and 53.7, respectively, As before, the CRM methods have a much
better chance of correctly identifying the MTD. However, the percentages
of patients treated at high dose levels (say, level 3 and above) are slightly
higher for the CRM methods, as are the percentages of DLT (28.7 and 27.3
for CRM 1 and CRM 3, compared to 27.0 for the 3+3). So while the CRM
methods are slightly more aggressive, the price paid in terms of higher risk
of DLT (a few percent) seems small relative to the much more dramatic
(roughly 25%) rise in correct selection probability, relative to the overly
conservative 3+3 method. We also note that the 65.4% chance of selecting
the lowest dose as the MTD actually includes a 21.3% chance that the
method will determine the MTD to be below even this initial dose, which
occurs with 3+3 when the ﬁrst dose is determined to be too toxic.
In summary, CRM designs can identify the MTD level much more accu-
rately compared to the 3+3 design. On the other hand, CRM designs treat
more patients at the MTD level and above. Choosing a CRM cohort size
of 3 instead of 1 oﬀers some protection, reducing the number of patients
treated at levels above the DLT.
Software note: The M.D. Anderson website,
http : //biostatistics.mdanderson.org/SoftwareDownload/
oﬀers a freely downloadable stand-alone program called CRMSimulator. This
program has a friendly user interface, and can do simulations and be used to
run trials for a modest set of settings (e.g., only the power model is imple-
mented).
Alternatively, this book’s software page for this chapter,
www.biostat.umn.edu/~brad/software/BCLM_ch3.html
contains an R program called phaseIsim.R that was used to produce the results
above. This program can run both 3+3 and CRM, and permits the hyperbolic
tangent, logistic, and power models for the latter. This page also oﬀers two
other programs, CRMexplore and CRMinteractive, that allow users to draw
the families of dose toxicity curves and interactively conduct CRM trials.

102
PHASE I STUDIES
Finally, since R (actually BRugs) source code is available, users can easily
modify the code to ﬁt their own purposes. For example, the R commands
R code
p.tox0 <- c(.05,.15,.3,.45,.6)
s.dose
<- log(p.tox0/(1-p.tox0)) - 3
phaseIsim(nsim=10000, npat=30, sdose=s.dose, prob.tox=p.tox0,
design=30, outfile=’sc1aout.txt’)
will carry out a simulation using the 3+3 method for the ﬁve standardized
doses (s.dose) in Scenario 1 above. The alternate use of the program,
R code
phaseIsim(nsim=10000, npat=30, sdose=s.dose, prob.tox=p.tox0,
design=2, outfile=’sc1bout.txt’)
instead illustrates the case of CRM with a cohort size of 1, while
R code
phaseIsim(nsim=10000, npat=30, sdose=s.dose, prob.tox=p.tox0,
crm.group.size=3, design=2, outfile=’sc1cout.txt’)
handles the case of a cohort size of 3.
3.2.2 Escalation with overdose control (EWOC)
As we mentioned, the CRM method’s greatest virtue (its eﬃcient use of
all available information) also leads to its greatest weakness (its potential
for exposing patients to overly toxic doses if the ﬁrst few patient responses
are atypical or the model is misspeciﬁed). The mechanistic modiﬁcations of
Goodman et al. (1995) and others mentioned just prior to Example 3.3 are
attempts to limit wild swings in the MTD estimates as data accumulate.
Babb, Rogatko, and Zacks (1998) introduced an alternative approach that
directly seeks to reduce the risk of overdose; see also Zacks et al. (1998).
Called escalation with overdose control (EWOC), the method is the same as
CRM except in the way that it selects each successive new dose. While CRM
always uses the middle (say, the mean or the mode) of the MTD’s posterior
distribution as the next dose, EWOC instead selects the αth quantile, where
α, called the feasibility bound, is taken to be less than or equal to 0.5. The
“overdose control” then comes from the fact that the predicted probability
that each successive patient’s dose exceeds the MTD is only α; Babb et al.
suggest α = 0.25.
To be more speciﬁc, denote the target toxicity level (TTL) by θ, and the
dose by x, so that P(DLT|x = MTD) = θ. Let d1, . . . , dr be the ordered
dose levels available for experimentation, where we assume that d1 is safe
for humans and d1 < γ < dr. The EWOC algorithm proceeds as follows:
Algorithm 3.3 (EWOC design)
Step 1: Start with the lowest dose level, i.e., set x1 = d1.
Step 2: For any patient k, let πk(γ) be the posterior cumulative distri-
bution function (CDF) of the MTD, i.e.
πk(γ) = P(MTD ≤γ | yk) ,

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
103
where yk denotes the data available at the time of treatment for patient
k. EWOC selects the dose level xk such that
πk(xk) = α .
Since this xk will almost surely not be identical to any of our prespeciﬁed
dose levels di, we would instead choose dose
x∗
k = max{d1, . . . , dr : di −xk ≤T1 and πk(xk) −α ≤T2}
for prespeciﬁed nonnegative tolerances T1 and T2. This permits treat-
ment of patients at doses only slightly above the optimal dose xk.
Step 3: As in CRM, we repeat Step 2 until a suﬃciently precise estimate
of the MTD is achieved, or the maximum sample size n is reached. In
either case we estimate the MTD by the middle (mean, mode, or median)
of its posterior distribution.
The EWOC doses xk have an attractive decision-theoretic interpretation:
they minimize risk with respect to the asymmetric loss function
L(x, γ) =
½
α(γ −x)
for x ≤γ (i.e., x is an underdose)
(1 −α)(x −γ)
for x > γ (i.e., x is an overdose)
.
Note that choosing the feasibility bound α < 0.5 corresponds to placing a
higher penalty on overdosing than on underdosing; adminstration of a dose
δ units above the MTD is judged (1 −α)/α times worse than treating a
patient δ units below the MTD. Choosing α = 0.5 implies a symmetric loss
function, and indeed leads to the posterior median of the MTD as the new
dose; see e.g. Carlin and Louis (2009, Appendix B, Section B.3.1).
A somewhat unusual feature of the basic EWOC method is that when
α << 0.5, the ﬁnal dose recommended for phase II study (say, the median
of the MTD posterior distribution) may be signiﬁcantly larger than the
dose any phase I patient has received (say, the 25th percentile of the same
distribution). For this reason, the possibility of a varying feasibility bound
has been discussed by Babb and Rogatko (2001, 2004), as well as other au-
thors. Chu et al. (2009) propose a hybrid method that begins with EWOC
using α = 0.1, then gradually increases α according to a ﬁxed schedule
up to α = 0.5 near the end of the trial (thus concluding with a posterior
median version of CRM).
Current research in EWOC methods focuses on the incorporation of
patient-speciﬁc covariates, so that the dose assigned at each stage can be
“individualized.” For example, Babb and Rogatko (2001) consider the case
of a single continuous covariate, while current work by these and other
authors deals with the case of a binary covariate, as well as multiple co-
variates.
Software note:
Software for some basic EWOC design formulations is avail-
able; see Rogatko, Tighiouart, and Xu (2008) as well as the website of the

104
PHASE I STUDIES
biostatistics group at the Winship Cancer Institute at Emory University,
http : //www.sph.emory.edu/BRI −WCI/ewoc.html .
The current version, EWOC 2.0, is a free standalone Windows XP/Vista pack-
age, and features a complete and easy-to-follow online user’s guide.
Of course, Algorithm 3.3 may be implemented in general purpose Bayesian
packages as well. The next example provides a sample WinBUGS implemen-
tation of EWOC, courtesy of Prof. Brani Vidakovic of Georgia Institute of
Technology and Emory University.
Example 3.4 Consider an EWOC implementation using the logistic model,
Prob(DLT|dose = x) ≡p(x) =
exp(β0 + β1x)
1 + exp(β0 + β1x) .
Because it is diﬃcult to specify prior distributions on regression β’s, we
instead follow the advice of Kadane et al. (1980) and reparameterize from
(β0, β1) to (ρ0, γ), where ρ0 = p(Xmin), the probability of DLT at the
minimum dose, Xmin, and γ is the MTD. This reparameterization is easy
since
logit(ρ0)
=
β0 + β1Xmin
and
logit(θ)
=
β0 + β1γ ,
where θ is the TTL. Subtracting these two equations, we can easily solve
for β1 and then for β0 as
β0
=
1
γ −Xmin
[γ logit(ρ0) −Xminlogit(θ)]
and
β1
=
1
γ −Xmin
[logit(θ) −logit(ρ0)] .
Here we are assuming that γ lies in (Xmin, Xmax) with probability 1; we
would typically take the starting dose d1 = Xmin.
The following WinBUGS code speciﬁes the priors on γ and ρ0 simply as
independent uniforms on the ranges (Xmin, Xmax) and (0, θ), respectively.
BUGS code
model{
for (i in 1:N){
# Likelihood
Y[i]~dbern(p[i])
logit(p[i])<- (1/(gamma - Xmin))*(gamma*logit(rho0)
- Xmin*logit(theta)+(logit(theta)-logit(rho0))*X[i])
}
#
end of for loop
# Priors
gamma ~ dunif(Xmin, Xmax)
rho0 ~ dunif(0,theta)
}
#
end of BUGS code

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
105
# Data (1st patient 140, no tox):
list(Y=c(0), X=c(140), Xmin=140, Xmax =425, theta=0.333, N=1)
# Data (1st patient 140, no tox; 2nd patient 210, no tox):
list(Y=c(0,0), X=c(140,210), Xmin=140, Xmax=425, theta=0.333, N=2)
# Data (1st patient 140, no tox; 2nd patient 210, tox):
list(Y=c(0,1), X=c(140,210), Xmin=140, Xmax=425, theta=0.333, N=2)
# Data (1st patient 140, no tox; 2nd patient 210, no tox;
#
3rd patient 300, no response yet):
list(Y=c(0,0,NA),X=c(140,210,300),Xmin=140,Xmax=425,theta=0.333,N=3)
#Inits:
list(rho0=0.05, gamma=160)
The ﬁrst two accumulating datasets shown in this code are the same as
those used in the illustration in the EWOC 2.0 user’s guide. Note this code
assumes d1 = Xmin = 140, Xmax = 425, and θ = 1/3. The 10th, 25th,
and 50th percentiles of the MTD γ, which can be taken as the next dose,
are available in WinBUGS by choosing the appropriate percentile from the
percentile selection box in the Sample monitoring tool.
Running the code above using two parallel MCMC chains and the ﬁrst
dataset for 1000 burn-in MCMC iterations followed by 10,000 production
iterations yields a 25th percentile for γ of 212, which rounded to the nearest
10 is 210. This is the dose for the second patient added to the subsequent
datasets. Running the second dataset in the same way produces a 25th per-
centile for γ of 242 and a 50th percentile of 304. The ﬁnal dataset shows
the third patient receiving this higher dose, illustrating the case of increas-
ing α from 0.25 to 0.50 as the trial wears on. Running this ﬁnal dataset
produces 25th and 50th percentiles for γ of 240 and 304 — very similar to
the previous results since Y3 is assumed still to be unknown by this code.
The posterior predictive mean of Y3 in this case emerges as about 0.40,
slightly less than 0.50, indicating WinBUGS expects even this higher dose
(300, rounded from 304) to produce no toxicity.
The EWOC algorithm is readily modiﬁed to handle the case of patient-
speciﬁc covariates; see the book’s website for the case of a single binary
covariate case. Subsequent extension to continuous and multiple covariates
is forthcoming.
3.2.3 Time-to-event (TITE) monitoring
One key feature of the CRM method is its reliance on binary outcomes
only. The advantage here is increased model robustness, but an important
disadvantage is the limited information contained in the binary outcome. In

106
PHASE I STUDIES
many trials this outcome is deﬁned as an indicator for some adverse event
happening within a certain time horizon. There are several directions to
generalize the basic CRM to the case of time-to-event (TITE) outcomes.
One reasonable choice is to base the design on a parametric event time
model. This is the approach chosen in Thall, Wooten, and Tannir (2005)
and Cheung, Inoue, Wathen, and Thall (2006). Of course, a concern with
any parametric model is robustness with respect to the chosen parametric
family. The problem is far more important in design than in data analy-
sis. Good statistical inference always includes a critical look at the model
assumptions using residual analysis, appropriate plots, and formal tests
to critically evaluate the model. If need be, we can revise the model as-
sumptions as indicated. But this is not possible in design. When available,
historical data permits model criticism, and operating characteristics can
be used to investigate robustness with respect to possible model violations.
Alternatively, one proceeds with a minimal set of model assumptions.
This is the approach of Cheung and Chappell (2000), who introduce the
TITE-CRM as an extension of the CRM to TITE outcomes. Let p(d, a)
denote the probability of a toxicity for a patient assigned to dose d. We
use models p(d, a) as in the usual CRM setup (see Subsection 3.2.1). There
the response is assumed to be a binary outcome yi ∈{0, 1}, with yi = 1
indicating that the toxicity outcome was observed for the i-th patient. Let
di denote the dose assigned to the i-th patient, and let y = (y1, . . . , yn).
The likelihood conditional on the data from the ﬁrst n patients is
L(a; y) =
n
Y
i=1
p(di, a)yi{1 −p(di, a)}1−yi .
The TITE-CRM replaces p(di, a) by g(di, a) ≡wip(di, a). The factor wi,
0 ≤wi ≤1, is a weight for the i-th patient. For example, we might let
wi = Ui/T for a horizon T and time to toxicity Ui (truncated by T) for
patient i. Except for replacing p by g, the design proceeds as in the basic
CRM.
The use of g(·) is justiﬁed as an approximation. Assume that a binary
toxicity event is deﬁned as toxicity by some (large) time T. Then P(Ui ≤
t) = P(Ui ≤T | di, a) P(Ui ≤t | Ui ≤T, di, a) = p(di, a) P(Ui ≤t | Ui ≤
T, di, a). Approximating the last factor by wi justiﬁes the use of g(·) in
the likelihood. Cheung and Chappell (2000) show that the recommended
dose under the TITE-CRM converges to the correct dose (under certain
conditions).
Software note: The TITE-CRM is implemented in the R package titecrm
(http://cran.r-project.org/web/packages/titecrm/index.html).
A basic implementation in R is shown in this chapter’s software page,
www.biostat.umn.edu/~brad/software/BCLM_ch3.html
The following steps summarize the proposed TITE-CRM of Cheung and

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
107
Chappell (2000). For this and all other algorithms in this section we give
the algorithm to simulate a single trial realization. To compute the next
dose in an actual trial we would start with Step 2 and drop the loop over
patient cohorts. To compute operating characteristics, we would instead
embed the algorithm in an additional outer loop over repeated artiﬁcial
datasets.
Algorithm 3.4 (TITE-CRM).
Step 0. Initialization: Fix an assumed truth, po
j = Pr(toxicity at dose j).
Set the initial starting dose d = 1 and initialize calendar time (months)
t = 0 and sample size n = 0.
Step 1. Initial dose escalation: Repeat Step 1.1 until the ﬁrst toxicity
response is observed or the maximum sample size is reached. Set batch
size k = 3.
Step 1.1. Simulate next cohort: Simulate Ui, i = n + 1, . . . , n + k.
Record the recruitment times t0i = t, and thus calendar event time
t0i + Ui.
Step 1.2. Stopping the initial escalation: If Ui ≤T is observed or
d = 6 is reached, stop escalation. Otherwise, increment d to d + 1, n
to n + k, t to t + 6, and repeat from Step 1.1.
At the end of the initial escalation change the cohort size to k = 1.
Step 2. Posterior update: Compute a = E(a | y1, d1, . . . , yn, dn) based
on the pseudo likelihood
p(yn | a) =
n
Y
i=1
g(di, a)yi{1 −g(di, a)}1−yi.
The (univariate) integral is easily carried out as a summation over a
grid (approximating the integral by a Riemann sum). Let (a1, . . . , aM)
denote an equispaced grid covering the range of possible a values; we
used M = 50, a1 = 0.01 and aM = 7.0.
Log likelihood and prior: Let wti = min {1, (t −t0i)/T}, and let yti =
I{(Ui < T) ∩(t > t0i + Ui)} denote the current weight and outcome
indicator for the ith observation at time t.
For m = 1, . . . , M, evaluate ℓ(am) ≡log p(yn | a = am), given by
X
i:yi=1
log[wnip(di; a = am)] +
X
i:yi=0
log[1 −wnip(di; a = am)] ,
and log p(am) = −am.
Posterior: Evaluate the pointwise posterior
p(am | yn) ∝exp(ℓ(am) + log p(am))
and compute a = P
m amp(am | yn)/ P
m p(am | yn).

108
PHASE I STUDIES
0
5
10
15
20
25
30
1.0
1.5
2.0
2.5
3.0
MONTH
ALLOC
*
0
5
10
15
20
25
30
0.0
0.5
1.0
1.5
2.0
MONTH
BETA
*
(a) Allocation d⋆by t
(b) Estimate a by t
Figure 3.12 TITE CRM: The left panel plots the allocated dose against calendar
time. The initial d = 1 is chosen by deﬁnition, to start the initial dose escalation.
The right panel shows the estimated parameter a against calendar time t. The
star shows the ﬁnal reported posterior estimate.
Step 3. Next dose: Evaluate the estimated toxicity probabilities ˆp(d, a) =
da, and select the dose d⋆= arg min |ˆp(d, a)−p⋆| with estimated toxicity
closest to the desired level p⋆. The next cohort is assigned to dose d⋆.
Step 4. Next cohort: Simulate Un+1 corresponding to k = 1 new patient
allocated at d⋆. Record the recruitment times t0i = t (and thus calendar
event time t0i +Ui). Increment n ≡n+1 and advance the calendar time
t = t + 0.5.
Step 5. Stopping: if n ≥nmax, stop and report posterior estimated tox-
icity probabilities (computed as in Step 2); else repeat from Step 2.
Example 3.5 (Simulation study). We implemented the proposed method
for the simulation study reported in Cheung and Chappell (2000, Section
5). We used an assumed simulation truth po = (0.05, 0.1, 0.2, 0.3, 0.5, 0.7)
for a dose grid d ∈{0.05, 0.1, 0.2, 0.3, 0.5, 0.7}; i.e., the assumed toxicity
probabilities follow the CRM model with a = 1. The target toxicity is
p⋆= 20%.
Figure 3.12 summarizes a simulated trial history. We use the CRM power
model, p(d, a) = da (recall that the doses are scaled between 0 and 1).
For the ﬁrst two cohorts, at t = 0 and 6 months, the dose assignment is
determined by the initial dose escalation of Step 1. After t = 6 the ﬁrst
toxicity was observed and the dose assignment switches to the allocation
described in Step 3. Note how the cohort size switches from k = 3 to k = 1
after the initial escalation. Panel (b) plots the posterior means a computed
in Step 2. The plot starts only after the ﬁrst toxicity is observed and the

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
109
algorithm leaves the initial dose escalation loop of step 1. The see-saw
pattern of the posterior mean trajectory is typical. Each time a toxicity is
observed the posterior mean drops, and then raises again slowly while no
toxicities occur. The ﬁnal star indicates the posterior mean a = E(a | y)
upon conclusion of the trial in Step 5. In this case the simulation truth
was ao = 1.0. The fact that the posterior estimate so closely matches the
simulation truth is coincidental. In general, with only 25 binary responses
the posterior mean could still be a biased estimate, far from asymptotic
posterior consistency.
Bekele et al. (2008) propose an extension of the TITE-CRM. The method
is based on a discretization of the time to event. The discretized variable
yi is an ordinal outcome. These authors assume a probit regression for the
conditional probabilities P(yi = j | yi ≥j, di). The basic model does not
include monotonicity constraints, which can be awkward to impose within
an MCMC algorithm. Instead, monotonicity across doses is enforced by
post-processing of the posterior estimated probabilities of toxicity using an
isotonic regression. The adjusted posterior probabilities are then used to
deﬁne rules for dose escalation, de-escalation, suspension of accrual, and
early stopping for excessive toxicity.
Braun et al. (2005) go a step further and assume a sampling model for
the time to event. The probability model for time to toxicity is deﬁned by
piecewise linear hazards. Based on posterior probabilities under this model,
the proposed trial conduct is then again analogous to the CRM design.
Alternative methods based on fully parametric models for the event times
are proposed in Cheung et al. (2006) and Thall et al. (2005).
3.2.4 Toxicity intervals
As already seen, many phase I designs require the speciﬁcation of a target
level of toxicity. The implicit assumption that such a single target toxicity
exists and can be reliably identiﬁed by the investigator is probably unreal-
istic. It is more likely that the investigator might have a range of acceptable
toxicity levels in mind. This idea is formalized by several approaches that
are based on toxicity probability intervals, rather than target levels.
Ji, Li, and Bekele (2007) deﬁne a target toxicity interval relative to the
current precision of posterior estimated toxicities. Let si denote the poste-
rior standard deviation of the mean toxicity at dose di and let p⋆denote
a nominal target toxicity level. These authors deﬁne a toxicity interval as
p⋆plus or minus si. For each dose di let ni, xi and pi denote the number
of patients enrolled at dose di, the number of observed toxicities, and the
unknown probability of toxicity at that dose. Posterior probabilities are
deﬁned with respect to a binomial sampling model xi ∼Bin(ni, pi), and
independent beta priors for pi.
Assume that the current patient cohort is assigned to dose di. After

110
PHASE I STUDIES
adding each patient cohort, one computes the posterior probabilities of
pi being below, within, and above the target interval. Let Y denote the
currently available data, and let d = di denote the currently assigned dose.
We deﬁne
qD
=
Pr(pi > p⋆+ K1si | Y )
qE
=
Pr(pi < p⋆−K2si | Y )
(3.1)
and qS
=
Pr(p⋆−K2si < pi < p⋆+ K1si | Y ) = 1 −(qD + qE) ,
where K1 and K2 are user-selected design constants (in our code below,
we chose K1 = 1 and K2 = 1.5). Depending on which of these probabili-
ties is largest, the design recommends dose escalation (when qE is largest),
remaining with the current dose (qS), or dose de-escalation (qD), respec-
tively. The design does not skip doses, and stops early for excessive toxicity
if the lowest dose is found to be excessively toxic. At the end of the trial
the maximum tolerated dose is reported as the dose with estimated pi clos-
est to p⋆. The estimate of pi that is used for this decision is an isotonic
regression of posterior mean toxicities E(pi | data) (Ji, Li, and Yin, 2007).
Software note: An implementation as a spreadsheet application is available at
http://odin.mdacc.tmc.edu/∼yuanj/software.htm. An implementation in
R, used in the example below, is listed in this chapter’s software page,
www.biostat.umn.edu/~brad/software/BCLM_ch3.html
The following outline shows a step-by-step implementation of the Ji,
Li, and Bekele (2007) method. We illustrate the simulation of a possible
trial realization. First, let d = 1, . . . , J denote the set of doses. We assume
independent priors pi ∼Beta(a0, b0), i = 1, . . . , J. Let y(n) denote all data
up to and including the n-th patient.
Algorithm 3.5 (Toxicity Intervals).
Step 0. Initialization: Fix an assumed scenario, i.e., a simulation truth
po
i for the true toxicity probabilities pi = Pr(toxicity at dose i). Fix
cohort size k = 3 and sample size n = 0. Start with dose d = 1.
Step 1. Next cohort: Record responses y ∼Bin(k, po
d). Increment n ≡
n + k.
Step 2. Posterior updating: The posterior distributions are given by
p(pi | y(n)) = Be(a0+xi, b0+ni−xi). Evaluate the posterior probabilities
qD, qE and qS as in (3.2).
Step 3. Utilities: We deﬁne utilities for the actions a ∈{E, D, S}, where
E, D, and S indicate escalation, de-escalation, and remaining at the
current dose, respectively, as follows:
• When d = 1, then u(D) = 0.
• When d = J then u(E) = 0.

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
111
• We ﬁx a threshold ξ for unacceptably high probability of toxicity. If
p(pd+1 > p⋆| Y ) > ξ, then u(E) = 0.
• Subject to these constraints u(a) = qa, a ∈{E, D, S}. In other words,
the probabilities qa are used to deﬁne utilities.
Step 4. Next dose: The next dose is deﬁned by maximizing the utilities
u(a)
d ≡



d −1
if u(D) = maxa u(a)
d + 1
if u(E) = maxa u(a)
d
otherwise
Step 5. Stopping: If n ≥nmax we stop for maximum sample size. If
p(p1 > p⋆| Y ) > ξ we stop early for excessive toxicity. Otherwise repeat
starting from Step 1. Early stopping for excessive toxicity is atypical;
the trial is expected to run to nmax.
Step 6. Recommended dose: Let pi = E(pi | Y ) denote the estimated
toxicity probabilities. Let ˜pi denote an isotonic regression of the pi. The
ﬁnal recommended dose is
arg min
i
|˜pi −p⋆| .
The isotonic regression in step 6 above is implemented via iterative pool-
ing of adjacent violators (Robertson et al., 1988). “Adjacent violators” are
two consecutive doses whose responses violate monotonicity. We use the
following simple ad hoc implementation of this algorithm:
Algorithm 3.6 (Pooling adjacent violators). We work with a set of indices
c = (c1, . . . , cJ) that indicate pooling of adjacent values. Any doses with
matching indices cj are pooled.
Step 0. Initialization: Let ˜p = p and c = (1, 2, . . . , J).
Step 1. Find adjacent violators: Let V = {i :
˜pi > ˜pi+1, i < J} de-
note the set of adjacent violators.
Step 2. Stopping the iteration: If V = ∅then stop the iteration. Oth-
erwise select the ﬁrst violator v = cV1. Let W = {i :
ci = v or ci =
v + 1}. Let mW = mean(˜pW ) denote the average value over W.
Step 3. Pool adjacent violators: Set ci ≡v, i ∈W and replace ˜pi ≡
mw, i ∈W. Repeat from Step 1.
As always, to carry out an actual trial, one would replace the simulation
in Step 1 with a simple recording of the actual response.
Example 3.6 (Dose escalation). We implement the proposed algorithm
for an example used in Ji, Li, and Bekele (2007). The example is based on

112
PHASE I STUDIES
2
4
6
8
10
0.0
0.1
0.2
0.3
0.4
0.5
0.6
WEEK
E(TOX | dose,  Y)
DOSE
1
2
3
2
4
6
8
10
1.0
1.5
2.0
2.5
3.0
WEEK
DOSE
D*
(a) pi
(b) Assigned doses d
Figure 3.13 Toxicity intervals: The left panel plots posterior estimated toxicity
probabilities pi for doses i = 1, 2, 3 against cohort. The right panel shows the
assigned dose (vertical axis) by cohort (horizontal axis).
a trial described in Goodman, Zahurak, and Piantadosi (1995). There are
eight allowable dose levels, d ∈{1, . . . , 8}. For the simulation study we as-
sume a scenario with toxicity probabilities po
i = 5, 25, 50, 60, 70, 80, 90, and
95%. We use p⋆= 20% and nmax = 100, and assume Beta(0.005, 0.005)
priors for all eight toxicity probabilities pi. This prior is chosen to reﬂect
little prior information, and essentially corresponds to equal masses of 0.5
at both 0 and 1.
Figure 3.13 summarizes the simulation output. Panel (a) plots the esti-
mated toxicity probabilities for the three lowest doses i = 1, 2, 3 against
week. Initially, at t = 0 all three toxicity probabilities are centered at
E(pi) = 0.5. The plot starts at t = 1. The ﬁrst cohort is assigned to dose
d = 1 (step 0) and we observe x1 = 0 toxicities. This shifts the posterior
distribution p(p1 | y(1)) down to E(p1 | y(1)) ≈0. The Be(0.005, 0.005)
prior provides only negligible shrinkage toward the prior mean 0.5. For the
next cohort the dose is escalated to d = 2, and again x2 = 0 toxicities are
observed. This shifts the posterior mean for p2 to approximately 0. In the
third cohort we continue dose escalation to d = 3, but now observe toxic-
ities. This leads to a dose de-escalation. For the rest of the trial we never
leave d = 2 anymore. The implication is that the posterior distributions for
p1 and p3 remain unchanged for the rest of the trial. Note, however, that
the ﬁgures only show one trial simulation. For the evaluation of operat-
ing characteristics we would carry out massive repeat simulations. Design
evaluation is then based on the average of many such simulations.
The design proposed by Ji, Li, and Bekele (2007) introduces the toxicity

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
113
intervals based on the uncertainty in estimating mean toxicities. In other
words, the use of intervals is motivated by the lack of precise estimates.
3.2.5 Ordinal toxicity intervals
Neuenschwander et al. (2008) go a step further and acknowledge that it is
impossible to deﬁne a single precise target toxicity p⋆. They extend the tra-
ditional binary classiﬁcation into acceptable versus unacceptable toxicity
to an ordinal scale over four sub-intervals of toxicity probabilities. Let pi
denote the posterior probability of a dose limiting toxicity (DLT) at dose
di. Neuenschwander et al. (2008) partition the range of toxicity probabilties
into pi ∈(0, 0.20] (“under-dosing”), pi ∈(0.20, 0.35] (“targeted toxicity”),
pi ∈(0.35, 0.60] (“excessive toxicity”), and pi ∈(0.60, 1.00] (“unacceptable
toxicity”). On the basis of this classiﬁcation, the authors propose to pro-
ceed with a pragmatic design that prescribes de-escalation, escalation, and
continued enrollment at the current dose depending on these four proba-
bilities. Alternatively, they note that one could deﬁne a loss function as
a linear combination of the four interval probabilities and proceed with a
decision theoretic design.
Software note: The paper includes WinBUGS code to evaluate posterior prob-
abilities and posterior expected losses under a logistic regression model with
normal priors (Neuenschwander et al., 2008, Appendix II). A basic implemen-
tation in R is given in the book’s website.
The following steps implement the design proposed in Neuenschwander et
al. (2008). We describe one simulation of a possible trial history. To compute
the next dose in an actual trial realization, start at Step 1, dropping the
loop over patient cohorts. To compute operating characteristics one would
embed the algorithm in an additional outer loop over repeat simulations.
We assume a dose grid d = (d1, . . . , d7) with do ≡d7 as a reference dose
(used in the algorithm below). Letting Φ(z) denote the standard normal
c.d.f. as usual, the algorithm assumes a probit sampling model
p(yi = 1 | di = d) = π(d) with π(d) = 1 −Φ [−log a −b log(d/do)] . (3.2)
This model allows an easy interpretation of the parameters, with a being
approximately the prior odds at the reference dose do, and b being a shift
in log odds for doses away from do. The interpretation would be exact for a
logistic regression model, as used in Neuenschwander et al. (2008). We use
the probit model instead, in order to use the built-in posterior simulation
in the R package Bayesm. The model is completed with a bivariate normal
prior, namely (log a, b) ∼N(µ, Σ).
Algorithm 3.7 (Ordinal Toxicity Intervals).
Step 0. Initialization: Fix an assumed scenario, i.e., a simulation truth
po
i for the true toxicity probabilities pi = Pr(toxicity at dose i). Fix
cohort size k = 3 and sample size n = 0. Start with dose d = 1.

114
PHASE I STUDIES
Step 1. Next cohort: Record responses y ∼Bin(k, po
d). Increment n ≡
n + k.
Step 2. Posterior updating: Let θ = (log a, b). The posterior distribu-
tion is given by p(θ | y) ∝p(θ) · p(y | θ), with the bivariate nor-
mal prior and the probit regression likelihood (3.2). Use the R pack-
age bayesm (Rossi et al., 2005) to generate a posterior MCMC sample
Θ ≡{θm; m = 1, . . . , M} where θm ∼p(θ | y).
Use the posterior MCMC sample Θ to evaluate posterior probabilities
of under, target, excess and unacceptable toxicity at each dose. Let
Pi(under) denote the probability of under-dosing at dose d = di. Let
θm = (log am, bm), and evaluate
Pi(under | y) = 1
M
M
X
m=1
I {lo ≤1 −Φ [−log(am) −bm log(di/do)] ≤hi} ,
where lo = 0 and hi = 0.20. Repeat similarly for Pi(target | y), Pi(excess |
y) and Pi(unacc | y), each using appropriate boundaries (lo, hi).
Step 3. Next dose: Let D = {i : Pi(excess) + Pi(unacc) < 0.25}. The
next dose is d⋆= di⋆with
i⋆= arg max
D {i : Pi(target)}
and i⋆= ∅if no dose is found with probability of excessive or unaccept-
able toxicity < 0.25.
If i⋆̸= ∅then set d = d⋆≡di⋆and continue with step 1.
Step 4. Stopping: If n ≥nmax or if no dose satisﬁes the constraint, i.e.,
i⋆= ∅, then stop and report the last assigned dose d = d⋆as optimal
dose. Otherwise continue with Step 1.
Example 3.7 (Phase I trial). We implemented the algorithm for an ex-
ample reported in Neuenschwander et al. (2008). They consider a phase I
dose escalation study to characterize safety and tolerability of a drug and
to determine the maximum tolerable dose (MTD).
We assume a dose grid d = (12.5, 25, 50, 100, 150, 200, 250) with do = 250
as a reference dose. Neuenschwander et al. (2008) use a bivariate normal
prior (log a, log b) ∼N(µ, Σ). The covariance matrix Σ is determined by
Var(log a) = 0.842, Var(log b) = 0.82 and correlation Corr(log a, log b) =
0.2. The mean is µ = (2.15, 0.52). The moments are chosen to best match
the 2.5%, 50%, and 97.5% quantiles for the toxicity probabilities that are
implied by a one-parameter CRM model (see Neuenschwander et al., 2008,
for details). We ﬁnd the implied prior moments m = E(θ) and S = Var(θ)
for θ = (log a, b) and use a bivariate normal prior p(θ) = N(m, S). The
maximum sample size was set to nmax = 30.

MODEL-BASED DESIGNS FOR DETERMINING THE MTD
115
1
2
3
4
5
6
7
0.0
0.2
0.4
0.6
0.8
1.0
DOSE
PBAR
P0
PBAR[m]
Figure 3.14 Ordinal toxicity intervals: assumed truth po (dashed line) and poste-
rior estimated toxicities E(pi | y) after each of 10 cohorts (dotted lines) and after
cohort 11 (solid line).
2
4
6
8
10
1.0
1.5
2.0
2.5
3.0
WEEK
DOSE
D*
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
WEEK
E(TOX | dose,  Y)
P0
1
2
3
4
5
6
7
(a) di
(b) E(pi | y) across weeks
Figure 3.15 Ordinal toxicity intervals: assigned doses di (left panel) and estimated
toxicity probabilities pi = E(pi | y) against weeks (panel b). For comparison, the
ﬁnal point on each curve (Week 11, labeled “P0”) shows the simulation truth.
Figure 3.14 shows the assumed simulation assumed truth
po = (0.11, 0.27, 0.52, 0.76, 0.84, 0.90, 0.94).
Under the assumed truth po, dose d2 is the maximum dose with true toxicity
within the target toxicity interval [0.20, 0.35]. The ﬁgure shows po, together
with the posterior mean toxicity probabilities pi = E(pi | y) after each of
10 patient cohorts.
Figure 3.15 summarizes one trial simulation. The initial dose assignment
at is ﬁxed at d = 1 by the algorithm. The simulated toxicity responses for

116
PHASE I STUDIES
the ﬁrst patient cohort were (0, 0, 1), shifting the posterior distribution on
(pi, i = 1, . . . , 7) to low estimated toxicities. However, notice how the prior
prevents inference from over-reacting and pushing the recommended dose
too high. The corresponding trajectory of posterior estimates is shown in
panel (b). For each week t (cohort), the ﬁgure shows the posterior means
E(pi | y) conditional on data up to and including week t. The plot starts at
week t = 1, thus prior means are not shown. Starting with week 6 posterior
estimates settle down to recommend d⋆= 1, until eventually switching to
the correct d⋆= 2 after week 11.
3.3 Eﬃcacy versus toxicity
In recent years, there has been increasing interest and eﬀort in developing
dose ﬁnding methods incorporating both toxicity and eﬃcacy endpoints
(e.g., Zohar and Chevret, 2007). After all, drug doses are acceptable only if
they are safe and eﬃcacious. One such method, EﬀTox developed by Thall
and Cook (2004) and later extended by Thall, Cook, and Estey (2006)
takes the approach of eﬃcacy-toxicity trade-oﬀs. As mentioned before, it
is commonly accepted that the drug’s toxicity increases with dose. Eﬃcacy
also increases with dose in general. However, for some biological agents it
is possible that eﬃcacy may plateau, or increase and then decrease, as the
dose is increased. Our goal is then to ﬁnd the best dose which provides
the highest eﬃcacy and lowest toxicity, and to treat most patients at that
dose. Since it is rare to ﬁnd drugs that are both eﬀective and nontoxic, it is
typically necessary to make a trade-oﬀbetween eﬃcacy and toxicity. This
type of study can be considered as a “phase I-II” trial, since it combines
the goals of conventional phase I and II studies. Later, in Subsection 4.3.2
we will provide a purely phase II joint eﬃcacy-toxicity approach, where
a single dose is chosen for further evaluation, and where requirements on
false positive and false negative rates are more stringent.
EﬀTox is a Bayesian outcome-based adaptive method featuring four key
statistical tasks:
• choose the trial parameters, including the deﬁnition of binary eﬃcacy
and toxicity outcomes, dose levels, cohort size, and maximum sample
size,
• specify the joint probability model for eﬃcacy and toxicity and prior
distributions of model parameters,
• deﬁne the acceptable doses based on the eﬃcacy and toxicity criteria,
and
• elicit and deﬁne the parameters for eﬃcacy-toxicity trade-oﬀs.
Patients are treated in cohorts. After observing the eﬃcacy and toxicity
outcomes, the posterior mean of the joint eﬃcacy-toxicity distribution for

EFFICACY VERSUS TOXICITY
117
each dose is computed. Then, the most desirable dose level based on the
eﬃcacy-toxicity trade-oﬀis identiﬁed to treat the next cohort of patients.
We now provide details for each of the four tasks above.
3.3.1 Trial parameters
Binary eﬃcacy and toxicity outcomes should be deﬁned according to the
context of disease. For example, major response (deﬁned as complete or
partial response) or disease control (major response or stable disease) are
useful eﬃcacy endpoints, while dose-limiting toxicity can be used for the
toxicity endpoint. Typically, there are only a few (e.g., 3 to 6) dose levels
to evaluate, and the cohort size is taken as 3 or larger. Too small a cohort
size is not recommended due to the instability of parameter estimates,
and potentially longer trial duration due to suspension of study enrollment
before the outcomes of the current cohort are observed. A large cohort size
is also not recommended because it limits adaptation and learning from
the trial’s interim results. The maximum sample size can be from 30 to
100, depending on number of doses, the accrual rate, and the precision of
the estimates at the end of the trial.
3.3.2 Joint probability model for eﬃcacy and toxicity
Thall and Cook (2004) speciﬁed the following joint model. Let Y = (YE, YT )
be the binary indicators of eﬃcacy (E) and toxicity (T). The bivariate
probabilities for a, b ∈{0,1} at dose x,
πa,b(x, θ) = Pr(YE = a, YT = b | x, θ) ,
are formulated in terms of the marginal toxicity probability πT (x, θ), ef-
ﬁcacy probability πE(x, θ), and an association parameter ψ. Speciﬁcally,
πT (x, θ) = π0,1(x, θ) + π1,1(x, θ) = logit−1{ηT(x, θ)} where ηT (x, θ) =
µT + xβT and πE(x, θ) = π1,0(x, θ) + π1,1(x, θ) = logit−1{ηE(x, θ)} where
ηE(x, θ) = µE + xβE,1 + x2βE,2. Note that the toxicity is assumed to be
monotone in x, but a more general form of eﬃcacy ηE(x, θ) is assumed to
allow quadratic non-monotonicity in x. Therefore, the full parameter vector
is θ = (µT , βT , µE, βE,1, βE,2, ψ)′. The bivariate distribution for eﬃcacy
and toxicity (suppressing x and θ in the notation) is
πa,b = πa
E(1−πE)1−aπb
T (1−πT )1−b+(−1)a+bπE(1−πE)πT (1−πT )eψ −1
eψ + 1 .
Consequently, the likelihood for a patient treated at dose x, L(Y, x|θ), is
π1,1(x, θ)YEYT π1,0(x, θ)YE(1−YT )π0,1(x, θ)(1−YE)YT π0,0(x, θ)(1−YE)(1−YT ) .
Given dose-outcome data Dn = {(Y1, x 1), ..., (Yn, xn)} from the ﬁrst n
patients in the trial, the full likelihood is Ln(Dn|θ) = Qn
i=1 Li(Yi, xi|θ).

118
PHASE I STUDIES
An appropriate prior distribution can then be chosen to reﬂect the physi-
cian’s overall assessment and uncertainty before the trial starts. It must be
suﬃciently uninformative such that the cumulative data will dominate in
calculating the posteriors for decisionmaking. Thall and Cook (2004) rec-
ommended the use of normal distributions for each of the six components
in θ.
3.3.3 Deﬁning the acceptable dose levels
A dose is deemed acceptable if it meets both minimum eﬃcacy and maxi-
mum toxicity requirements. Let πE be the lower limit of desirable eﬃcacy,
and ¯πT be an upper limit of the tolerable toxicity, both elicited from the
physician. For example, in treating a certain cancer, a lower limit of 20%
on eﬃcacy and an upper limit 50% on toxicity might be desired. Given the
current data Dn, a dose x is considered acceptable if
Pr{πE(x, θ) > πE|Dn} > pE ,
and
Pr{πT(x, θ) < ¯πT|Dn} > pT ,
where pE and pT are ﬁxed design parameters, typically chosen between
0.05 and 0.20. These are considered the “lower bars” or “gatekeepers” for
meeting the minimum eﬃcacy and maximum toxicity criteria. As pE in-
creases, the method is more likely to exclude doses due to lack of eﬃcacy.
As pT increases, the method is more likely to exclude doses due to excessive
toxicity. These two parameters can be “tuned” via simulations to include
clinically viable doses as acceptable doses. Based on the posterior proba-
bility, patients can be assigned only to the acceptable doses, denoted as
A(Dn).
3.3.4 Eﬃcacy-toxicity trade-oﬀcontours
Any point (πE(x, θ), πT (x, θ)) treated with dose x lies in a two-dimensional
space π = (πE, πT ) that spans [0, 1]2. Our goal is to deﬁne an eﬃcacy-
toxicity trade-oﬀcontour C such that all the points on C are equally
desirable. One thought is to use the Euclidean distance from the point
of interest to the most desirable point, π = (1, 0). Doing this, however,
puts equal weight on eﬃcacy and toxicity, which may not reﬂect real-life
clinical desires. A more sensible and general approach is to use the dis-
tance based on the LP norm. To ﬁnd the contour C, one needs to elicit
from the physician three equally desirable design points {π∗
1, π∗
2, π∗
3}, where
π∗
1 = (π∗
E, 0), π∗
2 = (1, π∗
T ), and π∗
3 = (π′
E, π′
T). π∗
E deﬁnes the smallest re-
sponse rate that the physician would consider acceptable if the treatment
has no toxicity. π∗
T reﬂects the highest toxicity level that the physician

EFFICACY VERSUS TOXICITY
119
Prob(Efficacy)
Prob(Toxicity)
 1 
 0.5 
 0 
−0.5 
 −1 
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
 1 
 0.5 
 0 
−0.5 
 −1 
D1
D2
D3
D4
Figure 3.16 Contour plot of the desirability measure by probabilities of toxicity
and eﬃcacy. The three design points are shown as circles, while the four doses
are shown as triangles. The shaded area has desirability measure greater than 0,
and is thus the “acceptable” dose region.
is willing to accept if the treatment is 100% eﬃcacious. The intermedi-
ate point π∗
3 depicts an eﬃcacy-toxicity trade-oﬀthat is more realistic but
equally desirable as the previous two extreme settings. The desirability
measure for point (πE, πT ) is deﬁned as δ = 1 −r, with r obtained from
µ1 −πE
1 −π∗
E
¶p
+
µπT
π∗
T
¶p
= rp .
Before calculating δ or r, we must ﬁrst ﬁnd p. We do this by plugging
(π′
E, π′
T ) in for (πE, πT ) and setting r = 1 in the above equation. Af-
ter solving for p, the desirability measure can be computed for any point
(πE, πT ) by re-solving the equation for r, hence δ. A larger desirability
measure indicates a more favorable eﬃcacy-toxicity proﬁle. Note that the
contours are concave, straight-line, and convex for p less than, equal to,
and greater than 1, respectively. R code is available (see this chapter’s soft-
ware page) to compute p and plot the contour lines given the three design
points.
Figure 3.16 shows the contour plot for the design points π∗
1 = (0.2, 0),
π∗
2 = (1, 0.5), and π∗
3 = (0.4, 0.3) on the probability of toxicity versus
probability of eﬃcacy plane. The resulting contour passing through the

120
PHASE I STUDIES
design points (shown as solid dots) is shown in a thick curve with δ = 0
or r = 1. Points “inside” this contour line (i.e., toward the lower right,
the shaded area) have positive δ and are more desirable. Conversely, points
“outside” this contour line (i.e., to the upper left) have negative δ and are
less desirable.
The algorithm for trial conduct is as follows:
Algorithm 3.8 (EﬀTox design)
Step 1: Treat the ﬁrst cohort of patients at the starting dose speciﬁed by
the physician. Typically, the ﬁrst (lowest) dose level is chosen.
Step 2: For subsequent cohorts, no untried dose may be skipped, either
when escalating or de-escalating.
Step 3: Observe the eﬃcacy and toxicity outcomes, then, compute the
posterior probability and posterior means, E{π(xi, θ)|Dn} for each dose
xi given the cumulative data Dn.
Step 4: Determine the acceptable dose(s) A(Dn).
Step 5: Compute the desirability measure δ for each dose xi in A(Dn).
Step 6a: If there are no acceptable doses, the trial is terminated and no
doses are selected for further evaluation.
Step 6b: Otherwise, there is at least one acceptable dose. Treat the next
cohort of patients at the dose with maximum desirability. Return to Step
2 until the maximum sample size is reached.
Step 7: When the trial reaches the maximum sample size and there is at
least one acceptable dose, select the dose with maximum desirability for
further evaluation.
Software note: The EffTox program can be used for both trial design (via sim-
ulations) and trial conduct, and can be downloaded from the M.D. Anderson
software page, biostatistics.mdanderson.org/SoftwareDownload/.
Example 3.8 We illustrate the design properties in a simulation study.
Suppose that we are interested in ﬁnding the best dose of a new agent.
There are four doses to be studied with the true probabities π1 = (0.10, 0.05),
π2 = (0.30, 0.15), π3 = (0.60, 0.20), and π4 = (0.70, 0.50). The four doses
are shown as solid triangles labeled as D1 through D4 in Figure 3.16. As-
sume the maximum sample size is 60 with a cohort size of 3; the starting
dose is dose level 1. The results of 1000 simulation studies are summarized
in Table 3.3. The ﬁrst three lines in the table list the true probabilities of
outcomes for characterizing the underlying joint probability model at each
dose. The desirability measures for dose 1, 2, 3, and 4 are –0.133, 0.055,
0.333, and –0.094, respectively. Based on the assumption, dose 3 has the
highest desirability measure and is the best dose among the four. The re-
sults show that 56.7% of the patients are treated at dose 3. At the end of

COMBINATION THERAPY
121
Dose
1
2
3
4
true Pr(eﬃcacy)
0.10
0.30
0.60
0.70
true Pr(toxicity)
0.05
0.15
0.20
0.50
true Pr(eﬃcacy w/o toxicity)
0.05
0.10
0.20
0.30
desirability measure
–0.133
0.055
0.333
–0.094
ave # patients treated
3.4
12.0
34.0
10.6
(% of patients treated)
(5.7%)
(20.0%)
(56.7%)
(17.7%)
selection probability
0.001
0.133
0.756
0.110
Table 3.3 Operating characteristics of the EﬀTox design with four doses, maxi-
mum sample size of 60, and cohort size of 3 based on 1000 simulations.
the trial, about 76% of the time, dose 3 will be chosen as the best dose.
For the two adjacent doses, 12% and 10.6% of the patients are treated at
doses 2 and 4 with selection probabilities of 0.133 and 0.110, respectively.
The results illustrate that the EﬀTox method performs well in this setting.
3.4 Combination therapy
Up until now in this chapter, we have addressed dose-ﬁnding for a single
drug. But increasingly, clinicians wish to investigate the therapeutic eﬀect
of multiple drugs used in combination, either sequentially or concurrently.
In many ﬁelds, two agents (say, A and B) may be more eﬀective than either
one used alone due to synergistic eﬀects. In such cases, clinicians may know
the MTDs for each drug used separately, but now need to discover the dose
combination (Aj, Bk) having probability of dose-limiting toxicity (DLT) no
larger than some prespeciﬁed limit π∗∈(0, 1).
Although this ﬁeld of research is fairly recent, several authors have al-
ready tackled this problem. For example, Thall et al. (2003) proposed a six-
parameter model for the toxicity probabilities arising from the various dose
combinations. The approach is reminiscent of the CRM of Section 3.2.1, in
that the authors specify two-parameter logistic models relating dose and
toxicity for each of the two agents separately, and then add two more pa-
rameters to control the correlation in the (now-bivariate) dose-response
space (note it will typically be inappropriate to assume the two agents op-
erate independently). Thall et al. (2003) also point out that, rather than
there being a single MTD as in the univariate case, we will now have a
contour of MTD values in two-dimensional dose space, all of which will

122
PHASE I STUDIES
Drug A
pα
j
1 −pα
j
Drug B
qβ
k
π(11)
jk
π(01)
jk
1 −qβ
k
π(10)
jk
π(00)
jk
Table 3.4 Joint (π) and marginal (p, q) probabilities of toxicity, latent contingency
table approach to combination therapy problem.
have the desired target toxicity π∗. Thus, in their dose-ﬁnding algorithm,
the authors recommend slowly increasing the dose in a diagonal direction
by increasing the dose of both agents, and then identifying two additional
dose combinations “oﬀthe diagonal” by randomly venturing out in opposite
directions along the current estimate of the toxicity equivalence contour.
While dose can often be thought of as continuous, in practice clinicians
typically prefer to establish a ﬁnite number of doses of each drug, say J for
Drug A and K for Drug B. This then determines a ﬁnite number (JK) of
dose combinations, perhaps none of which will have toxicity exactly equal
to π∗. As such, one often takes the combination with toxicity closest to π∗
as the MTD.
3.4.1 Basic Gumbel model
In this section, we follow the basic setup of Yin and Yuan (2009a), who let
Aj be the jth dose for drug A, A1 < · · · < AJ, and Bk be the kth dose
for Drug B, B1 < · · · < BK. For a patient treated with dose combination
(Aj, Bk), these authors assume Xjk = 1 if the patient experiences toxicity
from Drug A, with Xjk = 0 otherwise, and Yjk = 1 if the patient expe-
riences toxicity from Drug B, with Yjk = 0 otherwise. Next, suppose we
have physician-speciﬁed marginal probabilities of toxicity pj and qk asso-
ciated with doses Aj and Bk, respectively. These may be available from
previous studies of the two drugs separately. At the very least, pJ and qK
will be available since the highest doses AJ and BK will typically be equal
to the marginal MTDs, after which the pj and qk for the remaining, lower
doses can be guessed. To allow for uncertainty in this assessment, remi-
niscent of the CRM we incorporate two unknown, positive parameters α
and β, so that the marginal toxicity probabilities when (Aj, Bk) is given in
combination are pα
j and qβ
k .
Assuming the pα
j and qβ
k are strictly increasing in j and k, respectively,
Table 3.4 gives the probability model for the resulting 2 × 2 table. That is,
the table gives π(xy)
jk
for x = 0, 1, y = 0, 1, the joint probabilities associated

COMBINATION THERAPY
123
with the bivariate binary outcomes. To model the π(xy)
jk
as functions of the
marginal probabilities, similar to the approach of the previous section, Yin
and Yuan (2009a) suggest a Gumbel model,
π(xy)
jk
= pαx
j (1−pα
j )1−xqβy
k (1−qβ
k )1−y+(−1)x+ypα
j (1−pα
j )qβ
k (1−qβ
k )eγ −1
eγ + 1 ,
(3.3)
for dose jk where Xjk = x and Yjk = y. Notice that setting the association
parameter γ = 0 produces the case where the drugs produce toxicities
independently.
As usual, we require a likelihood and a prior to carry out the Bayesian
analysis. For the former, suppose that of the njk patients treated at com-
bination dose jk, we observe n(00)
jk
experiencing no toxicities, n(10)
jk
experi-
encing toxicities only from Drug A, n(01)
jk
experiencing toxicities only from
Drug B, and n(11)
jk
experiencing toxicities from both agents. Then the like-
lihood is a simple multinomial,
L(α, β, γ|Data) ∝
J
Y
j=1
K
Y
k=1
[π(00)
jk ]n(00)
jk [π(01)
jk ]n(01)
jk [π(10)
jk ]n(10)
jk [π(11)
jk ]n(11)
jk
.
(3.4)
Note that this likelihood assumes that data correponding to all four cells in
Table 3.4 are observable. This is actually not unreasonable in some cancer
studies, where two chemotherapeutic agents having nonoverlapping dose-
limiting toxicities (DLTs) can sometimes be identiﬁed, perhaps with the
help of chemoprotective agents that prevent patients from experiencing any
toxicities common to both agents. In such cases, any remaining toxicities
can be immediately identiﬁed as being the result of either one drug or the
other. However, in most cases, toxicities from the two drugs will be at least
partially overlapping, precluding the matching of toxicities to drugs. In this
setting, Yin and Yuan (2009a) proceed simply by assuming the contingency
table in Table 3.4 is latent, with the observed data corresponding to the
lower right cell (no toxicity) and the sum of the other three cells (toxicity
arising from one or both drugs). This alters likelihood (3.4) to
L(α, β, γ|Data) ∝
J
Y
j=1
K
Y
k=1
[π(00)
jk ]n(00)
jk [1 −π(00)
jk ]njk−n(00)
jk
.
In a subsequent, closely related paper, Yin and Yuan (2009b) replace this
latent 2×2 table approach with a direct speciﬁcation of πjk, now deﬁned as
the joint probability of any toxicity arising from dose jk. They continue to
assume the availability of the marginal guesses pj and qk, but now construct
the joint from the marginals using copula models (Shih and Louis, 1995;
Nelsen, 1999). For instance, using what they term the “Clayton copula”

124
PHASE I STUDIES
Drug A
Drug B
p1
α
p2
α
p3
α
p4
α
q1
β
q2
β
q3
β
q4
β
Figure 3.17 Sample dose escalation/de-escalation scheme, combination therapy
trial with 4 × 4 levels. From the current dose (A2, B2), only changes to dose
combinations indicated by the arrows are permitted.
enables replacing (3.3) with
πjk = 1 −
h
(1 −pα
j )−γ + (1 −qβ
k )−γ −1
i−1/γ
,
while the “Gumbel-Hougaard copula” instead produces
πjk = 1 −exp
³
−{[−log(1 −pα
j )]1/γ + [−log(1 −qβ
k )]1/γ}γ´
.
Returning to the 2 × 2 table model setting, Yin and Yuan (2009a) rec-
ommend a vague Gamma(0.1, 0.1) prior for γ, and moderately informative,
independent Unif(0.2, 2) priors for α and β. We note that the former in-
sists on γ > 0, which in our Gumbel model means positive association (syn-
ergy between the two drugs), while the latter is roughly centered around
α = β = 1, i.e., fully accurate initial prediction of the marginal toxicity
probabilities pj and qk.
MCMC can be easily used to obtain the posterior for the parameter
vector (α, β, γ), from which posterior distributions for the π(xy)
jk
can be
obtained from (3.3), which in turn determine the progression of our dose-
ﬁnding algorithm. Suppose ce and cd are predetermined probability thresh-
olds for dose escalation and de-escalation, respectively. We might choose
ce and cd so that the trial has acceptable operating characteristics, subject
to the constraint that ce + cd > 1. Following common practice by treating
small groups of patients in cohorts (say, of size 3, thus mimicking the classic

COMBINATION THERAPY
125
3+3 design), Yin and Yuan (2009a,b) recommend restricting to one-level
dose changes, and also not allowing moves along the diagonal (i.e., where
the doses of both drugs are escalated or de-escalated simultaneously). That
is, the only dose changes permitted are of the sort indicated by the arrows
in Figure 3.17, which illustrates the case of a trial having J = K = 4 dose
levels for each drug, and where the current dose is (A2, B2).
The basic dose-ﬁnding algorithm is as follows:
Algorithm 3.9 (Yin-Yuan combination dose-ﬁnding design)
Step 1: Treat patients in the ﬁrst cohort at the lowest dose combination,
(A1, B1).
Step 2: If for the current dose (jk) we have
P(πjk < π∗|Data) > ce ,
then escalate to an adjacent dose combination whose probability of toxi-
city is higher than that of the current dose and as close as possible to the
target π∗. If the current dose is already the highest possible, (AJ, BK),
do not change the dose.
Step 3: If for the current dose (jk) we have
P(πjk > π∗|Data) > cd ,
then de-escalate to an adjacent dose combination whose probability of
toxicity is lower than that of the current dose and as close as possible to
π∗. If the current dose is already the lowest possible, (A1, B1), the trial
is terminated.
Step 4: Otherwise, treat the next cohort at the current dose combination,
(Aj, Bk).
Step 5: Once the maximum sample size has been reached, take the dose
combination having probability of toxicity closest to π∗as the MTD.
Note that, as with any dose-ﬁnding algorithm, there is a certain amount
of “ad hockery” here, and apparently sensible and subtle changes to this
algorithm can have marked impacts on how the design performs. However,
and also as seen before, suﬃcient tinkering with any design after repeated
simulation of operating characteristics should enable sensible choice of de-
sign parameters, such as ce and cd. In particular, Yin and Yuan (2009a,b)
compare their designs to a “restricted CRM” method that ﬁrst ﬁxes Drug B
at each given dose and then searches over the doses of Drug A, essentially
reducing the bivariate search problem to a series of univariate searches.
As one might expect, the true combination methods emerge with superior
overall Type I error and power performance.

126
PHASE I STUDIES
3.4.2 Bivariate CRM
It is tempting to refer to the approach just described as “bivariate CRM,”
since it employs a CRM-type Bayesian algorithm, but with two drugs in-
stead of one. However, in the literature that moniker has become associated
with the related problem of dose-ﬁnding with two competing outcomes ob-
served from various doses of a single agent. The ﬁrst reference appears
to be Braun (2002), who considered the case of competing 0-1 outcomes
for toxicity, Yi, and disease progression, Zi. Conditional on the dose of a
(single) drug xj, the bivariate CRM method speciﬁes probability models
p1j = h1(xj, β1) = P(toxicity seen at dose j), and
p2j = h2(xj, β2) = P(progression seen at dose j)
(3.5)
where h1 and h2 are monotonic, and possibly parameterized so as to ensure
a low probability of toxicity and high probability of disease progression at
the lowest dose. The choice recommended and used by Braun (2002) is
log
µ
p1j
1 −p1j
¶
= −3 + β1xj , and log
µ
p2j
1 −p2j
¶
= 3 −β2xj ,
which is computationally convenient, and assumes additivity of the dose
eﬀects on the log-odds scale.
To specify the bivariate distribution of Y and Z, Braun (2002) assumes
that conditional on dose x, this distribution can be constructed using a
copula approach as
f(y, z|x) = k(p1, p2, ψ)py
1q1−y
1
pz
2(1 −p2)1−zψyz(1 −ψ)1−yz
for y, z ∈{0, 1} and 0 < ψ < 1, where p1 is the probability of toxicity at
dose x, p2 is the probability of progression, qi = 1 −pi, ψ is an association
parameter, and k is a normalizing constant. Since ψ/(1−ψ) is the odds ratio
between Y and Z, we have that Y and Z are independent if ψ = 1/2, are
positively associated if ψ > 1/2, and are negatively associated if ψ < 1/2.
For a prior on the parameter vector θ = (β1, β2, ψ)′, Braun (2002) rec-
ommends
p(θ) = 6ψ(1 −ψ)e−(β1+β2) ,
i.e., independent Exponential(1) priors on the βi, and an independent
Beta(2, 2), a minimally informative prior having mean 1/2, on ψ. We then
do the usual prior-to-posterior updating for θ, and in turn obtain the pos-
teriors for the toxicity and progression probabilities p1j and p2j.
Regarding the dosing algorithm, Braun suggests randomizing patients
in cohorts of size c (typically 3), obtaining the posterior means E[p1j|y, z]
and E[p2j|y, z], and then choosing the next dose by minimizing
v
u
u
t
2
X
ℓ=1
(E[pℓj|y, z] −p∗
ℓ)2 ,

COMBINATION THERAPY
127
the Euclidean distance between our current estimates and some desired
rates of toxicity and progression p∗
1 and p∗
2, respectively. A weighted version
of this metric may be useful when we wish to place more emphasis on
toxicity or progression; other, non-Euclidean metrics may also be sensible.
Software note: A program called bCRM to implement the bivariate continual re-
assessment method can be downloaded from biostatistics.mdanderson.org/
SoftwareDownload/SingleSoftware.aspx?Software Id=15.
3.4.3 Combination therapy with bivariate response
The obvious question at this point is how to merge the two major ideas of
this section, in order to handle bivariate responses (say, toxicity and disease
progression) with more than one therapy. That is, returning to the case of
a combination dose (Aj, Bk) indexed as jk, in the notation of Braun (2002)
used in (3.5), we would now need to specify
p1jk = h1(Aj, Bk, α1, β1) = P(toxicity seen at dose jk) , and
p2jk = h2(Aj, Bk, α2, β2) = P(progression seen at dose jk)
(3.6)
where h1 and h2 are again parametric functions, possibly monotonic in both
dose levels. The building blocks for such an approach (copula modeling,
dose-ﬁnding algorithm, and so on) are found in our work so far; here we
brieﬂy outline a few published references in this rapidly emerging area.
Mandrekar et al. (2007) give an adaptive phase I design for dual-agent
dose ﬁnding where both toxicity and eﬃcacy are considered as responses.
However, the authors do not use the full bivariate response setting indicated
in (3.6), but rather a “TriCRM” approach (Zhang et al., 2006; Fan and
Chaloner, 2004). Here, a continuation ratio (CR) model is utilized to turn
the bivariate toxicity-eﬃcacy response into a univariate combined endpoint
with three mutually exclusive and exhaustive outcomes: “no response” (no
eﬃcacy and acceptable toxicity), “success” (eﬃcacy and acceptable toxic-
ity), and “toxicity” (unacceptable toxicity, regardless of eﬃcacy outcome).
Letting ψ0(x, θ), ψ1(x, θ), and ψ2(x, θ) denote the probabilities of these
three outcomes, respectively, the CR model for the single-agent TriCRM
design is given by
log(ψ1/ψ0) = α1 + β1x and logit(ψ2) = α2 + β2x ,
(3.7)
where x is the dose and θ = (α1, α2, β1, β2) and β1, β2 > 0. The dual-
agent generalization of Mandrekar et al. (2007) employs a dual dose vector
x = (x1, x2), and replaces (3.7) with
log(ψ1/ψ0) = α1 + β1x1 + β3x2 and logit(ψ2) = α2 + β2x1 + β4x2 ,
where now we have θ = (α1, α2, β1, β2, β3, β4) and βi > 0 for i = 1, . . . , 4.

128
PHASE I STUDIES
Adding the constraint that the three probabilities must sum to 1, we obtain
ψ2(x, θ)
=
eα2+β2x1+β4x2
1 + eα2+β2x1+β4x2 ,
ψ1(x, θ)
=
eα1+β1x1+β3x2
(1 + eα1+β1x1+β3x2)(1 + eα2+β2x1+β4x2) ,
and
ψ0(x, θ)
=
1
(1 + eα1+β1x1+β3x2)(1 + eα2+β2x1+β4x2) .
These three probabilities readily determine the likelihood,
L(θ; x, y) ∝
n
Y
i=1
ψ0(xi, θ)y0iψ1(xi, θ)y1iψ2(xi, θ)y2i ,
where xi is the dose assigned to the ith cohort and yi = (y0i, y1i, y2i)′ is the
trinomial outcome arising from this cohort. Mandrekar et al. (2007) spec-
ify ﬂat priors (albeit over a bounded version of the parameter space), and
then estimate all parameters using a quasi-Bayesian (psuedo-likelihood)
CRM approach. The authors use a dose-ﬁnding algorithm similar to Algo-
rithm 3.9, and investigate their design’s performance across a broad range
of true eﬃcacy-toxicity scenarios, including one where the eﬃcacy of one
of the two drugs is not monotone in dose, as assumed by the model.
Huang et al. (2007) oﬀer a design for a combination therapy trial in the
presence of two binary responses (in their case, eﬃcacy and toxicity) that
maintains the full complexity of model (3.6). This design also incorporates
ideas from seamless (but two-stage) phase I-II designs, beginning with a
dose escalation phase that uses a modiﬁed 3+3 design to choose admissible
joint dose levels (Aj, Bk). In the second stage, patients are randomized
adaptively to the various admissible doses with probabilities proportional
to the current posterior probability that each dose is the best; see equation
(4.5) in Section 4.4.
The design also compares the beneﬁt of giving the two drugs sequentially
versus concurrently, adding yet another level of complication. However, the
design is not particularly sophisticated in terms of the modeling of the two
response probabilities. For instance, letting p2jk denote the probability of
eﬃcacy (complete remission, in the authors’ example) for a patient assigned
dose jk, a simple logistic link function h2 is selected for use with (3.6), i.e.,
logit(p2jk) =
½
γs + α2Aj + β2Bk
if therapies assigned sequentially
γc + α2Aj + β2Bk
if therapies assigned concurrently
.
That is, the probability of response is assumed to be additive on the logit
scale, even though some sort of interaction between the two drugs (as was
modeled above using the copula idea) is likely present. The toxicity proba-
bilities, p1jk, are left completely unmodeled, except for the assumption that
they are i.i.d. draws from a Beta(0.1, 0.9) distribution a priori. The design

COMBINATION THERAPY
129
permits stopping for toxicity, futility, or eﬃcacy; again we defer further
details until Sections 4.3 and 4.4. Like other authors in this area, Huang et
al. (2007) use simulations to investigate their design’s performance across
a range of eﬃcacy-toxicity scenarios.
Finally, full-blown decision theoretic approaches can be used. Houede et
al. (2010) choose the optimal dose pair of a chemotherapeutic agent and
a biologic agent in a phase I/II trial measuring both toxicity and eﬃcacy,
where ordinal (rather than merely binary) outcomes are permitted. Joint
response probabilities are again obtained via copula modeling, with the
marginal outcome probabilities arising from an extension of a model due
to Aranda-Ordaz (1983) that permits response probabilities that are not
monotone in dose. A particularly novel aspect of this design is that each
patient’s dose pair (Aj, Bk) is chosen adaptively from a two-dimensional
grid by maximizing the posterior expected utility of the patient’s outcome.
These utilities for each pair are elicited from a panel of physicians using
the Delphi method, in the manner of Brook et al. (1986). Because experts
are often overconﬁdent in their opinions, even when using a community of
experts, elicitation of priors and utilities should be approached with great
care; see e.g. the recent text of O’Hagan et al. (2006) for some of the key
issues involved. If elicited correctly, the utilities should free us from having
to “back out” any aspect of the design from a consideration of its operating
characteristics, but the authors still recommend simulations to check those
characteristics, an eminently sensible safety feature. See also Section 4.6
for more on decision-theoretic methods.
3.4.4 Dose escalation with two agents
Thall et al. (2003) propose an alternative model-based approach for two-
agent dose ﬁnding. In spirit the method is similar to the Yin-Yuan combi-
nation dose ﬁnding described in Section 3.4.1. But instead of the Gumbel
model, here we use a 6-parameter bivariate logistic model, and the re-
striction to adjacent doses is relaxed. Our main motivation to discuss this
approach here is the availability of public domain software; see the software
notes below.
Consider a phase I oncology trial for the combination of two cytotoxic
agents. We assume that each of the two agents has been studied before in
single agent trials and that the goal of the new trial is to establish a safe
dose combination for the two agents. Let (d1, d2) denote the doses of the
two cytotoxic agents. Let π⋆denote the target toxicity level, and let d⋆
k,
k = 1, 2, denote known single-agent acceptable doses, i.e., doses with mean
toxicity equal to π⋆. In the following discussion we use standardized doses,
xk = dk/d⋆
k, k = 1, 2. We do so to avoid scaling problems. Consider a dose
combination x = (x1, x2) and let π(x, θ) denote the unknown probability
of toxicity at x. Here θ is a parameter vector that indexes the probability

130
PHASE I STUDIES
model. We assume
π(x, θ) =
a1xb1
1 + a2xb2
2 + a3
³
xb1
1 xb2
2
´b3
1 + a1xb1
1 + a2xb2
2 + a3
³
xb1
1 xb2
2
´b3 .
(3.8)
The model is indexed by θ = (a1, b1, a2, b2, a3, b3). The model is chosen
to allow easy incorporation of information about single-agent toxicities.
For x2 = 0 the model reduces to the single agent dose-toxicity curve
π((x1, 0), θ) ≡π1(x1, θ) and similarly for π2. The parameters (a3, b3) char-
acterize the two-agent interactions.
Let Yn = (xi, yi; i = 1, . . . , n) denote observed indicators for toxicity
yi for n patients treated at dose combinations xi = (xi1, xi2), i = 1, . . . , n.
The outcome is a binary indicator yi with yi = 1 if patient i experienced
a dose-limiting toxicity and yi = 0 otherwise. When the sample size n is
understood from the context we will use Y, x = (x1, . . . , xn) and y =
(y1, . . . , yn) as short for all data, the vectors of all dose assignments and
all responses, respectively. The likelihood function is evaluated as
p(y | θ, x) =
Y
i: yi=1
π(xi, θ)
Y
i: yi=0
(1 −π(xi, θ)).
Thall et al. (2003) assume independent gamma priors for the parameters,
aj ∼Ga(α1j, α2j) and bj ∼Ga(β1j, β2j) ,
for j = 1, 2. Speciﬁcally, they adopt informative priors for (aj, bj), j = 1, 2,
corresponding to the single-agent dose-toxicity curves. The hyperparam-
eters (α1j, α2j, β1j, β2j), j = 1, 2 are chosen to match the known single-
agent toxicity curves as closely as possible. For the interaction parame-
ters we assume independent log normal priors, log a3 ∼N(µa3, σa3) and
log b3 ∼N(µb3, σb3). As default choices we propose to use µa3 = µb3 = 0.25
and σ2
a3 = σ2
b3 = 3. Note that Thall et al. (2003) also use gamma priors for
(a3, b3), but we found it numerically more stable to work with the normal
priors for (log a3, log b3) instead.
The proposed algorithm proceeds in two stages. First the dose com-
binations are increased along a pre-determined linear grid D1 to quickly
approximate the desired target toxicity π⋆. This pre-determined linear grid
is in the bivariate dose space. By default D1 is deﬁned on a 45 degree line
of equal proportions for the two agents. The dose escalation is subject to
overdose control. Think of this ﬁrst stage as a fast climb straight up the
expected toxicity surface. In the second stage, we modify the combination
of the two doses to explore alternative combinations of the two agents that
achieve similar toxicity probabilities. Think of the second stage as moving
horizontally along a curve of equal mean toxicity, exploring to either side of
the ﬁnal point on the ﬁrst stage climb. All moves are based on the currently

COMBINATION THERAPY
131
estimated posterior expected toxicity surface,
πn(x) = E[π(x, θ) | Yn] =
Z
π(x, θ) dp(θ | Yn) .
Let L2(π⋆, Yn) = {x :
πn(x) = π⋆} denote the equal (posterior mean)
toxicity contour conditional on the current data Yn.
Let Lleft
2
= {x ∈L2 and x2 > x1} denote the segment of L2 above the
45 degree line (x1 = x2; when D1 is not deﬁned on the 45 degree line,
change Lleft
2
accordingly). Similarly Lright
2
is the part of L2 below the 45
degree line. The move in the second stage is restricted on L2. In alternating
cohorts we use Lleft
2
and Lright
2
. This constraint to alternate moves to either
side avoids the algorithm getting trapped. Let Lside
s
denote the set for the
current cohort. To assign the doses in stage 2, we simply randomly select
one of the doses in Lside
2
with equal probability.
Algorithm 3.10 (Two-agent dose-ﬁnding).
Step 0. Initialization: Deﬁne a grid D1 = {x(r), r = 1, . . . , R} in the
bivariate dose space. By default D1 is on the 45 degree line.
Initialize the cohort size K = 3. Initialize the sample size, n = 0, and
cohort index, i = 1. Fix the treatment dose for the ﬁrst cohort as x1 =
x(d0) for some pre-determined d0.
Fix an assumed true toxicity surface by assuming a parameter vector θ0
for model (3.8).
Step 1. Stage 1 (initial dose escalation): Escalate on the grid D1, sub-
ject to overdose control.
Step 1.1. Next cohort: Treat cohort i at dose xi. Determine the prob-
ability of toxicity under the assumed simulation truth, p = π(xi, θ0).
Simulate yi ∼Bin(K, p) and record the number of observed responses
yi. Increment n ≡n + K.
Step 1.2. Reﬁning the dose grid: When the ﬁrst toxicity is observed,
i.e. yi > 0, we reﬁne the grid by adding half steps 1
2(x(r) + x(r+1)),
r = 1, . . . , R −1.
Step 1.3. Posterior updating: Run an MCMC simulation to gener-
ate a posterior Monte Carlo sample Θ with θ ∼p(θ | Y) for all θ ∈Θ,
approximately. Using Θ update πn(x) ≡P
Θ π(x, θ) for all x ∈D1.
Increment n = n + K.
Step 1.4. Next dose: Find the dose combination x(r) ∈D1 minimz-
ing |πn −π⋆|, subject to the condition of not skipping any untried
dose in D1 when escalating. Set xi+1 = x(r).
Step 1.5. Stopping: If n < n1, increment i = i + 1 and repeat with
Step 1.1. Otherwise continue with step 2.
Step 2. Stage 2 (explore at equal toxicity level): Let L ≡Lside
2
de-
note the current branch of L2.

132
PHASE I STUDIES
Step 2.1. Next dose (uniform assignment): Select xi+1 ∼Unif(L).
Step 2.2. Next cohort: same as Step 1.1.
Step 2.3. Posterior updating: same as Step 1.3.
Step 2.4. Stopping: If n < n1 + n2, increment i ≡i + 1 and repeat
with step 2.1. Otherwise continue with Step 3.
Step 3. Final recommendation: Let x⋆
ℓdenote the optimal dose pair in
L = Lleft
2
, computed as in Step 2. Similarly, let x⋆
r denote the optimal
dose combination in L = Lright
2
, and let x⋆
m denote the optimal dose pair
on L = D1.
Report {x⋆
ℓ, x⋆
m, x⋆
r} as three alternative MTD dose pairs.
The MCMC simulation in Steps 1.3 and 2.2. is implemented as a straight-
forward Metropolis-Hastings algorithm with random walk proposals. In our
implementation we used M = 100 parallel chains, and 100 iterations each
time. The chains were initialized using a normal approximation of the pos-
terior for η = log θ.
Software note: The ToxFinder program oﬀers an implementation of the ideas in
this subsection, and can be downloaded from biostatistics.mdanderson.org/
SoftwareDownload/SingleSoftware.aspx?Software Id=14. In addition, R code
for a basic implementation of Algorithm 3.10 is included in the online supple-
ment to this chapter,
www.biostat.umn.edu/~brad/software/BCLM_ch3.html.
Example 3.9 (Gemcitibine and Cyclophosphamide (CTX) Trial). We im-
plemented our algorithm for the study that motivated the discussion in
Thall et al. (2003) and which therein is described as an example applica-
tion. The study considers combination chemotherapy with gemcitibine and
CTX. The goal of the study is to identify three acceptable dose pairs that
can be carried forward in a following phase II trial. We follow Thall et al.
(2003), using the same prior means and variances for the ﬁrst four param-
eters, namely E(a1, b1, a2, b2) = (0.4286, 7.6494, 0.4286, 7.8019) with cor-
responding marginal variances (0.1054, 5.7145, 0.0791, 3.9933). These mo-
ments were carefully elicited by Thall et al. (2003). We then found param-
eters for the scaled gamma priors to match these marginal moments. For
the interaction parameters we assumed means E(a3, b3) = (0.25, 0.25), and
marginal variances Var(a3) = Var(b3) = 3. We then assume independent
log normal priors for a3 and b3 to match these moments.
The proposed two-agent dose escalation was implemented using maxi-
mum sample sizes n1 = 20 and n2 = 40 for stages 1 and 2 and a target
toxicity level of π⋆= 0.30.
Figures 3.18 through 3.20 summarize simulated trial histories under two
hypothetical simulation truths. Scenario S2 assumes weak interaction and

COMBINATION THERAPY
133
(a) Dose allocations and π(x)
(b) Simulation truth π(x)
Figure 3.18 Scenario S2: dose allocations, estimated toxicity surface (conditional
on all data) (panel a) and simulation truth (panel b). Empty circles show cohorts
with yi = 0, crossed circles show yi = 1, small bullets show yi = 2 and large
bullets show yi = 3. The (large) stars show the ﬁnally reported three MTD dose
pairs. The thick curve in the left panel shows the estimated π⋆toxicity curve.
(a) Dose allocations and π(x)
(b) Simulation truth π(x)
Figure 3.19 Scenario S4: same as Figure 3.18 for scenario S4.
moderate toxicity. The second scenario, S4, assumes strong interaction and
high toxicity. Figure 3.18 shows the allocated dose combinations as well as
the estimated toxicity under S2, while Figure 3.19 shows the same under
S4. In both ﬁgures we see how the ﬁrst dose allocations quickly walk up the
steep toxicity surface. At the end of Stage 1, the algorithm starts to explore
alternative dose combinations oﬀthe diagonal that still have the desired
target toxicity probability. Note that the dose allocations are always based

134
PHASE I STUDIES
(a) S2
(b) S4
Figure 3.20 Estimated toxicity probabilities π(xi) (line) and true toxicity prob-
abilities π(xi) (bullets) plotted against cohort i. The last three points show the
three reported MTD pairs, together with 95% credible intervals for the true toxi-
city π(x⋆).
on currently estimated toxicities. Changes in the posterior estimates lead
to some scatter in the assigned dose pairs. The ﬁgures also show the esti-
mated toxicity surface at the end of the simulated trial and for comparison
the assumed simulation truth. The thick lines in the left panels show the
estimted sets L2 at the last step. In both cases, the curves closely track
the estimated π⋆= 30% contours for E(π | Y), except for approximation
errors due to the discrete nature of the grid that is used to represent L2.
Figure 3.20 shows the estimated and true toxicity of the sequence of dose
allocations for the same two simulations. We can clearly see the bias in the
posterior estimated toxicities that is introduced by the prior model. When
the prior assumes higher (lower) toxicities than the simulation truth, the
posterior estimates show corresponding positive (negative) biases.
3.5 Appendix: R Macros
The online supplement to this chapter
www.biostat.umn.edu/~brad/software/BCLM_ch3.html
provides the R code that was used to illustrate the examples in this chap-
ter. In many cases, the R macros are written to simulate one realization
of a hypothetical trial using the proposed design. The main function in
these examples is named sim.trial(.). To compute operating character-
istics one would add an additional loop that repeatedly calls sim.trial.
To monitor an ongoing trial one would have have to (i) replace the simu-
lated data with the actually observed responses, and (ii) strip the top-level

APPENDIX: R MACROS
135
loop inside sim.trial and use only one iteration. The CRMinteractive,
CRMexplore, and phaseIsim programs supporting the work in Section 3.2
are provided on our website in zip ﬁles, containing full input, code, and
“readme” ﬁles.

CHAPTER 4
Phase II studies
In this chapter we address the design and statistical considerations for the
development of “middle phase” clinical trials, especially those associated
with phase II cancer trials. Here the focus shifts from toxicity to eﬃcacy,
and trials are run on much larger groups of patients (say, 40 to 200). We
also delve more deeply into the subject of adaptive designs, a subject of
greatly increasing interest and utility in middle-phase studies.
After obtaining preliminary information about the safety proﬁle, dose,
and administration schedule of a drug in early (phase I) development, the
next issue is to examine whether a drug has suﬃcient eﬃcacy to warrant
further development. Phase II studies can be further divided into two parts.
The initial assessment of the drug’s eﬃcacy is the primary goal for phase
IIA trials. Typically, phase IIA trials are conducted as single-arm studies
to assess the eﬃcacy of new drugs, with the goal of screening out those
that are ineﬀective. Subsequently, phase IIB trials are multi-arm studies
to compare the eﬃcacy of the new drug versus the standard treatment or
other experimental drugs, so that the most promising one can be selected
for large scale evaluation in late phase studies. The toxicity proﬁle of the
new agents may also be further evaluated in phase II studies.
Phase II studies provide important intermediate steps for a successful
drug development. In today’s post-genomic and high-throughput era, the
number of new candidate agents is growing by leaps and bounds. Since late
phase studies are large, time consuming, and expensive, middle phase trials
play a critical role in eliminating the “chaﬀ” from our drug collection, so
that only the most promising treatments are funneled through to late phase
development, thus ensuring a higher overall success rate.
4.1 Standard designs
Traditionally, After the toxicity proﬁle and/or the MTD for a treatment
has been investigated, phase II studies are conducted at the MTD or an
“optimal biological dose” estimated from phase I to evaluate whether the
new agent has suﬃcient activity and to reﬁne knowledge of its toxicity pro-
ﬁle. The primary endpoint of a phase IIA trial is often a binary endpoint

138
PHASE II STUDIES
of response/no response or success/failure. For cancer trials, the clinical
response is deﬁned as complete response (no evidence of disease) or par-
tial response. Partial response is often deﬁned as a 50% or more tumor
volume shrinkage based on a two-dimensional measurement, or a 30% or
more decrease in the sum of the longest diameters of target lesions based
on the one-dimensional RECIST criteria in solid tumors (Therasse et al.,
2000, 2006; Eisenhauer et al., 2009). As for phase IIB trials, time-to-event
endpoints such as disease-free survival or progression-free survival are of-
ten chosen as primary endpoints. Comprehensive overviews on the design
and analysis of phase II cancer trials can be found, for example, in papers
by Mariani and Marubini (1996), Scher and Heller (2002), and Gray et al.
(2006), and Seymour et al. (2010).
4.1.1 Phase IIA designs
To provide an initial eﬃcacy assessment, a phase IIA trial is often designed
as a single-arm, open-label study that requires treating 40 to 100 patients in
a multistage setting. Multi-stage designs are useful here for early stopping
due to lack of eﬃcacy should the interim data indicate that the study drug
is ineﬃcacious. In cancer trials, Gehan (1961) proposed the ﬁrst two-stage
design. In the early days of cancer drug development, there were few drugs
that had anticancer activity; a drug was considered active if it produced
a tumor response rate p of 20% or higher. To test the hypothesis of H0:
p = 0 versus H1: p = 0.2, Gehan’s design calls for enrolling 14 patients
in the ﬁrst stage. If none of them respond to the treatment, the drug is
considered ineﬀective and the trial is stopped. If at least one tumor response
is observed, additional patients (typically 20–40) are enrolled in the second
stage such that the response rate can be estimated with a prespeciﬁed
precision. This design has a Type I error rate of zero (because under the
null hypothesis of p = 0, no response can occur) and 95% power when
p = 0.2. The design can also be used to test other alternative response
rates under H1. For example, for p = 0.1 or 0.15, the corresponding sample
size in the ﬁrst stage to achieve 95% power will be 29 and 19, respectively.
The initial sample size n can be easily calculated by ﬁnding the smallest
n such that the speciﬁed power is greater than or equal to 1 −(1 −p)n.
The second stage for the sample size can be obtained by ﬁnding the total
sample size N such that the standard error of the estimated response rate,
p
p(1 −p)/N, is smaller than a certain precision. Here the true parameter
p can be estimated by taking, for example, the upper end of a one-sided 75
percent conﬁdence interval, or simply a conservative estimate of p = 0.5.
As treatments improve over time, the null response rate corresponding
to the response rate of the standard treatment is no longer zero. When the
null response is greater than 0, two-stage designs can be constructed to
test the hypothesis H0: p ≤p0 versus H1: p ≥p1. A primary motivation

STANDARD DESIGNS
139
for a two-stage design is that if the treatment does not work in the ﬁrst
stage, the trial can be stopped early, so that patients are not subjected to
a potentially toxic yet ineﬀective treatment. In addition, resources saved
can be devoted to developing other agents.
Among many two-stage designs which control Type I and Type II error
rates at α and β, the Simon (1989) optimal design was constructed to
minimize the expected sample size under the null hypothesis. Alternatively,
a minimax design can be constructed that minimizes the maximum trial
sample size. As in statistical decision theory, minimax designs are rather
conservative in that they focus all their attention on the worst-case scenario.
For example, when p0 = 0.1 and p1 = 0.3, with α = β = 0.1, the optimal
two-stage design needs to enroll 12 patients in the ﬁrst stage. If no response
or only one response is found, the trial is stopped and the drug is considered
ineﬀective. Otherwise, 23 more patients are enrolled to reach a total of 35
patients. At the end of the trial, if only ﬁve or fewer responses are observed,
the agent is deemed ineﬀective. Otherwise (i.e., with 6 or more responses
in 35 patients), the agent is considered eﬀective. Under the null hypothesis,
there is a 66% chance that the trial will be stopped early. The expected
sample size under H0 is then
12 + (35 −12)(1 −.66) = 19.8 ,
the ﬁrst-stage sample size plus the second-stage size times one minus the
probability of early stopping.
In comparison, the minimax design enrolls 16 patients in the ﬁrst stage.
If no response or only one response is seen, the trial is stopped early and
the drug is considered ineﬀective. Otherwise, 9 more patients are enrolled
in the second stage to reach a total of 25 patients. At the end of the trial,
the agent is considered ineﬀective if four or fewer responses are seen and
eﬀective otherwise. The expected sample size is 20.4 and the probability of
early stopping is 0.51 under the null hypothesis. In both designs, the trial
can be stopped early because of lack of eﬃcacy (i.e., futility), to save pa-
tients from receiving ineﬀective treatments and to save time and resources
for developing ineﬀective treatments. If the treatment works well, there is
little reason to stop the trial early in a phase II setting. More patients can
beneﬁt from the treatment while the trial continues. Larger sample sizes
also increase the precision in estimating the response rate.
A multi-stage design with early stopping for futility rules is desirable in
phase II settings. Other multi-stage designs can be found in the literature.
For example, Fleming (1982) proposed a two-stage design that allows for
early stopping due to futility or eﬃcacy. Bryant and Day (1995) developed
a two-stage design that allows the investigator to monitor eﬃcacy and
toxicity simultaneously. Three-stage designs were proposed by Ensign et
al. (1994) and Chen (1997). Three-stage designs improve the eﬃciency of
two-stage designs, but are more complicated to implement and can increase

140
PHASE II STUDIES
the cost and length of the study. The gain in eﬃciency of designs with
more than three stages often does not justify the additional complexity in
conducting them. To derive the sample size and stopping boundaries for a
multi-stage design, Schultz et al. (1973) provided a useful recursive formula
for computing the tail probabilities to meet the constraints of Type I and
Type II error rates.
Software note: A stand-alone program that does Simon two-stage optimal de-
sign calculations can be downloaded from linus.nci.nih.gov/~brb/Opt.htm.
Given a maximum sample size and probabilities of response under the null and
alternative hypotheses, the program searches all two-stage designs to ﬁnd the
minimax and optimal designs that satisfy the speciﬁed constraints on Type I
and II error.
For those who prefer web-driven calculators (rather than stand-alone pro-
grams that need to be installed locally), computations for Simon’s two-stage
designs are available at linus.nci.nih.gov/brb/samplesize/otsd.html. Sim-
ilarly, calculations for Bryant and Day designs are available online at the web-
site www.upci.upmc.edu/bf/ClinicalStudyDesign/Phase2BryantDay.cfm.
Finally, among many other useful statistical tools, the site
www.crab.org/Statistools.asp, and the Duke Cancer Center site,
www.cancer.duke.edu/modules/CTDSystems54/index.php?id=3, provide tools
for calculating the performance of general two-stage designs.
4.1.2 Phase IIB designs
After passing the initial eﬃcacy assessment of a new agent in a phase IIA
study, the subsequent phase IIB trial is often a randomized, multi-arm
study with the goal of identifying the most promising treatment regimen
to send to large-scale phase III trials for deﬁnitive testing. In addition to
testing the response rate, time-to-event endpoints, such as time to disease
progression, are often used as the primary endpoint in phase IIB trials.
The challenge is to accurately select the most promising regimens among
a large number of potentially active regimens for further development.
Compared to phase III trials, phase IIB trials are by deﬁnition smaller
and less deﬁnitive. They tend to use earlier endpoints, such as disease-free
survival, rather than overall survival in order to shorten study duration.
They also often have larger Type I and Type II error rates than their
phase III counterparts. For example, the acceptable Type I error rate is
usually increased from 5% to 10% or even 20%. The maximum Type II
error rate still needs to be controlled in the 10 to 20% range. The rationale
for this is that, in phase II trials, it is more important to control the Type
II error (false-negative) rate than the Type I error (false-positive) rate. By
controlling the false-negative rate, active treatments are less likely to be
missed. A false-positive result is of less concern in phase II because the ﬁnal
verdict of the eﬀectiveness of a regimen can be provided in another phase II
study or a phase III evaluation. A moderate to large expected diﬀerence is

STANDARD DESIGNS
141
often assumed for phase II studies as well. Many of the randomized phase II
cancer studies apply randomization to achieve patient comparability, while
embedding a one-sample phase II design within each treatment arm (Lee
and Feng, 2005). Owing to limited sample sizes, such designs typically do
not yield suﬃcient statistical power for a head-to-head comparison between
the treatment arms, as is possible in phase III trials.
As an alternative method, Simon, Wittes, and Ellenberg (1985; hence-
forth abbreviated SWE) proposed a pick-the-winner design based on rank-
ing and selection methodology for binary endpoints. Unlike the ordinary
hypothesis testing framework which controls both Type I and Type II er-
rors, the ranking and selection procedure controls only Type II errors. Ba-
sically, the response rate of each treatment arm is estimated and the arm
with the highest response rate is picked as the “winner” for further eval-
uation. The design is appealing because the required sample size is much
smaller than that for a randomized trial under a hypothesis testing frame-
work. For example, N = 146 patients per arm are required for testing the
response rates of 10% versus 25% with 90% power and a two-sided 5%
Type I error rate. On the other hand, the SWE method requires only N =
21 patients per arm to achieve the same power. The trade-oﬀ, however, is
that the false-positive rate can range from 20 to over 40%, as reported in
simulation studies (e.g. Liu et al., 1999).
The SWE method works best when there is only one true “winner,”
with all other contenders ranking well behind in eﬃcacy. When there are
several comparable, active regimens, the method struggles to accurately
diﬀerentiate the best one from the other good ones. At the end of the
trial, this method always picks the treatment arm with the best observed
outcome as the winner, regardless of whether none of the regimens work,
some of them work, or all of them work. In addition, another drawback
of the SWE method is that it does not provide for early stopping due to
futility. Therefore, there is no provision for terminating a non-performing
arm early on the basis of interim results. Although the SWE method oﬀers
small sample sizes, its ranking and selection methodology does not appear
to mesh well with the objectives for phase IIB studies.
Thus, whether based on one-sample or multi-sample hypothesis testing or
ranking and selection methods, none of the frequentist methods are fully
satisfactory in providing good solutions for phase IIB designs. Although
smaller sample sizes are usually found in phase II trials, they should not
be thought of as “poor man’s phase III trials.” Nevertheless, for phase II
evaluation, more eﬃciency is required, and this is where Bayesian methods
can often oﬀer a superior approach.

142
PHASE II STUDIES
4.1.3 Limitations of traditional frequentist designs
Multi-stage designs achieve better statistical properties than single-stage
designs by utilizing information gained in the interim data. By examin-
ing the interim data, such designs allow for an earlier decision to stop the
trial if convincing evidence to support the null or alternative hypothesis is
found. The frequentist analysis of such designs, however, is constrained by
the rigid requirement of examining the outcome at the speciﬁed sample size
at each predetermined stage. The strict sample size guideline in each stage
is particularly diﬃcult to adhere to in multi-center trials due to the com-
plexity of coordinating patient accrual and follow-up across multiple sites.
Temporarily halting study accrual can also stall the trial’s momentum and
lower its investigators’ enthusiasm for the project. In addition, when actual
trial conduct deviates from the original design (e.g., investigators perform-
ing interim analyses at unplanned time points), stopping boundaries are
left undeﬁned, and the anticipated statistical properties no longer hold.
Many authors (Green and Dahlberg, 1992; Herndon, 1998; Chen and Ng,
1998) have recognized these problems and proposed solutions, but none
have been completely satisfactory. This lack of design ﬂexibility exposes
a fundamental limitation of all such frequentist-based methods, because
statistical inferences are made by computing the probability of observing
certain data conditioned on a particular design and sampling plan. When
there is a disparity between the proposed design and the actual trial con-
duct (more the norm than an exception in clinical trials), adjustments must
be made to all statistical inferences.
All of these reasons support the need for more ﬂexible designs. Bayesian
methods oﬀer a diﬀerent approach for designing and monitoring clinical tri-
als by permitting calculation of the posterior probability of various events
given the data. Based on the Likelihood Principle (Subsection 2.2.3), all
information pertinent to the parameters is contained in the data and is not
constrained by the design. Bayesian methods are particular appealing in
clinical trial design because they inherently allow for ﬂexibility in trial con-
duct and impart the ability to examine interim data, update the posterior
probability of parameters, and accordingly make relevant predictions and
sensible decisions.
4.2 Predictive probability
In Subsection 2.5.1 we introduced predictive probabilities. In this section we
describe their use in the design of phase II clinical trials (e.g., Lee and Liu,
2008). A distinct advantage of this approach is that it mimics the clinical
decisionmaking process. Based on the interim data, predictive probability
is obtained by calculating the probability of a positive conclusion (reject-
ing the null hypothesis) should the trial be conducted to the maximum

PREDICTIVE PROBABILITY
143
planned sample size. In this framework, the chance that the trial will show
a conclusive result at the end of the study, given the current information,
is evaluated. The decision to continue or to stop the trial can be made
according to the strength of this predictive probability.
4.2.1 Deﬁnition and basic calculations for binary data
For a phase IIA trial, suppose our goal is to evaluate the response rate p for
a new drug by testing the hypothesis H0: p ≤p0 versus H1: p ≥p1. Suppose
we assume that the prior distribution of the response rate, π(p), follows a
Beta(a0, b0) distribution. As described earlier, the quantity a0/(a0 + b0)
gives the prior mean, while the magnitude of a0 + b0 indicates how infor-
mative the prior is. Since the quantities a0 and b0 can be considered as the
numbers of eﬀective prior responses and non-responses, respectively, a0+b0
can be thought of as a measure of prior precision: the larger this sum, the
more informative the prior and the stronger the belief it contains.
Suppose we set a maximum number of accrued patients Nmax, and as-
sume that the number of responses X among the current n patients (n ≤
Nmax) follows a Binomial(n, p) distribution. By the conjugacy of the beta
prior and binomial likelihood, the posterior distribution of the response
rate follows another a beta distribution, p|x ∼Beta(a0 + x, b0 + n −x).
The predictive probability approach looks into the future based on the cur-
rent observed data to project whether a positive conclusion at the end of
study is likely or not, and then makes a sensible decision at the present
time accordingly.
Let Y be the number of responses in the potential m = Nmax −n future
patients. Suppose our design is to declare eﬃcacy if the posterior probabil-
ity of p exceeding some prespeciﬁed level p0 is greater than some threshold
θT . Marginalizing p out of the binomial likelihood, it is well known that Y
follows a beta-binomial distribution, Y ∼Beta-Binomial(m, a0 + x, b0 +
n −x). When Y = i, the posterior distribution of p|(X = x, Y = i) is
Beta(a0 +x+i, b0 +Nmax −x−i). The predictive probability (PP) of trial
success can then be calculated as follows. Letting Bi = Pr(p > p0 | x, Y = i)
and Ii = I(Bi > θT ), we have
PP
=
E{I[Pr(p > p0|x, Y ) > θT ] | x}
=
Z
I[Pr(p > p0|x, Y ) > θT ] dP(Y |x)
=
m
X
i=0
Pr(Y = i | x) × I(Pr(p > p0 | x, Y = i) > θT )
=
m
X
i=0
Pr(Y = i | x) × I(Bi > θT )

144
PHASE II STUDIES
=
m
X
i=0
Pr(Y = i | x) × Ii .
The quantity Bi is the probability that the response rate is larger than
p0 given x responses in n patients in the current data and i responses in m
future patients. Comparing Bi to a threshold value θT yields an indicator
Ii for considering the treatment eﬃcacious at the end of the trial given
the current data and the potential outcome of Y = i. Example 4.1 below
oﬀers a concrete illustration of the calculation of PP using the preceding
formulae.
The weighted sum of indicators Ii yields the predictive probability of
concluding a positive result by the end of the trial based on the cumulative
information in the current stage. A high PP means that the treatment
is likely to be eﬃcacious by the end of the study, given the current data,
whereas a low PP suggests that the treatment may not have suﬃcient
activity. Therefore, PP can be used to determine whether the trial should
be stopped early due to eﬃcacy/futility or continued because the current
data are not yet conclusive. We deﬁne a rule by introducing two thresholds
on PP. The decision rule can be constructed as follows:
Algorithm 4.1 (Phase IIA basic PP design).
Step 1: If PP < θL, stop the trial and reject the alternative hypothesis;
Step 2: If PP > θU, stop the trial and reject the null hypothesis;
Step 3: Otherwise continue to the next stage until reaching Nmax patients.
Typically, we choose θL as a small positive number and θU as a large
positive number, both between 0 and 1 (inclusive). PP < θL indicates that
it is unlikely the response rate will be larger than p0 at the end of the trial
given the current information. When this happens, we may as well stop
the trial and reject the alternative hypothesis at that point. On the other
hand, when PP > θU, the current data suggest that, if the same trend
continues, we will have a high probability of concluding that the treatment
is eﬃcacious at the end of the study. This result, then, provides evidence
to stop the trial early due to eﬃcacy. By choosing θL > 0 and θU < 1.0,
the trial can terminate early due to either futility or eﬃcacy. For phase
IIA trials, we often prefer to choose θL > 0 and θU = 1.0 to allow early
stopping due to futility, but not due to eﬃcacy.
Example 4.1 (Calculating phase IIA predictive probabilities). Suppose an
investigator plans to enroll a maximum of Nmax = 40 patients into a phase
II study. At a given time, x = 16 responses are observed in n = 23 pa-
tients. What is P(response rate > 60%)? Assuming a vague Beta(0.6, 0.4)
prior distribution on the response rate p and letting Y be the number of

PREDICTIVE PROBABILITY
145
Y = i
Pr(Y = i|x)
Bi = Pr(p > 0.60 | x, Y = i)
I(Bi > 0.90)
0
0.0000
0.0059
0
1
0.0000
0.0138
0
2
0.0001
0.0296
0
3
0.0006
0.0581
0
4
0.0021
0.1049
0
5
0.0058
0.1743
0
6
0.0135
0.2679
0
7
0.0276
0.3822
0
8
0.0497
0.5085
0
9
0.0794
0.6349
0
10
0.1129
0.7489
0
11
0.1426
0.8415
0
12
0.1587
0.9089
1
13
0.1532
0.9528
1
14
0.1246
0.9781
1
15
0.0811
0.9910
1
16
0.0381
0.9968
1
17
0.0099
0.9990
1
Table 4.1 Bayesian predictive probability calculation for p0
=
0.60, θT
=
0.90, Nmax = 40, x = 16, n = 23, and a Beta(0.6, 0.4) prior distribution on p.
responses in a future m = 17 patients, Y ’s marginal distribution is Beta-
binomial(17, 16.6, 7.4). At each possible value of Y = i, the conditional pos-
terior of p follows a beta distribution, p|x, Y = i ∼Beta(16.6+i, 24.4−i).
In this example, we set θT = 0.90.
As can be seen from Table 4.1, when Y lies in [0, 11], the resulting
P(response rate > 0.60) ranges from 0.0059 to 0.8415. Hence, we would
conclude H0 for Y ≤11. On the other hand, when Y lies in [12, 17], the
resulting P(response rate > 0.60) ranges from 0.9089 to 0.9990. In these
cases we would instead decide in favor of H1. The predictive probability is
then the weighted average (weighted by the probability of the realization of
each Y ) of the indicator of a positive trial should the current trend continue
and the trial be conducted until the end of the study. The calculation yields
PP = 0.5656. If we were to choose θL = 0.10, the trial would not be stopped
due to futility because PP is greater than θL. Similarly, if we were to choose
θU = 0.95, the trial would not be stopped due to eﬃcacy either. Thus based
on the interim data, we should continue the study because the evidence is
not yet suﬃcient to draw a deﬁnitive conclusion in either direction.

146
PHASE II STUDIES
4.2.2 Derivation of the predictive process design
In this subsection we illustrate how to design a trial using the PP approach
by searching for Nmax, θL, θT , and θU values that satisfy a particular set of
Type I and Type II error rate constraints. Given p0, p1, the prior distribu-
tion on the response rate π(p), and the cohort size for interim monitoring,
we search the aforementioned design parameters to yield a design hav-
ing satisfactory operating characteristics. As mentioned earlier, we choose
θU = 1.0 because if the treatment is working, there is little reason to stop
the trial early; enrolling more patients to the active treatment is good.
Treating more patients until the maximum sample size is reached (usually,
less than 100) can also increase the precision in estimating the response
rate. Given Nmax, the question is, “Are there values of θL and θT that
yield desirable design properties”? Our goal is to identify the combinations
of θL and θT to yield the desired power within the error constraints. There
may exist ranges of θL and θT that satisfy the constraints. By varying Nmax
from small to large, the design with the smallest Nmax that controls both
the Type I and Type II error rates (α and β, respectively) at the nomi-
nal level is the one we choose. This idea is similar to ﬁnding the minimax
design (where we minimized the maximum sample size).
The framework of this PP method allows the investigator to monitor the
trial either continuously or by any cohort size. To implement Algorithm 4.1,
we recommend computing PP and making interim decisions only after the
ﬁrst 10 patients have been treated and evaluated for their response status.
Although the choice of treating a minimum of 10 patients is somewhat
arbitrary, a minimum number of patients is required to provide information
suﬃcient to obtain a good estimate of the treatment eﬃcacy, and avoid
making premature decisions based on spurious results from a small number
of patients. After 10 patients, we calculate PP continuously (i.e., with
cohort size of 1) to monitor the treatment eﬃcacy. A suﬃciently low PP
(e.g., PP ≤θL) suggests that the trial could be stopped early due to futility
(lack of eﬃcacy). Note that PP can be computed for any cohort size and at
any interim time. A trial can be stopped anytime due to excessive toxicity,
however.
Example 4.2 (Lung cancer trial). The primary objective of this study
is to assess the eﬃcacy of a particular combination therapy as front-line
treatment in patients with advanced non-small cell lung cancer. The study
involves a new epidermal growth factor receptor tyrosin kinase inhibitor.
The primary endpoint is the clinical response rate (i.e., the rate of com-
plete response and partial response combined) for the new regimen. The
current standard treatment yields a response rate of approximately 20%
(p0). The target response rate of the new regimen is 40% (p1). With the
constraint of both Type I and Type II error rates ≤0.1, Simon’s optimal
two-stage design yields n1=17, r1=3, Nmax=37, r=10, PET(p0) =0.55 and

PREDICTIVE PROBABILITY
147
E(N | p0)=26.02 with α=0.095 and β=0.097, where PET(p0) and E(N|p0)
denote the probability of early termination and the expected sample size
under the null hypothesis, respectively. Here, n1 is the sample size for the
ﬁrst stage; if there are r1 or fewer responses, the trial will be stopped early
and the treatment is considered ineﬀective. Otherwise, a total of Nmax
patients will be enrolled. If there are a total of r responders or less, the
treatment is declared ineﬀective. On the other hand, if there are at least
r + 1 responders, the null hypothesis is rejected and the treatment is con-
sidered eﬀective. The corresponding minimax design yields n1=19, r1=3,
Nmax=36, r=10, PET(p0) =0.46, and E(N | p0)=28.26 with α=0.086 and
β=0.098.
Switching to the PP approach, we assume a vague Beta(0.2, 0.8) prior
distribution for the response rate p. The trial is monitored continuously af-
ter evaluating the responses of the ﬁrst 10 patients. For each Nmax between
25 and 50, we search the θL and θT space to generate designs that have
both Type I and Type II error rates under 0.10. Table 4.2 lists some of the
results in order of increasing maximum sample size Nmax. Among all the
designs, the design with Nmax = 36 (third line of the second portion of the
table) is the design with the smallest Nmax that has both Type I and Type
II error rates less than 0.1. Note that for Nmax = 35, Type I and Type II
error rates cannot both be controlled at rates less than 0.1. For example,
the ﬁrst line with Nmax = 35 shows that when Type I error is constrained
to be under 0.1, Type I error is greater than 0.1. Likewise, the second line
shows that when Type I error is controlled, Type II error is not. Because
there is no solution for this (and smaller) sample sizes that meets both the
speciﬁed Type I and Type II error constraints, the parameters θL, θT , the
rejection boundary r, the probability of early stopping, and the expected
sample size under the null hypothesis are not provided for these cases.
Based on this setting, θL and θT are determined to be 0.001 and any
value in [0.852, 0.922], respectively. The corresponding rejection regions
(in number of responses / n) are 0/10, 1/17, 2/21, 3/24, 4/27, 5/29, 6/31,
7/33, 8/34, 9/35, and 10/36. The trial will be stopped and the treatment
determined to be ineﬀective at the ﬁrst moment when the number of re-
sponses falls into the rejection region. Based on these boundaries, if the
true response rate is 20%, the probability of accepting the treatment is
0.088. On the other hand, if the true response rate is 40%, the probability
of accepting the treatment is 0.906. The probability of stopping the trial
early is 0.86 and the expected sample size is 27.67 when the true response
rate is 20%. Compared to Simon’s minimax two-stage design, the PP de-
sign monitors the data more frequently, yet also has a larger probability of
early termination and a smaller expected sample size in the null case. Both
designs have the same maximum sample size with controlled Type I and
Type II error rates.
Figure 4.1 shows the stopping regions for Simon’s minimax design (de-

148
PHASE II STUDIES
Simon’s Minimax/Optimal Two-Stage designs:
r1/n1
r/Nmax
PET(p0)
E(N|p0)
α
β
Minimax
3/19
10/36
0.46
28.26
0.086
0.098
Optimal
3/17
10/37
0.55
26.02
0.095
0.097
Predictive Probability-based designs:
θL
θT
r/Nmax
PET(p0)
E(N|p0)
α
β
NA/35
0.126
0.093
NA/35
0.074
0.116
0.001
[0.852,0.922]
10/36
0.86
27.67
0.088
0.094
0.011
[0.830,0.908]
10/37
0.85
25.13
0.099
0.084
0.001
[0.876,0.935]
11/39
0.88
29.24
0.073
0.092
0.001
[0.857,0.923]
11/40
0.86
30.23
0.086
0.075
0.003
[0.837,0.910]
11/41
0.85
30.27
0.100
0.062
0.043
[0.816,0.895]
11/42
0.86
23.56
0.099
0.083
0.001
[0.880,0.935]
12/43
0.88
32.13
0.072
0.074
0.001
[0.862,0.924]
12/44
0.87
33.71
0.085
0.059
0.001
[0.844,0.912]
12/45
0.85
34.69
0.098
0.048
0.032
[0.824,0.898]
12/46
0.86
26.22
0.098
0.068
0.001
[0.884,0.936]
13/47
0.89
35.25
0.071
0.058
0.001
[0.868,0.925]
13/48
0.87
36.43
0.083
0.047
0.001
[0.850,0.914]
13/49
0.86
37.86
0.095
0.038
0.020
[0.832,0.901]
13/50
0.86
30.60
0.100
0.046
Table 4.2 Operating characteristics of Simon’s two-stage designs and the PP
design with Type I and Type II error rates 0.10, a Beta(0.2, 0.8) prior for p,
p0 = 0.2, and p1 = 0.4. The intervals in the second column indicate any θT in the
given closed interval (endpoints included) will deliver the operating characteristics
shown.
noted as “M”), the optimal two-stage design (denoted as “O”), and the
predictive probability design (denoted as “PP”). The “regions” for both
two-stage designs are at two discrete points (corresponding to the ﬁrst and
second stages of the design), while the stepwise stopping boundaries for
the PP design allows continuous monitoring of the trial. Under the PP ap-
proach, the trial can be stopped when there are no responses in the ﬁrst
10-16 patients, 1 response in the ﬁrst 17-20 patients, 2 responses in the
ﬁrst 21-23 patients, and so on. Thus the PP design allows more ﬂexible
and frequent monitoring. In addition, compared to the two-stage designs,

PREDICTIVE PROBABILITY
149
Number of Patients
Rejection Region (in Number of Responses)
0
10
20
30
0
2
4
6
8
10
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
32
34
36
0
2
4
6
8
10
M
M
M: MiniMax
O: Optimal
PP
Stopping Boundaries
O
O
Figure 4.1 Stopping regions for the Simon minimax design (“M”), the optimal
design (“O”), and the predictive probability design (“PP”).
it is harder to stop a trial under the PP design at 17 or 19 patients. For
example, with 0, 1, 2, or 3 responses in 17 patients, the trial will be stopped
under the two-stage optimal design, but the PP design only stops the tri-
als with 0 or 1 responses at this point. It is often undesirable to stop the
trial too early in phase II development. The PP design allows the trial to
be stopped at any time if the accumulating evidence does not support the
new treatment’s eﬃcacy over the standard therapy.
Software note: As usual, software to carry out PP calculations like those above
is available online. After specifying a relatively small sample size and the design
parameters p0, p1, α, and β, the software we used performs a two-dimensional
search over θT and θL to ﬁnd a design satisfying the desired constraints on α
and β. If both the α and β constraints are met then we have a solution; we
can even reduce the sample size and try again. If however there is no solution,
the current sample size is not big enough, and so the software increases the
sample size by 1 and tries again. The desktop version of this software, Phase II
PP Design, is available on the web at the M.D. Anderson software download
website, biostatistics.mdanderson.org/SoftwareDownload.

150
PHASE II STUDIES
4.3 Sequential stopping
One of the advantages of a Bayesian approach to inference is the increased
ﬂexibility to include sequential stopping compared to the more restrictive
requirements under a classical approach. Noninformative stopping rules
are irrelevant for Bayesian inference. In other words, posterior inference
remains unchanged no matter why the trial was stopped. By contrast, the
classical p-value depends crucially on the design of the experiment, of which
the stopping mechanism is a key component. The only condition placed on
the Bayesian is that the stopping rule be noninformative. Technically this
means that the stopping rule and the parameters need to be independent
a priori. See, however, the discussion near the end of Subsection 2.5.4 con-
cerning frequentist stopping bias. Several designs make use of this feature
of Bayesian inference to introduce early stopping for futility and/or for
eﬃcacy.
4.3.1 Binary stopping for futility and eﬃcacy
Thall and Simon (1994) and Thall, Simon, and Estey (1995) introduce a
class of phase II Bayesian clinical trial designs that include stopping rules
based on decision boundaries for clinically meaningful events. To illustrate,
let yi ∈{0, 1} denote an indicator for response for the i-th patient. Let θE
and θS denote the probability of response under the experimental therapy
(E) and standard of care (S), respectively. As mentioned earlier, many
phase IIA studies do not include randomization to control. In such cases
we assume that either θS is known, or at least that an informative prior
distribution p(θS) is available. Let y = (y1, . . . , yn) denote all data up to
patient n. We can meaningfully evaluate posterior probabilities of the form
πn = p(θE > θS + δ | y) .
(4.1)
The probability πn is the posterior probability that the response probability
under the experimental therapy dominates that under the standard of care
by at least δ. The oﬀset δ is ﬁxed by the investigator, and should reﬂect
the minimum clinically meaningful improvement. It also depends on the
nature of the response, the disease and the range of θS. The probability πn
is updated after each patient (or patient cohort), and is subsequently used
to deﬁne sequential stopping rules reminiscent of Algorithm 4.1 of the form
decision =



stop and declare E promising
if πn > Un
continue enrolling patients
if Ln < πn < Un
stop and declare E not promising
if πn < Ln
.
(4.2)
The decision boundaries {(Ln, Un), n = 1, 2, . . .} are parameters of the
design. For example, one could use Ln ≡0.05 and Un ≡0.95 for all n. The
considerations that enter the choice of these boundaries are similar to those

SEQUENTIAL STOPPING
151
for choosing stopping boundaries for frequentist group sequential designs
(e.g., Jennison and Turnbull, 2000, Sec. 2.3).
In practice, one starts with a reasonable ﬁrst choice, evaluates frequentist
operating characteristics (see Section 2.5.4), and iteratively adjusts the
decision boundaries until desired operating characteristics are achieved. For
example, we might start with Ln = 1% and Un = 80%. Next we compute
operating characteristics. We might consider two scenarios: a null scenario
S0 with θE = θS, and an alternative scenario S1 with θE > θS + δ as the
simulation truth. Type I error is then the probability with respect to repeat
experimentation under S0 of ending the trial with the (wrong) conclusion
that E is promising, while power is the probability, with respect to repeated
simulation of possible trial histories under S1, that the trial ends with the
(correct) conclusion that E is promising. Assume we ﬁnd that the Type I
error implied by rule (4.2) is 8%, a bit larger than desired. We would next
try an increased lower bound Ln to reduce the Type I error. Now we might
ﬁnd an acceptable Type I error under S0, but a power of only 70% under
S1. To increase power we might now try to reduce the upper bound, say
to Un = 75%. A sequence of such iterative corrections will eventually lead
to a set of bounds that achieve desirable operating characteristics.
4.3.2 Binary stopping for futility, eﬃcacy, and toxicity
Thall et al. (1995) extend the design from a single outcome to multiple
outcomes, including, for example, an eﬃcacy and a toxicity outcome. This
allows us to consider the phase II analog of Section 3.3, where we described
phase I-II dose-ﬁnding trials that traded oﬀeﬃcacy and toxicity following
the approach of Thall and Cook (2004). In our present context, let CR
denote an eﬃcacy event (e.g., complete response) and TOX a toxicity
event. Thall et al. (1995) describe an example with K = 4 elementary
events {A1 = (CR, TOX), A2 = (noCR, TOX), A3 = (CR, noTOX), A4 =
(noCR, noTOX)}. Eﬃcacy is CR = A1 ∪A3, while toxicity is TOX =
A1 ∪A2, etc. The design again involves stopping boundaries as in (4.2),
but now using posterior probabilities of CR and TOX.
Let (pT (A1), pT (A2), pT (A3), pT (A4)) denote the (unknown) probabili-
ties of the four elementary events A1, A2, A3, and A4 under treatment T,
where T ∈{E, S} (experimental or standard therapy). Suppose we as-
sume a Dirichlet prior for these probabilities. Under standard therapy, we
assume a priori that (pS1, . . . , pS4) ∼Dir(θS1, . . . , θS4). Similarly, under
experimental therapy we assume (pE1, . . . , pE4) ∼Dir(θE1, . . . , θE4). The
parameters θS and θE are ﬁxed. Let yn
i denote the number of patients
among the ﬁrst n who report event Ai and let yn = (yn
1 , . . . , yn
4 ). The
conjugate Dirichlet prior allows for easy posterior updating, since
pE(p1, . . . , p4 | yn) = Dir(θn
E1, . . . , θn
E4) ,

152
PHASE II STUDIES
where θn
Ei = θEi + yn
i . Let ηS(CR) = P
Ai∈CR pSi denote the probability
of complete remission under standard therapy, and similarly for ηE(CR),
ηS(TOX) and ηE(TOX). The posterior p(ηE(CR) | yn) then emerges as
a beta distribution, Be(θn
E1 + θn
E3, θn
E2 + θn
E4). Here we used the fact that
the beta is the special case of a Dirichlet distribution having just two prob-
abilities. Similarly, pE(ηE(TOX) | yn) = Be(θn
E1 + θn
E2, θn
E3 + θn
E4). The
distributions for ηS(·) remain unchanged throughout as p(ηS(TOX)) =
Be(θS1 + θS2, θS3 + θS4), and similarly for ηS(CR).
As before, thresholds on posterior probabilities determine sequential stop-
ping. We track the two posterior probabilities
πn(CR)
=
Pr(ηE(CR) > ηS(CR) + δCR | yn)
(4.3)
and πn(TOX)
=
Pr(ηE(TOX) > ηS(TOX) + δT OX | yn) .
After each patient cohort, the posterior probabilities πn(·) are updated and
compared against thresholds (in this sequence):
decision =







stop, declare E not promising
if πn(CR) < Ln(CR)
stop, declare E too toxic
if πn(TOX) > Un(TOX)
stop, declare E promising
if πn(CR) > Un(CR)
continue enrolling patients
otherwise
(4.4)
The evaluation of πn(CR) requires integration with respect to the two
independent beta-distributed random variables ηE(CR) and ηS(CR), and
similarly for πn(TOX).
Software note: Designs of this type are implemented in public domain software
MultcLean that is available from
http://biostatistics.mdanderson.org/SoftwareDownload/. A basic imple-
mentation using R functions is shown in this chapter’s online supplement.
The following algorithm explains in detail all the steps involved in the
implementation of the approach described above. This algorithm pertains
to a single-arm trial with all patients assigned to the experimental therapy.
Thus the parameters θS never change; only the prior on θE is updated.
Algorithm 4.2 (Phase II stopping for futility, eﬃcacy, and toxicity).
Step 0. Initialization: Initialize θ0
E = θE (posterior parameters = prior
parameters). Set n = 0 (number of patients), n1 = 0 (number of patients
with observed response), t = 0 (calendar time in months), yn
j = 0, j =
1, . . . , 4 (number of patients with event Ai), and k = 4 (cohort size). If
using this algorithm as part of a simulation of operating characteristics,
ﬁx an assumed scenario (simulation truth) which we notate as po
Ej ≡
Pr(Aj)
Step 1. Posterior updating: Update the posterior parameters θn
Ej =
θEj + yn
j .

SEQUENTIAL STOPPING
153
Step 2. Posterior probabilities: Evaluate πn(CR) and πn(TOX) using
Monte Carlo simulation as follows:
Step 2.1. Simulate probabilities under S: For m = 1, . . . , M, sim-
ulate ηm
S (CR) ∼Be(θS1 + θS3, θS2 + θS4) and ηm
S (TOX) ∼Be(θS1 +
θS2, θS3 + θS4).
Step 2.2. Estimate πE: πE(CR) ≈
1
M
P Pr(ηE(CR) > ηm
S (CR) +
δCR | yn) and πE(TOX) ≈
1
M
P Pr(ηE(TOX) > ηm
S (TOX)+δT OX |
yn). The probabilities in the sums are probabilities under the Beta
distributions given earlier.
Step 3. Stopping: If πE(CR) < Ln(CR), stop for lack of eﬃcacy. If
πE(TOX) > Un(TOX), stop for excessive toxicity. If πn(CR) > Un(CR)
then stop for eﬃcacy. If n1 > nmax, stop for maximum enrollment. Oth-
erwise continue to Step 4.
Step 4. Next cohort: If n < nmax then recruit a new cohort, i = n +
1, . . . , n + k, using Pr(xi = j) = po
Ej and recruitment time ti0 = t.
Update n ≡n + k, t = t + 1, yn
i and n1 = Pn
i=1 I(t ≥t0i + 3). Repeat
from Step 1.
Example 4.3 (A BMT trial). We implement the algorithm for an exam-
ple reported in Thall et al. (1995, Section 3.1). These authors consid-
ered a trial with patients who received bone marrow transplant (BMT)
from partially matched donors. The study was a phase II trial of a post-
transplant prophylaxis for graft versus host disease (GVHD). Patients were
monitored for 100 days post transplant. If no GVHD occurs within 100
days, the treatment is considered successful. A major complication here
is transplant rejection (TR). They implement Algorithm 4.2 using CR =
{no GVHD within 100 days}, and TOX = {TR within 100 days}, again
resulting in K = 4 elementary events. Let G and T indicate the events
of observing GVHD and TR within 100 days and let ¯G and ¯T denote the
complementary events. The elementary events are thus { ¯G ¯T, ¯GT, G ¯T, GT}.
We use the Dirichlet prior from Thall et al. (1995), p(pE1, . . . , pE4) =
Dir(θE1, . . . , θE4) with θE ∝(2.037, 6.111, 30.555, 2.037) and P θEj = 4.
The other design parameters are δCR = 20%, δT OX = 5%, and nmax = 75.
The probability bounds are LCR = 2% and UT OX = 80% (both constant
over n). No upper threshold UCR is used, i.e., we do not consider early
stopping for eﬃcacy.
The simulation output is summarized in Figure 4.2. The ﬁgure shows
eﬃcacy and toxicity probabilities under the assumed simulation truth. The
two horizontal lines show the lower bound LCR (at 2%) and the upper
bound UT OX (at 80%). Crossing these bounds triggers the stopping deci-
sions described in (4.4). At time t = 0 months we start with πn(TOX)
and πn(CR) for n = 0 computed under the prior model. As new data is

154
PHASE II STUDIES
5
10
15
20
0.0
0.2
0.4
0.6
0.8
MONTH
Pr(eta[S]+delta < eta[E] | Y)
EFF
TOX
Figure 4.2 Toxicity intervals: Posterior estimated probabilities of complete re-
mission, E(πn(CR) | data), and toxicity, E(πn(TOX) | data), plotted against
month; see (4.3).
accrued, the posterior distributions are updated and the posterior means
for ηE(CR) and ηE(TOX) quickly move down and up, respectively. Note
the 100 day (≈3 month) delay in the change of both posterior means.
4.3.3 Monitoring event times
The stopping rules discussed in the previous section are based on a binary
response variable. The nature of a response of this sort varies across studies.
For example, a typical response might be an indicator for patient survival
beyond seven months. Response variables based on a dichotomized contin-
uous outcome involve a loss of information compared to the original data.
Their main advantage is increased robustness; it is easy to be very general
about a probability model for a binary outcome. By contrast, inference
is often very sensitive with respect to the choice of a speciﬁc parametric
form for the distribution of a continuous outcome. On the other hand, the
likelihood function for the continuous outcome is more informative (i.e.,
more peaked) and allows more decisive inference with fewer observations.
In other words, we achieve faster learning with the same number of pa-
tients. Also, in some studies it is scientiﬁcally inappropriate to reduce the
outcome to a dichotomized binary variable. Another limitation of binary
outcomes is their inherent delays. For example, we might have to wait up

ADAPTIVE RANDOMIZATION AND DOSE ALLOCATION
155
to 100 days after treatment to record a response when the binary outcome
is deﬁned as transplant rejection within 100 days, as in Example 4.3.
Thall, Wooten, and Tannir (2005) propose study designs that allow early
stopping for futility and/or eﬃcacy based on a time-to-event outcome. As-
sume that an event time Ti is recorded for each patient; say, time to disease
progression (TTP). We assume a parametric model for the sampling dis-
tribution; say, an exponential distribution. Let µS denote the mean event
time under the standard of care, and let µE denote the unknown mean
event time under the experimental therapy. Rather than reducing Ti to a
binary outcome (such as TTP > 7), Thall et al. (2005) replace the pos-
terior probabilities πn in (4.1) with corresponding probabilities on the µ
scale, e.g.
πn = p(µE > µS + δ | y) .
On the basis of πn they deﬁne stopping rules similar to (4.2); for exam-
ple, stop for futility when πn < Ln, stop for eﬃcacy when πn > Un, and
continue enrollment otherwise. As before, the tuning parameters δ and
{(Ln, Un), n = 1, 2, . . .} are ﬁxed to achieve desired operating characteris-
tics.
Software note: A public domain software implementation of this approach is
available from
http://biostatistics.mdanderson.org/SoftwareDownload/.
Thall et al. (2005) also discuss extensions to multiple event times, such as
time to disease progression, severe adverse event, and death.
4.4 Adaptive randomization and dose allocation
In this section we consider a randomized phase IIB multi-arm clinical trial.
The multiple arms could be diﬀerent treatments (possibly including a con-
trol arm), diﬀerent doses or schedules of the same agent, or any combination
of such comparisons. There are many good reasons to introduce random-
ization in the assignment of patients to the competing arms; see e.g. Sub-
section 2.2.6 for a discussion. But most of the arguments for randomization
do not require randomization with equal probabilities to all arms. Adap-
tive dose allocation is an attractive device to maintain the advantages of
randomization while introducing increased assignment of patients to more
promising treatments.
4.4.1 Principles of adaptive randomization
Adaptive dose allocation as we discuss it here still includes randomization.
It is distinct from deterministic adaptive dose assignment, such as play-the-
winner rules. Berry and Eick (1995) give a comparative discussion of play-
the-winner versus various randomization rules, including equal randomiza-
tion, adaptive randomization, and a decision-theoretic solution. Here we

156
PHASE II STUDIES
focus on outcome-adaptive designs, as opposed to covariate-adaptive de-
signs that seek to balance covariates across treatments.
The idea of adaptive allocation goes back at least to Thompson (1933)
and, more recently, to Louis (1975, 1977). A recent review appears in Thall
and Wathen (2007), whose approach we discuss in more detail below be-
cause it emphasizes practical applicability and because an implementa-
tion in public domain software is available. Assume there are two arms,
A1 and A2. Let p1<2 denote the posterior probability that arm A2 dom-
inates arm A1. For example, assume that the outcome is a binary eﬃ-
cacy response, and let θ1 and θ2 denote the probability of response un-
der each treatment arm. Let y generically denote the currently available
data. Then p1<2 = p(θ1 < θ2 | y). Thall and Wathen (2007) propose
to randomize to treatments A1 and A2 with probabilities proportional to
r2(y) = {p1<2(y)}c and r1(y) = {1 −p1<2(y)}c. In general, for more than
two arms, use
rj(y) ∝{p(θj = max
k
θk | y)}c.
(4.5)
Thall and Wathen (2007) propose using c = n/2N, where N is the max-
imum number of patients and n is the number of currently enrolled pa-
tients. This recommendation is based on empirical evidence under typical
scenarios. Wathen and Cook (2006) summarize extensive simulations and
give speciﬁc recommendations for the implementation of Bayesian adaptive
randomization. Thall and Wathen (2005) apply the approach to a study
where the probability model for an ordinal outcome includes a covariate.
The outcome is trinary (response, stable, failure), while the covariates are
two binary patient-speciﬁc baseline values. The deﬁnition of (4.5) remains
unchanged; only the relevant probability model with respect to which the
posterior probabilities are evaluated changes. Cheung et al. (2006) applies
the method with rj based on posterior probabilities of survival beyond day
50 under three competing treatment regimens.
Software note: The Adaptive Randomization (AR) package, a Windows appli-
cation for designing and simulating outcome-adaptive randomized trials with
up to 10 arms, is another of those freely available from the M.D. Anderson
software website, biostatistics.mdanderson.org/SoftwareDownload. Out-
comes may be either binary or time-to-event (TITE). Adaptively randomized
trial designs are popular at M.D. Anderson and other institutions because
such designs place more patients on the more eﬀective treatments while also
preserving the beneﬁts of randomization. Between 2005 and 2009, there were
583 registered downloads of Adaptive Randomization.
We now describe the capabilities of the AR package in some detail, but
only for the binary response case; the program’s handling of the TITE
case is somewhat restrictive at present, permitting only an exponential
survival model with a conjugate inverse gamma prior. By contrast, the
binary response cases assumes a beta prior for the probability of response

ADAPTIVE RANDOMIZATION AND DOSE ALLOCATION
157
θk in Arm k, still conjugate but plenty general for most applications. AR
seeks to unbalance the randomization probabilities using (4.5) to favor the
treatment experiencing better interim results. The program comes with an
easy-to-read user’s guide, available online; here we summarize the main
points.
In a binary response setting, our goal may be to ﬁnd the k corresponding
to the largest θk (if the endpoint is eﬃcacy), or the smallest θk (if the
endpoint is toxicity). In either case, we begin by specifying the Beta(αk, βk)
prior distributions for the θk in one of three ways: either by choosing the
(αk, βk) pairs directly, or indirectly by specifying either two quantiles for
the distribution or the mean and the variance, since either permits AR to
“back out” the (αk, βk). Then assuming independent binary responses in
group k of which xk are positive and nk −xk are negative, the posterior
for θk emerges immediately as the familiar Beta(xk + αk , nk −xk + βk)
distribution. This beta posterior is then used to deﬁne a variety of stopping
rules, as follows.
Algorithm 4.3 (Phase IIB AR design)
Step 1. Early loser: If the probability that treatment arm k is the best
falls below some prespeciﬁed probability pL, i.e., if
P(θk > θj̸=k|Data) < pL ,
then arm k is declared a loser and is suspended. Normally one takes pL
fairly small; say 0.10 or less. We note that the software does permit an
arm to return to active status later in the trial if the other arms grow
worse and arm k becomes competitive again.
Step 2. Early winner: If the probability that treatment arm k is the
best exceeds some prespeciﬁed probability pU, i.e., if
P(θk > θj̸=k|Data) > pU ,
then arm k is declared the winner and the trial is stopped early. Normally
one takes pU fairly large; in a two-arm trial we would take pU = 1 −pL,
or else only one among this rule and the previous rule would be active.
Step 3. Final winner: If, after all patients have been evaluated, the prob-
ability that treatment arm k is the best exceeds some prespeciﬁed prob-
ability p∗
U, i.e., if
P(θk > θj̸=k|Data) > p∗
U ,
then arm k is declared the winner. If however no treatment arm can
meet this criterion, AR does not make a ﬁnal selection. One typically
sets p∗
U < pU (say, between 0.70 and 0.90) to increase the chance of
obtaining a ﬁnal winner.

158
PHASE II STUDIES
Step 4. Futility: If the probability that treatment arm k is better than
some prespeciﬁed minimally tolerable response rate θmin falls below
some prespeciﬁed probability p∗
L, i.e., if
P(θk > θmin|Data) < p∗
L ,
then arm k is declared futile and will not accrue more patients. (This
rule applies only when the goal is to ﬁnd the largest θk, i.e., eﬃcacy
trials.) We take p∗
L quite small, typically 0.10 or less. Once an arm is
declared futile, it cannot be re-activated.
As each new patient enters the trial, the randomization probability for
each arm is updated using (4.5) and the available outcome data from all
currently enrolled patients. Thus for a trial with m arms, the probability
of arm k being assigned next is
P(θk = maxj θj|Data)c
Pm
i=1 P(θk = maxj θj|Data)c ,
where c ≥0 (and clearly c = 0 corresponds to equal randomization). As
mentioned above, we might take c to be some signiﬁcant fraction of the
maximum sample size. This is a rather conservative approach, allowing
a substantial amount of information to be gained about both treatment
arms before signiﬁcant adapting occurs. The AR user manual recommends
values of c near 1, and perhaps no bigger than 2, depending on accrual
rates relative to observation time. That is, larger c may be appropriate for
trials with slow relative accrual. The user can also specify a minimum ran-
domization probability (say, 0.10), as well as an initial number of patients
to randomize fairly, before adaptive randomization begins. Both of these
are safeguards against excessively adapting away from a treatment arm. A
minimum number of trial patients (say, 10) can also be speciﬁed.
The software then permits the establishment of various “scenarios” that
describe certain true states of nature (e.g., a null case where the arms
actually all have the same eﬀectiveness). Data are then simulated from
each scenario, with patients randomized according to the rules established.
Repeating this process some large number of times (say, 100 for a quick
investigation, and 1000 or 10,000 for ﬁnal reporting), the design’s operat-
ing characteristics (i.e., empirical probabilities of selection, early selection,
early stopping, and the number of patients randomized for each arm) and
average trial length can be evaluated. Various designs can then be compared
across scenarios in terms of their performance. We illustrate the mechanics
of doing this by means of the following example:
Example 4.4 (Sensitizer trial). We illustrate the use of the AR package
with a University of Minnesota trial designed to assess the eﬃcacy of a “sen-
sitizer,” intended to be given concurrently with a certain chemotherapeutic

ADAPTIVE RANDOMIZATION AND DOSE ALLOCATION
159
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
θ
density
Arm 1 (control)
Arm 2 (sensitizer)
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
θ
density
Arm 1 (control)
Arm 2 (sensitizer)
Figure 4.3 Prior distributions for θ: left, standard priors; right, conservative pri-
ors. Both Arm 1 priors have mean 0.55 and both Arm 2 priors have mean 0.75;
conservative priors have standard deviations equal to twice those of the standard
priors.
agent to enhance its eﬀectiveness. A previous statistical consultant retained
by the investigator had concluded that 49 patients would be required to
implement a one-arm phase II investigation of the sensitizer’s eﬀectiveness
as measured by complete remission (CR) at 28 days post-treatment using
a Simon two-stage design. The investigator, however, would prefer to run a
two-arm comparison of the drug-plus-sensitizer versus the drug alone, and
fears the sample size required would likely be near 100, which is too large
to be practical with her expected patient accrual rate (about 30 per year).
She does have some prior information she is willing to use, and is hoping
that using a Bayesian adaptive design will also save enough patients that
she can reasonably expect success with, say, 60 total patients.
We use the AR software to design such a trial. Choosing the binary end-
point option (CR/no CR), we set the maximum patient accrual to 60, and
the minimum randomization probability to 0.10, and specify that the ﬁrst
14 patients should be randomized fairly (7 to each arm) before adaptive
randomization begins. We then set the crucial tuning parameter c = 1;
recall larger values of c correspond to potentially greater deviation from
equal randomization (after the ﬁrst 14 patients).
Turning to the prior distributions, the investigator wishes to specify re-
sponse rates of .55 in the control arm and .75 in the sensitizer arm. As-
suming standard deviations of 0.10 and 0.13 in the two arms, respectively,
determines the two beta distributions shown in the left panel of Figure 4.3.
The right panel of this ﬁgure shows two “conservative” versions of these

160
PHASE II STUDIES
Scenario 1
Average Trial Length: 22.5 months
True Pr
Pr
Pr(select
Pr(stop
# Patients
Arm
(success)
(select)
early)
early)
(2.5%, 97.5%)
Arm1
0.55
0.01
0
0.11
19.6 ( 5, 38 )
Arm2
0.55
0.16
0.11
0
35.6 ( 8, 53 )
Scenario 2
Average Trial Length: 16.4 months
True Pr
Pr
Pr(select
Pr(stop
# Patients
Arm
(success)
(select)
early)
early)
(2.5%, 97.5%)
Arm1
0.55
0
0
0.55
10.1 ( 4, 22 )
Arm2
0.7
0.74
0.55
0
30.8 ( 4, 51 )
Scenario 3
Average Trial Length: 10.8 months
True Pr
Pr
Pr(select
Pr(stop
# Patients
Arm
(success)
(select)
early)
early)
(2.5%, 97.5%)
Arm1
0.55
0
0
0.89
7.01 ( 4, 16 )
Arm2
0.8
0.96
0.89
0
20.1 ( 4, 51 )
Table 4.3 Operating characteristics, sensitizer trial design using the standard
prior and standard stopping rule.
two priors, obtained simply by doubling their standard deviations (to 0.20
and 0.26, respectively).
For our stopping rules, we begin with a “standard” rule that sets the
early loser selection probability pL = 0.025, the early winner selection
probability pU = 0.975, the ﬁnal winner selection probability p∗
U = 0.90,
and the futility parameters θmin = 0.50 and p∗
L = 0.05. We also consider
a more “liberal” stopping rule that instead uses pL = 0.05 and pU = 0.95,
making early losing and winning somewhat easier to achieve. We then run
AR, comparing results from three diﬀerent scenarios:
• Scenario 1: true response rates of .55 in both groups (the “null” scenario),
• Scenario 2: true response rates of .55 control, .70 sensitizer (the “most
likely” scenario), and
• Scenario 3: true response rates of .55 control, .80 sensitizer (the “opti-
mistic” scenario).
Tables 4.3–4.5 give the simulated operating characteristics for three dif-

ADAPTIVE RANDOMIZATION AND DOSE ALLOCATION
161
Scenario 1
Average Trial Length: 21.0 months
True Pr
Pr
Pr(select
Pr(stop
# Patients
Arm
(success)
(select)
early)
early)
(2.5%, 97.5%)
Arm1
0.55
0.19
0.05
0.15
26.2 ( 5, 45 )
Arm2
0.55
0.16
0.13
0.09
25.4 ( 4, 47 )
Scenario 2
Average Trial Length: 18.1 months
True Pr
Pr
Pr(select
Pr(stop
# Patients
Arm
(success)
(select)
early)
early)
(2.5%, 97.5%)
Arm1
0.55
0.04
0.02
0.39
15.2 ( 4, 44 )
Arm2
0.7
0.52
0.39
0.02
29.4 ( 4, 49 )
Scenario 3
Average Trial Length: 14.3 months
True Pr
Pr
Pr(select
Pr(stop
# Patients
Arm
(success)
(select)
early)
early)
(2.5%, 97.5%)
Arm1
0.55
0
0
0.63
10.6 ( 2, 26 )
Arm2
0.8
0.8
0.63
0
25.6 ( 3, 50 )
Table 4.4 Operating characteristics, sensitizer trial design using the conservative
prior and standard stopping rule.
ferent designs. All of our results are based on just 100 simulated trials
each — probably too small to be considered reliable for practical use, but
large enough for us to illustrate the diﬀerences across designs and scenarios.
First, Table 4.3 shows the results from the design using the standard prior
and standard stopping rule. This design has pretty good Type I error (17%
total selection probability in the “null” scenario), good power (74%) in the
“most likely” scenario, and outstanding power (96%) in the “optimistic”
scenario. But the total sample sizes are fairly high (10.1 + 30.8 ≈41 in
the most likely scenario), and the average trial lengths are fairly long (16.4
months in the most likely scenario, under our assumed accrual rate of 2.5
patients per month, or 30 per year).
By contrast, Table 4.4 shows the results from the design using the con-
servative prior with the standard stopping rule. As expected, this design
is more conservative, borrowing far less strength from the investigator’s
clinical opinion and thus forcing the data to largely stand on their own. As

162
PHASE II STUDIES
Scenario 1
Average Trial Length: 18.2 months
True Pr
Pr
Pr(select
Pr(stop
# Patients
Arm
(success)
(select)
early)
early)
(2.5%, 97.5%)
Arm1
0.55
0
0
0.33
18.1 ( 2, 39 )
Arm2
0.55
0.37
0.33
0
26.4 ( 3, 49 )
Scenario 2
Average Trial Length: 11.3 months
True Pr
Pr
Pr(select
Pr(stop
# Patients
Arm
(success)
(select)
early)
early)
(2.5%, 97.5%)
Arm1
0.55
0
0
0.73
9.56 ( 2, 26 )
Arm2
0.7
0.82
0.73
0
19.1 ( 2, 48 )
Scenario 3
Average Trial Length: 8.15 months
True Pr
Pr
Pr(select
Pr(stop
# Patients
Arm
(success)
(select)
early)
early)
(2.5%, 97.5%)
Arm1
0.55
0
0
0.92
7.2 ( 1, 27 )
Arm2
0.8
0.93
0.92
0
13.5 ( 2, 46 )
Table 4.5 Operating characteristics, sensitizer trial design using the standard
prior and liberal stopping rule.
a result, Type I errors are a bit higher, and power a bit lower. Still, the
investigator might adopt this design since thanks to adaptivity it continues
to save patient resources: it uses just 36 on average in the optimistic case,
and 44 in the most likely case (recall the Simon two-stage design required
49 patients in the one-arm case).
Finally, Table 4.5 shows the results from the design that returns to the
standard prior, but now couples it with the more liberal stopping rule.
Again as expected, this design has somewhat higher Type I error (37%,
all of it due to incorrect selections of the sensitizer arm) than the baseline
design in Table 4.3. But this design also ﬁnishes much more quickly, taking
just 11.3 months and using only 29 patients on average (instead of 16.4 and
41 for the baseline design) in the most likely scenario.
In summary, all three designs have strengths and weaknesses. And this
analysis was certainly not intended to be exhaustive: the investigator (or
her statistician) would likely wish to consider even more designs. For ex-

ADAPTIVE RANDOMIZATION AND DOSE ALLOCATION
163
ample, if we sought to reduce Type I error, we might consider a more
conservative stopping rule that sets pL = 0.01 and pU = 0.99.
4.4.2 Dose ranging and optimal biologic dosing
Dose ranging studies are phase II trials that seek to ﬁnd the dose with high-
est eﬃcacy within a range of safe doses. Many traditional designs assume
that the probability of toxicity increases monotonically with dose. However,
the increased use of molecularly targeted therapies requires alternative sta-
tistical approaches that target an optimal biological dose (OBD) without
assuming a monotone dose-response relationship. The OBD is the dose with
maximum therapeutic eﬀect.
One of the few such approaches is that of Bekele and Shen (2005), who
deﬁne dose-ﬁnding based on jointly modeling toxicity and biomarker re-
sponse. The marginal model for toxicity is a probit model with monotone
dose-speciﬁc means, while the marginal model for biomarker response is a
dynamic state space model (deﬁned below). The two marginal models are
linked by introducing correlation of dose-speciﬁc parameters for toxicity
and biomarker response.
Similar state space models for ﬂexible dose-response curves are also used
in M¨uller et al. (2006) and in Smith et al. (2006). Let f(d) denote the mean
response at dose d. Before we describe details of the model, we outline some
important features. Let Dj, j = 1, . . . , J, denote the range of allowable
doses, and θj ≡f(Dj) the vector of mean responses at the allowable doses.
The underlying idea is to formalize a model which locally (i.e., for d close
to Dj) ﬁts a straight line for the response y,
y = θj + (d −Dj)δj ,
having level θj and slope δj. When moving from dose Dj−1 to Dj, the
parameters αj = (θj, δj) change by adjusting the level to θj = θj−1 + δj−1
adding a (small) so-called evolution noise ej.
Let Yjk, k = 1, . . . , νj, denote the k-th response observed at dose Dj, i.e.,
Yj = {Yjk, k = 1, . . . , νj} is the vector of responses yi of all patients with
assigned dose di = Dj. Note the notational convention of using upper case
symbols for quantities Dj and Yjk indexed by doses, and lower case yi and
di for quantities indexed by patients. Also, we will use Y(j) = (Y1, . . . , Yj)′
for all responses up to and including dose Dj, and y(n) = (y1, . . . , yn)′ for
all data up to and including the n-th patient. The resulting model is
Yjk = θj + ϵjk, j = 1, . . . , n, k = 1, . . . , νj
and
(θj, δj) = (θj−1 + δj−1, δj−1) + ej ,
(4.6)
with independent errors ϵj ∼N(0, V σ2) and ej ∼N2(w, Wσ2). The ﬁrst

164
PHASE II STUDIES
equation describes the distribution of Yjk conditional on the state parame-
ters αj = (θj, δj) and is referred to as the observation equation; the second
equation formalizes the change of αj between doses and is referred to as the
evolution (or state) equation. For a given speciﬁcation of (V, W) and priors
p(α0) = N(m0, C0) and p(σ−2) = Gamma(v0/2, S0/2) with given moments
m0, C0 and S0, and degrees of freedom v0, there exists a straightforward re-
cursive algorithm to compute posterior distributions p(αj|Y1, . . . , Yj) and
any other desired posterior inference. The algorithm, known as Forward
Filtering Backward Sampling (FFBS), is described in Fr¨uhwirth-Schnatter
(1994).
Software note: Smith et al. (2006) and Smith and Richardson (2007) include
WinBUGS code to implement the FFBS for a normal dynamic linear model. We
outline an FFBS algorithm for our setting below, with an R implementation
again given in this chapter’s software page,
www.biostat.umn.edu/~brad/software/BCLM_ch4.html
The approach from M¨uller et al. (2006) is described by the following
algorithm, which implements the simulation of one possible trial history
under an assumed scenario θo = (θo
1, . . . , θo
J). In addition to the model
for the unknown mean response curve, implementation of trial simulation
requires the speciﬁcation of a dose allocation rule and stopping rules.
Dose allocation: Let D⋆= D⋆(θ) denote the (unknown) ED95 dose de-
ﬁned by θj⋆= min θj + 0.95(max θj −min θj). Usually equality cannot be
achieved; the ED95 is then deﬁned as the dose with mean response closest
to the target. Let r = (r1, . . . , rJ) denote the probability of allocating the
next patient to dose j, j = 1, . . . , J. We use rj ∝
p
Pr(D⋆= j | y(n)), sub-
ject to r1 ≥10% allocation to placebo. The allocation probability rj is a
variation of the adaptive allocation rule (4.5) discussed in Subsection 4.4.1.
Sequential stopping: Let ∆denote the smallest mean response that would
be considered a success. We follow the example from Krams et al. (2005)
who apply the proposed design to a stroke trial. The outcome is an im-
provement in stroke score, with an improvement of at least 3 points being
considered a success. Let sj = Pr(θj > 3 | y(n)). Following Krams et al.
(2005) we deﬁne the following stopping rules: (i) Stop for futility when
maxj sj < 0.05, and (ii) stop for eﬃcacy when s ˆ
D⋆> 0.95, where ˆD⋆is the
currently estimated ED95.
Algorithm 4.4 (Optimal Biologic Dose).
Step 0. Initialization: Use rj = 1/J. Initialize sample size n = 0, and
maximum sample size nmax (say, to 100).
Step 1. Next patient cohort: Select doses di, i = n+1, . . . , n+k using
the current allocation probabilities rj. Generate (simulated) responses
yi ∼N(θo
j, σ2). Increment n ≡n + k.

ADAPTIVE RANDOMIZATION AND DOSE ALLOCATION
165
Step 2. Posterior updating: Use FFBS (see Algorithm 4.5) to summa-
rize p(θ | y(n)). Record sj = Pr(θj > 3 | y(n)) and ˆD⋆= Dj∗where
j∗= arg maxj{Pr(Dj = D∗| y(n))}.
Step 3. Stopping: If maxj sj < 0.05 stop for futility. If s ˆ
D⋆> 0.95 stop
for eﬃcacy and report ˆD⋆as the recommended dose. If n > nmax stop
for maximum sample size. Otherwise continue with step 1. Note that the
stopping criterion includes stopping for both eﬃcacy and futility.
Step 2 requires an implementation of the FFBS algorithm. This chapter’s
software page,
www.biostat.umn.edu/~brad/software/BCLM_ch4.html
includes R macros for a basic FFBS implementation. The algorithm is exact,
and consists of two ﬁnite iterations. In the ﬁrst loop we evaluate p(αj |
Y(j)), j = 1, . . . , J. In the second loop we evaluate p(αj | αj+1,...,J, Y(J))
and ¯µj ≡E(θj | Y(J)), j = 1, . . . , J. All distributions are normal, and fully
characterized by the ﬁrst two moments. In the following description p(αj |
Y(j−1)) = N(rj, Rj), p(αj | Y(j)) = N(mj, Cj) and p(αj | αj+1, Y(J)) =
N(µj(αj+1), Σj) deﬁne three sequences of posterior distributions. The three
sets of posteriors condition upon the data up to the (j −1)st dose, up to
the jth dose, and on all data (and αj+1), respectively. A prior p(α0) =
N(m0, C0) deﬁnes a starting condition for the following iteration.
Algorithm 4.5 (FFBS).
Step 0. Initialize: We formalize prior information by including additional
fake observations yo
j ∼N(θj, σ2/no
j). Using fractional values for no
j al-
lows us to specify arbitrarily accurate prior equivalent sample sizes.
Step 1. Forward ﬁltering: For j = 1, . . . , J carry out the following up-
date. Let
yj = νj
P
k Yjk + no
jyo
j
νj + no
j
and nj = νj + no
j denote the sample mean and sample size of all ob-
servations (including the prior equivalent data) at dose j. Update the
posterior moments as Rj = GCj−1G′ + τI, rj = Gmj−1 and C−1
j
=
R−1
j
+ Z′Znj/σ2, mj = Cj(R−1
j rj + nj/σ2Z′yj).
Step 2. Backward smoothing: Set ΣJ = CJ and µJ = mJ. For j = J −
1, . . . , 1, evaluate Σ−1
j
= C−1
j
+GG′/τ 2 and µj = Σj
¡
C−1
j
mj + G′µj+1
¢
.
Step 3. Posterior simulation: For the evaluation of the posterior distri-
bution of the unknown ED95, D⋆it is useful to include (exact) posterior
simulation. We generate a set of M posterior draws θm ∼p(θ1, . . . , θJ |
DJ), m = 1, . . . , M. To do this, we start with the highest dose and
work our way down. That is, start with θm
J ∼N(µJ, ΣJ), and then for

166
PHASE II STUDIES
5
10
15
−1
0
1
2
3
4
5
6
DOSES
G
G
G
G
G
G
G
G
G
G
G
G
5
10
15
−1
0
1
2
3
4
5
6
DOSES
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
(a) Optimistic scenario 4
(b) Skeptical scenario 1
5
10
15
−1
0
1
2
3
4
5
6
DOSES
G
G
G
G
G
G
G
G
G
G
G
G
(c) Null scenario 0
Figure 4.4 The three plots summarize the simulation of one possible trial history
under three alternative curves as simulation truth (dotted curves in panels a, b,
and c). The ﬁgures show the posterior estimated curves after each cohort (thin
dashed) and after stopping (thick solid), the data (dots), and the estimated ED95
at the end of the trial (vertical line).
j = J −1, . . . , 1, evaluate µm
j = Σj(C−1
j
mj + 1/τ 2G′θm
j+1) and generate
θm
j ∼N(µm
j , Σj).
Example 4.5 (Stroke trial). We implemented Algorithm 4.5 for the stroke
study in Berry et al. (2001), where again the response yi is improvement
in stroke score over a 90-day period. We assume a range of J = 16 pos-
sible doses, j = 1, . . . , J, including placebo as D1 = 0. The maximum
number of patients was set to nmax = 100. We use three alternative sce-
narios of assumed true response proﬁles as simulation truth. The scenar-

ADAPTIVE RANDOMIZATION AND DOSE ALLOCATION
167
ios presented here correspond to an optimistic assumption of a signiﬁcant
treatment eﬀect at higher doses (scenario 4), a skeptical scenario with a
moderate treatment eﬀect only (scenario 1) and a null scenario assuming no
treatment eﬀect (scenario 0). The three scenarios are shown in Figure 4.4.
The ﬁgure shows the simulated responses under one simulated realization
of the entire trial. The data are shown as dots. Also shown are the posterior
mean response curve E(θ1, . . . , θJ | y(n)) (thick line), the simulation truth
(dotted line), and random draws from p(θ1, . . . , θJ | y(n)) (dashed lines).
The latter illustrate posterior uncertainty.
Under all three scenarios the sequential stopping rule leads to early ter-
mination. Under scenario 4, the simulation stopped early for eﬃcacy after
n = 12 patients. Under scenarios 1 and 0, the simulation stopped early for
futility after n = 46 and after n = 12 patients, respectively.
4.4.3 Adaptive randomization in dose ﬁnding
The adaptive randomization ideas of Subsection 4.4.1 can be applied much
more broadly than just adaptively randomizing to two treatments. Blending
in the ideas of the previous subsection, an adaptive randomization can also
be to one of K competing dose levels, bringing us into the area of adaptive
dose allocation. Here we oﬀer a simple example based on an setting already
seen in Chapter 3.
Example 4.6 (Stroke trial revisited). The stroke trial of Example 4.5 in-
cluded adaptive dose allocation. A variation of the allocation probabilities
(4.5) was used. We deﬁned
rj(y) ∝
q
Pr(D⋆= Dj | y) .
Recall that D⋆denoted the (unknown) ED95 dose. Under the model used
in Example 4.5, the posterior distribution of D⋆is only available via simu-
lation. The underlying model is not critical. For example, under an alterna-
tive implementation with, say, a shifted and scaled logistic dose/response
curve, the posterior probabilities for D⋆would also be easily available by
posterior simulation. In either case, for each dose Dj one would evaluate the
posterior probability Pr(D⋆= Dj | y). Using the allocation probabilities
rj, we favor allocation of the next patient cohort at doses that are likely
to be the desired ED95 dose. The adaptive allocation rule rj(y) formalizes
Step 2 in Algorithm 4.4.
For the implementation described in Example 4.5, Figure 4.5 shows the
sequence of dose allocations over time for one realization of the trial. Notice
how the allocation probabilities diﬀer under the three considered scenarios.
Recall that the three scenarios were an optimistic scenario assuming a
maximum beneﬁt over placebo of up to 4 points (Scenario 4), a sceptic
scenario with only a moderate 2-point advantage over placebo (Scenario

168
PHASE II STUDIES
G
G
G
G
G
G
G
G
G
G
G
G
2
4
6
8
10
12
5
10
15
PATIENT
DOSE
G
G
G
G
G
GG
GG
G
G
G
G
G
GGGG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
0
10
20
30
40
5
10
15
PATIENT
DOSE
(a) Optimistic scenario 4
(b) Skeptical scenario 1
G
G
G
G
G
G
G
G
G
G
G
G
2
4
6
8
10
12
5
10
15
PATIENT
DOSE
(c) Null scenario 0
Figure 4.5 Dose allocation (vertical axis) by patient (horizontal axis).
1) and a null scenario with a horizontal line at 0 points advantage over
placebo (Scenario 0). The algorithm correctly zooms in on the range of
interesting doses. For a practical implementation of adaptive dose allocation
it is important that the adaptation not be too greedy. In this example this
is achieved by enforcing a minimum allocation probability at placebo, and
by using a square root transformation.
4.4.4 Outcome adaptive randomization with delayed survival response
Many clinical studies involve responses that are observed with a substantial
delay between assignment of a treatment and the recording of the outcome.
A typical example is tumor response in phase II trials, as in Example 4.2.
For solid tumors, response might be deﬁned in terms of tumor size after a

ADAPTIVE RANDOMIZATION AND DOSE ALLOCATION
169
ﬁxed number of days. Another example is the phase II trial discussed in
Example 4.3. There the outcome was deﬁned as occurrence of graft versus
host disease (GVHD) within 100 days. In either case the investigator would
have to wait for the response of the currently recruited patients before being
able to make a decision about sequential stopping or treatment allocation
for the next patient.
The Bayesian paradigm provides a principled approach to address such
complications. If a decision is desired before all responses from the previous
patient (or patient cohort) are available, the investigator can proceed on
the basis of the partial information. Inference is based on the posterior
distribution conditional on all data available at the time of making the
decision. Subsection 4.3.3 discussed a trial design based on event times,
while the TITE-CRM design discussed in Subsection 3.2.3 used partial
information for a binary tumor response. The TITE-CRM assumed that
the response is an indicator for an event time occuring within a speciﬁed
period.
Some designs go a step further by introducing available early responses.
Let S denote an early response (for example, complete response based
on tumor size) and let T denote the desired ﬁnal response (for example
progression-free survival, PFS). Modeling the joint distribution of S and T,
we can use available data on S to update posterior inference on parameters
related to T. This allows the construction of more eﬃcient clinical trial
designs.
Recall that in Example 4.5 we discussed a stroke trial. The outcome here
was improvement by day 90 in stroke score over baseline. However, in stroke
patients most of the improvement occurs over the ﬁrst few weeks, making
it particularly attractive to use early responses to improve the trial design.
Let T denote the ﬁnal day 90 response and let St denote the improvement
by week t, t = 1, . . . , 12. In the implementation of this trial, Berry et al.
(2001) used a joint probability model for T and (S1, . . . , S12) to address
some of the challenges related to the delayed 90-day response.
Another good example of this strategy is the design proposed in Huang et
al. (2009). These authors propose a study design that uses progression-free
survival (henceforth simply “survival”) for adaptive treatment allocation.
They consider a phase IIb trial with two treatment arms, A and B. They ar-
gue that one of the reasons for the high failure rate of phase III therapeutic
trials is the use of tumor shrinkage, rather than the ultimately important
survival outcome, as a primary endpoint in phase IIb trials. The main rea-
son why investigators nevertheless continue to use tumor shrinkage are the
practical complications arising from the substantial lag between treatment
assignment and reporting of a delayed survival response.
Huang et al. (2009) propose a design that combines the relative advan-
tages of both the easily observed tumor response and the ultimately rele-
vant survival endpoint. The key feature of the design is a joint probability

170
PHASE II STUDIES
model for tumor response (S) and survival (T). Let xi ∈{A, B} denote the
treatment assignment for the i-th patient, and let (Si, Ti, δi) denote the
outcome for the i-th patient, with Si denoting tumor response (i.e., tumor
shrinkage), Ti denoting the survival time, and δi ∈{0, 1} a binary indica-
tor with δi = 1 when Ti is observed and δi = 0 when only a censored time
ti ≤Ti is recorded. In other words, at calendar time t the recorded response
for a patient who was recruited at (calendar) time t0
i is ti = min(Ti, t−t0
i ),
with δi indicating whether ti is an observed survival time. The authors
assume that tumor response is reported as a categorical outcome with 4
possibilities, Si ∈{1, 2, 3, 4}, referring to resistance to treatment or death
(Si = 1), stable disease (Si = 2), partial remission (Si = 3), and complete
remission (CR; Si = 4). The joint probability model for (Si, Ti) is
P(Si = j | xi = x) = pxj and P(Ti | Si = j, xi = x) = Exp(λxj) ,
(4.7)
where Exp(λ) indicates an exponential distribution with mean µ = 1/λ.
The model is completed with a prior
(px1, . . . , px4) ∼Dir(γx1, . . . , γx4), and µxj ≡
1
λxj
∼IG(αxj, βxj)
(4.8)
independently for x ∈{A, B}. Here Dir(a1, . . . , a4) denotes a Dirichlet
distribution with parameters (a1, . . . , a4) and IG(a, b) is an inverse gamma
distribution with mean b/(a −1). The model is chosen to allow closed-
form posterior inference. Let nxj = Pn
i=1 I(Si = j and xi = x) denote the
number of patients with response j under treatment x, let t denote the
current calendar time, let γ′
xj = γxj + nxj, and let
α′
xj = αxj +
X
i: Si=j, xi=x
δi and β′
xj = βxj +
X
i: Si=j, xi=x
ti
(4.9)
with ti = min{Ti, t −t0
i } denoting the observed survival time Ti if δi = 1,
and the censoring time t −t0
i if δi = 0. Letting Y generically denote the
observed data, we have
p(px1, . . . , px4 | Y ) = Dir(γ′
x1, . . . , γ′
x4) and p(µxj | Y ) = IG(α′
xj, β′
xj) .
(4.10)
Huang et al. (2009) propose a trial design that includes continuous updating
of the posterior distributions (4.10), adaptive allocation based on current
posterior inference and early stopping for futility and for superiority. For
adaptive allocation they consider the posterior probability
p = P(µA > µB | Y ),
with µx = P pxjµxj for x ∈{A, B} indicating the mean PFS on treatment
arm x. By allocating patients to arm A with probability p, the design
increases the probability that patients receive the best treatment. The same
posterior probability is used to deﬁne early stopping for futility when p <

ADAPTIVE RANDOMIZATION AND DOSE ALLOCATION
171
pL and for superiority when p > pU, using, for example, pL = 0.025 and
pU = 1−pL. The proposed design is summarized in the following algorithm.
Algorithm 4.6 (Adaptive allocation with survival response).
Step 0. Initialization: Initialize sample size and calendar time n = 0
and t = 0. Initialize p = 0.5.
Step 1. Next cohort: Recruit the next cohort of patients, i = n+1, . . . , n+
k, and then:
• Allocate treatments A and B with probabilities p and q = 1 −p,
respectively, by generating xi with P(xi = A) = p and P(xi = B) =
q.
• Increment calendar time by one week, t = t + 1.
• Simulate tumor responses, Si,k, k = 1, . . . , 4 for the newly recruited
patients.
• Generate simulation truth for the (future) progression free survival
times Ti for the newly recruited patients, and record recruitment time
(calendar time), t0
i = t.
• Increment n ≡n + k.
Step 2. Posterior updating: Update the posterior parameters α′
xj, β′
xj
and γ′
xj deﬁned in (4.9). Compute and record p = P(θA > θB | Y ), with
θx = P
j pxjµxj, x ∈{A, B}.
Step 3. Stopping: If p < pL then stop for futility. If p > pU stop for
eﬃcacy.
If t > tmax stop for having reached the maximum horizon.
Otherwise continue with step 1.
Note that this algorithm describes the simulation of one trial realization.
For the evaluation of operating characteristics, we would use repeated sim-
ulation under an assumed simulation truth, generating the tumor responses
from (4.7). By contrast, for use in an actual implementation of the trial
design, to evaluate the stopping criterion and the treatment allocation for
the respective next patient, we would start with Step 2 and use the actual
data in place of any simulated responses.
Software note: A basic R implementation of this algorithm is included in the
online supplement to this chapter,
www.biostat.umn.edu/~brad/software/BCLM_ch4.html .
The R code carries out simulation of one hypothetical realization of a trial
following the proposed algorithm. The code includes a loop over patient
cohorts. Within the loop we carry out posterior updating before recruiting
the respective next cohort. One of the posterior summaries is the proba-
bility of superiority for treatment arm A, p = P(µA > µB | y). We use

172
PHASE II STUDIES
0
5
10
15
20
25
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
WEEK
p(mu[A] > mu[B] | y)
0
5
10
15
20
25
0
20
40
60
80
100
120
WEEK
E(mu[A], mu[B] | y)
mu[B] (truth)
mu[A] (truth)
(a) P(µa > µb | y)
(b) E(µa, µb | y)
Figure 4.6 Adaptive leukemia trial with survival response. Posterior probability
of superiority by week (a) and posterior mean survival time by week (b). In panel
(a) the solid line shows the P(µa > µb | y) under the proposed model (4.7) and
(4.8). The dashed line shows the same probability under a corresponding model
without regression on early responses.
independent Monte Carlo simulation to numerically evaluate p and use p
to carry out the stopping decision.
Example 4.7 (Adaptive leukemia trial with survival response). Huang et
al. (2009) discuss an application of the proposed design to a phase II trial
for acute myelogenous leukemia. One of their scenarios (Scenario 3 in the
paper) assumes a simulation truth with higher response rates and longer
response durations (i.e., PFS) under treatment B. The simulation truth
is model (4.7) with pA = (0.2, 0.4, 0.1, 0.3), pB = (0.1, 0.1, 0.2, 0.6), µA =
(4, 30, 75, 110) and µB = (6, 45, 112, 165). The implied true marginal mean
PFS is 53 weeks under treatment A and 126 weeks under treatment B.
We implemented Algorithm 4.6 for this application. We assumed a max-
imum number of n = 120 patients, accruing one patient per week. After
enrolling the last patient we allow the trial to continue for up to 40 weeks,
recording survival times for the already enrolled patients. However, under
the stopping rule in Algorithm 4.6 the protocol almost always stops early.
Huang et al. (2009) report an average number of n = 62 patients. Fig-
ure 4.6 summarizes one simulated trial realization. The solid curve in panel
(a) plots the posterior probability p = P(µA > µB | y) as a function of the
sample size. When p crosses the threshold p < pL = 0.025, the trial stops
for futility around t = 27 weeks.
For comparison we also evaluated posterior inference under an alternative
probability model without regression on early responses Si. The alternative
model assumes P(Ti | Si = j) = Exp(λx) with a conjugate inverse gamma

HIERARCHICAL MODELS FOR PHASE II DESIGNS
173
prior µx ∼IG(αx, βx) for µx = 1/λx. Huang et al. (2009) refer to the
alternative model as the “common model.” We ﬁx the prior with αx = 2
and βx = 60, x ∈{A, B}. The dashed curve in Figure 4.6(a) shows the
sequence of posterior probabilities p = P(µA > µB | y) under the com-
mon model. Eventually the probabilities under the two models converge.
However, notice the substantial diﬀerences around Week 15. The posterior
probabilities p (correctly) decrease more steeply when we include regression
on early responses.
4.5 Hierarchical models for phase II designs
Hierarchical models are one of the big successes stories of Bayesian bio-
statistics. Hierarchical models are used to formalize borrowing strength
across related subpopulations; for example, diﬀerent trials of the same
treatment in slightly diﬀerent populations, or diﬀerent treatments of the
same disease. An important feature is that this sharing of information is
done in a coherent fashion. The use of an underlying encompassing prob-
ability model across related submodels ensures coherent behavior. In Sec-
tion 2.4 we reviewed the basic setup of hierarchical models and showed
an example from the realm of metaanalysis, including WinBUGS and BRugs
code. In this section we elaborate on the use of hierarchical models in phase
II clinical trial design.
Thall et al. (2003) develop a phase II design for related trials. Their
motivating application is a phase II trial for 12 diﬀerent sarcomas. On
the one hand it would be inappropriate to pool all patients across the
12 disease subtypes. On the other hand, one would certainly want to use
information from other subtypes when making a judgment about any of
the subpopulations. A practical limitation is the very slow accrual. Only
very small sample sizes (6 or fewer) are expected for some subtypes, making
it impossible to run separate trials. Besides practical concerns, there are
important ethical considerations that require investigators to pool across
closely related subpopulations.
Let j = 1, . . . , J index J related subpopulations (for example, the J = 12
sarcomas). Let yj generically denote the data for the j-th subpopulation.
Within each subpopulation we assume a submodel P(yj | θj). The submod-
els are combined into a joint encompassing hierarchical model by assuming
θj ∼P(θj | φ). The following algorithm outlines the speciﬁc steps of a
phase II trial across subpopulations. Let nj denote the number of patients
enrolled in population j. We assume that the submodel in population j
is binomial sampling, yj ∼Bin(nj, πj). The binomial success probabilities
are transformed to θj = log(πj/(1 −πj)) and given a common hyperprior
with moments η = (µ, Σ). In summary,
yj | θj ∼Bin(nj, πj(θj)) and θj ∼N(µ, Σ) .
(4.11)

174
PHASE II STUDIES
The model is completed with a hyperprior on η = (µ, Σ). We assume
Σ = τI, where
µ ∼N(mµ, sµ) and τ ∼Gamma(a0, b0) .
The following algorithm outlines the simulation of a hypothetical trial using
the design proposed in Thall et al. (2003). Let y(t) denote all data up to
and including month t. Thall et al. (2003) propose a design that involves
continuous updating of
p30,j = P(πj > 30% | y(t)).
Accrual for subpopulation j is stopped if p30,j < 0.05 at any time. We
assume known accrual rates. Let kj denote the number of recruited patients
in subpopulation j in a given month. We assume P(kj = k) = ajk is known,
with an upper bound kj ≤K. We assume a simulation truth πo
j.
Algorithm 4.7 (Phase II design for related subpopulations).
Step 0. Initialize: Initialize nj = 0, j = 1, . . . , J, t = 0, and the prior
moments (m0, s0).
Step 1. Simulate patient cohort: Generate a random cohort size kj,
where P(kj = k) = ajk, j = 1, . . . , J, k = 0, . . . , K. Simulate responses
xj ∼Bin(kj, πo
j), update nj ≡nj + kj, and augment yj with the new
responses xj to yj ≡yj + xj. Increment t = t + 1.
Step 2. Update posterior inference: Evaluate posterior probabilities
p30,j = P(πj > 30% | y(t)) and posterior moments E(πj | y(t)). The
computation of these posterior summaries requires several posterior in-
tegrations. (A possible implementation is outlined in Algorithm 4.8.)
Step 3. Drop trials: Identify all subpopulations with p30,j < 0.05 and
exclude them from further recruitment.
Step 4. Stopping: If all subpopulations are dropped, then stop the trial
for lack of eﬃcacy in all subpopulations. If the maximum enrollment is
reached, n = P nj > nmax, stop for maximum enrollment. Otherwise
repeat with Step 1.
The evaluation of p30,j in Step 2 requires the evaluation of the marginal
posterior distribution for πj, i.e., marginalization with respect to πh, h ̸= j
and µ, τ. Such posterior integrations can be routinely carried out using
MCMC posterior simulation when they are required once. The challenge
here is that these posterior probabilities are required for all J subpopu-
lations, for each period t of the trial, and for massive repeat simulation
during the evaluation of operating characteristics. We therefore use a fast
approximation based on adaptive bivariate quadrature for the integration
over (µ, log τ) and then exploiting the conditional (posterior) independence

HIERARCHICAL MODELS FOR PHASE II DESIGNS
175
of πj, j = 1, . . . , J, given (µ, τ). The latter reduces the integration with re-
spect to the J-dimensional parameter vector (π1, . . . , πJ) to J univariate
integrations. We use adaptive quadrature for each of these univariate inte-
grations. We use the R package adapt for the bivariate adaptive quadrature,
and the R function integrate for the univariate adaptive quadrature.
Software note: The R package adapt (Lumley and Maecher, 2007) is available
in CRAN at http://cran.r-project.org/src/contrib/Archive/adapt/.
Let mt = E(µ, log(τ) | y(t)), s2
µt = Var(µ | y(t)), s2
τt = Var(τ | y(t))
and st = (sµt, sτt) denote marginal means, variances and standard devia-
tions for (µ, τ). The following algorithm implements posterior integration
to evaluate p30,j and E(πj | y(t)).
Algorithm 4.8 (Numerical posterior integration for p30,j).
Step 1. Marginal posterior for µ, log τ: Find the (marginal) posterior
distribution P(µ, log τ | y(t)) for any (µ, log τ); we use (univariate) nu-
merical integration with respect to πj, j = 1, . . . , J. Let (yj, nj) denote
the number of successes and number of patients in subpopulation j, and
let σ = exp(−.5 log τ). The marginal likelihood is given by
P(yj | µ, τ) =
Z
Bin(yj; nj, πj) N(θj; µ, σ) dθj ,
with πj = 1/(1 + exp(−θj)). We evaluate the integral by numerical
quadrature using the R function integrate(). The desired posterior is
then determined by
P(µ, τ | y(t)) ∝p(µ) p(τ)
J
Y
j=1
p(yj | µ, τ)
(4.12)
Step 2. p30,j, integration with respect to µ, log τ: The posterior prob-
ability p30,j is obtained by another numerical integration. Letting θ30 =
log(0.3/0.7), we have
P(πj > 0.30 | µ, τ, y(t)) =
Z ∞
θ30
Bin(yj; nj, πj) N(θj; µ, σ)
p(yj | µ, τ)
dθj
Recall that θj = log(πj/(1 −πj)). The marginal probability p30,j =
R
P(πj > 0.30 | µ, τ, y(t)) dp(µ, τ | y(t)) is found by bivariate adaptive
quadrature, using (4.12) and the R package adapt.
Step 3. E(πj | y(t)): The marginal expectations are found similarly, using
E(πj | µ, τ, y(t)) =
Z
πj
Bin(yj; nj, πj) N(θj; µ, σ)
p(yj | µ, τ)
dθj
Example 4.8 (Imatinib in sarcoma). We implement the proposed design

176
PHASE II STUDIES
for the example discussed in Thall et al. (2003). They consider a phase II
trial of imatinib in sarcoma. Sarcoma is a very heterogeneous disease. In
this particular trial patients with J = 10 diﬀerent subtypes of sarcoma are
enrolled. The observed outcome is success deﬁned as a reduction in tumor
volume. Let πj denote the unknown success probability for sarcoma type j.
We deﬁne a hierarchical model as in (4.11) with a binomial sampling model,
a normal prior on the logit scale, and a conjugate hyperprior. Following
Thall et al. (2003) we set the hyperprior distributions as µ ∼N(−1.386, 10)
with the mean chosen to match a logit of 0.20, and τ ∼Ga(2, 20) with
E(τ) = 0.10.
Figure 4.7 summarizes a trial simulation under the assumed truth π =
(3, 1, 1, 1, 1, 3, 1, 1, 1, 1)/10. Assumed accrual rates are E(kj) = 5.5 for j =
1, . . . , 5 and E(kj) = 2 for j = 6, . . . , 10. Thus the total sample size is
around nj = 44 for the ﬁrst 5 subtypes and nj = 16 for the last 5, rarer
subtypes.
Panel (a) shows the posterior means E(πj | y(t)) plotted against t. The
two bullets at 0.1 and 0.3 indicate the simulation truths. Note the separa-
tion into two clearly distinguished groups according to the simulation truth.
Note how posterior inference for the rare subtypes borrows strength from
the more prevalent subtypes. Posterior inference for all subtypes quickly
moves away from the prior distribution centered around πj = 0.20. Panel
(b) plots the posterior probabilities p30,j against t. The horizontal dashed
line indicates the cutoﬀat ϵ = 0.05. When a posterior probability p30,j
drops below the cutoﬀthe corresponding subtype is closed for recruitment,
although data from the already accrued patients continues to be used in
posterior computations.
4.6 Decision theoretic designs
4.6.1 Utility functions and their speciﬁcation
Clinical trial design is naturally described as a decision problem (Berry
and Ho, 1988). Studies are usually carried out with a well-deﬁned primary
objective. Describing that objective as choosing an action d to maximize
some utility function speciﬁes a formal decision problem. As already out-
lined in Subsection 1.4.1, a utility function is the opposite of a loss function:
it describes the amount we “gain” (in some suitable units, such as dollars,
patient lives, QALYs, and so on) when we choose action d. The utility
function typically involves unknown quantities, including future data y and
unknown parameters θ. For example, in a phase II study d could be a se-
quential stopping decision, y could be the observed outcomes and θ could
be unknown true success probabilities for a list of possible dose levels. The
utility function u(d, θ, y) could be a combination of the number of success-
fully treated patients, sampling cost for the recruited patients, and a large

DECISION THEORETIC DESIGNS
177
1
2
3
4
5
6
7
8
0.05
0.10
0.15
0.20
0.25
0.30
0.35
MONTH
E(pj | Y)
G
G
G
G
1
2
3
4
5
6
7
8
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
MONTH
E(p30 | Y)
(a) E(πj | y(t))
(b) p30,j
Figure 4.7 Hierarchical model. Panel (a) shows the posterior estimated success
probabilities for J = 10 sarcoma subtypes, plotted against month t = 1, . . . , 6.
The points at t = 6 indicate the simulation truth. Panel (b) shows the posterior
estimated probabilities p30,j.
reward if a subsequent phase III trial concludes with a signiﬁcant result.
Under the expected utility maximization paradigm, the optimal decision is
determined by maximizing the utility u(·), conditional on all data that is
available at the time of decisionmaking, and marginalizing with respect to
all future data y and all unknown parameters θ.
In Subsection 2.5.5 we discussed a particular fully Bayesian method for
incorporation of trial costs via decision theoretic methods. In practice, how-
ever, decision theoretic clinical trial design remains relatively uncommon.
The main reasons for this lack of applications are fundamental concerns
with the decision theoretic setup, and practical diﬃculties in the imple-
mentation. Perhaps the most relevant concern is the diﬃculty of specifying
a good utility function. To start, whose utility is relevant? Assume we are
willing to settle this question by choosing the investigator’s utility. Part of
the investigator’s utility function is the need to satisfy regulatory require-
ments. This naturally adds additional perspectives into the problem. For
example, the utility in a phase II trial might include a large reward for a
signiﬁcant outcome in the following conﬁrmatory trial. Another problem
is that the choice of a utility function and a probability model implicitly
speciﬁes the optimal rule for the trial design. The implicit nature of this
speciﬁcation is sometimes awkward, as it might lead to counterintuitive
rules. For example, consider a dose ﬁnding trial to ﬁnd the optimal dose of a
drug. Assume that the outcome is desirable and the underlying probability
model assumes a nonlinear regression of outcome on dose with a monotone
increasing mean function. Let LD95 denote the dose that achieves 95% of

178
PHASE II STUDIES
the maximum possible beneﬁt over placebo. Assume that the utility func-
tion is posterior precision of the mean response at the LD95. The optimal
rule might allocate all patients at placebo and maximum dose only. This
is because the mean response at the LD95 is a deterministic function of
mean response at placebo and maximum dose. Of course no investigator
would want to use this rule. Another generic problem is the use of ran-
domization. For many good reasons randomization is desirable for trials
that involve multiple treatment arms. Yet, under a strict decision theoretic
framework there is an optimal treatment assignment for each patient, and
thus no good reason for randomization. The issue can be ﬁnessed, but the
basic problem remains that randomization is not natural under a decision
theoretic paradigm.
The other big impediment to wider use of decision theoretic rules is
the often computationally intensive nature of the solution. For example,
consider the adaptive dose allocation problem. The problem is a sequential
decision problem. The expected utility calculation for the dose assignment
for the i-th cohort requires that we know the optimal dose allocation for
the (i + 1)-st cohort, etc. In general such sequential decision problems are
computationally intractable.
However, several strategies exist to mitigate these problems and make
decision theoretic designs feasible. One diﬃculty in the choice of utility
functions is the speciﬁcation of tradeoﬀparameters for competing goals,
for example sampling cost, successful treatment of patients in the trial,
and treatment success for future patients. In general, utility functions of-
ten involve some parameters that are diﬃcult to ﬁx. A common strategy
is to ﬁx these parameters by considering (frequentist) operating charac-
teristics of the implied designs and then choose the utility parameters to
achieve desired operating characteristics. For example, consider a decision
d = (t1, . . . , tn, tn+1) that includes treatment allocation ti(y1, . . . , yi−1),
i = 1, . . . , n for n patients in a clinical trial, and upon conclusion of the
trial a treatment recommendation tn+1(y1, . . . , yn) for a generic (n + 1)-st
future patient. Assume yi is a binary indicator for treatment success. The
utility function could then be u(y, d, θ) = Pn
i=1 yi+λp(yn+1 = 1). Consider
a grid on λ and for each value of λ, compute operating characteristics, such
as Type I error probabilities and power under assumed true scenarios. We
might then ﬁx λ to best match desired operating characteristics.
Another important strategy is the use of decision boundaries to sim-
plify sequential decision problems; we will discuss an example in the next
subsection. Finally, we note that randomization is often introduced by ran-
domizing among a set of near-optimal decisions and similar compromises.

DECISION THEORETIC DESIGNS
179
4.6.2 Screening designs for drug development
Rossell et al. (2007) propose a Bayesian decision theoretic solution to the
drug screening problem. Consider a setup where new treatment options for
a well-deﬁned patient population arise in each period. We index treatments
in the order of appearance, i = 1, 2, . . .. At any given time, a number of
experimental treatments nt are being considered. Let At denote the set of
indices of treatments that are being studied at time t. For each treatment
i ∈At we observe responses yti, and have to decide for which treatments
we should stop recruitment (stopping, dti = 0), and for which we should
continue accrual (continuation, dti = 1). Upon stopping we make a termi-
nal decision of whether the treatment should be abandoned (ai = 0) or
recommended for a conﬁrmatory phase III study (ai = 1). Discontinued
treatments (dti = 0) are removed from the active set At, and new treat-
ments are added to form the new set At+1. We assume a ﬁnite horizon
T. In the last period, T, continuation is not possible and dT i = 0 for all
treatments under study.
Rossell et al. (2007) assume a binomial sampling model for yti. Let Nti
denote the accrual for treatment i in period t, and let θi denote an unknown
success probability for treatment i, so that
yti ∼Bin(Nti, θi)
for all i ∈At. The model is completed with a prior θi ∼Be(u, v) and
independent hyperpriors u ∼Ga(au, bu) v ∼Ga(av, bv), subject to u + v ≤
10.
Before we continue the discussion of the decision theoretic setup, we state
the ﬁnal algorithm. Let (mti, sti) denote the posterior mean E(θi | y) and
standard deviation SD(θi | y) conditional on all data up to time t. We con-
struct decision boundaries for (log sti, mti). The decision boundaries form
a cone in the (log sti, mti) plane; see Figure 4.8. The sequential stopping
condition dit is determined by these boundaries. While inside the cone,
continue dit = 0. Once we cross the boundaries, stop, dit = 1.
We now discuss one possible construction of these decision boundaries.
An alternative approach is outlined in Example 4.9. The construction will
involve the consideration of a follow-up conﬁrmatory phase III trial for
selected treatments (ai = 1). Let τ = min{t : dti = 0} denote the time of
stopping accrual for treatment i. We assume that the conﬁrmatory trial is
set up as a two-arm randomized trial with the same binary success outcome.
Let θ0 denote the assumed known success probability under standard of
care. We assume that a simple z-test is used to test H0 : θi = θ0 versus the
alternative Ha : θi = mti for a given signiﬁcance level α3 and power 1−β3,
where the “3” subscripts here refer to the phase III study. The required

180
PHASE II STUDIES
sample size for the follow-up conﬁrmatory trial is
n3(mτi = m, sτi = s)
= 2
µ
zβ3
√
m(1−m)+θ0(1−θ0)+zα3
√
2m(1−m)
m−θ0
¶2
.
Here m = (mτi + θ0)/2 and zp is the standard normal (1 −p) quantile.
Let B denote the event that the z-statistic falls in the rejection region. We
have
p(B | y) = P(ˆθi −ˆθ0 > zα3/2
p
2m(1 −m)/n3 | y)
where y = (y1, . . . , yτ) and ˆθi and ˆθ0 are the proportions of successes un-
der treatment and control in the z-test. The probability p(B | y) can be
approximated based on a normal approximation of the posterior predictive
distribution for ˆθi −ˆθ0 (Rossell et al., 2007, Section 2.2).
Algorithm 4.9 (Decision-theoretic screening design).
Step 0. Decision boundaries in (log sti, m): Consider two half lines in
the (log s, m) plane that form a cone intersecting at (log s0, b0) and pass-
ing through (log s1, b1) and (log s1, b2), respectively (see Figure 4.8 for
an illustration). Here s0 and s1 are ﬁxed. For example, ﬁx s0 to be a min-
imum standard deviation and s1 to approximately match the marginal
prior standard deviation. The parameters (b0, b1, b2) will be determined
below. The two half lines are decision boundaries that determine dti:
dti =
½
1
if (log(sti), mti) lies between the two lines
0
otherwise
Step 1. Utility parameters. Fix c1 and c2 to reﬂect the relative cost
of recruiting one patient and the reward that is realized if a following
conﬁrmatory trial ends with a signiﬁcant result (e.g., c1 = 1 and c2 =
10, 000). Deﬁne a utility function
u(b, y) = −c1
P
t
P
i∈At Nti
+ P
i:ai=1
h
−c1n3 + c2P(B | y1, . . . , yτi) E(ˆθi −ˆθ0 | B, y1, . . . , yτi)
i
.
(4.13)
For given data y and decision boundaries b, the realized utility u(b, y)
can be evaluated.
Step 2. Forward simulation: Simulate many possible trial realizations,
without stopping, i.e., using dti = 1 for t = 1, . . . , T −1, and save the
realized trajectories {log(sti, mti)}.
Step 3. Optimal decision boundaries: For a grid of (b0, b1, b2) values,
evaluate the average realized utility u(b, y), averaging over all simula-
tions saved in Step 3. Denote the average as U(b). The optimal decision
boundary b⋆is the one with maximum U(b).

DECISION THEORETIC DESIGNS
181
−3.0
−2.5
−2.0
−1.5
0.3
0.4
0.5
0.6
0.7
0.8
20  sample trajectories
log(S)
m
Phase III
Phase III
Phase III
Phase III
Phase III
Phase III
Phase III
Phase III
Phase III
Phase III
Phase III
Figure 4.8 Decision boundaries in the (log s, m) plane. The two half lines are
the decision boundaries intersecting at (log s0, b0). The trajectories show some
forward simulations (log sti, mti), together with the ﬁnal decision ai when the
trajectories cross the decision boundaries (dti = 0).
Step 4. Trial conduct: Use the optimal decision boundary b⋆to decide
continuation versus stopping for all treatments i ∈At at each time
t = 1, . . . , T.
The algorithm is justiﬁed as an approximation of the optimal sequential
stopping rule under the utility function u(·). The nature of the approxima-
tion is that we only allow sequential stopping rules that depend on the full
data indirectly through the summaries (mti, sti).
Software note: Designs of this type are implemented in an R package written
by David Rossell called sesdesphII. The package is available at
http://rosselldavid.googlepages.com/software
.
The program requires only speciﬁcation of the hyperparameters u, v, s0, s1 and
the grids on b0, b1, and b2.
Example 4.9 (Clinical immunology). We summarize an application to de-
signing a screening study for vaccines discussed in Rossell et al. (2007). The

182
PHASE II STUDIES
αmax
βmax
b∗
0
b∗
1
b∗
2
N
α
β
0.05
0.05
0.45
0.64
0.24
31.78
0.04
0.04
0.05
0.10
0.53
0.67
0.37
15.77
0.05
0.10
0.05
0.15
0.57
0.64
0.52
9.96
0.05
0.14
0.10
0.05
0.49
0.51
0.26
17.17
0.09
0.03
0.10
0.10
0.43
0.46
0.37
7.16
0.10
0.10
0.10
0.15
0.38
0.43
0.37
7.05
0.09
0.10
0.15
0.05
0.43
0.46
0.22
13.13
0.15
0.02
0.15
0.10
0.34
0.41
0.31
6.19
0.12
0.09
0.15
0.15
0.34
0.41
0.31
6.19
0.12
0.09
Table 4.6 Summary statistics for clinical immunology example.
utility function is a variation of the one in (4.13), using the average num-
ber of patients needed to recommend one treatment. Letting M = PnT
i=1 ai
denote the number of treatments that are recommended for phase III, the
utility chosen is u(b, y) = 1/M Pt1i
t=t0i
PnT
i=1 Nti. We assume that the suc-
cess probability for the standard of care is θ0 = 0.5, and use a Beta prior
θi ∼Be(0.3188, 0.5327), chosen to match the moments observed in histori-
cal data, E(θi) = 0.3743 and V ar(θi) = 0.1265.
We evaluate designs with M = 1000 simulations on a grid with 20 equally
spaced values of b0 in [0.3, 0.7], b1 in [0.3, 0.8], and b2 in [0.2, 0.6], and use
cohorts of N = 2 patients. After each batch, the posterior moments are
evaluated and the decision to stop is taken as in Step 0 above. For the
terminal decision we use a ﬁxed rule. Upon stopping the enrollment, a
treatment is recommended when stopping was indicated by crossing the
upper boundary, whereas a treatment is abandoned if stopping was indi-
cated by crossing the lower boundary. We then select b to maximize U(b),
the Monte Carlo sample average utility in the forward simulation. The max-
imization is restricted to designs b that satisfy constraints on Type I error
α and power 1 −β. Here α is the fraction of treatments with (simulation)
truth θi < θ0 and ai = 1, and β is the fraction of treatments with θi > θ0
and ai = 0. The maximization over U(b) is restricted to designs b with
α ≤αmax and β ≤βmax.
Table 4.6 summarizes results for several choices of αmax and βmax. The
column N ≡U(b) reports the average number of patients necessary to
recommend one treatment.

CASE STUDIES IN PHASE II ADAPTIVE DESIGN
183
4.7 Case studies in phase II adaptive design
In this section we present two recent high-proﬁle trials that employ inno-
vative trial designs using the approaches introduced earlier in this chapter.
Both trials are good examples of the beneﬁts that can be achieved through a
Bayesian adaptive approach. While we are not yet able to reveal ﬁnal results
in either case, we do discuss the conditions that encourage the adoption of
an adaptive Bayesian viewpoint, as well as corresponding implementational
details.
4.7.1 The BATTLE trial
The use of adaptive designs has gained much attention lately thanks to its
potential for improving study eﬃciency by reducing sample size, resulting
in higher statistical power in identifying eﬃcacious drugs or important
biomarkers associated with the drug eﬃcacy, and treating more patients
with more eﬀective treatments during the trial. As mentioned earlier, both
the Center for Drug Evaluation and Research (CDER) and the Center for
Biologics Evaluation and Research (CBER) at the U.S. FDA have issued
guidance documents for the use of adaptive methods in clinical trials (see
the Preface of this book for the websites of these two documents).
In this section, we illustrate an adaptive design case study, the Biomarker-
integrated Approaches of Targeted Therapy for Lung Cancer Elimination
(BATTLE) trial (Zhou et al., 2008). The goal of this trial is to evaluate the
eﬃcacy of four targeted treatments in patients with advanced non-small
cell lung cancer. The four treatments to be compared are erlotinib (TX
1), sorafenib (TX 2), vandetanib (TX 3), and the combination of erlotinib
and bexarotene (TX 4). To enroll in the study, all patients are required
to have biopsies to measure a tumor’s biomarker proﬁle. Based on this
proﬁle, patients are assigned to one of the ﬁve marker groups: EGFR mu-
tation/ampliﬁcation (MG 1), K-ras and/or B-raf mutation (MG 2), VEGF
and/or VEGFR expression (MG 3), RXR and/or cyclin D1 expression (MG
4), or no marker group (MG 5) if all markers are negative or there are insuf-
ﬁcient tissues for marker analysis. It is assumed that each treatment may
be more eﬃcacious in patients having a biomarker proﬁle that matches the
agent’s mechanism of action. Therefore, the speciﬁc goal of this trial is to
test the eﬃcacy of these targeted agents, and identify the corresponding
predictive biomarkers. In addition, the trialists aim to take advantage of
the information learned from the interim data, so as to treat more pa-
tients with better therapies during the trial. The primary endpoint is the
8-week disease control rate (DCR), which is deﬁned as the proportion of
patients without disease progression 8 weeks after randomization. We apply
a Bayesian hierarchical model to characterize the DCR, and use response

184
PHASE II STUDIES
adaptive randomization (RAR) to assign more patients into more eﬀective
treatments based on the accumulating trial data.
Hierarchical probit model
The Bayesian probit model (Albert and Chib, 1993) is used to deﬁne the
DCR for each treatment (TX) by marker group (MG). A probit link func-
tion is chosen to model the binary disease control status. A latent contin-
uous variable is introduced to model the hierarchical relationship of the
response data in treatment by marker groups. Let j denote the treatment,
k the marker group, and i the index for the patient running from 1 to njk,
where njk is the total number of patients in TX j and MG k. Let yijk be
the binary disease control random variable (progression-free at 8 weeks),
which takes the value 0 if the patient experiences progression or dies within
eight weeks of the TX, and 1 otherwise. A latent continuous variable zijk
is then introduced to model the DCR; let
yijk =
½
0
if zijk ≤0
1
if zijk > 0
(4.14)
The DCR for TX j and MG k is the probability that the latent vari-
able is greater than 0, deﬁned as γjk = P(yijk = 1) = P(zijk > 0). For
each subgroup, we assume that zijk follows a normal distribution with
mean µjk. Note that because only the sign of zijk matters for determining
yijk, the model can be identiﬁed only up to a multiplicative constant on
zijk. To ensure identiﬁability, we thus set the variance of zijk to 1 (Albert
and Chib, 1993; Johnson and Albert, 2000). For a given marker group, a
N(φj, σ2) hyperprior is imposed on the location parameters µjk of the la-
tent variables. The parameter φj is also assumed normal with mean 0 and
variance τ 2, which allows for the exchange of information across diﬀerent
treatments. This hierarchical model allows borrowing information across
diﬀerent marker groups (k) within a treatment (j). The full hierarchical
model is thus
zijk
∼
N(µjk, 1), for all i, j, and k
µjk
∼
N(φj, σ2), for allj and k
and φj
∼
N(0, τ 2), for all j .
(4.15)
The parameters σ2 and τ 2 control the extent of the borrowing across marker
groups within each treatment and across all treatments, respectively. Our
default conﬁguration is to use a vague speciﬁcation, namely σ2 = τ 2 = 106.
The posterior distributions can be routinely computed via Gibbs sampling
from the full conditional distributions given below.
Denote Lijk as the likelihood for patient i in marker group k receiving
TX j. Assuming independence across patients, the overall likelihood is the

CASE STUDIES IN PHASE II ADAPTIVE DESIGN
185
product L = Q
j
Q
k
Qnjk
i=1 Lijk, where Lijk is
¡
Pr(zijk ≥0|µjk, φj, σ2, τ 2)
¢I(yijk=1) ¡
Pr(zijk < 0|µjk, φj, σ2, τ 2)
¢I(yijk=0)
=
©R ∞
0
f(zijk|µjk, 1)
ªI(yijk=1) nR 0
−∞f(zijk|µjk, 1)
oI(yijk=0)
.
The full conditional distributions for the µjk and φk are normal thanks
to our use of conjugate priors. The latent variables zijk follow truncated
normal distributions. The distributions required for the Gibbs sampler are
thus
zijk | yijk, µjk
∝
½ N(µjk, 1)I(−∞, 0)
if yijk = 0
N(µjk, 1)I(0, ∞)
if yijk = 1
µjk | zijk, φj
∝
N(Pnjk
i=1 zijk + φj/σ2, 1/(njk + 1/σ2))
and φj | µjk
∝
N(P5
k=1 µjk, 1/(5 + 1/τ 2)) .
Note that a truncated normal can be straightforwardly sampled by simple
rejection, or via a more eﬃcient one-for-one sampling algorithm using only
the normal cdf and inverse cdf; see Carlin and Louis (2009, p.362)
Adaptive randomization
A response adaptive randomization (AR) is proposed for the BATTLE trial.
Because patients with certain biomarker proﬁles may respond diﬀerently
to diﬀerent treatments, the biomarker proﬁle must be taken into consid-
eration when assigning patients into various treatments. For example, if
patients with EGFR mutation are more likely to respond to erlotinib than
other treatments, it is desirable to assign more such patients to erlotinib.
Because the true DCR for each of the treatment (TX) by marker group
(MG) combinations is unknown when the trial begins, we apply equal ran-
domization (ER) in the ﬁrst part of the trial until at least one patient
with a known disease control status is enrolled in each TX by MG. Sub-
sequent to this, patients are adaptively randomized. Under the Bayesian
probit model described above, all accumulative data are used in computing
the posterior DCR, and thus in determining the randomization ratio. The
randomization rate is computed based on the estimated posterior mean of
the DCR of each TX in each MG.
The randomization ratio for a patient in MG k to receive TX j is taken
as proportional to the estimated mean DCR in that subgroup. That is,
ˆγjk/
X
w∈Ω
ˆγwk ,
(4.16)
where ˆγ corresponds to the posterior mean of the DCR, and Ωindicates
the subset of all eligible and non-suspended treatments for that patient at
the time of randomization. Note that another commonly used alternative is
to randomize patient in MG k to TX j with probability Pr(γjk > γj′k, j ∈

186
PHASE II STUDIES
{1, 2, 3, 4}, j
′ ̸= j). To ensure a certain minimal probability of randomiza-
tion for each non-suspended treatment, if the estimated DCR is less than
10%, 10% is used as the randomization percentage for this treatment. AR
is carried out until the last patient is enrolled, unless all four treatments
have been suspended due to futility. R code was developed to facilitate the
visualization of the dynamic nature of AR.
Interim and ﬁnal decision rules
We also add an early futility stopping rule to the trial design. If the cur-
rent data indicate that a treatment is unlikely to be beneﬁcial to patients
in certain marker groups, randomization to that treatment is suspended.
Speciﬁcally, denote the target DCR by θ1 and the critical probability for
early stopping (i.e., suspension of randomization due to futility) by δL.
The trial will be suspended for TX j and MG k if the probability that the
estimated DCR is at least θ1 is less than or equal to δL, i.e.,
Pr(γjk ≥θ1|Data) ≤δL .
(4.17)
We choose θ1 = 0.5 and δL = 0.1. The stopping rule will be applied after
AR begins.
Next, let θ0 and δU be the DCR for standard treatment and the critical
probability for declaring an eﬀective treatment, respectively. The treatment
will be considered a success at the end of a trial if the probability that the
estimated DCR being at least θ0 is greater than δU, i.e.,
Pr(γjk ≥θ0|Data) > δU .
(4.18)
In this study, we set θ0 = 0.3 and δU=0.8.
Finally, note that the trial design has no early stopping rule for eﬀective
treatments. If a treatment shows early signs of eﬃcacy, more patients will
continue to be enrolled to that treatment under the AR scheme, and the
declaration of eﬃcacy will occur at the end of the trial.
Operating characteristics
As usual, we evaluate the operating characteristics of the proposed trial
design through simulation. In this phase II trial with four treatments, ﬁve
marker groups, and a limited sample size of 200 evaluable patients, our
target is to achieve a 20% false positive rate (i.e., when we conclude a non-
eﬀective treatment to be eﬀective), and 80% power (i.e., concluding the
eﬀective treatments eﬀective). A higher false positive rate than 0.10 (com-
monly accepted in phase II trials) is allowed so that we will not miss any
potentially eﬀective treatments. Once eﬀective treatments are identiﬁed,
they will be conﬁrmed by larger studies in the future.
We conduct simulations with 1,000 generated datasets. We assume that
for MG 1, the true DCR for Treatment 1 is 80%, but only 30% for all

CASE STUDIES IN PHASE II ADAPTIVE DESIGN
187
Marker Group
TX
1
2
3
4
5
true disease control rate / observed disease control rate
1
0.80/0.77
0.30/0.22
0.30/0.24
0.30/0.22
0.30/0.20
2
0.30/0.21
0.60/0.54
0.30/0.24
0.30/0.24
0.30/0.18
3
0.30/0.22
0.30/0.22
0.60/0.57
0.30/0.23
0.30/0.19
4
0.30/0.22
0.30/0.23
0.30/0.24
0.60/0.55
0.30/0.19
average sample size (column percentage)
1
13.0 (43.2)
7.6 (19.1)
10.8 (17.9)
9.0 (18.1)
4.6 (22.9)
2
5.4 (17.9)
15.4 (38.8)
11.1 (18.4)
9.3 (18.7)
4.3 (21.4)
3
5.7 (18.9)
7.7 (19.4)
25.9 (43.0)
9.3 (18.7)
4.4 (21.9)
4
5.7 (18.9)
7.6 (19.1)
10.9 (18.1)
20.7 (41.6)
4.5 (22.4)
none
0.3 (1.0)
1.4 (3.5)
1.7 (2.8)
1.5 (3.0)
2.2 (10.9)
total
30.1
39.7
60.3
49.8
20.1
P(declaring an eﬀective TX) / P(suspending an ineﬀective TX)
1
0.95/0.04
0.14/0.56
0.14/0.63
0.12/0.61
0.14/0.57
2
0.13/0.56
0.82/0.12
0.14/0.61
0.14/0.58
0.14/0.61
3
0.17/0.54
0.15/0.58
0.90/0.07
0.14/0.60
0.13/0.57
4
0.14/0.53
0.14/0.56
0.14/0.62
0.86/0.09
0.15/0.57
Table 4.7 Operating characteristics for the BATTLE trial with one eﬀective treat-
ment for Marker Groups 1-4 and no eﬀective treatment for Marker Group 5.
other treatments. We assume that for MG 2, the true DCR for Treatment
2 is 60%, with 30% for all other treatments. Similarly, we assume MG 3
and MG 4 have only one eﬀective treatment, but that there is no eﬀective
treatment in MG 5.
Table 4.7 shows that the crude observed (sample mean) DCRs underes-
timate the true rates due to the AR. The model-based (posterior mean)
DCR estimates (not shown) also underestimate these true rates, but by
uniformly smaller margins.
With AR and an early stopping rule, more patients are randomized into
more eﬀective treatments. The percentage of patients receiving eﬀective
treatments are 43.2% for MG 1, 38.8% for MG 2, 43.0% for MG 3, and
41.6% for MG 4, compared to just 25% under ER. For patients in MG
5, since there is no eﬀective treatment, percentages of patients random-
ized into Treatments 1-4 are about the same, with a 10.9% chance that no
treatments are assigned to patients because all four treatments are ineﬃca-
cious. These patients may of course receive other treatments oﬀ-protocol.
The total number of patients in each MG is estimated from prior data.

188
PHASE II STUDIES
The probabilities of declaring eﬀective treatments are also shown. When
the treatments are eﬃcacious, the powers are 95%, 82%, 90%, and 86% for
TX1/MG1, TX2/MG2, TX3/MG3, and TX4/MG4, respectively. The false
positive rates for declaring ineﬀective treatments eﬀective (not shown in the
table) range between 0.12 to 0.17. The probabilities of suspending treat-
ments are also listed. When the treatments are eﬀective, the probabilities
of suspension are no larger than 0.12 (occurs in TX2/MG2). Conversely,
for ineﬀective treatments, the probabilities of suspension all exceed 0.53
(occurs in TX4/MG1).
Scenarios with diﬀerent informative priors obtained by varying σ2 and τ 2
were also evaluated. Basically, when the treatment eﬀect is more homoge-
neous across treatments or MGs, more borrowing yields better results with
respect to less biased estimates of DCR and more accurate declarations
of eﬀective treatments. On the other hand, when the treatment eﬀects are
heterogeneous, too much borrowing can lead to a higher chance of false pos-
itive conclusions. The amount of borrowing should be carefully calibrated
to ensure good control of false positive rates.
Discussion
In this case study, we illustrated an RAR design under the framework of
a hierarchical Bayes model. Based on simulation studies, we have shown
that with a total of 200 evaluable patients, the trial design has desirable
operating characteristics that select clinically eﬀective agents with a high
probability and ineﬀective agents with a low probability, treat more pa-
tients with eﬀective agents according to their tumor biomarker proﬁles,
and suspend ineﬀective agents from enrollment with a high probability by
applying an early stopping rule. The Bayesian AR design is a smart, novel,
and ethical design. In conjunction with an early stopping rule, it can be
applied to eﬃciently identify eﬀective agents and eliminate ineﬀective ones.
By aligning eﬀective treatments with patients’ biomarker proﬁles, more pa-
tients are treated with eﬀective therapies, and hence, more patients could
reach disease control status. AR design with early stopping is ideally suit-
able for the development of targeted therapy. The proposed trial design
continues to “learn” by updating the posterior distribution and improves
the estimates as the trial progresses. It is a “smart” design that matches
patients with the drugs best suited for them. This trial design presents a
step towards personalized medicine.
However, the success of the response adaptive randomization trial de-
pends on several key factors. First, the time for a patient’s biomarker as-
sessment needs to be relatively short (e.g., in a few days) because the pa-
tient’s treatment assignment depends on the determination of the marker
proﬁle. Second, the time for outcome assessment must be relatively short
as well, so that the decision based on up-to-date data can provide appropri-

CASE STUDIES IN PHASE II ADAPTIVE DESIGN
189
ate guidance for subsequent treatment assignments. Third, the trial accrual
cannot be too fast. If a trial has a fast accrual rate, many patients may have
been enrolled into the trial before the outcome data becomes available to
provide useful information for the adaptive randomization. Therefore, quick
and easily assessable endpoints and slow to moderate accrual rates (rela-
tive to the outcome assessment time) are most suitable for RAR designs.
Lastly, for AR to work, we must have good markers and good treatments.
4.7.2 The I-SPY 2 trial
I-SPY 2 (Barker et al., 2009) is an adaptive phase II clinical trial of
neoadjuvant treatments for women with locally advanced breast cancer;
the reader may see http://vimeo.com/10266694 for the launch of this
trial with a press conference at the National Press Club. The name of the
trial derives from the phrase, “Investigation of Serial studies to Predict
Your Therapeutic Response with Imaging And moLecular analysis”; see
ispy2.org. The trial caught the attention of mainstream media, including
the Wall Street Journal (see the writeup of April 19, 2010) and NBC’s News
4 (WRC-TV, Washington DC). Speakers at the press conference included
Senator Arlen Specter (D-PA) and Congresswoman Jackie Speier (D-CA).
There are good reasons for the hype. The trial is revolutionary, albeit not
for the mathematics; the underlying methods and models are sophisticated,
but not mathematically diﬃcult. The revolutionary aspect is that, similar
to BATTLE, the trial seeks to identify eﬀective drugs and drug combina-
tions for speciﬁc subtypes of the disease. Subtypes are characterized by
combinations of biomarkers, including binary indicators for hormone re-
ceptor status, HER2 status (+/-) and MammaPrint status. The latter is
recorded as a binary indicator for low versus high MammaPrint risk score
(Mook et al., 2007). Combinations and unions of combinations of these
markers deﬁne ten possible subpopulations (using practical relevance and
clinical judgment to narrow down from 255 combinatorially possible sub-
populations). The primary endpoint is pathologic complete response at 6
months (pCR).
Learning occurs as the trial proceeds, and data from all patients is used
to inform inference about any drug and any subpopulation. Also the word
“Your” in the name of the trial suggests another revolutionary aspect. Using
adaptive treatment allocation, patients in each subpopulation are assigned
the treatments that are considered most promising for them. Many of the
elements of this trial are similar to the methods described earlier in this
chapter.

190
PHASE II STUDIES
Sequential stopping
The protocol includes the evaluation of predictive probabilities as described
in Section 4.2. The trial uses a variation of Algorithm 4.2 to decide about
dropping or graduating drugs from the trial. Drugs are dropped when there
is little hope of future success, and graduated and recommended for fur-
ther development when results are promising. Speciﬁcally, for each drug
we compute posterior predictive probabilities of success in a hypothetical
future phase III trial. The future phase III trial is set up to compare the
drug under consideration versus control in a phase III trial with ﬁxed sam-
ple size. If the posterior predictive probability of success in this future trial
falls below a lower threshold for all possible subgroups, then the drug is
dropped from consideration (“defeat”). If on the other hand, the posterior
predictive probability of success in the future phase III trial is beyond an
upper threshold for some subpopulation, then the drug graduates from the
phase II trial and is recommended for a follow-up phase III study (“vic-
tory”). The use of success in a hypothetical future phase III study as a
criterion for selecting drugs in a phase II screening design is similar to the
drug screening trial discussed in Subsection 4.6.2.
Subpopulations
The outcome of I-SPY 2 is a stream of drugs that are recommended for
further development. One of the critical features of I-SPY 2 is that each rec-
ommendation includes an identiﬁcation of an appropriate subpopulation,
characterized by the recorded biomarkers. Such recommendations are facil-
itated by the use of a hierarchical model across all possible subpopulations.
The model is similar to the models described in Section 4.5.
The clinical relevance of this feature cannot be overstated. Despite widely
reported promise of biomarkers for informing all aspects of drug develop-
ment, the practical implementation of developing and validating biomark-
ers has proven extremely challenging. The hierarchical model together with
other features of I-SPY 2 promises to break this “biomarker barrier.”
Adaptive allocation
Patients are allocated to the competing treatment arms using adaptive ran-
domization. In Section 4.4 we described the general paradigm. Let π(z, t)
denote the probability of pCR for a patient characterized by biomarkers z
under treatment t. I-SPY 2 uses adaptive allocation probabilities propor-
tional to
P(π(z, t) > π(z, t′), t′ ̸= t | data),
i.e., the posterior probability of treatment t being optimal for subgroup
z. As usual the randomization is restricted to some minimum allocation
probability for all active treatment arms.

APPENDIX: R MACROS
191
Delayed responses
A potential limitation of I-SPY 2 is the delayed nature of the primary
endpoint pCR at 6 months. The need to wait 6 months for the outcome
would limit the beneﬁts of the adaptive features in the trial design. I-SPY 2
overcomes this limitation by using longitudinal magnetic resonance imaging
(MRI) measurements. Correlating the MRI measurements with the ﬁnal
response allows the investigators to impute the missing ﬁnal response for
patients who were recruited within the last 6 months. One of the important
features of the trial is that this imputation is done with full consideration
of the related uncertainties. Rather than relying on a plug-in estimate,
posterior inference in I-SPY 2 repeatedly imputes the missing outcomes,
thereby correctly adjusting for the uncertainty.
In summary, I-SPY 2 applies an innovative Bayesian design to screen
novel phase II drugs for women with locally advanced breast cancer. The
design allows for rapid identiﬁcation of eﬀective drugs and biomarkers that
characterize the breast cancer subtypes that are most susceptible to the
respective drug.
4.8 Appendix: R Macros
The online supplement to this chapter
www.biostat.umn.edu/~brad/software/BCLM_ch4.html
provides the R code that was used to illustrate the examples of this section.
In the typical case, as in the previous chapter, the R macros are written to
simulate one realization of a hypothetical trial using the proposed design.
The main function in these examples is named sim.trial(.). To compute
operating characteristics one would add an additional loop that repeatedly
calls sim.trial. To monitor an ongoing trial one would instead replace the
simulated data with the actually observed responses, and strip the top-level
loop inside sim.trial, using only one iteration.

CHAPTER 5
Phase III studies
In this chapter, we turn to Bayesian design and analysis of phase III stud-
ies. These are typically randomized controlled multicenter trials on large
patient groups (300-3,000 or more, depending upon the disease/medical
condition studied) aimed at being the deﬁnitive assessment of how eﬀec-
tive the drug is, in comparison with current “gold standard” treatment.
Such trials are often called conﬁrmatory trials.
The approach of the chapter is to focus on what is diﬀerent for a Bayesian
statistician in a conﬁrmatory trial. The development here is not meant as a
general reference for designing or analyzing conﬁrmatory trials, but rather
as a description and exempliﬁcation of the features and challenges for a
Bayesian. Especially early in the chapter, we rely heavily on demonstrating
these features and challenges through a running “example trial” that we
construct for pedagogical purposes. This chapter is not meant as a precise
recipe for doing every possible Bayesian conﬁrmatory trial. In fact, we
believe such an attempt would be counterproductive, both to the Bayesian
approach and to science in general. After all, an important aspect of the
Bayesian approach is its ﬂexibility and synthetic nature. The ability to
create a unique design speciﬁc to the challenges of each problem, guided by
Bayesian principles and philosophy, is one of the strengths of the approach.
Any attempt to describe in overly “cookbook-y” steps how this should be
done is likely to stiﬂe its eﬀectiveness.
5.1 Introduction to conﬁrmatory studies
There are multiple issues that arise in many applications we have seen.
We highlight these and discuss how we deal with and solve these issues in
practice. The regulatory industry is ever changing and so these issues may
become increasingly or decreasingly relevant through time, but despite this
it is unlikely that they will lose relevance any time soon. The issues that
are highlighted in the chapter are not speciﬁc to any particular therapeutic
area, regulatory agency, or type of medical therapy. Section 5.8 oﬀers a full
case study of a conﬁrmatory trial involving a medical device.
Conﬁrmatory studies raise diﬀerent statistical challenges. The most im-

194
PHASE III STUDIES
portant statistical aspect is that the conﬁrmatory study is typically over-
seen and judged by a regulatory agency. This creates predetermined statis-
tical thresholds or “hurdles” that must be met in order for the regulatory
agency to approve the medical therapy for public use. Earlier phase stud-
ies typically have learning goals, such as ﬁnding the largest safe dose or
the minimum eﬀective dose. These trials’ goals are clearly articulated, but
their deﬁnitions of statistically important eﬀects are not regulated by agen-
cies. These “learning” studies typically allow better decisions to be made
in later phases of medical therapy development. The desired result of a
conﬁrmatory trial is the approval of a therapy for public use.
The industry standard has been to design a ﬁxed sample size trial with
a deﬁned analysis on a deﬁned endpoint. The statistical hurdle is to get a
statistically signiﬁcant result at a speciﬁed Type I error level, typically the
0.05 level (could be one- or two-sided). At the conclusion of the study the
data are unblinded and analyzed according to the pre-planned endpoint
and analysis for statistical signiﬁcance at the agreed upon level. The power
of the design is relevant in the acceptance of the design by regulatory
agents, but at the analysis stage, at the completion of the study, the power
is irrelevant. The critical aspect is signiﬁcance at the speciﬁed level, which
makes the Type I error of the design the most important regulatory hurdle.
The exact Type I error of an adaptive Bayesian design can be extremely
diﬃcult, if not impossible, to calculate. The many diﬀerent adaptive aspects
of the design can have implications to the Type I error which individually
are hard to capture. Thus the standard way to measure the Type I error
of the adaptive features is through simulations. In this chapter simulations
are described and presented.
As discussed in Section 1.3, there are many diﬀerent adaptive features
that have and can be employed in conﬁrmatory trials. The ﬁrst and most
common is to adapt the sample size. Despite the early phase trials, the eﬀect
of a treatment, and especially relative to a blinded control, is still uncertain
at the outset of phase III. The ability to adapt the sample size to the results
of the trial creates an appropriate sample size, resulting in a more eﬃcient
trial. This adaptation of sample size includes the appropriate sample size to
determine success of the treatment, but also the ability to determine when
the treatment is unlikely to show trial success and, thus, stop the trial for
futility. Second, some trials may start with multiple treatment arms with
the desire that one of the arms may be dropped during the trial. A decision
may be made between two active treatment arms, whether it be diﬀerent
schedules, doses, regimen, or device styles, as to which is more appropriate
to carry forward in the conﬁrmation of the therapy. Finally, a trial may
start with multiple doses of an experimental agent, where the ﬁrst stage of
the trial determines the dose arm to move forward to a second stage that
undertakes a more traditional comparison to a control arm. This type of
conﬁrmatory trial is referred to as a seamless phase II/III trial.

BAYESIAN ADAPTIVE CONFIRMATORY TRIALS
195
All adaptive designs discussed in this book are prospectively adaptive
designs. By this we mean the design is completely speciﬁed before the start
of the trial. The results of the trial may change the trial’s features, but
these changes are by design, not ad hoc retrospective changes. We do not
consider having a committee of three people choosing a dose to move to
the second stage of the design to be a prospective adaptive design. The
methodology of selecting the dose moving forward is uncertain. Such a
design cannot be simulated and the operating characteristics are unknown.
There are certainly times in which such a design may be reasonable, but
it is not part of our investigation – especially in conﬁrmatory trials. As
described, a regulatory agency is typically the consumer of the results of
a conﬁrmatory trial and the ability to completely deﬁne a design, and to
understand the behavior of the design with its operating characteristics is
critical to the acceptance of adaptive designs.
5.2 Bayesian adaptive conﬁrmatory trials
In this section we describe common issues that arise in a Bayesian conﬁr-
matory design. These typically involve the adaptive features of the design
and their aﬀects on the Type I error. The selection of priors for conﬁrma-
tory trials is rarely an issue, as non- or minimally informative priors are
typically chosen. We return to the prior distributions and their eﬀects in
Section 5.5.
To demonstrate the idea of a conﬁrmatory trial we develop a very simple
example trial, and subsequently build upon it to add adaptive aspects.
The example is simple to allow the adaptive aspects and the eﬀects of these
adaptive features to be clear. We carry the following example through much
of this chapter.
Example 5.1 (Basic conﬁrmatory trial). Suppose we have a one-armed
trial in which each subject is considered a success or failure. The informa-
tion on whether a subject is a success or failure is observed immediately.
This may be the case in a pain study where the endpoint is “two hours
pain free” (in the world of clinical trials this is essentially immediate). Let
the probability a subject is a success be p. Each observation is assumed
independent. Therefore, for n observations the number of successes is
X | p ∼Binomial(n, p)
Assume the regulatory agency agrees that to demonstrate statistical success
of the medical therapy the trial must show p > 0.5. In a hypothesis testing
framework,
H0 : p ≤0.5
HA : p > 0.5.
A ﬁxed sample size trial at the one-sided 0.05-level, with n = 100, would

196
PHASE III STUDIES
result in a successful trial if X ≥59. The probability of X ≥59, assuming
p = 0.5 is 0.0443. A cutoﬀfor success of X ≥58 would result in a one-sided
Type I error of 0.066. For this example we assume the regulatory restriction
is a one-sided Type I error of 0.05. Thus the trial would be considered
successful if X ≥59. Observing 58 successes would be considered a failed
study. In reality this is not always the case as secondary endpoints and
additional information may mean that regulators allow the therapy to be
marketed. For our purposes, we focus on achieving statistical signiﬁcance
with the primary endpoint.
Suppose instead that a Bayesian approach is now used for the primary
analysis. The primary analysis is to conclude statistical success if the pos-
terior probability of HA is greater than 95%. For a Bayesian analysis we
assume a prior distribution of
p ∼Beta(1, 1) ≡Unif(0, 1) ,
resulting in the posterior distribution
p | X ∼Beta(1 + X, 1 + N −X) .
The posterior probabilities of superiority, Pr(p > 0.5|X) for X = 57, 58, 59,
and 60 are 0.918, 0.945, 0.964, and 0.977, respectively. Therefore, the rule
that statistical success is a posterior probability of superiority of at least
0.95 corresponds to a rule that 59 or more successes implies statistical
success.
Consistent with regulatory experience, this Bayesian rule above would
be completely acceptable for the primary analysis. In this case the Type I
error for this Bayesian rule is 0.044, identical to the traditional frequentist
rule, as the rule is eﬀectively the same. Regulatory agencies typically do
not have “rules” around the level of posterior probability that needs to be
achieved, but rather that the Bayesian rules have acceptable frequentist
properties. Therefore, in order to adjust Bayesian analyses for acceptable
frequentist characteristics the “hurdle,” in this case a posterior probability
of 0.95, can be adjusted to have adequate Type I error characteristics.
This idea is demonstrated below. We also note that adjusting the Bayesian
analysis to ﬁt frequentist properties has consequences for the use of prior
distributions and the Bayesian paradigm (see Section 5.5).
5.2.1 Adaptive sample size using posterior probabilities
To enhance the ﬁxed Bayesian design of Example 5.1 we create a Bayesian
adaptive design. Prospectively, we add the following interim analysis rules
to the design. If at n = 50 or n = 75 there is at least a 0.95 posterior
probability of superiority, then statistical success will be claimed. This rule
corresponds to claiming success if X50 ≥31 or X75 ≥45; this is in addition
to the claim of success if X100 ≥59. At any of these thresholds the posterior

BAYESIAN ADAPTIVE CONFIRMATORY TRIALS
197
probability of superiority is at least 0.95. From a frequentist perspective,
the Type I error of this design is 0.0958. The design is simple enough that
exact calculation of the Type I error is possible in this case (assuming
p = 0.5):
1 −
30
X
i=0
min(25+i,44)
X
j=i
min(25+j,58)
X
k=j
Pr(X1 = i) Pr(X2 = j −i) Pr(X3 = k −j),
where X1, X2, and X3 are independent binomial random variables with
sample sizes 50, 25, and 25, respectively, and a probability of success p. An
implementation of the above in R is simple enough that we provide it here,
as well as on the book’s website:
R code
simulateP <- 0.5
answer <- 0
for (i in 0:30){
for (j in i:min(44,25+i)){
for (k in j:min(58,25+j)){
pri <- dbinom(i,50,simulateP)
prj <- dbinom(j-i,25,simulateP)
prk <- dbinom(k-j,25,simulateP)
answer <- answer + pri*prj*prk
}
}
}
> 1-answer
[1] 0.09578662
While this calculation is straightforward, we also provide a simulation of
the design. In almost every Bayesian adaptive trial there is some form of
simulation, and this simple example oﬀers a good opportunity for illustra-
tion.
Example 5.2 (Basic conﬁrmatory trial, continued). The following R code
(again provided both here and online) provides an example function, adapt1,
which simulates the above design. The result of the function is the proba-
bility of statistical success (win), the mean and standard deviation of the
resulting sample size, and the probability of each possible sample size. The
output of this function for one million simulated trial runs is presented
below. The resulting probability of statistical success, 0.09577, is based on
1, 000, 000 simulated trials assuming that the true probability of success is
0.5. Recall that the theoretical value was calculated above at 0.09579.
R code
adapt1 <- function(simulateP,nsims,postcut,hypothesisP,nCuts){
win <- logical(nsims)
ss <- numeric(nsims)

198
PHASE III STUDIES
nInts <- nCuts
for (i in 2:length(nCuts)){
nInts[i] <- nCuts[i] - nCuts[i-1]
}
for (i in 1:nsims){
x <- rbinom(length(nCuts),nInts,simulateP)
x <- c(x[1],x[1]+x[2],x[1]+x[2]+x[3])
ProbSup <- 1 - pbeta(hypothesisP,1+x,1+nCuts-x)
# Probability of Success
win[i] <- any(ProbSup > postcut )
# Sample size
ss[i] <- min(nCuts[ProbSup > postcut],max(nCuts))
}
out <- c(length(win[win])/nsims,mean(ss),
sqrt(var(ss)),table(ss)/nsims)
names(out) <- c(’Pr(win)’,’MeanSS’,’SD SS’,as.character(nCuts))
out
}
## Input these values for the Type I error of Example 5.2.1:
> nsims <- 1000000
> postcut <- 0.95
> hypothesisP <- 0.5
> simulateP <- 0.5
> nCuts <- c(50,75,100)
> out <- adapt1(simulateP,nsims,postcut,hypothesisP,nCuts)
> out
Pr(win)
MeanSS
SD SS
50
75
100
0.095770 96.455625 12.247579
0.059165
0.023445
0.917390
The Type I error of this design would likely be judged too high by regulatory
agencies. A standard remedy for this is to raise the threshold of posterior
probability until the Type I error is less than the “required level,” which
we are assuming is 0.05 in this setting. Table 5.1 provides the Type I error
probability for various posterior thresholds, Pcut.
The posterior probability threshold creates a Bayesian rule for deﬁn-
ing a statistical success. This Bayesian threshold creates the three success
thresholds (at the looks of 50, 75, and 100) that deﬁne success. This situa-
tion creates an integer value problem and so the Type I error probabilities
shown in Table 5.2 have discrete drops. The posterior probability threshold
of Pcut = 0.976 creates a trial in which the Type I error probability, 0.0423,
has been “controlled” at the one-sided 0.05 level. This posterior probability
threshold implies that the values of 33, 47, and 60 successes are needed to
claim success at each of the three looks (n = 50, 75, 100). Table 5.2 shows

BAYESIAN ADAPTIVE CONFIRMATORY TRIALS
199
Pcut
Type I error
0.95
0.0958
0.96
0.0692
0.97
0.0591
0.9725
0.0591
0.975
0.0532
0.976
0.0423
0.9775
0.0347
0.98
0.0347
0.99
0.0195
Table 5.1 The Type I error for diﬀerent posterior probability levels of success
(Pcut) for Example 5.2.1.
p
Pr(Win)
Mean SS
SD SS
Pr(50)
Pr(75)
Pr(100)
0.50
0.0421
98.9
6.9
0.017
0.011
0.972
0.55
0.217
94.7
14.2
0.077
0.058
0.864
0.60
0.578
84.1
21.0
0.237
0.162
0.601
0.65
0.889
69.0
21.1
0.504
0.229
0.266
0.70
0.989
57.0
14.2
0.780
0.160
0.060
0.75
0.999
51.5
6.53
0.944
0.051
0.005
Table 5.2 Operating characteristics of the adaptive design with early stopping for
success for Example 5.2.1.
the operating characteristics of this adaptive Bayesian design for assumed
probabilities that are larger than p = 0.5, and thus represent power cal-
culations. These simulations (using the R function adapt1) are based on
100, 000 simulations for each case.
The operating characteristics of the design demonstrate the Type I error
control of the design in the previous example having Type I error of 0.0421.
The power of the design for a hypothesized value of 0.65 is 0.889. Under
this hypothesized value the mean sample size is 69, with a 0.504 probability
of stopping for success at the 50-subject interim analysis. The larger the
assumed probability of success the smaller the mean sample size. Under the
hypothesis that the probability of success is 0.65 the average sample size of
the adaptive trial would be 69. If a ﬁxed trial of 69 subjects were conducted
the power under the alternative hypothesis of 0.65 would be 0.802. Thus
the ﬁxed trial and the adaptive trial have the identical mean sample size,

200
PHASE III STUDIES
69, yet the adaptive design has a power of 0.889 relative to 0.802 for the
ﬁxed design.
These are common operating characteristics that are created in order
to justify a Bayesian design to a regulatory agency. The critical aspect of
these is the Type I error of the design; the power of the design is typically
not a major concern. Of course, if the design has very poor power then
the regulatory agency would likely deem it unethical to conduct such a
study, as it would be of little scientiﬁc credibility. In our experience, there
is rarely an interaction in which 80% or 90% power – or any other level –
is a regulatory restriction.
Of course, the power of a design is certainly an issue for the trial’s spon-
sor. The amount of risk and the cost beneﬁt of diﬀerent sample sizes and
adaptive features is of paramount concern. The power presented above is a
“frequentist” calculation and is done by conditioning on a speciﬁc value. As
discussed in Section 2.5, a Bayesian form of the power of this design can be
found by integrating over a company-speciﬁc prior distribution. Suppose
the sponsor had early phase data on the medical therapy in which there
were 7 successes in 10 subjects. Using the subjective company-speciﬁc prior
distribution (in this case, a Beta(7, 3)) would result in a predictive proba-
bility of success of 0.807, with a mean sample size of 62.1 (standard devia-
tion of 20.7). The probability of stopping at the ﬁrst look (50 subjects) is
0.734 and 0.049 at the second (75 subjects). These may be contrasted with
the frequentist operating characteristics achieved when conditioning on the
value of p = 0.7, which is the mean of the Beta(7,3) prior distribution.
Frequentists refer to the need for a “penalty” when performing interim
analyses. This is a reference to the idea seen here that by increasing the
number of opportunities for success, the Type I error is increased. Therefore
a penalty (in the form of requiring a more stringent stopping threshold)
must be paid at each analysis. In this example such a penalty has been
implemented by increasing the threshold for the probability of success from
0.95 at the ﬁnal look to 0.976 at each of the three looks. Therefore if the trial
gets to the ﬁnal analysis, the probability of success must be greater than
0.976, rather than 0.95. We ﬁnd the term “penalty” here to be unnecessarily
pejorative; in fact, the Bayesian adaptive design has merely redistributed
the Type I error across several analyses to produce a design that is more
powerful (as seen by the 0.889 > 0.802). The paradigm conﬂict inherent in
tweaking Bayesian designs using frequentist criteria is discussed further in
Section 5.5.
5.2.2 Futility analyses using predictive probabilities
Thus far our trial adaptation has been based only on success. Just as im-
portant is stopping for failure, or what is often termed futility. At each of
the interim analysis points, 50 and 75, we could stop the trial when the

BAYESIAN ADAPTIVE CONFIRMATORY TRIALS
201
likelihood of trial success is small. Stopping for success is based on reaching
a threshold of posterior probability, which is the agreed upon regulatory
hurdle for statistical success.
Stopping for futility based on posterior probabilities at the current look is
awkward. If the posterior probability of superiority is 0.05, 0.50, or 0.75 at
the interim analysis this provides little relevant information about whether
the trial is ultimately going to be successful. We utilize predictive probabil-
ities to address the likelihood that the ﬁnal statistical hurdle will be met.
Predictive probabilities are incredibly important tools in Bayesian adaptive
designs. The following subsection introduces futility analysis using predic-
tive probabilities.
Example 5.3 (Basic conﬁrmatory trial, re-continued). Continuing on with
our running example, suppose our goal is to create rules for stopping the
trial for futility at the 50- and 75-subject look, or at any other time point
of interest. In order to stop at the 50-subject look we are interested in the
probability that the trial will be successful if it continues accruing sub-
jects. For example, at the 50-subject look, suppose there are 25 successes.
The posterior probability of superiority is 0.50. This quantity provides lit-
tle information about the likelihood that the trial will result in statistical
success. A big diﬀerence between earlier phase trials and conﬁrmatory tri-
als are the strict hurdles for success. The structure of conﬁrmatory trials
is that the hurdle for statistical success, whether based on a p-value or
Bayesian posterior probability, is the most important metric and provides
a clear measure of success.
Conditional on observing 25 successes and 25 failures at the 50-subject
look, the current posterior distribution for p is a Beta(α = 26, β = 26).
The predictive distribution for the next n subjects is a beta-binomial dis-
tribution, having a probability density function for generic α and β of
f(x) =
Γ(α + β)Γ(n + 1)Γ(x + α)Γ(n −x + α)
Γ(α)Γ(β)Γ(x + 1)Γ(n −x + 1)Γ(n + α + β) .
Figure 5.1 shows this predictive distribution of the next 25 and the next 50
observations, based on the currently observed values of 25 successes in 50
subjects and the Beta(1,1) prior distribution. The trial is deﬁned so that
statistical success occurs if 47 of the ﬁrst 75 or 60 of the 100 total subjects
are successes. These regions are shown on the predictive distributions using
thicker line segments. The predictive probability of success at the n = 75
look is 0.00078, and is 0.0256 for the ﬁnal analysis at n = 100.
As part of our adaptive design, we can create stopping rules for futility
based on these predictive probabilities of success – typically the predictive
probability of success at the ﬁnal analysis. In this simple design, calculating
the predictive probability of success at any time was not diﬃcult, but in
more complicated designs it can be quite diﬃcult (because of the additional
adaptive aspects which may themselves require predictive probabilities).

202
PHASE III STUDIES
25
30
35
40
45
50
0.00
0.10
Predictive Distribution for Success at n=75
Successes at n=75
Probability
30
40
50
60
70
0.00
0.06
Predictive Distribution for Success at n=100
Successes at n=100
Probability
Figure 5.1 The predictive distribution of the number of successes at the 75 and
100 subject looks for the trial in Subsection 5.2.2.
Suppose we enhance the design by adding in a futility rule that if the
probability of success at the ﬁnal analysis is less than 0.05 then the trial
will stop for futility. In such a design, the result of 25 successes and 25
failures at the 50-subject look would result in stopping the trial for futility.
Despite the posterior probability of superiority still being moderate, the
probability that the trial will ultimately result in a statistical success is
remote.
Adding futility rules to a design does not increase the probability of
a Type I error, it usually decreases it. Table 5.3 presents the operating
characteristics of the adaptive design with an added rule that if at the
50- or 75-subject looks there is less than a 0.05 predictive probability of
success, then the trial stops for futility. The table presents the probability
of stopping for each look for success (upper number of the pair provided)
and for futility (lower number). The futility rule has little impact on the
probability of concluding success or failure; rather, it simply makes the
same decision sooner. The mean sample sizes in the statistically easier
scenarios are small, with mean sample sizes of 64.3 and 51.5 in the two
extreme scenarios. In the more statistically challenging cases, such as p =
0.55 and p = 0.60, the mean sample sizes are the largest, with means of
74.1 and 76.1, respectively. In the case where p = 0.60 the power is 0.569,

BAYESIAN ADAPTIVE CONFIRMATORY TRIALS
203
p
Pr(Win)
Mean SS
SD SS
Pr(50)
Pr(75)
Pr(100)
0.50
0.0407
64.3
18.2
0.016
0.555
0.011
0.275
0.014
0.129
0.55
0.215
74.1
20.7
0.078
0.283
0.059
0.253
0.078
0.248
0.60
0.569
76.1
21.1
0.238
0.099
0.161
0.122
0.170
0.210
0.65
0.882
67.3
20.1
0.506
0.021
0.227
0.028
0.148
0.069
0.70
0.987
56.8
13.9
0.782
0.003
0.158
0.003
0.048
0.008
0.75
0.999
51.5
6.4
0.945
0.000
0.050
0.000
0.005
0.000
Table 5.3 Operating characteristics of the adaptive design in Subsection 5.2.2 with
early stopping for success and futility. The two numbers in each of the last three
rows represent the probability of stopping for success (top) and futility (bottom).
which compares to 0.578 without the futility rule. In the columns (50 and
75) where early futility is demonstrated (with probability 0.099 and 0.122,
respectively), these are not necessarily “errors.” They are indeed incorrect
conclusions from the study, but they are almost always the same decision
that would have been made had the study continued to the full sample
size. These are circumstances where the early poor performance (due to
randomness) caused the trial to stop earlier because the ﬁnal conclusion of
“fail” was inevitable.
Stopping a trial for futility does not imply that the evidence is conclusive,
that the medical therapy is detrimental, or that it is conclusively inferior.
It is only conclusive that the statistical hurdle is very unlikely to be met.
If there are 41 successes and 34 failures at the 75-subject look then the
futility rule stops the trial. (If the maximum sample size were larger, this
futility rule would be diﬀerent; if the maximum were 150 then 38 or fewer
successes among the ﬁrst 75 would lead to stopping for futility.) The pre-
dictive probability rule is driven by the current trial and the conclusions
drawn in the current trial — not by some measure of learning or inference.

204
PHASE III STUDIES
5.2.3 Handling delayed outcomes
One of the ways in which Example 5.3 is simple is its notion that the in-
formation on all of the subjects is known immediately. During an interim
analysis point (say, at the 50-subject look), it is assumed that the informa-
tion on all 50 subjects is known. But in many trials, the time to information
for these subjects is such that there is a lag between the number of sub-
jects in the trial and the information known at the time of the analysis. In
this section we discuss adaptive sample sizes in the case where the analyses
taking place have incomplete information on the subjects in the trial.
Suppose in Example 5.3 that each subject is labeled a success or failure
based on a delayed outcome, such as a one-month visit for success. We
assume for now that this measure of success is a medical test that is con-
ducted one month after treatment, such that there is no information gained
earlier than one month. We discuss in Subsection 5.4 the ubiquitous prob-
lem of handling longer-term outcomes when there is, possibly, information
gained earlier than the ﬁnal endpoint.
We extend Example 5.3 in the following way. Assume that the result for
each subject is observed one month after treatment. The accrual rate of
subjects then becomes an important part of the design itself. If accrual is
slow relative to the one-month outcome (say, 1 subject per month) then
this lag-time in outcome is essentially irrelevant, as information on almost
all subjects is known at the time of an interim analysis. If there are 100
subjects accrued in a month then this lag-time in outcome would make
adaptation worthless, as no information would be gained soon enough for
any informed adaptations to a 100-subject trial. For this example we assume
an accrual rate of 20 subjects per month. Then at any particular look at the
data, there will be a lag of 20 subjects with no information. We still wish
to look when 50 and 75 subjects have been enrolled, for the possibility of a
smaller sample size. These looks at the data are statistically diﬀerent than
those in the previous trials of this chapter, in which we assumed complete
information was available at each look. If the trial is stopped at the 50-
subject look for (expected) success based on the 30 subjects with complete
information, it may be that the data on the remaining 20 that will be
collected will be negative and thus the result of the 50-subject trial will be
failure.
Consistent with the Bayesian approach we employ predictive probabil-
ities to determine when stopping is appropriate; that is, the likelihood of
success for the trial is high when the “lag” of 20 subjects is observed. We
consider the following design:
Algorithm 5.1 (Adaptive design with delayed outcomes).
Step 1: When 50 subjects are enrolled, a predictive probability of trial
success for the current 50 is calculated; if greater than 0.90, the trial
is stopped for expected success. If the predictive probability of trial

BAYESIAN ADAPTIVE CONFIRMATORY TRIALS
205
success for the maximum number of subjects, 100, is 0.05 or less, the
trial is stopped for futility.
Step 2: When 75 subjects are enrolled, a predictive probability of trial
success for the current 75 is calculated; if greater than 0.90, the trial
is stopped for expected success. If the predictive probability of trial
success for the maximum number of subjects, 100, is 0.05 or less, the
trial is stopped for futility.
Step 3: The ﬁnal analysis is conducted when the trial is complete; that
is, when all subjects have complete information. The prior distribution
assumed for p at this point is a Beta(1,1). If the posterior probability of
superiority at the ﬁnal analysis is at least Pcut, then statistical success
is claimed.
Example 5.4 (Basic conﬁrmatory trial with delayed outcomes). Recall
from Table 5.1 that when there was complete information on each sub-
ject, the threshold, Pcut, was set to 0.976. In this analysis the same number
of interim looks are employed, but these interim looks are diﬀerent. The
action that is taken by these looks is to stop accrual, but there is still a rea-
sonable amount of data that must be collected. Therefore these looks have
a diﬀerent implication for the Type I error. This is a common phenomenon
with the advent of new adaptive designs, where the adaptive actions taken
have unclear and unknown eﬀects on the Type I error.
In this example we determine the eﬀect by simulation. By simulating this
design under the null hypothesis that p = 0.5, we ﬁnd the eﬀect that all the
adaptive features combined have on the Type I error and then adjust the
statistical hurdle Pcut appropriately to control the Type I error. Table 5.4
shows the Type I error for this design for diﬀerent values of Pcut based on
100, 000 simulated trials.
A very important part of adaptive designs such as this is the accrual rate.
We have made the assumption in each of these simulations that the accrual
rate is 20 per month. If the accrual were 100 per month then the trial would
go to the maximum of 100 each time and the appropriate threshold would
be Pcut = 0.95. If accrual were slow enough that all subjects’ data were
known then the interim analyses would be perfect information looks, as in
the previous example, thus the appropriate cutoﬀwould be Pcut = 0.96.
We typically provide a range of accrual rates within the realm of possible
values. Table 5.4 shows the probability of a Type I error for accrual rates of
10, 20, and 30 per month. Based on this range of accrual rates the threshold
of Pcut = 0.96 is selected for the trial.
The operating characteristics for this trial, based on an accrual rate of
20 per month, are shown in Table 5.5. Relative to the values for all p,
the chance of early stopping at the 50-subject look is reduced relative to
Table 5.3, due to the loss of information. R code to evaluate the operating

206
PHASE III STUDIES
Pr(Win)
Pcut
10/month
20/month
30/month
0.95
0.0551
0.0521
0.0495
0.955
0.0518
0.0471
0.0453
0.96
0.0489
0.0454
0.0452
0.97
0.0372
0.0321
0.0311
0.976
0.0316
0.0288
0.0280
0.98
0.0217
0.0195
0.0184
0.99
0.0114
0.0110
0.0104
Table 5.4 The Type I error from diﬀerent posterior probability levels of success
and diﬀerent monthly accrual rates in Subsection 5.2.3.
p
Pr(Win)
Mean SS
SD SS
Pr(50)
Pr(75)
Pr(100)
0.50
0.0454
76.3
20.2
0.005
0.296
0.008
0.337
0.032
0.322
0.55
0.235
85.1
18.9
0.025
0.141
0.047
0.214
0.163
0.409
0.60
0.602
86.7
18.3
0.086
0.056
0.158
0.088
0.358
0.254
0.65
0.891
79.5
19.7
0.218
0.019
0.322
0.023
0.352
0.066
0.70
0.986
68.5
18.4
0.430
0.005
0.387
0.003
0.169
0.005
0.75
0.999
59.1
13.8
0.673
0.001
0.288
0.000
0.037
0.000
Table 5.5 Operating characteristics of the adaptive design in Subsection 5.2.3
with a lag-time in information. The two numbers in each of the last three rows
represent the probability of stopping for and achieving success (top) and failure
(bottom).

BAYESIAN ADAPTIVE CONFIRMATORY TRIALS
207
characteristics of this design is described in the appendix to this chapter,
and provided on the book’s website.
The adaptive looks in this trial are diﬀerent from traditional group se-
quential methods because these looks do not result in a claim of success.
The looks set the appropriate sample size based on the accruing informa-
tion. The selection of the cutoﬀs for stopping for the appropriate sample
size are typically sponsor- or designer-speciﬁed, based on the various costs
and utilities of the outcomes.
There are really two diﬀerent uses of “priors” in Algorithm 5.1. The ﬁrst
is for the ﬁnal analysis at the conclusion of the study, where a Beta(1,1)
prior is selected. We refer to this prior distribution as the regulatory prior;
it is analogous to a skeptical analysis prior in Section 2.5. This is the prior
that is important to regulatory agents and which dictates what prior is
used for the ﬁnal primary analysis. In Section 5.5 we discuss further the
idea of using subjective priors for the regulatory prior. Barring extenuat-
ing circumstances, such as relevant historical or clinical information, the
regulatory prior is ﬂat or non-informative. The regulatory prior is rarely
controversial or in dispute.
The second use of a prior distribution here is the prior that is used
in the calculation of the predictive distribution. There is no reason, either
foundationally Bayesian or regulatory, that these two distributions must be
the same. Indeed, their roles are quite diﬀerent in the trial. The regulatory
prior is clear, but the prior used for the predictive distribution carries
the risk of the sponsor in selecting the appropriate sample size. In many
applications this prior distribution is based on either historical or sponsor-
related information, similar to the design prior in Section 2.5. This is not
unlike a ﬁxed sample size in a classically designed trial that is typically
based on the sponsor’s beliefs about the relative eﬃcacy. In an adaptive trial
like this, the sponsor information is updated based on accruing information
and the ability to select a more appropriate sample size.
In some circumstances, selection of a noninformative prior for the pur-
poses of sample size selection can lead to poor performance. For example,
the “bathtub-shaped” Beta(0.01,0.01) prior is considered non-informative,
and is arguably less informative than our standard Beta(1,1). If a predic-
tive distribution is created from a Beta(0.01,0.01) prior when there are
either no successes or no failures, this will result in a predictive distribu-
tion that is incredibly heavily centered on future results being all failures
or all successes, respectively. This predictive distribution can lead to hasty
decisions regarding expected success or futility. Therefore, careful consid-
eration should be given to the prior distribution used in the sample size
selection. When the trial is complete, the regulatory prior is used in the ﬁ-
nal analysis, and the prior distribution used in the design stage is no longer
relevant.

208
PHASE III STUDIES
p
Pr(Win)
Mean SS
SD SS
Pr(50)
Pr(75)
Pr(100)
0.50
0.0485
81.3
19.3
0.011
0.194
0.006
0.333
0.032
0.424
0.55
0.241
87.7
18.2
0.048
0.093
0.035
0.174
0.158
0.492
0.60
0.606
85.6
20.1
0.150
0.050
0.115
0.062
0.342
0.281
0.65
0.886
76.2
21.9
0.332
0.028
0.217
0.015
0.337
0.070
0.70
0.980
64.3
18.9
0.577
0.012
0.247
0.002
0.157
0.006
0.75
0.997
55.7
12.3
0.802
0.002
0.162
0.000
0.034
0.000
Table 5.6 Operating characteristics of the adaptive design with a lag-time in in-
formation, Beta(8, 2) design prior. The two numbers in each of the last three rows
represent the probability of stopping for and achieving success (top) and failure
(bottom).
Table 5.6 presents the operating characteristics for the previous example
when the prior distribution used for the sample size selection is a Beta(8, 2),
rather than the Beta(1, 1); the regulatory prior remains a Beta(1, 1). In this
example the same cutoﬀvalue of 0.96 is used, resulting in a Type I error
of no more than 0.05. The eﬀect of the Beta(8, 2) design prior is that the
trial stops sooner for expected success, but takes longer to stop for futility.
There is little overall impact to the power or Type I error, but the total
sample size is aﬀected.
5.3 Arm dropping
There are many diﬀerent adaptive features that can happen within a con-
ﬁrmatory trial. The ability to construct a design that ﬁts the needs of
regulatory agents, the sponsor, and the subjects within the trial is the
essence of an adaptive design. In this section we add an additional exper-
imental medical therapy arm to the trial. The goal is to construct a trial
that learns which of the two therapies is better and then continues the trial
with only the better of the two. In this scenario the sponsor prefers one
of the treatments, and thus will select the preferable one if the results are
similar. This could be the situation if one dose has a more diﬃcult dosing

ARM DROPPING
209
schedule, more expensive dosing, or a less risky future safety proﬁle. The
assumptions made in Subsection 5.2.3 are retained. For example, the as-
sumption that there is a lag-time of one month to observe the response is
assumed in this discussion as well.
Algorithm 5.2 (Adaptive design with arm dropping). Label the therapies
as Treatments A and B, respectively. The trial starts with both experimen-
tal medical therapy arms. An analysis of the data will be made when there
are 50 subjects accrued in each arm, 100 total. At this ﬁrst analysis one of
the two treatment arms will be dropped. The following rules are used to
determine which arm is dropped:
Step 1: If either treatment has a 0.05 or smaller predicted probability of
success at the maximum sample size of 100, then that arm is dropped
for futility.
Step 2: If there are two arms remaining in the trial then Treatment B is
dropped unless Treatment B has a predictive probability of trial success
(by the maximum sample size) at least 0.10 larger than Treatment A,
in which case Treatment A would be dropped. This unbalanced rule is
chosen because Treatment A is the regimen that is preferred unless the
other has a reasonably larger probability of success.
Step 3: The single remaining treatment arm will continue to accrue until
there are 75 subjects enrolled. At this analysis the trial will stop for
expected success if the predictive probability of trial success with the
current sample size is at least 0.90. The trial stops for futility if the
probability of trial success at the maximum 100-subject look is less than
0.05.
Step 4: If the trial does not stop for futility then ﬁnal data on each sub-
ject are observed and the ﬁnal statistical analysis is conducted. The ﬁnal
statistical analysis is based on the posterior probability that the remain-
ing treatment has a better than 0.50 posterior probability of having a
success rate larger than 0.50. If this posterior probability is larger than
Pcut then statistical success will be claimed.
Example 5.5 (Basic conﬁrmatory trial with arm dropping). The design
above has an adaptive sample size, and the adaptive arm dropping has
the potential of increasing the Type I error. The trial usually selects the
better performing of the two treatments after 50 subjects are enrolled in
each arm. This creates a possible “cherry-picking” eﬀect and a possible
increase to the Type I error. As in the previous examples, the value of
Pcut is selected in order to preserve a Type I error restriction of 0.05.
Table 5.7 provides the operating characteristics for various selections of
Pcut (100,000 simulations). The probability each treatment is selected as
the target treatment is reported in the “Pick” columns, and the probability

210
PHASE III STUDIES
Pcut
Pr(Win)
Pick A
Win A
Pick B
Win B
0.95
0.0773
0.643
0.042
0.357
0.035
0.96
0.0730
0.644
0.038
0.356
0.035
0.97
0.0520
0.696
0.028
0.304
0.024
0.975
0.0463
0.699
0.025
0.301
0.021
0.98
0.0321
0.701
0.017
0.299
0.015
0.99
0.0188
0.701
0.010
0.299
0.009
Table 5.7 Operating characteristics of the adaptive design with two experimental
therapies from Section 5.3.
pA
pB
Pr(Win)
Pick A
Win A
Pick B
Win B
Mean SS
0.50
0.50
0.0463
0.699
0.025
0.301
0.021
131.4
0.50
0.70
0.9099
0.091
0.011
0.909
0.899
131.8
0.70
0.50
0.9539
0.966
0.950
0.034
0.004
131.6
0.70
0.70
0.9939
0.718
0.713
0.282
0.281
128.7
Table 5.8 Operating characteristics of the adaptive design with two experimental
therapies from Section 5.3.
each treatment is shown to have a success rate greater than 0.50 is reported
in the “Win” columns.
In order to satisfy regulatory constraints, a cutoﬀvalue of 0.975 could
be selected for the primary endpoint. In this design there are multiple
treatment arms that start the trial and information about the two arms is
used in order to select the preferred arm. Then adaptive sample size aspects
are used to select the appropriate sample size. This is both a “learn” trial
and a “conﬁrm” trial. As far as the operating characteristics are concerned
it is a “conﬁrm” trial. All aspects of the trial are simulated and thus the
Type I error probability is controlled.
Table 5.8 presents the operating characteristics of this design using a
cutoﬀof Pcut = 0.975 and 100,000 simulations. Note that the decision rule
around the two treatments favors Therapy A. Therefore, the second and
third scenarios in Table 5.8 are not symmetric. This design allows an initial
investigation of the two arms before selecting the treatment to be used in
the single-arm stage of the design. This demonstrates a so-called “Type
III error;” that is, the error of omitting a therapy. In the previous section
we investigated a single arm in a similar adaptive design. When the true
success rate is 0.70 in Table 5.5, the probability of statistical success is

MODELING AND PREDICTION
211
0.986. It may be that there are two possible therapies and one of them is a
“gold nugget,” with a success rate of 0.70 and the other is ineﬀective, with
success rate 0.50. If one of them is selected and the single-arm adaptive
design is conducted then there is a 0.986 probability of a statistical success
if the gold nugget therapy is selected. But, if the wrong therapy is selected
then there is a 0.0454 probability of statistical success (and correctly so).
In this example, where two arms are started and a decision is made after
observing the results of 30 subjects, there is at least a 0.90 probability of
the correct arm being selected through the empirical decision. The power
is smaller than the 0.986, but the 0.986 ignores the Type III error, which
is selecting the wrong therapy entirely.
This was a very simple example in which there is a “learn” component to
the conﬁrmatory trial: the initial stage of the trial, where 50 subjects are
enrolled in each arm before a decision is made as to which arm to continue
to the conﬁrmatory stage. All of the data on the selected arm are used in
the conﬁrmatory analysis. This is a simple example of a seamless phase
II/III trial (see Section 5.7). The idea readily extends to additional arms
and adaptive features in the learning stage of the trial.
5.4 Modeling and prediction
In Example 5.4, there was a lag in the receipt of information for subjects
who had been accrued but had not yet had a ﬁnal outcome reported. This
is standard in clinical trials, as scenarios are rare in which immediate in-
formation about a subject’s response is known. There are circumstances
where the accrual is slow enough (say, 2-3 subjects accrued per month)
that even a month delay in the response can be reasonably ignored.
A more common scenario is the one where a subject has earlier infor-
mation observed that is informative about the ﬁnal primary outcome, but
is not itself the primary outcome. This information can come from early
observations on an endpoint of interest, or from a diﬀerent variable that
is possibly informative about the primary endpoint. For example, a trial
in spinal implant devices may record clinical success at 24 months after
implantation. The observations of clinical success at 3, 6, and 12 months
are typically highly correlated with the 24-month outcome, and thus these
early observations can be critical to an eﬃcient adaptive design. In diabetes,
a primary endpoint in a conﬁrmatory trial may be the 12-month change
from baseline in HbA1c, a clinical measurement thought to be reﬂective of
the subject’s diet and medication behavior in recent weeks. Fasting blood
glucose is a more immediate measure of possible drug eﬃcacy. Regulatory
agencies may not accept the 6-month outcome of a spinal implant study
to be used in place of or as a surrogate for 24-month outcomes, nor do
regulatory agents accept fasting blood glucose as a primary endpoint. De-

212
PHASE III STUDIES
spite this, these variables can be utilized in an adaptive design to shape
the adaptive decisions of the trial.
These early measures are referred to as auxiliary variables. Their role
in adaptive designs is to inform the methodology and models used in the
adaptive aspects of the design. The challenge in an adaptive design is to
learn as much as possible about the treatments in order to make the most
eﬃcient adaptive decisions as possible. The auxiliary variables help in this
learning process.
These variables are not “surrogate” markers in these trials, nor do we use
the information on the auxiliary variable directly to shape the trial. Instead
any correlation between the auxiliary variable and the primary outcome is
harnessed. Using auxiliary variables involves creating a model for the rela-
tionship between the early endpoint and the ﬁnal primary endpoint. This
model is not a static model, but is informed by subjects that have early
auxiliary variable observations and ﬁnal primary endpoints. We create sta-
tistical models for this relationship, and these models will have parameters
that are updated by the accruing information in the trial. Typically this
model is selected to be ﬂexible, yet as information on the relationship ac-
crues, the models become critical to the adaptive decisions.
Utilizing these models allows for the predictive distribution of the ﬁnal
primary outcome to be calculated. Typically this predictive distribution
is calculated using Bayesian multiple imputation. We demonstrate this ap-
proach by yet another extension of the setting running through this chapter.
Suppose the primary outcome is observed when a subject reaches one
month. We now extend the example so that each subject observes an early
reading of success or failure at the one-week visit. Let X be the primary
observation at one month, and let Y be a dichotomous observation observed
at one week. Here we assume that Y is an early indication of success (Y = 1)
or failure (Y = 0), but it could be a diﬀerent endpoint entirely, or any other
dichotomous outcome observed at one week. We model the probability that
a subject is a primary success, given an intermediate success (or failure)
by deﬁning
γ0 = Pr(X = 1|Y = 0)
and
γ1 = Pr(X = 1|Y = 1) .
To these parameters we assign independent beta priors,
γ0 ∼Beta(α0, β0) and γ1 ∼Beta(α1, β1) .
Therefore, at any interim analysis there will be subjects with complete
information, an observed Y and X. There will also be a group of subjects
with an observed Y , but no X, and a third group of subjects with no
observed data (including those subjects that may be enrolled in the study
in the future). As before, two interim analyses are conducted to determine

MODELING AND PREDICTION
213
if the trial stops early for expected success or for futility. The rules for the
trial are identical to those of Algorithm 5.1, summarized again here:
Steps 1, 2: At the 50- and 75-subject interim analyses, if the predictive
probability of success with the current sample size is at least 0.90, the
trial stops for expected success (and all current subjects are followed
through their ﬁnal outcome). If the predictive probability of success at
the maximum sample size of 100 is less than 0.05, the trial is stopped
for futility.
Step 3: The ﬁnal analysis is conducted when the ﬁnal data on each of the
subjects enrolled in the study are observed, The prior distribution for
p is now ﬁxed as a Beta(1, 1), and success is claimed if the posterior
probability that p > 0.50 is at least Pcut.
Calculating the predictive distribution for the subjects with interim in-
formation allows us to calculate the predictive probability of eventual trial
success. In this example, the predictive probability of one-month success for
each subject with incomplete information depends on their interim value. In
this example it is suﬃcient to consider three groups of subjects for predic-
tions: those with no data, interim failures, and interim successes. Assume
that the ﬁrst nX subjects have an observed X and Y , the second set of
nY subjects have an observed Y , but not X, and n0 have neither an X or
Y , where n = nX + nY + n0. The predictive distribution of the number of
successes for each of these subgroups is a beta-binomial and the exact prob-
ability for each outcome can be calculated. In many examples this cannot
be done analytically, however, and so here we take an MCMC approach,
where values of the primary endpoint are simulated for each subject. This
collection of simulated ﬁnal outcomes allows for the characterization of the
predictive distribution. A straightforward sampling-based approach is as
follows:
Algorithm 5.3 (Using auxiliary variables).
Step 1: An observation of γ0 is drawn from its full conditional distribu-
tion,
γ0|X, Y ∼Beta
Ã
α0 +
nX
X
i=1
I[Xi=1|Yi=0], β0 +
nX
X
i=1
I[Xi=0|Yi=0]
!
Step 2: An observation of γ1 is drawn from its full conditional distribu-
tion,
γ1|X, Y ∼Beta
Ã
α1 +
nX
X
i=1
I[X=1|Y =1], β1 +
nX
X
i=1
I[X=0|Y =1]
!
Step 3: For each subject with an observed Y , but no X, an imputed value

214
PHASE III STUDIES
of X is generated as
Pr (Xi = 1|Y, γ0, γ1) = γYi
Step 4: For each subject with no data (including those subjects yet to be
accrued), an observed value is simulated as
Pr (Xi = 1|p) = p
Step 5: A value of p is simulated from its full conditional distribution,
p|X ∼Beta
Ã
α +
n
X
i=1
I[X=1], β +
n
X
i=1
I[X=0]
!
.
At convergence, this MCMC process of successive simulation from the
ﬁve distributions above creates an observation, (γ0, γ1, X, p) from the
joint posterior distribution.
At an interim analysis, the critical aspect is the value of X for the cur-
rent sample size and the full set of possible subjects. This vector of X is
an observation from the predictive distribution of X. Note that this al-
gorithm does not assume a single Bayesian model connecting all of the
longitudinal values. Rather, when a subject has an interim value we create
a beta-binomial model for the transitions – a separate “working model” for
the transition from each interim state to the ﬁnal success. This model is
“restricted” to be updated only by those subjects with “transitions” from
this model. We return to this issue in the context of Algorithm 5.7 below.
After each cycle through the simulation, we record whether a ﬁnal set of
X would result in a successful trial for both the current and ﬁnal sample
size. The proportion of simulated values that satisfy the ﬁnal success cri-
terion is the predictive probability of trial success. As is typical in MCMC
calculations, an initial set of sampled parameter vectors are discarded (the
MCMC “burn-in” period described in Algorithm 2.1). In this example, con-
vergence is typically observed in only a handful of draws. In the calculations
that follow, we discarded the ﬁrst 500 iterations as MCMC burn-in.
Example 5.6 (Conﬁrmatory trial with auxiliary variables). Tables 5.9 and
5.10 provide an example set of data for an interim analysis. Table 5.9 pro-
vides the state of data at the time of the interim analysis. There are 50
subjects included at the analysis, with 20 of them reaching the 1-month
endpoint, 15 as successes. There are also 20 subjects with interim infor-
mation, of which 10 are successes and 10 are failures. Finally there are 10
subjects with no interim data observed. In order to calculate the predic-
tive probabilities of success in the trial, the transition matrix reported in
Table 5.10 is also needed. This table reports the transitions from 1-week
results to the ﬁnal outcomes at 1 month for those 20 individuals whose 1-
month outcomes have been observed. Of the 15 1-month successes, 10 were

MODELING AND PREDICTION
215
S
F
1-Month
15
5
1-Week
10
10
No Data
10
Table 5.9 The current status of 50 subjects for an example predictive probability
calculation.
1-Month
S
F
1-Week S
10
1
1-Week F
5
4
sum
15
5
Table 5.10 The transition matrix for all subjects that have observed 1-month re-
sults.
1-week successes and 5 were 1-week failures. Similarly, of the 5 1-month
failures, 1 was a 1-week success and the other 4 were 1-week failures.
For the predictive distributions there are 10 subjects that are 1-week
successes, and have a Beta(1+10,1+1) for the probability of transitioning to
a 1-month success. The 1-week failures have a Beta(1+5,1+4) distribution
for transitioning to 1-month success. The subjects with no interim data
have a Beta(1+15,1+5) distribution for transitioning to successes.
A function for calculating the predictive probabilities at the 50- or 75-
subject interim analyses is given on the book’s website. The R code there
provides the calculation for the example of Tables 5.9 and 5.10. The predic-
tive probability of trial success for the current sample size of 50 subjects is
0.929. The predictive probability of success if full accrual to 100 is carried
out is 0.942. According to the design of the study, accrual would stop and
the full data would be collected to see if superiority is met.
We carry out simulations of the described design in order to characterize
the operating characteristics. We do this by embedding our R code within a
larger program that simulates the accrual of subjects and the appropriate
timing of the analyses, then reports the ﬁnal trial results.
In order to create operating characteristics for this design, subject re-
sponses need to be simulated, including the early interim values. Therefore
an assumption about the correlation between X and Y is needed. These
assumptions do not aﬀect the design in any way, but rather the operating

216
PHASE III STUDIES
characteristics observed. In order to simulate subjects in a straightforward
manner and demonstrate the eﬀects of the early observations we simulate
subjects using the following “backward” simulation method.
First, a ﬁnal observation is simulated for a subject, assuming a ﬁnal
probability of success, p,
Pr(X = 1) = p .
An interim value is then simulated conditionally on the ﬁnal value. With
probability ρ the value of Y is assumed the same as the value of X, while
with probability 1 −ρ the value of Y is assumed to be Bernoulli(0.5). If we
assume a value of ρ = 1, then the early predictor, Y , is a perfect predictor
of the ﬁnal value, and X = Y . If we instead assume ρ = 0, then the early
predictor is a Bernoulli(0.5), independent of the ﬁnal value. In this case the
early predictor is independent noise and will not aid the prediction. The
modeling should learn that the interim value is not predictive and then
account for the uncertainty. Values of ρ between 0 and 1 provide for various
mixing probabilities, and thus various correlations between the interim and
ﬁnal values. The value of Pcut = 0.965 was selected by simulation. With
the Beta(1, 1) prior distribution, this results in cutoﬀvalues of 32, 46, and
60, at looks 50, 75, and 100, respectively.
As before, the probability of a Type I error, and the trial in general,
is aﬀected by the accrual rate. For example, if the accrual rate is 1 per
month, then the interim analyses are done with complete information on
all subjects but one. If the accrual rate is 100 per week then all interim
analyses are done with no interim information. The accrual has a drastic
aﬀect on the information available at the interim analysis. This makes
perfect sense since in an adaptive design, the goal is to adapt the trial to
information that has been gained during the study, and in this case the
study will be over before any information is learned. With the cutoﬀvalue
of 0.965 the Type I error is 0.041 with an accrual rate of 1 per week, 0.037
with an accrual rate of 5 per week, and 0.030 with an accrual of 10 per
week. All of these results assume ρ = 1, i.e., the early value is a perfect
predictor; the FDA sometimes asks for results in extreme cases like this to
see how well the algorithm works.
For the default scenario, we simulate 5 subjects per week and assume
ρ = 0.50. Some example operating characteristics are presented in Ta-
ble 5.11. The “bell-shape” of the sample sizes over p is a very common
feature of adaptive designs with stopping for both eﬃcacy and futility. For
poor eﬃcacy the trial is able to stop early for known futility. Likewise for
strong eﬃcacy the trial is able to stop early for known success. For mod-
erate eﬃcacies, however, the sample size is larger since more subjects are
needed to determine the eﬃcacy. The design is able to adjust to the data
and create a more appropriate sample size (not always just a smaller one).
The probabilities of showing superiority (p > 0.5) for each assumed value

MODELING AND PREDICTION
217
p
Pr(Win)
Mean SS
SD SS
Pr(50)
Pr(75)
Pr(100)
0.50
0.035
69.5
21.1
0.01
0.48
0.01
0.23
0.02
0.25
0.55
0.187
79.4
21.8
0.04
0.27
0.04
0.17
0.11
0.38
0.60
0.541
82.1
21.2
0.14
0.11
0.13
0.07
0.27
0.27
0.65
0.855
74.7
21.3
0.32
0.04
0.26
0.02
0.27
0.08
0.70
0.979
63.0
17.7
0.60
0.01
0.27
0.00
0.12
0.01
0.75
0.997
55.0
11.3
0.82
0.00
0.16
0.00
0.02
0.00
Table 5.11 Operating characteristics of the adaptive design with a longitudinal
model for interim results. The two numbers in each of the last three rows represent
the probability of stopping for and achieving success (top) and fail (bottom).
of p and ﬁve diﬀerent weekly accrual rates, 1, 2, 5, 10, and 100, are shown in
Figure 5.2. The probabilities of showing superiority are very close in each of
these accrual scenarios, with a slight decrease when accrual is faster since
the trial has less ability to ﬁnd the correct sample size. The fastest accrual,
100/week, was selected because no data are known when 50 or 75 subjects
are accrued, and thus each trial advances to 100 subjects in each simulated
trial. The diﬀerences among accrual rates lie in the mean sample sizes in
Figure 5.3. The faster the accrual, the more subjects are needed, yet there
are essentially no diﬀerences in the probabilities of superiority. This demon-
strates in adaptive designs the critical role that accrual plays in the ability
of the design to be eﬃcient. By accruing extremely fast (100/week) the
sample size can be more than 50% larger, yet the conclusions are identical.
In order to evaluate the eﬃciency gained from modeling the early results,
we vary the correlation of the early result (1 week) to the ﬁnal result (1
month). We focus on the results for the scenario in which p = 0.65. Fig-
ure 5.4 shows the probabilities of concluding superiority (left axis; solid
line) and the mean sample sizes (right axis; dashed line). The probabil-
ity of concluding superiority generally increases as the predictive ability
of the interim 1-week value increases. Additionally, the mean sample size
generally decreases. In many adaptive designs it is the sample size that is

218
PHASE III STUDIES
0.50
0.55
0.60
0.65
0.70
0.75
0.2
0.4
0.6
0.8
1.0
π
Probability of Success
1
2
5
10
100
Figure 5.2 The probability of success for ﬁve diﬀerent accrual rates for the design
with longitudinal modeling of an early endpoint of Section 5.4.
aﬀected by the ability of interim values to predict ﬁnal endpoints. When
information takes longer to observe in an adaptive trial, the trial needs to
accrue longer before the ability to stop accrual is reached. Generally sim-
ilar conclusions are drawn, but the circumstances where early predictors
are available allows for these decisions to be reached more eﬃciently.
5.5 Prior distributions and the paradigm clash
Thus far, we have highlighted the role of the regulatory prior in conﬁrma-
tory phase trials. This is the prior that is used in combination with the data
to form the posterior distribution for the ﬁnal analysis. Because of the na-
ture of a conﬁrmatory trial, it is typical that this prior represents the view
of a relatively uninformed decision maker. There may be circumstances
where a diﬀerent analysis prior is used or desired. In these circumstances
there is a clash between the role of the prior and the need to control Type
I error. If a prior is used that provides favorable information for the exper-
imental treatment, then other things being equal, the likelihood of success
is increased by use of the prior. In the case where adaptive features that
inﬂate Type I error are added to the trial the threshold for success was
raised to control the Type I error. But if the threshold for success is raised
to counter the eﬀects of the prior distribution then this eﬀectively removes

PRIOR DISTRIBUTIONS AND THE PARADIGM CLASH
219
0.50
0.55
0.60
0.65
0.70
0.75
50
60
70
80
90
100
π
Mean Sample Size
1
2
5
10
100
Figure 5.3 The mean sample size for ﬁve diﬀerent accrual rates for the design
with longitudinal modeling of an early endpoint of Section 5.4.
any beneﬁt of using the prior distribution, and the two mechanisms cancel
each other out.
In order to demonstrate this clash, we recall the single arm trial with-
out adaptations from Example 5.1. Assume n = 100 subjects are enrolled
and the ﬁnal analysis must show that the posterior probability that the
treatment success rate, p, is larger than 0.5 is greater than 0.95. When a
Beta(1, 1) prior is used, we saw in Section 5.2 that 59 or more successes
resulted in a posterior probability larger than 0.95 and a Type I error of
0.0443.
Now suppose historical data that represents the entire clinical experience
of the treatment in a very similar patient population resulted in 9 successes
in 10 trials. Adding these data to the baseline Beta(1, 1) prior results in
a Beta(10, 2) prior distribution. If a Beta(10, 2) prior distribution is then
used, then after 100 subjects a posterior probability threshold of 0.95 results
in an eﬀective cutoﬀof X ≥55 successes. For the trial of 100 subjects, the
probability that X ≥55 conditional on p = 0.5 is 0.184. This inﬂation of
the Type I error is due to the eﬀects of the prior making it more likely
that our posterior probability is larger than 0.95. If the approach of raising
the threshold for the posterior probability to claim success is used, then
this threshold must be raised to 0.9934. Of course, this threshold results in
X ≥59 successes again being observed out of 100 subjects. This is because

220
PHASE III STUDIES
0.0
0.2
0.4
0.6
0.8
1.0
0.80
0.82
0.84
0.86
0.88
0.90
ρ
Probability of Success
70
72
74
76
78
80
Mean Sample Size
Figure 5.4 The probability of superiority (solid line, left axis) and the mean sam-
ple size (dashed line, right axis) for the case where p = 0.65 and various values
of ρ the parameter for the correlation between the early and late endpoint.
this cutoﬀis determined by the frequentist Type I error, not a Bayesian
analysis.
When the Bayesian design is required to control Type I error, it loses
many of its philosophical advantages. In eﬀect the Bayesian machinery is
creating a design that is inherently frequentist. This can be the major
constraint of a conﬁrmatory trial: it must meet the conditions of the reg-
ulatory agency. There are speciﬁc rules and conditions in place, and any
design must meet these rules. For this reason the role of prior distributions
in conﬁrmatory studies is rarely an issue (though see our discussion on the
adaptive use of historical data in Section 6.1).
The constraint that Type I error must be controlled at a speciﬁed level
creates additional conﬂict between the Bayesian and frequentist paradigms
regarding interim data looks – for example, the adaptive design in Sec-
tion 5.2.1 having interim analyses at 50 and 75 subjects. Because of these
looks, the posterior probability threshold for claiming success was raised
from 0.95 to 0.976. This implied that the number of successes threshold for
success at the 50-, 75-, and 100-subject looks were 33, 47, and 60 respec-
tively. In the non-adaptive design of 100 subjects, the trial is successful if
59 or more successes are observed at the 100-subject look. One trial could
be run with this adaptive design, for which there could be 32, 46, and 59

PHASE III CANCER TRIALS
221
successes at each of the three respective looks, and still result in an un-
successful trial. Now suppose a second trial is run with no adaptive looks,
and 59 successes are observed at 100 subjects. Suppose further that there
would have been 32 and 46 successes at the interim time periods had in-
terim analyses been conducted. In fact, assume the data for all 100 subjects
are identical in each trial. The result of this second trial would be success.
Therefore, we have two trials that achieve exactly the same data, yet one is
successful and one is not. This result highlights a violation of the Likelihood
Principle; see Subsections 2.2.3 and 2.2.7. To a Bayesian, the posterior dis-
tributions at the conclusion of each of the trials are identical. But because
the rules of the trial are dictated by frequentist notions (control of Type I
error), the results of the trials are diﬀerent.
At some future time trials may be evaluated using fully Bayesian notions
of utilities and decisions (see Section 4.6), which would enable designs to
be built that do not violate the Likelihood Principle or Bayesian notions.
But currently, the regulatory structure is such that conﬁrmatory trials are
usually judged and evaluated using Type I error.
5.6 Phase III cancer trials
In this section we present an example phase III cancer trial that is similar
to an actual trial, but has been changed in minor ways. There is nothing
cancer-speciﬁc about the design; the trial could have been constructed for
a device or a drug in another therapeutic area.
The primary endpoint is time-to-failure. In an oncology setting this is
typically time-to-death or time-to-disease progression. In each circumstance
the “event” is a negative for the patient. Extending the time-to-failure is
the desired outcome for the treatments. In this example we refer to the
event as the time-to-death. Assume the historical median time of survival
for subjects with this disease is 10 weeks (late stage, poor prognosis). The
trial will test the standard of care (SOC) control against the standard of
care plus the experimental agent (treatment). The goal is to demonstrate
superiority of the treatment arm.
Assume the standard ﬁxed design being contemplated in this circum-
stance is a 360-subject trial, randomized 1:1 to the treatment and SOC.
Here, the primary analysis at the end of the trial, with a six-month follow-
up on the last patient accrued, is a logrank test with a two-sided 0.05-level.
Assuming a median survival time of 10 weeks for the SOC and 14 for the
treatment arm leads to a power of 89% to demonstrate superiority. A treat-
ment median survival time of 13 weeks provides a power of 70%. In this
section we present a Bayesian adaptive alternative to this ﬁxed design.
The structure of the trial is similar to that of the ﬁxed trial. The ran-
domization ratio remains 1:1 throughout, and the ﬁnal analysis will remain

222
PHASE III STUDIES
a logrank test, though the nominal level of the test will be changed in order
to control the type I error of the design.
Analyses of the data will be made for the ﬁrst time when 100 subjects
have been randomized. This ﬁrst analysis takes place immediately upon
accrual of the 100th subject, not based on the length of follow-up of the
100th subject. Therefore, if the result of the 100-subject analysis is to stop
accrual, the ﬁnal sample size will be 100 plus a slight overrun of subjects
entering the trial. Additional analyses will be done in 20-subject increments
of accrual. Thus, if the trial accrual continues, analyses will be done at 100,
120, 140, and so on to a maximum sample size of 360. The analyses are
“sample size looks” only, and the conclusion of the analyses will be whether
or not accrual should continue in the trial. These looks are quite diﬀerent
from classical O’Brien-Fleming (1979) sequential analyses. The result of
these looks is not a determination of superiority, but rather a determination
that the sample size is suﬃcient.
The rules for these sample size looks are as follows:
Algorithm 5.4 (Phase III cancer trial design).
Step 1: If the predictive probability of trial success with the current sam-
ple size with 6 months additional follow-up is larger than 0.99, then
accrual will stop for expected success.
Step 2: If the predictive probability of trial success for the maximum sam-
ple size of 360 subjects with the full 6 months of follow-up is low less
than 0.01, then accrual will be stopped for futility.
Step 3: If neither of the above conditions hold then accrual will continue
until the next sample size analysis, or stop if the maximum of 360 sub-
jects has been reached.
If the trial stops for expected success, the last subject enrolled will be
followed for the six-month minimum and the ﬁnal analysis, a logrank test
for superiority at the nominal level, will be performed. The sample size
analyses being done shape the sample size to an appropriate value, but
are diﬀerent from classical sequential analyses because the analyses do not
result in a determination of superiority. There is one ﬁnal analysis that is
conducted, after the six-months follow-up of the last subject accrued.
There are a number of aspects of the design above that have been selected
without much explanation. The minimum sample size of 100 was selected
to represent the smallest sample size that regulatory agencies would accept
with a determination of superiority. Typically this number would be negoti-
ated with the various scientiﬁc review bodies. The maximum sample size of
360 was selected as the sample size of the classically powered study. There
is no reason that this should be the case; in fact, the strength of the trial is
the ability to shape the sample size appropriately and a larger maximum

PHASE III CANCER TRIALS
223
would provide greater power with little impact on the mean sample size.
We explore this eﬀect on power and the mean sample size below.
The cutoﬀvalues of 0.99 and 0.01 are selected by the sponsor of the study
to represent the amount of “risk” involved. The values of 0.01 and 0.99 are
quite conservative. For example, in a classically powered study, a power of
80% is frequently employed to select the sample size. These stopping cutoﬀs
represent selections with much more certainty around the ﬁnal outcome.
The accrual rate expected (and a range of values) and the relatively small
median times of survival allow for these conservative values to be selected. If
the median time of survival were larger, these “more extreme” values would
be harder to reach. The minimum of 6-month follow-up is a restriction
that is not always present, but in many cancer trials it is a requirement
of the trial. This has some ethical aspects since this restriction assures
each patient that they will contribute scientiﬁcally to the conclusions of
the study.
The ﬁnal analysis of the trial will be a logrank test. In order to model
the interim results and the predictive distribution of the study results, a
Bayesian time-to-event model is created. Due to the relatively short median
times to event and historical modeling of the times of events, an exponential
time-to-event model is selected with a hazard rate speciﬁc to each treatment
group. Let Yi be the time to event for subject i, for i = 1, . . . , n. Let the
trial arm for subject i be di, where d = 1 refers to the SOC and d = 2
refers to the treatment. We assume an exponential time-to-event model,
f(t) = λd exp (−λdt) for d = 1, 2 ,
and independent prior distributions for the two treatment hazard rates,
λd ∼Gamma (α = 1, β = 0.1) for d = 1, 2 .
These prior distributions are equivalent to assuming 1 event in 10 weeks
(the unit of time in the model), and do not aﬀect the ﬁnal logrank analysis.
These priors shape the sample size, but have only a small eﬀect on the
conclusions of the sample size looks. This is because at the ﬁrst analysis
(the 100-subject look), there will be enough follow-up and events that this
“one subject worth of information” in the prior will have little weight. We
refer to this model as an adaptive design working model because it drives
the adaptive aspects of the design, but is not the ﬁnal analysis, and has
little impact outside of the adaptive design.
At an interim analysis, the predictive distributions are the quantities that
drive the adaptive design. The calculation of the predictive probabilities of
trial success via MCMC is now described. The suﬃcient statistics within
each treatment arm d at the time of an interim analysis are the total
exposure, Ed, and the number of events, Xd. The following successive steps
are taken in the computation:
Algorithm 5.5 (Phase III cancer trial MCMC sampler).

224
PHASE III STUDIES
Step 1: Simulate hazard rates for each treatment group from their full
conditional distributions,
λd|Ed, Xd ∼Gamma (α + Xd, β + Ed) for d = 1, 2 .
Step 2: Simulate the ﬁnal time-to-event for each subject, conditional on
the hazard rate for the respective treatment group, λd, and the current
censored time, Y c
i , by taking advantage of the memoryless property of
the exponential distribution:
Yi|λd, Xd ∼Y c
i + Exponential (λd) .
Step 3: Evaluate the logrank test for the current sample size (Y1, . . . , Yn),
and an additional 6 months of follow-up for all subjects.
Step 4: Evaluate the logrank test for the maximum sample size, (Y1, . . . ,
Y360), and an additional 6 months of follow-up for all subjects. This
step involves an assumption in the calculations for the accrual rate. In
an actual trial a prospective rule will need to be made for the future
accrual rate, such as the average accrual over the last two months. In
this example we use the true accrual rate.
Steps 1-4 are then repeated M times; note that this is a “one-oﬀ” (not iter-
ative) Monte Carlo algorithm, so there is no need to delete any portion of
the sample as burn-in. For a nominal level of the logrank test, α, we record
the frequency of trials in Steps 3 and 4 that demonstrate superiority. The
frequency of trials in Step 3 that demonstrate superiority is the predictive
probability of superiority with the current sample size. The frequency of
trials in Step 4 that demonstrate superiority is the predictive probability
of superiority with the maximum sample size.
In order to evaluate the operating characteristics and to ﬁnd an appro-
priate nominal α-level for the logrank test, we perform simulations of the
design. Simulated trials are created, with an assumed accrual rate and as-
sumed median survival times in each treatment group. To simulate the
times-to-death we assume an exponential rate in each group with the spec-
iﬁed median survival times. The trials are carried out exactly as described
above. For each of the scenarios of interest, we simulate a mean accrual of
5 subjects per week, with an implied Poisson distribution for the number
of subjects per week (due to the exponential distribution for waiting times
until the next accrual).
Next we select a set of null hypotheses to evaluate the Type I error of the
design. We assume the regulatory restriction is a one-sided Type I error of
0.025. Figure 5.5 shows the simulated Type I error rate for each nominal
α-level used in the ﬁnal superiority analysis between 0.010 and 0.025. For
each nominal α level, 1000 simulated trials are conducted. The simulated
Type I error is the proportion of simulated trials where the simulated data

PHASE III CANCER TRIALS
225
0.010
0.015
0.020
0.025
0.00
0.02
0.04
0.06
0.08
Nominal Alpha
Simulation Type I Error
Figure 5.5 The simulated Type I error for each nominal α-level for the cancer
phase III trial.
meets the condition of signiﬁcance after 6 months of follow-up on the last
subject accrued.
Due to there being just 1000 simulated trials per nominal α level, there
is simulation error in the estimated Type I error rates. However, our re-
sults suggest a nominal α level of 0.015 does control the Type I error at
the one-sided 0.025 level. Typically in a submission to regulatory agencies
additional null scenarios as well as more simulated trials are used to justify
the nominal α chosen; in this trial we use the nominal value α = 0.015.
Simulations of the design using α = 0.015 for the ﬁnal analysis were then
carried out with 1000 simulated trials per scenario. Table 5.12 presents a
subset of operating characteristics for the adaptive design. In each case the
median time-to-event for the control group is 10 weeks, while the median
time-to-event for the treatment group is varied from 8 weeks to 16 weeks.
The probability the trial concludes superiority at the ﬁnal analysis is re-
ported in the “Sup” column. The mean of the sample size is reported in the
the “N” column. The probability the trial runs to the maximum sample
size is reported in the “Max” column. The last six columns then report
the reason for stopping accrual in the trial and the resulting conclusion
at the ﬁnal analysis. “Expect S” refers to stopping for expected success,
“Cap” refers to stopping because of reaching the maximum sample size,
and “Futility” refers to stopping for futility. If the maximum sample size of

226
PHASE III STUDIES
Expect S
Cap
Futility
Case
Sup
N
Max
Sup
No
Sup
No
Sup
No
8
.000
128.2
.00
.00
.00
.00
.00
.00
1.0
10
.020
202.4
.07
.02
.00
.01
.03
.00
.95
11
.129
249.3
.19
.09
.00
.04
.08
.00
.79
12
.356
276.1
.34
.25
.00
.11
.14
.00
.50
13
.634
271.3
.33
.50
.00
.13
.12
.00
.24
14
.819
247.8
.23
.72
.00
.10
.07
.00
.11
15
.945
211.1
.11
.90
.00
.04
.03
.00
.03
16
.983
186.0
.04
.96
.00
.02
.01
.00
.01
Table 5.12 Operating characteristics for the cancer phase III design. In each case
the control median time-to-event is 10 weeks. The case refers to the treatment
median, varied from 8 weeks to 16 weeks. The probability the trial concludes
superiority is listed in the “Sup” column. The mean sample size is reported in
the “N” column. The probability of the trial running to the maximum sample size
of 360 is reported in the “Max” column. The last six columns report the reason
the trial stopped (top row) and the conclusion of the trial, “Sup” for superiority
and “No” for no superiority.
360 is reached and the conclusion is expected success or futility, then the
result is recorded as the latter rather than the former. Regardless of when
it occurs, the result of the ﬁnal analysis is recorded as superiority (“Sup”)
or non-superiority (“No”).
The results demonstrate the typical bell shape to the expected sample
size. For cases where there is no eﬀect, or a negative eﬀect, then the mean
sample size is quite small. In cases where the eﬃcacy is strong, the sample
size is also small. For the more statistically challenging intermediate cases,
the sample size tends to be larger. Figure 5.6 presents the mean sample
size (triangles) for each of the treatment median values (circles). Figure 5.6
also presents the probability of concluding superiority for each treatment
median value. This ability to understand the underlying treatment eﬀect
and shape the appropriate sample size is a positive for the sponsor, for the
subjects in and out of the trial, and for the scientiﬁc community.
In cases where the treatment is detrimental or ineﬀective the design de-
termines this very eﬃciently. In the null hypothesis case (median=10) the
mean sample size is 202, only 56% of the maximum sample size. This mean
sample size saves resources, prevents subjects from being randomized to
ineﬀective treatments, and saves time. Additionally, there are limited ex-
perimental subjects and using these valuable resources on treatments that
are ineﬀective may prevent or delay other positive treatments from being
investigated. In cases where the treatment is eﬀective, the ability to deter-

PHASE III CANCER TRIALS
227
8
10
12
14
16
0.0
0.2
0.4
0.6
0.8
1.0
Treatment Median
Pr(Superiority)
100
150
200
250
300
360
Sample Size
Figure 5.6 The probability of concluding superiority (circles) and the mean sample
size (triangles) for each treatment median time-to-event.
mine this quickly and eﬃciently is critical. In the case where the treatment
has a median time-to-event of 15 weeks, the adaptive trial has 94.5% power
with a mean sample size of only 211 (59% of 360). The savings in subjects
is important, but the time savings to make the conclusion is perhaps more
important. The subjects outside of this trial, needing treatment, beneﬁt
greatly from the adaptive design.
The scientiﬁc community beneﬁts as well, because the trials are more
powerful per subject. Using 360 subjects unnecessarily has negative conse-
quences to regulators and the medical community alike. As a comparison
to the adaptive design, we present the results for several ﬁxed design trials.
For each of the ﬁxed trials, the same assumptions for accrual rate and the
underlying truth about the two treatment arms are made. For these ﬁxed
designs a nominal one-sided α of 0.025 is used (recall the adaptive design
uses α = 0.015). Figure 5.7 presents the probability of concluding supe-
riority for the adaptive design (solid line). The probability of superiority
for the ﬁxed designs with 360 subjects and 100 subjects are also reported.
These ﬁxed designs are for the minimum and maximum sample sizes of the
adaptive design. Note that the Bayes design does virtually as well as the
full 360-patient study, but with far fewer samples on average.
There is more to the adaptive design than just understanding the treat-
ment eﬃcacy and then selecting the appropriate sample size. For each case
we also created a ﬁxed trial with the same sample size as the mean of the

228
PHASE III STUDIES
8
10
12
14
16
0.0
0.2
0.4
0.6
0.8
1.0
Treatment Median
Pr(Superiority)
100
360
Mean
Figure 5.7 The probability of concluding superiority for the adaptive design, a
ﬁxed design with 360 subjects, a ﬁxed design with 100 subjects, and a ﬁxed design
where the sample size is equivalent to the mean sample size for each case.
adaptive trial. For instance, when simulating this ﬁxed design when the
case has a treatment median of 14, a ﬁxed sample size of 248 is used. This
represents a “super smart” ﬁxed trial, where the sample size depends on
the true underlying median. The probability of superiority for this ﬁxed
design (referred to as “Mean”) is also presented in Figure 5.7. Note that
the adaptive design has higher power than the ﬁxed design with exactly
the same mean sample size. The strength of the adaptive design is not just
getting the appropriately powered sample size, it is the ability to adjust
to the results of the individual trial. Even in the case where the treatment
median is 16, there are trials simulated where the results are not strong
enough for superiority with 188 subjects. The power of the ﬁxed-188 design
for this case is 0.889. The adaptive design also has a mean sample size of
188, but has a power of 0.983.
5.7 Phase II/III seamless trials
Of recent interest and promise for the Bayesian approach are so-called
seamless phase II/III designs. Typically in drug development, phase II tri-
als, whether adaptive or not, are run for the goal of determining the next
steps in the development of the drug. The data from the dose-ﬁnding phase

PHASE II/III SEAMLESS TRIALS
229
II trial are collected and analyzed. Decisions are then made as to whether
a phase III trial should be conducted and which dose or doses will be used.
Typically meetings take place with regulatory to review these decisions and
to design the phase III trial. This time between the phase II trial and the
phase III trial can be substantial (9 months to 2 years). Additionally, if
a phase III trial is conducted, the data collected in the phase II trial is
typically ignored in the evaluation of phase III.
Both of these issues have lead to the desire to develop seamless phase
II/III trials. The idea of a seamless phase II/III trial is to conduct a dose-
ﬁnding phase II trial, which depending on the data could spawn the start of
a phase III trial, which happens “seamlessly,” immediately after phase II.
Randomization is taking place during the phase II aspect of the trial and
a prospective decision is reached to shift to the phase III part of the trial.
When this happens subjects will continue to be randomized, but typically
in a diﬀerent manner (perhaps a 1:1 comparison). The protocol for the trial
is prospectively set up so that the trial would continue despite the shift from
phase II to phase III. The seamless shift removes the time between trials,
often resulting in shortened development time for the drug.
The creation of the phase III shift at the conclusion of phase II implies
that phase III and the rules for shifting to phase III have to be completely
prespeciﬁed before phase II starts. Thus the task of designing phase III
after a typical phase II trial is done instead is undertaken before the phase
II trial starts. Phase II, phase III, and the decisions to shift to phase III
are all predetermined and prespeciﬁed.
There are two distinct ways in which a seamless phase II/III trial is de-
signed. The ﬁrst, referred to as an operationally seamless design, ignores
the phase II aspect of the trial when evaluating the phase III part of the
trial. In this case the phase III trial is typical in that the analysis and
determination of the success of the drug is based exclusively on the data
in the phase III trial. By contrast, an inferentially seamless trial involves
analyzing all subjects from both the phase II and phase III stages of the
trial for the ﬁnal analysis. The beneﬁt of an operationally seamless trial is
the reduction of the time between the phase II and the phase III trial. Ad-
ditionally, the sites for the design are already created and actively accruing
subjects. The advantages of an inferentially seamless trial include those
of the operationally seamless trial, but additionally the subjects from the
phase II part of the trial are included in the ﬁnal analysis of phase III. This
allows for the phase II subjects to shape both the adaptive decisions of the
phase II trial (the learning aspect of the trial) as well as the conﬁrmation.
From a regulatory perspective, an inferentially seamless trial is really one
large phase III trial. The ﬁrst subject in the trial (at the phase II stage)
is possibly included in the conﬁrmatory analysis at the conclusion of the
phase III stage. We note that some companies rely upon the results from
phase II to raise capital for the phase III trial. An inferentially seamless

230
PHASE III STUDIES
trial, while more eﬃcient, usually blinds the sponsor from the results of
the data from the phase II stage of the trial and thus from using them
in raising capital. Employing an operationally seamless design allows the
sponsor to see and use the phase II trial data.
5.7.1 Example phase II/III trial
In this subsection we provide an example seamless phase II/III trial. This
is not a real trial, but is loosely based on similar trials that have been
built and conducted. The example is in spinal cord injury. In spinal cord
injury, a patient can suﬀer varying degrees of paralysis. The standard mea-
sure of the severity of paralysis is the American Spinal Injury Association
(ASIA) scale. The scale is categorical, with possible outcomes A (no func-
tion), B (sensory only), C (some sensory and some motor), D (useful motor
function), and E (normal). We assume that the trial enrolls subjects with
an acute injury who are initially classiﬁed as ASIA category A. A stan-
dard measure of success is the ASIA classiﬁcation at 6 months. Assume
the conﬁrmatory regulatory primary analysis is the probability a subject
progresses to ASIA C at 6 months. Each subject will have monthly mea-
sures of their ASIA score. It is possible for a subject to transition in either
direction on the ASIA score from month to month (e.g. B to A or B to C),
but positive transitions are most likely. Historically, about 10% of subjects
progress from ASIA A at entry to at least ASIA C at 6 months.
This is an ideal situation for a seamless phase II/III trial. The condition
is reasonably rare, providing a forced, slow accrual rate. The regulatory
endpoint of 6-month ASIA score is well known and well understood, and
thus there is little need for the time between trials to design the phase
III study. Finally, the phase III aspect of the trial is necessarily designed
before the phase II trial.
Four experimental doses are under consideration, and a placebo arm
is used for a control. We construct a seamless phase II/III design that
ﬁrst investigates the dose-response relationship of the treatment and makes
a determination about its eﬃcacy relative to placebo. The trial has the
possibility to shift seamlessly to a conﬁrmatory stage. At the conclusion
of the trial the subjects in the arm selected in Stage 1 will be compared
to a placebo. The design will be inferentially seamless; all subjects from
each of the arms will be included in the ﬁnal analysis, whether they were
accrued during Stage 1 or Stage 2. From a regulatory perspective this trial
is treated as a conﬁrmatory trial. That is, from the ﬁrst subject enrolled,
each is treated as though he or she is in a phase III type trial.

PHASE II/III SEAMLESS TRIALS
231
5.7.2 Adaptive design
Assume there are four experimental doses, d = 1, 2, 3, 4, and a control,
d = 0. We label the ASIA scores as 1 (A), 2 (B), 3 (C), 4 (D), and 5 (E).
Let Yit be the ASIA score for subject i at monthly visit t, for i = 1, ..., n and
t = 1, ..., 6. During Stage 1 there is a possibility of a target experimental
dose being selected to advance to Stage 2. At the conclusion of the trial, if
Stage 2 is conducted, a primary analysis is conducted testing the superiority
of the target dose selected in the ﬁrst stage against the control.
Let pd be the probability that a subject has an ASIA score of 3 or greater
at 6 months with treatment arm d. We use T to refer to the target dose d
selected in Stage 1 of the trial. We test
H0 : pT ≤p0
against the alternative
HA : pT > p0 .
A classical frequentist test is conducted and superiority will be claimed if
the test is signiﬁcant at the nominal α = 0.014 level (this value will be
explained and justiﬁed to control Type I error across both stages).
We construct the following design, explaining Stage 2 ﬁrst. If the decision
is made to conduct Stage 2, then the trial will shift immediately from the
Stage 1 design, with no delay, to Stage 2. In Stage 2 the target dose selected
during Stage 1 will be randomized 1:1 to placebo, with 100 subjects accrued
in each arm. Therefore, Stage 2 is a traditional-looking ﬁxed aspect of the
design.
During Stage 1 we begin with an initial allocation of 3 subjects per dose
(including placebo). During this initial period a total of 15 subjects will
be allocated in block fashion, 1:1:1:1:1, to the ﬁve arms. After the accrual
of the 15th subject an analysis occurs and the allocation probabilities of
the ﬁve arms are set. Initial analyses will be done monthly after the ﬁrst
analysis, and the following algorithm will be employed:
Algorithm 5.6 (Seamless phase II-III trial design).
Step 1: If there are at least 50 subjects in the trial and the predictive
probability of the most likely eﬀective dose 90 (ED90; deﬁned in Sub-
section 5.7.4) showing superiority by the end of Stage 2 is at least 0.90,
then Stage 1 ends and Stage 2 starts with the most likely ED90 as the
target dose.
Step 2: If there are at least 50 subjects in the trial and the predictive
probability of the most likely ED90 showing superiority by the end of
Stage 2 is less than 0.10, then the trial ends for futility.
Step 3: If the maximum sample size of 150 in Stage 1 has been reached
then if the predictive probability of the most likely ED90 showing su-
periority by the end of Stage 2 is at least 0.80 then Stage 1 ends and

232
PHASE III STUDIES
Stage 2 starts with the most likely ED90 as the target dose. If the pre-
dictive probability is not at least 0.80 then the trial ends for insuﬃcient
evidence of success.
Step 4: If none of the conditions above hold then the randomization prob-
abilities for the ﬁve arms are reset and Stage 1 continues.
The maximum sample size during Stage 1 is 150 and the maximum sample
size during Stage 2 is 200. The transition between Stage 1 and Stage 2 is
immediate and the decision to transition to Stage 2 happens immediately
upon accrual of the 150th subject, if the transition has not been made
earlier.
5.7.3 Statistical modeling
For our statistical model, we use a ﬁrst-order dynamic linear model (DLM;
see West and Harrison, 1989) for the dose-response model for the probabil-
ity of a subject having an ASIA score of 3 or better at 6 months. Let
pd = Pr (Yi6 ≥3|di = d) .
We model the log-odds, θd = pd/(1 −pd), as follows. The DLM is
θ1 ∼N
¡
−2, 22¢
and
θd ∼N
¡
θd−1, τ 2¢
for d = 2, 3, 4.
The log-odds for the placebo is modeled as
θ0 ∼N
¡
−2, 22¢
.
The parameter τ 2 is a variance component, referred to as the drift pa-
rameter. It dictates the amount of smoothing from dose to dose in the
dose-response model. In the ﬁrst-order DLM this parameter regresses the
value at each dose to the neighboring doses. The second-order DLM re-
gresses the parameter at each dose according to a linear growth model. We
model the parameter τ with an inverse-gamma prior distribution,
τ 2 ∼IG (α = 2, β = 1) ,
where the pdf of the inverse-gamma distribution is taken as
p(τ 2|α, β) =
exp
³
−
1
βτ 2
´
Γ(α)βα (τ 2)(α+1) .
In order to model the early values of the ASIA scores (weeks 1, . . . , 5),
we create an adaptive design longitudinal model. This model correlates the
6-month ASIA score and the earlier interim values. This model is updated
by the observations of subjects at the earlier times and the 6-month value.

PHASE II/III SEAMLESS TRIALS
233
For each time t, we model the probability of a subject being a 6-month
ASIA 3 or greater (Yi6 ≥3) conditional on a value of Yit = 1, Yit = 2, and
Yit = 3,
Pr (Yi6 ≥3|Yit = k, di = d) =
exp (θd + γtk)
1 + exp (θd + γtk) for t = 1, . . . , 5, k = 1, 2, 3 .
There are 3 parameters for each of the 5 time periods, for a total of 15
parameters. The following independent priors are selected for the γ’s:
γt1 ∼N (−2, 1) for t = 1, ..., 5
γt2 ∼N (1, 1) for t = 1, ..., 5
and
γt3 ∼N (2, 1) for t = 1, ..., 5.
A value of γ = 0 implies that the likelihood of success at 6 months is
equivalent to “no information,” or a subject with no interim information.
A value larger than 0 implies a higher likelihood of success and a value less
than 0 implies a smaller likelihood of success.
5.7.4 Calculation
The joint posterior distribution is calculated using MCMC. The sequential
draws are described below. The predictive probability of trial success (in-
cluding the 100 additional subjects per arm in Stage 2) is calculated by
simulating 6-month values for each subject in the trial and each possible
prospective subject in Stage 2. The following steps are used in the MCMC
routine:
Algorithm 5.7 (MCMC for seamless phase II-III trial).
Step 0: Set the starting value for each parameter.
Step 1: Simulate γtk for t = 1, ..., 5 and k = 1, 2, 3 from its full conditional
distribution,
p
¡
γtk|θ, γ, T +
tk, T −
tk
¢
∝
exp (θd + γtK)T +
tk
[1 + exp (θd + γtK)]T +
tk+T −
tk
exp
Ã
−(γtk −µk)2
2
!
,
where µk is the prior mean for γtk, T +
tk is the number of subjects that
transitioned from k at time t to a 3 or greater at time 6, and T −
tk is the
number of subjects that transitioned from k at time t to less than 3 at
time 6. Observations are simulated using a Metropolis-Hastings step.
Step 2: Simulate Y6 for each subject currently in Stage 1, assuming the
most recent observation of Yt = k:
Pr (Yi6 = 3|t, k, γ, θ) =
(
exp(θd)
1+exp(θd) ,
t = 0
exp(θd+γtk)
1+exp(θd+γtk) ,
t = 1, ..., 5

234
PHASE III STUDIES
Step 3: Simulate θD for D = 0, 1, ..., 4 from its conditional distribution,
p
¡
θD|Y6, θD−1, θD+1, τ 2¢
∝[exp (θD)]
P
i:di=D I[Yi6=3]
[1 + exp (θD)]nD
p(θD) ,
where nD is the sample size for dose D and the prior is
p(θD) =















exp
³
−(θD+2)2
2∗22
´
D = 0
exp
³
−(θD+2)2
2∗22
−(θD−θD+1)2
2∗τ 2
´
D = 1
exp
³
−(θD−θD−1)2
2∗τ 2
−(θD−θD+1)2
2∗τ 2
´
D = 2, 3
exp
³
−(θD−θD−1)2
2∗τ 2
´
D = 4
Step 4: Simulate τ 2 from its full conditional distribution,
p(τ 2|θ) = IG

α + 3
2,
"
β−1 + 1
2
4
X
d=2
(θd −θd−1)2
#−1

Step 5: Simulate ZD, the number of successes in 100 subjects (Stage 2)
for each arm, D = 0, 1, 2, 3, 4, where
ZD ∼Binomial
µ
100,
exp (θD)
1 + exp (θD)
¶
.
Repeat Steps 1 through 5 M + B times, where the ﬁrst B are burn-in
simulations and are ignored; values such as B = 1000 and M = 5000 are
typically appropriate.
We note that, as in Algorithm 5.3, in this algorithm we use “working mod-
els” to more easily handle the evolution of a subject’s data over time. That
is, if a subject has her last value at month 3 then we use the single model
from month 3 to month 6; it is updated only by those subjects making
that full transition from 3 to 6 months. If a subject has 4 months and we
simulate the ﬁnal 6-month value, we do not go back and use the subject’s
3-month value and her imputed 6-month value.
The predictive probability of trial success at the end of the entire trial is
estimated by the empirical frequency of all trial successes in both stages.
That is, the ﬁnal analysis includes both Stage 1 observations where XD =
P
i:di=d Yi6 Stage 1 successes and the Stage 2 observations in ZD in which
the combination is statistically signiﬁcantly superior at the one-sided nom-
inal 0.014-level over the observations for placebo. For each of the M ob-
servations from the posterior, there is a set (θ0, . . . , θ4) that deﬁnes the
probability of success in each group, (p0, ..., p4). For each simulation, as-
suming the probabilities p0, ..., p4 there is an eﬀective dose 90, or ED90,
deﬁned as the smallest dose that achieves at least 90% of the maximal

PHASE II/III SEAMLESS TRIALS
235
beneﬁt from placebo. This is the smallest d such that
pd > 0.90 ∗(max(pd) −p0) .
If no pd > p0 then we refer to the placebo, d = 0, as the ED90. The
posterior probability that dose d is the ED90 is estimated by the empirical
frequency of posterior simulations for which this is true. The dose with the
maximum posterior probability of being the ED90 is referred to as the most
likely ED90. At each interim analysis this dose has the possibility of being
deﬁned as the target dose and advancing to Stage 2.
Additionally, we use the posterior probability of each dose being the
ED90 in order to set the randomization vector for each month, during
Stage 1. When we reset the randomization probabilities each month we as-
sign a randomization vector such that the probability of each active dose,
d = 1, . . . , 4, is proportional to the probability it is the ED90. The placebo’s
randomization probability is proportional to the maximum probability an
experimental dose is the ED90. The randomization vector is then normal-
ized to sum to 1.
5.7.5 Simulations
In order to evaluate the operating characteristics of the design and set a
nominal α level, we again use simulation. In order to simulate the design as
described we must simulate subjects with known probabilities of being 6-
month successes, as well as each of their monthly ASIA scores, Yi1, . . . , Yi6.
We create the following mechanism for simulating subjects.
First, since states D and E are very rare, we simply group states C, D,
and E into a single category (“C+”), resulting in a space with three states
instead of ﬁve. We then create a Markov chain on this reduced state space,
1, 2, 3, for the subjects and simulate transitions starting at t = 0 and con-
tinuing at discrete time transitions for each monthly visit for t = 1, . . . , 6.
We construct diﬀerent transition matrices in order to create diﬀerent 6-
month success rates. We assume a default transition probability matrix for
the categorical values, k = 1, 2, 3, as


1 −λ1
λ1
0
λ−1
1 −λ1 −λ−1
λ1
0
λ−1
1 −λ−1


We assume default values of a subject improving by 1 state in a time period
as λ1 = δ1 = 0.10, and the default probability of a subject worsening by
1 state in one period as λ−1 = δ−1 = 0.03. Using these default values a
subject has a 0.10545 probability of being a 6-month success. In order to
vary this probability we create a log-odds eﬀect to the δ values to achieve
the desired 6-month success probability. We deﬁne the “up one” and “down

236
PHASE III STUDIES
0.010
0.015
0.020
0.025
0.00
0.01
0.02
0.03
0.04
0.05
Nominal Alpha
Simulation Type I Error
Figure 5.8 The Type I error of the seamless design for a range of nominal α
levels.
one” transition probabilities as
λ1 =
exp
n
log
³
δ1
1−δ1
´
+ φ
o
1 + exp
n
log
³
δ1
1−δ1
´
+ φ
o and λ−1 =
exp
n
log
³
δ−1
1−δ−1
´
−φ
o
1 + exp
n
log
³
δ−1
1−δ−1
´
−φ
o .
By changing the values of φ, diﬀerent 6-month success rate values can be
created in a manner consistent with the general structure provided in the
default transition matrix. For example, a value of φ = −0.033 creates a
6-month rate of 0.10, which is used to simulate the results for subjects in
the placebo arm. A value of φ = 0.725 provides a 6-month probability of
success of 0.30. Note that the methodology used to simulate patients does
not aﬀect the design itself; any approach can be used to simulate subjects.
We now provide a general structure to test the design. An accrual rate of
100 subjects per year is assumed. In each case, 10,000 simulated trials are
conducted. We ﬁrst simulate trials from the null hypothesis, where the true
probability of success, ASIA C or better at 6 months, is 0.10 for all four
experimental arms and the placebo. Figure 5.8 presents the proportion of
simulated trials that select a target dose in Stage 1 and advance to Stage
2, with an outcome of superiority of the target dose at the completion of
Stage 2. This would represent a Type I error for the combined trial.
We select the nominal value of 0.014 for the primary ﬁnal analysis. This

PHASE II/III SEAMLESS TRIALS
237
Dose
p
nI
Pr(Go)
Pr(Win)
0
.10
29.3
–
–
1
.10
21.8
.034
.005
2
.10
14.6
.012
.004
3
.10
13.9
.014
.005
4
.10
17.7
.025
.007
Total
–
97.3
.085
.021
Table 5.13 Operating characteristics of the seamless phase II-III design under the
null hypothesis.
value controls the Type I error based on this and other simulations of null
scenarios. Figure 5.8 shows that the Type I error inﬂation of the study is
not large. Based on the general pattern of the Type I error as a function
of the nominal value, there is about a 1% diﬀerence between the simulated
Type I error of the adaptive design and its nominal value. Despite the
design’s ability to select a high-performing arm in Stage 1 of the trial, this
“cherry picking” does not lead to a substantial increase in the Type I error.
In part this is because a dose-response model is used for the four doses. This
reduces the inﬂation of any arm’s performance; the model uses information
from neighboring doses and thus controls the multiplicities better than four
paired comparisons. Additionally, the size of Stage 2, 100 per arm, is quite
a bit larger than the size of Stage 1, which rarely places more than 40
subjects on a single dose.
Tables 5.13–5.17 present some operating characteristics of the seamless
phase II-III design for various null and non-null scenarios. In all of these
tables, the p column reports the true probability of 6-month outcomes of
ASIA C or better for each dose, while the nI column reports the mean
sample size per arm in Stage 1. The “Pr(Go)” column reports the proba-
bility of each dose being selected as the target dose and advancing to Stage
2 of the design. For placebo (Dose 0), the Total Pr(Go) is the probability
it is used in Stage 2, since it moves on only if the trial itself continues on.
Finally, the “Pr(Win)” column is the probability that a dose is selected as
the target dose and is shown to be superior to placebo in the ﬁnal analysis.
Table 5.13 summarizes the results under the null hypothesis. The mean
sample size of Stage 1 of the trial is 97.3. The probability that Stage 1
stops for futility is 0.662, with a 0.253 probability of Stage 1 going to
the maximum sample size of 150 and not advancing to Stage 2. The trial
advances to Stage 2 of the design with a probability of just 0.085. This
represents an error of the Stage 1 aspect of the design. With this type of
dose-ﬁnding design and small sample size, this is a very low Type I error

238
PHASE III STUDIES
Dose
p
nI
Pr(Go)
Pr(Win)
0
.10
32.0
–
–
1
.11
20.8
.040
.006
2
.12
15.1
.029
.014
3
.13
15.7
.039
.022
4
.15
21.4
.098
.074
Total
–
104.9
.206
.116
Table 5.14 Operating characteristics of the seamless phase II-III design for a case
where the treatment has a very minor advantage over the placebo.
for the “phase II” aspect of this design. The probability of an arm being
selected as the target dose is reported in the table. Dose 1 has the highest
probability of being selected because of the desire to ﬁnd the ED90; the
probability for the other doses is slightly smaller. The total probability of
a dose being selected as the target dose, advancing to Stage 2 and being
found superior to the placebo is just 0.021. This satisﬁes a restriction that
the entire combined phase II/III trial have a Type I error (one-sided) less
than 0.025. Incidentally, Dose 0 (placebo) can never “win” (as indicated
by the dash in the Pr(Win) column for this dose), but it does go on to
phase III if the trial does (in this case, with probability 0.085), despite the
presence of a dash in the Pr(Go) column.
We now present the results for several scenarios in which there is at
least one positive experimental treatment arm. Table 5.14 presents the
operating characteristics for a case in which there is a very small beneﬁt
to the experimental arms. The maximal beneﬁt is for the largest dose, a
15% success rate at 6 months. While this represents a better treatment
than the placebo, the beneﬁt is not large enough to see a predictive power
near 80%. The design does a good job of understanding the minor beneﬁt
and limiting the probability of advancing to Stage 2 to 0.206. Stage 1 stops
for futility in 47% of the trials, with 32.2% advancing to the full 150 and
then not advancing to Stage 2. The combined probability of ﬁnding the
target arm superior to the control is 0.116. While advancing to Stage 2 is
an undesired outcome for this case, about 50% of these times it chooses the
largest dose, Dose 4, which is the most eﬀective of the experimental doses.
Table 5.15 gives the simulation results for a scenario in which there is
a strong treatment eﬀect. The doses get more eﬀective as they increase,
resulting in Dose 4 being the most eﬀective (and the ED90) with a true
success rate of 30%. Doses 2 and 3 are also positive doses, but not as good
as Dose 4. Dose 4 is selected as the ED90 and advances to Stage 2 in 46%
of the trials (all of which ultimately lead to trial success). The second-best

PHASE II/III SEAMLESS TRIALS
239
Dose
p
nI
Pr(Go)
Pr(Win)
0
.10
28.6
–
–
1
.15
14.6
.049
.017
2
.20
13.2
.080
.074
3
.25
15.5
.172
.170
4
.30
21.4
.459
.459
Total
–
93.3
.760
.720
Table 5.15 Operating characteristics of the seamless phase II-III design for a
case where the treatment has linearly increasing eﬀectiveness with dose, to a very
eﬀective largest dose.
Dose
p
nI
Pr(Go)
Pr(Win)
0
.10
24.2
–
–
1
.30
16.5
.285
.282
2
.30
13.2
.191
.191
3
.30
12.6
.176
.176
4
.30
14.8
.246
.246
Total
–
81.4
.899
.896
Table 5.16 Operating characteristics of the seamless phase II-III design for a case
where each of the doses has strong eﬃcacy.
dose, Dose 3, is selected as the ED90 and advances to Stage 2 in 17% of the
simulations. Combined, a target dose is selected to move to Stage 2 in 76%
of all the trials, with another 15.6% of the trials running to the maximum
sample size of 150 in Stage 1 and not advancing to Stage 2.
Table 5.16 presents the simulation results for a case in which all four of
the experimental doses have a 6-month success rate of 0.30. The true ED90
is Dose 1. This dose is selected in 28.5% of the trials to advance to Stage
2. Altogether, there is an 89.9% chance of a target dose being selected and
advancing seamlessly to Stage 2. When a dose is selected as the target dose
to move forward, it is almost always successful in showing superiority (just
0.3% of trials went to Stage 2 and did not show superiority). While 89.9%
of trials advance seamlessly to Stage 2, 7.9% run to the Stage 1 cap of 150
and do not advance to Stage 2. When a seamless shift is not made, after
full follow-up of these trials, the decision could be made to conduct a phase

240
PHASE III STUDIES
Dose
p
nI
Pr(Go)
Pr(Win)
0
.10
31.3
–
–
1
.15
19.5
.077
.027
2
.30
22.2
.435
.435
3
.15
13.1
.038
.028
4
.15
15.5
.061
.043
Total
–
101.5
.612
.533
Table 5.17 Operating characteristics of the seamless phase II-III design for a case
where there is an inverted U-shaped dose-response curve.
III trial. While the advantage of the seamless aspect is gone, this worst
case scenario is quite similar to simply not running a seamless trial.
In the last case (Table 5.17), one quite positive dose (Dose 2) has a
30% success rate, while each of the remaining experimental arms has a less
impressive 15% success rate. This is a very challenging scenario for this
type of design because the best dose is an internal dose. The Bayesian dose-
response model “shrinks” the results of the neighboring doses to the middle
dose, thus making it harder to ﬁnd that a lone internal dose is eﬀective (this
is the same strength that prevents Type I errors from occurring during
Stage 1). Stage 1 is eﬀective at placing more subjects on Dose 2, with
the largest mean sample size of all experimental doses, 22.2, more than
50% larger than that of Dose 3. Dose 2 is selected as the target dose and
advances to Stage 2 in 43.5% of the simulated trials. It then successfully
demonstrates superiority in virtually all of these cases. All told there is a
probability of 0.612 that a target dose is selected and advances to Stage
2, with Dose 2 making up 71% of these trials. The rate of trials reaching
the 150 cap in Stage 1 and not advancing to Stage 2 is 21.2%, which not
surprisingly is the largest of any of the cases presented here.
As a ﬁnal remark, note that even after full accrual of all subjects not
advancing to Stage 2, an analysis may reveal the eﬃcacy of Dose 2, at
which time an independent, traditional phase III trial can be conducted.
The planning of a seamless phase II/III design enables the seamless shift to
happen, but it does not in any way prevent a traditional phase III trial from
being run. The seamless trial presented in this Section was constructed to
be conservative; the hurdle for advancing to Stage 2 was reasonably high.
The thought here was that if the seamless shift occurs, it was because the
decision was pretty clear cut. More aggressive designs may be desirable
here as well.

CASE STUDY: ABLATION DEVICE TO TREAT ATRIAL FIBRILLATION
241
5.8 Case study: Ablation device to treat atrial ﬁbrillation
In this ﬁnal section we present a case study based on a conﬁrmatory trial
for the NaviStar ThermoCoolT M catheter. This is a medical device for the
treatment of atrial ﬁbrillation, an abnormal heart rhythm in which the
heart’s two upper chambers (atria) merely “quiver,” instead of contracting
in coordination with the lower chambers. During treatment, the source
of the patient’s heart arrhythmia is mapped, localized, and then destroyed
(ablated) by applying radiofrequency energy through the catheter to create
a small scar in the the oﬀending area that is electrically inactive, rendering
it incapable of generating arrhythmias. Additional details of the device
as well as detailed trial information are available in Wilber et al. (2010).
The trial was a Bayesian adaptive design with multiple looks for selecting
the appropriate sample size, and with the possibility of making a claim of
success before full follow-up was reached. Some of the aspects of the design
must be kept conﬁdential, but much of the detail is presented here.
Subjects with paroxysmal (recurrent but terminating in less than 7 days)
atrial ﬁbrillation were enrolled. The experimental group received treat-
ment via the ThermoCool catheter, while the control group received an-
tiarrhythmic drug therapy, the standard of care. Patients were randomized
7:4 in favor of the treatment group. The randomization ratio remained
ﬁxed throughout the trial. The primary endpoint for the study is a di-
chotomous endpoint, chronic success, deﬁned as freedom from documented
symptomatic atrial ﬁbrillation and no changes in the anti-arrhythmia drug
regimen during a 9-month eﬃcacy window.
The primary outcome of the study is a dichotomous endpoint, but the
endpoint is achieved by being free of “failure” during a 9-month window.
Let pT and pC be the probability of a chronic success for a subject in the
treatment group and control group, respectively. The prior distributions for
each of the probabilities are taken as independent Beta(1, 1) distributions.
The treatment will be deemed superior to the control group if the posterior
probability of superiority is at least 0.98; i.e., if
Pr (pT > pC|Data) ≥0.98 .
The value of 0.98 was selected through simulation of the design to demon-
strate the control of Type I error at the one-sided 0.025 level.
The trial design calls for a minimum accrual of 150 subjects. When the
150th subject has been accrued, an interim analysis is conducted. If reached,
additional interim analyses take place at 175 and 200 subjects accrued, and
if accrual continues the study stops at the maximum sample size of 230
subjects. The rules at each of the interim looks are as follows:
Algorithm 5.8 (Atrial ﬁbrillation (AF) trial design).
Step 1. Expected Success: If the predictive probability of showing su-
periority with the current sample size is at least 0.90 for the 150-subject

242
PHASE III STUDIES
look or 0.80 for the 175- and 200-subject looks, then accrual stops for
expected success.
Step 2. Futility: If the predictive probabilities of success for the current
and maximum sample sizes (230) are each less than 0.01, then the trial
stops for futility.
Step 3. Early Success: If the predictive probability of success for the
current sample size is larger than 0.99, then the trial stops for expected
success; immediate success is claimed, and an application is ﬁled for
immediate approval. In addition, if this early success condition is not
met at the time accrual is stopped, an additional look for early success
takes place 4.5 months after stopping accrual.
At the time of an interim analysis there will be subjects who have com-
plete data, meaning their failure time is known or they have completed the
9-month period failure-free. There will be subjects who have interim time
in the study without failing, and subjects who have no time in the study
(this includes possibly future accrued subjects). We construct a model for
the time until failure, which is then used for the predictive probabilities of
subject and trial success. These predictive probabilities are used to guide
the sample size determination, but have no eﬀect on the primary analysis
when the study is complete, except for the possible early analysis claim of
success.
The dichotomous outcome of success and failure will be analyzed as de-
scribed in the primary analysis section. However, analyzing the data at
interim time points requires modeling the time-to-event data. The occur-
rence of a chronic failure is modeled as a time-to-event (failure) over the
9-month time period. For the 9-month eﬃcacy evaluation period we model
the hazard rates separately in each treatment group (the d index; 1 is con-
trol and 2 is treatment) for t in months. The time to chronic failure is
assumed to be piecewise exponential with hazard rates
Hd(t) =



θd,1
0 < t ≤0.5
θd,2
0.5 < t ≤2
θd,3
2 < t ≤9
.
A hierarchical prior distribution is used for the hazard rates within each
interval, within each treatment group. The prior distributions are
θd,j ∼Gamma (αd, βd) for d = 1, 2, j = 1, 2, 3 .
The independent hyperpriors are
αd ∼Exponential (1) , for d = 1, 2 ,
and
βd ∼Exponential (1) , for d = 1, 2 .

CASE STUDY: ABLATION DEVICE TO TREAT ATRIAL FIBRILLATION
243
These prior distributions were selected in discussions with regulatory agents.
A typical interaction involves a proposed prior distribution and its oper-
ating characteristics. In this particular case, because this is the primary
success analysis, it was desired and proposed by the sponsor that the
prior distribution have little eﬀect on the overall results. Regulatory agents
agreed that such a prior distribution was appropriate. The operating char-
acteristics of the design are calculated using this prior structure, so the
ramiﬁcations of this prior are well understood.
Because accrual is slow relative to the 9-month follow-up period, we
expect a reasonable number of subjects in each treatment group to have
progressed through the full 9-month interval, and thus the prior distribu-
tions we explored for the hazard rates were not very important. In these
circumstances, prior distributions for the parameters of the longitudinal
model can be quite important when there is not a large number of subjects
with complete data. It is not uncommon to use longitudinal models when
it is expected there will be no subjects having complete data at the time
of at least one interim analysis. In such cases, a strong prior for the lon-
gitudinal parameters is critical. Fortunately, in all the simulations of this
particular trial, the accrual rate was slow enough that there were ample
data to inform the longitudinal models. Therefore, in this trial, the priors
for the longitudinal model also did not play an important role.
The posterior distributions of each of the parameters are updated based
on each subject having greater than 0 exposure time during the primary
follow-up period. Based on this modeling, the predictive distribution for
each subject with no data or with partial data are deﬁned. These predictive
distributions are combined to ﬁnd the predictive probability of trial success.
At an interim analysis the predictive distributions are calculated using
MCMC. The approach successively simulates values from each of the model
parameters as well as for each of the ﬁnal observations of chronic success
for each subject. Let Xi be the exposure time, Fi an indicator of whether a
failure occurred, and di the treatment arm for subject i. If a subject reaches
their full follow-up without a failure, they are labeled as X = 9 and F = 0.
Letting Edj and Ydj be the current exposure and the number of failures,
respectively, within interval j for treatment d, our MCMC algorithm is as
follows:
Algorithm 5.9 (AF trial MCMC algorithm).
Step 0: Select initial values for each of the parameters θ1,1, θ1,2, θ1,3, θ2,1,
θ2,2, θ2,3, α1, β1, α2, and β2
Step 1: For each hazard rate, θ, simulate an observation from its full con-
ditional distribution,
θd,j|E, Y, αd, βd ∼Gamma (αd + Ydj, βd + Edj)
Step 2: Simulate a value of αd, d = 1, 2 from its full conditional distribu-

244
PHASE III STUDIES
tion,
αd|βd, θd,1, θd,2, θd,3 ∼
· βαd
d
Γ (αd)
¸3 Ã 3
Y
i=1
θd,i
!αd−1
exp (−αd)
Step 3: Simulate a value of βd, d = 1, 2 from its full conditional distribu-
tion,
βd|αd, θd,1, θd,2, θd,3 ∼Gamma
Ã
1 + 3αd, 1 +
3
X
i=1
θd,i
!
Step 4: For each subject with incomplete data, Xi < 9 and Fi = 0, simu-
late a predictive value of Xi from the piecewise exponential model with
parameter (θdi,1, θdi,2, θdi,3) and conditional on no event at time Xi.
Step 5: For the complete data X1, ...Xn, where n is the current sample
size, evaluate whether the data constitutes a primary endpoint success.
For the maximum sample size of 230, evaluate whether X1, . . . , X230
constitutes a success on the primary endpoint.
The iteration of steps 1 through 5 is done B + M times, where the ﬁrst B
observations are discarded as burn-in values. For the simulations, typical
values of B = 1000 and M = 5000 are used.
The proportion of times that success is achieved with the current sample
size estimates the predictive probability of success with the current sample
size, while the proportion of times success is achieved with the maximum
sample size estimates the predictive probability of success at the maximum
sample size. These predictive probabilities take in to account several sources
of variability. The data for subjects with complete information remain ﬁxed,
but the data for those subjects without complete information are simulated
using the Bayesian model. Therefore the predictive probability accounts for
the natural variability of the future data. This model integrates over the
uncertainty of the parameters of this model, namely, the hazard rates. Thus
the predictive distribution accounts for the uncertainty in the longitudinal
model as well.
In order to characterize the behavior of the design, we once again per-
form simulations. In order to simulate the design it is necessary to create
“virtual subjects.” The design incorporates the time-to-failure, as well as
the dichotomous endpoint of 9-month failure. We simulate subject failure
using the piecewise exponential longitudinal model structure (though ad-
ditional simulations using diﬀerent methods were also done). The default
parameters assumed are θ∗
1 = 0.65, θ∗
2 = 0.161, and θ∗
3 = 0.05. The default
probability of 9-month success is
exp (−0.5θ∗
1 −1.5θ∗
2 −7θ∗
3) = 0.40 .

CASE STUDY: ABLATION DEVICE TO TREAT ATRIAL FIBRILLATION
245
pT
pC
Pr(S)
Pr(F)
Sample
Time
Sample Size
Size
150
175
200
230
0.20
0.20
0.019
0.92
158.1
35.4
.01
.01
.00
.06
0.004
(21.5)
(7.2)
.83
.05
.03
0.40
0.40
0.021
0.92
158.2
35.4
.01
.01
.00
.07
0.004
(21.4)
(7.2)
.84
.05
.03
0.60
0.60
0.023
0.91
158.3
35.4
.01
.01
.00
.07
0.012
(21.6)
(7.2)
.83
.05
.03
Table 5.18 Operating characteristics for three null hypotheses for the ThermoCool
catheter trial. The ﬁrst two columns report the assumed probability of chronic suc-
cess in each treatment arm. The Pr(S) and Pr(F) columns report the probabilities
of success for the primary endpoint and the probability of stopping for futility. The
lower cell in the Pr(S) column is the probability of claiming success earlier than
full follow-up. The ﬁfth and sixth columns report the mean (standard deviation)
of the sample and time of the trial in months. The last four columns report the
probability of stopping at each sample size for expected success (upper cell) and
futility (lower cell).
In order to simulate from any arbitrary probability of chronic success, 0 <
p < 1, a value of δ is selected such that
δ = log
µ log p
log 0.4
¶
.
The value of δ is then used to alter each individual θ as
θj = exp (δ) θ∗
j .
This creates a case where the probability of success for the simulated sub-
jects is p. The default accrual rate used in the simulation is 2 per month for
the ﬁrst two months, 3 per month the next two months, and 5 per month
starting in the ﬁfth month. Relative to the speed of the observations of the
endpoint, the accrual rate is reasonably slow.
Using the above assumptions we simulate the adaptive trial. At each
planned interim analysis the steps are carried out and the results of the
simulated trials are recorded. Table 5.18 presents the results of 25, 000
simulated trials in which the treatment and control probability of chronic
success are the same. In each case it is an error to claim superiority. These
three cases represent “null” cases.
For each of these three scenarios the simulated Type I error is less than
the one-sided 0.025. Numerous other null hypotheses were simulated, in-
cluding diﬀerent assumptions on the time-to-failure, accrual rates, and dif-
ferent probabilities. For each of these null scenarios the probability of con-

246
PHASE III STUDIES
pT
pC
Pr(S)
Pr(F)
Sample
Time
Sample Size
Size
150
175
200
230
0.30
0.20
0.316
0.46
182.3
44.7
.12
.15
.10
.30
0.090
34.6
12.0
.34
.07
.05
0.40
0.20
0.845
0.06
175.9
44.1
.48
.17
.09
.20
0.429
31.6
10.6
.03
.01
.01
0.45
0.20
0.959
0.01
164.2
39.9
.69
.14
.07
.09
0.637
25.2
9.1
.01
.00
.00
0.50
0.20
0.993
0.00
155.9
36.4
.85
.09
.03
.03
0.813
16.2
6.5
.00
.00
.00
0.50
0.40
0.260
0.53
178.9
43.2
.10
.14
.10
.26
0.114
33.9
11.6
.41
.07
.05
0.60
0.40
0.785
0.10
177.7
44.9
.43
.17
.11
.22
0.644
32.2
8.7
.06
.02
.02
0.65
0.40
0.934
0.02
166.3
42.5
.66
.15
.08
.11
0.861
26.6
6.5
.01
.01
.00
0.70
0.40
0.989
0.00
156.5
40.2
.84
.09
.03
.03
0.961
17.3
4.0
.00
.00
.00
Table 5.19 Operating characteristics for eight cases where the device is assumed
superior to the control, for the ThermoCool catheter trial. The ﬁrst two columns
report the assumed probability of chronic success in each treatment arm. The
Pr(S) and Pr(F) columns report the probabilities of success for the primary end-
point and the probability of stopping for futility. The lower cell in the Pr(S)
column is the probability of claiming success earlier than full follow-up. The ﬁfth
and sixth columns report the mean (standard deviation) of the sample and time
of the trial in months. The last four columns report the probability of stopping at
each sample size for expected success (upper cell) and futility (lower cell).
cluding superiority, whether at full follow-up or an early claim of success,
was less than 0.025. The ﬁnal success threshold was varied in order to ﬁnd
the smallest value that resulted in Type I error probabilities less than 0.025
for every null case (0.98).
Table 5.19 reports the operating characteristics for cases in which the
control arm has a 0.20 or 0.40 probability of success and the treatment
arm varies from a 0.10 to 0.30 probability incremental improvement. The
probabilities of detecting an additive improvement of 0.20 over a baseline
of 0.20 or 0.40 are 0.845 and 0.785, respectively. These powers increase to
0.959 and 0.934 when the advantage enjoyed by the treatment arm increases
to 0.25. In the speciﬁc case where the treatment has a 45% success rate
and the control has a 20% success rate, the mean sample size is 164.2 with

APPENDIX: R MACROS
247
a standard deviation of 25.2. The probability of the trial stopping at the
ﬁrst look is 0.69, with only a 0.09 probability of advancing to the maximum
sample size of 230. The probability of achieving an early ﬁnding of success
is a robust 0.637.
The Bayesian adaptive design described here helped the device’s sponsor
(NaviStar) successfully navigate the approval process with the FDA. The
results of the trial were announced publicly during an FDA advisory panel
meeting in November 2008. The ﬁrst interim analysis took place in July
2007, and the predictive probability of success for the current sample size
(159 subjects) was greater than 0.999, thus resulting in a stopping of accrual
and an immediate claim of superiority. The ﬁnal posterior probability of
superiority was greater than 0.999, which is larger than the goal cutoﬀof
0.98. The ﬁnal Kaplan-Meier estimated success rates were 64% and 16%,
for the treatment and control groups, respectively. In February 2009, the
FDA approved the catheters for the treatment of atrial ﬁbrillation.
5.9 Appendix: R Macros
The online supplement to this chapter
www.biostat.umn.edu/~brad/software/BCLM_ch5.html
provides the R code that was used to illustrate the examples in this section.

CHAPTER 6
Special topics
In this chapter we discuss several important special topics that do not
neatly ﬁt into our earlier chapters, but are nonetheless important in the
actual practice of Bayesian clinical trials. By necessity, our views reﬂect our
own experience and interest to some extent, but the issues in this chapter
do seem to come up fairly regularly, in both in-house studies as well as later-
phase trials where working cooperatively with regulatory agencies comes
to the fore.
6.1 Incorporating historical data
As seen earlier in this text, Bayesian clinical trial designs oﬀer the possi-
bility of a substantially reduced sample size, increased statistical power,
and reductions in cost and ethical hazard. However when prior and current
information conﬂict, Bayesian methods can lead to higher than expected
Type I error, as well as the possibility of a costlier and lengthier trial. This
motivates an investigation of the feasibility of hierarchical Bayesian meth-
ods for incorporating historical data that are adaptively robust to prior
information that reveals itself to be inconsistent with the accumulating
experimental data.
In this section, we begin with a fairly standard hierarchical model that al-
lows sensible borrowing from historical controls, but in a way that requires
the user to be fairly explicit about the degree of borrowing. We then go
on to present novel modiﬁcations to the traditional power prior approach
(Ibrahim and Chen, 2000) that allows the commensurability of the infor-
mation in the historical and current data to determine how much historical
information is used. Power priors oﬀer a simple way to incorporate and
downweight historical data, by raising the historical likelihood to a power
α0 ∈[0, 1], and restandardizing the result to a proper distribution. These
priors have been applied in a variety of contexts, including the sample size
estimation problem by DeSantis (2007). We compare the frequentist per-
formance of several methods using simulation, and close with an example
from a colon cancer trial that illustrates the beneﬁt of our proposed adap-
tive borrowing approach. The commensurate prior design produces more

250
SPECIAL TOPICS
precise estimates of the model parameters, in particular conferring statisti-
cal signiﬁcance to the observed reduction in tumor size for the experimental
regimen as compared to the control regimen in our example.
6.1.1 Standard hierarchical models
We begin with a hierarchical model for a doubly controlled clinical trial
deﬁned as follows. Suppose that historical data exist for both the treatment
and control groups. Let g = 0, 1 indicate group (historical or current), and
let i = 1, . . . , ng index the patients in each group. The full hierarchical
model might look like
Likelihood:
Ygi
ind
∼
N(θg + βgxgi , σ2
g)
where xgi
=
½ 0
if patient gi received control
1
if patient gi received treatment
Prior:
θg
iid
∼
N(µθ, τ 2
θ )
and
βg
iid
∼N(µβ, τ 2
β) .
Consider how the parameters of this model control prior shrinkage. If τ 2
θ =
0, then θg = µθ for all g, and we have no borrowing among the control
groups. On the other hand, if τ 2
β = 0, then βg = µθ for all g, and we have
no borrowing among the treatment groups.
Next, we need a hyperprior speciﬁcation to complete the model. We
take ﬂat hyperpriors on the mean parameters µθ and µβ, since for most
datasets these parameters will be well-estimated by the data. However, for
the variances, we work (like WinBUGS) on the precision scale, and assume
ηθ ∼G(aθ, bθ)
and
ηβ ∼G(aβ, bβ) ,
where η = 1/τ 2. Thus, in the treatment group, if aβ = 1000 and bβ = 10, ηβ
is approximately 100 and hence τβ is approximately 0.1, a high-shrinkage
hyperprior. If instead aβ = 40 and bβ = 4, this is a vaguer prior having
ηβ ≈10, i.e., τβ ≈0.3, a moderate-shrinkage hyperprior. Finally, if aβ =
bβ = ϵ = 0.1, the hyperprior is vague and the data must do all the work,
a low-shrinkage hyperprior. Similar statements enabling diﬀering levels of
shrinkage assigned to the control group are possible via aθ and bθ.
To evaluate the quality of this hierarchical model setting, consider the
simulation of its operating characteristics. Suppose we take n0 = n1 = 20,
and without loss of generality take σ2
g = 1. We can simulate frequentist
power under a variety of scenarios. For instance, we might set θ0 = θ1 = 0
and β0 = β1 = 0. This corresponds to complete homogeneity; there is no
reason not to borrow from the historical data in both the treatment and
control groups. Alternatively, we could take θ0 = θ1 = 0 but β0 = 0, β1 =
2. Here we specify slight heterogeneity across treatment groups, so that
borrowing is somewhat suspect. Finally, if we were to take θ0 = 0, θ1 = 30,
and β0 = 0, β1 = 2, this would correspond to enormous heterogeneity

INCORPORATING HISTORICAL DATA
251
0.2
5.1
0.1
5.0
0.0
0.0
0.2
0.4
0.6
0.8
1.0
true beta_1
power
low shrink, G( 0.1 , 0.1 )
mid shrink, G( 40 , 4 )
high shrink, G( 1000 , 10 )
0.2
5.1
0.1
5.0
0.0
0.0
0.2
0.4
0.6
0.8
1.0
true beta_1
power
low shrink, G( 0.1 , 0.1 )
mid shrink, G( 40 , 4 )
high shrink, G( 1000 , 10 )
Figure 6.1 Power curve for treatment eﬀect in current trial (β1): (a) true
(θ0, θ1, β0) = (0, 30, 0); (b) true (θ0, θ1, β0) = (0, 30, 2).
across control groups, and slight additional heterogeneity across treatment
groups; borrowing would now be very suspect. In each case, we could lay out
a grid of “true” β1 values, choose the shrinkage level in the treatment and
control group hyperpriors, and simulate frequentist power under various
hypotheses.
Example 6.1 (Test for a treatment eﬀect in the current trial). Consider
comparing the hypotheses H0 : β1 = 0 and Ha : β1 ̸= 0, where we use
the decision rule, “Reject H0 if the central 95% credible interval for β1
excludes 0.” Figure 6.1 gives the power over a grid of β1 values assuming
θ0 = 0, θ1 = 30, and β0 = 0 (left panel) vs. β0 = 2 (right) under the high,
moderate, and low shrinkage priors in the treatment group (the moderate
shrinkage prior was used for the θ’s in the control group). Note that the
high-shrinkage hyperprior (dotted line) does well when β0 = 0 (left panel)
even though θ1 ̸= 0, but has high Type I error when β0 ̸= 0 (right panel).
The performance of the low shrinkage hyperprior (solid line) is almost un-
aﬀected by the true value of β0, showing good power and Type I error
behavior superior to that of the other two hyperpriors.
Example 6.2 (Test whether “to pool or not to pool”). Consider now an
FDA applicant who wishes to know if she may pool her historical and
experimental data in a drug or device approval study. We might now deﬁne
∆= β1 −β0, and test the hypotheses H0 : ∆∈(−c, c) for some c > 0,
versus Ha : ∆/∈(−c, c). It is now convenient to use a decision rule of the
form, “Reject H0 if P(∆∈(−1, 1)|y) < K,” for some prespeciﬁed posterior
coverage level K.
The power curves over a β1 grid are given in Figure 6.2, which assumes

252
SPECIAL TOPICS
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
true beta_1
power
low shrink, G( 0.1 , 0.1 )
mid shrink, G( 40 , 4 )
high shrink, G( 1000 , 10 )
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
true beta_1
power
low shrink, G( 0.1 , 0.1 )
mid shrink, G( 40 , 4 )
high shrink, G( 1000 , 10 )
Figure 6.2 Power curves for treatment eﬀect diﬀerence ∆: (a) true (θ0, θ1, β0) =
(0, 0, 0); (b) true (θ0, θ1, β0) = (0, 0, 2).
c = 1, K = 0.80, θ0 = θ1 = 0, and β0 = 0 (left) vs. β0 = 2 (right) for
the three treatment group hyperpriors and under the low shrinkage prior
for the θ’s. Here the high shrinkage prior stubbornly refuses to reject H0
unless β1 is quite far from the null value. The low shrinkage prior has good
power but perhaps slightly too much Type I error (0.18 and 0.25 in the two
panels, respectively); overall the moderate shrinkage prior oﬀers a sensible
compromise.
Repeating these sorts of simulations over a ﬁne grid of θ0, θ1, β0, and
β1 values should permit the development of guidelines for how tightly the
hyperprior can be set in a given model setting, so that borrowing from
historical data happens at eﬀective yet not overly Type I error-prone levels.
We could also compare these approaches head-to-head with our power prior
methods below, to help attach more meaning to the hyperparameters that
control the degrees of borrowing in each setting.
6.1.2 Hierarchical power prior models
A second, possibly more useful approach to incorporating historical data is
through power priors. Adopting the notation of Ibrahim and Chen (2000),
denote the historical data by D0 = (n0, x0), where n0 denotes the sample
size and x0 the n0 × 1 response vector, and similarly denote data from
the current study by D = (n, x). Let L(θ|D0) denote the general likeli-
hood function of the historical data. Then the conditional power prior for
parameter θ is deﬁned as
π(θ|D0, α0) ∝L(θ|D0)α0π0(θ),
(6.1)

INCORPORATING HISTORICAL DATA
253
where π0(θ) is referred to as the initial prior, since it represents prior knowl-
edge about θ before D0 is observed, and α0 ∈[0, 1] is the power parameter
that controls the “degree of borrowing” from the historical data. If α0 = 0,
(6.1) reduces to the initial prior (no borrowing), whereas if α0 = 1, equation
(6.1) returns the usual historical posterior (full borrowing).
In the case of normal historical data, x0i
iid
∼N(θ, σ2
0), σ2
0 known, i =
1, . . . , n0, under a ﬂat initial prior, (6.1) yields a N
¡
¯x0, σ2
0/(α0n0)
¢
power
prior distribution for θ. Hence α0 plays the role of a relative precision
parameter for the historical data. Since 0 ≤α0 ≤1, we might also think
of α0n0 as the “eﬀective” number of historical controls being incorporated
into our analysis. Ibrahim and Chen (2000) introduced power priors to the
broad statistical community, and illustrated their usefulness in a variety of
settings; see also Ibrahim et al. (2003) and Chen and Ibrahim (2006).
If we are willing to specify a particular value for α0, the conditional
posterior distribution for θ given D0, D, and α0 emerges as
q(θ|D0, D, α0) ∝π0(θ)L(θ|D0)α0L(θ|D) .
(6.2)
Again in the case of known-variance normal observations, xi
iid
∼N(θ, σ2), i =
1, . . . , n, this results in another normal distribution for the posterior of θ.
We may be able to use the power parameter’s interpretation as “im-
portance of each historical patient relative to each new patient” to select a
value for α0 (say, 1/2 or 1/3) for approximately Gaussian likelihoods. More
commonly, however, we are uncertain as to the degree to which our new
data will agree with the historical data, and thus are somewhat reluctant
to prespecify the degree of borrowing. In such cases, we can enable the data
to help determine probable values for α0 by adopting the usual Bayesian
solution of choosing a hyperprior π(α0) for α0.
Ibrahim and Chen (2000) propose joint power priors proportional to the
product of the conditional power prior in (6.1) and an independent proper
prior on α0,
πIC(θ, α0|D0) ∝π0(θ)π(α0)L(θ|D0)α0.
(6.3)
Duan et al. (2006, p. 98) and Neuenschwander et al. (2009) caution against
(6.3) since it violates the Likelihood Principle (Subsection 2.2.3). To see
this, note that if we use the likelihood of a suﬃcient statistic for θ instead of
the entire random sample, we may obtain disparate joint power priors. For
example, under normality,
£Qn0
i=1 Normal(x0i|θ, σ2
0)
¤α0 π0(θ)π(α0) is not
proportional to Normal( ¯x0|θ, σ2
0
n0 )α0π0(θ)π(α0) with respect to α0, since
¡
2πσ2
0
¢−α0n0
2
̸=
³
2πσ2
0
n0
´−α0
2 . Therefore, multiplying the historical likeli-
hood by a constant under the α0 exponent alters the amount of information
in the power prior.
If we specify π(α0) as a Beta(a, b) distribution for ﬁxed positive hy-
perparameters a and b, the joint posterior for θ and α0 given D and D0

254
SPECIAL TOPICS
becomes
qIC(θ, α0|D0, D) ∝αa−1
0
(1 −α0)b−1π0(θ)L(θ|D0)α0L(θ|D) .
(6.4)
In this setting, we can use the hyperparameters (a, b) to control the likely
degree of borrowing; for example, (a = 10, b = 1) would strongly encourage
borrowing.
Duan et al. (2006) modify the joint power prior to the product of the
normalized conditional power prior (6.1) and an independent proper prior
for α0, namely
πD(θ, α0|D0) ∝
L(θ|D0)α0π0(θ)
R
L(θ|D0)α0π0(θ)dθπ(α0).
(6.5)
Modiﬁed power priors obey the Likelihood Principle, and marginal poste-
riors for α0 under modiﬁed power priors typically emerge as proportional
to products of familiar probability distributions. Duan et al. (2006, p.98)
propose modiﬁed power priors as an improvement with respect to unwar-
ranted attenuation of historical data. Hobbs et al. (2009) present plots that
oﬀer mild support for this claim assuming small sample sizes, normal data,
and Beta(1, 1) hyperpriors. Furthermore, modiﬁed power priors tend to
produce marginal posteriors for α0 that are less skewed in these scenar-
ios. This all suggests that marginal posteriors of α0 derived from modiﬁed
power priors may be less sensitive to the power parameter hyperprior than
their Ibrahim-Chen counterparts. If we again specify π(α0) = Beta(a, b)
for ﬁxed positive a and b, the joint posterior for θ and α0 given D and D0
replaces (6.4) in the Ibrahim-Chen approach with
qD(θ, α0|D0, D) ∝αa−1
0
(1 −α0)b−1
L(θ|D0)α0π0(θ)
R
L(θ|D0)α0π0(θ)dθL(θ|D) .
(6.6)
Commensurate power priors
A problem with the joint power priors above is that they do not directly
parameterize the commensurability of the historical and new data. For
example, in (6.4) or (6.6) note that the full conditional distribution for α0
would be free of the current data D, since the current likelihood would
be nothing but a multiplicative constant. Furthermore, Duan et al. (2006),
Neelon et al. (2008), and Neelon and O’Malley (2010) all caution against
using Ibrahim-Chen and modiﬁed power priors since they both tend to
overattenuate the impact of the historical data, forcing the use of fairly large
α0 (or fairly informative hyperpriors for α0) in order to deliver suﬃcient
borrowing.
Suppose we assume diﬀerent parameters in the historical and current
group, θ0 and θ, respectively. This bivariate parameterization allows us
to extend the hierarchical model to include a parameter that directly mea-
sures the commensurabilty (similarity) of θ and θ0. Suppose we pick a vague

INCORPORATING HISTORICAL DATA
255
(or even ﬂat) initial prior π0(θ0), but construct the prior for θ to be de-
pendent upon θ0 and τ, where τ parameterizes commensurability. We use
the information in τ to guide the prior on α0. Specifying a vague prior
for τ and normalizing with respect to θ and θ0 results in a power prior
πC(θ0, θ, α0, τ|D0) proportional to
(L(θ0|D0) × p(θ|θ0 , τ))α0
R R
(L(θ0|D0) × p(θ|θ0 , τ))α0 dθ0dθ × p(α0|τ)p(τ) .
(6.7)
The posterior is then proportional to the product of (6.7) and the current
data likelihood L(θ|D). If inference on θ0 is not of primary interest in the
current analysis, we can integrate it out to obtain the joint commensurate
power prior on µ, α0, and τ, πC(θ, α0, τ|D0), proportional to
Z
(L(θ0|D0) × p(θ|θ0 , τ))α0
R R
(L(θ0|D0) × p(θ|θ0 , τ))α0 dθ0dθdθ0 × p(α0|τ)p(τ) .
(6.8)
This extended power prior model requires the estimation of more parame-
ters from the data (notably τ 2), but we can formulate the model such that
the information gained is aimed directly at improving estimation of the
crucial borrowing parameter α0.
Application to Gaussian linear models
We now illustrate the application of commensurate priors in hierarchical
linear models for Gaussian response data. Assume y0 is a vector of n0
responses from subjects in a previous investigation of an intervention that
is to be used as a control in a current trial testing a newly developed
intervention for which no reliable prior data exists. Let y be the vector
of n responses from subjects in the current trial in both treatment and
control arms. Suppose that both trials are designed to identically measure
p −1 covariates of interest. Let X0 be an n0 × p design matrix and X be
an n × p design matrix, both of full column rank p, such that the ﬁrst
columns of X0 and X are vectors of 1s corresponding to the intercept. Now
suppose y0 ∼Nn0(X0β0, σ2) and y ∼Nn(Xβ + Zλ, σ2) where Z is an
n × r design matrix containing variables relevant only to the current trial,
as well as an indicator for the new treatment. Let D0 = (y0, X0, n0, p) and
D = (y, X, Z, n, p, r).
Suppose we assume a normal prior on β with mean β0 and variance τ 2I,
as well as a Beta( aσ2
τ 2 , 1) prior on α0 for some a > 0. The variance τ 2
parameterizes commensurability, since τ 2 close to 0 corresponds to very
high commensurability, while very large τ 2 implies the two datasets do not
arise from similar populations. Furthermore, as τ 2 approaches 0, aσ2
τ 2
→
∞, leading to a point-mass prior at 1 on α0. Thus, the model virtually
forces borrowing from the historical data when the data are commensurate.
Alternatively, as τ 2 becomes large, aσ2
τ 2 →0, discouraging incorporation of

256
SPECIAL TOPICS
any historical information. The hyperparameter a can be chosen to deliver
acceptable Type I error and power behavior. Adding a vague prior on τ 2
completes the prior speciﬁcation.
As a side comment, note that many other hyperpriors for α0 may be
appropriate. For example, we might reparameterize to
α0 ∼Beta(µφ, (1 −µ)φ) .
In this parameterization, µ is the hyperprior mean and φ is the hyperprior
“precision parameter” (note the variance is decreasing in φ). Now we might
ﬁx φ, treating it like the tuning constant a above, and (setting σ2 = 1
without loss of generality) take µ = 1/(1 + τ 2). Thus α0 once again has
prior mean 1 when τ 2 = 0 and prior mean 0 when τ 2 = ∞, but now with
the same precision in both cases.
Specifying our commensurate power prior as in (6.7) leads to a full con-
ditional prior for β0, πC(β0 | β, λ, α0, τ 2, D0), proportional to
Np
µ
β0
(XT
0 X0)−1(XT
0 y0τ 2 + σ2XT
0 X0β)
σ2 + τ 2
, (XT
0 X0)−1σ2τ 2
α0(σ2 + τ 2)
¶
,
(6.9)
and a joint prior on β, λ, α0, and τ 2, πC(β, λ, α0, τ 2|D0), proportional to
Np
µ
β
ˆβ0, (XT
0 X0)−1 (σ2 + τ 2)
α0
¶
×B
µ
α0
aσ2
τ 2 , 1
¶
×
µ 1
τ 2
¶3/2
, (6.10)
where ˆβ0 = (XT
0 X0)−1XT
0 y0. Let V
=
³
α0XT
0 X0
σ2+τ 2
+ XT (I−w)X
σ2
´−1
and
M =
³
α0XT
0 y0
σ2+τ 2 + XT (I−w)y
σ2
´
, where w = Z(ZT Z)−1ZT . Then the joint
posterior qC(λ, β, α0, τ 2|D0, D) follows by multiplying πC(β, λ, α0, τ 2|D0)
by the likelihood of y, and is proportional to
N
³
λ
ˆλ, σ2(ZT Z)−1´
× Np
³
β
V M, V
´
× qC(α0, τ 2|D0, D) ,
(6.11)
where ˆλ = (ZT Z)−1ZT (y −Xβ) and, integrating β0 out of the model,
qC(α0, τ 2|D0, D) ∝
R R
qC(β, λ, α0, τ 2|D0, D)dλdβ.
Notice that the full conditional posterior mean for λ, ˆλ, is a function of
residuals (y −Xβ), whereas the conditional posterior mean of β, V M, is
an average of the historical and concurrent data relative to the power and
commensurate parameters, α0 and τ 2. If we ﬁx σ2 to be close to the “truth”,
then as τ 2 becomes large and α0 approaches 0, the marginal posterior for β
converges to a normal density with mean
³
XT X−XT Z(ZT Z)−1ZT X
σ2
´−1
(XT y
−XT Z(ZT Z)−1ZT y) and variance
³
XT X−XT Z(ZT Z)−1ZT X
σ2
´−1
, recovering
the result from a linear regression that ignores all of the historical data. In
this case, ˆλ also converges to the no borrowing estimate of the treatment
diﬀerence.

INCORPORATING HISTORICAL DATA
257
Example 6.3 (Application to Saltz/Goldberg colorectal cancer trial data).
We consider data from two successive randomized controlled colorectal can-
cer clinical trials originally reported by Saltz et al. (2000) and Goldberg et
al. (2004). The initial trial randomized N0 = 683 patients with previously
untreated metastatic colorectal cancer between May 1996 and May 1998
to one of three regimens: Irinotecan alone (arm A), Irinotecan and bolus
Fluorouracil plus Leucovorin (arm B; IFL), or a regimen of Fluorouracil
and Leucovorin (arm C; 5FU/LV). In an intent-to-treat analysis, arm B re-
sulted in signiﬁcantly longer progression-free survival and overall survival
than arms A and C (Saltz et al., 2000).
The subsequent trial compared three drug combinations in N = 795
patients with previously untreated metastatic colorectal cancer, random-
ized between May 1999 and April 2001. Patients in the ﬁrst drug group
received the then-current “standard therapy,” the IFL regimen identical
to arm B of the historical study. The second group received Oxaliplatin
and infused Fluorouracil plus Leucovorin (abbreviated FOLFOX), while
the third group received Irinotecan and Oxaliplatin (abbreviated IROX);
both of these latter two regimens were new as of the beginning of the trial.
While both trials recorded many diﬀerent patient characteristics and
responses, in our analysis we concentrate on the trial’s measurements of
tumor size, and how the FOLFOX regimen compared to the IFL regimen.
Therefore, the historical dataset consists of treatment arm B from the initial
study, while the current data consists of patients randomized to IFL or
FOLFOX in the subsequent trial.
Both trials recorded two measurements on each tumor for each patient at
regular cycles. The trial reported by Saltz et al. measured patients every 6
weeks for the ﬁrst 24 weeks and every 12 weeks thereafter until a response
(death or disease progression), while the trial reported by Goldberg et al.
measured every 6 weeks for the ﬁrst 42 weeks, or until death or disease
progression. We computed the sum of the longest diameter in cm (“ld
sum”) for up to 9 tumors for each patient at each cycle. We used the
average change in ld sum from baseline to test for a signiﬁcant treatment
diﬀerence in ld sum reduction among FOLFOX and control regimens. Our
analysis below also incorporates baseline ld sum as a predictor, as well as
two important covariates identically measured at baseline: age in years, and
aspartate aminotransferase (AST) in units/L.
We restricted our analysis to patients who had measurable tumors, at
least two cycles of follow-up, and a nonzero ld sum at baseline, bringing
the total sample size to 441: 171 historical and 270 current observations.
Among the current patients, there are 129 controls (IFL) and 141 patients
treated with the new regimen (FOLFOX). Suppose y0 and y are vectors of
lengths n0 and n for the historical and concurrent responses such that
y0 ∼Normal(X0β0 , σ2),
and
y ∼Normal(Xβ + Zλ , σ2) ,
(6.12)

258
SPECIAL TOPICS
Historical data
Current data
estimate
95% CI
estimate
95% CI
intercept
0.880
(−1.977, 3.738)
−0.467
(−2.275, 1.341)
BL ld sum
−0.232
(−0.310, −0.154)
−0.397
(−0.453, −0.340)
age
−0.022
(−0.067, 0.022)
0.014
(−0.014, 0.041)
AST
−0.001
(−0.017, 0.015)
0.005
(−0.007, 0.017)
FOLFOX
–
–
−0.413
(−1.017, 0.190)
Table 6.1 Linear regression ﬁts to colorectal cancer data: y0 ∼x0, DF = 167
(left); y ∼x + z, DF = 265 (right).
where X0 and X are n0 × 4 and n × 4 design matrices with columns cor-
responding to (1, ld sum at baseline, age, AST), and Z is the FOLFOX
indicator function. Thus the β0 and β parameters contain intercepts as
well as regression coeﬃcients for each of three baseline covariates, while λ
represents change in average ld sum attributed to FOLFOX. Histograms
of the average change in ld tumor sum from baseline (not shown) suggest
that our assumption of normality here is acceptable.
Table 6.1 summarizes results from separate classical linear regression ﬁts
on the historical data (y0, X0) alone and the current data (y, X, Z) alone.
The “current data” results thus represent the “no borrowing” analysis. Re-
sults from both datasets suggest that ld sum at baseline is highly signiﬁcant
while age and AST are not. Furthermore, while the estimated intercept cor-
responding FOLFOX in the current data is negative, −0.413, the estimate
is not precise enough to conclude a signiﬁcant treatment diﬀerence at the
0.05 signiﬁcance level.
Information about β0 appears to be relevant to β, so we implemented the
commensurate prior linear model. We ﬁxed the error variance, σ2, at the
historical maximum likelihood estimate of 9.32. The beta hyperparameter,
a, was ﬁxed at 0.01, which corresponds to a simulated Type I error rate
(falsely rejecting the null hypothesis λ = 0) of 0.05 given E(y) is set equal
to E(y0) + 3σ
5 . Other choices are certainly possible; for example, we could
decrease a to deliver the same Type I error rate were mean shifts smaller
than 3σ
5 of interest.
Table 6.1 clearly shows that ld sum at baseline is a highly signiﬁcant
covariate. Therefore, we also generated a fake historical dataset that re-
placed the real baseline ld sums with values randomly generated from a
Normal(12, 9) distribution independent of y0. We then ﬁt the same com-
mensurate power prior linear model using the real current data to see if
our model could properly identify the inconsistencies and downweight the
inﬂuence of the fake historical data.

INCORPORATING HISTORICAL DATA
259
Real x0
Fake x0
estimate
95% BCI
estimate
95% BCI
intercept
−0.058
(−1.791, 1.684)
−0.289
(−2.542, 1.902)
BL ld sum
−0.324
(−0.375, −0.271)
−0.380
(−0.451, −0.310)
age
0.003
(−0.024, 0.030)
0.012
(−0.021, 0.046)
AST
0.001
(−0.009, 0.012)
0.002
(−0.012, 0.016)
FOLFOX
−0.755
(−1.372, −0.142)
−0.549
(−1.278, 0.185)
α0
1
(1, 1)
0.067
(0.011, 0.236)
τ 2
0
(0,0)
0.003
(0.001, 0.028)
Table 6.2 Commensurable power prior ﬁts to colorectal cancer data.
Point estimates (posterior medians) and 95% equal-tail Bayesian credi-
ble intervals for both the real (left) and fake (right) data are displayed in
Table 6.2. First, notice that the posterior for α0 corresponding to the real
data has converged to a point mass at 1. Therefore, our power prior linear
model considers the real historical and current data to be commensurate,
and thus incorporates virtually all of the historical information, increas-
ing the precision of the parameter estimates. As a result, the 95% credible
interval upper bound for λ is now less than zero, and so we can now con-
clude that FOLFOX resulted in a signiﬁcant reduction in average ld sum
when compared to the IFL regimen. This ﬁnding is consistent with those
of Goldberg et al. (2004), who determined FOLFOX to have better times
to progression and response rates.
On the other hand, the model properly identiﬁes the inconsistencies in
the relationship between the response and baseline covariates among the
current and fake historical data. This is clear from the 95% credible interval
for α0 in this case, which is very far from 1. As a result of the decrease in
precision, the posterior for λ covers 0 and the posterior summaries for β
and λ mirror linear regression estimates on the right side of Table 6.1. Last,
notice that the power prior credible intervals in the right side of Table 6.2
are wider than their counterparts in Table 6.1. This occurs in part because
the error variance for the current data is an estimated 33% less than our
ﬁxed choice of σ2.
Hobbs et al. (2009) show that commensurate power priors also have good
frequentist Type I error and power performance, but these simulations all
assume Gaussian responses. Future work in this area looks toward extend-
ing to non-Gaussian settings, especially those involving categorical and
time-to-event data. Another important need is the development of com-
mensurate priors for adaptive borrowing that allows the sample size or al-
location ratio in the ongoing trial to be altered if this is warranted. The idea

260
SPECIAL TOPICS
would be to maintain the balance of samples encouraged by α0 by deﬁning
the allocation ratio as a function of the number of eﬀective historical con-
trols, n0α0. That is, if the model encourages greater borrowing from the
historical controls, we can randomize more new subjects to the experimen-
tal treatment. Otherwise, the historical data will be suppressed, and the
allocation of new subjects to treatment and placebo will remain balanced.
To do this, suppose sj and rj denote the number of subjects randomized
to treatment and control, respectively, in the current trial following the jth
enrollment. Deﬁne ηj to be the eﬀective proportion of controls after the
jth enrollment,
ηj =
rj + n0α0
sj + rj + n0α0
.
(6.13)
The posterior of α0 induces a posterior for each ηj, whose median could
be used as the probability that the (j + 1)st subject is assigned to the new
treatment. This imposes information balance by encouraging optimal use
of new patients relative to amount of incorporated prior information. In
practice this could be done in blocks, perhaps after an initial period where
ηj is ﬁxed at 1/2.
As a ﬁnal side comment, Fuquene et al. (2009) show that Cauchy priors
and also a class of noninformative priors described by Berger (1985) can
correctly avoid borrowing when this is unwarranted – say, due to a few
large outliers in one of the two datasets. However, the commensurate prior
methods are quicker to permit borrowing when this is warranted, the source
of any “Bayesian advantage” the adaptive procedure can oﬀer.
6.2 Equivalence studies
An increasingly common area of statistical investigation in clinical trials
concerns bioequivalence, where we wish to know whether the rates and
absorption extents of two diﬀerent drugs can be thought of as equivalent.
Oftentimes the two drugs are a reference product and a competing generic
alternative, and we are hoping to show that the two products have similar
drug concentration proﬁles over time, so that their therapeutic eﬀects can
be reasonably expected to be similar as well. In the United States, the FDA
will not permit a generic drug to be marketed unless it can be shown to be
bioequivalent to the innovator product. As such, statistical methods that
are appropriate and acceptable to the FDA are crucial.
Mechanically, a typical bioequivalence study measures the concentration
of drug in the blood just before and at certain set times after its adminis-
tration. A sensible summary of the resulting empirical time-concentration
curve, such as AUC (area under curve), Cmax (the maximum concentra-
tion over time), or Tmax (the time to maximum concentration), is then
used as a surrogate for the drug’s pharmacokinetics more generally. Since
these measures are all strictly positive, it is common to take their logs as

EQUIVALENCE STUDIES
261
the response variable in a normal (Gaussian) statistical model. Were the
goal to show a diﬀerence between the two drugs, an ordinary 2-sample t
test might even be appropriate here.
Before getting into further statistical detail, however, we must mention
that there are three distinct notions of “bioequivalence,” at least as de-
scribed in current FDA guidance documents (Food and Drug Administra-
tion, 1999, 2001, 2002). By far the most common is average bioequivalence,
or ABE. This refers to equivalence between population mean responses in
the two study groups. While this concept is easiest to understand (not to
mention model statistically), note that two drugs whose drug responses are
signiﬁcantly diﬀerent in terms of their variability could be declared “bio-
equivalent” under ABE. In population bioequivalence, or PBE, the variabil-
ities of drug responses are also considered. The resulting, more stringent
condition is also sometimes known as prescribability, since population bio-
equivalence implies interchangeability of the two drugs at the time of initial
prescription. Finally, in individual bioequivalence, or IBE, we also add the
notion of switchability, or exchangeability of the two drugs within the same
patient at any time during the regimen. The thinking here is that, for a
patient who had been taking the standard drug for some time, to safely
switch to the new drug, one would need to show that the concentrations
of the active ingredient for the two drugs are the same at any time during
the regimen, not merely prior to treatment as with prescribability. Both
PBE and IBE remain somewhat controversial, and virtually all practical
investigations remain concerned with the demonstration of ABE. As such,
in what follows we focus on ABE; however, see Erickson et al. (2006) and
Ghosh and Ntzoufras (2005) for more details on these distinctions, as well
as related Bayesian models and WinBUGS software for particular IBE and
PBE settings.
In the remainder of this section, we begin with a brief description of the
basic ABE problem and a few standard approaches to its solution, including
ﬁtting it into the indiﬀerence zone paradigm of Subsection 2.5.2. We then
describe two speciﬁc models often used in bioequivalence studies, one for
simply binary response data and the other for a more complex (though
fairly standard) 2 × 2 crossover design.
6.2.1 Statistical issues in bioequivalence
Equivalence testing is an area of broad interest in statistics, well beyond
the realm of clinical trials. But this is partly an accident of history, in the
sense that it emerged as its own research area largely due to the widespread
adoption of the Neyman-Pearson (N-P) statistical testing framework in the
latter half of the twentieth century. As we have seen, in this framework,

262
SPECIAL TOPICS
the usual setup for testing the signiﬁcance of some treatment diﬀerence ∆,
H0 : ∆= 0
vs.
HA : ∆̸= 0 ,
(6.14)
presumes that the hypothesis we hope to reject is the null, H0. Recall that
in N-P testing one can never “accept” the null, only “fail to reject” it. As
such, if our interest is in showing that two treatments are “equivalent,” it
is indeed the alternative we hope to reject. But simply switching the roles
of H0 and HA is not sensible, since N-P testing requires the null to be a
“reduction” (simpliﬁcation) of the alternative, such as the one in (6.14)
where ∆is set to 0. And in any event, we do not really need the treatment
diﬀerence to be exactly 0; in the case where ∆is continuous, we would never
expect any estimate b∆to be identically equal to 0. Rather, we simply need
∆to be “close enough” to zero. One possible formulation along these lines
might be
H0 : ∆/∈(−δ, δ)
vs.
HA : ∆∈(−δ, δ) ,
(6.15)
for some prespeciﬁed δ > 0. The traditional solution to this problem is the
so-called two one-sided tests (TOST) procedure (Schuirmann, 1987). This
procedure’s popularity arises from its relative ease of use (it is theoreti-
cally and operationally similar to a traditional test of equality as in (6.14))
and from both regulation and encouragement by the U.S. Food and Drug
Administration (1992, 1999).
We will describe the TOST approach in some detail in Subsection 6.2.2;
for now, suﬃce to say that, while clever, it seems unattractive from a scien-
tiﬁc point of view: surely we would prefer to avoid having to ﬁrst reformu-
late and then swap the hypotheses, carefully crafting the “right” problem
to match our available statistical technology. Moreover, since the Bayesian
approach allows direct probabilistic statements about the parameter space,
it would be very natural to use in assigning posterior probabilities to the
two hypotheses in (6.15), and thus their relative plausibility. A traditional
Bayesian solution here would compute the Bayes factor in favor of H0 in
(6.15), following the exact and approximate methods described in Subsec-
tion 2.2.3. A Bayes factor larger than 1 favors equivalence, while less than
one favors inequivalence. In practice, we might insist on a larger Bayes fac-
tor threshold than 1, in order to reduce the Type I error of our procedure.
Alternatively, the indiﬀerence zone approach of Subsection 2.5.2 could
be very naturally adopted here. Recall this is where we formulate the null
and alternative hypotheses similar to (6.15) as
H0 : ∆/∈(δL, δU)
vs.
HA : ∆∈(δL, δU) ,
(6.16)
for δL < δU. Referring again to Figure 2.10, which is shown again here
as Figure 6.3, we recall that “equivalence” in this setting is concluded
only when the 95% equal-tail posterior credible interval for ∆, (∆L, ∆U),
is entirely contained within the indiﬀerence zone (δL, δU). Again, Type I

EQUIVALENCE STUDIES
263
no decision
(∆L
∆U)
accept treatment
(∆L
∆U)
reject control
(∆L
∆U)
equivalence
(∆L
∆U)
reject treatment
(∆L
∆U)
accept control
(∆L
∆U)
δL
δU
control
better
treatment
better
Figure 6.3 Indiﬀerence zone (δL, δU) and corresponding conclusions for a clinical
trial based on the location of the 95% posterior credible interval for ∆.
error can be reduced by insisting on conﬁdence higher than 95%, though
this will of course also reduce power.
Before continuing, we mention that equivalence testing is closely related
to the notion of noninferiority testing, where the goal is simply to show
that one drug is not inferior to the other. For instance, if we want to show
the treatment is not inferior to the control in Figure 6.3, we would test
H0 : ∆≤δL
vs.
HA : ∆> δL ,
since here H0 refers to treatment inferiority. This return to the one-sided
testing makes the problem substantially easier for a frequentist; in the
Bayesian paradigm, we would likely reject H0 if P(∆> δL|data) were
suﬃciently large, or alternatively, if the lower limit of the Bayesian credible
interval ∆L were bigger than the inferiority threshold δL.
6.2.2 Binomial response design
Williamson (2007) lays out the hypotheses and the standard two one-sided
test (TOST) approach in the case of a simple binomial response in both
treatment groups. Suppose p1 and p2 are the probabilities of success in the
two drug groups, and let p∆= p1 −p2. The hypotheses of interest are then
the binomial model version of those in (6.15), which we rewrite as the pair
of hypotheses,
H01 : p∆≥δ
vs.
HA1 : p∆< δ
and
H02 : p∆≤−δ
vs.
HA2 : p∆> −δ .

264
SPECIAL TOPICS
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
p_1
p_2
p2 = p1+δ
p2 = p1−δ
H02
H01
HA1 ∩HA2
Figure 6.4 Hypothesis of interest for the binomial response TOST.
This setting is pictured in Figure 6.4 for δ = 0.25. The basic idea behind
the TOST is that we reject H0 if and only if H01 and H02 are both rejected
at a chosen level of signiﬁcance, since p∆< δ and p∆> −δ is equivalent to
−δ < p∆< δ. Note that H01, HA1, H02, and HA2 correspond to inferiority,
noninferiority, superiority, and nonsuperiority of treatment 2 relative to
treatment 1.
Schuirmann (1987) showed that the TOST procedure is operationally
identical to declaring equivalence if and only if the usual 100(1 −2α)%
conﬁdence interval for p∆is entirely contained in (−δ, δ). Note the possibly
surprising use of 2α instead of α in this expression; see Williamson (2007,
Appendix A) for a concise and easy-to-follow proof of this fact.
Let us consider the case of two binomial responses, X1 and X2, giving the
number of successes in n1 and n2 trials, respectively, in the two treatment
groups. Here we would naturally suppose Xi ∼Bin(ni, pi) for i = 1, 2, and
let ˆpi = Xi/ni be the respective sample proportions. The most common
TOST interval is then the 100(1−2α)% conﬁdence interval method applied

EQUIVALENCE STUDIES
265
to the usual asymptotic interval,
ˆp1 −ˆp2 ± zα
µ ˆp1(1 −ˆp1)
n1
+ ˆp2(1 −ˆp2)
n2
¶1/2
,
which again concludes bioequivalence if this interval is contained within
(−δ, δ). Barker et al. (2001) compare this method with ﬁve other variants
of it, plus two other frequentist tests, in terms of size, power, and ease
of implementation. Williamson (2007) compares a subset of these tests
to fully Bayesian versions based not on interval comparisons of the sort
encouraged by Figure 6.3, but on Bayes factors under standard hypotheses.
For instance, suppose we wish to test the hypotheses in (6.15). Williamson
(2007) evaluates the Bayes factor in favor of H0 under a ﬂat prior for
(p1, p2),
π(p1, p2) = 1, 0 < p1 < 1, 0 < p2 < 1 ,
and also under a product of noninformative Jeﬀreys (Beta(1/2, 1/2)) priors,
π(p1, p2) = p−1/2
1
(1 −p1)−1/2p−1/2
2
(1 −p2)−1/2, 0 < p1 < 1, 0 < p2 < 1 .
In either case, the posterior probability that H0 is true must be computed
numerically, but is fairly straightforward given our conjugate prior setup.
The simulation study by Williamson (2007) compares the power and
Type I error behavior of these methods with that of somewhat more in-
formative Bayesian methods that use Beta(2, 4) and Beta(4, 4) priors, re-
spectively. The noninformative Bayesian approach emerges as an attractive
alternative to the TOST procedures in terms of power.
We remark that multiplicity issues like those discussed in Sections 2.2.7
and 6.3 come up again here. Suppose we wish to test H0 : pi = pj for all
possible pairs of hypotheses (i, j), where the total number of populations is
k. Lauzon and Caﬀo (2009) show that multiplicity is easily controlled in the
TOST by simply scaling the nominal Type I error by k −1 (instead of the
total number of comparisons k(k −1)/2, the usual Bonferroni choice). This
is due to the fact that the maximum error is achieved at pi = pj = p for
all adjacent p’s; that is, only the k −1 comparisons with the closest mean
diﬀerences make any real contribution to the error. Berger and Hsu (1996)
oﬀer a critique of current TOST practice, arguing that the usual conception
of size-α bioequivalence test as corresponding to a 100(1−2α)% conﬁdence
set is only true in special cases, and should be discarded in favor of more
general intersection-union tests (of which the standard TOST is a special
case).
6.2.3 2 × 2 crossover design
A very common framework for equivalence testing is the 2 × 2 crossover
design. This setting is described in some detail by Chow and Liu (2000,

266
SPECIAL TOPICS
Ch. 3); here we adopt the notation of Ghosh and Rosner (2007). Suppose
yijk is the log of the response in the ith sequence from the kth period for
the jth subject, i = 1, 2, j = 1, . . . , ni, k = 1, 2. We model yijk as
yijk = mik + Si + Pk + δij + ϵijk ,
where mik is the direct eﬀect of the formulation in the ith sequence admin-
istered in the kth period, Si is the sequence eﬀect, Pk is the period eﬀect,
δij is the random eﬀect of the jth subject in the ith sequence, and ϵijk is
a within-subject random error. Without loss of generality, we may assume
the ﬁrst sequence is the reference formulation (R), then
mik =
½
mR
if k = i
mT
if k ̸= i
,
where it is often convenient to assume mR + mT = 0. The δij are typically
assumed to be i.i.d. N(0, σ2) random variables, mutually independent of
the ϵijk, which are i.i.d. N(0, σ2
ℓ), where ℓ= R if i = k and ℓ= T otherwise.
Again, average bioequivalence is concluded if we reject H0 as stated in
(6.16). The FDA has suggested taking δL = log(0.8) and δU = log(1.25),
which is sensible since the data themselves have been logged. If the posterior
probability of HA is greater than 0.5 then ABE may be concluded.
To fully specify the hierarchical model, Ghosh and Rosner (2007) actually
recommend a Dirichlet process mixture prior of the form
δij
∼
N(µj, σ2
δ)
µj
∼
G
G
∼
DP(αG0)
G0
∼
N(0, σ2
G)
and σ2
δ
∼
IG(c, d) ,
where DP denotes the Dirichlet process with base measure G0 and pre-
cision parameter α, which captures our degree of conﬁdence in our prior
“best guess” G0. Ghosh and Rosner (2007) follow Sethuraman (1994) and
many other MCMC-oriented authors by regarding the inﬁnite dimensional
parameter G as an inﬁnite mixture. A ﬁnite approximation then permits
feasible WinBUGS implementation. These authors also contemplate a stan-
dard normal hierarchical model, but ﬁnd the added ﬂexibility of the DP
mixture to oﬀer worthwhile advantages, at least in the context of their
real-data example.
The Bayesian hypothesis test requires estimation of the posterior proba-
bility of the null hypothesis. Using the Gibbs sampler draws m(g)
T
and m(g)
R
for g = 1, . . . , G, we can estimate
P(ABE|data)
=
P(log(0.8) < mT −mR < log(1.25)|data)

EQUIVALENCE STUDIES
267
≈
1
G
G
X
g=1
I(log(0.8) < m(g)
T
−m(g)
R < log(1.25)) .
Again, if this estimate exceeds 0.5, we are justiﬁed in choosing the alterna-
tive hypothesis, hence concluding bioequivalence.
Up until now we have considered the analysis of just one endpoint at a
time. But we may wish to evaluate bioequivalence based on a simultaneous
assessment of two endpoints – say, the area under the blood concentration-
time curve, AUC, and the maximum concentration, Cmax. Since these end-
points are likely to be (positively) associated, this requires a multivariate
response model that permits dependence among the endpoints considered.
Previous approaches include work by Hauck et al. (1995), Chinchilli and
Elswick (1997), and Quan et al. (2001), and have tended to achieve the de-
sired correlations with multivariate normal models. The intersection-union
tests of Berger and Hsu (1996) and Wang et al. (1999) tend to be conserva-
tive, the extent of which depends upon the between-endpoint correlation.
Ghosh and G¨onen (2008) extend the semiparametric setting of Ghosh
and Rosner (2007) to the case of multiple endpoints, but still using the
Dirichlet process mixture (DPM) formulation. The prior is constructed to
allow a positive probability of the null hypothesis for each endpoint. Prior
elicitation is rather more complex here, since the model is signiﬁcantly
more complex and results depend crucially on the between-endpoint corre-
lations. The authors illustrate their approach with a simultaneous analysis
of AUC and Cmax in a two-sequence, two-period crossover study of two
formulations of the drug erythromycin. Interestingly, these data feature
fairly strong evidence of bioinequivalence for AUC, but far more uncer-
tainty (bordering on mild evidence of bioequivalence) for Cmax. Ghosh and
G¨onen (2008) consider results over a set of assumed prior correlations ρ
between the two endpoints, ranging from 0 to 0.5. The marginal probabili-
ties of bioequivalence are modiﬁed in the expected directions as ρ increases
(e.g., the probability using Cmax starts fairly large, but drops as the impact
of the less encouraging AUC data is more keenly felt).
Once again the method is ﬁt in WinBUGS via a ﬁnite approximation to
the (inﬁnite-dimensional) DPM where a parameter L controls the number
of components in the mixture. Truly joint probabilities of bioequivalence
P(ABE|data) are computed as
P(log(0.8) < θ1 < log(1.25) ∩log(0.8) < θ2 < log(1.25)|data)
≈1
G
PG
g=1 I(log(0.8) < θ(g)
1
< log(1.25) ∩log(0.8) < θ(g)
2
< log(1.25)) ,
where g indexes the Gibbs samples for the treatment diﬀerences for AUC(θ1)
and Cmax(θ2).
Software note:
WinBUGS code to carry out this process is available from the
second author’s website, http://www.mskcc.org/mskcc/html/84563.cfm.

268
SPECIAL TOPICS
Finally, DeSouza et al. (2009) oﬀer a fully parametric analysis of the
multiple endpoint two-period crossover design of Ghosh and G¨onen (2008).
These authors give the relevant full conditional distributions under the
usual product of minimally informative normal-inverse gamma priors, as
well as an illustration using an artiﬁcial dataset taken from a bioequivalence
guide published by the Brazilian government. Straightforward WinBUGS
code is provided in Appendix A of the paper.
6.3 Multiplicity
As discussed previously in Subsection 2.2.7 and elsewhere, the problem of
multiplicity is one of the most diﬃcult for any statistician, Bayesian or
frequentist. Moreover, the problem arises in virtually every data analytic
context in some form or another. That is, we almost never come to a dataset
intending to analyze just one particular aspect and then never touch the set
again; rather, we expect to perform multiple analyses, altering our model,
transforming certain variables, and so on as we go. It is sometimes diﬃcult
to say when one crosses the line between “good statistical practice” and
“data dredging” that serves to inﬂate the perceived signiﬁcance of one of
many possible ﬁndings. Statistically, the problem is complicated by the fact
that the frequentist and Bayesian camps can seem particularly far apart in
their views as to its proper remedy. The former group typically argues for
sometimes drastic corrective action, e.g., Bonferroni and other adjustments
that are so conservative as to preclude signiﬁcant ﬁndings in all but the
very most extreme cases (where no adjustment and indeed no statistics
are really necessary). On the other hand, some pure Bayesians argue that,
since any marginal “slice” of a joint posterior distribution has prima facie
validity in its own right, and there is no sensible way to prespecify how
many of these slices one might ultimately choose to view, no adjustments
for multiplicity are required. But this of course serves to perpetuate the
problem that motivated the discussion in the ﬁrst place: an overabundance
of “false positives” when too many analyses are in the mix.
As usual, in clinical trials our outlook must be a practical one. Through-
out this book we have advocated Bayesian methods that enjoy good fre-
quentist properties, and it is natural to look for a similar compromise in
this setting as well. Here it would mean some sort of “partial correction”
that accounts for multiplicity when this is crucial (e.g., would otherwise
result in an abundance of false positives), but doesn’t overdo it when it is
not. Hierarchical Bayesian modeling of the sort introduced in Section 2.4
is a natural tool here in cases where we can realistically expect to be able
to “model our way out” of the problem to any signiﬁcant extent. That is,
we may be able to construct a model that (a) anticipates various structural
similarities among the model parameters that could lead to problems if not

MULTIPLICITY
269
acknowledged, and (b) is ﬂexible enough to allow the accumulating data to
help determine just how much correction is required.
6.3.1 Assessing drug safety
A good example of the use of hierarchical modeling to help correct for
multiplicity arises in the analysis of drug safety data. Here the problem is
one of determining which of several adverse events (AEs) are signiﬁcantly
associated with a particular experimental treatment. Typically this is in
the setting of a clinical trial comparing a treatment (T) and a control (C),
and we are interested in “ﬂagging” any AE that is diﬀerentially associ-
ated between these two groups. Note that Type II errors are equally if not
more important than Type I errors in such settings, since failing to ﬂag a
real drug-AE interaction is likely a greater public health risk than falsely
identifying such an interaction.
A number of Bayesian methods have been proposed in drug safety as-
sessment. In analyzing post-marketing spontaneous reports, DuMouchel
(1999) proposed a gamma-Poisson shrinkage algorithm in analyzing the
FDA adverse event reporting system (AERS) database, while Bate et al.
(1998) used a Bayesian Conﬁdence Propagation Neural Network (BCPNN)
approach to analyze a WHO database of adverse drug reactions. In this sec-
tion, we follow the model of Berry and Berry (2004), who point out two
advantages the Bayesian approach oﬀers here over the frequentist. First,
the rates for those AEs not being considered for ﬂagging, as well as their
similarity with those that are, can be explicitly measured and fed back into
the analysis. Second, a hierarchical structure may be useful in capturing
the biological relationships among the various AEs. In the latter case, it’s
important to stress that any similarities modeled among AEs must be based
on biological (or perhaps regulatory) grounds, and not merely on empirical
similarity. The most obvious choice for grouping here would be based on
body system: the model could assume that AEs arising from the same body
system are more likely to be similar than those in diﬀerent body systems.
Let us specify the problem a bit more so that appropriate statistical
notation can be established. At the present time, AEs are routinely coded
in Medical Dictionary for Regulatory Activities (MedDRA) terms with a
hierarchical structure. One such coding is by system organ class (SOC),
which intrinsically reﬂects biological relationships among various AEs in
the same class. AEs in the same SOC are thus more likely to be similar,
making hierarchical borrowing of strength natural. We can also allow for
borrowing across SOCs, though our model does not impose it; instead
it adapts based on the observed data. In our dataset, AEs are actually
identiﬁed by their “preferred term” (PT), such as “rash, varicella-like;”
MedDRA allows even further subcategorization of PTs, but in this paper
we use PT as the smallest unit of analysis.

270
SPECIAL TOPICS
This setup essentially matches that assumed by Berry and Berry (2004),
who adopt a binomial likelihood for the AE counts in the treatment (Y )
and control (X) groups. That is, they let Ybj ∼Bin(Nt, tbj) and Xbj ∼
Bin(Nc, cbj), where tbj and cbj are the probabilities of adverse event for PT
j and SOC b in the two groups, respectively. These authors then consider
a logistic regression mean structure,
logit(cbj) = log(cbj/(1 −cbj)) = γbj
and
logit(tbj) = γbj + θbj ,
so that γbj is the logit AE rate in the control group, and θbj is the relative
increase in this logit rate in the treatment group. Note that this means
θbj = log{[tbj(1 −cbj)]/[cbj(1 −tbj)]} is the log-odds ratio (OR).
The hierarchical model then proceeds as follows. We begin by setting the
ﬁrst stage prior distributions to be
γbj ∼N(µγb, σ2
γb)
and
θbj ∼N(µθb, σ2
θb) ,
(6.17)
both for j = 1, . . . , Jb and b = 1, . . . , B. These speciﬁcations encourage
borrowing of strength within each SOC. Next, at the second stage we use
the following prior distributions:
µγb ∼N(µγ0, τ 2
γ0),
σ2
γb ∼IG(αγ, βγ)
and
µθb ∼N(µθ0, τ 2
θ0),
σ2
θb ∼IG(αθ, βθ) ,
where again b = 1, . . . , B and IG denotes the inverse gamma distribu-
tion. This speciﬁcation permits borrowing across SOCs where appropriate
(though we do not expect such borrowing to be nearly as dramatic as that
within SOC). Finally, the model speciﬁcation is completed with priors for
the second stage model parameters, which we take as
µγ0 ∼N(µγ00, τ 2
γ00),
τ 2
γ0 ∼IG(αγ00, βγ00),
µθ0 ∼N(µθ00, τ 2
θ00),
and τ 2
θ0 ∼IG(αθ00, βθ00) .
We assume the hyperparameters µγ00, µθ00, τ 2
γ00, τ 2
θ00, αγ00, βγ00, αθ00, βθ00,
αγ, βγ, αθ, and βθ are ﬁxed constants. In our analysis, we specify minimally
informative values for these as µγ00 = µθ00 = 0, τ 2
γ00 = τ 2
θ00 = 10, αγ00 =
αθ00 = αγ = αθ = 3, and βγ00 = βθ00 = βγ = βθ = 1.
Berry and Berry (2004) actually recommend a slightly more sophisticated
ﬁrst stage model for the θbj. Speciﬁcally, they replace (6.17) with a mixture
distribution of the form
θbj ∼πbδ(0) + (1 −πb)N(µθb, σ2
θb) ,
(6.18)
where 0 ≤πb ≤1 and δ(0) is the Dirac delta function (point mass) at 0.
This permits a prior probability of πb that the treatment and control rates
are exactly the same, which is appropriate since many AEs are completely

MULTIPLICITY
271
unaﬀected by the treatment. The remainder of the model is the same,
although we now require prior distributions for the new hyperparameters
πb as follows:
πb
∼
Beta(απ, βπ)
απ
∼
Exponential(λα)I(απ > 1)
and
βπ
∼
Exponential(λβ)I(βπ > 1).
where we truncate the two exponential distributions in order to prevent too
much prior mass for the πb from accumulating near the “extreme” values
of 0 and 1. We also need to add ﬁxed values for the hyperparameters λα
and λβ, which we take as λα = λβ = 0.1.
Given the posterior distribution of our Bayesian hierarchical model, we
may use the posterior exceedance probability to identify potential signals.
For example, an AE of PT j in SOC b can be ﬂagged if
P(θbj > d | x, y) > p ,
(6.19)
where d and p are prespeciﬁed constants. For safety signal detection, we
might simply choose d = 0, indicating higher odds of AE in the treatment
arm (OR > 1). On the other hand, a θbj could have a high posterior prob-
ability of exceeding 0, but be clinically unimportant. As such, larger values
of d (say, log(2)) might be used to better capture a “clinically meaningful”
eﬀect.
The exceedance probabilities for other statistics are also easily obtainable
in our Bayesian framework. For instance, we can switch from the log-OR
scale to the risk diﬀerence (RD) scale by ﬂagging AEs for which
P(tbj −cbj > d∗| x, y) > p ,
(6.20)
where again d∗and p are appropriate constants.
Example 6.4 Xia, Ma, and Carlin (2008) apply the models above to AE
data aggregated from four double-blind, placebo-controlled, phase II-III
clinical trials of a particular drug. All studies were of 12 or 24 weeks in
duration and were fairly similar in design and population, justifying their
pooling for this analysis. The full dataset has 1245 subjects in the treatment
group and 720 subjects in the control group, with reported AEs coded to
465 PTs under 24 SOCs.
Tables 6.3 and 6.4 presents various posterior exceedance probabilities
from our hierarchical binomial model using the mixture prior (6.18) for the
ﬁve PTs that have two-sided Fisher’s exact test p-values less than 0.05 and
higher observed risks on the treatment arm than on the placebo arm. The
fourth column of Table 6.3 gives the posterior probability of OR = 1, i.e., no
diﬀerence between treatment and placebo arms in our mixture setting. The
last column of this table gives the posterior exceedance probabilities based
on OR using the null value (1) as the cutoﬀ. The third column of Table 6.4

272
SPECIAL TOPICS
exact
posterior probabilities
SOC
PT
p-value
OR = 1
OR > 1
General Disorders
and Administration
Fatigue
0.019
0.430
0.564
Site Conditions
Infections
and
Herpes
0.025
0.459
0.532
Infestations
Simplex
Infections
and
Sinusitis
0.012
0.302
0.697
Infestations
Injury, Poisoning
and Procedural
Excoriation
0.030
0.680
0.296
Complications
Skin and
Subcutaneous
Ecchymosis
0.005
0.457
0.535
Tissue Disorders
Table 6.3 Fisher’s exact test p-values and posterior summaries under the hierar-
chical binomial mixture model for the four-study data.
instead uses an OR cutoﬀof 2, potentially a more clinically meaningful
threshold. The last two columns of Table 6.4 give the exceedance prob-
abilities based on the risk diﬀerence scale for two potentially important
diﬀerences.
The tables reveal that smaller p-values do not necessarily correspond to
higher posterior exceedance probabilities. For example, ecchymosis has the
smallest p-value, but does not have the largest P(OR > 1). The hierarchi-
cal model assumes PTs in the same SOC to be more alike than those in
diﬀerent SOCs. This creates a shrinkage pattern in the log-ORs that may
lead to this reordering of the AEs in terms of their departure from the null.
For example, the p-value for ecchymosis is smaller than that for sinusitis.
However, the posteriors of the AEs in the “Skin and Subcutaneous Tissue
Disorders” SOC, to which ecchymosis belongs, do not show a consistent
pattern of adverse eﬀect; in fact, about half of them had negative treat-
ment diﬀerences. Thus, the Bayesian model is less eager to ﬂag ecchymosis
than Fisher’s exact test, which ignores the hierarchy. By contrast, in the
“Infections and Infestations” SOC, to which sinusitis belongs, most AEs do
show higher risk in the treatment arm. The hierarchical mixture procedure

MULTIPLICITY
273
posterior probabilities
SOC
PT
OR > 2
RD > 2%
RD > 5%
General Disorders
and Administration
Fatigue
0.319
0.099
0.000
Site Conditions
Infections
and
Herpes
0.357
0.000
0.000
Infestations
Simplex
Infections
and
Sinusitis
0.422
0.280
0.000
Infestations
Injury, Poisoning
and Procedural
Excoriation
0.175
0.000
0.000
Complications
Skin and
Subcutaneous
Ecchymosis
0.437
0.000
0.000
Tissue Disorders
Table 6.4 More posterior summaries under the hierarchical binomial mixture
model for the four-study data.
also tones down the somewhat alarming p-value of 0.03 for excoriation,
obtaining a posterior probability of just 0.296 that the OR exceeds 1.
Table 6.4 also shows that the posterior probabilities of the risk diﬀerence
exceeding 2% are all very low for these ﬁve AEs, further indicating that,
while there may be some limited statistical signiﬁcance to our ﬁndings here,
their clinical signiﬁcance is very much in doubt.
In the context of this example, Xia et al. (2008) also perform various
simulation studies that show better power and familywise error rates for
their Bayesian procedures over those based on (unadjusted) Fisher exact
tests under both binomial and Poisson likelihoods. However, the authors
emphasize the inevitable and now-familiar tradeoﬀbetween lower error
rates and good power.
Note that the models discussed above do not contemplate any sort of
nonexchangeability; shrinkage both within and across groups is assumed to
follow an i.i.d. normal speciﬁcation, albeit with the mixture enhancement
(6.18) in the case of θbj. But in some cases, there may be a “distance” metric
one can use to help shrink diﬀerentially within groups. Then one could use a
spatial-type model, reminiscent of those used in geostatistical data analysis

274
SPECIAL TOPICS
(see e.g. Banerjee et al., 2004, Ch. 2). For example, the covariance between
γbj and γbj′, corresponding to two AEs j and j′ within SOC b, might be
σ2
γ exp(−ργdbj,bj′) ,
where dbj,bj′ is the “distance” between these two AEs. In the Berry and
Berry (2004) setting we might set the distance between two rashes (say,
measles/rubella-like and varicella-like) in Group 10 equal to 1, but use a
larger distance (say, 2) between either of these rashes and eczema. Like
the assignment of AEs to SOCs, the selection of these distances would be
crucial, and need to be informed by biological (not empirical) mechanisms.
But if one could do this, this would encourage a diﬀerent (and perhaps
more sensible) kind of shrinkage in the random eﬀects that could in turn
lead to better signal detection overall in the face of multiplicity. Note that
the variance and range parameters above could be generalized to be SOC-
speciﬁc, i.e., σ2
γb and ργb. Alternatively, if distances between SOCs (say, b
and b′) can be sensibly deﬁned, we could model spatially at this level of the
hierarchy as well, e.g., using a spatial model for the correlation between
µθb and µθb′.
Other methodological enhancements to the basic approach are possible.
The purely “nested” borrowing of strength used above (PTs within SOC,
and then across SOCs) may not be the most sensible, since many SOCs
are inherently diﬀerent from each other. Instead, one might prefer to deﬁne
a second, entirely new hierarchy that focused more on grouping PTs in
similar or proximate regions of the body. This leads naturally to a model
having two sets of random eﬀects, say θ(1)
j
and θ(2)
ℓ, with nonnested indexing
systems j and ℓthat borrow strength over separate hierarchies while both
contributing to log-OR. Another extension would be to the case of AE
data having more than two possible outcomes. For instance, suppose each
AE was coded to a severity score, such as 1=mild, 2=moderate, 3=severe,
4=life-threatening, and 5=fatal. Such data could be accommodated via
a multinomial (instead of binomial) likelihood, which would in turn help
decide whether two ﬂagged AEs (say, pancreatitis and nausea) with similar
posterior exceedance probabilities were really of equal concern.
In their closing paragraph, Berry and Berry (2004) mention the possi-
bility of using their approach in other multiplicity contexts besides drug
safety, and speciﬁcally mention the problem of identifying genes that are
diﬀerentially expressed in cDNA microarray data. Indeed, this is an ex-
ample of a setting where genetic distance oﬀers a natural choice of dbj,bj′
in the formula above. In the AE setting, a careful study of the MedDRA
dictionary may enable a similar choice of a sensible distance metric, though
the problem here is clearly less straightforward.

MULTIPLICITY
275
6.3.2 Multiplicities and false discovery rate (FDR)
The discussion in the previous subsection showed how model-based poste-
rior inference can be used to adjust probabilities. Formally, the adjustment
is implemented as hierarchical shrinkage. Inference under the hierarchical
model includes posterior probabilities for each comparison, as summarized
for example in Tables 6.3 and 6.4. The probabilities for each adverse event
and organ class report the judgment in light of all the data, including data
for other organ classes and other adverse events. In this sense the proba-
bilities are adjusted.
However, reporting the probabilities is only part of the solution: we still
need a rule of how to threshold the probabilities. In other words, we need
to select adverse events to be reported for diﬀerential adverse event rates
across treatment and control groups. This selection should account for the
fact that we are carrying out many such comparisons simultaneously. For
example, if we consider 1000 comparisons and report all comparisons with
posterior probability greater than 0.9, then we could still be almost certain
to include some false decisions. The use of multiple comparisons with such
massive numbers of comparisons is still rare in clinical trial design. However,
with the increased use of molecular markers and high-throughput data, we
expect that problems related to massive multiple comparisons will become
increasingly relevant in clinical trial design.
In frequentist inference, several approaches exist to address multiplicity
concerns, including Bonferroni’s correction as perhaps the most popular
choice. Bonferroni and other adjustments control the experiment-wide er-
ror rate of reporting any false comparison. As already mentioned, for mas-
sive multiple comparisons this control becomes excessively conservative.
This led to the development of alternative approaches and criteria for error
control in massive multiple comparisons.
Benjamini and Hochberg (1995) proposed to control the false discovery
rate (FDR). Let δi ∈{0, 1} denote the (unknown) truth about the ith
comparison. For example, in the setting of the previous subsection, δi is an
indicator for a non-zero true diﬀerence in rates for an adverse event. Let
di ∈{0, 1} denote the decision about the ith comparison. In the example
this is an indicator for reporting diﬀerential rates across treatment and
control for the i-th adverse event. Let D = P di denote the number of
reported comparisons. The false discovery proportion is deﬁned as P
i di(1−
δi)/D. The FDR is the (usually frequentist) expectation
FDR = E
Ã
1
D
X
i
di(1 −δi)
!
.
Benjamini and Hochberg (1995) proposed a very elegant and easily imple-
mented algorithm that guarantees FDR < α for any desired error bound.
Alternatively, one could aim to control the posterior expectation of the

276
SPECIAL TOPICS
false discovery proportion, leading to the posterior FDR:
FDR = E
Ã
1
D
X
i
di(1 −δi) | y
!
.
Here we run into some good luck: the evaluation of this posterior expec-
tation is straightforward. Conditional on the data y, the only unknown
quantities are the δi. The decisions di are functions of the data, di(y), and
are ﬁxed conditional on y. This leaves us with FDR = 1
D
P di[1−E(δi | y)].
See, for example, Newton et al. (2004) for the use of FDR in the context
of a speciﬁc model, and M¨uller et al. (2007) for a discussion of alternative
Bayesian approaches for controlling error rates in multiple comparisons.
6.4 Subgroup analysis
6.4.1 Bayesian approach
Subgroup analysis is concerned with the question of whether an overall
conclusion about the eﬀectiveness of a treatment remains valid for sub-
populations of the overall patient population. For example, in a trial for
a new neuroprotective agent for stroke patients, it might be important to
consider patient subpopulations deﬁned by diﬀerent stroke types (for ex-
ample, ischemic vs. hemorrhagic strokes), severities, and so on. It is quite
plausible that an intervention is very eﬀective for a more homogeneous sub-
population even when investigators fail to show signiﬁcant eﬀects for the
more heterogeneous patient population at large. Conversely, it is possible
that a therapy that is eﬀective for most patients may be inappropriate for
important subpopulations.
Most clinical trials allow for many possible subgroups to be identiﬁed,
raising concerns about data dredging when subgroup eﬀects are investi-
gated in an unplanned manner after a trial fails to show a signiﬁcant treat-
ment eﬀect in the overall patient population. In particular, the large number
of possible subgroups that could be considered gives rise to serious multi-
plicity concerns. Guidelines for good practice of subgroup analyses (Pocock
et al. 2002, Rothwell 2005) include the recommendation that subgroups
should be pre-speciﬁed, should be limited to a small number of clinically
important subgroups, and should include appropriate adjustment for mul-
tiplicities. Dixon and Simon (1991) and Simon (2002) propose Bayesian
strategies for subgroup analysis based on inference for treatment–subgroup
interactions.
Simon (2002) proposes a speciﬁc approach that is suitable for general-
ized linear models, logistic models and proportional hazards models. For
example, for the proportional hazards model he proposes to include a
regression on a binary treatment indicator z, a binary covariate x, and
their interaction xz. Letting λ(t) denote the hazard, Simon uses λ(t) =

SUBGROUP ANALYSIS
277
λ0(t) exp(αz + βx + γzx). The discussion includes a careful consideration
of the prior probability model for the interaction eﬀect γ. The prior should
be calibrated by ﬁxing the prior probability p(γ ≤δ | α = 0) of a subgroup
eﬀect beyond some minimal, clinically meaningful eﬀect δ (for negative δ).
For example, δ = log(2/3) and π = 0.025 ﬁxes the prior probability that
the hazard reduces by more than one-third for patients in the subgroup
{x = 1} compared to {x = 0}. Posterior inference on γ formalizes the
subgroup analysis.
Hodges et al. (2007) describe an approach to smoothing balanced, single-
error-term ANOVA models. The method uses a hierarchical model to smooth
interaction terms, which eﬀectively remain in the model if they are im-
portant predictors, vanish from the model if they are not, and are partly
smoothed away if the data are indecisive. This approach is useful for proper
investigation of subgroups (whose signiﬁcance depends on the signiﬁcance
of the interactions between the main eﬀects), but also addresses unrepli-
cated designs and masked contrasts in eﬀects with many degrees of freedom.
6.4.2 Bayesian decision theoretic approach
Sivaganesan et al. (2008) and M¨uller et al. (2010) propose a speciﬁc im-
plementation of inference for treatment and subgroup interactions. The
proposed approach goes beyond the discussion in Simon (2002) by includ-
ing formal model selection for subgroup and treatment interaction eﬀects
and a formal consideration of the decision related to subgroup reports. The
approach illustrates the strength of the Bayesian perspective when infer-
ence involves a complex combination of borrowing strength across related
subgroups, a trade-oﬀbetween the competing goals related to the overall
hypothesis versus the subgroup eﬀects, and uncertainty about the probabil-
ity model. The proposed strategy is not established standard methodology,
but can be characterized as an application of familiar principles of Bayesian
clinical trial design to the problem of subgroup analysis.
Subgroups are characterized by available baseline covariates. Let xik, k =
1, . . . , K denote K baseline covariates recorded for patient i. We assume
that covariates are categorical, xik ∈{1, 2, . . . , Sk}. If necessary, we re-
code originally non-categorical covariates. The covariate levels then deﬁne
potential subgroups. When reporting subgroups with non-zero treatment
eﬀects, we include inference whether and how the non-zero treatment eﬀect
varies across the reported subgroups.
The proposed approach follows a decision theoretic motivation. For the
following discussion we assume a 2-arm clinical trial, with outcome yi for
the i-th patient. Let zi be an indicator for assignment to experimental
therapy (zi = 1) or control (zi = 0). Let θ generically indicate a treatment
eﬀect. The treatment eﬀect could be the diﬀerence in means for a continous
outcome, or the diﬀerence in success probabilities for a binary outcome. The

278
SPECIAL TOPICS
approach remains equally valid for any other design or outcome, with minor
variations in the speciﬁc algorithm only; we will indicate necessary changes
at the end of this discussion.
The proposed approach proceeds by deﬁning a decision rule for choos-
ing among the possible actions, namely, reporting eﬃcacy for the overall
population (H1), for some subpopulations (A∗, see below for deﬁnition),
or reporting no eﬃcacy for any subgroups (H0). We index a pattern of
subgroup eﬀects by a vector of indicators γk = (γkj; j = 1, . . . , Sk), with
γkj = 0 indicating no treatment eﬀect for the subpopulation of patients
{i : xik = j} and γkj = 1, 2, . . . indicating a non-zero treatment eﬀect for
{xk = j}, with distinct integers indicating subgroups with equal treatment
eﬀects. For example, for a covariate xk with Sk = 2 levels, the possible pat-
terns of subgroup eﬀects are as follows. There are two possible subgroups
characterized by the binary covariate xk: {xi = 1} and {xk = 2}. The
possible subgroup eﬀects distinct from H0 and H1 are (i) no treatment
eﬀect in the ﬁrst subgroup, but non-zero eﬀect in the second subgroup;
(ii) non-zero eﬀect in the ﬁrst and zero eﬀect in the second; (iii) non-zero
and diﬀerent eﬀects in the ﬁrst and second subgroup. The subgroup ef-
fects (i) through (iii) are described by γ = (0, 1), (1, 0) and (1, 2). Thus let
Γk = {(0, 1), (1, 0), (1, 2)} denote the possible subgroup eﬀects in this case.
The vectors γk = (0, 0) and (1, 1) are not included, as they correspond
to the overall null and alternative hypothesis. When reporting subgroup
eﬀects we allow reporting of subgroup patterns γk for multiple covariates.
Let AK ⊂{1, . . . , K} denote the covariates for which we report subgroups.
Thus a subgroup report A∗is of the form A∗= (AK, AΓ = {γk; k ∈AK}).
The proposed rule requires a probability model p(M) over competing
models. Here M could be the overall alternative H1, the overall null H0 or
a subgroup model Mk,γk. The latter, Mk,γ, assumes a pattern γk of distinct
treatment eﬀects for subgroups deﬁned by covariate xk. For reference we
will later describe a speciﬁc probability model. The algorithm remains valid
for any alternative probability model. Let p(M) = p(M | y) denote the
posterior probability of model M.
Decision rule: The decision rule is easy to describe, but a bit more com-
plicated to justify. We use posterior probabilities to further restrict allow-
able subgroup reports. Let γ∗
k = arg maxγk{p(Mk,γk)} denote the most
likely subgroup pattern for covariate xk. If subgroups for covariate xk are
reported, then it needs to be γ∗
k. In other words, if we decide to report non-
zero treatment eﬀects for subgroups characterized by covariate xk, we do
not allow the arbitrary selection of a possible pattern of subgroups; we only
allow reporting of the most likely arrangement. The rule is to decide for
δ∗∈{H1, A∗, H0} using two threshold parameters t0 and t1 for posterior

SUBGROUP ANALYSIS
279
odds in a sequence of pairwise comparisons
δ∗=





H1
if p(H1)
p(H0) > t0 and p(γ∗
k)
p(H1) < t1 for all k
A∗
if for some k : p(γ∗
k)
p(H0) > t0t1 and p(γ∗
k)
p(H1) > t1
H0
if neither H1 nor A∗are chosen
(6.21)
In the implementation we use thresholds t0 = t1 = 1.0 as default choices.
Model probabilities: We use model probabilities p(M) indexed by two
hyperparameters, p and α:
p(M) =







p2
for M = H0
(1 −p)2/(1 + α)
for M = H1
cpnk0αGk−1
QGk
g=1(nkg−1)!
QSk
j=2(α+j−1)
for M = all other Mk,γk
, (6.22)
with Gk = maxj γkj and nkg = P
j I(γkj = g). The speciﬁcation of these
prior probabilities involves two more hyperparameters, p and α, and can
be described as a zero-enriched Polya urn. As default choices we suggest
p = 0.6 and α = 1.0. The model is completed with any sampling model
p(y | M).
Utility function: The rule (6.21) can be justiﬁed as an approximate Bayes
rule under the probability model (6.22) and an assumed utility function.
The utility function assigns a value u(δ, M, y) for any decision δ under
assumed future data and a hypothetical truth M. Speciﬁcally,
u(δ, M, y) =







u0I(M = H0)
if δ = H0
u1I(M = H1)
if δ = H1
u2I(M = Mkγk)
if δ = A∗and k ∈AK and γk = γ∗
k
0
otherwise
The utilities u0, u1, u2 determine the thresholds t0 = u0/u1 and t1 = u1/u2;
see M¨uller et al. (2010) for details.
The approach is summarized in the following algorithm.
Algorithm 6.1 (Decision-theoretic subgroup analysis).
Step 0. Initialization: Fix thresholds t0, t1 and hyperparameters p and
α. Use defaults t0 = t1 = 1, p = 0.6 and α = 1.0.
Determine all the possible subgroup patterns for each covariate, Γk.
Step 1. Marginal posterior probabilities: Evaluate marginal posterior
probabilities p(M) ≡p(M | y) for M = H0, H1 and Mk,γk, γk ∈Γk.
Step 2. MAP subgroup patterns: Record the maximum a posteriori
(MAP) pattern of subgroup eﬀects for each covariate, computed as γ∗
k =
arg maxγk p(Mk,γk). Let AK = {k :
p(γ∗
k)
p(H0) > t0t1 and p(γ∗
k)
p(H1) > t1}. Note
the set Ak could be empty.
Step 3. Report (approximate) Bayes rule δ∗: Evaluate rule (6.21).

280
SPECIAL TOPICS
• If p(H1)/p(H0) > t0 and p(γ∗
k | y)/p(H1) < t1 for all k, then report
H1, an overall non-zero treatment eﬀect.
• If AK ̸= ∅, then report A∗= (AK, AΓ = {γ∗
k, k ∈AK}).
• Report H0 if neither H1 nor A∗are chosen.
6.5 Appendix: R Macros
The online supplement to this chapter
www.biostat.umn.edu/~brad/software/BCLM_ch6.html
provides the R and BRugs code that was used to illustrate the examples in
this chapter.

References
Abbruzzese, J.L., Grunewald, R., Weeks, E.A., Gravel, D., Adams, T., Nowak,
B., Mineishi, S., Tarassoﬀ, P., Satterlee, W., and Raber, M.N. (1991). A phase
I clinical, plasma, and cellular pharmacology study of gemcitabine. J. Clin.
Oncology, 9, 491-498.
Albert, J.H. (1996). Bayesian Computation Using Minitab. Belmont, CA:
Wadsworth.
Albert, J.H. (2007). Bayesian Computation with R. New York: Springer.
Albert, J.H. and Chib, S. (1993). Bayesian analysis of binary and polychotomous
response data. J. Amer. Statist. Assoc., 88, 669–679.
Aranda-Ordaz, F.J. (1983). On two families of transformations to additivity for
binary response data. Biometrika, 68, 357–363.
Babb, J. and Rogatko, A. (2001). Patient speciﬁc dosing in a cancer phase I
clinical trial. Statistics in Medicine, 20, 2079-2090.
Babb, J. and Rogatko, A. (2004). Bayesian methods for cancer phase I clinical
trials. In Contemporary Biostatistical Methods in Clinical Trials, ed. N. Geller,
New York: Marcel Dekker, pp. 1–40.
Babb, J., Rogatko, A., and Zacks, S. (1998). Cancer phase I clinical trials: eﬃcient
dose escalation with overdose control. Statistics in Medicine, 17, 1103–1120.
Banerjee, S., Carlin, B.P., and Gelfand, A.E. (2004). Hierarchical Modeling and
Analysis for Spatial Data. Boca Raton, FL: Chapman and Hall/CRC Press.
Barker, A.D., Sigman, C.C., Kelloﬀ, G.J., Hylton, N.M., Berry, D.A., and Es-
serman, L.J. (2009). I-SPY 2: An adaptive breast cancer trial design in the
setting of neoadjuvant chemotherapy. Clinical Pharmacology and Therapeu-
tics, 86, 97–100.
Barker, L., Rolka, H., Rolka, D., and Brown, C. (2001). Equivalence testing for
binomial random variables: which test to use? The American Statistician, 55,
279–287.
Bate, A., Lindquist, M., Edwards, I.R., Olsson, S., Orre, R., Lansner, A., and
De Freitas, R.M. (1998). A Bayesian neural network method for adverse drug
reaction signal generation. Eur. J. Clin. Pharmacol., 54, 315–321.
Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances.
Philos. Trans. Roy. Soc. London, 53, 370–418. Reprinted, with an introduction
by George Barnard, in 1958 in Biometrika, 45, 293–315.
Bekele, B., Ji, Y., Shen, Y., and Thall, P. (2008). Monitoring late-onset toxicities
in phase I trials using predicted risks. Biostatistics, 9, 442–57.
Bekele, B. and Shen, Y. (2005). A Bayesian approach to jointly modeling toxicity

282
REFERENCES
and biomarker expression in a phase I/II dose-ﬁnding trial. Biometrics, 61,
343–354.
Benjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery rate: A
practical and powerful approach to multiple testing. J. Roy. Statist. Soc., Ser.
B, 57, 289–300.
Berger, J.O. (1985). Statistical Decision Theory and Bayesian Analysis, 2nd ed.
New York: Springer-Verlag.
Berger, J.O. and Berry, D.A. (1988). Statistical analysis and the illusion of ob-
jectivity. American Scientist, 76, 159–165.
Berger, J.O. and Pericchi, L.R. (1996). The intrinsic Bayes factor for linear mod-
els. In Bayesian Statistics 5, eds. J.M. Bernardo, J.O. Berger, A.P. Dawid, and
A.F.M. Smith, Oxford: Oxford University Press, pp. 25–44.
Berger, J.O. and Wolpert, R. (1984). The Likelihood Principle. Hayward, CA:
Institute of Mathematical Statistics Monograph Series.
Berger, R.L. and Hsu, J.C. (1996). Bioequivalence trials, intersection-union tests
and equivalence conﬁdence sets. Statistical Science, 11, 283–319.
Bernardo, J.M. and Smith, A.F.M. (1994). Bayesian Theory. New York: John
Wiley & Sons.
Berry, D.A. (1989). Monitoring accumulating data in a clinical trial. Biometrics,
45, 1197–1211.
Berry, D.A. (1991). Bayesian methods in phase III trials. Drug Information Jour-
nal, 25, 345–368.
Berry, D.A. (1993). A case for Bayesianism in clinical trials (with discussion).
Statistics in Medicine, 12, 1377–1404.
Berry, D.A. (1996). Statistics: A Bayesian Perspective. Belmont, CA: Duxbury.
Berry, D.A. (2005). Introduction to Bayesian methods III: use and interpretation
of Bayesian tools in design and analysis (with discussion). Clinical Trials, 2,
295–300; discussion 301–304, 364–378.
Berry, D.A. (2006). Bayesian clinical trials. Nature Reviews Drug Discovery, 5,
27–36.
Berry, D.A. and Eick, S. (1995). Adaptive assignment versus balanced randomiza-
tion in clinical trials: a decision analysis. Statistics in Medicine, 14, 231–246.
Berry, D.A. and Ho, C.-H. (1988). One-sided sequential stopping boundaries for
clinical trials: A decision-theoretic approach. Biometrics, 44, 219–227.
Berry, D.A. and Hochberg, Y. (1999). Bayesian perspectives on multiple compar-
isons. J. Statist. Plann. Inf., 82, 215-227.
Berry, D.A., M¨uller, P., Grieve, A.P., Smith, M., Parke, T., Blazek, R., Mitchard,
N., and Krams, M. (2001). Adaptive Bayesian designs for dose-ranging drug
trials (with discussion and rejoinder). In Case Studies in Bayesian Statistics,
Volume V, eds. C. Gatsonis, R.E. Kass, B.P. Carlin, A. Carriquiry, A. Gelman,
I. Verdinelli, and M. West, Lecture Notes in Statistics, New York: Springer-
Verlag, pp. 99–181.
Berry, D.A. and Stangl, D.K., eds. (1996). Bayesian Biostatistics. New York:
Marcel Dekker.
Berry, D.A. and Stangl, D.K., eds. (2000). Meta-Analysis in Medicine and Health
Policy. Boca Raton, FL: Chapman and Hall/CRC Press.
Berry, S.M. and Berry, D.A. (2004). Accounting for multiplicities in assessing

REFERENCES
283
drug safety: a three-level hierarchical mixture model. Biometrics, 60, 418–426.
Birnbaum, A. (1962). On the foundations of statistical inference (with discussion).
J. Amer. Statist. Assoc., 57, 269–326.
Biswas, S., Liu, D.D., Lee, J.J., and Berry, D.A. (2009). Bayesian clinical trials
at the University of Texas M.D. Anderson Cancer Center. Clinical Trials, 6,
205–216.
Box, G.E.P. and Tiao, G. (1973). Bayesian Inference in Statistical Analysis. Lon-
don: Addison-Wesley.
Braun, T.M. (2002). The bivariate continual reassessment method: extending the
CRM to phase I trials of two competing outcomes. Controlled Clinical Trials,
23, 240–255.
Braun, T.M., Yuan, Z., and Thall, P.F. (2005). Determining a maximum-tolerated
schedule of a cytotoxic agent. Biometrics, 61, 335–343.
Brockwell, A.E. and Kadane, J.B. (2003). A gridding method for Bayesian se-
quential decision problems. Journal of Computational and Graphical Statistics,
12, 566–584.
Brook, R.H., Chassin, M.R., Fink, A., Solomon, D.H., Kosecoﬀ, J., and Park, R.E.
(1986). A method for the detailed assessment of the appropriateness of medical
technologies. International Journal of Technology Assessment and Health Care,
2, 53–63.
Brooks, S.P. and Gelman, A. (1998). General methods for monitoring convergence
of iterative simulations. J. Comp. Graph. Statist., 7, 434–455.
Bryant, J. and Day, R. (1995). Incorporating toxicity considerations into the
design of two-stage phase II clinical trials. Biometrics, 51, 1372–1383.
Buzdar, A.U., Ibrahim, N.K., Francis, D., Booser, D.J., Thomas, E.S., Rivera, E.,
Theriault, R.L., Murray, J.L., Pusztai, L., Rosales, M.F., Green, M.J., Walters,
R., Arun, B.K., Giordano, S.H., Cristofanilli, M., Frye, D.K., Smith, T.L.,
Hunt, K.K., Singletary, S.E., Sahin, A.A., Ewer, M.S., Buchholz, T.A., Berry,
D.A., and Hortobagyi, G.N. (2005). Signiﬁcantly higher pathological complete
remission rate following neoadjuvant therapy with trastuzumab, paclitaxel and
epirubicin-containing chemotherapy: results of a randomized trial in HER-2-
positive operable breast cancer. Journal of Clinical Oncology, 23, 3676–3685.
Carlin, B.P., Kadane, J.B., and Gelfand, A.E. (1998). Approaches for optimal
sequential decision analysis in clinical trials. Biometrics, 54, 964–975.
Carlin, B.P. and Louis, T.A. (2009). Bayesian Methods for Data Analysis, 3rd
ed. Boca Raton, FL: Chapman and Hall/CRC Press.
Carmer, S.G. and Walker, W.M. (1982). Baby bear’s dilemma: a statistical tale.
Agronomy Journal, 74, 122–124.
Chen, M.-H. and Ibrahim, J.G. (2006). The relationship between the power prior
and hierarchical models. Bayesian Analysis, 1, 554–571.
Chen, M.-H., Shao, Q.-M., and Ibrahim, J.G. (2000). Monte Carlo Methods in
Bayesian Computation. New York: Springer-Verlag.
Chen, T.T. (1997). Optimal three-stage designs for phase II cancer clinical trials.
Statist. Med., 16, 2701–2711.
Chen, T.T. and Ng, T.H. (1998). Optimal ﬂexible designs in phase II clinical
trials. Statist. Med., 17, 2301-2312.
Cheung, Y.K. and Chappell, R. (2000). Sequential designs for phase I clinical

284
REFERENCES
trials with late-onset toxicities. Biometrics, 56, 1177–1182.
Cheung, Y.K., Inoue, L.Y.T., Wathen, J.K., and Thall, P.F. (2006). Continu-
ous Bayesian adaptive randomization based on event times with covariates.
Statistics in Medicine, 25, 55–70.
Chib, S. and Jacobi, L. (2008). Analysis of treatment response data from eligi-
bility designs. J. of Econometrics, 144, 465–478.
Chinchilli, V.M. and Elswick, R.K. (1997). The multivariate assessment of bio-
equivalence. J. Biopharmaceutical Statistics, 7, 113-123.
Chow, S.-C. and Liu, J.-P. (2000). Design and Analysis of Bioavailability and
Bioequivalence Studies, 2nd ed. New York: Marcel Dekker.
Chu, P.-L., Lin, Y., and Shih, W.J. (2009). Unifying CRM and EWOC designs
for phase I cancer clinical trials. J. Statist. Plann. Inf., 139, 1146–1163.
Collins, J.M., Grieshaber, C.K., and Chabner, B.A. (1990). Pharmacologically
guided phase I clinical trials based upon preclinical drug development. J. Natl.
Cancer Inst., 82, 1321–1326.
Collins, J.M., Zaharko, D.S., Dedrick, R.L., and Chabner, B.A. (1986). Potential
roles for preclinical pharmacology in Phase I clinical trials. Cancer Treat. Rep.,
70, 73-80.
Cook, T.D. and Demets, D.L., eds. (2008). Introduction to Statistical Methods
for Clinical Trials. Boca Raton, FL: Chapman & Hall/CRC Press.
Cornﬁeld, J. (1966a). Sequential trials, sequential analysis and the likelihood
principle. The American Statistician, 20, 18–23.
Cornﬁeld, J. (1966b). A Bayesian test of some classical hypotheses – with appli-
cations to sequential clinical trials. J. Amer. Statist. Assoc., 61, 577–594.
Cornﬁeld, J. (1969). The Bayesian outlook and its applications. Biometrics, 25,
617–657.
Cowles, M.K. and Carlin, B.P. (1996). Markov chain Monte Carlo convergence
diagnostics: A comparative review. J. Amer. Statist. Assoc., 91, 883–904.
DeGroot, M.H. (1970). Optimal Statistical Decisions. New York: McGraw-Hill.
DeFinetti, B. (1992). Theory of Probability: A Critical Introductory Treatment,
Volumes 1 & 2. New York: John Wiley (Classics Library).
DeSantis, F. (2007). Using historical data for Bayesian sample size determination.
J. Roy. Statist. Soc., Ser. A, 170, 95–113.
DeSouza, R.M., Achcar, J.A., and Martinez, E.Z. (2009). Use of Bayesian meth-
ods for multivariate bioequivalence measures. J. Biopharmaceutical Statistics,
19, 42–66.
Dixon, D.O. and Simon, R. (1991). Bayesian subset analysis. Biometrics, 47,
871–882.
Dixon, W.J. and Mood, A.M. (1948). A method for obtaining and analyzing
sensitivity data. J. Amer. Statist. Assoc., 43, 109–126.
Duan, Y., Ye, K., and Smith, E.P. (2006). Evaluating water quality using power
priors to incorporate historical information. Environmetrics, 17, 95–106.
DuMouchel, W. (1990). Bayesian meta-analysis. In Statistical Methods for Phar-
macology, ed. D. Berry, New York: Marcel Dekker, pp. 509–529.
DuMouchel, W. (1999). Bayesian data mining in large frequency tables, with an
application to the FDA Spontaneous Reporting System (with discussion). The
American Statistician, 53, 177–202.

REFERENCES
285
Edwards, W., Lindman, H., and Savage, L.J. (1963). Bayesian statistical inference
for psychological research. Psych. Rev., 70, 193–242.
Efron, B. and Feldman, D. (1991). Compliance as an explanatory variable in
clinical trials. J. Amer. Statist. Assoc., 86, 9–17.
Eisenhauer, E.A., Therasse, P., Bogaerts, J., Schwartz, L.H., Sargent, D., Ford,
R., Dancey, J., Arbuck, S., Gwyther, S., Mooney, M., Rubinstein, L., Shankar,
L., Dodd, L., Kaplan, R., Lacombe, D., and Verweij, J. (2009). New response
evaluation criteria in solid tumours: Revised RECIST guideline (version 1.1).
Eur. J. Cancer, 45(2), 228–247.
Ensign, L.G., Gehan, E.A., Kamen, D.S., and Thall, P.F. (1994). An optimal
three-stage design for phase II clinical trials. Statistics in Medicine, 13, 1727–
1736.
Erickson, J.S., Stamey, J.D., and Seaman, J.W. (2006). Bayesian methods for
bioequivalence studies. Advances and Applications in Statistics, 6, 71–85.
Fan, S.K. and Chaloner, K. (2004). Optimal designs and limiting optimal designs
for a trinomial response. J. Statist. Plann. Inf., 126, 347–360.
Faries, D. (1994). Practical modiﬁcations of the continual reassessment method
for phase I cancer clinical trials. J. Biopharmaceutical Statistics, 4, 147–164.
Fleming, T.R. (1982). One-sample multiple testing procedure for phase II clinical
trials. Biometrics, 38, 143–151.
Food and Drug Administration (1992). Bioavailability and bioequivalence re-
quirements. U.S. Code of Federal Regulations, Vol. 21, Chap. 320. Washington,
DC: U.S. Government Printing Oﬃce.
Food
and
Drug
Administration
(1999).
Statistical
Approaches
to
Estab-
lishing Bioequivalence. U.S. Department of Health and Human Services,
FDA, Center for Drug Evaluation and Research (CDER), Rockville, MD
(www.fda.gov/cder/guidance).
Food and Drug Administration (2001). Average, Population, and Individual
Approaches to Establishing Bioequivalence. U.S. Department of Health and
Human Services, FDA, Center for Drug Evaluation and Research (CDER),
Rockville, MD (www.fda.gov/cder/guidance).
Food and Drug Administration (2002). Bioavailability and Bioequivalence Studies
for Orally Administered Drug Products – General Considerations. U.S. Depart-
ment of Health and Human Services, FDA, Center for Drug Evaluation and
Research (CDER), Rockville, MD (www.fda.gov/cder/guidance).
Freedman, L.S., Lowe, D., and Macaskill, P. (1984). Stopping rules for clinical
trials incorporating clinical opinion. Biometrics, 40, 575–586.
Freedman, L.S. and Spiegelhalter, D.J. (1983). The assessment of subjective opin-
ion and its use in relation to stopping rules for clinical trials. The Statistician,
32, 153–160.
Freedman, L.S. and Spiegelhalter, D.J. (1989). Comparison of Bayesian with
group sequential methods for monitoring clinical trials. Controlled Clinical Tri-
als, 10, 357–367.
Freedman, L.S. and Spiegelhalter, D.J. (1992). Application of Bayesian statistics
to decision making during a clinical trial. Statistics in Medicine, 11, 23–35.
Freireich, E.J., Gehan, E., Frei, E., Schroeder, L.R., Wolman, I.J., Anbari, R.,
Burgert, E.O., Mills, S.D., Pinkel, D., Selanry, O.S., Moon, J.H., Gendel, B.R.,

286
REFERENCES
Spurr, C.L., Storrs, R., Haurani, F., Hoogstraten, B., and Lee, S. (1963). The
eﬀect of 6-mercaptopurine on the duration of steroid-induced remissions in
acute leukemia: a model for evaluation of other potentially useful therapy.
Blood, 21, 699–716.
Fr¨uhwirth-Schnatter, S. (1994). Data augmentation and dynamic linear models.
Journal of Time Series Analysis, 15, 183–202.
Fuquene, J.P., Cook, J.D., and Pericchi, L.R. (2009). A case for robust Bayesian
priors with applications to clinical trials. Bayesian Analysis, 4, 817–846.
Gamerman, D. and Lopes, H.F. (2006). Markov Chain Monte Carlo: Stochastic
Simulation for Bayesian Inference, 2nd ed. Boca Raton, FL: Chapman and
Hall/CRC Press.
Gehan, E.A. (1961). The determination of the number of patients required in a
preliminary and a follow-up trial of a new chemotherapeutic agent. Journal of
Chronic Diseases, 13, 346–353.
Gelfand, A.E. and Ghosh, S.K. (1998). Model choice: A minimum posterior pre-
dictive loss approach. Biometrika, 85, 1–11.
Gelfand, A.E. and Smith, A.F.M. (1990). Sampling-based approaches to calcu-
lating marginal densities. J. Amer. Statist. Assoc., 85, 398–409.
Gelman, A. (2006). Prior distributions for variance parameters in hierarchical
models. Bayesian Analysis, 1, 515–534.
Gelman, A., Carlin, J., Stern, H., and Rubin, D.B. (2004). Bayesian Data Anal-
ysis, 2nd ed. Boca Raton, FL: Chapman and Hall/CRC Press.
Gelman, A., Roberts, G.O., and Gilks, W.R. (1996). Eﬃcient Metropolis jumping
rules. In Bayesian Statistics 5, eds. J.M. Bernardo, J.O. Berger, A.P. Dawid,
and A.F.M. Smith, Oxford: Oxford University Press, pp. 599–607.
Gelman, A. and Rubin, D.B. (1992). Inference from iterative simulation using
multiple sequences (with discussion). Statistical Science, 7, 457–511.
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions and
the Bayesian restoration of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6, 721-741.
Geyer, C.J. (1992). Practical Markov Chain Monte Carlo (with discussion). Sta-
tistical Science, 7, 473–511.
Ghosh, P. and G¨onen, M. (2008). Bayesian modeling of multivariate average
bioequivalence. Statistics in Medicine, 27, 2402–2419.
Ghosh, P. and Khattree, R. (2003). Bayesian approach to average bioequivalence
using Bayes’ factor. J. Biopharmaceutical Statistics, 13, 719–734.
Ghosh, P. and Ntzoufras, I. (2005). Testing population and individual bio-
equivalence: a hierarchical Bayesian approach. Technical report, Department
of Statistics, Athens University of Economics and Business.
Ghosh, P. and Rosner, G.L. (2007). A semi-parametric Bayesian approach to
average bioequivalence. Statistics in Medicine, 26, 1224–1236.
Giles, F.J., Kantarjian, H.M., Cortes, J.E., Garcia-Manero, G., Verstovsek, S.,
Faderl, S., Thomas, D.A., Ferrajoli, A., O’Brien, S., Wathen, J.K., Xiao, L.-C.,
Berry, D.A., and Estey, E.H. (2003). Adaptive randomized study of idarubicin
and cytarabine versus troxacitabine and cytarabine versus troxacitabine and
idarubicin in untreated patients 50 years or older with adverse karyotype acute
myeloid leukemia. Journal of Clinical Oncology, 21, 1722–1727.

REFERENCES
287
Gilks, W.R., Richardson, S., and Spiegelhalter, D.J., eds. (1996). Markov Chain
Monte Carlo in Practice. London: Chapman and Hall.
Gilks, W.R. and Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling.
J. Roy. Statist. Soc., Ser. C (Applied Statistics), 41, 337–348.
Goldberg, R.M., Sargent, D.J., Morton, R.F., Fuchs, C.S., Ramanathan, R.K.,
Williamson, S.K., Findlay, B.P., Pitot, H.C., and Alberts, S.R. (2004). A ran-
domized controlled trial of Fluorouracil Plus Leucovorin, Irinotecan, and Oxali-
platin combinations in patients with previously untreated metastatic colorectal
cancer. J. Clin. Oncol., 22, 23–30.
Goodman, S.N., Zahurak, M.L., and Piantadosi, S. (1995). Some practical im-
provements in the continual reassessment method for phase I studies. Statistics
in Medicine, 14, 1149–1161.
Gopalan, R. and Berry, D.A. (1998). Bayesian multiple comparisons using Dirich-
let process priors. J. Amer. Statist. Assoc., 93, 1130-1139.
Gray, R., Manola, J., Saxman, S., Wright, J., Dutcher, J., Atkins, M., Carducci,
M., See, W., Sweeney, C., Liu, G., Stein, M., Dreicer, R., Wilding, G., and
DiPaola, R.S. (2006). Phase II clinical trial design: methods in translational
research from the genitourinary committee at the Eastern Cooperative Oncol-
ogy Group. Clinical Cancer Research, 12, 1966–1969.
Green, S.J. and Dahlberg, S. (1992). Planned versus attained design in phase II
clinical trials. Statist. Med., 11, 853-862.
Greenland, S., Lanes, S., and Jara, M. (2008). Estimating eﬀects from random-
ized trials with discontinuations: The need for intent-to-treat design and G-
estimation. Clinical Trials, 5, 5–13.
Grieve, A.P. (1985). A Bayesian analysis of two-period crossover design for clinical
trials. Biometrics, 41, 979–990.
Haario, H., Saksman, E., and Tamminen, J. (2001). An adaptive Metropolis al-
gorithm. Bernoulli, 7, 223–242.
Hadjicostas, P. (1998). Improper and proper posteriors with improper priors in a
hierarchical model with a beta-binomial likelihood. Communications in Statis-
tics – Theory and Methods, 27, 1905–1914.
Hastings, W.K. (1970). Monte Carlo sampling methods using Markov chains and
their applications. Biometrika, 57, 97–109.
Hauck, W.W., Hyslop, T., Anderson, S., Bois, F.Y., and Tozer, T.N. (1995). Sta-
tistical and regulatory considerations for multiple measures in bioequivalence
testing. Clinical Research and Regulatory Aﬀairs, 12, 249-265.
Hern´an, M.A. and Robins, J.M. (to appear). Causal Inference. Boca Raton, FL:
Chapman and Hall/CRC Press.
Herndon II, J.E. (1998). A design alternative for two-stage, phase II, multicenter
cancer clinical trials. Controlled Clinical Trials, 19, 440-450.
Heyd, J.M. and Carlin, B.P. (1999). Adaptive design improvements in the con-
tinual reassessment method for phase I studies. Statistics in Medicine, 18,
1307–1321.
Hirano, K., Imbens, G.W., Rubin, D.B., and Zhou, X.-H. (2000). Assessing the
eﬀect of an inﬂuenza vaccine in an encouragement design. Biostatistics, 1, 69–
88.
Hobbs, B.P. and Carlin, B.P. (2008). Practical Bayesian design and analysis for

288
REFERENCES
drug and device clinical trials. J. Biopharmaceutical Statistics, 18, 54–80.
Hobbs, B.P., Carlin, B.P., Mandrekar, S., and Sargent, D.J. (2009). Hierarchical
power prior models for adaptive incorporation of historical information in clin-
ical trials. Research Report 2009–017, Division of Biostatistics, University of
Minnesota.
Hodges, J.S., Cui, Y., Sargent, D.J., and Carlin, B.P. (2007). Smoothing balanced
single-error-term analysis of variance. Technometrics, 49, 12–25.
Holland, P. (1986). Statistics and causal inference. J. Amer. Statist. Assoc., 81,
945–970.
Houede, N., Thall, P.F., Nguyen, H., Paoletti, X., and Kramar, A. (2010). Utility-
based optimization of combination therapy using ordinal toxicity and eﬃcacy
in Phase I/II trials. To appear Biometrics.
Huang, X., Biswas, S., Oki, Y., Issa, J.-P., and Berry, D.A. (2007). A parallel
Phase I/II clinical trial design for combination therapies. Biometrics, 63, 429–
436.
Huang, X., Ning, J., Li, Y., Estey, E., Issa, J.-P., and Berry, D.A. (2009). Us-
ing short-term response information to facilitate adaptive randomization for
survival clinical trials. Statistics in Medicine, 28, 1680–1689.
Ibrahim, J.G. and Chen, M.-H. (2000). Power prior distributions for regression
models. Statistical Science, 15, 46–60.
Ibrahim, J.G., Chen, M.-H., and Sinha, D. (2003). On optimality properties of
the power prior. J. Amer. Statist. Assoc., 98, 204–213.
Imbens, G.W. and Rubin, D.B. (1997). Bayesian inference for causal eﬀects in
randomized experiments with noncompliance. Ann. Statist., 25, 305–327.
Inoue, L.Y.T., Thall, P., and Berry, D.A.. (2002) Seamlessly expanding a ran-
domized phase II trial to phase III. Biometrics, 58, 823–831.
Janicak, P.G., Pandey, G.N., Davis, J.M, Boshes, R., Bresnahan, D., and Sharma,
R. (1988). Response of psychotic and nonpsychotic depression to phenelzine.
Amer. J. Psychiatry, 145, 93–95.
Jennison, C. and Turnbull, B.W. (2000). Group Sequential Methods with Appli-
cations to Clinical Trials. Boca Raton, FL: Chapman and Hall/CRC Press.
Ji, Y., Li, Y., and Bekele, B.N. (2007) Dose-ﬁnding in oncology clinical trials
based on toxicity probability intervals. Clinical Trials, 4, 235–244.
Ji, Y., Li, Y., and Yin, G. (2007). Bayesian dose-ﬁnding designs for phase I clinical
trials. Statistica Sinica, 17, 531–547.
Jin, H. and Rubin, D.B. (2008). Principal stratiﬁcation for causal inference with
extended partial compliance. J. Amer. Statist. Assoc., 103, 101–111.
Johnson, V.E. and Albert, J.H. (2000). Ordinal Data Modeling. New York:
Springer-Verlag.
Johnson, V.E. and Cook, J.D. (2009). Bayesian design of single-arm phase II
clinical trials with continuous monitoring. Clinical Trials, 6, 217–226.
Johnson, V.E. and Rossell, D. (2010). On the use of non-local prior densities for
default Bayesian hypothesis tests. J. Roy. Statist. Soc., Ser. B, 72, 143–170.
Kadane, J.B., ed. (1996). Bayesian Methods and Ethics in a Clinical Trial Design.
New York: John Wiley & Sons.
Kadane, J.B., Dickey, J.M., Winkler, R.L., Smith, W.S., and Peters, S.C. (1980).
Interactive elicitation of opinion for a normal linear model. J. Amer. Statist.

REFERENCES
289
Assoc., 75, 845–854.
Kass, R.E., Carlin, B.P., Gelman, A., and Neal, R. (1998). Markov chain Monte
Carlo in practice: A roundtable discussion. The American Statistician, 52,
93–100.
Kass, R.E. and Greenhouse, J.B. (1989). A Bayesian perspective. Invited com-
ment on “Investigating therapies of potentially great beneﬁt: ECMO,” by J.H.
Ware. Statistical Science, 4, 310–317.
Kass, R.E. and Raftery, A.E. (1995). Bayes factors. J. Amer. Statist. Assoc., 90,
773–795.
Kola, I. and Landis, J. (2004). Can the pharmaceutical industry reduce attrition
rates? Nature Reviews Drug Discovery, 3, 711–715.
Korn, E.L., Midthune, D., Chen, T.T., Rubinstein, L.V., Christian, M.C., and
Simon, R.M. (1994). A comparison of two phase I trial designs. Statistics in
Medicine, 13, 1799–1806.
Krams, M., Lees, K.R., and Berry, D.A. (2005). The past is the future: innovative
designs in acute stroke therapy trials. Stroke, 36, 1341–1347.
Lan, K.K.G. and DeMets, D.L. (1983). Discrete sequential boundaries for clinical
trials. Biometrika, 70, 659–663.
Lauzon, C. and Caﬀo, B. (2009). Easy multiplicity control in equivalence testing
using two one-sided tests. The American Statistician, 63, 147–154.
Le Tourneau, C., Lee, J.J., and Siu, L.L. (2009). Dose escalation methods in
phase I cancer clinical trials. J. Natl. Cancer Inst., 101, 708–720.
Lee, J.J. and Feng, L. (2005). Randomized phase II designs in cancer clinical
trials: current status and future directions. Journal of Clinical Oncology, 23,
4450–4457.
Lee, J.J. and Liu, D.D. (2008). A predictive probability design for phase II cancer
clinical trials. Clinical Trials, 5, 93–106.
Lee, P.M. (1997). Bayesian Statistics: An Introduction, 2nd ed. London: Arnold.
Lin, Y. and Shih, W.J. (2001). Statistical properties of the traditional algorithm-
based designs for phase I cancer clinical trials. Biostatistics, 2, 203–215.
Lindley, D.V. (1972). Bayesian statistics: A review. Philadelphia: SIAM.
Lindley, D.V. (1998). Decision analysis and bioequivalence trials. Statistical Sci-
ence, 13, 136–141.
Liu, P.Y., LeBlanc, M., and Desai, M. (1999). False positive rates of randomized
phase II designs. Controlled Clinical Trials, 20, 343–352.
Liu, J.S. (2008). Monte Carlo Strategies in Scientiﬁc Computing. New York:
Springer.
Louis, T.A. (1975). Optimal allocation in sequential tests comparing the means
of two Gaussian populations. Biometrika, 62, 359–370 (correction: 63, 218).
Louis, T.A. (1977). Sequential allocation in clinical trials comparing two expo-
nential survival curves. Biometrics, 33, 627–634.
Lumley,
T.
and
Maechler,
M.
(2007).
R package adapt – multidimensional numerical integration. Available online
at http://cran.r-project.org/src/contrib/Archive/adapt/.
MacEachern, S.N. and Berliner, L.M. (1994). Subsampling the Gibbs sampler.
The American Statistician, 48, 188–190.
Mandrekar, S.J., Cui, Y., and Sargent, D.J. (2007). An adaptive phase I design

290
REFERENCES
for identifying a biologically optimal dose for dual agent drug combinations.
Statistics in Medicine, 26, 2317–2330.
Mariani, L. and Marubini, E. (1996). Design and analysis of phase II cancer
trials: a review of statistical methods and guidelines for medical researchers.
International Statistical Review, 64, 61–88.
Marin, J.-M. and Robert, C.P. (2007) Bayesian Core: A Practical Approach to
Computational Bayesian Statistics. New York: Springer.
McCarthy, A. (2009). Is it time to change the design of clinical trials? Cure,
September 12, 2009; available online at www.curetoday.com/index.cfm/
fuseaction/article.PrintArticle/article_id/371.
Mealli, F. and Rubin, D.B. (2002). Assumptions when analyzing randomized
experiments with noncompliance and missing outcomes. Health Services and
Outcomes Research Methodology, 3, 225–232.
Mengersen, K.L., Robert, C.P., and Guihenneuc-Jouyaux, C. (1999). MCMC con-
vergence diagnostics: A reviewww (with discussion). In Bayesian Statistics 6,
eds. J.M. Bernardo, J.O. Berger, A.P. Dawid, and A.F.M. Smith. Oxford: Ox-
ford University Press, pp. 415–440.
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., and Teller, E.
(1953). Equations of state calculations by fast computing machines. J. Chem-
ical Physics, 21, 1087–1091.
Mook, S., Van’t Veer, L.J., Rutgers, E.J., Piccart-Gebhart, M.J. and Cardoso, F.
(2007). Individualization of therapy using MammaPrint: from development to
the MINDACT trial. Cancer Genomics Proteomics, 4, 147–155.
M¨uller, P., Berry, D., Grieve, A., and Krams, M. (2006). A Bayesian decision-
theoretic dose ﬁnding trial. Decision Analysis, 3, 197–207.
M¨uller, P., Parmigiani, G., and Rice, K. (2007). FDR and Bayesian multiple
comparisons rules. In Bayesian Statistics 8, eds. J.M. Bernardo, M.J. Bayarri,
J.O. Berger, A.P. Dawid, D. Heckerman, A.F.M. Smith, and M. West, Oxford:
Oxford University Press, pp. 349–370.
M¨uller, P., Sivaganesan, S., and Laud, P.W. (2010). A Bayes rule for subgroup
reporting. To appear in Frontiers of Statistical Decision Making and Bayesian
Analysis, eds. M.-H. Chen, D.K. Dey, P. M¨uller, D. Sun, and K. Ye, New York:
Springer-Verlag.
Natarajan, R. and McCulloch, C.E. (1995). A note on the existence of the poste-
rior distribution for a class of mixed models for binomial responses. Biometrika,
82, 639–643.
Neal, R.M. (2003). Slice sampling (with discussion). Annals of Statistics, 31,
705–767.
Neaton, J.D., Normand, S.-L., Gelijns, A., Starling, R.C., Mann, D.L., and Kon-
stam, M.A., for the HFSA Working Group (2007). Designs for mechanical
circulatory support device studies. J. Cardiac Failure, 13, 63–74.
Neelon, B. and O’Malley, A.J. (2010). The use of power prior distributions for
incorporating historical data into a Bayesian analysis. CEHI Working Paper
2010-01, Nicholas School of Business, Duke University.
Neelon, B., O’Malley, A.J., and Margolis, P.A. (2008). Bayesian analysis using
historical data with aplication to pediatric quality of care. ASA Proceedings of
the Bayesian Statistical Sciences Section, 2008, 2960–2967.

REFERENCES
291
Nelsen, R.B. (1999). An Introduction to Copulas. New York: Springer-Verlag.
Neuenschwander, B., Branson, M., and Gsponer, T. (2008). Critical aspects of
the Bayesian approach to phase I cancer trials. Statistics in Medicine, 27,
2420–2439.
Neuenschwander, B., Branson, M., and Spiegelhalter, D.J. (2009). A note on the
power prior. Statistics in Medicine, 28, 3562-3566,
Neuenschwander, B., Capkun-Niggli, G., Branson, M., and Spiegelhalter, D.J.
(2010). Summarizing historical information on controls in clinical trials. Clin-
ical Trials, 7, 5-18.
Newton, M., Noueriry, A., Sarkar, D. and Ahlquist, P. (2004). Detecting dif-
ferential gene expression with a semiparametric heirarchical mixture model.
Biostatistics, 5, 155–176.
O’Brien, P.C. (1983). The appropriateness of analysis of variance and multiple-
comparison procedures. Biometrics, 39, 787–788.
O’Brien, P.C. and Fleming, T.R., (1979). A multiple testing procedure for clinical
trials. Biometrics, 35, 549–556.
O’Hagan, A. (1995). Fractional Bayes factors for model comparison (with discus-
sion). J. Roy. Statist. Soc., Ser. B, 57, 99–138.
O’Hagan, A., Buck, C.E., Daneshkhah, A., Eiser, J.R., Garthwaite, P.H., Jenkin-
son, D.J., Oakley, J.E., and Rakow, T. (2006). Uncertain Judgements: Eliciting
Experts’ Probabilities. Chichester, UK: John Wiley & Sons.
O’Hagan, A. and Forster, J. (2004). Bayesian Inference: Kendall’s Advanced The-
ory of Statistics Volume 2B, 2nd ed. London: Edward Arnold.
O’Hagan, A. and Stevens, J.W. (2001). Bayesian assessment of sample size for
clinical trials of cost-eﬀectiveness. Medical Decision Making, 21, 219–230.
O’Hagan, A., Stevens, J.W., and Montmartin, J. (2000). Inference for the C/E
acceptability curve and C/E ratio. PharmacoEconomics, 17, 339–349.
O’Hagan, A., Stevens, J.W., and Montmartin, J. (2001). Bayesian cost eﬀective-
ness analysis from clinical trial data. Statistics in Medicine, 20, 733–753.
O’Quigley, J., Pepe, M., and Fisher, L. (1990). Continual reassessment method:
a practical design for phase I clinical trials in cancer. Biometrics, 46, 33–48.
Pazdur, R., Newman, R.A., Newman, B.M., Fuentes, A., Benvenuto, J., Bready,
B., Moore, Jr., D., Jaiyesimi, I., Vreeland, F, Bayssas, M.M.G., and Raber,
M.N. (1992). Phase I trial of taxotere: ﬁve-day schedule. J. Natl. Cancer Inst.,
84, 1781–1788.
Pearl, J. (2000). Causality: Models, Reasoning, and Inference. Cambridge, UK:
Cambridge University Press.
Piantadosi, S. (2005). Clinical Trials : A Methodologic Perspective. Hoboken, NJ:
Wiley-Interscience.
Piantadosi, S., Fisher, J.D., and Grossman, S. (1998). Practical implementation
of a modiﬁed continual reassessment method for dose-ﬁnding trials. Cancer
Chemother. Pharmacol., 41, 429–436.
Pocock, S.J. (1976). The combination of randomized and historical controls in
clinical trials. J. Chronic Disease, 29, 175–188.
Pocock, S.J. (1977). Group sequential methods in the design and analysis of
clinical trials. Biometrika, 64, 191–199.
Pocock, S.J. (1983). Clinical Trials: A Practical Approach. Chichester, UK: Wiley.

292
REFERENCES
Pocock, S.J., Assmann, S.E., Enos, L.E., and Kasten, L.E. (2002). Subgroup anal-
ysis, covariate adjustment and baseline comparisons in clinical trial reporting:
Current practice and problems. Statistics in Medicine, 21, 2917–2930.
Prentice, R.L., Langer, R.D., Stefanick, M.L., Howard, B.V., Pettinger, M., An-
derson, G.L., Barad, D., Curb, J.D., Kotchen, J., Kuller, L., Limacher, M., and
Wactawski-Wende, J., for the Women’s Health Initiative Investigators (2006).
Combined analysis of Women’s Health Initiative observational and clinical trial
data on postmenopausal hormone treatment and cardiovascular disease. Amer.
J. Epid., 163, 589–599.
Quan, H., Bolognese, J., and Yuan, W. (2001). Assessment of equivalence on
multiple endpoints. Statistics in Medicine, 20, 3159-3173.
Racine-Poon, A., Grieve, A.P., Fluhler, H., and Smith, A.F.M. (1987). A two
stage procedure for bioequivalence studies. Biometrics, 43, 847–856.
Robert, C.P. (2001). The Bayesian Choice, 2nd ed. New York: Springer-Verlag.
Robert, C.P. and Casella, G. (2005). Monte Carlo Statistical Methods, 2nd ed.
New York: Springer-Verlag.
Roberts, G.O. and Rosenthal, J.S. (2007). Coupling and ergodicity of adaptive
Markov chain Monte Carlo algorithms. J. Applied Probability, 44, 458–475.
Roberts, G.O. and Smith, A.F.M. (1993). Simple conditions for the convergence
of the Gibbs sampler and Metropolis-Hastings algorithms. Stochastic Processes
and their Applications, 49, 207–216.
Robertson, T., Wright, F.T., and Dykstra, R.L. (1988). Order Restricted Statis-
tical Inference. Chichester, UK: John Wiley and Sons.
Robins, J.M. (1998). Correction for non-compliance in equivalence trials. Statis-
tics in Medicine, 17, 269–302.
Robins, J.M. and Tsiatis, A.A. (1991). Correcting for non-compliance in random-
ized trials using rank preserving structural failure time models. Communica-
tions in Statistics, Ser. A, 20, 2609–2631.
Robins, J.M. and Greenland, S. (1994). Adjusting for diﬀerential rates of PCP
prophylaxis in high- versus low-dose AZT treatment arms in an AIDS random-
ized trial. J. Amer. Statist. Assoc., 89, 737–749.
Rogatko, A., Tighiouart, M., and Xu, R. (2008). EWOC User’s Guide,
Version 2.1. Winship Cancer Institute, Emory University, Atlanta, GA.
http://sisyphus.emory.edu/software_ewoc.php
Rossell, D., M¨uller, P., and Rosner, G. (2007). Screening designs for drug devel-
opment. Biostatistics, 8, 595–608.
Rossi, P.E., Allenby, G., and McCulloch, R. (2005). Bayesian Statistics and Mar-
keting. New York: John Wiley and Sons.
Rothman, K.J. (1990). No adjustments are needed for multiple comparisons.
Epidemiology, 1, 43-46.
Rothwell, P.M. (2005). Subgroup analysis in randomised controlled trials: impor-
tance, indications, and interpretation. Lancet, 365 176–186.
Rubin, D.B. (1984). Bayesianly justiﬁable and relevant frequency calculations for
the applied statistician. Ann. Statist., 12, 1151–1172.
Rubin, D.B. (2005). Causal inference using potential outcomes: design, modeling,
decisions. J. Amer. Statist. Assoc., 100, 322–331.
Saltz, L.B., Cox, J.V., Blanke, C., Rosen, L.S., Fehrenbacher, L., Moore, M.J.,

REFERENCES
293
Maroun, J.A., Ackland, S.P., Locker, P.K., Pirotta, N., Elfring, G.L., and
Miller, L.L., for the Irinotecan Study Group (2000). Irinotecan plus ﬂuorouracil
and leucovorin for metastatic colorectal cancer. New Engl. J. Med., 343, 905–
914.
Savage, L.J. (1972). The Foundations of Statistics, revised edition. New York:
Dover Publications.
Scher, H.I. and Heller, G. (2002). Picking the winners in a sea of plenty. Clinical
Cancer Research, 8, 400–404.
Schuirmann, D. (1987). A comparison of the two one-sided tests procedure and
the power approach for assessing the equivalence of average bioavailability. J.
Pharmacokinetics and Pharmacodynamics, 15, 657–680.
Schultz, J.R., Nichol, F.R., Elfring, G.L., and Weed, S.D. (1973). Multiple-stage
procedures for drug screening. Biometrics, 29, 293–300.
Schwarz, G. (1978). Estimating the dimension of a model. Ann. Statist., 6, 461–
464.
Sethuraman, J. (1994). A constructive deﬁnition of Dirichlet priors. Statistica
Sinica, 4, 639-650.
Seymour, L., Ivy, P., Sargent, D., Spriggs, D., Baker, L., Rubinstein, L., Ratain,
M., Le Blanc, M., Stewart, D., Crowley, J., Groshen, S., Humphrey, J., West,
P., and Berry, D. (2010). The design of Phase II clinical trials testing cancer
therapeutics: Consensus recommendations from the clinical trial design task
force of the National Cancer Institute Investigational Drug Steering Commit-
tee. Clinical Cancer Research, 16, 1764–1769.
Shih, J.H. and Louis, T.A. (1995). Inferences on the association parameter in
copula models for bivariate survival data. Biometrics, 51, 1384–1399.
Simon, R. (1989). Optimal two-stage designs for phase II clinical trials. Controlled
Clinical Trials, 10, 1–10.
Simon, R. (2002). Bayesian subset analysis: application to studying treatment-
by-gender interactions. Statistics in Medicine, 21, 2909–2916.
Simon, R., Freidlin, B., Rubinstein, L., Arbuck, S.G., Collins, J., and Christian,
M.C. (1997). Accelerated titration designs for phase I clinical trials in oncology.
J. Natl. Cancer Inst., 89, 1138–1147.
Simon, R., Wittes, R.E., and Ellenberg, S.S. (1985). Randomized phase II clinical
trials. Cancer Treatment Reports, 69, 1375–1381.
Sivaganesan, S., Laud, P., and M¨uller, P. (2008). A Bayesian subgroup analy-
sis with a zero-enriched Polya urn scheme. Technical report, Department of
Mathematical Sciences, University of Cincinnati.
Smith, M., Jones, I., Morris, M., Grieve, A., and Tan, K. (2006). Implementation
of a Bayesian adaptive design in a proof of concept study. Pharmaceutical
Statistics, 5, 39–50.
Smith, M.K. and Richardson, H. (2007). WinBUGSio: A SAS macro for the remote
execution of WinBUGS. J. Statistical Software, 23(9).
Smith, T.C., Spiegelhalter, D.J., and Parmar, M.K.B. (1996). Bayesian meta-
analysis of randomized trials using graphical models and BUGS. In Bayesian
Biostatistics, eds. D.A. Berry and D.K. Stangl, New York: Marcel Dekker, pp.
411-427.
Smith, T.L., Lee, J.J., Kantarjian, H.M., Legha, S.S., and Raber, M.N. (1996).

294
REFERENCES
Design and results of phase I cancer clinical trials: three-year experience at
M.D. Anderson Cancer Center. J. Clin. Oncology, 14, 287–295.
Spiegelhalter, D.J., Abrams, K.R., and Myles, J.P. (2004). Bayesian Approaches
to Clinical Trials and Health-Care Evaluation. Chichester, UK: John Wiley &
Sons.
Spiegelhalter, D.J., Best, N., Carlin, B.P., and van der Linde, A. (2002). Bayesian
measures of model complexity and ﬁt (with discussion). J. Roy. Statist. Soc.,
Ser. B, 64, 583–639.
Spiegelhalter, D.J., Freedman, L.S., and Parmar, M.K.B. (1994). Bayesian ap-
proaches to randomised trials (with discussion). J. Roy. Statist. Soc., Ser. A,
157, 357–416.
Storer, B.E. (1989). Design and analysis of phase I clinical trials. Biometrics, 45,
925–937.
Storer, B.E. (2001). An evaluation of phase I clinical trial designs in the contin-
uous dose-response setting. Statistics in Medicine, 20, 2399–2408.
Tanner, M.A. (1998). Tools for Statistical Inference: Methods for the Explo-
ration of Posterior Distributions and Likelihood Functions, 3rd ed. New York:
Springer-Verlag.
Thall, P.F. and Cook, J.D. (2004). Dose-ﬁnding based on eﬃcacy-toxicity trade-
oﬀs. Biometrics, 60, 684–693.
Thall, P.F., Cook, J.D., and Estey, E.H. (2006). Adaptive dose selection using
eﬃcacy-toxicity trade-oﬀs: illustrations and practical considerations. J. Bio-
pharmaceutical Statistics, 16, 623–638.
Thall, P.F., Millikan, R.E., M¨uller, P., and Lee, S.-J. (2003). Dose-ﬁnding with
two agents in phase I oncology trials. Biometrics, 59, 487–496.
Thall, P.F. and Simon, R.M. (1994). Practical Bayesian guidelines for phase IIB
clinical trials. Biometrics, 50, 337–349.
Thall, P.F., Simon, R.M., and Estey, E.H. (1995). Bayesian sequential monitor-
ing designs for single-arm clinical trials with multiple outcomes. Statistics in
Medicine, 14, 357–379.
Thall, P. and Wathen, J. (2005). Covariate-adjusted adaptive randomization in
a sarcoma trial with multi-stage treatments. Statistics in Medicine, 24, 1947–
1964.
Thall, P. and Wathen, J. (2007). Practical Bayesian adaptive randomisation in
clinical trials. European Journal of Cancer, 43, 859–866.
Thall, P.F., Wathen, J.K., Bekele, B.N., Champlin, R.E., Baker, L.H., and Ben-
jamin, R.S. (2003). Hierarchical Bayesian approaches to phase II trials in dis-
eases with multiple subtypes. Statistics in Medicine, 22, 763–780.
Thall, P., Wooten, L., and Tannir, N. (2005). Monitoring event times in early
phase clinical trials: some practical issues. Clinical Trials, 2, 467–478.
Therasse, P., Arbruck, S.G., Eisenhauer, E.A., Wanders, J., Kaplan, R.S., Ru-
binstein, L., Verweij, J., Van Glabbeke, M., van Oosterom, A.T., Christian,
M.C., and Gwyther, S.G. (2000). New guidelines to evaluate the response to
treatment in solid tumors. Journal of the National Cancer Institute, 92, 205–
216.
Therasse, P., Eisenhauer E.A., and Verweij, J. (2006). RECIST revisited: A re-
view of validation studies on tumour assessment. European Journal of Cancer,

REFERENCES
295
42, 1031–1039.
Thompson, W. (1933). On the likelihood that one unknown probability exceeds
another in view of the evidence of two samples. Biometrika, 25, 285–294.
Tierney, L. (1994). Markov chains for exploring posterior distributions (with dis-
cussion). Ann. Statist., 22, 1701–1762.
Wang, W., Hwang, J.T.G., and Dasgupta, A. (1999). Statistical tests for multi-
variate bioequivalence. Biometrika, 86, 395-402.
Ware, J.H. (1989). Investigating therapies of potentially great beneﬁt: ECMO
(with discussion). Statistical Science, 4, 298–340.
Wathen, J. and Cook, J. (2006). Power and bias in adaptively randomized clinical
trials. Technical report, Department of Biostatistics, M.D. Anderson Cancer
Center, http://www.mdanderson.org/pdf/biostats utmdabtr 002 06.pdf.
West, M. and Harrison, P.J. (1989). Bayesian Forecasting and Dynamic Models.
New York: Springer-Verlag.
Wilber, D., Pappone, C., Neuzil, P., De Paola, A., Marchlinski, F., Natale, A.,
Macle, L., Daoud, E.G., Calkins, H., Hall, B., Reddy, V., Augello, G., Reynolds,
M.R., Vinekar, C., Liu, C.Y., Berry, S.M., and Berry, D.A. for the ThermoCool
AF Trial Investigators (2010). Comparison of antiarrhythmic drug therapy and
radiofrequency catheter ablation in patients with paroxysmal atrial ﬁbrillation:
a randomized controlled trial. J. Amer. Med. Assoc., 303, 333–340.
Williamson, P.P. (2007). Bayesian equivalence testing for binomial random vari-
ables. J. Statist. Computation and Simulation, 77, 739–755.
Xia, H.A., Ma, H., and Carlin, B.P. (2008). Bayesian hierarchical modeling for
detecting safety signals in clinical trials. Research Report 2008–017, Division
of Biostatistics, University of Minnesota.
Yin, G., Li, Y., and Ji, Y. (2006). Bayesian dose-ﬁnding in Phase I/II clinical
trials using toxicity and eﬃcacy odds ratios. Biometrics, 62, 777–787.
Yin, G. and Yuan, Y. (2009a). A latent contingency table approach to dose ﬁnding
for combinations of two agents. Biometrics, 65, 866–875.
Yin, G. and Yuan, Y. (2009b). Bayesian dose ﬁnding in oncology for drug combi-
nations by copula regression. J. Roy. Statist. Soc., Ser. C (Applied Statistics),
58, 211-224.
Zacks, S., Rogatko, A., and Babb, J. (1998). Optimal Bayesian feasible dose
escalation for cancer phase I trials. Statistics and Probability Letters, 38, 215–
220.
Zhang, W., Sargent, D.J., and Mandrekar, S. (2006). An adaptive dose-ﬁnding de-
sign incorporating both toxicity and eﬃcacy. Statistics in Medicine, 25, 2365–
2383.
Zhou, X., Liu, S., Kim, E.S., Herbst, R.S., and Lee, J.J. (2008). Bayesian adap-
tive design for targeted therapy development in lung cancer – a step toward
personalized medicine. Clinical Trials, 5, 181–193.
Zohar, S. and Chevret, S. (2007). Recent developments in adaptive designs for
phase I/II dose-ﬁnding studies. J. Biopharmaceutical Statistics, 17, 1071–1083.

