
Introduction to English Phonetics and Phonology 

Textbooks in English Language 
and Linguistics (TELL) 
Edited by Magnus Huber and Joybrato Mukherjee 
Volume 1 
~ 
PETER LANG 
Frankfurt am Main . Berlin . Bern . Bruxelles . New York . Oxford . Wien 

Ulrike Gut 
Introduction to English Phonetics 
and Phonology 
4 
PETER LANG 
I nternationa ler Verlag der Wissenschaften 

Bibliographie Information published by the Deutsche 
Nationalbibliothek 
The Deutsche Nationalbibliothek lists this publication in the 
Deutsche Nationalbibliografie; detailed bibliographie data is 
available in the internet at <http://www.d-nb.de>. 
ISSN 1862-510X 
ISBN 978-3-631-56615-2 
© Peter Lang GmbH 
Internationaler Verlag der Wissenschaften 
Frankfurt am Main 2009 
All rights reserved. 
All parts of this publication are protected by copyright. Any 
utilisation outside the strict limits of the copyright law, without 
the permission of the publisher, is forbidden and liable to 
prosecution. This applies in particular to reproductions, 
translations, microfilming, and storage and processing in 
electronic retrieval systems. 
Printed in Germany 1 2 3 4 5 
7 
www.peterlang.de 

Acknowledgments 
I am indebted to many people for their valuable comments on earlier vers ions of 
this book: Lilian Coronel, Patrick Maiwald, Franziska Meltzer, Sandra Mollin, 
Christina Sanchez, Karin Tausend and Eva-Maria Wunder. 
My sincere thanks also go to those who gave me permission to use their 
photographs, films, icons or voice in the book and on the CD-ROM, and to those 
who helped with the compilation of the recordings: Rebecca Davies, Christoph 
Karl, Katarzyna Dziubalska-Kolaczyk, Martin Hennann, Peter James, Leonard 
Librizzi, Franziska Meltzer, Inger Moen, Kevin Munhall, Joe PerkeIl, Evgenia 
Slavianova, Kenneth Stevens, Fiona Tan, Charla Teufel, James Thomas, Holger 
Voonnann, Thomas Wittenberg and the International Phonetic Association 
<http://www.arts.gla.ac. ukllP Alipa.html>. 


Table of Contents 
1 
Introduction: Phonetics and Phonology .................................................... 5 
1.1 Phonetics and phonology 
6 
1.2 Sounds and letters 
9 
1.3 The structure of this book 
1.4 Exercises 
1.5 Further reading 
10 
11 
12 
2 
Speech production ...................................................................................... 13 
2.1 The respiratory system 
13 
2.2 The phonatory system 
16 
2.3 The articulatory system 
23 
2.4 ClassifYing speech sounds 
28 
2.5 Articulation in connected speech 
33 
2.6 The role ofthe brain 
37 
2.7 Leaming the production of speech 
40 
2.7.1 
Speech production in first language acquisition (advanced 
reading) 
41 
2.7.2 
Speech production in second language acquisition and 
teaching (advanced reading) 
42 
2.8 Methods ofresearching speech production (advanced reading) 
44 
2.9 Exercises 
47 
2.1 0 Further reading 
48 
3 
The Phonology of English: Phonemes, Syllables and Words ................ 49 
3.1 The phonemes ofEnglish 
50 
3.1.1 
The consonants ofRP and GA and their transcription 
53 
3.1.2 
The allophonic variation of consonants in RP and GA 
56 
3.1.3 
The vowels ofRP and GA and their transcription 
60 
3.1.4 
Phonemic and phonetic transcription 
67 
3.1.5 
Phonetic features (advanced reading) 
70 
3.2 The English syllable 
75 
3.2.1 
Types of syllables and phonotactic rules ofEnglish 
76 
3.2.2 
Syllabification in English 
82 
3.2.3 
Stress and speech rhythm in English 
83 

2 
3.3 The phonological word in English 
88 
3.3.1 
Word stress in English 
89 
3.3.2 
Phonological processes occurring at the level ofthe pword 
(advanced reading) 
95 
3.4 Theories ofthe acquisition ofEnglish phonology 
(advanced reading) 
96 
3.4.1 
English phonology in first language acquisition 
97 
3.4.2 
English phonology in second language acquisition 
98 
3.5 Exercises 
101 
3.6 Further reading 
102 
4 
The Phonology of English: Intonation ................................................... 105 
4.1 Intonational phrasing in English 
106 
4.2 Nucleus placement in English 
111 
4.3 English tones and their usage 
116 
4.3.1 
The tones of English and their transcription 
117 
4.3.2 
The ftmction ofEnglish tones and tunes 
121 
4.3.3 
Pitch range, key and register 
126 
4.4 The acquisition ofEnglish intonation (advanced reading) 
130 
4.4.1 
The acquisition ofEnglish intonation in first language 
acquisition 
130 
4.4.2 
The acquisition ofEnglish intonation in second language 
acquisition 
131 
4.4.3 
Teaching English intonation 
133 
4.5 Exercises 
134 
4.6 Further Reading 
135 
5 
Acoustic properties of English ................................................................ 137 
5.1 Acoustic properties of sound 
138 
5.1.1 
Acoustic properties ofsound waves 
139 
5.1.2 
Simple and complex waveforms (advanced reading) 
142 
5.2 The acoustic properties ofEnglish vowels 
150 
5.3 The acoustic properties ofEnglish consonants 
156 
5.4 Acoustic aspects of connected speech in English 
(advanced reading) 
163 
5.5 The acoustic properties of English intonation 
166 
5.5.1 
The acoustic properties of intonation phrases in English 
166 
5.5.2 
The acoustic properties of accents in English 
168 

3 
5.5.3 
Measuring pitch and pitch movement in English 
170 
5.6 Acoustic properties ofL2 learner English and the use of acoustic 
phonetics in pronunciation teaching (advanced reading) 
174 
5.7 How to make a good speech recording 
177 
5.8 Exercises 
179 
5.9 Further Reading 
180 
6 
Speech perception .................................................................................... 183 
6.1 The outer ear 
183 
6.2 The middle ear 
184 
6.3 The inner ear 
185 
6.4 The internal auditory system 
187 
6.5 The perception of loudness, pitch and voice quality 
190 
6.6 Measuring hearing sensitivity 
193 
6.7 Theories of speech perception (advanced reading) 
195 
6.8 Speech perception and language acquisition (advanced reading) 
200 
6.8.1 Speech perception in first language acquisition 
200 
6.8.2 Speech perception in second language acquisition and 
teaching 
201 
6.9 Exercises 
206 
6.10 Further reading 
206 
7 
List of References ..................................................................................... 207 
8 
Index .......................................................................................................... 217 


1 Introduction: Phonetics and Phonology 
This textbook is about the sounds English speakers produce and hear when 
speaking or listening to speech. Although many are not aware of it, speech and 
speech sounds playa central role in the life of human beings (except, of course, 
for the deat): on an average day, a person produces several thousand spoken 
words and hears a multiple of that. This means that speakers use several 
thousand speech sounds every day to communicate their feelings, wishes and 
intentions and encounter equally many speech sounds when listening to the 
feelings, wishes and intentions of others. Yet, alm ost none of the speakers are 
aware of what they do when they produce or perceive speech. If you ask a 
speaker who produces several thousand vowels a day which body parts and 
musc1es are active during the articulation of these vowels, he or she will most 
probably not be able to tell you. Similarly, if you ask native speakers of English 
how they employ the height oftheir voice to signal their intentions and attitudes, 
you will most probably only encounter a puzzled look and a shrug of the 
shoulders. Neither would most native speakers ofEnglish be able to explain why 
they understand what was said in a recording in which half of the speech sounds 
are masked by noise. All this shows that the knowledge native speakers have of 
the pronunciation oftheir language is not conscious. This textbook is thus about 
what native speakers of English do not know they know about speaking and 
understanding spoken English. 
For English language teachers and other professionals concemed with 
pronunciation (such as speech therapists or pronunciation trainers), conscious 
knowledge of English speech sounds, of their production, properties and 
perception, is of course essential. They need to know which organs and mental 
processes are involved in speech production; they need to know which 
unconscious knowledge native speakers ofEnglish have (and leamers ofEnglish 
need to acquire) about the sound system of English; they need to know about the 
organs and mental processes involved in the perception of speech. It is obvious 
that language teachers will not be able to support language learners adequately 
until they understand what exactly is entailed in the articulation and perception 
of speech and also have the means to describe and evaluate differences between 
the pronunciation of English language leamers and that of native speakers. It is 
the aim of this book to give you the necessary background knowledge and to 
introduce you to the terminology that is required for a successful pursuance of 
these professions. Last but not least this textbook is intended for English 
language leamers who wish to be able to identify and improve weaknesses in 
their own pronunciation of English. 

6 
Chapter 1 
1.1 Phonetics and phonology 
There are two fields or subdisciplines in linguistics concemed with 
pronunciation and sound, namely phonetics and phonology. Both of them 
describe and analyse speech from a different perspective. Phoneticians strive to 
find ways of describing and analysing the sounds humans use in language in an 
objective way. Three different areas of phonetics can be distinguished: 
mticulatory phonetics, acoustic phonetics and auditory phonetics. Articulatory 
phonetics analyses which organs and muscles are used by speakers to produce 
speech (see chapter 2). Acoustic phonetics is concemed with the physical 
properties of speech sounds as they travel in the air between a speaker's mouth 
and a listener's ear (see chapter 5). Auditory phonetics focuses on the effect 
those sounds have when they re ach the listener's ear and brain (see chapter 6). 
Phonetics is thus a linguistic field that draws heavily on other scientific 
disciplines including anatomy, physiology, neurology and physics. 
The beginnings of articulatory phonetics can be traced back to the very 
detailed descriptions of the pronunciation of Sanskrit, which were made by 
Indian scholars several centuries BC. The first theory on the function ofthe vocal 
folds was proposed by Antoine Ferrein (1693-1769) in the 18th century. 
Christian Gottlieb Kratzenstein (1723-1795) and Wolfgang von Kempelen 
(1734-1804) attempted to explain the production of vowels and consonants by 
building a talking machine. Henry Sweet (1845-1912) and Daniel Jones (1881-
1967) were the pioneers in the description and transcription of the articulation of 
English speech sounds. Nowadays, phoneticians are actively involved in the 
development of synthetic speech and automatic speech recognition systems, 
which we encounter alm ost every day in automatic announcements at railway 
stations and airports as weil as in telephone bookings and interactive computer-
based pronunciation training courses. 
Questions that phoneticians investigate include: Which speech organs are 
involved in the production of a particular speech sound or a particular pitch 
movement, and how do they work together? Which physical properties (i.e. 
frequency, amplitude) do different speech sounds or pitch movements have? 
Which body parts are involved in the perception of speech sounds, of pitch and 
of stress? The methods of investigation in phonetics have profited from many 
technological advances. As will be described in sections 2.8, 6.5 and 6.6, 
phoneticians can make use of the sophisticated methods developed in medicine 
for observing the activity of the speech organs, muscles and the brain during 
speech production and perception. Direct observation and measurement of 
nearly all speech organs involved in speech production and speech perception 
are possible, and computer software enables the exact measurement of the 
acoustic properties of all aspects of speech. 

Introduction 
7 
While phonetics deals with the production, properties and perception of the 
speech sounds of human languages, phonology is concerned with how these 
speech sounds form patterns in a particular language. Phonologists investigate, 
for example, which function asound has in a language and which sounds can be 
combined - follow each other - and which cannot. Phonology can be divided 
into two areas: segmental and suprasegmental phonology. While segmental 
phonology deals with speech sounds, suprasegmental phonology is concerned 
with larger units such as syllabIes, words and intonation phrases (see chapters 3 
and 4). The study of phonology began as early as in the third century BC with the 
Ancient Greek grammarians describing the sound patterns of Greek and Latin in 
Europe and scholars in India describing the phonology of Sanskrit. The first 
descriptions of the sounds of English were published by John Hart (An 
Orthographie; 1569) and William Bullokar (Booke at large; 1580), who were 
concerned with the divergence of spelling and pronunciation in 16th century 
English. Charles Butler's The English Grammar, published in 1634, probably 
contains the first descriptions of stress patterns of English words. 
In the long tradition of phonology many different ideas about language have 
influenced the methods of phonological analysis. In the later 19th century, for 
instance, it was popular to treat language as an 'organism' that evolves and dies, 
modelIed on the then revolutionary ideas of Charles Darwin on living 
organisms. Many German linguists working at that time investigated the 
'evolution' of languages, proposed language family trees and provided the first 
descriptions of sound changes in Indo-European languages. This approach, 
which was further developed by Ferdinand de Saussure (1857-1913) at the 
beginning of the 20th century, assumes that language is a system that can be 
described and analysed by the elements or units it consists of (such as speech 
sounds) and rules that apply to them (such as rules of sound order). Phonologists 
working in this framework strive to find units of sound structure and sets of 
rules that describe patterns and regularities of these units in a particular 
language. For example, when looking at vowels in unstressed syllables in 
English (as will be done in section 3.2.3 below), one finds that they have a 
different quality than vowels in stressed syllables - the technical term for this is 
that they are reduced. It is possible to describe this vowel reduction as a rule that 
applies when a syllable is unstressed in English. 
In another tradition in phonology, focus is put on the mental representation 
or knowledge of sounds and sound patterns by speakers. It is assumed that 
different speakers have the same 'mental idea' of asound although they might 
produce and hear this sound in different forms. When you compare the 
articulation of the Ipl in pot with the Ipl in spot and in top with the methods of 
phonetic analysis, you will find that they differ distinctly in their articulation and 
acoustic properties: for the Ipl in pot, there is a short but clearly audible burst of 
air after the speaker opened his or her lips which does not occur in the Ipl in 

8 
Chapter 1 
spot. For the Ipl in top, speakers might not even open their lips and there might 
be an accompanying stoppage ofthe airstream in the throat (section 3.1.2. gives 
you more information on this). However, when you ask speakers about those 
Ip/s, they are usually not aware that they produce or perceive different sounds. 
This leads phonologists to claim that speakers have just one mental 
representation (i.e. knowledge stored in the memory) of the speech sound Ip/. 
Different notation symbols are used in order to differentiate between speech 
sounds that form part of the speakers' knowledge and speech sounds that are 
actually produced and can be measured and perceived. The slashes I I indicate 
that a speaker's knowledge or mental representation is referred to; this is what 
phonology deals with. The square brackets [ I indicate that an actual sound is 
being talked about, which is what phonetics is concemed with. Figure 1.1 
illustrates the usage of the transcription symbols: a speaker's mental 
representation of "Oh!" is put into slashes, whereas the actual pronunciation of 
the word is put into square brackets. See chapter 3 for details on this distinction 
and further transcription conventions. 
"'--~-- .... 
( lauI') 
<'i:'~'o~ 
Figure 1.1. Transcription conventions of the mental representation and actual 
pronunciation of"Oh!". 
Recent theories in phonology (e.g. Bybee 2001) put emphasis on the view that 
language is a tool that is used by speakers for communication (this is called the 
functional approach). Phonologists working in this tradition claim that language 
cannot be separated from the speakers who use it. They treat the phonology of a 
language as the knowledge - represented in his or her brain - a speaker has of 
phonological units and processes that are necessary in order to produce and 
understand speech. Questions about English that phonologists investigate 

Introduction 
9 
include: Which sounds does English have in order to make meaningful 
distinctions between words? Which pitch movements does English have in order 
to make meaningful distinctions between different types of utterances such as 
questions and statements? Which syllables in English words are stressed and 
which are unstressed? Which sounds can be combined to form an English 
syllable? Do the same phonological mies apply to the different accents of 
English (e.g. British English and Australian English) and the different historical 
forms of English such as Old English and Early Modem English? The methods 
of phonology are indirect since we do not yet have the means to examine a 
speaker's phonological knowledge directly. Indirect methods include observing 
speaker productions and gathering speaker judgments (for example, asking a 
speaker ofEnglish whetherflin or lsin are potentially good English words). 
To put it simply, phonology is concemed with what speakers and listeners 
know and children and second language leamers need to leam in order to 
competently use and understand spoken language. Conversely, phoneticians are 
interested in the precise activities of the speech organs, the physical properties 
of speech and the activity of the body parts involved in speech perception. While 
a phonologist will claim that the vowel in the word leak is long and the vowel in 
the word lick is short, a phonetician will measure the exact length of the sound 
in milliseconds. This is not to say that there is always a clear-cut boundary 
between phonetic and phonological analysis. Increasingly, linguists combine 
questions and methods of both fields, for example in laboratory phonology, 
where phonetic techniques and measurements are used for phonological 
investigations. 
1.2 Sounds and (etters 
One of the central distinctions that are made in the study of language is that 
between letters and sounds. In English, more than in other languages, it is 
obvious that the spelling and pronunciation of words do not match. Take the 
words bone, done and gone (linguists use italics when they write about words), 
which are spelled with the same vowel letter but are actually pronounced with 
three different vowels. Bone rhymes with tone, done rhymes withfun and gone 
has the same vowel as rock. Or take the words rijf, loaf and tough. They all end 
with the same sound but the spelling varies from <ff> to <f> and <gh> (linguists 
use the pointed brackets < > when referring to a spelling - the technical term for 
a spelling symbol is grapheme). When talking about the pronunciation of 
English words it is therefore very confusing and misleading to use the spelling 
symbols ofthe Roman alphabet. Chapter 3 describes the symbols ofthe phonetic 
alphabet, 
which 
enable 
you to 
write 
unambiguously about English 
pronunciation. 

10 
Chapter 1 
Unfortunately, English spelling is also very deficient in not providing any 
information about other features of English pronunciation apart from speech 
sounds, for example features such as stress and intonation. All words in English 
that are longer than one syllable have one stressed syllable. The word English, 
for example, is stressed on the first syllable, the second one is unstressed. 
However, no indication of this is given in writing. Equally, the pitch of a 
speaker's voice moves up and down during speech, which can change the 
meaning of an utterance. Listeners will understand very different things when 
somebody says "Yes" with a falling pitch than when he or she says it with a 
rising pitch. In writing, the only option you have is to use punctuation marks like 
! and ?, which, however, is not enough to represent all significant pitch 
movements produced by speakers. In chapters 3 and 4 you will see which 
symbols are used in linguistics to describe stress and pitch movement in English. 
1.3 The structure of this book 
This textbook is intended as a companion for students of English throughout 
their university studies. Most ofthe sections were written for absolute beginners 
and require no previous knowledge ofphonetics and phonology. They serve as a 
first introduction to the sounds and the sound system of English - i.e. the 
production, perception and physical properties of English speech sounds as weIl 
as what we presume to be their organization and structure in the brains of native 
speakers. The sections marked as 'advanced reading' provide information for 
especially interested students wishing to specialize in one of the areas of 
phonetics and phonology. 
Chapter 2 describes the structure and function of the body parts involved in 
speech production and shows how they work together in the articulation of 
connected speech. Chapters 3 and 4 deal with the phonology of English and 
describe the sounds of English, English syllabies, words and intonation. In 
chapter 5, the acoustic properties of speech sounds are presented, and it is shown 
how acoustic phonetics can be employed in language teaching and learning. 
Chapter 6 provides an introduction to the structure and function of the body 
parts active during the perception of speech and the phonological knowledge 
necessary to understand speech. 
In order to help you with the reading, all technical terms are printed in bold 
when they first appear. These technical tenns form the foundation of phonetic 
and phonological description and analysis. Sometimes two alternative technical 
terms are given - as much as one may wish it, in academia there is unfortunately 
no one-to-one correspondence between scientific concepts and technical terms 
and different researchers use technical terms with different definitions. All ofthe 
basic technical terms are listed in the index at the end of the book so that when 

Introduction 
11 
you are looking for adefinition or deseription of a teehnieal tenn, you will find 
there a referenee to the speeifie page where it is given. Graphs and tables are 
included in the text wherever they help to illustrate a point. Further illustration is 
provided by the aeeompanying CD-ROM. The ieons shown in Figure 1.2 
indieate video and audio examples included on the CD-ROM. 
Figure 1.2 1cons referring to video (Zeft) and audio (right) examples on the CD. 
Exereises at the end of eaeh ehapter allow you to revise the most important 
telIDS and eoneepts of eaeh ehapter. The "Further reading" seetion points out 
books, further praetiee materials and useful websites for a more detailed 
deseription of the various topies introdueed in eaeh ehapter. Full referenees of 
the literature listed there are given at the end of the book. When a partieular 
study or book is referred to in the text, they will also be listed in the Referenees 
seetion (ehapter 7). 
1.4 Exercises 
1. In what way do phonetieians and phonologists view the sounds and 
pronuneiation ofEnglish? What are the major differenees? 
2. What is the differenee between <p>, /p/ and [p]? 
3. List all the different spellings of the sound [u] in English words. 
4. How many sounds and how many graphemes do the following words have: 
knee, spring, debt, wrapped, fire, stall, key, pint, lamb, print 
5. Put the following words into sets beginning with the same sound (not 
grapheme!) 
knight, cat, king, get, gnat, guest, calm, quay, quest, west, vest, want, vase 
6. Ask a native speaker ofEnglish how many vowels English has. Compare this 
to the deseription of the vowel system of English given in seetion 3.1.3. 

12 
Chapter 1 
1.5 Further reading 
Yava~ (2006, chapter 9) lists all the different spellings of the sounds of English. 
A history of the theories and methods of phonology and phonetics is given in 
Clark, Yallop & Fletcher (2007, chapter 11). For especially interested readers, 
Sampson (1980) describes the history of linguistic thought including phonology 
and phonetics. 

2 Speech production 
This chapter describes the anatomy (the structure) and physiology (the function) 
ofthe body parts involved in the production of speech. Humans do not have any 
organs or muscles that are used exclusively for speaking; a11 of the body parts 
participating in speech production have other, primary functions, which are 
more basic and often vital for keeping the organism alive. Producing speech is 
only a secondary, an 'overlaid function' ofthe speech organs, one that developed 
later in evolution. Traditiona11y, three 'systems' of speech organs are 
differentiated that have different functions in the speech production process (see 
Figure 2.1). The respiratory system described in section 2.1 produces the 
airstream that is necessary for speaking; the phonatory system described in 
section 2.2 produces voice, and the articulatory system described in section 2.3 
is responsible for the formation ofthe different speech sounds. The classification 
of sounds according to their manner and place of articulation is described in 
section 2.4. Section 2.5 illustrates how a11 three systems work together in the 
production of connected speech. The central role of the brain in speech 
production is described in section 2.6. Section 2.7 is concerned with speech 
production in first and second language acquisition, and section 2.8 explains 
some methods ofmeasuring the activity ofthe speech organs. 
Articulatory system 
Phonatory system 
Respiratory system 
Figure 2.1. The three systems of speech production. 
2.1 The respiratory system 
The respiratory system provides the airstream on which speech is produced. It 
consists of the rib cage, the intercostal muscles, the diaphragm, the lungs, the 

14 
Chapter2 
bronchial tubes and the trachea. Figure 2.2 shows the rib cage, which is barrel-
shaped and contains the lungs and the intercostal muscles, which run between 
the ribs. The floor of the rib cage is formed by the diaphragm, a dome-shaped 
muscle. The lungs are a pair of organs consisting of soft sponge-like material 
that rest on the diaphragm. They are connected to the windpipe, or trachea, by 
two bronchial tubes, which divide into increasingly smaller bronchioli that end 
in millions oftiny air-filled sacs, the alveoli. 
Ribcage 
Diaphragm 
Figure 2.2. The respiratory system. 
----\--___;_ Lung 
~,-----+Bronchioli 
"4---+ Bronchial 
tube 
The primary function of the respiratory system is to breathe and thus to keep the 
organism alive by providing oxygen for and removing carbon dioxide from the 
blood. The main function of the respiratory system for producing speech is to 
provide an airstream with which sound is produced. Various types of airstream 
mechanisms can be used for speaking: the most common one is the egressive 
pulmonic airstream, with air flowing from the lungs (hence pulmonic) through 
the trachea up into and out of the mouth and nose (hence egressive). Other types 
of egressive airstreams do not originate in the lungs but further up: an egressive 
glottalic airstream, for example, comes from the larynx (see phonatory system, 
section 2.2 below). When you hold your breath and then articulate a [tl, you are 

Speech production 
15 
producing it on a glottalic airstream. The resulting sound is called an ejective. 
Ejectives may occur in English as variants of sounds produced on a pulmonic 
airstream. They are, however, distinctive speech sounds for example in Hausa, a 
language spoken in Nigeria, Niger, Sudan and Cameroon. Speech can also be 
produced on an ingressive airstream, for which air is sucked into the body. 
Some people involuntarily speak on an ingressive airstream when they are out of 
breath or when they are sobbing. Similarly, the clicking sound that English 
speakers occasionally make with their tongue in order to signal disapproval is 
produced with an ingressive airstream. Clicks occur as distinctive speech sounds 
in South African languages such as Zulu. 
Speech in English is usually produced on an egressive pulmonic airstream. 
In egressive pulmonic speech, the lungs form an air reservoir that provides the 
energy for sound production. At first, speakers need to breathe in some air. 
Breathing in, or inhalation, does not involve any active sucking of air into the 
lungs. Rather, by using some ofthe intercostal muscles connected to the rib cage 
and the diaphragm muscle, speakers lift their rib cage upwards and outwards and 
lower their diaphragm. Both actions contribute to expanding the lungs and thus 
increasing the lung volume. This in turn causes a lowering of the air pressure 
within the lungs compared to the extemal atmospheric air pressure, which makes 
air flow into the lungs via the mouth and the nose so that the internal and the 
external air pressures are equalized. For breathing out (exhalation), the lungs 
are compressed by the interplay of some of the intercostal muscles and the 
upward movement of the diaphragm. Both actions reduce the volume of the 
lungs and press the air out. The total lung capacity of humans varies between 3 
and 7 litres, depending on body size, posture and general physical fitness. The 
lungs, however, never empty completely during exhalation. During the 
production of pulmonic egressive speech, some of the intercostal muscles and 
the muscles of the abdomen work together to maintain a relatively consistent 
level of pressure. Thus speakers can control the speed of exhalation and can vary 
how long they speak on one breath. When not speaking, during quiet breathing, 
breathing in and breathing out take roughly the same amount of time. During 
speech production, however, exhalation is typically about eight times longer 
than inhalation, a proportion that can easily be increased by trained singers and 
speakers. 
The fact that speech is produced on an airstream and the necessity to breathe 
in order to refill the lungs determines an important characteristic of speech: it is 
divided into units, which are often referred to as breath groups. Even in 
perfectly fluent speech, after a certain period of time a speaker needs to interrupt 
and take a breath. This typically happens at specific linguistic boundaries such 
as the end of an utterance or between two clauses rather than within syntactic 
phrases or within words (cf. Yang 2004). Listeners are so accustomed to these 
breath group breaks that they only notice them when they occur in unusual 

16 
Chapter 2 
places. When you record English speakers and measure the length of speech 
between two breaths you will find that, on average, breath groups are 2 to 3 
seconds long. However, they can be as short as 0.7 seconds or as long as 8.29 
seconds, depending on factors such as whether a speaker is giving a monologue 
or participating in a conversation and whether the speech is prepared (like in a 
presentation leamed by heart) or produced spontaneously. 
The overall activity of the body parts of the respiratory system involved in 
speech production also varies with the type of speech that is being produced. For 
loud shouting greater overall muscular effort is needed, which results in a higher 
level of air pressure. Similarly, stressed syllables in a word are usually produced 
with a higher level of air pressure than unstressed ones. Whispering, by contrast, 
requires very little activity from the body parts ofthe respiratory system. 
In summary, the respiratory system provides the airstream on which speech 
is produced. It detennines the fact that speech is divided into breath groups and 
contributes to the production of different levels of loudness and stress. 
2.2 The phonatory system 
The larynx, together with the vocal folds, constitutes the phonatory system. Its 
main ftmction is to provide voice with the help of the airstream coming from the 
respiratory system. The airstremn passes from the bronchial tubes into the 
trachea, a tube with a length of about 11 cm and a diameter of 2.5 cm. The 
trachea has a skeletal frame consisting of rings of cartilage, a material similar to 
bone but less hard. At the top of the trachea there is the larynx, a box-like 
structure that consists of aseries of cartilages (see Figure 2.3). 
I ~ 
Thyroid cartilage ______ 
Cricothyroid muscle 
Cricoid cartilage 
Trachea 
Figure 2.3. Front (Ieft) and side (right) view ofthe lalYnx. Adaptedfrom Clark, 
Yallop & Fletcher(2007: J 77) and Ashby & Maidment (2005: 23). 

Speech production 
17 
The cricoid cartilage functions as the top ring of the trachea and forms the 
base of the larynx. Two flat plates meeting at an angle at the front of the throat 
form the thyroid cartilage. This thyroid cartilage, which has a more acute angle 
in males than in females, can be seen and feit as the 'Adam's apple' moving up 
and down when somebody is speaking or swallowing. It acts as a shield for the 
vocal folds, which are situated inside the larynx. 
Figure 2.4 shows a photograph and a schematised top view of the vocal 
folds (the older terminology is vocal cords). The vocal folds basically consist of 
musc1e covered by several layers of fibrous tissue. When looking down on the 
larynx from above (for example with a laryngoscope mirror; see section 2.8) we 
can see the two roughly triangular folds of pink tissue with their white thickened 
edges (the vocal ligaments). In front, they are attached to the thyroid cartilage 
and at the back to the arytenoid cartilages. The two arytenoid cartilages 
themselves sit on the cricoid cartilage and are attached to it with a joint. This 
joint allows the arytenoid cartilages to move forward and backward and from 
side to side, which turns out to be important for speech as it allows a speaker to 
adjust the position and the tension ofthe vocal folds. For normal breathing, the 
back ends ofthe vocal folds are held apart and air passes in and out silently. 
Thyroid 
eartilage 
"" 
Vocal ligament 
Vocal fold 
Glottis 
Arytenoid / ~ 
-------
C··d 
·1 
eartilage 
neol cartl age 
Figure 2.4 Top view 01 the vocallolds: stroboscopic photograph (feft) and 
schematized view (right). 
The primary function ofthe vocal folds is that ofpreventing food and drink from 
falling into the trachea and the lungs and thus saving the organism from choking 
or suffocation. Further, the possibility to c10se the vocal folds enables one to 
hold one's breath during physical exertion, such as lifting weights. The 
secondary function of the vocal folds and the one important for the production 
of speech is that of phonation. Phonation refers to the vocal folds rapidly 

18 
Chapter2 
opening and closing, which is a process both controlled by the laryngeal musc1es 
and powered by the airstream from the lungs. The process of phonation involves 
an alternation of forces: it begins with the arytenoid cartilages pressed together 
so that the gap between the vocal folds is c10sed and the vocal folds are touching 
each other lightly. The airstream from the lungs then pushes the vocal folds 
apart, thus letting some air through. Immediately after that, the vocal folds are 
pulled together again both because of the elastic properties of their stretched 
tissue and because of the suction generated by the flow of air through the glottal 
constriction. This suction, called the Bernoulli force, is similar to the suction 
that occurs when two trains pass each other at high speed. When the vocal folds 
are c10sed again, the air from the lungs forces them apart again and the cyc1e 
begins a second time. An adult female speaker's voice is based on an average of 
250 openings and c10sings of the vocal folds per second. Children's voices have 
a higher rate of vocal fold openings and c1osings; male voices typically are 
based on an average of 150 opening and c10sing movements. 
" 
~----------------------------------------------------- -----------------, 
02_vocalfolds.gif on the CD-ROM shows a slow-
motion video of the opening and closing cyc1es of 
the vocal folds. It was recorded with an endoscope 
(see section 2.8). 
',---------------------------------------------------- --------------------~' 
This rapid opening and c10sing of the vocal folds is called vibration. The 
process of vibration can be likened to the movement your pursed lips make 
when you blow through them and make them flap. The sound created in the 
larynx is just a quiet buzz, but after it has passed through the articulatory system 
(see section 2.3 below) this buzz has changed into asound we perceive as voice. 
The buzz in the larynx is actually not created by the vibration ofthe vocal folds 
itself but by the sequence of puffs of air emitted through the vibrating vocal 
folds. Not all sounds produced in speech are generated with vocal fold vibration. 
Speech sounds can therefore be divided into voiced ones and voiceless ones. 
Only voiced speech sounds are produced with vocal fold vibration. During the 
production of voiceless sounds, which are technically speaking noise rather than 
sounds (see section 5.1.2), the vocal folds are open and do not vibrate. You can 
test whether a speech sound is voiced - as for example an [al - or not - as for 
example a [f] - by putting your hand on your throat above the larynx, where the 
vocal fold vibration in voiced sounds can be feIt. 

Speech production 
, 
~----------------------------------------------------- -------------------
Smokers who develop laryngeal cancer need to 
have their larynx removed, which means that they 
cannot produce voiced sounds anymore. An 
electrolarynx 
held 
to 
the 
throat 
creates 
electromechanical vibration which functions as an 
artificial source for articulation. Listen to speech 
produced 
with 
an 
electrolarynx 
(02_electrolarynx.wav on the CD-ROM, taken with 
permission from http://www.webwhispers.org/). 
'------------------------------------------------------------------------
19 
Different speakers have different voices, a quality of sound that is called timbre. 
A voice may sound 'sharp' or 'mellow' or 'squeaky'. These differences in voice 
quality are partly due to functional differences in the larynx and the vocal folds 
and partly due to conscious modulation of the voice. V oices can further be high 
or low, i.e. have a high or low pitch (the technical term for height ofvoice). The 
limits of a speaker's pitch are determined by the size, length and tenseness ofthe 
vocal folds. Roughly speaking, longer vocal folds vibrate more slowly than 
shorter vocal folds. Equally, tenser and thinner vocal folds cause faster vocal 
fold vibrations than more relaxed and thicker vocal folds. The faster the vocal 
fold vibration, the higher the pitch perceived by listeners (see section 6.5 for 
details). 
" 
,--------------------------------------------------------------------------
02_glissando.mov on the CD-ROM shows a video 
of the vocal folds during a glissando going from a 
very high to a very low pitch. Note the change in 
thickness and tension of the vocal folds. Note also 
that during breathing the vocal folds are held apart. 
',--------------------------------------------------------------------------, 
The length of the vocal folds varies with the speakers' body size and is typically 
around 3 mm at birth and between 12 and 17 mm for adult females and 17 to 25 
mm for adult males. Vocal fold vibration accordingly varies between different 
speakers. For someone speaking on a very low pitch, the vocal folds perform 
about 60 to 80 opening-closing cycles per second. Most female voices are 
produced with a vocal fold vibration of between 150 and 250 cycles per second; 
male voices are typically based on between 100 and 150 cycles per second. A 
small child's voice is produced with up to 800 opening-closing cycles per 
second. The cycles of vocal fold vibration are never perfectly regular, which is 
not perceptible in a healthy adult voice. It may become noticeable though in 

20 
Chapter 2 
older speakers, which is why we can fairly accurately guess the age of a speaker 
by only listening to his or her voice. 
Although the timbre and the highest and lowest pitch a speaker can produce 
are limited by biological factors, speakers are free to modulate their voices in a 
lot of ways. Throughout speech, pitch changes continuously and can largely be 
controlled consciously. Changes in pitch can be employed for linguistic reasons 
as for example when a speaker produces a rise in pitch across the word really 
and thus signals that the utterance is an incredulous question "Really?!" rather 
than the statement or rebuke "Really!", which would be produced with a falling 
pitch. This use of pitch is typical for intonation languages such as English and 
is described in detail in section 4.3. Tone languages, such as Yoruba and Igbo 
spoken in West Africa or the many different dialects ofChinese, employ pitch to 
signal meaning. In these languages, the pitch height or pitch change on a 
syllable determines the difference between words. In Mandarin Chinese, for 
example, [ba] produced with a falling pitch means 'father', whereas it means 
'pul!' when produced with a rising pitch. 
;-------------------------------------------------------------------------, 
Listen to the difference between "Really" with 
rising and with falling pitch (02Jeally.wav on the 
CD-ROM). 
'----------------------------------------------------- --------------------~ -' 
Speakers might also alter the height of their voice for non-linguistic reasons, for 
example when speaking on a high and squeaky voice in order to imitate 
someone else or when using a low voice in order to appear serious and strict. In 
physiological terms, pitch and pitch change are controlled by modifYing the 
length and tenseness of the vocal folds. This happens through a complex 
interplay of various muscles including the thyroarytenoid, vocalic and 
cricothyroid muscles. When the muscles in the vocal folds are tensed, vibration 
is faster and the pitch is higher. When a speaker uses these muscles to increase 
the tenseness even further, parts ofthe vocal folds are 'shut down', which causes 
the vibration to increase even more. Conversely, relaxed vocal folds cause 
slower vibration and a lower pitch. When you consider singing, it becomes clear 
which amazing control humans have over their laryngeal muscles. Not only can 
exact heights of pitch be produced, the pitch range can also be enlarged by 
switching from ehest register to a high falsetto. In chest register, which is used 
for speaking, the vocal folds are relatively short and thick whereas in falsetto, 
which underlies high sung notes, they are longer and thinner. One of the 
challenges of professional singing is to train amiddie voice register that 
smoothes the abrupt change between the two registers. Variation in pitch can 

Speech production 
21 
also happen unconsciously, for example when shouting. The increased air 
pressure from the lungs used for producing loud speech and the tension in the 
laryngeal muscles cause an increased vocal fold vibration - which is why most 
people's voices are higher when they shout. 
If the back ends of the vocal folds are held apart by moving the arytenoid 
cartilages, a small space opens up between the vocal folds. This space is called 
the glottis (see Figure 2.4). It is the place where the so-called glottal sounds are 
produced. [h], for example, the sound at the beginning of the word house, is 
created in the glottis with the vocal folds producing a turbulent airflow. The 
glottal stop [1], which some speakers of English produce at the end of the word 
what, is articulated by closing the vocal folds tightly until the air pressure 
increases in the trachea and then suddenly opening them and releasing the 
airstream. Speakers can actively change the shape of the glottis during speech. 
Through this, the airstream is modulated - which is called mode of phonation -
and the sound of the voice is altered. Five modes of phonation can be 
distinguished: 
-
VOlce 
-
voicelessness 
-
whisper 
-
breathy voice 
-
creak 
For the production ofvoice, the glottis is closed and the vocal folds are touching 
each other lightly. The airstream coming from the lungs causes vocal fold 
vibration as described above (see Figure 2.5 on the left). During the production 
ofvoiceless sounds the glottis is open, i.e. the vocal folds are wide apart and the 
airstream passes through without making them vibrate (see Figure 2.5 on the 
right). 
Figure 2.5 Vocal lolds during the production 01 voiced sounds (left) and 
voiceless sounds (right). 

22 
Chapter 2 
For whisper, the glottis is narrowed to about 25% of its maximal opening, 
the vocal folds are close together but do not vibrate. The arytenoids leave a 
sm all gap at the back of the larynx through which air passes with high velocity 
(see Figure 2.6 on the left). The airflow is strongly turbulent, which produces the 
characteristic hushing sound. 
Figure 2.6 Vocal folds during whisper (feft) and breathy voice (right). The 
arytenoids leave a sm all opening. The vocal folds only vibrate in 
breathy voice. 
Breathy voice is produced by pulling the vocal folds slightly apart but making 
them vibrate with considerable airflow (see Figure 2.6 on the right). Some 
speakers of English produce creaky voice at the end of utterances with falling 
pitch. In creaky voice, the arytenoid cartilages are held tightly together so that 
the vocal folds can only vibrate at the front end (see Figure 2.7). Creaky voice 
has very low subglottal pressure and vocal fold vibration is reduced to about 30 
to 50 cycles per second. 
Figure 2.7 Vocal folds during the production of creaky voice. Vocal folds are 
pressed together. Vibration occurs only in the front. 

Speech production 
Listen to an example of breathy voice, creaky 
voice and whisper (02-IJhonationmodes.wav 
on the CD-ROM). 
'--------------------------------------------------------------------------~ 
23 
The function of the laryngeal system can be summarized as follows: during 
silent breathing and for the production of most voiceless sounds the vocal folds 
are held wide apart and the airstream coming from the lungs passes silently 
through the open glottis. For glottal sounds such as [h] and the glottal stop [?] 
the vocal folds move to restrict or interrupt the airstream. For the production of 
voiced sounds the vocal folds are set into vibration by a complex interplay of 
various muscles, and the airstream is chopped up into little puffs that enter the 
pharynx. The thickness and degree of tension of the vocal folds, which can be 
largely controlled by the speaker via the various laryngeal muscles, determines 
the pitch of each voiced sound. In English utterances, movements in pitch can be 
employed to signal linguistic contrasts. Speakers can further produce different 
modes of phonation that alter the perceptual characteristics of their speech. 
2.3 The articulatory system 
After passing the larynx the airstreatn enters the articulatory system either 
undisturbed or chopped up into little puffs as described above. The articulatory 
system, sometimes also referred to as the vocal tract, consists of three cavities 
as well as the active and passive articulators. The organs that function as active 
articulators are the velum (or soft palate), the tongue, the lips and the mandible 
(the lower jaw). Passive articulators are the uvula, the palate, the alveolar ridge 
and the teeth. Just like the other organs of speech, the articulators have other, 
more vital functions for aperson. The mandible, the teeth and the tongue, for 
example, have the primary function of chewing food and passing it down to the 
stomach, thus keeping the organism alive. The cavities play an essential role in 
providing a passage for the incoming and outgoing breath, and they warm and 
humidify incoming air. 
The cavities that make up the articulatory system are the pharyngeal cavity, 
the oral cavity and the nasal cavity (1, 2 and 3 in Figure 2.8). The pharynx 
(which is another term for the throat) is a tube of muscle of about 12 cm length 
and stretches from the top of the larynx to the beginning of the oral and the nasal 
cavities. At the upper end it divides into two tubes, one leading up to the space 
inside the nose, the nasal cavity, and the other to the space inside the mouth, the 
oral cavity. The nasal cavity is about 10 cm long for most adult speakers; the 

24 
Chapter 2 
shape of the oral cavity varies widely between speakers. All cavities modulate, 
i.e. change, the airstream coming from the larynx by acting as resonating 
chambers for the buzz produced by the activity of the vocal folds. This means 
that some parts of the buzz are amplified and others are dampened, which alters 
the sound quality (see section 5.1.2 for details). This process is the same as the 
resonance a musical instrument provides, for example a flute or the body of a 
violino The particular shape of the instrument enhances some portions of the 
airstream blown into it (flute) or produced by the action ofthe bow setting the 
strings in motion (violin). 
Li ps 
Teeth 
Palate 
Velum 
Uvula 
Figure 2.8. The vocal tract. The three cavities and the articulators 0/ the 
articulatory system. (J = pharyngeal cavity; 2 = oral cavity; 3 = 
nasal cavity) 
An alteration of the shape of the resonating cavities in a speaker's head leads to 
the creation of different sound qualities. For example, when the nasal cavity is 
full of mucus and the tissue is swollen, as is the case when a speaker has the flu, 

Speech production 
25 
the sound of the speaker's voice is altered considerably. Of course, the sizes and 
specific shapes of the cavities differ among speakers, which leads to a different 
volume and different resonating characteristics. Thus, the particular shape ofthe 
three cavities in individual speakers contributes to their specific voice quality. 
Note that the cavities do not provide resonance for an airstream that passed the 
open glottis and did not set the vocal folds into vibration. 
In the oral cavity, the airstream from the larynx can be modified further by 
various actions of the articulators - the velum, the uvula, the tongue, the palate, 
the lips, the teeth and the mandible (see Figure 2.8). The velum (also called the 
soft palate) consists of muscular tissue and can be raised and lowered by a set of 
museles. The airstream coming from the larynx can only enter the nasal cavity 
when the velum is lowered. In quiet breathing the velum is lowered all the time 
so that air can enter through the nose and flow down to the lungs and flow out 
through the nose again. During speech, the velum is usually raised and the 
airstream flows through the oral cavity only. Accordingly, sounds produced with 
a raised velum are called oral sounds. Those speech sounds that are produced 
with a lowered velum and where the airstream flows only through the nasal 
cavity are called nasal sounds. For nasalized sounds such as the vowel in the 
French word dans, the air flows through both the oral and the nasal cavity. 
The uvula is the small fleshy appendage at the lower end of the velum 
consisting of musele and tissue. You can see it in a mirror when you open your 
mouth widely. When the uvula vibrates, much like in gargling, asound is 
produced that is common in German (for example at the beginning of the word 
Rose) and other European languages, but not in Standard English. 
Although the hard palate, the bony structure forming the roof ofthe mouth, 
is not mobile and thus does not play an active role in articulation, its importance 
is shown by studying the articulation of people born with a eleft palate. In about 
one in 700 newborn babies the two plates that form the palate are not completely 
joined. Although in most cases surgery is performed immediately to elose the 
gap, the voice quality often remains different. Since the oral cavity does not 
elose completely and parts of the airstream enter the nasal cavity, the speech 
produced has an overall nasalized quality. The domed roof of the mouth ends in 
a bony ridge just behind the upper front teeth, which is called the alveolar 
ridge. Many speech sounds ofEnglish such as [t] and [s] are articulated at this 
place. 
The teeth take part in the articulation of several sounds in English. Although 
they do not move actively themselves, they play an important role in the 
production of many sounds, which becomes evident when one considers the 
difference in sound in the speech of people with false teeth. For interdental 
sounds, the tip of the tongue is inserted between the teeth in such a fashion that 
the airstream is pressed through, producing friction. This, for example, happens 
at the begim1ing of the words think and that. For sounds such as [t] at the 

26 
Chapter 2 
beginning of the word fun, the friction is produced by forcing the airstream 
through a narrow space created by the lower lip and the upper teeth. 
The lips consist of two fleshy folds richly supplied with muscles, which 
control the opening and closing movements. The lips can for example block the 
airstream coming from the lungs by closing, or they can produce friction by 
being held close together so that the airstream is forced through and a hissing 
sound is produced. The lower lip can be held close to the upper teeth as for the 
production of [f]. Further, the lips can be held rounded and slightly protruded or 
they can be spread, which makes an important contribution to the production of 
certain vowels. Humans with a cleft lip, which often occurs together with a cleft 
palate, have a sm all gap in the upper lip, which, without surgery, prevents the 
fuH closure of the lips for articulation. 
The lower jaw or mandible is an approximately U-shaped bone structure 
joined with the skull base. Specific actions of sets of muscles can move the 
mandible downwards, forward and sideways with a maximal aperture of about 
40 mm. Downward movement of the mandible also influences the position of 
the tongue, which is attached to it with its root. This movement is important for 
the articulation of many vowels. 
The tongue is the most important active articulator and the most complex 
organ of speech. lt consists nearly entirely of muscle, is very versatile and can 
assume a wide variety of shapes. Due to its mobility and flexibility it makes the 
greatest contribution to changes in the shape of the oral cavity. For descriptive 
purposes the tongue is usually divided into several functional sections (though 
these parts cannot move independently of course): the root, the back, the front, 
the blade and the tip (see Figure 2.9). The root of the tongue is opposite the 
back wall of the pharynx. The back of the tongue lies beneath the velum when it 
is at rest and can be raised towards it with the help of specific muscles. Sounds 
that are produced with the back of the tongue moving close to the velum or 
touching it are for example [u] and [g]. The front of the tongue, which is 
actually fairly in the middle of it and which lies below the hard palate when the 
tongue is at rest, can also arch upwards and is very active in the production of 
vowels. The bl ade of the tongue can move upwards and come near to the hard 
palate, which forms the roof of the mouth. Asound articulated in such a manner 
is [j], The tip of the tongue, the very front end, is its most mobile part and also 
highly sensitive. lt can move between the teeth or curl up and touch the back of 
the upper teeth. Sounds that involve these kinds of articulation are for example 
the first sound in thing and the [d] in width. When the sides of the tongue are 
lowered and the airstream passes at both sides of the tongue, asound like [I] is 
produced. The tongue can f1ll1her be depressed to make a groove, which is 
important for the production ofthe sounds [s] and [z] in sink and zinc. 

Speech production 
27 
Blade 
Front 
Tip 
Figure 2.9. The parts of the tongue, side view (left) and top view (right). Right: 
Adaptedfrom Clark, Yallop & Fleteher (2007: 191). 
The articulators of the articulatory system can influence and shape the airstream 
in many different ways. For example, the airstream can be elosed off completely 
so that pressure is built up behind the blocking. This happens for example when 
two articulators such as the tip of the tongue and the alveolar ridge touch. After 
the contact is released, the air bursts out with a popping sound, which is why 
sounds produced in this manner are referred to as plosives. When two 
articulators, such as the lower lip and the upper teeth, are moved elose together 
but do not block the airstream completely and the air is forced through the sm all 
space, turbulence is created, which produces a typical hissing sound. Sounds 
produced in this way are called fricatives. When the two manners of articulation 
are combined and a blocking phase is followed by a friction phase the resulting 
sound is called an affricate. The tip of the tongue can further make contact with 
the alveolar ridge repeatedly and in rapid succession, which produces a trilled 
sound. When the tip of the tongue briefly strikes the alveolar ridge only once, 
the resulting sound is called a tap. 
To summarize, the articulatory system shapes the airstream, both when it 
passes the open glottis and after it has set the vocal folds into vibration. This 
shaping is due to the resonance created by the cavities and the position or 
movements of the articulators. The term articulation refers to the narrowing or 
constriction of the vocal tract by the movement of an active articulator towards a 
passive articulator. The articulators can perform many different actions, such as 
blocking the airstream by making contact or creating friction by forcing the 
airstream through a sm all gap between them. 

28 
Chapter 2 
2.4 Classifying speech sounds 
Speech sounds can be classified into different categories according to the 
activity of the organs of speech involved in their production. As already 
described above, speech sounds can be divided into voiced ones, which involve 
the vibration of the vocal folds, and voiceless ones, for which the airstream 
passes the open glottis. Oral speech sounds are produced with an airstream 
flowing out of the oral cavity only; nasals involve an airstream flowing out of 
the nasal cavity only. Speech sounds that are produced on both an oral and nasal 
airflow are called nasalized. In French, nasalized vowels have a distinctive 
function, which means that a sequence of sounds with a nasalized vowel (e.g. 
sans) means something else than the same sequence of sounds with an oral 
vowel (e.g. sa). In English, nasalized vowels do not have a distinctive function. 
They are often produced when a vowel is preceded or followed by a nasal 
consonant due to coarticulatory effects (see section 2.5 below). 
Speech sounds are usually divided into vowels and consonants. Generally 
speaking, consonants shape the airstream coming from the larynx in a more 
perceptible manner than vowels. Yet, there are no clear-cut boundaries in 
articulatory terms between a11 members of the two groups. The production of a 
special type of consonant called approximants, for example, involves about the 
same type of tongue activity than the production of vowels, which is why some 
classifications of speech sounds propose a third group of sounds called sem i-
vowels. The distinction between vowels and consonants is mainly made on 
phonological grounds, i.e. the function particular speech sounds have in a 
syllable (this will be explained in section 3.2.1 below). 
All speech sounds can be specified by describing 
-
the airstream mechanism 
-
the vocal fold action 
-
the position of the velum 
-
the place of articulation 
-
the manner of articulation 
However, usually not a11 of these features are employed for the classification of 
vowels and consonants alike. In the description of English vowels, airstream 
mechanism, vocal fold action and the position of the velum are irrelevant since 
a11 of them are produced with a pulmonic egressive airstream, all vowels are 
typica11y voiced - although in fast speech the neighbouring sounds may cause 
them to be produced without vibration ofthe vocal folds (see section 2.5 below) 
- and are produced with a closed velum (again, coarticulatory effects described 
in section 2.5 can overrule this). Furthermore, the manner of articulation of 
vowels is fairly restricted. The airstream passes the oral cavity relatively 

Speech production 
29 
unhindered; the particular vowel sounds are created by altering the resonating 
characteristics of the vocal tract through tongue position and the position of the 
lips. Vowels are usually elassified according to three characteristics: vowel 
height, vowellocation and Hp position (see Table 2.1). Vowel height refers to 
the highest point of the tongue in relation to the roof of the mouth. A high vowel 
such as [i] in bee is produced with the tongue elose to the roof ofthe mouth; for 
a low vowel such as [a] there is a considerable gap between tongue and roof of 
the mouth. Mid vowels such as [e] in bed are articulated with the tongue in mid 
position between high and low. Vowel location refers to the section of the 
tongue that is raised during the production of the vowel. In front vowels such as 
[i], the front ofthe tongue is raised towards the hard palate. In back vowels such 
as [u], the back ofthe tongue is raised towards the velum. Central vowels such 
as the vowel [3] in her are produced with a raised centre part of the tongue. The 
lips can be either rounded as for the production of [u] or unrounded 
(sometimes also spread) for vowels such as [i]. The vowel [i] in beat is thus 
elassified as a high, front, unrounded vowel. All vowels that occur in English are 
described in detail in section 3.1.3. 
Table 2.1. Vowel height, vowellocation and lip position ofvowels. 
vowel height 
high 
[i] 
mid 
[e] 
low 
[al 
vowellocation 
front 
[i] 
central 
[3] 
back 
[u] 
lip rounding 
rounded 
[u] 
unrounded 
[i] 
For English consonants it is usual to use a categorisation based on only three 
features of their production: voicing, their manner and their place of 
articulation. The airstream mechanism and the position of the velum are only 
described when they are not pulmonic egressive and elosed, respectively. Thus, 
a [tl is described as a voiceless alveolar (= place of articulation) plosive (= 
manner of articulation). Some textbooks elaim that English consonants can 
further be classified into fortis (consonants produced with greater articulatory 
effort) and lenis (consonants produced with less articulatory effort). These terms 
were coined before we had the methods to measure articulatory effort (see 
section 2.8 below), and it now appears that these categories are misleading: there 
is no measurable difference in articulatory effort between English consonants. 
The proposed categories probably stern from a confusion with the categories 
voiced and voiceless as well as aspirated and unaspirated (see section 3.1.2). All 
consonants of English can be described and elassified exhaustively by their 
manner and place ofarticulation alone without reference to 'articulatory effort'. 

30 
Chapter2 
Table 2.2 lists all the different manners of articulation that underlie 
consonants. Those consonants that involve a conspicuous obstruction of the 
airstream are referred to as obstruents. Obstruents can be further divided into 
fricatives and plosives. For the production of a plosive two articulators block 
the airstream completely and then suddenly release the pent-up air with an 
audible popping sound. A [p] is an example of a plosive. For fricatives, two 
articulators create an obstruction that leaves only a small space through which 
the airstream is forced. This action creates turbulence, which is perceived as a 
typical hissing sound, as for example in [t]. Consonants that consist of a plosive 
followed by a fricative are called affricates. The German word Pflaume, for 
example, begins with the affricate [pt]. Obstruents can be either voiced (e.g. [gD 
or voiceless (e.g. [s D, and English has both types. 
Consonants that are produced with the action of one articulator hitting 
another repeatedly and thus creating a mini-series of airstream blockages are 
called triUs. The 'rolled r' used in Italian or some South-Eastem German dialects 
is a trii!. This sound also occurred in Middle English and Early Modem English, 
as documented by the playwright Ben lonson, who wrote in the early 1 i h 
century that the 'r' is produced with the "tongue striking the inner palate with a 
trembling about the teeth" (1640, p. 491). If the tongue tip briefly strikes the 
alveolar ridge only once the resuiting speech sound is calied a tap (sometimes 
this term is used synonymously with the term flap, although this refers to a 
similar but different manner of articulation). A tap, for example, is often 
produced by American English speakers in the middle of the word letter. 
Laterals are consonants that are produced with the tongue tip touching the 
roof of the mouth and an airstream that passes the tongue at both sides like in the 
speech sound [I]. Laterals are included in the class ofthe approximants. For the 
production of approximants two articulators move towards each other, but not as 
closely as to create friction. For example, [j], the first sound in the word 
universe, is an approximant. These sounds are very similar to vowels in terms of 
their production, but are grouped with the consonants because of their behaviour 
in the sound system of a language (see section 3.2.1). 
For the production of nasal consonants the velum is lowered so that the 
airstream passes through the nasal cavity. Trills, taps, approximants and nasals 
are usually voiced unless coarticulatory effects (see section 2.5 below) are too 
strong. Vowels, approximants and nasals are often grouped together as 
sonorants. This is due to the fact that they are usually voiced sounds with little 
obstruction of the airstream, which gives them a full sonorant sound. All of the 
manners of articulation listed in Table 2.2 occur in English consonants (see also 
section 3.l.l). 

Speech production 
3\ 
Table 2.2. Manners 0/ articulation 0/ consonants. 
Name 
Example 
Action of articulators 
plosive 
[p] 
complete blockage of airstream with 
subsequent sudden release 
fricative 
[f1 
two articulators create obstruction that 
leaves only a small space through which 
airstream is forced; turbulence is created 
affricate 
[pfl 
combination of plosive plus fricative 
trill 
rolled [r] repeated hitting of one articulator with 
another 
tap 
[r] 
one articulator hitting another once 
lateral approximant 
[I] 
airstream passes at both sides of tongue 
approximant 
[j] 
two articulators move towards each 
other; no friction created 
nasal 
[n] 
velum lowered; airstream passes through 
the nasal cavity 
Consonants can further be characterised by their place of articulation. Table 2.3 
lists all places of articulation together with the corresponding activities of the 
articulators. Bilabial speech sounds such as [b] are produced with the 
involvement ofboth lips. When both the lower lip and the upper teeth participate 
in articulation - as for example for a [f] - the resulting speech sound is called 
labiodental. Dental or interdental speech sounds such as the first sound of the 
are produced with the involvement of the tongue and the teeth. For the 
production of alveolar speech sounds such as a [s], both the tip of the tongue 
and the alveolar ridge are involved. Postalveolar sounds such as the first sound 
in shoe are produced with the blade of the tongue raised just behind the alveolar 
ridge. For retroflex sounds, which many American English speakers produce as 
the last sound in the word writer and which can often be heard in Indian English, 
the tip of the tongue curls up and back and comes close to the palate. When the 
blade of the tongue touches or comes close to the palate as for the production of 
[j], the speech sound is called palatal. Velar speech sounds such as a [g] 
involve the back of the tongue and the velum. The first sound in the German 
word Rose when pronounced in the Northem standard way is a uvular sound 
because it involves the uvula. Pharyngeal sounds that are produced in the 
pharynx do not occur in English but for example in Arabic. English has two 

32 
Chapter 2 
glottal sounds, the fricative eh] and the glottal plosive or 'glottal stop' described 
in section 2.2 above. The first speech sound of the word was is classified as 
labiovelar because it involves both the rounding of the lips and the raising of 
the back of the tongue towards the velum. It is the only English consonant with 
two places of articulation. All the manners and places of articulation that playa 
role in the production of English speech sounds are described in detail in chapter 
3. 
Table 2.3. Places 0/ articulation 0/ consonants. 
Name 
Example Articulators involved 
bilabial 
[b] 
both lips 
labiodental 
[f] 
lower lip and upper teeth 
(inter-)dental 
[0] 
tongue and teeth 
alveolar 
es] 
tip oftongue and alveolar ridge 
postalveolar 
[J1 
blade oftongue and pi ace just behind alveolar ridge 
retroflex 
[t:J 
tip of tongue, curled up and back, and palate 
palatal 
[j] 
blade oftongue and palate 
velar 
[g] 
back oftongue and velum 
uvular 
[R] 
uvula 
pharyngeal 
eh] 
pharynx 
glottal 
eh] 
vocal folds 
labiovelar 
[w] 
lips, back of tongue and velum 
Tables 2.2 and 2.3 list examples oftypical consonants that are produced with the 
different manners and places of articulation. However, it needs to be pointed out 
that speakers seem to differ in their articulations of individual speech sounds. 
Measurements of the movement of the tongue or the place of contact at the roof 
of the mouth for particular speech sounds have revealed that speakers use 
different articulations. For example, some speakers' tongue tip does not meet the 
alveolar ridge but rather the back ofthe upper teeth for the production of a [t]. It 
is assumed that when leaming to pronounce a language speakers aim to achieve 
an acoustic goal rather than an articulatory one. This means that speakers control 
auditorily whether what they produce sounds correct and then leam to associate 
the corresponding articulatory movements with the specific speech sound. 

Speech production 
33 
Theories of the acquisition of speech production are described in section 2.7 
below. Chapter 6 is concemed with the details of speech perception. Methods of 
measuring articulation are described in section 2.8 below. 
2.5 Articulation in connected speech 
The above sections showed that the act of speaking involves the complex 
interplay of body parts such as the muscles connected to the rib cage, the 
laryngeal muscles and the muscles moving the velum, tongue and lips. Even for 
the production of a single speech sound the coordinated activities of numerous 
muscles working in close synchrony are necessary. Yet, speakers do more than 
produce single sounds. W ords consist of sometimes very long sequences of 
sounds, and utterances consist of sometimes very long sequences of words. In 
normal conversation, a speaker pro duces five words per second on average, 
which amounts to about 15 speech sounds per second. In very fast speech, this 
rate of articulation can be increased to about 25 speech sounds per second. The 
articulation of speech is thus an immensely complicated muscular feat. 
Let us consider the series of complex and coordinated actions that has to be 
carried out when pronouncing the short word pin. The speaker first needs to 
breathe in, which requires the concerted action of the diaphragm and the 
muscles associated to the rib cage and the lungs. Then the airstream needs to be 
pushed out, which again involves the muscles connected to the rib cage and the 
diaphragm. The airstream flowing out now needs to be kept constant by using 
muscles to hold back the collapsing lungs for as long as the utterance lasts. The 
vocal folds are held apart by the action ofthe arytenoids. Simultaneously, for the 
articulation of [p], the speaker needs to close the lips firmly so that no air can 
escape through the mouth. When the lips are opened, the tongue has already 
moved into the position needed for the production of [I] and the vocal folds have 
been brought together so that the voicing of the vowel can begin. During the few 
milliseconds it takes to produce the vowel, the velum, which was raised for [p] 
and [I], moves downwards. At the same time, the tip of the tongue moves 
upwards in the direction of the alveolar ridge. When the tongue tip has made 
contact there and the velum is lowered, the sound [n] is created. 
This simple example illustrates that the tongue is constantly moving during 
the production of speech and that the articulatory movements required for 
individual speech sounds are executed parallel to each other. This in turn means 
that the articulation of each sound is influenced by the articulation of the 
preceding and following sounds. It is one of the central insights of phonetic 
research that speaking does not involve producing one sound after another; 
rather, bundles of sounds are planned together (see section 2.6 below), which 
means that the articulation of one sound carries characteristics of its 

34 
Chapter 2 
neighbouring sounds. If you take a recording of someone saying the word pin, it 
is very difficult to cut it up so that each sound can be isolated. Not only is it 
impossible to find exact boundaries between the individual sounds, but the three 
parts of the word pin also influence each other in a perceptually distinct way. 
Parts of the [p] and the [n] always carry the sound of the vowel within them; 
listening to the cut-out sounds you are able to tell that the [p] is followed by an 
[I] and that the [n] is preceded by an [I]. 
" ;-------------------------------------------------------------------------. 
02_connectedspeech.mpg on the CD-ROM shows an 
X-ray recording of a speaker producing several 
utterances. Watch the continuous movement of the 
articulators, especially the tongue and the velum: 
there is no perceptible beginning or end to each of 
the sounds. 
'--------------------------------------------------------------------------; " 
This process of reciprocal influence in articulation is called coarticulation. 
Coarticulation works in both directions: the articulation of a preceding sound 
can influence the following one, and the following sound can influence the 
preceding one (see Table 2.4). When the articulation of a speech sound 
influences the preceding speech sound, the process is referred to as anticipatory 
coarticulation. This happens for example during the articulation of the word 
two. Typically, a speaker's lips are rounded during the production of the [tl, 
although this is not essential for the articulation of this speech sound. Rather, 
this action anticipates the lip rounding that is necessary for the production of [u]. 
When pronouncing the word teeth, by contrast, the lips are not rounded during 
the articulation of [t]. Perseverative coarticulation occurs when the articulation 
of a following speech sound is influenced by the preceding one. This happens 
often when a lateral approximant follows voiceless consonants, as for example 
in the word split. Typically, vocal fold vibration begins only on the vowel, so 
that the [I] is voiceless. 
Table 2.4. Types 0/ coarticulation. 
Name 
Example 
Description 
anticipatory 
two 
lips rounded for [tl when followed by rounded vowel 
perseverative 
spUt 
no voicing of lateral approximant [I] after voiceless 
consonants 
anticipatory 
have to 
no voicing of [v] preceding a voiceless consonant 
across words 

Speech production 
35 
Coarticulation of course does not only occur within words but also across 
words in a breath group. Words are not articulated as separate entities but merge 
into each other without boundary. This can easily be illustrated with speech 
analysis software that displays the continuous stream of speech in a breath group 
(see section 5.4 below). Coarticulation across words occurs for example when in 
the phrase have to the vibration of the vocal folds ends already after the vowel, 
and the voiced labiodental mcative [v] of have is produced as a voiceless [f]. 
The voicelessness ofthe [tl is anticipated and the two words are thus realized as 
[h~ftu]. Two neighbouring sounds becoming more similar is a process that is 
often referred to as assimilation. This term has different definitions and is used 
to describe a variety of phenomena, but we will use it synonymously with the 
term coarticulation here. Traditionally, three types of assimilation are 
differentiated. In assimilation of place, one of two adjacent sounds changes its 
place of articulation in order to make it more similar to the other sound. This is 
the case with the pronunciation ofthe alveolar [n] in unkind as a velar [1)] due to 
the following velar [k]. Assimilation of voicing is illustrated in the example 
have to, which is pronounced [h~ftu] (see Table 2.4). Assimilation of manner 
refers to two neighbouring sounds becoming similar in their manner of 
articulation. This, for example, happens in coalescence. When, in connected 
speech, two adjacent sounds are merged to form a new sound, the process is 
referred to as coalescence. Coalescence can be observed in the way many 
English speakers pronounce "I get you". Usually, the last sound of get and the 
first sound ofyou merge into the new sound [tJ1 ([t] and the first sound of shoe) 
so that the sequence is not pronounced [getju] but [getSu]. 
An important factor influencing the articulation of speech is speaking rate, 
the speed of articulation. In slow, careful articulation the articulatory organs 
perform the necessary movements for each speech sound in an ideal way. For 
instance, in the utterance "This could be true" speakers round their lips for the 
vowel [u] of could and move the back ofthe tongue close to the velum. Then the 
tip of the tongue touches the alveolar ridge for the production of [d] before the 
lips close for the production of [b]. In rapid speech, however, many of the 
articulatory movements are interrupted before they reach their ideal endpoint, a 
process which is often described as undershoot. In fast speech, the lips are only 
partially rounded for the pronunciation of the vowel in could, making it sound 
more like an [<l], the sound at the beginning ofthe word ahead, than an [u]. The 
back of the tongue, similarly, only moves a little way up towards the velum. 
Most probably, the tip ofthe tongue never touches the alveolar ridge before the 
lips close for the [b]. The rapid articulation of "could be" would thus come out 
as [k<lbi]. Sounds not realized at all in fast speech are described as being 
deleted, whereas sounds whose articulation is only partially realised are called 
reduced. In our example the [d] is deleted whereas the [u] is reduced. 

36 
Chapter 2 
Undershot articulation of sounds in English is especially frequent when two 
consonants follow each other and form a so-called consonant cluster. This 
occurs for example at the end of the words just, perfect and bland. Sometimes in 
normal speech, but particularly in fast speech, the second one of these 
consonants is not articulated so that the words are pronounced [d3AS], [p3fek] 
and [blren]. Many studies have shown that there is a systematic variation of such 
final consonant cluster deletion with linguistic factors: it is more frequent in 
unstressed syllables than in stressed syllables (thus more likely in governme!1J. 
than in arounfl), more frequent in uninflected word forms than in inflected word 
forms (thus more likely in lag than in laughefl) and more frequent in common 
words such as just than in rare words such as bust (Neu 1980, Labov 1989, 
Bybee 2002). 
English vowels also often undergo the processes of vowel reduction or 
vowel deletion. Whether English vowels have full or undershot articulatory 
movements varies systematically with stress (see section 3.2.3). For example, 
English vowels are always reduced in unstressed syllabies. This means they are 
produced without full articulation: the tongue is neither fully front nor back but 
highest in amiddie position of the mouth, it is neither high nor low and the lips 
are not rounded. This happens for example in the first syllable of the word 
around (which is pronounced [::uaund]) and the second syllable of the word 
comment (pronounced [knill<lnt] in British English). When these reduced vowels 
occur between voiceless consonants as in tQ come they are often voiceless, too. 
In many unstressed syllabies, vowels are not articulated in a perceptible way 
altogether, as for example in the first syllable of the word police, which is often 
realized as [p], so that the word is pronounced [p.lis] (the . stands for a syllable 
boundary). Vowel deletion also occurs regularly as for example in the second 
syllable ofthe wordfashion, which is usually pronounced [freSn]. 
Section 2.2 showed that some speech sounds in English are voiced and 
others are not. However, listeners do not perceive gaps in voicing when hearing 
speech but rather hear a continuous movement of pitch across a breath group. 
This continuous illovement of pitch is referred to as intonation; the perception 
of intonation is described in more detail in section 6.5. Similarly, some speech 
sounds are produced with greater airstream pressure than others. This varying 
loudness can be interpreted by listeners as differences in stress (see section 
3.2.3). In addition, careful, non-reduced articulations ofspeech sounds that carry 
the articulatory movement to the ideal target point are interpreted as indications 
for the presence of stress by listeners (see section 5.5.2 below). 
Articulation in connected speech can be summarized as follows: producing a 
stream of speech is enormously complicated when considering the parallel 
movements of all muscles concemed. Typically, for the production of a single 
sound all muscles ofthe respiratory, phonatory and articulatory systems interact 
in a specific manner. When sounds are produced in a sequence, the articulatory 

Speech production 
37 
movements of neighbouring sounds influence each other. Articulatory 
movements necessary for the production of a preceding or a following sound 
overlap and cause coarticulatory effects, which can be described as assimilation 
or coalescence. 
2.6 The role of the brain 
Sections 2.1 to 2.5 described the function of various muscles and organs in the 
production of speech. Yet, the most important organ of speech is the brain. 
Without the brain speech production would be impossible. It controls both the 
movements of all muscles involved in the articulation of speech sounds and all 
the other processes necessary for the production of speech, such as the planning 
of the speech message, the grammatical encoding and the combination of sounds 
in a way appropriate to convey meaning and to fulfil a communicative purpose. 
Figure 2.10. Top view 01 the two hemispheres 01 the brain (top 01 the picture = 
Iront olthe head). 
The brain can be divided into the left and right cerebral hemispheres (see 
Figure 2.10), each of which have an outer layer of grey matter called the 
cerebral cortex and an inner layer of white matter. The two hemispheres are 
linked among other things by the corpus callosum, which consists of a large 
bundle of nerve fibres. Each hemisphere contains billions of nerve cells, or 
neurons, which contribute to its complex functioning. Although, generally 
speaking, we still know very little about the function of the brain, its different 
palts have been known to fulfil certain specialized functions since the late 19th 
century. There are two ways of analysing the function of the brain: early 
research was limited to observing which dysfunctions occurred when parts of 
the brain were damaged by a stroke or an accident; more recently, techniques 

38 
Chapter 2 
such as EEG and fMRI (see seetion 2.8 below) are used to measure brain 
activity during different speech tasks. Studies with split-brain patients, whose 
hemispheres are disconnected, have shown that the two brain hemispheres fulfil 
different functions and process information in different ways. This fact is 
referred to as lateralization. For right-handed people and some left-handed 
people it is the left hemisphere that is generally responsible for speech 
production and perception, whereas the right hemisphere is primarily involved 
in the perception of the prosodie aspects of speech (pitch movement and stress, 
cf. section 6.5). For some left-handed people the functions of the hemispheres 
are distributed in the opposite way. 
Figure 2.11. The parts 01 the brain involved in speech production. 
Figure 2.11 illustrates that an area around the central sulcus is specialized for 
the control of the muscles in the larynx, tongue, jaw and lips. This area is 
referred to as the motor cortex and is responsible for the planning, control and 
execution of voluntary movements. Another specialized area of the brain 
connected with speech production lies in the anterior (front) parts of the left 
hemisphere for right-handed speakers (an area that was named Broca's area 
after the researcher who identified it). Patients with brain damage in this area 
suff er from a speech disorder that is referred to as Broca's aphasia. They 
articulate slowly and laboriously and have little control over pitch and loudness. 
Since the articulation of a single sound can involve the coordinated interplay 
of hundreds of muscles, and since the speed of articulation is so high with a rate 
of up to 25 speech sounds per second, it is unlikely that each muscular 
movement of articulation is controlled individually in the motor cortex. Rather, 

Speech production 
39 
it is assumed that the parallel movements of the different muscles necessary for 
the production of each speech sound are represented and executed as a 
coordinated bundle of motor activities. Moreover, as with other motor activities 
such as walking or riding a bicycle, the execution of articulatory movements is 
automatized, i.e. the movements are carried out routinely, very fast and without 
conscious contro!. Many of the complex patterns of muscular activities that 
underlie human behaviour involve an automatized motor pattern. That is, after 
learning the coordinated interplay of the muscles for a specific activity or 
procedure, the information is stored in the brain as a bundle and the activity is 
carried out below the level of consciousness. In fact, it becomes very difficult or 
even downright impossible to carry out the action when thinking consciously 
about it - try controlling consciously all the muscles involved in making a single 
step forward! 
The various muscular activities necessary for the production of speech 
sounds are believed to be stored as highly automatized motor bundles or 
articulatory gestures in the motor cortex. It appears that humans have evolved 
specialized neural mechanisms for the automatization of the articulation of 
speech sounds. For example, it has been found that the respiratory muscles are 
controlled in a way that their actions are coordinated with the length of an 
utterance that is being produced. The amount of air a speaker breathes in already 
reflects the length of the utterance that will be made. Speakers inhale less air for 
shorter utterances than for longer ones. While speaking, moreover, the muscles 
of the rib cage work in different ways to uphold the pulmonary air pressure for 
shorter utterances than they do for longer utterances. 
It is assumed that not only the articulatory movements for individual speech 
sounds are stored in the brain. Given the speed of articulation and the many 
kinds of reciprocal influence articulatory movements have on neighbouring ones 
(see coarticulation in section 2.5 above), it is likely that entire sequences of 
speech sounds are represented in the brain and can be executed automatically. 
This has been demonstrated in an experiment involving a bite-block, a sm all 
object that speakers clench between their teeth during speech production. Two 
groups of speakers, one with and one without a bite-block between their teeth, 
were asked to articulate [pi], and the activity in the relevant muscles was 
measured. It was found that the same muscles were activated for both groups 
although it is impossible to close one's lips for a [p] with a bite-block between 
one's teeth. The automatized commands for lip and jaw movements were 
activated although speakers knew that they could not move them. This 
experiment was interpreted to demonstrate that speakers have amental 
representation of articulatory movements which is executed automatically when 
planning to produce a certain sequence of sounds. 
Speech production involves more than the articulation of sound sequences. 
Speakers produce speech for a purpose, be it the sharing of some information or 

40 
Chapter 2 
the expression of a feeling. This involves further cognitive processes, which are 
located in various specialized areas in the brain. Psycholinguistic models of 
speech production (e.g. Levelt 1989) describe the different cognitive processes 
underlying speaking. Usually, three stages ofthe speech production process are 
proposed. In the first, speakers plan what to say by drawing on linguistic 
knowledge, world knowledge and situational knowledge. At this stage, it is 
decided what is going to be talked about and how it is going to be said, but as 
yet on apreverbal level. The elements of the message are stilI only 'ideas' and 
are not put into words yet. This preverbal message is tumed into a linguistic 
form in the second stage: first, the appropriate words are chosen and combined 
according to the syntactic and semantic requirements ofthe larlguage. Secondly, 
they are encoded in their appropriate phonological form. Wehave now reached 
the stage of 'inner speech' or 'thinking'. In the third stage, articulation is carried 
out and actual sounds are produced. All models of speech production agree that 
auditory feedback is indispensable for speech production. Auditory feedback 
refers to the immediate control speakers have over what they are saying by 
being able to listen to their own speech. This monitoring of speech is used to 
control the accuracy of speech production. For example, when an error is 
detected, say a speaker notices that she has pronounced [tm] instead of [pm], she 
will interrupt herself and 'repair' what has been said. Speech perception is 
described in detail in chapter 6. 
2.7 Learning the production of speech 
The fact that all organs of speech have other, more primary functions shows that 
speaking is an ability that evolved late in the development ofthe human species. 
It appears that the process of evolution of the specialized anatomy and neural 
mechanisms necessary for speech production covered aperiod of at least 
250,000 years and was probably completed 1.4 million years aga (Lieberman 
1984). The specialized anatomy that developed indudes a lowered larynx, which 
creates a Ionger pharyngeal cavity, leaving room for movements ofthe back part 
ofthe tongue, as weIl as a smaIIer tongue in relation to the oral cavity. Newbom 
humans have, for the first three months, a vocal tract that is shaped much more 
Iike that of our ancestors and that of other Iiving primates: the larynx is placed 
higher, the tongue is larger in relation to the oral cavity and leaves little room for 
vertical movements and especially movements of the back of the tongue, and the 
epiglottis is dose to the velum. This means that newboms are ilI-equipped for 
the complicated movements underlying the articulation of speech sounds, but it 
has the advantage of aIlowing them to drink and breathe at the same time. 

Speech production 
41 
2.7.1 Speech production in first language acquisition (advanced reading) 
The process of learning how to speak involves the acquisition of patterns of 
automatized muscular activity and the formation of mental representations of 
sounds and sound sequences. The acquisition of mental representations is 
described in section 3.4 below; this section focuses on the learning ofthe motor 
activities underlying speech production. Newborns have little control over their 
speech organs and the only sounds they can produce are cries and involuntary 
phonation. For cries air is pressed out of the lungs, causing vibration of the 
tightly held vocal folds. These sounds are usually classified as 'reflexive sounds', 
which arise as automatic responses to inner states such as hunger and 
discomfort. Phonation is thought to be involuntary and, due to the immature 
control over bodily functions and activities, rather occurs as a by-product of 
other actions. 
At around two months of age babies have gained some control over their 
speech organs. Infants begin to coo, Le. they produce voiced sounds with a 
resting tongue, sounds that can further be separated by glottal stops. With 
increasing control over laryngeal and articulatory muscles at between four and 
seven months of age vocal play begins. Infants try out voicing, changes in pitch 
and loudness as well as various manners and places of articulation. During the 
stage of babbling, which typically lasts from around six months of age to ab out 
one year, first sound sequences are produced that consist of combinations of 
consonant plus vowel such as [mal and [ga]. These combinations are often 
produced in long sequences and, according to some researchers, already show 
properties of the ambient language/s (i.e. the languages that are to become the 
infant's native language/s) (e.g. Boysson-Bardies et al. 1984, Boysson-Bardies et 
al. 1992). 
Around their first birthday most infants produce their first word, Le. asound 
sequence that is systematically associated with a particular meaning. First word 
productions show many phonetic differences from the corresponding adult 
sound sequences: for example, a child may produce [di] for see or [na] for 
banana. These early word productions reflect that infants usually produce only 
one word per breath group (one-word stage) and that some manners of 
articulation (for example approximants) and some places of articulation (for 
example alveolar) are more difficult to acquire than others. At around 1.5 years 
of age babies begin to put words into two-word sequences. This usually begins 
with each of the two words - for example Mummy sack - being produced in a 
separate breath group with an intervening pause. When two words are integrated 
into one breath group with an uninterrupted pitch movement, the so-called two-
word stage is reached (see e.g. D'Odorico & Carubbi 2003, Behrens & Gut 
2005). In the multiple word stage longer productions follow in which more 
than two words are combined into one breath group. It appears that control over 

42 
Chapter2 
the laryngeal muscles is acquired fairly early and that children can use rising and 
falling pitch for broadly defined linguistic purposes - e.g. attracting attention -
and communicative meaning from the one-word stage on (e.g. Furrow 1984). 
However, some of the complex articulatory manoeuvres that underlie the 
production of particular speech sounds and sequences of speech sounds, such as 
the lip movements connected with the production of vowels, do not reach adult 
standard before school age. Large studies with American children have shown 
that 90% of them produce [p], [m] and [w] correctly in word-initial, word-
medial and word-final position at age three. The sounds [b], [k], [d] and [g] are 
produced correctly by 90% of all children by age fOUf, whereas [t] and [IJ] have 
only been acquired by 90% of the children after age six. The last sound to be 
produced correctly by 90% of all children is [s], which is acquired by age eight 
(see Menn & Stoel-Gammon 1995). Overall, child speech is slower than that of 
adults. Due to the comparative lack of practice, articulatory movements by 
children take longer, coarticulation is far less pronounced (e.g. Kent 1983), and 
the appropriate reduction and deletion of vowels and consonants in connected 
speech is only acquired gradually (e.g. Allen & Hawkins 1978). 
2.7.2 Speech production in second language acquisition and teaching 
(advanced reading) 
Learning the pronunciation of a second language requires the learning of exactly 
the same muscular patterns and the formation ofthe same mental representations 
as when learning the language as a first language. Yet, the learning processes of 
first and second language acquisition differ fundamentally. While first language 
acquisition is inextricably interwoven with the development of muscular control 
and cognitive abilities, most second language learners have full command over 
their speech organs and fully developed cognitive competence. In contrast to 
first language learners, moreover, second language learners have already 
acquired patterns of muscular activities and have forrned the corresponding 
mental representations for the production of speech in their first language. It is 
generally assumed that this prior knowledge crucially influences the acquisition 
of the second language and leads to the 'foreign accent' many language learners 
exhibit in their speech. In empirical studies, numerous differences between 
speech produced by language learners and speech produced by native speakers 
have been found (see Leather & James 1996 for an overview). These include the 
articulation of individual speech sounds, coarticulatory movements as weil as 
laryngeal activity. For example, Gerrnan learners ofEnglish use different tongue 
positions for the production of the vowels [e] (in bed) and [re] (in bad) than 
native British English speakers do (Barry 1989). Spanish and French learners of 
English produce plosives with different articulatory movements than native 

Speech production 
43 
speakers do. In particular, the time span between the opening of the airstream 
blockage and the beginning of voicing is different (Flege & Eefting 1987, 
Laeufer 1996). Yet, a 'foreign accent' is not unavoidable for second language 
(L2) learners, even those that learned the L2 at an advanced age. A number of 
studies have demonstrated that some 1earners can acquire the pronunciation of 
an L2 so successfully that native speakers cannot identify them as leamers (e.g. 
Bongaerts et al. 1997, Moyer 2004). 
In general, speech produced by language leamers is slower than that 
produced by native speakers. The former produce, on average, fewer sounds per 
se co nd (e.g. Cucchiarini et al. 2002). This is partly due to the fact that the 
articulatory movements of language leamers are often far more extreme than 
those of native speakers. Russian leamers of English, for example, use less 
coarticulation, i.e. parallel movements of the individual articulators, than native 
speakers do (Zsiga 2003). Many leamers of English further do not delete 
consonants and do not reduce vowels in the appropriate contexts and to an 
adequate extent: German leamers of English do not often enough delete 
consonants in consonant clusters at the end of words and they do not reduce 
vowels in unstressed syllables like native speakers do (e.g. Gut 2007a). The 
usage of pitch also differs in a distinct way: leamers of English produce sm aller 
rises and sm aller falls than native speakers, and their overall pitch range, i.e. the 
difference between the highest and the lowest pitch in one utterance, is much 
smaller (Mennen 2007, Gut 2007a). 
These differences in speech production can in part be explained by the lack 
of practice language leamers have with the articulation of some sounds and 
sound sequences that do not occur in their native language. It has further been 
proposed that language leamers transfer some articulatory pattems of individual 
speech sounds and sound sequences from their native language to the second 
language. This notion of transfer has been integrated in many theories of second 
language acquisition. For example, the theories based on the theory of Natural 
Phonology (e.g. Dziubalska-Kolaczyk 1990) and Optimality Theory (e.g. 
Boersma 1998) claim that second language acquisition means unleaming 
patterns and processes that have been acquired for the first language. It is often 
proposed that this unleaming is obstructed by perceptual categories that speakers 
have formed for their first language (e.g. Flege 1995, Brown 2000). Put simply, 
it is assumed that speakers cannot hear the differences between sounds or 
linguistic usage of pitch and loudness in their first and second language and are 
therefore unable to produce them. The acquisition of perceptual abilities in 
second language leaming is described in more detail in section 6.8. 
The teaching of speech production in an L2 has changed over the past 
decades. While in traditional approaches, the focus of instruction lies on the 
production of individual speech sounds, recent approaches take into account that 
speech sounds do not appear in isolation in speech and that their production 

44 
Chapter 2 
depends on the neighbouring sounds as well as on prosodie factors, such as 
whether a syllable is stressed or in which position a word occurs in an utterance 
(e.g. Trouvain & Gut 2007). Instead of practising individual speech sounds, in 
those new approaches speech sounds of the L2 are practised in larger units such 
as syllabIes, words and utterances (see e.g. Missaglia 2007). Moreover, 
variations of sounds due to coarticulatory effects or stress (for example, vowel 
reduction in English) are taught as weIl. 
2.8 Methods of researching speech production (advanced reading) 
Our knowledge of the way in which the speech organs are involved in the 
production of speech sounds is fairly recent. It is only since the 193 Os that 
technical devices have become available for studying the physiology of speech 
production in detail. A few of the currently most commonly used ones will be 
described in this section. The functions of the organs of the respiratory system 
cannot be studied directly. In order to measure the direction of airflow as well as 
the air pressure in the oral or nasal cavities, a method called aerometry is 
applied. A mask is put over the speaker's nose and mouth, which is equipped 
with transducers that convert the air pressure into electrical signals, which in 
turn can be recorded onto a computer. 
Figure 2.12. A speaker with a nasameter. 
To examine nasal airflow a nasometer is used (see Figure 2.12). Small 
microphones separated by a metal plate are put on aseries of straps and are 

Speech production 
45 
either fitted to the speaker's head or attached to poles. The upper microphone 
records air flowing from the nose, whereas the lower one records air flowing 
from the oral cavity. This enables researchers to measure the relationship 
between both types of airflow, which is for example important for the 
description of speech produced by cIeft palate patients (see section 2.3 above). 
Phonation, the movement of the vocal folds, can be examined by various 
means. For an indirect measurement of vocal fold activity a laryngograph is 
used. Two electrodes are put on the speaker's throat on either side of the thyroid 
cartilage. A weak current is passed between the two electrodes, which measures 
how often the vocal folds touch. The degree of current reflects the contact 
between the vocal folds so that vocal fold vibration can be shown in a 
waveform. This type of measurement gives information about voicing, modes of 
phonation, pitch and other characteristics of voice quality. Direct observation of 
the vocal folds is also possible. They can either be seen with the help of a 
laryngoscope, which is an angled rod with a mirror at its end, or by using an 
endoscope, a tube that is fitted with a light source and is connected to a 
recording device. A very high film speed is needed to capture the rapid vocal 
fold vibration. Both laryngoscope and endoscope are inserted into the mouth and 
held directIy above the larynx, which of course means that no speech sounds can 
be produced that involve significant tongue movement. The videos 
02_vocalfolds.gif and 02_glissando.mov on the CD-ROM were recorded with 
an endoscope. The methods of laryngoscopy and endoscopy provide important 
information about whether a speaker's vocal folds are heaIthy. Many teachers 
experience problems with their voice during their career (e.g. Roy et al. 2004); 
the examination of the vocal folds and of vocal fold activity shows whether the 
ailment has an organic basis. 
The action ofthe articulators in the articulatory system can be examined in a 
number of different ways. X-ray techniques, which display the movement of the 
articulators without hindering the speaker in any way, have not been applied 
anymore since the harmfulness of the method was discovered. The video 
02_connectedspeech.mpg on the CD-ROM shows an X-ray recording that was 
done in 1974. A new method of measuring the exact movements of the tongue 
and 
lips 
during 
articulation 
is 
called 
electromagnetic 
mid-sagittal 
articulography, or EMMA. It aIIows the analysis of articulatory movements 
within the oral cavity with high time resolution. For an EMMA study, 
transducers, which are connected to an amplifier with wires, are placed on a 
speaker's tongue, Iips and nose. The speaker additionally wears a heImet in 
which the transducers create a magnetic field (see Figure 2.13). The 
measurement of the alternating currents aIIows the caIculation of the degree and 
velocity oftongue and lip movements during articulation. 

46 
Chapter 2 
Figure 2.13. A participant in an EMMA experiment. 
A device that records the movement ofthe tongue is the palatograph. For this, a 
thin artificial palate fitted with a large number of electrodes is put onto the 
speaker's palate. The electrodes fire when they come into contact with the 
tongue so that the position and degree of tongue contact can be measured with 
the help of an attached computer. Palatographic studies show that the patterns of 
articulatory movements can be quite different for various speakers producing the 
same sound. Figure 2.14 shows the place and strength of contact of the tongue 
with the roof of the mouth during the production of [tl and [d] for two different 
speakers. The upper part of the picture illustrates the region of the alveolar 
ridge, the bottom part the region of hard palate. Black areas signify strong 
contact; grey areas were only lightly touched by the tongue, and white areas not 
at all. The left-hand palatogram shows that the speaker's tongue has strong 
contact with the alveolar ridge and both sides of the palate, whereas the speaker 
whose palatogram can be seen on the right articulates [tl and [d] mainly with 
tongue contact at the alveolar ridge. 
The activity of the brain during speech production cannot be studied 
directly. However, with the technique of electroencephalography (EEG), the 
electrical activity of the brain can be recorded in an indirect way. Electrodes 
placed on the scalp measure voltage differences between different areas of the 
brain that occur during speech tasks. It is much easier to examine speech 
perception than speech production with this method since the brain activities 
involved in articulation complicate the interpretation of the results. Functional 
magnetic resonance imaging (fMRI) is another method for the examination of 
brain activity during speech tasks. Humans taking part in an fMRI experiment 
lie on a stretcher-like device and are put into a cylinder-shaped tube where they 

Speech production 
47 
have to perform different tasks such as listening to language or seeing pictures. 
The blood flow, which reflects neural activities in the different areas of the 
brain, is measured by the magnetic resonance reflecting the level of oxygen in 
the blood. 
front of mouth 
front of mouth 
Figure 2.14. Palatograms 01 [tl and [d} lor two speakers. 
Finally, the activity of all muscles involved in the production of speech can be 
examined. In order to measure which muscles are active at which stage of sound 
production, the technique of electromyography (EMG) is applied. With the 
help of electrodes, which are either inserted into the relevant muscle or placed 
on the skin above the muscle, it measures the tiny electrical charges that muscles 
produce when activated. However, raw electromyographic data are difficult to 
interpret since the activity of a particular muscle must be compared to that of 
others and the total of muscular patterns. Thus, large numbers of measurements 
are usually averaged across utterances and across speakers. 
2.9 Exercises 
I. What is the main function ofthe respiratory system of speech production? 
2. Which organs are involved in phonation and what is their function? 
3. List all articulators that make up the articulatory system. 
4. Which places and manners of articulation exist for consonants? 
5. Why is it difficult to describe the production of a particular speech sound in 
real connected speech? 
6. Which role do neural mechanisms in the brain play in speech production? 
7. Why is child speech and the speech produced by second language learners 
slower than that of adult native speakers? 

48 
Chapter 2 
8. Listen to recording 02_exercise8.wav on the CD-ROM. Mark the breath 
group boundaries in the text. 
9. Ask at least 20 people how they produce the sound [t]. Which articulator 
does their tip ofthe tongue touch? 
2.10 Further reading 
More details on the anatomy and physiology of speech production can be found 
in Clark, Yallop & Fletcher (2007, chapters 2 and 6) and Lieberman & 
Blumstein (1988, chapters 2 and 6). For especially interested readers, Laver 
(1994, chapters 6 to 11) and Ladefoged (2001a, chapters 11 and 12) provide 
very detailed ac counts of the different articulations of English vowels and 
consonants. Ladefoged and Maddieson (1996) describe the distribution of the 
different types of articulation in the languages of the world. Lieberman (1984) 
gives more information on the evolution of speech in humans. The theory of 
articulatory gestures is explained in Browman & Goldstein (1992). A 
comprehensive description of the development of speech production abilities in 
first language acquisition is given in Vihman (1996, chapter 5). Second 
language speech production is described further in Leather & James (1996) and 
Zampini (2008). 

3 The Phonology ofEnglish: Phonemes, Syllables and Words 
This chapter and chapter 4 deal with the phonology of English. Chapter 1 
explained that phonologists are concerned with units of sound strueture and 
sets of phonologieal roles that describe patterns and regularities of the sounds 
in a particular language. These phonological units and rules are assumed to be 
stored in a speaker's brain, which enables hirn or her to produce and understand 
speech. Which knowledge do speakers have about the phonological units of a 
language? Speakers will tell you, for example, that words consist of individual 
speech sounds. Some might even point out that these speech sounds can further 
be grouped into syllabies. Moreover, all speakers would agree that words 
themselves can be grouped into utterances. The units of speech thus seem to be 
organized as a hierarchy with larger units (such as words) containing sm aller 
units (such as syllables and speech sounds). Speaker knowledge ofphonological 
units can be illustrated in a prosodie hierarehy (e.g. Selkirk 1986, Nespor & 
Vogel 1986). Figure 3.1 shows the prosodie hierarchy of the (British English 
pronunciation of the) utterance "This figure". This utterance consists of the two 
words this and figure. The word this comprises one syllable, whereas the word 
figure contains two syllables (each represented by the symbol 0'). Each of the 
syllables is made up of a number of individual speech sounds. This consists of 
three speech sounds, fi and gure of two each. The term prosody refers to those 
phonological units that are on a higher level than the speech sounds and is thus 
another term for suprasegmental phonology (see chapter 1). Segmental 
phonology is concerned with the units and phonological rules of the lowest level 
of the prosodie hierarchy - the speech sounds. 
utterance 
word 
word 
I 
~ 
0' 
0' 
0' 
~ /\/\ 
DIS 
f 
I g ~ 
Figure 3.1. Some levels of the prosodie hierarehy: speech sounds (phonemes), 
syllables and words of the utteranee "This figure". 

50 
Chapter 3 
Section 3.1 describes the smallest of the phonological units in English: the 
phonemes. It explains both their articulatory properties as weIl as the way they 
are transcribed in phonological and phonetic analysis. Section 3.2 is concerned 
with syllables in English and describes their structure and patterns. The 
phonological properties of English words, including word stress, are presented 
in section 3.3. Section 3.4 describes how phonological representations are 
acquired in both first and second language acquisition. The higher levels of the 
prosodic hierarchy such as phrases, utterances and discourse are dealt with in 
chapter 4. 
3.1 The phonemes of English 
Section 2.5 has shown that speech sounds are articulated in different ways 
depending on their neighbouring sounds (this is called the phonetic context) 
and the general speed of articulation. In English, for example, the voiceless 
bilabial plosive Ipl in the word pin is aspirated. This means that between the 
opening of the closure formed with the lips and the beginning of vocal fold 
vibration for the vowel Irl there is a short time interval in which the airstream 
passes out of the mouth with an audible hiss. In the word spin, however, due to 
the influence of the preceding Is/, the Ipl is not aspirated. Moreover, for the 
articulation of Ipl at the end of a word (for example in stamp) or before another 
plosive (as in captain), the pulmonic airstream ends before the lips are opened-
the /p/ is unreleased. Similarly, other speech sounds in English can have very 
different types of articulation depending on the phonetic context. The Id/ in dish, 
for example, is produced with the tip of the tongue touching the alveolar ridge. 
When a /dl appears before an interdental sound as in the word width, however, it 
has a place of articulation that is dental, which means that the tip of the tongue 
touches the back of the teeth. 
Speakers of English are nonnally not aware of the fact that speech sounds 
can be articulated in very different ways. They do not pay attention to these 
articulatory differences because they are not functionally relevant in English. 
Despite producing and hearing many different kinds of voiceless bilabial 
plosives including unaspirated [p], aspirated [ph] and unreleased [p'], speakers 
have only one 'mental image' of Ip/. This mental representation of a specific 
speech sound is called phoneme. The actual speech sounds speakers produce 
and listeners hear are called phones. The phoneme /pl in English thus includes 
the phones [p], aspirated [ph] and unreleased [p']. The phoneme /d/ includes the 
alveolar and the dental voiced plosive. (Remember that the slashes / I are used to 
indicate the mental representation of a speech sound and the square brackets [ ] 
are used when we refer to an actual pronunciation). Some phoneticians, in 
contrast to 
phonologists, 
doubt whether speakers really use 
mental 

English phonology: phonemes, syllables and words 
51 
representations of sounds in speech production and speech perception. See 
section 2.7.2, which discusses whether the phoneme is a useful concept in 
language teaching. 
Not every phone has the function of a phoneme in every language. Phones 
only have the role of phonemes in a language when they have a contrastive 
function, which means that they cause changes in meaning. A phoneme is thus 
defined as the smallest meaning-distinguishing unit in a language. Whether a 
phone has phonemic status in a language or not can be shown with the minimal 
pair test. If you take the voiceless interdental fricative 181 in the English word 
think and replace it by the voiceless alveolar fricative Is/, you have changed the 
meaning ofthe word to sink. Think and sink thus form a minimal pair in English. 
The minimal pair test, moreover, shows that 181 does not have the function of a 
phoneme in German. Ifyou replace the Isl in Lust with a 18/, the meaning ofthe 
word does not change - listeners will merely assurne that you have a lisp. 
Typically, a phoneme of a language can be realized as different phones. As 
described above, the Ipl in English can be produced as the aspirated [ph]. 
AIternatively, it can be produced without aspiration as the phone [p] or without 
audible release as the phone [p ']. Equally, the Idl can be produced as an alveolar 
[d] or as a dental voiced plosive [g] depending on the phonetic context. The 
phones that are realisations of the same phoneme are called allophones. Thus, 
[p], [ph] and [p'] are allophones of the phoneme Ip/, and alveolar [d] and dental 
[g] are allophones of the phoneme /d/ in English. The distribution of the 
allophones of a phoneme in speech is usually complementary. This means that 
each allophone occurs excIusively in one specific phonetic context. This is, for 
example, the case for the allophones of Ipl in English. The complementary 
distribution of the allophones of Ip/ can be described by the following 
phonological rule: [ph] occurs when Ip/ is the only consonant at the beginning of 
a stressed syllable as in the word picture. The allophone [p'] occurs before other 
plosives - as in the word captain - or at the end of an utterance. The allophone 
[p] occurs in other positions, such as in the word spin (see section 3.1.2 below 
for a list ofmore allophones ofEnglish consonants). 
Some allophones have also been cIaimed to be distributed in free variation, 
i.e. without any systematic distribution according to phonetic context. This 
seems to be the case for the different realizations of Irl in German. The alveolar 
trill (which sounds like the ltalian Ir/), the uvular trill (a sound like gargIing) and 
the voiced uvular fricative (the IIfI of Standard High German) can occur in every 
position of Gennan words. Yet, speakers do not produce these different 
allophones randomly: the choice of allophone seems to be a matter of regional 
dialect. Bavarian speakers, for example, tend to use only the alveolar tri 1I , 
whereas speakers from Hannover use the voiced uvular fricative in all places. 
Many other apparent cases of free allophonic variation can be described as 
different stylistic variants of phonemes. This means that the choice of a 

52 
Chapter 3 
particular allophone depends on who the speaker is talking to and on the 
speaking situation. In British English, for example, word-final Itl tends to be 
produced as agiottal stop [?] - for example in the word what - by young 
speakers rather than older speakers, and they do so more in informal situations 
than in fonnal situations (Fabricius 2002). Similarly, in slow and careful speech 
in English, the Itl at the end of words is usually released, whereas in casual or 
fast speech it tends to be unreleased. 
The phones that function as phonemes in a language make up the phonemic 
system or phoneme inventory of a language. A description of the phoneme 
inventory of English, however, is far from trivial. English is spoken all over the 
world and often the different national varieties as well as the different regional 
(and even social) varieties that are spoken within a country differ widely. Thus, 
the phoneme inventory of Jamaican English does not match the phoneme 
inventory of Scottish English, although many phonemes will be shared by the 
two. For the interested reader, the phoneme inventories of many world-wide 
varieties of English are described in Kortmann & Schneider (2004). This 
textbook focuses primarily on the phonemes of Standard British English and 
Standard American English since these are the most widely taught varieties of 
English in second language teaching. The term standard refers to the variety of 
English that has the highest prestige in a country, that is the variety described in 
gramm ars and dictionaries and whose rules are taught at school. On the British 
Isles, the standard is variously called Received Pronunciation (RP), BBC 
English, Oxford English or Southem British Standard. The standard variety of 
the U.S.A. is referred to as General American (GA). Only about 5% of the 
population on the British Isles speak RP, and the majority of RP speakers 
originate from or live in the south-east ofEngland, have a middle-class or upper-
class background and a high level of education. RP is thus more of a social than 
a regional accent. General American is more wide-spread in the U.S.A. 
(approximately half of the speakers there are GA speakers). The term GA refers 
to a group of accents that does not bear any marked regional characteristics and 
is the accent most commonly used in radio and television in the U.S.A. 
The phoneme inventory of a language is usually divided into consonants 
and vowels. Section 2.4 explained that this classification is partly based on the 
manner of articulation of the different speech sounds. There are furthennore 
phonological reasons for making this distinction. Consonants and vowels are 
distributed in a distinct way. Some positions in a syllable can only be filled by 
vowels, other positions are always occupied by consonants in English (see 
section 3.2 below). Interestingly, the division of speech sounds into vowels and 
consonants is not the same across languages. For example, 111, which is 
considered a consonant in English, is classified as a vowel in some dialects of 
Chinese. 

English phonology: phonemes, syllables and words 
53 
3.1.1 The consonants of RP and GA and their transcription 
The phoneme inventory of RP and GA comprises 24 phonemes. In order to be 
able to represent phonemes in writing, the International Phonetic Alphabet 
(I PA) was developed. It contains transcription symbols for all distinctive speech 
sounds that occur in any language of the world. In addition, it offers 
transcription symbols for fine phonetic details and prosodie features, the so-
called diacritics. The IP A has a long history : in 1886, the International Phonetic 
Association, consisting of language teachers who wanted to devise a phonetic 
notation in order to support language learning, was founded in Paris. It 
distributed the first phonetic alphabet in the late 19th century. The symbols 
employed were chosen with the aim of keeping them as simple as possible for 
language learners in Western Europe so that most of them were taken from the 
Roman alphabet. In the past centuries, the IPA has undergone several revisions 
(the last one was completed in 2005). 
Bilabial 
Labiodental 
Dental I 
Alveolar jPostalveolar Retroflex 
Palatal 
Velar 
Uvular 
Pharyngeal 
Glottal 
Plosive 
p b 
t d 
t cl c J k 9 q G 
1 
71 
Nasal 
m 
11) 
n 
11 
J1 
I) 
N 
Trill 
B 
r 
R 
Tap or Flap 
V' 
[ 
r 
Fricative 
<I> ß f v 8 01 s z l J 3 
~ ~ 9 J X Y X K h 1 h fi 
Lateral 
i B 
fricative 
Approximant 
U 
J 
-l 
J 
uy 
Lateral 
1 
l 
f.. 
L 
approximant 
Figure 3.2. The IPA transeription symbols for the pulmonie eonsonants. 
Reprinted with permission from The International Phonetie 
Assoeiation. Copyright 2005 by International Phonetie Assoeiation. 
Figure 3.2 presents the IPA symbols for all pulmonic consonants, i.e. consonants 
produced with an egressive pulmonic airstream (see section 2.1). The chart can 
be read in the following way: each row represents a different manner of 
articulation, ranging from plosive to lateral approximant. Each column refers to 
a different place of articulation, moving from the lips (bilabial) to the larynx 
(glottal). Each cell of the chart thus represents a particular combination of a 
manner and a place of articulation. When two symbols appear in one cell, the 
one on the left is the symbol for the voiceless consonant and the one on the right 
is the symbol for its voiced counterpart. Thus, [p] is the symbol for the voiceless 
bilabial plosive and [b] is the symbol for the voiced bilabial plosive. Cells that 

54 
Chapter 3 
are shaded grey signify combinations of place and manner of articulation that 
are physiologically impossible. Empty cells hold possible combinations of place 
and manner of articulation, but no such sound has yet been discovered in the 
languages of the world. 
Table 3.1. The consonants 0/ RP and GA. 
Phonetic symbol 
Examples 
word-initial 
word-final 
p 
pin 
rip 
b 
bin 
rib 
t 
tin 
sit 
d 
din 
rid 
k 
kin 
Rick 
g 
go 
rig 
m 
mine 
rim 
n 
nine 
Rhine 
1) 
ring 
f 
fin 
riff 
v 
vase 
live 
e 
thin 
heath 
0 
this 
with 
s 
sin 
rice 
z 
zoo 
rise 
S 
shin 
wish 
3 
genre 
beige 
h 
him 
J 
rim 
beer 
(GA only) 
j 
year 
I 
Lynn 
bile 
w 
whim 
tS 
chin 
hatch 
d3 
gin 
lodge 
Of all the consonants depicted in the IP Achart in Figure 3.2, RP and GA use 24. 
These are Iisted in Table 3.1. The phoneme inventory of these two standard 
varieties of English comprises six plosives: two bilabial (lp/ and /b/), two 
alveolar (ltl and /d/) and two velar (lk/ and /g/) ones; one of each voiced and the 

English phonology: phonemes, syliables and words 
55 
other voiceless. Furthermore, RP and GA have three nasals, namely the bilabial 
Im/, the alveolar In! and the velar Ifj/. Of the fricatives, there are nine in RP and 
GA: a voiceless and a voiced labiodental (/fl and lvi) fricative, a voiceless (/SI) 
and a voiced (la/) interdental fricative, the voiceless alveolar Isl and the voiced 
alveolar Izi as weil as the voiceless and the voiced postalveolar ISI and 13/. In 
addition, there is the phoneme Ih/, a voiceless glottal fricative. Both RP and GA 
have three approximants, the alveolar IlI, the palatal Ij/ and the alveolar lateral 
approximant 11/. In addition, the phoneme Iwl occurs, which is characterized as a 
labiovelar approximant with two pI aces of articulation (it therefore does not fit 
into the IPA chart illustrated in Figure 3.2). Lastly, there are two affricates that 
are sometimes counted as phonemes ofRP and GA because they have meaning-
distinguishing function: the combination of Itl and ISI and the combination of Id/ 
and 13/. 
Table 3.1 lists examples of these phonemes occurring at the beginning of a 
word (= word-initially) and the end of a word (= word-finally). It is easy to see 
that not all phonemes can occur at the beginning ofwords (there are no English 
words or syllables beginning with Ifj/) and that not all of them occur at the end 
of words or syllables (the phonemes Ih/, Ijl or Iwl as weil as 111 in RP do not 
occur in this place). 131 is a very rare consonant that only occurs in a few words 
and does so mostly medially, i.e. in the middle of a word as in plea~ure. This 
can be explained with the history of English. This speech sound only emerged 
relatively recently, during the Early Modem English period (roughly 1400 to 
1650), and only occurs in words that were originally borrowed from French or 
Latin. The alveolar approximant 111 occurs in word- and syllable-final position 
only in GA but not in RP (see section 3.2.1 for further details). Again, this 
difference is due to a relatively recent development in RP. While the English 
spoken in Shakespeare's time and consequently that which was transported to 
the first settlements in Northem America had 111 in all positions of the word (it 
was most likely realized as the alveolar trill [r]), 111 was later dropped in specific 
positions in most varieties ofEnglish spoken on the British Isles, including RP. 
Many phonologists claim that Ifjl does not constitute a phoneme in its own 
right but should rather be analysed as an allophone of In/. Historically speaking, 
this view is true since up to about the 1 i h century Ifjl was indeed an allophone 
of In! that occurred when In! was followed by a Igl or /kI. In Shakespearean 
times, thus, the word sing was mostly pronounced as ISIfjgl and only few 
speakers began to pronounce it as ISIlJI as we do today. For an analysis of 
present-day English, however, it can be argued that since minimal pairs like sin 
and sing exist, Ifjl should be treated as a phoneme in its own right. 

56 
Chapter 3 
3.1.2 
The allophonic variation of consonants in RP and GA 
All of the phonemes of English listed in Table 3.1 can be realised as several 
different allophones, whose distribution is either deterrnined by the phonetic 
context or varies for stylistic reasons. When the variation of allophones is 
determined by the phonetic context, their distribution can be described by 
allophonic rules. The major allophonic rules ofEnglish are listed in Table 3.2. In 
order to be able to transcribe, i.e. represent in writing, the articulatory 
differences between allophones, the IPA includes a list of diacritic symbols (see 
Figure 3.3). These diacritics, when added to a phonetic symbol, indicate 
additional articulatory details of the production of a particular speech sound. 
Diacritic symbols can therefore be used to describe the allophonic variation of 
phonemes. 
Table 3.2 shows that, in both RP and GA, the voiceless and voiced plosives 
have a high number of allophones in complementary distribution. When Ip, t, kJ 
occur as the only consonant before a stressed vowel, they are aspirated and 
transcribed as [ph], [~] and [kh]. The voiceless plosives are therefore aspirated in 
pea, tea and key but not in spring, steam or clay. This allophonic variation can 
be formulated as a phonological rule. Rule (1) illustrates in which phonological 
context Ip, t, kJ are aspirated: 
(1) 
IP/} 
Itl 
IkJ 
It expresses that the phonemes Ip, t, kJ are aspirated when they are not preceded 
by another segment in the syllable. The arrow ~ indicates a phonological 
process, the slash (/) gives the condition (when) and the dot . stands for the 
boundary of a syllable. The _ refers to the position of the phonemes the 
allophonic rule applies to. When the phonemes Ip, t, kJ occur at the end of a 
syllable after a vowel, they are often glottalized, Le. accompanied or slightly 
preceded by agiottal stop. Thus, the word cup is pronounced [kA?p] and hit is 
pronounced [hI?t]. When followed by another oral plosive, as for example in the 
word captain or the phrase good day, the plosives Ip, t, k, b, d, g/ are unreleased. 
This means that there is no airstreatn coming from the lungs when the closure 
formed by the tongue and the other articulator is opened. The diacritic 
transcription symbol for an unreleased consonant is [']. The word captain is thus 
usually pronounced [keep 'tn] and good day is usually pronounced [gud'deI]. This 
phonological process also happens regularly at the end of utterances, for 
example in "He's a good chap", where chap is produced [tSeep T In all other 
positions, these plosives have no aspiration, i.e. audible release of air after the 
opening ofthe closure and before the onset of vocal fold vibration for the vowel. 

English phonology: phonemes, syllables and words 
Table 3.2. Allophonic variation 0/ some consonants 0/ RP and GA. 
Ip,t,k/ 
[ph], [th], [kh] 
[?p], [?t], [?k] 
[p], [tl, [k] 
as only consonant before stressed vowel 
syllable-final after vowels 
elsewhere 
Ip,t,k,b,d,gl 
[p '], [C], [k '], 
before other plosives and at the end of an 
[b'], [d'], [g'] 
utterance 
[pW] , [tW] , [kW] , before rounded segment 
[bW], [dW], [gW] 
Ik,gl 
[kJ, [&] 
before front vowel 
[k], [g] 
elsewhere 
Ib,d,g,z, v ,01 [q], [dJ, [g], [~], next to voiceless sounds, before and after 
[~], [q] 
silence 
[b], [d], [g], [z], elsewhere 
[v], [0] 
It,dl 
[t], [g] 
before interdental consonant 
[r] 
as only consonant at beginning of an 
unstressed syllable when preceded by a 
vowel or a sonorant consonant (GA only) 
It/ 
deleted 
in unstressed syllable after In! (GA only) 
In! 
Iml 
11/ 
IJ,w,j/ 
[I] 
in sy llable-final position 
[v] 
[n:] 
[n] 
[11]] 
[m] 
[tl 
m 
UJ 
[I] 
[~], [~], m 
[1], [w], [j] 
before a dental fricative 
before voiced obstruent in the same syllable 
elsewhere 
before labiodentals 
elsewhere 
after vowel 
after voiceless plosive 
after dental consonant 
elsewhere 
after a voiceless plosive 
elsewhere 
57 
The plosives in RP and GA show further allophonic variation (as shown in 
Table 3.2). When Ip, t, k, b, d, gl occur be fore asound produced with rounded 

58 
Chapter 3 
lips such as Iwl and lul, they are produced with lip rounding as weIl. For 
example, when articulating words such as quest and cool, speakers have rounded 
lips for the production of the first phoneme /k/. This lip rounding or 
labialisation is transcribed as [w]. Similarly, the Ig/ in good as well as the Itl in 
two are rounded, which is transcribed as [gW] and [tW], respectively. 
Voieeless 
n d 
Breathy voieed b a 
Dental 
t d 
0 
0 
0 
.. 
n 
n 
n 
Voieed 
S t 
Creaky voiced b a 
Apieal 
t d 
v 
v 
v 
-
u 
u 
u 
h 
th dh 
t d 
Laminal 
t d 
Aspirated 
-
Linguolabial 
c 
c 
c 
w 
tW dw -
e 
More rounded 
~ 
Labialized 
Nasalized 
) 
) 
~ 
j 
tj dj 
n 
dn 
Less rounded 
Palatal ized 
Nasal release 
( 
( 
U 
Y 
ty dY 
I 
d1 
Advanced 
Velarized 
Lateral release 
+ 
+ 
'I 
t'l d'l 
, 
d' 
Retracted 
e 
Pharyngeal ized 
No audible release 
-.. 
e 
t 
Centralized 
-
Velarized or pharyngeal ized 
x 
x 
Mid-centralized e 
Raised 
y 
(t = voiced alveolar fricative) 
... 
Syllabic 
n 
Lowered 
e 
(13 = voiced bilabial approximant) 
, 
, 
T 
T 
Non-syllabic 
~ 
Advanced Tongue Root 
e 
" 
~ 
~ 
"" 
Rhoticity 
~ ~ 
Retracted Tongue Root 
e 
~ 
~ 
Figure 3.3 The IPA transcription symbols/ar phonetic details: the diacritics. 
Furthermore, the velar plosives Ig/ and /k/ have different places of articulation 
depending on the following vowel. The /k/ in kif, cat and cot has three different 
places of articulation, which you can feel when you articulate these words 
slowly. When followed by a front vowel, for example in kit, the /k/ is fronted as 
well, which is transcribed as [~..]. When followed by a central vowel as in cat, the 
place of articulation is palatal. When preceding a back vowel like in cot, /k/ is 
produced as a velar plosive (see section 3.1.3 for a description of the different 
types ofvowels). 
The voiced obstruents /b, d, g, Z, v, öl in RP and GA are sometimes partly or 
fully devoiced, i.e. produced without vocal fold vibration. This depends on their 
phonetic environment: when they are preceded and followed by voiced sounds, 
they are always fully voiced, but when they are preceded or followed by 
voiceless sounds or silence (at the end or the beginning of an utterance), they are 
usually partly or fully devoiced. For example, the /bl in obtuse is partly devoiced 

English phonology: phonemes, syllables and words 
59 
as well as the lvi in dovetail. These words are transcribed [<lqtju:s] (RP), [<lqtu:s] 
(GA) and [dA ~eIt] respectively. Devoicing is indicated by the subscript circle 
under the IPA symbol. If you wanted to express this as an allophonic rule you 
would formulate it as in (2): 
{ 
[b], [d], [g], [z], [v], [5] I [+voice] _ [+voice] 
(2) 
Ib,d,g,z,v,51 ~ 
[q], [q], [g], [2.;], [\;;], [5J I elsewhere 
This rule states that the voiced obstruents Ib, d, g, z, v, 51 are realized as [b], [d], 
[g], [z], [v] and [5] only when they are in the position between two voiced 
segments (the meaning of the symbol [+voice] is explained in section 3.1.5 
below); in all other positions they are devoiced. This allophonic rule implies that 
there is very little difference in the articulation of /kI and Ig/ in leek and league, 
between the articulation of Itl and Id/ in right and ride and the articulation of Ipl 
and Ibl in rope and robe. In fact, the major difference between these minimal 
pairs lies in the glottalisation of Ip, t, k/ and the length of the preceding sonorant 
(vowel, nasal or lateral). Before a 'voiced' plosive a vowel, nasal or IlI is 
considerably longer than when preceding a voiceless plosive. Compare wide and 
white, send and sent andfelt andfelled. 
The alveolar plosives Itl and Idl are produced with the tip of the tongue 
touching the back of the teeth when a segment that has an interdental place of 
articulation folIows. This is the case in sit there and width, where the alveolar 
plosives are produced with a dental place of articulation. This is transcribed as 
IJ] and [g], respectively. One allophonic rule that only applies to GA concerns 
the phonemes Itl and Id/. When they occur as the only consonant at the 
beginning of an unstressed syllable and have either a vowel or a sonorant 
consonant preceding them, these phonemes are realized as an alveolar tap [f]. 
This phonological process, which is referred to as flapping (the term tapping 
would be more correct but is used less often), can be observed in words such as 
letter, writing and startle, which are pronounced [IEn], [laIfIlj] and [stOlrt], 
respectively. This means that for many GA speakers the words latter and ladder 
are homophones, i.e. are pronounced in nearly the same way. This rule also 
applies across word boundaries as in phrases like at all [<lfJI] and eat up [ifA?P]. 
Another phonological process that occurs only in GA is the deletion of Itl in 
unstressed syllables when following a In!. This phenomenon can be observed in 
words like winter and rental which are pronounced [WIll<ll] and [lEn<lI]. This 
phonological process is not an allophonic process in its strict sense because the 
phoneme Itl is not realized at all. In both RP and GA, the phoneme Itl is often 
produced as agIottal stop [?] when it appears at the end of a syllable. This is the 
case in words such as batman [bre?m<ln] and atlas [re?l<ls] as weil as across 
phrases like hit me [hr?mi]. As mentioned above, this allophonic variation 

60 
Chapter 3 
occurs mainly in informal styles, and younger speakers produce it more 
frequently than older ones. 
Table 3.2 further illustrates that the phoneme In! also has allophonic 
variants: when followed by a dental fricative such as [S] in tenth, the pI ace of 
articulation of the nasal is dental rather than alveolar and is transcribed with the 
symbol [9]. Before a voiced obstruent in the same syllable, such as in the words 
lend or lens, the alveolar nasal is lengthened and produced as [n:]. The bilabial 
nasal Iml is usually produced as a labiodental nasal [11]] when it is followed by a 
labiodental segment as in the words Humphrey [hA11]fJI] and emphatic 
([e11]fretIk] in RP and [E11]fretIk] in GA). This can even be noted across phoneme 
boundaries in phrases such as come forward, which is pronounced 
[kA11] b( 1 )w<ld]. 
In RP, 11/ has four different allophones: when it occurs at the beginning of a 
syllable as in leap or between vowels as in stealing and steal it, it is realized as 
the alveolar lateral approximant [1] (sometimes called 'clear I'). When preceded 
by a voiceless plosive, however, as in the word sleep, it is realized without 
voice, and is transcribed as [U. When followed by an interdental fricative as in 
wealth, the 111 is produced with a dental place of articulation - hence the 
pronunciation [welS] in RP and [wElS] in GA. At the end ofa syllable, as inpill 
or fold, the lateral approximant is typically realized with the tongue body 
retracted towards the uvula or raised towards the velum (sometimes the tip of 
the tongue does not even touch the palate anymore). It is 'velarized' (sometimes 
called 'dark I') and transcribed as [t]. Many American English accents, in 
contrast to most British English accents, have the allophone 'clear l' in only one 
position - when it occurs together with another consonant at the beginning of a 
word as in clean. In all other positions, such as between vowels (as in healing) 
or at the end ofwords (as infeel), the 11/ is velarized. 
Just as for 11/, devoicing, i.e. voiceless realization when preceded by a 
voiceless consonant, occurs also for the phonemes Iw, j, 11. This is, for example, 
the case in words like twin, crawl and tune (RP only), which are produced 
[t'YIn], [19;:,1] (GA: [1901]) and [tjun]. 
3.1.3 The vowels of RP and GA and their transcription 
The vowel systems ofRP and GA have many phonemes in common but differ in 
some respects. In total, RP has 23 phonemic vowels, whereas GA has 16. 
Traditionally, vowels are divided into monophthongs (one vowel, also 
sometimes referred to as 'pure vowel'), diphthongs (two vowels in a sequence) 
and triphthongs (a sequence of three vowels). The word see contains the 
monophthong liI, the word say contains the diphthong leIl, whereas the RP 
pronunciation of the word fire contains a triphthong - the sequence laI<l/. GA 

English phonology: phonemes, syllables and words 
61 
does not have triphthongs, which has to do with the fact that it has coda 111 (see 
section 3.2.1 for details). True monophthongs, howeve1', are difficult to find in 
English - when analysing speech instrumentally as desc1'ibed in chapter 5, one 
can see that many English vowels do not have stable qualities but are always 
slightly diphthongized. 
Figure 3.4 gives the IPA transcription symbols for vowels. They are usually 
presented in a quadrilateral 01' vowel chart that 1'oughly illustrates the shape of 
the oral cavity. On the vertical axis, the vertical position ofthe tongue and lower 
jaw is represented, 1'anging from elose (01' high) to open (01' low). The horizontal 
axis refers to the part of the tongue that is active during articulation, comprising 
front, central and back. The location of the vowel li/, which occurs in bee, in 
the vowel quadrilateral thus shows that it is produced with the tongue in a high 
position (the mouth is nearly closed) and with the front of the tongue raised in 
the front of the mouth. For the production of the vowel 101, by contrast, the 
mouth is mid-open with a mid-low tongue that arches at the back. Most tongue 
positions in the IP A vowel chart are filled by two symbols. In these cases, the 
vowel on the right is produced with rounded lips (for example lu/) and the one 
on the left is produced with sp1'ead lips (for example li/) . 
. 
. 
Close 1 
y 
l~ 
UI U 
IY 
U 
Close-mid 
e 
f/J 
~ e 
1( 
0 
g 
Open-mid 
e ce-3\G-A ~ 
re 
TI 
Open 
a CE \ 
Q 
D 
Figure 3.4. The IPA transcription symbols for the cardinal vowels. 
Unfortunately, the articulation of vowels cannot be described as neatly as the 
articulation of consonants (see also sections 2.3 and 2.4). The difference 
between the front and the central part of the tongue 01' a mid-closed and a mid-
open mouth, for example, is not as absolute and clear-cut as the differences in 
articulation between a plosive and a nasal. Rather, differences in vowel 
articulation are gradual and have no clear boundaries. This means that the 

62 
Chapter 3 
articulatory descriptions of the vowels in the quadrilateral in Figure 3.4 are 
idealized. The positions of tongue and jaw that are indicated are extreme 
positions which are rarely reached in real articulation. These idealized vowels 
are referred to as cardinal vowels. Their primary function is to serve as 
reference values for phoneticians and phonologists describing the vowel 
inventory of a language. Thus, it happens often that the descriptions of the same 
language by different authors do not match entirely. Together with the fact that 
the vowel inventories of the different varieties of English are continuously 
changing (cf. Hawkins & Midgley 2005 for RP), slight differences between 
descriptions of the vowel inventory of a variety of English may always occur. 
Table 3.3 lists the vowel inventories ofRP and GA as they are described by 
most phonologists. In terms ofmonophthongs, both varieties ofEnglish have the 
unrounded high (or close) front vowels lil and Irl, the rounded high back vowels 
lul and lul, the unrounded mid central vowels 131 and I~/, the rounded mid-open 
back vowel hl, the unrounded mid-open front vowel lrel, the unrounded mid-
open central vowel lAI and the unrounded low back vowel Ia!. GA furthermore 
has the mid-low front unrounded lei. RP has the unrounded mid-high front 
vowel lei (although in some descriptions you will find lei instead of leI) and the 
low back rounded monophthong 10/. Some phonologists claim that the vowellJI 
in GA is increasingly being replaced by the unrounded low back vowel Ia! in 
words such as caught. 
As far as the diphthongs are concemed, GA and RP share leI/, lau/, lall and 
I;)ll, which are called cIosing diphthongs because the second vowel III or lul is 
more closed than the first. Where GA speakers use the diphthong lou/, RP 
speakers have the diphthong I~u/, which starts higher and more central than the 
corresponding diphthong in GA. Only RP has the diphthongs lu~/, le~1 and li~/, 
which are referred to as centering diphthongs because they end in the central 
vowel I~/. Their occurrence in RP has to do with the loss of ill in syllable-final 
position in this variety, as will be explained in section 3.2.1 below. Triphthongs 
such as laI~1 only occur in RP. As with the centering diphthongs, this has to do 
with the loss of ill in syllable-final position in the history of Standard British 
English. According to Table 3.3, GA has the diphthong leI/, which occurs in 
words like fate. Many GA speakers, however, tend to produce these words with 
the monophthong lei as in [fet]. Similarly, some speakers produce words like 
bode with the monophthong 101 as in [bod], whereas others will pronounce these 
words with a diphthong loul as in [boud]. 
Listen to the vowels ofRP (03_vowelsRP.wav) 
and GA (03_vowelsGA.wav) on the CD-ROM. 

English phonology: phonemes, syllables and words 
63 
Table 3.3. The vowels oJ RP and GA and their transcription. 
Phonetic symbol 
RP 
GA 
key 
key 
u 
moon 
moon 
sit 
sit 
u 
good 
good 
e 
bed 
f 
bed 
3 
bird 
bird 
:} 
qrise 
qrise 
::l 
caught (caught) 
re 
bad 
bad 
A 
cut 
cut 
0 
hot 
u 
laugh 
hot, 
(caught) 
eI 
Jate 
Jate 
au 
mouth 
mouth 
:}u 
bode 
ou 
bode 
aI 
by 
by 
::lI 
boy 
boy 
u:} 
poor 
e:} 
there 
i:} 
here 
aI:} 
fire 
eI:} 
player 
au:} 
Eower 
Table 3.3 shows the vowel inventories of both RP and GA. However, the fact 
that RP and GA both have very similar vowel inventories does not mean that 
they use the same vowels in the same words. In fact, as you know, RP speakers 
pronounce laugh Iluf/, whereas GA speakers pronounce the word liref/. 
Similarly, GA speakers pronounce the word hot !hut!, while RP speakers 
pronounce it Ihot!. These differences can be described with phonological rules. 
One rule states that RP has Ia! and GA has lrel in monosyllabic words where the 
vowel is followed by a voiceless fricative. Thus, staff, path and ask are 
pronounced with the vowel Ia! in RP and with the vowel lrel in GA. Equally, 
where RP has the vowel 10/, as in the words top, not and spot, GA speakers 
produce the vowel as Ia!. As you can see by the choice ofthe phonetic symbol in 

64 
Chapter 3 
Table 3.3, the vowel in the word bed is produced with a lower tongue position in 
GA than in RP. 
When looking at Table 3.3, you can see that some of the vowels occur in 
pairs. In both RP and GA, the vowel pairs lil and II1 as weil as Iu/ and lul have 
the same vowel height, vowel location and degree of lip rounding. In fact, the 
greatest difference between these vowels is their length: lil and Iu/ are long, 
whereas Ir! and lul are short. Furthermore, the long member of the pair can occur 
in both open and closed syllables (see section 3.2 below), while the short 
member cannot occur in open syllabies, i.e. a consonant has to follow it. The 
fact that we have minimal pairs such as beat and bit and bead and bid in RP and 
GA shows that vowel length is phonemic in English. The monophthongs of RP 
and GA can be divided into long and short ones (see Table 3.4). Many textbooks 
suggest using the diacritic : to indicate phonemic vowel length, although this is 
redundant since the phonetic symbols themselves distinguish these vowels 
clearly. 
Table 3.4. Phanemically lang and shart vawels in RP and GA. 
RP 
GA 
-
long vowel 
short vowels long vowels 
short vowels 
1: 
I 
1: 
u: 
u 
u: 
u 
;J: 
A 
;J: 
A 
a: 
re 
a: 
re 
3: 
g 
3: 
g 
e 
E 
D 
The terms 'long' and 'short' are phonological terms and do not always correlate 
with phonetic reality. In actual speech, vowel length varies with the type of 
consonant following it and the type of syllable in which the vowel occurs. 
V oiced consonants trigger greater length in preceding vowels, voiceless 
consonants trigger shorter length in preceding vowels. For example, the 111 
followed by the voiceless plosive Itl in bit is always shorter than the Ir! followed 
by the voiced plosive Idl in bid. Equally, the lil in bead is longer than the lil in 
beat. In addition, vowels are longer before sonorants than before obstruents. 
This means that the 131 in girl is longer than the 131 in gird in RP. Finally, vowels 
are longer in syllables that end in vowels - such as bee - than in syllabi es that 
end in consonants - such as beat. Thus, phonemically 'long' vowels can actually 

English phonology: phonemes, syllabI es and words 
65 
be shorter than phonemically 'short' vowels when their real duration is 
measured, 
The pronunciation of words is never stable in languages, and the phoneme 
inventory of a language can change drastically in the course of a few 
generations. English is an excellent example of this, as you will find when you 
have a look at some diachronie descriptions of English phonology. The vowel 
inventory of languages seems much more susceptible to changes than the 
consonant inventory, and again, English is no exception. Current changes in RP 
and GA are weil documented (e.g. Upton 2004, Hawkins & Midgley 2005, 
Gordon 2001, 
Labov 
et 
al. 
1972). 
Processes 
in 
RP 
include 
the 
monophthongization of the diphthongs I'Jg/, lugl and IIg/, which are 
increasingly produced as monophthongs now (see Table 3.5). Similarly, the 
triphthongs largl and laugl are nowadays often realized as the monophthong la:1 
in RP. Conversely, many young speakers now produce the monophthongs Ii/ and 
lul in words like geese and goose as diphthongs - there is a slight onglide before 
the vowel. Finally, the vowel lrel in words like man is increasingly produced as 
an Ia!. 
Table 3.5. Some ongoing vowel changes in RP. 
Old vowel 
New vowel 
Example word 
IJgI 
~ h:1 
door 
lugl 
~ IJ:I 
poor 
IIgl 
~ li:1 
near 
largl 
~ la:1 
fire 
laugl 
~ la:1 
power 
li:1 
~ lIil 
geese 
lu:1 
~ luul 
goose 
lrel 
~ Ia! 
man 
In the U.S.A, a 'Northem Cities Shift' has been observed, which concems the 
pronunciation of some vowels by speakers living in cities in the Great Lakes 
Region (e.g. Chicago, Detroit and Buffalo). Table 3.6 illustrates that many 
speakers pronounce bet with the vowel Igl instead of lEI now. Cut is pronounced 
with an I'JI rather than an IAI and words that used to be pronounced with an I'JI, 
such as caught now tend to have an 101. Furthermore, speakers increasingly 
pronounce words like cot with an lrel rather than an 101, whereas words that used 
to have an lrel - like 
cat - are now rather pronounced with a centering 
diphthong Irg/. Finally, the Irl in words like kit can now often be heard as an Ig/. 

66 
Chapter 3 
Table 3.6. Vowel changes in the Northern eities Shift. 
Old vowel 
New vowel 
Example word 
lEI 
--j. 1.,,1 
bet 
IA! 
--j. I:JI 
cut 
I'JI 
--j. 10/ 
caught 
10/ 
--j. lrel 
cot 
lrel 
--j. Ir.,,1 
cat 
111 
--j. 1.,,1 
kit 
lust like the consonants, the vowels of RP and GA show allophonic variation. 
When followed by In!, Iml or 11]/, vowels are nasalized, which means that at the 
beginning ofthe articulation ofthe vowel the velum is stilliowered and some of 
the airstream passes through the nasal cavity. Nasalization of vowels is 
transcribed with the diacritic ~ (see Figure 3.3), so that a nasalized liI is 
represented as [1]. Rule (3) expresses this allophonic rule in an abstract way: all 
vowels (V) are nasalized when followed by a nasal consonant. 
~ 
{/ml 
(3) NI --j.lVJ I _ 
In! 
11]1 
Other systems for the transcription of speech sounds apart from the IP A have 
been suggested (e.g. by Gimson in 1962), and there is a tradition in Northem 
America to replace some IPA symbols with other symbols. Table 3.7 illustrates 
the differences between the IPA and the Northem American tradition. 
Table 3.7. DifJerences between the IP A and the Northern American tradition 0/ 
transcription symbols. 
IPA 
Northem American 
tradition 
S 
S 
3 
Z 
tS 
c 
d3 
aI, 'JI, er 
aj, oj, ej 
au,ou 
aw,ow 

English phonology: phonemes, syllables and words 
67 
In this book, only the IP A will be used since it is the most widely used 
phonetic alphabet and the one that is employed in most dictionaries. Note that 
although the symbols are usually called phonetic symbols they represent 
phonemes. 
3.1.4 Phonemic and phonetic transcription 
With the help of the symbols provided by the IP A, transcriptions of sounds, 
words and utterances can be made. For anyone concemed with the structure and 
realization of sounds, be it linguists, language teachers, speech therapists or 
people working in speech technology, transcription is an essential too1. This is 
due to the fact that the Roman alphabet, which is used as the conventional 
spelling system for English, has a poor grapheme-to-phoneme relationship 
(see also chapter 1). Sounds and corresponding writing symbols (graphemes) are 
not always matched in a systematic way. The same sound in English can be 
represented by different letters: for example, lil can be spelled <e> in me, <ee> 
in see, <ea> in sea, <ie> in briefand so forth (the pointed brackets < > are used 
to indicate spelling). Conversely, the same spelling may refer to different 
sounds: <th> in think corresponds to the phoneme 18/, whereas it corresponds to 
the phoneme 101 in this. Furthermore, there are 'silent' letters in English that are 
not pronounced at all, as for example in I2.sychology, lamf2. and timg,. Thus, a 
separate spelling system, a phonetic alphabet in which each symbol corresponds 
to one and only one phoneme, is necessary to describe the pronunciation of 
English words in a precise manner. 
Two types of transcription can be differentiated: phonemic and phonetic 
transcriptions. In a phonemic transcription the presumed underlying 
representations of sounds (i.e. the speaker's 'mental images' of sounds), the 
phonemes, are transcribed. It represents all and only the linguistically relevant 
information about articulation and is the kind of transcription given in 
dictionaries. The phonemic transcription of the word pin is thus IpIn/. Phonemic 
transcriptions can be carried out for written texts. For example, the sentence "A 
tiger and a mouse were walking in a field." can be transcribed phonemically as 
in example (4). 
(4) I::J taIg::J ::Jnd ::J maus W::J W::>kIl) In ::J fildl 
For GA, the transcription would be I::J taIg::Jl ::Jnd ::J maus W::Jl wakIl) In ::J fild/ 
with the difference lying in the last sound of the words tiger and were as well as 
the vowel in the word walking. (A note on the usage of the symbol IlI: very 
often in descriptions of English the symbol Irl is used when referring to the 
alveolar approximant, although this is the IPA symbol for the alveolar trill (the 

68 
Chapter 3 
'rolled r'). This widespread confusion has many reasons, amongst them the fact 
that it is easier to use the Irl symbol in handwriting and on a type-writer. This 
has to be borne in mind when reading or making transcriptions. In this book, the 
correct IP A symbol 111 will be used throughout to refer to the alveolar 
approximant. ) 
Phonemic transcriptions can also be made for real speech. Listen to 
recording 03 _transcription.wav on the CD-ROM. A phonemic transcription of 
the utterance "A tiger and a mouse were walking in a field" produced by this RP 
speaker is illustrated in (5). 
(5) I'J talg'J 'Jnd 'J maus W'J W;)krlJIll 'J fild/ 
Note that the spaces between the words are only inserted here to make the 
transcription more reader-friendly. In spoken language, there are obviously no 
pauses between the last sound of one word and the first one of the next. No 
differences from the phonemic transcription of the written sentence in (4) exist 
since transcription (5) too gives the 'ideal' or typical pronunciations of each 
word as they can be found in a dictionary. 
When you are interested in fine-grained articulatory details of an utterance, 
for example because you would like to inform a language learner of his or her 
phoneme realizations that are not native-like, you can carry out a phonetic 
transcription. 
This type of transcription includes information about 
phonologically not relevant phonetic details - for example, it might specity 
whether the Ipl in pin is aspirated or not. Phonetic transcriptions do not refer to 
speakers' mental representations but to actual realizations and can therefore only 
be carried out for real speech. Depending on the amount of phonetic information 
included in the transcription, the phonetic transcription can be broad (including 
only a few details) or narrow (with an exact transcription of all phonetic 
details). Apart from the IPA symbols for the phonemes, a phonetic transcription 
includes diacritics to indicate phonetic realizations (see Figure 3.3). Phonetic 
transcriptions are far more difficult to make than phonemic ones and require a 
lot oftraining. A fairly broad phonetic transcription ofthe utterance "A tiger and 
a mouse were walking in a field" produced by the RP speaker in 
03_transcription.wav on the CD-ROM is (6). 
Several differences from the phonemic transcription can be noted. First, it is 
difficult to determine word boundaries. Does the 'Iinking-r' (see section 3.2.1) 
belong to tiger or and, and does she say in a or rather innafield? For this reason, 
no blank spaces are given in this phonetic transcription. Second, the speaker 
produces a [1] linking the words tiger and and. Third, she produces [mas] for 

English phonology: phonemes, syllables and words 
69 
mouse. F ourth, the last phoneme of the word walking is a [n] rather than a [I]]. 
Fifth, the vowel in field is a distinct diphthong which is transcribed as [ia]. 
Moreover, this transcription shows that the first sound of tiger is realized as an 
aspirated [th], that the vowel in mouse is a nasalized [ä] and that the /1/ infield is 
velarized. A narrow phonetic transcription of the same utterance would include 
many more phonetic details such as the voicing of each segment 01' the exact 
place of articulation for each sound. 
Table 3.8. IPA symbols and corresponding SAMPA symbols. 
IPA 
SAMPA 
IPA 
SAMPA 
vowels 
consonants 
i 
p 
P 
I 
b 
b 
e 
e 
t 
t 
E 
E 
d 
d 
a: 
{ 
k 
k 
a 
a 
g 
g 
u 
u 
f 
f 
u 
U 
v 
V 
:l 
0 
8 
T 
D 
Q 
ö 
D 
Q 
A 
s 
s 
a 
@ 
z 
z 
3 
3 
S 
S 
A 
V 
3 
Z 
h 
h 
m 
m 
n 
n 
I] 
N 
I 
I 
1 
1'\ 
j 
w 
w 
Nowadays, few transcriptions are carried out by hand 01' on paper. Typically, 
especially when transcribing large amounts of speech, phonologists use a 
computer. Since it is tedious to use the special IP A fonts in text editors and since 
an exchange of data is difficult when everyone uses different fonts, a machine-

70 
Chapter 3 
readable version of the IPA has been invented, which comprises only symbols 
that can be found on any computer keyboard. This alphabet is called SAMP A 
(which stands for Speech Assessment Methods Phonetic Alphabet) and was 
introduced in 1992 by John WeHs and other phoneticians (WeHs et al. 1992). 
Table 3.8 lists the IPA symbols relevant for English and their corresponding 
SAMP A 
symbols. 
A 
phonemic 
SAMP A 
transcription 
of utterance 
03 _transcription.wav on the CD-ROM would thus look like in (7): 
(7) I@ taIg@ @nd @ maUs w@ wOkIN In @ fild/ 
3.1.5 Phonetic features (advanced reading) 
So far we have assumed that phonemes are the smallest units of a speaker's 
mental representation and the linguistic analysis of speech. In this section, 
several approaches in phonological theory will be reviewed that claim that 
phonemes consist of even sm aller units. These approaches are motivated by the 
observation that the phonemes of a language can form groups. For example, 
those phonemes that involve a complete closure of the oral cavity (e.g. Im, n, 
1], p, t, k, b, d, g/) can be separated from those that have no such complete 
closure. The first group is referred to as non-continuants, the second as 
continuants. Equally, in section 2.4 we saw that some consonants are produced 
with a fairly continuous airstream, whereas other consonants are produced with 
an obstruction of the airstream. The first group of phonemes, which comprises 
Im, n, 1], 1, j, w, JI as weil as all vowels, are called sonorants, whereas the other 
group of phonemes including Ip, t, k, b, d, g, f, v, e, ö, s, z, S, 3, hl are called 
obstruents. Note that this class of phonemes comprises as diverse manners of 
articulation as plosives and fricatives. 
The existence of such natural c1asses of phonemes can be further illustrated 
with rule (8) introduced in section 3.1.2. It illustrates that the three phonemes Ip, 
t, k/ are aspirated in a particular context: 
(8) 
IP/} 
It! 
Ik/ 
In order to explain why it is only these three phonemes to which the 
phonological rule applies but not other phonemes of English, one has to refer to 
the features these phonemes share but others do not. Ip, t, k/ have in common 
that they are consonantal, that they are non-continuants, non-sonorants and 
voiceless. In phonology, phonetic or phonological features are conceptualised 
as binary, which means that phonemes have one oftwo possible values: (+) or 

English phonology: phonemes, syllables and words 
71 
(-). Asound is either voiced or voiceless, either continuant or non-continuant -
there are no other options. The class of the English voiceless plosives Ip, t, k/ 
can thus be represented as [+consonantal], [-continuant], [-sonorant] and 
[-voice] (the convention is to use square brackets when referring to features). 
By the same token, all voiced phonemes in English share the feature [+voice], 
whereas aB voiceless phonemes have the feature [-voice] in common. 
Features are abstract, theoretical concepts, but they are assumed to have a 
phonetic basis. Many different sets of features have been proposed by different 
researchers, beginning with the classic work by lakobson & Halle (1956) and 
Chomsky & Halle (1968). Some of the features refer to articulatory aspects such 
as the feature [labial], which describes that the lips are involved in the 
production of a phoneme. The places and manners of consonant articulation 
listed in section 2.4 in Tables 2.2 and 2.3, for example, are articulatory features. 
Other features that have been proposed are perceptual or acoustic in nature, such 
as the feature [low], which describes the property of some vowels in acoustic 
(physical properties of the sound wave, see section 5.2 below) rather than 
articulatory terms. In order to be of any scientific value, phonetic features have 
to be contrastive 01' distinctive, i.e. they need to be able to differentiate between 
individual phonemes and individual phoneme classes. 
Phonologists do not only use features in order to describe the phonological 
behaviour of certain classes of phonemes. It is also assumed that phonemes are 
represented in a speaker's brain as a bundle of these features. The mental 
representation of ItI, for example, can be described to be specified as [-voice] 
which means that it does not have the feature voice, whereas Idl is specified as 
[+voice]. The properties ofphonemes can be illustrated in a feature matrix. 
p 
n 
-voice +voice 
+voice 
+labial -labial -labial 
+stop 
-stop 
-stop 
-nasal -nasal 
+nasal 
-high 
+high 
-high 
Figure 3.5. Feature matrix afthe ward pin. 
The feature matrix ofthe three phonemes ofthe wordpin is given in Figure 3.5. 
Five features are sufficient to describe adequately the difference between the 
three phonemes: the features [±voice], [±labial], [±stop], [±nasal]and [±high]. 
The Ipl has the features [-voice], [+labial], [+stop], [-nasal] and [-high]; the 

72 
Chapter 3 
phoneme 1I1 has the features [+voice], [-labial], [-stop], [-nasal] and [+high], 
whereas In! has the features [+voice], [-labial], [-stop], [+nasal] and [-high]. 
Recent approaches to describing phonetic features divide them into 
-
major-class features 
-
laryngeal features 
-
manner features 
-
place features 
The major-dass features are used to distinguish between the major classes of 
speech sounds: vowels and consonants, sonorants and obstruents (see also 
section 2.4 above). The major-class features are [±consonantal], [±sonorant] and 
[±syllabic]. The feature [±consonantal] distinguishes phonemes with a 
constriction in the vocal tract (oral plosives, fricatives, nasals and the Iiquids 1I1 
and IJI) from the vowels and the glides Iwl and Ij/, which are not articulated with 
a constriction in the vocal tract. The feature [±sonorant] distinguishes obstruents 
such as Itl from sonorant consonants such as 1I1 and vowels. [±syllabic] 
describes phonemes that can function as the nucleus of a syllable - which are 
mainly vowels (see section 3.2 below). The only laryngeal feature that plays a 
role in English is [±voice]. Phonemes that are produced with vocal fold 
vibration such as Iml and Ia! have the feature [+voice], whereas all phonemes 
produced without vocal fold vibration, such as Ifl and /k/, share the feature 
[-voice]. 
The manner features comprise [±continuant], [±nasal], [±strident] and 
[±Iateral]. As explained above, the continuants are articulated with a free flow of 
airstream through the oral cavity, which means that an Ia! is [+continuant] while 
a Iml is [-continuant]. Phonemes that are [+nasal] are produced with a lowered 
velum, whereas [-nasal] phonemes are not. Phonemes that have the feature 
[+strident] are obstruents whose articulation involves a noisy kind of friction as 
in the case of the phonemes Isl and IS!. In English, for example, this class of 
phonemes determines the choice of the allomorphs of the plural {s} and the 
third person singular {s}. When the stern of a word ends in a strident sound, the 
allomorph is [IZ], as in bushes and misses. In other cases, the morpheme is 
pronounced either [z] (following voiced phonemes except stridents) or [s] (after 
voiceless phonemes except stridents). Phonemes sharing the feature [+lateral] 
are produced with tongue contact in the oral cavity and the airstrealll passing at 
both sides of the tongue. 
There are monovalent and binary pi ace features. Monovalent features 
describe the pI aces of articulation [LABIAL], [CORONAL] and [DORSAL]. The 
binary features specify these places further. [LABIAL] phonemes such as Im, v, bl 
are articulated with the lips. In addition, they can be specified for [±round]. 
English phonemes that have lip rounding are for example lul and 101; phonemes 

English phonology: phonemes, syllabi es and words 
73 
that have no lip-rounding are lil and lei, Phonemes whose mental representation 
includes the feature [CORONAL] are articulated with the tip, blade or front ofthe 
tongue (see Figure 2,9 in chapter 2 above), This applies to a wide range of 
English phonemes such as It, 8, s, S, 1, j/. These phonemes can be further 
specified as [±anterior] and [±distributed]. All phonemes specified as [+anterior] 
are articulated at the alveolar ridge or further forward (this includes labials, 
labiodentals, interdentals and alveolars) as for example Izl. Phonemes that are 
[-anterior] are articulated behind the alveolar ridge as for example 13/. 
Phonemes that are articulated with a constriction that concems a relatively large 
part of the vocal tract are [+distributed], whereas phonemes articulated with a 
constriction that concems only a sm all area of the vocal tract are specified as 
[-distributed]. Because the articulation of /J! involves a constriction with the 
blade of the tongue along the post-alveolar area and the palate, it is specified as 
[+distributed]. ItI, which only involves the tip ofthe tongue at the alveolar ridge, 
by contrast, is specified as [-distributed]. 
The articulation of phonemes that are [DORSAL] involves the dorsum, the 
back ofthe tongue (see Figure 2.9), and includes /k, g, 1]/ and all vowels. These 
phonemes can be further specified as [±high], [±low], [±back] and [±tense]. 
[+high] phonemes such as li, I, u, ul are articulated with the dorsum close to the 
palate. je, 0, al are correspondingly specified as [-high]. The phonemes 
specified as [+low] such as la, 0, 01 have a bunched dorsum in a low position in 
the mouth; [-low] phonemes do not. [+back] phonemes such as 10, 0, 0, k/ are 
articulated with a high dorsum in the centre of the mouth or further back. 
[+tense] vowels are produced with a more peripheral tongue position, as is the 
case for /i, e, a, u/. This binary feature only exists in languages that have vocalic 
oppositions such as the ones in English described in section 3.1.2 above. This is 
why the long vowels listed in Table 3.4 are sometimes referred to as tense 
vowels, whereas the short vowels are called lax vowels. 
When we assume that the phonemes of English are represented in the minds 
of English speakers as bundles of features, it becomes obvious that some of the 
information is redundant. For example, vowels specified as [-high] are also 
characterized as [+low], which is like saying the same thing in two different 
ways. Equally, all sounds specified as [+sonorants] in English are also [+voice]. 
This has led some phonologists to suggest that only distinctive but no redundant 
features form part of a speaker's linguistic knowledge, an approach that is 
known as underspecification theory. This theory claims that in the underlying 
mental representation of phonemes those features whose values are predictable 
are omitted. For example, all front vowels in English are umounded, so that the 
feature [-round] can be predicted from the feature [-back] in these cases. These 
phonemes therefore do not need to be specified for the feature [±round]. The 
concept of underspecification has been used to explain the difficulties some 
leamers of a second language have with certain phonemes: for example, the 

74 
Chapter 3 
claim has been made that when the native language does not have a specified 
mental representation for some features, it will be difficult to acquire phonemes 
in a second language that require this feature distinction. The lack of 
specification of front vowels for the feature [±round] might explain why English 
speakers find the pronunciation of Iy/, the high rounded front vowel in German 
Tür and French tu, particularly difficult. 
MAJORCLASS 
/ 
±consonantal ~ 
±sonorant 
±syllabic 
LARYNGEAL 
±voice 
SUPRALARYNGEAL 
I 
MANNER 
±continuant 
±nasal 
±strident 
±lateral 
PLACE 
LABIAL ~DORSAL 
±round 
±anterior 
±high 
±distributed 
±low 
±back 
±tense 
Figure 3.6. Afeature tree. 
Another way of representing the internal structure of phonemes, apart from 
drawing a feature matrix, is that of drawing feature trees. This approach is called 
feature geometry and is based on work by Clements (1985) and McCarthy 
(1988). It suggests that the different features a phoneme comprises are 
hierarchically ordered. Taking the feature system proposed above, a feature tree 
would look like the one depicted in Figure 3.6. Each feature grouping in the 
feature tree is called anode (e.g. LARYNGEAL is anode). These nodes form 
natural classes of phonemes. Beneath each node those features are grouped that 
make up the subclasses (e.g. ±voice for the node LARYNGEAL). Nodes and 
features are ranked on tiers (the different levels of the hierarchy), which 

English phonology: phonemes, syllables and words 
75 
illustrate their hierarchical relationship. The features of each phoneme of 
English (and any other language, of course) can be specified with the help of 
such a feature tree. 
3.2 The English syllable 
Most adult speakers of English agree that the word dog consists of one syllable 
and the word trisyllabic of four syllabIes. Equally, most phonologists agree that 
phonemes can be grouped into the higher unit of the syllable (see e.g. Blevins 
1995). They propose the syllable both as a unit far linguistic analysis and a unit 
that speakers have amental representation of in their brains. This conviction is 
based on the fact that a number of phonological processes have been identified 
that can best be explained with the unit of a syllable. The distribution of stress in 
English words, for example, depends on the type of syllables involved (see 
section 3.2.3 below). Moreover, the motor patterns underlying articulation seem 
to comprise syllables rather than individual speech sounds, as explained in 
section 2.5. Similarly, speech errors such as sayingface spood instead of space 
food can be neatly explained with reference to a particular part of the syllable: 
the substitution of the speech sounds is not random, but concerns the onset 
position of each syllable. 
The internal structure of a syllable that is thought to be shared by all 
languages in the world is illustrated in Figure 3.7. A syllable - which is 
represented by the symbol (J - consists of an onset and a rhyme, which in turn 
can be divided into a nu deus and a coda. 
onset 
rhyme 
A 
nuc1eus 
coda 
Figure 3.7. Universal structure of a syllable. 
Languages vary as to which type of speech sounds are allowed to occur in which 
position of the syllable. In English, only consonants (C) can appear in the onset 
and the coda position, whereas the nuc1eus position is nearly always filled by a 
vowel (V). The syllable dog thus consists of /d/ in the onset position, /0/ as the 
nuc1eus and /g/ in the coda position. In English, all parts of a syllable can be 

76 
Chapter 3 
filled with more than one speech sound. Figure 3.8 illustrates the internal 
structure of the syllable sprint. It consists of the three consonants Is/, Ipl and 111 
in the onset position, the nucleus 111 and the consonants In! and Itl in the coda 
position. 
onset 
rhyme 
ffi nu,~ coda 
I 
1\ 
c c c 
V 
C 
C 
I s 
P 
1 
n 
t I 
Figure 3.8. The internal structure ofthe syllable sprint. 
In English syllabies, only the nucleus is compulsory, that is, every syllable must 
have a nucleus. Syllable onsets and codas are optional in English: syllables may 
or may not have an onset and a coda. The syllable on does not have an onset, 
whereas tea is a syllable without a coda. The shortest syllables in English thus 
consist of only a vowel as a nucleus, for example I, a, oh. This vowel has to be 
either phonemically long or a diphthong, as will be explained in the next section. 
In some special cases, a consonant can function as the syllable nucleus in a 
word. This is for example the case in the second syllable ofthe word button. The 
word consists ofthe two syllables IbA.tQl (the dot indicates a syllable boundary, 
the stroke under the Inl is the diacritic symbol for a syllabic consonant), with the 
first syllable comprising the onset Ibl and the nucleus IA! and the second syllable 
comprising the onset Itl and the nucleus In!. Only nasals and liquids (In, m, 1), I, 
1/) can function as the syllable nucleus - they are called syllabic consonants. 
Syllables with a syllabic consonant as the nucleus cannot stand on their own in 
English, but always belong to a word that has another syllable with a vocalic 
nucleus. 
3.2.1 Types of syllables and phonotactic rules of English 
Around twenty different types of syllables occur in English. There are, for 
example, CV syllables such as do and go, which consist of one consonant in the 
onset position and one vowel in the nucleus position. Further, there are VC 

English phonology: phonemes, syUables and words 
77 
syllables such as in or ofwith a vowel as nucleus and one consonant in the coda 
position. Other types incIude CVC (dog) , CCV (spa), CCVC (tram), CCCVC 
(split), and so forth. Syllables that have no consonant in the coda position are 
called open syllabies. Closed syllables have at least one consonant in the coda 
position. Sequences of two or more consonants in the syllable onset and coda, 
such as IspJI in the onset and Intl in the coda of the word sprint, are referred to 
as initial and final consonant clusters, respectively. Syllables that have a rhyme 
consisting of a lang vowel (e.g. bee), a diphthong (e.g. bay) or a short vowel 
plus consonant (e.g. beg) are called heavy syllabies. Long vowels and 
diphthongs are sometimes represented as VV so that the syllable structure of bee 
or bay can be given as CVV. Light syllabies, by contrast, have only a short 
vowel or a syllabic consonant in the rhyme (e.g. beloved). The different syllable 
types do not occur equally often in English speech. Dauer (1983) found that 
CVC (34%) and CV (30%) syllables are most frequent, followed by VC (15%), 
V (8%) and CVCC (6%). Gut (2005) found that 43% of all syllables in English 
are open syllabies. Furthermore, their distribution is constrained: stressed 
syllabi es are always heavy, light syllables are always unstressed (see also 
section 3.2.3), and light syllables cannot occur on their own. 
The number and type of consonants that can occur in both onset and coda 
position in English is restricted. The number of consonants in the onset position 
is Iimited to three - there are no English syllables with more than three 
consonants in the onset position. The maximum number of consonants in the 
coda position is limited to fouf. If there are four coda consonants, the last one is 
either an inflectional Isl as e.g. in texts [teksts] or an inflectional Itl as in 
glimpsed [girmpst]. Furthermore, the type of consonants that can occur in 
English syllable onsets and codas and their order is restricted, which can be 
formulated in phonotactic rules. There are two types ofphonotactic rules: 
-
rules of sound distribution 
-
rules of sound combinations 
Rules of distribution describe in which position of the syllable phonemes can 
occur. For example, not any kind of vowel can appear in an English syllable 
without onset and coda: it has to be a long vowel or a diphthong. This is why 
when the indefinite article a is pronounced on its own it is pronounced leI/. 
(Only in combination with other words is it pronounced l'dl, where it is often 
considered part of a phonological word; see section 3.3 below). Other 
phonotactic constraints concern the phoneme IIJ/, which never occurs in the 
onset of English syllabies. Some phonemes of English never occur in the coda 
position: these are IjI, Iwl and /h/. In RP, in contrast to GA, IJI does not occur in 
the syllable coda: syllables like car and bar in barman are pronounced [ku:] and 

78 
Chapter 3 
[ba:]. When phonemes cannot occur in all positions of the syllable, linguists 
speak of defective distribution. 
Varieties of English that do not pronounce coda 111 are called non-rhotic 
varieties (or accents), whereas those varieties that allow coda 111 are called 
rhotic accents. This phonological difference has historical reasons. Until the 
Early Modem English period English was a rhotic language, which is still 
reflected in the present-day spelling of words like car and barman. This rhotic 
accent was spoken by the settlers in the early colonies (e.g. in Northem 
America). During the 1 i h and 18th centuries, Standard English on the British 
Isles evolved into a non-rhotic accent and, consequently, many of the settlers of 
later colonies such as Australia, Singapore and Nigeria, which were founded at 
that time, spoke non-rhotic English. Present-day Australian English, Singapore 
English and Nigerian English are still non-rhotic accents. There is one 
phonological instance when coda 111 is pronounced even in non-rhotic accents: 
when a syllable ending in the spelling <r> is followed by a syllable without 
onset consonant, the 111 is pronounced. This process, which can be observed in 
pairing IpeJIl)1 and far away IfaJ;;,weI/, is called the 'Iinking Ir!' and reflects 
resyllabification, a phonological process that will be explained in section 3.2.2 
below. When speakers of a non-rhotic accent pronounce a 111 between two 
vowels although there is no spelled <r>, the result is called 'intrusive Ir/'. It can 
be observed in phrases such as law and order, which is pronounced Il:l:J;;,nd::l:d;;,1 
by some RP speakers. It is hypothesized that speakers produce this inserted 
(epenthetic) IJI to avoid a hiatus, i.e. the separate pronunciation of two adjacent 
vowels. 
Phonotactic constraints of distribution describe the order of consonants in 
English syllable onsets and codas. In English onsets consisting of a three-
consonant cluster, the first phoneme is always a Is/, the second either Ip/, Itl or 
Ik/ and the third 111, Ij/ or - in some cases - 11/ or Iw/. Thus, we have in English 
splay, spray, stray, squeal and stew (in RP only), but not *pstay or *wpjay 
(impossible forms are marked by the preceding asterisk *). This rule is 
illustrated in (9). GA does not allow many onset consonant clusters involving Ij/, 
for example stew is pronounced Istu/. 
(9) 
{
~) 
(w) 
j 

English phonology: phonemes, syllables and words 
79 
Table 3.9. Permissible two-consonant anset clusters in RP and GA. 
RP+GA RP only Example 
Ipl/ 
plan 
IPll 
pram 
Ipj/ 
pure 
Ibl/ 
blue 
Ibll 
brew 
Ibj/ 
beauty 
Ifll 
jlew 
Ifll 
friend 
Ifjl 
few 
Ivj/ 
view 
lai 
tram 
Itwl 
twist 
Itjl 
tune 
IdJl 
drink 
Idwl 
dweil 
Idj/ 
duel 
lerl 
throw 
lewl 
thwart 
Isml 
smell 
Isnl 
snake 
Isl/ 
sling 
Iswl 
swing 
Isj/ 
suit 
Izjl 
Zeus 
IN 
shrink 
Ikl/ 
claw 
IkJI 
crawl 
Ikwl 
quest 
Ikjl 
cute 
Igl/ 
glow 
IgJI 
graw 
Igwl 
Gwen 
Igjl 
gules 
Ihjl 
huge 
Imj/ 
mule 
/njl 
new 
Iljl 
lute 

80 
Chapter 3 
Similarly, not all possible combinations of two consonants in the onset can 
occur in English. In general, two consonants that share a place of articulation 
cannot occur together. Thus, while Ip, k, b, g, f, si can combine with 111 as in 
play, clay, bleak, glimmer,jlat and slip, */tll and */dll do not exist as onsets in 
English. Obstruents (plosives and fricatives) cannot combine with nasals (thus 
no */bm/or */fll/), and nasals cannot combine with the liquids 1I1 and 111. Some 
phonemes such as 101, IzJ, IJI, Iwl and Ij/ cannot appear in the first position of 
onset clusters at all. 
Table 3.9 lists all permissible two-consonant clusters in contemporary RP 
and GA. Some of the clusters like Igw/, Igj/, Iljl and 18wl are in fact very rare, 
and only a handful of words exist for each of them. You can see that GA lacks 
some two-consonant onset clusters, in particular, Itj/ as in tuna, Idj/ as in duel, 
Isj/ as in suit, IzJI as in Zeus, Injl as in new and Ilj/ as in lute, which occur only in 
RP. The cluster Ilj/ seems to be dying out in RP; only few speakers still use it in 
words like lute (/lju:tI). Instead, most RP speakers pronounce this word Ilu:tI 
now. Some words in English have the onset clusters ISwl (sehvartze), ISlI 
(sehlep), ISml (shmoo) and ISn/ (sehnapps), but since all ofthem occur only in 
loan words, these onsets are not considered generally permissible in English. 
Similar to English syllable onsets, the order of consonants in English 
syllable codas is restricted by phonotactic rules. Four-consonant coda clusters 
such as in texts (/ksts/) and glimpsed (lmpst/) end either in Isl or in ItI, which 
have morphemic status. This means that they, for example, consist of the 
inflectional plural morpheme {s} or the past tense morpheme {ed}. The first 
consonant ofthree-consonant coda clusters such as in elms, sprints andfriends is 
always a liquid or a nasal, i.e. either a 11/, Im/, Inl or Irjl in RP or further a 111 in 
GA (as in earts). The second consonant in such clusters is one ofthe obstruents 
Ip, k, t, d, f, 8, si or Im/. Three-consonant coda clusters end in either ItI, Is/, IzJ or 
181 as in gulped IgAlptl, links IhIJks/, friends RP: Iflendzl GA: IflmdzJ and 
twelfth RP: Itwelffil GA: Itwflffi/. As with the onset clusters, not all phonemes 
can combine to form two-consonant clusters in English syllable codas. For 
example, the second consonant of final two-consonant clusters can be any 
consonant except /h, w, j/ and IJI in RP. 
The phonotactic constraints that apply to English syllables have undergone 
some changes in the history of English. In Old English, onset clusters such as 
Ihl/, Ihw/, Ihrl, Iwl/, Iwrl, Ikn/ and Ign/ occurred in many words, but they were 
lost by or during the Middle English period in most English dialects. Table 3.7 
would thus look a bit different if it illustrated the permissible onset clusters of 
10th century or 15th century English. 
Is there any reason why the order of consonants is restricted in such a way? 
The phonotactic structure of syllables is often described with reference to the 
sonority of the phonemes involved. The sonority of a phoneme Is defined in 
acoustic and perceptual terms: it refers to the relative loudness of one phoneme 

English phonology: phonemes, syllabi es and words 
81 
compared to other phonemes. The sonority of all phonemes of English can be 
depicted on a sonority scale (Figure 3.9). It can be seen that vowels have the 
highest sonority of all phonemes in English, with low vowels being even more 
sonorous than high vowels. The approximants /j/ and /w/ have the closest 
sonority values to that of the vowels. Voiceless plosives are the least sonorous 
phonemes. Compared to them, the sonority increases with the voiced plosives, 
followed by the voiceless and the voiced fricatives, the nasals and the liquids. 
p,t,k 
b,d,g 
f,e,s,I 
v,O,Z,3 
m,n,l) 
1 
j,w 
i,u 
a,o 
sonority 
Figure 3.9. Some phonemes ofEnglish on the sonority seale. 
When illustrating the sonority of the phonemes of syllables such as print and 
twist in a diagram (see Figure 3.10), one can see that the syllables resemble a 
mountain with a peak. In the onset, the sonority of the first phoneme is lower 
than that of the second phoneme; the vowel in the nucleus has the highest 
sonority, whereas the first phoneme in the coda has a higher sonority value than 
the second one. The order of consonant sequences in the onset and coda position 
can therefore be explained in the following way: in English onsets, consonants 
are ordered with increasing sonority, whereas they are ordered with decreasing 
sonority in the coda position. Not all English syllabi es conform to this pattern, 
though. There is one exception to the rule in both onsets and codas, and this 
concerns the phoneme /s/. Despite having a higher sonority than the plosives, 
onsets in which a /s/ precedes a plosive, such as in /spl/ and /sU/, are permissible 
in English. In the coda position, equally, /s/ can occur in the last position after 
phonemes with a lower sonority value, e.g. in /ks/ and /ts/. This is why the /s/ is 
often treated as a special case and its position in the syllable is called 'extra-
syJlabic' or 'appendix'. 
P 1 
n 
t 
~ 
·C o 
t:: o 
[Jl 
Figure 3.10. The sonority of the phonemes in print and twist. 
W Ist 

3.2.2 
Syllabification in English 
When words consist of more than one syllable, when they are multisyllabic or 
polysyllabic, the problem of syllabification arises. Should the consonants Iksul 
in the word extreme IIksui:ml be analysed as coda consonants of the first 
syllable, as onset consonants of the second syllable or should they be split up 
somehow to fill both positions? Phonologists have proposed that these 
consonants are distributed according to the maximum onset principle. This 
principle states that intervocalic consonants are syllabified as the onset of the 
following syllable as far as the phonotactic constraints of the language allow it. 
This means that the two syllables of extreme are split up into Irk.sUi:ml as 
illustrated in Figure 3.11. Isul is syllabified as the onset of the second syllable, 
since this conforms to the phonotactic rules of onset clusters. Iksul violates these 
rules, so that Ik/ has to be syllabified as the coda ofthe first syllable. 
cr 
cr 
A 
A 
onset 
rhyme 
ons et 
rhyme 
~ 
!l\nUC~da 
nucleus coda 
I 
I 
I 
I 
V 
C 
C C 
C 
V 
C 
I r 
k 
s 
1 
ml 
Figure 3.11. The syllable structure afthe ward extreme. 
In some cases, the maximum onset rule leads to questionable results. For 
example, the words apple and epic would have to be syllabified into lre.pll and 
le.prkl (GA: IE.prk/), which violates the rule described above, namely that 
syllables have to consist of at least a long vowel or a short vowel plus 
consonant. Irel and lei are short vowels and, according to this rule, cannot stand 
on their own in a syllable. A way around this problem is to propose that the Ipl 
in both cases is an ambisyllabic consonant, i.e. that it belongs simultaneously to 
the coda of the first and the onset position of the second syllable (see Figure 
3.12). There seems to be good reason to assume that speakers indeed have a 
mental representation of ambisyllabicity for these consonants - when you ask 
native speakers to name the syllables of apple and epic, they will usually say 
appl and eppic. 

English phonology: phonemes, syliables and words 
83 
(J 
(J 
A 
A 
onset 
rhyme 
ons et 
rhyme 
~ / 
~ 
nuc1eus coda 
nuc1eus coda 
I 
"-
I 
V 
C 
C 
I re 
p 
I I 
Figure 3.12. The syllable structure ofthe word apple with /pI as an ambisyllabic 
consonant. 
In connected speech, the problem of syllabification arises also across word 
boundaries. Speakers often produce speech that contains resyllabification, 
where one or more consonants of one word are attached to the following word. 
Resyllabification also follows the maximum onset principle. This can be seen 
for example in the phrase have it, which is usually resyllabified into [hre.vIt]. 
The last consonant of the word have is produced as the onset of the second 
syllable. As mentioned in section 3.2.1 above, the 'linking Ir/' in non-rhotic 
ac cents can also be described as resyllabification. In a phrase such as Peter and 
Mary, which is pronounced Ipi:.t;u~nd.mre.1I/, the III in Peter is resyllabified as 
the onset of the following syllable. In non-rhotic accents of English, coda III is 
not pronounced. The III in phrases like Peter and Mary, however, is 
resyllabified as an onset consonant and is therefore pronounced. 
3.2.3 Stress and speech rhythm in English 
Syllables in English can be either stressed or unstressed. In the word English, 
for example, all native speakers ofRP and GA would agree that the first syllable 
is stressed, whereas the second syllable is unstressed. The phenomenon of stress 
can be explained both with reference to speech production mechanisms and to 
acoustic and perceptual properties. Sections 2.1 and 2.2 showed that at least 
three factors lead to a perceptual impression of stress. Speakers produce greater 
air pressure by increased muscular effort in the respiratory system and they 
produce higher pitch by tensioning the vocal folds on stressed syllabies. 
Moreover, the articulatory movements for the production of speech sounds in 
stressed syllabi es are carried out to a larger extent than for speech sounds in an 

84 
Chapter 3 
unstressed syllable. Section 5.5.2 explains that in acoustic terms this can be 
measured in an increased loudness or energy in the speech signal as weIl as an 
increased duration of individual segments and syllabies. The perceptual 
impression of stress is one of prominence - a stressed syllable appears more 
prominent (louder) than an unstressed syllable. Similarly, the speech sounds in 
stressed syllables have a different quality. These vowels and consonant are fully 
articulated rather than reduced. The physiological mechanisms underlying the 
production of stress determine that stress is a relative rather than an absolute 
property of syllabies. One syllable can be more or less stressed than another 
syllable, but it is very difficult to determine the degree of stress on a single 
syllable ifthis is the only one we hear. 
Speakers ofEnglish know which syllable or syllables in a word are stressed. 
They have amental representation of word stress, which will be described in 
section 3.3.1 below. Thus, stress is an abstract property of a syllable and part of 
the linguistic knowledge of a speaker. The degree of stress on a particular 
syllable, however, is also areal physical event that can be measured, as 
explained in section 5.5.2 below. In order to keep the two things -
the 
phonological representation and the phonetic event - separate, two different 
terms should be used. These terms are accent and stress. Unfortunately, 
different researchers use the two terms with the opposite meaning. Laver (1994) 
regards 'accent' as the abstract category and calls the actual physical occurrence 
'stress'. lassem & Gibbon (1980), on the other hand, call the mental 
representation 'stress' and refer to the observable manifestation of stress as 
'accent'. In this book, the term 'stress' is used to refer to the abstract property of 
syllables and the mental representation, whereas the term 'accent' is employed 
for the measurable phonetic event. Thus, a stressed syllable is the one a speaker 
knows to be stressed, whereas accents can be measured in real speech. 
Not every syllable in English can be stressed. Stress is correlated with the 
phonological properties of a syllable. In English, syllables consisting of a short 
vowel only and CV syllables with a short vowel are always unstressed. This 
means that the phonological distribution of vowels in stressed and unstressed 
syllables differs widely. While in stressed syllables all vowels listed in Table 3.3 
above except l'dl can occur, the only vowels that appear in unstressed syllables in 
both RP and GA are l'dl and 11/. This can be seen in the unstressed syllables of 
the words sofa I'S'duf'dl (GA: I'souf'd/), about l'd'bautl, police Ip'd'li:sl and family 
l'üemIlI/. (The stressed syllable is marked by a preceding '). This restricted 
occurrence of the schwa l'dl in both RP and GA has led many phonologists to 
propose that the schwa does not constitute a phoneme in its own right. Apart 
from a few minimal pairs with III, as in RP sofa (/'s'duf'dl) and Sophie (/'s'dufI/), 
l'dl does not contrast with any other vowel of English. For many speakers, 
furthermore, l'dl and III are interchangeable in pre-stress position so that extreme 
can be pronounced both l'dk'sUi:ml and IIk'sUi:m/. The rule that unstressed 

English phonology: phonemes, syllables and words 
85 
syllables can only contain the reduced vowels l'dl and 111 is a phonological 
description, In section 2.5 a phonetic explanation of this phenomenon was 
given: in unstressed syllabIes, due to the rapid articulation, the tongue height and 
horizontal position of the tongue in vowels is far removed from the ideal 
articulatory target. The tongue only briefly reaches a central position - neither 
high nor low, neither front nor back - before the articulators move on to form 
the next consonant, which means that the vowel appears 'reduced'. 
Unstressed syllables in English are often also called weak syllables (whereas 
stressed syllables are referred to as strong syllabIes). By the same token, the 
alternative pronunciations of a number of English function words are called 
weak and strong forms. Table 3.10 lists the different pronunciations of some of 
these words (for a fulllist of 40 function words see Roach 1991, chapter 12) and 
gives examples in RP. The weak form is the one that is usually used in 
connected speech. There, the processes of reduction and coarticulation described 
in section 2.5 apply and determine the articulation of the vowel as weil as the 
articulation of consonant clusters. Typically, the vowel is reduced (i.e. not fully 
articulated) and consonant clusters are reduced (i.e. one or more consonants are 
not realized at all). Only when such a function word is produced on its own or 
for purposes of contrast or when it appears at the end of an utterance do speakers 
use the strong form. Thus, in "Y ou and I" and is usually pronounced in its weak 
form ['dn] (/ju:;:mar/), whereas in "You AND I" with emphasis on and it is 
pronounced in its strong form Iju:'rendm/. 0/ appears in its weak form in "One of 
you" IWAn'dvju:1 but in the strong form IDVI in "The woman I saw a photo of" 
IO'dwum'dnars;)1'df'dut'dU'DVI (GA: IO'dwum'dnarsa:'dfoutou'a:vl). 
03 _ weakandstrongformsRP. wav on the 
CD-ROM is a recording ofthe strong and 
weak forms ofEnglish function words read 
by an RP speaker. 

86 
Chapter 3 
Table 3.10. Weak and strong forms of some function words in English (RP). 
Word 
Weak 
Example 
Strong 
Example 
form 
form 
and 
;:,ndhn/n 
jish and chips 
a:!nd 
it was me AND him 
IfIS,mtSlpsl 
IItw;:,zmi'a:!ndruml 
the 
0;:' 
open the window 
oi 
it was THE woman 
I;:,up;:,no;:,wmd;:,ul 
IItw;:,z'oi:wum;:,nl 
a 
take a look 
eI 
not A coat but THIS one 
Itelk;:,luk! 
Inot'elbutb;:,t'oIswAni 
of 
;:,v 
one ofyou 
ov 
not that I know of 
IWAn;:,vju:1 
Inoto;:,tam;:,uovl 
to 
t;:, 
time to go 
tu 
where has he gone to 
ItaImt;:,g;:,ul 
Iwe;:'l;:,zhIgontul 
from 
fl;:,m 
backfrom work 
flOm 
where did you get that fram 
Iba:!kfl;:,mw3:k! 
Iwe;:,dldjugetoa:!tflOml 
for 
RP: f;:, 
time for a break 
RP: f,,: 
what was that for 
(GA:f;:'l) 
ltaImf;:'l;:,blelk! 
(GA: fOl) 
lwotw;:,zoa:!thl 
he 
did he come 
hi 
itwas he 
IdldlkAml 
IItw;:,zhil 
his 
IZ 
take his dog 
hIZ 
that was HIS dog 
Iteda:zdogl 
10a:!tw;:,zhIZdogl 
her 
give her time 
RP:h3 
I gave it to her 
IgIv~taIlni 
GA: h3l 
I aIgelvlt;:,h31 
them 
o;:,m/;:,m 
let them go 
oem 
give it to them 
Ilet;:,mg;:,ul 
IgIvIt;:,oeml 
was 
w;:,z 
1 was there 
woz 
I don 't know who it was 
laIw<lZoe;:,1 
lald;:,untn;:,uhUltwozl 
but 
b;:,t 
small but cheap 
bAt 
I'd say so but 
Ism,,:lb~tSi:pl 
laldseIs;:,ubAtl 

English phonology: phonemes, syllables and words 
87 
The particular interplay of stressed and unstressed syllables in English is 
referred to as speech rhythm. English speech rhythm is said to be characterized 
by a regular occurrence of stress beats. Read utterance (10) and snap your 
fingers in synchrony with the stress beats occurring on the stressed syllabies: the 
first on the syllable ti, the second on the syllable mouse, the third on the syllable 
wal and the last on the syllable fleld. You will probably snap your fingers in a 
fairly regular rhythm. This is why English has traditionally been called a 'stress-
timed' language (Abercrombie 1967). The time interval between two stress 
beats in stress-timed languages is claimed to be isochronous, i.e. roughly equal 
in time. Other languages such as French, by contrast, are supposed to be 
syllable-timed with each syllable occurring at regular intervals. 
(10) A 'tiger and a 'mouse were 'walking in a 'field 
In example (10), the total number of syllables between two stress beats varies: 
there are three unstressed syllables between ti and mouse but only one 
unstressed syllable between mouse and wal. If speakers aim to produce stress 
beats at regular, isochronous intervals, they will have to adjust the duration of 
the intervening unstressed syllabies. The three syllables between ti and mouse 
should have the same duration as the single syllable were between the syllabi es 
mouse and wal. In order to achieve this, they have to be crushed together and be 
spoken much faster than the syllable were. This variable length of syllables has 
been taken as a characteristic property of stress-timed languages. 
utterance 
~ 
Figure 3.13. The utterance "This is a nice surprise" (in RP) divided into 
syllables and feet. 
The time interval comprising a stressed syllable plus all following unstressed 
syllables up to but not including the next stressed syllable is called the foot. It is 
therefore possible to insert another level into the prosodie hierarchy introduced 

88 
Chapter 3 
in Figure 3.1 - the level of the foot above syllables and below words. Figure 
3.13 shows how the phonemes of the utterance "This is a nice surprise" can be 
grouped into syllables, which themselves can be grauped into feet (F). 
Traditionally, stressed or strang syllables are indicated by a subscript g, whereas 
unstressed or weak syllables are transcribed by a subscript w. The utterance 
consists of three feet, the first one comprising the strang syllable IC)JSI and the 
weak syllables IrzJ and l'd/. The second foot consists of one strang and one weak 
syllable, and the third ofthe strong syllable IplarzJ only. 
The concepts of stress-timing and isochrony are very popular among 
researchers and language teachers. However, when you actually measure the 
time interval between stressed syllables in English, you will find that they do not 
have the same length (e.g. Roach 1982, Dauer 1983). Recent research on 
English speech rhythm therefore claims that isochrany is a perceptual 
phenomenon: speakers perceive regular beats although they do not really exist in 
speech (Couper-Kuhlen 1993). In recent appraaches, speech rhythm is measured 
using other phonological domains than the foot. For example, the relative vowel 
length and the duration of consonants (e.g. Ramus et al. 1999) or the relative 
duration of stressed and unstressed syllables is compared (Bolinger 1981, Gut 
2003). 
3.3 The phonologieal word in English 
All speakers of English agree that English utterances consist of words strung 
together. It is, however, quite controversial what a word actually iso In writing, a 
word is defined as the letters between two blanks - the sentence "What is a 
word?" thus consists of four orthographie words. Yet, even in writing, there 
are some difficult cases such as body-paint, flower pot, What's and l've. Should 
these cases be considered one or two words? In speaking, the problem is even 
greater. There are no pauses between words in an utterance, all phonemes are 
strung together. How can the boundaries of individual words be determined? In 
fact, this is a major challenge for language learners who initially find it 
impossible to segment the speech stream into words. Phonologists have 
praposed the unit of the phonologieal word (pword). This is based on the 
reasoning that this is a unit in which various phonological pracesses apply. In 
particular, word stress (see section 3.3.1 below) happens within the string of 
speech sounds defined as the phonological word. Other processes that occur 
within the unit of pwords such as aspiration and velarization are described below 
in section 3.3.2. 
Consider structures like What's and l've again. The Isl and the lvi in each 
example cannot stand on their own phonologically. They do not fonn a 
pennissible syllable but are incorporated into the syllable structure of the 

English phonology: phonemes, syliables and words 
89 
preceding lwotl and lall, respectively. These structures that are dependent on 
phonologically adjacent words are called clitics. Ifthe word they are attached to, 
the host, precedes the clitic, they are called enclitics; if the clitic precedes the 
host it is referred to as a proclitic. Although they may seem similar, clitics have 
a different phonological behaviour than bound morphemes. With affixes, there 
may be a change in the pronunciation ofthe root. For example, da changes into 
dan't and will changes into wan't. When clitics attach, conversely, there is rarely 
a change in the pronunciation ofthe root. For example, you do not have I have 
changing to *[iv] for I've. 
Many phonologists consider phrases such a the hause, agame or have it 
phonological words. This is based on the observation that the stress pattern in 
these syntactic phrases can be described with the same rules as those for 
polysyllabic words such as arause, aga in and habit. In fact, there is no 
difference in either pronunciation or stress between abridge and a bridge (both 
are produced 1;:,'blId3/), which is why both are considered phonological words in 
English. 
3.3.1 
Word stress in English 
The languages of the world can be divided into two broad classes regarding 
word stress. Some languages have fixed stress, where it is always a particular 
syllable of a word that is stressed. This is for example the case in Czech, where 
stress always falls on the first syllable of every word. Similarly, in Kiswahili, it 
is always the penultimate (last but one) syllable in a word that receives stress. 
Other languages have free word stress, which means that stress can be on any 
syllable of a word and cannot be predicted by rule. In English, word stress is 
neither entirely free nor fixed. Due to historical reasons such as the Norman 
Conquest in 1066 and the subsequent rule of French speakers in England, 
English is a language that has a particular richness of loan words in its 
vocabulary. French words tend to have stress on the last syllable, whereas 
Germanic ones tend to have initial stress, which contributes to the complexity of 
the rules describing English word stress. 
Since in English word stress is a property of each content (or lexical) word, 
it is referred to as lexical stress. Other word classes in English, such as 
determiners, conjunctions and prepositions, which are categorized as function 
words or grammatical words, do not have stressed syllabies. Typically, these 
function words are very short consisting of only one syllable and, as explained 
in section 3.2.3 above, stress is a relative term and cannot be applied 
meaningfully to monosyllabic words. 
English content words consisting of three or more syllables usually have 
more than one stressed syllable. Consider for example the words entertainment 

90 
Chapter 3 
and refrigerator. In each of them, two syllables have stress, but the strength or 
degree of stress is different. In lenter'tainment the syllable 'tain carries the main 
or primary stress, which is marked by the preceding apostrophe '. The syllable 
len carries less, secondary, stress and is marked by a preceding I' In the word 
re'frigqator, primary stress lies on the second syllable 'fri and secondary stress 
lies on the syllable ta. Phonologists thus assurne that English has at least two 
different levels of stress. Some approaches propose even more levels of stress 
(e.g. Chomsky & Halle 1968), which are, however, generally difficult to 
distinguish by listeners. 
Native speakers of English know without thinking which syllable of a word 
is stressed, i.e. which can receive an accent in speech. Even when encountering 
a completely unfamiliar word, native speakers usually agree on the syllable that 
can be accentuated. In contrast, learners of English find word stress very 
unpredictable and often hesitate which syllable to accent. (This is why good 
dictionaries indicate the stress pattern of each word.) What are these rules 
English speakers know? Several phonologists representing different theoretical 
traditions have worked on forrnulating rules that describe word stress in English. 
Most theoretical approaches agree that the following factors influence stress and 
accent placement in English words: 
-
the number and phonological shape of the syllables in a word (light and 
heavy syllabies) 
-
the morphological complexity of the word (morphologically simple or 
complex words, type of affix) 
-
the grammatical category the word belongs to (noun, verb etc.) 
In general, only heavy syllables can bear stress. Thus, if a word has only one 
heavy syllable, this is the one that is stressed. Examples for this are decay, 
attract, agenda and entertain. In de'cay, the second syllable is heavy with a 
diphthong as the nucleus, in a'ttract, the second syllable is heavy with two 
consonants in the coda position. Equally, agenda has a heavy syllable as the 
second syllable and in enter'tain the last syllable is heavy. As with alm ost every 
linguistic rule, there are some exceptions, such as co'mmittee and 'essay, where a 
light syllable is stressed. Very often, words with exceptional stress patterns are 
loan words for which the original stress pattern has been kept. 
In morphologically complex words that consist of astern and one or more 
affixes, stress patterns can be influenced by the type of affix. Some affixes are 
stress-attracting, i.e. they are always stressed. The prefix sem i- and the suffixes 
-ain, 
-ee 

English phonology: phonemes, syllables and words 
91 
-eer 
-ality 
-ese 
-ette (in RP) 
-esque 
-ique 
are always stressed in words such as 'semicircle, enter'tain, absen'tee, 
mountai'neer, nor'mality, Journa'lese, Giga'rette (RP only, in GA it is 'Gigarette, 
see Table 3.9), gro'tesque and u'nique. Other suffixes determine that stress in the 
word stern (i.e. the word without any affixes) shifts one syllable to the right, as 
for example in 
-graphy inpho'tography 
-eous in advan 'tageous 
-ial in ad'verbial 
-ic in a'tomic 
-ion in inhi'bition 
-ty in tran'quillity 
-ive in re'jlexive 
(but 'photograph) 
(but ad'vantage) 
(but 'adverb) 
(but 'atom) 
(but in'hibit) 
(but 'tranquil) 
(but 'reflex) 
These suffixes are called stress-shifting. Suffixes that are stress-neutral, by 
contrast, do not have any influence on the stress pattern of the stern. Examples 
for stress-neutral suffixes are -ness, -able, -ancel -ence, -al, -like, -ful, -ing, -ish, 
-less, -ment, -ly and -wise. 
The grammatical category of a word also influences stress placement in 
English. In general, English nouns with stress on the final syllable are rare. Most 
English nouns are not stressed on the final syllable and examples such as 
cham'pagne, maga'zine (GA: 'magazine) and ca'noe are loan words in which the 
original stress patterns have been preserved. Verbs and adjectives with final 
stress, conversely, are very common in English. The general rule for word stress 
in English nouns is that stress falls on the penultimate (the last but one) syllable 
if it is heavy and on the antepenultimate (the syllable preceding the penultimate 
syllable) if it is not. Thus, we have u'tensil where the second syllable is heavy 
and 'discipline where it is not and the preceding syllable is stressed. Should the 
antepenultimate syllable also be a light one, the onset consonant of the 
penultimate syllable becomes ambisyllabic as in j'drs.sr.plrnl. 
Compounds in English normally receive stress on the first element. Thus, it 
is 'jlower pot, 'greenhouse, 'White House, 'redhead and 'spoonfeed. In fact, 
stress is what makes the difference between the cOlnpounds 'g reenhouse (a 
house in which you grow vegetables) and 'blackbird (a particular type of bird) 
and the corresponding phrases green 'house (a house that is green) and black 

92 
Chapter 3 
'bird (a bird that is black). Some exceptional compounds that can be accented on 
the second element include 
-
compounds that have a number as the first element 
-
compounds that end in -ed and function as adjectives 
-
proper names of people, roads, institutions and public places 
-
compounds that have a place, time, material or ingredient as the first 
element 
-
compounds that function as adverbs 
An example of a compound with stress on the second element that has a number 
as the first element is second-'class. A compound ending in -ed and functioning 
as an adjective with stress on the second element is bad-'tempered. Proper 
names of people, places and institutions such as Mary 'Jones, Charing Cross 
'Road, Queen's 'College and Royal Albert 'Hall also have stress on the second 
element. When the first part of a compound describes a particular place, time, 
material or ingredient, stress falls on the second element. This applies to English 
words such as school 'dinner, summer 'fruit, olive 'ai! and leather 'jacket. An 
example ofa compound functioning as an adverb is Sauth-'East. However, when 
these compounds occur in connected speech, stress can shift to the left, 
especially when the word following the compound begins with a stressed 
syllable. Thus, half-'timbered becomes "a 'half-timbered house", secand-'class 
turns into "a 'second-class ticket" and after'noon appears as 'afternoon 'tea. 
To a limited extent, stress is phonemic in English. This means that a small 
number of minimal word pairs exist that are differentiated only by stress. Table 
3.11 lists some of these minimal pairs. These words are spelled in exactly the 
same way and have a prefix-plus-stem structure. It can be seen that these words, 
when functioning as verbs, have stress on the second syllable, whereas they have 
stress on the first syllable when they function as nouns or adjectives. 
It was shown above that some compounds have variable stress depending 
on the stress pattern of the following word. This is true for a number of English 
words: Heath'row for example changes the stress pattern in 'Heathrow 'airport 
and thir'teen does the same in 'thirteen 'girls. Equally, ho'tel turns into 'hotel in 
'hotel 'management, it is champagne but 'champagne 'breakfast and can'teen 
but 'canteen 'cook. In general, four types of words can be identified in English 
that often change their stress pattern when they are produced in combination 
with other words. These are 
-
adjectives ending in -ese 
-
adjectives starting with un-
-
adjectives that can be used adverbially 
-
numerals between 12 and 99, excepting 20,30,40,50,60, 70, 80 and 90 

English phonology: phonemes, syllables and words 
93 
For example, Japa'nese changes into 'Japanese girl, un'happy turns into "the 
'unhappy boy", up'stairs is produced with initial stress in 'upstairs room, and 
twenty'one turns into "with 'twentyone white swans". 
Table 3.11. Some English minimal word pairs that are differentiated by stress 
only. 
Verb 
Noun 
Adjective 
per'fect 
'perfect 
ab'stract 'abstract 
'abstract 
sub'ject 
'subiect 
'subiect 
up'date 
'update 
in'sert 
'insert 
ob'ject 
'obiect 
per'mit 
'permit 
in'crease 'increase 
de'crease 'decrease 
con'duct 
'conduct 
ex 'port 
'export 
in'sult 
'insult 
pro 'test 
'protest 
pro'duce 'produce 
sur'vey 
'survey 
re 'cord 
'record 
con'tract 'contract 
For many English words, two stress patterns are possible, even within the same 
variety ofEnglish. This variable stress reflects ongoing language change. In RP, 
'exquisite (used only by conservative speakers any more) is changing into 
ex'quisite (used primarily by younger speakers), dis 'pute is also heard as 
'dispute, 'comparable can also be pronounced com'parable and 'laboratory also 
occurs as la'boratory. Equally, eti'quette is changing into 'etiquette, 'integral 
into in'tegral, 
'kilometre into ki'lometre, 
'lamentable into la'mentable, 
'communal into co'mmunal and 'formidable into for'midable. A look at the 
history of English shows that stress patterns of words were particularly variable 
during the Early Modem English period when an enormous amount of loan 
words entered the language from Latin or French. Very often, the 'original' Latin 
or French stress pattern existed alongside a version in which stress followed the 
rule of initial stress for many Germanie words. For example e'ssay, a'spect and 

94 
Chapter 3 
dis'course eould be heard at that time as well as the stress pattern that has 
survived in eontemporary English. 
Differenees in stress furthermore exist between present-day varieties of 
English. Some differenees between word stress in RP and GA are shown in 
Table 3.12. It also includes two examples of differenees in eompound stress 
between the two varieties of English. 
Table 3.12. Differences in word stress between RP and GA. 
RP 
ciga'rette 
'harass (ment) 
la'boratory 
con 'troversy 
re'source 
re'search 
'ordinarily 
court 'martial 
GA 
'cigarette 
ha'rass(ment) 
'laboratory 
'controversy 
'resource 
'research 
ordi'narily 
'court martial 
country 'house 'country house 
It was stated at the beginning of this seetion that many multisyllabie words in 
English have two stressed syllabies, one bearing primary stress and the other 
bearing secondary stress. The word ,enter'taining for example has seeondary 
stress on the first syllable and primary stress on the third syllable. This 
differenee is usually illustrated in a tree indieating the relative stress of the 
syllabI es and feet of the word. The phonologieal theory ealled Metrical 
Phonology claims that stress is distributed depending on the relative weight of 
the syllables and feet of a pword. It thus assurnes that stress is assigned on eaeh 
tier (another teehnical term for level) of the prosodie hierarchy. Figure 3.14 
shows the proposed metrical strueture of,enter'taining. The word eonsists oftwo 
feet, and eaeh foot eonsists of two syllabies. It is a central claim of Metrieal 
Phonology that only one of a pair of elements on one tier can receive stress. In 
our example, of the two feet, the first one is weak and the second strong. 
Similarly, the two syllabI es in eaeh foot have a strong-weak pattern. 

English phonology: phonemes, syllables and words 
95 
e 
n t 
;:)t e In I 
lJ 
Figure 3.14. The stress pattern 01 the ward ,enter'taining. 
Since the first foot is weaker than the second one, the strong syllable len! of the 
first foot has relatively less stress than the strong syllable Itel/ ofthe second foot. 
Thus, primary stress lies on Itell and secondary stress on len!. An alternative 
illustration of the stress pattern of words is the metrical grid. Figure 3.15 shows 
how the weight ofthe syllables and feet ofthe word ,enter'taining combine to its 
final metrical shape. Each syllable receives one metrical weight on the syllable 
level, both strang syllables receive additional weights on the foot level and the 
strang foot receives another weight at the pword level. 
x 
X 
X 
X X X 
X 
en t::l tel nIlJ 
(pword level) 
(foot level) 
(syllable level) 
Figure 3.15. The stress pattern 01 the ward ,enter'taining. 
3.3.2 Phonological processes occurring at the level of the pword (advanced 
reading) 
Apart from word stress, there are other phonological processes in English that 
occur at the level ofthe phonological word (pword). For example, the aspiration 
of voiceless plosives is restricted to the beginning of a pword. As formulated in 
rule (1) in seetion 3.1.2 above, the three voiceless plosives of English are 
aspirated when they occur at the beginning of a pword, but not in any other 
position. This is why listeners can make the distinction between rice pot and rye 
spot. Both have exact1y the same segmental form lralspnt!. In the latter case, 
however, the Ipl is not aspirated since it occurs in combination with Isl at the 

96 
Chapter 3 
onset of the second syllable. For the same reason, many phonologists assurne 
that "to go" as weil as today constitute pwords: both have aspirated [th]. 
Likewise, the glottalization of the voiceless plosives and the velarization of 11/, 
which both occur at the end of syllabIes, refleet the boundaries of pwords. Since 
it'll has velarization of the 11/, this can be taken as an indication that this phrase 
constitutes a pword. Nevertheless, in phonological theory, there is still some 
uncertainty as to whieh entities in English can be considered pwords. Espeeially 
the status of function words and of host+enc1itic sequenees is still unresolved 
and seems to be influeneed by foeus and speaking style (Hall 1999, Raffelsiefen 
2004). For example, while in fast and informal speech it'll forms one pword, in 
careful pronunciation the phrase is pronounced "it will" in two separate pwords. 
There is some phonetie evidence that the pword is indeed a unit represented 
in speakers' brains. Experiments involving the exaet measurement of artieulatory 
movements of the lips, tongue, velum and vocal folds revealed that there are 
distinct differences between the articulation of pword-initial consonants and 
vowels compared to when these sounds oeeur in the middle of pwords. 
(Methods with which these articulatory movements ean be studied are deseribed 
in section 2.8). For example, for a In! at the beginning of a pword, the tip of the 
tongue has more and longer contact with the alveolar ridge than for a In! at the 
beginning of a syllable (Fougeron & Keating 1997). The voice onset time 
(VOT) of a pword-initial voiceless plosive is longer than that of the same 
consonant in word-medial position, if stress is kept eonstant (e.g. Turk & 
Shuttack-Hufnagel 2000). Vowels at pword boundaries furthermore show larger 
lip opening movements than at syllable boundaries (Byrd & Saltzman 1998, Cho 
2002, 2005). 
3.4 Theories of the acquisition of English phonology (advanced reading) 
How do children and other language learners acquire the phonological 
representation of phonemes, features, syllables and phonological words for a 
language? As yet we do not have the means to study direetly the structures and 
proeesses of the brain relevant for language aequisition. We therefore have to 
rely on the indirect study of mental representations of phonological structures 
and rules. For such an indirect study, one can, for example, analyse how 
speakers pronounee and stress unfamiliar words and draw conc1usions about 
rules they apply automatieally. Alternatively, one can present speakers with 
unfamiliar words and syllables and ask them to judge whether they are 'good' 
words and syllables in English. Obviously, this method cannot be applied with 
very young children, with whom one has to rely mainly on their reactions to 
sounds and words played to them (see seetion 6.8.1). 

English phonology: phonemes, syllabI es and words 
97 
3.4.1 English phonology in first language acquisition 
There are obvious differences between the pranunciation of words by young 
children and by adult speakers of a language. It is, however, debated whether 
productions such as [frt] for fish can be interpreted to mean that the child's 
mental representation of the phonological structure of the word fish is IfIt/. The 
question is: does the child say [fIt] because he or she 'knows' the word to be 
pronounced in this way or does the child 'know' thatfish is pronounced IfIS! but 
pronounces it [fIt] for some other reason? Some researchers argue that the child 
has an adult-like representation of words but that it is the child's inadequacies in 
motor contral that lead to the deviant production. This view is supported by 
observations that children understand much more than they are able to produce. 
When a child understands [fIS], then his or her mental representation is probably 
also IfIS!. Yet another theoretical approach claims that children have the adult 
fonn stored for ward recognition but use a different, also mentally represented, 
'output' form for their productions. The first phonological unit that seems to be 
acquired by children is the word (Menn & Stoel-Gammon 1995). Most children 
have amental representation of syllables and phonemes only by age six or seven 
- this seems to be a consequence of learning to read. 
The acquisition of word stress proceeds in various stages. The first words 
children praduce have a trochaic pattern, i.e. they comprise two syllables of 
which the first one is strong and the second one weak. It is therefore assumed 
that the initial mental representation of the metrical structure of words (the 
minimal word form W d min) consists of a single foot (F), which comprises two 
syllables (a), the first ofwhich is strang, as shown in Figure 3.16. 
Wd min 
I 
F 
/~ 
Figure 3.16. The minimal ward/arm as prapased by Fee (1995). 
This basic mental representation is reflected in child word forms that constitute 
mismatches to adult forms such as ['nana] for ba'nana (!ba'no:na/). The weak 
syllable !bgl preceding the strang syllable l'no:1 is not produced because it does 
not fit into the structure of the minimal word. Children might even over-
generalise this trochaic pattern and produce a stress shift in iambic words 
(which have a weak-strang pattern): gi'raffe is often pronounced 'giraffe or just 
'raffe [waf] by very young children. It is only at around age 2.5 that in trisyllabic 

98 
Chapter 3 
words ofthe SWS (strong weak strong) kind such as ,enter'tain all syllables are 
produced, but the two strong syllables typically receive the same degree of 
stress. It is assumed that children only produce iambic word stress and words 
with an SWS pattern with primary and secondary stress correctly when their 
mental representation of the metrical structure of words comprises more than 
one foot. 
The theory of Natural Phonology, founded by Stampe (1979), is also 
concerned with the explanation of phonological acquisition. It postulates 
universal natural phonological preferences that are inbom and form part of 
human cognition. These include processes that improve the articulation or 
perception of language and are conceptualized to have a phonetic basis. The 
process of first language acquisition is understood as a selection of those 
processes which conform to the language-specific requirements. This is 
achieved by a suppression, limitation and ordering of the natural processes or 
preferences, which eventually allows the correct production and perception of 
the target language's phonological categories. For example, a child is said to 
prefer the articulation ofvoiceless plosives to that ofvoiced plosives (this can be 
documented in productions such as [dok] for Idog/). Ifthe language that is being 
acquired contains voiced plosives, this natural preference has to be suppressed. 
3.4.2 English phonology in second language acquisition 
As has been shown in section 2.7.2, many differences between the pronunciation 
of native speakers and second language learners can be explained with the fact 
that language learners do not have the same mentally stored motor patterns for 
the activity of the muscles involved in the production of a particular sound 
sequence, or cannot execute these patterns as fast as native speakers can. This 
section describes other reasons why some language learners' pronunciation 
differs from that of native speakers. These reasons lie in the mental 
representations of phonological units and rules competent speakers of a 
language have. Language learners, whose first language (L 1) requires different 
phonological representations of phonological units and rules from the second 
language (L2), might, at least at the beginning of language learning, 
inappropriately use these units and rules in the L2. 
This negative transfer of linguistic knowledge has been noted in many 
studies. Some learners will make errors that involve altering the syllable 
structure and syllabification of the L2 when their LI and L2 have different 
syllable structures and different syllabification rules (e.g. Ishikawa 2002, 
Whitworth 2003). For example, some German learners ofEnglish insert agIottal 
stop between a ward ending in a vowel and the next one beginning with a vowel 
such as the apple. They probably do this because in Gennan syllabI es without a 

English phonology: phonemes, syllables and words 
99 
consonant in the onset posItion are rare. If syllables begin with a vowel in 
spelling (such as Apfel), usually agiottal stop /?/ is produced in the onset 
position (/?apfl/). Phonological knowledge of the LI might also interfere with 
the perception of a second language. Some leamers of English with an LI that 
has different rules of syllable structure find it difficult to recognize the number 
of syllables in an English word or misperceive word boundaries (e.g. Ishikawa 
2002, Broselow 1984). 
A theory of the acquisition of seeond language phonology within the 
framework ofNatural Phonology was proposed by Dziubalska-Kolaezyk (1990). 
In contrast to first language aequisition, which is considered to proeeed largely 
subconsciously, seeond language aequisition (SLA) ofphonology is assumed to 
be based on leaming in a eontrolled and eonscious manner. In the course of 
SLA, access to universal processes is considered to be more diffieult than during 
first language acquisition, as the phonologie al system of the LI is already 
established and thus limited to seleeted processes, underlying representations as 
weIl as rules. The essential prerequisite for the L2 leaming proeess is that the 
language leamer ean aeeess the universal processes. This allows hirn 01' her to 
modify the suppression, limiting and ordering of the universal process types of 
the LI. Aequiring the L2 phonology might involve the unsuppression of 
processes whieh were suppressed in LI aequisition and the reordering of proeess 
types in eomparison with the ordering of the LI phonology. It is assumed that 
aecess is facilitated by favourable psycholinguistie eonditions such as the 
amount of formallanguage instruetion, the leamer's attitude towards the L2 and 
his 01' her generallinguistie aptitude. 
The Natural Model of phonologieal aequisition distinguishes between 
phonemes and surfaee phonetie segments, and postulates that a speaker 
processes the sound intention (phoneme) rather than the aetual surfaee 
realization. A language leamer's task is therefore to aequire L2 sound intentions 
through the means of pereeiving L2 surfaee realizations. As shown in Figure 
3.17, in an initial state, a language leamer relates L2 surfaee realizations to LI 
phonemes. As a first step, L2 surfaee realizations need to be perceived without 
reference to LI eategories. This is supported by a high frequeney of L2 
realizations, formal language instruction and a favourable attitude towards the 
L2 and the language leaming proeess by the language leamer. In a second step, 
this perception leads to a representation of L2-speeifie phonemes (sound 
intentions), which then triggers the reordering and suppression (01' unsupression) 
of natural processes. The task of a Polish speaker aequiring the aspiration of 
voieeless plosives in English, for example, can be described as the 
unsuppression of a natural proeess. For LI acquisition of Polish, where 
aspiration appears only optionally in emphatie styles, it is hypothesised that the 
natural process of aspiration was suppressed. This suppression now needs to be 

100 
Chapter 3 
reversed in order to allow for the pword-initial aspiration of voiceless plosives in 
English. 
universal processes ........... . 
..... 
...... , 
L2 surface realizations 
fre7t L2 r:~.li7a'tions 
L 1-specific sound intentions / .................... ...-
I,am',. m,cha,',m { 
perception --- instruction 
j ~at,""d' 
production -
language aptitude 
Figure 3.17. The Natural Model oj L2 acquisition oj phonemes; reprinted with 
permissionjrom Dziubalska-Kolaczyk (1990). 
Archibald (1994) developed a model of L2 word stress leaming based on the 
theory of Universal Grammar (UG). This theory claims that all humans are 
innately endowed with a language faculty that contains grammatical principles 
and parameters. Principles are grammatical structures shared by allianguages of 
the world, whereas parameters have to be set according to language-specific 
requirements. lt is argued that only inbom knowledge in the form of language-
universal principles and parameters can account for the relative uniformity and 
speed of first language acquisition, which proceeds even with little positive 
evidence of some language structures in the ambient language ('poverty of the 
stimulus'). The question whether UG is still available, i.e. whether parameters 
can be reset, in L2 acquisition, is a matter of dispute. Archibald envisages the L2 
word stress leaming process as an interaction between UG and the input of the 
linguistic environment. Leaming is conceptualised as the (re-)setting of 
language parameters. This is influenced by three phenomena: indirect negative 
evidence in the ambient language, ability to choose appropriate cues and lexical 

English phonology: phonemes, syllables and words 
101 
dependency. In an initial state, learner speech will be highly variable or show a 
preference for the LI parameter setting, as the parameter for the L2 has not yet 
been set. After the threshold has been crossed, however, the parameter is set in 
the L2 and variation should stop. In order to be able to reset parameters, the 
learner is presumed to possess the ability to choose appropriate cues from the 
input language. A Czech learner of English, thus, resets his or her parameter -
which is set to stress on the initial syllable of each word - to learn the relatively 
free lexical stress of English. 
3.5 Exercises 
1. Which units can speech be divided into? Which phonological reasons can be 
given to support the linguistic plausibility of each ofthese units? 
2. What is the difference between a phoneme, an allophone and a phone? 
Describe some allophonic rules of English. 
3. What is the phoneme inventory ofRP and ofGA, and how do they differ? 
4. What is the difference between phonemic and phonetic transcription? 
5. What is the basic structure of an English syllable? Which consonants can 
occur in which position? 
6. What is stress? Which rules for English word stress can be formulated? Why 
is it difficult to capture English word stress with phonological rules? 
7. Transcribe the following words phonemically in both RP and GA (check a 
good dictionary for the solution): 
a. jungle 
d. bridges 
b. abbreviation 
e. camera 
c. confused 
f.obtain 
8. Analyse the following productions by German leamers of English. How do 
they differ from native pronunciations? Can you describe the eITors with a 
phonological rule? 
a. foot 
[fu:t] 
d. houses [hausIz] 
b. jeans 
[tSi:ns] 
e. buzzed [bAst] 
c. grab 
[glrep] 

102 
Chapter 3 
9. Draw the syllable stmcture of the words spring, laster and buzzard. 
Remember the mIes of syllabification and ambisyllabicity! 
10. Divide the following utterances into feet. 
a. It's raining cats and dogs. 
b. This is the house that Jean built. 
c. A bird in the hand is worth two in the bush. 
11. In order to illustrate the differences in vowel pronunciation between the 
world-wide varieties of English, Wells (1982) deveIoped the lexical sets - a 
list of words containing all distinctive vowels in RP and GA. Listen to the 
recordings of these lexical sets pronounced by an RP speaker (recording 
03_exercisel1_RP.wav on the CD-ROM), a GA speaker (recording 
03_exercisell_GA.wav on the CD-ROM) and a speaker from Nigeria 
(recording 03_exercisell_Nig.wav on the CD-ROM). Transcribe the vowels 
phonemically and comment on the differences between the three accents. 
3.6 Further reading 
The phonemic inventory of RP is described in Roach (2004), that of GA in 
Giegerich (1992, chapter 3). A description of Arnerican English (Far-
WesternIMid-Western accent) can be found in Ladefoged (1999). The phoneme 
inventories of other varieties of English are described in Kortmann & Schneider 
(2004). 
The IP A and its history together with descriptions of the phoneme 
inventories of a number of different languages are contained in the Handbook 01 
the International Phonetic Association (1999). The latest version ofthe IPA can 
be downloaded from the homepage of the International Phonetic Association at 
http://www.arts.gla.ac.uk/ipa/index.html.This website 
furthermore 
offers 
downloads of various phonetic fonts for the computer. An excellent practice 
CD-ROM containing the sounds of the International Phonetic Alphabet is 
available from University College London (produced in 1995). 
The pronunciation dictionaries by Wells (2000) and Jones (2006) give the 
transcription of English words and contain a CD-ROM with native 
pronunciation ofthese words. Eckert & Barry (2005, chapters III to VI) provide 
entertaining practice materials on all English phonemes and allophones, stress 
and weak fonns that prove especially challenging for German native speakers. 
Roach (1991, chapter 12) presents the strong and weak forms of many English 
words together with training material on audiocassettes. 

English phonology: phonemes, syllabIes and words 
103 
For advanced readers, Lass (1984) offers an elaborate discussion of feature 
systems, and feature trees are described in Halle (1992) and Kenstowicz (1994). 
Giegerich (1992, chapter 6) gives more details on the English syllable structure. 
See Hogg & McCully (1987) for an overview ofMetrical Phonology. Ramus et 
al. (1999) provide a detailed discussion of traditional and recent measurements 
of speech rhythm. 
For especially interested readers, a concise introduction to the theory of UG 
can be found in Radford (1997, chapter I). Vihman (1996, chapter 6) gives an 
overview of the emergence of phonological structure in first language 
acquisition. Leather & James (1996) and Hansen & Zampini (2008, chapters 2 to 
5) summarize the major issues in second language acquisition ofphonology. 


4 The Phonology of English: Intonation 
Chapter 3 showed how speech ean be divided into units of different size and 
explained that these units are thought to be ordered hierarehieally: phonemes, 
whieh themselves eonsist ofbundles ofphonetie features, form syllabies, whieh 
in turn eombine to make up feet and prosodie words. This ean be illustrated with 
the prosodie hierarehy (see Figures 3.1 and 4.1). Aeeording to the different 
levels of the prosodie hierarehy, phonology ean be divided into two branehes. 
Segmental phonology is eoneerned with phonological deseriptions of the 
segmental (or phoneme) inventory of languages, whereas suprasegmental 
phonology (also ealled prosody) deals with all units higher than the segment, 
i.e. syllabies, feet, prosodie words and the higher levels intonation phrase, 
utteranee and discourse. 
discourse 
~ 
utteranee utteranee 
I 
~ 
IP 
IP 
IP 
!f\ /\ /\ 
www 
ww 
w w 
Figure 4.1. Some of the higher levels of the prosodie hierarehy: words (w), 
intonation phrases (IP), utteranees and diseourse. 
This ehapter describes the three highest phonological levels of the prosodie 
hierarchy illustrated in Figure 4.1. When speakers produee words in a row, we 
can usually observe that they are structured: individual words are grouped 
together to form an intonation phrase. Section 4.1 explains the phonological 
unit of intonation phrases and shows how English words eombine into 
intonation phrases. Intonation phrases can coincide with breath groups that were 
deseribed in seetion 2.1, but they do not have to. Often, a breath group eontains 
more than one intonation phrase. As with all other phonological units, it is 
assumed that speakers have amental representation of intonation phrases, i.e. 
they know how to produce speech structured into intonation phrases and they 
rely on this knowledge when listening to the speech of others. 

106 
Chapter4 
Within an intonation phrase, there is typically one word that is most 
prominent. Section 4.2 explains the phonological rules describing which of the 
words in an English intonation phrase is most prominent. Moreover, the pitch of 
a speaker's voice changes across intonation phrases. The linguistie use of such 
pitch movements is called intonation and is described in section 4.3. When 
producing speech, speakers can combine intonation phrases into longer 
utterances by means of intonational phrasing and by intonation. Some 
utterances might contain just one intonation phrase, others might contain several 
of them. Moreover, speakers can put utterances together to form larger stretches 
of speech or discourse. This also happens with the help of intonational phrasing 
and intonation, as described in sections 4.1 and 4.3.3. Section 4.4 explains how 
language leamers acquire these phonological units and their rules. 
4.1 Intonational phrasing in English 
Speakers do not often produce single words when they communicate with each 
other but usually string several words together. However, speakers do not 
simply produce one word after the other. In speech, partieular groups of words 
are closer-knit than other groups of words. This is something listeners expect 
and make use of in interpreting speech. Many people can even hear these word 
groups in some (related) languages they do not speak - which shows that it is 
not the meaning alone that makes words belong together but phonological 
properties of speech (see section 5.5.1 for more details). The groups of words 
that belong closely together are called phrases. In order to avoid confusion with 
syntactic phrases, these phrases are referred to as intonation phrases. (Note that 
many other terms are in use for this type of phrase, including intonation group, 
phonological phrase, tone unit, tone group, breath group and sense group). In 
general, speakers use intonational phrasing in order to structure their discourse 
into units of information. They thus show a listener which words of an utterance 
belong together - which words form an intonation phrase. Intonational phrasing 
in English can have a meaning-distinguishing function. Consider utterances lla 
and Ilb: 
(11a) He washed and fed the dog 
(11 b) He washed land fed the dog 
If the utterance "He washed and fed the dog" is produced as one intonation 
phrase, its meaning is that a person both washed and fed a dog. Conversely, if 
the same utterance is produced as a sequence of two intonation phrases with an 
intonation phrase boundary after washed (indicated by the symbol 1), the 
meaning of the utterance changes into 'someone who washed hirnself and fed a 

The phonology of English: intonation 
107 
dog'. The end of an intonation phrase - an intonation phrase boundary - is 
sometimes but not always signalIed by a short pause. (Section 5.5.1 explains 
other ways of indicating the boundaries of intonation phrases, such as final 
syllable lengthening, anacrusis and change of pitch level). 
All examples in this chapter can be listened 
to on the CD-ROM. Please bear in mind that 
these examples were produced by a speaker 
reading printed sentences. As explained 
below, intonation in spontaneous English 
speech may show different characteristics. 
'~-----------------------------------------------------------------~~ 
When considering the function of intonational phrasing it is important to be 
aware of the fact that there are many different kinds of spoken language, many 
different kinds of speaking styles. Reading a text aloud and reciting a poem or a 
speech leamed by heart require different cognitive processes than telling a story 
or participating in a discussion. The first types of speaking style are called 
reading style and prepared speech respectively, whereas the second type of 
spoken language is referred to as spontaneous or free speech. In spontaneous 
speech, in contrast to prepared speech, speakers have to speak and plan their 
speech at the same time. While articulating one portion of the 'message' the 
speaker will already have to plan and prepare what to say next. Spontaneous 
speech is therefore delivered in chunks, and 'speaking time' typically consists of 
a large part (usually up to 30%) ofpauses. Clearly, the speaking style influences 
intonational phrasing. In spontaneous speech, speakers break their message up 
into smaller chunks than in reading style or prepared speech - both because they 
need time to plan ahead and because it allows listeners to follow their speech. 
The words joined into these chunks usually form one piece of information, a 
meaningful unit called topic unit. Often, in spontaneous speech, each intonation 
phrase corresponds to one meaningful unit. When producing prepared speech or 
reading a printed text aloud, speakers do not need to plan what to say next. They 
do not need time to search for the right words and put them into a grammatical 
order. Consequently, this type of speech is faster and contains fewer pauses. 
Moreover, intonational phrasing in this type of speech has a very elose 
relationship with punctuation and the grammatical structures of written 
language. 
When producing speech, speakers join several intonation phrases together 
and produce longer utterances and discourse. With intonation phrase boundaries, 
speakers can indicate how elosely the individual intonation phrases in an 

108 
Chapter4 
utterance belong together. Especially when reading out a text, speakers vary the 
length of pauses between intonation phrases according to the meaning 
relationship between these intonation phrases. 
(12) He washed 1 and fed the dog 11 Then he tumed to his own mealll 
In example (12), the intonation phrase boundary between the words washed and 
and is shorter (and therefore called a minor boundary) than that between dog 
and then (which is referred to as a major boundary). A minor intonation phrase 
boundary is transcribed with a single I; a major intonation phrase boundary is 
transcribed with all. The major boundary in example (12) indicates the 
beginning of a new topic - the speaker's now tuming to his own meal. 
There are many other examples of utterances in which speakers use 
intonational phrasing in order to indicate the structure of their speech. 
Coordinated structures, for example, are usually separated by intonation phrase 
boundaries. This is shown in example (13), which represents an utterance with a 
coordinate structure consisting of the two components "Y ou could put it over 
there" and "or leave it where it is". Especially when the two components of such 
structures are placed in contrast to each other by the speaker, they are separated 
by an intonation phrase boundary. 
(13) Y ou could put it over there 1 or leave it where it is 11 
Similarly, items in enumerations are often produced as separate intonation 
phrases. This is shown in example (14). It is, however, also possible to produce 
such lists as a single intonation phrase, as shown in example (15). 
(14) 1'11 buy tomatoes Ilettuce 1 a cucumber 1 and some carrots 11 
(15) Her trousers are red blue green and yellow 11 
In an utterance, 'heavy' subjects that consist of noun phrases with many words 
are typica11y produced as separate intonation phrases in English. This is 
illustrated in example (16), in which an intonation phrase boundary is produced 
after the heavy subject "The inhabitants of our beautiful village" . 
(16) The inhabitants of our beautiful village 1 do not care for this bypass 11 
Similarly, tags in questions tend to be separated by intonation phrase 
boundaries. This is shown in examples (17) and (18), which both have so-called 
reverse-polarity tags, i.e. the verb in the tag is negated when the verb in the main 
clause is not (example 17) and vice versa (example 18). 

The phonology ofEnglish: intonation 
(17) We should do it now 1 shouldn't we 11 
(18) You didn't see hirn 1 did you 11 
109 
Intonational phrasing can furthennore indicate whether the infonnation 
contained in a relative clause is additional or essential. If it is additional, the 
relative clause is called non-defining; defining relative clauses contain 
infonnation about the noun they refer to that is essential for the identification of 
this noun. In writing, non-defining relative clauses are usually indicated by 
inserted commas. Non-defining relative clauses seem to be rare in spontaneous 
language. In reading style and prepared speech, as in examples (19) and (20), 
they are often separated by intonation phrase boundaries. Thus, the speaker 
knows that the infonnation "an old woman of about 90" is not essential for the 
listener in order to know which neighbour he or she is talking about - the 
speaker probably only has one neighbour. Equally, the speaker ofutterance (20) 
indicates that he or she only has one partner and that the infonnation "who lives 
in Munich" is merely additional. Defining relative clauses, conversely, tend not 
to be separated by intonation phrase boundaries in speech. This is shown in 
example (21). Here, the speaker indicates to the listener that he or she has more 
than one daughter and that it is the one who lives in Munich who is being 
referred to. 
(19) My neighbour 1 an old woman of about 90 1 has given me this present 11 
(20) My partner 1 who lives in Munich 1 is 35 years old 11 
(21) My daughter who lives in Munich came to see me 11 
Speakers use intonational phrasing in order to indicate whether a piece of 
infonnation is defining or non-defining also in utterances that do not contain 
relative clauses. In example (22), the dog's name is additional and not necessary 
for the identification ofthe dog (the speaker only has one) and thus separated by 
an intonation phrase boundary. Similarly, in (23) the infonnation "opposite the 
house" is merely additional. Such utterance elements that contain circumstantial 
infonnation and which are typically produced as separate intonation phrases are 
referred to as parenthetical structures. 
(22) This is my dog 1 Tracy 11 
(23) Look at the huge tree 1 opposite the house 11 
Furthermore, clause-modifying adverbials in English are commonly separated 
by an intonation phrase boundary. This applies to adverbials preceding the main 
clause as in utterances (24) and (25) as weil as to the post-modifying adverbial 
in utterance (26). 

110 
Chapter 4 
(24) Unfortunately 1 there wasn't enough time for us to see the museum 11 
(25) During the last year 1 not a single bird has been sighted 11 
(26) The fight against global warming has begun 1 apparently 11 
Similarly, vocatives and imprecations, when in initial position, tend to be 
produced as separate intonation phrases. Example (27) illustrates an utterance 
with a vocative (i.e. an utterance element used to call someone or attract 
someone's attention); example (28) shows how an imprecation - the calling on a 
higher power or an expletive - is usually separated by an intonation phrase 
boundary. 
(27) Gillian and Tom 1 we are leaving 11 
(28) For heaven's sake Ileave hirn alone 11 
When a subject or an object of a elause is 'topicalised' in English, it is usually 
produced as a separate intonation phrase, too. Topicalisation means that the 
element is in some way emphasised, often by moving it to the beginning of the 
utterance as in (29) or by recapitulating it at the end ofthe utterance (30). 
(29) Quite interesting 1 the film 1 wasn't it 11 
(30) Always looks pretty 1 Jean does 11 
Syntactic structures that topicalize sentence elements are eleft sentences and 
pseudo-eleft sentences. These structures are also typically produced as separate 
intonation phrases in speech. Example (31) illustrates a eleft sentence and 
example (32) illustrates a pseudo-eleft sentence. 
(31) It was Tom 1 who was late again 11 
(32) What I have also wanted to know 1 is what you did last Friday 11 
When analysing intonational phrasing in spoken English, it becomes obvious 
that the relationship between the grammatical structures described in examples 
(13) to (32) and intonational phrasing depends crucially on the speaking style. In 
reading style, a elose relationship between punctuation and type of intonation 
phrase boundary can be observed - sentences separated by full stops in writing 
are typically separated by major intonation phrase boundaries; elauses and other 
sentence elements separated by commas in writing are usually produced as 
intonation phrases with minor boundaries. Practised readers will produce every 
printed sentence and every elause as one intonation phrase, and often the length 
of the pause that is inserted is correlated with the type of syntactic structure. 
Pauses after complete sentences are longer than those after elauses, which in 
turn are longer than those after syntactic phrases and subordinate structures. 

The phonology of English: intonation 
111 
Similarly, empirical studies have shown that in prepared speech and story-telling 
the correlation between intonation phrases and syntactic boundaries is very high. 
Spontaneous speech, however, is stmctured by intonation phrases that do not 
correspond weil to syntactic units (e.g. Yang 2004). Speakers do not produce 
complete grammatical sentences in conversations and spontaneous interactions. 
Many utterances are elliptical, i.e. do not have a verb or a subject, and are thus 
much shorter than written sentences. The relationship between type of boundary 
and pause length is less obvious in spontaneous speech than in reading style 
(Keseling 1992). Therefore, it is much more difficult to forrnulate phonological 
mies that predict where intonation phrase boundaries will occur in spontaneous 
speech. 
The length of intonation phrases seems to be fairly limited. In an analysis of 
a corpus of spontaneous British English, Crystal (1969) found that, on average, 
intonation phrases are five words long. Only few intonation phrases (20%) are 
longer than eight words. This means that, on average, intonation phrases are 
between one and two seconds long (Tench 1996). This, however, is not due to 
physiological limitations of speakers. Remember that in section 2.1 it was 
shown that speakers can produce more than eight seconds of speech before they 
need to take a breath. The relatively short duration of typical intonation phrases 
is more likely due to limitations of listeners: it is more difficult to understand 
speech that is stmctured into larger units. Think about how difficult it is to 
follow a presentation that consists of written language read aloud in comparison 
to a presentation delivered as free speech! On the whole, intonation phrases in 
spontaneous speech are shorter than when a speaker is producing prepared 
speech or when reading a text aloud. As explained above, this reflects the fact 
that in spontaneous speech speakers need to think about what to say next before 
articulating. 
4.2 Nucleus placement in English 
Section 3.3.1 showed that every lexical word in English has at least one stressed 
syllable, i.e. one syllable that is normally accented when the word is 
pronounced. Consequently, in intonation phrases, which consist of several 
lexical words, there are several potential places for accents. In every intonation 
phrase, one of the words (or, to be more precise: one of the syllables of one of 
the words) will receive the strongest accent; this is called the nucleus of the 
intonation phrase (not to be confused with the nucleus of a syllable!). Nucleus 
placement in English (sometimes also referred to as 'sentence stress'), like 
intonational phrasing, can distinguish the meaning of utterances. Roach's (1991: 
173) examples (33) and (34) illustrate this: 

112 
(33) I have plans to LEA VE 
(34) I have PLANS to leave 
Chapter4 
If, as in utterance (33), the nucleus or main accent ofthe utterance falls on leave 
(the nucleus is printed in capitalletters), the utterance means roughly 'I want to 
leave, I am planning to leave'. If, however, the nucleus is on plans as in (34), the 
speaker expresses that he or she is going to leave some plans (e.g. a drawing of a 
house by an architect) somewhere. 
It is difficult to establish exact rules for nucleus placement in English. 
Whether an accent is appropriate on one or the other word in an utterance 
depends a lot on the situational context, the participants of a conversation, the 
type of speech produced and many other factors. This means that a speaker may 
produce the same utterance with different accentual patterns for different 
purposes. For example, a speaker might say 
(35) Tomorrow 1 I am going HOME 11 
when telling someone else that she is going horne the next day. Alternatively, 
the speaker might produce this utterance as a reply to the question "Are you 
going to London tomorrow" and tell her conversation partner that she is not 
going to London, but rather horne (e.g. to Bristol). The same utterance with a 
different accentuation pattern such as in (36) 
(36) Tomorrow 1 I AM going horne 11 
might be produced by a speaker wishing to imply that after they have been 
planning to go horne for several weeks it will finally become true tomorrow. 
Despite this difficulty of formulating exact rules, some tendencies of nucleus 
placement in English can be described. As a rule of thumb, it is the last content 
word of an intonation phrase that receives the nuclear accent. Content words are 
words that belong to the classes noun (such as house), verb (such as go), 
adjective (such as green) and adverb (such as beautifully). Other types ofwords 
such as prepositions (o/), pronouns (her), determiners (a) and auxiliaries (am, 
will), which are referred to as grammatical or function words, rarely function as 
the nucleus in an intonation phrase. Thus, in example (37), in which the last 
word at is a function word, the nucleus is placed on the last content word, which 
is looking. Equally, in example (38), the last two words, since they are function 
words, do not receive the nucleus, which is instead placed on books. 
(37) Who is she LOOking at 11 
(38) She forgot to bring the BOOKS with her 11 

The phonology of English: intonation 
113 
In general, nouns are the most likely type of content word to function as a 
nucleus in an intonation phrase. In cases where a verb follows a noun, it is the 
noun that receives the nuclear accent and not the verb (examples 39 and 40). 
This is true despite the fact that, when there is no preceding noun (but e.g. a 
pronoun instead), the verb is accented - compare examples (40) and (41). This 
tendency to place the nucleus on nouns even when another content word follows 
it, can also be seen in many English fixed expressions such as "They got on like 
a HOUSE on fire", "Let's see which way the WIND'S blowing", "Keep your 
FINgers crossed" and "He's got a SCREW loose". 
(39) The PHONE rang 11 
(40) He got his BIKE repaired 11 
(41) He got something rePAIRED 11 
Some English nouns such as things, people and places, however, have very little 
meaning and thus tend not to be accented even when they appear in the last 
position of an intonation phrase (see examples 42 to 44). This is probably 
because they are used by speakers as vague express ions that do not refer to 
particular objects or living beings like 'real' nouns. 
(42) He's always iMAgining things 11 
(43) You shouldn't aNNOYpeople 11 
(44) I've always loved GOing places 11 
Only very few function words can receive the nucleus in English. One ofthem is 
the preposition of a phrasal verb. Phrasal verbs are verbs that consist of a verb 
plus preposition (such as up, down, off, in) or adverb (such as back, away). Thus, 
it is take IN (a phrasal verb), but SfT in (a prepositional verb). Examples (45) 
and (46) illustrate that the preposition of a phrasal verb receives the nucleus of 
an intonation phrase in spoken English. The only exception to this rule is when a 
noun is inserted between verb and preposition (examples (47) and (48)). 
(45) Please take it OFF 11 
(46) Just switch it ON then 11 
(47) Please take your SHOES offll 
(48) Just switch the LIGHT on then 11 
Other function words that attract the nucleus in English are too (see example 
49), either (example 50), as weil (example 51) and anyway (example 52). 
(49) I want some spaghetti TOO 11 
(50) He didn't see me EIther 11 

114 
(51) I am fond ofpizza as WELL 11 
(52) She was going horne Anyway 11 
Chapter4 
Looking at nucleus placement from a pragmatic point ofview, i.e. its function in 
a conversation, it turns out that speakers can signal a variety of meanings and 
assumptions by producing the nuclear accent on one particular utterance 
element. In general, in English utterances, it is new information in an utterance 
that is accented rather than given information. The term new information refers 
to things, events or concepts that have not been mentioned or shown yet in 
previous conversation and that the speaker assurnes the listener does not 
currently have in his or her consciousness. Conversely, given (or 'old') 
information has previously been referred to in the conversation or is in some 
other way present in the listener's mind. Thus, a speaker will typically produce 
utterance (53) as a reply to the question "Do you like the food". 
(53) It is very deLIcious food 
The nucleus is on the syllable li in delicious rather than onfood. The wordfood 
had just been mentioned in the question and thus constitutes given information, 
whereas delicious has not previously been referred to and thus constitutes new 
information. 
Given information does not always appear as the exact repetition ofthe same 
word in utterance. Sometimes, speakers use synonyms, hyperonyms or 
hyponyms for words mentioned before, but these do not receive the nucleus. In 
example (54), the second listener does not pi ace the nucleus on games -
although it is the last noun of the intonation phrase - because the first speaker 
has just used the wordfootball, a type of game, which caused the concept 'game' 
to be present in the listener's mind. Consequently, games is treated as given 
information. Equally, in example (55) the mentioning of poodles raises the 
general topic of dogs in the conversation, so that the word dogs later in the 
intonation phrase can be treated as given information. 
(54) Do you like FOOTball 11 
No I HA TE games 11 
(55) Breeding poodles and Other dogs 11 
Nucleus placement thus shows whether a speaker considers an element in an 
utterance as new or given. Alternatively, speakers can use nucleus placement in 
order to imply givenness of some piece of information. Consider examples (56) 
and (57). In example (56), the speaker does not accentuate joke, treating it as 
given information and thus implying that Linda's assertion that she hates 
tomatoes is a joke. If Linda was joking, then Tom's joke is just another joke -

The phonology of English: intonation 
115 
given information - and the wordjoke does not reeeive the nucleus. Had Linda's 
assertion that she hated tomatoes been taken seriously, the speaker would have 
plaeed the nucleus onjoke as in example (57). In this utteranee the implieation is 
that first Linda said something and then Tom made a joke, whieh in that ease -
sinee no other joke had been made yet- has the status ofnew information. 
(56) Linda said she hated toMAtoes 1 and then TOM made ajoke 11 
(57) Linda said she hated toMAtoes 1 and then Tom made a lOKE 11 
Nucleus plaeement thus serves to highlight eertain parts of an intonation phrase, 
whieh has also been deseribed as focus. A foeussed element of an utteranee is 
more prominent than an element that falls outside the foeus. New information is 
generally in foeus, whereas given information typieally falls outside the seope of 
foeus. Two types of foeus ean be distinguished: broad and narrow foeus. Broad 
focus means that everything - the entire intonation phrase - is brought into 
foeus, as in example (58). With broad foeus, the nucleus falls on the last eontent 
word. With narrow focus, a speaker highlights only aseleeted part of the 
intonation phrase. In example (59) the narrow foeus is on sister so that it is not 
the last eontent word of this intonation phrase that reeeives the nucleus. 
(58) What were you doing 11 
Everyone gave her a strange LOOK 11 
(59) Who eame with you 11 
It was my SISter who eame with me 11 
There are also grammatieal ways of foeussing in English - sueh as clefts, 
pseudo-clefts and passive eonstruetions - and normally these fall together with 
the nucleus. Utteranees (59) and (60) illustrate a cleft eonstruetion, utteranee 
(61) is an example of a pseudo-cleft and utteranee (62) shows a passive 
eonstruetion that coineides with the nucleus. 
(60) It was a FLOwer I saw 
(61) What he said was RUbbish 
(62) The house was hit by a COmet 
There are some oeeasions on whieh speakers may wish to put foeus on utteranee 
elements that eonstitute given information. This is usually done for purposes of 
eontrast, as in utteranee (63). There, the nucleus on black indieates whieh ofthe 
two dogs the speaker is referring to. One partieular type of eontrast is implied in 
insists, in whieh a speaker denies something that has been suggested (example 
64) or has been implied in a previous utteranee (see example 65). Furthermore, 
given information might be aeeented in eeho questions sueh as (66). 

116 
Chapter4 
(63) There was a black and a brown dog 11 It was the BLACK one that bit her 
(64) (Why are you looking at her) 
(65) (Go to bed now) 
(66) (He failed the exam) 
I am not LOOking at her or 
I am NOT looking at her 
But I am not TIRed 
HeFAILED 
As with intonational phrasing, the rules of nucleus placement presented here 
predict nucleus placement in reading style better than in spontaneous speech. In 
spontaneous speech some intonation phrases are usually unfinished and do not 
contain any nucleus at all. This happens, for example, when in conversations 
speakers are interrupted by others or when they interrupt themselves in order to 
rephrase or correct what they have said before. 
So far, the nucleus has been described as the primary accented syllable of an 
intonation phrase. Yet, apart from being accented by the mechanisms of 
increased air pressure and increased tension of the vocal folds described in 
section 2.1, nuclei are usually also associated with a distinct pitch movement. 
This is why the nucleus is often described as a pitch accent. In fact, most 
accented syllables in English have a distinct pitch height or pitch movement and 
most nuclei in English have a characteristic pitch movement. This is described 
in the next section. 
4.3 English tones and their usage 
Section 2.2 explained that all voiced sounds of an utterance have a certain height 
of voice or pitch. Normally, the pitch of a speaker's voice moves up and down 
across an utterance. (lt is in fact highly unusual to speak on an unvarying pitch, 
which creates the impression of a robot talking.) This varying pitch across an 
utterance is perceived by listeners as a continuous movement of pitch. Only 
some of these pitch movements are linguistically relevant, whereas others are 
by-products of physiological mechanisms. Linguistically relevant changes in 
pitch are 
-
controlled by the speaker 
-
perceptible 
-
associated with a certain meaning or function 
What does it mean that linguistically relevant pitch changes are controlled by a 
speaker? If a speaker's voice goes up and down because he or she is cycling on a 

The phonology of English: intonation 
117 
bumpy road, these are not linguistically relevant pitch changes. It is only those 
pitch changes that are intentionally produced by a speaker that can convey 
linguistic meaning. Equally, there are lots of changes in pitch across utterances 
that can be measured and seen on a computer (see section 5.5.3), but only those 
that can actually be perceived by listeners are of linguistic relevance. Section 6.5 
explains that only pitch changes that exceed a certain magnitude can be picked 
up by the human ear; others will simply not be noticed. Lastly and most 
importantly, pitch patterns that are linguistically relevant are associated with a 
particular meaning or function. This is true of language in general, where we 
always have a conventionalised connection between a particular form (a 
sequence of sounds or a pitch movement) and a particular meaning (the meaning 
ofthe sequence of sounds; the meaning ofthe pitch movement). 
Different languages use pitch in different linguistically relevant ways. 
Section 2.2 explained that in tone languages like Chinese or Igbo the pitch 
height or pitch movement on a syllable or word can determine the meaning of 
words. This is not the case for intonation languages such as English or 
German. In intonation languages, the meaning of a word does not change with 
pitch height. Whether you say two with a high or a low, a rising or a falling 
pitch, it will always mean 'two'. Nevertheless, some of the pitch movements 
occurring in English have distinctive functions. Consider utterances (67) and 
(68). They differ in meaning although they differ only in the pitch movement on 
the last word. 
(67) You are going \home 
(68) You are going /home 
In (67) the speaker's voice falls during the production ofthe word horne (which 
is indicated by the symbol \) so that the utterance is perceived as a statement 
about the addressee going home. In contrast, in utterance (68) the speaker's 
voice rises on horne (indicated by the I). This utterance is typically interpreted 
by listeners as a question with which the speaker asks the listener to verify 
whether he or she is going home. Thus, some pitch movements (the technical 
term is tones) in English are distinctive just like some speech sounds in English 
are distinctive. Consequently, English has a tone system or tone inventory just 
as it has a phoneme inventory. 
4.3.1 The tones of English and their transcription 
Scientific investigation into the tone inventory of English began in Britain in the 
early 20th century (e.g. Palmer 1922, Kingdon 1958, O'Connor & Arnold 1961). 
This British School of intonational analysis claims that each intonation phrase 

118 
Chapter4 
has one nucleus, which is its most prominent syllable and which is usually 
associated with a distinct pitch movement (see section 4.2 above). The pitch 
movement of the nucleus is referred to as the nuclear tone. Six basic nuclear 
tones are proposed for British English (see Table 4.1). These can be divided into 
the simple tones fall, transcribed with the symbol \ preceding the accented 
syllable, and rise, which is transcribed with the symbol /. The fall-rise V, the 
rise-fall /\ and the rise-fall-rise N are complex tones. Some authors further 
divide the falls into low and high falls and the rises into high and low rises. A 
high fall drops from a high point in the speaker's voice to a low point, whereas a 
low fall starts lower and has less pitch movement. Similarly, a low rise starts 
low and rises only slightly, whereas a high rise ends very much higher. When 
the voice sustains its height on a nucleus, this is referred to as level pitch and is 
transcribed with the symbol -. Level pitch can further be divided into high, mid 
and low level. 
Table 4.1. The nuclear tones of English and their transcription. 
English nuclear tones 
British School 
ToB! 
transcription 
transcription 
fall 
\ 
H*L% 
nse 
/ 
L*H% 
fall-rise 
V 
H* L-H% 
rise-fall 
/\ 
L* H-L% 
rise-fall-rise 
N 
L*+HL-H% 
level 
L*L% 
The British School proposes other elements of intonation phrases apart fonn the 
nucleus. If in an intonation phrase there are any pitch accents preceding the 
nucleus, they fonn the head. Unstressed syllables preceding the head are 
referred to as the pre-head. Pitch movements in heads can be falling (which is 
transcribed with the symbol ~ before the accented syllable), rising (which is 
transcribed with the symbol ? before the accented syllable) or be produced on a 
level pitch (which would be transcribed as '). In utterance (69), she constitutes 
the prehead, the head includes all syllables from did to to and the nucleus lies on 
the last word go. 
(69) she 
~ didn't want to 
\go 
PREHEAD 
HEAD 
NUCLEUS 

The phonology of English: intonation 
119 
The transcription symbols for the intonation of this intonation phrase 
indicate that the pitch is falling from did to to and then falling again on go. In 
order to fall on go, the voice will have to go up to a higher level at the beginning 
of go. This can be depicted in an interlinear transcription of this utterance 
given in Figure 4.2. Each syllable of the utterance is represented by a dot: the 
bigger the dot, the stronger the accent on the syllable. Both the syllable did and 
the syllable go in this utterance are strongly accented. In addition, the pitch 
height of each syllable is transcribed. The two horizontal lines symbolize the 
upper and lower range of a speaker's voice. The position of the dot between the 
two lines thus indicates the pitch height of each syllable. The syllable did is 
higher in pitch and more accented than the syllable she; the syllable to is lower 
in pitch and less accented then the last syllable go. This last syllable of the 
utterance has a falling pitch, which is symbolised by the falling line attached to 
the dot. 
she 
did 
n't 
want 
to 
go 
• 
Figure 4.2. Interlinear transcription of the intonation of the utterance "She 
didn't want to go". 
When any further syllables follow the nucleus - which would be called the tail 
in the British School tradition - the nuclear pitch movement begins on the 
accented syllable and stretches over the following syllables until the end of the 
utterance. Thus, in utterance (70) the nuclear fall is realized only on the one 
syllable of the word go. In utterance (71), however, the fall begins on go and 
stretches across the words with and hirn in the tail. In the transcription, no 
difference can be seen. In both cases, the transcription symbol for the nuclear 
fall is placed before the nucleus on go. The interlinear transcription of utterance 
(71) given in Figure 4.3 shows how the pitch of the syllables drops in what is 
called a step-down. Section 5.5.3 shows that nuclear falls can also be realized as 
a continuous pitch movement. 
(70) She didn't want to \go 
(71) She didn't want to \go with hirn 

120 
Chapter 4 
she 
did 
n't 
want 
to 
go 
with 
hirn 
• 
• 
Figure 4.3. Interlinear transcription of the intonation of the utterance "She 
didn't want to go with hirn". 
In the late 20th century a different system of intonational analysis was 
developed, the autosegmental-metrical (AM) approach, which differs from the 
system of the British School in some central assumptions. For example, while 
the British School tradition assurnes that speakers have mental representations of 
pitch rnovernents, the AM model claims that speakers have mental 
representations ofthe two tone targets high (H) and low (L). It is the production 
of combinations of these tone targets in speech that result in the perception of 
pitch movements. Within this theoretical framework the transcription system 
ToBI (which stands for Tones and Break Indices) was developed (Silverman et 
al. 1992). Slightly different ToBI vers ions exist for the transcription ofthe tones 
of American English and British English. 
In ToBI, the star (*) signifies that a tone is associated with an accented 
syllable - an H* thus stands for a high pitch accent. The - refers to a phrase 
accent and the % to a boundary tone, which both occur at the end of intonation 
phrases. Thus, by combining a high pitch accent H* and a low boundary tone 
L% the speaker's voice is falling -
which would be transcribed with a \ 
preceding the accented syllable in the British School system. Most language 
teachers use the transcription system of the British School when teaching 
English intonation, whereas most research articles on English intonation use the 
ToBI system. 
Despite these fundamental theoretical differences, the nuclear tone inventory 
of English proposed by both models is very similar. Table 4.1 above lists the six 
basic nuclear tones of English together with their transcription in the British 
School and in ToB!. The two transcription systems are compared in utterance 
(72), whose intonation was illustrated with an interlinear transcription in Figure 
4.2. The British School transcription given in (72a) marks a falling head and a 
falling nucleus. The equivalent ToBI transcription in (72b) indicates that a high 
pitch accent H* occurs on the syllable did, followed by a low tone L. The 
syllable go receives another high pitch accent H* and the nucleus falls towards 
the low boundary tone L %. 

The phonology of EngIish: intonation 
(na) She ,..,. didn't want to \go 
(nb) She didn't want to go 
H*+L 
H* L% 
4.3.2 The function of English tones and tunes 
121 
The meaning or function of particular tones in English has been analysed on at 
least three different levels: 
-
the attitudinal function 
-
the pragmatic function 
-
the discoursal function 
Many tones in English can convey certain attitudes or emotions of speakers 
such as surprise, politeness, deference or judiciousness. The reply "Yes" to the 
question "Do you like the food" can convey very different meanings depending 
on the tone the speaker chooses. A falling tone usually indicates real approval 
whereas a fall-rising tone conveys doubtfulness. Moreover, tones may be used 
with certain pragmatic aims and indicate different speech acts. Thus, the 
utterance "You've finished" can function as arequest for information when 
uttered with a rising nucleus. When produced with a falling tone, the utterance 
usually has the pragmatic function of a statement. In interactive conversations or 
discourse, tones can signal when a speaker has ended his or her turn and gives 
the floor to someone else. Furthermore, as shown in section 4.2 above, nucleus 
placement signals to the listeners what is new information and what is given 
information. 
In practice, it is difficult to separate the three functions of tones clearly -
often the attitudinal function overlaps with the discoursal function and the 
pragmatic function. They all have in common that they indicate the relationship 
between certain linguistic elements of the utterance and the context in which 
they are uttered. This in turn means that it is impossible to give detailed and 
exact rules for the use of tones by speakers. The possibilities of different 
utterances occurring in different contexts are far too many and the rules would 
be far too complex to be of any use for language learners. In the following, a 
few general tendencies of co-occurrence of some English tones will be given 
that may serve as guidelines for students of linguistics and English language 
learners. It has been suggested that in some respects the function of tones differs 
between American English and British English. Research has furthermore 
shown that many regional accents spoken on the British Isles use tones, 
especially rises, differently from Standard British English. In the following, only 

122 
Chapter 4 
the function of tones in Standard British and Standard American English, RP 
and GA, will be described. 
Falling tones in English are in general associated with finality, 
completeness and definiteness. A nuclear fall typically indicates to the listener 
that the content of the utterance a speaker produces is complete and does not 
require any additions. Thus, they are most often used in declarative utterances 
(see example 73). Commands are also usually uttered with a falling nucleus 
(example 74). Similarly, participants of a conversation indicate the end of their 
turn with a falling nucleus. This signals to the other participants that the speaker 
has nothing further to add and someone else can contribute to the conversation 
now. 
(73) They burst out \laughing 
(74) Give me the \book 
Furthermore, wh-questions in English, i.e. questions that contain a wh-word 
such as "Where do you live" and "What's your name", are also typically 
produced with a falling nucleus (see examples 75 and 76. The ' indicates a high 
level head). When question (77) is spoken with a rising nucleus it sounds very 
friendly and might be appropriate when talking to children or a frightened 
person. This rise has also been labelled 'encouraging rise'. 
(75) 'Where do you \live 
(76) 'What's your \name 
(77) 'What's your /name 
Rising tones mostly indicate non-finality and that the speaker is seeking or 
anticipating information. Moreover, rising nuclei are commonly used in English 
conversation to indicate that the listener is expected to draw a conclusion from 
what was said and to respond. Typical utterances produced with rising nuclei in 
English are yes/no questions - questions that can be answered by "Yes" or "No" 
- as in (78). Here, British English speakers typically produce low rises, whereas 
American English speakers tend to produce high rises. A yes/no question 
produced with a fall as in example (79) sounds insisting and perhaps threatening 
in English. High rises in British English can imply surprise or dis belief. Thus, 
when the pitch ofutterance (78) rises very high, the speaker might express that it 
was impossible to have seen her because (the speaker knows) she could not have 
been there at that time. 
(78) Did you /see her 
(79) Did you \see her 

The phonology of English: intonation 
123 
Equally, rising tones are used in English communication to indicate that the 
speaker has not finished yet and wishes to continue. In utterance (80), it is used 
to link two intonation phrases. In British English, these rises tend to be low rises 
(transcribed with /, starting low and with a finishing point at mid-level). In 
American English, by contrast, high rises are preferred in this context. 
(80) I was cycling horne from /work 1 when I saw this big \bird 
Rising nuclear tones can also be used to imply particular meanings in English. 
For example, a low rise on an imperative as in utterance (81) implies that 
something nice is going to happen and thus appears more reassuring and 
encouraging than a fall. 
(81) Come /here 1 and I'll ,/ give you a \present 
Increasingly, declarative utterances (i.e. statements) in English can be heard 
with nuclear rises. As in example (82), speakers use a rising nuclear tone 
although they are not producing the utterance in order to ask a question or 
because they want to check whether something is the case or not. This 
phenomenon, which is called upspeak, uptalk or high rising terminal (HRT) 
and which occurs in about 3 to 4% of all declarative utterances (e.g. Britain 
1992), was first documented for young female speakers in Australia but can now 
be found in many other accents of English, such as New Zealand English, 
Canadian English, different varieties of American English and some varieties of 
British English. In general, this intonation pattern tends to be used by younger 
speakers and to be frowned upon by older generations. In Belfast English and 
some British regional accents, by contrast, a rising nucleus on declarative 
utterances is the typical intonation pattern and therefore constitutes a different 
phenomenon than the HRT. 
(82) (Where do you come from) 
ILondon 11 
In British English, low rises are associated with utterances that express 
consolation, encouragement and confirrnation. Nuclear fall-rises, by contrast, 
often imply something that is thought to be unpleasant or unwe1come. They 
often occur with warnings, bad news, threats or disagreement. Thus, an 
imperative with a nuclear fall-rise conveys a threat or warning as in (83): 
(83) Go aVway 1 or I'll \hit you 

124 
Chapter4 
Fall-rises are often produced on utterances that express doubt or reservation and 
constitute an appeal to the listener to reconsider. A fall-rise on an utterance like 
"Yes" or "No" typically expresses a response with reserve - when replying 
"Yes" with a falling-rising tone the speaker gives only limited agreement. Thus, 
the response "Can't she" in (84) indicates doubt or polite disagreement: 
(84) (She can't be serious) 
VCan't she 
Furthermore, a fall-rise can be used to signal tentativeness as in (85) or to 
politely correct someone else as in (86). British English speakers also use fall-
rises on yes/no questions to indicate politeness. 
(85) (Is this the way to the station) 
(86) (She got an A) 
AVB 
I Vguess so 
In conversations, a fall-rise can indicate that the speaker has not finished yet and 
that there is more speech still to come, as illustrated in examples (87) and (88). 
Utterances can often be divided into material of primary importance, the topic, 
and material of secondary importance, the comment. Typically, the topic is 
associated with a falling tone, whereas the comment is produced with a non-
falling one. This also applies to constructions in which the comment follows the 
topic, as in (89). 
(87) Ifyou're Vready 1 we can \start 11 
(88) VFirst 1 I'll show you the \bathroom 11 
(89) He was a \doctor 1 my Vfather 11 
A rise-fall, when used with a declarative utterance, typically involves a sense of 
completeness and finality. It can, however, also convey surprise or strong 
feelings of approval or disapproval. Utterances with rising-falling nuclei can 
express arrogance, confidence, self-satisfaction, challenge, putting-down and 
mocking. A speaker saying "That was nicely done" (see example 90) with a 
rising-falling nucleus is highly sarcastic and utters a mockery rather than praise. 
(90) That was /\nicely done 
Surprise is also expressed by a speaker when using a rise-fall-rise as in (91). As 
a nuclear tone, however, this tone is not very common in speech. 

The phonology of English: intonation 
125 
(91) She really went to NFrance again 
Tag questions in English have different pragmatic functions when produced with 
a falling or a rising tone. In (92), the speaker invites an expression of 
confirmation and does not envisage disagreement. This utterance is appropriate 
when the speaker knows that the listener shares his or her belief. Conversely, the 
rising tone in (93) invites contradiction and is used by a speaker who expects the 
listener to know better. Clearly, the speaker did not see the play himself or 
herself and indicates that he or she is only reporting someone else's opinion. 
(92) The play was \nice I \wasn't it 
(93) The play was \nice I Iwasn't it 
In British English, furthermore, a low rise on a tag question might be interpreted 
as amenace as in (94): 
(94) You are \leaving I /are you 
Although the nuclear pitch movements are considered the most important aspect 
of intonation in English, they are rarely associated with a distinct meaning or 
function by themselves. In general, it is a combination of nucleus plus preceding 
pitch accents (the head in the British School tradition) that are associated with 
different meanings or functions. This combination of nuclear tone and preceding 
pitch accents in the head is referred to as a tune. 
O'Connor & Arnold (1973) give a complete list ofthe attitudinal meaning of 
ten different tunes in colloquial British English. For example, the tune consisting 
of a rising head followed by a falling nucleus is often used to express surprise in 
exclamations such as (95): 
(95) ? Where do you \live 
If a high level head is combined with a low fall (transcribed with the symbol J 
as in example (96), an utterance appears categorical, weighty or judicial. It can 
be used in answers to questions and statements where it gives extra weight to 
approvals or disapprovals. 
(96) He is the 'ugliest man I ,know 
Utterances that consist of a high level head and a low rise tend to sound soothing 
and reassuring when produced on a statement (see example 97). It is the typical 
tune for asking yes/no questions in which genuine information is sought and for 
softening commands and wh-questions which might appear too inquisitive. 

126 
Chapter 4 
(97) (Where are you going) 
'Just to post a /letter 
English speakers sometimes make use of stylised patterns of particular tones, 
which appear sung rather than spoken. This is because distinct pitch levels are 
produced instead of a sequence of pitch movements. The calling contour in (98), 
which consists of a step down by an interval of about a minor third from one 
pitch level to another, is an example in point. 
Such stylised patterns are stereotypical, i.e. highly conventionalised and almost 
ritualistic. They occur very frequently in child's play and in formulaic utterances 
such as greetings and conversation openers and closings. 
4.3.3 Pitch range, key and register 
The previous sections have shown that, in English intonation phrases and 
utterances, pitch moves up and down in a systematic, linguistically meaningful 
way. These movements of pitch can differ in their width. A speaker might 
produce a high pitch accent with an extremely high pitch, the highest that is 
physiologically possible. Alternatively, a high pitch can be just a little higher 
than a low pitch accent. Speakers normally produce speech in the bottom third 
of their potential pitch range. For particular linguistic reasons, however, the 
pitch range of an utterance can be varied. The term pitch range refers to the 
difference in pitch height between the highest and the lowest pitch produced in 
an intonation phrase or utterance. Figure 4.4 shows the pitch range of the 
reading of the sentence "A tiger and a mouse were walking in a field." by a 
female British English speaker. The curving line on top illustrates the movement 
of pitch across the utterance (see section 5.5.3 for details on automatic pitch-
tracking) in relation to the words produced. The boundaries of the words are 
marked by the vertical lines. The pitch curve is interrupted where voiceless 
sounds such as the [tl in tiger, the [s] of mouse, the [k] in walking and the [f] in 
field occur. The two pitch accents ofthis utterance can easily be seen in the pitch 
peaks on the syllable ti of tiger and the syUable walk of walking. The pitch range 
of this utterance comprises the area from the highest point reached on tiger to 
the lowest pitch reached onfield, as indicated by the arrows (see section 5.5.3 
for more infonnation on how to measure the pitch range of an utterance). 

The phonology of English: intonation 
127 
By changing their pitch range, speakers also change the key of an utterance. 
The term key describes the pitch level of an utterance in relation to the speaker's 
normal pitch level. A wide pitch range is associated with high key, whereas a 
narrow pitch range goes hand in hand with low key. 
~ 
300,-------------------------------------------------~ 
N 
~ 
200 
B 100 
a: 
o 
/l-~ 
/ '\,r \/ ""'~ 
tiger 
I and I a I 
mouse 
~er, 
walking 
I in I a I 
field 
2.0103 
Time(s) 
Figure 4.4. Pitch range of a reading of"A tiger and amause were walking in a 
jield". 
Both pitch range and key are used to link together successive utterances into 
larger discourse. When speakers produce speech, they do not simply string 
utterances together, but organize their spoken discourse (sometimes also called 
text) in a way so that the individual utterances are grouped together into topic 
units. Pitch range and key are used to indicate the beginning and end of the topic 
units. Typically, high key is produced in order to indicate the beginning of a new 
topic or the change of subject, and low key indicates the end of a topic. The use 
of tones in relation to topic units has also been called paratone, a term used in 
analogy to the term paragraph for written texts. Just like written texts are 
structured into orthographie paragraphs according to their topics and content 
(indicated by the beginning of a new paragraph in print), spoken texts are 
organized into paratones. At the beginning of a paratone, speakers usually use an 
introductory expression (for example "Guess what happened"). The first 
intonation phrase of a paratone is uttered with high key, whereas the end of a 
paratone is usually marked by low key and a lengthy pause. Figure 4.5 shows an 
example of the intonation al structure of the first two paratones of a fairy tale. H 
stands for high key, M indicates mid high key and L refers to low key. (There is 
no agreed transcription system for key, unfortunately.) 

128 
Chapter 4 
The topic of the first paratone - the king - is typically introduced with high 
key; the second intonation phrase "he lived in a beautiful castle" is generally 
produced with mid high key, whereas the last intonation phrase "together with 
his wife and daughter" is usually produced with low key. When a new topic is 
introduced - the prince - a new paratone typically begins with high key. Like in 
the first paratone, the following two intonation phrases in the second paratone 
are normally produced with mid and low key. 
H 
Once upon a time there was a rich kingl 
M 
he lived in a beautiful castlel 
L 
together with his wife and daughterll 
H 
One day a prince knocked at the doorl 
M 
he had come on a black horsel 
L 
that was stamping its feetll 
Figure 4.5. Key ofthefirst two paratones in afairy tale. The symbols H, M and 
L refer to H (high), M (mid) and L (low) key. 
Empirical studies have shown that paratones are marked in this way in different 
speaking styles. In general, the pitch range of speakers is wider when reading a 
text than when retelling a story or producing spontaneous speech in English. 
This can be seen when you compare Figures 4.4 and 4.6. When a speaker reads 
out a sentence, as illustrated in Figure 4.4, the pitch range is much wider than 
when a speaker spontaneously produces the utterance "So the tiger and the 
mouse walking along together" in a retelling ofthe story. 
Several empirical investigations have shown that British newsreaders 
structure their speech into intonationally marked paratones. They produce the 
first paratone with a wide pitch range and on a high key, whereas the last 
paratone is read with a narrow pitch range and a low key. The intonation phrases 
in between the first and last one have an increasingly narrower pitch range. 
Similarly, when you present readers with a written text split into two paragraphs 
they will produce a pause before the second paragraph and begin it with high 
key. When reading a story, English speakers typically introduce a new topic 
with high key, whereas elaborations of a topic are associated with intonation 
phrases beginning with mid high key and background information is marked by 
low key. 

The phonology ofEnglish: intonation 
129 
Not only is the reading of texts but also free speech is structured into topic 
groups by the means of pitch range and key. In lectures, speakers raise their 
pitch by about 50% between the end of one topic and the beginning of a new 
topic (Wennerstrom 1998). Intonation groups that constitute new topics have the 
widest pitch range, followed by intonation groups that are reformulations or 
extensions of what has been said before. The narrowest pitch range is found in 
intonation groups that form subordinations to the topic. This is illustrated in 
example (99), where the first intonation phrase constitutes a new topic, the third 
is an extension of this topic, and the second intonation phrase can be considered 
additional material or subordination. 
300,-------------------------------------------------, 
200 -
J'~-~ 
/J'V'- \ 
'.~ 
J',~ 
~J ~ 
N 
;S 
.<:: 
100 
.B 
i:i: 
O--~~-I-t~e I 
tiger 
landlthel mouse 
I 
walking 
I along 
I 
together 
8.47159 
10.119 
Time(s) 
Figure 4.6. Pitch range 0/ the utterance "So the tiger and the mouse walking 
along together" produced spontaneously by the same speaker as in 
Figure 4.4. 
Low key with a compressed pitch range is often used to indicate additional 
information. In addition, words that present additional infonnation are often 
spoken with a much faster speaking rate and a lower overall loudness than the 
adjacent intonation groups. This can be observed predominantly in parenthetical 
remarks such as asides, certain tag-questions, background information and meta-
discoursal remarks (see example 99 in Figure 4.7). While the topic is produced 
with high key, loud voice and in a 'normal' speech tempo, the aside "wearing 
sunglasses by the way" is spoken on a low key, low voice and in a faster tempo 
before the speaker picks up the key, loudness and tempo of the first intonation 
phrase again. 

130 
Chapter 4 
(99) 'She told me that she noticed hirn 
when she went to the zoo 
Iwearing sunglasses by the waYI 
Figure 4. 7.Pitch range varying with type 0/ information. 
Speakers sometimes also use different registers in their utterances. In contrast to 
differences in pitch range, register involves raising the average pitch level. In 
English and many other languages, a high register is associated with certain 
social or emotional roles. Putting on a 'high voice' in English does not signal any 
linguistically relevant information but is often used by speakers to indicate a 
subservient or deferential role. Since an overall higher pitch is caused by 
tensioning the vocal folds (see also section 2.2), a high register often goes hand 
in hand with speakers' emotional states of stress and tension. 
4.4 The acquisition and teaching of English intonation (advanced reading) 
In order to become competent speakers, both children and adult leamers of a 
language have to acquire the phonological rules of intonational phrasing and 
nucleus placement as well as the association between certain nuclear tones, 
entire tunes and the different types of pitch range with their respective linguistic 
functions. It seems that the acquisition of these prosodic features of speech 
proceeds independently of the acquisition of the speech sounds of a language. 
4.4.1 The acquisition of English intonation in first language acquisition 
Producing English speech that is structured appropriately into intonation phrases 
is a difficult task for children: they do not master this before age six. It appears 
that the acquisition of intonational phrasing is closely intertwined with children's 
general mental and physical development. The first utterances by children 
consist of a single syllable, later a single word (one-word stage). At around two 
years of age, most children produce two words in a row. Yet, these two words, 
although they belong together semantically, are initially not produced as a single 
intonation phrase but rather as two separate intonation phrases separated by a 
pause. Unfortunately, it is impossible to decide wh ether this is due to the limited 
mental abilities of children (can they only think of one thing at a time?), due to 
restrictions in their control of the muscles necessary for speech production (can 

The phonology of English: intonation 
131 
they not keep up the airstream necessary for the production of longer intonation 
phrases?) or due to a lack of amental representation of the phonological unh of 
the intonation phrase. Only at around age 3;5 (three years and five months) do 
children begin to use intonation phrase boundaries systematically (see e.g. Gut 
2000a) in utterances such as (100) 
(100) one piece is Imissing land one piece \missing 
The phonological use of nucleus placement with the function of marking new 
versus given information and focussing is also acquired relatively late by 
children. Utterances produced by most two-year-olds do not have a perceptible 
nucleus: all words are accented equally. The marking ofnew information begins 
at around age three. 9-year-olds place the nucleus on new information in 
statements in 68% of all cases and in questions in 90% of all cases. 
Conversely, the production and meaningful usage of nuclear tones in English 
are acquired fairly early. Some children systematically produce different speech 
acts such as questions and statements with distinct nuclear tones - rises with 
questions and falls with statements - from two years of age on. Similarly, rising 
pitch is used for enumerations and for attracting attention from two years on. 
When children begin to produce utterances with subordinate structures from 
about 2;6 years on, the subordinate part is often linked to the main clause with a 
rising tone. Some authors (e.g. Crystal 1981) even claim a distinct order of 
acquisition for the different nuclear tones. Falls and rises are distinguished first; 
then the difference between high and low falls and rises is acquired. Only after 
that do children produce nuclear rise-falls and fall-rises. 
Moreover, very young children seem to be able to mark utterances with 
different communicative function by a combination of nuclear pitch movement, 
loudness and pitch range. For example, utterances during which eye contact with 
another person is held are typically marked with higher pitch, higher loudness 
and a wider pitch range than utterances during which children appear to be 
talking to themselves as early as at age two. 
4.4.2 The acquisition of English intonation in second language acquisition 
Learners of a second language already have lots of knowledge about the rules of 
the use ofpitch in their first language(s). 1fthis language is a tone language, the 
phonological rules concerning the form and function of pitch patterns will be 
very different from the rules of nucleus placement, intonation and usage of pitch 
range in English. In tone languages, the pitch level or pitch movement on 
syllabi es determines the meaning of words. This means that nuclear tones, the 
tunes of utterances and pitch range are not used for linguistic purposes as they 

132 
Chapter4 
are in English: nucleus placement does not indicate new and given information 
or focus; tones, tunes and pitch range are not used for the expression of 
attitudinal meanings or certain discoursal functions in the same way in tone 
languages. Consequently, native speakers of tone languages have to acquire an 
entirely new phonological system with a whole range of new phonological rules. 
When acquiring English, native speakers of an intonation language like 
Gennan or French have already learned the fundamental concepts of using 
intonation phrases, nucleus placement, pitch patterns and pitch range. 
Nevertheless, individual phonological rules of the meaning and usage of 
particular nuclear tones may differ between the language learner's first and 
second language. For example, in British English a low rising or falling-rising 
nucleus is often produced on statements or commands, which carries certain 
attitudinal implications such as leaving open the possibility of agreement or 
disagreement. Utterance (l 0 1) thus invites the conversational partners to voice 
their own opinion. 
(l 01) That's /hot 
In German, this usage of a low rise or fall-rise does not exist and falls are 
produced instead. German learners of English who produce a falling nucleus in 
these cases may thus inadvertently create the impression that they want to 
impose a belief on the conversation partner. Furthermore, learners of English 
with an intonation language as their native language may find it especially 
difficult to acquire the phonetic aspects of English intonation. For example, 
pitch range in British English is much wider than in German although it is used 
for the same linguistic purposes (Mennen 2007). 
Analyses of the intonation of learners of English show that differences 
compared to native English intonation exist in all areas. German learners of 
English, for example, produce more and shorter intonation phrases when reading 
a text than British English speakers do. Moreover, they insert intonation phrase 
boundaries at places where native speakers do not. By the same token, learners 
of English produce more pitch accents than English native speakers and do not 
use nucleus placement for the marking of new and given information in the 
same way as native speakers do. Often, no distinction is made between the two 
types of information status, and given information receives the same accent as 
new information (e.g. Grosser 1997). For example, in the utterance "You don't 
even like cheese" where cheese is given infonnation, many learners of English 
produce a nuclear pitch accent on the word cheese. In general, there seems to be 
a tendency for learners of English to put the nucleus on the last word of an 
utterance. Similarly, while British English speakers always read sentences 102a 
and l03a with nucleus placement on phone and hi, German learnets of English 
tend to produce 102b and 103b with the nucleus on rang and Nick. 

The phonology of English: intonation 
(102a) The PHONE rang 
(1 02b) The phone RANG 
(103a) Oh HI Nick 
(103b) Oh hi NICK 
133 
Furthennore, learners of English use tones and tunes in a different way from 
native speakers. On the whole, they produce fewer nuclear fall-rises when 
speaking English, and while native English speakers often produce the tune 
rising head + falling nucleus (/' +\), learners of English have a tendency to 
produce falling heads. When measuring the phonetic extent of nuclear falls, it 
can be shown that learners of English produce shorter falls that do not reach the 
same bottom baseline. Finally, the overall pitch range in all speaking styles is 
narrower for learners of English than for native English speakers. Equally, new 
topics are marked by a wider pitch range by native speakers than by language 
learners. 
4.4.3 Teaching English intonation 
As with any other aspect of English phonetics and phonology, the acquisition of 
English intonation is not impossible - even for late learners who begin learning 
English after early childhood. Some learners acquire the rules of English 
intonation perfectly and are indistinguishable from native speakers. It is still not 
weil established in what way teaching can contribute to successful acquisition. It 
appears that creating language awareness is beneficial for learning a new 
intonational system. Exercises that raise language awareness in the area of 
intonation can for example show learners in what way their intonation of 
English differs from the intonation of native speakers. This can be achieved by 
visualising the pitch movement or pitch range of utterances with the help of 
specialized software (see sections 5.5.3 and 5.7). Figure 4.8 shows the pitch 
movement and pitch range of a reading of the sentence "A tiger and a mouse 
were walking in a field." by a German learner of English. Comparing this to 
Figure 4.4, which illustrates the pitch range of a native English (RP) speaker, 
distinct differences can be seen: the pitch range ofthe learner ofEnglish is much 
reduced and far more pitch accents (visible as the pitch peaks indicated by the 
arrows) are produced. 
By being able to see differences between their own intonation and the 
intonation of a native speaker, language learners gain language awareness and 
might be able to change their intonational patterns in the direction of native 
speaker values. 

134 
Chapter 4 
300,--------------------------------------------, 
200 
N :s 
.c 
100 
.'l 
c:: 
o a I tiger 
land H mouse I 
were I walking 
Hai 
field 
o 
1.81243 
Time (s) 
Figure 4.8. Pitch range 01 a reading ol"A tiger and a mouse were walking in a 
fleld. " produced by a German learner 01 English. 
4.5 Exercises 
1. What is an intonation language? 
2. What is a pitch accent? 
3. How can one show that pitch movement is linguistically relevant? 
4. What is intonational phrasing used for by English speakers? 
5. How do speakers signal the end oftheir turn? 
6. Mark where an intonation phrase boundary is appropriate in the following 
utterances: 
a. I'm not absolutely sure to be honest 
b. She seemed to spend most of the day in bed with a crime novel 
c. He usually leaves at nine doesn't he 
d. No that's not acceptable 
e. I went to buy a j acket a skirt and a pair of shoes 

The phonology of English: intonation 
135 
7. Mark the nucleus placement in the following utterances: 
a. Did you have a lot to do 
b. Yesterday I went to the doctor twice 
c. Has your mother left already 
d. Do you know when the first train leaves 
e. She is trying to fix the washing machine 
8. Mark the appropriate tone in the fOllOwing utterances: 
a. Is this a joke 
b. That's wonderful (enthusiastic) 
c. All right (doubtful) 
d. W ould you like some tea 
9. Listen to recordings 04_exercise9a, band c on the CD-ROM and transcribe 
the intonation ofthe utterances using the British School transcription system. 
10. Record yourself saying the utterances in recordings 04_ exercise9a, band c 
(instructions are included in the Praat manual on the CD-ROM) and compare 
your nucleus placement, intonational phrasing and intonation with that of the 
native speaker. Which differences do you notice? 
11. What are the major difficulties for leamers ofEnglish regarding intonation? 
Why? 
4.6 Further Reading 
Wells (2006) presents a very detailed and accessible description of (British) 
English intonation, including intonational phrasing, nucleus placement and the 
form and meaning of pitch movements. It is written for students of English and 
contains many examples and practical exercises as weIl as a CD-ROM. 
Cruttenden (1997) also offers an excellent overview of all aspects of (mostly 
British) English intonation. Roach (1991, chapters 15 to 19) gives a detailed 
introduction to British English intonation in the British School tradition together 
with practical exercises on audio cassettes. For general introductory reading on 
the attitudinal function of tunes in British English see O'Connor & Amold 
(1973), who also offer many practical exercises. 

136 
Chapter4 
For advaneed students, short overviews of the British English and the 
Ameriean English intonation systems are presented in Hirst (1998) and Bolinger 
(1998) respeetively, but they are based on yet another system of intonation 
transeription. The development ofthe ToB! transeription system and its history 
are deseribed in Beckman et al. (2005) and ean be found on the website 
<http://www.ling.ohio-state.edu/~tobil>. 
Teneh (1996) gives many examples of the grammatieal funetion of 
intonational phrasing in British English and eompares it to the grammatieal 
funetion of intonational phrasing in German. Many aspeets of the discourse 
intonation of English are deseribed in Wennerstrom (2001). For advaneed 
students, Pierrehumbert & Hirschberg (1990) present a deseription of the 
meaning of Ameriean English intonation in the ToB! framework. Brazil (1975) 
deseribes the usage of key in English discourse. More details on the intonational 
marking ofparatone are given in Brown & Yule (1983). 
Vihman (1996, ehapter 8) gives an introduetion to the prosodie development 
in first language aequisition. Trouvain & Gut (2007) eontains a eolleetion of 
aliicles on seeond language aequisition and teaehing of various prosodie 
features, and Chun (2002) deseribes discourse intonation by language leamers. 

5 Acoustic properties of English 
Chapter 2 explained how speech is produced by humans; chapter 6 explains how 
speech is perceived by humans. However, after having been produced by a 
speaker and before being perceived by a listener, speech exists as a physical 
property -
it 'floats through the air'. This chapter is concerned with the 
properties of speech as it is transmitted by air. The study of the physical 
properties of speech and of how they are perceived by listeners is called 
acoustics. Acoustic studies of speech have only become possible relatively 
recently: new technologies have enabled researchers to make speech 'floating 
through the air' visible and to measure its acoustic features. With the help of 
phonetic analysis software, we can see many of the acoustic properties of 
speech, for example the pitch movement across the utterance "Please help me" 
produced by a British English speaker shown in Figure 5.1. The acoustic study 
of speech has also enabled researchers to synthesize speech, i.e. to build 
machines that speak (nowadays many children's toys 'speak', and artificial voices 
talk to us from nearly every device ranging from our washing machine to the 
navigation system in our car). Similarly, with our increasing knowledge of the 
acoustic properties of speech, machines have been developed that 'und erstand' 
speech. Y ou will have encountered them in telephone bookings or telephone 
enquiries of any kind. 
30 0 
I 
: 
0 \~ ~ 
~ 
20 
(). 
0 
please 
I 
help 
me 
0.986125 
Time (5) 
Figure 5. J. Pitch movement across the utterance "Please help me". 

138 
Chapter 5 
The possibility to analyse the acoustic structure of speech has enabled 
linguists to improve their descriptions of some sounds. For example, it is easier 
to characterize vowels by describing their acoustic nature than their underlying 
articulatory movements. Furthermore, an acoustic study of speech recordings 
reveals many properties of speech that cannot be heard even by trained listeners. 
It appears, for example, that when English speakers produce an accented 
syllable, they do this mainly by increasing the height of pitch on the vowel, 
whereas speakers of German typically produce greater loudness on an accented 
syllable compared to an unaccented one. This finding is of course of great 
importance for a native German speaker aiming to acquire the pronunciation of 
English. 
The study of the acoustic properties of speech can thus be applied in many 
areas such as language teaching, clinical linguistics and speech therapy, and 
forensic linguistics. Teachers of English, for example, can show their students 
differences between their speech and that of native speakers. This knowledge 
can further be used to develop new teaching materials and techniques. By the 
same token, differences between the speech of people with speech disorders and 
healthy speakers can be described and used to develop therapeutic interventions. 
Finally, forensic phoneticians can give acoustic evidence at court in trials where 
it is important to ascertain whether the voice recorded on an answerphone is the 
one of a person accused of blackmail or not. The next section provides abrief 
introduction to the basic principles of the acoustic structure of sound. Sections 
5.2 and 5.3 are concemed with the acoustic properties of English vowels and 
consonants. In sections 5.4 and 5.5 the acoustic properties of connected speech 
and English intonation are described. Section 5.6 shows how acoustic phonetics 
can be applied in English language leaming and teaching. Section 5.7 explains 
what needs to be considered in order to make a good speech recording. 
5.1 Acoustic properties of sound 
The sensation of sound is caused by movement. Whether you drop a saucepan 
lid, whether you sIam a dOOf or whether you make an airstream pass from your 
lungs through your mouth 
you have caused air to move. To be more precise, 
you have caused sm all variations in air pressure that occur very rapidly. You can 
feel them by putting your hand in front of your mouth while talking. These 
variations in air pressure travel (the technical term is 'propagate') through the air, 
and when they reach the eardrum of a listener, they set in motion the process 
which results in the perception of a sound (see chapter 6). When recording 
speech, it is these variations that are picked up by the microphone. The 
propagation of sound does not depend on the medium air of course. It is 

Acoustic properties of English 
139 
perfectly possible to transmit speech under water - yet, the properties of the 
medium water create a very different sensation of sound. 
Sounds can travel across long distances and they do so in the form of sound 
waves. Since the type of air pressure variations caused by the production of 
different speech sounds can differ considerably, their corresponding sound 
waves differ as weIl. For a production of a vowel, for example, the vibrating 
vocal folds chop up the airstream into regular pulses that alternate between high 
pressure and low pressure. For plosives, the airstream is stopped completely and 
then rushes out with great force. Accordingly, the sound waves of vowels and 
plosives that are transmitted through the air have very different properties. The 
properties of the various sounds of English will be described in detail in sections 
5.2 and 5.3. 
5.1.1 Acoustic properties of sound waves 
Sound waves can be made visible in various forms. When you record some 
speech and transfer this recording to a computer, you can employ speech 
analysis software to create a waveform, the visible representation of the sound 
waves of the sounds in the recording. Figure 5.2 shows the waveform of the 
utterance "Please help me". Listen to it on the CD-ROM (05 ylease.wav). 
0.986125 
Time (s) 
Figure 5.2. Waveform ofthe utterance "Please help me". 
Two acoustic properties of this utterance are represented by the two axes of the 
waveform: duration and intensity. The horizontal axis shows the duration ofthe 

140 
Chapter 5 
speech signal (the utterance is approximately 0.98 seconds long). Moreover, the 
duration ofeach part (e.g. word, syllable and phoneme) ofthe utterance can be 
measured. Since speech events are typically very short, the unit of measurement 
is either seconds (s) or milliseconds (ms). The vertical axis in Figure 5.2 
indicates the variation in air pressure across the various parts of the utterance. 
This variation of pressure - which is perceived as loudness - corresponds to the 
physical property of intensity. Intensity is proportional to the amplitude of the 
waveform, which is visible in the relative distance of each point from the zero 
line. In other words, the height of each portion of a wavefonn above the zero 
line indicates the height of air pressure. Portions of the waveform below the zero 
line correspond to low air pressure. In Figure 5.2 you can see that there are three 
places ofvery high amplitude and some intervening places with lower amplitude 
in the utterance. Intensity is a relative value and has to be calculated with 
reference to some specific intensity level. In speech analysis software packages, 
it is usually expressed relative to the smallest amplitude value in the wavefonn. 
The most widely used unit of measurement for the intensity of a speech signal is 
decibel (dB). Because sound intensity is proportional to the square of the 
amplitude, decibel is a logarithmic measurement. This means that a sm all 
increase in dB values corresponds to a much larger increase in intensity and 
perceived loudness. As a rule of thumb, when asound is perceived as about 
twice as loud as another, the increase in intensity is 5 dB (see section 6.5 for 
details on the relationship between acoustic properties and human perception). 
Figure 5.3 illustrates the intensity changes across the utterance "Please help me" 
measured in dB. You can see that the intensity peaks occur exactly where the 
vowels in the utterance are. The two plosives [p] in please and help have the 
lowest intensity. 
One other important phonetic fact can be seen when observing the waveform 
in Figure 5.2. It immediately becomes clear that there are no boundaries between 
individual speech sounds and individual words in an utterance (this was 
explained with reference to articulation in section 2.5). Rather, each utterance in 
speech consists of a continuous sequence of sounds which have no visible (and 
indeed audible) boundaries between them. In Figure 5.3, the vertical dotted lines 
indicate the beginning and end of each sound in the utterance "Please help me". 
Due to the continuous nature of speech, these boundaries are not easy to draw. 
When you listen to each of the sounds between the given boundaries, you will 
easily be able to tell the influence of the neighbouring sounds. For acoustic 
analyses of individual speech sounds, especially vowels, it is thus the linguistic 
convention to indicate the 'stable parts' ofthe sounds. Stable parts are those parts 
of a speech sound that are not influenced by the neighbouring sounds. 

Acoustic properties of English 
iii 
"-
~61.76 
.~ 
Figure 5.3. Intensity curve ofthe utterance "Please help me". 
141 
When you select from Figure 5.2 just the portion of the waveform that 
represents the vowel [i] and zoom it in on the computer, the waveform 
illustrated in Figure 5.4 will be displayed to you. This waveform appears very 
regularly pattemed with regularly occurring peaks and valleys in air pressure. 
-0.2122.J--------------'-------I 
o 
0.027 
Time (8) 
Figure 5.4. Waveform ofthe vowel [iJ in please. 

142 
Chapter 5 
Each repetition of the pattern is called a cycle. When you measure the period of 
time each cycle takes, you can determine the frequency of asound. The 
frequency of asound is defined as cycles per second. In the [i] shown in Figure 
5.4, one cycle is 0.0054 seconds long (the entire recording is 0.027 seconds long 
and contains 5 cycles, so you divide 0.027 by 5 and arrive at 0.0054). To 
calculate the number of cycles per second of the [i], you now divide 1 (one 
second) by the length of one cycle (0.0054) and arrive at approximately 185. 
This means that the cycles of this vowel occur about 185 times per second, 
which in turn tells you that the speaker's vocal folds were vibratirrg 
approximately 185 times per second. The vibration of his vocal folds chopped 
up the airstream into approximately 185 puffs per second. Measurements of the 
frequency of a sound are expressed with the unit Hertz (abbreviated Hz). The 
frequency ofthe [i] in Figure 5.4 is thus 185 Hz. 
5.1.2 Simple and complex waveforms (advanced reading) 
The waveform shown in Figure 5.4 is a periodic waveform, which means that 
its cycles occur at regular periods oftime. Periodic or quasi-periodic waveforms 
are typically produced when speakers articulate vowels and voiced consonants 
(because they involve vocal fold vibration and thus a regular chopping up of the 
airstream). The periodic waves of voiced speech sounds are called complex 
periodic waves. This term reflects that complex periodic waves are composed of 
a combination of several simple periodic waves or sine (short for sinusoidal) 
waves. A sirre wave is a wave that has only one frequency. The waveform of a 
sine wave with the frequency of 200 Hz is displayed in Figure 5.5. Simple sine 
waves do not occur in real speech but are often used in the assessment of 
hearing. There they are referred to as pure tones (see section 6.6 for details). 
0 .. 50<-----,..-------,..-------,..-------,..------" 
-O.5,-I-----""------""------""------""------""----j 
o 
Time{s) 
Figure 5.5. Waveform of a pure tone of200 Hz. 

Acoustic properties of Eng1ish 
143 
A complex wave consists of several sine waves that differ in frequency. 
Thus, acoustically speaking, the voiced sounds in speech consist of a 
combination of various sine waves. Each of them has a different frequency and 
they differ in phase. The term phase refers to the relative beginning of each 
cycle ofthe wave. Two sine waves with the same frequency but which are out of 
phase are displayed in Figure 5.6. Y ou can see that they reach the amplitude 
maximum at different points in time. 
0.5r--------r~--r~-----""T',__""'71 
0.01 
Time(s) 
Figure 5.6. Waveform oftwo pure tones of200 Hz, out ofphase. 
When you combine several sine waves with different frequencies and different 
phase you arrive at a complex period waveform as the one illustrated in Figure 
5.4 for the vowel [i] inplease. You can see that within each cycle several peaks 
of amplitude occur - these reflect the peaks of each of the underlying simple 
waves and show you how they are out of phase with each other. Figure 5.7 
illustrates the components of a complex sound wave consisting of three sine 
waves with the frequencies 150 Hz, 300 Hz and 450 Hz, respectively. 
Listen to each of the individual sine waves 
shown 
in 
Figure 
5.7 
(05_sineI50.wav; 
05_sine300 .wav; 05_sine450.wav) on the CD-
ROM. The sound that results when you play all 
three of the sine waves at the same time - the 
complex waveform displayed in Figure 5.8 -
sounds like 05_ complexwave.wav on the CD-
ROM. 

144 
Chapter 5 
Figure 5.7. Waveforms ofthree sine waves with 150 Hz (top), 300 Hz (middle) 
and 450 Hz (bottom) frequency and different amplitudes. 
The sine wave with a frequency of 150 Hz has a higher amplitude than the sine 
wave with a frequency of 300 Hz, which in turn has a higher amplitude than the 
sine wave with a frequency of 450 Hz. In Figure 5.8, you can see the complex 
waveform that results when you combine these three simple waves. When you 
listen to the three sine waves and the resulting complex wave on the CD-ROM, 
you will notice that the complex wave has the same pitch (height of tone, see 
section 2.2 for a description of the articulatory properties of pitch) as the sine 
wave with 150 Hz frequency. The frequency of the lowest sine wave in a 
complex periodic wave is called the fundamental frequency, abbreviated as 
FO. This is the frequency that determines the pitch that is perceived. (Since the 
pitch of the vowel [i] in Figure 5.4 is at 185 Hz, you now know that the lowest 
sine wave of this sound lies at 185 Hz). The sine waves with frequencies above 
this fundamental frequency are referred to as the harmonics (they are called 
overtones in music). Their frequency stands in a mathematical relationship to the 
fundamental frequency. To be precise, they are integer multiples of the 
fundamental frequency. Thus, when the fundamental frequency is 150 Hz, the 

Acoustic properties of English 
145 
first harmonie will have a frequency of 300 Hz and the next a frequency of 450 
Hz. The tenth harmonie has a frequency of 1500 Hz and so on. 
0.8248·"'----,,-----------yr------,r------,,---, 
-0. 8248+------'"-----'L----..lL---lL---J 
o 
0.03 
Time (s) 
Figure 5.8. Complex waveform consisting ofthree sine waves with 150 Hz, 300 
Hz and 450 Hzfrequency, respectively. 
It is the vocal fold vibration in the larynx that produces the complex periodic 
wave of voiced speech sounds. The fundamental frequency of this wave 
corresponds nearly exactly to the rate ofvocal fold vibration. Thus, asound with 
a fundamental frequency of 150 Hz is produced by the vocal folds completing 
150 opening and c10sing cyc1es per second. The amplitude of each of the 
harmonics decreases so that the amplitude of the third harmonie is lower than 
that of the second but higher than that of the fourth and so on. 
If you heard a recording of the complex wave produced by vocal fold 
vibration it would not sound like speech to you. This is because when passing 
the vocal tract, the complex wave is modified significantly. These alterations 
that result in what we perceive as speech sounds are caused by the physical 
properties of the pharynx and the oral and nasal cavity. In acoustic tenns, the 
vocal tract acts as a resonator and acoustic filter. In a simplified manner this 
can be explained as folIows: when the vocal folds chop up the airstream into 
little puffs ofvibrating air and this vibrating air reaches the pharynx and oral and 
nasal cavities, specific frequencies in the complex wave cause specific tissues 
and membranes in the vocal tract to vibrate also. It also causes the bones in your 
head to vibrate, which you can feel especially c1early when you cover your ears 
while speaking or singing. When one vibrating body causes another body to 

146 
Chapter 5 
vibrate with maximum amplitude at its natural frequency, which also happens to 
be the natural frequency ofthe first vibrating body, this is called resonance. 
When one body vibrates with maximum amplitude at one particular 
frequency, this in turn means that other frequencies are dampened. This 
selective enhancing and damping of frequencies is called flltering. An acoustic 
filter thus passes or blocks speciflc frequencies of a complex wave. Low-pass 
filters block or reduce in intensity some higher frequencies, high-pass filters 
block or reduce in intensity some lower frequencies. By altering the position of 
your tongue, velum and the other articulators you alter the resonating function of 
your vocal tract. For example, when you change the position ofyour tongue and 
lips from the articulation of [a] to articulating [u], the shape of your vocal tract 
changes and thereby modifIes its resonance, i.e. the particular frequencies of the 
complex wave produced by the vocal fold vibration that are enhanced and 
dampened. When you elose your lips and lower your velum, you again cause 
different patterns of resonance in your vocal tract, particularly in the nasal 
cavity, which result in the sound [m]. 
As an upshot of all this, it can be summarized that the complex wave leaving 
a speaker's mouth consists of several sine waves of different frequencies, some 
of which have a higher intensity than others. The intensity of each frequency in 
a voiced speech sound can be made visible in apower spectrum. Figure 5.9 
illustrates the spectrum of the vowel [i] in please. The vertical axis expresses the 
intensity (sound pressure level) and the horizontal axis the different frequencies 
contained in the vowel. Y ou can see that only some of the frequencies have a 
high intensity (in particular the fundamental frequency at 185 Hz, which has the 
highest intensity). This is caused by the particular position of the speaker's 
tongue during the articulation of this vowel - a high tongue position with the 
front of the tongue elosest to the roof of the mouth. 
Another way of visualizing the intensity of the different frequencies of 
voiced speech sounds is the spectrogram. Speech analysis software usually 
creates spectrograms from recordings by performing a Fourier analysis (named 
after Jean Baptiste Fourier, who showed in the early 19th century that complex 
waves can be analysed as the sum of their sine wave components). With this 
analysis, the individual frequencies of each speech sound and their 
corresponding intensities can be made visible. Another technique for examining 
the spectral properties of sounds, which is offered by many speech analysis 
packages, is the linear prediction coefficient analysis (LPC). 

Acoustic properties of English 
147 
40 
N 
~ 
"' 
20 
'" 
~ 
~ 
~ 
~ 
<J) 
12000 
Frequency (Hz) 
Figure 5.9. Spectrum of the vowel [i} in please. 
Figure 5.10 shows a spectrogram of the utterance "Please help me". The 
boundaries of each speech sound are indicated by the dotted !ines. Y ou can see 
that in a spectrogram, time or duration is displayed on the horizontal axis, the 
different frequencies are shown on the vertical axis and the intensity of 
particular frequencies is expressed by the colour. Black areas thus indicate 
frequencies at high intensity, whereas grey areas indicate frequencies at lower 
intensity. Y ou can see that all the voiced sounds, the vowels as weil as [I] and 
[m], have bands of frequencies with high intensity - indicated by the black bars 
in the spectrogram. These frequency areas with high intensity are called 
formants and will be discussed in detail in the next section. 
There are two ways of displaying the different intensities of different 
frequencies of sounds in a spectrogram. In a broadband spectrogram such as 
shown in Figure 5.1 0, the individual harmonics are not clearly visible but are 
rather lumped together as broad areas ofhigh energy. It is thus relatively easy to 
detect changes in these formants over time. In the creation of narrowband 
spectrograms, by contrast, the frequency dimension of speech signals is 
displayed very accurately. However, this is at the expense of exact resolution in 
the time dimension, which is higher in broadband spectrograms. Figure 5.11 
shows a narrowband spectrogram of the utterance "Please help me". Especially 
in the lower frequency areas, the individual harmonics are clearly visible as thin 
!ines. 

148 
time (s) 
Figure 5.10. Broadband spectrogram ofthe utterance "Please help me". 
N 
I 
>-
u c 
Ql 
:::J er 
Ql 
U: 
olL§~~~U 
o 
0.986125 
Time(s) 
Chapter 5 
Figure 5.11. Narrowband spectrogram ofthe utterance "Please help me". 

Acoustic properties of English 
149 
In Figure 5.10, you can also see that the voiceless speech sound [p] looks 
different from the voiced sounds in the spectrogram. It has one large area of 
high intensity that covers many different frequencies. This is because [p], like all 
other voiceless speech sounds, does not consist of periodic but of aperiodic 
waves. This means that its waves do not have a regular pattern like voiced 
speech sounds but instead consist of waves of many frequencies with random 
amplitude and phase. This is illustrated in Figure 5.12, which displays the 
waveform of a [s]: no regular pattern is visible. Sounds that have underlying 
aperiodic waves are called noise, so that all voiceless speech sounds are strictly 
speaking noise rather than sounds. Section 5.3 gives more details about the 
acoustic properties of voiceless sounds. 
0.05319',------------,-----,--------, 
0.00310091 
Time (5) 
Figure 5.12. The aperiodic waveform of a [s]. 
The complex wave that is produced by vocal fold vibration in the larynx is 
referred to as the sound source. As described above, this is changed 
significantly when it passes the vocal tract (i.e. through the throat and oral and 
nasal cavities). You have also seen that, in acoustic terms, the vocal tract can act 
as a frequency-sensitive filter. Thus, the acoustic model of speech production is 
called the source-fiIter model. 
Table 5.1 summarizes how the various acoustic measurements are related to 
articulatory features and perceptual features. (For speech perception see chapter 
6.) What is measured in the speech signal as the fundamental frequency 
corresponds to the quasi-periodical vibrations ofthe vocal folds and is perceived 
as pitch. The measurement of intensity captures articulatory effort and subglottal 
air pressure and is perceived as loudness. The duration of speech elements 

150 
Chapter 5 
corresponds to the duration of the speech gestures and is perceived as length. 
The formant values measured for vowels retlect the vocal tract configuration 
and are perceived as vowel quality. 
Table 5.1. Relationship between articulation, acoustic features and perception. 
Articulation 
Acoustic features 
Perception 
quasi-periodic vibration of fundamental frequency (FO) 
pitch: low - high 
vocal folds 
measured in Hz 
articulatory effort, 
subglottal air pressure 
duration of speech 
gestures 
vocal tract configuration 
intensity 
measured in dB 
duration 
measured in ms 
formant values 
measured in Hz 
5.2 The acoustic properties of English vowels 
loudness: soft -loud 
length: short - long 
vowel quality: 
reduced - full 
types of vowels 
Section 5.1.2 above explained that vowels (and other voiced speech sounds) 
consist of periodic sound waves with different frequencies at different 
intensities. The lowest frequency is the one listeners perceive as pitch (the 
height of voice), the higher frequencies, which are called harmonics or 
overtones, provide the particular quality of the vowel. Of the frequencies 
produced by vocal fold vibration in the larynx, some are enhanced and some are 
blocked by the resonating properties of the vocal tract, depending on the size of 
the vocal tract and the shape and position ofthe speaker's articulators such as the 
tongue, lips and the velum. This means that the shape and position of the 
articulators and the size of the vocal tract determine the quality of vowels, i.e. 
the type of vowel a speaker produces and a listener perceives. You can test this 
easily by pronouncing the vowels [al, [i] and [u] in a row and on the same pitch 
level. Y ou can fee! the change in the position and shape of your tongue as weil 
as in the position of your lower jaw when going from [a] to [i]. The movement 
of your tongue and jaw changes the size and shape of your oral cavity, and this 
in turn influences which of the frequencies are enhanced in intensity and which 
are blocked. When going from [i] to [u], it is the position ofyour tongue and lips 
that changes and thereby causes different frequencies in the waves to be 

Acoustic properties of Eng1ish 
151 
enhanced or blocked. Those frequencies that have very high intensity (= energy) 
are called formants. Thus, formants are the most important acoustic cues for 
vowels - they distinguish the different vowels of a language. 
With the help of a speech analysis program it is possible to see and measure 
the formants of English vowels. Figure 5.13 presents a spectrogram of the 
vowels [i], [r], [e], [re], [0], [::>], [u] and [u]. The dark horizontal bands indicate 
the greater relative intensity of particular frequencies, i.e. the formants, in each 
vowel. Most of the vowels in Figure 5.13 have between three and five 
distinguishable formants. The formant with the lowest frequency is called Fl, 
the second F2, the third F3 and so on. For the differentiation of vowels in a 
language it is sufficient to refer to the first two or three formants. The higher 
formants are especially important in music and singing, where they differentiate 
between the timbres of different musical instruments and voices and, in the case 
of the singer's formant at around 3000 Hz, enable singers to be heard over an 
orchestra. 
Figure 5.13. Spectrogram 01 the vowels [i], [r], [e], [re], [0], [::>], [u] and [u] 
produced by an RP speaker. 
Figure 5.13 illustrates that the first formant in the vowel [r] is higher than in the 
vowel [i]. Equally, it is higher in [e] than in [r] and higher in [re] than in [e]. 
When you compare F1 in the vowels [0], [::>], [u] and [u] you can easily see that 
its height decreases steadily. Thus, it be comes clear that the position of F1 is 
correlated with tongue height. The lower the tongue during the production of the 
vowel (as in [re] and [on, the higher is F 1. V owels produced with a high tongue 

152 
Chapter 5 
position such as [i] and [u], correspondingly, have a low Fl. There is thus an 
inverse relationship between tongue height and height of F 1 in vowels. The 
relationship between vowel type and the second forrnant is more complicated. 
Figure 5.13 illustrates that there is some correlation between back vowels such 
as [n], [J], [u] and [u] and a relatively low F2 - the second forrnant in the front 
vowels [i], [I], [e] and [re] is clearly higher. Yet, F2 is also considerably affected 
by whether the vowel is produced with rounded or spread lips . 
..--.. 
N 
I 
>. 
c..> c: 
0) 
:::J 
0-
0) 
L.. 
L.1.. 
Figure 5.14. Theformants ofthe vowels [i], [I], [e], [re], [n], [J], [u] and [u] 
produced by an RP speaker. 
Speech software packages usually offer an automatie analysis of forrnants that 
allows a measurement of the actual frequencies of the forrnants. Figure 5.14 
displays the formants of the vowels in beat, bit, bet, bat, pot, bought, put and 
boot as given by the speech analysis software Praat (which can be downloaded 
for free at http://www.praat.org). The exact value of each forrnant frequency is 
displayed and can be measured and expressed in the unit Hertz (Hz). When 
measuring formant height in individual vowels, it is important to choose an 
appropriate place for this. The sounds preceding and following a vowel 
influence its formant shape. It is thus customary to select the 'stable part' at the 
mid-point of the vowel for forrnant measurements. This is of course far from 
straightforward when the vowel in question is very short. 

Acoustic properties of English 
153 
Several researchers have measured the average frequencies of F 1 and F2 in 
the different vowels of English. Table 5.2 gives these values for both Standard 
British English and Standard American English. 
Table 5.2. Typical formant frequencies (in Hz) of British English and American 
English vowels (adaptedfrom Kent & Read 2002, p. 111/ and 122). 
Fl 
F2 
British 
American 
British 
American 
English English 
English 
English 
300 
290 
2300 
2250 
360 
400 
2100 
1930 
e 
570 
1970 
c 
550 
1800 
Q 
680 
710 
1100 
1100 
u 
380 
440 
950 
1220 
u 
300 
330 
940 
1200 
A 
720 
600 
1240 
1300 
3 
580 
470 
1380 
1350 
When interpreting the values in Table 5.2, you have to bear in mind that these 
formant values are average values that were measured across several speakers. 
However, considerable variation in formant frequencies can occur between 
different speakers. As explained in section 5.1.2, the harmonics of every 
fundamental frequency have a mathematical relationship; they are multiples of 
each other. This means that the formant frequencies depend on the fundamental 
frequency of asound. The fundamental frequency itself corresponds to the rate 
ofvibration ofthe vocal folds, which, in turn, depends on the length, tension and 
thickness of the vocal folds. Thus, speech produced by speakers with relatively 
short vocal folds such as children has a much higher fundamental frequency and 
consequently different fonnant frequencies than speech produced by speakers 
with longer vocal folds (see section 5.3 for details). Equally, the length of a 
speaker's vocal tract influences the position of the formants, so that the same 
vowel produced by speakers with shorter pharynxes has different formant 
frequencies than when produced by speakers with longer pharynxes. However, 
even when speakers vary in the acoustic realization of vowels, they always 
maintain the phonological contrast between different vowels. For example, even 

154 
Chapter 5 
if the [e] produced by one speaker has similar acoustic properties to the [I] 
produced by another speaker, the first speaker's [e] and [I] will be sufficiently 
distinct from each other. Listeners adjust to the particular acoustic properties of 
individual speakers and normalize them in perception (see also section 6.7). 
Formant frequencies further vary between varieties of English. Table 5.2 
shows that F2 in the vowels lul and lul is much higher in American English than 
in British English. This means that these vowels tend to be more fronted in 
American English and/or have less lip rounding than in British English. 
Furthermore, you can see that the IP A symbols used for the different vowels are 
only approximations and that they cannot represent the actual acoustic properties 
of vowels. Tradition may be the reason why a vowel with similar F 1 and F2 
values is transcribed as lei in British English and as lEI in American English. 
This variation of formant frequencies between speakers and within speakers 
means that listeners cannot rely on absolute frequency values for the 
identification of vowels. It is the relative position of F 1 and F2 that make 
listeners perceive one vowel or another (see section 6.7 for more details on 
vowel perception). 
2500 
2000 
1500 
1000 
500 
0 
0 
200 
.i 
• 
u 
• 
3 •• u 
- 400 
• e 
600 
• 
A 
• 
Q 
800 
Figure 5.15. Acoustic map for some American English vowels. F2 values on top, 
F 1 values on the right. 
The average formant frequencies ofvowels can be taken to plot an acoustic map, 
in which the vowels are positioned according to the frequency of their first two 
formants. Figure 5.15 shows the formant chart for American English vowels 
based on the measurements ofthe formant frequencies in Table 5.2. The values 
of F 1 are displayed in inverse order on the vertical axis, whereas the values of 

Acoustic properties of English 
155 
F2 are displayed on the horizontal axis, again in inverse order. Thus, in a 
slightly unusual arrangement, the zero values of both axes appear in the top 
right-hand corner. This arrangement shows immediately that the vowels are 
positioned in such an acoustic map similar to how they are usually arranged in a 
vowel chart like the one drawn in section 3.1.3, where vowels were positioned 
based on their articulatory properties. Thus, the formants of vowels demonstrate 
a elear relationship to the traditional articulatory description of vowels. Tongue 
height is inversely correlated with height of F I. F2 varies with tongue retraction 
(back and front position) and lip rounding. Nevertheless, the better we are able 
to examine real articulatory movements with techniques such as X-ray and 
EMMA (described in section 2.8), the more obvious it becomes that vowels and 
the differences between vowels can be expressed much more accurately in 
acoustic terms than in articulatory tenns. When analysing real speech, a 
speaker's tongue position when artieulating an [i] is far less reliably high and 
front than FI being low and F2 high. Listeners perceive an [i] always with this 
partieular constellation ofthe first two formants but mayaiso perceive [i] with a 
tongue position other than high and front. It is thus much easier to establish 
relationships between acoustic and perceptual parameters of vowels than 
between articulatory and perceptual parameters. 
Some researchers have proposed further ways of measuring the acoustic 
properties of vowels apart from measuring formant frequencies. These inelude 
intrinsic vowel duration, intrinsie vowel fundamental frequency as weH as 
formant movement due to adjacent consonants. It was found that some vowels 
are generally (i.e. intrinsically) longer than others. For example, low vowels 
such as [a] are longer than high vowels such as [u]. Moreover, high vowels such 
as [i] have a slightly higher fundamental frequency than low vowels such as [a] 
(Lehiste 1970). In addition, the speech sounds surrounding vowels, especially 
plosives and fricatives, modity the position of the formants in vowels. Bilabial 
sounds such as [b] and [p] cause a general lowering of the formants. Velar 
sounds such as [k] and [g] cause F2 and F3 to come elose together at the 
beginning of the sound and to separate as the velar plosive is released. These 
changes in vowel formant frequencies are referred to as formant transitions 
and will be described in detail in the next section. 
The acoustic properties of diphthongs are similar to those of monophthongs. 
Diphthongs are vowels that consist of two vocalic parts. Unlike monophthongs, 
they do not have a single vocal tract shape - for the artieulation of a diphthong 
the tongue changes its position. This means that the acoustic properties of 
diphthongs are saliently different from the acoustic properties of monophthongs. 
Figure 5.16 shows the spectrogram of the diphthongs [eI], [ar], [;:n] , [au] and 
[;)u] produced by an RP speaker. Y ou can see that the formants change during 
the production ofthe diphthongs. In [er], the starting position (or onglide) has a 
higher Fl and a lower F2 than the final position or offglide. For [ar], this is also 

156 
Chapter 5 
the case. By the same token, the formant frequencies change considerably 
during the production ofthe diphthongs [:)1], [au] and [::m]. 
Figure 5.16. Spectrogram 01 the diphthongs [eI], [aI], [:}l], [au] and [.m]. 
5.3 The acoustic properties of English consonants 
The acoustic properties of English consonants, like English vowels, can be 
visualized and measured with the help of a spectrogram. Yet, there is no single 
acoustic measurement that can be applied to distinguish all consonants in 
English. Due to the varying manners of articulation that underlie the different 
consonants of English, it is useful to group them accordingly for an acoustic 
analysis. Generally speaking, voiced consonants, like vowels, are characterized 
by a formant structure. This is especially distinctive for the group of nasal 
consonants, as will be described below. Voiceless consonants, by contrast, are 
often characterized by stretches of high noise energy. This applies especially to 
fricatives as shown below. Plosives are produced with a complete obstruction of 
the airstremll in the vocal tract, which gives them their specific acoustic pattern. 
The acoustic pattern of plosives can be described as a sequence of at least 
three separate acoustic events: the closure part during the blockage of the 
airstream, the burst when the pent-up airstream is released suddenly, 
occasionally aspiration, and the formant transition part. Figure 5.17 shows the 
spectrogram of a speaker uttering the voiced plosives [b], [d] and [g] in the 
nonsense words aha, ada, aga. Y ou will immediately notice the gap in the 

Acoustic properties of English 
157 
spectrogram after the first [al, which of course reflects the time when no air 
passes through the mouth during the closure of the lips. 
N 
I 
>-
ü c 
a> 
:::J 
0-
a> 
Li: 
[aba] 
c10sure 
transition 
[ada] 
[aga] 
Figure 5.17. Spectrogram of aba, ada, aga illustrating the closure, burst and 
transition of plosives. 
Typically, the closure part of plosives is between 50 and 150 ms long. Longer 
closures are perceived as speech pauses by listeners. The burst of plosives is one 
ofthe shortest acoustic events in speech lasting for between 5 and 40 ms only. It 
is visible in spectrograms in the random high energy along the entire frequency 
spectrum. Voiceless plosives at the beginning of a stressed sy llable are aspirated 
in English when they do not form part of a consonant cluster. The term 
aspiration refers to the breathy noise generated as the airstream passes through 
the partially closed vocal folds. The acoustic characteristics of aspiration 
therefore are essentially the same as the acoustic characteristics of the fricative 
[h]. Aspirated [ph], [th] and [kh] can be seen in Figure 5.l8, which shows the 
spectrogram of [apha], [atha] and [akha]. Aspiration in these examples occurs, on 
average, for 100 ms. 

158 
Chapter 5 
Figure 5.18. Spectrogram of apha, atha, akha illustrating aspiration (the arrow 
points towards the aspiration phase of rk!'], which is marked by the 
box). 
As far as the voiceless consonants are concemed, in many cases it is difficult to 
distinguish them elearly in a spectrogram. The difference between the voiced 
plosives [b], [d] and [g], however, can be seen in the beginning ofthe formant 
structure of the following vowel. The difference between the three consonants 
lies in the formant transitions, the movements of the second and third formant at 
the beginning ofthe vowel in syllables like [ba] and [da], and the movements of 
the formants at the end of the vowel in syllables like [ag] and [ad]. These 
fonnant transitions reflect the changes in resonance connected with the 
movement of the articulators for the elosure phase of the plosive. They therefore 
carry information about the place of articulation of the plosive. Figure 5.19 
illustrates schematically the formant transitions of [b], [d] and [g] when these 
plosives precede the vowel [a]. For [b], F2 and F3 begin low and rise. For [d] 
and [g], F3 rises into the vowel and F2 drops into it. It is the specific 
characteristic of [g] that F2 and F3 are elose together at the beginning of the 
vowel. The formant transitions of the voiced plosives following a vowel in 
words like rob, odd and dog are practically mirror images of the formant 
transitions of these sounds preceding a vowel. 

Acoustic properties ofEnglish 
159 
F3 r - r - r = 
r = , , 
F2 
Figure 5.19. Schematic representation offormant transitions in ba, da, ga. 
Differences between voiced and voiceless plosives can be measured in voice 
onset time (VOT). This term refers to the time period between the burst and the 
beginning of voicing. For English voiced plosives, the burst typically occurs 
between 20 ms before and after voicing begins. In other words, when producing 
[ba], [da] or [ga], speakers usually release the blockage ofthe airstream for the 
[b], [d] or [g] between 20 ms before and 20 ms after their vocal folds start 
vibrating for [al. The corresponding VOT values are thus -20 ms to +20 ms. If 
voicing actually starts before the burst, the consonant is called prevoiced. For 
voiceless plosives in English, the typical VOT ranges between +40 and +80 ms. 
Aspirated plosives can have a VOT of up to + 120 ms, which means that there 
can be a 120 ms interval filled with friction between the release of the airstream 
obstruction and the vowel in words like pat, tack and cap. 
The group of nasal consonants is characterized by their specific formant 
structure. Figure 5.20 shows the spectrogram of [ama], [anal and [al)a]. Due to 
the resonating characteristics of the nasal cavity, the formant structure of nasals 
is clearly distinguishable from that of the vowel. The first formant of nasals is 
low at about 250 Hz to 300 Hz, and there is very little energy at frequencies 
above this up to about 2500 Hz where another nasal formant lies. Both 
formants can only be seen weakly in Figure 5.20. The region in between the two 
nasal formants (which appears almost white for [m] and [n] in Figure 5.20) is 
called the region ofthe antiformants. In general, nasals have less overall energy 
than vowels. The difference between [m], [n] and [1)] lies in the formant 
transition into and from the vowel. Usually, a vowel preceding a Iml shows a 
downward movement of F2 and F3, whereas in a vowel preceding a 11)1 F2 and 
F3 come together. Very little formant movement can typically be seen in a 
vowel before a In!. 
The voiced approximants [1], [J], [w] and UJ also have formants not unlike 
those of vowels. Figure 5.21 shows a spectrogram of a speaker saying [ala] , 
raJa], [awa] and raja]. You can see that the [1] has a low F2 (at around 1700 Hz) 
and a high F3 (at around 3000 Hz) which, however, generally have little energy. 
Postvocalic 'dark l' has a higher F1 and lower F2 than prevocalic 'clear l' in 
British English (Lehman & Swartz 2000). [J] has a low frequency in the third 

160 
Chapter 5 
formant - typically weil below 2000 Hz - which rises sharply at the beginning 
of the vowel. [w] has a low position of all formants with a sharp rise in F2. The 
approximant [j] has a low Fl and a high F2 which move towards each other. In 
fact, the formant structure of [w] resembles that of [u] very much just like the 
formant structure of [j] resembles that of an [I]. This explains why these two 
approximants are called semi-vowels and why some linguists transcribe the 
diphthong larl as laj/ and laul as law/. 
Figure 5.20. Spectrogram oJ ama, ana, alJa illustrating the nasal Jormants at 
about 250 Hz and 2500 Hz. 
The last group of consonants in English with a distinct acoustic pattern are the 
fricatives. During their articulation a narrow constriction in the vocal tract 
causes turbulence in the airstream, which is associated with noise in the acoustic 
signal. This noise is much more intense in the stridents (sometimes also referred 
to as sibilants) [s, z, S, 3] than in the non-stridents [f, v, 8, Ö, h]. The voiced 
fricatives only have a very faint formant structure where each formant has only 
little energy. This is illustrated in the spectrogram of [ava] , [aöa], [aza] and [a3a] 
in Figure 5.22. [z] and [3] are clearly characterized by their noise, the random 
energy in the higher frequency regions. 

Acoustic properties of English 
N 
I 
>-
ü c 
Q) 
:::J g 
'--
LL 
o 
161 
Figure 5.21. Spectrogram of ala, am, awa, aja illustrating the formant 
transitions. 
N 
I 
~ 
c 
Q) 
:::J g 
'--
LL 
o 
Figure 5.22. Spectrogram of ava, aoa, aza, a3a illustrating the faint formant 
structure and friction noise in [z 1 and f3]. 
The friction of voiceless fricatives is, like the burst of the voiceless plosives, 
visible as random noise in a spectrogram. In contrast to the burst of plosives, 

162 
Chapter 5 
however, the noise is restricted to the upper frequency regions and varies with 
the place of articulation. Figure 5.23 shows a spectrogram of the syllables [fa], 
[8a], [sa], [Ja] and [ha]. You can see that the noise ofthe labiodental [f] and the 
interdental [8] is randomly distributed from about 1000 Hz onwards. The 
difference between the two speech sounds lies in the formant transitions: for [8] 
F2 moves downwards at the beginning of the following vowel, whereas it stays 
relatively stable for [f]. The energy peaks in the noise of a [s] lie between 5000 
and 8000 Hz. Conversely, in [.f] the random noise has a peak at about 2500 Hz 
with high energy between about 4000 to 8000 Hz. Both of these consonants are 
also associated with distinct downward movements ofF2 at the beginning of [a]. 
The [h] does not show any formant transitions because its articulation does not 
involve any articulators in the vocal tract. During the production of [h] a speaker 
can already form the vocal tract appropriate for the articulation of the following 
vowel. 
9001+-==----~~----==~--~=_----~--~ 
8001~
,~ cc~, ........... 
~j-~,~'", ................• 
7001+
c ~+ "
················ 
E'y ·u', ·.· .............. . 
6001~
:~ 
i
-
"~~
-
................ -~ •................ 
I 
5001~
--~~
·:·-·-·j·,··------
~:~"}'~!h-'-··! ········-·-·~~~c!f~~,,~ ' .. c···-·-·-··---~R
;; -
-
- ~~;,~-. --- 1 
4001~
,~~"c ' -
O,:, · ,:,~","
-!"'"J!
,' 
; 
30011+ "-'l\: · ,L
· ' · ·:·:·
··············~ ·, ·:;.,.:· 
2001~
-L' :.' ~
' ............................•.... '~1~:·':'. 
·:: .• ·1" •
................. 
~';·T~: •••• 
······ ·········~~:: · •• 
··•···················· 
1 00 1~················C" .. _ 
.. 
L ·············, ······:··' ··l 
Figure 5.23. Spectrogram oJfa, 8a, sa, Ja, ha. 
Table 5.3 gives an overview of some acoustic propelties of consonants. These 
descriptions, however, can only function as rough guidelines since both the 
adjacent sounds and the prosodie function ofthe consonant (whether it occurs in 
a stressed or an unstressed syllable, at the beginning, in the middle or at the end 

Acoustic properties of English 
163 
of a word or an utterance and so forth) influence the articulation and hence also 
their acoustic features. 
Table 5.3. Acoustic properties of consonants (rough guidelines). 
Place or manner of articulation 
approximant 
nasal 
lateral 
fricative 
plosive 
bilabial 
alveolar 
velar 
retroflex 
Acoustic property 
formant structure like vowel 
formant structure with nasal formant at 
about 250 Hz, 2500 Hz and 3250 Hz 
fonnant structure with Fl at about 250, F2 at 
about 1200 and F3 at about 2400 
random noise in higher frequency depending 
on place of articulation 
gap followed by noise at all frequencies for 
voiceless plosives and formant structure for 
voiced plosives 
lowF2 and F3 
F2 at about 1700 Hz 
F2 high; F2 and F3 joined at beginning of 
formant transition 
F3 and F4 low 
5.4 Acoustic aspects of connected speech in English (advanced reading) 
Section 2.5 showed that speech does not consist of a sequence of speech sounds 
but that the articulatory gestures underlying one speech sound are always 
affected by the articulatory gestures underlying the surrounding speech sounds. 
This fact can be easily made visible and measured with the help of speech 
analysis software. In general, a comparison between a speech sound produced in 
isolation (= on its own) and the same speech sound produced in connected 
speech shows distinct differences between the two. The overall duration of a 
speech sound is shorter when produced in a sequence of sounds than when 
produced in isolation. Moreover, consonants in consonant clusters are shOlter 
than single consonants in the onset position of a syllable. Thus, the [w] in wing 
is longer than the [w] in swing. By the same token, the syllable [stlk] is longer in 
stick than in sticky and shortest in stickiness (Lehiste 1972). Furthermore, the 

164 
Chapter 5 
duration of speech sounds varies with their phonotactic position. For example, a 
vowel followed by a voiced sound is longer than when followed by a voiceless 
sound. This means that the [i] in heed is longer than the [i] in heat. The same 
consonant in syllable coda position is longer than in syllable onset position. 
Thus, the [d] in head is longer than the [d] in da. The length of speech sounds is 
furthermore determined by their prosodic position. For example, the VOT of a 
word-initial voiceless plosive such as the [tl in tack is longer than when the [tl 
appears in the middle of a word as in attack (e.g. Turk & Shuttack-Hufuagel 
2000). Equally, a vowel in a word at the beginning of an utterance is longer than 
the same vowel appearing in the middle of an utterance (Cho 2005). 
The duration of individual speech sounds depends crucially on the speaking 
rate. In careful speech - which speakers produce in an effort to be highly 
intelligible, for example when addressing young children, a hard-of-hearing 
person or foreigners - speech sounds are lengthened and the duration of pauses 
is increased. On average, between two and three syllables are produced per 
second in careful speech. In conversational speech, by contrast, between 5 and 
11 syllables are produced per second. This means that vowels and consonants 
often lose some of their acoustic distinctiveness as they are modified or reduced. 
In very rapid speech, some speech sounds are even 'deleted' altogether, i.e. their 
articulatory gestures are not carried out to an audible extent. Typically, only 5% 
of three-consonant coda clusters are realized. In other words, a word containing 
three consonants in the coda position such as facts Ifrektsl is nearly always 
produced as [freks]. This reduction is much more frequent in function words 
(e.g. prepositions, conjunctions and auxiliaries) than in content words (e.g. 
nouns, verbs, adjectives). 
Figure 5.24 shows the waveform and spectrogram ofthe sentence "Give her 
the post." read at a relatively slow speaking rate by a British English speaker. 
The duration of each speech sound is indicated by the boundaries given in the 
annotation below the spectrogram. When you measure the length ofthe vowel in 
give, you can see that it is 102 ms long. The [0] in the is 40 ms long. Compare 
this with Figure 5.25, which shows the spectrogram and waveform of the same 
sentence produced at a higher speaking rate. First of all you can see that one 
speech sound is deleted: there is no acoustic trace of the Ih/ in her. Furthermore, 
you can see and measure that many of the speech sounds have a much shorter 
duration when produced with higher tempo. The vowel in give, for example, is 
only 43.9 ms long in the utterance produced at the higher speaking rate. The 101 
in the is so short (13 ms) that it is perceived as a [d] rather than a [0]. When you 
carry out a segmentation of the individual speech sounds in an utterance - that 
is, when you indicate the beginning and end of each speech sound in the 
annotation - you will notice that this is increasingly difficult with increasing 
speaking rate. For some speech sounds it will be impossible to determine their 
exact boundaries. 

Acoustic properties of English 
165 
Figure 5.24. Duration ofthe individual speech sounds in a slow reading ofthe 
sentence "Give her the post. " by a British English speaker (listen to 
it on the CD-ROM· 05 _slow. wav). 
This difficulty to determine the boundaries between speech sounds in connected 
speech is due to coarticulatory processes. Section 2.5 described that during 
speech production the different articulators move in an overlapping way. While 
producing the [s] in words like stew, the lips are already rounded in anticipation 
of the following vowel [u]. This overlap of articulatory gestures for speech 
sounds in connected speech is referred to as coarticulation and can occur both 
as anticipatory coarticulation as with the [s] in stew and as perseverative 
coarticulation, in which case the articulatory gestures of one speech sound 
continue during the production of the following sound. For example, 
perseverative coarticulation can be seen in the word climb, where the [1] is 
produced as voiceless due to the voicelessness of the preceding [klo 
Coarticulatory effects can be viewed in a spectrogram. The vowel [ce] in am, for 
example, will typically be nasalized, i.e. the spectrogram will show the nasal 
resonance due to the lowering ofthe velum for the following [m] already during 
the production of the vowel. This is visible in the presence of low-frequency 
nasal formants and the appearance of antiformants with severely reduced energy 
in particular frequency regions. By the same token, spectrograms can show that 
in rapid speech many vowels between voiceless consonants - as in the word pit 
- are voiceless themselves. This is especially frequent in unaccented syllables in 
English. 

166 
Chapter 5 
Figure 5.25. Duration of the individual speech sounds in a fast reading of the 
sentence "Give her the post. " by a British English speaker (listen to 
iton the CD-ROM: 05Jast.wav). 
5.5 The acoustic properties of English intonation 
Chapter 4 has shown that intonation in English comprises both the placement of 
accents in an utterance and the employment of pitch and pitch movements across 
and between utterances. Nucleus placement has the main function of signalling 
which elements in the utterance are in focus (i.e. which are new and which are 
especially emphasized). Pitch is employed by speakers for various purposes 
including the marking of the function of an utterance (for example as arequest 
for information or as an aside) and the expression of specific attitudes. The 
acoustic properties of both - accents and pitch movements - can be visualised 
and measured with the help of speech analysis software packages. 
5.5.1 The acoustic properties of intonation phrases in English 
Section 4.1 explained that one of the major functions of intonation in English is 
the structuring of utterances and discourse. These basic units of speech are 
referred to as intonation phrases. Many acoustic cues conspire to give the 

Acoustic properties of English 
167 
perceptual impression of an intonational phrase boundary. Acoustic properties 
correlating with intonation phrases in English include 
-
length of following pause 
-
final syllable lengthening 
-
change in pitch level after a phrase boundary 
-
tempo ofunstressed syllables after a phrase boundary 
Typically, pauses occurring after intonation phrase boundaries are between 100 
and 600 ms long (Campione & Veronis 2002). There seems to be a tendency for 
speakers to produce longer pauses after more significant phrase boundaries 
(such as between the last utterance in a topic unit and the first utterance of the 
following topic unit) than after minor intonation phrase boundaries (e.g. between 
an adverbial and the main clause within an utterance), at least in reading style. In 
spontaneous speech, the relationship between pause length and status of 
intonation boundary is less clear (Keseling 1992). 
The length of the final syllable before an intonation phrase boundary is the 
most important cue indicating the end of an intonational phrase. The term final 
syllable lengthening describes the phenomenon that the last syllable of a word 
is distinctly longer when this word occurs at the end of an intonation phrase than 
when it occurs in the middle of an intonation phrase. Thus, the syllable ing in 
going is longer in the utterance "Where are you goi!!g" than in the utterance "I 
am goi!!g horne". The length of the final syllable increases with the rank of the 
intonation phrase boundary: there is less final syllable lengthening before less 
important boundaries (e.g. Yang 2004). 
Another acoustic correlate of intonation phrase boundaries is the resetting 
of pitch after the boundary. As will be explained in section 5.5.3 below, pitch 
usually drops throughout an utterance and reaches its lowest point at the end of 
the last intonation phrase. At the beginning of the next intonation phrase, pitch is 
picked up at a higher starting position, which is clearly visible in automatie pitch 
contour displays. Finally, the first unstressed syllables of a new intonation 
phrase are produced at a very high speaking rate. Thus, when a speaker pro duces 
utterance (104), the unstressed syllables but and un after the intonation phrase 
boundary are likely to be articulated very quickly. This sudden acceleration in 
speech tempo is called anacrusis. 
(104) I was going horne I but unfortunately I had forgotten my keys 

168 
Chapter 5 
5.5.2 
The acoustic properties of accents in English 
In English, syllables can be accented or unaccented. The first syllable of 
English, for example, is usually accented when a speaker produces this word, 
and the second one is unaccented. The acoustic differences between the two 
types ofsyllable can be measured in terms oftheir 
-
duration 
-
intensity 
-
pitch 
-
vowel quality 
Various studies have shown that accented syllables are longer than unaccented 
syllabi es in English (e.g. Fant et al. 1991, Williams & Hiller 1994). On average, 
accented syllables are about 300 ms long, whereas unaccented syllables are 
about 150 ms long. This difference in length between the two types of sy llable is 
mainly due to the processes of vowel reduction and vowel deletion, which 
occur in unaccented syllables in English. Vowels in unaccented syllables are 
much shorter than vowels in accented syllabies, and often they are not realized 
at all (see e.g. Delattre 1981). Vowels in accented syllables furthermore have a 
higher intensity and a higher pitch than vowels in unaccented syllabies. Since 
the intensity of vowels in accented syllables is typically increased, English has 
been described to have dynamic accentuation. Yet, intensity seems to be the 
least consistent and least salient property of accentuation in English. Perceptual 
experiments have shown that a rise in pitch on a syllable is the most important 
cue for accentuation in English. Vowels in accented syllables are associated with 
a prominent pitch peak, whereas vowels in unaccented syllables generally have 
lower pitch. Vowels in accented and unaccented syllables further have a 
different quality, which is reflected in a different formant structure in a 
spectrogram. Vowels in unaccented syllables are produced with a more central 
position of the tongue, with narrow jaw-opening and without lip rounding, 
which resuIts in nearly evenly spaced formants. Figure 5.26 shows the 
spectrograms of the words 'abject and ab'ject. When you analyse the acoustic 
parameters ofthe vowels [0] and [e] in both words, you can see that [0], when it 
is accented, is 122 ms long, has a pitch peak of 262 Hz and a peak intensity of 
72 dB. When it is produced in an unaccented syllable in ab'ject, however, it lasts 
83 ms, has a pitch peak at 226 Hz and a peak intensity of 72 dB. This pattern is 
also true for the second vowel [e]. In accented position it is 96 ms long, has a 
pitch peak of 179 Hz and a peak intensity of 72 dB. In unaccented position, by 
contrast, it has a length of 90 ms, a pitch peak at 170 Hz and a peak intensity of 
73 dB. 

Acoustic properties of English 
169 
(a) 'object 
(b) ob'ject 
Figure 5.26. Waveform and spectrogram ofthe words 'object (a) and object (b). 

170 
Chapter 5 
5.5.3 Measuring pitch and pitch movement in English 
Section 2.2 and section 5.1.1 above explained that the pitch of a speaker's voice 
is determined by the rate of vocal fold vibration, which in turn depends on the 
length and thickness ofthe vocal folds. By altering the tension (and thereby the 
length and thickness) of the vocal folds, speakers can vary the rate of vocal fold 
vibration and can produce different levels of pitch and pitch movements. What 
is perceived as pitch is in acoustic terms the fundamental frequency of the 
complex period wave underlying voiced speech sounds. The fundamental 
frequency (FO) of speech is measured acoustically in Hertz (Hz). 1t is possible to 
do this in a waveform (by counting the peaks of amplitude per second) or in a 
spectrogram (by reading off the frequency of FO, especially a narrowband 
spectrogram which has good frequency resolution, see section 5.1.2 above), but 
most speech analysis software packages also offer a more convenient pitch 
tracking too1. This tool extracts the fundamental frequency from the speech 
signal and displays it as a pitch contour. By clicking on any point of this 
contour, a measurement ofFO can be taken. Figure 5.27 shows the pitch track of 
a British English speaker reading "A tiger and a mouse were walking in a field." 
at the beginning of a story. 
500 
400 
300 
N ;:s 
200 
.r: 
.B 
0:: 
100 r 
~ 
~ 
~ 
~ 
0 
a 
tiger 
n a 
mouse 
er 
walking 
in 
a 
field 
0.0271456 
1.92151 
Time(s) 
Figure 5.27. Pitch track ofthe sentence "A tiger and amause were walking in a 
field. " read by a British English speaker. 

Acoustic properties of English 
171 
The first noticeable feature of the displayed pitch movement is that there are 
gaps in the pitch contour. These indicate the presence of voiceless speech 
sounds such as the [tl in tiger, the [s] in mouse and the [f] infield, which do not 
have pitch. It is furthermore clearly visible which syllables are accented in this 
utterance: both the syllable ti in tiger and the syllable wal in walking are 
produced with a distinct pitch peak; the syllable field has a smaller pitch peak. 
When comparing the pitch peaks, it becomes obvious that the first peak on the 
syllable ti is higher than the second on the syllable wal, which in turn is higher 
than that on field. This phenomenon is referred to as declination, a term that 
describes the overall drop of pitch height across utterances. It is assumed that 
declination is caused by the decreasing air pressure during speech production. 
The longer a speaker produces speech on an airstream, the less air there is left in 
the lungs and the lower the air pressure iso Listeners adjust for declination, 
which means that usually the overall drop of pitch is not noticeable. In fact, 
synthetic speech without declination sounds very unnatural. 
Several acoustic measurements can be taken that describe English 
intonation. First of all, the acoustic properties of tones (see section 4.3), for 
example the extent of nuclear falls and rises in utterances can be determined. 
The nucleus ofthe utterance displayed in Figure 5.27 lies on the last wordfield. 
A transcription of the intonation of this utterance is given both in the British 
School tradition (105a) and in ToB! (105b): 
(1 05a) A '\. tiger and a mouse were '\. walking in a \field 
(1 05b) A tiger and a mouse were walking in a field 
H*+L 
H*+L 
H* L% 
By measuring the exact pitch height of the highest pitch at the beginning of the 
wordfield (117.64 Hz) and the pitch height at the end of the wordfield (88.69 
Hz), one can calculate that the extent of the nuclear fall is 28.95 Hz (117.64 
minus 88.69). By the same token, the pitch range of this utterance can be 
measured. This is calculated by subtracting the lowest pitch at the end of the 
utterance (88.69 Hz at the end of the wordfield) from the highest pitch peak at 
the beginning ofthe utterance (287.23 Hz on the syllable ti). The pitch range of 
this utterance is thus 198.54 Hz. Section 6.5 explains that the distance between 
two frequencies expressed in Hz is not perceived equally for all frequency 
ranges. In short, the same difference of 198.54 Hz between two points in the 
pitch range sounds greater when produced in a low frequency region (i.e. by a 
low voice) than in a high frequency region (i.e. produced by a higher voice). In 
order to be able to compare the pitch range of different speakers, it is therefore 
common to employ the unit semitones as a measurement. A pitch range of the 
same number of semitones thus has exactly the same perceptual range for every 
speaker. 

172 
Chapter 5 
For every utterance, the average pitch height can be measured and speakers 
can be compared according to their average pitch. Acoustic measurements of 
this kind have shown that, on average, women speak with higher pitch than men 
do and that children's average pitch is highest (dropping with increasing age). 
This is of course related to the relative length of a speaker's vocal folds, which 
co-determines the rate of vocal fold vibration. V ocal folds of adult males have 
an average length of 17-25 mm while adult women's vocal folds are, on average, 
12 to 17 mm long. The vocal folds of newbom babies are ab out 2 mm long. 
Table 5.4 gives some average values ofpitch height for adult males, females and 
children. 
Table 5.4. Average pitch height of male speech, female speech, and children's 
speech. 
male speech 
female speech 
children aged 2 
children aged 6 
children aged 10 
Average pitch Pitch range 
100-150 Hz 
150-250 Hz 
500Hz 
400Hz 
300Hz 
70-250 Hz 
80-400 Hz 
300-1000 Hz 
300-700 Hz 
200-500 Hz 
Empirical studies suggest that pitch range is language-specific (e.g. van 
Bezooijen 1995). This means that speakers of different languages habitually use 
different pitch height and pitch range. For example, English speakers typically 
have a higher average pitch or a wider pitch range than speakers of German 
(Mennen 2007). On average, for female speakers the pitch range in English is 
7.16 semitones, while it is only 5.42 semitones in German. 
Pitch range and pitch height are used by English speakers to signal new 
information and the beginning of new topics (see also section 4.3.3). Acoustic 
measurements have shown that utterances containing a new topic have a wider 
pitch range and begin at a higher pitch than utterances that merely give 
additional information or constitute reformulations of a previous utterance (e.g. 
Wennerstrom 1998, Wichman 2000). When a speaker produces parenthetical 
material such as an aside or a meta-discoursal remark, this is often associated 
with a narrow pitch range. For example, a speaker would produce the aside "but 
you know this already" with a narrower pitch range than "Grandma told me that 
her neighbour has moved to Thailand" in example (106). 

Acoustic properties of English 
173 
(106) Grandma told me that her neighbour has moved to Thailand I but 
you know this already 
Acoustic measurements of speech can be employed to describe differences in 
voices between speaker groups, for example between young and old speakers, 
men and women, healthy speakers and speakers with handicaps such as cleft 
palate (see section 2.3) or dysarthria, a neurological speech disorder associated 
with Parkinson's disease. In general, voices of older speakers have less regular 
vocal fold vibration, which is visible in pitch trackings. Moreover, it has been 
found that women's voices are often breathier than men's voices and that 
speakers born with a cleft palate andlor cleft lip show a high degree of 
nasalization in their speech. Some acoustic measurements of speech can be used 
for speaker identification, for example in forensie phonetics. At court, 
phoneticians are sometimes asked to make positive identifications of criminals 
whose speech was recorded. Speaker identification is based on the fact that 
some aspects of speech are relatively characteristic for individual speakers due 
to the particular physiological characteristics of each speaker. For example, F4 
and higher formants in vowels are associated with a partieular voice quality, and 
the higher nasal formants depend on individual vocal tract characteristics. 
Equally, 
long-term 
average 
spectra (L T AS) 
reflect 
individual 
voice 
characteristics, and the rate of formant transitions after voiced plosives as weIl 
as the length of aspiration of voiceless plosives vary systematically among 
speakers. 
When measuring and interpreting an automatically generated pitch contour, 
it is important to know about human perception of pitch. Chapter 6 describes in 
detail how the frequencies in asound wave are related to the sensation of pitch 
height. It shows that humans, on average, can only pick up frequencies between 
20 and 20,000 Hz and that not every change in frequency is directly related to a 
change in the perception of pitch. Furthermore, changes in frequencies are not 
perceived as equal in the lower and the higher frequency ranges, which is why 
differences between speakers are often described using units other than Hz (for 
example semitones). Similarly, the automatie pitch tracking algorithm displays 
many small changes in pitch that are linguistically unimportant because listeners 
do not notice them. For example, prevocalic stops and fricatives produce short-
term perturbations (= a jump up or down) of the pitch contour, as you can see 
for the [g] in tiger in Figure 5.27. Some tools have therefore been developed that 
display only those pitch changes across utterances that are perceptually relevant 
(e.g. Mertens 2004) and that display 'smoothed' pitch curves which exclude 
short-term perturbations. 

174 
Chapter 5 
5.6 Acoustic properties of L2 learner English and the use of acoustic 
phonetics in pronunciation teaching (advanced reading) 
Many empirical studies have shown that the acoustic properties of speech 
produced by second language leamers of English differ from those of speech 
produced by native speakers of English. Barry (1989) analysed how ten German 
leamers ofEnglish, aged 17-19, who had been leaming English at school for six 
years produce the vowels in words like bat, bet and but. The acoustic 
measurements showed that these leamers of English confuse the vowels lrel and 
lei in words such as bat and bet. Especially the F2 values are roughly the same 
for both vowels instead ofbeing distinctly lower for the vowel lrel than for lei. It 
was further examined whether these leamers of English produce the difference 
in duration of a vowel before the voiced plosives Ib, d, g/ and the voiceless 
plosives Ip, t, k/. Native speakers of English produce a longer vowel 
(approximately 200 ms) in dog than in dock (approximately 135 ms). At the 
same time, the duration of the dosure of /b, d, gl is shorter (approximately 130 
ms) than the dosure of Ip, t, k/ (approximately 150 ms) in native English. It was 
found that many German leamers of English do not produce such a dear 
distinction between dosure durations and vowel durations. 
Many studies have been concemed with the acoustic measurement of voice 
onset time (VOT, see section 5.3 above) ofplosives in English. For example, it 
was found that some Spanish native speakers produce a shorter VOT for the 
plosives Itl and Ipl in English (Schmidt & Flege 1996) than native English 
speakers do. This means that the Spanish speakers' productions of Ipl are doser 
to what English speakers identify as Ibl and their productions of Itl are doser to 
an English native speaker Id/. 
Section 5.5.2 above showed that several acoustic changes can be measured 
between accented and unaccented syllabies. These indude a longer vowel with 
different formants as weil as higher pitch and higher intensity of the vowel in 
accented syllabi es, with high pitch being the most important perceptual cue. 
When German leamers of English produce an emphasized syllable, they 
sometimes make different use of these acoustic parameters than English native 
speakers (Gut 2000b). For some speakers, the greatest difference between an 
emphasized and an unemphasized syllable lies in a difference in intensity rather 
than in a difference in pitch height. This might be due to the fact that 
accentuation in German also relies more on intensity changes than on pitch 
changes. Most leamers of English furthermore do not reduce vowels in 
unaccented syllables to a native-like extent. An analysis ofthe LeaP corpus (Gut 
2007a) of non-native English has shown that the mean length of an unaccented 
syllable is 101.8 ms in native English but 155.07 ms in leamer English. When an 
unaccented syllable follows an accented one, as in the word measure, the 
accented syllable is on average 2.45 times longer than the unaccented one in 

Acoustic properties of English 
175 
native English. Many leamers of English do not succeed in producing the same 
durational difference: their accented syllable is, on average, only 1.98 times 
longer than the following unaccented one. Acoustic studies of the intonation that 
leamers of English produce also point out differences between native and 
leamer speech. Dutch leamers of English have a smaller dec1ination rate across 
utterances than native English speakers (Willems 1982). An analysis ofthe LeaP 
corpus of non-native English has shown that the nuc1ear falls produced by 
leamers of English at the end of an utterance are sm aller (they extend only 
across 3.64 semitones) than the falls produced by native English speakers (7.81 
semitones on average) (Gut 2007a). Moreover, some leamers of English use 
pitch height for the marking of new and given information to a different degree 
than native English speakers do. Wennerstrom (1998) found in her study that 
some leamers of English do not produce higher pitch on new information. By 
the same token, these leamers produce little difference in pitch height between 
the end of a topic and the beginning of a new topic. There is less resetting of 
pitch level after an intonation phrase boundary in leamer English than in native 
English (Willems 1982). Another study showed that Japanese and Thai leamers 
of English do not mark the first utterance in a new paragraph with a wider pitch 
range like native speakers do (Wennerstrom 1994). 
N 
I 
o o 
30.-----------------------------------------~ 
-12+-----------------------------------------.-4 
o 
4.16337 
Time(s) 
Figure 5.28. Pitch movement across the senten ce "A tiger and a mouse were 
walking in a field when they saw a big lump 01 cheese lying on the 
ground. " read by a German learner 01 English. 

176 
Chapter 5 
Acoustic measurements of pitch range have shown that leamers of English, 
on average, produce a narrower pitch range than native speakers do. Conversely, 
when English native speakers speak German, they have a wider pitch range. 
Figure 5.28 illustrates this with the pitch contour of the sentence "A tiger and a 
mouse were walking in a field when they saw a big lump of cheese lying on the 
ground." read by a German leamer of English who was aged 24 at the time of 
recording and had just spent six months studying at a British university. The 
pitch range of his utterance is displayed in semitones. Measured in Hz, the 
highest point of the pitch curve at the beginning of the utterance is 153 Hz, the 
lowest is 102.6 Hz. 
Compare this to Figure 5.29, where a British English speaker read the same 
sentence. His pitch ranges from 286.6 Hz at the highest point at the beginning of 
the utterance to 85 Hz at the end ofthe utterance. 
301.-----------------------------------------~ 
N~ 
I r 
0 ;:' 
~ 
J' ~\\ 
on 
~J\~ 
'" 
c 
0 E 
f\ 
'" 
~~ 
.!,,-
.r: 
.B 
CL 
'-
-12:+------------------------------------------1 
o 
4.18439 
Time(s) 
Figure 5.29. Pitch movement across the sentence "A tiger and amause were 
walking in a field when they saw a big lump 01 cheese lying on the 
ground. " read by a British English speaker. 
The methods and findings of acoustic phonetics have been used variously in 
pronunciation teaching and self-study courses (e.g. J ames 1977, Herry & Hirst 
2002, Gut 2006, 2007 a, b). In an early study, J ames (1977) found the 
visualization of the intonation curve of utterances to be effective in the 
acquisition of French. Similarly, Herry & Hirst (2002) deviseda computer-
assisted prosody leaming tool based on simultaneous visualisation and audition 

Acoustic properties of English 
177 
for students of English, which was used successfully at a French university. 
Acoustic measurements can moreover be used in order to explore typical leamer 
errors in the pronunciation of individual speech sounds. For example, students 
can compare the vowel qualities of lrel and lei in words such as have and said in 
both their own and native speech. This activity can be based on an auditory 
analysis combined with an acoustic measurement of the formant structure of 
these vowels. As shown in section 5.2, the first two formants, Fl and F2, are 
related to tongue position during articulation. By identifying the relevant vowels 
in a recording and measuring the formants as shown in Figure 5.30, students can 
explore differences between their own and a native speaker's speech. 
Figure 5.30. Measuring the first two lormants 01 a vowel. 
By the same token, students can measure the pitch range of different speakers, 
as weil as the length of pauses and the acoustic properties of pitch, loudness and 
length as they are employed by individual speakers in order to achieve 
accentuation. This type of occupation with the properties of sounds is assumed 
to increase language awareness of pronunciation problems in general. 
5.7 How to make a good speech recording 
In order to carry out reliable measurements of the acoustic properties of speech, 
it is necessary to obtain high-quality speech recordings. The basic prerequisite 
for a good speech recording is a suitable recording environment. If you do not 
have access to a sound-treated room that excludes all external noises, you need 
to pick a location with minimal background noise. Common sources of 
background noise contaminating a recording include electronic equipment such 
as computers, fluorescent lighting, nearby corridors, bathrooms, lifts, roads, 
parks and playgrounds. In general, it is important to remember that a 
microphone will pick up many sounds that human perception filters out or 
reduces automatically, for example the speaker's shoe hitting the table leg or 

178 
Chapter 5 
some background noise from outside. It is therefore always advisable to listen to 
the recording while making the recording and to make test runs before the real 
recording. 
Both the recording device and the microphone you use need to be sensitive 
enough to capture all acoustic cu es that are perceptually important. The choice 
of a suitable microphone - there are full-size microphones mounted on a table, 
clip-on microphones and head-mounted microphones - depends on whether the 
speakers are required to move around the room or can be placed on achair 
behind a table. Since sound waves travelling through air lose energy (it takes 
energy to move the molecules in air), the microphone should always be placed 
close to the speakers mouth. However, in order to avoid disturbances of the 
recording by the blowing noises produced by the speaker's breathing or the 
aspiration of plosives, the microphone should be either attached to the speaker's 
blouse or be kept to the side ofthe speaker's mouth as in a headset. 
When recording speech you have two basic options: either using analogue 
devices such as cassette tapes or recording digitally onto minidisks and the like 
or directly onto a computer. Analogue recording devices convert the continuous 
air pressure variations into equally continuous electrical signals. In order to be 
analysed on a computer, they have to be converted into digital signals, which 
entails some loss of information. In digital recordings, the continuous speech 
signal is converted into discrete points in time and stored as numbers. This 
means that the computer or digital recording devices represent the speech signal 
as a sequence of sam pies. They check the signal at regular intervals of time and 
store the properties of the signal at these points in time. When recording with a 
digital device you therefore have to decide on the sampling rate - how many 
points in time will be represented per second in your recording. Since speech 
events can be extremely small in duration (compare section 5.5.3), you need to 
choose a sampling rate that does not risk losing important information of the 
speech signal. All perceptible properties of speech are definitely recorded with a 
sampling rate of 48 kHz, but since recordings at this sampling rate require a lot 
of storage on your computer, many linguists record their data with a sampling 
rate of 22 kHz. This sampling rate still guarantees that all important acoustic 
aspects of speech are captured. 
Another decision that has to be made when recording speech is that of 
quantization. This term refers to the accuracy of representation of the different 
amplitudes in the speech signal, i.e. the number of separate amplitude levels that 
are represented on a computer. They are stored in binary digits (bits) so that 
quantization is expressed in bits. Most researchers encode speech signals with 
12 or 16 bits. When the volume ofthe speech that is being recorded goes beyond 
the range that can be represented, clipping occurs, which makes the recording 
useless for any phonetic analysis. You can see clipping in the waveform 
displayed in Figure 5.31: it is in one part cut off at the upper and lower ranges of 

Acoustic properties of English 
179 
amplitude. Most recording devices indicate whether the volume is within the 
critical range with a bar in the colour green, which switches to yellow and 
finally to red when the range is exceeded. 
O.~:H:l~:l~:lT------------
1.27297 
Time (5) 
Figure 5.31. Speech recording with clipping due to tao high recording va/urne. 
5.8 Exercises 
1. What are the acoustic properties of sound? 
2. Which features are displayed in a waveform, in a spectrogram and in a pitch 
track? 
3. What are the differences between vowels and sonorants on the one hand and 
voiceless consonants on the other, in acoustic terms? 
4. What are the characteristic properties of vowels that are displayed in a 
spectrogram? 
5. What does aspiration look like in a spectrogram? 
6. Why does the pitch track of an utterance have gaps? 
7. How is pitch range measured? 
8. What are the acoustic correlates of accentuation in English? 
9. Which aspects of English pronunciation can be taught with the help of 
acoustic phonetics? 

180 
Chapter 5 
10. Why is it so difficult to segment speech into individual speech sounds in a 
recording? 
11. Record yourself saying "A tiger and a mouse were walking in a field when 
they saw a big lump of cheese lying on the ground" and compare your pitch 
range to that displayed in Figure 5.29. 
12. Identify the vowels and fricatives in the following spectrogram. Can you 
guess what the utterance is? 
5.9 Further Reading 
Ladefoged (2001 b, chapter 8), Yava~ (2006, chapter 5) and Davenport & 
Hannahs (2005, chapter 5) offer good, basic introductions to acoustic 
measurements of vowels and consonants. Clark, Yallop & Fletcher (2007, 
section 7.19), Kent & Read (2002, chapter 7) and Ashby & Maidment (2005, 
chapter 10) give short introductions to the acoustic properties of intonation and 
accentuation. 
Students interested in the finer details of the acoustic analysis of the 
different speech sounds are recommended to read Kent & Read (2002, chapters 
3 to 6) and Clark, Yallop & Fletcher (2007, chapter 7). For advanced readers, 
acoustic theories of speech production are explained in J ohnson (1997, chapters 
5 to 9). Details on the source-filter theory of speech production can be found in 
the classic texts by Fant (1960) and Stevens & House (1961). 

Acoustic properties of English 
181 
Several software tools for sound visualisation, acoustic measurements and 
speech manipulation are available for free on the Web, e.g. the software 
Wavesurfer at <http://www.speech.kth.se/wavesurfer/> and the software Praat 
at <hrtp://uvafon.hum.uva.nl/praat/>. A Praat manual describing how to record 
yourself and some basic steps in measuring acoustic aspects of speech is 
available on the CD-ROM (Praat_ Manual.pdf). 


6 Speech perception 
Our ability to speak depends crucially on our ability to hear. Babies who are 
born deaf do not acquire speech (they use sign language for communication 
instead), although they have fully functioning speech organs. Not being able to 
hear, they cannot learn about the relevant units (such as phonemes, syllables and 
words) and rules of speech and cannot form mental representations (i.e. 
knowledge stored in their memory) of them. Yet, as chapters 3 and 4 explained, 
without mental representations of the units and rules of speech, the production 
of speech is not possible. This chapter describes the anatomy (structure) and 
physiology (function) of the organs that are involved in the perception of 
speech, although it has to be stressed that even today our knowledge about some 
of the fundamental processes of hearing is still incomplete. Like the speech 
organs used for articulation, the organs of speech perception are divided into 
'systems', namely the peripheral auditory system - the ear - and the internal 
auditory system - the relevant parts of the brain. The three components of the 
peripheral auditory system, namely the outer, the middle and the inner ear, are 
described in sections 6.1 to 6.3. Section 6.4 illustrates the fimction of the 
internal auditory system. The perception of the acoustic cues of the speech 
signal and the methods of measuring these are described in sections 6.5 and 6.6. 
Section 6.7 presents theories of the perception of sounds and words, and section 
6.8 describes what we know about the acquisition of perceptual abilities in first 
and second language acquisition. 
6.1 The outer ear 
Figure 6.1 illustrates the three parts of the human peripheral auditory system, 
which is divided into the outer, the middle and the inner ear. The outer ear 
consists of the pinna, the only visible part of the human auditory system, and 
the ear canal, or meatus. The pinna's main fimction is to protect the entrance of 
the ear canal from dirt and potentially harmful objects. The fact that humans 
have two ears, one on either side of the head, enables them to localize sounds, 
i.e. to tell where asound is coming from. It appears that the pinnae contribute to 
this ability, especially for the higher frequencies of sounds. 
The ear canal is a tube with a length of between 25 and 50 mm, which 
serves as the pathway for acoustic signals to reach the middle ear. The ear canal 
has two functions. On the one hand, it protects the complex musculature of the 
middle ear; on the other, it acts as a resonator for the incoming sound waves. 
The ear canal provides resonance, which means that it enhances the properties of 

184 
Speech perception 
a great range of sounds, especially those between 500 and 4000 Hz. This range 
includes all the major cues to linguistically relevant sounds, as shown in sections 
5.2 and 5.3. The ear canal further appears to be particularly sensitive to 
frequencies between 2000 and 4000 Hz, which means that it plays an important 
role in the perception of fricatives. 
Pinna 
Outer ear 
·Middle ear 
Inner ear 
Figure 6.1. The human peripheral auditory system. Adapted from Clark, Yallop 
& Fletcher (2007: 299). 
6.2 The middle ear 
The middle ear is a small air-filled cavity within the skull that comprises the 
eardrum, a set of three interconnected bones - called the auditory ossicles - and 
their associated muscles (see Figure 6.1). The ear canal ends in the eardrum, 
which consists of a membrane that provides a seal between the outer and the 
middle ear. It is connected with the first of the auditory ossieies, the mallet, 
which in turn is attached to the second ossicle, the anvil. The anvil itself is 
attached to the stirrup (incidentally the smallest bone we have in our body with 
about 3.3 mm of length), which in turn is connected to the oval window, a small 
gap in the skull's bone structure that constitutes the interface to the inner ear. 
One of the functions of the middle ear is that of passing on the incoming 
sounds to the inner ear. It does so by transforming the sound pressure variations 
in the air into equivalent mechanical movements. When sound pressure 

Chapter 6 
185 
variations reach the eardrum via the ear canal, the eardrum is deflected and sets 
the mallet into vibration. The mallet sets the anvil into motion, which in turn 
causes the stin-up to vibrate. 
Apart from the transmission of the speech signal, the middle ear also has the 
functions of arnplification and regulation of the incoming sound level. 
Amplification of the incoming sound is based on two facts. First, the eardrum 
has a much larger surface area than the oval window, and the same pressure 
applied to a smaller area is greater than when applied to a bigger area. Second, 
the three auditory ossicles act like a mechanical lever system because the mallet 
is longer than the anvil. Due to this, the movement of the ossicles produces a 
greater force than that which hits the eardrum. Taken together, these factors 
cause the acoustic energy at the oval window to be about 35 times greater than 
that at the eardrum. Especially the acoustic energy between 500 and 4000 Hz is 
thus maximized during the transmission of the speech signal to the inner ear. 
The musculature of the middle ear further regulates the sound level, for example 
by protecting the inner ear against damage with a reflex called the acoustic 
reflex mechanism. When excessively loud sounds reach the middle ear, the 
musculature contracts and adjusts the ossicles so that the efficiency of sound 
transmission to the inner ear is reduced. Similarly, the ossicles contract just 
before speaking. It is assumed that this has the function of reducing for speakers 
the perceived sound volume oftheir own voice. 
Figure 6.1 illustrates that the Eustachian tube links the middle ear and the 
oral cavity and thus provides an air pathway that can be opened in order to 
equalize pressure differences between the outer and the middle ear. Sometimes, 
after going up or down rapidly, for example in an airplane or lift, or when 
entering or leaving a tunnel, the changes in air pressure are feit painfully in the 
tension of the eardrum. By swallowing you open the Eustachian tube and thus 
allow the excess pressure to be released from the middle ear. 
6.3 The inner ear 
The inner ear is located within the skull and has a complex structure of which 
the cochlea is the most important organ for speech perception. Figure 6.1 
illustrates that the cochlea forms part of the labyrinth, which also contains the 
vestibular system that is essential for our sense of balance. The cochlea is a 
snail-like structure, a conical chamber of bone that is rolled up about 2.5 times. 
On the one end, the base, it connects with the stirrup at the oval window; the 
other end is called the apex. Looking inside the cochlea, one can see that it 
consists of three tube-like canals filled with various fluids and lying on top of 
each other. Figure 6.2 is an illustration of a cross-section through the cochlea. It 
shows that the three canals are divided by two membranes, the vestibular 

186 
Speech perception 
membrane and the basilar membrane. The basilar membrane is of central 
importance for the perception of speech. It contains the organ of Corti, which 
has little hair cells attached to it that are connected to the auditory nerve. It is 
important to note that the basilar membrane does not have the same thickness in 
all places. It is stiffer and thinner at the end with the oval window and less stiff 
and wider at the inner end, the apex. 
organ of Corti 
--+- hair cells 
~~~~ 
Figure 6.2. Cross-section through the cochlea. 
basilar 
membrane 
The function of the inner ear is to trans form the mechanical movements 
transmitted by the auditory ossicles into neural signals that can be passed on to 
the brain. This happens in the following way: first, the movements occurring at 
the oval window (the flexible membrane connected to the stirrup) are 
transmitted through the cochlear fluid and cause movement along the basilar 
membrane. This motion has been described as a travelling wave. The amplitude 
of this wave is greatest at the point where the frequency of the incoming sound 
matches the frequency of the movement of the basilar membrane. This in turn 
depends on the stiffness and width of the basilar membrane, which varies as 
described above. Thus, different frequencies of the incoming sound affect 
different areas ofthe basilar membrane - this is called tonographic organization. 
High frequencies result in movement near the oval window, whereas low 
frequencies produce movement at the curled-up inner end. Figure 6.3 shows the 
approximate locations of sensitivity to specific frequencies along the basilar 
membrane. Basically, the basilar membrane performs a frequency analysis of 
incoming sounds like the one performed by the Fourier analysis for the creation 
ofa spectrogram (see section 5.1.2). 

Chapter 6 
187 
The movement ofthe basilar membrane causes movement in the hair cells of 
the organ of Corti, which transforms them into neural signals. As different 
frequencies affect the basilar membrane in different places, the different 
frequencies of an incoming speech signal can be transmitted individually to the 
brain via the auditory nerve, as will be explained below in section 6.4. A very 
strong movement of the cochlear fluid due to very loud noise may cause some 
hair cells to die. This is a common reason for partial hearing loss, which is why 
users of heavy machinery or producers and listeners of loud music should wear 
earplugs or earmuffs. 
2000 
4000 
Figure 6.3. Frequency resolution on the basilar membrane. 
Like the outer and the middle ear, the inner ear does not simply transmit 
properties of sounds, but contributes to shaping the incoming sound. Active 
vibrations of the hair cells of the organ of Corti pre-amplify the sound and thus 
contribute to the frequency resolution, the detection of the different frequencies 
of an incoming sound. In addition, neural impulses from the brain can make the 
hair cells move the basilar membrane and thus create sound, a phenomenon 
called otoacoustic emission, which can be recorded with a microphone in the ear 
canal. It is thought that this process supports the ability of the basilar membrane 
to detect differences in the acoustic properties of incoming sounds. 
6.4 The internal auditory system 
The internal auditory system consists of the auditory nerve and the auditory 
cortex. The latter forms the essential organ of hearing, since perception does not 
happen in the ear but only when the auditory sensations have reached and been 
processed by the relevant area in the brain. Information from the inner ear is 

188 
Speech perception 
passed on to the brain in the following way: the organ of Corti on the basilar 
membrane converts the motion of the basilar membrane and the cochlear fluids 
into electrical signals that are communicated via neurotransmitters to thousands 
of nerve cells (approximately 28.000 in each organ of Corti). The nerve cells 
themselves create action potentials that travel along the auditory nerve to the 
relevant areas in the brain for further processing. Each fibre of the auditory 
nerve is connected to a small area of the basilar membrane or even to a single 
hair cell alone. Since specific areas on the basilar membrane react to specific 
frequencies only (see section 6.3 above), each fibre of the auditory nerve is 
optimally stimulated by a specific frequency. The ability to hear certain sounds 
thus depends on the frequency resolution ofthe fibres. Many sounds are too high 
(have a too high frequency) for human ears. Age-related hearing loss, which 
apparently begins as early as in the twenties, is due to the loss oftransmission of 
high frequencies by the relevant fibres. Damage of the inner ear (for example 
through excessive noise) leads to an increased insensitivity of the fibres, and 
their 'tuning' to certain frequencies is lost. This means that acoustically complex 
signals like speech (see sections 5.2 and 5.3) cannot be analysed precisely 
anymore, and speech perception is impaired. 
Figure 6.4. The auditory cortex. 
The auditory nerve does not only transmit the frequency of asound to the brain 
but also passes on information about the duration of the speech signal and its 
intensity. The area ofthe brain that is active in speech perception is the auditory 
cortex, which can be further divided into the primary, secondary and tertiary 
auditory cortex. It is located on the temporal lobe, as illustrated in Figure 6.4. 
This area is activated during the perception of speech and its decoding into 
words and utterances. Like for speech production (see section 2.6), the left 
hemisphere of the brain is crucially involved in the perception of speech for 

Chapter 6 
189 
right-handed people (for some left-handed people this does not apply): more 
activity can be measured in the left hemisphere than in the right hemisphere 
when listeners hear speech. The left hemisphere appears to be responsible for 
decoding the meaning of a linguistic message, whereas the right hemisphere 
seems to be more involved in the perception of music and the prosodie 
characteristics (especially pitch) of language. Investigations of patients who 
suffered damage to the left hemisphere show that they are likely to have 
difficulties with understanding simple words and with selecting an object that 
was named. Patients with damage in the right hemisphere, conversely, often lose 
their ability to understand metaphoric speech 
01' idioms. Instead of 
understanding the idiomatic meaning of an expression such as "He kicked the 
bucket", they will interpret this utterance literally. A study by Shaywitz et al. 
(1995) suggests gender-specific language processing: women tend to process 
phonologically relevant information with both hemispheres, whereas men use 
only one. 
At present, little is known about the exact location 01' function of the parts of 
the brain relevant for sound discrimination and sound identification, and most of 
our current knowledge is based on animal experiments. It is assumed that the 
various neurons of the auditory cortex are organised according to the frequency 
of sound to which they respond best. Furthermore, the area of the brain called 
the primary auditory cortex seems to be responsible for the differentiation of 
voiced and voiceless sounds, whereas changes in pitch are perceived in the 
secondary auditory cortex. Tone, the linguistic use of pitch for the 
differentiation of meaning (see section 4.3), appears to be perceived also in 
places in the cortex other than the auditory cortex. 
Both the peripheral and the internal auditory system are able to adapt to 
characteristics of the incoming speech signal and thus contribute to the 
differentiation of perception. For example, the hearing threshold can be altered 
so that the subjective difference between the loudness of two sounds is 
increased. This means that even in noisy conditions humans can focus their 
attention on a single sound source. This ability was first described by Cherry 
(1953), who termed it the Cocktail party effect. It comprises two different 
phenomena: first, in a mixture of conversations, listeners are able to concentrate 
on a single speaker. Second, even when concentrating on one conversation, 
when someone mentions your name in another conversation you will notice this. 
Both abilities are based on active noise suppression by the auditOl)' system: the 
sound source one is concentrating on seems up to three times louder than the 
other ambient noises. 

190 
Speech perception 
6.5 The perception of loudness, pitch and voice quality 
It is one of the characteristics of human speech perception that changes in the 
acoustic properties of speech sounds do not correspond linearly to changes in the 
sensations experienced by a listener. This holds true for the perception of pitch 
and its relation to the frequency of tones. It further applies to the perception of 
loudness, where amismatch between objectively measured sound pressure and 
subjectively perceived loudness exists, as we11 as for the perception of voice 
quality, or timbre. Studies concemed with the relationship between acoustic 
properties and human speech perception are ca11ed psychoacoustic studies. 
Humans can perceive a wide range of different frequencies (see section 5.1.1 
for a description of the physical property frequency) ranging from 20 to about 
20,000 Hz (with age, the ability to perceive the higher frequencies drops). 
Frequencies lower than 20 Hz do not activate the hair ce11s of the organ of Corti 
and are thus below the threshold of hearing. Psychoacoustic experiments have 
shown that changes in absolute frequency are not perceived equa11y we11 for a11 
frequency regions. Humans are by far most sensitive to changes in frequency 
below 1000 Hz. Above 1000 Hz the ability to notice sma11 changes in absolute 
frequency decreases progressively. The experiments show that a pitch change 
that corresponds to a change from 100 to 500 Hz is not perceptua11y equivalent 
to a pitch change experienced when the frequency of a tone is changed from 
6100 to 6500 Hz. This characteristic ofhuman hearing is caused by the structure 
of the basilar membrane. Figure 6.3 above illustrates that a large part of the 
thicker end of the basilar membrane responds to frequencies below 1000 Hz, 
whereas only a sm a11 part responds to frequencies of above 12,000 Hz. This 
means that changes in the lower frequency range are more easily detected than 
changes in the higher frequency range. 
In order to depict the relationship between subjective perception of pitch and 
absolute frequencies a scale was developed, the Mel scale. Figure 6.5 i11ustrates 
how the perceptual difference between two frequencies depends on the absolute 
magnitude of the frequencies. Asound wave with a frequency of 1000 Hz has 
the Mel value of 1000. It is perceived as twice as high as asound wave with 500 
Mel. This sound wave, however, has a frequency of 400 Hz. It can be seen that 
below 1000 Hz there is a fairly direct relationship between frequency and 
perceived pitch, but above this point the relationship becomes logarithmic. The 
units of a logarithmic scale represent powers of ten, i.e. the difference between 2 
and 3 corresponds to the difference between 102 and 103. 
The findings of psychoacoustic studies on the perception of frequency are 
not of direct relevance to the perception of sounds since they use simple 
sinusoidal waves with one frequency only. Many speech sounds, however, are 
complex and consist of a combination of different sinusoidal waves with 
different frequencies (see section 5.1.2). The lowest of them, the fundamental 

Chapter 6 
191 
frequency, is perceived as pitch, whereas the others contribute to the timbre of a 
sound. The perception of the timbre of asound is independent of its loudness 
and pitch. It is the energy distribution of the different frequencies of asound that 
determines the perception of its timbre. Thus, asound of the same loudness and 
same pitch produced by a flute or a human voice yields a very different 
sensation for listeners. Experiments have shown that even when the fundamental 
frequency is removed from asound it can still be perceived by listeners. It seems 
that listeners decode the composition of the sound (remember that there is a 
linear relationship between the fundamental frequency and the harmonics, see 
section 5.1.2) and derive the fundamental frequency from it. 
2500 
2000 
~ 1500 
1000 
500 
OL-____ 
~
 
____ 
~
 
______ 
~
 
____ _L ______ L_ ____ _L ____ 
~
 
____ 
~
 
o 
1000 
2000 
3000 
4000 
5000 
6000 
7000 
8000 
Hertz 
Figure 6.5. The Mel scale. 
Psychoacoustic experiments concemed with the sensation of loudness show that 
humans can perceive a remarkable range of sound intensities. The hearing 
threshold lies at about 20 microPascals (f1Pa), whereas the highest perceptible 
intensity is about a million times greater. Like with the perception of pitch, there 
is no linear relationship between changes in sound pressure and changes in the 
sensation of loudness. In a typical experiment on the relationship between sound 
pressure and the perception of loudness, listeners are asked to adjust the 
loudness of one sound so that they perceive it as twice as loud or as half as loud 
as another. It has been found that the relationship between sound intensity and 
perceived loudness is far from linear. A doubling of the intensity of asound is 
not perceived as a doubling of the sensation of loudness. In fact, the 
correspondence between intensity and perceived loudness is nearly logarithmic. 
With the help of these psychoacoustic experiments, a scale of subjective 
loudness can be drawn, similar to the Mel scale for the perception of pitch. 
Figure 6.6 illustrates the relationship between the subjectively perceived 

192 
Speech perception 
loudness, measured in the unit sone, and objective sound pressure level, 
measured in microPascal (/lPa). It can be seen that for quiet sounds relatively 
small changes in sound pressure lead to large changes in perceived loudness -
the left end of the curve rises steeply. Conversely, for loud sounds, changes in 
loudness are only perceived when relatively large changes in sound pressure 
occur. 
" 
c 
20 
18 
16 
14 
12 
5l 
10 
o~-----+------~----~----~ 
o 
500.000 
1.000.000 
1.500.000 
2.000.000 
sound pressure level (flP) 
Figure 6.6. Soundpressure level in relation to sone. 
Table 6.1. Sound press ure levels and corresponding dE values Jor different 
sensations. 
Sensation 
threshold of hearing 
whisper, quiet garden 
very quiet speech, tearing of paper 
normal conversation 
car passing, vacuum cleaner 
noise inside a jet, loud music 
brass band, train 
thunder, threshold ofpain 
ear damage 
Sound pressure (/lPa) 
20 
200 
2,000 
20,000 
100,000 
200,000 
2,000,000 
20,000,000 
200,000,000 
dB 
o 
20 
40 
60 
70 
80 
100 
120 
140 

Chapter 6 
193 
The other measurement of relative loudness used to describe human 
perception of loudness is decibel (dB). Its relationship to sound pressure is 
logarithmic and thus represents human perception very weil. Table 6.1 gives the 
dB levels and corresponding sound pressure levels of some sensations. It is 
important to remember that sound intensity expressed in dB, since it is a 
logarithmic measurement, is always relative to some reference point. This 
reference point is usually the threshold ofhearing. 
Listening experiments have further shown that perceived loudness varies as 
a function of frequency. The human auditory system is not equally sensitive to 
all frequencies. In psychoacoustic studies, listeners are asked to adjust tones of 
different frequencies so that they are perceived as equally loud. Very low 
frequencies and very high frequencies need to be amplified so that a human 
perceives them as equally loud as frequencies in the middle range. Figure 6.7 
illustrates the hearing threshold, i.e. the lowest sound pressure level necessary to 
perceive asound for different frequencies. Clearly, the human auditory system is 
most sensitive to frequencies between 2000 and 5000 Hz. In this frequency 
region, only a very low sound pressure level is necessary to cross the hearing 
threshold. Very low frequencies require the highest sound pressure levels In 
order to be perceptible to humans. 
,.-.. 
Pd 
80 
'-' 
'W 
60 
~ -
~ 
40 
S 
Yl 
Yl 
~ 
20 
.... 
0.. 
"0 § 
0 
0 
Yl 
20 50 100 200 
500 1 k 2 k 
5 k 
10 k 
frequency (Hz) 
Figure 6.7. Threshold 0/ hearing - the relationship between jrequency and 
intensity in human perception (J k = 1000 Hz). 
6.6 Measuring hearing sensitivity 
The method concerned with measuring hearing sensitivity is called audiometry. 
It is used both for the detection of hearing impainnent and hearing loss as weil 
as the examination of nonnal hearing. Hearing impairment or a complete 

194 
Speech perception 
hearing loss can have different causes. An infection ofthe middle ear can lead to 
so-called conductive hearing loss, in which case the sound pressure waves are 
not passed on through the ossicles in the middle ear. Other illnesses such as 
measles or meningitis can cause damage of the cochlea or the auditory nerve. A 
sudden loud noise or continued exposure to high noise levels (such as machines 
or music) can cause permanent damage to the hair cells on the basilar 
membrane, which then cannot transmit sounds effectively anymore. For the 
testing of hearing sensitivity, simple sinusoidal waves, so-called pure tones, 
with different frequencies (e.g. 250 Hz, 500 Hz, 1000 Hz, 2000 Hz and 4000 
Hz) are played successively to each ear, first at a very low intensity and then 
increasingly louder until the listener indicates that she or he can perceive it. As 
shown in Figure 6.7, the threshold of normal hearing for these frequencies is 
around -10 dB to +20 dB. The extent ofhearing loss is usually indicated using 
the unit dB. A hearing loss of 50 dB, for example, means that a particular 
frequency must have an intensity level of 50 dB before it can be perceived. 
When the hearing threshold for each frequency is established, an audiogram can 
be made for each ear. Figure 6.8 illustrates a pure tone audiogram for anormal 
hearing person. 
-20-
- -
--~------
-10 
o -
11 1 I 
10-
1 
20 
30 
dB 
40 
50 
60 
70 
80 
90 
100 
---,-------_ .. ------:--
0 
1000 
2000 
3000 
4000 
5000 
6000 
7000 
8000 
frequency (Hz) 
Figure 6.8. Average audiogram lor a normal hearing person. (The bars indicate 
the error interval 01 the measurement.) 
Several methods exist to treat hearing impairment and hearing loss. Hearing 
aids wom behind the pinna contain small microphones that amplify the 
incoming sounds. The amplification of specific frequencies is regulated for each 
individual patient depending on which frequencies are affected by hearing loss. 

Chapter 6 
195 
The weakness of hearing aids compared to normal hearing lies in the fact that 
they cannot adapt like the auditory system. As described above in section 6.4, 
these adaptive abilities of the internal auditory system enable listeners to tune 
into a particular sound source and to fade out others. A hearing aid, conversely, 
amplifies every sound equally much and does not perform this kind of 
focussing. Babies born without hearing often receive a cochlear implant that 
stimulates the auditory nerve with electrical impulses. This implant is placed 
under the skin behind the pinna. An additional sm all device containing a 
microphone, a speech processor and a transmitter is worn behind the pinna, 
filters sounds and transmits them to the cochlear implant. An implant does not 
create normal hearing but, together with speech therapy, enables children to 
learn to speak. Yet, several studies have shown that the acoustic properties of 
speech children with cochlear implants produce vary systematically from those 
of speech produced by normal-hearing children (e.g. Mildner & Liker 2003). 
6.7 Theories of speech perception (advanced reading) 
Section 6.5 described how the various parameters of speech, in particular pitch, 
loudness and timbre, are perceived by humans. In fact, psychoacoustic 
experiments have shown that the auditory system enables speakers to identify 
differences in frequency and intensity as weil as in time (duration) to a much 
higher degree than is necessary for the discrimination and identification of 
speech sounds. They have also shown that these perceptual abilities - which are 
referred to as psychoacoustic abilities - of humans are a necessary prerequisite 
for the understanding of speech but do not suffice to explain how speech 
perception works. Speech perception involves more than the perception of 
frequencies, intensities and duration: it requires the linguistic interpretation of 
the neural signal transmitted to the auditory cortex. In other words, speech 
perception describes how a listener decodes the acoustic signal as a meaningful 
message instead of as a sequence of noises with different frequencies and 
loudness. Speech perception is by no means a trivial task. Section 2.5 showed 
that the same sound and the same word can have very different acoustic 
realizations when produced by speakers with different anatomies of the 
laryngeal and the vocal tract, when produced at a different speaking rate or in 
different phonetic contexts. How are these variable productions of asound, word 
or utterance identified as the same by listeners? Section 2.5 further showed that 
there are no boundaries between successive speech sounds or words in a breath 
group. How do listeners extract meaningful units from this continuous acoustic 
stream? 
These are the questions all of the models and theories of speech perception 
are concerned with. It is still not possible to examine the activity of the brain in 

196 
Speech perception 
such detail that it would allow the identification of areas or processes underlying 
speech perception. We still cannot measure exactly how and where in the brain 
an auditory signal is converted into a linguistic message that can be interpreted 
according to its grammatical and semantic content. Therefore, we have to rely 
on theories and models to explain the various questions conceming speech 
perception: How are individual speech sounds identified? How can listeners 
cope with the variability of the speech signal? How is an auditory signal 
decoded and interpreted as a speech sound, a word or a sequence of words? 
Which mental representations of speech do speakers have? Which linguistic 
units are stored? How are they stored? 
Several experiments have been concemed with the question of how 
individual speech sounds are discriminated, Le. how listeners can tell that they 
are the same or different. It has been found that humans are very good at 
identifying small differences between two speech sounds, an ability which has 
been termed categorical perception. Categorical perception means that gradual 
differences between acoustic properties of sounds are not perceived in a linear 
fashion, but that listeners rather group sounds into categories. This can be 
exemplified with a typical experimental set-up: the two plosives [p] and [b], for 
example, differ in voicing. This means that during the production of the voiced 
plosive [b], the vocal folds usually vibrate, whereas during the production of a 
[p] they do not. Yet, for all [b]s produced by different speakers the exact timing 
of the vocal fold vibration can vary - it might already have started before the 
lips elose to obstruct the airstream, it might coincide exactly with the elosure of 
the lips, start a short while after the elosure, start with the opening of the lips or 
any time after it. In the production of a [p] vocal fold vibration might start 
immediately after the opening of the lips or several milliseconds later when the 
following sound is already being articulated. Many variations of this relative 
voice onset time (VOT, see section 5.3 for a detailed description) occur in 
plosives and determine whether they are perceived as voiced or voiceless. In 
English, a typical [b] has a VOT of about -20 to +20 ms, i.e. on average voicing 
begins between 20 ms before and 20 ms after the opening of the lips. Typical 
realisations of [p], by contrast, have a VOT of between +40 and +80 ms, which 
means that 40 to 80 ms pass after the lip opening and before the beginning of 
voicing. 
In an experiment on categorical perception, a set of synthesised, attificially 
created sound examples consisting of a plosive and a following vowel (e.g. [ba, 
pa]) are prepared that vary in VOT in a systematic way. Participants listen to 
these plosive-plus-vowel stimuli in a random order. For example, in one of 
them, the plosive has a VOT of -20 ms, the next might have a VOT of+15 ms, 
the next one a VOT of -10 ms and so forth. When listeners hear these stimuli 
and are asked to decide wh ether they perceive [pa] or [ba], they do not hesitate 
over those stimuli that form borderline cases between voiced and voiceless 

Chapter 6 
197 
plosives (e.g. with a VOT of +20 ms). Rather, listeners shift abruptly from the 
perception of [b] to the perception of [p] despite the fact that the listening 
examples are evenly spaced on a continuum in terms of their VOT. Within the 
two phonetic categories of [b] and [p], listeners do not differentiate between 
sounds with a slightly longer or shorter VOT; discrimination only occurs 
between the categories. 
Categorical perception enables listeners to differentiate between different 
speech sounds. But how do they identi:fY these speech sounds? Listeners cannot 
only tell that a [b] is different from a [p], but are also able to recognize a [b] as a 
/b/ and a [p] as a /p/. F or this recognition and identification of speech sounds, 
speakers must have mental representations of speech sounds that they can 
compare to the incoming speech signal. Chapters 3 and 4 explained that 
phonological theories assume that speakers have abstract mental representations 
of many different units of speech. The smallest unit that is claimed to be 
represented mentally is that of speech sounds, the phonemes. What does such a 
representation of a speech sound look like? This representation must be able to 
deal with the variability of the sounds speakers produce: depending on the 
particular shape of a speaker's speech organs, on the neighbouring sounds and 
on the speaking rate, a simple sound like [i] can have very different acoustic 
properties (see sections 2.5 and 5.2). Still, listeners can identi:fY the different 
physical signals as the same sound. 
Some authors argue that this is due to fact that there are invariant acoustic 
correlates of speech sounds, which enable listeners to recognize different sounds 
(e.g. Stevens & Blumstein 1981). These invariant cues lie on the level of the 
phonetic features and are thus smaller units than phonemes. Sections 5.2 and 5.3 
describe in detail which acoustic properties the individual speech sounds of 
English and classes of speech sounds in general have. For example, one acoustic 
cue that supports the identification of fricatives is the spectral structure of the 
'noise' or friction created by the narrow constriction of the vocal tract during 
articulation. Depending on the place of articulation, this friction has different 
acoustic properties: in a typical [j], for example, there is a peak at about 2500 
Hz with additional energy between about 4000 and 8000 Hz, whereas a /s/ has 
most ofthe noise in the frequency range of 5000 to 8000 Hz (see section 5.3 for 
further details). The identification of voiced plosives such as /b/, /d/ and /g/ 
relies on the so-called formant transitions, i.e. the change in formants from a 
preceding consonant to a vowel or from a vowel to a following consonant (see 
section 5.2 for an explanation of what a formant is). The particular place of 
articulation can be inferred from the movement of the second formant, F2 (see 
section 5.3 for details). The different manners ofarticulation for consonants also 
have invariant acoustic cues. Nasals, for example, have a band of frequency with 
very high energy at about 250 to 300 Hz and little energy above that and below 
2000 Hz (see section 5.3 for details). The perception of vowels is more 

198 
Speech perception 
complicated. Perception experiments have shown that vowels are distinguished 
from each other with the help of the relative position of the first three formants. 
Speakers are able to 'normalize' acoustic differences between vowels that occur 
in the speech of different speakers (for example, female, male and children's 
voices); they identify vowels not on the basis of absolute acoustic values but 
decode how the formants occur relative to each other (see section 5.2). 
However, most theories of speech perception assume that the identification 
of individual speech sounds is not necessary for the understanding of words and 
utterances. There is no doubt that listeners can identify individual speech sounds 
and have mental representations of them, but it is very unlikely that they use this 
ability when they listen to real speech. There is some evidence that listeners use 
the unit of the syllable for speech perception. The motor theory of speech 
perception (Liberman et al. 1967, Liberman & Mattingly 1985), for example, 
claims that listeners decode a sequence of sounds in a syllable with reference to 
their articulatory knowledge. They are assumed to compare intemally the 
incoming sound sequence to their mental representations ofthe muscle activities 
underlying the production of this sound sequence. Listeners have 'perceived' a 
syllable when a match to a stored motor sequence is made. Furthermore, studies 
on speech errors also claim that the syllable is a basic unit of representation for 
speech perception. 
In summary, there is still little agreement conceming the size or type of 
basic perceptual units in speech perception at present. Is it mental 
representations of phonetic features, phonemes or rather syllables that enable 
speakers to interpret acoustic signals as meaningfullanguage? Most probably, a 
combination of all units that have been suggested so far contribute to speech 
perception. Possibly depending on the task - whether we are listening to speech 
in noisy conditions, whether we are familiar with the language and whether the 
speaker is articulating carefully or speaking at very high speed - the different 
mental representations are employed for speech perception to a different degree. 
Listeners are not only able to recognize and identify sounds and syllabies, 
but they also understand words. Experiments have shown that words are 
recognized very quickly, in between 200 and 500 ms after the first sound is 
produced by the speaker (Marslen-Wilson 1987). It is assumed that spoken 
word recognition involves matching an acoustic signal to a representation in 
memory. All theories about the recognition of words share the assumption that 
this mental representation of words has the form of a mentallexicon. Apart from 
phonological information, the mental lexicon is considered to contain 
information about the spelling of each word and its syntactic, semantic and 
pragmatic properties. The mental lexicon is probably represented in a special 
area of memory located in the brain's left hemisphere and comprises 
approximately 75,000 words for an adult speaker of English with average 
education. Moreover, there is evidence that the mental lexicon does not only 

Chapter 6 
199 
contain words, but also groups of word forms for each word (the various 
inflected forms) as weil as formulaic sequences of words, such as phrases used 
very frequently in conversation (e.g. "howare you", "you know what I mean") 
and idioms. 
Several competing theoretical models exist that describe the stages necessary 
for the recognition and identification of words. Generally , it is assumed that the 
acoustic information reaching the brain activates several hypotheses in the 
mental lexicon (e.g. McClelland & Elman 1986, Norris 1994). At this stage, 
acoustic variation due to coarticulation and rate of speech is 'normalized'. This, 
however, does not exclude that information about a speaker's voice is used in 
speech perception (cf. Pisoni & Lively 1995). In the second stage, these 
hypotheses 'compete' with each other so that finally all hypotheses except one 
are discarded. The remaining hypothesis is what is perceived by the listener. 
There is empirical evidence that this lexical access is faster for familiar words 
than for unfamiliar words. That is, words that occur very often in a language and 
for a particular speakerllistener are recognized faster and more accurately than 
words that speakers/listeners use less often. 
Most theories on speech perception agree that understanding speech is not a 
passive process. It is not true that listeners simply process the incoming signal in 
a linear way, match the units one after another to a stored template and finally 
interpret them accordingly. This process, wh ich has been called bottom-up (from 
ear to brain), cannot describe why listeners understand words even if they are 
mispronounced or when some sounds are distorted by noise. Much rather, 
listeners must also employ top-down processes (i.e. an active participation of 
mental representations) in the interpretation of speech. This can be demonstrated 
with many different examples. Experiments show that listeners are able to 
perceive a word even when one or two sounds in the sound sequence are 
replaced by noise (Warren 1970, Samuel 1981) or when speech is produced in 
very noisy conditions. Even when part of the signal is effectively covered, as 
when the [n] in sand is masked by a loud cough, the listener will perceive 
[srend] and will not be able to say which of the sounds was obscured by the 
noise. Furthermore, listeners often perceive what was not said. A speaker might 
say "The women has come" but the listeners will still claim to have heard "The 
woman has come". These examples are taken to confirm the top-down nature of 
speech perception, i.e. the active role of the representations in the brain in 
speech perception. Probably, both bottom-up and top-down processes combine 
to ensure fast and robust speech perception. 
Speech perception does not rely on acoustic cues only. McGurk & 
McDonald (1976) showed in an experiment how much speakers rely on visual 
cues in speech perception. They played a video of a face saying one sound 
sequence, e.g. [gaga], but combined it with the audio recording of another sound 
sequence, e.g. [baba]. Listeners not looking at the face heard [baba] and 

200 
Speech perception 
watchers of the video with the sound turned off identified the motion of the 
articulators as [gaga]. However, when simultaneously watching and hearing the 
non-matching sound and video, listeners perceived something like [ dada], even 
those listeners that had heard or seen the video or the recording on their own 
before. This finding, which was named the McGurk effect, was interpreted in 
the following way: when the brain receives conflicting information from the 
auditory and the visual channel, it will settle on amiddie ground, in this case 
[ dada]. Many later studies have confirmed the effect; it is stable even with 
babies (Rosenblum, Schmuckler & Johnson 1997) or when the video screen is 
turned upside down (Campbell 1994) or with a combination of male face and 
fern ale voice (Green, Kuhl, Meltzoff & Stevens 1991). 
6.8 Speech perception and language acquisition (advanced reading) 
6.8.1 Speech perception in first language acquisition 
Newborn babies have a wide range of perceptual abilities: they can recognize 
their mother's voice, they can recognize their parents' language and they are able 
to differentiate a number of different speech sounds. These abilities result from 
the fact that a foetus in the womb can already hear during the last weeks of 
pregnancy. The middle ear physiology starts working at around 3 months before 
birth, and the foetus can perceive sounds - although these sounds have different 
acoustic characteristics, since they are not transmitted by air but by the fluids in 
the womb. Yet, these pre-natal sensations seem to be sufficient for babies to be 
born with first perceptual representations of speech. 
Several methods have been developed to study infant speech perception. The 
High-Amplitude Sucking technique (HAS) is based on the observation that 
humans, on the one hand, become bored by repeated sensations and, on the 
other, react to changes they perceive. In HAS studies, infants suck on a dummy 
that re cords their sucking rate. When the infant's 'normal' sucking rate - the 
baseline - is established, a repeating sound sequence such as [ba ba ba] is 
played. An immediate increase in sucking rate shows that the infant has 
perceived this sound. After a while, the sucking rate will decrease again, which 
is interpreted as waning interest. Then a new sound sequence is introduced, for 
example [ga ga ga] spoken at the same rate and by the same voice. Ifthe sucking 
rate increases, it is concluded that the infant is interested, i.e. has noticed the 
difference and therefore is able to differentiate between the two sounds [b] and 
[g]. Another method oftesting infant speech perception is the head-turn method, 
for which sounds are presented from different sources. If infants turn their head 
towards the source of a new sound, it is concluded that they can perceive the 
difference. 

Chapter 6 
201 
With the help ofthese methods, it has been found that right after birth babies 
prefer speech recordings from their native language to those of other languages 
(Mehl er et al. 1988). Infants furthermore can discriminate between nearly every 
acoustic contrast, including those that do not occur as functional contrasts in 
their language (see Werker & Pegg 1992 for a summary). Babies growing up in 
an English-speaking environment are able to discriminate voicing contrasts as 
for example between [ta] and [da], differences in pI ace of articulation as in [ta] 
and [ka], and differences in manner of articulation such as between [ta] and [sa]. 
Similarly, they can discriminate vowels such as Ci] and Ca], but vowel 
discrimination seems to be - as for adults - continuous rather than categorical. 
Moreover, they can discriminate voiceless aspirated [tha] and breathy voiced 
dental stops [dha], a contrast that is not phonologically relevant in English but in 
Hindi. 
While infants up to about 8 months of age are able to discriminate sounds 
that play no contrastive role in their native language/s, this ability is lost at 
around 9 months. From then on, babies can only discriminate sounds that 
function as phonemes (see section 3.1 for the definition of phoneme) in their 
native language/s. This is interpreted as the result ofthe influence ofthe ambient 
language/s - it probably stems from a reorganization of the initial auditory 
capabilities. It is assumed that a phonological system of the native language/s is 
constructed gradually over the first years of life. Yet, some phonemic contrasts 
remain difficult to discriminate for English-speaking children even at age three. 
These difficulties concern mainly the pairs 111 and Iwl and 181 and If/. The 
construction of amental lexicon with phonological representations of words 
proceeds slowly and continues well into the teens. At around 2.5 years of age, 
mental representations of some words seem to begin to be established. Babies, 
for example, look at a picture of a dog significantly longer when they hear [dog] 
than when [bog] is played to them. In general, perception seems to precede 
production. There are many anecdotes and studies that show that children have 
stable mental representations of the phonological shapes of words before they 
can pronounce them correctly themselves. Yet, even school-age children are still 
much slower to recognize words than adults. 
6.8.2 Speech perception in second language acquisition and teaching 
When comparing the acquisition of speech perception in first language (L 1) and 
second language (L2) acquisition, it becomes clear that the two differ distinctly. 
One of the major differences lies in the fact that learners of a second language 
have already acquired perceptual categories for the sounds, words and other 
linguistic units of their first language. The course of development of perceptual 
representations in an L2 can be roughly sketched as folIows: initially, when 

202 
Speech perception 
listening to the foreign language, second language learners do not understand 
anything. This is clearly due to the fact that they have no mental representations 
of speech sounds, syllables and words and their meaning to which the incoming 
acoustic signal could be matched. It is the task of language learners to acquire 
all phonological representations necessary for speech perception. This probably 
involves representations of phonetic features, phonemes, syllables and words as 
weIl as intonational units (section 6.7 showed that our knowledge in this area is 
still sketchy). Until these representations have reached a native-like form, 
language learners will always find it more difficult and take more time to 
und erstand L2 speech than native speakers do, especially in unfavourable 
conditions such as in noisy places and without visual cues (Werker et al. 1992 
found that L2 learners also use visual information in speech perception). This 
can be explained by the fact that language learners have fewer top-down 
processes available (compare section 6.7). 
Unfortunately, we still have very little empirical data on the acquisition of 
perception in an L2. As is the case with speech production, only few 
longitudinal studies exist that document the development of perceptual abilities 
across longer stretches oftime. UsuaIly, learners or learner groups are studied at 
one point in time only. These studies have shown that learners of a second 
language do not perceive speech in the L2 in the same way as native speakers of 
this language do. For example, Barry (1989) found that German learners of 
English confuse the vowels lei and lre/. They rely on durational differences in 
the identification of these vowels, whereas American English listeners use 
formant differences for identification (Bohn & Flege 1990). L2 learners also 
perceive intonation and word stress differently than native speakers (e.g. Grabe 
et al. 2003, Archibald 1997). Various factors seem to influence how weIl L2 
learners perceive sounds of the L2. For Italian immigrants to Canada, age at 
arrival and amount of continued use of the LI influence perceptual abilities 
(Flege & MacKay 2004). Those who arrive at a later age and continue to speak 
more Italian have more difficulties in perceiving English speech sounds than 
speakers emigrating at a younger age and not using their LI very often anymore. 
It seems, however, possible to acquire native-like perception - the ability to 
perceive new linguistic contrasts is not lost at any point in life. Training studies 
have shown that adult learners can acquire the differentiated perception of 
foreign sounds (see overview in Rvachew & Jamieson 1995). In Bohn & Flege's 
(1990) study, those German learners with a large amount of experience with 
English (more than five years of residence in the U.S.) perceived the vowels lei 
and lrel similar to native listeners, whereas learners with shorter periods of 
residence did not. Differences between children's and adults' perceptual abilities 
appear not to be based on physiological differences, but rather on their mental 
representations of LI sounds that make them pay attention to the particular 
acoustic cues relevant in the LI. 

Chapter 6 
203 
On the basis of these empirical findings, several theories of L2 perceptual 
acquisition have been formulated. Most of these theories focus on the unit of 
speech sounds (however, one of them - Archibald [1994] - is concerned with 
the acquisition of the perception and production of word stress). The theories 
disagree about whether the mental representations underlying the identification 
of speech sounds consist of phonemes (e.g. Flege 1995) or articulatory gestures 
(e.g. Best 1995, Dziubalska-Kofaczyk 1990), but they agree on the assumption 
that a language learner perceives the L2 sounds through categories of the 
phonological structure of the LI. These categories constrain which non-native 
sounds can be perceived correctly and, in turn, can be learned to produce 
correctly. 
Not all L2 contrasts and categories are equally easy or difficult to perceive. 
The acquisition of L2 sounds that do not exist in the speaker's LI and that are 
perceptually very different from LI sounds is relatively easy: new perceptual 
categories are created quickly for these sounds. The greater the perceived 
phonetic dissimilarity between an L2 sound and LI categories, the higher is the 
chance that a new mental category will be established. A German learner of 
Zulu, for example, will have litde trouble noticing that the click sounds (see 
section 2.1) are very different from German speech sounds and will easily 
acquire amental representation of them. Yet, this category for the L2 sound may 
still be different from the category of a native speaker. 
Flege (1995) predicts that when no phonetic differences between two sounds 
are perceived by the learner -
a process which is labelled equivalence 
classification - category formation for an L2 sound will be blocked and the 
learner will end up with the representation of a single phonetic category for both 
sounds. On the one hand, this happens when two L2 sounds are assimilated to 
the same LI category (for example, some German leamers perceive both Iwl and 
lvi in English as lvi). On the other hand, two L2 sounds might be assimilated 
into two different LI categories (/81 and 101 in English might be categorized as 
Isl and Id/, respectively). Not until learners notice these wrong categorizations 
will they be able to establish new perceptual categories. 
A theoretical model of L2 phonological acquisition within the general 
framework of Optimality Theory was proposed by Boersma (1998): Functional 
Phonology. It distinguishes between articulatory and perceptual representations 
and features, as illustrated in Figure 6.9. In speech perception, a hearer is 
confronted with the acoustic input of speech. This acoustic input consists of 
physical properties such as frequency, loudness and noise and is put into square 
brackets in Figure 6.9 because it is language-independent. The hearer's 
perception grammar, which consists of a perceptual categorization system, 
converts the raw acoustic input into a more perceptual, language-specific 
representation. This perceptual input is put between slashes in Figure 6.9. It is 
interpreted by the recognition system, which converts it into an underlying form 

204 
Speech perception 
(written between pipes). A speaker takes the perceptual specification of a word 
or syllable as stored in his or her lexicon as the input to the production grammar, 
which determines the surface form of the word or utterance. This results in an 
articulatory output in terms of articulatory gestures. The speaker's perceptual 
output is his or her acoustic output as perceived by himself or herself. This can 
be compared to the perceptual input of an acoustic input by another speaker and 
thus forms the essential part ofthe leaming system. 
IUnderlying fOlml 
Recognition 
grammar 
IlPerceptual input! I 
Perception 
gramm ar 
[Acoustic input] 
-
IPerceptual 
specificationl 
J 
Production 
gramm ar 
[Articulatory output] . __ 
.... -ART 
FAITH 
[Acoustic output] 
IlPerceptual output! I 
Figure 6.9. Perceptual representations proposed by Boersma (1998). FAITH 
stands for faithfulness constraints which monitor the correctness of 
pronunciation. 
Mental representations are imagined, as in Optimality Theory, to consist of 
ranked constraints, either articulatory or perceptual in nature. Since constraints 
are assumed to be violable and can dominate each other, the articulatory output 
depends on the ranking of a set of competing constraints. Functional Phonology 
describes language acquisition in terms of leaming how to rank constraints. 
Second language leamers of Hausa, for example, have a fully specified 
perception and production grammar and perceptual specifications for all 
phonological features of their native language/s, but have no specifications for 
the perception and production of ejectives, iftheir native language does not have 
them as phonemes. In this initial stage, the language leamer will hear the 

Chapter 6 
205 
syllable [k'a] containing the ejective I k'l as the acoustic input, but will perceive 
it as /kal. The resulting underlying form will also be Ikal, and the speaker will 
generate the articulatory candidate [ka] and perceive this as /kai. The first 
learning step consists of the acquisition of the correct perceptual categorization. 
This happens by listening to the language input and noticing that the phonetic 
difference is associated with differences in meaning. The learner will now 
categorize [k'a] as /k'al and have an underlying form of Ik'al. In production, 
however, he or she will still pronounce [ka]. Only when the difference between 
the learners' perception of their own output as Ikal and the perception of the 
native speaker's acoustic output as Ik'aI is noticed, is the next learning step 
triggered. This second stage in the language acquisition process consists of the 
acquisition of the sensorimotor skills for the production of the particular 
phonological feature. In stage 3, in many cases, a faithful rendering of the 
perceptual target becomes possible (especially in imitation and in the production 
of isolated words, Boersma [1998] claims), but in normal speech production the 
articulatory constraint still dominates. For an accurate production in continuous 
speech, a fourth step is necessary, in which the learners must break the identity 
between the perceptual input and the underlying form. In the fourth stage, the 
learners will produce overly faithful speech because they have not noticed yet 
that in certain cases, for example, Ik'l is realized as the allophone [k]. The 
change of the underlying form to Ik'al allows the learners to categorize both 
forms in the perceptual input. Stage 5 involves learning the rules for these 
alternating constraints. The learners must see patterns in the words of the 
language based on morphological and other alternations and must construct 
abstract underlying forms. The input representation of the learners' production 
grammar shifts from the native speaker word to this new underlying 
representation. 
Which implications for second language teaching can be drawn from the 
findings and theories of perceptual acquisition? Since our knowledge in this area 
is still very sketchy, only tentative recommendations can be made. It seems that 
successful perception training should involve multiple voices, the same sounds 
in different syllabic positions and in different phonetic contexts (Pisoni & Lively 
1995, Rvachew & Jamieson 1995) because perception ofL2 speech sounds does 
not occur on the level of phonemes but rather on that of allophones or phonetic 
features. In a perception experiment by Pisoni & Lively 1995, language learners 
learned an L2 contrast in a context-dependent manner and did not seem to 
encode phonetic contrasts in an abstract unh such as the phoneme. They rather 
seemed to store every instance of a sound in every phonetic context in which 
they heard it. Thus, listeners must learn what kind of variation is important and 
what specific attention should be paid - at least at the beginning of language 
learning - to allophonic and coarticulatory variation. This means that the 
perception of speech sounds is supported more when listeners hear reallanguage 

206 
Speech perception 
than when they are exposed to synthetic, artificially created speech. Before 
attempting to produce sounds, language 1earners should be exposed to the 
language and simply listen. It has also been suggested that exposure to 
orthography should be delayed because this may activate the wrang responses 
and create patterns of perception that interfere with apprapriate sound 
perception. Yet, the establishment of perceptual categories does not seem to be a 
necessary requirement for correct production in all cases. Some speakers can 
produce a phonetic distinction without being able to perceive it (e.g. Sheldon & 
Strange 1982). 
6.9 Exercises 
1. Which organs are involved in the perception of speech, and how do they 
work? 
2. What is the role ofthe brain in speech perception? 
3. How can perceptual abilities be measured? 
4. Which relationship is there between acoustic features of sound such as 
frequency and intensity on the one hand and human perception of pitch and 
loudness on the other? 
5. Which units of speech are stored in the brain as mental representations, and 
how are they used for understanding speech? 
6. Why can humans perceive and understand speech even in very noisy 
conditions? 
7. What is categorical perception, and how does this support the identification 
of speech sounds? 
8. What seem to be the major difficulties for second language learners in 
perceiving the second language? 
6.10 Further reading 
The anatomy and physiology of the peripheral auditory system are described in 
Clark, Yallop & Fletcher (2007, chapter 8). Details on the neural processing of 
speech can be gained from Delgutte (1997) and Moore (1997). Johnson (1997, 
chapter 4) describes psychoacoustic experiments, and Lively et al. (1994) give 
an overview of research on the different models of word recognition. Speech 
perception in first language acquisition is described in detail in Vihrnan (1996). 
An overview of speech perception by second language learners can be found in 
Strange (1995) and Strange & Shafer (2008). 

7 List of References 
Abercrombie, D. (1967): Elements 01 General Phonetics. Edinburgh: Edinburgh 
University Press. 
Allen, G. & Hawkins, S. (1978): "The development of phonological rhythm". 
Syllables and Segments, ed. A. Bell & J. Hooper. Amsterdam: North 
Holland Publishing Company. 173-185. 
Archibald, 1. (1994): "A formal model of leaming L2 prosodie phonology", 
Second Language Research 10,215-240. 
Archibald, J. (1997): "The acquisition of English stress by speakers of tone 
languages: lexical storage versus computation", Linguistics 35, 167-181. 
Ashby, M. & Maidment, J. (2005): Introducing Phonetic Science. Cambridge: 
Cambridge University Press. 
Barry, W. (1989): "Perception and production of English vowels by German 
leamers: instrumental-phonetic support in language teaching", Phonetica 
46, 155-168. 
Beckman, M., Hirschberg, J. & Shattuck-Hufnagel, S. (2005): "The original 
ToBI system and the evolution of the ToBI framework", Prosodie 
Typology, ed. S.-A. Jun. Oxford: Oxford University Press. 9-54. 
Behrens, H. & Gut, U. (2005): "The relationship between prosodie and syntactic 
organization in early multiword speech", Journal olChild Language 32, 1-
34. 
Best, C. (1995): "A direct realist view of cross-language speech perception", 
Speech Perception and Linguistic Experience: Issues in Cross-Language 
Research, ed. W. Strange. Timonium: York Press. 171-204. 
Blevins, 1. (1995): "The syllable in phonological theory", The Handbook 01 
Phonological Theory, ed. J. Goldsmith. Oxford: Blackwell. 206-244. 
Boersma, P. (1998): Functional Phonology. The Hague: Holland Academic 
Graphics. 
Bohn, O.-S. & Flege, J. (1990): "Interlingual identification and the role of 
foreign 
language 
experience 
in 
L2 
vowel 
perception", 
Applied 
Psycholinguistics 11, 303-328. 
Bolinger, D. (1981): Two Kinds olVowels, Two Kinds 01 Rhythm. Bloomington: 
Indiana. 
Bolinger, D. (1998): "Intonation in American English", Intonation Systems, ed. 
D. Hirst & A. Di Cristo. Cambridge: Cambridge University Press. 45-55. 
Bongaerts, T., van Summeren, C., Planken, B. & Schils, E. (1997): "Age and 
ultimate attainment in the pronunciation of a foreign language", Studies in 
Second Language Acquisition 19,447-465. 

208 
Chapter 7 
Boysson-Bardies, B. de, Sagart, L. & Durand, C. (1984): "Discemible 
differences in the babbling ofinfants according to target language", Journal 
o/Child Language 11, 1-15. 
Boysson-Bardies, B. de, Vihman, M., Roug-Hellichius, L., Durand, C., 
Landberg, 1. & Arao, F. (1992): "Material evidence ofinfant selection from 
the target language: A cross-linguistic phonetic study", Phonological 
development: Models, research, implications, ed. C. Ferguson, L. Menn & 
C. Stoel-Gammon. Timonium, Maryland: York Press. 369-391. 
Brazil, D. (1975): Discourse Intonation. Birmingham: Birmingham University. 
Britain, D. (1992): "Linguistic change in intonation: The use of high rising 
terminals in New Zealand English", Language Variation and Change 4, 77-
104. 
Broselow, E. (1984): "An investigation of transfer in second language 
phonology", International Review 0/ Applied Linguistics 22, 253-269. 
Browman, C. & Goldstein, L. (1992): "Articulatory phonology: an overview". 
Phonetica 49,155-180. 
Brown, C. (2000): "The interrelation between speech perception and 
phonological acquisition from infant to adult", Second Language 
Acquisition and Linguistic Theory, ed. J. Archibald. Oxford: Blackwell. 4-
63. 
Brown, G. & Yule, G. (1983): Discourse Analysis. Cambridge: Cambridge 
University Press. 
Bybee, J. (2001): Phonology and Language Use. Cambridge: Cambridge 
University Press. 
Bybee, J. (2002): "Word frequency and context ofuse in the lexical diffusion of 
phonetically conditioned sound change", Language Variation and Change 
14,261-290. 
Byrd, D. & Saltzman, E. (1998): "Intragestural dynamics of multiple prosodie 
boundaries", Journal 0/ Phonetics 26, 173-199. 
Campbell, R. (1994): "Audiovisual speech: Where, what, when, how?", Current 
Psychology o/Cognition 13, 76-80. 
Campione, E. & Veronis, J. (2002): "A large-scale multilingual study of silent 
pause duration", Proceedings 0/ Speech Prosody 2002, ed. B. Bel & 1. 
Marlin. Aix-en-Provence : Laboratoire Parole et Langages. 199-202. 
Cherry, E. (1953): "Some experiments on the recognition of speech, with one 
and with two ears", Journal 0/ the Acoustical Society 0/ America 25(5), 
975-979. 
Cho, T. (2002): The effects 0/ prosody on articulation in English. New York: 
Routledge. 
Cho, T. (2005): "Prosodie strengthening and featural enhancement: Evidence 
from acoustic and articulatory realizations of la,il in English i', Journal 0/ 
the Acoustical Society o/America 117,3867-3878. 

References 
209 
Chomsky, N. & Halle, M. (1968): The sound pattern of English. New York: 
Harper & Row. 
Chun, D. (2002): Discourse Intonation in L2. Amsterdam: Benjamins. 
Clark, l, Yallop, C. & Fleteher, l (2007): An Introduction to Phonetics and 
Phonology. Oxford: Blackwell. 3rd edition. 
Clements, G. (1985): "The geometry of phonological features", Phonology 
Yearbook 2, 223-252. 
Couper-Kuhlen, E. (1993): English Speech Rhythm. Form and Function in 
Everyday VerbalInteraction. Amsterdam: John Benjamins. 
Cruttenden, A. (1997): Intonation. Cambridge: Cambridge University Press. 2nd 
edition. 
Crystal, D. (1969): Prosodie Systems and Intonation in English. Cambridge: 
Cambridge University Press. 
Crystal, D. (1981): Clinical Linguistics. Wien: Springer Verlag. 
Cucchiarini, C., Strik, H. & Boves, L. (2002): "Quantitative assessment of 
second language learners' fluency: comparisons between read and 
spontaneous speech", Journal of the Acoustical Society of America 111, 
2862-2873. 
D'Odorico, L. & Carubbi, S. (2003): "Prosodie characteristics of early multi-
word utterances in Italian children". First Language 23,97-116. 
Dauer, R. (1983): "Stress-timing and syllable-timing reanalysed", Journal of 
Phonetics 11, 51-62. 
Davenport, M. & Hannahs, S. (2005): Introducing Phonetics and Phonology. 
London: Hodder Arnold. 2nd edition. 
Delattre, P. (1981): "An acoustic and articulatory study of vowel reduction in 
four languages " , Studies in Comparative Phonetics, ed. P. Delattre. 
Heidelberg: Groos. 63-93. 
Delgutte, B. (1997): "Auditory neural processing of speech", The Handbook 01 
Phonetic Sciences, ed. W. Hardcastle & J. Laver. Oxford: Blackwell. 507-
538. 
Dziubalska-Kofaczyk, K. (1990): A Theory of Second Language Acquisition 
within the Framework ofNatural Phonology. Poznan: AMU Press. 
Eckert, H. & Barry, W. (2005): The Phonetics and Phonology 01 English 
Pronunciation. Trier: Wissenschaftlicher Verlag. 
Fabricius, A. (2002): "Ongoing change in modern RP", English World-Wide 23, 
115-136. 
Fant, G. (1960): Acoustic theory 01 speech production. The Hague: Mouton. 
Fant, G., Kruckenberg, A. & Nord, L. (1991): "Durational correlates of stress in 
Swedish, French and English", Journal ofPhonetics 19,351-365. 
Fee, E. (1995): "Segments and syllables in early language acquisition", 
Phonological Acquisition and Phonological Theory, ed. J. Archibald. 
Hillsdale, N. J.: Erlbaum. 43-61. 

210 
Chapter 7 
Flege, J. & Eefting, W. (1987): "Production and perception ofEnglish stops by 
native Spanish speakers", Journal ofPhonetics 15,67-83. 
Flege, J. & MacKay, I. (2004): "Perceiving vowels in a second language". 
Studies in Second Language Acquisition 26, 1-34. 
Flege, J. (1995): "Second language speech leaming theory, findings and 
problems", Speech perception and linguistic experience: Issues in cross-
linguistic research, ed. W. Strange. Timonium: York Press. 233-277. 
Fougeron, C. & Keating, P. (1997): "Articulatory strengthening at edges of 
prosodie domains", Journal of the Acoustical Society of America 101, 
3728-3740. 
Furrow, D. (1984): "Young children's use of prosody". Journal of Child 
Langzwge 11,203-213. 
Giegerich, H. (1992): English phonology. Cambridge: Cambridge University 
Press. 
Gimson, A. (1962): An Introduction to the Pronunciation of English. London: 
Edward Amold. 
Gordon, M. (2001): Small-town Values and Big-city Vowels: A Study of the 
Northern CWes Shift in Michigan. Durham, N.C.: Duke University 
Press. 
Grabe, E., Rosner, B., Garcfa-Albea, J. & Zhou, X. (2003): "Perception of 
English Intonation by English, Spanish, and Chinese Listeners", Language 
and Speech 4, 375-40l. 
Green, K., Kuhl, P. Meltzoff, A. & Stevens, E. (1991): "Integrating speech 
information across talkers, gender, and sensory modality: Female faces and 
male voices in the McGurk effect", Perception & Psychophysics 50, 524-
536. 
Grosser, W. (1997): "On the acquisition of tonal and accentual features of 
English by Austrian leamers", Second Language Speech - Structure and 
Process, ed. A. James & J. Leather. Berlin: Mouton de Gruyter. 211-228. 
Gut, U. (2000a): Bilingual acquisition ofintonation. Tübingen: Niemeyer. 
Gut, U. (2000b): "The phonetic production of emphasis by German leamers of 
English". Proceedings ofNew Sounds 2000, Amsterdam, 155-157. 
Gut, U. (2003): "Non-native speech rhythm in German". Proceedings of the 
ICPhS Conference, Barcelona, Spain, 2437-2440. 
Gut, U. (2005): "Nigerian English prosody", English World-Wide 26, 153-177. 
Gut, U. (2006): "Leamer speech corpora in language teaching", Corpus 
Technology and Language Pedagogy: New Resources, New Tools, New 
Methods, ed. S. Braun, K. Kohn & J. Mukherjee. Frankfurt: Lang. 69-86. 
Gut, U. (2007a): "Leamer corpora in second language acquisition research and 
teaching", Non-native Prosody. Phonetic Description and Teaching 
Practice, ed. J. Trouvain & U. Gut. Berlin: Mouton de Gruyter. 145-167. 

References 
211 
Gut, U. (2007b): "Sprachkorpora im Phonetikunterricht", Zeitschrift für 
interkulturellen Fremdsprachenunterricht 12(2). 
Hall, A. (1999): "Phonotactics and the prosodie structure of German ftmction 
words", Studies on the phonological word, ed. A. Hall & U. Kleinhenz. 
Amsterdam: John Benjamins. 99-l31. 
Halle, M. (1992): "Phonological features", International Encyclopaedia 0/ 
Linguistics, ed. W. Bright. Oxford: Oxford University Press. 207-212. 
Handbook 0/ the International Phonetic Association (1999). Cambridge: 
Cambridge University Press. 
Hansen Edwards, J. & Zampini, M. (eds.) (2008): Phonology and Second 
Language Acquisition. Amsterdam: John Benjamins. 
Hawkins, S. & Midgley, J. (2005): "Formant frequencies of RP monophthongs 
in four age groups of speakers", Journal 0/ the International Phonetic 
Association 35, 183-199. 
Herry, N. & Hirst, D. (2002): "Subjective and objective evaluation of the 
prosody of English spoken by French speakers: the contribution of 
computer assisted leaming", Proceedings 0/ Speech Prosody 2002, ed. B. 
Bel & r. Marlin. Aix-en-Provence : Laboratoire Parole et Langages. 383-
386. 
Hirst, D. (1998): "Intonation in British English", Intonation Systems, ed. D. 
Hirst & A. Di Cristo. Cambridge: Cambridge University Press. 56-77. 
Hogg, R. & McCully, C. (1987): Metrical Phonology: A Coursebook. 
Cambridge: Cambridge University Press. 
Ishikawa, K. (2002): "Syllabification of intervocalic consonants by English and 
Japanese speakers", Language and Speech 45,355-385. 
Jakobson, R. & Halle, M. (1956): Fundamentals 0/ Language. The Hague: 
Mouton. 
James, E. (1977): "The Acquisition of Second-Language Intonation Using a 
Visualizer". Canadian Modern Language Review 33,503-506. 
Jassem, W. & Gibbon, D. (1980): "Re-defining English accent and stress". 
Journal 0/ the International Phonetic Association 10, 2-16. 
Johnson, K. (1997): Acoustic and Auditory Phonetics. Oxford: Blackwell. 2nd 
edition. 
Jones, D. (2006): Cambridge English Pronunciation Dictionary. Cambridge: 
Cambridge University Press. 
Jonson, B. (1640): The English Grammar. Published by Menston: Scolar Press, 
1972. 
Kenstowicz, M. (1994): Phonology in generative grammar. Oxford: Blackwell. 
Kent, R. & Read, C. (2002): The acoustic analysis 0/ speech. Albany: Delmar, 
Thompson Learning. 2nd edition. 
Kent, R. (1983): "The segmental organization of speech", The Production 0/ 
Speech, ed. P. MacNeilage. New York: Springer. 57-90. 

212 
Chapter 7 
Kent, R. (1997): The Speech Sciences. San Diego: Singular Publishing Group. 
Keseling, G. (1992): "Pause and intonation contours in written and oral 
discourse" , Cooperating with Written Texts. 
The Pragmatics and 
Comprehension 0/ Written Texts, ed. D. Stein. Berlin: Mouton de Gruyter. 
31-66. 
Kingdon, R. (1958): The Groundwork 0/ English Intonation. London: Longman. 
Kortmann, B. & Schneider. E. (2004): A Handbook 0/ Varieties 0/ English. 
Volume 1: Phonology. Amsterdam: Mouton de Gruyter. 
Labov, W. (1989): "The child as the linguistic historian", Language Variation 
and Change 1, 85-97. 
Labov, W., Yaeger, M. & Steiner, R. (1972): A Quantitative Study 0/ Sound 
Change in Progress. Philadelphia: US Regional Survey. 
Ladefoged, P. & Maddieson, I. (1996): The Sounds o/the World's Languages. 
Oxford: Blackwell. 
Ladefoged, P. (1999): "American English", Handbook 0/ the International 
Phonetic Association. Cambridge: Cambridge University Press. 41-44. 
Ladefoged, P. (2001a): Vowels and Consonants. Oxford: Blackwell. 
Ladefoged, P. (2001b): A Course in Phonetics. Boston: Heinle & Heinle. 4th 
edition. 
Laeufer, C. (1996): "The acquisition of a complex phonological contrast: voice 
timing patterns of English final stops by native French speakers". 
Phonetica 53,117-142. 
Lass, R. (1984): Phonology: an introduction to basic concepts. Cambridge: 
Cambridge University Press. 
Laver, 1. (1994): Principles 0/ Phonetics. Cambridge: Cambridge University 
Press. 
Leather,1. & James, A. (1996): "Second language speech", Handbook 0/ Second 
Language Acquisition, ed. W. Ritchie & T. Bhatia. San Diego: Academic 
Press. 269-316. 
Lehiste, I. (1970): Suprasegmentals. Cambridge, Mass.: MIT Press. 
Lehiste, I. (1972): "The timing of utterances and linguistic boundaries", Journal 
o/the Acoustical Society 0/ America 51, 2018-2024. 
Lehman, M. & Swartz, B. (2000): "Electropalatographic and spectrographic 
descriptions of allophonic variants of /1/", Perceptual & Motor Skills 90, 
47-61. 
Levelt, W. (1989): Speaking: From Intention to Articulation. Cambridge, Mass: 
MIT Press. 
Liberman, A. & Mattingly, I. (1985): "The motor theory of speech revised", 
Cognition 21,1-36. 
Liberman, A., Cooper, F., Shankweiler, F. & Studdert-Kennedy, M. (1967): 
"Perception ofthe speech code", Psychological Review 74, 431-461. 

References 
213 
Lieberman, & Blumstein, S. (1988): Speech physiology, speech perception, and 
acoustic phonetics. Cambridge: Cambridge University Press. 
Lieberman, P. (1984): The biology and evolution o/language. Cambridge, 
Mass.: Harvard University Press. 
Lively, S., Pisoni, D. & Goldinger, S. (1994): "Spoken word recognition: 
research and theory", Handbook 0/ Psycholinguistics, ed. M. Gernsbacher. 
San Diego: Academic Press. 265-30l. 
Marslen-Wilson, W. (1987): "Functional parallelism in spoken word-
recognition", Cognition 25,71-102. 
McCarthy, J. (1988): "Feature geometry and dependency: a review", Phonetica 
45,84-108. 
McClelland, J. & Elman, J. (1986): "The TRACE model of speech perception", 
Cognitive Psychology 18, 1-86. 
McGurk, H. & J. McDonald (1976): "Hearing lips and seeing voices", Nature 
264, 746-748. 
Mehler, J., Jusczyk, P. Lambertz, G. Halsted, N. Bertoncini, J. & Amiel-Tison, 
C. (1988): "A precursor of language acquisition in young infants" , 
Cognition 29,143-178. 
Menn, L. & Stoel-Gammon, C. (1995): "Phonological development", The 
Handbook o/Child Language, ed. P. Fletcher & B. MacWhinney. Oxford: 
Blackwell. 335-359. 
Mennen, 1. (2007): "Phonological and phonetic influences in non-native 
intonation", Non-native Prosody. Phonetic Description and Teaching 
Practice, ed. 1. Trouvain & U. Gut. Berlin: Mouton de Gruyter. 53-76. 
Mertens, P. (2004): "The prosogram: semi-automatic transcription of prosody 
based on atonal perception model". Proceedings 0/ Speech Prosody 2004, 
Nara, Japan, 549-552. 
Mildner, V. & Liker, M. (2003): "Acoustic analysis of the speech of children 
with cochlear implants and comparison with hearing controls", Proceedings 
o/the International Congress 0/ Phonetic Sciences, Barcelona, 2377- 2380. 
Missaglia, F. (2007): "Prosodic training for adult Italian leamers of German: the 
Contrastive Prosody Method", Non-native Prosody. Phonetic Description 
and Teaching Practice, ed. J. Trouvain & U. Gut. Berlin: Mouton de 
Gruyter.237-258. 
Moore, B. (1997): "Auditory processing related to speech perception", The 
Handbook 0/ Phonetic Sciences, ed. W. Hardcastle & J. Laver. Oxford: 
Blackwell. 538-565. 
Moyer, A. (2004): Age, Accent and Experience in Second Language 
Acquisition: An Integrated Approach to Critical Period Inquiry. Clevedon: 
Multilingual Matters. 
Nespor, M. & Vogel, 1. (1986): Prosodie Phonology. Dordrecht: Foris. 

214 
Chapter 7 
Neu, H. (1980): "Ranking of constraints on /t,d/ deletion in American English", 
Locating Language in Time and Space, ed. W. Labov. New York: 
Academic Press. 37-54. 
Norris, D. (1994): "Shortlist: A connectionist model of continuous speech 
recognition", Cognition 52, 189-234. 
O'Connor, 1.D. & Arnold, G. (1961): Intonation olColloquial English. London: 
Longman. 2nd edition 1973. 
Palmer, H. (1922): English Intonation with Systematic Exercises. Cambridge: 
Heffer. 
Pierrehumbert, J. & Hirschberg, 1. (1990): "The meaning of intonational 
contours in discourse", Intentions in Communication, ed. P. Cohen, 1. 
Morgan & M. Pollack. Cambridge, Mass.: MIT Press. 271-311. 
Pisoni, D. & Lively, S. (1995): "Variability and invariance in speech perception: 
A new look at some old problems in perceptual learning", Speech 
perception and linguistic experience. Theoretical and methodological 
issues in cross-Ianguage speech research, ed. W. Strange. Timonium: York 
Press. 433-459. 
Radford, A. (1997): Syntax: a minimalist introduction. Cambridge: Cambridge 
University Press. 
Raffelsiefen, R. (2004): "Paradigm uniformity effects versus boundary effects", 
Paradigms in Phonological Theory, ed. L. Downing, T.A. Hall & R. 
Raffelsiefen. Oxford: Oxford University Press. 211-262. 
Ramus, F., Nespor, M. & Mehler, J. (1999): "Correlates of linguistic rhythm in 
the speech signal", Cognition 73, 265-292. 
Roach, P. (1982): "On the distinction between 'stress-timed' and 'syllable-timed' 
languages", Linguistic Controversies, Essays in Linguistic Theory and 
Practice, ed. D. Crystal. London: Edward Arnold. 73-79. 
Roach, P. (1991): English Phonetics and Phonology. Cambridge: Cambridge 
University Press. 2nd ed. 
Roach, P. (2004): "British English: Received Pronunciation.", Journal 01 the 
International Phonetic Association 34, 239-245. 
Rosenblum, L., M. Schmuckler & J. Johnson (1997): "The McGurk effect in 
infants", Perception & Psychophysics 59, 347-357. 
Roy, N., Merrill, R., Thibeault, S., Parsa, R., Gray, S. & Smith, E. (2004): 
"Prevalence of voice disorders in teachers and the general population", 
Journal olSpeech and Hearing Research 47 (2), 281-293. 
Rvachew, S. & Jamieson, D. (1995): "Learning new speech contrasts: Evidence 
from adults learning a second language and children with speech 
disorders ", Speech perception and linguistic experience. Theoretical and 
methodological issues in cross-language speech research, ed. W. Strange. 
Timonium: YorkPress. 379-410. 

References 
215 
Sampson, G. (1980): Schools oj linguistics: competition and evolution. London: 
Hutchinson. 
Samuel, A. (1981): "Phonemic restoration: insights from a new methodology", 
Journal ojExperimental Psychology: General, 110,474-494. 
Schmidt, A. & Flege, J. (1996): "Speaking rate effects on stops produced by 
Spanish and English monolinguals and SpanishlEnglish bilinguals". 
Phonetica 53, 162-179. 
Selkirk, E. (1986): "On derived domains in sentence phonology", Phonology 
Yearbook 3,371-405. 
Shaywitz, B., Shaywitz, S., Pugh, K., Constable, R., Skudlarski, P., Fulbright, 
R., Bronen, R., Fletcher, J., Shankweiler, D., Katz, L. & Gore, 1. (1995): 
"Sex differences in the functional organization of the brain for language", 
Nature 373,607-609. 
Sheldon, A. & Strange, W. (1982): "The acquisition of Ir! and /1/ by Japanese 
learners of English: Evidence that speech production can precede speech 
perception", Applied Psycholinguistics 3, 243-261. 
Silverman, K., Beckman, M., Pitrelli, J., Ostendorf, M., Wightman, C., 
Pierrehumbert, J. & Hirschberg, J. (1992): "ToB!: a standard for labeling 
English prosody", Proceedings, Second International Conjerence on 
Spoken Language Processing 2, Banff, Canada, 867-70. 
Stampe, D. (1979): A Dissertation on Natural Phonology. New York: Garland. 
Stevens, K. & Blumstein, S. (1981): "The search for invariant acoustic correlates 
ofphonetic features". Perspectives on the Study ojSpeech, ed. P. Eimas & 
1. Miller. Hillsdale, NJ: Erlbaum. 1-38. 
Stevens, K. & House, A. (1961): "An acoustical theory ofspeech production and 
some of its implications", Journal oj Speech and Hearing Research 4, 303-
320. 
Strange, W. (ed.) (1995): Speech perception and linguistic experience. 
Theoretical and methodological issues in cross-Ianguage speech research. 
Timonium: York Press. 
Strange, W. & Shafer, V. (2008): "Speech perception in second language 
learners: The re-education of selective perception", Phonology and Second 
Language Acquisition, ed. J. Hansen Edwards & M. Zampini. Amsterdam: 
John Benjamins. 153-191. 
Tench, P. (1996): "Intonation and the differentiation of syntactic patterns in 
English and German", International Journal oj Applied Linguistics 6, 223-
256. 
Trouvain, J. & Gut, U. (eds.) (2007): Non-native Prosody. Phonetic Description 
and Teaching Practice. Berlin: Mouton de Gruyter. 
Turk, A. & Shattuck-Hufnagel, S. (2000): "Word-boundary-related duration 
patterns in English", Journal oj Phonetics 28, 397-440. 

216 
Chapter 7 
Upton, C. (2004): "Received Pronunciation", A Handbook of Varieties of 
English, ed. B. Kortmann & E. Schneider. Berlin: Mouton. 217-230. 
Van Bezooijen, R. (1995): "Sociocultural aspects of pitch differences between 
Japanese and Dutch women", Language and Speech 38, 253-265. 
Vihman, M. (1996): Phonological development. Oxford: Blackwell. 
Warren, R. (1970): "Perceptual restoration of missing speech sounds", Science 
176,392-393. 
WeHs, 1. (2000): Longman Pronunciation dictionary. London: Longman. 
WeHs, 1. (2006): English Intonation. Cambridge: Cambridge University Press. 
WeHs, 1., Barry, W., Grice, M., Fourcin, A. & Gibbon, D. (1992): "Standard 
computer-compatible transcription" . Technical Report, SAM Stage Report 
Sen.3 SAM UCL-037. 
WeHs, 1.c. (1982): Accents ofEnglish. Cambridge: Cambridge University Press. 
Wennerstrom, A. (1994): "Intonational meaning in English discourse: a study of 
non-native speakers", Applied Linguistics 15,399-420. 
Wennerstrom, A. (1998): "Intonation as cohesion in academic discourse", 
Studies ofSecond Language Acquisition 20: 1-25. 
Wennerstrom, A. (2001): The Music of Everyday Speech. Oxford: Oxford 
University Press. 
Werker, J. & Pegg, 1. (1992): "Infant speech perception and phonological 
acquisition", Phonological development, ed. C. Ferguson, L. Menn & C. 
Stoel-Gammon. Timonium: York Press. 285-311. 
Werker, 1., Frost, P. & McGurk, H. (1992): "La langue et les levres: Cross-
language influences on bimodal speech perception", Canadian Journal of 
Psychology 46,551-568. 
Whitworth, N. (2003): "Prevocalic boundaries in the speech of German-English 
bilinguals", Proceedings of the 15th International Conference of Phonetic 
Sciences, Barcelona, 1093-1096. 
Wichmann, A. (2000): Intonation in Text and Discourse. London: Longman. 
Willems, N. (1982): English Intonationfrom a Dutch Point of View. Ambach: 
Intercontinental Graphics. 
Williams, B. & Hiller, S. (1994): "The question ofrandomness in English foot 
timing: a control experiment", Journal ofPhonetics 22, 423-439. 
Yang, L.-C. (2004): "Duration and pauses as cues to discourse boundaries in 
speech", Proceedings ofSpeech Prosody 2004, Nara, Japan. 267-270. 
Yava~, M. (2006): Applied English Phonology. Oxford: Blackwell. 
Zsiga, E. (2003): "Articulatory timing in a second language", Studies in Second 
Language Acquisition 25, 399-432. 

8 Index 
accent 84 
acoustic filter 145 
acoustic reflex 185 
acoustics 137 
articulation 27 
articulators 23 
active 23 
passive 23 
articulatory ge sture 
39 
articulatory system 13 
adaption 189 
aerometry 44 
affricate 27,30 
airstream 14 
egressive glottalic 14 
egressive pulmonic 14 
ingressive 
15 
allomorph 72 
allophone 51 
stylistic variants 51 
alveolar 26, 31 
alveolar ridge 25 
ambisyllabicity 82 
amplitude 140 
anacrusis 167 
analog 178 
antiformants 159 
approximant 30,159 
articulation 27 
articu1atory gesture 39 
articu1atory phonetics 6 
articulatory system 13 
arytenoid cartilages 17 
aspiration 50, 156 
assimilation 35 
audiogram 194 
audiometry 193 
auditory cortex 188 
auditory feedback 40 
auditory nerve 187 
auditory ossicles 184 
auditory phonetics 6 
auditory system 183 
internal 183 
peripheral 183 
autosegmental-metrical model 120 
babbling 41 
basilar membrane 186 
bilabial 31 
binary 71 
breath group 15 
breathy voice 22 
Broca's area 38 
bronchial tubes 14 
burst 156 
cardinal vowe1s 62 
categorical perception 196 
'clear l' 60 
clipping 178 
clitic 89 
coalescence 35 
coarticulation 34, 165 
anticipatory 34 
perseverative 34 
cochlea 185 
coda 75 
complementary distribution 51 
consonant 28,52 
manner of articulation 29 
place of articulation 29 
consonant cluster 77 
continuant 70 
creaky voice 22 
cricoid cartilage l7 
'dark I' 60 
decibel 140,193 

218 
declination 171 
defective distribution 77 
dental 26 
diacritics 
53, 56 
diaphragm 14 
digital 178 
diphthong 60 
centering 62 
c10sing 62 
discourse 106, 127 
ear canal 183 
eardrum 184 
eleetroeneephalography 46 
eleetromagnetie mid-sagittal 
articulography 45 
electromyography 47 
endoscope 45 
epenthesis 78 
Eustachian tube 185 
exhalation 15 
'extra-syllabie' 81 
fall 122 
fall-rise 123 
feature geometry 74 
feature matrix 71 
filter 
aeoustie 145 
high-pass 146 
low-pass 146 
final eonsonant cluster deletion 36 
final syllable lengthening 167 
flapping 59 
foeus 115 
braod 115 
narrow 
115 
foot 87 
forensie phoneties 173 
fonnant 147, 151 
transition 155 
fortis 29 
Fourier analysis 146 
free variation 51 
Chapter 8 
frequency 142 
frieative 27, 30, 160 
funetional magnetic resonance 
imaging 46 
fundamental frequency 144, 153, 
170 
General American 52 
given information 114 
glide 72 
glottal 31 
glottalization 56 
grapheme 9 
grapheme-to-phoneme relationship 
67 
harrnonies 144, 150 
head 118 
hearing aid 194 
hearing loss 187 
Hertz 142 
high rising terminal 123 
High-Amplitude Sucking technique 
200 
information 114 
new 114 
given 114 
inhalation 15 
inner ear 185 
intensity 140 
intercostal muscles 14 
interdental 31 
interlinear transeription 119 
internal auditory system 186 
International Phonetie Alphabet 53 
intonation 36, 106 
intonation language 20, 117 
intonation phrase 105, 106, 167 
'intrusive Irl' 78 
isochrony 87 
key 127 
labialisation 58 
labiodental 31 
labiovelar 32 

Index 
laryngograph 45 
laryngoscope 45 
larynx 16 
lateral 30 
lenis 29 
level pitch 118 
lexical access 199 
lexical stress 89 
linear prediction coefficient analysis 
146 
'linking Irl' 78 
lips 26 
rounded 29 
unrounded 29 
liquid 72 
lungs 14 
mandible 26 
maximum onset principle 82 
McGurk effect 200 
Mel 190 
metrical grid 95 
Metrical Phonology 94 
middle ear 184 
minimal pair test 51 
modes of phonation 21 
monophthong 60 
monophthongization 65 
motor cortex 38 
multisyllabic 82 
nasal 25, 30, 159 
nasal cavity 23 
nasal formant 159 
nasalisation 
25 
nasometer 44 
natural class 70 
Natural Phonology 98 
new information 114 
non-rhotic accent 
78 
notation symbol 8 
nucleus, intonation 111 
nuclear tone 118 
nucleus placement 111 
nucleus, syllable 75 
obstruent 30, 70 
offglide 156 
onglide 65,156 
onset 75 
Optimality Theory 202 
oral cavity 23 
oral sound 25 
organ of Corti 186 
orthographie word 88 
outer ear 183 
oval window 184 
palatal 31 
palate 25 
palatograph 46 
paratone 127 
parenthetical structures 109 
peripheral auditory system 182 
pharyngeal 31 
pharyngeal cavity 23 
phase 143 
phonation 17 
mo des of 21 
phonatory system 13, 16 
phone 50 
phoneme 50 
phoneme inventory 52 
phonetic context 50 
phonetic feature 71 
major-class 72 
manner features 72 
place feature 73 
phonetic transcription 
broad 68 
narrow 68 
phonetics 6 
acoustic 6 
articulatory 6 
auditory 6 
forensic 173 
phonological rule 49 
phonological word 88 
219 

220 
phonology 6 
segmental 7,49, 105 
suprasegmental 7,49, 105 
phonotactics 77 
pinna 183 
pitch 19, 116, 144, 191 
pitch accent 116 
pitch range 126, 171 
pitch resetting 167 
pitch tracking 170 
plosive 27,30 
burst 156 
closure 156 
glottalized 56 
prevoiced 159 
unreleased 50 
polysyllabic 82 
postalveolar 31 
power spectrum 146 
pre-head 118 
primary stress 90 
propagation of sound 138 
prosodie hierarchy 49, 105 
prosody 49, 105 
psychoacoustics 190, 194 
pure tone 142, 194 
quantization 178 
Received Pronunciation 52 
register 130 
resonance 24, 146 
resonator 145 
respiratory system 13 
resyllabification 83 
retroflex 31 
rhotic accent 78 
rhyme 75 
rib cage 14 
rise 122 
rise-fall 124 
rise-fall-rise 124 
SAMPA 70 
sampling rate 178 
schwa 84 
secondary stress 90 
semitones 171 
semi-vowel 28 
sibilant 160 
sine wave 142 
sone 192 
sonorant 30, 70 
sonority 80 
sonority scale 81 
sound source 149 
sound wave 139 
source-filter model 149 
speaking rate 35, 164 
speaking style 107 
spectrogram 146 
broadband 147 
narrowband 147 
speech act 121 
speech perception 195 
speech rhythm 87 
speech sounds 
nasal 25 
nasalized 25 
voiced 18 
voiceless 18 
stress 36, 83, 84, 90 
lexical 92 
primary 90 
secondary 90 
stress-shift 91 
variable 92 
stress-timing 87 
strident 160 
strong form 85 
Chapter 8 
stylised intonation pattern 126 
syllabic consonant 76 
syllabification 82 
syllable 75 
coda 75 
closed 77 
heavy 77 

Index 
lengthening 167 
light 77 
onset 75 
open 77 
rhyme 75 
stressed 83 
strong 85 
unstressed 83 
weak 85 
systems of speech organs 13 
tail 119 
tap 27,30 
teeth 25 
thresho1d ofhearing 190 
thyroid cartilage 1 7 
timbre 19,191 
ToB! 120 
tone 117, 171 
tone inventory 11 7, 118 
tone language 20, 117 
tongue 26 
back 26 
blade 26 
front 26 
root 26 
tip 26 
topic unit 107 
topicalisation 110 
trachea 14 
transcription 67 
interlinear 119 
phonemic 67 
phonetic 68 
trill 27,30 
triphthong 60 
trochaic pattern 97 
tune 125 
undershoot 35 
underspecification theory 73 
units of sound structure 49 
unreleased 50 
upspeak 123 
utterance 106 
uvula 25 
uvular 31 
velum 25 
vocal folds 17 
vibration 18 
vocal tract 23 
voice 18,21 
voice onset time 159, 195 
voiced 18 
voiceless 18, 21 
vowel 28,52 
back 29,61 
central 29,61 
chart 61 
deletion 36, 168 
front 29,61 
height 29,64 
lax 73 
low 29 
mid 29 
phonemic 1ength 64 
reduction 36, 168 
tense 73 
vowel chart 61 
vowel deletion 168 
waveform 139, 142 
aperiodic 149 
complex 142 
periodic 142 
simple 142 
sinusodia1 142 
weak and strong forms 85 
whisper 22 
word recognition 198 
221 

Textbooks in English Language and Linguistics (TELL) 
Edited by Joybrato Mukherjee and Magnus Huber 
Band 
1 
Ulrike Gut: Introduction to English Phonetics and Phonology. 2009. 
www.peterlang.de 

