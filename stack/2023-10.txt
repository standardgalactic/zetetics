royalsocietypublishing.org/journal/rspa
Research
Cite this article: Marantan A, Tolkova I,
Mahadevan L. 2023 Image cognition using
contour curvature statistics. Proc.R.Soc.A 479:
20220662.
https://doi.org/10.1098/rspa.2022.0662
Received: 20 October 2022
Accepted: 26 April 2023
Subject Areas:
applied mathematics, pattern recognition,
computational biology
Keywords:
image cognition, curvature, statistical
geometry
Author for correspondence:
L. Mahadevan
e-mail: lmahadev@g.harvard.edu
†These authors are first co-authors and
contributed equally to this study.
Electronic supplementary material is available
online at https://doi.org/10.6084/m9.figshare.
c.6644081.
Image cognition using contour
curvature statistics
Andrew Marantan1,†, Irina Tolkova2,† and
L. Mahadevan1,2,3
1Department of Physics, 2School of Engineering and Applied
Sciences, and 3Department of Organismic and Evolutionary Biology,
Harvard University, Cambridge, MA 02138, USA
IT, 0000-0002-2989-7283; LM, 0000-0002-5114-0519
Drawing on elementary invariance principles, we
propose
that
a
statistical
geometric
object,
the
probability distribution of the normalized contour
curvatures (NCC) in the intensity ﬁeld of a planar
image has the potential to categorize objects. We show
that NCC is sufﬁcient for discriminating between
cognitive categories such as animacy, size and type,
and demonstrate the robustness of this metric to
variation in illumination and viewpoint, consistent
with psychological experiments. A generative model
for producing artiﬁcial images with the observed
NCC distributions highlights the key features that
our metric captures, and those that it does not. More
broadly, our study points to the need for statistical
geometric approaches to cognition that build in both
the statistics and the natural invariances of the visual
world.
1. Introduction
Humans and other primates are adept at the visual
recognition of objects remarkably quickly and accurately,
forming neural representations in the inferotemporal
(IT) cortex that are spatially organized by cognitive
categories, such as animacy, size, type (faces versus
places),
etc.
How
this
happens
is
still
not
well
understood. There are at least two ways to address
this question: a bottom-up approach to understand the
neural areas and circuits responsible using systematic
microscopic studies, or a top-down approach for image
representation that agrees with neural activation patterns
in response to coarse-grained image features [1]. Of
course the ultimate goal is to merge these levels
of understanding into a uniﬁed whole that accounts
for the natural statistics of categories in the visual
2023 The Author(s) Published by the Royal Society. All rights reserved.
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

2
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
world, and links the development of the visual cortex together with the evolutionary necessity to
compress, classify and comprehend the external environment. At a high level, visual processing in
primates is roughly divided across the ventral and dorsal pathways, with the former responsible
for characterizing objects in the visual ﬁeld (what), and the latter for guiding interactions with the
objects (how) [2,3]. Within the ventral stream, object recognition is known to occur in the IT cortex
[4,5]. Studies of this region using fMRI in both humans and non-human primates in response
to viewing different stimuli have uncovered multiple cognitive categories or dimensions—such
as size (from small to large objects), animacy (from animals to inanimate objects), body parts
(such as faces, hands, bodies)—which elicit neural activation in different spatial domains of the
IT cortex [6–9]. At the neural level, these analyses have iteratively simpliﬁed stimuli to recover
‘critical features’ which maximally activate a cell [10,11]. In some regions of the visual pathway,
such features permit intuitive explanations—for instance, the middle temporal (MT) cortex and
middle superior temporal (MST) cortex are known to represent visual motion through a collection
of neurons encoding direction and speed [12,13]. Interestingly, within the intermediate subregions
of the ventral cortical pathway leading to the IT cortex—areas V1, V2 and V4—the critical features
associated with images of varying contours are found to represent both simple properties, such
as position and orientation, but also a higher-order property: curvature [14–17] and metrics based
on this quantity are strongly correlated with neural dynamics [18,19].
In psychology and psychophysics, the signiﬁcance of contour curvature in perception has been
alluded to at least since the 1950s [20], when it was suggested that perceptual information along
a shape boundary is not equally distributed, but concentrated in regions of high curvature—a
proposal that has since been supported both empirically and through an information-theoretic
framework [21,22]. This observation that a single contour can be sufﬁcient for recognition of
some objects [23] has led to a number of studies of contour perception in humans and primates,
e.g. hyper-acuity in the perception of curved lines [24–26], the near-ideal ability to detect contours
resembling those in natural images [27], the ability to detect closed contours more easily than open
contours [28] and the anti-correlation between the perception of closed contours with contour
complexity [29,30], etc. Additionally, the mechanisms for discriminating contour curvature are
selective for spatial frequency and orientation, agreeing very closely with studies of neural
features [31], consistent with the evidence for orientational selectivity in the visual cortex, and
the fact that curvature (i.e. the spatial rate of change of orientation) is a relatively easy feature to
extract. Yet there is a gap between the understanding of perception of contours and that of visual
ﬁelds, driving the need for a broader mathematical framework to extend the analysis of contours
to whole shapes and natural images [30].
In machine vision, while the notion of curvature has been used widely [32–37], most image
descriptors for object recognition and classiﬁcation tasks are composed of histograms of pixel-
based metrics—e.g. the histogram of oriented gradients (HoG) approach [38]. While several
studies have combined HoG with locally binned histograms of curvature to show improved
performance in numerical tasks, they use curvature only to augment existing methods [39,40],
rather than as a distinct metric. A promising alternative is that of curvature scale space
(CSS)—a representation of shape through contour curvature calculated at different magnitudes
of Gaussian smoothing [41] that has proved efﬁcient and successful in problems of corner
detection, clustering, shape indexing and retrieval, and silhouette-based object recognition
[42–45]. However, while CSS is rooted in and motivated by the mathematical invariances
desirable for shape analysis, it does not seem to have been extended to quantifying two-
dimensional images, or to our understanding of biological perception. Finally and most recently,
while neural networks have been successful at solving many problems in computer vision [46],
and are promising models of biological processing [47,48], they do not usually provide an
interpretable understanding of the intermediary image representations that underlie perception.
Given these insights from neurobiology, psychology and computer vision, how might one
construct a computationally meaningful and interpretable image descriptor? Here, guided by the
need for an invariant description to Euclidean motions, we propose the use of a simple statistical
geometric measure, the ‘normalized contour curvature’ (NCC) distributions: a probability
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

3
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
distribution of curvatures within smoothed natural images. We construct NCC through pooling
of nonlinear transformations of an image’s contour curvature content—a simple calculation
which has plausible implementability within neural circuitry, emphasizing that it is important
to think of a statistical measure of this geometric quantity given the noisy nature of images. We
show that NCC satisﬁes certain desired properties of shape characterization, and then use this
to interpret example stimuli, while demonstrating that this metric carries sufﬁcient information
content for distinguishing between cognitive categories. Finally, we derive a generative model for
constructing images corresponding to a given NCC distribution to help us understand when it
works, and perhaps more importantly, when it does not work.
2. Contour curvature computation
For the compression, classiﬁcation and comprehension of images treated as shapes, any
meaningful perception mechanism should satisfy some basic invariances that include (i)
Global Translation Invariance: i.e. shape is independent of location in space (ii) Global
Rotation Invariance: i.e. shape is independent of orientation (iii) Resizing/Scale Invariance:
i.e. shape is independent of overall scale (iv) Image Representation Invariance: i.e. shape is
independent of rescaling the intensity map. While there are known cognitive exceptions (such
as squares/diamonds, upside-down faces, etc.), these are specialized and we will not consider
them here.
For an image that is characterized by a two-dimensional intensity ﬁeld, constant intensity
contours typically form closed contours. In a smooth differential-geometric setting, the curvature
of these curves is invariant to global translation and rotation and thus forms a natural candidate
for an invariant description. Letting f(x, y) represent the intensity at pixel (x, y) and fx, fy, fxx, fyy
and fxy represent the ﬁrst and second derivatives, we can write the contour curvature (CC) at
point (x, y) as
κ =
2fx fy fxy −f 2
y fxx −f 2
x fyy
(f 2x + f 2y )3/2
.
(2.1)
We note that this approach is different from using the intensity values of a given image to describe
a height map and then compute the curvature tensor of the surface and thence the Gaussian
or mean curvature at each pixel [49], although the two are of course related. Furthermore, we
note that calculation of curvature follows naturally from orientational information that the retina
is well known to respond to; curvature is just the spatial variation in orientational information
and can be deduced approximately via a differencing scheme that is analogous to a difference
of Gaussians. Though the contour curvature is invariant under translation, rotation and intensity
scaling, it is not invariant to scale changes. To overcome this issue, we take the largest dimension
of an image to be of unit length, so that the curvature of a circle ﬁtting just inside the (square)
image has radius r = 1/2 and hence curvature κ = 2. Finally, it is numerically useful to map the
contour curvature deﬁned on the whole real line to a ﬁnite interval which we choose to be [−1, 1].
Thus, we deﬁne the normalized contour curvature (NCC) as
ˆκ =
κ
κ1/2 + |κ|,
(2.2)
The parameter κ1/2 sets the value of κ which maps to ˆκ = 1/2, and makes the NCC easier to
interpret; by taking κ1/2 = 2, a circle with curvature κ = 2 (i.e. a circle inscribed in a square image)
is mapped to ˆκ = 1/2. Note that this deﬁnition corresponds to a closed-form inverse relation.
In a discrete computational setting, pixel intensities can be used to construct level sets of
constant intensity and compute the curvature of these contours at every pixel. While rescaling
the intensity map does change the intensities of the contours, it does not change their overall
shape, and so this method is manifestly invariant under intensity rescaling. To get a smooth
surface interpolant from the image intensities, we ﬁlter the image using a Gaussian kernel that
has zero mean and a standard deviation ρ, and thus avoid the computational task of constructing
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

4
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
filter
NCC
bin
(a)
(b)
(c)
image intensity field
N
N
pixel = 1/N 1/N



–1.0
–0.5
0.5
1.0
0
κˆ
P(kˆ)
NCC distribution
length = 1
r = 1/2
κ   = 2
κˆ = 1/2
Figure 1. Definition of normalized contour curvature (NCC). (a) The NCC is defined (by equation (2.2)) such that its value along
acircleinscribedinasquareofsidelength1ismappedto1/2.(b)WhencalculatingNCC,weconsiderthelevelcurvesofathree-
dimensionalsurfacedefinedbythepixelintensitiesofanimage.Thisfigureshowsthelevelcurvesforthelightbulbimagein(c).
(c) This sequence shows the pipeline for calculating NCC. Starting with an image of a lightbulb, we apply Gaussian smoothing,
calculate NCC for each pixel following equation (2.2), and finally histogram the values to derive a probability density.
the contours passing through each pixel and calculate the contour curvature directly in terms of
numerical derivatives [50] of the ﬁltered image intensity, and directly apply equation (2.1). This
produces an ‘image’ of the contour curvature, which can be converted to the normalized contour
curvature via equation (2.2) (ﬁgure 1).
Finally, we use the normalized contour curvature image to construct a histogram for the
original image, which we then convert to a probability density to produce the NCC distribution.
We use equally spaced bins spanning from ˆκ = −1 to ˆκ = 1, choosing an odd number of bins in
order to have a bin centred on ˆκ = 0. In order to count only curvatures corresponding to the object
in the image, we ignore pixels corresponding to background elements. Altogether, this conceptual
framework and image processing pipeline is shown in ﬁgure 1, with additional details provided
in Section A of the electronic supplementary material.
3. Bayesian image classification of animacy and size
To evaluate the role of NCC as an image classiﬁer, we investigate whether the NCC metric
carries sufﬁcient information content for a simple binary classiﬁer to distinguish between
different cognitive categories. Our data were drawn from those used in fMRI studies on human
participants who were shown real images distinguished by features such as animacy and size [7]
that led to the fMRI-based detection of spatially localized responses in the IT cortex, and various
artiﬁcial image sets [51] titled ‘texforms’—which, while designed to be unrecognizable, preserved
enough low-level structure to elicit a similar neural response to the images from which they were
generated (ﬁgure 3).
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

5
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
–1.0
–0.5
0
0.5
1.0
0
1
2
large animate
large inanimate
0.5
1.0
1.5
small animate
small inanimate
P(κˆ)
P(κˆ)
0.5
1.0
P(κˆ)
0
1
2
P(κˆ)
κˆ
–1.0
–0.5
0
0.5
1.0
κˆ 
–1.0
–0.5
0
0.5
1.0
κˆ
–1.0
–0.5
0
0.5
1.0
κˆ
Figure 2. Computing NCC. NCC probability densities calculated for example images from the stimulus dataset presented in
[7], across four categories: a moose (large animate), mouse (small animate), telephone booth (large inanimate) and computer
mouse (small inanimate). Notice that the prevalence of straight lines in the intensity contours of the telephone booth and
computer mouse results in peaks near ˆκ = 0, and that both mice contain higher probability mass for intermediate positive
values (around ˆκ = 0.5). We find both of these features to be characteristic of animacy and size (figure 4).
P(animacy)
P(size)
2.0
1.5
animate
inanimate
large
small
1.0
0.5
0
–1.0
–0.5
0
0.5
1.0
P(κˆ)
κˆ
2.0
1.5
1.0
0.5
0
–1.0
–0.5
0
0.5
1.0
P(κˆ)
κˆ
Figure3. CognitivecategoriesofsizeandanimacyshowcharacteristicNCChistograms.ThesolidlinesindicateNCCprobability
densities for each of the four image categories from the stimulus dataset presented in [7], and the shaded region indicates
data that are within one standard deviation. We see that NCC for the inanimate category is characterized by high density at
ˆκ = 0(representingtheamountofstraightnessintheimage)andNCCforthesmallcategoryischaracterizedbyslightlyhigher
probability density in intermediate positive NCC values (around ˆκ = 0.5).
(a) Methods
For the natural images, the stimulus set contained 60 objects per each of four subcategories
[7]—small animate, small inanimate, large animate and large inanimate—for a total of 240 images.
The animate images spanned a large part of the phylogenetic tree of animals, including mammals,
reptiles, birds and ﬁsh; the inanimate objects featured everyday items varying from a thimble to
a ﬁretruck. In all images, objects were centred on a white background. Examples of images from
each category, together with their NCC, are shown in ﬁgure 2.
To classify a given image into one of two predeﬁned classes C1 or C2, we use a slightly modiﬁed
log-likelihood scheme that relies on the supervised learning of two probability distributions,
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

6
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
P(ˆκ |C1) and P(ˆκ |C2), representing the probability for a pixel in a given image to have normalized
curvature ˆκ given that it belongs to either C1 or C2. In practice, we bin the normalized
curvatures, and the aforementioned distributions become probability vectors: P(ˆκ |C1) = pC1 and
P(ˆκ |C2) = pC2, where the nth element pC1
n (or pC2
n ) describes the probability for ˆκ to be in the nth
bin.
To construct pC1 (or pC2), we simply calculate NCC histograms for all images in the C1 (or C2)
training set, add them together, and normalize by the total number of counts. Then, we can
classify an image by calculating its NCC distribution pn, dividing by the total number of counts
to obtain qn, and compute the log-likelihood:
L =
N

n
qn log

pC1
n
pC2
n

,
(3.1)
where N is the number of bins. If L > 0, the image is classiﬁed as belonging to C1; is L ≤0, it is
classiﬁed as belonging to C2. We note that L can also be thought of as the difference between the
Kullback–Leibler (KL) divergence of q from pC1 and the KL divergence of q from pC2. Although we
only use the sign of L for predicting the category of an image, the magnitude of L can inform the
classiﬁcation likelihood; large values of |L| are linked with a higher conﬁdence of classiﬁcation,
while values with |L| ∼0 have a lower conﬁdence.
In addition to analysing natural images, we also considered the paired image/texform
stimulus set introduced in [51], which contains a dataset of 120 objects (30 from each of the
four size/animacy sub-categories), along with corresponding texforms. We perform the same pre-
processing steps as described previously, with one difference in the processing of the background,
whereby we take advantage of the provided green-screen variant to isolate the background pixels
and set them to white to match prior analysis; applying this same ‘background mask’ introduces
an outline and background to the texform.
(b) Natural image classification
To test the efﬁcacy of the normalized contour curvature as an image statistic and visualize the
distinctions between classes, we compare the mean and variance of NCC distributions between
classes in ﬁgure 3, and separately for each sub-class in ﬁgure 4. We highlight two important
features. First, unlike animals, inanimate objects have a high prevalence of straight lines and
edges, leading to a peak at ˆκ = 0. Second, small objects contain a higher density of intermediate
curvature values, while the NCC of larger objects is more heavily concentrated at the ends
of the distribution. This is likely because the characteristics and details of small objects are
proportionately larger, resulting in lower absolute curvatures, while the ﬁne-scale detail of images
of large objects results in higher absolute curvatures.
To classify animacy over both large and small objects, and classify size on both animate
and inanimate objects, we ran 1000 randomized trials for both tasks, adhering to a 30%/70%
training/testing split. Our aggregate results are shown in ﬁgure 5. From the comparison of true
positive and false positive rates in ﬁgure 5, we see that these distinctions are sufﬁcient for a simple
Bayesian classiﬁer to distinguish animacy within both large and small objects, and size within
inanimate objects (with poorer performance on classifying size within animals). This general
tripartite organization—of small objects, animals and large objects—is consistent with studies
of neural activation within the occipito-temporal cortex for human observers [7].
The failure of the classiﬁer is clearly exposed via misclassiﬁed samples, i.e. false positives and
false negatives for each of the four classiﬁcation tasks are shown in ﬁgure 6. Interpreting these
mistakes in the NCC-based classiﬁer is illuminating. For instance, in classifying large objects by
animacy, most errors occur in inanimate objects with thin protruding ‘appendages’. This is likely
because our image processing approach causes the thin components to fade due to Gaussian
blurring, resulting in high magnitude curvatures at their endpoints, which align more closely
with the animate distributions than with characteristically (straight) curvature-free inanimate
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

7
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
P(large and animate)
P(large and inanimate)
P(small and animate)
P(small and inanimate)
2.0
1.5
1.0
0.5
0
–1.00 –0.75 –0.50 –0.25
0.25
0.50 0.75
1.00
0
P(κˆ )
κˆ
2.0
1.5
1.0
0.5
0
–1.00 –0.75 –0.50 –0.25
0.25
0.50 0.75
1.00
0
P(κˆ )
1.50
1.25
1.00
0.75
0.50
0
0.25
P(κˆ )
κˆ
–1.00 –0.75 –0.50 –0.25
0.25
0.50 0.75
1.00
0
κˆ
2.0
1.5
1.0
0.5
0
–1.00 –0.75 –0.50 –0.25
0.25
0.50 0.75
1.00
0
P(κˆ )
κˆ
Figure 4. Pairwise cognitive categories of size and animacy also show characteristic NCC histograms. Here, we consider
distributions for each of the four sub-categories of images from the stimulus dataset presented in [7], separated both by size
and by animacy. We observe similar characteristics as described in figure 3, though the distinction between large and small is
much more prominent for inanimate images, which is consistent with the tripartite cognitive organization found in [7].
distributions. When classifying small objects, most errors occur in inanimate objects, such as a
pinecone and ﬂoral-patterned household items, all of which have rounded features and thus have
a resemblance to animacy. On the other hand, the snail is a false negative, suggesting that the
shell is uncharacteristic of animate images. In this context, we ﬁnd that the most difﬁcult task is
distinguishing size within animate images: misclassiﬁed samples include both large animals with
the rounded shape usually associated with small objects, and small animals of more irregular and
elongated shapes. Inanimate objects can more successfully be separated by size, and the mistakes
often correspond to box-like large objects and elongated small objects.
All our results so far use a particular value of the Gaussian blurring ﬁlter to remove high-
frequency information, namely ρ = 0.04 and a training fraction of 30%. Classiﬁcation success rates
exhibit some dependence on ρ and a weak dependence on the training fraction; performance
suffers when ρ is very small (image noise dominates) and when ρ is very large (image details
begin to be entirely washed out), but the success rates are not sensitive to small changes in ρ near
the optimally performing value (see electronic supplementary material, Section B, and ﬁgures S2,
S3 and S4 for visualizations of these dependencies).
(c) Texform image classification
Images of natural objects are distinguished by their correlation statistics. Texforms attempt to
scramble these [51] and thus serve as a test of our binary Bayesian classiﬁer for distinguishing
animacy and size. In ﬁgure 7, we show that texforms preserve similar features as natural images,
and moreover, have very similar classiﬁcation results. The top row of ﬁgure 7 shows the NCC
distributions for texforms across both animacy and size. While texforms do not retain the large
straight or ﬂat regions characteristic of inanimate images, we do see slightly higher zero-curvature
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

8
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.0
0.8
0.6
true ‘animate’ frequency
false ‘animate’ frequency
0.4
0.2
0
0
0.2
0.4
0.6
0.8
1.0
P(animate  large)
1.0
0.8
0.6
true ‘large’ frequency
false ‘large’ frequency
0.4
0.2
0
0
0.2
0.4
0.6
0.8
1.0
P(large  animate)
1.0
0.8
0.6
true ‘large’ frequency
false ‘large’ frequency
0.4
0.2
0
0
0.2
0.4
0.6
0.8
1.0
P(large  inanimate)
1.0
0.8
0.6
true ‘animate’ frequency
false ‘animate’ frequency
0.4
0.2
0
0
0.2
0.4
0.6
0.8
1.0
P(animate  small)
Figure 5. Performance of a Bayesian classifier based on NCC. We examine the accuracy of a binary classifier (equation (3.1))
by visualizing two-dimensional histograms of the false positive rate against true positive rate over 1000 randomized trials.
Classification is performed separately across the four categories for the stimulus dataset presented in [7]; for instance, the top
right histogram shows results for binary classification tasks in which images with large objects are classified as animate or
inanimate. We find that it is significantly more difficult to differentiate large animate and small animate images, while the
other categories have high classification accuracy. These results are consistent with the tripartite domain separation observed
in [7] across large inanimate, small inanimate and animate objects.
content in the inanimate distribution, and ﬁnd that this difference is sufﬁcient for classifying
animate from inanimate texforms with relatively high conﬁdence. In addition, the primary
difference between large and small texforms is in intermediate positive curvatures, as in natural
images. The bottom row of ﬁgure 7 shows the true positive/false positive results from 1000 trials
of a Bayesian classiﬁer (see electronic supplementary material, Section B, and ﬁgure S5 for results
across all four animacy/size subcategories).
Overall, we see that the accuracy of the classiﬁer for texforms is very similar to those of natural
images; in fact, two of the categories yield marginally higher results. Additionally, ﬁgure 8 shows
false positive and false negative samples across each of the four categories. When classifying
samples by animacy, we ﬁnd that some images with considerable straight regions—such as the
bull and bird—are confused for inanimate objects, while some highly textured samples—such as
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

9
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
P(animate | large) misclassifications
P(large | animate) misclassifications
false negative
false positive
false negative
false positive
false negative
false positive
false negative
false positive
P(animate | small) misclassifications
P(large | inanimate) misclassifications
Figure 6. Classification errors using NCC are informative about cognitive categories of images. To understand the sources
of error within our classifier (equation (3.1)), we consider and interpret misclassified images. Specifically, we show a false
negativeandafalsepositivesampleforeachofthefourclassificationtasks.Wefindthatthesemistakesareconsistentwithour
observed differences in large/small and animate/inanimate NCC distributions. Since small objects are characterized by a peak
in intermediate positive curvature, large objects are often misclassified when they have a rounded form, and small objects are
often misclassified when comprising an elongated or irregular form. On the other hand, inanimate objects can be misclassified
due to textured structure or when containing ‘appendages’. Animate objects are rarely misclassified as inanimate.
the stroller and wreath—are confused for animals. On the other hand, the samples mistakenly
classiﬁed as small—such as the seal and armchair—are very rounded, contributing intermediate,
positive curvatures, while the samples mistakenly classiﬁed as large—such as the Chihuahua and
mouse running wheel—have a disproportionate amount of high or near-zero curvatures for their
category.
4. Normalized contour curvatures distribution as an image feature
Our results so far suggest that NCC statistics are consistent with experimental observations
of cognitive categories across size and animacy, and thus raise the question of its use as an
image feature in other downstream neural classiﬁcation tasks. Complementing fMRI studies with
human subjects, studies on macaque monkeys that directly measure neural activity in the brain
have shown spatial localization and distinct topographical ordering of the neural responses in
the visual cortex [52]. The results show that data types such as alpha-numeric characters in
the Helvetica font, Tetris-like shapes and simple cartoon faces [52], each of which are clearly
distinguished by different geometric statistics, as shown in ﬁgure 9a,b are topographically ordered
within distinctive regions in the IT cortex. This raises the question: can normalized contour
curvature reveal any structure across these categories? In line with standard image classiﬁcation
approaches, we can think of the calculated NCC distribution as a ‘feature vector’ representing
an image. Thus, the NCC distribution can be combined with any other image features (such as
average intensity, colour information, etc.) by concatenation into a larger feature vector.
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

10
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.0
1.5
1.0
0.5
0
–1.0
–0.5
0.5
1.0
0
P(κˆ )
κˆ
animate
inanimate
2.0
1.5
1.0
0.5
0
–1.0
–0.5
0.5
1.0
0
P(κˆ )
κˆ
large
small
P(animacy)
P(animacy)
P(size)
P(size)
1.0
0.8
0.6
true ‘animate’ frequency
false ‘animate’ frequency
0.4
0.2
0
0
0.2
0.4
0.6
0.8
1.0
1.0
0.8
0.6
true ‘large’ frequency
false ‘large’ frequency
0.4
0.2
0
0
0.2
0.4
0.6
0.8
1.0
Figure7. CognitivecategoriesoftexformsshowcharacteristicNCChistograms.Similartofigure3,thetoptwosubfiguresshow
the mean ± s.d. of the NCC distributions for texforms corresponding to images of animate/inanimate and large/small objects
(adapted from [51]). The bottom two subfigures show the corresponding histograms of true positives and false positives from
1000 runs of a binary classifier (equation (3.1)).
(a) Methods
To visualize the relative clustering of NCC distributions across the different image categories,
we project the features to a lower-dimensional space by applying principal component analysis
(PCA). We ﬁrst construct a data matrix in which each column corresponds to an NCC
distribution for a speciﬁc image, and calculate its singular value decomposition (SVD). Then,
any NCC distribution can be approximated as a linear combination of orthonormal basis
vectors (components). As we will see, the ﬁrst three principal components have a natural
geometric interpretation, and the projections of the data vectors into the three-dimensional space
corresponding to these components allows us to evaluate separability across categories using the
NCC as a feature vector.
(b) Results
In calculating the NCC for these, we use a relative ﬁlter size of ρ = 0.018 (as for the analysis of
other images, the choice of this parameter does not change our results qualitatively). In ﬁgure 9c,
we see clear structure in the principal components. In particular, the ﬁrst component, which
captures 89% of the variation in the data, represents a simple peak at zero curvature, therefore
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

11
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
P(animate | large) misclassifications
P(large | animate) misclassifications
false negative
false positive
false negative
false positive
false negative
false positive
false negative
false positive
P(animate | small) misclassifications
P(large | inanimate) misclassifications
Figure 8. Classification errors using NCC are informative about cognitive categories of texforms. Following our analysis of
natural images, we examine the errors made by a Bayesian classifier over the texform dataset (adapted from [51]). We find
similarqualitativecharacteristicsinmisclassifiedsamplestothosedescribedinfigure6:roundlargeobjectsmaybemisclassified
assmall;inanimateobjectswithtexturedfeaturesmaybemisclassifiedasanimate;animalsaremoredifficulttoclassifybysize.
describing the amount of ‘straightness’ in the data. The second component can be interpreted to
represent concave content—from regions of the intensity surface which contain both positively
and negatively curved contours—and the third component can be interpreted to represent
convex content—which contains positive but not negative curvature content. This observation
agrees with the ﬁnding that convexity and concavity drive differential neural responses to visual
stimuli [53].
Visualizing the projections of the images onto the ﬁrst three principal components shown
in ﬁgure 9d, we see that the Tetris pieces with straight sides and corners cluster tightly due to
a high contribution from the ﬁrst component. Similarly, the cartoon faces also cluster tightly
due to their distinct structure—large positive curvatures (due to the round face), with small
amounts of negative and zero-curvature content due to the eyes, nose and other facial features.
Finally, the Helvetica glyphs span the space between the latter two categories, as they contain
a varied combination of straight rectangular portions and curved segments. In particular, a
few are structurally very similar to ‘Tetris pieces’ (such as the letters ‘I’, ‘L’ and ‘H’). This
ordering is particularly evident when we project the images onto the ﬁrst two singular vectors;
in fact, it roughly matches the topographical organization observed in the brain (see [52], ﬁg. 3).
Overall, we see that using the NCC statistics as a feature vector allows for linear separability
of image classes in terms of a few geometrically interpretable principal components. We note
that NCC permits interpretable geometric explanation for qualitative dimensions of ‘animate–
inanimate’ and ‘stubby–spiky’ discovered by an artiﬁcial neural network in [54]: animate–
inanimate distinctions are deﬁned by zero-curvature content, while stubby–spiky distinctions are
deﬁned through high-curvature content.
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

12
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
(b)
v
–1.0
–0.5
0
0.5
1.0
–0.25
0
0.25
0.50
0.75
1.00
first three principal components
Component 1
Component 2
Component 3
(a)
(c)
(d)
Component 1
Component 2
Component 3
weights for first three principal components
v
faces
helvetica
tetris
P(κˆ )
κˆ
Figure 9. NCC provides an interpretation of topographical ordering of image categories in monkey IT cortex. (a) Images of
Tetris, Helvetica and cartoon face stimuli used by Srihasam etal. [52] to demonstrate proto-structure of the macaque IT cortex.
(b) Spatial structure of neural activation measured in macaques in response to these stimuli, with cartoon faces in cyan,
Helveticafontsinblue,Tetrismotifsingreenandmonkeyfacesinred(bySrihasametal.[52]).(c)Weapplyprincipalcomponent
analysis of the NCC distributions across all stimuli. The first three components yield interpretation: Component 1 represents
the amount of zero-curvature content, or straightness, Component 2 represents relative concave content and Component 3
representsrelativeconvexcontent.(d)Byvisualizingthethree-dimensionalspaceofPCAcoefficientscorrespondingtothefirst
three components, we see clustering across categories consistent with the neural activation shown in (b).
5. Normalized contour curvatures as a classifier under varying illumination
and viewpoint
Robust cognitive classiﬁcation based on vision not only needs to be invariant to transformations
of scaling, translation and rotation, but also to variations in illumination and viewpoint. In the
current context, we can ask if NCC-statistics based classiﬁers are robust to these variations. While
it is difﬁcult to provide theoretical guarantees, we performed an experimental evaluation by
quantifying the variation in NCC across illumination/viewpoint changes for individual objects,
and comparing against inter-object variation using the Amsterdam Library of Object Images
(ALOI) [55]. The ALOI is a dataset of 1000 household items, systematically photographed to vary
viewing angle, lighting direction and lighting colour, for a total of 24 varied lighting conditions
and 72 varied viewpoints, samples from which are shown in ﬁgure 10a, c.
(a) Methods
For our analysis, we choose six example object categories from the ALOI dataset, and randomly
sample 15 images of varying lighting and viewpoint per object. To measure the similarity between
probability distributions, we calculate the Jenson–Shannon divergence—a symmetric and smooth
extension of the KL divergence—between NCC values for each pair of images. We then perform
a multi-dimensional scaling (MDS) analysis to ﬁnd a two-dimensional mapping which best
preserves the ‘distances’ between every pair of image samples, allowing us to visually evaluate
inter- and intra-object similarity for varying illumination and viewpoint conditions.
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

13
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
shoe
spool
cat
clock
birdie
shell
illumination
viewpoint
–0.10
–0.05
0
0.05
0.10
0.15
Dimension 1
–0.10
–0.05
0
0.05
0.10
0.15
Dimension 1
–0.15
(a)
(b)
(c)
(d)
–0.10
–0.05
0
0.05
0.10
0.15
0.20
Dimension 2
–0.15
–0.10
–0.05
0
0.05
0.10
0.15
0.20
Dimension 2
viewpoint
illumination
Figure 10. Robustness of NCC to viewpoint and illumination. We examine robustness of NCC to two additional perceptual
factors—viewpoint and illumination—by analysing images of household objects taken under varying conditions from the
Amsterdam Library of Object Images (ALOI) [55]. We consider six objects: a shoe, spool of thread, cat statue, small clock,
tennis shuttlecock/birdie and a seashell. After calculating NCC for images of the objects taken at varying conditions (varying
viewpointinthetoprow,orilluminationinthebottomrow),weprojecttheNCCdistributionstotwodimensionsthroughmulti-
dimensional scaling (MDS). The relative clustering of points in two dimensions can be considered to be a representation of the
similarityoftheNCCdistributionsforagivenobject.Forinstance,intheanalysisofviewpointvariation,wefindthebirdie(black)
andthespoolofthread(green)haveaverytightlyclustereddistribution,whileobjectssuchastheclock(cyan)andtheseashell
(purple) do not. This is understandable, as the former pair are radially symmetric, while the latter pair are difficult to recognize
from the back. In the analysis of illumination variation, we find that reflective objects (such as the clock and shell) once again
havemorevarianceinthedistributions,whileobjectswithamoremattetexture(suchastheshoeandcat)havemoreconsistent
NCC.
(b) Results
In ﬁgure 10b, d, we show that the two-dimensional representations derived from NCC
distributions represent consistent, and almost separable, clustering of images within object
categories for variations in viewpoint and illumination. This is notable, as NCC is a simple
metric which retains no correlative spatial information for an image. For example, in the case
of illumination, there is larger variability for the seashell—for which some illumination angles
cause part of the shell to fall into deep shadow, making it difﬁcult to recognize—and the clock—
in which the light casts shadows through the glass surface. In the case of changing viewpoint,
we see very low variability within the spool of string and the shuttlecock, as these are radially
symmetric, and higher variability in objects that look very different from behind, such as the cat
and clock. All together, our results show that NCC is relatively robust to changes in illumination
and viewpoint. Additionally, the spread of the clusters reﬂect sources of error that may be similar
to those of human observers.
6. A generative model for normalized contour curvatures distributions
Our use of the normalized contour curvature distribution as an cognitive classiﬁer has highlighted
how its invariance properties allow for an interpretable differentiation between image categories.
We now turn to ask if it possible to create a generative model for constructing artiﬁcial images
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

14
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
with speciﬁed NCC statistics related to known cognitive categories. To do so, we consider the
theoretical distributions for the curvature statistics of a spatially varying Gaussian ﬁeld with a
correlation length distribution corresponding to a given NCC probability density, determined
through an optimization procedure. This allows us to generate artiﬁcial images patch-by-patch
with sampled correlation lengths, and thus compare the empirical, theoretical and generated NCC
statistics (see electronic supplementary material, Section F for more details on Gaussian random
ﬁelds).
(a) Methods
We start with the assumption that Gaussian-ﬁltered natural images can be roughly described
in terms of distinct local patches drawn from Gaussian-correlated Gaussian ﬁelds with different
correlation lengths ξ. Formally, this can be expressed as
P(ˆκ) =
 ∞
0
dξP(ˆκ | ξ) P(ξ),
(6.1)
where P(ˆκ) is the NCC distribution, P(ξ) is the correlation length distribution for a given cognitive
category (e.g. animate), and P(ˆκ | ξ) represents the single-pixel distribution for the NCC given
that the local patch is described by a Gaussian-correlated Gaussian ﬁeld with correlation length
ξ. In discrete form, the probability distribution P(ˆκ) becomes the probability vector pˆκ
model, P(ξ)
becomes pξ, and P(ˆκ | ξ) becomes Ξ, a matrix whose columns represent the NCC distribution for
a Gaussian ﬁeld with the corresponding correlation length ξ, such that
pˆκ
model = Ξ pξ.
(6.2)
Formally, the elements of Ξ can be written in terms of the conditional cumulative distribution
function C(ˆκ |ξ) (see electronic supplementary material, Section G):
Ξnm = C(ˆκn+1 | ξm) −C(ˆκn | ξm).
(6.3)
Then, we ﬁt the correlation length probability vector pξ by taking it to be the probability vector
minimizing the Kullback–Leibler divergence of the model distribution pˆκ
model from the measured
distribution pˆκ
data,
pξ
ﬁt = argmin
pξ
DKL(pˆκ
data|| pˆκ
model).
(6.4)
Note that this is a convex problem, which can be computed with standard optimization solvers
(we used the ‘cvxpy’ toolbox). Once we extract pξ
ﬁt, we can start generating artiﬁcial images. We
ﬁrst divide the image into patches of a chosen size. Then, for each patch, we draw a correlation
length ξ from pξ
ﬁt, and ﬁll in pixel values from a Gaussian-correlated Gaussian ﬁeld with this
correlation length, conditioning on pixels in the neighbouring frames to maintain continuity
across patches. Finally, once all panels have been ﬁlled in, we threshold the image, ignoring all
pixels with intensities less than −σf (where σf is the standard deviation of the image).
(b) Results
In ﬁgure 11a, we show the ﬁt for the correlation length distribution and see that it has a
sparse three-peak structure. The ﬁrst peak, occurring close to ξ = 0, is a boundary artefact,
as calculated NCC distributions will have non-zero densities in the very ﬁrst and last bin,
which is theoretically impossible with nonzero correlation length. The second peak captures
the majority of the distribution, while the last peak contributes to the bump at intermediate
positive curvature values (around [0.5, 0.75])—which, as we have argued earlier, is indicative
of the circular characteristics of small objects. The relative weights associated with the peaks in
the correlation length distribution allow P(ξ) to effectively capture the differences across both
dimensions of animacy and size. Inanimate images have an additional peak at the maximum
normalized correlation length, corresponding to a peak at ˆκ = 0, while large images have higher
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

15
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
second peaks relative to third peaks, indicating greater probability density at the edges of the
NCC distribution (see electronic supplementary material, Section E ﬁgure S7, for additional
intuition and analysis of the structure of P(ξ)). In ﬁgure 11b, we show that there is a good
match between the NCC distribution of images from a category (e.g. animate), the theoretical
distribution computed using the above optimization procedure, and the NCC of the images
created using the generative algorithm. However, when the intensity ﬁelds associated with the
generative approach are visualized, as shown in ﬁgure 11c, we note that they do not correspond to
either natural images or even texforms. This is because the simple generative procedure captures
only local—not global—spatial correlations, leading to generated images that exhibit smooth
characteristics with irregular global geometric and topological structure, but do not represent
individual objects.
7. Discussion
There is a need for simple, interpretable, properly invariant metrics to describe cognitive
categorization and thereby illuminate the underlying mechanisms in both artiﬁcial neural
networks and the neural processing behind human cognition. Our use of the normalized
contour curvature distribution aims to rejuvenate an old geometric idea, but in a probabilistic
setting, recognizing that we need a metric that is invariant to the Euclidean group of rotations,
translations, and intensity scaling, while accounting for statistical variability within and across
images. We have shown that a simple metric based on the NCC distribution is consistent
with multiple experimental ﬁndings in humans and monkeys: it can distinguish between
cognitive categories, is robust to changes in lighting conditions and viewpoint, and has plausible
implementability in neural circuitry given that curvature has roots in characterizing orientational
information that can be pooled.
Though the origins of neural networks are deeply rooted in neurobiology, many recent
computational methods have diverged from their biological motivations, inspiring questions
about how to reconnect these disciplines [56–58] in both a biological context, as well as in
the development and reﬁnement of computer vision. For instance, encoding transformation
invariances within network architectures has been shown to improve performance and sample
complexity in some computer vision tasks [59,60]. Moreover, curvature has been incorporated
into deep networks for recognition of three-dimensional objects [61]. Additionally, while efforts
to understand and interpret trained deep learning systems are still underway [62,63], a number
of studies point to the signiﬁcance of curvature, e.g. convolutional neural networks have revealed
tuning of neurons to boundary curvature within simple images [64], curvature-driven mid-level
network structure [65] and differences in the perception of curvature between human vision and
computer vision [66].
Our study of biologically plausible image representations using normalized contour curvature
distributions is a step in the direction of improving interpretability using a probabilistic
framework for a simple geometric object. By considering intensity level set contours within an
object, NCC extends the study of boundary contour stimuli to consider perception of natural
images. We note that the curvature of internal contours is likely to be correlated with that
of the boundary contour—but there are also many examples where this is not the case. For
instance, within the illumination dataset demonstrated in ﬁgure 10, the outlines of some objects
would blend into the background and therefore not be well deﬁned, but the objects are still
identiﬁable by visible internal structure. On the other hand, while the boundary of the ﬂoral-
patterned cup in ﬁgure 6, top right has low curvature associated with inanimacy, the internal
texture likely contributes to its misclassiﬁcation as an animate object. Further work could consider
the integration of NCC within network architecture, examine correlations of input NCC with
corresponding layer activation to understand primitives learned by networks, and assess the
consistency of cognitive categorization between NCC and deep learning.
One limitation of the proposed NCC distribution as a metric is that it does not account
for the spatial distributions of curvature content. When perceiving complex natural scenes,
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

16
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0
0.2
0.4
0.6
0.8
1.0
scaled correlation length ξ
0
10
20
30
40
50
60
 probability density P(ξ)
correlation length distribution for generative model
–1.00 –0.75 –0.50 –0.25
0
0.25
0.50
0.75
1.00
normalized contour curvature kˆ
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
probability density P(kˆ)
validation of NCC statistics of generative model
generated images
theoretical distribution
true animate distribution
(a)
(b)
(c)
Figure 11. Generative model for NCC distributions. (a) Correlation length probability distribution for the animate image class
extracted according to equation (6.4). (b) Comparison of animate NCC distribution, the model fit pA
ﬁt from equation (6.2) and
the NCC distribution calculated from 100 thresholded (at an intensity of −σf) artificial images. (c) Three images generated by
constructing a Gaussian-correlated Gaussian random field with correlation length drawn from the distribution in (a).
or even more detailed objects, we clearly rely on non-local and/or higher-order statistics of
the visual ﬁeld [67]. To account for this, future development of this metric could integrate
NCC with a pyramid framework, in which a feature descriptor is applied at varying locations
and scales [68]. However, it has also been shown that the neural response to an image is not
distinct. By estimating the size of receptive ﬁelds it is possible to construct artiﬁcial images with
distorted peripheral intensities which are perceptually indistinguishable for a human observer,
known as metamers [69], as well as alternative artiﬁcial stimuli, such as texforms [51,70]. Both
metamers and texforms preserve some (scale-dependent) measures of spatial correlations in
contour curvature, and may be analogous to cubist, pointillist and other styles of representation
in art. Our analysis is at one extreme of the coarse-graining of spatial information and suggests
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

17
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
that local pooling of neural activation is an important aspect of image processing, consistent
with results that the NCC distributions of modiﬁed texforms contain similar deﬁning curvature
characteristics across animacy and size as in natural images. Finally, our generative model
for NCC highlights a particularly difﬁcult problem by failing to capture global geometric and
topological considerations in images; how we might augment a statistical-geometric approach
such as the one used here with ideas from integral geometry [71] remains an open question.
Data accessibility. The code and materials are available from the Zenodo digital repository: https://zenodo.org/
badge/latestdoi/339746850 [72].
The data are provided in electronic supplementary material [73].
Authors’contributions. A.M.: conceptualization, formal analysis, investigation, methodology, software, validation,
visualization, writing—original draft; I.T.: data curation, formal analysis, investigation, methodology,
software, validation, visualization, writing—original draft, writing—review and editing; L.M.: conceptualization,
formal analysis, funding acquisition, methodology, project administration, resources, supervision, validation,
writing—original draft, writing—review and editing.
All authors gave ﬁnal approval for publication and agreed to be held accountable for the work performed
therein.
Conflict of interest declaration. We declare we have no competing interests.
Funding. This work was supported in part by the US National Science Foundation (grant no. DMS-1764269 to
A.M., I.T. and L.M.), the Simons Foundation (L.M.) and the Henri Seydoux Fund (L.M.).
Acknowledgements. We thank Margaret Livingstone for asking a question that launched this study, and Talia
Konkle for providing us with the animate/inanimate and large/small image sets and both of them for
discussions.
References
1. Tsao
DY,
Cadieu
C,
Livingstone
MS.
2012
Object
recognition:
physiological
and
computational insights. In Primate neuroethology (eds ML Platt, AA Ghazanfar), ch. 24. Oxford,
UK: Oxford University Press.
2. Goodale MA, Milner AD. 1992 Separate visual pathways for perception and action. Trends
Neurosci. 15, 20–25. (doi:10.1016/0166-2236(92)90344-8)
3. Ungerleider LG, Haxby JV. 1994 ‘what’ and ‘where’ in the human brain. Curr. Opin Neurobiol.
4, 157–165. (doi:10.1016/0959-4388(94)90066-3)
4. Gross CG. 1973 Visual functions of inferotemporal cortex. In Visual centers in the brain (ed. CG
Gross), pp. 451–482. Berlin, Germany: Springer.
5. Tanaka K. 1996 Inferotemporal cortex and object vision. Annu. Rev. Neurosci. 19, 109–139.
(doi:10.1146/annurev.ne.19.030196.000545)
6. Kiani R, Esteky H, Mirpour K, Tanaka K. 2007 Object category structure in response patterns
of neuronal population in monkey inferior temporal cortex. J. Neurophysiol. 97, 4296–4309.
(doi:10.1152/jn.00024.2007)
7. Konkle T, Caramazza A. 2013 Tripartite organization of the ventral stream by animacy and
object size. J. Neurosci. 33, 10 235–10 242. (doi:10.1523/JNEUROSCI.0983-13.2013)
8. Kriegeskorte N, Mur M, Ruff DA, Kiani R, Bodurka J, Esteky H, Tanaka K, Bandettini PA. 2008
Matching categorical object representations in inferior temporal cortex of man and monkey.
Neuron 60, 1126–1141. (doi:10.1016/j.neuron.2008.10.043)
9. Konkle T, Oliva A. 2012 A real-world size organization of object responses in occipitotemporal
cortex. Neuron 74, 1114–1124. (doi:10.1016/j.neuron.2012.04.036)
10. Kobatake E, Tanaka K. 1994 Neuronal selectivities to complex object features in the
ventral visual pathway of the macaque cerebral cortex. J. Neurophysiol. 71, 856–867.
(doi:10.1152/jn.1994.71.3.856)
11. Tanaka K, Saito H-A, Fukada Y, Moriya M. 1991 Coding visual images of objects in the
inferotemporal cortex of the macaque monkey. J. Neurophysiol. 66, 170–189. (doi:10.1152/
jn.1991.66.1.170)
12. Kourtzi Z, Connor CE. 2011 Neural representations for object perception: structure,
category, and adaptive coding. Annu. Rev. Neurosci. 34, 45–67. (doi:10.1146/annurev-
neuro-060909-153218)
13. Britten KH. 2008 Mechanisms of self-motion perception. Annu. Rev. Neurosci. 31, 389–410.
(doi:10.1146/annurev.neuro.29.051605.112953)
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

18
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14. Hubel DH, Wiesel TN. 1962 Receptive ﬁelds, binocular interaction and functional architecture
in the cat’s visual cortex. J. Physiol. 160, 106. (doi:10.1113/jphysiol.1962.sp006837)
15. Yau JM, Pasupathy A, Brincat SL, Connor CE. 2013 Curvature processing dynamics in
macaque area V4. Cereb. Cortex 23, 198–209. (doi:10.1093/cercor/bhs004)
16. Pasupathy A, Connor CE. 1999 Responses to contour features in macaque area V4.
J. Neurophysiol. 82, 2490–2502. (doi:10.1152/jn.1999.82.5.2490)
17. Ito M, Komatsu H. 2004 Representation of angles embedded within contour stimuli in area
V2 of macaque monkeys. J. Neurosci. 24, 3313–3324. (doi:10.1523/JNEUROSCI.4364-03.2004)
18. Brincat SL, Connor CE. 2004 Underlying principles of visual shape selectivity in posterior
inferotemporal cortex. Nat. Neurosci. 7, 880–886. (doi:10.1038/nn1278)
19. Brincat SL, Connor CE. 2006 Dynamic shape synthesis in posterior inferotemporal cortex.
Neuron 49, 17–24. (doi:10.1016/j.neuron.2005.11.026)
20. Attneave F. 1954 Some informational aspects of visual perception. Psychol. Rev. 61, 183.
(doi:10.1037/h0054663)
21. Feldman J, Singh M. 2005 Information along contours and object boundaries. Psychol. Rev. 112,
243. (doi:10.1037/0033-295X.112.1.243)
22. Ghosh A, Petkov N. 2006 Effect of high curvature point deletion on the performance of two
contour based shape recognition algorithms. Int. J. Pattern Recognit Artif Intell. 20, 913–924.
(doi:10.1142/S0218001406005046)
23. Biederman I, Ju G. 1988 Surface versus edge-based determinants of visual recognition. Cognit.
Psychol. 20, 38–64. (doi:10.1016/0010-0285(88)90024-2)
24. Watt RJ, Andrews DP. 1982 Contour curvature analysis: hyperacuities in the discrimination
of detailed shape. Vision Res. 22, 449–460. (doi:10.1016/0042-6989(82)90193-6)
25. Wilson HR. 1985 Discrimination of contour curvature: data and theory. JOSA A 2, 1191–1199.
(doi:10.1364/JOSAA.2.001191)
26. Watt RJ. 1984 Further evidence concerning the analysis of curvature in human foveal vision.
Vision Res. 24, 251–253. (doi:10.1016/0042-6989(84)90127-5)
27. Fang F, Kersten D, Schrater PR, Yuille AL. 2003 Human and ideal observers for detecting
image curves. In Neurips 2003, Vancouver, Canada, 8–13 December 2003. NeurIPS.
28. Kovacs I, Julesz B. 1993 A closed curve is much more than an incomplete one: effect
of closure in ﬁgure-ground segmentation. Proc. Natl Acad. Sci. USA 90, 7495–7497.
(doi:10.1073/pnas.90.16.7495)
29. Wilder J, Feldman J, Singh M. 2015 Contour complexity and contour detection. J. Vis. 15, 6.
(doi:10.1167/15.6.6)
30. Wilder J, Feldman J, Singh M. 2016 The role of shape complexity in the detection of closed
contours. Vision Res. 126, 220–231. (doi:10.1016/j.visres.2015.10.011)
31. Wilson HR, Richards WA. 1989 Mechanisms of contour curvature discrimination. JOSA A 6,
106–115. (doi:10.1364/JOSAA.6.000106)
32. Malik J, Rosenholtz R. 1997 Computing local surface orientation and shape from texture for
curved surfaces. Int. J. Comput. Vision 23, 149–168. (doi:10.1023/A:1007958829620)
33. Perona P. 1995 Deformable kernels for early vision. IEEE Trans. Pattern Anal. Mach. Intell. 17,
488–499. (doi:10.1109/34.391394)
34. Mumford D. 1991 Mathematical theories of shape: do they model perception? In Geometric
methods in computer vision, vol. 1570 (ed. BC Vermuri), pp. 2–10. Washington, DC: International
Society for Optics and Photonics.
35. Kichenassamy S, Kumar A, Olver P, Tannenbaum A, Yezzi A. 1996 Conformal curvature
ﬂows: from phase transitions to active vision. Arch. Ration. Mech. Anal. 134, 275–301.
(doi:10.1007/BF00379537)
36. Calabi E, Olver PJ, Shakiban C, Tannenbaum A, Haker S. 1998 Differential and numerically
invariant signature curves applied to object recognition. Int. J. Comput. Vision 26, 107–135.
(doi:10.1023/A:1007992709392)
37. Mumford DB, Michor PW. 2006 Riemannian geometries on spaces of plane curves. J. Eur.
Math. Soc. 8, 1–48. (doi:10.4171/jems/37)
38. Dalal N, Triggs B. 2005 Histograms of oriented gradients for human detection. In 2005 IEEE
computer society Conf. on computer vision and pattern recognition (CVPR’05), vol. 1, San Diego,
CA, 20–26 June 2005, pp. 886–893. New York, NY: IEEE.
39. Fischer P, Brox T. 2014 Image descriptors based on curvature histograms. In German Conf.
on pattern recognition, Münster, Germany, 2–5 September 2014, pp. 239–249. Berlin, Germany:
Springer.
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

19
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40. Monroy A, Eigenstetter A, Ommer B. 2011 Beyond straight lines–object detection using
curvature. In 2011 18th IEEE Int. Conf. on Image Processing, Brussels, Belgium, 11–14 September
2011, pp. 3561–3564. New York, NY: IEEE.
41. Mokhtarian F, Mackworth AK. 1992 A theory of multiscale, curvature-based shape
representation for planar curves. IEEE Trans. Pattern Anal. Mach. Intell. 14, 789–805.
(doi:10.1109/34.149591)
42. Mokhtarian F, Abbasi S, Kittler J. 1997 Efﬁcient and robust retrieval by shape content through
curvature scale space. In Image databases and multi-media search (eds AWM Smeulders, R Jain),
pp. 51–58. Singapore: World Scientiﬁc.
43. Mokhtarian F. 1995 Silhouette-based isolated object recognition through curvature scale
space. IEEE Trans. Pattern Anal. Mach. Intell. 17, 539–544. (doi:10.1109/34.391387)
44. Abbasi S, Mokhtarian F, Kittler J. 1999 Curvature scale space image in shape similarity
retrieval. Multimedia Syst. 7, 467–476. (doi:10.1007/s005300050147)
45. Mokhtarian F, Suomela R. 1998 Robust image corner detection through curvature scale space.
IEEE Trans. Pattern Anal. Mach. Intell. 20, 1376–1381. (doi:10.1109/34.735812)
46. Bishop CM. 1994 Neural networks and their applications. Rev. Sci. Instrum. 65, 1803–1832.
(doi:10.1063/1.1144830)
47. Lindsay G. 2020 Convolutional neural networks as a model of the visual system: past, present,
and future. J. Cogn. Neurosci. 33, 1–15.
48. Yamins DLK, Hong H, Cadieu CF, Solomon EA, Seibert D, DiCarlo JJ. 2014 Performance-
optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl
Acad. Sci. USA 111, 8619–8624. (doi:10.1073/pnas.1403112111)
49. Koenderink JJ, Richards W. 1988 Two-dimensional curvature operators. JOSA A 5, 1136–1141.
(doi:10.1364/JOSAA.5.001136)
50. Reif U, Goldman R. 2005 Geometric modelling and differential geometry curvature
formulas for implicit curves and surfaces. Comput. Aided Geom. Des. 22, 632–658.
(doi:10.1016/j.cagd.2005.06.002)
51. Long B, Yu C-PB, Konkle T. 2018 Mid-level visual features underlie the high-level categorical
organization of the ventral stream. Proc. Natl Acad. Sci. USA 115, E9015–E9024. (doi:10.1073/
pnas.1719616115)
52. Srihasam K, Vincent JL, Livingstone MS. 2014 Novel domain formation reveals proto-
architecture in inferotemporal cortex. Nat. Neurosci. 17, 1776–1783. (doi:10.1038/nn.3855)
53. Pasupathy A, Connor CE. 2002 Population coding of shape in area V4. Nat. Neurosci. 5, 1332–
1338. (doi:10.1038/972)
54. Bao P, She L, McGill M, Tsao DY. 2020 A map of object space in primate inferotemporal cortex.
Nature 583, 103–108. (doi:10.1038/s41586-020-2350-5)
55. Geusebroek J-M, Burghouts GJ, Smeulders AWM. 2005 The amsterdam library of object
images. Int. J. Comput. Vision 61, 103–112. (doi:10.1023/B:VISI.0000042993.50813.60)
56. Marblestone AH, Wayne G, Kording KP. 2016 Toward an integration of deep learning and
neuroscience. Front. Comput. Neurosci. 10, 94. (doi:10.1101/058545)
57. Cox DD, Dean T. 2014 Neural networks and neuroscience-inspired computer vision. Curr.
Biol. 24, R921–R929. (doi:10.1016/j.cub.2014.08.026)
58. Medathati NVK, Neumann H, Masson GS, Kornprobst P. 2016 Bio-inspired computer
vision: towards a synergistic approach of artiﬁcial and biological vision. Comput. Vis. Image
Understand. 150, 1–30. (doi:10.1016/j.cviu.2016.04.009)
59. Cohen T, Welling M. 2016 Group equivariant convolutional networks. In Int. Conf. on machine
learning, New York, NY: 19–24 June 2016, pp. 2990–2999. PMLR.
60. Kanazawa A, Sharma A, Jacobs D. 2014 Locally scale-invariant convolutional neural
networks. (http://arxiv.org/abs/1412.5104)
61. Muzahid AAM, Wan W, Sohel F, Wu L, Hou L. 2020 Curvenet: curvature-based multitask
learning deep networks for 3D object recognition. IEEE/CAA J. Autom. Sinica 8, 1177–1187.
(doi:10.1109/JAS.2020.1003324)
62. Linardatos P, Papastefanopoulos V, Kotsiantis S. 2020 Explainable AI: a review of machine
learning interpretability methods. Entropy 23, 18. (doi:10.3390/e23010018)
63. Olah C, Satyanarayan A, Johnson I, Carter S, Schubert L, Ye K, Mordvintsev A. 2018 The
building blocks of interpretability. Distill 3, e10. (doi:10.23915/distill.00010)
64. Pospisil D, Pasupathy A, Bair W. 2016 Comparing the brain’s representation of shape to that of
a deep convolutional neural network. In Proc. of the 9th EAI Int. Conf. on Bio-inspired Information
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

20
royalsocietypublishing.org/journal/rspa Proc. R.Soc. A 479: 20220662
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
and Communications Technologies (formerly BIONETICS), New York, NY, 3–5 September 2016,
pp. 516–523.
65. Li SPD, Bonner M. 2020 Curvature as an organizing principle of mid-level visual
representation: a semantic-preference mapping approach. In NeurIPS 2020 Workshop SVRHM,
Online, 6–12 December 2020. NeurIPS.
66. Baker N, Erlikhman G, Kellman PJ, Lu H. 2018 Deep convolutional networks do not
perceive illusory contours. In Cognitive Science Society Meeting, Madison, WI, 25–28 July 2018.
Cambridge, UK: Cambridge University Press.
67. Lawlor M, Zucker SW. 2013 Third-order edge statistics: contour continuation, curvature, and
cortical connections. NeurIPS 2013, Stateline, NV, 5–10 December 2013. NeurIPS.
68. Lazebnik S, Schmid C, Ponce J. 2006 Beyond bags of features: spatial pyramid matching
for recognizing natural scene categories. In 2006 IEEE Computer Society Conf. on Computer
Vision and Pattern Recognition (CVPR’06), vol. 2, New York, NY, 17–22 June 2006, pp. 2169–2178.
New York, NY: IEEE.
69. Freeman J, Simoncelli EP. 2011 Metamers of the ventral stream. Nat. Neurosci. 14, 1195–1201.
(doi:10.1038/nn.2889)
70. Deza A, Chen Y-C, Long B, Konkle T. 2019 Accelerated texforms: alternative methods
for generating unrecognizable object images with preserved mid-level features. In
Conference on Cognitive Computational Neuroscience (CCN) 2019, Berlin, 13–16 September 2019.
(https://ccneuro.org/2019/proceedings/0000879.pdf)
71. Sors LAS, Santaló LA. 2004 Integral geometry and geometric probability. Cambridge, UK:
Cambridge university press.
72. Marantan A, Tolkova I, Mahadevan L. 2023 Image cognition using contour curvature statistics.
Zenodo. (https://zenodo.org/badge/latestdoi/339746850)
73. Marantan A, Tolkova I, Mahadevan L. 2023 Image cognition using contour curvature statistics.
Figshare. (doi:10.6084/m9.ﬁgshare.c.6644081)
 Downloaded from https://royalsocietypublishing.org/ on 07 June 2023 

