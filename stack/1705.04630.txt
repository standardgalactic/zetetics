arXiv:1705.04630v6  [cs.LG]  16 May 2019
Forecasting using incomplete models
Vanessa Kosoy∗
Abstract
We consider the task of forecasting an inﬁnite sequence of future observations based on
some number of past observations, where the probability measure generating the observations is
“suspected” to satisfy one or more of a set of incomplete models, i.e., convex sets in the space of
probability measures. This setting is in some sense intermediate between the realizable setting
where the probability measure comes from some known set of probability measures (which can
be addressed using, e.g., Bayesian inference) and the unrealizable setting where the probability
measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that,
whenever the true probability measure satisﬁes an incomplete model in a given countable set, the
forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-
Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that
convergence in the Kantorovich-Rubinstein metric is weaker than convergence in total variation.
keywords: statistical learning theory, online learning, sequence prediction, probability theory,
Knightian uncertainty
1
Introduction
Forecasting future observations based on past observations is one of the fundamental problems in
machine learning, and more broadly is one of the fundamental components of rational reasoning
in general. This problem received a great deal of attention, using diﬀerent methods (see e.g. [1] or
Chapter 21 in [2]). Most of those methods assume ﬁxing a class H of models or “hypotheses”, each
of which deﬁnes a probability measure on sequences of observations (and also conditional probability
measures), and produce a forecast F H which satisﬁes at least one of two kinds of guarantees:
• In the realizable setting, the guarantee is that if the observations are sampled from some
µ ∈H, then F H converges to an “ideal” forecast in some sense.
• In the unrealizable setting, the guarantee is that for any sequence of observations, F H is
asymptotically as good as the forecast produced by any µ ∈H.
The realizable setting is often unrealistic, in particular because it requires that the environment
under observation is simpler than the observer itself.
Indeed, even though we avoid analyzing
computational complexity in this work, it should be noted that the computational (e.g., time)
complexity of a forecaster is always greater than the complexities of all µ ∈H. On the other hand,
the unrealizable setting usually only provides guarantees for short-term forecasts (since otherwise
the training data is insuﬃcient). The latter is in contrast to, e.g., Bayesian inference where the
time to learn the model depends on its prior probability, but once “learned” (i.e., once F H has
∗E-mail: vanessa.kosoy@intelligence.org.
1

converged to a given total variation distance from the true probability measure), arbitrarily long-
term forecasts become reliable.
The spirit of our approach is that the environment might be very complex, but at the same time
it might possess some simple features, and it is these features that the forecast must capture. For
example, if we consider a sequence of observations {on ∈{0, 1}}n∈N s.t. o2k+1 = o2k, then whatever
is the behavior of the even observations o2k, the property o2k+1 = o2k should asymptotically be
assigned high probability by the forecast (this idea was discussed in [3] as “open problem 4j”).
Formally, we introduce the notion of an incomplete model, which is a convex set M in the space
P(Oω) of probability measures on the space of sequences Oω. Such an incomplete model may be
regarded as a hybrid of probabilistic and Knightian uncertainty. We then consider a countable1
set H of incomplete models. For any M ∈H and µ ∈M, our forecasts will converge to M in an
appropriate sense with µ-probability 1. This convergence theorem can be regarded as an analogue
for incomplete models of Bayesian merging of opinions (see [4]), and is our main result. Our setting
can be considered to be in between realizable and unrealizable: it is “partially realizable” since we
require the environment to conform to some M ∈H, it is “partially unrealizable” since µ ∈M can
be chosen adversarially (we can even allow non-oblivious choice, i.e., dependence on the forecast
itself).
The forecasting method we demonstrate is based on the principles introduced in [5] (similar
ideas appeared in [6] in a somewhat simpler setting). The forecast may be regarded as the pricing
of a certain combinatorial prediction market, with a countable set of gamblers making bets. The
market pricing is then deﬁned by the requirement that the aggregate of all gamblers doesn’t make
a net proﬁt. The existence of such a pricing follows from the Kakutani-Glicksberg-Fan ﬁxed point
theorem and the Kuratowski-Rill-Nardzewski measurable selection theorem (the latter in order to
show that the dependence on observation history can be made measurable). The fact that the
aggregate of all gamblers cannot make a net proﬁt implies that each individual gambler can only
make a bounded proﬁt.
The above method is fairly general, but for our purposes, we associate with each incomplete
model M ∈H a set of gamblers that make bets which are guaranteed to be proﬁtable assuming the
true environment µ satisﬁes the incomplete model (i.e. µ ∈M). The existence of these gamblers
follows from the Hahn-Banach separation theorem (where the incomplete model deﬁnes the convex
set in question) and selection theorems used to ensure that the choice of separating functional
depends in a suﬃciently “regular” way on the market pricing. In order to show that the forecaster
we described satisﬁes the desired convergence property, we use tools from martingale theory.
The structure of the paper is as follows.
Section 2 deﬁnes the setting and states the main
theorem. Appendix A lays out the formalism of “combinatorial prediction markets,” and the central
concept of a dominant forecaster. Appendix B introduces the concept of a prudent gambling strategy,
which is a technique for proving convergence theorems about dominant forecasters. Appendix C
describes the gamblers associated with incomplete models and completes the proof of the main
theorem. Appendix D lists two theorems by other authors that we use. Appendix E proves the
validity of some examples given in Section 2.
2
Results
N will denote the set of natural numbers {0, 1, 2 . . .}.
Let {On}n∈N be a sequence of compact Polish spaces.
On represents the space of possible
observations at time n. Denote On := Q
m<n Om and Oω := Q
n∈N On. πO
n : Oω →On is the
1This work only deals with the nonparametric setting in which H is discrete.
2

projection mapping and x:n := πO
n (x). Given y ∈On, yOω :=

πO
n
−1
(y) is a closed subspace of
Oω. Given A ⊆On, we denote AOω :=

πO
n
−1
(A). On will be regarded as a topological space
using the product topology and as a measurable space with the σ-algebra of universally measurable
sets2. For any measurable space X, P (X) will denote the space of probability measures on X.
When X is a Polish space with the Borel σ-algebra, P (X) will be regarded as a topological space
using the weak topology and as a measurable space using the σ-algebra of Borel sets.
Given
µ ∈P (X), we denote supp µ ⊆X the support of µ.
Deﬁnition 1. A forecaster F is a family of measurable mappings
{Fn : On →P(Oω)}n∈N
s.t. supp Fn (y) ⊆yOω.
Given a forecaster F and y ∈On, Fn (y) represents the forecast corresponding to observation
history y. The condition supp Fn (y) ⊆yOω reﬂects the obvious requirement of consistency with
past observations.
Example 1. Consider any µ ∈P(Oω).
Then we can take Fn(y) to be a regular conditional
probability of µ (where the condition is x ∈yOω). This corresponds to Bayesian forecasting with
prior µ.
In order to formulate a claim about forecast convergence, we will need a metric on P(Oω).
Consider ρ : Oω ×Oω →R a metrization of Oω. Let Lip (Oω, ρ) be the Banach space of ρ-Lipschitz
functions on Oω, equipped with the norm
∥f∥ρ := max
x
|f (x)| + sup
x̸=y
|f (x) −f (y)|
ρ (x, y)
(1)
P(Oω) can be regarded as a compact subset of the dual space Lip (Oω, ρ)′, yielding the following
metrization of P(Oω):
dρ
KR (µ, ν) :=
sup
∥f∥ρ≤1
 Eµ [f] −Eν [f]

(2)
We call dρ
KR the Kantorovich-Rubinstein metric3.
Fix any x ∈Oω. It is easy to see that
lim
n→∞
max
x′∈x:nOω ρ
 x′, x

= 0
(3)
Denote δx ∈P(Oω) the unique probability measure s.t. δx
 {x}

= 1. It follows that for any
sequence {µn ∈P(Oω)}n∈N s.t. suppµn ⊆x:nOω
lim
n→∞dρ
KR (µn, δx) = 0
(4)
2The fact we use universally measurable sets rather than Borel sets will play an important role in the proof of
Lemma C.1.
3This is slightly diﬀerent from the conventional deﬁnition but strongly equivalent (i.e. each metric is bounded
by a constant multiple of the other). Other names used in the literature for the strongly equivalent metric are “1st
Wasserstein metric” and “earth mover’s distance.”
3

Therefore, formulating a non-vacuous convergence theorem requires “renormalizing” dKR for
each n ∈N. To this end, we consider a sequence of metrizations of Oω: {ρn : Oω × Oω →R}n∈N.
We denote dn
KR := dρn
KR. In the special case when On = O for all n ∈N and some ﬁxed space O,
there is a natural class of metrizations with the property that ρn(yx, yx′) = ρ(x, x′) for some ﬁxed
metric ρ on Oω and any y ∈On, x, x′ ∈Oω4. However, in general we can choose any sequence.
Another ingredient we will need is a notion of regular conditional probability for incomplete
models. Given measurable spaces X and Y , we will use the notation K : X
k−→Y to denote a
Markov kernel with source X and target Y . Given x ∈X, we will use the notation K (x) ∈P (Y ).
Given µ ∈P (X), µ ⋉K ∈P (X × Y ) denotes the semidirect product of µ and K, that is, the
unique measure satisfying
(µ ⋉K)(A × B) =
Z
A
K(x)(B) µ(dx)
(5)
Given π : X →Y measurable, π∗µ ∈P(Y ) denotes the pushforward of µ by π. K∗µ ∈P (Y )
denotes the pushforward of µ by K (i.e. the pushforward of µ ⋉K by the projection to Y ). Of
course π can be regarded as a “deterministic” Markov kernel, so the notation π∗µ is truly a special
case of K∗µ. When X, Y are Polish and π : X →Y is Borel measurable, µ | π : Y
k−→X is
deﬁned to be s.t. π∗µ ⋉
 µ | π

is supported on the graph of π and
 µ | π

∗π∗µ = µ (i.e. µ | π is
a regular conditional probability). By the disintegration theorem, µ | π exists and is deﬁned up to
coincidence π∗µ-almost everywhere.
Deﬁnition 2. Let X, Y be compact Polish spaces, π : X →Y continuous and M ⊆P(X). We say
that N : Y →2P(X) is a regular upper bound for M | π when
i. The set graph N := {(y, µ) ∈Y × P(X) | µ ∈N(y)} is closed.
ii. For every y ∈On, N(y) is convex.
iii. For every µ ∈M and π∗µ-almost every y ∈Y , (µ | π)(y) ∈N(y).
The reason we call that “regular upper bound for M | π” rather than just “M | π” is that,
roughly, our conditions guarantee N is “big enough” but don’t guarantee it is not “too big”. For
example, setting N(y) := P(X) would trivially satisfy all conditions. Also note that, although
technically we haven’t assumed M is convex, we might as well have assumed it: it is not hard to
see that, if N is a regular upper bound for M | π and M′ is the convex hull of M, then N is a
regular upper bound for M′ | π. The same remark applies to Theorem 1 below.
We give a few examples for Deﬁnition 2.
The proofs that these examples are valid are in
Appendix E.
Example 2. Suppose that M is convex and Y is a ﬁnite set (in particular, this example is applicable
when the On are ﬁnite sets, X = Oω, Y = On and π = πO
n ). Then, there is a unique N : Y →2P(X)
which is a minimal (w.r.t. set inclusion) regular upper bound for M | π and we have
N(y) = {
 µ | π−1(y)

| µ ∈M, µ
 π−1(y)

> 0}
(6)
Here, µ | π−1(y) is the conditional probability measure and the overline stands for topological
closure.
4In this special case, the need for renormalization is a side eﬀect of our choice of notation where the forecaster
produces a probability measure over the entire sequence rather than over future observations only. Also, in this case
the choice of ρ determines everything, since we only use Kantorovich-Rubinstein distance between measures with
support in yOω for the same y ∈On.
4

Example 3. Consider some I ⊆N and suppose that for each n ∈I, we are given Kn : On k−→On.
Assume that each Kn is Feller continuous, that is, that the induced mapping On →P (On) is
continuous. Deﬁne MK by
MK := {µ ∈P(Oω) | ∀n ∈I : πO
n+1∗µ = πO
n∗µ ⋉Kn}
(7)
For each n ∈N, deﬁne MK
n : On →2P(Oω) by
MK
n (y) = {µ ∈P(Oω) | supp µ ⊆yOω, ∀m ∈I : m ≥n =⇒πO
m+1∗µ = πO
m∗µ ⋉Km}
(8)
Then, MK
n is a regular upper bound for MK | πO
n .
Example 4. Suppose that Y is a compact subset of Rd for some d ∈N (in particular, this example
is applicable when each On is a compact subset of Rdn, X = Oω, Y = On and π = πO
n ; in that case
d = P
m<n dn). We regard Y as a metric space using the Euclidean metric. For any y ∈Y and
r > 0, Br (y) will denote the open ball of radius r with center at y.
Fix any M ⊆P(X). Then, there is a unique N : Y →2P(X) which is minimal among mappings
which both satisfy conditions i and ii of Deﬁnition 2 and are s.t. for any y ∈Y , ν ∈P(X) and
µ ∈M, if y ∈suppπ∗µ and ν = limr→0

µ | π−1  Br (y)

, then ν ∈N(y) (the limit is deﬁned
using the the weak topology). Moreover, N is a regular upper bound for M | π.
We are now ready to formulate the main result.
Given a metric space X with metric ρ : X ×X →R, x ∈X and A ⊆X, we will use the notation
ρ (x, A) := inf
y∈A ρ (x, y)
(9)
Theorem 1. Fix any H ⊆2P(Oω) countable. For every M ∈H and n ∈N, let Mn be a regular
upper bound for M | πO
n . Then, there exists a forecaster F H s.t. for any M ∈H, µ ∈M and
µ-almost any x ∈Oω
lim
n→∞dn
KR

F H
n (x:n) , Mn (x:n)

= 0
(10)
That is, the forecaster F H ensures that, for any incomplete model M satisﬁed by the true
environment µ, the forecast will (almost surely) converge to the model (as opposed to complete
models, there may be several incomplete models satisﬁed by the same environment).
Note that F H as above exists for any choice of {ρn}n∈N but it depends on the choice. Informally,
we can think of this choice as determining the duration of the future time period over which we
need our forecast to be reliable. It also might depend on the choice of regular upper bounds (of
course, if there are minimal regular upper bounds, these yield a forecaster than works for any other
regular upper bounds).
The example from the Introduction section can now be realized as follows. Take On = O = {0, 1}
and let MK ⊆P(Oω) be as in Example 3, where I = 2N+1 and Pro∼K2n+1(y) [o = y2n] = 1. Deﬁne
ρn by
ρn(x, x′) :=
(
max{2n−m | xm ̸= x′
m} if x ̸= x′
0 if x = x′
(11)
If MK ∈H, and x ∈Oω is s.t. x2n+1 = x2n, then for any k ∈N, the forecaster F H of Theorem 1
satisﬁes
5

lim
n→∞
Pr
x′∼F2n(x:2n)
h
x′
2(n+k)+1 = x′
2(n+k)
i
= 1
(12)
lim
n→∞
Pr
x′∼F2n+1(x:2n+1)
h
x′
2(n+k)+1 = x′
2(n+k)
i
= 1
(13)
If we only cared about predicting the next observation (as opposed to producing a probability
measure over the entire sequence), this example (and any other instance of Example 3, at least in
the case On = O ﬁnite) would be a special case of the “sleeping experts” setting in online learning
(see [7]). However, our formalism is much more general, even for predicting the next observation.
For instance, we can consider O = {0, 1, 2, 3} and have an incomplete model specifying that the
next observation is an odd number (thus, the “expert” specializes by predicting speciﬁc properties
of the observation rather than only making predictions at speciﬁc times). As another example, we
can consider O = {0, 1, 2} and an incomplete model specifying that the probability distribution
(p0, p1, p2) of each observation satisﬁes p0, p1, p2 ≥0.1. This model would capture any environment
that can be regarded as a process observed through a noisy sensor that has a probability of 0.3 to
output a uniformly random observation instead of the real state of the process.
The rest of the paper is devoted to proving Theorem 1.
A
Appendix: Dominant Forecasters
In this section we explain our generalization of the methods introduced in [5] under the name
“logical inductors.” The main diﬀerences between our formalism and that of [5] are
• We are interested in sequence forecasting rather than formal logic.
• We consider probability measures on certain Polish spaces, rather than probability assignment
functions on ﬁnite sets.
• In particular, no special treatment of expected values is required.
• The observations are stochastic rather than deterministic.
Our terminology also diﬀers from [5]: their “market” is our “forecaster”, their “trader” is our
“gambler”.
Similar ideas were investigated in [6] under the name “defensive forecasting”. However in [6],
the forecast is a single probability of an impending observation in {0, 1} rather than a probability
measure in the space of inﬁnite sequences of observations taking values in arbitrary compact Polish
spaces.
In any case, our exposition assumes no prior knowledge about logical inductors or defensive
forecasting.
The idea is to consider a collection of gamblers making bets against the forecaster. If a gambler
with ﬁnite budget cannot make an inﬁnite proﬁt, the gambler is said to be “dominated” by the
forecaster. We then prove that for any countable collection of gamblers, there is a forecaster that
dominates all of them.
Given a compact Polish space X, C (X) will denote the Banach space of continuous functions
with uniform norm. We will also use the shorthand notation B (X) := C
 P (X) × X

. Equiva-
lently, B (X) can be regarded to be the space of continuous functions from P (X) to C (X), and we
will curry implicitly in our notation.
6

We regard B(Oω) as the space of bets that can be made against a forecaster. Given β ∈B(Oω),
a forecast µ ∈P(Oω) and observation sequence x ∈Oω, the payoﬀof the bet β is
V β (µ, x) := β (µ, x) −Ex′∼µ
h
β
 µ, x′i
(14)
This deﬁnition ensures that Ex∼µ

V β(µ, x)

= 0, so that the bet is fair from the perspective
of the forecaster.
Note that this deﬁnes a bounded linear operator V : B (X) →B (X).
Example A.1. Suppose that the On are ﬁnite sets with discrete topology. Fix some n ∈N and
A ⊆On. Deﬁne βA ∈B(Oω) by
βA(µ, x) :=
(
1 if x:n ∈A
0 if x:n ̸∈A
(15)
βy represents betting that the sequence of observations will begin with an element of A. The
payoﬀV βA(µ, x) is 1 −µ (AOω) when x:n ∈A and −µ (AOω) when x:n ̸∈A.
C (X) and B (X) will also be regarded as a measurable spaces, using the σ-algebra of Borel
sets. We remind that the σ-algebra on On is the algebra of universally measurable sets.
Deﬁnition A.1. A gambler is a family of measurable mappings
{Gn : On × P(Oω)n →B (Oω)}n∈N
A gambler is considered to observe a forecaster and bet against it. Given y ∈On and µ ∈
P(Oω)n, Gn (y, µ) = β means that, if y are the ﬁrst n observations and µ are the ﬁrst n forecasts
(made after observing n −1 out of the n observations), the gambler will make bet β (which is in
itself a function of the forecast the forecaster makes after the n-th observation).
Example A.2. Suppose that the On are ﬁnite sets with discrete topology. Consider a family of
mappings {fn : On →On}n∈N. Deﬁne the gambler Gf by
Gf
n(y, µ) := β{yf(y)}
(16)
That is, Gf always bets on the next observation being f(y).
We now introduce some notation regarding the interaction of gamblers and forecasters.
Given a gambler G and a forecaster F, we deﬁne the measurable mappings GF
n : On →B(Oω)
by
GF
n (y) := Gn
 y, F0, F1 (y:1) . . . Fn−1 (y:n−1)

(17)
Here, y:m denotes the projection of y to Om.
We deﬁne the measurable mappings V G
F
n : On →C(Oω) and ΣV GF
n : On−1 →C(Oω) by
V G
F
n (y) :=

V GF
n (y)
  Fn (y)

(18)
ΣV GF
n (y) :=
X
m<n
V G
F
m (y:m)
(19)
In the deﬁnition of ΣV GF
0 , O−1 is considered to be equal to O0 (i.e. the one point space).
7

That is, V G
F
n (y) (x) is the payoﬀof the n-th gamble of gambler G playing against forecaster
F, assuming initial history y and full history x. ΣV GF
n (y) is the total payoﬀof the ﬁrst n gambles.
We deﬁne the measurable mappings ΣVmin GF
n , ΣVmax GF
n : On−1 →R by
ΣVmin GF
n (y) := min
yOω ΣV GF
n (y)
(20)
ΣVmax GF
n (y) := max
yOω ΣV GF
n (y)
(21)
Thus, ΣVmin GF
n (y) is the minimal possible payoﬀof the ﬁrst n gambles and ΣVmax GF
n (y) is
the maximal possible payoﬀ. y appears twice on the right hand side of equations (20,21) because,
ﬁrst, it deﬁnes the space of histories over which we minimize/maximize and, second, it deﬁned the
input to the gambler and forecaster.
We are now ready to state the formally state the deﬁnition of “dominance” alluded to in the
beginning of the section.
Deﬁnition A.2. Consider a forecaster F and a gambler G. F is said to dominate G when for any
x ∈Oω, if condition 22 holds then condition 23 holds:
inf
n∈N ΣVmin GF
n (x:n−1) > −∞
(22)
sup
n∈N
ΣVmax GF
n (x:n−1) < +∞
(23)
That is, as long as the gambler doesn’t go into inﬁnite debt, it cannot make an inﬁnite proﬁt.
Example A.3. Suppose that the On are ﬁnite sets with discrete topology. Consider a family of
mappings {fn : On →On}n∈N. Deﬁne the forecaster F f by Fn(y) := δfn(y). Then, it is easy to see
that F f dominates Gf.
The following is the main theorem of this section, which is our analogue of “Theorem 4.0.6”
from [5].
Theorem A.1. Let {Gk}k∈N be a family of gamblers. Then, there exists a forecaster F G s.t. for
any k ∈N, F dominates Gk.
The rest of the section is devoted to proving Theorem A.1. The proof will consist of two steps.
First we show that for any single gambler, there is a dominant forecaster. Then, given a countable
set G of gamblers, we construct a single gambler s.t. dominating this gambler implies dominating
all gamblers in the set.
The following lemma shows that for any bet, there is a forecast which makes the bet unwinnable.
Lemma A.1. Consider X a compact Polish space and β ∈B (X). Then, there exists µ ∈P (X)
s.t.
supp µ ⊆arg max
x∈X
β (µ, x)
(24)
Proof. Deﬁne K : P (X) →2P(X) as follows:
K (µ) := {ν ∈P (X) | supp ν ⊆arg max β (µ)}
8

For any µ, K (µ) is obviously convex and non-empty. It is also easy to see that the graph of K
in P (X) × P (X) is closed. Applying the Kakutani-Glicksberg-Fan theorem, we conclude that K
has a ﬁxed point, i.e. µ ∈P (X) s.t. µ ∈K (µ).
Note that µ as above indeed makes β unwinnable since
V β(µ, x) = β (µ, x) −Ex′∼µ
h
β
 µ, x′i
= β(µ, x) −max
x′∈X β
 µ, x′
≤0
Next, we show that the probability measure of Lemma A.1 can be made to depend measurably
on past observations and the bet.
Lemma A.2. Fix Y1,2 compact Polish spaces and denote X := Y1 × Y2. Then, there exists a Borel
measurable mapping α : Y1 × B (X) →P (X) s.t. for any y ∈Y1 and β ∈B (X)
supp α (y, β) ⊆arg max
y×Y2
β
 α (y, β)

(25)
Proof. Deﬁne Z1, Z2, Z ⊆Y1 × B (X) × P (X) by
Z1 := {(y, β, µ) ∈Y1 × B (X) × P (X) | suppµ ⊆y × Y2}
Z2 := {(y, β, µ) ∈Y1 × B (X) × P (X) | Eµ

β (µ)

= max
y2∈Y2 β (µ, y, y2)}
Z := Z1 ∩Z2 = {(y, β, µ) ∈Y1 × B (X) × P (X) | suppµ ⊆arg max
y×Y2
β (µ)}
We can view Z as the graph of a multivalued mapping from Y1 × B (X) to P (X) (i.e a mapping
from Y1 × B (X) to 2P(X)). We will now show that this multivalued mapping has a selection, i.e. a
single-valued Borel measurable mapping whose graph is a subset of Z. Obviously, the selection is
the desired α.
Fix ρ a metrization of Y1. Z1 is the vanishing locus of the continuous function E(y1,y2)∼µ

ρ (y1, y)

.
Z2 is the vanishing locus of the continuous function Eµ

β (µ)

−maxy2∈Y2 β (µ, y, y2). Therefore
Z1,2 are closed and so is Z. In particular, the ﬁber Zyβ of Z over any (y, β) ∈Y1 × B (X) is also
closed.
For any y ∈Y1, β ∈B (X), deﬁne iy : Y2 →X by iy (y2) := (y, y2) and βy ∈B (Y2) by βy
 ν, y′ :=
β
 iy∗ν, y, y′
. Applying Lemma A.1 to βy we get ν ∈P (Y2) s.t.
supp ν ⊆arg max βy (ν)
It follows that
 y, β, iy∗ν

∈Z and hence Zyβ is non-empty.
Consider any U ⊆P (X) open. Then, AU :=
 Y1 × B (X) × U

∩Z is locally closed, and in
particular, it is an Fσ set. Therefore, the image of AU under the projection to Y1 × B (X) is also
Fσ and in particular Borel.
Applying the Kuratowski-Rill-Nardzewski measurable selection theorem, we get the desired
result (Z is the graph of the multivalued mapping, we established that the values Zyβ of this
mapping are closed and non-empty, the previous paragraph establishes that the mapping is weakly
measurable, and the theorem implies it has a selection, which is α).
9

We deﬁne the measurable mappings ¯GF
n : On →C(Oω) by
¯GF
n (y) := GF
n
 y; Fn (y)

(26)
Here, the notation GF
n
 y; Fn (y)

means GF
n (y)
 Fn (y)

(GF
n (y) is a function from P(Oω) to
C(Oω) which we apply to Fn (y) ∈P(Oω)). We will use the semicolon in a similar way in the rest
of paper as well: to indicate applying a function which is the result of another function.
The following is our analogue of “Theorem 1” from [6].
Corollary A.1. Let G be a gambler. Then, there exists a forecaster F s.t. for all n ∈N and
y ∈On
suppFn (y) ⊆arg max
yOω
¯GF
n
(27)
Proof. For every n ∈N, let αn : On × B(Oω) →P(Oω) be as in Lemma A.2. Observing that the
deﬁnition of GF
n depends only on Fm for m < n, we deﬁne F recursively as
Fn (y) := αn

y, GF
n (y)

In particular, the forecaster F of Corollary A.1 dominates G since, as easy to see, V G
F
n ≤0
and hence ΣV GF
n ≤0.
We will now introduce a way to transform a gambler in a way that enforces a “ﬁnite spending
budget.” Consider a gambler G and ﬁx b > 0 (the size of the “budget”). Deﬁne the measurable
functions ΣV Gn : On−1 × P(Oω)n →C(Oω) by
ΣV Gn (y, µ) :=
X
m<n
 V Gm (y:m, µ:m)

(µm)
(28)
Here, µ:m denotes the projection of µ ∈P(Oω)n to P(Oω)m that takes components l < m
and µm denotes the m-th component of µ. Deﬁne the measurable functions ΣVmin Gn, ΣVmax Gn :
On−1 × P(Oω)n →R by
ΣVmin Gn (y, µ) := min
yOω ΣV Gn (y, µ)
(29)
ΣVmax Gn (y, µ) := max
yOω ΣV Gn (y, µ)
(30)
Equations (29,30) are completely analogical to equations (20,21) except that we deﬁne functions
of forecast histories instead of considering a particular forecaster.
Deﬁne the measurable sets
Ab Gn := {(y, µ) ∈On−1 × P(Oω)n | ΣVmin Gn (y, µ) > −b}
(31)
Note that Ab Gn ×On is a measurable subset of On ×P(Oω)n and in particular can be regarded
as a measurable space in itself. Deﬁne the measurable functions Nb Gn : Ab Gn × On →C
 P(Oω)

by
Nb Gn (y, µ; ν) := max
 
1, max
yOω
−
 V Gn (y, µ)

(ν)
ΣV Gn (y:n−1, µ) + b
!−1
(32)
10

As before, the semicolon indicates currying i.e. Nb Gn (y, µ; ν) = Nb Gn (y, µ) (ν).
Finally, deﬁne the gambler ΛbG as follows
ΛbGn (y, µ) :=
(
0 if ∃m < n : (y:m, µ:m+1) ̸∈Ab Gm+1
Nb Gn (y, µ) · Gn (y, µ) otherwise
(33)
The next proposition shows that as long as G stays “within budget b,” the operator Λb has no
eﬀect.
Proposition A.1. Consider any gambler G, b > 0, n ∈N, y ∈On−1 and µ ∈P(Oω)n. Assume
that for all m < n, (y:m, µ:m+1) ∈Ab Gm+1. Then, for all m < n
ΛbGm (y:m, µ:m; µm) = Gm (y:m, µ:m; µm)
(34)
Proof. Consider any m < n. We have
ΣVmin Gm+1 (y:m, µ:m+1) + b > 0
∀x ∈y:mOω : ΣV Gm+1 (y:m, µ:m+1; x) + b > 0
∀x ∈y:mOω : ΣV Gm (y:m−1, µ:m; x) +
 V Gm (y:m, µ:m)

(µm, x) + b > 0
∀x ∈y:mOω : ΣV Gm (y:m−1, µ:m; x) + b > −
 V Gm (y:m, µ:m)

(µm, x)
Using the assumption again, the left hand side is positive. It follows that
∀x ∈y:mOω : 1 > −
 V Gm (y:m, µ:m)

(µm, x)
ΣV Gm (y:m−1, µ:m; x) + b
Nb Gm (y:m, µ:m; µm) = 1
Since we are in the second case in the deﬁnition of ΛbGm (y:m, µ:m), we get the desired result.
Now, we show that ΛbG indeed never “goes over budget.”
Proposition A.2. Consider any gambler G and b > 0. Then, for any n ∈N, y ∈On−1 and
µ ∈P(Oω)n
ΣVmin ΛbGn (y, µ) ≥−b
(35)
Proof. Let m0 ∈N be the largest number s.t. m0 ≤n and
∀m ≤m0 : ΣVmin Gm (y:m−1, µ:m) > −b
For any m ≤m0, we have, by Proposition A.1
ΣVmin ΛbGm (y:m−1, µ:m) = ΣVmin Gm (y:m−1, µ:m) > −b
(Note that the sum in the deﬁnition of ΣVmin ΛbGm only involves ΛbGl for l < m ≤m0)
For m = m0 + 1 and any x ∈y:m0Oω, we have
11

ΣV ΛbGm0+1
 y:m0, µ:m0+1; x

= ΣV ΛbGm0
 y:m0−1, µ:m0; x

+

V ΛbGm0
 y:m0, µ:m0
  µm0, x

The ﬁrst term only involves ΛbGl for l < m0, allowing us to apply Proposition A.1. The second
term is in the second case of the deﬁnition of Λb.
ΣV ΛbGm0+1
 y:m0, µ:m0+1; x

= ΣV Gm0
 y:m0−1, µ:m0; x

+
Nb Gm0
 y:m0, µ:m0; µm0

·

V Gm0
 y:m0, µ:m0
  µm0, x

If x is s.t.

V Gm0
 y:m0, µ:m0
  µm0, x

≥0, then
ΣV ΛbGm0+1
 y:m0, µ:m0+1; x

≥ΣV Gm0
 y:m0−1, µ:m0; x

≥ΣVmin Gm0
 y:m0−1, µ:m0

> −b
If x is s.t.

V Gm0
 y:m0, µ:m0
  µm0, x

< 0, then, by the deﬁnition of Nb
ΣV ΛbGm0+1
 y:m0, µ:m0+1; x

≥ΣV Gm0
 y:m0−1, µ:m0; x

+
ΣV Gm0
 y:m0−1, µ:m0; x

+ b
−

V Gm0
 y:m0, µ:m0
  µm0, x
 ·

V Gm0
 y:m0, µ:m0
  µm0, x

ΣV ΛbGm0+1
 y:m0, µ:m0+1; x

≥−b
Finally, consider m > m0 + 1.
ΣV ΛbGm (y:m−1, µ:m; x) = ΣV ΛbGm−1 (y:m−2, µ:m−1; x) +
 V ΛbGm−1 (y:m−1, µ:m−1)

(µm−1, x)
The second term is in the ﬁrst case of the deﬁnition of Λb and therefore vanishes.
ΣV ΛbGm (y:m−1, µ:m; x) = ΣV ΛbGm−1 (y:m−2, µ:m−1; x)
By induction on m, we conclude:
ΣVmin ΛbGm (y:m−1, µ:m) ≥ΣVmin ΛbGm−1 (y:m−2, µ:m−1) ≥−b
Next, we show that for any gambler there is “limited budget” gambler s.t. any forecaster that
dominates it also dominates the original gambler. For any x ∈R, we denote x+ := max(x, 0).
Proposition A.3. Let G be a gambler and ζ : N →(0, 1] be s.t.
bζ :=
∞
X
b=0
ζ (b) b < ∞
(36)
Deﬁne the gambler ΛζG by
ΛζG :=
∞
X
b=1
ζ (b) ΛbG
(37)
12

Then
ΣVmin ΛζGn ≥−bζ
(38)
Moreover, if F is a forecaster that dominates ΛζG then it also dominates G.
Proof. Equation 38 follows from Proposition A.2. Now, consider any F that dominates ΛζG. For
any x ∈Oω, 38 and Deﬁnition A.2 imply
sup
n∈N
ΣVmax ΛζGF
n (x:n−1) < +∞
Consider x ∈Oω s.t. condition 22 holds. Denote
b0 :=

−inf
n∈N ΣVmin GF
n (x:n−1)

+
By Proposition A.1, for any b > b0 we have ΛbGF = GF . We get
sup
n∈N
max
x′∈x:n−1Oω

X
b≤b0
ζ (b) ΣV ΛbGF
n
 x:n−1, x′
+
X
b>b0
ζ (b) ΣV GF
n
 x:n−1, x′

< +∞
Applying Proposition A.2 to the ﬁrst term
sup
n∈N
max
x′∈x:n−1Oω

−
X
b≤b0
ζ (b) b +
X
b>b0
ζ (b) ΣV GF
n
 x:n−1, x′

< +∞
sup
n∈N
ΣVmax GF
n (x:n−1) < +∞
The last ingredient we need to prove Theorem A.1 is the step from one gambler to a countable
set of gamblers.
Proposition A.4. Let {Gk}k∈N be a family of gamblers, ξ : N →(0, 1] and bξ > 0 s.t.
∞
X
k=0
ξ (k) min

ΣVmin Gk
n, 0

≥−bξ
(39)
Deﬁne the gambler Gξ by
Gξ
n :=
n
X
k=0
ξ (k) Gk
n
(40)
Consider a forecaster F that dominates Gξ. Then, F dominates Gk for all k ∈N.
Proof. Fix k ∈N and x ∈Oω. Equation 39 implies
inf
n∈N ΣVmin GξF
n
 x:n−1, x′
> −∞
Therefore we have
13

sup
n∈N
ΣVmax GξF
n (x:n−1) < +∞
sup
n≥k
max
x′∈x:n−1Oω




ξ (k) ΣV GkF
n
 x:n−1, x′
+
X
j̸=k
j≤n
ξ (j) ΣV GjF
n
 x:n−1, x′




< +∞
sup
n≥k
max
x′∈x:n−1Oω

ξ (k) ΣV GkF
n
 x:n−1, x′
−bξ

< +∞
sup
n≥k
ΣVmax GkF
n (x:n−1) < +∞
Since there are only ﬁnitely many values of n less than k, it follows that
sup
n∈N
ΣVmax GkF
n (x:n−1) < +∞
Proof of Theorem A.1. Deﬁne ζ (b) := max (b, 1)−3. By Proposition A.3, we have
ΣVmin ΛζGk
n ≥−π2
6
Deﬁne ξ (k) := (k + 1)−2.
We have
∞
X
k=0
ξ (k) min

ΣVmin ΛζGk
n, 0

≥−π4
36
By Corollary A.1, there is a forecaster F G that dominates ΛζGξ.
By Proposition A.4, F G
dominates ΛζGk for all k ∈N. By Proposition A.3, F G dominates Gk for all k ∈N.
B
Appendix: Prudent Gambling Strategies
In this section we construct a tool for proving asymptotic convergence results about dominant
forecasters. We start by introducing a notion of “prudent gambling,” which requires that bets are
only made when the expected payoﬀis a certain fraction of the risk. First, we need some notation.
Given X, Y Polish spaces, µ ∈P(X), π : X →Y Borel measurable and g : X →R Borel
measurable, we will use the notation
Eµ
h
g | π−1 (y)
i
:= E(µ|π)(y) [g]
(41)
We remind that πO
n is the projection mapping from Oω to On and yOω :=

πO
n
−1
(y).
Deﬁnition B.1. A gambling strategy is a uniformly bounded family of measurable mappings
{Sn : On →B(Oω)}n∈N
As opposed to a gambler, a gambling strategy doesn’t depend on the forecast history.
14

Deﬁnition B.2. Given a gambling strategy S and µ∗∈P(Oω), S is said to be µ∗-prudent when
there is α > 0 s.t. for any n ∈N, πO
n∗µ∗-almost any y ∈On and any µ ∈P(Oω)
Eµ∗
Sn (y; µ) | yOω
−Eµ

Sn (y; µ)

≥α

max
yOω Sn (y; µ) −min
yOω Sn (y; µ)

(42)
Here, we can think of µ as the forecast and µ∗as the true environment.
The “correct” way for a gambler to use a prudent gambling strategy is not employing it all
the time: in order to avoid running out of budget, a gambler shouldn’t place too many bets
simultaneously. To address this, we deﬁne a gambler that “plays” (employs the given strategy)
only when all previous bets are close to being settled.
Deﬁnition B.3. Consider any gambling strategy S. We deﬁne the gambler Γ S recursively as
follows
Γ Sn (y, µ) :=
(
Sn (y) if ΣVmax Γ Sn (y:n−1, µ) −ΣVmin Γ Sn (y:n−1, µ) ≤1
0 otherwise
(43)
Theorem B.1. Consider µ∗∈P(Oω), S a µ∗-prudent gambling strategy and a forecaster F.
Assume F dominates Γ S. Then, for µ∗-almost any x ∈Oω
∞
X
n=0

Eµ∗
h
Sn
 x:n; Fn (x:n)

| x:nOωi
−EFn(x:n)
h
Sn
 x:n; Fn (x:n)
i
< +∞
(44)
In particular, the terms of the series on the left hand-side converge to 0 (they are non-negative
since S is µ∗-prudent).
The rest of the section is dedicated to proving Theorem B.1.
We start by showing, in a more abstract setting (not speciﬁc to forecasting), that for any
sequences of bets that is “prudent” in the sense that the expected payoﬀis bounded below by a
ﬁxed fraction of the risk (= bet magnitude), the probability to lose an inﬁnite amount vanishes
and, moreover, it holds with probability 1 that if the total bet magnitude is unbounded then there
is inﬁnite gain.
Lemma B.1. Consider Ωa probability space, {Fn ⊆2Ω}n∈N a ﬁltration of Ω, α, M > 0, {Xn :
Ω→R}n∈N and {Yn : Ω→[0, M]}n∈N stochastic processes adapted to F (i.e. Xn and Yn are
Fn-measurable). Assume the following conditions5:
i. E

|X0|

< +∞
ii. ∀n ∈N, m ≥n : |Xn −Xm| ≤Pm−1
l=n Yl + M
iii. E

Xn+1 | Fn

≥Xn + αYn
Then, for almost all ω ∈Ω
a. infn Xn (ω) > −∞
b. If supn Xn (ω) < +∞then P
n Yn (ω) < +∞
5Equalities and inequalities between random variables are implicitly understood as holding almost surely.
15

Proof. Without loss of generality, we can assume M = 1 (otherwise we can renormalize X and Y
by a factor of M−1). Deﬁne {X0
n : Ω→R}n∈N by
X0
n := Xn −α
n−1
X
m=0
Ym
It is easy to see that X0 is a submartingale.
We deﬁne {Nn : Ω→N ⊔{∞}}n∈N recursively as follows
N0 (ω) := 0
Nk+1 (ω) :=



inf{n ∈N | Pn−1
m=Nk(ω) Ym (ω) ≥1} if Nk (ω) < ∞
∞if Nk (ω) = ∞
For each k ∈N, Nk is a stopping time w.r.t. F. As easy to see, for each k ∈N, {X0
min(n,Nk)}n∈N
is a uniformly integrable submartingale. Using the fact that Nk ≤Nk+1 to apply Theorem D.1 (see
Appendix D), we get
E
h
X0
Nk+1 | FNk
i
≥X0
Nk
Clearly, {X0
Nk}k∈N is adapted to {FNk}k∈N.
Doob’s second martingale convergence theo-
rem implies that E
h
|X0
Nk|
i
< ∞(X0
Nk is the limit of the uniformly integrable submartingale
{X0
min(n,Nk)}n∈N). We conclude that {X0
Nk}k∈N is a submartingale.
It is easy to see that |X0
Nk+1 −X0
Nk| ≤3. Applying the Azuma-Hoeﬀding inequality, we conclude
that for any positive integer k:
Pr
h
X0
Nk −X0 < −k
3
4
i
≤exp
 
−
k
3
2
2 · 32k
!
= exp
 
−k
1
2
18
!
It follows that
∞
X
k=1
Pr
h
X0
Nk −X0 < −k
3
4
i
< ∞
Pr
h
∃k ∈N∀l > k : X0
Nl −X0 ≥−l
3
4
i
= 1
Pr

∃k ∈N∀l > k : XNl −X0 ≥α
Nl−1
X
n=0
Yn −l
3
4

= 1
It is easy to see that if ω ∈Ωis s.t. P∞
n=0 Yn (ω) = ∞then PNl(ω)−1
n=0
Yn (ω) ≥l. We get
Pr


∞
X
n=0
Yn = ∞=⇒∃k ∈N∀l > k : XNl −X0 ≥αl −l
3
4

= 1
To complete the proof, observe that if ω ∈Ωis s.t. infn Xn (ω) = −∞then P∞
n=0 Yn (ω) = ∞
(since |Xn (ω) −X0 (ω)| ≤Pn−1
m=0 Yn (ω) + 1).
16

Corollary B.1. Consider Ωa probability space, {Fn ⊆2Ω}n∈N a ﬁltration of Ω, α, M > 0, {X′
n :
Ω→R}n∈N and {Yn : Ω→[0, M]}n∈N stochastic processes adapted to F and {Xn : Ω→R}n∈N an
arbitrary stochastic process. Assume the following conditions:
i. |Xn −X′
n| ≤M
ii. E

|X0|

< +∞
iii. |Xn+1 −Xn| ≤Yn
iv. E

Xn+1 −Xn | Fn

≥αYn
Then, for almost all ω ∈Ω
a. infn Xn (ω) > −∞
b. If supn Xn (ω) < +∞then P
n Yn (ω) < +∞
Proof. Deﬁne X′′
n := E

Xn | Fn

. We have
E

|X′′
0 |

= E
h
|E

X0 | F0

|
i
≤E
h
E

|X0| | F0
i
= E

|X0|

< ∞
|X′′
n −X′
n| = |E

Xn | Fn

−X′
n| = |E

Xn −X′
n | Fn

| ≤M
|X′′
n −Xn| ≤|X′′
n −X′
n| + |X′
n −Xn| ≤2M
∀m ≥n : |X′′
m −X′′
n| ≤|Xm −Xn| + 4M ≤
n−1
X
l=n
Yl + 4M
E

X′′
n+1 −X′′
n | Fn

= E
h
E

Xn+1 | Fn+1

−E

Xn | Fn

| Fn
i
= E

Xn+1 −Xn | Fn

≥αYn
Applying Lemma B.1 to X′′ and using |X′′
n −Xn| ≤2M, we get the desired result.
We will also need the following property of the operator Γ, reﬂecting that at any given moment,
the uncertainty of the gambler Γ S about the total worth of its bets is bounded.
Proposition B.1. Given any gambling strategy S, we have
ΣV Γ Sn −ΣVmin Γ Sn ≤2 max
m<n sup
y∈Om∥Sm (y)∥+ 1
(45)
In particular, the left hand side is uniformly bounded.
Proof. We prove by induction. For n = 0 the claim is obvious. Consider any n ∈N, y ∈On and
µ ∈P(Oω)n+1. First, assume that
ΣVmax Γ Sn (y:n−1, µ:n) −ΣVmin Γ Sn (y:n−1, µ:n) > 1
Then, by deﬁnition of Γ S, Γ Sn (y, µ:n) ≡0 and therefore
17

ΣV Γ Sn+1 (y, µ) = ΣV Γ Sn (y:n−1, µ:n)
ΣVmin Γ Sn+1 (y, µ) ≥ΣVmin Γ Sn (y:n−1, µ:n)
ΣV Γ Sn+1 (y, µ) −ΣVmin Γ Sn+1 (y, µ) ≤ΣV Γ Sn (y:n−1, µ:n) −ΣVmin Γ Sn (y:n−1, µ:n)
By the induction hypothesis, we get
ΣV Γ Sn+1 (y, µ) −ΣVmin Γ Sn+1 (y, µ) ≤2 max
m<n sup
y∈Om∥Sm (y)∥+ 1 ≤max
m≤n sup
y∈Om∥Sm (y)∥+ 1
Now, assume that
ΣVmax Γ Sn (y:n−1, µ:n) −ΣVmin Γ Sn (y:n−1, µ:n) ≤1
Then, Γ Sn (y, µ:n) = Sn (y) and therefore
ΣV Γ Sn+1 (y, µ) = ΣV Γ Sn (y:n−1, µ:n) +
 V Sn (y)

(µn)
We get
ΣV Γ Sn+1 (y, µ)−ΣVmin Γ Sn+1 (y, µ) ≤ΣVmax Γ Sn (y:n−1, µ:n)−ΣVmin Γ Sn (y:n−1, µ:n)+2∥Sn (y)∥
By the assumption, the ﬁrst two terms total to ≤1, yielding the desired result.
As a ﬁnal ingredient for the proof of Theorem B.1, we will need another property of Γ, namely
that when the total magnitude of all bets made is ﬁnite, the gambler eventually starts “playing”
in every round.
Proposition B.2. Let S be a gambling strategy, F a forecaster and x ∈Oω. Assume that
∞
X
n=0

max
x:nOω Γ S
F
n (x:n) −min
x:nOω Γ S
F
n (x:n)

< +∞
(46)
Then, for any n ≫0, Γ SF
n (x:n) = Sn (x:n).
Proof. Choose n0 ∈N s.t.
∞
X
n=n0

max
x:nOω Γ S
F
n (x:n) −min
x:nOω Γ S
F
n (x:n)

< 1
2
Since Γ S
F
n (x:n) , V Γ S
F
n (x:n) ∈C(Oω) diﬀer by a constant function, we have
∞
X
n=n0

max
x:nOω V Γ S
F
n (x:n) −min
x:nOω V Γ S
F
n (x:n)

< 1
2
In particular, for any n ≥n0
18

n−1
X
m=n0

max
x:nOω V Γ S
F
m (x:m) −min
x:nOω V Γ S
F
m (x:m)

< 1
2
On the other hand, it is easy to see that there is n1 ≥n0 s.t. for any n ≥n1
max
x:nOω ΣV Γ SF
n0
 x:n0−1

−min
x:nOω ΣV Γ SF
n0
 x:n0−1

< 1
2
Taking the sum of the last two inequalities, we conclude that for any n ≥n1
ΣVmax Γ SF
n (x:n−1) −ΣVmin Γ SF
n (x:n−1) < 1
Using the deﬁnition of Γ S, we get the desired result.
Proof of Theorem B.1. We regard Oω as a probability space using the σ-algebra F of universally
measurable sets and the probability measure µ∗. For any n ∈N, we deﬁne Fn ⊆F and Xn, X′
n, Yn :
Oω →R by
Fn := {AOω | A ⊆On universally measurable}
Xn (x) := ΣV Γ SF
n (x:n−1, x)
X′
n (x) := ΣVmin Γ SF
n (x:n−1)
Yn (x) := max
x:nOω Γ S
F
n (x:n) −min
x:nOω Γ S
F
n (x:n)
Clearly, F is a ﬁltration of Oω, X, X′, Y are stochastic processes and X′, Y are adapted to F.
S is uniformly bounded, therefore Γ S is uniformly bounded and so is Y . Obviously, Y is also
non-negative.
By Proposition B.1, |Xn −X′
n| are uniformly bounded. X0 vanishes and in particular E

|X0|

< ∞.
We have
|Xn+1 (x) −Xn (x)| = |V Γ S
F
n (x:n, x)| ≤Yn (x)
Moreover:
E

Xn+1 −Xn | Fn

(x) = Ex′∼µ∗
h
V Γ S
F
n
 x:n, x′
| x′ ∈x:nOωi
(As before, the notation on the right hand side signiﬁes the use of a regular conditional proba-
bility)
E

Xn+1 −Xn | Fn

(x) = Ex′∼µ∗
h
Γ S
F
n
 x:n, x′
| x′ ∈x:nOωi
−Ex′∼Fn(x:n)
h
Γ S
F
n
 x:n, x′i
Let α > 0 be as in Deﬁnition B.2. By deﬁnition of Γ S, Γ S
F
n (x:n) is equal to either Sn
 x:n; Fn (x:n)

or 0. In either case, we get that for µ∗-almost any x ∈Oω
E

Xn+1 −Xn | Fn

(x) ≥α

max
x:nOω Γ S
F
n (x:n) −min
x:nOω Γ S
F
n (x:n)

= αYn (x)
19

By Corollary B.1a, for µ∗-almost any x ∈Oω
inf
n∈N ΣVmin Γ SF
n (x:n−1) > −∞
Since F dominates Γ S, it follows that
sup
n∈N
ΣVmax Γ SF
n (x:n−1) < +∞
By Corollary B.1b, P
n Yn (x) < ∞.
By Proposition B.2, it follows that for any n ≫0,
Γ SF
n (x:n) = Sn (x:n). We get, for n ≫0
Eµ∗
h
Sn
 x:n; Fn (x:n)

| x:nOωi
−EFn(x:n)
h
Sn
 x:n; Fn (x:n)
i
=
Eµ∗
h
Γ S
F
n (x:n) | x:nOωi
−EFn(x:n)
h
Γ S
F
n (x:n)
i
≤Yn (x)
∞
X
n=0

Eµ∗
h
Sn
 x:n; Fn (x:n)

| x:nOωi
−EFn(x:n)
h
Sn
 x:n; Fn (x:n)
i
< +∞
C
Appendix: Proof of Main Theorem
In order to prove Theorem 1, we will need, given an incomplete model M ⊆P(Oω) and ǫ > 0, a
gambling strategy SMǫ s.t. SMǫ is µ-prudent for any µ ∈M and s.t. Theorem B.1 implies that
any F that dominates SMǫ satisﬁes equation 10 “within ǫ.” This motivates the following deﬁnition
and lemma. We remind that, given x ∈R, x+ := max(x, 0).
Deﬁnition C.1. Consider X a compact Polish metric space, M ⊆P (X) convex and r0 > 0.
β ∈B (X) is said to be (M, r0)-savvy when for any µ ∈P (X), denoting rµ := dKR (µ, M):
i. ∥β (µ)∥≤
 rµ −r0

+
ii. ∀ν ∈M : Eν

β (µ)

−Eµ

β (µ)

≥1
2
 rµ −r0

rµ
Lemma C.1. Consider X a compact Polish metric space, Y a compact Polish space and M : Y →
2P(X). Denote
graph M := {(y, µ) ∈Y × P (X) | µ ∈M (y)}
Assume that graph M is closed and that M (y) is convex for any y ∈Y . Regard Y as a measur-
able space using the σ-algebra of universally measurable sets and regard B (X) as a measurable space
using the σ-algebra of Borel sets. Then, for any ǫ > 0, there exists Sǫ : Y →B (X) measurable s.t.
for all y ∈Y , Sǫ (y) is
 M (y) , ǫ

-savvy.
In order to prove Lemma C.1, we will need a few other technical lemmas.
Lemma C.2. Consider X a compact Polish space, ρ a metrization of X, ϕ ∈Lip (X, ρ)′′ and
ǫ > 0. Then, there exists f ∈Lip (X, ρ) s.t.
i. ∥f∥ρ ≤∥ϕ∥
ii. ∀µ ∈P (X) : |Eµ [f] −ϕ (µ)| < ǫ
20

Proof. Without loss of generality, assume that ∥ϕ∥= 1 (if ∥ϕ∥= 0 the theorem is trivial, otherwise
we can rescale everything by a scalar). Using the compactness of P (X), choose a ﬁnite set A ⊆
P (X) s.t.
max
µ∈P(X) min
ν∈A dKR (µ, ν) < ǫ
4
Using the Goldstine theorem, choose f ∈Lip (X) s.t. ∥f∥ρ ≤1 and for any ν ∈A
|Eν [f] −ϕ (ν)| < ǫ
2
Consider any µ ∈P (X). Choose ν ∈A s.t. dKR (µ, ν) < ǫ
4. We get
|Eµ [f] −ϕ (µ)| ≤|Eµ [f] −Eν [f]| + |Eν [f] −ϕ (ν)| + |ϕ (ν) −ϕ (µ)| < ǫ
4 + ǫ
2 + ǫ
4 = ǫ
Lemma C.3. Consider X a compact Polish space, ρ a metrization of X, M ⊆P (X) convex
non-empty, r0 > 0 and µ0 ∈P (X) s.t. r := dKR (µ0, M) > r0. Then, there exists f ∈Lip (X, ρ)
s.t.
i. ∥f∥ρ < r −r0
ii. infν∈M Eν [f] −Eµ0 [f] > 1
2 (r −r0) r
Proof. Deﬁne Mr ⊆Lip (X, ρ)′ by
Mr := {µ ∈Lip (X, ρ)′ | inf
ν∈M∥µ −ν∥< r}
By the Hahn-Banach separation theorem, there is ϕ ∈Lip (X, ρ)′′ s.t. for all ν ∈Mr, ϕ (ν) >
ϕ (µ0). Multiplying by a scalar, we can make sure that ∥ϕ∥= 3
4 (r −r0). For any δ > 0, we can
choose ζ ∈Lip (X, ρ)′ s.t. ∥ζ∥< 1 and ϕ (ζ) > 3
4 (r −r0) −δ. For any ν ∈M, ν −rζ ∈Mr and
therefore
ϕ (ν −rζ) > ϕ (µ0)
ϕ (ν) > ϕ (µ0) + rϕ (ζ) > ϕ (µ0) + r
3
4 (r −r0) −δ

Taking δ to 0 we get ϕ (ν) ≥ϕ (µ0) + 3
4r (r −r0). Applying Lemma C.2 for ǫ =
1
16r (r −r0), we
get f ∈Lip (X, ρ) s.t.
∥f∥ρ ≤3
4 (r −r0) < r −r0
Eν [f] ≥ϕ (ν) −1
16r (r −r0) ≥ϕ (µ0) + 11
16r (r −r0) ≥Eµ0 [f] + 5
8r (r −r0) > Eµ0 [f] + 1
2r (r −r0)
21

Lemma C.4. Consider X a compact Polish space, ρ a metrization of X, M ⊆P (X) convex
non-empty and r0 > 0. Deﬁne U ⊆P (X) by
U := {µ ∈P (X) | dKR (µ, M) > r0}
Then, there exists β′ : U →Lip (X, ρ) continuous s.t. for all µ ∈U, denoting rµ := dKR (µ, M):
i. ∥β′ (µ)∥ρ < rµ −r0
ii. infν∈M Eν

β′ (µ)

−Eµ

β′ (µ)

> 1
2
 rµ −r0

rµ
Proof. Deﬁne B : U →2Lip(X,ρ) by
B (µ) := {f ∈Lip (X, ρ) | ∥f∥ρ < rµ −r0, inf
ν∈M Eν [f] −Eµ [f] > 1
2
 rµ −r0

rµ}
By Lemma C.3, for any µ ∈U, B (µ) ̸= ∅. Clearly, B (µ) is convex. Fix f ∈Lip (X, ρ) and
consider B−1 (f) := {µ ∈U | f ∈B (µ)}. Consider any µ0 ∈B−1 (f), and take ǫ > 0 s.t.
i. ∥f∥ρ < rµ0 −ǫ −r0
ii. infν∈M Eν [f] −Eµ0 [f] −ǫ > 1
2
 rµ0 + ǫ −r0
  rµ0 + ǫ

Deﬁne V ⊆U by
V := {µ ∈U | dKR (µ, µ0) < ǫ, Eµ [f] < Eµ0 [f] + ǫ}
Obviously V is open, µ0 ∈V and V ⊆B−1 (f). We got that µ0 has an open neighborhood inside
B−1(f), and since we could choose any µ0 ∈B−1(f), we showed that B−1 (f) is open. Applying
Theorem D.2 (see Appendix D), we get the desired result.
Corollary C.1. Consider X a compact Polish metric space, M ⊆P (X) convex and r0 > 0. Then,
there exists β ∈B (X) which is (M, r0)-savvy.
Proof. If M is empty, the claim is trivial, so assume M is non-empty. Let U ⊆P (X) be deﬁned
by
U := {µ ∈P (X) | dKR (µ, M) > r0}
Use Lemma C.4 to obtain β′ : U →Lip (X). Deﬁne β : P (X) →Lip (X) by
β (µ) :=
(
β′ (µ) if µ ∈U
0 if µ ̸∈U
Obviously β is continuous in U. Consider any µ ̸∈U and {µk ∈P (X)}k∈N s.t. limk→∞µk = µ.
We have
lim
k→∞dKR (µk, M) = dKR (µ, M) ≤r0
Denote the metric on X by ρ.
lim sup
k→∞
∥β (µk)∥ρ ≤lim sup
k→∞
 dKR (µk, M) −r0

+ = 0
Therefore, β is continuous everywhere.
22

Proof of Lemma C.1. Deﬁne Z1,2,3 ⊆Y × B (X) × P (X)4 by
Z1 := {(y, β, µ, ν, ξ, ζ) ∈Y × B (X) × P (X)4 | ζ ∈M (y)}
Z2 := {(y, β, µ, ν, ξ, ζ) ∈Y × B (X) × P (X)4 | ∥β (µ)∥≤
 dKR (µ, ξ) −ǫ

+}
Z3 := {(y, β, µ, ν, ξ, ζ) ∈Y ×B (X)×P (X)4 | Eν

β (µ)

−Eµ

β (µ)

≥1
2
 dKR (µ, ζ) −ǫ

+ dKR (µ, ζ)}
Deﬁne Z := Z1 ∩Z2 ∩Z3. It is easy to see that Z1,2,3 are closed and therefore Z also. Deﬁne
Z′ ⊆Y × B (X) × P (X)3 by
Z′ := {(y, β, µ, ν, ξ) ∈Y × B (X) × P (X)3 | ∃ζ ∈P (X) : (y, β, µ, ν, ξ, ζ) ∈Z}
Z′ is closed since it is the projection of Z and P (X) is compact. The M (y) are compact,
therefore dKR
 µ, M (y)

= dKR (µ, ζ) for some ζ ∈M (y), and hence (y, β, µ, ν, ξ) ∈Z′ if and only
if the following conditions hold:
i. ∥β (µ)∥≤
 dKR (µ, ξ) −ǫ

+
ii. Eν

β (µ)

−Eµ

β (µ)

≥1
2

dKR
 µ, M (y)

−ǫ

+ dKR
 µ, M (y)

Deﬁne W ⊆Y × B (X) × P (X)3 by
W := {(y, β, µ, ν, ξ) ∈Y × B (X) × P (X)3 | ν, ξ ∈M (y) , (y, β, µ, ν, ξ) ̸∈Z′}
W is locally closed and in particular it is an Fσ set. Deﬁne W ′ ⊆Y × B (X) by
W ′ := {(y, β) ∈Y × B (X) | ∃µ, ν, ξ ∈P (X) : (y, β, µ, ν, ξ) ∈W}
W ′ is the projection of W and P (X)3 is compact, therefore W ′ is Fσ. Let A ⊆Y × B (X) be
the complement of W ′. A is Gδ and in particular Borel. As easy to see, (y, β) ∈A if and only if β
is
 M (y) , ǫ

-savvy.
For any y ∈Y , Ay := {β ∈B (X) | (y, β) ∈A} is closed since
Ay =
\
µ∈P(X)
\
ν,ξ∈M(y)
{β ∈B (X) | (y, β, µ, ν, ξ) ∈Z′}
Moreover, Ay is non-empty by Corollary C.1.
Consider any U ⊆B (X) open. Then, A ∩(Y × U) is Borel and therefore its image under the
projection to Y is analytic and in particular universally measurable. Applying the Kuratowski–
Ryll-Nardzewski measurable selection theorem, we obtain a measurable selection of the multivalued
mapping with graph A. This selection is the desired Sǫ.
We now obtain a relation between the notions of savviness and prudence. We remind that
given X, Y Polish, π : X →Y Borel measurable and µ ∈P (X), µ | π : Y
k−→X stands for any
corresponding regular conditional probability.
Proposition C.1. Consider M ⊆P(Oω), {Mn : On →2P(Oω)}n∈N and ǫ > 0. Assume Mn is a
regular upper bound for M | πO
n . Let S be a gambling strategy s.t. for each n ∈N and y ∈On,
Sn (y) is
 Mn(y), ǫ

-savvy (relatively to ρn). Then, S is µ∗-prudent for any µ∗∈M.
23

Proof. Fix any µ∗∈M. By Deﬁnition 2, for πO
n∗µ-almost any y ∈On,

µ∗| πO
n

(y) ∈Mn(y).
Since Sn (y) is
 Mn(y), ǫ

-savvy, for any µ ∈P (X), denoting rµy := dn
KR
 µ, Mn(y)

:
Eµ∗
Sn (y; µ) | yOω
−Eµ

Sn (y; µ)

≥1
2
 rµy −ǫ

+ rµy ≥1
2∥Sn (y; µ)∥rµy
When rµy ≤ǫ, Sn (y; µ) = 0, hence for any µ ∈P (X), ∥Sn (y; µ)∥rµy ≥∥Sn (y; µ)∥ǫ. We get
Eµ∗
Sn (y; µ) | yOω
−Eµ

Sn (y; µ)

≥ǫ
2∥Sn (y; µ)∥
Corollary C.2. Consider M ⊆P(Oω), {Mn : On →2P(Oω)}n∈N and ǫ > 0. Assume Mn is a
regular upper bound for M | πO
n . Let S be a gambling strategy s.t. for each n ∈N and y ∈On,
Sn (y) is
 Mn(y), ǫ

-savvy. Let F be a forecaster that dominates Γ S. Then, for any µ ∈M and
µ-almost any x ∈Oω
lim sup
n→∞
dn
KR
 Fn (x:n) , Mn (x:n)

≤ǫ
(47)
Proof. By Proposition C.1, S is µ-prudent. By Theorem B.1, for µ-almost any x ∈X
lim
n→∞

Eµ
h
Sn
 x:n; Fn (x:n)

| x:nOωi
−EFn(x:n)
h
Sn
 x:n; Fn (x:n)
i
= 0
On the other hand,

µ | πO
n

(x:n) ∈Mn (x:n) and hence, since Sn (x:n) is
 Mn (x:n) , ǫ

-savvy,
denoting rn := dn
KR
 Fn (x:n) , Mn (x:n)

:
Eµ
h
Sn
 x:n; Fn (x:n)

| x:nOωi
−EFn(x:n)
h
Sn
 x:n; Fn (x:n)
i
≥1
2 (rn −ǫ) rn
We get
lim sup
n→∞(rn −ǫ) rn ≤0
lim sup
n→∞
rn ≤ǫ
We are ﬁnally ready to complete the proof of the main theorem.
Proof of Theorem 1. Consider any M ∈H and a positive integer k. By Lemma C.1, for any n ∈N,
there exists SMk
n
: On →B(Oω) measurable s.t. for all y ∈On, SMk
n
(y) is

Mn(y), 1
k

-savvy. We
ﬁx such a SMk
n
for each M and k. It is easy to see that the dn
KR-diameter of P(Oω) is at most
2, therefore ∥SMk
n
(y)∥≤2 and {SMk
n
}n∈N is a gambling strategy. By Theorem A.1, there is a
forecaster F H that dominates Γ SMk for all M and k. By Corollary C.2, for any M ∈H, µ ∈M,
positive integer k and µ-almost any x ∈X
lim sup
n→∞dn
KR

F H
n (x:n) , Mn (x:n)

≤1
k
It follows that
24

lim
n→∞dn
KR

F H
n (x:n) , Mn (x:n)

= 0
D
Appendix: Some Useful Theorems
The following variant of the Optional Stopping Theorem appears in [8] as Theorem 5.4.7.
Theorem D.1. Let Ωbe a probability space, {Fn ⊆2Ω}n∈N a ﬁltration of Ω, {Xn : Ω→R}n∈N a
stochastic process adapted to F and M, N : Ω→N ⊔{∞} stopping times (w.r.t. F) s.t. M ≤N.
Assume that {Xmin(n,N)}n∈N is a uniformly integrable submartingale.
Using Doob’s martingale
convergence theorem, we can deﬁne XN : Ω→R by6
XN (x) := lim
n→∞Xmin(n,N) (x) =
(
XN(x) (x) if N (x) < ∞
limn→∞Xn (x) if N (x) = ∞
(48)
We deﬁne XM : Ω→R is an analogous way7. We also deﬁne the σ-algebra FM ⊆2Ωby
FM := {A ⊆X measurable | ∀n ∈N : A ∩M−1 (n) ∈Fn}
(49)
Then:
E

XN | FM

≥XM
(50)
The following variant of the Michael selection theorem appears in [9] as Theorem 3.1.
Theorem D.2 (Yannelis and Prabhakar). Consider X a paracompact Hausdorﬀtopological space
and Y a topological vector space. Suppose B : X →2Y is s.t.
i. For each x ∈X, B (x) ̸= ∅.
ii. For each x ∈X, B (x) is convex.
iii. For each y ∈Y , {x ∈X | y ∈B (x)} is open.
Then, there exists β : X →Y continuous s.t. for all x ∈X, β (x) ∈B (x).
E
Appendix: Examples of Regular Upper Bounds
Proof of Example 2. In this case, condition i means that N must be pointwise closed and condi-
tion iii means that for every µ ∈M and y ∈Y s.t. µ
 π−1(y)

> 0, µ | π−1(y) ∈N(y). Since all
three conditions are closed w.r.t. arbitrary set intersections, there is a unique minimum (it is the
intersection of all multivalued mappings satisfying the conditions). It remains to show that N(y)
deﬁned by equation 6 is convex. Fix y ∈Y and denote A the set appearing on the right hand side
of equation 6 under the closure. Consider any µ1, µ2 ∈M s.t. µ1
 π−1(y)

, µ2
 π−1(y)

> 0 and
p ∈(0, 1). We have
6The limit in equation 48 is converges almost surely, which is suﬃcient for our purpose.
7This time we can’t use Doob’s martingale convergence theorem, but whenever M (x) = ∞we also have N (x) = ∞,
therefore the limit still almost surely converges.
25

pµ1 + (1 −p)µ2 ∈M
 pµ1 + (1 −p)µ2

| π−1(y) ∈A
µ1
 π−1(y)

p
 µ1 | π−1(y)

+ µ2
 π−1(y)

(1 −p)
 µ2 | π−1(y)

µ1
 π−1(y)

p + µ2
 π−1(y)

(1 −p)
∈A
It is easy to see that for any q ∈(0, 1) there is p ∈(0, 1) s.t.
q =
µ1
 π−1(y)

p
µ1
 π−1(y)

p + µ2
 π−1(y)

(1 −p)
It follows that for any q ∈(0, 1)
q

µ1 | π−1(y)

+ (1 −q)

µ2 | π−1(y)

∈A
We got that A is convex and therefore N(y) = A is also convex.
Proposition E.1. Consider W1,2,3,4 Polish spaces, X := Q4
i=1 Wi, Y := Q3
i=1 Wi and Z = W1 ×
W2. Let πY : X →Y , πZ : X →Z and πW : X →W1 be the projection mappings, µ ∈P(X) and
K : Z
k−→W3. Assume that πY
∗µ = πZ
∗µ ⋉K. Then, for πW
∗µ-almost any w ∈W1
πY
∗

µ | πW
(w) = πZ
∗

µ | πW 
(w) ⋉K
(51)
Proof. We know that µ =

µ | πW
∗πW
∗µ. It follows that
πY
∗µ = πY
∗

µ | πW 
∗πW
∗µ = πZ
∗

µ | πW
∗πW
∗µ ⋉K
Deﬁne K′ : Z
k−→Y by K′(z) := δz × K(z) and denote composition of Markov kernels by ◦.
Since pushforward commutes with composition, we have
πY
∗µ =

πY ◦µ | πW 
∗πW
∗µ =

K′ ◦πZ ◦µ | πW 
∗πW
∗µ
Let πY W : Y →W be the projection mapping and denote ν = πY
∗µ. We get
ν =

πY ◦µ | πW
∗πY W
∗
ν =

K′ ◦πZ ◦µ | πW 
∗πY W
∗
ν
We also know that
supp πY W
∗
ν ⋉µ | πW = supp πW
∗µ ⋉µ | πW ⊆graph πW
As easy to see, the above implies
supp πY W
∗
ν ⋉

πY ◦µ | πW
⊆graph πY W
supp πY W
∗
ν ⋉

K′ ◦πZ ◦µ | πW 
⊆graph πY W
26

By the uniqueness of regular conditional probability, we conclude that for πW
∗µ-almost any
w ∈W1

ν | πY W
(w) =

πY ◦µ | πW
(w) =

K′ ◦πZ ◦µ | πW 
(w)
This immediately implies the desired result.
Proof of Example 3. Fix ρ a metrization of Oω. The condition suppµ ⊆yOω is closed because it is
equivalent to the vanishing of the continuous function Ex∼µ

ρ (x, yOω)

. The other condition in the
deﬁnition of MK
n is closed because pushforward by a continuous mapping and semidirect product
with a Feller continuous kernel are continuous in the weak topology. This gives us condition i.
Condition ii holds since the convex combination of measures supported on yOω is supported on
yOω and since pushforward and semidirect product commute with convex combinations.
Consider any µ ∈MK.
We know that for any m ≥n, πO
m+1∗µ = πO
m∗µ ⋉Km.
Applying
Proposition E.1, this implies that for πO
n∗µ-almost any y ∈On
πO
m+1∗

µ | πO
n

(y) = πO
m∗

µ | πO
n

(y) ⋉Km
We conclude that

µ | πO
n

(y) ∈MK
n (y), proving condition iii.
Proposition E.2. Consider X a compact Polish space, d ∈N, Y a compact subset of Rd, π : X →
Y continuous and µ ∈P (X). Then, for π∗µ-almost any y ∈Y
lim
r→0 µ | π−1  Br (y)

=
 µ | π

(y)
(52)
Proof. Let {fk ∈C (X)}k∈N be dense in C (X). For any y ∈Y and r > 0 we denote χyr : Y →{0, 1}
the characteristic function of Br (y) and χπ
yr : X →{0, 1} the characteristic function of π−1  Br (y)

.
For any k ∈N and y0 ∈supp π∗µ we have µ

π−1  Br (y0)

= π∗µ
 Br (y0)

> 0 and
Eµ
h
fk | π−1  Br (y0)
i
=
Eµ
h
χπ
y0rfk
i
µ

π−1  Br (y0)
 =
Ey∼π∗µ

Eµ
h
χπ
y0rfk | π−1 (y)
i
π∗µ
 Br (y0)

Eµ
h
fk | π−1  Br (y0)
i
=
Ey∼π∗µ
h
χy0r Eµ

fk | π−1 (y)
i
π∗µ
 Br (y0)

In particular, the above holds for π∗µ-almost any y0 ∈Y . Applying the Lebesgue diﬀerentiation
theorem, we conclude that there is A ⊆Y s.t. π∗µ (A) = 1 and for any y ∈A
∀k ∈N : lim
r→0 Eµ
h
fk | π−1  Br (y)
i
= Eµ
h
fk | π−1 (y)
i
Now consider any f ∈C (X). For any ǫ > 0, there is k ∈N s.t. ∥f −fk∥< ǫ and therefore, for
any y ∈A
lim sup
r→0
Eµ
h
f | π−1  Br (y)
i
≤lim sup
r→0
Eµ
h
fk | π−1  Br (y)
i
+ ǫ = Eµ
h
fk | π−1 (y)
i
+ ǫ
lim sup
r→0
Eµ
h
f | π−1  Br (y)
i
≤Eµ
h
f | π−1 (y)
i
+ 2ǫ
27

Similarly
lim inf
r→0 Eµ
h
f | π−1  Br (y)
i
≥Eµ
h
f | π−1 (y)
i
−2ǫ
Taking ǫ to 0, we conclude that
lim
r→0 Eµ
h
f | π−1  Br (y)
i
= Eµ
h
f | π−1 (y)
i
lim
r→0 µ | π−1  Br (y)

=
 µ | π

(y)
Proof of Example 4. There is a unique minimum since all three deﬁning properties of N are closed
w.r.t. arbitrary set intersections. To see N is a regular upper bound, note that conditions i and ii
are part of the deﬁnition whereas condition iii follows immediately from Proposition E.2.
Acknowledgments
This work was supported by the Machine Intelligence Research Institute in Berkeley, California.
We wish to thank Michael Greinecker for pointing out Theorem D.2 to us.
References
[1] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University
Press, New York, NY, USA, 2006.
[2] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, New York, NY, USA, 2014.
[3] Marcus Hutter. Open problems in universal induction & intelligence. Algorithms, 2(3):879–906,
2009.
[4] David Blackwell and Lester Dubins. Merging of opinions with increasing information. Ann.
Math. Statist., 33(3):882–886, 09 1962.
[5] Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, and Jessica Taylor.
A
formal approach to the problem of logical non-omniscience. In Proceedings Sixteenth Conference
on Theoretical Aspects of Rationality and Knowledge, 2017.
[6] Vladimir Vovk, Akimichi Takemura, and Glenn Shafer.
Defensive forecasting.
pages 365–
372. Society for Artiﬁcial Intelligence and Statistics, 2005.
(Available electronically at
http://www.gatsby.ucl.ac.uk/aistats/).
[7] Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. Using and com-
bining predictors that specialize. In Proceedings of the Twenty-ninth Annual ACM Symposium
on Theory of Computing, STOC ’97, pages 334–343, New York, NY, USA, 1997. ACM.
[8] Rick Durrett. Probability: Theory and Examples. Cambridge University Press, New York, NY,
USA, 4th edition, 2010.
28

[9] Nicholas C. Yannelis and N.D. Prabhakar. Existence of maximal elements and equilibria in
linear topological spaces. Journal of Mathematical Economics, 12(3):233 – 245, 1983.
29

