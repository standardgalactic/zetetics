Neurons as hierarchies of quantum reference
frames
Chris Fieldsaâˆ—, James F. Glazebrookb,c and Michael Levind
a 23 Rue des Lavandi`eres, 11160 Caunes Minervois, FRANCE
ORCID: 0000-0002-4812-0744
b Department of Mathematics and Computer Science,
Eastern Illinois University, Charleston, IL 61920 USA
c Adjunct Faculty, Department of Mathematics,
University of Illinois at Urbana-Champaign, Urbana, IL 61801 USA
d Allen Discovery Center at Tufts University, Medford, MA 02155 USA
ORCID: 0000-0001-7292-8084
January 5, 2022
Abstract
Conceptual and mathematical models of neurons have lagged behind empirical understand-
ing for decades. Here we extend previous work in modeling biological systems with fully
scale-independent quantum information-theoretic tools to develop a uniform, scalable rep-
resentation of synapses, dendritic and axonal processes, neurons, and local networks of
neurons. In this representation, hierarchies of quantum reference frames act as hierarchical
active-inference systems. The resulting model enables speciï¬c predictions of correlations
between synaptic activity, dendritic remodeling, and trophic reward. We summarize how
the model may be generalized to nonneural cells and tissues in developmental and regener-
ative contexts.
Keywords
Activity-dependent remodeling; Bayesian inference; Bioelectricity; Computation; Learning;
Memory
âˆ—Corresponding author at: 23 Rue des Lavandi`eres, 11160 Caunes Minervois, FRANCE; E-mail address:
ï¬eldsres@gmail.com
1
arXiv:2201.00921v1  [q-bio.NC]  4 Jan 2022

1
Introduction
Neurons are canonical biological information processors.
Theoretical and, particularly,
conceptual models of neural information processing, however, lag increasingly far behind our
developing empirical understanding of neurons as electrically-excitable cells. Experimental
work over the past two decades has, for example, ï¬rmly established that dendrites undergo
activity-dependent remodeling [1, 2, 3], particularly alterations of spine location, density,
and function [4], even in adult humans. This ontogenic process is functionally analogous
to the evolution of structural and positional diversity of dendrites as they have adapted to
a spectrum of functional roles [5], e.g. the implementation of deep learning via synaptic
plasticity [6, 7]. Neurons are not, therefore, static structures, but rather can be regarded
as undergoing continuous development throughout the life cycle. This dynamic process has
signiï¬cant consequences for both neuron-level and organism-level function. For example, in
organisms (such as caterpillars transitioning into butterï¬‚ies or moths) that exhibit drastic
remodeling and reconstruction of their brains, some of their learned memories remain and
survive the process [8]. In other contexts, memories can be imprinted on newly regenerating
brains from other tissues [9, 10], underscoring the plasticity of large-scale neural structure
and of the information stored within it. These eï¬€ects of remodeling are not just an issue for
so-called lower animals, as applications in regenerative medicine are likely to soon produce
human patients in whom some portion of the brain has been replaced by the progeny of
naÂ¨Ä±ve stem cells to treat degenerative disease or brain damage.
It has also been shown that subnetworks of dendrites can compute local logical operations
including exclusive-or (XOR) [11]. Nonetheless, the idea that dendritic trees eï¬€ectively com-
pute weighted sums, dating back to [12], continues not only to dominate artiï¬cial neural
network (ANN) development, but also to guide biological thinking. Explicit cable-theory
modeling can reproduce time-dependent signal propagation and processing in simpliï¬ed
representations of dendritic trees [13], but rapidly becomes intractable with increasing geo-
metric complexity. Increasingly sophisticated hardware models of neurons allow exploration
of phase coding, frequency modulation, and other forms of hybrid analog-digital computing
[14, 15], but are not readily mapped to biological neural networks and are not standard
components of a theoretical neuroscientistâ€™s toolkit.
Conceptualizing neurons as biological wires â€“ even functionally sophisticated wires â€“ pro-
vides, moreover, no insight into the fundamental question of cognitive neuroscience: the
question of how information present in the external world is encoded, via some combination
of evolution, development, remodeling, and learning, into the functional architecture of a
nervous system. Brains are not constructed, as ANNs are, in the absense of functional
activity, nor do they â€œbegin learningâ€ from a default initial state of uniform connectivity.
Brains are not â€œwired upâ€ into some starting conï¬guration, after which they are turned on
and exposed to the world. Brains instead develop from neural primordia that are already
functionally complex multicellular microenvironments. The function of the nervous system,
from its earliest phylogenetic and ontogenetic role in directing morphogenesis [16] to adult
cognition, depends on the ability of individual neurons to negotiate this microenvironment,
2

both structurally and functionally. The local microenvironment, with its diverse cell types,
biochemical and (micro)anatomical complexity, and network of biomolecular and bioelectric
signaling, is the â€œworldâ€ with which each individual neuron interacts, and is each neuronâ€™s
sole source of nongenomic information. There are many interesting scenarios supporting
this viewpoint; some are of the â€˜socializationâ€™ and decision making type.
For instance,
assortative neural networks demonstrate collective resilience, whereby nodes of a certain
degree have an aï¬ƒnity to team up with those of the same topological type, and thus often
contribute to formation of a small world network structure [17, 18]. Neural network model-
ing of independent tasks in the prefrontal cortex in [19], for example, reveals how seemingly
separate neuronal groups engaged in their respective tasks may sometimes be drawn into a
coalition, casting votes for a stimulus, and then enacting a committee-like decision.
In line with the Free Energy Principle (FEP) and the idea of Bayesian active inference
[20, 21, 22, 23, 24], individual neurons can be considered cognitive agents that minimize
aggregate uncertainty about their future states (variational free energy or VFE) by actively
exploring and developing predictive models of their local microenvironments. It is the joint
activity of hundreds (in C. elegans [25]) to tens of billions (in primates [26]) of neurons within
this jointly-constructed environment that encodes information sourced from the external
world. Hence an adequate conceptual model of neurons as biological systems must explain
how each neuronâ€™s construction of a predictive model of its local microenvironment results
in the joint encoding of a predictive model of the external environment at the scale of the
entire brain. The utility of FEP based models of cell sorting in morphogenesis [27, 28],
cortical minicolumn and local-circuit organization [29, 30, 31, 32, 33], and network and
whole-brain function [21, 34, 35, 36, 37] suggests that a scale-free conceptual framework is
the right way to approach this question of functional coherence between microenvironment
models constructed by individual neurons and external environment models constructed by
brains. The FEP is, however, fundamentally a statement about error minimization; it is an
implementation-independent principle of thermodynamics [24]. It does not tell us anything
speciï¬c about neuronal functional architecture, or about how neurons interact with either
each other or the non-neuronal components of their microenvironments.
Here we suggest that the idea of a quantum reference frame (QRF) developed within quan-
tum information theory [38, 39] is usefully applied to characterize neurons both architec-
turally and functionally.
While quantum information theory employs the formalism of
standard quantum theory, it makes no scale-dependent assumptions and is applicable from
sub-microscopic to cosmological scales. The formalism of QRFs, in particular, is applicable
at any scale. A QRF is a physical system that assigns units of measurement, and hence
an operational semantics, to each observational outcome, enabling outcomes obtained at
diï¬€erent times or places, or with diï¬€erent QRFs, to be compared. Macroscopic systems
such as meter sticks, clocks, gyroscopes, and the Earth with its gravitational and magnetic
ï¬elds are canonical examples of QRFs. While reference frames are typically thought of as
fully-speciï¬able abstractions in classical physics, this is not the case in quantum theory:
QRFs are physical systems that encode unmeasurable but functionally relevant quantum
phase information. Every QRF is, therefore, in an important sense unique. A QRF cannot,
3

in particular, be fully speciï¬ed either structurally or functionally by any ï¬nite bit string;
it is â€œnonfungibleâ€ in the terminology of [39]. Alice can, in this case, share a QRF with a
distant observer Bob only by physically transferring the QRF to Bob; sending any ï¬nite de-
scription is provably insuï¬ƒcient. Alice can, moreover, transfer her QRF to Bob successfully
only if Bob already possesses a functionally equivalent QRF [40]; Alice cannot, for example,
successfully communicate the meaning of â€œ1 meterâ€ to Bob unless Bob already has a QRF
that eï¬€ectively functions as a meter stick. From an information processing point of view,
a QRF is a quantum computer: it outputs an outcome value that is reproducible within
some ï¬xed resolution, and that is assigned standardized units that confer an operational
semantics, when given a â€œrawâ€ measurement, or simply a physical interaction, as input.
We propose, in particular, that neurons are usefully regarded as hierarchies of QRFs. Neu-
rons in this representation are hierarchical measurement devices that sample their sur-
rounding cellular microenvironments at multiple scales and encode scale-speciï¬c expecta-
tions about how those microenvironments will behave. The output of a particular neuron
encodes, for each cell that receives it, a speciï¬c, nonfungible representation of the trans-
mitting neuronâ€™s local measurements. These representations are encoded in the â€œunits of
measurementâ€ deï¬ned by the sending neuronâ€™s particular combination of QRFs. They im-
pose operational semantic constraints on all downstream processing. These constraints are
inherited by, and give a particular operational meaning to, the activity patterns of the
neural system as a whole.
We develop this proposal using a general formalism, that of Barwise-Seligman classiï¬ers
[41] organized into networks with well-deï¬ned limits and colimits (cone-cocone diagrams
or CCCDs) developed in [42] and applied to human cognitive architecture in [43, 44, 45].
Barwise-Seligman classiï¬ers are naturally interpreted as representing either logical or prob-
abilistic constraints; hence CCCDs represent maximal collections of mutually-consistent
constraints, and indeed generalize hierarchical Bayesian inference [45]. To model neurons
as hierarchies of QRFs is to represent them as 1) quantum computers that 2) implement
hierarchical Bayesian inferences determined by 3) their speciï¬c three-dimensional (3d) ge-
ometries and 4) the diï¬€erential signal-transduction velocities that their geometries impose.
This formalism extends naturally to networks of neurons, at any scale from a local circuit
â€“ e.g. a minicolumn â€“ up to an entire brain.
We begin in Â§2 by brieï¬‚y reviewing the implementation of QRFs by generic quantum sys-
tems [46, 47, 48] and their representations by CCCDs. We then turn to biological systems,
reviewing the representation of simple homeostatic setpoints as QRFs [49, 50] and extending
this treatment to perisynaptic processes in neurons in Â§3. This allows us in Â§4 to represent
the hierarchical structures of dendrites as hierarchical QRFs that detect the states â€“ activ-
ity patterns â€“ of particular â€œobjectsâ€ â€“ spatially-organized aggregates of presynaptic axon
terminals â€“ in their microenvironments. We then extend this representation to neurons
in Â§5, showing how the integration of signals from multiple dendritic branches implements
an eï¬€ectively tomographic computation. This view of neurons as state identiï¬ers scales
upwards to functional networks, the global activity of which provide coarse-grained repre-
sentations of particular components of the external environment. We then brieï¬‚y review
4

in Â§6 results from a variety of systems showing that with suitable spatial and temporal
scaling, this model applies to electrically-excitable cells generally. We consider simulation
modeling and experimental approaches suggested by this framework in Â§7.
2
QRFs and their representation by CCCDs
2.1
QRFs as generic representations of measurement
Information processing in biological systems has traditionally been represented as classi-
cal; despite theoretical arguments from a variety of perspectives [51, 52, 53, 54, 55] and
experimental evidence for the functional relevance of quantum coherence in photoreception
and magnetoreception [56, 57, 58], quantum biology remains in its infancy [59, 60, 61, 62].
Motivated in part by recent analyses showing that cellular bioenergetic resources fall orders
of magnitude short of those needed for fully-classical computation at macromolecular scales
[63], here we adopt a quantum-theoretic perspective from the start. This perspective allows
full use of the quantum information toolkit, including the concept of a QRF; it oï¬€ers the
advantages of full generality and applicability at any scale [50].
Consider an isolated, ï¬nite, bipartite system AB for which the interaction HAB is weak
enough and the time interval of interest short enough that considering AB to be separable,
i.e. having a joint state |ABâŸ©that factors as |ABâŸ©= |AâŸ©|BâŸ©, is a good approximation. In
this case, it is always possible to choose a basis in which the interaction can be written:
HAB = Î²kkBT k
N
X
i
Î±k
i M k
i ,
(1)
where k =
A or B, the M k
i are N Hermitian operators with eigenvalues in {âˆ’1, 1},
the Î±k
i âˆˆ[0, 1] are such that PN
i Î±k
i = 1, and Î²k â‰¥ln 2 is an inverse measure of kâ€™s
thermodynamic eï¬ƒciency that depends on the internal dynamics Hk [40, 46, 49]. In the
form given by Eq.
(1), the interaction can, without loss of generality, be regarded as
deï¬ned at a holographic screen B separating A from B; the operators M k
i can in this
case be regarded as â€œpreparationâ€ and â€œmeasurementâ€ operators that alternately write and
read bit values encoded on B [46, 64]. The screen B can be realized as an ancillary qubit
array as in Fig. 1. The thermodynamic factor Î²kkBT k in Eq. (1) assures compliance with
Landauerâ€™s Principle [65, 66, 67], i.e. assures that the per-bit free-energy cost of classical
bit erasure is paid on each cycle (see [44, 50] for discussion).
5

Figure 1: A holographic screen B separating systems A and B with an interaction HAB
given by Eq. (1) can be realized by an ancillary array of noninteracting qubits that are
alternately prepared by A (B) and then measured by B (A). There is no requirement that
A and B share preparation and measurement bases, i.e. QRFs. Adapted from [45] Fig. 1,
CC-BY license.
Now consider A to be an â€œobserverâ€ that decomposes B into a â€œsystem of interestâ€ S and its
surrounding â€œenvironmentâ€ E. As a familiar example, S may be some item of laboratory
apparatus and E the surrounding laboratory.
We suppose further that A is primarily
interested in the state |PâŸ©of some proper component P of S, conventionally called the
â€œpointerâ€ of S, that is separable from the remaining proper component R (the â€œreferenceâ€
component), i.e. S = PR and |PRâŸ©= |PâŸ©|RâŸ©over the observation times of interest. Under
these conditions, repeated observations of a ï¬xed state |RâŸ©of R allow the identiï¬cation of S
as a system; in our example, repeated observations of the ï¬xed size, shape, location, etc. of
an item of laboratory apparatus allow its identiï¬cation over time and hence unambiguous
observations of its pointer state [47, 48]. These repeated observations eï¬€ect decoherence
of S, and hence enforce separability of S from E [46]. Nothing in the above changes when
pure states |PâŸ©and |RâŸ©are replaced by densities ÏP and ÏR, interpreted as distributions
over time series of pure states, with the separability condition ÏPR = ÏPÏR.
Assigning mutually-exclusive subsets of bits encoded on B to P, R, and E is clearly a
computational process that must be implemented by the internal interaction HA of A.
Indeed this process is completely independent of HB or, equivalently, of the decompositional
6

structure of B [47]. Tracking the states of P, R, and E over time requires an adequate
memory resource and a comparison function capable of detecting changes at some suitably
coarse-grained resolution. As shown in [47, 48], these computations can be regarded as
implementing QRFs for P, R, and E, the functions of which can, without loss of generality,
be speciï¬ed by networks (formally, cocones) of Barwise-Seligman classiï¬ers as described in
Â§2.3 below.
It is worth emphasizing here that the decomposition B = PRE is not â€œobjectiveâ€ in any
sense; the components P, R, and E are deï¬ned, relative to and hence â€œforâ€ A, by their re-
spective QRFs. These QRFs can, therefore, themselves be labeled â€˜Pâ€™, â€˜Râ€™, and â€˜Eâ€™ without
ambiguity. Similarly, the states |PâŸ©, |RâŸ©, and |EâŸ©encoded on B in the basis speciï¬ed by Eq.
(1) are not â€œobjectiveâ€ but rather deï¬ned as the domains, respectively, of the QRFs P, R,
and E. They are, therefore, also strictly relative to A (cf. the characterization of quantum
states as personal in [68, 69] or as observer-relative in [70]). The computations implemented
by P, R, and E result in classical, i.e. irreversible encodings of state information to Aâ€™s
memory; therefore they require free energy as discussed in [47]. This free energy can only
come from B; hence some fraction of the bits encoded on B must be viewed as supplying
the free energy required for computation. The values of these bits cannot, therefore, aï¬€ect
the computational outcomes of P, R, and E; they are eï¬€ectively traced over. This obligate
trace operation renders the outputs of P, R, and E coarse-grained representations ÏP, ÏR,
and ÏE that can be viewed as probability distributions, and eï¬€ectively as weighted aver-
ages, in the basis speciï¬ed by Eq. (1), or as pure states in a â€œcomputational basisâ€ with
reduced dimension. The dimension reduction or coarse-graining induced by the action of
any QRF, solely in consequence of its free-energy requirements, enables the construction of
QRF hierarchies with distinguishable layers as discussed in the case of neurons in Â§4 and
Â§5 below.
2.2
Commutativity constraints on QRFs
A system S is only useful, in practice, as a measurement apparatus if it can be identiï¬ed over
macroscopic time, i.e. has a stable coarse-grained reference state ÏR. â€œStabilityâ€ requires,
in particular, that observing the environment E cannot aï¬€ect ÏR; hence the QRFs R and
E must commute. The reference state ÏR that allows identiï¬cation of S must, moreover,
remain stable when the pointer component P is observed; hence R and P must commute.
Similarly P must commute with E. These commutativity conditions are, eï¬€ectively, con-
sequences of separability: they follow directly from the assumption that P, R, and E are
each distinguishable from the others.
These commutativity conditions on the QRFs P, R, and E can be summarized in diagram-
7

matic form by:
ÏP
MP
/ Ïâ€²
P
ÏR
MR
/ Ïâ€²
R
ÏE
ME
/ Ïâ€²
E
|BâŸ©
P
O
PU / |BâŸ©â€²
P
O
|BâŸ©
R
O
PU / |BâŸ©â€²
R
O
|BâŸ©
E
O
PU / |BâŸ©â€²
E
O
(2)
where |BâŸ©is the bit string encoded on B by the action of HAB, PU is the time propagator
of the joint system U = AB, and MP, MR, and ME are Markov kernels on the state
spaces of ÏP, ÏR, and ÏE, respectively. The boundary B functions as a Markov Blanket
(MB) [71, 72] that renders the system state |AâŸ©conditionally independent of |BâŸ©at the
â€œmicroscaleâ€ deï¬ned by HAB [73], while ÏP, ÏR, and ÏE are eï¬€ective MB states at the coarse-
grained â€œcomputational scaleâ€ at which A writes observed state information to memory.
The probabilities encoded by the MP, MR, and ME are posterior probabilities for A from
a Bayesian perspective; the task of A as an observer implementing active inference [20, 21,
22, 23, 24] is to learn priors, or equivalently, to implement actions on B, that predict MP,
MR, and ME as accurately as possible.
The operators MP, MR, and ME are, clearly, Markovian if but only if the joint density
ÏPRE factors as ÏPRE = ÏPÏRÏE, i.e.
if but only if P, R, and E can be assumed to
be separable.
As noted above, separability can be assumed only in the limit of weak
interactions and/or short observation times.
Hence Markovian evolution of P, R, and
E is an assumption valid in this limiting case, not a fact.
Evolution being Markovian
corresponds, by deï¬nition, to probabilities satisfying the Kolmogorov axioms and hence
to the Bell [74] and Leggett-Garg [75] inequalities being satisï¬ed, not violated.
Hence
physically, the Markovian evolution corresponds to phase correlations being negligible, i.e.
the absence of quantum entanglement or changes of â€œintrinsicâ€ measurement context [76,
77, 78] as discussed further below. These conditions can be summarized as observations
being â€œsuï¬ƒciently coarse-grainedâ€ at the computational scale to be treated as classical.
The commutativity conditions given by Eq.
(2) are instances of the general condition
required to interpret any physical process as computation [79]. We can, therefore, regard
ÏP, ÏR, and ÏE as encoding a â€œsemanticâ€ representation of the microscale states |PâŸ©, |RâŸ©,
and |EâŸ©encoded on B. It is with respect to this completely-general notion of semantics
that hierarchies of QRFs can be considered semantic, and hence virtual machine (VM)
hierarchies [80], in Â§4 - 5 below.
2.3
QRFs as constraint networks
In recent work [42, 43, 44, 45] we have drawn extensively upon the semantically enriched
Channel Theory of information ï¬‚ow developed in [41], outlining many examples and appli-
cations (see also in particular [47, 48]). Here we summarize the basic ideas, starting with
8

that of a (Barwise-Seligman) â€œclassiï¬erâ€ (or â€œclassiï¬cationâ€), which categorically accom-
modates a â€œcontextâ€ in terms of its constituent â€œtokensâ€ in some language and the â€œtypesâ€
to which they belong.
Deï¬nition 1. A classiï¬er A is a triple âŸ¨Tok(A), Typ(A), |=AâŸ©where Tok(A) is a set of
â€œtokensâ€, Typ(A) is a set of â€œtypesâ€, and |=A is a â€œclassiï¬cationâ€ relation between tokens
and types.
Note that this deï¬nition speciï¬es a classiï¬er/classiï¬cation as an object in the category of
Chu spaces [81, 82, 83] where â€˜|=Aâ€™ is realizable by a satisfaction relation valued in some
set K having no structure assumed.
Morphisms between classiï¬ers are speciï¬ed by the following:
Deï¬nition 2. Given two classiï¬ers A = âŸ¨Tok(A), Typ(A), |=AâŸ©and B = âŸ¨Tok(B), Typ(B), |=B
âŸ©, an infomorphism f : A â†’B is a pair of maps âˆ’â†’f
: Tok(B) â†’Tok(A) and â†âˆ’f
:
Typ(A) â†’Typ(B) such that âˆ€b âˆˆTok(B) and âˆ€a âˆˆTyp(A), âˆ’â†’f (b) |=A a if and only if
b |=B
â†âˆ’f (a).
This last deï¬nition can be represented schematically as the requirement that the following
diagram commutes:
Typ(A)
âˆ’
â†’
f / Typ(B)
|=B
Tok(A)
|=A
Tok(B)
â†
âˆ’
f
o
(3)
Following these deï¬nitions, Channel Theory can be represented as a category comprising
classiï¬ers as objects and infomorphisms as arrows, which can be seen to be equivalent to
the category comprising Chu spaces as objects and Chu morphisms as arrows [41].
Much of the formulism of [41] revolves around the idea of a distributed system in which the
classiï¬ers and infomorphisms between them function as â€˜logical gatesâ€™ as further exempliï¬ed
in [42, 43, 44, 45]. Signiï¬cantly instrumental in this schemata is a ï¬nite, commuting cocone
diagram (CCD) depicting a ï¬‚ow of infomorphisms sending inputs to a core Câ€² that is the
colimit of the underlying classiï¬ers if such exists:
Câ€²
A1
f1
8
g12
/ A2
f2
O
g23
/ . . . Ak
fk
g
(4)
There is a dual construction to this CCD, namely a commuting ï¬nite cone diagram (CD)
of infomorphisms on the same classiï¬ers, where all arrows are reversed. In this case the
core of the (dual) channel is the limit of all possible downward-going structure-preserving
9

maps to the classiï¬ers Ai. Hence we can deï¬ne the central idea of a ï¬nite, commuting
cone-cocone diagram (CCCD) as consisting of both a cone and a cocone on a single ï¬nite
set of classiï¬ers Ai linked by infomorphisms as depicted below:
Câ€²
A1
f1
6
g12
g21
/ A2
o
f2
O
g23
g32
/ . . . Ak
o
fk
i
Dâ€²
h1
h
h2
O
hk
5
(5)
Signiï¬cantly, the CD functions as a â€œmemory-write systemâ€ in Eq. (4); this is used to
represent the time-stamped â€œwriteâ€ operations of a QRF in [47, 48]. These diagrams can
be extended to obtain a â€œbow tieâ€ diagram as below in (6) which represents a coarse-graining
of the semantics of Ai via Câ€², into a compressed representation Aâ€²
i.
Aâ€²
1
gâ€²
12
gâ€²
21
/ Aâ€²
2
o
gâ€²
23
gâ€²
32
/ . . . Aâ€²
j
o
Câ€²
hâ€²
1
h
hâ€²
2
O
hâ€²
j
5
A1
f1
6
g12
g21
/ A2
o
f2
O
g23
g32
/ . . . Ak
o
fk
i
(6)
Diagrams such as (4) (6) can be further extended into hierarchical networks by adding
intermediate layers of classiï¬ers and infomorphisms. In this way they resemble artiï¬cial
neural networks (ANNs), and in the â€œbowtieâ€ form of Diagram (6), they resemble variational
autoencoders (VAEs) [42]. The core Câ€² in (6) can be viewed as both an â€œanswerâ€ computed
by the fi from inputs to the Ai and, dually, as an â€œinstructionâ€ propagated by the hi (or in
Diagram (6), the hâ€²
i), to drive outputs from the Ai (or in Diagram (6), the Aâ€²
i). This kind
of dual input/output is precisely that of a QRF. Crucially, a â€œbow tieâ€ diagram within the
distributed systems described above, is a descriptive mechanism for broadcasting control
signals to multiple recipients. The Global Neuronal Workspace, as a massively parallel,
distributed system, performs precisely this function for the mammalian central nervous
system [84, 85, 86].
We refer the reader to the concepts of a (regular) theory and local logic as introduced in
[41, Chs 9 12] (and extensively reviewed in [42, 45]) to specify the logical structure of a
given situation. Basically, a local logic is a classiï¬er with a theory, along with a subset of
tokens as speciï¬ed by a sequent; namely, a classiï¬cation M |=A N of a classiï¬er given by
a pair of subsets M, N of Typ(A), such that âˆ€x âˆˆTok(A), x |=A M â‡’x |=A N. In fact,
any classiï¬er generates its own natural local logic. Below, we will adopt the sequent as a
â€˜conditionalâ€™ when deï¬ning probabilities.
10

2.4
CCCDs implement Bayesian inferences in deï¬ned contexts
In general, given an information ï¬‚ow channel:
âˆ’â†’AÎ±âˆ’1 âˆ’â†’AÎ± âˆ’â†’AÎ±+1 âˆ’â†’Â· Â· Â·
(7)
the semantic content can be extended by postulating local logics LÎ± = L(AÎ±) generated by
the corresponding classiï¬ers AÎ± (assumed, in principle, to be in relationship to a (regular)
theory associated to the individual AÎ±, as speciï¬ed in [41, Ch 9] and reviewed in [45,
Appendix A]), thus postulating a ï¬‚ow of logic infomorphisms:
Â· Â· Â· âˆ’â†’LÎ±âˆ’1 âˆ’â†’LÎ± âˆ’â†’LÎ±+1 âˆ’â†’Â· Â· Â·
(8)
which may then comprise a CCD as in Eq. (4).
A probabilistic interpretation of information ï¬‚ow in Eq.
(8) results when the sequent
relation is weakened to require only that if x |=A M, there is some probability P(N|M)
that x |=A N. 1 In eï¬€ect, |=A becomes a conditional probability that inherits the semantics
of the local logic [87] (cf. [88]); that is, given a sequent in a classiï¬er A, along with its
satisfaction relation satisfying:
M |=A N ,
âˆ€x(x |=A M â‡’x |=A N),
(9)
we can deï¬ne the conditional probability as:
M |=P
A N := P(M|N).
(10)
In fundamental Bayesian terms, M above can be regarded as an unobserved (or as be-
low, unobservable) event, and N an observable datum, in which case P(M) becomes the
prior, and P(N) the evidence, that together generate a prediction. Given the likelihood
P(N|M) as the conditional obtained from weakening the sequent, Bayesâ€™ theorem speciï¬es
this conditional as the posterior:
P(M|N) = P(N|M)P(M)
P(N)
.
(11)
We introduce the idea of â€œcontextsâ€ of observation, following [45, Â§7.1] (see also [48, Ap-
pendix A.1]), by considering the following sets: i) X is a set of events in a very general sense
(e.g. a set of observed value combinations or atomic events); ii) Y is a set of conditions
(specifying objects/contents or inï¬‚uences; iii) Z is a set of contexts (e.g. detectors, mea-
surements, or methods); iv) W := Y Ã—Z, where X, Y and Z are taken to be subsets of some
(very) large probability space. Note that here Y can be decomposed as Y = Y + âˆªY âˆ’(dis-
joint union) where Y + consists of observed objects/contents/characteristics of the context
1We use the upper case â€˜Pâ€™ to denote a probability in relationship to classiï¬ers, and a lower case â€˜pâ€™ for
that pertaining to e.g. events or states as below.
11

Y , and Y âˆ’consists of what is not observed about Y . Hence W := Y Ã—Z = (Y + âˆªY âˆ’)Ã—Z.
The classiï¬er in question is then A = âŸ¨X, W, |=AâŸ©and represents an observable in context.
Among several possible interpretations for the classiï¬cation relation â€˜|=Aâ€™ is valuation by
the conditional probability p(a|x) = p(a|{b, c}), whenever deï¬ned, for a âˆˆX, b âˆˆY and
c âˆˆZ [45] (see also below).
As a brief example, consider arbitrary classiï¬ers A(a)
1 , . . . , A(e)
5
in some part of an infor-
mation channel where the classiï¬ers correspond to events a, b, c, d, e, respectively, together
with logic infomorphisms f13, . . . , f45 between them, and where the sequents are relaxed to
become conditional probabilities as above:
A(a)
1
f13
}
f14
!
A(b)
2
f24
}
A(c)
3
A(d)
4
f45

A(e)
5
(12)
Following e.g. [89], this particular channel then generates a joint probability distribution
given by:
p(abcde) = p(a)p(b)p(c|a)p(d|ab)p(e|d).
(13)
As developed in detail in [45], the above constructions provide a formal basis for classical
Bayesian inference. Speciï¬cally, the diagrams (4) and (5) depict channels of logic infomor-
phisms, when on relaxing the sequent in each case, the classiï¬cations |=AÎ± are now explicitly
realized as conditional probabilities pÎ±(Â·|Â·), whenever these are deï¬ned. Putting the above
details concerning conditionals into the diagrammatic framework reveals a typical portion
of a CCD computing a hierarchical Bayesian inference from a set of (posterior) observations
Ai to an outcome Câ€², as having the form:
Câ€²
A1
p10(Â·|Â·)
8
p12(Â·|Â·)
/ A2
p20(Â·|Â·)
O
p23(Â·|Â·)
/ . . . Ak
pk0(Â·|Â·)
g
(14)
The requirement that diagrams (4) and (14) commute amounts to the requirement that
branching â€œupwardâ€ to an overall logic L along any one of the fÎ±, is equivalent to following
the inferential sequence (8) up to its termination, and then following the last of the fÎ± to
L. Thus the fÎ± can be seen as shortcuts to reaching L: â€œinsightsâ€ that allow the rest of
the inferences in (8) to be bypassed. The local logic L can be seen as the â€œanswerâ€ to the
problem (8) addresses: formally, it is the logic that solves the problem in one step. The
probability that the answer is â€œrightâ€ is the product of the probabilities along (8). The
12

commutativity requirement on (14) requires, in addition, that this overall probability is
conserved; hence the probability associated with each â€œinsightâ€ fÎ± must be the combined
probability of the inferential steps it replaces.
Commutativity of a CCD thus enforces inferential coherence, and the same clearly applies
when the dual objects and maps of a CD are combined in constructing a CCCD as in
diagram (5) to implement a recurrent Bayesian network (see also [43] where CCCDs are
employed to model the interaction between bottom-up and top-down processing in visual
object categorization).
In contrast, non-commutativity of (4) implies that there is no
consistently deï¬nable (conditional) probability distribution across the system in question,
and characterises intrinsic contextuality [76, 77, 78] for that system [45, Th 7.1 and Coroll
7.1].
2.5
QRFs as memory resources
A QRF is only useful operationally, and hence only meaningful as a reference frame, if it
can be deployed consistently over multiple episodes of observation. A meter stick or a volt-
meter is, clearly, useful only if yields consistent measurements; the notion of a â€œstandardâ€
to which a device can be periodically recalibrated captures this consistency requirement
operationally.
Recalling that a QRF is by deï¬nition a physical system, not merely an abstraction [39],
the above condition can be stated as a requirement that a QRF have stable dynamics
over the relevant time period, or if it is a subsystem of a larger system A, that it is an
(or a component of an) attractor of the dynamics of A, i.e. of the self-interaction HA.
In computational language, a QRF must be a memory structure, one that is active (and
hence executed as a computation) at all times, or one that is executable on demand via
some higher-level control structure. We can interpret a CCD identifying S = RP that is
implemented by A as encoding an expectation, and hence a prior probability for A, that
bit patterns specifying S will be encoded on the holographic screen B separating A from
its complement B. As emphasized earlier, neither the existence of this expectation not its
satisfaction in practice imply that S exists in any â€œonticâ€ sense as an observer-independent
component of B (cf. [90]).
Operational utility and hence meaningfulness also require that a QRF write its outcomes to
a classical memory in a way that allows comparison of outcomes obtained at diï¬€erent times.
Following [47, 48], we consider the transition from writing an observed pointer outcome Pi
with timestamp Ï„i to writing Pk with timestamp Ï„j to be implemented by an operator Gij,
the complete set of which forms a groupoid under composition. â€œTimestampingâ€ here can be
considered merely to be imposition of an order on the recorded data; a separate, observation
independent clock is not assumed (cf. [50] for a general discussion of time QRFs). As
compliance with Eq. (1) requires all classical data to be written on the holographic screen
B, the memory-write CD must write on B as shown in Fig.
2.
Comparing previous
with current data, in this case, requires reversing the arrows of the relevant memory-write
13

CD to reconstruct the previously-written data. The comparison itself becomes a â€œmetaâ€
operation, i.e. one deï¬ned at the next-higher hierarchical level as described in the case of
neurons below (cf. [43, 44, 45] for examples from human cognitive processing).
Figure 2: A CD Wkj (green triangle) speciï¬es a memory-write operation of the time-
stamped state (ÏRPk, Ï„j) to B. The timestamp Ï„j is generated by the groupoid action Gij
on the previous (at Ï„i) output from the CCD PR. Adapted from [48] Fig. 9, CC-BY license.
We can, therefore, consider QRFs to provide two distinct memory resources: 1) the stable
QRF itself as an executable computation, and 2) the ordered sequence of classical data
accumulated by deploying the QRF to make measurements. The executable QRF is, in
general, a â€œquantumâ€ memory, as it is a quantum computation implemented by the operator
HA. The classical data, in contrast, are written on B and subject to both space and free-
energy constraints. As discussed in [50], these latter memories are eï¬€ectively stigmergic.
Any representation of the â€œselfâ€ that is readable as a classical memory, in particular, must
be written on B, and read from B when recalled [50]. These distinct memory resources
support distinct expectations: QRFs as executable computations encode expectations about
how the â€œworldâ€ faced by an agent is (or more properly, usefully can be) decomposed into
identiï¬able â€œsystemsâ€ while the classical data encode expectations about the states in
which these systems will be encountered. These expectations can be represented as Markov
kernels that are learned incrementally; the diï¬€erence between these â€œexpectedâ€ kernels and
the â€œtrueâ€ kernels given by Diagram (2) provides a representation of VFE to be minimized
14

by active inference, i.e. by exploration (novelty seeking that enlarges the â€œtraining setâ€) and
further learning. This deï¬nition of VFE for generic quantum systems allows a formulation
of the FEP [20, 21, 22, 23, 24] for such systems that is developed in detail elsewhere [91].
3
QRFs at the macromolecular scale
We now turn from generalities to the speciï¬c case of mammalian cortical neurons, and
show how these can be conceptualized as hierarchies of QRFs, i.e. hierarchies of physically-
implemented computations, each of which can be regarded as a measurement, at a deï¬ned
scale and with respect to deï¬ned units, of some aspect of its local environment. We then
generalize this treatment to arbitrary cells in Â§6.
3.1
QRFs as homeostatic setpoints
As discussed in [49, 50], the simplest biological QRFs are switches that induce opposing
behaviors whenever some parameter value is above or below some threshold value. The
CheY system regulating bacterial chemotaxis provides a familiar example: local concen-
trations of the phosphorylated form CheY-P above or below a essentially ï¬xed (at the
relevant timescales) default concentration induce swimming or tumbling behavior, respec-
tively [92, 93]. From a computational perspective, the CheY system implements a switch
between depth-ï¬rst and breadth-ï¬rst search, and hence a generic mechanism for avoiding lo-
cal maxima in a search space. More biologically, it implements both approach to resources
and avoidance of toxins and other irritants, and hence serves as a homeostatic setpoint.
Such setpoints are, from the present perspective, expectations consistent with continuing
structural and functional integrity, i.e. continuing separability from the surrounding world.
They can, therefore, be expected to arise in any system that maintains separability, i.e.
any system to which Eq. (1) applies.
A second familiar QRF is the default or â€œrestingâ€ membrane voltage V 0
mem across any local
patch of biological membrane. Fluctuations above or below this setpoint induce actions, e.g.
opening or closing voltage-gated ion channels. This QRF is obviously relevant to neuron
function as discussed below; at larger scales, it becomes a critical regulator of multicellular
morphology; see e.g. [94, 95, 96] and further discussion in Â§6.
Guided by these examples, it is straightforward to consider any system implementing linear
threshhold or sigmoid kinetics as a QRF that switches between â€œnegativeâ€ and â€œpositiveâ€
responses as its input ï¬‚uctuates below or above a default value (Fig. 3). With the ad-
vent of two-component signaling pathways, early enough in prokaryotic evolution that they
appear in all analyzed lineages [97], the default value becomes adjustable, i.e. context-
dependent via a classical â€œdirect inï¬‚uenceâ€ mechanism [78] (but see [98] for evidence that
lactose-glucose interference in E. coli [99] exhibits nonclassical contextuality). The multi-
component systems typical of eukaryotes, e.g. the Wnt [100], ERK [101], notch [102], BMP
15

[103] pathways and the interactions between them, introduce further, multifactorial context
dependence (see e.g. [104, 105] for general reviews). Such systems embody expectations
about the relationships between multiple, simultaneously-measured values, each eï¬€ectively
calibrated to an underlying standard, e.g. a molecular concentration or local value of Vmem.
Figure 3: a) A generic system executing sigmoid kinetics. b) Reï¬‚ecting the response curve
at its inï¬‚ection point redescribes the system as a QRF that switches behavior at a default
value.
As noted earlier, cellular-scale biochemical and bioelectric signaling pathways have tradi-
tionally been conceptualized and depicted as fully classical, with each component occupying
some determinate state at all relevant times. Cellular energy budgets cannot, however, sup-
port the thermodynamic cost of classicality, even at ms timescales, indicating that quantum
coherence and hence quantum computation may be signiï¬cant up to timescales of seconds
[63]. Classical data can, in this case, only be encoded on intra- or intercellular boundaries
as illustrated schematically in Fig. 1. â€œChunkingâ€ the cell into functional components that
process and exchange classical information or serve as classical memory structures (e.g. the
genome, proteome, transcriptome, and â€œarchitectomeâ€ [106]) is, eï¬€ectively, identifying the
boundaries at which classical information is encoded, i.e. the boundaries that function as
MBs. A signal transduction pathway that measures some environmental parameter at the
cell membrane and transfers a context-dependent representation of this information to the
genome is, in this picture, a single quantum computation, a QRF that detects, calibrates,
and reports an observational outcome. It can be treated as a black box characterized by
inputs and outputs, including free energy inputs and dissipated heat outputs. We adopt
this approach to characterize the functional architecture of neurons in what follows.
3.2
The postsynaptic complex as a QRF
As discussed in [107, 108, 109], the MB of a single neuron includes the pre- and postsynaptic
membranes that mediate interactions with other neurons, as well as the overall cell mem-
16

brane that mediates interactions with the neuronâ€™s local microenvironment. Single neurons
are, however, large, spatially-extended, computationally-complex structures (Fig. 4); it is
the need for improved models of such structures that motivates this paper. Hence we will
consider how the neuronâ€™s MB is constructed from those of its components, identifying in
this process sites at which intermediate steps in neuronal computations may be classically
encoded.
Figure 4: Cartoon structure of a mammalian cortical neuron; cf. [110, 111] for anatom-
ical details. a) Voltage gated channels and pumps are the primary input (sensory) and
output (active) MB components; b) Specialized postsynaptic structures, e.g. on dendritic
spines, collect incoming information from other neurons; c) dendritic subtrees integrate and
process this incoming information; d) dendritic information is integrated at the soma and
distributed to outgoing axonal channels. Postsynaptic receptors may also appear on the
soma or on axons; this level of detail is neglected here.
Starting from the input side, the smallest cellular-scale component is the specialized post-
synaptic area, typically located on a spine morphologically. Its computational role is to
contribute a positive (excitatory) or negative (inhibitory) time-dependent Vmem gradient to
the overall electrical activity of its local dendritic branch. Performing this function requires
17

energetic and molecular ï¬‚ows in addition to âˆ†Vmem(t), as illustrated in Fig. 5. At this level
of abstraction, the presynaptic specialization at the axonal bouton is essentially equivalent,
up to reversing the ï¬‚ow of small molecules from the cell membrane.
Figure 5: Cartoon structure of a mammalian postsynaptic specialization; cf [4] for anatom-
ical and functional details.
Primary energy (red), molecular (blue) and Vmem gradient
(orange) ï¬‚ows are shown. Considered as a system, the spineâ€™s MB includes the spine mem-
brane (solid line) and the cytoplasmic/cytoskeletal interface between the spine and the bulk
dendritic cytoplasm (dashed line).
Making the reasonable assumption of balanced mass ï¬‚ows, i.e. assuming that the volume
and density of the postsynaptic specialization, or in quantum terms, its Hilbert-space di-
mension remains constant, we can focus on information ï¬‚ows only, and treat the interaction
between the postsynaptic specialization and its surroundings using Eq. (1). The bound-
ary B in this case has two components, B = CD, where C separates the postsynaptic
specialization from the external microenvironment and D separates it from the bulk den-
dritic cytoplasm. Both components support both inward sensory ï¬‚ows and outward active
ï¬‚ows. As these information ï¬‚ows are both physically implemented and referenced to de-
fault homeostatic/allostatic setpoints, they can be considered to be implemented by QRFs.
Hence they can be represented as input â€“ output or perception â€“ action ï¬‚ows as in Fig. 6.
In this representation, the boundary B is decomposed functionally into B = IO, where I
and O are â€œinputâ€ and â€œoutputâ€ components respectively, each of which includes regions of
18

both anatomical components C and D. While each ï¬‚ow Q has its own characteristic time
constant Ï„ Q as in Fig. 2, the cross-modulatory couplings between pathways, and hence mu-
tual dependence between setpoints, requires that they be mutually coherent. Hence we can
represent the postsynaptic specialization as executing a single QRF with a vector output
and a time constant Ï„ PS = maxQÏ„ Q. Presynaptic specializations can be treated similarly.
Figure 6: Information ï¬‚ows abstracted to QRFs (X, Y , Z; colored triangles) that read from
an input boundary component I (right hand blue ellipse) and write to a output boundary
component O (left hand blue ellipse), where the overall system boundary B = IO. Each
QRF Q has an associated time constant Ï„ Q and a groupoid-structured set of operators GQ
ij
as depicted in Fig. 2.
As discussed in Â§2 above, all classical information in the system must be encoded on B.
Hence Fig. 6 depicts the ideal case in which the boundary component O encodes all out-
put information, i.e. the computations implemented by the QRFs X, Y , and Z have no
classically-encoded intermediate states. In this ideal case, the free-energy cost of compu-
tation per unit time is given by the output bandwidth in bits, i.e. the area in bits of O.
Assuming a minimal thermal energy âˆ†Eth = ln2kBT, the minimum dissipation timescale
is given by the time-energy uncertainty relation [112] as âˆ†tdiss â‰¥Ï€â„/(2âˆ†Eth) â‰ˆ50 fs.
Neuronal energy budgets, however, are insuï¬ƒcient for classical encoding at this timescale;
assuming 30,000 synapses per cortical neuron [111] and a maximum power consumption
of 250 Gbits/sec (where as a power unit 1 bit =def ln2kBT â‰ˆ3 Â· 10âˆ’21 J at T = 310 K)
[26], each synapse could encode at most 8.3 Mbits/sec if the cellâ€™s metabolic resources were
devoted entirely to synaptic encoding. In fact the bulk of neuron energy usage is devoted to
19

action potential (AP) generation and transmission, reducing the coding capacity of synapses
by up to two orders of magnitude (see [63, 113, 114, 115] for further discussion of neuronal
classical encoding costs and capacity). We can expect, therefore, that individual postsynap-
tic specializations can realistically encode at most about 100 kbits/sec, or 100 bits per unit
time at the ms scales of dendritic postsynaptic potentials. Hence the idealization of Fig. 6
is not unrealistic from an energetic perspective, suggesting that consistent with the general
considerations in [63], QRFs implemented at the scale of cellular compartments perform
essentially pure quantum computations, i.e. employ quantum coherence as a short-term
memory resource.
The form of Fig. 6 suggests a cobordism with I and O as boundaries. If the QRFs X,
Y , and Z perform pure quantum computation without classical intermediate steps, we
can treat them in a path-integral formalism [116], and hence as deï¬ning a manifold of
mappings from I to O. This suggests that information processing in neurons is amenable
to a treatment using topological quantum ï¬eld theory [117] and the emerging theory of
topological quantum neural networks [118]. We defer this possibility to future work.
4
Dendrites as spatially-organized QRFs
Dendritic branches are traditionally viewed as lossy but otherwise passive integrators of rel-
atively slow (10s of ms) postsynaptic potentials (PSPs), though faster spike-like activations
and backwards propagation of APs are also possible [110]; see [119] for detailed dendritic
spike models.
This passive view of dendritic activity renders the execution of complex
logical operations like XOR [11] surprising and activity-dependent remodeling [2, 3, 4] mys-
terious. It decouples the processes required to maintain a successful synapse: learning is
largely localized to the postsynaptic side, while target search and synapse formation are lo-
calized to the presynaptic side. Finally, it decouples dendritic function from the dendriteâ€™s
trophic requirements. Branches that transmit only low-amplitude noise signals cannot, on
the passive model, be regarded as less successful or less worthy of continued metabolic
maintenance in comparison to those branches that consistently transmit high-amplitude,
highly informative signals.
Dendritic branches satisfy the physical requirements for implementing QRFs and hence
engaging in active inference. Viewing dendrites in this way raises two immediate questions:
1. What is a dendritic branch measuring?
2. What is the VFE function that a dendritic branch is minimizing?
However, while dendrites can be described as coincidence and hence activity-correlation
detectors [110] even on the passive view, the second of these questions is diï¬ƒcult to even
formulate from a passive perspective. A partial answer to both questions is discussed in
[29, 30]: basically, dendrites can self-organize to minimize the VFE on surprise of their
20

presynaptic inputs, showing that postsynaptic gain is itself optimized with respect to VFE.
Extending the modeling approach of the previous section to dendritic branches suggests an-
swers to these questions that can be summarized by the hypothesis that dendritic branches
identify â€œobjectsâ€ with relatively small numbers of states, and execute approximately binary
logic on the measured state vectors.
4.1
Colocating consistently correlated synapses minimizes VFE
While both dendritic arborization and spine density vary with neuron type and location
[120, 121, 122, 123], both cortical and hippocampal pyramidal cells can exhibit dendritic
branches with on the order of 1,000 spines and full dendritic trees with up to 30,000 spines
[111]. Consider such a branch, neglecting information and resource ï¬‚ows through the non-
spine membrane to focus on ï¬‚ows to and from the spines as modeled in Fig.
5.
As
the magnitudes of resource ï¬‚ows correlate with activity and hence with the magnitude of
âˆ†Vmem, we can further focus on the latter. In this case, we can think of the branch as
â€œobservingâ€ on the order of 1,000 points of active signaling, analogous to 1,000 points of
light in a visual ï¬eld.
There are clearly two limiting cases for the activity observed by a branch. If every spine
receives input from a distinct, independently-activated neuron, the correlation Cij between
inputs i and j can be expected to be small, with Cij â†’0 in the limit. If all spines receive
input from a single neuron, Cij â†’1 in the limit. Assuming roughly constant frequencies
(though random phases) of presynaptic activity, the former limit yields noise with an input-
frequency dependent amplitude as an output PSP from the branch; the latter yields an
essentially digital signal encoding the activity of the single input neuron. An essentially
constant noise signal is eï¬€ectively a â€œdark roomâ€ from a VFE perspective, while a variable
digital signal poses a well-deï¬ned, potentially high-information prediction problem.
Microconnectome methods [124] allow, in principle, counting the numbers of presynaptic
(i.e. incoming) and postsynaptic (outgoing) partners for any neuron. Measured numbers
of presynaptic partners range up to 50 in C. elegans [25] and from tens to several hundred
in mammalian cortical cells [125, 126] to thousands for cerebellar Purkinje cells [124]. If
as estimated [127] neocortical pyramidal cells typically receive 4 or 5 synaptic connections
from each presynaptic partner, these cells may also have several thousand presynaptic
partners. Such cells would be operating very close to the noise limit of Cij â†’0 unless
the activity of the presynaptic partners is already highly correlated. Perhaps signiï¬cantly,
most microscale connectome mapping has been achieved in sensory cortices in which highly
correlated incoming information can be expected.
Following [110] and considering each dendritic branch to be a coincidence detector, the
fundamental problem faced by a branch is distinguishing between i) correlated signals from
a given presynaptic partner, ii) â€œtrueâ€ correlates from multiple partners, and iii) â€œran-
domâ€ correlates. We can treat this uncertainty as a VFE function, and ask how it can
be minimized. For a relatively short, distal, terminal branch exhibiting all three kinds of
21

correlates, the answer is shown in Fig. 7: localizing partially-correlated regular inputs to
distinct, non-branched branches and moving random inputs as far as possible from branch
junctions disambiguates signals from diï¬€erent synaptic partners, allows â€œtrueâ€ correlations
to be identiï¬ed speciï¬cally at branch junctions, and reduces the amplitude of noise signals.
Figure 7: a) Least and b) most eï¬ƒcient arrangements of synapses with partially-correlated
(red and blue) and random (green) activity on a two-sided symmetrical, terminal dendritic
branch.
By minimizing the ambiguity of correlation, arrangement b) allows maximally
digital processing at the branch junction.
A terminal branch in an initial state similar to Fig. 7a has synapse-level learning and spine
remodeling available as tools for moving toward a state similar to Fig. 7b, with trophic re-
ward as the imposed quality function. This is active inference, with spine remodeling as the
â€œactionâ€ that alters environmental input, i.e. synaptic activity, independently of synapse-
level priors. Success (given enough time) can be expected if trophic reward increases with
the overall informativeness of the branchâ€™s activity, i.e. if trophic reward to dendrites as
functional units has the same activity-dependence as trophic reward to neurons as func-
tional units [1, 128, 129], as studies of dendritic remodeling already suggest [3, 4]. This
is mechanistically reasonable, as it transfers both the task of determining informativeness
and that of adjusting trophic reward to the proximal part of the dendritic tree, and hence
eï¬€ectively to the soma itself. Both are, from the branchâ€™s perspective, outside of its MB
and hence part of the environment.
We can, therefore, state the following:
Prediction: Trophic reward to dendritic branches correlates with informative-
ness of the signal from the branch for more proximal dendritic branches, with
informativeness to the soma as a limit.
What the soma needs to know is whether to ï¬re an AP. A high-amplitude signal on a
low-amplitude background, i.e. an eï¬€ectively digital PSP, provides this information most
22

eï¬ƒciently. Hence we can predict that dendritic trees will remodel in the direction of maxi-
mizing digital processing.
4.2
Branches as object detectors
A branch that has segregated its non-noise inputs into diï¬€erent compartments as in Fig.
7b is, eï¬€ectively, an object detector. The object it detects is a presynaptic partner, or a
spatially-localized collection of axonal processes from a presynaptic partner. It measures
the detectable state of this object, its binned-average activity at some binning timescale,
typically a few ms. Hence we can think of it on the model of Eq. (2) or Fig. 2, with the
input correlation as the reference R, the binned-average activity as the pointer P, and any
background noise as the environment E. Hence a compartmentalized branch is a canonical
collection of QRFs.
From this perspective, we can see spine remodeling within dendritic branches as a process of
assembling QRFs that identify objects and hence enable measurements of their states. Spine
remodeling is thus an instance of the second, relatively neglected, form of active inference:
the minimization of uncertainty about the decomposition of an agentâ€™s world. A branch
with maximally-ambiguous inputs as in Fig. 7a decomposes its world only into â€œpoints of
lightâ€; a branch that has remodeling to approach Fig. 7b sees instead two objects with
well-deï¬ned states that can be compared in both amplitude and time dimensions. Spine
remodeling is, as are the processes of object segregation and object persistence during infant
development, learning what to see, and hence learning what coherent things populate the
perceived world.
5
Neurons as measurement devices
5.1
Dendritic geometry imposes salience
As the â€œcutâ€ and hence the boundary imposed to render a branch â€œterminalâ€ is moved
toward the soma, the branch accumulates junctions separating sub-branches, and object-
identity information is replaced by coincidence information. At each such sub-branch junc-
tion, the post-junction sector of the dendrite is faced with the task of reducing VFE by
distinguishing â€œtrueâ€ from â€œrandomâ€ coincidences.
Learning and remodeling driven by
trophic reward remain the tools available for amplifying the former at the expense of the
latter, thus increasing the signal-to-noise ratio and rendering signal processing eï¬€ectively
more digital. Hence one can expect neurons to remodel in the direction of segregating inputs
to their dendritic trees in a way that maximizes true coincidences at branch junctions. To
the extent that they do this, they increasingly operate as hierarchies of spatially-segregated
QRFs
Dendritic geometry gives proximal synapses more inï¬‚uence over somatic activity than distal
ones; the gating function of proximal inhibitory synapses, in particular, is well known [110].
23

Distal signals can only be processed with high (Bayesian) precision in periods of proximal
silence. Hence it is natural to think of dendritic geometry as imposing a salience gradient
from proximal to distal synapses. Stereotypical diï¬€erences in arborization pattern between
pyramidal cells from diï¬€erent cortical or hippocampal layers reï¬‚ect diï¬€erent salience as-
signments, with cortical cells typically giving distal inputs, and hence distal presynaptic
partners, higher salience than do hippocampal cells [110]; see [130, 131, 132] for speciï¬c
diï¬€erences between cortical cells from diï¬€erent layers. Higher distal salience is only useful
from an information processing perspective if it is low-noise, i.e. if PSPs from distal den-
dritic branches, e.g. from the elaborate apical tufts of Layer V cortical pyramidal cells, are
(primarily) time-convolutions of true coincidence signals. As remodeling takes time, one
can expect neurons that compute the same function over long periods, i.e. that are required
to exhibit only slow learning, to assign the greatest salience to distal inputs.
In the spiking neurons of interest here, somatic signal integration results, given suï¬ƒcient
amplitude, in AP generation. While APs are relatively high-amplitude, low-noise signals,
outside of myelinated axonal segments diï¬€erential AP degradation can be expected to
contribute to diï¬€erential presynaptic signal strength. Weak presynaptic signals are weak
actions on the signaling neuronâ€™s environment, and cannot be expected to yield strong
conï¬rmatory signals in return.
Hence relatively weak synapses can be expected to be
clustered, consistent with Fig. 7b.
5.2
Neurons as tomographic computers
As discussed in Â§2, a QRF corresponds to a speciï¬c choice of basis for a subset of the
operators M A
i deployed by an agent A to measure the state of its MB, or boundary B. We
can, therefore, think of the hierarchy of QRFs implemented by a neuronâ€™s dendritic tree as
a hierarchy of basis choices, with the â€œlow-levelâ€ basis components corresponding to detec-
tions of correlated inputs from local clusters of synapses from single presynaptic partners
and the â€œhigh-levelâ€ basis components corresponding to coincidences between increasingly
more highly â€œchunkedâ€ aggregates of input signals. Each neuron can thus be thought of
as deploying a speciï¬c hierarchy of basis vectors to measure the presynaptic activity of its
environment.
Consider now a set N of neurons sampling the â€œsameâ€ environment, e.g. a set of Purkinje
cells each sampling the axonal outputs of a set of cerebellar granule cells.
The causal
antecedents of the activity of the input neurons are clearly â€œunknownsâ€ from the perspective
of the neurons in N; the input activities can, therefore, be considered from this perspective
an ensemble E of samples from an unknown, time-varying state. We can, in this case,
think of N as making a set of measurements, each in a distinct basis, of the ensemble E of
time-varying states.
If we think of the states in E as quantum states, then the set N of neurons can be regarded
as performing quantum state tomography on E, and hence quantum process tomography on
the process generating time variation in E [133]. Quantum state tomography generalizes
24

classical tomography by generalizing the choice of basis away from â€œslicesâ€ of a three-
dimensional geometry in which the system of interest is embedded.
It is expensive in
terms of basis dimensionality, requiring d2 basis vectors for quantum states of (binary)
dimension d, and âˆ¼d4 basis vectors for processes on states of dimension d. This dimensional
cost translates well to the case of ensembles of neural activity, since as in the case of
quantum states, it is phase correlation information that the tomographic measurement
aims to extract.
Viewing neural computation as tomographic provides an immediate explanation for the
enormously high input and output bandwidths and apparent redundancy of neuronal ar-
chitectures, which are diï¬ƒcult to understand on the basis of simple logic-circuit models
[127, 134]. An input space with 100-bit states (d = 100) would require âˆ¼1004 = 108 binary
dimensions for process tomography, or roughly 105 neurons at 1,000 distinct presynaptic
partners per neuron. Following [135] and assuming âˆ¼108 minicolumns with 100 neurons
each in human neocortex, analyzing a 100-bit state would require a minimum of âˆ¼103
minicolumns or 0.001% of neocortical capacity. We can infer from this that tomographic
computation is approximate, with substantial reduction of the input dimension by salience
systems, e.g. active attention.
5.3
Scaling upwards: minicolumns to functional networks
The stereotypically layered, only sparsely interconnected minicolumns of mammalian cor-
tex, and in particular, human neocortex, are widely viewed as computational as well as
neuroanatomical units, with modeling studies increasingly suggesting that minicolumns ex-
ecute Bayesian predictive coding [30, 107, 109, 123]. Extending the model outlined above,
we can think of minicolumns as functionally analogous to neurons, gathering input from and
sending output to other minicolumns. As does a neuron, a minicolumn â€œseesâ€ a spatially-
distributed collection of (positive or negative) excitations, spatial and temporal correlations
between which are informative to the extent that they are non-random. Hence a minicol-
umn is faced with a coarse-grained version of the VFE minimization problem faced by
neuron: that of disambiguating â€œtrueâ€ coincidences from random ones. Minicolumns can
be expected to functionally remodel to increase signal-to-noise ratio for the same reasons
neurons can, with trophic reward from the environment as the selective criterion.
Functional networks spanning multiple cortical regions, e.g.
sensory pathways, impose
a higher-level hierarchical organization.
Predictive coding across layers of this higher-
level hierarchy have been analyzed in terms of typical connections between minicolumns in
adjacent layers; see e.g. [109, Fig. 4] or [123, Fig. 2] for interlayer connection maps. Top-
down connections encoding likelihood at pathway layer i arise mostly from minimcolumn
Layer V pyramidal cells and target minicolumn Layer III cells at pathway layer i âˆ’1.
Reciprocally, bottom-up connections arise mainly in minicolumn Layer II at pathway layer
i, and project to networks of Layer IV cells at pathway layer i + 1. Hence at any level
of the pathway, empirical priors are localized in minicolumn Layer III, and likelihoods (or
predictions) in minicolumn Layer V. This is further analyzed in [107] in terms of how MBs
25

inï¬‚uence connectivity of microcircuits. Internal and external states are implemented by
spiny stellate cells and interneurons of each minicolumn; via connections from and to these,
respectively, superï¬cial pyramidal cells of the next minicolumn become the (blanket) active
states, while the deep pyramidal cells of the previous minicolumn become the (blanket)
sensory states. Eï¬€ectively, the minicolumns are the functional units comprising an MB of
networks (either seen as e.g. a MB of MBs, or a Matryoshka of MBs). Once the MBs
are functional, the overriding principle is that neurons, microcolumns and networks appear
to dynamically self-organize thanks to the FEP. There are many interesting outcomes:
consider for instance the visual network as composed of internal states inï¬‚uencing the
dorsal and ventral attention networks, while the default-mode network plays the role of a
sensory system mediating the inï¬‚uence between these former and external, sensorimotor
states. Just as in the case of lower-level structures, such high-level, multi-layer processing
pathways exhibit activity-dependent trophic reward and remodel as necessary to achieve it,
as shown e.g. by studies of large-scale pathway remodeling following injuries that remove
expected inputs [136] (cf. [109, 137]).
Since Fristonâ€™s proposal that the mammalian brain is an active-inference device [20], numer-
ous studies have explored the implementation of active inference by large-scale networks,
up to and including the global neuronal workspace [44, 45]. What we have shown here
is that these principles extend downward to the scale of the individual synapse. Trophic
rewards select for informative activity, i.e. high signal-to-noise ratios, at every scale. We
can expect these considerations to generalize from neurons to non-neural cells, and from
neural communication to communications between biological structures at every scale.
6
Generalizing neural computation to cellular compu-
tation
The prior discussion was framed in the context of neurons; however, it becomes more
generally applicable when we note that neurons did not appear de novo but in fact evolved
slowly from other cell types which already shared many of their features (reviewed in
[16]). Not only are the basic molecular components of neurons (ion channels, electrical
synapses, neurotransmitter machinery) already present in most cells including unicellulars,
but all cells produce bioelectric gradients and comprise tissues with propagating changes
in resting potential [95]. It has been suggested that developmental bioelectricity is the
evolutionary precursor to neural dynamics, and indeed that evolution speed-optimized the
slow bioelectrical signaling that was ï¬rst used to solve problems in morphospace (exerting
anatomical control over body shape by regulating cell behaviors) before it was exapted to
solve problems in 3-dimensional behavior space (by regulating muscle function) [138, 139].
Consistent with this hypothesis, bioelectric signaling in non-neural cells has been implicated
in control of morphogenetic decisions in embryogenesis, regeneration, and cancer [140, 141,
142]. Recent work targeting the native ion channels and gap junctions in tissue has shown
that bioelectric states can be readily modulated to predictably alter organ identity, induce
26

regeneration of limbs, control the axial patterning of whole bodies, and repair complex
structures such as craniofacial birth defects [143]. Indeed, it has been suggested that parallel
to the architecture of brains, non-neural bioelectricity is the medium implementing the
information processing that enables collective intelligence of cellular swarms during body
construction and remodeling [144].
Morphological change is a deeply computational process that relies on calibrated measure-
ment, and hence QRFs, at multiple organizational levels. While the DNA determines the
micro-level hardware that each cell gets to have (e.g., its complement of ion channels),
genomes do not directly encode morphology. Instead, they specify machines that have to
deploy considerable intelligence, using William Jamesâ€™ deï¬nition of intelligence as the abil-
ity to reach the same outcome from diï¬€erent starting conditions and despite perturbations.
The physiology of cellular collectives implements very robust problem-solving capacities
by making important decisions about collective macrostates that are not deï¬ned at the
level of individual cells. For example, mammalian embryos cut in half do not result in two
half-embryos â€“ the system regulates to make complete, normal bodies of monozygotic twins
(regulative development). Tadpoles that are developmentally disrupted to have all of their
craniofacial organs in the wrong positions still result in normal frogs after metamorphosis
because the eyes, nostrils, mouth, etc. move through novel paths to get to the same invari-
ant outcome â€“ a correct frog face [145, 146, 147]. Salamanders whose arms are amputated
at the shoulder or wrist regenerate precisely what is needed and then stop when a correct
limb is complete. These are just a few examples of a nearly ubiquitous property of morpho-
genetic systems: anatomical homeostasis toward a speciï¬c pattern memory, and the ability
to reach that state despite sometimes drastic, unpredictable perturbations. This degree of
anatomical control requites fundamentally computational functions by cell collectives: they
need to be able to measure the current state (e.g. the length of a limb, conï¬guration of the
face, etc.), remember the correct state (represent aspects of the correct target morphology),
and execute a kind of means-ends analysis to reduce error by controlling cell proliferation,
migration, and diï¬€erentiation. Trophic reward plays a critical role in such processes: struc-
tures or bodies that are functionally insuï¬ƒcient to obtain nutrients and other resources
from their environments do not survive.
Consistent with a functional continuity between neural and non-neural dynamics, morpho-
genesis shares a number of major features with behavior in addition to the reliance on bio-
electric networks to implement large-scale coordination. The ï¬rst is the hardware/software
distinction: genomes do not encode ï¬nal outcomes, they encode the structure of a system
with plasticity, context-sensitivity, and the ï¬‚exibility to produce diï¬€erent outcomes from
the exact same hardware. This is why genetically wild-type ï¬‚atworm cells can generate
head structures appropriate to other species when their bioelectric signaling is shifted to
diï¬€erent attractors [148], why normal skin cells liberated from frogs spontaneously self-
assemble into diï¬€erent, motile proto-organisms (â€œXenobotsâ€) without any genomic editing
[149, 150], and why embryos with severe genetic defects can be bioelectrically coaxed to nor-
mal brain morphogenesis by reinforcing speciï¬c voltage patterns [151, 152]. Developmental
bioelectric circuits also feature a kind of re-writable memory. In addition to the default
27

voltage patterns (like the â€œelectric faceâ€ [153, 154]), new ones can be written into tissues
and maintained by the circuit, such as the two-headed planaria that result in genetically
wild-type worms when a diï¬€erent Vmem pattern memory is temporarily incepted into the
tissue [155, 156]. These two-headed worms continue to regenerate as two-headed in perpe-
tuity (without additional treatment) or can be bioelectrically switched back to one-headed
[155, 157], illustrating the stable but re-writable aspects of the information in the tissue
that guides behavior.
Bioelectric signaling in all tissues, like in nervous systems, integrates information across
distance to enable decisions and behavior toward adaptive outcomes in novel circumstances.
This basic scheme was already discovered by evolution as far back as the time of bacterial
bioï¬lms [158, 159], and is exploited very widely across the web of life from microbes to
humans [96]. Many aspects of cognitive neuroscience have clear parallels in developmental
biology [160], suggesting that much of the reasoning in Â§3â€“5 above applies not just to
nervous systems but in fact to all cells solving problems in various spaces. Indeed, all living
systems are deeply hierarchical, exhibiting aspects of basal cognition and decision-making
at levels including molecular pathways [161, 162], physiological problem-solving by cells
[163, 164], and tissue and organ memory [165, 166, 167] among others.
7
Conclusions and future directions
Neurons, networks of neurons, and biological structures generally exchange information
with their environments via physical interactions interpretable in terms of measurement (or
the gathering of sensory input) and manipulative action. This interpretation of biological
activity forms the basis of the active-inference principle [20, 21] and has achieved wide
currency in the biophysics, evo-devo, and neuroscience communities. We have shown here
how to formulate this view of biological activity in the very general yet powerful language
of quantum information theory, employing in particular the idea of a QRF as a calibrated
measurement, and in the output direction, a calibrated action. These results extend the
analysis of QRFs as hierarchical Bayesian systems developed in [43, 44, 45, 47].
They
allow us to model processes implemented by neurons and networks of neurons, from the
biomolecular pathway scale upward, as hierarchies of QRFs. This representation makes
explicit where and how classical information is encoded on successive layers of MBs, enabling
models that explicitly comply with energy-budget considerations. Such models require that
cells employ quantum coherence as a resource for bulk computation [63]. Hence the current
framework represents neurons as explicitly quantum devices.
Our goal here has been to develop a framework for building detailed models of speciï¬c
neuronal cell types in speciï¬c environments; these will be pursued in future work. Even
at the current abstract level, however, we are able to predict generically that dendritic
trees will remodel in the direction of maximal informativeness, with trophic reward as the
selection criterion. While this prediction is generally supported by the phenomenology of
dendritic remodeling [3, 4], it is speciï¬cally testable by correlating measures of branch level
28

activity and branch level active transport. As branch level remodeling is now known to be
involved in neurodegenerative conditions [3, 4], an understanding of the relations between
architecture, computation, and trophic reward at this scale may be useful in ameliorating
these conditions.
From a more general perspective, treating neurons in explicitly quantum-theoretic terms
introduces new possibilities for mathematical modeling, e.g. with topological ï¬eld theory
as discussed in Â§3. Neuroscience and computer science have both beneï¬tted from the ab-
straction of neurons to sum-threshold units connected in layered ANNs. As interest in and
tractable architectures for quantum computing continue to develop, we may expect a com-
parable, but considerably deeper, connection between biological and artiï¬cial computing
systems.
Acknowledgements
ML gratefully acknowledges funding from the Guy Foundation and the Finding Genius
Foundation.
Conï¬‚ict of interest
The authors declare no competing, ï¬nancial, or commercial interests in this research.
References
[1] Butz M, WÂ¨orgÂ¨otter F, van Ooyen A. 2009 Activity-dependent structural plasticity.
Brain Res. Rev. 60, 287â€“305.
[2] Carulli, D.; Foscarin, S.; Rossi, F. 2011 Activity-dependent plasticity and gene ex-
pression modiï¬cations in the adult CNS. Front. Mol. Neurosci. 4, 50.
[3] Hogan, M.K.; Hamilton, G. F.; Horner, P .J. 2020 Neural stimulation and molecular
mechanisms of plasticity and regeneration: A review. Front. Cell. Neurosci. 14, 271.
[4] Runge K; Cardoso C; de Chevigne A. 2020 Dendritic spine plasticity: Function and
mechanisms. Front. Synaptic Neurosci. 12, 36.
[5] Wittenberg, G. M.; Wang, S. S.-H. 2016. Evolution and scaling of dendrites. In (G.
Stuart, N. Spuston and M. HÂ´â€™auser , eds.) Dendrites. Oxford University Press, Oxford
UK.
[6] Guerguiev, J; Lillicrap, T. P., Richards, B. A. 2017. Towards deep learning with
segregated dendrites. eLife 6:e22901, 37 pages.
29

[7] Sardi, S.; Vardi, R.; Goldental, A.; Tugendhaft, Y.; Uzan, H.; Kanter, I. 2018 Den-
dritic learning as a paradigm shift in brain learning. ACS Chem. Neurosci. 9, 1230â€“
1232.
[8] Blackiston, D.; Shomrat, T.; Levin, M. 2015 The stability of memories during brain
remodeling: A perspective. Commun. Integr. Biol. 8, e1073424.
[9] Shomrat, T.; Levin, M. 2013 An automated training paradigm reveals long-term
memory in planarians and its persistence through head regeneration. J. Expt. Biol.
216, 3799â€“3810.
[10] McConnell, J. V. 1967 A Manual of Psychological Experimentation on Planarians.
Journal of Biological Psychology (Publisher), Ann Arbour, USA.
[11] Gidon, A.; Zolnik, T.A.; Fidzinski, P.; Bolduan, F.; Papoutsi, A.; Poirazi, P.;
Holtkamp, M.; Vida, I.; Larkum, M. E. 2020 Dendritic action potentials and compu-
tation in human layer 2/3 cortical neurons. Science 367, 83â€“87.
[12] McCulloch, W. S.; Pitts, W. 1943 A logical calculus of the ideas immanent in nervous
activity. Bull. Math. Biophys 5, 115â€“133.
[13] Segev, I; London, M. 2000 Untangling dendrites with quantitative models. Science
290, 744â€“750.
[14] Schuman, C.D.; Potok, T. E.; Patton, R. M.; Birdwell, D.; Dean, M. E.; Rose, G.
S.; Plank, J. S. 2017 A survey of neuromorphic computing and neural networks in
hardware. Preprint arXiv:1705.06963v1 [cs.NE].
[15] Tang, J.; Yuan, F.; Shen, X.; Wang, Z.; Rao, M.; He. Y.; Sun, Y.; Li, X.; Zhang, W.;
Li, Y.; Gao, B.; Qian, H.; Bi, G.; Song, S.; Yang, J.; Wu, H. 2019 Bridging biological
and artiï¬cial neural networks with emerging neuromorphic devices: Fundamentals,
progress, and challenges. Adv. Mater. 31, 1902761.
[16] Fields, C.; Bischof, J.; Levin, M. 2020 Morphological coordination: A common an-
cestral function unifying neural and non-neural signaling. Physiology (Bethesda) 35,
16â€“30.
[17] Barrat, A.; BarthÂ´elemy, M.; Vespignani, A. 2008 Dynamical processes on complex
networks. Cambridge University Press, Cambridge UK.
[18] Rubinov, M.; Sporns, O. 2010 Complex network measures of brain connectivity: Uses
and interpretations. NeuroImage 52, 1059â€“1069.
[19] Latham, P.; Dayan, P. 2005 TouchÂ´e: the feeling of choice. Nature Neuroscience 8(4),
408â€“409.
[20] Friston, K. J. 2010 The free-energy principle: A uniï¬ed brain theory? Nature Reviews
Neuroscience 11, 127â€“138.
30

[21] Friston, K. J. 2013 Life as we know it. Journal of The Royal Society Interface 10,
20130475.
[22] Friston, K. J.; Stephan, K. E. 2007 Free-energy and the brain Synthese 159(3), 417â€“
458
[23] Ramstead, M.J.D.; Friston, K. J.; HipÂ´olito, I. 2020 Is the free energy principle a formal
theory of semantics? From variational density dynamics to neural and phenotypic
representations. Entropy 22, 889.
[24] Friston, K. J. 2019 A free energy principle for a particular physics. Preprint
arXiv:1906.10184 [q-bio.NC]. https://arxiv.org/abs/1906.10184
[25] Varshney, L. R.; Chen, B. L.; Paniagua, E.; Hall, D. H.; Chklovskii, D. B. 2011
Structural properties of the Caenorhabditis elegans neuronal network. PLoS Comp.
Biol. 7, e1001066.
[26] Herculano-Houzel, S. 2011 Scaling of brain metabolism with a ï¬xed energy budget
per neuron: Implications for neuronal activity, plasticity and evolution. PLoS One 6,
e17514.
[27] Friston, K.; Levin, M.; Sengupta, B.; Pezzulo, G. 2015 Knowing oneâ€™s place: A free-
energy approach to pattern regulation. J. R. Soc. Interface 12, 20141383.
[28] Kuchling, F.; Friston, K.; Georgiev, G.; Levin, M. 2020 Morphogenesis as Bayesian
inference: A variational approach to pattern formation and control in complex bio-
logical systems. Phys. Life Rev. 33, 88â€“108.
[29] Kiebel, S. J., Friston, K, J. 2011 Free energy and dendritic self-organization. Frontiers
in Systems Neuroscience 5, 80 (13 pp).
[30] Bastos AM; Usrey WM; Adams RA; Mangun GR; Fries P; Friston KJ. 2012 Canonical
microcircuits for predictive coding. Neuron 76, 695â€“711.
[31] Shipp, S., Adams, R. A., Friston, K. J. (2013). Reï¬‚ections on agranular architecture:
Predictive coding in the motor cortex. Trends in Neuroscience 36, 706â€“716.
[32] Kanai, R., Komura, Y., Shipp, S., Friston, K. (2015). Cerebral hierarchies: Predictive
processing, precision and the pulvinar. Philosophical Transactions of the Royal Society
B 370, 20140169.
[33] Adams, R. A., Friston, K. J., Bastos, A. M. (2015). Active inference, predictive coding
and cortical architecture. In M. F. Casanova, I. Opris (Eds.), Recent advances in the
modular organization of the cortex (pp. 97â€“121). Berlin: Springer.
[34] Clark A. 2013 Whatever next? Predictive brains, situated agents, and the future of
cognitive science. Behav Brain Sci 36, 181â€“204.
31

[35] Hohwy J. 2013 The predictive mind. Oxford University Press, Oxford, UK.
[36] Seth AK. 2013 Interoceptive inference, emotion, and the embodied self. Trends Cogn
Sci 17(11), 565â€“573.
[37] Friston KJ, Rigoli F, Ognibene D, Mathys C, FitzGerald T, Pezzulo G. 2015 Active
inference and epistemic value. Cognit Neurosci 6, 187â€“214.
[38] Aharonov, Y.; Kaufherr, T. 1984 Quantum frames of reference. Phys. Rev. D 30,
368â€“385.
[39] Bartlett, S.D.; Rudolph, T.; Spekkens, R.W. 2007 Reference frames, superselection
rules, and quantum information. Rev. Mod. Phys. 79, 555â€“609.
[40] Fields, C.; Marcian`o, A. 2019 Sharing nonfungible information requires shared non-
fungible information. Quant. Rep. 1, 252â€“259.
[41] Barwise, J.; Seligman, J. 1997 Information Flow: The Logic of Distributed Systems
(Cambridge Tracts in Theoretical Computer Science 44). Cambridge University Press,
Cambridge, UK.
[42] Fields, C.; Glazebrook, J. F. 2019 A mosaic of Chu spaces and Channel Theory I:
Category-theoretic concepts and tools. J. Expt. Theor. Artif. intell. 31, 177â€“213.
[43] Fields, C.; Glazebrook, J. F. 2019 A mosaic of Chu spaces and Channel Theory II:
Applications to object identiï¬cation and mereological complexity. J. Expt. Theor.
Artif. intell. 31, 237â€“265.
[44] Fields, C.; Glazebrook, J. F. 2020 Do Process-1 simulations generate the epistemic
feelings that drive Process-2 decision making? Cogn. Proc. 21, 533â€“553.
[45] Fields,
C.;
Glazebrook,
J.
F.
2021
Information
ï¬‚ow
in
context-dependent
hierarchical Bayesian inference. J. Expt. Theor. Artif. intell. in press (doi:
10.1080/0952813X.2020.1836034).
[46] Fields, C.; Marcian`o, A. 2019 Holographic screens are classical information channels.
Quant. Rep. 2, 326â€“336.
[47] Fields, C.; Glazebrook, J. F. 2020 Representing measurement as a thermodynamic
symmetry breaking. Symmetry 12, 810.
[48] Fields, C.; Glazebrook, J. F.; Marcian`o, A. 2021 Reference frame induced symmetry
breaking on holographic screens. Symmetry 13, 408.
[49] Fields, C; Levin, M. 2020 How do living systems create meaning? Philosophies 5, 36.
[50] Fields, C.; Glazebrook, J. F.; Levin, M. 2021 Minimal physicalism as a scale-free
substrate for cognition and consciousness. Neurosci. Cons. 7(2), niab013.
32

[51] SchrÂ¨odinger, E. 1944 What is Life? Cambridge, UK: Cambridge University Press.
[52] Hameroï¬€, S.; Penrose, R. 1996 Orchestrated reduction of quantum coherence in brain
microtubules: A model for consciousness Math. Comput. Simul. 40, 453â€“480. (doi:
10.1016/0378-4754(96)80476-9)
[53] Bordonaro,
B.;
Ogryzko,
V.
2013
Quantum
biology
at
the
cellular
level
â€“
Elements
of
the
research
program.
BioSystems
112,
11â€“30.
(doi:
10.1016/j.biosystems.2013.02.008)
[54] Tononi, G.; Koch, C. 2015 Consciousness here, there and everywhere? Philos. Trans.
R. Soc. B 215, 216â€“242.
[55] Georgiev, D.D. 2020 Quantum information theoretic approach to the mind-brain
problem. Prog. Biophys. Mol. Biol. 18, 16â€“32.
[56] Arndt M. T.; Juï¬€mann, T.; Vedral, V. 2009 Quantum physics meets biology. HFSP
J. 3, 386â€“400. (doi: 10.2976/1.3244985)
[57] Lambert, N.; Chen, Y.-N.; Cheng, Y.-C.; Li C.-M.; Chen, G.-Y.; Nori, F. 2012
Quantum biology. Nat. Phys. 9, 10â€“18. (doi: 10.1038/NPHYS2474)
[58] Melkikh, A. V.; Khrennikov, A. 2015 Nontrivial quantum and quantum-like eï¬€ects
in biosystems: Unsolved questions and paradoxes. Prog. Biophys. Mol. Biol. 119,
137â€“161. (doi: 10.1016/j.pbiomolbio.2015.07.001)
[59] Marais, A. et al. 2018 The future of quantum biology. J. R. Soc. Interface 15,
20180640. (doi: 10.1098/rsif.2018.0640)
[60] Cao, J. et al. 2020 Quantum biology revisited. Science Adv. 6 eaaz4888 (doi:
10.1126/sciadv.aaz4888)
[61] Brookes J. C. 2017 Quantum eï¬€ects in biology: Golden rule in enzymes, olfaction,
photosynthesis and magnetodetection. Proc. R. Soc. A 473, 20160822.
[62] McFadden J, Al-Khalili J. 2018 The origins of quantum biology. Proc. R. Soc. A 474,
20180674.
[63] Fields, C.; Levin, M. 2021 Metabolic limits on classical information processing by
biological cells. BioSystems 209, 104513.
[64] Addazi, A.; Chen, P.; Fabrocini, F.; Fields, C.; Greco, E.; Lulli, M.; Marcian`o, A.;
Pasechnik, R. 2021 Generalized holographic principle, gauge invariance and the emer-
gence of gravity `a la Wilczek. Front. Astron. Space Sci. 8, 563450. (doi: 10.3389/fs-
pas.2021.563450)
[65] Landauer, R. 1961 Irreversibility and heat generation in the computing process. IBM
J. Res. Dev. 5, 183â€“195.
33

[66] Landauer, R. 1999 Information is a physical entity. Physica A 263, 63â€“67.
[67] Bennett, C.H. 1982 The thermodynamics of computation. Int. J. Theor. Phys. 21,
905â€“940.
[68] Fuchs, C. A.; Schack, R. 2013 Quantum-Bayesian coherence. Rev. Mod. Phys. 85,
1693â€“1715.
[69] Mermin, N. D. 2019 Making better sense of quantum mechanics. Rep. Preg. Phys. 82,
012002.
[70] Rovelli, C. 1996 Relational quantum mechanics. Int. J. Mod. Phys. 35, 1637â€“1678.
[71] Pearl, J. 1988 Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. San Mateo CA: Morgan Kaufmann.
[72] Clark A. 2017 How to knit your own Markov blanket: Resisting the second law with
metamorphic minds. In (T. Wetzinger and W. Wiese, eds.) Philosophy and Predictive
Processing 3, 19pp. Frankfurt am Mainz Mind Group.
[73] Fields, C.; Marcian`o, A. 2020 Markov blankets are general physical interaction sur-
faces. Phys. Life Rev. 33, 109â€“111.
[74] Bell, J. S. 1964 On the Einsteinâ€“Podolskyâ€“Rosen paradox. Physics 1, 195â€“200.
[75] Emary, C.; Lambert, N.; Nori, F. 2014 Leggettâ€“Garg inequalities. Rep. Prog. Phys.
77, 039501.
[76] Kochen, S.; Specker, E.P. 1967 The problem of hidden variables in quantum mechan-
ics. J. Math. Mech. 17, 59â€“87.
[77] Mermin, D. 1993 Hidden variables and the two theorems of John Bell. Rev. Mod.
Phys. 65, 803â€“815.
[78] Dzharfarov, E.N.; Kon, M. 2018 On universality of classical probability with contex-
tually labeled random variables. J. Math. Psych. 85, 17â€“24.
[79] Horsman C; Stepney S; Wagner RC; Kendon V. 2014 When does a physical system
compute? Proc. R. Soc. A 470, 20140182.
[80] Smith, J. E.; Nair, R. 2005 The architecture of virtual machines. IEEE Comp. 38(5),
32â€“38.
[81] Barr, M. 1979 *-Autonomous Categories, with an Appendix by Po Hsiang Chu; Lec-
ture Notes in Mathematics 752; Springer: Berlin, Germany.
[82] Pratt, V. 1999 Chu spaces. In School on Category Theory and Applications (Coimbra
1999); Volume 21 of Textos Mat. SÂ´er. B; University of Coimbra: Coimbra, Portugal,
pp. 39â€“100.
34

[83] Pratt, V. 1999 Chu spaces from the representational viewpoint. Ann. Pure Appl. Log.
96, 319â€“333.
[84] Baars, B. J.;Franklin, S. 2003 How conscious experience and working memory interact.
Trends in Cognitive Science 7, 166â€“172.
[85] Dehaene, S.; Naccache, L. 2001 Towards a cognitive neuroscience of consciousness:
Basic evidence and a workspace framework. Cognition 79, 1â€“37.
[86] Mashour, G. A.;Roelfsema, P.; Changeux, J.-P.; Dehaene, S. 2020 Conscious process-
ing and the Global Neuronal Workspace hypothesis. Neuron 105, 776â€“798.
[87] Allwein, G.; Moskowitz, I.S.; Chang, L.-W. 2004 A New Framework for Shannon
Information Theory. Technical Report A801024 Naval Research Laboratory, Wash-
ington, DC, USA, 17p.
[88] Barwise, J. 1997 Information and impossibilities. Notre Dame Journal of Formal Logic
38(4), 488â€“515.
[89] Cherniak, E. 1991 Bayesian networks without tears. AI Magazine 12(4), 50â€“63.
[90] Prakash, C.; Fields, C.; Hoï¬€man, D. D.; Prentner, R.; Singh, M. 2020 Fact, ï¬ction,
and ï¬tness. Entropy 22, 514.
[91] Fields C, Friston K, Glazebrook JF, Levin M 2021 A free energy principle for generic
quantum systems. In review (available as arXiv:2112.15242 [quant-ph]).
[92] Lyon, P. 2015 The cognitive cell: Bacterial behavior reconsidered. Front. Microbiol.
6, 264.
[93] Micali, G.; Endres, R.G. 2016 Bacterial chemotaxis: Information processing, thermo-
dynamics, and behavior. Curr. Opin. Microbiol. 30, 8â€“15.
[94] Levin, M. 2012 Morphogenetic ï¬elds in embryogenesis, regeneration, and cancer: Non-
local control of complex patterning. Biosystems 109, 243â€“261.
[95] Levin, M.; Pezzulo, G.; Finkelstein, J. M. 2017 Endogenous bioelectric signaling
networks: Exploiting voltage gradients for control of growth and form. Annu. Rev.
Biomed. Eng. 19, 353â€“387.
[96] Srivastava, P.; Kane, A.; Harrison, C.; Levin, M. 2021 A meta-analysis of bioelectric
data in cancer, embryogenesis, and regeneration. Bioelectricity 3, 42â€“67.
[97] Wuichet, K.; Cantwell, B. J.; Zhulin, I. B. 2010 Evolution and phyletic distribution
of two-component signal transduction systems. Curr. Opin. Microbiol. 13, 219â€“225.
[98] Basieva, I; Khrennikov, A; Ohya, M; Yamato, O. 2011 Quantum-like interference
eï¬€ect in gene expression: glucose-lactose destructive interference. Syst. Synth. Biol.
5, 59â€“68.
35

[99] Inada, T; Kimata, K; Aiba, H. 1996 Mechanism responsible for glucose-lactose diauxie
in Eschericha coli challenge to the cAMP model. Genes Cell 1, 293â€“301.
[100] Loh, K. M.; van Amerongen, R.; Nusse, R. 2016 Generating cellular diversity and
spatial form: Wnt signaling and the evolution of multicellular animals. Dev. Cell. 38,
643â€“655.
[101] Kolch, W. 2005 Coordinating ERK/MAPK signaling through scaï¬€olds and inhibitors.
Nat. Rev. Mol. Cell Biol. 6, 827â€“838.
[102] Schwanbeck, R.; Martini, S.; Bernoth, K.; Just, U. 2011 The notch signaling pathway:
molecular basis of cell context dependency. Eur Cell Biol 90(6-7), 572â€“581.
[103] Guo, X.; Wang, X.-F. 2009 Signaling cross-talk between TGF-Î²/BMP and other
pathways. Cell Res. 19, 71â€“88.
[104] Hunter, T. 2000 Signaling and beyond. Cell 100, 113â€“127.
[105] Adamska, M. 2015 Developmental signalling and emergence of animal multicellularity.
In: Evolutionary Transitions to Multicellular Life, edited by Ruiz-Trillo I, Nedelcu
AM. Dordrecht: Springer, pp. 425â€“450
[106] Fields, C.; Levin, M. 2018 Multiscale memory and bioelectric error correction in the
cytoplasm-cytoskeleton-membrane system. WIRES Syst. Biol. Med. 10, e1410.
[107] HipÂ´olito, I; Ramstead, M. J. D.; Convertino, L.; Bhat. A.; Friston, K.; Parr, T. 2021
Markov blankets in the brain. Neuroscience and Biobehavioral Reviews 125, 88â€“97.
[108] Palacios, E. R.; Razi, A.; Parr, T.; Kirchoï¬€, M.; Friston, K. 2020 On Markov blankets
and hierarchical self-organization. J. Theoretical Biology 486, 110089.
[109] Peters, A.; McEwen, B. S.; Friston, K. 2017 Uncertainty and stress: why it causes
diseases and how it can be mastered by the brain. Progress in Neurobiology 156,
164â€“188.
[110] Spruston, N. 2008 Pyramidal neurons: Dendritic structure and synaptic integration.
Nat. Rev. Neurosci. 9, 206â€“221.
[111] Rasia-Filho, A. A.; Guerra, K. T. K.; VÂ´asquez, C. E.; Dallâ€™Oglio, A.; Reberger,
R.; Jung, C. R.; Calcagnotto, M. E. 2021 The subcortical-allocortical-neocortical
continuum for the emergence and morphological heterogeneity of pyramidal neurons
in the human brain. Front. Synapt. Neurosci. 13, 616607.
[112] Lloyd, S. 2000 Ultimate physical limits to computation. Nature 406, 1047â€“1054.
[113] Attwell, D.; Laughlin S. B. 2001 An energy budget for signaling in the grey matter
of the brain. J. Cereb. Blood Flow Metab. 21, 1133â€“1145
36

[114] Sengupta, B.; Stemmler, M. B.; Friston, K. J. 2013 Information and eï¬ƒciency in the
nervous system: A synthesis. PLoS Comput Biol 9(7), e1003157.
[115] Georgiev, D; Kolev, S; Cohen, E; Glazebrook, JF. 2020 Computational capacity of
pyramidal neurons in the cerebral cortex. Brain Research 1748, 147069.
[116] Deutsch, D. 2002 The structure of the multiverse. Proc. R. Soc. A 458, 2911â€“2923.
[117] Atiyah, M. 1988 Topological quantum ï¬eld theory. Pub. Math. IH`ES 68, 175â€“186.
[118] Marcian`o, A.; Chen, D.; Fabrocini, F.; Fields, C.; Greco, E.; Gresnigt, N.; Jinklub,
K.; Lulli, M., Terzidis, K.; Zappala, E. 2021 Deep neural networks as the semi-classical
limit of quantum neural networks. Preprint arXiv:2007.00142v2 [cond-mat.diss-nn].
https://arxiv.org/abs/2007.00142
[119] Eyal, G.; Verhoog, M. B.; Testa-Silva, G.; Deitcher, Y.; Benavides-Piccione, R.;
DeFelipe, J.; de Kock, C. P.J.; Mansvelder, H. D.; Segev, I. 2018 Human cortical
pyramidal neurons:from spines to spikes via models. Front. Cellular Neurosci. 12,
181.
[120] Galloni, A. R.; Laï¬€ere, A.; Rancz, E. 2020 Apical length governs computational
diversity of layer 5 pyramidal neurons. eLife 9, e55761.
[121] Major, G; Larkum, M. E.; Schiller, J. 2013 Active properties of neocortical pyramidal
neuron dendrites. Annu. Rev. Neurosci. 36, 1â€“24.
[122] Shipp, S. 2007 Structure and function of the cerebral cortex. Curr. Biol. 17, R443â€“
R449.
[123] Shipp, S. 2016 Neural elements for predictive coding. Front. Psychol. 7, 1792.
[124] Swanson, L. W.; Lichtman, J. W. 2016 From Cajal to connectome and beyond. Annu.
Rev. Neurosci. 39, 197â€“216.
[125] VÂ´elez-Fort, M.; Rousseau, C. V.; Niedworok, C. J.; Wickersham, I. R.; Rancz, E.
A.; Brown, A. P. Y.; Strom, M.; Margrie, T. W. 2014 The stimulus selectivity and
connectivity of Layer Six principal cells reveals cortical microcircuits underlying visual
processing. Neuron 83, 1431â€“1443.
[126] De Nardo, L. A.; Berns, D. C.; DeLoach, K.; Luo, L. 2015 Connectivity of mouse
somatosensory and prefrontal cortex examined with trans-synaptic tracing. Nat. Neu-
rosci. 18, 1687â€“1697.
[127] Harris, K. D.; Shepherd, G. M. G. 2015 The neocortical circuit: Themes and varia-
tions. Nat. Neurosci. 18, 170â€“181.
[128] Mennerick S, Zorumsky CF. 2000 Neural activity and survival in the developing
nervous system. Mol. Neurobiol. 22, 41â€“54.
37

[129] Faust TE, Gunner G, Schafer DP. 2021 Mechanisms governing activity-dependent
synaptic pruning in the developing mammalian CNS. Nature Rev. Neurosci. 22, 657â€“
673.
[130] Deitcher Y.; Eyal G.; Kanari L.; et al. 2007 Comprehensive morpho-electronic analysis
shows 2 distinct classes of L2 and L3 pyramidal neurons in human temporal cortex.
Cereb. Cortex 27, 5398â€“5414.
[131] Mohan, H.; Verhoog, M. B.; Doreswamy, K. K.; et al. 2015 Dendritic and axonal
architecture of individual pyramidal neurons across layers of adult human neocortex.
Cereb. Cortex 25, 4839â€“4853.
[132] Beaulieu-Laroche, L; Toloza, E. H. S.; van der Goes, M. S. et al. 2018 Enhanced
dendritic compartmentalization in human cortical neurons. Cell 175, 643â€“651.
[133] Nielsen, M. A.; Chuang, I. L. 2000 Quantum Computation and Quantum Information.
New York, Cambridge University Press.
[134] Luo, L. 2021 Architectures of neuronal circuits. Science 373, eabg7285.
[135] Johansson, C.; Lansner, A. 2007 Towards cortex sized artiï¬cial neural systems. Neural
Networks 20, 48â€“61.
[136] Chen, R.; Cohen, L. G.; Hallett, M. 2002 Nervous system reorganization following
injury. Neuroscience 111, 761â€“773.
[137] Demekas, D.; Parr, T.; Friston, K. J. 2020 An ivestigation of the free energy principle
for emotional recognition. Frontiers in Computational Neuroscience 14, 30.
[138] Levin, M. 2019 The computational boundary of a â€œselfâ€: Developmental bioelectricity
drives multicellularity and scale-free cognition. Front. Psychol. 10, 1688.
[139] Levin, M. 2014 Endogenous bioelectrical networks store non-genetic patterning infor-
mation during development and regeneration. J. Physiol. 592, 2295â€“2305.
[140] Levin, M. 2021 Bioelectric signaling: Reprogrammable circuits underlying embryoge-
nesis, regeneration, and cancer. Cell 184, 1971â€“1989.
[141] Bates, E. 2015 Ion channels in development and cancer. Annu. Rev. Cell. Devel. Biol.
31, 231â€“247.
[142] Harris, M. P. 2021 Bioelectric signaling as a unique regulator of development and
regeneration. Development 148, dev180794.
[143] Mathews, J.; Levin, M. 2018 The body electric 2.0: Recent advances in developmental
bioelectricity for regenerative and synthetic bioengineering. Curr. Opin. Biotechnol.
52, 134â€“144.
38

[144] Levin, M. 2021 Life, death, and self: Fundamental questions of primitive cognition
viewed through the lens of body plasticity and synthetic organisms. Biochem. Biophys.
Res. Commun. 564, 114â€“133.
[145] Vandenberg, L. N.; Adams, D. S.; Levin, M. 2012 Normalized shape and location of
perturbed craniofacial structures in the Xenopus tadpole reveal an innate ability to
achieve correct morphology. Devel. Dyn. 241, 863â€“878.
[146] Pinet, K.; McLaughlin, K. A. 2019 Mechanisms of physiological tissue remodeling
in animals: Manipulating tissue, organ, and organism morphology. Devel. Biol 451,
134â€“145.
[147] Pinet, K.; Deolankar, M.; Leung, B.; McLaughlin, K. A. 2019 Adaptive correction
of craniofacial defects in pre-metamorphic Xenopus laevis tadpoles involves thyroid
hormone-independent tissue remodeling. Development 146, dev175893.
[148] Emmons-Bell, M.; Durant, F.; Hammelman, J.; Bessonov, M.; Volpert, V.; Mo-
rokuma, J.; Pinet, K.; Adams, D. S.; Pietak, A.; Lobo, D.; Levin, M. 2015 Gap
junctional blockade stochastically induces diï¬€erent species-speciï¬c head anatomies in
genetically wild-type Girardia dorotocephala ï¬‚atworms. Int. J. Mol. Sci. 16, 27865â€“
27896.
[149] Kriegman, S.; Blackiston, D.; Levin, M.; Bongard, J. 2020 A scalable pipeline for
designing reconï¬gurable organisms. Proc. Natl. Acad. Sci. USA 117, 1853â€“1859.
[150] Blackiston, D.; Lederer, E.; Kriegman, S.; Garnier, S.; Bongard, J.; Levin, M. 2021
A cellular platform for the development of synthetic living machines. Sci. Robot. 6,
eabf1571.
[151] Pai, V. P.; Pietak, A.; Willocq, V.; Ye, B.; Shi, N.-Q.; Levin, M. 2018 HCN2 Rescues
brain defects by enforcing endogenous voltage pre-patterns. Nat. Comms. 9, 998.
[152] Pai, V. P.; Lemire, J. M.; ParÂ´e, J.-F.; Lin, G.; Chen,Y.; Levin, M. 2015 Endogenous
gradients of resting potential instructively pattern embryonic neural tissue via Notch
signaling and regulation of proliferation. J. Neurosci 35, 4366â€“4385.
[153] Vandenberg, L. N.; Morrie, R. D.; Adams, D. S. 2011 V-ATPase-dependent ecto-
dermal voltage and pH regionalization are required for craniofacial morphogenesis.
Devel. Dyn. 240, 1889â€“1904.
[154] Pezzulo, G.; Lapalme, J.; Durant, F.; Levin, M. 2021 Bistability of somatic pattern
memories: Stochastic outcomes in bioelectric circuits underlying regeneration. Phil.
Proc. R. Soc. B 376, 20190765.
[155] Durant F.; Morokuma, J.; Fields, C.; Williams, K.; Adams, D. S.; Levin, M. 2017
Long-term, stochastic editing of regenerative anatomy via targeting endogenous bio-
electric gradients. Biophys. J. 112, 2231â€“2243.
39

[156] Oviedo, N. J.; Morokuma, J.; Walentek, P.; Kema, I. P.; Gu, M. B.; Ahn, J.-M.;
Hwang, J. S.; Gojobori, T.; Levin, M. 2010 Long-range neural and gap junction
protein-mediated cues control polarity during planarian regeneration. Devel. Biol.
339, 188â€“199.
[157] Durant, F.; Bischof, J.; Fields, C.; Morokuma, J.; LaPalme, J.; Hoi, A.; Levin,
M. 2019 The role of early bioelectric signals in the regeneration of planarian ante-
rior/posterior polarity. Biophys. J. 116, 948â€“961.
[158] Koshland, D. E. 1983 The bacterium as a model neuron. Trends Neurosci 6, 133â€“137.
[159] Prindle, A.; Liu, J.; Asally, M.; Ly, S.;Garcia-Ojalvo, J.; SÂ¨uel, G. M. 2015 Ion channels
enable electrical communication in bacterial communities. Nature 527, 59â€“63.
[160] Pezzulo, G.; Levin, M. 2015 Re-membering the body: Applications of computational
neuroscience to the top-down control of regeneration of limbs and other complex
organs. Integr. Biol. (Cambridge) 7, 1487â€“1517.
[161] Watson, R. A.; Buckley, C. L.; Mills, R.; Davies, A. 2010 In Artiï¬cial Life Conference
XII. (Odense, Denmark, 2010), pp. 194â€“201.
[162] Biswas, S.; Manicka, S.; Hoel, E.; Levin, M. 2021 Gene regulatory networks exhibit
several kinds of memory: Quantiï¬cation of memory in biological and random tran-
scriptional networks. iScience 24, 102131.
[163] Emmons-Bell, M., Durant, F.; Tung, A.; Pietak, A.; Miller, K.; Kane, A.; Martyniuk,
C. J.; Davidian, D.; Morokuma, J.; Levin, M. 2019 Regenerative adaptation to elec-
trochemical perturbation in planaria: A molecular analysis of physiological plasticity.
iScience 22, 147â€“165.
[164] Jacob, E. B.; Aharonov, Y.; Shapira, Y. 2004 Bacteria harnessing complexity. Bioï¬lms
1, 239â€“263.
[165] Goel, P.; Mehta, A. 2013 Learning theories reveal loss of pancreatic electrical connec-
tivity in diabetes as an adaptive response. PLoS One 8, e70366.
[166] Zoghi, M. 2004 Cardiac memory: Do the heart and the brain remember the same? J.
Interv. Card. Electrophysiol. 11, 177â€“182.
[167] Chakravarthy, S. V.; Ghosh, J. 1997 On Hebbian-like adaptation in heart muscle: A
proposal for â€˜cardiac memoryâ€™. Biol. Cybern. 76, 207â€“215.
40

