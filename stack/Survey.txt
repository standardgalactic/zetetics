SURVEY OF “UNDERSTANDING”
DEFINITIONS 
Northwest AGI Forum 
Mike Archbold 
Copyright September 2021
“to understand is to explain why something is the way it is. 
is to be able to explain why you did something. 
is to be able to explain why others did things.”
doddy  

"Understanding is a model that allows accurate prediction and explanation of 
phenomena." ~ Michael S. P. Miller 
source: email

Understanding is based on knowledge that has the potential to act as partial explainability 
and which  can be adaptive to new knowledge or experiences which fit well into that 
knowledge. I may not be able  to explain exactly how my car works but I have partial 
knowledge of operating it, some insight about  how it works, and why I depend on it. 
There are a lot of possible impediments to explainability and to adaptability. Human beings 
are not  capable of perfect explanations or of perfect adaptability. You can question whether 
"perfect  adaptability" even makes sense. On the other hand, a lack of adaptability as an 
agent has opportunities  to learn will become more obvious over a period of time if those 
should-be learning experiences are  obvious. And the ability to think about the reasons why 
you did something, or to entertain conjectural  reasons why some kind of event occurred 
seems like fundamental processes of thinking to me. 
Jim Bromer 
source: email

In my definition understanding is the ability of a system to describe a new concept as a set of
characteristics, the values of these characteristics and/or allowable ranges of these values,
and  functions  of  relationships  between  values  of  these  characteristics,  both  within  one
concept  and  between  values  of  characteristics  of  different  concepts  (other  new  or  old
concepts). Moreover, some characteristic or its value can be presented as another new
concept or as previously described "old" concept. The basic concepts/characteristics on
which the system builds other concepts are system input signals (signals from system’s
receptors). 
In  the  mentioned  system  each  characteristic  and  their  value  can  be  represented,
respectively, as a signal from the element of the system that detects this characteristic and
the magnitude of this signal. The function of the relationship between values of characteristics
can  be  represented  in  this  system  as  the  relationship  between  signals  generated  by
corresponding elements of system that detect these characteristics. In such system this
relationship between signals can be implemented as a relationship (connection) between
elements of the system that generate these signals. 
Different systems that have the abilities described above may represent one and the
same concept as different sets of characteristics, their values and functions of relationships.
Also, these systems may have different numbers of such interconnected multilevel concepts -
different depths of the internal map of the world. Therefore these systems will have different
understanding of this concept with different depth of understanding.  
For an example, some simple system can detect different dogs in pictures as sets of
different multicolored spots, detecting at each its level new concept like a combination of
these multicolored spots, a combination of their combination and so on. This system will have
its own level of understanding of what dog is. For this system, it is a set of interconnected
“meaningless” (which do not represent really existed distinguished objects or processes in the
real world) multicolored spots. To build a new concept of a cat, this system must use a new
sets of interconnected multicolored spots. On the other hand, a more complex system may be
capable to detect concepts such as paw, tail, ear, eye, distance, position and so on (really
existing distinguished objects or processes in the real world), and build the concept of a dog
like relationships of these concepts. To build a new concept of a cat, this system can use
those concepts that it can already detect (paw, tail, ear, eye, distance, position and so on).
This system can “learn” a new concept much faster as it does not need to build a huge sets of
new subconcepts, it mast only build a few new concepts that it has not yet built and that
specific to the concept of cat, if necessary modify some old concepts, and build from them
new concept of cat. More complex system can be capable to detect much more of such
“meaningful” concepts and interrelationships of these concepts. This can give for such system
the ability to connect concepts such as dog with concepts such as: animal, finger, blood, food,
bark, run, happy and so on. Although at the lowest level, all of these concepts will be based
on some kind of combination of signals from the receptors (multicolored spots for visual data).
All  of  the  described  systems  have  their  own  understanding  of  what  dog  is,  but  their
understanding and depth of understanding are different. Therefore, their ability to interact with
this concept, to find similarities and relationships between this concept and other concepts
and so on are different. 
In the case of systems like GPT-3 things are worse. Systems of this type at their
receptive (base) level have sets of words and sentences that, in turn, are not a direct
reflection of real world objects (processes), but are an encrypted reflection of the world map
of a certain system that created these

words. Therefore all concepts that this system (GPT-3 like) can build are combinations of
these  words  and  sentences  with  the  relationships  between  them.  Any  concept  which
represents an object (process) for this system is just a “meaningless” (which do not represent
really existing distinguished objects or processes in the real world) "multiworded" spots. Even
a very complex system of this type can only understand the different types of sentences and
their combinations, perhaps some grammar rules, some general phrases, and so on. For this
system, the whole world is just sets of words. 
In conclusion, I think that sales/marketing people that claim their AI “understand” are
right. But the understanding that their system has is not the understanding that the intelligent
system of the human level has. 
I hope my vision will be helpful. 
~~~~ 
The definition of the term “understanding” which I described in a previous email describes a
basic level of understanding (as I understand it), but often term “understand” used as high-
level understanding that may include even processes of awareness and consciousness, and,
from my point of view, is a result of  
interactions of certain types of chains of elements of intelligent system that detect basic 
concepts, and  are not possible without this basic “understanding”. 
Valentin Scherbak  
source: email

Understanding is the result of subconscious learning over a lifetime using an algorithm that 
we might  as well call "Intuition" that jumps to conclusions on scant evidence. 
We are only born with instincts. There are very few of those. Everything else is learned 
subconsciously, starting with vision, or consciously, such as memorizing facts and Models. 
Everything we perceive, what is in our visual field, what we hear, language we hear or read, 
and any  physical skill like walking, talking, skiing or tennis, is based on Understanding. When
using these  skills, you do not have time to Reason. See "Libet Delay". And anything that gets
better with practice is based on Understanding. It is a slow process of learning from micro-
mistakes and correcting them a  little at the time. All Understanding systems learn from their 
own mistakes, including brains. Building  an "AI" that is not learning from its mistakes is mere 
programming, and such efforts will never become  anything worthy of the terms "Intelligence" 
or "Understanding". 
If you can Reason your way to a correct answer without practicing any skills (emphasis 
mine) then  it's Reasoning (on top of, and based on, your Understanding). 
Monica Anderson 
source: email

1/ Understanding is a process in which one’s experiences are  
known to be true, reliable and logically consistent. 
2/ Understanding is “knowing with certainty”. 
And here is the one from Wikipedia: 
Understanding is a psychological process related to an abstract or physical object,  
such as a person, situation, or message whereby one is able to use concepts to model that
object.  
Understanding is a relation between the knower and an object of understanding.  
Understanding implies abilities and dispositions with respect to an object of 
knowledge that are sufficient to support intelligent behavior. 
~~~~~ 
Here is another definition which adds the concept of applying/using the 
knowledge and tries to explains the terms being used in the definition. 
Understanding is having an in-depth knowledge of a subject domain such that one can
reliably apply it. 
Where:  
1. in-depth means detailed, underlying principles, 
2. knowledge is spatial and temporal, structural and functional, cause and effect and
relational, 
3. to apply is to use, teach, explain, recognize, relate, manage, control, respond to, rephrase,
answer etc.
~~~~~~
A definition that can be used to help construct an AGI because it describes key properties
of what understanding is comprised.
Noun:

As a thing: An understanding is a model of something that you believe to be true because of your 
experiences.
As a process: Understanding is something you do to create the model.
Verb:
To understand is to have such a model. The model is a mental representation of the structure,
composition, relationships and operation of the thing that you understand.
Based on the model's level of detail, consistency and completeness you may have the ability to
recognize, predict, reason about, describe, explain, manage and/or use the thing.
Brett Martensen 
source: email

Summary: “‘understanding’ at its simplest is ‘being able to respond appropriately’ — a purely  
behavioral measure. The next level is to be able to tell back the information in your own 
words.  Another level is knowing the implications. Personal experience takes us to a much 
higher level of  integration — this is where the connection to the ‘I’ comes in. Having deep 
subject knowledge and skill is another dimension. Finally, knowing how something functions 
in detail — knowing the specific  cause-effect mechanisms.” 
Peter Voss  
source: https://chatbotslife.com/understanding-understanding-9dcc15759b5b 
video too: https://www.youtube.com/watch?v=02OtOg6g6vU

no, there is no short definition. 
this is one of the qualia that forever will have a different context in different people's mind. 
you should interpret the word "story" in a general context (any sequence of perceived events)
like  "narrative". It is not about written stories like books. in my view upon "thinking" and 
"awareness" and  "understanding", it is crucial that the system can IDENTIFY the beings or 
processes that cause  (observed) things happen. I call these the "actors", so actors are not 
always persons, rather you can not  see an action without knowing the actor (because then 
you have not understood the action). 
1. ability to classify the items in the story (tell of which type they are) 
2. ability to identify the items in the story (tell which specific instance of the class they 
are, e.g.  recognize person instead of only knowing it is a pedestrian) 
3. ability to determine which found classes and instances are actors (e.g. persons, 
machines but  also "the weather" like in "it is raining" the weather is the actor doing the
action raining) 4. ability to isolate changes that were actions 
5. ability to identify responses that followed those actions 
6. ability to combine the correct actions and reactions into interactions (which are 
steps for the  scenario or script needed later) 
7. ability to assign perceived actions to perceived actors in the story (needed to convert 
the story  data format into a scenario data format) 
8. ability to translate scenario to assign one-self as one of the actors (meaning 
reproduced/execute  the observed sequence of interactions) 
9. ability to tell something about each noun and pronoun that occurs in a text or each 
thing/being  (that occurs in the story) and each relation between any of them. 
10. ability to either paint or describe (visual shape) or point (locate in space) each real-
world thing in the story 
11. ability to tell what the further implications are of the told story (not only those 
already  told in the story). 
12. ability to rephrase the sentence or story (tell the same thing with different words). 
13. ability to tell which aspects are unusual (out of the ordinary) about the story. 14. 
ability to tell what is salient about the story (what one did not knew before and learned  
from the new sentence(s)). 
That is my short version, you will notice it is almost pseudocode for reasons you can also 
guess. 
Stephan Verbeeck 
source: email

To me understanding a concept means that the entity (human, robot, monkey, etc.  can act
appropriately on that concept. If I tell my robot to get me a glass of water,  there are multiple
concepts here that the robot needs to understand at some level. Let's just take the "glass of
water" idea. A glass is fairly easy to define. We can take  
the easy way out and use ML to recognize a glass. But we really need to add  something on the 
functionality of the glass as a container, then we need to  understand the concept of container. 
Again, we need to define the functionality. Water is another concept. do we mean just any water, 
clean water, dish water, toilet  water. So the robot needs to understand the concept of how the 
water is going to be  used, and how to source the water we want. The word of needs to be 
understood to  mean containing. So to understand a simple phrase like this requires the robot to  
understand a lot of concepts, and how they are related. I think this is what we really  want in 
terms of an AGI. 
[9:55 AM] The other thing we can do is train the robot to recognize the phrase, rather  than the 
individual words, and translate the phrase into a set of action that we want  the robot to 
accomplish. We get the same result, but the level of understanding is  much less. Although there 
is still some level of understanding, it is on the phrase  level, rather than the word level. So 
understanding seems to have levels, much as  we have talked about levels of intelligence. 
Robert Higgins 
source: Slack channel “IntelligentAgent”

Understanding is recognition. Which is a lossless component of compression from 
comparing input to  template. That's all there is to it, and it applies to countless other words 
that mean the same thing. 
Boris Kazachenko 
Source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-
machine understand-what-is-your-definition-of-understanding-for-an-agi

My definition of "understanding":  
In this universe, there is things/ problems/ states, and you can't "know" how to solve them i.e.
what will happen after thing_1 (ex. 1>2/3/4), and pick which future you want (1>69), unless 
you are dealing with patterns. So to solve a state is to know what comes next, if you can't 
predict, then you can’t use past  experience, hence can't solve/ predict/foresee. In this 
universe, we can, because there is patterns. We  can use past experiences to inform us of 
what should happen next for some state or states we could try,  whether it is trying to exit a 
fire or extending a cup design (where to put the handle, what should come  next). 
So our universe has patterns that reoccur, and we can only use this to solve problems. We 
can only use  patterns, or randomness, to solve problems. You may randomly move to avoid 
a bullet, this is  randomness, but you are trying to stay a pattern (survival), by using patterns 
(memories and  house/world). Acting random on purpose is you using patterns/memories to 
predict what to do/ what  will happen next. 
So to understand past experiences/ data, you want to find patterns. These can be exact 
matches, or  delay and/or holed matches ex. here are 2 similar memories: 123456 = 12456. 
Another: 123456 =  12h456. Anther: 123456 = 1h2h3h4h5h6. Another way to match a 
memory experience is translation by  using side matches (which can be similar) ex. dog ate =
cat ate.....so dog = cat because their side  contexts are similar. This allows you to see dog 
and predict meow comes next. This actually helps. So  does using hole/delay matches. And 
you probably just read "so dog" and realized I said "so does"  actually right? I did. 
Counting if dog>meow is more probable than dog>sleep also helps, it happens more after 
that word  maybe if look through the text data, you may see dog sleep less often! Also 
checking recent history ex.  cat cat cat cat cat, what word comes next? So recency has a big 
impact too! Patterns stick together, a  topic/ article/ movie will likely be on a single subject lots
ex. all about dogs or space ships. 
When you see "the cat and dog are here, and it is so cute!", the 'it' refers to a rare 
word/pattern in this  sentence (but not too rare), is it dog or cat it refers to? You could use the 
fact that most people buy dogs (if they do, idk), or that dog translates to ugly (not to me or 
others I'd say though). And note that seeing  dog....then it in the sentence is the recency 
thing; seeing cat 10 times and then ca>? predicts strongly a  't' comes next, so seeing 
'cat.....it', the it is trying to match recent history to predict what came next after this context 
recently, so it can be lots of words, see? So we are pattern matching here, cat = cat/dog/it is 
this sentence. So, understanding, is the looking for patterns. 
What is life/alive is not aliens but patterns that last long or come in large quantities. We use 
patterns  (memories/ homeworld) to be patterns (survival). Knowing where everything is in 
your world leads to  better prediction, you can walk through your home blindfolded using less 
energy to find things, if all is  the same shape (square/ circle), and grouped (restaurants, 
construction, clothes), and lined up and timed together. The future world will be a 3D 
gravityless airless metaloid-nanobot sphere that can instantly  regenerate and morph into 
tools as it desires, a fractal. We only like random paintings etc when young  because we 
explore for new data and not a specific domain, exploring is needed because things are hot  
like the big bang and it is trying to find patterns to cool down and last. DNAs form mom and 

dad merge
like memories trying mom and dad traits ex. wings+man= birdman, or wheels+home= trailer, 
this tries  new ideas, but is not as smart as brains doing it. 
Immortal Discoveries  
Source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-
machine understand-what-is-your-definition-of-understanding-for-an-agi

Let's take an autistic savant who memorizes the digits of pi to a million decimal places as an
example  of someone who "knows pi" but does not "comprehend pi". 
Let's now take Archemedies who described a method of finding the digits of pi by
construction of  regular polygons, to any desired degree of precision as someone who
may not "know pi" but does  "comprehend pi". 
Archimedes can describe his method in such a way that anyone reasonably educated can 
execute it. It  is an "algorithm". This algorithm's expression in, say Greek, has a length. 
Chaitan would say that Archimedes's algorithm is a compressed representation of pi very 
close to the  smallest one can achieve, and that, therefore, "compression is 
comprehension". 
By the way, something I should "comprehend" about Kolmogorov Complexity, but don't,
is why  halting is required of the algorithm that outputs the "comprehended" bit string. It 
seems just the  opposite is necessary in the case of, say, pi. How else can one use the 
Kolmogorov Complexity  program to make predictions? 
~~~~~~~~ 
Note that I avoided the word "understand" in my post, and, instead, used the word 
"comprehend". Others have brought in the notion of "value" to the notion of "understanding"; 
they bring up  "relevance" to one's "goals" etc. I suppose, therefore, one might say that one 
achieves "understanding"  when one "comprehends" the relationship between an object (say, 
pi) and one's decisions (say, I'm  considering building a geodesic dome for my family). 
Knowledge? Epistemology doesn't place any  _rigorous_ demands on us regarding 
comprehension (in the Chaitin sense), does it? (Please excuse the  prior misspelling of 
Chaitin.) 
James Bowery 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine-
understand what-is-your-definition-of-understanding-for-an-agi

Here's my crack at it: 
To understand a concept or statement is to determine its relevancy or relationship to 
one's personal  goals, experiences, or world-model. 
I recently read this paper and thought it was pretty interesting (and a relatively easy  
read): https://cse.buffalo.edu/~rapaport/Papers/crs.pdf If you don't already have 
something about  conceptual-role semantics in your literature review, it might be worth 
including. 
Writer of Minds 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine-
understand what-is-your-definition-of-understanding-for-an-agi

My opinion on understanding, which is still evolving: 
Understanding is one aspect of a conscious being, closest to knowing, but also including 
thinking,  feeling, intuition and emotion. Some claim these are inseparable, or that 
separating them limits the  types of knowledge that can be represented, and I tend to 
gravitate in that direction. 
We humans can know things we can't articulate, and we can be articulate about things that 
are not  really what we believe internally. "Understanding" as a test for AGI might be a 
gentleman's agreement  upon a certain level of articulation being reached for the idea at 
hand. We see the first inklings of this  in the XAI endeavors. However, some if not all 
understanding comes before the language needed to  express it, and I think this applies to 
both humans and AGI; therefore a more rudimentary framing of  understanding could be 
knowledge which gives the capability to inform an action. Still though, my  opinion is that 
understanding, or realization of knowing, is deeply intermingled with an idea of feeling. 
Understanding may be inseparable from other types of cognition. 
In any case, not taking advantage of that capability to inform an action can cause non-ideal  
situations for the individual, such as how I knew it would rain today but did not think to roll up 
my car  windows. 
It could also be evident in pathologies where we say things we don't believe, or believe 
things we  wouldn't say: both of those situations give a sense that feelings, possibly 
repressed, may be involved. 
In our AGI system, the most fundamental type of understanding is identical to knowledge 
which could  be traced back to learning of patterns by a type of Hebbian function. In common 
parlance, we think of  understanding as a type of knowledge orders of magnitude higher than 
the level of sensory spacetime  co-occurrence; however the Hebbian learning analogues 
continue at recursively higher levels on the  pattern matchers themselves. This is something 
still under development. One of the many challenges  we're facing here is the level at which 
we allow higher level patterns to be brought into the simulation  space for manipulations 
(thinking about it's own understanding of the pattern). The lower levels of  pattern recognition 
are not of the same types as the higher level patterns, and the difference between  them 
delineates the conscious from the subconscious. We also have not incorporated the concepts
of  hedonic feeling, however there are some simple proxies for intuition based in familiarity 
with a pattern. 
In an analogy, commonly seen in optical illusions, a layperson cannot understand nor explain
why we  see black dots on a grid illusion. Certain pattern matchers which were triggered in 
the optical  processing region are not able to be brought into question or simulation. There 
are areas of our own  being which we are helplessly unable to understand via articulation. By
this I argue that an AGI could  (should?) be average-human-level in understanding without 
the need of "understanding" it's full being. 
In a 1977 paper by Louis Arnaud Reid in the Proceedings of the Aristotelian Society #77 
"Thinking,  Feeling, Knowing" doi 10.1093/aristotelian/77.1.165 , the author gives a (IMO) 
great argument about  how "feeling" (not necessarily those marked by noticeable hedonic 
tones) are an inseparable part of our "knowing" something, both before and after we proclaim

to know that something. 
Daniel Jue
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine-
understand what-is-your-definition-of-understanding-for-an-agi

One of the problems in this grip on 'understanding' or 'knowing' is a confusion of 
competency in  knowing X as having anything to do with general intelligence. 
When the time comes I could probably toss my hat into the ring on this. 
I recently had an opportunity to explain this to a bunch of year 12 
wizkids here. https://www.youtube.com/watch?v=TtTt6MvIOig 
Maybe it would do as a primer. It's a half hour. Skip to 20 minutes to cut to the chase on 
my take for  purposes here. 
Colin Hales 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine-
understand what-is-your-definition-of-understanding-for-an-agi

You understand a sequence of symbols if you can predict or compress them. If I wanted to 
test if you  understand Chinese, I would show you some Chinese text and test how many 
characters you could  guess next. 
Matt Mahoney 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine-
understand what-is-your-definition-of-understanding-for-an-agi

Understanding is the editing distance between data points and the distance of objects on 
a traveling  map, done by a conscious mind. 
keghnfeem 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine-
understand what-is-your-definition-of-understanding-for-an-agi

Understanding is the skill of the machine. Machine X understands Y if it  can 
determine the completeness and consistency of Y according to its  internal 
application domain model. The AGI application area is the real  world. In simple 
words, understanding is the ability to determine whether something can be. 
How can a machine understand? 
The machine generates a real-time representation (model) of the  current 
situation (CSR). 
Determines whether all the values of the CSR elements correspond to  each 
other. 
If they do, then the machine understands the situation and acts in  
accordance with it. 
If not, then he does not understand the situation, identifies non 
conformities and acts to eliminate the non-conformities. 
Roman Kofman 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine-
understand what-is-your-definition-of-understanding-for-an-agi

to understand is to explain why something is the way it is. 
is to be able to explain why you did something. 
is to be able to explain why others did things. 
doddy  
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine-
understand what-is-your-definition-of-understanding-for-an-agi

Understanding is about the ability to create a worldview expressed with rich knowledge 
representation  …. to be able to model inside the machine, inside the person, the complex 
world that is outside. The  other capability of understanding is to acquire and interpret new 
information. And to enhance and  continuously update this worldview because the the world 
around us is very dynamic. The interests are  growing. So part of understanding is the ability 
to continuously update this internal view inside the  machine or on us of what is happening 
outside. And the third is the ability to effectively reason and  explain both our existing 
knowledge. As well as in real time, new information coming from the outside. 
Gadi Singer  
source: https://www.youtube.com/watch?v=aqfizAySe0E

This word “understand” is one of these suitcase words that no one agrees what it really 
means —  almost like a placeholder for mental phenomena that we can’t explain yet. But I 
think this mechanism  of abstraction and analogy is key to what we humans call 
understanding. It is a mechanism by which  understanding occurs. We’re able to take 
something we already know in some way and map it to  something new. 
~~~~~~~
Melanie Mitchell  
1) source: https://www.quantamagazine.org/melanie-mitchell-trains-ai-to-think-with-

analogies 20210714/?mc_cid=a4fbbe27a8&mc_eid=c7558b25d5 
2) source: https://arxiv.org/pdf/2104.12871.pdf

Understanding is the process of reaching a state when observations,
models  and predictions line up and network activity drops 
significantly indicating  low prediction errors. Although this happens 
all the time in adult brains,  it is especially noticeable (aha!) after a 
long running high activation  thought process, i.e. one with large 
prediction errors that aren't resolved  for a long time. 
In other words, understanding is when free energy reaches a 
minimum for a  given context. 
Amol Kelkar 
source: Slack channel “IntelligentAgent”

The common conception of “understanding” in the context of effectively any machine can be simply  
stated: given some input, the machine can be said to “understand” if the correct output results. As an 
example, a calculator “understands” addition. An ATM “understands” a withdrawal. Google  
“understands” a routine weather inquiry. 
In AI it is common to hear vendors claim that their AI “understands” various scenarios. Things quickly
become murky, however, since the vendor is making a subtle implication that their machine’s  
“understanding” ventures into some form of human-level intelligent understanding. We are, after all,  
talking about AI, which to most people really means something like human-level understanding. 
Invariably vendors making claims of the form “our machine X understands Y” don’t include any  
definition of understanding. If you hear somebody making such a claim, immediately challenge them  
to define what “understanding” means to their machine. The odds are they are trying to smoke you  
and they probably don’t really know. They want you to assume their machine’s “understanding” is just
like yours. 
My opinions about understanding are informally as follows. All designers start with a set of bedrock  
first principles upon which their AI/AGI architecture is based. A first principle can be generally defined 
as a principle that will be universally applicable in their system and is ideally not derived via 
something else. At the outset the designer has a hunch, later a theory, that set X of first principles 
integrated well  will result in AGI = Y. 
What counts as a first principle is hotly debated. For an example let us take causality. We don’t need 
to define causality in terms of something else. Aristotle basically started his metaphysics with  
causality. 
Simply put, the AGI must be able to reduce everything it does to its first principles, to explain its  
actions in terms of its first principles, and such principles need to be made explicitly clear at the outset
to the users. 
So in this very simple example using causality, the machine must provide ALONG WITH ITS OUTPUT
an  explanation  that  makes  clear  it  “understands”  in  terms  of  specific,  determinate  causality  in
whatever case is at hand.
There are users, developers, and the AGI. All three participants must know what the first principles  
used are, and all explanations provided by the AGI must be ultimately reducible to the first principles,
eg., “a caused b.” Then all three parties know what the machine “understands” and what its  

understanding is based upon. 
Of course, under this definition of understanding the resulting “understanding” will probably not be
human-level, but it at least would clarify the bounds of understanding. 
Mike Archbold

All forms and media of information - incl. language texts, numerical texts, programs, maps etc - 
constitute
*pictures of the world"
pictures which are both:
descriptive - YOU ARE GOING TO THE SUPERMARKET  &
prescriptive - GO TO THE SUPERMARKET
(n.b. descriptive/prescriptive info have basically same content/picture of world - merely different 
modes).
To *understand* any information you have to be able to
 translate the pictured world *****INTO THE REAL WORLD***
- either perceive what is described  by the picture, in the real world e.g.  perceive s.o. going to the 
supermarket in the real world 
- or achieve what is prescribed by the picture, in the real world  e.g. physically go to the supermarket
To repeat:
understanding information/ pictured worlds  means:being able to translate them into. real worldS 
("translate words into reality")
Non-understanding/incomprehension (which is NOT studied by either linguistics or cog sci) occurs 
when you
"don't know what in the world you are talking about" (or "what on earth")
e.g. a baffling description of quantum particles which you have no means of relating to reality.
Only an embodied human living in the world can understand or not understand information. Only a 
robot living in the world will be able to understand information in  future.
We can distinguish between *logical understanding"  (  being able to translate words/symbols into 
more words/symbols, e.g. dictionary definitions)   vs *real understanding* (being able to translate the 
pictured world into the real world).
Everyone in AGI gets very excited about "understanding" because they are all doing some form of 
Symbolic AI, which can only translate worlds into more words, symbols into more symbols. Their 
systems have no means of connecting information about the world to the real world. They are not 
prepared to contemplate robotic AGI - but there will be no other kind.
Mike Tintner source email

Understanding as a __process__ is just the interpretation of particular input (text, drawing, image, and so on).
Interpretation is totally dependent on previously acquired (or preloaded) knowledge (that is obvious) and on 
mission/motivation/goal (that is less obvious).
Understanding as a __consequence__/__result__ is an estimation of the usefulness of information extracted in the 
process of interpretation. As with any measure, it is based on some criterion, so the level of understanding depends 
on such criterion. In most cases of text/message interpretation, such criterion is closely related to intentions of 
text/message author; the more difference between message author`s knowledge set and interpreter`s knowledge set, 
the less chance to the identical interpretation of the message by the conversation sides (i.e., less potentially 
understanding level).
Mykola Rabchevskiy source email

I should probably put in my two cents on understanding which is similar to Amol's.
I would say that understanding involves having models that accurately predict the distributions of 
possible presents and futures. For example, let's say you have a ball. If I understand what a ball 
is then I can attend to one part of it, move my eye to another part and predict the distribution of 
shapes and colors my eye will see next. This includes more discrete distributions like the names 
of games that include a ball and their likelihood that they do so.
Lance Hughes
source: Slack channel “IntelligentAgent”

Understanding doesn’t exist
Greg Heil  source: in person event

I would like to add something about "predictions". I am not quite sure that we make predictions, in fact it is not the right
term to use. Instead we should talk about "open possibilities" or "expectations". When you try to understand something
you create a context with some assumptions and what will happen next :
●
Is compatible with the current assumptions (the current context), then you, may be, generate a variation in 
your assumptions (an extension or more constraint or a change, ....).
●
Is not compatible, in that case you detect the error and adapt the context with your assumptions to be 
compatible with what happen before and now or you do not understand, I mean you are not able to create a 
context compatible with your current perceptions and interpretations based on what you know from the 
external world.
Of course at any time what you perceive is filtered by the context you build that guide your expectations.
Philippe Delmeire, source email 
NOTE: this is a part of his view on understanding.

The relation between Truth, Meaning and Understanding is this: fully understanding the meaning of 
some utterance (or expression, or statement) is essentially the same as knowing what the state-of-
affairs would be like if that utterance was true. 
Walid Saba
source: https://medium.com/ontologik/on-the-difference-between-recognition-understanding-
46f20b292ef8

Understanding is an organization of information. The process of forming concepts.
More whimsically - Understanding is getting beneath something :-)
Rob Freeman source email

