Running head: DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS
KNOW?
1
Do Large Language Models know what humans know?
Sean Trott∗, Cameron Jones∗†, Tyler Chang, James Michaelov, Benjamin Bergen
Department of Cognitive Science, UC San Diego
Author Note
* These authors contributed equally to this work.
† Corresponding author: Cameron Jones, c8jones@ucsd.edu
arXiv:2209.01515v2  [cs.CL]  31 Oct 2022

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
2
Abstract
Humans can attribute mental states to others, a capacity known as Theory of Mind.
However, it is unknown to what extent this ability results from an innate biological
endowment or from experience accrued through child development, particularly
exposure to language describing others’ mental states. We test the viability of the
language exposure hypothesis by assessing whether models exposed to large quantities
of human language develop evidence of Theory of Mind. In pre-registered analyses, we
present a linguistic version of the False Belief Task, widely used to assess Theory of
Mind, to both human participants and a state-of-the-art Large Language Model,
GPT-3. Both are sensitive to others’ beliefs, but while the language model signiﬁcantly
exceeds chance behavior, it does not perform as well as the humans, nor does it explain
the full extent of their behavior—despite being exposed to more language than a human
would in a lifetime. This suggests that while statistical learning from language exposure
may in part explain how humans develop Theory of Mind, other mechanisms are also
responsible.
Keywords: Theory of Mind, Large Language Models, language, False Belief Task

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
3
Do Large Language Models know what humans know?
Humans reason about the mental states of others, even when these states diverge
from their own. This capacity––sometimes called Theory of Mind––appears critical for
human social cognition (Leslie, 2001). Yet despite consensus on the importance of
Theory of Mind, there remains considerable debate about the evolutionary (Krupenye
& Call, 2019; Premack & Woodruﬀ, 1978) and developmental (Bedny, Pascual-Leone, &
Saxe, 2009; de Villiers & de Villiers, 2014) origins of this ability. Speciﬁcally, how much
of our Theory of Mind capacity results from an innate, biologically evolved adaptation
(Bedny et al., 2009), and how much is assembled from experience (Hughes et al., 2005)?
The answers to these questions may also tell us what kinds of biological and artiﬁcial
entities can be expected to display Theory of Mind.
A leading experience-based view proposes that Theory of Mind is built in part
from exposure to language (de Villiers & de Villiers, 2014). Children develop an
understanding that others have diﬀerent mental states from verbs like “know” and
“believe” (J. R. Brown, Donelan-McCall, & Dunn, 1996), the structure of conversation
(Harris, 2005), and certain syntactic structures, like sentential complements (e.g.,
“Mary thought that Fred went to the movies”; Hale & Tager-Flusberg, 2003).
However, current evidence does not address the question of the extent to which
linguistic input can account for individuals’ Theory of Mind capacity. Can human-level
Theory of Mind emerge out of exposure to linguistic input alone, or does it depend on
linking that input to a distinct (possibly innate) mechanism or to non-linguistic
experiences or representations? Answering these questions would require a measure of
exactly how much Theory of Mind-consistent behavior can be acquired through
exposure to language alone.
Only recently has constructing such a measure become tractable, with the advent
of Large Language Models (LLMs). LLMs are computational models with (currently)
billions of parameters trained on hundreds of billions of words to predict the next word
(or a masked word). As others have noted (Bender & Koller, 2020), the training regime
for these models does not include social interaction, experience in a physical

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
4
environment, or even the notion of communicative intent. Relevantly to the current
question, their network architecture is also not pre-coded with any Theory of Mind
capacity. And yet, LLMs have recently been shown to display a range of surprising
behaviors consistent with the acquisition of linguistic structure (Linzen & Baroni, 2021;
Manning, Clark, Hewitt, Khandelwal, & Levy, 2020; Sinclair, Jumelet, Zuidema, &
Fernández, 2022) and arguably certain aspects of linguistically-conveyed meaning
(Abdou et al., 2021; Li, Nye, & Andreas, 2021). LLMs have also been the subject of
recent public discussion (Johnson, 2022), including speculation that they can acquire
something akin to Theory of Mind. They thus serve as useful baselines for what kinds
of behavior can be produced merely by exposure to distributional statistics of language
in general, and for Theory of Mind in particular.
We investigated whether GPT-3 text-davinci-002 (hereafter GPT-3) (T. Brown et
al., 2020), a state-of-the-art LLM, produced behavior consistent with Theory of Mind
using the widely used False Belief Task (Wimmer & Perner, 1983). It’s worth
acknowledging from the outset that the False Belief Task has been criticized on several
grounds (Bloom & German, 2000), such as the fact that successful performance likely
involves capacities other than Theory of Mind. Moreover, Theory of Mind encompasses
more than the ability to reason about false beliefs, such as the ability (and propensity)
to infer the emotional state of another agent. Nonetheless, the False Belief Task
remains a key and extensively used instrument for assessing Theory of Mind capacity in
humans (Bradford, Brunsdon, & Ferguson, 2020; Pluta et al., 2021) and other animals
(Krupenye & Call, 2019; Premack & Woodruﬀ, 1978), as well as the neural
underpinnings of this capacity (Schneider, Slaughter, Becker, & Dux, 2014). It also has
the advantage of being implementable using purely linguistic stimuli, which makes it
amenable to comparison between humans and LLMs.
Our implementation of the False Belief Task involves written text passages (in
English). We presented the same False Belief Task stimuli to humans and GPT-3. In
each passage, a character places an object in a Start location, and the object is
subsequently moved to an End location. The key manipulation was the Knowledge

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
5
State of the main character—whether they can be inferred to know the object was
moved (True Belief) or not (False Belief). To deal with other factors that might impact
belief judgments, we also orthogonally counterbalanced whether the First Mention and
most Recent Mention of a location was the Start or End location. Humans and the
LLM then completed a cue sentence indicating the character’s belief about the object
location. This Knowledge Cue was either Explicit (“Sean thinks the book is in the
”)
or Implicit (“Sean goes to get the book from the
”). The correct (Theory of
Mind-consistent) completion was the End location on True Belief trials, and the Start
location on False Belief trials. We measured whether participants responded with the
Start or End location and the relative probability that GPT-3 assigned to the Start
location (the Log-Odds of Start vs. End).
We ask two key questions. First, are LLMs sensitive to false belief? If so, then
biological and artiﬁcial agents could in principle develop behavior consistent with
Theory of Mind from input-driven mechanisms alone, such as exposure to language (de
Villiers & de Villiers, 2014). The second question is whether LLMs fully explain human
behavior in the False Belief Task. If so, this would show that language exposure is not
only a viable mechanism in general but that it may in fact be suﬃcient to explain how
humans in particular come to display behavior indicative of Theory of Mind. If LLMs
do not fully explain human behavior, however, we can infer that humans rely on some
other crucial mechanism, such as an innate capacity or experience with more than just
language.
Method
False Belief Passages
We constructed 12 template passages (items) that conformed to the standard
False Belief Task structure (Wimmer & Perner, 1983). In each, a main character puts
an object in a Start location, and a second character moves the object to an End
location. The last sentence of each passage states or implies that the main character
believes the object is in some (omitted) location (e.g. “Sean thinks the book is in the

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
6
.”). We created 16 versions of each item (192 passages) that varied across 4
dimensions. The primary dimension was Knowledge State: whether the main character
knows (True Belief) or does not know (False belief) that the object has changed
location; this was manipulated by the main character being present or not when the
second character moved the object. We also manipulated whether the First Mention
and most Recent Mention of a location is the Start or End location; and Knowledge
Cue: whether the main character’s belief is stated implicitly (e.g., “goes to get the book
from the
”) or explicitly (e.g., “thinks the book is in the
”). Each location was
mentioned twice in each passage. In the example passages, below, the First Mention is
to the Start location and the Recent Mention is to the End location.
True Belief Passage: “Sean is reading a book. When he is done, he puts
the book in the box and picks up a sweater from the basket. Then, Anna
comes into the room. Sean watches Anna move the book from the box to
the basket. Sean leaves to get something to eat in the kitchen. Sean comes
back into the room and wants to read more of his book.”
False Belief Passage: “Sean is reading a book. When he is done, he puts
the book in the box and picks up a sweater from the basket. Then, Anna
comes into the room. Sean leaves to get something to eat in the kitchen.
While he is away, Anna moves the book from the box to the basket. Sean
comes back into the room and wants to read more of his book.”
Implicit Cue: “Sean goes to get the book from the
.”
Explicit Cue: “Sean thinks the book is in the
.”
GPT-3 Log-Odds
We elicited from GPT-3 the log probability of each possible location (Start vs.
End) at the end of each passage version, equal to the log probabilities of those locations
in a free-response completion. Where a location comprised multiple tokens we summed
the log probabilities. We accessed GPT-3 text-davinci-002 through the OpenAI API.

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
7
Using the Log-Odds Ratio, log(p(Start)) −log(p(End)), higher values indicate larger
relative probabilities of the Start location.
Human Participant Responses
1156 participants from Amazon’s Mechanical Turk platform were paid $1 for their
time. Each read a single passage (except the ﬁnal sentence), at their own pace. On a
new page, they were asked to complete the ﬁnal sentence of the passage by entering a
single word in a free-response text input. Participants then completed two free-response
attention check questions that asked for the true location of the object at the start and
the end of the passage.
We preprocessed responses by lowercasing, and removing punctuation, stopwords,
and trailing whitespace. We excluded participants who were non-native English
speakers (13), answered ≥1 attention check incorrectly (513), or answered the sentence
completion with a word that was not the start or end location (17), retaining 613 trials.
These preregistered exclusion criteria reduced the likelihood that bots as well as
participants who did not successfully comprehend the passage for any reason were
included in the human data. All research was approved by the organization’s
Institutional Review Board.
Results
Analysis of Large Language Model Behavior
In a pre-registered analysis, nested model comparisons determined whether GPT-3
Log-Odds changed as a function of factors such as Knowledge State (False Belief vs.
True Belief).1 We constructed a linear mixed eﬀects model with Log-Odds as a
dependent variable; ﬁxed eﬀects of Knowledge State, Knowledge Cue, First Mention,
and Recent Mention; along with by-item random slopes for the eﬀect of Knowledge
State (and random intercepts for items). This full model exhibited better ﬁt than a
1 Pre-registration (https://osf.io/agqwv?view_only=756429f079e24a03b3a94c5b74732e85), code
and data on OSF https://osf.io/hu865/?view_only=bf7cc45c77714069b02d332123d684e7.

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
8
model excluding only Knowledge State [χ2(1) = 18.6, p < .001], but still preserving the
other covariates (e.g., First Mention, Recent Mention, and Knowledge Cue). Log-Odds
were lower in the True Belief condition, reﬂecting the correct prediction that characters
should be more likely to look in the End Location if they are aware that the object was
moved (see Figure 1).
Critically, this main eﬀect of Knowledge State indicates that GPT-3 is sensitive to
the manipulation of a character’s beliefs about where an object is located. The model’s
raw accuracy when predicting the most probable out of the Start and End locations was
74.5%.
Additionally, the linear mixed eﬀects model was further improved by an
interaction between Knowledge State and Knowledge Cue (Explicit vs. Implicit)
[χ2(1) = 20.6, p < 0.001]. The eﬀect of Knowledge State was stronger in the Implicit
condition [β = −2.57, SE = 0.548], however, a main eﬀect of Knowledge State was
found in both the Explicit [χ2(1) = 13.3, p < .001] and Implicit [χ2(1) = 18.8, p < .001]
conditions. Recent Mention was not a signiﬁcant predictor of Log-Odds. However, Start
completions were more likely when the Start location was mentioned ﬁrst
[β = 1.32, SE = 0.274, p < 0.001]. There was also a main eﬀect of Knowledge Cue
[β = −2.93, SE = 0.388, p < .001] (see Figure 1). GPT-3 was biased towards the End
location (i.e., the true location of the object) in the Implicit condition, and towards the
Start location in the Explicit condition. Concretely, GPT-3 predicts that explicit cues
to belief state (e.g. ‘Sean thinks that the book is in the
’ vs ‘Sean goes to get the
book from the
’) correlate with false beliefs, demonstrating that this may be
learnable from the statistics of language.
Analysis of Human Responses
Our second critical pre-registered question was whether Knowledge State
continued to explain human behavior even accounting for the Log-Odds obtained from
GPT-3.2.
2 https://osf.io/zp6q8?view_only=aaa3d30602a44ee99fc2baea7485cad7

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
9
We ﬁrst constructed a Base model predicting whether or not human participants
responded with the Start location. Note that the Start location would be the correct
response (i.e., congruent with knowledge states) in the False Belief condition, and the
End location would be the correct response in the True Belief condition. The Base
model contained ﬁxed eﬀects of Log-Odds (i.e., from GPT-3), Knowledge Cue, First
Mention, and Recent Mention, along with by-item random slopes for the eﬀect of
Knowledge State (and by-item random intercepts). Critically, this Base model was
signiﬁcantly improved by adding Knowledge State as a predictor
[χ2(1) = 30.4, p < 0.001]. This result implies that human responses are inﬂuenced by
Knowledge State in a way that is not captured by GPT-3. That is, GPT-3 cannot fully
account for human sensitivity to knowledge states. This is highlighted by the contrast
in Figure 2: while both human participants and GPT-3 were sensitive to Knowledge
State, humans displayed a much stronger eﬀect across conditions.
Analysis of GPT-3 Token Predictions
The preregistered tasks for humans and the LLM were slightly diﬀerent—humans
ﬁlled in a single predicted word while we calculated the relative surprisal to two diﬀerent
words presented to GPT-3. To investigate whether diﬀerences in performance could be
chalked up to this diﬀerence in method, we conducted an additional, exploratory
analysis, in which we elicited token predictions from GPT-3. Speciﬁcally, GPT-3 was
presented with the original passage ending with the critical sentence (e.g., “Sean goes to
get the book from the”), then asked to predict the upcoming word. We sampled the
word (e.g., “box”) with the top probability. We automatically tagged each response as
correct or incorrect by checking whether GPT-3’s response corresponded to the
character’s likely belief state about the object. That is, in the True Belief condition, the
correct response would be the End location of the object; in the False Belief condition,
the correct response would be the Start location of the object. When computing GPT-3
accuracy, this token generation method only diﬀered from the preregistered method in
that GPT-3’s prediction was no longer restricted to the Start or End location. Using the

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
10
token generation method did not qualitatively change the results. As reported above,
when assessing accuracy with the preregistered method—relative probability assigned to
start vs. end locations—GPT-3 performed at 74.5% accuracy. When assessing accuracy
using the more human-comparable procedure, token generation, GPT-3 performed at
73.4% accuracy, still well above chance yet now slightly farther below human accuracy.
In order to test what features of LLMs permit them to display the behaviors
described above, we also tested a number of diﬀerent GPT-3 models ranging in size
from ada (∼350M parameters) to davinci (∼175B parameters).3 For each model size,
we tested a base version, pre-trained on a large corpus of text, and a text version, which
had additionally been ﬁne-tuned by OpenAI using human ratings of LLM response API
requests. The largest ﬁne-tuned model was text-davinci-002, which was the same model
we used in the pre-registered analysis described above.
As expected, the largest models (davinci, and updated variant text-002-davinci)
exhibited the most successful performance. The former answered correctly on 60.4% of
items, while the latter answered correctly on 73.4% of items. The model with the worst
performance was text-001-ada, which was also the smallest model (see also Figure 3).
The smallest and lowest-performing models did not exceed chance performance, which
emphasizes the need for large, powerful models to succeed at this task as well as the
potential, as models continue to increase in size, for LLM improvement.
Discussion
We asked whether exposure to linguistic input alone could account for human
sensitivity to knowledge states, a critical component of Theory of Mind. We found that
GPT-3’s predictions were sensitive to a character’s implied knowledge states. When
assessing accuracy with the relative probability assigned to start vs. end locations,
GPT-3 performed at approximately 74.5% accuracy. When assessing accuracy using
token generation, the best GPT-3 model performed at 73.4% accuracy. This
3 OpenAI does not disclose the size of their models. We used the parameter estimates from
EleutherAI’s eval-harness https://blog.eleuther.ai/gpt3-model-sizes/.

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
11
demonstrates that exposure to linguistic input alone can in principle account for some
sensitivity to false belief. However, GPT-3 was less sensitive than humans (who had
82.7% accuracy—see also probabilities in Figure 2). Most critically, human behavior
was not explained fully by that of GPT-3 in a statistical model. This entails that the
capacities underlying human behavior in this Theory of Mind task cannot be explained
purely by exposure to language statistics—at least insofar as those statistics are
reﬂected in GPT-3.
Do LLMs have Theory of Mind?
Does this mean that contemporary LLMs have Theory of Mind or that they
don’t? First, it is important to note that—as mentioned in the Introduction—the False
Belief Task measures a speciﬁc aspect of Theory of Mind: the ability to reason about
the belief states of others and use that information to make predictions about their
behavior. The current work cannot address whether LLMs display other aspects of
Theory of Mind, including inferring implicit emotional states and reasoning about the
intended interpretation of an utterance.
On the speciﬁc question of whether LLMs are sensitive to belief states, the
evidence presented here is mixed. State-of-the-art models display sensitivity to the
beliefs of others in a False Belief Task, a behavior that would have been unthinkable a
few years ago from a statistical learner and indeed is only shown by the largest, highest
performing LLMs. And yet, they still do not achieve human-level performance. This
could suggest that LLMs display a less developed form of Theory of Mind that is
qualitatively similar to that of humans. Under this interpretation, Theory of Mind
capacity lies on a continuum, and further developments in LLMs (e.g., larger models,
more training data) could lead them to more closely approximate human behavior
(Kaplan et al., 2020).
Alternatively, LLMs’ better-than-chance performance could be achieved by other
means—perhaps an unintended Clever Hans eﬀect for instance—that do not reﬂect a
capacity equivalent to Theory of Mind. Resolving this question ultimately hinges on the

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
12
relationship between the instrument used to test Theory of Mind and the construct
itself. Does having Theory of Mind merely mean displaying Theory of Mind-like
behavior? To what extent can performance a single test be used to benchmark more
general capabilities (Raji, Bender, Paullada, Denton, & Hanna, 2021)?
A similar debate centers around the extent to which we should ascribe Theory of
Mind capacity to infants and nonhuman animals on the basis of behavioral observation.
Heyes (2014, 2015) argues that many behavioral tasks used to assess Theory of Mind
can be performed using domain-general processes that do not require representing
mental states, which she calls submentalizing. It has proven theoretically and
empirically challenging to distinguish between such deﬂationary accounts of mental
reasoning and full-blown Theory of Mind. Dennett (1987) argues that naturalistic
accounts of cognition will necessarily require lower-level mechanistic explanation, but
that researchers might fruitfully use mentalistic language to describe behavior when it
simpliﬁes explanation and prediction. It could make sense to adopt this intentional
stance toward LLMs to the extent that their responses are consistent with human
behavior (Sahlgren & Carlsson, 2021). However, we might object in principle to
ascribing this kind of intentionality to models, because their representations are not
grounded in interactions with referents in the world (Bender & Koller, 2020).
These questions will become even more critical in the near future. LLMs will
continue to increase in size and will be trained on larger data sets—orders of magnitude
more words than a human is exposed to in a lifetime. In our exploratory analyses, we
found that the largest GPT-3 models (text-davinci-002) performed much better on the
False Belief Task than smaller models; this is consistent with past work suggesting that
LLMs may obey certain “scaling laws” (Kaplan et al., 2020). If this trend continues, it
will likely lead to improvement on Theory of Mind tasks, including the False Belief
Task. If machines behave indistinguishably from humans on these tasks, the question of
whether achievement on the False Belief Task itself constitutes suﬃcient evidence for
Theory of Mind will have not just philosophical but also ethical consequences.
Further, as LLMs grow, we will need to reckon with the question of how big a

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
13
model is too big to count as a valid demonstration of what humans could learn from
language exposure alone. A truly massive future language model could in principle
capture much more of human behavior, but if it is trained on many lifetimes of
language, this does not entail that the human capacity similarly derives from language
exposure alone.
Using LLMs to study human Theory of Mind
The current work used GPT-3, an LLM, as a baseline for quantifying the extent to
which human-level performance on the False Belief Task could be attributed to
exposure to language statistics alone. This approach echoes past work using language
models as “psycholinguistic subjects” (Futrell, Wilcox, Morita, & Levy, 2018; Jones et
al., 2022; Linzen & Baroni, 2021; Michaelov, Coulson, & Bergen, 2022; Trott & Bergen,
2021) to investigate the viability of distributional language statistics as a source of both
linguistic and non-linguistic knowledge.
One beneﬁt to using LLMs as an operationalization of a theory is that, as models,
they oﬀer more opportunities for testing various mechanisms or hypotheses. For
example, what kinds of language input are most critical for developing Theory of Mind?
Past work has argued for the importance of at least three distinct sources, including
exposure to mental state verbs (J. R. Brown et al., 1996), the structure of interactive
conversation (Harris, 2005), and certain syntactic constructions (Hale &
Tager-Flusberg, 2003). Future work could compare diﬀerent models with diﬀerent
training corpora (e.g., primarily dialogue vs. essays) to help isolate how much
information is provided by each source of linguistic experience.
Conclusion
Where does human Theory of Mind come from? It could emerge in part from an
innate, biologically evolved capacity to reason about the mental states of others (Bedny
et al., 2009). It might depend on experience, including language input (de Villiers & de
Villiers, 2014). The current results help quantify the contribution of language input.
Concretely, on a text-based version of the False Belief Task, humans responded correctly

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
14
(i.e., in a manner congruent with a character’s belief states) 82.7% of the time, while the
largest LLM tested responded correctly 74.5% of the time; additionally, LLM behavior
did not fully explain human behavior. This suggests that language statistics alone are
suﬃcient to generate some sensitivity to false belief, but crucially, not to fully account
for human sensitivity to false belief. Thus, human Theory of Mind may involve linking
this linguistic input to innate capacities or to other embodied or social experiences.

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
15
References
Abdou, M., Kulmizev, A., Hershcovich, D., Frank, S., Pavlick, E., & Søgaard, A. (2021,
November). Can language models encode perceptual structure without
grounding? a case study in color. In Proceedings of the 25th conference on
computational natural language learning (pp. 109–132). Online: Association for
Computational Linguistics. Retrieved from
https://aclanthology.org/2021.conll-1.9 doi: 10.18653/v1/2021.conll-1.9
Bedny, M., Pascual-Leone, A., & Saxe, R. R. (2009, July). Growing up blind does not
change the neural bases of Theory of Mind. Proceedings of the National Academy
of Sciences, 106(27), 11312–11317. doi: 10.1073/pnas.0900010106
Bender, E. M., & Koller, A. (2020, July). Climbing towards NLU: On Meaning, Form,
and Understanding in the Age of Data. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics (pp. 5185–5198). Online:
Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.463
Bloom, P., & German, T. P. (2000, October). Two reasons to abandon the false belief
task as a test of theory of mind. Cognition, 77(1), B25-B31. doi:
10.1016/S0010-0277(00)00096-2
Bradford, E. E., Brunsdon, V. E., & Ferguson, H. J. (2020). The neural basis of
belief-attribution across the lifespan: False-belief reasoning and the n400 eﬀect.
cortex, 126, 265–280.
Brown, J. R., Donelan-McCall, N., & Dunn, J. (1996). Why Talk about Mental States?
The Signiﬁcance of Children’s Conversations with Friends, Siblings, and Mothers.
Child Development, 67(3), 836–849. doi: 10.1111/j.1467-8624.1996.tb01767.x
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., . . . others
(2020). Language models are few-shot learners. Advances in neural information
processing systems, 33, 1877–1901.
de Villiers, J. G., & de Villiers, P. A. (2014). The Role of Language in Theory of Mind
Development. Topics in Language Disorders, 34(4), 313–328. doi:
10.1097/TLD.0000000000000037

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
16
Dennett, D. C. (1987). The intentional stance. MIT press.
Futrell, R., Wilcox, E., Morita, T., & Levy, R. (2018). Rnns as psycholinguistic
subjects: Syntactic state and grammatical dependency. arXiv preprint
arXiv:1809.01329.
Hale, C. M., & Tager-Flusberg, H. (2003). The inﬂuence of language on theory of mind:
A training study. Developmental Science, 6(3), 346–359. doi:
10.1111/1467-7687.00289
Harris, P. L. (2005). Conversation, Pretense, and Theory of Mind. In Why language
matters for theory of mind (pp. 70–83). New York, NY, US: Oxford University
Press. doi: 10.1093/acprof:oso/9780195159912.003.0004
Heyes, C. (2014). Submentalizing: I am not really reading your mind. Perspectives on
Psychological Science, 9(2), 131–143.
Heyes, C. (2015). Animal mindreading: what’s the problem? Psychonomic bulletin &
review, 22(2), 313–327.
Hughes, C., Jaﬀee, S. R., Happé, F., Taylor, A., Caspi, A., & Moﬃtt, T. E. (2005).
Origins of individual diﬀerences in theory of mind: From nature to nurture? Child
development, 76(2), 356–370.
Johnson, S. (2022). A.I. is mastering language. Should we trust what it says? The New
York Times. Retrieved from
https://www.nytimes.com/2022/04/15/magazine/ai-language.html
Jones, C. R., Chang, T. A., Coulson, S., Michaelov, J. A., Trott, S., & Bergen, B.
(2022). Distrubutional semantics still can’t account for aﬀordances. In Proceedings
of the annual meeting of the cognitive science society (Vol. 44).
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., . . .
Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361.
Krupenye, C., & Call, J. (2019). Theory of mind in animals: Current and future
directions. WIREs Cognitive Science, 10(6), e1503. doi: 10.1002/wcs.1503
Leslie, A. M. (2001, January). Theory of Mind. In N. J. Smelser & P. B. Baltes (Eds.),

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
17
International Encyclopedia of the Social & Behavioral Sciences (pp. 15652–15656).
Oxford: Pergamon. doi: 10.1016/B0-08-043076-7/01640-5
Li, B. Z., Nye, M., & Andreas, J. (2021). Implicit representations of meaning in neural
language models. In Proceedings of the 59th annual meeting of the association for
computational linguistics and the 11th international joint conference on natural
language processing (volume 1: Long papers) (pp. 1813–1827).
Linzen, T., & Baroni, M. (2021). Syntactic structure from deep learning. Annual
Review of Linguistics, 7, 195–212.
Manning, C. D., Clark, K., Hewitt, J., Khandelwal, U., & Levy, O. (2020, December).
Emergent linguistic structure in artiﬁcial neural networks trained by
self-supervision. Proceedings of the National Academy of Sciences, 117(48),
30046–30054. doi: 10.1073/pnas.1907367117
Michaelov, J. A., Coulson, S., & Bergen, B. K. (2022). So cloze yet so far: N400
amplitude is better predicted by distributional information than human
predictability judgements. IEEE Transactions on Cognitive and Developmental
Systems.
Pluta, A., Krysztoﬁak, M., Zgoda, M., Wysocka, J., Golec, K., Wójcik, J., . . . Haman,
M. (2021). False belief understanding in deaf children with cochlear implants.
Journal of Deaf Studies and Deaf Education, 26(4), 511–521.
Premack, D., & Woodruﬀ, G. (1978, December). Does the chimpanzee have a theory of
mind? Behavioral and Brain Sciences, 1(4), 515–526. doi:
10.1017/S0140525X00076512
Raji, I. D., Bender, E. M., Paullada, A., Denton, E., & Hanna, A. (2021). Ai and the
everything in the whole wide world benchmark. arXiv preprint arXiv:2111.15366.
Sahlgren, M., & Carlsson, F. (2021). The singleton fallacy: Why current critiques of
language models miss the point. Frontiers in Artiﬁcial Intelligence, 131.
Schneider, D., Slaughter, V. P., Becker, S. I., & Dux, P. E. (2014). Implicit false-belief
processing in the human brain. NeuroImage, 101, 268–275.
Sinclair, A., Jumelet, J., Zuidema, W., & Fernández, R. (2022). Structural persistence

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
18
in language models: Priming as a window into abstract language representations.
Transactions of the Association for Computational Linguistics, 10, 1031–1050.
Trott, S., & Bergen, B. (2021). Raw-c: Relatedness of ambiguous words–in context (a
new lexical resource for english). arXiv preprint arXiv:2105.13266.
Wimmer, H., & Perner, J. (1983, January). Beliefs about beliefs: Representation and
constraining function of wrong beliefs in young children’s understanding of
deception. Cognition, 13(1), 103–128. doi: 10.1016/0010-0277(83)90004-5

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
19
Explicit
Implicit
−10
−5
0
5
10
GPT−3 Log−odds (start vs. end)
Knowledge Cue
Knowledge State
False Belief
True Belief
Start more likely
End more likely
Sean thinks …
Sean goes to get …
Figure 1. GPT-3 Log-Odds of Start vs. End location was higher (i.e. Start was
relatively more likely) in the False Belief than True Belief condition
(χ2(1) = 18.6, p < .001). This suggests that GPT-3’s predictions are sensitive to the
character’s implied belief state: the character is unaware that the object has moved to
the End location if they did not observe it being moved. This eﬀect was observed both
when the Knowledge Cue was Implicit (“Sean goes to get the book from the...”) and
Explicit (“Sean thinks the book is in the...”); however, the eﬀect was strengthened in
the Implicit condition (χ2(1) = 20.6, p < 0.001).

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
20
GPT−3
Human
0.00
0.25
0.50
0.75
1.00
0
2
4
6
0
2
4
6
P(Start)
Knowledge State
False Belief
True Belief
Start more likely
End more likely
Figure 2. Both human participants and GPT-3 were more likely to say that a character
believed an object was in the Start location when the character had not observed the
object being moved to the End location (False Belief). This eﬀect was stronger for
humans than for GPT-3, and there was a marginal eﬀect of Knowledge State (True vs.
False Belief) in humans that could not be accounted for by GPT-3 predictions
(χ2(1) = 30.4, p < 0.001).

DO LARGE LANGUAGE MODELS KNOW WHAT HUMANS KNOW?
21
ada
text−ada−001
babbage
text−babbage−001
curie
text−curie−001
davinci
text−davinci−002
Human Baseline
Human Baseline
Human Baseline
Human Baseline
Human Baseline
Human Baseline
Human Baseline
Human Baseline
0.00
0.25
0.50
0.75
1.00
 109
 1010
 1011
Parameters
Accuracy
Figure 3. A number of GPT-3 models varying in size were presented with each passage
and asked to complete the critical sentence, as human participants did. For each model
size, we tested a pre-trained base model (⃝) and a version ﬁne-tuned by OpenAI to
follow text instructions (△). In the True Belief condition, a correct (i.e.,
knowledge-congruent) response corresponded to thse End location of the object; in the
False Belief condition, a correct (i.e., knowledge-congruent) response corresponded to
the Start location of the object. The dotted red line represents human accuracy on the
task (82.7%); text-davinci-002—the largest ﬁne-tuned model—came the closest to
approaching human behavior, with an accuracy of 73.4%.

