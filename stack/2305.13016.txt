Iterative Forward Tuning Boosts
In-context Learning in Language Models
Jiaxi Yang1,
‡, Binyuan Hui2, , Min Yang1†, Binhua Li2, Fei Huang2, Yongbin Li2†
1 Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences
2 DAMO Academy, Alibaba Group
{jx.yang, min.yang}@siat.ac.cn
{binyuan.hby, shuide.lyb}@alibaba-inc.com
https://github.com/AlibabaResearch/DAMO-ConvAI
Abstract
Large language models (LLMs) have exhibited an emergent in-context learning
(ICL) ability. However, the ICL models that can solve ordinary cases are hardly
extended to solve more complex tasks by processing the demonstration examples
once. This single-turn ICL is incoordinate with the decision making process of
humans by learning from analogy. In this paper, we propose an effective and
efﬁcient two-stage framework to boost ICL in LLMs by exploiting a dual form
between Transformer attention and gradient descent-based optimization. Con-
cretely, we divide the ICL process into “Deep-Thinking” and inference stages. The
“Deep-Thinking” stage performs iterative forward optimization of demonstrations,
which is expected to boost the reasoning abilities of LLMs at test time by “thinking”
demonstrations multiple times. It produces accumulated meta-gradients by manip-
ulating the Key-Value matrices in the self-attention modules of the Transformer.
Then, the inference stage only takes the test query as input without concatenating
demonstrations and applies the learned meta-gradients through attention for output
prediction. In this way, demonstrations are not required during the inference stage
since they are already learned and stored in the deﬁnitive meta-gradients. LLMs can
be effectively and efﬁciently adapted to downstream tasks. Extensive experiments
on ten classiﬁcation and multiple-choice datasets show that our method achieves
substantially better performance than standard ICL in terms of both accuracy and
efﬁciency.
1
Introduction
Large language models (LLMs), e.g., OpenAI GPTs [3, 24] and PaLM [5], demonstrate the mys-
terious in-context learning (ICL) ability, where LLMs make predictions directly by prepending
demonstrations to the original input without updating model parameters. LLMs are expected to learn
the patterns hidden in demonstrations and make predictions accordingly. As illustrated in Figure 1 (a),
an LLM can correctly perform inference on an unseen task by conditioning on several demonstrations.
The ICL paradigm empowers LLMs to achieve impressive results on various downstream tasks by
providing a few demonstrations, making Language-Model-as-a-Service (LMaaS) [33] possible.
Since the performance of ICL is sensitive to speciﬁc prompt settings, considerable efforts have
been developed to improve the performance of ICL by reﬁning the prompt design from different
perspectives, such as demonstration selection [19, 17, 39], instruction design [37, 41], and inter-
mediate chain-of-thought (CoT) reasoning [38, 45, 11, 20]. These methods can facilitate LLMs to
Equal contribution.
‡Work done during an intern at Alibaba DAMO Academy.
†Corresponding authors.
arXiv:2305.13016v1  [cs.CL]  22 May 2023

 (a) In-Context Learning
 (b) Ours: Deep-Thinking
❄ LLM
Negative
Review: big time
Sentiment: Positive
Review: in boredom
 Sentiment: Negative
Review: gone wrong
 Sentiment:
demonstrations
test input
}
}
❄ LLM
❄ LLM
↺
 
Thinking…
Review: big time
Sentiment: Positive
Review: in boredom
 Sentiment: Negative
Review: gone wrong
 Sentiment:
Negative
f
K
f
V
demonstrations
test input
}
}
Figure 1: The illustrations of conventional ICL and our two-stage method through “Deep-Thinking”.
reduce inference variance and avoid poor worst-case accuracy to some extent by performing prompt
engineering.
Despite the great success of ICL, the working mechanism of ICL remains to be investigated. Recently,
Dai et al. [6] shed light on the connections between ICL and explicit ﬁne-tuning. Speciﬁcally,
ICL can be understood as a kind of implicit ﬁne-tuning. ICL computes meta-gradients via forward
computation, while explicit ﬁne-tuning obtains gradients by back-propagation. A dual form exists
between attention and gradient descent-based optimization [12], directly connecting the test input to
demonstrations. However, the ICL models that can solve ordinary cases are hardly extended to solve
more complex tasks by processing the demonstration examples once. This single-turn ICL strategy
is incoordinate with the decision process of humans by learning from analogy. Concretely, humans
usually learn from analogy via an iterative thinking process (e.g., analyzing demonstrations, reﬂecting
on demonstrations, and forming abstract concepts). The models learned from demonstrations are able
to extend their reasoning abilities at inference time by “thinking for longer” or “thinking multiple
times” [31]. These ﬁndings inspire us to ask a question: Can we improve the performance of ICL
by training demonstrations through several (iterative) forward inferences?
In this paper, we propose a two-stage framework to boost the ICL ability in LLMs. Instead of simply
concatenating demonstrations and test input together at the inference step, we decouple the ICL
process into a “Deep-Thinking” stage for demonstrations training and a test time inference stage, as
illustrated in Figure 1 (b). The meta-gradient in the form of Key-Value matrices serves as a bridge
between the two stages. In the “Deep-Thinking” stage, we perform iterative forward optimization of
demonstrations by exploiting the dual form between Transformer attention and gradient descent-based
optimization. In particular, we compute accumulated meta-gradients by manipulating the Key-Value
matrices in the self-attention modules of the Transformer. This “Deep-Thinking” strategy is motivated
by humans’ repeat logical thinking and reasoning process. LLMs are expected to extend their abilities
to solve unseen, complex tasks by “thinking” demonstrations for longer or “thinking” demonstrations
multiple times. In the inference stage, only the test input is passed into the model for inference since
the concepts contained in demonstrations are already stored in the deﬁnitive meta-gradients, i.e., the
Key and Value matrices. We apply the learned meta-gradients through attention to make predictions
of test input. Our two-stage ICL method allows LLMs to be efﬁciently adapted to solving complex
downstream tasks and speeding up the inference process.
We conduct extensive experiments on ten datasets involving text classiﬁcation and multiple-choice
tasks. We investigate four LLMs (OPT, BLOOM, E-GPT and GPT-2) with different model sizes.
Experimental results show that (1) our method with “Deep-Thinking” substantially outperforms
standard ICL across various model sizes and tasks; (2) our inference stage only requires the test input,
which further saves GPU memory and speeds up the inference process; (3) there is a tendency of
“gradient norm” with iteration, highlighting the inﬂuence of step size on gradient norm convergence
and the layer-dependent behavior. This is consistent with conventional gradient-based learning
methods [4]. These observations can provide further insights into future ICL design.
2

2
Preliminaries
2.1
In-Context Learning
This paper focuses on two classic tasks, including text classiﬁcation and multiple-choice tasks.
Formally, given a nature language test input xtest with a few (N-shot) input-output demonstrations
Cdemos = {(xi, yi)}N
i=1, the goal of in-context learning is to predict the label ˆy of xtest from a pre-
deﬁned candidate label set Y = {y1, y2, ..., ym} conditioned on N demonstrations. Given an LLM
M (e.g., a GPT model), the prediction process can be formulated as follows:
ˆy = arg max
yj∈Y PM(yj|Cdemos, xtest),
(1)
where P is the output probability of the LLM M. Generally, an LLM adopts the Transformer as
the backbone, which consists of a stack of several Transformer blocks. For clarity, we use Xall =
[Xdemos∥Xtest] to denote the input representations, where Xdemos and Xtest denote the representations of
Cdemos and xtest, respectively.
2.2
Attention as Meta Gradient
The self-attention module is a crucial component of the Transformer blocks. Let K = WKXall,
V = WV Xall and Q = WQXall denote the Key, Value, and Query in self-attention respectively, where
WK, WV , WQ ∈Rdout×din represent learnable weights. As discussed in [12, 6], in-context learning
performs implicit ﬁne-tuning since there exists a dual form relationship between the gradient and
attention. Speciﬁcally, in the ICL settings, when considering qj as the j-th element of Q, the result of
self-attention in an arbitrary layer for a head is formulated as:
Attention(K, V, qj) = WV [Xdemos∥Xtest]softmax((WK[Xdemos∥Xtest])T qj
√din
)
≈WV [Xdemos∥Xtest] (WK[Xdemos∥Xtest])T qj
= WV Xtest(WKXtest)T
|
{z
}
(A) Only test input.
qj + WV Xdemos(WKXdemos)T
|
{z
}
(B) Only demonstrations.
qj
= WZSLqj + ∆WICLqj
= (WZSL + ∆WICL)qj
(2)
where √din serves as a scaling factor. The term (A) WV Xtest(WKXtest)T could be denoted as WZSL,
representing the zero-shot learning setting where no demonstrations are available, as it only considers
the test input. The term (B) WV Xdemos(WKXdemos)T can be seen as the meta gradient ∆WICL [12, 6]
from demonstrations. The reader can refer to previous papers [12, 6] for more details.
3
Methodology
As shown in Equation 2, vanilla ICL can be analogous to a gradient descent process. In this paper, we
propose a two-stage ICL framework that improves performance through multiple forward iterations.
As shown in Figure 2, we assign the demonstrations and test input to the “Deep-Thinking” stage and
inference stage respectively, where a meta-gradient in the form of Key-Value matrices serves as a
bridge between the two stages. Next, we describe these two stages in detail.
3.1
Deep-Thinking Stage
In the “Deep-Thinking” stage, we perform iterative forward optimization of demonstrations by
manipulating the Key-Value matrices in self-attention modules. Concretely, given Xdemos from
demonstrations, it is fed into an LLM comprising L Transformer blocks, each consisting of a
self-attention module and some other components. For simplicity, we use Xl to denote the output
representation of the l-th layer for demonstrations and have X0 = Xdemos. At the t-th forward
optimization step, the self-attention module Attentionl at layer l receives not only the output Xl−1
t
that comes from the previous Transformer block, but also the Key-Value matrices eKl
t−1, eVl
t−1 that are
3

XL
an intoxicating experience.
1
one long string of cliches.
0
it never fails to engage us.
1
a quiet, pure, elliptical film
?
Review: a quiet, pure, elliptical film    Sentiment:
Review: an intoxicating experience. 
Sentiment: Positive
Review: one long string of cliches.
Sentiment: Negative
Review: it never fails to engage us. 
Sentiment: Positive
demonstrations
}
test input
}
❄ LLM
˜
V1
˜
K1
˜
Vt−1
˜
Kt−1
❄ LLM
˜
Vt
˜
Kt
⋯
❄ LLM
˜
V2
˜
K2
⋯
˜
VT−1
˜
KT−1
❄ LLM
˜
VT
˜
KT
Xl
t
Xl+1
t
Attentionl ℱ
Attentionl+1 ℱ
˜
Vl
t
˜
Kl
t
˜
Vl+1
t
˜
Kl+1
t
Xl−1
t
Attentionl−1 ℱ
˜
Vl−1
t
˜
Kl−1
t
Xl−2
t
⋯
⋯
˜
Vt−1
˜
Kt−1
˜
V1
t−1
˜
K1
t−1
⋯
˜
VL
t−1
˜
KL
t−1
⋯
˜
Vt
˜
Kt
˜
Vl
t−1
˜
Kl
t−1
˜
Vl+1
t−1
˜
Kl+1
t−1
˜
Vl−1
t−1
˜
Kl−1
t−1
˜
V1
t−1
˜
K1
t−1
⋯
˜
VL
t−1
˜
KL
t−1
⋯
WK
WV
WQ
Kl
t
Vl
t
Ql
t
concat
update
concat
update
{˜
Kl
t−1∥Kl
t}
{˜
Vl
t−1∥Vl
t}
Xl−1
t
Xl
t
˜
Kl
t−1
˜
Vl
t−1
˜
Vl
t
˜
Kl
t
Attentionl
ℱ
Xdemos
Xtest
❄ LLM
1
2
t
T
Output:
Positive
⋯
⋯
Xdemos
Xdemos
Xdemos
Figure 2: The overview of our two-stage ICL framework. Our method divides the ICL process into
iterative demonstration learning and test time inference stages, which take demonstrations and test
query as input, respectively.
produced by the same self-attention module at the (t −1)-th forward optimization step. Accordingly,
Attentionl outputs Xl
t and obtains updated Key-Value matrices eKl
t, eVl
t.
The internal working mechanism in each block is illustrated in Figure 2. The information ﬂowing
through a block can be observed from both horizontal and vertical processes. The horizontal process
represents the calculation of the input parameters in a conventional manner, while the vertical process
stands for the manipulation of the Key-Value matrices. Speciﬁcally, the input Xl−1
t
is ﬁrstly projected
by key, value and query weight matrices, respectively:
Kl
t = WKXl−1
t
,
Vl
t = WV Xl−1
t
,
Ql
t = WQXl−1
t
(3)
where Kl
t, Vl
t represent the present Key-Value matrices. For the horizontal process, we concatenate
the present Key-Value matrices with the history Key-Value matrices eKl
t−1, eVl
t−1 as the mixed
Key-Value to compute attention map and obtain the output Xl
t of current layer as follows:
Xl
t = F(Attentionl({eKl
t−1∥Kl
t}, {eVl
t−1∥Vl
t}, Ql
t))
(4)
where F refers to the operations after self-attention, namely the Feed-Forward Network (FFN), layer
normalization and residual connection.
Furthermore, the update process is jointly contributed by the present and history Key-Value matrices.
From a high-level abstract perspective, the update process can by formalized as:
eKl
t = update(eKl
t−1, Kl
t),
eVl
t = update(eVl
t−1, Vl
t)
(5)
where eKl
t and eVl
t are updated Key-Value matrices. Inspired by [9], we conduct a simple momentum-
based method to update the Key-Value matrices. The core idea is to accumulate history and present
meta-gradient with momentum iteration. A single update step can be formalized as follows:
eKl
t = eKl
t−1 + ηMeKl
t,
eVl
t = eVl
t−1 + ηMeVl
t
(meta-gradient accumulation)
(6)
4

where MeKl
t and MeVl
t denote momentum terms, which are initialized by zero matrices. Speciﬁcally,
MeKl
t = GeKl
t + βMeKl
t−1,
MeVl
t = GeVl
t + βMeVl
t−1
(momentum term)
(7)
where GeKl
t = Kl
t −eKl
t−1 and GeVl
t = Vl
t −eVl
t−1 denote the movement of gradients. β and η denote
the momentum constant and step size, respectively.
The modeling process for demonstrations takes up to T steps, where the value of T can be predeﬁned
by users. After the iterative optimization process, we can obtain the ﬁnal updated Key-Value matrices
eKl
T , eVl
T . By combining the updated Key-Value matrices of all layers in a given LLM, we have
eKT = {eKl
T }L
l=1,
eVT = {eVl
T }L
l=1
(8)
which can be stored statically. L denotes the number of Transformer blocks in an LLM.
3.2
Inference Stage
The inference process for test input is straightforward. Considering that we now have the Key-Value
matrices eKT , eVT that have been optimized for T steps, the information contained in them can be
regarded as a highly condensed modeling of the demonstrations. The inference process can be
performed using the same formulation as given by Eq.(3)-Eq.(4). Speciﬁcally, the inference process
for l-th layer can be formalized as:
Kl
test = WKXl−1
test ,
Vl
test = WV Xl−1
test ,
Ql
test = WQXl−1
test
Xl
test = F(Attentionl({eKl
T ∥Kl
test}, {eVl
T ∥Vl
test}, Ql
test))
(9)
In this way, we can obtain the representation XL
test produced by the ﬁnal layer, which is then used
to make predictions. It is noteworthy that there is no need to prepend demonstrations to test input,
signiﬁcantly reducing the number of tokens fed into language models and bringing substantial
improvements in efﬁciency. It may make a language model a plug-and-play service for a large variety
of tasks when the “Deep-Thinking” stage has been performed.
4
Experiments
4.1
Experimental Setup
Models
We conduct experiments on four open-source GPT-like (i.e., decoder-only Transformer)
models, including OPT [43](125M, 350M, 1.3B, 2.7B, 6.7B and 13B), E-GPT* [1, 35, 2] (Neo-
125M, Neo-1.3B, Neo-2.7B, J-6B and NeoX-20B), GPT2 [28] (Small, Meidum, Large and XL) and
BLOOM [30] (560M, 1.1B, 1.7B, 3B, 7.1B).
Datasets
We evaluate our method on 10 datasets involving both text classiﬁcation and multiple-
choice tasks. For text classiﬁcation, we choose AGNews [44], MR [25], SST2 [32], SST5 [32]
and TREC [18, 10] as experimental data. For multiple-choice tasks, we choose COPA [34], Hel-
laSwag [42], OpenBookQA [23], QASC [14] and WinoGrande [29]. Due to the limited space, the
information of these datasets are presented alongside the main results, as shown in Table 1 and
Table 2.
Implementation Details
All experiments are conducted on a single NVIDIA A100 GPU. INT8
quantization is applied to optimize GPU memory consumption. As described in Subsection 3.1,
a simple momentum-based optimization method is used, where β and η are set to 0.9 and 0.01,
respectively. In the experiments, we adopt a slightly different N-shot setting from previous ICL
works that receive a total number of N demonstrations. In our experiments, we follow the traditional
few-shot learning setting where N-shot is deﬁned as the number of demonstrations per class. It
requires C ∗N demonstrations for a task with C classes. We set N = 1 for all our experiments.
*EleutherAI has developed a series of pre-trained LLMs with different model sizes, including GPT-Neo,
GPT-J and GPT-NeoX. For the sake of simplicity, we denote them by E-GPT.
5

Table 1: The results of vanilla ICL and our method (denoted by
) on ﬁve classiﬁcation tasks.
SST2
Model & Size
ICL
+Acc(%)
OPT
125M
55.73
72.02+29.22%
350M
54.13
79.70+47.25%
1.3B
88.76
89.45+0.78%
2.7B
62.39
72.36+15.99%
6.7B
83.72
87.84+4.93%
13B
94.04
94.84+0.85%
BLOOM
560M
57.00
69.84+22.54%
1.1B
68.58
74.08+8.03%
1.7B
71.67
72.94+1.76%
3B
72.36
72.36 N/A
7.1B
73.39
76.83+4.69%
E-GPT
Neo-125M
70.64
78.56+11.20%
Neo-1.3B
76.49
84.98+11.09%
Neo-2.7B
84.75
87.96+3.79%
J-6B
92.09
92.09 N/A
NeoX-20B
93.12
93.58+0.49%
GPT2
Small
65.48
73.74+12.61%
Medium
54.13
61.47+13.56%
Large
81.31
86.35+6.21%
XL
63.19
79.36+25.59%
SST5
Model & Size
ICL
+Acc(%)
OPT
125M
26.70
33.24+24.49%
350M
26.61
31.79+19.45%
1.3B
38.96
42.51+9.09%
2.7B
45.78
47.68+4.17%
6.7B
47.23
47.50+0.58%
13B
45.78
47.14+2.98%
BLOOM
560M
30.15
38.15+26.51%
1.1B
39.78
40.05+0.68%
1.7B
42.05
43.96+4.54%
3B
42.69
42.69 N/A
7.1B
45.14
45.69+1.21%
E-GPT
Neo-125M
29.61
36.60+23.62%
Neo-1.3B
41.87
43.51+3.90%
Neo-2.7B
37.60
37.60 N/A
J-6B
46.78
46.78 N/A
NeoX-20B
47.14
47.77+1.35%
GPT2
Small
18.44
22.34+21.18%
Medium
32.88
36.33+10.50%
Large
38.33
41.96+9.48%
XL
32.24
40.15+24.51%
TREC
Model & Size
ICL
+Acc(%)
OPT
125M
25.00
47.00+88.00%
350M
37.20
45.80+23.12%
1.3B
43.00
43.60+1.40%
2.7B
37.00
50.00+35.14%
6.7B
54.80
64.80+18.25%
13B
45.20
55.40+22.57%
BLOOM
560M
35.20
50.00+42.05%
1.1B
50.20
64.00+27.49%
1.7B
57.40
60.20+4.88%
3B
56.00
67.20+20.00%
7.1B
59.60
63.00+5.70%
E-GPT
Neo-125M
29.80
29.80 N/A
Neo-1.3B
50.60
53.20+5.14%
Neo-2.7B
49.60
54.20+9.27%
J-6B
44.20
44.20 N/A
NeoX-20B
67.20
72.00+7.14%
GPT2
Small
43.60
52.00+19.27%
Medium
41.40
42.40+2.42%
Large
54.20
59.20+9.23%
XL
43.80
49.60+13.24%
MR
Model & Size
ICL
+Acc(%)
OPT
125M
44.65
65.76+47.27%
350M
73.17
73.36+0.26%
1.3B
82.74
85.46+3.29%
2.7B
86.21
89.02+3.26%
6.7B
89.59
90.53+1.05%
13B
89.49
90.53+1.15%
BLOOM
560M
50.56
50.75+0.37%
1.1B
56.85
56.85 N/A
1.7B
70.26
70.26 N/A
3B
72.70
78.80+8.39%
7.1B
85.65
86.30+0.77%
E-GPT
Neo-125M
60.51
63.98+5.74%
Neo-1.3B
68.11
69.51+2.07%
Neo-2.7B
85.18
86.30+1.32%
J-6B
90.53
90.53 N/A
NeoX-20B
91.18
91.37+0.21%
GPT2
Small
54.97
54.97 N/A
Medium
54.97
57.97+5.46%
Large
62.85
79.27+26.12%
XL
79.08
85.08+7.59%
AGNews
Model & Size
ICL
+Acc(%)
OPT
125M
41.70
50.55+21.22%
350M
42.90
64.30+49.88%
1.3B
83.30
85.25+2.34%
2.7B
88.60
88.90+0.34%
6.7B
77.80
85.80+10.28%
13B
88.70
88.75+0.06%
BLOOM
560M
46.55
50.05+7.52%
1.1B
53.05
53.60+1.04%
1.7B
77.55
77.80+0.32%
3B
70.45
70.75+0.43%
7.1B
81.90
82.45+0.67%
E-GPT
Neo-125M
28.10
32.25+14.77%
Neo-1.3B
75.70
76.00+0.40%
Neo-2.7B
83.60
84.90+1.56%
J-6B
73.10
78.10+6.84%
NeoX-20B
70.80
75.75+6.99%
GPT2
Small
48.80
48.80 N/A
Medium
70.45
73.90+4.90%
Large
77.75
79.40+2.12%
XL
83.15
85.50+2.83%
Classiﬁcation Tasks
SST2
Subject
Movie Review
Classes
2
|Dtest|
872
SST5
Subject
Movie Review
Classes
5
|Dtest|
1101
TREC
Subject
Question Type
Classes
6
|Dtest|
500
MR
Subject
Movie Review
Classes
2
|Dtest|
1066
AGNews
Subject
News Topic
Classes
4
|Dtest|
2000 (Sampled)
Evaluation Protocol
For both text classiﬁcation and multiple-choice tasks, we combine the test
input and each candidate answer, which are then passed to an LLM. The ﬁnal answer is selected by
summing the probabilities of the tokens belonging to the answer part and choosing the candidate
answer with the highest probability.
4.2
Experimental Results
In this section, we demonstrate the effectiveness and efﬁciency of our ICL framework by answering
the following research questions (RQs).
RQ1. Does our two-stage ICL method beneﬁt the performance?
Table 1 and Table 2 report the overall results of our ICL method (denoted by
) and the vanilla ICL
method (denoted by ICL) on ﬁve text classiﬁcation tasks and ﬁve multiple-choice tasks, respectively.
Note that we set the maximum optimization steps Tmax = 15. The ﬁnal results are reported as
the best results obtained within Tmax steps. We evaluate the inference performance of the updated
Key-Value matrices after each round of optimization. From the results, we can observe that our
method substantially outperforms vanilla ICL by a noticeable margin on almost all tasks. In particular,
although LLMs with small sizes achieve worse performance than those with large sizes, their relative
6

Table 2: The results of ICL and our method (denoted by
) on ﬁve multiple-choice tasks.
COPA
Model & Size
ICL
+Acc(%)
OPT
125M
68.00
69.00+1.47%
350M
68.00
70.00+2.94%
1.3B
73.00
76.00+4.11%
2.7B
72.00
77.00+6.94%
6.7B
79.00
80.00+1.27%
13B
84.00
87.00+3.57%
BLOOM
560M
63.00
68.00+7.94%
1.1B
72.00
74.00+2.78%
1.7B
76.00
79.00+3.95%
3B
74.00
76.00+2.70%
7.1B
83.00
83.00 N/A
E-GPT
Neo-125M
65.00
66.00+1.54%
Neo-1.3B
72.00
75.00+4.17%
Neo-2.7B
80.00
80.00 N/A
J-6B
83.00
85.00+2.41%
NeoX-20B
87.00
89.00+2.30%
GPT2
Small
66.00
66.00 N/A
Medium
74.00
76.00+2.70%
Large
75.00
77.00+2.67%
XL
75.00
76.00+1.33%
OpenBookQA
Model & Size
ICL
+Acc(%)
OPT
125M
15.60
16.40+5.13%
350M
17.80
18.20+2.25%
1.3B
20.60
21.60+4.85%
2.7B
24.40
25.80+5.74%
6.7B
26.00
27.20+4.62%
13B
27.00
28.40+5.19%
BLOOM
560M
28.80
28.80 N/A
1.1B
20.20
23.20+14.85%
1.7B
26.00
29.20+12.31%
3B
27.00
30.60+13.33%
7.1B
25.80
26.40+2.33%
E-GPT
Neo-125M
17.00
17.60+3.53%
Neo-1.3B
23.20
23.20 N/A
Neo-2.7B
25.40
26.40+3.94%
J-6B
26.40
27.00+2.27%
NeoX-20B
33.00
34.20+3.64%
GPT2
Small
16.60
17.60+6.02%
Medium
18.00
19.00+5.56%
Large
19.60
20.20+3.06%
XL
22.20
23.60+6.31%
WinoGrande
Model & Size
ICL
+Acc(%)
OPT
125M
52.17
53.28+2.12%
350M
50.43
52.72+4.54%
1.3B
54.14
56.20+3.79%
2.7B
58.96
59.19+0.40%
6.7B
60.46
60.93+0.78%
13B
63.38
64.33+1.49%
BLOOM
560M
51.62
52.25+1.22%
1.1B
52.64
54.14+2.85%
1.7B
56.75
56.75 N/A
3B
56.43
56.67+0.42%
7.1B
60.30
61.25+1.57%
E-GPT
Neo-125M
51.30
52.33+2.00%
Neo-1.3B
54.38
56.04+3.05%
Neo-2.7B
56.67
56.67 N/A
J-6B
60.54
61.88+2.22%
NeoX-20B
62.90
63.46+0.88%
GPT2
Small
50.43
51.93+2.97%
Medium
50.91
52.01+2.17%
Large
53.51
53.51 N/A
XL
52.01
54.06+3.95%
QASC
Model & Size
ICL
+Acc(%)
OPT
125M
17.60
23.11+31.29%
350M
21.38
26.35+23.23%
1.3B
35.21
38.12+8.28%
2.7B
36.72
40.28+9.71%
6.7B
43.52
45.25+3.97%
13B
44.28
44.92+1.46%
BLOOM
560M
19.22
19.22 N/A
1.1B
24.41
24.51+0.44%
1.7B
29.48
29.91+1.47%
3B
32.51
33.59+3.32%
7.1B
35.96
36.39+1.20%
E-GPT
Neo-125M
18.79
20.52+9.20%
Neo-1.3B
32.18
33.05+2.68%
Neo-2.7B
36.83
39.09+6.16%
J-6B
41.90
44.17+5.41%
NeoX-20B
49.57
51.08+3.05%
GPT2
Small
16.31
18.14+11.26%
Medium
21.49
27.00+25.63%
Large
28.83
30.24+4.87%
XL
36.50
39.09+7.10%
HellaSwag
Model & Size
ICL
+Acc(%)
OPT
125M
26.70
27.30+2.25%
350M
30.40
30.65+0.82%
1.3B
39.50
40.15+1.65%
2.7B
43.95
44.00+0.11%
6.7B
48.70
49.55+1.75%
13B
50.70
50.95+0.49%
BLOOM
560M
28.40
28.70+1.06%
1.1B
31.80
31.90+0.31%
1.7B
36.65
36.85+0.55%
3B
39.75
39.80+0.13%
7.1B
44.20
44.25+0.11%
E-GPT
Neo-125M
26.65
26.65 N/A
Neo-1.3B
36.45
36.65+0.55%
Neo-2.7B
40.35
40.50+0.37%
J-6B
48.20
48.40+0.41%
NeoX-20B
52.00
52.00 N/A
GPT2
Small
27.15
27.85+2.58%
Medium
31.65
31.70+0.16%
Large
33.80
34.30+1.48%
XL
38.60
38.75+0.39%
Multiple-Choice Tasks
COPA
Subject
Causal Reasoning
Choices
2
|Dtest|
100
OpenBookQA
Subject
Commonsense Reasoning
Choices
4
|Dtest|
500
WinoGrande
Subject
Commonsense Reasoning
Choices
2
|Dtest|
1267
QASC
Subject
Sentence Composition
Choices
8
|Dtest|
926
HellaSwag
Subject
Commonsense Reasoning
Choices
4
|Dtest|
2000 (Sampled)
performance improvement is remarkably high. Since the LLMs with small sizes are easily deployed
on real-time applications, the impressive performance improvements shown on LLMs with small
sizes reﬂect the practical value of our ICL method.
RQ2. Does this two-stage method offer efﬁciency improvements?
Instead of simply concatenating demonstrations and test input together for inference, we decouple
the ICL process into a “Deep-Thinking” stage for in-context example training and an inference
stage, being expected to accelerate inference time and reduce the computational workload effec-
tively. Speciﬁcally, as shown in Figure 2, after performing forward optimization for T steps on the
demonstrations, we can obtain Key-Value matrices (eKT and eVT ). During inference, only the test
input is required to be passed into the model for inference, which reduces the GPU RAM usage. It is
noteworthy that we report the maximum batch size that each model can handle for the dataloader
given a single NVIDIA A100 (80G) GPU instead of reporting the inference time. This is because we
ran the program on a cloud computing platform where system instances cannot guarantee exclusive
hardware resources, making it difﬁcult to measure the inference time concisely.
Due to the limited space, we conduct experiments on SST5 to evaluate the efﬁciency of vanilla ICL
and our method. As shown in Figure 3, our ICL method consistently outperforms the vanilla ICL
7

125M
350M
1.3B
2.7B
6.7B
13B
0
200
400
600
800
1000
Batchsize
OPT
20.0
40.0
60.0
80.0
560M
1.1B
1.7B
3B
7.1B
0
100
200
300
400
500
BLOOM
40.0
60.0
80.0
100.0
120.0
Neo-125M
Neo-1.3B
Neo-2.7B
J-6B
NeoX-20B
0
200
400
600
800
1000
E-GPT
20.0
40.0
60.0
80.0
Small
Medium
Large
XL
0
200
400
600
800
1000
GPT2
ICL
+Batchsize (%)
20.0
40.0
60.0
80.0
100.0
Improvement (%)
Figure 3: The maximum batch sizes that ICL and our method (denoted by
) can handle for the
dataloader on the SST5 dataset given a single NVIDIA A100 (80G) GPU by varying model sizes.
method in terms of the maximum batch size that the models can handle. In particular, our method
can provide 48.931.2% increase in batch size on average. Speciﬁcally, the improvement in batch
size stems from the elimination of the need to concatenate demonstrations for each individual test
input. In conventional ICL settings, the demonstrations are prepended to test input, which inevitably
introduces redundant information due to the repeated context. As the number of classes increases,
the redundancy also escalates. On the other hand, our method avoids concatenating demonstrations
at the inference stage and employs an efﬁcient reference-based copying mechanism. This brings a
noticeable enhancement in efﬁciency.
RQ3. How does the number of forward optimization steps T impact the inference performance?
1
3
5
7
9
11 13 15
20
40
60
80
OPT-125M
SST2
TREC
1
3
5
7
9
11 13 15
20
40
60
80
OPT-350M
SST2
TREC
1
3
5
7
9
11 13 15
20
40
60
80
OPT-1.3B
SST2
TREC
1
3
5
7
9
11 13 15
20
40
60
80
OPT-2.7B
SST2
TREC
1
3
5
7
9
11 13 15
20
40
60
80
OPT-6.7B
SST2
TREC
1
3
5
7
9
11 13 15
20
40
60
80
OPT-13B
SST2
TREC
Optimization Steps (T)
Accuracy
Figure 4: The performance curves of our method (denoted by
) by varying the forward optimization
steps T on SST2 and TREC datasets.
Layer (L)
121110 9 8 7 6 5 4 3 2 1
Steps (T)
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
Gradient Norm ∥G∥2
2
0
50
100
150
200
250
300
350
η = 0.1
β = 0.9
Accuracy
66.40
Layer (L)
121110 9 8 7 6 5 4 3 2 1
Steps (T)
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
Gradient Norm ∥G∥2
2
0
50
100
150
200
250
300
350
η = 0.01
β = 0.9
Accuracy
72.02
Layer (L)
121110 9 8 7 6 5 4 3 2 1
Steps (T)
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
Gradient Norm ∥G∥2
2
0
50
100
150
200
250
300
350
η = 0.001
β = 0.9
Accuracy
61.01
Figure 5: The gradient norms change along with optimization steps with respect to different step
sizes (η = {0.1, 0.01, 0.001} from left to right) using OPT-125M on the SST2 dataset.
We also investigate the impact of our iterative “Deep-Thinking” strategy on the overall performance
of our ICL method by varying the number of forward optimization steps T from 1 to 15. Note that the
forward optimization steps can be analogous to the optimization steps in conventional gradient-based
methods, playing a key role in test inference. Due to the limited space, we merely report the prediction
result curves of OPT by varying the value of T on SST2 and TREC datasets.
The experimental results are illustrated in Figure 4. For OPT with small sizes (i.e., 125M and 350M),
as T increases from 1 to 15, the prediction accuracy grows gradually until an optimal value, after
which the prediction accuracy decreases slightly. For OPT with large sizes (e.g., 1.3B, 2.7B and
13B), the prediction accuracy grows quite slightly until an optimal value, after which the prediction
8

accuracy tends to decrease sharply. This may be because iterating too many steps may make the
models overﬁt the demonstrations seriously.
The step size η is also a critical hyperparameter affecting the performance of ICL. We investigate
the impact of the step size η on the overall performance of our ICL method by choosing the value
of η from {0.1, 0.01, 0.001}. Figure 5 shows the results predicted by OPT-125M on SST2. We can
observe that increasing η leads to faster performance peaks, while large η may also result in inferior
performance compared to moderate η. We further illustrate the matrix norm of the pseudo-gradients
GeKl
t with respect to K of each layer in each optimization step. We can ﬁnd that larger η leads to
faster decay of GeKl
t, with the fastest gradient changes occurring near the input layer. These results are
consistent with the conclusions drawn from conventional gradient-based optimization methods [4].
RQ4. How does the momentum-based update process impact the inference performance?
Table 3: The results of our method and its non-momentum variant (denoted by w/o M).
Model & Size
SST2
SST5
TREC
MR
step
[w/o M]step
step
[w/o M]step
step
[w/o M]step
step
[w/o M]step
OPT
125M
72.027
68.1215
33.244
33.427
47.007
42.6015
65.768
59.0114
350M
79.7010
61.1215
31.799
29.8815
45.809
41.6015
73.362
76.172
1.3B
89.458
89.7910
42.5110
40.1515
43.603
43.606
85.469
85.2715
2.7B
72.3610
66.5114
47.6812
46.052
50.0014
37.001
89.0213
87.1514
6.7B
87.8415
83.941
47.5011
47.322
64.8013
57.0014
90.5311
90.0610
13B
94.844
95.079
47.145
47.2312
55.406
55.0014
90.536
89.875
We investigate the efﬁciency of the momentum-based update process by comparing our method with
a special setting where the momentum is disabled, i.e., β = 0.0 (denoted by w/o M). As illustrated
in Table 3, we report the prediction accuracy obtained by OPT on SST2 and the optimization steps
required to reach the corresponding performance. Our method performs substantially better than the
non-momentum method (called w/o M), exhibiting an average improvement of 6.07%. In addition,
our method can reach its peak performance faster when momentum is used, which is consistent with
the role of momentum optimization in conventional gradient-based learning methods [15].
5
Related Work
In-context learning (ICL) with large language models (LLMs) has made a breakthrough and become
mainstream in tackling various tasks [16, 7, 27]. Recently, great efforts have been made to improve
the performance of ICL from different perspectives, such as example selection [19, 17, 39], instruction
design [37], and intermediate chain-of-thought (CoT) reasoning [38, 45].
For example selection, Liu et al. [19] performed demonstration selection through a kNN-based
retriever, choosing the closest example to test input. Wu et al. [39] proposed self-adaptive ICL with
a general select-and-rank framework for demonstration selection. In addition to example selection,
Lu et al. [21] investigated the sensitivity of ICL to the permutation of demonstrations and proposed
entropy metrics to determine their order. The above ICL methods are usually restricted by the number
of demonstrations. To mitigate such a challenge, Hao et al. [8] attempted to scale ICL by grouping
demonstrations, which could increase the number of demonstrations to 1,000.
The formatting function also plays a crucial role in ICL, especially for tasks requiring complex
reasoning steps, such as commonsense reasoning. Wei et al. [38] introduced chain-of-thoughts
(CoT) prompting, where the reasoning steps generated by LLMs are used to provide further guidance.
Zhang et al. [45] stimulated the model’s ability for gradual reasoning by adding the “Let’s think
step-by-step” preﬁx, which showed impressive performance. Instead of generating reasoning steps,
Press et al. [26] investigated the compositional reasoning abilities by allowing LLMs to generate
follow-up questions. Subsequently, Madaan et al. [22] introduced a new framework to enhance the
initial outputs generated by LLMs via iterative feedback and reﬁnement.
Meanwhile, some studies [40, 6, 36] attempt to uncover the underlying working mechanism of ICL. In
particular, Xie et al. [40] showed that ICL happened via Bayesian inference, where certain concepts
were implicitly predicted before the ﬁnal prediction. Subsequently, Dai et al. [6] revealed that there
9

are connections between ICL and explicit ﬁne-tuning and explained LLMs as meta-optimizers [13].
Our method is closely related to [6]. The main difference between our method and [6] is that we
design a “Deep-Thinking” approach to iteratively learn from demonstrations, which is expected to
extend the ability of ICL to solve unseen and complex tasks more effectively.
6
Conclusion
In this paper, we propose a two-stage framework to improve the effectiveness and efﬁciency of ICL
by decomposing the ICL process into a “Deep-Thinking” stage and an inference stage. The “Deep-
Thinking” stage computes meta-gradients conditioned on demonstrations by performing iterative
forward optimization. It exploits the dual form between Transformer attention and gradient descent-
based optimization. In the inference phase, we apply the learned meta-gradients through attention for
output prediction, where only the test input is put into the model for inference without demonstrations.
Our two-stage ICL framework allows LLMs to be efﬁciently adapted to solving complex downstream
tasks and speeding up the inference. Extensive experiments on ten classiﬁcation and multiple-choice
datasets show that our method outperforms conventional ICL in terms of both accuracy and efﬁciency.
References
[1] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale
Autoregressive Language Modeling with Mesh-Tensorﬂow, 2021.
[2] Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,
Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth,
Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-
NeoX-20B: An open-source autoregressive language model. In Proceedings of BigScience
Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models,
pages 95–136, 2022.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
[4] Yixiong Chen, Alan Yuille, and Zongwei Zhou. Which layer is learning faster? a systematic ex-
ploration of layer-wise convergence rate for deep neural networks. In The Eleventh International
Conference on Learning Representations, 2023.
[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[6] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can
GPT learn in-context? language models implicitly perform gradient descent as meta-optimizers.
In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models,
2023.
[7] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing
Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.
[8] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting:
Scaling in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713, 2022.
[9] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for
unsupervised visual representation learning. 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pages 9726–9735, 2019.
[10] Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. Toward
semantics-based answer pinpointing. In Proceedings of the First International Conference on
Human Language Technology Research, 2001.
[11] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A
survey. arXiv preprint arXiv:2212.10403, 2022.
10

[12] Kazuki Irie, Róbert Csordás, and Jürgen Schmidhuber. The dual form of neural networks
revisited: Connecting test time predictions to training patterns via spotlights of attention. In
International Conference on Machine Learning, pages 9639–9659. PMLR, 2022.
[13] Kazuki Irie, Róbert Csordás, and Jürgen Schmidhuber. The dual form of neural networks
revisited: Connecting test time predictions to training patterns via spotlights of attention. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato,
editors, International Conference on Machine Learning, pages 9639–9659. PMLR, 2022.
[14] Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. Qasc: A
dataset for question answering via sentence composition. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 34, pages 8082–8090, 2020.
[15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd
International Conference on Learning Representations, 2015.
[16] Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin,
Rongyu Cao, Ruiying Geng, et al. Can llm already serve as a database interface? a big bench
for large-scale database grounded text-to-sqls. arXiv preprint arXiv:2305.03111, 2023.
[17] Xiaonan Li and Xipeng Qiu. Finding supporting examples for in-context learning. arXiv
preprint arXiv:2302.13539, 2023.
[18] Xin Li and Dan Roth. Learning question classiﬁers. In COLING 2002: The 19th International
Conference on Computational Linguistics, 2002.
[19] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.
What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside
Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep
Learning Architectures, pages 100–114, 2022.
[20] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun
Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language
models. arXiv preprint arXiv:2304.09842, 2023.
[21] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically
ordered prompts and where to ﬁnd them: Overcoming few-shot prompt order sensitivity. In
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 8086–8098, 2022.
[22] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-reﬁne: Iterative reﬁnement
with self-feedback. arXiv preprint arXiv:2303.17651, 2023.
[23] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, pages 2381–2391, 2018.
[24] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[25] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 115–124, 2005.
[26] Oﬁr Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.
Measuring and narrowing the compositionality gap in language models.
arXiv preprint
arXiv:2210.03350, 2022.
[27] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan,
Fei Huang, and Huajun Chen. Reasoning with language model prompting: A survey. arXiv
preprint arXiv:2212.09597, 2022.
[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
11

[29] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106,
2021.
[30] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow,
Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A
176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,
2022.
[31] Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum,
and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with
recurrent networks. Advances in Neural Information Processing Systems, 34:6695–6706, 2021.
[32] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1631–1642, 2013.
[33] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning
for language-model-as-a-service. In Proceedings of The International Conference on Machine
Learning, 2022.
[34] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose
language understanding systems. In Advances in Neural Information Processing Systems, pages
3261–3275, 2019.
[35] Ben Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language
model, 2021.
[36] Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly
topic models: Explaining and ﬁnding good demonstrations for in-context learning. arXiv
preprint arXiv:2301.11916, 2023.
[37] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The
Tenth International Conference on Learning Representations, 2022.
[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022.
[39] Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context
learning: An information compression perspective for in-context example selection and ordering.
arXiv preprint arXiv:2212.10375, 2022.
[40] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of
in-context learning as implicit bayesian inference. In The Tenth International Conference on
Learning Representations, 2022.
[41] Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Large language
models are versatile decomposers: Decompose evidence and questions for table-based reasoning.
arXiv preprint arXiv:2301.13808, 2023.
[42] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can
a machine really ﬁnish your sentence?
In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pages 4791–4800, 2019.
[43] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
transformer language models. arXiv preprint arXiv:2205.01068, 2022.
12

[44] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for
text classiﬁcation. In Advances in Neural Information Processing Systems, pages 649–657,
2015.
[45] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting
in large language models. In The Eleventh International Conference on Learning Representa-
tions, 2023.
13

A
Templates
Classiﬁcation
Task
Template
Labels
SST2
Review:
{query}
Sentiment:
{label}
negative / positive
SST5
Review:
{query}
Sentiment:
{label}
terrible / negative / neutral /
positive / great
TREC
Question:
{query}
Type:
{label}
Abbreviation / Entity / Description /
Person / Location / Number
MR
Review:
{query}
Sentiment:
{label}
negative / positive
AGNews
Article:
{query}
Category:
{label}
World / Sports / Business /
Technology
Table 4: Prompt templates and corresponding labels for classiﬁcation tasks.
Multiple-Choice
Task
Template
COPA
(cause) {query} because {choice}
(effect) {query} therefore {choice}
OpenBookQA
{query} {choice}
WinoGrande
{query_a} {choice} {query_b}
QASC
{query} {choice}
HellaSwag
{query} {choice}
Table 5: Prompt templates for multiple-choice tasks.
14

B
Minimal Implementation
class MomentumOptim:
def __init__(self, step_size=0.01, momentum=0.9):
self.step_size = step_size
self.momentum = momentum
self.m = None # velocity
def upd_m(self, old_m, g): return g + self.momentum * old_m
def upd_x(self, old_x, m): return old_x + self.step_size * m
def __call__(self, old_xs, new_xs):
pesudo_gs = [new_x - old_x for old_x, new_x in zip(old_xs, new_xs)]
if not self.m:
self.m = pesudo_gs
else:
self.m = [self.upd_m(old_m, g) for old_m, g in zip(self.m, pesudo_gs)]
updated_kv = [self.upd_x(old_x, m) for old_x, m in zip(old_xs, self.m)]
return updated_kv
class AttnOptimWrapper:
def __init__(self, llm, **optimizer_args):
self.model = llm
self.kv = None
self.update_k = MomentumOptim(**optimizer_args)
self.update_v = MomentumOptim(**optimizer_args)
def step(self, ctx_ids):
L = len(ctx_ids)
ctx_ids = ctx_ids.unsqueeze(0) # [1, L]
mask = torch.ones_like(ctx_ids).repeat(1, 2 if self.kv else 1)
out = self.model(ctx_ids, mask, past_key_values=self.kv, use_cache=True)
out_kv = out.past_key_values # kv @ (old_ctx + new_ctx)
cur_kv = [[k[:,:,-L:,:], v[:,:,-L:,:]] for k, v in out_kv] # kv @ (new_ctx)
if not self.kv:
self.kv = cur_kv
else:
(old_ks, old_vs), (cur_ks, cur_vs) = zip(*self.kv), zip(*cur_kv)
upd_ks = self.update_k(old_ks, cur_ks)
upd_vs = self.update_v(old_vs, cur_vs)
self.kv = list(zip(upd_ks, upd_vs)) # kv @ (merged_ctx)
return self.kv
def main():
# ... initialization (dataset, model, tokenizer, logger, etc)
ex_str = load_exemplar()
ex_ids, ex_mask = tokenize(ex_str)
meta_optim = AttnOptimWrapper(model)
for idx in range(args.meta_steps):
ex_kv = meta_optim.step(ex_ids)
for B_query_ids, B_query_mask in inference_loader:
bs = len(B_query_ids)
# [B(expanded), L+L’(concat)]
B_merged_mask = torch.cat((ex_mask.expand(bs, -1), B_query_mask), dim=1)
B_kv_shape = (bs, -1, -1, -1)
B_kv = [[k.expand(B_kv_shape), v.expand(B_kv_shape)] for k, v in ex_kv]
B_out = model(B_query_ids, B_merged_mask, past_key_values=B_kv).logits
B_out = F.log_softmax(B_out, dim=-1)
# ...
15

