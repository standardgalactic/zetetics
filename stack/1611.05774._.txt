What Do Recurrent Neural Network Grammars Learn About Syntax?
Adhiguna Kuncoro♠Miguel Ballesteros♦Lingpeng Kong♠
Chris Dyer♠♣Graham Neubig♠
Noah A. Smith♥
♠School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA
♦IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
♣DeepMind, London, UK
♥Computer Science & Engineering, University of Washington, Seattle, WA, USA
{akuncoro,lingpenk,gneubig}@cs.cmu.edu
miguel.ballesteros@ibm.com, cdyer@google.com, nasmith@cs.washington.edu
Abstract
Recurrent
neural
network
grammars
(RNNG) are a recently proposed prob-
abilistic generative modeling family for
natural language.
They show state-of-
the-art language modeling and parsing
performance.
We investigate what in-
formation they learn, from a linguistic
perspective,
through various ablations
to the model and the data, and by aug-
menting the model with an attention
mechanism (GA-RNNG) to enable closer
inspection. We ﬁnd that explicit modeling
of composition is crucial for achieving the
best performance. Through the attention
mechanism,
we ﬁnd that headedness
plays a central role in phrasal represen-
tation (with the model’s latent attention
largely agreeing with predictions made
by hand-crafted head rules, albeit with
some important differences). By training
grammars without nonterminal labels, we
ﬁnd that phrasal representations depend
minimally on nonterminals,
providing
support for the endocentricity hypothesis.
1
Introduction
In this paper, we focus on a recently proposed
class of probability distributions, recurrent neural
network grammars (RNNGs; Dyer et al., 2016),
designed to model syntactic derivations of sen-
tences. We focus on RNNGs as generative proba-
bilistic models over trees, as summarized in §2.
Fitting a probabilistic model to data has often
been understood as a way to test or conﬁrm some
aspect of a theory. We talk about a model’s as-
sumptions and sometimes explore its parameters
or posteriors over its latent variables in order to
gain understanding of what it “discovers” from the
data. In some sense, such models can be thought
of as mini-scientists.
Neural networks, including RNNGs, are capa-
ble of representing larger classes of hypotheses
than traditional probabilistic models, giving them
more freedom to explore. Unfortunately, they tend
to be bad mini-scientists, because their parameters
are difﬁcult for human scientists to interpret.
RNNGs are striking because they obtain state-
of-the-art parsing and language modeling perfor-
mance. Their relative lack of independence as-
sumptions, while still incorporating a degree of
linguistically-motivated prior knowledge, affords
the model considerable freedom to derive its own
insights about syntax. If they are mini-scientists,
the discoveries they make should be of particular
interest as propositions about syntax (at least for
the particular genre and dialect of the data).
This paper manipulates the inductive bias of
RNNGs to test linguistic hypotheses.1
We be-
gin with an ablation study to discover the impor-
tance of the composition function in §3. Based
on the ﬁndings, we augment the RNNG composi-
tion function with a novel gated attention mech-
anism (leading to the GA-RNNG) to incorporate
more interpretability into the model in §4. Using
the GA-RNNG, we proceed by investigating the
role that individual heads play in phrasal represen-
tation (§5) and the role that nonterminal category
labels play (§6). Our key ﬁndings are that lexi-
cal heads play an important role in representing
most phrase types (although compositions of mul-
tiple salient heads are not infrequent, especially
1RNNGs have less inductive bias relative to traditional
unlexicalized probabilistic context-free grammars, but more
than models that parse by transducing word sequences to
linearized parse trees represented as strings (Vinyals et al.,
2015).
Inductive bias is necessary for learning (Mitchell,
1980); we believe the important question is not “how little
can a model get away with?” but rather the beneﬁt of differ-
ent forms of inductive bias as data vary.
arXiv:1611.05774v2  [cs.CL]  10 Jan 2017

for conjunctions) and that nonterminal labels pro-
vide little additional information. As a by-product
of our investigation, a variant of the RNNG with-
out ensembling achieved the best reported super-
vised phrase-structure parsing (93.6 F1; English
PTB) and, through conversion, dependency pars-
ing (95.8 UAS, 94.6 LAS; PTB SD). The code and
pretrained models to replicate our results are pub-
licly available2.
2
Recurrent Neural Network Grammars
An RNNG deﬁnes a joint probability distribution
over string terminals and phrase-structure nonter-
minals.3
Formally, the RNNG is deﬁned by a
triple ⟨N, Σ, Θ⟩, where N denotes the set of non-
terminal symbols (NP, VP, etc.), Σ the set of all
terminal symbols (we assume that N ∩Σ = ∅),
and Θ the set of all model parameters.
Unlike
previous works that rely on hand-crafted rules to
compose more ﬁne-grained phrase representations
(Collins, 1997; Klein and Manning, 2003), the
RNNG implicitly parameterizes the information
passed through compositions of phrases (in Θ and
the neural network architecture), hence weakening
the strong independence assumptions in classical
probabilistic context-free grammars.
The RNNG is based on an abstract state ma-
chine like those used in transition-based parsing,
with its algorithmic state consisting of a stack
of partially completed constituents, a buffer of
already-generated terminal symbols, and a list of
past actions.
To generate a sentence x and its
phrase-structure tree y, the RNNG samples a se-
quence of actions to construct y top-down. Given
y, there is one such sequence (easily identiﬁed),
which we call the oracle, a = ⟨a1, . . . , an⟩used
during supervised training.
The RNNG uses three different actions:
• NT(X), where X ∈N, introduces an open non-
terminal symbol onto the stack, e.g., “(NP”;
• GEN(x), where x ∈Σ, generates a terminal
symbol and places it on the stack and buffer; and
• REDUCE indicates a constituent is now com-
plete. The elements of the stack that comprise
the current constituent (going back to the last
2https://github.com/clab/rnng/tree/
master/interpreting-rnng
3Dyer et al. (2016) also deﬁned a conditional version of
the RNNG that can be used only for parsing; here we focus
on the generative version since it is more ﬂexible and (rather
surprisingly) even learns better estimates of p(y | x).
The hungry cat
NP
(VP
(S
REDUCE
GEN
NT(NP)
NT(VP)
…
cat
hungry
The
a<t
p(at)
ut
Tt
z
}|
{
St
z
}|
{
Figure 1: The RNNG consists of a stack, buffer of
generated words, and list of past actions that lead
to the current conﬁguration. Each component is
embedded with LSTMs, and the parser state sum-
mary ut is used as top-layer features to predict a
softmax over all feasible actions. This ﬁgure is
due to Dyer et al. (2016).
open nonterminal) are popped, a composition
function is executed, yielding a composed rep-
resentation that is pushed onto the stack.
At each timestep, the model encodes the stack,
buffer, and past actions, with a separate LSTM
(Hochreiter and Schmidhuber, 1997) for each
component as features to deﬁne a distribution over
the next action to take (conditioned on the full
algorithmic state). The overall architecture is il-
lustrated in Figure 1; examples of full action se-
quences can be found in Dyer et al. (2016).
A key element of the RNNG is the composition
function, which reduces a completed constituent
into a single element on the stack. This function
computes a vector representation of the new con-
stituent; it also uses an LSTM (here a bidirectional
one). This composition function, which we con-
sider in greater depth in §3, is illustrated in Fig. 2.
NP
u
v
w
NP
u
v
w
NP
x
x
Figure 2: RNNG composition function on each
REDUCE operation; the network on the right mod-
els the structure on the left (Dyer et al., 2016).
Since the RNNG is a generative model, it at-
tempts to maximize p(x, y), the joint distribution

of strings and trees, deﬁned as
p(x, y) = p(a) =
n
Y
t=1
p(at | a1, . . . , at−1).
In other words, p(x, y) is deﬁned as a product
of local probabilities, conditioned on all past ac-
tions. The joint probability estimate p(x, y) can
be used for both phrase-structure parsing (ﬁnding
arg maxy p(y | x)) and language modeling (ﬁnd-
ing p(x) by marginalizing over the set of possi-
ble parses for x). Both inference problems can be
solved using an importance sampling procedure.4
We report all RNNG performance based on the
corrigendum to Dyer et al. (2016).
3
Composition is Key
Given the same data, under both the discrimina-
tive and generative settings RNNGs were found to
parse with signiﬁcantly higher accuracy than (re-
spectively) the models of Vinyals et al. (2015) and
Choe and Charniak (2016) that represent y as a
“linearized” sequence of symbols and parentheses
without explicitly capturing the tree structure, or
even constraining the y to be a well-formed tree
(see Table 1). Vinyals et al. (2015) directly predict
the sequence of nonterminals, “shifts” (which con-
sume a terminal symbol), and parentheses from
left to right, conditional on the input terminal se-
quence x, while Choe and Charniak (2016) used a
sequential LSTM language model on the same lin-
earized trees to create a generative variant of the
Vinyals et al. (2015) model. The generative model
is used to re-rank parse candidates.
Model
F1
Vinyals et al. (2015) – PTB only
88.3
Discriminative RNNG
91.2
Choe and Charniak (2016) – PTB only
92.6
Generative RNNG
93.3
Table 1: Phrase-structure parsing performance on
PTB §23. All results are reported using single-
model performance and without any additional
data.
The results in Table 1 suggest that the RNNG’s
explicit composition function (Fig. 2), which
4Importance sampling works by using a proposal distri-
bution q(y | x) that is easy to sample from. In Dyer et al.
(2016) and this paper, the proposal distribution is the discrim-
inative variant of the RNNG; see Dyer et al. (2016).
Vinyals et al. (2015) and Choe and Charniak
(2016) must learn implicitly, plays a crucial role in
the RNNG’s generalization success. Beyond this,
Choe and Charniak’s generative variant of Vinyals
et al. (2015) is another instance where generative
models trained on the PTB outperform discrimina-
tive models.
3.1
Ablated RNNGs
On close inspection, it is clear that the RNNG’s
three data structures—stack, buffer, and action
history—are redundant. For example, the action
history and buffer contents completely determine
the structure of the stack at every timestep. Every
generated word goes onto the stack, too; and some
past words will be composed into larger structures,
but through the composition function, they are all
still “available” to the network that predicts the
next action. Similarly, the past actions are redun-
dant with the stack. Despite this redundancy, only
the stack incorporates the composition function.
Since each of the ablated models is sufﬁcient to
encode all necessary partial tree information, the
primary difference is that ablations with the stack
use explicit composition, to which we can there-
fore attribute most of the performance difference.
We conjecture that the stack—the component
that makes use of the composition function—is
critical to the RNNG’s performance, and that the
buffer and action history are not. In transition-
based parsers built on expert-crafted features, the
most recent words and actions are useful if they
are salient, although neural representation learners
can automatically learn what information should
be salient.
To test this conjecture, we train ablated RN-
NGs that lack each of the three data structures (ac-
tion history, buffer, stack), as well as one that lacks
both the action history and buffer.5 If our conjec-
ture is correct, performance should degrade most
without the stack, and the stack alone should per-
form competitively.
Experimental settings. We perform our exper-
iments on the English PTB corpus, with §02–21
for training, §24 for validation, and §23 for test;
no additional data were used for training. We fol-
5Note that the ablated RNNG without a stack is quite sim-
ilar to Vinyals et al. (2015), who encoded a (partial) phrase-
structure tree as a sequence of open and close parentheses,
terminals, and nonterminal symbols; our action history is
quite close to this, with each NT(X) capturing a left parenthe-
sis and X nonterminal, and each REDUCE capturing a right
parenthesis.

low the same hyperparameters as the generative
model proposed in Dyer et al. (2016).6 The gen-
erative model did not use any pretrained word em-
beddings or POS tags; a discriminative variant of
the standard RNNG was used to obtain tree sam-
ples for the generative model. All further experi-
ments use the same settings and hyperparameters
unless otherwise noted.
Experimental results. We trained each abla-
tion from scratch, and compared these models on
three tasks: English phrase-structure parsing (la-
beled F1), Table 2; dependency parsing, Table 3,
by converting parse output to Stanford dependen-
cies (De Marneffe et al., 2006) using the tool by
Kong and Smith (2014); and language modeling,
Table 4. The last row of each table reports the
performance of a novel variant of the (stack-only)
RNNG with attention, to be presented in §4.
Model
F1
Vinyals et al. (2015)†
92.1
Choe and Charniak (2016)
92.6
Choe and Charniak (2016)†
93.8
Baseline RNNG
93.3
Ablated RNNG (no history)
93.2
Ablated RNNG (no buffer)
93.3
Ablated RNNG (no stack)
92.5
Stack-only RNNG
93.6
GA-RNNG
93.5
Table 2: Phrase-structure parsing performance on
PTB §23. † indicates systems that use additional
unparsed data (semisupervised). The GA-RNNG
results will be discussed in §4.
Discussion. The RNNG with only a stack is the
strongest of the ablations, and it even outperforms
the “full” RNNG with all three data structures.
Ablating the stack gives the worst among the new
results. This strongly supports the importance of
the composition function: a proper REDUCE oper-
ation that transforms a constituent’s parts and non-
terminal label into a single explicit (vector) repre-
sentation is helpful to performance.
It is noteworthy that the stack alone is stronger
than the original RNNG, which—in principle—
can learn to disregard the buffer and action his-
6The model is trained using stochastic gradient descent,
with a learning rate of 0.1 and a per-epoch decay of 0.08. All
experiments with the generative RNNG used 100 tree sam-
ples for each sentence, obtained by sampling from the local
softmax distribution of the discriminative RNNG.
Model
UAS
LAS
Kiperwasser and Goldberg (2016)
93.9
91.9
Andor et al. (2016)
94.6
92.8
Dozat and Manning (2016)
95.4
93.8
Choe and Charniak (2016)†
95.9
94.1
Baseline RNNG
95.6
94.4
Ablated RNNG (no history)
95.4
94.2
Ablated RNNG (no buffer)
95.6
94.4
Ablated RNNG (no stack)
95.1
93.8
Stack-only RNNG
95.8
94.6
GA-RNNG
95.7
94.5
Table 3:
Dependency parsing performance on
PTB §23 with Stanford Dependencies (De Marn-
effe and Manning, 2008). † indicates systems that
use additional unparsed data (semisupervised).
tory. Since the stack maintains syntactically “re-
cent” information near its top, we conjecture that
the learner is overﬁtting to spurious predictors in
the buffer and action history that explain the train-
ing data but do not generalize well.
A similar performance degradation is seen in
language modeling (Table 4):
the stack-only
RNNG achieves the best performance, and ablat-
ing the stack is most harmful.
Indeed, model-
ing syntax without explicit composition (the stack-
ablated RNNG) provides little beneﬁt over a se-
quential LSTM language model.
Model
Test ppl. (PTB)
IKN 5-gram
169.3
LSTM LM
113.4
RNNG
105.2
Ablated RNNG (no history)
105.7
Ablated RNNG (no buffer)
106.1
Ablated RNNG (no stack)
113.1
Stack-only RNNG
101.2
GA-RNNG
100.9
Table 4: Language modeling: perplexity.
IKN
refers to Kneser-Ney 5-gram LM.
We remark that the stack-only results are
the best published PTB results for both phrase-
structure and dependency parsing among super-
vised models.
4
Gated Attention RNNG
Having established that the composition function
is key to RNNG performance (§3), we now seek
to understand the nature of the composed phrasal
representations that are learned. Like most neural
networks, interpreting the composition function’s

behavior is challenging.
Fortunately, linguistic
theories offer a number of hypotheses about the
nature of representations of phrases that can pro-
vide a conceptual scaffolding to understand them.
4.1
Linguistic Hypotheses
We consider two theories about phrasal represen-
tation. The ﬁrst is that phrasal representations are
strongly determined by a privileged lexical head.
Augmenting grammars with lexical head informa-
tion has a long history in parsing, starting with
the models of Collins (1997), and theories of syn-
tax such as the “bare phrase structure” hypothe-
sis of the Minimalist Program (Chomsky, 1993)
posit that phrases are represented purely by sin-
gle lexical heads. Proposals for multiple headed
phrases (to deal with tricky cases like conjunction)
likewise exist (Jackendoff, 1977; Keenan, 1987).
Do the phrasal representations learned by RN-
NGs depend on individual lexical heads or mul-
tiple heads? Or do the representations combine all
children without any salient head?
Related to the question about the role of heads
in phrasal representation is the question of whether
phrase-internal material wholly determines the
representation of a phrase (an endocentric repre-
sentation) or whether nonterminal relabeling of a
constitutent introduces new information (exocen-
tric representations). To illustrate the contrast, an
endocentric representation is representing a noun
phrase with a noun category, whereas S →NP VP
exocentrically introduces a new syntactic category
that is neither NP nor VP (Chomsky, 1970).
4.2
Gated Attention Composition
To investigate what the stack-only RNNG learns
about headedness (and later endocentricity), we
propose a variant of the composition function
that makes use of an explicit attention mechanism
(Bahdanau et al., 2015) and a sigmoid gate with
multiplicative interactions, henceforth called GA-
RNNG.
At every REDUCE operation, the GA-RNNG as-
signs an “attention weight” to each of its chil-
dren (between 0 and 1 such that the total weight
off all children sums to 1), and the parent phrase
is represented by the combination of a sum of
each child’s representation scaled by its attention
weight and its nonterminal type.
Our weighted
sum is more expressive than traditional head rules,
however, because it allows attention to be divided
among multiple constituents.
Head rules, con-
versely, are analogous to giving all attention to one
constituent, the one containing the lexical head.
We now formally deﬁne the GA-RNNG’s com-
position function. Recall that ut is the concatena-
tion of the vector representations of the RNNG’s
data structures, used to assign probabilities to each
of the actions available at timestep t (see Fig. 1, the
layer before the softmax at the top). For simplicity,
we drop the timestep index here. Let ont denote
the vector embedding (learned) of the nonterminal
being constructed, for the purpose of computing
attention weights.
Now let c1, c2, . . . denote the sequence of vec-
tor embeddings for the constituents of the new
phrase. The length of these vectors is deﬁned by
the dimensionality of the bidirectional LSTM used
in the original composition function (Fig. 2). We
use semicolon (;) to denote vector concatenation
operations.
The attention vector is given by:
a = softmax

[c1 c2 · · · ]⊤V [u; ont]

(1)
Note that the length of a is the same as the num-
ber of constituents, and that this vector sums to
one due to the softmax. It divides a single unit of
attention among the constituents.
Next, note that the constituent source vector
m = [c1; c2; · · · ]a is a convex combination of the
child-constituents, weighted by attention. We will
combine this with another embedding of the non-
terminal denoted as tnt (separate from ont) using
a sigmoid gating mechanism:
g = σ (W1tnt + W2m + b)
(2)
Note that the value of the gate is bounded between
[0, 1] in each dimension.
The new phrase’s ﬁnal representation uses
element-wise multiplication (⊙) with respect to
both tnt and m, a process reminiscent of the
LSTM “forget” gate:
c = g ⊙tnt + (1 −g) ⊙m.
(3)
The intuition is that the composed represen-
tation should incorporate both nonterminal in-
formation and information about the constituents
(through weighted sum and attention mechanism).
The gate g modulates the interaction between
them to account for varying importance between
the two in different contexts.

Experimental results. We include this model’s
performance in Tables 2–4 (last row in all ta-
bles). It is clear that the model outperforms the
baseline RNNG with all three structures present
and achieves competitive performance with the
strongest, stack-only, RNNG variant.
5
Headedness in Phrases
We now exploit the attention mechanism to probe
what the RNNG learns about headedness on the
WSJ §23 test set (unseen before by the model).
5.1
The Heads that GA-RNNG Learns
The attention weight vectors tell us which con-
stituents are most important to a phrase’s vector
representation in the stack. Here, we inspect the
attention vectors to investigate whether the model
learns to center its attention around a single, or
by extension a few, salient elements, which would
conﬁrm the presence of headedness in GA-RNNG.
First, we consider several major nonterminal
categories, and estimate the average perplexity of
the attention vectors. The average perplexity can
be interpreted as the average number of “choices”
for each nonterminal category; this value is only
computed for the cases where the number of com-
ponents in the composed phrase is at least two
(otherwise the attention weight would be trivially
1). The minimum possible value for the perplexity
is 1, indicating a full attention weight around one
component and zero everywhere else.
Figure 3 (in blue) shows much less than 2
average “choices” across nonterminal categories,
which also holds true for all other categories not
shown. For comparison we also report the average
perplexity of the uniform distribution for the same
nonterminal categories (Fig. 3 in red); this repre-
sents the highest entropy cases where there is no
headedness at all by assigning the same attention
weight to each constituent (e.g. attention weights
of 0.25 each for phrases with four constituents).
It is clear that the learned attention weights have
much lower perplexity than the uniform distribu-
tion baseline, indicating that the learned attention
weights are quite peaked around certain compo-
nents. This implies that phrases’ vectors tend to
resemble the vector of one salient constituent, but
not exclusively, as the perplexity for most cate-
gories is still not close to one.
Next, we consider the how attention is dis-
tributed for the major nonterminal categories in
Table 5, where the ﬁrst ﬁve rows of each category
represent compositions with highest entropy, and
the next ﬁve rows are qualitatively analyzed. The
high-entropy cases where the attention is most di-
vided represent more complex phrases with con-
junctions or more than one plausible head.
NPs. In most simple noun phrases (representa-
tive samples in rows 6–7 of Table 5), the model
pays the most attention to the rightmost noun
and assigns near-zero attention on determiners and
possessive determiners, while also paying nontriv-
ial attention weights to the adjectives. This ﬁnding
matches existing head rules and our intuition that
nouns head noun phrases, and that adjectives are
more important than determiners.
We analyze the case where the noun phrase con-
tains a conjunction in the last three rows of Table
5. The syntax of conjunction is a long-standing
source of controversy in syntactic analysis (Johan-
nessen, 1998, inter alia). Our model suggests that
several representational strategies are used, when
coordinating single nouns, both the ﬁrst noun (8)
and the last noun (9) may be selected. However, in
the case of conjunctions of multiple noun phrases
(as opposed to multiple single-word nouns), the
model consistently picks the conjunction as the
head. All of these representational strategies have
been argued for individually on linguistic grounds,
and since we see all of them present, RNNGs face
the same confusion that linguists do.
VPs.
The attention weights on simple verb
phrases (e.g., “VP →V NP”, 9) are peaked around
the noun phrase instead of the verb. This implies
that the verb phrase would look most similar to the
noun under it and contradicts existing head rules
that unanimously put the verb as the head of the
verb phrase.
Another interesting ﬁnding is that
the model pays attention to polarity information,
where negations are almost always assigned non-
trivial attention weights.7
Furthermore, we ﬁnd
that the model attends to the conjunction terminal
in conjunctions of verb phrases (e.g., “VP →VP
and VP”, 10), reinforcing the similar ﬁnding for
conjunction of noun phrases.
PPs.
In almost all cases, the model attends
to the preposition terminal instead of the noun
phrases or complete clauses under it, regardless of
the type of preposition. Even when the preposi-
7Cf. Li et al. (2016), where sequential LSTMs discover
polarity information in sentiment analysis, although perhaps
more surprising as polarity information is less intuitively cen-
tral to syntax and language modeling.

tional phrase is only used to make a connection
between two noun phrases (e.g., “PP →NP after
NP”, 10), the prepositional connector is still con-
sidered the most salient element. This is less con-
sistent with the Collins and Stanford head rules,
where prepositions are assigned a lower prior-
ity when composing PPs, although more consis-
tent with the Johansson head rule (Johansson and
Nugues, 2007).
ADJP
VP
NP
PP
QP
SBAR
1
1.5
2
2.5
3
Figure 3: Average perplexity of the learned atten-
tion vectors on the test set (blue), as opposed to
the average perplexity of the uniform distribution
(red), computed for each major phrase type.
5.2
Comparison to Existing Head Rules
To better measure the overlap between the atten-
tion vectors and existing head rules, we converted
the trees in PTB §23 into a dependency represen-
tation using the attention weights. In this case,
the attention weight functions as a “dynamic” head
rule, where all other constituents within the same
composed phrase are considered to modify the
constituent with the highest attention weight, re-
peated recursively. The head of the composed rep-
resentation for “S” at the top of the tree is attached
to a special root symbol and functions as the head
of the sentence.
We measure the overlap between the resulting
tree and conversion results of the same trees us-
ing the Collins (1997) and Stanford dependencies
(De Marneffe et al., 2006) head rules. Results are
evaluated using the standard evaluation script (ex-
cluding punctuation) in terms of UAS, since the
attention weights do not provide labels.
Results. The model has a higher overlap with
the conversion using Collins head rules (49.8
UAS) rather than the Stanford head rules (40.4
UAS). We attribute this large gap to the fact that
the Stanford head rules incorporate more semantic
considerations, while the RNNG is a purely syn-
tactic model. In general, the attention-based tree
output has a high error rate (≈90%) when the de-
pendent is a verb, since the constituent with the
highest attention weight in a verb phrase is of-
ten the noun phrase instead of the verb, as dis-
cussed above. The conversion accuracy is better
for nouns (≈50% error), and much better for de-
terminers (30%) and particles (6%) with respect to
the Collins head rules.
Discussion. GA-RNNG has the power to in-
fer head rules, and to a large extent, it does. It
follows some conventions that are established in
one or more previous head rule sets (e.g., preposi-
tions head prepositional phrases, nouns head noun
phrases), but attends more to a verb phrase’s nom-
inal constituents than the verb.
It is important
to note that this is not the by-product of learn-
ing a speciﬁc model for parsing; the training ob-
jective is joint likelihood, which is not a proxy
loss for parsing performance.
These decisions
were selected because they make the data maxi-
mally likely (though admittedly only locally max-
imally likely). We leave deeper consideration of
this noun-centered verb phrase hypothesis to fu-
ture work.
6
The Role of Nonterminal Labels
Emboldened by our ﬁnding that GA-RNNGs learn
a notion of headedness, we next explore whether
heads are sufﬁcient to create representations of
phrases (in line with an endocentric theory of
phrasal representation) or whether extra nontermi-
nal information is necessary. If the endocentric
hypothesis is true (that is, the representation of a
phrase is built from within depending on its com-
ponents but independent of explicit category la-
bels), then the nonterminal types should be easily
inferred given the endocentrically-composed rep-
resentation, and that ablating the nonterminal in-
formation would not make much difference in per-
formance. Speciﬁcally, we train a GA-RNNG on
unlabeled trees (only bracketings without nonter-
minal types), denoted U-GA-RNNG.
This idea has been explored in research on
methods for learning syntax with less complete
annotation (Pereira and Schabes, 1992).
A key
ﬁnding from Klein and Manning (2002) was that,

Noun phrases
Verb phrases
Prepositional phrases
1
Canadian (0.09) Auto (0.31) Workers (0.2) union (0.22) president (0.18)
buying (0.31) and (0.25) selling (0.21) NP (0.23)
ADVP (0.14) on (0.72) NP (0.14)
2
no (0.29) major (0.05) Eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22)
ADVP (0.27) show (0.29) PRT (0.23) PP (0.21)
ADVP (0.05) for (0.54) NP (0.40)
3
Saatchi (0.12) client (0.14) Philips (0.21) Lighting (0.24) Co. (0.29)
pleaded (0.48) ADJP (0.23) PP (0.15) PP (0.08) PP (0.06)
ADVP (0.02) because (0.73) of (0.18) NP (0.07)
4
nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25)
received (0.33) PP (0.18) NP (0.32) PP (0.17)
such (0.31) as (0.65) NP (0.04)
5
the (0.1) Jamaica (0.1) Tourist (0.03) Board (0.17) ad (0.20) account (0.40)
cut (0.27) NP (0.37) PP (0.22) PP (0.14)
from (0.39) NP (0.49) PP (0.12)
6
the (0.0) ﬁnal (0.18) hour (0.81)
to (0.99) VP (0.01)
of (0.97) NP (0.03)
7
their (0.0) ﬁrst (0.23) test (0.77)
were (0.77) n’t (0.22) VP (0.01)
in (0.93) NP (0.07)
8
Apple (0.62) , (0.02) Compaq (0.1) and (0.01) IBM (0.25)
did (0.39) n’t (0.60) VP (0.01)
by (0.96) S (0.04)
9
both (0.02) stocks (0.03) and (0.06) futures (0.88)
handle (0.09) NP (0.91)
at (0.99) NP (0.01)
10
NP (0.01) , (0.0) and (0.98) NP (0.01)
VP (0.15) and (0.83) VP 0.02)
NP (0.1) after (0.83) NP (0.06)
Table 5: Attention weight vectors for some representative samples for NPs, VPs, and PPs.
given bracketing structure, simple dimensional-
ity reduction techniques could reveal conventional
nonterminal categories with high accuracy; Petrov
et al. (2006) also showed that latent variables can
be used to recover ﬁne-grained nonterminal cate-
gories. We therefore expect that the vector em-
beddings of the constituents that the U-GA-RNNG
correctly recovers (on test data) will capture cate-
gories similar to those in the Penn Treebank.
Experiments. Using the same hyperparameters
and the PTB dataset, we ﬁrst consider unlabeled
F1 parsing accuracy. On test data (with the usual
split), the GA-RNNG achieves 94.2%, while the
U-GA-RNNG achieves 93.5%. This result sug-
gests that nonterminal category labels add a rel-
atively small amount of information compared to
purely endocentric representations.
Visualization. If endocentricity is largely suf-
ﬁcient to account for the behavior of phrases,
where do our robust intuitions for syntactic cate-
gory types come from? We use t-SNE (van der
Maaten and Hinton, 2008) to visualize composed
phrase vectors from the U-GA-RNNG model ap-
plied to the unseen test data. Fig. 4 shows that the
U-GA-RNNG tends to recover nonterminal cate-
gories as encoded in the PTB, even when trained
without them.8
These results suggest nontermi-
nal types can be inferred from the purely endocen-
tric compositional process to a certain extent, and
that the phrase clusters found by the U-GA-RNNG
largely overlap with nonterminal categories.
Analysis of PP and SBAR. Figure 4 indicates
a certain degree of overlap between SBAR (red)
and PP (yellow). As both categories are interest-
ing from the linguistic perspective and quite sim-
ilar, we visualize the learned phrase vectors of 40
randomly selected SBARs and PPs from the test
set (using U-GA-RNNG), illustrated in Figure 5.
First, we can see that phrase representations for
PPs and SBARs depend less on the nonterminal
8We see a similar result for the non-ablated GA-RNNG
model, not shown for brevity.
Figure 4: t-SNE on composed vectors when train-
ing without nonterminal categories.
Vectors in
dark blue are VPs, red are SBARs, yellow are PPs,
light blue are NPs, and green are Ss.
categories9 and more on the connector. For in-
stance, the model learns to cluster phrases that
start with words that can be either prepositions
or complementizers (e.g., for, at, to, under, by),
regardless of whether the true nonterminal labels
are PPs or SBARs. This suggests that SBARs that
start with “prepositional” words are similar to PPs
from the model’s perspective.
Second, the model learns to disregard the word
that, as “SBAR →that S” and “SBAR →S” are
close together. This ﬁnding is intuitive, as comple-
mentizer that is often optional (Jaeger, 2010), un-
like prepositional words that might describe rela-
tive time and location. Third, certain categories of
PPs and SBARs form their own separate clusters,
such as those that involve the words because and
of. We attribute these distinctions to the fact that
these words convey different meanings than many
prepositional words; the word of indicates posses-
sion while because indicates cause-and-effect re-
lationship. These examples show that, to a cer-
tain extent, the GA-RNNG is able to learn non-
9Recall that U-GA-RNNG is trained without access to the
nonterminal labels; training the model with nonterminal in-
formation would likely change the ﬁndings.

trivial semantic information, even when trained on
a fairly small amount of syntactic data.
SBAR that S
SBAR that S
SBAR that S
SBAR S
SBAR S
SBAR S
SBAR S
SBAR because S
PP ADVP above NP
SBAR ADVP WHADVP S
SBAR for S
PP at NP
PP at NP
PP after NP
PP by NP
PP under NP
PP by NP
SBAR as S
PP to NP
PP to NP
PP to NP
PP on NP
PP ADVP above NP
SBAR SBAR and SBAR
SBAR than S
PP than NP
PP from S
PP about NP
SBAR WHADVP
PP about NP
SBAR WHNP S
SBAR WHNP S
SBAR WHNP S
SBAR S
PP of S
PP of NP
PP of NP
PP of NP
PP of NP
Figure 5: Sample of PP and SBAR phrase repre-
sentations.
7
Related Work
The problem of understanding neural network
models in NLP has been previously studied for se-
quential RNNs (Karpathy et al., 2015; Li et al.,
2016). Shi et al. (2016) showed that sequence-to-
sequence neural translation models capture a cer-
tain degree of syntactic knowledge of the source
language, such as voice (active or passive) and
tense information, as a by-product of the transla-
tion objective. Our experiment on the importance
of composition function was motivated by Vinyals
et al. (2015) and Wiseman and Rush (2016), who
achieved competitive parsing accuracy without ex-
plicit composition.
In another work, Li et al.
(2015) investigated the importance of recursive
tree structures (as opposed to linear recurrent mod-
els) in four different tasks, including sentiment
and semantic relation classiﬁcation. Their ﬁndings
suggest that recursive tree structures are beneﬁcial
for tasks that require identifying long-range rela-
tions, such as semantic relationship classiﬁcation,
with no conclusive advantage for sentiment classi-
ﬁcation and discourse parsing. Through the stack-
only ablation we demonstrate that the RNNG com-
position function is crucial to obtaining state-of-
the-art parsing performance.
Extensive
prior
work
on
phrase-structure
parsing
typically
employs
the
probabilistic
context-free grammar formalism, with lexicalized
(Collins, 1997) and nonterminal (Johnson, 1998;
Klein and Manning, 2003) augmentations.
The
conjecture that ﬁne-grained nonterminal rules and
labels can be discovered given weaker bracketing
structures was based on several studies (Chiang
and Bikel, 2002;
Klein and Manning, 2002;
Petrov et al., 2006).
In a similar work, Sangati and Zuidema (2009)
proposed entropy minimization and greedy famil-
iarity maximization techniques to obtain lexical
heads from labeled phrase-structure trees in an un-
supervised manner. In contrast, we used neural
attention to obtain the “head rules” in the GA-
RNNG; the whole model is trained end-to-end to
maximize the log probability of the correct ac-
tion given the history.
Unlike prior work, GA-
RNNG allows the attention weight to be divided
among phrase constituents, essentially propagat-
ing (weighted) headedness information from mul-
tiple components.
8
Conclusion
We probe what recurrent neural network gram-
mars learn about syntax, through ablation sce-
narios and a novel variant with a gated atten-
tion mechanism on the composition function. The
composition function, a key differentiator between
the RNNG and other neural models of syntax, is
crucial for good performance.
Using the atten-
tion vectors we discover that the model is learning
something similar to heads, although the attention
vectors are not completely peaked around a sin-
gle component. We show some cases where the
attention vector is divided and measure the rela-
tionship with existing head rules. RNNGs without
access to nonterminal information during training
are used to support the hypothesis that phrasal rep-
resentations are largely endocentric, and a visu-
alization of representations shows that traditional
nonterminal categories fall out naturally from the
composed phrase vectors. This conﬁrms previous
conjectures that bracketing annotation does most
of the work of syntax, with nonterminal categories
easily discoverable given bracketing.
Acknowledgments
This work was sponsored in part by the Defense Advanced
Research Projects Agency (DARPA) Information Innovation
Ofﬁce (I2O) under the Low Resource Languages for Emer-
gent Incidents (LORELEI) program issued by DARPA/I2O
under Contract No. HR0011-15-C-0114; it was also sup-
ported in part by Contract No. W911NF-15-1-0543 with
DARPA and the Army Research Ofﬁce (ARO). Approved for
public release, distribution unlimited. The views expressed
are those of the authors and do not reﬂect the ofﬁcial policy
or position of the Department of Defense or the U.S. Govern-
ment.

References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Proc.
of ACL.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. of ICLR.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In Proc. of COL-
ING.
Do Kook Choe and Eugene Charniak. 2016. Parsing
as language modeling. In Proc. of EMNLP.
Noam Chomsky. 1970. Remarks on nominalization.
In Readings in English Transformational Grammar.
Noam Chomsky. 1993. A Minimalist Program for Lin-
guistic Theory.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. of EACL.
Marie-Catherine De Marneffe and Christopher D.
Manning. 2008. Stanford typed dependencies man-
ual.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of LREC.
Timothy Dozat and Christopher D. Manning.
2016.
Deep biafﬁne attention for neural dependency pars-
ing. arXiv:1611.01734.
Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural net-
work grammars. In Proc. of NAACL.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Computation.
Ray Jackendoff. 1977. X’ Syntax.
T. Florian Jaeger.
2010.
Redundancy and reduc-
tion: Speakers manage syntactic information den-
sity. Cognitive Psychology, 61(1).
Janne Bondi Johannessen. 1998. Coordination.
Richard Johansson and Pierre Nugues.
2007.
Ex-
tended constituent-to-dependency conversion for
English. In Proc. of NODALIDA.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics.
Andrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.
Visualizing and understanding recurrent networks.
arXiv:1506.02078.
Edward L. Keenan.
1987.
Multiply-headed noun
phrases. Linguistic Inquiry, 18(3):481–490.
Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. TACL.
Dan Klein and Christopher D. Manning.
2002.
A
generative constituent-context model for improved
grammar induction. In Proc. of ACL.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. of ACL.
Lingpeng Kong and Noah A. Smith. 2014. An em-
pirical comparison of parsing methods for stanford
dependencies. arXiv:1404.4314.
Jiwei Li, Dan Jurafsky, and Eduard Hovy. 2015. When
are tree structures necessary for deep learning of rep-
resentations? In Proc. of EMNLP.
Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2016. Visualizing and understanding neural models
in NLP. In Proc. of NAACL.
Tom M. Mitchell. 1980. The need for biases in learn-
ing generalizations.
Fernando Pereira and Yves Schabes.
1992.
Inside-
outside reestimation from partially bracketed cor-
pora. In Proc. of ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of COLING-ACL.
Federico Sangati and Willem Zuidema. 2009. Unsu-
pervised methods for head assignments. In Proc. of
EACL.
Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural MT learn source syntax?
In
Proc. of EMNLP.
Laurens van der Maaten and Geoffrey E. Hinton.
2008.
Visualizing high-dimensional data using t-
sne. Journal of Machine Learning Research, 9.
Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In NIPS.
Sam Wiseman and Alexander M. Rush.
2016.
Sequence-to-sequence learning as beam-search op-
timization. In Proc. of EMNLP.

