Compositional Active Inference
A “Process Theory” for Finding Right Abstractions
Toby St Clere Smithe
Topos Institute
&
University of Oxford
12 May 2021
0 / 36

Some Background
My background is in computational neuroscience. (I’m a DPhil student at Oxford.)
But I was dissatisfied with its mathematical structure:
How do models relate, or plug together?
How can we translate ideas between groups?
How can we stop reinventing concepts?
I realized I was seeking a compositional lingua franca,
suﬀicient to express common paterns, translate ideas, and connect things together ...
something like category theory, in fact ...
1 / 36

This Talk
So today I’m going to tell you some things I have learned,
about the “structure of learning structure”.
I’m going to emphasize compositionality.
This entails taking interaction seriously.
I’m going to sketch the beginnings of a new story:
How do adaptive systems find abstractions?
(How do things learn anything?)
How are their beliefs structured?
(Do we ever have more than consensus?)
At Topos, we’re interested in telling such new stories for all kinds of systems:
we seek a new, interconnected, scientific ontology.
We hope that acknowledging interaction promotes sustainability.
2 / 36

Compositional Worlds
Overview
1 Compositional Worlds
Compositional Spaces
Compositional Things
2 Compositional Models
Compositional Probability
Statistical Games
3 Compositional Life
Polynomial Morphology
Active Inference Doctrines
3 / 36

Compositional Worlds
Compositionality at Large (1/3)
The world is complicated — and so are the things within it!
But at the same time, it seems to be made of parts plugged together.
So: if we are to have a hope of understanding complexity,
we might as well make use of this latent structure.
Isn’t “making use of latent structure” what “right abstractions” are all about?
4 / 36

Compositional Worlds
Compositionality at Large (2/3)
Categorical and compositional structures are everywhere:
Maps of the world
Topological spaces
Neural circuits
Qantum circuits
Bayesian networks
Dynamical systems
Logical statements
Natural-language statements
Recipes
Computer programs
Especially, any kind of ‘open’ system ...
5 / 36

Compositional Worlds
Compositionality at Large (3/3)
Taking connectedness and compositionality seriously
means acknowledging — really acknowledging! — two related things:
1 Interaction
2 Context-dependence
Afer all: only tautologies are true in a vacuous context, without assumptions!
Theorem (Fundamental Theorem of Category Theory)
Yoneda’s Lemma says: the ways that some thing connects to others determine the thing itself.
Firth’s maxim: “You shall know a word [or place or concept or ...] by the company it keeps”.
This applies equally to our abstractions, our models, and the stories they tell.
6 / 36

Compositional Worlds
Compositional Spaces
Compositional Spaces
Space is the stage on which events occur:
where we exist, and over which our beliefs lie.
Space itself is compositional, by ‘gluing’:
Hatcher’s ‘house’ [1]
A simplicial complex [2]
7 / 36

Compositional Worlds
Compositional Spaces
Categorified Spaces
Spaces are more than just collections of points glued together:
there are also paths between the points.
We can ‘categorify’ spaces accordingly: paths become morphisms.
This points to topos theory.
8 / 36

Compositional Worlds
Compositional Spaces
Topoi as Spaces
A topos is a particularly nice kind of categorified space.
In a “big topos”, we can think of the points (objects) themselves as spaces,
and the paths as maps between spaces:
−→
9 / 36

Compositional Worlds
Compositional Spaces
Bundles and Dependent Types
The slice over an object X is the category of objects with maps into X.
We can think of this as: “the topos in the context of X”.
Its objects are bundles:
Bundles represent dependent types:
“types with context”.
Think: “places” and “stuﬀthat can happen in each place”.
Examples: weather forecast; any sheaf (ataching data to spaces); polynomial functors ...
10 / 36

Compositional Worlds
Compositional Spaces
Logic and Dependence
Each topos comes with a powerful “internal language”:
a logic over its corresponding space.
This logic is higher-order:
We have conjunctions, disjunctions, implications;
and quantifiers.
We have a “subobject classifier”, assigning truth-values to
propositions:
Roughly, ‘where’ a proposition is true.
And we can construct dependent types, too.
The slice over an object B is the topos of
dependent types over B.
B gives us a logical context:
it encapsulates our axioms, or
our assumptions about the structure of the space.
Changing our assumptions (our context) corresponds to
moving around the base topos (where B lives).
11 / 36

Compositional Worlds
Compositional Things
Compositional Things
The world is full of things, not just spaces!
Biology, full of interconnection, supplies numerous examples:
(pictures from Wikipedia)
And of course: artificial objects, machines, etc.
We can formalize their interconnection.
12 / 36

Compositional Worlds
Compositional Things
Polynomials for Interconnection
We can describe the interconnection of systems using polynomials [3]: (input to output :: lef to right)
Z
C
C
Z
Y
B
Z
C
C
Z
Y
B
Y
B
CyZ
BZyYC
BZyYC ⊗CyZ →ByY
NB: ‘Polynomial’ notation: coeﬀicients are outputs, exponents are inputs
We will see later that these shapes can be much more sophisticated.
But ... how do we know what stuﬀis out there?
13 / 36

Compositional Models
Overview
1 Compositional Worlds
Compositional Spaces
Compositional Things
2 Compositional Models
Compositional Probability
Statistical Games
3 Compositional Life
Polynomial Morphology
Active Inference Doctrines
14 / 36

Compositional Models
Whence the élan vital?
So we can start to talk about stuﬀ. But what about animated stuﬀ?
And how does “animated stuﬀ” itself know about stuﬀ?
More precisely, we’ll ask the following questions:
What is the structure of an agent’s internal model?
So: what is the structure of Bayesian inversion?
What structure underlies the "modularity of mind"?
How do these models relate to the (interconnection of) their forms?
15 / 36

Compositional Models
Compositional Probability
Introducing Markov Categories
16 / 36

Compositional Models
Compositional Probability
Joint states and generative models
17 / 36

Compositional Models
Compositional Probability
Bayes’ rule, categorically
As a ‘dagger’ operation on stochastic channels:
Note that the Bayesian inverse channel depends on the prior!
18 / 36

Compositional Models
Compositional Probability
The bidirectionality of Bayesian inference
Given a ‘generative’ channel c : X →DY, the
corresponding ‘recognition’ channel has a
state-dependent type c† : DX × Y →DX.
A pair of a forwards map with a ‘dependent’
backwards map is called a lens.
Lenses are a common patern in ‘bidirectional’
systems (economic games, databases, etc).
They encapsulate what David Jaz was calling
“hierarchical planners”.
The inverse of a composite channel
(hierarchical generative model) is the lens
composite of its components [4].
This explains formally the bidirectional
structure of predictive coding (cf. right).
A “right abstraction” !
Figure: Bastos et al. [5]
19 / 36

Compositional Models
Compositional Probability
Bayesian lenses
These Bayesian lenses form a category of morphisms between (pairs of) spaces.
These morphisms (the lenses) compose like this:
c
c†
X
A
B
Y
X
d
d†
C
Z
Y
∼=
c
c†
X
A
d
d†
C
Z
c
This captures the structure of inverting a hierarchical or causal model.
(Example: brain’s visual predictions at cinema...)
20 / 36

Compositional Models
Statistical Games
Statistical Games for Approximate Inference
Given a stochastic channel c : X →DY and a prior π : DX, we can form the inversion
c†
π : Y →DX. And we’ve seen that these pairs form lenses.
Why is this useful?
Typically, obtaining c†
π is computationally diﬀicult: we usually need to approximate it.
This gives us a lot of freedom. Ofen, one approximation scheme might be ‘beter’ than
another, and we should like to quantify this.
And, ofen, the fitness of our approximation depends on how it interacts with the world:
the prior we choose, and the dataset we have.
So the approximation is typically context-dependent and parameterized.
21 / 36

Compositional Models
Statistical Games
The Category of Statistical Games
The objects of SGame will be the objects (X, A) of BayesLens.
Then a statistical game is a morphism (X, A) →(Y, B) in SGame:
a lens (X, A) 7→(Y, B) paired with a contextual loss function ctx
 (X, A), (Y, B)

→R.
ctx
 (X, A), (Y, B)

is the set of contexts for lenses (X, A) 7→(Y, B):
Everything needed to “close oﬀ” the lens.
ctx
 (X, A), (Y, B)

= BayesLens
 (1, 1), (X, A)

× BayesLens
 (Y, B), (1, 1)

That is: a prior π : DX on X and a continuation channel Y →DB.
Composition of statistical games is lens composition paired with the sum of the ‘local’ fitnesses.
Identities are identity lenses (which just pass on information) with 0 fitness.
22 / 36

Compositional Models
Statistical Games
Example: Maximum Likelihood Estimation
A Bayesian lens of the form (1, 1) 7→(X, X) is fully specified by a state π : 1→
• X.
A context for such a lens is given by a trivial ‘prior’ on 1 and k : X→
• X is any endochannel on X.
(Idea: “given a prediction, obtain a random observation”.)
A maximum likelihood game is any game of type (1, 1) →(X, X) for any X : C, and whose
loss function is Ek•π [−log pπ], where pπ is a density function for π.
NB: Ex∼π [f ] = E(f • π) =
R
x:X f (x) π(dx) when f : X →R and π : C(1, X).
Thinking of density as a measure of likelihood, note that an optimal strategy for an ML game is
one that maximises the likelihood of the state obtained from the context.
23 / 36

Compositional Models
Statistical Games
Example: Bayesian Inference
By generalizing the forwards part of the lens from states to channels, we obtain the following.
A simple Bayesian inference game is any game of type (Z, Z) →(X, X) with loss function
E
x∼k•c•π
h
DKL(c′
π(x), c†
π(x))
i
where π : I→
• Z, k : X→
• X, DKL denotes the Kullback-Leibler divergence,
and c† is the exact inversion of c.
NB: since Bayesian updates compose optically, these games are closed under composition,
giving hierarchical Bayesian inference games.
24 / 36

Compositional Models
Statistical Games
Example: Free Energy Game
Typically, computing DKL(c′
π(x), c†
π(x)) is hard, so systems approximate.
A common approximation is the free energy, which bounds DKL(c′
π(x), c†
π(x)) from above.
So, a free energy game is any game of type (Z, Z) →(X, X) with loss function
E
x∼k•c•π

E
z∼c′π(x) [−log pc(x|z)] + DKL(c′
π(x), π)

=
E
z∼c′π•k•c•π

−
Z
X
log pc(dk • c • π|z)

+ DKL(c′
π • k • c • π, π)
where π : I→
• Z and k : X→
• X, and where the equality holds by linearity of expectation.
(It’s easy to check that this gives an upper bound on DKL(c′
π(x), c†
π(x)), so I won’t..)
... But how do we breathe life into these systems?
How do they find abstractions?
25 / 36

Compositional Life
Overview
1 Compositional Worlds
Compositional Spaces
Compositional Things
2 Compositional Models
Compositional Probability
Statistical Games
3 Compositional Life
Polynomial Morphology
Active Inference Doctrines
26 / 36

Compositional Life
Polynomial Morphology
Polynomial Morphology (1/2)
Polynomials represent interfaces;
their morphisms paterns of interconnection:
Z
C
C
Z
Y
B
Y
B
BZyYC ⊗CyZ →ByY
But that’s not all!
27 / 36

Compositional Life
Polynomial Morphology
Polynomial Morphology (1/2)
Polynomials represent interfaces;
their morphisms paterns of interconnection:
Z
C
C
Z
Y
B
Y
B
BZyYC ⊗CyZ →ByY
But that’s not all!
Polynomials can describe much richer
kinds of interconnection:
Even systems that change their shape!
How does this work?
27 / 36

Compositional Life
Polynomial Morphology
Polynomial Morphology (2/2)
A general polynomial p looks like: AyX + ByY + CyZ + · · ·
More generally, that is: P
i:p(1) p[i]
For example: p(1) = A + B + C; p[i]i∈A = X, etc
So p : P
i:p(1) p[i] →p(1) is a bundle over p(1)!
I like to think of each polynomial as a phenotype:
The base type p(1) is configurations or morphology;
Each p[i] is the type of immanent signals or ‘sensorium’.
Think hedgehog, or “Markov blanket”!
Then poly. morphisms model interactions just as on the lef!
We can also assign positions to configurations using
dependent polynomials.
The ‘base’ polynomial represents the shape of the ‘world’.
(But we’ll not get into these details now...)
28 / 36

Compositional Life
Active Inference Doctrines
Internal Models: Polynomially-Indexed Statistical Games
Now, we can start to animate these polynomial shapes.
We can ‘index’ the category of statistical games by
polynomials: like a slice category.
That is, for each polynomial p, we obtain a category
of “statistical games over p”.
This gives us a fibration: a categorified bundle (lef).
Each object over p is a “generative model of
p-sensations”.
By sampling from the predicted configurations, we
get something like ‘action’.
(What more is action than a change in
configuration?)
We also get a recipe for describing the generative
model of a composite (‘multi-agent’) system, in
terms of the component systems’ models.
29 / 36

Compositional Life
Active Inference Doctrines
Active Inference Doctrines
But statistical games themselves are not dynamical!
Hence: active inference doctrines.
In the literature, there are many recipes for turning statistical games into dynamical systems.
Formally, these organize themselves into indexed functors:
−→
The resulting dynamical systems perceive their sensoria, and change their configurations in
action, to bring their beliefs into alignment with ‘reality’, and improve their chances of survival,
or ability to find abstractions, or their fulfilment,
... or whatever.
30 / 36

Compositional Life
Active Inference Doctrines
Systems with Volition (1/2)
Recall that a system’s performance at a statistical game is contextual:
it depends on prior beliefs (inc. its model structure!), and on its environment.
To improve its performance a system can:
1 Update its beliefs (perception);
2 update its model structure (part of its beliefs);
(NB. the ‘latent’ domain X of the internal model is roughly the “space of external states”)
3 change its shape (a kind of action);
4 couple with part of the world, and change that shape (another kind of action).
By equipping it with ‘strong’ initial beliefs, we can give it preferences for particular world states.
And the system will atempt to achieve those states: i.e., display volition.
31 / 36

Compositional Life
Active Inference Doctrines
Systems with Volition (2/2)
We can sketch a couple of examples, of increasing complexity:
Homeostasis
Suppose: system’s sensorium includes
a key parameter, such as the ambient
temperature.
Suppose: by adjusting configuration,
the system can move around to sample
the parameter.
Suppose: the ‘prior’ encodes a
high-precision distribution centred on
the acceptable range of this parameter.
Then: by minimizing free energy, the
system will atempt to configure itself
to remain within the acceptable
parameter range.
32 / 36

Compositional Life
Active Inference Doctrines
Systems with Volition (2/2)
We can sketch a couple of examples, of increasing complexity:
Homeostasis
Suppose: system’s sensorium includes
a key parameter, such as the ambient
temperature.
Suppose: by adjusting configuration,
the system can move around to sample
the parameter.
Suppose: the ‘prior’ encodes a
high-precision distribution centred on
the acceptable range of this parameter.
Then: by minimizing free energy, the
system will atempt to configure itself
to remain within the acceptable
parameter range.
Morphogenesis
(very briefly...)
Suppose: multiple ‘homeostasis’
systems, each sensing some signalling
molecule.
Suppose: poly map encodes patern of
signal molecule concentrations.
Suppose: ‘priors’ encode “target local
configurations”.
Then: free-energy minimization
induces systems to arrange themselves
to obtain global target patern.
32 / 36

Compositional Life
Active Inference Doctrines
The “Internal Universe” of an Active System
Speculatively, we can also model navigation in arbitrary spaces:
we can use dependent polynomials to atach the system to positions in some world;
and this world may in turn be a topos.
Then: there need not be any restriction on the form of the internal model.
In particular, it too might be structured like a topos:
Model parameters encode topos structure (the context, or “world shape”).
So: updating model parameters means learning “what maps where”, given the context.
The system’s state representation gives location (topos object).
Action corresponds to following a morphism in the topos.
(Sense-data varies ‘fibrationally’.)
Puting a prior on a particular location induces system to reach that location,
learning the structure along the way.
So: navigating a space is just like finding a mathematical proof!
Points towards ‘well-typed’ theory of cognition ...
In principle: use internal language to put logical constraints on the target state?
Lots lef to figure out here ...
33 / 36

Compositional Life
Active Inference Doctrines
Concluding Remarks (1/2)
It seems increasingly natural to think of the “world inside my head” like a topos.
So: structure-learning becomes logical “context-learning”.
The brain assigns beliefs to propositions (a kind of classifying object).
Represent beliefs about processes using Cartesian closed structure (internal implication)?
Then can ask: how do diﬀerent systems’ internal universes compare?
Do we have consensus?
What are the obstructions?
“Finding right abstractions” seems to mean: finding good explanations.
In our world, more ofen than not, these should be compositional; hence, reusable.
More poetically: finding compelling stories.
(The good ones hang around!)
So: in a complex world, considering systems in isolation is unhelpful.
Always ask: “what’s the context?”
34 / 36

Compositional Life
Active Inference Doctrines
Concluding Remarks (2/2)
Of course, there’s lots of work still to do.
Amongst other questions:
To what extent does the process by which we (=our brains) find abstractions agree with
‘black-boxing’, and other accounts?
What’s missing?
How do these processes relate to causal / temporal inference?
Can we see explicit signatures of “well-typed cognition” in the brain?
Can we use the internal language of a topos to encode constraints on active systems’
behaviours?
How can we compare systems’ internal universes?
35 / 36

Compositional Life
Active Inference Doctrines
References
[1]
Allen Hatcher. Algebraic Topology. Cambridge University Press, 2002. 556 pp. isbn:
0521795400. url:
http://pi.math.cornell.edu/~hatcher/AT/ATpage.html.
[2]
Greg Friedman. “An elementary illustrated introduction to simplicial sets”. In: Rocky
Mountain J. Math. 42 (2012), no. 2, 353-423 (09/24/2008). arXiv: 0809.4221v5
[math.AT].
[3]
David I. Spivak. “Poly: An abundant categorical seting for mode-dependent dynamics”. In:
(05/05/2020). arXiv: 2005.01894 [math.CT].
[4]
Toby St. Clere Smithe. “Bayesian Updates Compose Optically”. In: (05/31/2020). arXiv:
2006.01631v1 [math.CT].
[5]
A. M. Bastos et al. “Canonical microcircuits for predictive coding”. In: Neuron 76.4
(11/2012), pp. 695–711. doi: 10.1016/j.neuron.2012.10.038.
36 / 36

