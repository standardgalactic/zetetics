Who’s Afraid of Thomas Bayes?
Erick Galinkin
erick_galinkin@rapid7.com
Drexel University
Philadelphia, Pennsylvania, USA
Rapid7
Boston, Massachusetts, USA
ABSTRACT
In many cases, neural networks perform well on test data, but tend
to overestimate their confidence on out-of-distribution data. This
has led to adoption of Bayesian neural networks, which better cap-
ture uncertainty and therefore more accurately reflect the model’s
confidence. For machine learning security researchers, this raises
the natural question of how making a model Bayesian affects the
security of the model. In this work, we explore the interplay be-
tween Bayesianism and two measures of security: model privacy
and adversarial robustness. We demonstrate that Bayesian neural
networks are more vulnerable to membership inference attacks in
general, but are at least as robust as their non-Bayesian counterparts
to adversarial examples.
CCS CONCEPTS
• Security and privacy →Domain-specific security and pri-
vacy architectures; • Computing methodologies →Neural
networks.
KEYWORDS
adversarial machine learning, neural networks, uncertainty, ma-
chine learning security, privacy, membership inference
ACM Reference Format:
Erick Galinkin. 2021. Who’s Afraid of Thomas Bayes?. In AISEC ’21: ACM
WORKSHOP ON ARTIFICIAL INTELLIGENCE AND SECURITY, November
15–19, 2021, Seoul, South Korea. ACM, New York, NY, USA, 5 pages. https:
//doi.org/10.1145/1122445.1122456
1
INTRODUCTION
Deep learning has revolutionized machine learning over the last
decade, providing tremendous advancements across disciplines and
exploding the public’s interest in artificial intelligence. One of the
safety challenges faced by deep neural networks [1] has been the
inability to represent uncertainty, especially on data which lies
outside of the model’s training distribution. Attempts to rectify this
challenge have fueled new research in the field of Bayesian neural
networks, since Bayesianism reduces overconfidence [12] in these
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
AISEC ’21, November 15–19, 2021, Seoul, South Korea
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00
https://doi.org/10.1145/1122445.1122456
models. The research of Yarin Gal [7]; Wilson and Ismailov [24];
and Kristiadi et al. has provided tools to make familiar model ar-
chitectures more Bayesian, which enhances the trustworthiness
and safety of neural networks. The ease with which familiar ar-
chitectures can now incorporate a Bayesian component lowers
the barriers to development and deployment of Bayesian models,
suggesting that we will see growing deployment of these models.
Safety in machine learning is, however, much more than simply
avoiding overconfident predictions. In order for us to ensure that
machine learning systems are safe, we must also work to keep them
secure. Security-conscious AI researchers may wonder if there is
some side effect to making models more Bayesian on the security
and privacy of our models, a question that this work seeks to address.
Although there are other possible ways to quantify uncertainty,
Bayesian neural networks have gained traction in industry as a way
to alleviate this overconfidence, and have a reputation for being
theoretically sound.
In this work, we wish to examine both how Bayesianism impacts
the confidentiality of the model’s training data and the integrity
of the model’s predictions. We accomplish this by first exploring
how additional information conferred by making the model more
Bayesian can enable privacy-damaging attacks like membership
inference [17] which seek to determine if a sample was part of
the target model’s training data. Then, we examine how specially
crafted samples that aim to cause misclassification [22] – so-called
adversarial examples – are affected by Bayesianism in neural net-
works.
2
BACKGROUND
This work builds on the body of literature surrounding Bayesian
neural networks, membership inference attacks, and adversarial
machine learning. Neural networks are machine learning models
which couple an affine transformation with a nonlinear activation
function and have been widely used in the fields of computer vision,
and natural language processing. These models seek to approximate
complex relationships between input data and a set of correspond-
ing labels. Bayesian neural networks differ from these conventional
neural networks by including some feature that captures both the
uncertainty about the data – aleatoric uncertainty – and uncertainty
about the parameters – epistemic uncertainty.
Work by Gal and Ghahramani [7] demonstrated that a certain use
of a common deep learning regularization technique, dropout, can
approximate a spike and slab Bayesian prior. Dropout is a technique
where by setting a hyperparameter known as the dropout rate, the
neuron will output zero with that probability [19]. The work of Gal
and Ghahramani applied a high probability dropout to each layer
of the network to approximate Bayesian inference, capturing the
arXiv:2107.14601v1  [cs.LG]  30 Jul 2021

AISEC ’21, November 15–19, 2021, Seoul, South Korea
Galinkin
aforementioned aleatoric and epistemic uncertainty. Prior work by
the author [9] demonstrates tension between this sort of Bayesian
dropout and differential privacy, suggesting that some amount of
information may inadvertently be “leaked” by the use of uncertainty
quantification techniques.
Expressing uncertainty provides useful information – in many
cases, a highly confident prediction suggests that the network has
seen similar samples in training. This benefit to AI safety, in mak-
ing predictions on out-of-distribution data less confident, can also
provide extremely valuable information when conducting mem-
bership inference attacks. Membership inference attacks against
machine learning models seek to determine whether or not a data
sample was in the training data for the target. Shokri et al. [17]
developed a method for conducting these attacks against machine
learning models, conducting closed-box queries that return only
the output probabilities for each class to predict whether or not
the sample input to the model was in the training data set. This
was done by first training 𝑘so-called “shadow models” that imitate
the behavior of the target, trained on a synthetic dataset that is
from approximately the same distribution as the target, where 𝑘
is the number of output classes. Then an attack model is trained
for each of the 𝑘models, taking the outputs of the 𝑘models and
making a prediction of whether or not the sample was in or out of
the training data.
Salem et al. [16] refined Shokri et al.’s procedure, training only a
single shadow model, and making a number of other improvements
that improve efficacy and reduce both training time and the amount
of synthetic training data. The methods used in our paper are de-
rived from these refinements. Interestingly, Salem et al. recommend
dropout as a potential defense against membership inference, albeit
not enough to provide the approximate Bayesianism of Gal and
Ghahramani’s methods.
Adversarial examples – specially crafted samples which seek to
perturb images in imperceptible ways to cause misclassification
– were introduced to the world by Szegedy et al. [22] as a pecu-
liar feature of differentiable models. Further work by Carlini and
Wagner [5] expanded the understanding and evaluation of these
adversarial examples, and considered the potential threat models
associated with adversarial examples. This same work by Carlini
and Wagner also introduced three new attack algorithms that each
minimally distort images under a given distance metric. Work by
Suciu et al. [21] considers the specific question of adversarial exam-
ples against deep learning-based malware detectors, highlighting
the potential for more sophisticated detection evasion techniques
using adversarial machine learning.
Our work is thus further motivated by the assertion of Smith
and Gal [8] that Bayesian neural networks cannot have adversarial
examples under some set of conditions. In this work, the authors
develop a strong theoretical justification that under idealized cir-
cumstances, Bayesian networks will capture perfect epistemic un-
certainty and thus cannot have adversarial examples. To exercise
this theory in an empirical way, they generate a synthetic dataset
with complete knowledge of the ground-truth image-space density
and find that for Hamiltonian Monte Carlo – the gold standard
for Bayesian Neural Network inference – adversarial crafting fails.
Our work instead considers the susceptibility of Bayesian models
to adversarial examples on familiar benchmark datasets compared
with a baseline model rather than synthetic datasets.
In general, adversarial attacks work via the same principle –
given a neural network with fixed parameters, 𝐹(𝑥) = 𝑦, we define
the classifier 𝐶(𝑥) = arg max𝑖𝐹(𝑥)𝑖to output a label for the input
𝑥. Let 𝐶∗(𝑥) be the correct label of 𝑥. Then, given a valid input 𝑥
and a target class 𝑡≠𝐶∗(𝑋), we search for a small perturbation 𝛿
such that for distance metric 𝐷,
minimize 𝐷(𝑥,𝑥+ 𝛿)
such that 𝐶(𝑥+ 𝛿) = 𝑡
The distance metric 𝐷is most often some ℓ𝑝norm, with 𝑝most
often set to 0, 2, or ∞. In untargeted cases, we search instead for
some valid input 𝑥′ which meets the same minimization criteria
and constraints as 𝑥+ 𝛿. This approach, however, is less common
than generating noise or a patch to be applied.
3
METHODS
To assess the effect of Bayesianism on the efficacy of membership
inference attacks and adversarial examples, we leverage a total of
four models. Each model architecture features a standard, dropout-
less control and a Bayesian experimental model. All of the models
are trained and tested on both the MNIST [14] and the CIFAR-10 [13]
datasets, two well-researched benchmarks. This provides a good
basis for comparison to other model architectures and allows us
to draw more specific conclusions about the effect of Bayesianism
absent variables around the data and architecture. All of the models
and experiments were conducted on a single server, with a single
RTX 2080 Ti, 128 GB of RAM, and an AMD Threadripper 1950x.
3.1
Models
Our first experimental architecture is based on the LeNet-5 [15]
architecture, which features 5 hidden layers. The second experi-
mental architecture is based on the ResNet-18 [10] architecture that
takes advantage of residual connections to avoid the vanishing gra-
dient problem – a problem which plagues very deep, non-residual
networks. Although our datasets are quite simple, these two archi-
tectures are widely studied and easy to train given computational
limitations. In order to make the aforementioned models Bayesian,
our work leverages the spike and slab prior of Gal and Ghahra-
mani [7] within the experimental models. This ensures the highest
degree of similarity between the Bayesian and non-Bayesian ar-
chitectures in order to isolate the effect of Bayesianism. All of the
models were trained with early stopping to prevent overfitting,
but no hyperparameters were tuned in training the architectures
to assure the highest degree of similarity between Bayesian and
Non-Bayesian models.
3.2
Attacks
Attacks on machine learning systems come in many forms, from
more conventional denial of service or code execution-style attacks
on the libraries [20] to the ML-specific denial of service via sponge
examples [18], there are many facets where machine learning sys-
tems can be exploited. We consider two classes of attacks from
this array: membership inference attacks and adversarial examples.
These two attack classes have both been well-studied, and target

Who’s Afraid of Thomas Bayes?
AISEC ’21, November 15–19, 2021, Seoul, South Korea
wildly different parts of the process – membership inference seek-
ing to attack the confidentiality of training data, and adversarial
examles seeking to attack the integrity of the predictions. Back-
ground on these is provided in Section 2 and we detail the methods
used in our experiments below.
3.2.1
Membership Inference. We leverage the prior work done by
Salem et al. [16] on closed-box membership inference attacks. This
involves the creation of a synthetic “shadow” dataset with a similar
distribution to the training distribution of the target model. The
shadow dataset is used to train a shadow model that approximates
predictions for the target classifier. Additional synthetic examples
are generated and added to the shadow dataset, to be used as a val-
idation dataset on which the classifier is not trained. The shadow
model and shadow dataset is then used to generate a set of output
vectors from the classifier and assigned an “in” or “out” label de-
pending on whether or not the model was trained on the sample.
This in-or-out dataset is used to train a binary classifier to attack
the model. This process is visualized in Figure 1. Further technical
details are available in the paper by Salem et al.
Figure 1: Diagram of Membership Inference Attack
3.2.2
Adversarial Examples. Using the Microsoft Counterfit frame-
work1, we evaluate our models using two different attacks: a bound-
ary attack [3] and the hop-skip-jump attack [6]. These attacks are
both closed-box attacks – attacks which do not require direct ac-
cess to the model and instead consider only inputs to and outputs
from the model – making the attack scenario more realistic than
open-box models where all details are available to attackers. Each
model was targeted using both attacks over 100 iterations per at-
tack, model, and dataset combination. We evaluate attack efficacy
as a percentage equal to the number of successful attack attempts.
The boundary attack is a decision-based attack which leverages
an adversarial patching technique intended to cause misclassifica-
tion in the real world. It starts with a perturbation of the original
image that is already adversarial – classified as part of the target
class – and iteratively reduces the ℓ2 distance between the original
image and the adversarial example by randomly walking along the
boundary between the adversarial and non-adversarial region of
the classifier. This is done by first ensuring the adversarial sample
𝑥′ is in the input domain, in this case, a valid image. Then, at step
1https://github.com/azure/Counterfit
𝑘, the perturbation 𝜂𝑘, should have a relative size of 𝛿:
∥𝜂𝑘∥2 = 𝛿· 𝐷(𝑥,𝑥′)
where 𝐷is the distance metric being used. In this case, we set
𝐷(𝑥,𝑥′) = ∥𝑥−𝑥′∥2
2. The value of 𝜂is drawn from a Gaussian
distribution, than rescaled and clipped to ensure the above criteria
hold. In any case where perturbing 𝑥′ by 𝜂moves the adversarial
sample out of the target class, the perturbation is discarded.
The hop-skip-jump attack is inspired by the boundary attack,
but rather than rejecting perturbations that move the adversarial
sample out of the target class, those perturbations are used to esti-
mate the gradient direction and extends the attack to the ℓ∞𝑛𝑜𝑟𝑚.
The attack is also both very query efficient, making it useful for
evaluating real-world security of models, and comparable in effi-
cacy to open-box attacks like Carlini-Wagner [5]. Warde-Farley and
Goodfellow [23] have put forth the argument that ℓ∞is the optimal
distance metric for adversarial examples, and so that is the distance
metric we use in our implementation.
4
RESULTS
The results in Table 1 show that in all cases where a model is
more Bayesian, it features a higher susceptibility to membership
inference attacks compared with its non-Bayesian counterpart. This
strongly implies that a more Bayesian model is inherently more
susceptible to membership inference, which is likely an artifact of
the additional uncertainty captured on out-of-distribution samples.
When a Bayesian model makes an inference on a sample it has been
trained on, is is substantially more confident than compared with
samples that it has not seen, which lends itself to the efficacy of the
attacks on these models.
Figure 2: Area under the curve plot for membership infer-
ence against MNIST trained model
As we can observe from Figure 2, at all points for models trained
on MNIST, the ResNet with dropout is far more vulnerable to the
membership inference attack than the other 3 models, which are
clustered more closely together. The curve is an artifact of the attack
model described in Section 3.2.1, comparing the true positive and
false positive rate. In this case, a true positive is a correct identifica-
tion that an observed sample is from the training dataset and a false
positive is declaring the sample as part of the training dataset when

AISEC ’21, November 15–19, 2021, Seoul, South Korea
Galinkin
Dataset and Model
Test Accuracy
Hop-Skip-Jump
Boundary
Membership Inference
MNIST LeNet-5
98.48%
58%
52%
63.91%
Bayesian MNIST LeNet-5
98.01%
52%
50%
64.72%
MNIST ResNet-18
98.66%
64%
70%
62.49%
Bayesian MNIST ResNet-18
98.02%
66%
62%
73.38%
CIFAR-10 LeNet-5
58.43%
94%
92%
58.86%
Bayesian CIFAR-10 LeNet-5
58.56%
85%
87%
66.85%
CIFAR-10 ResNet-18
75.40%
69%
71%
72.10%
Bayesian CIFAR-10 ResNet-18
73.20%
60%
57%
77.16%
Table 1: Summary of results for test set accuracy, efficacy of adversarial attacks, and accuracy of membership inference against
the model. Results for attacks with the highest efficacy on the dataset are in bold.
Figure 3: Area under the curve plot for membership infer-
ence against CIFAR-10 trained model
it is not. The closeness of the curves other than the Bayesian ResNet
suggests that there may be too much noise in the predictions. This
issue could potentially be resolved with hyperparameter tuning.
The models trained on CIFAR-10, shown in Figure 3 show a
greater variability in vulnerability to membership inference attacks
than those trained on MNIST, though the AUC for ResNet with
dropout still greatly exceeds the other three models. We note, how-
ever, that no model trained on any dataset is especially close to the
baseline AUC of 0.5, with the LeNet-5 trained on CIFAR-10 having
the smallest AUC, 0.64.
Dataset
Bayesian
Non-Bayesian
MNIST
69.05%
63.2%
CIFAR-10
72.01%
65.48%
Table 2: Membership inference efficacy between Bayesian
and Non-Bayesian models per dataset
Table 2 breaks down the results from Table 1 in detail, showing
that in general, Bayesian models are approximately 10% more vul-
nerable to membership inference attacks than their non-Bayesian
counterparts independent of dataset. This suggests that there is
an inherent trade-off being made between quantifying uncertainty
and protecting the confidentiality of a model’s training data. We
discuss potential mitigations that may alleviate this trade-off in
Section 5.
In contrast with the membership inference results, Bayesian
and non-Bayesian models proved approximately equally vulnera-
ble to adversarial examples. Efficacy of attacks demonstrated little
correlation between model and dataset combination, and no one
model type clearly more vulnerable the others across datasets. On
CIFAR-10, the non-Bayesian LeNet-5 proved more vulnerable to
adversarial examples for both attacks, but the same model architec-
ture was the second-most robust on average for the MNIST dataset.
The presence of Bayesianism seems to have no effect on the effi-
cacy of adversarial examples in general. This is reasonable given
the fact that the decision boundaries for the various models and
datasets should be quite similar and the attack methods used have
no knowledge of the models inner workings, and must rely entirely
on gradient estimation techniques.
Dataset
Hop-Skip-Jump
Boundary
MNIST
60%
58.5%
CIFAR-10
77%
76.75%
Table 3: Average efficacy of adversarial attacks across all
models per dataset
Interestingly, the training dataset seems to have a much stronger
relationship to the efficacy of adversarial samples than any model-
specific artifact, as we can observe from Table 3. This is likely
due to the additional complexity of the sample space for CIFAR-
10, as compared with MNIST. This follows from the sensitivity
to well-generalizing features in the data, as described by Ilyas et
al [11]. Since CIFAR-10 images are both larger and are color images,
as compared with MNIST’s black and white images, they have a
comparative wealth of features that may be highly predictive but
not robust.
5
CONCLUSION
The results of our experiment are very clear on the question of
membership inference attacks – Bayesian neural networks are sub-
stantially more vulnerable to these types of attacks. In sensitive use
cases of artificial intelligence, we will want to have both uncertainty
quantification and data privacy. Unfortunately, these experiments
suggest that we likely cannot have both. Returning only the top-1

Who’s Afraid of Thomas Bayes?
AISEC ’21, November 15–19, 2021, Seoul, South Korea
result with a probability may help alleviate this, as it is quite dif-
ficult to build a membership inference attack off of so little data.
However, this may impact the usefulness of the model in cases
where humans need to make decisions based upon its output.
Luckily, it seems that in cases where adversarial examples are
part of our threat model, Bayesianism does not increase nor re-
duce the opportunity for attacks to be successful. Across the two
data sets, the hop-skip-jump attack was most effective on alterna-
tively the largest Bayesian model and the smallest non-Bayesian
model while the boundary attack proved effective on two different
non-Bayesian models across the two data sets. The results of our
experiment suggest that the most significant factor in the success
of adversarial examples is the complexity of the dataset – models
trained on datasets with more space to perturb the image are far
more vulnerable. This suggests that Bayesianism should not affect
your risk calculus in cases where adversarial examples are a threat.
In many cases where Bayesianism is being implemented, confidence
on unexpected samples should be low, so there may even be other
advantages against adversarial examples.
In future work, we seek to see how these attacks work on non-
image data, such as text. Privacy of text is more acutely relevant
for most use cases, and prior work [4] suggests that stronger forms
of training data recovery are possible on these sorts of models.
Coupling that work with the increased complexity in the adver-
sarial example space and recent breakthroughs [2] in adversarial
examples on text classifiers make this realm a natural extension of
the current work. Additionally, future work might consider how
certain defenses against membership inference and adversarial ex-
amplesperform when paired with Bayesianism.
ACKNOWLEDGMENTS
Funded by the Auerbach Berger Chair in Cybersecurity held by
Spiros Mancoridis, at Drexel University
REFERENCES
[1] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman,
and Dan Mané. 2016. Concrete Problems in AI Safety. arXiv:1606.06565 [cs.AI]
[2] Nicholas Boucher, Ilia Shumailov, Ross Anderson, and Nicolas Papernot. 2021.
Bad Characters: Imperceptible NLP Attacks. arXiv preprint arXiv:2106.09898
(2021).
[3] Tom B. Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer.
2017. Adversarial Patch. CoRR abs/1712.09665 (2017). arXiv:1712.09665 http:
//arxiv.org/abs/1712.09665
[4] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-
Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,
et al. 2020. Extracting Training Data from Large Language Models. arXiv preprint
arXiv:2012.07805 (2020).
[5] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness
of neural networks. In 2017 ieee symposium on security and privacy (sp). IEEE,
39–57.
[6] Jianbo Chen, Michael I. Jordan, and Martin J. Wainwright. 2020. HopSkipJumpAt-
tack: A Query-Efficient Decision-Based Attack. arXiv:1904.02144 [cs.LG]
[7] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation:
Representing model uncertainty in deep learning. In international conference on
machine learning. 1050–1059.
[8] Yarin Gal and Lewis Smith. 2018. Sufficient conditions for idealised models to
have no adversarial examples: a theoretical and empirical study with Bayesian
neural networks. arXiv preprint arXiv:1806.00667 (2018).
[9] Erick Galinkin. 2021. The Influence of Dropout on Membership Inference in
Differentially Private Models. arXiv preprint arXiv:2103.09008 (2021).
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[11] Andrew Ilyas, Shibani Santurkar, Logan Engstrom, Brandon Tran, and Aleksander
Madry. 2019. Adversarial Examples Are Not Bugs, They Are Features. Advances
in neural information processing systems 32 (2019).
[12] Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. 2020. Being Bayesian,
Even Just a Bit, Fixes Overconfidence in ReLU Networks. In Proceedings of the 37th
International Conference on Machine Learning (Proceedings of Machine Learning
Research, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR, 5436–5446.
http://proceedings.mlr.press/v119/kristiadi20a.html
[13] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features
from tiny images. (2009).
[14] Yann LeCun. 1998. The MNIST database of handwritten digits. http://yann. lecun.
com/exdb/mnist/ (1998).
[15] Yann LeCun et al. 2015. LeNet-5, convolutional neural networks. URL: http://yann.
lecun. com/exdb/lenet 20, 5 (2015), 14.
[16] Ahmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, and Michael Backes.
2019. ML-Leaks: Model and Data Independent Membership Inference Attacks
and Defenses on Machine Learning Models. In Network and Distributed Systems
Security Symposium 2019. Internet Society.
[17] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In 2017 IEEE Sympo-
sium on Security and Privacy (SP). IEEE, 3–18.
[18] Ilia Shumailov, Yiren Zhao, Daniel Bates, Nicolas Papernot, Robert Mullins, and
Ross Anderson. 2020. Sponge examples: Energy-latency attacks on neural net-
works. arXiv preprint arXiv:2006.03463 (2020).
[19] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958.
[20] Rock Stevens, Octavian Suciu, Andrew Ruef, Sanghyun Hong, Michael Hicks,
and Tudor Dumitraş. 2017. Summoning demons: The pursuit of exploitable bugs
in machine learning. arXiv preprint arXiv:1701.04739 (2017).
[21] Octavian Suciu, Scott E Coull, and Jeffrey Johns. 2019. Exploring adversarial
examples in malware detection. In 2019 IEEE Security and Privacy Workshops
(SPW). IEEE, 8–14.
[22] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199 (2013).
[23] David Warde-Farley and Ian Goodfellow. 2016. adversarial perturbations of deep
neural networks. Perturbations, Optimization, and Statistics 311 (2016).
[24] Andrew Gordon Wilson and Pavel Izmailov. 2020. Bayesian Deep Learning and
a Probabilistic Perspective of Generalization. arXiv preprint arXiv:2002.08791
(2020).

