ORCa: Glossy Objects as Radiance-Field Cameras
Kushagra Tiwary1*, Akshat Dave2*, Nikhil Behari1, TzoﬁKlinghoffer1,
Ashok Veeraraghavan2, Ramesh Raskar1
1Massachusetts Institute of Technology, 2Rice University
{ktiwary,behari,tzofi,raskar}@mit.edu, {akshat,vashok}@rice.edu
Figure 1. Objects as radiance-ﬁeld cameras. We convert everyday objects with unknown geometry (a) into radiance-ﬁeld cameras by
modeling multi-view reﬂections (b) as projections of the 5D radiance ﬁeld of the environment. We convert the object surface into a virtual
sensor to capture this radiance ﬁeld (c), which enables depth and radiance estimation of the surrounding environment. We can then query
this radiance ﬁeld to perform beyond ﬁeld-of-view novel view synthesis of the environment (d).
Abstract
Reﬂections on glossy objects contain valuable and hidden
information about the surrounding environment. By con-
verting these objects into cameras, we can unlock excit-
ing applications, including imaging beyond the camera’s
ﬁeld-of-view and from seemingly impossible vantage points,
e.g.
from reﬂections on the human eye.
However, this
task is challenging because reﬂections depend jointly on
object geometry, material properties, the 3D environment,
and the observer viewing direction. Our approach converts
glossy objects with unknown geometry into radiance-ﬁeld
cameras to image the world from the object’s perspective.
Our key insight is to convert the object surface into a vir-
tual sensor that captures cast reﬂections as a 2D projection
of the 5D environment radiance ﬁeld visible to the object.
We show that recovering the environment radiance ﬁelds
enables depth and radiance estimation from the object to
its surroundings in addition to beyond ﬁeld-of-view novel-
view synthesis, i.e. rendering of novel views that are only
directly-visible to the glossy object present in the scene, but
not the observer. Moreover, using the radiance ﬁeld we can
image around occluders caused by close-by objects in the
scene. Our method is trained end-to-end on multi-view im-
ages of the object and jointly estimates object geometry, dif-
fuse radiance, and the 5D environment radiance ﬁeld. For
more information, visit our website.
1. Introduction
Imagine that you’re driving down a city street that is
packed with lines of parked cars on both sides. Inspection of
the cars’ glass windshields, glossy paint and plastic reveals
sharp, but faint and distorted views of the surroundings that
might be otherwise hidden from you. Humans can infer
depth and semantic cues about the occluded areas in the
environment by processing reﬂections visible on reﬂective
objects, internally decomposing the object geometry and ra-
diance from the specular radiance being reﬂected onto it.
Our aim is to decompose the object from its reﬂections to
”see” the world from the object’s perspective, effectively
turning the object into a camera which images its environ-
ment. However, reﬂections pose a long-standing challenge
1
arXiv:2212.04531v2  [cs.CV]  12 Dec 2022

Figure 2. ORCa Overview. We jointly estimate the glossy object’s geometry and diffuse along with the environment radiance ﬁeld
estimation through a three-step approach. First, we model the object as a neural implicit surface (a). We model the reﬂections as probing
the environment on virtual viewpoints (b) estimated analytically from surface properties. We model the environment as a radiance ﬁeld
queried on these viewpoints (c). Both neural implicit surface and environment radiance ﬁeld are trained jointly on multi-view images of
the object using a photometric loss.
in computer vision as the reﬂections are a 2D projection of
an unknown 3D environment that is distorted based on the
shape of the reﬂector.
To capture the 3D world from the object’s perspective,
we model the object’s surface as a virtual sensor that cap-
tures the 2D projection of a 5D environment radiance ﬁeld
surrounding the object.
This environment radiance ﬁeld
consists largely of areas only visible to the observer through
the object’s reﬂections. Our use of environment radiance
ﬁelds not only enables depth and radiance estimation from
the object to its surroundings, but also enables beyond ﬁeld-
of-view novel-view synthesis, i.e. rendering of novel views
that are only directly visible to the glossy object present in
the scene, but not the observer. Unlike conventional ap-
proaches that model the environment as a 2D map, our ap-
proach models it as a 5D ﬁeld without assuming the scene is
inﬁnitely far away. Moreover, by sampling the 5D radiance
ﬁeld, instead of a 2D map, we can capture depth and images
around occluders, such as close-by objects in the scene, as
shown in Fig. 3. These applications cannot be done from a
2D environment map.
We aim to decompose reﬂections on the object’s surface,
from its surface and exploit those reﬂections to construct
a radiance ﬁeld surrounding the object, therefore captur-
ing the 3D world in the process. This is a challenging task
because the reﬂections are extremely sensitive to local ob-
ject geometry, viewing direction and inter-reﬂections due
to the object’s surface. To capture this radiance ﬁeld, we
convert glossy objects with unknown geometry and texture
into radiance-ﬁeld cameras. Speciﬁcally, we exploit neural
rendering to estimate the local surface of the object viewed
from each pixel of the real-camera. We then convert this
local surface into a virtual pixel that captures radiance from
Figure 3. Advantages of 5D environment radiance ﬁeld. Mod-
eling reﬂections on object surfaces (a) as a 5D env. radiance ﬁeld
enables beyond ﬁeld-of-view novel-view synthesis, including ren-
dering of the environment from translated virtual camera views
(b). Depth (c) and environment radiance of translated and parallax
views can further enable imaging behind occluders, for example
revealing the tails behind the primary Pokemon occluders (d).
the environment. This virtual pixel captures the environ-
ment radiance as shown in Fig 5. We estimate the outgo-
ing frustum from the virtual pixel as a cone that samples
2

the scene. By sampling the scene from many virtual pix-
els on the object surface, we construct an environment ra-
diance ﬁeld that can be queried independently of the object
surface, enabling beyond ﬁeld-of-view novel-view synthesis
from previously unsampled viewpoints.
Our approach jointly estimates object geometry, diffuse
radiance, and the environment radiance ﬁeld from multi-
view images of glossy objects with unknown geometry and
diffuse texture in three steps. First, we use neural signed
distance functions (SDF) and an MLP to model the glossy
object’s geometry as a neural implicit surface and diffuse
radiance, respectively, similar to PANDORA [8]. Then, for
every pixel on the observer’s camera, we estimate the vir-
tual pixels on the object’s surface based on the estimated
local geometry from the neural SDF. We analytically com-
pute parameters of the virtual cone through the virtual pixel.
Lastly, we use the cone formulation in MipNeRF [4] to cast
virtual cones from the virtual camera to recover the envi-
ronment radiance .
To summarize, we make the following contributions:
• We present a method to convert implicit surfaces into
virtual sensors that can image their surroundings using
virtual cones. (Sec. 3.3)
• We jointly estimate object geometry, diffuse radiance,
and estimate the 5D environment radiance ﬁeld sur-
rounding the object. (Fig. 7 & 8)
• We show that the environment radiance ﬁeld can be
queried to perform beyond-ﬁeld-of-view novel view-
point synthesis, i.e render views only visible to the ob-
ject in the scene (Section 3.4)
Scope. We only model glossy objects with low rough-
ness as such specular reﬂections tend to have a low signal-
to-noise ratio, therefore are a blurrier estimate of environ-
ment radiance ﬁeld. However, we note that the virtual cone
computation can be extended to model the cone radius as a
function of surface roughness. Deblurring approaches can
further improve resolution of estimated environment. In ad-
dition, we approximate the local curvature using mean cur-
vature, which fails for objects with varying radius of cur-
vature along the tangent space. We explain how our virtual
cone curvature estimation can be extended to handle gen-
eral shape operators in the supplementary material. Lastly,
similar to other multi-view approaches, our approach relies
on a sufﬁcient virtual baseline between virtual viewpoints
to recover the environment radiance ﬁeld.
2. Related Work
2.1. Modeling reﬂections
Catadioptric imaging systems aim to expand the ﬁeld of
view of conventional cameras using reﬂective mirrors [1]
Approach
Input
Scene Geometry
Algorithm Type
Environment
Dimension
Neural Illum
Single RGB
Normals
Supervised
2D
Lighthouse
Stereo RGB pair
Multi-plane Image
Supervised
5D
NeRFFactor
Multi-view RGB
Volumetric Albedo
Semi-supervised
2D
RefNeRF
Multi-view RGB
Volumetric Albedo
Self-supervised
2D
PANDORA
Multi-view 
RGB+Pol. 
Neural SDF
Self-supervised
2D
ORCa (Ours)
Multi-view RGB
Neural SDF
Self-supervised
5D
Figure 4. Comparison of environment estimation approaches.
Approaches such Neural Illum [26], Lighthouse [27], NeRFFac-
tor [37] train on datasets of natural illumination maps to regular-
ize ill-posedness of environment estimation. PANDORA [8] and
RefNeRF [31] exploit multi-view reﬂections on object but approx-
imate surrounding environment is inﬁnitely far away and model it
with a ﬂat 2D map. From multi-view reﬂections, ORCa converts
the object surface into virtual sensor and extracts 5D radiance ﬁeld
of the environment.
[19]. Recent work in catadioptric imaging proposes using
ellipsoidal mirrors to increase the baseline of a camera, such
that more of the light is observed [9] and novel view syn-
thesis from a single capture [34]. These works assume the
geometry of the reﬂecting surface is known or calibrated. In
contrast to these methods, we create a catadioptric imaging
system from everyday glossy objects of unknown geometry.
Recent progress in neural radiance ﬁelds (NeRF) has en-
abled impressive novel view rendering and geometry recon-
struction from multi-view images [17]. NeRF does this by
sampling the 5D light ﬁeld of the scene and learning a repre-
sentation that is consistent with the training images. MipN-
eRF [4] demonstrates better novel view synthesis by model-
ing outgoing rays as cones to enable anti-aliasing. However
MipNeRF fails to model sharp view dependencies of reﬂec-
tions. RefNeRF [31] shows improved novel view synthesis
on reﬂections using Integrated-Directional Encoding to ex-
plicitly separate diffuse and specular radiance. Similarly,
NeRFRN [13] separates diffuse and specular radiance by
using separate neural networks. Neural Catacaustics [14]
propose a neural warping method to improve novel view
synthesis of reﬂections by learning the caustics of the sur-
face. Comparatively, while all such works improve the qual-
ity of novel-view synthesis from the scene to the primary
camera, we perform view synthesis that is beyond the line-
of-sight of the primary camera, i.e. rendering views only
visible to the objects present in the scene, while jointly esti-
mating object geometry and separating diffuse and specular
radiance. We perform beyond line-of-sight view synthesis
by extracting a 5D environment radiance ﬁeld from the tar-
get object.
3

2.2. Environment Estimation
Recovering underlying scene properties from multiple
images is inherently ill-posed [24], but can be regularized
using the natural statistics of scene properties as a prior
[25] [3]. Recent works exploit this prior through deep neu-
ral networks and demonstrate inverse rendering of indoor
scenes from a single image [10] [15] [33] [38]. However,
these techniques typically recover only coarse representa-
tions of lighting and cannot reconstruct ﬁne details of the
environment. Lombardi et al. [16] recover environment and
reﬂectance, assuming the scene is composed of known ge-
ometry and uniform material. Georgolis et al. [11] recover
the environment map behind the camera from a single im-
age of a glossy object, assuming the object is composed
of textureless materials and using ground truth segmenta-
tion masks. Song et al. [26] estimate plausible environ-
ment maps by mapping reﬂections in the image and in-
painting unmapped regions. Srinivasan et al. [27] capture
stereo image pairs and estimate plausible spatially-coherent
environment maps. NeRD [6], NeRFactor [37] and Neu-
ralPIL [7] employ data-driven priors for lighting and BRDF
in a NeRF-based approach for radiance decomposition from
multi-view images.
While the above approaches, which rely on scene priors,
can generate realistic environment maps suitable for vir-
tual object insertion and re-lighting, the actual environment
might consist of occlusions. Other imaging modalities and
properties of light can aid in extracting information about
the surrounding environment. Park et al. [23] use RGB-D
videos to estimate environment map. Swedish et al. [29] re-
cover high-frequency illumination map from the shadows of
an object with known geometry. PhySG [36] and Munkberg
et al. [18] perform inverse rendering from multi-view im-
ages by modeling the surface as signed distance functions.
PANDORA [8] performs radiance decomposition from po-
larized RGB images.
3. Learning environment radiance ﬁelds from
multi-view reﬂections
3.1. Overview
Reﬂections on glossy objects offer a glimpse into the sur-
rounding environment beyond the camera’s ﬁeld-of-view.
From multi-view images of a glossy object with unknown
geometry and albedo, we aim to recover the 5D radiance
ﬁeld of the surrounding environment. The mapping from
images captured by the observer to the surrounding envi-
ronment depends on the glossy object’s surface properties,
in particular, the surface normals and curvature. We ﬁrst
cast a cone from the observer camera’s center-of-projection
through each pixel viewing the scene. When the cone in-
tersects the object surface, it reﬂects, causing the cone to
be transformed (Fig. 5). The transformed cone, referred to
Figure 5. Virtual Sensor. We image the world through the object
by modeling each pixel’s specular radiance as a projection of the
5D radiance ﬁeld of the environment onto the object’s surface. We
capture the radiance ﬁeld by treating the surface area on the object
that the pixel views, dSt, as a single-pixel virtual camera with
its center-of-projection at vo. We cast virtual cones through the
virtual sensor to capture the 5D radiance ﬁeld of the environment.
as a virtual cone, samples the environment and is primar-
ily responsible for the specular radiance observed on the
glossy object. Our key insight is that the reﬂections cap-
tured by the observer’s camera can be modeled as a pro-
jection of the environment radiance ﬁeld on to the object
surface. By modeling the reﬂected rays as a cone and com-
puting the parameters of the cone, we can more accurately
estimate the projected environment radiance ﬁeld onto the
pbject surface, as shown in Fig. 9.
ORCa is composed of three steps: modeling the object’s
geometry as a neural implicit surface (Sec. 3.2), convert-
ing the object’s surface into a virtual sensor (Sec.
3.3),
and modeling the environment radiance ﬁeld as a projection
along these virtual cones (Sec. 3.4). The learned environ-
ment radiance ﬁeld can then be queried on novel viewpoints
to show occluded areas in the scene. Fig. 2 depicts our out-
put for each component on a scene rendered with a complex
glossy object and 3D environment. Fig. 6 shows our system
architecture. Next, we describe each step in detail.
3.2. Learning Neural implicit Surfaces
Neural Signed Distance Function We model the object
geometry as a neural signed distance function (SDF). f :
R3 →R. SDFs provide a helpful inductive bias for learning
smooth surface geometry [35] [32] [22] that assists down-
stream tasks in our pipeline. Moreover, the surface proper-
ties crucial for our framework, surface normals and curva-
ture, can be conveniently computed from SDFs in a differ-
entiable manner. Consider the 3D spatial coordinates, x, in
the scene. The glossy object surface, S is then represented
4

Figure 6. Overview of our proposed architecture.
by the zero-level set of the SDF
S = {fS(x) = 0|x ∈R3}
(1)
Similar to Yariv et al. [35], we model the SDF fS as a
coordinate-based MLP.
Surface Normals Gradients of the SDF at the zero level set
point S towards the surface normals S,
n(x) =
∇xfS(x)
∥∇xfS(x)∥
x ∈S
(2)
Surface Curvature We employ differential geometry tech-
niques developed by Novello et al. [21] to estimate curva-
ture for neural implicit surfaces. In particular, we estimate
the mean curvature K(x) for the implicit surface from the
divergence, ∇of the surface normals
K(x) = ∇· n(x)
2
(3)
Mean curvature approximates the surface with an osculat-
ing sphere. Our approach also works for more generalized
notions of curvature through the shape operator, at the cost
of higher computational complexity. We refer our readers
to the supplement for the general case.
Diffuse Radiance We sepreate the captured radiance at the
obsever camera with diffuse radiance, that depends on the
glossy object’s albedo, and specular radiance that depends
on the environment radiance.
The diffuse radiance does
not have any view dependance and only depends on sur-
face point x. We denote the diffuse radiance as fd model it
using a coordinate-based MLP (Fig. 6).
Volume Rendering As proposed in [35], we perform vol-
umetric rendering on the SDF. We deﬁne the volume den-
sity σ(x) as the cumulative distribution function (CDF), de-
noted as Ψ(s), applied to fS:
σ(x) = αΨβ(fS)
(4)
In contrast to [35], however, we only aim to recover the
diffuse radiance of the object along a particular ray. We
deﬁne a function fd that estimates the diffuse radiance at
each point, x, along the ray. To get the ﬁnal diffuse radiance
along a given primary ray, rp(t), we perform volumetric
rendering:
ˆcd(r) =
Z ∞
0
fd(r(t), f k
S(r(t))τ(t)dt
(5)
Figure 7. Qualitative comparisons of diffuse-specular separa-
tion and geometry estimation on rendered dataset. The envi-
ronment contains nearby objects with complex occlusions when
seen through reﬂections on the glossy object.
RefNeRF fails
to perform accurate diffuse-specular separation and PANDORA
blurs the nearby objects in the specular map. ORCA can model the
complex specular reﬂections through environment radiance ﬁeld.
Note that there is no view dependence in Eq. 5 and interme-
diate features, f k
S, are used as input. τ(t) is the accumulated
transmittance along the ray.
3.3. Objects Surface as Virtual Sensor
Each pixel, p, with a ﬁnite surface area, dAp, on the
real-camera sensor views the surface of the object through
a frustum originating at that pixel. The object then samples
the environment radiance ﬁeld through this ﬁnite surface
converting the ﬁnite surface into a virtual pixel with surface
area, dS. Through this model, we can interpret the object
surface as a virtual sensor consisting of many virtual pix-
els that sample radiance from the environment ﬁeld based
on geometry of the object and observer viewing direction.
We now formulate a virtual pixel based on real camera post
and implicit surface geometry. Please refer to Fig. 5 for a
visualzation of the virtual sensor.
Consider a real camera origin as o and a pixel on the
real sensor pi,j that corresponds to ray direction d. The
primary ray for pixel pi,j is parameterized with ray length
t as rp(t) = o + td
Casting Real Cones We can approximate the outgoing con-
ical frustum from pixel pi,j as a cone originating at o with
axis-of-direction d and radius ˙r, equivalent to half the dis-
tance of the pixel in the x and y directions. We represent the
5

real-cone as parametric volume,
rcone( ˙r, s, θ) = ˙rs cos(θ)ˆeu + ˙rs sin(θ)ˆev + ˙rsd,
(6)
where ˆeu and ˆev are basis vectors in the plane perpendicular
to d, θ ∈[0, π] and s ∈[0, tmax]
Virtual Pixel Virtual pixels are characterized by the inter-
section of the real cone with the object surface. In Sec.
3.2, we model local surface properties using mean curva-
ture which enable efﬁcient analytical computations for the
virtual pixel parameters even though our approach works
with general shape operators. For a sampled point ti along
the ray, we have the surface normals n(ti) from Eq. 2 and
estimated mean curvature K(ti) from Eq. 3. The local ob-
ject surface at ti, can be approximated with an osculating
sphere, O(ti), centered at ˆ
oS(ti) with radius, R(ti) as fol-
lows:
R(ti) =
2
Kti
ˆ
oS(ti) = rp(ti) + R(ti) · ˆn(ti)
Note that for concave surfaces Kti < 0, so ˆ
oS will lie out-
side the object and, for Kti > 0, ˆ
oS will lie inside the ob-
ject.
The edges of the virtual pixel for rp(ti) would lie at
the intersection of the osculating sphere O(ti) and the pri-
mary cone given by rcone. Computing exact cone-sphere
intersections are computationally expensive so we approx-
imate the cone-sphere intersection using rays bound cone-
sphere interectional surface dS. We consider four rays that
bound the cone and sample them at θj ∈{0, π/2, π, 3π/2}
with Eq. 6. We perform intersections of the corresponding
bounding rays with the osculating sphere O(ti) to get cor-
ners of the virtual pixel dsj. These ray sphere intersections
can be computed analytically in an efﬁcient manner.
Virtual Cone Origin With an estimate of the virtual pixel
surface area, we can now compute the virtual cone that
samples the environment. We ﬁrst compute normal vectors
at virtual pixel corners dsj from the center of osculating
sphere ˆ
oS
ˆnj =
dsj −ˆ
oS
||dsj −ˆ
oS||
(7)
At each virtual pixel corner, we compute the reﬂected ray
directions, ωr
j, by computing the dot product between the
incoming ray directions, ωi
k, and the normals, ˆnk, where ωr
0
is the primary ray’s reﬂected vector.
ωr
0 = d −
 d · ˆn(ti)
ˆn(ti)
(8)
ωr
j = dj −
 dj · ˆnj(k)
ˆnj(k)
(9)
dj are the incident directions to the virtual pixel corners
dsj. The virtual cone origin is the intersection of these re-
ﬂected rays at the pixel corners and pixel center. However,
these rays might not intersect at a single point so we ap-
proximate a virtual origin to be the point that minimizes the
sum of distances to the reﬂected rays ωj.
vo = argminv
X
j
|(v −dsj) × ωr
j|
(10)
We pose this as a linear least squares problem and compute
the psuedo-inverse to efﬁciently compute the virtual cone
origin.
Virtual Cones Direction. The reﬂected ray at the center of
the virtual pixel reﬂects the object surface along the direc-
tion ωr
0 from Eq. 8. We consider this as the direction-of-axis
of the virtual cone.
ˆvd = ωr
0
(11)
Virtual Cone Radius. We compute the radius of the cone
by treating the reﬂection vectors of the bounding rays as
the neighboring ”pixel” directions. Similar to [4], we can
compute the distance between {ωr
kθ}2π
θ=0 and the primary
reﬂected ray ωr
0 in the (x, y) components (omitted below
for clarity).
ˆv ˙r = ∥{ωr
kθ}2π
θ=0 −ωr
0∥
(12)
Finally, for each sampled point ti, we can character-
ize our single-pixel virtual sensor located at the object sur-
face dS as a virtual cone with ˆvo as its apex, ˆvd as axis-
direction, ˆv ˙r as the radius.
Connections to caustics. Our work takes inspiration from
Catadiopritc Imaging systems. To covert objects into cam-
eras, we esentially compute the surface and ﬁnd a cor-
responding center-of-projection for this surface-as-sensor.
However, unlike conventional perspective cameras, objects
don’t have a ﬁxed center-of-projection, other than in a few
special conﬁgurations [2], but a locus of viewpoints that
vary with object geometry and viewing direction. These
viewpoins lie on the ”caustic surface” of the object. While
typical works in catadioptric imaging use an analytical
equation for the caustic surface by assuming known geome-
try [12] [28], or making assumptions about placement of the
observer [30], our formulation approximates the caustic sur-
face of unknonw geometry through intersection of reﬂected
rays on virtual pixels. We emperically show in supplemen-
tary that as the surface area of the virtual pixel goes to 0,
dS →0, our method estimates the true caustic of object
without assuming geometry. Our method also has applica-
tions in estimating the caustic surface of the unknown ge-
ometry.
6

3.4. Environment Radiance Fields
Our goal is to capture a 5D environment radiance ﬁeld of
the scene by imaging the world through these single-pixel
virtual sensors located at the object’s surface. We use our
formulation of virtual cones to recover 5D environment ra-
diance ﬁelds. We deﬁne an environment radiance ﬁeld as
fE : (ˆvo, ˆvd) →(σEnv, cs),
where fE outputs opacity and radiance along sampled
virtual cones. We note that this view dependent radiance
is equivalent to the specular radiance at point ti sampled
along the primary-camera ray rp(t). We can render the ﬁnal
specular radiance at pixel pi,j as follows:
ˆcs(r) =
Z ∞
0
fE(ˆvo, ˆvd)τ(t)dt
ˆc = ˆcd + ˆcs
Intuitively, fE learns the 5D radiance ﬁeld by sampling
single-pixel virtual sensors from the object surface area,
and must learn geometry and environment radiance that is
consistent with multiple views from the object’s reﬂections.
Moreover, we can query fE to render novel viewpoints and
associated depths that are beyond ﬁeld-of-view of the real
camera. We volume render each virtual cones by divid-
ing them into conical frustums using Integrated-Positional
Encoding as proposed in MipNeRF [4]. Our formulation
of virtual cones works well with Mip-Nerf’s rays-as-cones
method.
4. Experiments
Our experiments study the ability of our method to re-
cover 5D environment radiance ﬁelds (assessed through
quality of predicted surface normals, diffuse radiance, spec-
ular radiance, and 3D environment maps) from objects of
varying complexity, both in simulation (Fig. 7) and the real-
world (Fig. 8). Quantative results are provided in Table [1].
As in prior works on novel view synthesis, we report PSNR
and SSIM to evaluate estimated diffuse, specular, and mixed
radiance, and report mean angular error (MAE) to evaluate
estimated surface normals.
4.1. Implementation Details
As in PANDORA, we parameterize fS with an 8-layer
MLP to estimate the surface, and, as in MipNeRF, fd with
4-layer MLP with input geometric features of size 512 from
fS. We follow the sdf-to-opacity conversion and the itera-
tive sampling of the ray proposed in [35]. To aid the net-
work to learn the geometry quickly, we also train fS with a
mask-net as proposed in [8]. We use ﬁve losses in our archi-
tecture: photometric loss, mask loss [8], normal loss [31],
eikonal loss [35], and distortion loss [5]. Additional training
details are discussed in the supplementary materials.
Diffuse Radiance
Specular Radiance
Mixed Radiance
Normals
Scene
Approach
PSNR
SSIM
PSNR
SSIM
PSNR
SSIM
MAE
↑(dB)
↑
↑(dB)
↑
↑(dB)
↑
↓(°)
Ref-NeRF
17.59
0.7217
14.88
0.4750
19.58
0.7956
62.45
D1
PANDORA
13.23
0.4759
15.12
0.5231
12.87
0.4607
2.387
ORCA
13.29
0.4683
16.64
0.5148
18.23
0.5745
1.873
Ref-NeRF
11.86
0.6090
15.28
0.7059
21.80
0.8643
33.92
D2
PANDORA
22.53
0.8689
17.76
0.6326
22.73
0.7787
3.693
ORCA
23.47
0.8954
18.98
0.6954
22.31
0.8107
3.568
Table 1. Quantitative evaluation of rendered scenes. We com-
pare ORCa to other neural rendering techniques that model reﬂec-
tions, including Ref-NeRF and PANDORA, on the globe (D1) and
Pokemon (D2) datasets. ORCa is competitive with the comparison
methods in accurate diffuse and specular separation, and provides
consistent improvement in geometry and specular radiance esti-
mation.
4.2. Datasets
We conduct experiments on both simulated and real-
world datasets.
Simulated datasets are rendered in Mit-
suba2 [20]. Simulated datasets contain a range of increas-
ingly complex object geometries (elephant, Pokeball, and
orca) and scenes (living room and Pokemon). We train with
200 views for simulated datasets. We also show results for
a real-world dataset [8] capturing a glossy cup with a black
vase sitting atop it using 35 views. All datasets will be pub-
lically released upon publication.
4.3. Comparisons with Baselines
We compare our method to other neural rendering tech-
niques that model reﬂections, Ref-NeRF and PANDORA.
We ﬁrst discuss results in Fig. 3 which show the advan-
tages of recovering a 5D radiance ﬁeld with close-by objects
as they often cause occlusions which cannot be modeled by
2D environment maps. By estimating the radiance ﬁeld, we
can image behind occluders through sampling novel view-
points such as the translated viewpoints shown in Fig. 3.
Moreover, we can also show depth to surroundings from
these vritual viewpoints. We provide additional examples
of depth and beyond ﬁeld-of-view novel-view synthsis in the
supplementary materials.
While Ref-NeRF and PANDORA learn 2D environment
radiance ﬁelds, ORCa recovers a 5D environment radiance
ﬁeld. As shown in Fig. 7 and 8, ORCa estimates more ac-
curate surface normals than other methods. While the total
radiance predicted by Ref-NeRF and PANDORA are visu-
ally similar, the surface normals are less smooth than ORCa.
We also observe that ORCa is able to achieve better diffuse
and specular radiance separation than PANDORA, which is
evident in the Pokeball surface normals Fig. 7. In these ex-
amples, PANDORA recovers blurry specular radiance. We
see that ORCa’s predicted depth is highly interpretable and
matches the underlying geometry of the environment, as
shown in Fig. 3. Even on cylinderical real-world datasets,
such as the black vase in Fig. 8, the nearby hallway is vis-
7

Figure 8.
Comparisons on real dataset.
Using a real-world
dataset with only 35 views, ORCa can model the sharp specular-
ities on the ball arriving from regions of the nearby scene, such
as the table, and far away scene regions, such as the hallway, to
learn an environment radiance ﬁeld. We query the radiance ﬁeld
for depth of the far hallway (blue) and the nearby objects, such as
the table (red). We also render novel viewpoints that are beyond
the ﬁeld-of-view of the observer camera and show that ORCa in-
terpolates well between those views.
ible in both the virtual view and depth, despite never being
in the ﬁeld of view of the primary camera. Unlike Ref-
NeRF, our primary objective is not to perform novel-view
synthesis, but instead to capture the environment radiance
ﬁeld from the object surface.
As shown in Table 1, ORCa is competitive with both
Ref-NeRF and PANDORA in estimating diffuse radiance,
specular radiance, mixed radiance, and normals. Although
the comparison methods slightly outperform ORCa in full,
mixed-radiance scene rendering, ORCa consistently pro-
vides better specular radiance and object geometry esti-
mation across scenes and viewpoints.
This is again in-
dicative of a key strength of ORCa; whereas existing ap-
proaches aim to perform novel-view synthesis on reﬂective
objects, ORCa speciﬁcally focuses on accurate specular re-
ﬂection retrieval for environment radiance ﬁeld modeling.
This is achieved through accurate object geometry model-
ing, which enables high-accuracy specular radiance estima-
tion, thereby aiding in beyond ﬁeld-of-view novel-view syn-
thesis.
4.4. Impact of Correct Virtual Cones
We base our method on a physically accurate formula-
tion by modeling ray-cone intersections and using the sur-
face as a virtual sensor, as described in Sec 3.3. Naively, the
Figure 9. Ablation on virtual cone formulation. We study the
impact of our proposed virtual cone formulation for recovering
the 5D radiance ﬁelds compared to a naive formulation where the
emitted cones’ origins are placed on the object surface. In con-
trast, we place the emitted cones’ origins within the object based
on caustics and compute the cone radius. We see that this leads to
smoother estimated surface normals (left) and more accurate esti-
mated specular radiance (right).
origins of the virtual cones could instead be placed at the in-
tersection of the primary camera ray and surface, in essence
placing a Mip-NeRF at each intersection point. This al-
ternative formulation would not be physically accurate and
Fig. 9 shows the improvement that we achieve, underscor-
ing the importance of correctly modeling virtual cones.
5. Conclusion
In conclusion, we present a method to convert glossy ob-
jects with unknown geometry and texture into radiance-ﬁeld
cameras that capture the environment radiance ﬁeld around
them. Our method recovers object geometry and diffuse ra-
diance, in addition to capturing the depth and radiance of
the object’s surroundings from its perspective. Our model-
ing of environment as a radiance ﬁeld is effective in recov-
ering close-by objects (Fig 7), in addition to being occlu-
sion aware (Fig 3). Moreover, by recovering the environ-
ment radiance ﬁeld we can perform beyond ﬁeld-of-view
novel-view synthesis. Our work can unleash applications
in virtual object insertion and 3D perception, e.g. inferring
information beyond the line-of-sight of the camera using
predicted virtual views and depth.
Our formulation of the radiance ﬁeld beyond the conven-
tional direct-line-of-sight radiance ﬁeld can enable further
areas of research that aim to to extract more information
from the environment and the objects present in it.
8

References
[1] S. Baker and S.K. Nayar. A theory of catadioptric image
formation. In Sixth International Conference on Computer
Vision (IEEE Cat. No.98CH36271), pages 35–42, 1998. 3
[2] Simon Baker and Shree K. Nayar.
A theory of single-
viewpoint catadioptric image formation. International Jour-
nal of Computer Vision, 35:175–196, 2004. 6
[3] Jonathan T Barron and Jitendra Malik. Shape, illumination,
and reﬂectance from shading. IEEE transactions on pattern
analysis and machine intelligence, 37(8):1670–1687, 2014.
4
[4] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neu-
ral radiance ﬁelds. ICCV, 2021. 3, 6, 7
[5] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance ﬁelds.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 5470–5479, 2022. 7
[6] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Bar-
ron, Ce Liu, and Hendrik Lensch. Nerd: Neural reﬂectance
decomposition from image collections. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 12684–12694, 2021. 4
[7] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu,
Jonathan Barron,
and Hendrik Lensch.
Neural-pil:
Neural pre-integrated lighting for reﬂectance decomposi-
tion. Advances in Neural Information Processing Systems,
34:10691–10704, 2021. 4
[8] Akshat Dave, Yongyi Zhao, and Ashok Veeraraghavan. Pan-
dora: Polarization-aided neural decomposition of radiance.
arXiv preprint arXiv:2203.13458, 2022. 3, 4, 7
[9] Michael De Zeeuw and Aswin C Sankaranarayanan. Wide-
baseline light ﬁelds using ellipsoidal mirrors. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 2022.
3
[10] Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan
Carr, and Jean-Franc¸ois Lalonde. Fast spatially-varying in-
door lighting estimation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 6908–6917, 2019. 4
[11] Stamatios
Georgoulis,
Konstantinos
Rematas,
Tobias
Ritschel, Mario Fritz, Tinne Tuytelaars, and Luc Van Gool.
What is around the camera?
In Proceedings of the IEEE
International Conference on Computer Vision, pages 5170–
5178, 2017. 4
[12] J. Gluckman and S.K. Nayar. Planar catadioptric stereo: ge-
ometry and calibration. In Proceedings. 1999 IEEE Com-
puter Society Conference on Computer Vision and Pattern
Recognition (Cat. No PR00149), volume 1, pages 22–28 Vol.
1, 1999. 6
[13] Yuan-Chen Guo, Di Kang, Linchao Bao, Yu He, and Song-
Hai Zhang.
Nerfren: Neural radiance ﬁelds with reﬂec-
tions. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 18409–
18418, June 2022. 3
[14] Georgios Kopanas, Thomas Leimk¨uhler, Gilles Rainer,
Cl´ement Jambon, and George Drettakis. Neural point cata-
caustics for novel-view synthesis of reﬂections. ACM Trans-
actions on Graphics, 41(6):Article–201, 2022. 3
[15] Zhengqin Li,
Mohammad Shaﬁei,
Ravi Ramamoorthi,
Kalyan Sunkavalli, and Manmohan Chandraker. Inverse ren-
dering for complex indoor scenes: Shape, spatially-varying
lighting and svbrdf from a single image. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 2475–2484, 2020. 4
[16] Stephen Lombardi and Ko Nishino. Reﬂectance and natural
illumination from a single image. In European Conference
on Computer Vision, pages 582–595. Springer, 2012. 4
[17] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. Communications of the ACM, 65(1):99–106, 2021.
3
[18] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,
Wenzheng Chen, Alex Evans, Thomas M¨uller, and Sanja Fi-
dler. Extracting triangular 3d models, materials, and lighting
from images. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 8280–
8290, 2022. 4
[19] Shree K Nayar and Simon Baker. Catadioptric image forma-
tion. In Proceedings of the 1997 DARPA Image Understand-
ing Workshop, pages 1431–1437, 1997. 3
[20] Merlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wen-
zel Jakob. Mitsuba 2: A retargetable forward and inverse
renderer. ACM Transactions on Graphics (TOG), 38(6):1–
17, 2019. 7
[21] Tiago Novello, Guilherme Schardong, Luiz Schirmer, Vini-
cius da Silva, Helio Lopes, and Luiz Velho. Exploring dif-
ferential geometry in neural implicits, 2022. 5
[22] Michael Oechsle, Songyou Peng, and Andreas Geiger.
Unisurf:
Unifying neural implicit surfaces and radiance
ﬁelds for multi-view reconstruction. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 5589–5599, 2021. 4
[23] Jeong Joon Park, Aleksander Holynski, and Steven M Seitz.
Seeing the world in a bag of chips.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 1417–1427, 2020. 4
[24] Ravi Ramamoorthi and Pat Hanrahan. A signal-processing
framework for inverse rendering. In Proceedings of the 28th
annual conference on Computer graphics and interactive
techniques, pages 117–128, 2001. 4
[25] Fabiano Romeiro and Todd Zickler.
Blind reﬂectometry.
In European conference on computer vision, pages 45–58.
Springer, 2010. 4
[26] Shuran Song and Thomas Funkhouser. Neural illumination:
Lighting prediction for indoor environments. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, pages 6918–6926, 2019. 3, 4
[27] Pratul P Srinivasan, Ben Mildenhall, Matthew Tancik,
Jonathan T Barron, Richard Tucker, and Noah Snavely.
Lighthouse:
Predicting lighting volumes for spatially-
9

coherent illumination. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
8080–8089, 2020. 3, 4
[28] R. Swaminathan, M.D. Grossberg, and S.K. Nayar. Caus-
tics of catadioptric cameras. In Proceedings Eighth IEEE
International Conference on Computer Vision. ICCV 2001,
volume 2, pages 2–9 vol.2, 2001. 6
[29] Tristan Swedish, Connor Henley, and Ramesh Raskar. Ob-
jects as cameras: Estimating high-frequency illumination
from shadows. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pages 2593–2602,
2021. 4
[30] Yuichi Taguchi, Amit Agrawal, Ashok Veeraraghavan,
Srikumar Ramalingam, and Ramesh Raskar. Axial-cones:
Modeling spherical catadioptric cameras for wide-angle light
ﬁeld rendering. ACM Trans. Graph., 29(6), dec 2010. 6
[31] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,
Jonathan T. Barron, and Pratul P. Srinivasan.
Ref-NeRF:
Structured view-dependent appearance for neural radiance
ﬁelds. CVPR, 2022. 3, 7
[32] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
arXiv preprint arXiv:2106.10689, 2021. 4
[33] Zian Wang, Jonah Philion, Sanja Fidler, and Jan Kautz.
Learning indoor inverse rendering with 3d spatially-varying
lighting.
In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 12538–12547, 2021.
4
[34] Ziyu Wang, Liao Wang, Fuqiang Zhao, Minye Wu, Lan Xu,
and Jingyi Yu.
Mirrornerf: One-shot neural portrait radi-
ance ﬁeld from multi-mirror catadioptric imaging. In 2021
IEEE International Conference on Computational Photogra-
phy (ICCP), pages 1–12. IEEE, 2021. 3
[35] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman.
Volume rendering of neural implicit surfaces.
In Thirty-
Fifth Conference on Neural Information Processing Systems,
2021. 4, 5, 7
[36] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and
Noah Snavely. Physg: Inverse rendering with spherical gaus-
sians for physics-based material editing and relighting. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 5453–5462, 2021. 4
[37] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul De-
bevec, William T Freeman, and Jonathan T Barron. Ner-
factor: Neural factorization of shape and reﬂectance under
an unknown illumination. ACM Transactions on Graphics
(TOG), 40(6):1–18, 2021. 3, 4
[38] Rui Zhu, Zhengqin Li, Janarbek Matai, Fatih Porikli, and
Manmohan Chandraker. Irisformer: Dense vision transform-
ers for single-image inverse rendering in indoor scenes. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 2822–2831, 2022. 4
10

