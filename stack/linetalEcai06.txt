An Automated Agent for Bilateral Negotiation with
Bounded Rational Agents with Incomplete Information1
Raz Lin2 and Sarit Kraus2,3 and Jonathan Wilkenfeld 3,4 and James Barry5
Abstract. Many day-to-day tasks require negotiation, mostly under
conditions of incomplete information. In particular, the opponent’s
exact tradeoff between different offers is usually unknown. We pro-
pose a model of an automated negotiation agent capable of negotiat-
ing with a bounded rational agent (and in particular, against humans)
under conditions of incomplete information. Although we test our
agent in one speciﬁc domain, the agent’s architecture is generic; thus
it can be adapted to any domain as long as the negotiators’ prefer-
ences can be expressed in additive utilities. Our results indicate that
the agent played signiﬁcantly better, including reaching a higher pro-
portion of agreements, than human counterparts when playing one of
the sides, while when playing the other side there was no signiﬁcant
difference between the results of the agent and the human players.
1
Introduction
Many day-to-day tasks require negotiation, often in a bilateral con-
text. In this setting two agents are negotiating over one or several
issues in order to reach consensus [4, 6, 7]. The sides might have con-
ﬂicting interests, expressed in their utility functions, and they might
also cooperate in order to reach beneﬁcial agreements for both sides
[1, 12]. While models for bilateral negotiations have been extensively
explored, the setting of bilateral negotiation between an automated
agent and a bounded rational agent is an open problem for which
an adequate solution has not yet been found. Despite this, the ad-
vantages of succeeding in presenting an automated agent with such
capabilities cannot be understated. Using such an agent could help
in training people in negotiations and assist in e-commerce environ-
ments by representing humans and other activities in daily life. Our
proposed automated agent makes a signiﬁcant contribution in this
respect. An even more difﬁcult problem occurs when incomplete in-
formation is added into this environment. That is, there is also lack
of information regarding some parameters of the negotiation.
We consider a setting of a ﬁnite horizon bilateral negotiation with
incomplete information between an automated agent and a bounded
rational agent. The incomplete information is expressed as uncer-
tainty regarding the utility preferences of the opponent, and we as-
sume that there is a ﬁnite set of different agent types. The negoti-
ation itself consists of a ﬁnite set of multi-attribute issues and time-
constraints. If no agreement is reached by the given deadline a status-
1 This research was supported in part by NSF under grant #IIS-0208608.
2 Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel
52900. email: {linraz,sarit}@cs.biu.ac.il
3 Institute of Advanced Computer Studies, University of Maryland.
4 Department of Government and Politics, University of Maryland, College
Park, Maryland. email: jwilkenf@gvpt.umd.edu
5 Center for International Development and Conﬂict Management, University
of Maryland, College Park, Maryland. email: jbarry6899@aol.com
quo outcome is enforced. We propose a model of an automated agent
for this type of negotiation by using a qualitative decision making
approach [2, 13]. In our experiments we matched our automated
agent against human negotiators. By analyzing the results of the
experiments, we show that our agent is capable of negotiating efﬁ-
ciently and reaching multi-attribute agreements in such an environ-
ment. When playing one of the sides in the negotiation our agent
reached signiﬁcantly better results than the human players, and also
allowed both sides to reach an agreement signiﬁcantly faster. On the
other hand, while our agent was playing the other side, though it did
not reach signiﬁcantly better results than the human player, it did
not reach worse results. Also, there are no signiﬁcant differences be-
tween the results reached by the human players and the automated
agent playing that role.
The rest of the paper is organized as follows. Section 2 pro-
vides an overview of bilateral negotiation with incomplete informa-
tion. Section 3 surveys related work done in the ﬁeld of negotiation
with incomplete information and bounded rational agents. Section 4
presents the design of our automated agent, including its beliefs up-
dating and decision making mechanisms. Section 5 describes the ex-
perimental setting and methodology and reviews the results. Finally,
Section 6 provides a summary and discusses future work.
2
Problem Description
We consider a bilateral negotiation in which two agents negotiate to
reach an agreement on conﬂicting issues. The negotiation can end
either when (a) the negotiators reach a full agreement, (b) one of
the agents opts out, thus forcing the termination of the negotiation
with an opt-out outcome denoted OPT, or (c) a predeﬁned dead-
line is reached, denoted dl, in which, if a partial agreement was
reached it is implemented or, if no agreement was reached, a sta-
tus quo outcome, denoted SQ, is implemented. Let I denote the set
of issues in the negotiation, oi the ﬁnite set of values for each i ∈I
and O = o1 × o2 × . . . × o|I| a ﬁnite set of values for all issues.
Since we allow partial agreements ∅∈oi for each i ∈I. An of-
fer is then denoted as a vector ⃗o ∈O. It is assumed that the agents
can take actions during the negotiation process until it terminates.
Let Time denote the set of time periods in the negotiation, that is
Time = {0, 1, ..., dl}. Time also plays a factor and has an inﬂuence
on the agents’ utilities. Each agent is assigned with a time cost which
inﬂuences his utility as time passes.
In each period t ∈Time of the negotiation, if the negotiation has
not terminated earlier, each agent can propose a possible agreement,
and the opponent can either accept or reject the offer. Each agent
can either propose an agreement which consists of all the issues in
the negotiation, or a partial agreement. In contrast to the model of

alternating offers [12], each agent can perform up to M interactions
with the opponent agent in a given time period. Thus, an agent must
worry that its opponent may opt out in any time period.
Since we deal with incomplete information, we assume that there
is a ﬁnite set of agent types. These types are associated with different
additive utility functions. Formally, we denote the possible types of
the agents Types = {1, . . . , k}. We refer to an agent whose utility
function is ul as an agent of type l, and ul : {(O∪{SQ}∪{OPT})×
Time} →R. Each agent is given its exact utility function. The agent,
and the subjects in the experiments described later in the paper, are
also aware of the set of possible utility functions of the opponent.
However, the exact utility function of the rival is private information.
Our agent has some probabilistic belief about the type of the other
agent. This belief may be updated over time during the negotiation
process (for example, using Bayes formula).
3
Related Work
The problem of modeling an automated agent for bilateral negotia-
tion is not new for researchers in the ﬁelds of Multi-Agent Systems
and Game Theory. However, most research makes simplifying as-
sumptions that do not necessarily apply in genuine negotiations, such
as assuming complete information [3, 11] or the rationality of the op-
ponent [3, 4, 5, 7]. None of the above researchers has looked into the
negotiation process in which there is both incomplete information
and the opponent is bounded rational. While their approaches might
be appropriate in their context, they cannot be applied to our settings.
To deal with the opponent’s bounded rationality, researchers sug-
gested shifting from quantitative decision theory to qualitative deci-
sion theory [13]. In using such a model we do not necessarily assume
that the opponent will follow the equilibrium strategy or try to be a
utility maximizer. Also, this model is better suited for cases in which
the utility or preferences are unknown but can be expressed in ordinal
scales or as preference relations [2]. This approach seems appropri-
ate in our settings, and using the maximin criteria, which is generally
used in this context, enables our agent to follow a pessimistic ap-
proach regarding the probability that an offer will be accepted.
Another way to deal with bounded rationality was suggested by
Luce [8], who introduced the notion of Luce numbers. A Luce num-
ber is a non-negative number that is attached to each offer. The Luce
number of an offer ⃗o∗∈O is calculated using the following formula:
lu(o∗) =
u( ⃗o∗)
P
⃗o∈O u(⃗o)
(1)
That is, the Luce numbers assign probabilistic beliefs regarding the
acceptance of the offer by the opponent.
Several methods are proposed when dealing with the incomplete
information regarding the preferences of an opponent. For example,
Bayes theorem is the core component of the Bayesian Nash equilib-
rium ([12], p. 24-29), and it is used to deduce the current state given a
certain signal. It allows us to compensate for incomplete information
and enables good adaptation in a negotiation with time-constraints.
In ﬁnite horizon negotiations there are no past interactions to learn
from and not enough time periods to build a complete model. Thus
this model supplies a good probabilistic tool to model the opponent,
as opposed to neural networks [11] or genetic algorithms [7]. In our
settings, Bayes theorem can be used to update the believed type of
the opponent and at each time period act as if the opponent is of a
certain type.
4
Agent Design
Our agent is built with two mechanisms: (a) a decision making mech-
anism, and (b) a mechanism for updating beliefs. We describe both
mechanisms in the following subsections.
4.1
The Qualitative Decision Making Component
(QDM)
The qualitative decision making component takes into account the
agent’s utility function, as well as the believed type of the opponent.
This data is used both for deciding whether to accept or reject an
offer and for generating an offer. In our settings, although several
offers can be proposed in each time period, we restrict our agent to
making a single offer in each period. This is done due to the fact that
our mechanism for generating offers only produces one distinct of-
fer at a given time period. The bounded rational agent, on the other
hand, is free to propose several offers, and our agent can respond to
all the offers, which indeed happened in the simulations. While we
provide some theoretical foundations for our approach, we demon-
strate its effectiveness using simulations with human subjects in an
environment of incomplete information, as described in Section 5.
4.1.1
Generating Offers
The motivation behind the mechanism for generating offers is that
the automated agent would like to propose an offer, which yields
him the highest utility value. However, due to conﬂict of interests,
there is a high probability that this agreement will be rejected by
the opponent. To overcome this, our agent uses a qualitative decision
strategy. Basically, the agent evaluates all possible offers based on
its utility and the probability that the rival will accept them. Luce
numbers are used to estimate this probability. We note that for every
two offers x and y and agent j, where luj(x) and luj(y) denote the
Luce numbers associated with offers x and y respectively for agent j,
if uj(x) ≥uj(y) then luj(x) ≥luj(y). That is, the higher the Luce
number of an offer, the greater the probability of it being accepted.
Since the opponent himself also tries to reason whether an offer
will be accepted by our agent, we take the Luce numbers of both
our agent and the opponent into account. That is, our agent tries to
estimate, from the opponent’s point of view, whether the opponent
will accept the offer. This is done by calculating the sum of the the
Luce numbers of the agent and the opponent. This sum is used as
an estimation for the acceptance of the offer, and is multiplied by
the utility value of the opponent from that offer. Finally, our agent
compares those values with its own utility values. Similarly to the
qualitative decision theory, which uses the maximin value [2, 13], our
agent selects the minimum value between those two values, under the
pessimistic assumption that the probability that an offer is accepted
is based on the agent that favors the offer the least. After calculating
the minimum value between all the offers, our agent selects the offer
with the maximum value among all the minima, in order to also try
and maximize its own utility. Thus, our qualitative offer generation
mechanism selects, intuitively, the best offer among the offers that
the agent believes that might be accepted. In the rest of this section
we describe this process formally.
We assume that, at each time period t, the automated agent has a
belief about the type of its opponent. This believed type, denoted by
BT(t), is the one that the agent presumes to be the most probable for
the opponent. The agent uses the utility function associated with that
type in all of the calculations in that time period. In Section 4.2 we

describe in detail the elicitation of this belief. We denote by uBT (t)
opp
the utility function associated with the believed type of the opponent
at time t. From this utility function, our agent derives the Luce num-
bers [8]. Since the Luce number is calculated based on a given utility
function, we denote the Luce number of an offer derived from the op-
ponent’s believed utility at time t, BT(t), by luopp(offer | BT(t)),
and the Luce number for an offer derived by the agent’s own utility
simply as lu(offer). We denote our function by QO(t) (standing for
Qualitative Offer), where t ∈Time is the time of the offer. If the
current agent is j, the strategy selects an offer o in time t such that:
QO(t)
=
arg max
o∈O
min{α, β}
(2)
where
α =
uj(o, t)
and
β =
[luopp(o | BT(t)) + lu(o)] · uBT (t)
opp
(o, t)
Seemingly, our QO function is a non-classical method for generating
offers. However, not only were we able to show its efﬁcacy by empir-
ical experiments, in which it was used in negotiations with bounded
ration agents, as we describe in Section 5, we also showed (Section
4.1.3) that it also conforms to some properties from classical nego-
tiation theory, which are mainly used by mediators. Note that the
formula does not build on the bounded rationality of the opponent.
The next subsection deals with the question of when the agent
should accept an incoming offer or reject it.
4.1.2
Accepting Offers
The agent needs to decide what to do when it receives an offer
from its opponent, offeropp, at time t −1. If we refer to the auto-
mated agent as agent 1 and the bounded rational agent as agent 2, if
u1(offeropp) ≥u1(QO(t)) then our agent accepts the offer. Other-
wise, our agent should not immediately rule out accepting the offer it
has just received. Instead, it should take into consideration the prob-
ability that its counter-offer will be accepted or rejected by the oppo-
nent. This is done by comparing the believed utility of the opponent
from the original offer as compared with the opponent’s utility from
our offer. If the difference is lower than a given threshold6 T, that
is | u2(QO(t)) −u2(offeropp) | ≤T, then there is a high proba-
bility that the opponent will be indifferent between its original offer
and our counter-offer, so our agent will reject the offer and propose a
counter-offer (taking a risk that the offer will be rejected), since the
counter-offer has a better utility value for our agent. If the difference
is greater than the threshold, i.e., there is a higher probability that the
opponent will not accept our counter-offer, our agent will accept the
opponent’s offer with a given probability, which is attached to each
outcome. To this end we deﬁne the rank number, which is associated
with each offer and a given utility function u, denoted rank(offer).
The rank number of an offer is calculated by ordering all offers on
an ordinal scale between 1 and |O| according to their utility values,
and dividing the offer’s ordering number by |O|. That is, the agent
will accept an offer with a probability rank(offeropp) and reject and
make a counter-offer QO(t) with probability 1 −rank(offeropp).
The intuition behind this is to enable the agent also to accept agree-
ments based on their relative values, on an ordinal scale of [0..1], and
not based on their absolute values.
In the next subsection we demonstrate that our proposed solution
also conforms to some properties of the Nash bargaining solution.
This gives us the theoretical basis for the usage of our technique in
6 In the simulations, T was set to 0.05.
bilateral negotiation, and for the assumption that offers proposed by
our agent will also be considered to be accepted by the opponent.
4.1.3
QO: An Alternative to Nash Bargaining Solution
We employ from Luce and Raiffa [9] the deﬁnitions of a bargain-
ing problem and the Nash bargaining solution. We denote by B =
(u1(·), u2(·)) the bargaining problem with two utilities, u1 and u2.
The Nash bargaining solution (note that the solution is not the offer
itself, but rather the payoff of the offer) is deﬁned by several char-
acteristics and is usually designed for a mediator in an environment
with complete information. A bargaining (or a negotiation) solution
f should satisfy symmetry, efﬁciency, invariance and independence
of irrelevant alternatives, wherein symmetry states that if both play-
ers have the same bargaining power, then neither player will have
any reason to accept an agreement which yields a lower payoff for
it than for its opponent. For example, for the solution to be symmet-
ric, it should not depend on the agent who started the negotiation
process. Efﬁciency states that two rational agents will not agree on
an agreement if its utility is lower for both of them than another
possible agreement. This solution is said to be Pareto-optimal. In-
variance states that for all equivalent problems B and B′, that is
B′ = (α1+β1·u1(·), α2+β2·u2(·)), α1, α2 ∈R, β1, β2 ∈R+,
the solution is also the same, f(B) = f(B′). That is, two positive
afﬁne transformations can be applied on the utility functions of both
agents and the solution will remain the same. Finally, independence
of irrelevant alternatives asserts that the solution f(B) = f(B′)
whenever B′ ⊆B and f(B) ⊆B′. That is, if new agreements are
added to the problem in such a manner that the status quo remains
unchanged, either the original solution is unchanged or it becomes
one of the new agreements.
It was shown by Nash [10] that the only solution that satisﬁes all
of these properties is the product maximizing of the agents’ utili-
ties. However, as we stated, the Nash solution is usually designed
for a mediator. Since we propose a model for an automated agent
which negotiates with bounded rational agents following the QO
function (Equation 2), our solution cannot satisfy all of these proper-
ties. To this end, we modiﬁed the independence of irrelevant alterna-
tives property to allow for a set of possible solutions instead of one
unique solution:
• PROPERTY 4A (Independence of irrelevant alternatives solutions)
A negotiation solution f satisﬁes independence of irrelevant al-
ternatives solutions if the set of all possible solutions of f(B)
is equal to the set of all possible solutions of f(B′) whenever
B′ ⊆B and f(B) ⊆B′.
Proving that our agent’s strategy for proposing offers conforms to
those properties is important since although the agent should max-
imize its own utility, it should also ﬁnd agreements that would be
acceptable to its opponent.
Theorem 1 The QO function satisﬁes the properties of symmetry,
efﬁciency and independence of irrelevant alternatives solutions.
Due to space limitations, we do not present the proof of the theo-
rem. We also showed that in certain cases QO generates agreements
which are better for the automated agent than agreements that would
have been generated by following the Nash solution.
4.2
The Bayesian Updating Rule Component
The Bayesian updating rule (BUR) is based on Bayes Theorem de-
scribed above and it provides a practical learning algorithm. We as-

sert that there are several types (or clusters) of possible agents. The
bounded rational agent should be matched to one such type. In each
time period, our agent consults the BUR component in order to up-
date its belief regarding the opponent’s type.
Recall that there are k possible types for an agent. At time t = 0
the prior probability of each type is equal, that is, P(type) =
1
k, ∀type ∈Types. Then, for each time period t we calculate the pos-
teriori probability for each of the possible types, taking into account
the history of the negotiation. This is done incrementally after each
offer is received or accepted. Then, this value is assigned to P(type).
Using the calculated probabilities, the agent selects the type whose
probability is the highest and proposes an offer as if this is the op-
ponent’s type. Formally, at each time period t ∈Time and for each
type ∈Types and offert (the offer at time period t) we compute:
P(type|offert) = P(offert|type)P(type)
P(offert)
(3)
where P(offert) = P|Types|
i=1
P(offert|typei) · P(typei). Since
the Luce numbers actually assign probabilities to each offer,
P(offert|type) is computed using the Luce numbers.
Now we can deduce the believed type of the opponent for each
time period t, denoted as BT(t), using the following equation:
BT(t) =
arg max
type∈Types
P(type|offert),
∀t ∈Time
(4)
Using this updating mechanism enables our updating component to
conform to the following conditions, which are generally imposed
on an agent’s system of beliefs, and which are part of the conditions
for a sequential Bayesian equilibrium [12]: (a) consistency and (b)
never dissuaded once convinced. Consistency implies that agent j’s
belief should be consistent with its initial belief and with the possi-
ble strategies of its opponents. These beliefs are updated whenever
possible, while never dissuaded once convinced implies that once an
agent is convinced of its opponent’s type with a probability of 1, or
convinced that its opponent cannot be of a speciﬁc type, it is never
dissuaded from its view. The results of the simulation indeed show
that in more than 70% of the simulations our agent believed that its
opponent is of the correct type with a probability of 1 or with the
highest probability amongst the other possible types.
5
Experiments
We developed a simulation environment which is adaptable such that
any scenario and utility functions, expressed as multi-issue attributes,
can be used, with no additional changes in the conﬁguration of the in-
terface of the simulations or the automated agent. Our agent can play
either role in the negotiation, while the human counterpart accesses
the negotiation interface via a web address. The negotiation itself is
conducted using a semi-formal language. Each agent constructs an
offer by choosing the different values constituting the offers. Then,
the offer is constructed and sent in plain English to its counterpart.
To test the efﬁciency of our proposed agent, we have conducted
experiments on a speciﬁc negotiation domain7. In the following sub-
sections we describe our domain, the experimental methodology and
review the results.
7 To show that our agent is capable of negotiating in other domains as well,
we loaded another domain and tested the agent. As expected, the agent per-
forms as well as in the speciﬁed domain. That is, only the utility functions
play a role, and not the scenario or the domain.
5.1
Experimental Domain
The experimental domain adheres to the problem deﬁnitions de-
scribed in Section 2. In our scenario England and Zimbabwe are ne-
gotiating in order to reach an agreement growing out of the World
Health Organization’s Framework Convention on Tobacco Control,
the world’s ﬁrst public health treaty. The principal goal of the conven-
tion is ”to protect present and future generations from the devastating
health, social, environmental and economic consequences of tobacco
consumption and exposure to tobacco smoke.” There are three pos-
sible agent types, and thus a set of six different utility functions was
created. This set describes the different types or approaches towards
the negotiation process and the other party. For example, type (a) has
a long term orientation regarding the ﬁnal agreement, type (b) has a
short term orientation, and type (c) has a compromise orientation.
Each negotiator was assigned a utility function at the beginning
of the negotiation but had incomplete information regarding the op-
ponent’s utility. That is, the different possible types of the opponent
were public knowledge, but the exact type of the opponent was un-
known. The negotiation lasts at most 14 time periods, each with a
duration of two minutes. If an agreement is not reached by the dead-
line then the negotiation terminates with a status quo outcome. Each
party can also opt out of the negotiation if it decides that the nego-
tiation is not proceeding in a favorable way. Opting out by England
means trade sanctions imposed by England on Zimbabwe (including
the ban on the import of tobacco from Zimbabwe), while if Zim-
babwe opts out then it will boycott all British imports.
A total of 576 agreements exist, consisting of the following is-
sues: (a) the total amount to be deposited into the Tobacco Fund to
aid countries seeking to rid themselves of economic dependence on
tobacco production, (b) impact on other aid programs, (c) trade is-
sues, and (d) creation of a forum to explore comparable arrangements
for other long-term health issues. While on the ﬁrst two issues there
are contradicting preferences for England and Zimbabwe, for the last
two issues there are options which might be preferred by both sides.
5.2
Experimental Methodology
We tested our agent against human subjects, all of whom are com-
puter science undergraduates. The experiment involved 44 simula-
tions with human subjects, divided into 22 pairs. Each simulation
was divided into two parts: (i) negotiating against another human
subject, and (ii) negotiating against the automated agent. The sub-
jects did not know in advance against whom they played. Also, in
order not to bias the results as a consequence of the subjects getting
familiar with the domain and the simulation, for exactly half of the
subjects the ﬁrst part of the simulation consisted of negotiating with
a human opponent, while the other half negotiated ﬁrst with the auto-
mated agent. The outcome of each negotiation is either the reaching
of a full agreement, opting out, or reaching the deadline.
5.3
Experimental Results
The main goal of our proposed method was to enable the automated
agent to achieve higher utility values at the end of the negotiation
than the human negotiator, playing the same role, achieved, and to
allow the negotiation process to end faster as compared to negotia-
tions without our agent. Another secondary goal was to test whether
our agent increased the social welfare of the outcome.
Table 1 summarizes the average utility values of all the negotia-
tions and the average of the sums of utility values in all the exper-
iments. HZ and HE denote the utility value gained by the human

playing the role of Zimbabwe or England, respectively, and QZ and
QE denote the utility value gained by the QO agent playing either
role. The utility values ranged from -575 to 895 for the England role
and from -680 to 830 for the Zimbabwe role. The Status-Quo value
in the beginning of the negotiation was 150 for England and -610 for
Zimbabwe. England had a ﬁxed gain of 12 points per time period,
while Zimbabwe had a ﬁxed loss of -16 points.
Table 1.
Final negotiations utility values and sums of utility values
Parameter
Avg
Stdev
QZ vs. HE
-27.4
287.2
HZ vs. HE
-260
395.6
QE vs. HZ
150.5
270.8
HE vs. HZ
288.0
237.1
HZ vs. QE
113.09
353.23
HE vs. QZ
349.14
299.36
Sum - HE vs. QZ
321.7
223.4
Sum - HZ vs. QE
263.6
270.5
Sum - HE vs. HZ
27.86
449.9
First, we examined the ﬁnal utility values of all the negotiations
for each player, and the sums of the ﬁnal utility values. The results
show that when the automated agent played the role of Zimbabwe,
it achieved signiﬁcantly higher utility values as opposed to a human
agent playing the same role (using paired t-test: t(22) = 2.23, p <
0.03). On the other hand, when playing the role of England, there is
no signiﬁcant difference between the utility values of our agent and
the human player, though the average utility value for the human was
higher than for the automated agent. The results also show that the
average utility values when the human played against another human
are lower than the average utility values when he played against our
agent. However, these results are signiﬁcant only when the human
players played the role of Zimbabwe (t(22) = −3.43, p < 0.003).
One explanation for the higher values achieved by the QO agent is
that the QO agent is more eager to accept agreements than humans.
When playing Zimbabwe side, which has a negative time cost, ac-
cepting agreements sooner, rather than later, allowed the agent to
gain higher utility values than the human playing the same side.
Comparing the sum of utility values of both negotiators, based
on the role our agent played, we show that this sum is signiﬁ-
cantly higher when the negotiations involved our agent (either when
our agent played the role of Zimbabwe or England), as opposed to
negotiations involving only human players (using 2-sample t-test:
t(22) = 2.74, p < 0.009 and t(22) = 2.11, p < 0.04).
Another important aspect of the negotiation is the outcome -
whether a full agreement was reached or whether the negotiation
ended with no agreement (either status-quo or opting out) or with
a partial agreement. While only 50% of the negotiations involving
only humans ended with a full agreement, 80% of the negotiations
involving the automated agent ended with a full agreement. Using
Fishers Exact test we determined that there is a correlation between
the kind of the opponent agent (be it our agent or the human) and
the form of the ﬁnal agreement (full, partial or none). The results
show that there is a signiﬁcantly higher probability of reaching a full
agreement when playing against our agent (p < 0.006).
Then we examined the ﬁnal time period of the negotiations. All
of the test results showed that in negotiations in which our agent
was involved the ﬁnal time period was signiﬁcantly lower than the ﬁ-
nal time period in negotiations in which only humans were involved.
The average ﬁnal time period with two human negotiators was 11.36,
while the average ﬁnal time period against our agent playing the role
of Zimbabwe and the role of England was 6.36 and 6.27 respectively.
We used the 2-sample Wilcoxon test to compare between the ﬁnal
time periods of our agent and the human agent when playing against
the same opponent (for England role p < 0.001 and for Zimbabwe
role p < 0.002). In addition, we used the Wilcoxon signed rank test,
to examine the ﬁnal time period achieved by the same human player
in the two simulations in which he participated - once involving an-
other human player, and another involving an automated agent (for
England role p < 0.0004, for Zimbabwe role p < 0.001).
We also examined the total number of offers made and received in
each negotiation. The average number of offers made and received in
negotiations involving our agent was 10.86 and 10.59, while in nego-
tiation involving only human players the average number was 18.09.
Using 2-sample Wilcoxon test we show that signiﬁcantly fewer offers
were made when our agent was involved (p < 0.007 and p < 0.004).
6
Conclusions
This paper outlines an automated agent design for bilateral negoti-
ation with bounded rational agents where there is incomplete infor-
mation regarding the opponent’s utility preferences. The automated
agent incorporated a qualitative decision making mechanism. The re-
sults showed that our agent is indeed capable of negotiating success-
fully with human counterparts and reaching efﬁcient agreements. In
addition, the results demonstrated that the agent played at least as
well as, and in the case of one of the two roles, gained signiﬁcantly
higher utility values, than the human player. Though the experiments
were conducted on a speciﬁc domain, it is quite straightforward to
adapt the simulation environment to any other scenario.
Although much time was spent on designing the mechanism for
generating an offer, the results showed that most of the agreements
reached were offered by the human counterpart. Our agent accepted
the offers based on a probabilistic heuristic. A future research di-
rection would be to improve this mechanism and the probabilistic
heuristic and then test it by conducting the same sets of experiments.
REFERENCES
[1]
C. Boutilier, R. Das, J. O. Kephart, G. Tesauro, and W. E. Walsh, ‘Co-
operative negotiation in autonomic systems using incremental utility
elicitation’, in Proceedings of UAI-03, pp. 89–97, (2003).
[2]
D. Dubois, H. Prade, and R. Sabbadin, ‘Decision-theoretic foundations
of qualitative possibility theory’, European Journal of Operational Re-
search, 128, 459–478, (2001).
[3]
P. Faratin, C. Sierra, and N. R. Jennings, ‘Using similarity criteria to
make issue trade-offs in automated negotiations’, AIJ, 142(2), 205–237,
(2002).
[4]
S. Fatima and M. Wooldridge, ‘An agenda based framework for multi-
issue negotiation’, AIJ, 152, 1–45, (2004).
[5]
S. Fatima, M. Wooldridge, and N. R. Jennings, ‘Bargaining with in-
complete information’, AMAI, 44(3), 207–232, (2005).
[6]
P. Hoz-Weiss, S. Kraus, J. Wilkenfeld, D. R. Andersen, and A. Pate,
‘An automated agent for bilateral negotiations with humans’, in Proc.
of AAAI/IAAI-02, pp. 1000–1001, (2002).
[7]
R. J. Lin and S. T. Chou, ‘Bilateral multi-issue negotiations in a dy-
namic environment’, in Workshop on AMEC V, (2003).
[8]
R. D. Luce, Individual Choice Behavior: A Theoretical Analysis, John
Wiley & Sons, NY, 1959.
[9]
R. D. Luce and H. Raiffa, Games and Decisions - introduction and
critical survey, John Wiley & Sons, NY, 1957.
[10]
J. F. Nash, ‘The bargaining problem’, Econ., 18, 155–162, (1950).
[11]
M. Oprea, ‘An adaptive negotiation model for agent-based elec-
tronic commerce’, Studies in Informatics and Control, 11(3), 271–279,
(2002).
[12]
M. J. Osborne and A. Rubinstein, A Course In Game Theory, MIT
Press, Cambridge MA, 1994.
[13]
M. Tennenholtz, ‘On stable social laws and qualitative equilibrium for
risk-averse agents’, in KR, pp. 553–561, (1996).

