Between holism and reductionism: a philosophical
primer on emergence
MASSIMO PIGLIUCCI*
Philosophy Program, The Graduate Center, City University of New York, 365 Fifth Ave., New York,
NY 10016, USA
Received 14 December 2012; revised 17 January 2013; accepted for publication 17 January 2013
Ever since Darwin a great deal of the conceptual history of biology may be read as a struggle between two
philosophical positions: reductionism and holism. On the one hand, we have the reductionist claim that evolution
has to be understood in terms of changes at the fundamental causal level of the gene. As Richard Dawkins famously
put it, organisms are just ‘lumbering robots’ in the service of their genetic masters. On the other hand, there is a
long holistic tradition that focuses on the complexity of developmental systems, on the non-linearity of gene–
environment interactions, and on multi-level selective processes to argue that the full story of biology is a bit more
complicated than that. Reductionism can marshal on its behalf the spectacular successes of genetics and molecular
biology throughout the 20th and 21st centuries. Holism has built on the development of entirely new disciplines
and conceptual frameworks over the past few decades, including evo-devo and phenotypic plasticity. Yet, a number
of biologists are still actively looking for a way out of the reductionism–holism counterposition, often mentioning
the word ‘emergence’ as a way to deal with the conundrum. This paper brieﬂy examines the philosophical history
of the concept of emergence, distinguishes between epistemic and ontological accounts of it, and comments on
conceptions of emergence that can actually be useful for practising evolutionary biologists.
© 2013 The Linnean
Society of London, Biological Journal of the Linnean Society, 2014, 112, 261–267.
ADDITIONAL KEYWORDS: evo-devo – evolution – phenotypic plasticity – philosophy of science –
supervenience.
‘Emergence’ is a controversial concept with a convo-
luted history, in both science and philosophy. It is
therefore not surprising that it has been misused and
viliﬁed, as well as more often than not misunderstood.
Typically, the idea of emergence is brought up by
researchers who are – for one reason or another –
unhappy with an ultra-reductionist scientiﬁc pro-
gramme, preferring instead some kind of holism or
interactionism
in
the
way
they
approach
their
research questions (think of the always current
debates on gene–environment interactions: Lewontin,
1974a). Just as surely, biologists who are embedded in
reductionist programmes are skeptical of emergence
and similar ideas as obscurantist and leading only to
never ending and fruitless debates – think, say, of
Richard Dawkins’ (1976) dismissal of anything smell-
ing like group selection, or his famous quip that living
organisms are nothing but ‘lumbering robots’ control-
led by their selﬁsh genes.
It is undoubtedly the case that methodological
reductionism has an enviable track record both in
biology speciﬁcally and in science more generally. By
the onset of the 20th century chemistry had already
for all effective purposes been reduced to physics (Le
Poidevin, 2005; although this is by no means an
accepted
conclusion
in
philosophy
of
chemistry:
Hendry & Needham, 2007), and hopes were high
that biology would soon follow suit. Indeed, the
demise of vitalism and the contemporary surge of
*E-mail: massimo@platofootnote.org
bs_bs_banner
Biological Journal of the Linnean Society, 2014, 112, 261–267.
© 2013 The Linnean Society of London, Biological Journal of the Linnean Society, 2014, 112, 261–267
261
Downloaded from https://academic.oup.com/biolinnean/article/112/2/261/2415616 by guest on 04 September 2022

model-based genetics at the beginning of the 20th
century seemed to nail the argument in favour of
reductionists. In 1931 J. B. S. Haldane could conﬁ-
dently declare that ‘Biologists have almost unani-
mously
abandoned
vitalism
as
an
acknowledged
belief’, although he was at the same time uncon-
vinced by exclusively mechanistic explanations of
biological phenomena: ‘We must ﬁnd a different theo-
retical basis of biology, based on the observation that
all the phenomena concerned tend towards being so
coordinated that they express what is normal for an
adult organism’ (cited in Bedau & Cleland, 2010: 95).
The Modern Synthesis (MS) of the 1930s and 1940s
was, in some important respects, another manifesta-
tion of the same debate. On the one hand, population
geneticists like Dobzhansky and Fisher provided the
experimental and theoretical bases for a reduction of
organismal biology to genetics; on the other hand, the
infamous exclusion of developmental biology from the
MS, as well as Dobzhansky’s (1981) own research on
reaction norms and gene–environment interactions in
Drosophila pseudoobscura pointed to the limitations
of straightforward reductionist approaches in biology.
As is well known, a literature parallel to the MS thus
originated (e.g. with the early work of Schmalhausen,
1949, and Waddington, 1942) and eventually led –
among other factors – to the onset of ‘evo-devo’ as a
separate ﬁeld of investigation (Love, 2009), as well as
to the resurgence and widespread appreciation of
research on phenotypic plasticity (Pigliucci, 2001).
More recently, the genomic revolution led initially
to wildly optimistic claims of what would be learned
after
the
human
genome
had
been
sequenced
(Gannett, 2008). Yet, it soon became clear that having
the sequence on a CD would not straightforwardly
lead to the cure of diseases or to a deep understand-
ing of how to make a human being. Moreover, com-
parative genomics has quickly led to a shift in focus
from what individual genes do or don’t do, to emer-
gent complexities and systemic properties of genetic
networks (Vidal, Cusick & Barabási, 2011).
Given this convoluted and controversial history, it
may pay to get a bit clearer about where the concept
of emergence came from and how it evolved qua
concept (O’Connor, 2006). Perhaps the ﬁrst thing to
appreciate is that emergentism is usually put forth
as a third way between mechanism-reductionism
and vitalism-holism. Emergentists – particularly in
biology – reject quasi-mystical appeals to vital forces,
but at the same time argue for the appearance of
genuinely novel phenomena at various levels of com-
plexity of matter. Of course, much hinges on what
‘novel’ means in this context, as well as on the idea
that there are meaningful ‘levels’ of complexity in
nature. I will return to two (not mutually exclusive)
interpretations of both these ideas shortly.
One of the earliest articulations of the concept of
emergence in biology is due, perhaps surprisingly, to
John Stuart Mill (1843: III(6)1): ‘To whatever degree
we might imagine our knowledge of the properties of
the several ingredients of a living body to be extended
and perfected, it is certain that no mere summing up
of the separate actions of those elements will ever
amount to the action of the living body itself.’ The
idea being expressed here is familiar in both biology
and statistics nowadays: we know of plenty of exam-
ples, from chemistry and biology, where the whole –
as the saying goes – is more than (or, to be more
accurate, different from) the sum of its parts. Mill
himself contrasted an example from physics and one
from chemistry to make his point. In Newtonian
physics (which was, of course, the dominant paradigm
at the time), mechanical forces can be combined in an
additive fashion by means of vector analysis. So, for
instance, the total force being applied to a given
object from two different sources is simply the sum of
the individual forces thus applied. In chemistry,
however, the result of a chemical reaction is pretty
much never a simple sum of the reactants: we can
combine, say, an acid and a base to obtain a salt (and
water). The properties of the salt are in no meaning-
ful sense simply the sum of the properties of the
pre-reaction acid and base.
Arguably the next big step in the development of
emergentism in philosophy of science was due to C. D.
Broad (1925), who framed the issue in terms of the
independence (or lack thereof) of the ‘special’ sciences
(i.e. of anything that is not fundamental physics).
This is very much still a topic of discussion today,
usually pitting physicists on the side of one or another
‘theory of everything’ (Weinberg, 1992) and (some)
philosophers who defend the notion of a fundamental
disunity of science (Dupré, 1995). Broad aligned
himself with the disunity camp, on the basis that he
thought there are some emergent ‘trans-ordinal’ laws
that are compatible with, and yet irreducible to, more
fundamental principles of physics. Moreover, he did
not think these laws presented any particular reason
for
the
mystically
oriented
to
rejoice:
‘There
is
nothing, so far as I can see, mysterious or unscientiﬁc
about a trans-ordinal law or about the notion of
ultimate characteristics of a given order. A trans-
ordinal law is as good a law as any other; and, once
it has been discovered, it can be used like any other
to suggest experiments, to make predictions, and to
give us practical control over external objects’ (Broad,
1925: 79).
It is worth sketching out the approach of two of the
major modern critics of the standard reductionist
programme in science, a programme that seldom
appears to be questioned by many practising scien-
tists. John Dupré (1995) has advanced the idea that
262
M. PIGLIUCCI
© 2013 The Linnean Society of London, Biological Journal of the Linnean Society, 2014, 112, 261–267
Downloaded from https://academic.oup.com/biolinnean/article/112/2/261/2415616 by guest on 04 September 2022

the sciences are fundamentally dis-uniﬁed, which
means that any talk of a ‘theory of everything’ is
nonsense (and, in his view, even socially dangerous,
because it gives priority to a particular, mechanistic,
view of science and knowledge). Dupré’s argument
begins with the (uncontroversial, among philoso-
phers) observation that science itself depends on
metaphysical assumptions that are not justiﬁable
empirically, and proceeds to question three related
theses underlying the project of uniﬁcation: determin-
ism, essentialism, and reductionism. The ﬁrst and
last metaphysical positions are challenged by the idea
of emergence. This is not the place for an in-depth
analysis of Dupré’s views (which do have their critics,
e.g. Mitchell, 2003), my point simply being to note
that there is a vibrant ongoing discussion in philoso-
phy of science about issues that many scientists take
for granted without further examination.
A
similar
example
is
provided
by
Nancy
Cartwright’s (1983) criticism of the unity project, this
time hinging on her analysis of the concept of natural
laws (her positions too, of course, have been critically
evaluated, e.g. McArthur, 2006). Cartwright sees laws
as empirical generalizations with more or less broad
(but, crucially, not universal) domains of application.
Indeed, she maintains that the empirical evidence
itself strongly suggests that laws cannot be both
universal and true: the only circumstances when we
can verify a law of nature (say, Newtonian mechanics)
to a precise extent is when we create artiﬁcial worlds
characterized by highly controlled conditions. In the
real world, by contrast, laws only apply given a more
or less large number of ceteris paribus conditions. And
this, of course, is the case for physics, where usually
scientists take the very idea of a law of nature to be
uncontroversial. In biology there are still plenty of
debates – among both philosophers and biologists –
about whether we can even sensibly talk about laws
(as opposed, again, to mere empirical generalizations)
in the ﬁrst place (Mikkleson, 2003; Lange, 2005;
Carroll, 2006; Elgin, 2006). While at ﬁrst sight the
relationship between Cartwright’s views on natural
laws and emergence is a bit less obvious than in
Dupré’s case, consider that one of the metaphysical
ideas that Cartwright needs to challenge surely is
reductionism, and that emergence arguably repre-
sents the major challenge for reductionist explana-
tions of the world.
TWO TYPES OF EMERGENCE
Having brieﬂy examined the conceptual history of
emergence, it is time to make a crucial distinction
between two ways of thinking about it (or about
reductionism, for that matter) that have been lurking
unaddressed throughout the discussion so far. One
can think of emergent properties from either an onto-
logical or an epistemological stance, although the two
are not mutually exclusive. Ontology, of course, has to
do with the nature of things, while epistemology has
to do with how we (think we) come to have knowledge
of the nature of things. Ontological claims are inher-
ently metaphysical, while epistemological claims are
not (they can be purely pragmatic, or derived from
principles of logic). To complicate things further,
several philosophers (though by no means all!) from
the mid-20th century on began to agree that meta-
physical statements ought to be evaluated in terms of
our epistemic access to the world, meaning that what
we can know empirically should constrain how we
think metaphysically (e.g. Ladyman & Ross, 2009).
In terms of emergence and reductionism, my sense
of the literature is that most philosophers nowadays
are in agreement with most scientists: they reject
ontological emergence and accept ontological reduc-
tionism. What this means is that the standard meta-
physical position is that there are no true emergent
phenomena, only phenomena that cannot currently
(or even ever) be described or understood in terms of
fundamental physics, and yet are, in fact, only
complex manifestations of the microscopic world as
understood by fundamental physics. A simple way to
make sense of this idea is to deploy the concept of
supervenience: in philosophy a property A is super-
venient on another one, B, just in case A cannot
change unless there is a change in B. For instance, if
the total amount of money in my pocket is $20, this
fact cannot change unless the number of coins and/or
notes that make up that amount somehow diminishes
or increases inside said pocket (as opposed to, say,
simply exchanging a dollar bill for four 25 cent coins).
Analogously, higher-order phenomena in physics or
biology supervene on micro-physical phenomena just
in case the only way to change the former is to change
the latter (i.e. there are no genuinely emergent
phenomena).
I will not comment much further on the issue of
ontological emergence versus reductionism because it
is of hardly any concern to the practising biologist. I
will note, however, that the position I just described is
rather odd, because it actually contradicts the prima
facie
empirical
evidence:
as
Jerry
Fodor
(1974)
famously put it, an ‘immortal economist’ would in
vain attempt to derive the principles of his discipline
from knowledge of fundamental physics. It simply
cannot be done. But if our epistemology tells us that
the universe behaves as if it contained genuine emer-
gent
properties
(say,
the
properties
of
economic
systems, which do not seem to have much to do with
the properties of quarks), then is it not the case that
rejection of ontological emergence is a ﬂagrant viola-
tion of the principle that epistemology should inform
PRIMER ON EMERGENCE
263
© 2013 The Linnean Society of London, Biological Journal of the Linnean Society, 2014, 112, 261–267
Downloaded from https://academic.oup.com/biolinnean/article/112/2/261/2415616 by guest on 04 September 2022

metaphysics? All in all, I think the most reasonable
course of action is actually to take a neutral, agnostic,
stance on the matter and to proceed to where we are
going next: epistemological emergence.
O’Connor (2006) helpfully describes two types of the
latter, which he labels predictive and irreducible-
pattern. Predictive emergence is the idea that in
practice it is not possible to predict the features of a
complex system in terms of its constituent parts, even
if one were to know all the laws governing the behav-
ior of said parts. Irreducible-pattern emergentists
maintain that the problem is conceptual in nature,
i.e. that the lower-level laws simply do not have the
tools to deal with higher-level phenomena – as in the
already mentioned case of Fodor’s unfortunate econo-
mist with a misguided penchant for fundamental
physics.
As O’Connor himself acknowledges, the distinction
between predictive and irreducible-pattern views of
epistemic emergence is not sharp, but it does draw
attention
to
the
fact
that
emergent
phenomena
present both pragmatic and conceptual issues for the
practising scientist and aspiring reductionist. It is not
just, for instance, that it would be too computation-
ally cumbersome to develop a quantum mechanical
theory of economics (the predictive issue), it is that
one would not know where to start with the task of
deploying the tools of quantum mechanics (indeter-
minacy
principle,
non-locality,
etc.)
to
somehow
account for the phenomena studied by economists
(relation between supply and demand, boom-and-bust
cycles, etc.). So, again, one does not need to be an
ontological emergentist to ﬁrmly reject a greedy
reductionist
programme
in
biology
or
the
social
sciences.
TWO EXAMPLES OF EMERGENT
PROPERTIES: NK NETWORKS AND
G-BY-E INTERACTIONS
It will be instructive to anchor the somewhat esoteric
discussion we have engaged in so far with a couple of
examples from the actual biological literature, to
focus our ideas about what emergence may sensibly
mean in the context of biological research.
The ﬁrst such example comes from a paper by
Romero & Zertuche (2007) on NK networks, also
known as Kauffman-type networks. Stuart Kauffman
(1969) proposed these models – which are a type of
cellular automaton – as an early attempt at exploring
the properties of genetic networks (characterized, spe-
ciﬁcally, by N elements each with K input connections
and one output). Romero & Zertuche were interested
in the relationship between the properties of NK
cellular automata and the robustness displayed by
actual complex genetic networks in living organisms.
Robustness measures the tendency of genetic net-
works to withstand internal disruptions (e.g. muta-
tions) while maintaining functionality, and is related
to the broader concept of evolvability (Pigliucci, 2008).
While the mathematics of Romero & Zertuche
(2007) is not for the faint of heart, their results are
relatively straightforward: robustness emerges from
the statistical properties of a genotype–phenotype
map modelled as an NK Kauffman-type network,
although there are restrictions on the range of
values of both N and K that yield robust mapping.
Interestingly,
these
restrictions
are
within
the
empirical NK ranges that are derived from studies of
organisms
as
disparate
as
yeast
and
our
own
species. Here, then, emergence is the appearance of
a biological property (robustness) as a result of a
particular
type
of
non-linear
interaction
among
lower-level
entities
(the
genes
in
the
network).
Clearly, Romero & Zertuche are not making any type
of ontological statement here (indeed, there is no
reason they should even be concerned with ontology),
but are rather deploying something like O’Connor’s
predictive concept of epistemic emergence. It is an
open question whether research such as that of
Romero & Zertuche may lead to the stronger claim
of irreducible-pattern-type emergence. In this case,
my hunch is that this may not be the case, as the
levels of analysis – individual genes and gene net-
works – with which Romero & Zertuche (2007) are
concerned are sufficiently close to each other to be
described in terms of the same conceptual arsenal.
But, again, this is an open question.
My second example is a classic one, in reference to
which ‘emergence’ has often been brought to bear – or
disparaged, depending on a researcher’s metaphysical
preferences: the study of gene–environment interac-
tions (Lewontin, 1974a; Pigliucci, 2001). The debate
about nature and nurture has been going on at least
since Plato’s idea of learning as recollection in the
Phaedo, and later John Locke’s (opposite) contention
that the human mind is a tabula rasa on which
experience writes out our character. In modern times,
similar discussions have pitted social scientists who
are inclined toward a Lockean position (think of B. F.
Skinner’s emphasis on operant conditioning) versus
those more taken by a genetic perspective (as in The
Belle Curve book by Herrnstein & Murray, 1994) (see
Keller, 2010, for an analysis of the conceptual confu-
sions underlying the modern version of the debate).
A better tool for thinking about gene–environment
interactions has been available since the beginning
of the 20th century in the form of the idea of a
norm of reaction: a genotypic- and environment-
speciﬁc function that displays the range of pheno-
types produced by a given genotype within a given set
264
M. PIGLIUCCI
© 2013 The Linnean Society of London, Biological Journal of the Linnean Society, 2014, 112, 261–267
Downloaded from https://academic.oup.com/biolinnean/article/112/2/261/2415616 by guest on 04 September 2022

of
environments.
As
Lewontin
(1974b)
elegantly
showed in reference to the speciﬁc case of the herit-
ability of human IQ, grasping the concept of a reac-
tion
norm
allows
one
to
understand
seemingly
paradoxical ideas such as, for example, that a change
in environmental variance may affect estimates of
heritability (as it has been empirically demonstrated
several times since: Pigliucci, 2001: ch. 4), even
though the ‘genetic’ component of phenotypic variance
remains unaltered.
But
what
does
it
mean
to
think
of
gene–
environment interactions as ‘emergent properties’?
The phrase can be given at least two distinct inter-
pretations, one statistical and pretty straightfor-
ward,
the
other
one
a
bit
more
vague
but
particularly relevant to evolutionary developmental
biology. Consider the statistical meaning ﬁrst. In a
typical reaction norm diagram one can disentangle
the average effect of the environment on a given
trait – measured by the mean slope of the measured
reaction norms – from the average effect of genotype,
measured by the mean height of the reaction norms
sampled. These are both additive effects, respectively
quantiﬁable
by
the
so-called
Environmental
and
Genetic (E, G) variances in a standard analysis of
variance. In many cases, however, the individual (i.e.
genotype-speciﬁc, as opposed to average) reaction
norms are characterized by different heights in the
Environment versus Phenotype space, and they also
have different slopes (indicating genetic variation for
phenotypic plasticity). This so-called G-by-E interac-
tion variance is the result of (statistically) non-
additive effects that cannot simply be reduced to a
sum of genetic and environmental effects. A popula-
tion with a signiﬁcant G-by-E variance, therefore,
exhibits a quantiﬁable ‘emergent’ (at the statistical,
population-level) property.
There is a less straightforward, but more interest-
ing, sense in which G-by-E represents a case of emer-
gence in biology. As again Lewontin pointed out, if we
think in terms of genetic and environmental effects as
distinct causes shaping phenotypes in a more or less
additive-linear fashion, we put ourselves in the naive
position of trying to understand how a house is built
by simply weighing the total amount of bricks and
lime that goes into it. Clearly, the key to building the
house lies in the speciﬁc alternating pattern in which
bricks and lime interact to yield the ﬁnal construc-
tion. Similarly, genes and environments continuously
interact to build phenotypes throughout the process
we call development. And this is a major reason why
one simply cannot understand evolution without
development (and vice versa), an idea that has lurked
around for many decades before ﬁnally ﬂourishing
into a distinct ﬁeld of evo-devo studies (Love, 2009).
Of course, one thing is to appreciate Lewontin’s
house-building metaphor, another one is to cash out
on the promise of evo-devo in order to understand the
emergence of phenotypes in biological organisms.
Regardless, the point remains that this – as well as
the previous case of robustness – seems to represent
a genuine case of emergence, at least at the epistemic
level (as I mentioned above, ontological emergence is
a metaphysical notion that is likely not to be settled
empirically, and about which the best course of action
is to maintain philosophical neutrality).
WHY EMERGENCE?
A good number of scientists are understandably wary
of the notion of emergence, for the simple reason that
it sounds a bit too mystical and wool-eyed. Of course,
if emergence turns out to be an ontological reality,
then these scientists would simply be mistaken and
would have to accept a new metaphysics. However,
even if emergence is only an epistemic phenomenon,
there are good reasons to take it seriously, for
instance because it points toward current methodo-
logical or theoretical deﬁciencies that make straight-
forward reductionist accounts unfeasible in practice,
if not in principle.
Still, in order for more scientists to take emergence
seriously we need a coherent account of why we see
emergent phenomena to begin with. One such account
has been provided recently by Brian Johnson (2010),
and it is worth considering brieﬂy. I am not suggest-
ing that Johnson is necessarily correct, or that his
explanation is the only one on the table. But it rep-
resents a good example of the contribution that phi-
losophy of science (in this case, actually done by a
scientist) can give to the way in which scientists
themselves think of a given issue. Besides, Johnson
may very well turn out to be exactly right.
Johnson’s basic idea is simple: (at least some kinds
of) emergent properties are the result of a large
number of interactions among parts of a complex
system, all going on simultaneously in time and
space. In order to be able to grasp emergent outcomes
our brains should be able to think in parallel at the
conscious level (parallel unconscious thinking does
occur, but it leads to an ‘intuitive’, not rational, grasp
of phenomena). As the human brain is not capable of
parallel conscious processing of information, we are
faced with the impossibility of reasoning our way
through the mechanics of emergence.
How do we know that the human brain cannot do
parallel processing consciously? There are several
reasons to think so, but Johnson provides a simple
little exercise in ﬁgure 1 of his paper (Johnson, 2010)
which is worth trying out to see how difficult that sort
of thinking actually is, and how unsuitable we are at
carrying it out. (The exercise involves summing up
PRIMER ON EMERGENCE
265
© 2013 The Linnean Society of London, Biological Journal of the Linnean Society, 2014, 112, 261–267
Downloaded from https://academic.oup.com/biolinnean/article/112/2/261/2415616 by guest on 04 September 2022

numbers, ﬁrst on a single row – which is easy to do –
then on multiple rows, which becomes immediately
overwhelming.)
Interestingly, Johnson’s example of an emergent
property that is not mysterious, and yet that we
cannot cognitively deal with, is cellular automata (for
a similar take, also using cellular automata, see
various works by Mark Bedau, e.g. Bedau, 2002).
Johnson’s (2010) ﬁgure 2 presents a standard cellular
automaton, and argues that we cannot predict the
behaviour of the cells in the game because our brains
cannot process in parallel the various simple rules
that generate such behaviour. There is no magic here,
as we designed the rules and we can check – time
instant by time instant – that the behaviour of the
automaton is, in fact, the result of the application of
such rules. But we cannot help being baffled by the
complex and elegant pattern ‘emerging’ from the mas-
sively parallel deployment of the same rules. Analo-
gously,
there
may
be
no
mystery
in,
say,
the
emergence of robustness from the interactions going
on in genetic networks, or the emergence of pheno-
types during development (save, of course, for the
possibility that some of these behaviours may be
ontologically, not just epistemically, emergent).
If Johnson (2010) is correct, then emergence is a
necessary concept to deploy across scientiﬁc disci-
plines for eminently practical reasons, any time that
there is a mismatch in degree of complexity and
interactivity between the way the world that we try to
comprehend actually is and the capacities of the
brains with which we try to comprehend it work.
Nothing spooky or mysterious about it, just the
natural result of the fact that brains that evolved to
solve Pleistocene problems cannot compute in the way
in which cellular automata, and countless other phe-
nomena, ought to be computed in order to be deeply
understood.
REFERENCES
Bedau M. 2002. Downward causation and the autonomy of
weak emergence. Principia 6: 5–50.
Bedau MA, Cleland CE. 2010. The nature of life: classical
and contemporary perspectives from philosophy and science.
Cambridge: Cambridge University Press.
Broad CD. 1925. The mind and its place in nature. London:
Routledge & Kegan Paul.
Carroll JW. 2006. Laws of nature. Stanford Encyclopedia of
Philosophy. Available at: http://plato.stanford.edu/archives/
fall2006/entries/laws-of-nature/
(accessed
14
December
2012).
Cartwright N. 1983. How the laws of physics lie. Oxford:
Oxford University Press.
Dawkins R. 1976. The selﬁsh gene. Oxford: Oxford University
Press.
Dobzhansky T. 1981. Dobzhansky’s genetics of natural popu-
lations I-XLIII. Lewontin RC, Moore JA, Provine WB,
Wallace B. New York: Columbia University Press.
Dupré J. 1995. The disorder of things: metaphysical founda-
tions of the disunity of science. Cambridge, MA: Harvard
University Press.
Elgin M. 2006. There may be strict empirical laws in biology,
after all. Biology and Philosophy 21: 119–134.
Fodor J. 1974. Special sciences (or: the disunity of science as
a working hypothesis). Synthese 28: 97–115.
Gannett L. 2008. The Human Genome Project. Stanford
Encyclopedia
of
Philosophy.
Available
at:
http://plato.
stanford.edu/archives/win2008/entries/human-genome/
(accessed 14 December 2012).
Hendry RF, Needham P. 2007. Le Poidevin on the reduction
of chemistry. British Journal for the Philosophy of Science
58: 339–353.
Herrnstein RJ, Murray C. 1994. The bell curve: intelli-
gence and class structure in american life. New York: Free
Press.
Johnson BR. 2010. Eliminating the mystery from the
concept of emergence. Biology and Philosophy 25: 843–
849.
Kauffman SA. 1969. Metabolic stability and epigenesis in
randomly constructed genetic nets. Journal of Theoretical
Biology 22: 437–467.
Keller EF. 2010. The mirage of a space between nature and
nurture. Durham, NC: Duke University Press.
Ladyman J, Ross D. 2009. Every thing must go: metaphysics
naturalized. Oxford: Oxford University Press.
Lange M. 2005. Ecological laws: what would they be and why
would they matter? Oikos 110: 394–403.
Le
Poidevin
R.
2005.
Missing
elements
and
missing
premises: a combinatorial argument for the ontological
reduction of chemistry. British Journal for the Philosophy of
Science 56: 117–134.
Lewontin RC. 1974a. The genetic basis of evolutionary
change. New York: Columbia University Press.
Lewontin RC. 1974b. The analysis of variance and the
analysis of causes. American Journal of Human Genetics 26:
400–411.
Love AC. 2009. Marine invertebrates, model organisms,
and the modern synthesis: epistemic values, evo-devo, and
exclusion. Theoretical Bioscience 128: 19–42.
McArthur D. 2006. Contra Cartwright: structural realism,
ontological
pluralism
and
fundamentalism
about
laws.
Synthese 151: 233–255.
Mikkleson GM. 2003. Ecological kinds and ecological laws.
Philosophy of Science 70: 1390–1400.
Mill JS. 1843. A system of logic. London: Longmans, Green &
Co..
Mitchell
S.
2003.
Biological
complexity
and
integrative
pluralism. Cambridge: Cambridge University Press.
O’Connor T. 2006. Emergent properties. Stanford Encyclo-
pedia of Philosophy. Available at: http://plato.stanford.edu/
archives/win2006/entries/properties-emergent/ (accessed 14
December 2012).
Pigliucci M. 2001. Phenotypic plasticity. beyond nature
266
M. PIGLIUCCI
© 2013 The Linnean Society of London, Biological Journal of the Linnean Society, 2014, 112, 261–267
Downloaded from https://academic.oup.com/biolinnean/article/112/2/261/2415616 by guest on 04 September 2022

and nurture. Baltimore, MD: Johns Hopkins University
Press.
Pigliucci M. 2008. Is evolvability evolvable? Nature Reviews
Genetics 9: 75–82.
Romero D, Zertuche F. 2007. Number of different binary
functions generated by NK-Kauffman networks and the
emergence of genetic robustness. Journal of Mathematical
Physics 48: 083506. [11 pages].
Schmalhausen II. 1949. Factors of evolution. The theory of
stabilizing selection. Philadelphia, PA: Blakiston.
Vidal M, Cusick ME, Barabási AL. 2011. Interactome
networks and human disease. Cell 144: 986–998.
Waddington CH. 1942. Canalization of development and the
inheritance of acquired characters. Nature 150: 563–565.
Weinberg S. 1992. Dreams of a ﬁnal theory. New York:
Pantheon.
PRIMER ON EMERGENCE
267
© 2013 The Linnean Society of London, Biological Journal of the Linnean Society, 2014, 112, 261–267
Downloaded from https://academic.oup.com/biolinnean/article/112/2/261/2415616 by guest on 04 September 2022

