The Lazy Neuron Phenomenon: On Emergence of Activation
Sparsity in Transformers
Zonglin Li∗, Chong You∗, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J.
Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, and Sanjiv Kumar
Google Research, New York City
June 13, 2023
Abstract
This paper studies the curious phenomenon for machine learning models with Transformer architectures
that their activation maps are sparse. By activation map we refer to the intermediate output of the multi-layer
perceptrons (MLPs) after a ReLU activation function, and by “sparse” we mean that on average very few
entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each input to MLP. Moreover, larger
Transformers with more layers and wider MLP hidden dimensions are sparser as measured by the percentage
of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a
prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and
evaluation data, for Transformers of various configurations, at layers of all depth levels, as well as for other
architectures including MLP-mixers and 2-layer MLPs. We show that sparsity also emerges using training
datasets with random labels, or with random inputs, or with infinite amount of data, demonstrating that
sparsity is not a result of a specific family of datasets. We discuss how sparsity immediately implies a way
to significantly reduce the FLOP count and improve efficiency for Transformers. Moreover, we demonstrate
perhaps surprisingly that enforcing an even sparser activation via Top-k thresholding with a small value of k
brings a collection of desired but missing properties for Transformers, namely less sensitivity to noisy training
data, more robustness to input corruptions, and better calibration for their prediction confidence.
1
Introduction
The great success of modern machine learning for applications in computer vision, natural language processing,
game playing, and beyond is driven primarily by the computational model known as deep neural networks
(DNNs) [1]. With inspirations drawn from information processing in biological brains, DNNs are artificial
neural networks constructed from distributed computational nodes (a.k.a. neurons) with inter-connections
learned from data. Compared to shallow machine learning models, DNNs possess superior learning capacity
and hence can handle complex real-world tasks.
Although motivated from biological brains, there are differences at very fundamental levels on how artificial
and biological neural networks work. One of such differences is in the sparsity of computation. Evidence
from neuroscience suggests that neural activity in biological brains is sparse, namely, only a small percentage
of all neurons fire at each time [2, 3, 4, 5]. Sparse firing suggests that despite having billions of neurons,
only a small fraction of the brain participates in computation at each time. This may explain why brains can
sustain at a very low energy cost. In contrast, learning and inference with DNNs rely primarily on dense
computations where all neurons are involved for any input. In fact, modern computational hardware for deep
neural networks, such as GPUs and TPUs, are designed to facilitate massive scale dense computations. Even
with such dedicated hardware, DNNs are still notoriously resource-demanding to train and deploy. Aside
from computation efficiency, artificial neural networks also lag far behind biological ones in terms of robustness
to input perturbation, error correction for erroneous training labels, confidence calibration for the predictions,
etc.
∗Equal contribution
1
arXiv:2210.06313v2  [cs.LG]  9 Jun 2023

(a) T5 Encoder
(b) T5 Decoder
Figure 1: Percentage of nonzero entries (y-axis, log scale) in the activation map as a function of number of
training steps (x-axis) for a T5-Base model trained with the span corruption objective on the C4 dataset. Left:
layers (from shallow to deep) of the encoder. Right: layers of the decoder.
1.1
An Intriguing Observation: Activations are Sparse in Trained Transformers
This paper provides an extensive study on a surprising observation that despite performing dense computations,
DNNs produce very sparse activation in its intermediate layers once trained1. Specifically, we study Transformer
[6], a DNN model architecture that has become a workhorse for modern applications. Transformers are
constructed from interweaving a self-attention module and a multi-layer perceptrons (MLPs) of depth 2,
and the focus of this paper is on the activation map in the intermediate output of MLPs (after the activation
function). Figure 1 shows the sparsity of the activation maps from the training data, measured by the percentage
of nonzeros, in all MLP layers of a T5-Base model which is a Transformer based encoder-decoder model for
natural language processing [7]. We see that the percentage of nonzero entries is around 50% at initialization,
which is expected: randomly initialized weights produce roughly equal numbers of positive and negative
entries in the pre-activation map, resulting in ∼50 % non-zeros after the ReLU. However, at the end of training
the percentage of nonzero entries reduces drastically: the average value across all encoder-decoder layers
is 2.7% with the largest one being 12.0% and the smallest one being only 1.1%. The emergence of sparse
activation in Transformers bears a similarity to the sparsity of neural activities in biological brains, revealing
an interesting connection between artificial and biological neural networks. Moreover, unlike classical sparse
methods where such a connection is established via explicit sparse regularization [8], the sparsity observed in
Transformers is emergent without any explicit design.
It is worth noting that the observation that Transformers produce sparse activations is previously reported
in [9]. Our paper significantly extends upon results in [9] to demonstrate that sparsity emerges prevalently at
all layers of Transformers, for both language and vision tasks, on both training and evaluation data, and for
some architectures beyond Transformers. We also examine the activation of individual neurons, to show that
sparsity is not caused by “dead” neurons and that the percentage of activation has a long tail distribution. In
addition, by experiments with particularly designed datasets, as well as theoretical analysis of gradient at the
beginning of training, we show that the emergence of sparsity may be due to the training dynamics rather
than particular choices of datasets. Finally, our paper provides empirical evidence that sparsity is positively
correlated with model robustness and calibration.
1.2
Prevalence, Causes, and Benefits of Sparsity
This paper provides a study on the aforementioned phenomenon of sparse activation in trained Transformer
models, with a focus on answering the following three questions. First, is the phenomenon in Figure 1 a corner
case or is it occurring more broadly? Second, what are the causes for the emergence of sparsity? Third, why
should we care about the sparsity in DNNs, other than the appeal of its similarity to biological brains? Our
main results along these lines are summarized below.
1This implies, as we explain in details later, that a lot of the computations are spent in vain with multiplying a value by zero.
2

1. Sparsity is a prevalent phenomenon. We show in Section 2 that the emergence of sparsity in activation
maps of T5 as reported in Figure 1 is not an isolated and cherry-picked case. Rather, sparsity is prevalent,
and occurs broadly in Transformer models: it emerges in all layers of a Transformer, for Transformers trained
on both vision and natural language data, for Transformers of various configurations, and for activation
maps computed on both training and test data, etc. Moreover, through controlled experiments on the width
and depth of Transformers, we reveal that larger models are sparser, as measured by percentage of nonzero
entries. We also show in the Appendix B that sparsity emerges with many other architectures and with
different optimizers.
2. Sparsity comes from training dynamic? Towards understanding where sparsity comes from, one argument
is that commonly used image and natural language training datasets entail a compact representation due to
information in the labels or intrinsic low-dimensional structures of the natural data. Another hypothesis
is that sparsity has nothing to do with commonly used training datasets, and arises as a result of modern
over-parameterized models being able to fit the training data even when it is generated in random. In
Section 3, we design experiments using training datasets with random labels, or random images, or infinitely
amount of data, to show that none of the above fully explains the emergence of sparsity. Based on our
observations, we speculate that the sparsity may be attributed to the training dynamic in the optimization
process. In particular, we show theoretically with a simplified model architecture that the descending
direction of gradient at the beginning of training points to decreasing the value of activations.
3. Sparsity improves efficiency, robustness, and calibration. Sparsity of activation map in trained Trans-
formers implies that a large proportion of the computation during inference is spent on multiplying values
by zero. Hence, FLOPs can be drastically reduced by avoiding all such computations, which we discuss
in Section 4.1. Motivated by this observation, and to obtain reduced FLOPs not only after training but
throughout training, we introduce Top-k Transformer in Section 4.2, a simple modification of Transformers
where a Top-k thresholding is applied to the activation maps2. We show that Top-k Transformers with a
reasonable sized k has on par performance with vanilla Transformers. To demonstrate the computation
benefits of Top-k Transformers, we provide proof-of-concept results on wall time reduction for the task of
unbatched decoding on TPUv4 with a large Top-k T5. Meanwhile, we emphasise that this result is far from
fully realizing the benefit of sparse activation, due to a lack of hardware support for sparse computation.
While it is straightforward to associate sparsity with computational efficiency, it may be less obvious
and somewhat surprising that sparsity is associated with reliability of the models as well. We show in
Section 4.3 that enforcing explicit sparsity via Top-k Transformers improves model performance in terms of
less sensitivity to noisy training data, less sensitivity to input corruptions, and better confidence calibration.
1.3
Experimental Setup
We study the sparsity in activation maps of Transformers with two commonly used Transformer models,
namely Text-to-Text Transfer Transformer (i.e., T5) and Vision Transformer (i.e., ViT).
• T5 is an encoder-decoder model for natural language processing tasks [7]. We train T5 on the Colossal
Clean Crawled Corpus (C4) using the span corruption task as suggested by [7].
• ViT is an encoder model for vision tasks [12]. Unless specified otherwise, we train ViT on ImageNet-21k [13],
an image classification dataset with 14M images and 21k classes. For certain cases we also use ImageNet-1k
which is a subset of ImageNet-21k with 1.3M images and 1k classes.
Beyond T5 and ViT, we also present the results for BERT in the Appendix.
We measure the sparsity level at the intermediate output of the two-layer MLPs in a Transformer. Recall
that an MLP performs the following mapping
f(x; K, V ) .=
dff
X
i=1

σ(⟨ki, x⟩) · vi

, or equivalently, f(x; K, V ) .= V σ(K⊤x),
(1)
where x ∈IRdmodel is the input, K = [k1, . . . , kdff] ∈IRdmodel×dff and V = [v1, . . . , vdff] ∈IRdmodel×dff are learnable
layer parameters, and σ() is a nonlinear activation function. We use ReLU as the activation function σ() for
2The approach is previously adopted in ConvNets for improving model robustness [10], and more recently in [11] for improving
memory efficiency of Transformers.
3

(a) T5 vs ViT
(b) Train vs evaluation data
(c) Different training data size
(d) Varying configuration (ViT)
(e) Varying configuration (T5 Encoder)
(f) Varying configuration (T5 Decoder)
Figure 2: Percentage of nonzero entries across different layers of trained Transformers (a) for both language
data with T5 and vision data with ViT, (b) on both training and evaluation data, (c) for ViT trained on two
ImageNet of different scales (21k vs 1k classes), (d) on ViT of varying configurations, and (e, f) on T5 of
varying configurations. Please note that the y-axis is in log scale. Sparsity emerges in all cases.
both T5 and ViT3. A two-layer MLP may be regarded as having dff neurons where the i-th neuron performs the
computation σ(⟨ki, x⟩) · vi, and the final layer output is the sum of the output of all neurons. Each neuron is
called activated if σ(⟨ki, x⟩) is strictly positive. Hence, the sparsity of neuron activation can be measured by the
number of nonzero entries in the feature map
a .= σ(K⊤x),
(2)
which is a vector of dimension dff. Throughout the paper, the sparsity level is computed on the training set
unless otherwise specified.
Both T5 and ViT come with several configurations for dmodel, dff, number of layers, etc. Unless specified
otherwise, we will use the Base models (i.e., T5-Base and ViT-B/16) which have dmodel = 768, dff = 3072, and
12 layers (for ViT) and 12 encoder layers +12 decoder layers (for T5). Our experiment with T5 uses the T5X
codebase [14], and our experiment with ViT uses the Scenic codebase [15]. More training details of T5 and
ViT are provided in Appendix A.
2
Prevalence of Sparsity in Learned Transformers
This section shows thorough experiments on commonly used Transformers that sparsity in activation maps
is a prevalent phenomenon. We also show through some controlled experiments that deeper and wider
Transformers tend to be sparser measured by percentage of nonzero entries in activation maps.
3ViT originally uses GeLU as its activation function as in [12]. Here we switch to using ReLU as it allows us to more easily measure the
sparsity level using the number of nonzero entries with a very small performance drop (e.g., 47.78% with GeLU vs 47.58% with ReLU for
Top-1 evaluation accuracy on ImageNet-21k).
4

2.1
Sparsity is a Ubiquitous Phenomenon
We start by providing experimental evidence that the emergence of sparse activation in trained Transformers
is a ubiquitous phenomenon. To this end, we plot the percentage of nonzero entries of activation maps in
different Transformers, and present the results in Figure 2. Such results demonstrate the following.
• Sparsity emerges for both Vision and NLP tasks. Figure 2a shows the percentage of nonzero entries of trained T5
and ViT models evaluated on their respective training datasets. We see that both encoder and decoder of T5,
as well as the ViT, all exhibit sparsity.
• Sparsity emerges on both training and evaluation data. Figure 2b shows the percentage of nonzero entries in a
trained T5 evaluated on both the training data and the evaluation data. We see that the property of sparsity
generalizes very well to evaluation data as the curves for training and evaluation data align very closely
with each other.
• Sparsity emerges on datasets of varying scale. Figure 2c shows the percentage of nonzero entries in ViT trained
on both ImageNet-21k and ImageNet-1k, where the former is a superset of the later with approximately 10×
more images and 21× more classes. We see that the scale of data does not affect much of the sparsity level.
• Sparsity emerges on Transformers of varying configurations. Figure 2d shows the percentage of nonzero entries
for ViT of varying configurations in model size. Figure 2e and 2f show the percentage of nonzero entries for
encoder and decoder, respectively, of T5 with varying configurations in model size. We see that sparsity
persists for all cases.
• Sparsity emerges across all layers of a Transformer. Finally, all plots in Figure 2 show that sparsity emerges in
all layers of a Transformer. Moreover, in all cases the first few and last few layers tend to be denser than
intermediate layers.
Figure 3: Percentage of times that each
neuron in the first MLP layer of a
trained T5 is activated on C4 dataset.
The presence of sparsity in activation maps does not rule out the
possibility that a small percentage of the neurons are always activated
for all inputs, whereas the rest of the neurons are never activated. To
illustrate that this is not the case, we experiment with a pretrained
T5 base model4 to plot the percentage of layer inputs for which each
of the dff neurons is activated when evaluated on 800 examples taken
from C4 dataset with span corruption task. Note that there are 800
* 512 = 409600 samples as MLP activation is computed per token. The
results are presented in Figure 3 with x-axis being indices of neurons
in the first encoder layer of T5 sorted in descending order according
to percentage of layer inputs on which they are activated. It can be
seen that while a few neurons are activated for around 50% of the
time, the vast majority of neurons (around 93.5%) are activated less
than 10% of the time. Moreover, there are no dead neurons that are
never activated, and the least activated neuron is activated for around 0.001% of the time, and 99% of neurons
are activated over 1% of the time. Finally, while the results here are for neurons in the first MLP layer of a
pretrained T5 base encoder, all other MLP layers show qualitatively similar behavior.
2.2
The Larger, the Sparser
We next examine the effect of model size on the sparsity level of activation maps. Note that Figure 2e and
Figure 2f provide evidence with T5 of varying configuration that larger models tend to be sparser. Here we
perform controlled experiments to examine the effect of model depth, measured by the number of Transformer
layers, as well as the effect of model width, measured by the dimension of activation map of MLPs (i.e., dff),
separately. Towards that, we take a standard T5 model and vary the depth and width, respectively while
keeping the rest of the configuration fixed, and examine their sparsity level after training. The results are
presented in Figure 4 for the encoder, whereas we omit the results for the decoder as they are qualitatively the
same as those for encoder.
It can be seen from Figure 4a that deeper Transformers are arguably sparser. For example, many of the
middle layers of the 32-layer model have less than 1% nonzero entries while all shallower models have more
4https://github.com/google-research/t5x/blob/main/docs/models.md#t5-checkpoints
5

(a) Sparsity vs. depth
(b) Sparsity (percentage) vs. width
(c) Sparsity (count) vs. width
Figure 4: Activation sparsity across different encoder layers of trained T5 Transformers of (a) varying depth
and (b, c) varying width (i.e., dff). Since with varying width the dimension of activation maps also changes,
we evaluate sparsity both in term of the percentage (as in (b)) and the count (as in (c)) of nonzeros. Deeper
and wider models are sparser in terms of percentage of activated neurons.
(a) Random label
(b) Random image
(c) Infinite data
Figure 5: Percentage of nonzero entries in ViT trained on ImageNet-21k (IM-21K) with (a) random labels where
p% labels are replaced by labels drawn from a uniform distribution with p ∈{50%, 70%, 100%}, (b) random
images where each image is replaced by one where the pixels are drawn from i.i.d. uniform distribution in
[−1, 1], and (c) infinite data where sufficient training data is generated by drawing random image and random
label pairs so that the model is never trained on the same pair twice.
than 1% nonzero entries across all layers. For comparing networks of different widths, we measure the sparsity
with the percentage and the count of nonzero entries in Figure 4b and Figure 4c, respectively. It can be seen
that wider models have a lower percentage of nonzero entries, though a higher count of nonzero entries.
3
Sparsity from Training Dynamic?
In this section we study the causes of sparsity in activation maps of trained Transformers. Towards that, in
Section 3.1, 3.2, and 3.3 , we present a set of hypotheses and design corresponding experiments to validate or
disprove them. We discuss the observation from the experiments and draw a tentative conclusion in Section 3.4
on attributing sparsity to the training dynamic, with theoretical evidence.
3.1
Sparsity from Labels?
Transformers are usually trained via supervised learning (e.g., using the ImageNet dataset for ViT) or self-
supervised learning (e.g., using the span corruption task for T5). In both cases, the training labels provide a
pertinent and meaningful description of the corresponding training data (e.g., image for ViT and text for T5).
Sparsity may arise because the label set provides a structured organization of the massive training data, hence
the training dataset admits a compact representation. This motivates us to formulate the following hypothesis.
6

Hypothesis 3.1 (Sparsity from labels). Sparsity in trained Transformers arises from the labels for Transformer training,
e.g., human annotations in supervised training or generated labels from data itself in self-supervised learning.
We use a random label experiment with ViT for image classification to test Hypothesis 3.1. Specifically, we
generate a new training dataset by replacing p% of the labels in the ImageNet-21k dataset with random labels
drawn uniformly at random from the set of all possible labels, where p is varied to examine the effects. With
such a dataset, the labels for a certain percentage of images do not provide a meaningful description for the
content of the image. Hence, if Hypothesis 3.1 is valid, then the activation map will become dense.
The sparsity level of ViT trained on the random label datasets is shown in Figure 5a. It can be seen that the
percentage of activated neurons decreases with an increasing percentage of label noise up to 70%. An even
higher label noise level at 100% changes the sparsity level across layers as the shallow layers (i.e., layers 0 - 4)
becomes sparser, while the deep layers (i.e., layers 5 - 11) becomes denser. Nonetheless, even with 100% label
noise, all layers have < 10% activated neurons.
3.2
Sparsity from Data?
While modern image and text data are often of high-dimensional, their intrinsic degree of freedom is much
smaller, i.e., they are low-dimensional and admit compact representations [16, 17]. Hence, even if the labels
do not provide meaningful descriptions of the data, it may still be possible that Transformers extract low-
dimensional structures from data and produce compact representations in the form of sparse activation maps.
This motivates the following hypothesis.
Hypothesis 3.2 (Sparsity from natural data). Sparsity in trained Transformers arises from natural training data
(e.g., images for ViT and texts for T5).
We use a random image experiment to test Hypothesis 3.2. With the ImageNet-21k dataset, we replace each
image with a random image generated by drawing pixel values from an i.i.d. Uniform distribution in the range
of [0, 255], and use these images (instead of the original images in ImageNet-21k) for model training. Such
random images do not contain any low-dimensional structures nor compact representations.
The percentage of nonzero entries of a ViT trained on random image dataset is shown in Figure 5b. It can
be seen that the first four layers become sparser while the last few layers become relatively denser compared to
training with natural images in ImageNet-21k. Nonetheless, all layers have < 10% activated neurons.
3.3
Sparsity from Data-fitting?
Modern deep neural networks are often over-parameterized, with sufficient capacity to fit practical training
datasets and obtain close-to-zero training error. There is evidence suggesting that this result holds true even if
the data and label are generated in random [18]. Hence, there is the possibility that sparsity arises because the
training data, even if generated in random, is scarce relative to the scale of modern over-paremeterized models.
Hypothesis 3.3 (Sparsity from data-fitting). Sparsity in trained Transformers arises from the fact that models have
more than sufficient capacity to fit training data of practical scale.
To test Hypothesis 3.3, we design an infinite data experiment where the amount of training data is infinitely
large so that any practical Transformer becomes under-parameterized relative to the data and cannot fit the
data. The way we generate infinite training data is to sample images with random pixels as in the random
image experiment, and for each image we sample a random label as in the random label experiment. Moreover,
we generate sufficient amount of such training data to make sure that the model never sees the same data point
twice during the training. The number of training iterations in the infinite data experiment is kept the same as
that of the random image and random label experiments.
The result of this experiment is presented in Figure 5c. It can be seen that the first four layers produce
sparser activation maps, while middle layers with index 4 - 7 are considerably denser compared to the baseline
with near 10% to 20% nonzero entries.
7

3.4
Discussion: Sparsity from Training Dynamic?
The results of random label, random image, and infinite data experiments in Figure 5 show that labels, data,
and data-fitting as conjectured in Hypothesis 3.1, 3.2, and 3.3, respectively, all affect the sparsity level of the
activation map. Nonetheless, none of them fully explains the emergence of sparsity since for all results in
Figure 5, the percentage of nonzero entries is considerably smaller than at the initialization (i.e., 50%).
Our results point to the possibility that sparsity comes from the training dynamic. Namely, at early training
stage with any training data and a random initialization of network parameters, the descending direction
of the gradient on the Transformer parameters tends to point to a regime where their MLPs produce sparse
activation maps. In the following, we provide theoretical evidence for this argument by looking at the gradient
on the positive activation maps for a DNN with last two layers being a ReLU followed by a fully connected
layer. In particular, we have the follow result.
Theorem 3.1. Let f(x; V , θ) : IRn →IRK be a neural network given by
f(x) = V σ
 p(x; θ)

,
(3)
where V = [v1, . . . , vdff] ∈IRK×dff is network parameter for the last layer drawn from a random distribution, σ() is the
ReLU activation function, and p(x; θ) denotes all other layers with parameter θ. We write p = p(x; θ) for simplicity.
• Consider the mean squared error (MSE) loss ℓMSE(f(x), y) .=
1
2∥f(x) −y∥2
2, where y is an arbitrary vector
independent of V . Assume that V satisfies
E [V ] = 0, and E [⟨vi, vj⟩]
(
= 0,
if i ̸= j,
> 0,
otherwise5.
(4)
If there exist an i∗such that pi∗> 0, then we have
E
∂ℓMSE(f(x), y)
∂pi∗

> 0,
(5)
where the expectation is taken with respect to randomness in V .
• Consider the cross-entropy (CE) loss ℓCE(f(x), y) = −⟨y, log
exp(f(x))
⟨exp(f(x)),1⟩⟩, where y is an arbitrary vector that
sums up to one and independent of V . Assume that the entries of V are drawn from independent distributions, the
probability of any entry of V being 0 is less than 1, and E [V ] = 0. If there exist an i∗such that pi∗> 0, then we have
E
∂ℓCE(f(x), y)
∂pi∗

> 0,
(6)
where the expectation is taken with respect to randomness in V .
The proof of Theorem 3.1 is provided in Appendix D. Theorem 3.1 states that the gradient of either the MSE
or CE loss with respect to any positive activation pi∗is positive in expectation. Hence, any training algorithm
based on negative gradient directions tends to reduce the magnitude of such positive activations, which will
lead to a smaller training loss. Here, the expectation is taken with respect to the randomness in the last layer
parameter V . Hence, our result can be considered as an analysis for DNNs at initialization where weights are
often chosen randomly from a fixed distribution. In particular, the required properties for the distribution of
V in Theorem 3.1 for both MSE and CE losses are satisfied by commonly used initialization methods, such as
the one in [19]. On the other hand, Theorem 3.1 does not apply to subsequent training iterations since the
label y is no longer independent of V . However, it can be seen empirically from Figure 1 that the trend of a
decreasing percentage of nonzero entries persists for a certain number of iterations during the beginning of
training until such a percentage reaches a low level and stays relatively stable until the end of training.
5This requirement is generally satisfied unless the probability of vi = 0 is 1.
8

4
Efficient, Robust, and Calibrated: Sparsity is All You Need?
In this section we show that activation sparsity provides several practical benefits. In Section 4.1 we discuss how
the free sparsity in trained Transformers brings us free computation efficiency in terms of FLOPs count during
inference. In order to obtain sparsity hence FLOPs reduction throughout training, in Section 4.2 we introduce
Top-k Transformers, a simple modification of Transformers where a top-k thresholding operation is applied to
the activation maps in all MLPs. While existing hardware cannot well support sparse computation and fully
realize the benefit of FLOPs reduction, we provide a proof-of-concept experiment on preliminary benefits
of Top-k Transformer. Finally, in Section 4.3 we show that sparsity in activation is a good regularization for
Transformers. Namely, enforcing sparser activation with smaller values of k in Top-k Transformer (without
any other hacks, tweaks and hyperparameter tuning) bestows Transformers several desired properties, namely,
robustness of training with erroneous annotations, less sensitivity to input noise/perturbation, and better
confidence calibration of the predictions.
4.1
Efficiency for Free
Given an embedding dimension dmodel and an MLP intermediate dimension dff, the computational complexity
of a Transformer for an input sequence of length N is O(Nd2
model + N 2dmodel + Ndmodeldff), where the first
term comes from computing the key, query, and value matrices, the second term comes from computing the
self-attention matrix, and the third term comes from the MLP. For a fixed sequence length N, and considering
the fact that dff is often much larger than dmodel, it is arguable that MLP poses the computational bottleneck in
large Transformers. In the following, we explain how sparsity in activation map of MLP can be leveraged to
significantly reduce its computational cost, without affecting the model performance.
Efficiency for the Second MLP Layer. The sparse activation immediately suggests that a lot of the computation
for inference with Transformers is not needed at all. That is, while doing dense matrix-matrix multiplications,
much of it is about multiplying a vector by a value of zero, which can be avoided to save computation.
Specifically, we consider the second layer of the MLP in (1) which performs the computation
V a,
(7)
where a ∈IRdff is the intermediate activation map of MLP (see (2)) and V ∈IRdmodel×dff is the layer parameter.
Eq. (7) involves a simple matrix-vector multiplication which has a FLOP count of 2dmodel × dff. However, if a
is sparse with, say s nonzero entries, then the FLOP count for (7) reduces to 2dmodel × s. Hence,
FLOP in the second MLP layer is reduced by a factor of 1 −s
dff .
Note that
s
dff is exactly the percentage of nonzeros plotted in the y-axis of e.g. Figure 1, which is 2.7% averaged
across all layers. Hence, the computational cost of the second MLP layer can be reduced by a significant
amount. More excitingly, the reduction factor 1 −s
dff is likely to be even bigger for larger Transformer models
(see Figures 4a and 4b), pointing to a greater reduction in computation.
Efficiency for the First MLP Layer. The sparsity in the intermediate activation map of MLP does not immedi-
ately suggest a reduction in computation for the first MLP layer. Nonetheless, it is possible to significantly
reduce the computation in the first MLP layer by leveraging approximate nearest neighbor search, which we
explain next.
Recall from (1) that the computation in the first MLP layer is given by
σ(K⊤x),
(8)
with K = [k1, . . . , kdff] ∈IRdmodel×dff being the layer parameter and x being the layer input. If the output is
sparse with k nonzero entries, then the calculation in (8) may be formulated as finding k points from the set
{ki}dff
i=1 that are “closest” to the input x measured by values of inner product. Such a problem is well-known
as the nearest neighbor search (NNS) problem or the maximum inner product search problem. While naive
solving of the NNS problem has linear complexity in dff, there exists approximate algorithms [20, 21, 22, 23]
that are of sublinear complexity, and using them in Transformers means that
FLOP in the first MLP layer may be reduced to have sublinear complexity in dff.
There are of course the questions of whether such approximate NNS algorithms hurt Transformer performance
or not, which we leave for future study.
9

(a) T5
(b) ViT
Figure 6: Training and evaluation accuracy of Top-k T5 for three different sizes: base, large and 3B (left) and
Top-k ViT (right) with varying k. Top-k Transformer is on par with regular Transformer for a large enough k. E.g. for
T5 3B with k = 128, and ViT with k = 256, the drop is around 0.3%.
4.2
Sparsity in Training via Top-k Transformers
The benefits in terms of efficiency discussed in Section 4.1 comes with caveats. First, while the activation maps
are sparse on average, there is the possibility that some of the activation maps for certain inputs are denser
hence cannot benefit from sparse computation. Second, sparsity occurs only in trained Transformers while the
computation is dense during and particularly at the beginning of training.
Here we present Top-k Transformer, a simple modification to Transformer architecture that allows us to
control sparsity level for all model inputs, and throughout training. Top-k Transformer is built upon a regular
Transformer with the only modification being the MLP layers, where at the output of the activation function
σ() (see (1)) we add a Top-k thresholding operator. That is, the MLPs of Top-k Transformers perform the
following computation
f(x; K, V ) = V · Topk

σ(KT x)

,
(9)
where Topk(·) performs a thresholding that all entries other than those of the largest k values are set to zero
with k being a hyper-parameter subject to design choices. Note that Top-k Transformer reduces to a regular
Transformer if we set k = dff. By using a small value of k, the benefit of efficiency in terms of reduction in
FLOP as discussed in Section 4.1 applies to Transformer training as well.
Figure 7: Latency reduction for un-
batched greedy decoding in decoder
of Top-k Transformers on TPUv4.
The immediate question for Top-k Transformer is whether it of-
fers training sparsity at the cost of a reduced performance. Here we
conduct experiments with Top-k T5 and Top-k ViT, and evaluate their
performance measured by prediction accuracy for C4 span corruption
and ImageNet-21k classification tasks, respectively. The results are
provided in Figure 6. We see that with the Top-k T5 (resp., Top-k
ViT) Transformer, taking k to be 64 (resp., 256) is sufficient for closely
matching the test performance of the vanilla T5 (resp., ViT). Note that
this is achieved without any other hyper-parameter tuning for the
Top-k Transformers upon those used for a regular Transformer, and
it is possible that other hyper-parameter choices may further improve
the performance of Top-k Transformers.
We now provide experimental results with Top-k Transformers on
wall-time benefits from FLOPs reduction discussed in Section 4.1. In
particular, we evaluate the inference time latency reduction of Top-k
Transformer. In our experiment, we add a Top-k thresholding to T5X [14]6. We gain efficiency in the second
MLP layer by an implementation that avoids all multiplication by zero as described in Section 4.1. The decoder
per-token wall time for unbatched greedy decoding during inference on a single TPUv4 chip is presented in
Figure 7. We observe that larger models have more wall time reduction, due to the fact that they have larger
6We use the implementation of jax.lax.approx_max_k [23] with a recall target of 0.95.
10

Table 1: Evaluation of Top-128 ViT for ImageNet-1k classification in terms of 1) natural accuracy with ImageNet-
1k evaluation set, 2) robust accuracy with {40%, 80%} corrupted training labels, 3) robust accuracy under
input perturbation with additive {Gaussian, Impulse, Shot} noise on evaluation images, and 4) calibration
error on evaluation data measured by ECE. Top-128 ViT is on par with ViT for natural accuracy while is significantly
better for model robustness and calibration.
Methods
Natural
Accuracy
Accuracy w/
Train Label Noise
Accuracy under
Input Perturbation
Expected Calibration
Error (ECE)
40%
80%
Gaussian
Impulse
Shot
ViT
74.85%
59.44%
25.35%
39.54%
37.37%
38.56%
8.42%
Top-128 ViT
74.83%
62.13%
30.80%
42.29%
40.07%
40.68%
7.48%
dff hence more FLOPs reduction. In particular, for T5-11B we observe around 10% wall time reduction with
k ≤128, though this amount becomes smaller with a larger k = 256.
Finally, we emphasize that the sparsity in Top-k Transformers is unstructured and data-dependent, which is
not well supported on existing computation hardwares such as TPUs and GPUs. Hence, the results in Figure 7
are for proof-of-concept purposes, and is far from fully realizing the benefit of sparsity. We leave a study of
better implementation of sparse computation for obtaining wall time reduction to future work.
4.3
Bonus! Improved Robustness and Calibration
Despite not being explicitly designed for such purposes, inducing sparse activation via Top-k Transformer
has the benefits of improving model robustness7 and confidence calibration. We demonstrate this using the
image classification task with the ImageNet-1k dataset, and present a snapshot of key results in Table 1. All
results for Top-k ViT are obtained without any model and training hyper-parameter tuning upon those for ViT.
Contexts, details, and full results are presented below.
Robustness to Label Noise. An important challenge for DNNs is that they are highly susceptible to label
noise, the problem where a certain percentage of training labels are corrupted or erroneously generated. This
may be attributed to the fact that DNNs are often over-parameterized, hence too “capable” that they tend to
overfit, or “memorize” the noisy labels without generalizing to test data. While many dedicated techniques
exist (see e.g., [24, 25] for a review), here we show that a simple Top-k Transformer with a small value of k can
effectively address the label noise issue.
Figure 8: Training curves of Top-128 ViT under
80% label noise for ImageNet-1k classification.
We conduct experiments using the ImageNet-1k dataset
for which we replace p% of the labels in the training set with
a random label drawn uniformly from the set of all possible
labels. In Figure 8 we present the training and evaluation
accuracy curves for p = 80%. It can be seen that the vanilla
ViT starts to overfit at around 45,000 steps where the training
accuracy continues to increase but the evaluation accuracy
starts to drop. In contrast, the evaluation performance of Top-
128 ViT continues to improve until the end of training, which
leads to a better final performance. The final evaluation per-
formance under p ∈{40%, 80%} label noise is presented in
Table 1. It shows that Top-k offers a consistent performance
gain with label noise.
Confidence Calibration. Aside from the label noise issue,
another symptom of over-parameterization of DNNs is that they tend to be overly confident in their predictions.
In the context of classification problems, they tend to assign a high (i.e., close to 1) probability to the class of its
prediction, while it is more desirable that they produce a probability that is commensurate with its confidence
level [26]. A commonly used metric for confidence calibration is the expected calibration error (ECE) [27],
7This is previously demonstrated in [10] for ConvNets.
11

(a) Gaussian Noise
(b) Impulse Noise
(c) Shot Noise
Figure 10: Performance of Top-k ViT on corrupted ImageNet-1k test data with Gaussian noise (left), impulse
noise (middle), and shot noise (right), each under five severity levels. Top-k improves robustness for all noise
types and on all corruption levels with a suitable choice of k.
which is the discrepancy between the probability to the class of a model’s prediction and the probability that
its prediction is actually correct.
Figure 9: Confidence calibration of Top-k ViT
for ImageNet-1k classification.
Here we measure the calibration of Top-k ViT via ECE and
report the results in Figure 9. At the beginning of training
the model has a low ECE because the output probabilities
are mostly uniformly distributed across all classes hence the
model is not confident, and that its prediction is purely ran-
dom hence wrong with high probability. The model tends
to become overly confident as the training progresses, hence
the ECE increases particularly towards the end of training.
What we can observe is that Top-k enables the Transformer to
be more calibrated when compared to a vanilla Transformer,
particularly for small values of k. The results with k = 128
and its comparison with the vanilla ViT is also presented in
Table 1.
Robustness to Input Perturbation. Another important chal-
lenge with DNNs is that their outputs tend to be sensitive to
naturally occurring image corruptions, which limits their application to mission critical tasks [28]. Here we
evaluate the robustness of Top-k ViT to three types of additive noises, namely Gaussian noise, impulse noise,
and shot noise. For that purpose, we train Top-k ViT on standard ImageNet-1k training data and report their
classification accuracy on ImageNet-C [29], a benchmark that contains algorithmically generated Gaussian,
impulse, and shot noise (among many others types) applied to the ImageNet-1k test dataset. For each noise
type, there are five severity levels for which the results are all presented in Figure 10. We can see that Top-k
ViT offers a performance gain over the vanilla ViT, particular with k = 64 or k = 128 and the severity level is
high. We also report the averaged performance over all severity levels of each corruption type in Table 1.
5
Related Work
Prior efforts on introducing sparsity in deep neural networks abound, though often with diverse motivations
and objectives. Here we provide a brief overview of several popular lines of work.
Sparsity for Efficiency. Due to the high computational cost of dense matrix operations in standard DNNs,
a plethora of work has explored sparsity in either model weights or activation maps for improving training
and inference efficiency (see e.g. [30] for a review). For activation sparsity in particular, sparsity for efficiency
is explored perhaps first in fully connected and convolutional neural networks [31, 32, 33, 34, 35, 36] before
subsequently becomes a key design component in many of the largest Transformer based language and vision
models [37, 38, 39, 40, 41]. The Top-k thresholding that we use in Top-k Transformer has also been previously
used in [11] to improve memory efficiency of Transformers. However, it has been unclear a priori whether
12

sparsity hurts model performance, hence the practice often relies on wishful design, trial-and-error, and
post-hot justification [42]. Our discovery that Transformers naturally produce sparse activation maps, and
that larger models are even sparser, may provide principled perspectives towards efficiently training future
large models.
Sparsity for Robustness. Many works find that smaller networks obtained by model compression are more
robust to adversarial perturbation [43, 44, 45] and label noise [46]. Another line of work that uses sparsity for
robustness leverages the property that practical data corruption is often sparse [47, 48, 49]. None of the work
mentioned above is based on sparsity in activation maps. More closely related to ours is the work [10] where
sparsity in activation map of convolutional DNNs is shown to improve robustness to input perturbation, and
the work [50] that leverages sparse activation to derive robust generalization error bounds.
Sparsity for Explainability. Work on leveraging sparsity for interpreting deep learning models long exist but
often in a post-hoc fashion for examining the semantic meanings encoded by a neuron of a trained model
[51, 52]. For Transformers, evidence suggests that the learned knowledge is encoded mainly in its MLPs with
individual neurons expressing specific factual knowledge [53]. Moreover, enforcing neuron activation sparsity
in MLPs helps to improve the percentage of neurons that are interpretable [54]. Hence, our discovery may
point to new directions towards developing more interpretable DNNs [55, 56].
Sparsity for Data Modeling. Following the seminal work [8] on the emergence of V1 receptive fields in
primary visual cortex from sparse encoding of natural images, there are a lot of interests in sparsity as an
effective modeling of natural signals [57]. With the popularity of deep learning techniques and the close
resemblance of the computational structure of ReLU networks and sparse encoding algorithms [58], it became
natural to study a DNN as a multi-layer sparse modeling of the data [59]. Along with substantial theoretical
understanding of such a modeling are obtained [60, 61], there are also experimental results on their practical
benefits [62, 63] though less often on modern large-scale data. Our discovery on the sparsity of Transformers
offers evidence in favor of the perspective that deep neural networks perform sparse modeling of data, and
may motivate the design of more practical sparse modeling in the future.
Sparsity for Theory of Over-parameterized Models. Because of its simplicity and well-develped theory in
classical machine learning [64, 16, 17], sparse modeling is often used to provide theoretical understanding of
modern large and over-parameterized models. This include works on implicit regularization [65, 66, 67, 68, 69],
nonconvex optimization [70, 71], noise interpolators [72, 73, 74], parallel networks [75, 76], etc. However, the
aforementioned work uses sparsity as a testbed or toy model to gain insights, without implication of existence
of sparsity in DNNs. Exceptions include concurrent work [77, 78] which provide theoretical analysis for when
sparse activation emerges. Finally, related work also include those on neural collapse [79, 80, 81, 82, 83, 84, 85, 86],
showing that the last layer features in a trained classification neural network are approximately 1-sparse after
an appropriate rotation.
6
Discussion: Transformers are Parsimonious Models?
As the scale of deep learning models continues to grow, it may have been taken for granted that increasing model
size is necessary and possibly even sufficient for obtaining ever-improving performance. However, historically
the futuristic picture for large scale DNNs had not been entirely optimistic. For example, Geman et al. in 1992
[87] made an argument based on the notion of bias-variance trade-off that despite their excellent goodness-of-fit
to training data, DNNs may suffer from a high prediction variance hence poor generalization performance.
While recent evidence suggests that DNNs exhibit an unexpected unimodal shaped variance curve [88] where
variance is controlled in the over-parameterized regime by an implicit algorithmic regularization, it is largely
unclear whether such a regularization is pertinent and accounts for the good generalization of practical DNNs.
The emergence of sparse activation in Transformer models discussed in Section 2 (see also Appendix B)
may offer an explanation for why DNNs work well and do not overfit. The notion of sparsity pertains to the
law of parsimony, a.k.a. Occam’s razor, where among all possible explanations of observed data, the simplest
ones are preferred. As a fundamental scientific principle, the law of parsimony is broadly used in various
scientific and engineering subjects [89, 90], including classical machine learning [91]. However, it is often not a
critical design component in DNNs and goes against the recent deep learning practice of training increasingly
flexible, redundant, and powerful models. Hence, our discovery may be rather surprising. Even not explicitly
13

designed so, Transformers only use a small fraction of its parameters to parse a given input, hence may be
regarded as parsimonious models as well. As discussed in Section 3.4, sparsity may arise from the dynamic of
neural network training rather than any explicit sparsity regularizer. More importantly, evidence of improved
robustness and calibration in Section 4.3 indicates that sparsity is a pertinent prior for its good generalization.
Outlook. The emergence of sparsity and its many practical benefits point to sparsity and the law of parsimony
as a fundamental component of more powerful models in the future8. Along that line, our work is merely a
starting point and may have brought more challenges than it has solved. While enforcing sparsity via Top-k
thresholding demonstrates benefits, it is used as a proof-of-concept due to its simplicity and is not meant to
be the best way of inducing sparsity. The hope is that our results may motivate future study of introducing
sparsity in DNNs in a more principled way. Moreover, while sparsity readily implies a drastically reduced
computational cost in terms of FLOP count, the benefit may not be reflected by wall time since existing deep
learning platforms are often geared towards efficient dense computation. Hence, our result may motivate the
design of future platforms that excel at sparse computation. Finally, while our motivation of studying sparse
activation in Transformers comes (partly) from study of biological brains, establishing such a connection may
reciprocally benefits efforts on applying artificial intelligence to the study of biology and neuroscience [97].
Acknowledgments
We would like to acknowledge helpful discussions with René Vidal and Jeremias Sulam from Johns Hopkins
University, with Weijie Su from UPenn, with Yuxiang Wang from UC Santa Barbara, with Atlas Wang from UT
Austin, with Nishanth Dikkala, Nikhil Vyas, Preston McAfee and Mukund Sundararajan from Google, with
Subutai Ahmad from Numenta, with Wei Hu, Salar Fattahi, and Jianhao Ma from University of Michigan, with
Tuo Zhao from Georgia Tech. We particularly thank Donhauser Konstantin from ETH Zurich for interesting
discussion on hypothesis for emergence of sparsity.
References
[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.
[2] Jason ND Kerr, David Greenberg, and Fritjof Helmchen. Imaging input and output of neocortical networks in vivo.
Proceedings of the National Academy of Sciences, 102(39):14063–14068, 2005.
[3] Cindy Poo and Jeffry S Isaacson. Odor representations in olfactory cortex:“sparse” coding, global inhibition, and
oscillations. Neuron, 62(6):850–861, 2009.
[4] Alison L Barth and James FA Poulet. Experimental evidence for sparse firing in the neocortex. Trends in neurosciences,
35(6):345–355, 2012.
[5] Mohsin S Ahmed, James B Priestley, Angel Castro, Fabio Stefanini, Ana Sofia Solis Canales, Elizabeth M Balough,
Erin Lavoie, Luca Mazzucato, Stefano Fusi, and Attila Losonczy. Hippocampal network reorganization underlies the
formation of a temporal association memory. Neuron, 107(2):283–291, 2020.
[6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and
Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
[7] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,
Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res.,
21(140):1–67, 2020.
[8] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code
for natural images. Nature, 381(6583):607–609, 1996.
[9] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Moefication: Transformer feed-
forward layers are mixtures of experts. In Findings of the Association for Computational Linguistics: ACL 2022, pages
877–890, 2022.
8This view resonates with recent work on the pathway to artificial intelligence [92, 93, 94, 95, 96].
14

[10] Subutai Ahmad and Luiz Scheinkman. How can we be so dense? the benefits of using highly sparse representations.
arXiv preprint arXiv:1903.11257, 2019.
[11] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via
top-k attention. In Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing, pages 39–52,
2021.
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An
image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning
Representations, 2021.
[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.
[14] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan
Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc
van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha
Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H.
Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma,
Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander
Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling up models and data with t5x and seqio. arXiv preprint
arXiv:2203.17189, 2022.
[15] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A jax library for
computer vision research and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 21393–21398, 2022.
[16] Rene Vidal, Yi Ma, and Shankar Sastry. Generalized principal component analysis. Interdisciplinary Applied Mathe-
matics, 43:22–23, 2015.
[17] John Wright and Yi Ma. High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and
Applications. Cambridge University Press, 2022.
[18] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
(still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level
performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages
1026–1034, 2015.
[20] Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search
(mips). Advances in neural information processing systems, 27, 2014.
[21] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big
Data, 7(3):535–547, 2019.
[22] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating
large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages
3887–3896. PMLR, 2020.
[23] Felix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo, David Majnemer, and Sanjiv Kumar. Tpu-knn: K nearest
neighbor search at peak flop/s. arXiv preprint arXiv:2206.14286, 2022.
[24] Görkem Algan and Ilkay Ulusoy. Image classification with deep learning in the presence of noisy labels: A survey.
Knowledge-Based Systems, 215:106771, 2021.
[25] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep
neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.
[26] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International
conference on machine learning, pages 1321–1330. PMLR, 2017.
[27] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using
bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
15

[28] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and Andreas Veit. Under-
standing robustness of transformers for image classification. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 10231–10241, 2021.
[29] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and
perturbations. In International Conference on Learning Representations, 2019.
[30] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning
and growth for efficient inference and training in neural networks. J. Mach. Learn. Res., 22(241):1–124, 2021.
[31] Andrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation in deep neural
networks. arXiv preprint arXiv:1312.4461, 2013.
[32] Minsoo Rhu, Mike O’Connor, Niladrish Chatterjee, Jeff Pool, Youngeun Kwon, and Stephen W Keckler. Compressing
dma engine: Leveraging activation sparsity for training deep neural networks. In 2018 IEEE International Symposium
on High Performance Computer Architecture (HPCA), pages 78–91. IEEE, 2018.
[33] Georgios Georgiadis. Accelerating convolutional neural networks via activation map compression. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7085–7095, 2019.
[34] Shijie Cao, Lingxiao Ma, Wencong Xiao, Chen Zhang, Yunxin Liu, Lintao Zhang, Lanshun Nie, and Zhi Yang.
Seernet: Predicting convolutional neural network feature-map sparsity through low-bit quantization. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11216–11225, 2019.
[35] Beidi Chen, Tharun Medini, James Farwell, Charlie Tai, Anshumali Shrivastava, et al. Slide: In defense of smart
algorithms over hardware acceleration for large-scale deep learning systems. Proceedings of Machine Learning and
Systems, 2:291–306, 2020.
[36] Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson, Sage
Moore, Nir Shavit, and Dan Alistarh. Inducing and exploiting activation sparsity for fast inference on deep neural
networks. In International Conference on Machine Learning, pages 5533–5543. PMLR, 2020.
[37] Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser, Wojciech Gajewski, Henryk
Michalewski, and Jonni Kanerva. Sparse is enough in scaling transformers. Advances in Neural Information Processing
Systems, 34:9895–9907, 2021.
[38] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with
simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022.
[39] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi
Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In
International Conference on Machine Learning, pages 5547–5569. PMLR, 2022.
[40] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan,
Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power
next-generation ai scale. arXiv preprint arXiv:2201.05596, 2022.
[41] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint
arXiv:2209.01667, 2022.
[42] Cenk Baykal, Nishanth Dikkala, Rina Panigrahy, Cyrus Rashtchian, and Xin Wang. A theoretical view on sparsely
activated networks. arXiv preprint arXiv:2208.04461, 2022.
[43] Yiwen Guo, Chao Zhang, Changshui Zhang, and Yurong Chen. Sparse dnns with improved adversarial robustness.
Advances in neural information processing systems, 31, 2018.
[44] Artur Jordao and Hélio Pedrini. On the effect of pruning on adversarial robustness. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 1–11, 2021.
[45] Tianlong Chen, Zhenyu Zhang, Santosh Balachandra, Haoyu Ma, Zehao Wang, Zhangyang Wang, et al. Sparsity
winning twice: Better robust generalization from more efficient training. In International Conference on Learning
Representations, 2022.
[46] Yihao Xue, Kyle Whitecross, and Baharan Mirzasoleiman. Superior generalization of smaller models in the presence
of significant label noise. arXiv preprint arXiv:2208.08003, 2022.
16

[47] Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep neural networks.
In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, pages 1919–1925, 2017.
[48] Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepant learning rates for
double over-parameterization. Advances in Neural Information Processing Systems, 33:17733–17744, 2020.
[49] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-parameterization. In
International Conference on Machine Learning, 2022.
[50] Ramchandran Muthukumar and Jeremias Sulam. Adversarial robustness of sparse local lipschitz predictors. arXiv
preprint arXiv:2202.13216, 2022.
[51] Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, and James Glass. What is one grain
of sand in the desert? analyzing individual neurons in deep nlp models. In Proceedings of the AAAI Conference on
Artificial Intelligence, pages 6309–6317, 2019.
[52] Thomas McGrath, Andrei Kapishnikov, Nenad Tomasev, Adam Pearce, Demis Hassabis, Been Kim, Ulrich Paquet,
and Vladimir Kramnik. Acquisition of chess knowledge in alphazero. arXiv preprint arXiv:2111.09259, 2021.
[53] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained
transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 8493–8502, 2022.
[54] Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer ElShowk,
Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Jones, , Dawn
Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly,
Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom
Brown, Sam McCandlish, Dario Amodei, and Christopher Olah. Softmax linear units. Transformer Circuits Thread,
2022. https://transformer-circuits.pub/2022/solu/index.html.
[55] Xavier Suau Cuadros, Luca Zappella, and Nicholas Apostoloff. Self-conditioning pre-trained language models. In
International Conference on Machine Learning, pages 4455–4473. PMLR, 2022.
[56] Hassan Sajjad, Nadir Durrani, and Fahim Dalvi. Neuron-level interpretation of deep nlp models: A survey. arXiv
preprint arXiv:2108.13138, 2021.
[57] Julien Mairal, Francis Bach, Jean Ponce, et al. Sparse modeling for image and vision processing. Foundations and
Trends® in Computer Graphics and Vision, 8(2-3):85–283, 2014.
[58] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th international
conference on international conference on machine learning, pages 399–406, 2010.
[59] Vardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. Theoretical foundations of deep learning via
sparse representations: A multilayer sparse model and its connection to convolutional neural networks. IEEE Signal
Processing Magazine, 35(4):72–89, 2018.
[60] Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via convolutional sparse
coding. The Journal of Machine Learning Research, 18(1):2887–2938, 2017.
[61] Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad. Multilayer convolutional sparse modeling:
Pursuit and dictionary learning. IEEE Transactions on Signal Processing, 66(15):4090–4104, 2018.
[62] Xiaoxia Sun, Nasser M Nasrabadi, and Trac D Tran. Supervised deep sparse coding networks. In 2018 25th IEEE
International Conference on Image Processing (ICIP), pages 346–350. IEEE, 2018.
[63] Xili Dai, Mingyang Li, Pengyuan Zhai, Shengbang Tong, Xingjian Gao, Shao-Lun Huang, Zhihui Zhu, Chong You,
and Yi Ma. Revisiting sparse convolutional model for visual recognition. arXiv preprint arXiv:2210.12945, 2022.
[64] Emmanuel J Candès and Michael B Wakin. An introduction to compressive sampling. IEEE signal processing magazine,
25(2):21–30, 2008.
[65] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse recovery.
Advances in Neural Information Processing Systems, 32, 2019.
[66] Peng Zhao, Yun Yang, and Qiao-Chu He. Implicit regularization via hadamard product over-parametrization in
high-dimensional linear regression. arXiv preprint arXiv:1903.09367, 2019.
17

[67] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry,
and Nathan Srebro. Kernel and rich regimes in overparametrized models. In Conference on Learning Theory, pages
3635–3673. PMLR, 2020.
[68] Hung-Hsu Chou, Johannes Maly, and Holger Rauhut. More is less: Inducing sparsity via overparameterization.
arXiv preprint arXiv:2112.11027, 2021.
[69] Mor Shpigel Nacson, Kavya Ravichandran, Nathan Srebro, and Daniel Soudry. Implicit bias of the step size in linear
diagonal neural networks. In International Conference on Machine Learning, pages 16270–16295. PMLR, 2022.
[70] Rares-Darius Buhai, Yoni Halpern, Yoon Kim, Andrej Risteski, and David Sontag. Empirical study of the benefits
of overparameterization in learning latent variable models. In International Conference on Machine Learning, pages
1211–1219. PMLR, 2020.
[71] Jeremias Sulam, Chong You, and Zhihui Zhu. Recovery and generalization in over-realized dictionary learning.
Journal of Machine Learning Research, 23(135):1–23, 2022.
[72] Geoffrey Chinot, Matthias Löffler, and Sara van de Geer. On the robustness of minimum norm interpolators and
regularized empirical risk minimizers. The Annals of Statistics, 50(4):2306–2333, 2022.
[73] Frederic Koehler, Lijia Zhou, Danica J Sutherland, and Nathan Srebro. Uniform convergence of interpolators:
Gaussian width, norm bounds and benign overfitting. In Advances in Neural Information Processing Systems, 2021.
[74] Konstantin Donhauser, Nicolo Ruggeri, Stefan Stojanovic, and Fanny Yang. Fast rates for noisy interpolation require
rethinking the effects of inductive bias. arXiv preprint arXiv:2203.03597, 2022.
[75] Tolga Ergen and Mert Pilanci. Path regularization: A convexity and sparsity inducing regularization for parallel
relu networks. arXiv preprint arXiv:2110.09548, 2021.
[76] Kaiqi Zhang and Yu-Xiang Wang. Deep learning meets nonparametric regression: Are weight-decayed dnns locally
adaptive? arXiv preprint arXiv:2204.09664, 2022.
[77] Maksym Andriushchenko, Aditya Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Sgd with large step sizes
learns sparse features. arXiv preprint arXiv:2210.05337, 2022.
[78] Hongru Yang, Ziyu Jiang, Ruizhe Zhang, Zhangyang Wang, and Yingbin Liang. Sharper analysis of sparsely
activated wide neural networks with trainable biases. arXiv preprint arXiv:2301.00327, 2023.
[79] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep
learning training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020.
[80] Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Exploring deep neural networks via layer-peeled model:
Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences, 118(43):e2103091118, 2021.
[81] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of
neural collapse with unconstrained features. Advances in Neural Information Processing Systems, 34:29820–29834, 2021.
[82] Stephan Wojtowytsch et al. On the emergence of simplex symmetry in the final and penultimate layers of neural
network classifiers. arXiv preprint arXiv:2012.05420, 2020.
[83] Tom Tirer and Joan Bruna. Extended unconstrained features model for exploring deep neural collapse. In International
Conference on Machine Learning, pages 21478–21505. PMLR, 2022.
[84] Tomaso Poggio and Qianli Liao. Explicit regularization and implicit bias in deep network classifiers trained with the
square loss. arXiv preprint arXiv:2101.00072, 2020.
[85] Christos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina Behnia. Imbalance trouble: Revisiting
neural-collapse geometry. Advances in Neural Information Processing Systems, 35:27225–27238, 2022.
[86] Peter Súkeník, Marco Mondelli, and Christoph Lampert. Deep neural collapse is provably optimal for the deep
unconstrained features model. arXiv preprint arXiv:2305.13165, 2023.
[87] Stuart Geman, Elie Bienenstock, and René Doursat. Neural networks and the bias/variance dilemma. Neural
computation, 4(1):1–58, 1992.
18

[88] Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance trade-off for general-
ization of neural networks. In International Conference on Machine Learning, pages 10767–10777. PMLR, 2020.
[89] Robert Epstein. The principle of parsimony and some applications in psychology. The Journal of Mind and Behavior,
pages 119–130, 1984.
[90] Pedro Domingos. The role of occam’s razor in knowledge discovery. Data mining and knowledge discovery, 3(4):409–425,
1999.
[91] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B
(Methodological), 58(1):267–288, 1996.
[92] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel Hurt, Michael Isard,
Hyeontaek Lim, Ruoming Pang, Sudip Roy, et al. Pathways: Asynchronous distributed dataflow for ml. Proceedings
of Machine Learning and Systems, 4:430–449, 2022.
[93] Daniel A. Roberts. Why is ai hard and physics simple?, 2021.
[94] Yann LeCun. A path towards autonomous machine intelligence. preprint posted on openreview, 2022.
[95] Rama K Vasudevan, Maxim Ziatdinov, Lukas Vlcek, and Sergei V Kalinin. Off-the-shelf deep learning is not enough,
and requires parsimony, bayesianity, and causality. npj Computational Materials, 7(1):1–6, 2021.
[96] Yi Ma, Doris Tsao, and Heung-Yeung Shum. On the principles of parsimony and self-consistency for the emergence
of intelligence. Frontiers of Information Technology & Electronic Engineering, pages 1–26, 2022.
[97] Blake Richards, Doris Tsao, and Anthony Zador. The application of artificial intelligence to biology and neuroscience.
Cell, 185(15):2640–2643, 2022.
[98] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on
Learning Representations, 2015.
[99] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171–4186, 2019.
[100] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,
Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in
Neural Information Processing Systems, 34:24261–24272, 2021.
[101] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. Communications of the ACM, 60(6):84–90, 2017.
[102] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[103] Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. Deep isometric learning for visual recognition.
In International Conference on Machine Learning, pages 7824–7835. PMLR, 2020.
[104] Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero is all
you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pages 1352–1361. PMLR, 2021.
[105] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
Yangqing Jia, and Kaiming He.
Accurate, large minibatch sgd: Training imagenet in 1 hour.
arXiv preprint
arXiv:1706.02677, 2017.
[106] Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. In
International Conference on Learning Representations, 2019.
[107] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper with
image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32–42, 2021.
[108] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle
Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering
research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.
19

[109] Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. Decoupled context processing for context augmented language modeling.
arXiv preprint arXiv:2210.05758, 2022.
[110] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question
answering, 2020.
[111] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-
tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pages 6769–6781, Online, November 2020. Association
for Computational Linguistics.
20

Appendices
The appendices are organized as follows. In Section A we provide the implementation details for experiments
conducted in this paper. In Section B we demonstrate the emergence of sparse activation in other architectures
and with other optimizers than those used in Section 2. In Section C we provide additional experiments upon
those in Section 4 to demonstrate the benefits of sparsity. In Section D we provide the proof to Theorem 3.1. In
Section E we present insights on the emergence of activation sparsity from experiments on two-layer MLP
models. Finally, in Section F we provide answers to frequently asked questions.
A
Implementation Details
A.1
T5
Unless specified otherwise, we use vanilla T5 architecture [7]. We train the models with dropout of 0.1,
Adafactor optimizer, and an inverse square root learning rate schedule. For the first 10,000 steps we also use a
fixed learning rate of 0.01 as warm-up. The training task is span corruption without any mixture, and unless
specified otherwise, we train the model for 100,000 steps with batch size of 256 to save compute and time, as
the sparsity or accuracy trend is already clear by then. We use 512 tokens on the encoder side and 114 tokens
on the decoder side.
A.2
ViT
Following [12], we train ViT using ADAM [98] as the optimizer with β1 = 0.9, β2 = 0.999. Other training
details such as weight decay, dropout rate, and learning rate all follow the description in [12, Section B.1]
except that we train for 180 epochs (as opposed to 300) on ImageNet-1k.
A.3
T5 / ViT Configurations
For the reader’s convenience, we summarize the configuration of varying T5 / ViT models used in our paper in
Table A.1.
Table A.1: Configuration of T5 and ViT that are used in the experiments. dmodel and dff are defined in Section 1.3.
# Layers is the number of encoder + decoder layers for T5 and encoder layers for ViT.
T5
ViT
Small
Base
Large
3B
11B
Base
Large
Huge
dmodel
512
768
1024
1024
1024
768
1024
1280
dff
2048
3072
4096
16384
65536
3072
4096
5120
# Layers
6 + 6
12 + 12
24 + 24
24 + 24
24 + 24
12
24
32
# Parameters
60M
220M
770M
2,800M
11,000M
86M
307M
632M
B
Additional Results On Prevalence of Sparsity
B.1
Sparsity and Network Architecture
We evaluate the sparsity level of activation map in several commonly used network architectures beyond T5
and ViT. This includes BERT which is also a Transformer based architecture, as well as non-Transformer based
architectures such as MLP-Mixer and ConvNets. We also examine whether residual connection accounts for
the emergence of sparsity.
BERT. We evaluate the sparsity level of BERT models [99]. We specifically consider BERT Base (12 layers) and
BERT Large (24 layers) Transformer models, with ReLU activation in the MLP layers. We follow the same
21

(a) BERT Base
(b) BERT Large
(c) Pre-activations: Layer 1
(d) Pre-activations: Layer 12
Figure B.1: Plots a, b: Percentage of nonzero entries in activation maps of BERT Base and Large models [99]
trained on Wikipedia dataset. We observe high levels of sparsity (<10%) similar to other Transformer models.
Plots c, d: Histograms of pre-activation values for layers 1 and 12 of a Bert Base model. We notice that while
at initialization the activations are distributed with mean 0, the mean quickly shifts negative as the training
progresses, resulting in high levels of sparse activation values.
(a) GeLU Activation
(b) Sigmoid Activation
(c) Tanh Activation
Figure B.2: Layer 1 Preactivation histograms for BERT Base models with different activation functions. We
observe similar behavior as ReLU with GeLU and Sigmoid activations. However Tanh activation has different
distribution of preactivation values. The network doesn’t show sparsity and the accuracy is also worse in
comparison to ReLU/GeLU.
training receipe as [99] and pre-train these models on Wikipedia and Books dataset using Masked Language
Modelling (MLM) objective. We train for 450000 steps with a batch size of 1024 using AdamW optimizer with
1e −4 learning rate.
Figure B.3: Percentage of nonzero en-
tries in activation maps of MLP-Mixer
trained on ImageNet-21k.
Results
for token-mixing and channel-mixing
MLPs are plotted in separate curves.
In Figure B.1 we plot the sparsity levels of both BERT models for
all the intermediate MLP layers (plots a and b). We observe that
both these models exhibit high levels of sparsity (< 10%) as other
Transformer models. We further visualize the pre-activation values of
the MLP layers as histograms in plots c and d. We observe that while
they have mean 0 at initialization, the mean quickly becomes negative
as training progresses, resulting in high sparsity levels.
Finally, in Figure B.2 we provide a visualization of pre-activation
of values with several popular activation functions, including GeLU,
Sigmoid, and Tanh. We observe a similar distribution of preactivation
values as ReLU with GeLU and Sigmoid activations. However Tanh
activation has a different distribution of preactivation values. With
Tanh activation, the network does not show sparsity and the accuracy
is significantly worse in comparison to ReLU/GeLU.
MLP-Mixer. We evaluate the sparsity level of the MLP-Mixer [100],
an all-MLP architecture constructed from cascading token-mixing
and channel-mixing MLPs. Specifically, we use Mixer-B16 as the
architecture, ADAM with β1 = 0.9, β2 = 0.999 as the optimizer, and train on ImageNet-21k for 300 epochs.
While [100] sweeps over a product set of hyper-parameters, here for simplicity we use a fixed set of hyper-
parameters with weight decay of 0.03, gradient norm clipping at 1.0, base learning rate of 0.003, RandAugment
magnitude of 10, no mixup, no stochastic depth, and no dropout.
Figure B.3 shows the sparsity level at the intermediate layer of both token mixing and channel mixing MLPs
22

(a) ResNet-18
(b) ResNet-50
Figure B.4: Percentage of nonzero entries in activation maps of ResNet-18 and ResNet-50 trained on ImageNet-
1k. Results for the two (resp., three) layers in each residual block (resp., bottleneck residual block) of ResNet-18
(resp., ResNet-50) are plotted in separate curves.
(a) 1st Layers
(b) 2nd Layers
Figure B.5: Effect of batch normalization (BN) on sparsity level across layers of ResNet-18. Because ResNets
cannot be effectively train without BN, we reduce the learning rate (LR) by a factor of 10, and multiply the
residual branch by a trainable scalar initialized at 0.
of Mixer-B16. We also plot the sparsity level of ViT (i.e., the plot in Figure 2a) to Figure B.3 for a comparison.
It can be seen that the first four layers of channel mixing MLP and ViT have almost identical sparsity levels,
while the rest of the layers (other than the last one) of channel mixing MLP are denser than the corresponding
layer of ViT. On the other hand, the token mixing MLPs produce dense activation maps with more than 50%
nonzero entries, probably because the dimension of the activation maps (384) is too small.
Convolutional Neural Network (ConvNet). Sparsity in activation maps has been studied for ConvNets such
as the AlexNet [101] at least as early as in the work of [32]. There are also follow-up work [33, 36] on how
enforcing sparse activation maps can help to gain computation efficiency. For completeness, we evaluate
and present results for the sparsity level of residual networks (ResNets) [102], which is one of the most
commonly used ConvNets, trained on ImageNet-1k. In particular, we focus on ResNet-18 and ResNet-50 which
are constructed from stacking 8 standard residual blocks and 16 “bottleneck” residual blocks, respectively,
where each block has two and three convolutional and ReLU layers, respectively. We examine the sparsity of
activation maps after each of the ReLU layers in each residual block.
The results for ResNet-18 and ResNet-50 are reported in Figure B.4a and Figure B.4b, respectively. Here, the
x-axis is the index of the residual block, and the sparsity of different layers in the residual blocks are plotted
with separated curves in each figure. It can be observed that
• Layers near the network output tend to produce sparser activation maps than layers near the network input.
This is aligned with the observation with ViT trained on ImageNet-1k (see Figure 2b).
• For each residual block, the intermediate layers (i.e., the 1st layer for ResNet-18 and the 1st & 2nd layers for
ResNet-50) produce sparser activation maps than the output layer (i.e., 2nd layer for ResNet-18 and 3rd
23

(a) 1st Layers
(b) 2nd Layers
Figure B.6: Effect of network width ∈{64, 128, 256} on sparsity level across layers of ResNet-18.
layer for ResNet-50).
In addition, all residual blocks are divided into four stages that have different output feature map sizes. For
ResNet-50, the four stages are composed of blocks 0 - 2, 3 - 6, 7 - 12, and 13 - 15. Figure B.4b shows that there
are patterns on how sparsity level varies within each stage and across the boundary of the stages.
• For the 1st layers, percentage of nonzeros decreases within each stage, and jumps up from the last layer of
each stage to the first layer of next stage.
• For the 2nd layers, percentage of nonzeros decreases quickly at the beginning of each stage then becomes
stable.
• For the 3rd layers, percentage of nonzeros tend to increase slightly within each stage, and jumps down from
the last layer of each stage to the first layer of next stage.
Such observations may help to understand the role of each stage in ResNets.
Comparing the percentage of nonzero entries in ResNets (shown in Figure B.4a and Figure B.4b) and for
Transformers (shown in Figure 2b), both of which are trained on ImageNet-1k, we see that ResNets produce
much denser activation maps with more than 10% nonzero entries in all layers. One possible explanation is that
ResNet uses batch normalization (BN) before each activation function, while Transformer’s MLP does not have
BN before the activation function. To understand the effect of BN on sparsity, we conduct an experiment with
BN in ResNet removed. Because ResNet cannot be effectively trained without BN, we decrease the learning
rate from standard ResNet training by a factor of 10. Moreover, we add a learnable scalar multiplier that is
initialized as 0 to all the residual branches, following the study in [103, 104]. The results for comparing with
standard ResNet are reported in Figure B.5, where to separate the effect of using a smaller learning rate, we also
compare with the method of training a regular ResNet but with a small learning rate compared to standard
training. The two subfigures of Figure B.5 show the effect of width on sparsity of the first and second layers
in each residual block, respectively. It can be observed that, removing BN does not significantly change the
sparsity level, except for small set of layers.
Meanwhile, the trend that larger models are sparser for Transformers (see Section 2.2) holds for ResNets
as well, as seen in Figure B.6. Here, we vary the width of ResNet-18 by multiplying the number of output
channels of each convolutional layer by a factor of 1 (for width = 64), 2 (for width = 128), and 4 (for width =
256). The two subfigures show the effect of width on sparsity of the first and second layers in each residual
block, respectively. In both cases, wider models have smaller percentage of nonzero entries across all layers,
except for the very last layer (i.e., the 2nd layer in block #7 shown in Figure B.6b).
Sparsity and Residual Learning. We provide a study on the effect of residual connections on activation
sparsity. Each Transformer block contains two types of residual connections: the one that is in parallel with
the attention blocks, and the one that is in parallel with the MLP blocks. We focus on the residual connection
parallel to the MLP blocks. We perform two different studies.
• Effect of shortcut connection. Towards that, we train two T5-Large models, one using the vanilla Transformer
block and the other with residual connection removed for the Transformer block on encoder layer 6 (i.e.,
the 7th encoder layer, as we count from 0). There is a 1.6% evaluation accuracy drop with the latter model
compared to the former model.
24

0
20000
40000
60000
80000
100000
steps
0.1
0.2
0.3
0.4
0.5
0.6
0.7
% non zeros
encoder layer 0
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.0
0.1
0.2
0.3
0.4
0.5
0.6
% non zeros
encoder layer 1
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.0
0.1
0.2
0.3
0.4
0.5
% non zeros
encoder layer 2
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.0
0.1
0.2
0.3
0.4
% non zeros
encoder layer 3
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.0
0.1
0.2
0.3
0.4
% non zeros
encoder layer 4
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.0
0.1
0.2
0.3
0.4
% non zeros
encoder layer 5
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
% non zeros
encoder layer 6
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
% non zeros
encoder layer 7
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
% non zeros
encoder layer 8
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.00
0.05
0.10
0.15
0.20
0.25
0.30
% non zeros
encoder layer 9
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.00
0.05
0.10
0.15
0.20
0.25
0.30
% non zeros
encoder layer 10
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.00
0.05
0.10
0.15
0.20
0.25
0.30
% non zeros
encoder layer 11
Vanilla T5
MLP residual removed on encoder layer 6
Figure B.7: Percentage of nonzero entries in activation maps in vanilla T5-Large and in a T5-Large with the
residual connection parallel to MLP removed in the 7th encoder layer (i.e., encoder layer 6). Different
subplots correspond to different encoder layers (see Figure B.8 for results on decoder layers). The encoder
layer 6, which has its residual connection removed, shows a significant difference in both sparsity and the
trend of sparsity during training. Sparsity level in other layers changes from vanilla T5-Large as well, though
to a smaller extent.
The percentage of nonzero entries of these two Transformers are presented in Figure B.7 for the encoder
layers and in Figure B.8 for the decoder layers. It can be seen that in encoder layer 6 for which the residual
connection is removed, the sparsity has a very different trend during training compared to the corresponding
layer of the vanilla Transformer. Moreover, the sparsity level at all other layers also changes, though to a
much smaller extend.
• Effect of initialization scale of the residual branch. Many works have found that having the residual branch
initialized at a smaller scale helps with stabilizing and accelerating the training of residual (convolutional)
networks [105, 106] and Transformers [107]. Here for simplicity we consider the idea from [103, 104] where
a trainable scalar multiplier that is initialized at zero is applied to the residual branch (a.k.a., ReZero).
We consider ViT trained on ImageNet-21k with ReZero added to the MLP modules. We find that this
increases the training accuracy from 46.15% to 46.85% but reduces the validation accuracy from 47.58% to
25

0
20000
40000
60000
80000
100000
steps
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
% non zeros
decoder layer 12
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
% non zeros
decoder layer 13
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
% non zeros
decoder layer 14
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.025
0.050
0.075
0.100
0.125
0.150
0.175
% non zeros
decoder layer 15
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.025
0.050
0.075
0.100
0.125
0.150
0.175
% non zeros
decoder layer 16
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.025
0.050
0.075
0.100
0.125
0.150
0.175
% non zeros
decoder layer 17
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
% non zeros
decoder layer 18
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
% non zeros
decoder layer 19
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
% non zeros
decoder layer 20
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
% non zeros
decoder layer 21
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.04
0.06
0.08
0.10
0.12
0.14
0.16
% non zeros
decoder layer 22
Vanilla T5
MLP residual removed on encoder layer 6
0
20000
40000
60000
80000
100000
steps
0.04
0.06
0.08
0.10
0.12
0.14
0.16
% non zeros
decoder layer 23
Vanilla T5
MLP residual removed on encoder layer 6
Figure B.8: Same setup as Figure B.7, but showing the results for the last 12 layers of the decoder.
46.75%. We plot the sparsity level of ViT with ReZero and compare it with the vanilla ViT in Figure B.9. It
can be seen that ReZero reduces the percentage of nonzeros in layers near the network output.
B.2
Sparsity and Optimizer
Transformers are usually trained using ADAM or its variants as the optimizer [98]. It may be curious to ask
whether the emergence of sparsity is specific to such optimizers and whether other optimizers, such as stochastic
gradient descent (SGD), also leads to sparse activation maps. However, we find that SGD cannot effectively
train Transformer architectures such as T5 and ViT. Hence, we study the effect of optimizer on activation
sparsity by looking at ResNet trained on ImageNet-1k following the setup in Section B.1, since both SGD and
ADAM can effectively train the network. To train ResNet with ADAM, we use the same hyper-parameters
as those used in SGD, with the only difference being that the optimizer is ADAM with β1 = 0.9, β2 = 0.999.
To make the comparison with SGD fair, we tune the base learning rate for ADAM and select 3e −3, which is
the one that gives the highest training accuracy among the set of {1e −4, 3e −4, 1e −3, 3e −3, 1e −2}. The
training accuracy obtained by ADAM with base learning rate 3e −3 is similar to that obtained by SGD, namely,
67.8% by ADAM vs 69.3% by SGD with ResNet-18, and 75.0% by ADAM vs 78.5% by SGD with ResNet-50.
26

Figure B.9: Effect of initialization scale of the residual branch. We add a scalar multiplier that is initialized at 0
on the residual branch (a.k.a., ReZero [104]) of the MLP modules of a ViT, and train the model on ImageNet-
21k. The percentage of nonzero entries is compared with those obtained with a regular ViT.
(a) 1st Layers
(b) 2nd Layers
Figure B.10: Effect of optimizer on sparsity level across layers of ResNet-18.
The results for ResNet-18 and ResNet-50 are presented in Figure B.10 and Figure B.11, respectively. For
ResNet-18, we see that ADAM leads to a smaller percentage of nonzero entries particularly towards the output
of the network for the first layers of each residual block. In contrast, ADAM and SGD have very similar sparsity
level at the second layers of each residual block. Similar observation holds for ResNet-50, where the percentage
of nonzero entries is smaller with ADAM for the first and second layers of each residual block, while for the
third layer the sparsity level does not change much.
B.3
Sparsity in Finetuning
In this section we show that activation sparsity not only occurs after model pretraining but persists after further
finetuning on downstream tasks. Here we take a T5 that has been pretrained on C4 as described in Section 1.3,
and finetune the model on a open domain Natural Question [108] QA task. We follow the set up in [109],
where the retrieved passages are independently encoded by the encoder, and then passed to the decoder
via cross attention. The decoder takes the question as the prefix and produces the answer. The decoder is a
standard auto-regressive setup, despite passing the question as a prefix. We used the same Wikipedia passages
retrieved by FiD [110] which uses DPR [111] as retriever. For this experiment, we used 20 passages for each
question. When calculating the sparsity level, we ignored the activation produced from paddings, for both
encoder and decoder. The sparsity level of different encoder and decoder layers after finetuning is reported in
Figure B.12 and is compared to those before the finetuning. It can be seen that finetuning does not drastically
change the sparsity level.
27

(a) 1st Layers
(b) 2nd Layers
(c) 3rd Layers
Figure B.11: Effect of optimizer on sparsity level across layers of ResNet-50.
(a) Encoder
(b) Decoder
Figure B.12: Percentage of nonzero entries across different layers of trained T5 after pretraining vs after
finetuning on a question answering task. Left: Results on encoder layers. Right: Results on decoder layers.
C
Additional Results on Benefits of Sparsity
C.1
Top-k Does not Significantly Affect Training Convergence
Figure C.1: Learning curves for results reported
in Figure 6b. ViT and Top-k ViT have similar con-
vergence speed.
One may argue that the convergence of Top-k Transformer
can be slower than that of a vanilla Transformer because,
with fewer neurons being activated, the amount of parame-
ters that have nonzero gradient associated with each train-
ing sample is smaller. Here we plot the training curves
for ViT as well as Top-k ViT with k ∈{64, 128, 256} on
ImageNet-21K (this is the same setup as the experiments
in Figure 6b), and report the results in Figure C.1. It can be
observed that taking Top-k does not significantly reduce
convergence speed, particularly when k is relatively large
(e.g., k = 256). This may not be very surprising since, as
can be seen in Figure 1, even in training of vanilla Trans-
formers, the percentage of nonzeros drops very quickly
at the beginning of training, hence very few neurons are
being activated for each input token throughout most of
the training process.
28

Table C.1: Evaluation of ViT with a varying weights ∈{0.001, 0.01, 0.1, 1.0} on a ℓ1 regularization upon
activation maps for ImageNet-1k classification in terms of 1) averaged percentage of nonzero entries in
activation maps across all layers, 2) natural accuracy (i.e., on ImageNet-1k evaluation set), 3) robust accuracy
under input perturbation with additive {Gaussian, Impulse, Shot} noise, and 4) calibration error measured by
ECE.
Methods
Avg. Perc.
of Nonzeros
Natural
Accuracy
Accuracy under
Input Perturbation
Expected Calibration
Error (ECE)
Gaussian
Impulse
Shot
ViT
5.67%
74.85%
39.54%
37.37%
38.56%
8.42%
L1-ViT-0.001
5.39%
75.03%
42.42%
40.57%
41.33%
7.75%
L1-ViT-0.01
4.52%
74.98%
42.78%
41.11%
41.48%
6.86%
L1-ViT-0.1
3.34%
74.55%
41.90%
40.30%
40.44%
6.28%
L1-ViT-1.0
1.60%
73.21%
40.26%
38.01%
38.95%
6.34%
C.2
Benefits of Sparsity Persists with L1-Norm Induced Sparsity
While Top-k thresholding is used in Section 4.3 to demonstrate the benefit of sparsity, we show that other
means of obtaining sparsity, such as an explicit ℓ1 norm regularization, also provides such benefits.
We experiment with ViT for ImageNet-1k classification under the same setup as in Section 4.3. Here instead
of the Top-k ViT, we train a regular ViT but with an additional loss term, which is the sum of the ℓ1 norm of all
activation maps of ViT across all layers. We refer to the method as L1-ViT. We vary the weight λ on the ℓ1 loss
in the set λ ∈{0.001, 0.01, 0.1, 1.0} to control the strength of the regularization, and denote the corresponding
methods as L1-ViT-{0.001, 0.01, 0.1, 1.0}.
The sparsity level, natural accuracy, robust accuracy under input perturbation, and ECE of L1-ViT are
reported in Table C.1. We see that the averaged percentage of nonzero entries decreases with an increasing λ.
With λ = 0.001, 0.01 and λ = 0.1, the robust accuracy and calibration all improve over ViT without hurting the
natural accuracy. Using a λ = 1.0 drastically reduces the percentage of nonzero entries and hurts the natural
accuracy, with robust accuracy and ECE on par with ViT.
C.3
Activation Regularization with L2 Norm
In addition to using ℓ1 norm on the activation maps for enforcing sparsity, we also experiment with using ℓ2
norm on the activation maps and report the results in Table C.2. It can be observed that ℓ2 regularization also
reduces the percentage of nonzero entries and improves performance in terms of robust accuracy under input
perturbation and calibration error. However, the improvement in performance is less pronounced compared to
those obtained with L1-ViT. For example, if we compare L1-ViT-0.01 and L2-ViT-0.1 which has similar sparsity
level, the former one produces notably higher accuracy under input perturbation and lower ECE.
Conceptually, both ℓ1 and ℓ2 norm reduce the magnitude of entries in activation maps. The difference is
that ℓ2 norm penalizes more on entries with large magnitude, compared to the ℓ1 norm which relatively focuses
more on small entries. The fact that the benefit from ℓ2 regularization is less prominent than ℓ1 regularization
indicates that the magnitude of large entries is less relevant for obtaining robustness and calibration. What
matters more is whether the smaller entries can be suppressed or not.
D
Proof of Theorem 3.1
Proof of Theorem 3.1. For an arbitrary loss ℓ(f(x), y), we have
∂ℓ
∂pi∗=
 ∂ℓ
∂f , ∂f
∂pi∗

=
 ∂ℓ
∂f , vi∗

.
(D.1)
29

Table C.2: Same as Table C.1 but with ℓ1 regularization replaced by ℓ2 regularization of varying weights
∈{0.001, 0.01, 0.1, 1.0}.
Methods
Avg. Perc.
of Nonzeros
Natural
Accuracy
Accuracy under
Input Perturbation
Expected Calibration
Error (ECE)
Gaussian
Impulse
Shot
ViT
5.67%
74.85%
39.54%
37.37%
38.56%
8.42%
L2-ViT-0.001
5.65%
74.90%
40.55%
38.31%
39.32%
8.25%
L2-ViT-0.01
5.22%
75.03%
41.26%
39.63%
40.03%
7.66%
L2-ViT-0.1
4.43%
74.67%
40.48%
38.51%
39.24%
7.64%
L2-ViT-1.0
3.08%
74.49%
40.76%
38.29%
39.37%
7.10%
First, Consider ℓ= ℓMSE. We have
∂ℓMSE
∂f
= f(x) −y =
X
i
σ(pi) · vi −y.
(D.2)
Plugging this into (D.1), we obtain
∂ℓMSE
∂pi∗
=
 X
i
σ(pi)⟨vi, vi∗⟩
!
−⟨vi∗, y⟩
=

X
i̸=i∗
σ(pi)⟨vi, vi∗⟩

+ σ(pi∗)⟨vi∗, vi∗⟩−⟨vi∗, y⟩
(D.3)
Taking the expectation, and noting the conditions in (4), we have
E
∂ℓMSE
∂pi∗

= 0 + σ(pi∗)E [⟨vi∗, vi∗⟩] + 0 > 0.
(D.4)
This finishes the proof for MSE loss.
In the rest of the proof we consider ℓ= ℓCE. We have
∂ℓCE
∂f
=
exp(f(x))
⟨exp(f(x)), 1⟩−y =
exp(P
i σ(pi) · vi)
⟨exp(P
i σ(pi) · vi), 1⟩−y.
(D.5)
Plugging this into (D.1), we obtain
∂ℓCE
∂pi∗= ⟨exp(P
i σ(pi) · vi), vi∗⟩
⟨exp(P
i σ(pi) · vi), 1⟩−⟨vi∗, y⟩
(D.6)
For the enumerator in the first term on the RHS of the equation above, we have
*
exp
 X
i
σ(pi) · vi
!
, vi∗
+
=
X
m
 
vi∗,m · exp
 X
i
σ(pi) · vim
!!
=
X
m

vi∗,m · exp (pi∗· vi∗,m) · exp

X
i̸=i∗
σ(pi) · vi,m




(D.7)
Plugging this into (D.6) and denoting
C(1)
m = exp

X
i̸=i∗
σ(pi) · vi,m

,
30

we obtain
∂ℓCE
∂pi∗=
X
m
 
vi∗,m · exp (pi∗· vi∗,m) · C(1)
m
⟨exp(P
i σ(pi) · vi), 1⟩
!
−⟨vi∗, y⟩
(D.8)
For the denominator in the first term on the RHS of the equation above, we have
*
exp
 X
i
σ(pi) · vi
!
, 1
+
=
X
m′
exp
 X
i
σ(pi) · vim′
!
=
X
m′

exp (pi∗· vi∗,m′) · exp

X
i̸=i∗
σ(pi) · vim′




= exp (pi∗· vi∗,m) · exp

X
i̸=i∗
σ(pi) · vi,m


+
X
m′̸=m

exp (pi∗· vi∗,m′) · exp

X
i̸=i∗
σ(pi) · vim′




(D.9)
Plugging this into (D.8) and denoting
C(2)
m = exp

X
i̸=i∗
σ(pi) · vi,m

,
(D.10)
C(3)
m =
X
m′̸=m

exp (pi∗· vi∗,m′) · exp

X
i̸=i∗
σ(pi) · vim′



,
(D.11)
we obtain
∂ℓCE
∂pi∗=
X
m
 
vi∗,m · exp (pi∗· vi∗,m) · C(1)
m
exp (pi∗· vi∗,m) · C(2)
m + C(3)
m
!
−⟨vi∗, y⟩.
(D.12)
Taking expectation with respect to V on both sides, and using the assumption that all entries of V are
independent, we have
E
∂ℓCE
∂pi∗

=
X
m
E
"
vi∗,m · exp (pi∗· vi∗,m) · C(1)
m
exp (pi∗· vi∗,m) · C(2)
m + C(3)
m
#
−E [⟨vi∗, y⟩]
=
X
m
E{vi,l|(i,l)̸=(i∗,m)}
"
Evi∗,m
"
vi∗,m · exp (pi∗· vi∗,m) · C(1)
m
exp (pi∗· vi∗,m) · C(2)
m + C(3)
m
##
−E [⟨vi∗, y⟩] .
(D.13)
In above, Evi∗,m [] means expectation with respect to vi∗,m, and E{vi,l|(i,l)̸=(i∗,m)} [] means expectation with
respect to all other entries in V . Note that C(1)
m , C(2)
m , and C(3)
m are independent of vi∗,m. By Lemma D.1 and
using the assumption that the expectation of V is zero, we have
Evi∗,m
"
vi∗,m · exp (pi∗· vi∗,m) · C(1)
m
exp (pi∗· vi∗,m) · C(2)
m + C(3)
m
#
> 0,
(D.14)
and
E [⟨vi∗, y⟩] = 0.
(D.15)
Plugging the above two relations into (D.13), we obtain
E
∂ℓCE
∂pi∗

> 0.
(D.16)
31

The following lemma is used in the proof above.
Lemma D.1. Let V be a random variable with a probabilistic density function p(v) that satisfies P(V = 0) ̸= 1. Let
C1, C2, C3 and p be positive numbers. Then,
E
 C1V · exp(pv)
C2 exp(pV) + C3

>
C1
C2 + C3
E [V] .
(D.17)
Proof. We may calculate the expectation by using the probabilistic density function p(v) as
E
 C1V · exp(pV)
C2 exp(pV) + C3

= E

C1V
C2 + C3 exp(−pV)

=
Z ∞
−∞
C1v
C2 + C3 exp(−pv)p(v)dv .=
Z ∞
−∞
g(v) · vp(v)dv.
(D.18)
Since g(v) is monotonically increasing for v ∈IR, we have g(v) ≥g(0) for v ≥0 and g(v) ≤g(0) for v ≤0.
Hence,
Z 0
−∞
g(v) · vp(v)dv ≥g(0)
Z 0
−∞
vp(v)dv,
(D.19)
Z ∞
0
g(v) · vp(v)dv ≥g(0)
Z ∞
0
vp(v)dv.
(D.20)
Moreover, since P(V = 0) ̸= 1, there exists an interval (a, b) such that
R b
a p(v)dv > 0. Without loss of generality
we assume that b > a ≥0. Then,
Z b
a
g(v) · vp(v)dv > g(0)
Z b
a
vp(v)dv.
(D.21)
That is, the inequality in (D.20) holds with strict inequality. Hence we have
E
 C1V · exp(pV)
C2 exp(pV) + C3

=
Z ∞
−∞
g(v) · vp(v)dv > g(0)E [V] =
C1
C2 + C3
E [V] .
(D.22)
E
Insights from Sparsity in MLPs
We study the sparsity of activation maps in two-layer MLPs. By showing that sparsity emerges, the result here
extends the scope of prevalence of activation sparsity from modern DNNs to two-layer MLPs which are one of
the simplest neural network architectures. Moreover, by training such two-layer MLPs with different types of
data, we provide additional insights on the causes for emergence of sparsity.
Datasets. We conduct our experiment with the MNIST dataset, which contains 60,000 grey scale images of
handwritten digits. Similar to the experiment in Section 3, we also consider a dataset with random data, as
well as a dataset with infinite data. For the random data, we replace each image of MNIST with a random one
drawn from sampling i.i.d. pixels from uniform distribution, and each label with a random class amongst 10.
Note that the image-label pairs are fixed throughout training. For the infinite data, the random images and
random labels are generated on-the-fly, representing a random dataset of infinite size.
Models and Training. We train two-layer MLPs with ReLU activation maps with varying width (i.e., hidden
dimension): 32, 128, 512, 2048, 8192, 32768 and 131072. We use three different optimizers: SGD, SGD with
momentum, and Adam, all for 200 epochs (for the infinite data case, we use the same number of iterations as
that for training on MNIST and random data). We find that 200 epochs is sufficient for the reported metrics to
converge in most of the cases.
Results. We report training accuracy and the percentage of nonzero entries in the intermediate activation map
(i.e., non-zero rate) at the end of the training in Figure E.1. We have the following observations.
32

sgd_5e-2
0.0
0.2
0.4
0.6
0.8
1.0
MNIST
sgd_momentum_1e-3
adam_1e-4
0.0
0.2
0.4
0.6
0.8
1.0
Random Data
103
105
0.0
0.2
0.4
0.6
0.8
1.0
Infinite Data
103
105
103
105
Sparsity and Over/Under-Fitting in 2-Layer MLPs
hidden layer width
accuracy
non-zero rate
Figure E.1: Training accuracy and percentage of nonzero entries (both on the y-axis) in activation maps of
two-layer MLPs of varying width (on the x-axis, in log scale) after 200 epochs of training. Rows correspond to
different training datasets, and columns correspond to different training algorithms.
• For random data, we observe a uni-modal shaped curve for sparsity level. Namely, when the model width
is small hence the model cannot well-fit the training data, the percentage of nonzero entries is small. As
the model width increases, where the model is able to fit the training data evidenced by the fact that the
training accuracy increases, we observe that the percentage of nonzero entries starts to increase. However,
as we further increase the model size in the regime where model is able to perfectly fit the training data, we
see that the percentage of nonzeros starts to decrease.
• For infinite data, where the model cannot fit the training data (hence training accuracy is 0.1 which is the
same as result from random guessing), the percentage of nonzero entries is close to 0. This is aligned with
the result of random data experiment with a small model width.
• For MNIST, where the model of varying width in our experiment is able to fit the training data, we observe
that the percentage of nonzero entries decreases. This trend aligns with the random data experiment with
large model width
33

sgd_1e-1
0.0
0.2
0.4
0.6
0.8
1.0
MNIST
sgd_momentum_1e-2
adam_1e-3
0.0
0.2
0.4
0.6
0.8
1.0
Random Data
103
105
0.0
0.2
0.4
0.6
0.8
1.0
Infinite Data
103
105
103
105
Sparsity with Increased Learning Rate in 2-Layer MLPs
hidden layer width
accuracy
non-zero rate
Figure E.2: The same as in E.1 except that for each optimizer we use a larger learning rate.
The evidence above suggest that the sparsity level may be associated with the under- and over-parameterization
of the models. Namely, the percentage of nonzero entries is the highest when the model size is close to the point
that the model can start to fit the training data (i.e., the interpolation threshold), and is lower in both under
and over-parameterized regimes. It may be intriguing to note that a similar pattern exists for the variance (as
in the bias-variance tradeoff) curve of deep learning models, which as shown in [88] to exhibit a uni-modal
shape as well. Such a connection may help us understand the interplay between generalization and sparsity of
activation in deep learning models.
Finally, in Figure E.2 we report the results obtained with optimizers with larger learning rates compared to
those used in Figure E.1. It can be observed that larger learning rate produces sparser activation maps.
F
Frequently Asked Questions
Q. Isn’t that well-known that the output of ReLU is a sparse vector?
A. Typically, for randomly initialized networks the output of ReLU has around 50% nonzero entries, and may
34

be regarded sparse. However, what we observe is a much stronger level of sparsity, where in a trained network
the percentage of nonzeros at output of ReLU is much lower, e.g. 3% nonzeros in T5 (see Figure 1).
Q. Does the activation sparsity imply that many of the neurons (i.e., columns of K, V in (1)) can be
removed?
A. No. Sparsity means that each input to the MLP activates a very small set of neurons. However, different
inputs may activate different sets of neurons. Collectively, the set of all possible inputs to the MLP may
activate all the neurons; our result in Figure 3 shows that this is indeed the case, though the neuron activation
has a long tail distribution.
Q. Are you applying a thresholding to the entries when measuring sparsity level?
A. No. We are using ReLU as activations in both ViT and T5. ReLU produces exact zeros for non-positive
inputs, so no thresholding is needed in measuring the sparsity level.
Q. GELU is used more often than ReLU now. Does that mean that your paper is no longer relevant?
A. We opt to use ReLU in our study as it makes it very easy to measure the sparsity level without the hassle of
setting a thresholding. As shown in Figure B.2, even with GeLU we observe that the pre-activation is strongly
biased towards negativity.
Q. Is sparsity unique to Transformer? Or is it present in other architectures as well?
A. Sparsity emerges in convolutional networks (specificaly, ResNet) as well, though less sparse; see Figure B.5.
In addition, sparsity occurs in (pure) MLP as well, see Section E.
Q. Your Theorem 3.1 has nothing to do with Transformers. Does it apply to other architectures?
A. Yes. And we do also observe that sparsity occurs in other architectures as well, see the previous answer.
35

