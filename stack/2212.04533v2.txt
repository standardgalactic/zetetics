arXiv:2212.04533v2  [cs.CV]  12 Dec 2022
Deep Architectures for
Content Moderation and Movie Content Rating
Fatih Cagatay Akyon
Graduate School of Informatics, METU
Ankara, Turkey
Alptekin Temizel
Graduate School of Informatics, METU
Ankara, Turkey
Abstract—Rating a video based on its content is an im-
portant step for classifying video age categories. Movie con-
tent rating and TV show rating are the two most common
rating systems established by professional committees. How-
ever, manually reviewing and evaluating scene/ﬁlm content by
a committee is a tedious work and it becomes increasingly
difﬁcult with the ever-growing amount of online video content.
As such, a desirable solution is to use computer vision based
video content analysis techniques to automate the evaluation
process. In this paper, related works are summarized for
content moderation, video classiﬁcation, multi-modal learning,
and movie content rating. The project page is available at
https://github.com/fcakyon/content-moderation-deep-learning.
Index Terms—content moderation, action recognition, multi-
modal learning, movie content rating
I. INTRODUCTION
Many ﬁlms including movies, TV series, animations, and
other audio-visual content have inappropriate words or visual
content unsuitable for uncongenial audiences and adversely
affects the younger generation. Primarily, it affects under-aged
audiences through their exposure to visual content on TV
or other video-sharing and social networking websites like
YouTube, Facebook. Hence, it is desirable to automatically
detect sensitive content for ﬁltering, making Automated Film
Censorship and Rating (AFCR) one of the prominent applica-
tions of Machine Learning (ML) and attracting research.
Identiﬁcation of sensitive content is a challenging task due
to being subjective and open ended [37] [60]. While broadcast-
ers take proactive steps to identify sensitive content manually,
this is a slow, monotonous, and inefﬁcient process [53]. As a
result, AFCR is required to generate automatic and aggregated
ratings for ﬁlms or any audiovisual content to determine the
minimum age group suitable for viewing the content. Various
aspects of a ﬁlm that may be considered inappropriate include
violence, nudity, substance abuse, profanity, abnormal psy-
chological content, or other topics inappropriate for children
or teenagers in general [48]. Based on this content, ﬁlms or
any audiovisual content prepared for public display in many
developed countries are subject to appropriate classiﬁcation
standards.
Any audio-visual content is broadly categorized into three
parts: advisory, restricted, and adult or nudity. The advisory
categories are further divided into General (G), Parental Guid-
ance (PG), and Mature (M). The restricted types include
Mature Accompanied (MA15+) and Restricted (R18+). All
these classiﬁcations have speciﬁed minimum age for viewing
any audio-visual content under that speciﬁc class. ML plays
a vital role in ﬁlm censorship to detect and classify sensitive
and inappropriate content into a given classiﬁcation standard
[4] [73] [60]. Most researchers utilized ML techniques such
as Convolutional Neural Network (CNN) [79] [59] [66] [61]
[45], Support Vector Machine (SVM) [37] [32] [33], Long
ShortTerm Memory (LSTM) [59] [49], Gated Recurrent Units
(GRU) [79], Vision transformers [76] in their work for detect-
ing unsuitable audio-visual content.
II. RELATED WORK
This section of this paper aims to provide a brief overview
of existing research on the ﬁve main topics addressed in
this study. Speciﬁcally, we will discuss previous studies on
content moderation and movie datasets, action recognition,
multi-modal learning, movie genre classiﬁcation, and sensitive
content detection, highlighting their key ﬁndings and contribu-
tions to the ﬁeld. This literature review will provide a context
for multi-modal content rating research.
A. Content Moderation and Movie Datasets
There are many published datasets in the context of content
moderation and movies. Substance use dataset [24] focuses on
drug detection; however, it is not publicly available. Publicly
available HMDB-51 action recognition dataset [5] is loosely
related to the ﬁeld of video content moderation as it contains
smoking and drinking classes which can be considered as
substance use. VSD2014 [13] and Violent Scenes Dataset [12]
provide violence-related labels for Hollywood movie scenes
and annotations are publicly available. NDPI 800 [6], AIIA-
PID4 [7], NDPI 2k [17], Adult content dataset [27] and
LSPD [90] are image and video based datasets that provide
nudity and pornography detection related annotations which
are available to researchers upon request. Movie script dataset
[36] and Movie script severity dataset [72] are text (subtitle
and caption) based datasets that focus on severity and violence
detection. Lastly, LVU [71], MovieNet [50] and MM-Trailer
[68] are movie-related datasets that contain character level
relationship, background place, character bounding boxes, and
age rating per trailer however they do not contain any rating

TABLE I
LIST OF CONTENT MODERATION AND MOVIE DATASETS.
Name
Year
Modality
Task
Labels
LSPD [90]
2022
image, video
image/video classiﬁcation,
instance segmentation
porn, normal, sexy, hentai,
drawings, female/male genital,
female breast, anus
MM-Trailer [68]
2021
video
video classiﬁcation
age rating
Movienet [50]
2021
image, video, text
object detection,
video classiﬁcation
scene level actions and places,
character bboxes
Movie script
severity dataset [72]
2021
text
text classiﬁcation
frightening, mild,
moderate, severe
LVU [71]
2021
video
video classiﬁcation
relationship, place, like ration,
view count, genre, writer, year
per movie scene
Violence detection
dataset [43]
2020
video
video classiﬁcation
violent, not-violent
Movie script
dataset [36]
2019
text
text classiﬁcation
violent or not
Adult content
dataset [27]
2017
image
image classiﬁcation
nude or not
Substance use
dataset [24]
2017
image
image classiﬁcation
drug related or not
NDPI 2k [17]
2016
video
video classiﬁcation
porn or not
Violent Scenes
Dataset [12]
2014
video
video classiﬁcation
blood, ﬁre, gun, gore, ﬁght
VSD2014 [13]
2014
video
video classiﬁcation
blood, ﬁre, gun, gore, ﬁght
AIIA-PID4 [7]
2013
image
image classiﬁcation
bikini, porn, skin, non-skin
NDPI 800 [6]
2013
video
video classiﬁcation
porn or not
HMDB-51 [5]
2011
video
video classiﬁcation
smoke, drink
annotations. More information on the datasets are provided in
Table I.
B. Action Recognition
Human action recognition is an active research area and it
plays a signiﬁcant role in video understanding. Prior action
recognition works focus on 3D CNN based architectures [20]
[30] [31] [47]. A two stream approach where optical ﬂow
and frame sequence inputs are utilized is proposed in [20].
R(2+1)D convolutions, which decomposes the 3D convolution
into a 2D spatial convolution and a 1D temporal convolution
is used in [30]. In [31], a two-stream approach is proposed. It
simultaneously embeds slow and fast sampled frame sequences
from the target video clip and fuses the features extracted
from both paths. In [47], a family of efﬁcient video networks
which progressively expand a tiny 2D image classiﬁcation
architecture along multiple network axes, in space, time, width,
and depth are presented. At each step, one axis is expanded
after training and validation.
After the rise of vision transformers, many transformer-
based action recognition, and video classiﬁcation techniques
have been proposed [58] [81] [87] [85], which compare
favorably against 3D CNN based models. Since transformers
have no inductive bias about the nature of the data, they can
scale better compared to CNN based encoders provided that
large training data is available. TimeSformer [58] extends the
standard vision transformer (ViT) [46] with the addition of
temporal transformer encoder layers on top of a spatial encoder
to encode spatio-temporal information. Furthermore, Video
Swin Transformer [81] extends Swin Transformer [63] by
integrating an inductive bias of locality in videos utilizing the
pretrained weights of Swin Transformer. BEVT [87] studies
BERT pretraining for video transformers and proposes masked
pretraining [85] of the transformer encoder and holds the best
representation learning capabilities as of today.
These methods can be utilized in any video classiﬁcation
and video representation learning problems such as movie
scene content rating.
C. Multi-Modal Learning
With the increase of available multi-modal data (mainly
image, video, text modalities) and transformer-based encoders
that can unify different modalities into similar embedding
space, research around multi-modal neural networks has ac-
celerated. Multi-modal architectures can be grouped into two
main categories: asynchronous and synchronous models.
When different input modalities that are used in a training
step of a model do not align in time, this model is called
an asynchronous multi-modal architecture. In the training of
these models, independent image/video/text based datasets can
be utilized. PolyViT [62] proposes the usage of ViT-based
encoder for image, audio, and video modalities and proposes
a multi-task based pretraining with separate classiﬁcation head
for each dataset (with image/video/audio classiﬁcation tasks).

TABLE II
SUMMARY OF VIDEO BASED CONTENT MODERATION TECHNIQUES.
Paper
Year
Model
Features
Datasets
Tasks
Context
[84]
2021
separate model
for each task:
concat + LSTM,
object detector,
one-class CNN
embeddings
video frame
pixel values,
image
embeddings,
text
Nudenet,
private
dataset
profanity, violence,
nudity,
drug classiﬁcation
movie
content rating
[45]
2020
CNN + MLP
InceptionV3
image
embeddings
private dataset
cigarette
classiﬁcation
from video
general
video
content
moderation
[32]
2019
CNN + SVM
InceptionV3
image
embeddings,
AudioVGG
audio
embeddings
private dataset
inappropriate
(nudity+gore)
classiﬁcation
from video
general
video
content
moderation
[33]
2019
concat. features +
SVM, MLP
InceptionV3
image
embeddings,
AudioVGG
audio
embeddings
YouTube8M,
NDPI 800,
Cholec80
nudity
classiﬁcation
from video
e-learning
content
moderation
[38]
2019
CNN + LSTM
(late fusion)
CNN based
encoder for
image, video
and audio
spectrograms
private dataset
video
(classiﬁcation:
orignal,
fake explicit,
fake violent
social media
content
moderation
Similarly, Omnivore [78] proposes a multi-task based pretrain-
ing schema training a model on image/video classiﬁcation
and depth estimation. OmniMAE [77] also proposes a ViT-
based encoder but instead of supervised pretraining a self-
supervised masked pretraining is proposed for image and
video-based datasets. Then they ﬁne-tune the pretrained model
for downstream tasks as image and video classiﬁcation.
When a model requires different input modalities of the
same instance to be aligned in time, this model is called
synchronous multi-modal architecture. In the training of these
models, modalities should be aligned and cannot be regarded
as independent. There are works that use image + text [52] [44]
[67], video + audio [64] [65], video + optical ﬂow [10], video
+ text [82], image + depth map [74], image + audio + text
[57] [75], image + audio + optical ﬂow [88] modalities to learn
more representative multi-modal embeddings for classiﬁcation,
depth estimation, image retrieval and pretraining tasks. Some
of these works propose the usage of separate encoders per
modality [10] [67] [64] [65] [82] [57] while others propose a
single uniﬁed encoder [74] [88] [44]. Earlier works used CNN
based embedding extractors for image and video modalities
[10] while all previously mentioned other methods utilized
transformer-based encoders.
D. Movie Genre Classiﬁcation
The movie genre plays an important role in video analysis,
reﬂecting narrative elements, aesthetic approaches, and emo-
tional responses. Availability of a robust method for video
genre classiﬁcation enables a wide range of applications,
such as organizing related user videos from social networking
sites, ﬁxing mislabeled videos, extracting keyframes from long
videos, and searching for a speciﬁc movie type for recom-
mender systems. Motivated by these applications, researchers
have proposed different techniques [51] [56] [55] [54] [83]
[86] [80] [89] for genre classiﬁcation. [51] performs story-
based classiﬁcation for movie scenes using manually extracted
categorical features in a logistic regression based model. Some
methods utilize only the video frames while performing genre
classiﬁcation. [56] uses Inception v4 [25] image embeddings
in a CNN + LSTM based model, [55] utilizes a 3D CNN
based architecture to extract spatio-temporal features from
video frames, [86] uses EfﬁcientNet [39] image embeddings
in a CNN + GRU based model for genre classiﬁcation from
movie trailers. [83] proposes a KNN based multi-label movie
genre classiﬁcation technique that uses only text frequency
vectors extracted from trailer subtitles. There are some multi-
modal approaches for genre classiﬁcation. [54] proposes a
technique that uses MFCC [2] and Local Binary Pattern (LBP)
features of the audio, LBP and 3D CNN features extracted
from video frames, Inception v3 [18] embeddings of poster
images and Term Frequency-Inverse Document Frequency
(TF-IDF) features of movie synopsis for multi-label movie
genre classiﬁcation. This work proposes to train separate

TABLE III
SUMMARY OF VIDEO BASED MOVIE CONTENT RATING TECHNIQUES.
Paper
Year
Model
Features
Datasets
Tasks
Content Rating Context
[76]
2022
ViT-like
video encoder
+ MLP
ViT-like
video encoder
embedings
Private
movie scene
representation learning,
video classiﬁcation
(sex, violence,
drug-use)
movie scene
[79]
2022
CNN + GRU
+ MLP
CNN
embeddings
from video
frames
Violence
detection
dataset
violent/non-violent
classiﬁcation from
videos
movie scene
[69]
2021
multi-modal
+ multi output
concat. + MLP
CNN+LSTM
video features,
BERT text
embeddings,
MFCC audio
features
MM-Trailer
rating classiﬁcation
(red, yellow, green)
movie trailer
[49]
2020
3 CNN
model for 3
modality,
multi-label
dataset
CNN video
and audio
embeddings,
LSTM text
(subtitle)
embeddings
private dataset
gore, nudity, drug,
profanity classiﬁcation
from video and subtitle
movie scene
[37]
2019
meta-
learning
with Naive
Bayes, SVM
MFCC and
prosodic
features
from audio,
HoG and
TRoF features
from images
NDPI 2k
dataset,
VSD2014
violent and pornographic
scene localization from
video
movie scene
model for each input feature and target label which makes
the deployment and scaling harder. [80] proposes a novel
transformer-based architecture utilizing Resnet18 [15] image
embeddings and Resnet-VLAD [41] audio embeddings for
single-label news scene segmentation/classiﬁcation. [89] pro-
poses fusion of pretrained multi-modal CLIP [67] text and
image embeddings and PANN audio embedding for movie
genre classiﬁcation.
E. Sensitive Content Detection
Many movies, TV series, animations and other audio-visual
content, contain inappropriate words or visual content that is
unsuitable for particular audiences and younger generation.
On the other hand, in online platforms or in personal devices
there may be a need for ﬁltering out irrelevant, obscene,
illegal, harmful, or insulting content to emphasize useful or
informative samples. With these motivations, there have been
many ML based techniques proposed for the identiﬁcation of
sensitive content. These can be grouped into two categories
as movie content rating and content moderation. Although
target categories and motivations are similar between these
groups, content moderation deals with more open domain data
where movie content rating focuses on movies, TV series, and
trailers.
There are some works that focus on image based content
moderation [61] [66]. [61] proposes ensemble of CNN net-
works using image embeddings extracted with Mobilenet v2
[29], Densenet [23] and VGG16 [11] backbones for gore clas-
siﬁcation from images using a private dataset. [66] proposes a
network that performs SSD [16] based object detection using
Mobilenet v3 [35] backbone for on-device person detection
and nudity classiﬁcation. The source code and dataset are
not available. Moreover, some works focus on video-based
content moderation [38] [33] [32] [45]. [38] uses CNN-based
encoder for video frames and audio spectrograms and after late
fusion LSTM stage is applied to perform video classiﬁcation
for social media content moderation. [33] proposes to extract
image embeddings from Inception v3 [18] encoder and audio
embeddings from AudioVGG [22] encoder and training with
the concatenated features for nudity classiﬁcation from e-
learning videos. They use YouTube8M [14], NDPI 800 [6] and
Cholec80 [19] datasets in their work. Similarly, [32] uses In-
ception v3 [18] image embeddings and AudioVGG [22] audio
embeddings on an SVM for gore and nudity classiﬁcation from
video. [45] proposes a method for cigarette classiﬁcation from
video using Inception v3 [18] image embeddings on a CNN
based model. Summary of the video-based content moderation
papers can be seen in Table II.
Earlier movie content rating works focus on image based
rating [26] [40]. [26] proposes a technique consisting of SVM
classiﬁer, CNN image classiﬁer and rule-based decision maker
utilizing HoG and CNN features extracted from images for

rating movie frames in terms of nudity and violence. [40]
uses DCT [1] features extracted from images to train an
SVM classiﬁer for movie content rating classiﬁcation. Datasets
used in these papers are not available. Furthermore, there are
more comprehensive works that focus on multi-modal movie
content rating by utilizing video and/or text data [34] [37]
[49] [68] [72] [59] [79] [76]. [34] proposes a rule based
violent scene detection method utilizing Inception v3 [18]
image embeddings. They use Violent Scenes Dataset [12] and
a private dataset in their work. [37] proposes a method for
detecting violent and pornographic scenes using MFCC [2]
and prosodic features from audio and HOG [3] and TRoF
[17] features from images on a meta-learning based training
schema. [49] proposes a multi-label movie scene content rating
technique that contains 3 different CNN-based model for 3
different modalities (audio, video, text). Related dataset is not
available. [68] proposes a multi-modal architecture that utilizes
CNN+LSTM spatio-temporal video features, BERT [28] and
Deepmoji [21] text features and MFCC audio features for
rating movie trailers. Their dataset is available upon request.
[72] proposes a multi-task pairwise ranking & classiﬁcation
network utilizing GloVe [9], BERT and TextCNN [8] text
embeddings of the movie scripts to predict the movie script
severity. Their dataset is publicly available. [59] aims to solve
various rating tasks such as profanity, violence, nudity, drug
classiﬁcation but proposes separate network per task utilizing
raw video frame pixel values, image and text embeddings.
Most of their dataset is private and not available. [79] proposes
CNN and GRU based violent scene detection network using
the Violent Scenes Dataset [12]. None of the aforementioned
methods utilize attention-based feature extraction and fusion
methods and more recent vision and audio transformers such
as ViT [46], DEiT [70], Wav2Vec [42], Data2Vec [75]. Unlike
them, [76] proposes a movie scene representation learning
method for ViT-like video encoders and present some result
on movie scene content rating. However, the authors do not
consider multi-modal and multi-label setting and focus on
single-label classiﬁcation using only video modality as input.
A summary of the video-based movie content rating papers
can be seen in Table III.
III. CONCLUSION
In
this
paper,
related
works
are
summarized
for
content
action
classiﬁcation,
multi-modal
learning,
movie
genre
classiﬁcation,
and
sensitive
content
detection
in
the
context
of
content
moderation
and
movie content rating. The project page is available at
https://github.com/fcakyon/content-moderation-deep-learning
and will be updated as new papers are published.
REFERENCES
[1]
N. Ahmed, T. Natarajan, and K. R. Rao, “Discrete
cosine transform,” IEEE transactions on Computers,
vol. 100, no. 1, pp. 90–93, 1974.
[2]
S. Davis and P. Mermelstein, “Comparison of paramet-
ric representations for monosyllabic word recognition in
continuously spoken sentences,” IEEE transactions on
acoustics, speech, and signal processing, vol. 28, no. 4,
pp. 357–366, 1980.
[3]
N. Dalal and B. Triggs, “Histograms of oriented gra-
dients for human detection,” in 2005 IEEE computer
society conference on computer vision and pattern
recognition (CVPR’05), Ieee, vol. 1, 2005, pp. 886–893.
[4]
M. P. A. of America, Classiﬁcation and rating rules,
2010.
[5]
H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and
T. Serre, “Hmdb: A large video database for human
motion recognition,” in 2011 International conference
on computer vision, IEEE, 2011, pp. 2556–2563.
[6]
S. Avila, N. Thome, M. Cord, E. Valle, and A. D. A.
Ara´uJo, “Pooling in image representation: The visual
codeword point of view,” Computer Vision and Image
Understanding, vol. 117, no. 5, pp. 453–465, 2013.
[7]
S. Karavarsamis, N. Ntarmos, K. Blekas, and I. Pitas,
“Detecting pornographic images by localizing skin
rois,” International Journal of Digital Crime and Foren-
sics (IJDCF), vol. 5, no. 1, pp. 39–53, 2013.
[8]
Y. Kim, “Convolutional neural networks for sentence
classiﬁcation,” in Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing
(EMNLP), 2014, pp. 1746–1751.
[9]
J. Pennington, R. Socher, and C. D. Manning, “Glove:
Global vectors for word representation,” in Proceedings
of the 2014 conference on empirical methods in natural
language processing (EMNLP), 2014, pp. 1532–1543.
[10]
K. Simonyan and A. Zisserman, “Two-stream convo-
lutional networks for action recognition in videos,”
Advances in neural information processing systems,
vol. 27, 2014.
[11]
——, “Very deep convolutional networks for large-scale
image recognition,” arXiv preprint arXiv:1409.1556,
2014.
[12]
C.-H. Demarty, C. Penet, M. Soleymani, and G. Gravier,
“Vsd, a public dataset for the detection of violent scenes
in movies: Design, annotation, analysis and evaluation,”
Multimedia Tools and Applications, vol. 74, no. 17,
pp. 7379–7404, 2015.
[13]
M. Schedi, M. Sj¨oberg, I. Mironic˘a, B. Ionescu, V. L.
Quang, Y.-G. Jiang, and C.-H. Demarty, “Vsd2014: A
dataset for violent scenes detection in hollywood movies
and web videos,” in 2015 13th International Workshop
on Content-Based Multimedia Indexing (CBMI), IEEE,
2015, pp. 1–6.
[14]
S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G.
Toderici, B. Varadarajan, and S. Vijayanarasimhan,
“Youtube-8m: A large-scale video classiﬁcation bench-
mark,” arXiv preprint arXiv:1609.08675, 2016.
[15]
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual
learning for image recognition,” in Proceedings of the

IEEE conference on computer vision and pattern recog-
nition, 2016, pp. 770–778.
[16]
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed,
C.-Y. Fu, and A. C. Berg, “Ssd: Single shot multibox
detector,” in European conference on computer vision,
Springer, 2016, pp. 21–37.
[17]
D. Moreira, S. Avila, M. Perez, D. Moraes, V. Testoni,
E. Valle, S. Goldenstein, and A. Rocha, “Pornography
classiﬁcation: The hidden clues in video space–time,”
Forensic science international, vol. 268, pp. 46–61,
2016.
[18]
C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and
Z. Wojna, “Rethinking the inception architecture for
computer vision,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, 2016,
pp. 2818–2826.
[19]
A. P. Twinanda, S. Shehata, D. Mutter, J. Marescaux,
M. De Mathelin, and N. Padoy, “Endonet: A deep ar-
chitecture for recognition tasks on laparoscopic videos,”
IEEE transactions on medical imaging, vol. 36, no. 1,
pp. 86–97, 2016.
[20]
J. Carreira and A. Zisserman, “Quo vadis, action recog-
nition? a new model and the kinetics dataset,” in pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2017, pp. 6299–6308.
[21]
B.
Felbo,
A.
Mislove,
A.
Søgaard,
I.
Rahwan,
and S. Lehmann, “Using millions of emoji occur-
rences to learn any-domain representations for detect-
ing sentiment, emotion and sarcasm,” arXiv preprint
arXiv:1708.00524, 2017.
[22]
S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke,
A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A.
Saurous, B. Seybold, et al., “Cnn architectures for large-
scale audio classiﬁcation,” in 2017 ieee international
conference on acoustics, speech and signal processing
(icassp), IEEE, 2017, pp. 131–135.
[23]
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Wein-
berger, “Densely connected convolutional networks,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2017, pp. 4700–4708.
[24]
A. Roy, A. Paul, H. Pirsiavash, and S. Pan, “Automated
detection of substance use-related social media posts
based on image and text analysis,” in 2017 IEEE
29th International Conference on Tools with Artiﬁcial
Intelligence (ICTAI), IEEE, 2017, pp. 772–779.
[25]
C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi,
“Inception-v4, inception-resnet and the impact of resid-
ual connections on learning,” in Thirty-ﬁrst AAAI con-
ference on artiﬁcial intelligence, 2017.
[26]
K. N. Tofa, F. Ahmed, A. Shakil, et al., “Inappropriate
scene detection in a video stream,” Ph.D. dissertation,
BRAC University, 2017.
[27]
T. Connie, M. Al-Shabi, and M. Goh, “Smart content
recognition from images using a mixture of convolu-
tional neural networks,” in IT Convergence and Security
2017, Springer, 2018, pp. 11–18.
[28]
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
“Bert: Pre-training of deep bidirectional transform-
ers
for
language
understanding,”
arXiv
preprint
arXiv:1810.04805, 2018.
[29]
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and
L.-C. Chen, “Mobilenetv2: Inverted residuals and linear
bottlenecks,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2018,
pp. 4510–4520.
[30]
D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and
M. Paluri, “A closer look at spatiotemporal convolutions
for action recognition,” in Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition,
2018, pp. 6450–6459.
[31]
C. Feichtenhofer, H. Fan, J. Malik, and K. He, “Slowfast
networks for video recognition,” in Proceedings of the
IEEE/CVF international conference on computer vision,
2019, pp. 6202–6211.
[32]
P. V. de Freitas, P. R. Mendes, G. N. dos Santos, A. J. G.
Busson, ´A. L. Guedes, S. Colcher, and R. L. Milidi´u,
“A multimodal cnn-based tool to censure inappropriate
video scenes,” arXiv preprint arXiv:1911.03974, 2019.
[33]
P. V. de Freitas, G. N. d. Santos, A. J. Busson, ´A. L.
Guedes, and S. Colcher, “A baseline for nsfw video
detection in e-learning environments,” in Proceedings
of the 25th Brazillian Symposium on Multimedia and
the Web, 2019, pp. 357–360.
[34]
M. Gruosso, N. Capece, U. Erra, and N. Lopardo, “A
deep learning approach for the motion picture content
rating,” in 2019 10th IEEE International Conference
on Cognitive Infocommunications (CogInfoCom), IEEE,
2019, pp. 137–142.
[35]
A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen,
M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, et
al., “Searching for mobilenetv3,” in Proceedings of the
IEEE/CVF international conference on computer vision,
2019, pp. 1314–1324.
[36]
V. R. Martinez, K. Somandepalli, K. Singla, A. Ra-
makrishna, Y. T. Uhls, and S. Narayanan, “Violence
rating prediction from movie scripts,” in Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, vol. 33,
2019, pp. 671–678.
[37]
D. Moreira, S. Avila, M. Perez, D. Moraes, V. Testoni,
E. Valle, S. Goldenstein, and A. Rocha, “Multimodal
data fusion for sensitive scene localization,” Information
Fusion, vol. 45, pp. 307–323, 2019.
[38]
R. Tahir, F. Ahmed, H. Saeed, S. Ali, F. Zaffar, and
C. Wilson, “Bringing the kid back into youtube kids:
Detecting inappropriate content on video streaming
platforms,” in 2019 IEEE/ACM International Confer-
ence on Advances in Social Networks Analysis and
Mining (ASONAM), IEEE, 2019, pp. 464–469.
[39]
M. Tan and Q. Le, “Efﬁcientnet: Rethinking model
scaling for convolutional neural networks,” in Interna-
tional conference on machine learning, PMLR, 2019,
pp. 6105–6114.

[40]
G. Vishwakarma and G. S. Thakur, “Hybrid system
for mpaa ratings of movie clips using support vec-
tor machine,” in Soft Computing for Problem Solving,
Springer, 2019, pp. 563–575.
[41]
W. Xie, A. Nagrani, J. S. Chung, and A. Zisserman,
“Utterance-level aggregation for speaker recognition in
the wild,” in ICASSP 2019-2019 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP), IEEE, 2019, pp. 5791–5795.
[42]
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli,
“Wav2vec 2.0: A framework for self-supervised learn-
ing of speech representations,” Advances in Neural
Information Processing Systems, vol. 33, pp. 12 449–
12 460, 2020.
[43]
M. Bianculli, N. Falcionelli, P. Sernani, S. Tomassini, P.
Contardo, M. Lombardi, and A. F. Dragoni, “A dataset
for automatic violence detection in videos,” Data in
brief, vol. 33, p. 106 587, 2020.
[44]
Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z.
Gan, Y. Cheng, and J. Liu, “Uniter: Universal image-
text representation learning,” in European conference on
computer vision, Springer, 2020, pp. 104–120.
[45]
S. Dhanwal, V. Bhaskar, and T. Agarwal, “Automated
censoring of cigarettes in videos using deep learning
techniques,” in Decision Analytics Applications in In-
dustry, Springer, 2020, pp. 339–348.
[46]
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-
senborn, X. Zhai, T. Unterthiner, M. Dehghani, M.
Minderer, G. Heigold, S. Gelly, et al., “An image is
worth 16x16 words: Transformers for image recognition
at scale,” arXiv preprint arXiv:2010.11929, 2020.
[47]
C. Feichtenhofer, “X3d: Expanding architectures for
efﬁcient video recognition,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2020, pp. 203–213.
[48]
G. Gajula, A. Hundiwale, S. Mujumdar, and L. Saritha,
“A machine learning based adult content detection using
support vector machine,” in 2020 7th International
Conference on Computing for Sustainable Global De-
velopment (INDIACom), IEEE, 2020, pp. 181–185.
[49]
R. Gunawan and Y. Kristian, “Automatic parental guide
scene classiﬁcation menggunakan metode deep convo-
lutional neural network dan lstm,” INSYST: Journal
of Intelligent System and Computation, vol. 2, no. 2,
pp. 86–90, 2020.
[50]
Q. Huang, Y. Xiong, A. Rao, J. Wang, and D. Lin,
“Movienet: A holistic dataset for movie understanding,”
in European Conference on Computer Vision, Springer,
2020, pp. 709–727.
[51]
C. Liu, A. Shmilovici, and M. Last, “Towards story-
based classiﬁcation of movie scenes,” PloS one, vol. 15,
no. 2, e0228579, 2020.
[52]
J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee,
“12-in-1: Multi-task vision and language representation
learning,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2020,
pp. 10 437–10446.
[53]
H. S. Lyn, S. Mansor, N. AlDahoul, and H. A. Karim,
“No. 5 convolutional neural network-based transfer
learning and classiﬁcation of visual contents for ﬁlm
censorship,” Journal of Engineering Technology and
Applied Physics, vol. 2, no. 2, pp. 28–35, 2020.
[54]
R. B. Mangolin, R. M. Pereira, A. S. Britto, C. N.
Silla, V. D. Feltrim, D. Bertolini, and Y. M. Costa, “A
multimodal approach for multi-label movie genre classi-
ﬁcation,” Multimedia Tools and Applications, pp. 1–26,
2020.
[55]
P. G. Shambharkar, P. Thakur, S. Imadoddin, S.
Chauhan, and M. Doja, “Genre classiﬁcation of movie
trailers using 3d convolutional neural networks,” in
2020 4th International Conference on Intelligent Com-
puting and Control Systems (ICICCS), IEEE, 2020,
pp. 850–858.
[56]
A. Yadav and D. K. Vishwakarma, “A uniﬁed frame-
work of deep networks for genre classiﬁcation us-
ing movie trailer,” Applied Soft Computing, vol. 96,
p. 106 624, 2020.
[57]
H. Akbari, L. Yuan, R. Qian, W.-H. Chuang, S.-F.
Chang, Y. Cui, and B. Gong, “Vatt: Transformers for
multimodal self-supervised learning from raw video,
audio and text,” Advances in Neural Information Pro-
cessing Systems, vol. 34, pp. 24 206–24221, 2021.
[58]
G. Bertasius, H. Wang, and L. Torresani, “Is space-
time attention all you need for video understanding?” In
International Conference on Machine Learning, PMLR,
2021, pp. 813–824.
[59]
Z. X. Chai, “Automatic parental guide ratings for short
movies,” Ph.D. dissertation, UTAR, 2021.
[60]
A. Khaksar Pour, W. Chaw Seng, S. Palaiahnakote,
H. Tahaei, and N. B. Anuar, “A survey on video
content rating: Taxonomy, challenges and open issues,”
Multimedia Tools and Applications, vol. 80, no. 16,
pp. 24 121–24145, 2021.
[61]
W. LAROCQUE, “Gore classiﬁcation and censoring in
images,” Ph.D. dissertation, University of Ottawa, 2021.
[62]
V. Likhosherstov, A. Arnab, K. Choromanski, M. Lucic,
Y. Tay, A. Weller, and M. Dehghani, “Polyvit: Co-
training vision transformers on images, videos and
audio,” arXiv preprint arXiv:2111.12993, 2021.
[63]
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S.
Lin, and B. Guo, “Swin transformer: Hierarchical vision
transformer using shifted windows,” in Proceedings of
the IEEE/CVF International Conference on Computer
Vision, 2021, pp. 10 012–10022.
[64]
P. Morgado, I. Misra, and N. Vasconcelos, “Robust
audio-visual instance discrimination,” in Proceedings
of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2021, pp. 12 934–12945.
[65]
P. Morgado, N. Vasconcelos, and I. Misra, “Audio-
visual instance discrimination with cross-modal agree-
ment,” in Proceedings of the IEEE/CVF Conference

on Computer Vision and Pattern Recognition, 2021,
pp. 12 475–12486.
[66]
A. Pandey, S. Moharana, D. P. Mohanty, A. Panwar, D.
Agarwal, and S. P. Thota, “On-device content modera-
tion,” in 2021 International Joint Conference on Neural
Networks (IJCNN), IEEE, 2021, pp. 1–7.
[67]
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,
S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,
et al., “Learning transferable visual models from natural
language supervision,” in International Conference on
Machine Learning, PMLR, 2021, pp. 8748–8763.
[68]
M. Shafaei, C. Smailis, I. Kakadiaris, and T. Solorio, “A
case study of deep learning-based multi-modal methods
for labeling the presence of questionable content in
movie trailers,” in Proceedings of the International
Conference on Recent Advances in Natural Language
Processing (RANLP 2021), 2021, pp. 1297–1307.
[69]
——, “A case study of deep learning-based multi-
modal methods for labeling the presence of questionable
content in movie trailers,” in Proceedings of the Interna-
tional Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2021), 2021, pp. 1297–1307.
[70]
H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablay-
rolles, and H. J´egou, “Training data-efﬁcient image
transformers & distillation through attention,” in In-
ternational Conference on Machine Learning, PMLR,
2021, pp. 10 347–10357.
[71]
C.-Y. Wu and P. Krahenbuhl, “Towards long-form video
understanding,” in Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition,
2021, pp. 1884–1894.
[72]
Y. Zhang, M. Shafaei, F. Gonzalez, and T. Solorio,
“From none to severe: Predicting severity in movie
scripts,” arXiv preprint arXiv:2109.09276, 2021.
[73]
S. Afsha, M. Haque, and H. Nyeem, “Machine learning
models for content classiﬁcation in ﬁlm censorship and
rating,” in 2022 International Conference on Innova-
tions in Science, Engineering and Technology (ICISET),
IEEE, 2022, pp. 396–401.
[74]
R. Bachmann, D. Mizrahi, A. Atanov, and A. Za-
mir, “Multimae: Multi-modal multi-task masked autoen-
coders,” arXiv preprint arXiv:2204.01678, 2022.
[75]
A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and
M. Auli, “Data2vec: A general framework for self-
supervised learning in speech, vision and language,”
arXiv preprint arXiv:2202.03555, 2022.
[76]
S.
Chen,
X.
Hao,
X.
Nie,
and
R.
Hamid,
“Movies2scenes: Learning scene representations using
movie similarities,” arXiv preprint arXiv:2202.10650,
2022.
[77]
R. Girdhar, A. El-Nouby, M. Singh, K. V. Alwala, A.
Joulin, and I. Misra, “Omnimae: Single model masked
pretraining on images and videos,” arXiv preprint
arXiv:2206.08356, 2022.
[78]
R. Girdhar, M. Singh, N. Ravi, L. van der Maaten,
A. Joulin, and I. Misra, “Omnivore: A single model
for many visual modalities,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 16 102–16112.
[79]
M. HAQUE, “Detection and classiﬁcation of sensitive
audio-visual content for automated ﬁlm censorship and
rating,” 2022.
[80]
Y. Liu, L. Qiao, D. Yin, Z. Jiang, X. Jiang, D. Jiang, and
B. Ren, “Os-msl: One stage multimodal sequential link
framework for scene segmentation and classiﬁcation,” in
Proceedings of the 30th ACM International Conference
on Multimedia, 2022, pp. 6269–6277.
[81]
Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and
H. Hu, “Video swin transformer,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 3202–3211.
[82]
B. Ni, H. Peng, M. Chen, S. Zhang, G. Meng, J. Fu,
S. Xiang, and H. Ling, “Expanding language-image
pretrained models for general video recognition,” in
European Conference on Computer Vision, Springer,
2022, pp. 1–18.
[83]
N. K. Rajput and B. A. Grover, “A multi-label movie
genre classiﬁcation scheme based on the movie’s sub-
titles,” Multimedia Tools and Applications, pp. 1–22,
2022.
[84]
D. Son, B. Lew, K. Choi, Y. Baek, S. Choi, B. Shin,
S. Ha, and B. Chang, “Reliable decision from multiple
subtasks through threshold optimization: Content mod-
eration in the wild,” arXiv preprint arXiv:2208.07522,
2022.
[85]
Z. Tong, Y. Song, J. Wang, and L. Wang, “Video-
mae: Masked autoencoders are data-efﬁcient learners
for self-supervised video pre-training,” arXiv preprint
arXiv:2203.12602, 2022.
[86]
I. T¨urk¨oz and H. A. G¨uvenir, “Detection of animated
scenes among movie trailers,” EasyChair, Tech. Rep.,
2022.
[87]
R. Wang, D. Chen, Z. Wu, Y. Chen, X. Dai, M.
Liu, Y.-G. Jiang, L. Zhou, and L. Yuan, “Bevt: Bert
pretraining of video transformers,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 14 733–14743.
[88]
X. Xiong, A. Arnab, A. Nagrani, and C. Schmid, “M&m
mix: A multimodal multiview transformer ensemble,”
arXiv preprint arXiv:2206.09852, 2022.
[89]
Z. Zhang, Y. Gu, B. A. Plummer, X. Miao, J. Liu,
and H. Wang, “Effectively leveraging multi-modal fea-
tures for movie genre classiﬁcation,” arXiv preprint
arXiv:2203.13281, 2022.
[90]
D. D. Phan, T. T. Nguyen, Q. H. Nguyen, H. L. Tran,
K. N. K. Nguyen, and D. L. Vu, “Lspd: A large-scale
pornographic dataset for detection and classiﬁcation,”

This figure "fig1.png" is available in "png"
 format from:
http://arxiv.org/ps/2212.04533v2

