Siu-Kui Au
Operational Modal Analysis
Modeling, Bayesian Inference,
Uncertainty Laws
123

Siu-Kui Au
Center for Engineering Dynamics
and Institute for Risk and Uncertainty
University of Liverpool
Liverpool
UK
ISBN 978-981-10-4117-4
ISBN 978-981-10-4118-1
(eBook)
DOI 10.1007/978-981-10-4118-1
Library of Congress Control Number: 2017933941
© Springer Nature Singapore Pte Ltd. 2017
This Springer imprint is published by Springer Nature
The registered company is Springer Nature Singapore Pte Ltd.
The registered company address is:
152 Beach Road, #21-01/04 Gateway East, Singapore 189721, Singapore

Preface
Operational modal analysis (OMA) aims at identifying the modal properties
(natural frequencies, damping ratios, mode shapes, etc.) of a structure using
vibration response measurements only. The possibility of doing so without knowing
the input forces is exciting from both a theoretical and practical point of view. It
signiﬁcantly enhances the feasibility and reduces the cost of obtaining in-situ
information about structures. Together with the advent of sensing, data acquisition
and computer technology, the bar to enter the community of vibration testing or
related research is disappearing. Activities are booming in all directions, e.g., theory
development, experimentation and applications; and spanning across civil,
mechanical, aerospace and electrical power engineering.
I started exploring OMA in an industrial project in 2008 when I was looking for
a conventional method to identify the modal properties of several tall buildings
from acceleration time histories during typhoons. At that time the literature was
already quite mature, with many smart ideas of coming up with an estimate for
modal properties from apparently random vibration time histories that one can
hardly make any sense of. After experimenting with different methods and data
under various situations (some similar and some different) I soon realized that there
was a signiﬁcant scatter in the estimates, despite the fact that the ﬁt between the
theoretical and empirical statistics (from which the estimates were obtained)
appeared quite reasonable in each case. Damping ratio was the quantity of most
interest in the project, but its scatter was quite large, which inevitably undermined
the value of the work.
The main issue with the scatter was perhaps not so much with its existence but
rather with the lack of understanding, which motivated subsequent research in
OMA. The objective is not so much to come up with another estimate for the modal
properties (there are already many methods) but rather to have a scientiﬁcally sound
method that tells how large their ‘uncertainty’ is; and, if possible, how it depends on
the factors that can be quantiﬁed.
Uncertainty is as a lack of information. Using probability for quantiﬁcation,
Bayes’ theorem provides the fundamental principle for processing information and
hence uncertainty. It yields the probability distribution of modal properties

conditional on available data and modeling assumptions, naturally addressing both
estimation and uncertainty quantiﬁcation in OMA. The concept is metaphysical but
very useful for decision making. The mathematics is not trivial, but it can be
worked out and made efﬁcient in the form of computer algorithms. After some years
of development, I believe the algorithms for Bayesian OMA are now ready for
industrial applications and the theory allows us to understand how identiﬁcation
uncertainty depends on test conﬁguration. Of course, there are still many
uncertainty-related problems in OMA and applications that have yet to be
addressed. It will require concerted efforts in the future.
Adopting a Bayesian perspective means that we need to be clear about what
assumptions we make on OMA data, because our conclusions are all conditional on
them. Results are as good as assumptions. This is generally true but is particularly
important to bear in mind when making Bayesian inference. It is tempting to
believe that the calculated uncertainties are universal numbers; they are not.
Throughout this book, the assumptions on OMA data are invariably the same. They
are the conventional ones used in structural dynamic analysis and design, e.g.,
linear classically damped dynamics and stochastic stationary response.
This is intended to be a book for learning the basic assumptions and probabilistic
modeling of ambient vibration data, how to make Bayesian inference effectively,
and identiﬁcation uncertainty in OMA. The primary audience are graduate students
and researchers, although practitioners should also ﬁnd the application part of the
book a useful reference. Even if Bayesian OMA algorithms may not be adopted,
I hope the book can still arouse interests in ﬁeld testing and provide materials for
understanding ambient vibration data and uncertainties in OMA.
Liverpool, UK
Siu-Kui Au
November 2016

Contents
Part I
Modeling
1
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
Vibration Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.1
Free Vibration Test . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.1.2
Forced Vibration Test . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.1.3
Ambient Vibration Test . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.2
Uncertainties. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.2.1
Variability and Identiﬁcation Uncertainty . . . . . . . . . . .
9
1.2.2
Sources of Identiﬁcation Uncertainty. . . . . . . . . . . . . . .
10
1.3
OMA Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.4
Non-Bayesian Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.4.1
Eliminating Random Response . . . . . . . . . . . . . . . . . . .
12
1.4.2
Exploiting Statistics. . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.4.3
Identiﬁcation Uncertainty. . . . . . . . . . . . . . . . . . . . . . . .
14
1.5
Bayesian Approach. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.5.1
Philosophy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.5.2
Posterior Distribution and Statistics . . . . . . . . . . . . . . . .
16
1.5.3
Computing Posterior Statistics. . . . . . . . . . . . . . . . . . . .
17
1.5.4
Formulations and Algorithms . . . . . . . . . . . . . . . . . . . .
17
1.5.5
Maximum Likelihood Estimation . . . . . . . . . . . . . . . . .
18
1.5.6
Drawbacks and Limitations . . . . . . . . . . . . . . . . . . . . . .
18
1.6
Overview of This Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.6.1
Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.6.2
Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.6.3
Algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
1.6.4
Uncertainty Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
1.7
How to Use This Book. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.7.1
Student . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.7.2
Researcher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24

1.7.3
Practitioner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
1.7.4
Supporting Resources . . . . . . . . . . . . . . . . . . . . . . . . . .
25
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2
Spectral Analysis of Deterministic Process . . . . . . . . . . . . . . . . . . . .
29
2.1
Periodic Process (Fourier Series) . . . . . . . . . . . . . . . . . . . . . . . .
30
2.1.1
Complex Exponential Form. . . . . . . . . . . . . . . . . . . . . .
32
2.1.2
Parseval Equality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.2
Non-periodic Process (Fourier Transform) . . . . . . . . . . . . . . . . .
35
2.2.1
From Fourier Series to Fourier Transform . . . . . . . . . . .
35
2.2.2
Properties of Fourier Transform. . . . . . . . . . . . . . . . . . .
37
2.2.3
Dirac Delta Function . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.2.4
Parseval Equality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.3
Discrete-Time Approximation with FFT . . . . . . . . . . . . . . . . . . .
39
2.3.1
Fast Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.3.2
Approximating Fourier Transform
and Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.3.3
Parseval Equality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.4
Distortions in Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.4.1
Nyquist Frequency . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
2.4.2
Aliasing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
2.4.3
Leakage. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
2.5
Distortions in Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . .
49
2.6
Summary of FFT Approximations . . . . . . . . . . . . . . . . . . . . . . .
50
2.7
Summary of Fourier Formulas, Units and Conventions . . . . . . .
50
2.7.1
Multiplier in Fourier Transform. . . . . . . . . . . . . . . . . . .
50
2.8
Connecting Theory with Matlab . . . . . . . . . . . . . . . . . . . . . . . . .
53
2.9
FFT Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
2.9.1
Basic Idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.9.2
Computational Effort . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3
Structural Dynamics and Modal Testing . . . . . . . . . . . . . . . . . . . . . .
59
3.1
SDOF Dynamics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.1.1
Natural Frequency . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3.1.2
Damping Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
3.1.3
Damped Free Vibration . . . . . . . . . . . . . . . . . . . . . . . . .
63
3.1.4
Logarithmic Decrement Method . . . . . . . . . . . . . . . . . .
67
3.1.5
Harmonic Excitation . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
3.1.6
Simplifying Algebra with Complex Number . . . . . . . . .
71
3.1.7
Dynamic Ampliﬁcation . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.1.8
Half-Power Bandwidth Method . . . . . . . . . . . . . . . . . . .
74
3.1.9
Principle of Superposition . . . . . . . . . . . . . . . . . . . . . . .
77
3.1.10
Periodic Excitation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78

3.1.11
Ideal Impulse Excitation . . . . . . . . . . . . . . . . . . . . . . . .
80
3.1.12
Arbitrary Excitation. . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
3.1.13
Summary of SDOF Response . . . . . . . . . . . . . . . . . . . .
83
3.2
MDOF Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.2.1
Natural Frequencies and Mode Shapes . . . . . . . . . . . . .
86
3.2.2
Eigenvalue Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
3.2.3
Modal Superposition and Classical Damping. . . . . . . . .
92
3.2.4
Rayleigh Quotient . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
3.3
Remarks on Damping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
3.4
Harmonic Load Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
3.4.1
Collocated Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
3.4.2
Least Squares Approach . . . . . . . . . . . . . . . . . . . . . . . .
103
3.5
Impact Hammer Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
3.5.1
Frequency Response . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
3.5.2
Least Squares Approach . . . . . . . . . . . . . . . . . . . . . . . .
107
3.5.3
Covering DOFs in Multiple Setups . . . . . . . . . . . . . . . .
108
3.6
State-Space Approach. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
3.6.1
Matrix Exponential . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
3.6.2
Eigenvalue Properties of System Matrix . . . . . . . . . . . .
113
3.7
Time Integration Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
3.7.1
Numerical Stability and Accuracy . . . . . . . . . . . . . . . . .
117
3.7.2
Discrete-Time State-Space Analysis . . . . . . . . . . . . . . .
120
3.8
Newmark Scheme. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
3.8.1
SDOF Linear Acceleration . . . . . . . . . . . . . . . . . . . . . .
123
3.8.2
SDOF General Scheme . . . . . . . . . . . . . . . . . . . . . . . . .
124
3.8.3
General MDOF Scheme . . . . . . . . . . . . . . . . . . . . . . . .
125
3.8.4
Parameters and Numerical Stability . . . . . . . . . . . . . . . .
126
3.8.5
Derivation of Stability Criterion . . . . . . . . . . . . . . . . . .
127
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
4
Spectral Analysis of Stationary Stochastic Process . . . . . . . . . . . . . .
133
4.1
Correlation Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
4.2
Power Spectral Density. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
4.3
Fourier Series, Fourier Transform and PSD . . . . . . . . . . . . . . . .
137
4.4
Continuous-Time Sample Process. . . . . . . . . . . . . . . . . . . . . . . .
140
4.4.1
Sample Correlation Function . . . . . . . . . . . . . . . . . . . . .
140
4.4.2
Sample Power Spectral Density. . . . . . . . . . . . . . . . . . .
142
4.4.3
Wiener-Khinchin Formula . . . . . . . . . . . . . . . . . . . . . . .
142
4.4.4
Parseval Equality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
4.4.5
White Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
4.5
Discrete-Time Sample Process . . . . . . . . . . . . . . . . . . . . . . . . . .
147
4.5.1
Sample Correlation Function . . . . . . . . . . . . . . . . . . . . .
147
4.5.2
Sample Power Spectral Density. . . . . . . . . . . . . . . . . . .
149

4.5.3
Wiener-Khinchin Formula . . . . . . . . . . . . . . . . . . . . . . .
150
4.5.4
Parseval Equality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
4.6
Averaging Sample PSD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
4.7
Distortions in Sample Estimators . . . . . . . . . . . . . . . . . . . . . . . .
156
4.7.1
Sample Correlation Function . . . . . . . . . . . . . . . . . . . . .
156
4.7.2
Sample PSD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
4.8
Second Order Statistics of Scaled DTFT . . . . . . . . . . . . . . . . . .
163
4.8.1
Complex Covariance and Pseudo-covariance
Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
4.8.2
Convolution Formula. . . . . . . . . . . . . . . . . . . . . . . . . . .
164
4.8.3
Long-Data Asymptotics of Scaled FFT . . . . . . . . . . . . .
167
4.8.4
How Long Is Long? . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
4.9
Asymptotic Distribution of Scaled FFT . . . . . . . . . . . . . . . . . . .
171
4.10
Asymptotic Distribution of Sample PSD. . . . . . . . . . . . . . . . . . .
172
4.10.1
Scalar Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
4.11
Summary of Fourier Formulas, Units and Conventions . . . . . . .
174
4.11.1
Multiplier in Wiener-Khinchin Formula . . . . . . . . . . . .
174
4.11.2
One-Sided Versus Two-Sided Spectrum . . . . . . . . . . . .
174
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
5
Stochastic Structural Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
5.1
Stationary SDOF Response. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
5.1.1
Scaled Fourier Transform . . . . . . . . . . . . . . . . . . . . . . .
181
5.1.2
Power Spectral Density . . . . . . . . . . . . . . . . . . . . . . . . .
182
5.1.3
Response Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
5.1.4
Response to White Noise . . . . . . . . . . . . . . . . . . . . . . .
184
5.2
Stationary MDOF Response . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
5.2.1
Scaled Fourier Transform . . . . . . . . . . . . . . . . . . . . . . .
186
5.2.2
Power Spectral Density . . . . . . . . . . . . . . . . . . . . . . . . .
187
5.2.3
Response Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
5.2.4
Mode Shape Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
5.3
Transient Response Variance . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
5.3.1
Governing Equation. . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
5.3.2
Solution Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
5.3.3
Limiting Stationary Value . . . . . . . . . . . . . . . . . . . . . . .
194
5.3.4
Response to White Noise . . . . . . . . . . . . . . . . . . . . . . .
195
5.4
Transient Response Correlation . . . . . . . . . . . . . . . . . . . . . . . . .
197
5.4.1
Governing Equation. . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
5.4.2
Limiting Stationary Value . . . . . . . . . . . . . . . . . . . . . . .
199
5.4.3
Response to White Noise . . . . . . . . . . . . . . . . . . . . . . .
201
5.5
Summary of Theories and Connections . . . . . . . . . . . . . . . . . . .
203
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204

6
Measurement Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
6.1
Data Acquisition Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
6.2
Channel Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
206
6.3
Sensor/Hardware Noise. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
6.4
Sensor Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
6.5
Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
6.6
Quantization Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
6.6.1
Statistical Properties . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
6.6.2
Power Spectral Density . . . . . . . . . . . . . . . . . . . . . . . . .
215
6.7
Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
6.8
Channel Noise Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218
6.8.1
Base Isolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
6.8.2
Huddle Test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
6.8.3
Three Channel Analysis. . . . . . . . . . . . . . . . . . . . . . . . .
222
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
224
7
Ambient Data Modeling and Analysis . . . . . . . . . . . . . . . . . . . . . . . .
225
7.1
Resonance Band Characteristics . . . . . . . . . . . . . . . . . . . . . . . . .
226
7.1.1
Single Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
7.1.2
Multi-mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228
7.2
PSD Spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228
7.2.1
Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
7.3
Singular Value Spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
7.3.1
Single Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
7.3.2
Multi-mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
234
7.4
Illustration with Field Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
7.4.1
Time Histories. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
7.4.2
Sample PSD (No Averaging). . . . . . . . . . . . . . . . . . . . .
238
7.4.3
Sample PSD (Averaged) . . . . . . . . . . . . . . . . . . . . . . . .
239
7.4.4
Singular Value Spectrum. . . . . . . . . . . . . . . . . . . . . . . .
240
7.5
Asynchronous Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
7.5.1
Two Measurement Groups. . . . . . . . . . . . . . . . . . . . . . .
242
7.5.2
Multiple Measurement Groups . . . . . . . . . . . . . . . . . . .
247
7.6
Microtremor Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249
7.6.1
Background Seismic Noise . . . . . . . . . . . . . . . . . . . . . .
249
7.6.2
Site Ampliﬁcation and H/V Spectrum . . . . . . . . . . . . . .
252
7.7
Simulation of Ambient Data . . . . . . . . . . . . . . . . . . . . . . . . . . . .
255
7.7.1
Gaussian Scalar Process . . . . . . . . . . . . . . . . . . . . . . . .
255
7.7.2
Gaussian Vector Process . . . . . . . . . . . . . . . . . . . . . . . .
258
7.7.3
Quantifying Noise Level . . . . . . . . . . . . . . . . . . . . . . . .
260
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261

Part II
Inference
8
Bayesian Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265
8.1
Bayes’ Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
266
8.2
Updating Knowledge Using Data . . . . . . . . . . . . . . . . . . . . . . . .
267
8.3
System Identiﬁcation Framework . . . . . . . . . . . . . . . . . . . . . . . .
268
8.4
Identiﬁability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
268
8.5
Globally Identiﬁable Problems . . . . . . . . . . . . . . . . . . . . . . . . . .
274
8.5.1
Quality of Gaussian Approximation . . . . . . . . . . . . . . .
275
8.6
Locally Identiﬁable Problems . . . . . . . . . . . . . . . . . . . . . . . . . . .
283
8.7
Unidentiﬁable Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
284
8.8
Model Class Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
8.8.1
Comparing Model Classes with Evidence . . . . . . . . . . .
285
8.8.2
Model Trade-off . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
286
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
288
9
Classical Statistical Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
291
9.1
Statistical Estimators. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
293
9.1.1
Quality Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
293
9.1.2
Bias and Convergence. . . . . . . . . . . . . . . . . . . . . . . . . .
294
9.1.3
Empirical Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
294
9.2
Maximum Likelihood Estimator . . . . . . . . . . . . . . . . . . . . . . . . .
295
9.3
Cramér-Rao Bound. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
300
9.3.1
Easier but Looser Bounds . . . . . . . . . . . . . . . . . . . . . . .
307
9.3.2
General Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
9.3.3
Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
9.4
Fisher Information Matrix for Gaussian Data . . . . . . . . . . . . . . .
312
9.4.1
Real Gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
312
9.4.2
Complex Gaussian. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
315
9.5
Asymptotic Properties of ML Estimator . . . . . . . . . . . . . . . . . . .
316
9.6
Comparison with Bayesian Inference . . . . . . . . . . . . . . . . . . . . .
319
9.6.1
Philosophical Perspectives . . . . . . . . . . . . . . . . . . . . . . .
319
9.6.2
Maximum Likelihood Estimator . . . . . . . . . . . . . . . . . .
320
9.6.3
Cramér-Rao Bound and Uncertainty Law . . . . . . . . . . .
321
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324
10
Bayesian OMA Formulation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
325
10.1
Single Setup Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
325
10.1.1
Likelihood Function . . . . . . . . . . . . . . . . . . . . . . . . . . .
326
10.1.2
Single Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
10.2
Remarks to Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
10.2.1
Complex Gaussian FFT. . . . . . . . . . . . . . . . . . . . . . . . .
330
10.2.2
Selected Frequency Band . . . . . . . . . . . . . . . . . . . . . . .
330
10.2.3
Prediction Error Model . . . . . . . . . . . . . . . . . . . . . . . . .
331
10.2.4
Measurement Type . . . . . . . . . . . . . . . . . . . . . . . . . . . .
332

10.2.5
Mode Shape Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
10.2.6
Leakage. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
335
10.3
Multi-setup Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
336
10.3.1
Global and Local Mode Shape . . . . . . . . . . . . . . . . . . .
337
10.3.2
Reference DOFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
337
10.3.3
Parameters in Different Setups. . . . . . . . . . . . . . . . . . . .
338
10.3.4
Likelihood Function . . . . . . . . . . . . . . . . . . . . . . . . . . .
339
10.3.5
Single Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
10.4
Asynchronous Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
10.4.1
PSD Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
10.4.2
Single Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
343
11
Bayesian OMA Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
11.1
Posterior Most Probable Value . . . . . . . . . . . . . . . . . . . . . . . . . .
346
11.2
Posterior Covariance Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . .
348
11.2.1
Mapping with Free Parameters . . . . . . . . . . . . . . . . . . .
348
11.2.2
Transformation of Covariance Matrix . . . . . . . . . . . . . .
349
11.2.3
Hessian of Composite Function. . . . . . . . . . . . . . . . . . .
349
11.2.4
Transformation Invariance . . . . . . . . . . . . . . . . . . . . . . .
351
11.2.5
Constraint Singularity . . . . . . . . . . . . . . . . . . . . . . . . . .
352
11.2.6
Pseudo-inverse. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
353
11.2.7
Singular Vector Formula . . . . . . . . . . . . . . . . . . . . . . . .
355
11.2.8
Dimensionless Hessian . . . . . . . . . . . . . . . . . . . . . . . . .
356
11.3
Mode Shape Uncertainty. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
11.3.1
Norm Constraint Singularity . . . . . . . . . . . . . . . . . . . . .
360
11.3.2
Stochastic Representation . . . . . . . . . . . . . . . . . . . . . . .
360
11.3.3
Expected MAC and Mode Shape c.o.v . . . . . . . . . . . . .
361
Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
Part III
Algorithms
12
Single Mode Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
365
12.1
Alternative Form of NLLF . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
366
12.2
Algorithm for MPV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
368
12.3
High s/n Asymptotics of MPV . . . . . . . . . . . . . . . . . . . . . . . . . .
368
12.3.1
Initial Guess of MPV . . . . . . . . . . . . . . . . . . . . . . . . . .
370
12.4
Posterior Covariance Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . .
370
12.4.1
General Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . .
371
12.4.2
Condensed Expressions . . . . . . . . . . . . . . . . . . . . . . . . .
372
12.5
Synthetic Data Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
374
12.6
Laboratory/Field Data Examples. . . . . . . . . . . . . . . . . . . . . . . . .
381
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
390

13
Multi-mode Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
391
13.1
Mode Shape Subspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
393
13.1.1
Orthonormal Basis Representation. . . . . . . . . . . . . . . . .
393
13.2
Alternative Form of NLLF . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
394
13.3
Most Probable Mode Shape Basis . . . . . . . . . . . . . . . . . . . . . . .
396
13.3.1
Hyper Angle Representation . . . . . . . . . . . . . . . . . . . . .
396
13.3.2
Rotation Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
397
13.3.3
Newton Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
398
13.4
Most Probable Spectral Parameters. . . . . . . . . . . . . . . . . . . . . . .
402
13.4.1
Parameterizing Structured Matrices . . . . . . . . . . . . . . . .
402
13.5
Algorithm for MPV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
403
13.6
High s/n Asymptotics of MPV . . . . . . . . . . . . . . . . . . . . . . . . . .
404
13.6.1
Initial Guess of MPV . . . . . . . . . . . . . . . . . . . . . . . . . .
405
13.7
Posterior Covariance Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . .
405
13.7.1
General Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
13.7.2
Condensed Expressions . . . . . . . . . . . . . . . . . . . . . . . . .
408
13.8
Illustrative Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
413
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
418
14
Multi-setup Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
419
14.1
Local Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
420
14.2
Global Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
422
14.2.1
Partial Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
423
14.2.2
Limiting Behavior of Solution. . . . . . . . . . . . . . . . . . . .
424
14.2.3
Iterative Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
425
14.2.4
Reference Condensation . . . . . . . . . . . . . . . . . . . . . . . .
426
14.3
Bayesian Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
427
14.3.1
Alternative Form of NLLF . . . . . . . . . . . . . . . . . . . . . .
428
14.3.2
Partial MPV of Global Mode Shape . . . . . . . . . . . . . . .
430
14.3.3
Algorithm for MPV. . . . . . . . . . . . . . . . . . . . . . . . . . . .
431
14.3.4
High s/n Asymptotic MPV . . . . . . . . . . . . . . . . . . . . . .
431
14.3.5
Initial Guess . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
433
14.3.6
Asymptotic Weight for Global Least Squares . . . . . . . .
433
14.3.7
Posterior Covariance Matrix . . . . . . . . . . . . . . . . . . . . .
434
14.4
Representative Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
437
14.5
Field Applications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
438
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
450
Part IV
Uncertainty Laws
15
Managing Identiﬁcation Uncertainties . . . . . . . . . . . . . . . . . . . . . . . .
455
15.1
Context and Key Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
456
15.2
Understanding Uncertainty Laws . . . . . . . . . . . . . . . . . . . . . . . .
460
15.2.1
Data Length and Usable Bandwidth . . . . . . . . . . . . . . .
461

15.2.2
Signal-to-Noise Ratio . . . . . . . . . . . . . . . . . . . . . . . . . .
462
15.2.3
First Order Effect of Modal s/n Ratio . . . . . . . . . . . . . .
463
15.2.4
Governing Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . .
463
15.3
Demonstrative Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
464
15.4
Planning Ambient Vibration Tests . . . . . . . . . . . . . . . . . . . . . . .
467
15.4.1
Simple Rule of Thumb . . . . . . . . . . . . . . . . . . . . . . . . .
469
15.4.2
Accounting for Channel Noise
and Measured DOFs . . . . . . . . . . . . . . . . . . . . . . . . . . .
469
15.5
Common Sense. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
471
16
Theory of Uncertainty Laws. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
473
16.1
Long Data Asymptotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
474
16.1.1
Fisher Information Matrix . . . . . . . . . . . . . . . . . . . . . . .
475
16.2
Small Damping Asymptotics . . . . . . . . . . . . . . . . . . . . . . . . . . .
479
16.3
Asymptotic Decoupling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
479
16.3.1
Scalar Parameter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
479
16.3.2
Vector-Valued Parameter. . . . . . . . . . . . . . . . . . . . . . . .
481
16.4
Leading Order Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
482
16.5
First Order Effect of Signal-to-Noise Ratio. . . . . . . . . . . . . . . . .
485
16.6
Other Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
488
Appendix A Asymptotics of RkDa
kðbk  1Þbbc
k . . . . . . . . . . . . . . . . . . .
489
Appendix B Derivation of Small Damping Asymptotics
(Zeroth Order). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
493
Appendix C First Order of NLLF Derivatives w.r.t. f ; f; S . . . . . . . . . .
496
Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
498
Appendix A. Complex Gaussian and Wishart Distribution . . . . . . . . . . .
499
A.1 Complex Gaussian Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
499
A.2 Covariance and Pseudo-covariance Matrix . . . . . . . . . . . . . . . . . . . . . . .
501
A.3 Complex Gaussian PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
502
A.4 Circular Symmetry. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
505
A.5 Complex Wishart Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
507
A.5.1 Scalar Variable. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
509
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
510
Appendix B. Hessian Under Constraints . . . . . . . . . . . . . . . . . . . . . . . . . .
511
B.1 Direct Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
514
B.1.1 Derivation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
517

B.2 Lagrange Multiplier Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
519
B.2.1 Derivation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
520
B.2.2 Transformation Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
522
Appendix C. Mathematical Tools. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
525
C.1 Asymptotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
525
C.2 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
526
C.2.1 Linear Independence, Span, Basis and Dimension . . . . . . . . . . . .
526
C.2.2 Linear Transformation, Rank and Nullity . . . . . . . . . . . . . . . . . . .
527
C.2.3 Euclidean Norm, Inner Product and Orthogonality. . . . . . . . . . . .
527
C.2.4 Eigenvalue Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
528
C.2.5 Diagonalizable Matrices and Their Functions. . . . . . . . . . . . . . . .
529
C.2.6 Real Symmetric and Hermitian Matrices . . . . . . . . . . . . . . . . . . .
530
C.3 Lagrange Multiplier Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
532
C.4 Minimizing Quadratic Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
533
C.5 Identities and Inequalities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
535
C.5.1 Matrix Inverse Lemma and Determinant Theorem . . . . . . . . . . . .
536
C.5.2 Block Matrix Determinant and Inverse. . . . . . . . . . . . . . . . . . . . .
536
C.5.3 Derivatives of Log-Determinant and Inverse . . . . . . . . . . . . . . . .
536
C.5.4 Gradient and Hessian of Rayleigh Quotient . . . . . . . . . . . . . . . . .
537
C.5.5 Gradient and Hessian of Unit Vector . . . . . . . . . . . . . . . . . . . . . .
537
C.5.6 Cauchy-Schwartz Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
538
C.5.7 Inequality for Partition of Inverse. . . . . . . . . . . . . . . . . . . . . . . . .
539
C.5.8 Inequality for Covariance of Two Vectors . . . . . . . . . . . . . . . . . .
539
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
540
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
541

Abbreviations
c.o.v.
Coefﬁcient of variation = ratio of standard deviation to mean
CRB
Cramér Rao Bound
DAQ
Data acquisition
DFT
Discrete Fourier Transform
DOF
Degree of freedom
DTFT
Discrete-time Fourier Transform
e.g.
For example
FFT
Fast Fourier Transform
FIM
Fisher information matrix
FT
Fourier Transform
Hz
Hertz (cycles per second)
i.e.
That is
i.i.d.
Independent and identically distributed
LHS
Left hand side
MAC
Modal assurance criterion
MDOF
Multi degree of freedom
MPV
Most probable value
NLLF
Negative log-likelihood function
ODE
Ordinary differential equation
OMA
Operational modal analysis
PDF
Probability density function
PSD
Power spectral density
RHS
Right hand side
RMS
Root mean square
s/n
Signal-to-noise
SDOF
Single degree of freedom
SV
Singular value
w.r.t.
With respect to

Nomenclature
ln x
Natural logarithm of x, i.e., to base e (=2.71828…)
logp x
Logarithm of x to base p
i
Purely imaginary number, i2 ¼ 1
x
Complex conjugate of x
In, I
n  n identity matrix, identity matrix (dimension not indicated)
0n, 0nm
n  n zero matrix, n  m zero matrix
½a1; a2; a3
1  3 row vector with entries a1, a2 and a3
½a1; a2; a3
3  1 column vector with entries a1, a2 and a3
diag½x
Diagonal matrix formed by the entries of vector x
diagfxign
i¼1
Diagonal matrix formed by x1; . . .; xn
vecðAÞ
Vectorization of matrix A
A  B
Kronecker product of matrices A and B
jxj
Absolute value of scalar x
jAj
Determinant of matrix A
AT
Transpose of matrix A
A
Conjugate transpose of matrix A, i.e., A
T
trðAÞ
Trace of matrix A, i.e., sum of diagonal entries
r
Gradient of a function
r2
Hessian matrix of a function
x  y
x is asymptotic to y, i.e., x=y ! 1
x  y
x is approximately equal to y
x ¼ OðyÞ
x is of the order of y, i.e., x=y ! non-zero ﬁnite constant
jjxjj
Euclidean norm of vector x, i.e., the square root sum of squared
entries
fxjg
Collection of variables indexed by j
var½X
Variance of scalar random variable X
cov½X
(Auto) Covariance matrix of random vector X
cov½X; Y
(Cross) Covariance matrix between random vectors X and Y

Part I
Modeling

Chapter 1
Introduction
Abstract This chapter introduces modal testing and identiﬁcation of structures,
covering free vibration test, forced vibration test and ambient vibration test (op-
erational modal analysis). The uncertainties in operational modal analysis are dis-
cussed. Regarding uncertainty as a lack of information, sources due to variability
and identiﬁcation error are distinguished. The unknown nature of excitation is an
issue that must be tackled in any operational modal analysis method. Existing
strategies attempt to eliminate the effect of this uncertainty or model it. After a brief
tour on conventional (non-Bayesian) methods, Bayesian approach is introduced as a
fundamental means to address uncertainties using probability. An overview of the
book is given with speciﬁc advice for readers from different sectors.
Keywords Free vibration test  Forced vibration test  Ambient vibration test 
Modal identiﬁcation
The ‘modal properties’ of a structure include primarily its natural frequencies,
damping ratios and mode shapes. They are the characteristics governing the
response of a structure under dynamic loads, playing an important role in analysis
and design. Dynamic response is signiﬁcantly larger if the structure is in resonance
with the loading; and this is governed by natural frequencies. Damping is associated
with energy dissipation; the higher the damping, the lower the response. Mode
shape affects the nature and spatial distribution of vibration.
Forward and Backward Problems
In a conventional dynamics problem, structural properties are speciﬁed, from which
modal properties can be determined. The response of different modes, distinguished
by mode shapes, are then calculated and summed to give a prediction of the
structural response. These are referred as ‘forward problems’, depicted by the two
right-pointing arrows in Fig. 1.1.
Modal Identiﬁcation
‘Modal identiﬁcation’ aims at identifying the modal properties of a structure,
typically using measured vibration time history data (e.g., acceleration). ‘Structural
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_1
3

system identiﬁcation’ aims at identifying the structural properties. These are
‘backward’, or ‘inverse’, problems, depicted by the left-pointing arrows in Fig. 1.1.
Although not indicated in the ﬁgure, structural system identiﬁcation can also be
based directly on the data. Modal and structural system identiﬁcation are
inter-related objectives. The general aim is to bridge the gap between structural
models and their constructed counterparts, improve the quality of response pre-
dictions and provide updated structure-speciﬁc information for decision making. It
involves a range of expertise such as modeling, experimentation, data processing
and system identiﬁcation theory. See Catbas et al. (2011) for a state-of-the-art
report; Friswell and Mottershead (1995) and Doebling et al. (1998) on structural
system identiﬁcation.
In-situ Versus Modeled Modal Properties
Modal identiﬁcation provides information about the ‘in-situ’ modal properties of a
constructed structure, which can differ signiﬁcantly from model predictions.
Discrepancies in the order of 30% in predicting natural frequencies are not
uncommon. They can be due to deviations/variations in stiffness and mass distri-
bution. Boundary conditions and contributions from non-structural components are
difﬁcult to model. Natural frequencies can change with vibration amplitude, tem-
perature and humidity; and with aging throughout the lifetime of a structure.
Damping mechanism is difﬁcult to model at the structural level and there is cur-
rently no accepted method for prediction from ﬁrst principles. A further compli-
cation is its amplitude-dependent nature. Representative values or empirical
formulas used in design are based on databases of identiﬁed values (with signiﬁcant
scatter). See, e.g., EC1 Annex F (2005), ESDU 83009 (2012), ISO 4354:2009
Annex E and ISO 10137:2007 Annex B, Bachmann et al. (1995).
A Practical Means for Updating
Modal identiﬁcation is a practical means for updating the state of a structure and
response prediction in a future dynamic loading event. By virtue of modal super-
position, the modal properties of contributing modes are already sufﬁcient for a
600t
Mode 1
1.4Hz
1% damp.
600t
140kN/mm
100kN/mm
600t
Mode 2
3.5Hz
1.2% damp.
600t
+
Structural
properties
stiffness, mass, 
damping, etc.
Modal
properties
natural frequencies, 
damping ratios, mode 
shapes, modal masses
Structural
response
displacement, velocity, 
acceleration, stress, etc.
damping model
physical force
+
eigenvalue
problem
struct. 
sys. ID
modal super-
position
modal ID
modal force
⇔
⇔
Fig. 1.1 Forward (right-arrow) and backward (left-arrow) problems in structural dynamics
4
1
Introduction

good estimation of the dynamic response, without knowing the structural proper-
ties. The modal properties of the modes that signiﬁcantly contribute to the measured
data can be identiﬁed uniquely. The same is not necessarily true for structural
properties, however. It depends on the complexity of the structural model used, how
it is parameterized, and the sensitivity of modeled data to parameters.
Demand for Modal Identiﬁcation
New
design
concepts,
tightened
performance
criteria
and
targets
for
cost-effectiveness have led to an increasing demand for modal identiﬁcation. With
new architectural forms and the use of advanced materials, modern structures tend
to be lighter, more slender and more sensitive to dynamic loads. Serviceability
performance assumes increasing importance, besides ultimate load capacity and
resilience. In super-tall buildings, for example, designing and maintaining a
weathertight envelope and controlling vibrations for human comfort more fre-
quently becomes the governing challenge. Controlling vibrations below service-
ability limits is also relevant for long-span slabs, pedestrian bridges and roofs.
Design decisions outside or at the boundary of normal experience demand
higher-than-normal quality in modeling information and assumptions in capturing
reality. This leads to a demand for the knowledge of in-situ properties of con-
structed facilities in general. Databases of identiﬁed modal properties allow mod-
eling assumptions to be calibrated for structures of similar conﬁgurations, in the
long run improving the ﬁdelity of models at the design stage. In a vibration
assessment, control or retroﬁt project of an existing structure, modal identiﬁcation is
among the ﬁrst few tasks to establish baseline dynamic properties. In ‘structural
health monitoring’ (SHM) applications, modal properties are almost the default
quantities of investigation and affect many decisions downstream; see Brownjohn
(2007), Farrar and Worden (2012) and Xu and Xia (2012).
1.1
Vibration Tests
Whether and how the modal properties of a structure can be identiﬁed from mea-
sured vibration data depend critically on the available information about the input
excitation during test. Generally, the measured vibration data can be viewed as
measured vibration data ¼
free vibration from unknown initial condition
þ forced vibration from known excitation
þ forced vibration from unknown excitation
þ measurement noise (unknown)
ð1:1Þ
The ﬁrst three terms on the right hand side (RHS) depend on modal properties and
hence contain information for identifying them. Transforming this principle into a
1
Introduction
5

modal identiﬁcation method is not trivial, however. Different terms have different
sensitivities to modal properties. They may depend on other unknown properties
that may need to be identiﬁed together. The second term is completely determined
by modal properties and the known excitation. In addition to the modal properties,
the ﬁrst term also depends on initial conditions, i.e., displacement and velocity of
the structure, which are generally unknown. For the third term, if the unknown
excitation is totally arbitrary then the resulting forced vibration response will
involve too many unknowns that are impossible to identify. Possibility exists when
the unknown excitation is assumed to have some statistical properties so that the
resulting forced vibration response can be described statistically. Such information
can be used for modal identiﬁcation, although the theory is much more involved
and the results are only as good as the statistical assumptions.
Developing a modal identiﬁcation method that accounts for all the terms on the
RHS of (1.1) can be theoretically and computationally involved. It has not been
done so far; and it need not be cost-effective to do so in the future. Existing methods
mostly consider the special cases where only one of the ﬁrst three terms dominates.
Accounting for measurement noise (the last term) does not create much compli-
cation in method development. Dominance by the ﬁrst or second term can be
created artiﬁcially through experimental design, leading to ‘free vibration test’ and
‘forced vibration test’, respectively. There are many naturally occurring scenarios
where the third term is dominant with reasonably tractable and justiﬁable statistical
properties, leading to ‘ambient vibration test’.
The basic principles of the three types of tests are next described in Sects. 1.1.1–
1.1.3. This book is about ‘operational modal analysis’ (OMA), which aims at
identifying the modal properties using the data from ambient vibration test.
Methods for free and forced vibration tests are not formally covered but their
concepts are introduced in Chap. 3 as an application of structural dynamics. See
also McConnell (1995), Maia and Silva (1997) and Ewins (2000).
1.1.1
Free Vibration Test
In a free vibration test, the structure is initially set into motion by some artiﬁcial
means and then left to vibrate on its own. The response time histories during the
free vibration phase at selected locations are measured and used for modal iden-
tiﬁcation. To take advantage of free vibration test, the initiation excitation should be
large enough so that the induced dynamic response is signiﬁcantly larger than the
response due to other unmodeled (e.g., ambient) excitations. Figure 1.2 shows an
example where the ﬁrst vertical mode of a footbridge is excited by human jumping
near resonance. This of course is not feasible for massive structures. For example,
the horizontal response due to a few participants pushing on the wall (even at the
natural frequency) of a tall building is unlikely to be noticeable in the measured
data, which is dominated by ambient vibrations.
6
1
Introduction

Modal identiﬁcation using free vibration data dominated by a single mode is
conceptually straightforward. In this case the data is oscillatory with a single fre-
quency and exponentially decaying amplitude. For good quality data, the estimation
procedure can be as simple as counting cycles for the natural frequency and cal-
culating the rate of amplitude decay for the damping ratio (Sect. 3.1.4).
In reality, it can be difﬁcult to excite only a single mode of vibration. The data
may be dominated by more than one mode, as in Fig. 1.2b. Numerically ﬁltering
out the contribution from other unwanted modes is one option, as long as it does not
signiﬁcantly distort the contribution from the target mode. More generally, one can
exploit the full time history of data, rather than just their amplitudes. The theoretical
free vibration response can be written explicitly in terms of the natural frequencies,
damping ratios, mode shapes and modal initial conditions. These properties affect
the free vibration response in different characteristic ways. In principle, they can be
identiﬁed uniquely by curve-ﬁtting in either the time or frequency domain.
Alternatively, one can approximate the free vibration equation or response in
discrete-time and estimate the coefﬁcient matrices, whose eigenvalue properties can
be used for back-calculating the modal properties. See Sect. 1.4.2 later.
1.1.2
Forced Vibration Test
In a forced vibration test (known input) the structure is excited by some artiﬁcial
means, e.g., a shaker or a sledge hammer. In addition to the vibration response, the
force applied to the structure is also measured, and hence the term ‘known input’. The
(input) force and (output) response time histories are used together for modal iden-
tiﬁcation. To take advantage of forced vibration test, the response due to the known
force should dominate those from other unknown excitation sources. Forced vibra-
tion test could be expensive to perform due to the equipment and logistics required.
Safety and risk considerations also increase the overheads or discourage its use.
0
10
20
30
40
50
-0.02
-0.01
0
0.01
0.02
Time (sec)
Acceleration (g)
(b)
jump
free vibration
ambient
metronome
(a)
Fig. 1.2 Free vibration test with human excitation. a To excite the ﬁrst vertical mode, participants
tried to jump at the natural frequency, following the beeps of the metronome held in the lady’s
right hand; b acceleration data measured by sensor near the midspan
1.1
Vibration Tests
7

However, when suitably applied with sufﬁcient excitation power, forced vibration
test allows the modal properties to be identiﬁed with good precision and at a desired
vibration level, e.g., near serviceability limits (important for damping ratios). The
data in a forced vibration test naturally contains a free vibration phase after the
artiﬁcial force is removed. Figure 1.3 shows an example with an electro-magnetic
shaker. With a payload of a few hundreds of Newton, such shaker has been con-
ventionally applied to investigate the vibrations of ﬂoor slabs and footbridges. See
Sect. 3.4.
1.1.3
Ambient Vibration Test
If the excitation is not measured and no assumption is imposed, it is generally not
possible to determine the modal properties based on the (output) vibration response
only. This is because the latter reﬂects the dynamic characteristics of the structure as
well as the excitation. Possibility exists if one is willing to make some assumptions
on the ‘statistical properties’ of the excitation. One conventional assumption is that
the excitation is ‘broadband random’, which can be justiﬁed physically and is found
to give reasonable results. The speciﬁc assumptions and their validity vary from one
method to another, and so is the reliability of the identiﬁed modal properties.
Results are as good as assumptions. Typically, the structure can be under its ‘op-
erating condition’, i.e., no initial excitation or artiﬁcial excitation required during
the time span of recorded data; and hence the term ‘operational modal analysis’.
The structure is subjected to a variety of excitations which are not measured, and
almost invariably cannot be measured.
Cost-Effectiveness
High implementation economy is one primary advantage of ambient vibration test.
This is particularly attractive for civil engineering structures, where it can be
expensive, disruptive or risky to carry out free vibration or forced vibration tests. Of
course, ambient vibration test is not always the best solution. It is a matter offeasibility
0
10
20
30
40
50
60
70
-3
-2
-1
0
1
2
3
x 10
-3
Time (sec)
Acceleration (g)
shaker off
signal
generator
(shaker)
(a)
(b)
shaker on
shaker on
free vib. 
amb.
amplifier
(shaker)
sensors
shaker
transient
steady-state
Fig. 1.3 Forced vibration test for the vertical modes of a ﬂoor slab with a shaker. a Equipment
and b vertical acceleration measured near the midspan of the ﬂoor slab
8
1
Introduction

and cost-effectiveness. For example, mechanical systems or components, for their
small size, can often be tested with forced vibration under controlled laboratory
conditions with affordable effort, achieving a much higher identiﬁcation precision
than ambient vibration test. For its importance, one would not expect to identify the
modal properties of a satellite prototype primarily by ambient vibration test in the
assembly plant. A series offorced vibration tests is more appropriate. However, when
the satellite is in operation, forced vibration test can be difﬁcult to implement and
ambient vibration data can be useful for updating dynamic characteristics.
Drawbacks
The following are some limitations or disadvantages of ambient vibration test,
primarily due to the fact that the ambient excitation is not measured:
(1) It is not possible to identify the modal mass directly;
(2) Identiﬁed modal properties only reﬂect their values at ambient vibration level,
which is usually at least an order of magnitude lower than the serviceability
level or other design levels of interest. This is especially relevant for the
damping ratio, which can be amplitude-dependent;
(3) Identiﬁed modal properties often have much lower precision than their coun-
terparts from well-controlled free or forced vibration data. The results are only
as good as the statistical assumptions about the unknown excitation.
(4) Identiﬁcation methods are more sophisticated than those for free or forced
vibration data; and ‘closed’ in some sense like a ‘black-box’. They involve
statistical concepts in the modeling or treatment of the unknown excitation. The
results require more technical background to interpret correctly. These reduce
conﬁdence in the results.
1.2
Uncertainties
In OMA, information about the input excitation is absent and the vibration response
can be so small that measurement noise is no longer negligible. As a result, the
identiﬁed modal properties often have signiﬁcantly higher uncertainty than their
counterparts from free or forced vibration data.
1.2.1
Variability and Identiﬁcation Uncertainty
The meaning of ‘uncertainty’ has yet to be deﬁned, but intuitively it is associated
with the experience that one obtains different answers from different data sets. Two
sources can be distinguished here, namely, ‘variability’ and ‘identiﬁcation uncer-
tainty’. Variability refers to whether the modal properties of the structure have
changed from one data set to another. If there is such a change then it is reasonable to
expect different answers. Identiﬁcation uncertainty is associated with the precision
1.1
Vibration Tests
9

with which one can estimate the modal properties using a given data set. It stems
from incomplete knowledge in modeling (imperfect) and data (limited and noisy).
Ideally, if variability and identiﬁcation uncertainty were both absent, the answers
from two data sets must be identical. In the presence of identiﬁcation uncertainty, the
answers will differ even if there is no variability. In reality, both variability and
identiﬁcation uncertainty are present. It is not trivial to distinguish them, especially
when they have a similar order of magnitude. In terms of methodology, this book
focuses on identiﬁcation uncertainty, although variability can be naturally observed
in ﬁeld test examples with multiple setups (Chap. 14).
1.2.2
Sources of Identiﬁcation Uncertainty
Regardless of the method used, one should not expect to be able to identify the
modal properties with perfect precision from a given set of ambient vibration data,
simply because there is not enough information. The lack of information arises
primarily from the following sources:
(1) Limited data—the amount of data is ﬁnite in terms of the number of measured
degrees of freedom (DOFs), sampling rate and duration;
(2) Measurement error—the recorded data is contaminated by noise due to the
senor, transmission medium (e.g., cable) and digitizing hardware;
(3) Unknown excitation—the excitation is not measured but is modeled statistically
with unknown parameters, which can be part of the properties to be identiﬁed;
(4) Modeling error—theory only provides a way to view the world but nature does
not care. The measured data need not obey modeling assumptions, e.g., the
excitation need not be broadband; the structure need not be viscously or
classically damped.
In principle, Source 1 can be reduced using a sufﬁciently large amount of data.
Source 2 can be reduced or practically eliminated with high quality equipment.
Source 3 cannot be eliminated and its effect will always stay. Source 4 can lead to
bias in the identiﬁcation results. To a certain extent, it can be reduced with advances
in modeling but the progress can be slow; or for practical purposes one would still
choose to use a conventional model and live with the modeling error. Depending on
the source, it may be possible to verify, control or at least reveal the potential model
error in the interpretation of results.
1.3
OMA Methods
OMA is a research subject of a few decades old. For its importance, a variety of
methods have been developed. A quick tour of the scope and the ways where OMA
has been approached is given below. The measured vibration in OMA stems from
10
1
Introduction

unknown excitations, which must be addressed upfront in method development.
The common strategy is either to eliminate or model the vibration due to unknown
excitations. Methods can differ in the way they approach the governing dynamic
equation. A method can operate in the time or frequency domain, or take advantage
of both. It can operate in terms of physical or state-space coordinates. Methods can
also differ in the philosophy about the inverse problem. The two main schools of
thoughts are ‘frequentist’ (non-Bayesian) and ‘Bayesian’ perspective. The former
constructs an estimate (a proxy) for the modal properties and views uncertainty as
the variability of the estimate calculated using data from repeated experiments. The
Bayesian perspective models the modal properties as random variables whose
probability distribution (and hence uncertainty) depends on the available data and
modeling assumptions.
As methods populate, they expand in scope and become more sophisticated.
Early methods can only handle data from a single measured DOF, but now most
methods can handle data from multiple DOFs, allowing the mode shapes at the
measured DOFs to be identiﬁed. Conventional methods assume well-separated
modes but there are now algorithms dealing with close modes. Synchronous data in
a single setup is conventionally assumed, but now multiple setup data are also
considered. There are also algorithms that can handle asynchronous data (e.g., those
from different smart phones). Conventional focus is on constructing an estimate for
modal properties but contemporary methods also aim at calculating their uncer-
tainty and building insights. Table 1.1 summarizes the aspects discussed above.
Some notable classes of methods are next discussed in Sects. 1.4 and 1.5.
1.4
Non-Bayesian Approach
Non-Bayesian methods construct an estimate as a proxy for the modal properties,
leveraging on the developers’ insights about the physical nature of ambient
vibration and theoretical origin. Notable methods along the idea of eliminating
Table 1.1 Approach and scope of OMA methods
Aspect
Attribute
Treatment
Approach
Vibration due to unknown excitation
Eliminate or model
Governing equation
Physical or state-space
Analysis perspective
Time domain or frequency domain
Identiﬁcation philosophy
Frequentist or Bayesian
Scope
Measured DOFs
Single ! multiple
Mode proximity
Well-separated ! close
Setup
Single ! multiple
Data synchronization
Synchronous ! asynchronous
Identiﬁcation uncertainty
Not calculated ! calculated ! insight
‘!’ indicates increasing sophistication
1.3
OMA Methods
11

random response or exploiting their statistics are discussed below. The treatment of
uncertainty follows.
1.4.1
Eliminating Random Response
‘Random decrement method’ exploits the idea that averaging a sufﬁciently large
number of ambient vibration sample histories with some non-canceling initial
conditions (e.g., of the same sign) will result in a time history that mimics the free
vibration time history. The idea stems from the fact that dynamic response generally
comprises a free vibration (due to initial conditions) and forced vibration (due to
excitations) component. Under random excitations the latter will cancel out sta-
tistically in the average, following an inverse square root law, i.e., magnitude
/ 1=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
no. of samples
p
. The averaged time history is called a ‘random decrement
signature’ (RDS). It is used for modal identiﬁcation as if it were a free vibration
time history. This idea is conceptually simple and generally applicable for both
linear and nonlinear dynamic response. Care should be taken not to extrapolate the
meaning of RDS, however. RDS is a derived quantity rather than the actual free
vibration history in a real experiment. The physical response of a structure may
exhibit amplitude-dependent damping behavior, but it need not be reﬂected in the
same way as is detected in the RDS. There is generally a (non-vanishing) bias in the
modal property estimate because RDS is a statistical average conditional on some
speciﬁed initial conditions, biasing it away from the free vibration history. See Cole
(1973) (original work), Cole (2014) (recent review) and Aquino and Tamura (2016)
(recent discussion).
1.4.2
Exploiting Statistics
Instead of eliminating the random component of ambient vibration in the measured
data, one can model it using probability theory and exploit its theoretical statistical
properties for modal identiﬁcation. For the theoretical properties to be mathemat-
ically tractable, it is often assumed that the structure is time-invariant, the measured
data is stochastic stationary (i.e., statistics do not change with time) and the exci-
tation is broadband random. Exploiting statistical properties in the time or fre-
quency domain leads to different methods with different implied assumptions,
limitations, or computational effort.
Time Domain Statistics
In the time domain, assuming white noise excitation, the correlation function of
ambient data can be shown to satisfy the same free vibration equation of motion of
the structure, and so it can be treated as free vibration data for modal identiﬁcation.
In the general case, the correlation function contains dynamics arising from the
12
1
Introduction

non-white nature of excitation. Although derived for the same purpose, the corre-
lation function should not be confused with the RDS in random decrement method.
They are derived quantities used mathematically as free vibration data for modal
identiﬁcation, but their meaning should not be extrapolated to be associated with
the data physically measured in a free vibration test.
The theoretical correlation function is not known and so it must be estimated
from data. The sample correlation function has a statistical error, following an
inverse square root law. Direct ﬁtting of the sample correlation function with the
theoretical expression of free vibration response results in a high-dimensional
nonlinear optimization problem involving the modal properties. To resolve this, one
computational strategy is to work with the dynamic equation or response in the
discrete-time domain. Simpliﬁcation results by taking approximations for small
time interval, justiﬁed when the interval is small compared to the natural periods of
interest. An output ‘state vector’ is deﬁned to comprise the values of the correlation
function at a ﬁnite number of consecutive time steps. The latter is related to the
order of the modeled dynamics. The output state vector at the next time step can be
written recursively as a linear function of that at the current step, related by a
coefﬁcient matrix that depends on the modal properties. This matrix can be esti-
mated efﬁciently by least squares method or singular value decomposition. The
modal properties are estimated from the estimated coefﬁcient matrix.
Notable developments based on the above idea or its variants include the
Ibrahim Time Domain Method (Ibrahim and Milkulcik 1976), Poly Reference
Method (Vold et al. 1982) and Eigen Realization Algorithm (Juang and Pappa
1985). Stochastic Subspace Identiﬁcation (SSI, Overschee and De Moor 1996) is a
popular method of this class. ‘Covariance-driven’ SSI operates on the sample
correlation function. ‘Data-driven’ SSI operates directly on the measured data. See
Peeters and De Roeck (2001) for a review.
Frequency Domain Statistics
In the frequency domain, the power spectral density (PSD) matrix of ambient data
can be theoretically written in terms of the transfer functions and mode shapes of
the contributing modes. For a well-separated mode, the PSD matrix is dominated by
the contribution of the mode near the natural frequency. This allows the modal
properties of the mode to be directly estimated from the eigenvalue properties of the
PSD matrix. The eigenvalue can be used to estimate the natural frequency and
damping ratio; the eigenvector gives the mode shape. Estimation is non-trivial for
close modes, whose contributions are ‘entangled’ because generally modal exci-
tations are correlated and mode shapes (conﬁned to measured DOFs only) are not
orthogonal. The eigenvectors of the PSD matrix do not necessarily give the mode
shapes for close modes. They are orthogonal vectors spanning the subspace of the
mode shapes, and are commonly called ‘operational deﬂection shapes’. The same
issue and terminology also apply to time domain methods when the mode shapes
are directly estimated from matrix decomposition.
In implementation, the sample PSD matrix estimated from data is used as a
proxy for the theoretical PSD matrix. For a given frequency resolution, the
1.4
Non-Bayesian Approach
13

estimation error follows an inverse square root law. ‘Peak picking’ is a simple
procedure that uses the eigenvalue properties at the natural frequency for estimating
the modal properties. ‘Frequency domain decomposition’ (FDD, Brincker et al.
2001) with enhanced variants is a notable method exploiting the band near the
natural frequency. See also Pintelon and Schoukens (2001).
1.4.3
Identiﬁcation Uncertainty
The methods described in Sects. 1.4.1 and 1.4.2 focus on developing an algorithm
that makes use of the measured data to calculate an estimate that will be used as a
proxy for the modal properties. Using a particular method with a given data set,
suppose the natural frequency estimate is calculated to be 1.2 Hz. For practical use
(e.g., to feed into design calculations) this 1.2 Hz will be taken as the natural
frequency of the structure, i.e., a proxy. This is despite the fact that the estimate
based on another data set is unlikely to be exactly equal to 1.2 Hz. This motivates
one to deﬁne uncertainty to reﬂect the variability of the estimate over different data
sets. Implementing this idea quantitatively via statistics, identiﬁcation uncertainty is
deﬁned as the variance of the estimate over hypothetical ‘identically controlled
experiments’. Identically controlled experiments do not imply identical data,
because there are still factors that cannot be controlled, e.g., unknown ambient
excitation and measurement noise. Uncertainty here is deﬁned in the frequentist
sense via the relative frequency of occurrence of events, even though identically
controlled experiments are neither possible nor actually performed.
Inherent Uncertainty
Calculating the variance of modal property estimate requires a separate algorithm
from that for obtaining the estimate. Mathematically, the variance of the estimate is
deﬁned as the expectation (ensemble average) of squared deviation from the mean,
taken over random data assumed to arise from the ‘unknown true’ modal properties.
It will therefore depend on the unknown true modal properties but not on a par-
ticular data set. This is a ‘frequentist’ deﬁnition of uncertainty and is ‘inherent’ in
nature, in that there is an unknown but ﬁxed value regardless of the data at hand.
Clearly, the expression of the variance, if it can be derived, will depend on the
unknown true values of the modal properties. In implementation, the variance is
estimated by substituting the true values by their estimates.
Perturbation Expressions
Developing the expression of variance is not trivial and it can be challenging because
the estimate depends in a complicated (possibly implicit) manner on the data. One
strategy to simplify derivation is to consider the ﬁrst order sensitivity of the modal
property estimate due to variability of data from the measured set. The resulting
derivation and expression are still algebraically involved but at least mathematically
tractable as an algorithm. The resulting expression is approximate but sufﬁcient
for practical purposes. See Pintelon et al. (2007) and Reynders et al. (2008)
14
1
Introduction

for uncertainty in SSI with single setup data; and Döhler et al. (2013) with multiple
setup data.
Empirical Estimates
Besides analytical expressions, the uncertainty of modal properties in a frequentist
sense can also be calculated as the sample variance of the estimate using data from
multiple experiments. The motivation and idea are straightforward, but one should
note that the sample variance generally comprises identiﬁcation error and variability
stemming from the tested structure and environment. The sample variance need not
agree with the value calculated from the theoretical expression of variance, because
the latter is derived based on some assumed probability distribution of data that
nature does not care about.
1.5
Bayesian Approach
The methods described in Sect. 1.4 aim at a mathematically ‘smart’ and tractable
way of exploiting the information in the data to produce an estimate for the modal
properties. It should be smart so that the estimate is around the actual value to be
estimated and has small variance. It should be tractable so that it can be imple-
mented in an efﬁcient algorithm. This strategy is a natural step when one transforms
intuitions about the forward problem into a solution for the backward problem. It
need not be the best way, however. The algorithm and hence the quality of the
estimate depends on the insights and mathematical facility of the developer. For
example, why correlation function? Why least squares method? If one is not able to
identify a given mode, is it because the method has not made full use of the
information contained in the data, or such information is not there anyway (so no
method can do)? How does one come up with the best estimate? The methods
inevitably make use of some theoretical statistical properties of data, which are
unknown and must be substituted by their sample counterparts. Downstream cal-
culations based on the sample counterparts produce quantities that need not obey
the modeling assumptions in structural dynamic analysis or design. For example,
the coefﬁcient matrices estimated in a time domain approach need not correspond to
one realizable from a structure satisfying the Newton’s equation of motion. The
theoretical variance of the estimates for modal properties generally involves their
unknown true values, whose existence is questionable.
1.5.1
Philosophy
This book focuses on Bayesian methods for OMA, which differ at least philo-
sophically from non-Bayesian ones. Probability is still used to treat uncertainties.
Instead of constructing a proxy estimate, however, one models the modal properties
1.4
Non-Bayesian Approach
15

as random variables with a probability distribution that depends on the available
information. One still makes assumptions about structural dynamics, the unknown
ambient excitation and measurement noise. One still solves the forward problem to
obtain the theoretical statistical properties of the measured data for given modal
properties. In fact, the forward problem is addressed in full detail: given the modal
properties, what is the distribution of data? Bayes’ Theorem is invoked to swap the
roles of the modal properties and available data, so that one can address the
backward problem, also in full detail: given the data, what is the distribution of the
modal properties? Mathematically, Bayes’ Theorem is a just statement of condi-
tional probability, but its philosophical meaning is far reaching. It leads to a method
that fundamentally transforms one’s available information into the required infor-
mation that addresses the question being asked.
1.5.2
Posterior Distribution and Statistics
In a Bayesian approach, all identiﬁcation results about the modal properties are
encapsulated in the ‘posterior’ (i.e., given data) distribution, which is a joint
probability density function conditional on modeling assumptions and available
data. In modal identiﬁcation, ‘prior’ information (i.e., in the absence of data) is
typically negligible compared to that provided by data. In this case, the posterior
distribution is directly proportional to the ‘likelihood function’, i.e., the (modeled)
distribution of data for given modal properties. This is a function that depends on
both the data and modal properties. It was the answer in the forward problem where
the modal properties were ﬁxed. Fixing the data (as measured) now and viewing the
likelihood as a function of modal properties gives the posterior distribution.
For OMA, the likelihood function depends in a non-trivial manner on the modal
properties. As a result, the posterior distribution does not belong to any standard
distribution (e.g., Gaussian, exponential), but it still has a centralized shape. With
sufﬁcient data, it can be well-approximated by a Gaussian distribution. The peak
location of the distribution represents the posterior ‘most probable value’ (MPV) of
the modal properties. The covariance matrix of the modal properties implied by the
distribution, i.e., ‘posterior covariance matrix’, quantiﬁes their identiﬁcation
uncertainty.
In contrast to the frequentist approach, no true parameter value or concept of
repeated experiments is involved in the quantiﬁcation of identiﬁcation uncertainty.
Up to the same modeling assumptions and data, the likelihood function is unique,
and so are the posterior distribution and its descriptive statistics (e.g., MPV). The
identiﬁcation uncertainty depends on, and can be calculated from, a given data set.
There is no inherent uncertainty; one can get a bigger uncertainty with one data set
but a smaller one with another. Different data sets provide different information
about the modal properties and hence lead to different levels of identiﬁcation
uncertainty.
16
1
Introduction

1.5.3
Computing Posterior Statistics
Modeling assumptions deﬁne the likelihood function and hence a Bayesian ‘for-
mulation’ of the OMA inference problem. Deriving the likelihood function involves
solving the forward problem, for which techniques are well-established, e.g.,
stochastic processes and stochastic dynamics. The likelihood function should be
derived in explicit form so that its value can be calculated efﬁciently. The remaining
tasks are computational in nature, i.e., to determine the posterior MPV and
covariance matrix. Determining the MPV involves solving an optimization problem
with respect to (w.r.t.) the modal properties, whose number can be large.
Determining the covariance matrix involves calculating the Hessian matrix of the
negative logarithm of likelihood function (NLLF) w.r.t. the modal properties.
Brute-force numerical optimization for MPV is prohibitive and unlikely to converge
reliably. Finite difference method for Hessian is time-consuming and inaccurate.
Proper algorithms are required for accurate and efﬁcient computation.
Existing strategies for determining the posterior MPV exploit the mathematical
structure of the NLLF to derive semi-analytical solutions for the MPV of different
groups of modal properties. This allows the MPV to be determined in an iterative
manner, updating different groups until convergence. This is especially critical for
mode shape values because their number can be large in applications. The
semi-analytical solution for the MPV of mode shape values effectively suppresses
the growth of computational effort with the number of DOFs. On the other hand,
analytical expressions can be derived for the Hessian of NLLF to allow accurate
and efﬁcient calculation of the posterior covariance matrix. One major issue is how
to systematically account for the constraints among the modal properties (e.g.,
mode shape under scaling constraint). This can be handled through Lagrange
multiplier concepts.
1.5.4
Formulations and Algorithms
Bayesian OMA formulations can differ in their assumptions and the form of data
used. For synchronous data acquired in a single setup (the nominal setting in
OMA), formulations have been developed chronologically based on time domain
data (Yuen and Katafygiotis 2001a), sample power spectral density (Yuen and
Katafygiotis 2001b) and Fast Fourier Transform (FFT) of data (Yuen and
Katafygiotis 2003). They all originated from Yuen’s MPhil thesis (Yuen 1999).
Time domain formulation is too restrictive in the modeling assumptions on ambient
excitation and measurement noise. Sample PSD formulation requires averaging
concept (as in non-Bayesian methods) and is less fundamental than FFT formula-
tion. It gives no computational advantage either (Yan and Katafygiotis 2014;
Au 2016).
1.5
Bayesian Approach
17

This book focuses on Bayesian formulations based on the FFT of ambient
vibration data, which is found to be mathematically tractable with a fairly robust set
of assumptions. Computational algorithms for single setup data have been devel-
oped in Au (2011) for well-separated modes (see also Zhang and Au 2013) and Au
(2012a, b) for multiple modes. Formulation for multiple setup data and computa-
tional algorithms are developed in Au and Zhang (2012a) and Zhang et al. (2015),
which originated from Zhang’s PhD thesis (Zhang 2011). Field applications of
Bayesian OMA are growing but reports in the literature are still limited. See, Au
and To (2012), Au and Zhang (2012b), Au et al. (2012a, b), Kuok and Yuen (2012),
Ni et al. (2015), Liu et al. (2016), Zhang et al. (2016a, b, c), Ni and Zhang (2016),
Ni et al. (2016a, b, 2017) and Lam et al. (2017).
1.5.5
Maximum Likelihood Estimation
Bayesian methods are sometimes confused with ‘maximum likelihood estimation’
(MLE). MLE is a non-Bayesian method where the modal property estimate is
determined as the one that maximizes the likelihood function for a given data set.
When the same likelihood function in the Bayesian method is used and no prior
information on the modal properties is used, the posterior MPV is equal to the
estimate by MLE. This is perhaps the reason for the confusion. Identiﬁcation
uncertainty in MLE, however, is still deﬁned as the variance of the estimate over
repeated experiments, as in a non-Bayesian method.
MLE and Bayesian method share some similarities in their mathematical
structure (not philosophy) of the problem. For a sufﬁciently large data size and
assuming that the data is indeed distributed as the likelihood function for some
‘true’ properties (assumed to exist), the MLE estimator has the smallest variance
among all unbiased estimators. The smallest variance is given by the tightest
Cramér Rao Bound, mathematically equal to the inverse of the full Fisher infor-
mation matrix. It coincides with the leading order of the posterior variance in the
Bayesian method under the same set of (frequentist) assumptions just described.
See Sect. 9.6 for details. Matarazzo and Pakzad (2016a, b) proposed a MLE method
for OMA based on a state-space formulation.
1.5.6
Drawbacks and Limitations
Bayesian OMA methods are implicit, requiring numerical optimization to determine
the posterior MPV. This is typical for Bayesian methods in general, because of the
nonlinear relationship between the likelihood function and the model parameters to
identify. In contrast, most frequentist OMA methods are explicit in nature, requiring
no numerical optimization or iterations. Existing Bayesian OMA methods are
capable of reducing the dimension of the numerical optimization problem, but not
18
1
Introduction

eliminating the need for solving it. The methods are sustainable with the number of
measured DOFs but not the number of close modes that must be identiﬁed together.
Generally, well-separated modes can be identiﬁed in a matter of seconds on a laptop
regardless of the number of measured DOFs. Close modes require signiﬁcantly
longer time to converge, from a few seconds to a few minutes on a laptop
depending on the signal-to-noise ratio of data and collinearity of mode shapes; but
relatively insensitive to the number of measured DOFs. It is possible that the
solution of MPV does not converge when the data does not contain enough
information for identiﬁcation. In typical situations where the number of close
modes does not exceed three, the problem is still manageable.
Existing Bayesian OMA methods mostly address identiﬁcation uncertainty but
not variability. The posterior variance tells how precise a modal property can be
identiﬁed based on the given data and modeling assumptions, but it does not tell
what can be expected from another data set without additional assumptions. This
limitation applies to OMA in general. Recognizing this aspect is important for
proper interpretation of results and further advance. Hopefully, a scientiﬁcally
sound methodology can be established in the near future for distinguishing vari-
ability and identiﬁcation uncertainty. This is important for SHM, because system-
atic change in the monitored structure cannot be efﬁciently detected if it cannot be
distinguished from identiﬁcation error.
1.6
Overview of This Book
As the title tells, this book is about the modeling of ambient vibration data and
making inference about modal properties following a Bayesian approach. It has
taken a different perspective to OMA from the current conventional approaches,
which are mostly non-Bayesian. Insights about dynamics are still discussed, but
they no longer drive the development of OMA method. Rather, Bayes’ theorem is
invoked to give the answer to modal identiﬁcation. Non-Bayesian OMA methods
are not covered. They can be found in review papers and monographs; see, e.g.,
Wenzel and Pichler (2005), Reynders (2012) and Brincker and Ventura (2015).
Although only Bayesian methods are discussed formally for modal identiﬁcation,
Chaps. 2–7 on modeling are relevant to OMA in general. The same is also true for
Chap. 15 on managing identiﬁcation uncertainty. Chapters 8 and 9 discuss Bayesian
and non-Bayesian approaches in a general context, not just for OMA.
Basic Assumptions
Bayesian inference in this book makes use of the FFT of ambient vibration data.
This is especially suitable for OMA because the information of different vibration
modes is naturally partitioned in the frequency domain by way of resonance.
Time-invariant linear classically damped dynamics is assumed. This is the con-
ventional assumption in structural dynamic analysis and design; and so the iden-
tiﬁcation results are directly transferrable. The excitation is assumed to be
1.5
Bayesian Approach
19

broadband random in the sense that the modal forces of the subject modes have a
constant PSD near their natural frequencies. This assumption turns out to be fairly
robust (Sect. 10.2).
Application Context
Although the application context of this book is on civil engineering structures
(e.g., buildings, bridges), modal identiﬁcation can also be applied and is demanded
for mechanical and aerospace structures, e.g., ground testing of aircrafts. It is also
demanded in electrical power networks, i.e., ‘wide-area monitoring’, where
electro-mechanical oscillations obey exactly the same dynamic equation of motion
with analogous deﬁnition (e.g., voltage $ displacement). See Pierre et al. (1997),
Rogers (2000) and Seppanen et al. (2016). The theories and algorithms developed
in this book are applicable regardless of discipline, as long as the governing
equations and basic assumptions ﬁt in.
The chapters in this book can be grouped in terms of ‘modeling’, ‘inference’,
‘algorithms’ and ‘uncertainty laws’. They are outlined in the following.
1.6.1
Modeling
Chapters 2–7 contribute to the theoretical modeling of ambient vibration data.
Chapter 2 introduces the Fourier analysis of a deterministic process, providing the
basics for frequency domain perspective and techniques in a non-probabilistic
context. Chapter 3 discusses the dynamic response of a structure subjected to
known excitations. Principles of modal testing are introduced to strengthen
dynamics concepts and give an idea of how modal properties can be estimated.
Chapters 4 and 5 are probabilistic counterparts of Chaps. 2 and 3, respectively,
involving unknown excitations modeled by a stationary stochastic process. They
allow one to model ambient vibration response in a probabilistic manner. They do
not yet fully explain the measured ambient vibration data, which is the output of
sensors and data acquisition hardware and is contaminated by measurement noise.
These aspects are discussed in Chap. 6. Chapter 7 introduces the basic tools for
analyzing ambient vibration data and explains the results based on the theories in
the previous chapters.
1.6.2
Inference
Chapter 8 introduces Bayesian inference in a general context, covering different
cases of identiﬁability where the posterior distribution has different topologies and
requires different computational strategies for characterization. Chapter 9 discusses
the frequentist approach of system identiﬁcation. Similarities and differences with
20
1
Introduction

the Bayesian approach are highlighted. The mathematical connection between the
frequentist and Bayesian notion of identiﬁcation uncertainty leads to an effective
means for investigating the achievable precision of OMA in Chap. 16 later. Chapter
10 formulates Bayesian OMA problems for different situations, covering data from
a single setup, multiple setups and asynchronous channels. Chapter 11 discusses the
general computational strategies for determining the posterior MPV and covariance
matrix. Computing the Hessian of the NLLF incorporating constraints (e.g., from
mode shape scaling) is a major topic there. Systematic and efﬁcient strategies are
presented.
1.6.3
Algorithms
Chapters 12–14 develop and provide the recipe for Bayesian OMA algorithms in
different settings within the computational framework in Chap. 11. Chapters 12 and
13 consider respectively well-separated modes and multiple (possibly close) modes,
both assuming data from a single setup. Chapter 14 considers well-separated modes
with data from multiple setups. As a basic structure of these chapters, the likelihood
function is analyzed in order to develop an efﬁcient computational procedure for the
posterior MPV. For the posterior covariance matrix, analytical formulas are pre-
sented within the framework of Chap. 11. Examples based on synthetic data are
presented to illustrate the problem, method or interpretation of results. Application
is illustrated using laboratory and ﬁeld test data.
1.6.4
Uncertainty Laws
In addition to modeling and Bayesian inference, this book also covers the
‘uncertainty laws’ of OMA, which are closed form analytical expressions giving the
leading order term of the posterior variance of modal properties. This is one of the
latest advances in OMA, providing insights into the achievable precision of OMA
and a scientiﬁc basis for planning ambient vibration tests. Key results are discussed
in Chap. 15, requiring minimal mathematical facility. Examples are also given to
illustrate how the test conﬁguration in an ambient vibration test can be decided
based on the uncertainty laws. Detailed theory and derivations are provided in
Chap. 16.
Three appendix chapters provide supplementary materials to the main chapters.
Appendix A on complex Gaussian distribution supplements the theory on the
statistical properties of FFT in Chap. 4. Appendix B on Hessian under constraints
supplements the theory for calculating the posterior covariance matrix in Chap. 11.
Appendix C supplements the general mathematical tools referred in different
chapters.
1.6
Overview of This Book
21

1.7
How to Use This Book
This book is written with the primary aim to provide a one-stop reference for
learning the theoretical, computational and practical aspects of OMA and its
Bayesian solution. Each chapter is written in a self-contained manner while con-
nections with other chapters are highlighted in the beginning. The chapters have
been arranged in a logical manner where pre-requisites needed in the current
chapter can be found in the previous chapters. Following this order is natural in
presentation and writing but not necessarily the best way to learn OMA. In my
experience, a problem-driven approach, i.e., setting a target problem upfront, trying
to solve it, looking for and learning the required skills along the way, is a better way
to grasp a subject.
This book grows out of my personal need to advise undergraduate and master
degree projects, doctoral and postdoctoral research, and disseminate research
ﬁndings to academics and industry. This spectrum of audience presents conﬂicting
demands on the contents and the style of writing. For example, undergraduate and
master degree projects have a short time span, e.g., a year. It is vital for them to
have a quick start, to get interested and be on the right track. Proofs are of sec-
ondary importance, at least on ﬁrst reading. A quick start is also good for PhD
students and postdocs, but they have a longer term agenda and need a strong
theoretical foundation for original research and further developments. Proofs are
needed for scientiﬁc rigor and provide insights into the mathematical structure of
problems. They may also involve important techniques, mastering which allows
one to explore and solve new problems. Practitioners are interested in applications,
and in some cases, implementation (algorithms).
To achieve my primary aim while balancing interests for different groups of
audience, I have tried to present materials from a utility point of view. Theories are
motivated from use. Results are developed from mathematical reasoning when it is
not difﬁcult to understand. Proofs that may disturb the ﬂow of reading are post-
poned until the end of discussion. Proofs that are too technical are omitted.
Examples are provided to illustrate theory, method or application; or to provide
details relevant to speciﬁc contexts. Proofs and examples are supplementary;
skipping them should not affect understanding the main text.
Some speciﬁc advice is given below for different groups of audience. For this
purpose (only) I would stereotype their background and objectives.
1.7.1
Student
By a ‘student’ I suppose you are a senior undergraduate or master degree student
pursuing a project related to modal identiﬁcation or its applications. You have learnt
basic probability and statistics in junior years. You may not have taken a course in
22
1
Introduction

structural dynamics; or you are taking it concurrently with the project; or you have
taken it before but are rusty on it.
Structural Dynamics
You may ﬁrst take a look at Chap. 3. There is no need to ﬁnish the whole chapter at
this stage. Target to grasp the concepts about natural frequency, damping ratio,
dynamic ampliﬁcation factor and resonance in Sect. 3.1; mode shape and modal
superposition in Sect. 3.2. The derivations in this chapter should not be difﬁcult to
follow; otherwise skip them for the time being and revisit later. Build insights into
the results by plotting the theoretical dynamic response and changing some
parameters to see the effects. The response to any arbitrary loading can be computed
using the Newmark method (Sect. 3.8), or simply the function lsim if you are
using Matlab. Compute and plot the response for some simple load cases and see if
the results make sense.
Generating and Analyzing Synthetic Data
When you are comfortable with structural dynamics you may take a jump start on
analyzing ambient data. Although experimental data is always the target, it is easier
to start with synthetic data because you can have good control over it. Generate a
time history of white noise (Sect. 7.7) and take it as an excitation to a single-DOF
structure. Adding white measurement noise to the response gives the synthetic data.
Plot the data with time. Get the units right and indicate them. You may not ﬁnd
anything interesting in the plot but looking at the original data is always the ﬁrst
thing an analyst should do. The interesting things are in the PSD and singular value
(SV) spectra. Follow the procedures in Sects. 7.2 and 7.3 to plot and interpret these
spectra. Get the units right and indicate them. There are terminology and concepts
in Chap. 7 that you may not understand, e.g., PSD, SV, FFT. These are to do with
viewing a time series as a sum of sines and cosines, i.e., ‘Fourier analysis’, and
applying probability concepts for analysis. Chapter 2 introduces Fourier analysis for
a given time series that does not involve any probability concept. You have learnt
probability but the level required for PSD and SV is higher than that. You will also
need Chaps. 4 and 5. Sections 4.8–4.10 and 5.3–5.4 may be skipped at the moment.
Skip difﬁcult proofs on ﬁrst reading and revisit later if that can improve under-
standing. Study the theories in Chaps. 2, 4 and 5 as far as you need them in Chap. 7.
Analyzing Real Data
Once you are comfortable with analyzing synthetic data, it is time you meet reality.
Obtain a set of ambient vibration data from a simple structure in the laboratory as
soon as possible. Get the units right. It is time to take a look at Chap. 6 to know
something about the process that gives you the data, so that you are mindful of the
possible distortions and noise. Repeat experiments to build conﬁdence with the
measured data. Change settings, one at a time, and see if you get what you
expected. Once you are conﬁdent with laboratory data, plan and perform your ﬁeld
test. You may need a different type of sensor (e.g., servo-accelerometer) to pick up
the small ambient vibrations in civil engineering structures. Build conﬁdence with
your sensors in the laboratory ﬁrst. Make necessary preparations (e.g., software) so
1.7
How to Use This Book
23

that you can analyze ﬁeld data with your laptop on site (and you are sure your day is
worth before you leave).
Bayesian OMA
By now hopefully you are conﬁdent with acquiring and analyzing ambient data, and
estimating modal properties empirically from the SV spectrum. You may then
explore modal identiﬁcation more formally. For non-Bayesian methods, you may
consult the references mentioned in Sect. 1.4 or the beginning of Sect. 1.6. For
Bayesian methods, the algorithm for the posterior MPV of modal properties for
well-separated modes in Chap. 12 should be simple to program. The one for
posterior covariance matrix takes more time, involving analytical expressions for
the second derivatives of the NLLF. Program and check them individually before
assembling into a big program. If this sounds tedious, ﬁnite difference approxi-
mation may be resorted but do check its convergence with different step sizes.
Another alternative is the uncertainty laws in Chap. 15, which gives the ballpark
ﬁgure of identiﬁcation uncertainty and may be taken as a good approximation. They
are also quite insightful. If you are more ambitious and time permits, you are
encouraged to try multiple setups (Chap. 14), which is quite useful and allows you
to produce a (more interesting) detailed mode shape with a small number of sensors.
The algorithm for multiple (possibly close) modes (Chap. 13) can take some time to
program, be it for the posterior MPV or covariance matrix. When time does not
permit, it may be better to use an existing program (see Sect. 1.7.4) to explore
interesting applications. Other chapters may be consulted depending on interest and
background needed. Perhaps by this time you may have new ideas to pursue.
1.7.2
Researcher
By a ‘researcher’ I suppose you are not only interested in using OMA methods but
also mastering the underlying principles and mathematical techniques so that you
can develop original research. You may be a PhD student, postdoc, an academic,
engineer or scientist in a research laboratory. You may be familiar with Fourier
analysis, structural dynamics and their probabilistic form, though at varying degree.
Chapters 2–7 are reference materials that may be consulted if needed. In particular,
the long-data asymptotic behavior of FFT is an important fundamental result that is
worth pursuing; see Sects. 4.8 and 4.9. If you want to see what Bayesian OMA
methods can offer, a tour at the examples in Chaps. 12–14 and the uncertainty laws
in Chap. 15 will give you an idea. To have a better appreciation of results, Chap. 10
discusses the underlying assumptions and formulations in different cases. To see
how Bayesian identiﬁcation results may be interpreted differently from those by
conventional frequentist methods, Chaps. 8 and 9 may be consulted.
Chapters 12–14 provide the recipe for programing the posterior MPV and
covariance matrix of modal properties in different settings. Programming for the
MPV is generally easier than the covariance matrix. The algorithms for the MPV
24
1
Introduction

for well-separated modes are straightforward to program, be it for single or multiple
setups. The algorithm for the MPV for close modes takes more time. The same
applies to the covariance matrix using analytical formulas. Finite difference
approximation may provide a temporary solution and a reference for checking. So
are the uncertainty laws (Chap. 15) for well-separated modes.
The uncertainty laws are one of the latest advances in OMA. Take a look at
Chap. 15 to see if they make sense. Chapter 16 outlines how they can be derived,
with details provided to illustrate the techniques in the derivations. The connection
between frequentist and Bayesian uncertainty in Sect. 9.6 provides an effective
systematic means for deriving the uncertainty laws. As a result, the Cramér Rao
Bound (Sect. 9.3) and Fisher information matrix (Sect. 9.4) are relevant, despite
their non-Bayesian origin. Appendix A on complex Gaussian distribution and
Appendix B on Hessian under constraints are general tools that you may ﬁnd
applications in other research areas.
1.7.3
Practitioner
By a ‘practitioner’ I suppose you are an engineer primarily interested in applying
OMA to assist engineering solutions in projects. You may take a look at the ﬁeld
data examples in Chaps. 12–14 to see what Bayesian OMA methods can offer. You
will see that, in additional to the posterior MPV that is analogous to the modal
property estimate in a non-Bayesian method, the methods can also determine the
identiﬁcation uncertainty. How the uncertainty depends on test conﬁguration is
generally complicated, but the relationship reduces to remarkably simple form for
sufﬁciently long data and small damping, both typically encountered in applica-
tions. That is presented in a set of formulas called ‘uncertainty laws’ in Chap. 15.
Using such formulas, the required data length to achieve a desired identiﬁcation
precision can be calculated. The chapter shall convince you that, as far as natural
frequencies and damping ratios are concerned, getting a good sensor is more
cost-effective than getting more sensors. For algorithms, see the advice for
researchers in Sect. 1.7.2. See the advice for students in Sect. 1.7.1 if you are
interested in building some in-house experience with analyzing ambient data. Other
chapters may be consulted when being referred to. Proofs may be skipped.
1.7.4
Supporting Resources
Resources are being developed to disseminate Bayesian OMA research. They may
include computer codes, examples, tutorial notes, database for OMA data and
modal identiﬁcation results, method developments and applications. They may be
searched on the internet with keywords such as ‘Bayesian operational modal
analysis, ‘Bayesian OMA’ or ‘bayoma’.
1.7
How to Use This Book
25

References
Aquino RE, Tamura Y (2016) Potential pitfalls in the practical application of the random
decrement technique. In: Proceedings of 5th International Structural Specialty Conference,
London, ON, Canada, 1–4 June 2016
Au SK (2011) Fast Bayesian FFT method for ambient modal identiﬁcation with separated modes.
J Eng Mech, ASCE 137(3):214–226
Au SK (2012a) Fast Bayesian ambient modal identiﬁcation in the frequency domain, Part I:
posterior most probable value. Mech Syst Signal Process 26(1):60–75
Au SK (2012b) Fast Bayesian ambient modal identiﬁcation in the frequency domain, Part II:
posterior uncertainty. Mech Syst Signal Process 26(1):76–90
Au SK (2016) Insights on the Bayesian spectral density approach for operational modal analysis.
Mech Syst Signal Process 66–67:1–12
Au SK, To P (2012) Full-scale validation of dynamic wind load on a super-tall building under
strong wind. J Struct Eng, ASCE 138(9):1161–1172
Au SK, Zhang FL (2012a) Fast Bayesian ambient modal identiﬁcation incorporating multiple
setups. J Eng Mech, ASCE 138(7):800–815
Au SK, Zhang FL (2012b) Ambient modal identiﬁcation of a primary-secondary structure using
fast Bayesian FFT approach. Mech Syst Signal Process 28:280–296
Au SK, Zhang FL, To P (2012a) Field observations on modal properties of two tall buildings under
strong wind. J Wind Eng Ind Aerodyn 101:12–23
Au SK, Ni YC, Zhang FL et al (2012b) Full scale dynamic testing and modal identiﬁcation of a
coupled ﬂoor slab system. Eng Struct 37:167–178
Bachmann H et al (1995) Vibration problems in structures—practical guidelines. Birkhauser
Verlag, Basel
Brincker R, Ventura C (2015) Introduction to operational modal analysis. Wiley, London
Brincker R, Zhang L, Anderson P (2001) Modal identiﬁcation of output-only systems using
frequency domain decomposition. Smart Mater Struct 10(3):441–455
Brownjohn JMW (2007) Structural health monitoring of civil infrastructure. Philos Trans R Soc
Lond Ser A 365(1851):589–622
Catbas FN, Kijewski-Correa T, Aktan AE (eds) (2011) Structural identiﬁcation of constructed
systems: approaches. Methods, and technologies for effective practice of St-Id. American
Society of Civil Engineers
Cole HA (1973) On-line failure detection and damping measurement of aerospace structures by
random decrement signatures. CR-2205, NASA, Mountain View, CA, USA
Cole HA (2014) Randomdec in retrospect. USA, H.A, Cole, Los Altos, CA
Doebling SW, Farrar CR, Prime MB et al (1998) A review of damage identiﬁcation methods that
examine changes in dynamic properties. Shock Vib Dig 30(2):91–105
Döhler M, Lam XB, Mevel L (2013) Uncertainty quantiﬁcation for modal parameters from
stochastic subspace identiﬁcation on multi-setup measurements. Mech Syst Signal Process
36:562–581
EC1 (2005) Eurocode 1: Actions on structures, part 1–4, general actions: wind actions. EN
1991-1-4:2005+A1:2010. European Committee for Standardization, Brussels
ESDU 83009 (2012) Damping of structures. Part 1: tall buildings. Engineering Sciences Data Unit,
London
Ewins DJ (2000) Modal testing: theory and practice. Research Studies Press, PA, USA
Farrar CR, Worden K (2012) Structural health monitoring: a machine learning perspective. Wiley,
London
Friswell MI, Mottershead JE (1995) Finite element model updating in structural dynamics. Kluwer
Academic Publishers, Dordrecht
Ibrahim SR, Milkulcik EC (1976) The experimental determination of vibration test parameters
from time responses. Shock Vibr Bull 46(5):187–196
26
1
Introduction

ISO 10137 (2007) Bases for design of structures—serviceability of buildings and walkways
against vibrations. ISO 10137:2007. International Organization for Standards, Switzerland
ISO 4354 (2009) Wind actions on structures. ISO 4354:2009. International Organization for
Standards, Switzerland
Juang J, Pappa RS (1985) An eigensystem realization algorithm for modal parameter identiﬁcation
and modal reduction. J Guid Control Dyn 8(5):620–627
Kuok SC, Yuen KV (2012) Structural health monitoring of a reinforced concrete building during
the severe typhoon vicente in 2012. Sci World J 2013. doi:10.1155/2013/509350
Lam HF, Jun Hu, Yang JH (2017) Bayesian operational modal analysis and Markov chain Monte
Carlo-based model updating of a factory building. Eng Struct 132:314–336
Liu P, Zhang FL, Lian PY (2016) Dynamic characteristic analysis of two adjacent multi-grid
composite wall structures with a seismic joint by a Bayesian approach. J Earthquake Eng 20(8):
1295–1321. doi:10.1080/13632469.2016.1138168
Maia N, Silva J (1997) Theoretical and experimental modal analysis. Research Studies Press Ltd,
Baldock
Matarazzo TJ, Pakzad SN (2016a) STRIDE for structural identiﬁcation using expectation
maximization: iterative output-only method for modal identiﬁcation. J Eng Mech 142(4):
04015109
Matarazzo TJ, Pakzad SN (2016b) Sensitivity metrics for maximum likelihood system
identiﬁcation. ASCE-ASME J Risk Uncertainty Eng Syst, Part A: Civ Eng 2:B4015002
McConnell K (1995) Vibration testing—theory and practice. John Wiley & Sons, New York
Ni YC, Zhang FL (2016) Bayesian operational modal analysis of a pedestrian bridge using a ﬁeld
test with multiple setups. Int J Struct Stab Dyn 16(8):1550052
Ni YQ, Zhang FL, Xia YX, Au SK (2015) Operational modal analysis of a long-span suspension
bridge under different earthquake events. Earthquakes Struct 8(4):859–887
Ni YC, Lu XL, Lu WS (2016a) Field dynamic test and Bayesian modal identiﬁcation of a special
structure-the palms together dagoba. Struct Control Health Monit 23(5):838–856
Ni YC, Zhang FL, Lam HF (2016b) Series of full-scale ﬁeld vibration tests and Bayesian modal
identiﬁcation of a pedestrian bridge. J Bridge Eng ASCE 21(8):C4016002
Ni YC, Lu XL, Lu WS (2017) Operational modal analysis of a high-rise multi-function building
with dampers by a Bayesian approach. Mech Syst Signal Process 86:286–307
Overschee PV, De Moor BL (1996) Subspace identiﬁcation for linear systems, theory,
implementation, applications. Kluwer Academic Publishers, Dordrecht
Peeters B, De Roeck G (2001) Stochastic system identiﬁcation for operational modal analysis: a
review. J Dyn Syst, Meas, Control 123:659–667
Pierre JW, Trudnowski DJ, Donnelly MK (1997) Initial results in electromechanical mode
identiﬁcation from ambient data. IEEE Trans Power Syst 12(3):1245–1251
Pintelon R, Schoukens J (2001) System identiﬁcation, a frequency domain approach. Wiley-IEEE
Press
Pintelon R, Guillaume P, Schoukens J (2007) Uncertainty calculation in (operational) modal
analysis. Mech Syst Signal Process 21:2359–2373
Reynders E (2012) System identiﬁcation methods for (operational) modal analysis: review and
comparison. Arch Comput Methods Eng 19(1):51–124
Reynders E, Pintelon R, De Roeck G (2008) Uncertainty bounds on modal parameters obtained
from stochastic subspace identiﬁcation. Mech Syst Signal Process 22:948–969
Rogers G (2000) Power system oscillations. Springer, New York
Seppänen JM, Au SK, Turunen J et al (2016) Bayesian approach in the modal analysis of
electromechanical oscillations. IEEE Trans Power Syst 32(1):316–325
Vold H, Kundrat J, Rocklin GT et al (1982) A multi-input modal estimation algorithm for
mini-computer. SAE Technical Paper Series
Wenzel H, Pichler D (2005) Ambient vibration monitoring. John Wiley & Sons, UK
Xu YL, Xia Y (2012) Structural health monitoring of long-span suspension bridges. Spon Press
(Taylor& Francis), UK
References
27

Yan WJ, Katafygiotis LS (2014) A two-stage fast Bayesian spectral density approach for ambient
modal analysis, Part I: posterior most probable value and uncertainty. Mech Syst Signal
Process 54–55:139–155
Yuen, KV (1999) Structural modal identiﬁcation using ambient dynamic data. Master of
Philosophy Thesis, Hong Kong University of Science and Technology
Yuen KV, Katafygiotis LS (2001a) Bayesian time-domain approach for modal updating using
ambient data. Probab Eng Mech 16:219–231
Yuen KV, Katafygiotis LS (2001b) Bayesian spectral density approach for modal updating using
ambient data. Earthq Eng Struct Dyn 20:1103–1123
Yuen KV, Katafygiotis LS (2003) Bayesian fast Fourier transform approach for modal updating
using ambient data. Adv Struct Eng 6(2):81–95
Zhang FL (2011) Bayesian ambient modal identiﬁcation incorporating multiple setups. Doctor of
Philosophy Thesis, City University of Hong Kong
Zhang FL, Au SK (2013) Erratum for fast Bayesian FFT method for ambient modal identiﬁcation
with separated modes. J Eng Mech, ASCE 139(4):545
Zhang FL, Au SK, Lam HF (2015) Assessing uncertainty in operational modal analysis
incorporating multiple setups using a Bayesian approach. Struct Control Health Monit 22:
395–416
Zhang FL, Ni YQ, Ni YC (2016a) Mode identiﬁability of a cable-stayed bridge based on a
Bayesian method. Smart Struct Syst 17(3):471–489
Zhang FL, Xiong HB, Shi WX et al (2016b) Structural health monitoring of a super tall building
during different stages using a Bayesian approach. Struct Control Health Monit. doi:10.1002/
stc.1840
Zhang FL, Ni YQ, Ni YC et al (2016c) Operational modal analysis of Canton Tower by a fast
frequency domain Bayesian method. Smart Struct Syst 17(2):209–230
28
1
Introduction

Chapter 2
Spectral Analysis of Deterministic Process
Abstract This chapter analyzes a deterministic process in the frequency domain via
Fourier series (for period process) and Fourier Transform (for non-period process).
Estimators based on a sample time history of the process are introduced and their
possible distortions arising from ﬁnite sampling rate (aliasing) and duration (leak-
age) are discussed. The Fast Fourier Transform provides an efﬁcient computational
tool for digital implementation. The chapter contains a section that connects the
presented mathematical theory to implementation in Matlab, which is a convenient
platform for scientiﬁc computing.
Keywords Fourier series  Fourier Transform  Fast Fourier Transform  Matlab 
Aliasing  Leakage
A time series, or ‘process’, describes how a quantity varies with time. Viewing as a
function of time allows one to see, e.g., when it is zero, how fast it changes, the
minimum and maximum values. This ‘time domain’ view is not the only per-
spective. One useful alternative is the ‘frequency domain’ view, as a sum of
‘harmonics’ (sines and cosines) at different frequencies and studying how their
amplitude varies with frequency. This view is especially relevant for a variety of
processes that exhibit variations at different time scales, revealing the characteristics
of contributing activities. It also offers a powerful means for studying the oscillatory
response of systems with resonance behavior.
In this chapter we introduce the theory for analyzing a process in the frequency
domain, namely, ‘Fourier analysis’. The basic result is that a periodic process can
be written as a ‘Fourier series’ (FS), which is a sum of harmonics at discrete
frequencies. An analogous result holds for a non-periodic process with ﬁnite
energy, where the sum becomes an integral over a continuum of frequencies and is
called ‘Fourier Transform’ (FT). In digital computations, the integrals involved in
Fourier analysis can be approximated in discrete time and computed efﬁciently via
the ‘Fast Fourier Transform’ (FFT) algorithm. This approximation leads to dis-
tortions, namely, limited scope by ‘Nyquist frequency’, ‘aliasing’ and ‘leakage’.
These must be in check so that the calculated results reﬂect well their targets and are
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_2
29

correctly interpreted. Fourier theory is often introduced in undergraduate texts of
differential equations, e.g., Boyce and DiPrima (2005). See Champeney (1987) for
Fourier theorems and Sundararajan (2001) for FFT. Digital signal processing is a
closely related subject, see, e.g., Lathi (2000). Smith (2002) gives a non-technical
coverage.
As the title tells, this chapter is about ‘deterministic process’, where the subject
time series is taken as ‘ﬁxed’ or ‘given’. No probability concept is involved. While
all data (including the ones in operational modal analysis) are by deﬁnition deter-
ministic when they are obtained, their downstream effects can be understood much
better when they are modeled using probabilistic concepts. This will be taken up in
Chap. 4, where Fourier analysis is applied in a probabilistic context.
2.1
Periodic Process (Fourier Series)
A periodic process repeats itself at a ﬁxed time interval. We say that a function xðtÞ
of time t (s) is ‘periodic with period T’ if T is the smallest value such that
xðt þ TÞ ¼ xðtÞ
for any t
ð2:1Þ
The proviso ‘the smallest value’ is necessary to remove ambiguity, because if (2.1)
holds then for any integer m:
xðt þ mTÞ ¼ xðt þ ðm  1ÞT þ TÞ ¼ xðt þ ðm  1ÞTÞ ¼    ¼ xðt þ TÞ ¼ xðtÞ
ð2:2Þ
According to the Fourier theorem, a periodic process xðtÞ can be written as a
‘Fourier series’ (FS):
xðtÞ ¼ a0 þ
X
1
k¼1
ak cos xkt þ
X
1
k¼1
bk sin xkt
xk ¼ 2pk
T
ð2:3Þ
Here, ak and bk are called the (real) ‘Fourier series coefﬁcients’, associated with
harmonic oscillations with period T=k (s), i.e., frequency xk ¼ 2pk=T (rad/s). This
can be seen by noting that as t goes from 0 to T=k the argument xkt in the cosine
and sine terms goes from 0 to 2p, hence completing one cycle. The term a0
accounts for the constant ‘static’ level of the process. Generally, a periodic process
need not be just a ﬁnite sum of cosines and sines, but the Fourier theorem says that
by including an inﬁnite number of them with systematically increasing frequencies
it is possible to represent any periodic process.
30
2
Spectral Analysis of Deterministic Process

Amplitude and Phase
By writing
ak cos xkt þ bk sin xkt ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2
k þ b2
k
q

ak
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2
k þ b2
k
p
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
cos /k
cos xkt þ
bk
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2
k þ b2
k
p
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
sin /k
sin xkt

ð2:4Þ
and using the compound angle formula cosðh1  h2Þ ¼ cos h1 cos h2 þ sin h1 sin h2,
(2.3) can be written as
xðtÞ ¼ a0 þ
X
1
k¼1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2
k þ b2
k
q
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
amplitude
cosð
xk
|{z}
frequency
t  /k
|{z}
phase
Þ
tan /k ¼ bk
ak
ð2:5Þ
Expressions of Fourier Series Coefﬁcients
Clearly, ak and bk depend on xðtÞ. They are given by
a0 ¼ 1
T
ZT=2
T=2
xðtÞ dt
ð2:6Þ
ak ¼ 2
T
R
T=2
T=2
xðtÞ cos xkt dt
bk ¼ 2
T
R
T=2
T=2
xðtÞ sin xkt dt
k ¼ 1; 2; . . .
ð2:7Þ
In these expressions, the integration domain can be any continuous interval of
length T because xðtÞ has period T.
Proof of (2.6) and (2.7) (Fourier Series Coefﬁcients)
The expression of a0 in (2.6) can be shown by integrating both sides of (2.3) w.r.t.
t from T=2 to T=2 and noting that the integrals of sines and cosines on the RHS
are all equal to zero. The derivation of ak and bk ðk  1Þ makes use of the following
results (j and k are non-zero integers):
ZT=2
T=2
cos xjt cos xkt dt ¼
0
j 6¼ k
T=2
j ¼ k

ð2:8Þ
2.1
Periodic Process (Fourier Series)
31

ZT=2
T=2
sin xjt sin xkt dt ¼
0
j 6¼ k
T=2
j ¼ k

ð2:9Þ
ZT=2
T=2
cos xjt sin xkt dt ¼ 0
any j; k
ð2:10Þ
In particular, multiplying both sides of xðtÞ ¼ a0 þ P1
j¼1 aj cos xjt þ P1
j¼1 bj sin xjt
with cos xkt and integrating from T=2 to T=2;
ZT=2
T=2
xðtÞ cos xkt dt¼ a0
ZT=2
T=2
cos xkt dt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
0
þ
X
1
j¼1
aj
ZT=2
T=2
cos xjt cos xkt dt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
T=2 if j¼k; else 0
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
akT=2
þ
X
1
j¼1
bj
ZT=2
T=2
sin xjt cos xkt dt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
0
¼ akT
2
ð2:11Þ
Rearranging gives the expression of ak
in (2.7). Multiplying xðtÞ ¼ a0 þ
P1
j¼1 aj cos xjt þ P1
j¼1 bj sin xjt with sin xkt and following similar steps gives the
expression of bk in (2.7).
■
2.1.1
Complex Exponential Form
The FS in (2.3) contains for each frequency a sine and cosine term. It is possible to
combine the two terms into a single term using the ‘Euler formula’:
eih ¼ cos h þ i sin h
i2 ¼ 1
ð2:12Þ
32
2
Spectral Analysis of Deterministic Process

for any real number h. Conversely, cosine and sine can be written in terms of eih as
cos h ¼ 1
2 ðeih þ eihÞ
sin h ¼ 1
2i ðeih  eihÞ
ð2:13Þ
Applying these identities to (2.3) and writing 1=2i ¼ i=2 gives
xðtÞ ¼ a0 þ
X
1
k¼1
ak
2 ðeixkt þ eixktÞ þ
X
1
k¼1
 ibk
2 ðeixkt  eixktÞ
¼ a0
|{z}
c0
þ
X
1
k¼1
1
2 ðak  ibkÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
ck
eixkt þ
X
1
k¼1
1
2 ðak þ ibkÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
ck
eixkt
ð2:14Þ
The FS can then be written in a compact manner as
xðtÞ ¼
X
1
k¼1
ckeixkt
xk ¼ 2pk
T
ð2:15Þ
where fckg1
k¼1 are the ‘complex Fourier series coefﬁcients’, related to the real
ones by
c0 ¼ a0
ck ¼ 1
2 ðak  ibkÞ
ck ¼ 1
2 ðak þ ibkÞ
k ¼ 1; 2; . . .
ð2:16Þ
Substituting (2.7) into (2.16) gives ck in terms of xðtÞ:
ck ¼ 1
T
ZT=2
T=2
xðtÞeixkt dt
xk ¼ 2pk
T ; k ¼ 0; 1; 2; . . .
ð2:17Þ
It can be veriﬁed that ck has the ‘conjugate mirror property’:
ck ¼ ck
ð2:18Þ
where a bar on top denotes the complex conjugate, i.e., a þ bi ¼ a  bi for real
numbers a and b.
The complex form of FS signiﬁcantly simpliﬁes algebra in Fourier analysis and
is widely used. It takes some time to master algebraic skills with complex numbers
but is worthwhile to do so.
2.1
Periodic Process (Fourier Series)
33

2.1.2
Parseval Equality
The ‘energy’ of a process when viewed in the time or frequency domain can be
equated via the ‘Parseval equality’, which appears in different forms depending on
the context. For a periodic process with ﬁnite energy
R T=2
T=2 xðtÞ2 dt\1,
1
T
ZT=2
T=2
xðtÞ2dt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Time domain
¼ a2
0 þ 1
2
X
1
k¼1
ða2
k þ b2
kÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Freq: domain real coeff:
¼
X
1
k¼1
jckj2
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
Freq: domain complex coeff:
ð2:19Þ
The leftmost expression may be interpreted as the average energy per unit time.
The Parseval equality says that this energy can be viewed as the sum of contri-
butions from harmonics at different frequencies. If one substitutes the FS of xðtÞ in
(2.3) into the leftmost expression, after squaring, one will obtain an integral of a
double sum of cosine and sine products. The non-trivial (and beautiful) result is that
only the integrals of cos  cos and sin  sin terms with the same frequency are
non-zero, giving the neat frequency domain expressions in the middle and right-
most that contain no cross terms.
Proof of (2.19) (Parseval Equality, Fourier Series)
We prove the Parseval equality (2.19) using the complex FS in (2.15). Writing
x2 ¼ xx,
1
T
ZT=2
T=2
xðtÞ2 dt¼ 1
T
ZT=2
T=2
X
1
k¼1
ckeixkt
 
!
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
xðtÞ
X
1
j¼1
cjeixjt
 
!
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
xðtÞ
dt
¼ 1
T
X
1
k¼1
X
1
j¼1
ckcj
Z T=2
T=2
eiðxkxjÞt dt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
T if j¼k; else 0
¼
X
1
k¼1
jckj2
ð2:20Þ
In the second equality, the order of inﬁnite sum and integration has been swapped,
which is legitimate when the process has ﬁnite energy. The result of the integral can
be reasoned as follow. Clearly, it is equal to T when j ¼ k. Otherwise ðj 6¼ kÞ,
34
2
Spectral Analysis of Deterministic Process

ZT=2
T=2
eiðxkxjÞt dt ¼ eiðxk  xjÞT=2
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
pðkjÞ
 eiðxk  xjÞT=2
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
pðkjÞ
i ðxk  xjÞ
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
2pðkjÞ=T
¼ 2i sin pðk  jÞ
zﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄ{
0
2piðk  jÞ=T ¼ 0
ð2:21Þ
■
2.2
Non-periodic Process (Fourier Transform)
If a process is not periodic then it cannot be written as a FS. In this case, if it has
ﬁnite energy then it can still be represented as a sum of harmonics, although now
there is a continuum of contributing frequencies, each with inﬁnitesimal contri-
bution. Speciﬁcally, a process xðtÞ deﬁned for 1\t\1 with ﬁnite energy
R 1
1 xðtÞ2 dt\1 can be written as
xðtÞ ¼ 1
2p
Z1
1
XðxÞeixt dx
ð2:22Þ
XðxÞ ¼
Z 1
1
xðtÞeixt dt
ð2:23Þ
Here, XðxÞ is called the ‘Fourier Transform’ (FT) of xðtÞ; and xðtÞ is the ‘inverse
Fourier Transform’ of XðxÞ. In FT, the frequency x is continuous-valued. This is in
contrast to FS, where the frequencies fxkg are discrete-valued. The factor 1=2p in
(2.22) may appear peculiar but it can be explained by consideration of units; see
Sect. 2.7.1 later.
2.2.1
From Fourier Series to Fourier Transform
FT can be reasoned from FS as follow. Consider approximating xðtÞ by a periodic
function xpðtÞ with period T, where xpðtÞ ¼ xðtÞ for T=2\t\T=2 but simply
repeats itself elsewhere. This is illustrated in Fig. 2.1.
Intuitively, xpðtÞ (as a function) converges to xðtÞ as T ! 1. For a given T, let
ck be the complex FS coefﬁcients of xpðtÞ at frequency xk ¼ 2pk=T. Then
2.1
Periodic Process (Fourier Series)
35

ck ¼ 1
T
Z T=2
T=2
xpðtÞeixkt dt ¼ 1
T
Z T=2
T=2
xðtÞeixkt dt
ð2:24Þ
since
xpðtÞ ¼ xðtÞ
for
T=2\t\T=2.
Using
Cauchy-Schwartz
inequality
(Sect. C.5.6),
jckj2  1
T2
ZT=2
T=2
xðtÞ2 dt
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
 R1
1
xðtÞ2 dt

ZT=2
T=2
jeixktj2
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
1
dt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
T
 1
T
Z1
1
xðtÞ2 dt
ð2:25Þ
Since
R 1
1 xðtÞ2 dt\1, the above implies jckj ! 0 as T ! 1. This indicates that
FS coefﬁcients are not legitimate quantities for studying the frequency character-
istics of a non-period process, because they all diminish trivially as T ! 1, no
matter what the process is. The factor 1=T is the source of diminishing magnitude.
The following function is motivated by taking out the factor 1=T in (2.24) and
replacing xk by the continuous-valued frequency variable x:
XpðxÞ ¼
ZT=2
T=2
xðtÞeixt dt
ð2:26Þ
By construction, ck ¼ XpðxkÞ=T and so
Fig. 2.1 Original process xðtÞ and periodic proxy xpðtÞ
36
2
Spectral Analysis of Deterministic Process

xpðtÞ ¼
X
1
k¼1
ckeixkt ¼
X
1
k¼1
XpðxkÞeixkt 1
T
ð2:27Þ
As T ! 1; xpðtÞ ! xðtÞ as a function. Also, the frequency interval Dx ¼
xk þ 1  xk ¼ 2p=T diminishes. The inﬁnite sum on the RHS of (2.27) tends to an
integral. Thus,
xðtÞ ¼ lim
T!1 xpðtÞ ¼ lim
T!1
X
1
k¼1
XpðxkÞeixkt Dx
2p ¼ 1
2p
Z1
1
XðxÞeixt dx
ð2:28Þ
where
XðxÞ ¼ lim
T!1 XpðxÞ ¼
Z1
1
xðtÞeixt dt
ð2:29Þ
In the above reasoning, we have swapped the order of limit and integration. This is
legitimate when the process has ﬁnite energy.
2.2.2
Properties of Fourier Transform
Some properties of FT are listed in Table 2.1. They can be shown directly from
deﬁnition. The symbol Ffxg denotes the FT of xðtÞ. It is a function of frequency x
but this is omitted for simplicity.
Table 2.1 Some properties of Fourier Transform
Property
Description
Conjugate
mirror
Let XðxÞ be the FT of xðtÞ. Then XðxÞ ¼ XðxÞ
Linearity
For any scalars a and b, Ffax þ byg ¼ aFfxg þ bFfyg
Differentiation
Ff_xg ¼ ixFfxg
Time shift
For any s, let yðtÞ ¼ xðt þ sÞ. Then Ffyg ¼ eixsFfxg
Convolution
Let zðtÞ be the ‘convolution’ between xðtÞ and yðtÞ, deﬁned as
zðtÞ ¼ R 1
1 xðt  sÞyðsÞds
The FT of convolution is equal to the product of FTs, i.e.,
Ffzg ¼ FfxgFfyg
2.2
Non-periodic Process (Fourier Transform)
37

2.2.3
Dirac Delta Function
The ‘Dirac Delta function’ dðtÞ, or Delta function in short, is an idealized unit
impulse of arbitrarily short duration centered at t ¼ 0. It does not exist physically
but is frequently used in analysis and modeling. It has the property that
Ze
e
dðtÞxðtÞ dt ¼ xð0Þ
ð2:30Þ
for any e [ 0 and function xðtÞ. The FT of the Delta function is simply the constant
1 because
Z1
1
dðtÞeixtdt ¼ eixð0Þ ¼ 1
for any x
ð2:31Þ
The inverse FT of the constant 1 gives the FT representation of the Delta function:
dðtÞ ¼ 1
2p
Z1
1
eixtdx
ð2:32Þ
This is frequently used in Fourier analysis.
2.2.4
Parseval Equality
For a non-period process xðtÞ that has FT XðxÞ, the Parseval equality reads
Z1
1
xðtÞ2 dt ¼ 1
2p
Z1
1
jXðxÞj2dx
ð2:33Þ
Proof of (2.33) (Parseval Equality, Fourier Transform)
The proof for (2.33) has a similar structure as the one in (2.20) for FS, except that
the inﬁnite sums now become integrals:
38
2
Spectral Analysis of Deterministic Process

Z1
1
xðtÞ2 dt¼
Z1
1
1
2p
Z1
1
XðxÞeixtdx
2
4
3
5
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
xðtÞ
1
2p
Z1
1
Xðx0Þeix0t dx0
2
4
3
5
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
xðtÞ
dt
¼
1
ð2pÞ2
Z1
1
Z1
1
Z1
1
XðxÞXðx0Þeiðxx0Þt dx0 dxdt
¼ 1
2p
Z1
1
Z1
1
XðxÞXðx0Þ 1
2p
Z1
1
eiðxx0Þt dt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
dðxx0Þ
dx0 dx
¼ 1
2p
Z1
1
XðxÞ
Z1
1
Xðx0Þdðx  x0Þ dx0
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
XðxÞ
dx
¼ 1
2p
Z1
1
jXðxÞj2dx
ð2:34Þ
In arriving at the third equality, the order of integration w.r.t. t and ðx; x0Þ has been
swapped. This is legitimate when the process has ﬁnite energy.
■
2.3
Discrete-Time Approximation with FFT
In digital computations, a process is sampled at discrete time instants. The integrals
in FS and FT can be approximated by a Riemann sum on the grid of sampled time
instants. Let fxj ¼ xðjDtÞgN1
j¼0 be the N sample values of xðtÞ at equal time interval
Dt (s). A discrete-time approximation to the FT of xðtÞ is constructed by replacing
the integral in (2.23) with a Riemann sum:
XðxÞ ¼
Z1
1
xðtÞeixt dt  ^XðxÞ ¼
X
N1
j¼0
xjeixjDtDt
ð2:35Þ
Here, ^XðxÞ is called the ‘discrete-time Fourier Transform’ (DTFT) of xðtÞ.
Evaluating ^XðxÞ for a given value of x involves a summation in the time domain.
In practice it is only calculated at the following uniformly spaced frequencies:
2.2
Non-periodic Process (Fourier Transform)
39

xk ¼ 2pk
NDt rad/s
ð
Þ
k ¼ 0; . . .; N  1 FFT frequencies
ð
Þ
ð2:36Þ
It is because the values of ^XðxÞ at these frequencies can be evaluated very efﬁ-
ciently. This is discussed next.
2.3.1
Fast Fourier Transform
The ‘Fast Fourier Transform’ (FFT) algorithm (Cooley and Tukey 1965) provides
an efﬁcient means for calculating the values of DTFT at a speciﬁc set of frequencies
as in (2.36). It is commonly coded in commercial software or programming
packages; see Sect. 2.9 later for an introduction. Here we focus on the deﬁnition
and properties of FFT.
The FFT of fxjgN1
j¼0 is the sequence fykgN1
k¼0 deﬁned by
yk ¼
X
N1
j¼0
xje2pijk=N
k ¼ 0; . . .; N  1
FFT
ð
Þ
ð2:37Þ
The ‘inverse FFT’ of fykgN1
k¼0 is deﬁned as the sequence fzjgN1
j¼0 where
zj ¼ 1
N
X
N1
k¼0
yke2pijk=N
j ¼ 0; . . .; N  1
inverse FFT
ð
Þ
ð2:38Þ
Note that fykgN1
k¼0
is generally complex-valued, even though fxjgN1
j¼0
is
real-valued. In the literature, fykgN1
k¼0 is referred as the ‘discrete Fourier Transform’
(DFT, not to be confused with DTFT) of fxjgN1
j¼0 . In this book, we simply refer it as
FFT, because DFT is almost always evaluated via FFT and there is little distinction
between the two terms.
Inverse FFT Recovers the Original Sequence
The following shows that inverse FFT indeed recovers the original sequence that
produces the FFT, i.e., xj ¼ zj, j ¼ 0; . . .; N  1. Substituting yk ¼ PN1
r¼0 xre2pirk=N
into (2.38),
40
2
Spectral Analysis of Deterministic Process

zj ¼ 1
N
X
N1
k¼0
X
N1
r¼0
xre2pirk=N
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
yk
e2pijk=N ¼ 1
N
X
N1
r¼0
xr
X
N1
k¼0
e2piðjrÞk=N
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
N if r¼j;else 0
¼ xj
ð2:39Þ
The result of the sum over k used above is discussed next.
Exponential Sum Formula
Sums of exponentials are frequently encountered in Fourier analysis and it is worth
to get familiarized with their analytical formulas. One basic result is that for any
integer k,
X
N1
j¼0
e2pijk=N ¼
N
k=N ¼ 0; 1; 2; . . .
0
otherwise

ð2:40Þ
The ﬁrst case when k is an integer multiple of N is trivial because e2pip ¼ 1 for any
integer p. Otherwise, using the geometric series summation formula PN1
j¼0 a j ¼
ð1  aNÞ=ð1  aÞ with a ¼ e2pik=N,
X
N1
j¼0
e2pijk=N ¼ 1  e2pik
z}|{
1
1  e2pik=N
|ﬄﬄ{zﬄﬄ}
6¼1
¼ 0
ðk
N not integerÞ
ð2:41Þ
Taking complex conjugate on (2.40) shows that the result is the same if 2pijk=N is
replaced by 2pijk=N.
Conjugate Mirror Property
The FFT fykgN1
k¼0 of a real-valued sequence fxjgN1
j¼0
has the conjugate mirror
property that
yNk ¼ yk
ð2:42Þ
This is because
yNk ¼
X
N1
j¼0
xje2pijðNkÞ=N ¼
X
N1
j¼0
xj e2pij
|ﬄ{zﬄ}
1
e2pijk=N ¼
X
N1
j¼0
xje2pijk=N ¼ yk
ð2:43Þ
The conjugate mirror property is illustrated in Fig. 2.2. About half of the FFT
sequence carries redundant information, in the sense that it can be produced as the
complex conjugate of the other half.
2.3
Discrete-Time Approximation with FFT
41

2.3.2
Approximating Fourier Transform and Fourier Series
Back to the problem of approximating the FT XðxkÞ with the FFT of fxj ¼
xðjDtÞgN1
j¼0 as in (2.35). Denoting ^Xk ¼ ^XðxkÞ and noting xkjDt ¼ 2pjk=N,
^Xk ¼ Dt
X
N1
j¼0
xje2pijk=N
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
ykðFFTÞ
ð2:44Þ
Thus,
XðxkÞ  ^Xk ¼ ykDt
ð2:45Þ
For FS coefﬁcients, let fxj ¼ xðjDtÞgN1
j¼0 be the sampled sequence of a periodic
process xðtÞ with period T. Assume that NDt ¼ T so that the FFT frequency xk ¼
2pk=NDt coincides with the FS frequency xk ¼ 2pk=T. Approximating the integral
in (2.17) by a Riemann sum,
ck ¼ 1
T
ZT=2
T=2
xðtÞeixkt dt  ^ck ¼
1
NDt
X
N1
j¼0
xje2pijk=N
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
yk
Dt
ð2:46Þ
0
=
k
1
2
2
1
−
= N
N yq
1
−
yq
N
1
+
yq
N
1
−
N
2
−
N
2
+
yq
N
Re
Im
0
=
k
1
2
2
N
N yq =
1
−
yq
N
1
+
yq
N
1
−
N
2
−
N
2
−
yq
N
2
+
yq
N
Re
Im
(a) N odd
(b) N even
Fig. 2.2 Conjugate mirror property of the FFT of a real-valued sequence. a N odd; b N even.
Nyq ¼ integer part of N=2, is the index at or just below the ‘Nyquist frequency’ (Sect. 2.4.1)
42
2
Spectral Analysis of Deterministic Process

Thus
ck  ^ck ¼ yk
N
ð2:47Þ
The use of ^ck in (2.46) can be generalized to allow NDt 6¼ T, which may provide
convenience in practice, e.g., when the period is not known, data duration is not
equal to the period, or it is desirable to estimate the FS of measured data using more
than one period to average out noise.
2.3.3
Parseval Equality
For a real-valued sequence fxjgN1
j¼0 with FFT fykgN1
k¼0 , Parseval equality reads
X
N1
j¼0
x2
j ¼ 1
N
X
N1
k¼0
jykj2
ð2:48Þ
Proof of (2.48) (Parseval Equality, FFT)
The structure of the proof is similar to that for FS in (2.20) or FT in (2.34):
X
N1
j¼0
x2
j ¼
X
N1
j¼0
1
N
X
N1
k¼0
yke2pijk=N
"
#
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
xj
1
N
X
N1
r¼0
yre2pijr=N
"
#
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
xj
¼ 1
N2
X
N1
j¼0
X
N1
k¼0
X
N1
r¼0
ykyre2pijðkrÞ=N
¼ 1
N2
X
N1
k¼0
X
N1
r¼0
ykyr
X
N1
j¼0
e2pijðkrÞ=N
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
N if r¼k;else 0
¼ 1
N
X
N1
k¼0
jykj2
ð2:49Þ
■
2.4
Distortions in Fourier Series
The discrete-time approximation of FS and FT leads to errors of a characteristic
nature. In the ﬁrst place, the approximation is valid only up to the ‘Nyquist fre-
quency’ (1=2Dt Hz). It is also contaminated with harmonics in the original process
2.3
Discrete-Time Approximation with FFT
43

beyond the Nyquist frequency. This is known as ‘aliasing’, where high frequency
variations are mistaken as low frequency ones. It is the same mechanism that our
eyes see the rotor blade of a ﬂying helicopter as slowly rotating (sometimes
reversing direction). There is further contamination from harmonics that do not
have an integer multiple of cycles within the measured time span, in a Oð1=NDtÞ
neighborhood of the subject frequency (and aliased counterparts). This is known as
‘leakage’. Aliasing is not repairable after discrete-time sampling and so harmonics
beyond the Nyquist frequency should be ﬁltered out beforehand. Leakage can be
suppressed by increasing data duration. We ﬁrst discuss the distortions in the FFT
approximation of FS. The discussion for FT follows in the next section.
Recall the context in (2.46), where xðtÞ is a periodic process with complex
FS coefﬁcients fckg1
k¼1; fxj ¼ xðjDtÞgN1
j¼0 is a discrete-time sample sequence of
xðtÞ; fykgN1
k¼0 is the FFT of fxjgN1
j¼0 ; and ^ck ¼ yk=N is a discrete-time approxi-
mation of ck.
2.4.1
Nyquist Frequency
Although f^ckgN1
k¼0 is a sequence with N terms, only the ﬁrst half is informative.
This stems from the conjugate mirror property:
^cNk ¼ ^ck
ð2:50Þ
It implies that ^ck is conjugate symmetric about the index N=2, i.e., frequency
fNyq ¼ ðN=2Þ=NDt ¼ 1=2Dt (Hz), which is called the ‘Nyquist frequency’. As a
result, ^ck can only give a proper estimation of ck up to the Nyquist frequency.
2.4.2
Aliasing
Aliasing occurs when the original process xðtÞ contains harmonics at frequencies
beyond the Nyquist frequency. Suppose the data duration is equal to the period of
xðtÞ, i.e., NDt ¼ T. Then it can be shown that (see the end)
^ck ¼
X
1
m¼1
cmN þ k
ð2:51Þ
In addition to ck (the m ¼ 0 term), ^ck also contains other terms, ckN, ck2N and so
on. To see the contributing (positive) frequencies, separate the sum into positive
and negative m, and use the conjugate mirror property of ck:
44
2
Spectral Analysis of Deterministic Process

^ck ¼ ck þ
X
1
m¼1
cmN þ k þ
X
1
m¼1
cmN þ k
¼ ck þ ðcN þ k þ c2N þ k þ   Þ þ ðcN þ k þ c2N þ k þ   Þ
¼ ck þ ðcN þ k þ c2N þ k þ   Þ þ ðcNk þ c2Nk þ   Þ
ð2:52Þ
Combining the two inﬁnite sums,
^ck
|{z}
freq:fk
¼
ck
|{z}
freq:fk
þ
X
1
m¼1
ð cmN þ k
|ﬄﬄ{zﬄﬄ}
freq:mfs þ fk
þ
cmNk
|ﬄﬄ{zﬄﬄ}
freq:mfsfk
Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
aliasing
ð2:53Þ
That is, ^ck is contaminated with contributions from frequencies (in Hz) fs  fk,
2fs  fk, …, where fs ¼ 1=Dt is the sampling frequency and fk ¼ k=NDt is the
subject FFT frequency. Aliasing occurs by the same mechanism when the data
duration is an integer multiple of the period. When the data duration is not even an
integer multiple of the period, there will also be ‘leakage’; see the next section.
Example 2.1 (Aliasing with a single harmonic) Consider xðtÞ ¼ 2 cos 2pft,
which is a single harmonic with frequency f (Hz). Its real FS coefﬁcients fak; bkg
are zero except a1 ¼ 2. Its complex FS coefﬁcients fckg are zero except c1 ¼ 1 and
c1 ¼ 1. Suppose we obtain the samples fxj ¼ xðjDtÞgN1
j¼0
at Dt ¼ 0:1 s and
N ¼ 10, i.e., for a duration of NDt ¼ 1 s. Using fxjgN1
j¼0 , we estimate the complex
FS coefﬁcients by the FFT approximation ^ck ¼ N1 PN1
j¼0 xje2pijk=N in (2.46).
Figure 2.3 illustrates the possible distortions in ^ck, depending on the source
frequency f . The plots on the left column show xðtÞ (dashed line) and the sample
points fxjgN1
j¼0 (dots). The plots on the right column show j^ckj (dot with stick)
versus the FFT frequency fk ¼ k=NDt (Hz) for k ¼ 0; . . .; N  1. The shaded part
from 5 Hz (Nyquist frequency) to 10 Hz (sampling frequency) is just the mirror
image of that from 0 to 5 Hz. It is usually not plotted but is shown here for
illustration. When f ¼ 1 or 4 Hz, j^ckj is correctly estimated up to the Nyquist
frequency. When f ¼ 6 Hz or 11 Hz, which are beyond the Nyquist frequency, the
harmonic is mistaken (aliased) to be 4 Hz and 1 Hz, respectively.
■
Proof of (2.51) (Aliasing, Fourier Series)
Since NDt ¼ T, xr ¼ 2pr=T ¼ 2pr=NDt and so xrjDt ¼ 2pjr=N. Substituting
xj ¼ P1
r¼1 cre2pijr=N into ^ck in (2.46),
2.4
Distortions in Fourier Series
45

^ck ¼ 1
N
X
N1
j¼0
X
1
r¼1
cre2pijr=N
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
xj
e2pijk=N
¼ 1
N
X
1
r¼1
cr 
X
N1
j¼0
e2pijðrkÞ=N
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
N if ðrkÞ=N¼0;1;2;...;else 0
¼
X
1
m¼1
cmN þ k
ð2:54Þ
■
2.4.3
Leakage
Leakage occurs in the FFT approximation of FS when the data duration NDt is not
an integer multiple of the period T. In this case, a given FS frequency xr ¼ 2pr=T
need not be matched by a FFT frequency xk ¼ 2pk=NDt. When this occurs, the FS
harmonic will ‘leak out’ to other FFT frequencies. Leakage (and aliasing) can be
explained by the following general formula that expresses ^ck as the convolution of
fcrg1
r¼1 with a ‘kernel function’ K1ðxÞ:
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
-2
0
2
f = 1 Hz
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
-2
0
2
f = 4 Hz
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
-2
0
2
f = 6 Hz
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
-2
0
2
f = 11 Hz
t (sec)
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
Freq. fk = k/NΔt (Hz)
Fig. 2.3 FS amplitude of xðtÞ ¼ 2 cos 2pft estimated via the FFT of fxj ¼ xðjDtÞgN1
j¼0
with
Dt ¼ 0:1 s and N ¼ 10, for f ¼ 1; 4; 6 and 11 Hz. Left column: dashed line = xðtÞ, dot ¼ xj. Right
column: dot with stick = j^ckj
46
2
Spectral Analysis of Deterministic Process

^ck ¼
X
1
r¼1
crK1ðxk  xrÞ
ð2:55Þ
K1ðxÞ ¼ 1
N
X
N1
j¼0
eixjDt ¼ sinðNxDt=2Þ
N sinðxDt=2Þ eiðN1ÞxDt=2
ð2:56Þ
See the end for proof. In terms of the dimensionless variable u ¼ xDt=2p,
K1ðxÞ ¼ DNðuÞeipðN1Þu
u ¼ xDt
2p
ð2:57Þ
where
DNðuÞ ¼ sin Npu
N sin pu
ð2:58Þ
is known as the ‘Dirichlet kernel’, which plays an important role in Fourier theory.
Equation (2.55) indicates that ^ck contains contributions from all FS frequencies.
The sum need not even contain a term at the subject frequency xk. The contribution
from frequency xr is not directly cr, but is ‘attenuated’ by K1ðxk  xrÞ, which
depends on how far xr is from xk.
Figure 2.4 shows a schematic plot of jDNðuÞj. It has a period of 1, with a
symmetric basic branch on ð1=2; 1=2Þ. In this branch it has a global maximum of
1 at u ¼ 0; and a series of zeros at u ¼ 1=N; 2=N, …, up to 1=2. For u ranging
between 1=2, x ¼ 2pu=Dt ranges between p=Dt (rad/s), i.e., ±Nyquist fre-
quency. Due to convolution effect, ^ck comprises the harmonics in xðtÞ at frequencies
(1) near the subject frequency fk ¼ k=NDt Hz;
(2) in a Oð1=NDtÞ (Hz) neighborhood around fk (leakage); and
(3) in the Oð1=NDtÞ (Hz) neighborhoods around fs  fk, 2fs  fk, … (aliased
counterparts of leakage), where fs ¼ 1=Dt Hz is the sampling frequency.
u
0
N
1
N
2
N
3 L
2
1
2
1
−
Basic branch
… periodic …
… periodic …
−1
1
1
Fig. 2.4 Schematic plot of jDNðuÞj, the modulus of the Dirichlet kernel
2.4
Distortions in Fourier Series
47

Example 2.2 (Leakage from a single harmonic) Revisit Example 2.1. Everything
elsebeingthe same,the samplingdurationisnow slightlyextended toNDt ¼ 1.2 s, i.e.,
with N ¼ 1:2=0:1 ¼ 12 points (Dt ¼ 0:1 s). Results analogous to Fig. 2.3 are shown
in Fig. 2.5. For f ¼ 1, 4, 6 and 11 Hz, the number of cycles within the data duration is
fNDt ¼ 1.2, 4.8, 7.2 and 13.2. None of these are integer and so leakage occurs, in
addition to aliasing (when f ¼ 6 and 11 Hz). The FFT frequencies are now fk ¼
k=NDt ¼ 0, 0.833, 1.667, …, 9.167 Hz, instead of 0, 1, …, 9 Hz in Fig. 2.3.
■
Proof of (2.55) (Leakage, Fourier Series)
Substituting the FS xj ¼ P1
r¼1 creixrjDt into ^ck in (2.46),
^ck ¼ 1
N
X
N1
j¼0
X
1
r¼1
creixrjDt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
xj
eixkjDt ¼
X
1
r¼1
cr
1
N
X
N1
j¼0
eiðxkxrÞjDt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K1ðxk  xrÞ
ð2:59Þ
where
K1ðxÞ ¼ 1
N
X
N1
j¼0
eixjDt
ð2:60Þ
as deﬁned in (2.56). Using PN1
j¼0 a j ¼ ð1  aNÞ=ð1  aÞ with a ¼ eixDt,
K1ðxÞ ¼ 1  eiNxDt
Nð1  eixDtÞ
ð2:61Þ
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1 1.1 1.2
-2
0
2
f = 1 Hz
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1 1.1 1.2
-2
0
2
f = 4 Hz
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1 1.1 1.2
-2
0
2
f = 6 Hz
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1 1.1 1.2
-2
0
2
f = 11 Hz
t (sec)
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
Freq. fk = k/NΔt (Hz)
Fig. 2.5 FS amplitude of xðtÞ ¼ 2 cos 2pft estimated via the FFT of fxj ¼ xðjDtÞgN1
j¼0 with Dt ¼
0:1 s and N ¼ 12, for f ¼ 1; 4; 6 and 11 Hz. Same legend as in Fig. 2.3
48
2
Spectral Analysis of Deterministic Process

Note that for any real h,
1  eih ¼ eih=2 ðeih=2  eih=2Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
2i sinðh=2Þ
¼ 2ieih=2 sinðh=2Þ
ð2:62Þ
Using this identity,
K1ðxÞ ¼ 2ieiNxDt=2 sinðNxDt=2Þ
2ieixDt=2N sinðxDt=2Þ ¼ sinðNxDt=2Þ
N sinðxDt=2Þ eiðN1ÞxDt=2
ð2:63Þ
which is the rightmost expression in (2.56).
■
2.5
Distortions in Fourier Transform
Nyquist frequency limit, aliasing and leakage in the FFT approximation of FT
occurs by a similar mechanism as in FS. Analogous to (2.55), the DTFT approx-
imation ^XðxÞ in (2.35) is related to the target FT XðxÞ by a convolution integral:
^XðxÞ ¼ 1
2p
Z1
1
Xðx0ÞK2ðx  x0Þdx0
ð2:64Þ
K2ðxÞ ¼ NDtK1ðxÞ ¼ NDtDNðuÞeipðN1Þu
u ¼ xDt
2p
ð2:65Þ
See the end for proof.
Aliasing and leakage can be explained based on (2.64). To see aliasing, note that
jDNðuÞj has local maxima 1 at u ¼ 0; 1; 2; . . .; see Fig. 2.4. Correspondingly,
jK2ðx  x0Þj as a function of x0 has local maxima at x0 ¼ x  2pr=Dt for
r ¼ 0; 1; 2; . . ..
Using
the
conjugate
mirror
property
of
FT,
Xðx  2pr=DtÞ ¼ Xð2pr=Dt  xÞ, and so ^XðxÞ receives signiﬁcant contributions
from the frequencies 2pr=Dt  x ðr ¼ 0; 1; 2; . . .Þ. This is aliasing.
To see leakage, note that ^XðxÞ at frequency x receives contribution from the FT
Xðx0Þ of the original process at frequency x0. The contribution is attenuated by
K2ðx  x0Þ, which depends on how far x0 is from x. For the contribution to be
non-zero, x0 need not be the subject frequency x or aliased counterparts. This is
leakage. Since a non-periodic process generally contains harmonics at a continuum
of frequencies, leakage exists regardless of the data duration. Nevertheless, the
effect diminishes as the data duration increases, because jK2ðx  x0Þj is negligible
for jx  x0j 	 2p=NDt.
2.4
Distortions in Fourier Series
49

Proof of (2.64) (Distortion, Fourier Transform)
Substituting the inverse FT xj ¼ ð2pÞ1 R 1
1 Xðx0Þeix0jDtdx0 into ^XðxÞ in (2.35),
^XðxÞ ¼
X
N1
j¼0
1
2p
Z1
1
Xðx0Þeix0jDtdx0
2
4
3
5
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
xj
eixjDtDt
¼ 1
2p
Z1
1
Xðx0Þ NDt  1
N
X
N1
j¼0
eiðxx0ÞjDt
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
K1ðx  x0Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K2ðx  x0Þ
dx0
ð2:66Þ
which gives (2.64).
■
2.6
Summary of FFT Approximations
Table 2.2 summarizes the Fourier formulas and their FFT approximations. They are
generally related by a convolution (sum or integral). Relevant sections are indi-
cated. The last row for power spectral density applies to a stationary stochastic
process, which is discussed in Chap. 4.
2.7
Summary of Fourier Formulas, Units
and Conventions
Table 2.3 summarizes the Fourier formulas and their Parseval equalities. Unit
matters, and is indicated. The table assumes that xðtÞ has a unit of volt (V) and time
is measured in second (s). Relevant sections are indicated.
2.7.1
Multiplier in Fourier Transform
One common confusion in Fourier analysis is the deﬁnition of the inverse FT, in
particular, the multiplier 1=2p in xðtÞ ¼ ð2pÞ1 R 1
1 XðxÞeixtdx. Different authors
may use a different multiplier. It may appear arbitrary but in fact has a direct
implication on the unit of the FT XðxÞ. As seen in Table 2.3, if xðtÞ has unit V,
50
2
Spectral Analysis of Deterministic Process

Table 2.2 Summary of FFT approximations to Fourier series, Fourier Transform and power spectral density; yk ¼ PN1
j¼0 xje2pijk=N is the FFT of
fxj ¼ xðjDtÞgN1
j¼0 ; DNðuÞ ¼ sinðNpuÞ=N sin pu is the Dirichlet kernel
Process
Theoretical
FFT approximation at
xk ¼ 2pk=NDt (rad/s)
Relationship between theory and
approximation
Time domain
Frequency domain
Periodic
(period T)
xðtÞ ¼
X1
k¼1 ckeixkt
xk ¼ 2pk=T rad=s
ð
Þ
(Sect. 2.1.1)
ck ¼ 1
T
R T=2
T=2 xðtÞeixktdt
(Sect. 2.1.1)
^ck ¼ 1
N yk (Sect. 2.3.2)
^ck ¼
X1
r¼1 crK1ðxk  xrÞ
K1ðxÞ ¼ DNðuÞeipðN1Þu; u ¼ xDt=2p
(Sect. 2.4.3)
Non-periodic
xðtÞ ¼ 1
2p
R 1
1 XðxÞeixtdx
(Sect. 2.2)
XðxÞ ¼
R 1
1 xðtÞeixtdt
(Sect. 2.2)
^Xk ¼ Dtyk (Sect. 2.3.2)
^Xk ¼ 1
2p
Z 1
1
XðxÞK2ðxk  xÞdx
K2ðxÞ ¼ NDtDNðuÞeipðN1Þu; u ¼ xDt=2p
(Sect. 2.5)
Stochastic
stationary
xðtÞ ¼ vector process
XTðxÞ ¼
1ﬃﬃﬃ
T
p R T
0 xðtÞeixtdt
^Xk ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Dt=N
p
yk
fykgN1
k¼0 is the FFT of fxðjDtÞgN1
j¼0
RðsÞ ¼ E½xðt þ sÞxðtÞT
(Sect. 4.1)
SðxÞ ¼ lim
T!1 E½XTðxÞXTðxÞ
(Sect. 4.2)
^Sk ¼ ^Xk ^X

k (Sect. 4.5.2)
E½^Sk
 ¼ 1
2p
Z 1
1
SðxÞFNðxk  xÞdx
FNðxÞ ¼ NDtD2
NðuÞ; u ¼ xDt=2p
(Sect. 4.7.2)
RðsÞ ¼ 1
2p
R 1
1 SðxÞeixsdx
(Sect. 4.4.3)
SðxÞ ¼ R 1
1 RðsÞeixsds
(Sect. 4.4.3)
2.7
Summary of Fourier Formulas, Units and Conventions
51

Table 2.3 Summary of Fourier formulas and units
Quantity
Time domain
Frequency domain
Parseval equality
Fourier series
(period T)
xðtÞ
|{z}
V
¼ P1
k¼1 ck
|{z}
V
eixkt
ck
|{z}
V
¼
1
T
|{z}
1=s
R T=2
T=2 xðtÞ
|{z}
V
eixkt dt
|{z}
s
R T=2
T=2 xðtÞ2
|ﬄ{zﬄ}
V2
dt
|{z}
s
¼
T
|{z}
s
P1
k¼1 jckj2
|{z}
V2
(Sect. 2.1.2)
Fourier Transform
xðtÞ
|{z}
V
¼
1
2p
|{z}
1=rad
R 1
1 XðxÞ
|ﬄ{zﬄ}
V=Hz
eixt dx
|{z}
rad=s
XðxÞ
|ﬄ{zﬄ}
V=Hz
¼
R 1
1 xðtÞ
|{z}
V
eixt dt
|{z}
s
R 1
1 xðtÞ2
|ﬄ{zﬄ}
V2
dt
|{z}
s
¼
1
2p
|{z}
1=rad
R 1
1 jXðxÞj2
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
V2=Hz2
dx
|{z}
rad=s
(Sect. 2.2.4)
Fast Fourier
Transform
xj
|{z}
V
¼ 1
N
PN1
k¼0
yk
|{z}
V
e2pijk=N
yk
|{z}
V
¼ PN1
j¼0
xj
|{z}
V
e2pijk=N
PN1
j¼0
x2
j
|{z}
V2
¼ 1
N
PN1
k¼0 jykj2
|{z}
V2
(Sect. 2.3.3)
52
2
Spectral Analysis of Deterministic Process

XðxÞ has unit V/Hz rather than V/(rad/s), despite the fact that the integral in
xðtÞ ¼ ð2pÞ1 R 1
1 XðxÞeixtdx is w.r.t. x (rad/s). The factor 1=2p (with unit 1/rad)
makes up for this; x=2p is the frequency in Hz. If one omits 2p and writes
xðtÞ ¼
R 1
1 XðxÞeixtdx, then XðxÞ must have a unit of V/(rad/s), and vice versa.
The same rule applies to the FFT approximation ^XðxÞ. This issue is not relevant to
the FS coefﬁcient ck because the multiplier eixkt in xðtÞ ¼ P1
k¼1 ckeixkt is
dimensionless.
2.8
Connecting Theory with Matlab
Matlab
provides
a
convenient
platform
for
signal
processing
and
scientiﬁc/engineering computing in general. This section presents the connection of
some theoretical results with the functions in Matlab. The focus is on FFT and
related functions. In the following, quantities in Matlab are in typewriter font.
In Matlab, the index of an array starts from 1. In this book, the index of a
discrete-time sequence starts from 0, which is found to simplify presentation. Thus,
fxjgN1
j¼0 in this book is an array x of length N in Matlab with
xð1Þ ¼ x0;
xð2Þ ¼ x1; . . .; xðNÞ ¼ xN1:
Let fykgN1
k¼0 be the FFT of fxjgN1
j¼0 . In Matlab, FFT can be performed by the
built-in function fft. The call y = fft(x) returns an array y of length N. By
deﬁnition,
yk ¼
X
N1
j¼0
xje2pijk=N
k ¼ 0; . . .; N  1
ð2:67Þ
yðkÞ ¼
X
N
j¼1
xðjÞ e2piðj1Þðk1Þ=N
k ¼ 1; :::; N
ð2:68Þ
Correspondingly,
yð1Þ ¼ y0
ðzero frequencyÞ
yð2Þ ¼ y1
ðfrequency f1 ¼ 1=NDt HzÞ
yð3Þ ¼ y2
ðfrequency f2 ¼ 2=NDt HzÞ
and so on, up to
yðNq þ 1Þ ¼ yNyq
2.7
Summary of Fourier Formulas, Units and Conventions
53

where Nq = ﬂoor(N/2) (integer part of N=2). The remaining entries in y are just
the conjugate mirror image of those below the Nyquist frequency.
Table 2.4 shows the connection of some theoretical properties with Matlab. The
symbol ‘’ denotes that the quantities on both sides are numerically the same when
evaluated in Matlab. Unless otherwise stated, x and y are assumed to be real N by 1
array. See Chap. 4 for the last three properties regarding correlation function and
power spectral density.
2.9
FFT Algorithm
To supplement Sect. 2.3.1, the FFT algorithm is brieﬂy introduced here. Originally,
evaluating the DTFT in (2.35) at N frequencies requires a computational effort of
the order of N2, i.e., OðN2Þ. Using the FFT algorithm, it is reduced to OðN log2 NÞ,
although the values are evaluated at a speciﬁc set of equally spaced frequencies.
The key lies in the discovery that, for N being some power of 2, i.e., N ¼ 2m for
some integer m, a FFT sequence of length N can be obtained from two FFT
sequences of length N=2; and similarly each FFT sequence of length N=2 can be
obtained from another two FFT sequences of length N=4; and so on. The general
algorithm has provisions for other cases of N, but here we conﬁne our discussion to
N ¼ 2m.
Table 2.4 Connection between theory and Matlab
Property
Matlab
First entry of FFT
y = fft(x);
y(1)  sum(x)
Conjugate mirror
y = fft(x);
y(2:end)  conj(y(end:-1:2))
Inverse FFT
ifft(fft(x))  x
Parseval equality
sum(abs(fft(x)).^2)  N*sum(x.^2)
Symmetry of convolution
conv(x,y)  conv(y,x)
Convolution theorem
fft(conv(x,y)) fft([x;zeros(N-1,1)]).*fft([y;zeros(N-1,1)])
Wiener-Khinchin theorem
s = fft([0;xcorr(x,y)]); % (2N,1) array
s(1:2:end-1)  fft(x).*conj(fft(y))
Asymmetry of xcorr
xcorr(x,y)  xcorr(y(end:-1:1),x(end:-1:1))
xcorr and conv
xcorr(x,y)  conv(x,y(end:-1:1))
54
2
Spectral Analysis of Deterministic Process

2.9.1
Basic Idea
Given fxjgN1
j¼0 , let fykgN1
k¼0 be the FFT sequence to be computed, i.e.,
yk ¼
X
N1
j¼0
xje2pijk=N
k ¼ 0; . . .; N  1
ð2:69Þ
Let
wN ¼ e2pi=N
ð2:70Þ
so that ðwNÞN ¼ 1. Then yk can be written in terms of wN:
yk ¼
X
N1
j¼0
xjwjk
N
ð2:71Þ
Separating the sum into even and odd terms of j,
yk ¼
x0w0k
N þ x2w2k
N þ    þ xN2wðN2Þk
N
ðj evenÞ
þ x1w1k
N þ x3w3k
N þ    þ xN1wðN1Þk
N
ðj oddÞ
¼
X
ðN=2Þ1
r¼0
x2rw2rk
N þ
X
ðN=2Þ1
r¼0
x2r þ 1wð2r þ 1Þk
N
ð2:72Þ
Since w2
N ¼ wN=2, we can write w2rk
N
¼ wrk
N=2 and wð2r þ 1Þk
N
¼ wrk
N=2wk
N. Then
yk ¼
X
ðN=2Þ1
r¼0
x2rwrk
N=2 þ wk
N
X
ðN=2Þ1
r¼0
x2r þ 1wrk
N=2
ð2:73Þ
On the other hand, suppose we separate fxjgN1
j¼0 into two sequences of length
N=2, one containing the even j terms and the other containing the odd j terms, i.e.,
fx2rgðN=2Þ1
r¼0
and fx2r þ 1gðN=2Þ1
r¼0
, respectively. Their FFTs are respectively given by
y0
k ¼
P
ðN=2Þ1
r¼0
x2re2pirk=ðN=2Þ ¼
P
ðN=2Þ1
r¼0
x2rwrk
N=2
y00
k ¼
P
ðN=2Þ1
r¼0
x2r þ 1e2pirk=ðN=2Þ ¼
P
ðN=2Þ1
r¼0
x2r þ 1wrk
N=2
k ¼ 0; . . .; N
2  1
ð2:74Þ
2.9
FFT Algorithm
55

Comparing (2.73) and (2.74), we see that
yk ¼ y0
k þ wk
Ny00
k
k ¼ 0; . . .; N
2  1
ð2:75Þ
The ﬁrst half of fykgN1
k¼0 can thus be obtained from fy0
kgðN=2Þ1
k¼0
and fy00
kgðN=2Þ1
k¼0
.
The remaining half can be produced using the conjugate mirror property of FFT,
i.e., yk ¼ yNk.
The above shows that the FFT of a length N sequence can be obtained from the
FFTs of two length N=2 sequences. The FFT of each length N=2 sequence can be
further obtained from the FFTs of another two length N=4 sequences, and so on.
Carrying this on recursively, eventually it involves the FFT of a length 1 sequence,
i.e., a number, which is the sequence itself.
2.9.2
Computational Effort
To assess the computational effort of the FFT algorithm, let CN denote the number
of multiplications to produce a FFT sequence of length N. The effort for additions,
subtractions and complex conjugate can be considered negligible. Also, assume that
fwk
NgN1
k¼0 has been computed upfront. Its computational effort is not counted in CN.
Based on (2.75),
CN
|{z}
FFT of length N
¼ 2
CN=2
|ﬄ{zﬄ}
FFT of length N=2
þ
N=2
|{z}
Multiply wk
N for N=2 times
ð2:76Þ
By sequential substitution or induction, and recalling N ¼ 2m, it can be shown that
CN ¼ C1N þ N
2 log2 N
ð2:77Þ
Since the FFT of a length 1 sequence is just the sequence itself, C1 ¼ 0.
Consequently,
CN ¼ N
2 log2 N
ð2:78Þ
The computational effort for the FFT of a length N sequence is therefore
OðN log2 NÞ instead of OðN2Þ.
Example 2.3 (FFT algorithm) Figure 2.6 illustrates the recursive breakdown of the
calculations for a FFT sequence of length N ¼ 23 ¼ 8. On the right, each brace
56
2
Spectral Analysis of Deterministic Process

contains a sequence. The FFT of the original sequence {0, 1, …, 7} is obtained
from the FFTs of two shorter sequences {0, 2, 4, 6} and {1, 3, 5, 7}. The same
applies to other sequences.
The left side of the ﬁgure counts the number of multiplications involved in
producing the longer sequences from the shorter ones. Starting from the bottom, no
multiplication is needed to obtain the FFTs of the sequences of length 1. To produce
the FFT of {0, 4} from the FFTs of {0} and {4}, it requires 1 multiplication because
the ﬁrst FFT entry ðk ¼ 0Þ involves a multiplication with wk
2; the second can be
produced as the complex conjugate of the ﬁrst. The same applies to other sequences
{2, 6}, {1, 5} and {3, 7}. The number of multiplications to produce the four
sequences of length 2 from 8 sequences of length 1 is therefore 4  1. Similarly, to
produce the FFT of {0, 2, 4, 6} from the FFTs of {0, 4} and {2, 6}, it involves 2
multiplications for the ﬁrst two FFT entries ðk ¼ 0; 1Þ; the other two entries ðk ¼
2; 3Þ do not involve any multiplication as they are produced from the complex
conjugate of the ﬁrst two. The number of multiplications to produce the FFT of two
sequences of length 4 from four sequences of length 2 is therefore 2  2. Finally, it
involves 1  4 multiplications to obtain the FFT of {0, 1, …, 7} from the FFTs of
{0, 2, 4, 6} and {1, 3, 5, 7}. The total number of multiplications is 4  1 + 2 
2 + 1  4 = 12. This checks with (2.78), which gives C8 ¼ ð8=2Þ log2 8 ¼ 12. ■
References
Boyce WE, DiPrima RC (2005) Elementary differential equations and boundary value problems,
8th edn. John Wiley & Sons, New Jersey
Champeney DC (1987) A handbook of Fourier theorems. Cambridge University Press, Cambridge
Cooley JW, Tukey JW (1965) An algorithm for the machine calculation of complex Fourier series.
Math Comput 19(90):297–301
Lathi BP (2000) Signal processing and linear systems. Oxford University Press, UK
Smith SW (2002) Digital signal processing: a practical guide for engineers and scientists. Newnes,
USA. Downloadable from http://www.dspguide.com/
Sundararajan D (2001) The discrete Fourier transform: theory, algorithms and applications. World
Scientiﬁc Publishing Company, Singapore
{0}
{4}
{0, 4}
{2}
{6}
{2, 6}
{0, 2, 4, 6}
{1}
{5}
{1, 5}
{3}
{7}
{3, 7}
{1, 3, 5, 7}
{0, 1, 2, 3, 4, 5, 6, 7}
Mult. count
0
4 × 1
2 × 2
1 × 4
Sequence breakdown
Fig. 2.6 Illustration of FFT
algorithm for a sequence of
length N ¼ 8
2.9
FFT Algorithm
57

Chapter 3
Structural Dynamics and Modal Testing
Abstract This chapter analyzes the response of a structure subjected to deter-
ministic
excitations.
Conventional
topics
in
single-degree-of-freedom
and
multi-degree-of-freedom structures are covered, including free vibration, forced
vibration due to harmonic excitation, periodic excitation, impulsive excitation and
arbitrary excitation, and modal superposition. Basic concepts in experimental modal
testing are discussed as a simple application of structural dynamics, including
logarithmic decrement, half-power bandwidth, harmonic load test and impact
hammer test. The state-space approach is introduced for analyzing general
dynamical systems. The basic principles of numerical solution and Newmark
integration schemes are introduced, which allow one to compute the structural
response for a given time history of excitation.
Keywords Structural dynamics  Modal testing  Logarithmic decrement 
Half-power bandwidth  Newmark scheme  Impact hammer test
In this chapter we discuss the subject of ‘structural dynamics’, which studies the
response (e.g., displacement, velocity) of a structure obeying Newton’s law of
motion. The focus is on oscillatory behavior, commonly known as ‘vibration’.
‘Resonance’ plays an important role, where an excitation of oscillatory nature can
generate signiﬁcantly larger response if it varies at a pace near the ‘natural fre-
quency’ of the structure. Just as a musical instrument can produce sounds at dif-
ferent pitches, a structure can have more than one natural frequency. Associated
with each frequency there is a speciﬁc spatial vibration pattern, called ‘mode
shape’. Natural frequencies and mode shapes are determined by an ‘eigenvalue
equation’ that depends on the stiffness and mass of the structure. Together with
damping characteristics they completely determine the structural dynamic response
under applied loads. For textbooks on structural dynamics, see, e.g., Meirovitch
(1986), Clough and Penzien (1993) and Beards (1996).
We ﬁrst discuss the vibration response of a single-degree-freedom (SDOF)
structure, i.e., with only one variable in the equation of motion. Different types of
excitations are considered. Multi-degree-of-freedom (MDOF) structures are next
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_3
59

discussed. Their response can be obtained as the sum of contributions from different
modes, distinguished by mode shapes. Under conventional assumptions on
damping, the response contribution from each mode can be obtained via the
solution of a SDOF equation, and so the techniques and insights gained from SDOF
dynamics are generally useful. We also discuss ‘state-space approach’, which is a
powerful means for analyzing general dynamical systems. Time integration method
is then introduced. It allows one to compute structural dynamic response based on a
given discrete-time history of the excitation. This is an indispensable tool in
applications.
‘Modal testing’ concepts are introduced as an application of structural dynamics.
They illustrate the basic principles where the modal properties (i.e., natural fre-
quencies, damping ratios, etc.) of a structure can be estimated from conceptually
simple vibration tests. They also provide an interesting way of understanding
structural dynamics and building intuitions. The relevant sections are listed in
Table 3.1 for easy reference, skipping which will not affect understanding the main
theory. See also McConnell (1995), Maia and Silva (1997) and Ewins (2000).
3.1
SDOF Dynamics
Consider a mass m (kg) on a frictionless ﬂoor and connected with a spring of
stiffness k (N/m) to a ﬁxed wall, as shown in Fig. 3.1a. It is subjected to a force FðtÞ
(N) that can vary with time t (s). The position of the mass from its static equilibrium
position is measured by xðtÞ (m).
As shown in Fig. 3.1b, at a particular time t, the mass is at xðtÞ and it is subjected
to a restoring force kxðtÞ in the opposite direction, in addition to the applied force
FðtÞ: Applying Newton’s second law (mass  acceleration = net force) on the mass
gives m€xðtÞ ¼ kxðtÞ þ FðtÞ: This is conventionally written in the form
m€xðtÞ
|ﬄﬄ{zﬄﬄ}
inertia
þ kxðtÞ
|ﬄ{zﬄ}
stiffness
¼
FðtÞ
|{z}
applied force
ð3:1Þ
The stiffness and inertia terms refer to those proportional to displacement and
acceleration, respectively. This equation does not have ‘dissipative’ effect, which is
not realistic. According to the equation, if the mass is given an initial kick, it will
oscillate forever even if there is no applied force afterwards. In reality, it will
Table 3.1 Modal testing concepts introduced in this chapter
Dynamics
Concept
Section, example
Free vibration
Logarithmic decrement
Section 3.1.4
Forced vibration
Half-power bandwidth
Section 3.1.8, Example 3.2
Harmonic load
Section 3.4, Example 3.7
Impact hammer
Section 3.5
60
3
Structural Dynamics and Modal Testing

eventually stop, due to irreversible mechanisms (e.g., friction, forming of
micro-cracks) converting kinetic/strain energy to other forms (e.g., heat, sound).
This is collectively referred as ‘damping’. To match reality, a conventional way is
to introduce a damping term:
m€xðtÞ
|ﬄﬄ{zﬄﬄ}
inertia
þ c_xðtÞ
|ﬄ{zﬄ}
damping
þ kxðtÞ
|ﬄ{zﬄ}
stiffness
¼
FðtÞ
|{z}
applied force
ð3:2Þ
where c (N/(m/s)) is the ‘damping constant’. The damping term here is ‘viscous’ in
nature, as it is proportional to velocity. This need not be the case in reality, e.g.,
dissipation due to friction is not. Nevertheless, it is still the conventional assump-
tion in analysis, design and identiﬁcation of lightly damped structures. See Sect. 3.3
for further remarks.
3.1.1
Natural Frequency
For instructional purpose we ﬁrst consider ‘undamped free vibration’, where the
structure has no damping and the applied force is absent, i.e., c ¼ 0 and FðtÞ  0:
In this case the response is purely due to initial conditions. The equation of motion
in (3.2) reduces to
m€xðtÞ þ kxðtÞ ¼ 0
ð3:3Þ
Suppose the initial conditions are
xð0Þ ¼ u
_xð0Þ ¼ v
ð3:4Þ
(a)
(b)
Fig. 3.1 SDOF structure;
a schematic diagram; b free
body diagram at time t
3.1
SDOF Dynamics
61

Equation (3.3) is a second order ordinary differential equation (ODE). Its solution
has two integration constants, which can be determined by the two initial conditions
in (3.4). By inspection, it can be satisﬁed by a simple harmonic motion, e.g.,
xðtÞ ¼ a cos x1t, where x1 and a are constants to be determined. Substituting
xðtÞ ¼ a cos x1t and €xðtÞ ¼ ax2
1 cos x1t into (3.3) and collecting terms,
ðmx2
1 þ kÞa cos x1t ¼ 0
ð3:5Þ
This equation must hold for all t. Since a 6¼ 0 (otherwise xðtÞ  0) and cos x1t
cannot be zero for all t, the only possibility is mx2
1 þ k ¼ 0: Solving this equation
gives two roots for x1, i.e., 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
k=m
p
, but they lead to the same solution form for
xðtÞ: It is therefore sufﬁcient to consider only one of the roots. Taking the positive
root gives
x1 ¼
ﬃﬃﬃﬃ
k
m
r
Natural frequency in rad=s
ð
Þ
ð3:6Þ
This is called the ‘natural frequency’ of the structure. The unit of x1 is radian per
second, which can be reasoned by noting that the argument x1t in cos x1t has a unit
of radian and t has a unit of second. In vibration applications, it is more intuitive to
refer frequency in Hz (cycles per second), i.e.,
f1 ¼ x1
2p
Natural frequency in Hz
ð
Þ
ð3:7Þ
The ‘natural period’ is the time it takes to complete one cycle of oscillation:
T1 ¼ 1
f1
¼ 2p
x1
Natural period in sec
ð
Þ
ð3:8Þ
Repeating the above argument by substituting xðtÞ ¼ b sin x1t for some con-
stants x1 and b leads to the same conclusion that x1 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
k=m
p
. Since (3.3) is a
linear equation and there are two possible solution forms, the response is generally
of the form
xðtÞ ¼ a cos x1t þ b sin x1t
ð3:9Þ
Check that it satisﬁes (3.3) for arbitrary a and b. The constants a and b can be
determined from the two initial conditions in (3.4). According to (3.9), xð0Þ ¼ a
and _xð0Þ ¼ bx1. Enforcing the initial conditions in (3.4) gives a ¼ u and b ¼ v=x1.
As a result,
xðtÞ ¼ u cos x1t þ v
x1
sin x1t
undamped free vibration
ð
Þ
ð3:10Þ
62
3
Structural Dynamics and Modal Testing

Using the compound angle formula cosðh1  h2Þ ¼ cos h1 cos h2 þ sin h1 sin h2,
xðtÞ can be written in a form that shows explicitly the amplitude and phase:
xðtÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u2 þ v2=x2
1
q

u
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u2 þ v2=x2
1
p
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
cos /
cos x1t þ
v=x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u2 þ v2=x2
1
p
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
sin /
sin x1t

¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u2 þ v2=x2
1
q
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
amplitude
cosð x1
|{z}
frequency
t  /
|{z}
phase
Þ
ð3:11Þ
where tan / ¼ v=x1u:
3.1.2
Damping Ratio
Equation (3.11) indicates that undamped free vibration never dies down, because
the cosine term does not. This is not realistic. The general case of a SDOF structure
with damping, i.e., (3.2), will be considered in the remaining of this chapter. It is
mathematically more convenient to work with the following form
€xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ ¼ pðtÞ
ð3:12Þ
where we have divided (3.2) by m and replaced k=m by x2
1;
pðtÞ ¼ FðtÞ
m
ð3:13Þ
is the applied force per unit structural mass; and
f ¼
c
2
ﬃﬃﬃﬃﬃﬃ
mk
p
ð3:14Þ
is the (dimensionless) ‘damping ratio’. In most structural vibration applications, it is
f rather c that is referred to. Typically, 0\f\1 with f taking small values of a few
percent. Our discussion shall focus on the standard form (3.12) for different cases of
pðtÞ:
3.1.3
Damped Free Vibration
Consider again free vibration ðpðtÞ  0Þ but now with damping ðf [ 0Þ.
Equation (3.12) reduces to
3.1
SDOF Dynamics
63

€xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ ¼ 0
ð3:15Þ
As before let the initial conditions be
xð0Þ ¼ u
_xð0Þ ¼ v
ð3:16Þ
Consider solution of the form xðtÞ ¼ aeat for some constants a and a to be
determined. Then _xðtÞ ¼ aaeat and €xðtÞ ¼ a2aeat. Substituting into (3.15) and
collecting terms,
ða2 þ 2fx1a þ x2
1Þaeat ¼ 0
ð3:17Þ
This equation must hold for all t. Since a 6¼ 0 and it is not possible for eat to be zero
for all t, the only possibility is a2 þ 2fx1a þ x2
1 ¼ 0. Solving this equation for a
gives two roots
a1 ¼ fx1 þ x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
f2  1
q
a2 ¼ fx1  x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
f2  1
q
ð3:18Þ
Since there are two possibilities and the equation of motion is linear, the solution is
a linear combination of the two possibilities, i.e.,
xðtÞ ¼ a1ea1t þ a2ea2t
ð3:19Þ
The constants a1 and a2 can be determined from the initial conditions in (3.16),
which give two algebraic equations. The solutions of a1 and a2 can be
complex-valued, even though xðtÞ is always real-valued. Substituting the solution of
a1 and a2 into (3.19), the response can be written as
xðtÞ ¼ ug1ðtÞ þ vg2ðtÞ
ð3:20Þ
where g1ðtÞ and g2ðtÞ are given in Table 3.2 for different cases of f; see the end for
derivation. The under damped case ð0\f\1Þ is highlighted as it is typical in
applications. Clearly, xðtÞ is a linear function of u and v. Setting u ¼ 1 and v ¼ 0
shows that g1ðtÞ is the response with unit initial displacement and zero velocity.
Setting u ¼ 0 and v ¼ 1 shows that g2ðtÞ is the response with zero initial dis-
placement and unit velocity. For their nature, g1ðtÞ and g2ðtÞ are referred as the
‘complementary free vibration response’. These functions and hence the response
xðtÞ have characteristic behavior depending on whether 0  f\1 (oscillatory) or
f  1 (decay without rebound). In the former, the frequency of oscillation (ignoring
exponential decay effect) is xd ¼ x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
p
(rad/s), which is called the ‘damped
natural frequency’. In typical applications where f is small, xd 	 x1 and so the
64
3
Structural Dynamics and Modal Testing

damped natural frequency xd is seldom distinguished from the (undamped) natural
frequency x1.
The expressions in Table 3.2 match at the boundaries of f. The expressions for
f ¼ 0 agree with the limit of those for 0\f\1 as f ! 0. Similarly, taking limit
f ! 1 on the expressions for 0\f\1 or f [ 1 gives the ones for f ¼ 1.
Interestingly, the expressions for f [ 1 can be obtained from those for 0\f\1 by
replacing
xd
with
xD ¼ x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
f2  1
p
,
and
cosines/sines
with
hyperbolic
cosines/sines.
Table 3.2 Expressions for complementary free vibration responses g1ðtÞ and g2ðtÞ
3.1
SDOF Dynamics
65

Derivation of g1ðtÞ and g2ðtÞ in Table 3.2
We ﬁrst solve for a1 and a2 in (3.19) from the initial conditions in (3.16). From
(3.19), xðtÞ ¼ a1ea1t þ a2ea2t. Setting xð0Þ ¼ u and _xð0Þ ¼ v gives
a1 þ a2 ¼ u
a1a1 þ a2a2 ¼ v
ð3:21Þ
Solving for a1 and a2,
a1 ¼ a2u þ v
a1  a2
a2 ¼ a1u  v
a1  a2
ð3:22Þ
Substituting into xðtÞ ¼ a1ea1t þ a2ea2t and collecting terms of u and v gives
xðtÞ ¼ ug1ðtÞ þ vg2ðtÞ in (3.20) where
g1ðtÞ ¼ a1ea2t  a2ea1t
a1  a2
g2ðtÞ ¼ ea1t  ea2t
a1  a2
ð3:23Þ
Note that g1ðtÞ and g2ðtÞ are always real-valued even though a1 and a2 in (3.18) can
be complex-valued. To write them explicitly in terms of real quantities, we express
ea1t and ea2t in terms of cosines and sines when f\1; or hyperbolic cosines and
sines when f [ 1. Details are as follow.
Under Damped
When 0\f\1, a1 and a2 are complex. Using (3.18), writing
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
f2  1
p
¼ i
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
p
ði2 ¼ 1Þ and using the Euler formula eih ¼ cos h þ i sin h (for any real h),
ea1t
ea2t

¼ efx1t  eix1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
q
zﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄ{
xd
t
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
cos xdt  i sin xdt
¼ efx1tðcos xdt  i sin xdtÞ
ð3:24Þ
Substituting into (3.23) and simplifying gives g1ðtÞ and g2ðtÞ in Table 3.2 for
0\f\1.
Over Damped
When f [ 1, a1 and a2 are both real. The terms ea1t and ea2t can be expressed in
terms of hyperbolic cosines and sines by noting the following identities for any
real h:
cosh h ¼ eh þ eh
2
sinh h ¼ eh  eh
2
eh ¼ cosh h þ sinh h
ð3:25Þ
66
3
Structural Dynamics and Modal Testing

Using (3.18),
ea1t
ea2t

¼ efx1t 
e
x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
f2  1
q
t
zﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄ{
xD
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
cosh xDt  sinh xDt
¼ efx1tðcosh xDt  sinh xDtÞ
ð3:26Þ
Substituting into (3.23) gives g1ðtÞ and g2ðtÞ in Table 3.2 for f [ 1. The algebra
involved is similar to the case for 0\f\1, where the sine and cosine terms are
replaced by their hyperbolic counterparts.
Matching at Boundaries
The expressions for g1ðtÞ and g2ðtÞ for the boundary cases of f ¼ 0 and f ¼ 1 can
be derived from ﬁrst principle by substituting the value of f into (3.18) and then
obtaining the coefﬁcients a1 and a2. Alternatively, they can be obtained by taking
limits of f on the expressions in Table 3.2. The case for 0\f\1 as f ! 0 is
straightforward. For 0\f\1 as f ! 1, note that xd ¼ x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
p
! 0 as f ! 1.
Then
lim
f!1 g1ðtÞ ¼ lim
f!1 efx1t
|ﬄﬄ{zﬄﬄ}
! ex1t

cos xdt
|ﬄﬄﬄ{zﬄﬄﬄ}
! 1
þ fx1
|{z}
! x1
sin xdt
xd
|ﬄﬄﬄ{zﬄﬄﬄ}
! t

¼ ex1tð1 þ x1tÞ
ð3:27Þ
lim
f!1 g2ðtÞ ¼ lim
f!1 efx1t
|ﬄﬄ{zﬄﬄ}
! ex1t
sin xdt
xd
|ﬄﬄﬄ{zﬄﬄﬄ}
! t
¼ tex1t
ð3:28Þ
The case for f [ 1 as f ! 1 is similar.
■
3.1.4
Logarithmic Decrement Method
For lightly damped structures ðf 
 1Þ, the oscillatory nature of free vibration with a
decaying amplitude provides an intuitive way for estimating the natural frequency
and damping ratio. By combining the sine and cosine terms in g1ðtÞ and g2ðtÞ, one
can reason from (3.20) that the free vibration displacement starting from some
initial amplitude x0 can be written in the form
xðtÞ ¼
x0
|{z}
initial
amplitude
efx1t
|ﬄﬄ{zﬄﬄ}
exponential
decay
cos xdt
|ﬄﬄﬄ{zﬄﬄﬄ}
oscillatory
ð3:29Þ
This is illustrated in Fig. 3.2.
3.1
SDOF Dynamics
67

Since f 
 1, xd 	 x1. The natural period T1 ¼ 2p=x1 (sec) can be estimated by
‘cycle counting’ as the average time it takes to complete one cycle, i.e.,
Natural period; T1 	 Time for m cycles
m
cycle-counting
ð
Þ
ð3:30Þ
Estimating using multiple cycles helps reduce error from measurement noise. On
the other hand, the damping ratio is related to the rate of amplitude decay. By
noting that the cosine term in (3.29) is approximately equal to 1 at the peaks,
x0
xm
	
x0
x0efx1mT1 ¼ e2pmf
ð3:31Þ
since x1T1 ¼ 2p. Rearranging gives an estimation of the damping ratio:
f 	 lnðx0=xmÞ
2pm
log-decrement
ð
Þ
ð3:32Þ
This is known as the ‘logarithmic decrement method’. The same formula can be
applied to velocity or acceleration response data, as it can be reasoned that they can
also be written in the form of (3.29). In reality, vibration data contains measurement
noise and contribution from ambient excitations. Their spurious effects may be
reduced by averaging the data over repeated trials.
3.1.5
Harmonic Excitation
From now on we limit our scope to the under damped case ð0\f\1Þ, which is the
primary case of interest in structural vibrations. Suppose the excitation is ‘har-
monic’, say,
Fig. 3.2 Cycle counting and logarithmic decrement method (m = 3)
68
3
Structural Dynamics and Modal Testing

pðtÞ ¼ p0 cos xt
ð3:33Þ
where x (rad/s) is the ‘excitation frequency’ and p0 is the ‘excitation amplitude’
(per unit structural mass). As before, let the initial conditions be
xð0Þ ¼ u
_xð0Þ ¼ v
ð3:34Þ
The response to harmonic excitation comprises a ‘transient response’ and a
‘steady-state response’:
xðtÞ ¼ C1g1ðtÞ þ C2g2ðtÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
transient
!0 as t!1
þ
xsðtÞ
|ﬄ{zﬄ}
steadystate
ð3:35Þ
where C1 and C2 are constants (often not of concern) that can be determined from
initial conditions; g1ðtÞ and g2ðtÞ are the complementary free vibration responses in
Table 3.2.
Different Roles
In ODE theory, the transient response and steady-state response are respectively the
‘homogeneous solution’ and ‘particular solution’. They have different roles.
Substituting the transient response into the equation of motion (3.12) gives a zero
LHS. It is therefore the steady-state response that maintains a balance of dynamic
forces with the excitation. On the other hand, the steady-state response alone need
not satisfy initial conditions, i.e., generally xsð0Þ 6¼ u and _xsð0Þ 6¼ v. The transient
response makes this up with the right coefﬁcients C1 and C2 so that xðtÞ satisﬁes the
initial conditions. As time goes on ðt ! 1Þ the transient response diminishes,
leaving only the steady-state response, and hence the name.
Steady-State Response
The steady-state response only needs to satisfy the equation of motion. The func-
tional form of the excitation in (3.33) suggests the form
xsðtÞ ¼ a cos xt þ b sin xt
ð3:36Þ
for some constants a and b to be determined. Direct differentiation gives
_xsðtÞ ¼ ax sin xt þ bx cos xt
€xsðtÞ ¼ ax2 cos xt  bx2 sin xt
ð3:37Þ
Substituting into €xs þ 2fx1_xs þ x2
1xs ¼ p0 cos xt and collecting the cosine and sine
terms gives
½ðx2
1  x2Þa þ ð2fx1xÞb  p0 cos xt þ ½ð2fx1xÞa þ ðx2
1  x2Þb sin xt ¼ 0
ð3:38Þ
3.1
SDOF Dynamics
69

The bracketed terms must be zero because it is not possible for cos xt, sin xt or
their linear combination to be zero for all t, unless their coefﬁcients are zero. Setting
the bracketed terms to zero and solving for a and b gives
a ¼ p0
x2
1
1  b2
ð1  b2Þ2 þ ð2fbÞ2
"
#
b ¼ p0
x2
1
2fb
ð1  b2Þ2 þ ð2fbÞ2
"
#
ð3:39Þ
where
b ¼ x
x1
ð3:40Þ
is the ratio of the excitation frequency to the natural frequency.
Amplitude and Phase
Using the compound angle formula cosðh1  h2Þ ¼ cos h1 cos h2 þ sin h1 sin h2, the
steady-state response in (3.36) can be written explicitly in terms of amplitude and
phase:
xsðtÞ ¼ a cos xt þ b sin xt
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2 þ b2
p
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
Xs
ð
a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2 þ b2
p
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
cos /
cos xt þ
b
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2 þ b2
p
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
sin /
sin xtÞ
¼
Xs
|{z}
amplitude
cosð
x
|{z}
frequency
t  /
|{z}
phase
Þ
ð3:41Þ
where tan / ¼ b=a. Substituting (3.39) into Xs ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2 þ b2
p
and tan / ¼ b=a gives
Xs ¼
p0
x2
1
|{z}
static response

1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð1  b2Þ2 þ ð2fbÞ2
q
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
dynamic amplification
AðbÞ
tan / ¼
2fb
1  b2
ð0  /\pÞ
ð3:42Þ
The term p0=x2
1 is the static displacement if p0 were applied in a static manner. The
constraint 0  /\p arises from the consideration that sin / ¼ 2fbAðbÞ  0 and so
/ must lie in the ﬁrst or second quadrant, depending on whether cos / ¼
ð1  b2ÞAðbÞ is positive or negative, i.e., b\1 or b [ 1.
70
3
Structural Dynamics and Modal Testing

Sinusoidal Excitation
The steady-state response when the excitation is pðtÞ ¼ p0 sin xt has the same form
as (3.41):
xsðtÞ ¼ Xs sinðxt  /Þ
ð3:43Þ
where Xs and / are still given by (3.42). This can be reasoned by noting that
p0 sin xt ¼ p0 cosðxt  p=2Þ
and
so
has
the
steady-state
response
Xs cosðxt  p=2  /Þ, which is equal to Xs sinðxt  /Þ.
Transient Response
The constants C1 and C2 in the transient response in (3.35) are determined from
initial conditions. Setting xð0Þ ¼ u and _xð0Þ ¼ v, and noting g1ð0Þ ¼ 1, _g1ð0Þ ¼ 0,
g2ð0Þ ¼ 0 and _g2ð0Þ ¼ 1 gives
C1 þ xsð0Þ ¼ u
C2 þ _xsð0Þ ¼ v
)
C1 ¼ u  xsð0Þ
C2 ¼ v  _xsð0Þ
ð3:44Þ
Thus, C1 makes up for the difference in initial displacement between xðtÞ and
xsðtÞ; C2 makes up for the difference in initial velocity.
3.1.6
Simplifying Algebra with Complex Number
As in Fourier analysis (Sect. 2.1.1), complex number provides a convenient means
for obtaining the steady-state response. Suppose we want to determine the
steady-state response of xðtÞ and yðtÞ, driven respectively by cosine and sine
excitation:
€x þ 2fx1_x þ x2
1x ¼ p0 cos xt
€y þ 2fx1_y þ x2
1y ¼ p0 sin xt
ð3:45Þ
Instead of repeating the same procedure twice, we can determine the steady-state
response of
zðtÞ ¼ xðtÞ þ iyðtÞ
ð3:46Þ
After that, xðtÞ and yðtÞ can be recovered by taking the real and imaginary part of
zðtÞ, respectively. It turns out that zðtÞ can be obtained with simpler algebra, as seen
in the following.
Multiplying the second equation in (3.45) by i and adding with the ﬁrst, we
obtain the equation for zðtÞ:
3.1
SDOF Dynamics
71

ð€x þ i€yÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
€z
þ 2fx1 ð_x þ i_yÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
_z
þ x2
1 ðx þ iyÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
z
¼ p0 ðcos xt þ i sin xtÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
eixt
) €z þ 2fx1_z þ x2
1z ¼ p0eixt
ð3:47Þ
Assume steady-state solution of the form
zsðtÞ ¼ Zseixt
ð3:48Þ
for some constant Zs to be determined. Then _zs ¼ ixZseixt and €zs ¼ x2Zseixt.
Substituting into (3.47) and collecting terms,
ðx2
1  x2 þ 2fx1xiÞZseixt ¼ p0eixt
ð3:49Þ
This implies that
Zs ¼
p0
x2
1  x2 þ 2fx1xi ¼ p0
x2
1
|{z}
static

1
1  b2 þ 2fbi
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
dynamic
ð3:50Þ
where b ¼ x=x1 as deﬁned in (3.40). Substituting Zs into (3.48) and taking the
real/imaginary part gives the same steady-state response for cosine/sine excitations,
but now with simpler algebra.
3.1.7
Dynamic Ampliﬁcation
According to (3.42), the amplitude of the steady-state displacement to harmonic
excitation is equal to the static displacement p0=x2
1 multiplied by the ‘dynamic
ampliﬁcation factor’:
AðbÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð1  b2Þ2 þ ð2fbÞ2
q
b ¼ x
x1
ð3:51Þ
Figure 3.3 shows AðbÞ versus b for different values of f. It has a peak value of
Amax ¼
1
2f
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
p
at
b ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  2f2
q
ð3:52Þ
For small f 
 1, Amax  1=2f at b  1, i.e., x  x1. This is commonly referred as
‘resonance’, which is one key aspect of dynamics. For f ¼ 1%, Amax  50, i.e., a
72
3
Structural Dynamics and Modal Testing

harmonic load applied at resonance can generate 50 times the displacement than if it
is applied in a static manner.
As b ! 0, AðbÞ ! 1 regardless of the value of f. That is, when the excitation
frequency is low (i.e., slowly varying) compared to the natural frequency, there is
no ampliﬁcation; the response is ‘pseudo-static’. At the other extreme, AðbÞ  1=b2
as b ! 1. In this case the excitation is changing so fast that the structure cannot
‘catch up’. The faster the change, the smaller the response. Finally, AðbÞ decreases
monotonically with b for f  1=
ﬃﬃﬃ
2
p
, i.e., no resonance when the damping is too
high (yet still under damped).
Fig. 3.3 Dynamic ampliﬁcation factor AðbÞ
Fig. 3.4 SDOF response to sinusoidal excitations; b ¼ excitation frequency/natural frequency
3.1
SDOF Dynamics
73

Example 3.1 (Transient versus steady-state response) Consider a SDOF struc-
ture with natural frequency x1 ¼ 2p (rad/s), i.e., 1 Hz, and damping ratio f ¼ 2%.
Figure 3.4 shows the response xðtÞ to sinusoidal excitation pðtÞ ¼ sin xt for dif-
ferent frequencies indicated by the frequency ratio b ¼ x=x1. Generally, the
response is dominated by the steady-state component for time beyond 15 s (say).
For b ¼ 0:2, the excitation varies so slowly that the response is able to catch up
closely, except for the small transience in the beginning. At the other extreme, when
b ¼ 5 the response cannot catch up. The small fast varying ripples are due to the
excitation while the large slow variations are due to transience. At resonance
ðb ¼ 1Þ, the response amplitude grows towards the steady-state value.
■
3.1.8
Half-Power Bandwidth Method
The resonance nature of the dynamic ampliﬁcation factor provides an intuitive way
for estimating the natural frequency and damping ratio. Suppose we observe the
steady-state displacement amplitude of the structure when it is subjected to har-
monic excitations of different frequencies. If the excitation amplitude does not
change with frequency, a plot of the displacement amplitude with frequency will be
proportional to the dynamic ampliﬁcation factor. The shape of this plot depends on
the natural frequency and damping ratio, which allows one to estimate them.
For lightly damped structures ðf 
 1Þ, one conceptually simple way is to make
use of the ‘half-power frequencies’, which are the two points on either side of the
natural frequency such that the dynamic ampliﬁcation is equal to 1=
ﬃﬃﬃ
2
p
of the peak
value; it is 1=
ﬃﬃﬃ
2
p
instead of 1/2 because power is proportional to the square of
amplitude. Using (3.51) and (3.52), and solving AðbÞ ¼ Amax=
ﬃﬃﬃ
2
p
for b gives the
frequency ratios at the half-power points:
b ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  2f2  2f
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
q
r
ð3:53Þ
For small damping ratio f 
 1,
b  1  f
ð3:54Þ
That is, the half-power frequencies are located symmetrically on either side of the
natural frequency, differing by a fraction of f. These are illustrated in Fig. 3.5.
In implementation, the half-power frequencies can be located using the plot
of any quantity that is proportional to the dynamic ampliﬁcation factor. Based
on (3.54), the natural frequency can be estimated as the average of the
74
3
Structural Dynamics and Modal Testing

half-power frequencies. The ratio of the difference to the sum of the
half-power frequencies gives an estimate for the damping ratio. This is known
as the ‘half-power bandwidth method’.
Generalization
Equation (3.54) is derived based on the dynamic ampliﬁcation factor in (3.51),
which is associated with the displacement amplitude. When applying the
half-power bandwidth method with velocity or acceleration amplitudes, they should
be ﬁrst converted to displacement amplitudes. Dividing the velocity amplitude (for
example) by the excitation frequency (in rad/s) gives the displacement amplitude.
Modiﬁcation is also needed to remove the effect from any frequency-dependence of
the excitation amplitude. For example, if the excitation amplitude is proportional to
the square of frequency (e.g., in eccentric motors), the displacement amplitude
should be divided by the square of frequency.
Example 3.2 (Shaker test on ﬂoor slab) Consider a ﬂoor slab subjected to a
harmonic vertical force near the midspan generated by an electro-magnetic shaker,
as shown in Fig. 3.6a. The magnitude of vertical force exerted by the shaker on the
ﬂoor is equal to the product of the (moving) shaker mass and its acceleration. The
latter is measured by an accelerometer (circled by dashed line in the ﬁgure). The
vertical acceleration of the structure near the same location is measured by another
accelerometer.
With more than one DOF, the ﬂoor slab has more than one natural frequency.
The complete theoretical relationship between the acceleration amplitude of the
shaker mass and the structure requires multi-DOF dynamics concepts to obtain (see
Sect. 3.2 later). It can be reasoned that when the excitation frequency is near a
particular natural frequency the structural acceleration amplitude ðAxÞ is still
approximately related by the dynamic identiﬁcation factor to the shaker force
amplitude, and hence shaker mass acceleration amplitude ðApÞ. Thus, we still have,
for the dynamic ampliﬁcation factor in (3.51),
Fig. 3.5 Dynamic ampliﬁcation factor and half-power frequencies for small damping
3.1
SDOF Dynamics
75

A / Ax
Apf 2
ð3:55Þ
where f ¼ x=2p is the excitation frequency in Hz; the factor f 2 in the denominator
accounts for acceleration (rather than displacement) measurement. Note that A, Ax
and Ap depend on f but this has been omitted to simplify notation. The natural
frequency and damping ratio can be obtained by half-power bandwidth method
using a plot of the RHS of (3.55) for different values of f.
Table 3.3 shows the measured acceleration amplitudes of the shaker mass and
the structure for different excitation frequencies. The frequencies are not evenly
6
6.1
6.2
6.3
6.4
6.5
6.6
0
1
2
3
4
5
6 x 10
-5
max. = 
5.452e-05
max./√2 = 
3.855e-05
6.169
6.313
Excitation frequency f [Hz]
Ax / ( Ap f 2 ) [s2]
sensor
(shaker mass)
sensor
(structure)
shaker mass
(a)
(b)
Fig. 3.6 Floor slab subjected to harmonic vertical force by a shaker near the midspan. a Setup;
b dynamic ampliﬁcation and half-power frequencies
Table 3.3 Acceleration amplitudes of shaker mass and structure at different frequencies
No.
Excitation
frequency f (Hz)
Acc. amplitude
shaker mass Ap (milli-g)
Acc. amplitude
structure Ax (milli-g)
Ax=Apf 2
ð106 s2Þ
1
6.10
1171
1.146
26.30
2
6.15
1170
1.458
32.94
3
6.19
1170
2.016
44.96
4
6.21
1170
2.151
47.67
5
6.23
1170
2.409
53.03
6
6.24
1171
2.439
53.50
7
6.25
1174
2.500
54.52
8
6.26
1173
2.455
53.39
9
6.27
1173
2.347
50.91
10
6.29
1172
2.029
43.75
11
6.31
1171
1.825
39.14
12
6.34
1170
1.459
31.03
13
6.40
1169
0.974
20.35
14
6.50
1167
0.563
11.42
76
3
Structural Dynamics and Modal Testing

spaced as they were adjusted manually on a signal generator during the test to
populate around the natural frequency. Plotting the last column versus the excitation
frequency gives Fig. 3.6b. Dividing the peak value (5.452e−5) by
ﬃﬃﬃ
2
p
gives the
half-power value (3.855e−5), from which the half-power frequencies can be located
as 6.169 and 6.313 Hz. Using the half-power bandwidth method, the natural fre-
quency and damping ratio are estimated as
Natural frequency 	 6:313 þ 6:169
2
¼ 6:24 Hz
Damping ratio 	 6:313  6:169
6:313 þ 6:169 ¼ 1:15%
See Example 3.7 for the complete relationship between Ax and Ap, which also
allows one to estimate the effective (modal) mass of the structure associated with
the natural frequency. Modal testing with harmonic loads is discussed in more detail
in Sect. 3.4 later.
■
3.1.9
Principle of Superposition
To prepare for the study on periodic and arbitrary excitations in Sects. 3.1.10 to
3.1.12, we detour to discuss the ‘principle of superposition’. It generally holds for
linear elastic structures, or mathematically, linear equations, be it algebraic or
differential
in
nature.
An
equation
LðxÞ ¼ 0
is
‘linear’
if
Lðax þ byÞ ¼
aLðxÞ þ bLðyÞ for any scalars a and b. In our case, the equation of motion and initial
conditions are linear functions of x and its derivatives, and so the problem is linear.
The principle of superposition then holds, which says that the response to the sum
of multiple effects is simply the sum of the responses to the individual effects.
Speciﬁcally, let the response (with zero initial conditions) to excitation piðtÞ ði ¼
1; . . .; qÞ be xiðtÞ, satisfying
€xiðtÞ þ 2fx1_xiðtÞ þ x2
1xiðtÞ ¼ piðtÞ
xið0Þ ¼ 0
_xið0Þ ¼ 0
ð3:56Þ
Also, let x0ðtÞ be the free vibration response satisfying the initial conditions, i.e.,
€x0ðtÞ þ 2fx1_x0ðtÞ þ x2
1x0ðtÞ ¼ 0
x0ð0Þ ¼ u
_x0ð0Þ ¼ v
ð3:57Þ
When the structure is subjected to a linear combination of the excitations, say,
pðtÞ ¼ Pq
i¼1 cipiðtÞ for some constants fcigq
i¼1, and under initial conditions xð0Þ ¼
u and _xð0Þ ¼ v, the response is simply the same linear combination of the individual
responses, i.e.,
3.1
SDOF Dynamics
77

€xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ ¼ P
q
i¼1
cipiðtÞ
xð0Þ ¼ u; _xð0Þ ¼ v
9
=
; ) xðtÞ ¼ x0ðtÞ þ
X
q
i¼1
cixiðtÞ
ð3:58Þ
This can be shown by multiplying (3.56) with ci, summing over i ¼ 1; . . .; q, and
then adding with (3.57):
€x0 þ 2fx1_x0 þ x2
1x0


þ
X
q
i¼1
ci €xi þ 2fx1_xi þ x2
1xi


¼
X
q
i¼1
cipi
)
€x0 þ
X
q
i¼1
ci€xi
 
!
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
€x
þ 2fx1
_x0 þ
X
q
i¼1
ci_xi
 
!
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
_x
þ x2
1
x0 þ
X
q
i¼1
cixi
 
!
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
x
¼
X
q
i¼1
cipi
 
!
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
p
) €x þ 2fx1_x þ x2
1x ¼ p
ð3:59Þ
That is, xðtÞ satisﬁes the equation of motion. It also satisﬁes the initial conditions
because
xð0Þ ¼ x0ð0Þ
|ﬄ{zﬄ}
u
þ
X
q
i¼1
ci xið0Þ
|ﬄ{zﬄ}
0
¼ u
_xð0Þ ¼ _x0ð0Þ
|ﬄ{zﬄ}
v
þ
X
q
i¼1
ci _xið0Þ
|ﬄ{zﬄ}
0
¼ v
ð3:60Þ
The principle of superposition allows one to assess the response due to a
potentially complicated excitation by looking at the effects of its constituents. For
example, the response due to a periodic loading (Sect. 3.1.10) can be obtained by
simply summing the responses due to the individual harmonics; the response due to
an arbitrary excitation (Sect. 3.1.12) can be obtained as the sum of contributions
from individual impulses at different time instants in the past.
3.1.10
Periodic Excitation
When the excitation pðtÞ is periodic, it can be written as a Fourier series (FS,
Sect. 2.1):
pðtÞ ¼ a0 þ
X
1
r¼1
ar cos xrt þ
X
1
r¼1
br sin xrt
xr ¼ 2pr
T
ð3:61Þ
78
3
Structural Dynamics and Modal Testing

where T (s) is the period, i.e., the smallest value such that pðt þ TÞ ¼ pðtÞ for any t;
xr ¼ 2pr=T (rad/s) is the FS frequency of the rth harmonic;
a0 ¼ 1
T
ZT
0
pðtÞdt
ar ¼ 2
T
ZT
0
pðtÞ cos xrtdt
br ¼ 2
T
ZT
0
pðtÞ sin xrtdt
r  1
ð3:62Þ
are the real FS coefﬁcients. Applying the principle of superposition, the response
xðtÞ is the sum of those due to different harmonics and initial conditions. The
response due to each harmonic comprises a transient and a steady-state component.
Combining the transient components, one can write
xðtÞ ¼ efxdtðC1 cos xdt þ C2 sin xdtÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
transient
!0 as t!1
þ
xsðtÞ
|ﬄ{zﬄ}
steadystate
ð3:63Þ
where C1 and C2 are constants (often not of concern) which can be determined from
initial conditions; and (see (3.41))
xsðtÞ ¼ a0
x2
1
|{z}
static
þ
X
1
r¼1
ar
x2
1
|{z}
static
AðbrÞ
|ﬄﬄ{zﬄﬄ}
dyn:
amp:
cosðxrt  /rÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
lagged-time
harmonic
þ
X
1
r¼1
br
x2
1
|{z}
static
AðbrÞ
|ﬄﬄ{zﬄﬄ}
dyn:
amp:
sinðxrt  /rÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
lagged-time
harmonic
ð3:64Þ
AðbrÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð1  b2
rÞ2 þ ð2fbrÞ2
q
br ¼ xr
x1
tan /r ¼ 2fbr
1  b2
r
ð3:65Þ
Alternatively, the FS of excitation can be written in complex exponential form:
pðtÞ ¼
X
1
r¼1
creixrt
ð3:66Þ
cr ¼ 1
T
ZT
0
pðtÞeixrtdt
ð3:67Þ
The steady-state response is then given by (see (3.48))
xsðtÞ ¼
X
1
r¼1
cr
x2
1
|{z}
static
1
1  b2
r þ 2fbri
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
dynamic
eixrt
|{z}
excitation
harmonic
br ¼ xr
x1
ð3:68Þ
3.1
SDOF Dynamics
79

Although the individual terms in the sum of (3.66) are generally complex-valued,
pðtÞ is still real-valued because the rth and −rth term are complex conjugate of each
other. The same is also true for xsðtÞ in (3.68).
3.1.11
Ideal Impulse Excitation
An ‘impulsive excitation’ generally refers to one that lasts for a very short time
compared to the natural period of the structure. An ‘ideal impulse’ is a mathematical
idealization whose duration is arbitrarily small. Consider a unit rectangular impulse,
equal to 1=e for a duration of e. As e ! 0, the height of the impulse grows to inﬁnity
but the area under the impulse is still e  1=e ¼ 1 (hence the name ‘unit’). The ideal
unit impulse can be considered as the limit of the unit rectangular impulse as e ! 0,
although this representation is not unique. It does not exist in reality but it can
provide a good convenient approximation if the actual impulse duration is very short
compared to the natural period of the structure.
Consider a SDOF structure initially at rest and subjected to an ideal impulse at
t ¼ 0 þ :
€xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ ¼ dðt  0 þ Þ
xð0Þ ¼ _xð0Þ ¼ 0
ð3:69Þ
Here, 0 þ denotes symbolically the time ‘slightly’ (in a limiting sense) to the right
of the origin t ¼ 0; dðt  0 þ Þ denotes the Dirac Delta function (Sect. 2.2.3) slightly
shifted so that
R e
0 dðt  0 þ Þdt ¼ 1 (instead of 1/2) for any e [ 0.
The response to the ideal unit impulse with zero initial conditions ðxð0Þ ¼
_xð0Þ ¼ 0Þ is simply
xðtÞ ¼ g2ðtÞ ¼ efx1t
xd
sin xdt
xd ¼ x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
q
ð3:70Þ
where g2ðtÞ is the free vibration response with zero initial displacement and unit
initial velocity, as given in Table 3.2. This result can be reasoned as follow.
Integrating (3.69) from 0 to e [ 0,
_xðeÞ þ 2fx1xðeÞ þ x2
1
Ze
0
xðtÞdt ¼
Ze
0
dðt  0 þ Þdt ¼ 1
ð3:71Þ
As e ! 0, xðeÞ ! xð0Þ ¼ 0 and so is
R e
0 xðtÞdt ! 0. The equation then implies
_xð0 þ Þ ¼ lime!0 _xðeÞ ¼ 1. Since the excitation is zero after the impulse, the
response thereafter is a free vibration with zero initial displacement and unit initial
velocity, i.e., g2ðtÞ.
80
3
Structural Dynamics and Modal Testing

3.1.12
Arbitrary Excitation
For a general excitation pðtÞ, the response with zero initial conditions is given by
xðtÞ ¼
Zt
0
g2ðt  sÞpðsÞds
ð3:72Þ
This is a ‘convolution integral’, often called the “Duhamel’s integral” in structural
dynamics. It can be reasoned as the sum of responses due to the impulses at
different time instants up to t, as illustrated in Fig. 3.7.
Clearly xðtÞ depends on the excitation from time 0 up to t. Consider approxi-
mating the excitation on ½0; tÞ by a sequence of rectangular impulses of duration Ds
at time instants s1 ¼ 0, s2 ¼ Ds, s3 ¼ 2Ds; . . .; up to time t. The impulse at time s
has area pðsÞDs. At time t, which is t  s after the impulse, the induced response is
approximately g2ðt  sÞ  pðsÞDs. Summing the contributions from the impulses
occurring from time 0 to t gives
xðtÞ 	
X
0  s\t
g2ðt  sÞpðsÞDs
ð3:73Þ
As Ds ! 0, the sum becomes an integral and the approximation becomes exact:
xðtÞ ¼ lim
Ds!0
X
0  s\t
g2ðt  sÞpðsÞDs ¼
Zt
0
g2ðt  sÞpðsÞds
ð3:74Þ
Fig. 3.7 Illustration of Duhamel’s integral
3.1
SDOF Dynamics
81

In the general case when the structure starts from non-zero initial conditions, the
response is given by
xðtÞ ¼ ug1ðtÞ þ vg2ðtÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
initial condition effect
þ
Zt
0
g2ðt  sÞpðsÞds
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
excitation effect
ðxð0Þ ¼ u; _xð0Þ ¼ vÞ
ð3:75Þ
Check that it satisﬁes xð0Þ ¼ u and _xð0Þ ¼ v. It also satisﬁes €x þ 2fx1_x þ x2
1x ¼
pðtÞ because ug1ðtÞ þ vg2ðtÞ gives a zero LHS and the Duhamel’s integral gives a
LHS equal to pðtÞ (see proof below).
Proof of (3.72) (Duhamel’s integral satisfying the equation of motion)
Here we show that xðtÞ ¼
R t
0 g2ðt  sÞpðsÞds in (3.72) satisﬁes €x þ 2fx1_x þ x2
1x ¼
pðtÞ. To obtain _x and €x, use the ‘Leibniz rule of differentiation’ under integral sign,
i.e., for functions wðs; tÞ, aðtÞ and bðtÞ,
d
dt
ZbðtÞ
aðtÞ
wðs; tÞds
2
64
3
75 ¼ wðbðtÞ; tÞ_bðtÞ  wðaðtÞ; tÞ_aðtÞ þ
ZbðtÞ
aðtÞ
@w
@t ds
ð3:76Þ
Applying this rule,
_xðtÞ ¼ g2ðt  tÞ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
0
pðtÞ þ
Zt
0
_g2ðt  sÞpðsÞds ¼
Zt
0
_g2ðt  sÞpðsÞds
€xðtÞ ¼ _g2ðt  tÞ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
1
pðtÞ þ
Zt
0
€g2ðt  sÞpðsÞds ¼ pðtÞ þ
Zt
0
€g2ðt  sÞpðsÞds
ð3:77Þ
Using these expressions,
€xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ
¼ pðtÞ þ
Zt
0
€g2ðt  sÞpðsÞds þ 2fx1
Z t
0
_g2ðt  sÞpðsÞds þ x2
1
Zt
0
g2ðt  sÞpðsÞds
¼ pðtÞ þ
Zt
0
€g2ðt  sÞ þ 2fx1 _g2ðt  sÞ þ x2
1g2ðt  sÞ
	

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
¼ 0 since g2 satisfies free vibration equation
pðsÞds ¼ pðtÞ
ð3:78Þ
■
82
3
Structural Dynamics and Modal Testing

3.1.13
Summary of SDOF Response
The SDOF response xðtÞ satisfying
€xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ ¼ pðtÞ
xð0Þ ¼ u; _xð0Þ ¼ v
ð3:79Þ
is summarized in Table 3.4. Relevant sections are indicated.
3.2
MDOF Dynamics
The dynamics of a structure is generally governed by multiple (possibly inﬁnitely
many) DOFs. For instructional purpose, consider the two-DOF structure in
Fig. 3.8a.
The DOFs here comprise the displacements x1ðtÞ and x2ðtÞ of the two masses.
Two equations of motion can be derived by applying Newton’s second law on each
mass. As shown in Fig. 3.8b, at a particular time t, the mass m1 is displaced to the
right by x1ðtÞ and m2 by x2ðtÞ. In addition to the applied force F1ðtÞ, m1 is subjected
to a spring force of k1x1ðtÞ to the left and another k2½x2ðtÞ  x1ðtÞ to the right.
Applying Newton’s second law to m1 gives
m1€x1ðtÞ ¼ k1x1ðtÞ þ k2½x2ðtÞ  x1ðtÞ þ F1ðtÞ
ð3:80Þ
For m2, in addition to F2ðtÞ, it is subjected to a spring force of k2½x2ðtÞ  x1ðtÞ to
the left. Newton’s second law gives
m2€x2ðtÞ ¼ k2½x2ðtÞ  x1ðtÞ þ F2ðtÞ
ð3:81Þ
Writing the unknowns on the LHS and applied forces on the RHS, (3.80) and (3.81)
can be written as
m1€x1ðtÞ
þ
ðk1 þ k2Þx1ðtÞ

k2x2ðtÞ
¼
F1ðtÞ
m2€x2ðtÞ

k2x1ðtÞ
þ
k2x2ðtÞ
¼
F2ðtÞ
ð3:82Þ
These can be further assembled in matrix form:
m1
0
0
m2


|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
M
mass
matrix
€x1ðtÞ
€x2ðtÞ


|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
€xðtÞ
acc:
vector
þ
k1 þ k2
k2
k2
k2


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K
stiffness
matrix
x1ðtÞ
x2ðtÞ


|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
xðtÞ
disp:
vector
¼
F1ðtÞ
F2ðtÞ


|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
FðtÞ
load
vector
ð3:83Þ
3.1
SDOF Dynamics
83

Table 3.4 Summary of SDOF response to different excitations ð0\f\1Þ
Excitation pðtÞ
Response xðtÞ
Free vibration (3.1.3) ðpðtÞ  0Þ
xðtÞ ¼ ug1ðtÞ þ vg2ðtÞ
Harmonic (3.1.5) p0 cos xt
C1g1ðtÞ þ C2g2ðtÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
transient!0 as t!1
þ p0
x2
1
AðbÞ cosðxt  /Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
steady-state xsðtÞ
C1 ¼ u  xsð0Þ; C2 ¼ v  _xsð0Þ
AðbÞ ¼ ½ð1  b2Þ2 þ ð2fbÞ21=2; b ¼ x=x1; tan / ¼ 2fb=ð1  b2Þ; 0  /\p
Periodic (real Fourier series) (3.1.10)
a0 þ
X
1
r¼1
ðar cos xrt þ br sin xrtÞ
xr ¼ 2pr=T; T ¼ period
C1g1ðtÞ þ C2g2ðtÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
transient !0 as t!1
þ a0
x2
1
þ
X
1
r¼1
AðbrÞ
x2
1
ar cosðxrt  /rÞ þ br sinðxrt  /rÞ
½

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
steady-state xsðtÞ
C1 ¼ u  xsð0Þ; C2 ¼ v  _xsð0Þ
AðbrÞ ¼ ½ð1  b2
rÞ2 þ ð2fbrÞ21=2; br ¼ xr=x1; tan /r ¼ 2fbr=ð1  b2
rÞ; 0  /r\p
Periodic (complex Fourier series) (3.1.10)
X
1
r¼1
creixrt
xr ¼ 2pr=T; T ¼ period
C1g1ðtÞ þ C2g2ðtÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
transient!0 as t!1
þ
X
1
r¼1
hðbrÞ
x2
1
creixrt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
steadystate xsðtÞ
C1 ¼ u  xsð0Þ; C2 ¼ v  _xsð0Þ
hðbrÞ ¼ ½1  b2
r þ 2fbri1; br ¼ xr=x1
Arbitrary (3.1.12)
ug1ðtÞ þ vg2ðtÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
initial condition effect
þ
Z t
0
g2ðt  sÞpðsÞds
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
excitation effect
x1 ¼ natural frequency (rad/s), f ¼ damping ratio, xd ¼ x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
p
¼ damped natural frequency
u ¼ initial displacement; v ¼ initial velocity; g1ðtÞ ¼ efx1tðcos xdt þ fx1
xd sin xdtÞ, g2ðtÞ ¼ efx1t
xd sin xdt
84
3
Structural Dynamics and Modal Testing

This is a second order vector ODE in xðtÞ ¼ ½x1ðtÞ; x2ðtÞT. It does not have any
damping. For general problems where there are n DOFs and damping is modeled by
a viscous term, the equation of motion reads
M
nn €xðtÞ
n1
þ C
nn _xðtÞ
n1
þ K
nn xðtÞ
n1
¼ FðtÞ
n1
ð3:84Þ
where C is called the ‘damping matrix’. Finite element method (Hughes 1987;
Bathe 1982) is a conventional tool for constructing M, K and F from physical
speciﬁcations and has been implemented in a user-friendly fashion in commercial
software. In structural vibration applications, the damping matrix C is seldom
constructed from ﬁrst principles. Rather, damping is accounted empirically through
‘modal damping ratios’, in which case it may not even be necessary to construct
C in order to determine the response; see Sect. 3.2.3 later.
Positive Deﬁniteness and Energy
Typically, M, C and K are real symmetric and positive deﬁnite. That is, for M, all
entries are real-valued, MT ¼ M (symmetric) and uTMu [ 0 for any non-zero
n  1 real vector u (positive deﬁnite). The same properties apply to C and
K. Positive deﬁniteness of M reﬂects that the kinetic energy _xTM_x=2 is always
positive for non-zero _x. Of C it reﬂects positive energy dissipation rate _xTC_x [ 0.
Of K it reﬂects that the structure is stable, where displacement always involves
positive strain energy xTKx=2.
The rate of change of energies from kinetics, dissipation and strain is in balance
with the rate of work done (i.e., power) by the external force. This can be estab-
lished by pre-multiplying (3.84) with _xT, and noting that _xTM€x ¼ dð_xTM_x=2Þ=dt
and _xTKx ¼ dðxTKx=2Þ=dt:
(a)
(b)
Fig. 3.8 Two-DOF structure;
a schematic diagram; b free
body diagram at time t
3.2
MDOF Dynamics
85

d
dt
1
2 _xTM_x


|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
kinetic energy
þ
_xTC_x
|ﬄﬄ{zﬄﬄ}
energy
dissipation
rate
þ d
dt xTKx


|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
strain
energy
¼
_xTF
|ﬄ{zﬄ}
external rate
of work done
i:e:; power
ð3:85Þ
where the dependence on t has been omitted for simplicity.
3.2.1
Natural Frequencies and Mode Shapes
We ﬁrst consider the undamped free vibration of a MDOF structure to illustrate the
basic characteristics, i.e., the existence of more than one natural frequency, each
associated with a spatial vibration pattern called ‘mode shape’. When there is no
damping (C = 0) and the applied force is absent ðFðtÞ  0Þ, (3.84) reduces to
M€xðtÞ þ KxðtÞ ¼ 0
ð3:86Þ
Assume solution of the form xðtÞ ¼ wgðtÞ where w ðn  1Þ is a constant vector
and gðtÞ is a scalar function of t. This assumes that spatial and temporal variations
are ‘separable’. It need not be true but we will see later what needs to be revised to
arrive at the correct solution. Substituting xðtÞ ¼ wgðtÞ and €xðtÞ ¼ w€gðtÞ into (3.86)
gives, after rearranging,
Kw
|{z}
n1
¼  €gðtÞ
gðtÞ
|ﬄﬄ{zﬄﬄ}
scalar
Mw
|{z}
n1
ð3:87Þ
This equation must hold for all t. Since Kw and Mw do not depend on t, €gðtÞ=gðtÞ
must be a constant. This constant must be negative, which can be seen by
pre-multiplying both sides of (3.87) with wT and noting that K and M are positive
deﬁnite:
wTKw
|ﬄﬄﬄ{zﬄﬄﬄ}
[0
¼  €gðtÞ
gðtÞ
|ﬄﬄ{zﬄﬄ}
must be [0
wTMw
|ﬄﬄﬄ{zﬄﬄﬄ}
[0
ð3:88Þ
Thus, let €gðtÞ=gðtÞ ¼ x2 where x is a real positive number to be determined; the
possibility of a negative value for x can be eliminated as it gives the same solution
as the one by the positive value. We now have two equations, one in the time
domain and the other in the spatial domain:
86
3
Structural Dynamics and Modal Testing

€gðtÞ þ x2gðtÞ ¼ 0
ðtimeÞ
ð3:89Þ
Kw ¼ x2Mw
ðspatialÞ
ð3:90Þ
The ﬁrst equation in gðtÞ describes the dynamics of a SDOF undamped free
vibration with natural frequency x (rad/s). The second corresponds to the ‘gener-
alized eigenvalue problem’, which looks for the ‘eigenvalue’ k ¼ x2 and ‘eigen-
vector’ w ð6¼ 0Þ so that the equation holds. The square root of the eigenvalue gives
the natural frequency. The eigenvector gives the ‘mode shape’. To supplement, the
‘standard eigenvalue problem’ corresponds to the case when M is the identity
matrix. See Sect. C.2.4 for more details.
A discussion of the eigenvalue problem is postponed until the next section. It can
be shown that there are n different possible solutions, fxi; wign
i¼1, and corre-
spondingly n ‘modal responses’ fgiðtÞgn
i¼1. The response xðtÞ is then given by
xðtÞ ¼
X
n
i¼1
wigiðtÞ
ð3:91Þ
where each giðtÞ satisﬁes (3.89) with x ¼ xi:
€giðtÞ þ x2
i giðtÞ ¼ 0
ð3:92Þ
The solution to this SDOF equation is obtained by applying (3.10):
giðtÞ ¼ gið0Þ cos xit þ _gið0Þ
xi
sin xit
ð3:93Þ
The initial conditions of fgiðtÞgn
i¼1 can be determined from those of xðtÞ. Let
gðtÞ
n1
¼
g1ðtÞ
..
.
gnðtÞ
2
64
3
75
W
nn ¼ ½w1; . . .; wn
ð3:94Þ
Then xðtÞ ¼ WgðtÞ and _xðtÞ ¼ W_gðtÞ. Evaluating at t ¼ 0 and inverting gives
g1ð0Þ
..
.
gnð0Þ
2
64
3
75 ¼ gð0Þ ¼ W1xð0Þ
_g1ð0Þ
..
.
_gnð0Þ
2
64
3
75 ¼ _gð0Þ ¼ W1 _xð0Þ
ð3:95Þ
3.2
MDOF Dynamics
87

3.2.2
Eigenvalue Problem
Here we detour to discuss the natural frequencies and mode shapes of a structure,
which are solutions to the eigenvalue problem in (3.90). This prepares us to obtain
the response for the general case with damping and applied force in Sect. 3.2.3
later. For the discussion here, we write the eigenvalue problem explicitly in terms of
the eigenvalue k ¼ x2:
Kw ¼ kMw
ð3:96Þ
To see the basic nature of the eigenvalue problem, if we take an arbitrary vector
w, the vector Kw need not be a scalar multiple of the vector Mw. The ‘eigenvector’
w is the special vector that makes this true. When this is true, the proportionality
constant is not arbitrary either. The special constant is the ‘eigenvalue’ k.
Characteristic Equation
As another way of looking at the eigenvalue equation, rewrite it as
ðK  kMÞw ¼ 0
ð3:97Þ
For non-trivial solution of w to exist, i.e., w 6¼ 0, the determinant of K  kM must
be zero. This gives the ‘characteristic equation’ for the eigenvalue k:
K  kM
j
j ¼ 0
ð3:98Þ
Evaluating the determinant gives a polynomial in k of order n, and so there are
n (generally complex) eigenvalues. Once an eigenvalue is found, it can be substi-
tuted into (3.97), from which the corresponding eigenvector w can be determined.
There are efﬁcient algorithms for solving the eigenvalue problem. Standard
functions are available in commercial software, e.g., eig in Matlab. These will not
be discussed here. Rather, we mention some important properties relevant to
structural dynamics.
Mode Shape Scaling
Eigenvectors are only determined up to a scaling factor, i.e., if w is an eigenvector
then so is aw for any scalar a 6¼ 0. This follows because Kw ¼ kMw implies
KðawÞ ¼ kMðawÞ. In computations, a scaling constraint is applied to uniquely
determine the eigenvector. One common way is to scale it to have unit ‘Euclidean
norm’, i.e.,
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
wTw
p
¼ 1, which is equivalent to having the sum of the squared
values of the vector to be equal to 1. Another way is to scale it to be 1 at a particular
DOF. Scaling affects the modal response giðtÞ but not the physical response xðtÞ;
nature does not care about what scaling we use. It is a good practice to indicate the
scaling applied when presenting results.
88
3
Structural Dynamics and Modal Testing

Repeated Eigenvalues
Eigenvalues need not be distinct. Two different eigenvectors can have the same
eigenvalue. For example, the natural frequencies of the translational modes along
two principal horizontal directions of a building are theoretically the same when the
two directions have the same stiffness and mass properties.
For a given eigenvalue, the number of times it is repeated is called ‘algebraic
multiplicity’. The number of ‘different’, or technically, linearly independent,
eigenvectors associated with the eigenvalue is called ‘geometric multiplicity’. It can
be reasoned that geometric  algebraic multiplicity. If M and K are arbitrary
matrices, then it is possible that geometric < algebraic multiplicity. For example, an
eigenvalue can be repeated three times but one can only ﬁnd two linearly inde-
pendent eigenvectors; all other eigenvectors with the same eigenvalue are linear
combination of these two eigenvectors.
For structural dynamics problems where M and K are both real symmetric, it can
be shown that geometric = algebraic multiplicity. This is an important property,
implying that there are always as many linearly independent mode shapes as the
number of DOFs, regardless of the presence of repeated eigenvalues. It is then
always possible to write the structural response at any particular time instant as a
linear combination of the mode shapes, although the coefﬁcients may change with
time. This fact will be used in (3.104) later to obtain the response in the general
case.
Mode Shape Orthogonality
As just mentioned, the real symmetric property of M and K guarantees n linearly
independent eigenvectors fwign
i¼1 with eigenvalues fkign
i¼1. Not only are fwign
i¼1
linearly independent, they are also ‘orthogonal’ w.r.t. K and M, i.e.,
wT
i Kwj ¼ 0
wT
i Mwj ¼ 0
i 6¼ j
ð3:99Þ
Recall W ¼ ½w1; . . .; wn ðn  nÞ. Then WTKW is a n  n diagonal matrix because
its ði; jÞ-entry is wT
i Kwj. The same is also true for WTMW. In this sense, K and
M are ‘diagonalizable’ by the eigenvectors.
Example 3.3 (Two-DOF structure) Here we illustrate how to obtain analytically
the natural frequencies and mode shapes of the two-DOF structure in Fig. 3.8. For
simplicity, assume k1 ¼ k2 ¼ k0 and m1 ¼ m2 ¼ m0. Then
K  kM ¼ k0
2
1
1
1


 km0 1
0
0
1


¼ m0
2k0  k
k0
k0
k0  k


3.2
MDOF Dynamics
89

where k0 ¼ k0=m0. Setting K  kM
j
j ¼ 0 gives
ð2k0  kÞðk0  kÞ  k2
0 ¼ 0
) k2  3k0k þ k2
0 ¼ 0
) k ¼ k0
3
2 
ﬃﬃﬃ
5
p
2
 
!
) k1 ¼ 0:382k0
k2 ¼ 2:618k0
) x1 ¼
ﬃﬃﬃﬃﬃ
k1
p
¼ 0:618
ﬃﬃﬃﬃﬃ
k0
p
x2 ¼
ﬃﬃﬃﬃﬃ
k2
p
¼ 1:618
ﬃﬃﬃﬃﬃ
k0
p
Mode Shapes
Let the two mode shapes be
w1 ¼
/11
/21


w2 ¼
/12
/22


Substituting k1 ¼ 0:382k0 into ðK  k1MÞw1 ¼ 0 gives
m0k0 1:618
1
1
0:618


/11
/21


¼
0
0


The ﬁrst row gives 1:618/11  /21 ¼ 0 and so /21 ¼ 1:618/11. The second row
gives /11 þ 0:618/21 ¼ 0. Dividing it by 0.618 gives the same conclusion that
/21 ¼ 1:618/11. We see that one equation is redundant. This agrees with the fact
that the determinant of the matrix on the LHS is zero. The mode shape can only be
determined up to a scaling constant. If we scale it to be 1 at DOF 1, i.e., /11 ¼ 1,
then
w1 ¼ ½ 1
1:618 T.
If
we
scale
it
to
have
unit
norm,
w1 ¼ 1
1:618
½
T=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
12 þ 1:6182
p
¼ 0:526
0:851
½
T. This mode corresponds to
two masses moving in the same direction.
The procedure for the second mode is similar. Substituting k2 ¼ 2:618k0 into
ðK  k2MÞw2 ¼ 0 gives
mk0 0:618
1
1
1:618


/12
/22


¼
0
0


The ﬁrst row gives /22 ¼ 0:618/12. The second row gives the same result. If we
scale the mode shape to be 1 at DOF 1, w2 ¼ ½ 1
0:618 T. If we scale it to have
unit norm, w2 ¼ 1
0:618
½
T=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
12 þ ð0:618Þ2
q
¼ 0:851
0:526
½
T. This
mode corresponds to two masses moving in opposite directions.
Mode Shape Orthogonality
Using the mode shapes scaled to be 1 at DOF 1,
90
3
Structural Dynamics and Modal Testing

wT
1Kw2 ¼
1
1:618

T
k0
2
1
1
1


1
0:618


¼ 7:6  105k0
wT
1Kw1 ¼
1
1:618

T
k0
2
1
1
1


1
1:618


¼ 1:38k0
wT
2Kw2 ¼
1
0:618

T
k0
2
1
1
1


1
0:618


¼ 3:62k0
Due to round off errors, the calculated value of wT
1Kw2 is not zero. However, it is
small compared to wT
1Kw1 and wT
2Kw2, which can be used as a criterion for
checking. Checks w.r.t. the mass matrix give similar conclusions, where
wT
1Mw2 ¼ 7:6  105m0, wT
1Mw1 ¼ 3:62m0 and wT
2Mw2 ¼ 1:38m0.
■
Example 3.4 (Shear building) In a ‘shear building’, the restoring force between
two adjacent ﬂoors is proportional to the difference of their displacements. This is
illustrated in Fig. 3.9. Applying Newton’s second law to the ﬂoor masses gives a
matrix equation of motion where the stiffness and mass matrices have exactly the
same form as those for the mass-spring system in Fig. 3.8.
As an example, for a six-storied shear building (n = 6), the stiffness matrix
K and mass matrix M are given by
)
(
1 t
x
)
(
2 t
x
)
(t
xn
M
)
(
1 t
F
)
(
2 t
F
)
(t
Fn
External forces
1
m
1
k
Lateral
stiffness
2
m
2
k
Lateral
stiffness
n
m
n
k
Lateral
stiffness
)
(
1
2
2
x
x
k
−
1
1x
k
1
m
)
(
1 t
F
(a)
(b)
Fig. 3.9 Shear building; a schematic diagram; b free body diagram of the ﬁrst ﬂoor mass
3.2
MDOF Dynamics
91

K ¼
k1 þ k2
k2
k2
k2 þ k3
k3
k3
k3 þ k4
k4
k4
k4 þ k5
k5
k5
k5 þ k6
k6
k6
k6
2
6666664
3
7777775
ð3:100Þ
M ¼
m1
m2
m3
m4
m5
m6
2
6666664
3
7777775
ð3:101Þ
In the present case, it is prohibitive to obtain the natural frequencies and mode
shapes analytically. Instead, they are calculated numerically, as is usually done in
applications. For uniform stiffness ki ¼ 200 kN/mm and ﬂoor mass mi ¼ 100 tons
ði ¼ 1; . . .; 6Þ, the results calculated using the Matlab function eig are shown in
Fig. 3.10.
■
3.2.3
Modal Superposition and Classical Damping
Section 3.2.1 shows that the undamped free vibration response of a structure with
n DOFs is simply the sum of contributions from n modes, where each modal
response satisﬁes its own SDOF equation of motion ‘uncoupled’ from others. This
effectively reduces the original n  1 vector ODE to n scalar ODEs, which are
much easier to analyze. In this section, we show that when damping and applied
forces are present, the response can still be written as the sum of modal
Mode 1
1.72 Hz
Mode 2
5.05 Hz
Mode 3
8.09 Hz
Mode 4
10.7 Hz
Mode 5
12.6 Hz
Mode 6
13.8 Hz
Fig. 3.10 Natural frequencies and mode shapes of a six-storied shear building
92
3
Structural Dynamics and Modal Testing

contributions. However, the equations governing the modal responses are generally
coupled, unless the damping matrix has a special algebraic structure, known as
‘classical damping’.
Consider the equation of motion (3.84) in the general case:
M€xðtÞ þ C_xðtÞ þ KxðtÞ ¼ FðtÞ
ð3:102Þ
Recall that the natural frequencies fxign
i¼1 and mode shapes fwign
i¼1 satisfy the
eigenvalue equation:
Kwi ¼ x2
i Mwi
ð3:103Þ
Since fwign
i¼1 are n linearly independent vectors, they ‘span’ the n-dimensional
space and hence form a ‘basis’. That is, any n  1 vector can always be written as a
linear combination of fwign
i¼1. This implies that at any given time t the response
xðtÞ can be written as
xðtÞ ¼
X
n
i¼1
wigiðtÞ
ð3:104Þ
for some fgiðtÞgn
i¼1 that depend on t. Substituting xðtÞ ¼ Pn
j¼1 wjgjðtÞ into (3.102)
and pre-multiplying by wT
i for a particular mode i,
X
n
j¼1
ðwT
i MwjÞ€gjðtÞ þ
X
n
j¼1
ðwT
i CwjÞ_gjðtÞ þ
X
n
j¼1
ðwT
i KwjÞgjðtÞ ¼ wT
i FðtÞ
ð3:105Þ
Using the orthogonality property in (3.99), only the j ¼ i term in the ﬁrst and last
sum on the LHS is non-zero, giving
ðwT
i MwiÞ€giðtÞ þ
X
n
j¼1
ðwT
i CwjÞ_gjðtÞ þ ðwT
i KwiÞgiðtÞ ¼ wT
i FðtÞ
i ¼ 1; . . .; n
ð3:106Þ
Classical Damping
Equation (3.106) contains not only gi (and its derivatives) but also other _gj s ðj 6¼ iÞ,
through the cross terms fwT
i Cwjgj6¼i. One conventional assumption to simplify
analysis is ‘classical damping’, i.e., C is diagonalizable by the mode shapes as well:
wT
i Cwj ¼ 0
i 6¼ j
ð3:107Þ
Equation (3.106) is then uncoupled from other _gj s ðj 6¼ iÞ. Dividing by wT
i Mwi and
noting wT
i Kwi=wT
i Mwi ¼ x2
i gives
3.2
MDOF Dynamics
93

€giðtÞ þ 2fixi _giðtÞ þ x2
i giðtÞ ¼ piðtÞ
ði ¼ 1; . . .; nÞ
ð3:108Þ
where fi and piðtÞ are respectively the ‘modal damping ratio’ and ‘modal force’ (per
unit modal mass) of the ith mode, deﬁned through
2fixi ¼ wT
i Cwi
wT
i Mwi
piðtÞ ¼ wT
i FðtÞ
wT
i Mwi
ð3:109Þ
so that (3.108) has the same form as the SDOF equation in (3.12). The initial
conditions of fgiðtÞgn
i¼1 can be determined from those of xðtÞ, as in (3.95).
To summarize, for a classically damped structure, the response xðtÞ with initial
displacement xð0Þ and velocity _xð0Þ can be obtained as follow.
Modal superposition
1. Determine the natural frequencies fxign
i¼1 and mode shapes fwign
i¼1 by
solving the eigenvalue problem Kw ¼ x2Mw.
2. Determine the initial conditions of modal response, gð0Þ ¼ W1xð0Þ and
_gð0Þ ¼ W1 _xð0Þ, where W ¼ ½w1; . . .; wn.
3. For i ¼ 1; . . .; n, determine the modal response giðtÞ by solving the SDOF
equation (3.108) with initial conditions gið0Þ and _gið0Þ.
4. Obtain xðtÞ ¼ Pn
i¼1 wigiðtÞ.
3.2.4
Rayleigh Quotient
Given the stiffness matrix K and mass matrix M, consider the following scalar
function of n  1 real vector u:
RðuÞ ¼ uTKu
uTMu
ð3:110Þ
This is called the ‘Rayleigh quotient’. It is invariant to scaling, i.e., RðauÞ ¼ RðuÞ
for any non-zero scalar a. It gives the eigenvalue ki of the ith mode when
u ¼ wi, i.e.,
RðwiÞ ¼ wT
i Kwi
wT
i Mwi
¼ ki
ð3:111Þ
94
3
Structural Dynamics and Modal Testing

This follows directly by pre-multiplying both sides of the eigenvalue equation
Kwi ¼ kiMwi by wT
i and rearranging. Clearly, when u is not an eigenvector, RðuÞ
is not equal to the eigenvalue. However, it turns out that RðuÞ can give a good
approximation of the eigenvalue if u does not differ signiﬁcantly from the eigen-
vector. For this reason, the Rayleigh quotient is often used as a quick means for
estimating the natural frequency based on a guess of the mode shape.
Second Order Accuracy
The approximation capability of the Rayleigh quotient stems from the fact that its
gradient is zero at the eigenvectors, i.e., the latter are ‘stationary points’:
rRðuÞju¼wi¼
@R
@u1
; . . .; @R
@un


u¼wi
¼ 0
i ¼ 1; . . .; n
ð3:112Þ
A ﬁrst order deviation of u from wi then leads to only a second order (rather than
ﬁrst order) difference of RðuÞ from ki. Denote the difference between u and wi by
ei ¼ u  wi and assume that it is small. By noting that RðwiÞ ¼ ki, the Taylor series
of RðuÞ about wi reads
RðuÞ ¼ ki þ
1
2 eT
i Hiei
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
2nd order small
þ higher order terms of ei
ð3:113Þ
where the ﬁrst order term vanishes because the gradient is zero; Hi is the Hessian
matrix of RðuÞ at u ¼ wi, which is a n  n matrix with the ðr; sÞ-entry equal to
@2R=@ur@us

u¼wi. It can be shown that
Hi ¼ 2ðwT
i MwiÞ1ðK  kiMÞ
ð3:114Þ
The gradient and Hessian of the Rayleigh quotient can be derived by repeated
application of chain rule and differentiation rule for linear and quadratic forms. See
Sects. C.4 and C.5.4 for details.
Lower and Upper Bounds
The Rayleigh quotient is bounded between the smallest and largest eigenvalues, i.e.,
k1  RðuÞ  kn
ð3:115Þ
Clearly, the lower and upper bounds are attainable at u ¼ w1 and u ¼ wn,
respectively. The Rayleigh quotient therefore has a global minimum at the eigen-
vector of the lowest mode and a global maximum at the highest mode. The lower
bound is particularly useful for estimating the natural frequency of the lowest mode,
which often contributes signiﬁcantly to the dynamic response of a structure. It
3.2
MDOF Dynamics
95

implies that when estimating the lowest mode natural frequency using the Rayleigh
quotient with different ‘trial mode shapes’ (i.e., not exact), the smallest estimate of
natural frequency should be taken. This rule only applies to the lowest mode,
however. Higher modes do not have this lower bound property.
Proof of (3.115) (Bounds on Rayleigh quotient)
For any arbitrary vector u, let it be written as u ¼ Pn
i¼1 aiwi for some coefﬁcients
faign
i¼1. This is always possible since fwign
i¼1 forms a basis in the n-dimensional
space. Then
uTKu ¼
X
n
i¼1
aiwi
 
!T
K
X
n
j¼1
ajwj
 
!
¼
X
n
i¼1
X
n
j¼1
aiaj
wT
i Kwj
|ﬄﬄﬄ{zﬄﬄﬄ}
kiwT
i Mwi if i¼j;
zero otherwise
¼
X
n
i¼1
kia2
i wT
i Mwi
ð3:116Þ
A similar expression can be obtained for uTMu, giving
uTKu ¼
X
n
i¼1
kia2
i wT
i Mwi
uTMu ¼
X
n
i¼1
a2
i wT
i Mwi
ð3:117Þ
Using these expressions and noting k1  ki  kn ði ¼ 1; . . .; nÞ,
k1uTMu ¼
X
n
i¼1
k1
|{z}
 ki
a2
i wT
i Mwi 
X
n
i¼1
ki
|{z}
 kn
a2
i wT
i Mwi
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
uTKu
 knuTMu
ð3:118Þ
Dividing throughout by uTMu and noting RðuÞ ¼ ðuTKuÞ=ðuTMuÞ gives
(3.115).
■
Example 3.5 (Shear building, Rayleigh quotient) For a shear building structure
with stiffness and mass matrices of the form respectively in (3.100) and (3.101), the
numerator and denominator of the Rayleigh quotient simplify to
uTKu ¼
X
n
i¼1
kiðui  ui1Þ2
uTMu ¼
X
n
i¼1
miu2
i
ðshear buildingÞ ð3:119Þ
where u ¼ ½u1; . . .; unT and u0 ¼ 0.
Recall the six-storied shear building in Example 3.4, with uniform stiffness ki ¼
200 kN/mm and ﬂoor mass mi ¼ 100 tons ði ¼ 1; . . .; 6Þ. Here we estimate the
96
3
Structural Dynamics and Modal Testing

natural frequencies of the ﬁrst two modes using the Rayleigh quotient. The mode
shapes in Fig. 3.10 are typical and they provide intuitions for suggesting trial mode
shapes in general situations.
First Mode
For the ﬁrst mode, assume the trial mode shape
u ¼ ½ 1
2
3
4
5
6 T
whose linearly increasing nature with height is simple and intuitive. Substituting
into the Rayleigh quotient (3.110) and using (3.119) gives
RðuÞ ¼ 200  ð1  0Þ2 þ 200  ð2  1Þ2 þ    þ 200  ð6  5Þ2
100  ð1Þ2 þ 100  ð2Þ2 þ    þ 100  ð6Þ2
 106 N/m
103 kg
¼ 131:8 s2
Note the unit conversion factor, missing which the result will err by orders of
magnitude. An estimate for the ﬁrst mode natural frequency is given by
f1 	
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
131:8 s2
p
2p
¼ 1:83 Hz
This estimate is larger than the exact value of 1.72 Hz (see Fig. 3.10), as guaranteed
by the lower bound in (3.115). The error is 6%, which is sufﬁcient for typical use
(e.g., initial design).
Second Mode
For the second mode, assume the trial mode shape
u ¼ ½ 1
1:5
1
0
1
1:5 T
whose variation with positive and negative sides is intuitive. Then
RðuÞ ¼ 200  ð1  0Þ2 þ 200  ð1:5  1Þ2 þ    þ 200  ½1:5  ð1Þ2
100  ð1Þ2 þ 100  ð1:5Þ2 þ    þ 100  ð1:5Þ2
 106 N/m
103 kg
¼ 1000 s2
f2 	
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1000 s2
p
2p
¼ 5:03 Hz
This estimate is lower than the exact value of 5.05 Hz (see Fig. 3.10), demon-
strating that the natural frequency estimates of higher modes do not have the lower
bound property. The error is still small, less than 1%.
■
3.2
MDOF Dynamics
97

3.3
Remarks on Damping
Unlike the stiffness or mass matrix that can be modeled by basic mechanics, there is
still no accepted method for modeling structural damping, except for contributions
from major discrete damping elements (e.g., dampers). Classical damping is con-
ventionally assumed. Modal damping ratios are speciﬁed directly rather than cal-
culated from the damping matrix. The latter is seldom constructed from ﬁrst
principles. Mathematical convenience and lack of knowledge in modeling damping
are the main reasons for this practice.
Necessary and Sufﬁcient Condition
Theoretically, a necessary and sufﬁcient condition for classical damping is
KM1C ¼ CM1K
ð3:120Þ
The proof can be found at the end. See also Caughey and O’Kelly (1965); and
Adhikari (2014) for general damping models. In structural dynamic applications,
however, this condition is rarely checked to justify the use of classical damping;
C is not available in the ﬁrst place.
Repeatability
Measured vibrations from full-scale structures show ‘point-wise’ agreement with
the oscillatory behavior of response assuming viscous damping. That is, the mea-
sured vibration (e.g., acceleration) time history often agrees with the theoretical
prediction for some value of damping; but the same value of damping need not
predict well another set of measured time history. ‘Repeatability’ is the issue.
Amplitude Dependence
‘Amplitude-dependent damping’ is one important characteristic that is not
accounted by the viscous model with constant damping matrix. The identiﬁed
values of damping ratio from vibration measurements exhibit a general increase
(though with signiﬁcant scatter) with the ‘amplitude’ of vibration, the latter deﬁned
in some empirical way depending on the nature of vibration (e.g., steady-state or
stochastic). Even in this case the common treatment is still based on viscous
damping with empirical modiﬁcations, e.g., using different values of damping ratio
at different target amplitudes of vibration (see Example 3.6). In most structural
vibration applications, damping is more of an empirical parameter to match theory
with reality, rather than one that can be derived fundamentally from physical
modeling.
Damping is one important challenging subject in wind engineering of tall
buildings and long-span bridges. In addition to structural damping, it can also be
attributed to aerodynamic effects (relative motion between structure and wind) and
soil-structure interaction. Early discussion of structural damping mechanisms can
be found in Jeary (1986, 1997). Beards (1996) has a chapter devoted to damping
98
3
Structural Dynamics and Modal Testing

with introductory coverage on different sources. See recent reports in Tamura and
Kareem (2013) and Kwok et al. (2015). There have been many attempts on iden-
tifying and predicting amplitude-dependent damping from full-scale vibration
measurements but results are mixed with large scatter. See, e.g., Fukuwa et al.
(1996), Li et al. (2000), Satake et al. (2003) and Kijewski-Correa et al. (2006).
Identiﬁed values form the basis of guidelines currently used in design. See, e.g.,
EC1 Annex F (2005), ESDU 83009 (2012), ISO 4354:2009 Annex E, and ISO
10137:2007 Annex B, Bachmann et al. (1995).
Example 3.6 (Amplitude-dependent damping) Figure 3.11a shows the free
vibration time history of acceleration measured from a SDOF structure under
laboratory condition. Figure 3.11b shows the same plot but on a log10-scale. If the
damping does not depend on amplitude, the envelope of the peaks in (b) should
form a straight line, because the envelope is A / efx1t, i.e.,
log10 A ¼ ðfx1 log10 eÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
slope
t þ constant
ð3:121Þ
where x1 (rad/s) is the natural frequency and f is the damping ratio. In Fig. 3.11b,
the slope of the envelope decreases in magnitude as the amplitude decreases with
time, which is an evidence of amplitude-dependent damping. The damping ratios
indicated in the ﬁgure were estimated using logarithmic decrement method
(Sect. 3.1.4) with ﬁve cycles.
■
(a)
(b)
Fig. 3.11 Free vibration acceleration time history of a SDOF laboratory structure exhibiting
amplitude dependence. Data is plotted on linear scale in (a) and log10-scale in (b)
3.3
Remarks on Damping
99

Proof of (3.120) (Necessary and sufﬁcient condition for classical damping)
A conventional way to prove is to write the equation of motion in (3.84) as (after
pre-multiplying by M1=2)
M1=2€xðtÞ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
€qðtÞ
þ M1=2CM1=2
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
~C
M1=2 _xðtÞ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
_qðtÞ
þ M1=2KM1=2
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
~K
M1=2xðtÞ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
qðtÞ
¼ M1=2FðtÞ
ð3:122Þ
and use the result in linear algebra that the real symmetric (hence diagonalizable)
matrices ~C and ~K are diagonalizable by the same set of eigenvectors if and only if
they commute, i.e., ~C~K ¼ ~K~C, which is equivalent to (3.120). Here we use an
alternative approach to illustrate the algebraic structure of the problem.
Let W ¼ ½w1; . . .; wn, where wi ði ¼ 1; . . .; nÞ is the eigenvector associated with
eigenvalue ki, satisfying Kwi ¼ kiMwi. We show that D ¼ WTCW is a diagonal
matrix if and only if (3.120) holds, i.e., KM1C ¼ CM1K.
By
orthogonality,
A ¼ WTMW
and
B ¼ WTKW
are
diagonal
matrices.
Pre-multiplying these by WT and post-multiplying by W1,
M ¼ WTAW1
K ¼ WTBW1
ð3:123Þ
Using ðXYZÞ1 ¼ Z1Y1X1 for invertible matrices X, Y and Z,
M1 ¼ WA1WT
ð3:124Þ
Necessary Condition
We ﬁrst assume that D ¼ WTCW is a diagonal matrix and show that (3.120) holds.
Similar to (3.123),
C ¼ WTDW1
ð3:125Þ
Substituting (3.123) to (3.125) into the LHS of (3.120),
KM1C ¼ ðWTB W1ÞðW
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
In
A1 WTÞðWT
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
In
DW1Þ ¼ WTBA1DW1
ð3:126Þ
Since A1, B and D are diagonal matrices, BA1D ¼ DA1B. Then (3.120) follows
because
KM1C ¼ WTDA1BW1 ¼ ðWTDW1Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
C
ðWA1WTÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
M1
ðWTBW1Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
K
ð3:127Þ
100
3
Structural Dynamics and Modal Testing

Sufﬁcient Condition
Conversely, assume that (3.120) holds, i.e., KM1C ¼ CM1K. We will show that
WTCW is a diagonal matrix. Substituting (3.123) and (3.124) into this equation
gives
ðWTB W1ÞðW
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
In
A1WTÞC ¼ CðWA1 WTÞðWT
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
In
BW1Þ
) WTBA1WTC ¼ CWA1BW1
ð3:128Þ
Pre-multiplying by WT and post-multiplying by W,
BA1ðWTCWÞ ¼ ðWTCWÞA1B
ð3:129Þ
Note that BA1 and A1B are both equal to a diagonal matrix with entries fx2
i gn
i¼1.
Let Dij be the ði; jÞ-entry of WTCW. Reading the ði; jÞ-entry on both sides of (3.129)
and rearranging gives
ðx2
i  x2
j ÞDij ¼ 0
i; j ¼ 1; . . .; n
ð3:130Þ
This equation must hold for all i; j ¼ 1; . . .; n. For i ¼ j, it holds trivially. For i 6¼ j,
if xi 6¼ xj then Dij ¼ 0. This implies that the off-diagonal entries of WTCW are zero
for any two modes with distinct eigenvalues. The case for modes with the same
(repeated) eigenvalue requires more linear algebra arguments, whose details are
omitted here.
■
shaker force = ms ω2 ρ sinωt
mass in
shaker force
upper semi-circle
upward
lower semi-circle
downward
shaker force = −ms a(t)
mass accelerates
shaker force
upward
downward
downward
upward
ωt
acceleration
a(t)
shaker mass ms
(moving mass)
ms/2
ms/2
structure
structure
shaker
shaker
(a)
(b)
ωt
Fig. 3.12 Two examples of shaker. a Reciprocating motor; b electro-magnetic shaker.
Expressions of shaker force assumes upward positive
3.3
Remarks on Damping
101

3.4
Harmonic Load Test
Structural vibration generally comprises contributions from different modes. When
a structure is excited by a harmonic force near the natural frequency of a particular
mode, however, its response is dominated by the mode due to resonance; dynamic
ampliﬁcation of the mode is much greater than those of other modes. This obser-
vation suggests a conceptually simple method for identifying the modal properties
of a MDOF structure using harmonic excitations, often artiﬁcially generated by
‘shakers’. A shaker derives its payload through the motion of an electro-
mechanically driven moving mass. One simple mechanism is a ‘reciprocating
motor’, as shown in Fig. 3.12a. The two eccentric masses (ms=2 each) rotating at a
constant angular speed x (rad/s) in opposite sense generates a harmonic vertical
force with amplitude msqx2 but ideally no horizontal force due to cancellation.
Figure 3.12b is a more general mechanism where the motion of the moving mass
is driven by an electro-magnetic force that can be assigned arbitrarily (within
physical limits). Neglecting the inertia of other parts, the ‘shaker force’, i.e., force
exerted by the shaker on the structure, is simply msaðtÞ. The negative sign
accounts for the action-reaction pairs (Newton’s third law) of interactive forces
between the moving mass, shaker (excluding the moving mass) and the structure.
For convenience, the moving mass is referred as the ‘shaker mass’, which should
not be confused with the total mass of the shaker. For the shaker mass to accelerate
upward (say), it must receive an upward force from the shaker (e.g., magnetic force
by solenoid). By action-reaction pair, there is an equal but opposite (i.e., down-
ward) force on the shaker. This downward force is further transmitted from the
shaker to the structure by another action-reaction pair. The shaker force is often
measured indirectly by an accelerometer attached to the shaker mass.
3.4.1
Collocated Setup
Consider a simple setup where the shaker in Fig. 3.12b exerts a vertical harmonic
force on the structure. There are two accelerometers measuring vertical motion, one
on the shaker mass and the other on the structure at the same location of the shaker,
i.e., ‘collocated’. ‘Vertical direction’ here is only for instructional purpose and in
general it can be any DOF of the structure. For a particular mode, if we scale the
mode shape such that its value at this DOF is 1, the amplitude of modal force (per
unit modal mass) due to the shaker will be msAp=M, where Ap is the acceleration
amplitude of the shaker mass (ms), and M is the modal mass of the structure. If the
excitation frequency x (rad/s) is near the natural frequency x0 (rad/s) of a lightly
damped mode well-separated from others, the steady-state acceleration amplitude
102
3
Structural Dynamics and Modal Testing

Ax of the structure measured at the same location can be approximated by the
contribution from the mode only:
Ax
|{z}
struct:
acc:
amp:
	 msAp
Mx2
0
|ﬄﬄ{zﬄﬄ}
static disp:

A
|{z}
dyn: amp: between
modal force and disp:

x2
|{z}
convert disp:
to acc: amp:
ðx 	 x0Þ
ð3:131Þ
where A ¼ 1=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð1  b2Þ2 þ ð2fbÞ2
q
is the dynamic ampliﬁcation factor in (3.51);
b ¼ x=x0 and f is the damping ratio of the mode. Note that Ax, Ap and A depend on
x but this has been omitted to simplify notation. Equation (3.131) can be rear-
ranged to give A / Ax=Apx2, plotting which allows the half-power bandwidth
method in Sect. 3.1.8 to be applied for estimating the natural frequency and damping
ratio. See Example 3.2 in that section.
Modal Mass
The modal mass can also be further estimated, which is generally not possible
unless the input force is measured. When the excitation frequency is equal to the
natural frequency, i.e., x ¼ x0, A 	 1=2f for small f 
 1. Substituting into
(3.131) and rearranging gives
M 	 Ap
Ax
ms
2f
ðx ¼ x0Þ
ð3:132Þ
3.4.2
Least Squares Approach
The last section illustrates the basic principle of how modal properties can be
estimated from steady-state response amplitudes. Better (and often more sophisti-
cated) methods can be adopted that make use of the information at different exci-
tation frequencies and address the presence of measurement noise. For example, the
natural frequency x0, damping ratio f and shaker mass ratio r ¼ ms=M can be
estimated as the ones that minimize the following ‘objective function’, deﬁned as
the sum of squared difference between the LHS and RHS of (3.131):
Jðx0; f; rÞ ¼
X
x

AxðxÞ
|ﬄﬄ{zﬄﬄ}
measured
 rx2
x2
0
ApðxÞ
|ﬄﬄ{zﬄﬄ}
measured
AðxÞ
|ﬄ{zﬄ}
depends
on x0 and f
2
ð3:133Þ
3.4
Harmonic Load Test
103

where the sum is over all tested values of the excitation frequency x; the depen-
dence of Ax, Ap and A on x has been emphasized. As indicated, A ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð1  b2Þ2 þ ð2fbÞ2
q
ðb ¼ x=x0Þ
also
depends
on
x0
and
f.
The
three-dimensional minimization problem can be solved numerically using standard
algorithms, e.g., fminsearch in Matlab. Once r has been estimated, the modal
mass can be recovered from M ¼ ms=r. The above is only one of the many pos-
sibilities where the modal properties can be estimated.
Example 3.7 (Shaker test on ﬂoor slab, modal mass) Consider estimating the
modal mass of the ﬂoor slab in Example 3.2. The shaker mass is known to be ms ¼
13.2 kg. The shaker and accelerometer on the structure are close to each other
compared to the dimension of the slab (about 35 m  35 m) and so they may be
assumed to be collocated. Equation (3.132) then applies. It was estimated in
Example 3.2 that the natural frequency was 6.24 Hz and damping ratio was 1.15%.
From Table 3.3, at f ¼ 6.24 Hz, Ap ¼ 1171 milli-g and Ax ¼ 2:439 milli-g.
Substituting into (3.132) gives
M 	 1171
2:439
milli-g
milli-g


 13:2 ½kg
2ð1:15%Þ ¼ 276  103 kg
The value of modal mass depends on the scaling of mode shape, which has been
assumed to be 1 vertically at the shaker location. If, relative to this scaling, the
mode shape is multiplied by a factor, the modal mass should be multiplied by the
square of that factor.
Least Squares Method
Alternatively, using the values of Ax and Ap in Table 3.3, the modal properties that
minimize the objective function in (3.133) are found to be f0 ¼ x0=2p 	 6:24 Hz,
6
6.1
6.2
6.3
6.4
6.5
6.6
0
0.5
1
1.5
2
2.5
3 x 10
-3
Excitation frequency f [Hz]
Struct. acc. amp. [g]
best fit
measured
Fig. 3.13 Measured and best least squares ﬁt of structural acceleration amplitude
104
3
Structural Dynamics and Modal Testing

f 	 1:11% and r 	 46:78  106. The corresponding estimate for the modal mass is
then M ¼ ms=r 	 13:2=ð46:78  106Þ ¼ 282  103 kg. These estimates are simi-
lar to those found earlier, differing by less than 5%. Figure 3.13 shows the measured
and best ﬁtted structural acceleration amplitudes. Further details about modal testing
of the structure in this example can be found in Au et al. (2012).
■
3.5
Impact Hammer Test
The input-output relationship between an impulsive applied force and vibration
response in the frequency domain can be exploited for estimating the modal
properties of a structure. The basic idea is as follow. Consider a structure subjected
to an ideal impulsive force QdðtÞ at DOF r, where Q is the amount of impulse (area
of force with time) and dðtÞ is the Dirac Delta function (Sect. 3.1.11). Applying
(3.109), the modal force (per unit modal mass) is given by
pðtÞ ¼ /rQ
M dðtÞ
ð3:134Þ
where M ¼ wTMw is the modal mass and w ¼ ½/1; . . .; /nT is the mode shape.
Note that pðtÞ, M and w depend on the mode number but this has been omitted to
simplify notation.
3.5.1
Frequency Response
In the frequency domain, the Fourier Transform (FT, Sect. 2.2) of the applied force
QdðtÞ is simply the constant Q because the FT of dðtÞ is 1. Correspondingly, the FT
of the modal force pðtÞ is the constant /rQ=M. This turns out to provide conve-
nience for estimating the modal properties. The FT of structural response generally
comprises contributions from different modes. However, near the natural frequency
of a lightly damped mode well-separated from others, the FT is dominated by the
contribution from the mode, due to dynamic ampliﬁcation. Near the natural fre-
quency x0 (say), the FT of displacement at DOF j is then approximately given by
XjðxÞ 	 /jgðxÞ
ðx 	 x0Þ
ð3:135Þ
where x (rad/s) is the frequency variable of FT; /j is the mode shape value at DOF j;
gðxÞ is the FT of modal displacement whose time-domain counterpart satisﬁes the
modal equation
€gðtÞ þ 2fx0 _gðtÞ þ x2
0gðtÞ ¼ /rQ
M dðtÞ
ð3:136Þ
3.4
Harmonic Load Test
105

and f is the damping ratio of the mode. Taking FT on both sides of (3.136) and
noting that the FTs of _gðtÞ and €gðtÞ are respectively ixgðxÞ and x2gðxÞ, one
obtains, after rearranging,
gðxÞ ¼ GðxÞ /rQ
M
ð3:137Þ
where
GðxÞ ¼ x2
0
1  b2 þ 2fbi

1
b ¼ x
x0
ð3:138Þ
is the ‘transfer function’ of the mode. Note that
GðxÞ
j
j ¼ x2
0 AðbÞ
b ¼ x
x0
ð3:139Þ
where AðbÞ is the dynamic ampliﬁcation factor in (3.51). Substituting (3.137) into
(3.135) gives
XjðxÞ
|ﬄﬄ{zﬄﬄ}
FT of response
at DOF j
	
/j
|{z}
mode shape
value at DOF j
GðxÞ
|ﬄ{zﬄ}
modal
transfer
function
/rQ
M
|ﬄ{zﬄ}
FT of
modal force
ðx 	 x0Þ
ð3:140Þ
Taking modulus and using (3.139),
XjðxÞ

 	 /j/rQ
M

 GðxÞ
j
j / AðbÞ
b ¼ x
x0
ð3:141Þ
A plot of XjðxÞ

 versus x can then be used for estimating the natural frequency
and damping ratio, e.g., using the half-power bandwidth method. The estimates
based on different measured DOFs are generally different. They may be empirically
averaged to give a representative value.
Mode Shape
For two DOFs j and k, considering the ratio XjðxÞ=XkðxÞ using (3.140) gives
/j
/k
	 XjðxÞ
XkðxÞ
ðx 	 x0Þ
ð3:142Þ
This implies that the ratio of mode shape values can be estimated as the ratio of the
FTs of response near the natural frequency. Together with a chosen scaling, this
determines the mode shape values at the measured DOFs.
106
3
Structural Dynamics and Modal Testing

Velocity or Acceleration Measurement
In the above, the modal identiﬁcation principle has been explained in terms of
displacement measurement. It can also be applied with velocity or acceleration
measurement, provided that the transfer function GðxÞ is modiﬁed accordingly.
Multiply GðxÞ by ix for velocity and ðixÞ2 for acceleration measurement. In
implementation with digital data, the FT is replaced by its discrete-time counterpart
(Sect. 2.3).
Impact Source
Practically, the impulsive input force can be provided by, e.g., a drop weight or an
impact hammer. In the minimum setting, the input force need not be measured but
doing so offers more information (e.g., modal mass, see Sect. 3.5.2 later) and
ﬂexibility in testing (see Sect. 3.5.3 later). In reality, the input force is not an ideal
impulse. Its FT is only approximately constant with a ﬁnite bandwidth limited by
non-zero impulse duration. Correspondingly, the value of Q in the foregoing dis-
cussion should be a representative FT value of the input force near the natural
frequency. The accuracy of modal property estimates is not signiﬁcantly affected by
the limited bandwidth of the input impulse as long as its FT is reasonably constant
near the natural frequency and the mode is adequately excited. Measurement noise
can be reduced by averaging the FTs of the measured response over multiple trials.
3.5.2
Least Squares Approach
The modal properties can be estimated in a more sophisticated manner using a least
squares approach, making use of the FT of measured response near the natural
frequency. Suppose the mode shape is scaled so that /r ¼ 1. Let
aj ¼ /jQ
M
ð3:143Þ
Then x0, f and aj at the measured DOFs can be estimated as the ones that minimize
the following objective function, deﬁned as the squared modulus of the difference
between the LHS and RHS of (3.140) summed over x near the natural frequency
and all measured DOFs:
Jðx0; f; fajgÞ ¼
X
j
X
x
 XjðxÞ
|ﬄﬄ{zﬄﬄ}
measured
aj GðxÞ
|ﬄ{zﬄ}
depends
on x0 and f
2
ð3:144Þ
where fajg denotes the collection of aj s at the measured DOFs. Note that J depends
on x0 and f through GðxÞ. Since aj affects J in a quadratic manner, its optimal
value, denoted by ^aj, can be obtained analytically as:
3.5
Impact Hammer Test
107

^aj ¼
P
x Re½XjðxÞGðxÞ
P
x jGðxÞj2
ð3:145Þ
where GðxÞ denotes the complex conjugate of GðxÞ. Substituting aj ¼ ^aj into
(3.144) gives an expression that depends on x0 and f only. Numerically minimizing
such expression gives the optimal values for x0 and f. Substituting these optimal
values into GðxÞ and then into (3.145) gives the optimal value for aj.
Mode Shape and Modal Mass
If the vibration at the force DOF r is measured, one can use /r ¼ 1 to recover the
ratio Q=M ¼ ar, which follows directly from (3.143). The mode shape values at
other measured DOFs can be determined from /j ¼ aj=ar. If the input force is also
measured so that Q is known, the modal mass can be recovered from M ¼ Q=ar.
On the other hand, if the vibration at the force DOF r is not measured, one can still
use /j / aj and apply a scaling constraint to uniquely determine the mode shape
values at the measured DOFs. The potential conﬂict of the new scaling with /r ¼ 1
is irrelevant because DOF r is not measured. Since ar is not known (DOF r is not
measured), it is not possible to recover the modal mass from M ¼ Q=ar, unless ar is
estimated by other means, e.g., interpolation using the mode shape values at the
measured DOFs.
3.5.3
Covering DOFs in Multiple Setups
To obtain mode shape values at the desired DOFs, a direct way is to deploy
multiple vibration sensors (e.g., accelerometers) to cover them all in a single
setup. When this is not possible (e.g., not enough sensors), one can cover the DOFs
in multiple setups, assuming that the overall mode shape remains the same in
different setups. The possibilities are summarized in Table 3.5. When the input
force is not measured (Case 1), the setups need to share some common ‘reference
DOFs’ so that their ‘local mode shapes’ (i.e., with DOFs in a given setup only) can
Table 3.5 Cases where global mode shape can be determined from multiple setups
Case
Force
measured?
Force DOF
Sensor
DOF(s)
Remark
1
No
Fixed/Roved
Roved
Global mode shape covering sensor DOFs
can be determined if the setups share some
reference DOF(s)
2
Yes
Fixed
Roved
Global mode shape covering sensor DOFs
can be determined; reference sensor DOFs
not needed
3
Yes
Roved
Fixed
Global mode shape covering force DOFs
can be determined
108
3
Structural Dynamics and Modal Testing

be assembled to form the ‘global mode shape’ containing all DOFs. This belongs to
the general problem of ‘mode shape assembly’. See Sects. 14.1 and 14.2 for least
squares methods. When the input force is measured, one can either ﬁx the input
force and ‘rove’ the sensor(s) (Case 2); or, ﬁx the sensor(s) and rove the input force
(Case 3). These two cases are described next.
Roving Sensor DOF(s)
The principle in this case is similar to the single-setup setting, except that the input
force must now be measured because it need not be the same in different setups.
Consider two setups, say, 1 and 2, where the input force is always applied at DOF
r. Let the vibrations at DOFs j and k be measured in Setups 1 and 2, respectively.
Then (3.140) reads
XjðxÞ 	 /jGðxÞ /rQ1
M
XkðxÞ 	 /kGðxÞ /rQ2
M
x 	 x0
j in Setup 1
k in Setup 2
ð3:146Þ
where Q1 and Q2 are the FTs of the input force in Setups 1 and 2, respectively.
Considering the ratio XjðxÞ=XkðxÞ and rearranging gives
/j
/k
	 XjðxÞQ2
XkðxÞQ1
x 	 x0
j in Setup 1
k in Setup 2
ð3:147Þ
This allows the mode shape values of the DOFs from different setups to be com-
bined to form the global mode shape, even when the setups do not share any sensor
DOFs in common. The term Q2=Q1 in the expression explains why it is necessary
to measure the input force.
Roving Force DOF
Instead of roving vibration sensors, it is possible to obtain the global mode shape by
roving the input force over the DOFs in different setups. In the simplest setting,
suppose the vibration at only one DOF is measured, always at DOF j in all setups.
Let XjrðxÞ denote the FT of the response at DOF j when the input force is applied at
DOF r with FT Qr near the natural frequency. Consider the ratio of the measured
response FTs of two tests, one with input force at DOF r and the other at DOF s.
Equation (3.140) now reads
XjrðxÞ 	 /jGðxÞ /rQr
M
XjsðxÞ 	 /jGðxÞ /sQs
M
ðx 	 x0Þ
ð3:148Þ
3.5
Impact Hammer Test
109

Considering the ratio XjsðxÞ=XjrðxÞ and rearranging gives
/s
/r
	 XjsðxÞQr
XjrðxÞQs
ðx 	 x0Þ
ð3:149Þ
Thus, by applying the input force at different DOFs (s) in different setups, the mode
shape value ð/sÞ of these DOFs can be determined (together with a chosen scaling).
A simple choice for the sensor DOF j and the reference force DOF r is to take them
collocated at a DOF expected to have large mode shape value /r.
3.6
State-Space Approach
In Sects. 3.1 and 3.2, the response of a structure is obtained by direct analysis of the
equation of motion, which is a second order ODE. In this section we introduce the
‘state-space’ approach, where dynamics is studied through an equivalent ﬁrst order
ODE. Elegant theory for the latter has been developed, providing a means for
analyzing general dynamical systems. State-space approach is applied in Sects. 3.7
and 3.8 later for studying the numerical stability of time integration schemes; and in
Sects. 5.3 and 5.4 for studying the transient response statistics of structures sub-
jected to stochastic excitations.
State-Space Equation of Structural Dynamics
Recall the equation of motion (3.84) of a structure with n DOFs:
M
nn €xðtÞ
n1
þ C
nn _xðtÞ
n1
þ K
nn xðtÞ
n1
¼ FðtÞ
n1
ð3:150Þ
This is a second order vector ODE. In a state-space approach, it is converted to a
ﬁrst order vector ODE. Deﬁne the ‘state-vector’ yðtÞ by
yðtÞ
2n1
¼
y1ðtÞ
y2ðtÞ


¼
xðtÞ
_xðtÞ


ð3:151Þ
Then y1ðtÞ and y2ðtÞ satisfy
_y1ðtÞ ¼ y2ðtÞ
_y2ðtÞ ¼ M1Ky1ðtÞ  M1Cy2ðtÞ þ M1FðtÞ
ð3:152Þ
The ﬁrst equation follows from the deﬁnition in (3.151). The second is just the
equation of motion in (3.150) but now written in terms of y1 and y2. The two
equations in (3.152) can be written as a ﬁrst order ODE in yðtÞ:
110
3
Structural Dynamics and Modal Testing

_yðtÞ
2n1
¼
A
2n2n yðtÞ
2n1
þ B
2nn FðtÞ
n1
ð3:153Þ
A ¼
0n
In
M1K
M1C


B
2nn ¼
0n
M1


ð3:154Þ
where 0n and In denote the n  n zero and identity matrix, respectively. The
matrices A and B are often called the ‘system matrices’ of state-space dynamics.
Generalization
The above can be generalized. One can always convert a n-dimensional vector ODE
of order p to a np-dimensional ﬁrst order vector ODE:
ApxðpÞ þ Ap1xðp1Þ þ    þ A1xð1Þ þ A0x ¼ FðtÞ
, d
dt
x
xð1Þ
..
.
xðp1Þ
2
66664
3
77775
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
_yðtÞ
np1
¼
0n
In
..
.
..
.
0n
In
A1
p A0
A1
p A1
  
A1
p Ap1
2
66664
3
77775
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
A
npnp
x
xð1Þ
..
.
xðp1Þ
2
66664
3
77775
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
yðtÞ
np1
þ
0n
..
.
0n
A1
p
2
66664
3
77775
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
B
npn
FðtÞ
n1
, _yðtÞ
np1
¼
A
npnp yðtÞ
np1
þ B
npn FðtÞ
n1
ð3:155Þ
where xðrÞ denotes the rth derivative of xðtÞ w.r.t. t; the dependence on t has been
omitted for simplicity.
Analytical Solution
The solution of the ﬁrst order ODE _yðtÞ ¼ AyðtÞ þ BFðtÞ given the initial condition
yð0Þ has a simple analytical form (Duhamel’s integral):
yðtÞ ¼ eAtyð0Þ þ
Zt
0
eAðtsÞBFðsÞds
ð3:156Þ
The terms eAt and eAðtsÞ are ‘matrix exponentials’, to be discussed next in
Sect. 3.6.1. It can be shown that eAt becomes the identity matrix when t ¼ 0. Then
it is clear that (3.156) satisﬁes the initial condition because the integral is zero when
t ¼ 0. It can also be shown that deAt=dt ¼ AeAt, which is analogous to the differ-
entiation rule when A is a scalar. Using these results and the Leibniz rule of differ-
entiation in (3.76), the following shows that (3.156) satisﬁes the state-space equation:
3.6
State-Space Approach
111

_yðtÞ ¼ AeAtyð0Þ þ eAðttÞ
|ﬄﬄ{zﬄﬄ}
I
BFðtÞ þ
Zt
0
AeAðtsÞBFðsÞds
¼ A
eAtyð0Þ þ
Zt
0
eAðtsÞBFðsÞds
2
4
3
5
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
yðtÞ
þ BFðtÞ ¼ AyðtÞ þ BFðtÞ
ð3:157Þ
where I denotes the identity matrix.
3.6.1
Matrix Exponential
Let A be a q  q real-valued matrix with eigenvalues fkigq
i¼1 and linearly inde-
pendent eigenvectors fnigq
i¼1, which are generally complex-valued. Assembling the
eigenvalue equation Ani ¼ kini row-wise for i ¼ 1; . . .; q gives
A n1
  
nq
	

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Q
¼
n1
  
nq
	

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Q
k1
..
.
kq
2
64
3
75
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K
ð3:158Þ
) AQ ¼ QK
K
qq ¼
k1
..
.
kq
2
64
3
75
Q
qq ¼
n1
  
nq
	

ð3:159Þ
Post-multiplying by Q1 gives the ‘eigenmatrix representation’ of A:
A ¼ QKQ1
ð3:160Þ
The matrix exponential eAt, or generally an arbitrary function of A, is deﬁned
through eigenvalues and eigenvectors. Let gðuÞ be a scalar-valued function of scalar
variable u. Then gðAÞ is deﬁned as a matrix with eigenvalues fgðkiÞgq
i¼1 and the
same eigenvectors as A:
gðAÞ ¼ Q gðKÞ Q1
ð3:161Þ
112
3
Structural Dynamics and Modal Testing

where gðKÞ for a diagonal matrix K is simply
gðKÞ ¼
gðk1Þ
..
.
gðkqÞ
2
64
3
75
ð3:162Þ
Taking gðuÞ ¼ eut gives
eKt ¼
ek1t
..
.
ekqt
2
64
3
75
eAt ¼ Q eKtQ1
ð3:163Þ
Since Q does not depend on t,
d
dt eAt ¼ Q
d
dt eKt


Q1 ¼ Q
k1ek1t
..
.
kqekqt
2
664
3
775Q1
¼ Q
k1
..
.
kq
2
664
3
775Q1
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
A
Q
ek1t
..
.
ekqt
2
64
3
75Q1
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
eAt
¼ AeAt
ð3:164Þ
This is analogous to the ordinary differentiation rule when A is a scalar. Similar
analogies apply to many other functions or operations as well, although they should
be veriﬁed case by case. For example, by similar steps one can show that
sin2 A þ cos2 A ¼ I
Zt
0
eAtdt ¼ A1ðeAt  IÞ
ð3:165Þ
where I denotes the identity matrix.
3.6.2
Eigenvalue Properties of System Matrix
When the system matrix A is given by (3.154) and the structure is classically
damped, its eigenvalues and eigenvectors can be expressed analytically in terms of
modal properties. As in Sect. 3.2.3, let fxign
i¼1 and fwign
i¼1 be the natural fre-
quencies and mode shapes of the structure, satisfying the eigenvalue equation
3.6
State-Space Approach
113

Kwi ¼ x2
i Mwi ði ¼ 1; . . .; nÞ. Also, let ffign
i¼1 be the modal damping ratios, i.e.,
2fixi ¼ ðwT
i CwiÞ=ðwT
i MwiÞ. Then it can be shown that the eigenvalues of
A appear in complex conjugate pairs given by
ki ¼ fixi þ ixi
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
i
q
kn þ i ¼ ki ¼ fixi  ixi
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
i
q
i ¼ 1; . . .; n
ð3:166Þ
The eigenvector corresponding to ki ði ¼ 1; . . .; 2nÞ is of the form
ni ¼
wi
kiwi


ðreplace wi by win for i [ nÞ
ð3:167Þ
See the end for proof.
Example 3.8 (SDOF structure, state-space) Consider the SDOF equation
€xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ ¼ pðtÞ. In state-space form, the state vector is yðtÞ ¼
½xðtÞ; _xðtÞT and the system matrices are
A ¼
0
1
x2
1
2fx1


B ¼
0
1


ð3:168Þ
Here we evaluate the matrix exponential eAt in terms of the modal properties x1 and
f. For this purpose we ﬁrst evaluate the eigenvalues and eigenmatrix of A, as well
as the inverse of the eigenmatrix.
Using (3.166), the eigenvalues of A are given by
k1 ¼ fx1 þ ixd
k2 ¼ fx1  ixd
ð3:169Þ
where xd ¼ x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
p
is the damped natural frequency. Since there is only one
DOF, the mode shape is just a scalar. We can simply take w1 ¼ 1. Using (3.167),
the eigenmatrix of A is
Q ¼ ½n1; n2 ¼
w1
w1
k1w1
k2w1


¼
1
1
fx1 þ ixd
fx1  ixd


ð3:170Þ
The determinant and inverse of Q are given by
Q
j j ¼ 2ixd
Q1 ¼
1
2ixd
fx1  ixd
1
fx1  ixd
1


ð3:171Þ
114
3
Structural Dynamics and Modal Testing

Substituting k1, k2, Q and Q1 into (3.163) gives, after algebra,
eAt ¼
efx1t cos xdt þ
fﬃﬃﬃﬃﬃﬃﬃﬃ
1f2
p
sin xdt


efx1t
xd sin xdt

x1ﬃﬃﬃﬃﬃﬃﬃﬃ
1f2
p
efx1t sin xdt
efx1t cos xdt 
fﬃﬃﬃﬃﬃﬃﬃﬃ
1f2
p
sin xdt


2
664
3
775
ð3:172Þ
The entries of eAt can be identiﬁed with the complementary free vibration
responses in (3.20):
eAt ¼
g1ðtÞ
g2ðtÞ
_g1ðtÞ
_g2ðtÞ


ð3:173Þ
where g1ðtÞ is the free vibration displacement due to unit initial displacement and
zero velocity; and g2ðtÞ is the free vibration displacement due to unit initial velocity
and zero displacement. This can be reasoned by noting that the free vibration
response of yðtÞ ¼ ½xðtÞ; _xðtÞT starting from initial condition yð0Þ ¼ ½xð0Þ; _xð0ÞT is
yðtÞ ¼ eAtyð0Þ. The expressions of g1ðtÞ and g2ðtÞ, respectively in the (1,1)- and
(1,2)- entry in (3.172), are identical to those in Table 3.2 of Sect. 3.1.3.
■
Derivation of eigenvalue properties in (3.166) and (3.167)
Here we show that if the structure is classically damped then the eigenvalues and
eigenvectors of A in (3.154) are given by (3.166) and (3.167), respectively. Let k be
an eigenvalue and n ¼ ½u; v (u and v both n  1) be the corresponding eigenvector
of A. Then the eigenvalue equation An ¼ kn reads
0n
In
M1K
M1C


u
v


¼ k u
v


ð3:174Þ
Writing out the upper and lower partitions,
v ¼ ku
 M1Ku  M1Cv ¼ kv
ð3:175Þ
Substituting the ﬁrst equation into the second and pre-multiplying by M gives, after
rearranging,
ðk2M þ kC þ KÞu ¼ 0
ð3:176Þ
The eigenvalue k is a solution of the characteristic equation
k2M þ kC þ K

 ¼ 0
ð3:177Þ
3.6
State-Space Approach
115

Note that M, C and K can be expressed as
M ¼ MW
1
..
.
1
2
64
3
75W1
C ¼ MW
2f1x1
..
.
2fnxn
2
664
3
775W1
K ¼ MW
x2
1
..
.
x2
n
2
664
3
775W1
ð3:178Þ
where W ¼ ½w1; . . .; wn. The expression of M is trivial. The expression of
K follows from KW ¼ MWK K ¼ diag fx2
i gn
i¼1




, which is the assembled form
of Kwi ¼ x2
i Mwi ði ¼ 1; . . .; nÞ. The expression of C follows from classical
damping assumption. Using (3.178),
k2M þ kC þ K ¼ MW
k2 þ 2f1x1k þ x2
1
..
.
k2 þ 2fnxnk þ x2
n
2
64
3
75W1
ð3:179Þ
Using the fact that the determinant of the product of square matrices is equal to the
product of their determinants, and noting W
j
j W1

 ¼ 1, we have
k2M þ kC þ K

 ¼ M
j
j
Y
n
j¼1
ðk2 þ 2fjxjk þ x2
j Þ
ð3:180Þ
Since M
j
j 6¼ 0, k must make at least one of the terms in the product on the RHS to
be zero. First consider the case when it makes exactly one term to be zero, say, the
ith term, i.e., k2 þ 2fixik þ x2
i ¼ 0. Solving this equation for k gives the two roots
in (3.166). To determine the eigenvector, substitute (3.179) with k ¼ ki into (3.176)
and pre-multiply by ðMWÞ1. Writing u ¼ Wz, where z ¼ ½z1; . . .; znT, gives
k2
i þ 2f1x1ki þ x2
1
..
.
k2
i þ 2fnxnki þ x2
n
2
64
3
75
z1
..
.
zn
2
64
3
75 ¼ 0
ð3:181Þ
The jth row reads
ðk2
i þ 2fjxjki þ x2
j Þzj ¼ 0
j ¼ 1; . . .; n
ð3:182Þ
116
3
Structural Dynamics and Modal Testing

When j ¼ i the parenthesis on the LHS is zero and the equality is satisﬁed. When
j 6¼ i the parenthesis is not zero and so zj ¼ 0. Thus, only zi is non-zero and
u ¼ ziwi, which is proportional to wi. Together with v ¼ kiu, the eigenvector
corresponding to ki is of the form in (3.167).
In the general case when ki makes mi terms (say) in the product to be zero, the
same argument shows that u lies in the mi-dimensional subspace spanned by the
corresponding wi s but is otherwise arbitrary. We can then set the eigenvectors of
these mi repeated eigenvalues as the wi s so that the eigenvector n is still of the form
in (3.167).
■
3.7
Time Integration Scheme
In the previous sections, the response of a structure was determined analytically in
terms of the excitation time history. As a common situation in applications, one
needs to compute the response based on a given discrete-time excitation history.
The latter may come from real measurements (e.g., ground acceleration during an
earthquake) or it may be synthetically generated (e.g., in a parametric study).
A ‘time integration scheme’ sequentially calculates a discrete-time approximation
of the response at the future time instant based on information up to the present.
In this section we introduce the basic concepts of time integration. This is
followed by the ‘Newmark method’ in Sect. 3.8, which is a popular time integration
scheme in structural dynamic applications. The scope is conﬁned to linear
time-invariant dynamics, although the concepts can be applied in a more general
context to time-variant nonlinear dynamics. For general discussions on time inte-
gration methods, see Hildebrand (1987), Hughes (1987), Subbaraj and Dokainish
(1989 a, b) and Hulbert (2004).
3.7.1
Numerical Stability and Accuracy
A time integration scheme is developed based on a discrete-time approximation of
the governing equation. As a basic requirement, it should be ‘consistent’ (error
converges to zero as time interval decreases) and ‘numerically stable’ (error does
not grow unbounded with time). A further consideration is accuracy, or ‘conver-
gence rate’, i.e., the order at which error scales with the time interval. Depending on
the approximation applied, different schemes can have different stability and
accuracy characteristics.
To introduce these concepts, consider the ﬁrst order equation
_xðtÞ þ axðtÞ ¼ wðtÞ
xð0Þ ¼ x0
ð3:183Þ
3.6
State-Space Approach
117

Assume a [ 0 so that the system is stable. Given the values of wj ¼ wðtjÞ at time
instants tj ¼ jDt ðj ¼ 0; 1; 2; . . .Þ of interval Dt, we want to determine xðtÞ at these
time instants. Let the computed response at tj be ~xj. The tilde ‘*’ reminds that ~xj is
only an approximation to xðtjÞ.
Discrete-Time Approximation
To develop a time integration scheme, suppose we make the ‘forward-difference’
approximation:
_xðtjÞ 	 xðtj þ 1Þ  xðtjÞ
Dt
ð3:184Þ
Considering
(3.183)
at
tj
and
replacing
_xðtjÞ
by
ð~xj þ 1  ~xjÞ=Dt
gives
ð~xj þ 1  ~xjÞ=Dt þ a~xj ¼ wj. Rearranging,
~xj þ 1 ¼ A~xj þ wjDt
A ¼ 1  aDt
j ¼ 1; 2; . . .
ð3:185Þ
This recurrence relationship allows one to compute ~xj sequentially. Starting from
~x0 ¼ x0 (initial condition), ~x1 can be determined from ~x0, then ~x2 from ~x1, and so on.
Numerical Stability
The scheme (3.185) is supposed to give an approximation ~xj of xðtjÞ but it need not
even do a ‘proper’ job. By sequential substitution,
~xj þ 1 ¼ A ðA~xj1 þ wj1DtÞ
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
~xj
þ wjDt ¼ A2~xj1 þ ðwj þ wj1AÞDt ¼   
¼ Aj þ 1~x0 þ
X
j
r¼0
wjrArDt
ð3:186Þ
If A
j j [ 1, the ﬁrst term on the RHS and hence ~xj þ 1 will grow unbounded with the
time index j; the sum will be dominated by terms with large r, i.e., by excitations
that are in the far past. This is the case even when the original system is stable. Such
a scheme is ‘numerically unstable’ and is clearly not doing a proper job. A proper
scheme must be ‘numerically stable’, returning bounded output for bounded input
when the original system is stable. Here it requires A
j j  1, i.e., 0  Dt  2=a. This
effectively puts an upper bound on Dt, besides accuracy requirement.
Error Propagation and Convergence Rate
The error in ~xj propagates with the same dynamic characteristics as itself. Let
ej ¼ ~xj  xðtjÞ
ð3:187Þ
118
3
Structural Dynamics and Modal Testing

be the error at tj. As a reﬁned statement of (3.184), by Taylor series,
xðtj þ 1Þ ¼ xðtjÞ þ _xðtjÞDt þ Rj
ð3:188Þ
where Rj is the remainder term. Generally, it is OðDt2Þ, i.e., limDt!0 Rj=Dt2 is
non-zero and ﬁnite. Substituting _xðtjÞ ¼ axðtjÞ þ wj gives
xðtj þ 1Þ ¼ AxðtjÞ þ wjDt þ Rj
A ¼ 1  aDt
ð3:189Þ
Subtracting this equation from (3.185) and using the deﬁnition of ej gives
ej þ 1 ¼ Aej þ Rj
ð3:190Þ
Compared with (3.185), ej evolves with time by the same mechanism as ~xj but now
it is driven by Rj. If the scheme is numerically unstable, i.e., A
j j [ 1, then ej will
grow unbounded as time goes on. Similar to (3.186),
ej þ 1 ¼ Aj þ 1e0 þ
X
j
r¼0
Rjr
z}|{
OðDt2Þ
Ar
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
OðDtÞ
ð3:191Þ
The sum on the RHS can be reasoned to be OðDtÞ by writing it as a Riemann sum.
This shows that the error is OðDtÞ and the convergence rate is ﬁrst order.
Example 3.9 (First order process) Consider the ﬁrst order equation in (3.183)
with a ¼ 1, wðtÞ  0 and xð0Þ ¼ 1. Numerical stability requires Dt  2 s.
Figure 3.14 shows the computed response ~xj (line with dots) for Dt ¼ 0:5 s (stable),
2 s (marginally stable) and 2.1 s (unstable). These pictures also indicate how an
initial numerical error of 1 unit is propagated. In (a) it diminishes with time; in (b) it
stays; in (c) it grows.
0
5
10
15
20
-3
-2
-1
0
1
2
3
x(t)
time (sec)
0
5
10
15
20
-3
-2
-1
0
1
2
3
time (sec)
0
5
10
15
20
-3
-2
-1
0
1
2
3
(a) stable
(b) marginally stable
(c) unstable
time (sec)
Fig. 3.14 Numerical stability of a ﬁrst order equation. Dashed line exact solution, line with dots
numerical solution. a Dt ¼ 0:5 s (stable); b Dt ¼ 2 s (marginally stable); c Dt ¼ 2:1 s (unstable)
3.7
Time Integration Scheme
119

A numerically stable solution need not be accurate. Figure 3.15a shows ~xj (line
with dots) for Dt ¼ 0.2, 0.5 and 1 s, all numerically stable. In (b), the relative error
j~xj  xðtjÞj=jxðtjÞj at tj ¼ 3 s is plotted versus Dt. Here, the exact solution is
xðtÞ ¼ eat. Viewing macroscopically, for small Dt the error grows linearly. This
agrees with the fact that the convergence rate is ﬁrst order.
■
3.7.2
Discrete-Time State-Space Analysis
The forgoing concepts can be extended to general time integration schemes. One
can write the recurrence relationship in a ﬁrst order discrete-time state-space form:
~yj þ 1 ¼ A~yj þ Bjwj
ð3:192Þ
where ~yj denotes a vector of quantities being updated from one time step to another;
A and Bj are matrices that depend on the scheme; wj denotes a vector containing the
excitations. The scheme is stable if and only if all eigenvalues of A have a modulus
bounded above by 1. To see this, analogous to (3.186),
~yj þ 1 ¼ Aj þ 1~y0 þ
X
j
r¼0
ArBjrwjr
ð3:193Þ
Assume that the eigenvectors of A are linearly independent. One can write A ¼
QKQ1 where K is a diagonal matrix of eigenvalues and Q contains in its columns
the eigenvectors (Sect. 3.6). Then
A j ¼ ðQKQ1ÞðQKQ1Þ    ðQKQ1Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
j times
¼ QK jQ1
ð3:194Þ
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
(a)
time (sec)
x(t)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
(b)
Δt (sec)
relative error
Fig. 3.15 Numerical solution of ﬁrst order equation. In a, from top to bottom: exact, Dt ¼ 0.2, 0.5
and 1 s; b relative error ~xj  xðtjÞ

= xðtjÞ

 at tj ¼ 3 s
120
3
Structural Dynamics and Modal Testing

The diagonal entries of K j are simply the jth power of the eigenvalues. If there is an
eigenvalue with a modulus greater than 1, the corresponding entry in K j will grow
unbounded with j; and so will ~yj.
For a time integration scheme, stability criteria are established to guide the
choice of algorithmic parameters (e.g., Dt). In theory, they can be derived directly
from the analytical expression of eigenvalues. This is difﬁcult when the latter is
mathematically intractable. Methods have been developed to determine stability
based on the coefﬁcients in the characteristic equation, bypassing the determination
of eigenvalues. The Ruth-Hurwitz test is a classical method often discussed in
control theory for determining whether the roots of a polynomial equation (hence
eigenvalues) have a negative real part, which is relevant for stability analysis of
continuous-time systems. By making a transformation of the variable in the
equation, the method can be adapted to investigate whether the roots have a
modulus less than 1. This is relevant for discrete-time time systems and hence
stability analysis of time integration schemes. Other methods are available, e.g., the
Bistritz test (Bistritz 1984, 2002).
Example 3.10 (Central difference, numerical stability) Consider the SDOF
equation €xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ ¼ pðtÞ. Suppose we develop a scheme using the
‘central difference’ approximation of _x and €x (Fig. 3.16):
~_xj ¼ ~xj þ 1  ~xj1
2Dt
~€xj ¼ 1
Dt
ð~xj þ 1  ~xjÞ
Dt
 ð~xj  ~xj1Þ
Dt


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
approx: change of velocity during Dt
¼ ~xj þ 1  2~xj þ~xj1
Dt2
ð3:195Þ
Substituting (3.195) into ~€xj þ 2fx1~_xj þ x2
1~xj ¼ pj and solving for ~xj þ 1 gives
~xj þ 1 ¼ a1~xj  a2~xj1 þ a3pj
a1 ¼ 2  x2
1Dt2
1 þ fx1Dt
a2 ¼ 1  fx1Dt
1 þ fx1Dt
a3 ¼
Dt2
1 þ fx1Dt
ð3:196Þ
(a) velocity
(b) acceleration
Fig. 3.16 Central difference approximation, a for velocity; b for acceleration
3.7
Time Integration Scheme
121

This scheme allows one to determine the future ð~xj þ 1Þ based on the present ð~xjÞ and
the nearest past ð~xj1Þ. To investigate numerical stability, deﬁne the state vector
~yj ¼ ½~xj;~xj1T. It changes according to
~xj þ 1
~xj


|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
~yj þ 1
¼
a1
a2
1
0


|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
A
~xj
~xj1


|ﬄﬄﬄ{zﬄﬄﬄ}
~yj
þ
a3
0


|ﬄﬄ{zﬄﬄ}
Bj
pj
ð3:197Þ
which is in the form of (3.192). The ﬁrst row comes from the ﬁrst equation in
(3.196). The second row is just ~xj ¼ ~xj, which is trivial but necessary to make up the
state-space equation. The scheme is stable if and only if all eigenvalues of A have a
modulus bounded above by 1. The eigenvalues of A are solutions of the charac-
teristic equation:
A  kI
j
j ¼ 0
) k2  a1k þ a2 ¼ 0
ð3:198Þ
Solving the equation gives two roots, k ¼
a1 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2
1  4a2
p


=2.
A simple stability criterion can be derived by considering (conservatively)
f ¼ 0. In this case, a1 ¼ 2  x2
1Dt2, a2 ¼ 1 and k ¼
a1 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2
1  4
p


=2. If
a2
1  4 [ 0, there are two distinct real roots. Since their product is equal to 1
ð¼ a2Þ, one of them must have an absolute value greater than 1 and so the scheme is
unstable. If a2
1  4  0, the two roots are complex conjugate of each other and so
they have the same modulus. Since their product is equal to 1, their modulus must
both be equal to 1, and so the scheme is stable. In conclusion, when f ¼ 0, the
scheme is stable if and only if a2
1  4  0, i.e., 0  Dt  T=p where T ¼ 2p=x1 is
the natural period. In the general case when f [ 0, this is only a sufﬁcient condition
(i.e., possibly more stringent than it needs to be) but is simple enough to apply
regardless of f.
■
3.8
Newmark Scheme
We now introduce the Newmark method for time integration, which is a general
and popular scheme in structural dynamics. Consider ﬁrst the SDOF equation:
m€xðtÞ þ c_xðtÞ þ kxðtÞ ¼ FðtÞ
ð3:199Þ
Newmark scheme is developed based on the approximation of €xðtÞ.
122
3
Structural Dynamics and Modal Testing

(a) constant acceleration
(b) linear acceleration
(c) general case
Fig. 3.17 Approximation in Newmark scheme. a Constant acceleration; b linear acceleration;
c general case
3.8.1
SDOF Linear Acceleration
Suppose €xðtÞ varies piecewise linearly from one time step to another, as in
Fig. 3.17b. Then for 0  s  Dt,
€xðtj þ sÞ ¼ €xðtjÞ þ €xðtj þ 1Þ  €xðtjÞ
	

 s
Dt
ð3:200Þ
Integrating successively w.r.t. s from s ¼ 0 gives, after rearranging,
_xðtj þ sÞ ¼ _xðtjÞ þ s€xðtjÞ þ s2
2Dt €xðtj þ 1Þ  €xðtjÞ
	

xðtj þ sÞ ¼ xðtjÞ þ s_xðtjÞ þ 1
2 s2€xðtjÞ þ s3
6Dt €xðtj þ 1Þ  €xðtjÞ
	

ð3:201Þ
Evaluating at s ¼ Dt, noting _xðtj þ DtÞ ¼ _xðtj þ 1Þ and xðtj þ DtÞ ¼ xðtj þ 1Þ, and
collecting terms,
_xðtj þ 1Þ ¼ _xðtjÞ þ Dt
2 €xðtjÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
present
þ Dt
2 €xðtj þ 1Þ
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
future
xðtj þ 1Þ ¼ xðtjÞ þ Dt _xðtjÞ þ Dt2
3 €xðtjÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
present
þ Dt2
6 €xðtj þ 1Þ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
future
ð3:202Þ
Suppose fxðtjÞ; _xðtjÞ;€xðtjÞg at the present time step tj are known. The above
equations indicate that, in order to determine xðtj þ 1Þ and _xðtj þ 1Þ at the future time
step tj þ 1, one only needs to know the future acceleration €xðtj þ 1Þ. Substituting them
into (3.199) at t ¼ tj þ 1 and solving gives €xðtj þ 1Þ in terms of the present responses
fxðtjÞ; _xðtjÞ;€xðtjÞg and the future excitation Fðtj þ 1Þ. Here, the computed response is
exact when €xðtÞ is piecewise linear within the time interval. Otherwise it is only
approximate.
3.8
Newmark Scheme
123

3.8.2
SDOF General Scheme
Linear acceleration approximation is a special case in the Newmark scheme. In the
general case (Fig. 3.17c), one can write
€xðtj þ sÞ ¼ €xðtjÞ þ ½€xðtj þ 1Þ  €xðtjÞrjðsÞ
0  s  Dt
ð3:203Þ
for some (unknown) function rjðsÞ with the property that rjð0Þ ¼ 0 and rjðDtÞ ¼ 1.
Following similar steps as before, one obtains
_xðtj þ 1Þ ¼ _xðtjÞ þ ð1  cjÞDt €xðtjÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
present
þ cjDt €xðtj þ 1Þ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
future
xðtj þ 1Þ ¼ xðtjÞ þ Dt _xðtjÞ þ
1
2  bj


Dt2€xðtjÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
present
þ bjDt2€xðtj þ 1Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
future
ð3:204Þ
where
cj ¼ 1
Dt
ZDt
0
rjðsÞds
bj ¼ 1
Dt2
ZDt
0
Zu
0
rjðsÞdsdu
ð3:205Þ
Of course, cj and bj are generally unknown and they may depend on j. Taking
them (heuristically) to be constant, say, c and b, leads to a general approximation in
the Newmark scheme:
~_xj þ 1 ¼ ~_xj þ ð1  cÞDt ~€xj þ cDt ~€xj þ 1
~xj þ 1 ¼ ~xj þ Dt ~_xj þ
1
2  b


Dt2~€xj þ bDt2~€xj þ 1
ð3:206Þ
Again, the tilde ‘*’ reminds that the quantity is only an approximation.
Substituting into (3.199) at t ¼ tj þ 1 and rearranging gives
~€xj þ 1 ¼ D1Pj þ 1
D ¼ m þ cDt c þ bDt2k
Pj þ 1 ¼ Fj þ 1  c ~_xj þ ð1  cÞDt ~€xj
	

 k ~xj þ Dt ~_xj þ
1
2  b


Dt2~€xj


ð3:207Þ
where Fj þ 1 ¼ Fðtj þ 1Þ.
124
3
Structural Dynamics and Modal Testing

Given the initial conditions xð0Þ ¼ u and _xð0Þ ¼ v, the value of ~€x0 can be
obtained from (3.199) at t ¼ 0 to be ~€x0 ¼ m1 F0  cv  ku
ð
Þ. The Newmark
scheme is summarized as follow.
Newmark scheme for SDOF Eq. (3.199)
Given xð0Þ ¼ u, _xð0Þ ¼ v and fFj ¼ FðjDtÞ : j ¼ 0; 1; 2; . . .g
Step 1. Set ~x0 ¼ u, ~_x0 ¼ v and ~€x0 ¼ m1 F0  cv  ku
ð
Þ
Step 2. For j ¼ 0; 1; 2; . . .
2.1. Calculate ~€xj þ 1 from (3.207).
2.2. Calculate ~_xj þ 1 and ~xj þ 1 from (3.206).
End for j
3.8.3
General MDOF Scheme
The SDOF Newmark scheme can be directly generalized for solving the MDOF
equation:
M€xðtÞ þ C_xðtÞ þ KxðtÞ ¼ FðtÞ
ð3:208Þ
In this case, (3.206) and (3.207) read
~_xj þ 1 ¼ ~_xj þ ð1  cÞDt~€xj þ cDt ~€xj þ 1
~xj þ 1 ¼ ~xj þ Dt ~_xj þ
1
2  b


Dt2~€xj þ bDt2~€xj þ 1
ð3:209Þ
~€xj þ 1 ¼ D1Pj þ 1
D ¼ M þ cDt C þ bDt2K
Pj þ 1 ¼ Fj þ 1  C ~_xj þ ð1  cÞDt ~€xj
	

 K ~xj þ Dt ~_xj þ
1
2  b


Dt2~€xj


ð3:210Þ
Note that D does not depend on the time step j and so its inverse only needs to be
computed once in the beginning.
3.8
Newmark Scheme
125

Newmark scheme for MDOF Eq. (3.208)
Given xð0Þ ¼ u, _xð0Þ ¼ v and fFj ¼ FðjDtÞ : j ¼ 0; 1; 2; . . .g
Step 1. Set ~x0 ¼ u, ~_x0 ¼ v and ~€x0 ¼ M1 F0  Cv  Ku
ð
Þ
Step 2. For j ¼ 0; 1; 2; . . .
2.1. Calculate ~€xj þ 1 from (3.210).
2.2. Calculate ~_xj þ 1 and ~xj þ 1 from (3.209).
End for j
3.8.4
Parameters and Numerical Stability
The numerical properties of the Newmark scheme depend on the choice of c and b.
The linear acceleration scheme corresponds to c ¼ 1=2 and b ¼ 1=6. Setting c ¼
1=2 and b ¼ 1=4 gives the ‘constant acceleration scheme’, which is exact if the
acceleration is piecewise constant. See Fig. 3.17a, for which rjðsÞ  1=2 in (3.203).
Setting c ¼ 1=2 and b ¼ 0 gives the central difference scheme; see Example 3.10
and Example 3.11. Using c [ 1=2 introduces artiﬁcial positive damping and
c\1=2 introduces artiﬁcial negative damping. In most applications, c ¼ 1=2 is
used, for which there is no artiﬁcial damping and the accuracy is second order.
Assume (conservatively) zero structural damping. Then the Newmark scheme is
stable if c  1=2 and b  c=2, regardless of Dt. If c  1=2 but b\c=2, it is only
conditionally stable, requiring Dt=T  1=2p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c=2  b
p
, where T is the shortest
period of the structure. The constant acceleration scheme is therefore uncondi-
tionally stable. The linear acceleration and central difference scheme are only
conditionally stable. Substituting c ¼ 1=2 and b ¼ 1=6 gives the stability criterion
Dt=T 
ﬃﬃﬃ
3
p
=p for the linear acceleration scheme. Substituting c ¼ 1=2 and b ¼ 0
gives Dt=T  1=p for the central difference scheme, which agrees with the ﬁnding
in Example 3.10. For structures with a large number of DOFs, the highest mode can
have a very short period. This can impose a requirement on Dt that is much more
stringent than is necessary for acceptable accuracy. In this sense the constant
acceleration scheme is a more robust choice. Figure 3.18 summarizes the stability
criteria for the Newmark scheme for zero damping.
Example 3.11 (Central difference as special case of Newmark scheme) This
example shows that setting c ¼ 1=2 and b ¼ 0 in the Newmark scheme gives the
central difference scheme in Example 3.10, which may not be obvious from the
recurrence equations. Substituting these values into (3.206) gives
~xj þ 1 ¼ ~xj þ Dt~_xj þ Dt2
2
~€xj
ð3:211Þ
126
3
Structural Dynamics and Modal Testing

~_xj þ 1 ¼ ~_xj þ Dt
2
~€xj þ Dt
2
~€xj þ 1
ð3:212Þ
For the central difference scheme, from the ﬁrst equation of (3.195),
~xj1 ¼ ~xj þ 1  2Dt~_xj. Substituting into the expression of ~€xj in (3.195) and solving
for ~xj þ 1 gives (3.211). On the other hand, replacing j by j þ 1 in (3.195) gives
~_xj þ 1 ¼ ~xj þ 2  ~xj
2Dt
ð3:213Þ
Replacing j by j þ 1 in (3.211), which has just been proven,
~xj þ 2 ¼ ~xj þ 1 þ Dt ~_xj þ 1 þ Dt2
2
~€xj þ 1 ¼ ~xj þ Dt ~_xj þ Dt2
2
~€xj
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
~xj þ 1
þ Dt ~_xj þ 1 þ Dt2
2
~€xj þ 1
ð3:214Þ
Substituting into (3.213) and solving for ~_xj þ 1 gives (3.212).
■
3.8.5
Derivation of Stability Criterion
The stability criterion for the Newmark scheme can be derived using the concepts in
Sect. 3.7.2. For simplicity, assume zero structural damping, which also leads to a
conservative criterion. Consider ﬁrst the SDOF equation in (3.199). For conve-
nience, rewrite it as
Fig. 3.18 Stability diagram of the Newmark scheme for zero damping
3.8
Newmark Scheme
127

€xðtÞ þ x2
1xðtÞ ¼ pðtÞ
ð3:215Þ
where x1 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
k=m
p
is the natural frequency (rad/s) and pðtÞ ¼ FðtÞ=m. Substitute
~€xj ¼ pj  x2
1~xj and ~€xj þ 1 ¼ pj þ 1  x2
1~xj þ 1 into (3.206). Writing in matrix form,
~xj þ 1
~_xj þ 1


|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
~yj þ 1
¼
a11
a12
a21
a22


|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
A
~xj
~_xj


|ﬄ{zﬄ}
~yj
þ
B
|{z}
22
irrelevant
pj
pj þ 1


|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
wj þ 1
ð3:216Þ
a11 ¼ 1 
a2
2ð1 þ ba2Þ
a12 ¼
Dt
1 þ ba2
a21 ¼ a2
Dt
ca2
2ð1 þ ba2Þ  1


a22 ¼ 1 
ca2
1 þ ba2
a ¼ x1Dt
ð3:217Þ
The eigenvalue of A satisﬁes the characteristic equation:
A  kI
j
j ¼ 0
) k2  bk þ c ¼ 0
ð3:218Þ
where
b ¼ a11 þ a22
¼ 2  a2ð1 þ 2cÞ
2ð1 þ ba2Þ
c ¼ a11a22  a12a21
¼ 1  a2ðc  1=2Þ
1 þ ba2
ð3:219Þ
Solving (3.218) gives two eigenvalues:
k1 ¼ 1
2 b þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
b2  4c
p


k2 ¼ 1
2 b 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
b2  4c
p


ð3:220Þ
There are two cases to consider:
(1) If b2  4c  0, the eigenvalues are complex conjugates (including the case of
repeated roots). Since their product is c, and c  b2=4  0, the modulus of the
eigenvalues are both equal to
ﬃﬃﬃc
p . Stability then requires c  1.
(2) If b2  4c [ 0, the eigenvalues are real and distinct. If b  0 then k1 [ 0 and
k1
j
j  k2
j
j. Stability then requires k1  1. Solving this inequality gives
b  c  1. Similarly, if b\0 then k2\0 and k1
j
j\ k2
j
j. Stability then requires
k2   1. Solving this inequality gives b þ c   1.
Combining the above considerations, the value of ðb; cÞ when the scheme is stable
is shaded in Fig. 3.19. The eigenvalues are complex conjugates, real-repeated or
real-distinct when ðb; cÞ is above, on, or below the parabola, respectively.
128
3
Structural Dynamics and Modal Testing

The ﬁgure shows that the scheme is stable if and only if all the following
conditions are met:
c  1
b  c  1
b þ c   1
ð3:221Þ
Using (3.219),
c ¼ 1  a2ðc  1=2Þ
1 þ ba2
b  c ¼ 1 
a2
1 þ ba2
b þ c ¼ 3 
2ca2
1 þ ba2 ð3:222Þ
The ﬁrst inequality in (3.221) is equivalent to c  1=2. The second is always
satisﬁed. The third is equivalent to x2
1Dt2ðc=2  bÞ  1. If b  c=2 it is always
satisﬁed and hence imposes no constraint on Dt. Otherwise ðb\c=2Þ it is equiv-
alent to x1Dt  1=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c=2  b
p
, i.e., Dt=T  1=2p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c=2  b
p
where T ¼ 2p=x1 is the
natural period.
MDOF Equation
The proof for the case of MDOF equation has a similar logical structure, which is
now applied to the modal response. In this case, write
~xj ¼
X
n
i¼1
wi~gij
~_xj ¼
X
n
i¼1
wi~_gij
~€xj ¼
X
n
i¼1
wi~€gij
ð3:223Þ
where fwign
i¼1 are the mode shapes (with unit norm) corresponding to natural
frequencies fxign
i¼1, satisfying Kwi ¼ x2
i Mwi; f~gij; ~_gij; ~€gijg are the modal coun-
terparts of the computed response. This is always possible since fwign
i¼1 is a basis
in the n-dimensional space. Substituting (3.223) into (3.208) to (3.210),
pre-multiplying by wT
r ðr ¼ 1; . . .; nÞ and following similar steps in the SDOF case
shows that the state vector y ¼ ½~grj; ~_grjT satisﬁes a similar equation as (3.216), but
now x1 is replaced by xr. A similar stability criterion then follows by noting that
Fig. 3.19 Stability region (shaded and including boundaries)
3.8
Newmark Scheme
129

f~xj; ~_xjg are bounded if and only if f~grj; ~_grjg are bounded for all modes r ¼ 1; . . .; n.
When c  1=2 and b  c=2, all modes are stable and so is the scheme. When
c  1=2 but b\c=2, numerical stability requires xrDt  1=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c=2  b
p
for all
modes. The mode with the highest frequency then governs the upper bound of Dt.
As a remark, (3.221) gives the key inequalities governing stability. In the present
case it has been reasoned based on the speciﬁc properties of quadratic equation. The
inequalities could have been arrived more directly via the Ruth-Hurwitz or Bistritz
test (Bistritz 1984, 2002), which are also applicable to polynomial equations of
general order.
References
Adhikari S (2014) Structural damping analysis with generalized damping models. Wiley, New
York
Au SK, Ni YC, Zhang FL et al (2012) Full scale dynamic testing and modal identiﬁcation of a
coupled ﬂoor slab system. Eng Struct 37:167–178
Bachmann H et al (1995) Vibration problems in structures—practical guidelines. Birkhauser
Verlag, Basel
Beards CF (1996) Structural vibrations: analysis and damping. Halsted Press, New York
Bathe KJ (1982) Finite element procedures in engineering analysis. Prentice-Hall, Englewood
Cliffs
Bistritz Y (1984) Zero location with respect to the unit circle of discrete-time linear system
polynomials. Proc IEEE 72(9):1131–1142
Bistritz Y (2002) Zero location of polynomials with respect to the unit circle unhampered by
nonessential singularities. IEEE Trans Circ Syst 49(3):305–314
Caughey TK, O’Kelly MEJ (1965) Classical normal modes in damped linear dynamic systems.
J Appl Mech 32:583–588
Clough RW, Penzien J (1993) Dynamics of structures. McGraw Hill, New York
EC1 (2005) Eurocode 1: actions on structures, part 1–4, general actions: wind actions. EN
1991-1-4:2005 + A1: 2010. European Committee for Standardization, Brussels
ESDU 83009 (2012) Damping of structures, Part 1: tall buildings. Engineering Sciences Data Unit,
London
Ewins DJ (2000) Modal testing: theory and practice. Research Studies Press, PA
Fukuwa N, Nishizaka R, Yagi S et al (1996) Field measurement of damping and natural frequency
of an actual steel-framed building over a wide range of amplitudes. J Wind Eng Ind Aerodyn
59:325–347
Hildebrand FB (1987) Introduction to numerical analysis, 2nd edn. Dover, New York
Hughes TJR (1987) The ﬁnite element method: linear static and dynamic ﬁnite element analysis.
Prentice-Hall, Englewood Cliffs
Hulbert GM (2004) Computational structural dynamics. In: Encyclopedia of computational
mechanics. Wiley, New York
ISO 4354 (2009) Wind actions on structures. ISO 4354:2009. International Organization for
Standards, Switzerland
ISO 10137 (2007) Bases for design of structures—serviceability of buildings and walkways
against vibrations. ISO 10137:2007. International Organization for Standards, Switzerland
Jeary AP (1986) Damping in tall buildings: a mechanism and a predictor. Earthq Eng Struct Dyn
14:733–750
Jeary AP (1997) Damping in structures. Wind Eng Indus Aerodyn 72:345–355
130
3
Structural Dynamics and Modal Testing

Kijewski-Correa T, Kilpatrick J, Kareem A et al (2006) Validating wind-induced response of tall
buildings: synopsis of the Chicago full-scale monitoring program. J Struct Eng 132(10):1509–
1523
Kwok KCS, Burtn M, Abdelrazaq, A (2015) Wind-induced motion of tall buildings: designing for
habitability. American Society of Civil Engineers, Reston
Li QS, Liu DK, Fang JQ et al (2000) Damping in buildings: its neural network model and AR
model. Eng Struct 22:1216–1223
Maia N, Silva J (1997) Theoretical and experimental modal analysis. Research Studies Press Ltd,
Baldock
McConnell K (1995) Vibration testing—theory and practice. Wiley, New York
Meirovitch L (1986) Elements of vibration analysis. McGraw-Hill, London
Satake N, Suda K, Arakawa T et al (2003) Damping evaluation using full-Scale data of buildings
in Japan. J Struct Eng 129(4):470–477
Subbaraj K, Dokainish MA (1989a) A survey of direct time-integration methods in computational
structural dynamics–I: explicit methods. Comput Struct 32(6):1371–1386
Subbaraj K, Dokainish MA (1989b) A survey of direct time-integration methods in computational
structural dynamics–II: implicit methods. Comput Struct 32(6):1387–1410
Tamura Y, Kareem A (eds) (2013) Advanced structural wind engineering. Springer, Japan
References
131

Chapter 4
Spectral Analysis of Stationary Stochastic
Process
Abstract This chapter analyzes an unknown time series modeled by a stationary
stochastic process. It is a probabilistic version of Chap. 2, and allows one to model
the statistical properties of ambient vibration data. Assuming pre-requisites in
undergraduate probability and statistics, the chapter introduces the characterization
of a stochastic process by correlation function in the time domain and power
spectral density in the frequency domain. Estimators based on a sample time history
with ﬁnite duration and ﬁnite sampling rate are introduced; and their statistical
properties are discussed. For long data duration, the statistical properties have
remarkably simple form, which is conventionally adopted in applications and is one
of the pillars in Bayesian operational modal analysis in the frequency domain.
Keywords Stationary stochastic process  Fourier analysis  Correlation function 
Power spectral density
In Chap. 2 we studied the frequency characteristics of a deterministic process,
where analysis results were expressed directly in terms of its values. This chapter
has a similar objective but now the process is ‘unknown’ and modeled as a
‘stochastic process’ using probability theory. The analysis results are deﬁned and
expressed in terms of statistical properties of the process.
A ‘stochastic process’ is a sequence of random variables, conventionally indexed
w.r.t. time. Just as a random variable is characterized by its probability distribution,
a stochastic process is characterized by the joint distribution among its values at
different time instants. The distribution can be complicated for a general process.
Modeling assumptions allow it to be simpliﬁed, often in terms of parameters in the
mechanism driving the process or statistical parameters that can be estimated from
observations. A ‘stationary stochastic process’ refers to one whose statistical
properties do not change with time. It is much easier to analyze or model than a
non-stationary process.
Supported by theoretical arguments and ﬁeld observations, over suitable time
scales, it is reasonable to model ambient vibration data as a stochastic stationary
process. Whether this is justiﬁed depends on factors such as the time scale under
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_4
133

question and the nature of excitations that drive the process. During a typhoon that
lasts for a day, for example, the vibration of a building is non-stationary over the
whole event but it may be considered stationary within a thirty-minute time win-
dow. The free vibration response of a footbridge shortly after the footfall of a single
pedestrian is clearly non-stationary but the response in a time span of ten minutes
over which many pedestrians walk by in an unorganized fashion may be considered
stationary for vibration assessment purposes.
We start with continuous-time process and characterize it in the time domain
through the ‘correlation function’ and in the frequency domain through the ‘power
spectral density’ (PSD). Estimators for these statistics based on a sample history of
the process are then introduced. The ‘Wiener-Khinchin formula’ shows that the
correlation function and PSD are Fourier Transform pairs. The remaining sections
focus on discrete-time sample process, which is what one deals with in digital
computations. The Fast Fourier Transform (FFT) provides an efﬁcient tool. The
distribution of the scaled FFT of stochastic stationary data for long duration is
pivotal to Bayesian operational modal analysis (OMA) theory.
This chapter assumes acquaintance with basic probability concepts, e.g., random
variable, probability distribution, expectation, variance and correlation. One may
refer to textbooks, e.g., Ross (2011), Ang and Tang (2007). We omit mathematical
details when they are too technical or save to ignore in applications. For example,
details on the existence of derivatives/integrals or exchanging their order are
omitted. They are legitimate for ambient vibration data. Stochastic process is a
broad subject with applications in many disciplines. See, e.g., Cramer and
Leadbetter (1962), Papoulis (1991) and Ross (1996) for general theory. Further
theoretical details on statistical spectral analysis can be found in Brillinger (1981),
Bendat and Piersol (1993) and Kay (1993).
4.1
Correlation Function
Let xðtÞ ¼ x1ðtÞ; . . .; xnðtÞ
½
T be a real-valued stochastic vector process w.r.t. time
t. Without loss of generality, it is assumed to have zero mean:
E½xðtÞ ¼ 0
ð4:1Þ
where E½ denotes the ‘expectation’ (ensemble average). In the time domain, one
basic statistical characterization is the ‘correlation function’:
Rðs; tÞ
nn
¼ E½ xðsÞ
|{z}
n1
xðtÞT
|ﬄ{zﬄ}
1n

ð4:2Þ
This is a matrix function of two arguments, s and t. The ði; jÞ-entry of Rðs; tÞ is
E xiðsÞxjðtÞ


. The ‘covariance matrix’ of xðtÞ is
134
4
Spectral Analysis of Stationary Stochastic Process

Rðt; tÞ ¼ E xðtÞxðtÞT


ð4:3Þ
This is a n  n real symmetric and positive semi-deﬁnite matrix.
A stochastic process is ‘strict-sense stationary’ if the joint distribution of its
values at different time instants is invariant to a time shift. That is, for any time lag
s, the joint distribution of xðt þ sÞ and xðtÞ does not depend on the time shift t. In
practice this can be difﬁcult to model or verify. ‘Wide-sense stationary’ process is a
more common notion, where only the correlation function is required to be
invariant to a time shift. In this case, the correlation function depends only on the
time lag s:
RðsÞ ¼ E xðt þ sÞxðtÞT


ð4:4Þ
Generally, t þ s and t in the expectation cannot be swapped. That is,
RðsÞ 6¼ E xðtÞxðt þ sÞT


. The correlation function need not be a symmetric matrix.
Instead, for a stationary process it has the ‘transpose mirror property’ that
RðsÞ ¼ RðsÞT
ð4:5Þ
This holds because
RðsÞ ¼ E xðt  sÞxðtÞT


¼
replace t
by t þ s
E xðtÞxðt þ sÞT


¼ E xðt þ sÞxðtÞT


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
RðsÞ
T
ð4:6Þ
The second equality follows because the expectation is invariant to t (stationary).
Cross Correlation Function
For two scalar processes xðtÞ and yðtÞ, their ‘cross correlation function’ is deﬁned as
Rxyðs; tÞ ¼ E xðsÞyðtÞ
½

ð4:7Þ
If they are ‘jointly stationary’, their cross correlation function only depends on time
lag but not on time shift, giving
RxyðsÞ ¼ E xðt þ sÞyðtÞ
½

ð4:8Þ
Note that RxyðsÞ ¼ RyxðsÞ. The cross correlation function is often used for
studying the statistical relationship between two processes in the time domain. By
deﬁnition, the ði; jÞ-entry i 6¼ j
ð
Þ of the correlation function of a vector process is the
cross correlation function between its ith and jth component. Although one can
deﬁne the cross correlation function between two vector processes in an analogous
manner, this will not be pursued here because in principle one can combine two
vector processes into a single one comprising the two and study the correlation
function of the resulting vector process.
4.1
Correlation Function
135

4.2
Power Spectral Density
In the frequency domain, a stationary vector process can be characterized by the
‘power spectral density’ (PSD) matrix:
SðxÞ
nn
¼ lim
T!1 E½ XTðxÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
n1
XTðxÞ
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
1n

ð4:9Þ
where ‘*’ denotes the conjugate transpose; and
XTðxÞ ¼ 1ﬃﬃﬃﬃ
T
p
ZT
0
xðtÞeixtdt
ð4:10Þ
is the (ﬁnite-time) Fourier Transform (FT) of xðtÞ scaled by 1=
ﬃﬃﬃﬃ
T
p
. Taking con-
jugate transpose on (4.9) shows that SðxÞ is ‘Hermitian’:
SðxÞ ¼ SðxÞ
ð4:11Þ
It also has the transpose mirror property that
SðxÞ ¼ SðxÞT
ð4:12Þ
This follows because XTðxÞ ¼ XTðxÞ and so
SðxÞ ¼ lim
T!1 E½ XTðxÞ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
XTðxÞ
XTðxÞ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
XTðxÞT
 ¼ lim
T!1 E XTðxÞXTðxÞ
½

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
SðxÞ
T
ð4:13Þ
Finally, SðxÞ is positive semi-deﬁnite, because for any ﬁxed n  1 complex
vector u,
uSðxÞu ¼ lim
T!1 E½ u
1n XTðxÞ
n1
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
Z
XTðxÞ
1n
u
n1
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
Z
 ¼ lim
T!1 E½ Z
j j2  0
ð4:14Þ
Operationally, it is more convenient to interpret the deﬁnition of SðxÞ as
SðxÞ ¼ E½XðxÞXðxÞ
ð4:15Þ
where
136
4
Spectral Analysis of Stationary Stochastic Process

XðxÞ ¼ lim
T!1 XTðxÞ
ð4:16Þ
denotes a random vector with the same distribution as XTðxÞ in the limit T ! 1.
This deﬁnition is only ‘symbolic’ because the limit of a random vector has yet to be
deﬁned (omitted here). It appears as resulting from carrying the limit in (4.9) inside
the expectation and then the product. Omitting technical details, the interpretation in
(4.15) is legitimate for ‘well-behaved’ processes, justiﬁed for OMA. It provides
convenience for deriving the relationship between the PSDs of the input and output
of a linear system. When there is no confusion, XðxÞ is also referred as the scaled
FT of xðtÞ for convenience.
Cross PSD and Coherence
For two jointly stationary scalar processes xðtÞ and yðtÞ with corresponding scaled
FTs XðxÞ and YðxÞ, their ‘cross power spectral density’ (cross PSD) is deﬁned as
SxyðxÞ ¼ E½XðxÞYðxÞ
ð4:17Þ
Note that SxyðxÞ ¼ SyxðxÞ. By deﬁnition, the ði; jÞ-entry i 6¼ j
ð
Þ of the PSD matrix
of a vector process is the cross PSD between its ith and jth component.
Analogous to cross correlation function, the cross PSD is often used for studying
the statistical relationship between two processes in the frequency domain. When
t is measured in seconds, the unit of the cross PSD between xðtÞ and yðtÞ is equal to
the product of their units divided by Hz. The ‘coherence’ is a dimensionless
quantity that is conveniently used in analysis:
vxyðxÞ ¼
SxyðxÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
SxxðxÞSyyðxÞ
p
ð4:18Þ
where (consistent with notation) SxxðxÞ and SyyðxÞ are respectively the PSD of xðtÞ
and yðtÞ. Note that coherence is complex-valued and its modulus is bounded above
by 1.
4.3
Fourier Series, Fourier Transform and PSD
The deﬁnition of PSD may look peculiar but it turns out to offer a very powerful
tool for analyzing and modeling stationary stochastic processes. The following
motivates the deﬁnition, through a line of query on the legitimate quantity for
characterizing processes of different nature.
Consider ﬁrst a periodic process xðtÞ ¼ ½x1ðtÞ; . . .; xnðtÞT with period T. Then it
can be written as a Fourier series (Sect. 2.1.1):
4.2
Power Spectral Density
137

xðtÞ ¼
X
1
k¼1
ckeixkt
ð4:19Þ
where
ck ¼ 1
T
ZT
0
xðtÞeixktdt
ð4:20Þ
is the (complex) Fourier series (FS) coefﬁcient associated with frequency
xk ¼ 2pk=T. A periodic process is characterized by its FS coefﬁcients at a discrete
set of frequencies.
Next, suppose xðtÞ is a non-periodic process with ﬁnite energy, i.e.,
R 1
0 xiðtÞ2dt\1 i ¼ 1; . . .; n
ð
Þ. Conceptually, it can be considered periodic with an
inﬁnitely long period. Taking T ! 1, the sum in (4.19) becomes an integral,
giving (Sect. 2.2.1)
xðtÞ ¼ 1
2p
Z1
1
XðxÞeixtdx
ð4:21Þ
where
XðxÞ ¼
Z1
0
xðtÞeixtdt
ð4:22Þ
is the FT of xðtÞ. A non-periodic process with ﬁnite energy is characterized by its
FT over a continuum of frequencies.
Consider now when xðtÞ is a zero-mean stationary stochastic process, the focus
in this chapter. It does not have a FS because it is not periodic. Its FT does not exist
either because it extends over ð0; 1Þ without decaying;
R T
0 xiðtÞ2dt grows without
bound as T increases. As the value of xðtÞ at different ts are not fully correlated, the
integral
R T
0 xðtÞeixtdt is a random quantity whose magnitude grows with
ﬃﬃﬃﬃ
T
p
(not
T). Both the FS coefﬁcient and FT involve this integral but they have different
multipliers, 1=T and 1, respectively. As T ! 1, the FS coefﬁcient is O 1=
ﬃﬃﬃﬃ
T
p


and it tends to zero. The FT is O
ﬃﬃﬃﬃ
T
p


and it tends to inﬁnity. Neither one is a
legitimate quantity to characterize the process in the frequency domain. The proper
one has the multiplier 1=
ﬃﬃﬃﬃ
T
p
, i.e., XTðxÞ ¼ T1=2 R T
0 xðtÞeixtdt, so that it has
converging and bounded statistics. As XTðxÞ is a random complex vector with zero
mean, the PSD in (4.9) is simply its covariance matrix (generalized for complex
vectors) in the limit.
138
4
Spectral Analysis of Stationary Stochastic Process

Section 4.4.3 later shows that the correlation function and PSD are Fourier
Transform pairs, therefore carrying the same piece of information in the time and
frequency domain, respectively. Figure 4.1 summarizes the characterization of
different processes.
Proper Scaling
The above reveals the importance of using a transform that scales properly with the
data duration, depending on the nature of time series. Failing to do so gives trivially
diminishing or diverging results. Table 4.1 shows what happens when a time series
of a particular nature is analyzed by different tools as the data duration T increases.
For example, calculating the FS coefﬁcients of the sample history of a stationary
process, one will ﬁnd that the magnitude of the FS coefﬁcients reduces systemat-
ically with the data duration, in an inverse square root manner.
Table 4.1 Analyzing time series of duration T with different tools
Nature of xðtÞ
Analysis tool
Fourier series ck
PSD SðxÞ
Fourier Transform XðxÞ
Periodic
Proper and converging
Oð
ﬃﬃﬃﬃ
T
p
Þ; ! 1
OðTÞ; ! 1
Stochastic stationary
Oð1=
ﬃﬃﬃﬃ
T
p
Þ; ! 0
Proper and converging
Oð
ﬃﬃﬃﬃ
T
p
Þ; ! 1
Non-periodic,
ﬁnite energy
Oð1=TÞ; ! 0
Oð1=
ﬃﬃﬃﬃ
T
p
Þ; ! 0
Proper and converging
∑
∞
−∞
=
=
k
t
k
ke
c
t
x
ω
i
)
(
∫
−
=
T
t
k
k
dt
e
t
x
T
c
0
)
(
1
ω
i
|
| k
c
k
ω
T
T
/
2π
ω
∫
∞
−
= 0
)
(
)
(
dt
e
t
x
X
t
ω
ω
i
|)
(
|
ω
X
∫
∞
∞
−
=
ω
ω
π
ω d
e
X
t
x
t
i
)
(
2
1
)
(
)
(t
x
ω
ω
|)
(
|
ω
T
X
]
)
(
)
(
[
lim
)
(
*
ω
ω
ω
T
T
T
X
X
E
S
∞
→
=
)]
(
)
(
[
)
(
t
x
t
x
E
R
τ
τ
+
=
∫
−
=
T
t
T
dt
e
t
x
T
X
0
)
(
1
)
(
ω
ω
i
Periodic
Non-
Periodic
Stochastic
stationary
Pedestrian load, 
machine/rotor vibration
Earthquake motion,
blast pressure
Wind load, microtremor,
ambient vibration
Nature
Time domain
Frequency domain
Example/remark
∫
∫
∞
∞
−
∞
∞
−
−
=
=
ω
ω
π
τ
τ
τ
ω
ωτ
ωτ
d
e
S
R
d
e
R
S
i
i
)
(
2
1
)
(
)
(
)
(
Wiener-Khinchin formula
Fourier
Series
Fourier
Transform
PSD
)
(ω
S
Fig. 4.1 Characterization of different (scalar) processes (showing positive frequencies only)
4.3
Fourier Series, Fourier Transform and PSD
139

4.4
Continuous-Time Sample Process
The correlation function and PSD are theoretical characterization of a stationary
process in the time and frequency domain, respectively. In applications we need to
work with a given time history which is modeled as a stationary process. For
example, we may use the time history to estimate the statistical properties of the
process. In analysis we may generate random samples of the process and study its
effects on system response. In this section, we consider estimating the correlation
function and PSD using a continuous-time sample history. The next section extends
to discrete-time sample history, the typical scenario in modern applications with
digital computers.
4.4.1
Sample Correlation Function
The correlation function RðsÞ in (4.4) was deﬁned in terms of the expectation of
xðt þ sÞxðtÞT for a given t. Estimating it in this spirit requires multiple samples of
xðtÞ for a given t. This could imply a very tedious data collection process; imagine
measuring the temperature at the same place and same time everyday. For a sta-
tionary process, it is possible to estimate using only a single rather than multiple
time histories.
Deﬁnition from First Principle
Using a sample history
xðtÞ : 0  t  T
f
g of duration T, the correlation function at
time lag s
sj j\T
ð
Þ can be estimated by the ‘sample correlation function’, deﬁned as
^RTðsÞ ¼
1
T
R Ts
0
xðt þ sÞxðtÞTdt
0  s\T
1
T
R T
jsj xðt þ sÞxðtÞTdt
T\s\0
(
ð4:23Þ
This deﬁnition is motivated from E xðt þ sÞxðtÞT


and makes use of its invariance
w.r.t. t for a stationary process. For a given s, the term xðt þ sÞxðtÞT is averaged
over different t s. This works when the process is stationary and ‘ergodic’, i.e., the
(temporal) average over time of a single sample history converges to the (ensemble)
average over different histories as the data duration increases. Ergodicity is an
advanced topic which will not be further discussed here. Interested readers may
refer to Yaglom (1989) and Papoulis (1991). Typically, ambient vibration data are
assumed to be ergodic.
Some peculiar features are in place in the deﬁnition (4.23) for technical reasons.
Although the integral covers a duration of T  sj j, the averaging factor is T rather
than T  sj j. This is so that ^RTðsÞ satisﬁes the Wiener-Khinchin formula (see
Sect. 4.4.3 later), at the expense of a bias for ﬁnite T. The deﬁnition differs in the
140
4
Spectral Analysis of Stationary Stochastic Process

integration limit for s [ 0 or s\0, so that only the value of xðtÞ within ½0; T is
involved.
Transpose Mirror Property
The value of ^RTðsÞ for negative time lag can be produced from that for positive lag
because
^RTðsÞ ¼ ^RTðsÞT
 T\s\T
ð4:24Þ
This is analogous to (4.5). It can be veriﬁed by noting that, for s [ 0, changing
integration variable from t to s ¼ t  s,
^RTð s
|{z}
\0
Þ ¼ 1
T
ZT
s
xðt  sÞxðtÞTdt
¼ 1
T
ZTs
0
xðsÞxðs þ sÞTds ¼
1
T
ZTs
0
xðs þ sÞxðsÞTds
2
4
3
5
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^RTðsÞ
T
ð4:25Þ
The case for s\0 is similar. Equation (4.24) implies that only the values for
non-negative time lag are needed; the ones for negative time lag are redundant.
Deﬁnition Based on Windowed Process
The sample correlation function in (4.23) can be written in a compact manner as
^RTðsÞ ¼ 1
T
Z1
1
xTðt þ sÞxTðtÞTdt
ð4:26Þ
where
xTðtÞ ¼
xðtÞ
0  t  T
0
otherwise

ð4:27Þ
is a ‘windowed’ version of xðtÞ. Check that this gives the same expression in (4.23)
for T  s  T; and it is zero otherwise:
^RTðsÞ ¼ 0
sj j [ T
ð4:28Þ
Essentially, the integrand in (4.26) is zero whenever t þ s or t is outside ½0; T. This
allows ^RTðsÞ to be represented by a single expression regardless of the sign of s.
The integration domain can also extend over ð1; 1Þ independent of T or s.
Equation (4.28) extends the deﬁnition of ^RTðsÞ for sj j [ T. These features provide
some convenience in analysis.
4.4
Continuous-Time Sample Process
141

4.4.2
Sample Power Spectral Density
Using the sample history
xðtÞ : 0  t  T
f
g, the ‘sample PSD’ is deﬁned as
^STðxÞ ¼ XTðxÞXTðxÞ
ð4:29Þ
where XTðxÞ ¼ T1=2 R T
0 xðtÞeixtdt as in (4.10). Analogous to SðxÞ, it can be
veriﬁed that ^STðxÞ is Hermitian and it has the transpose mirror property:
^STðxÞ ¼ ^STðxÞ
^STðxÞ ¼ ^STðxÞT
ð4:30Þ
It is also positive semi-deﬁnite.
4.4.3
Wiener-Khinchin Formula
One important result in the theory of stationary process is that the correlation
function and PSD are Fourier Transform pairs:
SðxÞ ¼
Z1
1
RðsÞeixsds
ð4:31Þ
RðsÞ ¼ 1
2p
Z1
1
SðxÞeixsdx
ð4:32Þ
These
are
known
as
the
‘Wiener-Khinchin
formula’.
For
the
sample
counterparts,
^STðxÞ ¼
Z1
1
^RTðsÞeixsds
ð4:33Þ
^RTðsÞ ¼ 1
2p
Z1
1
^STðxÞeixsdx
ð4:34Þ
with the convention that ^RTðsÞ ¼ 0 for sj j [ T. As the correlation function and
PSD can be converted interchangeably, they are equivalent characterization of a
stationary process.
142
4
Spectral Analysis of Stationary Stochastic Process

Proof of (4.31) to (4.34) (Wiener-Khinchin Formula, Continuous-Time)
Here we show (4.33), i.e., ^STðxÞ is the FT of ^RTðsÞ. Then ^RTðsÞ is the inverse FT
of ^ST, i.e., (4.34). Taking limit T ! 1 of (4.33) and (4.34) gives (4.31) and (4.32),
respectively.
Starting with the RHS of (4.33) and using (4.26),
Z1
1
^RTðsÞeixsds ¼ 1
T
Z1
1
Z1
1
xTðt þ sÞxTðtÞTeixsdtds
ð4:35Þ
Let s ¼ t þ s and change integration variable from ðs; tÞ to ðs; tÞ. The determinant of
Jacobian of this transformation is 1. The new integration domain is still
ð1; 1Þ  ð1; 1Þ. Then
Z1
1
^RTðsÞeixsds¼ 1
T
Z1
1
Z1
1
xTðsÞxTðtÞTeixðstÞdtds
¼ 1
T
Z1
1
Z1
1
xTðsÞeixsxTðtÞTeixtdsdt
¼
1ﬃﬃﬃﬃ
T
p
Z 1
1
xTðsÞeixsds
	

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
XTðxÞ
1ﬃﬃﬃﬃ
T
p
Z 1
1
xTðtÞeixtdt
	


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
XTðxÞ
¼ ^STðxÞ
ð4:36Þ
■
4.4.4
Parseval Equality
For a stationary process, the Parseval equality explains the variance as a sum of
PSD contributions from different frequencies. It results directly from the
Wiener-Khinchin formula at zero time lag. Setting s ¼ 0 in (4.32) gives
E xðtÞxðtÞT


|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
Rð0Þ
¼ 1
2p
Z1
1
SðxÞdx
ð4:37Þ
For the sample process
xðtÞ : 0  t  T
f
g, the Parseval equality explains the
mean square value as a sum of sample PSD contributions. Setting s ¼ 0 in (4.34)
gives
4.4
Continuous-Time Sample Process
143

1
T
ZT
0
xðtÞxðtÞTdt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^RTð0Þ
¼ 1
2p
Z1
1
^STðxÞdx
ð4:38Þ
For a scalar process, the Parseval equality implies that the area under the PSD in
the frequency domain (w.r.t. x=2p, in Hz) gives the variance. This allows one to
calculate the variance contributed only by the activities of interest. For example, to
calculate the variance due to a particular vibration mode, one integrates the PSD
over the resonance band of the mode only.
4.4.5
White Noise
Consider a scalar process wðtÞ with a constant PSD given by
SðxÞ 	 Sw
ð4:39Þ
Using the Wiener-Khinchin formula, the correlation function is
RðsÞ ¼ 1
2p
Z1
1
Sweixsdx ¼ SwdðsÞ
ð4:40Þ
where dðsÞ ¼ ð2pÞ1 R 1
1 eixsdx is the Dirac Delta function (Sect. 2.2.3). The
process wðtÞ is called ‘white noise’. It is used widely in stochastic modeling and
analysis because of the simple constant PSD. It does not exist in reality, however.
Some unrealistic features are: the variance var½wðtÞ ¼ Rð0Þ is unbounded; the
values of wðtÞ at any two distinct time instants are uncorrelated, no matter how
close they are. The PSD SðxÞ is non-zero for all frequencies, no matter how high.
These features may not affect the result of a particular analysis but should be kept in
mind to ascertain their effects. In reality, the PSD of a physical process decays to
zero for sufﬁciently high frequency. Conceptually, a process may be modeled as a
‘band-limited white noise’. That is, the PSD is constant only within some frequency
band of relevance.
Example 4.1 (First order process driven by white noise) Consider a scalar
process xðtÞ governed by
_xðtÞ þ axðtÞ ¼ wðtÞ
ð4:41Þ
where a [ 0 and wðtÞ is white noise with PSD Sw. Here we determine the corre-
lation function and PSD of xðtÞ using their original deﬁnitions. After that we verify
the Wiener-Khinchin formula and Parseval equality.
144
4
Spectral Analysis of Stationary Stochastic Process

Correlation Function from Deﬁnition
The correlation function RðsÞ ¼ E½xðt þ sÞxðtÞ can be obtained by a standard
procedure in stochastic dynamics as follow. See Sect. 5.4 for a general treatment.
For s [ 0, write (4.41) at t þ s and multiply by xðtÞ. Taking expectation,
E _xðt þ sÞxðtÞ
½

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
_RðsÞ
þ a E xðt þ sÞxðtÞ
½

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
RðsÞ
¼ E wðt þ sÞxðtÞ
½

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
0
ð4:42Þ
On the RHS above, E wðt þ sÞxðtÞ
½
 ¼ 0 because xðtÞ depends only on the past
wðsÞ : s  t
f
g, which is independent of the future wðt þ sÞ for s [ 0 (white noise).
Solving _RðsÞ þ aRðsÞ ¼ 0 yields
RðsÞ ¼ Rð0Þeas
s [ 0
ð4:43Þ
It remains to determine Rð0Þ. Multiplying (4.41) by xðtÞ and taking expectation,
E _xðtÞxðtÞ
½
 þ aE xðtÞ2
h
i
¼ E wðtÞxðtÞ
½

ð4:44Þ
Note that E xðtÞ2
h
i
¼ Rð0Þ and
E _xðtÞxðtÞ
½
 ¼ E
d
dt
xðtÞ2
2
"
#
(
)
¼ d
dt E xðtÞ2
2
"
#
¼ 0
ð4:45Þ
since we are considering xðtÞ to be in a stationary state. Substituting into (4.44)
gives
Rð0Þ ¼ 1
a E wðtÞxðtÞ
½

ð4:46Þ
To evaluate E½wðtÞxðtÞ, express xðtÞ in terms of wðtÞ as the Duhamel’s integral
solution to (4.41) (Sect. 3.1.12):
xðtÞ ¼ eatxð0Þ þ
Z t
0
eaðtsÞwðsÞds
ð4:47Þ
Multiplying by wðtÞ and taking expectation,
E wðtÞxðtÞ
½
 ¼ eatxð0Þ E½wðtÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
0
þ
Z t
0
eaðtsÞ E wðtÞwðsÞ
½

|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
SwdðtsÞ
ds ¼ Sw
2
ð4:48Þ
4.4
Continuous-Time Sample Process
145

where the factor 1=2 results because the integral only covers the right half of the
Delta function. Substituting into (4.46) gives Rð0Þ ¼ Sw=2a. Equation (4.43) then
gives RðsÞ ¼ Sweas=2a for s [ 0. Since RðsÞ ¼ RðsÞ for a scalar process, we
conclude
RðsÞ ¼ Sw
2a ea sj j
 1\s\1
ð4:49Þ
PSD from Deﬁnition
The PSD of xðtÞ is given by SðxÞ ¼ E XðxÞXðxÞ
½
 where XðxÞ is the scaled FT of
xðtÞ. Taking scaled FT on (4.41) gives ixXðxÞ þ aXðxÞ ¼ WðxÞ, where WðxÞ is
the scaled FT of wðtÞ. Rearranging gives XðxÞ ¼ WðxÞ=ða þ ixÞ. Thus,
SðxÞ ¼ E XðxÞXðxÞ
½
 ¼ E WðxÞWðxÞ
½

ða þ ixÞða  ixÞ ¼
Sw
a2 þ x2
ð4:50Þ
Figure 4.2 shows a schematic plot of the correlation function and PSD of xðtÞ.
Wiener-Khinchin Formula
Using (4.49),
Z1
1
RðsÞeixsds ¼
Z1
1
Sw
2a eajsjeixsds
ð4:51Þ
Separate the integral into two, one for s  0 and the other for s\0. For the latter,
change integration variable from s to s ¼ s. This gives
Z1
1
RðsÞeixsds¼
Z1
0
Sw
2a easeixsds þ
Z1
0
Sw
2a easeixsds
¼ Sw
2a
1
a þ ix þ
1
a  ix
	

¼
Sw
a2 þ x2
ð4:52Þ
This is the same expression in (4.50), verifying the Wiener-Khinchin formula.
)
(ω
S
2
a
Sw
0
τ
)
(τ
R
a
Sw
2
0
ω
Fig. 4.2 Correlation function R(s) and PSD S(x) of a ﬁrst order process driven by white noise
146
4
Spectral Analysis of Stationary Stochastic Process

Parseval Equality
Using (4.50),
1
2p
Z1
1
SðxÞdx ¼ 1
2p
Z1
1
Sw
a2 þ x2 dx ¼ Sw
2a ¼ Rð0Þ
ð4:53Þ
where the integral can be obtained by letting x ¼ a tan / and changing integration
variable from x to /. This veriﬁes the Parseval equality.
■
4.5
Discrete-Time Sample Process
When a continuous-time process is recorded on a digital computer, it is sampled in
discrete time. The estimators for the correlation function and PSD based on such
sample history are discussed in this section. This connects the theoretical properties
of the process to its sample counterparts that can be calculated and analyzed on a
digital computer.
As before, let xðtÞ be a stationary process with zero mean, correlation function
RðsÞ and PSD SðxÞ. The discrete-time sample process is the sequence
xj ¼ xðjDtÞ

N1
j¼0 where Dt (s) is the sampling interval.
4.5.1
Sample Correlation Function
Analogous to ^RTðsÞ in (4.23), the value of the correlation function RðsÞ at time lag
s ¼ rDt can be estimated by averaging xj þ rxT
j over different j s. This leads to the
following estimator for RðrDtÞ:
^Rr ¼
1
N
P
N1r
j¼0
xj þ rxT
j
0  r  N  1
1
N
P
N1
j¼jrj
xj þ rxT
j
ðN  1Þ  r   1
8
>
>
>
<
>
>
>
:
ð4:54Þ
Based on
xj
 N1
j¼0 , the maximum time lag at which the correlation can be estimated
is ðN  1ÞDt and so ^Rr is only deﬁned for ðN  1Þ  r  ðN  1Þ. The averaging
factor is N instead of N  rj j, for the same reason that the averaging factor for
4.4
Continuous-Time Sample Process
147

^RTðsÞ in (4.23) is T instead of T  sj j, i.e., so that ^Rr satisﬁes the Wiener-Khinchin
formula (Sect. 4.5.3).
The expressions for positive and negative r differ in their summation limit, as
constrained by the values available in the sequence
xj
 N1
j¼0 . Analogous to (4.24),
the transpose mirror property reads
^Rr ¼ ^R
T
r
ð4:55Þ
This can be veriﬁed by noting that for r [ 0,
^R r
|{z}
\0
¼ 1
N
X
N1
j¼r
xjrxT
j
¼
change index
from j to i¼jr
1
N
X
N1r
i¼0
xixT
i þ r ¼
1
N
X
N1r
i¼0
xi þ rxT
i
"
#T
¼ ^R
T
r
ð4:56Þ
Equation (4.55) implies that only the values for r  0 are needed; the ones for r\0
are redundant.
Example 4.2 (Structure of sample correlation function) Here we illustrate the
structure of the sum in the sample correlation function and its transpose mirror
property. Consider a discrete-time process with N ¼ 4 samples:
xj
 N1
j¼0 ¼ x0; x1; x2; x3
f
g
For 0  r  N  1, i.e., 0  r  3, using the ﬁrst expression in (4.54),
r ¼ 0;
^R0 ¼ 1
4
X
3
j¼0
xjxT
j ¼ 1
4 x0xT
0 þ x1xT
1 þ x2xT
2 þ x3xT
3


r ¼ 1;
^R1 ¼ 1
4
X
2
j¼0
xj þ 1xT
j ¼ 1
4 x1xT
0 þ x2xT
1 þ x3xT
2


r ¼ 2;
^R2 ¼ 1
4
X
1
j¼0
xj þ 2xT
j ¼ 1
4 x2xT
0 þ x3xT
1


r ¼ 3;
^R3 ¼ 1
4
X
0
j¼0
xj þ 3xT
j ¼ 1
4 x3xT
0


For ðN  1Þ  r   1, i.e., 3  r   1, using the second expression in
(4.54),
148
4
Spectral Analysis of Stationary Stochastic Process

r ¼ 1;
^R1 ¼ 1
4
X
3
j¼1
xj1xT
j ¼ 1
4 x0xT
1 þ x1xT
2 þ x2xT
3


¼ ^R
T
1
r ¼ 2;
^R2 ¼ 1
4
X
3
j¼2
xj2xT
j ¼ 1
4 x0xT
2 þ x1xT
3


¼ ^R
T
2
r ¼ 3;
^R3 ¼ 1
4
X
3
j¼3
xj3xT
j ¼ 1
4 x0xT
3


¼ ^R
T
3
■
4.5.2
Sample Power Spectral Density
Analogous to ^STðxÞ ¼ XTðxÞXTðxÞ in (4.29), the sample PSD using the
discrete-time history
xj
 N1
j¼0 is deﬁned as
^SNðxÞ ¼ ^XðxÞ^XðxÞ
ð4:57Þ
where
^XðxÞ ¼
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
j¼0
xjeixjDt
ð4:58Þ
is a discrete-time approximation to XTðxÞ ¼ T1=2 R T
0 xðtÞeixtdt in (4.10). It can
be veriﬁed that ^SNðxÞ is Hermitian and it has the transpose mirror property:
^SNðxÞ ¼ ^SNðxÞ
^SNðxÞ ¼ ^SNðxÞT
ð4:59Þ
It is also positive semi-deﬁnite.
Sample PSD at FFT Frequencies
Evaluating ^SNðxÞ for different values of x involves a separate summation. In
practice it is calculated at the following frequencies:
xk ¼ 2 p k
NDt rad=s
ð
Þ
k ¼ 0; . . .; N  1
ð4:60Þ
This is because the values of ^XðxÞ at these frequencies can be evaluated efﬁciently
by the Fast Fourier Transform (FFT) algorithm (Sect. 2.3). As these values play a
central role in applications, they are denoted speciﬁcally by
4.5
Discrete-Time Sample Process
149

^Sk ¼ ^Xk ^X

k
ð4:61Þ
where, noting xkjDt ¼ 2pjk=N,
^Xk ¼ ^X xk
ð
Þ ¼
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
j¼0
xje2pijk=N
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
FFT of xj
f g
N1
j¼0
ð4:62Þ
The sequence
^Xk

N1
k¼0 is referred as the ‘scaled FFT’ of
xj
 N1
j¼0 ; the scaling
factor is
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Dt=N
p
.
4.5.3
Wiener-Khinchin Formula
Analogous to (4.33) and (4.34), the Wiener-Khinchin formula for a discrete-time
sample process reads
^SNðxÞ ¼ Dt
X
N1
r¼ðN1Þ
^RreixrDt
ð4:63Þ
^Rr ¼ 1
2p
Zp=Dt
p=Dt
^SNðxÞeixrDtdx
ð4:64Þ
Equation (4.63) can be viewed as a Fourier series for ^SNðxÞ, where the roles of
time and frequency are swapped. This agrees with the fact that ^SNðxÞ ¼
^XðxÞ^XðxÞ is a periodic function of x with period 2p=Dt, because ^XðxÞ is.
Equation (4.64) results from the expression of the FS coefﬁcients.
Evaluating (4.63) at the FFT frequency xk ¼ 2pk=NDt and noting ^SN xk
ð
Þ ¼ ^Sk,
^Sk ¼ Dt
X
N1
r¼ðN1Þ
^Rre2pirk=N
ð4:65Þ
150
4
Spectral Analysis of Stationary Stochastic Process

Despite the appearance in (4.65), the sequence
^Sk
n
oN1
k¼0 is not the FFT of
^RrDt

N1
r¼ðN1Þ. Rather, ^Sk is equal to the 2 k th entry of the FFT of the 2 N-
sequence formed by padding a zero in the beginning of
^RrDt

N1
r¼ðN1Þ. See
Table 2.4 in Sect. 2.8. The inverse FFT of
^Sk
n
oN1
k¼0 does not return ^RrDt either.
Proof of (4.63) and (4.64) (Wiener-Khinchin formula, discrete-time)
From deﬁnition,
^SNðxÞ ¼
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
i¼0
xieixiDt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^XðxÞ
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
j¼0
xT
j eixjDt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^XðxÞ
¼ Dt
N
X
N1
i¼0
X
N1
j¼0
xixT
j eixðijÞDt
ð4:66Þ
Change index from ði; jÞ to ðr; jÞ where r ¼ i  j ranges between ðN  1Þ and
N  1. For every r, the summation limit for j can be determined from Fig. 4.3. For
r  0, j ranges from 0 to N  1  r. For r\0, j ranges from
rj j to N  1.
Equation (4.66) can then be written as
^SNðxÞ ¼ Dt
X
N1
r¼0
1
N
X
N1r
j¼0
xj þ rxT
j
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^Rr for r  0
eixrDt þ Dt
X
1
r¼ðN1Þ
1
N
X
N1
j¼jrj
xj þ rxT
j
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^Rr for r\0
eixrDt
¼ Dt
X
N1
r¼ðN1Þ
^RreixrDt
ð4:67Þ
This proves (4.63).
N – 1 – r
r ≥0
0 ≤j ≤N – 1 – r
r < 0
|r| ≤j ≤N – 1
N – 1
| r |
(0,0)
N – 1
i
j
Fig. 4.3 Summation limit
of j
4.5
Discrete-Time Sample Process
151

To show (4.64), starting from the RHS and using (4.63),
1
2p
Zp=Dt
p=Dt
^SNðxÞeixrDtdx¼ 1
2p
Zp=Dt
p=Dt
Dt
X
N1
j¼ðN1Þ
^RjeixjDt
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
^SNðxÞ
eixrDtdx
¼ Dt
2p
X
N1
j¼ðN1Þ
^Rj
Zp=Dt
p=Dt
eixðrjÞDtdx
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
2p=Dt when j¼r; else 0
¼ ^Rr
ð4:68Þ
■
4.5.4
Parseval Equality
Setting r ¼ 0 in (4.64) gives the Parseval equality for the discrete-time sample
process
xj
 N1
j¼0 :
1
N
X
N1
j¼0
xjxT
j
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
^R0
¼ 1
2p
Zp=Dt
p=Dt
^SNðxÞdx
ð4:69Þ
On the other hand, summing ^Sk in (4.65) over k ¼ 0; . . .; N  1 and multiplying by
the FFT frequency interval Df ¼ 1=NDt (Hz) gives
Df
X
N1
k¼0
^Sk ¼ Dt
NDt
X
N1
k¼0
X
N1
r¼ðN1Þ
^Rre2pirk=N ¼ 1
N
X
N1
r¼ðN1Þ
^Rr
X
N1
k¼0
e2pirk=N
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
N for r¼0; else 0
¼ ^R0
ð4:70Þ
where the exponential sum has been evaluated in Sect. 2.3.1. Thus,
1
N
X
N1
j¼0
xjxT
j
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
^R0
¼ Df
X
N1
k¼0
^Sk
ð4:71Þ
152
4
Spectral Analysis of Stationary Stochastic Process

Equations (4.69) and (4.71) express the sample covariance matrix as a sum of PSD
contributions from different frequencies.
Example 4.3 (Simulation of discrete-time white noise) A sample history
wj

N1
j¼0
of a scalar Gaussian white noise process with PSD Sw band-limited between

1=2Dt (Hz) can be generated by
wj ¼
ﬃﬃﬃﬃﬃ
Sw
Dt
r
Zj
j ¼ 0; . . .; N  1
ð4:72Þ
where Dt (s) is the sampling interval and
Zj
 N1
j¼0 are i.i.d. standard Gaussian (zero
mean and unit variance). Here we verify that the expectation of the sample PSD is
indeed Sw. The scaled FFT of
wj

N1
j¼0 at frequency fk ¼ k=NDt (Hz) is
Wk ¼
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
j¼0
wje2pijk=N
ð4:73Þ
The expectation of the sample PSD at fk is then
E ^Sk


¼ E WkW
k


¼ Dt
N
X
N1
i¼0
X
N1
j¼0
E wiwj


|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
Sw=Dt for j¼i;
else 0
e2piðijÞk=N ¼ Dt
N
X
N1
i¼0
Sw
Dt ¼ Sw
ð4:74Þ
Alternatively, the simulation formula (4.72) can be reasoned as follow. To be
white noise the wj s must be i.i.d. Gaussian and so wj ¼ cZj where
Zj
 N1
j¼0 are i.i.d.
standard Gaussian and c is some constant. To determine c, note that var wj


¼ c2.
From the Parseval equality, variance is equal to the area of PSD in the frequency
domain (in Hz). Since the PSD is equal to Sw over the band from 1=2Dt to 1=2Dt
(Hz), the area is Sw=Dt. This implies that c2 ¼ Sw=Dt and so c ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Sw=Dt
p
.
■
4.6
Averaging Sample PSD
The sample PSD ^SNðxÞ in (4.57) was motivated from its theoretical counterpart
SðxÞ. In application it is not used directly for estimating SðxÞ because its statistical
error does not diminish as data length increases. We ﬁrst consider an example to
illustrate this point. After that a converging estimator shall be introduced.
4.5
Discrete-Time Sample Process
153

Example 4.4 (Variability of raw sample PSD) Consider synthetic acceleration
data
^€xj
 N1
j¼0 generated at sampling rate 100 Hz Dt ¼ 0:01
ð
s) according to
^€xj ¼ €x tj
 
þ ej
j ¼ 0; . . .; N  1
ð4:75Þ
where
ej
 N1
j¼0 is white noise with PSD Se ¼ 1ðlgÞ2=Hz; xðtÞ satisﬁes
€xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ ¼ pðtÞ
ð4:76Þ
with natural frequency x1 ¼ 2p (rad/s) and damping ratio f ¼ 1%; pðtÞ is white
noise with PSD Sp ¼ 0:04ðlg)2=Hz. It can be shown that (Sect. 5.1.2) the theo-
retical
PSD
of
^€xðtÞ
is
SðxÞ ¼ SpDðxÞ þ Se,
where
DðxÞ ¼ ð1  b2Þ2 þ
h
ð2fbÞ21; b ¼ x1=x.
Figure 4.4 shows the sample PSD calculated using data of different durations
NDt. As the duration increases, the frequency resolution increases (horizontal
separation between dots narrows). There are more points in the spectrum but they
show no continuity. There is no sign of convergence with increasing duration. The
values at different frequencies appear uncorrelated.
■
Simple Average
As demonstrated in Example 4.4, the sample PSD in (4.57) does not converge to the
theoretical PSD as the data duration increases. Essentially, the additional data goes
into improving the frequency resolution rather than reducing the statistical vari-
ability at a given frequency. The ‘raw’ spectra in Fig. 4.4 comprising scattered
points around the theoretical PSD are difﬁcult to interpret or infer about the
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
50 sec
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
100 sec
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
200 sec
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
500 sec
Frequency [Hz]
[(μg)2/Hz]
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
1000 sec
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
2000 sec
Fig. 4.4 Sample PSD (dots at FFT frequencies) calculated from data of different durations.
Dashed line theoretical PSD
154
4
Spectral Analysis of Stationary Stochastic Process

theoretical spectrum. A converging estimator for the PSD can be obtained by
averaging the sample PSDs calculated from multiple sample histories.
Let
xðrÞ
j
n
oN1
j¼0
be the rth set of sample history
r ¼ 1; . . .; M
ð
Þ of xðtÞ. The
(raw) sample PSD at the FFT frequency xk ¼ 2pk=NDt (rad/s) based on this
sample is
^S
ðrÞ
k
¼ ^X
ðrÞ
k ^X
ðrÞ
k
ð4:77Þ
^X
ðrÞ
k
¼
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
j¼0
xðrÞ
j e2pijk=N
ð4:78Þ
The ‘averaged sample PSD’, or simply sample PSD when understood, is deﬁned as
^Ek ¼ 1
M
X
M
r¼1
^S
ðrÞ
k
¼ 1
M
X
M
r¼1
^X
ðrÞ
k ^X
ðrÞ
k
ð4:79Þ
As a result of averaging, if the segments
xðrÞ
j
n
oN1
j¼0 are i.i.d. for different r, then the
variance of ^Ek will be 1=M of the variance of ^S
ðrÞ
k , thereby producing a converging
estimator. The statistical properties of ^Ek are discussed in Sect. 4.10.
In typical situations there is only one time history available, say
xj
 N1
j¼0 . It may
be divided into M non-overlapping time histories to give M segments. That is,
assuming N=M is an integer,
xðrÞ
j
¼ xðr1ÞN=M þ j
j ¼ 0; . . .; N
M  1
ð4:80Þ
In this case, each segment has only N=M points and hence shorter duration NDt=M.
The frequency resolution of the averaged spectrum is M times lower, with a fre-
quency interval of M=NDt (Hz). The choice of M is a trade-off between frequency
resolution and statistical accuracy in the spectrum. See Sect. 7.2.
The averaged spectrum here is based on non-overlapping segments and is
conceptually simple. There are more sophisticated means for averaging. It is pos-
sible to make use of segments that overlap or modiﬁed by a window function
(Welch 1967). The choice of window function allows one to improve certain sta-
tistical features of the spectrum. Interested readers can refer to signal processing
texts.
Example 4.5 (Sample PSD, averaged) Consider the 2000 s data in Example 4.4.
Dividing it into M segments, we calculate the averaged sample PSD. The results are
4.6
Averaging Sample PSD
155

shown in Fig. 4.5. As M increases, the spectrum looks smoother, as a result of
averaging. This is at the expense of reducing frequency resolution and increasing
bias in the estimation; see Sect. 4.7 next.
■
4.7
Distortions in Sample Estimators
The sample correlation function and sample PSD are ‘statistical estimators’ for the
theoretical quantity they estimate. Clearly, their values depend on the particular data
set. Variability in data induces variability in the sample estimator. The quality of a
sample estimator is often quantiﬁed in terms of its bias (mean deviation from the
target value) and variance (mean squared deviation from the target value).
In this section we derive the expectation and discuss the bias of the sample
correlation function and sample PSD. Of particular attention is the sample PSD,
whose bias is of a characteristic nature stemming from the properties of
discrete-time Fourier Transform (DTFT). Finite data duration causes leakage and
limits frequency resolution. Finite sampling rate limits the highest frequency that
can be properly estimated and causes aliasing. Discussion on the variance of the
sample PSD is postponed until Sect. 4.10.
4.7.1
Sample Correlation Function
Based on a continuous time sample
xðtÞ : 0  t  T
f
g, the sample correlation
function ^RTðsÞ in (4.23) is a biased estimator for RðsÞ for ﬁnite T but is asymp-
totically unbiased as T ! 1. To see this, take expectation on the ﬁrst expression in
(4.23) 0  s\T
ð
Þ and swap the order of expectation and integration:
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
M=1
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
M=2
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
M=4
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
M=10
Frequency [Hz]
[(μg)2/Hz]
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
M=20
0.9
0.95
1
1.05
1.1
10
-1
10
0
10
1
10
2
10
3
M=40
Fig. 4.5 Sample PSD (dot) averaged over different number of segments M from 2000 s data.
Dashed line theoretical PSD
156
4
Spectral Analysis of Stationary Stochastic Process

E ^RTðsÞ


¼ 1
T
ZTs
0
E xðt þ sÞxðtÞT


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
RðsÞ
dt ¼
1  s
T


RðsÞ
ð4:81Þ
For a given s, as T ! 1; E ^RTðsÞ


! RðsÞ, i.e., asymptotically unbiased. The
case for T\s\0 follows similarly by using the transpose mirror property.
An analogous result holds in discrete time. Using
xj ¼ xðjDtÞ

N1
j¼0 , the sample
correlation function ^Rr in (4.54) is an asymptotically unbiased estimator for RðrDtÞ
as N ! 1. Taking expectation of the ﬁrst expression 0  r  N  1
ð
Þ,
E ^Rr


¼ 1
N
X
N1r
j¼0
E xj þ rxT
j
h
i
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
RðrDtÞ
¼
1  r
N


RðrDtÞ
ð4:82Þ
and so for a given r we have E ^Rr


! RðrDtÞ as N ! 1. The case for ðN  1Þ
 r   1 follows similarly from the transpose mirror property. The above argu-
ment assumes that Dt is ﬁxed, which implies T ¼ NDt ! 1 as N ! 1. Otherwise
it need not hold. For example, suppose T is ﬁxed. Then Dt ¼ T=N ! 0 as N ! 1.
If we estimate RðsÞ for ﬁxed s ¼ rDt, then r ¼ s=Dt ¼ Ns=T has to increase with
N. In this case, in (4.82), 1  r=N ¼ 1  s=T will not tend to 1 and E ^Rr


will not
tend to RðrDtÞ even when N ! 1. Intuitively, increasing the sampling rate alone
does not eliminate bias. The data duration has to increase as well.
4.7.2
Sample PSD
Similar to the sample correlation function, the sample PSD is also asymptotically
unbiased. While the bias in the sample correlation function is simply by a factor, the
bias in the sample PSD is of a more characteristic nature. The expectation of the
sample PSD is equal to the convolution of the theoretical PSD with a kernel function.
Continuous-Time Sample PSD
The expectation of the sample PSD in (4.29) is given by
E ^STðxÞ
h
i
¼ 1
2p
Z1
1
S x0
ð
ÞFT x  x0
ð
Þdx0
ð4:83Þ
FTðxÞ ¼ 4 sin2ðxT=2Þ
x2T
ð4:84Þ
4.7
Distortions in Sample Estimators
157

See the end for proof. Figure 4.6 shows a schematic plot of the kernel function
FTðxÞ. It is concentrated in a neighborhood of Oð2p=TÞ around the origin. This
implies that the value of E ^STðxÞ
h
i
is affected by the values of SðxÞ in a neigh-
borhood of Oð2p=TÞ around the subject frequency x. This is leakage, as encoun-
tered in Sect. 2.4.3. It implies a frequency resolution of Oð2p=TÞ (rad/s), or Oð1=TÞ
(Hz), because one is not able to distinguish harmonics in the neighborhood. It can
be
shown
that
the
area
of
FTðxÞ=2p
is
always
equal
to
1,
i.e.,
R 1
1 FTðxÞdx=2p ¼ 1. As T ! 1, FTðxÞ=2p tends to the Dirac Delta function
dðxÞ. This implies that E ^STðxÞ
h
i
! SðxÞ, i.e., asymptotically unbiased.
Discrete-Time Sample PSD
Analogous to (4.83), the expectation of the sample PSD in (4.57) is given by
E ^SNðxÞ
h
i
¼ 1
2p
Z1
1
S x0
ð
ÞFN x  x0
ð
Þdx0
ð4:85Þ
FNðxÞ ¼ Dt sin2ðNxDt=2Þ
N sin2ðxDt=2Þ
ð4:86Þ
See the end for proof. It is more insightful to write FNðxÞ in terms of the
dimensionless variable u ¼ xDt=2p:
FNðxÞ ¼ NDtD2
NðuÞ
u ¼ xDt
2p
ð4:87Þ
(Hz)
2
/ π
ω
T
0
T
1
T
2
T
3
T
3
−
T
2
−
T
1
−
Fig. 4.6 Schematic plot of FT(x)
158
4
Spectral Analysis of Stationary Stochastic Process

where
DNðuÞ ¼ sin Npu
N sin pu
ð4:88Þ
is the Dirichlet kernel that appeared in Sects. 2.4.3. Figure 4.7 shows a schematic
plot of FNðxÞ, whose properties stem from those of DNðuÞ. It is periodic with a
basic branch in ðp=Dt; p=DtÞ, i.e., ±Nyquist frequency. Within the branch,
FNðxÞ is similar to FTðxÞ in Fig. 4.6. It can be shown that the area of FNðxÞ=2p in
the branch is always equal to 1, i.e.,
R p=Dt
p=Dt FNðxÞdx=2p ¼ 1; see Sect. 4.8.2 later.
For ﬁxed data duration T ¼ NDt, taking Dt ! 0 N ¼ T=Dt ! 1
ð
Þ yields
FNðxÞ ! FTðxÞ and so E ^SNðxÞ
h
i
! E ^STðxÞ
h
i
. That is, the discrete-time and
continuous-time estimator coincide as the sampling interval diminishes, which can
be intuitively expected. For ^SNðxÞ to be asymptotically unbiased, it is required that
Dt ! 0 and T ¼ NDt ! 1.
It can be reasoned that ^SNðxÞ is periodic with the same period as FNðxÞ. As a
result, ^SNðxÞ can at best estimate SðxÞ for frequencies up to the Nyquist frequency,
i.e., p=Dt (rad/s) or 1=2Dt (Hz). Not only is the frequency band of estimation
limited, it is also contaminated by frequency components outside the band. Since
FNðxÞ is periodic with a basic branch on ðp=Dt; p=DtÞ, the integral in (4.85)
includes the contribution of SðxÞ from the frequency neighborhoods x 
2pk=Dt k ¼ 1; 2; . . .
ð
Þ recurring outside the band. This is aliasing, as encountered in
Sect. 2.4.2. Considering positive frequencies only and taking into account the
conjugate mirror property of PSD, the aliased neighborhoods are around 2pk=Dt 
x
k ¼ 1; 2; . . .
ð
Þ. Figure 4.8 illustrates the distortion of the sample PSD due to
ﬁnite duration and ﬁnite sampling rate.
Example 4.6 (Leakage) Consider the sample PSD in Example 4.4 computed from
synthetic ambient acceleration data. Figure 4.9 shows a plot of the theoretical PSD
(solid line) and the expectation of sample PSD (dots) for different Nc ¼ data
duration/natural period. The expectation is calculated based on (4.85), where the
(Hz)
2
/ π
ω
t
N
T
Δ
=
0
T
1
T
2
T
3
t
Δ
2
1
t
Δ
−2
1
t
Δ
1
t
Δ
−1
Basic branch
… periodic …
… periodic …
Fig. 4.7 Schematic plot of FN(x)
4.7
Distortions in Sample Estimators
159

convolution is performed numerically. As Nc increases, the expectation of the
sample PSD tends to the theoretical PSD. When Nc ¼ 50 the difference is quite
signiﬁcant. When Nc  500 the two PSDs visually coincide.
Figure 4.10 shows the ratio of the two PSDs at the natural frequency versus Nc.
This ratio is less than 1, indicating that the sample PSD is biased low at the natural
frequency. Here, Nc ¼ 500 is required to have a percentage bias less than 5%. ■
0
0
0
… periodic
(Hz)
2
/ π
ω
t
Δ
2
/
1
t
Δ
/
1
B
A
B
A
B
A
A
B
Sampling band
(Hz)
2
/ π
ω
(Hz)
2
/ π
ω
)
(ω
S
∫
∞
∞
−
′
′
−
′
=
ω
ω
ω
ω
π
ω
d
F
S
S
E
T
T
)
(
)
(
2
1
)]
(
ˆ
[
∫
∞
∞
−
′
′
−
′
=
ω
ω
ω
ω
π
ω
d
F
S
S
E
N
N
)
(
)
(
2
1
)]
(
ˆ
[
(a) Theoretical PSD
(b) Sample PSD
Finite duration
(leakage)
(c) Sample PSD
Finite duration & 
finite sampling rate
(leakage & aliasing)
)
(ω
S
)
(
ˆ
ω
T
S
)
(
ˆ
ω
N
S
Conjugate
mirror
Fig. 4.8 Distortion in the sample PSD. a Theoretical; b continuous-time ﬁnite duration sample;
c discrete-time ﬁnite duration sample
0.9
0.95
1
1.05
1.1
0
1000
2000
3000
Nc=50
0.9
0.95
1
1.05
1.1
0
1000
2000
3000
Nc=100
0.9
0.95
1
1.05
1.1
0
1000
2000
3000
Nc=200
0.9
0.95
1
1.05
1.1
0
1000
2000
3000
Nc=500
Freq. (Hz)
[(μg)2/Hz)]
0.9
0.95
1
1.05
1.1
0
1000
2000
3000
Nc=1000
0.9
0.95
1
1.05
1.1
0
1000
2000
3000
Nc=2000
Fig. 4.9 Theoretical PSD (solid line) and expectation of sample PSD (dots) for different data
durations, Nc = data duration/natural period
160
4
Spectral Analysis of Stationary Stochastic Process

Proof of (4.83) (Expectation of Sample PSD, Continuous-Time)
From deﬁnition,
E ^STðxÞ
h
i
¼ E
	
1ﬃﬃﬃﬃ
T
p
ZT
0
xðtÞeixtdt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
XTðxÞ
1ﬃﬃﬃﬃ
T
p
ZT
0
xðsÞTeixsds
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
XTðxÞ

¼ 1
T
ZT
0
ZT
0
E xðtÞxðsÞT


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
RðtsÞ
eixðtsÞdtds
ð4:89Þ
Substituting the Wiener-Khinchin formula
Rðt  sÞ ¼ 1
2p
Z 1
1
S x0
ð
Þeix0ðtsÞdx0
ð4:90Þ
and carrying the integral w.r.t. x0 outside,
E ^STðxÞ
h
i
¼
1
2pT
Z1
1
S x0
ð
Þ
ZT
0
ZT
0
ei xx0
ð
ÞðtsÞdtds
2
4
3
5dx0
¼ T
2p
Z1
1
S x0
ð
Þ
1
T
ZT
0
ei xx0
ð
Þtdt
2
4
3
5
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K xx0
ð
Þ
1
T
ZT
0
ei xx0
ð
Þsds
2
4
3
5

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K xx0
ð
Þ
dx0
¼ 1
2p
Z1
1
S x0
ð
ÞFT x  x0
ð
Þdx0
ð4:91Þ
10
1
10
2
10
3
10
4
0
10
20
30
40
50
60
70
80
90
100
Nc=Data duration/Natural period
[%]
Fig. 4.10 Ratio of the expectation of sample PSD to theoretical PSD at the natural frequency
4.7
Distortions in Sample Estimators
161

where
FTðxÞ ¼ T KðxÞ
j
j2
ð4:92Þ
Using the identity 1  eih ¼ 2ieih=2 sinðh=2Þ for any real h,
KðxÞ ¼ 1
T
ZT
0
eixtdt ¼ 1  eixT
ixT
¼ 2
xT eixT=2 sinðxT=2Þ
ð4:93Þ
KðxÞ
j
j2¼ 4 sin2ðxT=2Þ
x2T2
ð4:94Þ
Substituting into (4.92) gives the expression of FTðxÞ in (4.84).
The following shows that the area of FTðxÞ=2p is equal to 1:
1
2p
Z1
1
FTðxÞdx¼ T
2p
Z1
1
1
T
ZT
0
eixtdt
2
4
3
5
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
KðxÞ
1
T
ZT
0
eixsds
2
4
3
5
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
KðxÞ
dx
¼
1
2pT
Z1
1
ZT
0
ZT
0
eixðtsÞdtdsdx
¼ 1
T
ZT
0
ZT
0
1
2p
Z1
1
eixðtsÞdx
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
dðtsÞ
dtds ¼ 1
T
ZT
0
ds ¼ 1
ð4:95Þ
■
Proof of (4.85) (Expectation of Sample PSD, Discrete-Time)
The structure of the proof is similar to that for (4.83). From deﬁnition,
E ^SNðxÞ
h
i
¼ E
	
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
r¼0
xreixrDt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^XðxÞ
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
s¼0
xT
s eixsDt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^XðxÞ

¼ Dt
N
X
N1
r¼0
X
N1
s¼0
E xrxT
s


|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
RððrsÞDtÞ
eixðrsÞDt
ð4:96Þ
162
4
Spectral Analysis of Stationary Stochastic Process

Substituting the Wiener-Khinchin formula
Rððr  sÞDtÞ ¼ 1
2p
Z1
1
S x0
ð
Þeix0ðrsÞDtdx0
ð4:97Þ
and carrying the integral outside,
E ^SNðxÞ
h
i
¼ Dt
2pN
Z1
1
S x0
ð
Þ
X
N1
r¼0
X
N1
s¼0
ei xx0
ð
ÞðrsÞDtdx0
¼ NDt
2p
Z1
1
S x0
ð
Þ
1
N
X
N1
r¼0
ei xx0
ð
ÞrDt
"
#
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K1 xx0
ð
Þ
1
N
X
N1
s¼0
ei xx0
ð
ÞsDt
"
#
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K1 xx0
ð
Þ
dx0
¼ 1
2p
Z1
1
S x0
ð
ÞFN x  x0
ð
Þdx0
ð4:98Þ
where
FNðxÞ ¼ NDt K1ðxÞ
j
j2
ð4:99Þ
K1ðxÞ appeared in Sect. 2.4.3 and was found to be
K1ðxÞ ¼ sin NxDt=2
ð
Þ
N sinðxDt=2Þ eiðN1ÞxDt=2
ð4:100Þ
This gives
K1ðxÞ
j
j2¼ sin2 NxDt=2
ð
Þ
N2 sin2ðxDt=2Þ
ð4:101Þ
Substituting into (4.99) gives the expression of FNðxÞ in (4.86).
■
4.8
Second Order Statistics of Scaled DTFT
In this section we study the second order statistics of the scaled DTFTs
^XðxÞ


in
(4.58):
4.7
Distortions in Sample Estimators
163

^XðxÞ ¼
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
j¼0
xjeixjDt
ð4:102Þ
As before,
xj ¼ xðjDtÞ

N1
j¼0 are sample values of a zero-mean stationary process
xðtÞ with PSD matrix SðxÞ. The statistical properties of ^XðxÞ at the FFT fre-
quencies are of fundamental importance to Bayesian OMA theory.
4.8.1
Complex Covariance and Pseudo-covariance Matrix
Since ^XðxÞ is a complex vector, its second order statistics comprise those of the real
and imaginary parts. For two frequencies a and b, the second order statistics between
^XðaÞ and ^XðbÞ comprise the following real-valued cross covariance matrices:
E Re^XðaÞRe ^XðbÞT


E Re ^XðaÞIm ^XðbÞT


E Im ^XðaÞRe ^XðbÞT


E Im ^XðaÞIm ^XðbÞT


ð4:103Þ
Working with these matrices can be cumbersome for analysis or computation,
involving algebraic operations that are similar or repetitive. An equivalent but more
elegant way is to work with the complex-valued generalization of the cross
covariance matrix and the cross ‘pseudo-covariance matrix’, deﬁned respectively as
E ¼ E ^XðaÞ^XðbÞ


Q ¼ E ^XðaÞ^XðbÞT


ð4:104Þ
Both E and Q are n  n complex matrix. They can be determined from the matrices
in (4.103), and vice versa (Section A.2):
E Re ^XðaÞRe ^XðbÞT


¼ Re E þ Re Q
2
E Re ^XðaÞIm ^XðbÞT


¼ Im E þ Im Q
2
E Im ^XðaÞRe^XðbÞT


¼ Im E þ Im Q
2
E Im ^XðaÞIm ^XðbÞT


¼ Re E  Re Q
2
ð4:105Þ
Two
complex
vectors
are
uncorrelated
if
their
cross
covariance
and
pseudo-covariance matrices are zero.
4.8.2
Convolution Formula
The cross covariance and pseudo-covariance matrices of scaled DTFTs can be
expressed in terms of the PSD matrix SðxÞ. For any two frequencies a and b (rad/s),
164
4
Spectral Analysis of Stationary Stochastic Process

E ^XðaÞ^XðbÞ


¼ 1
2p
Z1
1
SðxÞFabðxÞdx
ð4:106Þ
FabðxÞ ¼ NDtK1ða  xÞK1ðb  xÞ
ð4:107Þ
where
K1ðxÞ ¼ 1
N
X
N1
j¼0
eixjDt ¼ sinðNxDt=2Þ
N sinðxDt=2Þ eiðN1ÞxDt=2
ð4:108Þ
appeared in Sect. 2.4.3. See the end for proof.
PSD Matrix
Setting
b ¼ a
in
(4.106)
gives
the
expectation
of
sample
PSD
matrix
E ^SNðaÞ
h
i
¼ E ^XðaÞ^XðaÞ


. The resulting expression agrees with (4.85) because
FaaðxÞ ¼ NDt K1ða  xÞ
j
j2¼ FNða  xÞ
ð4:109Þ
Cross Pseudo-covariance Matrix
Although presented like a covariance matrix, (4.106) can be used for obtaining the
pseudo-covariance by replacing b with −b. This is because ^XðbÞT ¼ ^XðbÞ and
so E ^XðaÞ^XðbÞT


¼ E ^XðaÞ^XðbÞ


. Then
E ^XðaÞ^XðbÞT


¼ 1
2p
Z1
1
SðxÞFa;bðxÞdx
ð4:110Þ
Kernel Function
As mentioned in Sect. 2.4.3, in terms of the dimensionless variable u ¼ xDt=2p,
K1ðxÞ ¼ DNðuÞeipðN1Þu
u ¼ xDt
2p
ð4:111Þ
where DNðuÞ ¼ sinðNpuÞ=N sin pu is the Dirichlet kernel. Substituting into (4.107)
gives
FabðxÞ ¼ GabðuÞeipðN1Þ uaub
ð
Þ
ð4:112Þ
GabðuÞ ¼ NDtDN ua  u
ð
ÞDN ub  u


ð4:113Þ
4.8
Second Order Statistics of Scaled DTFT
165

u ¼ xDt
2p
ua ¼ aDt
2p
ub ¼ bDt
2p
ð4:114Þ
It can be shown that (see the end)
Zp=Dt
p=Dt
K1ða  xÞK1ðb  xÞdx ¼ 2p
NDt K1ða  bÞ
ð4:115Þ
and so
1
2p
Zp=Dt
p=Dt
FabðxÞdx ¼ K1ða  bÞ
ð4:116Þ
Setting b ¼ a gives
R p=Dt
p=Dt FaaðxÞdx=2p ¼ K1ð0Þ ¼ 1. This proves the claim in
Sect. 4.7.2 that
R p=Dt
p=Dt FNðxÞdx=2p ¼ 1, because FaaðxÞ ¼ FNða  xÞ and its
integral over one period is invariant to the shift a.
Proof of (4.106) (Cross Covariance Matrix of Scaled DTFT)
The structure of the proof is similar to that for (4.85). From deﬁnition,
E ^XðaÞ^XðbÞ


¼ E
	
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
r¼0
xreiarDt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^XðaÞ
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
s¼0
xT
s eibsDt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^XðbÞ

¼ Dt
N
X
N1
r¼0
X
N1
s¼0
E xrxT
s


|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
RððrsÞDtÞ
eiðarbsÞDt
ð4:117Þ
Using the Wiener-Khinchin formula,
Rððr  sÞDtÞ ¼ 1
2p
Z1
1
SðxÞeixðrsÞDtdx
ð4:118Þ
Substituting into (4.117) and carrying the integral outside,
166
4
Spectral Analysis of Stationary Stochastic Process

E ^XðaÞ^XðbÞ


¼ Dt
2pN
Z1
1
SðxÞ
X
N1
r¼0
X
N1
s¼0
eixðrsÞDteiðarbsÞDt
"
#
dx
¼ Dt
2pN
Z1
1
SðxÞ
X
N1
r¼0
X
N1
s¼0
eiðaxÞrDteiðbxÞsDt
"
#
dx
¼ NDt
2p
Z1
1
SðxÞ
1
N
X
N1
r¼0
eiðaxÞrDt
"
#
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K1ðaxÞ
1
N
X
N1
s¼0
eiðbxÞsDt
"
#
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K1ðbxÞ
dx
ð4:119Þ
■
Proof of (4.115) (Integral of Kernel Product)
K1ða  xÞK1ðb  xÞ ¼
1
N
X
N1
r¼0
eiðaxÞrDt
"
#
1
N
X
N1
s¼0
eiðbxÞsDt
"
#
¼ 1
N2
X
N1
r¼0
X
N1
s¼0
eiðarbsÞDteixðrsÞDt
ð4:120Þ
Integrating w.r.t. x,
Zp=Dt
p=Dt
K1ða  xÞK1ðb  xÞdx¼ 1
N2
X
N1
r¼0
X
N1
s¼0
eiðarbsÞDt
Zp=Dt
p=Dt
eixðrsÞDtdx
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
2p=Dt if s¼r; else 0
¼ 2p
NDt  1
N
X
N1
r¼0
eiðabÞrDt
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K1ðabÞ
ð4:121Þ
■
4.8.3
Long-Data Asymptotics of Scaled FFT
The second order statistics of scaled FFTs can be obtained by evaluating (4.106) at
the FFT frequencies. The resulting expression simpliﬁes signiﬁcantly when the data
duration is ‘long’. Consider the scaled FFTs
4.8
Second Order Statistics of Scaled DTFT
167

^Xp ¼ ^XðaÞ
^Xq ¼ ^XðbÞ
ð4:122Þ
at the FFT frequencies
a ¼ 2pp
NDt
b ¼ 2pq
NDt
rad=s
ð
Þ
ð4:123Þ
For ﬁxed Dt, it can be shown that as N ! 1 (and hence data duration NDt ! 1Þ:
For positive integers p and q below the Nyquist index (integer part of N=2),
A1. Cross-covariance E ^Xp ^X

q
h
i
! 0 for any p 6¼ q
A2. Cross pseudo-covariance E ^Xp ^X
T
q
h
i
! 0 for any p; q
In addition, if there is no aliasing (e.g., Dt ! 0Þ, then
A3. Auto-covariance E ^Xp ^X

p
h
i
! S xp


These asymptotic results are remarkably clean. Omitting technical details, they can
be reasoned based on (4.106) and the behavior of GabðuÞ in (4.113). The latter is
illustrated in Fig. 4.11 a 6¼ b
ð
Þ and Fig. 4.12 a ¼ b
ð
Þ. The function GabðuÞ has a
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-1
-0.5
0
0.5
1
p = 15
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-4
-2
0
2
4
 p = 3
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-4
-2
0
2
4
 p = 2
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-8
-4
0
4
8
 p = 1
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
0
10
20
30
40
50
 p = 0
u
-0.5 -0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-1
-0.5
0
0.5
1
p - q = 20
-0.5 -0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-2
-1
0
1
2
 p - q = 10
-0.5 -0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-5
-2.5
0
2.5
5
 p - q = 3
-0.5 -0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-8
-4
0
4
8
 p - q = 2
-0.5 -0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-20
-10
0
10
20
 p - q = 1
u
)
(
(a)
,
u
G
α
α −
)
(
(b)
u
Gαβ
Fig. 4.11 Plot of GabðuÞ for a ¼ 2pp=NDt, b ¼ 2pq=NDt and N ¼ 50. In a q ¼ p, i.e.,
pseudo-covariance at xp; the function is symmetric about the origin; for large N it is concentrated
near u ¼ 
p=N; when p ¼ 0 it tends to the Delta function as N ! 1. In b p=N ¼ 0:3 and q 6¼ p,
i.e., cross covariance or pseudo-covariance between two different frequencies
168
4
Spectral Analysis of Stationary Stochastic Process

period of 1 and so the pattern in the ﬁgures is repeating outside the range
ð1=2; 1=2Þ. It is fast varying over a scale of Oð1=NÞ. For a 6¼ b, the positive and
negative part of Gab tend to cancel out each other, giving an integral that approa-
ches zero as N ! 1. This leads to A1 and A2. In Fig. 4.12, GaaðuÞ tends to the
Dirac Delta function as N ! 1 and so E ^Xp ^X

p
h
i
! S xp


þ aliased contributions
from other branches. A3 then follows if there is no aliasing.
White noise
When SðxÞ ¼ Sw is constant, i.e., white noise, A1 and A2 holds for any duration,
i.e., even when NDt\1. To see this, note that if the process is band-limited white
between 
p=Dt (rad/s) then, using (4.116),
E ^XðaÞ^XðbÞ


¼ SwK1ða  bÞ
ð4:124Þ
This is zero when a and b are distinct FFT frequencies because then a  b is an
integer multiple of 2p=NDt, at which K1ða  bÞ ¼ 0 (see Fig. 4.7). For (ideal)
white noise over frequency 
1, the integral in (4.106) is still zero because the
integrand has a period of 2p=Dt.
For A3 to hold, it is still required that N ! 1 and there is no aliasing. Thus, for
general non-white process and ﬁnite data duration in practice, A1 and A2 are more
robust approximations than A3.
4.8.4
How Long Is Long?
For ﬁnite data duration, the quality of long-data asymptotic approximation depends
on the shape of the PSD SðxÞ. As discussed in the last section, when SðxÞ is
constant, i.e., white noise, A1 and A2 hold even for ﬁnite duration. In the general
case, the approximation is good when FabðxÞ is ‘fast varying’ compared to SðxÞ.
Translating this in the context of OMA,
In the resonance band of a vibration mode, the number of FFT points should
be large compared to 1
N
1
α
u
u
N
2
1
0
2
1
−
Fig. 4.12 Plot of GaaðuÞ. It becomes the Delta function centered at ua ¼ aDt=2p as N ! 1
4.8
Second Order Statistics of Scaled DTFT
169

This can be reasoned as follow.
Consider the resonance band of a mode where SðxÞ varies signiﬁcantly. The
frequency x ranges between 2pf ð1 
 fjÞ (rad/s) where f is the natural frequency
(Hz), f is the damping ratio and j is a dimensionless bandwidth factor; e.g., j ¼ 1
corresponds to the half-power band. The resonance bandwidth is then 4pfjf (rad/s).
The function FabðxÞ can be considered fast varying compared to SðxÞ when it is
concentrated in a small region compared to the resonance bandwidth. Note that
FabðxÞ is proportional to GabðuÞ
u ¼ xDt=2p
ð
Þ. The latter is concentrated in a
neighborhood of Oð1=NÞ w.r.t. u. This implies that FabðxÞ is concentrated in a
neighborhood of Oð2p=TÞ w.r.t. x, where T ¼ NDt is the data duration. Thus,
long-data asymptotic approximation is good when 2p=T  4pfjf . That is,
Nf ¼ 2fjfT  1
ð4:125Þ
Note that Nf is the number of FFT points in the resonance band because the
bandwidth is 2fjf (Hz) and the FFT frequencies are equally spaced at Df ¼ 1=T
(Hz). This is an important scale in the study of ‘uncertainty laws’ in Chaps. 15 and
16.
Example 4.7 (Correlation of scaled FFT) This example illustrates the cross cor-
relation of scaled FFTs at different frequencies of the synthetic acceleration data in
Example 4.4. Section 4.8.3 says that E ^Xp^X
q
h
i
p 6¼ q
ð
Þ is asymptotically zero for
long data. Consider the cross correlation coefﬁcient between two FFT frequencies
fp ¼ p=NDt and fq ¼ q=NDt (Hz), deﬁned as
E ^Xp^X
q
h
i

=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
SpSq
p
, where Sp ¼
E
^Xp

2
h
i
and Sq ¼ E
^Xq

2
h
i
. It is dimensionless and ranges between 0 and 1. When
p ¼ q it is equal to 1. Figure 4.13 shows the cross correlation coefﬁcients between
0.9
0.95
1
1.05
1.1
0
0.5
1
Nc=50
0.9
0.95
1
1.05
1.1
0
0.5
1
Nc=100
0.9
0.95
1
1.05
1.1
0
0.5
1
Nc=200
0.9
0.95
1
1.05
1.1
0
0.5
1
Nc=500
Freq. [Hz]
Corr. Coef.
0.9
0.95
1
1.05
1.1
0
0.5
1
Nc=1000
0.9
0.95
1
1.05
1.1
0
0.5
1
Nc=2000
Fig. 4.13 Cross correlation coefﬁcient of scaled FFTs between the natural frequency and other
frequencies; Nc ¼ data duration/natural period
170
4
Spectral Analysis of Stationary Stochastic Process

the natural frequency and other frequencies for different data durations. They are
estimated from 100,000 i.i.d. time histories. As the data duration increases, the
cross correlation tends to zero.
■
4.9
Asymptotic Distribution of Scaled FFT
As
^Xk


are complex vectors, their joint distribution is given by the joint distri-
bution of their real and imaginary parts. For long data, they asymptotically have a
‘complex Gaussian distribution’, i.e., their real and imaginary parts are jointly
Gaussian. This is by virtue of the Central Limit Theorem (details omitted), as each
^Xk is a sum of N correlated random complex vectors. When xðtÞ is a Gaussian
process,
^Xk


are complex Gaussian regardless of the data duration because
Gaussian vectors remain to be so under a linear transformation (FFT is linear). For
long data, since
^Xk


are asymptotically Gaussian and uncorrelated (Sect. 4.8.3),
they are also independent, i.e., their joint PDF is equal to the product of their
marginal PDFs.
The marginal PDF of the n  1 complex vector ^Xk is the PDF of the 2n  1 real
vector Re ^Xk; Im ^Xk


. For long data, it is asymptotically given by
p^Xk xk
ð
Þ ¼ ð2pÞn Ck
j
j1=2
 1
2
Re xk
Im xk
	

T
C1
k
Re xk
Im xk
	

(
)
xk
n1 complex
ð4:126Þ
where
Ck ¼
E Re ^XkRe ^X
T
k
h
i
E Re ^XkIm ^X
T
k
h
i
E Im ^XkRe ^X
T
k
h
i
E Im ^XkIm ^X
T
k
h
i
2
4
3
5
ð4:127Þ
Equation (4.126) can be written in a compact form in terms of the complex
covariance matrix Ek ¼ E ^Xk ^X

k
h
i
and pseudo-covariance matrix Qk ¼ E ^Xk ^X
T
k
h
i
:
p^Xk xk
ð
Þ ¼ pn Ek
j
j1=2 Pk
j
j1=2exp
 1
2
xk
xk
	


Ek
Qk
Q
k
Ek
	

1 xk
xk
	

(
)
ð4:128Þ
where
Pk ¼ Ek  Q
kE1
k Qk
ð4:129Þ
4.8
Second Order Statistics of Scaled DTFT
171

See Sect. A.3 for proof and Chap. A for further details on complex Gaussian
distribution.
Further simpliﬁcation results by noting that for long data the pseudo-covariance
matrix Qk is asymptotically zero. The marginal PDF of ^Xk in (4.128) then simpliﬁes
to
p^Xk xk
ð
Þ ¼ pn Ek
j
j1exp x
kE1
k xk


long data
ð
Þ
ð4:130Þ
If there is no aliasing (e.g., Dt ! 0Þ; Ek can be further replaced by the theoretical
PSD matrix S xk
ð
Þ.
4.10
Asymptotic Distribution of Sample PSD
To supplement Sect. 4.6, this section presents the distribution of the averaged
sample PSD based on M i.i.d. data segments of a zero-mean stationary process:
^Ek ¼ 1
M
X
M
r¼1
^X
ðrÞ
k ^X
ðrÞ
k
ð4:131Þ
Assume long data so that ^X
ðrÞ
k
s are complex Gaussian. Then it can be shown that
the sum PM
r¼1 ^X
ðrÞ
k ^X
ðrÞ
k
has a ‘complex Wishart distribution’ with M degrees of
freedom. The PDF of ^Ek is given by
p^EkðEÞ ¼ MnM E
j jMn
I Ek
ð
Þ eM tr E1
k E
ð
Þ
ðM  n; E HermitianÞ
ð4:132Þ
I Ek
ð
Þ ¼ pnðn1Þ=2 Ek
j
jMY
n
i¼1
ðM  iÞ!
ð4:133Þ
Ek ¼ E ^Ek


ð4:134Þ
This PDF is in fact the joint PDF of the (real) diagonal entries and the real and
imaginary parts of the lower (or upper) off-diagonal entries of ^Ek. It is valid for
M  n, which ensures that ^Ek is non-singular. The sample PSDs at different fre-
quencies are asymptotically independent because the scaled FFTs are. Their joint
172
4
Spectral Analysis of Stationary Stochastic Process

PDF is equal to the product of their marginal PDFs. See Sect. A.5 for further details
on complex Wishart distribution.
4.10.1
Scalar Process
Applying (4.132) to a scalar process n ¼ 1
ð
Þ gives the PDF of the averaged sample
PSD, which is often plotted for visualization. For M ¼ 1 (no averaging) the PDF
reduces to the exponential PDF:
p^EkðSÞ ¼ 1
Ek
exp  S
Ek


ðS  0Þ
ð4:135Þ
where Ek is the expectation of ^Ek. Note that the standard deviation of an exponential
random variable is equal to its mean. This implies that the standard deviation of the
sample PSD without averaging is as large as its mean, no matter how high the
sample rate or how long the data is. Intuitively, the additional information from data
goes into exploring higher frequencies (former) or improving frequency resolution
(latter) rather than improving the statistical accuracy of the sample PSD at a par-
ticular frequency.
For the general case M  1, the PDF is given by
p^EkðSÞ ¼
MMSM1
ðM  1Þ!EM
k
exp  MS
Ek


S  0
ð
Þ
ð4:136Þ
Using this expression, it can be reasoned by PDF transformation that the random
variable 2M^Ek=Ek has a Chi-square distribution with 2M degrees of freedom and so
it has mean 2M and variance 4M. This implies that var½^Ek ¼ E2
k=M, which agrees
with the fact that ^Ek is the average of M i.i.d. random variables with mean Ek and
variance E2
k.
Consider the dimensionless random variable Y ¼ ^Ek=Ek. Its PDF is, by PDF
transformation,
pYðyÞ ¼ MMyM1
ðM  1Þ! exp My
ð
Þ
y  0
ð
Þ
ð4:137Þ
Clearly, E½Y ¼ 1 and var½Y ¼ 1=M. Figure 4.14 shows pYðyÞ for different
M. When M ¼ 1 it is simply the exponential PDF with mean 1. As M increases it
tends to a Gaussian PDF centered at 1 with diminishing variance 1=M. This agrees
with the Central Limit Theorem, as Y is in fact the average of M i.i.d. variables,
each with mean 1 and variance 1.
4.10
Asymptotic Distribution of Sample PSD
173

4.11
Summary of Fourier Formulas, Units
and Conventions
Table 4.2 summarizes the expectation of sample estimators for the correlation
function and PSD; and their relationship with the theoretical quantity they estimate.
The units indicated assume that xðtÞ has a unit of volt (V) and time is measured in
second (s). Relevant sections are indicated.
4.11.1
Multiplier in Wiener-Khinchin Formula
As in the case of deterministic process (Sect. 2.7.1), the multiplier 1=2p in the
Wiener-Khinchin formula RðsÞ ¼ ð2pÞ1 R 1
1 SðxÞeixsdx has a direct implication
on the units of SðxÞ and the scaled FT XTðxÞ. The PSD SðxÞ here has unit V2=Hz,
not V2=ðrad=sÞ, despite the fact that the integral is over x (rad/s). Correspondingly,
XTðxÞ has unit V=
ﬃﬃﬃﬃﬃﬃ
Hz
p
, not V=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
rad/s
p
. If one uses RðsÞ ¼
R 1
1 SðxÞeixsdx then
SðxÞ must have unit V2=ðrad=sÞ and XTðxÞ has unit V=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
rad=s
p
; and vice versa.
The same rule applies to the sample counterparts.
4.11.2
One-Sided Versus Two-Sided Spectrum
The PSDs in this chapter have been deﬁned in a ‘two-sided’ manner, where the
frequency x takes on both positive and negative values. In the Parseval equality and
the inverse FT for the correlation function (Wiener-Khinchin formula), the inte-
gration domain covers both positive and negative frequencies. The two-sided
0
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
2
2.5
3
y
pY(y)
1
2
4
10
M = 40
Fig. 4.14 PDF of Y ¼ ^Ek=Ek for different number of segments M
174
4
Spectral Analysis of Stationary Stochastic Process

Table 4.2 Summary of sample estimators, their units and expectation; DNðuÞ ¼ sinðNpuÞ=N sin pu is the Dirichlet kernel
Type
Statistic
Expectation
Theoretical
RðsÞ
|ﬄ{zﬄ}
V2
¼ E½ xðt þ sÞ
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
V
xðtÞT
|ﬄ{zﬄ}
V

(Sect. 4.1)
SðxÞ
|ﬄ{zﬄ}
V2=Hz
¼ lim
T!1 E½ XTðxÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
V= ﬃﬃﬃﬃ
Hz
p
XTðxÞ
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
V= ﬃﬃﬃﬃ
Hz
p

XTðxÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
V= ﬃﬃﬃﬃ
Hz
p
¼
1ﬃﬃﬃﬃ
T
p
|{z}
1= ﬃﬃs
p
RT
0
xðtÞ
|{z}
V
eixt dt
|{z}
s
(Sect. 4.2)
Sample
xðtÞ : 0  t  T
f
g
continuous time of duration T
^RTðsÞ
|ﬄﬄ{zﬄﬄ}
V2
¼
1
T
|{z}
1=s
R T
0 xðt þ sÞ
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
V
xðtÞT
|ﬄ{zﬄ}
V
dt
|{z}
s
0  s\T
ð
Þ
(Sect. 4.4.1)
E ^RTðsÞ


¼ 1  s=T
ð
ÞRðsÞ
(Sect. 4.7.1)
^STðxÞ
|ﬄﬄ{zﬄﬄ}
V2=Hz
¼ XTðxÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
V= ﬃﬃﬃﬃ
Hz
p
XTðxÞ
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
V= ﬃﬃﬃﬃ
Hz
p
(Sect. 4.4.2)
E ^STðxÞ
h
i
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
V2=Hz
¼
1
2p
|{z}
1=rad
R 1
1 S x0
ð
Þ
|ﬄﬄ{zﬄﬄ}
V2=Hz
FT x  x0
ð
Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
s
dx0
|{z}
rad=s
FTðxÞ ¼ 4 sin2ðxT=2Þ=x2T
(Sect. 4.7.2)
(continued)
4.11
Summary of Fourier Formulas, Units and Conventions
175

Table 4.2 (continued)
Type
Statistic
Expectation
Sample
xj ¼ xðjDtÞ

N1
j¼0
discrete-time at interval Dt and duration NDt
^Rr
|{z}
V2
¼ 1
N
P
N1r
j¼0
xj þ r
|ﬄ{zﬄ}
V
xT
j
|{z}
V
0  r  N  1
ð
Þ
(Sect. 4.5.1)
E ^Rr


¼ 1  r=N
ð
ÞRðrDtÞ
(Sect. 4.7.1)
^SNðxÞ
|ﬄﬄ{zﬄﬄ}
V2=Hz
¼ ^XðxÞ
|ﬄﬄ{zﬄﬄ}
V= ﬃﬃﬃﬃ
Hz
p
^XðxÞ
|ﬄﬄ{zﬄﬄ}
V= ﬃﬃﬃﬃ
Hz
p
^XðxÞ
|ﬄﬄ{zﬄﬄ}
V= ﬃﬃﬃﬃ
Hz
p
¼
ﬃﬃﬃﬃﬃﬃ
Dt
N
r
|ﬄﬄ{zﬄﬄ}
ﬃﬃs
p
P
N1
j¼0
xj
|{z}
V
eixjDt
(Sect. 4.5.2)
E ^SNðxÞ
h
i
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
V2=Hz
¼
1
2p
|{z}
1=rad
R 1
1 S x0
ð
Þ
|ﬄﬄ{zﬄﬄ}
V2=Hz
FN x  x0
ð
Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
s
dx0
|{z}
rad=s
FNðxÞ ¼ NDtD2
NðuÞ
u ¼ xDt=2p
(Sect. 4.7.2)
^SNða; bÞ
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
V2=Hz
¼ ^XðaÞ
|ﬄ{zﬄ}
V= ﬃﬃﬃﬃ
Hz
p
^XðbÞ
|ﬄﬄ{zﬄﬄ}
V= ﬃﬃﬃﬃ
Hz
p
(Sect. 4.8.1)
E½^SNða; bÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
V2=Hz
¼
1
2p
|{z}
1=rad
R 1
1 SðxÞ
|ﬄ{zﬄ}
V2=Hz
FabðxÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
s
dx
|{z}
rad=s
FabðxÞ ¼ NDtDNðua  uÞDNðub  uÞeipðN1ÞðuaubÞ
u ¼ xDt=2p; ua ¼ aDt=2p; ub ¼ bDt=2p
(Sect. 4.8.2)
176
4
Spectral Analysis of Stationary Stochastic Process

deﬁnition stems mathematically from the FTs. For a scalar process, since the PSD
for negative frequencies is just the mirror image of the PSD for positive frequen-
cies, it is sufﬁcient in applications to consider positive frequencies only. In terms of
this ‘one-sided’ PSD, the integration domain of the Parseval equality and the
inverse FT for the correlation function only covers positive frequencies. To
maintain correctness of these theoretical relationships, the one-sided PSD is deﬁned
as twice of the two-sided PSD. In the study of input-output relationship of a system,
whether the PSD is deﬁned in a one-sided or two-sided manner does not affect the
derived results, as long as their inputs and outputs are consistently deﬁned. That is,
when the input is one-sided, so is the output; and vice versa.
References
Ang AHS, Tang WH (2007) Probability concepts in engineering: emphasis on applications in civil
and environmental engineering. Wiley, New York
Bendat JS, Piersol AG (1993) Engineering applications of correlation and spectral analysis. Wiley,
New York
Brillinger DR (1981) Time series: data analysis and theory. Holden-Day Inc, San Francisco
Cramer H, Leadbetter MR (1962) Stationary and related stochastic processes. Wiley, New York
Kay S (1993) Fundamentals of statistical signal processing: estimation theory. Prentice-Hall,
Englewood Cliffs, NJ
Papoulis A (1991) Probability, random variables and stochastic processes. McGraw-Hill,
Singapore
Ross SM (1996) Stochastic process. John Wiley & Sons, New York
Ross SM (2011) Introduction to probability models. Academic Press, Oxford
Welch PD (1967) The use of fast fourier transform for the estimation of power spectra: a method
based on time averaging over short, modiﬁed periodograms. IEEE Trans Audio Electroacoust
AU-15(2):70–73
Yaglom AM (1989) Basic results. In: Correlation theory of stationary and related random
functions, vol 1. Springer, NY
4.11
Summary of Fourier Formulas, Units and Conventions
177

Chapter 5
Stochastic Structural Dynamics
Abstract This chapter analyzes the statistical response of a linear-elastic structure
subjected to unknown excitations modeled by a stationary stochastic process. It is a
probabilistic version of Chap. 3. Both stationary and transient response statistics are
analyzed. For stationary statistics, a modal superposition approach in the frequency
domain is adopted. For transient statistics, a state-space approach in the time
domain is adopted. Although transient statistics are of less relevance in operational
modal analysis, they are discussed for better appreciation of the meaning of sta-
tionary statistics. Their large time limits are the stationary statistics.
Keywords Response variance  Transient statistics  Lyapunov equation
If we know the properties of a structure, its initial conditions and excitation time
history at all DOFs, then theoretically by solving the equation of motion we can
predict the response as a function of time. This is a (deterministic) structural
dynamics problem, as discussed in Chap. 3. In that context there is no uncertainty
associated with the response, even when its time history may look erratic.
If the excitation is unknown then logically we cannot predict the response.
Nevertheless, if we are willing to make some ‘probabilistic assumptions’ about the
excitation then it is possible to predict the response in a probabilistic manner.
Instead of predicting what its value will be (which is not possible), we aim at
predicting the statistical properties (e.g., mean, variance) or even the probability
distribution of the response (generally a more difﬁcult task). This is a rational
approach for studying the ambient vibration of structures because the excitation
sources (e.g., wind, microtremor and cultural activities) are so abundant that it is
impractical to measure them all or to make assumptions about what their time
histories are in a precise manner.
In this chapter we discuss the statistical properties of the response of a
linear-elastic time-invariant structure subjected to unknown excitations modeled as
a stationary stochastic process. This belongs to the subject of ‘random vibration
analysis’ or ‘stochastic structural dynamics’. Comprehensive references are avail-
able, e.g., Lin (1967), Soong and Grigoriu (1993) and Lutes and Sarkani (1996).
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_5
179

In a general context, this subject can be technically involved, e.g., in the existence
of limits of integrals and derivatives of stochastic processes. These issues are treated
formally in the subject of ‘stochastic calculus’, which has wide applications in
science and engineering; see, e.g., Kloeden and Platen (1992) and Grigoriu (2002).
We assume that the excitation is ‘well-behaved’ so that the mathematical operations
involved are valid. This has traded off simplicity with mathematical rigor but is
sufﬁcient for practical purposes.
Analogous to the deterministic case, for a structure under stochastic stationary
excitation, the response statistics generally comprise a transient and a stationary
component. We ﬁrst study the stationary response statistics in Sects. 5.1 and 5.2.
They are the main target in applications and are easier to analyze. Transient
statistics are studied later in Sects. 5.3 and 5.4. They are of secondary importance in
operational modal analysis (OMA) but their discussion helps clarify the meaning of
stationary statistics.
The response statistics can be obtained in different ways, e.g., by modal
superposition or state-space approach, in the time or frequency domain. When
studying stationary response statistics we adopt modal superposition in the fre-
quency domain. It yields insightful expressions often used in applications.
A time-domain state-space approach is adopted when studying transient response
statistics. See Sect. 5.5 for a quick summary of the theories in this chapter and their
connections.
5.1
Stationary SDOF Response
Consider a single-degree-of freedom (SDOF) structure with natural frequency
x1 (rad/s), damping ratio f and subjected to excitation wðtÞ:
€xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ ¼ wðtÞ
ð5:1Þ
Generally, the response xðtÞ is a sum of contributions from initial conditions and
excitation. When wðtÞ is harmonic (i.e., sine or cosine), we learnt in Sect. 3.1.5 that
as time goes on the response will approach a ‘steady state’ where the transience due
to initial conditions has died out, leaving the excitation effect in the steady-state
response. An analogous result holds for the response statistics when wðtÞ is a
stationary stochastic process. Initially, they change with time but as time goes on
they converge to a constant (stationary) value. At the stationary state, the correlation
function RðsÞ ¼ E½xðt þ sÞxðtÞ depends only on the time lag s but not the time shift
t. By the Wiener-Khinchin formula (Sect. 4.4.3), the Fourier Transform (FT) of this
correlation function gives the power spectral density (PSD). Instead of going
through the correlation function, we will obtain the PSD directly from the scaled FT
of the equation of motion, which is algebraically easier.
180
5
Stochastic Structural Dynamics

5.1.1
Scaled Fourier Transform
By deﬁnition, the PSD of xðtÞ is SxðxÞ ¼ E½F xðxÞF xðxÞ where
F xðxÞ ¼ lim
T!1
1ﬃﬃﬃﬃ
T
p
ZT
0
xðtÞeixtdt
ð5:2Þ
is the scaled FT of xðtÞ. It can be expressed in terms of the scaled FT of wðtÞ.
Taking scaled FT on both sides of (5.1),
F€xðxÞ þ 2fx1F _xðxÞ þ x2
1F xðxÞ ¼ F wðxÞ
ð5:3Þ
where F _xðxÞ, F €xðxÞ and F wðxÞ denote the scaled FTs of _xðtÞ, €xðtÞ and wðtÞ,
respectively. The scaled FTs F _xðxÞ and F €xðxÞ can be expressed in terms of
F xðxÞ:
F _xðxÞ ¼ ixF xðxÞ
F €xðxÞ ¼ ixF _xðxÞ ¼ x2F xðxÞ
ð5:4Þ
Substituting into (5.3) and rearranging gives
F xðxÞ ¼ gðxÞF wðxÞ
ð5:5Þ
where
gðxÞ ¼ x2
1  x2 þ 2fxx1i

1¼ x2
1
1  b2 þ 2fbi

1
b ¼ x
x1
ð5:6Þ
is the ‘transfer function’ between xðtÞ and wðtÞ. Substituting (5.5) into (5.4),
F _xðxÞ ¼ ixgðxÞF wðxÞ
F€xðxÞ ¼ x2gðxÞF wðxÞ
ð5:7Þ
Transfer Function of Response Acceleration
The scaled FTs F wðxÞ and F€xðxÞ have the same unit and so gðxÞ has the same
unit as x2. When working with acceleration (often measured in OMA) it is more
convenient to express F€xðxÞ as
F€xðxÞ ¼ hðxÞF wðxÞ
ð5:8Þ
where
hðxÞ ¼ 1  b2  2fbi

1
b ¼ x1
x
ð5:9Þ
5.1
Stationary SDOF Response
181

is the (dimensionless) transfer function between €xðtÞ and wðtÞ. Note that in the
expression of hðxÞ the frequency ratio b is deﬁned as x1=x rather than x=x1. In
terms of hðxÞ,
F xðxÞ ¼ x2hðxÞF wðxÞ
F _xðxÞ ¼ ix1hðxÞF wðxÞ
ð5:10Þ
5.1.2
Power Spectral Density
Using the scaled FTs of xðtÞ, _xðtÞ and €xðtÞ in (5.8) and (5.10), their PSDs are given
by
SxðxÞ ¼ E½F xF 
x ¼ x4DðxÞSwðxÞ
S_xðxÞ ¼ E½F _xF 
_x ¼ x2DðxÞSwðxÞ
S€xðxÞ ¼ E½F€xF 
€x ¼ DðxÞSwðxÞ
ð5:11Þ
where SwðxÞ ¼ E½F wF 
w is the PSD of wðtÞ;
DðxÞ ¼ hðxÞ
j
j2¼ ð1  b2Þ2 þ ð2fbÞ2
h
i1
b ¼ x1
x
ð5:12Þ
is the dynamic ampliﬁcation factor between the PSDs of €xðtÞ and wðtÞ.
Figure 5.1 shows a schematic plot of D versus b for 0\f\1. The square
root of D in terms of b has the same expression as the dynamic ampliﬁcation
factor
AðbÞ
in
harmonic
response
(Sect. 3.1.7)
but
now
b
is
deﬁned
differently.
0
β
4
/
1
~
β
2
2
2
4
1
~
)
1(
4
1
ζ
ζ
ζ
−
1
1
~
2
1
2
ζ
−
1
2
2
2
]
)
2
(
)
1
[(
−
+
−
=
ζβ
β
D
Fig. 5.1 Dynamic
ampliﬁcation factor
D between the PSDs of €xðtÞ
and wðtÞ. Asymptotic
expressions (after ‘*’) are
indicated for small damping
f  1
182
5
Stochastic Structural Dynamics

5.1.3
Response Variance
The intensity of a stochastic vibration response can be measured in terms of its
variance. At the stationary state this does not depend on time. Without loss of
generality, assume that wðtÞ has zero mean. Taking expectation on the equation of
motion (5.1) shows that xðtÞ, _xðtÞ and €xðtÞ also have zero mean. Their variance is then
equal to the expectation of their square. Using the Parseval equality (Sect. 4.4.4),
var½x ¼ E½x2 ¼ 1
2p
Z1
1
SxðxÞdx ¼ 1
2p
Z1
1
x4DðxÞSwðxÞdx
var½_x ¼ E½_x2 ¼ 1
2p
Z1
1
S_xðxÞdx ¼ 1
2p
Z1
1
x2DðxÞSwðxÞdx
var½€x ¼ E½€x2 ¼ 1
2p
Z1
1
S€xðxÞdx ¼ 1
2p
Z1
1
DðxÞSwðxÞdx
ð5:13Þ
The covariance between any pair of xðtÞ, _xðtÞ and €xðtÞ can also be obtained
through the Parseval equality. For example,
E½x_x ¼ 1
2p
Z1
1
Sx_xðxÞdx
ð5:14Þ
where
Sx_xðxÞ ¼ E½F xðxÞF _xðxÞ
ð5:15Þ
is the cross PSD between xðtÞ and _xðtÞ. Using F _xðxÞ ¼ ixF xðxÞ,
Sx_xðxÞ ¼ ixE½F xðxÞF xðxÞ ¼ ixSxðxÞ
ð5:16Þ
Substituting into (5.14) and noting SxðxÞ ¼ SxðxÞ gives
E½x_x ¼ 0
ð5:17Þ
That is, at the stationary state, displacement and velocity response are uncorrelated.
The same applies to velocity and acceleration response.
5.1
Stationary SDOF Response
183

5.1.4
Response to White Noise
Simpliﬁcation results when the excitation is white noise, where SwðxÞ is a constant.
In this case, the integrals for var½x and var½_x in (5.13) can be determined ana-
lytically (e.g., using ‘residue theorem’ in complex analysis, out of scope here),
giving
var½x ¼
Sw
4fx3
1
¼
Sw
32p3ff 3
1
var½_x ¼
Sw
4fx1
¼
Sw
8pff1
ð5:18Þ
where f1 ¼ x1=2p is the natural frequency in Hz. Note that var½_x ¼ x2
1var½x,
which is intuitive but not trivial from the integrals.
Theoretically, var½€x is unbounded for white noise excitation because DðxÞ ! 1
as x ! 1. This can also be reasoned from (5.1), where var½w is inﬁnite but var½x
and var½_x are bounded, forcing var½€x to be inﬁnite. The unbounded nature of var½€x
stems from the fact that white noise has a non-zero PSD no matter how high the
frequency is. In reality the PSD of a physical process decays to zero when the
frequency is sufﬁciently high, for which var½€x is bounded. One simple approxi-
mation, which is justiﬁed for small damping, is to take
var½€x  x2
1var½_x ¼ Swx1
4f
¼ pSwf1
2f
ð5:19Þ
Example 5.1 (SDOF variance to white noise) Consider the SDOF structure in
(5.1) with natural frequency x1 ¼ 2p rad/s, i.e., f1 = 1 Hz, damping ratio f ¼ 1%,
and subjected to white noise excitation with PSD Sw ¼ 0:04 ðlgÞ2=Hz (l ¼ 106,
g = 9.81 m=s2). The stationary variance of displacement is given by (5.18):
var½x ¼
ð0:04Þ
32p3ð0:01Þð1Þ3
ðlgÞ2=Hz
Hz3
"
#
¼ 4:031  103 ðlgÞ2
Hz4
"
#
¼ 0:3879 ðlmÞ2
Taking square root gives a standard deviation of 0.623 lm. Similarly, the standard
deviation of velocity is calculated to be 3.91 lm/s. Using (5.19), the standard
deviation of acceleration is approximately 24.6 lm=s2, i.e., 2.51 lg.
■
184
5
Stochastic Structural Dynamics

5.2
Stationary MDOF Response
Consider a classically damped multi-degree-of-freedom (MDOF) structure satis-
fying the equation of motion
M€xðtÞ þ C_xðtÞ þ KxðtÞ ¼ wðtÞ
ð5:20Þ
where xðtÞ, _xðtÞ and €xðtÞ are respectively the n  1 displacement, velocity and
acceleration response vector; M, C and K are respectively the n  n mass, damping
and stiffness matrix, all real-symmetric and positive-deﬁnite; n is the number of
DOFs. Recall from Sect. 3.2.1 that the natural frequencies fxign
i¼1 and mode
shapes fwign
i¼1 (n  1) of the structure are solutions of the eigenvalue problem:
Kwi ¼ x2
i Mwi
i ¼ 1; . . .; n
ð5:21Þ
The response is given by the sum of modal contributions:
xðtÞ ¼
X
n
i¼1
wigiðtÞ
ð5:22Þ
where fgiðtÞgn
i¼1 are the modal responses, each satisfying its own SDOF equation of
motion:
€giðtÞ þ 2fixi _giðtÞ þ x2
i giðtÞ ¼ piðtÞ
i ¼ 1; . . .; n
ð5:23Þ
fi and piðtÞ are respectively the modal damping ratio and modal force of the ith
mode, deﬁned such that
2fixi ¼ wT
i Cwi
wT
i Mwi
piðtÞ ¼ wT
i wðtÞ
wT
i Mwi
ð5:24Þ
Equation (5.22) can be written in matrix form:
xðtÞ
n1
¼ W
nn gðtÞ
n1
ð5:25Þ
where
W ¼ ½w1; . . .; wn
gðtÞ ¼
g1ðtÞ
..
.
gnðtÞ
2
64
3
75
ð5:26Þ
5.2
Stationary MDOF Response
185

5.2.1
Scaled Fourier Transform
Using (5.22), the scaled FT of xðtÞ is
F xðxÞ ¼
X
n
i¼1
wiF giðxÞ
ð5:27Þ
where F giðxÞ is the scaled FT of giðtÞ. Since each giðtÞ satisﬁes the SDOF equation
in (5.23), (5.10) can be applied to give:
F giðxÞ ¼ x2hiðxÞF piðxÞ
ð5:28Þ
where F piðxÞ is the scaled FT of piðtÞ; and
hiðxÞ ¼ 1  b2
i  2fibii

1
bi ¼ xi
x
ð5:29Þ
Substituting (5.28) into (5.27),
F xðxÞ ¼ x2 X
n
i¼1
wihiðxÞF piðxÞ
ð5:30Þ
In matrix form,
F xðxÞ ¼ x2W diag½hðxÞF pðxÞ
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
F €gðxÞ
ð5:31Þ
where
F pðxÞ ¼
F p1ðxÞ
..
.
F pnðxÞ
2
64
3
75 ¼ ðWTMWÞ1WTF wðxÞ
ð5:32Þ
F €gðxÞ ¼
F €g1ðxÞ
...
F €gnðxÞ
2
64
3
75 ¼
hiðxÞ
..
.
hnðxÞ
2
64
3
75
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
diag½hðxÞ
F p1ðxÞ
...
F pnðxÞ
2
64
3
75
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
F pðxÞ
ð5:33Þ
186
5
Stochastic Structural Dynamics

Using F _xðxÞ ¼ ixF xðxÞ and F €xðxÞ ¼ x2F xðxÞ,
F _xðxÞ ¼ ix1Wdiag½hðxÞF pðxÞ
F €xðxÞ ¼ Wdiag½hðxÞF pðxÞ
ð5:34Þ
5.2.2
Power Spectral Density
Using (5.31) and (5.34), the PSD matrices of xðtÞ, _xðtÞ and €xðtÞ are given by
SxðxÞ ¼ E½F xF 
x ¼ x4WHðxÞWT
S_xðxÞ ¼ E½F _xF 
_x ¼ x2WHðxÞWT
S€xðxÞ ¼ E½F €xF 
€x ¼ WHðxÞWT
ð5:35Þ
where
HðxÞ ¼ E½F €gðxÞF €gðxÞ ¼ diag½hðxÞSpðxÞdiag½hðxÞ
ð5:36Þ
and
SpðxÞ ¼ E½F pðxÞF pðxÞ ¼ ðWTMWÞ1WTSwðxÞWðWTMWÞ1
ð5:37Þ
is the PSD matrix of modal forces; SwðxÞ ¼ E½F wðxÞF wðxÞ is the PSD matrix
of wðtÞ. The ði; jÞ-entry of HðxÞ and SpðxÞ are (omitting dependence on x)
Hði; jÞ ¼ E½F €giF 
€gj ¼ hih
j Spði; jÞ
ð5:38Þ
Spði; jÞ ¼ E½F piF 
pj ¼
wT
i Swwj
ðwT
i MwiÞðwT
j MwjÞ
ð5:39Þ
Correlation Between Modal Forces
By orthogonality, WTMW and hence ðWTMWÞ1 are diagonal matrices. However,
the same need not be true for SpðxÞ because the modal forces of different modes
can be dependent, or equivalently, because WTSwW in (5.37) need not be a diagonal
matrix. Independence or uncorrelation among the excitations at different DOFs does
not carry over to the modal forces, and vice versa. The modal forces of different
modes can be correlated even if the excitations at different DOFs are uncorrelated.
Conversely they can be uncorrelated even if the excitations are correlated.
5.2
Stationary MDOF Response
187

5.2.3
Response Variance
Similar to the SDOF case, the stationary response covariance matrix of a MDOF
structure can be obtained by integrating the PSD matrix via the Parseval equality.
For displacement response, the covariance matrix is given by
E½xxT ¼ 1
2p
Z1
1
SxðxÞdx
ð5:40Þ
The ði; jÞ-entry of E½xxT is E½xixj. The covariance matrix between other quantity
pairs can also be obtained via the Parseval equality. For example,
E½x_xT ¼ 1
2p
Z1
1
Sx_xðxÞdx
ð5:41Þ
where
Sx_xðxÞ ¼ E½F xðxÞF _xðxÞ
ð5:42Þ
is the cross PSD between xðtÞ and _xðtÞ.
5.2.4
Mode Shape Scaling
The scaling of mode shape wi affects the scaling of the modal response giðtÞ and
modal force piðtÞ. If wi is multiplied by a factor, giðtÞ and piðtÞ will be divided by
the same factor. The modal force PSD Spi will be divided by the square of that
factor. These can be easily reasoned by noting that the physical response xðtÞ ¼
Pn
i¼1 wigiðtÞ is invariant to mode shape scaling. Let fcign
i¼1 be the factors applied to
fwign
i¼1. Then the modal force PSD matrix Sp0ðxÞ under the newly scaled mode
shapes fw0
i ¼ ciwign
i¼1 is given by
Sp0ðxÞ ¼
c1
1
..
.
c1
n
2
64
3
75SpðxÞ
c1
1
..
.
c1
n
2
64
3
75
ð5:43Þ
If we take the mode shapes fwign
i¼1 to be dimensionless then the modal
responses fgiðtÞgn
i¼1 will have the same unit as the physical displacement xðtÞ. If the
mass matrix M has a unit of kg and the excitation wðtÞ has a unit of N, then the
188
5
Stochastic Structural Dynamics

modal forces fpiðtÞgn
i¼1 will have a unit of N/kg and their PSD matrix SpðxÞ has a
unit of ðN=kgÞ2=Hz.
Example 5.2 (Modal force PSD, two-DOF shear building) Consider the
two-storied shear building in Fig. 5.2 with uniform ﬂoor mass m1 ¼ m2 = 1000
tons and interstory stiffness k1 ¼ k2 = 1000 kN/mm. The ﬁrst and second ﬂoor are
subjected to independent white noise excitations with PSDs S1 and S2, respectively.
Here we illustrate how the modal force PSD matrix depends on the scaling of mode
shapes.
The stiffness and mass matrix are given by
K ¼
k1 þ k2
k2
k2
k2


¼
2000
1000
1000
1000


 106 N=m
½

M ¼
m1
0
0
m2


¼
1000
0
0
1000


 103 kg
½

Solving the eigenvalue problem Kw ¼ x2Mw gives the natural frequencies and
mode shapes:
x1 ¼ 19:5 rad=s 3:11 Hz
ð
Þ
w1 ¼
0:5257
0:8507


x2 ¼ 51:2 rad=s 8:14 Hz
ð
Þ
w2 ¼
0:8507
0:5257


These mode shapes have been scaled to have unit norm, i.e.,
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
wT
i wi
q
¼ 1. Under
this scaling, the modal masses are calculated to be the same:
l1 ¼ wT
1Mw1 ¼ 106 kg
l2 ¼ wT
2Mw2 ¼ 106 kg
Since the excitations at the two ﬂoors are independent, their PSD matrix is
diagonal:
Sw ¼
S1
S2


story stiffness
)
(
1 t
w
)
(
2 t
w
)
(
1 t
x
)
(
2 t
x
1
k
2
m
1
m
story stiffness
2
k
Fig. 5.2 Two-DOF shear
building
5.2
Stationary MDOF Response
189

Coherence of Modal Forces
Suppose S1 ¼ 0:2ðkNÞ2=Hz and S2 ¼ 0:1ðkNÞ2=Hz. Using (5.37), the modal force
PSD matrix is calculated to be
Sp ¼
0:1276
0:0447
0:0447
0:1724


 106
ðN=kgÞ2=Hz
h
i
which
is
not
a
diagonal
matrix.
If
instead
S1 ¼ 0:1ðkNÞ2=Hz
and
S2 ¼ 0:1ðkNÞ2=Hz,
Sp ¼
0:1
0
0
0:1


 106 ðN=kgÞ2=Hz
h
i
which is now a diagonal matrix. Here, the modal force PSD matrix can be
non-diagonal simply because the excitations at different locations do not have the
same PSD.
Scaling of Modal Forces
Instead of scaling the mode shapes to have unit norm, suppose we scale them so
that the value at the second DOF is 1. The mode shapes under this scaling are
w0
1 ¼
0:618
1


w0
2 ¼
1:618
1


Check that w0
1 ¼ c1w1 and w0
2 ¼ c2w2 where c1 ¼ 1=0:8507 and c2 ¼ 1=0:5257.
The modal masses look very different now:
l0
1 ¼ w0T
1 Mw0
1 ¼ 1:382  106 kg
l0
2 ¼ w0T
2 Mw0
2 ¼ 3:618  106 kg
Check
that
l0
1 ¼ c2
1l1
and
l0
2 ¼ c2
2l2.
For
S1 ¼ 0:2ðkNÞ2=Hz
and
S2 ¼ 0:1ðkNÞ2=Hz, the modal force PSD matrix is
Sp0 ¼
0:0923
0:02
0:02
0:0476


 106
ðN=kgÞ2=Hz
h
i
Check that
Sp0 ¼
c1
1
c1
2


Sp c1
1
c1
2


Ambient Vibration Level
The modal force PSDs in this example may look small but in fact they are sufﬁcient
to create ambient vibrations large enough for OMA. Assuming a damping ratio of
f ¼ 1%, the PSD of the acceleration at the roof at the ﬁrst mode frequency is about
190
5
Stochastic Structural Dynamics

w2
21Sp1
4f2
¼ ð0:8507Þ2ð0:1276  106ÞðN=kgÞ2=Hz
ð4Þð0:01Þ2
¼ 0:232  103ðm=s2Þ2=Hz
 2:4  106ðlgÞ2=Hz
since
1 m=s2 ¼ ð106=9:81Þ lg.
In
terms
of
root
PSD,
this
is
about
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2:4  106
p
 155 lg/
ﬃﬃﬃﬃﬃﬃ
Hz
p
, which is well above the noise level of many
servo-accelerometers (in the order of 1 lg/
ﬃﬃﬃﬃﬃﬃ
Hz
p
).
■
5.3
Transient Response Variance
The response variance derived in Sects. 5.1 and 5.2 applies when the response is in
a statistically stationary state where transient effects have died out. In this section
we consider the transient variation of the response variance when the structure is
subjected to stochastic stationary excitations. The next section considers the tran-
sience in the correlation. Generally, transience in response statistics occurs when
they are not equal to their stationary value. The latter can be seen as the limit for
large time. This and the next section may be skipped for readers primarily interested
in OMA, which is concerned with stationary response statistics.
A state-space approach (Sect. 3.6) is adopted for analysis, as it elegantly pro-
duces the response statistics for all quantities. Recall the MDOF equation of motion
in (5.20):
M€xðtÞ þ C_xðtÞ þ KxðtÞ ¼ wðtÞ
ð5:44Þ
It is ﬁrst converted to a state-space form:
_yðtÞ ¼ AyðtÞ þ BwðtÞ
yðtÞ
2n1
¼
xðtÞ
_xðtÞ


A
2n2n ¼
0n
In
M1K
M1C


B
2nn ¼
0n
M1


ð5:45Þ
The solution is
yðtÞ ¼ eAtyð0Þ þ
Zt
0
eAðtsÞBwðsÞds
ð5:46Þ
As in Sect. 5.2, assume that wðtÞ is a stationary stochastic process with zero
mean, PSD matrix SwðxÞ and correlation function RwðsÞ. The initial condition yð0Þ
can be either known or random, but is otherwise independent of wðtÞ. Using (5.46)
and noting E½wðsÞ ¼ 0, the expectation of yðtÞ is
5.2
Stationary MDOF Response
191

lðtÞ ¼ E½yðtÞ ¼ eAtE½yð0Þ
ð5:47Þ
The covariance matrix of yðtÞ is given by
E ½yðtÞ  lðtÞ½yðtÞ  lðtÞT

	
¼ E½yðtÞyðtÞT  lðtÞlðtÞT
ð5:48Þ
where the expression on the RHS can be obtained by expanding the product on the
LHS and evaluating the expression of the individual terms. It remains to determine
VðtÞ ¼ E½yðtÞyðtÞT
ð5:49Þ
Clearly,
Vð0Þ ¼ E½yð0Þyð0ÞT
ð5:50Þ
This is assumed to be given as initial condition. We shall derive the equation
governing the change of VðtÞ and then solve it together with this initial condition.
5.3.1
Governing Equation
Differentiating (5.49) w.r.t. t,
_VðtÞ ¼ E½_yðtÞyðtÞT þ E½yðtÞ_yðtÞT
ð5:51Þ
Post-multiplying
the state-space equation in (5.45) by yðtÞT
and taking
expectation,
E½_yðtÞyðtÞT ¼ AE½yðtÞyðtÞT þ BE½wðtÞyðtÞT ¼ AVðtÞ þ BE½wðtÞyðtÞT
ð5:52Þ
Substituting into (5.51) and noting VðtÞT ¼ VðtÞ,
_VðtÞ ¼ AVðtÞ þ VðtÞAT þ E½yðtÞwðtÞTBT þ fE½yðtÞwðtÞTBTgT
ð5:53Þ
To obtain E½yðtÞwðtÞT, post-multiply (5.46) with wðtÞT, take expectation and
note that yð0Þ and wðtÞ are independent. This gives
E½yðtÞwðtÞT ¼ eAtE½yð0Þ E½wðtÞT
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
0
þ
Zt
0
eAðtsÞB E½wðsÞwðtÞT
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
RwðtsÞT
ds
ð5:54Þ
192
5
Stochastic Structural Dynamics

Changing integration variable from s to u ¼ t  s gives
E½yðtÞwðtÞT ¼
Zt
0
eAuBRwðuÞTdu
ð5:55Þ
Substituting into (5.53) yields
_VðtÞ ¼ AVðtÞ þ VðtÞAT þ PðtÞ
ð5:56Þ
PðtÞ ¼
Zt
0
eAuBRwðuÞTBTdu þ
Zt
0
eAuBRwðuÞTBTdu
2
4
3
5
T
ð5:57Þ
5.3.2
Solution Procedure
Equation (5.56) is a ﬁrst order 2n  2n matrix ODE in VðtÞ. Taking the symmetry
of VðtÞ into account, in principle the equation can be converted to a ﬁrst order
vector ODE for the nð2n þ 1Þ lower (or upper) and diagonal entries of VðtÞ.
Obtaining the solution from such equation is not preferred because the dimension of
the system matrix (nð2n þ 1Þ by nð2n þ 1Þ) is unnecessarily large. A more accurate
and efﬁcient strategy (Hammarling 1982) is to ﬁrst determine
ZðtÞ ¼ Q1VðtÞQT
ð5:58Þ
(QT ¼ ðQ1ÞT) and then recover VðtÞ from
VðtÞ ¼ QZðtÞQT
ð5:59Þ
where Q is the eigenmatrix of A such that A ¼ QKQ1; K is the diagonal matrix of
eigenvalues of A. This works because each entry of ZðtÞ satisﬁes its own ODE, as
seen in the following.
Uncoupled ODEs for ZðtÞ
Substituting A ¼ QKQ1 into (5.56),
_VðtÞ ¼ QKQ1VðtÞ þ VðtÞQTKQT þ PðtÞ
ð5:60Þ
Pre-multiplying
by
Q1
and
post-multiplying
by
QT,
and
noting
that
Q1 _VðtÞQT ¼ _ZðtÞ,
5.3
Transient Response Variance
193

_ZðtÞ ¼ KZðtÞ þ ZðtÞK þ LðtÞ
ð5:61Þ
LðtÞ ¼ Q1PðtÞQT
ð5:62Þ
Equation (5.61) is still a matrix ODE but the entries of ZðtÞ now satisfy their
own ODE uncoupled from each other, because K is a diagonal matrix. The ði; jÞ-
entry of the equation reads:
_ZijðtÞ ¼ ðki þ kjÞZijðtÞ þ LijðtÞ
ð5:63Þ
where ZijðtÞ and LijðtÞ denote the ði; jÞ-entry of ZðtÞ and LðtÞ, respectively. The
initial condition for ZðtÞ is given by Zð0Þ ¼ Q1Vð0ÞQT. Note that PðtÞ is
symmetric and so is LðtÞ, i.e., LijðtÞ ¼ LjiðtÞ. This implies that ZijðtÞ and ZjiðtÞ
satisfy the same equation. This agrees with the fact that ZðtÞ is symmetric, as
evidenced from (5.58).
In summary, to determine VðtÞ ¼ E½yðtÞyðtÞT where yðtÞ satisﬁes (5.45) with
initial condition Vð0Þ ¼ E½yð0Þyð0ÞT,
1. Determine the eigenvalues fkig2n
i¼1 and eigenmatrix Q of A.
2. Determine the initial condition Zð0Þ ¼ Q1Vð0ÞQT.
3. Solve ZijðtÞ from (5.63) for i ¼ 1; . . .; n; j 	 i, with initial condition Zijð0Þ
from the ði; jÞ-entry of Zð0Þ. Set ZjiðtÞ ¼ ZijðtÞ.
4. Obtain VðtÞ ¼ QZðtÞQT.
5.3.3
Limiting Stationary Value
According to Sect. 3.6.2, if the structure is stable (all xi [ 0) with positive
damping (all fi [ 0), the eigenvalues of A will all have a negative real part. Then
VðtÞ will converge to a bounded value as t ! 1. This limiting value should
coincide
with
the
stationary
response
variance
derived
in
Sect. 5.2.
Let
V1 ¼ limt!1 VðtÞ. Setting _VðtÞ ¼ 0 (stationary) in (5.56) gives
AV1 þ V1AT þ P1 ¼ 0
ð5:64Þ
P1 ¼
Z1
0
eAuBRwðuÞTBTdu þ
Z1
0
eAuBRwðuÞTBTdu
2
4
3
5
T
ð5:65Þ
194
5
Stochastic Structural Dynamics

Equation (5.64) is known as the ‘Lyapunov equation’. The solution is
V1 ¼ QZ1QT
ð5:66Þ
where Z1 ¼ limt!1 ZðtÞ. Setting _ZijðtÞ ¼ 0 and solving (5.63) gives the ði; jÞ-entry
of Z1:
Z1ði; jÞ ¼  L1ði; jÞ
ki þ kj
L1 ¼ Q1P1QT
ð5:67Þ
5.3.4
Response to White Noise
Consider the case when wðtÞ is white noise with covariance matrix
RwðsÞ ¼ SwdðsÞ
ð5:68Þ
where dðsÞ is the Dirac Delta function and Sw is the (constant) PSD matrix of wðtÞ.
Check that the FT of RwðsÞ is Sw. Also, Sw is real symmetric because Rwð0Þ is.
Substituting (5.68) into (5.57) and noting that the integrals only cover the right half
of the Delta function,
PðtÞ ¼ BSwBT
ð5:69Þ
Substituting into (5.62),
LðtÞ ¼ Q1BSwBTQT
ð5:70Þ
Here, PðtÞ and LðtÞ do not depend on t and so they are equal to their limiting
values, P1 and L1, respectively. Equation (5.63) can be solved analytically, giving
ZijðtÞ ¼ eðki þ kjÞtZijð0Þ  L1ði; jÞ
ki þ kj
1  eðki þ kjÞt
h
i
ð5:71Þ
As a check, setting t ¼ 0 on the RHS gives Zijð0Þ. Differentiating the RHS w.r.t.
t gives the RHS of (5.63). Taking t ! 1 gives Z1ði; jÞ in (5.67), as the expo-
nential terms tend to zero.
Example 5.3 (First order process under white noise, transient variance)
Consider a scalar process yðtÞ satisfying
_yðtÞ þ ayðtÞ ¼ wðtÞ
yð0Þ ¼ 0
ð5:72Þ
where a [ 0 and wðtÞ is white noise with PSD Sw and correlation function
RwðsÞ ¼ SwdðsÞ. The PSD and correlation function of yðtÞ at the stationary state
5.3
Transient Response Variance
195

was obtained in Example 4.1 of Sect. 4.4.5. Here we obtain the transient variance
VðtÞ ¼ E½yðtÞ2. In the context of the state-space equation in (5.45), A ¼ a and
B ¼ 1. Then (5.69) gives PðtÞ ¼ Sw and (5.56) reads
_VðtÞ ¼ 2aVðtÞ þ Sw
ð5:73Þ
Solving this equation with initial condition Vð0Þ ¼ 0 gives
VðtÞ ¼ Sw
2a ð1  e2atÞ
ð5:74Þ
This is a monotonic increasing function of t. It converges to Sw=2a as t ! 1,
which agrees with the ﬁnding in Example 4.1.
■
Example 5.4 (SDOF under white noise, transient variance) Consider the SDOF
response
xðtÞ
satisfying
€xðtÞ þ 2fx1_xðtÞ þ x2
1xðtÞ ¼ wðtÞ,
starting
from
rest
(xð0Þ ¼ _xð0Þ ¼ 0) and subjected to white noise excitation wðtÞ with PSD Sw. In the
context of the state-space equation in (5.45),
A ¼
0
1
x2
1
2fx1


B ¼
0
1


ð5:75Þ
The eigenvalues and eigenmatrix of A were obtained in Example 3.8 of Sect. 3.6.2:
k1 ¼ fx1 þ ixd
k2 ¼ fx1  ixd
ð5:76Þ
Q ¼
1
1
fx1 þ ixd
fx1  ixd


Q1 ¼
1
2ixd
fx1  ixd
1
fx1  ixd
1


ð5:77Þ
where xd ¼ x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
p
. Equations (5.69) and (5.70) give
PðtÞ ¼ Sw 0
0
0
1


LðtÞ ¼ Sw
4x2
d
1
1
1
1


ð5:78Þ
Using (5.71) with Zijð0Þ ¼ 0 (since Vð0Þ ¼ 0) to obtain ZðtÞ and then substi-
tuting into VðtÞ ¼ QZðtÞQT gives, after algebra,
V11ðtÞ ¼
Sw
4fx3
1
1  e2fx1t
1  f2 1 þ f sinð2xdt  /Þ
½



tan / ¼
f
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
p
ð5:79Þ
V22ðtÞ ¼
Sw
4fx1
1  e2fx1t
1  f2 1  f sinð2xdt þ /Þ
½



ð5:80Þ
196
5
Stochastic Structural Dynamics

V12ðtÞ ¼
Swe2fx1t
2x2
1ð1  f2Þ sin2 xdt
ð5:81Þ
Note that V21ðtÞ ¼ V12ðtÞ. Taking t ! 1 gives the stationary covariance matrix:
E½x2
E½x_x
E½_xx
E½_x2


¼ lim
t!1
V11ðtÞ
V12ðtÞ
V21ðtÞ
V22ðtÞ


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
V1
¼
Sw
4fx3
1
0
0
Sw
4fx1
"
#
ð5:82Þ
This result could have been obtained directly from the solution of the Lyapunov
equation in (5.66). It also agrees with (5.18), which was derived in the frequency
domain using the Parseval equality.
Figure 5.3 shows the response statistics with time for x1 ¼ 2p rad/s (1 Hz),
f ¼ 1% and Sw ¼ 0:04ðlgÞ2=Hz as in Example 5.1. For large t they tend to a limiting
stationary value. In (5.79)–(5.81), since transient effect decays through the term
expð2fx1tÞ, the time it takes to approach near the stationary state is Oð1=fx1Þ.
Check from the ﬁgure that the limiting values of
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
E½x2
p
and
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
E½_x2
p
agree with
those found in Example 5.1, i.e., 0.623 lm and 3.91 lm/s, respectively.
■
5.4
Transient Response Correlation
We next consider the transience in the correlation of response. The context is the
same as the last section and analysis technique is similar. Analogous to (5.48), the
correlation function of yðtÞ at time lag s can be expressed as
0
2
4
6
8
10
12
14
16
18
20
0
0.2
0.4
0.6
0.8
(a) [μm]
0
2
4
6
8
10
12
14
16
18
20
0
1
2
3
4
(b) [μm/sec]
0
2
4
6
8
10
12
14
16
18
20
0
0.2
0.4
0.6
0.8
1
(c) corr.
Time t (sec)
Fig. 5.3 Transient response statistics of SDOF structure under white noise. a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
E½xðtÞ2
q
,
b
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
E½_xðtÞ2
q
, c correlation coefﬁcient = E½xðtÞ_xðtÞ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
E½xðtÞ2E½_xðtÞ2
q
5.3
Transient Response Variance
197

E ½yðt þ sÞ  lðt þ sÞ½yðtÞ  lðtÞT

	
¼ E½yðt þ sÞyðtÞT  lðt þ sÞlðtÞT ð5:83Þ
where lðtÞ ¼ E½yðtÞ as in (5.47). The main task is therefore to determine
RtðsÞ ¼ E½yðt þ sÞyðtÞT
ð5:84Þ
This function depends on both the time shift t and time lag s because the response is
non-stationary.
Transpose Mirror Property
For ﬁnite t\1, the transpose mirror property does not hold for RtðsÞ, i.e., gen-
erally RtðsÞ 6¼ RtðsÞT. Rather, for any t and s,
Rt þ sðsÞ ¼ RtðsÞT
ð5:85Þ
This holds because
Rt þ sðsÞ ¼ E½yðt þ s  sÞyðt þ sÞT ¼ E½yðt þ sÞyðtÞTT ¼ RtðsÞT
ð5:86Þ
Equation (5.85) reduces to the transpose mirror property as t ! 1, because then Rt
and Rt þ s both become R1.
5.4.1
Governing Equation
We ﬁrst derive the equation governing the change of RtðsÞ w.r.t. s. Note that _RtðsÞ ¼
E½_yðt þ sÞyðtÞT where the dot in _RtðsÞ denotes a derivative w.r.t. s. Consider the
state-space equation in (5.45) at t þ s, i.e., _yðt þ sÞ ¼ Ayðt þ sÞ þ Bwðt þ sÞ.
Post-multiplying by yðtÞT and taking expectation,
_RtðsÞ ¼ ARtðsÞ þ BE½wðt þ sÞyðtÞT
ð5:87Þ
Following a similar procedure in obtaining (5.55), it can be shown that
E½wðt þ sÞyðtÞT ¼
Zt
0
Rwðu þ sÞBTeATudu
ð5:88Þ
Substituting into (5.87) yields
_RtðsÞ ¼ ARtðsÞ þ PtðsÞ
t þ s 
 0
ð5:89Þ
PtðsÞ ¼
Zt
0
BRwðu þ sÞBTeATudu
ð5:90Þ
198
5
Stochastic Structural Dynamics

Equation (5.89) is a matrix ODE in RtðsÞ w.r.t. s. It is only valid for t þ s 
 0,
i.e., s 
  t, for otherwise the time argument t þ s in yðt þ sÞ and hence RtðsÞ ¼
E½yðt þ sÞyðtÞT is before time 0, when the initial condition yð0Þ is speciﬁed. The
solution is given by
RtðsÞ ¼
eAsVðtÞ þ
Rs
0
eAðssÞPtðsÞds
s 
 0
Vðt þ sÞeATjsj þ
Rjsj
0
Pt þ sðsÞTeATðjsjsÞds
t\s\0
8
>
>
>
<
>
>
>
:
ð5:91Þ
where VðtÞ ¼ E½yðtÞyðtÞT was determined in the last section.
In (5.91), the ﬁrst expression for s 
 0 is the Duhamel’s integral solution to
(5.89) starting with the initial condition Rtð0Þ ¼ VðtÞ. To show the second
expression, it can be deduced from (5.85) that RtðsÞ ¼ Rt þ sðjsjÞT for s\0. Since
the time lag jsj in Rt þ sðjsjÞ is positive, the ﬁrst expression in (5.91) can be applied.
Replacing s by jsj and t by t þ s, the expression gives
Rt þ sðjsjÞ ¼ eAjsjVðt þ sÞ þ
Zjsj
0
eAðjsjsÞPt þ sðsÞds
ð5:92Þ
Taking transpose and noting the symmetry of Vðt þ sÞ gives the second expression
in (5.91).
5.4.2
Limiting Stationary Value
Taking limit t ! 1 on (5.91), the correlation function at the stationary state is
given by
R1ðsÞ ¼
eAsV1 þ
Rs
0
eAðssÞP1ðsÞds
s 
 0
V1eATjsj þ
Rjsj
0
P1ðsÞTeATðjsjsÞds
s\0
8
>
>
>
<
>
>
>
:
ð5:93Þ
Here we show that the FT of R1ðsÞ gives the PSD matrix of yðtÞ, thereby con-
necting the large-time limit of the transient correlation function with the stationary
statistics via the Wiener-Khinchin formula.
PSD Directly From Stationary Statistics
By deﬁnition, the PSD of yðtÞ is SyðxÞ ¼ E½F yF 
y. Taking scaled FT on (5.45)
and using F _yðxÞ ¼ ixF yðxÞ gives, after rearranging,
5.4
Transient Response Correlation
199

F yðxÞ ¼ ðA  ixIÞ1BF wðxÞ
ð5:94Þ
where F wðxÞ is the scaled FT of wðtÞ and I is the identity matrix. Substituting
gives
SyðxÞ ¼ E½F yF 
y ¼ ðA  ixIÞ1BSwðxÞBTðAT þ ixIÞ1
ð5:95Þ
PSD from Large-Time Limit of Transient Correlation Function
On the other hand, taking limit t ! 1 of (5.89),
_R1ðsÞ ¼ AR1ðsÞ þ P1ðsÞ
 1\s\1
ð5:96Þ
Let F R1ðxÞ and F _R1ðxÞ denote respectively the FT of R1ðsÞ and _R1ðsÞ.
Taking FT of (5.96) and using F _R1ðxÞ ¼ ixF R1ðxÞ gives, after rearranging,
F R1ðxÞ ¼ ðA  ixIÞ1~P1ðxÞ
ð5:97Þ
where
~P1ðxÞ ¼
Z1
1
P1ðsÞeixsds
ð5:98Þ
is the FT of P1ðsÞ. Using (5.90) with t ! 1 and swapping the order of integration,
~P1ðxÞ ¼
Z1
0
B
Z1
1
Rwðu þ sÞeixsds
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
SwðxÞeixu
BTeATudu
¼ BSwðxÞBT
Z1
0
eðAT þ ixIÞudu
ð5:99Þ
where the inner integral was obtained by changing variable from s to s ¼ u þ s; and
SwðxÞ ¼
Z1
1
RwðsÞeixsds
ð5:100Þ
is the PSD of wðtÞ (Wiener-Khinchin formula). Note that
200
5
Stochastic Structural Dynamics

Z1
0
eðAT þ ixIÞudu ¼ ðAT þ ixIÞ1eðAT þ ixIÞu
1
0 ¼ ðAT þ ixIÞ1
ð5:101Þ
where we have used limu!1 eðAT þ ixIÞu ¼ 0, which holds because the eigenvalues
of A (and hence AT þ ixI) have a negative real part. Substituting into (5.99),
~P1ðxÞ ¼ BSwðxÞBTðAT þ ixIÞ1
ð5:102Þ
Substituting into (5.97),
F R1ðxÞ ¼ ðA  ixIÞ1BSwðxÞBTðAT þ ixIÞ1
ð5:103Þ
This is the same expression as in (5.95) and so the FT of R1ðsÞ is SyðxÞ.
5.4.3
Response to White Noise
When wðtÞ is white noise with a constant PSD matrix Sw and correlation function
RwðsÞ ¼ SwdðsÞ, it can be reasoned using (5.90) that PtðsÞ ¼ 0 for s [ 0 and Ptð0Þ
has a ﬁnite value. The integrals in (5.91) are then zero, giving
RtðsÞ ¼
eAsVðtÞ
s 
 0
Vðt þ sÞeATjsj
t\s\0

ð5:104Þ
where VðtÞ was obtained in Sect. 5.3.4. Taking limit t ! 1 gives the correlation
function at the stationary state:
R1ðsÞ ¼
eAsV1
s 
 0
V1eATjsj
s\0

ð5:105Þ
Example 5.5 (First order process under white noise, transient correlation)
Revisit Example 5.3 but now consider the transient correlation. Using (5.104) with
A ¼ a and VðtÞ ¼ ðSw=2aÞð1  e2atÞ from (5.74),
RtðsÞ ¼ E½yðt þ sÞyðtÞ ¼
Sw
2a easð1  e2atÞ
s 
 0
Sw
2a easð1  e2aðt þ sÞÞ
t\s\0

ð5:106Þ
Check that Rtð0Þ ¼ VðtÞ for both expressions. Also, RtðtÞ ¼ 0 for the second
expression, which agrees with RtðtÞ ¼ E½yð0ÞyðtÞ ¼ 0 since the initial condition
yð0Þ ¼ 0 was assumed.
5.4
Transient Response Correlation
201

Figure 5.4 shows a schematic plot of RtðsÞ. Clearly, it is not symmetric about
s ¼ 0. As t increases, the part for s\0 extends to the left, eventually becomes
symmetric to the part for s [ 0 as t ! 1.
■
Example 5.6 (SDOF under white noise, transient correlation) Revisit the SDOF
structure in Example 5.4 with natural frequency x1 ¼ 2p rad/s (1 Hz), damping
ratio f ¼ 1% and subjected to white noise excitation with PSD Sw ¼ 0:04ðlgÞ2=Hz
as before. The state vector is yðtÞ ¼ ½xðtÞ; _xðtÞT. By deﬁnition,
RtðsÞ ¼ E½yðt þ sÞyðtÞT ¼ E xðt þ sÞxðtÞ
xðt þ sÞ_xðtÞ
_xðt þ sÞxðtÞ
_xðt þ sÞ_xðtÞ


ð5:107Þ
This can be computed using (5.104). Recall from Example 3.8 in Sect. 3.6.2 that
eAs ¼
g1ðsÞ
g2ðsÞ
_g1ðsÞ
_g2ðsÞ


ð5:108Þ
where
g1ðsÞ ¼ efx1sðcos xds þ
f
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
p
sin xdsÞ
g2ðsÞ ¼ efx1s
xd
sin xds
ð5:109Þ
and xd ¼ x1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  f2
p
. Using (5.104), the entries of RtðsÞ can be obtained by
matrix-multiplying the entries of eAs and VðtÞ. For example, the (1,1)-entry of RtðsÞ
is given by
E½xðt þ sÞxðtÞ ¼
g1ðsÞV11ðtÞ þ g2ðsÞV12ðtÞ
s 
 0
g1ðjsjÞV11ðt þ sÞ þ g2ðjsjÞV12ðt þ sÞ
t\s\0

ð5:110Þ
where VijðtÞ denotes the ði; jÞ-entry of VðtÞ ¼ E½yðtÞyðtÞT, which was obtained in
Example 5.4. Other entries of RtðsÞ can be similarly obtained. Figure 5.5 shows
E½xðt þ sÞxðtÞ versus s. As t increases the part for negative s extends to the left,
tending to a mirror image of that for positive s. The value at s ¼ 0 is equal to
E½xðtÞ2, whose square root was plotted versus t in Fig. 5.3a.
■
0
τ
)
(t
V
t
−
Fig. 5.4 Schematic plot of RtðsÞ for ﬁrst order process under white noise
202
5
Stochastic Structural Dynamics

5.5
Summary of Theories and Connections
Figure 5.6 summarizes the theories in this chapter and their connections. Relevant
sections are indicated in brackets.
-20
-15
-10
-5
0
5
10
15
20
25
30
35
40
-0.5
-0.25
0
0.25
0.5
t = 5 sec
[(μm)2]
-20
-15
-10
-5
0
5
10
15
20
25
30
35
40
-0.5
-0.25
0
0.25
0.5
t = 10 sec
-20
-15
-10
-5
0
5
10
15
20
25
30
35
40
-0.5
-0.25
0
0.25
0.5
t = 20 sec
Time lag τ (sec)
Fig. 5.5 Transient correlation E½xðt þ sÞxðtÞ of SDOF structure under white noise
w
Kx
x
C
x
M
=
+
+
&
&&
x =
x
x
& =
=
]
[
]
[
]
[
*
*
*
x
x
x
x
x
x
x
x
x
x
S
S
S
&
&
&
&
&
E
E
E
=
=
=
π
ω
π
ω
π
ω
2
/
]
[
2
/
]
[
2
/
]
[
∫
∫
∫
∞
∞
−
∞
∞
−
∞
∞
−
=
=
=
d
E
d
E
d
E
T
T
T
x
x
x
x
S
x
x
S
x
x
S
xx
&
&
&
&
&
Bw
Ay
y
+
=
&
]
)
(
)
(
[
)
(
T
t
t
E
t
y
y
V
=
]
)
(
)
(
[
)
(
T
t
t
t
E
y
y
R
τ
τ
+
=
⎥⎦
⎤
⎢⎣
⎡
=
−
−
=
−
x
x
w
y
B
I
i
A
&
)
(
1
ω
⎥⎦
⎤
⎢⎣
⎡
=
=
x
x
x
x
x
x
y
y
y
S
S
S
S
S
&
&
&
]
[
*
E
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
=
= ∫
∞
∞
−
]
[
]
[
]
[
]
[
2
/
]
[
T
T
T
T
T
E
E
E
E
d
E
x
x
x
x
x
x
xx
S
yy
y
&
&
&
&
π
ω
)
(τ
∞
R
)
0
(
∞
R
∞
V
State-space
Physical space
Modal
Scaled
FT
Scaled
FT
Inverse FT
FT
t
↓
∞
t
↓
∞
=
=
t = 0
⇔
Parseval
equality
Parseval
equality
stationary
transient
[3.6]
[5.3]
[5.3.3]
[5.2.1]
[5.2.2]
[5.2.3]
[5.4]
0
=
⇐
τ
⎥⎦
⎤
⎢⎣
⎡
= x
x
y
&
⇔
[5.4.2]
⇔
[5.4.2]
Fig. 5.6 Summary of theories in different sections and their connections
5.5
Summary of Theories and Connections
203

References
Grigoriu M (2002) Stochastic calculus: applications in science and engineering. Springer, Boston
Hammarling SJ (1982) Numerical solution of the stable, non-negative deﬁnite Lyapunov equation.
IMA J Numerical Anal 2:303–325
Kloeden PE, Platen E (1992) Numerical solution of stochastic differential equations. Springer,
Berlin
Lin YK (1967) Probabilistic theory of structural dynamics. McGraw Hill, New York
Lutes DL, Sarkani S (1996) Stochastic analysis of structural and mechanical vibrations. Prentice
Hall, New Jersey
Soong TT, Grigoriu M (1993) Random vibration of mechanical and structural systems. Prentice
Hall, New Jersey
204
5
Stochastic Structural Dynamics

Chapter 6
Measurement Basics
Abstract This chapter introduces the typical hardware and software process that
leads to the measured ambient data in operational modal analysis. Common aspects
are discussed, including sensor/hardware noise, aliasing, quantization error and
synchronization. Basic concepts for calibrating measurement noise are also
discussed.
Keywords Data acquisition  Noise  Aliasing  Quantization  Synchronization 
Huddle test
The vibration data that we analyze on a computer is a series of digital signals
supposed to carry information about the motion of the target measurement point.
The data and the actual motion are never the same. Their difference is commonly
referred as ‘noise’. A basic understanding of how vibration signal is picked up
mechanically, transmitted electrically and eventually converted into digital form
allows one to appreciate the existence and nature of noise; and to devise methods to
reduce, model or account for it. This is the objective of this chapter.
We ﬁrst take an overview of a typical process that produces a digital time history
of ambient vibration data. Important aspects are then discussed individually. After
that we discuss how to calibrate the noise of a data channel.
6.1
Data Acquisition Process
A typical data acquisition (DAQ) process that produces the data recorded on a
digital computer is schematically shown in Fig. 6.1. For discussion purpose,
assume
that
the
sensor
is
an
accelerometer
producing
analogue
(i.e.,
continuous-time) voltage signal. Starting from the left, ambient excitation has a
broadband power spectral density (PSD) ①. The PSD ②of the resulting structural
acceleration response is equal to the product of the excitation PSD and structural
dynamic ampliﬁcation. It has a peak near the natural frequency due to resonance. At
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_6
205

very high frequencies it takes the same shape as the excitation PSD because
dynamic ampliﬁcation approaches a constant there. The sensor measures the
structural response, generating analogue data signal that is transmitted through
cable to the DAQ hardware. During transmission the analogue data is contaminated
with ‘noise’ arising from the sensor, cable or electronics in the DAQ hardware.
The PSD in ③is equal to the sum of the acceleration response PSD in ②and the
‘noise PSD’. To avoid aliasing, the DAQ hardware ﬁlters out frequency compo-
nents in the analogue data near and beyond the Nyquist frequency (half of the
sampling frequency). The resulting analogue data is digitized and subsequently
recorded on a computer. This is the ‘measured data’. Its PSD in ④is tapered off
near the Nyquist frequency. To supplement the above picture, some details are next
discussed individually.
6.2
Channel Noise
‘Channel noise’ refers to the difference between the measured (digital) data and the
actual vibration response being measured. It may be attributed to
1. random noise in sensor/hardware (Sect. 6.3),
2. distortion from anti-aliasing ﬁlter (Sect. 6.5),
3. quantization error of digitizer (Sect. 6.6),
These are discussed in the sections indicated. For proper instrumentation, all
sources should be kept small compared to the target signal. The ﬁrst source is
inevitable and its characteristics depend on the type of sensor/hardware. The second
and third source can be managed to a negligible level by proper choice of
equipment/sampling conﬁguration. One also needs to be vigilant against other
potential sources, e.g., ‘clipping’ (saturation/exceedance of measurement limit) and
[Hz]
[N2/Hz]
[Hz]
[g2/Hz]
[Hz]
[g2/Hz]
[Hz]
[g2/Hz]
Ambient
Excitation
Structural
response
Sensor
Cable
Data
[Hz]
[g2/Hz]
Noise
spectrum
fs/2
- Anti-aliasing
- Digitization
Analogue data
(voltage)
Digital data
(bits)
1
2
3
4
DAQ
Fig. 6.1 Typical data acquisition process. PSD shown next to each quantity. For illustration,
acceleration measurement is assumed. The dashed line in ③and ④is the PSD of the target in ②;
fs is the sampling rate in Hz
206
6
Measurement Basics

electrical/magnetic interference from the test environment. Managed properly,
channel noise is typically dominated by sensor/hardware noise.
6.3
Sensor/Hardware Noise
Sensor/hardware noise refers to the noise in the analogue data before it is digitized.
After digitization the data can be practically considered to be transmitted without
noise; or it will not be successfully transmitted at all. The noise can come from
sensor, cable or DAQ hardware. Sensor/hardware noise can be considered a sta-
tionary stochastic process independent of the response being measured. Its strength
is commonly characterized by the PSD, i.e., the ‘noise spectrum’. The PSD can
differ from one data channel to another, although it tends to be similar for sensors or
DAQ hardware of the same model. For example, the three data channels of a triaxial
accelerometer usually have a similar noise spectrum.
Fig. 6.2 shows the root PSD calculated from 15 min of horizontal acceleration
of a single-degree-of-freedom (SDOF) structure under laboratory environment by
different sensors (with different DAQ hardware), namely, servo-accelerometer,
piezoelectric accelerometer, MEMS (Micro-Electro-Mechanical Sensor) and the
MEMS on a smart phone.
The square root of PSD is plotted as it scales linearly with the unit of the
measured quantity (g = 9.81 m/s2 in this case). The dashed line illustrates what the
ideal measured spectrum (②in Fig. 6.1) would be if there were no channel noise. It
has a resonance peak at the natural frequency (about 7.5 Hz). At low frequencies
the spectra of measured data are all dominated by noise. The servo-accelerometer
has the smallest noise PSD, less than 1 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
. Its spectrum captures quite well
the target near and off the resonance peak. The piezoelectric and MEMS sensor can
also do so, although the band dominated by response is narrower. For the sensor on
smart phone, the resonance peak appears to be ‘drowned’ by noise. In terms of PSD
0
2
4
6
8
10
12
14
16
10
-7
10
-6
10
-5
10
-4
10
-3
[g/√Hz]
Frequency [Hz]
Servo
Piezo
MEMS
Smart phone (MEMS)
Ideal
Smart
phone
Piezo
MEMS
Servo
Fig. 6.2 One-sided root PSD (averaged) of acceleration measured on a SDOF structure by
different sensors. Dashed line illustrates the ideal measured acceleration if there were no channel
noise
6.2
Channel Noise
207

at the natural frequency, the signal-to-noise (s/n) ratios for Servo, Piezo, MEMS
and Smart phone are in the order of 10000, 1000, 100 and 1, respectively. In the
general case, servo-accelerometers tend to have the smallest noise (especially for
low frequencies) among the types of sensors here; but the ordering between
piezoelectric and MEMS sensors do depend on the speciﬁc model.
Physical Limits
Noise is inevitable as a matter of nature. Sensing elements converting mechanical
motion to electrical signal are subjected to mechanical-thermal noise associated
with disturbance at molecular level (Brownian motion). The electronic noise in
DAQ hardware is attributed to a variety of sources with different spectral charac-
teristics. For example, thermal (Johnson-Nyquist) noise is associated with the
random thermal motion of electrons and its PSD is approximately constant with
frequency, i.e., white; ‘ﬂicker’ noise is associated with the spatial ﬂuctuation of
electronic properties and its PSD is inversely proportional to frequency, i.e., ‘pink’.
The PSD of noise discussed above increases with the absolute temperature. See
Gabrielson (1993), Kogan (1996) and Levinson (2004).
Equipment Speciﬁcation
Nominal information about the noise spectrum can be obtained from equipment
speciﬁcations provided by manufacturer. In the absence of other information, this
can be the main source for decision making at the procurement stage. Common
formats are in terms of the noise PSD applicable over a certain frequency band, or
RMS (root-mean-square) value. For the latter, attention should be paid to the fre-
quency band over which the RMS is calculated; the wider the band, the higher the
RMS. An average value for the noise PSD can be back-calculated as the square of
RMS value divided by the bandwidth (Parseval equality).
Sensitivity and Signal-to-Noise Ratio
One common measure of data quality is the ‘signal-to-noise’ (s/n) ratio. Clearly, it
can be enhanced by increasing the signal or reducing the noise. In ambient tests the
level of vibration cannot be actively controlled but the measured analogue signal
can be enhanced by using a sensor with a higher sensitivity. The sensitivity of
accelerometers is often measured in V/g. This is more than just a factor for scaling
data from V to g. A sensor with a higher sensitivity generates a higher voltage
signal for the same level of vibration. For acceleration data, the s/n ratio in terms of
PSD is given by
s=n ratio ¼ ðsensor sensitivity; V=gÞ2  ðacceleration response PSD; g2=HzÞ
channel noise PSD; V2=Hz
ð6:1Þ
This can also be written as
s=n ratio ¼ acceleration response PSD; g2=Hz
channel noise PSD; g2=Hz
ð6:2Þ
208
6
Measurement Basics

where the channel noise PSD in g2=Hz is equal to that in V2=Hz divided by the
square of sensor sensitivity in V/g. It is possible to enhance the sensitivity by
amplifying the analogue signal, although the additional hardware can also con-
tribute to channel noise. Strictly speaking, the s/n ratio deﬁned in terms of PSD is
frequency-dependent. In practice, a representative value over a certain frequency
range of interest may be reported for convenience.
As an example, in Fig. 6.2 the root PSD of structural acceleration at the natural
frequency is about 200 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
. According to manufacturer speciﬁcations, the
piezoelectric sensor has sensitivity 0.5 V/g and noise 6 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
near that fre-
quency. The DAQ hardware has noise 1 lV=
ﬃﬃﬃﬃﬃﬃ
Hz
p
, equivalent to 1/0.5 = 2
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
. Neglecting the contribution from cable and assuming that the sensor and
DAQ hardware noise are uncorrelated, the channel noise has a PSD of
62 þ 22 ¼ 40 (lg)2/Hz. The s/n ratio in terms of PSD (near the natural frequency) is
then (200)2/40 = 1000. Here, the DAQ hardware contributes to 10% of noise PSD.
For lower frequencies this contribution can be higher or even dominate.
6.4
Sensor Principle
The characteristics of a sensor are attributed to its measuring principle. A basic
understanding of the latter allows one to choose the appropriate type of sensor
depending on application. In a basic setup, the motion to be measured causes
deformation in the sensing element, which in turn induces an electrical signal
carrying information of the motion. To see the relationship between the deformation
and input motion, consider the idealized SDOF system of a sensor in Fig. 6.3.
For discussion purpose, suppose we want to measure the acceleration €xgðtÞ of the
moving base on which the sensor is placed. The relative displacement uðtÞ (or its
derivatives) of the proof mass can be detected by electro-mechanical means. For
example, when subjected to dynamic stress (hence deformation), piezoelectric
materials produce a charge/voltage. In capacitive accelerometers, the motion of the
proof mass changes the distance between charged surfaces and hence electrical
capacitance, which results in a voltage proportional to acceleration.
To see the relationship between uðtÞ and €xgðtÞ, apply Newton’s second law on
the proof mass to give
)
(t
u
Moving base
(motion to be measured)
Fixed
reference
)
(t
xg
)
(t
x
Lateral
stiffness k
Proof mass m
Sensor
Damping
constant c
Fig. 6.3 Schematic diagram
of the mechanical part of a
sensor
6.3
Sensor/Hardware Noise
209

m€xðtÞ
inertia
þ c_uðtÞ
damping
þ kuðtÞ
stiffness
¼ 0
ð6:3Þ
The inertia term is proportional to €xðtÞ (not €uðtÞ) but the damping and stiffness terms
are still proportional respectively to _uðtÞ and uðtÞ because they arise from defor-
mation. Substituting €xðtÞ ¼ €xgðtÞ þ €uðtÞ and rearranging gives the equation for uðtÞ:
m€uðtÞ þ c_uðtÞ þ kuðtÞ ¼ m€xgðtÞ
ð6:4Þ
In terms of the natural frequency x1 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
k=m
p
(rad/s) and damping ratio
f ¼ c=2
ﬃﬃﬃﬃﬃﬃ
km
p
,
€uðtÞ þ 2fx1 _uðtÞ þ x2
1uðtÞ ¼ €xgðtÞ
ð6:5Þ
It is more convenient to study the relationship between uðtÞ and €xgðtÞ in the
frequency domain. Taking Fourier Transform (FT, denoted by ‘F’) on (6.5), using
F _uðxÞ ¼ ixF uðxÞ and F €uðxÞ ¼ x2F uðxÞ, and rearranging gives
F uðxÞ ¼ GðxÞF€xgðxÞ
ð6:6Þ
where GðxÞ is the transfer function between €xgðtÞ and uðtÞ:
GðxÞ ¼
1
x2
1ð1  b2 þ 2fbiÞ
jGðxÞj ¼
1
x2
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð1  b2Þ2 þ ð2fbÞ2
q
b ¼ x
x1
ð6:7Þ
Figure 6.4 shows the modulus of GðxÞ
j
j versus the frequency ratio b ¼ x=x1.
When €xgðtÞ is a simple harmonic (i.e., sine or cosine) at frequency x, jGðxÞj tells
how much the amplitude of €xgðtÞ is ampliﬁed in uðtÞ. When €xgðtÞ is a stationary
stochastic process, GðxÞ
j
j2 tells how much the PSD of €xgðtÞ at x is ampliﬁed in the
PSD of uðtÞ. As seen in the ﬁgure, for frequencies well below the natural frequency
of the sensor, i.e., b  1, GðxÞ
j
j is almost ﬂat and this gives the ‘usable’ band for
the sensor. To widen the usable band, the natural frequency x1 should be as high as
possible and the damping ratio should be near 1=
ﬃﬃﬃ
2
p
 71%. Since GðxÞ
j
j / 1=x2
1
and x1 is large, the mechanical deformation uðtÞ is very small and so is the induced
electrical signal. For effective transmission, the latter needs to be ampliﬁed.
Electronic noise from the ampliﬁer is inevitable, which can be signiﬁcant or even
dominate the overall noise of the sensor for low frequencies.
210
6
Measurement Basics

Force-Balance Principle
Servo-accelerometers operate on a different mechanism, known as ‘force-balance
principle’. For this reason they are also called ‘force-balance accelerometers’
(FBA). Instead of sourcing from the (small) electrical signal induced by mechanical
deformation, the output signal for acceleration is associated with the required
feedback force on the proof mass so that it does not move. To illustrate the idea,
suppose we apply a ‘control force’ FðtÞ on the proof mass in Fig. 6.3 so that (6.4)
reads
m€uðtÞ þ c_uðtÞ þ kuðtÞ ¼ m€xgðtÞ þ FðtÞ
ð6:8Þ
Ideally, if we can assign FðtÞ so that the proof mass remains stationary w.r.t. the
base, i.e., uðtÞ  0, then the LHS of the equation is identically zero, giving
FðtÞ ¼ m€xgðtÞ. That is, the control force is proportional to the acceleration we want
to measure. In reality this can only be achieved approximately. The control force
FðtÞ generally depends on uðtÞ and its derivatives. The relationship is designed
based on ‘feedback control theory’. In typical implementations, the detection of uðtÞ
and the generation of FðtÞ are electro-magnetic in nature. Delay in feedback is
inevitable, which can be critical for high frequency motions. This imposes an upper
frequency limit on the sensor. Electronic noise is still inevitable but the electrical
signal (proportional to control force) and hence s/n ratio can be much bigger now.
Servo-accelerometers are especially useful for low frequencies (e.g., <1 Hz) or low
level vibrations (e.g., \1 milli-g=
ﬃﬃﬃﬃﬃﬃ
Hz
p
). See Wielandt (2012) for details on
servo-accelerometers and other types of sensors.
10
-2
10
-1
10
0
10
1
10
-2
10
-1
10
0
10
1
Frequency ratio β = ω/ω1
|G(ω)|     (× 1/ω1
2)
ζ = 5%
10%
1/√2
90%
Fig. 6.4 Modulus of transfer function between input acceleration and relative displacement
6.4
Sensor Principle
211

6.5
Aliasing
A discrete-time history sampled at uniform rate fs (Hz) can at best capture correctly
the frequency components in the original analogue (continuous-time) signal up to
the Nyquist frequency, equal to fs=2. Components in the analogue data with fre-
quencies higher than fs=2 will be ‘aliased’ as ones with frequencies lower than fs=2.
In particular, a sine (or cosine) signal with frequency fsr  f (Hz) for any integer
r and f \fs=2 will appear as one with frequency f. This is because at any sampled
time instant tj ¼ jDt Dt ¼ 1=fs
ð
Þ,
sin 2pðfsr  f Þtj


¼ sin ð2p fs
|{z}
1=Dt
r jDt
|{z}
tj
2pftjÞ ¼ sin 2prj  2pftj


¼  sinð2pftjÞ
This is illustrated in Fig. 6.5. See Sects. 2.4.2 and 4.7.2 for further mathematical
details on aliasing.
Paper Folding
Due to aliasing, the PSD of digital data at the Fast Fourier Transform
(FFT) frequency fk ¼ k=NDt fk\fs=2
ð
Þ contains the contributions in the original
analogue data at frequencies fsr  fk for integers r 	 1. This can be visualized by
folding the portion of the spectrum beyond the Nyquist frequency back and forth
into the sampling band ½0; fs=2
; see Fig. 6.6.
Low-Pass Filter
In Fig. 6.1, the analogue data in ③can contain harmonics beyond the Nyquist
frequency. Simply taking its values at time interval Dt ¼ 1=fs as data will lead to
aliasing. To avoid this, ‘anti-aliasing ﬁlter’ is applied to the analogue data before it
is sampled, signiﬁcantly ‘attenuating’ (reducing) the components beyond the
Nyquist frequency. Bounded by fundamental limits, the attenuation cannot be
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
0
1
(a)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
0
1
(b)
Time (sec)
Fig. 6.5 Aliasing. Solid line original signal; dashed line with dots sampled data (dots) at 10 Hz.
a Samples of a sine at 10–2 = 8 Hz appear as a negative sine at 2 Hz. b Samples of a sine at
10 + 2 = 12 Hz appear as a positive sine at 2 Hz
212
6
Measurement Basics

perfectly sharp at the Nyquist frequency. The components with frequencies slightly
lower than the Nyquist frequency are inevitably be affected. To avoid distortion, the
Nyquist frequency should be much higher than the frequencies of interest.
Decimation
Depending on DAQ hardware settings, it can happen that the raw digital data is
sampled at a much higher rate than is necessary for a particular analysis. For
example, to analyze a 1 Hz vibration mode, a sampling rate of 20 Hz is usually
sufﬁcient, but the raw data may be sampled at 1000 Hz. To reduce storage demand
and speed up computations, one may ‘decimate’ (down-sample) the raw data to
produce a set with a lower sampling rate. The basic idea is simple, e.g., taking every
two sample points reduces the sampling rate (and hence data size) by half. Before
this operation, however, the data needs to be low-pass ﬁltered (digitally) to remove
harmonics beyond the Nyquist frequency to avoid aliasing. Standard functions for
decimation are commonly available in commercial software, e.g., decimate in
Matlab.
6.6
Quantization Error
When analogue data is digitized by DAQ hardware, its value is converted into a
binary number. The set of possible values of the digitized data is no longer a
continuum but is conﬁned to a discrete set of numbers dictated by the resolution of
DAQ hardware. This is illustrated in Fig. 6.7, where the original analogue signal
Flip once
Flip twice
+
+
+
…
0
fs/2
fs
3fs/2 [Hz]
…
=
PSD of sampled digital data
[Hz]
[g2
g
[
]z
H
/
2/Hz]
PSD of original analogue data
A
B
C
B
C
C
B
A
Fig. 6.6 Visualizing aliasing by paper folding. In the PSD of the sampled digital data at the
bottom, the dashed line is the target PSD, solid line is the PSD of digital data distorted by aliasing
6.5
Aliasing
213

that can range continuously over ±5 V is sampled by a 3 bit digitizer. The digitized
data can only take on 23 ¼ 8 values at the grid points along the vertical axis, whose
spacing is called ‘quantization interval’. The difference between the dots and the
continuous line is ‘quantization error’.
As a more realistic example, in a 24 bit digitizer, the sampled data can only have
224 different values. Suppose the original analogue data ranges between ±5 V. The
range is evenly divided into ð224  1Þ intervals, giving a quantization interval of
5  2=ð224  1Þ = 5:96  107V. If this is used with a sensor with sensitivity
0.5 V/g, the quantization interval in g is 5:96  107V=ð0:5V=gÞ  1lg.
Example 6.1 (Data resolution, smart phone) The MEMS of a smart phone is set
to output ±2 g with 12 bit resolution, giving a quantization interval of
2g  2=ð212  1Þ ¼ 9:7680  104g  0:001g. Figure 6.8 shows a set of acceler-
ation data sampled at 50 Hz measured along the z-direction (vertical) of a smart
phone placed on the ﬂoor. The value at each time instant is plotted with a dot. The
horizontal clusters indicate the possible values of the sampled data. Going hori-
zontally along each cluster, the values are not exactly the same due to numerical
round off error; the data was ﬁrst written to a text ﬁle with a limited number of
Quantization
interval x
-5V
+5V
Fig. 6.7 Quantization error. Solid line analogue signal. Dots digitized data
Fig. 6.8 Acceleration data along the z-direction of a smart phone put on the ﬂoor. The value at
each time instant is plotted with a dot. The separation between horizontal clusters indicates the
quantization interval
214
6
Measurement Basics

digits. The separation between neighboring clusters indicates a quantization interval
of about 0.001 g, as expected.
■
6.6.1
Statistical Properties
Quantization error is a nonlinear function of the signal being digitized. If it is
signiﬁcant it can give distortions that render the data useless. It should be reduced to
a negligible level by a suitable choice of data range and resolution of DAQ hard-
ware. Nevertheless, it would still be desirable to assess the statistical properties of
quantization error and its effect on data. This has been studied extensively in the
signal processing and information theory literature. See Bennett (1948) for a
seminal paper; Widrow et al. (1995) a review paper; and Widrow and Kollár (2008)
a monograph. Mathematically, quantization in the input signal domain is analogous
to sampling in the time domain. Fourier analysis is still a useful tool, although it is
now w.r.t. ‘wave number’ (unit = 1/unit of data) instead of frequency. One classical
result is that, under some ‘smoothness’ condition on the probability density func-
tion (PDF) of the input signal, for sufﬁciently small quantization interval Dx the
quantization error
1. is uncorrelated from the input signal;
2. has a uniform distribution over Dx=2; and
3. is white, i.e., independent at different sampled time instants.
This result allows the quantization error to be conveniently modeled as an additive
noise and is commonly used. The smoothness condition requires that the charac-
teristic function (i.e., Fourier Transform of PDF) of the input signal is bandlimited
on 2p=Dx. A necessary and sufﬁcient condition has also been derived (Sripad and
Snyder 1977), although it is more difﬁcult to apply. Even when the smoothness
condition is violated, these properties can still give a good approximation in many
cases.
6.6.2
Power Spectral Density
Under uniform white assumption, the PSD of quantization error can be easily
derived using the Parseval equality. Being uniformly distributed on ½Dx=2; Dx=2
,
quantization error has a variance of Dx2=12. As a white noise, it has a constant PSD
SQ (say, one-sided) up to the Nyquist frequency 1=2Dt Hz
ð
Þ. The area under the
PSD over the sampling band is therefore SQ=2Dt, which should also be equal to the
variance Dx2=12 (Parseval equality). This implies
6.6
Quantization Error
215

SQ ¼ Dx2Dt
6
(one-sided PSD of quantization error)
ð6:9Þ
Note that the noise PSD from quantization error diminishes as the sampling rate
increases, essentially because the same variance is spread over a wider frequency
band. This is in contrast to sensor/hardware noise, whose PSD does not depend on
the sampling rate and whose variance increases with the sampling rate.
In Example 6.1, the MEMS data on smart phone was sampled at 50 Hz, i.e.,
Dt ¼ 1=50 ¼ 0:02
s,
with
a
quantization
interval
of
about
Dx ¼ 0:001g.
Substituting into (6.9) gives a (one-sided) PSD of SQ ¼ ð0:001gÞ2ð0:02sÞ=
6  3000ðlg)2=Hz. This is about 3% of the overall channel noise PSD
(3002ðlg)2=Hz, see Fig. 6.2), and is negligible.
6.7
Synchronization
When multiple channels of data are acquired and analyzed, they are often assumed to
be ‘synchronized’, i.e., recorded simultaneously at the same time scale. This should
not be taken for granted, however. Synchronization is not only to do with whether
data recording starts at the same time for different channels, but also with the pace at
which they are recorded afterwards. Simply recording multiple data channels on the
same computer does not mean that they are synchronized. Each DAQ unit has its
own clock running at its own pace, which is not perfectly constant and is temperature
dependent. To be synchronized, the data at different channels need to be sampled
based on the same clock. Practically, synchronization means that the time difference
at which data channels are sampled is within a certain tolerance, much smaller than
the periods of interested harmonics. Data channels of the same DAQ unit are often
synchronized but the same need not be true for channels of different units. For
example, the three data channels of triaxial acceleration measured on one smart
phone are synchronized but they are not synchronized with those on another smart
phone, unless special provisions are made. Instrument manufacturers often allow
multiple DAQ units to be synchronized by plugging into a central module to produce
a large array of synchronized data channels.
Synchronized data are much easier to process than asynchronous ones and they
are conventionally assumed in mathematical theories. The cross spectral charac-
teristics of unsynchronized data are different from synchronized ones, creating
additional difﬁculties for modal identiﬁcation. For example, asynchronous data
exhibit artiﬁcial modes in the singular value spectrum and they make mode shapes
more difﬁcult to identify; see Sect. 7.5.
Common Conﬁgurations
Acquiring synchronized data comes with an overhead and has implications on test
conﬁguration. Figure 6.9 illustrates some conﬁgurations for ﬁeld tests that do not
216
6
Measurement Basics

rely on communication infrastructures such as internet or Wi-Fi. Clearly they
depend on instrumentation technology which is continually advancing. In the basic
setting in Fig. 6.9a, analogue data from different sensors are transmitted through
cables to a central DAQ console, where they are synchronously recorded. The
cables required can be very long (e.g., 100 m). This can be laborious to set up or
manage, especially for tests with multiple setups. It is also vulnerable to environ-
mental noise or distortion due to signal voltage drop along the cable. Generally it is
preferable to shorten the path (cable) through which the analogue data is trans-
mitted. Once data is digitized on a DAQ unit it will no longer be contaminated by
noise. In Fig. 6.9b, analogue data is recorded locally on a DAQ unit. Multiple DAQ
units are synchronized digitally, e.g., by Ethernet cable. Here, the analogue cables
are relatively short but the digital cables (for synchronization) are long. It is still
necessary to manage long cables.
For outdoor applications, one conﬁguration that avoids the use of long cables is
to synchronize the DAQ units through GPS (Global Positioning System). In
Fig. 6.9c, the DAQ units are synchronized through GPS receivers with a common
GPS clock. For indoor applications where one cannot use GPS receiver, it is
possible to synchronize the DAQ units with high precision clocks (Fig. 6.9d).
Before ﬁeld test the clocks are synchronized with a common time base (e.g., GPS).
synchronize
sensor
(a)
(b)
(c)
(d)
GPS
receiver
High precision clock
sensor
sensor
sensor
Satellite
DAQ
DAQ
DAQ
DAQ
DAQ
DAQ
DAQ
DAQ
DAQ
DAQ
Fig. 6.9 Some test conﬁgurations for acquisition of synchronized data. Thick and thin lines
represent analogue and digital data transmission, respectively. d High precision clocks are
synchronized shortly before the test begins
6.7
Synchronization
217

The high precision clocks can provide effectively synchronized data for a reason-
able time span (e.g., a day) after they are unplugged from the common time base.
Time Precision
To give an idea of time precision, below are some ballpark ﬁgures from published
instrument speciﬁcations. A precision of 1 ls can be maintained by synchronizing
with GPS. The clock on a good DAQ unit can drift by 10 min/year (20 ls/s), where
a high precision clock can drift by 1 s/year (0.03 ls/s). As an example, two DAQ
units synchronized in the beginning of a test but unplugged afterwards can have
after 12 h a relative drift of 2  20  106  12  3600  1:6 s, which is not
acceptable unless the vibrations of interest have very long periods (1.6 s). Using
a high precision clock, the drift is about 0.26 ms, which is acceptable unless one is
interested in high frequency activities beyond (say) 100 Hz.
6.8
Channel Noise Calibration
It is a good practice to assess the noise spectrum of data to establish conﬁdence on
its use and allow proper interpretation of analysis results. This is relevant when
commissioning a new piece of equipment or starting a new experiment.
Speciﬁcations or calibration certiﬁcates (if available) from manufacturers provide
some basic information but it is no substitute for a calibration that informs the
current state of the equipment system at hand under the test environment.
Calibrating the noise of seismometers (e.g., servo-accelerometers) is an important
task in seismology. See Aki and Richards (2002) and Havskov and Alguacil (2016).
Assuming a linear measurement system, the input-output relationship between
the scaled FTs of input motion and output data can be modeled as
XðxÞ
output data
¼
GðxÞ
transfer function
ZðxÞ
input motion
þ
eðxÞ
channel noise
ð6:10Þ
The transfer function GðxÞ accounts for both the characteristics of sensor and DAQ
hardware. Assuming that the input motion and noise are independent, the PSDs are
related by
Sx
output data ¼ jGj2
Sz
input motion
þ
Se
channel noise
ð6:11Þ
where Sx ¼ E½XX
, Sz ¼ E½ZZ
 and Se ¼ E½ee
 are respectively the PSDs of the
output data, input motion and channel noise; the dependence on x has been omitted
for simplicity. For calibration, the noise PSD Se is the target to be determined but
only Sx can be directly obtained (estimated) from data. By its very nature, channel
noise is mixed with the signal being measured. It is not possible to separate noise
from data unless the target signal is negligible or known. The same applies to their
218
6
Measurement Basics

PSDs. Strategies for estimating the noise PSD based on these two possibilities have
been explored.
6.8.1
Base Isolation
The ﬁrst idea is straightforward. In (6.11), if jGj2Sz  Se then Sx  Se gives the
noise PSD. In Fig. 6.2, for frequencies lower than 4 Hz (say), the root PSD of
MEMS data
 30 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p


is reﬂective of channel noise because the structural
response there \1 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p


is negligible. The same of course is not true near the
natural frequency. Pursuing the ﬁrst idea, the main challenge lies in setting up an
experiment that isolates the sensor from vibration due to air (on the structure) and
supports. The former can usually be well-controlled under laboratory environment,
leaving the latter a dominant source. The basic strategy is then ‘base isolation’.
Consider the SDOF structure subjected to base motion in Fig. 6.10. This ﬁgure
is the same as Fig. 6.3 for a sensor, except that the proof mass in the sensor is
now the structure. Here, we want to design an ‘isolation system’ so that the
acceleration of the structure picked up by accelerometer, i.e., €xðtÞ (not €uðtÞ), is
small regardless of the base motion €xgðtÞ. Proceeding in the frequency domain,
F €xðxÞ ¼ F€xgðxÞ þ F €uðxÞ, where ‘F’ denotes the scaled FT of the subscripted
quantity. Substituting F €uðxÞ ¼ x2F uðxÞ and using the expression of F uðxÞ
from (6.6) gives
F€xðxÞ ¼ HðxÞF €xgðxÞ
ð6:12Þ
where HðxÞ is the (dimensionless) transfer function between €xgðtÞ and €xðtÞ:
HðxÞ ¼
1 þ 2fbi
1  b2 þ 2fbi
jHðxÞj ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ 4f2b2
ð1  b2Þ2 þ 4f2b2
s
b ¼ x
x1
ð6:13Þ
)
(t
u
Moving base
(excitation to be isolated)
Fixed
reference
)
(t
xg
)
(t
x
Lateral
stiffness k
Mass m
Damping
constant c
Structure
Fig. 6.10 SDOF structure
subjected to base motion
6.8
Channel Noise Calibration
219

where x1 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
k=m
p
rad=s
ð
Þ and f ¼ c=2
ﬃﬃﬃﬃﬃﬃ
mk
p
are respectively the natural frequency
and damping ratio of the structure.
Figure 6.11 shows jHðxÞj versus the frequency ratio b ¼ x=x1. Base isolation
exploits the decay of jHðxÞj for large b. By reducing x1, we can shift the dominant
frequencies of €xgðtÞ to have high b and hence low jHðxÞj. Note that jHðxÞj ¼ 1 at
b ¼
ﬃﬃﬃ
2
p
regardless of f. Beyond that, jHðxÞj  1=b2 for moderate values of b  1.
For very large b  1=4f2, jHðxÞj  2f=b but this is of little relevance here. Based
on jHðxÞj  1=b2, to allow transmitting at most jHðxÞj ¼ 1% of base motion (in
terms of root PSD) at 1 Hz and beyond, we need a structure with one-tenth of this
frequency, i.e., 0.1 Hz. This can impose an experimental challenge. Suppose the
structure is a simple pendulum with length L, whose natural frequency is
ﬃﬃﬃﬃﬃﬃﬃﬃ
g=L
p
=2p Hz. This would require a length of L ¼ ð9:81Þ=ð0:1  2pÞ2  25m, say,
hanging a cable from eight stories high! More sophisticated isolation systems have
been developed. See Brownjohn and Botﬁeld (2008) that reports the performance of
a laboratory-scale folded pendulum system.
6.8.2
Huddle Test
The second idea of noise calibration explores the algebraic input-output relationship
of PSDs together with special experimental design. In (6.11) if jGj2Sz is known then
we can simply subtract it from Sx to obtain the noise PSD Se. While this is usually
not the case with a single data channel, possibilities exist with multiple channels.
It is possible to determine the noise PSD from multiple data channels driven by a
single common input without knowing it. Intuitively, multiple channels provide
10
-1
10
0
10
1
10
2
10
-4
10
-3
10
-2
10
-1
10
0
10
1
10
2
Frequency ratio β = ω/ω1
|H(ω)|
1%
5%
10%
1%
5%
10%
Fig. 6.11 Modulus of transfer function between base and structural acceleration
220
6
Measurement Basics

redundant information about the input for distinguishing the noise. In a conven-
tional approach called ‘huddle test’, multiple sensors with the same orientation are
placed as close as possible with each other (but no contact) so that they can be
assumed to measure the same motion; see Fig. 6.12. The noise PSD can be
determined from the PSDs and cross PSDs of the channels measuring the same
motion. The basic mathematics is as follow.
Let XðxÞ ðn  1Þ comprise the scaled FT of output data which are all subjected
to a common (scalar) input motion ZðxÞ. Then, analogous to (6.10),
XðxÞ
n1
output
data
¼ GðxÞ
n1
transfer
function
ZðxÞ
scalar
input
motion
þ eðxÞ
n1
channel
noise
ð6:14Þ
where GðxÞ is a vector of transfer functions and eðxÞ is the scaled FT of the vector of
channel noise. Assume that the noise at different channels are mutually independent
and they are independent of the input Z. The PSD matrices are then related by
Sx
nn ¼ GG
|ﬄ{zﬄ}
nn
Sz
scalar
þ Se
nn
ð6:15Þ
where, analogous to (6.11), Sx ¼ E½XX
 is the PSD matrix of the output data and
Sz ¼ E½ZZ
 is the PSD of the input motion; the dependence on x has been omitted
for simplicity;
Se ¼ E½ee
 ¼
Se1
..
.
Sen
2
64
3
75
ð6:16Þ
is the PSD matrix of channel noise; Sei is the noise PSD of the ith channel.
Fig. 6.12 Huddle test
6.8
Channel Noise Calibration
221

Determinacy
For given Sx at each frequency, (6.15) gives an algebraic system of complex-valued
equations for the unknowns fGign
i¼1, Sz and fSeign
i¼1. Since Gi is complex-valued,
the total number of (real-valued) unknowns is 3n þ 1. When the primary interest is
on fSeign
i¼1, the number of unknowns can be reduced by 1, by combining Sz and
G to form G0 ¼
ﬃﬃﬃﬃ
Sz
p
G so that:
Sx
nn ¼ G0G0
|ﬄﬄ{zﬄﬄ}
nn
þ Se
nn
ð6:17Þ
There are now 3n unknowns, comprising fSeign
i¼1 and the (complex-valued) entries
of G0. The number of real-valued linearly independent equation is n2 because Sx is
Hermitian ðS
x ¼ SxÞ. Reading the diagonal and upper off-diagonal entries of (6.17)
gives
Sii ¼ G0
iG0
i þ Sei
i ¼ 1; . . .; n
Sij ¼ G0
iG0
j
i\j
ð6:18Þ
where Sij denotes the ði; jÞ-entry of Sx; G0
i is the ith entry of G0. The second equation
in (6.18) is complex-valued and hence contains two real-valued equations. There
are nðn  1Þ=2 equations of this type over all possible pairs of ði; jÞ indicated in the
equation.
Table 6.1 shows the number of (real-valued) equations and unknowns for dif-
ferent n. When n is 1 or 2 there are not enough equations for a unique solution.
There are just enough equations when n ¼ 3. When n 	 4 the unknowns are
over-constrained. In this case the problem is better formulated as an inference
problem accounting for possible estimation and modeling errors. For example, in
the actual implementation, Sx is substituted by its sample estimate. The latter
contains estimation error and need not have the form in (6.17).
6.8.3
Three Channel Analysis
The huddle test with n = 3 channels is of practical signiﬁcance as it requires the
smallest number of channels. This possibility was discovered by Sleeman et al. (2006)
Table 6.1 Determinacy of Huddle test with a single common input
n
No. of unknowns, 3n
No. of equations, n2
Remark
1
3
1
Under-determined
2
6
4
Under-determined
3
9
9
Unique solution
4
12
16
Over-constrained
222
6
Measurement Basics

and has been used for calibrating a variety of seismometers (Ringler and Hutt 2010).
Solving (6.18) in this case gives
Sei ¼ Sii  SjiSik
Sjk
i; j; k
distinct
ð6:19Þ
In implementation, the PSDs on the RHS are replaced by their sample estimates
calculated from measured data. Statistical estimation error in the sample PSDs can
be controlled by using sufﬁcient data length, trading off with frequency resolution.
The huddle test relies on the single common input assumption, which should be
managed by experimental setup. The accuracy of noise PSD tends to be lower at
high frequencies due to lower spatial coherence (a modeling error).
Theoretically, when (6.18) holds the value of Sei obtained from (6.19) is always
real-valued and remains the same when j and k are swapped, because the solution is
real and unique. However, this is not necessarily true in implementation because the
sample PSDs need not obey (6.18). It can be easily veriﬁed that the second term in
(6.19) becomes its complex conjugate when j and k are swapped; the same is also
true when sample cross PSDs are substituted. This suggests that the real part of the
expression should be taken in implementation.
Example 6.2 (Huddle test) The noise PSDs of the North (parallel to handles)
channels of the left three triaxial servo-accelerometers in Fig. 6.12 are estimated by
three channel analysis. Ambient data of 9000 s duration and sampling rate 100 Hz
was collected under laboratory environment. The data is divided into 900
non-overlapping segments (each 10 s) to produce averaged sample PSD estimates
at a frequency interval of 1/10 = 0.1 Hz and a relative error of 1=
ﬃﬃﬃﬃﬃﬃﬃﬃ
900
p
 3%. The
results are shown in Fig. 6.13.
In Fig. 6.13a, the frequency axis is shown on a log-scale up to the Nyquist
frequency (50 Hz). The root PSD of the three data channels almost overlap, as
expected. Below 10 Hz the estimated root PSDs of noise almost overlap, sug-
gesting similar noise characteristics among the three channels. On this log-log plot,
the slope is about 1/2, demonstrating the 1/f (in PSD) nature of noise. For high
frequencies, the noise PSD estimates display high ﬂuctuations. They are not
accurate because of low coherence in the input ground motion among the three
sensors, violating (6.18). This is despite the fact that the sample PSDs of data at
different frequencies all have the same relative error of about 3%. Figure 6.13b
shows the root PSD of noise with a linear scale in the frequency on a typical range
of interest. The noise PSD appears to be robust to the nature of data. For example, it
is not affected by the isolated peaks in the data PSD between 12 and 16 Hz.
■
6.8
Channel Noise Calibration
223

References
Aki K, Richards PG (2002) Quantitative seismology, 2nd edn. University Science Books,
Sausalito, CA
Bennett WR (1948) Spectra of quantized signals. Bell Syst Tech J 27(4):446–472
Brownjohn JMW, Botﬁeld T (2008) A folded pendulum isolator for evaluating accelerometer
performance. Experimental Techniques 33(1):33–37
Gabrielson TB (1993) Mechanical-thermal noise in micromachined acoustic and vibration sensors.
IEEE Trans Electron Devices 40(5):903–909
Havskov J, Alguacil G (2016) Instrumentation in earthquake seismology. Springer, Switzerland
Kogan S (1996) Electronic noise and ﬂuctuations in solids. Cambridge University Press, UK
Levinson FE (2004) Fundamental noise limit of piezoelectric accelerometer. IEEE Sens J 4
(1):108–111
Ringler AT, Hutt CR (2010) Self-noise models of seismic instruments. Seismol Res Lett 81
(6):972–983
Sleeman R, van Wettum A, Trampert J (2006) Three-channel correlation analysis: a new technique
to measure instrumental noise of digitizers and seismic sensors. Bull Seismol Soc Am 96
(1):258–271
Sripad AB, Snyder DL (1977) A necessary and sufﬁcient condition for quantization errors to be
uniform and white. IEEE Trans Acoust Speech Signal Process 25(5):442–448
Widrow B, Kollár I, Liu MC (1995) Statistical theory of quantization. IEEE Trans Instrum Meas
45(6):353–361
Widrow B, Kollár I (2008) Quantization Noise: Roundoff Error in Digital Computation. Signal
Processing, Control, and Communications. Cambridge University Press, Cambridge, UK
Wielandt E (2012) Seismic sensors and their calibration. In: Bormann P (ed) New manual of
seismological observatory practice 2 (NMSOP-2). Deutsches GeoForschungsZentrum GFZ,
Potsdam. pp 1–51. doi:10.2312/GFZ.NMSOP-2_ch5
10
-1
10
0
10
1
10
-8
10
-7
10
-6
10
-5
Frequency (Hz)
Root PSD (g/√Hz)
(a)
Data
Noise
0
5
10
15
20
10
-8
10
-7
10
-6
10
-5
Frequency (Hz)
Root PSD (g/√Hz)
(b)
Data
Noise
Fig. 6.13 (Two-sided) Root PSD of data and estimated root PSD of noise. a The slope at low
frequency demonstrates 1/f nature (of PSD). Accuracy of noise PSD is low at high frequencies due
to low spatial coherence among sensors. b Zooms into a typical frequency range of interest
224
6
Measurement Basics

Chapter 7
Ambient Data Modeling and Analysis
Abstract This chapter presents some basic tools for analyzing a sample history of
ambient data. Procedures for plotting the power spectral density and singular value
spectra are presented. Their theoretical statistical properties near the resonance band
of a structural vibration mode are discussed, allowing one to interpret and detect
potential modes in applications. A detailed example with ﬁeld data is presented to
illustrate in a step-by-step manner the procedures and considerations in the pro-
duction and analysis of these spectra. Ambient data of other nature are also dis-
cussed, including asynchronous data that exhibits artiﬁcial vibration modes in their
spectra, and microtremor data that can be used to infer site characteristics.
Procedures for generating synthetic ambient data are presented, which are useful for
verifying algorithms or parametric studies.
Keywords Power spectral density  Singular value spectrum  Microtremor  H/V
method  Site ampliﬁcation
Under ambient conditions, the vibration response of a structure appears random in
the time domain as it is subjected to excitations with a variety of frequency char-
acteristics. Modeled as a stationary stochastic process, its statistical properties have
been investigated in Chap. 5. Ambient vibration response can be measured, but it is
inevitably contaminated with noise (Chap. 6). With these considerations in mind, in
this chapter we discuss how to analyze ambient vibration data measured on a
structure. For statistical analysis of time series, see, e.g., Brillinger (1981), Bendat
and Piersol (1993), Kay (1993).
We ﬁrst study the theoretical frequency characteristics of ambient data in the
resonance band of a vibration mode. The ‘power spectral density’ (PSD) and
‘singular value’ (SV) spectrum allow one to visualize the modes potentially present
in a frequency band. The picture can be different when the data is not perfectly
synchronized across its channels. All structures are excited by small ground
motions at their support, often known as ‘microtremor’. The nature and possible
exploitation of microtremor data for identifying site characteristics are discussed.
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_7
225

Finally, we consider how to generate synthetic data with a given PSD. This is
needed when one wants to verify an algorithm or perform parametric studies.
7.1
Resonance Band Characteristics
A proper set of ambient data measured on a structure can be assumed to comprise
structural response and channel noise; and is free from distortions such as clipping,
aliasing or quantization error. The structural response at a given time instant is
contributed by all modes and it is generally difﬁcult to separate them by operating in
the time domain. The contributions from different modes can be naturally separated
in the frequency domain because they are ampliﬁed by resonance and ‘stand out’
near their natural frequency. This is illustrated in Fig. 7.1, which shows the root
PSD (no averaging) of the horizontal acceleration measured on a tall building roof.
Peaks near 0.2, 0.4, 0.7, 1.2 Hz, etc., are dominated by vibration modes. The
resonance band of the mode near 1.2 Hz is shaded. It can be reasoned from the
dynamic ampliﬁcation factor that for a mode with natural frequency f (Hz) and
damping ratio f, the resonance band is f ð1  jfÞ Hz for some dimensionless
constant j and hence has a width of the order of f f Hz.
Let f^yjgN1
j¼0 ðn  1Þ be the time-domain data with n measured channels at
interval Dt (s); N is the number of sample points and so the data duration is NDt.
Let ^F k (n  1 complex) be the (two-sided) scaled Fast Fourier Transform (FFT) of
f^yjgN1
j¼0 at frequency fk ¼ k=NDt (Hz):
0
200
400
600
800
1000
1200
1400
1600
1800
-2
0
2
x 10
-4
[g]
Time [s]
0
0.5
1
1.5
2
2.5
3
10
-7
10
-6
10
-5
10
-4
10
-3
Frequency [Hz]
(a)
(b)
Fig. 7.1 Resonance band of a vibration mode (shaded); a Time history data; b Root PSD (no
averaging); f = natural frequency (Hz), f = damping ratio, j = dimensionless constant
226
7
Ambient Data Modeling and Analysis

^F k ¼
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
j¼0
^yje2pijk=N
ð7:1Þ
In Matlab, the sequence f ^F kgN1
k¼0 can be obtained using the function fft (Sect. 2.8).
Only the ﬁrst half of the sequence (0  k  integer part of N=2) is informative. The
second half is redundant as it is just the conjugate mirror image of the ﬁrst half.
7.1.1
Single Mode
Consider a frequency band dominated by a single mode with natural frequency
f (Hz) and damping ratio f. Within the band, ^F k can be assumed to comprise the
modal response (Sect. 5.2) and channel noise:
^F k
n1 ¼ u
n1
hkpk þ ek
n1
ð7:2Þ
where u ðn  1Þ is the ‘partial mode shape’ (i.e., conﬁned to the measured DOFs);
pk (complex) and ek (n  1 complex) are respectively the scaled FFT of modal
excitation and channel noise;
hk ¼
ð2pifkÞq
1  b2
k  2fbki
bk ¼ f
fk
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð7:3Þ
is the modal transfer function. Assuming that pk and ek are independent, the PSD
matrix of ambient data is given by
Ek
nn ¼ E½ ^F k
n1
^F 
k
1n
 ¼ SkDk u
n1
uT
1n
þ Sek
nn
ðsingle modeÞ
ð7:4Þ
where Sk ¼ E½pkp
k is the modal force PSD and Sek ¼ E½eke
k (n  n Hermitian) is
the PSD matrix of channel noise;
Dk ¼
ð2pfkÞ2q
ð1  b2
kÞ2 þ ð2fbkÞ2
bk ¼ f
fk
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð7:5Þ
is the dynamic ampliﬁcation factor.
7.1
Resonance Band Characteristics
227

7.1.2
Multi-mode
Consider now m contributing modes in the band, with natural frequencies
ffigm
i¼1 ðHzÞ, damping ratios ffigm
i¼1 and partial mode shapes fuigm
i¼1 ðn  1Þ. The
scaled FFT of measured data now comprises contributions from m modes and
channel noise:
^F k
n1 ¼
X
m
i¼1
uihikpik þ ek ¼ U
nm diag½hk
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
mm
pk
m1
þ ek
n1
ð7:6Þ
where fpikgm
i¼1 are the scaled FFT of modal forces; pk ¼ ½p1k; . . .; pmkT; U ¼
½u1; . . .; um is the partial mode shape matrix; and diag½hk denotes a diagonal
matrix with entries fhikgm
i¼1;
hik ¼
ð2pifkÞq
1  b2
ik  2fibiki
bik ¼ fi
fk
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð7:7Þ
The PSD matrix of measured data is given by
Ek
nn ¼ E½ ^F k
n1
^F 
k
1n
 ¼
X
m
i¼1
X
m
j¼1
hikh
jkSijk ui
n1
uT
j
1n
þ Sek
nn ¼ U
nm Hk
mm UT
mn þ Sek
nn
ðmultimodeÞ
ð7:8Þ
where Sijk ¼ E½pikp
jk is the cross PSD between the modal forces of the ith and jth
mode at frequency fk; Hk is a m  m Hermitian matrix whose ði; jÞ-entry is
Sijkhikh
jk.
7.2
PSD Spectrum
Time histories of ambient data look erratic and it is often difﬁcult to gain insights
from them. The characteristics of data can be better analyzed in the frequency
domain. The PSD of ambient data exhibits dynamic ampliﬁcation near the natural
frequencies of a structure, providing a useful tool for investigating the modes or
generally frequency characteristics. The theoretical PSD is generally unknown but it
can be estimated using the measured data. Plotting the squared modulus of the
scaled FFT of data with frequency produces an erratic spectrum like Fig. 7.1
because the values at different frequencies are almost uncorrelated and their
228
7
Ambient Data Modeling and Analysis

standard deviation is as large as their mean (Sect. 4.10). The frequency charac-
teristics are better viewed through the averaged sample PSD, which is an asymp-
totically unbiased and converging estimator for the theoretical PSD.
Frequency Resolution and Accuracy
To produce the averaged sample PSD using a single time history, the data is divided
into M non-overlapping segments of equal length. The sample PSDs of the indi-
vidual segments are calculated and averaged. The choice of M is a trade-off between
frequency resolution and accuracy in the spectrum. Increasing M reduces statistical
error in the averaged sample PSD at each frequency but it decreases frequency
resolution and increases leakage bias. The averaged PSD over M values has a c.o.v.
of 1=
ﬃﬃﬃﬃﬃ
M
p
if the segments were independent; c.o.v. = coefﬁcient of variation = ratio
of standard deviation to mean. Suppose the original data has a duration of N0Dt (s).
Dividing it into M equal segments gives a segment duration of N0Dt=M (s) and
hence a FFT frequency interval of Df ¼ M=N0Dt (Hz). To achieve a frequency
interval of Df the number of segments can be taken as
M ¼ N0DtDf
b
c
ð7:9Þ
where bc denotes the integer part of the argument; e.g., b2:8c ¼ 2. Each segment
has Nw ¼ bN0=Mc samples. Note that NwM  N0. Those samples in the original data
with index greater than NwM may be discarded. The wastage is insigniﬁcant for
large M.
Usually a spectrum is plotted within a limited frequency band of interest, say,
ðfL; fUÞ Hz. For visualization it is desired to have a speciﬁed resolution, say, Mf
intervals (e.g., Mf ¼ 300) in the band ðfL; fUÞ. The required frequency interval is
then
Df ¼ fU  fL
Mf
ðHzÞ
ð7:10Þ
7.2.1
Procedure
Given the time history data f^yjgN01
j¼0 ðn  1Þ, below is a procedure that produces the
(averaged) sample PSD matrix with a frequency resolution of Mf intervals dis-
played over the frequency band ðfL; fUÞ Hz. Plotting the ith diagonal entry of the
PSD matrix with frequency gives ‘the sample PSD spectrum’, or ‘PSD spectrum’ in
short when understood, for the ith measured channel. For i 6¼ j, the ði; jÞ-entry gives
the sample cross PSD between channels i and j. Statistical properties of the sample
PSD spectrum are discussed in Sect. 4.10. The averaging process for a single
channel ðn ¼ 1Þ is illustrated in Fig. 7.2.
7.2
PSD Spectrum
229

As a word of caution, at each frequency it is the sample PSD matrices
f ^F ðrÞ
k ^F ðrÞ
k
gM
r¼1 that should be averaged; not the time-domain data f^yðrÞ
j gM
r¼1, nor the
scaled FFTs f ^F ðrÞ
k gM
r¼1.
Sample PSD Matrix with Mf Intervals Over Frequency Band ðfL; fUÞ Hz
0. Calculate
Df ¼ ðfU  fLÞ=Mf
(frequency interval in Hz)
M ¼ bN0DtDf c
(No. of segments)
Nw ¼ bN0=Mc
(No. of samples per segment)
N ¼ NwM
(Truncated data length)
1. Divide the data f^yjgN1
j¼0
into M segments, each with length Nw. Let
f^yðrÞ
j gNw1
j¼0
be
the
rth
segment.
Then
^yðrÞ
j
¼ ^yNwðr1Þ þ j
ðr ¼ 1; . . .; M; j ¼ 0; . . .; Nw  1Þ.
2. For each segment r, calculate the scaled FFT f ^F ðrÞ
k g and (raw) sample
PSD f^E
ðrÞ
k g by
^F ðrÞ
k
n1
¼
ﬃﬃﬃﬃﬃﬃ
Dt
Nw
s
X
Nw1
j¼0
^yðrÞ
j
n1
e2pijk=Nw
^E
ðrÞ
k
nn
¼ ^F ðrÞ
k
n1
^F ðrÞ
k
1n
ð7:11Þ
1
0
}
)1(
ˆ
{
1
0
}
)1(ˆ
{
w
N
k
k
E
w
N
j
j
y
Discarded
1
0
}
)
2
(
ˆ
{
1
0
}
)
2
(ˆ
{
w
N
k
k
E
w
N
j
j
y
1
0
}
)
(
ˆ
{
1
0
}
)
(ˆ
{
w
N
k
M
k
E
w
N
j
M
j
y
samples
w
N
samples
w
N
samples
w
N
samples
w
N
M
w
N
N
1
0
}
ˆ
{
w
N
k
k
E
Time [sec]
[Hz]
[Hz]
[Hz]
[Hz]
[g]
[g2/Hz]
[g2/Hz]
[g2/Hz]
[g2/Hz]
Averaged spectrum
t
n
e
m
g
e
S
Segment 2
Segment 1
M
Fig. 7.2 Averaged PSD spectrum; f^EðrÞ
k gNw1
k¼0
is the sample PSD of the rth segment; for
illustration, data is assumed to be acceleration in g with N0 samples
230
7
Ambient Data Modeling and Analysis

3. Calculate the (averaged) sample PSD matrix f^Ekg by
^Ek
nn ¼ 1
M
X
M
r¼1
^E
ðrÞ
k
ð7:12Þ
4. Plotting ^Ekði; iÞ versus fk ¼ k=NwDt and zooming into the band ðfL; fUÞ
gives the averaged sample PSD spectrum for the ith measured channel
with the desired frequency resolution.
7.3
Singular Value Spectrum
A peak displaying dynamic ampliﬁcation in the PSD spectrum indicates the pres-
ence of a mode but it need not indicate the number of modes. For example, three
lines showing a peak at the same frequency need not indicate three modes. The
number of modes can be assessed based on a plot of the eigenvalues of the real part
of the PSD matrix versus frequency. The number of lines (eigenvalues) signiﬁcantly
above the remaining ones indicates the number of modes within the band. We call
this plot the ‘singular value (SV) spectrum’. The term ‘singular value’ stems
originally from ‘singular value decomposition’, which can be applied to any
complex rectangular matrix. Taking the real part of the PSD matrix is a provision
for detecting synchronization; see the end of Sect. 7.5.1. Singular value spectrum
analysis is applicable in more general settings outside operational modal analysis
(OMA). See, e.g., Elsner and Tsonis (1996), Golyandina and Zhigljavsky (2013).
Figure 7.3 illustrates the difference between the PSD and SV spectrum.
ˆ
,
ˆ
,
ˆ
ˆ
Eigenvalues of Re
3
2
1
k
k
k
k
E
Freq. [Hz]
PSD
(3,3)
ˆ
Hermitian
(2,3)
ˆ
(2,2)
ˆ
(1,3)
ˆ
(1,2)
ˆ
(1,1)
ˆ
ˆ
k
k
k
k
k
k
k
E
E
E
E
E
E
E
SV
k
1ˆ
k
2ˆ
k
3ˆ
kf
kf
Freq. [Hz]
(1,1)
ˆ k
E
(2,2)
ˆ k
E
(3,3)
ˆ k
E
Fig. 7.3 PSD and SV spectrum for n ¼ 3 channels near a well-separated mode; ^Ek is the sample
PSD matrix at frequency fk. The ordering of ^Ekð1; 1Þ, ^Ekð2; 2Þ and ^Ekð3; 3Þ is for illustration only
as it depends on data
7.2
PSD Spectrum
231

In the resonance band, the SV spectrum has characteristic properties related to
the contributing modes. When there is only one mode dominating the band, the
largest eigenvalue varies as the dynamic ampliﬁcation factor. Near the natural
frequency, the eigenvector of the largest eigenvalue is equal to the partial mode
shape. For modes with close natural frequencies, the eigenvectors need not give the
partial mode shapes. Rather, they are orthogonal vectors spanning the subspace
containing the partial mode shapes. Sections 7.3.1 and 7.3.2 give an account of
these properties based on the theoretical PSD matrix. In the actual implementation,
the SV spectrum is produced based on the (averaged) sample PSD. Conclusions
will be approximately true if statistical error is small.
7.3.1
Single Mode
When there is only one mode in the frequency band, the PSD matrix of measured
data, Ek, is given by (7.4). To simplify discussion, assume i.i.d. channel noise, i.e.,
Sek ¼ SekIn
ð7:13Þ
where Sek is the common noise PSD and In denotes the n  n identity matrix. Then
ReEk ¼ SkDkuuT þ SekIn
ð7:14Þ
This is a positive-deﬁnite and real-symmetric matrix. It can be expressed in a form
that shows explicitly the eigenvalues and eigenvectors. Write In ¼ Pn
i¼1 bibT
i ,
where fbign
i¼1 ðn  1Þ is an orthonormal basis with b1 ¼ u (assuming unit norm,
i.e.,
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
uTu
p
¼ 1). Substituting into (7.14) and collecting terms,
ReEk ¼ ðSkDk þ SekÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
eigenvalue
uuT þ
X
n
i¼2
Sek
|{z}
eigenvalue
bibT
i
ð7:15Þ
Since fbign
i¼1 is an orthonormal basis, the above is an eigenvector representation of
ReEk. The largest eigenvalue is k1k ¼ SkDk þ Sek with eigenvector u. The
remaining ðn  1Þ eigenvalues are all equal to Sek with eigenvectors fbign
i¼2. This is
illustrated in Fig. 7.4. The eigenvalues and hence SV spectrum have the same unit
as the PSD.
Example 7.1 (Single mode, synthetic data) Consider synthetic data f^yjgN1
j¼0 with
three measured DOFs generated from
232
7
Ambient Data Modeling and Analysis

^yj ¼ u€gðtjÞ þ eðtjÞ
tj ¼ jDt; j ¼ 0; 1; 2; . . .
ð7:16Þ
where Dt ¼ 0:01 s; u ¼ 1
2
2
½
T=3; and gðtÞ satisﬁes
€gðtÞ þ 2fx_gðtÞ þ x2gðtÞ ¼ pðtÞ
ð7:17Þ
with x ¼ 2p rad/s ðf ¼ 1 HzÞ and f ¼ 1%; pðtÞ is Gaussian white noise with PSD
S ¼ 1ðlgÞ2=Hz; and eðtÞ is 3  1 Gaussian white noise vector with i.i.d. compo-
nents and PSD Se ¼ 1ðlgÞ2=Hz. Check that u has unit norm. Data is generated for
2000 s. It is divided into M ¼ 20 segments of 100 s each to produce the averaged
PSD matrix ^Ek. The resulting frequency interval is Df ¼ 1=100 ¼ 0:01 Hz.
Plotting the three diagonal entries of ^Ek with frequency gives the PSD spectrum.
Plotting the three eigenvalues of Re^Ek with frequency gives the SV spectrum.
PSD Spectrum
Figure 7.5a shows the PSD spectrum. The top two lines correspond to DOFs 2 and 3.
They almost overlap because the mode shape values at these two DOFs are the same.
Except for statistical errors, the three lines vary as SDk/2
i þ Se where Dk is the
dynamic ampliﬁcation factor in (7.5) (with q ¼ 0) and /i is the mode shape value,
equal to 1/3, 2/3 and 2/3 for DOFs 1, 2 and 3, respectively. As a check, at low
frequencies the PSDs are all approximately equal to Se ¼ 1ðlgÞ2=Hz because Dk 	
0 there. At the natural frequency, Dk ¼ 1=4f2 and the PSD of DOF 1 (lowest line) is
approximately (ignoring Se) SDk/2
1 ¼ ð1Þ½1=4ð0:01Þ2ð1=3Þ2 	 278 ðlgÞ2=Hz:
SV Spectrum
The three lines in the PSD spectrum in Fig. 7.5a suggest a mode at around 1 Hz
but they do not imply three modes. The number of modes should be examined from
the SV spectrum in Fig. 7.5b. Except for statistical errors, the top line (largest
eigenvalue) is equal to SDk þ Se. As a check, at low frequencies it is approximately
SV
ek
k
k
k
S
D
S
1
ek
k
k
S
3
2 ,
kf
Freq. [Hz]
ek
k
k
k
S
D
S
2
1
)1,1(
E
ek
k
k
k
S
D
S
2
2
)
2,2
(
E
ek
k
k
k
S
D
S
2
3
)
3,3
(
E
Freq. [Hz]
PSD
kf
Fig. 7.4 Theoretical PSD and SV spectrum near the resonance band of a single mode;
Ek ¼ SkDkuuT þ SekIn; u ¼ ½/1; /2; /3T with /2
1 þ /2
2 þ /2
3 ¼ 1; k1k; k2k; k3k are the eigenval-
ues (in descending order) of ReEk. Ordering of Ekð1; 1Þ, Ekð2; 2Þ and Ekð3; 3Þ is for illustration
only as it depends on data
7.3
Singular Value Spectrum
233

equal to Se ¼ 1 ðlgÞ2=Hz; at resonance it is approximately equal to SDk ¼
ð1Þ½1=4ð0:01Þ2 ¼ 2500 ðlgÞ2=Hz. The remaining two lines are theoretically equal
to Se ¼ 1 ðlgÞ2=Hz.
Eigenvector and Partial Mode Shape
The dot product of two vectors measures their proximity. In OMA, it is commonly
called the ‘modal assurance criterion’ (MAC). It ranges between −1 and 1. The
closer the absolute value of the MAC is to 1, the closer the direction of the two
vectors (up to a sign reversal). Figure 7.6 shows the MAC between the eigenvector
(largest eigenvalue) of Re^Ek and the partial mode shape u. Near the resonance band
the MAC is close to −1, showing that the eigenvectors there can give a good
approximation to the partial mode shape.
■
7.3.2
Multi-mode
Consider now the case when there are m contributing modes in the resonance band.
As in Sect. 7.3.1, assume Sek ¼ SekIn (i.i.d. channel noise). Then (7.8) gives
0
0.5
1
1.5
2
10-1
100
101
102
103
Frequency [Hz]
[( g)2/Hz]
(a) PSD
0
0.5
1
1.5
2
10-1
100
101
102
103
104
Frequency [Hz]
[( g)2/Hz]
(b) SV
Fig. 7.5 a PSD spectrum and b SV spectrum, synthetic data with three measured DOFs and a
single mode at 1 Hz
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-1
-0.5
0
0.5
1
Frequency [Hz]
MAC
Fig. 7.6 MAC of the eigenvector (largest eigenvalue) of Re^Ek with the partial mode shape u
234
7
Ambient Data Modeling and Analysis

ReEk ¼ UðReHkÞUT þ SekIn
ð7:18Þ
This is a positive-deﬁnite real-symmetric matrix. At each frequency, let faikgn
i¼1 be
the eigenvalues (in descending order) and fuikgn
i¼1ðn  1Þ be the eigenvectors of
UðReHkÞUT so that
U
nm ðReHkÞ
mm
UT
mn ¼
X
n
i¼1
aik uik
n1 uT
ik
1n
ð7:19Þ
Let m0 be the rank of UðReHkÞUT. Then aik ¼ 0 for i [ m0. Substituting (7.19) and
In ¼ Pn
i¼1 uikuT
ik into (7.18), gives, after rearranging,
ReEk ¼
X
m0
i¼1
ðaik þ SekÞ
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
eigenvalue
uikuT
ik þ
X
n
i¼m0 þ 1
Sek
|{z}
eigenvalue
uikuT
ik
ð7:20Þ
This shows that the ﬁrst m0 largest eigenvalues of ReEk are equal to kik ¼ aik þ Sek
ði ¼ 1; . . .; m0Þ. The remaining ones are all equal to Sek. For high signal-to-noise
(s/n) ratio ðaik 
 SekÞ, kik 	 aik for i ¼ 1; . . .; m0. Thus, in the SV spectrum the
number of lines signiﬁcantly above the remaining ones indicates the rank m0 of
UðReHkÞUT. If ReHk is non-singular, m0 is equal to the rank of U, i.e., dimension
of ‘mode shape subspace’. From elementary matrix algebra, m0  minðm; nÞ, where
typically m0  m\n in OMA. Note that m0 can be smaller than m because the partial
mode shapes (columns of U) can be linearly dependent. In the typical case when
m\n and the partial modes shapes are linearly independent (but need not be
orthogonal), m0 ¼ m and so the number of lines signiﬁcantly above the remaining
ones in the SV spectrum is equal to the number of modes.
Perfectly Incoherent Modal Forces and Orthonormal Partial Mode Shapes
Consider the special case when the modal forces are perfectly incoherent and the
partial mode shapes fuigm
i¼1 are orthonormal. Then ReHk ¼ diag½fSiikDikgm
i¼1 is a
real diagonal matrix and
ReEk ¼
X
m
i¼1
SiikDikuiuT
i þ SekIn
ð7:21Þ
where Dik ¼ jhikj2 is the dynamic ampliﬁcation factor for mode i. Following a
similar argument as before, it can be reasoned that the ﬁrst m largest eigenvalues of
ReEk are fSiikDik þ Sekgm
i¼1 with eigenvectors fuign
i¼1. The remaining eigenvalues
are all equal to Sek. Since Dik has a peak near the natural frequency fi, the top
m lines in the SV spectrum together show dynamic ampliﬁcation of the modes
7.3
Singular Value Spectrum
235

versus frequency. This is illustrated in Fig. 7.7. Note that k1k 6¼ S11kD1k þ Sek but
rather k1k ¼ maxfSiikDik þ Sekgm
i¼1.
In the general case when ReHk is not diagonal or fuigm
i¼1 are not orthogonal, the
ﬁrst m largest eigenvalues of ReEk need not be equal to fSiikDik þ Sekgm
i¼1. The
eigenvectors need not be fuigm
i¼1. Nevertheless, Fig. 7.7 still provides the basic
picture for interpreting the SV spectrum.
Example 7.2 (Multi-mode, synthetic data) Consider synthetic data with two
modes generated according to
^yj ¼ u1€g1ðtjÞ þ u2€g2ðtjÞ þ eðtjÞ
tj ¼ jDt; j ¼ 0; 1; 2; . . .
ð7:22Þ
where Dt ¼ 0:01 s; giðtÞ ði ¼ 1; 2Þ satisﬁes
€giðtÞ þ 2fixi _giðtÞ þ x2
i giðtÞ ¼ piðtÞ
ð7:23Þ
with xi ¼ 2pfi (rad/s); f1 = 1 Hz, f2 = 1.05 Hz, f1 ¼ f2 ¼ 1%;
u1 ¼ 1
2
2
½
T=3
u2 ¼ 2
1
2
½
T=3
ð7:24Þ
p1ðtÞ and p2ðtÞ are modal forces, stationary Gaussian with constant PSD S11 ¼
S22 ¼ 1 ðlgÞ2=Hz and coherence S12=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
S11S22
p
¼ 0:5eip=4. The channel noise eðtÞ
has i.i.d. white components with PSD Se ¼ 1 ðlgÞ2=Hz. Here, the modal forces
have non-zero coherence and the partial mode shapes are not orthogonal.
Based on 2000 s of data, Fig. 7.8a shows the PSD spectrum with a frequency
interval of 0.01 Hz (averaged over 20 segments with 100 s each). Figure 7.8b
shows the SV spectrum. Theoretically the lowest line (smallest eigenvalue) is equal
to Se ¼ 1 ðlgÞ2=Hz. The top two lines (largest two eigenvalues) are contributed by
the two modes. Slightly below 1 Hz, the top line is dominated by the ﬁrst mode and
the middle line by the second. Their roles are swapped after 1.05 Hz. Dynamic
ampliﬁcation of the ﬁrst mode can be visualized by following the top line up to
about 1 Hz, and then continuing with the middle line thereafter. Similarly, for the
second mode it is the middle line up to 1 Hz and then the top line.
[Hz]
fk
(largest)
1k
ek
k
k
S
D
S
2
22
mode
2nd
ek
k
k
S
D
S
1
11
mode
1st 
largest)
(2nd
2k
Fig. 7.7 Eigenvalues of
ReEk and SV spectrum,
assuming two modes,
perfectly incoherent modal
forces and orthonormal partial
mode shapes
236
7
Ambient Data Modeling and Analysis

Figure 7.9 shows the MAC of the eigenvector (largest eigenvalue) of Re^Ek with
u1 (black circle) and u2 (white circle). The values have large scatter but their
absolute values are close to 1 near the natural frequencies. The MAC with the ﬁrst
mode is close to −1 for frequencies slightly below 1 Hz. The MAC with the second
mode is close to 1 for frequencies slightly above 1 Hz. In this example, the
eigenvector of Re^Ek near the natural frequency can still give a good approximation
to the partial mode shape. This should not be taken for granted in the general case,
however.
■
7.4
Illustration with Field Data
In this section we illustrate the analysis of a typical set of ambient data. The data
was collected from a triaxial servo-accelerometer placed near one corner on a tall
building roof. It was recorded for 30 min at sampling rate 2048 Hz by a 24 bit
digitizer. For analysis it is decimated to 64 Hz. This is considered adequate for
the building where the modes of interest are at most a few Hz. The original
0
0.5
1
1.5
2
10
-1
10
0
10
1
10
2
10
3
10
4
Frequency [Hz]
[( g)2/Hz]
(a) PSD
0
0.5
1
1.5
2
10
-1
10
0
10
1
10
2
10
3
10
4
Frequency [Hz]
[( g)2/Hz]
(b) SV
Fig. 7.8 a PSD spectrum and b SV spectrum, synthetic data with three measured DOFs and two
modes at 1 and 1.05 Hz
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-1
-0.5
0
0.5
1
Frequency [Hz]
MAC
Fig. 7.9 MAC of the eigenvector (largest eigenvalue) of Re^Ek with the partial mode shape of
mode 1 (black circle) and mode 2 (white circle)
7.3
Singular Value Spectrum
237

analogue data is measured in volts and it is converted to be in g (=9.81 m/s2) when
divided by the sensitivity of the sensor (100 V/g). All spectra shown in this section
are one-sided.
7.4.1
Time Histories
Figure 7.10 shows the time histories of the raw data. The x and y direction are
roughly parallel to the two faces of the building. The z direction is vertical. The data
ﬂuctuates around a non-zero value, about −6.4, −9.8 and 3.5 milli-g for the x-, y-
and z-direction, respectively. This merely results from the non-zero baseline voltage
in the DAQ system. There is a slight increasing trend, or very low frequency
variations, due to channel noise. This does not matter unless one is interested in
very low frequency activities. The time histories could have been plotted with the
very low frequency trends ﬁltered for better viewing. It has not been done to
highlight this practical aspect.
7.4.2
Sample PSD (No Averaging)
Figure 7.11 shows the root PSD spectrum (no averaging) calculated using the
x-direction data. The sampling rate is 64 Hz and so the Nyquist frequency is
64/2 = 32 Hz. The data duration is 1800 s, giving a frequency interval of
1/1800 = 5.56  10−4 Hz. The spectrum drops signiﬁcantly after 27 Hz. This
0
200
400
600
800
1000
1200
1400
1600
1800
-6.8
-6.6
-6.4
-6.2
-6
-5.8
x [milli-g]
0
200
400
600
800
1000
1200
1400
1600
1800
-10.2
-10
-9.8
-9.6
-9.4
y [milli-g]
0
200
400
600
800
1000
1200
1400
1600
1800
3.2
3.4
3.6
3.8
z [milli-g]
Time [sec]
Fig. 7.10 Ambient acceleration time history at one corner on a tall building roof
238
7
Ambient Data Modeling and Analysis

merely results from decimation, where the raw digital data at 2048 Hz was low-pass
ﬁltered near 32 Hz before it was down-sampled to avoid aliasing. This part of the
spectrum is distorted and should not be used for analysis. The spectrum in Fig. 7.11
exhibits no continuity w.r.t. frequency because the values at different frequencies
are almost uncorrelated and their standard deviation is as large as their mean
(Sect. 4.10).
Viewing the spectrum over the whole sampling bandwidth, i.e., from zero to the
Nyquist frequency, is not convenient as the usual interest is on vibration modes in
some frequency bands. Suppose one is interested in the modes below 3 Hz. Then
one would zoom into the band from 0 to 3 Hz. This is shown in Fig. 7.12. The root
PSD increases as the frequency approaches zero. This is due to very low frequency
noise of the data channel. The peaks at other frequencies are indicative of potential
vibration modes.
7.4.3
Sample PSD (Averaged)
The resonance peaks in Fig. 7.12 are obscured by the random ﬂuctuation of values.
For better visualization, the averaged sample PSD is calculated over 18 segments
0
5
10
15
20
25
30
10-7
10-6
10-5
10-4
10-3
[g/ Hz]
Frequency [Hz]
Fig. 7.11 Root
PSD
spectrum
of
x-acceleration,
no
averaging
(frequency
interval
1/1800 = 5.56  10−4 Hz), shown up to the Nyquist frequency (32 Hz)
0
0.5
1
1.5
2
2.5
3
10
-7
10
-6
10
-5
10
-4
10
-3
[g/ Hz]
Frequency [Hz]
Fig. 7.12 Root
PSD
spectrum
of
x-acceleration,
no
averaging
(frequency
interval
1/1800 = 5.56  10−4 Hz), shown up to 3 Hz
7.4
Illustration with Field Data
239

(100 s each) of the original 1800 s data. Taking square root gives the spectrum in
Fig. 7.13. As a result of averaging, the c.o.v. of the sample PSD at each frequency
is reduced from 100% to 100%=
ﬃﬃﬃﬃﬃ
18
p
¼ 24%. The c.o.v. of the root PSD is
approximately half of this value, i.e., 12%. The frequency resolution of the aver-
aged spectrum is lower now, with an interval of 1/100 = 0.01 Hz. Resolution is
sacriﬁced for smoothness/accuracy.
Using a similar procedure, the averaged root sample PSD of y- and z-direction
data are produced and shown in Fig. 7.14. Potential modes are near 0.2, 0.4, 0.6,
0.7–0.8, 1.2, 1.4–1.7, 1.8–1.9 Hz. The part above 2.2 Hz is difﬁcult to interpret.
The resonance peaks indicate potential modes but they do not tell the number of
modes. To see the latter, one needs to examine the SV spectrum.
7.4.4
Singular Value Spectrum
Figure 7.15 shows the root SV spectrum with frequency interval 0.01 Hz. In a res-
onance band the number of lines signiﬁcantly above the remaining ones indicates the
number of modes. The ﬁgure suggests two modes near 0.2 Hz but only one mode near
0.4 Hz, even though for both peaks there weretwosigniﬁcant linesin Fig. 7.14. In this
0
0.5
1
1.5
2
2.5
3
10
-7
10
-6
10
-5
10
-4
10
-3
[g/ Hz]
Frequency [Hz]
Fig. 7.13 Root PSD spectrum of x-acceleration, averaged over 18 time windows of 100 s each
(frequency interval 1/100 = 0.01 Hz)
0
0.5
1
1.5
2
2.5
3
10
-7
10
-6
10
-5
10
-4
10
-3
[g/ Hz]
Frequency [Hz]
Fig. 7.14 Root PSD spectrum of acceleration data; x (blue), y (green), z (red); frequency interval
0.01 Hz
240
7
Ambient Data Modeling and Analysis

example, it is possible to tell the nature of modes since the x- and y-directions of the
sensor align with the two perpendicular translational direction of the building. The x
and y spectra in Fig. 7.14 peak at almost the same frequency, suggesting that around
0.2 Hz there is one translational mode along the x-direction and another along the
y-direction. The same observation at the peak near 0.4 Hz in Fig. 7.14 combined with
a single mode nature suggests that it is a rotational mode. In the general case, the nature
of mode may not be trivial and one will need to study the mode shape identiﬁed from
data.
Other comments are as follow. There is likely to be one mode near 0.6, 1.2 and
1.8 Hz. There are likely to be two modes in the band 0.65–0.8 Hz and three modes
near the band 1.4–1.7 Hz. There might be a mode near 2.15 Hz. Modes (if any)
between 2.2 and 3 Hz can be difﬁcult to identify. Note that modes appearing close
in one spectrum may be distinguished as well-separated in another spectrum with a
higher frequency resolution. It is prudent to examine spectra with different reso-
lutions when in doubt.
This example reveals some practical aspects of ambient data analysis.
Ascertaining the existence of modes and their number is the most important but
challenging task in modal identiﬁcation, requiring some human intelligence. Once
the data is ascertained to ﬁt into theory, the remaining computations are relatively
well-deﬁned.
7.5
Asynchronous Data
In Sect. 7.3 we saw that the number of potential modes in a frequency band is
indicated by the number of lines displaying dynamic ampliﬁcation and signiﬁcantly
above the remaining ones in the SV spectrum. Implicit in this result is that the data
at different channels are recorded in a ‘synchronous’ manner (Sect. 6.7). Otherwise,
as we will see in this section, the SV spectrum will exhibit artiﬁcial modes with the
same dynamic ampliﬁcation. For every mode the number of lines signiﬁcantly
above the remaining ones is equal to the number of measurement groups not
synchronized with each other. The partial mode shapes within each group can still
0
0.5
1
1.5
2
2.5
3
10
-7
10
-6
10
-5
10
-4
10
-3
[g/ Hz]
Frequency [Hz]
Fig. 7.15 Root SV spectrum (frequency interval 0.01 Hz)
7.4
Illustration with Field Data
241

be estimated from the eigenvectors but it is generally non-trivial to determine the
scaling among different groups to form the overall mode shape.
Asynchronous data is generally difﬁcult to model as it is intrinsically a
non-stationary process. Here, it is modeled empirically by a stationary process with
imperfect coherence in the modal response. This is found to give reasonable results
while being conducive to analysis (Zhu and Au 2017).
7.5.1
Two Measurement Groups
For instructional purpose, ﬁrst consider the case when there are two groups of data
channels. Channels of the same group are synchronized. Channels of different
groups are not. Suppose a particular frequency band is dominated by a single mode.
Let t1ðn1  1Þ and t2ðn2  1Þ be the ‘local mode shapes’ of the DOFs in Groups 1
and 2, respectively. The ‘global mode shape’ comprising all measured DOFs is
u
n1
¼
t1
t2


jjujj2 ¼ jjt1jj2 þ jjt2jj2 ¼ 1
ð7:25Þ
where n ¼ n1 þ n2.
The modal contribution to the response of the two groups of DOFs are t1n1ðtÞ
and t2n2ðtÞ, where n1ðtÞ and n2ðtÞ denote the modal response (displacement,
velocity or acceleration) according to the individual time scale (clock) of the two
groups. If the groups are synchronized, n1ðtÞ  n2ðtÞ. Otherwise they are generally
different. In the resonance band, the scaled FFT of measured data f^yjgN1
j¼0
at
frequency fk ¼ k=NDt (Hz) is modeled by
^F k ¼
t1n1k
t2n2k


þ ek
ð7:26Þ
where n1k and n2k are the scaled FFT of fn1ðtjÞgN1
j¼0 and fn2ðtjÞgN1
j¼0 , respectively;
ek (n  1 complex) is the scaled FFT of noise, with i.i.d. components and PSD Sek.
Imperfect synchronization between the two groups is modeled by
E½n1kn
1k ¼ E½n2kn
2k ¼ SkDk
E½n1kn
2k ¼ vkSkDk
ð7:27Þ
where Sk is the modal force PSD and Dk is the dynamic ampliﬁcation factor as in
(7.5); vk (complex, jvkj  1) is the coherence between n1k and n2k. That is, n1ðtÞ and
n2ðtÞ have the same statistical properties but they are not perfectly coherent. This
gives
242
7
Ambient Data Modeling and Analysis

ReEk ¼ ReE½ ^F k ^F

k ¼ SkDk
t1tT
1
v0
kt1tT
2
v0
kt2tT
1
t2tT
2


þ SekIn
ð7:28Þ
where
v0
k ¼ Revk
ð7:29Þ
Eigenvalue Properties of ReEk
To see the eigenvalues of ReEk, write it as
ReEk
nn ¼ SkDk
t1
t2


|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
U
c2
1
v0
kc1c2
v0
kc2c1
c2
2
"
#
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
C0k
t1
t2

T
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
UT
þ SekIn
¼ SkDk U
n2 C0
k
22
UT
2n þ SekIn
ð7:30Þ
where
ci ¼ ti
k k
ti ¼ ti
k k1ti
U ¼
t1
t2


ð7:31Þ
C0
k ¼
c2
1
v0
kc1c2
v0
kc2c1
c2
2


¼
c1
c2


1
v0
k
v0
k
1


c1
c2


ð7:32Þ
Note that C0
k is real-symmetric and positive semi-deﬁnite. Let fl1k; l2k  0g and
faikg2
i¼1ð2  1Þ be respectively its eigenvalues (in descending order of magnitude)
and eigenvectors, so that C0
k ¼ P2
i¼1 likaikaT
ik. Substituting into (7.30) and rear-
ranging gives
ReEk ¼
X
2
i¼1
ðlikSkDk þ SekÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
eigenvalue
uikuT
ik þ
X
n
i¼3
Sek
|{z}
eigenvalue
uikuT
ik
ð7:33Þ
where fuikgn
i¼1ðn  1Þ is an orthonormal basis with uik ¼ Uaik (i = 1, 2); and we
have used In ¼ Pn
i¼1 uikuT
ik. This shows that the largest two eigenvalues of ReEk
are (in descending order of magnitude)
kik ¼ likSkDk þ Sek
i ¼ 1; 2
ð7:34Þ
with eigenvectors fuikg2
i¼1. The remaining eigenvalues are all equal to Sek.
7.5
Asynchronous Data
243

Perfectly Synchronized Groups
If v0
k ¼ 1 then
C0
k ¼
c2
1
c1c2
c2c1
c2
2


¼
1
|{z}
eigenvalue

c1
c2


|ﬄﬄ{zﬄﬄ}
eigenvector
c1
c2

T
ð7:35Þ
which has only one non-zero eigenvalue equal to 1 with eigenvector ½c1; c2. In this
case, ReEk has only one non-zero eigenvalue, being k1k ¼ SkDk þ Sek; the
remaining ones are all equal to Sek. The eigenvector is ½c1t1; c2t2 ¼ ½t1; t2 ¼ u,
i.e., the global mode shape. This result agrees with that for synchronized data.
Perfectly Incoherent Groups
At the other extreme, if v0
k ¼ 0 then
C0
k ¼
c2
1
0
0
c2
2


¼
c2
1
|{z}
eigenvalue

1
0


|ﬄ{zﬄ}
eigenvector
1
0

T
þ
c2
2
|{z}
eigenvalue

0
1


|ﬄ{zﬄ}
eigenvector
0
1

T
ð7:36Þ
which has eigenvalues fc2
i g2
i¼1 and eigenvectors ½1; 0 and ½0; 1. Without loss of
generality, assume that c1  c2. Then the largest eigenvalue of ReEk is k1k ¼
c2
1SkDk þ Sek with eigenvector ½t1; 0. The second largest eigenvalue is k2k ¼
c2
2SkDk þ Sek
with eigenvector ½0;t2. For high s/n
ratio
ðSkDk=Sek 
 1Þ,
k1k=k2k 	 c2
1=c2
2, i.e., the ratio of eigenvalues is proportional to the scaling con-
stants of local mode shapes.
General Case
In the general case of 0  jv0
kj  1, it can be shown that the eigenvalues of C0
k are
given by
l1k ¼ 1
2 ð1 þ
ﬃﬃﬃﬃﬃﬃ
Dk
p
Þ
l2k ¼ 1
2 ð1 
ﬃﬃﬃﬃﬃﬃ
Dk
p
Þ
Dk ¼ 1  4c2
1c2
2ð1  v02
k Þ
ð7:37Þ
For ReEk, the largest two eigenvalues k1k and k2k center at SkDk=2 þ Sek with a
separation of SkDk
ﬃﬃﬃﬃﬃﬃ
Dk
p
. Their eigenvectors do not give the global mode shape; and
they depend on v0
k, c1 and c2. It can be shown that Dk is an increasing function of v0
k
ranging between 0 and 1. The separation between k1k and k2k therefore increases
with v0
k. Taking v0
k ¼ 0 or v0
k ¼ 1 gives results consistent with the previous ﬁndings
for perfectly incoherent or perfectly synchronized groups, respectively.
The above results are summarized in Fig. 7.16.
244
7
Ambient Data Modeling and Analysis

In summary, when two measurement groups are not synchronized, the SV
spectrum will show two lines signiﬁcantly above the remaining ones in the
resonance band of a single mode. The largest two eigenvalues of ReEk
depend on the coherence between the groups and the norms of local mode
shapes. For these eigenvalues, the partition in the eigenvector for the DOFs of
each group still gives the local mode shape but the whole eigenvector does
not give the global mode shape. When s/n ratio is high and the two groups
have zero coherence, the eigenvalues bear approximately the same ratio as the
squares of local mode shape norms.
Why Real Part of PSD Matrix?
It is instructive to consider the eigenvalues of Ek, which explains why in Sect. 7.3 it
is the eigenvalues of ReEk rather than Ek that are plotted in the SV spectrum.
Following a similar procedure as before, it can be shown that the largest two
eigenvalues
of
Ek
are
still
given
by
SkDkð1 
ﬃﬃﬃﬃﬃﬃ
Dk
p
Þ=2 þ Sek
but
now
Dk ¼ 1  4c2
1c2
2ð1  jvkj2Þ, i.e., with v02
k
replaced by jvkj2. Two asynchronous
groups can have small v02
k ¼ ðRevkÞ2 but not necessarily small jvkj2. For example,
two data channels measuring the same quantity with a constant time shift s, say,
xðt þ sÞ and xðtÞ, have a coherence of vk ¼ expð2pifksÞ. This gives jvkj ¼ 1
regardless of s, even though v02
k ¼ cos2ð2pfksÞ is a decreasing function of fk up to
1=4s ðHzÞ. Since jvkj ¼ 1, jDkj ¼ 1 and so k1k ¼ SDk þ Sek and k2k ¼ Sek, the
same values for synchronized data. A plot of the eigenvalues of Ek then gives no
sign of imperfect synchronization.
Example 7.3 (Asynchronous data) Consider three channels of data from
accelerometers measuring the same horizontal acceleration of a SDOF structure
with a natural frequency of about 9.85 Hz under ambient condition in the labora-
tory, as schematically shown in Fig. 7.17. Channels 1 and 2 were recorded on the
same DAQ unit but Channel 3 was on another unit. Although the digital data of all
1
|
|
0
(b)
k
ek
k
k
k
S
D
S
1
ek
nk
k
S
,...,
2
ek
k
k
k
k
S
D
S
)
1(
2
1
ek
k
k
k
k
S
D
S
)
1(
2
2
ek
nk
k
S
,...,
3
ek
k
k
S
D
S
2
1
[Hz]
fk
1
|
|
(a)
k
[Hz]
fk
0
(c)
k
ek
k
k
k
S
D
S
c2
2
2
ek
nk
k
S
,...,
3
ek
k
k
S
D
S
2
1
[Hz]
fk
ek
k
k
k
S
D
S
c2
1
1
Fig. 7.16 SV spectrum in the resonance band of a single mode for two measurement groups;
a Synchronized; b General case; c Perfectly incoherent; Dk ¼ 1  4c2
1c2
2ð1  v02
k Þ. In (c), it is
assumed that c1  c2
7.5
Asynchronous Data
245

channels were transmitted by USB cable to the same laptop and operated by the
same software to start recording at the same time, only the data from Channels 1
and 2 were synchronized. The data was acquired for one hour at 2048 Hz and later
decimated to 256 Hz for analysis.
Figure 7.18a shows the (averaged) PSD spectrum of all channels. The three lines
almost overlap because the three channels measure the same motion. Figure 7.18b
shows the SV spectrum produced from the 2  2 PSD matrix of Channels 1 and 2
only. There is one line signiﬁcantly above the other, correctly suggesting a single
mode. Figure 7.18c shows the SV spectrum produced from the 2  2 PSD matrix of
Channels 1 and 3. There are two lines exhibiting dynamic ampliﬁcation. If one did
not know that the channels were unsynchronized, these two lines would have
suggested erroneously two modes with almost the same frequency and damping
ratio. For reference, the real part of the coherence between Channels 1 and 3 near
the natural frequency is estimated to be about 0.6. Substituting this value into the
theoretical eigenvalues in (7.34) produces two lines (not shown) almost overlapping
with those in Fig. 7.18c.
■
DAQ
DAQ
Analogue data
Digital data
1
2
3
Fig. 7.17 Experimental setup in Example 7.3
8
9
10
11
12
10
-5
10
-4
10
-3
Frequency [Hz]
[g/ Hz]
(a) Root PSD
8
9
10
11
12
10
-5
10
-4
10
-3
Frequency [Hz]
(b) Root SV (syn.)
8
9
10
11
12
10
-5
10
-4
10
-3
Frequency [Hz]
(c) Root SV (unsyn.)
Fig. 7.18 Spectra of synchronous and asynchronous data. a Root PSD of Channels 1–3; b Root
SV spectrum of Channels 1 and 2 (synchronous); c Root SV of Channels 1 and 3 (asynchronous)
246
7
Ambient Data Modeling and Analysis

7.5.2
Multiple Measurement Groups
The results for two measurement groups can be generalized to multiple, say,
q groups, of data channels which are only synchronized within the group. The
mathematical arguments are similar. Analogous to (7.26),
^F k ¼
t1n1k
..
.
tqnqk
2
64
3
75 þ ek ¼
t1
..
.
tq
2
64
3
75
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
UðnqÞ
c1
..
.
cq
2
64
3
75
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
diagfcigq
i¼1ðqqÞ
n1k
..
.
nqk
2
64
3
75
|ﬄﬄﬄ{zﬄﬄﬄ}
nkðq1Þ
þ ek
ð7:38Þ
where ti ¼ ti=jjtijj and ci ¼ jjtijj ði ¼ 1; . . .; qÞ. The global mode shape is
u ¼
t1
...
tq
2
64
3
75 ¼
c1t1
...
cqtq
2
64
3
75
jjujj2 ¼ jjt1jj2 þ    þ jjtqjj2 ¼ 1
ð7:39Þ
Imperfect synchronization is modeled by E½nkn
k ¼ SkDkvk, where vk is a q  q
matrix with the ði; jÞ-entry equal to vijk (coherence between nik and njk; jvijkj  1).
Then
ReEk
nn ¼ ReE½ ^F k ^F 
k ¼ SkDk U
nq C0
k
qq
UT
qn þ SekIn
ð7:40Þ
C0
k ¼ diagfcigq
i¼1v0
kdiagfcigq
i¼1
ð7:41Þ
where v0
k ¼ Revk. It can be shown that ReEk has the eigenvector representation:
ReEk ¼
X
q
i¼1
ðlikSkDk þ SekÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
eigenvalue
uikuT
ik þ
X
n
i¼q þ 1
Sek
|{z}
eigenvalue
uikuT
ik
ð7:42Þ
where flik  0gq
i¼1 are the eigenvalues (in descending order of magnitude) of C0
k
with eigenvectors faikgq
i¼1; and
uik ¼ Uaik
i ¼ 1; . . .; q
ð7:43Þ
The largest q eigenvalues (in descending order of magnitude) of ReEk are therefore
given by
7.5
Asynchronous Data
247

kik ¼ likSkDk þ Sek
i ¼ 1; . . .; q
ð7:44Þ
with eigenvectors fuikgq
i¼1. The remaining eigenvalues are all equal to Sek.
Perfectly Synchronized Groups
If vijk ¼ 1 for all i and j, then
C0
k ¼
1
|{z}
eigenvalue

c1
...
cq
2
64
3
75
|ﬄﬄﬄ{zﬄﬄﬄ}
eigenvector
c1
...
cq
2
64
3
75
T
ð7:45Þ
which has only one non-zero eigenvalue equal to 1 with eigenvector ½c1; . . .; cq.
The largest eigenvalue of ReEk is then k1k ¼ SkDk þ Sek with eigenvector equal to
the global mode shape u ¼ ½c1t1; . . .; cqtq. This result agrees with that for syn-
chronous data.
Perfectly incoherent groups
At the other extreme, if v0
k is the identity matrix then
C0
k ¼
c2
1
..
.
c2
q
2
64
3
75
¼
c2
1
|{z}
eigenvalue

1
0
...
0
2
664
3
775
|ﬄﬄ{zﬄﬄ}
eigenvector
1
0
...
0
2
664
3
775
T
þ    þ
c2
q
|{z}
eigenvalue

0
...
0
1
2
664
3
775
|ﬄﬄ{zﬄﬄ}
eigenvector
0
...
0
1
2
664
3
775
T
ð7:46Þ
which has eigenvalues fc2
i gq
i¼1 and eigenvectors equal to the standard Euclidean
basis vectors. Without loss of generality, assume that c1  c2      cq. Then the
q largest eigenvalues (in descending order of magnitude) and eigenvectors of ReEk
are given by
kik ¼ c2
i SkDk þ Sek
ði ¼ 1; . . .; qÞ
u1k ¼
t1
0
..
.
0
2
66664
3
77775
; u2k ¼
0
t2
0
..
.
0
2
66664
3
77775
; . . .; uqk ¼
0
..
.
0
tq
2
66664
3
77775
ð7:47Þ
For high s/n ratio ðSkDk 
 SekÞ, this implies kik=kjk 	 c2
i =c2
j .
248
7
Ambient Data Modeling and Analysis

Conservation of Signal Strength
Note that 0  lik  1 since C0
k is positive semi-deﬁnite and its eigenvalues are less than
the maximum of diagonal entries (all  1). Correspondingly, kik  SkDk þ Sek (=
largest eigenvalue for synchronous data). This implies that the top line in the SV
spectrum of asynchronous data always has a smaller value than if the data were
synchronous. Nevertheless, the signal strength is ‘conserved’, in the sense that the sum
of the top q values in the SV spectrum remains the same, equal to SkDk þ qSek. This is
because the sum of eigenvalues is equal to the sum of diagonal entries, which implies
X
q
i¼1
lik ¼
X
q
i¼1
jjtijj2 ¼ 1
X
q
i¼1
kik ¼ SkDk þ qSek
ð7:48Þ
In summary, when there are q measurement groups whose data channels are
only synchronized within the group, the SV spectrum will have q lines sig-
niﬁcantly above the remaining ones. The sum of the q largest eigenvalues of
ReEk is the same regardless of the coherence among groups. These eigen-
values and their eigenvectors depend in a non-trivial manner on the coherence
among groups and the norms of local mode shapes. The partition in the
eigenvector for the DOFs of each group still gives the local mode shape but
the whole eigenvector does not give the global mode shape. When s/n ratio is
high and the groups have zero coherence, the eigenvalues bear approximately
the same ratio as the squares of local mode shape norms.
7.6
Microtremor Data
Structures situated on earth are subjected to support excitations due to motion of the
ground. Except during earthquakes, such ground motion is typically very small,
often known as ‘microtremor’. It is attributed to many sources, e.g., vibration at the
bedrock, ocean waves, winds, trafﬁc and human activities. Unlike structural
vibrations, microtremors do not exhibit distinct resonance peaks in their PSD
spectrum because they are affected by soil ampliﬁcation and surface waves rather
than structural dynamics with small damping. See Aki and Richards (2002) for
physics of seismic waves and quantitative seismology.
7.6.1
Background Seismic Noise
The characteristics of microtremor are strongly inﬂuenced by local factors (e.g., soil
ampliﬁcation) and disturbances (e.g., culture activities). The USGS New High
Noise Model (NHNM) and New Low Noise Model (NLNM) provide a reference
7.5
Asynchronous Data
249

for the background seismic activities predominantly associated with the earth
(Peterson 1993). They were produced based on the upper (NHNM) and lower
(NLNM) envelopes encompassing a large collection of PSDs calculated from a
global network of microtremor measurements at 75 (typically remote) locations,
situated at ground levels to a couple of hundred meters below ground. NLNM is
often used for benchmarking the noise of seismometers, assessing the quality of
seismic stations and determining whether certain small signals can be detected.
In the original form, the PSD in dB (decibel) is given as a piecewise linear
function of log-period:
10 log10 S
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
PSD in dB
¼ a þ b log10 T
|ﬄﬄﬄ{zﬄﬄﬄ}
log-period
ð7:49Þ
where S is the (one-sided) PSD in (m/s2)2/Hz and T is the period in s; a and b are
coefﬁcients taking different values on different intervals of periods. The values of
a and b at the segment points are given in Table 7.1 (NHNM) and Table 7.2
(NLNM). The PSD in dB and root PSD in lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
are also given. To be consistent
with (7.49), linear interpolation should only be applied between the values of PSD
in dB and log-period.
Figure 7.19a shows the PSD in dB versus the period. Figure 7.19b shows the
root PSD in g=
ﬃﬃﬃﬃﬃﬃ
Hz
p
versus frequency. Some features of the spectra have been
explained by seismologists/geophysicists; see Bormann and Wielandt (2013). For
example, the hump in Fig. 7.19a from about 3 to 30 s is associated with ocean
microseisms. The right end for periods beyond 300 s is associated with free
oscillations (‘normal modes’) of the earth. As mentioned before, these spectra
represent an ‘envelope’ of PSD observed from a variety of sites in the database
network rather than one at a particular site. That is, it is unlikely to ﬁnd a site with a
PSD taking the shape of the NHNM or NLNM. The microtremor level measured
near urban areas can be an order of magnitude higher than the NHNM.
Table 7.1 Parameter values and PSD of NHNM at segment points
T (s)
a
b
PSD in dB
Root PSD in lg/√Hz
0.1
−108.73
−17.23
−91.5
2.71E+00
0.22
−150.34
−80.5
−97.4
1.37E+00
0.32
−122.31
−23.87
−110.5
3.04E−01
0.8
−116.85
32.51
−120.0
1.02E−01
3.8
−108.48
18.08
−98.0
1.28E+00
4.6
−74.66
−32.95
−96.5
1.53E+00
6.3
0.66
−127.18
−101.0
9.08E−01
7.9
−93.37
−22.42
−113.5
2.16E−01
15.4
73.54
−162.98
−120.0
1.02E−01
20
−151.52
10.01
−138.5
1.21E−02
354.8
−206.66
31.63
−126.0
5.11E−02
100,000
−206.66
31.63
−48.5
3.83E+02
250
7
Ambient Data Modeling and Analysis

Table 7.2 Parameter values and PSD of NLNM at segment points
T (s)
a
b
PSD in dB
Root PSD in lg/√Hz
0.1
−162.36
5.64
−168.0
4.06E−04
0.17
−166.7
0.00
−166.7
4.71E−04
0.4
−170
−8.3
−166.7
4.71E−04
0.8
−166.4
28.9
−169.2
3.53E−04
1.24
−168.6
52.48
−163.7
6.66E−04
2.4
−159.98
29.81
−148.6
3.77E−03
4.3
−141.1
0.00
−141.1
8.98E−03
5
−71.36
−99.77
−141.1
8.99E−03
6
−97.26
−66.49
−149.0
3.62E−03
10
−132.18
−31.57
−163.8
6.62E−04
12
−205.27
36.16
−166.2
4.97E−04
15.6
−37.65
−104.33
−162.1
7.98E−04
21.9
−114.37
−47.1
−177.5
1.36E−04
31.6
−160.58
−16.28
−185.0
5.74E−05
45
−187.5
0.00
−187.5
4.30E−05
70
−216.47
15.7
−187.5
4.30E−05
101
−185
0.00
−185.0
5.73E−05
154
−168.34
−7.61
−185.0
5.74E−05
328
−217.43
11.9
−187.5
4.30E−05
600
−258.28
26.6
−184.4
6.16E−05
10,000
−346.88
48.75
−151.9
2.60E−03
100,000
−346.88
48.75
−103.1
7.11E−01
10
-1
10
0
10
1
10
2
10
3
-200
-180
-160
-140
-120
-100
-80
Period T (sec)
10 log10 S (dB)
NHNM
NLNM
(a)
10
-3
10
-2
10
-1
10
0
10
1
10
-12
10
-11
10
-10
10
-9
10
-8
10
-7
10
-6
10
-5
10
-4
Frequency (Hz)
Root PSD (g/ Hz)
NHNM
NLNM
(b)
Fig. 7.19 USGS New High Noise Model (NHNM) and New Low Noise Model (NLNM). In a,
the (one-sided) PSD S is in ðm=s2Þ2=Hz. Taking the square root of S and dividing by 9.81 m/s2
gives the (one-sided) root PSD in (b)
7.6
Microtremor Data
251

Example 7.4 (Ambient vibration level) How large is the ambient vibration of a
structure typically generated by micro-tremors? This question is related to whether it
is possible to identify vibration modes using ambient data under little or no dis-
turbance from wind or cultural activities. Here we make a rough assessment for a
shear building (see Example 3.4 in Sect. 3.2.2). For simplicity, assume uniform
ﬂoor mass and linear mode shape, i.e., w ¼ ½1; . . .; nsT. Then it can be reasoned that
Root PSD at building top at natural frequency 	 nsq
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
DmaxS€xg
q
ð7:50Þ
where Dmax ¼ 1=4f2 is the dynamic ampliﬁcation at the natural frequency; S€xg is
the PSD of microtremor;
q ¼ wTM1
wTMw ¼
Pns
r¼1 r
Pns
r¼1 r2 ¼
nsðns þ 1Þ=2
nsðns þ 1Þð2ns þ 1Þ=6 ¼
3
2ns þ 1
ð7:51Þ
is the ‘modal participation factor’; M denotes the mass matrix of the structure and 1
denotes a ns  1 vector of ones. Substituting these values,
Root PSD at building top at natural frequency 	
3ns
2ns þ 1 
ﬃﬃﬃﬃﬃﬃ
S€xg
p
2f
ð7:52Þ
To assess the order of magnitude, the ﬁrst factor 3ns=ð2ns þ 1Þ increases with ns and
ranges between 1 and 1.5, and so we may conservatively take it as 1. Taking f ¼ 1%
and
ﬃﬃﬃﬃﬃﬃ
S€xg
p
¼ 1 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(typical in urban areas) then (7.52) gives a root PSD of about
50 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
. This is above the noise level of servo-accelerometers (typically in the
order of 1 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
), suggesting that microtremor alone should be sufﬁcient to pro-
duce ambient data of reasonable s/n ratio for modal identiﬁcation.
■
7.6.2
Site Ampliﬁcation and H/V Spectrum
When suitably measured and analyzed, microtremor data can be used as a
non-destructive economic means to assess the dynamic ampliﬁcation characteristics
of the soil between the bedrock and the ground surface (Okada 2003; Foti et al.
2015). One empirical way is based on the ‘H/V spectrum’, which is a plot of the
PSD ratios of the horizontal to the vertical vibration (Nogoshi and Igarashi 1971;
Nakamura 1989, 2000). The method is often applied with either acceleration or
velocity measurement obtained triaxially at the site. The premise is that the
microtremor at the bedrock has similar frequency characteristics in the horizontal
and vertical directions. Horizontal motions are signiﬁcantly ampliﬁed near the
resonance frequency of the soil layer, while vertical motion is not affected because
252
7
Ambient Data Modeling and Analysis

the soil stiffness in the vertical direction is much higher. The peak location of the
H/V spectrum is insensitive to the presence of surface waves as their energy content
there is small.
Figure 7.20 shows a schematic diagram where the microtremor measured on the
ground surface comprises surface waves due to activities at the ground level and
vibrations of the bedrock transmitted and ampliﬁed through the soil. In the fre-
quency domain,
H ¼ AHHB þ HS
V ¼ AVVB þ VS
ð7:53Þ
where H, HB and HS are respectively the scaled FTs of horizontal vibration (e.g.,
acceleration) at the ground surface, at the bed rock and due to surface waves; AH is
the site ampliﬁcation (transfer function) of horizontal motion from the bed rock to
the ground surface. Analogous deﬁnitions apply to V, VB, VS and AV for the vertical
motion. These quantities are frequency dependent, although this has been omitted in
notation for simplicity.
In microtremor tests, the target is to determine jAHj but only H and V can be
calculated from measurement. A plot of jAHj versus frequency is referred as the
‘site spectrum’. This is an important quantity in earthquake engineering as it affects
the intensity and frequency characteristics of ground motions at the site.
The ‘H/V ratio’ refers to jH=Vj. The following argues that H=V 	 AH near the
resonance band of the soil layer. Using (7.53),
H
V ¼ ðHB
VB
ÞðAH þ HS=HB
AV þ VS=VB
Þ
ð7:54Þ
At the bed rock, the frequency characteristics of the horizontal and vertical motion
are similar, which means HB=VB 	 1. In typical situations where the soil is much
stiffer in the vertical direction than the horizontal, AV 	 1. These imply
H
V 	 AH þ HS=HB
1 þ VS=VB
ð7:55Þ
HB
VB
H = AHHB+HS
V = AVVB+VS
Sensor
Body waves
Bedrock
Surface waves
Soil
Fig. 7.20 Schematic diagram of microtremor
7.6
Microtremor Data
253

If there is no surface wave then HS ¼ VS ¼ 0 and so H=V 	 AH as desired. In
the general case, theoretical and empirical evidence reveal that the frequency
content of surface waves is small near the ‘site frequency’ (where jAHj is peaked).
The lowest frequency at which surface wave energy is concentrated is about twice
of the site frequency, where vertical motion is dominant. As a result, the H/V ratio
has a similar shape as jAHj near the site frequency and it has a trough at about twice
the site frequency.
The above picture is essentially one-dimensional. It grosses over spatial vari-
ability in the subsurface soil properties, surface waves and bed rock excitation in the
neighborhood of the measured location. The extent to which the H/V ratio repre-
sents site ampliﬁcation depends on the degree to which the above argument applies.
The method requires a proper choice of test location, quality assurance of calculated
spectra and interpretation of site spectrum incorporating information from site
conditions. In implementation, the H/V ratio is computed via the PSDs of
microtremor:
H
V

 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Sx þ Sy
Sz
s
ð7:56Þ
where Sx, Sy and Sz are the PSDs of microtremor data along the x, y and z (vertical)
directions, respectively. They all depend on frequency but it has been omitted in
notation.
Example 7.5 (Microtremor test) Figure 7.21 shows 900 s (sampling rate 200 Hz)
of microtremor data obtained on a site in an urban area measured by a triaxial
servo-accelerometer. Dividing the data into 90 segments produces an average
spectrum with frequency interval 0.1 Hz and c.o.v. 10% in the PSD. The resulting
root PSD spectra are shown in Fig. 7.22a. In the context of (7.56), X ¼
ﬃﬃﬃﬃﬃ
Sx
p
,
Y ¼
ﬃﬃﬃﬃﬃ
Sy
p
and Z ¼
ﬃﬃﬃﬃ
Sz
p
. The root PSDs along the x and y directions are very
similar. Although the shape of the individual spectra is non-trivial to explain, the
H/V spectrum in Fig. 7.22b exhibits dynamic ampliﬁcation between 2 and 5 Hz. It
suggests a site frequency of about 3 Hz.
■
0
100
200
300
400
500
600
700
800
900
-1
0
1 x 10
-3
x [g]
0
100
200
300
400
500
600
700
800
900
-1
0
1 x 10
-3
y [g]
0
100
200
300
400
500
600
700
800
900
-1
0
1 x 10
-3
z [g]
Time [sec]
Accelerometer
Data recorder
Console for
on-site analysis
Compass
Fig. 7.21 Microtremor test and data
254
7
Ambient Data Modeling and Analysis

7.7
Simulation of Ambient Data
To verify procedures or perform parametric studies, it is necessary to generate
synthetic data to provide numerical evidence of statistical signiﬁcance. The
time-domain data measured on a structure can be modeled as a sum of structural
response and measurement noise. Given a discrete-time history of excitation, the
structural response can be computed using a time integration scheme (Sects. 3.7
and 3.8). In this section we discuss how to generate a sample time history of a
stationary Gaussian process with a given PSD matrix. This allows one to generate
ambient excitation and measurement noise. We also discuss how to specify channel
noise level to produce data of speciﬁed s/n ratio.
7.7.1
Gaussian Scalar Process
A discrete time history fXjgN1
j¼0 of white noise with (two-sided) PSD S and time
interval Dt (s) can be generated by
Xj ¼
ﬃﬃﬃﬃﬃ
S
Dt
r
Zj
j ¼ 0; . . .; N  1
ð7:57Þ
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
10
-7
10
-6
10
-5
10
-4
(a) Root PSD [g/ Hz]
Z
Y
X
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
0
2
4
6
8
(b) H/V = (X2+Y2)0.5/Z
Frequency [Hz]
Fig. 7.22 Microtremor spectra; a Root PSD; b H/V
7.7
Simulation of Ambient Data
255

where fZjgN1
j¼0
are i.i.d. standard Gaussian random variables. As a check,
var½Xj ¼ S=Dt. This is consistent with the Parseval equality; the Nyquist frequency
is 1=2Dt ðHzÞ and so the area under the two-sided PSD is 2S  ð1=2DtÞ ¼ S=Dt.
In the general case, the following algorithm generates a random time history
fXjgN1
j¼0 at time interval Dt (s) of a stochastic stationary process based on its PSD
values fSk  0gNyq
k¼0 at the FFT frequencies ffk ¼ k=NDtgNyq
k¼0, where Nyq = integer
part of N=2, is the index at or just below the Nyquist frequency.
Gaussian Scalar Process with PSD fSkgNyq
k¼0
1. Generate an i.i.d. standard Gaussian sequence fZkgN1
k¼0 .
2. Generate a complex Gaussian sequence fWkgN1
k¼0 by
W0 ¼ ðS0N
Dt Þ1=2Z0
ð7:58Þ
Wk ¼ ðSkN
2DtÞ1=2ðZk þ iZNkÞ
k ¼ 1; . . .; Nyq  1
ð7:59Þ
WNyq ¼
ð
SNyqN
2Dt Þ1=2ðZNyq þ iZNNyqÞ
N odd
ð
SNyqN
Dt Þ1=2ZNyq
N even
(
ð7:60Þ
Wk ¼ W
Nk
k ¼ Nyq þ 1; Nyq þ 2; . . .; N  1
ð7:61Þ
3. Compute fXjgN1
j¼0 as the inverse FFT of fWkgN1
k¼0 :
Xj ¼ 1
N
X
N1
k¼0
Wke2pijk=N
j ¼ 0; . . .; N  1
ð7:62Þ
To remove residual imaginary part in fXjgN1
j¼0 due to round off errors, its
real part should be taken as the sample history.
Proof of Algorithm
To see why the algorithm works, ﬁrst note that the FFT of fXjgN1
j¼0
is simply
fWkgN1
k¼0 . It is real-valued because fWkgN1
k¼0 has conjugate mirror pattern about the
index Nyq. By deﬁnition, the PSD of fXjgN1
j¼0 is
eSk ¼ Dt
N E½WkW
k 
ð7:63Þ
256
7
Ambient Data Modeling and Analysis

This can be shown to be equal to Sk as follow. For k ¼ 0,
eS0 ¼ Dt
N E½W0W
0 ¼ Dt
N  S0N
Dt E½Z2
0
|ﬄ{zﬄ}
1
¼ S0
ð7:64Þ
For k ¼ 1; . . .; Nyq  1, since Zk and ZNk are independent,
eSk ¼ Dt
N E½WkW
k  ¼ Dt
N  SkN
2Dt fE½Z2
k 
|ﬄ{zﬄ}
1
þ E½Z2
Nk
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
1
g ¼ Sk
ð7:65Þ
When Nyq is odd,
eSNyq ¼ Dt
N E½WNyqW
Nyq ¼ Dt
N  SNyqN
2Dt fE½Z2
Nyq
|ﬄﬄﬄ{zﬄﬄﬄ}
1
þ E½Z2
NNyq
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
1
g ¼ SNyq
ð7:66Þ
When Nyq is even,
eSNyq ¼ Dt
N E½WNyqW
Nyq ¼ Dt
N  SNyqN
Dt
E½Z2
Nyq
|ﬄﬄﬄ{zﬄﬄﬄ}
1
¼ SNyq
ð7:67Þ
For k ¼ Nnyq þ 1; Nnyq þ 2; . . .; N  1, eSk ¼ Sk follows because Wk ¼ W
Nk.
■
Example 7.6 (Conjugate mirror pattern) Here we illustrate the calculation of
fWkgN1
k¼0 according to (7.58)–(7.61), which is slightly different depending on
whether N is odd or even. Suppose N ¼ 7 (odd). Then Nyq = integer part of
7/2 = 3. Equation (7.58) gives
W0 ¼ ðS0N
Dt Þ1=2Z0
Equation (7.59) gives, for k ¼ 1; 2,
W1 ¼ ðS1N
2DtÞ1=2ðZ1 þ iZ6Þ
W2 ¼ ðS2N
2DtÞ1=2ðZ2 þ iZ5Þ
Equation (7.60) gives (N odd)
W3 ¼ ðS3N
2DtÞ1=2ðZ3 þ iZ4Þ
7.7
Simulation of Ambient Data
257

Equation (7.61) gives the conjugate mirror image, for k ¼ 4; 5; 6,
W4 ¼ W
3
W5 ¼ W
2
W6 ¼ W
1
The resulting sequence has the pattern
W0; W1; W2; W3; W
3; W
2; W
1
Suppose now N ¼ 8 (even). Then Nyq = integer part of 8/2 = 4. Equation (7.58)
gives
W0 ¼ ðS0N
Dt Þ1=2Z0
Equation (7.59) gives, for k ¼ 1; 2; 3,
W1 ¼ ðS1N
2DtÞ1=2ðZ1 þ iZ7Þ
W2 ¼ ðS2N
2DtÞ1=2ðZ2 þ iZ6Þ
W3 ¼ ðS3N
2DtÞ1=2ðZ3 þ iZ5Þ
Equation (7.60) gives (N even)
W4 ¼ ðS4N
Dt Þ1=2Z4
Equation (7.61) gives, for k ¼ 5; 6; 7,
W5 ¼ W
3
W6 ¼ W
2
W7 ¼ W
1
The resulting sequence has the pattern
W0; W1; W2; W3; W4; W
3; W
2; W
1
The mirror image patterns are visualized in Fig. 7.23.
■
7.7.2
Gaussian Vector Process
The algorithm for scalar process can be extended to vector process given the PSD
matrices fSkgNyq
k¼0 (n  n Hermitian) at the FFT frequencies ffk ¼ k=NDtgNyq
k¼0. For
each k, let fkikgn
i¼1 be the eigenvalues of Sk; and Uk be its unitary eigenmatrix, i.e.,
U
kUk ¼ In. Then Sk ¼ Ukdiagfkikgn
i¼1U
k. Let Lk ¼ Ukdiagf
ﬃﬃﬃﬃﬃﬃ
kik
p
gn
i¼1 so that
Sk ¼ LkL
k. The algorithm for generating a sample history fXjgN1
j¼0 ðn  1Þ is as
follow.
258
7
Ambient Data Modeling and Analysis

Gaussian Vector Process with PSD Matrix fSkgNyq
k¼0
1. Generate an i.i.d. standard Gaussian vector sequence fZkgN1
k¼0 ðn  1Þ.
2. Generate a complex Gaussian vector sequence fWkgN1
k¼0 (n  1 complex)
by
W0 ¼ ðN
DtÞ1=2L0Z0
ð7:68Þ
Wk ¼ ð N
2DtÞ1=2LkðZk þ iZNkÞ
k ¼ 1; . . .; Nyq  1
ð7:69Þ
WNyq ¼
ð N
2DtÞ1=2LNyqðZNyq þ iZNNyqÞ
N odd
ðN
DtÞ1=2LNyqZNyq
N even
(
ð7:70Þ
Wk ¼ W
Nk
k ¼ Nyq þ 1; Nyq þ 2; . . .; N  1
ð7:71Þ
3. Compute fXjgN1
j¼0 ðn  1Þ as the inverse FFT of fWkgN1
k¼0 :
Xj ¼ 1
N
X
N1
k¼0
Wke2pijk=N
j ¼ 0; . . .; N  1
ð7:72Þ
To remove residual imaginary part in fXjgN1
j¼0 due to round off errors, its
real part should be taken as the sample history.
Re
Im
Re
Im
0
k
1
2
3
4
5
6
7
0
k
1
2
3
4
5
6
7
N
(a)
8
N
(b)
Fig. 7.23 Conjugate mirror pattern. a N = 7 (odd); b N = 8 (even)
7.7
Simulation of Ambient Data
259

It can be shown by similar steps in Sect. 7.7.1 that the PSD matrix of fXjgN1
j¼0 is
equal to Sk. For example, for k ¼ 1; . . .; Nyq  1, the PSD matrix is given by
~Sk ¼ Dt
N E½WkW
k ¼ Dt
N  N
2Dt LkE½ðZk þ iZNkÞðZT
k  iZT
NkÞL
k
¼ 1
2 LkfE½ZkZT
k 
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
In
þ E½ZNkZT
Nk
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
In
gL
k ¼ LkL
k ¼ Sk
ð7:73Þ
7.7.3
Quantifying Noise Level
The performance of modal identiﬁcation algorithms are often tested w.r.t. the level
of noise. For OMA, a direct measure is the ratio of the PSDs of modal response to
channel noise at the natural frequency. For discussion purpose, consider accelera-
tion data in g. For a single mode with damping ratio f and modal force PSD
S ðg2=HzÞ, the PSD of modal acceleration at the natural frequency is S=4f2.
Assuming a channel noise PSD of Se ðg2=HzÞ, the s/n ratio is given by
c ¼
S
4Sef2
ð7:74Þ
In the literature, one conventional measure for the s/n ratio is in terms of RMS
(root mean square) value. For modal identiﬁcation this is not a direct measure
because the RMS value depends on the PSD level in the resonance band as well as
other bands. It also depends on the sampling bandwidth. For example, suppose the
channel noise is white with (two-sided) PSD Se. For a sampling rate of fs (Hz), the
Nyquist frequency is fs=2 and the variance of noise is Sefs, giving a RMS value of
ﬃﬃﬃﬃﬃﬃﬃ
Sefs
p
. On the other hand, the RMS acceleration of the mode is about
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pSf =2f
p
(Sect. 5.1.4), where f (Hz) is the natural frequency. Then
RMS of signal
RMS of noise ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pSf =2f
Sefs
s
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c  2pf f
fs
s
ð7:75Þ
As this ratio depends on the sampling rate fs, it would be difﬁcult to directly judge
the quality of data based on its order of magnitude.
To illustrate, suppose f = 1 Hz, f ¼ 1%, S ¼ 1 ðlgÞ2=Hz and Se ¼ 1 ðlgÞ2=Hz.
Then
the
s/n
ratio
in
terms
of
PSD
at
the
natural
frequency
is
c ¼ 1=4ð1Þð0:01Þ2 ¼ 2500. This gives a SV-spectrum similar to Fig. 7.5b in
Example 7.1. It corresponds to very good quality data and the mode can be
well-identiﬁed. Quantifying based on RMS ratio, however, may give a different
impression depending on the sampling rate fs. If fs ¼ 100 Hz, then the s/n RMS
260
7
Ambient Data Modeling and Analysis

ratio is about 1.25, which appears a very high noise level and suggests poor quality
data. This is misleading, simply because the RMS of noise is made arbitrary large as
a result of the sampling rate. If fs ¼ 25 Hz, the s/n RMS ratio is doubled to 2.5. If
fs ¼ 400 Hz, the s/n RMS ratio is reduced by half to 0.63. In all these cases, the SV
spectrum of data remains essentially the same as in Fig. 7.5b with a similar value of
c 	 2500; and hence the same quality of data for modal identiﬁcation.
As an example with experimental data, Fig. 7.24 shows the time history and root
PSD spectrum of acceleration measured on a SDOF structure by a MEMS
accelerometer, sampled at (a) 250 Hz and (b) 50 Hz. The data in (b) is obtained by
decimating the data in (a). The time history in (a) appears noisier than that in (b).
Nevertheless, both will give very similar identiﬁcation results for the mode near
7 Hz because their spectra near the natural frequency are the essentially same. In
terms of the PSD at the natural frequency, the s/n ratio of (a) and (b) are the same.
However, the s/n RMS ratio in (b) is more than double of that in (a).
References
Aki K, Richards PG (2002) Quantitative seismology, 2nd edn. University Science Books,
Sausalito, CA
Bendat JS, Piersol AG (1993) Engineering applications of correlation and spectral analysis. Wiley,
New York
Bormann P, Wielandt E (2013) Seismic signals and noise. In: Bormann P (ed) New manual of
seismological observatory practice 2 (NMSOP2). Deutsches GeoForschungsZentrum GFZ,
Potsdam, pp 1–62. doi:10.2312/GFZ.NMSOP-2_ch4
0
200
400
600
-2
-1
0
1
2
(a)     
Time history [milli-g]
0
5
10
15
20
25
10
-6
10
-5
10
-4
10
-3
Root PSD [g/ Hz]
0
200
400
600
-2
-1
0
1
2
(b)     
Time [sec]
0
5
10
15
20
25
10
-6
10
-5
10
-4
10
-3
Frequency [Hz]
Fig. 7.24 Acceleration data sampled at a 250 Hz and b 50 Hz. Time history appears noisier in
(a) than in (b) but both give essentially the same identiﬁcation results for the mode near 7 Hz
7.7
Simulation of Ambient Data
261

Brillinger DR (1981) Time series: data analysis and theory. Holden-Day Inc, San Francisco
Elsner JB, Tsonis AA (1996) Singular spectrum analysis: a new tool in time series analysis.
Plenum Press
Foti S, Lai CG, Rix GJ et al (2015) Surface wave methods for near-surface site characterization.
CRC Press, Boca Raton
Golyandina N, Zhigljavsky A (2013) Singular spectrum analysis for time series. Springer Briefs in
Statistics, Springer, Berlin, ISBN 978-3-642-34912-6
Kay S (1993) Fundamentals of statistical signal processing: estimation theory. Prentice-Hall,
Englewood Cliffs, NJ
Nagoshi M, Igarashi T (1971) On the amplitude characteristics of microtremor (part 2). J Seismol
Soc Jpn 24:26–40
Nakamura Y (1989) A method for dynamic characteristics estimation of subsurface using
microtremor on the ground surface. Q Rep Railway Tech Res Inst (RTRI) 30(1)
Nakamura Y (2000) Clear identiﬁcation of fundamental idea of Nakamura’s technique and its
applications. In: Proceedings of 12th world congress on earthquake engineering, Auckland,
New Zealand
Okada H (2003) The microtremor survey method. Soc Explor Geophys
Peterson J (1993) Observations and modeling of seismic background noise. US Geological Survey,
Open File Report (No. 93-322), US
Zhu YC, Au SK (2017) Spectral characteristics of asynchronous data in operational modal
analysis. Struct Control Health Monit. doi:10.1002/stc.1981
262
7
Ambient Data Modeling and Analysis

Part II
Inference

Chapter 8
Bayesian Inference
Abstract This chapter introduces the Bayesian approach for system identiﬁcation
in a general context. The concept of ‘identiﬁability’ is introduced, which affects
how computations should be performed for making inference. Computational
strategies for ‘globally identiﬁable’, ‘local identiﬁable’ and ‘unidentiﬁable’ situa-
tions are further discussed in individual sections. ‘Model class selection’ is also
introduced, which is an important topic for validating models beyond system
identiﬁcation.
Keywords Bayesian  Identiﬁability  Gaussian approximation  Model class
selection
System identiﬁcation is concerned with making inference about the parameters of
interest based on the measured data and mathematical model of a system.
Philosophically, it is impossible to identify the parameters exactly because of
incomplete information in the data in terms of amount and precision, and the fact
that the identiﬁcation model can be potentially wrong. A Bayesian approach
explicitly acknowledges this fact. Through Bayes’ Theorem, it provides a funda-
mental means for processing information in the data and making inference on the
parameters in a manner consistent with modeling assumptions and probability as a
logic.
Not all inference problems demand a Bayesian approach to solve. However,
there are many problems where Bayesian approach offers a clear advantage over
non-Bayesian ones. Examples are problems where it is non-trivial to construct an
estimator for the parameters of interest; problems with signiﬁcant identiﬁcation
uncertainty, for which it is necessary to quantify; problems where identiﬁcation
results depend critically on the underlying assumptions and it is necessary to
quantify the effect or to determine which set of assumptions is better.
Bayesian system identiﬁcation is a general topic with wide applications in sci-
ence and engineering. Classical texts on Bayesian probability include Cox (1961),
Jeffreys (1961), Box and Tiao (1992) and Jaynes (2003). See also book reviews
with highlights of concepts, e.g., Garrett (2004), Fine (2004) and Robert et al.
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_8
265

(2009). A good exposition of Bayesian perspective with structural system identi-
ﬁcation background can be found in Beck (2010).
In this chapter we introduce the Bayesian approach for system identiﬁcation. The
problem is classiﬁed based on ‘identiﬁability’, which determines the computational
tool that should be applied. The chapter does not cover topics such as the choice of
prior distribution and Markov Chain Monte Carlo (MCMC) method. These are
important general topics but are currently of little relevance to operational modal
analysis (OMA). The ﬁrst topic regarding priors can be found in the classical texts
just mentioned. References for MCMC can be found in Sect. 8.7.
8.1
Bayes’ Theorem
Bayes’ Theorem is the fundamental equation that allows one to infer the plausibility
of a statement of interest conditional on the stated information. Mathematically, it is
simply a statement of conditional probability but the underlying philosophy is far
reaching. In a Bayesian perspective, all probabilities are conditional on the infor-
mation used to obtain them, although for notational simplicity the conditioning
statement may not be fully indicated when it is understood. The probability of an
event is a function of its conditioning information. There is no ‘true’ or ‘intrinsic’
probability. The probability of ﬂipping a fair coin and getting a head is commonly
perceived to have an intrinsic probability of 50%. The usual conditioning (as-
sumption) behind this answer is that the conditions determining the outcome are
unknown (all we know is the coin being ﬂipped) and they are not expected to be
biased towards head or tail. Theoretically, if one knows with perfect precision the
initial conditions of ﬂipping and all environmental conditions throughout the pro-
cess of ﬂipping and landing, it is possible to determine the outcome (imagine
solving the governing equation of motion with known physical properties, initial
conditions and excitation time history). Conditional on all of this information (a
huge amount), the probability of obtaining a head is either 0 or 1, depending on
what the information implies.
According to Bayes’ theorem, for three propositions A, B and C,
PðAjB; CÞ ¼ PðBjA; CÞ PðAjCÞ
PðBjCÞ
ð8:1Þ
Here, C contains the information (e.g., a probability model) that speciﬁes the
probability of A and B in the absence of any other information. The conditional
probability PðAjB; CÞ asks question about A given the (additional) information
speciﬁed by B when B is assumed to be true. Bayes’ Theorem swaps the roles of A
and B, turning this question into one about B given A. This is of utility when it is
non-trivial to answer the question about A using the information about B, but is
feasible to do the reverse. The term PðAjCÞ refers to the probability in the absence
of the information speciﬁed by B. The term PðBjCÞ can be interpreted similarly.
266
8
Bayesian Inference

Since the probabilities in (8.1) are all conditional on the same information speciﬁed
by C, such information is usually omitted for notational simplicity. Conventionally,
the equation is written as
PðAjBÞ ¼ PðBjAÞ PðAÞ
PðBÞ
ð8:2Þ
8.2
Updating Knowledge Using Data
Let h be a vector containing the parameters of a model to be identiﬁed from
measured data Z. Without much loss of generality, h and Z are assumed to be
continuous-valued. In modal identiﬁcation problems where a structural dynamic
model is assumed, h may comprise the natural frequencies, damping ratios, mode
shapes, etc. The data Z may comprise, e.g., the measured acceleration time histo-
ries. We want to ask questions about h given the information in Z. Using Bayes’
Theorem,
pðhjZÞ ¼ pðZjhÞ pðhÞ
pðZÞ
ð8:3Þ
The term pðhjZÞ is called the ‘posterior distribution’, or posterior PDF (proba-
bility density function), of h given Z. ‘Posterior’ here means that it is after using the
information in Z. The posterior distribution encapsulates all information about h
that can be inferred from Z in a manner consistent with the speciﬁed modeling
assumptions, which have been omitted in the conditioning of probabilities in (8.3).
Instead of inferring a single value of h from the data Z, the Bayesian approach
yields the conditional distribution of h, pðhjZÞ, which is a function of the available
data (and modeling assumptions). Based on the information in the data, the likely
values that h may take are those with a high posterior PDF value. The identiﬁcation
uncertainty associated with h is related to the spread of the posterior PDF. In
applications, these concepts may be quantiﬁed in terms of ‘descriptive statistics’,
e.g., the mean and variance, associated with the posterior PDF.
Normalizing Constant, Prior and Likelihood Function
Note that pðhjZÞ is a probability distribution for h. On the RHS of (8.3), the term
pðZÞ does not depend on h. Its role is a normalizing constant and it does not affect
the shape of the posterior distribution. It plays no role in the identiﬁcation of h for a
given set of modeling assumptions, but it is the key measure when the latter is
scrutinized, as will be discussed in Sect. 8.8. The term pðhÞ is called the ‘prior
distribution’. It reﬂects one’s knowledge about h in the absence of data. ‘Prior’ here
means that it is before the information in Z is used. Standard distributions (e.g.,
Gaussian, exponential) are commonly used for assigning prior distribution. The
term pðZjhÞ is called the ‘likelihood function’. It is derived based on modeling
8.1
Bayes’ Theorem
267

assumptions that give a probabilistic description (distribution) for the possible
values of Z for a given h. It is the heart of the Bayesian approach and it directly
affects the validity of identiﬁcation results.
8.3
System Identiﬁcation Framework
Solving a system identiﬁcation problem using the Bayesian approach involves the
following basic ingredients:
1. Formulate the likelihood function pðZjhÞ based on modeling assumptions. This
involves physical/logical modeling of system prediction as well as its discrep-
ancy from measured data (prediction error).
2. Assign a prior distribution pðhÞ based on available knowledge.
3. Determine the descriptive posterior statistics (e.g., mean, variance) of h asso-
ciated with the posterior distribution pðhjZÞ for convenient interpretation and
decision making. This is primarily a computational problem.
The difﬁculty of computing the posterior statistics depends on the topology of
the posterior distribution. This can be classiﬁed in terms of ‘identiﬁability’ of the
problem, which is related to the nature of measured data in providing sufﬁcient
information for identifying the model parameters. This is discussed in the next
section.
8.4
Identiﬁability
One way to classify a Bayesian identiﬁcation problem is based on the topology of
the region in the model parameter space that carries signiﬁcant probability content
of the posterior distribution. The problem can be classiﬁed as either ‘globally
identiﬁable’, ‘locally identiﬁable’ or ‘unidentiﬁable’ (Beck and Katafygiotis 1998).
It is ‘globally identiﬁable’ if the posterior PDF has a single (unique) peak in the
parameter space. If there is more than one peak, the problem is said to be ‘locally
identiﬁable’. If there is a continuum of points with the same value of the posterior
PDF, the problem is ‘unidentiﬁable’.
Identiﬁability is related to whether the measured data provides sufﬁcient infor-
mation for identifying the model parameters. It is concerned with the nature of
identiﬁcation results rather than their precision, although these two aspects are
related; unidentiﬁable problems often have much larger posterior uncertainty than
identiﬁable ones. Note that if a constant prior PDF is chosen, then identiﬁability is
determined by the likelihood function. Problems are unidentiﬁable because the data
does not provide sufﬁcient information about the parameters, often because of its
nature rather than its amount. For example, a structural mode is unidentiﬁable when
268
8
Bayesian Inference

it is identiﬁed based on vibration data measured near the node of the mode shape,
no matter how long the data is. Identiﬁability is determined by the joint PDF rather
than the marginal ones. It is possible for some parameters to have a unimodal
marginal distribution in an unidentiﬁable problem.
The computational strategy for calculating posterior statistics depends on iden-
tiﬁability. We discuss the strategies in different cases in Sects. 8.5–8.7. Modal
identiﬁcation problems are globally identiﬁable, or else one better change the
experimental setup to make it so. Readers primarily interested in OMA may just
focus on Sect. 8.5. Before we leave this section we consider an example that
illustrates the identiﬁability of the stiffness of a simple structure, depending on the
amount of modal information available.
Example 8.1 (Two-DOF structure, identiﬁability) Consider a two-DOF structure
as shown in Fig. 8.1. The masses are known and equal to 1 kg. The stiffness
parameters are unknown and are denoted by h1 and h2 (N/m).
Suppose we want to identify the stiffness parameters using vibration data of the
structure, denoted by Z. A common way is to adopt a ‘two-stage approach’ where
one ﬁrst identiﬁes the natural frequencies and mode shapes (Stage I), collectively
denoted by -, and then uses this information to identify the stiffness parameters
(Stage II). Let pIð-jZÞ denote the posterior PDF of - obtained in Stage I assuming
a uniform prior PDF for -. It can be reasoned that pIð-jZÞ is approximately a
Gaussian PDF and the modal properties are independent when the signal-to-noise
(s/n) ratio of data is high (Au and Zhang 2015). Let ~-ðhÞ denote the theoretical
prediction of - for a given h ¼ ½h1; h2T. Assuming a uniform prior PDF for h, it
can be shown that
pðhjZÞ / pIð~-ðhÞjZÞ
ð8:4Þ
Model Prediction of Modal Properties
The theoretical modal properties can be obtained by solving the eigenvalue equa-
tion Kw ¼ x2Mw, where K and M are respectively the stiffness and mass matrix;
x and w are respectively the natural frequency (in rad/s) and mode shape. In the
present case,
K ¼
h1 þ h2
h2
h2
h2


½N=m]
M ¼
1
0
0
1


½kg]
ð8:5Þ
1kg
1kg
[N/m]
1
[N/m]
2
dof 1
dof 2
Fig. 8.1 Two-DOF structure
8.4
Identiﬁability
269

Solving the eigenvalue problem yields the natural frequencies (in rad/s) as
~x1 ¼ 1ﬃﬃﬃ
2
p
h1 þ 2h2  ðh2
1 þ 4h2
2Þ1=2
h
i1=2
~x2 ¼ 1ﬃﬃﬃ
2
p
h1 þ 2h2 þ ðh2
1 þ 4h2
2Þ1=2
h
i1=2
ð8:6Þ
Normalizing the mode shapes to be 1 at DOF 2, they are given by
~w1 ¼
~u1
1


~w2 ¼
~u2
1


ð8:7Þ
where
~u1 ¼ 1  ~x2
1
h2
~u2 ¼ 1  ~x2
2
h2
ð8:8Þ
Clearly, ~x1, ~x2, ~u1 and ~u2 are determined by ðh1; h2Þ. Figure 8.2 shows the con-
tours of their values in the space of ðh1; h2Þ.
4
4
4
6
6
8
8
10
10
12
0
200
400
600
0
200
400
600
12
18
24
24
30
30
36
0
200
400
600
0
200
400
600
0 1
0.1
0.3
0.3
0.5
0.5
0.7
0.7
0.9
1 [N/m]
2 [N/m]
0
200
400
600
0
200
400
600
-10
-10
-5
-5
-2.5
-2.5
-2
-2
-1.8
-1.8
-1.6
-1.6
-1.4
-1.4
-1.2
0
200
400
600
0
200
400
600
[rad/sec]
~
1
[rad/sec]
~
2
1~u
2
~u
Fig. 8.2 Contour of modal properties, two-DOF structure
270
8
Bayesian Inference

Posterior PDF of Stiffness Parameters
The posterior PDF of ðh1; h2Þ depends on what modal properties are identiﬁed as
well as their identiﬁcation uncertainty in Stage I. Figure 8.3 shows the posterior
PDF based on one hypothetical data set. Depending on the measured DOFs and
identiﬁed modes, ðh1; h2Þ can be globally identiﬁable (‘G’), locally identiﬁable (‘L’)
or unidentiﬁable (‘U’). These are further explained in Example 8.2.
■
Example 8.2 (Two-DOF structure, identiﬁability explained) This example
explains the identiﬁability in Fig. 8.3 of Example 8.1. The story is a bit long and it
may be skipped on ﬁrst reading. To avoid unnecessary complications, we omit
numerical details of the identiﬁed modal properties during the discussion. They can
be found at the end of this example.
DOF 1 Measured, Mode 1 Identiﬁed
First, consider the case when only DOF 1 is measured and only mode 1 is iden-
tiﬁed. In this case no mode shape information is available. The identiﬁed modal
U
dof 1
mode 1
0
200
400
600
0
200
400
600
U
mode 2
0
200
400
600
0
200
400
600
L
mode 1 & 2
0
200
400
600
0
200
400
600
U
dof 2
0
200
400
600
0
200
400
600
U
0
200
400
600
0
200
400
600
L
0
200
400
600
0
200
400
600
G
dof 1 & 2
0
200
400
600
0
200
400
600
G
0
200
400
600
0
200
400
600
G
1 [N/m]
2 [N/m]
0
200
400
600
0
200
400
600
θ
θ
Fig. 8.3 Contour of posterior PDF of stiffness parameters ðh1; h2Þ identiﬁed based on different
modal information. Measured DOFs indicated in different rows, identiﬁed modes in different
columns. ‘G’—globally identiﬁable; ‘L’—locally identiﬁable; ‘U’—unidentiﬁable
8.4
Identiﬁability
271

properties comprise only the ﬁrst mode natural frequency, i.e., -  x1.
Equation (8.4) reads
pðhjZÞ / pIð~x1ðhÞjZÞ ¼
1
ﬃﬃﬃﬃﬃﬃ
2p
p
^rx1
exp
 ½~x1ðhÞ  ^x12
2^r2
x1
(
)
ð8:9Þ
where ^x1 and ^r2
x1 are respectively the posterior MPV (most probable value) and
variance of x1 identiﬁed in Stage I. This is an exponentially decaying function of
½~x1ðhÞ  ^x12 and so it peaks at those values of h for which ~x1ðhÞ ¼ ^x1. These
values correspond to the contour line at level ^x1 (8 rad/s, see Table 8.1) in the
(1,1)-subplot of Fig. 8.2. This line is reproduced in the (1,1)-subplot in Fig. 8.4.
Since there is a continuum of MPVs, the stiffness parameters are unidentiﬁable.
Table 8.1 Numerical data of identiﬁed modal properties, two-DOF structure
Mode i
^xi [rad/s]
^rxi [rad/s]
^ui
^rui
1
8
0.8
0.16
0.016
2
24
2.4
−6.5
0.65
U
dof 1
mode 1
0
200
400
600
0
200
400
600
U
mode 2
0
200
400
600
0
200
400
600
L
mode 1 & 2
0
200
400
600
0
200
400
600
U
dof 2
0
200
400
600
0
200
400
600
U
0
200
400
600
0
200
400
600
L
0
200
400
600
0
200
400
600
G
dof 1 & 2
0
200
400
600
0
200
400
600
G
0
200
400
600
0
200
400
600
G
0
200
400
600
0
200
400
600
1 [N/m]
2 [N/m]
θ
θ
Fig. 8.4 MPV of ðh1; h2Þ based on different modal information
272
8
Bayesian Inference

DOF 1 Measured, Mode 2 Identiﬁed
The situation is similar when only DOF 1 is measured and only mode 2 is iden-
tiﬁed. In this case -  x2 and
pðhjZÞ ¼ pIð~x2ðhÞjZÞ ¼
1
ﬃﬃﬃﬃﬃﬃ
2p
p
^rx2
exp
 ½~x2ðhÞ  ^x22
2^r2
x2
(
)
ð8:10Þ
where ^x2 and ^r2
x2 are respectively the posterior MPV and variance of x2 identiﬁed
in Stage I. The stiffness parameters h are unidentiﬁable as there is a continuum of
MPVs that satisfy ~x2ðhÞ ¼ ^x2. These form the line in the (1,2)-subplot of Fig. 8.4.
DOF 1 Measured, Mode 1 and 2 Identiﬁed
When only DOF 1 is measured but both mode 1 and 2 are identiﬁed, - ¼ fx1; x2g
and
pðhjZÞ / pIð~x1ðhÞjZÞpIð~x2ðhÞjZÞ
¼
1
2p^rx1^rx2
exp
 ½~x1ðhÞ  ^x12
2^r2
x1
 ½~x2ðhÞ  ^x22
2^r2
x2
(
)
ð8:11Þ
The MPVs of h are the values that satisfy both ~x1ðhÞ ¼ ^x1 and ~x2ðhÞ ¼ ^x2, i.e.,
the intersection of the two lines in the (1,3)-subplot of Fig. 8.4. The two inter-
sections give two (local) MPVs, and so the problem is locally identiﬁable. The three
cases in the second row of Fig. 8.3 when DOF 2 is measured can be explained in a
similar manner.
DOFs 1 and 2 Measured, Mode 1 Identiﬁed
Consider now when DOFs 1 and 2 are measured. When only mode 1 is identiﬁed,
- ¼ fx1; u1g, where u1 is the mode shape value of mode 1 at DOF 1. Then
pðhjZÞ / pIð~x1ðhÞjZÞpIð~u1ðhÞjZÞ
¼
1
2p^rx1^ru1
exp
 ½~x1ðhÞ  ^x12
2^r2
x1
 ½~u1ðhÞ  ^u12
2^r2
u1
(
)
ð8:12Þ
where ^u1 and ^r2
u1 are respectively the posterior MPV and variance of u1. The MPVs
of h are the values that satisfy both ~x1ðhÞ ¼ ^x1 and ~u1ðhÞ ¼ ^u1, i.e., the inter-
section of the two lines in the (3,1)-subplot of Fig. 8.4. Similarly, when both DOFs
1 and 2 are measured and only mode 2 is identiﬁed, the MPVs correspond to the
intersection of the two lines in the (3,2)-subplot of Fig. 8.4. In both cases there is
only one intersection and so the problem is globally identiﬁable.
DOFs 1 and 2 Measured, Mode 1 and 2 Identiﬁed
Finally, when DOFs 1 and 2 are measured and mode 1 and 2 are identiﬁed,
- ¼ fx1; u1; x2; u2g and
8.4
Identiﬁability
273

pðhjZÞ / pIð~x1ðhÞjZÞpIð~u1ðhÞjZÞpIð~x2ðhÞjZÞpIð~u2ðhÞjZÞ
ð8:13Þ
The MPVs of h need not correspond to those values that satisfy ~x1ðhÞ ¼ ^x1,
~x2ðhÞ ¼ ^x2, ~u1ðhÞ ¼ ^u1 and ~u2ðhÞ ¼ ^u2, because it is generally impossible to
satisfy four equations with two variables. The nature of MPVs depends on whether
the identiﬁed modal properties are consistent with the theoretical models. For the
(3,3)-subplot in Fig. 8.3, they are roughly consistent at the global MPV at the lower
right.
Identiﬁed Modal Properties
For reference, Table 8.1 gives the numerical data of the identiﬁed modal properties
used. The data is hypothetical. The posterior uncertainties of modal properties have
been taken to be much larger than typical values so that the contours in Fig. 8.3 can
be seen.
■
8.5
Globally Identiﬁable Problems
When the posterior PDF has a unique regular maximum, it may be approximated by
a Gaussian PDF. The location of the maximum has the highest PDF value and is
called the ‘most probable value’ (MPV), also as known as the ‘maximum a pos-
teriori value’. The mean value of the Gaussian PDF is equal to the MPV and its
covariance matrix is related to the curvature of the posterior PDF at the MPV. The
main computational effort for determining the posterior statistics is spent on
locating the MPV (e.g., by numerical optimization) and calculating the curvatures.
For convenience, let
LðZ; hÞ ¼  ln½pðZjhÞpðhÞ
ð8:14Þ
so that the posterior PDF is expressed as
pðhjZÞ / eLðZ;hÞ
ð8:15Þ
When pðhÞ is a uniform distribution one can deﬁne LðZ; hÞ ¼  ln pðZjhÞ. In this
case, LðZ; hÞ is called the ‘negative log-likelihood function’ (NLLF).
Let ^hðZÞ be the (unique) MPV of h, where its dependence on data Z has been
emphasized. It maximizes the posterior PDF, or equivalently minimizes L. That is,
rhLðZ; hÞjh¼^hðZÞ¼ 0
ð8:16Þ
and the Hessian r2
hLðZ; hÞ

h¼^hðZÞ is positive deﬁnite. Here, the subscript h in rh
indicates that the gradient is taken w.r.t. h. Approximating L by a second order
Taylor series about ^h,
274
8
Bayesian Inference

LðZ; hÞ  LðZ; ^hÞ þ rhLðZ; hÞjh¼^h
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
0
ðh  ^hÞ þ 1
2 ðh  ^hÞTr2
hLðZ; hÞ

h¼^hðh  ^hÞ
¼ LðZ; ^hÞ þ 1
2 ðh  ^hÞTr2
hLðZ; hÞ

h¼^hðh  ^hÞ
ð8:17Þ
Substituting into (8.15) and omitting the constant LðZ; ^hÞ gives
pðhjZÞ / exp  1
2 ðh  ^hÞT ^C
1ðh  ^hÞ


ð8:18Þ
where
^CðZÞ ¼ r2
hLðZ; hÞ

h¼^h
h
i1
posterior covariance matrix
ð
Þ
ð8:19Þ
is a positive deﬁnite matrix. Equation (8.18) is just the variable part of a
multi-variate Gaussian PDF with mean ^h and covariance matrix ^C. This is sufﬁcient
to conclude that pðhjZÞ can be approximated by a Gaussian PDF. The propor-
tionality constant is ð2pÞnh=2j^Cj1=2 (nh is the number of parameters in h), so that
pðhjZÞ integrates w.r.t. h to 1. Thus,
pðhjZÞ  ð2pÞnh=2j^Cj1=2 exp  1
2 ðh  ^hÞT ^C
1ðh  ^hÞ


ð8:20Þ
As a property of Gaussian PDF, the marginal PDF of a particular parameter hi
(i ¼ 1; . . .; nh) is also a Gaussian PDF with a mean ^hi (the ith entry of ^h) and
variance equal to the ith diagonal entry of ^C. The covariance between hi and hj is
the ði; jÞ-entry of ^C. These are desirable properties, as they eliminate the need for
integrating the joint PDF to yield the marginal PDFs.
8.5.1
Quality of Gaussian Approximation
The quality of Gaussian approximation to the posterior PDF pðhjZÞ depends on
how close LðZ; hÞ is to a quadratic function of h. This depends on the problem, the
parameter under question and data size. The posterior marginal PDF of one
parameter can be approximately Gaussian while another is not. Practically, the
MPV and posterior variance are of primary concern while the quality of Gaussian
8.5
Globally Identiﬁable Problems
275

approximation is of secondary importance. There are many cases where LðZ; hÞ
systematically tends to a quadratic function of h as the sample size increases and
hence the posterior PDF converges to a Gaussian PDF. The asymptotic behavior of
functions of this type has been studied extensively, e.g., in Laplace type integrals
(Bleistein and Handelsman 1986; Erdelyi 1956). For general problems, asymptotic
Gaussian nature of the posterior PDF should not be taken for granted.
Example 8.3 (Mean parameter, exact posterior) Let f^ykgN
k¼1, abbreviated as f^ykg,
be a set of measured data assumed to be related to an unknown parameter h1 by
^yk ¼ h1 þ ek
ð8:21Þ
where fekgN
k¼1 are prediction errors modeled as i.i.d. Gaussian variables with zero
mean and unknown variance h2. Although we are primarily interested in h1, the
likelihood function pðf^ykgjh1Þ is non-trivial to formulate from ﬁrst principles. We
need to include h2 in h so that the likelihood function pðf^ykgjh1; h2Þ can be
explicitly derived. According to (8.21), for given h ¼ ½h1; h2T, f^ykg are i.i.d.
Gaussian variables with mean h1 and variance h2. Thus
pðf^ykgjhÞ ¼
Y
N
k¼1
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2ph2
p
exp  ð^yk  h1Þ2
2h2
"
#
ð8:22Þ
Assuming a uniform prior PDF for h, pðhjf^ykgÞ / pðf^ykgjhÞ. As a function of
f^ykg this is a joint Gaussian PDF but as a function of h the dependence is less
trivial. Expanding the quadratic term in the exponent, it can be written more
explicitly in terms of ðh1; h2Þ as
pðhjf^ykgÞ / pðf^ykgjhÞ ¼ ð2ph2ÞN=2 exp  N
2h2
ðh1  l1Þ2 þ l2  l2
1
h
i


ð8:23Þ
l1 ¼ 1
N
X
N
k¼1
^yk
l2 ¼ 1
N
X
N
k¼1
^y2
k
ð8:24Þ
It can be further separated into two terms:
pðhjf^ykgÞ / ð2ph2ÞðN1Þ=2 exp  Nðl2  l2
1Þ
2h2


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
function of h2 only

1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2ph2=N
p
exp ðh1  l1Þ2
2h2=N
"
#
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
/ðh1; l1; h2=NÞ
ð8:25Þ
where /ðh1; l1; h2=NÞ denotes a Gaussian PDF with mean l1, variance h2=N and
evaluated at h1. By the deﬁnition of conditional probability,
276
8
Bayesian Inference

pðh1; h2jf^ykgÞ ¼ pðh2jf^ykgÞ  pðh1jh2; f^ykgÞ
ð8:26Þ
Comparing with (8.25), we see that
pðh1jh2; f^ykgÞ ¼ /ðh1; l1; h2
NÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2ph2=N
p
exp ðh1  l1Þ2
2h2=N
"
#
ð8:27Þ
pðh2jf^ykgÞ ¼ C hðN1Þ=2
2
exp  Nðl2  l2
1Þ
2h2


ð8:28Þ
where C is a normalizing constant so that
R 1
0 pðh2jf^ykgÞdh2 ¼ 1. The integral on
the RHS of (8.28) can be evaluated analytically, which implies
C ¼ ½Nðl2  l2
1Þ=2ðN3Þ=2
CððN  3Þ=2Þ
ð8:29Þ
where CðÞ is the Gamma function. The marginal PDF of h1 can be obtained by
integrating out h2 from (8.23). The integration can be performed analytically, giving
(omitting constants)
pðh1jf^ykgÞ ¼
Z1
0
pðhjf^ykgÞdh2 / ðh1  l1Þ2 þ ðl2  l2
1Þ
h
iðN2Þ=2
ð8:30Þ
This can be written in the form (omitting constants)
pðh1jf^ykgÞ /
1 þ 1
m
h1  l1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðl2  l2
1Þ=m
p
 
!2
2
4
3
5
ðm þ 1Þ=2
ð8:31Þ
where m ¼ N  3. This is a (non-standard) Student’s t PDF with m degrees of
freedom with shift parameter l1 and scale parameter r ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðl2  l2
1Þ=m
p
. That is,
t ¼ ðh1  l1Þ=r has a (standard) Student’s t PDF with m degrees of freedom. Using
the standard formula for Student’s t PDF,
pðh1jf^ykgÞ ¼
Cðm þ 1
2 Þ
Cðm
2Þ
ﬃﬃﬃﬃﬃ
pm
p
r 1 þ 1
m
h1  l1
r
	

2
"
#ðm þ 1Þ=2
m ¼ N  3
r ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðl2  l2
1Þ=m
p
ð8:32Þ
In this example the posterior marginal PDFs can be obtained analytically and
they can be related to a standard distribution that has been studied before. For
general problems this is seldom the case. It could be difﬁcult (if possible) to
8.5
Globally Identiﬁable Problems
277

discover the joint posterior PDF as a product of marginal and conditional PDF; or to
obtain an analytical expression for the integrals in the marginal PDFs or normal-
izing constants. The joint, conditional or the marginal PDF need not belong to any
standard distribution. Computational methods are therefore usually needed for
determining the descriptive statistics of the posterior PDF.
■
Example 8.4 (Mean parameter, Gaussian posterior) We revisit Example 8.3 but
here we obtain the posterior MPV and covariance matrix of h based on a Gaussian
approximation of the posterior PDF. Using (8.23),
Lðf^ykg; hÞ ¼  ln pðf^ykgjhÞ ¼ N
2
lnð2ph2Þ þ h1
2
ðh1  l1Þ2 þ l2  l2
1
h
i
n
o
ð8:33Þ
Posterior MPV
Direct differentiation gives
@L
@h1
¼ Nh1
2 ðh1  l1Þ
@L
@h2
¼ N
2
h1
2
 h2
2
ðh1  l1Þ2 þ l2  l2
1
h
i
n
o
ð8:34Þ
Solving @L=@h1 ¼ 0 and @L=@h2 ¼ 0 for ðh1; h2Þ gives the posterior MPVs:
^h1 ¼ l1
^h2 ¼ l2  l2
1
ð8:35Þ
The MPVs in (8.35) are intuitive and coincide with the estimators in classical
statistics.
Posterior Covariance Matrix
Further differentiating (8.34) gives
@2L
@h2
1
¼ Nh1
2
@2L
@h2
2
¼ N
2
h2
2 þ 2h3
2
ðh1  l1Þ2 þ l2  l2
1
h
i
n
o
ð8:36Þ
@2L
@h1@h2
¼
@2L
@h2@h1
¼ Nh2
2 ðh1  l1Þ
ð8:37Þ
Substituting h1 ¼ ^h1 and h2 ¼ ^h2 from (8.35) gives, after simpliﬁcations,
@2^L
@h2
1
¼ Nðl2  l2
1Þ1
@2^L
@h2
2
¼ N
2 ðl2  l2
1Þ2
@2^L
@h1@h2
¼
@2^L
@h2@h1
¼ 0
ð8:38Þ
where the hat ‘^’ denotes that the derivative is evaluated at the MPV. Assembling
these gives the Hessian of L at the MPV:
278
8
Bayesian Inference

^HL ¼
@2^L=@h2
1
@2^L=@h1@h2
@2^L=@h2@h1
@2^L=@h2
2


¼
Nðl2  l2
1Þ1
0
0
Nðl2  l2
1Þ2=2


ð8:39Þ
Taking inverse gives the posterior covariance matrix of h:
^C ¼ ^H
1
L
¼
ðl2  l2
1Þ=N
0
0
2ðl2  l2
1Þ2=N


ð8:40Þ
Interpretation
In summary, given the data f^ykgN
k¼1 and assuming the model in (8.21), the posterior
PDF of ðh1; h2Þ is approximately a Gaussian PDF with mean given by (8.35) and
covariance matrix given by (8.40). The posterior variances of h1 and h2 are
var½h1jf^ykg ¼ ^Cð1; 1Þ ¼ 1
N ðl2  l2
1Þ
var½h2jf^ykg ¼ ^Cð2; 2Þ ¼ 2
N ðl2  l2
1Þ2
ð8:41Þ
The covariance between h1 and h2 is ^Cð1; 2Þ, which is zero. This implies that, given
the data, they are uncorrelated.
As a remark, in general problems the MPV and posterior covariance matrix can
seldom be obtained analytically in terms of data. The Hessian need not be a
diagonal matrix and so its inverse often needs to be evaluated numerically. The
covariance matrix need not be diagonal and the model parameters could be cor-
related according to their posterior PDF.
Quality of Gaussian Approximation
To demonstrate the quality of Gaussian approximation of the posterior PDF of
ðh1; h2Þ, we generated synthetic data according to ^yk ¼ 1 þ ek where N ¼ 30 and
fekgN
k¼1 are i.i.d. zero-mean Gaussian with variance 0.2. A particular data set gave
l1 ¼ 0:9516 and l2 ¼ 1:1453. Figure 8.5 shows the contour of the posterior joint
PDF using (a) the exact expression in (8.26) and (b) the Gaussian approximation.
The values that generated the data (h1 ¼ 1, h2 ¼ 0.2) need not correspond to the
MPV of a particular data set. Figure 8.6 shows the posterior marginal PDFs. The
exact marginal PDFs can be obtained by numerically integrating the joint PDF, or
in this example using (8.28) and (8.32). The Gaussian approximation is good for h1
but not for h2. Although the exact and Gaussian PDF clearly differ, they have a
similar spread.
■
Example 8.5 (linear regression, Gaussian posterior) Consider identifying h ¼
½h1; h2; h3T from data f^ykgN
k¼1, abbreviated as f^ykg, assuming the simple linear
regression model
8.5
Globally Identiﬁable Problems
279

^yk ¼ h1 þ h2xk þ ek
ð8:42Þ
where fxkgN
k¼1 are given; fekgN
k¼1 are prediction errors modeled as i.i.d. Gaussian
variables with zero mean and variance h3. Similar to Example 8.4, we obtain the
posterior MPV and covariance matrix of h based on a Gaussian approximation of
the posterior PDF.
Given h, f^ykg are i.i.d. Gaussian, each with mean h1 þ h2xk and variance h3.
Assuming a uniform prior PDF for h,
pðhjf^ykgÞ / pðf^ykgjhÞ ¼
Y
N
k¼1
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2ph3
p
exp  1
2h3
ð^yk  h1  h2xkÞ2


ð8:43Þ
Lðf^ykg; hÞ ¼  ln pðf^ykgjhÞ ¼ N
2 lnð2ph3Þ þ 1
2h3
X
N
k¼1
ð^yk  h1  h2xkÞ2
ð8:44Þ
1
2
(a) Exact
0.6
0.8
1
1.2
1.4
0
0.1
0.2
0.3
0.4
0.5
0.6
1
2
(b) Gaussian approx.
0.6
0.8
1
1.2
1.4
0
0.1
0.2
0.3
0.4
0.5
0.6
Fig. 8.5 Contour of posterior PDF in Example 8.4. a Exact, b Gaussian approximation. The value
that generates the data is marked with a cross ‘x’
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
0
1
2
3
4
5
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0
2
4
6
8
2
Fig. 8.6 Posterior marginal PDF in Example 8.4. Solid line exact; Dashed line Gaussian
approximation
280
8
Bayesian Inference

To facilitate analysis, we expand the squares in (8.44) and rewrite it as
Lðf^ykg; hÞ ¼ N
2 lnð2ph3Þ þ h1
3 ðlyy þ h2
1 þ lxxh2
2  2lyh1  2lxyh2 þ 2lxh1h2Þ


ð8:45Þ
where
lx ¼ 1
N
X
N
k¼1
xk
ly ¼ 1
N
X
N
k¼1
^yk
lxx ¼ 1
N
X
N
k¼1
x2
k
lyy ¼ 1
N
X
N
k¼1
^y2
k
lxy ¼ 1
N
X
N
k¼1
xk^yk
ð8:46Þ
Posterior MPV
Solving @L=@hi ¼ 0 (i ¼ 1; 2; 3) for ðh1; h2; h3Þ gives the posterior MPV:
^h1 ¼ lxxly  lxlxy
lxx  l2
x
^h2 ¼ lxy  lxly
lxx  l2
x
ð8:47Þ
^h3 ¼
lyyl2
x þ 2lxlylxy  l2
xy  lxxl2
y þ lxxlyy
lxx  l2
x
ð8:48Þ
These MPVs are identical to the best estimates from linear regression theory.
Posterior Covariance Matrix
The Hessian of L w.r.t. h at the MPV can be evaluated to be
^HL ¼ N
^h3
1
lx
0
lx
lxx
0
0
0
1=2^h3
2
4
3
5
ð8:49Þ
Unlike the last example, the Hessian is not diagonal, although the cross derivatives
involving the prediction error variance h3 are zero as before. Taking inverse gives
the posterior covariance matrix:
^C ¼ ^H
1
L
¼
^h3
Nðlxx  l2
xÞ
lxx
lx
0
lx
1
0
0
0
2^h3ðlxx  l2
xÞ
2
4
3
5
ð8:50Þ
Interpretation
Reading off the diagonal entries of ^C gives
8.5
Globally Identiﬁable Problems
281

var½h1jf^ykg ¼
^h3lxx
Nðlxx  l2
xÞ
var½h2jf^ykg ¼
^h3
Nðlxx  l2
xÞ
ð8:51Þ
var½h3jf^ykg ¼ 2^h2
3
N
ð8:52Þ
According to the posterior PDF, h1 and h2 are correlated. Their covariance is
given by ^Cð1; 2Þ:
cov½h1; h2jf^ykg ¼ 
lx^h3
Nðlxx  l2
xÞ
ð8:53Þ
The negative sign implies that h1 and h2 are negatively correlated when lx [ 0 (and
positive correlated when lx\0). Intuitively, when lx [ 0 a line with a bigger slope
(i.e., bigger h2) will need to have a lower y-intercept (i.e., smaller h1) in order to
achieve the same ‘ﬁt’ to data.
Illustration with Synthetic Data
Consider the synthetic data (dots) in Fig. 8.7, which were generated according to
^yk ¼ 1 þ xk þ ek, N ¼ 30 and fekgN
k¼1 being i.i.d. zero-mean Gaussian with variance
0.2. The data set gave
lx ¼ 0:4354
ly ¼ 1:3615
lxx ¼ 0:2801
lyy ¼ 2:0610
lxy ¼ 0:6604
Figure 8.8 shows the posterior marginal PDFs. Similar to the last example, except
for the prediction error variance h3, the posterior marginal PDFs are approximately
Gaussian.
■
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
x
y
Fig. 8.7 Data in Example 8.5. Dots data. Dashed line model prediction at the MPV of parameters
282
8
Bayesian Inference

8.6
Locally Identiﬁable Problems
When the posterior PDF has more than one peak, it may be approximated using a
similar strategy as in the globally identiﬁable case. Let f^higm
i¼1 denote the local
MPVs (m is their number) and ^Hi denote the Hessian of LðZ; hÞ ¼  ln½pðZjhÞpðhÞ
w.r.t. h and evaluated at ^hi. One may approximate the posterior PDF as a mixture
Gaussian PDFs centered on the MPVs:
pðhjZÞ 
X
m
i¼1
wi/ðh; ^hi; ^CiÞ
ð8:54Þ
where fwigm
i¼1 are weights that sum to 1; and
/ðh; ^hi; ^CiÞ ¼ ð2pÞnh=2j^Cij1=2 exp  1
2 ðh  ^hiÞT ^C
1
i ðh  ^hiÞ


ð8:55Þ
is a Gaussian PDF with mean ^hi and covariance matrix ^Ci ¼ ^H
1
i . The approxi-
mation in (8.54) is unlikely to be good when the local MPVs are close because their
interaction has not been accounted for. The weights fwigm
i¼1 may be determined so
that the ratios of the approximated PDF at the local MPVs are the same as those of
the exact posterior PDF:
pð^hijZÞ 
X
m
j¼1
wj/ð^hi; ^hj; ^CjÞ  wi/ð^hi; ^hi; ^CiÞ ¼ wið2pÞnh=2j^Cij1=2
ð8:56Þ
where we have taken /ðhi; ^hj; ^CiÞ  0 for j 6¼ i. Since pð^hijZÞ / pðZj^hiÞpð^hiÞ, this
implies
wi / j^Cij1=2pðZj^hiÞpð^hiÞ
ð8:57Þ
0
0.5
1
1.5
2
0
1
2
3
4
1
-1
-0.5
0
0.5
1
1.5
2
0
0.5
1
1.5
2
2
0
0.1
0.2
0.3
0.4
0.5
0
5
10
3
Fig. 8.8 Posterior marginal PDFs in Example 8.5. Solid line exact; Dashed line Gaussian
approximation
8.6
Locally Identiﬁable Problems
283

Although the above suggests that the strategy for globally identiﬁable problems
can in principle be extended to the case of locally identiﬁable problems, in reality
the computational effort can be signiﬁcantly larger. This is because the number of
local MPVs is unknown and how to ﬁnd them is another problem. Unless a pre-
liminary (often qualitative) analysis is performed to ascertain the number of local
MPVs, practically the sum in (8.54) is only over all local MPVs that have been
found. ‘Global optimization’ algorithms aim at locating multiple minima. See, e.g.,
Spall (2003) and Marti (2010).
8.7
Unidentiﬁable Problems
A system identiﬁcation problem is unidentiﬁable when there is a manifold (con-
tinuum) of points in the model parameter space along which the posterior PDF is
constant. This situation reﬂects that the available information is not sufﬁcient to
resolve even locally the plausibility of model parameters. Over-parameterization is
one common cause. Attempts were made to devise computational algorithms to
generate points on the manifold (Katafygiotis et al. 1998, 2000; Katafygiotis and
Lam 2002). The amount of information required to describe the manifold grows
dramatically with the number of model parameters and so the idea did not ﬂourish.
Monte Carlo methods have been found to provide a promising computational
tool for Bayesian inference problems in general and they are still under intensive
research. The basic idea is to generate random samples according to the posterior
PDF and estimate the posterior statistics by averaging over them. Generally, the
expectation of a quantity of interest hðhÞ (say) when h is distributed as pðhjZÞ can
be estimated by
E½hðhÞjZ  1
M
X
M
k¼1
hðhkÞ
ð8:58Þ
where fhkgM
k¼1 are samples drawn from pðhjZÞ. This idea is simple but its efﬁcient
implementation is never straightforward. Generating samples according to an
arbitrarily given distribution is a highly non-trivial task. One powerful class of
methods is Markov Chain Monte Carlo simulation (MCMC), which has revolu-
tionized the ﬁeld of Bayesian inference. The random samples are generated from a
specially designed Markov chain whose distribution converges to the target
distribution.
MCMC was originally invented by Metropolis et al. (1953) in statistical physics.
A major generalization was due to Hastings (1970). The method has found appli-
cations in a wide spectrum of disciplines involving Bayesian inference, e.g.,
astronautics, biostatistics, image processing, ﬁnancial engineering (Malakoff 1999;
Beichl and Sullivan 2000). An extensive body of research exists and is
well-documented (Fishman 1996; Gilks et al. 1998; Liu 2001; Robert and Casella
284
8
Bayesian Inference

2004). See Au and Wang (2014) for an introduction without pre-requisites in
Markov Chain theory. See also the MCMC Preprint service available on the
internet, http://www.statslab.cam.ac.uk/*mcmc/). The development of efﬁcient
MCMC algorithms in different disciplines is an ever growing area. Developments in
structural dynamics and civil engineering may be referred to, e.g., Beck and Au
(2002), Ching and Chen (2007), Cheung and Beck (2009) and Straub and
Papaioannou (2015). See Green and Worden (2014) for a tutorial in the context of
structural dynamics.
8.8
Model Class Selection
In the discussion so far, modeling assumptions have been omitted in conditioning
statements as they are understood to be the same throughout. This conditioning
matters and should be reinstalled when one compares the identiﬁcation results
between two Bayesian procedures formulated based on different assumptions; or
when one evaluates the plausibility of a model compared to another, i.e., in a
‘model selection’ problem.
Let M denote a given set of modeling assumptions necessary to formulate the
likelihood function pðZjhÞ. It is referred as a ‘model class’ (Beck 2010), i.e., the set
of models induced by all possible values of h. Indicating modeling assumptions
explicitly, the posterior distribution in (8.3) reads
pðhjZ; MÞ ¼ pðZjh; MÞ pðhjMÞ
pðZjMÞ
ð8:59Þ
A different model class M will give a different posterior distribution pðhjZ; MÞ
and so there is no ‘universal’ or ‘inherent’ posterior distribution. Identiﬁcation
results all depend on the modeling assumptions applied (and the data used).
Analysis results are as good as the modeling assumptions. A natural question is
what a good model class is. Based solely on probability reasoning and information
from data (i.e., setting aside convenience, computational effort, etc.), this is
answered by pðMjZÞ, which measures the plausibility of the model class for
representing the system and its associated uncertainty based on the information
implied by data. The higher the pðMjZÞ the better.
8.8.1
Comparing Model Classes with Evidence
Using Bayes’ theorem,
8.7
Unidentiﬁable Problems
285

pðMjZÞ ¼ pðZjMÞ pðMÞ
pðZÞ
ð8:60Þ
The term pðMÞ reﬂects prior information. The term pðZjMÞ incorporates the
information from the data and is often called the ‘evidence’ (or ‘marginal likeli-
hood’) for the model class. It is the normalizing constant in the posterior distri-
bution of h in (8.59). Interestingly, it does not play a material role in the
identiﬁcation of h for a given model class M but it is now the fundamental quantity
that determines how plausible M is.
While pðMÞ can be assigned based on prior information and pðZjMÞ can be
calculated when data is given, pðZÞ is still unknown and subjected to interpretation.
Implicit in (8.60) is the set of candidate model classes such that, by the theorem of
total probability,
pðZÞ ¼
X
M
pðZjMÞpðMÞ
ð8:61Þ
where the sum is over all candidate model class M. Trying to determine pðMjZÞ in
an ‘absolute’ sense requires one to exhaust all possible candidates, which is not
practical (if possible). It is possible, however, to tell in a relative sense whether one
model class M1 is better than another model class M2, because pðZÞ cancels out in
the ratio:
pðM2jZÞ
pðM1jZÞ ¼ pðZjM2ÞpðM1Þ
pðZjM1ÞpðM2Þ
ð8:62Þ
8.8.2
Model Trade-off
The computational problem in model class selection lies in the determination of
pðZjMÞ for a given M. One approach is to express it via the theorem of total
probability:
pðZjMÞ ¼
Z
pðZjh; MÞpðhjMÞdh
ð8:63Þ
The difﬁculty in calculating pðZjMÞ is similar to that in calculating the posterior
statistics of h because the integrand is proportional to the posterior distribution
pðhjZ; MÞ. Computational strategies have been investigated with insights on what
constitute a good model, e.g., Beck and Yuen (2004), Muto and Beck (2008),
Cheung and Beck (2010).
For globally identiﬁable problems, the Gaussian approximation in Sect. 8.5
gives
286
8
Bayesian Inference

pðZjMÞ 
pðZj^h; MÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
Prefer model fit to data

ð2pÞnh=2j^Cj1=2pð^hjMÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Ockham factor that penalizes parameterization
ð8:64Þ
where ^h is the MPV and ^C is the covariance matrix associated with the posterior
distribution pðhjZ; MÞ. The term pðZj^h; MÞ is larger for models that ‘ﬁt’ the data
better and hence it tends to favor those with more parameters. The term
ð2pÞnh=2j^Cj1=2pð^hjMÞ is called the ‘Ockham factor’ (Gull 1988; Jefferys and
Berger 1992). It tends to decrease with the number of parameters nh in h and hence
can be viewed as a penalty against parameterization. For example, when the prior
distribution is Gaussian with mean h0 and covariance matrix C0,
Ockham factor ¼ j^Cj1=2
jC0j1=2 exp  1
2 ð^h  h0ÞC1
0 ð^h  h0Þ


ð8:65Þ
When the data is informative about h, j^Cj=jC0j is smaller than 1 and decreases with
nh.
Equation (8.64) spells out quantitatively the traditional wisdom that a good
model trades off between the amount of parameterization and the agreement with
data. In the general case, the trade-off can be stated as (see the end for proof)
ln pðZjMÞ ¼ E½ln pðZjh; MÞjZ; M
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Prefer model fit to data
 E ln pðhjZ; MÞ
pðhjMÞ
Z; M


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Relative entropy  0
ð8:66Þ
where
E½ln pðZjh; MÞjZ; M ¼
Z
ln pðZjh; MÞ
½
pðhjZ; MÞdh
E ln pðhjZ; MÞ
pðhjMÞ
Z; M


¼
Z
ln pðhjZ; MÞ
pðhjMÞ


pðhjZ; MÞdh
ð8:67Þ
The term Efln½pðhjZ; MÞ=pðhjMÞjZ; Mg is the ‘relative entropy’ of the posterior
distribution to the prior distribution. It reﬂects the amount of information gained
from the data and tends to be higher for models with more parameters. Subtracting
this term therefore reﬂects a penalty against parameterization.
Proof of (8.66) (Model Trade-Off)
Rearranging (8.59) and taking logarithm,
8.8
Model Class Selection
287

ln pðZjMÞ ¼ ln

pðZjh; MÞ pðhjMÞ
pðhjZ; MÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
constant independent of h

¼
Z
ln pðZjh; MÞ pðhjMÞ
pðhjZ; MÞ


pðhjZ; MÞdh
¼
Z
ln pðZjh; MÞ
½
pðhjZ; MÞdh
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
E½ln pðZjh; MÞjZ; M

Z
ln pðhjZ; MÞ
pðhjMÞ


pðhjZ; MÞdh
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
E ln pðhjZ; MÞ
pðhjMÞ
Z; M


ð8:68Þ
■
References
Au SK, Wang Y (2014) Engineering Risk Assessment with subset simulation. Wiley, Chichester
Au SK, Zhang FL (2015) Fundamental two-stage formulation for Bayesian system identiﬁcation,
Part I: general theory. Mech Syst Signal Process 66–67:31–42
Beck JL (2010) Bayesian system identiﬁcation based on probability logic. Struct Control Health
Monit 17(7):825–847
Beck JL, Au SK (2002) Bayesian updating of structural models and reliability using Markov Chain
Monte Carlo simulation. J Eng Mech ASCE 128(4):380–391
Beck JL, Katafygiotis LS (1998) Updating models and their uncertainties. I: Bayesian statistical
framework. J Eng Mech ASCE 124(4):455–461
Beck JL, Yuen KV (2004) Model class selection using response measurements: Bayesian
probabilistic approach. J Eng Mech ASCE 130(2):192–203
Beichl I, Sullivan F (2000) The metropolis algorithm. Comput Sci Eng 2(1):65–69
Bleistein N, Handelsman RA (1986) Asymptotic expansions of integrals. Dover, New York
Box GEP, Tiao GC (1992) Bayesian inference in statistical analysis. Wiley, New York
Cheung SH, Beck JL (2009) Bayesian model updating using hybrid Monte Carlo simulation with
application to structural dynamic models with many uncertain parameters. J Eng Mech ASCE
135(4):243–255
Cheung SH, Beck JL (2010) Calculation of the posterior probability for Bayesian model class
assessment and averaging from posterior samples based on dynamic system data. Comput
Aided Civil Infrastruct Eng 25:304–321
Ching J, Chen YC (2007) Transitional Markov chain Monte Carlo method for Bayesian model
updating, model class selection, and model averaging. J Eng Mech ASCE 133(7):816–832
Cox RT (1961) The algebra of probable inference. Johns Hopkins Press, Baltimore
Erdelyi A (1956) Asymptotic expansions. Dover, New York
Fine TL (2004) The ‘only acceptable approach’ to probabilistic reasoning. SIAM News 37(2): 723
Fishman GS (1996) Monte Carlo: concepts, algorithms and applications. Springer, New York
Garrett AJM (2004) Review: probability theory: the logic of science (by Jaynes ET). Law, Probab
Risk 3:243–246
Gilks WR, Richardson S, Spiegelhalter DJ (1998) Markov Chain Monte Carlo in practice.
Chapman & Hall, London
288
8
Bayesian Inference

Green PL, Worden K (2014) Bayesian and Markov chain Monte Carlo methods for identifying
nonlinear systems in the presence of uncertainty. Philos Trans Roy Soc A 373(2051):20140405
Gull SF (1988) Bayesian inductive inference and maximum entropy. In: Erickson GJ, Smith CR
(eds) Maximum entropy and Bayesian methods. Kluwer Academic Publishers, Dordrecht
Hastings WK (1970) Monte Carlo sampling methods using Markov chains and their applications.
Biometrika 57(1):97–109
Jaynes ET (2003) Probability theory: the logic of science. Cambridge University Press, UK
Jeffreys H (1961) Theory of probability, 3rd edn. Oxford University Press, Oxford
Jefferys WH, Berger JO (1992) Ockham’s Razor and Bayesian Analysis. Am Sci 80:64–72
Katafygiotis LS, Lam HF (2002) Tangential-projection algorithm for manifold representation in
unidentiﬁable model updating problems. Earthquake Eng Struct Dynam 31:791–812
Katafygiotis LS, Lam HF, Papadimitriou C (2000) Treatment of unidentiﬁability in structural
model updating. Adv Struct Eng 3(1):19–39
Katafygiotis LS, Papadimitriou C, Lam HF (1998) A probabilistic approach to structural model
updating. Soil Dyn Earthq Eng 17:495–507
Liu GS (2001) Monte Carlo Strategies in Scientiﬁc Computing. Springer, New York
Malakoff D (1999) Bayes offers a ‘new’ way to make sense of numbers. Science 286:460–1464
Marti K (2010) Stochastic optimization methods. Springer, Berlin
Metropolis N, Rosenbluth AW, Rosenbluth MN et al (1953) Equations of state calculations by fast
computing machines. J Chem Phys 21(6):1087–1092
Muto M, Beck JL (2008) Bayesian updating and model class selection for hysteretic structural
models using stochastic simulation. J Vib Control 14:7–34
Robert C, Casella G (2004) Monte Carlo statistical methods. Springer, New York
Robert CP, Chopin N, Rousseau J (2009) Harold Jeffreys’s theory of probability revisited. Stat Sci
24(2):141–172
Spall JC (2003) Introduction to stochastic search and optimization: estimation, simulation, and
control. Wiley, New York
Straub D, Papaioannou I (2015) Bayesian updating with structural reliability methods. J Eng Mech
141(3):04014134. doi:10.1061/(ASCE)EM.1943-7889.0000839
References
289

Chapter 9
Classical Statistical Inference
Abstract This chapter introduces the classical statistical approach for system
identiﬁcation in a general context. This is commonly referred as a ‘non-Bayesian’
approach and is currently the conventional perspective in operational modal anal-
ysis. Basic quantiﬁcation of statistical estimators are presented and illustrated with
examples. The Cramér-Rao bound and Fisher information matrix are presented to
provide the theoretical lower bound for the variance of unbiased estimators.
Maximum likelihood estimators and their asymptotic properties for large data size
are discussed. The Bayesian and classical statistical approach have different
philosophies but share some similarities in mathematics. These shall be discussed
so that the two approaches can be applied correctly and advantage can be taken of
their mathematical tools developed.
Keywords Cramér-Rao bound  Fisher information matrix  Maximum likeli-
hood  Uncertainty law
In Chap. 8 we discussed the Bayesian approach for making inference about the
parameters of interest based on available data. The ‘classical statistical approach’ is
a large class of methods adopting a different perspective to the inference problem.
In this approach, an unknown parameter is estimated by an ‘estimator’ constructed
as a function of the available data. The quality of the estimator is assessed through
its statistical properties associated with repeated experiments. This is referred as a
‘non-Bayesian’ or ‘frequentist’ inference approach, as it views probability as a
relative frequency of occurrence of random events in the long run, in contrast to the
Bayesian interpretation as a measure of plausibility for given information.
Within
the
classical
statistical
approach,
the
‘maximum
likelihood’
(ML) estimator is constructed as the model parameter value that maximizes the
likelihood function. The latter refers to the probability distribution of data for a
given model class and value of model parameter vector, the same deﬁnition as in
the Bayesian approach. Perhaps due to the use of the likelihood function, the ML
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_9
291

method is sometimes perceived to be equivalent to a Bayesian method with a
uniform prior distribution. This is a misconception. Their mathematical structures
share some similarities for globally identiﬁable problems but their philosophies are
fundamentally different, which affects the way identiﬁcation uncertainty is mea-
sured and interpreted.
In this chapter we discuss the classical statistical approach. We clarify similar-
ities and differences with the Bayesian approach. Discovering similarities allows
mathematical tools to be shared for solving computational problems. Recognizing
differences allows proper interpretation of the identiﬁcation results.
As in Chap. 8, without much loss of generality, the unknown parameter h and
data Z are assumed to be continuous-valued. Comparing this chapter with Chap. 8,
one will ﬁnd that the mathematical roles of h and Z are in some sense swapped. In
Chap. 8, h was a parameter value in the posterior distribution but in this chapter it is
the ‘unknown true value’ that is always ﬁxed. Data Z was ﬁxed in all conditioning
statements as the actual data being used but now it is treated as a random variable
whose variability is averaged to obtain ensemble statistics of the estimator.
Identiﬁcation uncertainty is now associated with the variability of the estimator for
h, rather than h itself. All derived results on the statistical properties of estimator do
not depend on data Z but rather on the unknown true value h. In the Bayesian
approach, all inference results depend on Z; and the unknown true parameter value
(or such concept) is irrelevant. See Table 9.2 in Sect. 9.6.1 for a summary of
comparisons between the two approaches.
A series of examples are presented to illustrate the concepts in Chaps. 8 and 9,
centering around three inference problems. They are listed in Table 9.1 for easy
reference.
Table 9.1 Examples in Chaps. 8 and 9 centering around three inference problems
Problem
Examples in Chap. 8
Examples in Chap. 9
Mean parameter
8.3 (Exact posterior)
8.4 (Gaussian posterior)
9.1 (Estimator)
9.4 (CRB)
9.7 (Easy CRB)
9.10 (FIM)
9.12 (Asymptotics and FIM)
Linear regression
8.5 (Gaussian posterior)
9.2 (Estimator)
9.5 (CRB)
9.8 (Easy CRB)
9.11 (FIM)
Modal identiﬁcation
9.3 (MLE)
9.6 (CRB)
9.9 (Easy CRB)
CRB Cramér-Rao bound; FIM Fisher information matrix; MLE maximum likelihood estimation
292
9
Classical Statistical Inference

9.1
Statistical Estimators
In classical statistical estimation, it is assumed that there is a ‘true’ value h ðnh  1Þ
of the set of unknown model parameters that results in the measured data Z. An
estimator GðZÞ ðnh  1Þ is designed to estimate h. Different values of Z lead to
different values of GðZÞ. Identiﬁcation uncertainty is measured as the variability of
GðZÞ over different samples of Z. When it is desired to assess the theoretical
statistical properties of GðZÞ, it is assumed that Z is distributed as the likelihood
function pðZjhÞ.
9.1.1
Quality Statistics
How GðZÞ depends on Z is open to choice and so the estimator is not unique. One
estimator can be better than another estimator in one sense but worse in another
sense. Two common statistics for assessing the quality of an estimator are the mean
E½GðZÞjh and covariance matrix cov½GðZÞjh:
E½GðZÞjh ¼
Z
GðzÞpðzjhÞdz
ð9:1Þ
cov½GðZÞjh ¼ Ef½GðZÞ  gðhÞ½GðZÞ  gðhÞTjhg
¼
Z
½GðzÞ  gðhÞ½GðzÞ  gðhÞTpðzjhÞdz
ð9:2Þ
where
gðhÞ ¼ E½GðZÞjh
ð9:3Þ
In these equations, the expectations are all conditional on the unknown true value
h because the distribution of Z depends on h. This dependence is usually omitted but
is deliberately indicated throughout this chapter. Omitting it does not cause any
mathematical problem when one is conﬁned to the classical statistical approach,
because then h is always considered ﬁxed. It matters in this chapter because h may be
viewed from a Bayesian perspective as a random vector whose probability distri-
bution is a function of the conditioning information. According to the theorem of
total probability, the expectation E½GðZÞ without the conditioning refers to
E½GðZÞ ¼
Z
E½GðZÞjhpðhÞdh ¼
Z Z
GðzÞpðzjhÞpðhÞdzdh
ð9:4Þ
This is the expectation of GðZÞ under the prior distribution pðhÞ. It tells us what the
estimator would be like in the absence of data (as in a ‘forward’ prediction
9.1
Statistical Estimators
293

problem), instead of identifying what h is in the presence of data (as in a ‘back-
ward’, or ‘inverse’, inference problem).
9.1.2
Bias and Convergence
In the classical statistical approach, an estimator GðZÞ is ‘unbiased’ if its expec-
tation is equal to the true value h. It is ‘convergent’ if its variance tends to zero as
data size increases. Intuitively, if GðZÞ is unbiased, the scatter of its values for
different samples of Z should be centered around h. If it is convergent, the extent of
scatter will diminish as data size increases. Unbiasedness is a nice intuitive property
but it is not a prerequisite. Many useful estimators are biased for ﬁnite sample size
but asymptotically unbiased as sample size increases, which is sufﬁcient for prac-
tical purposes. It is important for an estimator to be convergent, however.
9.1.3
Empirical Statistics
The difﬁculty of assessing the statistical properties of an estimator depends on its
relationship with data. For complex problems, it may not be possible to obtain
analytical expressions, in which case one may resort to numerical means.
Empirically, the statistics can be estimated by averaging over multiple data sets,
say, fZkgM
k¼1:
E½GðZÞjh  ~G ¼ 1
M
X
M
k¼1
GðZkÞ
ð9:5Þ
cov½Gjh  1
M
X
M
k¼1
½GðZkÞ  eG½GðZkÞ  eGT
ð9:6Þ
Calculating these statistics requires multiple data sets, even though it is sufﬁcient to
have a single data set to estimate h. If fZkgM
k¼1 are modeled as i.i.d. random
variables then by the Strong Law of Large Numbers the sample average will
converge to its expectation as M increases. This is only a theoretical statement
applicable for synthetic data. In reality, it need not hold because the data may not
follow the assumed probability distribution, or there is no such distribution at all.
Nature does not care or know about distributions. This is one fundamental differ-
ence from the Bayesian approach: probability distribution is a property (although
unknown) of nature’s random events from the frequentist perspective; but it is a
modeling choice describing the analyst’s uncertainty about variables from the
Bayesian perspective.
294
9
Classical Statistical Inference

9.2
Maximum Likelihood Estimator
In the maximum likelihood (ML) method, the estimator is deﬁned as the value of h
that maximizes the likelihood function pðZjhÞ for given data Z. Here, there is an
abuse of notation that h is a (generic) parameter value of a function rather than the
true value that results in the data Z. For a globally identiﬁable problem, the ML
estimator is mathematically the same as the posterior most probable value (MPV) in
the Bayesian approach assuming a uniform prior distribution. Let ^hðZÞ denote the
ML
estimator,
whose
dependence
on
Z
has
been
emphasized.
Then
rhpðZjhÞjh¼^hðZÞ¼ 0 (gradient) and r2
hpðZjhÞ

h¼^hðZÞ (Hessian) is a negative deﬁnite
matrix. Equivalently, ^hðZÞ minimizes the ‘negative log-likelihood function’
(NLLF) LðZ; hÞ ¼  ln pðZjhÞ w.r.t. h for a given Z, i.e., rhLðZ; hÞjh¼^hðZÞ¼ 0 and
r2
hLðZ; hÞ

h¼^hðZÞ is a positive deﬁnite matrix. Working with the NLLF is often
algebraically easier than the likelihood function.
The ML estimator has good theoretical asymptotic properties when the data is
assumed to be distributed as the likelihood function. It is convergent and asymp-
totically unbiased as data size increases. Its variance asymptotically achieves the
smallest possible value of any unbiased estimator. These are discussed in Sect. 9.5
later.
The reader may notice that the same symbol ^h is used for the ML estimator and
the posterior MPV (Bayesian) in Chap. 8. This is deliberate because they are
mathematically the same for globally identiﬁable problems with a uniform prior
distribution. Nevertheless, conceptual problems arise with the ML estimator when
the problem is not globally identiﬁable. For example, if the NLLF has two local
minima, one must resort to take only one of them as the ML estimate, because it is
logically impossible to have a parameter assuming two distinct (true) values.
Example 9.1 (Mean parameter, estimator) Consider estimating h1 from scalar
data f^ykgN
k¼1 modeled by
^yk ¼ h1 þ ek
ð9:7Þ
where fekgN
k¼1 are prediction errors. This problem was investigated in Example 8.3
using the Bayesian approach. Here we adopt a classical statistical approach.
Estimator for h1
Based on ‘intuition’, an estimator G1 for h1 is proposed to be the average of the
data:
G1 ¼ 1
N
X
N
k¼1
^yk
ð9:8Þ
9.2
Maximum Likelihood Estimator
295

To analyze the statistical properties of G1, we need to impose some assumptions
on f^ykgN
k¼1, or equivalently on fekgN
k¼1. As in Example 8.3, assume that fekgN
k¼1 are
i.i.d. zero-mean Gaussian variables with variance h2. Equation (9.7) then implies
that, given h ¼ ½h1; h2T, f^ykgN
k¼1 are i.i.d. Gaussian variables with mean h1 and
variance h2. This gives
E½G1jh ¼ 1
N
X
N
k¼1
E½^ykjh
|ﬄﬄﬄ{zﬄﬄﬄ}
h1
¼ h1
ð9:9Þ
var½G1jh ¼ 1
N2
X
N
k¼1
var½^ykjh
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
h2
¼ h2
N
ð9:10Þ
Thus, G1 is unbiased ðE½G1jh ¼ h1Þ and convergent (var½G1jh ! 0 as N ! 1).
Note that these expressions depend on h1 and h2, which are unknown.
Estimator for h2
By intuition, an estimator G2 for h2 is constructed as the average of the squared
difference between ^yk and G1:
G2 ¼ 1
N
X
N
k¼1
ð^yk  G1Þ2 ¼ ð1
N
X
N
k¼1
^y2
kÞ  G2
1
ð9:11Þ
after algebra. Taking expectation and using the identities
E½^y2
kjh ¼ E½^ykjh2
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
h2
1
þ var½^ykjh
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
h2
E½G2
1jh ¼ E½G1jh2
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
h2
1
þ var½G1jh
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
h2=N
ð9:12Þ
we have
E½G2jh ¼ f1
N
X
N
k¼1
E½^y2
kjh
|ﬄﬄﬄ{zﬄﬄﬄ}
h2
1 þ h2
g  E½G2
1jh
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
h2
1 þ h2=N
¼
1  1
N


h2
ð9:13Þ
Thus, G2 is biased ðE½G2jh 6¼ h2Þ for ﬁnite sample size N but asymptotically unbiased
ðE½G2jh ! h2Þ as N ! 1. Deriving the variance of G2 directly from (9.11) is more
involved. As a short cut, one can use the fact that NG2=h2 ¼ PN
k¼1 ð^yk  G1Þ2=h2
has a Chi-square distribution with ðN  1Þ degrees of freedom and so its variance is
equal to 2ðN  1Þ. Taking variance on both sides and rearranging gives
var½G2jh ¼ 2h2
2
N
1  1
N


ð9:14Þ
Since var½G2jh ! 0 as N ! 1, G2 is convergent.
296
9
Classical Statistical Inference

Modiﬁed Estimator
An unbiased estimator for h2, denoted by G0
2, can be constructed by correcting for
the factor ð1  1=NÞ in the expression of E½G2jh in (9.13) that causes the bias, i.e.,
G0
2 ¼ ð1  1
NÞ1G2 ¼
1
N  1
X
N
k¼1
ð^yk  G1Þ2
ð9:15Þ
so that E½G0
2jh ¼ ð1  1=NÞ1E½G2jh ¼ h2. This is the conventional estimator
used in statistics. The variance of G0
2 is
var½G0
2jh ¼ ð1  1
NÞ2var½G2jh ¼ 2h2
2
N
1  1
N

1
ð9:16Þ
Note that var½G0
2jh [ var½G2jh. An estimator should not be preferred simply
because it has a smaller variance. Its bias needs to be taken into account.
The mathematical steps and considerations in this example are typical in clas-
sical statistics. Some observations are in order. An estimator can be intuitive but
biased ðG2Þ. The expressions of G1 and G2 coincide with those of the posterior
MPV in Example 8.4 and so they are ML estimators. This demonstrates that an ML
estimator can be unbiased ðG1Þ or biased ðG2Þ. The derived statistical properties of
an estimator, e.g., (9.9) and (9.10), are in terms of the unknown true values h1 and
h2.
■
Example 9.2 (Linear regression, estimator) Consider estimating h1 and h2 from
scalar data f^ykgN
k¼1, assuming the linear regression model
^yk ¼ h1 þ h2xk þ ek
ð9:17Þ
where fxkgN
k¼1 are given; fekgN
k¼1 are prediction errors. This problem was investi-
gated in Example 8.5 using the Bayesian approach. Here we adopt a ‘least squares’
approach. An estimator ðG1; G2Þ for ðh1; h2Þ is constructed as the one that mini-
mizes the following objective function deﬁned as the sum of squared errors:
Jðh1; h2Þ ¼
X
N
k¼1
e2
k ¼
X
N
k¼1
ð^yk  h1  h2xkÞ2
ð9:18Þ
Expanding the square gives a quadratic function of ðh1; h2Þ. Minimizing w.r.t.
ðh1; h2Þ gives
G1 ¼ lxxly  lxlxy
lxx  l2
x
G2 ¼ lxy  lxly
lxx  l2
x
ð9:19Þ
9.2
Maximum Likelihood Estimator
297

where
lx ¼ 1
N
P
N
k¼1
xk
ly ¼ 1
N
P
N
k¼1
^yk
lxx ¼ 1
N
P
N
k¼1
x2
k
lyy ¼ 1
N
P
N
k¼1
^y2
k
lxy ¼ 1
N
P
N
k¼1
xk^yk
ð9:20Þ
The least squares approach is intuitive but heuristic (e.g., why squares?). It does
not even involve probability concepts. The expressions of the estimators G1 and G2
coincide with those of the posterior MPVs in Example 8.5 that maximize the
likelihood function. They are thus ML estimators.
Statistical Properties
To analyze the statistical properties of G1 and G2, it is necessary to impose prob-
abilistic assumptions on the data. Consistent with Example 8.5, assume that fekgN
k¼1
are i.i.d. zero-mean Gaussian variables with variance h3 (unknown). The derived
statistical properties will then depend on the unknown true value h ¼ ½h1; h2; h3T.
Equation (9.17) implies that, given h, f^ykgN
k¼1 are i.i.d. Gaussian variables with
mean h1 þ h2xk and variance h3. Correspondingly, ly, lyy and lxy are random
variables whose distributions depend on h. This gives, using (9.19),
E½G1jh ¼ lxxE½lyjh  lxE½lxyjh
lxx  l2
x
¼ lxxðh1 þ h2lxÞ  lxðh1lx þ h2lxxÞ
lxx  l2
x
¼ h1
ð9:21Þ
E½G2jh ¼ E½lxyjh  lxE½lyjh
lxx  l2
x
¼ ðh1lx þ h2lxxÞ  lxðh1 þ h2lxÞ
lxx  l2
x
¼ h2
ð9:22Þ
where we have used
E½lyjh ¼ h1 þ h2lx
E½lxyjh ¼ h1lx þ h2lxx
ð9:23Þ
Equations (9.21) and (9.22) show that G1 and G2 are unbiased. Their covariance
matrix is more involved to derive but can be shown to be
var½G1jh
cov½G1; G2jh
cov½G2; G1jh
var½G2jh


¼
h3
Nðlxx  l2
xÞ
lxx
lx
lx
1


ð9:24Þ
It depends on h3, which is unknown.
Prediction Error Variance
In the present example, there is no need to estimate h3 if one is only interested in h1
or h2. When it is desired to estimate h3, e.g., so that the covariance matrix in (9.24)
can be estimated, one will need to construct an estimator for it. One ‘intuitive’
estimator, denoted by G3, is the minimized average of e2
k, i.e., JðG1; G2Þ=N:
298
9
Classical Statistical Inference

G3 ¼ JðG1; G2Þ
N
¼ 1
N
X
N
k¼1
ð^yk  G1  G2xkÞ2
ð9:25Þ
Expanding the square and collecting terms,
G3 ¼ lyy þ G2
1 þ G2
2lxx  2G1ly  2G2lxy þ 2G1G2lx
ð9:26Þ
Substituting G1 and G2 from (9.19) gives, after simplifying,
G3 ¼
lyyl2
x þ 2lxlylxy  l2
xy  lxxl2
y þ lxxlyy
lxx  l2
x
ð9:27Þ
This expression coincides with the posterior MPV of h3 in Example 8.5. Thus, G3 is
the ML estimator for h3. It can be shown that
E½G3jh ¼
1  2
N


h3
ð9:28Þ
and so G3 is biased for ﬁnite N. An unbiased estimator for h3, denoted by G0
3, can
be constructed by correcting for the factor ð1  2=NÞ:
G0
3 ¼
1  2
N

1
G3 ¼
1
N  2
X
N
k¼1
ð^yk  G1  G2xkÞ2
ð9:29Þ
This is the conventional estimator used in linear regression theory.
■
Example 9.3 (Modal identiﬁcation, ML estimator) Consider identifying the
natural frequency and damping ratio of a vibration mode using a single channel of
ambient acceleration time history f^yjgN1
j¼0 measured on a structure. Here we con-
struct an ML estimator using the sample power spectral density (PSD) of data
within a selected resonance band of the mode. The sample PSD at the Fast Fourier
Transform (FFT) frequency fk ¼ k=NDt
(Hz) is ^Sk ¼ j ^F kj2, where
^F k ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Dt=N
p
PN1
j¼0 ^yje2pijk=N is the scaled FFT of data; Dt (s) is the sampling interval.
The likelihood function pðf^SkgjhÞ is the joint PDF of f^Skg for a given h (to be
deﬁned shortly). Within the selected band, the theoretical PSD is modeled as
(Sect. 7.1)
SkðhÞ ¼ SDkðf ; fÞ þ Se
ð9:30Þ
where
Dkðf ; fÞ ¼ ½ð1  b2
kÞ2 þ ð2fbkÞ21
bk ¼ f =fk
ð9:31Þ
9.2
Maximum Likelihood Estimator
299

is the dynamic ampliﬁcation factor, f (Hz) is the natural frequency, f is the damping
ratio, S is the modal force PSD and Se is the prediction error PSD. Here,
h ¼ ½f ; f; S; SeT. Although the primary interest is to estimate f and f only, S and Se
need to be included in h so that the likelihood function can be obtained explicitly.
To obtain the likelihood function, we assume long data and make use of the
asymptotic distribution of the sample PSD. According to Sect. 4.10.1, for a given h,
f^Skg are asymptotically independent and each ^Sk is exponentially distributed with
mean SkðhÞ. The likelihood function is then given by
pðf^SkgjhÞ ¼
Y
k
SkðhÞ1e^Sk=SkðhÞ
ð9:32Þ
where the product is taken over all frequencies within the selected band. The ML
estimator is the value of h that minimizes the NLLF:
Lðf^Skg; hÞ ¼  ln pðf^SkgjhÞ ¼
X
k
ln SkðhÞ þ
^Sk
SkðhÞ
"
#
ð9:33Þ
This is not a quadratic function of h and so it does not give a least squares estimate.
Its form is not suggested heuristically, but is rather derived from modeling
assumptions. As this function depends in a nontrivial manner on h, it is difﬁcult to
obtain an analytical expression for the ML estimator. It is then difﬁcult to inves-
tigate analytically whether the estimator is biased or what its variance is. This
situation is typical in applications. One strategy is to investigate the bias and
variance numerically using synthetic data before the estimator can be used with
conﬁdence. The Cramér-Rao bound in the next section provides useful information
on the variance of general estimators.
■
9.3
Cramér-Rao Bound
When the estimator depends on data in a complicated manner, it is difﬁcult to derive
analytical expressions for its statistical properties. The ‘Cramér-Rao bound’
(CRB) (Rao 1945; Cramér 1946) gives the smallest possible variance that can be
achieved by any unbiased estimator, assuming that the data is distributed as the
likelihood function. Under mild assumptions on the likelihood function, it can be
shown that for any unbiased estimator GðZÞ for h,
cov½GðZÞjh  JðhÞ1
ð9:34Þ
300
9
Classical Statistical Inference

where cov½GðZÞjh ðnh  nhÞ is the covariance matrix of GðZÞ, as deﬁned in (9.2);
JðhÞ
nhnh
¼ Ef½rhLðZ; hÞT
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
nh1
½rhLðZ; hÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
1nh
jhg
¼
Z
½rhLðz; hÞT½rhLðz; hÞpðzjhÞdz
ð9:35Þ
is the ‘Fisher information matrix’ (FIM) for h; LðZ; hÞ ¼  ln pðZjhÞ is the NLLF;
and rhLðZ; hÞ is the gradient of LðZ; hÞ w.r.t. h. Note that the ði; jÞ-entry of JðhÞ is
E½ð@L=@hiÞð@L=@hjÞjh. The equality in (9.34) holds if and only if
GðZÞ
nh1
¼
h
nh1  JðhÞ1
|ﬄﬄﬄ{zﬄﬄﬄ}
nhnh
rT
hLðZ; hÞ
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
nh1
ð9:36Þ
Matrix Inequality and Sylvester’s Criterion
Equation (9.34) is a ‘matrix inequality’ which should be interpreted as the LHS
minus RHS being a positive semi-deﬁnite matrix. That is, all eigenvalues of the
following matrix are non-negative:
DðhÞ ¼ cov½GðZÞjh  JðhÞ1
ð9:37Þ
The inequalities from the CRB are seldom obtained from the eigenvalues of
DðhÞ as the latter are difﬁcult to determine analytically. An alternative way to use
(9.34) is via the “Sylvester’s criterion” (Gilbert 1991; Meyer 2000), which says that
a Hermitian matrix is positive semi-deﬁnite if and only if all its principal minors are
non-negative. That is, let Dij denote the ði; jÞ-entry of DðhÞ; and Dp denote the upper
left p  p partition:
Dp ¼
D11
  
D1p
...
..
.
...
Dp1
  
Dpp
2
64
3
75
ð9:38Þ
Then (9.34) is equivalent to
jDpj  0
for all
p ¼ 1; . . .; nh
ð9:39Þ
This provides an equivalent but easier way to derive the inequalities implied from
the CRB when the determinants are easier to obtain than eigenvalues. Even easier
bounds can be obtained as necessary conditions deduced from the CRB, at the
expense of loosening them. This is discussed in Sect. 9.3.1 later.
Generalization
The CRB presented above is for GðZÞ being an unbiased estimator for h. It holds in
a more general context where GðZÞ can be any function of Z. This is discussed in
9.3
Cramér-Rao Bound
301

Sect. 9.3.2 and proven in Sect. 9.3.3. For the CRB to hold, the NLLF should be
twice differentiable w.r.t. h; and the order of expectation over Z and differentiation
w.r.t. h can be swapped. These conditions are often satisﬁed.
The CRB is only a theoretical bound, assuming that the data Z is distributed as
the likelihood function. It need not agree with the variability of the estimator
empirically estimated based on real data as in (9.6). This is simply because, besides
estimation error, the data need not be distributed as the likelihood function
assumed. Nevertheless, the CRB is very useful for investigating the achievable
precision of estimators in classical statistics. It is also related to the leading order of
the posterior covariance matrix in a Bayesian approach for asymptotically large
amount of data; see Sect. 9.6.3 later.
Fisher Information Matrix via Hessian
The FIM can be written as the expectation of the Hessian of NLLF:
JðhÞ ¼ E½r2
hLðZ; hÞjh ¼
Z
½r2
hLðz; hÞpðzjhÞdz
ð9:40Þ
This often provides an easier way for obtaining an analytical expression for JðhÞ.
The proof can be found in Sect. 9.3.3.
Example 9.4 (Mean parameter, CRB) In this example, we determine the CRB for
the problem in Example 9.1. Recall that the data is modeled as ^yk ¼ h1 þ ek, where
fekgN
k¼1 are i.i.d. zero-mean Gaussian variables with variance h2. The likelihood
function and NLLF are given by
pðf^ykgjhÞ ¼
Y
N
k¼1
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2ph2
p
exp  ð^yk  h1Þ2
2h2
"
#
ð9:41Þ
Lðf^ykg; hÞ ¼  ln pðf^ykgjhÞ ¼ N
2 lnð2ph2Þ þ 1
2h2
X
N
k¼1
ð^yk  h1Þ2
ð9:42Þ
Fisher Information Matrix
Direct differentiation of (9.42) gives
rhL ¼
Nh1
2 ðh1  l1Þ
N
2 h1
2  1
2 h2
2
X
N
k¼1
ð^yk  h1Þ2
"
#
ð9:43Þ
r2
hL ¼
Nh1
2
Nh2
2 ðh1  l1Þ
Nh2
2 ðh1  l1Þ
 N
2 h2
2 þ h3
2
P
N
k¼1
ð^yk  h1Þ2
2
4
3
5
ð9:44Þ
302
9
Classical Statistical Inference

where
l1 ¼ PN
k¼1 ^yk=N.
Using
(9.40)
and
noting
that
E½l1jh ¼ h1
and
E½ð^yk  h1Þ2jh ¼ h2, the FIM is given by
JðhÞ ¼ E½r2
hLjh ¼
Nh1
2
0
0
Nh2
2 =2


ð9:45Þ
CRB for unbiased estimator
According to (9.34), for any unbiased estimator G ¼ ½G1; G2T for h ¼ ½h1; h2T,
cov½Gjh ¼
var½G1jh
cov½G1; G2jh
sym:
var½G2jh


 JðhÞ1 ¼
h2=N
0
0
2h2
2=N


ð9:46Þ
in the sense that the LHS minus RHS is a positive semi-deﬁnite matrix. That is, the
eigenvalues of the following matrix are all non-negative:
DðhÞ ¼ cov½Gjh  JðhÞ1 ¼
var½G1jh  h2=N
cov½G1; G2jh
sym:
var½G2jh  2h2
2=N


ð9:47Þ
Let Dij denote the ði; jÞ-entry of DðhÞ. Then the eigenvalues of DðhÞ are given by
1
2 ðD11 þ D22Þ 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðD11  D22Þ2 þ 4D2
12
q


ð9:48Þ
Requiring the eigenvalues to be non-negative results in the following set of
inequalities (details omitted):
D11  0
D22  0
D2
12 	 D11D22
ð9:49Þ
That is,
var½G1jh  h2=N
var½G2jh  2h2
2=N
cov½G1; G2jh2 	 var½G1jh  h2=N
f
g var½G2jh  2h2
2=N
	

ð9:50Þ
Alternatively, applying the Sylvester’s criterion gives the ﬁrst and third equality in
(9.49), from which the second inequality can be deduced.
Applying CRB to Speciﬁc Context
The bounds in (9.50) apply to any unbiased estimators G1 and G2 for h1 and h2,
respectively. We now demonstrate them speciﬁcally for the estimators in Example
9.1. The estimator in (9.8) for h1 is unbiased and so the CRB applies. From (9.10),
its variance is h2=N and so it achieves the CRB, i.e., the minimum possible vari-
ance. For h2, the CRB applies to the estimator in (9.15) but not to the one in (9.11),
because the latter is biased. From (9.16), the variance of the former is
ð1  1=NÞ12h2
2=N. For ﬁnite N, this is greater than the CRB 2h2
2=N, although it
9.3
Cramér-Rao Bound
303

reduces to the CRB as N ! 1. The third inequality in (9.50) puts an upper bound
on the covariance between any unbiased estimators G1 and G2, depending on how
far their variance is from the CRB. For the estimator in (9.8), var½G1jh ¼ h2=N and
so cov½G1; G2jh ¼ 0 for any unbiased G2, including the one in (9.15).
Fisher Information Matrix via Gradient
It is typically more tedious to evaluate the FIM using the original deﬁnition in
(9.35) via the gradient. Let ðrT
hLrhLÞij denote the ði; jÞ-entry of rT
hLrhL. Using
(9.43),
ðrT
hLrhLÞ11 ¼ N2h2
2 ðh1  l1Þ2
ð9:51Þ
ðrT
hLrhLÞ12 ¼ ðrT
hLrhLÞ21 ¼ N2
2 h2
2 ðh1  l1Þ  N
2 h3
2 ðh1  l1Þ
X
N
k¼1
ð^yk  h1Þ2
ð9:52Þ
ðrT
hLrhLÞ22 ¼ N2
4 h2
2
 N
2 h3
2
X
N
k¼1
ð^yk  h1Þ2 þ 1
4 h4
2
X
N
k¼1
ð^yk  h1Þ2
"
#2
ð9:53Þ
Taking expectation on (9.51) gives
E½ðrT
hLrhLÞ11jh ¼ N2h2
2 E½ðh1  l1Þ2jh ¼ N2h2
2 ðh2=NÞ ¼ Nh1
2
ð9:54Þ
which is equal to E½ðr2
hLÞ11jh, as expected. The expectation of ðrT
hLrhLÞ12 and
ðrT
hLrhLÞ22 are more tedious to evaluate as they involve expectations of up to the
fourth order of ^yk. Using (9.40) bypasses these difﬁculties.
■
Example 9.5 (Linear regression, CRB) Consider Example 9.2 again, where the
data is modeled by
^yk ¼ h1 þ h2xk þ ek
k ¼ 1; . . .; N
ð9:55Þ
with fxkgN
k¼1 given and fekgN
k¼1 i.i.d. zero-mean Gaussian with variance h3. Here
we derive the CRB for unbiased estimator of h ¼ ½h1; h2; h3T. Given h, f^ykgN
k¼1 are
i.i.d. Gaussian with mean h1 þ h2xk and variance h3. The likelihood function and
NLLF are given by
pðf^ykgjhÞ ¼
Y
N
k¼1
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2ph3
p
exp  1
2h3
ð^yk  h1  h2xkÞ2


ð9:56Þ
Lðf^ykg; hÞ ¼  ln pðf^ykgjhÞ ¼ N
2 lnð2ph3Þ þ 1
2h3
X
N
k¼1
ð^yk  h1  h2xkÞ2
ð9:57Þ
304
9
Classical Statistical Inference

Direct differentiation gives
@L
@h1
¼ h1
3
X
N
k¼1
ð^yk  h1  h2xkÞ
@L
@h2
¼ h1
3
X
N
k¼1
ð^yk  h1  h2xkÞxk
@L
@h3
¼ N
2 h1
3
 1
2 h2
3
X
N
k¼1
ð^yk  h1  h2xkÞ2
ð9:58Þ
@2L
@h2
1
¼ Nh1
3
@2L
@h2
2
¼ Nlxxh1
3
@2L
@h2
3
¼  N
2 h2
3 þ h3
3
X
N
k¼1
ð^yk  h1  h2xkÞ2
ð9:59Þ
@2L
@h1@h2
¼ Nlxh1
3
@2L
@h1@h3
¼ h2
3
X
N
k¼1
ð^yk  h1  h2xkÞ
@2L
@h2@h3
¼ h2
3
X
N
k¼1
ð^yk  h1  h2xkÞxk
ð9:60Þ
where lx ¼ N1 PN
k¼1 xk and lxx ¼ N1 PN
k¼1 x2
k. The FIM is then given by
JðhÞ ¼ E½r2
hLjh ¼
Nh1
3
Nlxh1
3
0
Nlxxh1
3
0
sym:
Nh2
3 =2
2
4
3
5
ð9:61Þ
where we have used
E½^yk  h1  h2xkjh ¼ E½ekjh ¼ 0
E½ð^yk  h1  h2xkÞ2jh ¼ E½e2
kjh ¼ h3
ð9:62Þ
The inverse of the FIM is given by
JðhÞ1 ¼
lxxh3
Nðlxx  l2
xÞ
lxh3
Nðlxx  l2
xÞ
0
h3
Nðlxx  l2
xÞ
0
sym:
2h2
3
N
2
6666664
3
7777775
ð9:63Þ
9.3
Cramér-Rao Bound
305

Here it is difﬁcult to obtain an analytical expression for the eigenvalue of DðhÞ ¼
cov½Gjh  JðhÞ1 in manageable form. Alternatively, using the Sylvester’s
criterion,
D11  0
D11D22  D2
12  0
D11D22D33 þ 2D12D13D23  D11D2
23  D22D2
13  D33D2
12  0
ð9:64Þ
where Dij denote the ði; jÞ-entry of DðhÞ. These inequalities can be written explicitly
in terms of the variance and covariance of estimators by substituting the entries of
cov½Gjh and JðhÞ1 into Dij. Easier bounds can be obtained, although they are less
tight. See Example 9.8 later.
■
Example 9.6 (Modal identiﬁcation, CRB) In this example, we derive the CRB for
unbiased estimator for the vector of modal parameters h ¼ ½f ; f; S; SeT in Example
9.3. Recall the NLLF in (9.33):
Lðf^Skg; hÞ ¼
X
k
ln SkðhÞ þ
^Sk
SkðhÞ
"
#
ð9:65Þ
where SkðhÞ ¼ SDk þ Se and Dk is the dynamic ampliﬁcation factor in (9.31) that
depends on f and f. The gradient and Hessian of L w.r.t. h are respectively given by
rhL ¼
X
k
ðS1
k
 S2
k ^SkÞrSk
ð9:66Þ
r2
hL ¼
X
k
ð2S3
k ^Sk  S2
k ÞrTSkrSk þ
X
k
ðS1
k
 S2
k ^SkÞr2Sk
ð9:67Þ
where the dependence of Sk on h has been omitted for notational simplicity; rSk
and r2Sk are respectively the gradient and Hessian of Sk (w.r.t. h). The FIM is
given by
JðhÞ ¼ E½r2
hLjh
¼
X
k
f2S3
k
E½^Skjh
|ﬄﬄﬄ{zﬄﬄﬄ}
Sk
S2
k grTSkrSk þ
X
k
fS1
k
 S2
k
E½^Skjh
|ﬄﬄﬄ{zﬄﬄﬄ}
Sk
gr2Sk
¼
X
k
S2
k rTSkrSk
ð9:68Þ
Here, it is difﬁcult to obtain an analytical expression for JðhÞ1 in manageable
form. Easier but looser bounds are obtained in Example 9.9 later.
■
306
9
Classical Statistical Inference

9.3.1
Easier but Looser Bounds
Determining the FIM and taking inverse are two major difﬁculties in obtaining the
CRB. As illustrated in Examples 9.5 and 9.6, it is not always feasible to obtain
analytical expressions in manageable form. It is possible to obtain ‘easier’ bounds
at the expense of loosening the inequality, i.e., a bound that is a necessary con-
sequence of the original CRB and is therefore less tight.
The key is that inequalities similar to the original CRB also apply to a subset of
the parameters. Let a denote a subset of the index set f1; . . .; nhg; Ga and Ja denote
the partition of G (estimator) and J (FIM) corresponding to the indices in a. Then
the CRB in (9.34) implies that
cov½Gajh  ½J1a
|ﬄﬄ{zﬄﬄ}
partition
of inverse

J1
a
|{z}
inverse
of partition
ð9:69Þ
where ½a denotes the partition of the argument matrix corresponding to the index
set a.
The ﬁrst inequality in (9.69) follows from the fact that any partition of a positive
semi-deﬁnite matrix is also positive semi-deﬁnite, and is consistent with the
Sylvester’s criterion. It is just a partitioned version of (9.34). The second inequality
is a general result that holds for any positive deﬁnite matrix (see Sect. C.5.7).
Equation (9.69) provides an easier way for deriving lower bounds because the
FIM involved ðJaÞ is now of a smaller dimension. As a expands to include more
indices, the dimension of the FIM increases and the resulting bound tightens. The
tightest (but most difﬁcult) bound is the one involving all parameters in the original
CRB in (9.34), i.e., a ¼ f1; . . .; nhg. The loosest (but easiest) bound is obtained by
considering a particular parameter. Taking a ¼ i in (9.69) gives
var½Gijh  ðJ1Þii  J1
ii
i ¼ 1; . . .; nh
ð9:70Þ
where Jii and ðJ1Þii denote respectively the ði; iÞ-entry of J and J1. Note that this
bound holds only for diagonal entries. For i 6¼ j, it is not necessary that
cov½Gi; Gjjh  ðJ1Þij or  J1
ij .
Example 9.7 (Mean parameter, easy CRB) In Example 9.4, the bounds in (9.50)
were obtained by requiring the eigenvalues of DðhÞ ¼ cov½Gjh  JðhÞ1 to be
non-negative. Easier bounds can be obtained by using (9.70). From (9.45), J is a
diagonal matrix and so ðJ1Þii ¼ J1
ii . The middle and rightmost term in (9.70) are
then the same. Applying (9.70) with a ¼ 1; 2, for any unbiased estimator
G ¼ ½G1; G2T;
9.3
Cramér-Rao Bound
307

var½G1jh  h2=N
|ﬄ{zﬄ}
1=J11
var½G2jh  2h2
2=N
|ﬄﬄﬄ{zﬄﬄ}
1=J22
ð9:71Þ
which coincide with the ﬁrst two inequalities in (9.50). Here, easier bounds are
obtained at the expense of not discovering the third inequality in (9.50).
■
Example 9.8 (Linear regression, easy CRB) In Example 9.5, the CRB was
obtained in terms of the entries of DðhÞ ¼ cov½Gjh  JðhÞ1 in (9.64), which is
recalled here:
D11  0
D11D22  D2
12  0
D11D22D33 þ 2D12D13D23  D11D2
23  D22D2
13  D33D2
12  0
ð9:72Þ
Easier bounds can be obtained by applying the ﬁrst inequality in (9.70) with
a ¼ 1; 2; 3:
D11  0
D22  0
D33  0
ð9:73Þ
These bounds are at most as tight as those in (9.72) because they can be deduced
from it. The ﬁrst inequality in (9.73) is the same as the ﬁrst inequality in (9.72). The
second inequality in (9.73) can be deduced from the ﬁrst and second inequality in
(9.72). The third inequality in (9.73) can be deduced from the second and third
inequality in (9.72). Substituting the entries of JðhÞ1 in (9.63), the inequalities in
(9.73) read:
var½G1jh 
lxxh3
Nðlxx  l2
xÞ
var½G2jh 
h3
Nðlxx  l2
xÞ
var½G3jh  2h2
3
N
ð9:74Þ
The easiest bounds that do not require inverting JðhÞ are obtained by the
rightmost term in (9.70) with a ¼ 1; 2; 3:
var½G1jh  h3
N
|{z}
1=J11
var½G2jh 
h3
Nlxx
|ﬄﬄ{zﬄﬄ}
1=J22
var½G3jh  2h2
3
N
|{z}
1=J33
ð9:75Þ
where Jii denotes the ði; iÞ-entry of JðhÞ. The values on the RHS of the ﬁrst two
inequalities in (9.75) are smaller than their counterparts in (9.74). This implies that
the ﬁrst two bounds in (9.75) are looser than (i.e., necessary consequence of) those
in (9.74), as expected. The third inequality in (9.75) is the same as that in (9.74).
This stems from the fact that in this example ðJ1Þ33 ¼ J1
33 .
■
308
9
Classical Statistical Inference

Example 9.9 (Modal identiﬁcation, easy CRB) Recall the FIM in (9.68) of
Example 9.6:
JðhÞ ¼
X
k
S2
k rTSkrSk
ð9:76Þ
where h ¼ ½f ; f; S; SeT, SkðhÞ ¼ SDk þ Se and Dk is the dynamic ampliﬁcation
factor in (9.31) that depends on f and f. As it is difﬁcult to invert JðhÞ analytically,
we shall obtain the easiest bound by the rightmost term of (9.70). Note that the ith
diagonal entry of JðhÞ is Jii ¼ P
k S2
k ð@Sk=@hiÞ2. For any unbiased estimator G ¼
½G1; G2; G3; G4T for h, the easiest CRB gives
var½Gijh 
X
k
S2
k ð@Sk
@hi
Þ2
"
#1
i ¼ 1; . . .; 4
ð9:77Þ
Note that
@Sk
@f ¼ S @Dk
@f
@Sk
@f ¼ S @Dk
@f
@Sk
@S ¼ Dk
@Sk
@Se
¼ 1
ð9:78Þ
Substituting into (9.77) gives
var½G1jh 
X
k
ðSDk þ SeÞ2S2ð@Dk
@f Þ2
"
#1
frequency
ð
Þ
ð9:79Þ
var½G2jh 
X
k
ðSDk þ SeÞ2S2ð@Dk
@f Þ2
"
#1
damping
ð
Þ
ð9:80Þ
var½G3jh 
X
k
ðSDk þ SeÞ2D2
k
"
#1
modal force PSD
ð
Þ
ð9:81Þ
var½G4jh 
X
k
ðSDk þ SeÞ2
"
#1
prediction error PSD
ð
Þ
ð9:82Þ
The expressions on the RHS can be further simpliﬁed by considering long data and
small damping. The techniques are discussed in Sect. 16.2.
■
9.3
Cramér-Rao Bound
309

9.3.2
General Form
The CRB in (9.34) is a special form of a more general result where GðZÞ need not
even be an estimator for h. Let GðZÞ ðm  1Þ now be a general function of h where
m can be different from nh. As in (9.3), let gðhÞ ¼ E½GðZÞjh with Z distributed as
pðZjhÞ. Then it can be shown that
cov½GðZÞjh
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
mm
 ½rgðhÞ
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
mnh
JðhÞ1
|ﬄﬄﬄ{zﬄﬄﬄ}
nhnh
½rgðhÞT
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
nhm
ð9:83Þ
where
rg
|{z}
mnh
¼
@g
@h1
; . . .; @g
@hnh


ð9:84Þ
is the gradient of gðhÞ; and JðhÞ is the FIM in (9.35) as before. The equality in
(9.83) holds if and only if
GðZÞ
m1
¼ gðhÞ
m1
 ½rgðhÞ
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
mnh
JðhÞ1
|ﬄﬄﬄ{zﬄﬄﬄ}
nhnh
rT
hLðZ; hÞ
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
nh1
ð9:85Þ
where LðZ; hÞ ¼  ln pðZjhÞ is the NLLF. Equations (9.83) and (9.85) reduce
respectively to (9.34) and (9.36) when GðZÞ is an unbiased estimator for h, in
which case gðhÞ ¼ h and rg is the nh  nh identity matrix.
9.3.3
Derivation
Here we derive the general form of CRB in (9.83) and the alternative formula for
the FIM in (9.40). The derivation is based on the fundamental property that the
likelihood function is the (assumed) PDF of data. Recall Lðz; hÞ ¼  ln pðzjhÞ.
Since pðzjhÞ ¼ exp½Lðz; hÞ is the PDF of data Z for a given h, its integral over
z is always equal to 1:
Z
eLðz;hÞdz ¼ 1
for any h
ð9:86Þ
Taking gradient w.r.t. h and swapping the order of gradient and integration,
Z
½rhLðz; hÞeLðz;hÞdz ¼ 0
ð9:87Þ
310
9
Classical Statistical Inference

This identity can be written in terms of the ‘score function’:
SðZ; hÞ
nh1
¼ rT
hLðZ; hÞ
ð9:88Þ
When Z is distributed as pðzjhÞ, (9.87) says that
E½SðZ; hÞjh ¼ 0
ð9:89Þ
The FIM in (9.35) is just the covariance of SðZ; hÞ:
JðhÞ ¼ E½rT
hLðZ; hÞrhLðZ; hÞjh ¼ E½SðZ; hÞSðZ; hÞTjh
ð9:90Þ
Consider the cross covariance between GðZÞ and SðZ; hÞ:
cov½G; Sjh
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
mnh
¼ E½GSTjh  E½Gjh E½STjh
|ﬄﬄﬄ{zﬄﬄﬄ}
0
¼
Z
GðzÞSðz; hÞTpðzjhÞdz
ð9:91Þ
Substituting (9.88),
cov½G; Sjh ¼ 
Z
GðzÞ rhLðz; hÞpðzjhÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
rhpðzjhÞ
dz ¼ rh
Z
GðzÞpðzjhÞdz
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
gðhÞ
¼ rg
ð9:92Þ
where we have swapped the order of gradient and integration.
On the other hand, cov½G; Sjh can be related to cov½Gjh and cov½Sjh through a
matrix inequality. For any two random column vectors X and Y with positive
deﬁnite covariance matrices, it can be shown that (Sect. C.5.8)
E½YYT  E½YXTE½XXT1E½XYT
ð9:93Þ
where equality holds if and only if Y ¼ E½YXTE½XXT1X. This is a matrix
inequality that should be interpreted as the LHS minus RHS being a positive
semi-deﬁnite matrix. Taking X ¼ S and Y ¼ G  g, we have E½YYT ¼ cov½Gjh,
E½YXT ¼ cov½G; Sjh ¼ rg
[from
(9.92)]
and
E½XXT ¼ J
[from
(9.90)].
Substituting these into (9.93) gives (9.83), where equality holds if and only if
G  g ¼ ðrgÞJ1S, i.e., (9.85).
9.3
Cramér-Rao Bound
311

Fisher Information Matrix via Hessian
The expression of JðhÞ in (9.40) can be shown by taking gradient w.r.t. h of (9.87)
and swapping the order of gradient and integration:
Z
½r2
hLðz; hÞeLðz;hÞdz
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
E½r2
hLðZ; hÞjh

Z
½rT
hLðz; hÞrhLðz; hÞeLðz;hÞdz
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
JðhÞ
¼ 0
ð9:94Þ
9.4
Fisher Information Matrix for Gaussian Data
A general expression can be derived for the FIM for Gaussian data with mean and
covariance matrix that depend on model parameters, which is frequently encoun-
tered in applications.
9.4.1
Real Gaussian
Let data Z ðN  1Þ be modeled as a real-valued Gaussian vector with mean lðhÞ ¼
E½Zjh ðN  1Þ and covariance matrix CðhÞ ¼ E½ðZ  lÞðZ  lÞTjh ðN  NÞ,
both possibly depending on h ðnh  1Þ . Then the likelihood function and NLLF are
respectively given by
pðZjhÞ ¼ ð2pÞN=2jCðhÞj1=2 exp  1
2 ½Z  lðhÞTCðhÞ1½Z  lðhÞ


ð9:95Þ
LðZ; hÞ ¼  ln pðZjhÞ ¼ N
2 ln 2p þ 1
2 ln jCðhÞj þ 1
2 ½Z  lðhÞTCðhÞ1½Z  lðhÞ
ð9:96Þ
Let Jij denote the ði; jÞ-entry of the FIM JðhÞ. Then it can be shown that
Jij ¼ @lT
@hi
|{z}
1N
C1
|{z}
NN
@l
@hj
|{z}
N1
þ 1
2 tr
C1 @C
@hi
C1 @C
@hj


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
NN
ð9:97Þ
where trðÞ denotes the trace (i.e., sum of diagonal entries) of the argument matrix.
Using the symmetry of C1 and the cyclic property of trace ðtrðXYZÞ ¼ trðZXYÞÞ,
it can be veriﬁed that Jij ¼ Jji, as expected.
312
9
Classical Statistical Inference

If C does not depend on h, the second term in (9.97) is zero. The ﬁrst term can
then be assembled into a matrix to give the FIM in a compact form:
J
nhnh
¼ ðrlÞT
|ﬄﬄﬄ{zﬄﬄﬄ}
nhN
C1
|{z}
NN
ðrlÞ
|ﬄ{zﬄ}
Nnh
rl
Nnh
¼
@l
@h1
; . . .; @l
@hnh


ð9:98Þ
Proof of (9.97) (FIM for real Gaussian data)
To show (9.97), we evaluate Jij as the expectation of the second derivative of NLLF
in (9.96). The second derivative of the log-determinant term is given by
(Sect. C.5.3)
@2 ln jCj
@hi@hj
¼ tr C1 @2C
@hi@hj


 tr C1 @C
@hi
C1 @C
@hj


ð9:99Þ
For the quadratic term, applying chain-rule on the product of the three terms leads
to nine terms:
@2½ðZ  lÞTC1ðZ  lÞ
@hi@hj
¼
 @lT
@hi
@C1
@hj
ðZ  lÞ
 ðZ  lÞT @C1
@hi
@l
@hj
þ @lT
@hi
C1 @l
@hj
 @lT
@hj
@C1
@hi
ðZ  lÞ
 ðZ  lÞT @C1
@hj
@l
@hi
þ @lT
@hj
C1 @l
@hi
 @2lT
@hi@hj
C1ðZ  lÞ
þ ðZ  lÞT @2C1
@hi@hj
ðZ  lÞ
 ðZ  lÞTC1 @2l
@hi@hj
The randomness of these terms comes from the data Z. There are six terms that are
linear in ðZ  lÞ. Their expectations are zero because E½Zjh ¼ l. Thus, only the
expectations of the third, sixth and eighth term are non-zero. Since C1 is sym-
metric, the third and sixth terms are the same. The above considerations give
E @2ðZ  lÞTC1ðZ  lÞ
@hi@hj
h
"
#
¼ 2 @lT
@hi
C1 @l
@hj
þ E ðZ  lÞT @2C1
@hi@hj
ðZ  lÞ
h


ð9:100Þ
It remains to evaluate the expectation on the RHS. The term inside the bracket is a
scalar and so is equal to its trace. Using the cyclic property ðtrðXYZÞ ¼ trðZXYÞÞ
and linearity ðE½trðÞ ¼ trfE½gÞ of trace,
9.4
Fisher Information Matrix for Gaussian Data
313

E ðZ  lÞT @2C1
@hi@hj
ðZ  lÞ
h


¼ E tr ðZ  lÞT @2C1
@hi@hj
ðZ  lÞ

h


¼ E tr ðZ  lÞðZ  lÞT @2C1
@hi@hj

h


¼ tr E ðZ  lÞðZ  lÞTh

 @2C1
@hi@hj


¼ tr C @2C1
@hi@hj


ð9:101Þ
Note that (Sect. C.5.3)
@2C1
@hi@hj
¼ C1 @C
@hi
C1 @C
@hj
þ @C
@hj
C1 @C
@hi
 @2C
@hi@hj


C1
ð9:102Þ
Substituting into (9.101) and using the cyclic property of trace and symmetry of
C gives
E ðZ  lÞT @2C1
@hi@hj
ðZ  lÞ
h


¼ 2tr C1 @C
@hi
C1 @C
@hj


 tr C1 @2C
@hi@hj


ð9:103Þ
Substituting into (9.100) and then combining with (9.99) gives (9.97).
■
Example 9.10 (Mean parameter, FIM) Consider Example 9.4 again but now we
use (9.97) to obtain the FIM. Here, h ¼ ½h1; h2T, Z ¼ ½^y1; . . .;^yNT, lðhÞ ¼ h11N
and CðhÞ ¼ h2IN, where 1N ¼ ½1; . . .; 1T ðN  1Þ and IN is the N  N identity
matrix. Direct differentiation gives
@l
@h1
¼ 1N
@l
@h2
¼ 0
@C
@h1
¼ 0
@C
@h2
¼ IN
ð9:104Þ
Using (9.97),
J11 ¼ ð1NÞTðh2INÞ1ð1NÞ þ 1
2 tr ðh2INÞ1ð0Þðh2INÞ1ð0Þ
h
i
¼ N
h2
ð9:105Þ
J22 ¼ ð0ÞTðh2INÞ1ð0Þ þ 1
2 tr ðh2INÞ1ðINÞðh2INÞ1ðINÞ
h
i
¼ N
2h2
2
ð9:106Þ
J12 ¼ ð1NÞTðh2INÞ1ð0Þ þ 1
2 tr ðh2INÞ1ð0Þðh2INÞ1ðINÞ
h
i
¼ 0
ð9:107Þ
and J21 ¼ J12 by symmetry. Assembling these into a matrix gives the same result as
in (9.45).
■
314
9
Classical Statistical Inference

Example 9.11 (Linear regression, FIM) Consider Example 9.5 again but now we
use
(9.97)
to
obtain
the
FIM.
Here,
h ¼ ½h1; h2; h3T,
Z ¼ ½^y1; . . .;^yNT,
lðhÞ ¼ h11N þ h2x, x ¼ ½x1; . . .; xNT and CðhÞ ¼ h3IN, where 1N ¼ ½1; . . .; 1T
ðN  1Þ and IN is the N  N identity matrix. Direct differentiation gives
@l
@h1
¼ 1N
@l
@h2
¼ x
@l
@h3
¼ 0
@C
@h1
¼ @C
@h2
¼ 0
@C
@h3
¼ IN
ð9:108Þ
Using (9.97) and recalling lx ¼ N1 PN
k¼1 xk and lxx ¼ N1 PN
k¼1 x2
k,
J11 ¼ ð1NÞTðh3INÞ1ð1NÞ þ 1
2 tr ðh3INÞ1ð0Þðh3INÞ1ð0Þ
h
i
¼ Nh1
3
ð9:109Þ
J22 ¼ xTðh3INÞ1x þ 1
2 tr ðh3INÞ1ð0Þðh3INÞ1ð0Þ
h
i
¼ h1
3 xTx ¼ Nlxxh1
3
ð9:110Þ
J33 ¼ ð0ÞTðh3INÞ1ð0Þ þ 1
2 tr ðh3INÞ1ðINÞðh3INÞ1ðINÞ
h
i
¼ 1
2 Nh2
3
ð9:111Þ
J12 ¼ ð1NÞTðh3INÞ1ðxÞ þ 1
2 tr ðh3INÞ1ð0Þðh3INÞ1ð0Þ
h
i
¼ Nlxh1
3
ð9:112Þ
J13 ¼ ð1NÞTðh3INÞ1ð0Þ þ 1
2 tr ðh3INÞ1ð0Þðh3INÞ1ðINÞ
h
i
¼ 0
ð9:113Þ
and similarly J23 ¼ 0. Other entries can be determined by symmetry. Assembling
these into a matrix gives the same result in (9.61).
■
9.4.2
Complex Gaussian
The result in (9.97) can be extended to Z ¼ lðhÞ þ Z0 where lðhÞ ¼ E½Zjh (N  1
complex) and Z0 (N  1 complex) is a (zero-mean) circularly-symmetric complex
Gaussian vector with (complex) covariance matrix EðhÞ ¼ E½Z0Z0
jh. See
Appendix A for details on complex Gaussian vectors. The likelihood function and
NLLF are given respectively by
pðZjhÞ ¼ pNjEðhÞj1 exp ½Z  lðhÞ
EðhÞ1½Z  lðhÞ
n
o
ð9:114Þ
LðZ; hÞ ¼  ln pðZjhÞ ¼ N ln p þ ln jEðhÞj þ ½Z  lðhÞ
EðhÞ1½Z  lðhÞ
ð9:115Þ
9.4
Fisher Information Matrix for Gaussian Data
315

Let Jij denote the ði; jÞ-entry of the FIM JðhÞ. Then it can be shown that
Jij ¼ 2Re @l
@hi
E1 @l
@hj


þ tr E1 @E
@hi
E1 @E
@hj


ð9:116Þ
where l
 denotes the conjugate transpose of l. Using the Hermitian nature of E and
the cyclic property of trace, it can be veriﬁed that Jij ¼ Jji, as expected. Despite its
appearance, it can be shown that the second term on the RHS is always real-valued.
Equation (9.116) is used in Chap. 15 for deriving the long-data asymptotic
expressions for the posterior covariance matrix of modal properties in the Bayesian
approach.
9.5
Asymptotic Properties of ML Estimator
The ML estimator has good theoretical asymptotic properties for large sample size.
Under mild assumptions, it is asymptotically unbiased and Gaussian; and its covariance
matrix achieves the CRB. Speciﬁcally, assuming that Z is distributed as the likelihood
function pðZjhÞ, the ML estimator ^hðZÞ is asymptotically Gaussian with mean h and
covariance matrix JðhÞ1 as the sample size increases. Roughly speaking, for the
asymptotic properties to hold, the data should have limited correlation so that their
amount of information increases indeﬁnitely as sample size increases.
The asymptotic properties can be reasoned from the deﬁnition of ML estimator
and using the Strong Law of Large Numbers and Central Limit Theorem. Here we
illustrate the argument assuming Z ¼ ½Z1; . . .; ZN, where fZkg are i.i.d., each with
identical NLLF L0ðZk; hÞ ¼  ln pðZkjhÞ. In this case, the NLLF based on all data
Z is the sum of the individual ones:
LðZ; hÞ ¼
X
N
k¼1
L0ðZk; hÞ
ð9:117Þ
The FIM based on Z is then given by
JðhÞ ¼ E½r2
hLðZ; hÞjh ¼
X
N
k¼1
E½r2
hL0ðZk; hÞjh
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
J0ðhÞ
¼ NJ0ðhÞ
ð9:118Þ
where J0ðhÞ is the FIM based on each Zk. For a given Z, since LðZ; hÞ is minimized
at h ¼ ^hðZÞ
rhLðZ; hÞjh¼^hðZÞ¼ 0
ð9:119Þ
316
9
Classical Statistical Inference

On the other hand, using a ﬁrst order Taylor expansion,
rhLðZ; hÞjh¼^hðZÞ¼ rhLðZ; hÞ þ ½^hðZÞ  hTr2
hLðZ; hÞ þ Remainder
ð9:120Þ
where the remainder on the RHS is second order in ½^hðZÞ  h. Equation (9.119)
and (9.120) imply
ﬃﬃﬃﬃ
N
p
½^hðZÞ  h   Y1X
ð9:121Þ
where ‘*’ denotes that the LHS and RHS are asymptotically the same when
N ! 1;
X ¼
1ﬃﬃﬃﬃ
N
p
rT
hLðZ; hÞ ¼
1ﬃﬃﬃﬃ
N
p
X
N
k¼1
rT
hL0ðZk; hÞ
ð9:122Þ
Y ¼ 1
N r2
hLðZ; hÞ ¼ 1
N
X
N
k¼1
r2
hL0ðZk; hÞ
ð9:123Þ
Consider the distributions of X and Y when data Z is distributed as pðZjhÞ,
where h is the true parameter value. From (9.87), E½Xjh ¼ 0. The covariance
matrix of X is
E½XXTjh ¼ 1
N E½rT
hLðZ; hÞrhLðZ; hÞjh ¼ 1
N JðhÞ ¼ J0ðhÞ
ð9:124Þ
Since fZkgN
k¼1 and hence frhL0ðZk; hÞgN
k¼1 are i.i.d., by the Central Limit
Theorem, X is asymptotically Gaussian with mean zero and covariance matrix
J0ðhÞ as N ! 1. On the other hand, E½Yjh ¼ N1JðhÞ ¼ J0ðhÞ. Since Y is the
average of the i.i.d. matrices fr2
hL0ðZk; hÞgN
k¼1, by the Strong Law of Large
Numbers it tends to its expectation J0ðhÞ as N ! 1. The asymptotic properties of
X and Y imply that
ﬃﬃﬃﬃ
N
p
½^hðZÞ  h   Y1X is asymptotically Gaussian with zero
mean and covariance matrix equal to J0ðhÞ1. Equation (9.121) then implies that
^hðZÞ is asymptotically Gaussian with mean h (i.e., asymptotically unbiased) and
covariance matrix ½NJ0ðhÞ1, i.e., JðhÞ1, reaching the CRB.
The asymptotic properties of the ML estimator hold in more general situations
where the data are neither independent nor identically distributed. As seen in the
above reasoning, the conditions are related to those that ensure the Central Limit
Theorem to hold for X ¼ rT
hLðZ; hÞ=
ﬃﬃﬃﬃ
N
p
and the Strong Law of Large Numbers to
hold for Y ¼ r2
hLðZ; hÞ=N. Roughly speaking, the conditions ensure that data are
not too strongly correlated so that the effective number of independent observations
9.5
Asymptotic Properties of ML Estimator
317

can increase indeﬁnitely as N ! 1; and that probabilistically the sum is not
dominated by a small number of terms whose magnitudes are signiﬁcantly larger
than the remaining ones.
Example 9.12 (Mean parameter, asymptotics of MLE) In Example 9.1, the
estimator G2 for the prediction error variance was found to be the ML estimator but
it was biased for ﬁnite sample size N. Nevertheless, since E½G2jh ¼ ð1 
1=NÞ1h2 ! h2 as N ! 1, G2 is asymptotically unbiased. These results were
concluded based on the expression for the variance of estimator, which may not be
available in general problems. Here we illustrate the asymptotic behavior of the
NLLF that accounts for the asymptotic properties of the ML estimator G2.
Recall from (9.42), (9.43) and (9.44) that
Lðf^ykg; hÞ ¼  ln pðf^ykgjhÞ ¼ N
2 lnð2ph2Þ þ 1
2h2
X
N
k¼1
ð^yk  h1Þ2
ð9:125Þ
rhL ¼
Nh1
2 ðh1  l1Þ
N
2 h1
2  1
2 h2
2
X
N
k¼1
ð^yk  h1Þ2
"
#
ð9:126Þ
r2
hL ¼
Nh1
2
Nh2
2 ðh1  l1Þ
Nh2
2 ðh1  l1Þ
 N
2 h2
2 þ h3
2
P
N
k¼1
ð^yk  h1Þ2
2
4
3
5
ð9:127Þ
where l1 ¼ PN
k¼1 ^yk=N. Given h ¼ ½h1; h2T, f^ykgN
k¼1 are i.i.d. Gaussian with mean
h1 and variance h2. By the Strong Law of Large Numbers, as N ! 1,
Y ¼ 1
N r2
hL !
h1
2
0
0
h2
2 =2


¼ 1
N JðhÞ
ð9:128Þ
since l1 ! h1 and N1 PN
k¼1 ð^yk  h1Þ2 ! h2; JðhÞ is the FIM in (9.45).
On the other hand, it can be reasoned from (9.126) that rhL has zero expec-
tation. Its covariance matrix is by deﬁnition equal to JðhÞ. The ﬁrst entry of rhL is
Gaussian because l1 is. By the Central Limit Theorem, the sum PN
k¼1 ð^yk  h1Þ2
and hence the second entry of rhL is asymptotically Gaussian. Consequently,
X ¼ rhL=
ﬃﬃﬃﬃ
N
p
is asymptotically a Gaussian vector with zero mean and covariance
matrix JðhÞ=N. The asymptotic properties of X and Y in this example agree with
those discussed in the general case.
■
318
9
Classical Statistical Inference

9.6
Comparison with Bayesian Inference
The foregoing discussions of the classical statistical approach reveals many simi-
larities in mathematics with the Bayesian approach in Chap. 8, e.g., the likelihood
function, its gradient and Hessian. Nevertheless, there are important differences that
distinguish the two approaches fundamentally and that matter in the interpretation
of results and application to general inference problems. Here we compare the two
approaches within the scope of system identiﬁcation. General discussions can be
found in the references mentioned in the introduction of Chap. 8.
To allow a proper comparison, assume that when applying the Bayesian or
classical statistical approaches the same (modeled) likelihood function pðZjhÞ is
used. In the discussion of the CRB and the asymptotic behavior of ML estimators,
we also assume that data Z is indeed distributed as pðZjhÞ for some unknown true
parameter value h.
9.6.1
Philosophical Perspectives
Given the measured data Z, the Bayesian approach asks the question:
‘What do we know probabilistically about h?’
The answer to this question is encapsulated in the posterior distribution pðhjZÞ,
which is directly related to the likelihood function. In a Bayesian context, h is
always uncertain and modeled as a random variable. The presence of data does not
make it precisely known. Its distribution depends on available information, which is
indicated in the conditioning statement. In the absence of data, or without using the
information in data, the distribution is pðhÞ (prior PDF). When information in the
data is used, the distribution is pðhjZÞ (posterior PDF). When interpreting or using
this function, Z is ﬁxed at the available data and h is a variable. Under the same
modeling assumptions, data and prior information, the posterior distribution is
unique. This is all one needs to address the system identiﬁcation question for a
given model and data. The remaining issues are only on calculating the descriptive
statistics (e.g., mean and variance) of the posterior distribution for use in decision
making. These issues are important but nevertheless of computational nature only.
The classical statistical approach constructs a statistical estimator GðZÞ whose
value can be calculated for given measured data Z and is intended to be used as a
proxy for h. This estimator is not unique. It is subjected to the developer’s math-
ematical facility, imagination, familiarity with the problem, etc. Assessing the
quality of this estimator (e.g., bias, convergence) then becomes relevant.
Identiﬁcation uncertainty in the classical statistical approach refers to the
ensemble variability of estimator GðZÞ (not the unknown parameter h) over
hypothetical repeated experiments. This uncertainty does not depend on the par-
ticular data set and is therefore conceptually ‘inherent’ in nature. Analytical
expression for the variance of estimator, when it is feasible to derive, assumes that
9.6
Comparison with Bayesian Inference
319

Z is distributed as the likelihood function conditional on the ‘true’ parameter value,
i.e., pðZjhÞ. Here h is not known but mathematically it is always ﬁxed in all
statistical statements about GðZÞ. For example, the expression for the variance of
GðZÞ depends on h. Since it is unknown in applications, it is replaced by the value
of the estimate (calculated from available data) to given a value that approximates
the ‘true’ variance. When it is not possible to derive an analytical expression, the
variance of estimator can be estimated by the sample average of estimator values
over repeated experiments. In this case, it need not be the same as the theoretical
one because the data in reality need not be distributed as the likelihood function.
The resulting sample variance consists of modeling error as well, although it is
generally not trivial to distinguish between identiﬁcation error (for a given model
class) and modeling error.
The posterior distribution in the Bayesian approach is conditional on a given
model but it does not involve the concept of a ‘true parameter value’. Neither are
‘true variance’ or ‘inherent uncertainty’ relevant. The posterior distribution can
always be calculated for a given set of data (and model class). It is a function of data
Z and so there is no concept of intrinsic distribution or uncertainty. Different Zs
give different posterior distributions. Except for computational reasons, it is irrel-
evant to talk about ‘estimating’ the posterior distribution or posterior variance. The
ensemble average concept using hypothetical repeated experiments is irrelevant.
Averaging the posterior distribution for different Zs has no direct meaning.
Table 9.2 gives a summary of comparisons of the Bayesian and classical sta-
tistical approaches for system identiﬁcation.
9.6.2
Maximum Likelihood Estimator
Within the classical statistical perspective, consider now ML estimators. To avoid
conceptual problems arising from non-uniqueness of the ML estimator value, we
consider globally identiﬁable situations. Being the value that maximizes the like-
lihood function for given data, the ML estimator is mathematically the same as the
posterior MPV in a Bayesian context assuming a uniform prior distribution. As
discussed in Sect. 9.5, the ML estimator is asymptotically unbiased and Gaussian as
the data size increases. Its covariance matrix asymptotically approaches the inverse
of the FIM. The latter is equal to the expectation of Hessian of NLLF. These are
theoretical statements that assume the data is indeed distributed as the likelihood
function conditional on the true parameter value. In a Bayesian context, within a
Gaussian approximation of the posterior distribution w.r.t. the model parameters,
the posterior covariance matrix is equal to the inverse of the Hessian of NLLF at the
posterior MPV.
The distribution of the ML estimator should not be confused with the posterior
distribution of model parameters. Neither should the asymptotic Gaussian nature of
the ML estimator be confused with the Gaussian approximation of the posterior
distribution. The latter is merely an approximation arising from the second order
320
9
Classical Statistical Inference

Taylor approximation of the NLLF w.r.t. the model parameters about the MPV; and
it is computational in nature. No asymptotic concept is involved. The approxima-
tion can be exact even for ﬁnite data size if the NLLF is indeed a quadratic function
of the model parameters. On the other hand, the posterior distribution can be
non-Gaussian even if the data size increases to inﬁnity.
Being the inverse of the FIM, the asymptotic covariance matrix of the ML
estimator shares some similarity with the posterior covariance matrix in the
Gaussian approximation, as they are both related to the Hessian of the NLLF. There
are some important differences, however. For the posterior covariance matrix, the
Hessian is evaluated at the posterior MPV and so it is always positive deﬁnite. The
Hessian is calculated for a given set of data and no expectation (ensemble average)
is involved. For the FIM, the Hessian is evaluated at the unknown true parameter
value, and so it is not necessarily positive deﬁnite; only its expectation is. It cannot
be calculated using a single set of data, as it involves expectation over data.
Figure 9.1 compares Bayesian and ML inference. To avoid confusion in notation
the posterior distribution is denoted by pðnjZÞ (instead of pðhjZÞ), where n denotes
a dummy value of h. The asymptotic property of the posterior covariance matrix in
relation to the FIM is discussed in the next section.
9.6.3
Cramér-Rao Bound and Uncertainty Law
Recall from Sect. 8.5 that, for globally identiﬁable problems, the posterior distri-
bution for given data Z can be approximated by a Gaussian PDF with covariance
matrix ^CðZÞ given by:
^CðZÞ ¼ r2
hLðZ; hÞ

h¼^hðZÞ
h
i1
ð9:129Þ
Table 9.2 Comparison of the Bayesian and classical statistical approaches
Bayesian
Classical statistics
h is unknown and modeled as a random
variable whose distribution depends on
available information
h is the ‘true’ parameter value, unknown but
ﬁxed
Identiﬁcation result is the posterior PDF
pðhjZÞ
Identiﬁcation result is estimator GðZÞ,
suggested by a developer
Different data Z gives different pðhjZÞ
Different data Z gives different values of GðZÞ
Identiﬁcation uncertainty is related to the
spread of pðhjZÞ w.r.t. h for given Z
Identiﬁcation uncertainty is the variability of
GðZÞ over hypothetical repeated trials of Z
Identiﬁcation uncertainty (for a give model
class) does not tell modeling error
Variability of GðZÞ estimated from real data is
an aggregate of identiﬁcation uncertainty and
other effects (e.g., changing conditions,
modeling error)
9.6
Comparison with Bayesian Inference
321

where the dependence on Z has been emphasized; h here is a dummy variable for
the model parameters. On the other hand, in classical statistical estimation, the CRB
is given by the inverse of the FIM:
JðhÞ1 ¼ E½r2
hLðZ; hÞjh1
ð9:130Þ
where h here is the true parameter value. Under similar assumptions for the
asymptotic properties of the ML estimator, it is possible to connect mathematically
the posterior covariance matrix and the (tightest) CRB. For a large sample size,
denoted by N, the ‘leading order’ of the posterior covariance matrix is asymptoti-
cally equal to the CRB, i.e.,
^CðZÞ ¼
JðhÞ1
|ﬄﬄﬄ{zﬄﬄﬄ}
leading order

I þ OZðN1=2Þ

N ! 1
ð9:131Þ
where I denotes the identity matrix; OZðN1=2Þ denotes that the remainder depends
on data Z and is of order N1=2.
Fig. 9.1 Comparison of Bayesian and ML inference, assuming data Z is distributed as the
likelihood function pðZjhÞ, where h is the true value of the unknown parameter. The posterior PDF
is denoted by pðnjZÞ where n denotes a dummy parameter value of h. NLLF = Negative
log-likelihood function; JðhÞ ¼ E½r2
h ln pðZjhÞ = Fisher information matrix
322
9
Classical Statistical Inference

Equation (9.131) can be deduced as follow. First, note that ^hðZÞ is asymptoti-
cally unbiased and its covariance matrix is OðN1Þ. We can then write
^hðZÞ ¼ ½I þ OZðN1=2Þh. Assuming that the third order derivatives of NLLF exist,
1
N r2
hLðZ; hÞ

h¼^hðZÞ¼ 1
N r2
hLðZ; hÞ I þ OZðN1=2Þ
h
i
! 1
N JðhÞ I þ OZðN1=2Þ
h
i
ð9:132Þ
since N1r2
hLðZ; hÞ ! N1JðhÞ (Sect. 9.5). Equation (9.131) then follows by
noting that matrix inverse is a continuous mapping. The leading order term of the
posterior covariance matrix is called ‘uncertainty law’ and will be discussed in
Chaps. 15 and 16 for operational modal analysis.
Example 9.13 (Mean parameter, asymptotics of posterior covariance matrix)
In this example, we demonstrate the mathematical connection between the posterior
covariance matrix (under Gaussian approximation of the posterior PDF) and the
inverse of the FIM for the problem in Example 9.4. Recall that h ¼ ½h1; h2T is to be
identiﬁed from data modeled as ^yk ¼ h1 þ ek, where fekgN
k¼1 are assumed to be i.i.d.
zero-mean Gaussian with variance h2. The FIM was obtained in (9.45):
JðhÞ ¼
Nh1
2
0
0
Nh2
2 =2


ð9:133Þ
The Hessian of NLLF at the posterior MPV was obtained in Example 8.4:
^HL ¼
Nðl2  l2
1Þ1
0
0
Nðl2  l2
1Þ2=2


ð9:134Þ
where l1 ¼ PN
k¼1 ^yk=N and l2 ¼ PN
k¼1 ^y2
k=N. This can be written as
^HL ¼ JðhÞ X1
0
0
X2


X1 ¼ h2ðl2  l2
1Þ1
X2 ¼ h2
2ðl2  l2
1Þ2
ð9:135Þ
Assume the data is indeed distributed according to the likelihood function for a
true parameter h. It can then be reasoned that l2  l2
1 is asymptotically Gaussian
with mean h2 and variance 2h2=N as N ! 1. This implies that asymptotically X1
and X2 have a mean of 1 and a variance of OðN1Þ, i.e., a standard deviation of
OðN1=2Þ. Then we can write
X1
0
0
X2


¼ I2 þ OZðN1=2Þ
ð9:136Þ
9.6
Comparison with Bayesian Inference
323

where I2 denotes the 2  2 identity matrix and OZðN1=2Þ denotes the remainder
term that depends on data Z and is OðN1=2Þ. Substituting into (9.135),
^HL ¼ JðhÞ½I2 þ OZðN1=2Þ
ð9:137Þ
A similar relationship holds for its inverse, as in (9.131).
■
References
Cramér H (1946) Mathematical methods of statistics. Princeton University Press, NJ
Gilbert GT (1991) Positive deﬁnite matrices and Sylvester’s criterion. Am Math Monthly 98
(1):44–46
Meyer CD (2000) Matrix analysis and applied linear algebra. Society for Industrial and Applied
Mathematics, PA
Rao CR (1945) Information and the accuracy attainable in the estimation of statistical parameters.
Bull Calcutta Math Soc 37:81–89
324
9
Classical Statistical Inference

Chapter 10
Bayesian OMA Formulation
Abstract This chapter develops Bayesian formulations in operational modal
analysis for data of different natures, including synchronous data in a single setup
(the conventional setting), data from multiple setups and asynchronous data in a
single setup. Modeling assumptions and their justiﬁcations are discussed.
Keywords OMA formulation  Single mode  Multi-mode  Multi-setup 
Asynchronous
In this chapter we formulate Bayesian inference problems for operational modal
analysis (OMA) using the scaled Fast Fourier Transform (FFT) of ambient vibration
data. This involves stating the modeling assumptions, the parameters to be iden-
tiﬁed and deriving the likelihood function, which is practically the posterior
probability density function (PDF) of parameters. Typical scenarios are considered,
including data from a single setup, data from multiple setups at different time
periods, and asynchronous data in a single setup. This chapter covers formulations
but not computational issues in the determination of posterior statistics. The latter
are discussed in a general context in Chap. 11. Efﬁcient algorithms in speciﬁc
contexts are considered in Chaps. 12–14. See Sect. 1.5.4 for a literature note.
10.1
Single Setup Data
Let f^yjgN1
j¼0 (n  1) be the ambient vibration time history data measured at n DOFs
of a structure, where N is the number of samples per data channel. It is modeled as a
stationary stochastic process. Its scaled FFT at frequency fk ¼ k=NDt (Hz) is
deﬁned as
^F k ¼
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
j¼0
^yje2pijk=N
ð10:1Þ
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_10
325

where Dt (s) is the sampling time interval. The FFTs on a selected frequency band,
denoted by f ^F kg, is used for identifying the modal properties. Typically, the
selected band covers the resonance band of the mode(s) of interest.
Let h denote the set of modal parameters to be identiﬁed. It primarily comprises
the natural frequencies, damping ratios, mode shapes and other parameters (see
later) necessary to determine explicitly the distribution of f ^F kg. Using Bayes’
Theorem, the posterior PDF of h given f ^F kg is
pðhjf ^F kgÞ ¼ pðf ^F kgÞ1pðf ^F kgjhÞpðhÞ
ð10:2Þ
The term pðf ^F kgÞ1 is a normalizing constant and is immaterial to the posterior
PDF. For typical data size, the likelihood function pðf ^F kgjhÞ is fast varying
compared to the prior PDF pðhÞ: Without loss of generality, pðhÞ is assumed to be a
uniform distribution. The posterior PDF is then directly proportional to the likeli-
hood function:
pðhjf ^F kgÞ / pðf ^F kgjhÞ
ð10:3Þ
10.1.1
Likelihood Function
The likelihood function is the joint PDF of f ^F kg for given h: Assuming long data
(Sect. 4.8.3), f ^F kg are (circularly symmetric) complex Gaussian and independent
at different frequencies. This gives
pðf ^F kgjhÞ ¼
Y
k
pð ^F kjhÞ
ð10:4Þ
where the product is taken over all frequencies in the selected band with Nf FFT
points, assumed to be large compared to 1 (long data); and
pð ^F kjhÞ ¼
pn
jEkðhÞj exp  ^F 
kEkðhÞ1 ^F k
h
i
ð10:5Þ
EkðhÞ ¼ E½ ^F k ^F 
kjh (n  n Hermitian) is the theoretical power spectral density
(PSD) matrix of data for given h: This complex Gaussian PDF is central to
Bayesian OMA as it holds for general stationary ambient data regardless of the
326
10
Bayesian OMA Formulation

frequency content of contributing activities. The latter affects only EkðhÞ but not the
form of the PDF.
Resonance band modeling
Deriving EkðhÞ involves modeling the ambient data. Within the selected frequency
band, it is assumed that
^F k ¼ F k þ ek
ð10:6Þ
where F k and ek denote respectively the scaled FFT of the theoretical structural
dynamic response and prediction error (e.g., data noise). Their distributions depend
on h: Suppose the selected band is dominated by m vibration modes, referred as
mode 1, 2, …, m. Then F k ¼ Pm
i¼1 uinik where ui (n  1) is the partial mode shape
(i.e., conﬁned to measured DOFs) of mode i, and nik is the scaled FFT of modal
response at frequency fk. Let pik be the scaled FFT of the ith modal force at
frequency fk. Then nik ¼ hikpik (Sect. 5.2.1) and
^F k ¼
X
m
i¼1
uihikpik þ ek
ð10:7Þ
where, for mode i,
hik ¼
ð2pifkÞq
1  b2
ik  2fibiki
bik ¼ fi
fk
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð10:8Þ
is the transfer function, fi (Hz) and fi are respectively the natural frequency and
damping ratio. The modal forces are assumed to have a constant PSD matrix
S (m  m Hermitian) in the selected band, whose ði; jÞ-entry is Sij ¼ E½pikp
jkjh. The
prediction errors at different measured DOFs are assumed to be i.i.d. with a constant
PSD Se in the band:
E½eke
kjh ¼ SeIn
ð10:9Þ
where In denotes the n  n identity matrix. They are also assumed to be inde-
pendent of the modal forces. Consequently,
EkðhÞ ¼ E½F kF 
kjh þ E½eke
kjh ¼
X
m
i¼1
X
m
j¼1
hikh
jkSijuiuT
j þ SeIn
ð10:10Þ
10.1
Single Setup Data
327

This can be written in matrix form as
Ek ¼
U
nm Hk
mm UT
mn
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
modal contribution
þ
SeIn
|{z}
prediction error
ð10:11Þ
where
U ¼ ½u1; . . .; um
ð10:12Þ
is the partial mode shape matrix;
Hk ¼ diagðhkÞ S diagðhkÞ
hk ¼ ½h1k; . . .; hmk
ð10:13Þ
and diagðhkÞ denotes a diagonal matrix of the entries in hk. The ði; jÞ-entry of Hk is
Sijhikh
jk.
Posterior PDF and negative log-likelihood function
Combining (10.3)–(10.5), the posterior PDF of h is given by
pðhjf ^F kgÞ / pðf ^F kgjhÞ ¼
pnNf
Q
k jEkðhÞj exp 
X
k
^F 
kEkðhÞ1 ^F k
"
#
ð10:14Þ
where EkðhÞ is given by (10.11). The set of modal parameters h comprises ffigm
i¼1,
ffigm
i¼1, S, Se and U. For convenience in analysis and computation, the posterior
PDF is expressed as
pðhjf ^F kgÞ / pðf ^F kgjhÞ ¼ eLðhÞ
ð10:15Þ
where
LðhÞ ¼ nNf ln p þ
X
k
ln jEkðhÞj þ
X
k
^F 
kEkðhÞ1 ^F k
ð10:16Þ
is the ‘negative log-likelihood function’ (NLLF).
For typical data size, modal identiﬁcation problem is globally identiﬁable
(Sect. 8.5). The posterior PDF has a unique maximum at the most probable value
(MPV). Equivalently, the NLLF has a unique minimum at the MPV. A second
328
10
Bayesian OMA Formulation

order Taylor approximation of the NLLF at the MPV leads to a Gaussian
approximation of the posterior PDF:
pðhjf ^F kgÞ  ð2pÞnh=2j^Cj1=2 exp  1
2 ðh  ^hÞT ^C
1ðh  ^hÞ


ð10:17Þ
where ^h is the MPV; ^C is the covariance matrix, equal to the inverse of the Hessian
of NLLF at the MPV; nh is the number of parameters in h. Both ^h and ^C depend on
the FFT data f ^F kg and can be calculated when it is given.
10.1.2
Single Mode
The expression of Ek in (10.11) simpliﬁes when there is only one mode in the
selected band (m ¼ 1):
Ek ¼ SDk u
n1
uT
1n
þ SeIn
ð10:18Þ
where, omitting mode index i, S is the modal force PSD, u (n  1) is the partial
mode shape;
Dk ¼
ð2pfkÞ2q
ð1  b2
kÞ2 þ ð2fbkÞ2
bk ¼ f
fk
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð10:19Þ
is the dynamic ampliﬁcation factor; f (Hz) is the natural frequency and f is the
damping ratio. The set of parameters h reduces to comprise f , f, S, Se and u.
10.2
Remarks to Formulation
A Bayesian formulation and its identiﬁcation results are as good as the underlying
assumptions. Understanding the latter helps one interpret and judge the validity of
results in applications. Several aspects are discussed in the following.
10.1
Single Setup Data
329

10.2.1
Complex Gaussian FFT
The formulation has made use of the long-data asymptotic result that the scaled FFT
of a stationary stochastic process has a (circularly symmetric) complex Gaussian
distribution and they are independent at different frequencies. Data is considered
‘long’ if the number of FFT points in the resonance band is large compared to 1.
See Sect. 4.8.4 for details, and Example 4.7 that demonstrates the correlation of
FFTs under non-asymptotic situations.
The Gaussian nature of FFT is a robust result supported by several layers of
arguments by virtue of the Central Limit Theorem. In the ﬁrst place, FFT is
Gaussian whenever the time-domain modal response, modal excitation or physical
excitation is Gaussian; but it can be so even if none of them is Gaussian, provided
that the data is long. Modal excitation is a linear combination of the physical
excitations at various DOFs of the structure and so it can be Gaussian even if the
physical excitations are not. Even if the modal excitation is not Gaussian, the
resulting time-domain modal response can still be Gaussian because the response at
a given time instant is a linear combination of the past excitations. Even if the
time-domain modal response is not Gaussian, its FFT can still be Gaussian because
FFT is a linear combination of values in the time domain.
10.2.2
Selected Frequency Band
The selected frequency band determines the FFTs that are included in the likelihood
function for modal identiﬁcation. It can be determined with the help of a singular
value (SV) spectrum (Sect. 7.3), from which the number of modes m dominating
the band can also be decided. The choice of the selected band is a trade-off between
the information included for modal identiﬁcation and modeling error risk. Widening
the band includes more FFTs in the likelihood function and hence potentially more
information for identiﬁcation. As long as modeling assumptions are valid in the
selected band, this can reduce identiﬁcation uncertainty. Since the FFT of modal
response diminishes as the frequency moves away from the resonance peak,
including the FFTs at the tail has only a diminishing marginal effect on reducing
identiﬁcation uncertainty. Widening the selected band always increases modeling
error risk, e.g., with regard to a constant PSD in the modal force or prediction error.
The computed posterior statistics are conditional on modeling assumptions and they
do not reﬂect modeling errors. They are likely to be biased when the selected band
is excessively wide. As a trade-off, for a band dominated by a single mode, it may
be chosen such that in the SV spectrum the top line (which reﬂects modal response)
almost meets the second top line (which reﬂects prediction error). Further widening
it increases modeling error risk with little or no gain in information for modal
identiﬁcation. A similar rule can be applied for the general case of multiple (pos-
sibly close) modes.
330
10
Bayesian OMA Formulation

Ambient data contaminated with isolated harmonic signals can still be used for
modal identiﬁcation. If the frequency lies outside the selected band then the signal
does not affect the FFT data used in the likelihood function. Otherwise the FFT at or
near the isolated frequency can be simply excluded from the selected band. The
selected band need not be continuous.
10.2.3
Prediction Error Model
The prediction error ek has been modeled to account primarily for measurement
noise. Instead of being white over the whole sampling band, it is assumed to have a
constant PSD Se within the selected band only. This is a robust assumption as the
selected band typically covers the resonance band of the modes of interest. The
bandwidth (in Hz) is Oðf fÞ, much smaller than the sampling bandwidth (1=2Dt).
This is an advantage of operating in the frequency domain rather than the time
domain.
In reality, different data channels can have different noise levels and so it is
reasonable to assume that their prediction errors have different PSDs. A more
realistic assumption than (10.9) is
E½eke
kjh ¼
Se1
..
.
Sen
2
64
3
75
ð10:20Þ
where Sei is the prediction error PSD of the ith measured DOF. Despite the obvious
violation of reality, the one parameter prediction error model in (10.9) is adopted
because it leads to a simpler algebraic structure in the likelihood function, sim-
plifying analysis and computation. Numerical experiments showed that identiﬁca-
tion
results
are
insensitive
to
this
assumption
for
data
with
reasonable
signal-to-noise (s/n) ratios.
It is tempting to think that the prediction error can account for any discrepancy
between the modeled response and the measured data. It cannot. It can only account
for discrepancies whose statistical properties agree with the probability model
assumed. For example it does not account for discrepancy due to unmodeled
modes. The presence of unmodeled modes generally increases the MPV of Se but
its effect on identiﬁcation uncertainty need not be fully reﬂected in the posterior
covariance matrix.
10.2
Remarks to Formulation
331

10.2.4
Measurement Type
The type of measurement affects only the transfer function hik in (10.8) through the
parameter q. In structural vibration problems, velocity or acceleration data are
common, although reliable displacement measurements are becoming viable using
laser or high precision camera. From ﬁrst glance, it may seem that the type of
measurement is immaterial because data of one type can be converted to another, by
numerical differentiation/integration in the time domain, or equivalently by
multiplying/dividing by 2pfki in the frequency domain. For example, multiplying
the FFT of displacement data by ð2pfkiÞ2 gives the FFT of acceleration data. For
modal identiﬁcation, however, the type of measurement matters to the formulation.
This is because the associated prediction error model is different. If the prediction
error in displacement data has a constant PSD, then the derived acceleration data
will have a prediction error whose PSD is proportional to f4
k. This is illustrated in
Fig. 10.1, where the displacement data PSD in (a) has a ﬂat noise ﬂoor but the
derived acceleration data PSD in (b) has a noise ﬂoor increasing with frequency.
Using the derived acceleration data and assuming a constant prediction error PSD
will lead to modeling error. The effect of this modeling error depends on the width
of the selected band (the wider the worse) and s/n ratio (the lower the worse).
The type of data that should be assumed in the formulation should be determined
based on the validity of prediction error model rather than the actual type of
measured raw data. For example, suppose the raw data is velocity but upon
investigation of PSD one has reason to believe that its data noise PSD is indeed
inversely proportional to f2
k. Then it should be converted to acceleration data for
modal identiﬁcation and take q ¼ 0 in (10.8).
0
0.5
1
1.5
2
10
-2
10
-1
10
0
10
1
(a) Disp. PSD [( m)2/Hz]
Frequency [Hz]
0
0.5
1
1.5
2
10
0
10
1
10
2
10
3
10
4
(b) Acc. PSD [( m/s2)2/Hz]
Frequency [Hz]
µ
µ
Fig. 10.1 a Displacement PSD calculated from synthetic data (3600 s at 100 Hz) of a SDOF
structure (1 Hz natural frequency, 1% damping) with constant data noise PSD; b acceleration PSD
produced by multiplying the displacement PSD by ð2pfkÞ4
332
10
Bayesian OMA Formulation

10.2.5
Mode Shape Scaling
In (10.7), the scaling of the partial mode shape ui affects the scaling of the modal
response nik and modal force pik. As discussed in Sect. 5.2.4, when ui is multiplied
by a factor, nik and pik will be divided by the same factor; and the modal force PSD
Sii will be divided by the square of the factor. A robust way is to scale ui to have
unit norm, i.e., uT
i ui ¼ 1. In this case, the mode shape value of a particular DOF
will decrease when a new measured DOF is added. The modal response and modal
force PSD will then increase. This issue matters when one wants to compare the
intensity of the modal response or modal force identiﬁed from data of different
measurement arrays. For this purpose, the two sets of measurement arrays must
have some DOF(s) in common. Only modal quantities with the same mode shape
scaling can be compared. This can be done by scaling them so that the norm of the
common DOFs is equal to 1.
Example 10.1 (Scaling of modal PSD) Consider the horizontal vibration of a
ten-storied shear building with uniform mass 1000 tons per ﬂoor, interstory stiffness
1767 kN/mm and damping ratio 1% in all modes. The ﬁrst three modes have
natural frequencies 1, 2.98 and 4.89 Hz. The structure is subjected to i.i.d. white
noise excitation at all ﬂoors, each with PSD Sw ¼ 96:2N2=Hz. The equation of
motion is M€xðtÞ þ C_xðtÞ þ KxðtÞ ¼ FðtÞ where xðtÞ is the displacement vector; M,
C and K are respectively the mass, damping and stiffness matrix; FðtÞ is the force
vector. Consider the ﬁrst mode with mode shape w (10  1). The modal force PSD
is given by (Sect. 5.2.2)
Sp ¼
wTSFw
ðwTMwÞ2
ð10:21Þ
where SF is the PSD matrix of FðtÞ. Here, SF ¼ SwI10 ¼ 96:2I10 (N2=Hz) where I10
denotes the 10  10 identity matrix. The mass matrix is M ¼ 1000  103I10 (kg).
Let w ¼ ½w1; . . .; w10T and suppose it is scaled to have unit norm. Solving the
eigenvalue problem Kw ¼ x2Mw numerically gives the mode shape in Table 10.1.
Check that P10
i¼1 w2
i ¼ 1. Substituting M, SF and w into (10.21) gives
Sp ¼
P10
i¼1 w2
i ð96:2Þ
½P10
i¼1 w2
i ð1000  103Þ2 ¼ 96:2
1012
N2=Hz
kg2


¼
96:2
1012ð9:81Þ2 g2=Hz


¼ 1:00ðlgÞ2=Hz
Table 10.1 Mode shape
i
1
2
3
4
5
6
7
8
9
10
wi
0.0650
0.1286
0.1894
0.2459
0.2969
0.3412
0.3780
0.4063
0.4255
0.4352
10.2
Remarks to Formulation
333

This value of modal force PSD is seldom identiﬁed from data because the number
of measured DOFs is limited, i.e., the full mode shape vector w is not known.
Suppose the acceleration on 5/F and the roof is measured. The partial mode
shape u (2  1) at the measured DOFs is scaled to have unit norm. Under this
scaling,
u ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
w2
5 þ w2
10
q
w5
w10


w ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
w2
5 þ w2
10
q
w1
...
w10
2
64
3
75
ð10:22Þ
In reality, w is only referred conceptually because it cannot be identiﬁed. Since w is
now divided by
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
w2
5 þ w2
10
q
, the modal force PSD is multiplied by the square of this
factor:
S ¼ Spðw2
5 þ w2
10Þ ¼ ð1:00Þð0:29692 þ 0:43522Þ ¼ 0:278ðlgÞ2=Hz
ð10:23Þ
This value can be identiﬁed using the measured data, in contrast to Sp. It will
increase if more DOFs are measured.
■
Example 10.2 (Comparing modal force PSDs) Consider the ﬁve-storied building
in Fig. 10.2. In Test 1, DOFs 2, 3, 5 were measured. In Test 2, DOFs 1, 2, 3 were
measured. The identiﬁed mode shapes and modal force PSDs are shown in
Table 10.2. Which test has a higher modal force intensity in terms of PSD?
The modal force PSDs in Table 10.2 cannot be compared directly because their
mode shapes are scaled differently. To compare, the mode shapes are scaled so that
the sum of values at the common DOFs (2 and 3) is equal to 1. The mode shape in
Test 1 is divided by
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:39822 þ 0:55672
p
¼ 0:6844 and that in Test 2 by
DOF 1
DOF 2
DOF 3
DOF 4
DOF 5
Measured DOFs in Test 1
Measured DOFs in Test 2
All DOFs
Fig. 10.2 Five-storied building
334
10
Bayesian OMA Formulation

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:55682 þ 0:77832
p
¼ 0:9570. Correspondingly, the modal force PSD in Test 1 is
multiplied by 0:68442 and that in Test 2 by 0:95702. The results under the new
scaling are shown in Table 10.3. The identiﬁed mode shapes no longer have a unit
norm but this is immaterial for the purpose here. They now have the same scaling
and their modal force PSDs can be compared. In Table 10.2, although Test 1
appears to have a higher modal force PSD than Test 2, its intensity of modal force is
actually smaller, as evidenced by the lower modal force PSD value in Table 10.3.
As a remark, in this example the mode shape values identiﬁed in Setups 1 and 2
at the common DOFs (2 and 3) are exactly the same (see Table 10.3). In reality, this
is generally not true due to identiﬁcation error but the principle of comparison
remains the same.
■
10.2.6
Leakage
FFT has leakage due to ﬁnite data duration (Sect. 4.7.2). The formulation in Sect.
10.1 uses the long-data asymptotic form of the PSD matrix Ek, and so it does not
account for leakage in FFT. In principle, this modeling error can be eliminated by
simply replacing Ek with its ﬁnite-duration counterpart. The latter is equal to the
long-data asymptotic PSD in (10.11) convoluted with a sampling kernel. This gives
the same form as before:
Table 10.2 Identiﬁed mode
shape (normalized to unit
norm) and modal force PSD
Test 1
Test 2
Modal force PSD (ðlgÞ2=Hz)
1.0
0.6
Identiﬁed mode shape
DOF 1
–
0.2901
DOF 2
0.3982
0.5568
DOF 3
0.5567
0.7783
DOF 4
–
–
DOF 5
0.7291
–
Norm
1
1
Table 10.3 Identiﬁed mode
shape and modal force PSD
Test 1
Test 2
Modal force PSD (ðlgÞ2=Hz)
0.4685
0.5495
Identiﬁed mode shape
DOF 1
–
0.3032
DOF 2
0.5818
0.5818
DOF 3
0.8133
0.8133
DOF 4
–
–
DOF 5
1.0652
–
Norm
1.461
1.045
Mode shape normalized so that the sum of squares of DOF 2 and
3 is equal to 1
10.2
Remarks to Formulation
335

Ek ¼ UH0
kUT þ SeIn
ð10:24Þ
except now
H0
k ¼
Z1
1
HðfÞFNðfk  fÞdf
ð10:25Þ
where HðfÞ is a m  m matrix whose ði; jÞ-entry is equal to SijhiðfÞhjðfÞ;
hiðfÞ ¼
ð2pifÞq
1  b2
i  2fibii
bi ¼ fi
f
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð10:26Þ
and
FNðfÞ ¼ Dt sin2ðNpfDtÞ
N sin2ðpfDtÞ
ð10:27Þ
is the sampling kernel. Note that HðfÞ and hiðfÞ are continuous-frequency coun-
terparts of Hk and hik, respectively. The convolution in (10.25) can be computed
numerically on a grid of frequencies for f with a ﬁner resolution than fk.
Although the above is computationally feasible, it is not necessary in practice.
This is because the long-data criterion for suppressing leakage imposes a similar
requirement on data length for reasonable accuracy in the damping ratio, which is
the property most affected by leakage. If data is not long then the identiﬁcation
uncertainty in damping will be unreasonably large, in which case leakage bias will
be relatively insigniﬁcant. In short, use a data length so that the identiﬁcation
uncertainty in damping is small, in which case leakage is automatically suppressed.
See Chap. 15 for guidance.
10.3
Multi-setup Data
The formulation in Sect. 10.1 assumes that the data at different measured DOFs are
collected in a synchronous manner during the same time span. This requires an
array of sensors and data acquisition channels large enough to cover all the DOFs of
interest in a single setup. To identify a mode shape with more DOFs than the
available number of synchronous data channels, one common strategy is to perform
multiple setups with each one covering a different part of the structure while sharing
some reference DOFs in common.
336
10
Bayesian OMA Formulation

10.3.1
Global and Local Mode Shape
Let ui (n  1) be the ‘global mode shape’ of the ith mode (i ¼ 1; . . .; m) covering
all DOFs of interest. Assume that there are q setups, each covering a possibly
different set of DOFs. For Setup r (r ¼ 1; . . .; q), let Lr (nr  n) be the ‘selection
matrix’ that relate ui to the ‘local mode shape’ tri (nr  1) covering only the nr
measured DOFs in the setup:
tri
nr1 ¼ Lr
nrn ui
n1
ð10:28Þ
The ðj; kÞ-entry of Lr is equal to 1 if DOF k in ui is measured by data channel j in
Setup r; and zero otherwise. Since some DOFs are measured in more than one
setup, n\ Pq
r¼1 nr. Each DOF in ui can be measured by more than one data
channel in a given setup, but each channel can measure only one DOF in ui. That
is, for each Lr, there can be more than one ‘1’ in each column, but only one ‘1’ in
each row.
10.3.2
Reference DOFs
A measured DOF that appears in more than one setup is called a ‘reference DOF’.
Its presence allows the global mode shape to be determined from multiple setup
data. Having the same set of reference DOFs in all setups is easy to implement but
is not a necessary requirement. Nor is it necessary to have some DOFs measured in
all setups. A necessary and sufﬁcient condition is that the setups form a ‘connected
graph’. Two setups are ‘connected’ if they share at least one reference DOF with
non-zero mode shape value. Let C be a q  q matrix whose ðr; sÞ-entry is 1 if
Setups r and s are connected. Then C must not be a block diagonal matrix.
Example 10.3 (Six-storied building, multi-setup plan) Consider the six-storied
building in Fig. 10.3. A global mode shape with n ¼ 6 DOFs is identiﬁed using
three uniaxial sensors (A, B, C). The setup scheme appears somewhat unsystematic
but it can result from equipment or logistic constraints. Here is a hypothetical story.
Originally, A and B were planned to act as reference DOFs in all setups. Placing A
at the roof is intuitive as the response there is expected to be high. Placing B on 3/F
allows more modes to be captured in all setups. By roving C to other ﬂoors, a global
mode shape with six stories can be identiﬁed in four setups. C started on 4/F in
Setup 1. It was roved to 2/F in Setup 2. In preparing for Setup 3, A was found to be
defective. Also, the roof was no longer accessible (this was not known at planning
stage). Setups 3 and 4 were then re-planned on site. In Setup 3, B was placed on 5/F
(where response is expected to be high) while C remained on 2/F. In Setup 4, C was
roved to 1/F.
10.3
Multi-setup Data
337

Assume that in all setups the data from A, B and C are recorded in channels 1, 2
and 3, respectively. The global mode shape is u ¼ ½/1; . . .; /6T. The local mode
shapes (arranged according to data channels) are
t1 ¼
/6
/3
/4
2
4
3
5
A
B
C
t2 ¼
/6
/3
/2
2
4
3
5
A
B
C
t3 ¼
/5
/2


B
C
t4 ¼
/5
/1


B
C
ð10:29Þ
The selection matrices are
L1 ¼
0
0
0
0
0
1
0
0
1
0
0
0
0
0
0
1
0
0
2
64
3
75
L2 ¼
0
0
0
0
0
1
0
0
1
0
0
0
0
1
0
0
0
0
2
64
3
75
L3 ¼
0
0
0
0
1
0
0
1
0
0
0
0


L4 ¼
0
0
0
0
1
0
1
0
0
0
0
0


ð10:30Þ
Check that tr ¼ Lru, r ¼ 1; . . .; 4. Note the following: the setups do not have the
same number of measured DOFs; two setups can share more than one reference
DOF; no DOFs appear in all setups. These are all admissible.
■
10.3.3
Parameters in Different Setups
In multi-setup problems, different setups are assumed to be performed in different
time periods. This reﬂects the typical situation and has implications on modeling
assumptions. The data measured in the same setup are assumed to be synchronous,
while those in different setups are not. The ambient excitations and sensor noise in
different setups are assumed to be statistically independent; and so are their
DOF 5
All DOFs
DOF 6
DOF 3
DOF 4
DOF 1
DOF 2
A
B
C
A
B
B
B
C
Setup 1
Setup 2
Setup 3
Setup 4
C
C
Fig. 10.3 Setup plan, six-storied building example
338
10
Bayesian OMA Formulation

measured data. Except for the mode shapes, the natural frequencies, damping ratios,
modal excitations and sensor noise characteristics in different setups need not be the
same. The set of parameters h therefore comprises:
fff ðrÞ
i
gm
i¼1; ffðrÞ
i gm
i¼1; SðrÞ; SðrÞ
e gq
r¼1
fuigm
i¼1
where f ðrÞ
i
denotes the natural frequency of mode i in Setup r; the same notation
applies to other quantities. There are m norm constraints on the global mode shapes,
uT
i ui ¼ 1, i ¼ 1; . . .; m.
10.3.4
Likelihood Function
Let f^yðrÞ
j gNr1
j¼0
(nr  1) be the ambient vibration data in Setup r and f ^F ðrÞ
k g (nr  1
complex) be the collection of scaled FFTs in the selected frequency band:
^F ðrÞ
k
¼
ﬃﬃﬃﬃﬃﬃﬃ
Dtr
Nr
s
X
Nr1
j¼0
^yðrÞ
j e2pijk=Nr
frequency fðrÞ
k
¼
k
NrDtr
Hz
ð10:31Þ
where Dtr is the sampling interval of Setup r. Different setups can have different
measured DOFs, sampling interval or duration. Their selected frequency band can
also differ. Let the full data set be D ¼ ff ^F ð1Þ
k g; . . .; f ^F ðqÞ
k gg. Assuming a uniform
prior PDF for h, pðhjDÞ / pðDjhÞ. Assume that the prediction errors and modal
forces in different setups are independent. This implies that the scaled FFT in
different setups are independent, giving
pðhjDÞ /
Y
q
r¼1
pðf ^F ðrÞ
k gjhÞ
ð10:32Þ
For long data in each setup r, f ^F ðrÞ
k g are i.i.d. complex Gaussian with a joint PDF
given by
pðf ^F ðrÞ
k gjhÞ ¼ pnrNðrÞ
f
Q
k jEðrÞ
k j
exp 
X
k
^F ðrÞ
k
EðrÞ 1
k
^F ðrÞ
k
"
#
ð10:33Þ
EðrÞ
k
¼ Ur
nrm HðrÞ
k
mm
UT
r
mnr
þ SðrÞ
e Inr
ð10:34Þ
Ur ¼ ½tr1; . . .; trm
tri ¼ Lrui
ð10:35Þ
10.3
Multi-setup Data
339

HðrÞ
k
¼ diagðhðrÞ
k ÞSðrÞdiagðhðrÞ
k Þ
hðrÞ
k
¼ ½hðrÞ
1k ; . . .; hðrÞ
mk
ð10:36Þ
hðrÞ
ik ¼
ð2pifðrÞ
k Þq
1  bðrÞ2
ik
 2fðrÞ
i bðrÞ
ik i
bðrÞ
ik ¼ f ðrÞ
i
fðrÞ
k
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð10:37Þ
The NLLF is given by
LðhÞ ¼  ln pðDjhÞ ¼
X
q
r¼1
LrðhrÞ
ð10:38Þ
LrðhrÞ ¼ nrNðrÞ
f
ln p þ
X
k
ln jEðrÞ
k j þ
X
k
^F ðrÞ
k
EðrÞ 1
k
^F ðrÞ
k
ð10:39Þ
where hr ¼ fff ðrÞ
i
gm
i¼1; ffðrÞ
i gm
i¼1; SðrÞ; SðrÞ
e ; Urg comprises the parameters relevant to
Setup r. For r 6¼ s, hr and hs are not necessarily distinct. They may share in
common some mode shape parameters at the reference DOFs.
10.3.5
Single Mode
The expression of EðrÞ
k
in (10.34) simpliﬁes when there is only one mode in the
selected band ðm ¼ 1Þ:
EðrÞ
k
¼ SðrÞDðrÞ
k
tr
nr1 tT
r
1nr
þ SðrÞ
e Inr
ð10:40Þ
where (omitting mode index i) for Setup r, SðrÞ is the modal force PSD, tr (nr  1)
is the mode shape;
DðrÞ
k
¼
ð2pfðrÞ
k Þ2q
ð1  bðrÞ 2
k
Þ2 þ ð2fðrÞbðrÞ
k Þ2
bðrÞ
k
¼ f ðrÞ
fðrÞ
k
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð10:41Þ
is the dynamic ampliﬁcation factor, f ðrÞ (Hz) is the natural frequency and fðrÞ is the
damping ratio. The set of parameters h reduces to comprise ff ðrÞ; fðrÞ; SðrÞ; SðrÞ
e gq
r¼1
and u, subjected to constraint uTu ¼ 1.
340
10
Bayesian OMA Formulation

10.4
Asynchronous Data
The formulation in Sect. 10.1 assumes that the data at different DOFs are measured
in a synchronous manner. In this section we consider the case when they are not
perfectly synchronized. The analysis of asynchronous data is discussed in Sect. 7.5.
In a given setup suppose there are q groups of measured DOFs. Data channels of the
same group are synchronized but those of different groups are not. The total number
of measured DOFs is n ¼ Pq
r¼1 nr where nr is the number in Group r. As before let
f ^F kg (n  1 complex) denote the scaled FFT of data in the selected frequency band
and Nf be their number. Drifting of digital clock is a statistically nonstationary
process and so is asynchronous data. Despite this, the data is assumed to be sta-
tionary. This is approximate but is found to give reasonable results with real data
when the relative drift between data channels is small compared to the natural period
of interest. Under this assumption, for long data, f ^F kg is still complex Gaussian and
independent among different frequencies. Assuming a uniform prior PDF, the
posterior PDF of the set of modal parameters h is still given by (10.14):
pðhjf ^F kgÞ / pðf ^F kgjhÞ ¼
pnNf
Q
k jEkðhÞj exp 
X
k
^F 
kEkðhÞ1 ^F k
"
#
ð10:42Þ
where EkðhÞ ¼ E½ ^F k ^F 
kjh is the PSD matrix of data. The departure point lies in the
expression of EkðhÞ.
10.4.1
PSD Matrix
Deriving EkðhÞ requires modeling the spectral properties of asynchronous modal
response. This is done by associating different modal responses with different
groups. Let ui be partitioned as
ui
n1
¼
t1i
...
tqi
2
64
3
75
gn1  1
..
.
gnq  1
ð10:43Þ
That is, tri (nr  1) is the local mode shape of the measured DOFs in Group r. The
scaled FFT is modeled as
^F k
n1 ¼
X
m
i¼1
t1inð1Þ
ik
...
tqinðqÞ
ik
2
64
3
75 þ ek
ð10:44Þ
10.4
Asynchronous Data
341

where nðrÞ
ik (r ¼ 1; . . .; q) is the scaled FFT of the ith modal response following the
clock of Group r. Imperfect coherence between groups is modeled by
E½nðrÞ
ik nðsÞ
jk jh ¼ hikh
jkSijvijrs
ð10:45Þ
where vijrs (complex) is a coherence factor; Sij (modal force cross PSD) and hik
(transfer function) are deﬁned as before in Sect. 10.1. By deﬁnition, jvijrsj  1 and
vijrr ¼ 1. It has been assumed that vijrs and Sij are constant in the selected frequency
band, i.e., they do not depend on the frequency index k.
Equation (10.44) can be written in a more compact form as
^F k ¼
X
m
i¼1
ui
nq nik
q1
þ ek
n1
ui
nq ¼
t1i
n11
..
.
tqi
nq1
2
66664
3
77775
nik
q1
¼
nð1Þ
ik
..
.
nðqÞ
ik
2
64
3
75
ð10:46Þ
Correspondingly,
Ek ¼ E½ ^F k ^F 
kjh ¼
X
m
i¼1
X
m
j¼1
hikh
jkSij ui
nq vij
qq
uT
j
qn
þ diagðfSerInrgq
r¼1Þ
ð10:47Þ
where the ðr; sÞ-entry of vij is vijrs; diagðfSerInrgq
r¼1Þ denotes a block-diagonal
matrix formed by fSerInrgq
r¼1. The double sum over modes can be further assem-
bled to give
Ek ¼ U
nmq ½diagðhkÞ
zﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄ{
mm
	 Iq
z}|{
qq

|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
mqmq
Sv
mqmq
½diagðhkÞ 	 Iq
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
mqmq
UT
mqn þ diagðfSerInrgq
r¼1Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
nn
ð10:48Þ
where ‘	’ denotes the Kronecker product, hk ¼ ½h1k; . . .; hmk and U ¼ ½u1; . . .; um;
Sv is a matrix with m  m partitions and its ði; jÞ-partition is Sijvij (q  q):
Sv ¼
S11v11
S12v12
  
S1mv1m
S22v22
S2mv2m
..
.
...
block sym:
Smmvmm
2
6664
3
7775
ð10:49Þ
342
10
Bayesian OMA Formulation

The set of modal parameters h comprises
ffigm
i¼1
Natural frequencies
ffigm
i¼1
Damping ratios
S
mm
Hermitian
modal force
PSD matrix
fSergq
r¼1
Prediction error PSD
fuigm
i¼1
n1Mode shapes
fvij : j 
 igm
i¼1
qq
Hermitian
coherence
matrix
The diagonal entries of vij are equal to 1. The mode shapes are subjected to norm
constraints, uT
i ui ¼ 1, i ¼ 1; . . .; m.
10.4.2
Single Mode
The expression of Ek in (10.47) simpliﬁes when there is only one mode in the
selected band (m ¼ 1):
Ek ¼ SDk u
nq v
qq
uT
qn þ diagðfSerInrgq
r¼1Þ
ð10:50Þ
where (omitting mode index i)
u
n1
¼
t1
..
.
tq
2
64
3
75
gn1  1
..
.
gnq  1
u
nq ¼
t1
n11
..
.
tq
nq1
2
66664
3
77775
v
qq
¼
1
v12
  
v1q
1
v2q
..
.
..
.
Herm:
1
2
6664
3
7775
ð10:51Þ
The dynamic ampliﬁcation factor Dk and modal force PSD S have the same deﬁ-
nition in Sect. 10.1. The prediction error PSD Ser is analogous to Se. The set of
modal parameters h reduces to comprise
f
Natural frequency
f
Damping ratio
S
Modal force PSD
fSergq
r¼1
Prediction error PSD
u
n1
Mode shape
v
qq
Hermitian
coherence
matrix
10.4
Asynchronous Data
343

Chapter 11
Bayesian OMA Computation
Abstract This chapter discusses the computational issues in Bayesian operational
modal analysis and presents their solutions in a general context. The main theo-
retical issue lies in the computation of the posterior covariance matrix of modal
parameters under constraints, which can be addressed systematically using
Lagrange multiplier concepts. The uncertainty quantiﬁcation of mode shape, which
is of vectorial nature under scaling constraint, is also discussed. The computational
framework in this chapter will be followed in Chaps. 12–14 where speciﬁc algo-
rithms are developed.
Keywords OMA computation  Posterior covariance matrix  Constrained
Hessian
In this chapter we discuss the computational issues in determining the posterior
statistics of modal parameters in Bayesian operational modal analysis (OMA).
Modal identiﬁcation problem is globally identiﬁable and so Gaussian approxima-
tion can be applied to the posterior probability density function (PDF) of modal
parameters. The posterior statistics can then be conveniently described in terms of
the most probable value (MPV) and covariance matrix. General issues are discussed
in this chapter; efﬁcient algorithms for speciﬁc contexts (single mode, multi-mode,
etc.) are postponed to Chap. 12–14. Mode shapes, among other parameters, have a
special role because of their vectorial nature. Issues on their computation and
identiﬁcation uncertainty quantiﬁcation are discussed. Although this chapter is
presented in the context of OMA, its theory is generally applicable for globally
identiﬁable problems of Bayesian inference.
Let D denote the data for Bayesian inference. Assuming a uniform prior PDF,
the posterior PDF of the set of parameters h to be identiﬁed can be written as
pðhjDÞ / eLðhÞ
ð11:1Þ
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_11
345

where
LðhÞ ¼  ln pðDjhÞ
ð11:2Þ
is the negative log-likelihood function (NLLF), whose expression has been derived
in Chap. 10 for OMA under different situations. The posterior MPV of h minimizes
the NLLF. A second order approximation of NLLF at the MPV leads to a Gaussian
approximation of the posterior PDF centered at the MPV and with a covariance
matrix equal to the inverse of the Hessian of NLLF at the MPV (Sect. 8.5).
Computational issues on the MPV are concerned with how it can be determined
efﬁciently from the NLLF without brute-force numerical optimization. Issues with
the posterior covariance matrix are concerned with how it can be computed cor-
rectly when the parameters are under constraints (e.g., from mode shape scaling).
11.1
Posterior Most Probable Value
Determining the MPV by brute-force numerical minimization of the NLLF is far
from being efﬁcient, or defective in many cases. For example, consider the case of
single setup data in Sect. 10.1. The NLLF is given by
LðhÞ ¼ nNf ln p þ
X
k
ln jEkðhÞj þ
X
k
^F 
kEkðhÞ1 ^F k
ð11:3Þ
where f ^F kg (n  1 complex) is the scaled Fast Fourier Transform (FFT) of data in
the selected frequency band with Nf points and n measured DOFs; and Ek (n  n
Hermitian) is the theoretical power spectral density (PSD) matrix of data at FFT
frequency fk (Hz) for given h. Typically, Ek is almost singular because it is dom-
inated by the contribution from modal response. The latter a has rank of at most m
(the number of modes), often smaller than the dimension n of Ek. The number of
modal parameters, nh, is typically large. It increases with m and n. The increase with
m is quadratic, although m is typically small. For well-separated modes, one can
identify each mode separately and so m ¼ 1. In general, m only need to be the
number of close modes, which often does not exceed three. Arising from mode
shapes, nh increases linearly with n, which can be large in applications (e.g., >10).
An additional complication is that the parameters in h are subjected to constraints,
e.g., arising from the scaling of mode shapes.
The NLLF can be rewritten in a form that facilitates analysis and computation.
The basic strategy is to write it as a quadratic function (or alike) of the mode shapes
using linear algebra techniques, from which the most probable mode shapes can be
determined analytically or semi-analytically in terms of the remaining parameters.
In doing so, the singularity of Ek can be resolved analytically so that the matrices in
the resulting expression are all well-conditioned. Structured matrices such as the
346
11
Bayesian OMA Computation

modal force PSD matrix (for multiple modes) and coherence matrix (for asyn-
chronous data) can be parameterized by a necessary set of free parameters so that
they can be optimized directly. ‘Partial optimal solutions’, i.e., in terms of the
remaining parameters, allow the development of iterative algorithms where the
MPV of different groups of parameters are updated in turn until convergence.
Example 11.1 (Singularity of PSD matrix) Consider single setup acceleration data
with only one mode in the selected band, i.e., m ¼ 1. From Sect. 10.1, the theo-
retical PSD matrix of data is
Ek ¼ SDkuuT þ SeIn ¼ SDkðuuT þ Se
SDk
InÞ
ð11:4Þ
where S is the modal force PSD, Dk ¼ ½ð1  b2
kÞ2 þ ð2fbkÞ21, bk ¼ f =fk, f is the
natural frequency (Hz), f is the damping ratio and u n  1
ð
Þ is the mode shape. The
term Se=SDk represents a noise-to-signal ratio in terms of PSD at frequency fk. At
fk ¼ f , Dk ¼ 1=4f2 and so Se=SDk ¼ 4Sef2=S. For good quality data this ratio is
small compared to 1. Consequently Ek  SDkuuT has rank 1. Suppose u ¼
½1; 2; 3T=
ﬃﬃﬃﬃﬃ
14
p
and assume the following typical values: f ¼ 1%, Se ¼ 1012 g2=Hz
and S ¼ 1012 g2=Hz. Then Se=SDk ¼ 4  104  1 at fk ¼ f . The eigenvalues of
Ek are 1012, 1012 and 2:50  109 g2=Hz. The ﬁrst two eigenvalues are signif-
icantly smaller than the last one. In this sense, Ek is almost singular with rank 1. ■
Example 11.2 (Counting modal parameters) In a single setup test suppose we use
six triaxial sensors. Then there are n ¼ 6  3 ¼ 18 measured DOFs. Suppose there
are m ¼ 2 modes in the selected frequency band. The number of parameters are
counted as follow: frequencies ff1; f2g, damping ratios ff1; f2g, prediction error
PSD Se, mode shapes
u1 ¼
/11
/21
..
.
/18;1
2
6664
3
7775
u2 ¼
/12
/22
..
.
/18;2
2
6664
3
7775
ð11:5Þ
The modal force PSD matrix is Hermitian and so it can be represented as
S ¼
S11
S12
S21
S22


¼
S11
u þ vi
u  vi
S22


ð11:6Þ
where u ¼ ReS12 and v ¼ ImS12. It has four (real-valued) parameters S11; S22; u; v
ð
Þ.
The total number of parameters is
nh ¼
2
|{z}
frequencies
þ
2
|{z}
damping ratios
þ
1
|{z}
prediction error PSD
þ
36
|{z}
mode shapes
þ
4
|{z}
modal force PSD
¼ 45
11.1
Posterior Most Probable Value
347

There are two norm constraints on mode shapes:
uT
1u1 ¼ /2
11 þ /2
21 þ . . . þ /2
18;1 ¼ 1
uT
2u2 ¼ /2
12 þ /2
22 þ . . . þ /2
18;2 ¼ 1
ð11:7Þ
■
11.2
Posterior Covariance Matrix
Under Gaussian approximation of the posterior PDF, its covariance matrix is equal
to the inverse of the Hessian of NLLF at the MPV. The Hessian can be calculated
using ﬁnite difference approximation or analytical expressions for the derivatives of
NLLF. The former is approximate but requires only the values of the NLLF. The
latter is exact but has overheads in derivation and programing.
Modal parameters can be subjected to equality constraints such as those arising
from norm constraints on mode shapes. In some formulation (e.g., multi-setup) the
set of modal parameters is deliberately expanded so that the partial derivatives of
the NLLF can be obtained more easily. In this case there are additional constraints
arising from the relationships among the expanded set of parameters. The con-
straints need to be accounted for when calculating the Hessian of the NLLF.
11.2.1
Mapping with Free Parameters
The posterior covariance matrix of h under constraints can be calculated system-
atically via the covariance matrix of a set of ‘free’ (i.e., unconstrained) parameters.
Suppose there are nc independent constraints given by
GiðhÞ ¼ 0
i ¼ 1; . . .; nc
ð11:8Þ
Let vcðuÞ nh  1
ð
Þ be a function that maps u (p  1 free parameters) to h so that it
always satisﬁes the constraints, i.e.,
GiðvcðuÞÞ ¼ 0
i ¼ 1; . . .; nc for any u
ð11:9Þ
With nh parameters in h subjected to nc constraints, the dimension of the admissible
space is nh  nc. The number of parameters in u must then be
p 	 nh  nc
ð11:10Þ
Suppose the Bayesian inference problem is formulated in terms of u instead of h.
The NLLF for u, denoted by LcðuÞ, is then given by
LcðuÞ ¼ LðvcðuÞÞ
ð11:11Þ
348
11
Bayesian OMA Computation

11.2.2
Transformation of Covariance Matrix
The MPV of u is related to the MPV of h by ^h ¼ vcð^uÞ, where a hat ‘^’ denotes the
MPV. Under Gaussian approximation of the posterior PDF of u, its posterior
covariance matrix is the inverse of the Hessian of LcðuÞ at ^u. From this the posterior
covariance matrix of h ¼ vcðuÞ can be obtained by transformation of variables. Let
Du and Dh be uncertain variations in u and h from their MPV, respectively. To the
ﬁrst order,
Dh
nh1 ¼
X
p
i¼1
@^vc
@ui
Dui ¼
@^vc
@u1
  
@^vc
@up


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
r^vc
Du1
...
Dup
2
64
3
75
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
Du
¼ r^vc
nhp Du
p1
ð11:12Þ
where the hat ‘^’ in @^vc=@ui denotes that it is evaluated at the MPV. The posterior
covariance matrix of h is then
^C ¼ E½DhDhTjD ¼ ðr^vcÞ E½DuDuTjD
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^Cu
ðr^vcÞT ¼ ðr^vcÞ^Cuðr^vcÞT
ð11:13Þ
where ^Cu p  p
ð
Þ is the posterior covariance matrix of u, equal to the inverse of
r2^Lc (Hessian of Lc at the MPV). Due to the mapping vcðuÞ, r2^Lc (p  p) only has
rank nh  nc and so is singular when p [ nh  nc. Nevertheless this singularity is
immaterial to the posterior uncertainty of h. Section 11.2.6 shows that
^C
nhnh ¼ ðr^vcÞ
nhp
ðr2^LcÞ þ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
pp
ðr^vcÞT
pnh
ð11:14Þ
where ‘ þ ’ denotes the ‘pseudo-inverse’, i.e., evaluated via eigenvector represen-
tation
ignoring
the
p  nh þ nc
zero
eigenvalue
components
arising
from
constraints.
11.2.3
Hessian of Composite Function
Since LcðuÞ ¼ LðvcðuÞÞ is a composite function of u, its Hessian r2Lc (w.r.t. u) is
analytically more complicated to derive than the Hessian r2L (w.r.t. h). It is
11.2
Posterior Covariance Matrix
349

possible to obtain r2Lc in terms of the derivatives of vcðuÞ and LðhÞ so that their
information can be combined systematically. It can be shown that (Sect. B.1)
r2Lc
pp ¼ ðrvcÞT
pnh
r2L
nhnh ðrvcÞ
nhp
þ ð Ip
pp

 rL
1nhÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
pnhp
r2vc
nhpp
ð11:15Þ
where ‘
’ denotes the Kronecker product; Ip denotes the p  p identity matrix;
rvc
nhp ¼
@vc
@u1
  
@vc
@up
h
i
r2vc
nhpp ¼
@2vc
@u2
1
@2vc
@u1@u2
  
@2vc
@u1@up
..
.
@2vc
@u2@up
..
.
block sym:
@2vc
@u2p
2
6666664
3
7777775
ð11:16Þ
rL
1nh ¼ rLðhÞjh¼vcðuÞ
r2L
nhnh ¼ r2LðhÞ

h¼vcðuÞ
ð11:17Þ
Equation (11.15) is applicable for any u, not just at the MPV. It involves the ﬁrst
two derivatives of LðhÞ and vcðuÞ. The second term involves the product of two
large matrices, although the ﬁrst matrix is sparse.
Generally, rL does not vanish at the MPV due to the presence of constraints.
The Hessian r2Lc at the MPV is not equal to the ﬁrst term in (11.15). Rather, it can
be shown using Lagrange multiplier concept that at the MPV (Sect. B.2)
r2^Lc
pp ¼ ðr^vcÞT
pnh
ðr2^J
nhnhÞ r^vc
nhp
ð11:18Þ
where
JðhÞ ¼ LðhÞ þ
X
nc
i¼1
kiGiðhÞ
ð11:19Þ
is the ‘Lagrangian’; fkignc
i¼1 are Lagrange multipliers;
r2^J ¼ r2^L þ
X
nc
i¼1
^kir2 ^Gi
ð11:20Þ
350
11
Bayesian OMA Computation

is the Hessian of J at the MPV;
r^vc ¼ rvcðuÞju¼^u
r^Gi ¼ rGiðhÞjh¼^h
r2 ^Gi ¼ r2GiðhÞ

h¼^h
ð11:21Þ
^k ¼
^k1
..
.
^knc
2
64
3
75 ¼ ½ðr^GÞ
ncnh
ðr^GÞT
nhnc
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
ncnc
1 ðr^GÞ
ncnh
ðr^LÞT
nh1
ð11:22Þ
r^L ¼ rLðhÞjh¼^h
r2^L ¼ r2LðhÞ

h¼^h
r^G
ncnh ¼
r^G1
...
r^Gnc
2
64
3
75
ð11:23Þ
Equation (11.18) involves the ﬁrst two derivatives of LðhÞ but only the ﬁrst
derivatives of vcðuÞ. It also involves the ﬁrst two derivatives of fGignc
i¼1. The deﬁ-
nition of vc and fGignc
i¼1 are not unique but they do not affect r2^Lc; see next section.
11.2.4
Transformation Invariance
The posterior covariance matrix of h is invariant to the choice of the constraint
functions fGignc
i¼1 and mapping vcðuÞ. For the former, it can be shown that
(Sect. B.2.2) r2^Lc remains the same when Gi is replaced by HiðGiðhÞÞ for any
scalar function Hi with a non-zero derivative at 0. On the other hand, if one works
with a new set of free parameters a instead of u, the resulting posterior covariance
matrix of h remains the same.
To illustrate, let u ¼ TðaÞ where T is a transformation; and wðaÞ ¼ vcðTðaÞÞ.
To be admissible, the transformation should be such that h ¼ wðaÞ always satisﬁes
constraints. The NLLF for a is KðaÞ ¼ LðwðaÞÞ. Applying (11.14) but now taking
a ¼ ½a1;    ; aqT as the set of free parameters, the posterior covariance matrix of h
is given by
^C0 ¼ ðr^wÞðr2 ^KÞ þ ðr^wÞT
ð11:24Þ
where
r2 ^K ¼ ðr^wÞTðr2^JÞðr^wÞ
ð11:25Þ
11.2
Posterior Covariance Matrix
351

Note that
@w
@ai
¼
X
p
j¼1
@vc
@uj
@uj
@ai
¼
@vc
@u1
  
@vc
@up


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
rvc
@u1=@ai
...
@up=@ai
2
64
3
75
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
@T=@ai
¼ rvc
nhp
@T
@ai
p1
ð11:26Þ
rw ¼
@w
@a1
  
@w
@aq


¼ rvc
@T
@a1
  
@T
@aq


¼ rvc
nhp rT
pq
ð11:27Þ
Thus, r^w ¼ r^vcr^T, where the hat ‘^’ denotes that it is evaluated at the MPV.
Substituting into (11.25) and then into (11.24) gives
^C0 ¼ ðr^wÞ ðr^wÞTðr2^JÞðr^wÞ

 þ ðr^wÞT
¼ r^vcr^T

ðr^TÞT ðr^vcÞTðr2^JÞðr^vcÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
r2^Lc
ðr^TÞ
 þ
ðr^TÞTðr^vcÞT
ð11:28Þ
Using singular value decomposition, it can be shown that this gives the same
expression as ^C in (11.14) and so the posterior covariance matrix of h remains the
same (details omitted). Essentially, the effect of the r^Ts inside the pseudo-inverse
‘cancels’ out the effect of those outside.
11.2.5
Constraint Singularity
As a result of vcðuÞ mapping u (dimension p 	 nh  nc) to the admissible space of
lower dimension s ¼ nh  nc, r2^Lc has p  nh þ nc zero eigenvalues. To see this,
take gradient w.r.t. u on GiðvcðuÞÞ ¼ 0, transpose and evaluate at the MPV to obtain
ðr^vcÞT
pnh
ðr^GiÞT
nh1
¼ 0
i ¼ 1; . . .; nc
ð11:29Þ
Assuming that fr^Gignc
i¼1 are linearly independent, this implies that ðr^vcÞT has a
null space of dimension (i.e., nullity) nc. The rank-nullity theorem (rank + nul-
lity = number of columns) implies that ðr^vcÞT (with nh columns) has a rank of
s ¼ nh  nc. The same is also true for r^vc (column rank = row rank). Using the
rank-nullity theorem again, r^vc (with p columns) then has a nullity of p  s.
Right-multiplying r2^Lc ¼ ðr^vcÞTðr2^JÞðr^vcÞ in (11.18) with p  s linearly
352
11
Bayesian OMA Computation

independent null vectors of r^vc gives a zero vector, implying that they are also null
vectors of r2^Lc. This is referred as ‘constraint singularity’. Note that r2Lc need not
be singular when it is not evaluated at the MPV.
11.2.6
Pseudo-inverse
Apparently, constraint singularity causes legitimacy problem when taking the inverse
of r2^Lc. As far as posterior uncertainty is concerned, however, this singularity can be
ignored, in the sense that the posterior covariance matrix can be evaluated via the
eigenvector representation of r2^Lc where the zero eigenvalue components associ-
ated with constraint singularity are ignored. This is legitimate because to the ﬁrst
order h ¼ vcðuÞ is invariant to variations of u in the null space of r2^Lc.
Speciﬁcally, let fbigp
i¼1 be the (orthonormal) eigenvectors of r2^Lc with eigen-
values fjigp
i¼1 (in descending order of magnitude). Then r2^Lc ¼ Pp
i¼1 jibibT
i .
Replacing the eigenvalues by their reciprocals gives (symbolically) the inverse,
ðr2^LcÞ1 ¼ Pp
i¼1 j1
i bibT
i .
The
posterior
covariance
matrix
of
h
is
then
(symbolically)
^C ¼ ðr^vcÞð
X
p
i¼1
j1
i bibT
i Þðr^vcÞT ¼
X
p
i¼1
j1
i ðr^vcbiÞðr^vcbiÞT
ð11:30Þ
Due to constraint singularity, for i ¼ s þ 1; . . .; p, ji ¼ 0 and bi is a null vector of
r^vc, i.e., r^vcbi ¼ 0. According to (11.12), any uncertain variations in u from the
MPV along the directions fbigp
i¼s þ 1 will not lead to any variation in h. Thus,
despite the reciprocal of zero eigenvalues, the components along the directions
fbigp
i¼s þ 1 are mapped to zero in ^C. The summands in (11.30) for i ¼ s þ 1; . . .; p
are then zero, giving
^C ¼ ðr^vcÞð
X
s
i¼1
j1
i bibT
i Þðr^vcÞT
ð11:31Þ
Symbolically, this is written as
^C ¼ ðr^vcÞðr2^LcÞ þ ðr^vcÞT
ð11:32Þ
where
ðr2^LcÞ þ ¼
X
s
i¼1
j1
i bibT
i
ð11:33Þ
11.2
Posterior Covariance Matrix
353

denotes the ‘pseudo-inverse’ of r2^Lc obtained by ignoring the zero eigenvalue
components along fbigp
i¼s þ 1.
Example 11.3 (Pseudo-inverse) Let h ¼ ½h1; h2; h3T and A be a 2  2 real sym-
metric and positive deﬁnite matrix with eigenvalues fa1; a2g (in ascending order of
magnitude) and eigenvectors fb1; b2g (with unit norm). Consider minimizing
FðhÞ ¼ h2
1 þ xTAx
x ¼
h2
h3


ð11:34Þ
subjected to constraint h2
2 þ h2
3 ¼ 1. It can be reasoned that the optimal point is
^h ¼ ½0; b1.
The
constraint
can
be
expressed
as
GðhÞ ¼ 0
where
GðhÞ ¼ 1  h2
2  h2
3. For u ¼ ½u1; u2; u3T, the mapping
vcðuÞ ¼
u1
u2=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u2
2 þ u2
3
p
u3=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u2
2 þ u2
3
p
2
4
3
5 ¼
u1
jjxjj1x


x ¼
u2
u3


jjxjj ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
u2
2 þ u2
3
q
ð11:35Þ
always satisﬁes the constraint, i.e., GðvcðuÞÞ ¼ 0 for any u. Deﬁne the composite
function
FcðuÞ ¼ FðvcðuÞÞ ¼ u2
1 þ xT
jjxjj A x
jjxjj ¼ u2
1 þ xTAx
xTx
x ¼
u2
u3


ð11:36Þ
By direct differentiation and using Sect. C.5.4 for the derivative of ðxTAxÞ=ðxTxÞ,
it can be shown that the Hessian of FcðuÞ at the optimum is
r2^Fc ¼
2
2ðA  a1I2Þ


ð11:37Þ
where I2 denotes the 2  2 identity matrix. Substituting I2 ¼ b1bT
1 þ b2bT
2 and
A ¼ a1b1bT
1 þ a2b2bT
2, one can write
r2^Fc ¼ 0 
0
b1


0
b1

T
þ 2ða2  a1Þ 
0
b2


0
b2

T
þ 2 
1
0


1
0

T
ð11:38Þ
This shows that r2^Fc has eigenvalues f0; 2ða2  a1Þ; 2g and eigenvectors ½0; b1,
½0; b2, ½1; 0; 0T. Its pseudo-inverse is
ðr2^FcÞ þ ¼
1
2ða2  a1Þ
0
b2


0
b2

T
þ 1
2
1
0


1
0

T
¼ 1
2
1
ða2  a1Þ1b2bT
2


ð11:39Þ
■
354
11
Bayesian OMA Computation

11.2.7
Singular Vector Formula
The posterior covariance matrix of h can be expressed in terms of the singular
vectors of r^vc. Consider the singular value decomposition of r^vc:
r^vc
nhp ¼ L
nhs K
ss RT
sp
ð11:40Þ
where K is a diagonal matrix of the s ¼ nh  nc singular values of r^vc; L and R are
orthonormal matrices, i.e., LTL ¼ Is and RTR ¼ Is; and their columns are singular
vectors of r^vc. Substituting into (11.18) and evaluating the pseudo-inverse in
(11.14), it can be shown that
^C
nhnh ¼ L
nhs LT
snh r2^J
nhnh L
nhs

1
LT
snh
ð11:41Þ
The bracketed matrix has full rank and hence is invertible.
Proof of (11.41) (Singular Vector Formula)
Substituting (11.40) into (11.18) and noting that K ¼ KT,
r2^Lc ¼ R
ps ½KLTðr2^JÞLK
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
ss
RT
sp
ð11:42Þ
The bracketed matrix is real symmetric. Let frigs
i¼1 and faigs
i¼1
s  1
ð
Þ be its
eigenvalues (real) and eigenvectors (orthonormal). Replacing the bracketed matrix
by Ps
i¼1 riaiaT
i gives r2^Lc ¼ Ps
i¼1 riðRaiÞðRaiÞT. Since RTR ¼ Is and faigs
i¼1
are orthonormal, ðRaiÞTðRajÞ ¼ aT
i RTRaj ¼ aT
i aj is equal to 1 if i ¼ j and zero
otherwise. This implies that the eigenvalues and eigenvectors of r2^Lc are respec-
tively ri
and Rai, i ¼ 1; . . .; s; the remaining eigenvalues are zero. The
pseudo-inverse of r2^Lc is then
ðr2^LcÞ þ ¼
X
s
i¼1
r1
i ðRaiÞðRaiÞT
¼ Rð
X
s
i¼1
r1
i aiaT
i ÞRT
¼ R½ K
ss LTðr2^JÞL
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
ss
K
ss1RT
¼ RK1 LTðr2^JÞL

1K1RT
ð11:43Þ
Substituting this and (11.40) into (11.14) gives (11.41).
■
11.2
Posterior Covariance Matrix
355

11.2.8
Dimensionless Hessian
Modal parameters can have different units and hence potentially different orders of
magnitude and sensitivity in the likelihood function. Large disparity in magnitude
among the entries in r2^Lc leads to ill-conditioning in computation. One way to
circumvent this issue is to normalize the parameters in u by their MPV (except for
mode shapes) so that r2^Lc is dimensionless. Consider h ¼ vcðuÞ as before but now
u ¼ Ta where T (p  p, invertible) transforms the set of dimensionless free
parameters a (p  1) to give u. When the problem is formulated in terms of a the
NLLF is LcðaÞ ¼ LðvcðTðaÞÞÞ. To the ﬁrst order, uncertain variations of h and a
from their MPVs are related by Dh ¼ ðr^vcTÞDa. Following the same argument in
Sect. 11.2.3, r^Lc and ^C are now given by (11.18) and (11.14) with r^vc replaced
by r^vcT. Thus
^C
nhnh ¼ ðr^vcTÞ
nhp
½ðr^vcTÞT
pnh
ðr2^JÞ
nhnh
ðr^vcTÞ
nhp
 þ ðr^vcTÞT
pnh
ð11:44Þ
The bracketed matrix is now dimensionless. Note that a is only conceptually
involved in arriving at the result. All quantities in the formula have the same
deﬁnition as before. The singular vector formula (11.41) still applies, but (11.40)
now reads r^vcT ¼ LKRT, from which K, L and R should be obtained accordingly.
Example 11.4 (Quadratic likelihood, different methods) Here we use a simple
example to illustrate different ways of calculating the Hessian and posterior
covariance matrix under constraints. Let h ¼ ½h1; h2T (h1; h2 	 0) be under con-
straint h1h2 ¼ 1. Suppose the NLLF is LðhÞ ¼ h2
1 þ 2h2
2. What is the posterior
covariance matrix of h?
By Elimination
Without using the theory in this section, it is not difﬁcult in this example to
determine the posterior covariance matrix by eliminating h2. Since h2 ¼ 1=h1, the
problem can be formulated in terms of only h1 with the NLLF L1ðh1Þ ¼ h2
1 þ 2h2
1 .
Minimizing
L1
gives
the
MPV
^h1 ¼ 21=4.
Direct
differentiation
gives
@2L1=@2h1 ¼ 2 þ 12h4
1 , which is equal to 8 at the MPV. Under Gaussian
approximation, the posterior variance of h1 is therefore 1/8. Its c.o.v. (coefﬁcient of
variation) is
ﬃﬃﬃﬃﬃﬃﬃﬃ
1=8
p
=21=4 ¼ 27=4  30%. Using h2 ¼ 1=h1, the MPV of h2 is
^h2 ¼ 21=4. Taking log and differential of h2 ¼ 1=h1 gives Dh2=h2 ¼ Dh1=h1 and
so to the ﬁrst order the c.o.v. of h2 is equal to that of h1. The variance of h2 is then
equal to ðc:o:v:  MPVÞ2 = ð27=4  21=4Þ2 ¼ 1=16. Clearly, h1 and h2 are fully
negatively correlated and so their covariance is equal to the negative of the product
356
11
Bayesian OMA Computation

of their standard deviations, i.e., 
ﬃﬃﬃﬃﬃﬃﬃﬃ
1=8
p

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1=16
p
¼ 
ﬃﬃﬃ
2
p
=16. In summary, the
posterior covariance matrix of h ¼ ½h1; h2T is
^C ¼
1=8

ﬃﬃﬃ
2
p
=16

ﬃﬃﬃ
2
p
=16
1=16


ð11:45Þ
General Formula
We now determine the posterior covariance matrix of h using (11.14) where r2^Lc is
calculated using (11.15) at the MPV. The constraint is expressed as GðhÞ ¼ 0 where
GðhÞ ¼ 1  h1h2. There are nh ¼ 2 parameters and nc ¼ 1 constraint and so u must
have p 	 nh  nc ¼ 1 parameters. Suppose we deﬁne u ¼ ½ u1
u2 T and vcðuÞ ¼
½ u1
u1
1 T so that GðvcðuÞÞ ¼ 1  u1u1
1
¼ 0 for any u. Direct differentiation
gives
rL ¼ 2½ h1
2h2 
rG ¼ ½ h2
h1 
rvc ¼
1
0
u2
1
0


ð11:46Þ
r2L ¼
2
0
0
4


r2G ¼
0
1
1
0


r2vc ¼
0
0
2u3
1
0
0
0
0
0
2
664
3
775
ð11:47Þ
Substituting into (11.15) gives r2Lc for any u. Evaluating at the MPV ^u ¼
½ 21=4
21=4 T gives r2^Lc. The results are
r2Lc ¼
12u4
1 þ 2
0
0
0


r2^Lc ¼
8
0
0
0


ð11:48Þ
These expressions check with the Hessian obtained by direct differentiation of
LcðuÞ ¼ LðvcðuÞÞ ¼ u2
1 þ 2u2
1 . The eigenvector representation and pseudo-inverse
of r2^Lc are respectively given by
r2^Lc ¼ 0 
0
1


0
1

T
þ 8 
1
0


1
0

T
ðr2^LcÞ þ ¼ 1
8 
1
0


1
0

T
ð11:49Þ
Substituting into (11.14) gives the same result in (11.45).
Lagrange Multiplier Formula
Consider using (11.14) but now r2^Lc is calculated using (11.18). Evaluating
(11.46) and (11.47) at the MPV and substituting into (11.22) gives
^k ¼ ½ðr^GÞðr^GÞT1ðr^GÞðr^LÞT ¼ 6^h1^h2
^h2
1 þ ^h2
2
¼ 23=2
ð11:50Þ
11.2
Posterior Covariance Matrix
357

r2^J ¼ r2^L þ ^kr2 ^G ¼
2
0
0
4


þ 23=2
0
1
1
0


¼
2
23=2
23=2
4


ð11:51Þ
Equation (11.18) then gives
r2^Lc ¼ ðr^vcÞTðr2^JÞr^vc
¼
1
0
21=2
0

T
2
23=2
23=2
4
"
#
1
0
21=2
0


¼
8
0
0
0


ð11:52Þ
as before. Substituting into (11.14) gives the same result in (11.45).
Singular Vector Formula
Consider using (11.41). Here, L in (11.40) can be obtained by observing that
r^vc ¼
1
0
21=2
0


¼
ﬃﬃﬃ
2
3
r
1
21=2


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
unit norm

ﬃﬃﬃ
3
2
r
 1
0
½

|ﬄﬄﬄ{zﬄﬄﬄ}
unit norm
¼
ﬃﬃﬃﬃﬃﬃﬃﬃ
2=3
p
1=
ﬃﬃﬃ
3
p


|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
L

ﬃﬃﬃ
3
2
r
|{z}
K
 1
0
½

|ﬄﬄﬄ{zﬄﬄﬄ}
RT
ð11:53Þ
Check that LTL and RTR are both equal to 1. Substituting into (11.41) gives
^C ¼ L LTðr2^JÞL

1LT
¼
ﬃﬃﬃﬃﬃﬃﬃﬃ
2=3
p
1=
ﬃﬃﬃ
3
p
"
#
ﬃﬃﬃﬃﬃﬃﬃﬃ
2=3
p
1=
ﬃﬃﬃ
3
p
"
#T
2
23=2
23=2
4
"
#
ﬃﬃﬃﬃﬃﬃﬃﬃ
2=3
p
1=
ﬃﬃﬃ
3
p
"
#
(
)1
ﬃﬃﬃﬃﬃﬃﬃﬃ
2=3
p
1=
ﬃﬃﬃ
3
p
"
#T
ð11:54Þ
which returns the same result in (11.45). Note that the term in the brace is just a
scalar.
Dimensionless Hessian
Consider using (11.44) with dimensionless a ¼ ½ h1=^h1
h2=^h2 T. Here, u ¼ Ta
where
T ¼
^h1
^h2


¼
21=4
21=4


r^vcT ¼
21=4
0
21=4
0


ð11:55Þ
358
11
Bayesian OMA Computation

Substituting into (11.44) gives
^C ¼ ðr^vcTÞ½ðr^vcTÞTðr2^JÞðr^vcTÞ þ ðr^vcTÞT
¼
21=4
0
21=4
0
"
#
21=4
0
21=4
0
"
#T
2
23=2
23=2
4
"
#
21=4
0
21=4
0
"
#
(
) þ
21=4
0
21=4
0
"
#T
ð11:56Þ
which returns the same result in (11.45). Note that the matrix in the brace is
dimensionless.
Remark In this problem the minimum number of parameters required in u is p ¼ 1.
The problem could have been solved by deﬁning u ¼ u1 while vcðuÞ and GðhÞ are
the same as before. In the calculation, only the following quantities are affected:
r^vc ¼
1
21=2


¼
ﬃﬃﬃﬃﬃﬃﬃﬃ
2=3
p
1=
ﬃﬃﬃ
3
p


|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
L

ﬃﬃﬃﬃﬃﬃﬃﬃ
3=2
p
|ﬄﬄ{zﬄﬄ}
K
 ½1
|{z}
RT
r2^vc ¼
0
21=4


r2^Lc ¼ 8
T ¼ 21=4
In this case, r2^Lc is no longer singular. Substituting these into (11.14), (11.41) or
(11.44) gives the same result as before.
■
11.3
Mode Shape Uncertainty
Mode shape is a vector quantity and it is subjected to norm constraint. Unlike other
scalar parameters such as natural frequency or damping ratio, the standard deviation
or c.o.v. of the mode shape value at a particular DOF is not a direct indication of
uncertainty. For example, if there is only one DOF then the mode shape value is
always 1 and so its uncertainty is irrelevant. If there are two DOFs, the mode shape
values are correlated because their squares must sum to 1 to satisfy norm constraint.
Consider a particular mode shape u (n  1) and let the set of modal parameters
be partitioned as h ¼ ½-; u where - ða ðnh  nÞ  1 vectorÞ contains the
remaining parameters (it can contain other mode shapes). Let GðhÞ be the constraint
function enforcing unit norm in u; and vcðuÞ be the mapping such that GðvcðuÞÞ ¼
0 for any u. One simple choice is
GðhÞ ¼ 1  uTu
vcðuÞ ¼
-
jjujj1u


ð11:57Þ
11.2
Posterior Covariance Matrix
359

The discussion in this subsection is based on this choice but the resulting conclu-
sions are applicable in general because the posterior covariance matrix is invariant
to it (Sect. 11.2.4).
11.3.1
Norm Constraint Singularity
As mode shapes are subjected to norm constraints, the posterior covariance matrix
of h and its partition corresponding to each mode shape is singular along the most
probable mode shape directions. Speciﬁcally, let ^h ¼ ½^-; ^u and ^C
nh  nh
ð
Þ be
respectively the posterior MPV and covariance matrix of h; and ^Cu (n  n) be the
covariance matrix of u, equal to the n  n partition of ^C corresponding to u. Then
^C
0
^u


¼ 0
^Cu^u ¼ 0
ð11:58Þ
The second equation can be obtained by multiplying out the partitions in the ﬁrst
equation. To show the ﬁrst equation, since GðvcðuÞÞ ¼ 0 for any u, taking gradient
and evaluating at the MPV gives r^Gr^vc ¼ 0. Using (11.14),
^Cðr^GÞT ¼ ðr^vcÞðr2^LcÞ þ ðr^Gr^vc
|ﬄﬄﬄﬄ{zﬄﬄﬄﬄ}
0
ÞT ¼ 0
ð11:59Þ
and so ^C is singular along the direction ðr^GÞT. The argument follows by noting
that ðr^GÞT ¼ ½0nhn; 2^u. Applying the same argument to different modes, it fol-
lows that ^C has as many singularities as the number of modes along their most
probable mode shape directions.
11.3.2
Stochastic Representation
According to the mapping vcðuÞ in (11.57), an uncertain mode shape distributed as
the posterior PDF can be represented as
u ¼ jjnjj1n
ð11:60Þ
where n is a Gaussian vector with mean ^u (unit norm) and covariance matrix ^Cu.
The relationship between u and n is illustrated in Fig. 11.1. To the ﬁrst order, any
uncertain deviation Dn is perpendicular to ^u.
360
11
Bayesian OMA Computation

Since ^Cu is positive semi-deﬁnite and has a zero eigenvalue with eigenvector ^u, it
has the eigenvector representation
^Cu ¼
X
n
i¼2
d2
i aiaT
i
ð11:61Þ
where fd2
i gn
i¼1 and faign
i¼1 denote respectively the eigenvalues (in ascending order
of magnitude) and orthonormal eigenvectors of ^Cu; d1 ¼ 0 and a1 ¼ ^u. Then n can
be represented as
n ¼ ^u þ
X
n
i¼2
Zidiai
ð11:62Þ
where Zi s are i.i.d. standard Gaussian random variables. Using orthogonality and
unit norm property of faign
i¼1,
jjnjj2 ¼ 1 þ
X
n
i¼2
Z2
i d2
i
ð11:63Þ
Substituting into (11.60),
u ¼ ð1 þ
X
n
i¼2
Z2
i d2
i Þ1=2ð^u þ
X
n
i¼2
ZidiaiÞ
ð11:64Þ
This can be used for generating random samples of uncertain mode shape dis-
tributed as the posterior PDF.
11.3.3
Expected MAC and Mode Shape c.o.v
The ‘modal assurance criterion’ (MAC) is commonly used for quantifying the
discrepancy between two mode shapes. It is deﬁned as the dot product between two
mode shape vectors, or equivalently, the cosine of the hyper-angle between them.
The higher the MAC value the closer the two mode shapes. This idea can be
ˆ
Cˆ
matrix 
covariance
ith
Gaussian w
mean 
zero
~
1
||
||
Unit norm
0
Fig. 11.1 Uncertain mode
shape u
11.3
Mode Shape Uncertainty
361

extended to quantifying the posterior uncertainty of mode shape in a Bayesian
context. Consider the MAC between the uncertain mode shape u and its MPV ^u. If
the uncertainty in u is small, it will be close to ^u and their MAC will be close to 1
(in a statistical sense). Using (11.64) and noting
^uT ^u ¼ 1 and
^uTai ¼ 0
i ¼ 2; . . .; n
ð
Þ,
MAC between
u and ^u
¼
^uTu
jj^ujj jjujj ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ Pn
i¼2 Z2
i d2
i
q
ð11:65Þ
This is a random variable whose expectation, denoted by q, is bounded above by 1.
It can be reasoned that q ¼ 1 if and only if all di s are zero. Analytical expression
for q is not available but it can be shown that asymptotically (Shi et al. 2010)
q 
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ d2
u
q
n ! 1 or all di ! 0
ð11:66Þ
where
d2
u ¼
X
n
i¼2
d2
i
ð11:67Þ
is the sum of the eigenvalues of ^Cu (note that d1 ¼ 0). Typically, q is close to 1.
One can interpret q ¼ cos a where a is a hyper-angle reﬂecting the identiﬁcation
uncertainty of u. Note that cos a  1  a2=2 for small a. From (11.66), q  1 
d2
u=2 for small du and so a  du. Thus, du can be used as a measure of mode shape
uncertainty, the lower the better. Being the square root sum of eigenvalues of ^Cu, it
can be interpreted as the ‘mode shape c.o.v.’, analogous to the c.o.v. of a scalar
parameter:
Mode shape c:o:v: ¼ du ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Sum of eigenvalues of
mode shape covariance matrix
r
ð11:68Þ
Reference
Shi X, Wu Y, Liu Y (2010) A note on asymptotic approximations of inverse moments of
nonnegative random variables. Stat Probab Lett 80:1260–1264
362
11
Bayesian OMA Computation

Part III
Algorithms

Chapter 12
Single Mode Problem
Abstract This chapter develops the computational algorithm for determining the
most probable value (MPV) and covariance matrix of modal parameters in
Bayesian operational modal analysis with data in a single setup and for
well-separated modes. The MPV of the mode shape is analytically derived in terms
of other parameters, which effectively reduces the numerical optimization problem
for MPV to involve only four parameters independent of the number of measured
degrees of freedom. The asymptotic behavior of MPV for modes with high
signal-to-noise ratio is analyzed. Analytical formulas are derived for systematically
computing the covariance matrix of parameters given the measured data. Examples
with synthetic data, laboratory data and ﬁeld data are presented to illustrate the
algorithms and their applications.
Keywords Single mode  Tall building  Signal-to-noise ratio
In this chapter we discuss efﬁcient methods for calculating the posterior most
probable values (MPV) and covariance matrix of modal parameters in Bayesian
operational modal analysis (OMA) with data collected in a single setup. The mode
of interest is assumed to be well-separated from others so that it dominates the
selected frequency band. The general case of multiple modes is considered in
Chap. 13. Similar techniques in this chapter are used for developing algorithms for
well-separated modes from multi-setup data in Chap. 14.
Mathematically, the posterior MPV minimizes the negative log-likelihood
function (NLLF) and so its determination involves an optimization problem. The
algorithm for MPV makes use of the fact that the NLLF can be written as a
quadratic function of mode shape. This allows the most probable mode shape to be
determined analytically in terms of the remaining parameters. The dimension of the
optimization problem is then reduced to four only, regardless of the number of
measured DOFs. A good initial guess can be obtained by considering the asymp-
totic behavior of the MPV when the signal-to-noise ratio of data is high.
The algorithm presented in this chapter is based on Au (2011); see also Zhang
and Au (2013). The presentation has been simpliﬁed and reorganized. Computer
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_12
365

programing of the posterior covariance matrix using the analytical formulas in
Sect. 12.4 is simpler as it has taken advantage of a more systematic procedure based
on Lagrange multiplier concepts (Sect. 11.2.3).
Recall the problem context from Sect. 10.1 where the measured ambient
vibration history is denoted by f^yjgN1
j¼0
n  1
ð
Þ; n is the number of measured DOFs
and N is the number of samples per data channel. The scaled Fast Fourier
Transform (FFT) of f^yjgN1
j¼0 at frequency fk ¼ k=NDt (Hz) is
^F k ¼
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
j¼0
^yje2pijk=N
ð12:1Þ
where Dt (s) is the sampling interval. The set of modal parameters h comprises
f
Natural
frequencyðHzÞ
f
Damping
ratio
S
Modal force
PSD
Se
Prediction
error PSD
u
n1
Mode shape
The mode shape is subjected to unit norm constraint, uTu ¼ 1: The modal force
PSD has the same unit as the PSD of acceleration. The prediction error PSD has the
same unit as the PSD of data.
12.1
Alternative Form of NLLF
The NLLF can be rewritten in a form that facilitates the determination of the most
probable mode shape. Recall the NLLF from Sect. 10.1:
LðhÞ ¼ nNf ln p þ
X
k
ln jEkðhÞj þ
X
k
^F 
kEkðhÞ1 ^F k
ð12:2Þ
where EkðhÞ is the theoretical PSD matrix of data for given h: Assuming a single
mode in the selected frequency band,
Ek ¼ SDkuuT þ SeIn
ð12:3Þ
Dk ¼
ð2pfkÞ2q
ð1  b2
kÞ2 þ ð2fbkÞ2
bk ¼ f
fk
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð12:4Þ
where In denotes the n  n identity matrix. The key is to express jEkj and E1
k
in
explicit form using the eigenvector representation of Ek. Let fbign
i¼1 (n  1Þ be an
orthonormal basis with b1 ¼ u. Substituting In ¼ Pn
i¼1 bibT
i into (12.3) and col-
lecting terms,
366
12
Single Mode Problem

Ek ¼ ðSDk þ SeÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
eigenvalue
uuT þ
X
n
i¼2
Se
|{z}
eigenvalue
bibT
i
ð12:5Þ
This shows that Ek has eigenvalues fSDk þ Se; Se; . . .; Seg and eigenvectors
fu; b2; . . .; bng. The determinant of Ek is equal to the product of its eigenvalues:
jEkj ¼ ðSDk þ SeÞSn1
e
ð12:6Þ
Replacing the eigenvalues in (12.5) by their reciprocals gives E1
k :
E1
k
¼ ðSDk þ SeÞ1uuT þ
X
n
i¼2
S1
e bibT
i
ð12:7Þ
Substituting Pn
i¼2 bibT
i ¼ In  uuT and rearranging gives
E1
k
¼ S1
e
In 
1 þ Se
SDk

1
uuT
"
#
ð12:8Þ
Substituting (12.6) and (12.8) into (12.2) and noting
^F 
k
1n
u
n1
|ﬄﬄﬄ{zﬄﬄﬄ}
scalar
uT
1n
^F k
n1
|ﬄﬄﬄ{zﬄﬄﬄ}
scalar
¼ ðuT ^F kÞ
scalar
ð ^F

kuÞ
scalar
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
real scalar
¼ uT
1n
Reð ^F k ^F

kÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
nn real sym:
u
n1
ð12:9Þ
gives
L ¼ nNf ln p þ
X
k
lnðSDk þ SeÞ þ ðn  1ÞNf ln Se þ S1
e d  S1
e uTAu
ð12:10Þ
where
A ¼
X
k
ð1 þ Se
SDk
Þ1 ^Dk
ð12:11Þ
^Dk ¼ Reð ^F k ^F 
kÞ
d ¼
X
k
^F 
k ^F k
ð12:12Þ
In deriving (12.10), we have assumed uTu ¼ 1. When using this form of the
NLLF for determining the MPV by optimization, the mode shape substituted in the
expression should always have unit norm.
12.1
Alternative Form of NLLF
367

12.2
Algorithm for MPV
In (12.10), only the term uTAu depends on u. The most probable mode shape,
denoted by ^u, should therefore maximize the term subjected to uTu ¼ 1. As A is
positive deﬁnite, it follows that ^u is the eigenvector of A with the largest eigenvalue
(Section C.4). Evaluating L at u ¼ ^u, the MPV of ff ; f; S; Seg can be obtained by
minimizing
~Lðf ; f; S; SeÞ ¼ nNf ln p þ
X
k
lnðSDk þ SeÞ þ ðn  1ÞNf ln Se þ S1
e ðd  ^kÞ
ð12:13Þ
where ^k ¼ ^uTA^u is the largest eigenvalue of A: Note that ^k depends on ff ; f; S; Seg
but the dependence has been omitted for notational simplicity.
Below are two algorithms for calculating the MPV of h ¼ ff ; f; S; Se; ug. The
ﬁrst one follows directly from the considerations above. The second one is iterative.
It is a special case of the multi-mode algorithm in Chap. 13.
Algorithm 1
0. Set initial guess for ff ; f; S; Seg. See Sect. 12.3.1.
1. Determine the MPV of ff ; f; S; Seg by minimizing ~L in (12.13).
2. Determine the MPV of u as the eigenvector (largest eigenvalue) of A in
(12.11) at the MPV of ff ; f; S; Seg.
Algorithm 2
The following is an iterative algorithm. At any one step, only the mentioned
parameters are updated; the remaining ones are kept at their current value.
0. Set initial guess for ff ; f; S; Se; ug. See Sect. 12.3.1.
1. Update ff ; f; S; Seg by minimizing L in (12.10) w.r.t. ff ; f; S; Seg.
2. Update u as the eigenvector (largest eigenvalue) of A in (12.11).
Repeat Steps 1 and 2 until convergence.
12.3
High s/n Asymptotics of MPV
The MPV of modal parameters exhibits characteristic behavior when the
signal-to-noise (s/n) ratio of data is high, in the sense that
368
12
Single Mode Problem

ck ¼ SDk
Se
 1
ð12:14Þ
In this case ð1 þ Se=SDkÞ  1. Substituting into (12.11),
A  A0 ¼
X
k
^Dk ¼ Re
X
k
^F k ^F

k
ð12:15Þ
The most probable mode shape is therefore asymptotically given by the eigenvector
(largest eigenvalue) of A0, which can be directly computed from data without
iterations.
Replacing ^k in the expression of ~L in (12.13) by the largest eigenvalue of A0
results in an expression that does not yield a reasonable value for the MPV of S or
Se, suggesting that the leading order w.r.t. these parameters has been lost in the
approximation. To obtain the correct asymptotic form, the ﬁrst order of A should be
retained. Substituting ð1 þ Se=SDkÞ1  1  Se=SDk into (12.11) gives
A 
X
k
^Dk  Se
S
X
k
D1
k ^Dk
ð12:16Þ
For the summand in (12.13),
lnðSDk þ SeÞ ¼ ln SDkð1 þ Se
SDk
Þ


¼ ln SDk þ lnð1 þ Se
SDk
Þ  ln S þ ln Dk þ Se
SDk
ð12:17Þ
Substituting (12.16) and (12.17) into (12.13) gives
~Lðf ; f; S; SeÞ  nNf ln p þ
X
k
ln Dk þ Se
S
X
k
D1
k
þ ðn  1ÞNf ln Se þ S1
e ðd  ^k0Þ
h
i
þ Nf ln S þ S1 X
k
^dk
Dk
"
#
ð12:18Þ
^k0 ¼ ^uTA0^u
^dk ¼ ^uT ^Dk ^u
ð12:19Þ
The variation of SeS1 P
k D1
k
w.r.t. Se or S is asymptotically small compared to
the bracketed terms and so ~L depends on Se dominantly through the ﬁrst bracket and
on S through the second bracket. Minimizing the ﬁrst and second bracket gives the
MPV of Se and S, respectively:
12.3
High s/n Asymptotics of MPV
369

^Se 
d  ^k0
ðn  1ÞNf
^S  1
Nf
X
k
^dk
Dk
ð12:20Þ
12.3.1
Initial Guess of MPV
The initial guess of MPV for numerical optimization can be set as follow. The
initial guess for f can be picked from the singular value (SV) spectrum of data
(Sect. 7.3). The initial guess for f can be set as 1%. The initial guess for S and Se
can be calculated using (12.20). The initial guess for u can be taken as the
eigenvector (largest eigenvalue) of A0 in (12.15).
12.4
Posterior Covariance Matrix
Under a Gaussian approximation of the posterior PDF, the posterior covariance
matrix of the set of modal parameters h nh  1
ð
Þ satisfying constraint (on mode
shape norm) is given by (Sect. 11.2)
^C
nhnh ¼ ðr^vcÞ
nhp
ðr2^LcÞ þ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
pp
ðr^vcÞ
pnh
T
ð12:21Þ
where ‘+’ denotes the pseudo-inverse, i.e., ignoring the zero eigenvalues from
constraint singularity; r2^Lc is the Hessian of LcðuÞ ¼ LðvcðuÞÞ w.r.t. a set of free
parameters u
p  1
ð
Þ at the MPV; vcðuÞ maps u to h that always satisﬁes the
constraint GðvcðuÞÞ ¼ 0; GðhÞ is the norm constraint equation on mode shape; r^vc
is the gradient of vcðuÞ at the MPV. A hat ‘^’ on quantity denotes that it is evaluated
at the MPV. Using the formula based on Lagrange multiplier (Sect. 11.2.3),
r2^Lc
pp ¼ ðr^vcÞT
pnh
ðr2^L
nhnh þ ^k1 r2 ^G
nhnhÞ ðr^vcÞ
nhp
ð12:22Þ
where r2^L is the Hessian of the NLLF in (12.10) (ignoring mode shape norm
constraint) and
^k1 ¼  ðr^GÞðr^LÞT
ðr^GÞðr^GÞT
ð12:23Þ
is the value of the Lagrange multiplier at the MPV.
370
12
Single Mode Problem

The terms in (12.22) can be determined as follow. Let - ¼ ½f ; f; S; SeT comprise
the modal parameters other than mode shape. One can take
u ¼
-
u


h ¼
-
u


vcðuÞ ¼
-
u=jjujj


GðhÞ ¼ 1  uTu
ð12:24Þ
so that GðvcðuÞÞ 	 0 for any u. Direct differentiation and evaluating at the MPV
gives
r^G ¼ ½ 014
2^uT 
r2 ^G ¼
044
2In


ð12:25Þ
r^L ¼ ½ r-^L
2S1
e ^uT ^A 
r^vc ¼
I44
In  ^u^uT


ð12:26Þ
where r- denotes the gradient w.r.t. -; ^A denotes the value of A at the MPV.
Substituting into (12.23) gives
^k1 ¼ S1
e ^k
ð12:27Þ
where ^k is the largest eigenvalue of ^A: Due to mode shape norm constraint, r^Lc has
one zero eigenvalue with eigenvector ½041; ^u:
It remains to determine r2^L; which can be obtained via ﬁnite difference
approximation or analytical expression. The latter is discussed in the next two
sections. Section 12.4.1 expresses the derivatives of L in terms of those of Ek. It has
the advantage of unifying derivation and computer coding effort with other for-
mulations. Section 12.4.2 presents alternative expressions based on a condensed
form of the NLLF. It can speed up computation especially when n is large, but
requires additional coding effort speciﬁc to the present context.
12.4.1
General Expressions
Let a superscripted variable in parenthesis denote a derivative w.r.t. the variable.
For any parameters x and y,
LðxÞ ¼
X
k
ðlnjEkjÞðxÞ þ
X
k
^F

kðE1
k ÞðxÞ ^F k
LðxyÞ ¼
X
k
ðlnjEkjÞðxyÞ þ
X
k
^F

kðE1
k ÞðxyÞ ^F k
ð12:28Þ
The derivatives of ln jEkj and E1
k
can be expressed in terms of the derivatives of
Ek. For any x and y;
12.4
Posterior Covariance Matrix
371

ðln jEkjÞðxÞ ¼ tr½E1
k EðxÞ
k 
ðE1
k ÞðxÞ ¼ E1
k EðxÞ
k E1
k
ðln jEkjÞðxyÞ ¼ tr½E1
k EðxyÞ
k
 E1
k EðxÞ
k E1
k EðyÞ
k 
ðE1
k ÞðxyÞ ¼ E1
k ½EðxÞ
k E1
k EðyÞ
k þ EðyÞ
k E1
k EðxÞ
k
 EðxyÞ
k
E1
k
ð12:29Þ
The derivatives of Ek are given in Table 12.1. The expressions involve the
derivatives of Dk, which are given in Table 12.2. For better conditioning, E1
k
should be calculated using the formula below (recalled from (12.8)), where the
ill-conditioning due to small Se has been segregated out in the factor S1
e :
E1
k
¼ S1
e
In 
1 þ Se
SDk

1
uuT
"
#
ð12:30Þ
12.4.2
Condensed Expressions
Computing the derivatives of the NLLF via those of Ek involves matrix compu-
tations of size n (the number of DOFs) which can be large in applications. It is
Table 12.1 Derivatives of Ek ¼ SDkuuT þ SeIn
EðxyÞ
k
y ¼
f ; f
S
Se
/r
x ¼
f ; f
SDðxyÞ
k
uuT
DðxÞ
k uuT
0n
SDðxÞ
k ½eruT þ ueT
r 
S
0n
0n
Dk½eruT þ ueT
r 
Se
0n
0n
/i
Sym.
SDk½eir þ eT
ir
EðyÞ
k
SDðyÞ
k uuT
DkuuT
In
SDk½eruT þ ueT
r 
See Table 12.2. for the derivatives of Dk; u ¼ ½/1; . . .; /nT; er is a n  1 vector with the r th entry
equal to 1 (zero elsewhere); eir is a n  n matrix with the ði; rÞ-entry equal to 1 (zero elsewhere)
Table 12.2 Derivatives of D1
k
¼ ½ð1  b2
kÞ2 þ ð2fbkÞ2ð2pfkÞ2q and Dk
DðxÞ
k
D2
kðD1
k ÞðxÞ for any x
DðxyÞ
k
2D3
kðD1
k ÞðxÞðD1
k ÞðyÞ  D2
kðD1
k ÞðxyÞ for any x; y
ðD1
k ÞðxyÞ
y ¼
f
f
x ¼
f
4f2
k ð3b2
k  1 þ 2f2Þð2pfkÞ2q
16f1
k fbkð2pfkÞ2q
f
Sym.
8b2
kð2pfkÞ2q
ðD1
k ÞðyÞ
4f1
k ½b3
k  bkð1  2f2Þð2pfkÞ2q
8fb2
kð2pfkÞ2q
bk ¼ f =fk; q ¼ 0; 1; 2 for acceleration, velocity and displacement data
372
12
Single Mode Problem

possible to obtain the derivatives in a condensed form using (12.10) to speed up
computations. Let - ¼ ½f ; f; S; SeT and r2L be partitioned as
r2L ¼
Lð--Þ
44
Lð-uÞ
4n
sym:
LðuuÞ
nn
2
4
3
5
ð12:31Þ
The partition Lð-uÞ involves LðxuÞ for x ¼ f ; f; S; Se. These are given in Table 12.3.
The partition Lð--Þ involves LðxyÞ for x; y ¼ f ; f; S; Se. To facilitate presentation, the
NLLF in (12.10) is separated into two parts:
L ¼ LD þ LQ
ð12:32Þ
LD ¼ nNf ln p þ
X
k
lnðSDk þ SeÞ þ ðn  1ÞNf ln Se
ð12:33Þ
LQ ¼ S1
e ðd  uTAuÞ ¼ S1
e ðd 
X
k
BkqkÞ
ð12:34Þ
Bk ¼ ð1 þ Se
SDk
Þ1
qk ¼ uT ^Dku
ð12:35Þ
Note that Bk only depends on ff ; f; S; Seg and qk only on u: The second derivatives
of LD and LQ are given in Tables 12.4 and 12.5, respectively. They involve the
derivatives of Dk and Bk, which are given in Tables 12.2 and 12.6, respectively.
Figure 12.1 shows the information ﬂow. The quantities Dk, Bk, ck ¼ SDk=Se and
qk ¼ uT ^Dku appear repeatedly and so they may be computed and stored upfront.
The expressions in Tables 12.3, 12.4, 12.5 and 12.6 are applicable for general
parameter values, not necessarily at the MPV.
Table 12.3 Derivatives LðxuÞ for x ¼ f ; f; S; Se
x ¼
f ; f; S
Se
LðxuÞ
2uTS1
e
P
k BðxÞ
k ^Dk
2uTS1
e
P
k BðSeÞ
k
^Dk þ 2S2
e uTA
See Table 12.6 for derivatives of Bk
Table 12.4 Derivatives of LD ¼ nNf ln p þ P
k lnðSDk þ SeÞ þ ðn  1ÞNf ln Se
LðxyÞ
D
y ¼
f; f
S
Se
x ¼
f ; f
P
k½BkD1
k DðxyÞ
k
 B2
kD2
k DðxÞ
k DðyÞ
k 
S1 P
k c1
k B2
kD1
k DðxÞ
k
S1
e
P
k c1
k B2
kD1
k DðxÞ
k
S
S2 P
k B2
k
S1S1
e
P
k c1
k B2
k
Se
Sym.
S2
e Nf ðn  1Þ
S2
e
X
k c2
k B2
k
See Table 12.2 for derivatives of Dk
12.4
Posterior Covariance Matrix
373

12.5
Synthetic Data Examples
In this section, we present examples on Bayesian OMA with well-separated modes
based on synthetic data. Example 12.1 provides a basic illustration of Bayesian
modal identiﬁcation results and their interpretation. Examples 12.2 and 12.3
illustrate the effect of modeling error and the choice of bandwidth, respectively.
Example 12.1 (Ten-storied building) Consider the horizontal vibration of a
ten-storied shear building with uniform mass 1000 tons per ﬂoor, interstory stiffness
1767 kN/mm and damping ratio 1% in all modes. The ﬁrst three modes have natural
frequencies 1, 2.98 and 4.89 Hz. The structure is subjected to i.i.d. white noise
excitation at all ﬂoors, each with PSD Sw ¼ 96:2N2=Hz: This structure was
Table 12.5 Derivatives of LQ ¼ S1
e ðd  uTAuÞ
LðxyÞ
Q
y ¼
f ; f; S
Se
x ¼
f ; f; S
S1
e
P
k BðxyÞ
k
qk
S2
e
P
k BðxÞ
k qk  S1
e
P
k BðxSeÞ
k
qk
Se
Sym.
2S3
e ðd  P
k BkqkÞ þ 2S2
e
P
k BðSeÞ
k
qk  S1
e
P
k BðSeSeÞ
k
qk
See Table 12.6 for derivatives of Bk
Table 12.6 Derivatives of B1
k
¼ ð1 þ Se=SDkÞ and Bk
BðxÞ
k
B2
kðB1
k ÞðxÞ for any x
BðxyÞ
k
2B3
kðB1
k ÞðxÞðB1
k ÞðyÞ  B2
kðB1
k ÞðxyÞ for any x; y
ðB1
k ÞðxyÞ
y ¼
f ; f
S
Se
x ¼
f ; f
c1
k DkðD1
k ÞðxyÞ
S1c1
k D1
k DðxÞ
k
c1
k S1
e D1
k DðxÞ
k
S
2c1
k S2
c1
k S1
e S1
Se
Sym.
0
ðB1
k ÞðyÞ
c1
k D1
k DðyÞ
k
c1
k S1
c1
k S1
e
See Table 12.2 for derivatives of Dk
k
Table 12.2
D
L
Table 12.4
k
B
Table 12.6
Q
L
Table 12.5
)
(x
L
Table 12.3
D
Fig. 12.1 Information ﬂow
of derivative calculations
374
12
Single Mode Problem

considered in Example 10.1. The acceleration on 5/F and the roof are measured for
600 s at 100 Hz. They are contaminated by i.i.d. data noise of PSD Se ¼
1ðlgÞ2=Hz: Figure 12.2 shows the root PSD and SV spectrum of data. The fre-
quency band used for modal identiﬁcation is indicated by the horizontal bar below
each peak.
Posterior Statistics
Table 12.7 summarizes the modal identiﬁcation results. The exact value of modal
force PSD of the ﬁrst mode was calculated in Example 10.1; the values for other
0
1
2
3
4
5
6
10
-7
10
-6
10
-5
10
-4
[ ]
[
]
[
]
[g/ Hz]
(a) Root PSD
0
1
2
3
4
5
6
10
-7
10
-6
10
-5
10
-4
[ ]
[
]
[
]
Frequency [Hz]
Frequency [Hz]
[g/ Hz]
(b) Root SV
Fig. 12.2 a Root PSD and b root SV spectrum using data on 5/F and the roof, ten-storied building
(synthetic data). Horizontal bar indicates selected frequency band
Table 12.7 Modal identiﬁcation results, ten-storied building (synthetic data)
Mode 1
Mode 2
Mode 3
Modal s/n ratio c
643
1013
297
Frequency f
Exact (Hz)
1.000
2.978
4.889
MPV (Hz)
1.003
2.977
4.891
c.o.v. (%)
0.190
0.092
0.091
Damping f
Exact (%)
1
1
1
MPV (%)
1.04
0.78
1.13
c.o.v. (%)
21
13
10
Modal force PSD S
Exact
[ðlgÞ2=Hz
0.28
0.30
0.23
MPV [ðlgÞ2=Hz
0.30
0.33
0.25
c.o.v. (%)
16
9
8
Prediction error PSD Se
Exact
[ðlgÞ2=Hz
1
1
1
MPV [ðlgÞ2=Hz
1.1
1.3
1.6
c.o.v. (%)
12
7
5
Mode shape u
c.o.v. (%)
0.67
0.35
0.43
Angle between exact and MPV mode shape
(%)
0.55
0.31
0.25
12.5
Synthetic Data Examples
375

modes can be calculated similarly. The MPVs are close to the exact values in a
manner consistent with the posterior c.o.v.s (coefﬁcient of variation = standard
deviation/MPV). The exact values are generally within two standard deviations
from the MPVs. The MPV of the prediction error PSD appears to be biased high as
the mode number increases. This is because the frequency band of the higher modes
contains unmodeled pseudo-static contribution from the lower modes.
The most probable mode shapes (omitted here) are very close to the exact mode
shapes. In Table 12.7, the mode shape c.o.v. is equal to the square root sum of
eigenvalues of the mode shape covariance matrix (Sect. 11.3.3). It is of the same
order of magnitude as the hyper-angle between the exact and most probable mode
shape.
Table 12.8 shows the posterior correlation coefﬁcients among f , f, S and Se.
Correlation is generally small, with the only exception between f and S, of about
66%. This is no coincidence and can be explained based on uncertainty laws
(Chap. 15).
Posterior marginal PDF
Figure 12.3 shows the posterior marginal PDFs of modal parameters. The exact
marginal PDF (solid line) was obtained by numerically integrating the likelihood
0.995
1
1.005 1.01
0
100
200
Mode 1
0
1
2
0
1
2
0.1 0.2 0.3 0.4 0.5
0
5
0.5
1
1.5
0
2
2.97
2.98
2.99
0
100
Mode 2
0.4 0.6 0.8
1
1.2
0
2
4
0.2
0.3
0.4
0
10
1
1.2 1.4 1.6
0
5
4.87 4.88 4.89 4.9 4.91
0
50
100
Mode 3
f [Hz]
0.6 0.8
1 1.2 1.4 1.6
0
2
4
 [%]
0.2
0.25
0.3
0
10
20
S [( g)2/Hz]
1.2
1.4
1.6
1.8
2
0
5
Se [( g)2/Hz]
Fig. 12.3 Posterior marginal PDF of modal parameters. Solid line exact by numerical integration;
dashed line Gaussian approximation
Table 12.8 Posterior
correlation coefﬁcient (%),
ten-storied building (synthetic
data)
f
f
S
Se
f
100
2.1
4.8
0.29
f
100
66
−1.65
S
100
−2.94
Se
Sym.
100
376
12
Single Mode Problem

function w.r.t. the remaining parameters and normalizing so that its area is 1. The
approximate PDF (dashed line) is a Gaussian PDF centered at the MPV and with a
standard deviation obtained from the posterior covariance matrix. Obtaining the
exact marginal PDF is computationally expensive. It is only done here for com-
parison. The ﬁgure shows that the Gaussian approximation is generally good.
Effect of Data Duration
Modal identiﬁcation results depend on data, and clearly its duration. Figure 12.4
shows the results for data durations of 600, 1200, 2400 and 4800 s. The MPV
(dot) need not coincide with the exact value but the latter is generally within
the ± two standard deviation error bar. As data duration increases, the MPV
converges (in a random manner) to the exact value and the error bar shortens. In
real applications the exact value (dashed line) is unknown (or does not even exist)
and so identiﬁcation precision is assessed primarily based on the error bars.
Effect of Signal-to-Noise Ratio
Identiﬁcation uncertainty depends on data noise intensity. Figure 12.5 shows the
posterior c.o.v. of modal parameters of the ﬁrst mode when the data is contaminated
with measurement noise of PSD Se, ranging from 100 ðlgÞ2=Hz to 0.01 ðlgÞ2=Hz:
The results are plotted w.r.t. the ‘modal s/n ratio’ (Sect. 7.7.3)
c ¼
S
4Sef2
ð12:36Þ
calculated using the MPVs determined in each case. For reference, in the nominal
case, Se ¼ 1 ðlgÞ2=Hz and c 
 700: The ﬁgure shows that the posterior c.o.v.
10
2
10
3
10
4
1
1.005
1.01
f [Hz]
10
2
10
3
10
4
0.5
1
1.5
 [%]
10
2
10
3
10
4
0.2
0.3
0.4
S [( g)2/Hz]
Data duration (sec)
10
2
10
3
10
4
0.6
0.8
1
1.2
1.4
Se [( g)2/Hz]
Fig. 12.4 Modal identiﬁcation results for different data durations. Dot MPV; error bar ±two
standard deviations. Dashed line exact value
12.5
Synthetic Data Examples
377

decreases signiﬁcantly with c when c is small (say, <100). When c is large the
posterior c.o.v. settles at a non-zero value. Intuitively, even when there is no
measurement noise, there is still uncertainty in the modal parameters because the
input excitation is unknown.
■
Example 12.2 (Modeling error from unmodeled modes) Consider synthetic data
generated at 100 Hz for 1200 s according to
^yj ¼ u1€g1ðtjÞ þ u2€g2ðtjÞ þ eðtjÞ
ð12:37Þ
where €giðtÞ (i ¼ 1; 2Þ is the modal acceleration satisfying
€giðtÞ þ 2fixi _giðtÞ þ x2
i giðtÞ ¼ piðtÞ
ð12:38Þ
with xi ¼ 2pfi (rad/s); f1 ¼ 1 Hz, f2 ¼ 1.5 Hz, f1 ¼ f2 ¼ 1%;
0
0.1
0.2
0.3
0.4
c.o.v. f [%]
0
10
20
30
40
50
c.o.v.  [%]
0
10
20
30
40
50
c.o.v. S [%]
10
0
10
1
10
2
10
3
10
4
10
5
10
0
10
1
10
2
10
3
10
4
10
5
10
0
10
1
10
2
10
3
10
4
10
5
10
0
10
1
10
2
10
3
10
4
10
5
10
0
10
1
10
2
10
3
10
4
10
5
0
5
10
15
20
c.o.v. Se [%]
10
-2
10
-1
10
0
10
1
c.o.v.  [%]
Modal s/n ratio  = S/4Se
2
Fig. 12.5 Posterior c.o.v. of modal parameters (ﬁrst mode) for different Se
378
12
Single Mode Problem

u1 ¼ 1
2
2
½
T=3
u2 ¼ 2
1
2
½
T=3
ð12:39Þ
and
piðtÞ
is
the
modal
excitation,
stationary
Gaussian
with
PSD
Si;
S1 ¼ 1ðlgÞ2=Hz; S2 will be speciﬁed later. The prediction error eðtÞ is i.i.d.
Gaussian white noise with PSD Se ¼ 1ðlgÞ2=Hz. The mode at 1 Hz is identiﬁed
using FFT in the band [0.95, 1.00] Hz and assuming single mode. Here we
investigate the effect of modeling error due to the unmodeled mode at 1.5 Hz.
When S2 ¼ 0 the second mode is absent and there is no modeling error.
The PSD and SV spectrum of data in this case are shown in Fig. 12.6. The lowest
two lines in (b) reﬂect data noise PSD. When S2 [ 0, the contribution of the second
mode in the selected band induces modeling error. Figure 12.7 shows the root PSD
and SV spectrum when S2 ¼ 300ðlgÞ2=Hz. As seen in (b), the presence of the
second mode increases the second largest singular value. This appears like an
increase in the noise PSD in the selected band.
Figure 12.8 shows the modal identiﬁcation results for different values of S2 up to
300ðlgÞ2=Hz. The results are plotted w.r.t. the ‘modeling error ratio’, deﬁned as the
PSD ratio of the second to the ﬁrst mode at 1 Hz. When the ratio is 105 the results
essentially correspond to S2 ¼ 0. Even in this case the MPV need not be equal to
the exact value because the data duration is ﬁnite. Departure of identiﬁcation results
from this nominal (‘model error free’) case reﬂects modeling error. For ff1; f1; S1g,
there is a slight departure in MPV from the nominal value but it is still small
compared to identiﬁcation uncertainty (error bar). For Se, when the modeling error
ratio is small, (say \103), the identiﬁcation result reﬂects the PSD of data noise
ð1ðlgÞ2=HzÞ. For larger ratios it reﬂects the PSD contribution of the second mode.
In all cases the identiﬁed mode shapes (not shown) are close to the exact ones, in
the worst case with a c.o.v. of about 10%.
■
Example 12.3 (Bandwidth trade-off) Consider Example 12.2 again with the lar-
gest modeling error ratio, i.e., S2 ¼ 300ðlgÞ2=Hz. It was demonstrated that the
0
0.2 0.4 0.6 0.8 1
1.2 1.4 1.6 1.8
2
10
-1
10
0
10
1
10
2
10
3
10
-1
10
0
10
1
10
2
10
3
[
]
Frequency [Hz]
[ g/ Hz]
(a) Root PSD
0
0.2 0.4 0.6 0.8 1
1.2 1.4 1.6 1.8
2
[
]
Frequency [Hz]
[ g/ Hz]
(b) Root SV
Fig. 12.6 a Root PSD and b root SV spectrum when S2 ¼ 0
12.5
Synthetic Data Examples
379

effect of the unmodeled second mode on the identiﬁcation results of ff1; f1; S1g was
insigniﬁcant. This example further illustrates that this in fact depends on the
selected bandwidth, which in general should play a balance between identiﬁcation
precision (the wider the better) and modeling error (the narrower the better).
Figure 12.9 shows the modal identiﬁcation results for different selected bands,
parameterized as ð1  0:01jÞ where j is a ‘bandwidth factor’. For reference, the
selected band in the last example was [0.95, 1.05], i.e., j ¼ 5. As seen in the ﬁgure,
as j increases from small values, identiﬁcation uncertainty (error bar) decreases,
reﬂecting the positive effect of increasing FFT data size. Identiﬁcation uncertainty is
0
0.2 0.4 0.6 0.8
1
1.2 1.4 1.6 1.8
2
[
]
Frequency [Hz]
[ g/ Hz]
(a) Root PSD
0
0.2 0.4 0.6 0.8 1
1.2 1.4 1.6 1.8
2
[
]
Frequency [Hz]
[ g/ Hz]
(b) Root SV
10
-1
10
0
10
1
10
2
10
3
10
-1
10
0
10
1
10
2
10
3
Fig. 12.7 a Root PSD and b root SV spectrum when S2 ¼ 300ðlgÞ2=Hz
0.994
0.996
0.998
1
1.002
1.004
f1 [Hz]
0.4
0.6
0.8
1
1.2
1.4
1 [%]
0.4
0.6
0.8
1
1.2
1.4
S1 ( g)2/Hz
Modeling error ratio
10
-5
10
-4
10
-3
10
-2
10
-1
10
0
10
-5
10
-4
10
-3
10
-2
10
-1
10
0
10
-5
10
-4
10
-3
10
-2
10
-1
10
0
10
-5
10
-4
10
-3
10
-2
10
-1
10
0
10
-1
10
0
10
1
10
2
Se ( g)2/Hz
Fig. 12.8 Modal identiﬁcation results for different values of S2, plotted versus modeling error
ratio, deﬁned as the PSD ratio of the second to the ﬁrst mode at 1 Hz. Dot MPV; error bar ±two
standard deviations
380
12
Single Mode Problem

insensitive to j when 5\j\10, where it is trading off with modeling error. Bias is
not signiﬁcant until j [ 15, as reﬂected by the error bar not covering the exact
value (dashed line). Thus, when the selected band is narrow, modeling error is small
but identiﬁcation uncertainty could be unacceptably large. When the band is too
wide, results can be signiﬁcantly biased due to modeling error. The band should
trade off these two factors. Here a value between 5 and 10 is appropriate.
■
12.6
Laboratory/Field Data Examples
In this section we present examples on Bayesian OMA with well-separated modes
based on experimental data measured under laboratory or ﬁeld environment. They
illustrate the typical results one may obtain in reality and how to interpret them,
taking into account potential modeling errors.
Example 12.4 (Laboratory shear frame) Consider the three-storied laboratory
shear frame in Fig. 12.10. The four corners of each ﬂoor are instrumented with two
piezoelectric accelerometers along the horizontal x- and y-directions. This gives a
total of 3  4  2 ¼ 24 measured DOFs. Ambient data was recorded for 600 s at
2048 Hz. It was later decimated to 512 Hz for analysis.
Data and spectra
Figure 12.11 shows the measured time histories at DOFs 1 and 2. Figure 12.12
shows the root PSD and SV spectrum. The spectra indicate a noise PSD in the order
0
5
10
15
20
0.99
1
1.01
1.02
1.03
1.04
f1 [Hz]
0
5
10
15
20
0
2
4
6
1 [%]
0
5
10
15
20
0
2
4
6
8
S1 ( g)2/Hz
Bandwidth factor 
0
5
10
15
20
40
60
80
100
120
Se ( g)2/Hz
Fig. 12.9 Modal identiﬁcation results with different selected bands ð1  0:01jÞ
12.5
Synthetic Data Examples
381

of 100 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
to 20 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
from low to high frequencies. The peaks of the top
line in (b) indicate potential modes. Ten well-separated modes can be easily
recognized.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
1/F
2/F
3/F
(a) Photo
(b) Measured DOFs
858 mm
y (strong dir.)
x (weak dir.)
571 mm
555 mm
Fig. 12.10 Laboratory shear frame; a photo; b measured DOFs
0
100
200
300
400
500
600
-0.01
-0.005
0
0.005
0.01
DOF 1 [g]
0
100
200
300
400
500
600
-0.01
-0.005
0
0.005
0.01
DOF 2 [g]
Time [sec]
Fig. 12.11 Measured time histories at DOFs 1 and 2, laboratory frame (detrended)
382
12
Single Mode Problem

Modal Identiﬁcation Using DOFs 1 and 2
The measured data at DOFs 1 and 2 are already sufﬁcient for reliable modal
identiﬁcation. The horizontal bars below the peaks in Fig. 12.12b indicate the bands
selected for modal identiﬁcation. They are hand-picked by zooming in the indi-
vidual bands. The identiﬁcation results are summarized in Table 12.9. The c.o.v. of
frequencies are all below 0.1%. The c.o.v. of damping ratio ranges from a few to
ﬁfty percent. The c.o.v. of modal force PSD is of a similar order as the c.o.v. of
damping ratio. The MPV of prediction error PSD decreases with the mode number,
which is consistent with the spectra in Fig. 12.12. The modal s/n ratio c ¼ S=4Sef2
is deﬁned as before in (12.36), calculated using the MPVs in each case. It is
generally quite high, ranging from a few hundreds to a few thousands.
Modal Identiﬁcation Using All Measured DOFs
Identifying modes using more DOFs allows one to examine their nature through the
mode shapes. The results using all the 24 measured DOFs are summarized in
Table 12.10. The increase in the number of measured DOFs directly increases the
modal force PSD S and modal s/n ratio c ¼ S=4Sef2. Despite the substantial
increase in c, there is little difference in the MPV and c.o.v. of the natural frequency
and damping ratio.
0
5
10
15
20
25
30
35
40
45
50
10
-5
10
-4
10
-3
10
-2
10
-5
10
-4
10
-3
10
-2
[]
[ ] [][ ][ ]
[ ]
[
]
[ ]
[ ]
[
]
[g/ Hz]
0
5
10
15
20
25
30
35
40
45
50
[]
[ ] [][ ][ ]
[ ]
[
]
[ ]
[ ]
[
]
Frequency [Hz]
[g/ Hz]
TX1
TX2
TY1
TX3
R1
TY2
R2
TY3
S1
R3
(b) Root-SV
(a) Root-PSD
Fig. 12.12 One-sided Root PSD a and root SV b spectrum of data at DOFs 1 and 2 of laboratory
frame. Horizontal bar indicates selected frequency band
12.6
Laboratory/Field Data Examples
383

Figure 12.13 shows the identiﬁed mode shapes (MPV) with their nature indi-
cated, e.g., TX2 for the second translational mode along the x direction; R1 for the
ﬁrst torsional mode. Based on lumped mass structural dynamics, the frame theo-
retically has nine modes, comprising three translational modes in the x and the y
direction, and three rotational modes. Figure 12.13 contains an additional mode S1
whose frequency and mode shape lie between R2 and R3.
■
Table 12.9 Modal identiﬁcation results using DOFs 1 and 2, laboratory frame
Mode
s/n ratio
Frequency
Damping
Root modal force
PSD
Root prediction
error PSD
Mode shape
c
f
f
ﬃﬃﬃ
S
p
ﬃﬃﬃﬃﬃ
Se
p
u
MPV
c.o.v.
MPV
c.o.v.
MPV
c.o.v.
MPV
c.o.v.
c.o.v.
(Hz)
(%)
(%)
(%)
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(%)
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(%)
(%)
1
335
0.81
0.10
0.26
47
24
17
250
12
2.07
2
7647
3.52
0.03
0.13
26
15
4
69
4
0.28
3
6173
4.82
0.02
0.06
31
7.3
5
72
5
0.38
4
3293
5.90
0.02
0.12
22
8.5
5
62
4
0.35
5
9496
7.11
0.03
0.16
17
17
3
52
3
0.16
6
650
17.7
0.01
0.12
13
2.1
3
35
2
0.45
7
1100
26.4
0.02
0.25
7
4.7
2
28
1
0.20
8
519
31.5
0.01
0.08
12
1.0
3
28
2
0.46
9
14
35.4
0.01
0.06
19
0.1
10
32
4
3.49
10
1156
45.9
0.01
0.12
8
1.9
2
23
2
0.21
Table 12.10 Modal identiﬁcation results using all 24 measured DOFs, laboratory frame
Mode
s/n ratio
Frequency
Damping
Root modal force
PSD
Root prediction
error PSD
Mode shape
c
f
f
ﬃﬃﬃ
S
p
ﬃﬃﬃﬃﬃ
Se
p
u
MPV
c.o.v.
MPV
c.o.v.
MPV
c.o.v.
MPV
c.o.v.
c.o.v.
(Hz)
(%)
(%)
(%)
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(%)
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(%)
(%)
1
631
0.81
0.10
0.25
47
91
16
731
3
7.33
2
34,960
3.52
0.03
0.12
26
40
4
86
1
0.64
3
68,359
4.82
0.02
0.07
30
32
5
90
1
0.53
4
47,781
5.90
0.02
0.12
22
43
5
84
1
0.44
5
89,022
7.11
0.03
0.16
17
68
3
70
1
0.25
6
8200
17.7
0.01
0.13
12
5.8
3
26
1
0.59
7
13,376
26.4
0.02
0.25
7
12
1
21
0.3
0.27
8
19,308
31.5
0.01
0.08
11
4.8
2
22
0.4
0.36
9
502
35.4
0.01
0.07
14
0.6
5
20
1
2.34
10
48,835
45.9
0.01
0.12
8
8.5
2
16
0.3
0.15
384
12
Single Mode Problem

Example 12.5 (Tall building) Consider a tall building measuring 50 m  50 m in
plan and over 300 m in height. Accelerations along two horizontal (x and y)
directions at four corners on the roof were measured for 1800 s under normal wind
situation in the late afternoon. Figure 12.14 shows the measured time histories at
one corner. The root PSD and SV spectrum are shown in Fig. 12.15. The SV
y
(1) TX1, 0.808Hz, 0.25%
x
z
y
(2) TX2, 3.52Hz, 0.12%
x
z
y
(4) TX3, 5.9Hz, 0.12%
x
z
y
(3) TY1, 4.82Hz, 0.068%
x
z
y
(6) TY2, 17.7Hz, 0.13%
x
z
y
(8) TY3, 31.5Hz, 0.079%
x
z
y
(5) R1, 7.11Hz, 0.16%
x
z
y
(7) R2, 26.4Hz, 0.25%
x
z
y
(10) R3, 45.9Hz, 0.12%
x
z
y
(9) S1, 35.4Hz, 0.071%
x
z
Fig. 12.13 Mode shapes (MPV) identiﬁed using all 24 measured DOFs. Title above each plot
shows the mode number, nature, frequency and damping ratio (MPV)
12.6
Laboratory/Field Data Examples
385

0
200
400
600
800
1000
1200
1400
1600
1800
0
200
400
600
800
1000
1200
1400
1600
1800
-2
-1
0
1
2
x 10
-4
x [g]
-2
-1
0
1
2
x 10
-4
y [g]
Time [sec]
Fig. 12.14 Biaxial (x and y) measured acceleration time histories (detrended) at one corner on the
roof of a tall building
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10
-7
10
-6
10
-5
10
-4
10
-3
[ ][ ]
[
]
[
]
[
]
[
]
[g/ Hz]
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10
-7
10
-6
10
-5
10
-4
10
-3
[ ][ ]
[
]
[
]
[
]
[
]
Frequency [Hz]
[g/ Hz]
(b) Root-SV
(a) Root-PSD
TX1
TY1
R1
TX2
TY2
R2
Fig. 12.15 One-sided root PSD (a) and root SV spectrum (b) of biaxial (x and y) data measured
at one corner on the roof of a tall building. Horizontal bar indicates selected band
386
12
Single Mode Problem

spectrum suggests six potential modes in the bands shown. The nature of each
mode is determined from the mode shape found later.
Modal Identiﬁcation
The modal identiﬁcation results using the biaxial data at one corner on the roof are
shown in Table 12.11. The MPV of
ﬃﬃﬃﬃﬃ
Se
p
is a few lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
and tends to decrease
with the mode number. This is consistent with the spectra in Fig. 12.15. The modal
s/n ratio ranges from a few hundreds to a thousand. The modal force has a root PSD
of about 1 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
. The c.o.v. of damping ratio tends to decrease with the mode
number. This is partly because the effective data length as a multiple of natural
period is longer for higher modes with higher frequencies.
Table 12.12 shows the modal identiﬁcation results using data at all four corners
(8 DOFs). Comparing with Table 12.11, comments similar to Example 12.4 can be
made. Using all measured DOFs directly increases the modal force PSD and modal
Table 12.11 Modal identiﬁcation results using biaxial data at one corner on the roof of tall
building
Mode
s/n ratio
Frequency
Damping
Root modal force
PSD
Root prediction
error PSD
Mode shape
c
f
f
ﬃﬃﬃ
S
p
ﬃﬃﬃﬃﬃ
Se
p
u
MPV
c.o.v.
MPV
c.o.v.
MPV
c.o.v.
MPV
c.o.v.
c.o.v.
(Hz)
(%)
(%)
(%)
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(%)
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(%)
(%)
1
650
0.155
0.11
0.21
66
0.72
13
6.6
10
2.13
2
689
0.173
0.16
0.43
40
1.06
11
4.7
9
1.35
3
233
0.309
0.16
0.74
24
0.37
8
1.6
6
1.35
4
556
0.532
0.10
0.54
21
0.34
6
1.3
5
0.78
5
1226
0.685
0.07
0.36
22
0.36
6
1.4
5
0.56
6
636
0.823
0.08
0.56
17
0.39
5
1.4
4
0.57
Table 12.12 Modal identiﬁcation results using data at four corners on the roof of tall building
Mode
s/n ratio
Frequency
Damping
Root modal force
PSD
Root prediction
error PSD
Mode shape
c
f
f
ﬃﬃﬃ
S
p
ﬃﬃﬃﬃﬃ
Se
p
u
MPV
c.o.v.
MPV
c.o.v.
MPV
c.o.v.
MPV
c.o.v.
c.o.v.
(Hz)
(%)
(%)
(%)
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(%)
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(%)
(%)
1
1400
0.155
0.11
0.22
61
1.49
12
8.9
4
3.7
2
1214
0.173
0.16
0.46
39
2.17
10
6.8
3
2.6
3
235
0.309
0.16
0.68
25
0.66
8
3.2
2
3.7
4
706
0.532
0.10
0.52
21
0.68
6
2.5
2
1.9
5
1592
0.685
0.07
0.36
22
0.70
6
2.4
2
1.3
6
714
0.823
0.09
0.56
17
0.75
5
2.5
1
1.4
12.6
Laboratory/Field Data Examples
387

s/n ratio but it makes little difference in the MPV and posterior c.o.v.s. The root
modal force PSD roughly doubles because the number of DOFs with similar mode
shape values is quadrupled. Figure 12.16 shows the identiﬁed mode shapes
(MPV).
■
Example 12.6 (Smart phone data, footbridge) This example demonstrates
ambient modal identiﬁcation using triaxial acceleration data recorded on a smart
phone. As shown in Fig. 12.17, the phone was placed on one side of the bridge
deck at 1/4 span of the Queen’s Park Bridge in Chester, UK. Figure 12.18 shows
the measured time history collected for 300 s at 50 Hz. Figure 12.19 shows the root
PSD and SV spectrum of data. The spectra suggest a noise PSD of a few hundreds
of lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
, consistent with Fig. 6.2 in Sect. 6.3. The peaks suggest potential
modes between 0.5 and 5 Hz. The band around 0.8 Hz is likely to contain a single
mode. The situation is less clear with other bands, which can possibly contain close
modes. This can usually be resolved using sensors from multiple locations.
Modal Identiﬁcation
Table 12.13 shows the modal identiﬁcation results. The most probable mode shapes
are shown in Table 12.14, where their likely nature are also indicated. These results
should be interpreted with a level of conﬁdence consistent with the quality of data,
x
y
(1) TX1, 0.155Hz, 0.22%
x
y
(2) TY1, 0.173Hz, 0.46%
x
y
(3) R1, 0.309Hz, 0.68%
x
y
(4) TX2, 0.532Hz, 0.52%
x
y
(5) TY2, 0.685Hz, 0.36%
x
y
(6) R2, 0.823Hz, 0.56%
Fig. 12.16 Identiﬁed mode shapes (MPV) using data at four corners on the roof of tall building.
Title above each plot shows the mode number, nature, frequency and damping ratio (MPV)
388
12
Single Mode Problem

smart
phone
(a)
(b)
Fig. 12.17 a Queen’s Park Bridge and b location of sensor (smart phone)
0
50
100
150
200
250
300
-0.05
0
0.05
x [g]
0
50
100
150
200
250
300
-0.05
0
0.05
y [g]
0
50
100
150
200
250
300
-0.05
0
0.05
z [g]
Time [sec]
Fig. 12.18 Triaxial data (detrended) at 1/4 span of Queen’s Park Bridge. The x, y and z direction
correspond to the transverse, longitudinal and vertical direction of the bridge
0
1
2
3
4
5
10
-4
10
-3
10
-2
10
-1
10
-4
10
-3
10
-2
10
-1
[ ]
[ ]
[ ][ ]
Frequency [Hz]
[g/ Hz]
(a) Root PSD
0
1
2
3
4
5
[ ]
[ ]
[ ][ ]
Frequency [Hz]
[g/ Hz]
(b) Root SV
Fig. 12.19 One-sided root PSD (a) and SV (b) spectrum of triaxial data, Queen’s Park Bridge.
Horizontal bar indicates selected band
12.6
Laboratory/Field Data Examples
389

which is much lower than the previous two examples. For reference, the quanti-
zation resolution is about 0.001 g (Example 6.1) and its contribution to the root
PSD of noise is about 60 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(Sect. 6.6.2). The overall noise PSD is a few
hundreds of lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
; see Fig. 12.19 or Table 12.13
ﬃﬃﬃﬃﬃ
Se
p

	
: The identiﬁcation
results assume a single mode dominating the selected band, which is debatable in
the current case. The Bayesian algorithm always returns results as along as a
converged solution in the MPV can be obtained. The posterior c.o.v. need not
account for modeling error.
■
References
Au SK (2011) Fast Bayesian FFT method for ambient modal identiﬁcation with separated modes.
J Eng Mech ASCE 137(3):214–226
Zhang FL, Au SK (2013) Erratum for fast Bayesian FFT method for ambient modal identiﬁcation
with separated modes. J Eng Mech ASCE 139(4):545–545
Table 12.13 Modal identiﬁcation results, triaxial smart phone data, Queen’s Park Bridge
Mode
s/n ratio
Frequency
Damping
Root modal force
PSD
Root prediction
error PSD
Mode shape
c
f
f
ﬃﬃﬃ
S
p
ﬃﬃﬃﬃﬃ
Se
p
u
MPV
c.o.v.
MPV
c.o.v.
MPV
c.o.v.
MPV
c.o.v.
c.o.v.
(Hz)
(%)
(%)
(%)
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(%)
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
(%)
(%)
1
77
0.794
0.44
1.7
34
193
16
659
6.7
3.6
2
415
1.501
0.43
2.4
26
637
15
655
5.1
1.0
3
25
2.872
0.19
0.9
30
77
17
909
5.6
5.0
4
92
3.163
0.24
1.6
22
168
13
545
4.2
1.8
Table 12.14 Most probable
mode shapes, triaxial smart
phone data, Queen’s Park
Bridge
Mode
Nature
X
Y
Z
1
Transverse
−0.9972
−0.0340
0.0671
2
Vertical
−0.0117
0.0243
0.9996
3
Torsional
−0.2482
0.0266
0.9684
4
Vertical
−0.0315
−0.0335
0.9989
390
12
Single Mode Problem

Chapter 13
Multi-mode Problem
Abstract This chapter develops the computational algorithm for determining the
most probable value (MPV) and covariance matrix of modal parameters in
Bayesian operational modal analysis with data in a single setup and for the general
case of multiple (possibly close) modes. The partial mode shapes, i.e., conﬁned to
the measured degrees of freedom, are not necessarily orthogonal and this presents
difﬁculty in their efﬁcient identiﬁcation. Representing them via an orthogonal basis
of a ‘mode shape subspace’ allows efﬁcient determination in terms of such basis
and the associated coordinates. An iterative algorithm is presented for determining
the MPV, where the MPVs of different groups of parameters are updated until
convergence. The asymptotic behavior of the MPV for modes with high
signal-to-noise ratio is analyzed. Analytical formulas are derived for systematically
computing the covariance matrix of parameters given the measured data. Examples
with synthetic data and ﬁeld data are presented to illustrate the algorithms and their
applications.
Keywords Multi-mode  Mode shape subspace  Tall building  Signal-to-noise
ratio
In this chapter we discuss efﬁcient methods for calculating the posterior most
probable value (MPV) and covariance matrix of modal parameters in Bayesian
operational modal analysis (OMA) using data collected in a single setup and
considering the general case of multiple (possibly close) modes in the selected
frequency band. Mode shapes present the major hurdle as they contain a large
number of parameters. Conﬁned to the measured DOFs, they are not necessarily
orthogonal. One strategy is to represent them via a lower dimensional orthonormal
basis and then develop techniques for determining the MPV of the basis and
associated coordinates. This leads to an iterative algorithm where different groups
of parameters are updated sequentially until convergence.
Due to tangling of modes, the considerations and algorithms in this chapter are
much more complicated than those for well-separated modes in Chap. 12. It is
based on Au (2012a, b). The presentation has been simpliﬁed and reorganized.
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_13
391

Computer programming of the posterior covariance matrix using the analytical
formulas in Sect. 13.7 is simpler as it has taken advantage of a more systematic
procedure based on Lagrange multiplier concepts (Sect. 11.2.3).
Recall the problem context from Sect. 10.1 where the measured ambient
vibration history is denoted by f^yjgN1
j¼0 ðn  1Þ; n is the number of measured DOFs
and N is the number of samples per data channel. The scaled Fast Fourier
Transform (FFT) of f^yjgN1
j¼0 at frequency fk ¼ k=NDt (Hz) is
^F k ¼
ﬃﬃﬃﬃﬃ
Dt
N
r
X
N1
j¼0
^yje2pijk=N
ð13:1Þ
where Dt (s) is the sampling interval. The set of modal parameters h comprises
ffigm
i¼1
Natural frequencies ðHzÞ
ffigm
i¼1
Damping ratios
S
mm Hermitian modal force PSD
Se
Prediction error PSD
fuigm
i¼1
n1 Mode shapes
The mode shapes are subjected to unit norm constraint, uT
i ui ¼ 1 ði ¼ 1; . . .; mÞ.
The modal force PSD has the same unit as the PSD of acceleration. The prediction
error PSD has the same unit as the PSD of data.
Using the scaled FFT f ^F kg in a selected frequency band, the negative
log-likelihood function (NLLF) is given by:
LðhÞ ¼ nNf ln p þ
X
k
ln jEkðhÞj þ
X
k
^F

kEkðhÞ1 ^F k
ð13:2Þ
where Nf is the number of FFT points in the selected band;
Ek
nn ¼ U
nm Hk
mm UT
mn þ Se In
nn
ð13:3Þ
is the theoretical PSD matrix of measured data for given h; In denotes the n  n
identity matrix; U ¼ ½u1; . . .; um ðn  mÞ is the (partial) mode shape matrix;
Hk
mm ¼ diagðhkÞ
mm
S
mm diagðh
kÞ
mm
hk ¼ ½h1k; . . .; hmk
ð13:4Þ
hik ¼
ð2pifkÞq
1  b2
ik  2fibiki
bik ¼ fi
fk
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð13:5Þ
392
13
Multi-mode Problem

13.1
Mode Shape Subspace
The most probable mode shapes can be determined efﬁciently by separately iden-
tifying the subspace they lie in and the associated coordinates. The ‘mode shape
subspace’ is the subspace in Rn (the n-dimensional Euclidean space) spanned by
fuigm
i¼1. Its dimension, denoted by m0, is equal to the rank of U ¼ ½u1; . . .; um.
Elementary linear algebra shows that m0  minðn; mÞ. Table 13.1 lists the possible
ordering of m0, m and n. The most typical case is m0 ¼ m\n. It is possible for
m0\m when some mode shapes are linearly dependent. For example, when all
DOFs measure the same quantity then trivially ui ¼ ½1; . . .; 1T=
ﬃﬃﬃn
p
for all modes
and so m0 ¼ 1 regardless of n and m.
The mode shape dimension m0 is part of the modeling assumptions. Assuming m0
larger than the actual dimension increases computational effort. Assuming m0 less
than the actual dimension over-constrains the inference problem and leads to bias in
results. A robust choice is m0 ¼ m. When it is justiﬁed to do so, assuming m0\m
may speed up convergence of the MPV. The number of lines (eigenvalues) sig-
niﬁcantly above the remaining ones in the singular value (SV) spectrum gives an
indication of m0 (Sect. 7.3).
Example 13.1 (Dimension of mode shape subspace) Consider identifying the
horizontal modes of a building with sensors placed on the same ﬂoor. Figure 13.1
shows the dimension of the mode shape subspace ðm0Þ depending on the measured
DOFs and selected frequency band. For reference, the mode shapes conﬁned to two
locations are shown at the bottom of the table.
■
13.1.1
Orthonormal Basis Representation
Let B0 ¼ ½b1; . . .; bm0 ðn  m0Þ contain in its columns an orthonormal basis fbigm0
i¼1
ðn  1Þ in the mode shape subspace. That is, bT
i bj ¼ 1 if i ¼ j and zero otherwise.
The mode shape matrix U ¼ ½u1; . . .; um ðn  mÞ can then be represented as
U ¼ B0a
ð13:6Þ
where
a
m0m ¼ ½ a1
m01; . . .; am
m01
ð13:7Þ
Table 13.1 Possible ordering of m0, m and n
Case
I
IIa
III
IV
V
VI
Condition
m0\m\n
m0 ¼ m\n
m0 ¼ m ¼ n
m0\m ¼ n
m0\n\m
m0 ¼ n\m
aMost typical
13.1
Mode Shape Subspace
393

and ai contains the coordinates of ui w.r.t. the basis B0. Let B0
? ¼ ½bm0 þ 1; . . .; bn
ðn  ðn  m0ÞÞ where fbign
i¼m0 þ 1 ðn  1Þ are orthonormal and are orthogonal to
fbigm0
i¼1. Combining B0 and B0
? gives a matrix containing in its columns an
orthonormal basis in Rn, B ¼ ½B0; B0
? ðn  nÞ. Comparing (13.6) with the singular
value decomposition U ¼ LKRT shows that B0 ¼ L and a ¼ KRT.
13.2
Alternative Form of NLLF
Analogous to the case of single mode, the NLLF can be written in a form to
facilitate determination of the most probable basis in terms of the remaining
parameters. The key is to express jEkj and E1
k
via the eigenvector representation of
Ek. First, we write
(Plan view) 
1
m
2
m
3
m
4
n
II 
n
m
m
1
II 
n
m
m
2
I 
n
m
m
2
2
n
II 
n
m
m
1
III 
n
m
m
2
VI 
m
n
m
2
2
n
II 
n
m
m
1
II 
m
n
m
1
V 
m
n
m
1
DOFs
measured
Frequency
band
TX1
TY1
R1
TX1
TY1
R1
TX1
TY1
R1
TX1
TY1
R1
Fig. 13.1 Dimension of mode shape subspace in different situations
394
13
Multi-mode Problem

U ¼ ½B0; B0
? a
0


¼ B a
0


ð13:8Þ
Substituting this and In ¼ BInBT into (13.3),
Ek ¼ B a
0


Hk a
0

T
BT þ SeBInBT ¼ B E0
k
0
0
SeInm0


BT
ð13:9Þ
where
E0
k
m0m0 ¼
a
m0m Hk
mm aT
mm0 þ Se Im0
m0m0
ð13:10Þ
As the determinant of a block diagonal matrix is equal to the product of
determinants of the individual blocks,
jEkj ¼ jBjjE0
kjjSeInm0jjBTj ¼ Snm0
e
jE0
kj
ð13:11Þ
since jBj ¼ jBTj ¼ 1 and jSeInm0j ¼ Snm0
e
. On the other hand, using B1 ¼ BT
and ðABCÞ1 ¼ C1B1A1 for invertible matrices A, B and C,
E1
k
¼ B E01
k
0
0
S1
e Inm0


BT
ð13:12Þ
Substituting B ¼ ½B0; B0
? and expanding,
E1
k
¼ ½B0; B0
? E01
k
0
0
S1
e Inm0


½B0; B0
?T ¼ B0E01
k
B0T þ S1
e B0
?B0T
?
ð13:13Þ
Substituting B0
?B0T
? ¼ In  B0B0T and rearranging,
E1
k
¼ S1
e In  S1
e B0ðIm0  SeE01
k
ÞB0T
ð13:14Þ
Substituting (13.11) and (13.14) into (13.2),
L ¼ nNf ln p þ ðn  m0ÞNf ln Se þ S1
e d
þ
X
k
ln jE0
kj  S1
e
X
k
^F

kB0ðIm0  SeE01
k
ÞB0T ^F k
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
the only term that depends on B0
ð13:15Þ
13.2
Alternative Form of NLLF
395

where
d ¼
X
k
^F

k ^F k
ð13:16Þ
The signiﬁcance of (13.15) in comparison with (13.2) is that it involves E0
k
whose dimension m0 is typically much smaller than n. The mode shape basis B has
been segregated out and its spatial effect has been condensed through the projection
of the scaled FFT onto the mode shape subspace, i.e., B0T ^F k. In (13.2), Ek is almost
singular for small Se but now E0
k has full rank. The ill-conditioning due to small Se
has been segregated out in the factor S1
e .
13.3
Most Probable Mode Shape Basis
When m ¼ 1 (single mode), m0 ¼ 1 and, trivially, a ¼ 1 and U ¼ B0. It can then be
shown that the MPV of B0 is simply equal to the eigenvector of A in Sect. 12.1 with
the largest eigenvalue. If m0 ¼ n then the mode shape subspace is trivially Rn and
B0 ¼ In. In the general case where 1\m0\n, the MPV of B0 ¼ ½b1; . . .; bm0
minimizes the quadratic form in (13.15) subjected to orthonormal constraints. The
latter can be handled by representing B0 by a necessary and sufﬁcient set of (free)
hyper angles that can be determined by an iterative algorithm. Details are explained
next in Sects. 13.3.1–13.3.3.
13.3.1
Hyper Angle Representation
Apparently, the number of free parameters in B0 is m0n  m0ðm0 þ 1Þ=2 because it
contains m0n parameters under m0ðm0 þ 1Þ=2 distinct constraints, i.e., bT
i bj ¼ 1 if
i ¼ j and zero otherwise ði ¼ 1; . . .; m0; j  iÞ. However, the MPV of B0 is deter-
mined only up to arbitrary rotations within the planes formed by any two of its
columns because the mode shape subspace is invariant under such rotations. There
are m0ðm0  1Þ=2 such rotations (m0 choose 2). The same number of additional
constraints can be applied to remove this arbitrariness. Subtracting m0ðm0  1Þ=2
from m0n  m0ðm0 þ 1Þ=2, the number of free parameters is m0ðn  m0Þ.
We next characterize B0 with m0ðn  m0Þ hyper angles from a reference basis B0,
which can play the role of the current value of B ¼ ½B0; B0
? in an iterative algorithm
for determining the MPV of B0. Let B0 ¼ ½B0
0; B0
0? ðn  nÞ contain in its columns
an orthonormal basis in Rn, where B0
0 ðn  m0Þ contains the ﬁrst m0 vectors and B0
0?
ðn  ðn  m0ÞÞ has the remaining n  m0 vectors. Then the m0ðn  m0Þ angles can
be deﬁned as the rotations within the planes formed by all possible column pairs,
396
13
Multi-mode Problem

one from B0
0 and the other from B0
0?. To see this, B can be obtained by transforming
B0 through nðn  1Þ=2 rotations (n choose 2) within all possible planes spanned by
any two columns in B0. These rotations comprise three types:
(1) m0ðm0  1Þ=2 rotations within the planes formed by any two columns in B0
0;
(2) ðn  m0Þðn  m0  1Þ=2 rotations within the planes formed by any two columns
in B0
0?;
(3) m0ðn  m0Þ rotations within the planes formed by one column in B00 and the
other in B00?.
Check that the total number of rotations is nðn  1Þ=2. Only rotations of the third
type are necessary because rotations of the ﬁrst two types do not change the mode
shape subspace.
13.3.2
Rotation Matrix
Let aij ði ¼ 1; . . .; m0; j ¼ 1; . . .; n  m0Þ be the rotation (in radian) from the ith
column vector in B0
0 towards the jth column vector in B0
0?. The m0ðn  m0Þ angles
characterizing B0 are collected in the vector
a ¼ ½a11; a12; . . .; a1ðnm0Þ; a21; a22; . . .; a2ðnm0Þ; . . .; am01; am02; . . .; am0;ðnm0ÞT
ð13:17Þ
The rotations aij are applied in the same sequence as they appear in a, i.e., a11 ﬁrst,
a12 second, and so on.
The basis B can be written in terms of a and B0 by
BðaÞ ¼ PðaÞB0
ð13:18Þ
where
PðaÞ ¼ Pm0;ðnm0ÞPm0;ðnm0Þ1. . .Pm01
 Pm01;ðnm0ÞPm01;ðnm0Þ1. . .Pm01;1
 . . .P1;ðnm0ÞP1;ðnm0Þ1. . .P11
ð13:19Þ
is the rotation matrix that transforms B0 through the m0ðn  m0Þ angles in a;
Pij ¼ B0RijBT
0
ð13:20Þ
13.3
Most Probable Mode Shape Basis
397

is the rotation matrix associated with aij;
Rij ¼ ðeieT
i þ ej0eT
j0 Þ cos aij þ ðej0eT
i  eieT
j0 Þ sin aij þ
X
n
k6¼i;j0
ekeT
k
j0 ¼ m0 þ j
ð13:21Þ
is the matrix that rotates an angle aij from the ith axis towards the j0th axis in Rn.
Here, ei denotes a n  1 vector with the ith entry being the only nonzero entry equal
to 1. The sequence in which the aijs appear in (13.17) is exactly the opposite of the
Pijs in (13.19), because the rotation matrices on the right take action ﬁrst.
Substituting (13.20) into (13.19) and then into (13.18), and noting BT
0B0 ¼ In,
we have
BðaÞ ¼ B0RðaÞ
ð13:22Þ
where
RðaÞ ¼ Rm0;ðnm0ÞRm0;ðnm0Þ1. . .Rm01
 Rm01;ðnm0ÞRm01;ðnm0Þ1. . .Rm01;1
 . . .R1;ðnm0ÞR1;ðnm0Þ1. . .R11
ð13:23Þ
Note that Bð0Þ ¼ B0 because Rð0Þ ¼ In.
13.3.3
Newton Iteration
Given the remaining parameters, consider determining the MPV of B0 by repre-
senting it via the hyper angles in a and minimizing the NLLF in (13.15) w.r.t. a.
This can be done by Newton iteration. Suppose the reference basis B0 is sufﬁciently
close to the MPV so that the optimal value of a is expected to be small. This is
justiﬁed in an iterative algorithm where B0 is continually being updated. Thus,
consider the Taylor approximation of the gradient of NLLF about a ¼ 0 (omitting
dependence on other parameters):
rT
aLð^aÞ  rT
aLð0Þ þ ½r2
aLð0Þð^a  0Þ
ð13:24Þ
where ^a is the MPV of a given the remaining parameters. Solving raLð^aÞ ¼ 0 for ^a
gives the approximation
^a ¼ ½r2
aLð0Þ1rT
aLð0Þ
ð13:25Þ
398
13
Multi-mode Problem

The gradient and Hessian of the NLLF w.r.t. a at 0 can be derived analytically:
@Lð0Þ
@aij
¼ 2S1
e Re
X
k
AkFð1Þ
k Fð2Þ
k
"
#
ði;jÞ
ð13:26Þ
@2Lð0Þ
@aij@ars
¼ 2S1
e djsRe
X
k
AkFð1Þ
k Fð1Þ
k
"
#
ði;rÞ
2S1
e Re
X
k
Akðr; iÞ Fð2Þ
k Fð2Þ
k
h
i
ðj;sÞ
(
)
ð13:27Þ
where djs denotes the Kronecker Delta (equal to 1 if j ¼ s and zero otherwise); ½ði;jÞ
denotes the ði; jÞ-entry of the argument matrix;
Ak
m0m0 ¼ Im0  SeE01
k
Fð1Þ
k
m01
¼ B0T
0
m0n
^F k
n1
Fð2Þ
k
ðnm0Þ1
¼
B0T
0?
ðnm0Þn
^F k
n1
ð13:28Þ
Equation (13.27) assumes that ars is either aij or appears after (i.e., to the right of)
aij in (13.17). It allows the diagonal and upper triangular entries of the Hessian r2
aLð0Þ
to be determined. The lower triangular entries can be determined by symmetry.
Proof (Derivatives of NLLF w.r.t. a at 0)
Let
qkðaÞ ¼ ^F

kB0AkB0T ^F k
ð13:29Þ
where Ak is deﬁned in (13.28), so that the NLLF in (13.15) and its derivatives can
be written as (omitting dependence on other parameters)
LðaÞ ¼ S1
e
X
k
qkðaÞ þ terms independent of a
ð13:30Þ
@Lð0Þ
@aij
¼ S1
e
X
k
@qkð0Þ
@aij
@2Lð0Þ
@aij@ars
¼ S1
e
X
k
@2qkð0Þ
@aij@ars
ð13:31Þ
We shall show that the ﬁrst two derivatives of qkðaÞ at a ¼ 0 are given by
@qkð0Þ
@aij
¼ 2Re AkFð1Þ
k Fð2Þ
k
h
i
ði;jÞ
ð13:32Þ
@2qkð0Þ
@aij@ars
¼ 2djsRe AkFð1Þ
k Fð1Þ
k
h
i
ði;rÞ þ 2Re Akðr; iÞ Fð2Þ
k Fð2Þ
k
h
i
ðj;sÞ


ð13:33Þ
Substituting these into (13.31) gives (13.26) and (13.27).
13.3
Most Probable Mode Shape Basis
399

We ﬁrst write qk explicitly in terms of the rotation matrix RðaÞ. Using (13.22),
½ B0
nm0;
B0
?
nðnm0Þ

zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
B
¼ B0R ¼ B0R BT
0B0
|ﬄﬄ{zﬄﬄ}
In
¼ B0RBT
0 ½ B0
0
nm0;
B0
0?
nðnm0Þ

zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
B0
ð13:34Þ
Reading the ﬁrst m0 columns gives
B0 ¼ B0RBT
0B0
0
ð13:35Þ
On the other hand, let R be partitioned as
R
nn ¼ ½ R1
nm0;
R2
nðnm0Þ

ð13:36Þ
Substituting (13.35) and (13.36) into (13.29),
qk ¼ ^F

kB0
|ﬄﬄ{zﬄﬄ}
F
k
R
|{z}
½R1;R2
BT
0B0
0
|ﬄﬄ{zﬄﬄ}
Im0
0


Ak B0T
0 B0
|ﬄﬄ{zﬄﬄ}
Im0
0

	
RT
|{z}
RT
1
RT
2


BT
0 ^F k
|ﬄﬄ{zﬄﬄ}
Fk
¼ F
kR1AkRT
1Fk
ð13:37Þ
where Fk ¼ BT
0 ^F k. Differentiating and noting that Ak is Hermitian,
@qk
@aij
¼ F
k
@R1
@aij
AkRT
1Fk þ F
kR1Ak
@RT
1
@aij
Fk ¼ 2Re F
k
@R1
@aij
AkRT
1Fk


ð13:38Þ
@2qk
@aij@ars
¼ 2Re F
k
@2R1
@aij@ars
AkRT
1Fk


þ 2Re F
k
@R1
@aij
Ak
@RT
1
@ars
Fk


ð13:39Þ
Using (13.21) and (13.23), and noting that R1 contains the ﬁrst m0 columns of R,
@Rð0Þ
@aij
¼ ej0
n1
eT
i
1n
 ei
n1 eT
j0
1n
@R1ð0Þ
@aij
¼ ej0
n1
e0T
i
1m0
j0 ¼ m0 þ j
ð13:40Þ
where e0
i contains the ﬁrst m0 entries of ei. The second derivative @2Rð0Þ=@aij@ars
depends on the ordering of aij and ars. Assume that ars appears after (i.e., to the
right of) aij in (13.17). Then
R ¼ . . .Rrs. . .Rij. . .
ð13:41Þ
400
13
Multi-mode Problem

This gives
@2Rð0Þ
@aij@ars
¼ ðes0eT
r  ereT
s0Þðej0eT
i  eieT
j0 Þ ¼ dires0eT
j0  djsereT
i
j0 ¼ m0 þ j
@2R1ð0Þ
@aij@ars
¼ djsere0T
i
s0 ¼ m0 þ s
ð13:42Þ
where (13.40) and the following have been used:
eT
r ej0 ¼ eT
s0ei ¼ 0
eT
r ei ¼ dir
eT
s0ej0 ¼ djs
ð13:43Þ
It can be reasoned that this result is also applicable when ði; jÞ ¼ ðr; sÞ. Substituting
@R1ð0Þ=@aij from (13.40) into (13.38) at a ¼ 0 and noting Rð0Þ ¼ In,
@qkð0Þ
@aij
¼ 2Re½ F
kej0
|ﬄ{zﬄ}
scalar
e0T
i AkR1ð0ÞTFk
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
scalar
 ¼ 2Re½e0T
i Ak R1ð0ÞTFk
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
Fð1Þ
k
F
k
|{z}
½Fð1Þ
k
;Fð2Þ
k

ej0
¼ 2Re e0T
i
AkFð1Þ
k Fð1Þ
k
AkFð1Þ
k Fð2Þ
k
h
i
ej0
n
o
¼ 2Re AkFð1Þ
k Fð2Þ
k
h
i
ði;jÞ
ð13:44Þ
which
is
(13.32).
Similarly,
substituting
@R1ð0Þ=@aij
from
(13.40)
and
@2R1ð0Þ=@aij@ars from (13.42) into (13.39) at a ¼ 0 gives two terms, where
1st term ¼ 2djsRe½ F
ker
|ﬄ{zﬄ}
scalar
e0T
i AkR1ð0ÞTFk
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
scalar

¼ 2djsRe½e0T
i Ak R1ð0ÞTFk
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
Fð1Þ
k
F
k
|{z}
½Fð1Þ
k
;Fð2Þ
k

er
¼ 2djsRe e0T
i
AkFð1Þ
k Fð1Þ
k
AkFð1Þ
k Fð2Þ
k
h
i
er
n
o
¼ 2djsRe AkFð1Þ
k Fð1Þ
k
h
i
ði;rÞ
ð13:45Þ
2nd term ¼ 2Re½F
kej0
|ﬄ{zﬄ}
scalar
e0T
i Ake0
reT
s0Fk
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
scalar
 ¼ 2Re½e0T
i Ake0
r
|ﬄﬄﬄ{zﬄﬄﬄ}
Akði;rÞ
eT
s0FkF
kej0
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
½Fð2Þ
k Fð2Þ
k
ðs;jÞ

¼ 2Re Akðr; iÞ Fð2Þ
k Fð2Þ
k
h
i
ðj;sÞ


ð13:46Þ
since Rez ¼ Rez, Akði; rÞ ¼ Akðr; iÞ and ½Fð2Þ
k Fð2Þ
k
ðs;jÞ ¼ ½Fð2Þ
k Fð2Þ
k
ðj;sÞ. Combining
the ﬁrst and second term gives (13.33).
■
13.3
Most Probable Mode Shape Basis
401

13.4
Most Probable Spectral Parameters
Consider now determining the MPV of the remaining parameters for given B0:
ffigm
i¼1
ffigm
i¼1
a
S
Se
The quadratic term of the NLLF in (13.15) is re-grouped to read
L ¼ nNf ln p þ ðn  m0ÞNf ln Se þ S1
e ðd  d0Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
sensitive to Se
þ
X
k
ln jE0
kj þ
X
k
^F
0
k E01
k
^F
0
k
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
sensitive to ffi; fig; a; S
ð13:47Þ
where
^F
0
k ¼ B0T ^F k
d0 ¼
X
k
^F
0
k ^F
0
k
ð13:48Þ
For high s/n ratio, E0
k is not sensitive to Se. Equation (13.47) then roughly
separates the effect of Se and that of the remaining parameters. The MPV of Se
effectively minimizes the second and third term; see Sect. 13.6 later. These two
terms measure the spatial discrepancy arising from incomplete dimension ðm0  nÞ
of the mode shape subspace. They are identically zero when m0 ¼ n because then B0
has full rank, ^F
0
k ^F
0
k ¼ ^F

kB0B0T ^F k ¼ ^F

k ^F k and hence d0 ¼ d. In the general case,
ðd  d0Þ reﬂects the spectral contribution in the measured response that cannot be
accounted by the mode shape subspace.
13.4.1
Parameterizing Structured Matrices
While ffigm
i¼1, ffigm
i¼1 and Se are free parameters, a and S are subjected to con-
straints. The entries of S are subjected to mðm þ 1Þ=2 constraints so that S is
Hermitian. Each column of a, say, ai ðm0  1Þ, is subjected to unit norm constraint
because ui ¼ B0ai and
1 ¼ uT
i ui ¼ aT
i B0TB0ai ¼ aT
i ai
i ¼ 1; . . .; m
ð13:49Þ
The following represents a and S by free parameters. Write
S ¼ diagðSÞ1=2S0diagðSÞ1=2
ð13:50Þ
402
13
Multi-mode Problem

where S0 (m  m Hermitian) is a dimensionless form of S and its ði; jÞ-entry is
S0
ij ¼ Sij=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
SiiSjj
p
, i.e., the coherence between the ith and jth modal force. Clearly,
S0
ii ¼ 1, S0
ij ¼ S0
ji and jS0
ijj  1. One can represent
S0
ij ¼ eivij sin uij
i ¼ 1; . . .; m; j\i
ð13:51Þ
where uij and vij are free parameters. Note that
uij ¼ sin1 jS0
ijj
vij ¼ \S0
ij
ð13:52Þ
Substituting (13.50) into (13.10), E0
k can be written as
E0
k ¼ aSH0
kaT
S þ SeIm0
ð13:53Þ
where
H0
k ¼ diagðhkÞS0diagðh
kÞ
ð13:54Þ
aS ¼ a diagðSÞ1=2 ¼ ½S1=2
11 a1; S1=2
22 a2; . . .; S1=2
mmam
ð13:55Þ
The m diagonal entries of S and m0m parameters in a under m norm constraints are
now combined into m0m free parameters in aS.
Based on the above, the MPV of aS ðm0  mÞ and fuij; vij: i ¼ 1; . . .; m; j\ig are
ﬁrst determined by minimizing the NLLF where E0
k is given by (13.53). The MPV
of a and S can then be recovered. Let aSðiÞ denote the ith column of aS. Then
aðiÞ ¼
aSðiÞ
jjaSðiÞjj
Sii ¼ jjaSðiÞjj2
i ¼ 1; . . .; m
ð13:56Þ
Sij ¼ eivij
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
SiiSjj
p
sin uij
Sji ¼ Sij
i ¼ 1; . . .; m; j\i
ð13:57Þ
13.5
Algorithm for MPV
Below is an iterative algorithm for determining the MPV of modal parameters. The
following groups of parameters are optimized sequentially given the remaining ones
and iterated until convergence:
ffigm
i¼1
ffigm
i¼1
aS
fuij: i ¼ 1; . . .; m; j\ig
fvij: i ¼ 1; . . .; m; j\ig
Se
a
13.4
Most Probable Spectral Parameters
403

Iterative algorithm for MPV
0. Set initial guess for ffig, ffig, aS, fuijg, fvijg, Se and B. See Sect. 13.6.1.
1. Update aS and fuijg by minimizing (13.47).
2. Update fvijg by minimizing (13.47).
3. Update ffig and ffig by minimizing (13.47).
4. Update Se by minimizing (13.47).
5. Update a ¼ faijg in (13.17) using (13.25). Calculate R in (13.23) and
update B to be BR.
Repeat Steps 1–5 until convergence.
Steps 1–4 can be performed using conventional optimization algorithm, e.g.,
fminsearch in Matlab. They can be arranged in any order. Optimization is better
performed w.r.t. non-dimensional parameters, e.g., as a multiplier of the last iterated
value. Updating the mode shape basis last (via a) preserves norm constraint sin-
gularity in the posterior covariance matrix (Sect. 11.3.1).
13.6
High s/n Asymptotics of MPV
When the modal s/n ratio is high, the MPV of B0 and Se can be obtained analyti-
cally. For multiple modes, high modal s/n asymptotics refers to Im0  SeE01
k
	 Im0.
Let bi be the ith column of B0 so that B0 ¼ ½b1; . . .; bm0. Then in (13.15)
X
k
^F

k B0 ðIm0  SeE01
k
Þ
zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{
	 Im0
B0T
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
	 Pm0
i¼1 bibT
i
^F k 	
X
k
X
m0
i¼1
^F

kbi
|ﬄ{zﬄ}
scalar
bT
i ^F k
|ﬄﬄ{zﬄﬄ}
scalar
¼
X
k
X
m0
i¼1
bT
i ^F k
|ﬄﬄ{zﬄﬄ}
scalar
^F

kbi
|ﬄ{zﬄ}
scalar
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
real scalar
¼
X
m0
i¼1
bT
i Dbi
ð13:58Þ
where
D ¼ Re
X
k
^F k ^F

k
ð13:59Þ
In (13.58), there are no cross terms between bi and bj for i 6¼ j. Under
orthonormal constraints on fbigm0
i¼1, it is maximized (and hence the NLLF is min-
imized) when fbigm0
i¼1 are the eigenvectors (with unit norm) corresponding to the m0
largest eigenvalues of D. Thus, the asymptotic MPV of B0 ¼ ½b1; . . .; bm0 can be
404
13
Multi-mode Problem

determined directly from data without iterations. The dimension of the mode shape
subspace ðm0Þ can be empirically determined based on the eigenvalues of D.
On the other hand, for high s/n ratio, E0
k in (13.10) is dominated by the ﬁrst term,
which is insensitive to Se. The MPV of Se then minimizes the second and third term
in (13.47), giving
^Se 	
d  d0
ðn  m0ÞNf
ðm0\nÞ
ð13:60Þ
Using the norm-preserving property of B, it can be shown that d0  d and so ^Se is
non-negative. When n ¼ m0, the second and third term in (13.47) are identically
zero. The remaining terms then become the leading order, rendering the asymptotic
MPV of Se non-trivial.
13.6.1
Initial Guess of MPV
Initial guess for the iterative algorithm in Sect. 13.5 may be set as follow. The initial
guess for the natural frequencies ffjg can be picked from the singular value
(SV) spectrum (Sect. 7.3). The initial guess for damping ratios ffig may be assigned
as 1%. By noting that at the natural frequency the top line in the SV spectrum is
approximately Sii=4f2
i ð2pfiÞ2q, multiplying it by 4f2
i ð2pfiÞ2q gives the initial guess
for Sii; q = 0, 1, 2 for acceleration, velocity and displacement data, respectively. The
asymptotic MPV in (13.60) can be used as the initial guess for Se. The initial guess
for B ¼ ½B0; B0
? can be taken as the eigenmatrix of D in (13.59) with eigenvalues
arranged in descending order of magnitude. The initial guess for ui can be set as the
eigenvector (largest eigenvalue) of the real part of sample PSD matrix of data at the
initial guess of natural frequency. With the initial guess for B0 and U ¼ ½u1; . . .; um,
the initial guess for a can be determined as the least squares solution to U ¼ B0a,
i.e., a ¼ ðB0TB0Þ1B0TU. Combine fSiig and a to form the initial guess for aS
according to (13.55). The initial guess for fuijg and fvijg may be assigned as 0.1.
13.7
Posterior Covariance Matrix
Under a Gaussian approximation of the posterior PDF, the posterior covariance matrix
of modal parameters satisfying constraints (on mode shape norms) is given by
(Sect. 11.2)
^C
nhnh ¼ ðr^vcÞ
nhp
ðr2^LcÞ þ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
pp
ðr^vcÞT
pnh
ð13:61Þ
13.6
High s/n Asymptotics of MPV
405

where ‘ þ ’ denotes the pseudo-inverse, i.e., ignoring zero eigenvalues from con-
straint singularity; r2^Lc is the Hessian of LcðuÞ ¼ LðvcðuÞÞ w.r.t. a set of free
parameters u ðp  1Þ; vcðuÞ maps u to h that always satisﬁes the constraints
GiðvcðuÞÞ ¼ 0, i ¼ 1; . . .; m; fGiðhÞgm
i¼1 are the norm constraint equations on mode
shapes. A hat ‘^’ on quantity denotes that it is evaluated at the MPV. Using the
formula based on Lagrange multiplier (Sect. 11.2.3),
r2^Lc
pp ¼ ðr^vcÞT
pnh
r2^L
nhnh þ
X
m
i¼1
^ki r2 ^Gi
nhnh
 
!
ðr^vcÞ
nhp
ð13:62Þ
where r2^L is the Hessian of the NLLF in (13.2) (ignoring mode shape norm
constraints) and
^k ¼
^k1
...
^km
2
64
3
75 ¼ ½r^G
mnh ðr^GÞT
nhm
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
mm
1 ðr^GÞ
mnh
ðr^LÞT
nh1
ð13:63Þ
r^L ¼ rLðhÞjh¼^h
r2^L ¼ r2LðhÞ


h¼^h
r^G
mnh ¼
r^G1
..
.
r^Gm
2
64
3
75
ð13:64Þ
In the present case, one can take u and h to comprise the same set of parameters:
ffigm
i¼1
ffigm
i¼1
fSiigm
i¼1
fUijgm
j\i;i¼1
fVijgm
j\i;i¼1
Se
U ¼ ½u1; . . .; um
where Uij ¼ Re Sij and Vij ¼ Im Sij so that Sij ¼ Uij þ iVij. Let - comprise the
modal parameters other than mode shapes. One can take
GiðhÞ ¼ 1  uT
i ui
ði ¼ 1; . . .; mÞ
vcðhÞ ¼
-
jju1jj1u1
..
.
jjumjj1um
2
6664
3
7775
ð13:65Þ
so that that GiðvcðuÞÞ 
 0 ði ¼ 1; . . .; mÞ for any u. Direct differentiation and
evaluating at the MPV gives
r- ^Gi ¼ 0
r2
- ^Gi ¼ 0
ruj ^Gi ¼ 2^uT
i dij
r2
uj ^Gi ¼ 2Indij
ð13:66Þ
where dij is the Kronecker Delta (1 if i ¼ j, zero otherwise). Using these results and
ruiðjjuijj1uiÞ ¼ In  uiuT
i for jjuijj ¼ 1;
406
13
Multi-mode Problem

^ki ¼ 1
2 ðrui^LÞ^ui
ð13:67Þ
X
m
i¼1
^kir2 ^Gi ¼
0n-n-
2^k1In
..
.
2^kmIn
2
6664
3
7775
ð13:68Þ
r^vc ¼
In-n-
In  ^u1^uT
1
..
.
In  ^um^uT
m
2
6664
3
7775
ð13:69Þ
Due to norm constraints, r2^Lc has m zero eigenvalues with eigenvectors equal
to the null vectors of r^vc, which are given by
0
n-1
^u1
2
664
3
775
0
n-1
^u2
2
664
3
775
  
0
n-1
^um
2
664
3
775
ð13:70Þ
It remains to determine r^L and r2^L, which can be via ﬁnite difference
approximation or analytical expressions. The latter is discussed in Sects. 13.7.1 and
13.7.2. Section 13.7.1 discusses a method that expresses the results in terms of the
derivatives of Ek. It is generic but can be slow for large n. Section 13.7.2 discusses
an alternative method which is more efﬁcient for n [ m.
13.7.1
General Expressions
For any parameters x and y,
LðxÞ ¼
X
k
ðlnjEkjÞðxÞ þ
X
k
^F

kðE1
k ÞðxÞ ^F k
LðxyÞ ¼
X
k
ðlnjEkjÞðxyÞ þ
X
k
^F

kðE1
k ÞðxyÞ ^F k
ð13:71Þ
The derivatives of ln jEkj and E1
k
can be expressed in terms of the derivatives of
Ek. For any parameters x and y,
13.7
Posterior Covariance Matrix
407

ðln jEkjÞðxÞ ¼ tr½E1
k EðxÞ
k 
ðE1
k ÞðxÞ ¼ E1
k EðxÞ
k E1
k
ðln jEkjÞðxyÞ ¼ tr½E1
k EðxyÞ
k
 E1
k EðxÞ
k E1
k EðyÞ
k 
ðE1
k ÞðxyÞ ¼ E1
k ½EðxÞ
k E1
k EðyÞ
k þ EðyÞ
k E1
k EðxÞ
k
 EðxyÞ
k
E1
k
ð13:72Þ
The derivatives of Ek are given in Table 13.2. The expressions involve the
derivatives of Hk in Table 13.3, which in turn depends on the derivatives of the
transfer function h in Table 13.4. Figure 13.2 shows the information ﬂow. Since Ek
is almost singular, its inverse should be calculated using the eigenvector repre-
sentation in (13.14), where the ill-conditioning due to small Se has been segregated
out in the factor S1
e :
E1
k
¼ B0E01
k
B0T þ S1
e ðIm  B0B0TÞ
ð13:73Þ
13.7.2
Condensed Expressions
Computing the derivatives of the NLLF via those of Ek involves matrix compu-
tations of size n (the number of DOFs) which can be large in applications. It is
possible to obtain the derivatives in a condensed form that involves matrix com-
putations of size m (the number of modes) only. This can be done using the Matrix
Inverse Lemma and Matrix Determinant Theorem (Sect. C.5.1), where for any
matrices A, C, U and V of appropriate size with A and C invertible:
Table 13.2 Derivatives of
Ek ¼ UHkUT þ SeIn
EðxyÞ
k
y ¼
fj; fj;
Sjj; Urs; Vrs
Se
Urs
x ¼
fi; fi;
Sii; Uij; Vij
UHðxyÞ
k
UT
0n
UHðxÞ
k eT
rs þ ðÞ
Se
0n
0n
Uij
Sym.
eijHkeT
rs þ ðÞ
EðyÞ
k
UHðyÞ
k UT
In
UHkeT
rs þ ðÞ
See Table 13.3 for derivatives of Hk; ðÞ denotes a term equal to
the complex conjugate of the previous one.
408
13
Multi-mode Problem

Table 13.3 Derivatives of Hk ¼ diagðhkÞS diagðh
kÞ
HðxyÞ
k
y ¼
fj; fj
Sjj
Urs; Vrs
x ¼
fi; fi
½SijhðxÞ
ik hðyÞ
jk eij þ dijhðxyÞ
ik
diagðhkÞS eii þ ðÞ
dijhðxÞ
ik h
ikeii þ ðÞ
c hðxÞ
ik h
skers þ ðÞ
i ¼ r
c hðxÞ
ik hrkers þ ðÞ
i ¼ s
(
c ¼ 1 for Urs; c ¼ i for Vrs
Sii
0
0
Uij; Vij
Sym.
0
HðyÞ
k
hðyÞ
jk diagðhkÞS ejj þ ðÞ
hjkh
jkejj
c hrkh
skers þ ðÞ
c ¼ 1 for Urs; c ¼ i for Vrs
See Table 13.4 for derivatives of h; ðÞ denotes a term equal to the complex conjugate of the previous one.
13.7
Posterior Covariance Matrix
409

ðUCV þ AÞ1 ¼ A1  A1UðVA1U þ C1Þ1VA1
ð13:74Þ
jUCV þ Aj ¼ jAjjCjjVA1U þ C1j
ð13:75Þ
Using these with A ¼ SeIn, C ¼ Hk, U ¼ U and V ¼ UT,
jEkj ¼ Snm
e
jHkjjPkj
E1
k
¼ S1
e In  S1
e UP1
k UT
ð13:76Þ
Pk
mm ¼ UT
mn U
nm þ Se H1
k
mm
ð13:77Þ
Substituting into the NLLF in (13.2),
L ¼ nNf ln p þ ðn  mÞNf ln Se
þ
X
k
ln jHkj þ
X
k
ln jPkj þ S1
e ðd 
X
k
r
k
1m
P1
k
mm
rk
m1Þ
ð13:78Þ
where
rk
m1 ¼ UT
mn
^F k
n1
ð13:79Þ
and d is given by (13.16). Comparing (13.78) with (13.2), it only involves Hk and
Pk of dimension m rather than n. The matrices Pk and Hk depend on all parameters,
while rk only depends on mode shapes.
Table 13.4 Derivatives of hðb; fÞ ¼ ð1  b2  2fbiÞ1ð2pfkiÞq
hðxyÞ
y ¼
b
f
x ¼
b
2ð3b2 þ 1  4f2 þ 6ifbÞh3ð2pfkiÞq
2ið3b2 þ 1 þ 2ifbÞh3ð2pfkiÞq
f
Sym.
8b2h3ð2pfkiÞq
hðyÞ
2ðb þ ifÞh2ð2pfkiÞq
2ibh2ð2pfkiÞq
q ¼ 0; 1; 2 for acceleration data, velocity data and displacement data, respectively
h
Table 13.4
k
H
Table 13.3
k
E
Table 13.2
|
|
ln
k
E
Eq. 13.71
L
1
−
k
E
Fig. 13.2 Information ﬂow of derivative calculations
410
13
Multi-mode Problem

To facilitate presentation of derivatives, the NLLF in (13.78) is written as
L ¼ Le þ LH þ LP þ Lq
ð13:80Þ
where
Le ¼ nNf ln p þ ðn  mÞNf ln Se
LH ¼
X
k
ln jHkj
LP ¼
X
k
ln jPkj
ð13:81Þ
Lq ¼ S1
e ðd  qÞ
q ¼
X
k
r
kP1
k rk
ð13:82Þ
Clearly, Le depends only on Se. Its derivatives are non-zero only when taken
w.r.t. Se:
LðSeÞ
e
¼ ðn  mÞNf S1
e
LðSeSeÞ
e
¼ ðn  mÞNf S2
e
ð13:83Þ
The derivatives of LH are simply equal to the sum of the derivatives of ln jHkj.
The latter can be obtained by exactly the same formulas in (13.72) (replace Ek by
Hk) in terms of the derivatives of Hk, which are given in Table 13.3. The deriva-
tives of LP can be obtained in the same way in terms of the derivatives of Pk, which
are given in Table 13.5. The derivatives of Lq are given in Table 13.6. The
expressions depend in a hierarchical manner on the derivatives of q (Table 13.7),
Pk, Hk and h. In the development of software it is recommended that these
derivatives be coded and checked separately as they are used repeatedly.
Figure 13.3 illustrates the information ﬂow.
Table 13.5 Derivatives of Pk ¼ UTU þ SeH1
k
PðxyÞ
k
y ¼
fj; fj;
Sjj; Urs; Vrs
Se
Urs
x ¼
fi; fi;
Sii; Uij; Vij
SeðH1
k ÞðxyÞ
ðH1
k ÞðxÞ
0
Se
0
0
Uij
Sym.
direjs þ ðÞT
PðyÞ
k
SeðH1
k ÞðyÞ
H1
k
UTers þ ðÞT
See Table 13.3 for derivatives of H1
k ; ðÞT denotes a term equal to the transpose of the previous
one.
13.7
Posterior Covariance Matrix
411

Table 13.6 Derivatives of Lq ¼ S1
e ðd  qÞ
LðxyÞ
q
y ¼
fj; fj; Sjj;
Urs; Vrs; Urs
Se
x ¼
fi; fi; Sii;
Uij; Vij; Uij
S1
e qðxyÞ
S2
e qðxÞ  S1
e qðxSeÞ
Se
Sym.
2S3
e ðd  qÞ
þ 2S2
e qðSeÞ  S1
e qðSeSeÞ
LðyÞ
q
S1
e qðyÞ
S2
e ðd  qÞ  S1
e qðSeÞ
See Table 13.7 for derivatives of q.
Table 13.7 Derivatives of q ¼ P
k r
kP1
k rk
qðxyÞ
y ¼
fj; fj; Sjj;
Urs; Vrs; Se
Urs
x ¼
fi; fi; Sii;
Uij; Vij; Se
P
k r
kðP1
k ÞðxyÞrk
X
k r
kðP1
k ÞðxUrsÞrk
þ 2Re
X
k ^F kðrÞr
kðP1
k ÞðxÞðsÞ
Uij
Sym.
X
k r
kðP1
k ÞðUijUrsÞrk
þ 2Re
X
k ^F kðrÞr
kðP1
k ÞðUijÞðsÞ
þ 2Re
X
k ^F kðiÞr
kðP1
k ÞðUrsÞðjÞ
þ 2Re
X
k ^F kðrÞ ^F kðiÞP1
k ðs; jÞ
qðyÞ
P
k r
kðP1
k ÞðyÞrk
X
k r
kðP1
k ÞðUrsÞrk
þ 2Re
X
k ^F kðrÞr
kP1
k ðsÞ
See (13.72) for derivatives of P1
k
(replace Ek by Pk) and Table 13.5 for derivatives of Pk; ^F kðrÞ
denotes the rth entry of ^F k; P1
k ðsÞ denotes the sth column of P1
k ; P1
k ðs; jÞ denotes the ðs; jÞ-entry
of P1
k .
h
Table 13.4
k
H
Eq. 13.72
1
−
k
H
k
P
Table 13.5
1
−
k
P
q
Table 13.7
q
L
Table 13.6
H
L
P
L
e
L
Table 13.3
Eq. 13.72
Eq. 13.83
Eq.13.81
Eq.13.81
Fig. 13.3 Information ﬂow of derivative calculations in condensed form
412
13
Multi-mode Problem

13.8
Illustrative Examples
In this section we present examples on Bayesian OMA with close modes. The ﬁrst
example is based on synthetic data. The second example is based on ﬁeld data from
a tall building.
Example 13.2 (Synthetic data) Consider synthetic data with two modes generated
at 100 Hz by
^yj ¼ u1€g1ðtjÞ þ u2€g2ðtjÞ þ eðtjÞ
ð13:84Þ
where €giðtÞ ði ¼ 1; 2Þ is the modal acceleration satisfying
€giðtÞ þ 2fixi _giðtÞ þ x2
i giðtÞ ¼ piðtÞ
ð13:85Þ
with natural frequencies xi ¼ 2pfi (rad/s), f1 ¼ 1 Hz, f2 ¼ 1:02 Hz; and damping
ratios f1 ¼ f2 ¼ 1%; p1ðtÞ and p2ðtÞ are modal excitations, stationary Gaussian
with PSD S11 ¼ S22 ¼ 1ðlgÞ2=Hz and cross PSD S12 ¼ 0:5eip=4ðlgÞ2=Hz;
u1 ¼ 1
2
2
½
T=3
u2 ¼ 2
1
2
½
T=3
ð13:86Þ
Figure 13.4 shows the root PSD and SV spectrum of 600 s data. The top two
lines in (b) indicate two modes with very similar frequencies. Table 13.8 summa-
rizes the modal identiﬁcation results. The posterior c.o.v. is about 0.2% for the
natural frequencies and 20% for the damping ratios. The modal force PSDs have a
c.o.v. of about 15%, which is similar to the c.o.v. of the damping ratios. The c.o.v.
of S12 is quite high, about 100% in the real part and 30% in the imaginary
part. Table 13.9 shows that the exact and most probable mode shapes are close,
with a hyper angle of about 10%. The mode shape c.o.v. is of the same order of
magnitude.
0
0.5
1
1.5
2
[
]
Frequency [Hz]
[ g/ Hz]
(a) Root PSD
0
0.5
1
1.5
2
10
-1
10
0
10
1
10
2
10
-1
10
0
10
1
10
2
[
]
Frequency [Hz]
[ g/ Hz]
(b) Root SV
Fig. 13.4 Root PSD (a) and root SV spectrum (b), synthetic data. Horizontal bar indicates
selected band
13.8
Illustrative Examples
413

Figure 13.5 shows the results for different data durations. The results generally
converge to the exact value that generated the data. The error bars generally cover
the exact values, reﬂecting consistency of the posterior c.o.v. with the MPV.
■
Example 13.3 (Field data, tall building with close modes) Consider a tall building
measuring 50 m  50 m in plan and over 300 m in height. This building has similar
dimensions asthe one in Example12.5 but its ﬁrst two modes are much closer. Triaxial
acceleration (x, y and z) at four corners on the roof were measured for 1800 s under
normal wind situation in the late afternoon. The x and y directions are parallel to the
two perpendicular sides of the building. Figure 13.6 shows the measured time his-
tories at one corner. The root PSD and SV spectrum are shown in Fig. 13.7. The
spectra suggest six potential modes. Their nature is determined based on mode shape
information found later. The ﬁrst two modes are very close and so they are identiﬁed
Table 13.8 Modal identiﬁcation results, synthetic data, close modes
Parameter
Mode 1
Mode 2
Frequency f
Exact (Hz)
1.000
1.020
MPV (Hz)
1.002
1.021
c.o.v. (%)
0.19
0.16
Damping f
Exact (%)
1.000
1.000
MPV (%)
1.12
0.86
c.o.v. (%)
18
21
Modal force PSD Sii
Exact (ðlgÞ2=Hz)
1.000
1.000
MPV (ðlgÞ2=Hz)
1.086
1.035
c.o.v. (%)
16
15
Cross PSD, real part ReS12
Exact (ðlgÞ2=Hz)
0.35
MPV (ðlgÞ2=Hz)
0.17
c.o.v. (%)
97
Cross PSD, imaginary part ImS12
Exact (ðlgÞ2=Hz)
0.35
MPV (ðlgÞ2=Hz)
0.25
c.o.v. (%)
31
Prediction error PSD Se
Exact (ðlgÞ2=Hz)
1.000
MPV (ðlgÞ2=Hz)
1.05
c.o.v. (%)
9
Table 13.9 Identiﬁed mode shape (MPV), synthetic data, close modes
Mode 1
Mode 2
Exact
0.333
0.667
0.667
−0.667
0.333
0.667
MPV
0.235
0.667
0.707
−0.611
0.373
0.698
Angle between exact and MPV
mode shape (%)
10.6
7.6
Mode shape c.o.v. (%)
8.1
7.7
414
13
Multi-mode Problem

together assuming m ¼ 2 modes in the selected band. The third and fourth mode are
well-separated from others and can be identiﬁed assuming a single mode ðm ¼ 1Þ.
The ﬁfth and sixth modes are not very close but their resonance bands do
overlap. They are identiﬁed on the same band assuming m ¼ 2 modes.
Results Using Triaxial Data at One Corner
Table 13.10 shows the modal identiﬁcation results using triaxial data at one corner
only. The root modal force PSD is in the order of 1 lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
. The root prediction
error
ﬃﬃﬃﬃﬃ
Se
p
is a few lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
, which is consistent with the noise PSD reﬂected in
Fig. 13.7. The values for modes 1 and 2 are by deﬁnition the same because they are
0.995
1
1.005
f1 [Hz]
1.015
1.02
1.025
f2 [Hz]
0.5
1
1.5
1 [%]
0.5
1
1.5
2 [%]
0.5
1
1.5
S11 [( g)2/Hz]
0.5
1
1.5
S22 [( g)2/Hz]
-0.4
-0.2
0
0.2
0.4
0.6
0.8
Re S12 [( g)2/Hz]
10
2
10
3
10
4
10
2
10
3
10
4
10
2
10
3
10
4
10
2
10
3
10
4
10
2
10
3
10
4
10
2
10
3
10
4
10
2
10
3
10
4
10
2
10
3
10
4
10
2
10
3
10
4
0
0.2
0.4
Im S12 [( g)2/Hz]
0.8
1
1.2
1.4
Se [( g)2/Hz]
Data duration (sec)
Fig. 13.5 Modal identiﬁcation results for different data durations. Dot MPV; error bar ±two
standard deviations. Dashed line exact value
13.8
Illustrative Examples
415

0
200
400
600
800
1000
1200
1400
1600
1800
-4
-2
0
2
4
x 10
-4
x [g]
0
200
400
600
800
1000
1200
1400
1600
1800
-4
-2
0
2
4
x 10
-4
y [g]
0
200
400
600
800
1000
1200
1400
1600
1800
-4
-2
0
2
4
x 10
-4
z [g]
Time [sec]
Fig. 13.6 Measured data (detrended) at one corner on the roof of a tall building
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10
-6
10
-5
10
-4
10
-3
10
-6
10
-5
10
-4
10
-3
[
]
[
]
[
]
[
]
[g/ Hz]
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
[
]
[
]
[
]
[
]
Frequency [Hz]
[g/ Hz]
(b) Root SV
(a) Root PSD
TX1
TY1
R1
TX2
TY2
R2
Fig. 13.7 One-sided root PSD spectrum (a) and root SV spectrum (b) of triaxial data at one
corner on the roof of a tall building. Horizontal bar indicates selected band
416
13
Multi-mode Problem

identiﬁed on the same band. To supplement, the MAC (modal assurance criterion)
of the most probable mode shapes of modes 1 and 2 is 0.127; they are not
orthogonal. The MPV of modal force coherence is 0:435  0:178i for the ﬁrst two
modes; and 0:275  0:116i for the last two modes.
Results Using Triaxial Data at Four Corners
Table 13.11 shows the modal identiﬁcation results when triaxial data at four corners
(12 measured DOFs) are used. The identiﬁed mode shapes are shown in Fig. 13.8.
Comments similar to Example 12.5 can be made. The root modal force PSD is
roughly doubled because there are now four sensors and their mode shape values
are similar in magnitude. The modal s/n ratio is roughly quadrupled. Despite this,
there is no signiﬁcant reduction in the posterior c.o.v.s. One exception is mode 4
where the damping c.o.v. reduces from 31 to 16%. This is partly attributed to the
increase in the s/n ratio from a low value before.
■
Table 13.10 Modal identiﬁcation results using triaxial data at one corner of tall building
Mode
s/n
ratio c
Frequency f
Damping f
Root modal force
PSD
ﬃﬃﬃﬃﬃ
Sii
p
Root prediction
error PSD
ﬃﬃﬃﬃﬃ
Se
p
Mode
shape
u
MPV
(Hz)
c.o.v.
(%)
MPV
(%)
c.o.v.
(%)
MPV
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
c.o.v.
(%)
MPV
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
c.o.v.
(%)
c.o.v.
(%)
1
10745
0.184
0.17
0.5
30
5.7
6
5.1
3.6
7
2
3968
0.189
0.23
0.9
23
6.0
6
8
3
44
0.428
0.20
1.2
19
1.1
8
6.7
3.1
3
4
6
0.567
0.24
1.1
31
0.3
19
5.9
2.9
11
5
21
0.712
0.33
3.6
12
1.9
7
5.5
2.6
3
6
50
0.758
0.13
0.8
17
0.6
6
7
Table 13.11 Modal identiﬁcation results using triaxial data at four corners on the roof of tall
building
Mode
s/n
ratio c
Frequency f
Damping f
Root modal force
PSD
ﬃﬃﬃﬃﬃ
Sii
p
Root prediction
error PSD
ﬃﬃﬃﬃﬃ
Se
p
Mode
shape
u
MPV
(Hz)
c.o.v.
(%)
MPV
(%)
c.o.v.
(%)
MPV
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
c.o.v.
(%)
MPV
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
c.o.v.
(%)
c.o.v.
(%)
1
45885
0.184
0.18
0.6
28
12.6
6
4.5
1.1
7
2
18210
0.189
0.24
1.1
20
13.4
5
8
3
245
0.427
0.19
1.3
17
2.1
6
4.9
1.4
3
4
48
0.568
0.22
1.8
16
0.9
8
3.5
1.3
5
5
217
0.711
0.24
2.8
10
2.9
5
3.6
0.9
3
6
532
0.758
0.12
1.0
13
1.6
4
4
13.8
Illustrative Examples
417

References
Au SK (2012a) Fast Bayesian ambient modal identiﬁcation in the frequency domain, Part I:
posterior most probable value. Mech Syst Signal Process 26(1):60–75
Au SK (2012b) Fast Bayesian ambient modal identiﬁcation in the frequency domain, Part II:
posterior uncertainty. Mech Syst Signal Process 26(1):76–90
(1) TX1, 0.184Hz, 0.65%
x
y
x
(2) TY1, 0.189Hz, 1.1%
y
x
(3) R1, 0.427Hz, 1.3%
y
(4) R2, 0.568Hz, 1.8%
x
y
x
(5) TX2, 0.711Hz, 2.8%
y
x
(6) TY2, 0.758Hz, 0.99%
y
Fig. 13.8 Identiﬁed mode shapes (MPV) using triaxial data at four corners on the roof of tall
building. Title above each plot shows the mode number, nature, frequency and damping ratio
(MPV)
418
13
Multi-mode Problem

Chapter 14
Multi-setup Problem
Abstract This chapter discusses operational modal analysis using data from
multiple setups, each covering a possibly different set of measured degrees of
freedom. Motivated from practical applications, the focus is on obtaining a ‘global
mode shape’ covering a potentially large number of degrees of freedom by incor-
porating the information from different setups that are measured with a small
number of sensors. Least squares methods based only on the ‘local mode shapes’ in
different setups are ﬁrst discussed, which offer a mathematically simple yet reliable
solution for modes with high signal-to-noise ratios. Bayesian algorithm is then
developed to offer a more robust solution, incorporating all information in different
setups and accounting for their data quality. The asymptotic behavior of the MPV
for modes with high signal-to-noise ratio is analyzed, which can be mathematically
connected with the least squares solution. Analytical formulas are derived for
systematically computing the covariance matrix of parameters given the measured
data. Field data examples are presented to illustrate the practical aspects of ambient
vibration tests with multiple setups and their modal identiﬁcation.
Keywords Local least squares  Global least squares  Footbridge  Coupled
slab  Velodrome
In this chapter we discuss efﬁcient methods for Bayesian operational modal analysis
(OMA) using data from multiple setups performed in different time periods and
covering possibly different sets of DOFs. This context is demanded by practical
situations where it is desired to obtain the ‘global mode shape’ of a structure with more
measured DOFs than the number of synchronous data channels. The global mode
shape is the main target, as other modal properties (e.g., natural frequency, damping
ratio) can otherwise be identiﬁed separately using the data in individual setups.
This chapter is structured somewhat differently from the previous two. It ﬁrst
introduces (non-Bayesian) least squares methods for assembling the global mode
shape directly from local ones. These heuristic methods are easier to understand and
allow one to appreciate the basic nature of the mode shape assembly problem. They
yield a global mode shape asymptotically the same as the most probable mode
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_14
419

shape in a Bayesian approach when the quality of local mode shapes is high. As
least squares methods only make use of local mode shapes, they ignore the
information in other modal properties, which matters when the quality of data in
some setup is poor. The Bayesian algorithm developed in this chapter is limited to
well-separated modes. The techniques for determining the posterior statistics
leverage on those for single-setup problem (Chap. 12). Efﬁcient algorithm for
multiple modes with multiple setup data has not been developed.
We ﬁrst recall the problem context from Sect. 10.3, simplifying for the case of a
single mode dominating the selected frequency band. Ambient vibration data is
collected from q setups, each performed in different time periods and measuring a
possibly different set of DOFs. Modal properties other than mode shape can pos-
sibly vary in different setups. The local mode shape in Setup r, i.e., conﬁned to the
measured DOFs in the setup, is denoted by tr (nr  1). The global mode shape u
(n  1) covers the measured DOFs in all setups. It is scaled to have unit norm, i.e.,
uTu ¼ 1. Each tr is related to u by tr ¼ Lru, where Lr (nr  n) is a selection
matrix. The ði; jÞ-entry of Lr is 1 if the i th data channel in Setup r measures DOF j
of u. Different setups are connected through some ‘reference DOFs’ they have
measured in common, although the same set of reference DOFs need not appear in
all setups. The basic mechanism of mode shape assembly is to enforce a common
scaling among the local mode shapes via the reference DOFs.
14.1
Local Least Squares
Least squares approach provides a heuristic means for determining the global mode
shape from local ones. Let ~tr (nr  1) be the local mode shape identiﬁed using the
data in Setup r, with unit norm, i.e., ~tT
r ~tr ¼ 1. It can be the most probable mode
shape obtained from a single-setup algorithm in Chaps. 12 or 13, or more generally
the mode shape estimate from a non-Bayesian method. Given f~trgq
r¼1, the ‘best’
global mode shape ~u is determined so that its local counterparts fLr ~ugq
r¼1 are
‘closest’ to f~trgq
r¼1 in a predeﬁned least squares sense. This only makes use of the
identiﬁed local mode shapes, regardless of whether the mode is well-separated from
others or not. Simplicity is a clear advantage, at the expense of being heuristic (e.g.,
why ‘square’) and ignoring the information of other modal properties that are
potentially useful.
In the ‘local least squares method’, the identiﬁed mode shape of one setup is
scaled relative to another so that their common DOFs agree in a least squares sense.
Applying this to different pairs of setups and combining the results from all setups
gives the global mode shape. Without loss of generality, let the setups be 1 and 2,
and let ~u and ~v be respectively their mode shapes conﬁned to the reference DOFs
they share in common. If one takes Setup 1 as reference, then ~v is scaled by c so that
c~v is closest to ~u in a least squares sense. That is, c minimizes the following
objective function:
420
14
Multi-setup Problem

JðcÞ ¼ ðc~v  ~uÞTðc~v  ~uÞ ¼ ð~vT~vÞc2  2ð~vT~uÞc þ ð~uT~uÞ
ð14:1Þ
Solving for dJ=dc ¼ 0 gives the least squares estimate of c:
~c ¼ ~vT~u
~vT~v
ð~u ¼ referenceÞ
ð14:2Þ
Check that @2J=@c2 ¼ 2ð~vT~vÞ [ 0 and so ~c minimizes J. To obtain the global mode
shape comprising the DOFs of Setups 1 and 2, the mode shape value of a reference
DOF is determined as the average of the two setups. The mode shape value at a
non-reference DOF is simply equal to the scaled value. The global mode shape is
normalized to have unit norm.
Example 14.1 (Six-storied building, local least squares) Revisit the multi-setup
scenario in Example 10.3, now shown in Fig. 14.1. Suppose the data in each setup
was analyzed individually, yielding the local mode shapes in Table 14.1 that have
been scaled to have unit norm. To obtain the global mode shape, we take Setup 2
(say) as reference and scale the mode shape of Setup 1 so that the common DOFs 3
and 6 best ﬁt those of Setup 2. Using (14.2),
~u ¼ ½0:420; 0:804T
and
~v ¼ ½0:454; 0:642T, which gives ~c ¼ 1:143. The negative sign reﬂects that
the local mode shapes of Setups 1 and 2 are given in opposite sense. The scaled
mode shape of Setup 1 is shown in Table 14.2, which is now in the same sense of
Setup 2. Its norm shown at the bottom is equal to the absolute value of the
DOF 5
All DOFs
DOF 6
DOF 3
DOF 4
DOF 1
DOF 2
A
B
C
A
B
B
B
C
Setup 1
Setup 2
Setup 3
Setup 4
C
C
Fig. 14.1 Setup plan of six-storied building example
Table 14.1 Local mode
shape (with unit norm) in
each setup
DOF\Setup
1
2
3
4
1
0.284
2
0.421
0.470
3
−0.454
0.420
4
−0.618
5
0.883
0.959
6
−0.642
0.804
Norm
1
1
1
1
14.1
Local Least Squares
421

multiplier because the original local mode shape in Table 14.1 has unit norm. No
scaling is applied to Setup 2 (reference) and so its columns in Tables 14.1 and 14.2
are the same. Setup 3 is also scaled using Setup 2 as reference, where the common
DOF is 2. Using Eq. (14.2), ~u ¼ 0:421 and ~v ¼ 0:470, which gives ~c ¼ 0:896.
Note that this is simply the ratio 0.421/0.470 because DOF 2 is the only common
DOF. For Setup 4, it does not have any DOF in common with Setup 2 and so the
latter cannot be taken as reference. Instead, Setup 3 is used as reference, with
common DOF 5. Since Setup 3 is already scaled, matching at DOF 5 using (14.2)
corresponds to ~u ¼ 0:791 (not 0.883) and ~v ¼ 0:959, which gives ~c ¼ 0:825. The
local mode shapes in Table 14.2 are said to have been scaled to best ﬁt in the local
least squares sense described above. The global mode shape is obtained by aver-
aging the mode shape values across the setups. The result is shown under the
column ‘Averaged’. Dividing this mode shape by its norm (1.472, shown at the
bottom) gives the ﬁnal assembled global mode shape with unit norm, shown under
the column ‘Normalized’.
■
14.2
Global Least Squares
Local least squares method requires the choice of a reference setup. When there is
no setup that shares some DOFs with all other setups, it must be applied repeatedly
to different setup pairs, which is not convenient for implementation. The choice of
reference setup(s) affects the quality of results. Its non-uniqueness is an indication
of a possible suboptimal solution where the information in the local mode shapes
has not been fully utilized. These issues motivated the ‘global least squares method’
that aims at ﬁtting the local mode shapes f~trgq
r¼1 in an overall sense. Speciﬁcally,
the global mode shape is determined as the u that minimizes the following
objective function subjected to unit norm constraint uTu ¼ 1:
JðuÞ ¼
X
q
r¼1
wrðtr  jjtrjj~trÞTðtr  jjtrjj~trÞ
tr ¼ Lru
ð14:3Þ
Table 14.2 Scaled local mode shapes and global mode shape (last column)
DOF\Setup
1
2
3
4
Averaged
Normalized
1
0.234
0.234
0.159
2
0.421
0.421
0.421
0.286
3
0.519
0.420
0.469
0.319
4
0.707
0.707
0.480
5
0.791
0.791
0.791
0.537
6
0.734
0.804
0.769
0.522
Norm
1.143
1.000
0.896
0.825
1.472
1.000
422
14
Multi-setup Problem

where fwr  0gq
r¼1 are chosen weights with Pq
r¼1 wr ¼ 1. In the summand in
(14.3), tr ¼ Lru and jjtrjj ¼ uTLT
r Lru are functions of u but these are not written
explicitly for simplicity. The difference tr  jjtrjj~tr measures the discrepancy
between the local mode shape tr and its identiﬁed counterpart jjtrjj~tr. The latter
has been scaled to have the same norm as tr (recall that jj~trjj ¼ 1), ensuring a
proper measure of vector proximity.
The norm jjtrjj in (14.3) makes the optimal solution of u non-trivial to ﬁnd. This
can be resolved by introducing the auxiliary variables cr ¼ jjtrjj2 (r ¼ 1; . . .; q).
Using Lagrange multiplier method (Sect. C.3), the global mode shape is determined
from the stationary solution of the following objective function (Lagrangian):
Jðu; c; kÞ ¼
X
q
r¼1
wr
ðtr 
ﬃﬃﬃﬃcr
p ~trÞTðtr 
ﬃﬃﬃﬃcr
p ~trÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
discrepancy in local mode shape
þ
X
q
r¼1
krðtT
r tr  crÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
enforce cr ¼ jjtrjj2
þ
k0ð1  uTuÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
enforce jjujj2 ¼ 1
ðtr ¼ LruÞ
ð14:4Þ
where c ¼ ½c1; . . .; cq and k ¼ ½k0; k1; . . .; kq comprises the Lagrange multipliers.
It has not been possible to determine simultaneously the optimal value of
fu; c; kg in closed form. The next section shows that the optimal value of each
parameter can be obtained in terms of the remaining ones. These partial solutions
can be used to determine the optimal solution through an iterative algorithm.
14.2.1
Partial Solutions
The optimal solution of fcrgq
r¼1 and fkrgq
r¼1 can be obtained in terms of u as
follow. Solving for @J=@cr ¼ 0 from (14.4) gives
cr ¼
~tT
r tr
1  kr=wr

2
ð14:5Þ
Combining with the constraint cr ¼ jjtrjj2
gives kr ¼ wrð1  j~tT
r trj=jjtrjjÞ.
Evaluating
@2J=@c2
r
at
(14.5)
gives
@2J=@c2
r ¼ wrð1  kr=wrÞ3=2ð~tT
r trÞ2.
Requiring this to be positive (looking for minimum) eliminates the larger root of kr.
In terms of u (through tr ¼ Lu), the optimal values of cr and kr are then given by
cr ¼ jjtrjj2
kr ¼ wr
1  j~tT
r trj
jjtrjj


r ¼ 1; . . .; q
ð14:6Þ
Since j~tT
r trj=jjtrjj  jj~trjjjjtrjj=jjtrjj ¼ 1,
14.2
Global Least Squares
423

0  ~kr  wr
ð14:7Þ
To derive the optimal solution of u in terms of c and k, rewrite (14.4) explicitly
as a quadratic function of u:
J ¼ uTAu þ 2uTb þ k0ð1  uTuÞ þ C
ð14:8Þ
where
A
nn ¼
X
q
r¼1
ðwr þ krÞ LT
r
nnr
Lr
nrn
b
n1 ¼ 
X
q
r¼1
wr
ﬃﬃﬃﬃcr
p
LT
r
nnr
~tr
nr1
C ¼
X
q
r¼1
ðwr  krÞcr
ð14:9Þ
Note that A is positive deﬁnite since wr þ kr  0 and for any u there is at least one
setup with local mode shape norm jjtrjj ¼ jjLrujj [ 0 so that
uTAu ¼
X
q
r¼1
ðwr þ krÞuTLT
r Lru ¼
X
q
r¼1
ðwr þ krÞjjLrujj2 [ 0
ð14:10Þ
Setting the gradient of J w.r.t. u to zero gives a constrained eigenvalue problem:
Au þ b ¼ k0u
ð14:11Þ
This can be solved by constructing an augmented vector that satisﬁes the standard
eigenvalue problem (Sect. C.4). As a result, for given c and k, the optimal global
mode shape can be obtained as the ﬁrst half of the eigenvector with the smallest
real eigenvalue of
Dðc; kÞ
2n2n
¼
AðkÞ
bðcÞbðcÞT
In
AðkÞ


ð14:12Þ
where the dependence of A on k and b on c has been indicated.
14.2.2
Limiting Behavior of Solution
When the identiﬁed local mode shapes f~trgq
r¼1 match well at the reference DOFs,
tr=jjtrjj 	 ~tr at the optimum. Equation (14.6) then implies kr 	 0 and (14.5)
implies cr 	 ð~tT
r trÞ2 ¼ ð~tT
r LruÞ2. Substituting these into (14.9) gives, at the
optimum,
424
14
Multi-setup Problem

A 	
X
q
r¼1
wrLT
r Lr
b 	 
X
q
r¼1
wrLT
r ~tr~tT
r Lru
ð14:13Þ
Substituting into (14.8) and optimizing w.r.t. u shows that the optimal global mode
shape is the eigenvector of the following matrix with the smallest eigenvalue:
A0
nn ¼
X
q
r¼1
wr LT
r
nnr
ð Inr
nrnr
 ~tr
nr1
~tT
r
1nr
Þ Lr
nrn
ð14:14Þ
This matrix can be computed directly from the identiﬁed local mode shapes
f~trgq
r¼1. The optimal global mode shape in the limiting case can then be obtained
without iterations.
14.2.3
Iterative Algorithm
The global mode shape u that best ﬁts the identiﬁed local mode shapes
f~trgq
r¼1ðjj~trjj ¼ 1Þ in a global least squares sense can be determined by the fol-
lowing iterative algorithm. In a given setup, parameters not updated are kept at their
latest value.
Global Least Squares Algorithm
Step 0. Set initial guess of u as the eigenvector (smallest eigenvalue) of A0 in
(14.14)
Step 1. Update fcrgq
r¼1 and fkrgq
r¼1 using (14.6)
Step 2. Update u as the ﬁrst half of the eigenvector (smallest real eigenvalue)
of D in (14.12).
Repeat Steps 1 and 2 until convergence.
When the quality of local mode shapes is good (they agree at the common
DOFs) the results are insensitive to the choice of weights fwrgq
r¼1. In the general
case, Sect. 14.3.6 suggests that the weights may be chosen proportional to the
modal signal-to-noise (s/n) ratio of the setup so that the global least squares solution
coincides asymptotically with the most probable global mode shape in the Bayesian
approach.
14.2
Global Least Squares
425

Example 14.2 (Six-storied building, global least squares) Consider applying the
global least squares method to Example 14.1. The selection matrices are
L1 ¼
0
0
0
0
0
1
0
0
1
0
0
0
0
0
0
1
0
0
2
64
3
75
L2 ¼
0
0
0
0
0
1
0
0
1
0
0
0
0
1
0
0
0
0
2
64
3
75
L3 ¼
0
0
0
0
1
0
0
1
0
0
0
0


L4 ¼
0
0
0
0
1
0
1
0
0
0
0
0


ð14:15Þ
From Table 14.1, the identiﬁed local mode shapes (with unit norm) are
~t1 ¼
0:642
0:454
0:618
2
4
3
5
~t2 ¼
0:804
0:420
0:421
2
4
3
5
~t3 ¼
0:883
0:470


~t4 ¼
0:959
0:284


ð14:16Þ
where the local DOFs follow the alphabetical order (A, B, C). Using uniform
weights, i.e., wr ¼ 1=4 ðr ¼ 1; . . .; 4Þ, the iterative algorithm yields the best global
mode shape (with unit norm) as
u ¼ 0:162
0:286
0:315
0:477
0:546
0:516
½
T:
■
14.2.4
Reference Condensation
The global least squares algorithm involves repeated solution (during iterations) of
the eigenvalue problem for D in (14.12) of dimension n (the total number of DOFs),
which can be large in applications. It is possible to reduce the problem size by
observing the theoretical fact that non-reference DOFs retain their ‘shape’ in the
assembled global mode shape (Au 2011). Speciﬁcally, let uR (n00  1) denote the
mode shape containing the reference DOFs (no repetitions) from all setups; u0
r
denote the local mode shape in Setup r conﬁned to non-reference DOFs and ~u0
r
denote its identiﬁed counterpart. Note that a reference DOF is one that is measured
by more than one data channel, regardless of whether it is in the same or different
setups. Suppose the global mode shape is arranged as u ¼ ½uR; u0
1;    ; u0
q. Then it
can be reasoned that, for the optimal global mode shape, u0
r ¼ ar ~u0
r=jj~u0
rjj where ar
is a multiplier to be determined. The optimal global mode shape is then of the form
426
14
Multi-setup Problem

u ¼
uR
a1~u0
1= ~u0
1


...
aq~u0
q= ~u0
q


2
66664
3
77775
¼
In00
~u0
1= ~u0
1


..
.
~u0
q= ~u0
q


2
66664
3
77775
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Lc
uR
a1
..
.
aq
2
6664
3
7775
|ﬄﬄﬄ{zﬄﬄﬄ}
uc
¼ Lc
nn0 uc
n01
ð14:17Þ
where n0 (n00  n0  n) is the number of condensed parameters. Determining u then
reduces to determining uc. Note that u0
r and ar can be null (empty) if all DOFs in
Setup r are reference DOFs. As LT
c Lc ¼ In0 (identity matrix), uc has unit norm
because uT
c uc ¼ uT
c LT
c Lcuc ¼ uTu ¼ 1. Substituting u ¼ Lcuc into (14.11) and
pre-multiplying by LT
c gives a constrained eigenvalue problem of dimension n0:
Acuc þ bc ¼ k0uc
ð14:18Þ
Ac
n0n0 ¼ LT
c
n0n
A
nn Lc
nn0
bc
n01
¼ LT
c
n0n
b
n1
ð14:19Þ
where A and b are given by (14.9). The solution of uc is given by the ﬁrst half of
the real eigenvector (smallest real eigenvalue, unit norm) of
Dc
2n02n0 ¼
LT
c ALc
LT
c bbTLc
In0
LT
c ALc


ð14:20Þ
Step 2 of the algorithm in Sect. 14.2.3 can be modiﬁed to ﬁrst determining uc
and then recovering u ¼ Lcuc. It is more efﬁcient especially when n 
 n0. This
condensed procedure assumes that the DOFs are arranged as u ¼ ½uR; u0
1;    ; u0
q,
i.e., reference DOFs ﬁrst, followed by non-reference DOFs of each setup. In the
actual implementation, a re-mapping of DOFs is necessary to ﬁt the analyst’s
preference of DOF ordering.
In Example 14.1, DOFs 2, 3, 5 and 6 appear in more than one setup and so
uR ¼ ½/2; /3; /5; /6T. For the non-reference DOFs, u0
1 ¼ ½/4, u0
2 and u0
3 are null,
and
u0
4 ¼ ½/1.
This
gives,
in
(14.17),
n00 ¼ 4,
Lc ¼ I6
and
uc ¼ ½/2; /3; /5; /6; a1; a4T. In this example the condensed procedure has no
computational advantage because the number of condensed parameters (n0 ¼ 6) is
the same as the total number of DOFs (n ¼ 6).
14.3
Bayesian Method
We now consider an algorithm for identifying the global mode shape using data
from multiple setups, following a Bayesian approach and assuming a single mode
in the selected frequency band. Recall the context from Sect. 10.3. Let
14.2
Global Least Squares
427

f^yðrÞ
j gNr1
j¼0
ðnr  1Þ be the ambient vibration data in Setup r and f ^F ðrÞ
k g ðnr 
1 complexÞ be the collection of scaled Fast Fourier Transform (FFT) in the selected
band:
^F ðrÞ
k
¼
ﬃﬃﬃﬃﬃﬃﬃ
Dtr
Nr
s
X
Nr1
j¼0
^yðrÞ
j e2pijk=Nr
ð14:21Þ
where Dtr is the sampling interval. Using the FFTs from all setups, the negative
log-likelihood function (NLLF) for identifying the set of modal parameters h is
LðhÞ ¼  ln pðf ^F ð1Þ
k g;    ; f ^F ðqÞ
k gjhÞ ¼
X
q
r¼1
Lr
ð14:22Þ
where, for Setup r,
Lr ¼ nrNðrÞ
f
ln p þ
X
k
ln jEðrÞ
k j þ
X
k
^F
ðrÞ
k
EðrÞ1
k
^F ðrÞ
k
ð14:23Þ
is the NLLF; NðrÞ
f
is the number of FFT points in the selected band;
EðrÞ
k
¼ SðrÞDðrÞ
k trtT
r þ SðrÞ
e Inr
tr ¼ Lru
ð14:24Þ
is the theoretical power spectral density (PSD) matrix of data;
DðrÞ
k
¼
ð2pfðrÞ
k Þ2q
ð1  bðrÞ2
k
Þ2 þ ð2fðrÞbðrÞ
k Þ2
bðrÞ
k
¼ f ðrÞ
fðrÞ
k
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð14:25Þ
f ðrÞ is the natural frequency in Hz, fðrÞ is the damping ratio, SðrÞ is the modal force
PSD and SðrÞ
e
is the prediction error PSD. The set of modal parameters h comprises
ff ðrÞ; fðrÞ; SðrÞ; SðrÞ
e gq
r¼1 and u, subjected to unit norm constraint uTu ¼ 1.
14.3.1
Alternative Form of NLLF
To facilitate determining the posterior MPV of h, the NLLF is rewritten using
similar techniques in Chap. 12. Let
428
14
Multi-setup Problem

tr ¼ jjtrjj1tr
ð14:26Þ
be the normalized counterpart of tr; and
S0ðrÞ ¼ SðrÞjjtrjj2
ð14:27Þ
be the modal force PSD of Setup r when the local mode shape is scaled to have unit
norm. Note that SðrÞ is the modal force PSD of Setup r when the global mode shape
is scaled to have unit norm. Working with fS0ðrÞgq
r¼1 instead of fSðrÞgq
r¼1 leads to
the same form of the NLLF as in a single-setup problem, allowing a similar
mathematical treatment.
Using (14.26) and (14.27), (14.24) is rewritten as
EðrÞ
k
¼ S0ðrÞDðrÞ
k trtT
r þ SðrÞ
e Inr
ð14:28Þ
Since jjtrjj ¼ 1, this is of the same form as the theoretical PSD in a single-setup
problem and so the results in Sect. 12.1 can be applied to give
jEðrÞ
k j ¼ S0ðrÞDðrÞ
k þ SðrÞ
e
h
i
SðrÞ
e
nr1
ð14:29Þ
EðrÞ1
k
¼ SðrÞ1
e
Inr 
1 þ
SðrÞ
e
S0ðrÞDðrÞ
k
"
#1
trtT
r
8
<
:
9
=
;
ð14:30Þ
Substituting into (14.23) gives the same alternative form in a single-setup problem:
Lr ¼ nrNðrÞ
f
ln p þ
X
k
ln S0ðrÞDðrÞ
k þ SðrÞ
e
h
i
þ ðnr  1ÞNðrÞ
f
ln SðrÞ
e þ 1
SðrÞ
e
^dðrÞ  tT
r AðrÞtr
h
i
ð14:31Þ
AðrÞ
nrnr ¼
X
k
1 þ
SðrÞ
e
S0ðrÞDðrÞ
k
"
#1
^D
ðrÞ
k
nrnr
ð14:32Þ
^D
ðrÞ
k
nrnr
¼ Re½ ^F
ðrÞ
k
nr1
^F
ðrÞ
k
1nr

^dðrÞ
scalar ¼
X
k
^F
ðrÞ
k
1nr
^F
ðrÞ
k
nr1
ð14:33Þ
Using L ¼ Pq
r¼1 Lr, substituting tr ¼ Lru=jjtrjj and collecting terms leads to a
form for determining the most probable global mode shape:
14.3
Bayesian Method
429

L ¼ ðln pÞ
X
q
r¼1
nrNðrÞ
f
þ
X
q
r¼1
ðnr  1ÞNðrÞ
f
ln SðrÞ
e þ
X
q
r¼1
^dðrÞ
SðrÞ
e
þ
X
q
r¼1
X
k
ln S0ðrÞDðrÞ
k þ SðrÞ
e
h
i
 uTAu
ð14:34Þ
A
nn ¼
X
q
r¼1
jjtrjj2SðrÞ1
e
LT
r
nnr
AðrÞ
nrnr Lr
nrn
ð14:35Þ
14.3.2
Partial MPV of Global Mode Shape
The NLLF in (14.34) depends on u through uTAu. This term is not quadratic in u
because A in (14.35) depends on u through jjtrjj2 ¼ uTLT
r Lru. To obtain the MPV
of u in terms of the remaining parameters, introduce the auxiliary parameters
fcrgq
r¼1 to replace fjjtrjj2gq
r¼1 so that A reads
A ¼
X
q
r¼1
c1
r SðrÞ1
e
LT
r AðrÞLr
ð14:36Þ
The NLLF should now be minimized w.r.t. ff ðrÞ; fðrÞ; S0ðrÞ; SðrÞ
e gq
r¼1, u and fcrgq
r¼1
subjected to constraints uTu ¼ 1 and
cr ¼ uTLT
r Lru
r ¼ 1; . . .; q
ð14:37Þ
The constraints can be incorporated by considering the Lagrangian:
J ¼ L þ k0ð1  uTuÞ þ
X
q
r¼1
krðuTLT
r Lru  crÞ
ð14:38Þ
where fkrgq
r¼0 are Lagrange multipliers.
The MPV of u and fkrgq
r¼1 can be determined analytically in terms of the
remaining parameters. Setting the gradient of J w.r.t. u to zero gives the standard
eigenvalue problem
Bu ¼ k0u
ð14:39Þ
B ¼
X
q
r¼1
krLT
r Lr
 
!
 A
ð14:40Þ
where A is given by (14.36). Note that B depends on fcrgq
r¼1 (through A) and
fkrgq
r¼1. Evaluating L at an eigenvector of B gives a value proportional to k0. This
implies that the MPV of u is the eigenvector of B with the smallest eigenvalue.
430
14
Multi-setup Problem

Setting
the
derivative
of
J
w.r.t.
cr
and
solving
for
kr
gives
kr ¼ uTLT
r AðrÞLru=c2
rSðrÞ
e . Substituting cr ¼ uTLT
r Lru gives the MPV in terms of
other parameters
kr ¼ tT
r AðrÞtr
SðrÞ
e ðtT
r trÞ2
tr ¼ Lru
r ¼ 1; . . .; q
ð14:41Þ
14.3.3
Algorithm for MPV
The MPV of h ¼ fff ðrÞ; fðrÞ; S0ðrÞ; SðrÞ
e gq
r¼1; ug can be determined by the iterative
algorithm below. After that, fSðrÞgq
r¼1 can be recovered from SðrÞ ¼ S0ðrÞ=jjLrujj2.
In a given step, parameters not updated are kept at their latest value.
Iterative Algorithm for MPV
Step 0. Set
initial
guess
of
ff ðrÞ; fðrÞ; S0ðrÞ; SðrÞ
e gq
r¼1,
u,
fkrgq
r¼1
(see
Sect. 14.3.5).
Step 1. For r ¼ 1; . . .; q, update ff ðrÞ; fðrÞ; S0ðrÞ; SðrÞ
e g by minimizing Lr in
(14.31).
Step 2. Update u as the eigenvector (smallest eigenvalue, unit norm) of B in
(14.40).
Step 3. Update fkrgq
r¼1 using (14.41) and fcrgq
r¼1 using (14.37).
Repeat Steps 1 to 3 until convergence.
This algorithm is a modiﬁed version of that in Au and Zhang (2012). By
working with S0ðrÞ instead of SðrÞ, it involves in Step 1 optimizing the same NLLF as
in a single-setup problem (Chap. 12).
14.3.4
High s/n Asymptotic MPV
As in a single-setup problem, the MPV of modal parameters for multi-setup
problem exhibits characteristic behavior when the modal signal-to-noise (s/n) ratio
is high, in the sense that in the selected frequency band,
14.3
Bayesian Method
431

cðrÞ
k
¼ S0ðrÞDðrÞ
k
SðrÞ
e

 1
r ¼ 1; . . .; q
ð14:42Þ
Since L ¼ Pq
r¼1 Lr and each NLLF Lr depends on S0ðrÞ and SðrÞ
e
in the same
manner as in the case of single-setup problem, one can apply the results in Sect. 12.3
to obtain the asymptotic MPV as
^SðrÞ
e 	
dr  ^dðrÞ
ðnr  1ÞNðrÞ
f
^S0ðrÞ 	
1
NðrÞ
f
X
k
^dðrÞ
k
DðrÞ
k
ð14:43Þ
where ^dðrÞ
k
¼ ^tT
r DðrÞ
k ^tr=^tT
r ^tr and ^tr ¼ Lr ^u; a hat (‘^’) denotes that the quantity is at
the MPV. These are the same as those in a single-setup problem.
Determining the asymptotic MPV of the global mode shape requires a somewhat
different argument from that for single-setup problem because of the Lagrange
multiplier terms. Applying (14.42) on (14.32),
AðrÞ 	 AðrÞ
0
¼
X
k ^D
ðrÞ
k
ð14:44Þ
Let faðrÞ
i gnr
i¼1 be the eigenvalues (in ascending order of magnitude) of AðrÞ
0 . As in a
single-setup problem, it can be reasoned that asymptotically ^tr is the eigenvector
with the largest eigenvalue of AðrÞ
0 . Using (14.41) at the MPV,
^kr 	
aðrÞ
nr
^SðrÞ
e jj^trjj2
r ¼ 1; . . .; q
ð14:45Þ
since ^tT
r AðrÞ
0 ^tr=^tT
r ^tr ¼ aðrÞ
nr . Substituting into (14.40) and using
A 	
X
q
r¼1
jj^trjj2^SðrÞ1
e
LT
r AðrÞ
0 Lr
ð14:46Þ
implied from (14.35), we have
B 	
X
q
r¼1
^SðrÞ1
e
jj^trjj2LT
r aðrÞ
nr Inr  AðrÞ
0
h
i
Lr
ð14:47Þ
The bracketed term in the summand is positive semi-deﬁnite because all eigen-
values of AðrÞ
0
are no greater than aðrÞ
nr . Its smallest eigenvalue is zero with eigen-
vector ^tr ¼ Lr ^u. It follows that B^u 	 0 and so asymptotically the most probable
global mode shape ^u is a null vector of B.
432
14
Multi-setup Problem

14.3.5
Initial Guess
Initial guess for the iterative algorithm in Sect. 14.3.3 can be set as follow. The
initial guess for the natural frequency f ðrÞ can be picked from the singular value
(SV) spectrum of data in Setup r. The initial guess for the damping ratio can be set
as 1%. The initial guess for the global mode shape can be taken as the eigenvector
(unit norm) of B0 with the smallest eigenvalue:
B0 ¼
X
q
r¼1
LT
r aðrÞ
nr Inr  AðrÞ
0
h
i
Lr
ð14:48Þ
where aðrÞ
nr is the largest eigenvalue of AðrÞ
0
¼ P
k ^D
ðrÞ
k
in (14.44). The matrix B0 is
adapted from B in (14.47) by removing the unknown factor ^SðrÞ
e jj^trjj2, which
asymptotically does not affect the null vector. The initial guess for the local mode
shapes can be extracted from that for the global mode shape. Substituting the initial
guess for the local mode shapes into (14.43) gives the initial guess for S0ðrÞ and SðrÞ
e .
The initial guess for kr can be determined from (14.45).
14.3.6
Asymptotic Weight for Global Least Squares
For high modal s/n ratio, the most probable global mode shape is closely related to the
global least squares estimate in Sect. 14.2. The relationship leads to a simple choice for
the weights in the global least squares method. Let fbðrÞ
i gnr
i¼1 be the eigenvectors (unit
norm, in ascending order of eigenvalues) of AðrÞ
0
¼ P
k ^D
ðrÞ
k
in (14.44). Then bðrÞ
nr ¼
^tr=jj^trjj ¼ ^tr (say). Substituting AðrÞ
0
¼ aðrÞ
nr ^tr^t
T
r þ Pnr1
i¼1 aðrÞ
i bðrÞ
i bðrÞT
i
into (14.47),
B 	
X
q
r¼1
aðrÞ
nr
SðrÞ
e jj^trjj2 LT
r ðInr  ^tr^t
T
r ÞLr 
X
q
r¼1
X
nr1
i¼1
aðrÞ
i
SðrÞ
e jj^trjj2 LT
r bðrÞ
i bðrÞT
i
Lr
ð14:49Þ
Post-multiplying the second sum by ^u gives a zero vector because Lr ^u ¼ ^tr ¼
jj^trjjbnr is orthogonal to fbignr1
i¼1 . This implies that asymptotically the most
probable global mode shape is a null vector of the ﬁrst sum. Comparing the ﬁrst
sum with (14.14) and taking ~tr ¼ ^tr suggests that the weights fwrgq
r¼1 in the global
least squares method may be chosen as
wr /
aðrÞ
nr
SðrÞ
e jj^trjj2
ð14:50Þ
so that its estimate is asymptotically the same as the most probable global mode
shape in the Bayesian approach. The quantities on the RHS can all be determined
by analyzing individually the data in Setup r.
14.3
Bayesian Method
433

14.3.7
Posterior Covariance Matrix
Under a Gaussian approximation of the posterior PDF, the posterior covariance
matrix of the set of modal parameters h (nh  1) satisfying constraints is given by
(Sect. 11.2)
^C
nhnh ¼ ðr^vcÞ
nhp
ðr2^LcÞ þ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
pp
ðr^vcÞT
pnh
ð14:51Þ
where ‘+’ denotes the pseudo-inverse, i.e., ignoring zero eigenvalues from con-
straint singularity; r2^Lc is the Hessian of LcðuÞ ¼ LðvcðuÞÞ w.r.t. a set of free
parameters u (p  1); vcðuÞ maps u to h that always satisﬁes the constraints
GiðvcðuÞÞ ¼ 0 (i ¼ 1; . . .; nc); GðhÞ ¼ ½G1ðhÞ; . . .; GncðhÞT. A hat ‘^’ on quantity
denotes that it is evaluated at the MPV. Using the formula based on Lagrange
multiplier (Sect. 11.2.3),
r2^Lc
pp ¼ ðr^vcÞT
pnh
r2^L
nhnh þ
X
nc
i¼1
^ki r2 ^Gi
nhnh
 
!
r^vc
nhp
ð14:52Þ
where r2^L is the Hessian of the NLLF in (14.22) (ignoring constraints); and
^k ¼
^k1
...
^knc
2
64
3
75 ¼ ½r^G
ncnh ðr^GÞT
nhnc
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
ncnc
1 ðr^GÞ
ncnh
ðr^LÞT
nh1
ð14:53Þ
is a vector of Lagrange multipliers at the MPV. Note that f^kignc
i¼1 here are not those
in Sect. 14.3.2 when obtaining the MPV.
The main derivation and programing effort for calculating ^C stems from r2^L.
This depends on the deﬁnition of h, which affects how it appears in LðhÞ and the
deﬁnitions of vcðuÞ and GðhÞ. It is possible to leverage on the computer code for
single-setup problem (Chap. 12), by making use of the additive relationship:
LðhÞ ¼
X
q
r¼1
LrðhrÞ
ð14:54Þ
where Lr is the NLLF in (14.31) that depends only on the parameters in Setup r,
i.e., hr ¼ ff ðrÞ; fðrÞ; S0ðrÞ; SðrÞ
e ; trg. Deﬁne
-r ¼ ½f ðrÞ; fðrÞ; S0ðrÞ; SðrÞ
e T
ð14:55Þ
434
14
Multi-setup Problem

u ¼
-1
..
.
-q
u
2
6664
3
7775
h ¼
-1
..
.
-q
t1
..
.
tq
u
2
6666666664
3
7777777775
vcðuÞ ¼
-1
..
.
-q
L1u=jjujj
...
Lqu=jjujj
u=jjujj
2
66666666664
3
77777777775
GðhÞ ¼
t1  L1u
..
.
tq  Lqu
1  uTu
2
6664
3
7775
ð14:56Þ
Here,
u
has
p ¼ 4q þ n
parameters;
h
has
nh ¼ 4q þ Rnr þ n
parameters
ðRnr ¼ n1 þ    þ nrÞ; and G contains nc ¼ Rnr þ 1 constraints. Check that
GðvcðuÞÞ ¼ 0 for any u. Since ftrgq
r¼1 are explicitly contained in h, they can be
treated as independent variables when taking partial derivatives of L. The partial
derivative of L ¼ Pq
r¼1 Lr w.r.t. u is zero because Lr depends on u only through
tr, but tr has already been included in h. The global mode shape u is deliberately
included in h, for otherwise it is difﬁcult to express systematically in G the con-
straints among the local mode shapes ftrgq
r¼1. Since tr ¼ tr=jjtrjj instead of tr
appears in the expression of Lr in (14.31), the gradient and Hessian of Lr w.r.t.
hr ¼ ff ðrÞ; fðrÞ; S0ðrÞ; SðrÞ
e ; trg is equal to those of the NLLF for single-setup problem
with unit norm constraint in the local mode shape. This can be computed by exactly
the same computer code developed for single-setup problem.
To clarify, apply the procedure in Sect. 12.4 for Setup r to obtain the Hessian of
Lr w.r.t. hr with constraint jjtrjj ¼ 1. Expand it (ﬁlling zeros) to a nh  nh matrix
so that it corresponds to the Hessian of Lr w.r.t. h. The sum of the resulting matrices
(r ¼ 1; . . .; q) is the Hessian of L w.r.t. h, whose pseudo-inverse gives the posterior
covariance matrix of h.
The terms in (14.52) are determined as follow. Direct differentiation and eval-
uating at the MPV gives
r^vc
nhp ¼
I
4q4q
L1ðIn  ^u^uTÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
n1n
..
.
LqðIn  ^u^uTÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
nqn
In  ^u^uT
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
nn
2
66666666666664
3
77777777777775
ð14:57Þ
14.3
Bayesian Method
435

r^G
ncnh ¼
0
n14q
I
n1n1
L1
n1n
...
..
.
...
0
nq4q
I
nqnq
Lq
nqn
0
14q
0
1n1
  
0
1nq
2^uT
1n
2
66666664
3
77777775
ð14:58Þ
r2 ^Gi ¼
0
nhnh
i ¼ 1; . . .; Rnr
r2 ^G1 þ Rnr ¼
0
ð4q þ RnrÞð4q þ RnrÞ
2I
nn
2
4
3
5
ð14:59Þ
Note that hr ¼ ½-r; tr and so the gradient and Hessian of LrðhrÞ w.r.t. hr at the
MPV can be partitioned as
r^Lr ¼
^Lð-rÞ
r
14
^LðtrÞ
r
1nr


r2^Lr ¼
^Lð-r-rÞ
r
44
^Lð-rtrÞ
r
4nr
sym:
^LðtrtrÞ
r
nrnr
2
64
3
75
ð14:60Þ
These can be obtained from the computer code developed for single-setup problem.
The gradient and Hessian of LðhÞ ¼ Pq
r¼1 LrðhrÞ w.r.t. h at the MPV are related to
those of fLrgq
r¼1 by
r^L
1nh ¼
^Lð-1Þ
1
14
  
^Lð-qÞ
q
14
^Lðt1Þ
1
1n1
  
^LðtqÞ
q
1nq
0
1n


ð14:61Þ
r2^L
nhnh ¼
^Lð-1-1Þ
1
44
^Lð-1t1Þ
1
4n1
..
.
..
.
^Lð-q-qÞ
q
44
^Lð-qtqÞ
q
4nq
^Lðt1t1Þ
1
n1n1
..
.
^LðtqtqÞ
q
nqnq
sym:
0
nn
2
666666666666666664
3
777777777777777775
ð14:62Þ
Coding the posterior covariance matrix using the procedure here is simpler than
using the formulas in Zhang et al. (2015) due to the use of Lagrange multiplier
formula and leverage on computer code developed for single-setup problem.
436
14
Multi-setup Problem

14.4
Representative Statistics
In multi-setup problems, it is assumed that all modal properties except the mode
shape can possibly change over different setups. A direct way to report identiﬁ-
cation results is in terms of the posterior MPV and c.o.v. (standard deviation/MPV)
in each setup. While this gives a detailed picture of each setup, in applications it is
also useful to have some representative statistics for the modal properties without
distinguishing between setups. One empirical way is to think of the modal property
as having equal chance of taking the value in one of the setups (Zhang and Au
2016). Speciﬁcally, let h be the subject modal parameter. It is modeled as
h ¼ hðIÞ
ð14:63Þ
where hðIÞ denotes the modal parameter in Setup I; and I is a discrete-random
variable uniformly distributed on f1; . . .; qg; q is the number of setups. Conditional
on I ¼ r, h has the same distribution as the posterior distribution of hðrÞ. For a given
Setup r, let the posterior MPV and variance of hðrÞ be ^hðrÞ and ^vðrÞ, respectively.
Using conditional expectation, it can be shown that the expectation l and variance v
of h are given by
l ¼ 1
q
X
q
r¼1
^hðrÞ
ð14:64Þ
v ¼ 1
q
X
q
r¼1
½^hðrÞ  l2
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
setupsetup variability
þ
1
q
X
q
r¼1
^vðrÞ
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
identification uncertainty
ð14:65Þ
The expectation l is the sample mean of the MPV ^hðrÞ among the setups. The
variance v is the sum of the sample variance of ^hðrÞ among the setups (ﬁrst term) and
the sample mean of the posterior variances f^vðrÞgq
r¼1 (second term). Clearly, the
second term reﬂects identiﬁcation uncertainty. Although the ﬁrst term is also
affected by identiﬁcation uncertainty, it is dominated by the variability of the modal
parameter among different setups when it is signiﬁcantly larger than the identiﬁ-
cation uncertainty. For this reason, the ﬁrst term may be empirically referred as a
‘setup-setup variability’.
More generally, if the modal parameter is modeled to have value in one of the
setups with probability fwrgq
r¼1 (Pq
r¼1 wr ¼ 1), its mean and variance are given by
l ¼
X
q
r¼1
wr^hðrÞ
v ¼
X
q
r¼1
wr½^hðrÞ  l2 þ
X
q
r¼1
wr^vðrÞ
ð14:66Þ
This is relevant, e.g., when the setups have signiﬁcantly different durations.
14.4
Representative Statistics
437

14.5
Field Applications
In this section, we consider several ﬁeld applications where global mode shapes
with a large number of DOFs are identiﬁed using a small number of sensors. These
examples illustrate some practical aspects and the quality of mode shapes that can
be identiﬁed with limited resources under ambient conditions. The identiﬁed global
mode shapes are interesting in themselves and give a good idea of dynamics. When
the modal s/n ratio and hence the quality of local mode shape is high in all setups,
the global mode shape is of good quality regardless of the method used. Otherwise,
there can be signiﬁcant difference between the results from different methods. The
ﬁrst example illustrates that Bayesian method produces the most reasonable result
even when the modal s/n ratio in some setups is low.
Practical Considerations
Field vibration tests with multiple setups require substantially more planning in
equipment and logistics. In a typical test, a visit was made to the site to observe
accessible locations and environment. A setup plan was made that took into account
equipment, human resources and time constraints. Locations to be measured were
numbered and details such as sensor locations and orientations were clearly indi-
cated. Team members were briefed of the schedule and their responsibility before
the test. For calibration, data with sensors placed together and oriented in the same
direction were obtained and their PSDs examined to spot any anomalies, e.g.,
whether measurement noise was reasonable, whether data channels measuring the
same direction had similar PSDs. This was done in the laboratory before equipment
was transported to the site, and in the site before regular test setups. The latter also
gave an idea of potential modes and ambient environment. The data of each setup
was analyzed shortly after it was acquired. The ﬁrst few modes were identiﬁed to
produce global mode shapes covering the DOFs measured so far. Contingent
planning of setups were made when necessary, e.g., due to failure of data channels
(Example 14.4). Equipment was powered by battery and did not rely on power
supply from the site. In all tests, triaxial servo-accelerometers (noise about 1
lg=
ﬃﬃﬃﬃﬃﬃ
Hz
p
) were used and DAQ (data acquisition) hardware had a 24 bit resolution.
Example 14.4 had six sensors with analogue data transmitted through cables (about
30 m) to a central console and recorded synchronously. In Example 14.3 and
Example 14.5, four sensors were deployed and their data were synchronized by
GPS. In Example 14.6, ﬁve sensors with local DAQ and high precision clocks were
deployed. Their clocks were synchronized shortly before the test but afterwards
they were disconnected and left to run independently during the test.
Example 14.3 (Footbridge) Consider a footbridge 216 m long and continuously
supported by six intermediate piers over the Tolo Harbor in Hong Kong, as shown
in Fig. 14.2a. A three-dimensional global mode shape with 37 locations on each
side of the bridge was identiﬁed with four triaxial servo-accelerometers. The total
number of measured DOFs is n ¼ 37  2  3 ¼ 222. As shown in Fig. 14.2b, two
sensors were placed near the mid- and quarter span in all setups as reference. The
438
14
Multi-setup Problem

other two sensors were placed on two sides and roved through the length of the
bridge, towards the left side in Setups 1–19 and then the right side in Setups 20–37.
When the location of rover sensor coincided with that of the reference, the rover
sensor was placed at the next (for Setup 1) or the previous location (for Setup 24).
At each location, acceleration data was acquired and stored locally by a data
recorder, as shown in Fig. 14.3. Timing at different recorders was synchronized
(b)
36 @ 6m = 216m
Sensor
orientation
12m
Pier
Pier
Pier
Pier
Pier
Pier
Ref.2
Ref.1
Rov.2
Rov.1
Setups 1 to 19 (@ 5 min. data)
Setups 20 to 37 (@ 5 min. data)
Ma On Shan side
CUHK side
(a)
x
y
z
Fig. 14.2 a Overview of footbridge. b Setup plan. Square reference sensor location; dot rover
sensor location. Rover positions shown for Setup 3
Servo
Acc.
DAQ &
recorder
GPS
(clock)
Fig. 14.3 Equipment per location
14.5
Field Applications
439

through GPS receivers. The sensors were placed with the same orientation in all
setups.
Five minutes of data at 200 Hz were acquired in each setup. This was expected
to provide at least a thousand natural periods of data, as the fundamental natural
frequency of the bridge was expected to be a few Hz. The transition between setups,
including transporting equipment, senor orientation and levelling typically took
5 min. Each setup thus required 10 min. Including initial setup and calibration, the
whole series of tests took 7 h (0930–1630 h).
Modal Identiﬁcation
Figure 14.4 shows the SV spectrum of the data in Setup 1. Four well-separated
modes in the spectrum are indicated. The fourth mode indicated in the ﬁgure is not
apparent in this setup but it is more so in other setups. The modes are identiﬁed
separately using the FFT in their bands indicated for this setup. The bands for other
setups are similar.
Figure 14.5 shows the identiﬁed global mode shapes using Bayesian method
incorporating the data in all setups (Sect. 14.3). The ﬁrst mode shows that the DOFs
at the piers do have transverse movements, probably due to expansion joints.
Although there is no exact answer for the mode shapes, they generally look rea-
sonable and make physical sense. The representative statistics of natural frequency
and damping ratio are also indicated for each mode in the ﬁgure. The mean value l
is calculated based on (14.64), i.e., the sample mean of the MPVs among the setups.
The c.o.v. is equal to
ﬃﬃﬃv
p =l, where v is the ‘total variance’ calculated according to
(14.65). Recall that v is attributed to setup-setup variability and identiﬁcation
uncertainty in different setups. The percentage contribution of these two sources are
shown in Table 14.3. In the present case, the total variance is dominated by
setup-setup variability, especially for the natural frequency.
Comparison with Least Squares Solution
The global mode shapes are also estimated using least squares methods based on
the local mode shapes (MPV) identiﬁed in individual setups. The quality depends
on the quality of local mode shapes, which varies over modes and setups. For
modes 1 to 3, the modal s/n ratio is consistently high in all setups and the local
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
10
-7
10
-6
10
-5
10
-4
10
-3
10
-2
[
]
[
]
[
][
]
Frequency [Hz]
[g/√Hz]
Fig. 14.4 One-sided SV spectrum of footbridge data in Setup 1. Horizontal bar indicates the
selected frequency band for modal identiﬁcation. The bands for other setups are similar
440
14
Multi-setup Problem

mode shapes can be well identiﬁed. Least squares methods (local or global) then
give similar results as in Fig. 14.5. The same is not true for mode 4, where some
setups have low modal s/n ratio, essentially when the roving sensors were near the
left or right end of the bridge. For local least squares method, the quality also
depends on the choice of the reference setup, as demonstrated in Fig. 14.6a–c. The
mode shape in Fig. 14.6d by global least squares method (with uniform weights)
has a kink near the mid-spans, which appears unreasonable. Without additional
information, it is difﬁcult to judge and may be misinterpreted as an indication of
structural damage.
Posterior Uncertainty
Figure 14.7 shows the posterior statistics of the natural frequency and damping
ratio of mode 1 in different setups. Results obtained by analyzing the setups
together (the method in Sect. 14.3) and individually (the method in Chap. 12) are
shown for comparison. The MPVs vary in different setups, revealing the necessity
x
y
x
z
x
y
z
x
y
x
z
x
y
z
Mode 1
1.67Hz (0.6%)
1.4% (36%)
Mode 2
3.02Hz (0.7%)
3.3% (39%)
x
y
x
z
x
y
z
Mode 3
3.76Hz (1.0%)
0.77% (40%)
x
y
x
z
x
y
z
Mode 4
4.16Hz (0.9%)
1.2% (57%)
Fig. 14.5 Identiﬁed mode shapes (MPV) of footbridge. Representative statistics of natural
frequency and damping ratio are indicated in the form ‘mean (c.o.v.)’
Table 14.3 Contribution of setup-setup variability and identiﬁcation uncertainty to total variance
Natural frequency
Damping ratio
Contribution\mode
1
2
3
4
1
2
3
4
Setup-setup var. (%)
86
56
98
93
67
65
78
80
ID uncertainty (%)
14
44
2
7
33
35
22
20
14.5
Field Applications
441

for separate parameterization of modal properties. The ﬁgure suggests that it makes
no difference to the posterior statistics whether the setups are analyzed together or
separately. This is only true when the modal s/n ratio is high, which is the case for
mode 1. Figure 14.8 shows the results for mode 4 where signiﬁcant discrepancy
appears in setups with low modal s/n ratio. When the setups are analyzed sepa-
rately, the ‘problematic’ setups (shaded) have larger (unreasonable) ﬂuctuation in
the MPV with time and larger posterior uncertainty. This suggests that, not only
x
z
(a) Ref. = Setup 20 (Good)
(b) Ref. = Setup 3 (Acceptable)
(c) Ref. = Setup 21 (poor)
(d) Global least square
Fig. 14.6 Global mode shapes by local least squares method (a–c) with different setups as
reference; and by global least squares method (d)
Natural frequency (Hz)
Damping ratio (%)
Rovers from middle to left end
Rovers from middle to right end
Time (minutes). Origin = 1020hrs
Fig. 14.7 Posterior statistics of mode 1 frequency and damping in different setups; hoi: setups
analyzed separately; j    j: setups analyzed together
442
14
Multi-setup Problem

does analyzing setups together produce a better global mode shape, it also improves
the identiﬁcation results in the natural frequency and damping ratios. This cannot be
achieved by the least squares methods in Sects. 14.1 or 14.2 because they only
assemble the global mode shape from local ones. Further details on this example are
referred to Au and Zhang (2012) and Zhang et al. (2015).
■
Example 14.4 (Coupled ﬂoor slab) Consider a coupled ﬂoor slab shown
schematically in Fig. 14.9. It is formed by two mega-truss systems on 2/F and 3/F
of a community recreation center. Different from typical situations, the vertical
vibrations of the two ﬂoors are coupled due to the interior steel columns. A series of
tests (ambient, shaker, occupant jumping) were performed during and after con-
struction
to
update
serviceability
vibration
performance
of
the
ﬂoors.
Figure 14.10a, b show the conditions on 2/F and 3/F during the ambient test. Six
Natural frequency (Hz)
Damping ratio (%)
Time (minutes). Origin = 1020hrs
1
3
16
18
20 21
20
21
Rovers from middle to left end
Rovers from middle to right end
Fig. 14.8 Posterior statistics of mode 4 frequency and damping in different setups; hoi: setups
analyzed separately; j    j: setups analyzed together; shaded: setups with large discrepancy
35m
8.6m
2/F (recreation & function rooms)
3/F (basketball courts)
Fig. 14.9 Coupled ﬂoor slab (elevation)
14.5
Field Applications
443

triaxial servo-accelerometers were deployed with their analogue data transmitted
through cables to a DAQ console. Data was acquired at 2048 Hz and later deci-
mated to 256 Hz for analysis.
Setup Plan
Figure 14.11 shows the setup plan. A three-dimensional global mode shape cov-
ering 63  2 ¼ 126 locations were to be obtained, with n ¼ 126  3 ¼ 378 DOFs.
One reference sensor was placed on 2/F and 3/F at the reference locations (square)
to connect their mode shapes. On 2/F, each roving sensor (A, B, C, D) basically
covers two column lines of locations. The console was placed on the ﬂoor
instrumented. To avoid using cable of excessive length (which increases data
noise/distortion and safety risk), a hole was drilled on 3/F (feasible during con-
struction) to allow passage of sensor cable to 2/F. Each setup typically took 20 min
(15 min for data and 5 min for transition). Including initial calibration and setup,
the sixteen setups on 2/F required one day (0800–1800 h) to ﬁnish.
Originally, the setups on 3/F followed the same plan on 2/F. However, Sensor A
(or its data channel) was found to be defective after Setup 8. In the interest of time,
Servo. Acc.
DAQ Console
(a) 2/F
(b) 3/F
(c) Equipment
Fig. 14.10 Condition on 2/F (a) and 3/F (b), and equipment (c) for ambient vibration test
B3
C12
A1
A2
A3
A4
A5
A6
A7
A9
A10
A11
A12
A13
A14
A8
B1
B2
B4
B5
B6
B7
B9
B10,11
B12
B13
B14
B8
C1
C2
C3
C4
C5
C6
C7
C9
C10
C11
C13
C14
C8
D1
D2
D3
D4
D5
D6
D7
D9
D10
D11
D12
D13
D14
D8
(a) 2/F (Day 1)
A15
B15
C15
A16
B16
C16
D15
16
31m
A1
A8
B1
B8
C1
C8
D1
D8 B17
A2
B2
B9
C2
C9
D2
D9
B15
B18,19
A3
B10,11
C3
C10
D3
D10
B3
C17
A4
B4
C4
C11
D4
D11
C15
C18,19
A5
B5
B12
C5
D5
D12
C12
C16
D17
A6
B6
B13
C6
C13
D6
D13
D15
D18
A7
D14
D19
B7
B14
C14
D16
C7
D7
B16
31m
21.4m
(b) 3/F (Day 2)
Fig. 14.11 Setup plan for coupled ﬂoor slab. Square reference sensor locations; dot roving sensor
locations (A/B/C/D, followed by setup number). In (b), hollow circle indicates a hole drilled on
3/F to pass cable to 2/F
444
14
Multi-setup Problem

the setups were continued as planned with the remaining sensors, during which a
contingent plan was made. Changes were made from Setup 15 onwards to make up
for the locations missed by Sensor A with the remaining three roving sensors. This
required three additional setups. The test ﬁnished by 1800 h.
Modal Identiﬁcation
Figure 14.12 shows the SV spectrum of data in Setup 1 on 2/F. The modes below
4 Hz and the one just below 6 Hz were attributed to lateral vibration of the building
(not of concern), as conﬁrmed from the PSD of data. The ﬁrst three modes of slab
vertical vibration have frequencies around 6, 8 and 9 Hz. Their global mode shapes
identiﬁed using Bayesian method with all setups incorporated are shown in
Fig. 14.13. The quality is generally good. The results by global least squares or
local least squares method (with reasonable choice of reference setup) are similar
(not shown here). Similar to the last example, setup-setup variability dominates the
0
1
2
3
4
5
6
7
8
9
10
10
-7
10
-6
10
-5
10
-4
10
-3
10
-2
Frequency [Hz]
[g/√Hz]
Building mode
(lateral)
Slab
mode 1
Slab
mode 2
Slab
mode 3
Fig. 14.12 One-sided SV spectrum of Setup 1 on 2/F, coupled ﬂoor slab
Mode 1
6.2Hz (0.3%)
1.1% (15%)
Mode 2
7.74Hz (0.6%)
2.1% (16%)
Mode 3
9.1Hz (0.8%)
2.4% (22%)
Fig. 14.13 Identiﬁed global mode shapes of coupled ﬂoor slab (vertical not to scale with
horizontal). Representative statistics of natural frequency and damping ratio are indicated in the
form ‘mean (c.o.v.)’
14.5
Field Applications
445

representative variance in the modal properties. For the natural frequencies, it
accounts for over 95% of the total variance. For the damping ratios, it accounts for
over 80%.
Further details on ambient vibration test and shaker test of this structure can be
found in Au et al. (2012). The identiﬁed modal properties were used for structural
model updating (Lam et al. 2014, 2016). Design considerations of the slabs for ﬂoor
vibration against rthymic activities are discussed in Li et al. (2011).
■
Example 14.5 (Velodrome roof) Consider a velodrome roof with principal
diameter 120 m  80 m, as shown in Fig. 14.14a. It is formed by mega trusses
spanning over the minor diameter. Ambient vibration tests were performed on the
roof to identify the modal properties. Scaffold mounted on the top side of the mega
truss provided a platform feasible for instrumentation during construction. See
Fig. 14.14b viewing from below and Fig. 14.15a on the platform.
Setup Plan
As shown in Fig. 14.15b, the same set of equipment with four triaxial sensors in
Example 14.3 was deployed, where the data of different sensors were synchronized
by GPS. Sensors were leveled on mounts tied securely on truss members
(Fig. 14.15c). Horizontally they were aligned with their y-axis (local North) along
the local truss member axis. Half of the roof was measured at 48 locations as shown
Fig. 14.14 Velodrome during test
Fig. 14.15 Instrumentation details of velodrome roof
446
14
Multi-setup Problem

in Fig. 14.16, producing a three-dimensional mode shape with n ¼ 48  3 ¼ 144
DOFs. There were two reference sensors, indicated by the squares in the ﬁgure. The
other two roving sensors covered the left and right part of the roof. The locations
near the left and right end of the major axis were not measured as they could not be
accessed during the test.
Setup planning was much constrained by environmental and construction
activities. The data was to be acquired with minimal logistics assistance from
contractor and no intervention of construction activities. Except for the reference
sensor locations, the roving sensor locations reﬂect more of what was feasible on
the test day rather than what was planned. Mobility was limited on the platform.
Most setup transitions took place along the major axis direction (left–right in
Fig. 14.16), as the minor axis direction was blocked by sky-window panels (see left
side of Fig. 14.15a). To go to another row one had to go to the perimeter ﬁrst. The
data of different sensors were synchronized by GPS receivers, which required good
exposure to the sky. During the test, the (automatic) sky-windows were not in full
commission yet. The locations of windows that could be opened were only known
on the test day or the day before. Each setup took 30 min (15 min for data and
15 min for transition). The whole test with twenty three setups spanned over three
consecutive days, with six setups on Day 1, seven on Day 2 and ten on Day 3.
Modal Identiﬁcation
Figure 14.17 shows the SV spectrum of data in Setup 1. Four potential modes are
indicated. Their global mode shapes identiﬁed with all data incorporated are shown
in Fig. 14.18. The quality is generally good. Local least squares (with reasonable
reference) and global least squares method gave similar results, except for occa-
sional slope discontinuity in some higher modes. Setup-setup variability accounts
for 85–95% of the total variance for the natural frequencies, and 50–70% for the
damping ratios.
■
A1 A2
A3
A4
6
A
5
A
9
A
8
A
7
A
A10
A11
A12
A13
A14
A15 A16
A17A18
A19
A20
2
2
A
1
2
A
A23
B1
B2
B3
B4
B5 B6
9
B
8
B
7
B
B10
B11 B12
B13
B14
B15 B16
B17 B18
B19
B20
2
2
B
1
2
B
B23
40m
120m
Fig. 14.16 Setup plan. Square reference sensor locations; dot roving sensor locations (A/B,
followed by setup number); Setups 1–6 on Day 1, 7–13 on Day 2; 14–23 on Day 3
14.5
Field Applications
447

Example 14.6 (Brodie Tower) Consider the Brodie Tower at the University of
Liverpool (UK) in Fig. 14.19. It is a seven-storied reinforced concrete building with
a basement. A three-dimensional global mode shape with 28 locations was iden-
tiﬁed with ﬁve triaxial servo-accelerometers. The total number of measured DOFs is
n ¼ 28  3 ¼ 84.
Setup Plan
A reference sensor was placed on 7/F. The remaining four sensors roved from 7/F
down to 1/F, covering all measured DOFs in seven setups; see Fig. 14.20a. The
acceleration data at each location was acquired and stored locally on the sensor with
integrated DAQ. The sensors were equipped with high precision clocks and so they
do not need to be synchronized by wired or wireless means; see Fig. 14.20b.
Synchronized shortly before the test by GPS, their data were practically synchro-
nized within the test duration. Twenty minutes of data at 50 Hz were acquired in
each setup. Including initial setup and calibration, the whole series of tests took 4 h
in the afternoon.
0
0.5
1
1.5
2
2.5
3
10
-6
10
-5
10
-4
10
-3
10
-2
[
] [
][
]
[
]
Frequency [Hz]
[g/√Hz]
Fig. 14.17 One-sided SV spectrum of data in Setup 1, velodrome roof
Mode 1
1.60Hz (0.8%)
2.1% (21%)
Mode 2
1.92Hz (1.1%)
2.5% (44%)
Mode 3
2.15Hz (2.9%)
2.8% (52%)
Mode 4
2.51Hz (0.3%)
1.7% (18%)
Fig. 14.18 Identiﬁed mode shapes of velodrome roof. Representative statistics of natural
frequency and damping ratio are indicated in the form ‘mean (c.o.v.)’
448
14
Multi-setup Problem

Modal Identiﬁcation
Figure 14.21 shows the SV spectrum of the data in Setup 1. Three potential modes
are indicated. Their global mode shapes identiﬁed with all data incorporated are
shown in Fig. 14.22. The ﬁrst two modes correspond to translations primarily along
the x and y direction, respectively. The third mode is rotational. The quality of these
modes is generally good. Local least squares and global least squares method gave
similar results. Setup-setup variability accounts for over 75% of the total variance
for the natural frequencies, and about 50% for the damping ratios. Higher modes
can also be identiﬁed (omitted here), although the results from different methods
differ for modes with low modal s/n ratio, similar to Example 14.3.
■
Ref. (7/F)
Rov.1
Rov.2
Rov.3
Rov.4
(b)
(a)
Fig. 14.19 Brodie Tower. a View from NE of the plan in (b). b Typical ﬂoor plan
0
5
10
15
20
25
0
5
10
15
20
0
5
10
15
20
y (m)
x (m)
z (m)
Ref.
Rov.1
Rov.4
Rov.2
Rov.3
G/F
7/F
(a)
(b)
Fig. 14.20 Setup plan. a Setup 3. b Equipment for measurement per location
14.5
Field Applications
449

References
Au SK (2011) Assembling mode shapes by least squares. Mech Syst Signal Process 25(1):163–
179
Au SK, Ni YC, Zhang FL et al (2012) Full scale dynamic testing and modal identiﬁcation of a
coupled ﬂoor slab system. Eng Struct 37:167–178
Au SK, Zhang FL (2012) Fast Bayesian ambient modal identiﬁcation incorporating multiple
setups. J Eng Mech ASCE 138(7):800–815
Lam HF, Peng HY, Au SK (2014) Development of a practical algorithm for Bayesian model
updating of a coupled slab system utilizing ﬁeld test data. Eng Struct 79:182–194
Lam HF, Yang JH, Au SK (2016) Bayesian model updating of a coupled-slab system using ﬁeld
test data utilizing an enhanced markov chain monte carlo simulation algorithm. Eng Struct
102:144–155
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
10
-7
10
-6
10
-5
10
-4
10
-3
[
][
]
[
]
Frequency [Hz]
[g/√Hz]
Fig. 14.21 One-sided SV spectrum of data in Setup 1
y
2.42Hz (0.19%)
1% (17%)
x
z
y
2.71Hz (0.22%)
1.1% (13%)
x
z
y
3.75Hz (0.24%)
0.74% (17%)
x
z
x
y
x
y
x
y
Fig. 14.22 Identiﬁed mode shapes (MPV) of Brodie Tower. Representative statistics of natural
frequency and damping ratio are indicated in the form ‘mean (c.o.v.)’
450
14
Multi-setup Problem

Li WW, Wong T, Leung MK et al (2011) Floor vibration due to human rhythmic activities: Tin
Shui Wai Public Library Cum Indoor Recreation Centre. Proc Eng 14:3285–3292
Zhang FL, Au SK, Lam HF (2015) Assessing uncertainty in operational modal analysis
incorporating multiple setups using a Bayesian approach. Struct Control Health Monit 22
(3):395–416
Zhang FL, Au SK (2016) A probabilistic model for modal properties based on operational modal
analysis. ASCE-ASME J Risk Uncertain Eng Syst Part A Civil Eng 2(3):B4015005
References
451

Part IV
Uncertainty Laws

Chapter 15
Managing Identiﬁcation Uncertainties
Abstract This chapter presents closed form analytical formulas that govern the
identiﬁcation uncertainty of modal parameters in operational modal analysis for
well-separated modes. These are collectively referred as ‘uncertainty laws’, which
is one of the latest advances in operational modal analysis. Underlying assumptions
and contexts are discussed to allow proper interpretation and application of the
formulas. The uncertainty laws govern the achievable precision of operational
modal analysis and they can be used for planning ambient vibration tests. The
chapter is written for general readers with minimal preparation in mathematics.
Keywords Uncertainty laws  Asymptotic  Bandwidth  Uncertainty manage-
ment  Test planning
Suppose we are going to perform an ambient vibration test on a structure to identify
its modal properties. One typical question is,
How much data do we need?
‘How much’ here practically refers to the sampling rate, data duration and the
number of sensors. A more general question is,
What is the required test conﬁguration?
In addition to data size, this question now involves other factors such as the quality
of sensors, their locations, the quality of data acquisition hardware, or even the
number of setups if available sensors are not enough to cover all required DOFs in a
single setup.
These questions are not trivial to answer. They depend on the required precision
in the identiﬁcation results. Answering them requires insights into how identiﬁca-
tion uncertainty depends on test conﬁguration. Experience certainly helps but a
quantitative account will be a valuable tool.
The Bayesian algorithms for operational modal analysis (OMA) discussed in
Chaps. 12–14 allow us to calculate the most probable value and identiﬁcation
uncertainty of modal parameters when data is given. The results are numerical and
point-wise in nature. Suppose the posterior c.o.v. of damping ratio is calculated to
be 30% for a given data set. This helps us judge how accurate the damping ratio has
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_15
455

been estimated from the given data. This 30%, however, is only speciﬁc to the data
obtained, for the particular mode, of the particular structure, and under the particular
ambient environment. It gives little or no insight about why it is 30%, or more
generally, how identiﬁcation uncertainty depends on test conﬁguration. Knowing
the why will allow us to master the identiﬁcation uncertainty in OMA, which has
both scientiﬁc and engineering signiﬁcance. For example, what is the achievable
precision of modal parameters identiﬁed from ambient data? Would adding a sensor
signiﬁcantly reduce the identiﬁcation uncertainty of the damping ratio? If so, by
how much? How about using a sensor with much lower noise?
One way to master the relationship between test conﬁguration and identiﬁcation
uncertainty is to have simple expressions or graphs relating them to comprehend
and build insights. Glancing over the analytical expressions for programming the
posterior covariance matrix in Chaps. 12–14, the reader would be convinced that
this is by no means a simple task, if at all possible. The posterior covariance matrix
is the inverse of the Hessian of NLLF (negative log-likelihood function). The
Hessian contains second derivatives which are no simple expressions and involve
stochastic data. Also, how to quantify test conﬁguration?
The general situation may not admit simple expressions, but it turns out that
under some ‘asymptotic’ conditions, namely, long data and small damping, the
posterior covariance matrix does reduce to mathematically tractable form that
allows us to build insights. Asymptotic expressions related to the posterior uncer-
tainty are referred as ‘uncertainty laws’. This chapter discusses the uncertainty laws
so that they can be properly interpreted and applied for planning ﬁeld tests. The
scope is limited to well-separated modes with single setup data. Deriving uncer-
tainty laws is technical and tedious. It is postponed until Chap. 16, which may be
skipped for readers primarily interested in applications.
Figure 15.1 explains how the objective of uncertainty laws differs from that of
conventional modal identiﬁcation. We ﬁrst present the key results of uncertainty
laws and discuss their meaning. Examples are given to demonstrate the quality of
approximation in general non-asymptotic situations. Their use for planning ambient
vibration tests are discussed and illustrated through examples.
15.1
Context and Key Formulas
Consider a classically-damped mode with natural frequency f (Hz) and damping
ratio f, and is well-separated from other modes. It is identiﬁed using the scaled Fast
Fourier Transform (FFT) of ambient data on a selected frequency band. Within the
band, the scaled FFT of data is assumed to be the sum of modal response and
prediction error. The modal contribution of displacement response (in the time
domain) is ugðtÞ; where u (n  1, with unit norm) is the partial mode shape vector
conﬁned to the measured DOFs, and gðtÞ (scalar) is the modal response. The latter
satisﬁes the modal equation of motion €gðtÞ þ 2fx_gðtÞ þ x2gðtÞ ¼ pðtÞ, where pðtÞ
456
15
Managing Identiﬁcation Uncertainties

Fig. 15.1 Objective of uncertainty laws
15.1
Context and Key Formulas
457

is the modal force and x ¼ 2pf . The modal force and prediction error are assumed
to be uncorrelated and have respectively a constant power spectral density (PSD) of
S and Se within the selected band. The above context corresponds to that of
Chap. 12. In this chapter, the prediction error is assumed to be solely attributed to
channel (measurement) noise and so it is simply referred as channel noise. The
modal properties to identify are
f
Natural
frequency
ðHzÞ
f
Damping
ratio
ðdimensionlessÞ
S
Modal force
PSD
same unit as
acceleration
ð
Þ
Se
Channel noise
PSD
same unit
as data
ð
Þ
u
n1
Mode shape
sum of squares
equal to 1
ð
Þ
Let x denote a modal parameter, i.e., f , f, S, Se or u. The c.o.v. of x is denoted by
dx; c.o.v. = coefﬁcient of variation = standard deviation/mean. Assume that the
selected frequency band is f ð1  jfÞ Hz, where j is a ‘bandwidth factor’ (e.g.,
j ¼ 1 corresponds to the half-power band). Let Td (s) be the data duration and Nf
be the number of FFT points in the selected band. Since the FFT frequency interval
is Df ¼ 1=Td and the selected bandwidth is 2jff ,
Nf ¼ 2jffTd
ð15:1Þ
It can be shown that for long data (Nf ! 1) and small damping (f ! 0),
d2
x  d2
x1 ¼ d2
x0ð1 þ ax
c Þ
Nf ! 1; f ! 0
ð15:2Þ
where d2
x0 and d2
x1 are referred respectively as the ‘zeroth order law’ and ‘ﬁrst order
law’; ‘*’ (read as ‘asymptotic to’) denotes that the ratio of the two sides tends to 1;
c ¼
1
4mf2
ð15:3Þ
is the (dimensionless) ‘modal signal-to-noise (s/n) ratio’;
m ¼ Se
S ð2pf Þ2q
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð15:4Þ
is the ‘noise-to-environment’ (n/e) ratio; ax is the ‘ﬁrst order coefﬁcient’ associated
with the ﬁrst order effect of c when it is ﬁnite. The expressions for d2
x0 and ax for
different parameters are summarized in Table 15.1.
Figure 15.2 illustrates the above context for acceleration data (q = 0) in an
idealized singular value spectrum, i.e., a plot of eigenvalues of the PSD matrix of
data (Sect. 7.3). In this case the modal s/n ratio is c ¼ S=4Sef2. At the natural
458
15
Managing Identiﬁcation Uncertainties

Table 15.1 Key results of uncertainty laws
Parameter x
Zeroth order law d2
x0
Data length factor BxðjÞ
First order law coefﬁcient axðjÞ
Frequency f
f
2pNcBx
2
p
tan1 j 
j
j2 þ 1


4ðj  tan1 jÞ
tan1 j 
j
j2 þ 1
Damping f
1
2pfNcBx
2
p tan1 j þ
j
j2 þ 1  2ðtan1 jÞ2
j
"
#
4ðj2 þ 1Þð3 tan1 j  3j þ j2 tan1 jÞ tan1 j
3½ðj2 þ 1Þðj  2 tan1 jÞ tan1 j þ j2
Modal force
PSD S
1
Nf Bx
1  2
j ðtan1 jÞ2 tan1 j þ
j
j2 þ 1

1
2 þ 2
3 j2 
ðtan1 jÞ2
2ðtan1 jÞ2  bj
8 tan1 j
b

8j
tan1 j þ 4
3 j2 þ 4


b ¼ tan1 j þ
j
j2 þ 1
Channel noise
PSD Se
1
ðn  1ÞNf Bx
1
0
Mode shape u
Covariance matrix
^Cu ¼
2mf
pNcBx
ðIn  uuTÞ
u ¼ jjujj1u
Square of mode shape c.o.v.
d2
u ¼ 2ðn  1Þmf
pNcBx
2
p tan1 j
–
Generally, d2
x  d2
x0ð1 þ ax=cÞ where c ¼ 1=4mf2 is the modal signal/noise ratio, m ¼ ð2pf Þ2qSe=S is the noise/environment ratio, q ¼ 0; 1; 2 for acceleration,
velocity and displacement data, respectively
The selected band for modal identiﬁcation is f ð1  jfÞ where j is the bandwidth factor; Nc ¼ Nf =2fj ¼ Data length/natural period
Symbols f , f, S, Se and u here denote the actual value of modal properties rather than the variable in the likelihood function
15.1
Context and Key Formulas
459

frequency, the largest eigenvalue is S=4f2 þ Se and the remaining eigenvalues are
all equal to Se. The ratio of the top to the second top line in the singular value
spectrum is then approximately equal to the modal s/n ratio when it is large.
In the presentation of uncertainty laws, for notational convenience, f , f, S, Se and
u denote the actual values of modal parameters that result in the data, rather than
the variables in the likelihood function. In Table 15.1, ^Cu is the uncertainty law for
the posterior covariance matrix of mode shape u (with unit norm). The mode shape
c.o.v., denoted by du, is deﬁned in Sect. 11.3.3 as the square root sum of eigen-
values of ^Cu. According to the uncertainty laws, given the data, signiﬁcant cor-
relation exists only between f and S, which is Oðj1=2Þ. The correlation between
any other parameter pair is asymptotically small, at most OðfÞ.
15.2
Understanding Uncertainty Laws
Uncertainty laws are asymptotic expressions in the sense that the ratio of their value
to the exact value of posterior uncertainty tends to 1 under the stated asymptotic
conditions. The symbol ‘*’ in (15.2) denotes that the ratio of the LHS to the RHS,
i.e., d2
x=d2
x1, tends to 1 as Nf ! 1 and f ! 0. Here, dx is the posterior c.o.v. that we
want to understand and dx1 is the asymptotic analytical expression that we have
derived, from which we can build insights. Equation (15.2) implies
d2
x
d2
x0
¼ 1 þ ax
c þ Remainder
Nf ! 1; f ! 0
ð15:5Þ
Selected band with
Nf = 2κζf Td FFT points
f (1+κζ)
f (1−κζ)
Δf = 1/Td
~ S/4ζ2
Frequency (Hz)
f
Largest
eigenvalue
(n−1) eigenvalues
equal to Se
Se
Fig. 15.2 Idealized singular value spectrum of acceleration data (q = 0), illustrating the context of
uncertainty laws. At the natural frequency, the ratio of the top to second top line is approximately
equal to the modal s/n ratio c ¼ S=4Sef2
460
15
Managing Identiﬁcation Uncertainties

The remainder depends on data, as d2
x does. However, it vanishes asymptotically,
leaving only the ‘information content’ of data captured in the uncertainty law
d2
x1 ¼ d2
x0ð1 þ ax=cÞ. Note that ax=c ! 0 because c ¼ 1=4mf2 ! 1, and so
d2
x=d2
x0 ! 1 as well. Nevertheless dx1 captures the effect of c when it is ﬁnite and is
more useful than dx0.
Data is not available before it is measured. In deriving the uncertainty laws, the
ambient data is modeled by a stochastic process that obeys the assumptions in
Bayesian OMA. In other words the uncertainty laws do not account for modeling
errors. They reﬂect identiﬁcation uncertainty if the modeling assumptions are
correct.
15.2.1
Data Length and Usable Bandwidth
The uncertainty laws depend on the following (dimensionless) scales: f, m, j, Nc or
Nf . Damping ratio f is a property of the subject structure. The n/e ratio m represents
a modal noise-to-signal ratio excluding structural dynamic ampliﬁcation. The
posterior c.o.v.s are inversely proportional to the square root of data length in terms
of either Nf or Nc ¼ Nf =2fj. This is common in statistical estimation. The ‘nor-
malized’ data length Nc is related to the maximum amount of information available
in data for inferring the mode of interest. On the other hand, j and Nf are related to
the amount of information that can be actually utilized. The bandwidth factor j
must trade off modeling error and information included for modal identiﬁcation.
One can have a long time history of data so that Nc is large. However, in the
neighborhood of the natural frequency, other unknown colored noise exists. Then
only a small bandwidth can be used to avoid modeling error. This limits j and Nf .
The uncertainty laws in Table 15.1 have been written in a form that isolates the
effect of j into the ‘data length factor’ BxðjÞ. The latter is an increasing function of
j from 0 to 1. The remaining part in the formula represents the lower limit of the
posterior uncertainty when the full bandwidth can be utilized. For example,
d2
f  f=2pNc for the natural frequency. In reality, one is not able to make use of the
full bandwidth for modal identiﬁcation, limiting j to be ﬁnite. The term NcBf ðjÞ
represents an effective (dimensionless) data length accounting for limited
bandwidth.
Figure 15.3 shows the variation of the data length factors with j. Note that
Bu [ Bf [ BS [ Bf ; Bf and BS almost overlap. For j ¼ 6 (typical) the value is
about 80% for the natural frequency and 90% for the mode shape. The values for f
and S are smaller, about 60%. The ﬁgure suggests that the accuracy in the mode
shape or natural frequency is typically less sensitive to the bandwidth than f or S,
which can be explained intuitively.
15.2
Understanding Uncertainty Laws
461

15.2.2
Signal-to-Noise Ratio
The term ‘signal-to-noise ratio’ (s/n ratio) is often used to indicate the contribution
of the target signal compared to ‘noise’. The latter arises from sensors, cables
transmitting
the
signal
(especially
for
analogue
signals),
data
acquisition
(DAQ) hardware, etc. Different deﬁnitions have been used depending on applica-
tion. The deﬁnition in (15.3), combined with (15.4), stems from the theory in
Chap. 12:
c ¼
S
4Sef2ð2pf Þ2q
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð15:6Þ
This is the PSD ratio of the response at resonance (S=4f2ð2pf Þ2q) to measurement
noise (Se). The modal force PSD S depends on environmental excitation intensity
and (less trivially) on the measured DOFs. Due to mode shape scaling, it is pro-
portional to the sum of squares of mode shape values at the measured DOFs.
The posterior c.o.v. dx does not vanish even when the modal s/n ratio c is
inﬁnite. Rather, it stays at a non-zero level given by the zeroth order law dx0. High
s/n ratio only removes uncertainty due to measurement noise, but not uncertainty
due to modal force, which is not measured and hence cannot be eliminated. The
zeroth order laws therefore reﬂect the achievable precision of modal parameters in
OMA.
0
2
4
6
8
10
12
14
16
18
20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Bandwidth factor κ
BΦ
Bf
BS
Bζ
Fig. 15.3 Data length factor
462
15
Managing Identiﬁcation Uncertainties

15.2.3
First Order Effect of Modal s/n Ratio
The ﬁrst order law d2
x1 ¼ d2
x0ð1 þ ax=cÞ captures the ﬁrst order effect of the modal
s/n ratio c through the term ax=c. The ﬁrst order law coefﬁcients for f , f and S are
plotted in Fig. 15.4. Figure 15.5 shows the ratio of the ﬁrst order law to the zeroth
order law. Regardless of j, the ratio converges to 1 as c increases because the ﬁrst
order correction ax=c vanishes in the limit.
15.2.4
Governing Uncertainty
The dependence of the posterior c.o.v. of a modal parameter on f has important
implications on how difﬁcult it can be estimated from ambient data. From
Table 15.1, d2
f 0 and d2
u are proportional to f, while on the contrary d2
f0 is inversely
proportional to f. For small f encountered in applications, say, 0.5–5%, this means
that the identiﬁcation uncertainty in the damping ratio is much larger than that in the
0
1
2
3
4
5
6
0
5
10
15
20
κ
af(κ)
0
1
2
3
4
5
6
0
5
10
15
20
κ
aζ(κ)
0
1
2
3
4
5
6
0
10
20
30
40
κ
aS(κ)
Fig. 15.4 First order coefﬁcients af , af and aS
10
0
10
1
10
2
10
3
10
4
0
1
2
3
4
5
6
7
8
9
10
3
6
κ = 9
γ
δf1/δf0 = (1+af/γ)1/2
10
0
10
1
10
2
10
3
10
4
0
1
2
3
4
5
6
7
8
9
10
3
6
κ = 9
γ
δζ1/δζ0 = (1+aζ/γ)1/2
10
0
10
1
10
2
10
3
10
4
0
1
2
3
4
5
6
7
8
9
10
3
6
κ = 9
γ
δS1/δS0 = (1+aS/γ)1/2
Fig. 15.5 Ratio of ﬁrst order law to zeroth order law
15.2
Understanding Uncertainty Laws
463

natural frequency or mode shape, and so its accuracy requirement is likely to
govern planning decisions.
The dependence on f can be explained intuitively. Smaller f implies that reso-
nance oscillations decay slower and stay longer in the data. In the frequency
domain, the resonance peak is more pronounced and sharper, giving better accuracy
in the natural frequency. The quality of mode shape also improves because reso-
nance oscillations dominate the measured vibration signal. Vanishingly small
damping makes it impossible to identify to the same relative accuracy, in the
presence of uncertainty arising from unknown modal excitation. The posterior
standard deviation of the damping ratio does reduce in a square root manner as the
damping ratio decreases. Nevertheless the reduction rate is slower than linear, and
so on a relative basis the identiﬁcation uncertainty increases.
15.3
Demonstrative Examples
Generally, uncertainty laws capture well identiﬁcation uncertainties even under
non-asymptotic situations. This section provides some demonstrative examples
based on synthetic, laboratory and ﬁeld data. The ﬁeld data example also gives an
idea of the order of magnitude of the governing scales encountered in practice.
Example 15.1 (Synthetic data, ten-storied shear building) Revisit the ten-storied
building in Example 12.1, where synthetic acceleration data (q ¼ 0) was generated
at 100 Hz for 600 s. The selected band for identiﬁcation was [0.94, 1.06] Hz
(j ¼ 6). Figure 15.6 compares the uncertainty laws (cross) with the exact values of
the posterior c.o.v. (for the given data) recalled from Fig. 12.5. Data was generated
with different values of Se, giving different values of modal s/n ratio c ¼ S=4Sef2.
The values of uncertainty laws are obtained by substituting modal parameter values
with the posterior MPV determined in each case. The ﬁgure shows that the
uncertainty laws agree with the exact values of the posterior c.o.v., especially for
large modal s/n ratios. The posterior c.o.v. generally decreases to a constant level
(equal to the zeroth order law) as the modal s/n ratio increases.
■
Example 15.2 (Laboratory
data, three-storied shear
frame) Revisit the
three-storied laboratory shear frame in Example 12.4, where each ﬂoor was mea-
sured biaxially at four corners, giving a total of 3  4  2 ¼ 24 measured DOFs.
Ambient acceleration data was recorded for ten minutes. Consider performing
modal identiﬁcation with an increasing set of measured DOFs, starting from the
smallest set with DOFs 1 and 2. Adding one DOF at a time, this gives for each
mode 23 cases, corresponding to DOFs {1, 2}, {1, 2, 3} and so on, until {1, 2, ,
24}. These cases have different modal s/n ratios because their modal force PSDs are
different.
The plots on the left column of Fig. 15.7 show the posterior c.o.v. of modal
parameters of Mode 1 (TX1) calculated from data (exact, circles) and those from
464
15
Managing Identiﬁcation Uncertainties

uncertainty laws (crosses). The uncertainty laws give a good approximation of the
exact values. The apparent random ﬂuctuation in the trend between the posterior
c.o.v. and modal s/n ratio stems from the ﬂuctuation in the MPV of modal
parameters determined from data with different measured DOFs. If the MPVs were
the same in all cases, the points would have formed a smooth decaying trend to the
zeroth order law. The plots on the right column of Fig. 15.7 illustrate this point,
where the ratio of the posterior c.o.v. to the zeroth order law is plotted. For the ﬁrst
order law (cross), this ratio is simply 1 þ ax=c, which only depends on the band-
width factor j (through ax) and modal s/n ratio c. As the selected band has been
ﬁxed, j is relatively constant and so the ratio is essentially a function of c only.
Again, the circles (exact/zeroth order) and the crosses (ﬁrst order/zeroth order)
agree with each other, both converging to 1 as c increases. Figure 15.8 shows the
results of mode 9 (S1), analogous to Fig. 15.7. This mode has the lowest c among
all modes and so it illustrates more clearly the decaying trend of the posterior c.o.v.
with c.
■
Example 15.3 (Field data, tall building) Revisit the tall building in Example 12.5.
Acceleration data is now collected from a triaxial servo-accelerometer (n ¼ 3
DOFs) placed in a room on the roof near the core, acquired for 18 h at 50 Hz under
normal wind condition. The data is divided into 36 non-overlapping segments, each
30 min long. Modal identiﬁcation is performed for the six modes below 1 Hz for
each segment, each giving 36 sets of MPVs and posterior c.o.v.s. Figure 15.9
10
0
10
1
10
2
10
3
10
4
10
5
0
0.1
0.2
0.3
0.4
c.o.v. f [%]
10
0
10
1
10
2
10
3
10
4
10
5
0
10
20
30
40
50
c.o.v. ζ [%]
10
0
10
1
10
2
10
3
10
4
10
5
0
10
20
30
40
50
c.o.v. S [%]
10
0
10
1
10
2
10
3
10
4
10
5
0
5
10
15
20
c.o.v. Se [%]
10
0
10
1
10
2
10
3
10
4
10
5
10
-2
10
-1
10
0
10
1
c.o.v. φ [%]
Modal s/n ratio γ = S/4Seζ2
Fig. 15.6 Posterior c.o.v. of modal parameters of ten-storied building (synthetic data). Circle:
exact value (dx); cross: uncertainty law (dx1); x ¼ f ; f; S; Se; u
15.3
Demonstrative Examples
465

compares the exact values of the posterior c.o.v. with the values from the uncer-
tainty laws, calculated using the MPVs in each data set. The agreement is generally
good. Table 15.2 shows the statistics of the 36 identiﬁed values (MPV) of Se, S and
n/e ratio m ¼ Se=S, giving an idea of their order of magnitude in the ﬁeld test. ■
10
1
10
2
10
3
10
4
0
0.05
0.1
0.15
f
c.o.v. [%]
10
1
10
2
10
3
10
4
0.5
0.75
1
1.25
1.5
Ratio to zeroth order law
10
1
10
2
10
3
10
4
0
20
40
60
ζ
10
1
10
2
10
3
10
4
0.5
0.75
1
1.25
1.5
10
1
10
2
10
3
10
4
0
20
40
S
10
1
10
2
10
3
10
4
0.5
0.75
1
1.25
1.5
10
1
10
2
10
3
10
4
0
10
20
30
Se
10
1
10
2
10
3
10
4
0.5
0.75
1
1.25
1.5
10
1
10
2
10
3
10
4
0
2
4
6
8
10
Modal s/n ratio γ = S/4Seζ2
φ
10
1
10
2
10
3
10
4
0.5
0.75
1
1.25
1.5
Fig. 15.7 Posterior c.o.v. of modal parameters versus modal s/n ratio c, Mode 1 (TX1), laboratory
frame data. Left column circle = exact value (dx), cross = uncertainty law (dx1), x ¼ f ; f; S; Se; u;
right column ratio to zeroth order law, i.e., dx=dx0 (circle) and dx1=dx0 (cross)
466
15
Managing Identiﬁcation Uncertainties

15.4
Planning Ambient Vibration Tests
In this section, we discuss some implications of the uncertainty laws with regard to
planning for ambient vibration tests. According to the uncertainty laws, the pos-
terior c.o.v.s of modal parameters depend only on the following scales:
1. damping ratio f
2. bandwidth factor j
10
1
10
2
10
3
10
4
0
0.005
0.01
0.015
f
c.o.v. [%]
10
1
10
2
10
3
10
4
0.5
0.75
1
1.25
1.5
Ratio to zeroth order law
10
1
10
2
10
3
10
4
0
10
20
ζ
10
1
10
2
10
3
10
4
0.5
0.75
1
1.25
1.5
10
1
10
2
10
3
10
4
0
10
20
S
10
1
10
2
10
3
10
4
0.5
1
1.5
2
10
1
10
2
10
3
10
4
0
2.5
5
7.5
10
Se
10
1
10
2
10
3
10
4
0.5
0.75
1
1.25
1.5
10
1
10
2
10
3
10
4
0
1
2
3
4
5
Modal s/n ratio γ = S/4Seζ2
φ
10
1
10
2
10
3
10
4
0.5
0.75
1
1.25
1.5
Fig. 15.8 Posterior c.o.v. of modal parameters versus modal s/n ratio c, Mode 9 (S1), laboratory
frame data. Left column circle = exact value (dx), cross = uncertainty law (dx1), x ¼ f ; f; S; Se; u;
right column ratio to zeroth order law, i.e., dx=dx0 (circle) and dx1=dx0 (cross)
15.4
Planning Ambient Vibration Tests
467

3. normalized data length Nc ¼ Tdf
4. noise-to-environment (n/e) ratio m ¼ ð2pf Þ2qSe=S, q ¼ 0 (acceleration noise), 1
(velocity noise) or 2 (displacement noise)
The damping ratio f is a property of the subject structure. Assuming that one
always uses the widest possible bandwidth for modal identiﬁcation, the bandwidth
factor j is a result of test conﬁguration rather than a parameter than can be actively
changed. Thus, only the data length Nc and n/e ratio m are related to test conﬁgu-
ration, through the following:
0
0.1 0.2 0.3 0.4 0.5
0
0.1
0.2
0.3
0.4
0.5
c.o.v. f [%]
0
20
40
60
0
20
40
60
c.o.v. ζ [%]
0
10
20
30
40
0
10
20
30
40
c.o.v. S [%]
0
5
10
15
0
5
10
15
c.o.v. Se [%]
Exact
Uncertainty law
0
1
2
3
4
5
0
1
2
3
4
5
c.o.v. φ [%]
Fig. 15.9 Comparison of exact posterior c.o.v. with uncertainty law
Table 15.2 Statistics of Se, S (MPV, one-sided) and m ¼ Se=S, tall building example
Mode
Se [ðlgÞ2=Hz]
S [ðlgÞ2=Hz]
m ¼ Se=S
Min
Mean
Max
Min
Mean
Max
Min
Mean
Max
1
68
301
977
4.0
19
61
5.2
17
41
2
44
208
501
4.6
19
61
6.2
13
36
3
0.8
2.2
7.9
0.1
0.4
1.2
3.0
5.5
11
4
0.4
0.8
2.1
0.2
0.6
1.5
0.8
1.5
2.5
5
0.6
2.3
6.4
0.1
0.4
1.0
4.2
6.7
11
6
0.3
1.6
6.1
0.02
0.1
0.2
10
19
36
468
15
Managing Identiﬁcation Uncertainties

1. data duration Td
2. channel noise level Se
3. measured DOFs (number and location)
The sophistication of decision-making process depends on the available infor-
mation or assumption one is willing to make. The damping ratio has the largest
posterior uncertainty and so it is the governing factor for planning decisions.
15.4.1
Simple Rule of Thumb
The zeroth order law of damping uncertainty provides a simple decision rule for the
data duration independent of equipment conﬁguration, as it is the limit for inﬁnite
modal s/n ratio. The required data length as a multiple of natural period to achieve a
posterior c.o.v. of d in the damping ratio is given by
Nc ¼
1
2pfd2BfðjÞ
ð15:7Þ
where BfðjÞ can be found in Table 15.1. For f ¼ 1%, d ¼ 30% and j ¼ 6 (for
which Bf 	 0:6), (15.7) gives a rule of thumb of 300 natural periods. To allow for
deviations from these assumptions, one may use some convenient multiple of this
rule of thumb as long as the measured data can still be considered stationary, e.g.,
500 natural periods or 1000 natural periods.
15.4.2
Accounting for Channel Noise and Measured DOFs
To account for channel noise and measured DOFs, one needs to go beyond the rule
of thumb that is based on the zeroth order law. One can apply the ﬁrst order law of
damping uncertainty to obtain the required data length:
Nc ¼ 1 þ afðjÞc1
2pfd2BfðjÞ
ð15:8Þ
where afðjÞ can be found in Table 15.1 and c ¼ S=4Sef2ð2pf Þ2q is the modal s/n
ratio. For j ¼ 6, af 	 12:7. To assess c, assume (say) acceleration data (q ¼ 0) and
a nominal value of 1% for the damping ratio. Here, Se is the channel noise PSD in
the vicinity of the natural frequency. This depends on the quality of sensor and data
acquisition system (cables, DAQ hardware, etc.). For example, well-managed
systems with servo-accelerometers have a channel noise PSD Se in the order of 1
ðlgÞ2=Hz. The accelerometer data acquired by smart phone in Sect. 6.3 has a Se in
15.4
Planning Ambient Vibration Tests
469

the order of 105ðlgÞ2=Hz. The modal force PSD S depends on environmental
excitation intensity and the measured DOFs. On a normal day, S may range
(say) between 0.01 to 10 ðlgÞ2=Hz for a single DOF suitably placed for the mode.
If the additional DOFs have a mode shape value less than the existing measured
DOF, adding such DOFs can at most increase the value of S linearly.
The sensitivity of df1 to c determines whether the posterior c.o.v. can be further
reduced effectively by improving the quality of equipment or increasing the number
of measured DOFs. For this purpose, plotting df1=df0 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ af=c
p
and locating
where the existing conﬁguration stands on the curve can give an idea of sensitivity.
Increasing data duration can always reduce identiﬁcation uncertainty in an inverse
square root manner, although this needs to be traded-off with model error risk in
terms of, e.g., stationarity of measured response (the longer the higher).
Example 15.4 (Planning for tall building) Here we consider a hypothetical
ambient test on a tall building to illustrate the use of uncertainty laws in determining
whether a mode can be identiﬁed reliably under different situations. Consider the
fundamental translational mode with natural frequency f ¼ 0:2 Hz and damping
ratio f ¼ 1%. In terms of data length requirement, the fundamental mode is the
most demanding because it has the longest period.
Low channel noise
Suppose we use a triaxial servo-accelerometer (i.e., 3 DOFs) with a noise PSD of
Se ¼ 1ðlgÞ2=Hz, typical of good quality data channel for this purpose. On a normal
day with a sensor placed at one corner on the roof, assume a modal force PSD of
S ¼ 0:1ðlgÞ2=Hz
(microtremor
level).
Then
m ¼ 1=0:1 ¼ 10
and
c ¼ 1=4ð10Þð0:01Þ2 ¼ 250. From Fig. 15.5 the ratio df1=df0 is ﬂat for c [ 100
(regardless of j) and so the posterior c.o.v. is insensitive to the measured DOFs and
quality of data channel. They are already good enough to give a high modal s/n
ratio. The zeroth order and ﬁrst order law give practically the same posterior c.o.v.
value. Assume j＝6 and so Bf 	 0.6. With 300 natural periods of data, i.e., (5)
(300) = 1500 s,
df 	 df1 	 df0 ¼ 1=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2pð0:01Þð300Þð0:6Þ
p
	 30%
Increasing the data duration (while trading off with modeling error risk) is the only
effective means that can further reduce identiﬁcation uncertainty. Adding sensors or
further reducing channel noise (e.g., better sensor) will have little or no effect.
High channel noise
Suppose now the sensor or data acquisition system has lower quality, in the sense
that Se ¼ 100ðlgÞ2=Hz (100 times higher than before). This gives m ¼ 100=0:1 ¼
1000 and c ¼ 1=ð4Þð1000Þð0:01Þ2 ¼ 2:5. The zeroth order law is the same as
before (about 30%) because it does not depend on the modal s/n ratio. For j ¼ 6,
af 	 12:7 and so the ﬁrst order law gives
470
15
Managing Identiﬁcation Uncertainties

df1 ¼ df0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ af=c
q
¼ ð0:3Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ 12:7=2:5
p
¼ 74%
This is quite large. From Fig. 15.5, the ratio df1=df0 is sensitive to c when c\10
and so the posterior c.o.v. may be effectively reduced by improving the quality of
sensor or DAQ system, or increasing the number of measured DOFs. Some options
are considered below:
Reducing channel noise. Reducing the channel noise PSD Se from 100 to 10
ðlgÞ2=Hz reduces m to 100 and increases c to 25. The zeroth order law df0 is
unaffected but the ratio df1=df0 is reduced from
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ 12:7=2:5
p
¼ 2:47 to
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ 12:7=25
p
¼ 1:23, i.e., by half. As a result, the posterior c.o.v. of damping
ratio is reduced from 74% to 37%. Further reducing the channel noise PSD to 1
ðlgÞ2=Hz or lower can at best bring df1 down to 30% and so has diminishing return.
Additional sensors. Suppose we keep using the same sensor and DAQ system,
i.e., Se remains at 100 ðlgÞ2=Hz, but now we install additional triaxial sensors at
other corners on the roof. Since the mode shape values at other corners is similar to
the existing ones, modal force PSD S grows roughly linearly with the number of
sensors. Suppose we put another three sensors at the remaining three corners on the
roof. This will increase S by four times from 0.1 to 0.4 ðlgÞ2=Hz and hence c from
2.5 to 10. The resulting ratio of df1=df0 reduces from 2.47 to
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ 12:7=10
p
¼ 1:5
and the posterior c.o.v. df1 reduces from 74% to (30%) (1.5) = 45%.
Longer data length. Without additional sensors, the posterior c.o.v. can be
reduced effectively by increasing data duration. Doubling data duration from 300 to
600 natural periods reduces the posterior c.o.v to 1=
ﬃﬃﬃ
2
p
of its original value (i.e., by
about 30%) to 32%, which may be acceptable. The resulting data duration is
(5) (600) = 3000 s (about an hour). One needs to examine the data to see if a
stochastic stationary assumption is justiﬁed. This is especially relevant for damping
ratios and modal force PSDs.
■
15.5
Common Sense
Quick comments are in order, some we knew and now conﬁrmed by uncertainty
laws; and some we were puzzled and now obtain surprising answer.
Identiﬁcation uncertainty in terms of c.o.v. reduces in an inverse square root
manner with data length. To reduce c.o.v. by half, one needs to quadruple data
length. The natural frequency and mode shape are much easier to identify than the
damping ratio, because the posterior c.o.v. of the former is Oð
ﬃﬃﬃf
p Þ but that of the
latter is Oð1=
ﬃﬃﬃf
p Þ. These agree with our intuitions.
If we want to obtain mode shape at more locations, then obviously we need more
sensors. Discovering more modes or separating close modes are also common
reasons for deploying more sensors. Otherwise, improving the precision of modal
15.4
Planning Ambient Vibration Tests
471

properties is not a good justiﬁcation for deploying more sensors. One either has
good enough accuracy or it would not help with just a few more sensors. This is
because if the quality of sensor is good (say servo-accelerometer with micro-g/
ﬃﬃﬃﬃﬃﬃ
Hz
p
noise), then chances are the modal s/n ratio is already good enough even with one
or two sensors. Otherwise, a material reduction in the posterior c.o.v. of natural
frequency or damping ratio would require an order of magnitude increase in the
modal s/n ratio and hence the number of sensors. Thus, a structural health moni-
toring (SHM) system with only one or two good accelerometers can give practically
the same accuracy for the natural frequency or damping ratio as another one with
hundreds. If the sensors have high noise, they are simply not ﬁt for the purpose and
should be substituted by better ones. Trying to make this up by using more sensors
is not cost-effective.
472
15
Managing Identiﬁcation Uncertainties

Chapter 16
Theory of Uncertainty Laws
Abstract This chapter discusses the theory behind the uncertainty laws in
Chap. 15. It is intended for researchers who are interested in understanding the
mathematics of the theory or learning the techniques for further development.
Keywords Uncertainty laws  Asymptotics  Fisher information matrix
In this chapter we discuss the reasoning and derivation behind the uncertainty laws
for operational modal analysis (OMA) in Chap. 15. Recall the context of identi-
fying well-separated modes with single setup data in Chap. 12. The set of modal
parameters is h ¼ ff ; f; S; Se; ug, where f is the natural frequency (Hz), f is the
damping ratio, S is the modal force PSD, Se is the prediction error PSD and u
(n  1) is the mode shape (conﬁned to the measured DOFs); n is the number of
measured DOFs. Assuming a uniform prior probability density function (PDF), the
posterior PDF of h given the scaled Fast Fourier Transform (FFT) f ^F kg (n  1
complex) of data in a selected frequency band containing the mode is
pðhjf ^F kgÞ / exp½Lðf ^F kg; hÞ, where
Lðf ^F kg; hÞ ¼ nNf ln p þ
X
k
ln jEkj þ
X
k
^F

kE1
k ^F k
ð16:1Þ
is the negative log-likelihood function (NLLF), whose dependence on f ^F kg and h
has been explicitly indicated; Nf is the number of FFT points in the selected band;
Ek ¼ SDk uuT þ SeIn
ð16:2Þ
is the theoretical PSD matrix of data for given h;
Dk ¼
ð2pfkÞ2q
ð1  b2
kÞ2 þ ð2fbkÞ2
bk ¼ f
fk
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð16:3Þ
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1_16
473

is the dynamic ampliﬁcation factor at FFT frequency fk ¼ k=Td; Td is the data
duration (s); and
u ¼ jjujj1u
ð16:4Þ
is the normalized mode shape. Here Ek is written in terms of u so that the Hessian
of L w.r.t. u automatically accounts for mode shape norm constraint.
The posterior covariance matrix of h is equal to the pseudo-inverse (i.e., ignoring
norm constraint singularity) of the Hessian of L w.r.t. h at the posterior most
probable value (MPV). Uncertainty laws are asymptotic expressions for the leading
order of the posterior covariance matrix for long data (Nf ! 1) and small damping
(f ! 0). The FFT data f ^F kg is unknown before test but it is assumed to be
distributed as the likelihood function.
Long data asymptotics, small damping asymptotics, and asymptotic decoupling
are three important concepts in the derivation. Long data asymptotics retains in the
leading order of the Hessian of NLLF the ‘information content’ of data (rather than
its speciﬁc details), equal to the Fisher information matrix. The latter involves sums
of dynamic ampliﬁcation factor and derivatives over the frequency band.
Consideration of small damping and hence high modal signal-to-noise (s/n) ratio
leads to closed form expressions. Taking inverse of the resulting Hessian is still
algebraically intractable. For long data, the mode shape is decoupled from the
remaining parameters, in the sense that the cross derivatives vanish asymptotically.
For long data and small damping, the prediction error PSD and natural frequency
are also decoupled from the remaining parameters. These ﬁndings reduce the
dimension of inverse, making it algebraically manageable and eventually leading to
closed form expressions.
The uncertainty laws are generally applicable for displacement, velocity or
acceleration data. Acceleration data is assumed in the main derivation. Necessary
modiﬁcations to the derivation for velocity or displacement data are discussed in
Sect. 16.6.
16.1
Long Data Asymptotics
The leading order of the posterior covariance matrix for long data can be obtained
from ﬁrst principle or by using its connection with the Fisher information matrix
(FIM). From ﬁrst principle, substituting the stochastic representation of scaled FFT
into the second derivatives of the NLLF leads to an expression comprising sums of
random terms, whose leading order is its expectation for large Nf . This approach
was adopted when the uncertainty laws were ﬁrst derived (Au 2014). A more
systematic approach is to make use of the connection with the FIM (Sect. 9.6.3).
Let ^LðxyÞ denote the derivative of the NLLF w.r.t. variables x and y, and evaluated at
the MPV for given f ^F kg distributed as the likelihood function pðf ^F kgjhÞ. Then
474
16
Theory of Uncertainty Laws

^LðxyÞ ¼ JxyðhÞ½1 þ OðN1=2
f
Þ
Nf ! 1
ð16:5Þ
where
JxyðhÞ ¼ E½LðxyÞjh
ð16:6Þ
is the entry corresponding to parameter pair ðx; yÞ in the FIM. This implies that
^LðxyÞ  Jxy if Jxy 6¼ 0. Otherwise, the leading order of ^LðxyÞ will need to be deter-
mined by other means. Since the likelihood function is a complex Gaussian PDF,
one can apply the standard result in Sect. 9.4.2 to give
Jxy ¼
X
k
tr½E1
k EðxÞ
k E1
k EðyÞ
k 
ð16:7Þ
where a superscripted variable denotes a partial derivative w.r.t. it; trðÞ denotes the
trace (i.e., sum of diagonal entries) of the argument matrix.
Abuse in Notation
Some abuse in notation has been used to simplify presentation but it should be
clariﬁed to avoid conceptual confusion. In (16.5) and (16.6), h denotes the actual
parameter value that gives rise to the FFT data f ^F kg. In (16.7), x and y (which
come from h) are variables, for otherwise it does not make sense to take derivative
of Ek w.r.t. them. The derived expression of Jxy will involve x and y, and hence h,
which should then be interpreted as the actual parameter value again. This type of
confusion or interpretation arises from the classical statistical context of FIM.
16.1.1
Fisher Information Matrix
The trace in (16.7) can be evaluated effectively using the eigenvector representation
of Ek. Let fbign
i¼1 be an orthonormal basis in Rn with b1 ¼ u. Clearly, it depends
on u but not on other parameters (f ; f; S; Se). Substituting In ¼ Pn
i¼1 bibT
i into Ek in
(16.2) and rearranging gives the eigenvector representation of Ek, from which E1
k
can also be obtained:
Ek ¼ ðSDk þ SeÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
eigenvalue
b1bT
1 þ
Se
|{z}
eigenvalue
X
n
i¼2
bibT
i
E1
k
¼ ðSDk þ SeÞ1
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
eigenvalue
b1bT
1 þ
S1
e
|{z}
eigenvalue
X
n
i¼2
bibT
i
ð16:8Þ
16.1
Long Data Asymptotics
475

Terms Not Involving Mode Shape
For x ¼ f ; f; S; Se, differentiation simply operates on the eigenvalues:
EðxÞ
k
¼ ðSDk þ SeÞðxÞb1bT
1 þ SðxÞ
e
X
n
i¼2
bibT
i
ð16:9Þ
The same also applies to the product. For x; y ¼ f ; f; S; Se,
E1
k EðxÞ
k E1
k EðyÞ
k
¼ ðSDk þ SeÞðxÞðSDk þ SeÞðyÞ
ðSDk þ SeÞ2
b1bT
1 þ SðxÞ
e SðyÞ
e
S2
e
X
n
i¼2
bibT
i
ð16:10Þ
Taking trace, noting trðbibT
i Þ ¼ 1 (i ¼ 1; . . .; n) and summing over k,
Jxy ¼
X
k
ðSDk þ SeÞðxÞðSDk þ SeÞðyÞ
ðSDk þ SeÞ2
þ ðn  1ÞNf
SðxÞ
e SðyÞ
e
S2
e
x; y ¼ f ; f; S; Se
ð16:11Þ
Terms Involving Mode Shape
The terms Jxu (x ¼ f ; f; S; Se) and Juu involve differentiation w.r.t. u. Analysis can
be simpliﬁed by making use of orthogonality between u and the gradient of
u ¼ jjujj1u. Let u ¼ ½/1; . . .; /nT. Then it can be shown that (Section C.5.5)
ru ¼
@u
@/1
  
@u
@/n


¼ In  uuT
ðjjujj ¼ 1Þ
ð16:12Þ
It follows that
ðruÞT ¼ ru
ðruÞðruÞ ¼ ru
ðruÞu ¼ 0
uTðruÞ ¼ 0 ð16:13Þ
Eð/iÞ
k
¼ SDk½uð/iÞuT þ uuð/iÞT ¼ SDk½ðruÞeiuT þ ueT
i ðruÞ
ð16:14Þ
where ei is a n  1 vector whose ith entry is the only non-zero entry equal to 1.
Using these, it can be shown that for jjujj ¼ 1 and x ¼ f ; f; S; Se,
E1
k EðxÞ
k
¼ ðSDk þ SeÞðxÞ
SDkð1 þ ekÞ uuT þ SðxÞ
e
Se
ru
E1
k Eð/iÞ
k
¼ ð1 þ ekÞ1ueT
i ðruÞ þ e1
k ðruÞeiuT
ð16:15Þ
E1
k EðxÞ
k E1
k Eð/iÞ
k
¼ ðSDk þ SeÞðxÞ
SDkð1 þ ekÞ2 ueT
i ðruÞ þ SðxÞ
e
Seek
ðruÞeiuT
ð16:16Þ
476
16
Theory of Uncertainty Laws

E1
k Eð/iÞ
k
E1
k E
ð/jÞ
k
¼ ð1 þ ekÞ1e1
k
ueT
i ðruÞejuT þ ðruÞeieT
j ðruÞ
h
i
ð16:17Þ
where
ek ¼ Se
SDk
ð16:18Þ
Taking trace on (16.16) and summing over k gives
Jx/i ¼ 0
x ¼ f ; f; S; Se
ð16:19Þ
since
tr½ueT
i ðruÞ ¼ tr½ðruÞueT
i  ¼ 0
and
tr½ðruÞeiuT ¼ tr½uTðruÞei ¼ 0
(cyclic property of trace). This indicates that the leading order of ^Lðx/iÞ is not given
by the FIM. Direct investigation of ^Lðx/iÞ, which is a sum of Nf random terms,
shows that it is OðN1=2
f
Þ.
On the other hand, taking trace on (16.17) and summing over k gives
J/i/j ¼ 2
X
k
ð1 þ ekÞ1e1
k ½eT
i ðruÞej
ð16:20Þ
since, using the cyclic property of trace,
tr½ueT
i ðruÞejuT ¼ tr½ uT u
|ﬄ{zﬄ}
1
eT
i ðruÞej
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
scalar
 ¼ eT
i ðruÞej
tr½ðruÞeieT
j ðruÞ ¼ tr½eT
j ðruÞðruÞ
zﬄﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄﬄ{
ru
ei
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
scalar
 ¼ eT
j ðruÞ
|ﬄﬄ{zﬄﬄ}
sym:
ei ¼ eT
i ðruÞej
ð16:21Þ
Since eT
i ðruÞej is simply the ði; jÞ-entry of ru, (16.20) implies
Juu ¼ 2
X
k
ð1 þ ekÞ1e1
k ðruÞ ¼ 2
X
k
ð1 þ ekÞ1e1
k ½In  uuT ð16:22Þ
Table 16.1 summarizes the long data asymptotic expressions for the second
derivatives of the NLLF at the MPV. The derivatives involving Dk have been
written in terms of those of D1
k , as the latter are easier to analyze. For acceleration
data (q ＝0),
16.1
Long Data Asymptotics
477

Table 16.1 Leading order of ^LðxyÞ for Nf ! 1; ek ¼ Se=SDk; jjujj ¼ 1
y ¼
f ; f
S
Se
u
x ¼
f ; f
P
k
D2
kðD1
k ÞðxÞðD1
k ÞðyÞ
ð1 þ ekÞ2
Sym.
S
S1 P
k
DkðD1
k ÞðyÞ
ð1 þ ekÞ2
S2 P
k
ð1 þ ekÞ2
Se
S1 P
k
ðD1
k ÞðyÞ
ð1 þ ekÞ2
S2 P
k
D1
k
ð1 þ ekÞ2
S2
e
ðn  1ÞNf þ P
k
e2
k
ð1 þ ekÞ2
"
#
u
OðN1=2
f
Þ
OðN1=2
f
Þ
OðN1=2
f
Þ
2 P
k
ð1 þ ekÞ1e1
k


ðIn  uuTÞ
478
16
Theory of Uncertainty Laws

ðD1
k Þðf Þ ¼ 4f 1b2
kðb2
k  1 þ 2f2Þ
ðD1
k ÞðfÞ ¼ 8fb2
k
ð16:23Þ
For ^LðSeSeÞ, the second term in the bracket can be omitted as it is dominated by the
ﬁrst term. It is retained to be consistent with the expression of Jxy in (16.11).
16.2
Small Damping Asymptotics
The long data asymptotic expressions in Table 16.1 involve discrete sums over
frequencies. More insightful expressions can be obtained for small damping. Using
the Taylor approximation ð1 þ ekÞ1  1  ek, the leading order of the sums can be
written in the standard form P
k Da
kðbk  1Þbbc
k, where a, b and c are integers.
Viewed as a Riemann sum, this can be shown to be asymptotic to an integral:
X
k
Da
kðbk  1Þbbc
k  ð1Þb22aNcfb2a þ 1
Zj
j
ð1 þ fuÞ4abcub
½u2ð1 þ fu=2Þ2 þ ð1 þ fuÞ2a du
Nf ! 1
ð16:24Þ
Analytical expression for the integral can be obtained for f ! 0. Details can be
found in Appendix A of this chapter. Using these results, closed-form asymptotic
expressions for the second derivatives of the NLLF at the MPV can be obtained.
Details can be found in Appendix B of this chapter. The results are summarized in
Table 16.2.
16.3
Asymptotic Decoupling
Inverting the Hessian of the NLLF is analytically intractable even when using the
leading order expressions in Table 16.2. Further simpliﬁcation results by discov-
ering that the mode shape, prediction error PSD and natural frequency are
asymptotically ‘decoupled’ from the remaining parameters, reducing the dimension
of inverse.
16.3.1
Scalar Parameter
A parameter hi is ‘perfectly decoupled’ from the remaining ones if ^LðhihjÞ ¼ 0 for all
j 6¼ i. In general, coupling can be considered small if jqhihjj  1 for all j 6¼ i, where
16.1
Long Data Asymptotics
479

Table 16.2 Leading (zeroth) order of ^LðxyÞ for Nf ! 1 and f ! 0; jjujj ¼ 1
y ¼
f
f
S
Se
u
x ¼
f
4Nc
f 2f
tan1 j 
j
j2 þ 1


Sym.
f
4Nc
f
tan1 j þ 2j3 þ j
ðj2 þ 1Þ2
"
#
4Nc
f
tan1 j þ
j
j2 þ 1


S
 2Ncf
fS
2 tan1 j þ
j3
j2 þ 1


 4Nc
S tan1 j
Nf
S2
Se
 8Ncf3
3fS ð7j3 þ 6jÞ
 16Ncf2j
S
8Ncf3
3S2 ðj3 þ 3jÞ
ðn  1ÞNf
S2
e
u
OðN1=2
f
Þ
OðN1=2
f
Þ
OðN1=2
f
Þ
OðN1=2
f
Þ
Nc tan1 j
mf
ðIn  uuTÞ
480
16
Theory of Uncertainty Laws

qhihj ¼
^LðhihjÞ
½^LðhihiÞ1=2½^LðhjhjÞ1=2
ð16:25Þ
is the (dimensionless) ‘cross sensitivity coefﬁcient’ between hi and hj.
When hi is asymptotically decoupled from all the remaining parameters, its
leading order posterior variance is simply given by 1=^LðhihiÞ. This can be reasoned
as follow. The full Hessian of L at the MPV, denoted by ^H (nh  nh), can be
factored as
^H ¼ diag½f
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^LðhjhjÞ
p
gnh
j¼1Qdiag½f
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^LðhjhjÞ
p
gnh
j¼1
ð16:26Þ
where diag½fgnh
j¼1 denotes a diagonal matrix formed by the entries in the argument;
Q is the sensitivity matrix whose ði; jÞ-entry is qhihj. The diagonal entries of Q are
all equal to 1. Taking inverse,
^H1 ¼ diag½f1=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^LðhjhjÞ
p
gnh
j¼1Q1diag½f1=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^LðhjhjÞ
p
gnh
j¼1
ð16:27Þ
Without loss of generality, suppose the cross sensitivity coefﬁcient of h1 with all
other parameters is small, i.e., qh1hj  1 for all j 6¼ 1. Let Q and Q1 be partitioned
as
Q ¼
1
qT
1ðnh1Þ
q
ðnh1Þ1
Q1
ðnh1Þðnh1Þ
2
64
3
75
Q1 ¼
a
bT
1ðnh1Þ
b
ðnh1Þ1
B
ðnh1Þðnh1Þ
2
4
3
5
ð16:28Þ
where qT ¼ ½qh1h2; qh1h3; . . .; qh1hnh  and a is a scalar. Substituting (16.28) into
QQ1 ¼ I (identity matrix) and solving gives a ¼ 1 þ qTBq. The posterior vari-
ance of h1 is then given by
r2
h1 ¼ ^H1ð1; 1Þ ¼
a
^Lðh1h1Þ ¼ 1 þ qTBq
^Lðh1h1Þ
ð16:29Þ
Thus, if qh1hj  1 for all j 6¼ 1, then qTBq  1 and so r2
h1  1=^Lðh1h1Þ.
16.3.2
Vector-Valued Parameter
For a vector-valued parameter such as mode shape u (n  1), it is asymptotically
decoupled from the remaining parameters if the cross sensitivity
16.3
Asymptotic Decoupling
481

qhju ¼
max
u 2 Rn
jjujj ¼ 1
^LðhjuÞu
½^LðhjhjÞ1=2½uT^LðuuÞu1=2
ð16:30Þ
is small with all other parameters hj. This can be reasoned as follow. Consider the
second order Taylor series of L for small perturbations Dhj and Du about the MPV:
DL ¼ Lð^hj þ Dhj; ^u þ DuÞ  Lð^hj; ^uÞ
	 1
2 ½^LðhjhjÞDh2
j
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
Qhjhj
þ 2 Dhj^LðhjuÞDu
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
Qhju
þ DuT^LðuuÞDu
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Quu

ð16:31Þ
where the ﬁrst order terms vanish because L is minimized at the MPV. Writing in
complete-square form,
DL 	 1
2 ðQ1=2
hjhj þ Q1=2
uu Þ2  Q1=2
hjhjQ1=2
uu ð1  qÞ
ð16:32Þ
where q ¼ Qhju=Q1=2
hjhjQ1=2
uu . Writing Du ¼ jjDujju, where u is a unit vector, we
have q ¼ ^LðhjuÞu=½^LðhjhjÞ1=2½uT^LðuuÞu1=2. Thus, if jqj  1 for any unit vector u
then
the
second
term
in
(16.32)
is
approximately
Q1=2
hjhjQ1=2
uu
and
hence
DL 	 ðQ2
hjhj þ Q2
uuÞ=2, which does not involve any cross derivative term between
hj and u. This condition is equivalent to jqhjuj  1 deﬁned in (16.30).
16.4
Leading Order Uncertainty
Using Table 16.2, the order of magnitude of cross sensitivity coefﬁcients can be
assessed. The results are summarized in Table 16.3. For cross sensitivities
involving u, only the scaling order w.r.t. Nf is indicated. Terms with f or N1
f
to
some power are asymptotically zero.
Posterior Variance of f , Se and u
Among all parameter pairs, the only non-vanishing sensitivity coefﬁcient is qfS.
Thus, f , Se and u are asymptotically decoupled from the remaining parameters.
This implies that their posterior variances are asymptotically given by
r2
f 
1
^Lðff Þ
r2
Se 
1
^LðSeSeÞ
^Cu  ½^LðuuÞ1
ð16:33Þ
482
16
Theory of Uncertainty Laws

Substituting ^Lðff Þ, ^LðSeSeÞ and ^LðuuÞ from Table 16.2 gives their leading (zeroth)
order posterior covariance in Table 16.4. The inverse of ^LðuuÞ is obtained via its
eigenvector representation.
Posterior Variance of f and S
The parameters f and S are coupled and need to be considered together. Their
posterior covariance matrix is asymptotically the inverse of
^LðffÞ
^LðfSÞ
^LðfSÞ
^LðSSÞ


¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^LðffÞ
p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^LðSSÞ
p


1
qfS
qfS
1


ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^LðffÞ
p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^LðSSÞ
p


ð16:34Þ
where qfS ¼ ^LðfSÞ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^LðffÞ^LðSSÞ
p
. Substituting ^LðffÞ, ^LðSSÞ and ^LðfSÞ in Table 16.2 gives
qfS  
ﬃﬃﬃ
2
p
ðtan1 jÞj1=2ðtan1 j þ
j
j2 þ 1Þ1=2
ð16:35Þ
Taking inverse on (16.34),
^LðffÞ
^LðfSÞ
^LðfSÞ
^LðSSÞ

1
¼
1ﬃﬃﬃﬃﬃﬃﬃ
^LðffÞ
p
1ﬃﬃﬃﬃﬃﬃﬃ
^LðSSÞ
p
2
4
3
5
1
1  q2
fS
1
qfS
qfS
1


1ﬃﬃﬃﬃﬃﬃﬃ
^LðffÞ
p
1ﬃﬃﬃﬃﬃﬃﬃ
^LðSSÞ
p
2
4
3
5
ð16:36Þ
Reading off the (1,1) and (2,2) entries gives the posterior variances of f and S:
r2
f 
1
^LðffÞð1  q2
fSÞ
r2
S 
1
^LðSSÞð1  q2
fSÞ
ð16:37Þ
Substituting ^LðffÞ and ^LðSSÞ from Table 16.2 and qfS from (16.35) gives the corre-
sponding posterior variances in Table 16.4.
Table 16.3 Leading order of sensitivity coefﬁcients qxy
y ¼
f
f
S
Se
u
x ¼
f
1
f
OðfÞ
1
Sym.
S
Oðj1=2fÞ
Oðj1=2Þ
1
Se
Oðmj5=2f3n1=2Þ
Oðmj1=2f2n1=2Þ
Oðmj2f2n1=2Þ
1
u
OðN1=2
f
Þ
OðN1=2
f
Þ
OðN1=2
f
Þ
OðN1=2
f
Þ
1
16.4
Leading Order Uncertainty
483

Table 16.4 Leading (zeroth) order posterior statistics for Nf ! 1 and f ! 0. Variance on diagonal; correlation coefﬁcient on off-diagonal
f
f
S
Se
u
f
f 2f
4Nc
tan1 j 
j
j2 þ 1

1
f
OðfÞ
f
4Nc
tan1 j þ
j
j2 þ 1  2ðtan1 jÞ2
j
"
#1
Sym.
S
Oðj1=2fÞ
Oðj1=2Þ
S2
Nf
1 
2ðtan1 jÞ2
jðtan1 j þ
j
j2 þ 1Þ
"
#1
Se
Oðmj5=2f3n1=2Þ
Oðmj3=2f2n1=2Þ
Oðmj2f2n1=2Þ
S2
e
ðn  1ÞNf
u
OðN1=2
f
Þ
OðN1=2
f
Þ
OðN1=2
f
Þ
OðN1=2
f
Þ
mf
Nc tan1 j ðIn  uuTÞ
For correlations involving u only the scaling order w.r.t. Nf is indicated
484
16
Theory of Uncertainty Laws

Posterior Correlations
Among all parameter pairs, only f and S are asymptotically correlated. From
(16.36), their posterior correlation coefﬁcient is qfS   qfS. The correlation
coefﬁcient between any other parameter pair is asymptotically zero. The order of
magnitude can be assessed as follow. The correlation of u with any other parameter
is clearly OðN1=2
f
Þ. The correlation among other parameters (f ; f; S; Se) does not
depend on Nf . It can be investigated through the sensitivity matrix. Let
Q ¼
1
qf f
qfS
qfSe
1
qfS
qfSe
1
qSSe
sym:
1
2
664
3
775
ð16:38Þ
Under long data asymptotics where u is decoupled, the correlation between f and
f is given by qf f ¼ Q1ð1; 2Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Q1ð1; 1ÞQ1ð2; 2Þ
p
where Q1(i,j) denotes the
(i,j)-entry of Q1. It can be reasoned that Q1ð1; 1Þ and Q1ð2; 2Þ are Oð1Þ. On the
other hand, Q1ð1; 2Þ ¼ c12=jQj, where
c12 ¼
qf f
qfS
qfSe
qfS
1
qSSe
qfSe
qSSe
1
2
4
3
5


¼ qf f  qfSqfS  qfSeqfSe  qf fq2
SSe þ qfSqfSeqSSe þ qfSeqfSqSSe
ð16:39Þ
is the cofactor. It can be reasoned that jQj ¼ Oð1Þ and so qf f ¼ Oðc12Þ. The order
of magnitude of c12 can be determined using those of the sensitivity coefﬁcients in
Table 16.3. The correlation of other pairs can be determined using the same rea-
soning above. The results are summarized in the off-diagonal entries of Table 16.4.
In most cases the correlation is of the same order as the sensitivity coefﬁcient. The
only exception is qfSe, which is Oðj3=2Þ but qfSe ¼ Oðj1=2Þ.
16.5
First Order Effect of Signal-to-Noise Ratio
The asymptotic expressions of the NLLF derivatives and posterior covariance
matrix derived in Sect. 16.4 are of the ‘zeroth order’, in the sense that they (except
for the mode shape) do not depend on the n/e ratio m ¼ Se=S and hence modal s/n
ratio c ¼ 1=4mf2. The leading order posterior variance in Table 16.4 reﬂects the
achievable identiﬁcation precision limit of modal parameters when the modal
excitation is not known but assumed to be broadband random. To investigate the
effect of test conﬁguration, it is necessary to capture the ﬁrst order behavior w.r.t. m.
The basic procedure is to retain the ﬁrst order term ek in the long data asymptotic
expressions in Table 16.1 and apply small damping asymptotics to obtain the ﬁrst
order terms of the NLLF derivatives. The ﬁrst order terms of the posterior variances
16.4
Leading Order Uncertainty
485

are then obtained by perturbation w.r.t. the NLLF derivatives. For ^LðSeSeÞ, it can be
reasoned from the expression in Table 16.1 that the effect of m is second order. On
the other hand, m already appears in the leading order of ^LðuuÞ. It therefore remains
to consider f , f and S.
As a reﬁned statement of (16.33) and (16.37), the posterior variances of f ; f; S
are given by
r2
f  1 þ Oðq2Þ
^Lðff Þ
r2
f 
1 þ Oðq2Þ
^LðffÞð1  q2
fSÞ
r2
S 
1 þ Oðq2Þ
^LðSSÞð1  q2
fSÞ
ð16:40Þ
where Oðq2Þ denotes a remainder term that is generally a quadratic function of the
sensitivity coefﬁcients. For example, for r2
f , Oðq2Þ is the leading order among any
product of qf f, qfS and qfSe. Table 16.5 shows the order of these products based on
Table 16.3. The leading order among these products is Oðf2Þ and so for r2
f the
remainder term Oðq2Þ is Oðf2Þ. Using a similar reasoning, it can be deduced that the
remainder terms for r2
f and r2
S are also Oðf2Þ. These remainder terms do not contain
m. The ﬁrst order effect of m on the posterior variances therefore stems purely from
the ﬁrst order terms in ^Lðff Þ, ^LðffÞ, ^LðSSÞ and qfS.
First Order Perturbation
Taking differential on the natural log of (16.40), the ﬁrst order term (denoted with a
preﬁx D) of r2
x (x ¼ f ; f; S) about the zeroth order is related to those of ^LðxxÞ and q2
fS
by
Dðr2
f Þ
r2
f 0
¼  D^Lðff Þ
^Lðff Þ
0
Dðr2
fÞ
r2
f0
¼  D^LðffÞ
^LðffÞ
0
þ Dðq2
fSÞ
1  q2
fS0
Dðr2
SÞ
r2
S0
¼  D^LðSSÞ
^LðSSÞ
0
þ Dðq2
fSÞ
1  q2
fS0
ð16:41Þ
Table 16.5 Scaling order of q1q2. Only the scaling w.r.t. f and m is indicated
q2
qf f ¼ OðfÞ
qfS ¼ OðfÞ
qfSe ¼ Oðmf3Þ
q1
qf f ¼ OðfÞ
Oðf2Þ
Oðf2Þ
Oðmf4Þ
qfS ¼ OðfÞ
Oðf2Þ
Oðmf4Þ
qfSe ¼ Oðmf3Þ
Sym.
Oðm2f6Þ
486
16
Theory of Uncertainty Laws

where a subscript ‘0’ denotes the zeroth order. See Table 16.2 for ^LðxyÞ
0
(x; y ¼ f ; f; S) and Table 16.4 for r2
x0 (x ¼ f ; f; S). Also, using (16.35),
q2
fS0 ¼ ð^LðfSÞ
0
Þ2
^LðffÞ
0
^LðSSÞ
0
¼ 2ðtan1 jÞ2j1ðtan1 j þ
j
j2 þ 1Þ1
ð16:42Þ
Using ð1 þ ekÞ1  1  ek, where ek ¼ Se=SDk ¼ mD1
k , it can be shown that
generally (see Appendix C of this chapter)
^LðxyÞ  ^LðxyÞ
0
ð1  cxyc1Þ
x; y ¼ f ; f; S
ð16:43Þ
where c ¼ S=4Sef2 is the modal s/n ratio; and, for the relevant terms here,
cff ¼ 4ðj  tan1 jÞ
tan1 j 
j
j2 þ 1
cff ¼
4 tan1 j
tan1 j þ
j
j2 þ 1
cSS ¼ 2 1 þ j2
3


cfS ¼
2j
tan1 j
ð16:44Þ
Equation (16.43) indicates that
D^LðxyÞ
^LðxyÞ
0
¼ cxyc1
ð16:45Þ
On
the
other
hand,
taking
differential
of
the
natural
log
of
q2
fS ¼
ð^LðfSÞÞ2=^LðffÞ^LðSSÞ about the zeroth order,
Dðq2
fSÞ
q2
fS0
¼ 2 D^LðfSÞ
^LðfSÞ
0
 D^LðffÞ
LðffÞ
0
 D^LðSSÞ
LðSSÞ
0
¼ ðcff þ cSS  2cfSÞc1
ð16:46Þ
after using (16.45). Substituting (16.45) and (16.46) into (16.41) and using
r2
x  r2
x0½1 þ Dðr2
xÞ=r2
x0 gives the asymptotic expression of the posterior variance
with ﬁrst order term in the form
r2
x  r2
x0ð1 þ axc1Þ
x ¼ f ; f; S
ð16:47Þ
where
af ¼ cff
af ¼ cff þ
q2
fS0
1  q2
fS0
ðcff þ cSS  2cfSÞ
aS ¼ cSS þ
q2
fS0
1  q2
fS0
ðcff þ cSS  2cfSÞ
ð16:48Þ
16.5
First Order Effect of Signal-to-Noise Ratio
487

Substituting cff , cff, cSS and cfS in (16.44) and q2
fS0 in (16.42) gives, after
algebra:
af ¼ 4ðj  tan1 jÞ
tan1 j 
j
j2 þ 1
ð16:49Þ
af ¼ 4ðj2 þ 1Þð3 tan1 j  3j þ j2 tan1 jÞ tan1 j
3½ðj2 þ 1Þðj  2 tan1 jÞ tan1 j þ j2
ð16:50Þ
aS ¼ 2 þ 2
3 j2 
ðtan1 jÞ2
2ðtan1 jÞ2  bj
8 tan1 j
b

8j
tan1 j þ 4
3 j2 þ 4


ð16:51Þ
where b ¼ tan1 j þ j=ðj2 þ 1Þ.
16.6
Other Data Types
The foregoing derivations assume acceleration data. In the general case, the same
derivation and result still apply, except that the dynamic ampliﬁcation factor is now
given by the general expression
Dk ¼
ð2pfkÞ2q
ð1  b2
kÞ2 þ ð2fbkÞ2
q ¼
0
acceleration data
1
velocity data
2
displacement data
8
<
:
ð16:52Þ
and the noise-to-environment ratio m should be deﬁned as
m ¼ Se
S ð2pf Þ2q
ð16:53Þ
Check that m is still dimensionless because the unit of the channel noise PSD Se
changes with data type. For example, for velocity data (q ¼ 1) the unit is m/s and
the unit of Se is ðm/sÞ2=Hz. Multiplying by ð2pf Þ2 (with unit s2) gives a unit of
ðm=s2Þ2=Hz, the same unit as acceleration PSD and hence modal force PSD.
The derivation in the general case can be reasoned as follow. Any possible
difference must result from the factor ð2pfkÞ2q in Dk. The long data asymptotic
results in Table 16.1 were derived for generic Dk and so are still applicable. Except
for ^LðuuÞ and cross derivatives involving Se, i.e., ^LðfSeÞ, ^LðfSeÞ and ^LðSSeÞ, the
expressions for ^LðxyÞ are invariant to the scaling of Dk because their leading order
does not depend on ek ¼ Se=SDk, or the scaling cancels out due to the
co-appearance of Dk and D1
k . They therefore have the same small damping
asymptotic expression in Table 16.2. For ^LðuuÞ, substituting the general expression
of Dk in (16.52) and following the same steps in small damping asymptotics gives
488
16
Theory of Uncertainty Laws

the same asymptotic expression in Table 16.2 with m deﬁned as in (16.53). For
^LðfSeÞ, ^LðfSeÞ and ^LðSSeÞ, their small damping asymptotic expressions for q 6¼ 0 are
different from the ones in Table 16.2 but they still result in the same (vanishing)
order of magnitude in the sensitivity coefﬁcients in Table 16.3. That is, asymptotic
decoupling still applies when q 6¼ 0. As a result the leading order expressions in
Table 16.4 still apply in the general case for q 6¼ 0, provided that m is deﬁned as in
(16.53). Similarly, it can be shown that ^Lðff Þ
1 , ^LðffÞ
1
, ^LðSSÞ
1
and ^LðfSÞ
1
remain the same,
provided
that
m
is
given
by
(16.53)
and
correspondingly
c1 ¼ 4mf2 ¼ 4Sef2ð2pf Þ2q=S.
Appendix A Asymptotics of RkDa
kðbk  1Þbbc
k
This appendix derives an asymptotic expression for P
k Da
kðbk  1Þbbc
k, where a, b
and c are integers and b 
 0; Dk ¼ ½ð1  b2
kÞ2 þ ð2fbkÞ21 is the dynamic ampli-
ﬁcation factor (acceleration data); bk ¼ f =fk; fk ¼ k=Td is the FFT frequency; Td is
the data duration (s); and the sum is over fk between f ð1  fjÞ. The strategy is to
express it as a Riemann sum and then approximate by an integral. This is
asymptotically correct when Nf ! 1. A closed form expression can be obtained by
further considering f ! 0.
Long Data
The frequency ratios fbkg are not evenly spaced, i.e., bk þ 1  bk is not a constant of
k. In order to write as a Riemann sum we deﬁne and work with an evenly spaced
(dimensionless) coordinate. Let uk ¼ ðfk=f Þ  1 ¼ ð1=bkÞ  1 so that fukg are
evenly spaced at Du ¼ uk þ 1  uk ¼ Df =f ¼ 1=Nc; Df ¼ T1
d
is the FFT frequency
interval and Nc ¼ data duration/natural period. Since fk ranges over f ð1  jfÞ, uk
ranges over jf. Substituting bk ¼ 1=ð1 þ ukÞ and rearranging gives
Dk ¼
ð1 þ ukÞ4
u2
kð2 þ ukÞ2 þ 4f2ð1 þ ukÞ2
ð16:54Þ
X
k
Da
kðbk  1Þbbc
k ¼ ð1Þb22aNc
X
k
ð1 þ ukÞ4abcub
k
½u2
kð1 þ uk=2Þ2 þ f2ð1 þ ukÞ2a Du
ð16:55Þ
The Riemann sum on the RHS tends to an integral, which is asymptotically correct
when Du ¼ N1
c
 2jf, or equivalently, Nf  1. Thus
16.6
Other Data Types
489

X
k
Da
kðbk  1Þbbc
k  ð1Þb22aNc
Zjf
jf
ð1 þ uÞ4abcub
½u2ð1 þ u=2Þ2 þ f2ð1 þ uÞ2a du
Nf ! 1
ð16:56Þ
Changing variable from u to u=f removes f from the integration limit:
X
k
Da
kðbk  1Þbbc
k  ð1Þb22aNcfb2a þ 1
Z j
j
ð1 þ fuÞ4abcub
½u2ð1 þ fu=2Þ2 þ ð1 þ fuÞ2a du
Nf ! 1
ð16:57Þ
Small Damping
For small f the absolute value of the integrand is almost symmetric about the origin.
When b is even, the integrals on the positive and negative side are both positive.
When b is odd they tend to cancel out. Their difference can be captured by a Taylor
approximation w.r.t. f. It can be neglected when b is even but it becomes the
leading order when b is odd.
Separate the integral in (16.57) on two intervals, ½0; j and ½j; 0. For the latter,
change integration variable from u to u. This gives
X
k
Da
kðbk  1Þbbc
k ¼ ð1Þb22aNcfb2a þ 1
Zj
0
ð1 þ fuÞ4abcub
½u2ð1 þ fu=2Þ2 þ ð1 þ fuÞ2a þ
ð1  fuÞ4abcubð1Þb
½u2ð1  fu=2Þ2 þ ð1  fuÞ2a du
2
4
3
5
ð16:58Þ
The two integrands mainly differ by terms involving f. For small f,
ð1  fuÞ4abcub
½u2ð1  fu=2Þ2 þ ð1  fuÞ2a ¼
ub
ð1 þ u2Þa
1 
fu
1 þ u2 ð2a  b  cÞ þ ð3a  b  cÞu2
	



þ Oðf2Þ
ð16:59Þ
When b is even the OðfÞ terms in (16.58) cancel out, giving
490
16
Theory of Uncertainty Laws

X
k
Da
kðbk  1Þbbc
k  22a þ 1Ncfb2a þ 1
Zj
0
ub
ð1 þ u2Þa du
b even; Nf ! 1; f ! 0
ð16:60Þ
When b is odd, the zeroth order terms cancel out, leaving the OðfÞ terms as the
leading order:
P
k
Da
kðbk  1Þbbc
k
  22a þ 1Ncfb2a þ 2 Rj
0
ub þ 1
ð1 þ u2Þa þ 1 ð2a  b  cÞ þ ð3a  b  cÞu2
	

du
b odd; Nf ! 1; f ! 0
ð16:61Þ
The integrals in (16.60) and (16.61) can be evaluated analytically. Table 16.6
shows some results for c ¼ 0 that are frequently used.
Caution with Shortcuts
When analyzing the asymptotic behavior of P
k Da
kðbk  1Þbbc
k it is tempting to
take bk  1 or bk  1  0 but this need not be correct. When b is even it is correct to
take bc
k  1, i.e.,
X
k
Da
kðbk  1Þbbc
k 
X
k Da
kðbk  1Þb
b even
ð16:62Þ
because (16.60) does not depend on c. This rule can be generalized to
X
k
Da
kðbk  1ÞbPðbkÞ 
X
k
Da
kðbk  1ÞbPð1Þ
b even
ð16:63Þ
for any polynomial PðbkÞ with Pð1Þ 6¼ 0. When b is odd, this rule does not apply
and (16.61) should be used. On the other hand, terms of ðbk  1Þ should not be
16.6
Other Data Types
491

Table 16.6 Asymptotic expression for P
k Da
kðbk  1Þb for Nf ! 1 and f ! 0
anb
0
1
2
1
Nc
2f tan1 j
 Ncf
4
4j  5 tan1 j þ
j
j2 þ 1


Ncf
2 ðj  tan1 jÞ
2
Nc
16f3
tan1 j þ
j
j2 þ 1


 Nc
32f 9 tan1 j  jð11j2 þ 9Þ
ðj2 þ 1Þ2
"
#
Nc
16f
tan1 j 
j
j2 þ 1


3
Nc
256f5 3 tan1 j þ jð3j2 þ 5Þ
ðj2 þ 1Þ2
"
#

Nc
512f3
13 tan1 j þ jð13j4  8j2  13Þ
ðj2 þ 1Þ3
"
#
Nc
256f3
tan1 j þ jðj2  1Þ
ðj2 þ 1Þ2
"
#
492
16
Theory of Uncertainty Laws

approximated by zero, since then their asymptotic order is grossed out. The sum
P
k Da
kðbk  1Þb should be evaluated using (16.60) or (16.61).
Appendix B Derivation of Small Damping Asymptotics
(Zeroth Order)
This appendix derives the small damping (in addition to long data) asymptotic
expressions in Table 16.2.
Auto-Derivatives ^Lðff Þ, ^LðffÞ, ^LðSSÞ, ^LðSeSeÞ and ^LðuuÞ
From
Table 16.1,
^Lðff Þ  P
k D2
kðD1
k Þðf Þ2ð1 þ ekÞ2  P
k D2
kðD1
k Þðf Þ2
since
ek  1.
Substituting
ðD1
k Þðf Þ ¼ 4f 1b2
kðb2
k  1 þ 2f2Þ
from
(16.23),
writing
b2
k  1 ¼ ðbk  1Þðbk þ 1Þ and expanding the square,
^Lðff Þ  16f 2
X
k
D2
kðbk  1Þ2ðbk þ 1Þ2b4
k þ 4f2 X
k
D2
kðbk  1Þðbk þ 1Þb4
k þ 4f4 X
k
D2
kb4
k
(
)
ð16:64Þ
Since ðbk þ 1Þ and bk are Oð1Þ, the ﬁrst sum P
k D2
kðbk  1Þ2ðbk þ 1Þ2b4
k has the
same order as P
k D2
kðbk  1Þ2, which is OðNcf1Þ from Table 16.6. Similarly, the
second sum P
k D2
kðbk  1Þðbk þ 1Þb4
k has the same order as P
k D2
kðbk  1Þ ¼
OðNcf1Þ and so the second term is OðNcfÞ. The third sum P
k D2
kb4
k has the same
order as P
k D2
k ¼ OðNcf3Þ and so the third term is OðNcfÞ. Thus, the ﬁrst term in
the brace dominates, giving
^Lðff Þ  16f 2 X
k
D2
kðbk  1Þ2ðbk þ 1Þ2b4
k
ð16:65Þ
Since ðbk  1Þ2 has even power, using (16.63) we can take bk þ 1  2 and bk  1,
and so ^Lðff Þ  64f 2 P
k D2
kðbk  1Þ2. Using Table 16.6 to evaluate the sum
(a ¼ 2,b ¼ 2) gives the expression in Table 16.2.
For ^LðffÞ, from Table 16.1 and using ð1 þ ekÞ2  1,
^LðffÞ 
X
k
D2
kðD1
k ÞðfÞ2ð1 þ ekÞ2 
X
k
D2
kðD1
k ÞðfÞ2
ð16:66Þ
Substituting ðD1
k ÞðfÞ ¼ 8fb2
k from (16.23), ^LðffÞ  64f2 P
k D2
kb4
k  64f2 P
k D2
k.
Using Table 16.6 to evaluate the sum gives the expression in Table 16.2.
For ^LðSSÞ, from Table 16.1,
16.6
Other Data Types
493

^LðSSÞ  S2 X
k
ð1 þ ekÞ2  S2 X
k
1 ¼ S2Nf
ð16:67Þ
as in Table 16.2.
For ^LðSeSeÞ, from Table 16.1, ^LðSeSeÞ  S2
e ½ðn  1ÞNf þ P
k e2
kð1 þ ekÞ2. The
second term in the bracket is dominated by the ﬁrst and so
^LðSeSeÞ  S2
e ðn  1ÞNf
ð16:68Þ
as in Table 16.2.
For ^LðuuÞ, from Table 16.1, using 1 þ ek  1 and e1
k
¼ SDk=Se,
^LðuuÞ  ½2
X
k
ð1 þ ekÞ1e1
k ðIn  uuTÞ  ð2S
Se
X
k
DkÞðIn  uuTÞ
ð16:69Þ
Using Table 16.6 to evaluate the sum gives the expression in Table 16.2.
Cross Derivatives ^Lðf fÞ, ^LðfSÞ and ^LðfSÞ
From Table 16.1,
^Lðf fÞ 
X
k
D2
kðD1
k Þðf ÞðD1
k ÞðfÞð1 þ ekÞ2 
X
k
D2
kðD1
k Þðf ÞðD1
k ÞðfÞ
ð16:70Þ
Substituting
ðD1
k Þðf Þ ¼ 4f 1b2
kðb2
k  1 þ 2f2Þ
and
ðD1
k ÞðfÞ ¼ 8fb2
k,
and
expanding,
^Lðf fÞ  32f 1f
X
k
D2
kðbk  1Þb5
k þ
X
k
D2
kðbk  1Þb4
k þ 2f2 X
k
D2
kb4
k
"
#
ð16:71Þ
Evaluating the ﬁrst two sums using (16.61) and the third using Table 16.6 gives the
expression in Table 16.2.
For ^LðfSÞ, from Table 16.1,
^LðfSÞ   S1 X
k
DkðD1
k Þðf Þð1 þ ekÞ2   S1 X
k
DkðD1
k Þðf Þ
ð16:72Þ
Substituting ðD1
k Þðf Þ ¼ 4f 1b2
kðb2
k  1 þ 2f2Þ and expanding,
^LðfSÞ   4f 1S1 X
k
Dkðbk  1Þb3
k þ
X
k
Dkðbk  1Þb2
k þ 2f2 X
k
Dkb2
k
"
#
ð16:73Þ
Evaluating the ﬁrst two sums using (16.61) and the third using Table 16.6 gives the
expression in Table 16.2.
494
16
Theory of Uncertainty Laws

For ^LðfSÞ, from Table 16.1,
^LðfSÞ   S1 X
k
DkðD1
k ÞðfÞð1 þ ekÞ2   S1 X
k
DkðD1
k ÞðfÞ
ð16:74Þ
Substituting
ðD1
k ÞðfÞ ¼ 8fb2
k,
^LðfSÞ   8fS1 P
k Dkb2
k   8fS1 P
k Dk.
Evaluating the sum using Table 16.6 gives the expression in Table 16.2.
Cross Derivatives ^LðfSeÞ, ^LðfSeÞ and ^LðSSeÞ
From Table 16.1,
^LðfSeÞ   S1 X
k
ðD1
k Þðf Þð1 þ ekÞ2   S1 X
k
ðD1
k Þðf Þ
ð16:75Þ
Substituting ðD1
k Þðf Þ ¼ 4f 1b2
kðb2
k  1 þ 2f2Þ and expanding gives
^LðfSeÞ   4f 1S1 X
k
ðbk  1Þb3
k þ
X
k
ðbk  1Þb2
k þ 2f2 X
k
b2
k
"
#
ð16:76Þ
Evaluating the ﬁrst two sums using (16.61) and noting P
k b2
k  Nf ¼ 2fjNc gives
the expression in Table 16.2.
For ^LðfSeÞ, from Table 16.1,
^LðfSeÞ   S1 X
k
ðD1
k ÞðfÞð1 þ ekÞ2   S1 X
k
ðD1
k ÞðfÞ
ð16:77Þ
Substituting ðD1
k ÞðfÞ ¼ 8fb2
k gives
^LðfSeÞ   8fS1 X
k
b2
k   8fS1Nf ¼  16Ncf2j
S
ð16:78Þ
as in Table 16.2.
For ^LðSSeÞ, from Table 16.1,
^LðSSeÞ  S2 X
k
D1
k ð1 þ ekÞ2  S2 X
k
D1
k
ð16:79Þ
Evaluating the sum using (16.60) gives the expression in Table 16.2.
16.6
Other Data Types
495

Appendix C First Order of NLLF Derivatives w.r.t. f ; f; S
In this appendix we derive expressions for ^Lðff Þ, ^LðffÞ, ^LðSSÞ and ^LðfSÞ up to the ﬁrst
order of the n/e ratio m ¼ Se=S. It leads to results in the form of (16.43).
Second Derivative ^Lðff Þ
From Table 16.1 and using ð1 þ ekÞ2  1  2ek,
^Lðff Þ ¼
X
k
D2
kðD1
k Þðf Þ2ð1 þ ekÞ2 
X
k
D2
kðD1
k Þðf Þ2ð1  2ekÞ ¼ ^Lðff Þ
0
þ ^Lðff Þ
1
ð16:80Þ
where ^Lðff Þ
0
¼ P
k D2
kðD1
k Þðf Þ2 is the zeroth order whose expression is given in
Table 16.2; and, using Dkek ¼ m,
^Lðff Þ
1
¼ 2
X
k
D2
kðD1
k Þðf Þ2ek ¼ 2m
X
k
DkðD1
k Þðf Þ2
ð16:81Þ
is the ﬁrst order. Substituting ðD1
k Þðf Þ ¼ 4f 1b2
kðb2
k  1 þ 2f2Þ from (16.23),
writing b2
k  1 ¼ ðbk  1Þðbk þ 1Þ and expanding the square,
X
k
DkðD1
k Þðf Þ2 ¼ 16
f 2
X
k
Dkðbk  1Þ2ðbk þ 1Þ2b4
k
"
þ 4f2 X
k
Dkðbk  1Þðbk þ 1Þb4
k þ 4f4 X
k
Dkb4
k
#
ð16:82Þ
In the bracket, the ﬁrst sum is of the same order as P
k Dkðbk  1Þ2 ¼ OðNcfÞ from
Table 16.6. Similarly, the second sum is of the same order as P
k Dkðbk  1Þ ¼
OðNcfÞ and so the second term is OðNcf3Þ. The last sum is of the same order as
P
k Dk ¼ OðNcf1Þ and so the last term is also OðNcf3Þ. Thus, the ﬁrst term in the
bracket dominates:
X
k
DkðD1
k Þðf Þ2  16f 2 X
k
Dkðbk  1Þ2ðbk þ 1Þ2b4
k
ð16:83Þ
Since ðbk  1Þ2 has even power, using (16.63) we can take bk þ 1  2 and bk  1 to
get P
k DkðD1
k Þðf Þ2  64f 2 P
k Dkðbk  1Þ2. Using Table 16.6 to evaluate the
sum,
X
k
DkðD1
k Þðf Þ2  32Ncf
f 2
ðj  tan1 jÞ
ð16:84Þ
496
16
Theory of Uncertainty Laws

Substituting into (16.81),
^Lðff Þ
1
¼  64Ncmf
f 2
ðj  tan1 jÞ
^Lðff Þ
1
^Lðff Þ
0
¼ 
4ðj  tan1 jÞ
tan1 j  j=ðj2 þ 1Þ


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
cff
ð4mf2Þ
|ﬄﬄ{zﬄﬄ}
c1
ð16:85Þ
We can then write
^Lðff Þ  ^Lðff Þ
0 ð1  cff c1Þ
cff ¼
4ðj  tan1 jÞ
tan1 j  j=ðj2 þ 1Þ
ð16:86Þ
Second Derivative ^LðffÞ
From Table 16.1 and using ð1 þ ekÞ2  1  2ek,
^LðffÞ ¼
X
k
D2
kðD1
k ÞðfÞ2ð1 þ ekÞ2 
X
k
D2
kðD1
k ÞðfÞ2ð1  2ekÞ ¼ ^LðffÞ
0
þ ^LðffÞ
1
ð16:87Þ
where ^LðffÞ
0
¼ P
k D2
kðD1
k ÞðfÞ2 is the zeroth order in Table 16.2; and ^LðffÞ
1
¼
2m P
k DkðD1
k ÞðfÞ2 is the ﬁrst order. Substituting ðD1
k ÞðfÞ ¼ 8fb2
k from (16.23)
gives ^LðffÞ
1
¼ 128mf2 P
k Dkb4
k   128mf2 P
k Dk. Using Table 16.6 to evaluate
the sum,
^LðffÞ
1
  64Ncmf tan1 j
^LðffÞ
1
^LðffÞ
0
 
4 tan1 j
tan1 j þ j=ðj2 þ 1Þ


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
cff
ð4mf2Þ
|ﬄﬄ{zﬄﬄ}
c1
ð16:88Þ
We can then write
^LðffÞ  ^LðffÞ
0
ð1  cffc1Þ
cff ¼
4 tan1 j
tan1 j þ j=ðj2 þ 1Þ
ð16:89Þ
Second Derivative ^LðSSÞ
From
Table 16.1
and
using
ð1 þ ekÞ2  1  2ek; ^LðSSÞ  S2 P
k ð1  2ekÞ ¼
^LðSSÞ
0
þ ^LðSSÞ
1
where ^LðSSÞ
0
¼ S2Nf is the zeroth order; and ^LðSSÞ
1
¼ 2mS2 P
k D1
k
is the ﬁrst order. Using (16.60) to evaluate the sum gives
16.6
Other Data Types
497

^LðSSÞ
1
¼ 8S2Nf mf2 1 þ j2
3


^LðSSÞ
1
^LðSSÞ
0
¼  2 1 þ j2
3


|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
cSS
ð4mf2Þ
|ﬄﬄ{zﬄﬄ}
c1
ð16:90Þ
We can then write
^LðSSÞ  ^LðSSÞ
0
ð1  cSSc1Þ
cSS ¼ 2 1 þ j2
3


ð16:91Þ
Cross Derivative ^LðfSÞ
From Table 16.1, using ð1 þ ekÞ2  1  2ek and ðD1
k ÞðfÞ ¼ 8fb2
k,
^LðfSÞ   S1 X
k
Dkð8fb2
kÞð1  2ekÞ ¼ ^LðfSÞ
0
þ ^LðfSÞ
1
ð16:92Þ
where ^LðfSÞ
0
¼ 8S1f P
k Dkb2
k   4S1Nc tan1 j is the zeroth order and
^LðfSÞ
1
¼ 16S1mf
X
k
b2
k  32S1Ncmf2j
ð16:93Þ
is the ﬁrst order. Thus
^LðfSÞ
1
^LðfSÞ
0
¼ 
2j
tan1 j


|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
cfS
ð4mf2Þ
|ﬄﬄ{zﬄﬄ}
c1
ð16:94Þ
and we can write
^LðfSÞ  ^LðfSÞ
0
ð1  cfSc1Þ
cfS ¼
2j
tan1 j
ð16:95Þ
Reference
Au SK (2014) Uncertainty law in ambient modal identiﬁcation. Part I: theory. Mech Syst Sig
Process 48(1–2):15–33
498
16
Theory of Uncertainty Laws

Appendix A. Complex Gaussian and Wishart
Distribution
When analyzing the spectral properties of stochastic processes, we often need to
deal with their
Fast Fourier Transform (FFT), which
is a sequence of
complex-valued vectors. Under a variety of situations the real and imaginary parts
of these vectors are jointly Gaussian, by virtue of the Central Limit Theorem. The
vectors are then said to have a ‘complex Gaussian distribution’. From ﬁrst principle
one can analyze the joint distribution of their real and imaginary parts but this is
algebraically cumbersome. By working with complex vectors and matrices rather
than real-valued ones, it turns out that the algebra can be signiﬁcantly simpliﬁed.
The probability density function (PDF) and other statistical results can be expressed
in a compact manner.
This chapter gives a basic introduction to complex Gaussian vectors, which has
been studied extensively in multivariate statistics. The theory provides an efﬁcient
tool for analyzing the distribution of the FFT of ambient data (Chap. 4) and is useful
for general applications. Complex Wishart distribution is also discussed, which is
related to the distribution of the averaged sample power spectral density matrix.
This chapter is intended for readers with a theoretical pursuit. It may be skipped on
ﬁrst reading or for those primarily interested in applications.
A.1
Complex Gaussian Vectors
Without much loss of generality, assume in this chapter that all complex Gaussian
variables or vectors have zero mean. A complex-valued scalar random variable Z is
said to have a complex Gaussian distribution if its real and imaginary parts are
jointly Gaussian. That is, if X ¼ Re Z and Y ¼ Im Z, then they have a joint
Gaussian PDF:
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1
499

pXYðx; yÞ ¼
1
2p
ﬃﬃﬃﬃﬃﬃﬃ
jCj
p
exp
 1
2
x
y
 T
C1 x
y
 
(
)
x; y real scalar
ðA:1Þ
where
C ¼
E½X2
E½XY
E½YX
E½Y2


¼
E½Re Z Re Z
E½Re Z Im Z
E½Im Z Re Z
E½Im Z Im Z


ðA:2Þ
is the covariance matrix of X and Y. Since two Gaussian variables are not neces-
sarily jointly Gaussian, it is not sufﬁcient to have just X and Y individually Gaussian
distributed. They need to be jointly Gaussian, i.e., with a joint Gaussian PDF.
Clearly, there is an invertible transformation between Z and ðX; YÞ. The PDF of
Z is the joint PDF of ðX; YÞ and is denoted by
pZðzÞ ¼ pXYðx; yÞ
z ¼ x þ iy
x; y real scalar
ðA:3Þ
A complex-valued vector Z (n  1 complex) is called a complex Gaussian
vector if its real and imaginary parts are jointly Gaussian. The PDF of Z is the joint
PDF of X ¼ Re Z and Y ¼ Im Z, denoted by
pZðzÞ ¼ pXYðx; yÞ ¼
1
ð2pÞn
ﬃﬃﬃﬃﬃﬃﬃ
jCj
p
exp
 1
2
x
y

T
C1 x
y


(
)
z ¼ x þ iy
x; y n  1 real
ðA:4Þ
where
C ¼
E½XXT
E½XYT
E½YXT
E½YYT


¼
E½Re Z Re ZT
E½Re Z Im ZT
E½Im ZRe ZT
E½Im ZIm ZT


ðA:5Þ
A group of n  1 complex-valued vectors fZkgM
k¼1 are said to have a complex
Gaussian distribution when their real and imaginary parts are jointly Gaussian. That
is, if Xk ¼ Re Zk and Yk ¼ Im Zk, then Xp, Yp, Xq and Yq are jointly Gaussian for
any p; q ¼ 1; . . .; M. Let
Z ¼
Z1
...
ZM
2
64
3
75
ðA:6Þ
Clearly, the joint PDF of fZkgM
k¼1 is the PDF of Z. This PDF is characterized by the
following covariance matrices
500
Appendix A. Complex Gaussian and Wishart Distribution

E½Re Zp Re ZT
q; E½Re ZpIm ZT
q; E½Im ZpRe ZT
q; E½Im ZpIm ZT
q
p; q ¼ 1; . . .; M
ðA:7Þ
The PDF is still given by (A.4) but now with
z ¼
z1
..
.
zM
2
64
3
75
z1; . . .; zM
n  1 complex
ðA:8Þ
The covariance matrix C in (A.4) is now a 2nM  2nM matrix containing in its
partitions the matrices in (A.7).
A.2
Covariance and Pseudo-covariance Matrix
Characterizing the second order statistics of complex vectors in terms of the
covariance matrices of their real and imaginary parts is cumbersome for both
analysis and computation, involving operations that are similar or repetitive.
Consider two complex vectors Z1 and Z2 (both n  1). The second order statistics
between these two vectors involve the following matrices:
E½Re Z1Re ZT
2; E½Re Z1Im ZT
2; E½Im Z1Re ZT
2; E½Im Z1Im ZT
2
ðA:9Þ
Instead of working with these matrices, an equivalent but more elegant way is to
work with the ‘(complex) covariance matrix’ and ‘pseudo-covariance matrix’ (also
called ‘relation matrix’), deﬁned respectively as
E12 ¼ E½Z1Z
2
Q12 ¼ E½Z1ZT
2
ðA:10Þ
where ‘*’ denotes conjugate transpose. Clearly, E12 and Q12 can be determined by
the matrices in (A.9). Conversely,
E½Re Z1Re ZT
2 ¼ 1
2 ðRe E12 þ Re Q12Þ
E½Im Z1Im ZT
2 ¼ 1
2 ðRe E12  Re Q12Þ
E½Re Z1Im ZT
2 ¼ 1
2 ðIm E12 þ Im Q12Þ
E½Im Z1Re ZT
2 ¼ 1
2 ðIm E12 þ Im Q12Þ
ðA:11Þ
These relationships can be veriﬁed by taking expectation of the following identities:
Re Z1Re ZT
2 ¼ 1
2 ½ReðZ1Z
2Þ þ ReðZ1ZT
2Þ
Im Z1Im ZT
2 ¼ 1
2 ½ReðZ1Z
2Þ  ReðZ1ZT
2Þ
Re Z1Im ZT
2 ¼ 1
2 ½Im ðZ1Z
2Þ þ Im ðZ1ZT
2Þ
Im Z1Re ZT
2 ¼ 1
2 ½Im ðZ1Z
2Þ þ Im ðZ1ZT
2Þ
ðA:12Þ
Appendix A. Complex Gaussian and Wishart Distribution
501

The above in turn can be shown by direct algebra using the following identities:
Re Zi ¼ 1
2 ðZi þ ZiÞ
Im Zi ¼  i
2 ðZi  ZiÞ
i ¼ 1; 2
ðA:13Þ
It can be veriﬁed that
E
12 ¼ E21
QT
12 ¼ Q21
ðA:14Þ
This also implies that E
ii ¼ Eii (Hermitian) and QT
ii ¼ Qii (symmetric).
A.3
Complex Gaussian PDF
The PDF in (A.4) can be rewritten in a compact manner in terms of E ¼ E½ZZ
and Q ¼ E½ZZT:
pZðzÞ ¼
1
pn
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
jEjjPj
p
exp
 1
2
z
z
 
E
Q
Q
E

1 z
z
 
(
)
z n  1 complex
ðA:15Þ
P ¼ E  QE1Q
n  n Hermitian
ð
Þ
ðA:16Þ
See the end for proof. Analogous to the standard Gaussian distribution, the ‘stan-
dard complex Gaussian distribution’ refers to the case with identity covariance
matrix (E ¼ In) and zero pseudo-covariance matrix (Q ¼ 0).
As a literature note, complex Gaussian distribution was introduced in Wooding
(1956). Goodman (1963) is often cited for introduction of the subject with dis-
cussion on applications. The topic is discussed in textbooks, e.g., Miller (1974).
Reviews can be found in Miller (1969) and Krishnaiah (1976). In common appli-
cations
the
pseudo-covariance
matrix
is
zero.
Generalization
to
non-zero
pseudo-covariance matrix was considered by Van den Bos (1995).
Example A.1 (Complex Gaussian scalar) Consider Z ¼ X þ iY where X and Y are
real-valued jointly Gaussian variables with zero mean. The joint PDF of ðX; YÞ is
determined by
502
Appendix A. Complex Gaussian and Wishart Distribution

cxx ¼ E½X2
cyy ¼ E½Y2
cxy ¼ E½XY
ðA:17Þ
The PDF of Z is the PDF of ðX; YÞ and is given by (A.4):
pZðzÞ ¼
1
2p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
cxxcyy  c2
xy
q
exp
 1
2
x
y
 T cxx
cxy
cxy
cyy

1 x
y
 
(
)
z ¼ x þ iy
ðA:18Þ
Alternatively, the PDF of Z can be obtained via the PDF of a complex Gaussian
variable. Using (A.15),
E ¼ E½ZZ ¼ E½X2 þ Y2 ¼ cxx þ cyy
ðA:19Þ
Q ¼ E½ZZT ¼ E½X2  Y2 þ 2iXY ¼ cxx  cyy þ 2icxy
ðA:20Þ
P ¼ E  QE1Q ¼ cxx þ cyy 
ðcxx  cyyÞ2 þ 4c2
xy
cxx þ cyy
¼
4ðcxxcyy  c2
xyÞ
cxx þ cyy
ðA:21Þ
jEjjPj ¼ 4ðcxxcyy  c2
xyÞ
ðA:22Þ
The PDF is then given by
pZðzÞ ¼
1
p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
4ðcxxcyy  c2
xyÞ
q

exp
 1
2
x þ iy
x  iy


cxx þ cyy
cxx  cyy þ 2cxyi
cxx  cyy  2cxyi
cxx þ cyy

1 x þ iy
x  iy


(
)
ðA:23Þ
Equations (A.18) and (A.23) look different but they are in fact identical. First
note that the constants in front of the exponential term are the same. It remains to
verify that the quadratic terms in the exponent are the same. This can be done by
writing
x þ iy
x  iy


¼
1
i
1
i

 x
y
 
¼
1
2
1
1
i
i



1 x
y
 
x þ iy
x  iy


¼
x
y
 T
1
2
1
1
i
i



1
ðA:24Þ
Appendix A. Complex Gaussian and Wishart Distribution
503

Substituting into the quadratic form in (A.23) gives a quadratic form in ½x; y with
the middle matrix given by
1
2
1
1
i
i



1
cxx þ cyy
cxx  cyy þ 2cxyi
cxx  cyy  2cxyi
cxx þ cyy

1 1
2
1
1
i
i



1
¼
1
2
1
1
i
i


cxx þ cyy
cxx  cyy þ 2cxyi
cxx  cyy  2cxyi
cxx þ cyy

 1
2
1
1
i
i



1
¼
cxx
cxy
cxy
cyy

1
ðA:25Þ
This is the same matrix inside the inverse of the quadratic form in (A.18).
■
Proof of (A.15) (Complex Gaussian PDF)
The key to deriving (A.15) from (A.4) is to express ½Re z; Im z in terms of ½z;z;
and C in (A.5) in terms of E ¼ E½ZZ and Q ¼ E½ZZT. For the former, assem-
bling Re z ¼ ðz þzÞ=2 and Im z ¼ ðz  zÞi=2 in matrix form gives
Re z
Im z


¼ 1
2
In
In
iIn
iIn


z
z
 
¼
In
iIn
In
iIn


|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
T
1 z
z
 
¼ T1 z
z
 
ðA:26Þ
Since ½Re z; Im z is a real vector,
Re z
Im z

T
¼
Re z
Im z


¼
z
z
 
ðT1Þ ¼
z
z
 
ðTÞ1
ðA:27Þ
The quadratic form in the exponent of (A.4) can then be written as
Re z
Im z

T
C1 Re z
Im z


¼
z
z
 
ðTÞ1C1T1 z
z
 
¼
z
z
 
ðTCTÞ1 z
z
 
ðA:28Þ
We next write TCT in terms of E and Q. Using (A.11) and (A.5),
C ¼ 1
2
Re E þ Re Q
Im E þ Im Q
Im E þ Im Q
Re E  Re Q


ðA:29Þ
Then
TCT ¼ 1
2
In
iIn
In
iIn


Re E þ Re Q
Im E þ Im Q
Im E þ Im Q
Re E  Re Q


In
iIn
In
iIn


ðA:30Þ
504
Appendix A. Complex Gaussian and Wishart Distribution

Multiplying the matrices and using Q ¼ Q gives
TCT ¼
E
Q
Q
E


ðA:31Þ
Substituting into (A.28) gives
Re z
Im z

T
C1 Re z
Im z


¼
z
z
 
E
Q
Q
E

1 z
z
 
ðA:32Þ
which is the quadratic form in the exponent of (A.15).
It remains to write jCj in terms of E and Q. Taking determinant on both sides of
(A.31),
jTjjCjjTj ¼
E
Q
Q
E




ðA:33Þ
Using jA  Bj ¼ jAjbjBja where A  B denotes the Kronecker product of two
square matrices A and B respectively of size a and b,
jTj ¼
1
i
1
i


 In

 ¼
1
i
1
i




n
jInj2 ¼ ð2iÞn
ðA:34Þ
On the other hand, using the block matrix determinant formula (Sect. C.5.2),
E
Q
Q
E



 ¼ jEjj E  QE1Q
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
P
j ¼ jEjjPj
ðA:35Þ
Substituting (A.34) and (A.35) into (A.33), noting jTjjTj ¼ ð2iÞnð2iÞn ¼ 22n and
rearranging gives
jCj ¼ 22njEjjPj
ðA:36Þ
Substituting (A.32) and (A.36) into (A.4) gives (A.15).
■
A.4
Circular Symmetry
A complex Gaussian vector Z is said to be ‘circularly symmetric’ when its distri-
bution does not change under an arbitrary rotation in the complex plane. That is,
Z and Z0 ¼ ei/Z have the same distribution for any real /. Circularly symmetric
Appendix A. Complex Gaussian and Wishart Distribution
505

Gaussian vectors appear frequently in applications and they are traditionally the
central subject of interest in the study of complex Gaussian vectors.
Circular symmetry is directly associated with pseudo-covariance. It can be
shown that Z is circularly symmetric if and only if it has zero mean and zero
pseudo-covariance, i.e., E½Z ¼ 0 and E½ZZT ¼ 0. A standard complex Gaussian
vector is therefore circularly symmetric with identity covariance matrix. Circular
symmetry is also related to linear transformation. It can be shown that Z is circu-
larly symmetric if and only if Z ¼ Rn for some complex matrix R (n  n) and
standard complex Gaussian vector n (n  1). See the end for proof.
When Z is circularly symmetric, its distribution is fully determined by the
covariance matrix E ¼ E½ZZ. In this case the PDF in (A.15) reduces to
pZðzÞ ¼
1
pnjEj exp zE1z
	

ðA:37Þ
In terms of real-valued covariance matrices, circular symmetry means that
E½Re Z Re ZT ¼ E½Im Z Im ZT
E½Re Z Im ZT ¼ E½Im Z Re ZT
ðA:38Þ
Proof (Circular Symmetry , Zero Mean and Zero Pseudo-covariance)
If Z is circularly symmetric then it has the same distribution as Z0 ¼ ei/Z for any
real /. Then Z and Z0 have the same mean, covariance and pseudo-covariance
matrix. Since E½Z0 ¼ ei/E½Z and generally ei/ 6¼ 1, it is necessary that E½Z ¼ 0.
For covariance, E½Z0Z0 ¼ ei/ei/E½ZZ ¼ E½ZZ is automatically satisﬁed. For
pseudo-covariance, E½Z0Z0T ¼ e2i/E½ZZT. Again, e2i/ 6¼ 1 and so E½ZZT ¼ 0.
Conversely, suppose Z has zero mean and zero pseudo-covariance. Since Z0 ¼
ei/Z is complex Gaussian, its PDF is fully determined by the mean, covariance and
pseudo-covariance. It is sufﬁcient to show that these quantities are the same as those
of Z. This follows directly from the properties of Z:
E½Z0 ¼ ei/ E½Z
|ﬄ{zﬄ}
0
¼ 0
E½Z0Z0 ¼ ei/ei/E½ZZ ¼ E½ZZ
E½Z0Z0T ¼ ei/ei/ E½ZZT
|ﬄﬄﬄ{zﬄﬄﬄ}
0
¼ 0
ðA:39Þ
■
506
Appendix A. Complex Gaussian and Wishart Distribution

Proof (Linear Transformation , Zero Mean and Zero Pseudo-covariance)
Suppose Z ¼ Rn where R (n  n) is a constant complex matrix and n (n  1) is
standard complex Gaussian. Then
E½Z ¼ R E½n
|{z}
0
¼ 0
E½ZZT ¼ R E½nnT
|ﬄﬄﬄ{zﬄﬄﬄ}
0
RT ¼ 0
ðA:40Þ
Conversely, suppose E½Z ¼ 0 and E½ZZT ¼ 0; and let E be the covariance matrix
of Z. Since E is Hermitian and positive deﬁnite, it has the representation E ¼
UKU where K (n  n) is a diagonal matrix of (real positive) eigenvalues; and
U (n  n complex) is a unitary matrix (UU ¼ In) consisting in its columns the
eigenvectors of E. Then Z can be represented as Z ¼ U
ﬃﬃﬃﬃ
K
p
n where n is a n  1
standard complex Gaussian vector, because then
E½Z ¼ U
ﬃﬃﬃﬃ
K
p
E½n
|{z}
0
¼ 0
E½ZZT ¼ U
ﬃﬃﬃﬃ
K
p
E½nnT
|ﬄﬄﬄ{zﬄﬄﬄ}
0
ﬃﬃﬃﬃ
K
p
UT ¼ 0
E½ZZ ¼ U
ﬃﬃﬃﬃ
K
p
E½nn
|ﬄﬄ{zﬄﬄ}
In
ﬃﬃﬃﬃ
K
p
U ¼ UKU ¼ E
ðA:41Þ
■
A.5
Complex Wishart Distribution
Let fZrgM
r¼1 (n  1) be an i.i.d. sequence of circularly symmetric complex Gaussian
vectors, each with covariance matrix E. An estimator for E based on simple
averaging is
^E ¼ 1
M W
ðA:42Þ
where
W ¼
X
M
r¼1
ZrZ
r
ðA:43Þ
Appendix A. Complex Gaussian and Wishart Distribution
507

Note that E½W ¼ ME and E½^E ¼ E; and so ^E is an unbiased estimator of E. It can
also be shown that ^E is the maximum-likelihood estimator of E (Goodman 1963).
The statistical properties of W (and hence ^E) have been studied extensively in
multivariate statistics. Clearly, W is a sum of M independent matrices, each of rank
1. It is Hermitian, i.e., W ¼ W. For M\n, W has rank M, less than its dimension
and is therefore singular. The case of relevance in applications is M  n. In this case
W has rank n and its PDF is given by
pWðwÞ ¼ jwjMn
IðEÞ etrðE1wÞ
M  n
ð
Þ
ðA:44Þ
IðEÞ ¼ pnðn1Þ=2jEjM Y
n
i¼1
ðM  iÞ!
ðA:45Þ
This is called the ‘complex Wishart distribution’ with M degrees of freedom.
The PDF is valid for Hermitian positive semi-deﬁnite n  n matrix w (and is zero
otherwise). It is in fact the joint PDF of the (real-valued) diagonal entries and the
real and imaginary parts of the lower off-diagonal entries of W.
The PDF of ^E ¼ W=M can be obtained by a PDF transformation, which gives
p^EðsÞ ¼ MnM jsjMn
IðEÞ eM trðE1sÞ
M  n
ð
Þ
ðA:46Þ
Again, this PDF is valid for n  n Hermitian positive semi-deﬁnite matrix s (and
zero otherwise). It is the joint PDF of the (real-valued) diagonal entries and the real
and imaginary parts of the lower off-diagonal entries of ^E.
As a literature note, Wishart distribution was ﬁrst introduced by Wishart (1928)
as a generalization of Chi-square distribution to multi-dimensions. Details can be
found in texts on multivariate Gaussian distributions, e.g., Anderson (1984) and
Tong (1990). Goodman (1963) gave an introduction to complex Wishart distribu-
tion with discussion on applications. Srivastava (1965) gave a simpliﬁed derivation
of the distribution, though requiring some familiarity with PDF transformation with
variables of complex matrices. See Maiwald & Kraus (2000) for statistical moments
of complex Wishart distribution.
In the scalar case (n ¼ 1), the complex Wishart PDF can be derived based on
basic probability principles; see Sect. A.5.1. The general case ðn  1Þ is much more
involved because W is a ‘structured matrix’, i.e., with constraints among its entries.
The Jacobian of transformation between the entries of W and those of Zr requires
special consideration. See Srivastava (1965).
508
Appendix A. Complex Gaussian and Wishart Distribution

A.5.1
Scalar Variable
Consider the scalar case where fZrgM
r¼1 are i.i.d. scalar circularly symmetric
Gaussian variables with covariance E½ZrZ
r  ¼ S [ 0. Denote W in (A.43) by WM:
WM ¼
X
M
r¼1
ZrZ
r ¼
X
M
r¼1
jZrj2
ðA:47Þ
Equations (A.44) and (A.46) reduce to
pWMðwÞ ¼
wM1
SMðM  1Þ! ew=S
w  0
ðA:48Þ
p^EðsÞ ¼
sM1MM
SMðM  1Þ! eMs=S
s  0
ðA:49Þ
The PDF of WM (and hence ^E ¼ WM=M) can be derived as follow. Being circularly
symmetric, Zr can be represented as
Zr ¼
ﬃﬃﬃ
S
2
r
ðXr þ iYrÞ
ðA:50Þ
where Xr and Yr are i.i.d. standard Gaussian. Check that
E½Zr ¼ 0
E½ZrZT
r  ¼ 0
E½ZrZ
r  ¼ S
ðA:51Þ
Note that
WM ¼ S
2
X
M
r¼1
ðX2
r þ Y2
r Þ
ðA:52Þ
This implies that 2WM=S is a sum of the squares of 2M i.i.d. standard Gaussian
variables, and hence has a Chi-square distribution with 2M degrees of freedom.
The PDF of WM can then be obtained by a PDF transformation of the Chi-square PDF.
Alternatively, by viewing WM as the sum of M i.i.d. variables distributed as W1, its
PDF can be obtained by successive convolution of PDFs. For M ¼ 1, 2W1=S is
Chi-square with 2 degrees of freedom, i.e., exponentially distributed with mean 2.
This implies that W1 is exponentially distributed with mean S. Its PDF is then given by
pW1ðwÞ ¼ 1
S expð w
SÞ
w  0
ðA:53Þ
For M ¼ 2, since W2 is the sum of two i.i.d. variables distributed as W1, its PDF can
be obtained as the convolution of two PDFs of W1:
Appendix A. Complex Gaussian and Wishart Distribution
509

pW2ðwÞ ¼
Zw
0
pW1ðxÞpW1ðw  xÞdx ¼ 1
S2
Zw
0
ex=SeðwxÞ=Sdx ¼ w
S2 ew=S
ðA:54Þ
Similarly,
pW3ðwÞ ¼
Zw
0
pW2ðxÞpW1ðw  xÞdx ¼ 1
S3
Zw
0
xex=SeðwxÞ=Sdx ¼ w2
2S3 ew=S ðA:55Þ
pW4ðwÞ ¼
Zw
0
pW3ðxÞpW1ðw  xÞdx ¼ 1
2S4
Zw
0
x2ex=SeðwxÞ=Sdx ¼
w3
ð2  3ÞS4 ew=S
ðA:56Þ
Continuing this procedure, we see that in general
pWMðwÞ ¼
wM1
½1  2  3    ðM  1ÞSM ew=S
ðA:57Þ
This agrees with (A.48).
References
Anderson TW (1984) An introduction to multivariate statistical analysis. Wiley, New York
Goodman NR (1963) Statistical analysis based on a certain multivariate complex Gaussian
distribution. Ann Math Stat 34:152–177
Krishnaiah PR (1976) Some recent developments in complex multivariate distributions. J Multivar
Anal 6:1–30
Maiwald D, Kraus D (2000) Calculation of moments of complex Wishart and complex inverse
Wishart distributed matrices. IEE Proc Radar Sonar Navig 147(4):162–168
Miller KS (1969) Complex Gaussian processes. SIAM Rev 11(4):544–567
Miller KS (1974) Complex stochastic processes. Addison-Wesley, Reading, MA
Srivastava MS (1965) On the complex Wishart distribution. Ann Math Stat 36(1):313–315
Tong YL (1990) The multivariate normal distribution. Springer, New York
Van den Bos A (1995) The multivariate complex Normal distribution—a generalization. IEEE
Trans Inf Theory 41(2):537–539
Wishart J (1928) The generalized product moment distribution in samples from a Normal
multivariate population. Biometrika 20A(1–2):32–52
Wooding RA (1956) The multivariate distribution of complex normal variables. Biometrika
43:212–215
510
Appendix A. Complex Gaussian and Wishart Distribution

Appendix B. Hessian Under Constraints
This chapter discusses the Hessian of a scalar function under constraints. It sup-
plements the theory on posterior covariance matrix in Sect. 11.2. Consider the
minimum of a scalar function LðhÞ w.r.t. h (nh  1) subjected to nc independent
constraints:
GiðhÞ ¼ 0
i ¼ 1; . . .; nc
ðB:1Þ
Simply taking the Hessian of LðhÞ w.r.t. h at the minimum does not account for
constraints. Such Hessian is not even positive deﬁnite. One way to account for
constraints is to consider the Hessian of the composite function
LcðhÞ ¼ LðvcðhÞÞ
ðB:2Þ
where vcðhÞ (nh  1) is a mapping that automatically satisﬁes constraints, i.e.,
GiðvcðhÞÞ ¼ 0 (i ¼ 1; . . .; nc) for any h. In this chapter we consider more generally
the Hessian of LcðuÞ ¼ LðvcðuÞÞ where vcðuÞ maps a set of free parameters
u (p  1) to h (nh  1) that always satisﬁes constraints; p can be different from nh.
Since there are nh parameters subjected to nc constraints, it is necessary that
p  nh  nc.
The Hessian of LcðuÞ ¼ LðvcðuÞÞ w.r.t. u is more complicated than that of LðhÞ
w.r.t. h. Formulas are presented that express the former in terms of the latter.
Section B.1 presents a formula based on direct differentiation. It is applicable for
any parameter value. Section B.2 presents a formula based on Lagrange multiplier
concepts. It is only applicable at the optimum but is more insightful and involves
much less computation.
Example B.1 (Quadratic form) This simple example demonstrates that incorrect
conclusion results if the constraint is not taken into account when evaluating the
Hessian. Let A be a p  p real symmetric and positive deﬁnite matrix. Consider
maximizing FðuÞ ¼ uTAu w.r.t. u (p  1) under norm constraint jjujj2 ¼ uTu ¼ 1.
According to Lagrange multiplier method, the optimum is a stationary point of
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1
511

Jðu; kÞ ¼ FðuÞ þ kð1  uTuÞ ¼ uTAu þ kð1  uTuÞ
ðB:3Þ
where k is Lagrange multiplier.
Optimum
Direct differentiation of J and using symmetry of A yields
ruJ ¼ 2uTA  2kuT
@J
@k ¼ 1  uTu
ðB:4Þ
Solving ruJ ¼ 0 shows that the optimum satisﬁes the eigenvalue equation:
Au ¼ ku
ðB:5Þ
Note that @J=@k ¼ 0 simply gives the constraint equation. Since A is positive
deﬁnite, all its eigenvalues are positive. Substituting (B.5), the value of F at the
optimum is k. As we want to maximize F, the optimum ^u should be the eigenvector
of A with the largest eigenvalue and scaled to have unit norm. Let fbigp
i¼1 be the
eigenvectors (with unit norm) of A with eigenvalues fjigp
i¼1 (in ascending order of
magnitude). Then ^u ¼ bp and the value of k at the optimum is ^k ¼ jp.
Hessian at Optimum
Consider now the Hessian of F at the optimum. Intuitively it should be negative
deﬁnite because F is maximized there. This is only true when the constraint is
correctly accounted for. Direct differentiation gives r2Fð^uÞ ¼ 2A, which is posi-
tive deﬁnite (on the contrary). The Hessian of F that accounts for norm constraint
can be obtained as the Hessian of the following composite function
FcðuÞ ¼ Fðjjujj1uÞ ¼ ð u
jjujjÞTAð u
jjujjÞ ¼ uTAu
uTu
ðB:6Þ
Direct differentiation gives
rFc ¼ 2ðuTuÞ1ðuTAÞ  2ðuTuÞ2ðuTAuÞuT
ðB:7Þ
r2Fc ¼ 2ðuTuÞ1A  4ðuTuÞ2ðuuTAÞ þ 8ðuTuÞ3ðuTAuÞuuT
 4ðuTuÞ2ðAuuTÞ  2ðuTuÞ2ðuTAuÞIp
ðB:8Þ
where Ip denotes the p  p identity matrix. Substituting u ¼ ^u into rFc and using
A^u ¼ jp^u shows that rFcð^uÞ ¼ 0, as expected. Substituting u ¼ ^u into r2Fc gives
the Hessian at the optimum:
512
Appendix B. Hessian Under Constraints

r2^Fc ¼ 2ðA  jpIpÞ
ðB:9Þ
where ‘^’ denotes that it is evaluated at optimum. Substituting A ¼ Pp
i¼1 jibibT
i
and Ip ¼ Pp
i¼1 bibT
i into (B.9) and rearranging gives the eigenvector representation:
r2^Fc ¼
X
p
i¼1
2ðji  jpÞ
|ﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄ}
eigenvalue
bibT
i
ðB:10Þ
Since jp  ji, the largest eigenvalue of r2^Fc is zero and the remaining ones are all
negative. This implies that r2^Fc is a negative semi-deﬁnite matrix. The zero
eigenvalue is due to norm constraint.
■
Example B.2 (Quadratic form, geometric illustration) Suppose in Example B.1
A ¼
1:2
0:4
0:4
1:8


Let u ¼ ½u1; u2T. Figure B.1a shows the mesh of FðuÞ ¼ uTAu, whose uncon-
strained minimum is at the origin. Figure B.1b shows the contour of F. The thick
line shows the unit circle uTu ¼ 1. Going along this circle and observing the value
of F encounters the constrained minima and maxima of F subjected to uTu ¼ 1.
The value of F along this circle is plotted in Fig. B.1c. The two minima are opposite
to each other; the same applies to the maxima. The coordinates of the minima and
maxima correspond to the eigenvectors of A with the smallest and largest eigen-
value, respectively.
Figure B.2 shows the mesh and contour of FcðuÞ ¼ FðvcðuÞÞ, where vcðuÞ ¼
½u1; u2T=ðu2
1 þ u2
2Þ1=2 maps all points ðu1; u2Þ to the unit circle and therefore
Fig. B.1 Plots of quadratic form. a Mesh; b contour; thick solid line: unit circle; c value of
quadratic form along unit circle
Appendix B. Hessian Under Constraints
513

automatically satisﬁes unit norm constraint. The value of vcðuÞ does not change
along any ray stemming from the origin. The same is also true for FcðuÞ.
■
B.1
Direct Formula
By direct differentiation and repeated application of chain rule, it can be shown that
r2Lc
pp ¼ ðrvcÞT
pnh
r2L
nhnh ðrvcÞ
nhp
þ ð Ip
pp
 rL
1nhÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
pnhp
r2vc
nhpp
ðB:11Þ
where ‘’ denotes the Kronecker product;
rL
1nh ¼ rLðhÞjh¼vcðuÞ
r2L
nhnh ¼ r2LðhÞ

h¼vcðuÞ
ðB:12Þ
Equation (B.11) is applicable for general u, not just at the optimum. It is also
applicable for any mapping vcðuÞ that need not be related to constraints. See
Sect. B.1.1 for proof.
Gradient and Hessian of Matrix Functions
Equation (B.11) involves the gradient and Hessian of the vector function vcðuÞ. For
general matrix function XðuÞ (m  n), r and r2 are interpreted via the Kronecker
product:
Fig. B.2 Plots of constrained quadratic form. a Mesh; b contour; thick solid line: unit circle
514
Appendix B. Hessian Under Constraints

rX
mnp ¼ r
1p  X
mn ¼ ½@X
@u1
mn
; . . .; @X
@up
mn

ðB:13Þ
r2X
mpnp ¼ r2
pp  X
mn ¼
@2X
@u2
1
@2X
@u1@u2
  
@2X
@u1@up
..
.
@2X
@u2@up
...
Block sym:
@2X
@u2
p
2
66666666664
3
77777777775
ðB:14Þ
This interpretation is consistent with the conventional deﬁnition of gradient and
Hessian when X is a scalar. Thus, in (B.11),
rvc
nhp ¼ ½@vc
@u1
; . . .; @vc
@up

r2vc
nhpp ¼
@2vc
@u2
1
@2vc
@u1@u2
  
@2vc
@u1@up
..
.
@2vc
@u2@up
..
.
Block sym:
@2vc
@u2
p
2
66666666664
3
77777777775
ðB:15Þ
Example B.3 (Quadratic form, direct formula) Consider Example B.1 again to
determine the constrained Hessian of FðuÞ ¼ uTAu subjected to uTu ¼ 1, where
A is a p  p real symmetric and positive deﬁnite matrix. In this example we obtain
the Hessian of FcðuÞ ¼ Fðjjujj1uÞ using (B.11). Although the expression is
generally applicable for any u, to simplify presentation we evaluate it at the opti-
mum. The mapping satisfying constraint can be taken as
vcðuÞ ¼ u ¼
u
jjujj ¼
u
ðuTuÞ1=2
ðB:16Þ
Direct differentiation and algebra gives
rFðuÞ ¼ 2uTA
r2FðuÞ ¼ 2A
ðB:17Þ
rvcðuÞ ¼ jjujj1ðIp  uuTÞ
ðB:18Þ
Appendix B. Hessian Under Constraints
515

r2vcðuÞ ¼ jjujj2½ð3uuT  IpÞ  u  u  Ip  vecðIpÞuT
ðB:19Þ
where vecðIpÞ denotes the ‘vectorization’ of Ip, i.e., a n2  1 vector obtained by
stacking column-wise the columns of Ip. Evaluating at u ¼ ^u (optimum),
r^F ¼ 2jp^uT
r2^F ¼ 2A
ðB:20Þ
r^vc ¼ Ip  ^u^uT
r2^vc ¼ ð3^u^uT  IpÞ  ^u  ^u  Ip  vecðIpÞ^uT
ðB:21Þ
where ‘^’ denotes that the quantity is evaluated at the optimum. Applying (B.11) at
the optimum,
r2^Fc ¼ ðr^vcÞTðr2^FÞðr^vcÞ þ ðIp  r^FÞðr2^vcÞ
ðB:22Þ
Direct substitution gives an expression that hardly looks like (B.9) in Example B.1.
It can be shown to be equivalent after simpliﬁcations. For the ﬁrst term in (B.22),
ðr^vcÞTðr2^FcÞðr^vcÞ ¼ 2ðIp  ^u^uTÞTAðIp  ^u^uTÞ ¼ 2ðA  jp^u^uTÞ
ðB:23Þ
after expanding and making use of ^uT^u ¼ 1 and A^u ¼ jp^u. For the second term in
(B.22),
ðIp  r^FÞðr2^vcÞ
¼ ðIp  2jp^uTÞ½ð3^u^uT  IpÞ  ^u  ^u  Ip  vecðIpÞ^uT
¼ 2jp ðIp  ^uTÞ½ð3^u^uT  IpÞ  ^u  ðIp  ^uTÞð^u  IpÞ  ðIp  ^uTÞvecðIpÞ^uT


ðB:24Þ
The ﬁrst two terms in the brace can be further simpliﬁed using the following
identity ðA  BÞðC  DÞ ¼ ðACÞ  ðBDÞ where A, B, C and D are matrices of
appropriate size. Applying this to the ﬁrst term,
ðIp  ^uTÞ½ð3^u^uT  IpÞ  ^u ¼ ½Ipð3^u^uT  IpÞ  ð^uT^uÞ ¼ 3^u^uT  Ip
ðB:25Þ
For the second term,
ðIp  ^uTÞð^u  IpÞ ¼ ðIp^uÞ  ð^uTIpÞ ¼ ^u  ^uT ¼ ^u^uT
ðB:26Þ
The third term in (B.24) can be simpliﬁed using the identity vecðAXBÞ ¼
ðBT  AÞvecðXÞ for matrices A, B and X of appropriate size:
ðIp  ^uTÞvecðIpÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
vecð^uTIpIT
pÞ
^uT ¼ vecð^uTIpIT
pÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
^u
^uT ¼ ^u^uT
ðB:27Þ
516
Appendix B. Hessian Under Constraints

Substituting (B.25), (B.26) and (B.27) into (B.24) gives
ðIp  r^FÞðr2^vcÞ ¼ 2jpð^u^uT  IpÞ
ðB:28Þ
Substituting (B.23) and (B.28) into (B.22) gives the same result in (B.9). This
example demonstrates that, although the direct formula in (B.11) can be imple-
mented systematically, its calculation involves a large number of intermediate terms
and so it may not be computationally efﬁcient.
■
B.1.1
Derivation
Recall the context of (B.11). Let LðhÞ be a scalar function of h (nh  1); and
LcðuÞ ¼ LðvcðuÞÞ, where vcðuÞ is a nh  1 vector function of u (p  1). We shall
show that
rLc
1p ¼ rL
1nh rvc
nhp
ðB:29Þ
r2Lc
pp ¼ ðrvcÞT
pnh
r2L
nhnh ðrvcÞ
nhp
þ ð Ip
pp
 rL
1nhÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
pnhp
r2vc
nhpp
ðB:30Þ
Let vcðuÞ ¼ ½v1ðuÞ; . . .; vnhðuÞT and u ¼ ½u1; . . .; upT. For simplicity in notation,
omit dependence on variables. For i ¼ 1; . . .; p,
@Lc
@ui
¼
X
nh
j¼1
@L
@hj
@vj
@ui
¼ ½ @L
@h1
; . . .; @L
@hnh
½@v1
@ui
; . . .; @vnh
@ui
T ¼ rL @vc
@ui
ðB:31Þ
Assembling this result row-wise for i ¼ 1; . . .; p,
rLc ¼ ½@Lc
@u1
; . . .; @Lc
@up
 ¼ ½rL @vc
@u1
; . . .; rL @vc
@up
 ¼ ðrLÞ½@vc
@u1
; . . .; @vc
@up
 ¼ rLrvc
ðB:32Þ
which proves (B.29). To show (B.30), note that
r2Lc ¼
@rLc
@u1
...
@rLc
@up
2
66664
3
77775
ðB:33Þ
Appendix B. Hessian Under Constraints
517

For i ¼ 1; . . .; p, using (B.29) and chain rule,
@rLc
@ui
¼ @rL
@ui
rvc þ rL @rvc
@ui
ðB:34Þ
For the ﬁrst term,
@rL
@ui
¼ @
@ui
@L
@h1
; . . .; @L
@hnh


¼
X
nh
j¼1
@2L
@hj@h1
; . . .;
@2L
@hj@hnh

 @vj
@ui
¼ @v1
@ui
; . . .; @vnh
@ui


ðr2LÞ ¼ ð@vc
@ui
ÞTðr2LÞ
ðB:35Þ
Thus,
ð@rL=@uiÞrvc ¼ ð@vc=@uiÞTðr2LÞrvc.
Assembling
column-wise
for
i ¼ 1; . . .; p,
@rL
@u1
rvc
..
.
@rL
@up
rvc
2
66664
3
77775
¼
ð@vc
@u1
ÞT
..
.
ð@vc
@up
ÞT
2
66664
3
77775
ðr2LÞrvc ¼ ðrvcÞTðr2LÞrvc
ðB:36Þ
For the second term in (B.34),
ðrLÞ @rvc
@ui
¼ ðrLÞ
@2vc
@ui@u1
; . . .; @2vc
@ui@up


ðB:37Þ
If we divide r2vc (nhp  p) column-wise into p partitions of nh  p matrices, then
the ith partition, say Pi (nh  p), is the rightmost term in the above equation. That is,
r2vc ¼
P1
...
Pp
2
64
3
75
Pi ¼
@2vc
@ui@u1
; . . .; @2vc
@ui@up


ðB:38Þ
Assembling (B.37) column-wise for i ¼ 1; . . .; p,
ðrLÞ @rvc
@u1
...
ðrLÞ @rvc
@up
2
66664
3
77775
¼
ðrLÞP1
...
ðrLÞPp
2
64
3
75 ¼
rL
..
.
rL
2
4
3
5
P1
..
.
Pp
2
64
3
75 ¼ ðIp  rLÞðr2vcÞ
ðB:39Þ
Assembling (B.34) column-wise for i ¼ 1; . . .; p and using (B.36) and (B.39) gives
(B.30).
518
Appendix B. Hessian Under Constraints

B.2 Lagrange Multiplier Formula
The direct formula in (B.11) is generally applicable for any u but it involves
multiplication of large matrices and hence may not be computationally efﬁcient. It
is applicable for any vcðuÞ and so it has not used the fact that it does satisfy
constraints. In this section we present a formula that takes advantage of this but is
only applicable at the optimum. It is derived based on Lagrange multiplier concepts.
Assume that vcðuÞ satisﬁes the constraints, i.e., GiðvcðuÞÞ ¼ 0 (i ¼ 1; . . .; nc) for
any u. Then it can be shown that the Hessian of LcðuÞ ¼ LðvcðuÞÞ w.r.t. u at the
optimum ^u (correspondingly ^h ¼ vcð^uÞ) can be calculated as
r2^Lc
pp ¼ ðr^vcÞT
pnh
r2^L
nhnh þ
X
nc
i¼1
^ki r2 ^Gi
nhnh
 
!
r^vc
nhp
ðB:40Þ
where
r^vc ¼ rvcðuÞju¼^u
r^Gi ¼ rGiðhÞjh¼^h
r2 ^Gi ¼ r2GiðhÞ

h¼^h
ðB:41Þ
^k ¼
^k1
...
^knc
2
64
3
75 ¼ ½ðr^GÞ
ncnh
ðr^GÞT
nhnc
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
ncnc
1 ðr^GÞ
ncnh
ðr^LÞT
nh1
ðB:42Þ
is a nc  1 vector of Lagrange multipliers at the optimum; and
r^L ¼ rLðhÞjh¼^h
r2^L ¼ r2LðhÞ

h¼^h
r^G
ncnh ¼
r^G1
...
r^Gnc
2
64
3
75
ðB:43Þ
Equation (B.40) is invariant to the choice of fGiðhÞgnc
i¼1 as long as they are con-
sistent with vcðuÞ; see Sect. B.2.2.
Example B.4 (Quadratic form, Lagrange formula) Consider Example B.1 again
to determine the constrained Hessian of FðuÞ ¼ uTAu subjected to uTu ¼ 1, where
A is a p  p real symmetric and positive deﬁnite matrix. In this example we obtain
the Hessian of FcðuÞ ¼ FðvcðuÞÞ using (B.40). There is only one constraint,
GðuÞ ¼ 1  uTu ¼ 0. As in Example B.3, the mapping satisfying constraint is
taken as vcðuÞ ¼ u=jjujj ¼ u=ðuTuÞ1=2 so that GðvcðuÞÞ 	 0 for any u. Recall the
derivatives at the optimum from (B.20) and (B.21) in Example B.3:
Appendix B. Hessian Under Constraints
519

r^F ¼ 2jp^uT
r2^F ¼ 2A
r^vc ¼ Ip  ^u^uT
ðB:44Þ
where ^u is the eigenvector of A with the largest eigenvalue and scaled to have unit
norm; ‘^’ denotes that the quantity is evaluated at the optimum. By direct differ-
entiation and evaluating at the optimum,
r^G ¼ 2^uT
r2 ^G ¼ 2Ip
ðB:45Þ
Applying (B.42) gives
^k ¼ ½ðr^GÞðr^GÞT1ðr^GÞðr^FÞT ¼ ½ð2^uTÞð2^uÞ1ð2^uTÞð2jp^uÞ ¼ jp
ðB:46Þ
Using (B.40),
r2^Fc ¼ 2ðIp  ^u^uTÞðA  jpIpÞðIp  ^u^uTÞ
ðB:47Þ
Recall from (B.9) that
r2^Fc ¼ 2ðA  jpIpÞ
ðB:48Þ
Equations (B.47) and (B.48) look different but they are in fact the same. To see this,
ðA  jpIpÞðIp  ^u^uTÞ ¼ A  A^u
|{z}
jp^u
^uT  jpIp þ jp^u^uT ¼ A  jpIp
ðB:49Þ
Similarly,
ðIp  ^u^uTÞðA  jpIpÞðIp  ^u^uTÞ ¼ ðIp  ^u^uTÞðA  jpIpÞ ¼ A  jpIp
ðB:50Þ
Equations (B.47) and (B.48) are therefore the same.
■
B.2.1
Derivation
Here we derive (B.40), which is recalled below:
r2^Lc
pp ¼ ðr^vcÞT
pnh
r2^L
nhnh þ
X
nc
i¼1
^ki r2 ^Gi
nhnh
 
!
r^vc
nhp
ðB:51Þ
520
Appendix B. Hessian Under Constraints

^k ¼
^k1
...
^knc
2
64
3
75 ¼ ½ðr^GÞ
ncnh
ðr^GÞT
nhnc
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
ncnc
1 ðr^GÞ
ncnh
ðr^LÞT
nh1
ðB:52Þ
The key is to show that the second term in (B.11) at the optimum is given by
ðIp  r^LÞðr2^vcÞ ¼ ðr^vcÞTð
X
nc
i¼1
^kir2 ^GiÞðr^vcÞ
ðB:53Þ
First note that vcðuÞ satisﬁes the constraints:
GiðvcðuÞÞ 	 0
i ¼ 1; . . .; nc
for any u
ðB:54Þ
Taking the Hessian of (B.54) w.r.t. u gives a LHS identical to the RHS of (B.11)
except that Lc is replaced by Gi. This Hessian is always zero. Thus, evaluating at the
optimum,
ðr^vcÞTðr2 ^GiÞðr^vcÞ þ ðIp  r^GiÞðr2^vcÞ ¼ 0
i ¼ 1; . . .; nc
ðB:55Þ
The second term on the LHS can be expressed in terms of a similar term associated
with L. Consider the Lagrangian
Jðh; kÞ ¼ LðhÞ þ
X
nc
i¼1
kiGiðhÞ
ðB:56Þ
where k ¼ ½k1; . . .; kncT contains the Lagrange multipliers. If ^h minimizes L under
constraints GiðhÞ ¼ 0 (i ¼ 1; . . .; nc), then there is a ^k ¼ ½^k1; . . .; ^kncT such that
ð^h; ^kÞ is a stationary point of J, i.e.,
rhJð^h; ^kÞ ¼ r^L þ
X
nc
i¼1
^kir^Gi ¼ 0
ðB:57Þ
where rhJ denotes the gradient of J w.r.t. h. This can be written as a matrix
equation for ^k:
^k1
  
^knc


|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
^k
T ð1ncÞ
r^G1
..
.
r^Gnc
2
64
3
75
|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}
r^GðncnhÞ
¼  r^L
1nh
ðB:58Þ
Appendix B. Hessian Under Constraints
521

Despite the fact that there can be more equations than unknowns (nc 
 nh), solution
exists for ^k because r^L is a linear combination of the rows in r^G.
Right-multiplying both sides by ðr^GÞT gives a full rank system of dimension nc,
inverting which and taking transpose gives the expression of ^k in (B.52).
Taking Kronecker product of Ip with (B.57) and right-multiplying the resulting
expression with r2^vc gives, after rearranging,
ðIp  r^LÞðr2^vcÞ ¼ 
X
nc
i¼1
^kiðIp  r^GiÞðr2^vcÞ
ðB:59Þ
Substituting the term ðIp  r^GiÞðr2^vcÞ from (B.55) gives (B.53), which completes
the proof.
B.2.2
Transformation Invariance
Equation (B.40) is invariant to the choice of constraint functions fGiðhÞgnc
i¼1 as long
as they are consistent with vcðuÞ. Speciﬁcally, the value computed in (B.40)
remains the same if the constraint functions are replaced by fHiðGiðhÞÞgnc
i¼1, where
HiðÞ is any scalar function with a non-zero derivative at 0.
Proof (Transformation Invariance)
Let KiðhÞ ¼ HiðGiðhÞÞ, i ¼ 1; . . .; nc. Let ^k0 ¼ ½^k0
1; . . .; ^k0
ncT and ^k ¼ ½^k1; . . .; ^kncT
be the vectors of Lagrange multipliers at the optimum when the constraint functions
are fKiðhÞgnc
i¼1 and fGiðhÞgnc
i¼1, respectively. It is sufﬁcient to show
ðr^vcÞTð^k0
ir2 ^KiÞr^vc ¼ ðr^vcÞTð^kir2 ^GiÞr^vc
i ¼ 1; . . .; nc
ðB:60Þ
since these are the only terms in (B.40) affected by the choice of constraint
functions.
For any h,
rKi ¼ H0
irGi
r2Ki ¼ H00
i ðrGiÞTrGi þ H0
ir2Gi
ðB:61Þ
where
H0
i ¼ dHiðxÞ
dx

x¼GiðhÞ
H00
i ¼ d2HiðxÞ
dx2

x¼GiðhÞ
ðB:62Þ
Evaluating at the optimum and noting Gið^hÞ ¼ 0,
522
Appendix B. Hessian Under Constraints

r^Ki ¼ H0
ið0Þr^Gi
r2 ^Ki ¼ H00
i ð0Þðr^GiÞTr^Gi þ H0
ið0Þr2 ^Gi
ðB:63Þ
We now evaluate ^k0 based on (B.42) for constraint functions fKiðhÞgnc
i¼1. Using
(B.61), at the optimum,
r^K ¼
r^K1
...
r^Knc
2
64
3
75 ¼
H0
ið0Þ
..
.
H0
ncð0Þ
2
64
3
75
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
D
r^G1
...
r^Gnc
2
64
3
75 ¼ Dr^G
ðB:64Þ
Then
^k0 ¼ ½ðr^KÞðr^KÞT1ðr^KÞðr^LÞT
¼ ½Dðr^GÞðr^GÞTDT1Dðr^GÞðr^LÞT
¼ D1½ðr^GÞðr^GÞT1D1Dðr^GÞðr^LÞT ¼ D1^k
ðB:65Þ
This implies
^k0
i ¼
^ki
H0
ið0Þ
i ¼ 1; . . .; nc
ðB:66Þ
Using this and (B.63),
^k0
ir2 ^Ki ¼ ^ki
H00
i ð0Þ
H0
ið0Þ ðr^GiÞTr^Gi þ ^kir2 ^Gi
ðB:67Þ
and so
ðr^vcÞTð^k0
ir2 ^KiÞr^vc ¼ ^ki
H00
i ð0Þ
H0
ið0Þ ðr^Gir^vcÞTr^Gir^vc þ ðr^vcÞTð^kir2 ^GiÞr^vc
ðB:68Þ
Taking gradient w.r.t. u on GiðvcðuÞÞ ¼ 0 and evaluating at the optimum gives
r^Gir^vc ¼ 0, and so the ﬁrst term in (B.68) is zero. Equation (B.60) then follows,
which completes the proof.
■
Appendix B. Hessian Under Constraints
523

Appendix C. Mathematical Tools
This appendix provides some mathematical background to supplement the main
chapters.
C.1
Asymptotics
Let FðxÞ and GðxÞ be functions of x. Then FðxÞ is ‘asymptotic to’ GðxÞ as x ! a if
FðxÞ=GðxÞ ! 1 as x ! a. This is denoted by
FðxÞ  GðxÞ
x ! a
ðC:1Þ
Asymptotics and limit are related but different concepts. Limit is deﬁned in terms of
the absolute difference of two quantities; asymptotics is in terms of their ratio.
When the limit of the subject quantity is non-zero and ﬁnite, asymptotics and limit
are equivalent. Otherwise they are different and neither one implies the other. As a
counter example, x þ 1  x as x ! 1 because ðx þ 1Þ=x ! 1; but x does not
converge to x þ 1 because their difference is always 1. On the other hand, x2 ! x as
x ! 0 but x2 is not asymptotic to x as x ! 0 because x2=x ¼ x tends to 0 instead of
1 as x ! 0.
Dominant Behavior
Asymptotics is a useful notion for investigating the limiting behavior of functions.
For example, although we know that x2 þ x þ ln x ! 1 as x ! 1, it would be
useful to know the ‘dominant behavior’ for large x. Since
x2 þ x þ ln x
x2
¼ 1 þ 1
x þ ln x
x2 ! 1
x ! 1
ðC:2Þ
it can be concluded that
x2 þ x þ ln x  x2
x ! 1
ðC:3Þ
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1
525

Thus, for large x the function x2 þ x þ ln x ‘behaves like’ or is ‘dominated by’ x2.
On a relative basis the ‘dominated’ terms x and ln x can be ignored. Since
x2 þ x þ ln x does not converge to x2 as x ! 1, the notion of limit does not help
(in this case too stringent). Recognizing the dominant term x2 is trivial here but it
could be an ‘art’ in practice when dealing with expressions that are complicated or
cannot even be explicitly written.
Non-uniqueness
Unlike limit, asymptotic expression is not unique. For example, x2 þ x þ ln x is
also asymptotic to x2 þ x as x ! 1. Both x2 and x2 þ x describe the asymptotic
behavior of x2 þ x þ ln x. Which one to use is a matter of trade-off between the
simplicity of expression and the quality of approximation under non-asymptotic
situations.
Order of Magnitude
In analysis, it is useful to know the order of magnitude of a quantity and how it
scales with other quantities. Two quantities FðxÞ and GðxÞ are asymptotically ‘of
the same order’ as x ! a if limx!a FðxÞ=GðxÞ ¼ C where 0\C\1. Equivalently,
we may say that ‘FðxÞ is of the order of GðxÞ’ or simply ‘FðxÞ ¼ OðGðxÞÞ’ as
x ! a.
The concept of asymptotics can be applied to obtain useful approximation of
complicated solutions. See, e.g., Holmes (1995) for solving equations and Bleistein
and Handelsman (1986) for integrals.
C.2
Linear Algebra
For convenience, we use Rn to denote the n-dimensional Euclidean space; x 2 Rn to
denote that x is a n  1 real vector; x 2 Rmn to denote that x is a m  n real-valued
matrix; and x 2 Cmn to denote that x is a m  n complex-valued matrix.
C.2.1
Linear Independence, Span, Basis and Dimension
A set of vectors fxi 2 Rngm
i¼1 is said to be ‘linearly independent’ if any (non-trivial)
linear combination of the vectors is a non-zero vector. That is, the only possibility
for
a1x1 þ a2x2 þ    þ amxm ¼ 0
ðC:4Þ
to hold is a1 ¼ a2 ¼    ¼ am ¼ 0. Otherwise fxigm
i¼1 are said to be linearly
dependent. A set of m vectors in Rn are always linearly dependent if m [ n.
Intuitively if one already has n linearly independent vectors in Rn then it is
impossible to ﬁnd an additional one that is linearly independent of all the n vectors.
526
Appendix C. Mathematical Tools

Such additional vector can always be expressed as a linear combination of the
n vectors.
A ‘linear space’ S is a collection of vectors such that ax þ by 2 S for any scalars
a and b, and vectors x; y 2 S. A set of vectors fxigm
i¼1 is said to ‘span’ S if any
member in S can be expressed as a linear combination of fxigm
i¼1. That is, for any
y 2 S, one can always ﬁnd scalars fcigm
i¼1 such that y ¼ c1x1 þ    þ cmxm. A set of
vectors forms a ‘basis’ of S if the vectors are linearly independent and they span S. A
basis is not unique but it always has the same number of vectors, which is referred as
the ‘dimension’ of S.
C.2.2
Linear Transformation, Rank and Nullity
A ‘linear transformation’ L from a linear space X to another linear space Y maps a
member in X to one in Y such that Lða1x1 þ a2x2Þ ¼ a1Lðx1Þ þ a2Lðx2Þ for any
scalars a1 and a2, and x1; x2 2 X. A matrix A 2 Rmn can be viewed as a linear
transformation from Rn to Rm, because y ¼ Ax maps x 2 Rn to y 2 Rm; and
Aða1x1 þ a2x2Þ ¼ a1ðAx1Þ þ a2ðAx2Þ. The ‘range space’ of A is the collection of
vectors y ¼ Ax for all x 2 Rn. The ‘row rank’ of A is the number of linearly
independent rows; the ‘column rank’ is the number of linearly independent col-
umns. It can be shown that row rank and column rank are equal; and so they are
simply referred as the ‘rank’ of A. The ‘null space’ of A is the collection of all z in
Rn for which Az ¼ 0. The dimension of the null space is called ‘nullity’. The
‘rank-nullity theorem’ says that
Rank þ Nullity ¼ Number of columns
ðC:5Þ
It can be reasoned that range space and null space are linear space.
C.2.3
Euclidean Norm, Inner Product and Orthogonality
The ‘Euclidean norm’ of a real vector x ¼ ½x1; . . .; xnT is deﬁned as
jjxjj ¼ ðxTxÞ1=2 ¼ ð
X
n
i¼1
x2
i Þ1=2
ðC:6Þ
It is a measure of the ‘length’ of a vector. The ‘Euclidean inner product’ between
two real vectors x ¼ ½x1; . . .; xnT and y ¼ ½y1; . . .; ynT is deﬁned as
hx; yi ¼ xTy ¼
X
n
i¼1
xiyi
ðC:7Þ
Appendix C. Mathematical Tools
527

When two real vectors have unit norm, their Euclidean inner product can be
interpreted as the cosine of the hyper-angle between them, and is therefore a
measure of proximity. Two real vectors x and y are ‘orthogonal’ (w.r.t. Euclidean
inner product) if xTy ¼ 0. Two orthogonal vectors are linearly independent but the
reverse is generally not true.
C.2.4
Eigenvalue Problem
Let A be a n  n real-valued square matrix. The ‘standard eigenvalue problem’
associated with A, or simply ‘eigenvalue problem’ when understood, is to ﬁnd
‘eigenvalue’ k 2 C and ‘eigenvector’ q 2 Cn (with q 6¼ 0) such that
Aq ¼ kq
ðC:8Þ
By writing the equation as ðA  kInÞq ¼ 0, for non-trivial solution (q 6¼ 0) to exist,
the eigenvalue must satisfy the ‘characteristic equation’:
jA  kInj ¼ 0
ðC:9Þ
This is a nth order polynomial in k and so there are n roots. A n  n matrix
therefore always has n eigenvalues. These eigenvalues need not be real-valued or
distinct. The ‘algebraic multiplicity’ of an eigenvalue is the number of times it is
repeated among all eigenvalues. The n eigenvalues correspond to n eigenvectors but
these eigenvectors need not be linearly independent. One example is
A ¼
0
1
0
0


which has two eigenvalues both equal to 0 (hence algebraic multiplicity 2) but there
is only one eigenvector q ¼ ½ 1
0 T such that Aq ¼ 0  q. One can check that any
other vector that satisﬁes this equation must be proportional to ½ 1
0 T. The ‘ge-
ometric multiplicity’ of an eigenvalue is the number of linearly independent
eigenvectors associated with it.
Sum and Product of Eigenvalues
It can be shown that the sum of eigenvalues of a matrix is equal to its ‘trace’, i.e.,
the sum of its diagonal entries; the product of eigenvalues of a matrix is equal to its
determinant. These can be reasoned as follow.
The characteristic polynomial of A has the form
pðkÞ ¼ jA  kInj ¼ ð1Þnkn þ ð1Þn1trðAÞkn1 þ    þ jAj
ðC:10Þ
528
Appendix C. Mathematical Tools

where trðAÞ denotes the trace of A. The kn and kn1 term can be shown by
induction w.r.t. n. The constant term follows from the fact that it is equal to the
value of pðkÞ when k ¼ 0, and noting that pð0Þ ¼ jA  0  Inj ¼ jAj. On the other
hand, if fkign
i¼1 are the eigenvalues of A, then
ðk  k1Þðk  k2Þ    ðk  knÞ ¼ 0
ðC:11Þ
Expanding and collecting terms gives
kn  ð
X
n
i¼1
kiÞkn1 þ    þ ð1Þn Y
n
i¼1
ki ¼ 0
ðC:12Þ
Multiplying by ð1Þn so that the kn term matches that of (C.10),
ð1Þnkn þ ð1Þn1ð
X
n
i¼1
kiÞkn1 þ    þ
Y
n
i¼1
ki ¼ 0
ðC:13Þ
Comparing the kn1 and constant term with their counterparts in (C.10) shows
respectively
X
n
i¼1
ki ¼ trðAÞ
Y
n
i¼1
ki ¼ jAj
ðC:14Þ
C.2.5
Diagonalizable Matrices and Their Functions
Let fqi 2 Cngn
i¼1 be n linearly independent eigenvectors corresponding to eigen-
values fki 2 Cgn
i¼1 of A 2 Cnn. Collecting the eigenvectors row-wise gives the
n  n ‘eigenmatrix’:
Q ¼ ½q1; . . .; qn
ðC:15Þ
Assembling the eigenvalue equation Aqi ¼ kiqi row-wise for i ¼ 1; . . .; n,
A q1; . . .; qn
½

|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
Q
¼ q1; . . .; qn
½

|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
Q
k1
..
.
kn
2
64
3
75
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
K
i:e:; AQ ¼ QK
ðC:16Þ
Appendix C. Mathematical Tools
529

Since fqign
i¼1 are linearly independent, Q is invertible and so Q1AQ ¼ K is a
diagonal matrix.
In this sense A is said to be ‘diagonalizable’ by Q.
Right-multiplying (C.16) by Q1 gives the eigenmatrix representation of A:
A ¼ QK Q1
ðA diagonalizableÞ
ðC:17Þ
Let FðxÞ be a scalar function of scalar variable x. The function FðAÞ of a
diagonalizable matrix A is a n  n matrix deﬁned as
FðAÞ ¼ QFðKÞQ1
ðC:18Þ
where FðKÞ for diagonal matrix K is deﬁned as a diagonal matrix with entries
fFðkiÞgn
i¼1. It follows from the deﬁnition that FðAÞ has the same eigenvectors as
A but its eigenvalues are fFðkiÞgn
i¼1.
C.2.6
Real Symmetric and Hermitian Matrices
Let A be a n  n real symmetric matrix, i.e., all its entries are real-valued and
A ¼ AT. Then it can be shown that all its eigenvalues and eigenvectors are
real-valued. For each eigenvalue, the algebraic multiplicity is equal to the geometric
multiplicity. There are then n linearly independent eigenvectors that span Rn. The
eigenvectors of distinct eigenvalues are orthogonal to each other. For eigenvectors
of the same eigenvalue, it is always possible to ﬁnd a set of orthogonal eigenvectors
whose
number
is
equal
to
the
algebraic
multiplicity
of
the
eigenvalue.
Consequently, A has a set of orthogonal eigenvectors that spans Rn. If we scale
these eigenvectors to have unit Euclidean norm, they are ‘orthonormal’. This
orthonormal set of eigenvectors is a basis for Rn.
Eigenvector Representation
Let fqign
i¼1 be the orthonormal eigenvectors of A corresponding to eigenvalues
fkign
i¼1; and Q ¼ ½q1; . . .; qn. Then
QTQ ¼ In
ðC:19Þ
since the ði; jÞ-entry of QTQ is qT
i qj, which is equal to 1 if i ¼ j and zero otherwise.
Right-multiplying both sides of (C.19) by Q1 gives QT ¼ Q1 and so QQT ¼ In
as well. Substituting Q1 ¼ QT into (C.17) and expanding gives the eigenvector
representation of A:
A ¼ QKQT ¼
X
n
i¼1
kiqiqT
i
ðA real symmetricÞ
ðC:20Þ
530
Appendix C. Mathematical Tools

Positive Deﬁnite Matrix
A n  n real square matrix A is said to be ‘positive deﬁnite’ if
xTAx [ 0
for any non-zero x 2 Rn
ðC:21Þ
It can be reasoned that a real symmetric matrix is positive deﬁnite if and only if all
its eigenvalues are positive. A similar terminology and result hold if the eigenvalues
are all negative (negative deﬁnite), non-negative (positive semi-deﬁnite), etc.
Hermitian Matrix
A n  n complex-valued square matrix A is ‘Hermitian’ if A ¼ A, where ‘*’
denotes the conjugate transpose. Hermitian matrices are analogous to real sym-
metric matrices. A Hermitian matrix A has real eigenvalues fki 2 Rgn
i¼1 and an
orthonormal set of eigenvectors fqi 2 Cngn
i¼1, where orthogonality is now deﬁned
as q
i qj ¼ 0 (i 6¼ j). Analogous to (C.19) and (C.20),
QQ ¼ In
ðC:22Þ
A ¼ QK Q ¼
X
n
i¼1
kiqiq
i
ðA HermitianÞ
ðC:23Þ
A real symmetric matrix can be considered as a special case of a Hermitian matrix.
Proof (Real Eigenvalues and Orthogonal Eigenvectors)
Let A be a Hermitian matrix. Here we show that its eigenvalues are real-valued and
its eigenvectors are orthogonal.
If k is an eigenvalue with eigenvector q then Aq ¼ kq. Pre-multiplying both
sides by q gives qAq ¼ kðqqÞ. Taking conjugate transpose and noting A ¼ A
gives another equation qAq ¼ kðqqÞ. Subtracting these two equations gives
ðk  kÞðqqÞ ¼ 0. Since qq [ 0, this implies k ¼ k, i.e., k is real-valued.
To show orthogonality of eigenvectors, ﬁrst consider two distinct eigenvalues k1
and k2 with corresponding eigenvectors q1 and q2. Then
Aq1 ¼ k1q1
)
q
2Aq1 ¼ k1q
2q1
ðC:24Þ
Aq2 ¼ k2q2 ) q
1Aq2 ¼ k2q
1q2
)
conjugate
transpose;
A¼A;k2¼k2
q
2Aq1 ¼ k2q
2q1
ðC:25Þ
Subtracting (C.25) from (C.24) gives ðk1  k2Þq
2q1 ¼ 0, which implies q
2q1 ¼ 0
since k1 6¼ k2.
For repeated eigenvalues, let k be an eigenvalue repeating m times, i.e., with
algebraic multiplicity m. The eigenvectors associated with k form a linear subspace.
Appendix C. Mathematical Tools
531

To see this, if x1 and x2 are eigenvectors associated with k, then so is y ¼
a1x1 þ a2x2 for any scalars a1 and a2:
Ax1 ¼ kx1
Ax2 ¼ kx2

) A ða1x1 þ a2x2Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
y
¼ k ða1x1 þ a2x2Þ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
y
ðC:26Þ
Since A is Hermitian, there are always m linearly independent eigenvectors asso-
ciated with k (geometric = algebraic multiplicity), and so the subspace has a
dimension of m. We can then always choose m mutually orthogonal vectors in the
subspace as the eigenvectors associated with k. These eigenvectors are also
orthogonal to the eigenvectors of other eigenvalues different from k.
■
C.3
Lagrange Multiplier Method
Let FðxÞ and GðxÞ be scalar functions of x ¼ ½x1;    ; xnT 2 Rn. Suppose we want
to determine the optimal value ^x that minimizes FðxÞ subjected to constraint
GðxÞ ¼ 0. The Lagrange multiplier method converts this constrained optimization
problem into an unconstrained one by introducing an auxiliary scalar variable k
(‘Lagrange multiplier’) and considering the following objective function, called
‘Lagrangian’:
Jðx; kÞ ¼ FðxÞ þ kGðxÞ
ðC:27Þ
It can be shown that if there is a solution to the original constrained optimization
problem then one can ﬁnd ð^x; ^kÞ as a stationary point of J. That is, rxJ (gradient of
J w.r.t. x) and @J=@k are zero at ð^x; ^kÞ.
Using (C.27) and setting @J=@k ¼ 0 simply returns the constraint equation
GðxÞ ¼ 0. On the other hand, post-multiplying rxJ ¼ 0 by ðrGÞT and rearranging
gives the solution of k:
^k ¼ ½ðr^GÞ
|ﬄﬄ{zﬄﬄ}
1n
ðr^GÞT
|ﬄﬄﬄ{zﬄﬄﬄ}
n1
1 ðr^GÞ
|ﬄﬄ{zﬄﬄ}
1n
ðr^FÞT
|ﬄﬄﬄ{zﬄﬄﬄ}
n1
ðC:28Þ
where a hat ‘^’ denotes the value at the optimum ^x.
Multiple Constraints in Vector Form
The above can be extended for multiple constraints. To minimize FðxÞ subjected to
constraints fGiðxÞ ¼ 0gp
i¼1, one can consider the following Lagrangian:
532
Appendix C. Mathematical Tools

Jðx; kÞ ¼ FðxÞ þ
X
p
i¼1
kiGiðxÞ ¼ FðxÞ þ kT
|{z}
1p
GðxÞ
p1
ðC:29Þ
where ki is the Lagrange multiplier associated with GiðxÞ; k ¼ ½k1; . . .; kpT and
GðxÞ ¼ ½G1ðxÞ; . . .; GpðxÞT. The value of k at the optimum is analogous to (C.28):
^k ¼
^k1
..
.
^kp
2
64
3
75 ¼ ½ðr^GÞ
|ﬄﬄ{zﬄﬄ}
pn
ðr^GÞT
|ﬄﬄﬄ{zﬄﬄﬄ}
np
1 ðr^GÞ
|ﬄﬄ{zﬄﬄ}
pn
ðr^FÞT
|ﬄﬄﬄ{zﬄﬄﬄ}
n1
ðC:30Þ
where r^G ¼ ½@ ^G=@x1; . . .; @ ^G=@xn (p  n) is the gradient of GðxÞ at the
optimum.
Multiple Constraints in Matrix Form
In some problems the constraint functions may be structured by two indices, say,
fGijðxÞ : i ¼ 1; . . .; p; j ¼ 1; . . .; qg. One of course can re-structure these with a
single index and use the vector form of constraints just discussed. In some cases, it
may be possible to exploit the double-index structure to facilitate analysis. One may
then consider the Lagrangian
Jðx; KÞ ¼ FðxÞ þ
X
p
i¼1
X
q
j¼1
kijGijðxÞ
ðC:31Þ
where kij is the Lagrange multiplier associated with GijðxÞ; K is a p  q matrix of
Lagrange multipliers whose ði; jÞ-entry is kij. The Lagrangian can be further
expressed in a compact form by writing the double sum as the trace (sum of
diagonal entries) of a matrix:
Jðx; KÞ ¼ FðxÞ þ tr½KTGðxÞ
ðC:32Þ
where trðÞ denotes the trace of the argument matrix; GðxÞ is a p  q matrix whose
ði; jÞ-entry is GijðxÞ. This form follows because the jth diagonal entry of KTGðxÞ is
Pp
i¼1 kijGijðxÞ and so summing it over j ¼ 1; . . .; q gives the trace equal to the
double sum in (C.31).
C.4
Minimizing Quadratic Forms
Let x ¼ ½x1; . . .; xnT 2 Rn, b ¼ ½b1; . . .; bnT 2 Rn and A 2 Rnn whose ði; jÞ-entry
is aij. Then
Appendix C. Mathematical Tools
533

bTx ¼
X
n
i¼1
bixi
xTAx ¼
X
n
i¼1
X
n
j¼1
aijxixj
ðC:33Þ
are scalar functions of x, referred respectively as ‘linear form’ and ‘quadratic form’.
Their gradient and Hessian w.r.t. x are
rðbTxÞ ¼ bT
r2ðbTxÞ ¼ 0
rðxTAxÞ ¼ xTðA þ ATÞ
r2ðxTAxÞ ¼ A þ AT
ðC:34Þ
These can be derived from ﬁrst principle by differentiating w.r.t. each xi and
assembling into matrix form, although it could be cumbersome. Alternatively, they
can be derived by considering differentials w.r.t. x, using chain rule and noting from
Taylor series that the ﬁrst and second order differentials of a scalar function FðxÞ
are ðrFÞDx and DxTðr2FÞDx=2, respectively; ‘D’ denotes a differential. For
example,
DðxTAxÞ ¼ ðDxTÞAx þ xTADx ¼ xTATDx þ xTADx ¼ xTðAT þ AÞ
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
rðxTAxÞ
Dx
ðC:35Þ
In the following, let A be a n  n real symmetric and positive deﬁnite matrix.
We consider minimizing some common quadratic forms with or without norm
constraints. Let FðxÞ ¼ xTAx. Clearly, FðxÞ  0 and it has a unique minimum
value of 0 at x ¼ 0.
Consider the non-trivial problem of minimizing FðxÞ w.r.t. x subjected to
constraint xTx ¼ 1. Using Lagrange multiplier method, the solution is a stationary
point of the Lagrangian Jðx; kÞ ¼ xTAx þ kð1  xTxÞ where k is a Lagrange
multiplier. The gradient of J w.r.t. x is rxJ ¼ 2xTA  2kxT ¼ 2xTðA  kInÞ.
Setting this to zero gives the standard eigenvalue problem Ax ¼ kx. Substituting
into FðxÞ and noting xTx ¼ 1 shows that its value at the optimum is k. This implies
that the minimizing solution of x is the eigenvector of A with the smallest eigen-
value and scaled to have unit norm.
Consider now minimizing FðxÞ ¼ xTAx þ 2bTx. This function has gradient
rF ¼ 2ðxTA þ bTÞ and Hessian r2F ¼ 2A (positive deﬁnite). Solving rF ¼ 0
gives x ¼ A1b, at which FðxÞ attains its minimum equal to bTA1b.
Finally, consider minimizing FðxÞ ¼ xTAx þ 2bTx subjected to constraint
xTx ¼ 1. Deﬁne the Lagrangian JðxÞ ¼ xTAx þ 2bTx þ kð1  xTxÞ. Solving for
rxJ ¼ 0 gives Ax þ b ¼ kx. This is a ‘constrained eigenvalue problem’ whose
solution is generally different from the standard one. Details can be referred to
Gander (1980) and Gander et al. (1989). The problem can be converted into a
standard eigenvalue problem by deﬁning an augmented vector. Let
534
Appendix C. Mathematical Tools

y ¼ ðA  kInÞ1x
z ¼
x
y


ðC:36Þ
Then it can be shown that z satisﬁes the standard eigenvalue problem:
A
bbT
In
A


|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
D
x
y


|ﬄ{zﬄ}
z
¼ k
x
y


|ﬄ{zﬄ}
z
i:e:; Dz ¼ kz
ðC:37Þ
Taking the ﬁrst half of the eigenvector of D with the smallest real eigenvalue and
scaling it to have unit norm gives the minimizing solution of x.
Equation (C.37) can be shown as follow, although the process is somewhat
tricky. Rearrange Ax þ b ¼ kx to give x ¼ ðA  kIÞ1b. Substituting into y in
(C.36) gives
y ¼ ðA  kInÞ2b
ðC:38Þ
bTy ¼ bTðA  kIÞ2b ¼ xTx ¼ 1
ðC:39Þ
Thus Ax þ b ¼ kx can be written as Ax þ bbTy ¼ kx. On the other hand,
pre-multiplying both sides of (C.36) by ðA  kInÞ and rearranging gives
x þ Ay ¼ ky. Assembling these two equations gives (C.37).
Table C.1 summarizes the solutions of the problems discussed.
C.5
Identities and Inequalities
Some identities and inequalities relevant to the main chapters are summarized in
this
section.
Unless
otherwise
mentioned,
all
matrices
and
vectors
are
complex-valued. See Horn and Johnson (2013) and Brookes (2011) for further
details on matrix algebra and identities.
Table C.1 Summary of minimizing solutions of quadratic forms
Objective
function FðxÞ
Constraint
Solution ^x
Minimized value
Fð^xÞ
xTAx
–
0
0
xTAx
xTx ¼ 1
Eigenvector of A (smallest eigenvalue)
and scaled to unit norm
Smallest
eigenvalue of A
xTAx þ 2bTx
–
A1b
bTA1b
xTAx þ 2bTx
xTx ¼ 1
First half of eigenvector (smallest
real eigenvalue) of D ¼
A
bbT
In
A


and scaled to unit norm
k þ bT^x
k= smallest real
eigenvalue of D
Appendix C. Mathematical Tools
535

C.5.1
Matrix Inverse Lemma and Determinant Theorem
For any matrices A, C, U and V of appropriate size, with A and C invertible,
ðUCV þ AÞ1 ¼ A1  A1UðVA1U þ C1Þ1VA1
ðC:40Þ
jUCV þ Aj ¼ jAjjCjjVA1U þ C1j
ðC:41Þ
C.5.2
Block Matrix Determinant and Inverse
Let A11 and A22 be square matrices; A12 and A21 be matrices of appropriate size.
Then
A11
A12
A21
A22



 ¼ jA11jjA22  A21A1
11 A12j
ðC:42Þ
A11
A12
A21
A22

1
¼
B11
A1
11 A12B22
A1
22 A21B11
B22


ðC:43Þ
B11 ¼ ðA11  A12A1
22 A21Þ1 ¼ A1
11 þ A1
11 A12B22A21A1
11
B22 ¼ ðA22  A21A1
11 A12Þ1 ¼ A1
22 þ A1
22 A21B11A12A1
22
ðC:44Þ
C.5.3
Derivatives of Log-Determinant and Inverse
Let a superscripted variable in parenthesis denote differentiation w.r.t. it. For any
square invertible matrix A that depends on real variables x and y (possibly x ¼ y),
ðln jAjÞðxÞ ¼ tr½A1AðxÞ
ðC:45Þ
ðln jAjÞðxyÞ ¼ tr½A1ðAðxyÞ  AðxÞA1AðyÞÞ
ðC:46Þ
ðA1ÞðxÞ ¼ A1AðxÞA1
ðC:47Þ
ðA1ÞðxyÞ ¼ A1½AðxÞA1AðyÞ þ AðyÞA1AðxÞ  AðxyÞA1
ðC:48Þ
where trðÞ denotes the trace (sum of diagonal entries) of the argument matrix.
536
Appendix C. Mathematical Tools

C.5.4
Gradient and Hessian of Rayleigh Quotient
Let K and M be n  n real symmetric matrices and u be a n  1 real vector. The
‘Rayleigh quotient’ is deﬁned as
RðuÞ ¼ uTKu
uTMu
ðC:49Þ
As a scalar function of u, the gradient and Hessian of RðuÞ are given respectively by
rRðuÞ ¼ 2ðuTMuÞ1uTK  2ðuTMuÞ2ðuTKuÞuTM
ðC:50Þ
r2RðuÞ ¼ 2ðuTMuÞ1K  4ðuTMuÞ2MuuTK þ 8ðuTMuÞ3ðuTKuÞMuuTM
 4ðuTMuÞ2KuuTM  2ðuTMuÞ2ðuTKuÞM
ðC:51Þ
When uTMu ¼ 1, (C.50) and (C.51) simplify to
rRðuÞjuTMu¼1¼ 2uTK  2ðuTKuÞuTM
ðC:52Þ
r2RðuÞ

uTMu¼1¼ 2K  4MuuTK þ 8ðuTKuÞMuuTM  4KuuTM  2ðuTKuÞM
ðC:53Þ
When u is an eigenvector (with eigenvalue k) of the generalized eigenvalue
problem Ku ¼ kMu, (C.50) and (C.51) simplify to
rRðuÞjKu¼kMu¼ 0
ðC:54Þ
r2RðuÞ

Ku¼kMu¼ 2ðuTMuÞ1ðK  kMÞ
ðC:55Þ
C.5.5
Gradient and Hessian of Unit Vector
Let u ¼ ½u1; . . .; unT be a n  1 real vector. Deﬁne the unit vector
u ¼ jjujj1u ¼ ðuTuÞ1=2u
ðC:56Þ
The gradient and Hessian of u w.r.t. u are respectively
ru
nn ¼
@u
@u1
  
@u
@un


ðC:57Þ
Appendix C. Mathematical Tools
537

r2u
n2n ¼
@2u
@2u1
@2u
@u1@u2
  
@2u
@u1@un
@2u
@u2@u1
@2u
@u2@u2
  
@2u
@u2@un
..
.
..
.
..
.
..
.
@2u
@un@u1
@2u
@un@u2
  
@2u
@u2
n
2
66666666664
3
77777777775
ðC:58Þ
By direct differentiation and assembling in matrix form, it can be shown that
ru ¼ jjujj1ðIn  uuTÞ
ðC:59Þ
r2u ¼ jjujj2½ð3uuT  InÞ  u  u  In  vecðInÞuT
ðC:60Þ
where  denotes the Kronecker product and vecðInÞ denotes the ‘vectorization’ of
In, i.e., a n2  1 vector obtained by stacking the columns of In.
C.5.6
Cauchy-Schwartz Inequality
Let x ¼ ½x1; . . .; xnT and y ¼ ½y1; . . .; ynT be two complex-valued vectors. The
Cauchy-Schwartz inequality says that
jxyj2 
 ðxxÞðyyÞ
ðC:61Þ
where x denotes the complex conjugate transpose of x. In terms of the entries of
the vectors,
j
X
n
i¼1
xiyij2 
 ð
X
n
i¼1
jxij2Þð
X
n
j¼1
jyjj2Þ
ðC:62Þ
where xi denotes the complex conjugate of xi. Replacing xi by xi and noting that
they have the same modulus gives also
j
X
n
i¼1
xiyij2 
 ð
X
n
i¼1
jxij2Þð
X
n
j¼1
jyjj2Þ
ðC:63Þ
Equation (C.61) can be shown as follow. When y ¼ 0 it holds trivially. In the
general case for y 6¼ 0, note that for any complex number s,
ðx  syÞðx  syÞ  0
ðC:64Þ
Expanding the LHS and setting s ¼ ðyxÞ=ðyyÞ gives (C.61) after simpliﬁcation.
538
Appendix C. Mathematical Tools

The
Cauchy-Schwartz
inequality
also
holds
for
functions.
For
two
complex-valued scalar functions FðuÞ and GðuÞ of real scalar variable u,
j
Z
FðuÞGðuÞduj2 
Z
jFðuÞj2du

 Z
jGðvÞj2dv


ðC:65Þ
C.5.7
Inequality for Partition of Inverse
Let a real symmetric positive deﬁnite matrix A and its inverse be partitioned as
A ¼
A11
A12
A21
A22


A1 ¼
B11
B12
B21
B22


ðC:66Þ
Then
B11  A1
11
ðC:67Þ
in the sense that ðB11  A1
11 Þ is a positive semi-deﬁnite matrix.
This inequality can be reasoned using the (1,1)-partition in (C.44) of the block
matrix inverse formula:
B11  A1
11 ¼ A1
11 A12B22A21A1
11 ¼ ðA21A1
11 ÞTB22ðA21A1
11 Þ
ðC:68Þ
since ðA1
11 ÞT ¼ A1
11
and AT
21 ¼ A12. For any non-zero column vector x of
appropriate size,
xTðB11  A1
11 Þx ¼ ðA21A1
11 xÞT
|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}
yT
B22 ðA21A1
11 xÞ
|ﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄ}
y
¼ yTB22y  0
ðC:69Þ
since B22 is positive deﬁnite. The equality cannot be ruled out because it is possible
that y ¼ 0.
C.5.8
Inequality for Covariance of Two Vectors
For any two random real vectors X (m  1) and Y (n  1) with positive deﬁnite
covariance matrices,
E½YYT  E½YXTE½XXT1E½XYT
ðC:70Þ
in the sense that the LHS minus RHS is a positive semi-deﬁnite matrix. Equality
holds if and only if Y ¼ E½YXTE½XXT1X.
Appendix C. Mathematical Tools
539

This inequality can be reasoned as follow. Let A be a n  m constant matrix.
Then Y  AX is a n  1 random vector. Clearly,
E½ðY  AXÞðY  AXÞT  0
ðC:71Þ
in the sense that the LHS is a positive semi-deﬁnite matrix. Expanding gives
E½YYT  AE½XYT  E½YXTAT þ AE½XXTAT  0
ðC:72Þ
Taking A ¼ E½YXTE½XXT1 and simplifying gives (C.70). It is clear from (C.71)
that the equality holds if and only if Y ¼ AX, i.e., Y ¼ E½YXTE½XXT1X.
References
Bleistein N, Handelsman RA (1986) Asymptotic expansions of integrals. Dover, New York
Brookes M (2011) The matrix reference manual. [online] http://www.ee.ic.ac.uk/hp/staff/dmb/
matrix/intro.html
Gander W (1980) Least squares with a quadratic constraint. Numerische Mathematik 36:291–307
Gander W, Golub GH, Matt UV (1989) A constrained eigenvalue problem. Linear Algebra Appl
114(115):815–839
Holmes M (1995) Introduction to perturbation methods. Springer, New York
Horn RA, Johnson CR (2013) Matrix analysis. Cambridge University Press, Cambridge
540
Appendix C. Mathematical Tools

Index
A
Aliasing
in Fourier series, 44
in Fourier transform, 49
paper folding, 212
Asynchronous data, 241, 341
C
Complex Gaussian distribution
circular symmetry, 505
justiﬁcation, 330
Correlation function
continuous-time sample, 140
discrete-time sample, 147
Cramér Rao bound
connection with uncertainty law, 321
easy, 307
full, 300
modal ID example, 306, 309
D
Damping
amplitude dependence, 98
classical, 93
Decimation, 213
Delta function, 38, 80
Dirichlet kernel, 47, 159
Duhamel’s integral, 81, 111, 199
Dynamic ampliﬁcation factor
asymptotic sum, 489
deﬁnition, 72, 182
derivatives, 372
E
Eigenvalue problem
constrained, 424, 534
example, 89
generalized, 87
standard, 528
Evidence, 286
F
Fast Fourier Transform
algorithm, 54
distribution, 171
Field data, 75, 104, 226, 237, 254, 385, 414,
438, 465
Field test, 438
Fisher information matrix
complex Gaussian data, 315
deﬁnition, 301
Gaussian data, 312
Fourier series
complex form, 33
real form, 30
Fourier Transform, 35
H
Half-power bandwidth method, 74
Huddle test, 220
H/V method, 252
I
Identiﬁability, 268
Identiﬁable
global, 274
local, 283
un-, 284
Impact hammer test, 105
L
Laboratory data, 207, 223, 245, 381, 464
Lagrange multiplier, 350, 519, 532
Leakage
in Fourier series, 46
in Fourier Transform, 49
© Springer Nature Singapore Pte Ltd. 2017
S.-K. Au, Operational Modal Analysis, DOI 10.1007/978-981-10-4118-1
541

power spectral density, 158
Leibniz rule, 82
Linear regression
Bayesian solution, 279
Cramér Rao bound, 304, 308
statistical estimator, 297
Logarithmic decrement method, 67
Lyapunov equation, 195
M
Markov Chain Monte Carlo, 284
Maximum likelihood estimator
asymptotic properties, 316
compare with Bayes, 320
modal ID example, 299
Microtremor, 249
Model class selection, 285
Mode shape
assembly by Bayes, 427
assembly by least squares, 420, 422
scaling, 88, 188, 333
subspace, 393
uncertainty, 359
N
Newmark method, 122
Noise
modeling, 331
sensor/hardware, 207
Nyquist frequency, 44, 159, 206
O
Ockham factor, 287
P
Parseval equality
for Fast Fourier transform, 43
for Fourier series, 34
for Fourier transform, 38
for power spectral density, 143
for sample power spectral density, 152
Power spectral density
averaged, 153, 229
continuous-time sample, 142
discrete-time sample, 149, 228
sample distribution, 172
theoretical, 136
Pseudo-covariance, 164, 501
Q
Quantization error, 213
R
Rayleigh quotient
deﬁnition, 94
derivatives, 537
shear building, 96
Reference DOF, 337, 420
S
Shaker, 8, 75, 103
Shear building
laboratory model, 381
microtremor vibration, 252
modal force PSD, 189, 333
modal ID example, 374
mode shape, 91
Rayleigh quotient, 96
stiffness matrix, 92
Signal-to-noise ratio
asymptotic MPV, 368, 404, 431
deﬁnition, 208, 260, 458
effect on ID uncertainty, 463, 485
Singular value spectrum
asynchronous data, 242, 247
deﬁnition, 231
multi-mode, 234
single mode, 232
State-space approach, 110, 191
T
Transpose mirror property
correlation function, 135, 141, 148, 198
example, 148
power spectral density, 136, 142, 149
U
USGS noise model, 249
W
White noise
deﬁnition, 144
response, 144, 184, 195, 201
simulation, 153, 255
Wiener-Khinchin formula, 142, 150, 174, 200
Wishart distribution, 172, 507
542
Index

