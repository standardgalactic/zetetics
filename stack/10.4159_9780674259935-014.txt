Chapter 13
• • •
I N FER ENCE A N D L A NGUAGE I
To talk to Eugene Goostman, you have to chat with him on text. He’s 
not available for a phone call, and you ­can’t have lunch with him. Text 
him, and ­he’ll tell you he’s a thirteen-­year-­old kid from Odessa, 
Ukraine. Like a lot of teen­agers, his texts are flippant, evasive, over-
confident, and prone to misdirection and dissembling. He’s rude, 
then playful. He quips. What he ­won’t tell you is that he’s actually a 
computer program, a chatbot designed by Rus­sian researchers to con-
vince ­humans he’s flesh and blood.
Goostman made history, purportedly, on June 7, 2014, by passing 
the Turing test, sixty years ­after Turing’s death. In the much-­ballyhooed 
event hosted by the University of Reading in ­England and conducted 
at the Royal Society in London, Goostman the chatbot convinced 
thirty-­three ­percent of selected judges in a five-­minute text exchange 
that he was ­human.
The event made major news—­blogs and news organ­izations around 
the world covered it—­even though it ­wasn’t a real Turing test. Fully 
two-­thirds of the judges ­didn’t fall for Goostman’s tricks, ­after the test 
was cut off at five minutes. Still, press coverage of the event, billed as 
the “at long last” moment, the passing of Turing’s challenge to usher 
in AI, was predictably euphoric. The In­de­pen­dent hailed Goostman as 
a “breakthrough,” adding that the program was a “supercomputer.”1 
Time proclaimed that “The Age of Robots is ­Here.” The BBC called it 

192	
T h e  P r o b ­l e m  o f  I n f e r e n c e
a “world first”—­technically accurate—­while the popu­lar tech blog 
Gizmodo informed its readers, “This is big.” The press reaction was a 
testament to the continuing fascination with Alan Turing, no doubt. 
It was also recognition of the momentousness of ­really passing his 
test. Imagine, if you can, ­really chatting with your computer. The 
Goostman success dangled this perennial sci-fi dream in front of us.2
EUG E N E GOOST M A N I S A F R AU D
­There was only one prob­lem. Goostman was a fraud. Not long ­after 
the London spectacle, which won the Rus­sian team the $100,000 
Loebner Prize for passing the Turing test, computer scientists and 
commentators began complaining. For one, Goostman ­didn’t ­really 
answer questions. He usually changed the topic or blurted out sar-
casm. Gary Marcus, then at New York University, wrote in the New 
Yorker that the Goostman per­for­mance was ­little more than a “parlor 
trick.” He echoed Hector Levesque, who also dismissed Goostman-­
like per­for­mances as “cheap tricks.” As Marcus explained, the Goostman 
misdirection tactic creates the illusion of intelligence without re-
quiring any:
Marcus: Do you read the New Yorker?
Goostman: I read a lot of books . . . ​so many—­I ­don’t even 
know which ones.3
­These exchanges are like the ones ­people had with the 1960s chat 
program ELIZA, which mimicked a Rogerian psychotherapist:
Patient: Well, I’ve been having prob­lems with my husband.
ELIZA: Tell me more about your husband
While admittedly fun, they fail to make any headway on the very 
real natu­ral language challenges facing AI. In fact, Goostman (and 

	
I n f e r e n c e  a n d  L a n g u a g e  I 	
193
ELIZA) succeed by avoiding language understanding, and for that 
reason ­they’re irrelevant to a serious conversational test.
Perhaps understandably, pundits and commentators in the wake of 
the Goostman flap have resorted to decrying the Turing test itself as 
an illegitimate milestone. Gizmodo quickly ran a piece claiming 
Turing’s test was “b*llshit.” Wired questioned ­whether the goal of 
having a fireside chat with a computer should ­really be the benchmark 
for full-­fledged AI.4
The critics have a point, considering the Goostman debacle. 
Cheating by spitting back sarcastic answers without real under-
standing hardly mea­sures language ability in the way Turing in-
tended. Cheating at the Turing test by resorting to ploys and cheap 
tricks exposes a vulnerability we have to lowering the bar for our de-
tection of intelligence when expectations about conversational dia-
logue are themselves lowered.5 We might play along with thirteen-­
year-­old Ukrainian boys, thinking they know (and could care) less 
about adult dialogue. Likewise we might assume in a therapy session 
with ELIZA that typical therapeutic interaction involves a deliberate 
attempt to get us to talk—­thereby excusing the therapist from ­doing 
our thinking for us. Our expectations in ­these types of situations fit a 
social context that precludes assessment of intelligence by the respon-
dents in the first place. So it’s no won­der that AI researchers have 
mostly abandoned the Turing test challenge. Stuart Russell’s dis-
missing remark that “mainstream AI researchers have expended al-
most no effort to pass the Turing test” reflects this frustration with 
media frenzy about parlor trick per­for­mances.6 It’s a weakness in the 
field to accept them.
But the dismissal is entirely unnecessary. For one, an honest Turing 
test ­really is a high-­water mark for language understanding. As Ray 
Kurzweil has pointed out, an alien intelligence might not understand 
En­glish conversation, but any intelligence that did pass a legitimate 
Turing test must be intelligent. “The key statement is the converse,” 

194	
T h e  P r o b ­l e m  o f  I n f e r e n c e
he says: “In order to pass the test, you must be intelligent.”7 Kurzweil 
suggests that ­future competitions simply allow for a longer test, en-
suring that cheap tricks get filtered out in continuing dialogue.
This is just one suggestion. To go further, we might simply exclude 
the contribution of cheap tricks by adding a rule of play: Contestants 
could be required to answer questions directly, as if in front of a judge 
and sworn to “tell the ­whole truth and nothing but the truth.” Expec-
tations in a courtroom certainly preclude ELIZA-­like per­for­mances. 
Imagine responding to a prosecutor’s or judge’s question with “Tell 
me more about that night yourself. How did that make you feel?” Or 
we could instruct all contestants to play as if interviewing for the job 
of understanding conversational English—­not a bad test for ­future 
voice-­activated personal assistants! In such cases, tricks would be 
immediate violations of conversational rules baked into the test. 
Goostman would be sunk.
Computational linguists and AI researchers have known all along 
that engaging in open-­ended dialogue is formally more difficult than 
interpreting monologue, as in understanding a newspaper article. 
Yet another way to preserve Turing’s intuition that natu­ral language 
ability is a suitable test of human-­level intelligence is to consider a 
simplification of his original test, requiring only monologue. We can do 
this in the context of a question-­answering session, as with the orig-
inal test. Consider a test simplification: ­we’ll call it the Turing Test 
Monologue. In a Turing Test Monologue, the judge simply pastes in a 
news article or other text, then asks questions requiring an under-
standing of what it says. The respondent must answer ­those questions 
accurately. (Goodbye to tricks.) For instance, a judge might paste in 
the AP article “Your Tacos or Your Life!” and ask the respondent 
­whether the story is funny or not, and why? Passing this test would 
be, strictly speaking, a logical subset of a completely open-­ended 
Turing test, so it would be entirely fair to use it—in fact, it would give 
advantage to the machine, which might not completely understand 

	
I n f e r e n c e  a n d  L a n g u a g e  I 	
195
how to ­handle “pragmatic phenomena” in back-­and-­forth dialogue—­
more on this ­later.
Unfortunately, with the Turing Test Monologue, we already have 
evidence that AI systems are hopelessly lost. Gary Marcus and Ernest 
Davis point out that the state of the art for reading comprehension by 
machines is pitiful. Microsoft and Alibaba ­were much celebrated by 
the media for increasing a baseline score on a reading test known as the 
Stanford Question Answering Dataset (SQuAD), but only when the 
complete answers ­were in the text. This was therefore a simplified 
task of “underlining” answers as explic­itly provided, cued by ques-
tions that clearly pointed to them.8 Marcus and Davis go on to high-
light embarrassing per­for­mances on seemingly ­simple questions, 
such as one asking only for the name of the quarterback mentioned in 
a snippet of Super Bowl coverage. The failure of understanding in 
such cases is obvious. So even when we eliminate trickery, AI lan-
guage understanding is in trou­ble. The Turing test remains a legiti-
mate assessment, whose bar is, if anything, set too high.
The more we take a serious look at the requirements of language un-
derstanding, the more daunting passing even the simplified Turing Test 
Monologue becomes. Hector Levesque devised a vastly simplified ver-
sion of the test and called his quizzes Winograd schemas (­after AI pio-
neer Terry Winograd, who worked on natu­ral language understanding). 
Winograd schemas require answers to multiple-­choice questions about 
the meaning of single sentences in En­glish. That’s a far cry from the 
Turing test. And yet, AI researchers are a far cry from mastering them.
T H E C U R IOUS C A SE OF W I NOGR A D SC H E M A S
Hector Levesque is one of the few AI scientists ­today still focused on 
knowledge repre­sen­ta­tion and reasoning. Admirably, Levesque wants 
to imbue AI programs ­today with more than statistical techniques for 
analyzing big data: he wants to give them common sense.

196	
T h e  P r o b ­l e m  o f  I n f e r e n c e
Levesque proposed a simplified version of the Turing test which is 
much easier than the original open-­ended and unrestricted test, but 
that, importantly, still frustrates all known automated approaches to 
language understanding. In 2013, Levesque presented a paper, “On 
Our Best Be­hav­ior,” to the Internet Joint Center for Artificial Intelli-
gence, which was quickly recognized as a call to arms for genuine 
AI.9 Inspired by the full Turing test, Levesque suggested we pose 
questions to machines that require some deeper understanding of 
what’s being said. The test questions are single sentences, not entire 
conversations. For example: Can a crocodile run a steeplechase? Levesque 
deliberately chooses examples like this ­because non-­expert ­humans, 
having ordinary common sense, can figure them out (crocodiles cannot 
run a steeplechase), but popu­lar tricks like using a search engine to 
look up the answer ­won’t work. Since ­there ­won’t (one assumes) be 
any web pages discussing crocodiles ­running steeplechases, ­there 
­will be no way to bypass the need for understanding. AI systems run 
aground on such examples; an answer occurs to ­humans almost 
immediately.
A Winograd schema is a multiple-­choice exercise, which removes 
the possibility that the machine can resort to misdirection, sarcasm, 
jokes, or the appearance of a bad mood to bluff ­human judges in any 
situation where a direct answer would reveal its lack of understanding. 
The schemas are based on a common feature of natu­ral language, as 
Winograd’s original question inspiring them makes clear:
The town councilors refused to give the angry demonstrators a permit 
­because they feared vio­lence. Who feared vio­lence?
a) The town councilors
b) The angry demonstrators
Note the pronoun they. It’s a plural pronoun; it might refer to the 
councilors or the demonstrators. It’s ambiguous, in other words, ­because 
­either answer is pos­si­ble without breaking the rules of grammar. Yet 

	
I n f e r e n c e  a n d  L a n g u a g e  I 	
197
only one ­really makes sense. ­Humans get the right answer to ques-
tions like this effortlessly and with near hundred-­percent accuracy. 
AI systems do not. Their per­for­mance on Winograd schemas is not 
much better than random guessing.10
Working with other AI researchers, Levesque exhumed Wino-
grad’s challenge in 2012, when big data and machine learning clearly 
dominated approaches to AI (as they still do). He gathered a test set 
of multiple-­choice questions, all exploiting the common and ubiqui-
tous feature of ambiguity in natu­ral language. He called the ambi-
guity challenge the pronoun disambiguation prob­lem. It captured the 
inspiration ­behind the Turing test, but in a simplified form: ordinary 
natu­ral language understanding, like En­glish or French (or what have 
you), requires general intelligence. In par­tic­u­lar, Levesque believed 
that AI systems would require knowledge about what the words in 
language actually mean to do well on the test. ­Here’s another example 
of a Winograd schema:
Joan made sure to thank Susan for all the help she had given. Who 
had given the help?
a) Joan
b) Susan
To insulate the test from the prob­lem of cheap tricks used to fool 
judges on the Turing test, Levesque added a twist: two words desig-
nated as special that “flip” the answer, while leaving the rest of the 
question unchanged. In this example, the special words are given and 
received. Exchanging ­these designated special words generates another 
question:
Joan made sure to thank Susan for all the help she had received. Who 
had received the help?
a) Joan
b) Susan

198	
T h e  P r o b ­l e m  o f  I n f e r e n c e
­Here’s another schema, employing the special words golfers and 
dogs:
Sam tried to paint a picture of shepherds with sheep, but they ended 
up looking more like golfers. Who looked like golfers?
a) The shepherds
b) The sheep
Winograd schemas are vastly simplified compared to the orig-
inal, conversational Turing test, but by posing multiple-­choice 
questions that require resolving pronoun reference (pronoun disam-
biguation), they capture what ­earlier researchers called “common 
sense holism”—­the idea that natu­ral language cannot be understood 
by dissecting sentences, but requires a general understanding. Thus 
Winograd schemas are ­simple but typically unusual questions that 
make perfect sense to readers that have ordinary knowledge about 
the world. To take Marcus’s example, with basic knowledge of alliga-
tors and high hedges, it’s clear that the short legs of an alligator dis-
qualify it from competing by clearing hurdles. Why is this a difficult 
prob­lem for AI?
In part, schemas are difficult ­because the two referents—­the 
choice of nouns and noun phrases like alligator and hurdles—rarely 
(if ever) occur together in web pages and other text. Since data ap-
proaches to AI rely on lots of examples to analyze statistically, odd 
questions like ­those found in Winograd schemas represent a signifi-
cant challenge—­all in one sentence. In fact, Winograd schemas are 
quite bulletproof to tricks like counting web pages. ­They’re “Google-­
proof,” as Levesque puts it. But the requirement of ordinary knowl-
edge for their interpretation is a still deeper reason computers ­don’t 
perform well on them. Changing Marcus’s subject from alligators to 
gazelles would change the answer (gazelles can leap hurdles, no 
prob­lem), but again the question is odd and thus quite rare on the 
web. Machine learning and big data ­don’t help. AI systems ­can’t look 
up the answer.

	
I n f e r e n c e  a n d  L a n g u a g e  I 	
199
As we might expect, AI researchers have devised new tricks to 
tackle Winograd schemas. ­Because so much content is available for 
analy­sis on the web ­today, frequency patterns can still be exploited in 
some cases. For instance, to answer questions about ­whether x is 
taller than y, researchers can mimic real knowledge by performing 
web searches with the pattern “x is taller than y.” If the ­people, build-
ings, or what­ever ­things standing in for x and y happen to be in reposi-
tories like Wikipedia, data can be found, and ­math employed to answer 
the question—­all by pro­cessing info-­fields and other data sources on 
the web. But, again, Levesque anticipated this technique and suggested 
examples that use common nouns (gazelles, alligators, pens, paper, 
bowling balls, sheep, and so forth) that ­don’t appear in curated databases 
or online encyclopedias. This stymies the search-­the-­web trick, while 
also highlighting the commonsense, ordinary quality of knowledge 
required—­preserving Turing’s original intent to use basic conversation 
that every­one engages in day to day.
Winograd schemas also guard against search-­engine tricks in an-
other impor­tant way. The relation between x and y can be modified 
too, just like the ­things discussed (alligators versus gazelles). This is 
another roadblock to using data tricks, all the while preserving the 
simplicity of the questions. Consider first a schema requiring knowl-
edge of the relative sizes of common objects:
The trophy would not fit in the brown suitcase ­because it was so 
small. What was so small?
a) The trophy
b) The brown suitcase
­There is a trick—­a technique—­for this schema, actually. As Levesque 
puts it, in quasi “computer-­ese” language, we can dissect the question 
into a relation (call it R), and a single property (call it P):
R = does not fit in
P = is so small

200	
T h e  P r o b ­l e m  o f  I n f e r e n c e
Then we use “big data,” he says, and “search all the text on the web 
to determine which is the more common pattern: ‘x does not fit in y’ + 
‘x is so small’ vs. ‘x does not fit in y’ + ‘y is so small.’ ”11 For the scores of 
example sentences returned by search, a pattern might emerge where, 
for instance, the second item mentioned is more frequently smaller 
than the first. Imagine a social media post in which someone is com-
plaining about packing and keeps mentioning that such-­and-­such 
­won’t fit ­because their backpack is too small. Simply counting search 
results for the two patterns can yield a statistical answer, and if it’s 
correct, a system can answer Winograd schema questions without 
knowledge.
Unfortunately, the approach is quite shallow. It is powerless against 
even small modifications that change the meaning of the question, and 
hence switch the preferred answer. Our original “relation R” trick fails, 
for instance, on this question:
The trophy would not fit in the brown suitcase despite the fact that it 
was so small. What was so small?
a) The trophy
b) The brown suitcase
It’s the trophy that’s so small now, not the suitcase. Thus the speci-
fication of R and P ­will generate an incorrect response. The modified 
question is prob­ably less frequent than its original version, so big data 
is a hindrance, not a help. This is the rub. Even with ­simple, one-­
sentence questions, the meanings of words—­alligators, hurdles—­and 
the meanings of the relations between ­things—­smaller than—­frustrate 
techniques relying on data and frequency without understanding. 
Winograd schemas are a win­dow into the much greater difficulty of 
the Turing test.
Machine learning and big data have made significant pro­gress on 
some prob­lems in the last de­cade. As a rule, though, workarounds 

	
I n f e r e n c e  a n d  L a n g u a g e  I 	
201
that sidestep ­actual knowledge and understanding account for the 
successes. ­There is an ongoing confusion ­here, particularly with auto-
mated language translation systems which seem to require some sort 
of language understanding; take Google Translate, which is often 
touted as a runaway success story proving how AI is quickly taming 
natu­ral language. But the relatively recent availability of large vol-
umes of translated texts on the web facilitates data-­intensive approaches, 
which deliver “good enough” results by exploiting mappings between 
words and sentences in texts translated into dif­fer­ent languages. The 
mappings are mostly all in the data, so inductive strategies work. 
Understanding is not required.
For instance, official Canadian parliamentary documents are 
translated word for word from En­glish into French; in a case like this, 
approaches like deep learning suffice to “learn” the mappings be-
tween the languages. No ­actual understanding of En­glish or French 
is required. For example, if you type John met Mary at the café into 
Google Translate, and ask for a translation to French, you’ll get 
John a rencontré Mary au café. This is a perfectly acceptable transla-
tion. Other examples fail, however. The failures typically involve 
ambiguity somewhere, as with referential phenomena like pro-
nouns, or polysemous (many-­meanings) words embedded in context. 
(If you want to experiment with Google Translate on sentences 
containing ambiguity, try The box is in the pen or I loved the river. I walked 
to the bank.)
Confusingly, work in the 1960s on so-­called fully-­automated 
high-­quality machine translation began with statistical approaches, 
albeit simpler ones. They ­didn’t work very well, and soon apostates 
like Yehoshua Bar-­Hillel concluded that automatic translation was 
hopeless, ­because knowledge and an appreciation of context seemed 
necessary—­reasons that ­later inspired Winograd and then Levesque 
to formulate knowledge-­based schemas. But Bar-­Hillel ­didn’t antici-
pate the contribution that big data would make a few de­cades ­later. 

202	
T h e  P r o b ­l e m  o f  I n f e r e n c e
Statistical approaches started showing promise in the 1980s, with work 
by IBM researchers, and web ­giants like Google used similar tech-
niques (and now deep learning) to provide decent statistically-­driven 
translation ser­vices. Fully-­automated high-­quality machine translation 
came full circle.
But Bar-­Hillel’s original skepticism is still germane; translations 
requiring knowledge and context are still a black box to modern ap-
proaches, big data and all. For instance (and ironically), Google 
Translate as of October  2020 still gets Bar-­Hillel’s 1960s example 
wrong. Bar-­Hillel asked how to program a machine to translate The 
box is in the pen correctly. ­Here, pen is ambiguous. It might mean a 
writing instrument, or it might mean a small enclosure for animals. 
Using Google Translate, The box is in the pen translates to La boîte est 
dans le stylo, in French, where stylo means writing instrument, which 
­isn’t the preferred interpretation (­because boxes are typically larger 
than writing pens). In other words, “good enough” translation de-
pends on one’s requirements. Google Translate might get less contex-
tualized sentences most of the time, but ­there ­will be errors—­and the 
errors might ­matter, depending on the person using the ser­vice. Ser­
vices like Google Translate actually underscore the long-­tail prob­lem 
of statistical, or inductive, approaches that get worse on less likely ex-
amples or interpretations. This is yet another way of saying that likeli-
hood is not the same as genuine understanding. It’s not even in the 
same conceptual space. ­Here again, the frequency assumption repre-
sents real limitations to getting to artificial general intelligence.
“Good enough” results from modern AI are themselves a kind of 
trick, an illusion that masks the need for understanding when reading 
or conversing. On image-­recognition tasks, systems might be for-
given for incorrectly classifying certain images (but not if the system 
is a self-­driving car). But on language tests like schemas, a 20 ­percent 
error rate (thus 80 ­percent accuracy) means that fully two out of ten 
examples are opaque to the system. And accuracy rates on Winograd 

	
I n f e r e n c e  a n d  L a n g u a g e  I 	
203
schema tests are much worse—­barely better than random. As schema 
questions make clear, errors are less excusable when they are ­simple 
sentences, with no inherent difficulty for ­humans.
The point is that accuracy is itself contextual, and on tests that ex-
pose the absence of any understanding, getting six out of ten answers 
correct (as with state-­of-­the-­art systems) ­isn’t pro­gress at all. It’s evi-
dence of disguised idiocy. Like the mythical androids in the sci-fi 
thriller Blade Runner, a test looking for mind (or real emotion) ­will 
eventually unmask machines as programmed impostors. Only, the 
Winograd schemas are short and quite ­simple. All it takes is one ques-
tion, requiring some basic knowledge of what’s being said.
Levesque’s “pronoun disambiguation prob­lem” is a vastly simpli-
fied step in a very large minefield that automated systems ­will have to 
navigate to have a prayer of managing ordinary conversations. Re-
solving pronoun references is just one knowledge-­based prob­lem 
among a plethora of ­others that must be solved for genuine language 
understanding. And Winograd schemas hugely simplify in other ways, 
as well. In addition to the one-­sentence limit for each schema, the test 
also simplifies by ignoring most of the pragmatic phenomena in con-
versational dialogue. Pragmatics is hell for automation. To see why, 
­we’ll next tour through the field of natu­ral language pro­cessing, or 
understanding.

