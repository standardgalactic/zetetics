Physics Reports 441 (2007) 1–46
www.elsevier.com/locate/physrep
Causality detection based on information-theoretic approaches in
time series analysis
Katerina Hlaváˇcková-Schindlera,∗, Milan Palušb, Martin Vejmelkab,
Joydeep Bhattacharyaa,c
aCommission for Scientiﬁc Visualization, Austrian Academy of Sciences, Donau-City Str. 1, A-1220 Vienna, Austria
bInstitute of Computer Science, Academy of Sciences of the Czech Republic Pod Vodárenskou vˇeží 2, 18207 Praha 8, Czech Republic
cDepartment of Psychology, Goldsmiths College, University of London, New Cross SE14 6NW, London, UK
Accepted 24 December 2006
Available online 6 February 2007
editor: I. Procaccia
Abstract
Synchronization, a basic nonlinear phenomenon, is widely observed in diverse complex systems studied in physical, biological
and other natural sciences, as well as in social sciences, economy and ﬁnance. While studying such complex systems, it is important
not only to detect synchronized states, but also to identify causal relationships (i.e. who drives whom) between concerned (sub)
systems. The knowledge of information-theoretic measures (i.e. mutual information, conditional entropy) is essential for the analysis
of information ﬂow between two systems or between constituent subsystems of a complex system. However, the estimation of these
measures from a set of ﬁnite samples is not trivial. The current extensive literatures on entropy and mutual information estimation
provides a wide variety of approaches, from approximation-statistical, studying rate of convergence or consistency of an estimator
for a general distribution, over learning algorithms operating on partitioned data space to heuristical approaches. The aim of this
paper is to provide a detailed overview of information theoretic approaches for measuring causal inﬂuence in multivariate time series
and to focus on diverse approaches to the entropy and mutual information estimation.
© 2007 Elsevier B.V. All rights reserved.
PACS: 05.10.−a; 0.45.−a; 07.05.−t
Keywords: Causality; Entropy; Mutual information; Estimation
Contents
1.
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.
Causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2.
Causal measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.
Information theory as a tool for causality detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.1.
Deﬁnitions of basic information theoretic functionals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.2.
Information, entropy and dynamical systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.3.
Coarse-grained entropy and information rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.4.
Conditional mutual information and transfer entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
∗Corresponding author. Tel.: +43 1 515 81 6708; fax: +43 1 20501 18900.
E-mail addresses: katerina.schindler@assoc.oeaw.ac.at, katerina.schindler@networld.at (K. Hlaváˇcková-Schindler).
0370-1573/$ - see front matter © 2007 Elsevier B.V. All rights reserved.
doi:10.1016/j.physrep.2006.12.004

2
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
3.
Basic classiﬁcation of current methods for entropy and mutual information estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
3.1.
Conditions and criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
3.2.
Classiﬁcation of methods for entropy estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
4.
Non-parametric estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
4.1.
Plug-in estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
4.1.1. Integral estimates of entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
4.1.2. Resubstitution estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
4.1.3. Splitting data estimate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
4.1.4. Cross-validation estimate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
4.1.5. Convergence properties of discrete plug-in estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
4.2.
Estimates of entropy based on partitioning of the observation space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
4.2.1. Fixed partitioning of the observation space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
4.2.2. Adaptive partitioning of the observation space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
4.3.
Ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
4.4.
Estimates of entropy and mutual information based on computing distances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
4.4.1. Based on sample spacings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
4.4.2. Based on nearest neighbor search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
4.5.
Estimates based on learning theory methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
4.5.1. Motivated by signal processing problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
4.5.2. Estimates by neural network approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
4.6.
Entropy estimates based on maximum likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
4.7.
Correction methods and bias analysis in undersampled regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
4.8.
Kernel methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
4.8.1. Transfer entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
5.
Parametric estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
5.1.
Entropy expressions for multivariate distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
5.2.
Entropy estimators by higher-order asymptotic expansions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
5.2.1. Mutual information estimation by Gram–Charlier polynomial expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
5.2.2. Edgeworth approximation of entropy and mutual information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
6.
Generalized Granger causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
6.1.
Nonlinear Granger causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
6.2.
Non-parametric Granger causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
7.
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
1. Introduction
1.1. Causality
Detection and clariﬁcation of cause–effect relationships among variables, events or objects have been the fundamental
questions of most natural and social sciences over the history of human knowledge. Despite some philosophers of
mathematics like Russel [198] (1872–1970) tried to deny the existence of the phenomenon “causality” in mathematics
and physics, saying that causal relationships and physical equations are incompatible, calling causality to be “a word
relic” (see Ref. [171]), the language of all sciences, including mathematics and physics, has been using this term actively
until now. Mathematical and physical relations are not limited only to equations. To advocate the Russell’s view, any
exact and sufﬁciently comprehensive formulation of what is causality is problematic. Causality can be understood in
terms of a “ﬂow” among processes and expressed in mathematical language and mathematically analysed. Current
statistics understands causal inference as one of its most important problems.
The general philosophical deﬁnition of causality from the Wikipedia Encyclopedia [253] states: “The philosophical
concept of causality or causation refers to the set of all particular “causal” or “cause-and-effect” relations. A neutral
deﬁnition is notoriously hard to provide, since every aspect of causation has received substantial debate. Most generally,
causation is a relationship that holds between events, objects, variables, or states of affairs. It is usually presumed that
the cause chronologically precedes the effect.”

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
3
Causality expresses a kind of a “law” necessity, while probabilities express uncertainty, a lack of regularity. Despite of
these deﬁnitions, causal relationships are often investigated in situations which are inﬂuenced by uncertainty. Probability
theory seems to be the most used “mathematical language” of most scientiﬁc disciplines using causal modeling, but it
seems not to be able to grasp all related questions. In most disciplines, adopting the above deﬁnition, the aim is not only
to detect a causal relationship but also to measure or quantify the relative strengths of these relationships.Although there
is an extensive literature on causality modeling, applying and combining mathematical logic, graph theory, Markov
models, Bayesian probability, etc. (Pearl [171]), the aim of our review is to focus only on the information-theoretic
approaches which understand causality as a phenomenon which can be “measured” and quantiﬁed.
We want to provide a detailed overview of the information-theoretic approaches for measuring of a causal inﬂuence
in multi-variate time series. Based on the deﬁnition of causality in the information-theoretic framework, we focus on
approaches to the estimation of entropy and mutual information. The previous review papers on entropy estimation
(Beirlant et al. [22] and Erdogmus [70]) focused on non-parametric entropy estimation. In this paper we not only update
the state of art on non-parametric entropy estimation, but discuss also parametrical estimation.
1.2. Causal measures
As mentioned earlier, there has been no universally accepted deﬁnition of causality (see Granger [91] for a lively
discussion on this issue), so it would be futile to search for a unique causality measure. However, we mention here
brieﬂy the salient features of this debate for convenience. Most of the earlier research literature attempts to discuss
unique causes in deterministic situations, and two conditions are important for deterministic causation: (i) necessity:
if X occurs, then Y must occur, and (ii) sufﬁciency: if Y occurs, then X must have occurred. However, deterministic
formulation, albeit appealing and analytically tractable, is not in accordance with reality, as no real-life system is strictly
deterministic (i.e. its outcomes cannot be predicted with complete certainty). So, it is more realistic if one modiﬁes the
earlier formulation in terms of likelihood (i.e. if X occurs, then the likelihood of Y occurring increases). This can be
illustrated by a simple statement such as if the oil price increases, the carbon emission does not necessarily decrease,
but there is a good likelihood that it will decrease. The probabilistic notion of causality is nicely described by Suppes
(1970) as follows: an event X is a cause to the event Y if (i) X occurs before Y, (ii) likelihood of X is non zero, and
(iii) likelihood of occurring Y given X is more than the likelihood of Y occurring alone. Although this formulation is
logically appealing (however, see Ref. [157] for a critique of Suppe’s causality), there are some arbitrariness in practice
in categorizing an event [91].
Till 1970, the causal modeling was mostly used in social sciences. This was primarily due to a pioneering work by
Selltiz et al. [210] who speciﬁed three conditions for the existence of causality:
(1) There must be a concomitant covariation between X and Y.
(2) There should be a temporal asymmetry or time ordering between the two observed sequences.
(3) The covariance between X and Y should not disappear when the effects of any confounding variables (i.e. those
variables which are causally prior to both X and Y) are removed.
The ﬁrst condition implies a correlation between a cause and its effect, though one should explicitly remember that
a perfect correlation between two observed variables in no way implies a causal relationship. The second condition
is intuitively based on the arrow of time. The third condition is problematic since it requires that one should rule out
all other possible causal factors. Theoretically, there are potentially an inﬁnite number of unobserved confounding
variables available, yet the set of measured variables is ﬁnite, thus leading to indeterminacy in the causal modeling
approach. In order to avoid this, some structure is imposed on the adopted modeling scheme which should help to
deﬁne the considered model. The way in which the structure is imposed is crucial in deﬁning as well as in quantifying
causality.
The ﬁrst deﬁnition of causality which could be quantiﬁed and measured computationally, yet very general, was given
in 1956 by Wiener [252]: “For two simultaneously measured signals, if we can predict the ﬁrst signal better by using
the past information from the second one than by using the information without it, then we call the second signal causal
to the ﬁrst one.”
The introduction of the concept of causality into the experimental practice, namely into analyses of data observed
in consecutive time instants, time series, is due to Clive W. J. Granger, the 2003 Nobel prize winner in economy.

4
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
In his Nobel lecture [92] he recalled the inspiration by the Wiener’s work and identiﬁed two components of the
statement about causality:
(1) the cause occurs before the effect; and
(2) the cause contains information about the effect that is unique, and is in no other variable.
As Granger put it, a consequence of these statements is that the causal variable can help to forecast the effect variable
after other data has been ﬁrst used [92].This restricted sense of causality, referred to as Granger causality, GC thereafter,
characterizes the extent to which a process Xt is leading another process Yt, and builds upon the notion of incremental
predictability. It is said that the process Xt Granger causes another process Yt if future values of Yt can be better
predicted using the past values of Xt and Yt rather than only past values of Yt. The standard test of GC developed by
Granger [88] is based on a linear regression model
Yt = ao +
L

k=1
b1kYt−k +
L

k=1
b2kXt−k + t,
(1)
where t are uncorrelated random variables with zero mean and variance 2, L is the speciﬁed number of time lags, and
t = L + 1, . . . , N. The null hypothesis that Xt does not Granger cause Yt is supported when b2k = 0 for k = 1, . . . , L,
reducing Eq. (1) to
Yt = ao +
L

k=1
b1kYt−k + ˜t.
(2)
This model leads to two well-known alternative test statistics, the Granger–Sargent and the Granger–Wald test. The
Granger–Sargent test is deﬁned as
GS = (R2 −R1)/L
R1/(N −2L),
(3)
where R1 is the residual sum of squares in Eq. (1) and R2 is the residual sum of squares in Eq. (2). The GS test
statistic has an F-distribution with L and N −2L degrees of freedom [2]. On the other hand, the Granger–Wald test is
deﬁned as
GW = N
ˆ2
˜t −ˆ2
t
ˆ2
t
,
(4)
where ˆ2
˜t is the estimate of the variance of ˜t from model (2) and ˆ2
t is the estimate of the variance of t from
model (1). The GW statistic follows the 2
L distribution under the null hypothesis of no causality.
This linear framework for measuring and testing causality has been widely applied not only in economy and ﬁnance
(see Geweke [85] for a comprehensive survey of the literature), but also in diverse ﬁelds of natural sciences such as
climatology (see Ref. [236] and references therein) or neurophysiology, where speciﬁc problems of multichannel elec-
troencephalogram recordings were solved by generalizing the Granger causality concept to multivariate case [126,36].
Nevertheless, the limitation of the present concept to linear relations required further generalizations.
Recent development in nonlinear dynamics [1] evoked lively interactions between statistics and economy (econo-
metrics) on one side, and physics and other natural sciences on the other side. In the ﬁeld of economy, Baek and
Brock [14] and Hiemstra and Jones [110] proposed a nonlinear extension of the Granger causality concept. Their
non-parametric dependence estimator is based on so-called correlation integral, a probability distribution and entropy
estimator, developed by physicists Grassberger and Procaccia in the ﬁeld of nonlinear dynamics and deterministic
chaos as a characterization tool of chaotic attractors [94]. A non-parametric approach to nonlinear causality testing,
based on non-parametric regression, was proposed by Bell et al. [23]. Following Hiemstra and Jones [110], Aparicio
and Escribano [10] succinctly suggested an information-theoretic deﬁnition of causality which include both linear and
nonlinear dependence.

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
5
In physics and nonlinear dynamics, a considerable interest recently emerged in studying cooperative behavior of
coupled complex systems [177,37]. Synchronization and related phenomena were observed not only in physical, but
also in many biological systems. Examples include the cardio-respiratory interaction [199,200,166,38,220,120] and
the synchronization of neural signals [201,187,228,160,159]. In such physiological systems it is not only important to
detect synchronized states, but also to identify drive–response relationships and thus the causality in evolution of the
interacting (sub)systems. Schiff et al. [201] and Quyen et al. [187] used ideas similar to those of Granger, however, their
cross-prediction models utilize zero-order nonlinear predictors based on mutual nearest neighbors.A careful comparison
of these two papers [201,187] reveals how complex is the problem of inferring causality in nonlinear systems. The
authors of the two papers use contradictory assumptions for interpreting the differences in prediction errors of mutual
predictions, however, both the teams were able to present numerical examples in which their approaches apparently
worked.
While the latter two papers use the method of mutual nearest neighbors for mutual prediction, Arnhold et al. [12]
proposed asymmetric dependence measures based on averaged relative distances of the (mutual) nearest neighbors.
As pointed out by Quian Quiroga et al. and by Schmitz [188,203], these measures, however, might be inﬂuenced
by different dynamics of individual signals and different dimensionality of the underlying processes, rather than by
asymmetry in coupling.
Another nonlinear extension of the Granger causality approach was proposed by Chen et al. [46] using local linear
predictors. An important class of nonlinear predictors are based on so-called radial basis functions [42] which were
used for nonlinear parametric extension of the Granger causality concept [7,143]. Although they are not exactly based
on information theory, they are connected to methods reviewed here and will be discussed more in detail in Section 6.
A non-parametric method for measuring causal information transfer between systems was proposed by Schreiber
[206]. His transfer entropy is designed as a Kullback–Leibler distance (Eq. (15) in Section 2.1) of transition probabilities.
This measure is in fact an information-theoretic functional of probability distribution functions.
Paluš et al. [160] proposed to study synchronization phenomena in experimental time series by using the tools of
information theory. Mutual information, an information-theoretic functional of probability distribution functions, is a
measure of general statistical dependence. For inferring causal relation, conditional mutual information can be used.
It will be shown that, with proper conditioning, the Schreiber’s transfer entropy [206] is equivalent to the conditional
mutual information [160]. The latter, however, is a standard measure of information theory [50].
Turning our attention back to econometrics, we can follow further development due to Diks and DeGoede [62].
They again applied a non-parametric approach to nonlinear Granger causality using the concept of correlation integrals
[94] and pointed out the connection between the correlation integrals and information theory. Diks and Panchenko [64]
critically discussed the previous tests of Hiemstra and Jones [110].As the most recent development in economics, Baghli
[15] proposes information-theoretic statistics for a model-free characterization of causality, based on an evaluation of
conditional entropy.
The nonlinear extension of the Granger causality based the information-theoretic formulation has found numerous
applications in various ﬁelds of natural and social sciences. Let us mention just a few examples. The Schreiber’s transfer
entropy was used in climatology [149,245], in physiology [245,130], in neurophysiology [45] and also in analysis of
ﬁnancial data [144]. Paluš et al. [159,160] applied their conditional mutual information based measures in analyses of
electroencephalograms of patients suffering from epilepsy. Other applications of the conditional mutual information in
neurophysiology are due to Hinrichs et al. [111] and Pﬂieger and Greenblatt [176]. Causality or coupling directions in
multimode laser dynamics is another diverse ﬁeld where the conditional mutual information was applied [156]. Paluš
and Stefanovska [158] adapted the conditional mutual information approach [160] to analysis of instantaneous phases
of interacting oscillators and demonstrated suitability of this approach for analyzing causality in cardio-respiratory
interaction [160]. The latter approach has also been applied in neurophysiology [39].
Having reviewed the relevant literature and also after extensive practical experience, we can state that the information-
theoretic approach to the Granger causality plays an important, if not a dominant role in analyses of causal relationships
in nonlinear systems. Therefore, we focus in this review to the information theory and its applications in inference of
causality from experimental time series, although we do not refrain mentioning other approaches.
The outline of the paper is the following. In Section 1 we explain the terms causality and its measures. Basic
notions of information theory and their approaches to causality detection are discussed in Section 2. Section 3 classiﬁes
the methods which will be reviewed. The rest of the sections deals with the concrete methods: the non-parametric
methods are treated in Section 4, the parametric methods in Section 5. Generalized Granger causality, although not

6
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
being explicitly an information-theoretic approach, deserves our attention, too. It is discussed in Section 6. Section 7
is devoted to our conclusion.
2. Information theory as a tool for causality detection
2.1. Deﬁnitions of basic information theoretic functionals
We begin with the deﬁnition of differential entropy for a continuous random variable as it was introduced in 1948 by
Shannon [211], the founder of the information theory. Let X be a random vector taking values in Rd with probability
density function (pdf) p(x), then its differential entropy is deﬁned by
H(x) = −

p(x) log p(x) dx,
(5)
where log is natural logarithm. We assume that H(x) is well deﬁned and ﬁnite. One can deﬁne the discrete version
of differential entropy as follows. Let S be a discrete random variable having possible values s1, . . . , sm, each with
corresponding probability pi = p(si), i = 1, . . . , m. The average amount of information gained from a measurement
that speciﬁes one particular value si is given by the entropy H(S):
H(S) = −
m

i=1
pi log pi.
(6)
EntropyH(S)canbeunderstoodasthe“quantityofsurpriseoneshouldfeeluponreadingtheresultofameasurement”
[78]. So entropy of S can be seen as the uncertainty of S.
More general term of entropy for which is Shannon differential entropy a special case, is Rényi entropy. Rényi
entropy is for a continuous case deﬁned as [191]
H(x) =
1
1 −

logp(x) dx
(7)
and for the discrete case
H(S) =
1
1 − log
n

i=1
p
i ,
(8)
where  > 0,  ̸= 1. As  →1, H(x) converges to H(x) (or H(S) converges to H(S)), which is Shannon’s measure
of entropy. Rényi’s measure satisﬁes H(x)⩽H′(x) for  > ′.
Besides Shannon and Rényi entropy, other entropy deﬁnitions (i.e. Tsallis, Havrda-Charvát, etc.) are studied in the
mathematical literature, but Shannon entropy is the only one possessing all the desired properties of an information
measure. Therefore its efﬁcient and accurate estimate is of prime importance. Based on this, although Rényi entropy
will be also discussed, we focus in our review mainly on Shannon entropy estimators and their application to mutual
information. The deﬁnition of the latter follows, without lack of generality, only for discrete distributions.
The joint entropy H(X, Y) of two discrete random variables X and Y is deﬁned analogously
H(X, Y) = −
mX

i=1
mY

j=1
p(xi, yj) log p(xi, yj),
(9)
where p(xi, yj) denotes the joint probability that X is in state xi and Y in state yj (the number of possible states mX
and mY may differ). If the random variables X and Y are statistically independent, the joint entropy H(X, Y) becomes
H(X, Y) = H(X) + H(Y). In general, the joint entropy may be expressed in terms of conditional entropy H(X|Y) as
follows:
H(X, Y) = H(X|Y) + H(Y),
(10)

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
7
where
H(X|Y) = −
mX

i=1
mY

j=1
p(xi, yj) log p(xi|yj)
(11)
and p(xi|yj) denotes the conditional probability. The joint entropy expresses how much uncertainty remains in X when
Y is known.
The mutual information I(X, Y) between two random variables X and Y is then deﬁned as [211]
I(X; Y) = H(X) + H(Y) −H(X, Y).
(12)
Mutual information of two variables reﬂects the mutual reduction in uncertainty of one by knowing the other one. This
measure is non-negative since H(X, Y)⩽H(X) + H(Y); the equality holds if and only if X and Y are statistically
independent. It is invariant under one-to-one measurable transformations. Mutual information (MI) measures the
strength of dependence in the sense that: (1) I(X, Y) = 0 iff X is independent of Y; (2) for the bivariate normal
distributions, I(X, Y) = −1
2 log(1 −2(X, Y)), where  is the coefﬁcient of correlation between X and Y.
The conditional mutual information [211] between random variables X and Y given Z is deﬁned as
I(X, Y|Z) = H(X|Z) + H(Y|Z) −H(X, Y|Z).
(13)
For Z independent of X and Y we have
I(X, Y|Z) = I(X, Y).
(14)
Beside MI, there are other measures of relationships among variables. The most used measures like Pearson’s
correlation or Euclidean distance can reﬂect the degree of linear relationship between two variables. MI is sensitive to
other than linear functional relationships (i.e. nonlinear) and therefore provides a more general criterion to investigate
relationships between variables.
In the following we mention some other useful entropies and their relationship to MI.
The Kullback–Leibler divergence (KLD, also called relative entropy or cross-entropy), introduced by Kullback and
Leibler [137], is an alternative approach to MI. The Kullback entropy K(p, p0) between two probability distributions
{p} and p0 is
K(p, p0) =
m

i=1
pi log

pi
p0
i

.
(15)
It can be interpreted as the information gain when an initial probability distribution p0 is replaced by a ﬁnal distribution
p. This entropy is however not symmetric and therefore not a distance in the mathematical sense. The KLD is always
non-negative and is zero iff the distributions p and p0 are identical (Cover and Thomas [50]).
The neg-entropy is deﬁned as
K(p, p) =

i
pi log

pi
p

(16)
(i.e. p0 = p), where p is multivariate Gaussian distribution having the same mean vector and covariance matrix as
p. MI is the Kullback–Leibler divergence of the product P(X)P(Y) of two marginal probability distributions from the
joint probability distribution P(X, Y), see Ref. [84]. So we can look at the results about Kullback–Leibler entropy as
if they were applied to MI (relationship of KLD to entropy and conditional entropy can be also found in Ref. [84]).
2.2. Information, entropy and dynamical systems
A considerable amount of approaches to inferring causality from experimental time series have their roots in studies
of synchronization of chaotic systems. It is therefore useful to make a few remarks about the connection between the
theory of dynamical systems and information theory.

8
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
A.N. Kolmogorov, who introduced the theoretical concept of classiﬁcation of dynamical system by information
rates [134], was inspired by information theory and together with Y.G. Sinai generalized the notion of entropy of
an information source [134,216]. A possibility to use ideas and methods from the information theory in the ﬁeld
of nonlinear dynamics and related analyses of experimental data was demonstrated by Shaw [212,213]. Fraser [79]
analyzed information aspects of chaotic dynamics on strange attractors. Paluš [163] concentrated on attributes of
dynamical systems studied in the ergodic theory, such as mixing and generating partitions, and demonstrated how they
were reﬂected in the behaviour of information-theoretic functionals estimated from chaotic data. Let us review several
important details.
Consider n discrete random variables X1, . . . , Xn with sets of values 1, . . . , n, respectively. The probability
distribution for an individual Xi is p(xi) = Pr{Xi = xi}, xi ∈i. We denote the probability distribution function
by p(xi), rather than pXi(xi), for convenience. Analogously, the joint distribution for the n variables X1, . . . , Xn is
p(x1, . . . , xn) = Pr{(X1, . . . , Xn) = (x1, . . . , xn)}, (x1, . . . , xn) ∈1 × · · · × n.
The marginal redundancy 	(X1, . . . , Xn−1; Xn), which in the case of two variables reduces to the above-deﬁned
MI I(X1; X2), quantiﬁes the average amount of information about the variable Xn contained in the n −1 variables
X1, . . . , Xn−1, and is deﬁned as
	(X1, . . . , Xn−1; Xn) =

x1∈1
· · ·

xn∈n
p(x1, . . . , xn) log
p(x1, . . . , xn)
p(x1, . . . , xn−1)p(xn).
(17)
Now, let {Xi} be a stochastic process, i.e. an indexed sequence of random variables, characterized by the joint
probability distribution function p(x1, . . . , xn). The entropy rate of {Xi} is deﬁned as
h = lim
n→∞
1
n H(X1, . . . , Xn),
(18)
where H(X1, . . . , Xn) is the joint entropy of the n variables X1, . . . , Xn with the joint distribution p(x1, . . . , xn):
H(X1, . . . , Xn) = −

x1∈1
· · ·

xn∈n
p(x1, . . . , xn) log p(x1, . . . , xn).
(19)
Consider further two processes {Xi} and {Yi}, their mutual information rate [118,178] is
™(X; Y) = lim
n→∞
1
nI((X1, . . . , Xn); (Y1, . . . , Yn)).
(20)
A way from the entropy rate of a stochastic process to the Kolmogorov–Sinai entropy (KSE) of a dynamical system can
be straightforward due to the fact that any stationary stochastic process corresponds to a measure-preserving dynamical
system, and vice versa [173]. Then for the deﬁnition of the KSE we can consider Eq. (18), however, the variables
Xi should be understood as m-dimensional variables, according to the dimensionality of a dynamical system. If the
dynamical system is evolving in continuous (probability) measure space, then any entropy depends on a partition chosen
to discretize the space and the KSE is deﬁned as supremum over all ﬁnite partitions [48,173,217].
Possibilities to compute the entropy rates from data are limited to a few exceptional cases: for stochastic processes it
is possible, e.g. for the ﬁnite-state Markov chains [50]. In the case of a dynamical system in a continuous measure space,
the KSE can be in principle reliably estimated, if the system is low-dimensional and a large amount of (practically
noise-free) data is available. In such a case, Fraser [79] proposed to estimate the KSE of a dynamical system from the
asymptotic behavior of the marginal redundancy, computed from a time series generated by the dynamical system. In
such an application one deals with a time series {y(t)}, considered as a realization of a stationary and ergodic stochastic
process {Y(t)}. Then, due to ergodicity, the marginal redundancy (17) can be estimated using time averages instead of
ensemble averages, and, the variables Xi are substituted as
Xi = y(t + (i −1)
).
(21)
Due to stationarity, the marginal redundancy
	n(
) ≡	(y(t), y(t + 
), . . . , y(t + (n −2)
); y(t + (n −1)
))
(22)
is a function of n and 
, independent of t.

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
9
If the underlying dynamical system is m-dimensional and the marginal redundancy 	n(
) is estimated using a partition
ﬁne enough (to attain the so-called generating partition [79,48,217]) then the asymptotic behavior
	n(
) ≈H1 −|
|h
(23)
is attained for n = m + 1, m + 2, . . . , for some range of 
 [79,164,163]. The constant H1 is related to 	n(0) and h is
the estimate of the Kolmogorov–Sinai entropy of the dynamical system underlying the analyzed time series {y(t)}.
2.3. Coarse-grained entropy and information rates
In order to obtain such estimates as those in Eq. (23), large amounts of data are necessary [163]. Unfortunately, such
data requirement are not realistic in usual experiments. To avoid this, Paluš [162] proposed to compute “coarse-grained
entropy rates” (CERs) as relative measures of “information creation” and of regularity and predictability of studied
processes.
Let {x(t)} be a time series considered as a realization of a stationary and ergodic stochastic process {X(t)}, t =
1, 2, 3, . . . . In the following we denote x(t) as x and x(t + 
) as x
 for simplicity. To deﬁne the simplest form of CER,
we compute the MI I(x; x
) for all analyzed datasets and ﬁnd such 
max that for 
′ ⩾
max : I(x; x
′) ≈0 for all the
data sets. Then we deﬁne a norm of the mutual information
∥I(x; x
)∥=


max −
min + 

max


=
min
I(x; x
)
(24)
with 
min = 
 = 1 sample as a usual choice. The CER h1 is then deﬁned as
h1 = I(x, x
0) −∥I(x; x
)∥.
(25)
It was shown that the CER h1 provides the same classiﬁcation of states of chaotic systems as the exact KSE [162]. Since
usually 
0 = 0 and I(x; x) = H(X) which is given by the marginal probability distribution p(x), the sole quantitative
descriptor of the underlying dynamics is the MI norm (24). Paluš et al. [160] called this descriptor the coarse-grained
information rate (CIR) of the process {X(t)} and denoted by i(X).
Now, consider two time series {x(t)} and {y(t)} regarded as realizations of two processes {X(t)} and {Y(t)} which
represent two possibly linked (sub) systems. These two systems can be characterized by their respective CIRs i(X)
and i(Y). In order to characterize an interaction of the two systems, in analogy with the above CIR, Paluš et al. [160]
deﬁned their mutual coarse-grained information rate (MCIR) by
i(X, Y) =
1
2
max

max;
̸=0


=−
max
I(x; y
).
(26)
Due to the symmetry properties of I(x; y
) is the mutual CIR i(X, Y) symmetric, i.e. i(X, Y) = i(Y, X).
Assessing the direction of coupling between the two systems, i.e. causality in their evolution, we ask how is the
dynamics of one of the processes, say {X}, inﬂuenced by the other process, {Y}. For the quantitative answer to this
question, Paluš et al. [160] proposed to evaluate the conditional coarse-grained information rate CCIR i0(X|Y) of {X}
given {Y}:
i0(X|Y) =
1

max

max


=1
I(x; x
|y),
(27)
considering the usual choice 
min = 
 = 1 sample. Recalling Eq. (14), we have i0(X|Y) = i(X) for {X} independent
of {Y}, i.e. when the two systems are uncoupled. In order to have a measure which vanishes for an uncoupled system
(although then it can acquire both positive and negative values), Paluš et al. [160] deﬁne
i(X|Y) = i0(X|Y) −i(X).
(28)

10
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
For another approach to a directional information rate, let us consider the MI I(y; x
) measuring the average amount
of information contained in the process {Y} about the process {X} in its future 
 time units ahead (
-future thereafter).
This measure, however, could also contain an information about the 
-future of the process {X} contained in this process
itself if the processes {X} and {Y} are not independent, i.e. if I(x; y) > 0. In order to obtain the “net” information about
the 
-future of the process {X} contained in the process {Y}, we need the conditional MI I(y; x
|x).
Next, we sum I(y; x
|x) over 
 as above
i1(X, Y|X) =
1

max

max


=1
I(y; x
|x).
(29)
In order to obtain the “net asymmetric” information measure, we subtract the symmetric MCIR (26):
i2(X, Y|X) = i1(X, Y|X) −i(X, Y).
(30)
Using a simple manipulation, we ﬁnd that i2(X, Y|X) is equal to i(X|Y) deﬁned in Eq. (28). By using two different
ways for deﬁnition of a directional information rate, Paluš et al. [160] arrived to the same measure which they denoted
by i(X|Y) and called the coarse-grained transinformation rate (CTIR) of {X} given {Y}. It is the average rate of the
net amount of information “transferred” from the process {Y} to the process {X} or, in other words, the average rate of
the net information ﬂow by which the process {Y} inﬂuences the process {X}.
Using several numerical examples of coupled chaotic systems, Paluš et al. [160] demonstrated that the CTIR is
able to identify the coupling directionality from time series measured in coupled, but not yet fully synchronized
systems. As a practical application, CTIR was used in analyses of electroencephalograms of patients suffering from
epilepsy. Causal relations between EEG signals measured in different parts of the brain were identiﬁed. In transients
from normal brain activity to epileptic seizures, asymmetries in information ﬂow emerge or are ampliﬁed. The poten-
tial of the CTIR method for anticipating seizure onsets and for localization of epileptogenic foci was discussed in
Ref. [159]. Paluš and Stefanovska [158] adapted the conditional MI approach [160] to the analysis of instanta-
neous phases of interacting oscillators and demonstrated suitability of this approach for analyzing causality in cardio-
respiratory interaction [160].
2.4. Conditional mutual information and transfer entropy
The principal measure, used by Paluš et al. [160] for inferring causality relations, i.e. the directionality of coupling
between the processes {X(t)} and {Y(t)}, is the conditional MI I(y; x
|x) and I(x; y
|y). If the processes {X(t)} and
{Y(t)} are substituted by dynamical systems evolving in measurable spaces of dimensions m and n, respectively, the
variables x and y in I(y; x
|x) and I(x; y
|y) should be considered as n- and m-dimensional vectors. In experimental
practice, however, usually only one observable is recorded for each system. Then, instead of the original components
of the vectors ⃗X(t) and ⃗Y(t), the time delay embedding vectors according to Takens [224] are used. Then, back in
time-series representation, we have
I( ⃗Y(t); ⃗X(t + 
)| ⃗X(t))
= I((y(t), y(t −), . . . , y(t −(m −1))); x(t + 
)|(x(t), x(t −), . . . , x(t −(n −1)))),
(31)
where  and  are time lags used for the embedding of systems ⃗X(t) and ⃗Y(t), respectively. For simplicity, only
information about one component x(t + 
) in the 
-future of the system ⃗X(t) is used. The opposite CMI I( ⃗X(t); ⃗Y(t +

)| ⃗Y(t)) is deﬁned in the full analogy. Exactly the same formulation can be used for Markov processes of ﬁnite orders
m and n.
Using the idea of ﬁnite-order Markov processes, Schreiber [206] introduced a measure quantifying causal information
transfer between systems evolving in time, based on appropriately conditioned transition probabilities. Assuming that
the system under study can be approximated by a stationary Markov process of order k, the transition probabilities
describing the evolution of the system are p(in+1|in, . . . , in−k+1). If two processes I and J are independent, then the

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
11
generalized Markov property
p(in+1|in, . . . , in−k+1) = p(in+1 | i(k)
n , j(l)
n ),
(32)
holds, where i(k)
n =(in, . . . , in−k+1) and j(l)
n =(jn, . . . , jn−l+1) and l is the number of conditioning state from process J.
Schreiber proposed using the Kullback–Leibler divergence (15) to measure the deviation of the transition probabilities
from the generalized Markov property (32). This results into the deﬁnition
TJ→I =

p(in+1, i(k)
n , j(l)
n ) log p(in+1|i(k)
n , j(l)
n )
p(in+1|i(k)
n )
,
(33)
denoted as transfer entropy. The transfer entropy can be understood as the excess amount of bits that must be used
to encode the information of the state of the process by erroneously assuming that the actual transition probability
distribution function is p(in+1|i(k)
n ), instead of p(in+1|i(k), j(l)
n ).
Considering the relation between the joint and conditional probabilities, from Eq. (33) we can obtain
TJ→I =

p(in+1, i(k)
n , j(l)
n ) log
p(in+1, i(k)
n , j(l)
n )
p(in+1|i(k)
n )p(i(k)
n , j(l)
n )
,
and, after a few simple manipulations we have
TJ→I =

p(in+1, i(k)
n , j(l)
n ) log p(in+1, j(l)
n |i(k)
n )
−

p(in+1, i(k)
n ) log p(in+1|i(k)
n ) −

p(i(k)
n , j(l)
n ) log p(j(l)
n |i(k)
n ).
(34)
Now, considering Eq. (13), let us go back to the expression for conditional MI (31) and express it using conditional
entropies as
I( ⃗Y(t); ⃗X(t + 
)| ⃗X(t))
= H((y(t), y(t −), . . . , y(t −(m −1)))|(x(t), x(t −), . . . , x(t −(n −1))))
+ H(x(t + 
)|(x(t), x(t −), . . . , x(t −(n −1))))
−H((y(t), y(t −), . . . , y(t −(m −1))), x(t + 
)|(x(t), x(t −), . . . , x(t −(n −1)))).
(35)
Now, we express the conditional entropies using the probability distributions. However, let us change our notations
according to Schreiber by equating I ≡{X(t)}, m = k, and J ≡{Y(t)}, n = l, substitute t for n and set  =  = 
 = 1.
We can see that we obtain the same expression as Eq. (34) for the transfer entropy. Thus the transfer entropy is in fact
an equivalent expression for the conditional MI.
3. Basic classiﬁcation of current methods for entropy and mutual information estimation
Calculations of MI occur mainly in the literature in four contexts in the analysis of observational data: learning theory
questions, identiﬁcation of nonlinear correlation (and consequently causality detection), determination of an optimal
sampling interval and in the investigation of causal relationships concretely with directed MI.
The key problem for causality detection by means of conditional MI is to have an estimator of MI. Most entropy
estimators in the literature, which are designed for multi-dimensional spaces, can be applied to MI estimation. Therefore
this paper focuses mainly to entropy estimation in multidimensional spaces. In the following, we adopt the classiﬁcation
and mathematical criteria for evaluation of the differential entropy estimators from the overview of non-parametric
methods from Beirlant et al. [22].
The basic properties of differential entropy are summarized, e.g. in Ref. [50]. The differential entropy has some
important extremal properties:
(i) If the density f is concentrated on the unit interval [0, 1] then the differential entropy is maximal iff f is uniform
on [0, 1].

12
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
(ii) If the density is concentrated on the positive half line and has a ﬁxed expectation then the differential entropy
takes its maximum for the exponential distribution.
(iii) If the density has ﬁxed variance then the differential entropy is maximized by the Gaussian density.
3.1. Conditions and criteria
If for the identically independent distributed (i.i.d.) sample X1, . . . , Xn, Hn is an estimate of H(f ), then the following
types of consistencies can be considered:
Weak consistency: limn→∞Hn = H(f ) in probability.
Mean square consistency: limn→∞E(Hn −H(f ))2 = 0.
Strong (universal) consistency: limn→∞Hn = H(f ) a.s. (almost sure).
Slow-rate convergence: lim supn→∞
E|Hn−H|
an
=∞for any sequence of positive numbers {an} converging to zero.
Root-n consistency results are either of form of asymptotic normality, i.e. limn→∞n1/2(Hn −H(f )) = N(0, 2) in
distribution, of L2 rate of convergence: limn→∞nE(Hn −H(f ))2 = 2 or the consistency in L2, i.e. limn→∞E(Hn −
H(f ))2 = 0.
The following usual conditions on the underlying density f are:
Smoothness conditions:
(S1) f is continuous.
(S2) f is k times differentiable.
Tail conditions:
(T 1) H([X]) < ∞, where [X] is the integer part of X.
(T 2) inff (x)>0f (x) > 0.
Peak conditions:
(P 1)

f (log f )2 < ∞. (This is also a mild tail condition.)
(P 2) f is bounded.
Many probability distributions in statistics can be characterized as having maximum entropy and can be generally
characterized by Kagan–Linnik–Rao theorem [123]. When dealing with the convergence properties of the presented
estimators, one needs the following deﬁnitions. By means of Asymptotically consistent estimator one understand that
the series of the approximants converge in inﬁnity to the function to be approximated (see Ref. [22]). Asymptotically
unbiased estimator is that one which is unbiased in the limit.
3.2. Classiﬁcation of methods for entropy estimation
There is an extensive literature dealing with entropy estimates, and in their classiﬁcation we will roughly keep the
schema given by Beirlant et al. [22] and by Erdogmus [70]. We extend them to newer methods and approaches and
also to the non-parametric ones. At this point it is necessary to note that a great proportion of the literature dealing with
entropy and MI estimation was originally motivated by other questions than detection of causality: by learning theory
questions, i.e. blind separation, necessary for application of principal or independent component analysis (PCA and
ICA) or by nonlinear dynamics applications.
Many of these methods, although accurate in one or two dimension, become inapplicable in higher-dimensional
spaces (because of their computational complexity). In this review paper we focus mainly on entropy estimation
methods which are applicable in higher-dimensional spaces. The older methods (mostly adopted from Ref. [22]) will
be presented brieﬂy and the newer methods will be discussed more in detail.
4. Non-parametric estimators
4.1. Plug-in estimates
Plug-in estimates are based on a consistent density estimate fn of f such that fn depends on X1, . . . , Xn. Their name
“plug-in” was introduced by Silverman [215]. In these estimates, a consistent probability density function estimator is
substituted into the place of the pdf of a functional.

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
13
4.1.1. Integral estimates of entropy
These estimates have form given by
Hn = −

An
fn(x) log fn(x) dx,
(36)
where with the set An one typically excludes the small or tail values of fn. These estimates evaluate approximately (or
exactly) the integral. The ﬁrst such estimator was introduced by Dmitriev and Tarasenko [66], who proposed to estimate
Hn by Eq. (36) for d = 1, where An = [−bn, bn] and fn is the kernel density estimator (see Section 4.8). The strong
consistency of Hn deﬁned by formula (36) was shown in Ref. [66] and in Ref. [181]. Mokkadem [150] calculated the
expected Lr error of this estimate, also for the estimation of MI.
To evaluate the (inﬁnite) integral form of entropy (the exact or approximate one), numerical integration must be
performed, which is not easy to be computed if fn is a kernel density estimator.
Joe [121] estimated entropy H(f ) by the sequence of integral estimators Hn given by formula (36) when f is a
multivariate pdf, but he pointed out that the calculation of Hn, when fn is a kernel estimator, gets more difﬁcult for
d ⩾2. He therefore excluded the integral estimate from further study (curse of dimensionality). The integral estimator
can however be easily calculated if, for example, fn is a histogram [99].
4.1.2. Resubstitution estimates
The resubstitution estimation is of the form
Hn = −1
n
n

i=1
log fn(Xi).
(37)
This approach includes the approximation of the expectation operator (i.e. the expected value of an argument) in the
entropy deﬁnition with the sample mean or by polynomial expansions. Polynomial expansions of pdfs in order to
estimate entropy were lately applied by Van Hulle [241] who used Edgeworth expansion (see Section 5, Parametric
methods) and by Viola [251]. Ahmad and Lin [3] proposed estimating H(f ) by Eq. (37), where fn is a kernel density
estimate. They showed the mean square consistency of Eq. (37) under some mild conditions.
Joe [121] considered the estimation of H(f ) for multivariate pdf’s by Eq. (37), also based on a kernel-based estimate.
Joe obtained asymptotic bias and variance terms, and showed that non-unimodal kernels satisfying certain conditions
can reduce the mean square error. He concluded that in order to obtain accurate estimates especially in multivariate
situations, the number of samples required increased rapidly with the dimensionality d of the multivariate density (curse
of dimensionality). These results strongly rely on conditions T2 and P2.
Hall and Morton [105] investigated both the case when fn is a histogram density estimator and when it is a kernel
estimator in Eq. (37). Root-n consistency of form of asymptotic normality was proven for histogram under certain tail
and smoothness conditions with 2=Var(log f (X)). The histogram-based estimator can only be root-n consistent when
d =1 or 2. However, the estimator has in case of d =2 signiﬁcant bias. They suggest an empirical rule for the bandwidth,
using a penalty term. The effects of tail behavior, distribution smoothness and dimensionality on convergence properties
were studied with the conclusion that root-n consistency of entropy estimation requires appropriate assumptions about
each of these three features. These results are valid for a wide class of densities f having unbounded support.
4.1.3. Splitting data estimate
The approach of these methods is similar to the approach of Section 4.1.2 except that the sample set is divided into
two subsets, X1, . . . , Xl and X∗
1, . . . , X∗
m, n=l +m; one is used for density estimation, while the other one for sample
mean. Based on X1, . . . , Xl, one constructs a density estimate fl and then, using this density estimate and then the
second sample, estimates H(f ) by
Hn = −1
m
m

i=1
I[X∗
i ∈Al] log fl(X∗
i ).
(38)
This approach was used for fl being the histogram density estimate in Ref. [98], for fl being the kernel density estimate
in Ref. [99] and for fl being any L1-consistent density estimate such that [X∗
i ∈Al] = [fl(X∗
i ⩾al], 0 < al →0

14
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
in Ref. [100]. Under some mild tail and smoothness conditions of f, the strong consistency was shown for general
dimension d.
4.1.4. Cross-validation estimate
This class of estimators uses a leave-one-out principle in the resubstitution estimate. The entropy estimate is obtained
by averaging the leave-one-out resubstitution estimates of the data set. If fn,i denotes a density estimate based on
X1, . . . , Xn leaving Xi out, then the corresponding density estimate is of the form
Hn = −1
m
n

i=1
I[Xi∈An] log fn,i(Xi).
(39)
Ivanov and Rozhkova [119] proposed such an estimator for Shannon entropy when fn,i is a kernel-based pdf estimator.
They showed strong consistency, and also made a statement regarding the rate of convergence of the moments E|Hn −
H(f )|r, r ⩾1.
Hall and Morton [105] also studied entropy estimates of type (39) based on kernel estimator. For d = 1, properties
of Hn were studied in the context of Kullback–Leibler distance by Hall [103]. Under some conditions the analysis by
Hall and Morton [105] yields a root-n consistent estimate of the entropy when 1⩽d ⩽3.
4.1.5. Convergence properties of discrete plug-in estimates
Convergence properties of discrete plug-in estimators were studied by Antos and Kontoyiannis [9] in a more general
scope. They investigated a class of additive functionals where a discrete random variable is given by its distribution
{p(i); i ∈H}. The plug-in estimate for F is deﬁned by
ˆFn = g

i∈H
f (i, p(i)

,
where
pn(i) = 1
n
n

j=1
I{Xj =i}
is the empirical distribution induced by the samples (X1, . . . , Xn) on H. In other words, ˆFn =F(pn). It is assumed that
f and g are arbitrary real-valued functions with the only restriction that f is always non-negative. For additive functionals,
including the cases of the mean, entropy, Rényi entropy and MI, satisfying some mild conditions, the plug-in estimates
of F were shown to be universally consistent and consistent in L2. The L2-error of the plug-in estimate is of order
O( 1
n). In other words, in the case of discrete estimators, the convergence results obtained by Antos and Kontoyiannis
[9] are in agreement with the convergence results of the all above-mentioned plug-in methods but they were done in
general for additive functionals among which all plug-in methods belong (under some mild conditions).
On the other hand, for a wide class of other functionals, including entropy, it was shown that the universal convergence
rates cannot be obtained for any sequence of estimators. Therefore, for positive rate-of-convergence results, additional
conditions need to be placed on the class of considered distributions.
It was shown in Ref. [9] that there is no universal rate at which the error goes to zero, no matter what estimator we
select, even when our sample space is discrete (albeit inﬁnite). Given any such assumed rate aN, we can always ﬁnd
some distribution P for which the true rate of convergence is inﬁnitely slower than aN. Antos and Kontoyiannis [9]
proved identical theorems for the MI, as well as a few other functionals of P.
4.2. Estimates of entropy based on partitioning of the observation space
This popular class of estimators divides the observation space into a set of partitions. The methods belonging to
this class can be classiﬁed according to the number of features. The partition is generated either directly or recursively
(iteratively). The algorithms employ a ﬁxed scheme independent of the data distribution or an adaptive scheme which
takes the actual distribution of the data into account. In the following, algorithms employing ﬁxed schemes as well as
algorithms using adaptive schemes are presented.

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
15
4.2.1. Fixed partitioning of the observation space
Consider a pair of random variables x and y with values in the measurable spaces X and Y, respectively. Recalling
deﬁnitions (6), (9) and (12), their MI is
I(X, Y) =
mX

i=1
mY

j=1
p(xi, yj) log p(xi, yj)
p(xi)p(yj).
(40)
Classical histogram methods: One of the most straightforward and widely used non-parametric approach to estimate
(40) is approximation of the probability distributions p(xi, yj), p(xi) and p(yj) by a histogram estimation [43]. The
range of a variable, say x, is partitioned into mX discrete bins ai ∈A, each with width hX. Let ki denote the number
of measurements which lie in the bin ai. The probability p(xi) is approximated by relative frequencies of occurrence
pX(ai) = ki/N, where N is the size (the number of points) of the dataset. Analogously, we estimate the probability
p(yi) using elements bj of the width hy belonging to the partition B as pY (bj) = kj/N.
The joint probability p(xi, yj) is then approximated using the product partition A × B: pX,Y (ai × bj) = ki,j/N.
Then the estimator of the MI is
I(X, Y) = log N + (1/N)
mX

i=1
mY

j=1
ki,j log ki,j
kikj
,
(41)
where ki,j is the number of measurements for which x lies in ai and y in bj. This method is also referred to as equidistant
binning, as all the bins of the histogram have the same size (Fig. 1).
It can be demonstrated [221] that the estimate of MI given by Eq. (41) ﬂuctuates around the true value or gets
systematically overestimated. Moreover, these methods fail in higher dimensions and work well only for two or three
scalars.An insufﬁcient amount of data, occurring especially in higher dimensions, leads to a limited occupancy of many
histogram bins giving incorrect estimations of the probability distributions and consequently leads to heavily biased,
usually overestimated values of MI.
The accuracy of the histogram entropy estimator is closely related to the histogram problem: given a scalar dataset
X, how many elements should be used to construct a histogram of X? The histogram problem has a long history and has
been examined by several investigators. A systematic theoretic development of the question is given by Rissanen et al.
[194], who use a minimum description length argument to conclude that the optimal value of the number of elements to
use in a histogram is the value that gives a minimum value of the stochastic complexity. As a representative application
of histogram methods to MI estimation we mention here Moddemeijer [148] (using a simple histogram-based method
in a procedure to estimate time-delays between recordings of electroencephalogram (EEG) signals originating from
epileptic animals or patients) or the work from Knuth et al. [133] who introduced so-called optimal binning techniques,
developed for piecewise-constant, histogram-style models of the underlying density functions.
Generalized binning with B-splines: Daub et al. [56] developed a method for estimating multidimensional entropies
using B-splines. In classical histogram approaches to MI estimation, data points close to bin boundaries can cross over
to a neighboring bin due to noise or ﬂuctuations, and in this way they introduce additional variance into the computed
estimate. Even for sets of moderate size, this variance is not negligible. To overcome this problem, Daub et al. [56]
proposed a generalized histogram method, which uses B-spline functions to assign data points to bins. The sample
Fig. 1. An example of equidistant binning (the method from Butte and Kohane)—the bins have the same size.

16
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Membership value [-]
Sample [-]
B-Spline of order 3 with 4 bins
Fig. 2. Generalized binning with B-splines from Daub et al.—an example of B-splines of order 3 for four bins.
space is divided into equally sized bins as in equidistant binning. The major difference between the classical histogram
methods and the generalized binning is that a data point is assigned to multiple bins simultaneously with weights given
by B-spline functions which are implicitly normalized. The shape of the B-spline functions is determined by their order
k, which is a parameter of the method. As an example, B-splines of order 3 are shown for 4 bins in Fig. 2.
When B-splines of order 1 are selected, each point is assigned to one bin only and the method is equivalent to
simple equidistant binning. The proposed method is therefore a ﬁxed binning scheme extended by a preprocessing
step to reduce the variance. This strategy also somewhat alleviates the choice-of-origin problem of classical histogram
methods by smoothing the effect of transition of data points between bins due to shifts in origin.
The probability p(ai) of each bin is estimated by
ˆp(ai) = 1
N
N

j=1
Bi,k(˜xj),
(42)
where Bi,k is a B-spline function of order k evaluated at bin i; ˜xj is an appropriately scaled data sample mapping the
values of x into the domain of the B-spline functions [56,58]. In two dimensions the joint pdf is computed as
ˆp(ai, bj) = 1
N
N

l=1
Bi,k(˜xl) × Bj,k( ˜yl).
(43)
The MI IM,k(X; Y) can then be estimated from
IM,k(X; Y) = HM,k(X) + HM,k(Y) −HM,k(X, Y)
(44)
and each of the terms may be computed using the standard formulas applied to probabilities (42), (43). The notation
IM,k(X; Y) and HM,k(X, Y) indicates that the method has two parameters: M, the number of bins and k, the order of
the B-spline. The procedure can be theoretically extended to a higher number of dimensions, but the performance of
this estimator in the multidimensional case has not been systematically studied.
Consistency or other properties in the framework of Section 3.1 are not known. Daub et al. [56] gave numerical
estimates of bias and variance with dataset size of N for the estimator IM,k(X; Y) for statistically independent datasets
and for k = 3. The IM,3 estimator was found to have bias scaling as ∼1/N but the slope was signiﬁcantly lower than
for the classical histogram method (equivalent to IM,1). The same is true for the standard deviation which also scaled
as 1/N, but with a signiﬁcantly lower slope than IM,1.
Daub et al. compared their estimator to the estimator ˆHBUB also using binning, introduced by Paninski [167] (more
details in Section 4.6) for independent datasets. The scaling behavior of the bias was found to be similar. The standard

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
17
deviation of their algorithm is however lower than that of ˆHBUB. The estimator from Daub et al. [56] was also compared
to the kernel density estimator KDE, i.e. [151,215,221].
4.2.2. Adaptive partitioning of the observation space
Marginal equiquantization: Any method for computation of MI based on partitioning of data space is always con-
nected with the problem of quantization. By the quantization we understand a deﬁnition of ﬁnite-size boxes covering the
state (data) space. The probability distribution is then estimated as relative frequencies of the occurrence of data sam-
ples in particular boxes (the histogram approach described above). A naive approach to estimate the MI of continuous
variables would be to use the ﬁnest possible quantization, e.g. given by a computer memory or measurement precision.
One must however keep in mind that a ﬁnite number N of data samples is available. Hence, using a quantization that is
too ﬁne, the estimation of entropies and MI can be heavily biased: estimating the joint entropy of n variables using q
marginal bins one obtains qn boxes covering the state space. If the value qn approaches the number N of data samples,
or even qn > N, the estimate of H(X1, . . . , Xn) can be equal to log N, or, in any case, it can be determined more by
the number of data samples and/or by a number of distinct data values than by a structure in the data, i.e. by properties
of the system under study. In such a case we say that the data are overquantized. Even a “natural” quantization of
experimental data given by an A/D (analog-to-digital) converter can be too ﬁne for reliable estimation of the MI from
limited number of samples.
Emergence of overquantization is given by the number of boxes covering the state space, i.e. the higher the space
dimension (the number of variables), the lower the number of marginal quantization levels that can cause the overquan-
tization. Recalling the deﬁnition of MI by formula (12), one can see that while the estimate of the joint entropy can be
overquantized, i.e. saturated on a value given by the number of the data samples and/or by the number of distinct data
values, the estimates of the individual (marginal) entropies are not and they increase with ﬁning the quantization. Thus
the overquantization causes an overestimation of the MI and in the case of the lagged MI, it obscures its dependence
on the lag 
 [163,161].
As a simple data adaptive partitioning method, Paluš [164,161,163] used a simple box-counting method with marginal
equiquantization. It means that the marginal boxes are not deﬁned equidistantly but so that there is approximately the
same number of data points in each marginal bin. The choice of the number of bins is, however, crucial. An example
of an equiquantized observation space is in Fig. 3.
In Ref. [161] Paluš proposed that computing the MI I n of n variables, the number of marginal bins should not exceed
the (n + 1)st root of the number of the data samples, i.e. q ⩽
n+1√
N.
The equiquantization method effectively transforms each variable (in one dimension) into a uniform distribution, i.e.
the individual (marginal) entropies are maximized and the MI is fully determined by the value of the joint entropy of the
studied variable. This type of MI estimate, even in its coarse-grained version, is invariant against any monotonous (and
nonlinear) transformation of the data [165]. Due to this property, the MI, estimated using the marginal equiquantization
Fig. 3. An example of marginally equiquantized binning method from Paluš—the bins can have different size but contain the same number of data
points in their marginals.

18
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
method, is useful for quantifying dependence structures in data as well as for statistical tests for nonlinearity which are
robust against static nonlinear transformations of the data [161].
Equiprobable binning was recently used also by Celluci et al. [44], however, the number of bins is determined using
the minimum description length criterion. It is proposed in their work that calculation of MI should be statistically
validated by application of a 2 test of the null hypothesis of statistical independence. Additionally, the partition of the
XY plane, which is used to calculate the joint probability distribution PXY , should satisfy the Cochran criterion on the
expectancies EXY [44]. A procedure for a non-uniform XY partition is proposed which reduced sensitivity to outlying
values of X and Y and provides an approximation of the highest partition resolution consistent with the expectation
criterion.
Celluci et al. compare this simple algorithm, adaptive in one dimension, with the locally data adaptive approach of
Fraser and Swinney [78] which is technically also equivalent to the Darbellay–Vajda algorithm [54,55] (more details
in the following section). The latter approach is probably the method with the smallest bias provided the unlimited
amount of data. Using the limited number of samples (less than 104 samples), this algorithm introduced false structures
and the simple marginal equiquantization method gives better results, not to speak about the CPU time used (see the
comparison done by Celluci et al. [44]).
It should be noted that while Fraser and Swinney algorithm uses a 2 criterion to control subdivisions of the XY plane
locally, it does not, in contrast to the algorithm proposed by [44], provide a global statistical assessment of an I(X, Y)
calculation that includes the probability of the null hypothesis of statistical independence.
Adaptive partitioning in two (and more) dimensions: Darbellay and Vajda [54,55] demonstrated that MI can be
approximated arbitrarily closely in probability (i.e. the weak consistency was proven) by calculating relative frequencies
on appropriate partitions and achieving conditional independence on the rectangles of which the partitions are made.
This method was experimentally compared to MLEs (see Section 4.6). The partitioning scheme used by Darbellay and
Vajda [54,55] (described below) was originally proposed by Fraser and Swinney [78,79] and in physical literature is
referred to as the Fraser–Swinney algorithm. Darbellay and Vajda [54,55] proved the weak consistency of this estimate
and tested the method on a number of probability distributions. In the mathematical and information-theoretic literature
the method has recently been referred to as the Darbellay–Vajda algorithm.
The consistency proof of Darbellay and Vajda [54,55] starts from the following deﬁnition of ME due to
Dobrushin [67]:
I(Xa, Xb) ≡
sup
{Ai}{Bi}

i,j
PX(Ai × Bj) log
PX(Ai × Bj)
PXa(Ai)PXb(Bj),
(45)
where Xa, Xb are random vectors with values in Rda, Rdb, respectively.
PXa(Ai)PXb(Bj)=(PXa ×PXb)(A×B) is the product measure deﬁned as the probability measure for A, B elements
of the respective -algebras of Rda and Rdb.
The supremum in Eq. (45) is taken over all ﬁnite partitions a = {Ai|i ∈1, . . . , m} of Rda and all ﬁnite partitions
b = {Bj|j ∈1, . . . , n} of Rdb.
A ﬁnite partition of a set X is any ﬁnite system of sets  = {Ck|k ∈1, . . . , q} which satisﬁes Ci ∩Cj = ∅for i ̸= j
and q
k=1Ck = X. Each set Ck is called a cell of the partition . A partition  = {Dl|l = 1, . . . , r} is a reﬁnement of
the partition  if for each Dl ∈ there exists Ck ∈ such that Dl ⊂Ck.
Darbellay [55] notes that the sequence of numbers
D ≡

k
PX(Ck) log
PX(Ck)
PXa×Xn(Ck)
(46)
never decreases as the partition  is made successively ﬁner and ﬁner. An important fact with respect to the developed
algorithm is that if  is a reﬁnement of the partition  such that Ck ∈ = 
lDk,l ∈ for some set of indices l
depending on k then
D = D
⇔
PX(Dk,l)
PXa×Xb(Dk,l) =
PX(Ck)
PXa×Xb(Ck) ∀k, l.
(47)

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
19
Fig. 4. An example of a partition that can arise from the splitting procedure deﬁned by the Darbellay and Vajda algorithm.
This means that the random vectors Xa, Xb must be conditionally independent if they attain values in the cell Ck. If
this is true for all cells Ck, then the MI I(Xa, Xb) can be estimated as D.
The algorithm works with d-dimensional hyperrectangles. To split a given cell, each of its d edges is split into ⩾2
equiprobable intervals (marginal equiquantization). At every partitioning step, a cell is split into d subcells. Initially,
the entire space Rd is one cell. The algorithm follows by ﬁrst checking the condition on the right-hand side of Eq. (47)
for each cell Ck and if the cell does not satisfy the condition, then it is split by marginal equiquantization. The parameter
 is usually set to 2 since the recursive nature of the algorithm allows further splitting of regions where conditional
independence is not achieved. Fig. 4 illustrates how such a partition might look like.
It is advantageous to combine all the conditional independence tests (47) into one statistic. Here the 2({Dk,l})
statistic is used. To increase robustness of the test, when testing for conditional independence, the splitting can be done
at multiple levels of reﬁnement t = 1, 2, . . . , . This means that the cell Ck is broken into td cells for each t. A higher
value of  prevents the algorithm from stopping the partitioning process too early, while a value of  too high might
force the splitting process to continue until there is a very small number of points in each partition. The choice of 
also depends on the number of points available and the depth of the cell. When testing the algorithms numerically, 
was set to 2 for partitioning depth up to 3 and then to 1 for deeper cells if the problem was more than two-dimensional.
For two-dimensional problems the authors used  = 2. This setup provides some guidelines for selecting the values of
 and .
The estimator was tested on correlated Gaussians, where another estimate of the MI is available via a MLE. The
recursive space partitioning estimator appears to be asymptotically unbiased and numerical tests also suggest that
it is
√
N-consistent for a large class of distributions. The estimator is not universally consistent since the examples
of distributions, where the estimator does not converge, are known. These distributions are however rather ‘exotic’
(e.g. exhibiting some symmetries, which prevent the conditional independence test to succeed). Asymptotically, the
estimates of MI ˆI have a normal distribution, except for very low values of MI.
4.3. Ranking
Pompe [179] proposed an estimator of dependencies of a time series based on second-order Rényi entropy (more
details on Rényi entropy in Section 4.5.1). In general, Rényi entropy does not possess the attractive properties of Shannon
entropy, such as non-negativity and it is not possible to infer independence with vanishing Rényi entropy. However,
Pompe noticed that if the time series is uniformly distributed, some of the desirable properties of Shannon entropy
can be preserved for the second-order Rényi entropy. Moreover, the second-order Rényi entropy can be effectively
estimated using the Grassberger–Procaccia–[Takens Algorithm (GPTA) [94,225].
Consider a stationary discrete time series {Xt} such that Xt attains one of k different values x(n), n ∈1, 2, . . . , k.
The statistical dependency between a d-dimensional vector of ‘past’ values ⃗Xt = (Xt−d−1, . . . , Xt−0) and one
‘future’value Xt+
 is examined, d is 1, 2, 3, . . . and d−1 > d−2 > · · · > 0=0 and also 
⩾0. The joint probabilities

20
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
are denoted as
pmd−1,...,m0,n(
) = p{Xt−d−1 = x(m0), . . . , Xt−0 = x(m0), Xt+
 = x(n)},
(48)
where each of the indices mi, i ∈{d −1, . . . , 0} and n are in 1, 2, . . . , k. The vector of values (md−1, . . . , m0) is
hereafter denoted as ⃗m. Using the above notation, Pompe deﬁnes the contingency
2
d(
) ≡
k

⃗m,n=1
[p ⃗m,n(
) −p ⃗mpn]2
pmd−1 . . . pm0pn
.
(49)
The key point is the assumption that Xt is uniformly distributed. All the probabilities thus satisfy pmd−1 = · · · = pm0 =
pn = 1/k = ϵ and it is possible to rewrite the equation for contingency
2
d(
) ≡ϵ−(d−1)
k

⃗m,n=1
p2
⃗m,n(
) −ϵ−d
k

⃗m,n
p2
⃗m.
(50)
The contingency 2
d(
) can be related to the generalized mutual information
I (2)
d (
) ≡H (2)
1
+ H (2)
d
−H (2)
d+1(
)
(51)
with the formula
I (2)
d (
) = log

2
D(
)
ϵ−d
⃗m=1 p2
⃗m
+ 1

.
(52)
From Eq. (52) it can be seen that I (2)
d (
)⩾0 because 2
d(
)⩾0 and the argument of the logarithm is always ⩾1. It was
also noted in Ref. [179] that I (2)
d (
) = 0 iff ⃗Xt and ⃗Xt+
 are independent, i.e. it is true that p ⃗m,n(
) = p ⃗mpn for all
combinations ⃗m, n. It is further proven that
0⩽I (2)
d (
)⩽H (2)
1
= log k = −log ϵ,
(53)
always assuming the uniform distribution of data.
The transformation of an arbitrarily distributed time series to a uniform distribution is accomplished by sorting the
samples using some common fast sorting algorithm such as quicksort or heapsort and replacing each sample by its
rank in the sorted sequence. If the generating process Yt is continuous then the above transformation function would
be equivalent to the distribution function of Yt. A side-effect is that the estimation of I (2)
d
in this way is insensitive
to nonlinear invertible distortions of the original signal. After the transformation, Pompe recommends estimating the
generalized mutual information using the GPTA algorithm as
I (2)
d (
) ≃log Cd+1,ϵ/2(
)
Cd,ϵ/2C1,ϵ/2
for ϵ →0,
(54)
where CX, represents the correlation integral [94,108,106] with the appropriate time-delay embedding ⃗Xt for Cd,ϵ/2,
( ⃗Xt, Xt+
) for Cd+1,ϵ/2, Xt+
 for C1,ϵ/2 and the neighborhood size .
The above considerations are however not entirely applicable to the quantized and sampled time-series, as the
possibility of two different samples coinciding is non-zero. In fact this commonly occurs in practice. Under these
conditions it is not possible to obtain a unique ranking of the original sequence as equal samples can be arbitrarily
interchanged. Pompe suggests using a neighborhood size equal to at least maxϵq, where ϵq represents the relative
quantization error and max is the maximum value of the one-dimensional distribution density of the original data. In
practice max would be estimated as leq,max/T where leq,max is the maximal number of equal data samples in the series.
By adhering to this policy, problems stemming from the non-uniqueness of the samples are circumvented.

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
21
4.4. Estimates of entropy and mutual information based on computing distances
4.4.1. Based on sample spacings
These methods are deﬁned only for d = 1 and their generalization to multivariate cases is not trivial. Let X1, . . . , X2
be a sample of i.i.d. real-valued random values and let Xn,1⩽Xn,2 ⩽. . . , Xn,n be the corresponding order statistics.
Then Xm,i+m −Xn,i is called m-spacing (1⩽i ⩽i + m⩽n). Based on spacings, it is possible to construct a density
estimate:
fn(x) = m
n
1
Xn,im −Xn,(i−1)m
(55)
if x ∈[Xn,(i−1)m, Xn,im). This density estimate is consistent if for n →∞hold
mn →∞, mn/n →0.
(56)
The estimate of entropy based on sample spacings can be derived as a plug-in integral estimate or resubstitution estimate
using a spacing density estimate. Surprisingly, although the m-spacing density estimates might not be consistent, their
corresponding m-spacing entropy estimates might turn out to be (weakly) consistent [101].
(i) m-spacing estimate for ﬁxed m has the form
Hm,n = 1
n
n−m

i=1
log
	 n
m(Xn,i+m −Xn,i)

−(m) + log m,
(57)
where (x) = −(log (x))′ is the digamma function. Then the corresponding density estimate is not consistent.
This implies that in Eq. (57) there is an additional term correcting the asymptotic bias. For uniform f the consistency
of Eq. (57) was proven by Tarasenko [227] and by Beirlant and van Zuijlen [20]. Hall proved the weak consistency
of Eq. (57) for densities satisfying T2 and P2 [101]. The asymptotic normality of Hm,n was studied under the
conditions T2 and P2 in Refs. [53,68,101,21], who all proved the asymptotic normality under T2 and P2 with
2 = (2m2 −2m + 1)′(m) −2m + 1 + Var(log f (X)),
(58)
which for m = 1 gives
2 = 2
6 −1 + Var(ln f (X)).
(ii) mn-spacing estimate with mn →∞
This case is considered in the papers of Vašíˇcek [243], Dudewitz and van der Meulen [68], Beirlant and van Zuijlen
[20], and of Beirlant [21], Hall [106], van Es [239]. In these papers, the weak and strong consistencies are proven
under condition (56). Consistencies for densities with unbounded support is proved only in Tarasenko [227] and
in Beirlant [20]. Hall [106], van Es [239] proved asymptotic normality with 2 = Var(log f (X)) if f is not uniform
but satisﬁes T2 and P2. Hall [106] showed this result also for the non-consistent choice of Mn/n → if  is
irrational. This asymptotic variance is the smallest one for an entropy estimator if f is not uniform. If f is uniform
on [0, 1] then Dudewitz and van Meulen [68] and van Es [239] showed, respectively, for mn = o(n1/3−),  > 0,
and for mn = o(n1/3) that
lim
n→∞(mn)1/2( ˆHn −H(f )) = N(0, 1/3)
(59)
for slight modiﬁcations ˆHn of the mm-spacing estimate Hn. (Since sample spacings are deﬁned only in one
dimension, for our application are not these methods suitable. The generalization of these estimates is in higher
dimension non-trivial.)

22
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
4.4.2. Based on nearest neighbor search
Estimators of Shannon entropy based on k-nearest neighbor search in one dimensional spaces were studied in
statistics already almost 50 years ago by Dobrushin [67] and by Vašíˇcek [243], but they cannot be directly generalized
to higher-dimensional spaces (and therefore not applied to MI).
For general multivariate densities, the nearest-neighbor entropy estimate is deﬁned as the sample average of the
algorithms of the normalized nearest-neighbor distances plus the Euler constant. More precisely, let n,i be the nearest-
neighbor distance of Xi and the other Xj : n,i = minj̸=i,j ⩽n∥Xi −Xj∥. Then the nearest neighbor entropy estimate
is deﬁned as
Hn = 1
n
n

i=1
log(nn,i) + log 2 + CE,
(60)
where CE is the Euler constant CE=−
 ∞
0
e−t log t dt. Under the condition (P1) introduced in Section 3.1, Kozachenko
and Leonenko [135] proved the mean square consistency for general d ⩾1. Tsybakov and van der Meulen [237] showed
root-n rate of convergence for a truncated version of Hn when d =1 for a class of densities with unbounded support and
exponential decreasing tails, such as the Gaussian density. Bickel and Breiman [32] considered estimating a general
functional of density. Under general conditions on f they proved asymptotic normality. Their study unfortunately
excludes the entropy.
We will describe here more in detail two nearest-neighbor entropy estimators: KL introduced by Kozachenko and
Leonenko [135] and its theoretical analysis and then its improved modiﬁcation from Kraskov et al. [136].
Victor [248] applied the KL estimator and claimed that the algorithm was dramatically more efﬁcient than standard
bin-based approaches, such as the direct method from Strong at al. [222] for amounts of data typically available from
laboratory experiments.
The KL estimator: For simplicity reasons, we describe the estimators in R2. The idea is to rank, for each point
zi = (xi, yi) ∈R2 its neighbors by distance di,j = ∥zi −zj∥: di,j1 ⩽di,j2 ⩽· · · (supposing ∥.∥be a metrics) and than
to estimate H(X) from the average distance to the k-nearest neighbor, averaged over all xi.
Shannon entropy H(X) = −

dx(x) log (x) can be understood as an average of log (x). Having an unbiased
estimator 
log (x) of log (x), one would get an unbiased estimator 
H(X) = −1
N
N
i=1

log (xi). In order to estimate

log (xi), the probability distribution Pk(ϵ) is considered for the distance between xi and its k-th nearest neighbor. The
probability Pk(ϵ)dϵ can be derived from the trinomial formula
Pk(ϵ) = k

N −1
k

dpi(ϵ)/dϵpk−1
i
(1 −pi)N−k−1.
(61)
The expectation value of log pi(ϵ) can be from Pk(ϵ) derived as E(log pi) = (k) −(N), where (x) is the digamma
function (i.e. logarithmic derivative of the gamma function, see Ref. [2]). The expectation is taken over the positions
of all other N −1 points, xi is kept ﬁxed. An estimator for log (x) is then obtained by assuming that (x) is
constant in the whole ϵ > 0 ball. This gives pi(ϵ) ≈cdϵd(xi), where d is the dimension of x and cd is the volume
of the d-dimensional unit ball. (For the maximum norm cd = 1, for the Euclidean cd = d/2/(1 + d/2)/2d). Then
log (xi) ≈(k) −(N) −dE(log ϵ) −log cd, which leads to
ˆH(X) = −(k) + (N) + log(cd) + d
N
N

i=1
log ϵ(i),
(62)
where ϵ(i) is twice the distance from xi to its k-th nearest neighbor. In most investigated cases (including Gaussian
and uniform densities in bounded domains with a sharp cutoff) the approximation error is approximately k/N or
k/N log(N/k) [136].
MI can be obtained by estimating H(X), H(Y), and H(X, Y) separately and by applying formula (12). But in this
way, the errors made in the individual estimates would not have to cancel (see also the discussion below).
Leonenko et al. [139] studied a class of k-nearest-neighbor-based Rényi estimators for multidimensional densities
(as we have already mentioned above, Shannon entropy is Rényi entropy for q = 1). They investigated theoretically
a class of estimators of the Rényi and Tsallis (also called Havrda–Charvát, see Ref. [51]) entropies of an unknown
multidimensional distribution based on the k-nearest distances in a sample of independent identically distributed

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
23
vectors. It was shown that Rényi entropy of any order can be estimated consistently with minimal assumptions on
the probability density. For Shannon entropy, and for any k > 0 integer, the expected value of the k-nearest neighbor
estimator (including both versions of KSG algorithm of the MI estimators I (1,2) described below) converges with the
increasing size of data set N to inﬁnity to the entropy of f if f is a bounded function (asymptotical unbiasedness). For
any k > 0 integer, the k-nearest neighbor estimator converges for the Euclidean metric (L2 rate of convergence), with
the increasing size of data set N to inﬁnity, to the entropy of f if f is a bounded function (consistency). These statements
in a more general form for Rényi and Tsallis entropies were proven in Ref. [139]. Kullback–Leibler distance (KLD)
of two functions in d-dimensional Euclidean space was also examined and for its nearest-neighbor estimator similarly
proven to be asymptotically unbiased and consistent. A central limit theorem for functions h of the nearest-neighbor
method was proven by Bickel and Breiman (for k = 1 in Ref. [32]) and Penrose (for k > 1 in Ref. [172]) but only under
the condition that the nearest-neighbor estimator computing entropy is bounded (nearest-neighbor estimators of Rényi
entropies, including Shannon entropy are not in general bounded). At present, neither exact nor asymptotic results on
the distribution of the k-nearest neighbor entropy estimator are known. Goria et al. [87] presented a simulation study
showing that for many distribution families used in statistics, the hypothesis of asymptotic normal distribution of the
nearest neighbor estimator seems to be acceptable (for Beta, Cauchy, Gamma, Laplace and Student t-distributions).
It was shown that by increasing of parameter k, one can inﬂuence the approximation precision in higher-dimensional
spaces (the estimator with bigger k was much more accurate as for k = 1). Similarly, by setting k = kN, the precision
can be inﬂuenced for increasing N [139]. For the KLD, three various nearest-neighbor estimators were tested on one-
dimensional function f given by 10,000 data points generated by a Student distribution with 5 degrees of freedom
(t5). All the three tested estimators converged to t5. Although the asymptotical unbiasedness and consistency of the
estimators was proven for multidimensional spaces, up to our best knowledge, any functional relationship of k with
respect to the approximation error of the estimate is not known.
The KSG estimators: The following algorithm is from Kraskov, Stögbauer and Grassberger (KSG) [136]. Assume
Z = (X, Y) to be the joint random variable with maximum norm. The estimator differs from formula (62) that vector
x is replaced by z, dimension d is replaced by dZ = dX + dY and volume cd by cdXcdY . We get
ˆH(X, Y) = −(k) + (N) + log cdXcdY −((dX + dY )/N)
N

i=1
log ϵ(i),
(63)
where ϵ(i) is twice the distance from xi to its k-th neighbor. Now formula (12) for MI can be applied for the same k. In
this way, the different distance scales would be effectively used in the joint and marginal spaces. However, the biases
of formula (62) resulting from the non-uniformity of the density would be different for the estimates H(X), H(Y)
and H(X, Y) and would not cancel. To avoid this, Kraskov et al. recommend not to use ﬁxed k for marginal entropy
estimation.Twoestimatorsareproposed.Assumethatthek-thneighborofxi isononeoftheverticalsidesofthesquareof
size ϵ(i) (in two dimensions). Then if there are altogether nx(i) points within the vertical lines (xi −ϵ(i)/2, xi +ϵ(i)/2),
then ϵ(i)/2 is the distance to the (nx(i) + 1)th neighbor of and xi and
ˆH(X) = −1
N

[nx(i) + 1] + (N) + log cdX + dX
N
N

i=1
log ϵ(i).
(64)
For the coordinate Y, this is not exactly true, i.e. ϵ(i) is not exactly equal twice the distance to the (ny(i)+1)th neighbor
if ny(i) is analogously deﬁned as the number of points in (yi −ϵ(i)/2, yi +ϵ(i)/2). The ﬁrst estimator uses hyper-cubes
in the joint space and is given in dimension d by
I (1)(X1, . . . , Xd) = (k) −(d −1)(N) −⟨(nx1) + · · · + (nxd)⟩
(65)
where ⟨· · ·⟩=(1/N)N
i=1 E[. . . (i)] and nxi is the number of points xj so that ∥xj −xj∥< ϵ(i)/2. The second estimate
uses hyper-rectangles and is given in dimension d by
I (2)(X1, . . . , Xd) = (k) −d −1
k
+ (d −1)(N) −⟨(nx1) + · · · + (nxd)⟩.
(66)
More details can be found in Ref. [136]. Both estimators (for k = 1) for correlated Gaussian distributions give approx-
imately the same results, only in very high dimensions gives I (2) better results because ϵ(i) tends to be much larger

24
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
a
b
c
Fig. 5. Illustration of the ϵ-neighborhoods for KSG estimators I(1) and I(2) from Kraskov et al. Detailed description is given in the text. Used with
permission.
Fig. 6. Convergence of the KSG estimator from Kraskov et al. for Gaussians with given correlation coefﬁcient. Used with permission.
than the marginal ϵxj (i). Some hints for selection of parameter k, inﬂuencing the precision of approximation, can be
found in Ref. [136]. Both estimators appeared in the experiments adaptive (i.e. the resolution is higher where data are
more numerous) and had minimal bias.
Fig. 5(a) shows how ϵ(i), nx(i), and ny(i) are determined in the ﬁrst algorithm (I (1)), for k = 1 and some ﬁxed i. In
this case, nx(i) = 5 and ny(i) = 3. The two bottom images of Fig. 5 show how to ﬁnd ϵx(i), ϵy(i), nx(i), ny(i) in the
second algorithm (I (2)) for k =2. The left image (b) indicates the case where the above are determined by a single point
and the right image (c) depicts a situation where two different points inﬂuence the values ϵx(i), ϵy(i), nx(i), ny(i).
KSG has the following important property. Numerically, both estimators become exact for independent distributions,
i.e. the estimator error vanishes (up to statistical ﬂuctuations) if (x, y) = (x)(y) as conﬁrmed by experiments. The
results for correlated Gaussians are shown in Fig. 6.This holds for all tested marginal distributions and for all dimensions
of x and y (see below). Many points in a large set may have identical coordinates. In that case, the numbers nx(i) and ny(i)

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
25
need no longer be unique (the assumption of continuously distributed points is violated). The nearest-neighbor counting
can lead to wrong results. [136] solved this by adding very low-amplitude noise to the data. The k-nearest neighbor
search in KSG is done by so called Box-assisted algorithm from Grassberger [93]. This algorithm is recommended to be
used with KSG in lower dimensions (up to 3), while k–d trie (a data representation structure similar to k–d trees) showed
up to be considerably more advantageous in higher dimensional spaces (Vejmelka and Hlaváˇcková-Schindler [244]).
The estimators were applied to assess the actual independence of components obtained from independent component
analysis (ICA), to improve ICA and to estimate blind source separation. Rossi et al. [195] applied the KSG estimator
of MI in higher-dimensional spaces to selection of relevant variables in spectrometric nonlinear modeling, another
application is from Sorjamaa et al. [219].
The KSG method was experimentally compared to the adaptive partitioning method from Darbellay and Vajda [54]
and was slower. On the other hand, MI estimated by KSG estimates I (1,2), worked for more non-Gaussian general
distributions, where the adaptive partitioning method failed. KSG and Edgeworth expansion method (for more details
about Edgeworth expansion, see Section 5.2.2) for entropy and MI of Gaussian distributions were experimentally
compared in Ref. [241]. One can see that the Edgeworth expansion has an advantage for Gaussian distributions or
distributions “close” to Gaussian, since the error is caused only by the cumulants. On the other hand, the parameter k
gives ﬂexibility both to the KL and KSG estimator to widen its approximation ability to more general distributions.
4.5. Estimates based on learning theory methods
4.5.1. Motivated by signal processing problems
Entropy and MI are often used as a criterion in learning theory. For example, classiﬁcation and pattern recognition
literature uses it them for feature extraction, i.e. Torkkola [233] or for principal or independent component analysis, i.e.
Xu et al. [254,184,185]. Entropy as a measure of dispersion is applied in many other areas, in control, search, or in the
area of neural networks and supervised learning, i.e. Refs. [174,186,70,71,205,251]. Many of the developed methods
belong as well to non-parametric plug-in estimators.
Learning theory is interested in computationally simpler entropy estimators which are continuous and differentiable
in terms of the samples, since the main objective is not to estimate the entropy itself but to use this estimate in optimizing
the parameters of an adaptive (learning) system. The consistency properties of an estimator are not questioned strictly
in this ﬁeld since for relatively small data sets it is not critical to have a consistent or an inconsistent estimate of the
entropy as long as the global optimum lies at the desired solution. Since these methods work in general also in higher-
dimensional spaces (and therefore can be applicable to MI), they deﬁnitely deserve our attention. From this variety of
learning theory applications, we mention here the non-parametric estimator of Rényi entropy from Erdogmus [70] and
some neural network-based approaches. Concerning the former estimator, we will ﬁrst explain the Parzen estimation
and then the properties of Parzen estimation of Rényi entropy, including Rényi divergence, MI and their estimators.
Quadratic Rényi entropy estimator: The non-parametric estimator for Rényi quadratic entropy introduced by Principe
and Erdogmus [70] uses Parzen windowing with Gaussian kernels in the following manner. Let the (continuous)
quadratic entropy be given by
H2(X) = −log
 ∞
−∞
f 2
X(x) dx.
(67)
Let xi, . . . , xN are identically distributed samples of the random variable X.
The Parzen (Window) estimate [170] of the pdf using an arbitrary kernel function (.) is given by
ˆfX(x) = 1
N
N

i=1
(x −xi),
(68)
where the kernel function  is a valid pdf in general and is continuous and differentiable. If Gaussian kernels G(.)
with standard deviation 
(y) = G(y, 2I) =
1
2/M/2M exp

−yT y
22

,
(69)

26
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
is substituted into the quadratic entropy expression (67), the following quadratic Rényi entropy estimator is derived by
Erdogmus in Ref. [70]:
ˆH old
2 (X) = −log 1
N2
N

i=1
N

j=1
G
√
2(xj −xi).
(70)
Consider the discrete version of the Rényi entropy written with the expectation operator
H(X) =
1
1 − log EX[f −1
X
(X)].
(71)
By approximating the expectation operator with the sample mean we get
H(X) ≈
1
1 − log 1
N
N

j=1
f −1
X
(xj).
(72)
By substituting the Parzen window estimator into the previous equation, we get
H new

(X) =
1
1 − log 1
N
N

j=1
 N

i=1
(xj −xi)
−1
.
(73)
For  = 2 and Gaussian kernels with standard deviation 
√
2, the old and new estimator become identical. The new
estimator can be used for entropy evaluation or when it is desired to adapt the weights of a learning system based on
entropic performance index [257]. The new estimator is consistent if the Parzen windowing and the sample mean are
consistent for the actual pdf of the iid samples. In case of estimating the joint entropy of an n-dimensional random
vector X from its samples {x1, . . . , xN}, using a multidimensional kernel that is the product of single-dimensional
kernels, the estimate of the joint entropy and the estimate of the marginal entropies are consistent [70].
Similarly as for Shannon entropy, the KLD is deﬁned also for Rényi entropy. Erdogmus [70] derived analogously
a kernel-based resubstitution estimate for Rényi order  divergence. The computational complexity in both cases is
O(N2).
In the Shannon’s case is the MI between the components of an n-dimensional random vector X equal to the KLD
of the joint distribution of X from the product of the marginal distributions of the components X. Rényi order- MI is
deﬁned as the Rényi divergence between the same quantities.
Letting fX(.) be the joint distribution and fXb(.) be the marginal density of the bth component, Rényi mutual
information becomes (Rényi [192])
I (X) =
1
 −1 log
N

i=1
· · ·
N

i=1
f 
X(xi
1, . . . , xi
n)
n
b=1f −1
Xb (xi
b)
.
(74)
It is again possible to write kernel-based resubstitution estimator for Rényi MI by approximating the joint expectation
with the sample mean and then by replacing the pdfs with their Parzen estimators that use consistent kernels between
the marginal and joint pdf estimates. The non-parametric MI estimator is then
ˆI(X) =
1
 −1 log 1
N
N
j=1
⎛
⎝
	
1
N
N
i=1
n
b=1b(xj
b −xi
b)

n
b=1
	
1
N
N
i=1b(xj
b −xi
b)

⎞
⎠
−1
.
(75)
This estimator can be used in problems where it is necessary to evaluate the MI between sets of samples and in
adaptation scenarios, where optimizing according to the MI between certain variables is the primary objective.
In order to improve the performance by smoothing its learning curve, Erdogmus [70] designed two recursive non-
parametric quadratic entropy estimators. One is an exact recursion that provides the exact estimate given by the batch

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
27
estimator, and the other one a forgetting recursion that incorporates the advantages of a forgetting factor for successful
entropy tracking in non-stationary environments. The gradient of the latter estimator directly yields a recursive entropy
gradient, called recursive information gradient (RIG). (The stochastic information gradient is shown to be a special
case of this corresponding to zero memory, as expected).
Other entropy applications in signal processing are from Bercher and Vignat [27] and Viola [250,251]; they use
spectral-estimation based or polynomial expansion type pdf estimates substituted for the actual pdf in Shannon entropy
deﬁnition. Viola derived a differential learning rule called EMMA that optimizes entropy by kernel (Parzen) density
estimation. Entropy and its derivative can then be calculated by sampling from this density estimate. EMMA was
applied for the alignment of three-dimensional models to complex natural images and for detection and correction of
corruption in magnetic resonance images. These applications outperform the results done by parametrical methods.
Bercher and Vignat [27] presented an entropy estimator of continuous signals. The estimator relies on a simple
analogy between the problems of pdf estimation and power spectrum estimation. The unknown probability density of
data is modeled in the form of an autoregressive (AR) spectrum density and regularized long-AR models are applied
to identify the AR parameters. The corresponding estimator does not require the explicit estimation of the pdf but only
of some samples of a correlation sequence. It was evaluated and compared with other estimators based on histograms,
kernel density models, and order statistics. An adaptive version of this entropy estimator was applied for detection of
law changes, blind deconvolution, and source separation.
4.5.2. Estimates by neural network approaches
The computation of entropy from a dataset by a neural network unfortunately requires explicit knowledge of the
local data density. This information is usually not available in the learning from samples case. Schraudolph [204]
analyzed three following methods for making density estimation accessible to a neural network: parametric modeling,
probabilistic networks and non-parametric estimation (by Parzen window estimator). By imposing their own structure
to the data, parametric density models implement impoverished but tractable forms of entropy such as the log-variance
(see Section 5).
In the probabilistic networks, neural network node activities are interpreted as the deﬁning parameters of a stochastic
process. The net input to such a node determines its probability of being active rather than its level of activation. The
distribution of states in a stochastic network of these nodes can be calculated with models from statistical mechanics by
treating the net inputs as energy levels. Since the distribution is analytically available, entropy can be optimized directly,
but it is the entropy of the network rather than that of the data itself. The result of entropy optimization in such a system
therefore depends on the nature of the stochastic model. A well-known example of this type of network is Boltzmann
Machine (Ref. [112]). The entropy of the process can then be calculated from its parameters, and hence optimized. The
non-parametric technique by Parzen or kernel density estimation leads to an entropy optimization algorithm in which
the network adapts in response to the distance between pairs of data samples. Such entropy estimate is differentiable
and can therefore be optimized in a neural network, allowing to avoid the limitations encountered with parametric
methods and probabilistic networks.
The non-parametric estimate of the empirical entropy of Y by Parzen method was derived in the form [204]:
ˆH(Y) = −1
|S|

yi∈S
log ˆp(yi) = −1
|S|

yi∈S
log

yj ∈T
(yi −yj) + log |T |,
(76)
where  is deﬁned by formula (69). Note that Schraudolph does not use Renyi entropy as is used in the estimators in
formulas (70) and (73) but the Shannon one. The Parzen density was used to estimate and optimize the entropy at the
output of a parametrized mapping such as a neural network. This resulted in a simple yet efﬁcient batch learning rule
that operates on pairs of input samples.
Taleb and Jutten [226] proposed optimization of entropy by neural networks. To optimize the output entropy, one
needs to estimate the output pdf (more precisely its derivatives). They suggest to apply a multilayer perceptron (MLP)
in unsupervised learning with the weight vector w. Let x be the input vector, and y the output of this MLP. The deﬁnition
of Shannon entropy (5) can be expressed as
−E[log pY (y)].
(77)

28
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
The weight vector w is trained (under some constraints) to optimize (77) and the stochastic gradient learning
algorithm is
wt+1 = wt + t∇wyT∇y log pY (y),
(78)
where t is the learning rate and the sign of the rate depends on if we want to maximize or minimize output entropy.
Taleb and Jutten applied a method for the estimation of ∇y log p(y) called score functions. Let X=(X1, . . . , Xn) ∈Rn
be a random variable, with differentiable pdf pX(x). Score function in the multivariate case is deﬁned as
X(x) =
 log pX(x)
x1
, . . . ,  log pX(x)
xn
T
.
Suppose that X(x) is known. Then, using function approximation ability of neural networks, one can use a simple
MLP with one input and one output unit to provide an estimation h(w, x) of X(x). The parameter vector w is trained
to minimize the mean squared error:
ϵ = 1
2 E[(h(w, x) −X(x))2].
(79)
A gradient descent algorithm on Eq. (79) leads to the following weights update:
wt+1 = wt −t∇wϵ,
where
∇wϵ = E

h(w, x)∇wh(w, x) + ∇w
h(w, x)
x

.
(80)
Since X(x) in Eq. (80) disappears, the supervised learning algorithm changes into the unsupervised one. The method
can be easily extended into the multivariate case by using a multilayer perceptron with n inputs. To improve the speed
of the learning algorithm based on a simple gradient descent, one can use second order minimization techniques. This
algorithm was applied to the blind source separation.
To mention another application of entropy estimation, Rigoll [193] used entropy as a learning criterion for perceptron-
like networks using self-organizing training. On the other hand, based on the entropy values, the complexity of a neural
network approximating a function can be determined [113].
4.6. Entropy estimates based on maximum likelihood
Maximum likelihood estimation (MLE) is a popular statistical method used to make inferences about parameters of
the underlying probability distribution of a given data set. The method was pioneered by geneticist and statistician Sir
R. A. Fisher already in 1912 [75,76]. We could classify this approach as well as a parametrical one (Section 5).
When maximizing the likelihood, we may equivalently maximize the log of the likelihood, since log is a continuous
(monotonously) increasing function over the range of the likelihood (and the number of calculations may be reduced).
The log-likelihood is closely related to entropy and Fisher information (the latter is the negative of expectation of the
second derivative of the log of f with respect to , where f is the probability function and  is a parameter). Popular
methods for maximum likelihood are the Expectation-Maximization (EM) (i.e. Demster et al. [60] and Berger et al.
[152]) and Improved Iterative Scaling (IIS) algorithms (Berger [26]). These methods are often used in classiﬁcation
tasks, especially in speech recognition.
Maximum likelihood estimator: The joint pdf f (X, ), given in a parametric form, is computed, where X =
{x1, . . . , xN} is the set of randomly drawn samples from this pdf. By statistical independence, f (X, )=N
i=1f (xi, ),
which is known as the likelihood function of . The maximum likelihood (ML) method estimates  such that the like-
lihood function takes its maximum value [232], that
ˆML = arg max

N

i=1
f (xi, ).
(81)

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
29
The maximum a posteriori probability (MAP) estimate ˆMAP is deﬁned as the point where f (|X) = f ()f (X|)
becomes maximum. A method applying log maximum likelihood approach to kernel entropy estimation and using EM
algorithm will be discussed in Section 4.8.
Paninski [167] used an exact local expansion of the entropy function and proved almost sure consistency (strong
consistency) and central limit theorems for three of the most commonly used discretized information estimators, namely
the maximum likelihood (MLE) estimator ˆHMLE(pN) = N
i=1 pN,i log pN,i, in our terminology plug-in (see above),
the MLE with the so-called Miller–Madow bias correction [146,145]
ˆHMM(pN) = ˆHMLE(pN) + ˆm −1
2N ,
(82)
where ˆm is some estimate of the number of bins with non-zero p-probability (Ref. [167] considers ˆm to be the number
of bins with non-zero pN probability), and the jackknifed version of MLE from Efron and Stein [69]
ˆHJK = N ˆHMLE −N −1
N
N

j=1
ˆHMLE−j,
(83)
where ˆHMLE−j is the MLE based on all but the j-th sample.
This framework leads to the estimator ˆHBUB (Best Upper Bounds estimator) equipped with the bounds on the
maximum error over all possible underlying probability distributions; this maximum error is very small. This estimator
was applied both on real and simulated data.
Mixture models: Mixture models provide more ﬂexibility into the density estimation. Here, the unknown density is
modeled as a mixture of M densities
f (x) =
M

m=1
f (x|m)Pm,
(84)
where 
m=1 Pm=1. Thus, this modeling assumes that each point x may be drawn from any of the M model distributions
with probability Pm, m=1, . . . , M. The density components have a parametric form, f (x|m, ) and then the unknown
parameters  and Pm, m = 1, . . . , M must be computed from the samples. Since the contribution of mixture density is
not known, the maximum likelihood principle cannot be easily employed, and one can apply the EM algorithm to solve
the problem. Because of the additional ﬂexibility the mixture models add to the parametric models, this method may
be regarded as semi-parametric. There are basically two drawbacks associated with the parametric density estimation
schemes discussed above. In the case that an information theoretic measure is used as a cost function for training
adaptive systems, the parametric methods require solving an optimization problem within an optimization problem,
where the ‘external’ optimization is the adaption process (using for example gradient-based learning algorithms). The
second drawback is the insufﬁciency of parametric models for general-purpose modeling tasks. The selected parametric
family may be too limiting to be able to accurately model the data distributions in question; it may be as well difﬁcult
to select the right parametric class.
4.7. Correction methods and bias analysis in undersampled regime
Basharin [18] and Herzel [109] pointed out that to the second order, the bias for an entropy estimation is independent
of actual distribution. One can use Bayesian approaches or use very strong assumptions about the estimator to get a
small bias, but estimators with very small bias, i.e. Refs. [202,180] have unfortunately large statistical errors. In this
subsection we discuss entropy estimates, which are mostly analytical and their bias can be computed.
Let us ﬁrst consider the simplest and the most straightforward one, the naive (“likelihood”) estimator, where one
replaces the discrete probabilities pi, i =1, . . . , K (N is the number of observations, and ni the frequency of realization
i among all observations) in the Shannon entropy formula by ˆpi = ni/N. We get
ˆHnaive = −
K

i=1
ˆpi log ˆpi.
(85)

30
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
ˆHnaive is also a MLE SML, since the maximum likelihood estimate of the probabilities is given by the frequencies. This
estimator leads to a systematic underestimation of entropy H (i.e. the difference of the real entropy and its estimator is
positive).
Among the ﬁrst corrections of the estimation error belongs the work of Miller [146], applying a Taylor expansion
around pi to the log function into the naive estimator with the correction term of O(1/N). Paninski [167] applied
Bernstein polynomials (i.e. a linear combination of binomial polynomials, from which he derived the estimator ˆHBUB
discussed in Section 4.6 above, more details [167]) and achieved that the maximum (over all pi) systematic deviations
are of O(1/N2). Unfortunately, the variance of the corresponding estimator turns out to be very large [167]. Thus a
good estimator should have the bounds on bias and variance minimized simultaneously. The result is a regularized
least-squares problem. There is however no guarantee that the solution of the regularized problem implies a good
polynomial approximation of the entropy function; this also depends on the priority, what is more important whether
reducing bias or variance, or vice versa.
In a more recent work by the same author [168], the entropy estimation is investigated in the undersampled regime
(i.e. on m bins given fewer than m samples). It has been long known [146] that the crucial quantity in this estimation
problem is the ratio N/m: if the number of samples is much greater than the number of bins, the estimation problem
is easy, and vice versa. Paninski concentrated on this part of the problem: how can one estimate the entropy when is
N/mN bounded? He showed that a consistent estimator H(pN) exists in this regime (by proving his main conjecture
from Ref. [167]). The most surprising implication of this result is that it is possible to accurately estimate the entropy
on m bins, given N samples, even when N/mN is small (provided that both N and m are sufﬁciently large).
Nemenman et al. [153,154] studied properties of near-uniform (Dirichlet) priors for learning undersampled proba-
bility distributions on discrete non-metric spaces and entropy and information in neural spike trains. The authors argue
that for the estimates of entropy using knowledge of priors, ﬁxing one parameter (beta in the Dirichlet priors) speciﬁes
the entropy almost uniquely.
A Bayesian entropy estimator SML was introduced by Nemenman and Bialek [155] as a MLE and was applied to
synthetic data inspired by experiments and to real experimental spike trains. The estimator SML was inspired by the
Ma’s entropy estimation by counting coincidences for uniform distributions. Ma’s idea was generalized to an arbitrary
distribution. It is well known that one needs N ∼K (N is the size of the data set and K the number of all possible
values of a distribution) to estimate entropy universally with small additive or multiplicative errors [167]. Thus the
main question is: does a particular method work well only for abstract model problems, or does work also on natural
data? The goal of [154] was to show that the method introduced in [153] can generate reliable estimates well into a
classically undersampled regime for an experimentally relevant case of neurophysiological recordings.
In an experiment, in N examples each possibility i occurred ni times. If N?K, one can use the naive estimator
ˆHnaive given by formula (85). It is known that SML underestimates entropy [167].With good sampling (N?K), classical
arguments due to Miller [146] show that the SML estimate should be corrected by a universal term K −1/2N (compare
to the negative results of Paninski for K ∼N in the mentioned discussion above from Ref. [167], where K = mN).
There are other correcting approaches, but however they work only when the sampling errors are in some sense a small
perturbation. To make progress outside of the asymptotically large N regime, one needs an estimator that does not have
a perturbative expansion in 1/N with SML as the zero-order term. The estimator SML from Ref. [153] has this property.
SML is the limiting case of Bayesian estimation with Dirichlet priors.
Maximum likelihood estimation is Bayesian estimation with this prior in the limit  →0, while the natural “uniform”
prior is  = 1. The key observation of Ref. [153] is that while these priors are quite smooth on the state space of p,
the distributions drawn at random from P all have very similar entropies, with a variance that vanishes as K becomes
large. This is the origin of the sample size dependent bias in entropy estimation. The goal is to construct a prior on
the space of probability distributions which generates a nearly uniform distribution of entropies. It is probable that
such a uniform distribution prior would largely remove the sample size dependent bias in entropy estimation, but it
is crucial to test it experimentally. In particular, there are inﬁnitely many priors which are approximately (and even
exactly) uniform in entropy, and it is not clear which of them will allow successful estimation in real world problems.
An estimator SNSB was computed in Ref. [154] and about it proven that the NSB prior almost completely removed the
bias in a model problem and that the SNSB is a consistent Bayesian estimator (the derivation of this and of the entropy
estimator SNSB can be found in Refs. [153,154]). Since the analysis is Bayesian, one obtains not only SNSB but also the
a posteriori standard deviation, an error bar on our estimate. Secondly, for real data in a regime where undersampling
can be beaten down by data, the bias is removed to yield agreement with the extrapolated ML estimator even at a very

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
31
small sizes. Finally and most crucially, applied to natural and nature-inspired synthetic signals, the NSB estimation
performs smoothly and stably over a wide range of K?N. This opens new possibilities for the information theoretic
analysis of experiments.
In the following we focus on other correction methods, which do not use Bayesian analysis. Grassberger [95] derived
an estimator (in more general form for Rényi entropy) which is at least asymptotically unbiased for large N, and is also
a ‘good’ approximation in the case of small samples. The corresponding estimator of the Shannon entropy (assumes
that the observation space is divided into M?1 boxes, each with probability pi, 
ipi = 1 so that each ni is a random
variable with a Poisson distribution) has the form
ˆH =
M

i=1
ni
N

log N −(ni) −
(−1)ni
ni(ni + 1)

,
(86)
where (n)=log (n)/dn is the digamma function. In the case of small probabilities pi>1, this estimate is less biased
than both the above-mentioned naive estimator and Miller’s correction estimate.
Grassberger [96] modiﬁed the previous estimator into the form
ˆHG =
M

i=1
ni
N

(N) −(ni) −(−1)ni
 1
0
tni−1
t + 1 dt

.
(87)
In the high sampling regime (i.e. ?1 points in each box), both estimators have exponentially small biases. In the low
sampling regime, the errors increase but are smaller than for most other estimators (i.e. Miller’s correction [146]). The
correction term of estimator (86) is recovered by a series expansion of the integrand in Eq. (87) up to the second order.
The higher-order terms of the integrand lead to successive bias reductions compared to Eq. (86).
Schürmann [209] proposed a class of parametrical entropy estimators and determined their systematical error ana-
lytically. The estimator of Shannon entropy is of the form
ˆHS() = (N) −1
N
M

i=1
niSni(),
(88)
where ˆHS() = M
i=1 ˆh(, ni) and Sn() = (n) + (−1)n  1/−1
0
tn−1
1+t dt and ˆh is an estimator of h(p) = −p log p
satisfying ˆh. For bias of ˆh holds
b(, p) = −p
 1−p/
0
tN−1
1 −t dt
and
E[ ˆH(, n)] = −p log p + b(, p).
This estimator is unbiased for  = p and there is a turning point for  = pN. The estimator is asymptotically unbiased,
i.e. b(, p) →0 for N →∞if ⩾p/2. The mean square error (i.e. statistical error) is 2(, p)=E[(ˆh(, n)−h(p))2]
(where h(p)=−p log p). For =1 the estimator ˆh in the asymptotic regime n?1 it leads to the Miller’s correction. For
=e−1/2 is the Grassberger estimator ˆH a special case. For =1/2 is the estimator identical to the estimator ˆHG from
Grassberger. This estimator is less biased than Miller’s correction and the estimator ˆh(e−1/2, n), but the statistical error
is bigger. The experiments in Ref. [205] indicate that it is not possible to decide which estimator should be generally
preferred. A good choice of the parameter is always application dependent.
4.8. Kernel methods
Kernel density estimation methods: MI was ﬁrst estimated by this approach by Moon et al. [151].According to Steuer
et al. [221], the KDE methods were found to be superior to the classical histogram methods (see Section 4.2.1) from
the following reasons: (1) they have a better mean square error rate of convergence of the estimate to the underlying
density; (2) they are insensitive to the choice of origin; (3) the window shapes are not limited to the rectangular window.

32
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
Kernel density estimator introduced by Silverman [215] in one-dimensional space is deﬁned
f (x) = 1
Nh
N

i=1
K
x −xi
h

,
(89)
where h is the kernel width parameter. Kernel function K(x) is required to be a (normalized) probability density
function. It follows that also f itself is a probability density. The selection of h is crucial but the methods for selection
of h are usually computationally intensive. Silverman suggests, as the optimal width h to use the one which minimizes
the mean integrated square error, assuming the underlying distribution is Gaussian:
hopt =

4
3N
1/5
 ≈(1.06N1/5),
(90)
where  denotes the standard deviation of the data. For two-dimensional spaces, we use two-dimensional Gaussian
kernel estimate
Fg(x) =
1
2Nh2
N

i=1
expdi(x,y)2/2h2,
(91)
where di(x, y) is the Euclidean distance of (x, y) from (xi, yi). According to Silverman [215], under the assumption
that the density Fg is Gaussian, an approximately optimal value is given by
hopt ≈

4
d + 2
1/(d+4)
N−1/(d+4),
(92)
where d is the dimension of the data set and s the average marginal standard deviation. Steuer et al. [221] made
objections against a straightforward introduction of a kernel density estimator into the logarithmic formula of MI. The
reason is that kernel estimation can be used for a continuous form of MI while we are interested in the MI of discrete
states. The discretization of the (x, y)-plane into inﬁnitesimal bins corresponds to the continuous form of MI
I(X, Y) =

X

Y
f (x, y) log
 f (x, y)
f (x)f (y)

dx dy.
(93)
But such a correspondence does not hold for the individual entropies used in the formula I(X, Y) = H(X) + H(Y) −
H(X, Y). The discretization introduced by numerical integration for computing the above integral does not correspond
to the partition of data. It is shown in Ref. [221] that the estimated MI is much less sensitive than the probability density
itself.
Generalized cross-redundancies: Prichard and Theiler [182] introduced a method to compute information theoretic
functionals based on MI using correlation integrals. Correlation integrals were introduced by Grassberger and Procaccia
[94] as
Cq(x, ϵ) = 1
N2
N

i=1
N

j=1
(ϵ −∥xi −xj∥),
(94)
where q is the order of the integral, N is the number of samples, ϵ is the radius of the neighborhood and (x) is
the Heaviside function. A similar work on correlation integrals is from Refs. [108,106]. Prichard and Theiler [182]
introduced the generalized redundancy
Iq(x1; x2, l, ϵ) = Hq(x1(t), ϵ) + Hq(x2(t −l), ϵ) −Hq(x1(t), x2(t −l), ϵ),
(95)
which is a time-lagged MI functional between x1(t) and x2(t −l) parametrized by ϵ > 0 and q which is the order of
Rényi entropy. They were inspired by the work of Green and Savit [97] on statistics quantifying dependencies between

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
33
variables. Setting q =1, MI based on Shannon entropy is obtained. This cross-redundancy [182] can be expressed using
correlation integrals as
Iq(x1; x2, l, ϵ) = −log2
Cq(x1(t), ϵ)Cq(x2(t −l), ϵ)
Cq((x1(t), x2(t −l)), ϵ)
.
(96)
Entropy from maximum likelihood kernel density estimation:Although both maximum likelihood methods and Parzen
estimator were discussed already in the previous chapters, we will allow ourselves to present here these methods once
more in the framework of multidimensional kernel methods. Schraudolph [205] proposed an estimate of entropy based
on KDE (Parzen window estimation). The underlying assumption is that the probability density p(y) of the generating
process is a smoothed version of the empirical pdf of the sample. The estimate based on Parzen windows based on a
sample of data T can be written as
ˆp(y) = 1
|T |

yj ∈T
K(y −yj),
(97)
where K is the kernel. This is an unbiased density estimate of the true density. The kernel used in the work [205] is
K(y) = N(0, ) = exp(−1
2yT−1y)
(2)n/2||1/2
,
(98)
with dimensionality n and the covariance matrix . The obvious problem is the choice of the covariance matrix : in
one extreme the estimated pdf will converge to the form of the kernel regardless of the sample distribution and in the
other extreme the estimated pdf is too dependent on the particular set of samples in T (thus inducing large variance in
the estimate).A suitable kernel between these extremes can be found by the maximum likelihood method.An empirical
estimate of the maximum likelihood kernel is the kernel which makes a second sample S drawn independently from
the pdf p(y). Usually, the logarithm of the maximum likelihood is maximized for numerical reasons
ˆL = log

yi∈S
ˆp(yi) =

yi∈S
log

yj ∈T
K(yi −yj) −|S| log |T |.
(99)
The estimated log-likelihood from formula (99) (which equals to formula (76) multiplied by −|S|) assumes two
independent sample sets S and T. In practice, not enough data might be available to create separate sample sets S and
T. The data is also likely to be quantized by some measurement process. Both of the above effects distort the shape
of the log-likelihood function and thus can shift the position of the maximum. Schraudolph uses a technique called
leave-one-out to ensure S ∩T = ∅. When estimating the pdf at the sample yi the set Ti = S −{yi} is used. This method
ensures optimal use of the sample T while respecting the maximum likelihood requirement S ∩T =∅. The quantization
effect is mitigated by reintroducing the quantization noise into the kernel
K(y) = [exp(−1
2(yT −1y + bT −1b)]
(2)
n
2 ||
1
2
,
(100)
where b is the vector of the quantization bin widths in each dimension and  = 1
12 if the condition y /∈T holds.
Schraudolph [205] showed how the maximum likelihood ˆL can be maximized using gradient ascent for a diagonal
 matrix. Because the performance of standard gradient ascent is not satisfactory due to the shape of the ˆL function,
it is recommended to use exponentiated gradient ascent with step-size adaptation [205]. Using this approach, the
convergence of the method has signiﬁcantly improved.
Schraudolph also presents an EM algorithm variant (for the original EM algorithm see Ref. [107]), which has  as
the only optimized variable, with ﬁxed centers (i.e. data points yi). This is possible since the kernel used the problem
can be understood as the estimation of a mixture of Gaussians. In the E-step, using a given , so-called proximity
factors ij are computed. The proximity factors indicate how is each data point yi responsible for the mixture j; they
are estimated by
ij =
K(yi −yj)

yk∈T K(yi −yk).
(101)

34
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
In the maximization step of the EM algorithm, the new covariance matrix is computed as the covariance of the
proximity weighted data
 = 1
|S|

yi∈S

yj ∈T
ij(yi −yj)(yi −yj)T.
(102)
The convergence of the algorithm is further improved by overrelaxation, where the covariance matrix is modiﬁed as
(t) = (t)−1(t −1)(t).
(103)
It is recommended to use the covariance matrix of the entire sample (uniformly weighted) to initialize the EM
algorithm.
An estimate of the Shannon entropy can be computed using the kernel density estimate deﬁned above by
formula (76).
4.8.1. Transfer entropy
The causality detection approach based on so-called transfer entropy [206] was introduced and discussed in Section
2.4, showing that the expression for the transfer entropy (33) is equivalent to the conditional MI deﬁned in the same
set-up (dimensions, time lags). Here we remind again the paper of Schreiber [206], now from the point of view of the
estimation method.
Schreiber [206] proposed to compute the transfer entropy using the correlation integrals [94]
ˆpr(xn+1, xn, yn) = 1
N

n′


r −

xn+1 −xn′+1
xn −xn′
yn −yn′


,
(104)
where  is a suitable kernel and | · | is a norm. The generalized correlation integral based on time series xn, yn
approximates the probability measure p(in+1|i(k)
n , j(l)
n ). The step kernel (x > 0)=1; (x ⩽0)=0 and the maximum
norm are used. It is recommended to exclude dynamically correlated pairs (e.g. using a Theiler window [231]).
The transfer entropy was tested on a lattice of 100 unidirectionally coupled Ulam maps (for the deﬁnition see e.g.
Ref. [238]). The direction of information transfer was correctly shown. Moreover, the bifurcation points, where the
behavior of the lattice changed, was identiﬁed. The analysis was done using 105 data points in each series [206].
Verdes [245] proposed a modiﬁcation of Schreiber’s method which generalizes the above-mentioned correlation
integral to non-cubic neighborhoods. Instead of using the standard neighborhood, which uses the same ϵ value in each
dimension, Verdes uses a neighborhood
p∗(xi, yi, zi) =
1
Npairs
n(xij < ϵ, yij < Y , zij < Z).
(105)
Verdes conjectures that due to the limited amount of available data and noise in the data bounds, reasonable Y values
from below and for large Y the conditioning has no effect; a possible choice of Y is
Y = arg max n(xij < ϵ, yij < Y )
n(yij < Y )
.
(106)
The value for Z is selected analogically.
5. Parametric estimators
No ﬁnite sample can determine density or the entropy directly.Therefore, some assumption about either the functional
form of the density or about its smoothness can be appropriate in some cases. The most common approach is to assume
that the density has a parametric form.
This approach is preferred when there is conﬁdence that the pdf underlying the samples belongs to a known parametric
family of pdf’s. It is effective when the assumed parametric family is accurate but it is not appropriate in adaptation
scenarios where the constantly changing pdf of the data under consideration may not lie in a simple parametric family.

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
35
Then, it becomes necessary to estimate the entropy non-parametrically. Parametric entropy estimation is a two step
process. First, the most probable density function is selected from the space of possible density functions. This often
requires a search through parameter space (for example maximum likelihood methods). Second, the entropy of the
most likely density is evaluated.
When the parametric assumption is violated, the resulting algorithms are incorrect. The most common assumption,
that the data follow the Gaussian density, is especially restrictive.An entropy maximization technique that assumes that
data are Gaussian, but operates on data drawn from a non-Gaussian density, may in fact end up minimizing entropy.
The popularity of the Gaussian function is based on three considerations: (1) ﬁnding the Gaussian that ﬁts the data best
is very easy, (2) the entropy of the Gaussian can be directly calculated from its variance, and (3) an afﬁne transformation
of a Gaussian random variable remains Gaussian. Entropy of a Gaussian density is h(X) = −EX[log g(x −)] =
1
2 log 2 exp , where g(x −) is the Gaussian density with variance  and mean  and EX is the expectation over the
random variable X. It is well known that given a sample set A, the most likely Gaussian density has its mean the mean
of A and as its variance the variance of A. As a result, if we assume that a random variable is Gaussian, its empirical
entropy is proportional to the log of the sample variance. More simply, when the data is assumed Gaussian, maximizing
entropy is equivalent to maximizing variance.
Schraudolph [204] argues that one does not have to assume a particular shape for the density in order to set up a
parametric model for entropy optimization: Let X be a continuous random variable with density P(x) = Prob[X = x],
and letY be the linear function Y = X + . Since the density ofY is given by P(x) = Prob[X = x] = p((y −)/)/,
its entropy is
H(Y) = −E[log(p(y −)//)] = −E[log p(x) −log ] = H(X) + log .
That is, regardless of the shape of pdf p(x), the entropy of a linear function of X scales with the log of the variance.
Matching p(x) to empirical data with a linear transformation thus changes the model’s entropy in proportion to the
log-variance log  of the data.
5.1. Entropy expressions for multivariate distributions
Verdugo Lazo and Rathie [246] computed a table of explicit Shannon entropy expressions for many commonly used
univariate continuous pdfs. Ahmed and Gokhale [4] extended this table and results to the entropy of several families
of multivariate distributions, including multivariate normal, normal, log-normal, logistic and Pareto distributions.
Consistent estimators for the parametric entropy of all the above-listed multivariate distributions can be formed by
replacing the parameters with their consistent estimators (computed by Arnold [11]). Besides an explicit functional
form or a smoothness condition for density estimation, one can assume that the pdf could be estimated, i.e. by a neural
network of a given type. In this way we can deﬁne parametric neural network estimators for pdf and then consequently
entropy estimation. Neural network entropy estimation was discussed already in Section 4.5.2 (since neural network
approaches can be classiﬁed both as learning theory methods and parametric methods).
5.2. Entropy estimators by higher-order asymptotic expansions
This class includes Fourier Expansion, Edgeworth Expansion and Gram–Charlier Expansion and other expansions
[107]. We will discuss here only the last two. An earlier work applying the Gram–Charlier polynomial expansion to
MI estimation for a blind separation algorithm is from Hua Yang and Amari [116]. They applied the Gram–Charlier
expansion and the Edgeworth expansion (both to the fourth-order cumulants) to approximate the pdf of the outputs.
Their computer simulations showed that Gram–Charlier expansion is superior to the Edgeworth expansion for blind
separation.
5.2.1. Mutual information estimation by Gram–Charlier polynomial expansion
Trappenberg et al. [234] introduced a variable selection scheme to a statistical dependency test based on MI. They
compared several methods for MI estimation, namely a standard (equally binned) histogram method (HG), an adaptive
partitioning histogram method (AP) from Darbellay and Vajda [54] and the MI estimation based on the Gram–Charlier
polynomial expansion (GC) [35].

36
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
The CG method of MI estimation is based on the Gram–Charlier polynomial expansion of a probability density
function derived by Blinnikov and Moessner [35] in the form
f (x) ≈
∞

n=0
cn
dnZ(x)
dxn
,
(107)
where Z(x) = exp(−x2/2)/
√
2 is a Gaussian function and cn are factors that determine the weights of different
order derivations of Z(x). Using the truncated polynomial expansion for marginal pdf’s, Amari et al. [6] derived an
approximation of the marginal entropy
ˆH(x) = 2e
2
−(kx
3)2
2.3! −(kx
4)2
2.4! + (5.kx
3)2kx
4
8
+ (kx
4)3
16 ,
(108)
where kx
3 and kx
4 are third- and fourth-order cumulants. Using the fourth-order Gram–Charlier expansion for two-
dimensional joint pdf, Akaho et al. [5] derived the joint entropy
H(x, y) = H(r, s) + 1
2 log(1 −2),
(109)
where  = E[xy], r and s are a linear combination of x and y, [ r
s ] = ( c+
c−
c−
c+ )[ x
y ] c+ = [(1 + )−1/2 + (1 −)−1/2]/2,
c−= [(1 + )−1/2 −(1 −)−1/2]/2 and ˆH(r, s) = 1 + log 2 −
1
2.3![(3,0)2 + 3(2,1)2 + 3(1,2)2 + (0,3)2] −
1
2.4![(4,0)2 + 4(3,1)2 + 6(2,2)2 + 4(1,3)2 + (0,4)2] where k,l = E{rksl} −k,l
0 ,
k,l
0 =
3
k = 4 or l = 4
1
k = l = 2
0
otherwise.
(110)
MI can then be calculated from these estimates using formula (12), which corresponds to a polynomial of high-order
cumulants.
The comparison of these three MI estimators can be summarized as follows: The advantage of MI estimation with
Gram–Charlier expansion is that it only calculates the expectation value of different powers of the samples. Thus, it
is fast and easy to calculate. The disadvantage of the GC method is that the estimate might suffer from the truncation
of the expansion in the case of non-Gaussian signals and result into the underestimation of MI. The histogram-based
methods are in this sense more general than polynomial expansion-based methods because they are less sensitive to the
bin partitioning. A rough partitioning might result in bias towards high MI, while ﬁne-grained partitions might result
in underestimating MI. A good choice of bin width is particularly important for MI estimation as the regions with low
data densities carry large information content (such as the tails of a distribution).
5.2.2. Edgeworth approximation of entropy and mutual information
The Edgeworth expansion, similarly as the Charlier–Gram expansion approximates a probability distribution in
terms of its cumulants. According to Hall [104], it provides in general accurate approximations to the ﬁnite-sample
distribution and can be used in deriving the higher-order accuracy of the bootstrap methods. The advantage of the
Edgeworth series with respect to the Gram–Charlier series is that the error is controlled, so it is a true asymptotic
expansion. Edgeworth expansion is consistent, i.e. in inﬁnity converges to the function which it expands, Cramer [52].
The Edgeworth expansion of a function is estimated in terms of a known distribution f with the same pdf as the
function to be approximated, and cumulants i. The density f is generally chosen to be that of normal distribution.
Here we mention the deﬁnition of the Edgeworth expansion for multivariate density p(v), v ∈Rd and up to the ﬁfth
order about its best normal estimate p(v) (i.e. with the same mean and covariance matrix as p) and corresponding
multivariate entropy H(p) is it was used in Ref. [242]:
p(v) ≈p(v)(1 + (1/3!)

i,j,k
i,j,khijk(v) + )
(111)
with the ijk-th Hermite polynomial and i,j,k = ijk/(s2
i s2
j s2
k)−1/2 where ijk is the sample third cumulant over input
dimensions i, j, k, and si the sample second central moment over input dimension i. The term  collects the terms in

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
37
hijkl and hijklpq that are dropped out in the ﬁrst-order entropy derivation below. (For the precise calculation of the terms
of expansion we refer the reader to Blinnikov and Moessner [35]). Since the differential entropy H(p)=H(p)−J(p),
with J neg-entropy, the H(p) can be approximated as [242]
H(p) ≈H(p) −1
12
⎡
⎣
d

i=1
(i,i,i)2 + 3
d

i,j=1,i̸=j
(i,i,j)2 + 1
6
⎛
⎝
d

i,j,k=1,i<j<k
(i,j,k)2
⎞
⎠
⎤
⎦,
(112)
which converges on the order of O(N−2) with N number of data points. The term H(p) is the expression for the
d-dimensional entropy: H(p) = 0.5 log |  | + d
2 log 2 + d
2 , where |.| denotes determinant.
To our best knowledge, the ﬁrst application of Edgeworth expansions for neg-entropy in the univariate case in the
literature was proposed in Ref. [122] and was generalized for multivariate case and for entropy and Kullback–Leibler
distance in Ref. [141].
The Edgeworth approximations for the KLD and neg-entropy in the experiments of Lin et al. [141] required in the
simulations that the distributions p and p0 are not far from the Gaussian distribution [103] (to avoid a big approximation
error). In the case of differential entropy, one can get it from KLD by using the formula H(X) = H(p) = H(p) −
K(p, p), where H(X) is given by Eq. (6) and p is distribution of the random event. So by using KLD, one can also
obtain differential entropy.
To summarize, to approximate both KLD and differential entropy, and consequently MI by Edgeworth expansion
makes sense only for “close”-to-Gaussian distributions. On the other hand, differential entropy by the Edgeworth ex-
pansion avoids the density estimation problems. Furthermore, the order of Edgeworth approximation of differential
entropy is O(N−3/2) and for KLD O(N−1/2), while the density method approximation is of order O(N−1/2) where
N is size of processed sample. The density estimation cannot be used for differential entropy and KLD estimation for
dimension d > 2 (because of its speed), while the Edgeworth expansion of neg-entropy produces very good approxi-
mations also for more-dimensional Gaussian distributions [141]. Furthermore, the error rate of the histogram estimator
depends not only on sample size N but also on the choice of the bandwidth value h; the total error is O(h2)+O(N−1/2)
[103]. In the case of histogram, density and kernel estimation, the error is O(N−1/2) for dimension d < 3. The kernel
estimator is much less sensitive to choices of the bandwidth h compared to the histogram estimator.
Finally, both the differential entropy and KLD estimator by Edgeworth expansion can be evaluated for distributions
of arbitrary dimension, while the other three mentioned methods can be practically applied only to low-dimensional
distributions (d ⩽3).
It appears from the above that the most important advantage of the Edgeworth expansion is its applicability in
multidimensional spaces. On the other hand, it has the following drawbacks: (1) its behavior on probability distri-
butions that differ signiﬁcantly from Gaussian distribution and (2) the EE approximation can give approximation
of pdf having negative values [190]. Gaztanga et al. [83] addressed the negativity problem by exploring expansions
around such pdfs, which would yield positive densities when the variance is large enough and proposed to use Gamma
probability function (the Gamma pdf, also called negative binomial or Pearson Type 3 (PT3) arises from the 2 dis-
tribution with N degrees of freedom, where 1/2 = N/2 is taken to be a continuous parameter the Gamma pdf).
Gamma expansion has, by construction, the exponential tails and a better general behavior than the Edgeworth ex-
pansion, both with respect to the positivity of approximating pdf and the approximation error. The experiments in
Ref. [83] conﬁrmed that the Gaussian EE has tails dropping quickly to zero as the underlying Gaussian pdf, while
the Gamma EE has exponential-type tails. Differences in both expansions might be slight, especially around the peak
of pdf, as to the ﬁrst order both expansions are formally equivalent. Therefore, which one best ﬁts the data set is a
matter of careful data analysis. The Gamma EE is a real competitor to the Gaussian EE as it can be generalized to
multivariate case.
A multivariate case of the Gaussian EE expansion estimate of differential entropy and MI was experimentally
compared to the following methods in Ref. [241]: 1-spacings from Ref. [101], Parzen window plug-in estimate from
Ref. [3] and KL and KSG method [135,136]. The Edgeworth expansion up to the order three was used and comparisons
were performed on the normal and exponential distributions. Each distribution was considered along each dimension
and size of the data sample. The best performance results were achieved for the KSG method, the EE method was since
it was biased for the exponential distribution.

38
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
6. Generalized Granger causality
TheclassicalapproachofGrangercausalityasmentionedinSection1.2isintuitivelybasedonthetemporalproperties,
i.e. the past and present may cause the future but the future cannot cause the past [88]. Accordingly, the causality is
expressed in terms of predictability: if the time series Y causally inﬂuences the time series X, then the knowledge of
the past values of X and Y would improve a prediction of the present value of X compared to the knowledge of the past
values of X alone. The causal inﬂuence in the opposite direction can likewise be checked by reversing the role of the two
time series. Although this principle was originally formulated for wide classes of systems, both linear and nonlinear
systems, the autoregressive modeling framework (Eq. (1)) proposed by Granger was basically a linear model, and such
a choice was made primarily due to practical reasons [90]. Therefore, its direct application to nonlinear systems may or
may not be appropriate. In the following subsection, we discuss some recent methods to extend the Granger’s concept
to nonlinear cases.
6.1. Nonlinear Granger causality
Ancona et al. [7] extended Granger causality deﬁnition to nonlinear bivariate time series. To deﬁne linear Granger
causality [88], the vector autoregressive model (VAR) (for two series x and y) is used, which considers the time series
x as a vector-weighted sum of both series X and Y (similarly for x) and AR (x = V1.X and y = V2.Y, V1 and V2 to be
estimated by least square ﬁt). A directionality index is introduced measuring the unidirectional, bidirectional inﬂuence
or uncorrelation. The index
D = c2 −c1
c1 + c2
(113)
(where c1 = ϵx −ϵxy and c2 = ϵy −ϵyx) varies from 1 in the case of unidirectional inﬂuence (x →y) to −1 in
the opposite case (y →x), with the intermediate values corresponding to bidirectional inﬂuence. According to this
deﬁnition of causality, the following property holds for sufﬁciently large M (M = N −m, N is the length of the time
series, m the order of the model).
If the ﬁrst time series Y is uncorrelated with X and x then ϵx = ϵxy (where ϵx is the estimate of the variance of
x −V1.X, V1.X is prediction of x, similarly ϵy is the estimate of the variance of y −V2.Y, V2.Y is prediction of y;
ϵxy and ϵyx are the prediction errors of the VAR model, deﬁned as the estimated variance of x −W11.X −W12.Y and
y −W21.X −W22.Y, respectively). This means that in this case VAR and AR modelings of the xi time series coincide.
Analogously, if X is uncorrelated with Y and y then ϵy = ϵyx. These properties are fundamental and make the linear
prediction approach suitable to evaluate causality. On the other hand, for nonlinear systems higher-order correlations
may be relevant.
Ancona et al. proposed that any prediction scheme providing a nonlinear extension of Granger causality should
satisfy the following property: (P1) if Y is statistically independent of X and x, then ϵx = ϵxy; if X is statistically
independent of Y and y, then ϵy =ϵyx. The approach applying locally linear models suggested by Hua-Yang and Amari
[114] for evaluation of nonlinear causality needs very long time series to satisfy P1. To construct a method working
effectively on moderately long time series, the problem of extending Granger causality can be formulated as ﬁnding
classes of nonlinear models satisfying property P1. Radial basis function method (RBF) [42] is suggested to be applied
to the family of models given by
x = w11.(X) + w12.(Y),
y = w21.(X) + w22.(Y),
(114)
where {w} are four n-dimensional real vectors,  = (1, . . . , n) are n given nonlinear real functions of m variables,
and  = (1, . . . , n) are n other real functions of m variables. The prediction errors are given by empirical risks
ϵxy = 1
M

k=1
M[xk −w11(Xk) −w12(Yk)]2,
ϵyx = 1
M

k=1
M[yk −w21(Xk) −w22(Yk)]2.
(115)

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
39
Fixed M?n where n is the number of centers { ˆX}n
=1 in the space of X vectors, are determined by a clustering (or
other) procedure applied to data {X}M
k=1. Analogously, n centers { ˆY}n
=1 in the space of Y vectors, are determined by
a clustering applied to data {Y}M
k=1. Ancona et al. suggest to choose
(X) = exp(−∥X −ˆX∥2/22),
 = 1, . . . , n,
(116)
(Y) = exp(−∥Y −ˆY∥2/22),
 = 1, . . . , n,
(117)
where  is a ﬁxed parameter, whose order of magnitude is the average spacing between the centers. The centers ˆX are
prototypes of the X variable. Functions  measure the similarity to these typical patterns, analogously,  measure the
similarity to typical patterns of Y.
The method was tested on chaotic maps and on time series of heart rate and breath rate of a sleeping human suffering
from sleep apnea. There is a growing evidence that suggests a causal link between sleep apnea and cardiovascular
disease. This data set has been already analyzed by Schreiber [206], measuring the rate of information ﬂow (transfer
entropy), and a stronger ﬂow of information from the heart rate to the breath rate was found. In this example, the
rate of information ﬂow entropy and Granger nonlinear causality give consistent results. Both these quantities, in
the end, measure the departure from the generalized Markov property [206]. The results in Ref. [7] showed that the
value of the directionality index D may in some cases be very sensitive to statistical ﬂuctuations, especially when the
interdependence is weak.
According to Ancona et al., the standard RBF model of bivariate time series in comparison to formula (115) (as
described by Bishop [34]) does not satisfy in general property P1 and therefore is not suited to evaluate causality.
Ancona and Stramaglia [8] argued that not all nonlinear prediction schemes are suitable to evaluate causality between
two time series, since they should be invariant if statistically independent variables are added to the set of input
variables. This property guarantees that, at least asymptotically, one would be able to recognize variables without
causality relationship. Marinazzo et al. [143] used the theoretical results from Ref. [8] to ﬁnd the largest class of RBF
models suitable to evaluate causality, and in this sense they extended the results of Ref. [8]. Moreover, they showed the
application of causality to the analysis of cardio-circulatory interaction and studied the mutual inﬂuences in inhibitory
and excitatory model neurons.
6.2. Non-parametric Granger causality
Despite the computational beneﬁt of model-based (linear and/or nonlinear) Granger causality approaches, it should
be noted that the selected model must be appropriately matched to the underlying dynamics, otherwise model mis-
speciﬁcation would arise, leading to spurious causality values. A suitable alternative would be to adopt non-parametric
approaches which are free from model mismatch problems. Since the topic of this paper is causality based on information
theory, we discuss primarily those nonparametric approaches which can be expressed in the information theoretic terms.
Let us ﬁrst reformulate the Granger causality in information theoretic terms [64,62]: For a pair of stationary, weakly
dependent, bivariate time series {Xt, Yt}, Y is a Granger cause of X if the distribution of Xt given past observations of
X and Y differs from the distribution of Xt given past observations of X only. Thus {Yt} is a Granger cause of {Xt} if
FXt+1(x|FX(t), FY (t)) ̸= FXt+1(x|FX(t)),
(118)
where FXt+1 represents the cumulative distribution function of Xt+1 given F, and FX(t) and FY (t) represents the
information contained in past observations of X and Y up to and including time t.
Given two time series, the delay vectors are ﬁrst constructed as follows: Xt =(Xt, Xt−
x, . . . , Xt−
x(dx−1)), and Yt =
(Yt, Yt−
y, . . . , Yt−
y(dy−1)) where time delays are 
x and 
y, and embedding dimensions are dx and dy, respectively.
The idea of the Granger causality is to quantify the additional amount of information on Xt+1 contained in Yt,
given Xt.
Now, the average amount of information which a random variable X contains about another random variable Y can
be expressed in terms of generalized correlation integrals (see the equivalent Eq. (9)) as
Iq(X, Y) = log Cq(X, Y) −log Cq(X) −log Cq(Y),
(119)

40
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
where the generalized correlation integral [94], Cq can be estimated by
Cq(X, ϵ) =
1
N(N −1)q−1
N

j=1
⎡
⎣
i̸=j
(∥Xj −Xi∥−ϵ)
⎤
⎦
q−1
;
(120)
 is the Heaviside function, ∥.∥a norm and the last term is related to KDE. C2(X, ϵ) is simply the probability that
a distance between two independent realizations of X is smaller than or equal to ϵ. For computational ease, q = 2
is preferred [62], though q = 1 is also used elsewhere [45]. We refer the interested readers to Refs. [45,182] for
computational and statistical properties of correlation integral with different choices of order (q) and the length scale
(ϵ). For visual clarity, both of these indices are omitted from the following equations.
Now, the amount of information about Xt+1 contained in both Xt and Yt will be:
I(Xt, Yt; Xt+1) = log C(Xt, Yt, Xt+1) −log C(Xt, Yt) −log C(Xt+1),
(121)
whereas the amount of information of Xt+1 contained in Xt is
I(Xt; Xt+1) = log C(Xt, Xt+1) −log C(Xt) −log C(Xt+1).
(122)
Given the past values of X at any speciﬁc time instant t, if past values ofY does not contain any information about the
future values of X, then I(Xt, Yt; Xt+1) = I(Xt; Xt+1), otherwise when the past values of Y do contain information
about the future, the following inequality I(Xt, Yt; Xt+1) > I(Xt; Xt+1) is expected.
Accordingly, the extra amount of information that Yt contains about Xt+1 in addition to the information already
contained in Xt will be Eqs. (121)–(122) which provides the information-theoretic measure of Granger causality:
I GC
Y→X = I(Xt, Yt; Xt+1) −I(Xt; Xt+1)
= log C(Xt, Yt, Xt+1) −log C(Xt, Xt+1) −log C(Xt, Yt) + log C(Xt).
(123)
In order to obtain the statistical signiﬁcance, bootstrapping procedure is recommended to check if the statistic is
signiﬁcantly larger than zero [62,115].
Here the causality measure is based on conditional entropy, and unlike mutual or time-lagged information measures,
can distinguish actually transported information from that produced as a response to a common driver or past history
[86,206]. Interestingly, these entropies can be expressed in terms of generalized correlation integrals whose non-
parametric estimation is well known. Correlation integrals are routinely employed in nonlinear time series analysis [1].
Additionally, correlation integral based entropies require minimal assumptions about the underlying dynamics of the
systems and the nature of their coupling, thus the applications of these entropies are no longer restricted to deterministic
systems but are suitable for any arbitrary, stationary and weakly mixing systems [182].
Correlation integral based non-parametric Granger causality test was originally proposed by Baek and Brock [14]
and then later modiﬁed by Hiemstra and Jones [110] in the ﬁeld of econometrics. They proposed the test statistic as
T GC
Y→X = C(Xt, Yt; Xt+1)
C(Xt, Yt)
−C(Xt, Xt+1)
C(Xt)
.
(124)
The null hypothesis—Y is not Granger causing X—could be rejected if T GC is too large because higher values of
T GC are expected when past observations ofY contain information about future observations of X. T GC has some initial
bias since it depends on the length N of the series [14] but it was shown [110] that asymptotically the statistic under
the null hypothesis is normally distributed. However, one has to be careful in accepting the null hypothesis by using
this statistic for real data applications [65].
Both statistic, I GC and T GC, are closely related (compare Eq. (124) to Eq. (123)), however, there is no one-to-one
mapping between the outcomes of two statistic.Additionally, the statistical signiﬁcance of the two statistic are measured
in different ways, one by using Monte Carlo bootstrapping and the other by asymptotical distribution theory.
Itisworthmentioningthatbothstatisticsneitherprovideanyspeciﬁcinformationaboutthenature(linearornonlinear)
nor the sign (positive or negative inﬂuence) of the causality. For I GC statistic, one could calculate the linearized version
on the basis of redundancies as proposed by Paluš [161] (see also Ref. [45]) which reﬂects only the dependence
contained in the linear correlation matrix (of the variables which are assumed to be Gaussian). If the general statistic

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
41
I GC passes the test of signiﬁcance, then its linearized counterpart is subject to test; if the linear version indicates a
signiﬁcant causality, then the causal inﬂuence is most probably due to a linear Gaussian process, otherwise a nonlinear
type of causal inﬂuence can be inferred. For T GC statistic, one could use the residuals from the linear autoregressive
model ﬁt to the two time series and any remaining incremental predictive power of one residual series for another can
be considered nonlinear [14].
The T GC statistic was applied primarily in the ﬁeld of econometrics and ﬁnance, for example, an unidirectional
information ﬂow from relative money supply to exchange rate in European Monetary System [142], bidirectional
causality between daily stock returns and trading volume in Korean market [214] where as volume Granger causes
returns for Standard and Poor’s index [65] (but see also their result of weakening this inﬂuence on the basis of a modiﬁed
test statistic), bi-directional causality between volume and volatility in the New York Stock Exchange [41], to name
a few.
On the other hand, IGC statistic is relatively new but has been applied to wide classes of systems, from climatological
[63], cardiological [115], to neurophysiological ones [45].
Other alternative nonparametric tests, such as non-causality test based on additive models proposed by Bell et al.
[23], or test for conditional independence based on Hellinger distances [223], also exist, however, their applications
have been quite limited, so far.
7. Conclusion
Natural phenomena usually emerge from behavior or evolution of so-called complex systems which consist of many
parts or subsystems. These subsystems interact in a complex, non-linear way and the behaviour of the whole system
cannot be explained by a linear summation of dynamics of the system parts. In real-life situations, we do not have a
direct access to the equations governing the underlying dynamics; instead, we are faced with a dataset representing the
temporal dynamics of possibly interacting variables recorded from possibly interacting subsystems. How can we tell
from these observed sequences whether there exists any causal relationship between two ore more variables?
The basic rationale of this paper was that information theory provides a crucial key to this answer, and information
theoretical measures, in particular conditional mutual information, can detect and measure causal link and information
ﬂow between observed variables. However, it opens a more difﬁcult question: How to reliably estimate these measures
from a ﬁnite dataset? Research literature abounds with various estimators with a diverse range of assumptions and
statistical properties. The overall objective of this paper was to present the current state of the art of these estimators
of information theoretical measures which could be applied to detect causality. To the best of our knowledge, there is
no other review paper available in the literature which deals with causality and its estimation from this point of view.
We classiﬁed and discussed two types of estimators: parametric and non-parametric estimators.
Theoretically, for a good entropy estimator, the condition of consistency seems to be important. We speciﬁcally
highlighted those estimators whose consistency results are known or could be derived. However, it should be noted
that the conditions for desired consistency might be too restrictive for an experimental environment. Accordingly, we
also critically reviewed those methods which have surprisingly good overall performance (i.e. small systematic and
statistical error for a wide class of pdfs) though their consistency properties are not yet known.
Last but not least, let us mention some informal comments on the detection of causality which are relevant to
any causality measure applied. One needs to be extra careful before claiming a causal relationship between observed
variables. From the viewpoint of establishing new models, inferences and control strategies, establishing a causal
relationship is always tempting. However, one has to ﬁrst carefully scrutinize the statistical properties of the observed
data sequences and the completeness of the model or the assumptions necessary for the estimation of the information-
theoretic measures. Otherwise, spurious results could often be obtained. Despite these precautionary remarks, we would
like to stress again that there are enough good reasons, contrary to B. Russel’s arguments [198], to investigate causality,
offering numerous applications in natural and physical sciences.
Acknowledgments
K.H.-S. was supported by grant of Austrian Research Fonds FWF-H-226 (2005) under Charlotte Bühler Program
and partly byASCR 1ET 100 750 401, Project Baddyr. M.P. and M.V. were supported by the EC FP6 project BRACCIA

42
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
(Contract No. 517133 NEST), and partly by the Institutional Research Plan AV0Z10300504. J.B. was supported by
JST.ERATO Shimojo project.
References
[1] H.D.I. Abarbanel, Introduction to Nonlinear Dynamics for Physicists, Lecture Notes in Physics, World Scientiﬁc, Singapore, 1993.
[2] M. Abramowitz, I.A. Stegun, Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 9th printing, Dover,
New York, 1972, pp. 946–949.
[3] I.A. Ahmad, P.E. Lin, A nonparametric estimation of the entropy for absolutely continuous distributions, IEEE Trans. Inform. Theory 22
(1976) 372–375.
[4] N.A. Ahmed, D.V. Gokhale, Entropy expressions and their estimators for multivariate distributions, IEEE Trans. Inform. Theory 35 (1989)
688–692.
[5] S. Akaho, Y. Kiuchi, S. Umeyama, MICA: multimodal independent component analysis, Proc. IJCNN (1999) 927–932.
[6] S. Amari, A. Cichocki, H.H. Yang, A new learning algorithm for blind signal separation, in: D.S. Touretzky, M.C. Mozer, M.E. Hasselmo
(Eds.), Advances in Neural Information Processing Systems, vol. 8, MIT Press, Cambridge, MA, 1996, pp. 757–763.
[7] N. Ancona, D. Marinazzo, S. Stramaglia, Radial basis function approach to nonlinear Granger causality of time series, Phys. Rev. E 70 (2004)
056221.
[8] N. Ancona, S. Stramaglia, An invariance property of predictors in kernel-induced hypothesis spaces, Neural Comput. 18 (2006) 749–759.
[9] A. Antos, I. Kontoyiannis, Convergence properties of functional estimates for discrete distributions, Random Structures and Algorithms,
Special issue: Average-Case Analysis of Algorithms 19 (2002) 163–193.
[10] F.M. Aparicio, A. Escribano, Information-theoretic analysis of serial dependence and cointegration, Stud. Nonlinear Dynam. Econometr. 3
(1998) 119–140.
[11] B.C. Arnold, Pareto Distributions, International Co-Operative Publishing House, Burtonsvile, MD, 1985.
[12] J. Arnhold, P. Grassberger, K. Lehnertz, C.E. Elger, A robust method for detecting interdependences: application to intracranially recorded
EEG, Physica D 134 (1999) 419–430.
[14] E.G. Baek, W.A. Brock, A general test for nonlinear Granger causality: Bivariate model, Working paper, Iowa State University and University
of Wisconsin, Madison, 1992.
[15] M. Baghli, A model-free characterization of causality, Econ. Lett. 91 (2006) 380–388.
[18] G.P. Basharin, On a statistical estimate for the entropy of the sequence of independent random variables, Theory Probab. Appl. 4 (1959)
333–338.
[20] J. Beirlant, M.C.A. van Zuijlen, The empirical distribution function and strong laws for functions of order statistics of uniform spacings,
J. Multivar. Anal. 16 (1985) 300–317.
[21] J. Beirlant, Limit theory for spacing statistics from general univariate distributions, Pub. Inst. Stat. Univ. Paris XXXI Fasc. 1 (1986) 27–57.
[22] J. Beirlant, E.J. Dudewitz, L. Györﬁ, E.C. van der Meulen, Nonparametric entropy estimation: an overview, Int. J. Math. Statist. Sci. 6 (1997)
17–39.
[23] D. Bell, J. Kay, J. Malley, A non-parametric approach to non-linear causality testing, Econ. Lett. 51 (1996) 7–18.
[26] A. Berger, The improved iterative scaling algorithm: a gentle introduction ⟨http://www.cs.cmu.edu/afs/cs/user/aberger/www/ps/scaling.ps⟩,
1997.
[27] J.F. Bercher, C. Vignat, Estimating the entropy of a signal with applications, IEEE Trans. Signal Process. 48 (2000) 1687–1694.
[32] P. Bickel, L. Breiman, Sums of functions of nearest neighbor distances, moment bounds, limit theorems and a goodness of ﬁt test, Ann. Statist.
11 (1983) 185–214.
[34] C.M. Bishop, Neural Networks for Pattern Recognition, Oxford University Press, New York, 1995.
[35] S. Blinnikov, R. Moessner, Expansions for nearly Gaussian distributions, Astronom. Astrophys. Suppl. Ser. 130 (1998) 193–205.
[36] K.J. Blinowska, R. Ku´s, M. Kami´nski, Granger causality and information ﬂow in multivariate processes, Phys. Rev. E 70 (2004) 050902(R).
[37] S. Boccaletti, J. Kurths, G. Osipov, D.L. Valladares, C.S. Zhou, The synchronization of chaotic systems, Phys. Rep. 366 (2002) 1–101.
[38] M. Braˇciˇc Lotriˇc, A. Stefanovska, Synchronization and modulation in the human cardiorespiratory system, Physica A 283 (2000) 451–461.
[39] J. Brea, D.F. Russell, A.B. Neiman, AB Measuring direction in the coupling of biological oscillators: a case study for electroreceptors of
paddleﬁsh, Chaos 16 (2006) 026111.
[41] C. Brooks, Predicting stock index volatility: can market volume help?, J. Forecast. 17 (1998) 59–80.
[42] D.S. Broomhead, D. Lowe, Multivariate functional interpolation and adaptive networks, Complex Systems 2 (1988) 321–355.
[43] A.J. Butte, I.S. Kohane, Mutual information relevance networks: functional genomic clustering using pairwise entropy measurements, Pac.
Symp. Biocomput. (2000) 418–429.
[44] C.J. Cellucci,A.M.Albano, P.E. Rapp, Statistical validation of mutual information calculations: comparison of alternative numerical algorithm,
Phys. Rev. E 71 (2005) 066208.
[45] M. Chávez, J. Martinerie, M. Le Van Quyen, Statistical assessment of nonlinear causality: application to epileptic EEG signals, J. Neurosci.
Methods 124 (2003) 113–128.
[46] Y. Chen, G. Rangarajan, J. Feng, M. Ding, Analyzing multiple nonlinear time series with extended Granger causality, Phys. Lett. A 324 (2004)
26–35.
[48] I.P. Cornfeld, S.V. Fomin, Y.G. Sinai, Ergodic Theory, Springer, New York, 1982.
[50] T. Cover, J. Thomas, Elements of Information Theory, Wiley, New York, 1991 (Chapter 9).
[51] T. Cover, J. Thomas, Elements of Information Theory, Wiley, New York, 1991 (Chapter 5).
[52] H. Cramer, On the composition of elementary errors, Skand. Aktuarietidskr. 11 (1928) 13–14, 141–180.

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
43
[53] N. Cressie, On the logarithm of higher order spacings, Biometrika 63 (1976) 343–355.
[54] G. Darbellay, I. Vajda, Estimation of the information by an adaptive partitioning of the observation space, IEEE Trans. Inform. Theory 45
(1999) 1315–1321.
[55] G. Darbellay, An estimator of the mutual information based on a criterion of independence, Comput. Statist. Data Anal. 32 (1999) 1–17.
[56] C. Daub, R. Steuer, J. Selbig, S. Kloska, Estimating mutual information using B-spline functions—an improved similarity measure for
analysing gene expression data, BMC Bioinformatics 5 (118) (2004) 1–12.
[58] C. De Boor, A Practical Guide to Splines, Springer, New York, 1978.
[60] A. Dempster, N. Laird, D. Rubin, Maximum likelihood from incomplete data via the EM algorithm, J. Roy. Statist. Soc. B 39 (1977) 138.
[62] C. Diks, J. DeGoede, A general nonparametric bootstrap test for Granger causality, in: Broer, Krauskopf, Vegter (Eds.), Global Analysis of
Dynamical Systems, 2001, pp. 391–403, (Chapter 16).
[63] C. Diks, M. Mudelsee, Redundancies in the Earth’s climatological time series, Phys. Lett. A 275 (2000) 407–414.
[64] C. Diks, V. Panchenko, A note on the Hiemstra–Jones test for Granger non-causality, Stud. Nonlinear Dynam. Econometr. 9 (2005) 4/1-7.
[65] C. Diks, V. Panchenko, A new statistic and practical guidelines for nonparametric Granger causality testing, J. Econom. Dynam. Control 30
(2006) 1647–1669.
[66] Y.G. Dmitriev, F.P. Tarasenko, On the estimation functions of the probability density and its derivatives, Theory Probab. Appl. 18 (1973)
628–633.
[67] R.L. Dobrushin,A simpliﬁed method of experimentally evaluating the entropy of a stationary sequence, TeoriyaVeroyatnostei i ee Primeneniya
3 (1958) 462–464.
[68] E.J. Dudewitz, E.C. Van der Meulen, Entropy-based test of uniformity, J. Amer. Statist. Assoc. 76 (1981) 967–974.
[69] B. Efron, C. Stein, The jackknife estimate of variance, Ann. Statist. 9 (1981) 586–596.
[70] D. Erdogmus, Information theoretic learning: Renyi’s Entropy and its application to adaptive system training, Ph.D. Thesis, University of
Florida, 2002.
[71] D. Erdogmus, J. Principe, An error-entropy minimization algorithm for supervised training of nonlinear adaptive systems, IEEE Trans. Signal
Processing 50 (2002) 1780–1786.
[75] R.A. Fisher, On an absolute criterion for ﬁtting frequency curves, Messenger Math. 41 (1912) 155–160.
[76] R.A. Fisher, On the mathematical foundations of theoretical statistics, Philos. Trans. Roy. Soc. London Ser. A 222 (1922) 309–368.
[78] A. Fraser, H. Swinney, Independent coordinates for strange attractors from mutual information, Phys. Rev. A 33 (1986) 1134–1140.
[79] A. Fraser, Information and entropy in strange attractors, IEEE Trans. Inform. Theory 35 (1989) 245–262.
[83] E. Gaztanga, P. Fosalba, E. Elizande, Gravitational evolution of the large-scale probability density distribution: the Edgeworth and Gamma
expansions, Astrophys. J. 539 (1) (2000) 522–531.
[84] A. German, J.B. Carlin, H.S. Stern, D.B. Rubin, Bayesian DataAnalysis, Chapman & Hall/A CRC Press Company, Texts in Statistical Science
Series, 2004.
[85] J. Geweke, Inference and causality in economic time series models, in: Z. Griliches, M.D. Intriligator (Eds.), Handbook of Econometrics, vol.
2, North-Holland, 1984, pp. 1101–1144.
[86] I.M. Gelfand, A.M. Yaglom, Calculation of the amount of information about a random function contained in another such function, Amer.
Math. Soc. Transl. 12 (1959) 199–236.
[87] M.N. Goria, N.N. Leonenko, V.V. Mergel, P.L. Novi Inverardi, A new class of random vector entropy estimators and its applications in testing
statistical hypotheses, Nonparametr. Statist. 17 (2005) 277–297.
[88] C.W.J. Granger, Investigating causal relations by econometric and cross-spectral methods, Econometrica 37 (1969) 424–438.
[90] C.W.J. Granger, P. Newbold, Forecasting Economic Time Series, Academic Press, New York, 1977.
[91] C.W.J. Granger, Testing for causality: a personal viewpoint, J. Econ. Dynam. Control 2 (1980) 329–352.
[92] C.W.J. Granger, Time series analysis, cointegration, and applications. Nobel Lecture, December 8, 2003, in: Les Prix Nobel. The
Nobel Prizes 2003, ed. Tore Frängsmyr, [Nobel Foundation] (Stockholm, 2004) pp. 360–366. http://nobelprize.org/nobel_prizes/
economics/laureates/2003/granger-lecture.pdf.
[93] P. Grassberger, An optimized box-assisted algorithm for fractal dimensions, Phys. Lett. A 148 (1990) 63–68.
[94] P. Grassberger, I. Procaccia, Measuring of strangeness of strange attractors, Physica D (1983) 189–208.
[95] P. Grassberger, Finite sample corrections to entropy and dimension estimates, Phys. Lett. A 128 (1988) 369–373.
[96] P. Grassberger, Entropy Estimates from Insufﬁcient Samplings, Arxiv preprint physics/0307138, (2003), arxiv.org.
[97] M.L. Green, R. Savit, Dependent variables in broad band continuous time series, Physica D 50 (1991) 521–544.
[98] L. Györﬁ, E.C. Van der Meulen, Density-free convergence properties of various estimators of entropy, Comput. Statist. Data Anal. 5 (1987)
425–436.
[99] L. Györﬁ, E.C. Van der Meulen, An entropy estimate based on a kernel density estimation, in: I. Berkes, E. Csaki, P. Revesz (Eds.), Limit
Theorems in Probability and Statistics, North-Holland, Amsterdam, 1989, pp. 229–240.
[100] L.Györﬁ,E.C.VanderMeulen,Onnonparametricestimationofentropyfunctionals,in:G.Roussas(Ed.),NonparametricFunctionalEstimation
and Related Topics, Kluwer Academic Publisher, Amsterdam, 1990, pp. 81–95.
[101] P. Hall, Limit theorems for sums of general functions of m-spacings, Math. Proc. Camb. Philos. Soc. 96 (1984) 517–532.
[103] P. Hall, On Kullback–Leibler loss and density estimation, Ann. Statist. 15 (1987) 1491–1519.
[104] P. Hall, The Bootstrap and Edgeworth Expansion, Springer, New York, 1992.
[105] P. Hall, S.C. Morton, On the estimation of entropy, Ann. Inst. Statist. Math. 45 (1993) 69–88.
[106] T.C. Halsey, M.H. Jensen, L.P. Kadanoff, I. Procaccia, B. Shraimann, Fractal measures and their singularities: the characterization of strange
sets, Phys. Rev. A 33 (1986) 1141.
[107] S. Haykin, Neural Networks: A Comprehensive Foundation, second ed., Prentice-Hall, Englewood Cliffs, NJ, 1998.

44
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
[108] H.G.E. Hentschel, I. Procaccia, The inﬁnite number of dimensions of probabilistic fractals and strange attractors, Physica D 8 (1983) 435.
[109] H. Herzel, Complexity of symbol sequences, Syst. Anal. Model. Simul. 5 (1988) 435–444.
[110] C. Hiemstra, J.D. Jones, Testing for linear and nonlinear Granger causality in the stock price-volume relation, J. Finance 49 (1994)
1639–1664.
[111] H. Hinrichs, H.J. Heinze, M.A. Schoenfeld, Causal visual interactions as revealed by an information theoretic measure and fMRI, NeuroImage
31 (2006) 1051–1060.
[112] G. Hinton, T. Sejnowski, Learning and relearning in Boltzmann machines, in: D. Rumelhart, J. McClelland (Eds.), Parallel Distributed
Processing, vol. 1, MIT Press, Cambridge, 1986, pp. 282–317, (Chapter 7).
[113] K. Hlaváˇcková-Schindler, Some comparisons of the complexity of neural network models and linear methods by means of metric entropy, in:
Andrýsek, Kárný, Kracík (Eds.), Multiple Participant Decision Making, Int. Ser. Adv. Intell. 9 (2004) 149–160.
[114] M.-Ch. Ho, Y.-Ch. Hung, I.-M. Jiang, Phase synchronization in inhomogeneous globally coupled map lattices, Phys. Lett. A 324 (2004)
450–457.
[115] B.P.T. Hoekstra, C.G.H. Diks, M.A. Allessie, J. DeGoede, Non-linear time series analysis: methods and applications to atrial ﬁbrillation, Ann.
Ist. Super. Sanita 37 (2003) 325–333.
[116] H. Hua-Yang, S. Amari, Adaptive on-line learning algorithms for blind separation—maximum entropy and minimum mutual information,
Neural Comput. 9 (1997) 1457–1482.
[118] S. Ihara, Information Theory for Continuous Systems, World Scientiﬁc, Singapore, 1993.
[119] A.V. Ivanov, A. Rozhkova, Properties of the statistical estimate of the entropy of a random vector with a probability density, Probl. Inform.
Transm. 10 (1981) 171–178.
[120] J. Jamšek, A. Stefanovska, P.V.E. McClintock, Nonlinear cardio-respiratory interactions revealed by time-phase bispectral analysis, Phys.
Med. Biol. 49 (2004) 4407–4425.
[121] H. Joe, On the estimation of entropy and other functionals of a multivariable density, Ann. Inst. Statist. Math. 41 (1989) 638–697.
[122] M.C. Jones, R. Sibson, What is projection pursuit?, J. Roy. Statist. Soc. London Ser. A 150 (1987) 1–36.
[123] A.M. Kagan, Y.V. Linnik, C.R. Rao, Characterization Problems in Mathematical Statistics, Wiley, New York, 1973.
[126] M. Kami´nski, M. Ding, W.A. Truccolo, S.L. Bressler, Evaluating causal relations in neural systems: Granger causality, directed transfer
function and statistical assessment of signiﬁcance, Biol. Cybern. 85 (2001) 145–157.
[130] T. Katura, N. Tanaka, A. Obata, H. Sato, A. Maki, Quantitative evaluation of interrelations between spontaneous low-frequency oscillations
in cerebral hemodynamics and systemic cardiovascular dynamics, Neuroimage 31 (2006) 1592–1600.
[133] K.H. Knuth, A. Gotera, C.T. Curry, K.A. Huyser, K.R. Wheeler, W.B. Rossow, Revealing relationships among relevant climate variables with
information theory, Earth–Sun System Technology Conference 2005, Adelphi, MD, 2005.
[134] A.N. Kolmogorov, Entropy per unit time as a metric invariant of automorphism, Dokl. Akad. Nauk SSSR 124 (1959) 754–755.
[135] L.F. Kozachenko, N.N. Leonenko, Sample estimate of the entropy of a random vector, Probl. Inform. Transm. 23 (1987) 95–100.
[136] A. Kraskov, H. Stögbauer, P. Grassberger, Estimation mutual information, Phys. Rev. E 69 (2004) 066138.
[137] S. Kullback, R.A. Leibler, On information and sufﬁciency, Ann. Math. Statist. 22 (1951) 79–86.
[139] N.Leonenko,L.Pronzato,V.Savani,AclassofRényiinformationestimatorsformultidimensionaldensities,LaboratoireI3S,CNRS–Université
de Nice-Sophia Antipolis, Technical Report I3S/RR-2005-14-FR, 2005.
[141] J.J. Lin, N. Saito, R.A. Levine, Edgeworth approximations of the Kullbach-Leibler distance towards problems in image analysis, 2001,
⟨http://www.math.ucdavis.edu/∼saito/publications/saito_ekld2.pdf⟩.
[142] Y. Ma, A. Kanas, Testing for nonlinear Granger causality from fundamentals to exchange rates in the ERM, J. Int. Finance Markets, Inst.
Money 10 (2000) 69–82.
[143] D. Marinazzo, M. Pellicoro, S. Stramaglia, Nonlinear parametric model for Granger causality of time series, Phys. Rev. E 73 (2006) 066216.
[144] R. Marschinski, H. Kantz, Analysing the information ﬂow between ﬁnancial time series—an improved estimator for transfer entropy, Eur.
Phys. J. B 30 (2002) 275–281.
[145] G. Miller, W. Madow, On the maximum likelihood estimate of the Shannon–Wiener measure of information, Air Force Cambridge Research
Center Technical Report, vol. 75, 1954, pp. 54–75.
[146] G. Miller, Note on the bias of information estimates, in: H. Quastler (Ed.), Information Theory in Psychology II-B, Free Press, Glencoe, IL,
1955, pp. 95–100.
[148] R. Moddemeijer, On estimation of entropy and mutual information of continuous distribution, Signal Processing 16 (1989) 233–246.
[149] I.I. Mokhov, D.A. Smirnov, El Nino-Southern Oscillation drives NorthAtlantic Oscillation as revealed with nonlinear techniques from climatic
indices, Geophys. Res. Lett. 33 (2006) L03708.
[150] A. Mokkadem, Estimation of the entropy and information for absolutely continuous random variables, IEEE Trans. Inform. Theory 35 (1989)
195–196.
[151] Y. Moon, B. Rajagopalan, U. Lall, Estimation of mutual information using kernel density estimators, Phys. Rev. E 52 (1995) 2318–2321.
[152] R. Neal, G. Hinton, A view of the EM algorithm that justiﬁes incremental, in: M.I. Jordan (Ed.), Learning in Graphical Models, MIT Press,
Cambridge, MA, 1999, pp. 355–368.
[153] I. Nemenman, F. Shafee, W. Bialek, Entropy and inference revisited, in: T.G. Dietterich, S. Becker, Z. Ghahramani (Eds.), Advances in Neural
Information Processing Systems, vol. 14, MIT Press, Cambridge, MA, 2002.
[154] I. Nemenman, W. Bialek, R. de Ruyter van Stevenick, Entropy and information in neural spike trains: progress on sampling problem, Phys.
Rev. E 69 (2004) 056111.
[155] I. Nemenman,W. Bialek, Occam factors and model-independent Bayesian learning of continuous distributions, Phys. Rev. E 65 (2002) 026137.
[156] K. Otsuka,Y. Miyasaka, T. Kubota, Formation of an information network in a self-pulsating multimode laser, Phys. Rev. E 69 (2004) 046201.
[157] R. Ottes, A critique of Suppe’s theory of probabilistic causality, Synthese 48 (1981) 167–189.

K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
45
[158] M. Paluš, A. Stefanovska, Direction of coupling from phases of interacting oscillators: an information-theoretic approach, Phys. Rev. E 67
(2003) 055201(R).
[159] M. Paluš, V. Komárek, T. Procházka, Z. Hrnˇcíˇr, K. Štˇerbová, Synchronization and information ﬂow in EEG of epileptic patients, IEEE Eng.
Med. Biol. Mag. 20 (2001) 65–71.
[160] M. Paluš, V. Komárek, Z. Hrnˇcíˇr, K. Štˇerbová, Synchronization as adjustment of information rates: detection from bivariate time series, Phys.
Rev. E 63 (2001) 046211.
[161] M. Paluš, Testing for nonlinearity using redundancies: quantitative and qualitative aspects, Physica D 80 (1995) 186–205.
[162] M. Paluš, Coarse-grained entropy rates for characterization of complex time series, Physica D 93 (1996) 64–77.
[163] M. Paluš, Kolmogorov entropy from time series using information-theoretic functional, Neural Network World 7 (3) (1997) 269–292
⟨http://www.uivt.cas.cz/∼mp/papers/rd1a.pdf⟩.
[164] M. Paluš, Identifying and quantifying chaos by using information-theoretic functionals, in: A.S. Weigend, N.A. Gershenfeld (Eds.), Time
Series Prediction: Forecasting the Future and Understanding the Past, Santa Fe Institute Studies in the Sciences of Complexity, Proceedings
of vol. XV, Addison-Wesley, Reading, MA, 1993, pp. 387–413.
[165] M. Paluš, Detecting nonlinearity in multivariate time series, Phys. Lett. A 213 (1996) 138–147.
[166] M. Paluš, D. Hoyer, Detecting nonlinearity and phase synchronization with surrogate data, IEEE Eng. Med. Biol. 17 (1998) 40–45.
[167] L. Paninski, Estimation of entropy and mutual information, Neural Comput. 15 (2003) 1191–1253.
[168] L. Paninski, Estimating entropy on m bins given fewer than m samples, IEEE Trans. Inform. Theory 50 (2004) 2200–2203.
[170] E. Parzen, On estimation of a probability density function and mode, in: Time SeriesAnalysis Papers, Holden-Day Inc., San Diego, California,
1967.
[171] J. Pearl, Causality: Models, Reasoning and Inference, Cambridge University Press, New York, 2000.
[172] M. Penrose, Central limit theorems for k-nearest neighbor distances, Stochastic Processes Appl. 85 (2000) 295–320.
[173] K. Petersen, Ergodic Theory, Cambridge University Press, Cambridge, 1983.
[174] M.A. Peters, P.A. Iglesias, Minimum entropy control for discrete-time varying systems, Automatica 33 (1997) 591–605.
[176] M.E. Pﬂieger, R.E. Greenblatt, Using conditional mutual information to approximate causality for multivariate physiological time series, Int.
J. Bioelectromagn. 7 (2005) 285–288.
[177] A. Pikovsky, M. Rosenblum, J. Kurths, Synchronization.A Universal Concept in Nonlinear Sciences, Cambridge University Press, Cambridge,
2001.
[178] M.S. Pinsker, Information and Information Stability of Random Processes, Holden Day, San Francisco, 1964.
[179] B. Pompe, Measuring statistical dependencies in a time series, J. Statist. Phys. 73 (1993) 587–610.
[180] T. Poschel, W. Ebeling, H. Rose, Dynamic entropies, long-range correlations and ﬂuctuations in complex linear structures, J. Statist. Phys. 80
(1995) 1443–1452.
[181] B.L.S. Prasaka Rao, Nonparametric Functional Estimation, Academic Press, New York, 1983.
[182] D. Prichard, J. Theiler, Generalized redundancies for time series analysis, Physica D 84 (1995) 476–493.
[184] J.C. Principe, J.W. Fischer, D. Xu, Information theoretic learning, in: S. Haykin (Ed.), Unsupervised Adaptive Filtering, Wiley, New York,
2000, pp. 265–319.
[185] J.C. Principe, D. Xu, Q. Zhao, J.W. Fischer, Learning from examples with information theoretic criteria, J. VLSI Signal Processing Systems,
Special Issue on Neural Networks (2000) 61–77.
[186] L. Pronzato, H.P. Wynn, A.A. Zhigljavsky, Using Renyi entropies to measure: uncertainty in search problems, Lect. Appl. Math. AMS 33
(1997) 253–268.
[187] M. Le Van Quyen, J. Martinerie, C. Adam, F.J. Varela, Nonlinear analyses of interictal EEG map the brain interdependences in human focal
epilepsy, Physica D 127 (1999) 250–266.
[188] R. Quian Quiroga, J.Arnhold, P. Grassberger, Learning driver-response relationships from synchronization patterns, Phys. Rev. E 61 (5) (2000)
5142–5148.
[190] N. Reid, Asymptotic expansions, in: S. Kotz, C.B. Read, D.L. Banks (Eds.), Encyclopedia for Statistical Sciences, Update Volume, Wiley,
New York, 1996, pp. 32–39.
[191] A. Rényi, On measures of entropy and information, in: Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and
Probability, vol. 1, University of California Press, Berkeley, CA, 1961, pp. 547–561.
[192] A. Rényi, Some fundamental questions of information theory, Selected Papers of Alfred Rényi, vol. 2, Akademia Kiado, Budapest, 1976, pp.
526–552.
[193] G. Rigoll, Maximum mutual information neural networks for hybrid connectionist-HMM speech recognition systems, IEEE Trans. Speech
Audio Processing, Special Issue on Neural Networks for Speech 2 (1994) 175–184.
[194] J. Rissanen, T.P. Speed, B. Yu, Density estimation by stochastic complexity, IEEE Trans. Inform. Theory 38 (1992) 315–323.
[195] F. Rossi,A. Lendasse, D. Francois,V.Wertz, M.Verleysen, Mutual information for the selection of relevant variables in spectrometric nonlinear
modeling, in: Chemometrics and Intelligent Laboratory Systems, Lille, France, November 30–December 1, 2005, pp. 52–55.
[198] B. Russel, On the notion of cause, Proc. Aristotelian Soc. New Ser. 13 (1913) 1–26.
[199] C. Schäfer, M.G. Rosenblum, J. Kurths, H.H. Abel, Heartbeat synchronized with ventilation, Nature 392 (1998) 239–240.
[200] C. Schäfer, M.G. Rosenblum, J. Kurths, H.H. Abel, Synchronization in the human cardiorespiratory system, Phys. Rev. E 60 (1999) 857–870.
[201] S.J. Schiff, P. So, T. Chang, R.E. Burke, T. Sauer, Detecting dynamical interdependence and generalized synchrony through mutual prediction
in a neural ensemble, Phys. Rev. E 54 (1996) 6708–6724.
[202] A.O. Schmitt, H. Herzel, W. Ebeling, A new method to calculate higher-order entropies from ﬁnite samples, Europhys. Lett. 23 (1993)
303–309.
[203] A. Schmitz, Measuring statistical dependence and coupling of subsystems, Phys. Rev. E 62 (2000) 7508–7511.

46
K. Hlaváˇcková-Schindler et al. / Physics Reports 441 (2007) 1–46
[204] N. Schraudolph, Optimization of entropy with neural networks, Ph.D. Thesis, University of California, San Diego, USA, 1995.
[205] N. Schraudolph, Gradient-based manipulation of non-parametric entropy estimates, IEEE Trans. Neural Networks 14 (2004) 828–837.
[206] T. Schreiber, Measuring information transfer, Phys. Rev. Lett. 85 (2000) 461–464.
[209] T. Schürmann, Bias analysis in entropy estimation, J. Phys. A Math. Gen. 37 (2004) L295–L301.
[210] C. Selltiz, L.S. Wrightsman, S.W. Cook, Research Methods in Social Relations, Holt, Rinehart and Winston, New York, 1959.
[211] C.E. Shannon, A mathematical theory of communication, Bell System Tech. J. 27 (1948) 379–423.
[212] R. Shaw, Strange attractors, chaotic behavior and information ﬂow, Z. Naturforsch 36A (1981) 80–112.
[213] R.S. Shaw, The Dripping Faucet as a Model Chaotic System, Aerial, Santa Cruz, 1985.
[214] P. Silvapulle, J.S. Choi, Testing for linear and nonlinear Granger causality in the stock price-volume relation: Korean evidence, Quart. Rev.
Econ. Finan. 39 (1999) 59–76.
[215] B.W. Silverman, Density Estimation, Chapman & Hall, London, 1986.
[216] Y.G. Sinai, On the concept of entropy for a dynamic system, Dokl. Akad. Nauk SSSR 124 (1959) 768–771.
[217] Y.G. Sinai, Introduction to Ergodic Theory, Princeton University Press, Princeton, NJ, 1976.
[219] A. Sorjamaa, J. Hao, A. Lendasse, Mutual information and k-nearest neighbors approximator for time series prediction, in: Proceedings of
ICANN 2005, Lecture Notes in Computer Science, vol. 3697, 2005, pp. 553–558.
[220] A. Stefanovska, H. Haken, P.V.E. McClintock, M. Hožiˇc, F. Bajrovi´c, S. Ribariˇc, Reversible transitions between synchronization states of the
cardiorespiratory system, Phys. Rev. Lett. 85 (2000) 4831–4834.
[221] R. Steuer, J. Kurths, C.O. Daub, J. Weise, J. Selbig, The mutual information: detecting and evaluating dependencies between variables,
Bioinformatics 18 (Suppl. 2) (2002) S231–S240.
[222] S.P. Strong, R. Koberle, R.R. de Ruyter van Steveninck, W. Bialek, Entropy and information in neural spike trains, Phys. Rev. Lett. 80 (1998)
197–202.
[223] H.J. Su, H.White,A nonparametric Hellinger metric test for conditional independence,Technical Report, Department of Economics, University
of California, San Diego, 2003.
[224] F. Takens, Detecting strange attractors in turbulence, in: D.A. Rand, L.S.Young (Eds.), Dynamical Systems and Turbulence, Springer, Berlin,
1981, pp. 366–381.
[225] F. Takens, Invariants related to dimension and entropy, in: Atas do 13 Coloquio Brasileiro de Mathematica, Rio de Janeiro, Brasil, 1983.
[226] A. Taleb, C. Jutten, Entropy optimization—application to source separation, Lecture Notes in Computer Science, vol. 1327, 1997,
pp. 529–534.
[227] F.P.Tarasenko, On the evaluation of an unknown probability density function, the direct estimation of the entropy from independent observations
of a continuous random variable, and the distribution-free entropy test of goodness-of-ﬁt, Proc. IEEE 56 (1968) 2052–2053.
[228] P. Tass, M.G. Rosenblum, J. Weule, J. Kurths, A. Pikovsky, J. Volkmann, A. Schnitzler, H.-J. Freund, Detection of n:m phase locking from
noisy data: application to magnetoencephalography, Phys. Rev. Lett. 81 (1998) 3291–3294.
[231] J. Theiler, Estimating fractal dimension, J. Opt. Soc. Amer. A 7 (1990) 1055–1071.
[232] S. Theodoridis, K. Koutroumbas, Pattern Recognition, Academic Press, San Diego, 1999.
[233] K. Torkkola, Feature extraction by non-parametric mutual information maximization, J. Mach. Learn. Res. 3 (2003) 1415–1438.
[234] T. Trappenberg, J. Ouyang, A. Back, Input variable selection: mutual information and linear mixing measures, IEEE Trans. Knowledge Data
Eng. 18 (2006) 37–46.
[236] U. Triacca, Is Granger causality analysis appropriate to investigate the relationships between atmospheric concentration of carbon dioxide and
global surface air temperature?, Theor. Appl. Climatol. 81 (2005) 133–135.
[237] A.B. Tsybakov, E.C. van Meulen, Root-n consistent estimators of entropy for densities with unbounded support, Scand. J. Statist. 23 (1994)
75–83.
[238] ⟨http://mathworld.wolfram.com/UlamMap.html⟩.
[239] B. Van Es, Estimating functional related to a density by a class of statistics based on spacings, Scand. J. Statist. 19 (1992) 61–72.
[241] M.M. Van Hulle, Edgeworth approximation of multivariate differential entropy, Neural Comput. 17 (2005) 1903–1910.
[242] M.M. Van Hulle, Multivariate Edgeworth-based entropy estimation, in: Proceedings of the IEEE Workshop on Machine Learning for Signal
Processing, Mystic, Connecticut, USA, September 28–30, 2005.
[243] O. Vašíˇcek, A test for normality based on sample entropy, J. Roy. Statist. Soc Ser. B Methodol. 38 (1976) 54–59.
[244] M. Vejmelka, K. Hlaváˇcková-Schindler, Mutual Information Estimation in Higher Dimensions: A Speed-Up of a k-Nearest Neighbor Based
Estimator, submitted to ICANNGA’07.
[245] P.F. Verdes, Assessing causality from multivariate time series, Phys. Rev. E 72 (2005) 026222.
[246] A.C.G. Verdugo Lazo, P.N. Rathie, On the entropy of continuous probability distributions, IEEE Trans. Inform. Theory 24 (1978) 120–122.
[248] J. Victor, Binless strategies for estimation of information from neural data, Phys. Rev. E 66 (2002) 051903.
[250] P. Viola, Alignment by maximization of mutual information, Ph.D. Thesis, MIT, 1995.
[251] P.Viola,N.Schraudolph,T.Sejnowski,Empiricalentropymanipulationforreal-worldproblems,in:AdvancesinNeuralInformationProcessing
Systems (NIPS 8), The MIT Press, Cambridge, MA, 1996, pp. 851–857.
[252] N. Wiener, The theory of prediction, in: E.F. Beckenbach (Ed.), Modern Mathematics for Engineers, McGraw-Hill, New York, 1956.
[253] ⟨http://en.wikipedia.org/wiki/Causality⟩.
[254] L. Xu, C. Cheung, H. Yang, S. Amari, Maximum equalization by entropy maximization and mixture of cumulative distribution functions, in:
Proceedings of ICNN’97, Houston, 1997, pp. 1821–1826.
[257] J. Yang, P. Grigolini, On the time evolution of the entropic index, Phys. Lett. A 263 (1999) 323–330.

