SHARP CONVERGENCE RATES FOR MATCHING PURSUIT
Jason M. Klusowski
Department of Operations Research
and Financial Engineering
Princeton University
Princeton, NJ 08544
jason.klusowski@princeton.edu
Jonathan W. Siegel
Department of Mathematics
Texas A&M University
College Station, TX 77843
jwsiegel@tamu.edu
July 26, 2023
ABSTRACT
We study the fundamental limits of matching pursuit, or the pure greedy algorithm, for approximating
a target function by a sparse linear combination of elements from a dictionary. When the target
function is contained in the variation space corresponding to the dictionary, many impressive works
over the past few decades have obtained upper and lower bounds on the error of matching pursuit,
but they do not match. The main contribution of this paper is to close this gap and obtain a sharp
characterization of the decay rate of matching pursuit. Specifically, we construct a worst case
dictionary which shows that the existing best upper bound cannot be significantly improved. It turns
out that, unlike other greedy algorithm variants, the converge rate is suboptimal and is determined
by the solution to a certain non-linear equation. This enables us to conclude that any amount of
shrinkage improves matching pursuit in the worst case.
1
Introduction
Matching pursuit [17] is a widely used algorithm in signal processing that approximates a target signal by selecting a
sparse linear combination of elements from a given dictionary.
Over the years, matching pursuit has garnered significant attention due to its effectiveness in capturing essential features
of a signal with a parsimonious representation, offering reduced storage requirements, efficient signal reconstruction,
and enhanced interpretability of the underlying signal structure. Because of this, its applications span various domains,
including image, video, and audio processing and compression [2,19].
While previous works have explored the convergence properties of matching pursuit, several open questions and
challenges remain. In particular, the relationship between the characteristics of the target signal, the chosen dictionary,
and the convergence rate warrants further investigation. The main objective of this paper is to provide a comprehensive
analysis of the convergence properties of matching pursuit. Understanding the convergence rate is crucial for assessing
the algorithm’s efficiency and determining the number of iterations required to achieve a desired level of approximation
accuracy.
Let H be a Hilbert space and D ⊂H be a symmetric collection of unit vectors, i.e., ∥d∥= 1 for d ∈D and d ∈D implies
−d ∈D, called a dictionary. Non-linear dictionary approximation methods, which attempt to approximate a target
function f by a sparse linear combination
f ≈
s
∑
n=1
andn,
(1)
where both the sequence of dictionary elements di ∈D and the coefficients ai depend upon the function f to be
approximated, are common method in machine learning and signal processing. Such methods aim to generate an
approximation of the form (1) with a small number of terms s, and include gradient boosting [9], L2-boosting [4], basis
pursuit [5], and matching pursuit [17].
arXiv:2307.07679v2  [stat.ML]  25 Jul 2023

A PREPRINT - JULY 26, 2023
In this work, we consider matching pursuit [17], which is a classical method for algorithmically generating a convergent
non-linear dictionary expansion of the form
f =
∞
∑
n=1
andn.
(2)
Matching pursuit is also known as the pure greedy algorithm [7], and is given by
f0 = 0, dn ∈argmax
d∈D
⟨rn−1,d⟩, fn = fn−1 −⟨rn−1,dn⟩dn,
(3)
where rn = f −fn is the residual at step n. An equivalent way of writing this method, which explains the name pure
greedy algorithm, is
(an,dn) = argmin
a∈R,d∈D
f −
 
n−1
∑
i=1
aidi +ad
! = argmin
a∈R,d∈D
∥rn−1 −ad∥.
(4)
In each step we add the single term which minimizes the error the most, hence the name pure greedy algorithm. In
other words, we fit a single term to the residual in each step.
An important variant of this algorithm is the pure greedy algorithm which shrinkage 0 < s ≤1, see [29, p. 375], given
by
f0 = 0, dn ∈argmax
d∈D
⟨rn−1,d⟩, fn = fn−1 −s⟨rn−1,dn⟩dn.
(5)
Here we scale down the greedy term by a factor s, called the shrinkage factor, in each step.
Finding the optimal term dn in (5) could potentially involve solving a high-dimensional, non-convex optimization
problem. To address this computational issue, gradient tree boosting [9] (i.e., L2 boosting) features an additional step in
which ⟨rn−1,d⟩is itself greedily optimized. Here the dictionary consists of normalized piecewise constant functions (or
decision trees), which are fit to the residuals rn−1 via the CART algorithm [3]. While CART is usually not motivated as
a greedy way to optimize the inner product ⟨rn−1,d⟩over a collection of normalized piecewise constant functions, it
can be equivalently formulated as such. To begin, we first find the pair (t, j) ∈R×N such that ⟨rn−1,ψ⟩is maximized
among all functions, or decision stumps, of the form
ψ(x) = b11(xj ≤t)+b21(x j > t),
where b1 and b2 are chosen so that ∥ψ∥2 = 1 and ⟨ψ,1⟩= 0. The splitting of x along the j-th direction, xj, at the split
point t creates a partition of the x-space, or the root node, into two subcells, or child nodes, namely, R = {x : xj ≤t}
and R′ = {x : xj > t}. Next, we perform the same step as before, but this time we restrict x to be in either R or R′, in
turn. More generally, we divide a cell R, or a parent node, into two subcells, or child nodes, by finding (t, j) such that
⟨rn−1,ψ⟩is maximized among all functions of the form ψ(x) = b11(xj ≤t, x ∈R)+b21(xj > t, x ∈R), where b1 and
b2 are chosen so that ∥ψ∥2 = 1 and ⟨ψ,1⟩= 0. This process is terminated when a desired level of granularity in the
partition (or a certain number of leaf nodes) is reached, yielding a collection of orthonormal functions Ψ, including the
constant function ψ ≡1. The decision tree output dn ∈D admits the orthogonal expansion
dn(x) ∝∑
ψ∈Ψ
⟨rn−1,ψ⟩ψ(x), where ⟨rn−1,dn⟩=
r
∑
ψ∈Ψ
|⟨rn−1,ψ⟩|2.
Except for decision stumps (or depth-one trees) which involve only a 2-dimensional optimization, understanding the
optimization gap for this greedy heuristic (CART) in general is currently an open problem, one that we do not address
in the present paper. That is, we assume herein that the optimization problem is solved exactly in (5).
Despite the practical success and great interest generated by the method of matching pursuit, the precise convergence
properties of the algorithm (3) for general dictionaries D have not been precisely determined. To describe this problem,
we introduce the variation norm [6,13] with respect the the dictionary D, defined by
∥f∥K1(D) = inf{t > 0 : f ∈tB1(D)},
(6)
where the set B1(D) is the closed convex hull of D, i.e.
B1(D) =
(
N
∑
i=1
aidi, ai ≥0,
N
∑
i=1
ai = 1, di ∈D
)
.
(7)
This norm is a common measure of complexity when studying non-linear dictionary approximation, since it is well-
defined and useful for any abstract dictionary D ⊂H [6,29]. Results for the variation space K1(D) can also often be
extended to interpolation spaces which allow an even wider class of target functions to be analyzed [1].
2

A PREPRINT - JULY 26, 2023
For some context, let us give a few examples of dictionaries D of interest and their corresponding variation spaces. For
specific dictionaries of interest, the variation spaces have often been characterized and other descriptions are available,
for instance the Besov space B1
1,1 is equivalent to K1(D) for the dictionary
D = {e(x−c)2/(2σ2), c ∈R, σ > 0}
(8)
of Gaussian bumps [18]. Another example are the spaces K1(D) for the dictionaries of ridge functions
D = {σ(ω ·x+b), ω ∈Sd−1, b ∈R}
(9)
which correspond to shallow neural networks have been characterized and intensively studied [8,11,21–23,25,27].
Here σ = max(0,x)k is the popular ReLUk activation function.
Typically the convergence of abstract greedy algorithms is studied on the variation space K1(D) [29]. For instance, it is
shown in [7] that the pure greedy algorithm (3) satisfies
∥f −fn∥≤∥f∥K1(D)n−1/6.
(10)
This was subsequently improved by Konyagin and Temylakov [12] to
∥f −fn∥≤C∥f∥K1(D)n−11/62,
(11)
and finally by Sil’nichenko [28] to
∥f −fn∥≤C∥f∥K1(D)n−α,
(12)
where α = γ/(2(2+γ)) ≈0.182 and γ > 1 is a root of the non-linear equation
(1+γ)
1
2+γ

1+
1
1+γ

−1−1
γ = 0.
(13)
For the pure greedy algorithm with shrinkage (5) the method of Sil’nichenko implies that
∥f −fn∥≤C∥f∥K1(D)n−α,
(14)
where α = γ/(2(2+γ)) and γ > 1 is a root of the non-linear equation
(1+γ)
1
2+γ

1+
1
1+γ

−1−2−s
γ
= 0.
(15)
As s →0, the exponent α ≈0.305 (c.f., the exponent 2/7 ≈0.285 obtained in [20]). On the other hand, for functions
f ∈K1(D) it is known [10,24] that for each n there exists an approximation
fn =
n
∑
i=1
aidi,
(16)
such that ∥f −fn∥≤∥f∥K1(D)n−1/2, and that the exponent n−1/2 is optimal for all dictionaries and f ∈K1(D) [13].
Thus, in general the best we could hope for in the convergence of matching pursuit is a convergence rate like n−1/2,
which is attained by other greedy algorithms such as the orthogonal or related greedy algorithm [7,10] (in fact, the
orthogonal greedy algorithm has been shown to converge even faster for compact dictionaries [14,26]). Remarkably,
the convergence of matching pursuit is strictly worse. Specifically, it was shown in [16] that there exists a dictionary
D ⊂H and an f ∈K1(D) such that the iterates of the pure greedy algorithm (3) satisfy
∥f −fn∥≥C∥f∥K1(D)n−0.27.
(17)
This estimate was finally improved in [15] to
∥f −fn∥≥C∥f∥K1(D)n−0.1898.
(18)
Comparing the estimates (12) and (18) we see that there is a still a significant, though small, gap between the best upper
and lower bounds. The goal of this work is to close this gap nearly completely. We show the following.
Theorem 1. Let γ > 1 be the root of the equation (13). Then for every α > γ/(2(2 + γ)), there exists a dictionary
D ⊂H and a function f ∈K1(D) such that the iterates of the pure greedy algorithm (i.e., matching pursuit) (3) satisfy
∥f −fn∥≥C∥f∥K1(D)n−α.
(19)
3

A PREPRINT - JULY 26, 2023
Combined with the upper bound (12), this gives a precise characterization of the exponent in the convergence rate
of matching pursuit and therefore makes significant progress towards solving an open problem posed in [29], that is,
to find the order of decay of the pure greedy algorithm. Additionally, this shows that the rate of convergence with
shrinkage s < 1 is strictly better than for s = 1, i.e., that any amount of shrinkage improves the algorithm in the worst
case, lending theoretical support to the empirical observation that some shrinkage in gradient tree boosting can result in
significant performance improvements.
Although we do not give all of the details, our method shows that the rate obtained by Sil’nichenko (14) is optimal also
for shrinkage s0 < s < 1 for a numerically computable value s0. However, for sufficiently small shrinkage s our method
breaks down and it is an open problem to determine the rate of convergence as s →0.
2
Construction of the Worst-Case Dictionary
In this section, we give the proof of Theorem 1. This is based upon an extension and optimization of the constructions
developed in [15,16].
2.1
Basic Construction
We let H = ℓ2 and denote by {en}∞
n=1 its standard basis. Let 0 < β < 1
2. We will attempt to construct a realization of
the pure greedy algorithm (3), i.e., a dictionary D ⊂H and an initial iterate f ∈K1(D), for which the convergence rate
is bounded below by
∥f −fn∥≥Cn−1
2 +β.
We will show that this construction succeeds as long as

β
1−β
β (1−β)2
1−2β

< 1.
(20)
Let us begin by verifying that this implies Theorem 1. It suffices to show that with equality in (20), we have
α = 1
2 −β =
γ
(2(2+γ)),
(21)
where γ is the root of (13). Solving the relation (21) for γ in terms of β we get
γ = 1−2β
β
.
(22)
Plugging this into (13), we readily see that (13) is equivalent to equality in (20).
Let us proceed with the lower bound construction. It will be convenient in what follows to work with the residual form
of the pure greedy algorithm, given by
r0 = f0, dn ∈argmin
d∈D
⟨rn−1,d⟩, rn = rn−1 −⟨rn−1,dn⟩dn.
(23)
To begin the construction, let K > 1 be an integer to be determined later. Let {hn}∞
n=K ⊂H be a sequence of elements
of H which satisfy hn ∈span(e1,...,en−1), which are a parameter of the construction that we leave unspecified for now.
Define two sequences of elements {rn}∞
n=K−1 ⊂H and {dn}∞
n=K ⊂H inductively by
rK−1 = −
CK
√K −1
K−1
∑
i=1
ei,
dn = γnrn−1 +hn +ξnen,
rn = rn−1 −qndn.
(24)
Here CK,γn,ξn and qn are parameters, which are chosen so that
∥rn∥= (n+1)−1
2 +β, qn = ⟨rn−1,dn⟩, and ∥dn∥= 1.
(25)
These conditions immediately determine CK and qn. Specifically, we calculate CK = ∥rK−1∥= K−1
2 +β and
∥rn∥2 = ∥rn−1∥2 −q2
n.
(26)
4

A PREPRINT - JULY 26, 2023
This implies that
q2
n = ∥rn−1∥2 −∥rn∥2 = n−1+2β −(n+1)−1+2β.
(27)
Further, we set γn = (q−1
n −q−1
n−1). The conditions (25) and the construction (24) imply that
qn = ⟨rn−1,gn⟩= γn∥rn−1∥2 +⟨rn−1,hn⟩= γnn−1+2β +⟨rn−1,hn⟩,
(28)
which means that we must ensure that our choice of hn satisfies
⟨rn−1,hn⟩= qn −γnn−1+2β.
(29)
Next, since we require that hn ∈span(e1,...,en−1), we see by induction that dn,rn ∈span(e1,...,en−1). Using the
construction (24) this implies that
1 = ∥dn∥2 = ∥γnrn−1 +hn∥2 +ξ 2
n .
(30)
This means that our parameter hn must satisfy ∥γnrn−1 + hn∥≤1. We now solve equation (30) for ξn and choose
ξn =
p
1−∥γnrn−1 +hn∥2 to be the positive root.
Finally, we choose parameters N > K sufficiently large and ε > 0 sufficiently small, and set
f = rN,
˜dN = ε rN
∥rN∥+
p
1−ε2dN,
D = { ˜dN}∪{dn}n≥N.
(31)
From (24) and (25), we see that ⟨rN,dN⟩= ⟨rN−1 −qNdN,dN⟩= 0. This implies that ∥˜dN∥= 1 and that
⟨f, ˜dN⟩= ε∥rN∥.
(32)
It is also clear that f ∈span( ˜gN,gN), so that ∥f∥K1(D) < ∞.
Note that everything has been determined up until this point except the sequence hn and the parameters K,N, and
ε. The proof will proceed as follows. First, we will choose a sequence hn which is parameterized by a function φ.
Then, we will show that for sufficiently large K conditions (29) and (30) are satisfied, which implies that the recursive
construction (24) can be carried out. Next, we will derive conditions on φ which guarantee that for sufficiently large N
and sufficiently small ε we have
|⟨rn−1, ˜dN⟩| < qn and |⟨rn−1,dk⟩| < qn
(33)
for each n > N and k ≥N with k ̸= n. This implies that rn for n ≥N is a realization of the pure greedy algorithm (23)
with step size s for the dictionary D and the target function f. Finally, we will optimize the parameter β subject to the
existence of a function φ which satisfies the required conditions.
2.2
Inner Product Formulas
We begin by deriving formulas for the inner products in (33), following [15,16]. Consider the case k < n. Combining
the last two equations in (24) we see that
rm = (1−qmγm)rm−1 −qmhm −qmξmem.
(34)
Plugging the choice γm = q−1
m −q−1
m−1 into this, we get
rm =
qm
qm−1
rm−1 −qmhm −qmξmem.
(35)
Taking the inner product with dk, noting that dk ∈span(e0,...,ek), and assuming that k < m, we get
⟨rm,dk⟩=
qm
qm−1
⟨rm−1,dk⟩−qm⟨hm,dk⟩.
(36)
Using (25) and the last line in (24), we see that by construction ⟨rk,dk⟩= 0. From this and the preceding equation we
see by induction on m that
⟨rn−1,dk⟩= −qn−1
 
n−1
∑
i=k+1
⟨hi,dk⟩
!
(37)
for k < n.
5

A PREPRINT - JULY 26, 2023
The case of ⟨rn−1, ˜dN⟩is a bit more complicated. We use the relation (32) as a base case in the induction to get
⟨rn−1, ˜dN⟩= −qn−1
 
n−1
∑
i=N+1
⟨hi, ˜dN⟩−ε ∥rN∥
qN
!
.
(38)
Next, we consider the case k > n. In this case the relations (24) give, for k > n
⟨rn−1,dk⟩= γk⟨rn−1,rk−1⟩+⟨rn−1,hk⟩,
(39)
since rn−1 ∈span(e0,...,en−1). Further, we see that
⟨rn−1,rk−1⟩= ⟨rn−1,rk−2⟩−qk−1⟨rn−1,dk−1⟩.
(40)
Shifting the indices in (39) we also get
⟨rn−1,dk−1⟩= γk−1⟨rn−1,rk−2⟩+⟨rn−1,hk−1⟩.
(41)
Solving this equation for ⟨rn−1,rk−2⟩and plugging that back into equations (39) and (40), we get the inductive relation
⟨rn−1,dk⟩= (γ−1
k−1 −qk−1)γk⟨rn−1,dk−1⟩+

rn−1,hk −γk
γk−1
hk−1

.
(42)
2.3
Initial Asymptotics
Our goal will be to show that the construction detailed in (31) is valid and satisfies the conditions (33) for sufficiently
large K,N and sufficiently small ε. For this purpose, we will evaluate asymptotics of some of the parameters introduced
so far, which we collect here. These asymptotics were first derived in [15,16].
We use the notation from [15]. In particular, for a sequence yn we write yn ≂bnα if yn = bnα(1+c/n+O(n−2)) for
some c. We have from (27) that
q2
n ≂(1−2β)n−2+2β.,
(43)
which implies that
qn ≂
p
1−2βnβ−1.
(44)
From this, we obtain the following asymptotics for γn,
γn = (q−1
n −q−1
n−1) ≂(1−β)
p
1−2β
n−β(1+O(n−1)).
(45)
These two formulas give us the asymptotics of the right hand side of (29)
qn −γnn−1+2β =
 p
1−2β −(1−β)
p
1−2β
!
nβ−1(1+O(n−1)) =
−β
p
1−2β
nβ−1(1+O(n−1)).
(46)
Finally, we will also need the asymptotics for the inductive factor in (42)
(γ−1
m−1 −sqm−1)γm =
γm
γm−1
(1−sγm−1qm−1) =
γm
γm−1
qm−1
qm−2
.
(47)
The asymptotics for γm (45) and the asymptotics for qn (44) imply that
γm
γm−1
=

1−β
m +o(m−1)

, qm−1
qm−2
=

1+ β −1
m
+o(m−1)

.
(48)
Multiplying these, we get
(γ−1
m−1 −sqm−1)γm = (1−(1+om(1))m−1).
(49)
Note that here and in the following, we use the notation ok(1) to denote a quantity depending upon the parameter k
whose limit is 0 as k →∞. We also adopt the convention that the constants in big-O expressions and the rate of decay to
0 in little-o expressions will be uniform in any lower case indices n,m,k, or l which do not appear in the argument (or
the subscript if the argument is 1), but may depend upon other parameters of the construction.
Finally, we recall the basic fact, which we will use without reference in the following, that (using this convention)
1
n
n
∑
k=1
ok(1) = on(1).
(50)
6

A PREPRINT - JULY 26, 2023
2.4
Choice of Main Parameter
So far we have specified all parameters in the construction except the sequence hn, which we proceed to describe in this
section. Recalling the preceding construction, in doing so we must ensure that hn ∈span(e0,...,en−1), that condition
(29) is satisfied, and that ∥γnrn−1 +hn∥≤1 in order to define ξn via (30).
Let φ : [0,1] →R≥0 be a non-zero C∞function such that for some δ > 0, we have φ(x) = 0 on [0,δ]. Define hn by the
relation
hn =
n−1
∑
i=1
αn
n φ
 i
n

ei.
(51)
Here the parameters αn are chosen so that the relation (29) is satisfied, i.e., such that ⟨rn−1,hn⟩= qn −γnn−1+2β.
We must ensure that (51) and (29) can always be solved for αn and that the resulting value satisfies ∥γnrn−1 +hn∥≤1,
so that we can define ξn via (30). We proceed to show that for sufficiently large K, this will always be the case. For this,
we will need the following formula for the coefficients of rn, which will also be useful later.
Lemma 1. The following formula holds for K ≤k ≤n
⟨rn,ek⟩= −sqn
 
ξk +
n
∑
j=k+1
⟨hj,ek⟩
!
.
(52)
Further, for 0 < k ≤K −1 ≤n, we have
⟨rn,ek⟩= −qn
 
CK−1
qK−1
√K −1 +
n
∑
j=K
⟨hj,ek⟩
!
.
(53)
Since we also have ⟨rn,ek⟩= 0 for k > n, this lemma gives the components of all of the iterates rn.
Proof. We prove this formula for any fixed k > 0 by induction on n. The base case follows in the case where k ≥K
from
⟨rk,ek⟩= ⟨rk−1 −qkdk,ek⟩= −qk⟨gk,ek⟩= −qkξk
(54)
since rk−1,hk ∈span(e0,...,en−1). For k ≤K −1, we use the definition of rK−1 to get (53) in the case n = K −1.
The inductive step follows from (35) since
⟨rn,ek⟩=
qn
qn−1
⟨rn−1,ek⟩−qn⟨hn,ek⟩.
(55)
Using the inductive hypothesis, this gives
⟨rn,ek⟩= −qn
qn−1
qn−1
 
ξk +
n−1
∑
j=k+1
⟨h j,ek⟩
!
−qn⟨hn,ek⟩= −qn
 
ξk +
n
∑
j=k+1
⟨hj,ek⟩
!
,
(56)
when k ≥K, and similarly when k ≤K −1.
Using this, we prove the following fundamental result showing that the given choice of hn results in a valid construction
for sufficiently large K.
Proposition 1. Let φ : [0,1] →R≥0 be a non-zero C∞function. Then for sufficiently large K, we will be able to solve
equations (51) and (29) for αn. In addition, αn will be non-negative and that the resulting value of hn will satisfy
∥γnrn−1 +hn∥≤1, so that we can define ξn via (30). Moreover, the construction will also satisfy
lim
n→∞ξn = 1.
(57)
Proof. Note that since φ ̸= 0, we have
Z 1
0 φ(x)dx = Cφ > 0.
(58)
7

A PREPRINT - JULY 26, 2023
We begin by calculating the inner product ⟨rn−1,hn⟩for n ≥K using Lemma 1 and the definition of hn in (51). We get
⟨rn−1,hn⟩=
n−1
∑
k=1
⟨rn−1,ek⟩⟨hn,ek⟩=−qn−1αn
n
 
n−1
∑
k=1
n−1
∑
j=max(k+1,K)
αj
j φ
k
n

φ
k
j

+
CK−1
qK−1
√K −1
K−1
∑
k=1
φ
k
n

+
n−1
∑
k=K
ξkφ
k
n
!
.
(59)
Next, since CK−1 = K−1
2 +β, we use the asymptotics (44) to see that
lim
K→∞
CK−1
qK−1
√K −1 =
1
p
1−2β
> 1.
(60)
In addition, by assumption φ is Riemann integrable. Thus we have
lim
K→∞
1
K
K−1
∑
k=1
φ
 k
K

= Cφ.
(61)
Using these facts, that ∥rn−1∥= n−1
2 +β by construction, and the asymptotics in (44), (45) and (46), we see that we can
choose K large enough such that the following conditions hold:
1.
CK−1
qK−1
√K−1 ≥1
2.
2. For some Cq > 0 and any n ≥K, we have qn ≥Cqnβ−1.
3. For some Cγ > 0 and any n ≥K, we have 0 ≥qn −γnn−1+2β ≥−Cγnβ−1.
4. For any n ≥K, we have
1
n
n−1
∑
k=1
φ
k
n

≥Cφ
2 .
(62)
5. For any n ≥K, we have
γn∥rn−1∥+ 4Cγ
CφCq
"
sup
x∈[0,1]
φ(x)
#
n−1
2 ≤
√
3
2 .
(63)
We proceed to show by induction on n ≥K that we can solve (51) and (29) for αn ≥0, and that the resulting hn will
satisfy (29) and ∥γnrn−1 +hn∥≤
√
3
2 , which implies that for the ξn satisfying (30) we will have ξn ≥1
2.
So assume inductively that for all K ≤k < n we have ξk ≥1
2 and αk ≥0. (Note that in the base case n = K there is
nothing to prove.) Since φ ≥0, we use the inductive assumption on αk to remove the initial double sum on the right
hand side of (59) and get the inequality
⟨rn−1,hn⟩≤−qn−1αn
n
 
CK−1
qK−1
√K −1
K−1
∑
k=1
φ
k
n

+
n−1
∑
k=K
ξkφ
k
n
!
.
(64)
We see from (64), the inductive hypothesis on ξk, and conditions 1, 2, and 4 that
⟨rn−1,hn⟩≤−αn
CφCq
4
.
(65)
This, combined with condition 3 implies that we solve (51) and (29) for αn which satisfies the bound
0 ≤αn ≤4Cγ
CφCq
.
(66)
Next, we estimate, using the definition of hn (51),
∥γnrn−1 +hn∥≤γn∥rn−1∥+∥hn∥≤γn∥rn−1∥+αn
"
sup
x∈[0,1]
φ(x)
#
n−1
2 .
(67)
8

A PREPRINT - JULY 26, 2023
Finally, using condition 5 and (66), we get that
∥γnrn−1 +hn∥≤
√
3
2 ,
(68)
which completes the inductive step. Moreover, the bound on αn (66) and the estimate (67) imply that
lim
n→∞∥γnrn−1 +hn∥= 0,
(69)
from which it follows that limn→∞ξn = 1. This completes the proof.
Finally, we will need an estimate relating sums to integrals of φ in the following, which we collect here.
Lemma 2. Suppose that φ : [0,1] →R≥0 is a C∞function such that for some δ > 0, we have φ(x) = 0 on [0,δ]. Let
n ≥l ≥k, then we have
n−1
∑
j=l
1
j φ
k
j

=
Z
k
l
k
n
φ(x)dx
x +O(k−1).
(70)
Recall that by our convention the constant in O(k−1) only depends upon φ and not upon n and l.
Proof. Consider the sequence of n−l points
x0 := k
n < x1 :=
k
n−1 < ··· < xn−l−1 :=
k
l +1 < xn−l := k
l .
(71)
This sequence of points forms a partition ∆of the interval [ k
n, k
l ] and the gap (or mesh/norm) of ∆satisfies
|∆| := sup
i
|xi+1 −xi| ≤k
l −
k
l +1 =
k
l(l +1) < 1
k .
(72)
Moreover, we have the following estimate
log
k
j

−log

k
j +1

= log
 j +1
j

= log

1+ 1
j

= 1
j +O
 1
j2

.
(73)
Since φ is bounded, we obtain
n−1
∑
j=l
1
j φ
k
j

=
n−l
∑
i=1
φ
k
j

log
k
j

−log

k
j +1

+O
 
n−1
∑
j=l
1
j2
!
=
n−l
∑
i=1
φ(xi)[log(xi)−log(xi−1)]+O(k−1).
(74)
Finally, since φ is smooth and log(x) is increasing and bounded on [δ,1], we obtain, by comparing with the Riemann-
Stieltjes integral and using that the mesh ∆satisfies |∆| < k−1,
n−l
∑
i=1
φ(xi)[log(xi)−log(xi−1)] =
Z
k
l
k
n
φ(x)d log(x)+O(k−1) =
Z
k
l
k
n
φ(x)dx
x +O(k−1).
(75)
2.5
Main Estimates
In the following analysis, we assume that K has been chosen sufficiently large so that the conclusion of Proposition 1
holds.
Similar to the argument given in [15], the next step will be to study the asymptotics of the sequence αn. We also give an
asymptotic formula for the coefficients of rn.
Proposition 2. Suppose that φ satisfies
Z 1
0 φ(x)

1+
Z 1
x φ(z)dz
z

dx

=
β
1−2β .
(76)
9

A PREPRINT - JULY 26, 2023
Then, in the preceding construction, with K chosen large enough such that the conclusion of Proposition 1 holds, the
sequence αn in (51) will satisfy
lim
n→∞αn = 1.
(77)
Moreover, the sequence αn will satisfy
lim
n→∞
αn+1
αn
= 1+o(n−1).
(78)
In addition, under these conditions as k →∞we will have the following asymptotics for the coefficients of rn
⟨rn,ek⟩= −qn

1+
Z 1
k
n
φ(z)dz
z +ok(1)

(79)
for any n ≥k. (Recall that by our convention ok(1) goes to 0 as k →∞uniformly in n.)
Proof. The proof is quite similar to the proof of Lemma 2 in [15]. Suppose that
liminf
n→∞αn ≥α0.
(80)
Using the formula (59), we see that
liminf
n→∞
−⟨rn−1,hn⟩
qn−1αn
≥
 
α0 lim
n→∞n
"
n−1
∑
k=1
n−1
∑
j=max(k+1,K)
1
j φ
k
n

φ
k
j
#
+ lim
n→∞
1
n
"
CK−1
qK−1
√K −1
K−1
∑
k=1
φ
k
n

+
n−1
∑
k=K
ξkφ
k
n
#!
.
(81)
We proceed to evaluate the limits in the above equation. Using the fact that φ is Riemann integrable, that K is fixed and
that ξk →1 as k →∞, we easily see that
lim
n→∞
1
n
"
CK−1
qK−1
√K −1
K−1
∑
k=1
φ
k
n

+
n−1
∑
k=K
ξkφ
k
n
#
=
Z 1
0 φ(x)dx.
(82)
Further, applying Lemma 2 to φ, we see that
n−1
∑
j=max(k+1,K)
1
j φ
k
j

=
Z min(1, k
K )
k
n
φ (z) dz
z +O(k−1).
(83)
From this, by comparing a Riemann sum with an integral, we see that
lim
n→∞
1
n
"
n−1
∑
k=1
n−1
∑
j=max(k+1,K)
1
j φ
k
n

φ
k
j
#
= lim
n→∞
1
n
"
n−1
∑
k=1
φ
k
n
"Z min(1, k
K )
k
n
φ (z) dz
z
#
+O
 
n−1
∑
k=1
k−1
!#
= lim
n→∞
"
1
n
n−1
∑
k=1
φ
k
n
"Z min(1, k
K )
k
n
φ (z) dz
z
#
+O
logn
n
#
= lim
n→∞
"
1
n
n−1
∑
k=1
φ
k
n
Z 1
k
n
φ (z) dz
z

−1
n
K
∑
k=1
φ
k
n
Z 1
k
K
φ(z)dz
z
#
=
Z 1
0 φ(x)
Z 1
x φ(z)dz
z

dx,
(84)
since for large enough n, φ(k/n) = 0 for k = 1,...,K (recall that φ is assumed to vanish on [0,δ] for some δ).
Plugging this into (81), we get
liminf
n→∞
−⟨rn−1,hn⟩
qn−1αn
≥
Z 1
0 φ(x)

1+α0
Z 1
x φ(z)dz
z

dx

.
(85)
Utilizing the asymptotics (44) and (46), we obtain that
lim
n→∞
−⟨rn−1,hn⟩
qn−1
=
β
1−2β .
(86)
10

A PREPRINT - JULY 26, 2023
Together with (85), this implies that
limsup
n→∞
αn ≤

β
1−2β
Z 1
0 φ(x)

1+α0
Z 1
x φ(z)dz
z

dx
−1
.
(87)
In an entirely analogous manner, we prove that limsupn→∞αn ≤α0 implies that
liminf
n→∞αn ≥

β
1−2β
Z 1
0 φ(x)

1+α0
Z 1
x φ(z)dz
z

dx
−1
.
(88)
We proceed to use the same fixed point argument from [15] to complete the proof. Define Fφ,s by
Fφ,s(α) =

β
1−2β
Z 1
0 φ(x)

1+α
Z 1
x φ(z)dz
z

dx
−1
.
(89)
Proposition 1 implies that liminfn→∞αn ≥0. Iterating the relations (87) and (88) we see that for any n ≥1
limsup
n→∞
αn ≤F2n−1
φ,s
(0), liminf
n→∞αn ≥F2n
φ,s(0).
(90)
Note here that the exponent represents composition. Thus (77) will be proved if we can show that limn→∞Fn
φ,s(0) = 1.
To see this, rewrite Fφ,s as
Fφ,s(α) =
A
B+Cα ,
(91)
where A =
β
1−2β > 0, B =
R 1
0 φ(x)dx > 0, and C =
R 1
0 φ(x)
R 1
x φ(z) dz
z

dx > 0.
Further, the assumption (76) implies that Fφ,s(1) = 1, i.e., that A = B+C. We wish to show that iterating the map Fφ,s
converges to this fixed point. This follows from the following simple calculation
|F2
φ,s(α)−1| =

C2
C2 +B2 +BCα +BC(α −1)
 ≤
C2
C2 +B2 |α −1|,
(92)
since
C2
C2+B2 < 1. This completes the proof of (77).
Next, we prove the asymptotic formula (79). We may assume that k > K and use Lemma 1 and the fact that ξk
approaches 1, see Proposition 1, to get
⟨rn,ek⟩= −qn
 
1+
n
∑
j=k+1
⟨h j,ek⟩+ok(1)
!
.
(93)
Using the definition of hn (51), we get
⟨rn,ek⟩= −qn
 
1+
n
∑
j=k+1
αj
j φ
k
j

+ok(1)
!
= −qn
 
1+
n
∑
j=k+1
1+ok(1)
j
φ
k
j

+ok(1)
!
.
(94)
Using Lemma 2 and noting that
R 1
0
φ(z)
z dz < ∞(since φ = 0 on [0,δ]), we get
⟨rn,ek⟩= −qn

1+
Z 1
k
n
φ(z)dz
z +ok(1)

,
(95)
as desired.
Finally, we will prove (78). For this, we note that (29), combined with the asymptotics (46) implies that
⟨rn,hn+1⟩
⟨rn−1,hn⟩= 1+ β −1
n
+o(n−1).
(96)
Using the formula (35) to rewrite this as
⟨rn,hn+1⟩
⟨rn−1,hn⟩=
⟨qn
qn−1 rn−1 −qnhn −qnξnen,hn+1⟩
⟨rn−1,hn⟩
=
qn
qn−1
⟨rn−1,hn+1⟩
⟨rn−1,hn⟩−qn
⟨hn +ξnen,hn+1⟩
⟨rn−1,hn⟩
(97)
11

A PREPRINT - JULY 26, 2023
Using the asymptotics (44) and (46), we see that
qn
⟨rn−1,hn⟩= c′ +O(n−1)
(98)
for a constant c′. In addition, using the definition of hn, the fact that ξn →1, and that φ is smooth, we see that
⟨hn +ξnen,hn+1⟩=
1
n+1
 
1
n
n−1
∑
i=1
φ
 i
n

φ

i
n+1

+φ

n
n+1

+on(1)
!
= c′′n−1 +o(n−1),
(99)
for a constant c′′ = φ(1)+
R 1
0 φ 2(x)dx. This means that the last term in (97) is
qn
⟨hn +ξnen,hn+1⟩
⟨rn−1,hn⟩
= cn−1 +o(n−1),
(100)
for a constant c.
Now, we calculate, using the definition of hn,
⟨rn−1,hn⟩=
n−1
∑
k=1
⟨rn−1,ek⟩⟨hn,ek⟩= αn
n
n−1
∑
k=1
⟨rn−1,ek⟩φ
k
n

.
(101)
Similarly, we obtain
⟨rn−1,hn+1⟩=
n−1
∑
k=1
⟨rn−1,ek⟩⟨hn,ek⟩= αn+1
n+1
n−1
∑
k=1
⟨rn−1,ek⟩φ

k
n+1

.
(102)
Next, we observe that
n−1
∑
k=1
⟨rn−1,ek⟩φ

k
n+1

=
n−1
∑
k=1
⟨rn−1,ek⟩φ
k
n

+
n−1
∑
k=1
⟨rn−1,ek⟩

φ
k
n

−φ

k
n+1

.
(103)
Using that φ is smooth, we see that
n−1
∑
k=1
⟨rn−1,ek⟩

φ
k
n

−φ

k
n+1

=
n−1
∑
k=1
⟨rn−1,ek⟩

k
n(n+1)φ ′
k
n

+O
k2
n4

.
(104)
Applying the asymptotics (79) and comparing the Riemann sum with an integral, we get
n−1
∑
k=1
⟨rn−1,ek⟩

φ
k
n

−φ

k
n+1

= −qn−1
Z 1
0 xφ ′(x)
Z 1
x φ(z)dz
z dx+on(1)

.
(105)
Similarly, we obtain
n−1
∑
k=1
⟨rn−1,ek⟩φ
k
n

= −qn−1n
Z 1
0 φ(x)
Z 1
x φ(z)dz
z dx+on(1)

.
(106)
Using all of this, we obtain
⟨rn−1,hn+1⟩
⟨rn−1,hn⟩
= αn+1
αn
n
n+1
 
1+n−1
R 1
0 xφ ′(x)
R 1
x φ(z) dz
z dx+on(1)
R 1
0 φ(x)
R 1
x φ(z) dz
z dx+on(1)
!
.
(107)
Since φ ̸= 0 is non-negative, the integral in the denominator above is non-zero and we obtain
⟨rn−1,hn+1⟩
⟨rn−1,hn⟩
= αn+1
αn
n
n+1
 1+cn−1 +o(n−1)

(108)
for a constant c. Using that
n
n+1 = 1−n−1 +o(n−1) and combining the above estimates with (97) and (96), we get
αn+1
αn
= 1+κn−1 +o(n−1),
(109)
for some constant κ. Finally, since
T
∏
k=n
αk+1
αk
= αT+1
αn
(110)
is uniformly bounded in T (since the sequence αk converges to 1), we must in fact have κ = 0, which completes the
proof.
12

A PREPRINT - JULY 26, 2023
Next, we determine conditions on φ which guarantee that for sufficiently large N, the conditions (33) will be satisfied.
We consider first the case k > n. We have the following result.
Proposition 3. Suppose that φ satisfies the condition of Proposition 2 and also the inequality
sup
a∈[0,1]

Z a
0 (φ ′(x)x−(β −1)φ(x))

1+
Z 1
a−1x φ(z)dz
z

dx
 < 1.
(111)
Then there is a sufficiently large N such that for k > n > N we have |⟨rn−1,gk⟩| < qn.
Proof. We utilize the formula (42). If we can show that for some δ > 0, we have


rn−1,hk −γk
γk−1
hk−1
 < (1−δ)qnk−1
(112)
for every k > n > N, then the asymptotics (49) combined with the base case ⟨rn−1,gn⟩= qn (which holds by construction)
will imply the desired result by induction.
Define pk = hk −
γk
γk−1 hk−1. From the definition of hk, we see that
⟨pk,em⟩= αk
k φ
m
k

−γk
γk−1
αk−1
k −1φ
 m
k −1

.
(113)
Using (78), (77), and that φ is bounded, we get
⟨pk,em⟩= (1+ok(1))
1
k φ
m
k

−γk
γk−1
1
k −1φ
 m
k −1

+o(k−2).
(114)
We rewrite the term in parenthesis as
1
k −γk
γk−1
1
k −1

φ
m
k

+ γk
γk−1
1
k −1

φ
m
k

−φ
 m
k −1

.
(115)
Utilizing the asymptotics (48) and the boundedness of φ, this can be rewritten as
(β −1)
k2
φ
m
k

+

1−β
k

1
k −1

φ
m
k

−φ
 m
k −1

+o(k−2).
(116)
Next, we use that φ is infinitely differentiable and that m
k −
m
k−1 = −
m
k(k−1) to obtain
φ
m
k

−φ
 m
k −1

= −φ ′ m
k

m
k(k −1) +O
m2
k4

.
(117)
Restricting m ≤n−1 < k (since otherwise ⟨rn−1,em⟩= 0), we get
⟨pk,em⟩= (1+ok(1))
(β −1)
k2
φ
m
k

−1
k2 φ ′ m
k
 m
k

+o(k−2).
(118)
Since φ is smooth (and thus φ ′ is bounded), we get
⟨pk,em⟩= (β −1)
k2
φ
m
k

−1
k2 φ ′ m
k
 m
k +o(k−2).
(119)
We finally calculate, using the asymptotic formula (79) for the components of rn−1 to get
⟨rn−1, pk⟩=
n−1
∑
m=1
⟨rn−1,em⟩⟨pk,em⟩
= k−1qn−1
 
1
k
n−1
∑
m=1
h
φ ′ m
k
 m
k −(β −1)φ
m
k

+ok(1)
i 
1+
Z 1
m
n−1
φ(z)dz
z +om(1)
!!
.
(120)
Comparing this Riemann sum with an integral, setting a = n−1
k , using that φ (and thus the integrand) is C∞, we get
⟨rn−1, pk⟩= k−1qn−1
Z a
0 (φ ′(x)x−(β −1)φ(x))

1+
Z 1
a−1x φ(z)dz
z

dx+on(1)

.
(121)
Combined with the assumption (111), this completes the proof, since for sufficiently large n > N the term in brackets
above will be strictly less than 1 in magnitude uniformly in a.
13

A PREPRINT - JULY 26, 2023
Next, we consider the case where k < n.
Proposition 4. Suppose that φ satisfies the condition of Proposition 2 and also the inequality
sup
a∈[0,1]

Z 1
0

(β −1)+(β −1)
Z 1
x φ(z)dz
z +φ(x)
Z x
ax φ(z)dz
z

dx+
Z 1
a φ(x)dx
x
 < 1.
(122)
Then there is a sufficiently large N such that for n > k ≥N we have |⟨rn−1,gk⟩| < qn. Further, by increasing N if
necessary, the denominator in the definition of ˜gN will be non-zero and we can choose ε > 0 small enough so that we
also have |⟨rn−1, ˜gN⟩| < qn for all n > N.
Proof. We will need to use the formulas (37) and (38). For this, we will need to estimate
n−1
∑
i=k+1
⟨hi,gk⟩.
(123)
Utilizing the the inductive definition of gk (24), we get
n−1
∑
i=k+1
⟨hi,gk⟩= γk
n−1
∑
i=k+1
⟨hi,rk−1⟩+
n−1
∑
i=k+1
⟨hi,hk⟩+ξk
n−1
∑
i=k+1
⟨hi,ek⟩.
(124)
We consider the last term above first. From the definition of hi (51) we get
ξk
n−1
∑
i=k+1
αi
i φ
k
i

.
(125)
Since limk→∞ξk = 1 by Proposition 1 and limk→∞αk = 1 by Proposition 2, we see that
ξk
n−1
∑
i=k+1
αi
i φ
k
i

= (1+ok(1))
n−1
∑
i=k+1
1
i φ
k
i

.
(126)
Applying Lemma 2, we see that
ξk
n−1
∑
i=k+1
αi
i φ
k
i

=
Z 1
k
n
φ(z)dz
z +ok(1).
(127)
Setting a = k
n we write this as
ξk
n−1
∑
i=k+1
αi
i φ
k
i

=
Z 1
a φ(z)dz
z +ok(1).
(128)
Next, we consider the term
n−1
∑
i=k+1
⟨hi,hk⟩=
n−1
∑
i=k+1
k−1
∑
j=1
⟨hi,ej⟩⟨hk,ej⟩=
n−1
∑
i=k+1
k−1
∑
j=1
αi
i φ
 j
i
 αk
k φ
 j
k

.
(129)
Using again (77), we get
n−1
∑
i=k+1
⟨hi,hk⟩= (1+ok(1))
n−1
∑
i=k+1
k−1
∑
j=1
1
i φ
 j
i
 1
k φ
 j
k

,
(130)
and Lemma 2 implies (recalling the choice a = k
n)
n−1
∑
i=k+1
⟨hi,hk⟩= (1+ok(1))1
k
k−1
∑
j=1
"
φ
 j
k
Z
j
k
j
n
φ(z)dz
z +O(j−1)
#
= (1+ok(1))1
k
k−1
∑
j=1
φ
 j
k
Z
j
k
a j
k
φ(z)dz
z +O
logk
k

.
(131)
Comparing a Riemann sum with an integral finally yields
n−1
∑
i=k+1
⟨hi,hk⟩=
Z 1
0 φ(x)
Z x
ax φ(z)dz
z +ok(1).
(132)
14

A PREPRINT - JULY 26, 2023
Finally, we consider the first term in (124). Using (79), we get
γk
n−1
∑
i=k+1
k−1
∑
j=1
⟨hi,ej⟩⟨rk−1,ej⟩= −γkqk−1
n−1
∑
i=k+1
k−1
∑
j=1
αi
i φ
 j
i

1+
Z 1
j
k
φ(z)dz
z +ok(1)

.
(133)
Utilizing the asymptotics (44), (45), and (77), we get
γk
n−1
∑
i=k+1
k−1
∑
j=1
⟨hi,ej⟩⟨rk−1,e j⟩= (β −1+ok(1))1
k
n−1
∑
i=k+1
k−1
∑
j=1
1
i φ
 j
i

1+
Z 1
j
k
φ(z)dz
z +ok(1)

.
(134)
Proceeding as before, using Lemma 2, recalling the choice a = k
n, and comparing a Riemann sum with an integral, we
finally obtain
γk
n−1
∑
i=k+1
k−1
∑
j=1
⟨hi,ej⟩⟨rk−1,ej⟩= (β −1)
Z 1
0
Z x
ax φ(z)dz
z

1+
Z 1
x φ(z)dz
z

dx+ok(1).
(135)
Combining the estimates (135), (132), and (128), we get
n−1
∑
i=k+1
⟨hi,gk⟩=
Z 1
0

(β −1)+(β −1)
Z 1
x φ(z)dz
z +φ(x)
Z x
ax φ(z)dz
z

dx+
Z 1
a φ(x)dx
x +ok(1).
(136)
Our assumption (122) on φ now imply that for sufficiently large k, the sum ∑n−1
i=k+1⟨hi,gk⟩will satisfy the bounds
−1 <
n−1
∑
i=k+1
⟨hi,gk⟩< 1
(137)
uniformly in n and k. So choose N large enough to guarantee this. We now use equation (37) to see that
|⟨rn−1,gk⟩| = |qn−1|

n−1
∑
i=k+1
⟨hi,gk⟩
 ≤|qn|(1+O(n−1))|

n−1
∑
i=k+1
⟨hi,gk⟩
.
(138)
The bound (137) implies that the final absolute value above is strictly smaller than 1 uniformly in n and k. Thus, by
increasing N if necessary, we get that for all n > k ≥N
|⟨rn−1,gk⟩| < qn,
(139)
as desired.
Finally, we bound ⟨rn−1, ˜gN⟩. Consider the inner product formula (38). Utilizing the definition of ˜gN, we obtain
n−1
∑
i=N+1
⟨hi, ˜gN⟩= Cε
n−1
∑
i=N+1
⟨hi,gN⟩+cε
n−1
∑
i=N+1
⟨hi,rN⟩,
(140)
where
Cε =
p
1−ε2
and
cε = ε∥rN∥−1.
We note that since rN ∈span(e1,...,eN), we get
n−1
∑
i=N+1
⟨hi,rN⟩=
n−1
∑
i=N+1
N
∑
j=1
⟨hi,ej⟩⟨rN,e j⟩=
N
∑
j=1
⟨rN,e j⟩
n−1
∑
i=N+1
αi
i φ
 j
i

.
(141)
Since φ is assumed to vanish on [0,δ] we get that
n−1
∑
i=N+1
⟨hi,rN⟩≤
N
∑
j=1
⟨rN,ej⟩
δ −1N
∑
i=N+1
αi
i φ
 j
i

< ∞,
(142)
and so this sum is bounded uniformly in n.
Plugging these facts into (38) we see that
|⟨rn−1, ˜gN⟩| ≤qn(1+O(n−1))
Cε
n−1
∑
i=k+1
⟨hi,gN⟩+Kε
,
(143)
for a large enough constant K. The bound (137) together with the estimate (143) implies that for small enough ε > 0
the absolute value above can be made smaller than 1 uniformly in n. Increasing N again if necessary, we then have
|⟨rn−1, ˜gN⟩| < qn
(144)
for all n > N. This completes the proof.
15

A PREPRINT - JULY 26, 2023
2.6
Completion of the Construction
Finally, to complete the construction, we will specify how to choose the function φ, which must satisfy the conditions
(76), (111), and (122).
To construct such a function φ, let ν ≥0 be a C∞bump function which is supported on [−1,1] and satisfies
Z ∞
−∞ν(x)dx = 1.
Let t > 0 and write νt = t−1ν
 t−1x

.
Fix a parameter τ ∈(0,1) to be determined later. For a continuously differentiable function f : [τ,1] →R+, we extend
f to the whole real line by setting f(x) = 0 for x < τ and f(x) = f(1) for x > 1. Then, we smooth f using the bump
function ν and normalize so that (76) is satisfied. Specifically, we set
¯φt(x) =
Z ∞
−∞f(x−y)νt(y)dy,
(145)
and φ(x) = Ct ¯φt, where the constant Ct is chosen to satisfy (76).
Since ¯φt →f in L1 as t →0 and both functions vanish in a neighborhood of 0 for t < τ, we see that
lim
t→0
Z 1
0
¯φt(x)

1+
Z 1
x
¯φt(z)dz
z

dx

=
Z 1
τ f(x)

1+
Z 1
x f(z)dz
z

dx

.
(146)
This means that if f satisfies
Z 1
τ f(x)

1+
Z 1
x f(z)dz
z

dx

=
β
1−2β ,
(147)
then the normalizing constant will satisfy Ct →1 as t →0.
Using this, we calculate that if f satisfies
sup
a∈[τ,1]
 f(τ)τ

1+
Z 1
a−1τ f(z)dz
z

+
Z a
τ (f ′(x)x−(β −1)f(x))

1+
Z 1
a−1x f(z)dz
z

dx
 < 1
(148)
and
sup
a∈[τ,1]

Z 1
τ

(β −1)+(β −1)
Z 1
x f(z)dz
z + f(x)
Z x
ax f(z)dz
z

dx+
Z 1
a f(x)dx
x
 < 1,
(149)
in addition to (147), then for sufficiently small t > 0, φ will satisfy (76), (111), and (122).
Indeed, for any t < τ, we clearly have φ ∈C∞and φ(x) = 0 on [0,δ] if δ < τ −t. Now, since φ vanishes in a
neighborhood of 0, we see that the integral in (122) converges to the corresponding integral for f uniformly in a. For
the condition (111), we note that
1+
Z 1
x φt(z)dz
z →1+
Z 1
x f(z)dz
z
(150)
as t →0 uniformly in x. Further, φt converges to f in L1, so that the only problematic term arises from the xφ ′
t (x) term
in (111). However, φ ′
t (x) converges uniformly to f ′(x) for x outside of [τ −t,τ +t]. For this term, we note that
x

1+
Z 1
a−1x φt(z)dz
z

(151)
is uniformly continuous in x,t and a ≥τ −t (for smaller values of a the entire integral in (111) vanishes). On the
interval [τ −t,τ +t] the function φt increases rapidly from 0 to f(τ), so the contribution from the integral of this term
over [τ −t,τ +t] converges to
f(τ)τ

1+
Z 1
a−1τ f(z)dz
z

(152)
uniformly in a ≥τ +t as t →0. For a ∈[τ −t,τ +t], the integral in (111) reduces to
Z a
τ−t xφ ′
t (x)

1+
Z 1
a−1x φt(z)dz
z

dx−(β −1)
Z a
τ−t φt(x)

1+
Z 1
a−1x φt(z)dz
z

dx.
(153)
16

A PREPRINT - JULY 26, 2023
Since φt is bounded and a ∈[τ −t,τ +t] we see that this converges to
τ
Z a
τ−t φ ′
t (x)dx = τφt(a)
(154)
as t →0 uniformly in a ∈[τ −t,τ +t]. Since |τ f(τ)| < 1 (setting a = τ in (148)), for sufficiently small t we will
have |τφt(a)| ≤τ f(τ) + ε < 1 for all a ∈[τ −t,τ +t] (as φt increases to φt(τ +t) on the interval [τ −t,τ +t] and
|φt(τ + t) −f(τ)| →0). Thus φ satisfies all of the conditions in our construction. The task now is to choose τ
appropriately and to find such an f.
We remark that the method in [15] essentially corresponds to choosing
f(x) =
c
x ≥τ
0
x < τ,
(155)
for a constant c and then optimizing over c, τ, and β. We improve this method by allowing general functions f, which
enables us to match the upper bound from [28].
The key to this is to introduce the function
F(a) =
Z a
τ f(x)

1+
Z 1
a−1x f(z)dz
z

dx,
(156)
and to rewrite the conditions on f in terms of F. The condition (147) then becomes
F(1) =
β
(1−2β).
(157)
Integrating by parts to remove the f ′ term in (148) and observing that
F′(a) = f(a)+a−1
Z a
τ f(x)f(a−1x)dx,
(158)
we obtain
sup
a∈[τ,1]
|aF′(a)−βF(a)| < 1.
(159)
To determine the function F, we choose to set (note that this is also what is needed to make Sil’nichenko’s analysis
tight)
aF′(a)−βF(a) = c,
(160)
for a positive constant c < 1. Combined with the definition (156), which implies that F(τ) = 0, and the endpoint
condition (157), this forces
c =
β 2
(1−2β)(τ−β −1),
(161)
and
F(a) = c
β

τ−βaβ −1

.
(162)
Since c < 1 must hold, we obtain the following condition on β and τ
β 2
(1−2β)(τ−β −1) < 1.
(163)
Differentiating the integrand in (149) with respect to a, rewriting in terms of F, and noting that for a = 1 the integrals
vanish, we see that the condition (149) becomes
sup
b∈[τ,1]

Z 1
b a−2(c−(1−2β)F(a))da
 < 1.
(164)
Plugging (162) into these conditions and noting that the integral above is minimized in b when the integrand is 0, which
occurs for
b = τ
 1−β
1−2β
 1
β
,
17

A PREPRINT - JULY 26, 2023
and is maximized when b = τ, we obtain the following two conditions on β, and τ
β
(1−2β)(τ−β −1)
 
β −1+ τ−β(1−2β)
1−β
−βτ−1
1−2β
1−β
 1
β
!
> −1,
(165)
β
(1−2β)(τ−β −1)
 
β −1+ τ−β(1−2β)
1−β
+ τ−1β 2
(1−β)
!
< 1.
(166)
A routine calculation shows that condition (163) means that τ must be chosen so that
(1−β)2
1−2β
< τ−β.
(167)
Let τ∗be the value of τ which gives equality in the above bound. By choosing τ < τ∗, we see that it suffices for the
inequalities (165) and (166) to hold (strictly, of course) for τ = τ∗. Plugging in this value and performing a routine, yet
tedious, calculation, we see that (165) becomes
β(1−β)
1
β < 1,
(168)
and (166) becomes

β
1−β
β (1−β)2
1−2β

< 1.
(169)
We see that (168) holds for any β ∈(0, 1
2) since the left hand side is < 1 and the right hand side is ≥1. Thus the only
condition on β which must be satisfied is (169), which is exactly the relation (20).
The only step left in the proof of Theorem 1 is to show that the equation (156) can be solved for f when F is given by
(162). Differentiating (156) with respect to a, we obtain the integral differential equation on [τ,1]
f(a)+a−1
Z a
τ f(x)f(a−1x)dx = G(a) := F′(a) = cτ−βaβ−1.
(170)
We will use the following Lemma, based on the Schauder fixed point theorem, to solve this equation.
Lemma 3. Suppose that G(x) is continuously differentiable and satisfies the bound
RG := sup
a∈[τ,1]
a−2
Z a
τ G(a−1x)dx < 1.
(171)
In addition, assume that there exist functions 0 ≤g1(x) ≤g2(x) ≤G(x) on [τ,1] such that
G(a)−a−1
Z a
τ g1(x)g1(a−1x)dx ≤g2(a),
(172)
and
G(a)−a−1
Z a
τ g2(x)g2(a−1x)dx ≥g1(a).
(173)
Then the equation (170) has a continuously differentiable solution f ≥0.
Proof. Consider the map TG : C([τ,1]) →C([τ,1]) defined by
TG(f)(a) = G(a)−a−1
Z a
τ f(x)f(a−1x)dx.
(174)
For a fixed M to be determined later, define the set
SM := { f ∈C1([τ,1]), g1(x) ≤f(x) ≤g2(x), |f ′(x)| ≤M} ⊂C([τ,1]).
(175)
By the Arzela-Ascoli theorem, SM is a compact subset of C([τ,1]). We will show that for sufficiently large M, TG maps
SM into itself. Indeed, if g1(x) ≤f(x) ≤g2(x), then we clearly have (since g1,g2 and thus f are non-negative)
TG(f)(a) = G(a)−a−1
Z a
τ f(x)f(a−1x)dx ≤G(a)−a−1
Z a
τ g1(x)g1(a−1x)dx ≤g2(a)
(176)
and
TG(f)(a) = G(a)−a−1
Z a
τ f(x)f(a−1x)dx ≥G(a)−a−1
Z a
τ g2(x)g2(a−1x)dx ≥g1(a),
(177)
18

A PREPRINT - JULY 26, 2023
by assumption on g1 and g2. Further, taking derivates, we see that
TG(f)′(a) = G′(a)+a−2
Z a
τ f(x)f(a−1x)dx−a−1 f(a)f(1)+a−3
Z a
τ f(x)f ′(a−1x)dx.
(178)
We now integrate the last integral above by parts to obtain
a−3
Z a
τ f(x)f ′(a−1x)dx = a−2
Z a
τ f(x) d
dx f(a−1x)dx
= a−2(f(a)f(1)−f(τ)f(a−1τ))−a−2
Z a
τ f ′(x)f(a−1x)dx.
(179)
Since G is fixed, f ≤g2 ≤G and |f ′(x)| ≤M by assumption, collecting the preceding equations gives a bound of
|TG(f)′(a)| ≤K +RGM,
(180)
where K is a constant only depending upon G and not upon the bound M. If RG < 1, we can choose M > K(1−RG)−1
to guarantee that |TG(f)′(a)| ≤M as well. This proves that TG maps SM to itself. The proof is now completed by
invoking the Schauder fixed point theorem.
Finally, we verify the assumptions of Lemma 3 for the function G in (170) in the case where s = 1. A straightforward
calculation gives that the integral in (171) is maximized when a = min(1,τ(1+β)
1
β ) and its value is given by
ca−(β+1)
β
a
τ
β
−1

.
(181)
Plugging in the values β = β ∗and τ = τ∗which give equality in equations (169) and (167) for s = 1, a straightforward
numerical calculation gives that RG < 1. Since this value is continuous in β and τ, the same holds for close enough β
and τ.
Finally, we verify the existence of the functions g1 and g2 in Lemma 3. We consider the modified map
˜TG(f)(a) = max

0,G(a)−a−1
Z a
τ f(x)f(a−1x)dx

.
(182)
Starting with the function f0 = G, we iterate to obtain the sequence fk = ˜TG(fk−1). The map ˜TG is monotone, so that
f0 ≥f2 ≥f4 ··· and f1 ≤f3 ≤f5 ···. Consequently, if f2n+1 > 0 for some n, then g1 = f2n+1 and g2 = f2n will satisfy
the conditions of Lemma 3. This follows since by the monotonicity of ˜TG, f ≤f2n implies that ˜TG(f) ≥f2n+1 and
f ≥f2n+1 implies that ˜TG(f) ≤f2n+2 ≤f2n. Finally, ˜TG(f) ≥f2n+1 > 0 implies that ˜TG and TG coincide, which shows
that the conditions of the Lemma are satisfied.
We verify numerically for G(x) = cτ−βxβ−1 given in (170), with β = β ∗and τ = τ∗(the optimal values for s = 1), that
the iterates satisfy f3 > 0. A plot of f0, f1, f2, and f3 is shown in Figure 1. Since f3 depends continuously on G (and
thus on β and τ), this completes the proof of Theorem 1. Finally, in Figure 1 we solve equation (170) numerically to
give an idea of what the optimal f looks like.
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Iterates of Modified Map T for s=1
f0
f1
f2
f3
0.5
0.6
0.7
0.8
0.9
1
0.5
1
1.5
2
Numerical Solution for f
G
f
Figure 1: Left: The first 3 iterates of ˜TG, which demonstrate that f3 > 0. Right: The numerically calculated solution to
(170) for the G corresponding to β ∗and τ∗for s = 1.
19

A PREPRINT - JULY 26, 2023
3
Acknowledgements
We would like to thank Andrew Barron, Ron DeVore, Jinchao Xu, Vladimir Temlyakov, and Matias Cattaneo for helpful
discussions. JWS was supported in part by the National Science Foundation through DMS-2111387 and CCF-2205004.
JMK was supported in part by the National Science Foundation through CAREER DMS-2239448, DMS-2054808, and
HDR TRIPODS CCF-1934924.
References
[1] Barron, A.R., Cohen, A., Dahmen, W., DeVore, R.A.: Approximation and learning by greedy algorithms. The
Annals of Statistics 36(1), 64–94 (2008)
[2] Bergeaud, F., Mallat, S.: Matching pursuit of images. In: Proceedings., International Conference on Image
Processing, vol. 1, pp. 53–56. IEEE (1995)
[3] Breiman, L., Friedman, J., Olshen, R., Stone, C.: Classification and Regression Trees. Belmont, Calif.: Wadsworth
International Group, c1984. (1984). DOI https://doi.org/10.1201/9781315139470
[4] Bühlmann, P., Yu, B.: Boosting with the L2 loss: regression and classification. Journal of the American Statistical
Association 98(462), 324–339 (2003)
[5] Chen, S.S., Donoho, D.L., Saunders, M.A.: Atomic decomposition by basis pursuit. SIAM Review 43(1), 129–159
(2001)
[6] DeVore, R.A.: Nonlinear approximation. Acta Numerica 7, 51–150 (1998)
[7] DeVore, R.A., Temlyakov, V.N.: Some remarks on greedy algorithms. Advances in Computational Mathematics
5(1), 173–187 (1996)
[8] E, W., Ma, C., Wu, L.: The barron space and the flow-induced function spaces for neural network models.
Constructive Approximation 55(1), 369–406 (2022)
[9] Friedman, J.H.: Greedy function approximation: A gradient boosting machine. The Annals of Statistics 29(5),
1189 – 1232 (2001). DOI 10.1214/aos/1013203451. URL https://doi.org/10.1214/aos/1013203451
[10] Jones, L.K.: A simple lemma on greedy approximation in hilbert space and convergence rates for projection
pursuit regression and neural network training. The Annals of Statistics 20(1), 608–613 (1992)
[11] Klusowski, J.M., Barron, A.R.: Approximation by combinations of ReLU and squared ReLU ridge functions with
ℓ0 and ℓ1 controls. IEEE Transactions on Information Theory 64(12), 7649–7656 (2018)
[12] Konyagin, S., Temlyakov, V.: Rate of convergence of pure greedy algorithm. East J. Approx 5(4), 493–499 (1999)
[13] Kurková, V., Sanguineti, M.: Bounds on rates of variable-basis and neural-network approximation. IEEE
Transactions on Information Theory 47(6), 2659–2665 (2001)
[14] Li, Y., Siegel, J.: Entropy-based convergence rates of greedy algorithms. arXiv preprint arXiv:2304.13332 (2023)
[15] Livshits, E.D.: Lower bounds for the rate of convergence of greedy algorithms. Izvestiya: Mathematics 73(6),
1197 (2009)
[16] Livshitz, E., Temlyakov, V.: Two lower estimates in greedy approximation. Constructive Approximation 19(4),
509–523 (2003)
[17] Mallat, S.G., Zhang, Z.: Matching pursuits with time-frequency dictionaries. IEEE Transactions on Signal
Processing 41(12), 3397–3415 (1993)
[18] Meyer, Y.: Wavelets and Operators: Volume 1. 37. Cambridge University Press (1992)
[19] Neff, R., Zakhor, A.: Very low bit-rate video coding based on matching pursuits. IEEE Transactions on Circuits
and Systems for Video Technology 7(1), 158–171 (1997)
[20] Nelson, J.L., Temlyakov, V.N.: Greedy expansions in Hilbert spaces. Proceedings of the Steklov Institute of
Mathematics 280(1), 227–239 (2013)
[21] Ongie, G., Willett, R., Soudry, D., Srebro, N.: A function space view of bounded norm infinite width ReLU nets:
The multivariate case. In: International Conference on Learning Representations (ICLR 2020) (2019)
[22] Parhi, R., Nowak, R.D.: Banach space representer theorems for neural networks and ridge splines. The Journal of
Machine Learning Research 22(1), 1960–1999 (2021)
[23] Parhi, R., Nowak, R.D.: What kinds of functions do deep neural networks learn? Insights from variational spline
theory. SIAM Journal on Mathematics of Data Science 4(2), 464–489 (2022)
20

A PREPRINT - JULY 26, 2023
[24] Pisier, G.: Remarques sur un résultat non publié de B. Maurey. Séminaire Analyse fonctionnelle (dit “Maurey-
Schwartz") pp. 1–12 (1981)
[25] Siegel, J.W., Xu, J.: Characterization of the variation spaces corresponding to shallow neural networks. arXiv
preprint arXiv:2106.15002 (2021)
[26] Siegel, J.W., Xu, J.: Optimal convergence rates for the orthogonal greedy algorithm. IEEE Transactions on
Information Theory 68(5), 3354–3361 (2022)
[27] Siegel, J.W., Xu, J.: Sharp bounds on the approximation rates, metric entropy, and n-widths of shallow neural
networks. Foundations of Computational Mathematics pp. 1–57 (2022)
[28] Sil’nichenko, A.: Rate of convergence of greedy algorithms. Mathematical Notes 76(3), 582–586 (2004)
[29] Temlyakov, V.: Greedy approximation, vol. 20. Cambridge University Press (2011)
21

