
Jacob Burak is a Tel Aviv-based writer who draws from psychology, science and
art to examine life and human nature. Since retiring from his business career in
2005, Burak has authored three other bestsellers: Do Chimpanzees Dream of
Retirement (2007), Noise (2009) and Why Kamikaze Pilots Wear Helmets (2011).
Burak holds a BSc in Engineering and Management from the Technion and
studied management at Harvard. He is active in social causes, including Round-
up Israel, which enables credit card users to donate fractions of their purchases,
and formerly chaired the Midot organization that rates the effectiveness of
NGOs. In 2013, Burak founded Alaxon a digital magazine featuring articles on
culture, art and popular science, for which he writes regularly.

HOW TO FIND
A BLACK CAT IN
A DARK ROOM*
The Psychology of Intuition
Influence, Decision Making and Trust
*ESPECIALLY WHEN
THERE IS NO CAT
Jacob Burak
Translated form the Hebrew by Ira Moscowitz

TABLE OF CONTENTS
Introduction
Part I The Life Worth Living
If Only
Life offers a second opportunity, and it’s called tomorrow
Escape from the Matrix
The fear of missing out haunts our social networks and our real lives alike,
but there is a way to break free
The Rule of One Foot on the Ground
Knowing when to let go of our ambitions
Picking Your Battles
Our willpower depletes like a muscle – let’s save it for when we really need
it
One Day, When I’m Younger
Hope is the emotion that most affects our lives
Challenging the Bottom-Line Approach
Valuing process over outcome
Happiness: Cut Out and Save
Ten rules that lead to happiness and that we control
A Bit of Humble Pie Goes a Long Way
Overvaluing confidence, we’ve forgotten the power of humility
Part II Why Smart People Make Stupid Mistakes
Why Do Smart People Make Stupid Mistakes?
How our brains continue to protect us against threats that no longer exist
And Merci to the French Teacher
Why introducing a problem in a foreign language overcomes major
cognitive biases

The Prisoner’s Dilemma
Judges as humans—food for thought
If I’m Not for Myself
On the powerful egocentric bias
Knowing What We Don’t Know
Why incompetent people don’t recognize their ineptitude
Birdbrained
When some birds are smarter than people
I Saw a Monkey Playing Mozart
On the roots of urban legends
Lake Victoria and Uncle Albert
Is it possible to swindle honest people?
I Accuse, Falsely
The path to justice is paved with deceptive evidence
Part III All in Good Order
List for Life
The magical powers of a to-do list
On the Shoulders of Giants
The lists of rankings are another level of order
Outlook: Gloomy
Humans are wired for bad news, angry faces and sad memories. Is this
negativity bias useful or something to overcome?
Cold Hands or a Warm Heart
Ranking the traits that form our views of others
I’ve Seen Happy Conservatives
What are the psychological dynamics that differentiate between liberals and
conservatives?
The Matthew Effect

The mysterious engine that generates inequality
Ode to a Bureaucrat
What empowers the bureaucrats of the world?
Part IV Alone in the Crowd
Me, Myself and I
The sorrows of the narcissist as a social animal
No Friends Among Secrets
Choosing the people to whom we surrender our asset called privacy
Long Live the Small Difference
Are we more similar or different?
An Embarrassment of Riches
Signs of social discomfort might carry a positive message of authenticity
Trust Games
The self-reinforcing circle of trust and trustworthiness
The Beginning of a Beautiful Rivalry
Rivalry is as old as humanity – how our rivals own a deep part of ourselves
that spurs us to our greatest achievements
Guardians of the Lake
When personal interests mobilize to protect social capital
Epilogue
Memento Mori
Recommendations for Further Reading

INTRODUCTION
This book takes its name from the timeless advice that the Chinese philosopher
Confucius offered to those who seek enlightenment: “The hardest thing of all is
to find a black cat in a dark room, especially if there’s no cat.” That is, you better
know what you’re looking for before you begin. Those who go in search of
something whose nature is not entirely clear to them will not succeed in finding
it. But what about those who find a black cat in a dark room even when no cat is
there? Shall we say they are creative, innovative or maybe just illusionists?
And what about people who are shown the image of a black cat taken with a
heat-sensitive camera, in an effort to convince them that it is indeed possible to
find black cats, even in places where they ostensibly are not present? Should
they dismiss the photograph as a fake devised by interested parties, or should
they see it as an opportunity to broaden their minds about black cats in general,
and such cats in a dark room in particular?
I was born in 1948, the year the State of Israel was founded, and I was raised
and educated there. Like the state, I’ve experienced a lot: I studied engineering
at the Technion and later graduated from the PMD program at Harvard Business
School; I served as a naval officer with the rank of commander, created a leading
management consultancy firm and was a member of Shimon Peres’ “100 days”
team that prepared him to take the reins as prime minister; and I later formed the
Evergreen investment firm that became a venture capital pioneer in Israel. In
2007, I left the business world to devote myself to writing and social activity. If
there’s one thing I learned during these years of diverse experience, it is that the
essence of military, business and philanthropy alike is people, and that the
essence of people is psychology. Therefore, the black cats I’m searching for
usually hide between the pages of extraordinary studies in the field of behavioral
science.
This book presents some of the research findings that have been helpful in my
personal journey. However, the choice of which research to present in the book
is not arbitrary and is by no means a function of their scientific importance. Alas,
the choice reflects the beliefs of the author and is designed to serve his
worldview. Non-fiction writing in the Internet era entails curating thoughts, and
non-fiction writing in the behavioral sciences ultimately entails curating studies
in the field.
Nonetheless, other sources of insights and findings will be cited here that stem
from authentic curiosity, untainted by the author’s biases. Is it important to

distinguish between them? Not necessarily, if we understand that both types of
sources stimulate thinking and that therein lies their primary importance. In any
case, it’s worth remembering that time is the best curator, not the author, and that
the overriding rule here is: what is important is not new, and what is new is
probably unimportant.
Beyond the interesting findings of one study or another, the central question
this book poses is: to what extent are we really bound by the experience of
others and by these research findings? Are we part of a large audience of
potential guinea pigs, whose inclinations are summarized in behavioral studies?
Or rather, are we distinct individuals in this audience, endowed with the freedom
of choice to behave differently? In other words, do we even have anything to
learn from them, or do they apply to everyone around us, but not to us?
Paradoxically, if you identify yourself in the research conclusions, as I do—even
when they shed light on our less flattering sides, such as envy, vindictiveness,
materialism or procrastination—you have the opportunity to learn from them, to
draw conclusions, and to choose a path that will express greater responsibility
toward your emotional wellbeing and the welfare of the society in which you
live. If, on the other hand, you continue to feel that you’re so unique that the
findings do not apply to you, you’ll perhaps miss an important opportunity to
change, and for the better.
The book is divided into four sections, each with several articles.
In the section entitled The Life Worth Living, you’ll discover the damages of
FoMO—the Fear of Missing Out. This fear of missing a social or other
happening makes us dash frenetically from one electronic gadget to another. But
the truly important question is whether we’re really missing something if, in any
case, most of us are doomed to lead meaningless lives of mere spiritual survival.
In this section, we’ll also discover the limited importance of the test of the
ultimate outcome, the blessed calm that humility engenders and ten rules to
happiness that are all under our control, some of which are not self-evident.
The Why Smart People Make Stupid Mistakes section maps some of the
prevalent biases that accompany our routine, everyday decisions. You’ll discover
that we are almost strangers to ourselves, and that our statistical ignorance, when
combined with excessive confidence, produces a volatile mixture resulting in
erroneous decisions. Here too, humility, in typical shyness, raises its head and
explains that wisdom is not knowledge. In fact, wisdom is a moral quality of

those who recognize the limitations of their knowledge.
The All in Good Order section reviews some of the common human tools for
making order in the chaos we encounter in life. Some of them are conscious,
such as the “to-do” lists we prepare with obsessive diligence, and some are
unconscious, like our inclination to attribute greater weight to the negative than
to the positive. It turns out that negative events are etched more deeply in our
brains than positive events, and most of the neurons in the areas of the brain
linked to emotional activity hunt for bad news. But why?
Alone in the Crowd deals with the disruption of the historical balance between
our desire to make an impact as individuals and our social need to belong. A
unique combination of technological means and the erosion of social values,
such as interpersonal trust, is paving the way to an era that has already been
defined as the Narcissistic Era. Can the social pendulum still swing back into
balance? This section also tries to address the question of whether we are more
similar to or different from one another, and deciphers the roots of the disgust we
feel toward a bitter rival.
Here are 50 studies (and stories) that spurred me to re-examine my worldview.
Now, they’re yours too and I hope you’ll find yourself in them, as I did.

PART I
THE LIFE WORTH LIVING

IF ONLY
Life offers a second opportunity, and it’s called tomorrow
“The ways we miss our lives are life”
Randall Jarrell
“The unexamined life is surely worth living, but is the unlived life worth
examining?” The British psychoanalyst Adam Phillips asks this seemingly
strange question at the beginning of his book Missing Out: In Praise of the
Unlived Life. This question becomes significant, he asserts, when we discover
how much of our thinking is devoted to the life we haven’t lived, the life we
continue to experience in our thoughts as a shadow that stalks the life we live.
The unlived life is the life that could have been ours, the chances we didn’t take
and the opportunities we missed.
Phillips adds: “We discover these unlived lives most obviously in our envy of
other people, and in the conscious (and unconscious) demands we make on our
children to become something that was beyond us.” And there are also people
whose lives are consumed by the nagging story of the life they were unable to
live.
One of the sad by-products of Darwin’s theory is acceptance of the fact that as
individuals belonging to a particular species, there is nothing unique about us.
We see ourselves as unique only in order to give meaning to our lives. The sense
of uniqueness that begins with parental education is reinforced by the consumer
culture, which entirely depends on its ability to meet the ostensibly “unique”
needs of its subjects. Regret sets in when these needs are not met.
In the past, especially in cultures with a stricter code of conduct (for example,
arranged marriages and limited freedom to choose a lifestyle), we had fewer
opportunities for regret. In an achievement-oriented society that sanctifies the
freedom of choice, it is difficult to suddenly rid oneself of remorse. When the
individual is pushed to achieve everything possible, it seems that precisely there,
in the life we didn’t live, we could have been even more unique. Reality almost
always disappoints in the end, and regret becomes unavoidable. Research in this
area distinguishes between regret that stems from action (something we did and
wish we hadn’t done) and regret that results from inaction or omission
(something we didn’t do and would be happy to have done).
Studies show that in the short term, regret stemming from action (choosing an

unsuitable job) is greater than regret from inaction (not completing studies).
However, in the long term, when respondents are asked about their biggest
regrets in life, they focus on what they refrained from doing—the man or woman
they didn’t approach, the job they didn’t pursue, the failure to properly part from
their parents before they died. Studies also indicate that we tend to most strongly
regret inaction related to what we perceive as representing a great opportunity:
first and foremost, a missed educational opportunity, followed in distant second
place by a missed career opportunity. Next, in descending magnitude of remorse,
are romantic relations, parenthood, self-development and use of leisure time.
The direct reason that regret stemming from an action is less troubling than
regret stemming from inaction is the fact that here we at least have a chance to
rectify the results of the action (quitting an unsuitable job we chose). On the
other hand, we forever carry in our minds the woman or man we didn’t
approach. Another explanation of the intensity of regret over inaction is that the
consequences of our action are limited and final, while the results of inaction are
only limited by the imagination of those who assess them and tend to be
magnified to unreasonable proportions.
Nonetheless, the findings of all of the researchers on this important topic are
based on the response of healthy participants, some of them students who are too
young to assess their lives in proper perspective. This raises the simple question:
would their responses change at the end of their days? And if reliability is our
guiding light, it would be desirable, perhaps, to check the answer with those who
know their days are numbered.
This is precisely what Bronnie Ware did. Ware, an Australian palliative nurse,
provided care for terminal patients who returned to their homes to die, and thus
learned about the biggest regrets that occupied them during the final weeks of
their lives. She documented their insights in a blog that later became a book (The
Top Five Regrets of the Dying). In her book, Ware describes the clarity of
introspection that people gain during their final days. She tells how she soon
discovered that the topics that arose in her conversations with her patients, the
regrets they expressed and the things they would have done differently perhaps,
were common to almost everyone. And these are the five most common regrets
Ware heard from people who knew they would no longer be able to live their
unlived lives:
1 I wish I’d had the courage to live a life true to myself, not the life others
expected of me.
This is the top regret. “When people realize that their life is almost over and look
back clearly on it, it is easy to see how many dreams have gone unfulfilled,”

Ware says in her blog. “Most people had not honoured even a half of their
dreams and had to die knowing that it was due to choices they had made, or not
made … Health brings a freedom very few realize, until they no longer have it.”
2 I wish I hadn’t worked so hard.
All of the men she nursed, without exception, expressed this regret. “They
missed their children’s youth and their partner’s companionship. Women also
spoke of this regret. But as most were from an older generation, many of the
female patients had not been breadwinners.” Ware says in summary: “All of the
men I nursed deeply regretted spending so much of their lives on the treadmill of
a work existence.”
3 I wish I’d had the courage to express my feelings.
Ware further explains in her blog: “Many people suppressed their feelings in
order to keep peace with others. As a result, they settled for a mediocre existence
and never became who they were truly capable of becoming.” In fact, the former
palliative nurse claims, some of the illnesses they developed were attributable to
the festering bitterness and resentment they felt. “We cannot control the
reactions of others. However, although people may initially react when you
change the way you are by speaking honestly, in the end it raises the relationship
to a whole new and healthier level,” she advises.
4 I wish I had stayed in touch with my friends.
Ware’s patients acknowledged that they hadn’t appreciated the important benefit
of keeping up with old friends until it was too late. Many were so caught up in
their own lives that they let “golden friendships” slip away from them over the
years. Many regretted not having devoted the time to friendships that they
deserved.
“Everyone misses their friends when they are dying,” Ware says, noting that
“it is common for anyone in a busy lifestyle to let friendships slip.” Money and
status suddenly lose their luster when death approaches. “It all comes down to
love and relationships in the end,” she concludes. “That is all that remains in the
final weeks, love and relationships.”
5 I wish that I had let myself be happier.
Ware was surprised by this regret, which was shared by many of her patients.
“Many did not realize until the end that happiness is a choice. They had stayed
stuck in old patterns and habits. The so-called ‘comfort’ of familiarity
overflowed into their emotions, as well as their physical lives. Fear of change

had them pretending to others, and to their selves, that they were content. When
deep within, they longed to laugh properly and have silliness in their life again.”

ESCAPE FROM THE MATRIX
The fear of missing out haunts our social networks and
our real lives alike, but there is a way to break free
Here’s a test you might enjoy: rate these scenarios on a number scale, ranging
from 1 for mild discomfort to 7 for outrageous distress.
Scenario 1
You’re flicking through news websites, as you do every morning. Today,
however, you’re behind schedule and have only 15 minutes to read articles,
instead of your usual 30. You have to skip some of your favorite columns and
sections. How would you rate your level of discomfort? (Most of us would
probably choose a low level, say 2.)
Scenario 2
You’re visiting New York City and realize there’s no way you’ll be able to get to
all the exhibits, see all the recommended plays, or take in even a fraction of the
“musts” your local friends have raved about. How do you feel now? Something
like 5?
Scenario 3
You’re at dinner with friends, and you’ve all agreed to make it a strictly phone-
free evening. But your smartphone won’t stop beeping Twitter and text alerts.
Something is obviously up in your social network, but you can’t check. Even 7
wouldn’t match the stress you’re feeling now.
Welcome to FoMO (Fear of Missing Out), the latest cultural disorder that is
insidiously undermining our peace of mind. FoMO, a spawn of technological
advancement and proliferating social information, is the feeling that we’re
missing out on something more exciting, more important, or more interesting

going on somewhere else. It is the unease of feeling that others are having a
more rewarding experience and we are not a part of it. According to a recent
study, 56% of those who use social networks suffer from this modern plague.
Of course, that sense of missing out is nothing new. An entire body of
literature describes the heart-wrenching conflict between romantic aspirations
and social conservatism. Edith Wharton, Charlotte Brontë and Stendhal, to name
but a few, described the angst of missing out long before we could look up high-
school friends on Facebook.
But while 19th-century protagonists spent a lifetime grappling with a single
missed opportunity, today’s incessant flow of information is a disturbing
reminder of the world rushing by. As you read this, you might be missing a party
that some friends are throwing or the meal that other friends are eating without
you. Perhaps you’re willing to cut one phone call short—in mid-sentence—to
take another call, without even knowing who might be on the other end. At
night, when you’ve solemnly sworn yet again to put the phone aside or turn off
the computer, you grab one last peek at the screen on your way to bed—lest you
miss some titbit supplied by mere acquaintances or even strangers requesting
your “friendship” or announcing news.
As discussed in the previous chapter, end-of-life regrets tend to center on what
we didn’t do, rather than on what we did. If so, constantly watching others doing
things that we are not doing is fertile ground for a future of looking back in
sorrow. A lively conversation at the other end of the table can give us the FoMO
itch, just as can the dizzying array of shows, parties, books or the latest in
consumer trends pumped at us by social media.
Our attractive online personas—so alluring from afar—make FoMO more
virulent still. The Massachusetts Institute of Technology social psychologist
Sherry Turkle, author of Alone Together: Why We Expect More from Technology
and Less from Each Other (2011), says that technology has become the major
construct through which we define intimacy. We confuse our hundreds, or even
thousands, of “friends” on social networks with the handful of intimate friends
we have in reality. Drawing on hundreds of interviews, Turkle claims that the
price we are paying for technological prosperity is the gradual decline of
important relationships—with our parents, children or partners—and the birth of
a new type of loneliness. “Insecure in our relationships, and anxious about
intimacy,” she writes, “we look to technology for ways to be in relationships,
and protect ourselves from them at the same time.” If you have ever looked on in
wonder as someone taps out an endless text message instead of actually talking
to the person they’re with, you will find comfort in Turkle’s assessment that our
relationship with technology is still maturing. Being connected to everyone, all

the time, is a new human experience; we’re just not equipped to cope with it yet.
Turkle says our dependence on technology can be mitigated if we manage to
detach ourselves, even for short periods of time, from our gadgets. Will we one
day buy devices from FA (FoMO Anonymous) to help us recover from our
technology addiction? I envision devices that relay information at random,
unanticipated intervals—with neither sender nor receiver cognizant of the delay
in advance; this would force owners to miss out on some communiqués and
discover, to their surprise, that they can still function without them.
Even with such interventions, the problem might be resolved only when we
grasp that our brains and our humanity—not our technologies—enable this
addiction, in the end. We cannot seek solutions without honestly asking
ourselves why we are so afraid of missing out.
The University of Oxford social scientist Andrew Przybylski conducted the
first empirical study on the exploding disorder, with the results published in
Computers in Human Behavior in 2013. Among his conclusions, there and
elsewhere, is that FoMO is a driving force behind social media use. FoMO levels
are highest in young people, in particular young men. FoMO is high in distracted
drivers, who engage in other activities while behind the wheel. And perhaps
most revealingly, FoMO occurs mostly in people with unfulfilled psychological
needs in realms such as love, respect, autonomy and security. All in all, we are
afraid of missing out on love and on feeling that we belong; those of us heavily
invested in work also fear missing an opportunity for professional advancement
or a profitable deal.
The University of Oxford evolutionary psychologist Robin Dunbar, author of
How Many Friends Does a Person Need? (2010), says that the problem might be
mitigated if only we understood ourselves more. Dunbar claims that we lack the
emotional and intellectual capacity to distinguish between more than about 150
members of a group—the average size of a Neolithic farming village. But just
tell that to the average US teen, who sends 3,000 text messages a month
(according to a 2010 report by Nielsen, the marketing-survey giant) and who
fears that she will be ostracized if she doesn’t respond immediately, sometimes
dealing with a cast of thousands online.
Freedom from other people’s opinions and release from social comparison is a
triumph reserved for very few. The self-discipline strong enough to withstand the
power of FoMO is no less rare.
What, then, can we do about something so detrimental to our quality of life?
Psychotherapy for the underlying emotional causes of FoMO is far too costly
and invasive, and simply vowing to disconnect from our gadgets fails to work.
Instead, the best way to cope with FoMO might be to recognize that, at our

frenetic pace of life, we are sometimes bound to miss out. And that, when we do,
we might actually improve the outcomes of the options we have chosen.
This simple approach was first introduced in 1956 by Herbert Simon, an
American multidisciplinary researcher and Nobel Prize Laureate in Economics.
He used the term “satisfice”—a portmanteau of “satisfy” and “suffice”—to
suggest that instead of trying to maximize our benefits, we seek a merely “good
enough” result. Simon’s strategy relies on the assumption that we simply do not
have the cognitive capacity to optimize complex decision-making. We cannot
process the mass of information entailed in weighing all available options and
probable outcomes—both on the social networks and off. Thus, the best move is
“satisficing”—choosing the first available option that meets our predetermined
criteria, which is good enough.
In 1996, Simon published an autobiography describing his life as a series of
discrete decisions in which he chose the “good enough” option over a possible
best one. Simon claims that most people who favor optimization are unaware of
the heavy toll that gathering information takes on their overall benefit. In routine
decisions, the price we pay is in well-being: anyone with a friend who will not
agree to eat anywhere but the most fashionable restaurant or who insists on
shopping until the perfect outfit is found can appreciate the relief of a “good
enough” strategy.
Studies of Simon’s method have shown that people who insist on optimizing
decisions are ultimately less satisfied with their choices than those who make do
with “good enough.” Other studies clarify why: the achievements of the former
are actually lower than those of the latter, especially when the decision involves
weighing possible outcomes. In a series of experiments led by the Swarthmore
College social psychologist Barry Schwartz, participants filled out a self-
assessment questionnaire determining their tendency to optimize decisions
(based on their agreement with statements such as “I never settle for second
best” or “I often find it difficult to shop for gifts for a friend”). Another
questionnaire measured subjects’ propensity to feel regret; participants were then
classified according to their answers on both questionnaires. The researchers
found a negative correlation between the tendency to optimize and happiness,
self-esteem and satisfaction, and a positive correlation between the same
tendency and depression, perfectionism and regret. Another study in the series
found that people who optimize also engage in more social comparison, and are
adversely affected when they come up short.
Wait a minute—isn’t FoMO of the social networks based on exactly this type
of comparison? If so, could “satisficing” bring relief? Analyzing FoMO through
Simon’s parameters reveals an uncanny similarity to the decision-making

processes he studied, marked by cognitive overload and a heavy toll on well-
being.
Today’s wealth of information, especially online, is costing us another
valuable resource: our attention, which is limited enough to begin with. The
difficulty of spreading our already taxed attention span over unprecedented
amounts of information derives not just from our cognitive problem with
prioritizing, but also from our inability to consume and process it all. FoMO-
related distress is our soul crying out for help, imploring us to limit our
superficial connectivity and our frantic hopping from site to site before our
quality of life and our ability to express intimacy and individualism erode.
Taking the “good enough” approach to this crushing problem is not merely a
tactic for improving our decision-making. It is first and foremost a world-view, a
way of life; some researchers even believe it is a hereditary personality trait.
Testimony to the method’s effectiveness abounds. In business, sacrificing
maximization in favor of a predefined “good enough” is known to be the best
strategy in the long run. As the saying goes, “bulls make money, bears make
money, pigs get slaughtered”: greediness that looks to maximize doesn’t pay.
Business people also know to “leave something on the table,” especially in deals
leading to long-term partnerships. Experienced capital market investors
understand that aiming to “sell at the peak” will ultimately be less profitable than
selling once a satisfactory profit is gained. Corporate graveyards are full of
companies that did not stop at a “good enough,” profitable product that they
could easily market, surrendering instead to ambitious engineers with
sophisticated specifications and unrealistic plans.
In his outstanding book Why the Allies Won (1995), the British historian
Richard Overy analyzes the outcomes of the Second World War, which were not,
he claims, a given. One explanation he offers is the German army’s attempt to
optimize use of its military munitions at the expense of tactical combat
efficiency. At one point in the war, the Germans had no fewer than 425 different
kinds of aircraft, 151 kinds of trucks and 150 kinds of motorcycles. The price
they paid for the technical superiority of German-made munitions was difficulty
in mass-production, which was ultimately more important from a strategic point
of view. In the decisive battles fought in Russia, one German force had to carry
approximately one million spare parts for hundreds of types of armed carriers,
trucks and motorcycles. The Russians, in contrast, used only two types of tanks,
making for much simpler munitions maintenance during war. It was “good
enough” for them.
Perfectionism is the personality trait most associated with aspiring toward
maximizing the outcome of decisions. However, those of us who know

perfectionists, know that for them life is one never-ending score sheet that
throws them into a self-assessment tizzy of frustration, anxiety and sometimes
even depression. Perfectionists tend to confuse error with failure, and their
attempt to hide their errors, even the inevitable ones, prevents them from
accepting the critical feedback so necessary for personal growth. They would
probably give a great deal for the relief of being able to “satisfice.”
Even when it comes to emotional intimacy and love, “good enough” works
best. It was the British psychologist Donald Winnicott who gave us the concept
of the “good-enough mother”—a mother sufficiently attentive and adequately
responsive to her baby’s basic needs. As the baby develops, the mother
occasionally “fails” to answer his needs, preparing him for a reality in which he
will not always get exactly what he wants, whenever he wants it. The child
learns to delay gratification, a key to any form of adult success. As we mature,
we make do with “good enough” partners almost by definition. Yes, out there is
someone probably more suited to our needs—but we might not live long enough
to find him or her.
Even if feeling that we are missing out is testament to our spirited drive for
life, the way in which social networks now enhance our optimization fallacy
beyond all proportion is taking a serious toll on our quality of life. If you still
doubt that “good enough” is the best antidote to FoMO, the words of the
American essayist and poet Ralph Waldo Emerson might strike the right chord:
“For everything you have missed, you have gained something else, and for
everything you gain, you lose something else.”

THE RULE OF ONE FOOT ON THE GROUND
Knowing when to let go of our ambitions
The Cheltenham Gold Cup is the name of a steeplechase race, the most
prestigious of its kind in England, which is open to horses aged five years or
older. These are supposed to gallop a distance of three miles, two and a half
furlongs while jumping over 22 fences along the way. The race takes place once
a year, in March, as part of the four-day Cheltenham Festival. The race has
known a few four-legged heroes of bygone days, and their statues adorn the
lawns around the well-kempt track. Naturally, it is also one of the most widely
covered races in the country, and the 2012 race drew special interest because of a
particular horse—Kauto Star, a relatively old horse of 12, brown with a white
stripe on his shapely nose, whose competence may have suffered when he was
injured during training two and a half weeks before the race. But an
incomparable competitive spirit and Ruby Walsh, one of the best jockeys in the
country, mounted on his back make this horse the talk of the town.
Kauto Star won this race twice already, in 2007 and 2009. After an illustrious
career (the winner in 23 of the 40 races he entered) that earned his owners more
than $5 million, everyone understands that Kauto’s race today is probably his
last. If he wins it, he will have set a record for years to come and cemented his
standing as the world’s greatest jump-racing horse. Kauto Star’s main rival is
Long Run, five years his junior, who beat him in this race the previous year by
eight full lengths. But Kauto came back from temporarily being put out to
pasture and beat his young rival in two important races in the months leading up
to today’s race. Although the stats are not encouraging—only two horses his age
have won the race in its 88 years of existence and not one of them since 1969;
the course at Cheltenham is especially long; and its famous finish up the hill
grants young horses a natural advantage—Kauto has already shown that if any
horse can write a new page in the book of equine statistics, he is that horse.
Kauto has more than 10,000 fans on his Facebook page who are rooting for
him to win, but just as much for him not to fall and get injured. According to the
data published by the British Horseracing Authority, a horse’s odds of dying in a
steeplechase race are 4/1,000. If you consider the number of races such a horse
runs, you will understand why a horse’s chances of ending its life munching
grass in peace are not very high. And indeed, the lives of most racehorses come
to an end before they are five. Leg structure is responsible for racehorses’ speed,
but also for their vulnerability. During a race a horse may place on its legs a

weight three to ten times greater than its body weight. It is very tough to treat a
fracture in a horse’s leg, and the likelihood of developing gangrene or infection
is strong. Such an injury is usually a death sentence for a horse. On the
preceding Race Day at Cheltenham three horses failed to clear hurdles, were
injured and received a lethal injection to put them out of their misery.
But today it appears that everyone wants Kauto to win, even those betting on
another horse. It seems like his victory would signal to everyone that eternal life
is an actual possibility.
Sixty-five thousand spectators, a record crowd at Cheltenham, come to watch
the race on March 16, 2012. The winner is Synchronised, who was given odds of
1/8 by bookmakers. Long Run finishes third. And Kauto Star? Well, Kauto never
makes it to the finish line. The horse is in the lead and jumps the first hurdles
well, but after the ninth fence the jockey Ruby Walsh decides, to the audience’s
chagrin, to stop racing, return the decorated horse to the area where the race
ends, and unsaddle him. Walsh says he felt the horse straining more than usual
over the water hurdle, and feared that leaving him in the race would end in a
fatal injury. His fears receive sad confirmation when Synchronised, the winning
horse, falls to his death just a month later in the Grand National race.
Despite understandable attempts by the media to anthropomorphize the horses
taking part in the race (“you want to watch him compete over and over, like
Federer”), my interest is focused entirely on the human rider—the person who
had to decide between gloria mundi for the horse and for himself and the risk of
losing this special horse. And to focus more closely still, I am interested in the
question of whether his decision as to the circumstances in which he would
withdraw from the race was made even before the horses came out of the gate, or
during the race, when he surmised that the hopes of winning had begun to
recede. If he determined in advance under what circumstances he would pull out,
Walsh needed tremendous willpower to forfeit the race and the big prize along
with it. Except that Walsh’s skill level and close familiarity with Kauto point to
the possibility that his decision came during the race: the attentive and
experienced jockey recognized Kauto’s decline in ability, minuscule as it may
have been, and decided not to take the risk. But what about all the rest of us, who
are forced to make important decisions even though we are not equipped with
the immense skill and sensitivity that were at Walsh’s disposal when he made
his?
“When a ground crew is getting ready to launch a hot-air balloon, they have to
hold on to the basket to prevent it from taking flight prematurely. They grasp the

edge of the basket with both hands and plant a foot on a hold near the base.
Only, ever, one. The one sacred unbreakable rule of balloon ground handling is:
always keep one foot on the ground.”
The insight reflected in this important rule that Jeff Wise offered his readers
(in the January 2011 issue of Psychology Today) is meant to ensure the safety of
the ground crews of these capricious aircraft, should a rapid change in weather
disrupt their stability. “If a gust of wind catches the balloon and it starts to rise,”
an expert on the matter explains to Wise, “and you get lifted six inches into the
air, you think, ‘Oh, that’s no big deal, I can just step down if I need to.’ Then
before you know it you’re at six feet, and you think, ‘I could twist an ankle, I’d
better hang on and wait ‘til it gets lower.’ Then you’re at thirty feet, and if you
jump you’re going to break a leg. But if you don’t jump…”
That is the mental trap to which Wise was referring—hanging on too long in
the hope that a bad situation will get better without assessing the devastating
consequences expected if the improvement is late in coming. The rule described
here was never more relevant than on one spring morning in 1932, when the
biggest helium-filled airship in the world, the USS Akron, tried to land in an
open field near a military base in San Diego, California. The immense ship, 785
feet long, which had been christened just a year earlier and represented the
height of aviation technology at the time, emerged, as it slowly descended, out of
the fading morning mists. Each time it approached the ground, the 200 naval
academy cadets who had been assigned that morning readied, for the first time in
their lives, to dock an airship by means of the ropes that trailed from it. But the
first three attempts were disrupted by sudden gusts of wind that diverted the
ship, and only on the fourth try did the ground crew manage to grab hold of the
ropes and drag the enormous craft to the ground. But then, because of a
malfunction in one of the docking rings, the ship listed on one side and made
things very difficult for the crewmen on the other side. Out of desperation,
several of them climbed up the rope in an effort to put all of their body weight on
it, but in vain. The ship slowly rose, and the sun’s heat began warming the
helium left inside it. The young and inexperienced ground crew dropped the trail
ropes and hit the ground (when they did not land by mistake on a fellow
crewman).
But as the Akron took off, after it had offloaded its human cargo, the many
spectators at the event were horrified to discover that three of the naval cadets
were left hanging in mid-air. The first dropped to his death from a height of 160
feet. The next to crash on the hard ground, in a little cloud of dust, was the
camp’s star athlete, Sailor Nigel Henton. Only one man remained amid the
dwindling ropes from the ship. The dumbstruck crowd watched the ship rise in

the growing heat of the day to a height of 2,300 feet. The tension was
unbearable. From this height, all the spectators could see was a tiny dark spot
dangling beneath the body of the ship. Its detachment from there seemed
unavoidable. But the sailor on the rope, Charles Cowart, a boxer who was in
training for the Navy championship, refused to give up. He bound himself to the
rope in a way that conserved his strength, and after two hours of landing
attempts realized that his only chance of surviving was to climb slowly up to the
ship, and so he did. When it finally landed in the evening, the resourceful
survivor stubbornly refused to talk about the terrible hours he had spent in the
air. The Akron itself was shattered a year later when it ran into a storm off the
coast of New Jersey, and that put an end to the era of airships.
“If you happen to be a balloon handler, the answer is obvious,” says Wise. But
the important and hidden message applies to all of us: “When a situation starts to
turn bad, it’s easy to delude yourself into thinking that it might get better on its
own—until suddenly we find ourselves in such dire straits that our only hope is
to cling on for dear life.” Whether you have invested in shares or in a
relationship, the principle of “one foot on the ground” goes for you too. Instead
of trusting in your willpower muscle that is prone to erosion even in the most
steadfast among us, determine ahead of time the price or circumstances at which
you will let go of the rope. If the book is not sufficiently interesting by page 70, I
forgo reading the rest. If the bank mistakenly bounces a check of mine more than
twice, I leave it. And if an entrepreneur I’ve invested in shows me more than two
erroneous representations, I don’t continue investing in him.

PICKING YOUR BATTLES
Our willpower depletes like a muscle – let’s save it for
when we really need it
If eternal life is the Holy Grail that humanity aspired to since time immemorial,
then as of last century it has been replaced by “success”—the devout wish of
many, who are even prepared to curtail their lives to achieve it. Even someone
who believes he would learn more about human nature from reading the 37 plays
of Shakespeare than from reading an entire library of studies in the behavioral
sciences can no longer overlook a wide study that indicates that the contribution
of talent to success is limited. With talent explaining no more than 25% of your
success, self-discipline, determination and persistence carry much of the weight
when it comes to attaining the longed-for economic and social prize, mainly
because these traits serve you well in handling obstacles and inevitable mishaps
on the long road to success. Walter Mischel and his colleagues found in a
seminal study from 1972 that children between the ages of four and six who
managed to delay gratification for a few minutes and hold off on eating a
marshmallow that was placed in front of them, in return for receiving two
afterward, also proved more successful in their academic studies years later.
Likewise, if you were to analyze the résumés of successful individuals who like
sharing their stories with readers of their autobiographies, you would be hard-
pressed to ignore two conspicuous common denominators: determination and
willpower (another common denominator, which is absent from these books for
some reason, is luck).
Roy Baumeister is considered a leading researcher in the field of willpower. In
a lecture in 2011 to an intellectual group known as Zurich Minds, he surveyed
other fields in which self-restraint is known to have a positive effect: success in
relationships over the long run, mental and physical health, and even life
expectancy. Baumeister also clarified in his lecture that the folk belief that holds
there are different kinds of willpower is not substantiated by research. In the
introduction to his 2011 book Willpower: Rediscovering the Greatest Human
Strength (cowritten with John Tierney), Baumeister deals with the odd and
mistaken way in which we treat this subject’s importance. When asked to name
our good qualities, he says, we mention sincerity, courage, creativity, humor and
even modesty, but not self-restraint. That quality will appear at the end of the

list, if at all. By contrast, when people are asked to cite their flawed traits, self-
restraint jumps to the top of the list.
Everyone’s life is based on satisfying one’s desires, most of them the dictates
of nature and evolution; some the products of sublimating basic human impulses
that are satisfied by tools that the important scientific developments of the last
century have to offer. So, for example, the abundant means that social networks
and other online content provide allow us to satisfy our curiosity and to feel
wanted or like we belong. But an unbridled response to our desires distances us
from the ability to achieve important goals we set for ourselves and might even
jeopardize our health. According to one study, more than a third of deaths in
Western societies can be attributed to the long-term consequences of succumbing
to such desires (sex, drugs, unhealthy food and also work). In a hedonistic world
rife with distractions, success derives from the ability to resist temptation more
than from money, appearance or intelligence. How does this work exactly?
Researchers Wilhelm Hofmann, Kathleen Vohs and Roy Baumeister have
collaborated for a long time in an effort to trace the frequency and intensity at
which we are subject to the influence of our desires—the behavioral and
personal translation of our hidden impulses. To what degree, they ask, do these
urges collide with important personal goals we have defined for ourselves?
Under what circumstances do we resist desires so as not to harm those goals?
And more importantly, what distinguishes the circumstances in which we
succeed or fail in our efforts to resist?
The researchers asked 205 participants, residents of the German city of
Wurzburg, to report on their urges for the course of a week. Seven times a day,
the researchers texted participants on mobile phones distributed for the purpose
of the study, and asked them to report whether they had experienced a temptation
in the previous 30 minutes, and if so, to assign it to one of 14 predetermined
categories. Participants were then asked to note the intensity of the urge and the
extent to which its realization clashes with personal goals, and to report whether
because of this conflict they had exercised resistance that succeeded in
controlling the urge slightly. All the reports were documented using a mobile
phone application prepared for the purpose. The study benefited from a
remarkably high compliance rate (92.2% of all participants) and yielded 7,827
reports on urges and desires with which the participants had contended—a
faithful reflection, in the researchers’ opinion, of the array of temptations we
routinely experience.
The data collected suggested that there is broad variance in the frequency of
the urges reported, their intensity, and the intensity of the clash between them
and important personal goals. It turned out that half the urges reported conflict to

some degree with goals, values or other impulses of ours. The urges that topped
the list in terms of intensity and relative prevalence were, surprisingly, the wish
to sleep, and, unsurprisingly, the desire for food and drink, sex, social contact,
leisure and meeting hygiene needs. The researchers also designated categories
for personal goals that conflicted with succumbing to temptation. Among the
goals: good health (conflicts with the desire for sugary goodness, for example),
savings (conflicts with the temptation to shop, for example), professional and
social accomplishments (conflicts with the desire to sleep, among other things)
and effective use of time (conflicts with media consumption, for example).
The desires for sleep, sex, leisure, food and spending money conflicted most
starkly with other goals of participants and therefore rated highest in attempts at
self-control, most of which were successful. The desire to use media—check e-
mail, surf social networks and watch television—which also appeared frequently
on the urges scale, met with the highest failure rates when it came to exercising
willpower resistance (42% of those who reported resisting the urge were unable
to overcome it). This was also the sad fate of resistance to work, which the
researchers defined as one of the temptations.
In addition, the researchers noticed that willpower is depleted when exercised,
similarly to how a muscle used for a lengthy time is worn down when its
endurance comes to an end. Whoever exercised frequent inner resistance in the
face of the temptations he reported discovered that his ability to resist further
temptation later in the day had waned, especially by the end of the day. The
importance of this finding, which had already been discovered in lab
experiments, is difficult to overstate. Years of believing in the power of the
unconscious to guide our actions have minimized the place of willpower in
affecting our fate. The observation that willpower operates like a muscle is based
on the assessment that human ability to control impulses is the product of
relatively late evolutionary development, and is therefore unstable and difficult
to use for an extended stretch of time.
The physical attributes of willpower are also affected by, among other things,
nutrition (glucose in the bloodstream has a positive impact on willpower) and
sleep. The decisions we make on an empty stomach or in a state of fatigue are
very different in nature from decisions made in the opposite situations. In terms
of utilizing the willpower muscle, we are talking about a zero-sum game—if you
overuse it at work, when you come home you may lose patience with your
partner or spouse, or plunge into a binge of unrestrained eating.
The practical significance of this important finding is that if willpower indeed
acts as a muscle, then a strategy of maintaining a daily schedule that avoids
temptations might be more effective than exercises to strengthen willpower.

Such a schedule would be based on habits and routines that limit the chance of
encountering temptations: regular meal hours, two to three cups of coffee a day,
a time limit on television viewing, and so forth. I met a woman who has adopted
a rule of throwing out an old item of clothing for each new item of clothing she
buys, to resist the fashionable and expensive temptation of an up-to-date
wardrobe, and I know someone who has decided not to drink on Sundays and
Mondays so he can limit his alcohol intake without having to cope with the
temptation all over again daily. People with strong self-control use it to arrange a
daily schedule that is low on temptations more than to resist incidental
temptations. And when they do encounter these, their willpower muscle is fresh
and ready to cope. People with willpower try to plan their lives in such a way
that they perform as an acquired habit a good share of the actions for which most
people require self-control and willpower. If you have decided that you tidy up
your study every Sunday morning, you will need less willpower to fulfil the task
because the task has become a habit and habits do not deplete willpower. By
contrast, those who do not possess a developed willpower muscle must take care
to select the truly important things for which they are prepared to tire it out.
Instead of always trying to excel, it is better to be good at what really matters.
Hofmann et al.’s study presents modern life as a routine of phased desires and
urges that generally clash with personal goals or values we hold dear, and
therefore encounter resistance. While most of our attempts to fend off temptation
are successful, we may be defeated by certain temptations on a given day
because of their nature, but no less so because of the history of exercising
resistance that day. If we sum up the researchers’ work, we will see that an
average citizen spends eight hours a day coping with sundry temptations, three
hours resisting them, and half an hour delightfully giving in to temptations, some
of which he had earlier managed to resist. The actress Mae West said it: “I
generally avoid temptation unless I cannot resist it.”

ONE DAY, WHEN I’M YOUNGER
Hope is the emotion that most affects our lives
“Hope is a good breakfast, but it is a bad supper” 
Francis Bacon
Not long ago, I visited friends who live near the neighborhood where I myself
lived over 30 years ago. When I began driving home, which today is far from
there, in another city, I went by the block of buildings where I spent so many
years. A wave of nostalgia swept over me when I passed by the sports club
where I had won and lost so many tennis games, near the commercial center that
has undergone a complete facelift, and alongside the broad sidewalks that remain
as inviting as ever.
I tried to understand the secret of why memories from early periods of our
lives have such allure for us when we are older. After rejecting all of the other
answers, I was left with this explanation: we feel a powerful longing for times
when “everything could still happen.”
But wait a moment—in my case, almost everything did happen. Most
everything I could have dreamt of then, and much more, actually came true in a
rich life of action and leisure. So, for me, why indeed are memories from the
past so enchanting? I mulled over another series of possible answers, and
suddenly it struck me: I miss the sense of hope. The dream that one day, later on,
something good will happen. Something that will free me from the worries of the
present, fill me with a sense of satisfaction and enable me to view the world
painlessly, from a safe distance. The longing for hope is completely disconnected
from the fact that—for me, at least—my dreams came true. The longing for hope
stands on its own, and is not connected to actual events.
Ostensibly, an adult who is not in denial of reality should already know what
he can expect and what will never occur. Hope was supposed to be reserved for
young people, for whom “everything is still possible,” and in whose shoes we,
the older folks, would like to be. Hope is an assault upon Divine Providence,
argues Gustave Flaubert. But it is also an assault upon life experience that limits
what can still occur, upon early signs of what will never occur, signs that we tend
to ignore. And as Flaubert suggests, it is also an assault upon faith in the will of
a Supreme Power, who is busy at the moment somewhere else. But hope is
stronger than all this, and we need it to balance our lives, young and old alike.
Hope is born in sin. Greek mythology tells about Prometheus, who stole fire
from Mount Olympus and gave it to human beings. Zeus, the father of the gods,

became enraged and punished humankind by building a box (a jug in the original
version) that contained every possible evil. Pandora, the first woman created by
the gods, received the box and was warned not to open it. She, like most of the
heroes of mythology, could not resist the temptation (otherwise, mythologies
would be reduced to a few pages), and thus all of the illnesses, hardships and
afflictions we know were released into the world. Hope, which was placed at the
bottom of the box, remained inside for safekeeping in order to strengthen
people’s hearts.
From there, hope traveled a long path until finding itself in one of the
prominent works of art of the 19th century. Hope, painted in 1886, is the most
famous work of George Frederic Watts, who specialized in painting allegories
about the existential condition of humankind. Hope is personified by a young
woman with a blindfold over her eyes, sitting on a globe and playing a lyre. All
of the musical instrument’s strings are broken, except for one. Hope’s head leans
toward the instrument, eager to hear every faint sound of the lone string. Critics
described the painting as resistance to despair when it appeared in a Paris
exhibition in 1889. Today it is part of the Tate collection in England, displayed
in the comprehensive permanent exhibit of the last 500 years of British art.
Watts’ painting has a long tradition of encouraging and strengthening people
in distress: Nelson Mandela hung a reproduction of the work on the wall of his
prison cell and Egypt distributed small copies of the picture to its defeated
soldiers after the Six Day War. In a sermon devoted to the subject of hope in
1990, Pastor Jeremiah Wright described Watts’ depiction of hope: “… with her
clothes in rags, her body scarred and bruised and bleeding, her harp all but
destroyed and with only one string left, she had the audacity to make music and
praise God … To take the one string you have left and to have the audacity to
hope … that’s the real word God will have us hear … from Watt’s painting.”
Barack Obama, then 29, was in the church when Wright delivered this sermon
and later adopted the expression “audacity of hope” as the title of his stirring
keynote address at the Democratic National Convention in 2004 and as the name
of his second book.
Indeed, every beginning author knows that many books would never have
reached the second chapter without offering at least a hint of hope for the
hero/main protagonist—or, even more importantly, for the reader. From the rich
reservoir of poetic references to hope, I’ve chosen two inspiring quotations that
illustrate this subject: “‘Hope’ is the thing with feathers / that perches in the
soul” (Emily Dickinson) and “Hope springs eternal in the human breast”
(Alexander Pope).

An Engine Named “Hope”
Israel’s national anthem speaks of a hope of 2,000 years, but psychology started
to seriously study hope only about 20 years ago, as psychologists slowly began
to take interest in new fields of research. Previously, psychology had focused
almost exclusively on the negative dimensions of our feelings.
The psychologist Charles Snyder, one of the pioneers of positive psychology
(which generated the change in the balance of research) is largely responsible for
reviving the study of hope. In his book Psychology of Hope: You Can Get Here
from There, published in 1994, Snyder presented a “theory of hope” that defines
hope as the sum of three components: setting a goal, having the willpower to
achieve it (“I can do this”), and having “waypower”—a mental roadmap for
achieving the goal (“I can find a way to do this”).
Snyder also proposed a Hope Scale, based on a respondent’s agreement (on a
scale of 1 to 8) with six statements. Three statements try to assess the
respondent’s determination to achieve existential goals (“In the current situation,
I’ll energetically pursue my goals”), while the three others seek to examine the
respondent’s confidence in his or her ability to find ways of achieving them (“I
can think of many ways to achieve my goals”). The determination to achieve the
goal represents the (mental) power, while the ability to find ways represents the
direction. One should not confuse optimism with hope. Optimism, a close
relative of hope, meets only part of Snyder’s scientific definition because it leads
us to hope for the best without explicitly telling us how to get there. Indeed,
while the roots of optimism are planted in the present, hope sets its eyes on the
future.
After defining a method of measuring hope, Snyder continued with a series of
experiments examining the connection between a person’s hopefulness and
academic achievements. In one experiment, he asked students: “You expected to
receive a ‘B’ on an exam that constitutes 30% of the final grade, but received
only a ‘C.’ A week has passed. What do you plan to do about it?” Students who
ranked high on the Hope Scale were determined to try to find a way to improve
their grade, while students with a low level of hope completely gave up. Again,
it turned out that those who enjoy a high level of hope are sure that everything
will work out in the end—and if this hasn’t occurred, that means the end has yet
to come.
In another experiment, Snyder and his colleagues examined whether there is a
correlation between a student’s achievements and level of hope. They found that
the level of hope was a better predictor of grade point average than SAT scores
(which are highly correlated to IQ). Students who ranked high on the Hope Scale
were also more likely to complete their studies. Similarly, the researchers found

that the ranking of law students on the scale of hope was a better predictor of
their achievements than any other factor, including LSAT scores.
Other studies have shown a correlation between hope and success in a wide
range of fields, and not only in academia (where it is more important than
intelligence). Among professional athletes, for example, hope was found to be
more important than self-esteem and mood—and sometimes even more
important than natural athletic ability. Incidentally, athletes are more hopeful to
begin with than non-athletes.
The cognitive psychologist Scott Kaufman argued in his blog in January 2012
that talent, ability or skill will not get you “there”—however you define it. “A
wealth of psychological research over the past few decades show loud and clear
that it’s the psychological vehicles that really get you there,” he wrote. “You can
have the best engine in the world, but if you can’t be bothered to drive it, you
won’t get anywhere.”
Over the years, psychologists have proposed many different vehicles,
including grit, self-awareness, optimism, passion and inspiration. While these
are all important, Kaufman believes that one vehicle is “particularly undervalued
and under-appreciated in psychology and society. That’s hope.”
He goes on to explain: “Hope is not just a feel-good emotion, but a dynamic
cognitive motivational system.” According to this view of hope, emotions appear
after cognition, and not vice versa. That is, hope precedes the positive feeling.
The behavioral alternative Kaufman identifies among those who lack hope is the
choice of “mastery” goals—easy tasks that offer limited possibilities for growth
and do not promote a vital belief in controlling one’s own destiny.
Research that compared the contribution of various traits to success found that
hope leads by a large margin over self-efficacy (the belief that one can master a
domain) and optimism. Those ranked as hopeful also reported a higher level of
personal wellbeing (happiness, in pure scientific language). Shane Lopez, one of
the leading researchers in the field and a student of Snyder, analyzes the
psychology of hope in his book Making Hope Happen, published in 2013. Lopez
argues that hope is the most important indicator of a healthy and happy life, and
of success in relationships, careers or business. It is clear from reading his book
that Lopez sees hope more as a strategy than a feeling. Together with the Gallup
polling company, Lopez developed a questionnaire to assess the level of hope,
involvement and emotional wellbeing of pupils in the United States between the
ages of five and twelve, with the understanding that this data is what ultimately
determines the education system’s ability to meet its goals. Over a million pupils
have filled in this questionnaire.
We’re accustomed to thinking that our current ability is the best predictor of

our future success, Kaufman notes. However, he argues that numerous studies
have demonstrated that the psychological vehicle is more important; it is what
eventually brings us to our goal. And hope is an especially important vehicle,
perhaps the most important of all.
To Swim or Die
Faint-hearted readers are welcome to skip the next two paragraphs as they
describe one of the cruelest experiments in the history of psychology.
The psychobiologist Curt Richter, who sought to examine the connection
between the stamina of lab mice and water temperature, placed each helpless
rodent in a separate container and filled the containers with water. Richter chose
containers whose shape did not allow the poor animals to climb on them, and
they were left with the cruel existential choice of swimming or drowning.
Richter and his colleagues discovered that even when the water temperature was
identical, there were very wide disparities in swimming time among mice with
similar physical characteristics, ranging from several minutes to many hours of
swimming before drowning. The researchers tried to discover the reason why
some of the participants in the experiment pushed their rodential endurance far
beyond that of their fellow sufferers.
Later in the experiment, instead of tossing the mice straight into the
containers, the researchers took the mice in their hands and released them,
allowing them to momentarily escape the sad fate awaiting them in the water.
This process was repeated several times and only then were the mice placed in
the containers. There, they were showered with water for several minutes before
being returned to their cages to recover. This process was also repeated several
times. At this stage, the researchers believed, the mice were ready for the
gruesome test of “swim or die.” The mice that underwent the process described
here swam for over 60 hours on average before surrendering to exhaustion and
drowning. The researchers assumed that once the mice had tasted a bit of
freedom, their rodent minds associated it with their efforts to escape, leading
them to believe they had some “control” over their fate. The sense of control
they acquired (contrary to actual control) was enough to give them the hope that
if they continued to paddle the water with their little legs, they might survive.
A large number of studies have affirmed the notion that positive feelings in
general and hope in particular have a positive impact on our health and
resilience. In his book The Anatomy of Hope, Jerome Groopman, a professor at
Harvard Medical School, describes the role of hope in the recovery of several

cancer patients, including a number of his colleagues. Hope, Groopman argues,
gives many patients the ability to cope with the side effects of the aggressive
chemical treatments and devastating radiation sometimes required to eradicate
the disease.
Two of the studies Groopman describes are particularly illustrative of the
power of hope in relieving pain. One of these studies, conducted at the
University of Turin in Italy, examined the pain response of volunteer examinees.
A sort of cuff was attached to their arms that could be tightened to a level that
induced real groans as it quickly cut off the flow of blood. Before each
successive tightening of the scientific torture device (which measured pulse,
blood pressure, perspiration and muscle contraction), the examinees received a
shot of morphine and, as expected, did not express any pain.
After the researchers repeated this process several times, they stopped
injecting morphine and replaced it with an innocent saline solution—without
informing the examinees. The examinees, believing that they had received a
painkiller, expressed no pain. Fabrizio Benedetti, who conducted the research,
concluded that the examinees’ belief that they had received another dose of the
effective morphine and their expectation that it would free them of pain,
activated a release mechanism of endorphins and other painkillers that the brain
produces. The expectation and belief, according to Benedetti, are also able to
block the activity of other substances in the brain that are responsible for
intensifying the sense of pain.
In the second experiment that Groopman describes, 180 patients suffering
from arthritic symptoms in their knees were treated in one of two ways. Half of
them underwent full arthroscopic surgery to solve the problem, while the other
half underwent “sham placebo surgery”—which included minor incisions and
splashing of saline solution. Both groups received identical preparation for
surgery, remained in the operating theater for the same amount of time and
received the same care from the nurses, who didn’t know which patients
belonged to each group. Most importantly, the patients in the two groups showed
similar results of recovery after the (different) procedures they underwent.
The key components in the two experiments described above, belief and
expectation, underlie the placebo effect, the ultimate example of the connection
between body and soul. According to this effect, one of three patients who take
an innocent pill that resembles a medication and believe that they took a real
medication, reacts to the placebo as if it were real, even if only for a limited
time. This mechanism apparently begins as the nervous system’s response to a
person’s expectation and belief that positive change will occur. And this, in turn,
triggers a chain reaction that improves the chances of recovery and

rehabilitation.
Some of the new researchers of hope have tried to slightly broaden this field
of inquiry to include religious prayer. In one study, researchers sought to
determine whether the knowledge that someone is praying for the patient, rather
than the prayer itself, affects the patient’s state of health. The results in this
study, which was conducted with superb statistical precision, confounded the
researchers: patients who underwent bypass surgery and were told that people
were praying for them (which was true), developed a significantly higher
percentage of complications than patients in a similar condition who were told
that people might or might not be praying for them (regardless of whether or not
people actually prayed for them).
Benjamin Moses, who describes this experiment in his excellent book The
Truth of Scientific Medicine, offers a possible explanation for this phenomenon:
a person who is seriously ill or needs to undergo a complicated operation might
regard the fact that someone is being asked to pray as evidence of the
seriousness of his condition. Perception of his situation as serious increases
anxiety and is liable to adversely affect the recovery and healing process. That is,
hope has a positive impact when it emerges autonomously. Others who hope for
us, and tell us this, might actually upset the delicate mechanism that makes hope
one of the most influential positive emotions.
“A leader is a dealer in hope,” said Napoleon. Two hundred years later, the
iconic portrait of Barack Obama, imprinted with the word “Hope,” became the
symbol of his presidential election campaign (“Yes We Can”) and attracted the
largest number of volunteers ever mobilized for a presidential campaign in the
United States. In his speeches, Obama showed that he was very familiar with
Napoleon’s insight: “Hope is not blind optimism. It’s not ignoring the enormity
of the task ahead or the roadblocks that stand in our path,” he declared in one of
his speeches. “Hope is the belief that destiny will not be written for us, but by us,
by the men and women who are not content to settle for the world as it is, who
have the courage to remake the world as it should be.” Obama was quick to
understand that the test of politicians in the modern world is their ability to
provide hope to their citizenry—economic, political and social.
The philosopher Richard Rorty refers to hope as a “super story” that we tell
ourselves and which symbolizes for us a promise and chance for a better future.
In his view, both communism and capitalism can be defined as super stories.
However, in light of the failure of both, we need, at least from a social
perspective, a new story to inspire hope.

Snyder’s definition of hope (a combination of desire and direction) has never
been more relevant, as most of the governments in the developed world are full
of ambition but severely lacking in direction. To paraphrase Machiavelli, nothing
causes such great despair as the inability to find a reason for hope.

CHALLENGING THE BOTTOM-LINE
APPROACH
Valuing process over outcome
On September 23, 1999, a NASA research spacecraft—the Mars Climate Orbiter
—burned when it came too close to the red planet. The source of the accident
was discovered in the software, which mistakenly based its calculations on the
metric system instead of the imperial system. Due to this error, the spacecraft
was instructed to orbit Mars at a distance of 60 kilometers instead of the planned
and safe distance of 140 kilometers. Considering the fact that the spacecraft had
to travel 50 million kilometers to reach Mars, this error in calculation appears
negligible. Nonetheless, this made all the difference between a scientific success
and a resounding failure. A spacecraft that fails to reach its destination due to a
programing flaw is a simple example of a mission that does not meet the test of
ultimate outcome.
But not all events are so clear-cut. Intention and outcome entered the world in
tandem and sometimes it is even necessary to go to court to separate them. How
do we assess the service of a well-intentioned bear (swatting a fly from his
master’s face) when this service leads to an unintentional outcome (the master’s
broken nose)? Or an operation aimed at freeing hostages that results in a heavy
loss of life?
Such puzzling questions also appear in the world of literature. I found a
simple example in Through the Looking-Glass, and What Alice Found There by
Lewis Carroll. Alice tries to form her opinion of the two heroes of a poem
recited to her by Tweedledee, the almost twin brother of Tweedledum. The poem
tells about a walrus and a carpenter who gorge themselves on live and helpless
oysters while walking along the beach at night. At first, Alice is inclined to favor
the walrus from a moral perspective because he felt a bit sorry for the oysters he
devoured and even shed a tear for them, as recounted in the poem. But then
Tweedledee informs her that the walrus in fact ate more than the carpenter,
hiding his face behind a handkerchief in order to prevent the carpenter from
seeing how many oysters he ate. When Alice responds by shifting her moral
preference to the carpenter, Tweedledum quickly explains that the carpenter ate
as many as he could catch. Alice is confused, suddenly facing the moral
dilemma of judging by outcome versus judging by intention.

Alice is not alone. Important thinkers in other centuries have wrestled with the
question of what is more important—the outcome or the intention? Immanuel
Kant (1724–1804) believed that the intention was the more important of the two.
The first chapter of his book Groundwork of the Metaphysic of Morals begins as
follows: “Nothing in the world, or out of it, can possibly be conceived that could
be called ‘good’ without qualification, excepting only a good will.” That is, good
intention driven by good will is the determining factor in judging a person’s
morality. And an action whose consequences are positive but did not spring from
good intention is not considered moral. In contrast, John Stuart Mill (1806–
1873) offers an approach that is wholly focused on the outcome and completely
ignores the intention. In his view, “pleasure and freedom from pain are the only
things desirable as ends,” and every action is judged by its utilitarian result—
increasing pleasure or reducing pain.
The relation between intention and outcome is also a possible criterion for
examining the spiritual world, as various religions suggest. Catholics, for
example, prefer the test of intention to the test of action, as expressed in Paul’s
Epistle to the Galatians (and by the prophet Habakkuk): “The righteous will live
by his faith.” While observing the commandments is the main test of a Jewish
believer, the rabbinic tradition stresses that God takes greater interest in the heart
and the heart’s intention. This also explains the importance accorded to the idea
of repentance in Judaism.
If we indeed recognize the limitations of assessing the morality of our actions
by their ultimate outcome, we are free to grow from situations where we fail in
the test of end results. If we look to the East, we also see that even in the context
of the “law of cause and effect,” one of the foundations of Buddhism, the
intention is important. Karma, which describes a full cycle of cause–effect
relations in a person’s life, is based on the intention behind an action rather than
its physical expression.
Society uses law as a tool for narrowing the personal interpretation of events
in order to secure the social order. According to most of the world’s legal
systems, a crime cannot be defined without addressing both the emotional and
factual basis; if one of the two is missing, there is no crime. Someone who takes
an object from its owner with the intention of returning it later is not a thief, and
someone who kills a person unintentionally is charged with manslaughter rather
than murder. The emotional basis—a person’s intention during the action—is
central to the judicial process.
However, as philosophers are killed over the question of which of the two—
intention or outcome—is more important, and as religious wars erupt between
those who believe in action and those who believe in intention, and as reams of

legal arguments seek to distinguish between outcome and intention, the business
world operates as if the discussion has already been decided: salary level, growth
rates, return on investment and share price have long become the be-all and end-
all. The circumstances, the starting point, the support you received or the ethical
path you chose—all these are unimportant. Your economic achievements and
how far you’ve advanced in the race for social visibility constitute the bottom
line. But those who boastfully claim exclusive responsibility for the outcome are
in fact saying, in the same haughty breath, that they are also exclusively
responsible for all of the factors affecting that outcome.
Why does a person attribute success to his abilities, while blaming failure on
bad luck? Aren’t we failing here by underestimating the power of chance,
probably the most important factor in the equation of success? (And this is
regardless of whether we measure success by economic profit, social visibility or
other criteria.)
Nassim Taleb (author of Fooled by Randomness and The Black Swan) is
certain that economic success is largely a matter of luck, and that great success is
the result of a lot of luck. Indeed, the capital market—Taleb’s main frame of
reference—is the preferred playing field of randomness, the place where
coincidence and tailwind are the determining factors, more than professional
ability. We crown (and depose) business heroes primarily because of our flawed
understanding of the laws of statistics and the very human need to attribute
meaning to random events, and less because of their business acumen. Just as
one of a million monkeys equipped with typewriters and with unlimited time on
their hands may compose Hamlet, an investment manager, one of a great many,
may beat the market for 30 consecutive years—but both will do so through sheer
chance.
The economist John Kay does not subscribe to the approach that focuses on
the ultimate outcome. In his book Obliquity: Why Our Goals Are Best Achieved
Indirectly (2010), he argues that the most profitable companies are not
necessarily those that pursue profits at any cost; the wealthiest people in the
world are not the most materialistic; and the happiest do not necessarily chase
happiness. An approach that is not directly results-oriented is liable to yield
more than a targeted approach. For example, the credo of the pharmaceutical and
medical equipment company Johnson & Johnson, which describes the values
that guide its decision-making, places responsibility toward consumers and
medical personnel above its commitment to shareholders. Nonetheless, Johnson
& Johnson has achieved better long-term results than any other company in the
medical field, creating the highest value for its stockholders. And let’s not forget
—most of the important discoveries in medical history were discovered by

chance: X-rays, penicillin, the tuberculosis bacterium, insulin, Valium and
Viagra (although, as Louis Pasteur noted: “Chance favours the prepared mind”).
The test of outcome for these discoveries would primarily examine their
effectiveness (and the preparedness of the mind).
The world of sports has long been used as a metaphor for the more
competitive dimensions of our lives, those which are often judged by the end
results (“winning isn’t everything; it’s the only thing”). Therefore, it is not
surprising that the language of the business world draws a lot from the
experience of competitive sports, without noting that many professional athletes
are driven by the process no less than by the result.
Scott Molina, a winner and coach in international Iron Man competitions, says
that he tries to teach his students that if they learn to love the challenge in daily
training, the desired results will follow on their own. The legendary volleyball
coach Terry Pettit thinks that a good coach focuses on the process and not on the
final result. If a basketball player thinks about his final shot as determining the
fate of the game, he may become stressed and miss it. If, on the other hand, he
thinks about the familiar touch of the ball, the deep breath before shooting and
the support of his teammates, which is not conditional upon the shot’s outcome,
he’ll have a better chance of scoring the decisive basket. Tennis players know
that they should concentrate on winning the particular point they’re currently
playing or on improving their success rate on opening serves rather than
focusing on winning the entire match. And a baseball player who perfectly times
a fastball hurtling toward him at 100 miles per hour and smashes the ball to the
deepest corner of the outfield, only to be robbed of a home run by a spectacular
catch, can hardly be considered a failure, though the bottom line is that he made
an out.
In the Alexander Technique, which I’ve practiced for years, there is a concept
called “end gaining”—rushing while focused on the end result, without enjoying
the process at all. Professional athletes know the formula: the way to generate
results is actually by focusing on the process. Creators in all fields concur that
the process is no less important than the final product. The process is where they
spend most of their time and invest most of their emotional resources. Thus, they
expect it to be enjoyable, satisfying and inspiring. Correct emotional
management of the process is the real key for improving their level of work in
the future. And those who arrive at a different outcome than planned, often
discover that it offers them important possibilities for enriching their inner
world.
When it comes to the education of our children, the great danger in applying a
bottom-line approach to this process is that it reinforces the simplistic view of

the world as essentially a competitive place. Studies show that parents who
praise their children for their effort (process) contribute more to their success in
adulthood than parents who praise them for their accomplishments. The ability
to succeed is largely the ability to cope with failure, and those who won praise
for effort in childhood are able to mobilize the resources to tackle a challenge a
second time after failing in their first attempt. Those who won praise for
achievements are liable to give up when facing unanticipated difficulty. Parents
who impose excessive demands for achievement on their children and level
undue criticism when they fail to meet these expectations are raising future
perfectionists—people who cannot distinguish between a mistake and failure.
After all, the fear of making a mistake is the greatest threat hovering above a
child who was taught that parental approval and acceptance are conditional upon
their performance (rather than on their effort).
The modern test of outcome, the shortcut of a culture drowning in numbers
and information, completely disregards the process as a basis for analysis and
development. It is difficult to grow within our comfort zone, the place that is
familiar and secure. But stepping outside of this zone is impossible when there is
no room for making mistakes and even failing sometimes. Thus, the current
outcome becomes the sole predictor of the outcome in the future, with no room
for improvement. Moreover, in a culture where the bottom-line view prevails, we
become forgiving vis-à-vis the means used to achieve the ends, even when these
means are dubious.

HAPPINESS: CUT OUT AND SAVE
Ten rules that lead to happiness and that we control
“What was a Buddhist monk doing at the 2014 World Economic Forum in
Davos?” asks William Davies in his book The Happiness Industry, published in
2015. The monk, the author argues, like “happiness officers” in business
corporations, reflects a trend that has developed over the past decade: various
entities are increasingly interested in measuring how people feel, solely to
exploit this data for their own political or commercial needs. Brain scientists
working for the consumer industry hope to finally discover the “buy button” in
our brain, and advertisers for pharmaceutical firms seek research to substantiate
the claims of their costly products.
The flood of literature and research on happiness in recent years leaves no
room for doubt: while half of the population strives to be happy, the other half is
busy examining whether the former succeeded in finding happiness.
Sophisticated scanners that search for happiness centers in our brain are
suddenly replacing our simple subjective feeling: “Yes, I’m happy at the
moment.”
When the UK’s Office for National Statistics published its first report on
happiness in 2012, it was also able to cite regions and jobs in which British
citizens were happiest. It appears that the color green has a beneficial effect on
our happiness—and we’re not talking about greenbacks here. Rather, green
regions in Scotland with breath-taking scenery were home to the happiest people
and forest rangers led the list of contented workers.
Intuitively, we all understand what the studies show: above a certain level,
wealth does not bring happiness, though it can buy a lot of comfort. We also
know from experience that many of the moments that give us great pleasure
require only modest means or no monetary expense at all. Strangely, even
though we all feel that wealth does not guarantee happiness, we all must,
absolutely must, test this premise ourselves.
You can perhaps save yourself the frustrating mission of accumulating and
maintaining wealth if you are willing to truthfully answer the following simple
question: you have the choice of living in one of two worlds. In one, you earn
$5,000 a month, while most people earn only $2,500. In the second world, you
earn $10,000 a month, while most others are pocketing $20,000 monthly.
Assuming that the purchasing power of the money is identical in both worlds,
which would you choose? Tens of thousands of respondents in various studies

immediately opted for the world in which they earn more relative to others,
declining the option of earning more money in absolute terms. This cruel
compulsion to compare ourselves to others has plagued human society ever since
Homo sapiens took their first steps.
Then and now, we don’t compare ourselves to the Bill Gates and Warren
Buffetts of the world, only to about 150 people: acquaintances, family members,
schoolmates from elementary school or college, colleagues at work, partners in
athletic activity and so on. Why 150? This is “Dunbar’s number,” named for
British anthropologist Robin Dunbar, who asserts that 150 is the maximum
number of social relationships we can maintain, due to our cognitive and
emotional limitations. His complicated calculations are based on the correlation
between the brain size of non-human primates and the average size of the group
with which they roamed the African savannahs 150,000 years ago.
Cameron Marlow, Facebook’s in-house sociologist, provides fresh
confirmation of this theory in an interview with The Economist. According to
Marlow, the average Facebook member has 120 “friends.” The philosopher
Bertrand Russell showed a keen understanding of human nature when he stated:
“Beggars don’t envy millionaires, they envy other beggars who have more.” And
as the acerbic writer Gore Vidal admitted: “Whenever a friend succeeds, a little
something in me dies.”
The second reason why economic assets have a limited effect on our
happiness is their temporary nature. This phenomenon, coined the “hedonic
adaptation” or “hedonic treadmill,” reflects our rapid adjustment to changes in
general, including the acquisition of new material assets. The latest salary
increase quickly becomes the basis for the next increase, and the scent of leather
upholstery in the luxury car we bought dissipates a month later. This evolution-
based phenomenon tempers the extreme highs and lows we experience in
emotional events, bringing us back to our preset personal level of happiness. The
mechanism of hedonic adaptation is similar to that of a thermostat, warming or
cooling a room at a set temperature in changing conditions of heat and cold.
According to this view, each of us have a personal “happiness thermostat” that
defines the level of happiness we enjoy throughout most of our lives. Winning
the lottery will improve our feelings for a while, but within a year, more or less,
we’ll return to the basic level of happiness to which we’ve been calibrated. We’ll
also return to this level after a similar period following a traumatic injury in a
traffic accident. (Only the loss of a job or the death of a spouse requires a longer
adjustment period.) It is not surprising, therefore, that this phenomenon is
described in many cases as a treadmill on which we need to run in order to
maintain a set level of happiness.

More than anything, heredity determines the settings on our personal
happiness thermostat. It is customary to attribute 50% of a person’s happiness
level to genetic factors, and another 10% of our happiness capacity to a
combination of circumstantial factors (such as age, family situation, socio-
demographic profile, occupation, intelligence, external appearance and faith).
So, we have gathered here today to discuss the remaining 40%: the factors we
control that affect our happiness.
In the summer of 2004, the highly regarded analyst James Montier surprised
his employers at the DKW investment bank in his second-quarter report: instead
of providing economic insights as expected, he offered the bank’s clients some
tips on how to boost their happiness. Montier also was not reticent about stating
in his subversive document that the capital gains they might derive from his
economic advice would not necessarily make them happier. You won’t be
surprised to learn that this colorful analyst no longer works at the bank, but some
of his advice is still worth applying today. In 2007, I published a summary of his
conclusions in my first book, Do Chimpanzees Think About Retirement? I have
continued to follow the various sources of research mentioned in the book
(especially the work of the psychologist David Myers) and some of the
important updates published in the field, and I’m happy to present to you here
the ten rules for happiness, all of which are under your control.
1 Internalize the insight that happiness is not a function of economic success
Indeed, not only is it true that wealth doesn’t buy happiness; the belief that
material success leads to happiness actually breeds unhappiness. In part, this is
due to the destructive mechanism of comparison inherent in us—that is, we’ll
always find someone who is richer than us. This is also attributable to hedonic
adaptation, which modulates any emotional high stemming from economic
success.
The American psychologist Tim Kasser is the most prominent proponent of
the notion that materialism undermines happiness. In one of his studies on the
price of materialism, he examined the correlation between the happiness of
business majors and their materialism. He found that those who tend to measure
their self-worth in material terms of money and publicity report lower levels of
self-fulfillment, even when they meet the goals they set for themselves. On the
other hand, when students who prefer intrinsic values such as self-development
and community involvement succeed in meeting their goals, they report a higher
level of self-fulfillment.
In addition, it turns out that materialism and social isolation are mutually
reinforcing: lonely people compulsively pursue material assets, and materialistic

people are more exposed to the dangers of loneliness. It seems that materialistic
people are also more likely to develop personality disorders such as paranoia,
narcissism, anxiety and attention problems. You can learn a lot from the
illustrated video clip produced by Knox College, where Kasser teaches
psychology (see the list of additional reading at the end of this book). Kasser
practices what he preaches—he lives modestly with his wife, his two children
and several pets in a rural region of Illinois. In an interview with the American
Psychological Association (APA) in December 2014, Kasser noted an additional
social price that materialistic people pay: they are perceived as more
competitive, manipulative, selfish and lacking in empathy. He asserts that people
become materialistic because of the messages they receive from parents, friends
or the media, but also due to a lack of confidence, rejection and economic
anxieties.
A comprehensive analysis that Kasser conducted with colleagues from Sussex
University in England, comparing research spanning a relatively long period of
time (up to 12 years), helped to further erase any lingering doubt that
materialism indeed detracts from happiness, and makes depression and physical
pain more acute. Those defined as “materialistic” also reported that they have
fewer pleasant experiences and are less satisfied with their lives.
2 Invest in experiences rather than assets
When it comes to happiness, experiences have clear advantages over assets. First
of all, they are less affected by hedonic adaptation. Therefore, go see a show
instead of buying a new shirt; take a trip abroad instead of purchasing jewelry or
a fashionable watch. Experiences offer another important advantage: we can
embellish them in our minds when recalling them afterwards. And most
important—our identity is composed of the experiences we accumulate (and
what we remember from them), and not from a list of our assets. Contrary to
what many advertisers would have us believe, we are not what we buy.
And if you’ve already decided to purchase assets, then make them small
purchases that correspond to positive events in your life routine, rather than one
large expenditure that offers only fleeting pleasure under the heartless millstone
of hedonic adaptation. When will we finally understand that we cannot buy
enough (of what we don’t need to begin with) to be happy?
3 Make time for regular physical activity
There are few things on which researchers agree more. Modest physical exertion
for 20 minutes is enough to release endorphins into the blood stream; endorphins
are chemicals produced by our brains that help to relieve pain and stress, and

overcome minor depression. As their name suggests, these substances are similar
in structure to morphine, but their source is natural—the body itself. Regular
physical activity has a number of positive health benefits, which in turn affect
our happiness. The recommended dosage is 150 minutes a week of such activity,
preferably outdoors. Recent studies show that contrary to popular belief, there
are some marginal health benefits in exceeding this basic dosage, peaking at
eight hours of exercise per week.
4 Invest in developing close social relationships
In the introduction to his inspiring book Outliers: The Story of Success, Malcolm
Gladwell presents the mystery of Roseto, a small town in Pennsylvania, founded
by immigrants from a picturesque Italian village of the same name. The first
small group of immigrants arrived in 1882 and gravitated to this part of
Pennsylvania to work in slate quarrying—the traditional source of livelihood of
Rosetans for centuries.
Before long, a large number of immigrants from the Italian village, hearing of
the unlimited possibilities in the New World, joined the pioneering group in
Pennsylvania. By the end of the century, there were already several thousand
immigrants in the American town of Roseto, who continued to carry out the
customs of their native land, exuberantly conversing in the dialect of the Foggia
region of Italy. A spirit of social and economic momentum swept the isolated
town when a dynamic young priest arrived in 1896. Under his leadership,
spiritual societies were founded and festivals were organized. The visionary
priest encouraged the townspeople to plant gardens and orchards in their
backyards, and distributed seeds and bulbs. Other community institutions were
soon to follow and schools, a convent and parks created new social
infrastructure.
In the late 1950s, Stewart Wolf, head of the Department of Internal Medicine
at the University of Oklahoma, came to a farm near Roseto for his summer
vacation. In a casual conversation with a local physician, he heard that heart
disease was very rare among Rosetans under the age of 65. (This was in the
1950s, when heart attacks had reached epidemic proportions in the U.S. and
before the advent of cholesterol-lowering drugs and the use of aspirin as a blood
thinner.) Wolf had a hunch that there was a scientific sensation here and quickly
organized a research team to investigate the mystery. He initially believed that
this apparent immunity to heart disease could be attributed to the Rosetans’
lifestyle and eating habits, but soon discovered that 41% of their caloric intake
was fat-based and that they preferred wine to any other beverage, including milk.
Many of the residents were heavy smokers and overweight. The answer, Wolf

realized, was not to be found in the residents’ lifestyle.
Next, Wolf ruled out the possibility that the good health of Roseto residents
was genetic: he traced immigrants from the same Italian village who settled in
other locations in North America, and found that their mortality rate was similar
to the national average. An attempt to find the answer in the geographic region
was also to no avail. The medical records of the residents of two neighboring
towns presented a dismal picture: mortality rates from heart disease were three
times higher than in Roseto. Another statistical wonder was that widowers
outnumbered widows in the town, contrary to the situation elsewhere. It goes
without saying, of course, that the crime rate was zero.
Wolf realized that the answer was in the town itself. Indeed, as he walked
around town he noticed the residents chatting in the street, occasionally inviting
each other for an impromptu meal at their home, where three generations usually
lived under a single roof. Housewives were highly respected, and the elderly
were integrated into the community.
Wolf decided that the low rate of heart disease in Roseto was attributable to a
stress-free lifestyle. “The community was very cohesive. There was no keeping
up with the Joneses. Houses were very close together, and everyone lived more
or less alike,” he later wrote. The residents who did well were not comfortable
flaunting their success, and those who failed could easily hide their situation in a
society whose social compass was an egalitarian community. Simply put, those
who lived in Roseto during the first half of the 20th century could not feel lonely
(or envious).
This phenomenon, coined the Roseto Effect, triggered intensive research and
follow-up when first discovered. One study, which continued for no less than 50
years, found that as the Rosetans strayed from their Italian social heritage and
adopted the characteristics of American society, their rates of mortality climbed.
In 1971, the town recorded its first case of a heart attack suffered by a person
under the age of 45.
The story of Roseto represents for me the biggest innovation of happiness
research in recent years: community, someone to share life with in good times
and bad, and the development of close relationships contribute to personal well-
being more than most of the factors you’ll find in this list. Friends, and in a
certain sense a spouse, are not exposed to hedonic adaptation. We get used to
things that money can buy more quickly than things that money can’t buy.
5 Give your body the rest it deserves
Is it best to give up hours of sleep in order to advance a professional career and
the economic rewards it brings? If you ask the advice of Daniel Kahneman, a

Nobel Prize recipient in Economics and a major influence in behavioral research
on happiness, he would decisively reply that if the criterion for choosing
between the two is your happiness, then sleeping is more important than a salary
increase. The correlation between satisfaction in life and sleep quality is higher
than the correlation between satisfaction in life and income (and many other
factors), Kahneman asserted in an article published in 2006. Happy people are
very active, but make sure to give their bodies proper rest. A lack of sleep leads
to fatigue, loss of concentration and a gloomy mood. There is a positive
connection between proper sleep and improvement in memory and creativity,
less dependence on stimulants like caffeine, and even easier weight loss.
6 Control your time and set achievable goals
In 2004, James Smith and Beren Aldridge started a farm in the Lake District in
England for alternative treatment of patients suffering from various mental
disorders. The doctors or welfare officers of these patients had decided they
could not be treated with medication for one reason or another. It seemed like a
simple idea: the encounter with nature, milking cows and growing vegetables to
sell in the local market should have a positive effect on those who were
unnerved by noisy urban life. The program was very successful and the
condition of the farm “volunteers” (as they were called) significantly improved.
And the reason? The proximity to Mother Nature? Physical activity in the
open air? Tree hugging? Not according to Aldridge. The key factor, in his
opinion, was the freedom given to the “volunteers” to plan their daily schedules
and lives in a cooperative-like organizational structure. The gardening, pasturing
and harvesting had a limited therapeutic effect without the central component in
life at the farm: the members’ participation in decisions that determined their
daily routine and their appropriate place in the farm’s developing hierarchy.
We all tend to overestimate what we can accomplish in a single day, but
underestimate what we can achieve during the course of a year. Happy people
are aware of this and set achievable goals, at the daily level too. They feel happy
when they control their fate and meet the daily goals they set for themselves.
With all due respect to endorphins, I know deep inside that when I go out for a
run on a very cold day, I derive profound satisfaction from the fact that I’m
maintaining some control over my life, even when the weather conditions tempt
me to perhaps give up.
7 Flow with what you’re doing
“Flow” is a concept coined by Mihaly Csikszentmihalyi, a professor of Business
Administration at Claremont Graduate University in California.

Csikszentmihalyi, who would have long ago received a Nobel Prize if the
members of the Swedish Academy of Sciences could only pronounce his last
name, defines “flow” as a state in which we are completely immersed in our
current activity, but are not drowning in it. When we’re in the “zone” of flow, all
of our emotions are mobilized for performing the task at hand and we’re not
focused on anything else. Those in a state of flow (which brings to mind similar
states described in Eastern philosophies) report a feeling of spontaneous joy and
excitement. Flow is conditional upon maintaining a delicate balance between the
challenge posed by a particular task and our skill in performing it: skill that is
greater than the challenge leads to boredom, while a challenge that is greater
than the level of skill leads to frustration.
Flow is not achieved when you’re cruising on a private yacht in the
Caribbean, agonizing over the question of whether it’s too early in the day for a
cocktail or wondering whether your bathing suit meets the latest fashion craze.
Flow is easier achieved when knitting or arranging your bookcases during the
weekend. And those who write books are familiar with the surprised look at the
clock when discovering that it’s already two in the morning.
8 Turn off the TV
And this is going to hurt. More and more studies have shown the negative
connection between watching television and happiness. Watching television is
not consistent with almost any of the rules we’ve already listed. In the test of
comparison, we’re doomed to watch people on TV who are more attractive than
us, smarter than us and more skilled than us in operating various types of exotic
weapons. And if you claim that participants on reality programs are just people
like you and me, then you’ll have to admit that sometimes you can’t help
thinking that perhaps they’re a bit more courageous.
If you’ve already come to terms with the fact that materialism harms you, Tim
Kasser offers another insight. His research also shows that the more people
watch TV, the more materialistic they become. Television networks are driven by
commercial motives, and those can be met almost only through advertisements.
The last thing that advertisers think about when formulating their messages is
our happiness. On the contrary, they’ll succeed in selling us their products only if
they make us feel miserable and convince us that the only way to cure our
misery is to buy the product they’re promoting.
This is also the logic that suggests limiting the use of social networks. Any bit
of joy that wasn’t already destroyed by comparing our lives to our friends’ happy

experiences, as posted on social media, is immediately eroded by advertisements
that are tailored to our needs with a precise and ghastly statistical scalpel. An
eye-opening study Kasser conducted with Jean Twenge, a psychology professor
at San Diego State University, found that the materialism of adolescents
increased in direct correlation to the growth of advertising spending in the U.S.
economy.
We’re likely to spend more of our lives opposite the television screen than in
the office; we won’t be using this time for physical activity or for developing
social relationships. Indeed, obesity and social isolation are directly correlated
with watching more than two hours of television a day and eating in front of the
TV. An exceptionally comprehensive study, published in 2008 and based on a
sample of 30,000 people, reveals that people who are not happy in their lives
spend 30% more time in front of the TV screen than people who are happy.
In Happiness by Design: Change What You Do, Not How You Think,
published in 2014, Paul Dolan argues that the delicate balance between pleasure
and meaning is the key to happiness. According to Dolan, an economics
professor who advises the British government on “personal well-being,”
watching television is an excellent example of pleasurable yet meaningless
activity. Raising children, for example, is the opposite: not always pleasurable,
but meaningful.
In addition, in many cases we watch television by ourselves, and we’ve
already learned that those who distance themselves from friends and family also
distance themselves from happiness. T. S. Eliot defined this well: “Television is
a medium of entertainment which permits millions of people to listen to the
same joke at the same time, and yet remain lonesome.”
9 Step outside of yourself, connect with something bigger, volunteer and
help the needy
Happy people tend to share their good fortune with others—the “feel good, do
good” phenomenon. However, it turns out that this works both ways: when you
give to others, you boost your own sense of happiness. Abraham Lincoln
recognized this 150 years ago (“When I do something good, I feel good”), and
today behavioral scientists are confirming that people who volunteer, help others
or express altruism in other ways feel better and often experience the type of
“high” that many feel after physical activity. Is this merely the release of
hormones that improve our blood flow or is the mechanism of social comparison
again rearing its head? Hormones are important, but let’s also recognize: when
we help another person, we also feel—at least in comparison—that our situation
is not so bad.

If you also choose to participate in a project aimed at reducing social
inequality, you may be a double winner. Shigehiro Oishi and Ed Diener, highly
esteemed researchers of happiness at the national level, found in a study
involving 150,000 American respondents that the happiness of U.S. citizens
during the years 1972–2008 was inversely related to the level of economic
inequality. The more inequality, the less happiness. And vice versa—the more
equality, the more happiness.
10 Be grateful and don’t chase happiness
You don’t need to be a Buddhist monk to feel gratitude for your blessings:
family, friends, education, health or freedom, to mention a few. Proponents of
positive psychology, a relatively new branch in behavioral science, claim that if
you remember to be grateful every evening about three good things that
happened to you that day, you’ll notice a positive change in your mood within
three weeks. If you think a moment about the damage caused by the worm of
social comparison, which hungrily gnaws away at us, you’ll understand why
thankfulness about our blessings, without any comparison to others, is a sure
prescription for improving our sense of personal well-being.
Don’t chase happiness as such, because even though we usually know when
we’re happy, sometimes, as John Barrymore noted, “happiness often sneaks in
through a door you didn’t know you left open.” And in other cases, we know that
we’ve received a royal visit only after happiness, a bashful fellow who prefers to
evade our glance, has left our room. Since most of us don’t know what exactly
makes us happy, the focused pursuit of happiness is doomed to fail. Happiness
has been compared to a butterfly—the more you chase it, the more it eludes you.
However, if you focus on other things, it might just come and sit on your
shoulder. And it’s important to remember: if success is to achieve what we want,
happiness is to want what we’ve achieved.

A BIT OF HUMBLE PIE GOES A LONG WAY
Overvaluing confidence, we’ve forgotten the power of
humility
“If I only had a little humility, I’d be perfect,” the media mogul Ted Turner
supposedly said sometime in the 1990s, in a moment of narcissistic exuberance.
While Turner has been much humbler since, today’s breed of tech entrepreneurs
often display a similar arrogance.
Why be humble? After all, Aristotle said: “All men by nature desire to know.”
Intellectual humility is a particular instance of humility, since you can be down-
to-earth about most things and still ignore your mental limitations. Intellectual
humility means recognizing that we don’t know everything—and what we do
know, we shouldn’t use to our advantage. Instead, we should acknowledge that
we’re probably biased in our belief about just how much we understand, and
seek out the sources of wisdom that we lack.
The Internet and digital media have created the impression of limitless
knowledge at our fingertips. But, by making us lazy, they have opened up a
space that ignorance can fill. On the Edge website, the psychologist Tania
Lombrozo of the University of California explained how technology enhances
our illusions of wisdom. She argues that the way we access information about an
issue is critical to our understanding—and the more easily we can recall an
image, word or statement, the more likely we’ll think we’ve successfully learned
it, and so refrain from effortful cognitive processing. Logical puzzles presented
in an unfriendly font, for example, can encourage someone to make extra effort
to solve them. Yet this approach runs counter to the sleek designs of the apps and
sites that populate our screens, where our brain processes information in a
deceptively “smooth” way.
What about all the commenting and conversations that happen online? Well,
your capacity to learn from them depends on your attitudes to other people.
Intellectually humble people don’t repress, hide or ignore their vulnerabilities,
like so many trolls. In fact, they see their weaknesses as sources of personal
development, and use arguments as an opportunity to refine their views. People
who are humble by nature tend to be more open-minded and quicker to resolve
disputes, since they recognize that their own opinions might not be valid. The
psychologist Carol Dweck at Stanford University in California has shown that if
you believe intelligence can be developed through experience and hard work,
you’re likely to make more of an effort to solve difficult problems, compared

with those who think intelligence is hereditary and unchangeable.
Intellectual humility relies on the ability to prefer truth over social status. It is
marked primarily by a commitment to seeking answers, and a willingness to
accept new ideas—even if they contradict our views. In listening to others, we
run the risk of discovering that they know more than we do. But humble people
see personal growth as a goal in itself, rather than as a means of moving up the
social ladder. We miss out on a lot of available information if we focus only on
ourselves and on our place in the world.
At the other end of the scale lies intellectual arrogance—the evil twin of
overconfidence. Such arrogance almost always stems from the egocentric bias—
the tendency to overestimate our own virtue or importance, ignoring the role of
chance or the influence of other people’s actions on our lives. This is what
makes us attribute success to ourselves and failure to circumstance. The
egocentric bias makes sense, since our own personal experience is what we
understand best. It becomes a problem when that experience is too thin to form a
serious opinion, yet we still make do with it. Studies have shown that people find
it difficult to notice their own blind spots, even when they can identify them
easily in others.
From an evolutionary perspective, intellectual arrogance can be seen as a way
of achieving dominance through imposing one’s view on others. Meanwhile,
intellectual humility invests mental resources in discussion and working toward
group consensus. The Thrive Center for Human Development in California,
which seeks to help young people turn into successful adults, is funding a series
of major studies about intellectual humility. Their hypothesis is that humility,
curiosity and openness are key to a fulfilling life. One of their papers proposes a
scale for measuring humility by examining questions such as whether people are
consistently humble or whether it depends on circumstances. Acknowledging
that our opinions (and those of others) vary by circumstance is, in itself, a
significant step toward reducing our exaggerated confidence that we are right.
In the realm of science, if necessity is the mother of invention, then humility is
its father. Scientists must be willing to abandon their theories in favor of new,
more accurate explanations in order to keep up with constant innovation. Many
scientists who made important findings early on in their careers find themselves
blocked by ego from making fresh breakthroughs. In his fascinating blog, the
philosopher W. Jay Wood argues that intellectually humble scientists are more
likely to acquire knowledge and insight than those lacking this virtue.
Intellectual humility, he says, “changes scientists themselves in ways that allow
them to direct their abilities and practices in more effective ways.”
Albert Einstein knew as much when he reportedly said that “information is not

knowledge.” Laszlo Bock, the former head of human resources at Google,
agrees. In an interview with the New York Times, he said that humility is one of
the top attributes he looks for in candidates, but that it can be hard to find among
successful people, because they rarely experience failure. “Without humility, you
are unable to learn,” he notes. A little ironic, perhaps, for a company that has
done more than any other to make information seem instant, seamless and
snackable. Perhaps humility’s the sort of thing you can have only when you’re
not aware of it.

PART II
WHY SMART PEOPLE MAKE
STUPID MISTAKES

WHY DO SMART PEOPLE MAKE STUPID MISTAKES?
How our brains continue to protect us against threats that
no longer exist
Close your eyes and imagine that the entire history of the universe, from the Big
Bang to now, occurred in 24 hours. It started at midnight, but our precious sun
rose only at 3:43 PM. Bacteria, the first sign of life, appeared at 6:40 PM. Insects
showed up five hours later, and an asteroid hit the earth with a huge blast at six
minutes to midnight, obliterating the dinosaurs. Humans and monkeys came
down from the trees only at 31 seconds to midnight, and we Homo sapiens came
into being at less than a second to midnight. All of human history as we know it
happened in the last three hundredths of that second.
In that miniscule time span, in galactic terms, we have invented medieval
romance, the atom bomb, careers, nationalism (the last two only about 150 years
ago), and the wondrous phenomenon of reality shows. Our daily concerns are
nothing more than an evolutionary blink of the eye. This begs the question: if the
human brain has spent more time among insects and monkeys than with other
humans, shouldn’t that influence the way we respond to modern-day challenges?
Indeed, it appears that human development in recent centuries has been too rapid
for the slow process of evolution, and various parts of our brains have still not
adapted to the requirements of modern life.
Our similarity to other species indicates our limitations: we share 98.76% of
our genetic makeup with chimpanzees, the species most like us. It is hardly
surprising, therefore, that when facing a challenge, we are most likely to use a
skill that we share with chimpanzees and lesser species, without even knowing
we’re doing it. Our brain, like theirs, is a sophisticated machine constantly on the
lookout for threats. In its futile search for existential threats long gone from our
world, the human brain enlists its primitive ability to detect patterns: it is a law
of nature that survival depends on early detection of threat patterns.
For our brain, it is better to pay the price of 99 false alarms than miss one real
threat. The result is an apparently wasteful mechanism that does not “punish”
false detection or the enlistment of resources to ward off every detected potential
threat. In financial investment, by comparison, false detection of patterns or
trends is almost immediately penalized.
Much attention has been paid recently to the two systems that amicably share
our brains: the emotional system (1) and the rational system (2). The emotional
system is what we perceive as our “gut feeling”; its quick, automatic responses

are the brain’s default reactions. System 1 is responsible for our survival and
manages vast quantities of information coming our way through use of
heuristics, or shortcuts—often at the expense of sound judgment. But when your
life is on the line, speedy decisions are paramount. When we speak in our mother
tongue or utter truths, it is the emotional system in action. The same system
makes us pay closer attention to changes in a speaker’s tone than to what he is
saying. System 2 makes more complicated evaluations and subsequently okays
or changes decisions made by System 1. However, when it is distracted or too
slow (the emotional system is twice as fast), we wrongly assume that we’re
being rational when we’re actually responding through our highly bias-prone
emotional system.
The major drawback of the emotional system is its utter blindness to
probability. As a result, we allow random events to influence how we think.
Einstein thought that “coincidence is God’s way of remaining anonymous.” In a
similar vein, Nobel-winning physicist Wolfgang Pauli suggested that
coincidences are visible traces of invisible principles. When my wife and I run
into a long-lost childhood friend of hers in London, it takes an effort to remind
ourselves that in a city of ten million residents, a one-in-a-million occurrence
happens ten times a day. When it comes to coincidence, we tend to reject
statistics and succumb to the temptation of lending our lives a semblance of
meaning—which creates a sense of control. Ignoring the role that sheer
randomness plays in our lives makes us attribute success to talent and put failure
down to bad luck.
It is much more boring to acknowledge statistical truths like “regression
toward the mean”: a phenomenon in which a variable that is extreme on its first
measurement will tend to be closer to the average on its second measurement.
This tendency was first noticed by Francis Galton in the late 19th century, when
he observed that extreme characteristics (such as height) in parents are not fully
passed on to their offspring. Children of tall parents tend to be tall, but not as
much as their parents; children of short parents will be shorter than most, but
probably taller than their parents. The popular magazine Sports Illustrated had a
real problem on its hands when top athletes refused to go on its cover, because of
a rumor that appearing on the magazine’s cover would lead to bad performance
or injury in following weeks. The editors investigated what they thought was an
urban legend and discovered, much to their dismay, that it was true: in 913 of the
2,456 editions checked, they found that athletes’ performance dropped after
appearing on the magazine’s cover. Regression toward the mean explains this
perfectly: athletes grace front pages after outstanding achievements. After that,
barring some immediate leap in performance-enhancing technology, they will

naturally return to their average scores, which are not the ones to garner
headlines.
The media loves an unusual story, but never follows up on the dreary return to
average. All we receive is the number of road casualties on a particularly sad
weekend, a record high or low of soccer goals, or pilot performance in the Air
Force Academy as famously discussed by Daniel Kahneman in his Nobel Prize
acceptance speech. If you don’t know about regression toward the mean and tend
to seek out meaning and patterns, you’re bound to place too much weight on a
single unusual occurrence.
To be sure, nature provides us with a tempting abundance of random patterns.
With a large enough sample, we can detect any pattern we want: our brains will
connect the dots on their own. Look up at the sky on a starry night and you’ll see
whatever you wish—a lion, a scorpion or a dipper, you name it. British
mathematician and philosopher Frank Ramsey devoted his short life to studying
chaos. He found that a certain order can be discovered even in relatively small
samples. According to Ramsey’s Law, if we rearrange the first 101 numbers in
any order we like, we will always find 11 numbers in a sequential (yet not
necessarily consecutive) increasing or decreasing sequence.
Can you see the bad decision just waiting to happen here? On one hand,
primal parts of our brain look for patterns to detect threats; on the other hand,
nature (and humanity) create random patterns all the time. The meeting of the
two generates that all-too-familiar human weakness: overconfidence. Eighty
percent of us believe that we’re better drivers, lovers and parents than average,
and that we’ll live longer than our fellow students. Seventy percent of lawyers
believe that their case is more founded than that of their adversary. Given that
19% of U.S. citizens believe they’re in the top 1% in terms of wealth,
overconfidence is bound to make you attractive at any job interview or party.
Overconfidence has its roots in skills that our ancestors acquired in order to
deceive others, and—more importantly—themselves. To steal some of the hunt’s
loot before it was officially shared out, you had to know how to deceive the
others. Nowadays, we use deception mostly to improve our social status. How
annoying, then, that our brain doesn’t like it when we lie and keeps giving us
away by changing our physiological responses or choice of phrase and a variety
of other tell-tale signs. To get away with deception, we have to fool our own
brains first. It’s hard to cope with the failures that life throws at us without
developing some feisty repression mechanisms. The path to overconfidence
begins there.
Our present-day information overdose cultivates overconfidence: we think we
can efficiently process the endless stream of information coming at us from

every direction and always separate the wheat from the chaff. Both assumptions,
unfortunately, are wrong. In a famous experiment, psychologist Paul Slovic
tested the connection between levels of information and overconfidence. He
asked eight horseracing bookmakers what factors they thought determined a
horse’s chances to win a race. The outcome was a list of 88 variables such as the
horse’s performance history, the quality of the racecourse, and the jockey’s
weight. Next, Slovic gave them data on 40 past races based on the order of
importance they had listed. The data was presented in four stages: at first, five
variables, then ten, then 20, and finally, 40 factors together. At every stage, the
researcher asked the bookies which five horses they thought had won the race,
and how sure they were of their answer. He found that the accuracy of the picks
remained the same, no matter how much information the bookmaker had.
However, the bookies’ confidence in their answer rose sharply as the information
grew. Why go as far as a racecourse? Google’s search engine works better, the
less words you feed it.
Overconfidence leads us to bad decisions because it makes us act even when
we shouldn’t. Investors know that over-action is terrible for business, and a
particularly original study found that goalkeepers would do better standing still
during a penalty kick than leaping at one of the beams. Overconfidence also
makes us deny sheer chance, gives us a false sense of control and leads us to
underestimate other people’s reactions—all classic blunders that engender bad
decisions. People who get constant feedback about their decision, like
meteorologists, are less prone to overconfidence. Also, apparently, being
depressed can keep you from being overconfident and gives you a better grip on
reality than that annoyingly jolly colleague.
We make decisions based on three kinds of information: things we know that
we know, things we know that we don’t know, and things we don’t know that we
don’t know. The last category is influenced by overconfidence—we make better
decisions the more we acknowledge what we don’t know. But do we have to be
clinically depressed to do that? As previously mentioned, a relatively new
branch of psychology offers a more optimistic option termed “intellectual
humility” (see page 59 “A Bit of Humble Pie Goes a Long Way”).
Two leading researchers of intellectual humility, Peter Samuelson and Ian
Church, published an article in 2014 titled “Known Unknowns or: How we
learned to stop worrying about uncertainty and love intellectual humility.” They
stress how important it is for policy makers (Donald Rumsfeld, in this case) and
the rest of us to acknowledge that some information out there just isn’t known.
They define intellectual humility as “holding a belief with the firmness the belief
merits. Some beliefs, like the belief that 2+2=4, merit being held with the utmost

firmness; to do otherwise—to have serious, lingering doubts as to whether or not
2+2=4—is to be intellectually diffident or intellectually self-deprecating. Other
beliefs, like the beliefs regarding the number of angels that can dance on the
head of a pin, merit being held with very little firmness; to do otherwise, to be
convinced that exactly five angels can dance on the head of a pin—is to be
intellectually arrogant.”
The basic questions, however, remain unresolved: can one be aware of being
intellectually humble? If you proclaim that you’re intellectually humble, isn’t
that a form of hubris? Crime writer Helen Nielsen offered one answer: “Humility
is like underwear; essential, but indecent if it shows.”

AND MERCI TO THE FRENCH TEACHER
Why introducing a problem in a foreign language
overcomes major cognitive biases
We live in a time when existential challenges that accompanied prehistoric man
on the African savannah have been displaced by new challenges in the form of a
necessity to choose between options, whether in matters of consumption,
investment or career. While our emotional system is comprehensively equipped
with a number of outdated instincts that are focused on risk reduction and
immediate reward (before the food runs out), as we already have seen, its main
disadvantage is its utter blindness to probability and complex calculations that
are needed for meeting some of the modern world’s demands. The source of a
large share of our miscalculations is our confidence that our rational system is
operating, whereas we are actually responding to the domineering and prompt
emotional system. This phenomenon has been given the general name “cognitive
biases,” and many dozens of these have been identified to date. The individuals
most responsible for this are the researchers Amos Tversky and Daniel
Kahneman, and the latter’s book Thinking, Fast and Slow, published in 2011, is
a fascinating journey deep into the way we make decisions and a sad portrait of
the circumstances under which our minds succumb to the dictates of the
emotional system only to be mistaken in their decisions.
Two of the better-known biases belong to the prospect theory that Kahneman
and Tversky developed in the late 1970s—“presentation bias” and “risk
aversion.” According to the first, we tend to prefer one possibility to another
possibility of identical statistical expectancy, merely because of the way each is
presented to us. For example, if a person close to you is sick and needs surgery,
which would you prefer—an operation that has a 30% mortality rate or an
operation that has a 70% chance of ending in success? Risk aversion, on the
other hand, reflects the lack of symmetry between the emotional pain of failure
and the pleasure of success. Most of us, it turns out, are willing to pass on a
gamble that stands a positive chance of profit just to avoid the pain of loss. In
fact, as Kahneman and Tversky discovered in an important experiment from
1979, the potential profit has to be double the possible loss or more to offset our
risk aversion.
If indeed the emotional system is responsible for many of our decision biases,
it is interesting to ascertain whether making decisions while using a foreign
language creates an original bypass for the decision-making circuit in a mind

that does not even know there are foreign languages, the kind of bypass that
manages to neutralize the emotional component in the process. A group of
researchers led by Boaz Keysar of the University of Chicago picked up the
research gauntlet and tried to find out whether thinking in a foreign language in
fact reduces cognitive biases that may affect decision-making. The researchers
drew encouragement from earlier studies that had shown that people do not react
to taboo words, scolding or even words of love that are spoken in a foreign
language with the same intensity as they would if spoken in their mother tongue;
in other words, activation of the part of our brain that deals with processing
foreign languages reduces the influence of the emotional system, hopefully
allowing a more rational decision-making system to gain ascendance.
The researchers conducted six studies on three continents with more than 600
subjects who speak five different languages. In the first series of experiments the
researchers presented subjects with a version of the “Asian flu problem” that
Kahneman and Tversky had developed when they wanted to model presentation
bias. The pair presented their subjects with an imaginary scenario in which the
United States is preparing for the outbreak of an epidemic. Subjects were asked
to choose between two action plans in view of the impending epidemic. One is
perceived as relatively safe by ensuring the survival of a third of the patients; the
other seems riskier, might save the lives of all, but if it fails, two thirds of the
patients will die. Even though in statistical utility terms both alternatives are
identical (only the cost/reward ratio is different), it turns out that people tend
toward clear preference for what looks like a safer option when the outcome is
presented in a positive light (patients will be saved), but tend to bet on the risky
option when they are trying to avoid an outcome that is presented in a negative
light (patients who will die). However, when Keysar and his colleagues
presented the question to their subjects in a foreign language, they coped better
with the influences of presentation bias and managed to spot the similarity in the
two proposals.
To test the effect of language on another bias, risk aversion, each of the
subjects received $15 in single dollar bills, from which s/he took one dollar for
each separate round of betting (subjects were offered a total of 15 betting
rounds). In each round, subjects could decide whether to keep the dollar or bet it
on a coin toss that could earn them an additional $1.50 if they won, at the risk of
losing the dollar if they lost the bet. When the gamble was presented in English,
their mother tongue, they were more affected by their risk aversion (the fear of
loss) and only 54% decided to bet, even though the bet had a positive expected
value. When the process took place in Spanish, a language they had acquired,
subjects bet on 71% of the occasions. Making decisions in a foreign language,

the researchers concluded, reduces the emotional response in the process, and
therefore reduces the possibility of bias in making the decision.
And if this is indeed the case, is it not time for the managers of our investment
plans to phrase their proposals to us in a foreign language? Absolutely, but only
when they themselves decide to make their investments using a foreign language
of their own.

THE PRISONER’S DILEMMA
Judges as humans—food for thought
The cynical saying that a judge’s rulings depend on what he ate for breakfast has
actually received scientific substantiation. A study published in 2010 by Shai
Danziger and Liora Avnaim-Pesso (of Ben-Gurion University of the Negev) and
Jonathan Levav (of the Columbia Business School) points to a connection
between a judge’s meals—primarily their timing—and her decisions on whether
to grant prisoners early release for good behavior.
The study, which looked at 1,112 cases that had been discussed over a ten-
month period by eight judges on two parole boards, determined simply that there
is a high correlation between a prisoner’s chances of securing early release and
the point at which his case comes up for discussion in the course of the day.
Neither the severity of prisoners’ crimes nor their nationality or gender
determined the rulings—the main decisive factor was the order of cases on the
judges’ rosters. Inmates whose cases are brought before the parole board early in
the morning have a two-out-of-three chance of receiving the longed-for release.
These odds drop to nearly zero before a morning or noontime food break, but are
subsequently restored after the break and dwindle again the more time passes.
The researchers offer an explanation for this phenomenon: a sequence of
decisions requires mental resources that gradually become depleted with usage.
In such circumstances, decision-makers tend to go with the default option that
conserves these resources. In the case at hand: rejecting the request for parole or
scheduling a new hearing—a quicker decision than granting early release (five
minutes on average compared to seven minutes, respectively)—which involves a
lower inherent risk of error. Taking a break and eating renew the mental
resources, and with them the prevalence of decisions to grant parole. And no, we
are not dealing here with preliminary preparation of the cases by a court clerk
guaranteeing that the simple cases be discussed first (the order of cases is
determined by the order in which the prisoners’ lawyers show up), but rather
with a phenomenon that is gaining increasing attention from researchers and
deserves the term “decision fatigue.” The longer we are called upon to make
decisions, the more our tired brain looks for shortcuts. One option is to dispense
with assessment of the possible consequences of our decisions (which leads to
irresponsible behavior), but the more common option is to refrain from doing
anything—the ultimate default option.
John Tierney, a science writer for the New York Times, went a step further in

surveying the phenomenon. In an article published in his newspaper in 2011,
ahead of the release of the book he co-wrote with Roy Baumeister, Willpower:
Rediscovering the Greatest Human Strength, Tierney claims that decision fatigue
takes a heavy toll on each and every one of us. In its wake, we tend to get angry
with colleagues and relatives, buy fast and unhealthy food, and have trouble
rejecting the insurance agent’s offer to insure some exotic risk.
Indeed, whoever exerts the “willpower muscle” for long undermines his
ability to go on using it. Test subjects who restrained themselves from scarfing
down candy succumbed later to other temptations more than their friends. Those
who made an effort, at the researchers’ behest, not to think about white bears
during an experiment in which their thoughts were allowed to roam freely,
afterward had trouble restricting themselves while shopping to products from a
limited variety they were offered. Test subjects who managed to muster the
necessary self-discipline to forgo a morning snack wound up eating much more
than their colleagues, who had not abstained from eating the snack, at an ice-
cream tasting event later on.
However, most studies on the subject of willpower, like the one that opened
this chapter, concentrated until now more on the results of self-restraint and
exercising self-discipline and less on the cognitive taxing levied by a sequence
of decisions in which we have to choose between two temptations or two
different options for acting. It turns out that such a choice is even more tiring
than resisting temptation or delaying gratification, because once our mental
resources are depleted, our ability to calculate trade-offs amongst various
alternatives is greatly reduced. Calculating trade-offs is a process unique to man
(nature does not award much room to the relations between predator and prey),
and it occurs in areas in the front of the brain that developed relatively late from
an evolutionary standpoint. In keeping with the rule that in stressful situations
the most recent “evolutionary acquisition” is the first to go, with the erosion of
self-discipline the ability to calculate trade-offs is immediately compromised.
When we are in the midst of a shopping spree, for example, comparing prices
and qualities in the process of selecting various products erodes our mental
resources and leaves us vulnerable to the capabilities of salespeople who are
skillful at timing their offers.
Tierney cites in his column a study that Jonathan Levav conducted based on
buyers of new cars. Customers were asked to choose a combination that suited
them from among 56 possible car colors, four types of gearshift knobs, 13 kinds
of wheel rims, and 25 configurations of the engine and gearbox. They were soon
overcome by decision fatigue and took the default option they were offered, or,
more problematically, the recommendation of the courteous but biased salesman.

The latter presents the particularly tiring choices first (choosing a color),
hastening the stage at which the weary customer’s decision-making powers
expire. The average difference in price registered in the experiment between the
options a “fresh” customer chose and those that a tired one chose came to
$2,000. The fate of indigent people, Tierney argues, was unfortunate here as
well. Decisions that do not give most of us even a moment’s pause entail
anguished wavering for those lacking the wherewithal. Under such
circumstances, the poor suffer decision fatigue in no time, and so fall into mental
shortcuts like taking out a loan at a murderous interest rate as a result of erosion
in their power to resist.
Decision fatigue is also the main reason why candy is located near
supermarket checkout lanes. Once shoppers have used up their willpower on a
long series of shopping decisions, they are less able to withstand the temptation
of a hefty dose of carbohydrates that renews their strength, like the judges
following a meal. The part carbohydrates play in boosting willpower is also the
crux of the trap that weight-loss dieters are in: to keep from eating they require
willpower, but in order to build up willpower they have to eat, and primarily
foods high in carbohydrates. Carbohydrates play an important role in
strengthening willpower, and the main reason it declines during illness is the
decline in the glucose level the immune system needs. The next time that you are
tempted to come in to work sick, you would do well to keep in mind the research
finding that driving with a cold and mild flu symptoms is more hazardous than
driving while in a state of mild intoxication (the sort for which they suspend
your license). And if coping with such a simple task as driving becomes
problematic, all the more so complex assignments at work. The steep drop in
glucose levels, which also occurs during menstruation, explains, in the
researchers’ opinion, the decline in willpower and surrender to temptations
(food, smoking, shopping) at such times.
But as we have seen already, the people who cope with decision fatigue in the
best possible way are those who construct their daily schedule in a way that
conserves willpower resources. Not unlike pilots, who rely on prepared
checklists during take-off and landing, they adopt habits that are designed to
reduce the number of decisions they must make in the routine course of their
day. And I have also heard of businesspeople who refrain from making important
financial decisions after four in the afternoon, out of a realization that this late in
the day they are more susceptible to the arguments of the other party to a deal, be
it a skilled salesman or an experienced negotiator.

IF I’M NOT FOR MYSELF

On the powerful egocentric bias
Here’s an interesting parlor game for your next Friday night gathering with
friends. Ask the assembled company, those who share a household, to estimate
their relative contributions to the household’s upkeep and to jot down the
number as a percentage on a piece of paper. Collect the notes and present
everyone with the average result, which you could have told them from the start:
on average, each of those present estimates that he bears 60–70% of the burden.
Most of us tend to ascribe to ourselves more than we deserve. When we’re
asked to estimate our share in a partnership, in the success of the organization we
work for or the team we’re on, we tend to overestimate. This phenomenon is
known to behavioral scientists as “egocentric bias,” and it is so powerful that
even researchers are not immune to it whenever they claim that their
contribution to a study was greater than that of their collaborators. In a
particularly refreshing study, the authors of scientific papers, which are generally
written collaboratively by several researchers (four in the case of our study),
were asked to estimate their own contributions and the contributions of others to
studies they published together. If you tally the rates of contribution that
researchers attributed to themselves in writing the paper you will get 140%.
Frequently that kind of biased estimate also leads to an argument over the order
of the authors listed on the paper, which by the academic code reflects their
relative contribution to the study. Nor should you be fooled by the self-righteous
pronouncements of MVPs in team competitions. Anyone who reads the sports
section recognizes that moment when the star of the game is interviewed at the
end and says magnanimously, “It was a team effort, they all gave it their best.”
Well, he can say this with the requisite equanimity only after the sports writers
have crowned him the game’s hero.
Egocentric bias is also behind the difficulty inherent in conflict resolution. As
in the case of credit for a scientific paper, here too different people address
different facts when estimating their righteousness in a domestic, business or
national dispute. We focus on our own contribution and interpretation of the data
and fail to notice the contribution or interpretation of others, even those close to
us. The bias leads us to evaluate data in a manner that serves our own needs first
and foremost: we decide in advance the outcome we prefer, and then try to
justify our subjective preference as fairness by distorting the criteria that define
what is fair. In other words, the tendency in a dispute to examine the
circumstances “from a position colored by one’s own vested interests” affects
both sides’ perception concerning the essence of a fair compromise.

Disputes that reach court are another fine example. Even when the
representatives for the plaintiff and the defendant have identical information,
both sides will process it in their heads differently and in a manner that supports
their positions. Defendants have better recall of details that support their interest
and almost no memory of details that support the plaintiff’s position, and vice
versa. Therefore, it is not surprising that people who are facing a trial or
arbitration frequently overestimate their chance of success. One study found that
70% of lawyers who are asked before a trial about the legal strength of their case
think they have an advantage over the other side. The explanation is to be found
in, among other things, their tendency to adopt those details of the legal case that
support their position and to ignore those that do not support it (overconfidence
is another contributing factor).
In a book published in 2011 by Max Bazerman of Harvard Business School
and Ann Tenbrunsel of the University of Notre Dame, entitled Blind Spots: Why
We Fail to Do What’s Right and What to Do about It, the authors survey the
various reasons restricting our judgment and consequently the limits of our
ethical behavior. Egocentric bias is awarded a place of honor in their book. The
duo documents an experiment in which researchers provided test subjects,
students in a negotiations class, with all the information from a lawsuit that was
filed after a collision between a car and a motorcycle. The students were divided
into pairs, with one partner representing the plaintiffs in the case and the other
the defendants. Each pair was instructed to try to resolve the lawsuit by reaching
a settlement. It was explained to them that failing to do so would make their
client’s situation worse. In addition, they were told that in the event of a
stalemate, the amount of damages would be determined by a neutral person who
had already finalized his decision based on the same data that was placed at their
disposal. Before they began negotiating, participants were asked to share with
the researchers, in total confidence, their assessment regarding the judge’s
verdict. The researchers found that representatives for the plaintiffs estimated the
amount of damages the judge had awarded at more than double the estimation by
those who represented the defendants. But the more interesting finding was that
the gap in the estimates by pairs in the experiment was an excellent predictor of
their ability to settle the lawsuit. The smaller the gap between those representing
the plaintiffs and those representing the defendants, the greater was their chance
of resolving the lawsuit by reaching a settlement.
The ultimate fertilizer for egocentric bias is data ambiguity, because when
data is unequivocal, the rioting of our mind greedy for manipulations (which
naturally serve our interest) is limited. Likewise in international disputes over air
pollution, fishing rights and the subsidization of agricultural products, the

difficulty of reaching suitable international arrangements stems from this bias,
this time on a national level. In a large share of cases, the parties tend to adopt
different standards with regard to what will be considered a fair solution,
according to their needs. Frustratingly enough, they have no idea that they are in
this situation because of the bias; they are well and truly certain that their
interpretation matches the data precisely. The effect of the bias is so great that
even someone familiar with the phenomenon can easily see it in others but not in
himself; yet another example of egocentric interpretation—this time of the
egocentric bias itself.
The proven way to reduce the bad damage wrought by this bias is to try and
put yourself in the other’s shoes and consider what is going through his mind. To
which data does he prefer to attend? Do not the data before us also justify some
of the positions the other side is taking? Does he necessarily see our point of
view? We must understand that usually the opposing side is no less right. It
simply interprets the data differently. Just like us.

KNOWING WHAT WE DON’T KNOW
Why incompetent people don’t recognize their ineptitude
If you wonder why people with no sense of humor continue telling jokes that
aren’t funny, why day traders go on gambling (and losing) in the daily money
market, and why people without a shred of political sense are determined to
manage a hopeless election campaign, the answer may already be blowing in the
wind.
A study by David Dunning and Justin Kruger, published in 1999 when they
were at Cornell University, has re-emerged on the Web in recent years and may
explain why incompetent people don’t recognize their ineptitude. The study also
clarifies why we face a fundamental difficulty whenever we come to assess what
we don’t know. This bias, referred to as the Dunning-Kruger effect, conceals a
convoluted logic circuit that takes a moment to grasp: people who have limited
ability in certain areas rate their ability erroneously for the same reasons that
render it limited. In other words, the skills that serve the ability are the same
skills that serve the ability to evaluate it, in themselves and in others. The
incompetents among us thus shoulder a dual burden: not only are they mistaken
in their decisions and choices in a given area, but their incompetence in this area
also prevents them from noticing it.
Dunning and Kruger carried out a series of four studies to encompass the
various dimensions of the phenomenon. In the first two experiments, the
researchers administered a series of tests in three areas: logical reasoning, humor
(subjects were asked to rate jokes in comparison to a rating by professional
comedians) and grammar. When subjects were asked to assess their
performance, it turned out that the lower the subjects scored on the tests, the
greater their tendency to overestimate their performance. That is, the gap
between actual and perceived performance was inversely related to test scores.
Subjects who ranked, for example, in the 12th percentile on the logical-
reasoning tests estimated that their test scores were above average and placed
them in the 62nd percentile in relation to fellow participants. A similar error in
assessment characterized those who scored low on the tests in English grammar
and humor.
Those who were highly competent, on the other hand, tended to underestimate
their skills (though to a lesser degree). The researchers attribute this to the fact
that in the absence of information regarding the performance of others, those
who are highly competent estimate that there will be others with scores similar

to their own and therefore tend to ascribe to others a higher ability than they
actually possess.
In the third study in the series, the researchers attempted to trace subjects’
ability to improve their self-assessment after seeing the answers that others gave.
The researchers gave a few of the participants in the study the answers of five
other participants and asked them to assess the level of those respondents. They
were then asked to go back and re-evaluate their own performance. The
incompetent ones had difficulty assessing the level of others and failed to
improve their ability to evaluate their own performance (a few even revised their
self-assessment upward). The highly competent ones, on the other hand, were
quick to revise their assessment of their own ability once they saw the others’
work. The results of the third study would not have surprised the philosopher
Bertrand Russell, who long ago argued that a fool will never be able to
accurately decipher the words of a clever man, because the fool unconsciously
translates what he hears into something he is capable of understanding.

BIRDBRAINED
When some birds are smarter than people
If you think that “birdbrained” is a term for someone whose understanding is
limited to locating seeds in the dirt, think again. An eye-opening study of
flexible thinking, which pitted the winged creatures in competition against the
crown of creation, ended in a crushing defeat for man.
The contest took place in the framework of resolving the “Monty Hall
Dilemma,” a mathematic puzzle that never ceases, like a jug of research oil that
has yet to run out in more than 20 years, to provide insights into our limitations.
And more than it being about the difficulty of solving the puzzle, this is actually
about the difficulty of accepting the solution, even when it comes from an
authoritative and reliable source.
The Monty Hall Dilemma, which I have mentioned in previous books, was
conceived on the television show Let’s Make a Deal, hosted by Monty Hall,
which aired in the early 1990s. Here is the dilemma again briefly: contestants on
the show were asked to choose one of three doors presented to them. Behind one
door was a new car, and behind the other two were gloomy-looking goats.
Imagine you are a contestant on the show and after ruminating deeply you point
at the door that you have chosen in the hope of winning the car. The host, who
knows where the car is hiding, opens one of the other two doors and reveals the
goat behind it. And now, he offers you the option of changing your first choice
and picking the other door he did not open. Naturally, you ask yourself whether
the host’s move alters in the least the probability that the car is behind the door
you picked (which was one out of three before the move). And if it does not,
why change the selection and suffer the terrible pangs of regret belonging to one
who has switched lanes at airport security only to find that the lane he left is
moving faster?
Most of the contestants do not tend to switch their selection, and the reason
for their decision is that only two doors remain in any case and the odds of the
car being behind either of them are equal, so why switch? And indeed, the
probability that the longed-for car is behind the door you chose has not changed;
but you would do well to take advantage of the host’s offer and opt for the other
door, because the probability that the car is behind it, the door the host did not
open, has gone up to two thirds.

The key to the solution embedded within the puzzle is the fact that the host,
who knows which door conceals the car, will never open it. Therefore, if to
begin with you chose a door that conceals a goat (two out of three cases), you
should take advantage of the host’s offer and switch your choice. If you have
chosen the door behind which the car is hiding (a third of the cases), the
substitution strategy will cost you the loss of the car. In short, if you’re faced
with the dilemma numerous times and decide to switch your choice every time,
then in two thirds of the cases the change will pay off and in one third it will not.
Hence, the decision to accept the host’s offer and switch your choice is the
correct decision.
Anyone who sees the readers’ letters to the editor of the newspaper that
published this solution will understand the difficulty involved in changing one’s
opinion and the frustration that accompanies that. Indeed, wherever the dilemma
is presented it exposes the trouble respondents have discerning that the
probability embodied in the other door justifies the change of choice. Some of
them erroneously estimate the probabilities as equal, and so believe it is not
worth changing one’s choice. The worst of all is that they cling to their version
even when the solution is explained to them, and refuse to accept the information
that explains how to solve the puzzle.
Walter Herbranson and Julia Schroeder, researchers from Whitman College in
the state of Washington, wanted to find out if the difficulty of switching the door
choice is unique to man or if other species are also susceptible to it. Six Silver
King Pigeons (Columba livia) reported for the test. They were presented with an
avian version of the game that matched their dimensions and in particular the
size of their beaks. Each of the pigeons found before it three illuminated circles
it could peck at to release a mixture of grain. After the first pecking attempt all
the circles went dark, and a second later only two of them lit up again, including
the one the pigeon had chosen to peck at in its first try. A computer playing the
role of the human host on the original television show chose one of the two lit-
up circles to conceal the grain behind. The experimenters wanted to know
whether repeating the initial experiment, in which pigeons that switched their
pecking choice the second time were rewarded, would lead them to waive their
initial choice consistently. The computer ensured that in two thirds of the cases
switching their initial choice granted the pigeons the food they craved—just like
in the famous puzzle.
On the first day of the trial the pigeons switched their selection in only slightly
more than a third of the cases (36%). But after 30 days in which the researchers
repeated the experiment daily, all six pigeons switched their choice almost every
time (96%) to win almost all the food possible. The pigeons simply learned that

the odds of obtaining food improve upon a change of choice, and so they
changed it nearly every time.
Next, Herbranson and Schroeder asked 13 students to play the role of the
pigeons in an identical game about which the researchers were sparing in their
explanations, and asked the subjects to score points instead of grain, and as
many as possible. Each student was given 200 attempts to assess which circle to
press to earn points. In the initial stages of the experiment the rate of those who
opted to stick with their original choice was identical to the rate of those who
decided to change it. At the end of a month in which the students repeated the
selection process and experienced the outcome of their decisions, only two thirds
changed their decision, less than the results of the birds’ wisdom.
How is it that humans fail where pigeons succeed? Well, there are two ways to
reach the winning strategy in the Monty Hall Problem. One is built on the
analytical approach of calculating the probability of success in each of the
decisions (whether to stick with the original choice or change it). The other is to
form a strategy empirically, in the wake of repeated experience. The researchers
believe that humans prefer the first way but have trouble calculating
probabilities, especially if they are conditional (“What are the odds of this
happening, if this has already happened?”), and so they get tangled up in the
calculation. Pigeons, on the other hand, base their decisions on experience. If
they discover in the course of the experiment that it is correct for them to switch
their choice in two thirds of the cases, then they will always change it. In
contrast to them, humans are susceptible to a deviation known as “probability
matching,” and if they discover that switching their selection is worthwhile in
two thirds of the cases they will tend to do so only in two thirds of the cases (and
not as it would be right to do, every time).
An interesting corollary to the study was the connection between participants’
age and their aptitude for dealing with the problem. It turned out that the
younger they are, the better participants do, and eighth-graders score better than
college students. Could it be that the cost of acquiring an education is adopting
biases that get in the way of our solving a certain kind of logic problem?
Certainly, Herbranson asserts in his article, even the famous mathematician Paul
Erdős “refused to accept colleagues’ explanations for the appropriate solution to
the MHD that were based on classical probability. He was eventually convinced
only after seeing a simple Monte Carlo computer simulation that demonstrated
beyond any doubt that switching was the superior strategy. Until he was able to
approach the problem like a pigeon—via empirical probability—he was unable
to embrace the optimal solution.”

I SAW A MONKEY PLAYING MOZART
On the roots of urban legends
“A lie can travel halfway around the world
while the truth is putting on its shoes” 
Mark Twain
In an experiment conducted years ago at the University of Chicago, five
monkeys were placed in a cage. A banana was suspended from the center of the
cage and beneath it the researchers positioned a ladder. Not long went by before
one of the monkeys began climbing in the direction of the banana. The moment
its foot touched the first step, the other monkeys in the cage were sprayed with
ice water. After a while another monkey tried his luck and again the researchers
sprayed cold water on its cage mates, and the same happened several more
times. The cold-water hose was eventually removed from the cage, but whenever
one of the monkeys headed for the ladder, the other monkeys prevented it from
doing so, sometimes employing unrestrained violence. At this point, the
researchers took one of the monkeys out of the cage and replaced it with another
monkey. The new monkey immediately spotted the banana and tried to grab it,
but as soon as it set foot on the first step it was attacked by the rest of the
monkeys, which wanted to prevent its carrying on. After another attempt this
monkey too learned that if it valued its physical integrity, it had best give up the
banana. Next, another monkey was replaced by a fresh monkey and the process
repeated itself—the monkey that had recently joined the group took part in the
assault on the new monkey as it tried its luck at securing the banana, and even
displayed a marked enthusiasm that is ordinarily reserved for those who have
just converted their faith or for young warriors bent on impressing the veterans
in the combat unit they joined lately. And then a third monkey was substituted.
Its replacement made its way to the ladder and was immediately punished
savagely by all the rest. Two of those that were beating it, the newcomers, had
no idea whatsoever why they had been prevented from climbing the ladder and
even less so why they were taking part in assaulting the new monkey.
After the fourth monkey and fifth monkey from the original group of monkeys
were replaced, there remained no monkeys in the cage that had physically
experienced the cold-water spray. Nevertheless, none of the monkeys left in the
cage tried to climb the steps en route to the banana.
I heard this catchy story a while back from a ski instructor who was
attempting to persuade me that, like the monkeys, I had adopted several harmful

skiing habits whose source I had never bothered to examine. At this point I was
supposed to have an epiphany and adopt a new skiing technique better suited to
the modern equipment that had grown in sophistication since I was introduced,
many years ago, to this invigorating sport. I preferred to go surfing instead, to
track down online the research roots of that story. A search of the Web quickly
brought the story and its educational lessons to the surface. Certain versions
preferred a ladder to a staircase and others made do with four monkeys. In one
version, in which human words were placed in the monkeys’ mouths, the
battered monkey asks the other monkeys: “But why?” and they reply in unison:
“Because we have always done it that way here.”
The main problem with the study described above is that it never took place.
The behavior of rhesus monkeys was in fact studied in 1967 by a Canadian
researcher, who hypothesized that a naïve monkey placed in a cage with other
monkeys that had adapted to certain conditioning might adopt that conditioning,
but that, more or less, was it. Welcome to the department of studies that turned
into urban legends. I have told my readers this story more than once, but that is
precisely how urban legends take on a life of their own—when they are repeated
and recounted time and again. However, this begs the question: what is it about
an urban legend that makes it such catchy information, when so many more
important pieces of information cannot even cross the threshold of our
consciousness?
Urban legends frequently carry a moral lesson, such as in the story of the
caged monkeys, and they are first and foremost stories—they have a setting,
protagonists, a plot that comes to a climax, and a punch line that unravels the
narrative thicket. They help people to amuse themselves and instill in others
social values and norms, and a few of them reflect fears and concerns shared by
many. Urban legends take hold in our hearts because they provide illuminating
social insights in cultural or economic contexts that preoccupy us all. It is hard to
trace their sources, and the roots of some are planted in the past, but urban
legends like the monkey story, which purportedly originated in a scientific study,
have an uncharacteristic advantage over other legends: we can chart their
evolution because their source is defined and known.
One such famous legend claims that listening to classical music (and
particularly works by Mozart) enhances babies’ intelligence. This belief
originated in a study published by the journal Nature in 1993, which found that
college students who listened to a Mozart sonata for ten minutes improved their
performance on a spatial intelligence test by eight to ten IQ points. The findings,
which were dubbed the “Mozart effect,” sparked a wave of subsequent studies
that tried to replicate the results of the original study but yielded mixed results at

best. A comprehensive comparative analysis that considered 16 different studies
on the subject concluded that the overall effect is negligible. But even though the
Mozart effect failed to meet scientific standards, it enjoyed widespread
popularity with the public at large. The study was cited in countless public
debates that dealt with education in general and with developing the skills of
infants in particular (the participants of the original research, don’t forget, were
college students). At the height of the craze, the state of Georgia passed a bill
that promised to hand out a classical music CD to the mothers of newborns. The
state of Florida, for its part, passed a bill requiring state-funded daycare facilities
to play classical music daily. Stores displayed appropriate books and CDs, and
public awareness of the phenomenon reached a rate of 80%. The phenomenon
spread overseas as well and became one of the most successful urban legends in
the world.
Unfortunately, it is impossible to trace most urban legends with the same ease
that legends based, ostensibly, on scientific research afford. The flagship website
for aficionados of urban legends is snopes.com and it crowns itself the Web’s
most comprehensive source of reference for such legends, folktales, myths,
rumors and misleading information. You can surf the site by categories (there are
no fewer than 43) or jump straight to the list of the 25 hottest stories. In the
crime category, for example, you can find the story about a cigar smoker who
took out fire insurance on several hundred cigars. After he had enjoyed smoking
all of them, he filed a claim with the insurance company on the grounds that the
cigars had gone up in smoke. The insurance company refused to pay and the
man sued it. The judge ordered the company to compensate the insuree, but once
he received the money the company promptly sought his arrest for arson.
According to snopes.com, the story came into being in the mid-1960s and was
found to be groundless.
The website’s diligent editors regale surfers with hundreds of legends of this
sort, analyze their origins, and pronounce them true, bogus, or in many cases
stories that contain a grain of truth. One such story is about a driver from
California who got a speeding ticket in the mail along with the picture that was
taken by the camera he sped past, and an order to pay a $40 fine (this was in the
1960s). The furious driver returned the letter together with a photograph of two
$20 bills, the requested fine. A week later, he received a reply from the police
and when he opened the letter he found a photograph of a pair of handcuffs.The
website TopTenz.net ranks, among other things, the ten myths and urban legends
that earned the most fans. Starting with the claim that if you place a tooth in a

glass of Coke at night it will be consumed by the morning, through the horror
stories about alligators released into the city sewer system, and ending with the
story that took the distinguished first place, the legend about the kidney robbery,
which was even enshrined in the movies. The story probably got its start in 1997,
when a letter began to circulate on the Web warning of a new crime. Most
versions tell of a traveling businessman who finds himself at a bar and accepts
an offer from a stranger, who strikes up a conversation with him, to join him for
a drink. Soon enough the traveling salesman’s consciousness is impaired and he
wakes up in an unfamiliar hotel room, usually inside an ice-filled bathtub. A note
left nearby suggests that he telephone the emergency services, and when they
arrive it turns out he is the victim of a network of swindlers that drugged and
stole a kidney from his body to sell on the black market. The story is made up,
and an American health organization that asked that anyone who had been hurt
by the theft of a kidney contact it did not receive a single call.
In 1981, Jan Harold Brunvand’s book The Vanishing Hitchhiker: American
Urban Legends and Their Meanings was published, and spurred broad public
awareness of the urban legends phenomenon. Brunvand pointed out in his book
that urban legends and folktales are not unique to primitive societies, and that
analyzing them can teach us a great deal about the culture of their creators. One
of the traits of urban legends is an absence of specific details of place, time, the
names of those involved and similar identifying information. Many preference
themes of horror, crime, poisoned food or other situations liable to affect many
people. According to Brunvand, anyone who feels threatened by the story will
rush to warn those he cares about, and thus the story takes on wings that ensure
its dissemination.
Like myths, urban legends fall on ready ears because they reinforce
worldviews previously held by the public and help to explain events that seem
complex to understand. The researchers realize also that urban legends help us
cope with our repressed fears. A story about a dog that was smuggled from
another country and turned out to be an overdeveloped rat reflects fear of illegal
immigrants; a story about a teenage boy who accidentally ingested a snake egg
reflects fear of substances that cause stomach infections; and a story about the
bride who called off her wedding after she found out that her intended groom
had slept with her sister a few days before the ceremony reflects fear of
infidelity. Through stories like these we are supposed to acquire a certain
measure of control over our fears and to warn others against them.
Sundry social psychology theories try to explain the way ideas are distributed

and the reason they manage to gain a hold on people’s hearts. Most of these
theories suppose that distributing ideas fulfills a social function in the sense that
they meet a genuine need of individuals or a society. Putting the findings of a
scientific study in layman’s terms, for example, helps the general population
cope with the threat inherent in such studies for those not versed in reading their
results. Rumors, for their part, spread through the population in reaction to
uncertainty and anxiety, and one study even located a correlation in test subjects
between a tendency toward anxiety and a tendency to spread rumors. Conspiracy
stories—kin to rumors—are based on the notion that behind a major social,
political or economic event lurks a secret plan unknown to the public. This plan
is enacted by devious elements that are powerful and influential to attain a
diabolical objective. These stories as well explain in their own way a complex
world and greatly simplify it by dividing it into forces of dark and light.
Particularly intriguing is the theory that ascribes evolutionary roots to this
phenomenon. In 1976, the famous evolutionary biologist and author Richard
Dawkins published his book The Selfish Gene. In it he introduced for the first
time the possibility that cultural baggage in general, and ideas in particular, are
spread, like genes or viruses, by means of information units he termed “memes.”
The invention of the wheel, wedding rings or a piece of juicy gossip—all are
units of cultural information that are spread in this way. For a meme to spread it
needs several features that lend it “stickiness.” A stale joke, for instance, has no
stickiness and it dies a miserable death on the lips of the first teller. Sticky ideas
are simply put, they are surprising, concrete, credible, storylike and emotional.
That distinction is offered by Chip and Dan Heath in a book they published in
2007 entitled Made to Stick: Why Some Ideas Survive and Others Die.
A study that Chip Heath and colleagues conducted and published in 2001
focused on the possibility that the diffusion of memes involves a process of
selection that is influenced primarily by their ability to evoke a strong emotional
response, preferably one that is shared by many. The stronger the emotional
response a meme elicits, the greater its chance of being remembered, of being
passed on, and of winning the competition against the other memes. Urban
legends are memes, and Heath and his colleagues tried to test their theory with
their help. They used the story about a man who drank a soft drink from a bottle
at the bottom of which he spotted a dead mouse, and created three versions of
the story that differed from each other only in the level of repulsion they
prompted. In the “light” repulsion version the man noticed the little corpse
before he brought the bottle to his lips, whereas in the “heavy” version, well, you
get the picture. When the researchers tried to establish which ideas manage to
get through more, they found that subjects had a greater tendency to pass on the

version of the story that arouses the most disgust. A complementary study they
conducted compared assorted websites devoted to urban legends and tallied the
relative frequency of the various motifs that appear in the legends posted there.
Here too it turned out that the memes that survive are not necessarily those that
adhere to the truth, but rather those that inspire a powerful emotional response.
Repulsion is a leading motif in these stories, as reflected in a popular urban
legend from Japan.
Aka Manto (Japanese for “red cape”) is a spirit that prefers to manifest in
restrooms. Generally in the last toilet stall of the women’s bathroom. When the
hapless victim sits down on the toilet, she hears a mysterious voice ask her
whether she prefers red or blue paper. If she says red paper, she will be murdered
in bloody violence (red). If she asks for the blue, she will be strangled to death
(and her face will turn blue). Any other color she asks for will immediately call
forth hands to drag her off to the flames of hell. The only way to save herself is
not to ask for anything. I wondered whether this might not be, in the manner of
urban legends, an allegory warning us against the consumer culture that
consumes us in countless ways, and the only way to survive is not to ask
anything for ourselves.

LAKE VICTORIA AND UNCLE ALBERT
Is it possible to swindle honest people?
“My friend, are you by chance related to Albert Burak, the pilot whose plane
crashed with its passengers last year in the Atlantic Ocean?” Dr. Tuti Wisku asks
me in an email message. Wisku introduces himself as the executor of the
unlucky pilot’s estate and he’s searching high and low for heirs. His efforts have
been in vain so far, and if I’m indeed a relative—as the dedicated attorney from
Abuja hopes, for my sake—I can expect to receive a windfall of riches as the
sole heir to the deceased pilot’s savings and the fruits of his successful business
ventures. Attached to the email message is a photocopy of a newspaper report
about the plane crash, though Albert, my long-lost relative, is not mentioned in
it. I dismiss this promise of riches out of hand. If I ever had an Uncle Albert, he
was killed in the Holocaust. “Nice try,” I say to myself. “Try again with a better
idea.”
My thoughts fall upon fertile ground and, several days later, another email
arrives. This time Ben Gifi, the chief cashier of the Central Bank of Nigeria,
offers me $45.6 million. I’m legally entitled to this money, he notes, because of
the work I did on a government project. Unfortunately, the payment was never
processed due to an embarrassing computer glitch at the bank. But all I need to
do is to send my personal details and the international bank clearing system will
do the rest. As you guessed, both email messages were part of an extensive
global industry known as the “Nigerian Sting” or the “419 Scam” (the latter a
reference to Section 419 in Nigerian law, which addresses the phenomenon).
The basic structure of the various sting letters is similar. The recipient, who
does not know the sender, is informed of an opportunity to receive a real fortune
—thanks to a stroke of good luck, the recipient’s renowned trustworthiness, or
an innocent mistake. In another version, the recipient is asked to help the heirs of
a deceased ruler withdraw funds held at a local bank, and is promised a
substantial share of the inheritance in return. There are many other variations of
these letters, limited only by the creativity of their writers.
The hottest scam in recent years is an updated version of the Nigerian Sting
known as the “Spanish Prisoner.” The recipient receives notice that a relative or
acquaintance is in trouble in a foreign land and desperately needs cash because
his or her wallet was stolen or lost. This email message, unlike the others, is sent

from the acquaintance’s email account, previously hacked by the scammer, thus
adding to the authenticity of the appeal for help. (In the original version, the
writer asks for bail money after being arrested in a case of mistaken identity.) In
one of the more creative scams, the recipient receives a check from a “customer”
for an amount that far exceeds the value of the product or service purchased
from the recipient. In light of this “mistake,” the recipient is asked to refund the
excess amount. Due to the relatively long process of international check
clearing, the duped person is liable to pay the con artist before discovering that
the “excessive” check is phony. A very prevalent Internet scam is to congratulate
the recipient on winning the lottery, though the lottery winner cannot even recall
having purchased a lottery ticket.
Those who respond to such sting letters discover that before receiving the big
award, they are asked to take various steps that involve transferring sums of
money to their kind-hearted partners overseas. After each stage, they are
required to send just one more payment, not too much, to iron out the final legal
wrinkles. Eventually, even the most gullible patsies will realize that they’ve
fallen victim to a scam, paying the imaginary expenses of the swindlers.
Most people assume that the Nigerian Sting can only entrap a small number of
uneducated and unsophisticated people. In fact, however, the fraudulent emails
ensnare a broad range of people, many of them intelligent, highly educated and
informed. Business executives, top-level civil servants and many other highly
capable people are among the victims. The important question was and remains:
why do they respond to the offer in the first place?
The human race has been inclined to cheat since the dawn of evolution.
Already in the savannah of Africa, human beings developed the ability to
deceive, and moreover—the ability to detect deceit. In prehistoric times, when
primitive weapons produced a relatively meagre haul of food, the ability to
finagle a disproportionate share was very important. And the ability to detect
those who tried to cheat the system was sometimes of vital importance. As
noted, the brain does not feel comfortable when we tell a lie and it reveals this
discomfort in certain gestures and body language, but primarily in verbal
expression: liars tend to describe many details, and avoid the use of the first
person and words that express emotion.
Do we become less effective at detecting deceit when we cannot see our
interlocutors or hear their voice? Or do those crooks succeed in disrupting the
delicate mechanism of fraud detection by appealing to basic needs that are no
less vital? After all, most of the signs of deception are there: the email address is
not a personal one; the promised reward is unreasonably large and, in fact, we
don’t even know anyone who has ever won such a bonanza. So how do the

senders of the enticing messages succeed in overcoming our evolutionary
suspiciousness? And what distinguishes between people who respond to the
offer and those who don’t even bother to read it?
It seems that the flood of official correspondence in the modern world tends to
dull our primordial instinct for detecting deceit. It turns out that people are more
inclined to believe messages they receive in writing than those communicated to
them orally. This can be attributed to the large number of written messages sent
to us from official authorities and service providers, which reinforces the
perception that those sending the letters are institutions that stand behind their
words. In addition, it turns out that all of us are sensitive to the presence of
authority. Indeed, the fraudsters behind the Nigerian Sting often adopt the
authoritative status of attorneys, bankers and senior government officials. When
the message comes in regular mail, the letter is usually printed on official
stationery, in an envelope adorned with every possible symbol of authority and
prominently labelled “Official Mail.”
There is always a call to respond, at first without any request for money. The
recipient’s response to the initial communication is what creates the commitment
to act consistently and respond to the next letter, which already includes a
request for money. The American psychologist Robert Cialdini, author of the
bestseller Influence: The Psychology of Persuasion, provides a behavior
explanation. People tend to value consistency in their own behavior and in the
behavior of others, he notes. Consistency gives a sense of control and fosters a
person’s confidence in understanding the world and even foreseeing future
events. This tendency is familiar to every investor: the readiness to continue to
sink additional money into a failing investment solely for the sake of
consistency.
Research on the victims of these scams indicates that many of them respond to
the letters because they overestimate their understanding of the field discussed in
the letter and overrate their ability to identify deceit and contend with their
counterparts. Surprisingly, those who work in investments, for example, are
more prone to fall victim to fraud in this field. Some of the fraudulent messages
are intentionally written in broken English in order to instill a false sense of
superiority in the recipients and reinforce their overconfidence.
The sting letters enjoy the tailwind provided by mythological archetypes of
“rags to riches” stories of sudden and unexpected wealth. Such tales exist in
nearly every culture. However, what drives people to ignore the warning signs in
sting letters is primarily greed and the hope for easy money, and this is further
enflamed by the enormous size of the reward relative to the modest sacrifice it
requires. In 2009, the UK’s Office of Fair Trading, disturbed by the large number

of scams in the kingdom, commissioned a comprehensive study on this subject,
conducted by researchers at the School of Psychology at the University of
Exeter. The innovation in the researchers’ approach was the assumption that
responding to fraudulent initiatives is not an anomaly, but rather an error in
judgment—the type of mistake that characterizes many of the economic
decisions we make in our daily routines.
These mistakes are the inevitable outcome of shortcuts that our brains try to
employ to cope with the impossible task of processing all of the relevant
information required for rational decision-making. These essential shortcuts
allow emotional motivation to infiltrate and affect the decision-making process,
including urges such as greed, thrill-seeking and sometimes a lack of self-
control. The letter writers appeal precisely to these factors in order to dissuade
the recipients from looking for information that supports or opposes the decision
they’re about to make.
The researchers believe that a combination of cognitive and emotional
processes underlies the victim’s decision to respond to the letter. The cognitive
side responds to the authoritative and official tone of the offer; this side may also
look at the offer as a long-term wager entailing reasonable risks. The wording of
the offer, on the other hand, targets the emotional system, whose purview
includes greed, quick gratification and the need for thrills.
The researchers sent 10,000 simulation letters to a sample of recipients and
examined eight types of different wordings to determine their relative
effectiveness. The most important finding in this part of the study was that
regardless of the wording and other factors, the best predictor of the likelihood
of response was whether the recipient had responded to a similar letter in the
past. The 10–20% of the population prone to respond to these offers are not
necessarily poor decision-makers, but they’re certainly more susceptible to
influence and persuasion. Some of them, the researchers claimed, have problems
regulating their emotions and exercising self-discipline. This difficulty is largely
responsible for our inclination to prefer immediate rewards, one of the most
common biases contributing to errors in judgment. Consequently, socially
isolated people are particularly vulnerable to fraud because social relations
encourage self-control. The researchers were very surprised to find that many of
the victims of scams are reticent to tell others about the offer they received, as if
worried that others would point out the error that a hidden part of their brain
already recognized.
The Internet is the ultimate platform for the 419 Scam to thrive. The Internet
is also the foundation for understanding the business model of what has been
defined as “Nigeria’s largest export industry.” This model is not necessarily

based on high profits (several thousand dollars that the victim sends as an
advance to cover “expenses”), but rather on the fact that the Internet enables
cost-free communication. (The response rate to the letters, incidentally, is
estimated at one to two per thousand.) Internet technology plays an additional
role in the success of scams: studies show that the scammer’s distance from the
potential victim ensures that the former will not be struck by the sudden burst of
empathy that impedes many acts of deceit that are committed face-to-face.
Since many of the fraudulent acts are also based on the willingness of the
victims themselves to deceive the authorities or adopt a fake identity of an
imaginary heir, the question arises: do the scammer and the scammed share
similar character traits? David Maurer, author of The Big Con: The Story of the
Confidence Man, first published in 1940, was sure of this when he noted: “You
can’t cheat an honest man.”

I ACCUSE, FALSELY
The path to justice is paved with deception
In January 1988, a young woman was attacked the moment she stepped out of
her car. The assailant robbed her at gunpoint, forced her to take off her clothes
and raped her. The victim identified a picture of Troy Webb in two series of
photographs, leading to his conviction of rape, kidnapping and robbery in 1989.
The first time she identified him, the victim pointed to the picture of Webb, but
said he looked too old to her. In the second series of photographs, the police used
a picture of Webb taken four years earlier, in order to confirm her identification
of him. Tests conducted on traces of sperm were inconclusive.
Throughout the trial and afterwards, Webb continued to plead his innocence.
When he finally gained access to the biological evidence in 1996, a DNA test
proved that he was not the attacker. He was released from prison that year and
was later pardoned by the governor of Virginia based on his innocence.
The Innocents is a chilling book of photographs and interviews, published in
2003 by the American photographer Taryn Simon. The book documents the
portraits and stories of 50 people, including Troy Webb, who were convicted of
serious crimes and spent long years in prison before proving their innocence and
winning release. Simon’s work is a sad commentary on an uncaring system of
law, judicial negligence and in some cases real corruption. The ex-convicts
profiled in the book served a total of 558 years in prison for no reason, over ten
years on average. Some of them were on death row and most had no prior
criminal record.
The fate of most of the wrongfully convicted people profiled in the book was
determined by other photographs, less professional and too often even blurred—
those that led to their conviction in hasty police lineups, based on identification
by witnesses prodded by police detectives anxious to wrap up the case. Simon
tries to restore the camera’s integrity, bringing those who were mistakenly
convicted to the scene of the crime—the place they never were, where their fate
was decided and their lives changed forever. Alternatively, she brings them to
the place where they actually were at the time the crime was committed, or the
site where they were arrested. While the interviews reflect the distress of those
who wasted a significant part of their lives behind bars, they also shed light on
the role of the photographic medium in the legal process—the limitations of this
medium and the distorted way in which it is sometimes used.

The suspicious profile
New DNA data collected at the crime scene, sometimes years after the verdict,
has led to the acquittal of over 340 men and women in the United States in
recent years. These people had been convicted of serious crimes they never
committed. In about 75% of the cases, the wrongful conviction resulted from a
key witness mistakenly identifying the suspect in a police photograph or lineup.
But doesn’t Simon, as a photographer, assign excessive weight to the inferior
quality of some of the photographs of suspects, while completely ignoring a
broad range of human biases that might also lead to wrongful conviction?
Eric Wargo, editor of the Association of Psychological Sciences’ Observer,
suggests an answer to this question in the November 2011 issue of the magazine.
In an article entitled “From the Lab to the Courtroom,” Wargo discusses the
effects of such biases on the judicial system and the efforts of behavioral
scientists to restore honor to an error-prone system.
According to Wargo, the first use of a psychological profile to help narrow the
search for a criminal came in response to the grisly acts of Jack the Ripper, who
sowed terror in the streets of London in the late 19th century. After conducting
an autopsy on one of the victims, Thomas Bond, a police pathologist, concluded
that the murderer was “a quiet, middle-aged, well-dressed, cape-wearing,
hypersexual loner who didn’t know anatomy and thus wasn’t a doctor or a
butcher.”
However, in light of the prevailing fashions of Victorian London, this
description did not offer much help in catching the murderer, whose identity
remains a mystery today. Police investigators had to wait another century before
practical capabilities developed for criminal profiling. In the late 1980s, David
Canter, a psychologist from the University of Huddersfield in England, provided
essential assistance to investigators trying to solve a series of murder cases. The
murderer, referred to as the “Railway Rapist,” raped and then strangled young
women who were waiting alone for late-night trains.
Citing Canter, Wargo explains: “Most psychological scientists interested in
personality and individual differences look at people’s characteristics to predict
how they will behave in various life situations. The criminal investigator has the
opposite, and considerably tougher, task: to use evidence of a person’s behavior
to construct a picture of his or her individual characteristics.” Using Canter’s
method of “psychological offender profiling,” the police were able to narrow
their search to “a skilled or semi-skilled laborer in his mid to late twenties who
worked weekends and had few friends; who was interested in martial arts,

swords and knives; who was physically small and had feelings of
unattractiveness; who lived within the vicinity of his first crime.” This
description precisely matched the characteristics of one of the suspects, who was
indeed convicted and received seven life sentences after the police discovered
incriminating evidence at his home.
Eyewitnesses
Profiles can guide police investigators in their work, but a conviction is almost
always based on eyewitness evidence. Indeed, the central impact of witnesses on
verdicts engendered no fewer than 2,000 studies from 1974–99 designed to
assess the credibility of witnesses. In one study, investigators staged an attack
against a faculty member on a university campus, and there were 141 witnesses
to the incident. However, their descriptions of the event varied widely in regard
to the attacker’s appearance, weight, clothing and other relevant dimensions of
the description of the “assault.”
The witnesses’ descriptions were rated for accuracy and, on a scale of 1 to
100, the average rating was only 25. The most common mistake was to
overestimate the duration of the attack. In a similar staged experiment, a mugger
snatched a wallet. Only seven of 52 witnesses correctly identified the assailant in
a video lineup. Ten witnesses did not identify anyone in the lineup and 35
identified an innocent person—a rate of error that no judicial system can
tolerate. Cognitive abilities of memory and visual perception are key to being an
eyewitness, yet many studies have proven their limitations in conditions of stress
and, in fact, in almost all conditions.
The psychologist Elizabeth Loftus from the University of California often
appears as an expert witness in criminal proceedings. In the courtroom, she
explains to jurors the findings of her scientific work, which includes the effect of
leading questions on memory distortion. In one of her early studies, she showed
a group of subjects a video clip of a car accident. Some of the subjects were then
asked: “About how fast were the cars going when they smashed into the wall?”
They estimated an average of 40.5 miles per hour. Other subjects were asked the
same question, phrased in a “gentler” way: “About how fast were the cars going
when they hit the wall?” Their average estimate was only 34 miles per hour. In
another study, Loftus discovered that the presence of a weapon at the crime
scene diminished the accuracy of the descriptions given by eyewitnesses, whose
minds were distracted by the threatening weapon.
Research conducted in 2014 shows how inclined we all are to adopt

incriminating memories. Julia Shaw and Stephen Porter employed a technique of
“planting memories” during a series of interviews with students, including
descriptions of both real and fictional childhood incidents in which they
committed a crime of theft or assault. In the end, most of the students (70%)
gave detailed accounts of criminal incidents that had never actually occurred,
including a description of their encounter with the police.
A police lineup, as we’re accustomed to seeing in films, is still a central tool
in the investigation process. However, many studies have demonstrated the
vulnerability of this method to errors in identification, particularly due to the
logical flaw in the assumption that the criminal is one of the people in the lineup.
This assumption is not necessarily true, since the principal suspect might be
innocent. When witnesses see a lineup and are not immediately sure they
recognize one of the suspects, yet feel obliged to point to one of them, they
begin a process of matching the suspects to their elusive memories.
At this stage, Wargo contends, even unconscious signals from the detective
working on the case can lead witnesses to point to a particular suspect, certain
that their memories are guiding their selection. It turns out that witnesses
identify suspects more accurately if each person in the lineup is presented
individually—that is, one at a time. And this is also true when selecting a suspect
from a series of photos. Without the ability to compare the suspects side by side,
witnesses feel less compelled to choose one of them. The accuracy of the
witnesses’ selections also improves when the person directing the lineup is not
familiar with the investigation file.
Confessions
While erroneous identification is the primary cause of incarcerating innocents,
false confession is the second most common reason. Saul Kassin and his
research colleagues found that 15–20% of the false convictions later overturned
by DNA evidence were based on false confessions. Suspects are liable to do this
for various and strange reasons—for example, in order to protect the real
perpetrator or even to achieve notoriety. However, Wargo explains that the main
reason lies in the conventional method of interrogation, which presumes that the
suspect is guilty.
In this accusatory method, suspects are isolated in order to heighten their
anxiety, and then presented with evidence, sometimes fabricated, connecting
them to the crime. Denials of guilt are received with complete disbelief and the
suspects are warned of dire consequences if they continue to insist on their

innocence. At this stage, the interrogator sometimes “softens” and, in an attempt
to win the suspect’s confidence, admits that the victim received his just due or
suggests another reason to justify the crime. This is intended to allow the suspect
to confess and still save face.
Wargo says in summary: “Innocent suspects may end up confessing to
extricate themselves from the stress of the situation or even, in some cases,
because the ‘proof’ presented to them makes them begin to doubt their own
innocence … Some suspects, such as people with mental disabilities, are
particularly vulnerable.”
Undoubtedly, the ability to definitively detect falsehood would be of great
help in the interrogation process. A change in heart rate, increased sweating,
fluctuations in blood pressure and galvanic skin response are known as signs of
falsehood, but truth-tellers are also liable to experience stress under
interrogation, so these signs do not constitute conclusive proof.
Any experienced investigator would confirm that liars are talkative, but avoid
using words that express emotion and refrain from speaking in the first person. If
every story has a beginning, middle and end, then liars jump straight to the
middle part. Prevaricators are prone to subtle changes in their body movements
(fidgeting or shrugging), intonation and pace of speaking. Laboratory testing of
saliva can also identify an increase in the concentration of cortisol, a hormone
associated with emotional stress. But all these signs are insufficient to
unequivocally convict suspects, especially if they have an experienced attorney
at their side.
An effective way to identify liars involves what researchers call “cognitive
load.” The underlying assumption is that the cognitive ability of liars is
diminished due to the diversion of substantial mental resources required to keep
track of the disparity between the truth and the false story they told. These
diverted resources are needed to prevent the body from displaying tell-tale signs
and to avoid inconsistencies in the story itself. Researchers believe that adding a
cognitive task during interrogation makes it harder for the suspect to bear this
“load” and can lead to the collapse of the pyramid of falsehood and a confession.
One of the techniques is to ask the suspect to tell his story backwards, from
the end to the beginning. This is relatively easy for someone telling the truth, but
liars have no embedded memory of the events they concocted and this question
creates real cognitive overload for them. In another technique, researchers ask
suspects for a very detailed description of the setting described in their story, or
even ask them to draw it. Liars provide few details in this exercise.

Expert opinion
Making a Murderer is one of the most successful documentary series ever
produced. The series describes the murder trial of Steven Avery and the human
weaknesses that sealed his fate—especially the excessive assuredness of the
police officers, prosecution attorneys, judges and jurors. The defense attorneys in
the series claim that the police planted evidence (keys and blood) to ensure a
conviction and that forensic investigators also contributed their human biases,
manipulating their findings to match the prosecution’s version, which they
wholeheartedly believed.
Itiel Dror, a cognitive psychologist at University College London, focuses his
research on forensic scientists. In one experiment, he presented a series of
fingerprints to five experts who were told the prints were taken from Brandon
Mayfield, an American attorney mistakenly accused of involvement in a terror
attack in Madrid. The fingerprint experts were asked to assess the accuracy of
the FBI’s analysis, which found that Mayfield’s fingerprints did not match those
collected at the scene of the incident.
But Dror misled the experts. Each of them received the fingerprints of
suspects in previous incidents they themselves had investigated and in which
they had found a match between the suspects’ fingerprints and those collected at
the crime scene. After hearing that Mayfield had been acquitted, four of the five
experts (who were sure they were examining Mayfield’s fingerprints) concluded
that there was insufficient similarity to firmly identify the fingerprints with those
collected at the crime scene.
In another study, after Dror told fingerprint experts that the suspect had
confessed, two thirds of them changed their previous assessment.In another
interesting project, Brandon Garrett documented over 150 wrongful convictions
and found that in 61% of the cases in which a forensics expert testified on behalf
of the prosecution, the analysis was invalid. Most of the cases involved less
reliable findings from the outset, such as hair samples, yet 17% involved DNA
samples.
Will the court system, and the entire society, continue to be plagued by such
profound flaws in the fragile process that is supposed to bring justice to light?
If we go back and ask Eric Wargo, we’ll learn that he believes the answer lies
with brain scientists. They already know today that different types of lies trigger
different areas of the brain—lies that are planned and rehearsed activate areas of
the brain connected to memory, unlike spontaneous lies. Response time can also
be an indication of falsehood—a lie related to visual stimulation activates the

relevant area of the brain 200 milliseconds faster than the truth. So far, U.S.
courts have rejected evidence based on brain imaging, but it probably won’t be
long before imaging machines will determine not only who is lying about the
facts, but also the credibility of those who plead temporary insanity in criminal
cases.

PART III
ALL IN GOOD ORDER

LIST FOR LIFE
The magical powers of a to-do list
What can a to-do list tell us about the person who wrote it? Plenty, if we recall
that this is the most common means by which many of us break down our grand
dreams into a collection of earthly tasks. Just forget yours in the pocket of a shirt
that’s sent to the laundry, and you’ll realize how much to-do lists have become
part of your identity.
Our to-do list is like a secular version of a personal prayer, our way of telling
the world and ourselves what we want, and in which order. To-do lists have no
narrative line. Organizing a sock drawer and writing a book of poems could
appear one after the other on such a list. And as many list-makers know, the very
act of committing the tasks to paper has a magical effect on the odds that they
will actually get done—even if a long period of staring at them is sometimes a
prerequisite for obtaining this magical effect. We’re all familiar with the guilt we
feel after satisfying our curiosity by peeking into somebody else’s medicine
cabinet, or at their lists of cell-phone contacts. Reading someone else’s to-do list
has a similar effect. They illuminate the writer’s secret self, the difference
between who the person really is and the more polished version he’s trying to
present. Reading other people’s to-do lists is a type of low-budget voyeurism,
like that of a reality show. We all wonder at times if we’re normal, and reading
the lists of tasks facing others provides a calming response. We find that other
people’s lives are also filled with challenges.
A to-do list is prepared in several stages. The preliminary one is full of hope
and subtle emotions, and makes us confront the blank page face-to-face. At this
point, even if the list is not yet fully formed, thinking about the tasks ahead fills
us with a sense of purposefulness that begets pride. Then comes the pleasant
stage of calling the topics to mind, and the mild euphoria that comes with the
thought of all the vast possibilities still open to us.
And finally there’s the real thrill of satisfaction experienced by those who
have gone as far as working out a strict timetable for completing all the tasks,
most of which are unachievable to begin with. There’s no problem with this if
one accepts the words of American artist Charles Green Shaw: “Real happiness
consists in not what we actually accomplish, but what we think we accomplish.”
But let’s also admit it: crossing a task off the list after it is completed does

have an exhilarating effect which verges on pure euphoria. Psychologists say that
compulsive list-makers are trying to create a sense of control over their lives,
which, were it not for the lists, would be seen as overly chaotic. Such people
have an unconscious fear that their world will spin out of control if they do not
keep making their lists. When you make a list, it’s easier to see which tasks are
more important, and even one that entails a lot of work suddenly seems more
doable when shrunken into a single line on a page. A survey conducted in the
United States found that 42% of the population compose such lists. Still, the
question remains: do they help us function better, or only encourage the bad
habit of procrastination? Because people don’t put off doing a task just because
of the absence of a list on which that task appears.
If you ever do stop to look up from your list-making, you will find that we are
living in an entire world made up of lists: from human civilization’s greatest
cultural assets—such as the Holy Scriptures—all the way down to mundane
things like shopping lists or ideas for what to pack for a trip; restaurant menus;
wills; Google search results; “Ten Best” lists or even the Ten Plagues in Egypt.
Eco’s lists
In 2009, Italian writer Umberto Eco (author of The Name of the Rose and
Foucault’s Pendulum) curated an exhibition at the Louvre in Paris that was made
up entirely of lists. “The Infinity of Lists” constituted a journey through the
worlds of art, literature and music, inspired by the magic that numbers exert
upon Eco and, equally, upon exhaustive research conducted by the writer inside
the world’s most famous museum.
In an interview with Germany’s Der Spiegel weekly, Eco explained that
culture was born out of the human need to make the infinite comprehensible. He
says we use lists, catalogues, museum collections, dictionaries and
encyclopedias to try to grasp that which is ungraspable. Behind every list, Eco
says, lies the difficulty in finding the precise expression for something. If asked,
he would probably say that recitation of the names of those who perished in the
Holocaust is an attempt to anchor the inconceivable in the conceivable.
Eco says it was Homer who served as his inspiration for the entire exhibition.
In The Iliad, when the ancient epic poet seeks to depict the daunting size of the
Greek army, he first invokes the image of a forest fire, which he likens to the
gleam of the fighters’ weapons. When this image does not convey the magnitude
of the spectacle to Homer’s satisfaction, he transcends the limitations of
descriptive expression by means of a long list of names of commanders, their
background and the number of ships they are bringing to battle. The list is 350
lines long.

Lists represent high culture and advanced civilization inasmuch as they enable
us to examine basic definitions that can be used in describing our world. Eco
asserts that cultural history is full of lists: of saints, armies or medicinal plants,
treasures or books. One section of the exhibition was playfully titled “Mille e
Tre” (1,003), the number that concludes the list of women whom the legendary
Don Juan aimed to seduce in Spain. Eco believes that our attraction to lists
derives from the consciousness of the finiteness of our lives. This leads us to
love that which we feel has no limit or end, and thus we distract ourselves from
the inevitability of death. “We love lists because we don’t want to die,” he says.
The exhibition curated by Eco was not the only one on the subject. After this
came a show in Washington (at the Smithsonian) that revealed, by means of the
lists they kept, the compulsive, control-seeking side of some of the world’s most
famous artists. Lists of ideas, instructions, ambitions, biographical details,
paintings, tasks, colors. One list in Picasso’s handwriting proposed names for the
1913 Armory Show, the first international exhibition of modern art in the United
States. The list shows that Picasso did not know how to correctly spell the name
of one of the biggest artists of his time: Marcel Duchamp. Another list, from
1961, written by the great Finnish architect-designer Eero Saarinen, lays out the
tasks for a major architectural project in Oslo. The week after he made the list,
he was diagnosed with a brain tumor and died just days later—a bracing
reminder that none of us will ever be able to accomplish all of the tasks on our
lists.
The word that describes the compulsive urge to make lists is glazomania, and
the Internet is the ultimate instrument for fulfilling this desire. There are more
lists to be found there than one could ever read or use. The Internet is changing
the way in which we consume information. Less words per page, more pictures
—and, of course, more lists. On many list sites, you can also influence the
rankings of the items in question.
Web wonders
Tools for creating and managing lists are available for free. One of these is
listproducer.com, which aims to inspire the would-be list-maker, and help him
become more efficient and organized. The site was launched in April 2011 by
Paula Rizzo, an Emmy-winning Fox News producer, who says she owes her
achievements to her compulsion for list-making. She is certain that making lists
of pros and cons can decide the fate of any relationship.
Listography.com helps users create lists and share them with others, and it

features an application that offers possible topics for lists. Lisa Nola, the site’s
founder, says the idea came to her when she decided to post online lists that she
wrote for her mother before her mother passed away from cancer. She said she
published the personal lists out of a need to share her pain with others, and to be
comforted by people who read the lists.
The advent of social networking sites has made us interested in the daily
doings of people we know (or think we know)—as long as we feel that they are
also interested in us. Listography and similar sites are borne along by the
narcissism that has become the great engine of our century. It seems that our
lives have no meaning unless others are aware of them, and so we are moved to
post on Facebook random lists of the things that describe us.
The first bestseller list was published in 1895 in the American magazine The
Bookman, and since then there has been no end to our thirst for lists: the 20 most
important quotes by Ronald Reagan; the 15 weirdest coincidences; the ten most
common excuses women use to avoid sex; the eight most expensive things I
must buy sometime; the five most common reasons why relationships end; and
three ways to observe the honeysucker.
A whole host of servers support the activity of sites that list the top 100, top
50 or just the top ten things that you absolutely must see, read or do before you
leave this world—like swim with dolphins, grow a beard and keep it for a
month, write a will (how logical), ride an animal larger than a horse, attend the
Olympics, shake the hand of someone who truly changed the fate of a country, or
photograph an animal from an endangered species (aside from the picture, which
you can keep forever, this will also remind you of the fragility of life). Basically,
these are all experiences that entail some sort of genuine physical effort, a foray
into uncharted territories and encounters with unusual beings, which force us to
grapple with our hidden fears. One particularly tempting idea is to “get yourself
an enemy”—a crude way of suggesting that you display so much commitment to
a specific subject that you end up antagonizing someone. But most surprising of
all is actually the similarity of all the lists online. As if all of our dreams and
desires were designed by the same factory.
On ranker.com you can find thousands of lists, the most popular featuring the
names of people and movies. The site boasts 3,325 lists about celebrities, 136
lists about death (for example, the last words of famous people who committed
suicide) and no less than 16 lists about vampires (for example, five reasons to
give into a vampire’s wishes and become one, with the main advantage being
eternal life; the main disadvantage being you have to give up junk food and take
up drinking blood). In a world where the quantitative measurement of
phenomena around us has become a key tool in defining them, we’ve become

enslaved to lists that rank things; if it can’t be put in some sort of order, it
doesn’t exist. But it seems that this ceaseless measuring of the world’s pulse lets
a few important truths elude us. Or could it be that we’re so busy listing
everything in a certain order because we don’t want to have to face up to these
truths?
Noteworthy lists
Listsofnote.com offers lists made by famous figures like Edison, Hitchcock,
Henry Miller and Mark Twain. One reveals director Stanley Kubrick’s ideas for
possible movie titles: “Movie Titles in Search of a Script.” One of the titles listed
there, “If the Fuhrer Only Knew,” comes from a common expression in 1930s
Germany, which was apparently used whenever someone goofed or something
went wrong. Twain’s list was meant to help a gentleman who wants to save the
tenants of a burning house but is unsure in what order things must be done to
save people, and who should be assisted first. The list includes 26 types of
people and items of furniture, in order of importance. In the top spot are
fiancées, followed by ladies toward whom the rescuer harbors a certain
sentiment although he has yet to inform them of this. At the bottom of the list:
fireman, furniture and the mother-in-law.
On the listsofnote.com website you can also find dozens of names proposed
by Thomas Edison and others for the invention that came to be known as the
phonograph, a device that can record sound and play it back. Among the
suggestions: cosmophone, melodagraph and chronophone. The most-read list on
the site is from 1933, and which formed part of a letter from the writer F. Scott
Fitzgerald to his 11-year-old daughter Scottie. In it he lists things to worry about
(courage, cleanliness, efficiency, horsemanship); things not to worry about
(popular opinion, dolls, the past, the future, triumph, mosquitoes, parents, boys,
disappointments and pleasures), and things to think about (what am I really
aiming for in my life?).
In 1927, the Association of American Movie Producers and Distributors
published a list of the 11 subjects that should absolutely be avoided in future
films, childbirth scenes, disparagement of clergy, interracial sex. It also listed 25
subjects to be cautious about portraying, use of the flag, surgery and sympathy
toward criminals. In July 1838, six months before he married his cousin Emma
Wedgwood, Charles Darwin made a list of the pros and cons of marriage. The
pros won out and the couple remained married until Darwin’s death in 1882.
Among the reasons to marry? To have children (“If it please God”); a constant

companion; someone to look after the household; and to enjoy the charms of
music and conversation despite the waste of time entailed in such. Darwin had
trouble picturing himself spending his life alone, toiling away like a worker bee.
Reasons to not marry included loss of freedom to go where one liked; having to
forgo the conversation of clever men at clubs; the need to visit relatives; loss of
time and less chance to read in the evenings; and expenses that would affect
one’s ability to purchase books.
Poetry also took note of the role of lists, as reflected in an excerpt from a
wonderful poem by Wisława Szymborska:
“A list”
I’ve made a list of questions 
to which I no longer expect answers, 
since it’s either too early for them, 
or I won’t have time to understand. 
The list of questions is long, 
and takes up matters great and small, 
but I don’t want to bore you, 
and will just divulge a few.

ON THE SHOULDERS OF GIANTS
The lists of rankings are another level of order The only
thing we like more than ordinary lists are lists of
rankings. In a world drowning in information, rankings
represent a slight chance to create some order in the
brutal bombardment of names, events and products we’re
supposed to buy. Newspaper editors are crazy about lists
of rankings and use them to increase their circulation.
And players in the capital market are happy to head a list
that ranks the performance of money managers.
One aficionado of lists is Steven Skiena, a Computer Science professor at
Stony Brook University and head of its Data Science Lab. Skiena conducts
large-scale text analytics to define quantitative relations between people, places
and various things. His colleague, Charles Ward, is an engineer in Google’s
ranking group, specializing in the quantitative analysis of texts, like only Google
knows how. Ward is also a gifted pianist and devoted player of strategy games
based on history. The two met when Ward worked as a developer in Skiena’s
laboratory and, from that moment, it was inevitable that they would create an
ultimate list—ranking the most influential people in human history.
The important innovation in their approach to building the list was their
preliminary assumption that historical figures act like memes, the term coined by
Richard Dawkins in his book The Selfish Gene. In this view, ideas and genes
spread in the same way. Therefore, to understand how various ideas take hold in
our lives, all we need to do is imagine “genes” that carry a cultural or
psychological load rather than a genetic one (meme), and which succeed in
surviving natural selection (in this case, in the cultural environment as opposed
to nature) and mutation—like the genes carrying a biological load. Ward and
Skiena give the example of the meme of teenage pop idol Justin Bieber, which
“reproduces every time someone reads his Wikipedia page, or he makes news for
some performance or gossip-worthy transgression.” This meme will continue to
be dominant until it loses in a future battle of survival to a meme of a new pop
star that takes over this “environmental niche.”

Here’s another example: the meme defining the “greatest living saint”
belonged to Albert Schweitzer in the mid-1900s, shifted to Mother Teresa later
in the century, and today is ascribed to the Dalai Lama. The researchers’
approach explains how certain figures who did not enjoy real fame in their
lifetimes win a place of glory in human consciousness many years after their
death, and how people who were considered important during their lifetimes
disappear from the pages of history when a stronger analogic meme replaces
them.
So, how is it possible to determine who is more important, Isaac Newton or
Justin Bieber? Is it possible to create a common basis for evaluating the
historical impact of two figures who lived in such different periods?
Ward and Skiena, who are not historians, admit that they cannot assess the real
importance of historical figures by their achievements and contribution to
humanity. In what they believe to be a reasonable alternative, they mobilize their
ability to analyze data and run a complex statistical algorithm to distil the views
of millions of Internet users, as reflected in the way the users conduct searches
for information today. The underlying assumption of the analysis is that past
heroes leave significant statistical testimony to their importance in the Wikipedia
entries that document their life’s work.
The two researchers analyzed the Wikipedia entries of over 800,000 men and
women, adopting the same principles that Google applies when ranking pages.
They also assumed that the most significant figures would have longer entries in
this leading Internet encyclopedia, and that today’s celebrities would have more
page visits, though it’s clear that most of them will fall into obscurity within a
few generations.
The researchers refer to the pace of this descent into oblivion as “the half-life
of fame” (borrowing from the world of science, where “half-life” refers to the
time required for a quantity to decrease by half its initial value). The algorithm
developed by Ward and Skiena calculated all of these factors to arrive at a
ranking of each person’s historical importance. The results of their fascinating
work appear in their book Who’s Bigger: Where Historical Figures Really Rank,
published in 2013.
The authors admit in a detailed description of their ranking methodology that
the main statistical challenge was two-fold: to distinguish between fame and
significance, and to create an index for assessing the fame of current celebrities
in historical terms.
A look at the results reveals that the methodology indeed produced a list that
does not assign undue weight to contemporary figures: Justin Bieber is ranked
8,633 and even President Obama does not make the top 100 (only 111). On the

other hand, Sir Isaac Newton (1643–1727) is accorded the venerable 21st place
on the list.
Here is the list of the 50 most significant people in human history, according
to Ward and Skiena: 1 Jesus
2 Napoleon
3 Muhammad
4 William Shakespeare 5 Abraham Lincoln
6 George Washington 7 Adolf Hitler
8 Aristotle
9 Alexander the Great 10 Thomas Jefferson 11 Henry VIII
12 Charles Darwin
13 Queen Elizabeth I of England 14 Karl Marx
15 Julius Caesar
16 Queen Victoria
17 Martin Luther
18 Joseph Stalin
19 Albert Einstein 20 Christopher Columbus 21 Isaac Newton
22 Charlemagne
23 Theodore Roosevelt 24 Wolfgang Amadeus Mozart 25 Plato
26 Louis XIV
27 Ulysses S. Grant 28 Leonardo da Vinci 29 Ludwig van Beethoven 30
Caesar Augustus 31 Carl Linnaeus
32 Ronald Reagan
33 Charles Dickens 34 Paul the Apostle 35 Benjamin Franklin 36 George W.
Bush
37 Winston Churchill 38 Genghis Khan
39 King Charles I of England 40 Thomas Edison
41 King James I of England 42 Friedrich Nietzsche 43 Franklin Roosevelt 44
Sigmund Freud
45 Alexander Hamilton 46 Mahatma Gandhi
47 Woodrow Wilson
48 Johann Sebastian Bach 49 Galileo Galilei 50 Oliver Cromwell Ward and
Skiena draw great encouragement from the banality of the list. In their view,
the fact that its names are predictable is the true guarantee of its authenticity.
A quarter of the top 100 are important philosophers or religious figures, eight
are scientists or inventors, 13 are literary giants and three represent the
greatest artists the world has known.
Ward and Skiena find further evidence of the list’s quality when comparing it
to two lists compiled by human experts: Michael Hart’s list in his book The 100:

A Ranking of the Most Influential Persons in History and Time magazine’s
“Millennium Top Ten.” The correlation of the list produced by Ward and Skiena
with each of the two older lists is higher than the correlation of those lists with
each other. Those who doubt the authenticity of Wikipedia as a source will be
happy to learn that the prestigious journal Nature found that the rate of “serious
errors” in Wikipedia is similar to that of the renowned Encyclopedia Britannica.
At the top of the list are many people recognized as leaders, including
presidents and explorers. It turns out that the renown of scientists and inventors
outlives that of politicians and cultural heroes. Women do not receive significant
representation: only two appear in the top 50 listed above (Elizabeth I and Queen
Victoria), and only one other enters the top 100 (Joan of Arc). The lists are also
culturally biased, relying on Wikipedia in English, which emphasizes the heroes
of Western history.
While developing the general list, the researchers also created many sublists,
including painters (completely dominated by Renaissance painters), athletes,
popes and judges. In the ranking of the 100 most significant writers, there are
also only two women—Jane Austen (11th) and Emily Dickinson (21st). Thirty-
nine of the writers also appear on a similar list, compiled according to different
criteria by the American writer and literary critic Daniel Burt. Ward and Skiena
also see this as an important confirmation of their quantitative ranking method.
Here are the top ten writers (and their rankings within the general list): 1
William Shakespeare (4) 2 Charles Dickens (33) 3 Mark Twain (53) 4 Edgar
Allan Poe (54) 5 Voltaire (64)
6 Oscar Wilde (77) 7 Goethe (88)
8 Dante Alighieri (96) 9 Lewis Carroll (118) 10 Henry David Thoreau (131)
The only contemporary author to enter the list of the 50 most prominent
writers is Stephen King, ranked in 20th place.
Ward and Skiena are amused by the disappointment expressed by patriots of
various countries whose national heroes do not appear in the list of 100 most
important historical figures. Those who are disappointed by the lack of
prominence of their compatriots in human history apparently miss the list’s most
important lesson: human culture is rich and varied enough to teach us all a bit of
humility.

OUTLOOK: GLOOMY
Humans are wired for bad news, angry faces and sad
memories. Is this negativity bias useful or something to
overcome?
I have good news and bad news. Which would you like first? If it’s bad news,
you’re in good company—that’s what most people pick. But why?
Negative events affect us more than positive ones. We remember them more
vividly and they play a larger role in shaping our lives. Farewells, accidents, bad
parenting, financial losses and even a random snide comment take up most of
our psychic space, leaving little room for compliments or pleasant experiences to
help us along life’s challenging path. The staggering human ability to adapt
ensures that joy over a salary hike will abate within months, leaving only a
benchmark for future raises. We feel pain, but not the absence of it.
Hundreds of scientific studies from around the world confirm our negativity
bias: while a good day has no lasting effect on the following day, a bad day
carries over. We process negative data faster and more thoroughly than positive
data, and they affect us longer. Socially, we invest more in avoiding a bad
reputation than in building a good one. Emotionally, we go to greater lengths to
avoid a bad mood than to experience a good one. Pessimists tend to assess their
health more accurately than optimists. In our era of political correctness,
negative remarks stand out and seem more authentic. People—even babies as
young as six months old—are quick to spot an angry face in a crowd, but slower
to pick out a happy one; in fact, no matter how many smiles we see in that
crowd, we will always spot the angry face first.
The machinery by which we recognize facial emotion, located in a brain
region called the amygdala, reflects our nature as a whole: two thirds of neurons
in the amygdala are geared toward bad news, immediately responding and
storing it in our long-term memory, points out neuropsychologist Rick Hanson,
Senior Fellow of the Greater Good Science Center at the University of
California, Berkeley. This is what causes the “fight or flight” reflex—a survival
instinct based on our ability to use memory to quickly assess threats. Good news,
by comparison, takes 12 whole seconds to travel from temporary to long-term
memory. Our ancient ancestors were better off jumping away from every stick
that looked like a snake than carefully examining it before deciding what to do.
Our gloomy bent finds its way into spoken language, with almost two thirds of

English words conveying the negative side of things. In the vocabulary we use to
describe people, this figure rises to a staggering 74%. And English is not alone.
Aside from Dutch, all other languages lean toward the bleak.
We’re so attuned to negativity that it penetrates our dreams. The late
American psychologist Calvin Hall, who analyzed thousands of dreams over
more than 40 years, found the most common emotion to be anxiety, with
negative feelings (embarrassment, missing a flight, threats of violence) much
more frequent than positive ones. A study from 1988 found that, among residents
of developed countries, American men have the highest rate of aggressive
dreams, reported by 50%, as opposed to 32% of Dutch men—apparently a
compulsively positive group.
One of the first researchers to explore our negative slant was the Princeton
psychologist Daniel Kahneman with his long-term research partner Amos
Tversky. In 1983, Kahneman and Tversky coined the term “loss aversion” to
describe their finding that we mourn loss more than we enjoy benefit. The upset
felt after losing money is always greater than the happiness felt after gaining the
same sum.
The psychologist Roy Baumeister, now professor at Florida State University,
has expanded on this concept. “Centuries of literary efforts and religious thought
have depicted human life in terms of a struggle between good and bad forces,”
he wrote in 2001. “At the metaphysical level, evil gods or devils are the
opponents of the divine forces of creation and harmony. At the individual level,
temptation and destructive instincts battle against strivings for virtue, altruism
and fulfillment. ‘Good’ and ‘bad’ are among the first words and concepts learnt
by children (and even by house pets).” After reviewing hundreds of published
papers, Baumeister and his team reported that Kahneman’s finding extended to
every realm of life—love, work, family, learning, social networking and more.
“Bad is stronger than good,” they declared in their seminal, eponymous paper.
Following the Baumeister paper, the psychologists Paul Rozin and Edward
Royzman of the University of Pennsylvania invoked the term “negativity bias”
to reflect their finding that negative events are especially contagious. The Penn
researchers give the example of brief contact with a cockroach, which “will
usually render a delicious meal inedible,” as they say in a 2001 paper. “The
inverse phenomenon—rendering a pile of cockroaches on a platter edible by
contact with one’s favourite food—is unheard of. More modestly, consider a dish
of a food that you are inclined to dislike: lima beans, fish, or whatever. What
could you touch to that food to make it desirable to eat—that is, what is the anti-
cockroach? Nothing!” When it comes to something negative, minimal contact is
all that’s required to pass on the essence, they argue.

Of all the cognitive biases, the negative bias might have the most influence
over our lives. Yet times have changed. No longer are we roaming the savannah,
braving the harsh retribution of nature and a life on the move. The instinct that
protected us through most of the years of our evolution is now often a drag—
threatening our intimate relationships and destabilizing our teams at work.
It was the University of Washington psychologist John Gottman, an expert on
marital stability, who showed how eviscerating our dark side could be. In 1992,
Gottman found a formula to predict divorce with an accuracy rate of more than
90% by spending only 15 minutes with a newly-wed couple. He spent the time
evaluating the ratio of positive to negative expressions exchanged between the
partners, including gestures and body language. Gottman later reported that
couples needed a “magic ratio” of at least five positive expressions for each
negative one if a relationship was to survive. So, if you have just finished
nagging your partner over housework, be sure to praise him/her five times very
soon. Couples who went on to get divorced had four negative comments to three
positive ones. Sickeningly harmonious couples displayed a ratio of about 20:1—
a boon to the relationship but perhaps not so helpful for the partner needing
honest help navigating the world.
Other researchers applied these findings to the world of business. The Chilean
psychologist Marcial Losada, for instance, studied 60 management teams at a
large information-processing company. In the most effective groups, employees
were praised six times for every time they were put down. In especially low-
performing groups, there were almost three negative remarks to every positive
one.
Losada’s controversial “critical positivity ratio,” devised with psychologist
Barbara Fredrickson of the University of North Carolina at Chapel Hill and
based on complex mathematics, aimed to serve up the perfect formula of 3–6:1.
In other words, hearing praise between three and six times as often as criticism,
the researchers said, sustained employee satisfaction, success in love, and most
other measures of a flourishing, happy life. The paper with the formula, entitled
“Positive Affect and the Complex Dynamics of Human Flourishing,” was
published by the respected journal American Psychologist in 2005.
Achieving the critical ratio soon became a major part of the toolkit developed
by positive psychology, a recent sub-discipline of psychology focused on
enhancing positive measures such as happiness and resilience instead of treating
negatives like disorders of the mind. Yet the ratio provoked pushback, starting
with Nicholas Brown, a Masters student in psychology at the University of East
London, who thought the mathematics was bunk. Brown approached the
mathematician Alan Sokal, of New York University and the University of

London, who helped him dismantle the formula in a paper called “The Complex
Dynamics of Wishful Thinking: The Critical Positivity Ratio” (2013). The
Fredrickson-Losada paper has since been partially withdrawn—and Fredrickson
has disavowed the work in full.
Ultimately, there might be no way to extinguish the negative bias of our
minds. Unable to rise above this negativity bias with praise, affirmations, magic
formulas and the like, it might be time to embrace the advantage that our
negative capability confers—most especially, the ability to see reality straight
and, so, to adjust course and survive. In fact, studies show that depressed people
may be sadder, but they are also wiser, to evoke the famous words of Samuel
Taylor Coleridge. This “depressive realism” gives the forlorn a more accurate
perception of reality, especially in terms of their own place in the world and their
ability to influence events.
When it comes to resolving conflicts on the world stage, the negativity bias
must be part of the mix. International disputes are not going to be resolved by
positive thinking without a huge dose of realism as well. In the end, we need
both perspectives to help us share resources, negotiate peace and get along. In an
article published this June in Behavioral and Brain Sciences, a team led by
University of Nebraska-Lincoln political scientist John Hibbing argue that
differences between conservatives and liberals can be explained, in part, by their
psychological and physiological reactions to negatives in the environment.
Compared with liberals, they say, “conservatives tend to register greater
physiological responses to negative stimuli and also to devote more
psychological resources to them.” That might explain why supporters of
tradition and stability are so often pitted against supporters of reform, and why
the tug-of-war between the two—the middle ground—is often where we end up.
In November 2013, Daniel Kahneman gave an interview in Hebrew to the
New Israel Fund to mark International Human Rights Day. In it, he addressed the
influence that the negativity bias might have on the Israeli-Palestinian peace
talks. He claimed that the bias encourages hawkish views (which usually
emphasize risk or immediate loss) over dovish proposals (which emphasize the
chance of future benefits). The best leaders, he suggested, would offer a vision
where “future gains” were great enough to compensate for the risks involved in
venturing peace—yet without a magic formula, on both sides of the line,
negativity prevails.

COLD HANDS OR A WARM HEART
Ranking the traits that form our views of others
How do we form an opinion about someone we don’t know? If we’re lucky,
something the person says will provide a tentative starting point. Otherwise, the
person’s facial characteristics, body language and our biases will combine to
help us form a quick impression. One of the questions psychologists began
addressing in the early 1940s is whether the impression a person makes is a
function of several personality traits or can be traced to one salient characteristic
that shapes our impressions more than others do.
Solomon Asch (1907–1996), one of the pioneers of social psychology,
conducted a series of classic experiments in 1946 in which he tried to identify
the way people form their opinions about others. In one experiment, Asch
presented lists of character traits to two groups of participants. The first group
received a list of a person’s characteristics: intelligent, skillful, industrious,
warm, determined, practical and cautious. The second group received the same
list, with one difference—the person was described as “cold” instead of “warm.”
The participants were asked to write a brief description of their general
impression of the person, based on the list of characteristics.
Asch discovered that switching the words “warm” and “cold” had a decisive
impact on the participants. The first group expressed a much more positive
impression of the person.
Asch also presented the participants with lists that included other opposite
traits, such as reliable/unreliable and persistent/unstable, and sought to gauge
their impact on impression formation. He found that the significance of many of
the other characteristics suddenly changed when accompanied by the words
“warm” or “cold.” The participants described those with “warm” personalities as
generous, happy, imaginative, humorous and even good-looking. While “warm”
and “cold” had a dramatic impact, other pairs of opposite characteristics, such as
“polite” and “blunt,” had no effect on impression formation.
In another well-known study conducted 40 years later, a “guest lecturer”
appeared before 240 students who had received a detailed briefing in advance. In
the information pages distributed before the lecture, the guest professor was
described to half of the students as a “warm” person and the other half were told
he was a “cold” person. After class, the students were asked their opinion about
the lecturer. The first group described the lecturer as more effective, more
pleasant and friendly, less irritable, more humorous, less formal and, in general,

more human in comparison to the descriptions provided by the second group.
Needless to say, all of the participants attended the same lecture. Earlier research
already found that the participation rate of students in a discussion conducted by
someone described as “warm” was higher (56%) than the participation rate in a
discussion led by a “cold” person (32%).
Since Asch’s time, many behavioral scientists have continued to try to
decipher the human mystery of exactly how we form our opinion of others and,
perhaps more importantly, how others decide what they think about us. One
milestone study found that the two main traits in forming our opinions of others
are warmth and competence. If we focus on these two dimensions when
evaluating others, then we feel admiration for people who are warm and
competent; we pity those who are warm and incompetent; we envy people who
are cold and competent; and we feel contempt for cold and incompetent people.
Geoffrey Goodwin and his University of Pennsylvania colleagues re-
examined the question of impression formation in an article published in the
Journal of Personality and Social Psychology (January 2014), and suggested
that the perception of moral character is the primary factor, exerting a greater
impact than the perception of warmth. In one of their studies, the researchers
looked outside of the laboratory, at the real world: they examined obituaries to
learn how newspaper editors sought to memorialize the dearly departed and how
the information contained in obituaries influenced the opinions of readers who
did not know the deceased. No fewer than 1,289 people participated in this
experiment. They were asked to read and respond to 250 obituary
announcements published in the New York Times during the years 2009–12 in a
special section memorializing well-known figures or people who made notable
contributions to society. Whether due to social discrimination or other reasons,
the 250 distinguished people included 193 men and only 57 women. The list
featured broad ethnic and national diversity, since only those who had achieved a
certain level of national or international recognition made the list.
The researchers chose the special obituary section because of its relatively
rich content, including extensive descriptions of the deceased person’s character
(compared to paid death notices). The obituaries of the 250 notable people often
included quotes from friends and family members, facts about their social lives,
achievements, hobbies and so on. On average, each obituary contained about
1,500 words (over 20% longer than the chapter you’re now reading).
Two research assistants were hired to code the information in the obituaries,
but were not informed about the goals of the study. They read each obituary and
evaluated the competence (or incompetence), moral (or immoral) personality and
social warmth (or coldness) of the person described in the obituary on a scale of

1 to 9, and also gave their general impression of the deceased notable.
In the next stage of the experiment, each of the participants was given three
obituaries to read. (Particularly long obituaries—of Steve Jobs, Osama bin
Laden and Muammar Gadhafi—were removed from the list, as were a dozen
other obituaries describing people about whom the participants were likely to
have already formed opinions.) Each obituary was rated by an average of 16
participants.
From this point, computers took over, programmed with advanced statistical
tools for analyzing the various correlations in the responses. The findings
confirmed the assumption that the obituaries primarily emphasize the
dimensions pertaining to the moral character of the deceased, rather than their
warmth and sociability. More importantly, however, the correlation between the
general impression the obituaries left on the participants and the moral traits
described in them was higher than the correlation between this general
impression and the social warmth ascribed to the distinguished person.
The cautious researchers ran the data again to offset the fact that the
obituaries, from the outset, included more information about the morality of the
eulogized persons than about their warmth and sociability. The outcome
remained the same and is contrary to the findings of Asch and others—at least in
regard to how we form our views about someone who is no longer with us. Not
only do the obituary writers place greater emphasis on information describing
moral character than on warmth and sociability, readers form their general
opinions about the deceased persons primarily based on their moral character
rather than the heart-warming personalities they displayed during their lifetimes.
So how, really, do we decide what we think of others (and how do they form
an opinion of us)? Perhaps both schools of thought are correct: when we meet
someone face to face, warmth is the determining factor in impression formation.
On the other hand, when we read about someone in the newspaper, the person’s
moral character is the salient determinant.

I’VE SEEN HAPPY CONSERVATIVES
What are the psychological dynamics that differentiate
between liberals and conservatives?
“An ounce of algebra is worth a ton of verbal argument,” the engineer and
mathematician John Maynard Smith used to say, quoting the naturalist and
evolutionary biologist J.B.S. Haldane. Maynard Smith stood behind these words
when he attempted to use mathematical tools to answer a question that bothered
animal behavioral biologists in the 1960s: how can we explain the ceremonial,
almost chivalrous, behavior of animals that prefer to retreat from confrontation
with other animals?
Maynard Smith chose to frame the question in terms of game theory: let’s
assume that two sub-species of a particular natural species are competing for an
identical resource, such as food. One species, which Maynard Smith—writing
during the Vietnam War—called “hawks,” chooses a militant strategy in which
at every contact it displays signs of aggression as a first stage, and then proceeds
to a struggle in which it is either victorious or mortally wounded. The other
species, which Maynard Smith called “doves,” chooses a strategy of refraining
from conflict.
In his game, at first contact, the dove indeed displays aggressiveness, but if its
opponent decides to stand up to it, the dove flees to a place of safety. If a hawk
meets a dove, it will win all the food. If it meets another hawk, it will win half of
the time and lose half of the time. If a dove meets a hawk, it will flee and get
nothing, even if it isn’t harmed. If a dove meets another dove, they divide the
food between them. The two species will meet each other, in this game,
according to their relative frequency in the population.
Maynard Smith’s important contribution was the understanding that the
existence of a strategy that ensured evolutionary stability between species (ESS)
also ensured the survival of each one of them. Maynard Smith also managed to
calculate the point of equilibrium between the number of hawks and doves that
ensured perfect balance between gains (food) and losses (injuries and time
wasted) of all participants in the game. Even someone who knows nothing of
game theory can understand that without the doves, the hawks will kill each
other, and without the hawks, the dove population will increase until there’s not
enough food for everyone. The good of the bird population as a whole requires
hawks and doves to exist side by side.

Conservative genetics and liberal psychology
Hawks and doves are as old in political thought as democracy itself. This basic
political template is documented throughout history, from the days of Athens and
Sparta and up to the major parties of almost all present-day democracies.
Debates over tradition vs. innovation, progress vs. stability or “us vs. them” have
always arisen and will continue to arise in cultures where political views are
discussed openly. The circumstances common to the outbreak of wars and
disastrous family holiday meals are the ancient ideological debates between
conservatives and liberals, hawks and doves, or left and right—and these are
only some of the common names for the two camps. Yet the question of why
some people adopt a liberal view of the world while others adopt a conservative
one remains open.
Conservatives differ from liberals in a number of characteristics, from their
taste in art to their orderliness (conservatives leave their desk clear at the end of
the day). Liberals may be more optimistic, but conservatives, it seems, are
happier and less neurotic. Brain scientists tell us that conservatives and liberals
use different parts of their brains to make risk-related decisions and that the
amount of gray matter in the brains of liberals is greater. Even if we would like
to believe that our ideological positions developed in the course of a rational
process of evaluating facts and the opinions of others, an increasing body of
research proves that we are mistaken.
While politics and genetics may seem to be independent of each other, the
attempt to connect them is a natural step in a scientific era that has cracked the
secrets of the human genome. A review of the development of this relatively
young scientific field shows that the first landmark study appeared in 1974, in
the course of an experiment aiming to connect the similarity of the positions of
identical twins (on issues such as the death penalty, unemployment, trade unions
and abortion) to a common genetic basis.
Thirty years later, John Alford led a group of scientists who claimed that no
less than 43% of the difference in political ideology in the United States could be
attributed to genetics. The scientists admitted that there is no single gene directly
responsible for forming political positions, but asserted that genetics has an
indirect effect on them by influencing the cognitive and emotional processes
involved in forming these views. Both psychologists and statisticians received
the results of the study with reservation, and cast doubt on the reliability of the
studies of twins—a common tool in genetic research. The critics claim that the
environment treats identical twins very differently from fraternal (non-identical)

twins, who serve as a control group in such studies.
The social psychologist John Jost proposed in 2003 a different model for
understanding the difference between liberals and conservatives. Following a
meta-study (a comparison of previous research) of 88 published studies in the
field, Jost stated that conservatives are characterized by a high level of anxiety
regarding death, a need for certainty and an inability to cope with ambiguity.
“The core ideology of conservatism,” he said, “stresses resistance to change and
justification of inequality and is motivated by needs … to manage uncertainty
and threat.” His wider claim in a psychological context was that stability and
hierarchy provide conservatives with a soothing reinforcement and a shock-
resistant structure, while change might herald disorder and threatening surprises.
The study of Jost and his colleagues was subject to widespread criticism, but Jost
persevered. In a follow-up study four years later, he repeated his claim that the
psychological need to manage uncertainty and threat lies at the base of our
political leanings. In their desire to avoid, as much as possible, uncertainty in a
world they perceive as threatening, conservatives need order and are not open to
new experiences. The desire to manage uncertainty is also the reason that
conservatives tend to make decisions faster, regardless of their cognitive ability.
In the next stage, researchers attempted to understand the connections
between liberals’ and conservatives’ political views and personalities. They
chose an accepted definition of personality based on five main traits, known as
The Big Five: openness to new experiences, conscientiousness (willingness to
work hard, responsibility and focus), extraversion, agreeableness and
neuroticism. It was discovered that the two traits most relevant to political
orientation are openness to new experiences and conscientiousness.
In the name of science, researchers sniffed around the homes and offices of
participants in the study and looked for clues to their ideological preferences
according to the way they designed their private spaces. The study found that
conservatives had more organized and cleaner rooms, and their well-lit offices
were decorated simply and were usually less comfortable than those of liberals.
In contrast, liberals’ bedrooms contained more books, maps, travel documents
and many musical items, including world music. Their offices were more
colorful and held more books. Liberals, the researchers confirmed, were more
open to experiences. The innocent reader of the study might think for a moment
that if conservatives fear a world ruled by chaos, liberals fear a world empty of
feelings and experiences.
A particularly important observation on the way to formulating a more general
understanding of the topic was published in 2008 in the respected journal
Science. Using eye-tracking devices and skin-conductance sensors, scientists

discovered that individuals who had previously self-defined as conservatives
responded more intensely to negative stimuli. Pictures of a very large spider on
the face of a frightened person, a dazed individual with a bloody face, and an
open wound with maggots in it, interspersed among neutral pictures, caused the
needles on the sensitive instruments connected to the participants to jump more.
The startle reflex upon hearing a loud noise was also more distinct in this group.
Conservatives, the researchers concluded, are quick to notice the negative,
spend a longer time looking at it, and are more easily distracted by it. The
phenomenon has evolutionary roots, anchored in an early period when a quick
response meant the difference between a live conservative and a dead liberal.
This explaines why those biologically predisposed to hold back threats prefer
political positions that favor military strength, limiting immigration (seen as
spreading disease), fighting assimilation and tough law enforcement. However, it
is important to understand that from a scientific point of view, of course, there is
no advantage to either approach. Moreover, if Einstein was right in saying that
we cannot solve our problems with the same way of thinking that created them,
then seeking innovation becomes no less important for our survival than
sensitivity to threats—the property of our evolutionary past.
An article published in 2014 continues from this point and attempts to
definitively identify the central factor differentiating between conservatives and
liberals. The authors (some of whom participated in other studies mentioned
here) claim that although there are many traits that differentiate between the
reactions of conservatives and liberals, the “organizing factor”—the central trait
that other factors revolve around—is indeed the different physiological and
psychological reaction to negative environmental factors. Conservatives are far
more influenced by them, and they tend to respond more to negative stimuli and
to invest more psychological resources in this. In contrast, liberals are less
threatened by the negative, a behavioral template congruent with their tendency
to try to examine new approaches to life and manage them, even at the price of
social sensitivity to threats and disorder.
The researchers sum up their work: “It is reasonable to assume that those who
react more, physiologically or psychologically, to negative stimuli tend to
support a public policy that minimizes the perceived threats, since it bestows
importance upon traditional solutions from the past and limits human discretion
(supporting ideas like the free market, that do not involve expressions of
generosity, consideration or altruism). For this purpose, it also promotes internal
groups (“us”) at the expense of external ones (“them”) and adopts a decisive and
unifying policy set by authoritative figures.” One of the reasons for the
negativity bias being such a convenient diagnostic tool for research is the great

difference that people evince in relation to it.
The journal that published this article, Behavioral and Brain Sciences, has the
refreshing custom of publishing, alongside particularly interesting or
controversial articles, responses to its conclusions by other researchers. Of the
26 articles published in response to this one, 22 supported its conclusions. The
other four also accepted its main claim, but wished to improve the definition of
the term “negativity bias” and the differences between conservatives and liberals
in the specific context in which the article relates to them.
Anyone even slightly familiar with the competitive world of scientific
research understands that this is sweeping support. The common denominator of
the authors of the article and those commenting on it was the understanding that
conservatives and liberals may be distinguished on the basis of a personality trait
that differentiates between them, and which is expressed not only in
psychological characteristics but also in physical ones, sometimes even genetic
ones. John Jost, who waited for over a decade for his early findings to be
confirmed, was a prominent name among the responders.
John Hibbing and his colleagues, who wrote the article, summed up their
views in a book, Predisposed: Liberals, Conservatives, and the Biology of
Political Differences, published in 2013. In an interview conducted as part of the
book launch, Hibbing said: “Conservatives often say ‘liberals don’t get it,’ while
the latter are convinced that conservatives only increase the sense of threat. Both
are right.” He added: “If we could get people to see politics in the same light as
sexual orientation or left-or right-handedness, maybe we could be a bit more
tolerant.”
Eagles don’t flock
The work of Hibbing and his colleagues forms the psychological reflection of
the biological equilibrium that Maynard Smith found in the hawks and doves
model he developed. Society needs both: conservatives guard us as a society
against exploitation and attack, while liberals bring us forward with the
innovation they encourage and their openness to new experiences. Societies gain
from the combination of those open to outside influences and those who attempt
to block any tie with them—even a group of spiders gains advantages from
coexistence between more and less sociable ones.
Anyone who has internalized this surely can also understand that someone
who disagrees with us is not shallow, ignorant or the product of mistaken intent.
He or she simply experiences the world in a different way from how we do,
processes his or her experience and responds appropriately. We all have a moral
preference for the way a society should be structured—some of us prefer

hierarchy, others prefer egalitarianism; some of us support severe punishment of
lawbreakers, while others are more lenient; some of us view external groups
with curiosity and interest, while others see them as a threat.
That said, the idea that conservatives and liberals would be able to reach
agreement through dialogue is unrealistic. It is more practical and rational to
recognize the differences between us, the sources of those differences, and the
fact that we notice different things. Perhaps then we will be ready to
compromise, understanding that this is the best way to realize our interests.
Additionally, it is important to understand that our ideological positions
represent points on a continuum rather than a clear and definite worldview suited
to all topics on our personal and national agendas. Indeed, some studies focus on
what members of both camps have in common. One such study examined the
expectations of conservatives and liberals from a matchmaking site to which
they had registered. It turned out that everyone is looking for the same thing: a
partner similar to themselves.
Another study, conducted at the University of Winnipeg, found additional and
unexpected areas of similarity. If you thought that conservatives favored
obedience while liberals were inclined to protest and favored obedience less, you
would be mistaken. According to the study, both sides valued obedience to
authority equally, but differed in their definition of which authority deserved
obedience.
Another quality that conservatives and liberals have in common is their
tendency to exaggerate the power of the rival group’s moral positions. While
there are real differences in the ways in which liberals and conservatives
perceive various moral issues, both sides overestimate the dimensions of these
differences and in this way reinforce opposing stereotypes of each other.
Understanding the fact that liberals and conservatives are closer to each other
than they think is another milestone on the path to a possible bridge over the
hostility and lack of trust between them.
However, something important has gone wrong on the way to translating the
insights of up-to-date psychological research into a process of reconciliation
between conservatives and liberals at the social and national level. It appears that
in almost all democratic countries, the hawks have more influence over political
decision-makers than the doves. The reason for this lies in the influence of
various cognitive biases on the decision-makers and their advisers, which is
expressed—disturbingly enough—particularly during wartime. Both of the
warring sides tend to exaggerate their enemy’s hostile intentions, make mistakes
in evaluating the enemy’s perception of themselves, display unwarranted
optimism when hostilities begin and stubbornly refuse to make any concessions

in negotiations after the battles have ended. These biases encourage wars to
break out and prevent them from being ended quickly.
In an article published in Foreign Policy in 2007, Daniel Kahneman described
a long list of biases that cause decision-makers to give greater weight to the
opinions of hawkish advisers. Hawks, Kahneman claims, are sure of themselves,
don’t trust “the other side” and tell a story that is simpler and more coherent than
any story to be found in the real world. This is the story of the hedgehog in a
world where “foxes know many things, but the hedgehog knows one big thing,”
as Isaiah Berlin famously said in his 1953 essay entitled “The Hedgehog and the
Fox.”
A central bias that provides support for hawkishness is our tendency to judge
the deeds of others as reflecting their nature (“They are a religious group who
will use any means to achieve their ends”), while our own deeds are always a
response to circumstances (“We had our backs to the wall, so we were forced to
respond”). The problem, of course, is that the other side thinks in the same way.
Who remembers today that all of the parties involved in World War I regarded
themselves as less threatening than their rivals?
Another well-known bias, which has already taken a heavy toll on humanity,
is inherent in human optimism. This bias, which is the source of initiating wars,
can be found among advisers, leaders and military men. All sides in World War I
believed they would be home by Christmas—they simply neglected to say which
year. The commander of the French army, General Noël de Castelnau, declared
before the most destructive war in human history, “Give me 700,000 soldiers and
I will conquer Europe.”
In addition to optimism, we suffer, as we have already seen, from
overconfidence and an exaggerated sense of control in various situations. Just as
we are certain that we are superior drivers or have the stronger case in court,
advisers and leaders believe that the balance of power is in their favor and that
they will succeed in controlling the course of the war once it has begun.
Even before the battle is over and each side begins to take stock of its losses,
the hawks are able to express another human bias. If we are presented with the
opportunity either to make a certain gain or, alternatively, to possibly make a
larger gain at the risk of losing everything, we’ll prefer the certain gain. Yet
when it comes to losses, we behave differently: here we’re willing to gamble that
we’ll manage to limit our losses even at the price of risking much greater losses.
Since the decision-makers on both sides are exposed to this bias (loss aversion),
wars are prolonged unnecessarily because of the rivals’ difficulty in giving up
the battlefield gamble that may yet decrease their losses.
The last bias that Kahneman mentions deals with the difficulty in ending

conflicts through negotiation due to our different attitude toward gains and
losses. According to the prospect theory that he himself formulated, we need a
gain that is double a given loss in order to compensate for the emotional pain of
that loss. Since we see what we have given up in negotiations as a loss and what
we have received as a gain, we need to receive at least twice of what we give up
in order to feel comfortable. The only problem is that the other side feels the
same way.
Kahneman understands that this does not mean that hawks are always wrong.
He only thinks that they are too persuasive. It seems that in the standing
argument between hawks and doves we all really need eagles—leaders who can
see beyond their biases to the real threats, while at the same time detecting
opportunities, even when they involve uncertainty. The problem is, as Ross Perot
said, eagles don’t flock. You find them one at a time.

HE MATTHEW EFFECT
The mysterious engine that generates inequality
“The Matthew effect” came into the world 2,000 years ago in a verse from the
New Testament that reads, “For unto every one that hath shall be given, and he
shall have abundance: but from him that hath not shall be taken away even that
which he hath” (Matthew 25:29). In other words, whoever already enjoys a
position of advantage excels at increasing that advantage and enjoys additional
recompense, whereas those who have not are prone to losing even the little they
have. Simply put, it is a cycle of self-enhancement in which the rich get richer
and the poor poorer, until the winner takes all.
The term was coined in 1968 by Robert Merton, a sociologist at Columbia
University, who identified the phenomenon within the field of academic
research. He noticed that well-known researchers accrue publicity and credit
owing to their being such, whereas new ideas from lesser-known researchers are
rejected by editors of the important journals. Similarly, the important prizes are
always garnered by the senior researchers taking part in a study, even if most of
the work is done by junior researchers. The 1987 Nobel Prize in economics was
awarded to Robert Solow of MIT, even though that same year Trevor Swan, who
is less well-known, published identical work, and the same goes for the 2000
Nobel Prize in chemistry for the conductivity of polymers. The phenomenon
strikes in the early stages of an academic career: whoever was fortunate enough
to get into a more prestigious university benefits from a team of better research
assistants, more advanced equipment and more convenient access to research
grants. Students who enter the job market in years of economic recession are
doomed to go on dragging their pay lag (compared to those who began working
in a boom year) throughout their professional lives.
The Matthew effect has a known impact on many fields. Children, for
example, acquire on average ten new words daily during their school years while
listening or reading. The difference between them is great and those who read
more are exposed to a wider vocabulary. But as time goes by, the gap grows
increasingly large between children with a rich vocabulary and those with a
meagre vocabulary, and if that were not enough, the students with the rich
vocabulary also receive the best instruction, sometimes in gifted classes, and the
greatest amount of educational attention from sympathetic teachers. In short, the

good students get better.
In his book Outliers, Malcolm Gladwell surveys the circumstances that lead to
the growth of outstanding ice hockey players. His survey documents a
fascinating aspect of the phenomenon: when he analyzed the birth dates of
outstanding hockey players, Gladwell discovered that most of them were born in
the first quarter of the year. Solving the mystery does not require sophisticated
lab equipment. When these players started attending school, Gladwell explains,
their physical dimensions were above average in the age group to which they
were assigned. This starting point, seemingly marginal, was enough to grant
them a slight advantage when they were chosen for their age group’s
representative team and hence they gained more practice sessions and playing
time. That advantage led to an opportunity that increased the advantage a bit
more, and the new advantage in its turn created another opportunity, and so forth
up to the development of a truly outstanding player. Outliers “are invariably the
beneficiaries of hidden advantages,” Gladwell claims, but the other side of this
equation is sad tidings for human talent that remains unexpressed only because
its starting point was not good enough.
The economic aspect of the Matthew effect recalls the power of compound
interest—the dizzying growth of an economic investment when the interest it
yielded has been reinvested. The wealthy are the main beneficiaries of this
phenomenon in a large variety of privileges and benefits, but mainly in
occupying a good starting position to take advantage of business opportunities
that cross their paths more than the paths of others.
The Matthew effect is the engine that generates the growth in inequality in
society but it is not preordained and can be restrained by setting a different set of
social priorities. If we go on thinking that economic growth at any price is the be
all and end all, even when only a few enjoy the fruits but many bear the price
burden, the Matthew effect will be sure to whirl the dimensions of inequality to a
level that will not be socially sustainable for long.

ODE TO A BUREAUCRAT
What empowers the bureaucrats of the world?
In 1600, a battle held in the strategic mountain pass of Sekigahara decided the
fate of Japan. Tokugawa Ieyasu was outnumbered, yet gained the upper hand
after persuading several enemy commanders to join his ranks. The victory won
him the position of shogun—supreme ruler of Japan. Under the dynasty he
founded, which held power for more than 200 years, Japan was stable and
culture flourished as never before. The samurai, warriors dedicated to protecting
their noble master, fell from their elite status and found themselves bereft of
masters and of war. They became governors, judges, tax collectors, police chiefs
and clerks—servants of a bureaucracy that was a far cry from the world they
knew and for which they had been lengthily trained. The adjustment was
difficult, especially as it meant a loss of finances. For years, the samurai kept up
the displays of loyalty to which they had been accustomed, avenging their
masters and committing suicide over their graves. With time, however, they
yielded to the shogun and shifted their loyalty to the state and its institutions.
The only way to keep their heritage alive was to develop martial arts, which
could not be put into practical use. It was during this time that discipline and
obedience grew central to Japanese culture. This historical example illustrates
how bureaucratic structures force individuals into modes of behavior that change
everything they have been brought up on, and in many ways even their
emotional makeup.
“There is something about a bureaucrat that does not like a poem,” observed
American writer Gore Vidal. Indeed, most bureaucrats are psychologically built
differently from the rest of society. They draw their identity from the
organization they serve—sometimes, at the cost of blurring their personal
boundaries. A bureaucratic organization cannot permit its workers the freedom to
evaluate what is right and what is wrong, what is moral and what is not.
Consequently, bureaucrats are not influenced by an inner value system that
guides their actions, and their professional compass functions separately from
their emotional world. That is also why bureaucrats often cannot be swayed by
considerations of goodness, fairness or justice. They go by the protocol or by the
letter of the law. Discretion is not on their job description and is not part of the
formula that ensures promotion. There is little reason to envy bureaucrats: many

experience a crisis when required to replace their personal identity with that of
the organization. They are frustrated by depending on their employer for their
identity and by relinquishing discretion in general, and value judgements in
particular. The bureaucrat is required to leave his feelings at home, in a world in
which every human encounter generates expression of emotion.
I have met many bureaucrats throughout my career, yet almost every attempt
to explain my position to them in my usual business format has met with abject
failure. Eventually, I realized that bureaucrats are essentially different from
business people and therefore guided by different considerations. Business cares
about end results; bureaucracy cares about the process. If an entrepreneur is a
person who can make the impossible possible, many bureaucrats have perfected
the art of making the possible impossible.
Bureaucrats have nothing to gain and something to lose. Their wages do not
motivate them to take unnecessary risks in decision-making, and bonuses for
taking successful risks are a distant dream. They are primarily engaged with
minimizing personal costs. A bureaucrat is like a weather vane when it comes to
detecting the direction from which minimal danger blows. His behavior can only
be altered by pointing out a greater risk in a different direction. Then, he may
decide in favor of the applicant, whose request appears to entail less risk in the
given situation.
A common tactic is threatening to sue for damages over delaying a decision.
However, if you know the mind of a bureaucrat, you don’t need to go that far. A
business acquaintance of mine once related how he had been questioned by the
SEC (Securities and Exchange Commision, the US financial regulator) as a
witness in a case of the use of insider information by a third party. The SEC’s
lawyer who questioned him, and who was eager to have his suspicions
corroborated, kept repeating what the witness was saying, occasionally
misquoting him, as part of the questioning technique. My acquaintance, tired of
correcting him, decided to shift the power balance in the room by introducing a
new risk. “If you don’t stop quoting me imprecisely, I will start speaking very
slowly and you will be stuck in the office until after five.” The unpredicted risk
worked like magic, and the questioning came to a quick end.

PART IV
ALONE IN THE CROWD

ME, MYSELF AND I
The sorrows of the narcissist as a social animal
It was late March and the sun had just risen on a cool day in Paris as I went out
for an early-morning run. Thirty minutes later, I decided to turn back and return
to the hotel. I quickly climbed the stairs leading from the bank of the Seine,
where I ran, to the street adjacent to the Notre Dame Cathedral. The city, whose
beauty was a bit distant from river level, became palpable as early-rising
pedestrians emerged, along with a few cars that moved unimpeded on the streets
at that hour. An illuminated billboard was the first to greet me in the new
surroundings. At first, I almost didn’t notice it, but there was something in the
advertisement that was sufficiently disturbing to make me stop, take a few steps
backwards and look again.
I stood facing a billboard with a framed photograph of a young woman,
casually draped in bright burgundy-colored cloth, reclining on a worn sofa. Her
elegant and fashionable clothing hung loosely on her body, generously exposing
shapely legs; heavy jewelry dangled from the delicate joints of her body. With
one of her arms propped against the sofa, her head tilted sensuously toward what
seemed at first to be the head of another woman, who was making an identical
gesture and reciprocated with a head tilt of her own.
The talented advertiser had achieved his or her objective: I interrupted my run
and approached the photograph to examine it more closely. Then I realized there
was no other woman. The model leaned longingly toward an image of her own
face, peering at her from a mirror.
This advertisement, for a well-known Parisian fashion house, expresses the
spirit of the times, I thought to myself as I returned to the monotone pace of
running. Gone are the days of producing perfume from the sex glands of animals
in order to capture the attention of members of the opposite sex. Gone too are the
days when women dressed nicely to impress men, or equally important, other
women. It seems that today we suffice with impressing ourselves. What is the
common denominator between a 16-year-old who boasts of accumulating 3,000
Facebook friends in a single month, and a racecar driver who defies the team
manager and passes his teammate in an important race, and a corporate
executive who pays no heed to others and only focuses on aggrandizing himself?
All of them are standing at the edge of the same pool of water and looking at
their social reflection. Welcome to the global lake of narcissism.

Once, when we were more social
From the dawn of history, the human race has adopted a survival strategy based
on culture: a consensual system of beliefs, values, practices, ceremonies and
symbols that serve as the foundation for the way people conduct their social
relations. The place of the individual has largely been defined by the culture,
which changes as humanity wrestles with new challenges. While it was
impossible to physically survive in the savannah of Africa 150,000 years ago
without belonging to a tribe, today social affiliation meets other needs, primarily
psychological.
Indeed, if we try to define a particular individual, we’ll discover that we can’t
do this without referring to others, usually members of the group to which the
individual belongs. Among many mammals, the distinction between individuals
in the group is made via a hierarchy in the social structure, and this is especially
prominent in the primate population, including some of the species that are
particularly close to human beings.
Nature researchers have identified social behavior in many other species,
including some that are very distant from us in their level of development.
Wolves share their food in a relatively fair way with other members of their pack
and rely on them to help raise their offspring. Among horses, elephants, hyenas
and dolphins, we can observe friendships that span many years. One study found
almost humanlike social behavior among 120 collies—behavior that was even
more social than that of nature’s greatest co-operators: the chimpanzees.
Already in the early 20th century, the psychologist Alfred Adler suggested
that the basic need for belonging has evolutionary survival advantages. He also
asserted that the main motivation for our activity is social (not necessarily
sexual, as his predecessor Freud believed), and that the need to belong is a
profound human need. Adler’s hypotheses have recently resonated in the work
of two researchers of animal behavior from the University of Pennsylvania.
Dorothy Cheney and Robert Seyfarth challenged the traditional Darwinian
assumption that the most aggressive, competitive and dominant in the packs of
different species receive the privilege of producing more offspring.
The two conducted research on the social behavior of baboons, examining the
social habits of 90 monkeys in Botswana for over 15 (!) years. They wrote about
their work in a fascinating book—Baboon Metaphysics: The Evolution of a
Social Mind. The authors discovered that the best predictor of a female baboon’s
fertility (number of offspring) is actually the strength of her social connection
with other females in the group. They also found that survival rates and life
expectancy were higher among offspring of female baboons who exhibited well-
developed social connections.

For the sake of science, the two researchers did not hesitate to get their hands
dirty: They examined the droppings of the female baboons in order to trace
substances that are usually secreted when experiencing tension and stress. The
death of a close member of the group led to a higher presence of these
substances in their droppings. But the researchers soon found that after such
tragic occurrences, the females created social connections with new females, and
the indicators of distress in their droppings disappeared. It seems that even
natural selection prefers baboons who develop social connections within and
outside of the family. A different study found that the level of oxytocin (a
“cooperation hormone” also called the “trust molecule”) among chimpanzees
who spend their time delousing other chimpanzees is similar to the high
hormone level stimulated by the presence of close blood relations.
However, many of the human alternatives to the social activity of delousing
disappeared near the end of the 19th century. The traditional connection of
people to their land and to their small community gave way to the new
possibilities that industrialization engendered, as well as the alienation created
by the process of accelerated urbanization. In the earlier way of life, people drew
most of their psychological well-being, identity and even their sense of self-
worth from their group affiliation. Today, however, we have slowly and surely
reached a situation in which people derive their sense of psychological well-
being and self-esteem from their unique personal characteristics and individual
achievements.
It is interesting, therefore, to examine what happened along the way—from
the rural society of the 19th century to the individualistic society of today. It is
an interesting coincidence that the science of psychology developed during this
same period. In the years since Adler presented his theories, many behavioural
scientists have been asked their learned opinions on the important connection
between the individual and society.
The balance between egoism and belonging
“We can picture a person standing on an axis with his selfish needs on one end
and his basic need for belonging and connection on the other end,” Prof. Gilad
Hirschberger from the IDC Herzliya wrote in the inaugural edition of the Alaxon
digital magazine (in Hebrew). “The individual continually tries to maintain the
fine balance between personal advancement and acceptance by others. If he acts
too selfishly, the society will react with rejection and exclusion. If he invests a
lot in others, his own personal needs are liable to suffer. The equilibrium is
elusive and is influenced by many dynamic forces.”
I went back to Prof. Hirschberger to try to understand the nature of the forces

that affect the equilibrium. “The tension between egoism and belonging is in my
view the very heart of psychology,” he said in an interview. “If we had no need
for others and their approval, we’d act in a purely selfish way. Since our
existence is conditional upon cooperating with others, and our place in society
depends on how others assess us, we have to continually balance the naked
personal interest and the desire to belong.”
A study conducted by Roy Baumeister and his colleagues at Florida State
University found that the group also benefits when it maintains distinctions
between its individual members and that it loses this advantage when its
members take on a homogenous identity. That is, the society, and not only the
individual, benefits when “the elusive balance point” is in the right place.
People whose connections with their surroundings are insufficiently
developed and whose sense of social affiliation is deficient are prone to suffer
behavioral and health problems, Baumeister asserts in an article he co-authored
with Mark Leary of Duke University in 1995. The article, entitled “The Need to
Belong: Desire for Interpersonal Attachments as a Fundamental Human
Motivation,” argues that social behavior can be largely explained by the basic
need to belong. In their seminal article, the authors claim that the need to belong
is more important than most of the other sources of motivation, and that our
thoughts, emotions and behavior focus on this need. Many of our anxieties stem
from the fear of rejection and social isolation, Baumeister and Leary contend.
Their research drew extraordinary attention, which reflected the interest in their
new thesis, which was based on sociology, anthropology, political science and,
of course, psychology.
But the desire to belong to a group is not enough, Leary said in an interview
filmed in 2015. Members of the group also need to accept us. The traditional
emphasis on the desire to belong overlooks the fact that we don’t merely want to
belong—once we become part of the group, we also seek to become prominent
in some way. We might try to stand out as a hilarious joke-teller, as an authority
on the latest trends in the movie industry, or in a myriad of other ways.
Igor Grossmann and Michael Varnum of the University of Waterloo wrote an
article introducing another innovative and primarily social approach to
understanding the growth of individualism. Grossman shared the key points of
the article at a conference of the Association for Psychological Science (APS)
held in Washington in May 2013. His presentation, entitled “The Rise of Middle
Class Individualism in America,” began with two illustrations showing a family
sitting at the dinner table. In one, from the 1950s, everyone is engaged in lively
conversation. In the second illustration, from a few years ago, everyone is
looking at the television in the corner of the room. The television, which is

absent from the first illustration, is showing a football game. The two
illustrations, Grossman argues, represent the cultural change that has occurred in
the past 50 years. It appears that the traditional society of the mid-1900s, when
people enjoyed direct interaction with those living in close proximity, was
supplanted by a society in which people forgo human interaction, opting instead
for media tailored to their needs and tastes.
Drawing upon data indicating that Americans have become more self-
centered, Grossman wondered whether this represents a cultural change (as
opposed to a political-economic one) or a change dictated by the media. Thus, he
sought to trace the role of changes related to the urbanization of America and the
development of social classes. And instead of relying on questionnaires with
self-reporting by impatient students, he examined real data.
Grossman based his research on several factors that he views as indicators of
the level of individualism in a society: the type of names given to newborns (the
relative frequency of unique versus conventional names), the prevalence of
words reflecting individualism versus collectivism in statements by leaders, and
the frequency of the appearance of these words in books over a period of time.
In the analysis of baby names, there is an implicit assumption that unique
names indeed reflect individualism. The conventional index is the percentage of
boys and girls who receive one of the 20 most popular names in their year of
birth. (A low percentage reflects a high level of individualism.) An analysis
conducted by the psychologist Jean Twenge and her colleagues on baby names
during the years 1880–2011 indicates that the percentage of unique names has
risen continually, especially since World War II.
In 1946, for example, over 5% of the boys were named James and more than
4% of the girls were given the name Mary. During that period, one of every three
newborn boys received one of the ten most popular names, as did one of every
four newborn girls. Half of all the boys born in the U.S. immediately after World
War II received one of the 23 most common names. In the middle of the first
decade of the 21st century, on the other hand, the relative prevalence of the most
popular names (Jacob and Emily) was only 1%. (In Israel too, the names of
biblical patriarchs and matriarchs of the 1950s gave way to “cooler” names, and
in 2007 only 2.5% of newborn boys shared the most popular name, Itai.)
In a particularly interesting analysis, Grossman and his research partners
examined the speeches of U.S. presidents dating back to 1860, identifying verbs
that express individualism (prefer, differ, own, achieve) versus collectivist verbs
(give, belong, share, together). They indeed found an increase in the use of
individualist words, peaking during the period of Bill Clinton’s presidency. A
similar analysis of literary texts from 1860–2006 found comparable results with

a particularly high statistical correlation (which weakened slightly toward the
end of the period).
Grossman tried to isolate the factors that combined to create a more
individualistic society. Using complex statistical equations, he ruled out
technological change as a possible cause. Changes in population density and
urbanization, the incidence of contagious disease and natural disasters (people
tend to give unique names after collective traumas like wars) also fail to offer a
satisfactory correlation with individualism. The only factor that correlates with
the phenomenon is social class, as measured by a combination of average
income and level of education. As income and education level rise, there is a
greater tendency to give newborns unique names, and certain words appear more
frequently in presidential speeches. Thus, according to Grossman, social class is
the best indicator of individualism in a society.
The most narcissistic generation in history
The importance of unique cultural components in the self-image of people from
different nations offers some hope that the trend of rising individualism may
slow globally. When asked to choose a pen from a pile of different colored pens,
Koreans will prefer the most common color, while Americans will select the
least common color, the unique one. Advertisements in Korea emphasize that
behaving like others is the proper thing to do.
But whether the viewpoint is psychological or socio-cultural, the learned
discussion on balancing the need to belong and the need for uniqueness cannot
easily explain the cultural whirlwind in Western countries that led to a
completely new equilibrium—one that enables self-expression bordering on
narcissism, while maintaining a sense of belonging. In a process that began
somewhere in the early 2000s, the consumer culture and the creation of social
networks upset the delicate historical balance between the need to belong and the
need to express our desires as individuals whose interests don’t necessarily
correspond to those of the society. As the consumer culture enabled us to
emphasize our uniqueness by choosing our favorite brands (many of which start
with “My” or “I”), the social networks offered the ultimate technology for
presenting ourselves to others in the most positive way possible. Our brains
easily resolve the random dissonance in the way we describe ourselves on the
Internet as compared to the glum reality of our lives. After all, we have long
become accustomed to relegating information to our subconscious that might
show our character (and especially our integrity) in a negative light. The
psychological model underlying the social networks seems to award the highest
scores to those who leverage their narcissism most effectively—that is, those

who receive the greatest amount of social attention while investing relatively
little in others. This achievement is not obvious, if we remember that we
ultimately need others more than they need us. In addition, social networks are
always enticing as a means of conveying a coveted sense of belonging without
requiring a burdensome commitment.
How did this happen? Is it possible to identify other characteristics—besides
the increase in buying power, education and the possibilities offered by Internet
technology—that engendered a narcissistic and corrosive version of
individualism?
The psychologist Jean Twenge addresses this question in her book entitled
Generation Me: Why Today’s Young Americans Are More Confident, Assertive,
Entitled—and More Miserable Than Ever Before. The book describes in
disturbing detail the price that America is paying for the education that more and
more young people are receiving, which prioritizes a sense of self-worth over
achievement. Consequently, these young people prefer “me” to anything else.
The author argues that both parents and educators have contributed to creating
the most narcissistic generation in history—parents by failing to set limits for
their children and the education system by sanctifying the pupils’ sense of self-
worth at the expense of self-discipline and educational seriousness. In this
system, prizes and awards are routine, and everyone eventually receives one.
The “Pupil of the Month,” the “Spelling Prize” or “Outstanding Member of the
Debating Team” are some of the creative titles designed to boost a pupil’s
confidence. Hard work is more highly valued than learning achievements, and
two of every three educators are willing to raise the grades of pupils who
convince them that they had tried hard enough. In such an atmosphere, it is no
surprise that some of the ethnic groups with the lowest self-esteem lead U.S.
pupils in academic performance.
Twenge wrote her second book, The Narcissism Epidemic: Living in the Age
of Entitlement, together with Keith Campbell, a psychologist who also
specializes in this subject. The book presents the conventional questionnaire
used to identify excessive self-esteem—the respondents’ desire to wield control
in the world, their inclination to define themselves as special, and the ease with
which they perceive themselves as deserving. The responses of American
college students indicate a 15% increase in the number of narcissists in 2008
compared to 1996. In general, one of every four Americans is today ranked as
highly narcissistic, and 10% are defined as clinically narcissistic. This
percentage increases annually at the same threatening pace as the spiraling rise
in obesity.
Of the 23 women and men who served in President Eisenhower’s cabinet

(1953–61), only one—the Secretary of Agriculture—published memoirs after
leaving office. In comparison, 12 of the 30 cabinet members in the Reagan
administration considered their lives important enough to interest the public and
published memoirs after Reagan completed his term in 1989.
The growing incidence of narcissism is accompanied by a decrease in the
level of interpersonal trust and empathy, and a steep decline in the importance
young people attribute to a “meaningful philosophy” in their lives. While 12% of
young people in 1950 regarded themselves as an “important person,” this soared
to 80% in the early 1990s. Research conducted by the psychologist Nathan
DeWall and his colleagues also found an increase in narcissistic expressions in
the lyrics of pop music.
Twenge and Campbell also contend that the economic crisis in 2008 is
attributable to the epidemic of narcissism, which encourages people to consume
beyond their means in order to look richer and more successful than they really
are. The credit offered by financial institutions fosters an atmosphere in which
the growth in plastic credit cards is second only to the growth in plastic surgery
(up 300% since the mid-1970s). This reinforces Grossman’s findings, which
suggest that the increase in disposable income is responsible for the growth of
individualism in American society.
In a lecture in 2014, Keith Campbell expressed hope that this trend and its
associated cultural values can be reversed without suffering another economic
collapse. In the meantime, he suggests that we stop focusing on boosting a sense
of self-worth as a social objective, develop self-control and adopt a bit more
compassion in our personal and public lives.

NO FRIENDS AMONG SECRETS
Choosing the people to whom we surrender our asset
called privacy
Stand in place, spread out your arms and spin around. The imaginary circle your
fingers sketch in the air marks the distance from our bodies we prefer to keep for
ourselves. Strangers who invade this personal space in a way that threatens us, or
even people crowding next to us in an elevator, make us feel uncomfortable. The
boundaries we want others to respect (so we won’t feel uncomfortable or
threatened) are an important part of our privacy.
All living things have boundaries: the cells that comprise our body tissues are
wrapped in a membrane that demarcates what lies inside and outside of the cell,
just as the walls of our home define who lives inside. These boundaries
distinguish between what is exposed to the entire public and what is reserved
only for those situated within these borders. Privacy sets similar boundaries. It’s
not just the distance we want others to keep from us so that we can feel
comfortable. Privacy is also our right to distance ourselves from others in order
to contemplate various details that create our inner world, which is familiar only
to us.
There is no unequivocal definition of “privacy,” and the term is used in
various ways in law, politics, communications and philosophy. Aristotle was the
first to distinguish between the social space (polis) in which public discourse is
conducted and the private space (oikos) reserved for individuals and their
families. The famous researcher Margaret Mead added the anthropological
dimension, noting the way various cultures protect the individual’s privacy
through hiding, celibacy or preventing public access to secret ceremonies. Other
studies discovered that animals also need privacy.
Differences in body language among various cultures are also reflected in the
physical space they define as private. Italians often greet acquaintances on the
street with a hearty embrace and kisses on both cheeks, while the Japanese prefer
to bow to each other, without touching. Touching is considered a serious
violation of privacy in Japan. In South America, the distance between people
when conversing is very small, even if they aren’t well acquainted. In Asia,
physical proximity to others is very accepted, largely because of the high
population density in that part of the world. On the other hand, people in the

U.S. don’t feel comfortable standing too close to others, particularly when they
don’t know them.
But the tangible border of physical proximity is only one aspect of privacy.
The other and even more important aspect pertains to the imaginary border we
draw around ourselves, separating what others know about us from what is only
known to us. Privacy is also essential for the society’s very existence. A world
without privacy is an intolerable world, in which everything said between a
patient and physician, between an attorney and client, and between two friends
becomes public knowledge. In a world without privacy, no society could be
sustained.
All cultures value privacy, but are differentiated by the importance they assign
to it and the ways in which they protect it. Mixed couples from different cultures
may discover to their chagrin that one of them spoke freely with their friends
about the couple’s last quarrel, while the other views it as a private family
matter. Cultures that are far-removed from technology, for example, will adopt
different criteria for privacy than those that routinely reveal personal information
through social media. Some consumers are willing to surrender their privacy in
order to receive commercial offers tailored to the information they agree to share
with suppliers of products and services on the Internet. And these suppliers
monitor our preferences, buying habits and areas of interest, as reflected in the
Internet searches we conduct. Powerful computers process all of the tracks we
unwittingly leave in order to precisely describe, in horrifying detail, our most
private predilections—which color we prefer, which celebrities we find
appealing or appalling, and so on.
We use privacy as a means of controlling people’s access to us and to our
secrets. When we decide who is allowed to enter our private space, we are also
expressing our social choices. The temptation to reveal our secrets to others
stems from the hope that those with whom we share confidential information
will prove worthy of our trust and will reciprocate with a similar gesture of trust
and even become our friends. On the other side of this equation, however, we
know that if we share all of our secrets, we’re liable to lose something important:
the sense that there are things that belong only to us and that no one else has the
right to come near them. This includes special memories that are ours alone,
things we like that are considered a bit weird, and things we really hate but are
embarrassed to admit it.
Privacy prevents those around us from gaining free access to us—physically
or via information about us. It is very tempting to waive privacy because it might
make us the center of attention, at least temporarily. At the other extreme,
absolute privacy is achieved when no other person can step into our personal

space, and we can maintain such absolute privacy simply by isolating ourselves
in a remote location. However, then we would need to bid farewell to all of our
friends and lose the chance of showing them our virtues.
Indeed, privacy and intimacy are linked. Without relinquishing privacy,
intimacy is impossible. Privacy means controlling information about our
personal preferences, but is also essential in enabling us to develop into adults
with the social and moral ability to form relationships of trust, respect and love.
The control of personal information enables us to release this information at
various levels of openness, in accordance with the closeness we develop with
others, or wish to develop. And since waiving privacy is a cornerstone in our
ability to exercise choice and develop feelings of love and friendship, it’s clear
why the danger to privacy is a threat to our very existence as human beings—a
quintessentially social species. Privacy is what enables us to shape our
relationships with others and with ourselves.
Without intimacy with others, something will always be lacking in our lives.
After all, who are we and what do our experiences amount to if we don’t have
loved ones with whom to share them? The friends we allow into our private
world are also the friends with whom we dare to express ourselves freely,
without feeling embarrassed. But it’s important for us to remember that we don’t
only aspire to be loved by others, but also to love and care for them. And the
resource we have for fostering the affinity required for this is our privacy—
control in sharing our personal information with those we’ve chosen to share it
with. Anyone who takes this information without our permission violates what is
most important to us.
How strange. It seems that the only reason we need privacy is to enable us to
willingly relinquish it. In surrendering our privacy, we award a great gift to those
we hold dear—family members, friends and others deemed worthy of our trust.
And lo and behold, when we surrender a piece of our privacy and expose our
secrets to others, we are also ready to shift the physical boundaries we set around
us and allow others to come closer without this upsetting us.

LONG LIVE THE SMALL DIFFERENCE
Are we more similar or different?
The famous mentalist suddenly emerged out of nowhere on the stage, shrouded
in smoke. His body was wrapped in a long crimson robe, with a pair of white
silk shoes peeking out from it and a yellow turban on his head fastened with a
shiny emerald. Two thick eyebrows, arched diagonally, stood out on his white-
powdered face. Powerful spotlights followed every movement of his nimble
body, as the crowd cheered nonstop. An unseen announcer invited the first
volunteer to the stage for the evening’s opening act, which promised to be
entertainment at its best.
Quick as a flash, a young man appeared on the stage. His obvious
bewilderment suggested that he had no prior acquaintance with the performer.
The volunteer was asked to sketch a drawing on a piece of paper and then fold it
well, insert it into an envelope and place it in the delicate hands of the mentalist.
After the volunteer completed this task, the mind reader concentrated on the
folded piece of paper, pondered for a moment and finally declared with
confidence: “You drew a house.” The look of surprise on the volunteer’s face
said it all, along with the victorious smile of the celebrity mentalist.
Were supernatural forces at play here? Or perhaps just a profound
understanding of human nature? Indeed, it turns out that when asked to draw a
picture, nearly all of us will sketch one of five basic shapes—a house, a tree, a
car, a flower or a stick figure of a person. All the “mind reader” needed was to
quickly identify the volunteer’s hand movements and the time it took to
complete the task. His experience would already tell him which of the five basic
shapes is on the page.
How embarrassing. Are we so alike, to the point of becoming a statistic for
entertainers and many others, including advertisers who try to target our tastes
without knowing us at all?
Are we all similar under the surface?
Darwin claimed that great diversity exists within a particular species: no two
individuals are identical in their anatomical, physical or behavioral makeup. The
members of a particular species differ in the structure of their cells, their fighting
abilities and social skills—traits that are thought to be genetic and make our
offspring more similar to us than to others.

Every living creature has a similar genetic code, including mice and men.
However, with the exception of identical twins, each person has unique DNA
that determines his or her eye color, blood type and countless other physical and
biological characteristics. The closer two people are to each other, the smaller
the “spelling” differences in their genetic code. But even when the difference in
genetic code is small, dissimilarities will appear depending on which parts of the
genetic makeup are active or dormant. We, like chimpanzees, have genes that are
responsible for growing a tailbone. Luckily, they are dormant in our case.
And not only are we born different, we continually change through our
experiences, which modify our brains by creating new neurons and forging new
connections between existing neurons. Studies indicate that our brains may
change within the course of just a single day. The unique combination of our
genes and experiences is what physically differentiates us. Dr. Eran Elinav and
Prof. Eran Segal from the Weizmann Institute assert that even the composition of
intestinal bacteria varies from one person to the next. And this diversity is so
wide, they contend, that certain types of food that help some of us lose weight
may have precisely the opposite effect on others. Similarly, the effectiveness of
medication varies in accordance with our microbiome, or “second genome”—the
unique composition of intestinal bacteria in each one of us.
Sir Francis Galton, an exceptionally prolific researcher (who also happened to
be Charles Darwin’s half-cousin), initiated a very ambitious project in the 1870s
designed to categorize people through composite portraits of individuals in the
same sitting position, using the same photographic plate. He believed he was
able to identify two portrait types that represent the faces of all of the criminals
in the Victorian era. Galton was a leading proponent of the scientific fashion of
the late 19th century, phrenology, which believed that the shape of the human
skull and facial features express an individual’s personality.
Galton later admitted that he was mistaken in advocating phrenology, not
before offering Scotland Yard an identification method based on fingerprints. He
proved that we’re sufficiently different from one another to make us
distinguishable by our fingerprints, but not similar enough to justify crime-
prevention measures based on our facial features. Galton and others failed in
their attempt to detect physical similarity among people who are not members of
the same family, and the widespread use of various methods of biological
identification today reflects the irrefutable fact that we are indeed physically
different.
But the important question is not how similar or different we are from a
physical perspective. After all, the quality of our lives is shaped primarily by the
emotional and moral dimensions of our personality: the emotional dimension

affects both our highs and lows, great joy and bitter disappointment, while the
moral dimension is very important in generating a sense of social belonging,
which is a vital human need. The important question, therefore, is how similar
are we in these dimensions?
A tempting point of departure could be to view human beings in
anthropological terms, comparing cultures and different peoples throughout
history. A study of ancient cultures, for example, indicates that while their
religions are ostensibly different, most share a similar basic pattern: a temple to
gods and goddesses representing natural phenomena or related to basic
existential human experiences. Diversity among religions came later in the
development of human culture. Anyone interested in finding a contemporary
bridge to these patterns of the past is welcome to look under “collective
unconscious”—a term coined by the psychologist Carl Jung to describe the
unconscious layer shared by all humankind.
Kindred spirits
Research published in 2010 confirms the premise that people are similar in their
emotions, regardless of their cultural background. The study, conducted at
University College London, compared people from England to members of the
Himba tribe from Namibia. The 20,000 members of the tribe live a completely
traditional life in geographic isolation, without electricity, running water or
formal education. Sophie Scott, who led the research, tried to answer the
question of whether the various sounds associated with emotions such as anger,
happiness, fear, sadness, disgust and surprise are shared by people in different
cultures.
The study was based on telling both groups the same story, invoking a
particular emotion. For example, sadness was expressed through a story about
the death of a family member. Then the researchers played sounds of weeping
and laughter and asked the participants to indicate which of the two sounds best
reflects their feelings after hearing the story. The Namibians’ sounds of weeping
and laughter were played to the English participants, and vice versa—the Himba
heard sounds of emotion from England. It turned out that both groups were able
to identify the emotions behind the sounds.
The findings also indicated that emotions such as surprise, amusement, anger
and fear are common to everyone. We share most of our genetic code with our
fellow human beings, and we all have complex systems of communication we
use to convey thoughts, intentions and feelings to those around us. Body gestures

and facial expressions often communicate the message without the use of words.
But these can vary from one culture to another, and what one culture interprets
as a sign of affection might be considered sexual harassment in another.
These findings support earlier studies indicating that facial expressions
represent basic emotions common to many cultures. According to the
researchers, despite differences in facial structure, all of us are equipped with the
muscles required to create facial expressions that convey universally identifiable
emotions. The researchers found that sounds of laughter represent the broadest
cross-cultural common denominator, and they attribute this to the sounds we
make, even as infants, in response to tickling. These sounds have an
evolutionary role in the joyful communication between an infant and mother,
which has also been identified in the sounds of chimpanzees and other primates.
The research findings are consistent with Darwin’s contention that emotions
evolved as part of human nature.
In a lecture at the annual convention of the Association for Psychological
Science (APS) in 2014, Jessica Tracy noted that body gestures and facial
expressions signaling pride also transcend culture, race and gender. She argued
that demonstrating pride to others is a basic human mechanism for promoting
social status. Signs of pride—a smile, raising hands in the air and expanding the
chest—are common to winners of sports competitions from different nations and
cultures. Tracy examined the body gestures and facial expressions of the winners
in judo matches at the 2004 Athens Olympics and found great similarity among
them.
The researchers broke new ground in studying the question of human
similarity by successfully identifying signs of pride among blind participants at
the Paralympics who had never seen such gestures. Signs of pride are so
universal, the researchers claim, that 89% of the world’s population would
recognize them anywhere on earth.
The narcissism of the small differences
In 2010, I visited the last day of the annual summer exhibition of the Royal
Academy in London in order to conduct a small experiment of my own to
examine whether we are similar or different when it comes to purchasing art,
which is essentially an emotional subject. Part of the exhibition that was
particularly suitable for my experiment was a collection of engravings and
prints. There, in contrast to the oil paintings and sculptures, the works were
relatively inexpensive and a large number of copies were available for purchase.
I defined the characteristics of the various prints in terms that could be
compared—such as the price, size, colorfulness, subject matter, level of

abstraction, artist’s fame and even the height at which the work was hung. Next,
I examined whether I could come up with a formula that would compare the
prints by these characteristics to predict how well they would sell. (A small red
dot below the work marked each sale.) I hoped to discover a common
denominator in people’s tastes, and the sample I chose included no fewer than 45
works of art.
To illustrate, here is some data I fed into the amateurish model I developed:
the highest demand was for two works by the well-known and provocative
British artist Tracey Emin. All 200 copies of her first work—an etching of a
nude female, with three male sex organs at her feet—sold quickly. The first penis
is leaping over a high jump bar, while the other two wait their turn. The drawing
includes the words: “No idea why they can jump so high” (incidentally, a great
title for a book). The second work was a small etching of a blurry cat in a bluish
tint, pale and charmless. The price was affordable—£280— and again, the artist
was Tracey Emin. All 300 copies were sold. Did people purchase the blurry
etching of the cat because of the artist’s name, or did the placement of the
innocent animal next to the lewd drawing afford buyers an excellent opportunity
to make a clear moral statement?
It was easy to notice, even without knowing anything about the “golden
rectangle” (the particular ratio of length and width in a geometric form that
pleases the human eye), that the visitors at the exhibition consistently preferred
horizontal works, where the length is greater than the height. It also became
clear that the British are crazy about animals. Rabbits, dogs and cats in particular
are bestsellers. And lions, of course, which my formula calculated as big cats.
Hedgehogs don’t make the grade—not a single copy of the three works featuring
the spiny creatures was sold. It also turned out that movement in a picture
doesn’t help: a herd of deer falling from the sky like rain sold only four of 15
copies offered for sale.
Several clear insights about the purchasers’ inclinations already emerged when
applying the first version of the formula I developed. The first consideration was
the price. Purchasing a work of art at a low price is still the central motive in the
decisions of visitors at the summer exhibition, more than any artistic value. It
was also evident that works whose subject is a bit blurred, but not really abstract,
are popular among those who see in them a faithful reflection of their own lives,
immersed in frustrating uncertainty. Humoristic works are also in high demand.
A work showing the silhouettes of three women standing in a line to the
bathroom, with an identical silhouette sketched above the door, was very
successful—all 60 copies were sold. Incidentally, size matters. Small works of
art were bestsellers. The reason is simple: It’s easy to find a place for them on

the wall, and they generally are less expensive.
After working for several hours and classifying the data statistically, I was
able to create a multi-variable regression model (thanks to Francis Galton, who
developed statistical correlation) that successfully estimated the sales volume of
many of the prints at the exhibition. The exercise I conducted on a sizable
sample of artwork and visitors indicated that the audience’s tastes could be
statistically quantified and suggested that people share a common set of tastes.
But when I told a friend who owns a gallery of contemporary art about my
findings, she smiled and scoffed: “It doesn’t take a scientist to know that people
prefer kitsch, especially if it’s cheap.”
Perhaps there is no need for research to confirm that we all have a warm place
in our hearts for inexpensive kitsch, but this assumption becomes less self-
evident when we look at other lists expressing cultural taste. Every literary
publisher with a bit of integrity will admit that he or she cannot predict in
advance which book will make the list of bestsellers. Even Hollywood studios,
which have made an art of appealing to the broadest common denominator, are
unable to foresee failures that cost tens and hundreds of millions of dollars, when
that elusive common denominator refuses to buy tickets.
We know today that the sense of security we feel when we identify a familiar
pattern is one of the important benefits that evolutionary psychology offers. This
dates back to ancient times when early recognition of a pattern representing a
predator’s threat could mean the difference between life and death. In such
circumstances, rapid classification of human faces as a general category—
without distinguishing among them—conserves cognitive resources required in
the critical process of assessing potential threats. Thus, for example, we tend to
see great similarity in people from all parts of the world. The alternative—
devoting specific attention to each individual—entails a substantial investment
of emotional energy that would come at the expense of functions we deem more
important.
Freud coined the expression “narcissism of small differences” to define our
ability to detect minor differences in the behavior of others—for example, in
their dress or mannerisms. “As social animals,” the writer and philosopher
Stephan Cave notes in an amusing story published on the digital magazine Aeon,
“we continually try to read the moods and intentions of others” from the smallest
signals and differences in their behavior. These differences are defined as
“narcissistic” because we ultimately view them in reference to ourselves, thus
helping us form our own identity.
The phenomenon of the narcissism of small differences is also familiar in
rivalries between neighboring peoples, when small differences are amplified in

order to deepen the distinction between them. For example, there is no difference
between Turkish coffee and Greek coffee, except in the minds of Turks and
Greeks. In ancient days, when rivalries between tribes reflected a real threat to
survival, the ability to detect small differences was very valuable. Today,
however, this bias merely blurs the great similarity between these peoples,
which, if appropriately recognized, could significantly contribute to improving
the relations between them.
A question of resolution
Even if each of us is singular and unique by nature, social uniqueness remains an
unfulfilled wish for most of us. We are similar in aspiring to attain a social status
that recognizes our originality and some of us are willing to go a long way to
achieve this goal. Advertisements for many consumer products, particularly the
most prestigious ones, aim to reinforce our sense of being special. This, of
course, creates a paradox for advertisers: buying their product makes us
exceptional, they claim, but if their message succeeds, we’ll be just one of many
who have purchased it.
Our shared belief that we’re special is what makes us so similar. Is this just a
question of resolution? Are all of us different up close, yet from a sufficient
distance (and marketing perspective) quite similar or at least classifiable into
distinct groups?
Perhaps, despite our primordial longing for order, nature has a logic of its
own: if electromagnetic phenomena can sometimes act as energy and sometimes
as matter, why can’t human beings? One moment they applaud a successful
musical performance with the spontaneity of individuals, and the next moment
they’re already adapting themselves to the rising tempo of applause from the
entire audience.
I believe that we’re similar in our basic aspirations for love, recognition and
social belonging, but different in the way we seek to fulfil them. We all have free
choice in choosing how to contend with the challenges we face, and thus bear
responsibility that we cannot share with anyone else. Here, in our choices, lies
our true uniqueness.

AN EMBARRASSMENT OF RICHES
Signs of social discomfort might carry a positive message
of authenticity
“There’s no limit to chutzpah,” I wrote, and clicked “Send.” The unflattering
description referred to a well-known businessman who had asked for additional
payment for a service that, in my opinion, was already included in the agreement
between us. I sent the e-mail to my lawyer, but as chance would have it, I didn’t
notice that I had clicked “Reply All”—among them the person I had just accused
of unbounded chutzpah. I learned of the mistake from a phone call that followed
almost immediately. The speaker asked, “What did you mean when you wrote
‘There’s no limit to chutzpah’?” It wasn’t my lawyer.
The origin of embarrassment, such as that felt in the above case, is behavior
that inadvertently violates a social rule and leads to a negative sense of self.
Most of us are embarrassed when we attract inadvertent attention. That happens
when we lose control of our bodies, for example (slip, spill a drink or fart in
public), or forget someone’s name, or when a secret thought is revealed. Even
compliments that lead to exaggerated attention in the positive sense are likely to
cause embarrassment; they undermine modesty, a familiar social norm.
Essential components of embarrassment therefore include an act that violates
social consensus, the presence of other people and the feeling that we have made
a bad impression. Those who are easily embarrassed enjoy—and some say suffer
from—a relatively high degree of self-awareness, and contrary to what is
commonly believed, they are not necessarily shy or lacking in social skills.
Expressions of embarrassment include a forced smile (where only the corners of
the lips curve upward and which does not extend to the eyes), an averted or
downward gaze, a nervous laugh, touching your face and occasionally blushing
as well.
Sociologist Erving Goffman, in the mid-20th century, was the first to discuss
the importance of embarrassment in promoting societal functioning. He claimed
that showing embarrassment indicates a person’s desire to maintain the norms
that underlie every society. In this way, people declare that they are aware of
some sort of unpleasant conduct, regret it and are now committed to observing
social order in the future. In Goffman’s world, people’s lives are so dependent on
the way others see them that they will do anything in their power so as not to

distort or violate a social expectation. Goffman, who sees the world as a stage
for the individual’s performance, believed that one wrong note—an incident
involving embarrassment—could spoil the entire show. If the root of the word
“embarrassment” means “an obstacle” in several Romance languages, for him it
is an obstacle to successful face-to-face social interaction.
Other sociologists thought that the central importance assigned by Goffman to
avoiding embarrassment was—how shall we put it—somewhat embarrassing,
and tried to discuss additional aspects of the phenomenon. The main body of
these studies confirms the role of embarrassment in maintaining social order, but
also explains that those who exhibit signs of embarrassment are rewarded
socially. They are well-liked and are seen as worthy of forgiveness, and as more
trustworthy than those who don’t show such signs.
The most recent harvest of research on this subject comes from the University
of California, Berkeley. Matthew Feinberg, a doctoral student in psychology, and
two of his colleagues believe that embarrassment is a sign of pro-social behavior
—concern for the welfare of others and a desire to avoid harming them. That is,
no longer a kind of nonverbal apology or conciliatory gesture aimed at
rehabilitating one’s social status, but a type of genuine character reference. In a
study published in the January 2012 issue of Journal of Personality and Social
Psychology, they claim that people regard expressions of embarrassment as
evidence of social behavior, concern for maintaining social norms and a
commitment to pro-social relationships—and accordingly they react to
embarrassed people by expressing trust and a desire to become closer to them.
The researchers conducted a series of five studies. In one of them, the subjects
were asked to reconstruct an embarrassing incident in which they were involved,
in front of a video camera. A separate questionnaire attempted to examine their
social values, and asked them how they would distribute certain things (such as
raffle tickets) among themselves and others. Those who told a particularly
embarrassing story and expressed greater embarrassment in their facial
expressions were also seen to be more pro-social in their preferences, and gave
away more to those playing alongside them in a particular economic game.
The researchers wanted to examine whether other people see victims of
embarrassment in the same way. They presented to a new group of subjects four
of the images from the embarrassment video of the participants in the first
experiment, and asked them to assess the pro-social behavior of the
interviewees. The spectators attributed greater pro-sociality to those who
demonstrated greater embarrassment in their stories and in their expressions in
front of the camera. Pro-social behavior in this connection was highly related to
traits such as generosity, cooperation, integrity, trustworthiness and loyalty to

rules of social behavior. The spectator subjects also expressed a preference for
forming social relationships with those who showed greater embarrassment.
It appears that blushing—the involuntary dilation of blood vessels in the face
and neck—holds a special place of honor when it comes to expressions of
embarrassment. In this respect, the good news comes from Holland. Researchers
led by psychologist Corine Dijk found that blushing also plays an important
social function. Those who violated a social norm or were involved in some
other embarrassing mishap were likely to be forgiven by the others if they
blushed and exhibited the trait that Charles Darwin described as “the most
peculiar and the most human of all expressions.”
While words or behavior are not always reliable evidence of a person’s
feelings, hundreds of thousands of years of evolution have taught human beings
to rely on involuntary expressions as successful predictors of future behavior.
Blushing, like crying, is an uncontrollable human trait and therefore constitutes a
reliable psychological sign of conciliation and social goodwill.
The Dutch researchers attached facial photographs of women to invented
stories of “social transgressions” (such as missing a funeral because of a party,
driving away after a car crash). The subjects were asked to assess and rate the
women according to a variety of criteria, such as their overall impression of this
person, how sympathetic they found her, how trustworthy and so on. The
researchers also used the computer to give some of the women in the photos an
obvious blush. These women were seen more favorably by those rating them,
and were considered more trustworthy. The researchers concluded that “blushing
is a helpful bodily signal with face-saving properties. It seems therefore unwise
to hide the blush or to try not to blush in these types of contexts.” If we believe
the researchers, embarrassment and blushing are a kind of seal of quality that
attests to the personality of someone to whom valuable emotional and material
assets can be entrusted. Which is somewhat embarrassing, if you think about it.

TRUST GAMES
The self-reinforcing circle of trust and trustworthiness
For nearly 40 years, I’ve been conducting research whose results are very
important to me. I’m trying to determine whether placing trust in others is
worthwhile, or more precisely: in which circumstances, if any, should we take a
material or emotional risk in our relations with a person we don’t know?
Behavioral scientists have their own methods of addressing this intriguing
question, but I stubbornly contend that life experience in general, and business
experience in particular, can be no less enlightening. After all, what are the
human encounters of over four decades if not a diverse sample of participants
that any researcher would consider representative?
During those long years, I gave and received, bought and sold, but did not
reach a definite conclusion. People who didn’t owe me a thing treated me with
human warmth usually reserved only for family members, while close friends
and others with whom I was especially generous failed to even acknowledge this
with simple words of appreciation. I was still at the point where I started.
Clearly, I could trust certain people I knew well, but prior familiarity is
inconsistent with the simplest requirements of scientific research: the
participants must be completely foreign to the researcher and know nothing
about the subject of inquiry.
As someone who chose as a young man to forgo the academic lab in favor of a
business career, I had already come to terms with the fact that I would probably
never find a reliable answer to this important question concerning trust. Until
one day, nearly 20 years ago, something happened. My wife had to renege on
going to the theater with me that evening and I was left with an extra ticket.
Since the show we wanted to see was in high demand and we had reserved very
good seats, I assumed I could easily sell the ticket for the performance that
evening. I came to the ticket office about 20 minutes before show time and
discovered that life is more complicated than I thought.
If you try to sell a ticket on the evening of a performance, you’re also likely to
find that the box office almost always has unsold tickets. And even if the seats
are not nearly as good, they will satisfy the needs of most theatergoers. That
evening, I was unable to sell the extra ticket and I wondered if I could have done
something to promote the sale.

The theater lobby as a research lab
Besides the slight disappointment, I suddenly realized that this interesting
situation, if repeatable, could perhaps provide an answer to my question. A
number of similar opportunities arose over the years, usually due to business
trips, and a familiar routine developed: standing near the box office, I would
offer the ticket at a discounted but non-negotiable price—a “take it or leave it”
offer. I didn’t budge from the price even if a potential buyer offered to pay just
slightly less than my asking price. In those cases, I remained with the ticket in
hand and the potential buyers were left to hope that the box office still had less
expensive tickets, even if the seats were not so great.
I varied the discount I offered from time to time and meticulously documented
the results as befits someone aspiring to conduct a small research project of his
own. The results were quite disappointing, and the average refunds I was able to
collect didn’t cover even half the cost of the tickets I offered to sell.
And suddenly, a few months ago, I saw the light. It was a popular dance
performance, and I found myself facing the familiar box-office salesperson, who
smiled at me compassionately. A young woman burst in from the cold outside.
While still removing her wool hat and unbuttoning her heavy coat, she quickly
surveyed the area near the box office and noticed the ticket I was offering in my
outstretched hand. “How much?” she asked. “Whatever is good for you,” I
replied. After her initial surprise, she looked for a moment at the ticket, pulled
out her wallet and paid the full price listed. It was clear that I had hit upon
something, but what exactly was it that made this instance so different?
Behavioral scientists use games to try to answer the question I posed,
analyzing the game results in their laboratory. The best known is the Ultimatum
Game. Like the situation I examined, it’s based on a “one-time deal” between
two players who are foreign to each other and cannot establish a “reputation”
based on their pattern of behavior over a series of transactions.
Even if a math exercise starting with the words “Two trains leave a station at
the same time” makes you queasy, I recommend reading about the two games
described below. They offer a relatively simple exercise in thinking, together
with an important insight.
Here is a general summary of the Ultimatum Game, two players who don’t
know each other compete in a coin toss. The winner (the proposer) receives
$100, for example, and must decide how much of this sum, if any, to share with
the other player. If the losing player (the responder) accepts the offer, the
proposer will keep the $100 minus the sum he or she chose to share, which is
handed over to the second player to keep. If the responder chooses to reject the
offer, neither player will receive a cent. Both participants are aware of the rules

of the game.
If you think about the nature of my brief encounters with culture-lovers
hoping to obtain a ticket on the night of a performance, you’ll realize that the
primary human trait my personal experiment tried to examine was trust—in this
case, between strangers. The ticket in my hand was equivalent to “winning”
$100 and the discount I offered on the original price was the sum I was willing
to relinquish in order to sell the ticket. If the potential buyer chose to reject the
offer, I was left with the unsold ticket, and the potential buyer had to go to the
ticket office to buy another ticket or miss the performance if it was sold out.
When I offered the ticket at a discount, I took the economic and emotional “risk”
that the discount would not satisfy the buyer, my offer would be rejected and I’d
be left with the lonely ticket in my hand. This risk is the very heart of the
definition of trust.
Hundreds of studies on trust based on the Ultimatum Game indicate that the
sum offered depends on the gender of the player making the offer, the cultural
background of both players, their education, the testosterone level in their blood
and other variables. (Surprisingly, the size of the sum is not one of these
variables.) The salient factor, however, is the general level of trust characterizing
the participants. Most participants in these experiments offer 40–50% of their
winnings. Half of the responders in the game who receive offers of less than
20% of the winnings scornfully reject the sum.
The Ultimatum Game was initially developed in 1982 to demonstrate that
human behavior is not always rational. Aversion to risk, a subjective perception
of the concept of “a fair offer” and a sense of discrimination are a few of the
human traits that may stand between a person and what economists consider
rational economic behavior. From a purely economic perspective, the winner
should offer as little as possible, and the loser should accept practically any sum
offered.
The Ultimatum Game is still the most common tool used in research on
interpersonal trust. It’s based on the assumption that the level of trust each of us
is endowed with, and even more, our level of trustworthiness, are fixed traits that
seldom change according to the circumstances. The game has many applications,
but it cannot explain the moment of grace that occurred on the floor of my mini-
laboratory of life in the lobby of the concert hall, when the ticket in my hand was
purchased at full price. For this, another game is required: the Trust Game.
The Trust Game also pairs two participants who have no prior acquaintance in
a one-time encounter. The winner (Player A) of a coin toss receives $100 and
must decide how much, if any, to share with the other player (Player B). The
experimenter promises to triple this amount (at his own expense), so that Player

B will actually receive three times the sum that Player A is willing to relinquish.
In the next and final stage of the game, Player B must decide how much, if any,
of the tripled sum he or she is willing to send back to Player A. Both players are
fully aware of the rules of the game.
Let’s say you’re Player A and have just won $100. How much will you offer
Player B? If you give $50 to Player B, he or she will receive $150. (Remember,
the experimenter triples the sum offered by the winning player.) And if Player B
decides to split this sum with you evenly, you’ll end up with $125 ($50 + $75),
which is $25 more than you started with. If you give $33 to Player B, he or she
will receive $99. And if Player B decides to send $30 back to you (and indeed,
30% is the average returned in this game), the trust you placed in Player B will
not pay off. If you decide to give the entire $100, an expression of complete
trust, you enlarge the pie to the maximum of $300. And if Player B reciprocates
your expression of trust and splits the sum evenly, you’ll each end up with $150.
Those who decide to share only part of the winnings discover that the trust
they place in the other player pays only marginal dividends, if at all. But the 25%
of the winners who choose to express complete trust and transfer the entire sum
discover what I discovered that evening at the theater, when I let the buyer set
the price for purchasing the ticket—trust pays when given fully. It turns out that
your level of trustworthiness is not a fixed trait. Rather, it varies according to the
level of trust that others place in you. Trust boosts trustworthiness, and as the
level of trust increases, it further encourages trustworthy behavior.
Trust as an elixir for longevity
An article published in the Psychology Science journal in 2015 offers several
insights from the latest studies in the field, including evidence that trust is good
for your health: individuals characterized by a high level of trust experience
greater emotional and physical well-being. One of the studies goes even further,
asserting that people who trust others live longer than their suspicious
counterparts. People who are trusting are also less reluctant to expose their
vulnerability, as they expect a positive response from others or believe in the
sincerity of their intentions.
Paul Van Lange, author of the article in Psychology Science, drew from recent
studies to investigate the impact of genetic and cultural factors on the
development of trust. Today, we know that genetics is a major factor influencing
schizophrenia (80%), extraversion (at least 40%) and even divorce or political
inclinations (about 25%). So it is only natural to investigate whether there is a
genetic component affecting a person’s level of trust. But according to the
findings, the correlations between trust and heredity are quite low—only 10–

20%. (The research examined the level of trust between identical twins and
compared this to the trust between fraternal twins.)
We are in an era of declining trust, Van Lange believes. He argues that in
addition to the conventional view that childhood experiences play a central role
in our development, later experiences have no less of an impact when it comes to
trust. A burglary in our home, mistreatment by authorities or unanticipated
unemployment are examples of powerful experiences that undermine trust.
The post-childhood experiences that affect our level of trust include our
consumption of information from the media and social networks, and the nature
of this information, which is sometimes critical and negative, and at times even
condescending. Studies on the new era of negativity engulfing us cite a number
of phenomena that erode our trust in each other, including the sense of
superiority we feel toward others on matters involving integrity and pro-social
behavior, and our growing inclination to ascribe selfish motives to the actions of
others.
Nonetheless, Van Lange concludes the article on an optimistic note. Despite
the trust-eroding filters of the media and social networks through which we
examine much of our reality today, it is still true that “a healthy dose of trust
does yield good outcomes in social life,” primarily in interactions with others we
don’t know well. We fine-tune the level of trust we need through our contacts
with family members and close friends and, to a lesser extent, through our
contacts with colleagues and strangers.
A high wall keeps us from implementing this insight about the importance of
trust, as we discover that the level of trust in the world has declined in recent
decades. The common index for measuring the level of trust in a society is the
percentage of respondents who concur that “most people are trustworthy” versus
those who believe that “you can never be too cautious with people.” (Other
indexes trace the level of trust that citizens place in various social institutions
like the government, the police, the media and the courts.) The level of trust in
some Western countries has plummeted during the past 70 years by over 50%.
This phenomenon is widely attributed to the invasiveness of the media and
primarily to the major impact of television.
Even those who regarded Churchill as a revered leader in the 1940s may have
changed their minds had they watched him every evening on the television,
sloppily dressed (and rarely out of bed before noon), often drunk, and racist in
many of his remarks. According to one source, even the appreciation for his
important speeches would change were it known that a talented actor was
reading them in his place. And if that’s not enough, try to imagine a panel of
experts on television analyzing Churchill’s military decisions.

A wallet in Japan
But the problem is not limited to the image of a particular leader. Since the level
of trust in a society is seen as a reliable indicator of its social capital, the steep
decline in the level of trust in many countries affects a range of subjects, headed
by economic growth.
The wide diversity in trust levels in different countries provides some hope
that all is not lost. These differences perhaps suggest that cultural characteristics
can stop the erosion of this important social asset. In one study, researchers
“lost” their wallets on the sidewalks of different world capital cities to see how
likely it was for passers-by to return the wallets to their owners.
The researchers found that you’re more likely to retrieve a lost wallet in
Tokyo, Helsinki or Oslo, than in Rio de Janeiro, Istanbul or Phnom Penh. In a
similar study, researchers investigated how long it would take unlocked bicycles
to disappear in different cities in the world. Edward C. Banfield, who studied the
enormous economic gap between southern and northern Italy in the late 1950s,
attributed this gap to the huge disparity in the respective levels of trust prevailing
in the two parts of Italy. Social trust ultimately reflects our ability to anticipate
the behavior of strangers with relative certainty. When the other person is also a
potential business partner, trust can encourage commerce or a joint investment,
and thus becomes an elusive factor behind many stories of economic success at
the national level.
Since the first Ultimatum Game and “lost” wallets study, research has come a
long way in attempting to assess the social and personal impact of trust and
identify the factors affecting the level of trust of individuals and entire societies.
We have learned, for example, that democratic countries with a Protestant
majority enjoy a higher level of trust and that a centralized and bureaucratic
regime is detrimental to trust. Researchers found that the Protestant countries
ranked at the bottom of the scale of trust are those where the regime is
centralized, bureaucratic and arbitrary.
When comparing Scandinavian states to Turkey, Indonesia and Brazil, we find
that the level of trust in Scandinavia is six times higher than the level in the
second group. The researchers attribute the gaping difference in trust levels to
cultural characteristics, primarily social equality and the level of corruption (and
transparency) in the different countries. To illustrate the cultural impact on trust,
the researchers note that even first-generation immigrants from countries with a
low level of trust adopt the norms of higher trust prevailing in their new country.
And while we’re talking about immigration, the more homogenous a state, the

higher the level of trust. The survival needs of our prehistoric ancestors endowed
us with the psychological process enabling us to identify “others” who are
similar to us and to trust them. And in homogeneous societies, there are more
people similar to us. Another finding is that countries that discriminate against
their minority populations harm the level of trust of their entire citizenry and not
only that of the discriminated minorities.
Even if a bit of suspiciousness is essential to prevent those with economic or
political interests from exploiting us, we need a proper dosage of trust to avoid
collapsing under the weight of doubting the motives or trustworthiness of others.
Free from disquieting suspicions, people who trust others are happier, healthier
and even, as noted, live longer. Trust is the foundation for our ability to
cooperate with others in the community—for our own well-being and for the
benefit of the entire community. Therefore, it is not surprising that social
punishment is more effective in societies with a high level of trust.
And if you agree that we should learn from those with experience, it’s worth
noting that studies have found that a person’s level of trust does not decline with
age, and even increases a bit. Our life experience teaches us that trust enhances
our ability to constructively solve problems, improves the stability of our
relationships and enables us to sometimes give others the benefit of the doubt
when they make an accidental mistake.
But most importantly, as I discovered that evening in the improvised
laboratory in the theater lobby, if we take a risk and place full trust in a person,
the latter may very well surprise us and repay us in kind. Sometimes we’ll
receive the full price of a ticket and nothing more, but other times perhaps we’ll
gain a worthy business partner or even someone with whom we can share a full
life of cooperation, health and longevity.

THE BEGINNING OF A BEAUTIFUL RIVALRY
Rivalry is as old as humanity – how our rivals own a deep
part of ourselves that spurs us to our greatest
achievements
On May 25, 1832, John Constable was busy adding the final touches to his
masterpiece, The Opening of Waterloo Bridge. One of England’s greatest 19th-
century landscape artists, he had been working on the painting for more than ten
years and was finally set to reveal it to the world the next day, at the opening of
the Royal Academy of Arts’ 64th annual exhibition. Next to his piece hung
Helvoetsluys by J. M. W. Turner, an artistic genius in his own right. Watching
Constable’s last-minute efforts, Turner decided to add an extra brushstroke of his
own: a red buoy floating on the water.
That single daub of red paint against a background of grey sky and sea was so
arresting that visitors couldn’t take their eyes off it, certainly not to look at
Constable’s painting. It was yet another landmark in the bitter rivalry between
the two artists. A year earlier, Constable had used his position in an exhibition
committee to have a Turner painting taken down and hung in a side room,
replacing it with a painting of his own.
Turner and Constable are not alone in the pantheon of epic rivalries between
creative giants. Thomas Edison and Nikola Tesla both invented electrical
systems in the 1880s. Steve Jobs and Bill Gates went head-to-head as pioneers of
the computer age. Sherlock Holmes and Professor Moriarty, and the Montagues
and the Capulets are other examples of famous adversaries, whether real or
imagined, in Western history. If you Google almost any famous figure along
with the word “rivalry,” you’ll find some interesting results.
Think of rivalry as a type of über competition driven by mutual obsession,
with the rivals propelling each other to spiraling achievement, and investing
more mental and emotional resources in each other than circumstances would
ever dictate on their own. In 2014, across two sets of studies involving
undergraduate students and runners, Gavin Kilduff, a psychologist at New York
University, found that rivals tend to be of the same age, gender and social status.
True rivals know each other and, indeed, often have long, enmeshed histories.
Rivals are, by definition, evenly matched—but the higher the level of their
attainment, the more they propel each other on.
Rivalry can be double-edged: it motivates not just heightened accomplishment

but, sometimes, unethical behavior such as lying, cheating or stealing. In a series
of studies, Kilduff found that those primed for rivalry were more open to
Machiavellian acts and more likely to exaggerate positive results in a cognitive
task. Rivalry could account for scandals and malfeasance at the highest levels of
industry, and might even explain some of the risky behavior behind the
economic collapses of the recent past.
The social drama of rivalry, with its hostility and aggression, masks a deeper
subconscious dynamic. We might think of our nemesis as the polar opposite of
ourselves, but as Kilduff’s research suggests, our rivals are much more like us
than we dare admit. While this might seem counter-intuitive, it follows that
rivalry can actually be good for us: acknowledging that our rivals share our most
essential traits, good and bad, can help us up our game and gain some of the
insight we need for greater success.
Orson Welles summed up this idea in his movie The Third Man (1949): “In
Italy, for 30 years under the Borgias, they had warfare, terror, murder and
bloodshed—but they produced Michelangelo, Leonardo da Vinci and the
Renaissance. In Switzerland, they had brotherly love—they had 500 years of
democracy and peace, and what did that produce? The cuckoo clock.” Although
this might seem cynical, art historians tend to agree: the birth of the High
Renaissance is attributed to the rivalry between two artists over who would
design the bronze doors of the Florence Baptistery. In 1401, the cloth importers’
guild declared a competition to design a set of doors for this building—one of
the oldest in Florence, where the poet Dante and members of the prominent
Medici family had been baptized. Lorenzo Ghiberti, aged 23, won the
commission, ousting his more established opponent, Filippo Brunelleschi.
Ghiberti’s victorious design ushered in a new style of art, more naturalistic and
with greater emphasis on perspective and idealization of the subject. While it
took him another 21 years to complete the assignment, the episode began a
competitive frenzy that became a trademark of the Renaissance.
In fact, the most important artistic achievements of the Renaissance occurred
in the small area between Rome, Florence and Venice, home to just a couple of
hundred thousand people at the time. One of the largest cathedral domes in the
Christian world, the Duomo in Florence; the realistic representation of the
human body; and linear perspective in painting all came into existence thanks to
the rivalry between Renaissance giants such as Brunelleschi (1377–1446), Da
Vinci (1452–1519), Michelangelo (1475–1564), and Raphael (1483–1520).
According to their contemporary, the art historian Giorgio Vasari, rivalry was
common among elite artists of the period. Renaissance Rome was home to any
skilled artist aspiring to work for the Vatican—the biggest and almost single

employer of the time. The natural intensity of competition in such a restricted
setting yielded works of art that still hang in the world’s elite museums. The
practice of exhibiting paintings by different artists side by side in order to
compare technique and style naturally heightened the pressure on each artist.
Raphael achieved new heights in his work when he designed ten tapestries,
commissioned by Pope Leo X, to hang in the Sistine Chapel under
Michelangelo’s divine ceiling. The results were applauded by all—all, that is,
except Michelangelo.
That should come as no surprise. The famous sculptor and painter was also
renowned for his temper. When the handsome young Raphael first arrived on the
Rome scene and was quickly commissioned by Pope Julius II, Michelangelo
labelled him a bitter rival and proceeded to repeatedly accuse him of plagiarism.
At one point, Michelangelo worked on his ceiling masterpiece behind a partition
in order to hide it from Raphael. The latter, no shrinking wallflower himself,
managed to arrange a view of it and later, in his fresco The School of Athens,
incorporated a seated figure taken straight from Michelangelo’s work. Thanks to
these machinations, the rivalry between the two giants became one of the most
famous in the annals of art.
It wasn’t until the establishment of science societies in the late 16th century
that major scientific rivalries reared their head. Perhaps the most notable early
outbreak was the fierce war between Newton and Leibniz, each of whom
claimed to be the first to invent calculus—today widely considered to have been
developed independently by each of them. The feud caused such a rift between
the English and European mathematics communities that, for more than a
century, almost no scientific knowledge was exchanged between them.
In the early 18th century, Newton balked at nothing in his campaign for
priority over the invention of calculus: in 1712, the Royal Society of London
published a document granting Newton ownership of the invention and
discrediting Leibniz. The paper, however, should be taken with an exceptionally
large grain of salt, since Newton, who was president of the society at the time,
personally appointed all the committee members and even wrote large parts of
the document himself. The two colossi of mathematics never met in person and
it is not clear that Leibniz was ever exposed to Newton’s work. One can only
imagine how a productive exchange of their ideas, disputed over a public
platform, could have enhanced the introduction of calculus and the scientific
developments that followed.
“The aim of argument, or of discussion, should not be victory but progress,”
said the 19th-century French essayist Joseph Joubert. Once the new societies and
their publications made information more accessible, rivalry between scientists,

research institutes and even states began to drive new discoveries. Journalistic
interest in the drama enabled more public exposure to science. In one notable
case, the dispute between Thomas Huxley and Richard Owen, two of the leading
biologists in 19th-century Britain, focused an important spotlight on Charles
Darwin’s theory of evolution, little known to the general public at the time.
One of the stormiest scientific rivalries of recent years raged between the
paleoanthropologists Donald Johanson and Richard Leakey over the discovery
of some of the oldest fossils from prehuman species. Johanson discovered the
skeleton “Lucy,” thought to be around 3.2 million years old, while Leakey
discovered “the Turkana boy,” believed to be more than 1.5 million years
younger than Lucy—each cited by its discoverer as the proverbial “missing link”
between humans and apes. Their public falling out was remarkable even for
science. The researchers had refused to share a platform since 1981, but finally
met on stage in May 2011, explaining their positions and giving interviews at a
highly publicized event at the American Museum of Natural History in New
York, where their vocal discord had first erupted 30 years before.
Thirty years later, older and wiser, they expressed a genuine desire to integrate
their findings with many of the dramatic discoveries that had taken place since
their feud began. It also became clear how those two men complemented each
other: while Leakey had generated an abundance of fossils, it was Johanson who
was better at interpreting his findings.
Entire societies and social groups can rival each other, too. “Cruel is the snow
that sweeps Glencoe and covers the grave o’Donald,” begins a ballad by Jim
McLean about one of the most brutal events in the bloody history of Scotland.
The massacre of Glencoe took place early one morning in February 1692,
conceived by the British authorities as a punishment for the failing of the
MacDonald clan of Glencoe to swear allegiance to William and Mary, the new
co-regents over England, Scotland and Ireland. Thirty-eight men were killed by
British soldiers who lived among them, and 40 women and children were killed
when their homes were torched, or died later from starvation. The mass murder
was presented to the MacDonald clan as a revenge spree by the Campbell clan—
a claim that fell on willing ears given the long history of clashes between the two
groups. This bitter tribal rivalry, which began in the 14th century, continues in
different forms to this day.
Competitive sport is rife with rivalry. Glaswegian football fans can back either
the Rangers or Celtics, a late sublimation of the warring Scottish clans
mentioned above, and there is an endless array of favorites from boxers to
racing-car drivers. Nothing can match the fervor that caused El Salvador to
declare war on Honduras following the “Football War” of 1969. While the true

causes were economic, emotions first flared when fans of both teams clashed
violently at a FIFA World Cup qualifier. The third, decisive game was held in
Mexico City on June 26, 1969. El Salvador won 3–2 after extra time. The same
day, El Salvador dissolved all diplomatic ties with Honduras and the two
countries were at war less than three weeks later.
While tribal loyalty surely played its part in certain scientific and national
rivalries, as reflected in some of the stories, it cannot explain many other historic
rivalries. To that end, many have tried to uncover what the great rivalries in
science or modern-day entrepreneurship have in common. One interesting
finding is that many of those who wage battle for priority and fame suffered
from the absence of a parent in childhood.
In 2006, I assisted existential psychologist and philosopher Carlo Strenger of
Tel Aviv University in researching the characteristics of Israeli high-tech
entrepreneurs. The paper we co-authored, named “The Leonardo Effect,” was
inspired by a 1910 article in which Freud discussed how growing up without a
father influenced Leonardo da Vinci’s early development. Our study consistently
found that many male entrepreneurs tend to experience their fathers as weak,
inefficient, abusive or absent. “Fatherlessness” was the term we used to describe
the phenomenon that drove some of these hi-tech whizzes to success, as they
learned to become their own fathers early on. This illustrious list includes
Newton, Darwin, Lavoisier and Oracle’s Larry Ellison, who was Bill Gates’ rival
in the 1990s.
When people get so worked up over a rival, isn’t something deeper going on?
The fiercest rivals are often firstborn, says the American science historian Frank
Sulloway in Born to Rebel: Birth Order, Family Dynamics, and Creative Lives
(1996). Sulloway cites evolution as the basis for his claim that the finite resource
of parental attention is a source of sibling rivalry. Firstborn children use their
size and strength advantage to uphold their status, and are more likely to
compete over physical or intellectual territory. Younger siblings tend to
undermine the status quo and develop a rebellious personality. In a particularly
meticulous study, Sulloway analyzed the biographies of almost 4,000 researchers
and scientists from the 18th and 19th centuries, including 83 pairs of siblings. He
found that a younger sibling was 7.3 times more likely than a firstborn to support
an innovative theory. But a firstborn’s chances of engaging in rivalry were 3.2
times greater than those of younger siblings. You guessed it: Newton and
Leibniz were the eldest sons in their families. Turner was an older brother, and
Constable’s older brother was intellectually disabled, so the onus of success fell
on him as if he were the eldest.
The prototype, of course, is Cain, who committed the first envy-driven murder

in the Bible. A comprehensive study of sibling relations by the Dublin Institute
of Technology in 2012 found that, although most people support their siblings,
some exhibit signs of rivalry verging on outright hostility. Given our
achievement-oriented culture, it should come as no surprise that a third of
siblings report rivalry and emotional distance, with 15% not even talking to each
other. Sibling rivalry is greater when there is a small age gap, no gender
difference, or when one sibling is intellectually gifted.
Yet all these explanations are still missing something. People don’t get that
emotionally intense about science, sports or business; it’s only the personal stuff
that really gets us going. In that case, what could be more personal than
ourselves?
An especially profound exploration of rivalry comes from the psychologist
Carl Jung, the founder of analytical psychology, who said that we have more in
common with our rivals than we would like to admit. The qualities in our rival
that arouse our hostility are exactly the ones we prefer to repress in ourselves:
weakness, anxiety, greed, aggression, lust and rudeness are a few common
examples. Jung called this panoply of traits “the shadow.”
According to Freudian theory, we defend ourselves from urges we don’t want
to acknowledge by denying their existence and “projecting” them onto others.
This makes us attribute qualities, intentions and desires to others that are actually
our own. According to Jung, such urges are buried deep within the “shadow”
part of our mind. The less cognizant we are of the shadow inside us, the darker
and denser it becomes.
If we project qualities from our own “shadow” onto a potential rival, we can
easily find ourselves spiraling into a heated conflict when our rival behaves like
us. Even worse, without our rival, we might feel that we lack an independent
existence and wallow in the darkness of our “shadow” with no one to project it
upon.
Jung’s notion of the “shadow” adds dimension to the relationship between our
rivals and ourselves. According to Jung, our “persona” is what we would like to
be and what we wish the world to see in us—the social face we don when
meeting others. The “ego” is our conscious self, and the “shadow” is the dark
side that hides behind our social mask, which we prefer to ignore and repress. As
soon as we are old enough to comprehend the cultural mores around us, we
select those parts of the self that are socially acceptable and classify them as
“ego,” while repressing socially undesirable traits—transporting them to the
shadow, where they continue to exist unbeknown to us. Those traits are generally
negative, but can also be good. They may even be noble qualities that are not
valued in our particular social or cultural setting. Anyone who manages to

contain his or her “shadow” and become aware of it may be surprised to discover
not only shameful qualities, but also some especially positive ones.
Jung claimed that the “ego” and the “shadow” have the same origin and
maintain a perfect balance: the clearer the conscious part of our personality, the
more well-defined our “shadow” self. The opposite is true, too: a “shadow” that
is not contained can wreak mental havoc.
Look to your shadow to identify your lifelong rival—the source of your rage
and, perhaps, your creativity. If you have a particularly strong negative response
to someone and think he or she is a real jerk, think again. That might be a
reflection of your “shadow” in action.
Edward Bennett, a friend of Jung’s, elaborates on this in What Jung Really
Said (1967). He describes the phenomenon as a gut reaction that projects the
source of our emotion onto another, usually by means of sharp criticism or
outright accusation. When we hate someone, we hate something in them that is
part of us; if we do not subconsciously recognize our own traits in the other
person, then we will not be too bothered by them.
Projecting our shadow onto someone else is always easier than acknowledging
and containing it. When someone else projects their shadow onto us, it
encourages us to project our shadow back onto them, unless we are aware of
what is happening. But withstanding that dynamic takes an unusual level of self-
awareness—even for brilliant minds. And why would we want to resist? The
shadow is the seat of creativity, as far as Jung was concerned. In Owning Your
Own Shadow (1991), Robert Johnson, a popular American Jungian author and
analyst, explains why rivalries tend to erupt between especially creative people:
“Narrow creativity always brings a narrow shadow with it, while broader talents
call up a greater portion of the dark.” The more creative you are, the greater your
chances for rivalry. And the fiercer your rivalry—the higher your chances of
remarkable progress.

GUARDIANS OF THE LAKE
When personal interests mobilize to protect social capital
Five percent of the population does not derive any enjoyment from music.
Researchers from the University of Barcelona, while screening candidates for a
study designed to assess the emotional impact of listening to music, were
surprised to discover that one in every 20 candidates showed no physiological
response to melodies played to them, had no music player of any type in their
home, and did not listen to music via the computer. The same proportion of the
population (5%) is color-blind or has a food allergy.
There is something fascinating in the way nature dispenses exceptional
characteristics, including important personal traits. One in every 20 Americans
suffers from a severe psychological problem such as schizophrenia, persistent
depression or a bipolar disorder. Therefore, it should come as no surprise that
another study found that nearly one in every 20 business executives may be a
psychopath. The ratio of 1:20 is appealing when it reflects personal choice: when
you belong to 5% of the population by choice, you are special, for better or
worse, yet you’re not alone. One in every 20 U.S. citizens chooses
vegetarianism, and one in every 20—not the same one, apparently—chooses to
believe that bin Laden is alive.
The Big Five model in psychology (which we’ve already encountered in the
chapter entitled “I’ve Seen Happy Conservatives”) attributes the range of human
behavior to a combination of five dimensions of basic human characteristics
(openness to experience, conscientiousness, extraversion, agreeableness and
neuroticism), and contends that we can define our personality by ranking
ourselves in each of these traits. Conscientiousness represents our readiness to
work hard, focus and assume responsibility—in short, all the qualities we
attribute to those we define as “serious people.”
The American radio personality and writer Rush Limbaugh is quoted as
saying: “You want at least 5% of the population being serious. That 5, 6% of the
population carries the rest of the people. You’ve heard that old axiom: ‘5% of the
people pull the wagon; 95% are in it.’”
Indeed, when we look at social and personal responsibility—where our values,
conscientiousness and willpower are expressed—we find support for the 1:20
ratio coming from unexpected directions. For example, only one in 20 people

wash their hands properly after going to the bathroom. Researchers from the
University of Michigan observed 3,749 visitors to public bathrooms and were
appalled to discover that 10% didn’t wash their hands at all, a third skipped
using soap, and only 5% bothered to use soap and wash their hands for more
than 15 seconds, the time required to destroy germs and various contaminants.
(Incidentally, women were stricter than men in their personal-hygiene habits.)
One in 20 is also the percentage of the population that lost weight following a
diet and maintained their lower weight over time. Five percent is the proportion
of organ donors in Western countries that lack legislation or special programs to
encourage organ donation. And if I have to guess, probably one in 20 people
write a will and thus show responsibility for what they’ve accumulated during
their lives and for what they’ll leave behind them.
Imagine for a moment that the pool of social capital and level of interpersonal
trust in a society are like a lake with a limited supply of fish, and the individuals
in that society are fishermen living in a village abutting the lake. It’s easy to
understand why the village’s future requires it to place limits on fishing by its
residents—in order to prevent the fish population from dropping to a dangerous
level. A fisherman who exceeds the quota will substantially improve his
situation, but if many others act the same way, the fish supply will quickly
dwindle and everyone will suffer, including the fishermen who exceeded the
quota allotted to them. Many people cannot withstand the temptation to exceed
fishing limits, and the social system does not denounce them and sometimes
even lauds their success.
But there are a few people who not only comply with the allotted fishing
quota, but also try to convince others to do the same; they work hard on ways to
enrich the fishing and ensure its long-term sustainability. I call them the
“Guardians of the Lake,” and it’s already clear to me that one in every 20 people
qualifies for membership in this fraternity. Are you one?

EPILOGUE
Memento Mori *
Carmine Forte was born on November 26, 1908 in Italy, the son of a humble
coffee shop proprietor, and died in his sleep almost 100 years later, on February
28, 2007, as Charles Forte—an English baron, founder of an extensive chain of
hotels known as the Forte Group. Forte emigrated from Italy to Scotland as a
four-year-old and at 26 he opened a coffee shop on London’s upscale Regent
Street. He quickly went on to expand his catering and hotel business, received a
knighthood in 1970, and was dubbed a baron in 1982. And, of course, Lord Forte
and his wife became renowned as diligent collectors of paintings and other art
works.
In June 2012 Forte’s heirs asked the London auction house Christie’s to sell
part of the deceased’s collection. The sale offered various objects from the
couple’s lavishly appointed home in Belgravia. The many items displayed at a
preview on the walls of four of Christie’s spacious exhibition rooms, and in
suitable cabinets, attested to the Fortes’ refined tastes and the breadth of their
artistic interests: from Russian Imperial Porcelain, through Venetian landscape
paintings to the chair the lord sat on when at his desk. The crowning glory of the
auction was a sad testament to potential capital-government ties: the gifts Forte
received from Umberto II, the last king of Italy who ruled for a mere 34 days in
1946, left following the vote that made Italy a republic, and never returned.
The sale of the collection of items that had given the Fortes such pleasure
when they were alive was slated to begin at 10:30 A.M. in the main hall of the
auction house. Fifty or so chairs were set up in the room for anyone interested,
and two long counters on the sides of the room were manned by 12 Christie’s
employees ready to take the telephone orders of unknown clients who had seen
the items at the auction preview and wished to remain anonymous. At auctions
of this sort most of the deals are handled over the phone, so the buyers do not see
each other. Several of the paintings up for sale were hanging on the walls of the
room. Among them stood out the pair of Venetian landscapes painted by
Francesco Guardi, a 16th-century painter whose distinct style made it easy for
generations of forgers to imitate his works, to the point where he became more
prolific after his death than when he was alive.
Ten-thirty on the dot, the deft auctioneer from Christie’s takes his place at the

podium, and in his hand is the wooden gavel without which an auctioneer at a
public auction house has no existence. Over the auctioneer’s head is a big screen
that displays each item on the block and the latest bid on it. The bottom part of
the screen converts the bid from pounds sterling, in which the auction is held,
into five other currencies to ease the suffering of those who got rich without
knowing how to translate the British currency rate into the one in which they
count their fortune. The auctioneer gets ready to present the first item, a pair of
grand wall ornaments from the 19th century, originally from Veneto, Italy,
starting at a modest price of £5,000. The experienced traders always sit in the
back row, where they can more comfortably observe the fascinating dynamic
that suddenly heats up in auctions of this kind, when two buyers want the item
and each raises his bid, often well beyond his initial intention. The adept
auctioneer does not waste time on the inexpensive items. These are gone in 30
seconds and also include the great bargains for amateur buyers who want to dip
their toes in the waters of the sumptuous lake in which the rich bathe, without
having to pawn their home to do so. Pricier items preoccupy their auctioneer and
buyers for about a minute. The sale of especially expensive items that attract
experienced traders and professional collectors entails telephone consultations,
an understandable hesitation before the fateful decision and a few deep breaths.
A total of about three minutes per item.
Ten forty-five, a dozen people are in the room. One of them, a corpulent man
wearing a suit and a blasé look, is waiting for an item he desires—a reddish
tortoiseshell armoire with ebony and brass inlay, which has a starting price of
£12,000. The catalog goes into detail about the groups of French artisans who
made it and waxes eloquent in describing the history of the handsome item.
One can only imagine the excitement of the lord’s wife when she first saw it,
probably at another public auction. The armoire fails to soar beyond the upper
realm of the auction-house experts’ assessment and falls like a ripe fruit into the
hands of the heavy man at a price of £17,000, which undoubtedly would have
led the late lord to reconsider his much-discussed departure from the world had
he known this would be the fate of the precious objects he collected.
The few people sitting in the hall soon witness the telephone struggle of the
new rich who seek to acquire the scattering ornaments of an old moneybags.
Item number 13 prompts a stir in the room. This is the first of four gifts that the
last Italian king from the House of Savoy gave Lord Forte. It is an inkstand
modeled as the Fontana dei Dioscuri in Rome. The little sculpture is made of
lapis lazuli, porphyry and onyx, and its price climbs in less than a minute from
£17,000 to £52,000, offered over the phone by an Italian buyer, a fan of royal
houses, fountains or onyx stones.

The auctioneer, a master of timing and pace and an expert at utilizing the
human vocal register, loses some of his effectiveness in the face of the telephone
competitors who are not affected by the human dynamic that usually develops
between those physically present in the public auction rooms. With mere words,
the Christie’s staffers by the phones toss around in the air tens of thousands of
pounds sterling with an enthusiasm reserved for those who have stepped for a
moment into the shoes of one they would like to be someday.
The smartly dressed young Japanese woman, who has been sitting in the room
from the beginning of the auction, readies herself for item 23—a simple glass
table that she buys at the auctioneer’s opening price, £200, granting her the touch
of nobility that brought her to the room in the first place. Item number 28, a
prayer rug from Isfahan, rouses from his nap one who hitherto had seemed out of
place in the lavish hall with his disheveled clothes. The seasoned rug merchant
discovered beneath the shabby coat handily adds the item to the inventory of
carpets in his possession. The lord’s spirit pervading the room cannot help but be
saddened by the circumstances in which this beautiful object that had welcomed
guests to his study is wending its way to foreign fields, to people he would never
have associated with in his lifetime. Next, ten Russian papier-mâché boxes are
snapped up, and afterward a long series of handsome items that had broadened
the late lord’s mind in their time. His dispersing wealth goes to whoever bids
highest in a room where there are not many high bidders.
A sneeze by a woman present in the hall nearly disrupts the sale of item 33, a
mahogany wall clock. The auctioneer tarries briefly to ascertain whether the
interruption was an agreed-upon signal on the buyer’s part to raise the bid. The
tissue that the woman whisked out convinced even him that health comes before
amassing property. The few people present in the room leaf through the catalog
in their hands, trying to estimate how much time is left until the item they are
interested in comes up for sale.
The average pace at which the auction takes place is under a minute per item.
Dozens of years of collecting come to an inevitable end in three hours’ time.
Most of the items go for near their minimum sale price. The only hope left is that
these gave their owner a little joie de vivre, provided, of course, that he managed
to find time amid his many preoccupations, including his other acquisitions, to
enjoy them.
**
I’ve seen things you people wouldn’t believe. Attack ships on fire off the shoulder

of Orion. I watched C-beams glitter in the dark near the Tannhäuser Gate. All
those moments will be lost in time, like tears … in … rain. Time to die.
Those are the last words of Roy Batty in the final scene of Ridley Scott’s
wonderful film Blade Runner (1982). This unforgettable film text became known
as the “Tears in Rain” soliloquy. Batty is a humanlike robot who speaks the sad
words in a downpour, just before the programed expiration of his short life. The
Dutch actor Rutger Hauer portrayed Batty and also wrote the replicant’s final
words. Was Hauer speaking here as an actor about the humanlike robot he
played in the film, whose memories were woven from glorious galactic battles,
or was he referring to himself as a man and his personal memories: people he
had met and loved, the rustling of leaves he heard in his childhood when the
wind blew through the trees of the garden by his house, the special light of the
weekly day of rest, or the peaks and nadirs of his career? All of them, he already
knows, will be swept away like tears in rain.
Here’s the question I ask myself when I think of Roy Batty’s speech: in my
life today, is there a way I can influence my future response to the question of
the life I didn’t live? I’m not talking about derailing the prophecy of the Oracle
from Delphi or rewriting the plot in the inevitable final chapter of a Greek
tragedy. Since I’m unable to live the lives of others (because other people are
already living them, to paraphrase Oscar Wilde), all that remains for me is to live
my life with the sense of responsibility of someone who has received a one-time
opportunity to create a meaningful life.
The name I chose for one of the chapters in the book, One Day, When I’m
Younger, may express acceptance of the irreversible march of time, but it also
signals the optimism of someone who realizes that our actions today shape the
memories we’ll want to cherish at the end of our lives. In order to live a
meaningful life, we should already consider the things we’ll regret on our death
bed. We should recognize the limitations of our intellect and our susceptibility to
bias, and adopt the humility that follows from this recognition and from an
understanding of our limited place in the universe. If we also understand how
similar we are to other human beings, we’ll be able to conduct ourselves with the
inclusion, generosity and acceptance that offers hope to all of us, but first of all
to ourselves.
*“Remember that you will die” in Latin.

RECOMMENDATIONS FOR FURTHER READING
If Only
Phillips, A. Missing Out: In Praise of the Unlived Life.
Hamish Hamilton, 2012.
Gilovich, T. and Medvec, V.H. “The Experience of Regret:
What, When, and Why.” Psychological Review, 102(2).
Apr 1995. 379–395.
Landman, J. Regret: The Persistence of the Possible. Oxford
University Press, 1993.
Kray, L.J.; George, L.G.; Liljenquist, K.A.; Galinsky, A.D.; Tetlock,
P.E.; Roese, N.J. “From what might have been to what must have been:
Counterfactual thinking creates meaning.” Journal of Personality and
Social Psychology, 98(1). Jan 2010. 106–118.
http://dx.doi.org/10.1037/a0017905.
Ware, B. The Top Five Regrets of the Dying: A Life Transformed by the Dearly
Departing. Hay House, 2012.
Escape from the Matrix
London, B. “FOMO costs the average Briton £1,500 a year:
We just can’t say no to social events for ‘fear of missing out.’” Mail Online.
http://www.dailymail.co.uk/femail/article-2398687/FOMO-costs-average-
Briton-1-500-year-We-say-events-fear-missing-out.html.
Kross, E.; Verduyn, P.; Demiralp, E.; Park, J.; Lee, D.S.; Lin, N.; et al.
“Facebook Use Predicts Declines in Subjective Well-Being in Young Adults.”
PLoS ONE 8(8). Aug 2013. doi:10.1371/journal.pone.006984.
Turkle, S. Alone Together: Why We Expect More from Technology and
Less from Each Other. Basic Books, 2012.
Przybylski, A.K.; Murayama, K.; DeHaan, C.R.; Gladwell, V.
“Motivational, emotional, and behavioral correlates of fear of missing out.”
Computers in Human Behavior, 29(4). July 2013. 1841-1848.
doi:10.1016/j.chb.2013.02.014.
———. “Fear of Missing Out Quiz.”

http://www.ratemyfomo.com/
Dunbar, R. How Many Friends Does One Person Need?: Dunbar’s
Number and Other Evolutionary Quirks. Harvard University Press, 2010.
Hofmann, W.; Vohs, K.D.; Baumeister, R.F. “What People Desire,
Feel Conflicted About, and Try to Resist in Everyday Life.” Psychological
Science, 23(6). June 2012. 582–588.
Simon, H.A. Models of My Life. MIT Press, 1996.
Schwartz, B.; Ward, A.; Monterosso, J.; Lyubomirsky, S.; White,
K.; Lehman, D.R. “Maximizing Versus Satisficing: Happiness Is a Matter
of Choice.” Journal of Personality and Social Psychology, 38(5). 2002.
1178–1197.
Picking Your Battles
Baumeister, R.F. and Tierney, J. Willpower: Rediscovering the
Greatest Human Strength. Penguin Books, 2012.
Hofmann, W.; Vohs, D.K.; Baumeister, R.F. “What People Desire,
Feel Conflicted About, and Try to Resist in Everyday Life.” Psychological
Science, 23. 2012. 582–588.
Mischel, W.; Ebbe, B.E.; Antonette, R.Z. “Cognitive and Attentional
Mechanisms in Delay of Gratification.” Journal of Personality and Social
Psychology, 21. 1972. 204–218.
Mokdad, A.H.; Marks, J.S.; Stroup, D.F.; Gerberding, J.L. “Actual
Causes of Death in the United States.” Journal of the American Medical
Association, 291(10). 2004. 1238–1245.
One Day, When I’m Younger
Watts, G.F. and assistants. Hope. 1886.
“Democratic National Convention keynote address 2004.”
Wikipedia. https://en.wikipedia.org/wiki/2004_
Democratic_National_Convention_keynote_address.
Myers, D.G. “The Funds, Friends and Faith of Happy People.”
American Psychologist, 55(1). Jan 2000. 56–67. http://dx.doi.
org/10.1037/0003-066X.55.1.56.
Snyder, C.R., Psychology of Hope: You Can Get Here from There.
Free Press, 2003.
Snyder, C.R.; Shorey, H.S.; Cheavens, J.; Pulvers, K.M.; Adams,
V.H., III; Wiklund, C. “Hope and Academic Success in College.” Journal of

Educational Psychology, 94(4). Dec 2002. 820–826.
Rand, K.L.; Martin, A.D.; Shea, A.M. “Hope, but not optimism, predicts
academic performance of law students beyond previous academic
achievement.” Journal of Research in Personality, 45(6). Dec 2011. 683–686.
Curry, L.A.; Snyder, C.R.; Cook, D.L.; Ruby, B.C.; Rehm, M.
“Role of hope in academic and sport achievement.” Journal of Personality
and Social Psychology, 73(6). Dec 1997. 1257–1267.
http://dx.doi.org/10.1037/0022-3514.73.6.1257.
Kaufman, S.B. “The Will and Ways of Hope.” The Huffington
Post, Blog. Mar 4, 2012. http://www.huffingtonpost.com/scott-barry-
kaufman/hopesuccess_b_1174856.html.
Magaletta, P.R. and Oliver, J.M. “The hope construct, will, and ways: Their
relations with self-efficacy, optimism, and general well-being.” Journal of
Clinical Psychology, 55(5). May 1999. 539–551.
Lopez, S.J. Making Hope Happen: Create the Future You Want for Yourself and
Other. Atria Books, 2013.
Steptoe, A.; Dockray, S.; Wardle, J. “Positive Affect and Psychobiological
Processes Relevant to Health.” Journal of Personality, 77(6). 2009. 1747–
1776.
Groopman, J. The Anatomy of Hope: How People Prevail in the Face of Illness.
Random House, 2004.
Moses, B. The Truth of Scientific Medicine. Magnes, 2008 [Hebrew].
Challenging the Bottom-Line Approach
Taleb, N.N. The Black Swan: The Impact of the Highly Improbable.
Random House, 2007.
Kay, J. Obliquity: Why Our Goals Are Best Achieved Indirectly.
Profile Books (GB), 2011.
Macdonald, P. The Alexander Technique as I See It. The Alpha Press, 1989.
“Outcome bias.” Wikipedia. https://en.wikipedia.org/wiki/
Outcome_bias.
Happiness: Cut Out and Save
“Hedonic treadmill.” Wikipedia. https://en.wikipedia.org/wiki/
Hedonictreadmill.
Brickman, P.; Coates, D.; Janoff-Bulman, R. “Lottery Winners and Accident
Victims: Is Happiness Relative?” Journal of Personality and Social
Psychology, 36(8). 1978. 917–927.
Lykken, D. and Tellegen, A. “Happiness Is a Stochastic Phenomenon.”

Psychological Science, 7(3). May 1996.
Kasser, T. The High Price of Materialism. A Bradford Book, 2002.
“The High Price of Materialism.” YouTube.
https://www.youtube.com/watch?v=oGab38pKscw.
“What Psychology Says About Materialism and the Holidays:
Six Questions for Materialism Expert Tim Kasser, PhD.” American
Psychological Association (APA). Dec 16, 2014.
http://www.apa.org/news/press/releases/2014/12/materialism-holidays.aspx.
Kasser, T.; Rosenblum, K.L.; Sameroff, A.J.; Deci, E.L.; Niemiec, C.P.; Ryan,
R.M.; Árnadóttir, O.; Bond, R.; Dittmar, H.; Dungan, N.; Hawks, S. “Changes
in materialism, changes in psychological well-being: Evidence from three
longitudinal studies and an intervention experiment.” Motivation and
Emotion, 38(1). 2013. 1–22. http://dx.doi.org/10.1007/s11031-013-9371-4.
Bauers, S. “Doctors’ new prescription: ‘Don’t just exercise, do it outside.’” The
Guardian. Feb 10, 2015.
http://www.theguardian.com/lifeandstyle/2015/feb/10/health-prescriptions-
doctors-healthcare-fitness-exerciseparks.
Gladwell, M. Outliers: The Story of Success. Little, Brown and Company, 2008.
Wolf, S. and Bruhn, J.G. The Power of Clan: The Influence of Human
Relationships on Heart Disease. Transaction Publishers, 1998.
Egolf, B.; Lasker, J.; Wolf, S.; Potvin, L. “The Roseto effect:
A 50-year comparison of mortality rates.” American Journal of Public
Health, 82(8). Aug 1992. 1089–1092.
Kahneman, D. and Krueger, A.B. “Developments in the Measurement of
Subjective Well-Being.” Journal of Economic Perspectives, 20(1). 2006. 3–
24.
Csikszentmihalyi, M. Flow: The Psychology of Optimal Experience.
Harper Perennial Modern Classics, 2008.
Twenge, J.M. and Kasser, T. “Generational Changes in Materialism and Work
Centrality, 1976–2007: Associations with Temporal Changes in Societal
Insecurity and Materialistic Role Modeling.” Personality and Social
Psychology Bulletin, 39(7). July 2013. 883–897.
Robinson, J.P. and Martin, S. “What Do Happy People Do?”
Social Indicators Research, 89(3). 2008. 565–571.
Dolan, P. Happiness by Design: Change What You Do, Not How You Think.
Avery, 2014.
Anik, L.; Aknin, L.B.; Norton, M.I.; Dunn, E.W. “Feeling Good about Giving:
The Benefits (and Costs) of Self-Interested Charitable Behavior.” HBS
working paper 10-012, Sep 2009. http://hbswk.hbs.edu/item/feeling-good-

about-giving-the-benefits-and-costs-of-self-interested-charitable-behavior.
A Bit of Humble Pie Goes a Long Way
Lombrozo, Tania. “Illusions of Understanding and the Loss of Intellectual
Humility.” Edge, Annual Question 2013: What Should We Be Worried About?
https://www.edge.org/response-detail/23731.
Samuelson, P.L. and Church, I.M., “Known Unknowns or:
How we learned to stop worrying about uncertainty and love intellectual
humility.” Thrive: Center for Human Development, hosted articles. May
2014.
Wood, W.J. “How Might Intellectual Humility Lead to Scientific
Insight?” Big Questions Online. Dec 2012.
https://www.bigquestionsonline.com/2012/12/10/how-might-intellectual-
humility-lead-scientific-insight/.
Friedman, T.L. “How to Get a Job at Google.” The New York
Times. Feb 22, 2014.
http://www.nytimes.com/2014/02/23/opinion/sunday/friedman-how-to-get-
a-job-at-google.html?_r=3.
“Dunbar’s Number: Why We Can’t Have More Than 150
Friends.” YouTube. https://www.youtube.com/watch?v=ppLFce5uZ3I.
“Primates on Facebook.” The Economist. Feb 26, 2009.
http://www.economist.com/node/13176775.
Why Do Smart People Make Stupid Mistakes?
“Dual process theory.” Wikipedia.
https://en.wikipedia.org/wiki/Dual_process_theory.
“Regression toward the mean.” Wikipedia. https://en.wikipedia.
org/wiki/Regression_toward_the_mean.
“Nobel Prize Lecture by Daniel Kahneman.”
http://www.nobelprize.org/mediaplayer/?id=531.
“Egocentric bias.” Wikipedia. https://en.wikipedia.org/wiki/
Egocentric_bias.
“Bias blind spot.” Wikipedia. https://en.wikipedia.org/wiki/
Bias_blind_spot.
Hall, M. “Goalies stay put—Jacob Burak explains the virtues of standing still in
business.” Gruender Szene. July 22, 2010.
http://www.gruenderszene.de/expert-articles/goalies-stay-put-%E2%80%93-
jacob-burak-explains-the-virtues-of-standing-still-in-business.
“Feeling grumpy ‘is good for you.’” BBC News. Nov 5, 2009.

http://news.bbc.co.uk/2/hi/health/8339647.stm.
And Merci to the French Teacher
Keysar, B.; Hayakawa, S.L.; An, S.G. “The Foreign-Language
Effect Thinking in a Foreign Tongue Reduces Decision Biases.”
Psychological Science. April 18, 2012. 1–8.
Kahneman, D. Thinking, Fast and Slow. Farrar, Straus and Giroux, 2011.
The Prisoner’s Dilemma
Danziger, S.; Jonathan, L.; Avnaim-Pesso, L. “Extraneous Factors in Judicial
Decisions.” Proceedings of the National Academy of Sciences, 108(17). 2011.
6889–6892.
Conley, M. “Is Driving with a Cold the Same as Driving
Drunk?” ABC News. Jan 5, 2012.
http://abcnews.go.com/blogs/health/2012/01/05/is-driving-with-a-cold-the-
same-as-driving-drunk/
If I’m Not for Myself
Bazerman, M.H. and Tenbrunsel, A.E. Blind Spots: Why We Fail to Do What’s
Right and What to Do about It. Princeton University Press, 2011.
Caruso, E.M.; Epley, N.; Bazerman, M.H. “The Costs and Benefits of Undoing
Egocentric Responsibility Assessments in Groups.” Journal of Personality
and Social Psychology, 91(5). 2006. 857–871.
Knowing What We Don’t Know
Kruger, J. and Dunning, D. “Unskilled and Unaware of It:
How Difficulties in Recognizing One’s Own Incompetence Lead to Inflated
Self-Assessments.” Journal of Personality and Social Psychology, 77(6).
1999. 1121–1134.
Birdbrained
Herbranson, W. and Schreder, J. “Are Birds Smarter Than Mathematicians?
Pigeon (Columba livia) Perform Optimally on a Version of Monty Hall
Dilemma.” Journal of Comparative Psychology, 124(1). 2010. 1–13.
I Saw a Monkey Playing Mozart
Bangerter, A. and Heath, C. “The Mozart Effect: Tracking the Evolution of a
Scientific Legend.” British Journal of Social Psychology, 43. 2004. 605–623.

Brunvand, J.H. The Vanishing Hitchhiker: American Urban Legends and Their
Meanings. W. W. Norton & Company, 1981.
Heath, C.; Bell, C.; Sternberg, E. “Emotional Selection in Memes: The Case of
Urban Legends.” Journal of Personality and Social Psychology, 81(6). 2001.
1028–1041.
Dawkins, R. The Selfish Gene. Oxford University Press, 1976.
Heath, C. and Heath D. Made to Stick: Why Some Ideas Survive and Others Die.
Random House, 2007.
Lake Victoria and Uncle Albert
Cialdini, R.B. Influence: The Psychology of Persuasion, revised edition. Harper
Business, 2006.
The Psychology of Scams: Provoking and committing errors of judgment.
Office of Fair Trading (prepared by University of Exeter School of
Psychology),
2009.http://webarchive.nationalarchives.gov.uk/20140402142426/http://www
I Accuse, Falsely
Shaw, J. and Porter, S. “Constructing Rich False Memories of Committing
Crime.” Psychological Science, 26(3). March 2015. 291–301.
Dror, I.E.; Charlton, D.; Péron, A.E. “Contextual information renders experts
vulnerable to making erroneous identifications.” Forensic Science
International, 156(1). Jan 2006. 74–78.
Garrett, B.L. Convicting the Innocent: Where Criminal Prosecutions
Go Wrong, reprint edition. Harvard University Press, 2012.
Wargo, E. “How Psychological Scientists are Having an Impact on the Legal
System.” Association for Psychological Sciences, Observer, 24(9). Nov 2011.
http://www.psychologicalscience.org/index.php/publications/observer/2011/nov
11/from-the-lab-to-the-courtroom.html.
List for Life
Cagen, S. To-Do List: From Buying Milk to Finding a Soul Mate, What Our Lists
Reveal About Us. Touchstone, 2007.
Beyer, S. and Gorris, L. “Spiegel Interview with Umberto Eco:
‘We Like Lists Because We Don’t Want to Die.’” Spiegel Online,
International. Nov 2009.

http://www.spiegel.de/international/zeitgeist/spiegelinterview-with-
umberto-eco-we-like-lists-because-we-don-t-want-to-die-a-659577.html.
Kirwin, L. Lists: To-dos, Illustrated Inventories, Collected Thoughts, and Other
Artists’ Enumerations from the Collections of the Smithsonian Museum.
Princeton Architectural Press, first edition, 2010.
Beard, M. “The Infinity of Lists by Umberto Eco.” The Guardian.
Dec 12, 2009. http://www.theguardian.com/books/2009/dec/12/umberto-
eco-lists-book-review.
On the Shoulders of Giants
Skiena, S.S. and Ward, C.B. Who’s Bigger?
Where Historical Figures Really Rank. Cambridge University Press, first
edition, 2013.
Hart, M.H. The 100: A Ranking of the Most Influential Persons in History.
Citadel, 1992.
“Millennium Top Ten.” Time. Oct 15, 1992. http://content.time.
com/time/magazine/article/0,9171,976745,00.html.
Burt, D.S. The Literary 100: A Ranking of the Most Influential
Novelists, Playwrights, and Poets of All Time. Checkmark Books, revised
edition, 2008.
Outlook: Gloomy
Fox, E.; Lester, V.; Russo, R.; Bowles, R.J.; Pichler, A.; Dutton, K.
“Facial Expressions of Emotion: Are Angry Faces Detected More
Efficiently?” Cognition & Emotion, 14(1). 2000. 61–92.
Averill, J.R. “On the Paucity of Positive Emotions.” Assessment and
Modification of Emotional Behavior. Edited by Blankstein, K.R.; Pliner, P.;
Polivy, J. Springer U.S., 1980. 7–45.
Rozin, P. and Royzman, E.B. “Negativity Bias, Negativity
Dominance, and Contagion.” Personality and Social Psychology Review,
5(4). 2001. 296–320.
Baumeister, R.F.; Bratslavsky, E.; Finkenauer, C.; Vohs, K.D. “Bad is stronger
than good.” Review of General Psychology, 5(4). 2001. 323–370.
http://dx.doi.org/10.1037/1089-2680.5.4.323.
Zenger, J. and Folkman, J. “The Ideal Praise-to- Criticism
Ratio.” Harvard Business Review. March 15, 2013.
https://hbr.org/2013/03/the-ideal-praise-tocriticism/.

Siegrist, M. and Cvetkovich, G. “Better Negative than Positive?
Evidence of a Bias for Negative Information about Possible Health
Dangers.” Risk Analysis, 21(1). 2001. 199–206. doi: 10.1111/0272-
4332.211102.
“Hillary Clinton Ad — 3 AM White House Ringing Phone.”
YouTube. https://www.youtube.com/watch?v=7yr7odFUARg.
Cold Hands or a Warm Heart
Asch, S.E. “Forming impressions of personality.” Journal of Abnormal and
Social Psychology, 41(3). 1946. 258–290.
http://dx.doi.org/10.1037/h0055756.
Widmeyer, W.N. and Loy, J.W. “When You’re Hot, You’re Hot!
Warm-Cold Effects in First Impressions of Persons and Teaching
Effectiveness.” Journal of Educational Psychology, 80(1). 1988. 118–121.
“Warmth and Competence.” Changing Minds.
http://changingminds.org/explanations/emotions/warmth_competence.htm.
I’ve Seen Happy Conservatives
Bromund, T. “Liberals Speak, a Conservative Responds.”
Heritage. July 15, 2014.
http://www.heritage.org/research/commentary/2014/7/liberals-speak-
aconservative-responds.
Napier, J.L. and Jost, J.T. “Why Are Conservatives Happier
Than Liberals?” Psychological Science, 19(6). 2008. 565–572. doi:
10.1111/j.1467-9280.2008.02124.x.
Schreiber, D.; Fonzo, G.; Simmons, A.N.; Dawes, C.T.; Flagan, T.;
Fowler, J.H.; et al. “Red Brain, Blue Brain: Evaluative Processes Differ in
Democrats and Republicans.” PLoS ONE, 8(2). 2013. e52970.
doi:10.1371/journal. pone.0052970.
Kanai, R.; Feilden, T.; Firth, C.; Rees, G. “Political Orientations
Are Correlated with Brain Structure in Young Adults.” Current Biology,
21(8). 2011. 677–680.
Eaves, L.J. and Eysenck, H.J. “Genetics and the development of social
attitudes.” Nature, 249(54). 1974. 288–289.
Hatemi, P.K. and McDermott, R. “The genetics of politics: discovery, challenges,
and progress.” Trends in Genetics, 28(10). 2012. 525–533.
Jost, J.T.; Glaser, J.; Kruglanski, A.W.; Sulloway, F.J. “Political
Conservatism as Motivated Social Cognition.” Psychological Bulletin,
129(3). 2003. 339–375.

Carney, D.R.; Jost, J.T.; Gosling, S.D; Potter, J. “The Secret Lives of Liberals
and Conservatives: Personality Profiles, Interaction Styles, and the Things
They Leave Behind.” Political Psychology, 29(6). 2008. 807–840.
Oxley, D.R.; Smith, K.B.; Alford, J.R.; Hibbing, M.V.; Miller, J.L.; Scalora, M.;
Hatemi, P.K.; Hibbing, J.R. “Political Attitudes Vary with Physiological
Traits.” Science, 321. 2008. 1667. doi: 10.1126/science.1157627.
Hibbing, J.R.; Smith, K.B.; Alford, J.R. “Differences in negativity bias underlie
variations in political ideology.” Behavioral and Brain Sciences, 37(3). 2014.
297–307.
———. Predisposed: Liberals, Conservatives, and the Biology of
Political Differences. Routledge, 2013.
Pruitt, J.N. and Riechert, S.E. “How within-group behavioural variation and task
efficiency enhance fitness in a social group.” Proceedings Biological
Sciences, 278(1709). 2011. 1209–1215. doi: 10.1098/rspb.2010.1700.
Klofstad, C.A.; McDermott, R.; Hatemi, P.K. “The Dating
Preferences of Liberals and Conservatives.” Political Behavior, 35(3).
2012. 519–538.
Graham, J.; Nosek, B.A.; Haidt, J. “The Moral Stereotypes of Liberals and
Conservatives: Exaggeration of Differences across the Political Spectrum.”
PLoS ONE. Dec 12, 2012. http://dx.doi.org/10.1371/journal.pone.0050092.
Kahneman, D. and Renshon, J. “Why Hawks Win.” Foreign Policy.
Oct 13, 2009. http://foreignpolicy.com/2009/10/13/why-hawks-win/.
Swann, W.B. Jr.; Pelham, B.W.; Roberts, D.C. “Causal Chunking:
Memory and Inference in Ongoing Interaction.” Journal of Personality and
Social Psychology, 53(5). 1987. 858–865.
The Matthew Effect
Ben Bassat, A. and Dahan, M. The Balance of Power in the
Budgeting Process. The Israel Democracy Institute, 2006.
Gladwell, M. Outliers: The Story of Success. Little, Brown and
Company, 2008.
Levie, R. “Delayed Morpho-Lexical Development in Hebrew in the Shadow of
Congenital Impairment and Social Disadvantage.” Ph.D. diss., Tel Aviv
University School of Education, 2012.
Levie, R. Interview with Bar Hayun, Dec 27, 2012.
Kahneman, D. Lecture at The New School, New York, 2013.
watch?v=l91ahHR5-i0 https://www.youtube.com/watch?v=l91ahHR5-i0

Me, Myself and I
Baumeister, R.F. and Leary, M.R. “The Need to Belong: Desire for Interpersonal
Attachments as a Fundamental Human Motivation.” Psychological Bulletin,
117(3). 1995. 497–529.
Grossmann, I. and Varnum, M.E.W. “Social structure, infectious diseases,
disasters, secularism, and cultural change in America.” Psychological Science,
26(3). Mar 2015. 311–24. doi: 10.1177/0956797614563765.
Twenge, J.M. Generation Me: Why Today’s Young Americans Are More
Confident, Assertive, Entitled—and More Miserable Than Ever Before. Atria
Books, 2007.
Twenge, J.M. and Campbell, W.K. The Narcissism Epidemic: Living in the Age
of Entitlement. Atria Books, 2010.
“Narcissistic Personality Inventory.” Wikipedia.
https://en.wikipedia.org/wiki/Narcissistic_Personality_Inventory.
Keeney, D. “Study: Narcissism on the Rise in American Music.”
Death and Taxes. April 26, 2011.
http://www.deathandtaxesmag.com/82083/study-narcissism-on-the-rise-in-
american-music/.
Long Live the Small Difference
“The Big Religion Chart: Comparison Chart.”
ReligionFacts.com. Nov 12, 2015.
http://www.religionfacts.com/bigreligion-chart.
Nauert, R. “Are Emotions Universal?” Psych Central. 2015.
http://psychcentral.com/news/2010/01/27/are-
emotionsuniversal/10999.html.
“Narcissism of small differences.” Wikipedia.
https://en.wikipedia.org/wiki/Narcissism_of_small_differences.
An Embarrassment of Riches
Goffman, E. “Embarrassment and Social Organization.” American
Journal of Sociology, 62(3). Nov 1956. 264–271.
Feinberg, M.; Willer, R.; Keltner, D. “Flustered and Faithful:
Embarrassment as a Signal of Prosociality.” Journal of Personality and
Social Psychology. Advance online publication. Sept 19, 2011. doi:
10.1037/a0025403.

Crozier, W.R. and de Jong, P.J. (eds). The Psychological Significance of the
Blush. Cambridge University Press, 2015.
Bering, J. “Why We Blush: The Social Purpose of Showing
Embarrassment.” Scientific American. Sept 11, 2009.
http://blogs.scientificamerican.com/beringin-mind/why-we-blush-the-
social-purpose-of-showing-embarrassment/.
Keltner, D.; Willer, R.; Feinberg, M. “Flustered and Faithful:
Embarrassment as a Signal of Prosociality.” Journal of Personality and
Social Psychology, 102(1). Jan 2012. 81–97.
Trust Games
Van Lange, P.A.M. “Generalized Trust: Four Lessons from Genetics and
Culture.” Current Directions in Psychological Science, 24(1). Feb 2015. 71–
76. doi:10.1177/0963721414552473.
Barefoot, J.C.; Maynard, K.E.; Beckham, J.C.; Brummett, B.H.; Hooker, K.;
Siegler, I.C. “Trust, Health, and Longevity.” Journal of Behavioral Medicine,
21(6). Dec 1998. 517–526.
Morrone, A.; Tontoranelli, N.; Ranuzzi, G. “How Good is Trust?
Measuring Trust and its Role for the Progress of Societies.” OECD
Statistics Working Papers, No. 2009/03. OECD Publishing, Oct 2009.
http://dx.doi.org/10.1787/220633873086.
The Beginning of a Beautiful Rivalry
Kilduff, G.J. “The Psychology of Rivalry.” UC Berkeley: Business
Administration, Ph.D. Program, 2010.
http://escholarship.org/uc/item/2k10z38x.
“Human Evolution and Why It Matters: A Conversation with Leakey and
Johanson.” YouTube. https://www.youtube.com/watch?v=pBZ8o-lmAsg.
“List of sports rivalries.” Wikipedia.
https://en.wikipedia.org/wiki/List_of_sports_rivalries.
Strenger, C. and Burak, J. “The Leonardo Effect: Why entrepreneurs become
their own fathers.” International Journal of Applied Psychoanalytic Studies,
2(2). June 2005. 103–128. doi: 10.1002/aps.36.
Sulloway, F.J. Born to Rebel. Vintage, 1997.
Wallace, E. “The Sibling Relationship: Friendship or Rivalry?”
Masters diss., Dublin Institute of Technology. Sept 2012.
Mersky Leder, J. “Adult Sibling Rivalry: Sibling rivalry often lingers through

adulthood.” Psychology Today, last reviewed June 20, 2013.
https://www.psychologytoday.com/articles/199301/adult-sibling-rivalry.
“Carl Jung—Shadow Projection.” YouTube. https://www.youtube.com/watch?
v=zMHqqXYaB8g.
Johnson, R.A. Owning Your Own Shadow: Understanding the Dark
Side of the Psyche. Bravo Ltd, reprinted edition, 1994.
Bennet, E.A. What Jung Really Said. Schocken, 1983.
Guardians of the Lake
Rivero, E. “Nearly 5% of U.S. population suffers from persistent depression or
anxiety.” UCLA Newsroom. Dec 2, 2008.
http://newsroom.ucla.edu/releases/nearly-5-percent-of-the-u-s-population-
72195.
Morris, S. “One in 25 business leaders may be a psychopath, study finds.” The
Guardian. Sept 1, 2011.
https://www.theguardian.com/science/2011/sep/01/psychopath-workplace-
jobs-study.
Newport, F. “In U.S., 5% Consider Themselves Vegetarians.”
Gallup, July 26, 2012. http://www.gallup.com/poll/156215/consider-
themselves-vegetarians.aspx.
Borchgrevink, C.P.; Jaemin, C.; SeungHyun, K. “Handwashing
Practices in a College Town Environment.” Journal of Environmental
Health, 75(8). 2013. 18–24.
Kramer, M. “5 Secrets of the 5%: What You Can Learn from Successful
Dieters.” Spark People.
http://www.sparkpeople.com/resource/wellness_articles.asp?id=423.

The story of Watkins dates back to 1893, when the scholar of esotericism John Watkins founded a
bookshop, inspired by the lament of his friend and teacher Madame Blavatsky that there was nowhere in
London to buy books on mysticism, occultism or metaphysics. That moment marked the birth of Watkins,
soon to become the home of many of the leading lights of spiritual literature, including Carl Jung, Rudolf
Steiner, Alice Bailey and Chögyam Trungpa.
Today, the passion at Watkins Publishing for vigorous questioning is still resolute. Our wide-ranging and
stimulating list reflects the development of spiritual thinking and new science over the past 120 years. We
remain at the cutting edge, committed to publishing books that change lives.
DISCOVER MORE ...
Read our blog
Watch and listen to our authors in
action
Sign up to our mailing list
JOIN IN THE CONVERSATION
 WatkinsPublishing   
 @watkinswisdom
 watkinsbooks   
 watkinswisdom   
 watkins-media
Our books celebrate conscious, passionate, wise and happy living. Be part of the
community by visiting
www.watkinspublishing.com

This edition first published in the UK and USA 2017 by Watkins, an imprint of Watkins Media Limited
19 Cecil Court
London WC2N 4EZ
enquiries@watkinspublishing.com
Design and typography copyright © Watkins Media Limited 2017
Text copyright © Jacob Burak 2017
Jacob Burak has asserted his right under the Copyright, Designs and Patents Act 1988 to be identified as the
author of this work.
All rights reserved.No part of this book may be reproduced or utilized in any form or by any means,
electronic or mechanical, without prior permission in writing from the Publishers.
1 3 5 7 9 10 8 6 4 2
Designed and typeset by Welmoet Wartena Printed and bound in Finland
A CIP record for this book is available from the British Library ISBN: 978-1-78678085-0
www.watkinspublishing.com

