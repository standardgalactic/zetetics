An Introduction to Bayesian Thinking
A Companion to the Statistics with R Course
Merlise Clyde
Mine Ã‡etinkaya-Rundel
Colin Rundel
David Banks
Christine Chai
Lizzy Huang
Last built on 2022-06-15

2

Contents
Preface
5
1
The Basics of Bayesian Statistics
7
1.1
Bayesâ€™ Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2
Inference for a Proportion . . . . . . . . . . . . . . . . . . . . . .
15
1.3
Frequentist vs. Bayesian Inference
. . . . . . . . . . . . . . . . .
19
1.4
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2
Bayesian Inference
23
2.1
Continuous Variables and Eliciting Probability Distributions
. .
23
2.2
Three Conjugate Families . . . . . . . . . . . . . . . . . . . . . .
30
2.3
Credible Intervals and Predictive Inference . . . . . . . . . . . . .
37
3
Losses and Decision Making
43
3.1
Bayesian Decision Making . . . . . . . . . . . . . . . . . . . . . .
43
3.2
Loss Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.3
Working with Loss Functions . . . . . . . . . . . . . . . . . . . .
45
3.4
Minimizing Expected Loss for Hypothesis Testing . . . . . . . . .
50
3.5
Posterior Probabilities of Hypotheses and Bayes Factors . . . . .
52
4
Inference and Decision-Making with Multiple Parameters
57
4.1
The Normal-Gamma Conjugate Family
. . . . . . . . . . . . . .
58
4.2
Monte Carlo Inference . . . . . . . . . . . . . . . . . . . . . . . .
72
4.3
Predictive Distributions . . . . . . . . . . . . . . . . . . . . . . .
78
4.4
Reference Priors
. . . . . . . . . . . . . . . . . . . . . . . . . . .
83
4.5
Mixtures of Conjugate Priors . . . . . . . . . . . . . . . . . . . .
87
4.6
Markov Chain Monte Carlo (MCMC)
. . . . . . . . . . . . . . .
88
5
Hypothesis Testing with Normal Populations
93
5.1
Bayes Factors for Testing a Normal Mean: variance known . . . .
93
5.2
Comparing Two Paired Means using Bayes Factors . . . . . . . .
99
5.3
Comparing Independent Means: Hypothesis Testing
. . . . . . . 103
5.4
Inference after Testing . . . . . . . . . . . . . . . . . . . . . . . . 106
3

4
CONTENTS
6
Introduction to Bayesian Regression
113
6.1
Bayesian Simple Linear Regression . . . . . . . . . . . . . . . . . 113
6.2
Checking Outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
6.3
Bayesian Multiple Linear Regression . . . . . . . . . . . . . . . . 138
6.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
7
Bayesian Model Choice
147
7.1
Bayesian Information Criterion (BIC)
. . . . . . . . . . . . . . . 147
7.2
Bayesian Model Uncertainty . . . . . . . . . . . . . . . . . . . . . 154
7.3
Bayesian Model Averaging . . . . . . . . . . . . . . . . . . . . . . 158
7.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
8
Stochastic Explorations Using MCMC
165
8.1
Stochastic Exploration . . . . . . . . . . . . . . . . . . . . . . . . 165
8.2
Other Priors for Bayesian Model Uncertainty . . . . . . . . . . . 169
8.3
R Demo on BAS Package . . . . . . . . . . . . . . . . . . . . . . . 175
8.4
Decision Making Under Model Uncertainty
. . . . . . . . . . . . 186
8.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194

Preface
This book was written as a companion for the Course Bayesian Statistics from
the Statistics with R specialization available on Coursera. Our goal in develop-
ing the course was to provide an introduction to Bayesian inference in decision
making without requiring calculus, with the book providing more details and
background on Bayesian Inference.
In writing this, we hope that it may be
used on its own as an open-access introduction to Bayesian inference using R for
anyone interested in learning about Bayesian statistics. Materials and examples
from the course are discussed more extensively and extra examples and exer-
cises are provided. While learners are not expected to have any background in
calculus or linear algebra, for those who do have this background and are inter-
ested in diving deeper, we have included optional sub-sections in each Chapter
to provide additional mathematical details and some derivations of key results.
This book is written using the R package bookdown; any interested learners are
welcome to download the source code from github to see the code that was used
to create all of the examples and figures within the book. Learners should have
a current version of R (3.5.0 at the time of this version of the book) and will
need to install Rstudio in order to use any of the shiny apps.
Those that are interested in running all of the code in the book or building the
book locally, should download all of the following packages from CRAN:
# R packages used to create the book
library(statsr)
library(BAS)
library(ggplot2)
library(dplyr)
library(BayesFactor)
library(knitr)
library(rjags)
library(coda)
library(latex2exp)
library(foreign)
library(BHH2)
5

6
CONTENTS
library(scales)
library(logspline)
library(cowplot)
library(ggthemes)
We thank Amy Kenyon and Kun Li for all of their support in launching the
course on Coursera and Kyle Burris for contributions to lab exercises and quizzes
in earlier versions of the course.

Chapter 1
The Basics of Bayesian
Statistics
Bayesian statistics mostly involves conditional probability, which is the the
probability of an event A given event B, and it can be calculated using the
Bayes rule. The concept of conditional probability is widely used in medical
testing, in which false positives and false negatives may occur. A false positive
can be defined as a positive outcome on a medical test when the patient does
not actually have the disease they are being tested for. In other words, itâ€™s the
probability of testing positive given no disease. Similarly, a false negative can
be defined as a negative outcome on a medical test when the patient does have
the disease. In other words, testing negative given disease. Both indicators are
critical for any medical decisions.
For how the Bayesâ€™ rule is applied, we can set up a prior, then calculate pos-
terior probabilities based on a prior and likelihood. That is to say, the prior
probabilities are updated through an iterative process of data collection.
1.1
Bayesâ€™ Rule
This section introduces how the Bayesâ€™ rule is applied to calculating conditional
probability, and several real-life examples are demonstrated. Finally, we com-
pare the Bayesian and frequentist definition of probability.
1.1.1
Conditional Probabilities & Bayesâ€™ Rule
Consider Table 1.1. It shows the results of a poll among 1,738 adult Americans.
This table allows us to calculate probabilities.
For instance, the probability of an adult American using an online dating site
7

8
CHAPTER 1. THE BASICS OF BAYESIAN STATISTICS
Table 1.1: Results from a 2015 Gallup poll on the use of online dating sites by
age group
18-29
30-49
50-64
65+
Total
Used online dating site
60
86
58
21
225
Did not use online dating site
255
426
450
382
1513
Total
315
512
508
403
1738
can be calculated as
ğ‘ƒ(using an online dating site) =
Number that indicated they used an online dating site
Total number of people in the poll
= 225
1738 â‰ˆ13%.
This is the overall probability of using an online dating site. Say, we are now
interested in the probability of using an online dating site if one falls in the age
group 30-49. Similar to the above, we have
ğ‘ƒ(using an online dating site âˆ£in age group 30-49) =
Number in age group 30-49 that indicated they used an online dating site
Total number in age group 30-49
= 86
512 â‰ˆ17%.
Here, the pipe symbol â€˜|â€™ means conditional on. This is a conditional prob-
ability as one can consider it the probability of using an online dating site
conditional on being in age group 30-49.
We can rewrite this conditional probability in terms of â€˜regularâ€™ probabilities by
dividing both numerator and the denominator by the total number of people in
the poll. That is,
ğ‘ƒ(using an online dating site âˆ£in age group 30-49)
= Number in age group 30-49 that indicated they used an online dating site
Total number in age group 30-49
=
Number in age group 30-49 that indicated they used an online dating site
Total number of people in the poll
Total number in age group 30-49
Total number of people in the poll
= ğ‘ƒ(using an online dating site & falling in age group 30-49)
ğ‘ƒ(Falling in age group 30-49)
.
It turns out this relationship holds true for any conditional probability and is
known as Bayesâ€™ rule:
Definition 1.1 (Bayesâ€™ Rule). The conditional probability of the event ğ´con-
ditional on the event ğµis given by

1.1. BAYESâ€™ RULE
9
ğ‘ƒ(ğ´âˆ£ğµ) = ğ‘ƒ(ğ´& ğµ)
ğ‘ƒ(ğµ)
.
Example 1.1. What is the probability that an 18-29 year old from Table 1.1
uses online dating sites?
Note that the question asks a question about 18-29 year olds. Therefore, it
conditions on being 18-29 years old. Bayesâ€™ rule provides a way to compute this
conditional probability:
ğ‘ƒ(using an online dating site âˆ£in age group 18-29)
= ğ‘ƒ(using an online dating site & falling in age group 18-29)
ğ‘ƒ(Falling in age group 18-29)
=
Number in age group 18-29 that indicated they used an online dating site
Total number of people in the poll
Total number in age group 18-29
Total number of people in the poll
= Number in age group 18-29 that indicated they used an online dating site
Total number in age group 18-29
= 60
315 â‰ˆ19%.
1.1.2
Bayesâ€™ Rule and Diagnostic Testing
To better understand conditional probabilities and their importance, let us con-
sider an example involving the human immunodeficiency virus (HIV). In the
early 1980s, HIV had just been discovered and was rapidly expanding. There
was major concern with the safety of the blood supply. Also, virtually no cure
existed making an HIV diagnosis basically a death sentence, in addition to the
stigma that was attached to the disease.
These made false positives and false negatives in HIV testing highly undesirable.
A false positive is when a test returns postive while the truth is negative. That
would for instance be that someone without HIV is wrongly diagnosed with HIV,
wrongly telling that person they are going to die and casting the stigma on them.
A false negative is when a test returns negative while the truth is positive.
That is when someone with HIV undergoes an HIV test which wrongly comes
back negative. The latter poses a threat to the blood supply if that person is
about to donate blood.
The probability of a false positive if the truth is negative is called the false
positive rate.
Similarly, the false negative rate is the probability of a false
negative if the truth is positive.
Note that both these rates are conditional
probabilities: The false positive rate of an HIV test is the probability of a
positive result conditional on the person tested having no HIV.
The HIV test we consider is an enzyme-linked immunosorbent assay, commonly
known as an ELISA. We would like to know the probability that someone (in

10
CHAPTER 1. THE BASICS OF BAYESIAN STATISTICS
the early 1980s) has HIV if ELISA tests positive. For this, we need the following
information. ELISAâ€™s true positive rate (one minus the false negative rate), also
referred to as sensitivity, recall, or probability of detection, is estimated as
ğ‘ƒ(ELISA is positive âˆ£Person tested has HIV) = 93% = 0.93.
Its true negative rate (one minus the false positive rate), also referred to as
specificity, is estimated as
ğ‘ƒ(ELISA is negative âˆ£Person tested has no HIV) = 99% = 0.99.
Also relevant to our question is the prevalence of HIV in the overall population,
which is estimated to be 1.48 out of every 1000 American adults. We therefore
assume
ğ‘ƒ(Person tested has HIV) = 1.48
1000 = 0.00148.
(1.1)
Note that the above numbers are estimates. For our purposes, however, we will
treat them as if they were exact.
Our goal is to compute the probability of HIV if ELISA is positive, that is
ğ‘ƒ(Person tested has HIV âˆ£ELISA is positive). In none of the above numbers
did we condition on the outcome of ELISA. Fortunately, Bayesâ€™ rule allows is to
use the above numbers to compute the probability we seek. Bayesâ€™ rule states
that
ğ‘ƒ(Person tested has HIV âˆ£ELISA is positive) = ğ‘ƒ(Person tested has HIV & ELISA is positive)
ğ‘ƒ(ELISA is positive)
.
(1.2)
This can be derived as follows.
For someone to test positive and be
HIV positive, that person first needs to be HIV positive and then sec-
ondly test positive.
The probability of the first thing happening is
ğ‘ƒ(HIV positive) = 0.00148.
The probability of then testing positive is
ğ‘ƒ(ELISA is positive âˆ£Person tested has HIV) = 0.93, the true positive rate.
This yields for the numerator
ğ‘ƒ(Person tested has HIV & ELISA is positive)
= ğ‘ƒ(Person tested has HIV)ğ‘ƒ(ELISA is positive âˆ£Person tested has HIV)
= 0.00148 â‹…0.93 = 0.0013764.
(1.3)
The first step in the above equation is implied by Bayesâ€™ rule: By multiplying
the left- and right-hand side of Bayesâ€™ rule as presented in Section 1.1.1 by ğ‘ƒ(ğµ),
we obtain
ğ‘ƒ(ğ´âˆ£ğµ)ğ‘ƒ(ğµ) = ğ‘ƒ(ğ´& ğµ).

1.1. BAYESâ€™ RULE
11
The denominator in (1.2) can be expanded as
ğ‘ƒ(ELISA is positive)
= ğ‘ƒ(Person tested has HIV & ELISA is positive) + ğ‘ƒ(Person tested has no HIV & ELISA is positive)
= 0.0013764 + 0.0099852 = 0.0113616
where we used (1.3) and
ğ‘ƒ(Person tested has no HIV & ELISA is positive)
= ğ‘ƒ(Person tested has no HIV)ğ‘ƒ(ELISA is positive âˆ£Person tested has no HIV)
= (1 âˆ’ğ‘ƒ(Person tested has HIV)) â‹…(1 âˆ’ğ‘ƒ(ELISA is negative âˆ£Person tested has no HIV))
= (1 âˆ’0.00148) â‹…(1 âˆ’0.99) = 0.0099852.
Putting this all together and inserting into (1.2) reveals
ğ‘ƒ(Person tested has HIV âˆ£ELISA is positive) = 0.0013764
0.0113616 â‰ˆ0.12.
(1.4)
So even when the ELISA returns positive, the probability of having HIV is only
12%. An important reason why this number is so low is due to the prevalence
of HIV. Before testing, oneâ€™s probability of HIV was 0.148%, so the positive
test changes that probability dramatically, but it is still below 50%. That is, it
is more likely that one is HIV negative rather than positive after one positive
ELISA test.
Questions like the one we just answered (What is the probability of a disease if
a test returns positive?) are crucial to make medical diagnoses. As we saw, just
the true positive and true negative rates of a test do not tell the full story, but
also a diseaseâ€™s prevalence plays a role. Bayesâ€™ rule is a tool to synthesize such
numbers into a more useful probability of having a disease after a test result.
Example 1.2. What is the probability that someone who tests positive does
not actually have HIV?
We found in (1.4) that someone who tests positive has a 0.12 probability of
having HIV. That implies that the same person has a 1âˆ’0.12 = 0.88 probability
of not having HIV, despite testing positive.
Example 1.3. If the individual is at a higher risk for having HIV than a
randomly sampled person from the population considered, how, if at all, would
you expect ğ‘ƒ(Person tested has HIV âˆ£ELISA is positive) to change?

12
CHAPTER 1. THE BASICS OF BAYESIAN STATISTICS
If the person has a priori a higher risk for HIV and tests positive, then the prob-
ability of having HIV must be higher than for someone not at increased risk who
also tests positive. Therefore, ğ‘ƒ(Person tested has HIV âˆ£ELISA is positive) >
0.12 where 0.12 comes from (1.4).
One can derive this mathematically by plugging in a larger number in (1.1)
than 0.00148, as that number represents the prior risk of HIV. Changing the
calculations accordingly shows ğ‘ƒ(Person tested has HIV âˆ£ELISA is positive) >
0.12.
Example 1.4. If the false positive rate of the test is higher than 1%, how, if at
all, would you expect ğ‘ƒ(Person tested has HIV âˆ£ELISA is positive) to change?
If the false positive rate increases, the probability of a wrong positive result
increases. That means that a positive test result is more likely to be wrong and
thus less indicative of HIV. Therefore, the probability of HIV after a positive
ELISA goes down such that ğ‘ƒ(Person tested has HIV âˆ£ELISA is positive) <
0.12.
1.1.3
Bayes Updating
In the previous section, we saw that one positive ELISA test yields a probability
of having HIV of 12%.
To obtain a more convincing probability, one might
want to do a second ELISA test after a first one comes up positive. What is
the probability of being HIV positive if also the second ELISA test comes back
positive?
To solve this problem, we will assume that the correctness of this second test is
not influenced by the first ELISA, that is, the tests are independent from each
other. This assumption probably does not hold true as it is plausible that if the
first test was a false positive, it is more likely that the second one will be one
as well. Nonetheless, we stick with the independence assumption for simplicity.
In the last section, we used ğ‘ƒ(Person tested has HIV) = 0.00148, see (1.1), to
compute the probability of HIV after one positive test. If we repeat those steps
but now with ğ‘ƒ(Person tested has HIV) = 0.12, the probability that a person
with one positive test has HIV, we exactly obtain the probability of HIV after
two positive tests. Repeating the maths from the previous section, involving
Bayesâ€™ rule, gives

1.1. BAYESâ€™ RULE
13
ğ‘ƒ(Person tested has HIV âˆ£Second ELISA is also positive)
= ğ‘ƒ(Person tested has HIV)ğ‘ƒ(Second ELISA is positive âˆ£Person tested has HIV)
ğ‘ƒ(Second ELISA is also positive)
=
0.12 â‹…0.93
ğ‘ƒ(Person tested has HIV)ğ‘ƒ(Second ELISA is positive âˆ£Has HIV)
+ ğ‘ƒ(Person tested has no HIV)ğ‘ƒ(Second ELISA is positive âˆ£Has no HIV)
=
0.1116
0.12 â‹…0.93 + (1 âˆ’0.12) â‹…(1 âˆ’0.99) â‰ˆ0.93.
(1.5)
Since we are considering the same ELISA test, we used the same true positive
and true negative rates as in Section 1.1.2. We see that two positive tests makes
it much more probable for someone to have HIV than when only one test comes
up positive.
This process, of using Bayesâ€™ rule to update a probability based on an event
affecting it, is called Bayesâ€™ updating. More generally, the what one tries to
update can be considered â€˜priorâ€™ information, sometimes simply called the prior.
The event providing information about this can also be data. Then, updating
this prior using Bayesâ€™ rule gives the information conditional on the data, also
known as the posterior, as in the information after having seen the data.
Going from the prior to the posterior is Bayes updating.
The probability of HIV after one positive ELISA, 0.12, was the posterior in the
previous section as it was an update of the overall prevalence of HIV, (1.1).
However, in this section we answered a question where we used this posterior
information as the prior. This process of using a posterior as prior in a new
problem is natural in the Bayesian framework of updating knowledge based on
the data.
Example 1.5. What is the probability that one actually has HIV after test-
ing positive 3 times on the ELISA? Again, assume that all three ELISAs are
independent.
Analogous to what we did in this section, we can use Bayesâ€™ updating for this.
However, now the prior is the probability of HIV after two positive ELISAs, that
is ğ‘ƒ(Person tested has HIV) = 0.93. Analogous to (1.5), the answer follows as

14
CHAPTER 1. THE BASICS OF BAYESIAN STATISTICS
ğ‘ƒ(Person tested has HIV âˆ£Third ELISA is also positive)
= ğ‘ƒ(Person tested has HIV)ğ‘ƒ(Third ELISA is positive âˆ£Person tested has HIV)
ğ‘ƒ(Third ELISA is also positive)
=
0.93 â‹…0.93
ğ‘ƒ(Person tested has HIV)ğ‘ƒ(Third ELISA is positive âˆ£Has HIV)
+ğ‘ƒ(Person tested has no HIV)ğ‘ƒ(Third ELISA is positive âˆ£Has no HIV)
=
0.8649
0.93 â‹…0.93 + (1 âˆ’0.93) â‹…(1 âˆ’0.99) â‰ˆ0.999.
(1.6)
1.1.4
Bayesian vs. Frequentist Definitions of Probability
The frequentist definition of probability is based on observation of a large num-
ber of trials. The probability for an event ğ¸to occur is ğ‘ƒ(ğ¸), and assume we
get ğ‘›ğ¸successes out of ğ‘›trials. Then we have
ğ‘ƒ(ğ¸) = lim
ğ‘›â†’âˆ
ğ‘›ğ¸
ğ‘›.
(1.7)
On the other hand, the Bayesian definition of probability ğ‘ƒ(ğ¸) reflects our
prior beliefs, so ğ‘ƒ(ğ¸) can be any probability distribution, provided that it is
consistent with all of our beliefs.
(For example, we cannot believe that the
probability of a coin landing heads is 0.7 and that the probability of getting
tails is 0.8, because they are inconsistent.)
The two definitions result in different methods of inference. Using the frequentist
approach, we describe the confidence level as the proportion of random samples
from the same population that produced confidence intervals which contain the
true population parameter. For example, if we generated 100 random samples
from the population, and 95 of the samples contain the true parameter, then
the confidence level is 95%.
Note that each sample either contains the true
parameter or does not, so the confidence level is NOT the probability that a
given interval includes the true population parameter.
Example 1.6. Based on a 2015 Pew Research poll on 1,500 adults: â€œWe are
95% confident that 60% to 64% of Americans think the federal government does
not do enough for middle class people.
The correct interpretation is: 95% of random samples of 1,500 adults will pro-
duce confidence intervals that contain the true proportion of Americans who
think the federal government does not do enough for middle class people.
Here are two common misconceptions:

1.2. INFERENCE FOR A PROPORTION
15
â€¢ There is a 95% chance that this confidence interval includes the true pop-
ulation proportion.
â€¢ The true population proportion is in this interval 95% of the time.
The probability that a given confidence interval captures the true parameter
is either zero or one. To a frequentist, the problem is that one never knows
whether a specific interval contains the true value with probability zero or one.
So a frequentist says that â€œ95% of similarly constructed intervals contain the
true valueâ€.
The second (incorrect) statement sounds like the true proportion is a value that
moves around that is sometimes in the given interval and sometimes not in it.
Actually the true proportion is constant, itâ€™s the various intervals constructed
based on new samples that are different.
The Bayesian alternative is the credible interval, which has a definition that is
easier to interpret. Since a Bayesian is allowed to express uncertainty in terms
of probability, a Bayesian credible interval is a range for which the Bayesian
thinks that the probability of including the true value is, say, 0.95. Thus a
Bayesian can say that there is a 95% chance that the credible interval contains
the true parameter value.
Example 1.7. The posterior distribution yields a 95% credible interval of 60%
to 64% for the proportion of Americans who think the federal government does
not do enough for middle class people.
We can say that there is a 95% probability that the proportion is between 60%
and 64% because this is a credible interval, and more details will be introduced
later in the course.
1.2
Inference for a Proportion
1.2.1
Inference for a Proportion: Frequentist Approach
Example 1.8. RU-486 is claimed to be an effective â€œmorning afterâ€ contracep-
tive pill, but is it really effective?
Data: A total of 40 women came to a health clinic asking for emergency con-
traception (usually to prevent pregnancy after unprotected sex).
They were
randomly assigned to RU-486 (treatment) or standard therapy (control), 20 in
each group. In the treatment group, 4 out of 20 became pregnant. In the control
group, the pregnancy rate is 16 out of 20.
Question: How strongly do these data indicate that the treatment is more ef-
fective than the control?
To simplify the framework, letâ€™s make it a one proportion problem and just
consider the 20 total pregnancies because the two groups have the same sample

16
CHAPTER 1. THE BASICS OF BAYESIAN STATISTICS
size. If the treatment and control are equally effective, then the probability that
a pregnancy comes from the treatment group (ğ‘) should be 0.5. If RU-486 is
more effective, then the probability that a pregnancy comes from the treatment
group (ğ‘) should be less than 0.5.
Therefore, we can form the hypotheses as below:
â€¢ ğ‘= probability that a given pregnancy comes from the treatment group
â€¢ ğ»0 âˆ¶ğ‘= 0.5 (no difference, a pregnancy is equally likely to come from the
treatment or control group)
â€¢ ğ»ğ´âˆ¶ğ‘< 0.5 (treatment is more effective, a pregnancy is less likely to
come from the treatment group)
A p-value is needed to make an inference decision with the frequentist approach.
The definition of p-value is the probability of observing something at least as
extreme as the data, given that the null hypothesis (ğ»0) is true. â€œMore extremeâ€
means in the direction of the alternative hypothesis (ğ»ğ´).
Since ğ»0 states that the probability of success (pregnancy) is 0.5, we can cal-
culate the p-value from 20 independent Bernoulli trials where the probability of
success is 0.5. The outcome of this experiment is 4 successes in 20 trials, so the
goal is to obtain 4 or fewer successes in the 20 Bernoulli trials.
This probability can be calculated exactly from a binomial distribution with
ğ‘›= 20 trials and success probability ğ‘= 0.5. Assume ğ‘˜is the actual number
of successes observed, the p-value is
ğ‘ƒ(ğ‘˜â‰¤4) = ğ‘ƒ(ğ‘˜= 0) + ğ‘ƒ(ğ‘˜= 1) + ğ‘ƒ(ğ‘˜= 2) + ğ‘ƒ(ğ‘˜= 3) + ğ‘ƒ(ğ‘˜= 4)
.
sum(dbinom(0:4, size = 20, p = 0.5))
## [1] 0.005908966
According to R, the probability of getting 4 or fewer successes in 20 trials is
0.0059. Therefore, given that pregnancy is equally likely in the two groups,
we get the chance of observing 4 or fewer preganancy in the treatment group
is 0.0059.
With such a small probability, we reject the null hypothesis and
conclude that the data provide convincing evidence for the treatment being
more effective than the control.
1.2.2
Inference for a Proportion: Bayesian Approach
This section uses the same example, but this time we make the inference for
the proportion from a Bayesian approach. Recall that we still consider only
the 20 total pregnancies, 4 of which come from the treatment group.
The
question we would like to answer is that how likely is for 4 pregnancies to occur

1.2. INFERENCE FOR A PROPORTION
17
Table 1.2: Prior, likelihood, and posterior probabilities for each of the 9 models
Model (p)
0.1000
0.2000
0.3000
0.4000
0.5000
0.6000
0.70
0.80
0.90
Prior P(model)
0.0600
0.0600
0.0600
0.0600
0.5200
0.0600
0.06
0.06
0.06
Likelihood P(data|model)
0.0898
0.2182
0.1304
0.0350
0.0046
0.0003
0.00
0.00
0.00
P(data|model) x P(model)
0.0054
0.0131
0.0078
0.0021
0.0024
0.0000
0.00
0.00
0.00
Posterior P(model|data)
0.1748
0.4248
0.2539
0.0681
0.0780
0.0005
0.00
0.00
0.00
in the treatment group. Also remember that if the treatment and control are
equally effective, and the sample sizes for the two groups are the same, then the
probability (ğ‘) that the pregnancy comes from the treatment group is 0.5.
Within the Bayesian framework, we need to make some assumptions on the
models which generated the data. First, ğ‘is a probability, so it can take on any
value between 0 and 1. However, letâ€™s simplify by using discrete cases â€“ assume
ğ‘, the chance of a pregnancy comes from the treatment group, can take on nine
values, from 10%, 20%, 30%, up to 90%. For example, ğ‘= 20% means that
among 10 pregnancies, it is expected that 2 of them will occur in the treatment
group. Note that we consider all nine models, compared with the frequentist
paradigm that whe consider only one model.
Table 1.2 specifies the prior probabilities that we want to assign to our assump-
tion. There is no unique correct prior, but any prior probability should reflect
our beliefs prior to the experiement. The prior probabilities should incorpo-
rate the information from all relevant research before we perform the current
experiement.
This prior incorporates two beliefs: the probability of ğ‘= 0.5 is highest, and
the benefit of the treatment is symmetric. The second belief means that the
treatment is equally likely to be better or worse than the standard treatment.
Now it is natural to ask how I came up with this prior, and the specification
will be discussed in detail later in the course.
Next, letâ€™s calculate the likelihood â€“ the probability of observed data for each
model considered. In mathematical terms, we have
ğ‘ƒ(data|model) = ğ‘ƒ(ğ‘˜= 4|ğ‘›= 20, ğ‘)
The likelihood can be computed as a binomial with 4 successes and 20 trials
with ğ‘is equal to the assumed value in each model. The values are listed in
Table 1.2.
After setting up the prior and computing the likelihood, we are ready to calculate
the posterior using the Bayesâ€™ rule, that is,

18
CHAPTER 1. THE BASICS OF BAYESIAN STATISTICS
ğ‘ƒ(model|data) = ğ‘ƒ(model)ğ‘ƒ(data|model)
ğ‘ƒ(data)
The posterior probability values are also listed in Table 1.2, and the highest
probability occurs at ğ‘= 0.2, which is 42.48%. Note that the priors and poste-
riors across all models both sum to 1.
In decision making, we choose the model with the highest posterior probability,
which is ğ‘= 0.2. In comparison, the highest prior probability is at ğ‘= 0.5 with
52%, and the posterior probability of ğ‘= 0.5 drops to 7.8%. This demonstrates
how we update our beliefs based on observed data. Note that the calculation of
posterior, likelihood, and prior is unrelated to the frequentist concept (data â€œat
least as extreme as observedâ€).
Here are the histograms of the prior, the likelihood, and the posterior probabil-
ities:
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Prior
0.0
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Likelihood
0.00
0.05
0.10
0.15
0.20
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Posterior
0.0
0.1
0.2
0.3
0.4
Figure 1.1: Original: sample size ğ‘›= 20 and number of successes ğ‘˜= 4
We started with the high prior at ğ‘= 0.5, but the data likelihood peaks at
ğ‘= 0.2. And we updated our prior based on observed data to find the posterior.
The Bayesian paradigm, unlike the frequentist approach, allows us to make
direct probability statements about our models. For example, we can calculate
the probability that RU-486, the treatment, is more effective than the control as
the sum of the posteriors of the models where ğ‘< 0.5. Adding up the relevant
posterior probabilities in Table 1.2, we get the chance that the treatment is
more effective than the control is 92.16%.
1.2.3
Effect of Sample Size on the Posterior
The RU-486 example is summarized in Figure 1.1, and letâ€™s look at what the
posterior distribution would look like if we had more data.
Suppose our sample size was 40 instead of 20, and the number of successes was
8 instead of 4. Note that the ratio between the sample size and the number
of successes is still 20%. We will start with the same prior distribution. Then
calculate the likelihood of the data which is also centered at 0.20, but is less
variable than the original likelihood we had with the smaller sample size. And

1.3. FREQUENTIST VS. BAYESIAN INFERENCE
19
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Prior
0.0
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Likelihood
0.00
0.05
0.10
0.15
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Posterior
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 1.2: More data: sample size ğ‘›= 40 and number of successes ğ‘˜= 8
finally put these two together to obtain the posterior distribution. The posterior
also has a peak at p is equal to 0.20, but the peak is taller, as shown in Figure
1.2. In other words, there is more mass on that model, and less on the others.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Prior
0.0
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Likelihood
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Posterior
0.0
0.2
0.4
0.6
0.8
Figure 1.3: More data: sample size ğ‘›= 200 and number of successes ğ‘˜= 40
To illustrate the effect of the sample size even further, we are going to keep
increasing our sample size, but still maintain the the 20% ratio between the
sample size and the number of successes. So letâ€™s consider a sample with 200
observations and 40 successes. Once again, we are going to use the same prior
and the likelihood is again centered at 20% and almost all of the probability
mass in the posterior is at p is equal to 0.20. The other models do not have
zero probability mass, but theyâ€™re posterior probabilities are very close to zero.
Figure 1.3 demonstrates that as more data are collected, the likelihood
ends up dominating the prior. This is why, while a good prior helps, a bad
prior can be overcome with a large sample. However, itâ€™s important to note that
this will only work as long as we do not place a zero probability mass on any of
the models in the prior.
1.3
Frequentist vs. Bayesian Inference
1.3.1
Frequentist vs. Bayesian Inference
In this section, we will solve a simple inference problem using both frequentist
and Bayesian approaches. Then we will compare our results based on decisions

20
CHAPTER 1. THE BASICS OF BAYESIAN STATISTICS
based on the two methods, to see whether we get the same answer or not. If we
do not, we will discuss why that happens.
Example 1.9. We have a population of M&Mâ€™s, and in this population the
percentage of yellow M&Mâ€™s is either 10% or 20%. You have been hired as a
statistical consultant to decide whether the true percentage of yellow M&Mâ€™s is
10% or 20%.
Payoffs/losses: You are being asked to make a decision, and there are associated
payoff/losses that you should consider. If you make the correct decision, your
boss gives you a bonus. On the other hand, if you make the wrong decision,
you lose your job.
Data: You can â€œbuyâ€ a random sample from the population â€“ You pay $200 for
each M&M, and you must buy in $1,000 increments (5 M&Ms at a time). You
have a total of $4,000 to spend, i.e., you may buy 5, 10, 15, or 20 M&Ms.
Remark: Remember that the cost of making a wrong decision is high, so you
want to be fairly confident of your decision. At the same time, though, data
collection is also costly, so you donâ€™t want to pay for a sample larger than you
need. If you believe that you could actually make a correct decision using a
smaller sample size, you might choose to do so and save money and resources.
Letâ€™s start with the frequentist inference.
â€¢ Hypothesis: ğ»0 is 10% yellow M&Ms, and ğ»ğ´is >10% yellow M&Ms.
â€¢ Significance level: ğ›¼= 0.05.
â€¢ One sample: red, green, yellow, blue, orange
â€¢ Observed data: ğ‘˜= 1, ğ‘›= 5
â€¢ P-value: ğ‘ƒ(ğ‘˜â‰¥1|ğ‘›= 5, ğ‘= 0.10) = 1 âˆ’ğ‘ƒ(ğ‘˜= 0|ğ‘›= 5, ğ‘= 0.10) =
1 âˆ’0.905 â‰ˆ0.41
Note that the p-value is the probability of observed or more extreme outcome
given that the null hypothesis is true.
Therefore, we fail to reject ğ»0 and conclude that the data do not provide con-
vincing evidence that the proportion of yellow M&Mâ€™s is greater than 10%.
This means that if we had to pick between 10% and 20% for the proportion of
M&Mâ€™s, even though this hypothesis testing procedure does not actually con-
firm the null hypothesis, we would likely stick with 10% since we couldnâ€™t find
evidence that the proportion of yellow M&Mâ€™s is greater than 10%.
The Bayesian inference works differently as below.
â€¢ Hypotheses: ğ»1 is 10% yellow M&Ms, and ğ»2 is 20% yellow M&Ms.
â€¢ Prior: ğ‘ƒ(ğ»1) = ğ‘ƒ(ğ»2) = 0.5

1.3. FREQUENTIST VS. BAYESIAN INFERENCE
21
Table 1.3: Frequentist and Bayesian probabilities for larger sample sizes
Frequentist
Bayesian H_1
Bayesian H_2
Observed Data
P(k or more | 10% yellow)
P(10% yellow | n, k)
P(20% yellow | n, k)
n = 5, k = 1
0.41
0.45
0.55
n = 10, k = 2
0.26
0.39
0.61
n = 15, k = 3
0.18
0.34
0.66
n = 20, k = 4
0.13
0.29
0.71
â€¢ Sample: red, green, yellow, blue, orange
â€¢ Observed data: ğ‘˜= 1, ğ‘›= 5
â€¢ Likelihood:
ğ‘ƒ(ğ‘˜= 1|ğ»1) = ( 5
1 ) Ã— 0.10 Ã— 0.904 â‰ˆ0.33
ğ‘ƒ(ğ‘˜= 1|ğ»2) = ( 5
1 ) Ã— 0.20 Ã— 0.804 â‰ˆ0.41
â€¢ Posterior
ğ‘ƒ(ğ»1|ğ‘˜= 1) = ğ‘ƒ(ğ»1)ğ‘ƒ(ğ‘˜= 1|ğ»1)
ğ‘ƒ(ğ‘˜= 1)
=
0.5 Ã— 0.33
0.5 Ã— 0.33 + 0.5 Ã— 0.41 â‰ˆ0.45
ğ‘ƒ(ğ»2|ğ‘˜= 1) = 1 âˆ’0.45 = 0.55
The posterior probabilities of whether ğ»1 or ğ»2 is correct are close to each
other. As a result, with equal priors and a low sample size, it is diï¬€icult to
make a decision with a strong confidence, given the observed data. However,
ğ»2 has a higher posterior probability than ğ»1, so if we had to make a decision
at this point, we should pick ğ»2, i.e., the proportion of yellow M&Ms is 20%.
Note that this decision contradicts with the decision based on the frequentist
approach.
Table 1.3 summarizes what the results would look like if we had chosen larger
sample sizes. Under each of these scenarios, the frequentist method yields a
higher p-value than our significance level, so we would fail to reject the null
hypothesis with any of these samples. On the other hand, the Bayesian method
always yields a higher posterior for the second model where ğ‘is equal to 0.20.
So the decisions that we would make are contradictory to each other.
However, if we had set up our framework differently in the frequentist method
and set our null hypothesis to be ğ‘= 0.20 and our alternative to be ğ‘< 0.20,
we would obtain different results. This shows that the frequentist method is

22
CHAPTER 1. THE BASICS OF BAYESIAN STATISTICS
highly sensitive to the null hypothesis, while in the Bayesian method, our
results would be the same regardless of which order we evaluate our models.
1.4
Exercises
1. Conditioning on dating site usage. Recall Table 1.1. What is the
probability that an online dating site user from this sample is 18-29 years
old?
2. Probability of no HIV. Consider the ELISA test from Section 1.1.2.
What is the probability that someone has no HIV if that person has a neg-
ative ELISA result? How does this compare to the probability of having
no HIV before any test was done?
3. Probability of no HIV after contradictive tests.
Consider the
ELISA test from Section 1.1.2.
What is the probability that someone
has no HIV if that person first tests positive on the ELISA and secondly
test negative? Assume that the tests are independent from each other.

Chapter 2
Bayesian Inference
This chapter is focused on the continuous version of Bayesâ€™ rule and how to
use it in a conjugate family.
The RU-486 example will allow us to discuss
Bayesian modeling in a concrete way.
It also leads naturally to a Bayesian
analysis without conjugacy.
For the non-conjugate case, there is usually no
simple mathematical expression, and one must resort to computation. Finally,
we discuss credible intervals, i.e., the Bayesian analog of frequentist confidence
intervals, and Bayesian estimation and prediction.
It is assumed that the readers have mastered the concept of conditional proba-
bility and the Bayesâ€™ rule for discrete random variables. Calculus is not required
for this chapter; however, for those who do, we shall briefly look at an integral.
2.1
Continuous Variables and Eliciting Proba-
bility Distributions
We are going to introduce continuous variables and how to elicit probability
distributions, from a prior belief to a posterior distribution using the Bayesian
framework.
2.1.1
From the Discrete to the Continuous
This section leads the reader from the discrete random variable to continuous
random variables. Letâ€™s start with the binomial random variable such as the
number of heads in ten coin tosses, can only take a discrete number of values:
0, 1, 2, up to 10.
When the probability of a coin landing heads is ğ‘, the chance of getting ğ‘˜heads
in ğ‘›tosses is
23

24
CHAPTER 2. BAYESIAN INFERENCE
ğ‘ƒ(ğ‘‹= ğ‘˜) = ( ğ‘›
ğ‘˜) ğ‘ğ‘˜(1 âˆ’ğ‘)ğ‘›âˆ’ğ‘˜
.
This formula is called the probability mass function (pmf) for the binomial.
The probability mass function can be visualized as a histogram in Figure 2.1.
The area under the histogram is one, and the area of each bar is the probability
of seeing a binomial random variable, whose value is equal to the x-value at the
center of the bars base.
0.0
0.1
0.2
0.3
0
1
2
3
Figure 2.1: Histogram of binomial random variable
In contrast, the normal distribution, a.k.a. Gaussian distribution or the bell-
shaped curve, can take any numerical value in (âˆ’âˆ, +âˆ). A random variable
generated from a normal distribution because it can take a continuum of values.
In general, if the set of possible values a random variable can take are separated
points, it is a discrete random variable. But if it can take any value in some
(possibly infinite) interval, then it is a continuous random variable.
When the random variable is discrete, it has a probability mass function
or pmf. That pmf tells us the probability that the random variable takes each
of the possible values.
But when the random variable is continuous, it has
probability zero of taking any single value. (Hence probability zero does not
equal to impossible, an event of probabilty zero can still happen.)
We can only talk about the probability of a continuous random variable lined
within some interval.
For example, suppose that heights are approximately
normally distributed. The probability of finding someone who is exactly 6 feet
and 0.0000 inches tall (for an infinite number of 0s after the decimal point) is 0.

2.1. CONTINUOUS VARIABLES AND ELICITING PROBABILITY DISTRIBUTIONS25
But we can easily calculate the probability of finding someone who is between
5â€™11â€ inches tall and 6â€™1â€ inches tall.
A continuous random variable has a probability density function or pdf,
instead of probability mass functions. The probability of finding someone whose
height lies between 5â€™11â€ (71 inches) and 6â€™1â€ (73 inches) is the area under the
pdf curve for height between those two values, as shown in the blue area of
Figure 2.2.
0.00
0.05
0.10
0.15
0.20
65
70
75
80
Height (inches)
Probability Density
Figure 2.2: Area under curve for the probability density function
For example, a normal distribution with mean ğœ‡and standard deviation ğœ(i.e.,
variance ğœ2) is defined as
ğ‘“(ğ‘¥) =
1
âˆš
2ğœ‹ğœ2 exp[âˆ’1
2ğœ2 (ğ‘¥âˆ’ğœ‡)2],
where ğ‘¥is any value the random variable ğ‘‹can take.
This is denoted as
ğ‘‹âˆ¼ğ‘(ğœ‡, ğœ2), where ğœ‡and ğœ2 are the parameters of the normal distribution.
Recall that a probability mass function assigns the probability that a random
variable takes a specific value for the discrete set of possible values. The sum
of those probabilities over all possible values must equal one.
Similarly, a probability density function is any ğ‘“(ğ‘¥) that is non-negative and
has area one underneath its curve. The pdf can be regarded as the limit of
histograms made from its sample data. As the sample size becomes infinitely
large, the bin width of the histogram shrinks to zero.
There are infinite number of pmfâ€™s and an infinite number of pdfâ€™s.
Some
distributions are so important that they have been given names:
â€¢ Continuous: normal, uniform, beta, gamma

26
CHAPTER 2. BAYESIAN INFERENCE
â€¢ Discrete: binomial, Poisson
Here is a summary of the key ideas in this section:
1. Continuous random variables exist and they can take any value within
some possibly infinite range.
2. The probability that a continuous random variable takes a specific value
is zero.
3. Probabilities from a continuous random variable are determined by the
density function with this non-negative and the area beneath it is one.
4. We can find the probability that a random variable lies between two values
(ğ‘and ğ‘‘) as the area under the density function that lies between them.
2.1.2
Elicitation
Next, we introduce the concept of prior elicitation in Bayesian statistics. Often,
one has a belief about the distribution of oneâ€™s data. You may think that your
data come from a binomial distribution and in that case you typically know
the ğ‘›, the number of trials but you usually do not know ğ‘, the probability of
success. Or you may think that your data come from a normal distribution. But
you do not know the mean ğœ‡or the standard deviation ğœof the normal. Beside
to knowing the distribution of oneâ€™s data, you may also have beliefs about the
unknown ğ‘in the binomial or the unknown mean ğœ‡in the normal.
Bayesians express their belief in terms of personal probabilities. These personal
probabilities encapsulate everything a Bayesian knows or believes about the
problem. But these beliefs must obey the laws of probability, and be consistent
with everything else the Bayesian knows.
::: {.example #200percent} You cannot say that your probability of passing
this course is 200%, no matter how confident you are. A probability value must
be between zero and one. (If you still think you have a probability of 200% to
pass the course, you are definitely not going to pass it.) :::
Example 2.1. You may know nothing at all about the value of ğ‘that generated
some binomial data. In which case any value between zero and one is equally
likely, you may want to make an inference on the proportion of people who
would buy a new band of toothpaste. If you have industry experience, you may
have a strong belief about the value of ğ‘, but if you are new to the industry you
would do nothing about ğ‘. In any value between zero and one seems equally
like a deal. This major personal probability is the uniform distribution whose
probably density function is flat, denoted as Unif(0, 1).
Example 2.2. If you were tossing a coin, most people believed that the prob-
ability of heads is pretty close to half. They know that some coins are biased
and that some coins may have two heads or two tails. And they probably also

2.1. CONTINUOUS VARIABLES AND ELICITING PROBABILITY DISTRIBUTIONS27
know that coins are not perfectly balanced. Nonetheless, before they start to
collect data by tossing the coin and counting the number of heads their belief
is that values of ğ‘near 0.5 are very likely, whereas values of ğ‘near 0 or 1 are
very unlikely.
Example 2.3. In real life, here are two ways to elicit a probability that you
cousin will get married. A frequentist might go to the U.S. Census records and
determine what proportion of people get married (or, better, what proportion
of people of your cousinâ€™s ethnicity, education level, religion, and age cohort are
married). In contrast, a Bayesian might think â€œMy cousin is brilliant, attractive,
and fun. The probability that my cousin gets married is really high â€“ probably
around 0.97.â€
So a Bayesian will seek to express their belief about the value of ğ‘through
a probability distribution, and a very flexible family of distributions for this
purpose is the beta family. A member of the beta family is specified by two
parameters, ğ›¼and ğ›½; we denote this as ğ‘âˆ¼beta(ğ›¼, ğ›½). The probability density
function is
ğ‘“(ğ‘) = Î“(ğ›¼+ ğ›½)
Î“(ğ›¼)Î“(ğ›½)ğ‘ğ›¼âˆ’1(1 âˆ’ğ‘)ğ›½âˆ’1,
(2.1)
where 0 â‰¤ğ‘â‰¤1, ğ›¼> 0, ğ›½> 0, and Î“ is a factorial:
Î“(ğ‘›) = (ğ‘›âˆ’1)! = (ğ‘›âˆ’1) Ã— (ğ‘›âˆ’2) Ã— â‹¯Ã— 1
When ğ›¼= ğ›½= 1, the beta distribution becomes a uniform distribution, i.e. the
probabilty density function is a flat line. In other words, the uniform distribution
is a special case of the beta family.
The expected value of ğ‘is
ğ›¼
ğ›¼+ğ›½, so ğ›¼can be regarded as the prior number of
successes, and ğ›½the prior number of failures. When ğ›¼= ğ›½, then one gets a
symmetrical pdf around 0.5. For large but equal values of ğ›¼and ğ›½, the area
under the beta probability density near 0.5 is very large. Figure 2.3 compares
the beta distribution with different parameter values.
These kinds of priors are probably appropriate if you want to infer the prob-
ability of getting heads in a coin toss. The beta family also includes skewed
densities, which is appropriate if you think that ğ‘the probability of success in
ths binomial trial is close to zero or one.
Bayesâ€™ rule is a machine to turn oneâ€™s prior beliefs into posterior beliefs. With
binomial data you start with whatever beliefs you may have about ğ‘, then you
observe data in the form of the number of head, say 20 tosses of a coin with 15
heads.
Next, Bayesâ€™ rule tells you how the data changes your opinion about ğ‘. The same
principle applies to all other inferences. You start with your prior probability

28
CHAPTER 2. BAYESIAN INFERENCE
0
1
2
3
4
5
0.00
0.25
0.50
0.75
1.00
p
Probability Density
Beta Distributions
Î± = 1, Î² = 1
Î± = 0.5, Î² = 0.5
Î± = 5, Î² = 1
Î± = 1, Î² = 3
Î± = 2, Î² = 2
Î± = 2, Î² = 5
Figure 2.3: Beta family
distribution over some parameter, then you use data to update that distribution
to become the posterior distribution that expresses your new belief.
These rules ensure that the change in distributions from prior to posterior is the
uniquely rational solution. So, as long as you begin with the prior distribution
that reflects your true opinion, you can hardly go wrong.
However, expressing that prior can be diï¬€icult. There are proofs and methods
whereby a rational and coherent thinker can self-illicit their true prior distribu-
tion, but these are impractical and people are rarely rational and coherent.
The good news is that with the few simple conditions no matter what part
distribution you choose. If enough data are observed, you will converge to an
accurate posterior distribution. So, two bayesians, say the reference Thomas
Bayes and the agnostic Ajay Good can start with different priors but, observe
the same data. As the amount of data increases, they will converge to the same
posterior distribution.
Here is a summary of the key ideas in this section:
1. Bayesians express their uncertainty through probability distributions.
2. One can think about the situation and self-elicit a probability distribution
that approximately reflects his/her personal probability.
3. Oneâ€™s personal probability should change according Bayesâ€™ rule, as new
data are observed.
4. The beta family of distribution can describe a wide range of prior beliefs.

2.1. CONTINUOUS VARIABLES AND ELICITING PROBABILITY DISTRIBUTIONS29
2.1.3
Conjugacy
Next, letâ€™s introduce the concept of conjugacy in Bayesian statistics.
Suppose we have the prior beliefs about the data as below:
â€¢ Binomial distribution Bin(ğ‘›, ğ‘) with ğ‘›known and ğ‘unknown
â€¢ Prior belief about ğ‘is beta(ğ›¼, ğ›½)
Then we observe ğ‘¥success in ğ‘›trials, and it turns out the Bayesâ€™ rule implies
that our new belief about the probability density of ğ‘is also the beta distribu-
tion, but with different parameters. In mathematical terms,
ğ‘|ğ‘¥âˆ¼beta(ğ›¼+ ğ‘¥, ğ›½+ ğ‘›âˆ’ğ‘¥).
(2.2)
This is an example of conjugacy. Conjugacy occurs when the posterior dis-
tribution is in the same family of probability density functions as the prior
belief, but with new parameter values, which have been updated to reflect
what we have learned from the data.
Why are the beta binomial families conjugate? Here is a mathematical expla-
nation.
Recall the discrete form of the Bayesâ€™ rule:
ğ‘ƒ(ğ´ğ‘–|ğµ) =
ğ‘ƒ(ğµ|ğ´ğ‘–)ğ‘ƒ(ğ´ğ‘–)
âˆ‘
ğ‘›
ğ‘—=1 ğ‘ƒ(ğµ|ğ´ğ‘—)ğ‘ƒ(ğ´ğ‘—)
However, this formula does not apply to continuous random variables, such as
the ğ‘which follows a beta distribution, because the denominator sums over all
possible values (must be finitely many) of the random variable.
But the good news is that the ğ‘has a finite range â€“ it can take any value only
between 0 and 1. Hence we can perform integration, which is a generalization
of the summation. The Bayesâ€™ rule can also be written in continuous form as:
ğœ‹âˆ—(ğ‘|ğ‘¥) =
ğ‘ƒ(ğ‘¥|ğ‘)ğœ‹(ğ‘)
âˆ«
1
0 ğ‘ƒ(ğ‘¥|ğ‘)ğœ‹(ğ‘)ğ‘‘ğ‘
.
This is analogus to the discrete form, since the integral in the denominator will
also be equal to some constant, just like a summation. This constant ensures
that the total area under the curve, i.e. the posterior density function, equals 1.
Note that in the numerator, the first term, ğ‘ƒ(ğ‘¥|ğ‘), is the data likelihood â€“ the
probability of observing the data given a specific value of ğ‘. The second term,
ğœ‹(ğ‘), is the probability density function that reflects the prior belief about ğ‘.
In the beta-binomial case, we have ğ‘ƒ(ğ‘¥|ğ‘) = Bin(ğ‘›, ğ‘) and ğœ‹(ğ‘) = beta(ğ›¼, ğ›½).

30
CHAPTER 2. BAYESIAN INFERENCE
Plugging in these distributions, we get
ğœ‹âˆ—(ğ‘|ğ‘¥) =
1
some number Ã— ğ‘ƒ(ğ‘¥|ğ‘)ğœ‹(ğ‘)
=
1
some number[( ğ‘›
ğ‘¥) ğ‘ğ‘¥(1 âˆ’ğ‘)ğ‘›âˆ’ğ‘¥][ Î“(ğ›¼+ ğ›½)
Î“(ğ›¼)Î“(ğ›½)ğ‘ğ›¼âˆ’1(1 âˆ’ğ‘)ğ›½âˆ’1]
=
Î“(ğ›¼+ ğ›½+ ğ‘›)
Î“(ğ›¼+ ğ‘¥)Î“(ğ›½+ ğ‘›âˆ’ğ‘¥) Ã— ğ‘ğ›¼+ğ‘¥âˆ’1(1 âˆ’ğ‘)ğ›½+ğ‘›âˆ’ğ‘¥âˆ’1
Let ğ›¼âˆ—= ğ›¼+ ğ‘¥and ğ›½âˆ—= ğ›½+ ğ‘›âˆ’ğ‘¥, and we get
ğœ‹âˆ—(ğ‘|ğ‘¥) = beta(ğ›¼âˆ—, ğ›½âˆ—) = beta(ğ›¼+ ğ‘¥, ğ›½+ ğ‘›âˆ’ğ‘¥),
same as the posterior formula in Equation (2.2).
We can recognize the posterior distribution from the numerator ğ‘ğ›¼+ğ‘¥âˆ’1 and
(1 âˆ’ğ‘)ğ›½+ğ‘›âˆ’ğ‘¥âˆ’1.
Everything else are just constants, and they must take the
unique value, which is needed to ensure that the area under the curve between
0 and 1 equals 1.
So they have to take the values of the beta, which has
parameters ğ›¼+ ğ‘¥and ğ›½+ ğ‘›âˆ’ğ‘¥.
This is a cute trick. We can find the answer without doing the integral simply
by looking at the form of the numerator.
Without conjugacy, one has to do the integral. Often, the integral is impossible
to evaluate. That obstacle is the primary reason that most statistical theory in
the 20th century was not Bayesian. The situation did not change until modern
computing allowed researchers to compute integrals numerically.
In summary, some pairs of distributions are conjugate. If your prior is in one
and your data comes from the other, then your posterior is in the same family
as the prior, but with new parameters. We explored this in the context of the
beta-binomial conjugate families. And we saw that conjugacy meant that we
could apply the continuous version of Bayesâ€™ rule without having to do any
integration.
2.2
Three Conjugate Families
In this section, the three conjugate families are beta-binomial, gamma-Poisson,
and normal-normal pairs. Each of them has its own applications in everyday
life.
2.2.1
Inference on a Binomial Proportion
Example 2.4. Recall Example 1.8, a simplified version of a real clinical trial
taken in Scotland. It concerned RU-486, a morning after pill that was being

2.2. THREE CONJUGATE FAMILIES
31
studied to determine whether it was effective at preventing unwanted pregnan-
cies. It had 800 women, each of whom had intercourse no more than 72 hours
before reporting to a family planning clinic to seek contraception.
Half of these women were randomly assigned to the standard contraceptive, a
large dose of estrogen and progesterone. And half of the women were assigned
RU-486. Among the RU-486 group, there were no pregnancies. Among those
receiving the standard therapy, four became pregnant.
Statistically, one can model these data as coming from a binomial distribution.
Imagine a coin with two sides. One side is labeled standard therapy and the
other is labeled RU-486. The coin was tossed four times, and each time it landed
with the standard therapy side face up.
A frequentist would analyze the problem as below:
â€¢ The parameter ğ‘is the probability of a preganancy comes from the stan-
dard treatment.
â€¢ ğ»0 âˆ¶ğ‘â‰¥0.5 and ğ»ğ´âˆ¶ğ‘< 0.5
â€¢ The p-value is 0.54 = 0.0625 > 0.05
Therefore, the frequentist fails to reject the null hypothesis, and will not con-
clude that RU-486 is superior to standard therapy.
Remark: The significance probability, or p-value, is the chance of observing data
that are as or more supportive of the alternative hypothesis than the data that
were collected, when the null hypothesis is true.
Now suppose a Bayesian performed the analysis. She may set her beliefs about
the drug and decide that she has no prior knowledge about the eï¬€icacy of RU-
486 at all. This would be reasonable if, for example, it were the first clinical
trial of the drug. In that case, she would be using the uniform distribution
on the interval from 0 to 1, which corresponds to the beta(1, 1) density. In
mathematical terms,
ğ‘âˆ¼Unif(0, 1) = beta(1, 1).
From conjugacy, we know that since there were four failures for RU-486 and no
successes, that her posterior probability of an RU-486 child is
ğ‘|ğ‘¥âˆ¼beta(1 + 0, 1 + 4) = beta(1, 5).
This is a beta that has much more area near ğ‘equal to 0. The mean of beta(ğ›¼, ğ›½)
is
ğ›¼
ğ›¼+ğ›½. So this Bayesian now believes that the unknown ğ‘, the probability of
an RU-468 child, is about 1 over 6.

32
CHAPTER 2. BAYESIAN INFERENCE
The standard deviation of a beta distribution with parameters in alpha and
beta also has a closed form:
ğ‘âˆ¼beta(ğ›¼, ğ›½) â‡’Standard deviation = âˆš
ğ›¼ğ›½
(ğ›¼+ ğ›½)2(ğ›¼+ ğ›½+ 1)
Before she saw the data, the Bayesianâ€™s uncertainty expressed by her standard
deviation was 0.71. After seeing the data, it was much reduced â€“ her posterior
standard deviation is just 0.13.
We promised not to do much calculus, so I hope you will trust me to tell you
that this Bayesian now believes that her posterior probability that ğ‘< 0.5 is
0.96875. She thought there was a 50-50 chance that RU-486 is better. But now
she thinks there is about a 97% chance that RU-486 is better.
Suppose a fifth child were born, also to a mother who received standard chip
therapy. Now the Bayesianâ€™s prior is beta(1, 5) and the additional data point
further updates her to a new posterior beta of 1 and 6. As data comes in,
the Bayesianâ€™s previous posterior becomes her new prior, so learning
is self-consistent.
This example has taught us several things:
1. We saw how to build a statistical model for an applied problem.
2. We could compare the frequentist and Bayesian approaches to inference
and see large differences in the conclusions.
3. We saw how the data changed the Bayesianâ€™s opinion with a new mean
for p and less uncertainty.
4. We learned that Bayesianâ€™s continually update as new data arrive. Yes-
terdayâ€™s posterior is todayâ€™s prior.
2.2.2
The Gamma-Poisson Conjugate Families
A second important case is the gamma-Poisson conjugate families. In this case
the data come from a Poisson distribution, and the prior and posterior are both
gamma distributions.
The Poisson random variable can take any non-negative integer value all
the way up to infinity. It is used in describing count data, where one counts
the number of independent events that occur in a fixed amount of time, a fixed
area, or a fixed volume.
Moreover, the Poisson distribution has been used to describe the number of
phone calls one receives in an hour. Or, the number of pediatric cancer cases in
the city, for example, to see if pollution has elevated the cancer rate above that
of in previous years or for similar cities. It is also used in medical screening for

2.2. THREE CONJUGATE FAMILIES
33
diseases, such as HIV, where one can count the number of T-cells in the tissue
sample.
The Poisson distribution has a single parameter ğœ†, and it is denoted as ğ‘‹âˆ¼
Pois(ğœ†) with ğœ†> 0. The probability mass function is
ğ‘ƒ(ğ‘‹= ğ‘˜) = ğœ†ğ‘˜
ğ‘˜! ğ‘’âˆ’ğœ†for ğ‘˜= 0, 1, â‹¯,
where ğ‘˜! = ğ‘˜Ã—(ğ‘˜âˆ’1)Ã—â‹¯Ã—1. This gives the probability of observing a random
variable equal to ğ‘˜.
Note that ğœ†is both the mean and the variance of the Poisson random variable.
It is obvious that ğœ†must be greater than zero, because it represents the mean
number of counts, and the variance should be greater than zero (except for
constants, which have zero variance).
Example 2.5. Famously, von Bortkiewicz used the Poisson distribution to
study the number of Prussian cavalrymen who were kicked to death by a horse
each year. This is count data over the course of a year, and the events are
probably independent, so the Poisson model makes sense.
He had data on 15 cavalry units for the 20 years between 1875 and 1894, inclu-
sive. The total number of cavalrymen who died by horse kick was 200.
One can imagine that a Prussian general might want to estimate ğœ†. The average
number per year, per unit. Perhaps in order to see whether some educational
campaign about best practices for equine safety would make a difference.
Suppose the Prussian general is a Bayesian. Introspective elicitation leads him
to think that ğœ†= 0.75 and standard deviation 1.
Modern computing was unavailable at that time yet, so the Prussian general
will need to express his prior as a member of a family conjugate to the Poisson.
It turns out that this family consists of the gamma distributions.
Gamma
distributions describe continuous non-negative random variables. As we know,
the value of ğœ†in the Poisson can take any non-negative value so this fits.
The gamma family is flexible, and Figure 2.4 illustrates a wide range of gamma
shapes.
The probability density function for the gamma is indexed by shape ğ‘˜and scale
ğœƒ, denoted as Gamma(ğ‘˜, ğœƒ) with ğ‘˜, ğœƒ> 0. The mathematical form of the density
is
ğ‘“(ğ‘¥) =
1
Î“(ğ‘˜)ğœƒğ‘˜ğ‘¥ğ‘˜âˆ’1ğ‘’âˆ’ğ‘¥/ğœƒ
(2.3)
where

34
CHAPTER 2. BAYESIAN INFERENCE
0.0
0.1
0.2
0.3
0.4
0.5
0
5
10
15
20
x
Probability Density
Gamma Distributions
k = 1, Î¸ = 2
k = 2, Î¸ = 2
k = 3, Î¸ = 2
k = 5, Î¸ = 1
k = 9, Î¸ = 0.5
k = 7.5, Î¸ = 1
k = 0.5, Î¸ = 1
Figure 2.4: Gamma family
Î“(ğ‘§) = âˆ«
âˆ
0
ğ‘¥ğ‘§âˆ’1ğ‘’âˆ’ğ‘¥ğ‘‘ğ‘¥.
(2.4)
Î“(ğ‘§), the gamma function, is simply a constant that ensures the area under
curve between 0 and 1 sums to 1, just like in the beta probability distribution
case of Equation (2.1). A special case is that Î“(ğ‘›) = (ğ‘›âˆ’1)! when ğ‘›is a positive
integer.
However, some books parameterize the gamma distribution in a slightly different
way with shape ğ›¼= ğ‘˜and rate (inverse scale) ğ›½= 1/ğœƒ:
ğ‘“(ğ‘¥) =
ğ›½ğ›¼
Î“(ğ›¼)ğ‘¥ğ›¼âˆ’1ğ‘’âˆ’ğ›½ğ‘¥
For this example, we use the ğ‘˜-ğœƒparameterization, but you should always check
which parameterization is being used. For example, R uses the ğ›¼-ğ›½parameter-
ization by default.
In the the later material we find that using the rate parameterization is more
convenient.
For our parameterization, the mean of Gamma(ğ‘˜, ğœƒ) is ğ‘˜ğœƒ, and the variance is
ğ‘˜ğœƒ2. We can get the generalâ€™s prior as below:
Mean = ğ‘˜ğœƒ= 0.75
Standard deviation = ğœƒ
âˆš
ğ‘˜= 1

2.2. THREE CONJUGATE FAMILIES
35
Table 2.1: Before and after seeing the data
lambda
Standard Deviation
Before
0.75
1.000
After
0.67
0.047
Hence
ğ‘˜= 9
16 and ğœƒ= 4
3
For the gamma Poisson conjugate family, suppose we observed data ğ‘¥1, ğ‘¥2, â‹¯, ğ‘¥ğ‘›
that follow a Poisson distribution.Then similar to the previous section, we would
recognize the kernel of the gamma when using the gamma-Poisson family. The
posterior Gamma(ğ‘˜âˆ—, ğœƒâˆ—) has parameters
ğ‘˜âˆ—= ğ‘˜+
ğ‘›
âˆ‘
ğ‘–=1
ğ‘¥ğ‘–and ğœƒâˆ—=
ğœƒ
(ğ‘›ğœƒ+ 1).
For this dataset, ğ‘= 15Ã—20 = 300 observations, and the number of casualities
is 200. Therefore, the general now thinks that the average number of Prussian
cavalry oï¬€icers who die at the hoofs of their horses follows a gamma distribution
with the parameters below:
ğ‘˜âˆ—= ğ‘˜+
ğ‘›
âˆ‘
ğ‘–=1
ğ‘¥ğ‘–= 9
16 + 200 = 200.5625
ğœƒâˆ—=
ğœƒ
(ğ‘›ğœƒ+ 1) =
4/3
300 Ã— (4/3) = 0.0033
How the general has changed his mind is described in Table 2.1. After seeing the
data, his uncertainty about lambda, expressed as a standard deviation, shrunk
from 1 to 0.047.
In summary, we learned about the Poisson and gamma distributions; we also
knew that the gamma-Poisson families are conjugate. Moreover, we learned the
updating fomula, and applied it to a classical dataset.
2.2.3
The Normal-Normal Conjugate Families
There are other conjugate families, and one is the normal-normal pair. If your
data come from a normal distribution with known variance ğœ2 but unknown
mean ğœ‡, and if your prior on the mean ğœ‡, has a normal distribution with self-
elicited mean ğœˆand self-elicited variance ğœ2, then your posterior density for the
mean, after seeing a sample of size ğ‘›with sample mean
Ì„ğ‘¥, is also normal. In
mathematical notation, we have

36
CHAPTER 2. BAYESIAN INFERENCE
ğ‘¥|ğœ‡âˆ¼ğ‘(ğœ‡, ğœ2)
ğœ‡âˆ¼ğ‘(ğœˆ, ğœ2)
As a practical matter, one often does not know ğœ2, the standard deviation of the
normal from which the data come. In that case, you could use a more advanced
conjugate family that we will describe in 4.1. But there are cases in which it is
reasonable to treat the ğœ2 as known.
Example 2.6. An analytical chemist whose balance produces measurements
that are normally distributed with mean equal to the true mass of the sample
and standard deviation that has been estimated by the manufacturer balance
and confirmed against calibration standards provided by the National Institute
of Standards and Technology.
Note that this normal-normal assumption made by the anayltical chemist is
technically wrong, but still reasonable.
1. The normal family puts some probability on all possible values between
(âˆ’âˆ, +âˆ). But the mass on the balance can never be negative. How-
ever, the normal prior on the unknown mass is usually so concentrated on
positive values that the normal distribution is still a good approximation.
2. Even if the chemist has repeatedly calibrated her balance with standards
from the National Institute of Standards and Technology, she still will not
know its standard deviation precisely. However, if she has done it often
and well, it is probably a suï¬€iciently good approximation to assume that
the standard deviation is known.
For the normal-normal conjugate families, assume the prior on the unknown
mean follows a normal distribution, i.e. ğœ‡âˆ¼ğ‘(ğœˆ, ğœ2). We also assume that the
data ğ‘¥1, ğ‘¥2, â‹¯, ğ‘¥ğ‘›are independent and come from a normal with variance ğœ2.
Then the posterior distribution of ğœ‡is also normal, with mean as a weighted
average of the prior mean and the sample mean. We have
ğœ‡|ğ‘¥1, ğ‘¥2, â‹¯, ğ‘¥ğ‘›âˆ¼ğ‘(ğœˆâˆ—, ğœâˆ—2),
where
ğœˆâˆ—= ğœˆğœ2 + ğ‘›Ì„ğ‘¥ğœ2
ğœ2 + ğ‘›ğœ2
and ğœâˆ—= âˆš
ğœ2ğœ2
ğœ2 + ğ‘›ğœ2 .
Letâ€™s continue from Example 2.6, and suppose she wants to measure the mass
of a sample of ammonium nitrate.

2.3. CREDIBLE INTERVALS AND PREDICTIVE INFERENCE
37
Her balance has a known standard deviation of 0.2 milligrams. By looking at the
sample, she thinks this mass is about 10 milligrams and based on her previous
experience in estimating masses, her guess has the standard deviation of 2. So
she decides that her prior for the mass of the sample is a normal distribution
with mean, 10 milligrams, and standard deviation, 2 milligrams.
Now she collects five measurements on the sample and finds that the average
of those is 10.5. By conjugacy of the normal-normal family, our posterior belief
about the mass of the sample has the normal distribution.
The new mean of that posterior normal is found by plugging into the formula:
ğœ‡âˆ¼ğ‘(ğœˆ= 10, ğœ2 = 22)
ğœˆâˆ—= ğœˆğœ2 + ğ‘›Ì„ğ‘¥ğœ2
ğœ2 + ğ‘›ğœ2
= 10 Ã— (0.2)2 + 5 Ã— 10.5 Ã— 22
(0.2)2 + 5 Ã— 22
= 10.499
ğœâˆ—= âˆš
ğœ2ğœ2
ğœ2 + ğ‘›ğœ2 = âˆš(0.2)2 Ã— 22(0.2)2 + 5 Ã— 22 = 0.089.
Before seeing the data, the Bayesian analytical chemist thinks the ammonium
nitrate has mass 10 mg and uncertainty (standard deviation) 2 mg. After seeing
the data, she thinks the mass is 10.499 mg and standard deviation 0.089 mg.
Her posterior mean has shifted quite a bit and her uncertainty has dropped by
a lot. Thatâ€™s exactly what an analytical chemist wants.
This is the last of the three examples of conjugate families. There are many
more, but they do not suï¬€ice for every situation one might have.
We learned several things in this lecture.
First, we learned the new pair of
conjugate families and the relevant updating formula. Also, we worked a realistic
example problem that can arise in practical situations.
2.3
Credible Intervals and Predictive Inference
In this part, we are going to quantify the uncertainty of the parameter by
credible intervals after incorporating the data.
Then we can use predictive
inference to identify the posterior distribution for a new random variable.
2.3.1
Non-Conjugate Priors
In many applications, a Bayesian may not be able to use a conjugate prior.
Sometimes she may want to use a reference prior, which injects the minimum
amount of personal belief into the analysis. But most often, a Bayesian will
have a personal belief about the problem that cannot be expressed in terms of
a convenient conjugate prior.
For example, we shall reconsider the RU-486 case from earlier in which four
children were born to standard therapy mothers. But no children were born

38
CHAPTER 2. BAYESIAN INFERENCE
to RU-486 mothers. This time, the Bayesian believes that the probability p
of an RU-486 baby is uniformly distributed between 0 and one-half, but has a
point mass of 0.5 at one-half. That is, she believes there is a 50% chance that
no difference exists between standard therapy and RU-486. But if a difference
exists, she thinks that RU-486 is better, but she is completely unsure about how
much better it would be.
In mathematical notation, the probability density function of ğ‘is
ğœ‹(ğ‘) =
â§
{
â¨
{
â©
1
for
0 â‰¤ğ‘< 0.5
0.5
for
ğ‘= 0.5
0
for
ğ‘< 0 or ğ‘> 0.5
We can check that the area under the density curve, plus the amount of the
point mass, equals 1.
The cumulative distribution function, ğ‘ƒ(ğ‘â‰¤ğ‘¥) or ğ¹(ğ‘¥), is
ğ‘ƒ(ğ‘â‰¤ğ‘¥) = ğ¹(ğ‘¥) =
â§
{
â¨
{
â©
0
for
ğ‘¥< 0
ğ‘¥
for
0 â‰¤ğ‘¥< 0.5
1
for
ğ‘¥â‰¥0.5
Why would this be a reasonable prior for an analyst to self-elicit? One reason
is that in clinical trials, there is actually quite a lot of preceding research on
the eï¬€icacy of the drug. This research might be based on animal studies or
knowledge of the chemical activity of the molecule.
So the Bayesian might
feel sure that there is no possibility that RU-486 is worse than the standard
treatment. And her interest is on whether the therapies are equivalent and if
not, how much better RU-486 is than the standard therapy.
As previously mentioned, the posterior distribution ğœ‹âˆ—(ğ‘) for ğ‘has a complex
mathematical form. That is why Bayesian inference languished for so many
decades until computational power enabled numerical solutions. But now we
have simulation tools to help us, and one of them is called JAGS (Just An-
other Gibbs Sampler).
If we apply JAGS to the RU-486 data with this non-conjugate prior, we can find
the posterior distribution ğœ‹âˆ—(ğ‘), as in Figure 2.5. At a high level, this program
is defining the binomial probability, that is the likelihood of seeing 0 RU-486
children, which is binomial. And then it defines the prior by using a few tricks
to draw from either a uniform on the interval from 0 to one-half, or else draw
from the point mass at one-half. Then it calls the JAGS model function, and
draws 5,000 times from the posterior and creates a histogram of the results.
The posterior distribution is decreasing when ğ‘is between 0 and 0.5, and has
a point mass of probability at 0.5. But now the point mass has less weights
than before. Also, note that the data have changed the posterior away from the
original uniform prior when ğ‘is between 0 and 0.5. The analyst sees a lot of

2.3. CREDIBLE INTERVALS AND PREDICTIVE INFERENCE
39
0.0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
4
5
p
posterior distribution, Ï€*(p)
Figure 2.5: Posterior with JAGS
probability under the curve near 0, which responds to the fact that no children
were born to RU-486 mothers.
This section is mostly a look-ahead to future material. We have seen that a
Bayesian might reasonably employ a non-conjugate prior in a practical applica-
tion. But then she will need to employ some kind of numerical computation to
approximate the posterior distribution. Additionally, we have used a computa-
tional tool, JAGS, to approximate the posterior for ğ‘, and identified its three
important elements, the probability of the data given ğ‘, that is the likelihood,
and the prior, and the call to the Gibbs sampler.
2.3.2
Credible Intervals
In this section, we introduce credible intervals, the Bayesian alternative to con-
fidence intervals. Letâ€™s start with the confidence intervals, which are the fre-
quentist way to express uncertainty about an estimate of a population mean, a
population proportion or some other parameter.
A confidence interval has the form of an upper and lower bound.
ğ¿, ğ‘ˆ= pe Â± se Ã— cv
where ğ¿, ğ‘ˆare the lower bound and upper bound of the confidence interval
respectively, pe represents â€œpoint estimatesâ€, se is the standard error, and cv is
the critical value.
Most importantly, the interpretation of a 95% confidence interval on the mean
is that â€œ95% of similarly constructed intervals will contain the true

40
CHAPTER 2. BAYESIAN INFERENCE
meanâ€, not â€œthe probability that true mean lies between ğ¿and ğ‘ˆis 0.95â€.
The reason for this frequentist wording is that a frequentist may not express his
uncertainty as a probability. The true mean is either within the interval or not,
so the probability is zero or one. The problem is that the frequentist does not
know which is the case.
On the other hand, Bayesians have no such qualms. It is fine for us to say
that â€œthe probability that the true mean is contained within a given
interval is 0.95â€. To distinguish our intervals from confidence intervals, we
call them credible intervals.
Recall the RU-486 example. When the analyst used the beta-binomial family,
she took the prior as ğ‘âˆ¼beta(1, 1), the uniform distribution, where ğ‘is the
probability of a child having a mother who received RU-486.
After we observed four children born to mothers who received conventional
therapy, her posterior is ğ‘|ğ‘¥âˆ¼beta(1, 5). In Figure 2.6, the posterior probability
density for beta(1, 5) puts a lot of probability near zero and very little probability
near one.
0
1
2
3
4
5
0.00
0.25
0.50
0.75
1.00
p
Probability Density f(p)
Beta(1,5)
Figure 2.6: RU-486 Posterior
For the Bayesian, her 95% credible interval is just any ğ¿and ğ‘ˆsuch that the
posterior probability that ğ¿< ğ‘< ğ‘ˆis 0.95. The shortest such interval is
obviously preferable.
To find this interval, the Bayesian looks at the area under the beta(1, 5) distri-
bution, that lies to the left of a value x.

2.3. CREDIBLE INTERVALS AND PREDICTIVE INFERENCE
41
The density function of the beta(1, 5) is
ğ‘“(ğ‘) = 5(1 âˆ’ğ‘)4 for 0 â‰¤ğ‘â‰¤1,
and the cumulative distribution function, which represents the area under the
density function ğ‘“(ğ‘) between 0 and ğ‘¥is
ğ‘ƒ(ğ‘â‰¤ğ‘¥) = ğ¹(ğ‘¥) = âˆ«
ğ‘¥
0
ğ‘“(ğ‘) ğ‘‘ğ‘= 1 âˆ’(1 âˆ’ğ‘¥)5 for 0 â‰¤ğ‘â‰¤1.
The Bayesian can use this to find ğ¿, ğ‘ˆwith area 0.95 under the density curve
between them, i.e. ğ¹(ğ‘ˆ)âˆ’ğ¹(ğ¿) = 0.95. Note that the Bayesian credible interval
is asymmetric, unlike the symmetric confidence intervals that frequentists often
obtain. It turns out that ğ¿= 0 and ğ‘ˆ= 0.45 is the shortest interval with
probability 0.95 of containing ğ‘.
What have we done? We have seen the difference in interpretations between the
frequentist confidence interval and the Bayesian credible interval. Also, we have
seen the general form of a credible interval. Finally, we have done a practical
example constructing a 95% credible interval for the RU-486 data set.
2.3.3
Predictive Inference
Predictive inference arises when the goal is not to find a posterior distribu-
tion over some parameter, but rather to find a posterior distribution over some
random variable depends on the parameter.
Specifically, we want to make an inference on a random variable ğ‘‹with probabil-
ity densifity function ğ‘“(ğ‘¥|ğœƒ), where you have some personal or prior probability
distribution ğ‘(ğœƒ) for the parameter ğœƒ.
To solve this, one needs to integrate:
ğ‘ƒ(ğ‘‹â‰¤ğ‘¥) = âˆ«
âˆ
âˆ’âˆ
ğ‘ƒ(ğ‘‹â‰¤ğ‘¥|ğœƒ) ğ‘(ğœƒ)ğ‘‘ğœƒ= âˆ«
âˆ
âˆ’âˆ
(âˆ«
ğ‘¥
âˆ’âˆ
ğ‘“(ğ‘ |ğœƒ) ğ‘‘ğ‘ ) ğ‘(ğœƒ) ğ‘‘ğœƒ
The equation gives us the weighted average of the probabilities for ğ‘‹, where the
weights correspond to the personal probability on ğœƒ. Here we will not perform
the integral case; instead, we will illustrate the thinking with a discrete example.
Example 2.7. Suppose you have two coins. One coin has probability 0.7 of
coming up heads, and the other has probability 0.4 of coming up heads. You
are playing a gambling game with a friend, and you draw one of those two coins
at random from a bag.
Before you start the game, your prior belief is that the probability of choosing
the 0.7 coin is 0.5. This is reasonable, because both coins were equally likely to
be drawn. In this game, you win if the coin comes up heads.

42
CHAPTER 2. BAYESIAN INFERENCE
Suppose the game starts, you have tossed twice, and have obtained two heads.
Then what is your new belief about ğ‘, the probability that you are using the
0.7 coin?
This is just a simple application of the discrete form of Bayesâ€™ rule.
â€¢ Prior: ğ‘= 0.5
â€¢ Posterior:
ğ‘âˆ—=
ğ‘ƒ(2 heads|0.7) Ã— 0.5
ğ‘ƒ(2 heads|0.7) Ã— 0.5 + ğ‘ƒ(2 heads|0.4) Ã— 0.5 = 0.754.
However, this does not answer the important question â€“ What is the predictive
probability that the next toss will come up heads? This is of interest because
you are gambling on getting heads.
Fortunately, the predictive probability of getting heads is not diï¬€icult to calcu-
late:
â€¢ ğ‘âˆ—of 0.7 coin = 0.754
â€¢ ğ‘âˆ—of 0.4 coin = 1 âˆ’0.754 = 0.246
â€¢ ğ‘ƒ(heads) = ğ‘ƒ(heads|0.7) Ã— 0.754 + ğ‘ƒ(heads|0.4) Ã— 0.246 = 0.626
Therefore, the predictive probability that the next toss will come up heads is
0.626.
Note that most realistic predictive inference problems are more complicated
and require one to use integrals. For example, one might want to know the
chance that a fifth child born in the RU-486 clinical trial will have a mother
who received RU-486. Or you might want to know the probability that your
stock brokerâ€™s next recommendation will be profitable.
We have learned three things in this section.
First, often the real goal is a
prediction about the value of a future random variable, rather than
making an estimate of a parameter. Second, these are deep waters, and often
one needs to integrate. Finally, in certain simple cases where the parameter can
only take discrete values, one can find a solution without integration. In our
example, the parameter could only take two values to indicate which of the two
coins was being used.

Chapter 3
Losses and Decision Making
In the previous chapter, we learned about continuous random variables. That
enabled us to study conjugate families, such as the beta binomial, the Poisson
gamma, and the normal normal. We also considered the diï¬€iculties of eliciting
a personal prior, and of handling inference in nonconjugate cases. Finally, we
introduced the credible interval and studied predictive inference.
In this new chapter, we will introduce loss functions and Bayesian decision
making, minimizing expected loss for hypothesis testing, and define posterior
probabilities of hypothesis and Bayes factors. We will then outline Bayesian
testing for two proportions and two means, discuss how findings from credible
intervals compare to those from our hypothesis test, and finally discuss when to
reject, accept, or wait.
3.1
Bayesian Decision Making
To a Bayesian, the posterior distribution is the basis of any inference, since
it integrates both his/her prior opinions and knowledge and the new informa-
tion provided by the data. It also contains everything she believes about the
distribution of the unknown parameter of interest.
However, the posterior distribution on its own is not always suï¬€icient. Some-
times the inference we want to express is a credible interval, because it in-
dicates a range of likely values for the parameter. That would be helpful if
you wanted to say that you are 95% certain the probability of an RU-486
pregnancy lies between some number ğ¿and some number ğ‘ˆ. And on other
occasions, one needs to make a single number guess about the value of the pa-
rameter.
For example, you might want to declare the average payoff for an
insurance claim or tell a patient how much longer he/she has to live.
Therefore, the Bayesian perspective leads directly to decision theory. And in
43

44
CHAPTER 3. LOSSES AND DECISION MAKING
Table 3.1: Loss Functions
Loss
Best Estimate
Linear
Median
Quadratic
Mean
0/1
Mode
decision theory, one seeks to minimize oneâ€™s expected loss.
3.2
Loss Functions
Quantifying the loss can be tricky, and Table 3.1 summarizes three different
examples with three different loss functions.
If youâ€™re declaring the average payoff for an insurance claim, and if you are
linear in how you value money, that is, twice as much money is exactly twice as
good, then one can prove that the optimal one-number estimate is the median
of the posterior distribution. But in different situations, other measures of loss
may apply.
If you are advising a patient on his/her life expectancy, it is easy to imagine
that large errors are far more problematic than small ones. And perhaps the
loss increases as the square of how far off your single number estimate is from
the truth. For example, if she is told that her average life expectancy is two
years, and it is actually ten, then her estate planning will be catastrophically
bad, and she will die in poverty. In the case when the loss is proportional to
the quadratic error, one can show that the optimal one-number estimate is the
mean of the posterior distribution.
Finally, in some cases, the penalty is 0 if you are exactly correct, but constant if
youâ€™re at all wrong. This is the case with the old saying that close only counts
with horseshoes and hand grenades; i.e., coming close but not succeeding is not
good enough. And it would apply if you want a prize for correctly guessing the
number of jelly beans in a jar. Here, of course, instead of minimizing expected
losses, we want to maximize the expected gain. If a Bayesian is in such
a situation, then his/her best one-number estimate is the mode of his/her
posterior distribution, which is the most likely value.
There is a large literature on decision theory, and it is directly linked to risk
analysis, which arises in many fields. Although it is possible for frequentists to
employ a certain kind of decision theory, it is much more natural for Bayesians.
When making point estimates of unknown parameters, we should make the
choices that minimize the loss. Nevertheless, the best estimate depends on the
kind of loss function we are using, and we will discuss in more depth how these

3.3. WORKING WITH LOSS FUNCTIONS
45
best estimates are determined in the next section.
3.3
Working with Loss Functions
Now we illustrate why certain estimates minimize certain loss functions.
Example 3.1. You work at a car dealership. Your boss wants to know how
many cars the dealership will sell per month.
An analyst who has worked
with past data from your company provided you a distribution that shows the
probability of number of cars the dealership will sell per month. In Bayesian
lingo, this is called the posterior distribution.
A dot plot of that posterior
is shown in Figure 3.1. The mean, median and the mode of the distribution
are also marked on the plot. Your boss does not know any Bayesian statistics
though, so he/she wants you to report a single number for the number of cars
the dealership will sell per month.
0
5
10
15
20
25
30
35
40
45
50
mean
median
mode
Figure 3.1: Posterior
Suppose your single guess is 30, and we call this ğ‘”in the following calculations.
If your loss function is ğ¿0 (i.e., a 0/1 loss), then you lose a point for each value
in your posterior that differs from your guess and do not lose any points for
values that exactly equal your guess. The total loss is the sum of the losses
from each value in the posterior.
In mathematical terms, we define ğ¿0 (0/1 loss) as
ğ¿0,ğ‘–(0, ğ‘”) = { 0
if ğ‘”= ğ‘¥ğ‘–
1
otherwise
The total loss is ğ¿0 = âˆ‘ğ‘–ğ¿0,ğ‘–(0, ğ‘”).
Letâ€™s calculate what the total loss would be if your guess is 30.
Table 3.2
summarizes the values in the posterior distribution sorted in descending order.
The first value is 4, which is not equal to your guess of 30, so the loss for that
value is 1. The second value is 19, also not equal to your guess of 30, and the

46
CHAPTER 3. LOSSES AND DECISION MAKING
Table 3.2: L0: 0/1 loss for g = 30
i
x_i
L0: 0/1
1
4
1
2
19
1
3
20
1
...
...
14
30
0
...
...
50
47
1
51
49
1
Total
50
loss for that value is also 1. The third value is 20, also not equal to your guess
of 30, and the loss for this value is also 1.
There is only one 30 in your posterior, and the loss for this value is 0 â€“ since
itâ€™s equal to your guess (good news!). The remaining values in the posterior are
all different than 30 hence, the loss for them are all ones as well.
To find the total loss, we simply sum over these individual losses in the posterior
distribution with 51 observations where only one of them equals our guess and
the remainder are different. Hence, the total loss is 50.
Figure 3.2 is a visualization of the posterior distribution, along with the 0-1
loss calculated for a series of possible guesses within the range of the posterior
distribution. To create this visualization of the loss function, we went through
the process we described earlier for a guess of 30 for all guesses considered, and
we recorded the total loss. We can see that the loss function has the lowest
value when ğ‘”, our guess, is equal to the most frequent observation in the
posterior. Hence, ğ¿0 is minimized at the mode of the posterior, which means
that if we use the 0/1 loss, the best point estimate is the mode of the posterior.
Letâ€™s consider another loss function.
If your loss function is ğ¿1 (i.e., linear
loss), then the total loss for a guess is the sum of the absolute values of the
difference between that guess and each value in the posterior. Note that the
absolute value function is required, because overestimates and underestimates
do not cancel out.
In mathematical terms, ğ¿1 (linear loss) is calculated as ğ¿1(ğ‘”) = âˆ‘ğ‘–|ğ‘¥ğ‘–âˆ’ğ‘”|.
We can once again calculate the total loss under ğ¿1 if your guess is 30. Table 3.3
summarizes the values in the posterior distribution sorted in descending order.
The first value is 4, and the absolute value of the difference between 4 and 30
is 26. The second value is 19, and the absolute value of the difference between

3.3. WORKING WITH LOSS FUNCTIONS
47
L0
46
48
50
0
5
10
15
20
25
30
35
40
45
50
mean
median
mode
Figure 3.2: L0 is minimized at the mode of the posterior
19 and 30 is 11. The third value is 20 and the absolute value of the difference
between 20 and 30 is 10.
There is only one 30 in your posterior, and the loss for this value is 0 since it is
equal to your guess. The remaining value in the posterior are all different than
30 hence their losses are different than 0.
To find the total loss, we again simply sum over these individual losses, and the
total is to 346.
Again, Figure 3.3 is a visualization of the posterior distribution, along with a
linear loss function calculated for a series of possible guesses within the range
of the posterior distribution. To create this visualization of the loss function,
we went through the same process we described earlier for all of the guesses
considered. This time, the function has the lowest value when ğ‘”is equal to
the median of the posterior. Hence, ğ¿1 is minimized at the median of the
posterior one other loss function.
If your loss function is ğ¿2 (i.e. a squared loss), then the total loss for a guess
is the sum of the squared differences between that guess and each value in the
posterior.
We can once again calculate the total loss under ğ¿2 if your guess is 30. Table
3.4 summarizes the posterior distribution again, sorted in ascending order.
The first value is 4, and the squared difference between 4 and 30 is 676. The
second value is 19, and the square of the difference between 19 and 30 is 121.
The third value is 20, and the square difference between 20 and 30 is 100.
There is only one 30 in your posterior, and the loss for this value is 0 since it is
equal to your guess. The remaining values in the posterior are again all different

48
CHAPTER 3. LOSSES AND DECISION MAKING
Table 3.3: L1: linear loss for g = 30
i
x_i
L1: |x_i-30|
1
4
26
2
19
11
3
20
10
...
...
14
30
0
...
...
50
47
17
51
49
19
Total
346
L1
250
600
950
1300
1650
0
5
10
15
20
25
30
35
40
45
50
mean
median
mode
Figure 3.3: L1 is minimized at the median of the posterior

3.3. WORKING WITH LOSS FUNCTIONS
49
Table 3.4: L2: squared loss for g = 30
i
x_i
L2: (x_i-30)^2
1
4
676
2
19
121
3
20
100
...
...
14
30
0
...
...
50
47
289
51
49
361
Total
3732
than 30, hence their losses are all different than 0.
To find the total loss, we simply sum over these individual losses again and the
total loss comes out to 3,732. We have the visualization of the posterior distri-
bution. Again, this time along with the squared loss function calculated for a
possible serious of possible guesses within the range of the posterior distribution.
Creating the visualization in Figure 3.4 had the same steps. Go through the
same process described earlier for a guess of 30, for all guesses considered, and
record the total loss. This time, the function has the lowest value when ğ‘”is
equal to the mean of the posterior. Hence, ğ¿2 is minimized at the mean of the
posterior distribution.
L2
0
20000
40000
60000
0
5
10
15
20
25
30
35
40
45
50
mean
median
mode
Figure 3.4: L2 is minimized at the mean of the posterior
To sum up, the point estimate to report to your boss about the number of cars

50
CHAPTER 3. LOSSES AND DECISION MAKING
the dealership will sell per month depends on your loss function. In any
case, you will choose to report the estimate that minimizes the loss.
â€¢ ğ¿0 is minimized at the mode of the posterior distribution.
â€¢ ğ¿1 is minimized at the median of the posterior distribution.
â€¢ ğ¿2 is minimized at the mean of the posterior distribution.
3.4
Minimizing Expected Loss for Hypothesis
Testing
In Bayesian statistics, the inference about a parameter is made based on the
posterior distribution, and letâ€™s include this in the hypothesis test setting.
Suppose we have two competing hypothesis, ğ»1 and ğ»2. Then we get
â€¢ ğ‘ƒ(ğ»1 is true | data) = posterior probability of ğ»1
â€¢ ğ‘ƒ(ğ»2 is true | data) = posterior probability of ğ»2
One straightforward way of choosing between ğ»1 and ğ»2 would be to choose
the one with the higher posterior probability. In other words, the poten-
tial decision criterion is to
â€¢ Reject ğ»1 if ğ‘ƒ(ğ»1 is true | data) < ğ‘ƒ(ğ»2 is true | data).
However, since hypothesis testing is a decision problem, we should also consider
a loss function.
Letâ€™s revisit the HIV testing example in Section 1.1.2, and
suppose we want to test the two competing hypotheses below:
â€¢ ğ»1: Patient does not have HIV
â€¢ ğ»2: Patient has HIV
These are the only two possibilities, so they are mutually exclusive hypotheses
that cover the entire decision space.
We can define the loss function as ğ¿(ğ‘‘) â€“ the loss that occurs when decision ğ‘‘
is made. Then the Bayesian testing procedure minimizes the posterior expected
loss.
The possible decisions (actions) are:
â€¢ ğ‘‘1: Choose ğ»1 - decide that the patient does not have HIV
â€¢ ğ‘‘2: Choose ğ»2 - decide that the patient has HIV
For each decision ğ‘‘, we might be right, or we might be wrong. If the decision
is right, the loss ğ¿(ğ‘‘) associated with the decision ğ‘‘is zero, i.e. no loss. If the
decision is wrong, the loss ğ¿(ğ‘‘) associated with the decision ğ‘‘is some positive
value ğ‘¤.
For ğ‘‘= ğ‘‘1, we have

3.4. MINIMIZING EXPECTED LOSS FOR HYPOTHESIS TESTING
51
â€¢ Right: Decide patient does not have HIV, and indeed they do not. â‡’
ğ¿(ğ‘‘1) = 0
â€“ Wrong: Decide patient does not have HIV, but they do. â‡’ğ¿(ğ‘‘1) =
ğ‘¤1
For ğ‘‘= ğ‘‘2, we also have
â€¢ Right: Decide patient has HIV, and indeed they do. â‡’ğ¿(ğ‘‘2) = 0
â€“ Wrong: Decide patient has HIV, but they donâ€™t â‡’ğ¿(ğ‘‘2) = ğ‘¤2
The consequences of making a wrong decision ğ‘‘1 or ğ‘‘2 are different.
Wrong ğ‘‘1 is a false negative:
â€¢ We decide that patient does not have HIV when in reality they do.
â€¢ Potential consequences: no treatment and premature death! (severe)
Wrong ğ‘‘2 is a false positive:
â€¢ We decide that the patient has HIV when in reality they do not.
â€¢ Potential consequences: distress and unnecessary further investigation.
(not ideal but less severe than the consequences of a false negative decision)
Letâ€™s put these definitions in the context of the HIV testing example with ELISA.
Hypotheses
â€¢ ğ»1: Patient does not have HIV
â€¢ ğ»2: Patient has HIV
Decision
â€¢ ğ‘‘1: Choose ğ»1 - decide that the patient does not have HIV
â€¢ ğ‘‘2: Choose ğ»2 - decide that the patient has HIV
Losses
â€¢ ğ¿(ğ‘‘1) = {
0
if ğ‘‘1 is right
ğ‘¤1 = 1000
if ğ‘‘1 is wrong
â€¢ ğ¿(ğ‘‘2) = {
0
if ğ‘‘2 is right
ğ‘¤2 = 10
if ğ‘‘2 is wrong
The values of ğ‘¤1 and ğ‘¤2 are arbitrarily chosen. But the important thing is that
ğ‘¤1, the loss associated with a false negative determination, is much higher than
ğ‘¤2, the loss associated with a false positive determination.
Posteriors
The plus sign means that our patient had tested positive on the ELISA.
â€¢ ğ‘ƒ(ğ»1|+) â‰ˆ0.88 - the posterior probability of the patient not having HIV
given positive ELISA result

52
CHAPTER 3. LOSSES AND DECISION MAKING
â€¢ ğ‘ƒ(ğ»2|+) â‰ˆ0.12 - the posterior probability of the patient having HIV
given positive ELISA result, as the complement value of ğ‘ƒ(ğ»1|+)
Expected losses
â€¢ ğ¸[ğ¿(ğ‘‘1)] = 0.88 Ã— 0 + 0.12 Ã— 1000 = 120
â€¢ ğ¸[ğ¿(ğ‘‘2)] = 0.88 Ã— 10 + 0.12 Ã— 0 = 8.8
Since the expected loss for ğ‘‘2 is lower, we should make this decision â€“ the patient
has HIV.
Note that our decision is highly influenced by the losses we assigned to ğ‘‘1 and
ğ‘‘2.
If the losses were symmetric, say ğ‘¤1 = ğ‘¤2 = 10, then the expected loss for ğ‘‘1
becomes
ğ¸[ğ¿(ğ‘‘1)] = 0.88 Ã— 0 + 0.12 Ã— 10 = 1.2,
while the expected loss for ğ‘‘2 would not change. Therefore, we would choose ğ‘‘1
instead; that is, we would decide that the patient does not have HIV.
To recap, Bayesian methodologies allow for the integration of losses into the
decision making framework easily. And in Bayesian testing, we minimize the
posterior expected loss.
3.5
Posterior Probabilities of Hypotheses and
Bayes Factors
In this section, we will continue with the HIV testing example to introduce
the concept of Bayes factors. Earlier, we introduced the concept of priors and
posteriors. The prior odds is defined as the ratio of the prior probabilities
of hypotheses.
Therefore, if there are two competing hypotheses being considered, then the
prior odds of ğ»1 to ğ»2 can be defined as ğ‘‚[ğ»1 âˆ¶ğ»2], which is equal to ğ‘ƒ(ğ»1)
over probability of ğ‘ƒ(ğ»2). In mathematical terms,
ğ‘‚[ğ»1 âˆ¶ğ»2] = ğ‘ƒ(ğ»1)
ğ‘ƒ(ğ»2)
Similarly, the posterior odds is the ratio of the two posterior probabil-
ities of hypotheses, written as
ğ‘ƒğ‘‚[ğ»1 âˆ¶ğ»2] = ğ‘ƒ(ğ»1|data)
ğ‘ƒ(ğ»2|data)

3.5. POSTERIOR PROBABILITIES OF HYPOTHESES AND BAYES FACTORS53
Using Bayesâ€™ rule, we can rewrite the posterior probabilities as below:
ğ‘ƒğ‘‚[ğ»1 âˆ¶ğ»2] = ğ‘ƒ(ğ»1|data)
ğ‘ƒ(ğ»2|data)
= (ğ‘ƒ(data|ğ»1) Ã— ğ‘ƒ(ğ»1))/ğ‘ƒ(data))
(ğ‘ƒ(data|ğ»2) Ã— ğ‘ƒ(ğ»2))/ğ‘ƒ(data))
= (ğ‘ƒ(data|ğ»1) Ã— ğ‘ƒ(ğ»1))
(ğ‘ƒ(data|ğ»2) Ã— ğ‘ƒ(ğ»2))
= ğ‘ƒ(data|ğ»1)
ğ‘ƒ(data|ğ»2) Ã— ğ‘ƒ(ğ»1)
ğ‘ƒ(ğ»2)
= Bayes factor Ã— prior odds
In mathematical notation, we have
ğ‘ƒğ‘‚[ğ»1 âˆ¶ğ»2] = ğµğ¹[ğ»1 âˆ¶ğ»2] Ã— ğ‘‚[ğ»1 âˆ¶ğ»2]
In other words, the posterior odds is the product of the Bayes factor and the
prior odds for these two hypotheses.
The Bayes factor quantifies the evidence of data arising from ğ»1 versus ğ»2.
In a discrete case, the Bayes factor is simply the ratio of the likelihoods of the
observed data under the two hypotheses, written as
ğµğ¹[ğ»1 âˆ¶ğ»2] = ğ‘ƒ(data|ğ»1)
ğ‘ƒ(data|ğ»2).
On the other hand, in a continuous case, the Bayes factor is the ratio of the
marginal likelihoods, written as
ğµğ¹[ğ»1 âˆ¶ğ»2] = âˆ«ğ‘ƒ(data|ğœƒ, ğ»1)ğ‘‘ğœƒ
âˆ«ğ‘ƒ(data|ğœƒ, ğ»2)ğ‘‘ğœƒ.
Note that ğœƒis the set formed by all possible values of the model parameters.
In this section, we will stick with the simpler discrete case. And in upcoming
sections, we will revisit calculating Bayes factors for more complicated models.
Letâ€™s return to the HIV testing example from earlier, where our patient had
tested positive in the ELISA.
Hypotheses
â€¢ ğ»1: Patient does not have HIV
â€¢ ğ»2: Patient has HIV

54
CHAPTER 3. LOSSES AND DECISION MAKING
Priors
The prior probabilities we place on these hypothesis came from the prevalence of
HIV at the time in the general population. We were told that the prevalence of
HIV in the population was 1.48 out of 1000, hence the prior probability assigned
to ğ»2 is 0.00148. And the prior assigned to ğ»1 is simply the complement of
this.
â€¢ ğ‘ƒ(ğ»1) = 0.99852 and ğ‘ƒ(ğ»2) = 0.00148
The prior odds are
â€¢ ğ‘‚[ğ»1 âˆ¶ğ»2] = ğ‘ƒ(ğ»1)
ğ‘ƒ(ğ»2) = 0.99852
0.00148 = 674.6757
Posteriors
Given a positive ELISA result, the posterior probabilities of these hy-
potheses can also be calculated, and these are approximately 0.88 and
0.12. We will hold on to more decimal places in our calculations to avoid
rounding errors later.
â€¢ ğ‘ƒ(ğ»1|+) = 0.8788551 and ğ‘ƒ(ğ»2|+) = 0.1211449
The posterior odds are
â€¢ ğ‘ƒğ‘‚[ğ»1 âˆ¶ğ»2] = ğ‘ƒ(ğ»1|+)
ğ‘ƒ(ğ»2|+) = 0.8788551
0.1211449 = 7.254578
Bayes Factor
Finally, we can calculate the Bayes factor as the ratio of the posterior
odds to prior odds, which comes out to approximately 0.0108. Note that
in this simple discrete case the Bayes factor, it simplifies to the ratio of
the likelihoods of the observed data under the two hypotheses.
ğµğ¹[ğ»1 âˆ¶ğ»2] = ğ‘ƒğ‘‚[ğ»1 âˆ¶ğ»2]
ğ‘‚[ğ»1 âˆ¶ğ»2] = 7.25457
674.6757 â‰ˆ0.0108
= ğ‘ƒ(+|ğ»1)
ğ‘ƒ(+|ğ»2) = 0.01
0.93 â‰ˆ0.0108
Alternatively, remember that the true positive rate of the test was 0.93 and the
false positive rate was 0.01. Using these two values, the Bayes factor also comes
out to approximately 0.0108.
So now that we calculated the Bayes factor, the next natural question is, what
does this number mean? A commonly used scale for interpreting Bayes factors
is proposed by Jeffreys (1961), as in Table 3.5. If the Bayes factor is between
1 and 3, the evidence against ğ»2 is not worth a bare mention. If it is 3 to 20,
the evidence is positive. If it is 20 to 150, the evidence is strong. If it is greater
than 150, the evidence is very strong.

3.5. POSTERIOR PROBABILITIES OF HYPOTHESES AND BAYES FACTORS55
Table 3.5: Interpreting the Bayes factor
BF[H_1:H_2]
Evidence against H_2
1 to 3
Not worth a bare mention
3 to 20
Positive
20 to 150
Strong
> 150
Very strong
Table 3.6: Interpreting the Bayes factor
2*log(BF[H_2:H_1])
Evidence against H_1
0 to 2
Not worth a bare mention
2 to 6
Positive
6 to 10
Strong
> 10
Very strong
It might have caught your attention that the Bayes factor we calculated does not
even appear on the scale. To obtain a Bayes factor value on the scale, we will
need to change the order of our hypotheses and calculate ğµğ¹[ğ»2 âˆ¶ğ»1], i.e. the
Bayes factor for ğ»2 to ğ»1. Then we look for evidence against ğ»1 instead.
We can calculate ğµğ¹[ğ»2 âˆ¶ğ»1] as a reciprocal of ğµğ¹[ğ»1 âˆ¶ğ»2] as below:
ğµğ¹[ğ»2 âˆ¶ğ»1] =
1
ğµğ¹[ğ»1 âˆ¶ğ»2] =
1
0.0108 = 92.59259
For our data, this comes out to approximately 93. Hence the evidence against
ğ»1 (the patient does not have HIV) is strong.
Therefore, even though the
posterior for having HIV given a positive result, i.e. ğ‘ƒ(ğ»2|+), was low, we
would still decide that the patient has HIV, according to the scale based on a
positive ELISA result.
An intuitive way of thinking about this is to consider not only the posteriors,
but also the priors assigned to these hypotheses. Bayes factor is the ratio of
the posterior odds to prior odds.
While 12% is a low posterior probability
for having HIV given a positive ELISA result, this value is still much higher
than the overall prevalence of HIV in the population (in other words, the prior
probability for that hypothesis).
Another commonly used scale for interpreting Bayes factors is proposed by Kass
and Raftery (1995), and it deals with the natural logarithm of the calculated
Bayes factor. The values can be interpreted in Table 3.6.
Reporting of the log scale can be helpful for numerical accuracy reasons when

56
CHAPTER 3. LOSSES AND DECISION MAKING
the likelihoods are very small. Taking two times the natural logarithm of the
Bayes factor we calculated earlier, we would end up with the same decision that
the evidence against ğ»1 is strong.
2 Ã— log(92.59259) = 9.056418
To recap, we defined prior odds, posterior odds, and the Bayes factor.
We
learned about scales by which we can interpret these values for model selection.
We also re-emphasize that in Bayesian testing, the order in which we evaluate
the models of hypotheses does not matter. The Bayes factor of ğ»2 versus ğ»1,
ğµğ¹[ğ»2 âˆ¶ğ»1], is simply the reciprocal of the Bayes factor for ğ»1 versus ğ»2, that
is, ğµğ¹[ğ»1 âˆ¶ğ»2].

Chapter 4
Inference and
Decision-Making with
Multiple Parameters
We saw in 2.2.3 that if the data followed a normal distribution and that the
variance was known, that the normal distribution was the conjugate prior dis-
tribution for the unknown mean. In this chapter, we will focus on the situation
when the data follow a normal distribution with an unknown mean, but now
consider the case where the variance is also unknown. When the variance ğœ2 of
the data is also unknown, we need to specify a joint prior distribution ğ‘(ğœ‡, ğœ2) for
both the mean ğœ‡and the variance ğœ2. We will introduce the conjugate normal-
gamma family of distributions where the posterior distribution is in the same
family as the prior distribution and leads to a marginal Student t distribution
for posterior inference for the mean of the population.
We will present Monte Carlo simulation for inference about functions of the
parameters as well as sampling from predictive distributions, which can also be
used to assist with prior elicitation. For situations when limited prior infor-
mation is available, we discuss a limiting case of the normal-gamma conjugate
family, the reference prior, leading to a prior that can be used for a default or
reference analysis. Finally, we will show how to create a more flexible and robust
prior distribution by using mixtures of the normal-gamma conjugate prior, the
Jeffreys-Zellner-Siow prior. For inference in this case we will introduce Markov
Chain Monte Carlo, a powerful simulation method for Bayesian inference.
It is assumed that the readers have mastered the concepts of one-parameter
normal-normal conjugate priors. Calculus is not required for this section; how-
ever, for those who are comfortable with calculus and would like to go deeper,
we shall present optional sections with more details on the derivations.
57

58CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
Also note that some of the examples in this section use an updated version of
the bayes_inference function. If your local output is different from what is
seen in this chapter, or the provided code fails to run for you please update to
the most recent version of the statsr package.
4.1
The Normal-Gamma Conjugate Family
You may take the safety of your drinking water for granted, however, residents
of Flint, Michigan were outraged over reports that the levels of a contaminant
known as TTHM exceeded federal allowance levels in 2014. TTHM stands for
total trihalomethanes, a group of chemical compounds first identified in drink-
ing water in the 1970â€™s. Trihalomethanes are formed as a by-product from the
reaction of chlorine or bromine with organic matter present in the water being
disinfected for drinking.
THMs have been associated through epidemiologi-
cal studies with some adverse health effects and many are considered carcino-
genic. In the United States, the EPA limits the total concentration of the four
chief constituents (chloroform, bromoform, bromodichloromethane, and dibro-
mochloromethane), referred to as total trihalomethanes (TTHM), to 80 parts
per billion in treated water.
Since violations are based on annual running averages, we are interested in infer-
ence about the mean TTHM level based on measurements taken from samples.
In Section 2.2.3 we described the normal-normal conjugate family for inference
about an unknown mean ğœ‡when the data ğ‘Œ1, ğ‘Œ2, â€¦ , ğ‘Œğ‘›were assumed to be
a random sample of size ğ‘›from a normal population with a known standard
deviation ğœ, however, it is more common in practice to have data where the
variability of observations is unknown, as in the example with TTHM. Con-
ceptually, Bayesian inference for two (or more) parameters is not any different
from the case with one parameter. As both ğœ‡and ğœ2 unknown, we will need
to specify a joint prior distribution, ğ‘(ğœ‡, ğœ2) to describe our prior uncertainty
about them. As before, Bayes Theorem leads to the posterior distribution for
ğœ‡and ğœ2 given the observed data to take the form
ğ‘(ğœ‡, ğœ2 âˆ£ğ‘¦1, â€¦ , ğ‘¦ğ‘›) = ğ‘(ğ‘¦1, â€¦ , ğ‘¦ğ‘›âˆ£ğœ‡, ğœ2) Ã— ğ‘(ğœ‡, ğœ2)
normalizing constant
.
(4.1)
The likelihood function for ğœ‡, ğœ2 is proportional to the sampling distribution
of the data, ğ¿(ğœ‡, ğœ2) âˆğ‘(ğ‘¦1, â€¦ , ğ‘¦ğ‘›âˆ£ğœ‡, ğœ2) so that the posterior distribution can
be re-expressed in proportional form
ğ‘(ğœ‡, ğœ2 âˆ£ğ‘¦1, â€¦ , ğ‘¦ğ‘›) âˆğ¿(ğœ‡, ğœ2)ğ‘(ğœ‡, ğœ2).
(4.2)
As in the earlier chapters, conjugate priors are appealing as there are nice expres-
sions for updating the prior to obtain the posterior distribution using summaries
of the data. In the case of two parameters or more parameters a conjugate pair

4.1. THE NORMAL-GAMMA CONJUGATE FAMILY
59
is a sampling model for the data and a joint prior distribution for the unknown
parameters such that the joint posterior distribution is in the same family of
distributions as the prior distribution. In this case our sampling model is built
on the assumption that the data are a random sample of size ğ‘›from a normal
population with mean ğœ‡and variance ğœ2, expressed in shorthand as
ğ‘Œ1, â€¦ ğ‘Œğ‘›
iid
âˆ¼Normal(ğœ‡, ğœ2)
where the â€˜iidâ€™ above the distribution symbol â€˜âˆ¼â€™ indicates that each of the
observations are independent of the others (given ğœ‡and ğœ2) and are identically
distributed. Under this assumption, the sampling distribution of the data is the
product of independent normal distributions with mean ğœ‡and variance ğœ2,
ğ‘(ğ‘¦1, â€¦ , ğ‘¦ğ‘›âˆ£ğœ‡, ğœ2) =
ğ‘›
âˆ
ğ‘–=1
1
âˆš
2ğœ‹ğœ2 ğ‘’
{âˆ’1
2 ( ğ‘¦ğ‘–âˆ’ğœ‡
ğœ
)
2}
(4.3)
which, after some algebraic manipulation and simplification, leads to a likelihood
function for ğœ‡and ğœ2 that is proportional to
ğ¿(ğœ‡, ğœ2) âˆ(ğœ2)âˆ’ğ‘›/2 Ã— exp {âˆ’1
2
âˆ‘
ğ‘›
ğ‘–=1(ğ‘¦ğ‘–âˆ’Ì„ğ‘¦)2
ğœ2
} Ã— exp {âˆ’1
2
ğ‘›( Ì„ğ‘¦âˆ’ğœ‡)2
ğœ2
}
function of ğœ2 and data Ã— function of ğœ‡, ğœ2 and data
which depends on the data only through the sum of squares âˆ‘
ğ‘›
ğ‘–=1(ğ‘¦ğ‘–âˆ’Ì„ğ‘¦)2 (or
equivalently the sample variance ğ‘ 2 = âˆ‘
ğ‘›
ğ‘–=1(ğ‘¦ğ‘–âˆ’
Ì„ğ‘¦)2/(ğ‘›âˆ’1)) and the sample
mean
Ì„ğ‘¦. From the expression for the likelihood, we can see that the likelihood
factors into two pieces: a term that is a function of ğœ2 and the data; and a term
that involves ğœ‡, ğœ2 and the data.
Based on the factorization in the likelihood and the fact that any joint distri-
bution for ğœ‡and ğœ2 can be expressed as
ğ‘(ğœ‡, ğœ2) = ğ‘(ğœ‡âˆ£ğœ2) Ã— ğ‘(ğœ2)
as the product of a conditional distribution for ğœ‡given ğœ2 and a marginal
distribution for ğœ2, this suggests that the posterior distribution should factor
as the product of two conjugate distributions. Perhaps not surprisingly, this is
indeed the case.
4.1.1
Conjugate Prior for ğœ‡and ğœ2
In Section 2.2.3, we found that for normal data, the conjugate prior distribution
for ğœ‡when the standard deviation ğœwas known was a normal distribution. We
will build on this to specify a conditional prior distribution for ğœ‡as a normal
distribution
ğœ‡âˆ£ğœ2 âˆ¼N(ğ‘š0, ğœ2/ğ‘›0)
(4.4)

60CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
with hyper-parameters ğ‘š0, the prior mean for ğœ‡, and ğœ2/ğ‘›0 the prior variance.
While previously we represented the prior variance as a fixed constant, ğœ2, in
this case we will replace ğœ2 with a multiple of ğœ2. Because ğœhas the same
units as the data, the presence of ğœin the prior variance automatically scales
the prior for ğœ‡based on the same units. This is important, for example, if we
were to change the measurement units from inches to centimeters or seconds to
hours, as the prior will be re-scaled automatically. The hyper-parameter ğ‘›0 is
unitless, but is used to express our prior precision about ğœ‡relative to the level
of â€œnoiseâ€, captured by ğœ2, in the data. Larger values of ğ‘›0 indicate that we
know the mean with more precision (relative to the variability in observations)
with smaller values indicating less precision or more uncertainty. We will see
later how the hyper-parameter ğ‘›0 may be interpreted as a prior sample size.
Finally, while we could use a fixed value ğœ2 as the prior variance in a conditional
conjugate prior for ğœ‡given ğœ2, that does not lead to a joint conjugate prior for
ğœ‡and ğœ2.
As ğœ2 is unknown, a Bayesian would use a prior distribution to describe the
uncertainty about the variance before seeing data. Since the variance is non-
negative, continuous, and with no upper limit, based on the distributions that
we have seen so far a gamma distribution might appear to be a candidate prior
for the variance,. However, that choice does not lead to a posterior distribution
in the same family or that is recognizable as any common distribution. It turns
out that the the inverse of the variance, which is known as the precision, has a
conjugate gamma prior distribution.
For simplification letâ€™s express the precision (inverse variance) as a new param-
eter, ğœ™= 1/ğœ2. Then the conjugate prior for ğœ™,
ğœ™âˆ¼Gamma (ğ‘£0
2 , ğ‘£0ğ‘ 2
0
2 )
(4.5)
is a gamma distribution with shape parameter ğ‘£0/2 and rate parameter of
ğ‘£0ğ‘ 2
0/2. Given the connections between the gamma distribution and the Chi-
Squared distribution, the hyper-parameter ğ‘£0 may be interpreted as the prior
degrees of freedom.
The hyper-parameter ğ‘ 2
0 may be interpreted as a prior
variance or initial prior estimate for ğœ2.
Equivalently, we may say that the
inverse of the variance has a
1/ğœ2 âˆ¼Gamma(ğ‘£0/2, ğ‘ 2
0ğ‘£0/2)
gamma distribution to avoid using a new symbol 1. Together the conditional
normal distribution for ğœ‡given ğœ2 in (4.4) and the marginal gamma distribution
1In some other references, you will see that ğœ2 will have an inverse gamma distribution.
Rather than introduce an additional distribution for the inverse-gamma, we will restrict our
attention to the gamma distribution since the inverse-gamma is equivalent to saying that the
inverse of ğœ2 has a gamma distribution and â€˜Râ€˜ has support for generating random variables
from the gamma that we will need in later sections.

4.1. THE NORMAL-GAMMA CONJUGATE FAMILY
61
for ğœ™in (4.5) lead to a joint distribution for the pair (ğœ‡, ğœ™) that we will call the
normal-gamma family of distributions:
(ğœ‡, ğœ™) âˆ¼NormalGamma(ğ‘š0, ğ‘›0, ğ‘ 2
0, ğ‘£0)
(4.6)
with the four hyper-parameters ğ‘š0, ğ‘›0, ğ‘ 2
0, and ğ‘£0.
We can obtain the density for the Normal-Gamma(ğ‘š0, ğ‘›0, ğœˆ0, ğ‘ 2
0) family of distri-
butions for ğœ‡, ğœ™by multiplying the conditional normal distribution for ğœ‡times
the marginal gamma distribution for ğœ™:
ğ‘(ğœ‡, ğœ™) = (ğ‘›0ğœ™)1/2
âˆš
2ğœ‹
ğ‘’âˆ’ğœ™ğ‘›0
2
(ğœ‡âˆ’ğ‘š0)2
1
Î“(ğœˆ0/2)(ğœˆ0ğ‘ 2
0)ğœˆ0/2âˆ’1ğ‘’âˆ’ğœ™
ğœˆ0ğ‘ 2
0
2
(4.7)
The joint conjugate prior has simple rules for updating the prior hyper-
parameters given new data to obtain the posterior hyper-parameters due to
conjugacy.
4.1.2
Conjugate Posterior Distribution
As a conjugate family, the posterior distribution of the pair of parameters (ğœ‡, ğœ™)
is in the same family as the prior distribution when the sample data arise from
a normal distribution, that is the posterior is also normal-gamma
(ğœ‡, ğœ™) âˆ£data âˆ¼NormalGamma(ğ‘šğ‘›, ğ‘›ğ‘›, ğ‘ 2
ğ‘›, ğ‘£ğ‘›)
(4.8)
where the subscript ğ‘›on the hyper-parameters indicates the updated values
after seeing the ğ‘›observations from the sample data. One attraction of conju-
gate families is there are relatively simple updating rules for obtaining the new
hyper-parameters:
ğ‘šğ‘›
=
ğ‘›Ì„ğ‘Œ+ ğ‘›0ğ‘š0
ğ‘›+ ğ‘›0
ğ‘›ğ‘›
=
ğ‘›0 + ğ‘›
ğ‘£ğ‘›
=
ğ‘£0 + ğ‘›
ğ‘ 2
ğ‘›
=
1
ğ‘£ğ‘›
[ğ‘ 2(ğ‘›âˆ’1) + ğ‘ 2
0ğ‘£0 + ğ‘›0ğ‘›
ğ‘›ğ‘›
( Ì„ğ‘¦âˆ’ğ‘š0)2] .
Letâ€™s look more closely to try to understand the updating rules. The updated
hyper-parameter ğ‘šğ‘›is the posterior mean for ğœ‡; it is also the mode and median.
The posterior mean ğ‘šğ‘›is a weighted average of the sample mean
Ì„ğ‘¦and prior
mean ğ‘š0 with weights ğ‘›/(ğ‘›+ ğ‘›0 and ğ‘›0/(ğ‘›+ ğ‘›0) that are proportional to the
precision in the data, ğ‘›, and the prior precision, ğ‘›0, respectively.
The posterior sample size ğ‘›ğ‘›is the sum of the prior sample size ğ‘›0 and the
sample size ğ‘›, representing the combined precision after seeing the data for

62CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
the posterior distribution for ğœ‡. The posterior degrees of freedom ğ‘£ğ‘›are also
increased by adding the sample size ğ‘›to the prior degrees of freedom ğ‘£0.
Finally, the posterior variance hyper-parameter ğ‘ 2
ğ‘›combines three sources of
information about ğœ2 in terms of sums of squared deviations. The first term in
the square brackets is the sample variance times the sample degrees of freedom,
ğ‘ 2(ğ‘›âˆ’1) = âˆ‘
ğ‘›
ğ‘–=1(ğ‘¦ğ‘–âˆ’
Ì„ğ‘¦)2, which is the sample sum of squares. Similarly, we
may view the second term as a sum of squares based on prior data, where ğ‘ 2
0
was an estimate of ğœ2. The squared difference of the sample mean and prior
mean in the last term also provides an estimate of ğœ2, where a large value of
( Ì„ğ‘¦âˆ’ğœ‡0)2 increases the posterior sum of squares ğ‘£ğ‘›ğ‘ 2
ğ‘›.
If the sample mean is far from our prior mean, this increases the probability
that ğœ2 is large. Adding these three sum of squares provides the posterior sum
of square, and dividing by the posterior posterior degrees of freedom we obtain
the new hyper-parameter ğ‘ 2
ğ‘›, which is an estimate of ğœ2 combining the sources
of variation from the prior and the data.
The joint posterior distribution for the pair ğœ‡and ğœ™
(ğœ‡, ğœ™) âˆ£data âˆ¼NormalGamma(ğ‘šğ‘›, ğ‘›ğ‘›, ğ‘ 2
ğ‘›, ğ‘£ğ‘›)
is in the normal-gamma family, and is equivalent to a hierarchical model
specified in two stages: in the first stage of the hierarchy the inverse variance
or precision marginally has a gamma distribution,
1/ğœ2 âˆ£data âˆ¼Gamma(ğ‘£ğ‘›/2, ğ‘ 2
ğ‘›ğ‘£ğ‘›/2)
and in the second stage, ğœ‡given ğœ
ğœ‡âˆ£data, ğœ2 âˆ¼Normal(ğ‘šğ‘›, ğœ2/ğ‘›ğ‘›)
has a conditional normal distribution. We will see in the next chapter how this
representation is convenient for generating samples from the posterior distribu-
tion.
4.1.3
Marginal Distribution for ğœ‡: Student ğ‘¡
The joint normal-gamma posterior summarizes our current knowledge about ğœ‡
and ğœ2, however, we are generally interested in inference about ğœ‡uncondition-
ally as ğœ2 is unknown. This marginal inference requires the unconditional or
marginal distribution of ğœ‡that â€˜averagesâ€™ over the uncertainty in ğœ. For con-
tinuous variables like ğœ, this averaging is performed by integration leading to a
Student ğ‘¡distribution.
The standardized Student ğ‘¡-distribution tğœˆwith ğœˆdegrees of freedom is defined
to be
ğ‘(ğ‘¡) =
1
âˆšğœ‹ğœˆ
Î“( ğœˆ+1
2 )
Î“( ğœˆ
2) (1 + ğ‘¡2
ğœˆ)
âˆ’ğœˆ+1
2

4.1. THE NORMAL-GAMMA CONJUGATE FAMILY
63
where the Î“(â‹…) is the Gamma function defined earlier in Equation (2.4). The
standard Studentâ€™s ğ‘¡-distribution is centered at 0 (the location parameter), with
a scale parameter equal to 1, like in a standard normal, however, there is an
additional parameter, ğœˆ, the degrees of freedom parameter.
The Student ğ‘¡distribution is similar to the normal distribution as it is symmetric
about the center and bell shaped, however, the tails of the distribution are fatter
or heavier than the normal distribution and therefore, it is a little â€œshorterâ€ in
the middle as illustrated in Figure ??
0.0
0.1
0.2
0.3
0.4
âˆ’4
âˆ’2
0
2
4
density
standard normal
Student t t(4,0,1)
Figure 4.1: Standard normal and Student t densities.
Similar to the normal distribution, we can obtain other Student ğ‘¡distributions
by changing the center of the distribution and changing the scale. A Student t
distribution with a location ğ‘šand scale ğ‘ with ğ‘£degrees of freedom is denoted
as t(ğ‘£, ğ‘š, ğ‘ 2), with the standard Student t as a special case, t(ğœˆ, 0, 1).
The density for a ğ‘‹âˆ¼t(ğ‘£, ğ‘š, ğ‘ 2) random variable is
ğ‘(ğ‘¥) =
Î“ ( ğ‘£+1
2 )
âˆšğœ‹ğ‘£ğ‘ Î“ ( ğ‘£
2) (1 + 1
ğ‘£(ğ‘¥âˆ’ğ‘š
ğ‘ 
)
2
)
âˆ’ğ‘£+1
2
(4.9)
and by subtracting the location ğ‘šand dividing by the scale ğ‘ :
ğ‘‹âˆ’ğ‘š
ğ‘ 
â‰¡ğ‘¡âˆ¼t(ğ‘£, 0, 1)
we can obtain the distribution of the standardized Student ğ‘¡distribution with
degrees of freedom ğ‘£, location 0 and scale 1. This latter representation allows
us to use standard statistical functions for posterior inference such as finding
credible intervals.
We are now ready for our main result for the marginal distribution for ğœ‡. :::
{.definition #unnamed-chunk-1} If ğœ‡and 1/ğœ2 have a NormalGamma(ğ‘šğ‘›, ğ‘›ğ‘›, ğ‘£ğ‘›, ğ‘ 2
ğ‘›)
posterior distribution, then ğœ‡given the data has a distribution, t(ğ‘£ğ‘›, ğ‘šğ‘›, ğ‘ 2
ğ‘›/ğ‘›ğ‘›),
expressed as
ğœ‡âˆ£data âˆ¼t(ğ‘£ğ‘›, ğ‘šğ‘›, ğ‘ 2
ğ‘›/ğ‘›ğ‘›)

64CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
with degrees of freedom ğ‘£ğ‘›, location parameter, ğ‘šğ‘›, and squared scale param-
eter, ğ‘ 2
ğ‘›/ğ‘›ğ‘›, that is the posterior variance parameter divided by the posterior
sample size.
:::
The parameters ğ‘šğ‘›and ğ‘ 2
ğ‘›play similar roles in determining the center and
spread of the distribution, as in the normal distribution, however, as Student ğ‘¡
distributions with degrees of freedom less than 3 do not have a mean or variance,
the parameter ğ‘šğ‘›is called the location or center of the distribution and the
ğ‘ ğ‘›/âˆšğ‘›is the scale.
Letâ€™s use this result to find credible intervals for ğœ‡.
4.1.4
Credible Intervals for ğœ‡
To find a credible interval for the mean ğœ‡, we will use the marginal posterior
distribution for ğœ‡as illustrated in Figure 4.2. Since the Student ğ‘¡distribution
of ğœ‡is unimodal and symmetric, the shortest 95 percent credible interval or the
Highest Posterior Density interval, HPD for short, is the interval given by
the dots at the lower endpoint L and upper endpoint U where the heights of the
density at L and U are equal and all other values for ğœ‡have higher posterior
density. The probability that ğœ‡is in the interval (L, U) (the shaded area) equals
the desired probability, e.g. 0.95 for a 95% credible interval.
Using the standardized Student ğ‘¡distribution and some algebra, these values
are
ğ¿= ğ‘šğ‘›+ ğ‘¡0.025âˆšğ‘ 2ğ‘›/ğ‘›ğ‘›
ğ‘ˆ= ğ‘šğ‘›+ ğ‘¡0.975âˆšğ‘ 2ğ‘›/ğ‘›ğ‘›
or the posterior mean (our point estimate) plus quantiles of the standard ğ‘¡
distribution times the scale. Because of the symmetry in the Student ğ‘¡distri-
bution, the credible interval for ğœ‡is ğ‘šğ‘›Â± ğ‘¡0.975âˆšğ‘ 2ğ‘›/ğ‘›ğ‘›, which is similar to the
expressions for confidence intervals for the mean.
4.1.5
Example: TTHM in Tapwater
A municipality in North Carolina is interested in estimating the levels of TTHM
in their drinking water. The data can be loaded from the statsr package in R,
where the variable of interest, tthm is measured in parts per billion.
library(statsr)
data(tapwater)
glimpse(tapwater)
## Rows: 28
## Columns: 6
## $ date
<fct> 2009-02-25, 2008-12-22, 2008-09-25, 2008-05-14, 2008-04-14,~
## $ tthm
<dbl> 34.38, 39.33, 108.63, 88.00, 81.00, 49.25, 75.00, 82.86, 85~

4.1. THE NORMAL-GAMMA CONJUGATE FAMILY
65
0.0
0.3
0.6
0.9
1.2
40
50
mu
Density
Figure 4.2: Highest Posterior Density region.

66CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
## $ samples
<int> 8, 9, 8, 8, 2, 8, 6, 7, 8, 4, 4, 4, 4, 6, 4, 8, 10, 10, 10,~
## $ nondetects <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ min
<dbl> 32.00, 31.00, 85.00, 75.00, 81.00, 26.00, 70.00, 70.00, 80.~
## $ max
<dbl> 39.00, 46.00, 120.00, 94.00, 81.00, 68.00, 80.00, 90.00, 90~
Using historical prior information about TTHM from the municipality, we
will adopt a normal-gamma prior distribution, NormalGamma(35, 25, 156.25, 24)
with a prior mean of 35 parts per billion, a prior sample size of 25, an estimate
of the variance of 156.25 with degrees of freedom 24. In Section 4.3, we will
describe how we arrived at these values.
Using the summaries of the data,
Ì„ğ‘Œ= 55.5, variance ğ‘ 2 = 540.7 and sample
size of ğ‘›= 28 with the prior hyper-parameters from above, the posterior hyper-
parameters are updated as follows:
ğ‘›ğ‘›
=
25 + 28 = 53
ğ‘šğ‘›
=
28 Ã— 55.5 + 25 Ã— 35
53
= 45.8
ğ‘£ğ‘›
=
24 + 28 = 52
ğ‘ 2
ğ‘›
=
(ğ‘›âˆ’1)ğ‘ 2 + ğ‘£0ğ‘ 2
0 + ğ‘›0ğ‘›(ğ‘š0 âˆ’
Ì„ğ‘Œ)2/ğ‘›ğ‘›
ğ‘£ğ‘›
=
1
52 [27 Ã— 540.7 + 24 Ã— 156.25 + 25 Ã— 28
53
Ã— (35 âˆ’55.5)2] = 459.9
in the conjugate NormalGamma(45.8, 53, 459.9, 52) posterior distribution that
now summarizes our uncertainty about ğœ‡and ğœ™(ğœ2) after seeing the data.
We can obtain the updated hyper-parameters in R using the following code in R
# prior hyper-parameters
m_0 = 35; n_0 = 25;
s2_0 = 156.25; v_0 = n_0 - 1
# sample summaries
Y = tapwater$tthm
ybar = mean(Y)
s2 = var(Y)
n = length(Y)
# posterior hyperparameters
n_n = n_0 + n
m_n = (n*ybar + n_0*m_0)/n_n
v_n = v_0 + n
s2_n = ((n-1)*s2 + v_0*s2_0 + n_0*n*(m_0 - ybar)^2/n_n)/v_n
Using the following code in R the 95% credible interval for the tap water data
may be obtained using the Student ğ‘¡quantile function qt.
m_n + qt(c(0.025, 0.975), v_n)*sqrt(s2_n/n_n)
## [1] 39.93192 51.75374

4.1. THE NORMAL-GAMMA CONJUGATE FAMILY
67
The qt function takes two arguments: the first is the desired quantiles, while
the second is the degrees of freedom. Both arguments may be vectors, in which
case, the result will be a vector.
While we can calculate the interval directly as above, we have provided the
bayes_inference function in the statsr package to calculate the posterior
hyper-parameters, credible intervals and plot the posterior density and the HPD
interval given the raw data:
bayes_inference(tthm, data=tapwater, prior="NG",
mu_0 = m_0, n_0=n_0, s_0 = sqrt(s2_0), v_0 = v_0,
stat="mean", type="ci", method="theoretical",
show_res=TRUE, show_summ=TRUE, show_plot=FALSE)
## Single numerical variable
## n = 28, y-bar = 55.5239, s = 23.254
## (Assuming proper prior:
mu | sigma^2 ~ N(35, *sigma^2/25)
## (Assuming proper prior: 1/sigma^2 ~ G(24/2,156.25*24/2)
##
## Joint Posterior Distribution for mu and 1/sigma^2:
##
N(45.8428, sigma^2/53) G(52/2, 8.6769*52/2)
##
## Marginal Posterior for mu:
## Student t with posterior mean = 45.8428, posterior scale = 2.9457 on 52 df
##
## 95% CI: (39.9319 , 51.7537)
Letâ€™s try to understand the arguments to the function. The first argument of
the function is the variable of interest, tthm, while the second argument is a
dataframe with the variable. The argument prior="NG" indicates that we are
using a normal-gamma prior; later we will present alternative priors. The next
two lines provide our prior hyper-parameters.
The line with stat="mean",
type="ci" indicate that we are interested in inference about the population
mean ğœ‡and to calculate a credible interval for ğœ‡.
The argument method =
theoretical indicates that we will use the exact quantiles of the Student ğ‘¡
distribution to obtain our posterior credible intervals. Looking at the output
the credible interval agrees with the interval we calculated from the summaries
using the t quantiles. The other arguments are logical variables to toggle on/off
the various output.
In this case we have suppressed producing the plot of
the posterior distribution using the option show_plot=FALSE, however, set-
ting this to TRUE produces the density and credible interval shown in Figure
@ref{fig:tapwater-post-mu}.
How do we interpret these results? Based on the updated posterior, we find
that there is a 95% chance that the mean TTHM concentration is between 39.9
parts per billion and 51.8 parts per billion, suggesting that for this period that
the municipality is in compliance with the limits.

68CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
ggplot(data=tapwater, aes(x=tthm)) + geom_histogram()
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
0
1
2
3
4
30
60
90
tthm
count
4.1.6
Section Summary
The normal-gamma conjugate prior for inference about an unknown mean and
variance for samples from a normal distribution allows simple expressions for
updating prior beliefs given the data.
The joint normal-gamma distribution
leads to the Student ğ‘¡distribution for inference about ğœ‡when ğœ2 is unknown.
The Student ğ‘¡distribution can be used to provide credible intervals for ğœ‡using
R or other software that provides quantiles of a standard ğ‘¡distribution.
For the energetic learner who is comfortable with calculus, the optional material
at the end of this section provides more details on how the posterior distributions
were obtained and other results in this section.
For those that are ready to move on, we will introduce Monte Carlo sampling in
the next section; Monte Carlo sampling is a simulation method that will allow
us to approximate distributions of transformations of the parameters without
using calculus or change of variables, as well as assist exploratory data analysis
of the prior or posterior distributions.

4.1. THE NORMAL-GAMMA CONJUGATE FAMILY
69
4.1.7
(Optional) Derivations
From Bayes Theorem we have that the joint posterior distribution is propor-
tional to the likelihood of the parameters times the joint prior distribution
ğ‘(ğœ‡, ğœ2 âˆ£ğ‘¦1, â€¦ , ğ‘¦ğ‘›) âˆğ¿(ğœ‡, ğœ2)ğ‘(ğœ‡, ğœ2).
(4.10)
where the likelihood function for ğœ‡and ğœ2 is proportional to
ğ¿(ğœ‡, ğœ2) âˆ(ğœ2)âˆ’ğ‘›/2 Ã— exp {âˆ’1
2
âˆ‘
ğ‘›
ğ‘–=1(ğ‘¦ğ‘–âˆ’Ì„ğ‘¦)2
ğœ2
} Ã— exp {âˆ’1
2
ğ‘›( Ì„ğ‘¦âˆ’ğœ‡)2
ğœ2
}
function of ğœ2 and data Ã— function of ğœ‡, ğœ2 and data
which depends on the data only through the sum of squares âˆ‘
ğ‘›
ğ‘–=1(ğ‘¦ğ‘–âˆ’Ì„ğ‘¦)2 (or
equivalently the sample variance ğ‘ 2 = âˆ‘
ğ‘›
ğ‘–=1(ğ‘¦ğ‘–âˆ’
Ì„ğ‘¦)2/(ğ‘›âˆ’1)) and the sample
mean
Ì„ğ‘¦. Since the likelihood function for (ğœ‡, ğœ™) is obtained by just substituting
1/ğœ™for ğœ2, the likelihood may be re-expressed as
ğ¿(ğœ‡, ğœ™) âˆğœ™ğ‘›/2 Ã— exp {âˆ’1
2ğœ™(ğ‘›âˆ’1)ğ‘ 2} Ã— exp {âˆ’1
2ğœ™ğ‘›( Ì„ğ‘¦âˆ’ğœ‡)2}.
(4.11)
This likelihood may be obtained also be obtained by using the sampling distri-
bution for the summary statistics, where
Ì„ğ‘Œâˆ£ğœ‡, ğœ™âˆ¼Normal(ğœ‡, 1/(ğœ™ğ‘›))
and is independent of the sample variance (conditional on ğœ™) and has a gamma
distribution
ğ‘ 2 âˆ£ğœ™âˆ¼Gamma (ğ‘›âˆ’1
2
, (ğ‘›âˆ’1)ğœ™
2
)
with degrees of freedom ğ‘›âˆ’1 and rate (ğ‘›âˆ’1)ğœ™/2; the likelihood is the product
of the two sampling distributions: ğ¿(ğœ‡, ğœ™) âˆğ‘(ğ‘ 2 âˆ£ğœ™)ğ‘( Ì„ğ‘Œâˆ£ğœ™).
Bayes theorem in proportional form leads to the joint posterior distribution
ğ‘(ğœ‡, ğœ™âˆ£data) âˆğ¿(ğœ‡, ğœ™)ğ‘(ğœ™)ğ‘(ğœ‡âˆ£ğœ™)
=ğœ™(ğ‘›âˆ’1)/2) exp {âˆ’ğœ™(ğ‘›âˆ’1)ğ‘ 2
2
} (sampling distribution for ğœ™)
Ã— (ğ‘›ğœ™)1/2 exp {âˆ’1
2ğ‘›ğœ™( Ì„ğ‘¦âˆ’ğœ‡)2} ( sampling distribution for ğœ‡)
Ã— ğœ™ğœˆ0/2âˆ’1 exp{âˆ’ğœ™ğœˆ0ğ‘ 2
0
2
} (prior for ğœ™)
Ã— (ğ‘›0ğœ™)1/2
1
âˆš(2ğœ‹)
exp {âˆ’1
2ğ‘›0ğœ™(ğœ‡âˆ’ğ‘š0)2} (prior for ğœ‡)
where we have ignored constants that do not involve ğœ™or ğœ‡. Focusing on all
the terms that involve ğœ‡, we can group the lines corresponding to the sampling

70CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
distribution and prior for ğœ‡together and using the factorization of likelihood
and prior distributions, we may identify that
ğ‘(ğœ‡âˆ£ğœ™, data) âˆexp {âˆ’1
2ğ‘›ğœ™( Ì„ğ‘¦âˆ’ğœ‡)2 âˆ’1
2ğ‘›0ğœ™(ğœ‡âˆ’ğ‘š0)2}
where the above expression includes the sum of two quadratic expressions in the
exponential. This almost looks like a normal. Can these be combined to form
one quadratic expression that looks like a normal density? Yes! This is known
as â€œcompleting the squareâ€. Taking a normal distribution for a parameter ğœ‡
with mean ğ‘šand precision ğœŒ, the quadratic term in the exponential may be
expanded as
ğœŒÃ— (ğœ‡âˆ’ğ‘š)2 = ğœŒğœ‡2 âˆ’2ğœŒğœ‡ğ‘š+ ğœŒğ‘š2.
From this we can read off that the precision is the term that multiplies the
quadratic in ğœ‡and the term that multiplies the linear term in ğœ‡is the product
of two times the mean and precision; this means that if we know the precision,
we can identify the mean. The last term is the precision times the mean squared,
which we will need to fill in once we identify the precision and mean.
For our posterior, we need to expand the quadratics and recombine terms to
identify the new precision (the coeï¬€icient multiplying the quadratic in ğœ‡) and
the new mean (the linear term) and complete the square so that it may be
factored. Any left over terms will be independent of ğœ‡but may depend on ğœ™.
For our case, after some algebra to group terms we have
âˆ’1
2 (ğ‘›ğœ™( Ì„ğ‘¦âˆ’ğœ‡)2 + ğ‘›0ğœ™(ğœ‡âˆ’ğ‘š0)2) = âˆ’1
2 (ğœ™(ğ‘›+ ğ‘›0)ğœ‡2 âˆ’2ğœ™ğœ‡(ğ‘›Ì„ğ‘¦+ ğ‘›0ğ‘š0) + ğœ™(ğ‘›Ì„ğ‘¦2 + ğ‘›0ğ‘š2
0))
where we can read off that the posterior precision is ğœ™(ğ‘›+ğ‘›0) â‰¡ğœ™ğ‘›ğ‘›. The linear
term is not yet of the form of the posterior precision times the posterior mean
(times 2), but if we multiply and divide by ğ‘›ğ‘›= ğ‘›+ ğ‘›0 it is in the appropriate
form
âˆ’1
2 (ğœ™(ğ‘›+ ğ‘›0)ğœ‡2 âˆ’2ğœ™(ğ‘›+ ğ‘›0)ğœ‡(ğ‘›Ì„ğ‘¦+ ğ‘›0ğ‘š0)
ğ‘›+ ğ‘›0
+ ğœ™(ğ‘›Ì„ğ‘¦2 + ğ‘›0ğ‘š2
0))
(4.12)
so that we may identify that the posterior mean is ğ‘šğ‘›= (ğ‘›Ì„ğ‘¦+ ğ‘›0ğ‘š0)/(ğ‘›+ ğ‘›0)
which combined with the precision (or inverse variance) is enough to identity
the conditional posterior distribution for ğœ‡. We next add the precision times
the square of the posterior mean (the completing the square part), but to keep
equality, we will need to subtract the term as well:
âˆ’1
2 (ğ‘›ğœ™( Ì„ğ‘¦âˆ’ğœ‡)2 + ğ‘›0ğœ™(ğœ‡âˆ’ğ‘š0)2) = âˆ’1
2 (ğœ™ğ‘›ğ‘›ğœ‡2 âˆ’2ğœ™ğ‘›ğ‘›ğœ‡ğ‘šğ‘›+ ğœ™ğ‘›ğ‘›ğ‘š2
ğ‘›âˆ’ğœ™ğ‘›ğ‘›ğ‘š2
ğ‘›+ ğœ™(ğ‘›Ì„ğ‘¦2 + ğ‘›0ğ‘š2
0))
which after factoring the quadratic leads to
âˆ’1
2 (ğ‘›ğœ™( Ì„ğ‘¦âˆ’ğœ‡)2 + ğ‘›0ğœ™(ğœ‡âˆ’ğ‘š0)2) = âˆ’1
2 (ğœ™ğ‘›ğ‘›(ğœ‡âˆ’ğ‘šğ‘›)2)
(4.13)
âˆ’1
2 (ğœ™(âˆ’ğ‘›ğ‘›ğ‘š2
ğ‘›+ ğ‘›Ì„ğ‘¦2 + ğ‘›0ğ‘š2
0))
(4.14)

4.1. THE NORMAL-GAMMA CONJUGATE FAMILY
71
where the first line is the quadratic for the posterior of ğœ‡given ğœ™while the
second line includes terms that involve ğœ™but that are independent of ğœ‡.
Substituting the expressions, we can continue to simplify the expressions further
ğ‘(ğœ‡, ğœ™âˆ£data) âˆğ¿(ğœ‡, ğœ™)ğ‘(ğœ™)ğ‘(ğœ‡âˆ£ğœ™)
=ğœ™(ğ‘›+ğœˆ0+1)/2âˆ’1 exp {âˆ’ğœ™(ğ‘›âˆ’1)ğ‘ 2
2
} Ã— exp {âˆ’ğœ™ğœˆ0ğ‘ 2
0
2
} Ã— exp {âˆ’1
2 (ğœ™(âˆ’ğ‘›ğ‘›ğ‘š2
ğ‘›+ ğ‘›Ì„ğ‘¦2 + ğ‘›0ğ‘š2
0))}
Ã— exp {âˆ’1
2 (ğœ™ğ‘›ğ‘›(ğœ‡âˆ’ğ‘šğ‘›)2)}
=ğœ™(ğ‘›+ğœˆ0)/2âˆ’1 exp {âˆ’ğœ™
2 ((ğ‘›âˆ’1)ğ‘ 2 + ğœˆ0ğ‘ 2
0 + ğ‘›0ğ‘›
ğ‘›ğ‘›
(ğ‘š0 âˆ’Ì„ğ‘¦)2)}
(gamma kernel)
Ã— (ğ‘›ğ‘›ğœ™)1/2 exp {âˆ’1
2 (ğœ™ğ‘›ğ‘›(ğœ‡âˆ’ğ‘šğ‘›)2)}
(normal kernel)
until we can recognize the product of the kernels of a gamma distribution for ğœ™
ğœ™âˆ£data âˆ¼t(ğ‘£ğ‘›/2, ğ‘£ğ‘›ğ‘ 2
ğ‘›/2)
where ğœˆğ‘›= ğ‘›+ ğœˆ0 and ğ‘ 2
ğ‘›= ((ğ‘›âˆ’1)ğ‘ 2 + ğ‘›0ğ‘ 2
0 + (ğ‘š0 âˆ’Ì„ğ‘¦)2ğ‘›ğ‘›0/ğ‘›ğ‘›) /ğœˆğ‘›times a
normal: and the kernel of a normal for ğœ‡
ğœ‡âˆ£ğœ™, data âˆ¼Normal(ğ‘šğ‘›, (ğœ™ğ‘›ğ‘›)âˆ’1)
where ğ‘šğ‘š= (ğ‘›Ì„ğ‘¦+ ğ‘›0ğ‘š0)/(ğ‘›+ ğ‘›0) a weighted average of the sample mean and
the prior mean, and ğ‘›ğ‘›= ğ‘›+ ğ‘›0 is the sample and prior combined sample size.
4.1.7.1
Derivation of Marginal Distribution for ğœ‡
If ğœ‡given ğœ2 (and the data) has a normal distribution with mean ğ‘šğ‘šand
variance ğœ2/ğ‘›ğ‘›and 1/ğœ2 â‰¡ğœ™(given the data) has a gamma distribution with
shape parameter ğœˆğ‘›/2 and rate parameter ğœˆğ‘›ğ‘ 2
ğ‘›/2
ğœ‡âˆ£ğœ2, data âˆ¼Normal(ğ‘šğ‘š, ğœ2/ğ‘›ğ‘›)
1/ğœ2 âˆ£data âˆ¼Gamma(ğœˆğ‘›/2, ğœˆğ‘›ğ‘ 2
ğ‘›/2)
then
ğœ‡âˆ£data âˆ¼t(ğœˆğ‘›, ğ‘šğ‘š, ğ‘ 2
ğ‘›/ğ‘›ğ‘›)
a Student ğ‘¡distribution with mean ğ‘šğ‘šand scale ğ‘ 2
ğ‘›/ğ‘›ğ‘›with degrees of freedom
ğœˆğ‘›.
This applies to the prior as well, so that without any data we use the prior
hyper-parameters ğ‘š0, ğ‘›0, ğœˆ0 and ğ‘ 2
0 in place of the updated values with the
subscript ğ‘›.
To simplify notation, weâ€™ll substitute ğœ™= 1/ğœ2. The marginal distribution for
ğœ‡is obtained by averaging over the values of ğœ2. Since ğœ2 takes on continuous

72CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
values rather than discrete, this averaging is represented as an integral
ğ‘(ğœ‡âˆ£data) = âˆ«
âˆ
0
ğ‘(ğœ‡âˆ£ğœ™, data)ğ‘(ğœ™âˆ£data)ğ‘‘ğœ™
= âˆ«
âˆ
0
1
âˆš
2ğœ‹
(ğ‘›ğ‘›ğœ™)1/2ğ‘’{âˆ’ğ‘›ğ‘›ğœ™
2
(ğœ‡âˆ’ğ‘šğ‘›)2}
1
Î“(ğœˆğ‘›/2) (ğœˆğ‘›ğ‘ 2
ğ‘›
2
)
ğœˆğ‘›/2
ğœ™ğœˆğ‘›/2âˆ’1ğ‘’{âˆ’ğœ™ğœˆğ‘›ğ‘ 2
ğ‘›/2} ğ‘‘ğœ™
= (ğ‘›ğ‘›
2ğœ‹)
1/2
1
Î“ ( ğœˆğ‘›
2 ) (ğœˆğ‘›ğ‘ 2
ğ‘›
2
)
ğœˆğ‘›/2
âˆ«
âˆ
0
ğœ™(ğœˆğ‘›+1)/2âˆ’1ğ‘’
{âˆ’ğœ™( ğ‘›ğ‘›(ğœ‡âˆ’ğ‘šğ‘›)2+ğœˆğ‘›ğ‘ 2ğ‘›
2
)} ğ‘‘ğœ™
where the terms inside the integral are the â€kernelâ€ of a Gamma density. We
can multiply and divide by the normalizing constant of the Gamma density
ğ‘(ğœ‡âˆ£data) = (ğ‘›ğ‘›
2ğœ‹)
1/2
1
Î“ ( ğœˆğ‘›
2 ) (ğœˆğ‘›ğ‘ 2
ğ‘›
2
)
ğœˆğ‘›/2
Î“ (ğœˆğ‘›+ 1
2
) (ğ‘›ğ‘›(ğœ‡âˆ’ğ‘šğ‘›)2 + ğœˆğ‘›ğ‘ 2
ğ‘›
2
)
âˆ’ğœˆğ‘›+1
2
Ã—
âˆ«
âˆ
0
1
Î“ ( ğœˆğ‘›+1
2
)
(ğ‘›ğ‘›(ğœ‡âˆ’ğ‘šğ‘›)2 + ğœˆğ‘›ğ‘ 2
ğ‘›
2
)
ğœˆğ‘›+1
2
ğœ™(ğœˆğ‘›+1)/2âˆ’1ğ‘’
{âˆ’ğœ™( ğ‘›ğ‘›(ğœ‡âˆ’ğ‘šğ‘›)2+ğœˆğ‘›ğ‘ 2ğ‘›
2
)} ğ‘‘ğœ™
so that the term in the integral now integrates to one and the resulting distri-
bution is
ğ‘(ğœ‡âˆ£data) = (ğ‘›ğ‘›
2ğœ‹)
1/2 Î“ ( ğœˆğ‘›+1
2
)
Î“ ( ğœˆğ‘›
2 )
(ğœˆğ‘›ğ‘ 2
ğ‘›
2
)
ğœˆğ‘›/2
(ğ‘›ğ‘›(ğœ‡âˆ’ğ‘šğ‘›)2 + ğœˆğ‘›ğ‘ 2
ğ‘›
2
)
âˆ’ğœˆğ‘›+1
2
.
After some algebra this simplifies to
ğ‘(ğœ‡âˆ£data) =
1
âˆšğœ‹ğœˆğ‘›ğ‘ 2ğ‘›/ğ‘›ğ‘›
Î“ ( ğœˆğ‘›+1
2
)
Î“ ( ğœˆğ‘›
2 )
(1 + 1
ğœˆğ‘›
(ğœ‡âˆ’ğ‘šğ‘›)2
ğ‘ 2ğ‘›/ğ‘›ğ‘›
)
âˆ’ğœˆğ‘›+1
2
and is a more standard representation for a Student ğ‘¡distribution and the kernel
of the density is the right most term.
4.2
Monte Carlo Inference
In Section 4.1, we showed how to obtain the conditional posterior distribution for
the mean of a normal population given the variance and the marginal posterior
distribution of the precision (inverse variance). The marginal distribution of the
mean, which â€œaveraged over uncertaintyâ€ about the unknown variance could be
obtained via integration, leading to the Student t distribution that was used for
inference about the population mean. However, what if we are interested in the

4.2. MONTE CARLO INFERENCE
73
distribution of the standard deviation ğœitself, or other transformations of the
parameters? There may not be a closed-form expression for the distributions or
they may be diï¬€icult to obtain.
It turns out that Monte Carlo sampling, however, is an easy way to make an
inference about parameters, when we cannot analytically calculate distributions
of parameters, expectations, or probabilities. Monte Carlo methods are compu-
tational algorithms that rely on repeated random sampling from distributions
for making inferences. The name refers to the famous Monte Carlo Casino in
Monaco, home to games of chance such as roulette.
4.2.1
Monte Carlo Sampling
Letâ€™s start with a case where we know the posterior distribution. As a quick
recap, recall that the joint posterior distribution for the mean ğœ‡and the precision
ğœ™= 1/ğœ2 under the conjugate prior for the Gaussian distribution is:
â€¢ Conditional posterior distribution for the mean
ğœ‡âˆ£data, ğœ2 âˆ¼Normal(ğ‘šğ‘›, ğœ2/ğ‘›ğ‘›)
â€¢ Marginal posterior distribution for the precision ğœ™or inverse variance:
1/ğœ2 = ğœ™âˆ£data âˆ¼Gamma(ğ‘£ğ‘›/2, ğ‘ 2
ğ‘›ğ‘£ğ‘›/2)
â€¢ Marginal posterior distribution for the mean
ğœ‡âˆ£data âˆ¼t(ğ‘£ğ‘›, ğ‘šğ‘›, ğ‘ 2
ğ‘›/ğ‘›ğ‘›)
For posterior inference about ğœ™, we can generate ğ‘†random samples from the
Gamma posterior distribution:
ğœ™(1), ğœ™(2), â‹¯, ğœ™(ğ‘†) iid
âˆ¼Gamma(ğ‘£ğ‘›/2, ğ‘ 2
ğ‘›ğ‘£ğ‘›/2)
Recall that the term iid stands for independent and identically distributed. In
other words, the ğ‘†draws of ğœ™are independent and identically distributed from
the gamma distribution.
We can use the empirical distribution (histogram) from the ğ‘†samples to approx-
imate the actual posterior distribution and the sample mean of the ğ‘†random
draws of ğœ™can be used to approximate the posterior mean of ğœ™. Likewise, we
can calculate probabilities, quantiles and other functions using the ğ‘†samples
from the posterior distribution. For example, if we want to calculate the pos-
terior expectation of some function of ğœ™, written as ğ‘”(ğœ™), we can approximate
that by taking the average of the function, and evaluate it at the ğ‘†draws of ğœ™,
written as 1
ğ‘†âˆ‘
ğ‘†
ğ‘–=1 ğ‘”(ğœ™(ğ‘–)).
The approximation to the expectation of the function, ğ¸[ğ‘”(ğœ™âˆ£data)] improves

74CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
1
ğ‘†
ğ‘†
âˆ‘
ğ‘–=1
ğ‘”(ğœ™(ğ‘–)) â†’ğ¸[ğ‘”(ğœ™âˆ£data)]
as the number of draws ğ‘†in the Monte Carlo simulation increases.
4.2.2
Monte Carlo Inference: Tap Water Example
We will apply this to the tap water example from 4.1. First, reload the data
and calculate the posterior hyper-parameters if needed.
# Prior
m_0 = 35;
n_0 = 25;
s2_0 = 156.25; v_0 = n_0 - 1
# Data
data(tapwater); Y = tapwater$tthm
ybar = mean(Y); s2 = var(Y); n = length(Y)
# Posterior Hyper-paramters
n_n = n_0 + n
m_n = (n*ybar + n_0*m_0)/n_n
v_n = v_0 + n
s2_n = ((n-1)*s2 + v_0*s2_0 + n_0*n*(m_0 - ybar)^2/n_n)/v_n
Before generating our Monte Carlo samples, we will set a random seed using
the set.seed function in R, which takes a small integer argument.
set.seed(42)
This allows the results to be replicated if you re-run the simulation at a later
time.
To generate 1, 000 draws from the gamma posterior distribution using the hyper-
parameters above, we use the rgamma function R
phi = rgamma(1000, shape = v_n/2, rate=s2_n*v_n/2)
The first argument to the rgamma function is the number of samples, the second
is the shape parameter and, by default, the third argument is the rate parameter.
The following code will produce a histogram of the Monte Carlo samples of ğœ™
and overlay the actual Gamma posterior density evaluated at the draws using
the dgamma function in R.
df = data.frame(phi = sort(phi))
df = mutate(df,
density = dgamma(phi,
shape = v_n/2,
rate=s2_n*v_n/2))
ggplot(data=df, aes(x=phi)) +
geom_histogram(aes(x=phi, y=..density..), bins = 50) +

4.2. MONTE CARLO INFERENCE
75
geom_density(aes(phi, ..density..), color="black") +
geom_line(aes(x=phi, y=density), color="orange") +
xlab(expression(phi)) + theme_tufte()
0
250
500
750
1000
0.001
0.002
0.003
0.004
Ï†
density
Figure 4.3: Monte Carlo approximation of the posterior distribution of the
precision from the tap water example
Figure 4.3 shows the histogram of the 1, 000 draws of ğœ™generated from the
Monte Carlo simulation, representing the empirical distribution approximation
to the gamma posterior distribution.
The orange line represents the actual
gamma posterior density, while the black line represents a smoothed version of
the histogram.
We can estimate the posterior mean or a 95% equal tail area credible region
using the Monte Carlo samples using R
mean(phi)
## [1] 0.002165663
quantile(phi, c(0.025, 0.975))
##
2.5%
97.5%
## 0.001394921 0.003056304

76CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
The mean of a gamma random variable is the shape/rate, so we can compare
the Monte Carlo estimates to the theoretical values
# mean
(v_n/2)/(v_n*s2_n/2)
1/s2_n
## [1] 0.002174492
qgamma(c(0.025, 0.975), shape=v_n/2, rate=s2_n*v_n/2)
## [1] 0.001420450 0.003086519
where the qgamma function in R returns the desired quantiles provided as the first
argument. We can see that we can estimate the mean accurately to three signif-
icant digits, while the quantiles are accurate to two. It increase our accuracy,
we would need to increase ğ‘†.
Exercise Try increasing the number of simulations ğ‘†in the Monte Carlo sim-
ulation to 10, 000, and see how the approximation changes.
4.2.3
Monte Carlo Inference for Functions of Parameters
Letâ€™s see how to use Monte Carlo simulations to approximate the distribution
of ğœ. Since ğœ= 1/âˆšğœ™, we simply apply the transformation to the 1, 000 draws
of ğœ™to obtain a random sample of ğœfrom its posterior distribution. We can
then estimate the posterior mean of ğœby calculating the sample mean of the
1,000 draws.
sigma = 1/sqrt(phi)
mean(sigma) # posterior mean of sigma
## [1] 21.80516
Similarly, we can obtain a 95% credible interval for ğœby finding the sample
quantiles of the distribution.
quantile(sigma, c(0.025, 0.975))
##
2.5%
97.5%
## 18.08847 26.77474
and finally approximate the posterior distribution using a smoothed density
estimate
Exercise
Using the 10, 000 draws of ğœ™for the tap water example, create a histogram for
ğœwith a smoothed density overlay for the tap water example.

4.2. MONTE CARLO INFERENCE
77
0.00
0.05
0.10
0.15
0.20
16
20
24
28
Ïƒ
density
Figure 4.4: Monte Carlo approximation of the posterior distribution of the
standard deviation from the tap water example

78CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
4.2.4
Summary
To recap, we have introduced the powerful method of Monte Carlo simulation
for posterior inference. Monte Carlo methods provide estimates of expectations,
probabilities, and quantiles of distributions from the simulated values. Monte
Carlo simulation also allows us to approximate distributions of functions of the
parameters, or the transformations of the parameters where it may be diï¬€icult
to get exact theoretical values.
Next, we will discuss predictive distributions and show how Monte Carlo simu-
lation may be used to help choose prior hyperparameters, using the prior pre-
dictive distribution of data and draw samples from the posterior predictive dis-
tribution for predicting future observations.
4.3
Predictive Distributions
In this section, we will discuss prior and posterior predictive distributions of
the data and show how Monte Carlo sampling from the prior predictive dis-
tribution can help select hyper-parameters, while sampling from the posterior
predictive distribution can be used for predicting future events or model check-
ing.
4.3.1
Prior Predictive Distribution
We can obtain the prior predictive distribution of the data from the joint dis-
tribution of the data and the parameters (ğœ‡, ğœ2) or equivalently (ğœ‡, ğœ™), where
ğœ™= 1/ğœ2 is the precision:
Prior:
ğœ™âˆ¼Gamma (ğ‘£0
2 , ğ‘£0ğ‘ 2
0
2 )
ğœ2 = 1/ğœ™
ğœ‡âˆ£ğœ2 âˆ¼N(ğ‘š0, ğœ2/ğ‘›0)
Sampling model:
ğ‘Œğ‘–âˆ£ğœ‡, ğœ2 iid
âˆ¼Normal(ğœ‡, ğœ2)
Prior predictive distribution for ğ‘Œ:
ğ‘(ğ‘Œ) = âˆ¬ğ‘(ğ‘Œâˆ£ğœ‡, ğœ2)ğ‘(ğœ‡âˆ£ğœ2)ğ‘(ğœ2)ğ‘‘ğœ‡ğ‘‘ğœ2
ğ‘Œâˆ¼ğ‘¡(ğ‘£0, ğ‘š0, ğ‘ 2
0 + ğ‘ 2
0/ğ‘›0)

4.3. PREDICTIVE DISTRIBUTIONS
79
By averaging over the possible values of the parameters from the prior distribu-
tion in the joint distribution, technically done by a double integral, we obtain
the Student t as our prior predictive distribution. For those interested, details
of this derivation are provided later in an optional section. This distribution of
the observables depends only on our four hyper-parameters from the normal-
gamma family. We can use Monte Carlo simulation to sample from the prior
predictive distribution to help elicit prior hyper-parameters as we now illustrate
with the tap water example from earlier.
4.3.2
Tap Water Example (continued)
A report from the city water department suggests that levels of TTHM are
expected to be between 10-60 parts per billion (ppb). Letâ€™s see how we can use
this information to create an informative conjugate prior.
Prior Mean First, the normal distribution and Student t distributions are
symmetric around the mean or center parameter, so we will set the prior mean
ğœ‡to be at the midpoint of the interval 10-60, which would lead to
ğ‘š0 = (60 + 10)/2 = 35
as our prior hyper-parameter ğ‘š0.
Prior Variance Based on the empirical rule for bell-shaped distributions, we
would expect that 95% of observations are within plus or minus two standard
deviations from the mean, Â±2ğœof ğœ‡. Using this we expect that the range of the
data should be approximately 4ğœ. Using the values from the report, we can use
this to find our prior estimate of ğœ, ğ‘ 0 = (60 âˆ’10)/4 = 12.5 or
ğ‘ 2
0 = [(60 âˆ’10)/4]2 = 156.25
Prior Sample Size and Degrees of Freedom To complete the specification,
we also need to choose the prior sample size ğ‘›0 and degrees of freedom ğ‘£0. For
a sample of size ğ‘›, the sample variance has ğ‘›âˆ’1 degrees of freedom. Thinking
about a possible historic set of data of size ğ‘›0 that led to the reported interval,
we will adopt that rule to obtain the prior degrees of freedom ğ‘£0 = ğ‘›0 âˆ’1,
leaving only the prior sample size to be determined. We will draw samples from
the prior predictive distribution and modify ğ‘›0 so that the simulated data agree
with our prior assumptions.
4.3.3
Sampling from the Prior Predictive in R
The following R code shows a simulation from the predictive distribution with
the prior sample size ğ‘›0 = 2. Please be careful to not confuse the prior sample
size, ğ‘›0, that represents the precision of our prior information with the number
of Monte Carlo simulations, ğ‘†= 10000, that are drawn from the distributions.

80CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
These Monte Carlo samples are used to estimate quantiles of the prior pre-
dictive distribution and a large value of ğ‘†reduces error in the Monte Carlo
approximation.
m_0 = (60+10)/2; s2_0 = ((60-10)/4)^2;
n_0 = 2; v_0 = n_0 - 1
set.seed(1234)
S = 10000
phi = rgamma(S, v_0/2, s2_0*v_0/2)
sigma = 1/sqrt(phi)
mu = rnorm(S, mean=m_0, sd=sigma/(sqrt(n_0)))
Y = rnorm(S, mu, sigma)
quantile(Y, c(0.025,0.975))
##
2.5%
97.5%
## -140.1391
217.7050
Letâ€™s try to understand the code. After setting the prior hyper-parameters and
random seed, we begin by simulating ğœ™from its gamma prior distribution. We
then transform ğœ™to calculate ğœ. Using the draws of ğœ, we feed that into the
rnorm function to simulate ğ‘†values of ğœ‡for each value of ğœ. The Monte Carlo
draws of ğœ‡, ğœare used to generate ğ‘†possible values of TTHM denoted by ğ‘Œ.
In the above code we are exploiting that all of the functions for simulating
from distributions can be vectorized, i.e. we can provide all ğ‘†draws of ğœ™to
the functions and get a vector result back without having to write a loop. Fi-
nally, we obtain the empirical quantiles from our Monte Carlo sample using the
quantile function to approximate the actual quantiles from the prior predictive
distriubtion.
This forward simulation propagates uncertainty in ğœ‡and ğœto the prior predic-
tive distribution of the data. Calculating the sample quantiles from the samples
of the prior predictive for ğ‘Œ, we see that the 95% predictive interval for TTHM
includes negative values. Since TTHM cannot be negative, we can adjust ğ‘›0
and repeat. Since we need a narrower interval in order to exclude zero, we can
increase ğ‘›0 until we achieve the desired quantiles.
After some trial and error, we find that the prior sample size of 25, the empirical
quantiles from the prior predictive distribution are close to the range of 10 to
60 that we were given as prior information.
m_0 = (60+10)/2; s2_0 = ((60-10)/4)^2;
n_0 = 25; v_0 = n_0 - 1
set.seed(1234)
phi = rgamma(10000, v_0/2, s2_0*v_0/2)
sigma = 1/sqrt(phi)
mu = rnorm(10000, mean=m_0, sd=sigma/(sqrt(n_0)))
y = rnorm(10000, mu, sigma)
quantile(y, c(0.025,0.975))

4.3. PREDICTIVE DISTRIBUTIONS
81
##
2.5%
97.5%
##
8.802515 61.857350
Figure 4.5 shows an estimate of the prior distribution of ğœ‡in gray and the more
dispersed prior predictive distribution in TTHM in orange, obtained from the
Monte Carlo samples.
0.00
0.05
0.10
0.15
0
25
50
75
100
TTHM (ppb)
density
parameter
mu
Y
Figure 4.5: Prior density
Using the Monte Carlo samples, we can also estimate the prior probability of
negative values of TTHM by counting the number of times the simulated values
are less than zero out of the total number of simulations.
sum(y < 0)/length(y)
# P(Y < 0) a priori
## [1] 0.0049
With the normal prior distribution, this probability will never be zero, but may
be acceptably small, so we may use the conjugate normal-gamma model for
analysis.
4.3.4
Posterior Predictive
We can use the same strategy to generate samples from the predictive distri-
bution of a new measurement ğ‘Œğ‘›+1 given the observed data. In mathematical
terms, the posterior predictive distribution is written as
ğ‘Œğ‘›+1 âˆ£ğ‘Œ1, â€¦ , ğ‘Œğ‘›âˆ¼t(ğ‘£ğ‘›, ğ‘šğ‘›, ğ‘ 2
ğ‘›(1 + 1/ğ‘›ğ‘›))

82CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
In the code, we replace the prior hyper parameters with the posterior hyper
parameters from last time.
set.seed(1234)
phi = rgamma(10000, v_n/2, s2_n*v_n/2)
sigma = 1/sqrt(phi)
post_mu = rnorm(10000, mean=m_n, sd=sigma/(sqrt(n_n)))
pred_y =
rnorm(10000,post_mu, sigma)
quantile(pred_y, c(.025, .975))
##
2.5%
97.5%
##
3.280216 89.830212
Figure 4.6 shows the Monte Carlo approximation to the prior distribution of ğœ‡,
and the posterior distribution of ğœ‡which is shifted to the right. The prior and
posterior predictive distributions are also depicted, showing how the data have
updated the prior information.
0.00
0.05
0.10
0.15
0
50
100
TTHM (ppb)
density
parameter
posterior mu
posterior predictive Y
prior mu
prior predictive Y
Figure 4.6: Posterior densities
Using the Monte Carlo samples from the posterior predictive distribution, we
can estimate the probability that a new TTHM sample will exceed the legal
limit of 80 parts per billion, which is approximately 0.06.
sum(pred_y > 80)/length(pred_y)
# P(Y > 80 | data)
## [1] 0.0619

4.4. REFERENCE PRIORS
83
4.3.5
Summary
By using Monte Carlo methods, we can obtain prior and posterior predictive
distributions of the data.
â€¢ Sampling from the prior predictive distribution can help with the selection
of prior hyper parameters and verify that these choices reflect the prior
information that is available.
â€¢ Visualizing prior predictive distributions based on Monte Carlo simula-
tions can help explore implications of our prior assumptions such as the
choice of the hyper parameters or even assume distributions.
â€¢ If samples are incompatible with known information, such as support on
positive values, we may need to modify assumptions and look at other
families of prior distributions.
4.4
Reference Priors
In Section 4.3, we demonstrated how to specify an informative prior distribution
for inference about TTHM in tapwater using additional prior information. The
resulting informative Normal-Gamma prior distribution had an effective prior
sample size that was comparable to the observed sample size to be compatible
with the reported prior interval.
There are, however, situations where you may wish to provide an analysis that
does not depend on prior information. There may be cases where prior informa-
tion is simply not available. Or, you may wish to present an objective analysis
where minimal prior information is used to provide a baseline or reference anal-
ysis to contrast with other analyses based on informative prior distributions.
Or perhaps, you want to use the Bayesian paradigm to make probability state-
ments about parameters, but not use any prior information. In this section, we
will examine the qustion of Can you actually perform a Bayesian analy-
sis without using prior information? We will present reference priors for
normal data, which can be viewed as a limiting form of the Normal-Gamma
conjugate prior distribution.
Conjugate priors can be interpreted to be based on a historical or imaginary
prior sample. What happens in the conjugate Normal-Gamma prior if we take
our prior sample size ğ‘›0 to go to zero? If we have no data, then we will define the
prior sample variance ğ‘ 2
0 to go to 0, and based on the relationship between prior
sample sized and prior degrees of freedom, we will let the prior degrees of freedom
go to the prior sample size minus one, or negative one, i.e. ğ‘£0 = ğ‘›0 âˆ’1 â†’âˆ’1.
With this limit, we have the following properties:
â€¢ The posterior mean goes to the sample mean.
â€¢ The posterior sample size is the observed sample size.

84CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
â€¢ The posterior degrees of freedom go to the sample degrees of freedom.
â€¢ The posterior variance parameter goes to the sample variance.
In this limit, the posterior hyperparameters do not depend on the prior hyper-
parameters.
Since ğ‘›0 â†’0, ğ‘ 2
0 â†’0, ğ‘£0 = ğ‘›0 âˆ’1 â†’âˆ’1, we have in mathematical terms:
ğ‘šğ‘›= ğ‘›Ì„ğ‘Œ+ ğ‘›0ğ‘š0
ğ‘›+ ğ‘›0
â†’
Ì„ğ‘Œ
ğ‘›ğ‘›= ğ‘›0 + ğ‘›â†’ğ‘›
ğ‘£ğ‘›= ğ‘£0 + ğ‘›â†’ğ‘›âˆ’1
ğ‘ 2
ğ‘›= 1
ğ‘£ğ‘›
[ğ‘ 2
0ğ‘£0 + ğ‘ 2(ğ‘›âˆ’1) + ğ‘›0ğ‘›
ğ‘›ğ‘›
( Ì„ğ‘Œâˆ’ğ‘š0)2] â†’ğ‘ 2
This limiting normal-gamma distribution, NormalGamma(0, 0, 0, âˆ’1), is not re-
ally a normal-gamma distribution, as the density does not integrate to 1. The
form of the limit can be viewed as a prior for ğœ‡that is proportional to a con-
stant, or uniform/flat on the whole real line. And a prior for the variance is
proportional to 1 over the variance. The joint prior is taken as the product of
the two.
ğ‘(ğœ‡âˆ£ğœ2) âˆ1
ğ‘(ğœ2) âˆ1/ğœ2
ğ‘(ğœ‡, ğœ2) âˆ1/ğœ2
This is refered to as a reference prior because the posterior hyperparameters
do not depend on the prior hyperparameters.
In addition, NormalGamma(0, 0, 0, âˆ’1) is a special case of a reference prior,
known as the independent Jeffreys prior. While Jeffreys used other arguments
to arrive at the form of the prior, the goal was to have an objective prior
invariant to shifting the data by a constant or multiplying by a constant.
Now, a naive approach to constructing a non-informative distribution might be
to use a uniform distribution to represent lack of knowledge. However, would
you use a uniform distribution for ğœ2, or a uniform distribution for the preci-
sion 1/ğœ2? Or perhaps a uniform distribution for ğœ? These would all lead to
different posteriors with little justification for any of them. This ambiguity led
Sir Harold Jeffreys to propose reference distributions for the mean and variance
for situations where prior information was limited. These priors are invariant
to the units of the data.
The unnormalized priors that do not integrate to a constant are called im-
proper distributions. An important consideration in using them is that one

4.4. REFERENCE PRIORS
85
cannot generate samples from the prior or the prior predictive distribution to
data and are referred to as non-generative distributions.
While the reference prior is not a proper prior distribution, and cannot reflect
anyoneâ€™s actual prior beliefs, the formal application phase rule can still be used
to show that the posterior distribution is a valid normal gamma distri-
bution, leading to a formal phase posterior distribution. That depends only on
summary statistics of the data.
The posterior distribution NormalGamma( Ì„ğ‘Œ, ğ‘›, ğ‘ 2, ğ‘›âˆ’1) breaks down to
ğœ‡âˆ£ğœ2, data âˆ¼Normal( Ì„ğ‘Œ, ğœ2/ğ‘›)
1/ğœ2 âˆ£data âˆ¼Gamma((ğ‘›âˆ’1)/2, ğ‘ 2(ğ‘›âˆ’1)/2).
â€¢ Under the reference prior ğ‘(ğœ‡, ğœ2) âˆ1/ğœ2, the posterior distribution after
standardizing ğœ‡has a Student ğ‘¡distribution with ğ‘›âˆ’1 degrees of freedom.
ğœ‡âˆ’
Ì„ğ‘Œ
âˆšğ‘ 2/ğ‘›
âˆ£data âˆ¼t(ğ‘›âˆ’1, 0, 1)
â€¢ Prior to seeing the data, the distribution of the standardized sample mean
given ğœ‡and ğœalso has a Student t distribution.
ğœ‡âˆ’
Ì„ğ‘Œ
âˆšğ‘ 2/ğ‘›
âˆ£ğœ‡, ğœ2 âˆ¼t(ğ‘›âˆ’1, 0, 1)
â€¢ Both frequentist sampling distributions and Bayesian reference posterior
distributions lead to intervals of this form:
( Ì„ğ‘Œâˆ’ğ‘¡1âˆ’ğ›¼/2 Ã— ğ‘ /âˆšğ‘›,
Ì„ğ‘Œ+ ğ‘¡1âˆ’ğ›¼/2 Ã— ğ‘ /âˆšğ‘›)
â€¢ However, only the Bayesian approach justifies the probability statements
about ğœ‡being in the interval after seeing the data.
ğ‘ƒ( Ì„ğ‘Œâˆ’ğ‘¡1âˆ’ğ›¼/2 Ã— ğ‘ /âˆšğ‘›< ğœ‡<
Ì„ğ‘Œ+ ğ‘¡1âˆ’ğ›¼/2 Ã— ğ‘ /âˆšğ‘›) = 1 âˆ’ğ›¼
We can use either analytic expressions based on the t-distribution, or Monte
Carlo samples from the posterior predictive distribution, to make predictions
about a new sample.
Here is some code to generate the Monte Carlo samples from the tap water
example:
phi = rgamma(10000, (n-1)/2, s2*(n-1)/2)
sigma = 1/sqrt(phi)
post_mu = rnorm(10000, mean=ybar, sd=sigma/(sqrt(n)))
pred_y =
rnorm(10000,post_mu, sigma)
quantile(pred_y, c(.025, .975))

86CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
##
2.5%
97.5%
##
6.692877 104.225954
Using the Monte Carlo samples, Figure 4.7 shows the posterior distribution
based on the informative Normal-Gamma prior and the reference prior. Both
the posterior distribution for ğœ‡and the posterior predictive distribution for a
new sample are shifted to the right, and are centered at the sample mean. The
posterior for ğœ‡under the reference prior is less concentrated around its mean
than the posterior under the informative prior, which leads to an increased
posterior sample size and hence increased precision.
0.00
0.05
0.10
0
100
TTHM (ppb)
density
parameter
NG posterior mu
NG posterior predictive
ref posterior mu
ref posterior predictive
Figure 4.7: Comparison of posterior densities
The posterior probability that a new sample will exceed the legal limit of 80
ppb under the reference prior is roughly 0.15, which is more than double the
probability of 0.06 from the posterior under the informative prior.
sum(pred_y > 80)/length(pred_y)
# P(Y > 80 | data)
## [1] 0.1534
In constructing the informative prior from the reported interval, there are two
critical assumptions. First, the prior data are exchangeable with the observed

4.5. MIXTURES OF CONJUGATE PRIORS
87
data. Second, the conjugate normal gamma distribution is suitable for repre-
senting the prior information. These assumptions may or may not be verifiable,
but they should be considered carefully when using informative conjugate priors.
In the case of the tap water example, there are several concerns: One, it is
unclear that the prior data are exchangeable with the observed data. For ex-
ample, water treatment conditions may have changed. Two, the prior sample
size was not based on a real prior sample, but instead selected so that the prior
predictive intervals under the normal gamma model agreed with the prior data.
As we do not have access to the prior data, we cannot check assumptions about
normality that would help justify the prior. Other skewed distributions may be
consistent with the prior interval, but lead to different conclusions.
To recap, we have introduced a reference prior for inference for normal data
with an unknown mean and variance. Reference priors are often part of a prior
sensitivity study and are used when objectivity is of utmost importance.
If conclusions are fundamentally different with an informative prior and a ref-
erence prior, one may wish to carefully examine assumputions that led to the
informative prior.
â€¢ Is the prior information based on a prior sample that is exchangable with
the observed data?
â€¢ Is the normal-gamma assumption appropriate?
Informative priors can provide more accurate inference when data are limited,
and the transparency of explicitly laying out prior assumptions is an important
aspect of reproducible research. However, one needs to be careful that certain
prior assumptions may lead to un-intended consequences.
Next, we will investigate a prior distribution that is a mixture of conjugate pri-
ors, so the new prior distribution provides robustness to prior mis-specification
in the prior sample size.
While we will no longer have nice analytical expressions for the posterior, we can
simulate from the posterior distribution using a Monte Carlo algorithm called
Markov chain Monte Carlo (MCMC).
4.5
Mixtures of Conjugate Priors
In this section, we will describe priors that are constructed as a mixture of
conjugate priors â€“ in particular, the Cauchy distribution. As these are no longer
conjugate priors, nice analytic expressions for the posterior distribution are not
available. However, we can use a Monte Carlo algorithm called Markov chain
Monte Carlo (MCMC) for posterior inference.
In many situations, we may have reasonable prior information about the mean
ğœ‡, but we are less confident in how many observations our prior beliefs are

88CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
equivalent to. We can address this uncertainty in the prior sample size, through
an additional prior distribution on a ğ‘›0 via a hierarchical prior.
The hierarchical prior for the normal gamma distribution is written as
ğœ‡âˆ£ğœ2, ğ‘›0 âˆ¼Normal(ğ‘š0, ğœ2/ğ‘›0)
ğ‘›0 âˆ£ğœ2 âˆ¼Gamma(1/2, ğ‘Ÿ2/2)
If ğ‘Ÿ= 1, then this corresponds to a prior expected sample size of one because
the expectation of Gamma(1/2, 1/2) is one.
The marginal prior distribution from ğœ‡can be attained via integration, and we
get
ğœ‡âˆ£ğœ2 âˆ¼C(ğ‘š0, ğœ2ğ‘Ÿ2)
This is a Cauchy distribution centered at the prior mean ğ‘š0, with the scale
parameter ğœ2ğ‘Ÿ2. The probability density function (pdf) is:
ğ‘(ğœ‡âˆ£ğœ) =
1
ğœ‹ğœğ‘Ÿ(1 + (ğœ‡âˆ’ğ‘š0)2
ğœ2ğ‘Ÿ2
)
âˆ’1
The Cauchy distribution does not have a mean or standard deviation, but the
center (location) and the scale play a similar role to the mean and standard
deviation of the normal distribution. The Cauchy distribution is a special case
of a student ğ‘¡distribution with one degree of freedom.
As Figure 4.8 shows, the standard Cauchy distribution with ğ‘Ÿ= 1 and the
standard normal distribution Normal(0, 1) are centered at the same location.
But the Cauchy distribution has heavier tails â€“ more probability on extreme
values than the normal distribution with the same scale parameter ğœ. Cauchy
priors were recommended by Sir Harold Jeffreys as a default objective prior for
testing.
4.6
Markov Chain Monte Carlo (MCMC)
The Cauchy prior described in Section 4.5 is not a conjugate prior, and therefore,
the posterior distribution from (ğœ‡âˆ£ğœ2), is not a Cauchy or any well-known
distribution. Fortunately, the conditional distribution of (ğœ‡, ğœ2 âˆ£ğ‘›0, data), is
normal-gamma and easy to simulate from, as we learned in the previous sections.
The conditional distribution of (ğ‘›0 âˆ£ğœ‡, ğœ2, data) is a gamma distribution, also
easy to simulate from the given ğœ‡, ğœ2.
It turns out that if we alternate generating Monte Carlo samples from these
conditional distributions, the sequence of samples converges to samples from
the joint distribution of (ğœ‡, ğœ2, ğ‘›0), as the number of simulated values increases.

4.6. MARKOV CHAIN MONTE CARLO (MCMC)
89
0.0
0.1
0.2
0.3
0.4
âˆ’5.0
âˆ’2.5
0.0
2.5
5.0
y
density
distribution
Cauchy
Normal
Figure 4.8: Cauchy distribution

90CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
The Monte Carlo algorithm we have just described is a special case of Markov
chain Monte Carlo (MCMC), known as the Gibbs sampler.
Letâ€™s look at the pseudo code for the algorithm.
# initialize MCMC
sigma2[1] = 1; n_0[1]=1; mu[1]=m_0
#draw from full conditional distributions
for (i in 2:S) {
mu[i]
= p_mu(sigma2[i-1], n_0[i-1],
m_0, r, data)
sigma2[i] = p_sigma2(mu[i], n_0[i-1],
m_0, r, data)
n_0[i]
= p_n_0(mu[i], sigma2[i],
m_0, r, data)
}
We start with the initial values of each of the parameters for ğ‘–= 1. In theory,
these can be completely arbitrary, as long as they are allowed values for the
parameters.
For each iteration ğ‘–, the algorithm will cycle through generating each parameter,
given the current value of the other parameters. The functions p_mu, p_sigma2,
and p_n_0 return a simulated value from the respective distribution conditional
on the inputs.
Whenever we update a parameter, we use the new value in the subsequent
steps as the ğ‘›draws for ğœ, ğ‘›0. We will repeat this until we reach iteration ğ‘†,
leading to a dependent sequence of s draws from the joint posterior distribution.
Incorporating the tap water example in Section 4.1, we will use MCMC to
generate samples under the Cauchy prior. We set 35 as the location parameter
and ğ‘Ÿ= 1. To complete our prior specification, we use the Jeffreyâ€™s reference
prior on ğœ2. This combination is referred to as the Jeffreyâ€™s Zellner-Siow Cauchy
prior or â€œJZSâ€ in the BayesFactor branch of the R statsr package.
bayes_inference(y=tthm, data=tapwater, statistic="mean",
mu_0 = 35, rscale=1, prior="JZS",
type="ci", method="sim")
## Single numerical variable
## n = 28, y-bar = 55.5239, s = 23.254
## (Assuming Zellner-Siow Cauchy prior:
mu | sigma^2 ~ C(35, 1*sigma)
## (Assuming improper Jeffreys prior: p(sigma^2) = 1/sigma^2
##
## Posterior Summaries
##
2.5%
25%
50%
75%
97.5%
## mu
45.5713714 51.820910 54.87345 57.87171 64.20477
## sigma 18.4996738 21.810376 23.84572 26.30359 32.11330
## n_0
0.2512834
2.512059
6.13636 12.66747 36.37425
##

4.6. MARKOV CHAIN MONTE CARLO (MCMC)
91
## 95% CI for mu: (45.5714, 64.2048)
0.000
0.025
0.050
0.075
40
50
60
70
mu
Density
Using the bayes_inference function from the statsr package, we can obtain
summary statistics and a plot from the MCMC output â€“ not only ğœ‡, but also
inference about ğœ2 and the prior sample size.
The posterior mean under the JZS model is much closer to the sample mean
than what the normal gamma prior used previously.
Under the informative
normal gamma prior, the sample made a 55.5, about eight standard deviations
above the mean â€“ a surprising value under the normal prior. Under the Cauchy
prior, the informative prior location has much less influence.
This is the robustness property of the Cauchy prior, leading the posterior
to put more weight on the sample mean than the prior mean, especially when
the prior location is not close to the sample mean. We can see that the central
50% interval for ğ‘›0 is well below the value 25 used in the normal prior, which
placed almost equal weight on the prior in sample mean.
Using the MCMC draws of ğœ‡, ğœ, we can obtain Monte Carlo samples from the
predictive distribution of ğ‘¦, by plugging ğœ‡and ğœinto the corresponding func-
tions. Figure 4.9 compares the posterior densities estimated from the simulative
values of ğœ‡and the predicted draws of TTHM under the Jeffrey Zellner-Siow
prior, and the informative normal prior from ğœ‡with ğ‘›0 = 25 and the reference
prior on ğœ2.

92CHAPTER 4. INFERENCE AND DECISION-MAKING WITH MULTIPLE PARAMETERS
0.00
0.03
0.06
0.09
0
100
TTHM (ppb)
density
parameter
JZS posterior mu
JZS posterior predictive
NG posterior mu
NG posterior predictive
Figure 4.9: Comparison of posterior densities
To recap, we have shown how to create more flexible prior distributions, such
as the Cauchy distribution using mixtures of conjugate priors. As the posterior
distributions are not available in closed form, we demonstrated how MCMC
can be used for inference using the hierarchical prior distribution.
Starting
in the late 1980â€™s, MCMC algorithms have led to an exponential rise in the
use of Bayes in methods, because complex models built through hierarchical
distributions suddenly were tractable. The Cauchy prior is well-known for being
robust prior mis-specifications. For example, having a prior mean that is far
from the observed mean. This provides an alternative to the reference prior as
a default or objective distribution that is proper.
In the next sections, we will return to Bayes factors and hypothesis testing
where the Cauchy prior plays an important role.

Chapter 5
Hypothesis Testing with
Normal Populations
In Section 3.5, we described how the Bayes factors can be used for hypothesis
testing.
Now we will use the Bayes factors to compare normal means, i.e.,
test whether the mean of a population is zero or compare the means of two
groups of normally-distributed populations. We divide this mission into three
cases: known variance for a single population, unknown variance for a single
population using paired data, and unknown variance using two independent
groups.
Also note that some of the examples in this section use an updated version of
the bayes_inference function. If your local output is different from what is
seen in this chapter, or the provided code fails to run for you please make sure
that you have the most recent version of the package.
5.1
Bayes Factors for Testing a Normal Mean:
variance known
Now we show how to obtain Bayes factors for testing hypothesis about a normal
mean, where the variance is known. To start, letâ€™s consider a random sample
of observations from a normal population with mean ğœ‡and pre-specified variance
ğœ2. We consider testing whether the population mean ğœ‡is equal to ğ‘š0 or not.
Therefore, we can formulate the data and hypotheses as below:
Data
ğ‘Œ1, â‹¯, ğ‘Œğ‘›
iid
âˆ¼Normal(ğœ‡, ğœ2)
Hypotheses
93

94CHAPTER 5. HYPOTHESIS TESTING WITH NORMAL POPULATIONS
â€¢ ğ»1 âˆ¶ğœ‡= ğ‘š0
â€¢ ğ»2 âˆ¶ğœ‡â‰ ğ‘š0
Priors
We also need to specify priors for ğœ‡under both hypotheses.
Under ğ»1, we
assume that ğœ‡is exactly ğ‘š0, so this occurs with probability 1 under ğ»1. Now
under ğ»2, ğœ‡is unspecified, so we describe our prior uncertainty with the con-
jugate normal distribution centered at ğ‘š0 and with a variance ğœ2/n0. This is
centered at the hypothesized value ğ‘š0, and it seems that the mean is equally
likely to be larger or smaller than ğ‘š0, so a dividing factor ğ‘›0 is given to the
variance. The hyper parameter ğ‘›0 controls the precision of the prior as before.
In mathematical terms, the priors are:
â€¢ ğ»1 âˆ¶ğœ‡= ğ‘š0 with probability 1
â€¢ ğ»2 âˆ¶ğœ‡âˆ¼Normal(ğ‘š0, ğœ2/n0)
Bayes Factor
Now the Bayes factor for comparing ğ»1 to ğ»2 is the ratio of the distribution,
the data under the assumption that ğœ‡= ğ‘š0 to the distribution of the data
under ğ»2.
BF[ğ»1 âˆ¶ğ»2] =
ğ‘(data âˆ£ğœ‡= ğ‘š0, ğœ2)
âˆ«ğ‘(data âˆ£ğœ‡, ğœ2)ğ‘(ğœ‡âˆ£ğ‘š0, n0, ğœ2) ğ‘‘ğœ‡
BF[ğ»1 âˆ¶ğ»2] = (ğ‘›+ n0
n0
)
1/2
exp {âˆ’1
2
ğ‘›
ğ‘›+ n0
ğ‘2}
ğ‘= ( Ì„ğ‘Œâˆ’ğ‘š0)
ğœ/âˆšğ‘›
The term in the denominator requires integration to account for the uncertainty
in ğœ‡under ğ»2. And it can be shown that the Bayes factor is a function of the
observed sampled size, the prior sample size ğ‘›0 and a ğ‘score.
Letâ€™s explore how the hyperparameters in ğ‘›0 influences the Bayes factor in
Equation (5.1). For illustration we will use the sample size of 100. Recall that
for estimation, we interpreted ğ‘›0 as a prior sample size and considered the
limiting case where ğ‘›0 goes to zero as a non-informative or reference prior.
BF[ğ»1 âˆ¶ğ»2] = (ğ‘›+ n0
n0
)
1/2
exp {âˆ’1
2
ğ‘›
ğ‘›+ n0
ğ‘2}
(5.1)
Figure 5.1 shows the Bayes factor for comparing ğ»1 to ğ»2 on the y-axis as ğ‘›0
changes on the x-axis. The different lines correspond to different values of the
ğ‘score or how many standard errors
Ì„ğ‘¦is from the hypothesized mean. As
expected, larger values of the ğ‘score favor ğ»2.

5.1. BAYES FACTORS FOR TESTING A NORMAL MEAN: VARIANCE KNOWN95
1eâˆ’02
1eâˆ’01
1e+00
1e+01
1e+02
0.00
0.25
0.50
0.75
1.00
n0
BF[H1:H2]
Z
1.65
1.96
2.81
3.62
Figure 5.1: Vague prior for mu: n=100

96CHAPTER 5. HYPOTHESIS TESTING WITH NORMAL POPULATIONS
But as ğ‘›0 becomes smaller and approaches 0, the first term in the Bayes factor
goes to infinity, while the exponential term involving the data goes to a constant
and is ignored. In the limit as ğ‘›0 â†’0 under this noninformative prior, the Bayes
factor paradoxically ends up favoring ğ»1 regardless of the value of
Ì„ğ‘¦.
The takeaway from this is that we cannot use improper priors with ğ‘›0 = 0, if
we are going to test our hypothesis that ğœ‡= ğ‘›0. Similarly, vague priors that use
a small value of ğ‘›0 are not recommended due to the sensitivity of the results to
the choice of an arbitrarily small value of ğ‘›0.
This problem arises with vague priors â€“ the Bayes factor favors the null model
ğ»1 even when the data are far away from the value under the null â€“ are known
as the Bartlettâ€™s paradox or the Jeffreyâ€™s-Lindleys paradox.
Now, one way to understand the effect of prior is through the standard effect
size
ğ›¿= ğœ‡âˆ’ğ‘š0
ğœ
.
The prior of the standard effect size is
ğ›¿âˆ£ğ»2 âˆ¼Normal(0, 1
n0
)
This allows us to think about a standardized effect independent of the units of
the problem. One default choice is using the unit information prior, where the
prior sample size ğ‘›0 is 1, leading to a standard normal for the standardized effect
size. This is depicted with the blue normal density in Figure 5.2. This suggested
that we expect that the mean will be within Â±1.96 standard deviations of the
hypothesized mean with probability 0.95. (Note that we can say this only
under a Bayesian setting.)
In many fields we expect that the effect will be small relative to ğœ. If we do not
expect to see large effects, then we may want to use a more informative prior
on the effect size as the density in orange with ğ‘›0 = 4. So they expected the
mean to be within Â±1/âˆšğ‘›0 or five standard deviations of the prior mean.
Example 5.1. To illustrate, we give an example from parapsychological re-
search. The case involved the test of the subjectâ€™s claim to affect a series of
randomly generated 0â€™s and 1â€™s by means of extra sensory perception (ESP).
The random sequence of 0â€™s and 1â€™s are generated by a machine with probabil-
ity of generating 1 being 0.5. The subject claims that his ESP would make the
sample mean differ significantly from 0.5.
Therefore, we are testing ğ»1 âˆ¶ğœ‡= 0.5 versus ğ»2 âˆ¶ğœ‡â‰ 0.5. Letâ€™s use a prior
that suggests we do not expect a large effect which leads the following solution

5.1. BAYES FACTORS FOR TESTING A NORMAL MEAN: VARIANCE KNOWN97
âˆ’6
âˆ’4
âˆ’2
0
2
4
0.0
0.2
0.4
0.6
0.8
Î´
density
N(0,1)
N(0, .25)
Figure 5.2: Prior on standard effect size
for ğ‘›0. Assume we want a standard effect of 0.03, there is a 95% chance that it
is between (âˆ’0.03/ğœ, 0.03/ğœ), with ğ‘›0 = (1.96ğœ/0.03)2 = 32.72.
Figure 5.3 shows our informative prior in blue, while the unit information prior
is in orange. On this scale, the unit information prior needs to be almost uniform
for the range that we are interested.
A very large data set with over 104 million trials was collected to test this
hypothesis, so we use a normal distribution to approximate the distribution the
sample mean.
â€¢ Sample size: ğ‘›= 1.0449 Ã— 108
â€¢ Sample mean:
Ì„ğ‘¦= 0.500177, standard deviation ğœ= 0.5
â€¢ ğ‘-score: 3.61
Now using our prior in the data, the Bayes factor for ğ»1 to ğ»2 was 0.46, implying
evidence against the hypothesis ğ»1 that ğœ‡= 0.5.
â€¢ Informative BF[ğ»1 âˆ¶ğ»2] = 0.46
â€¢ BF[ğ»2 âˆ¶ğ»1] = 1/BF[ğ»1 âˆ¶ğ»2] = 2.19
Now, this can be inverted to provide the evidence in favor of ğ»2. The evidence
suggests that the hypothesis that the machine operates with a probability that
is not 0.5, is 2.19 times more likely than the hypothesis the probability is 0.5.
Based on the interpretation of Bayes factors from Table 3.5, this is in the range

98CHAPTER 5. HYPOTHESIS TESTING WITH NORMAL POPULATIONS
0.40
0.45
0.50
0.55
0.60
0
10
20
30
40
50
60
Âµ
density
N(.5,.5/32.7^2)
N(.5, .5)
Figure 5.3: Prior effect in the extra sensory perception test
of â€œnot worth the bare mentionâ€.
To recap, we present expressions for calculating Bayes factors for a normal
model with a specified variance. We show that the improper reference priors
for ğœ‡when ğ‘›0 = 0, or vague priors where ğ‘›0 is arbitrarily small, lead to Bayes
factors that favor the null hypothesis regardless of the data, and thus should
not be used for hypothesis testing.
Bayes factors with normal priors can be sensitive to the choice of the ğ‘›0. While
the default value of ğ‘›0 = 1 is reasonable in many cases, this may be too non-
informative if one expects more effects. Wherever possible, think about how
large an effect you expect and use that information to help select the ğ‘›0.
All the ESP examples suggest weak evidence and favored the machine generating
random 0â€™s and 1â€™s with a probability that is different from 0.5. Note that ESP
is not the only explanation â€“ a deviation from 0.5 can also occur if the random
number generator is biased.
Bias in the stream of random numbers in our
pseudorandom numbers has huge implications for numerous fields that depend
on simulation. If the context had been about detecting a small bias in random
numbers what prior would you use and how would it change the outcome?
You can experiment it in R or other software packages that generate random
Bernoulli trials.
Next, we will look at Bayes factors in normal models with unknown variances

5.2. COMPARING TWO PAIRED MEANS USING BAYES FACTORS
99
Table 5.1: Zinc in drinking water
location
bottom
surface
difference
1
0.430
0.415
0.015
2
0.266
0.238
0.028
3
0.567
0.390
0.177
4
0.531
0.410
0.121
5
0.707
0.605
0.102
6
0.716
0.609
0.107
7
0.651
0.632
0.019
8
0.589
0.523
0.066
9
0.469
0.411
0.058
10
0.723
0.612
0.111
using the Cauchy prior so that results are less sensitive to the choice of ğ‘›0.
5.2
Comparing Two Paired Means using Bayes
Factors
We previously learned that we can use a paired t-test to compare means from
two paired samples. In this section, we will show how Bayes factors can be
expressed as a function of the t-statistic for comparing the means and provide
posterior probabilities of the hypothesis that whether the means are equal or
different.
Example 5.2. Trace metals in drinking water affect the flavor, and unusually
high concentrations can pose a health hazard. Ten pairs of data were taken
measuring the zinc concentration in bottom and surface water at ten randomly
sampled locations, as listed in Table 5.1.
Water samples collected at the the same location, on the surface and the bot-
tom, cannot be assumed to be independent of each other. However, it may be
reasonable to assume that the differences in the concentration at the bottom
and the surface in randomly sampled locations are independent of each other.
To start modeling, we will treat the ten differences as a random sample from a
normal population where the parameter of interest is the difference between the
average zinc concentration at the bottom and the average zinc concentration at
the surface, or the main difference, ğœ‡.
In mathematical terms, we have
â€¢ Random sample of ğ‘›= 10 differences ğ‘Œ1, â€¦ , ğ‘Œğ‘›

100CHAPTER 5. HYPOTHESIS TESTING WITH NORMAL POPULATIONS
â€¢ Normal population with mean ğœ‡â‰¡ğœ‡ğµâˆ’ğœ‡ğ‘†
In this case, we have no information about the variability in the data, and we
will treat the variance, ğœ2, as unknown.
The hypothesis of the main concentration at the surface and bottom are the
same is equivalent to saying ğœ‡= 0. The second hypothesis is that the difference
between the mean bottom and surface concentrations, or equivalently that the
mean difference ğœ‡â‰ 0.
In other words, we are going to compare the following hypotheses:
â€¢ ğ»1 âˆ¶ğœ‡ğµ= ğœ‡ğ‘†â‡”ğœ‡= 0
â€¢ ğ»2 âˆ¶ğœ‡ğµâ‰ ğœ‡ğ‘†â‡”ğœ‡â‰ 0
The Bayes factor is the ratio between the distributions of the data under each
hypothesis, which does not depend on any unknown parameters.
BF[ğ»1 âˆ¶ğ»2] = ğ‘(data âˆ£ğ»1)
ğ‘(data âˆ£ğ»2)
To obtain the Bayes factor, we need to use integration over the prior distribu-
tions under each hypothesis to obtain those distributions of the data.
BF[ğ»1 âˆ¶ğ»2] = âˆ¬ğ‘(data âˆ£ğœ‡, ğœ2)ğ‘(ğœ‡âˆ£ğœ2)ğ‘(ğœ2 âˆ£ğ»2) ğ‘‘ğœ‡ğ‘‘ğœ2
This requires specifying the following priors:
â€¢ ğœ‡âˆ£ğœ2, ğ»2 âˆ¼Normal(0, ğœ2/ğ‘›0)
â€¢ ğ‘(ğœ2) âˆ1/ğœ2 for both ğ»1 and ğ»2
ğœ‡is exactly zero under the hypothesis ğ»1. For ğœ‡in ğ»2, we start with the same
conjugate normal prior as we used in Section 5.1 â€“ testing the normal mean with
known variance. Since we assume that ğœ2 is known, we model ğœ‡âˆ£ğœ2 instead of
ğœ‡itself.
The ğœ2 appears in both the numerator and denominator of the Bayes factor.
For default or reference case, we use the Jeffreys prior (a.k.a. reference prior)
on ğœ2. As long as we have more than two observations, this (improper) prior
will lead to a proper posterior.
After integration and rearranging, one can derive a simple expression for the
Bayes factor:
BF[ğ»1 âˆ¶ğ»2] = (ğ‘›+ ğ‘›0
ğ‘›0
)
1/2
(
ğ‘¡2
ğ‘›0
ğ‘›+ğ‘›0 + ğœˆ
ğ‘¡2 + ğœˆ
)
ğœˆ+1
2
This is a function of the t-statistic

5.2. COMPARING TWO PAIRED MEANS USING BAYES FACTORS
101
ğ‘¡=
| Ì„ğ‘Œ|
ğ‘ /âˆšğ‘›,
where ğ‘ is the sample standard deviation and the degrees of freedom ğœˆ= ğ‘›âˆ’1
(sample size minus one).
As we saw in the case of Bayes factors with known variance, we cannot use the
improper prior on ğœ‡because when ğ‘›0 â†’0, then BF[ğ»1 âˆ¶ğ»2] â†’âˆfavoring ğ»1
regardless of the magnitude of the t-statistic. Arbitrary, vague small choices for
ğ‘›0 also lead to arbitrary large Bayes factors in favor of ğ»1. Another example
of the Barlettâ€™s or Jeffreys-Lindley paradox.
Sir Herald Jeffrey discovered another paradox testing using the conjugant nor-
mal prior, known as the information paradox. His thought experiment as-
sumed that our sample size ğ‘›and the prior sample size ğ‘›0. He then considered
what would happen to the Bayes factor as the sample mean moved further and
further away from the hypothesized mean, measured in terms standard errors
with the t-statistic, i.e., |ğ‘¡| â†’âˆ. As the t-statistic or information about the
mean moved further and further from zero, the Bayes factor goes to a constant
depending on ğ‘›, ğ‘›0 rather than providing overwhelming support for ğ»2.
The bounded Bayes factor is
BF[ğ»1 âˆ¶ğ»2] â†’(
ğ‘›0
ğ‘›0 + ğ‘›)
ğ‘›âˆ’1
2
Jeffrey wanted a prior with BF[ğ»1 âˆ¶ğ»2] â†’0 (or equivalently, BF[ğ»2 âˆ¶ğ»1] â†’
âˆ), as the information from the t-statistic grows, indicating the sample mean
is as far as from the hypothesized mean and should favor ğ»2.
To resolve the paradox when the information the t-statistic favors ğ»2 but the
Bayes factor does not, Jeffreys showed that no normal prior could resolve
the paradox.
But a Cauchy prior on ğœ‡, would resolve it. In this way, BF[ğ»2 âˆ¶ğ»1] goes to
infinity as the sample mean becomes further away from the hypothesized mean.
Recall that the Cauchy prior is written as C(0, ğ‘Ÿ2ğœ2). While Jeffreys used a
default of ğ‘Ÿ= 1, smaller values of ğ‘Ÿcan be used if smaller effects are expected.
The combination of the Jeffreyâ€™s prior on ğœ2 and this Cauchy prior on ğœ‡under
ğ»2 is sometimes referred to as the Jeffrey-Zellener-Siow prior.
However, there is no closed form expressions for the Bayes factor under the
Cauchy distribution. To obtain the Bayes factor, we must use the numerical
integration or simulation methods.
We will use the bayes_inference function from the statsr package to test
whether the mean difference is zero in Example 5.2 (zinc), using the JZS
(Jeffreys-Zellener-Siow) prior.

102CHAPTER 5. HYPOTHESIS TESTING WITH NORMAL POPULATIONS
library(statsr)
bayes_inference(difference, data=zinc, statistic="mean", type="ht",
prior="JZS", mu_0=0, method="theo", alt="twosided")
## Single numerical variable
## n = 10, y-bar = 0.0804, s = 0.0523
## (Using Zellner-Siow Cauchy prior:
mu ~ C(0, 1*sigma)
## (Using Jeffreys prior: p(sigma^2) = 1/sigma^2
##
## Hypotheses:
## H1: mu = 0 versus H2: mu != 0
## Priors:
## P(H1) = 0.5 , P(H2) = 0.5
## Results:
## BF[H2:H1] = 50.7757
## P(H1|data) = 0.0193
P(H2|data) = 0.9807
##
## Posterior summaries for mu under H2:
## Single numerical variable
## n = 10, y-bar = 0.0804, s = 0.0523
## (Assuming Zellner-Siow Cauchy prior:
mu | sigma^2 ~ C(0, 1*sigma)
## (Assuming improper Jeffreys prior: p(sigma^2) = 1/sigma^2
##
## Posterior Summaries
##
2.5%
25%
50%
75%
97.5%
## mu
0.03644165 0.06330793 0.07537836
0.08717155
0.11219575
## sigma 0.03667137 0.04739824 0.05528498
0.06554032
0.09551376
## n_0
0.16110471 1.88398728 4.72113317 10.06572004 32.38149654
##
## 95% CI for mu: (0.0364, 0.1122)

5.3. COMPARING INDEPENDENT MEANS: HYPOTHESIS TESTING 103
0
5
10
15
20
âˆ’0.1
0.0
0.1
0.2
mu
Density
With equal prior probabilities on the two hypothesis, the Bayes factor is the
posterior odds. From the output, we see this indicates that the hypothesis ğ»2,
the mean difference is different from 0, is almost 51 times more likely than the
hypothesis ğ»1 that the average concentration is the same at the surface and the
bottom.
To sum up, we have used the Cauchy prior as a default prior testing hypothesis
about a normal mean when variances are unknown. This does require numerical
integration, but it is available in the bayes_inference function from the statsr
package. If you expect that the effect sizes will be small, smaller values of ğ‘Ÿare
recommended.
It is often important to quantify the magnitude of the difference in addition to
testing. The Cauchy Prior provides a default prior for both testing and inference;
it avoids problems that arise with choosing a value of ğ‘›0 (prior sample size) in
both cases. In the next section, we will illustrate using the Cauchy prior for
comparing two means from independent normal samples.
5.3
Comparing Independent Means: Hypothesis
Testing
In the previous section, we described Bayes factors for testing whether the mean
difference of paired samples was zero. In this section, we will consider a slightly
different problem â€“ we have two independent samples, and we would like to
test the hypothesis that the means are different or equal.
Example 5.3. We illustrate the testing of independent groups with data from
a 2004 survey of birth records from North Carolina, which are available in the

104CHAPTER 5. HYPOTHESIS TESTING WITH NORMAL POPULATIONS
statsr package.
The variable of interest is gained â€“ the weight gain of mothers during pregnancy.
We have two groups defined by the categorical variable, mature, with levels,
younger mom and older mom.
Question of interest: Do the data provide convincing evidence of a difference
between the average weight gain of older moms and the average weight gain of
younger moms?
We will view the data as a random sample from two populations, older and
younger moms. The two groups are modeled as:
ğ‘Œğ‘‚,ğ‘–
iid
âˆ¼N(ğœ‡+ ğ›¼/2, ğœ2)
ğ‘Œğ‘Œ,ğ‘–
iid
âˆ¼N(ğœ‡âˆ’ğ›¼/2, ğœ2)
(5.2)
The model for weight gain for older moms using the subscript ğ‘‚, and it assumes
that the observations are independent and identically distributed, with a mean
ğœ‡+ ğ›¼/2 and variance ğœ2.
For the younger women, the observations with the subscript ğ‘Œare independent
and identically distributed with a mean ğœ‡âˆ’ğ›¼/2 and variance ğœ2.
Using this representation of the means in the two groups, the difference in means
simplifies to ğ›¼â€“ the parameter of interest.
(ğœ‡+ ğ›¼/2) âˆ’(ğœ‡âˆ’ğ›¼/2) = ğ›¼
You may ask, â€œWhy donâ€™t we set the average weight gain of older women to
ğœ‡+ ğ›¼, and the average weight gain of younger women to ğœ‡?â€ We need the
parameter ğ›¼to be present in both ğ‘Œğ‘‚,ğ‘–(the group of older women) and ğ‘Œğ‘Œ,ğ‘–
(the group of younger women).
We have the following competing hypotheses:
â€¢ ğ»1 âˆ¶ğ›¼= 0 â‡”The means are not different.
â€¢ ğ»2 âˆ¶ğ›¼â‰ 0 â‡”The means are different.
In this representation, ğœ‡represents the overall weight gain for all women. (Does
the model in Equation (5.2) make more sense now?) To test the hypothesis, we
need to specify prior distributions for ğ›¼under ğ»2 (c.f. ğ›¼= 0 under ğ»1) and
priors for ğœ‡, ğœ2 under both hypotheses.
Recall that the Bayes factor is the ratio of the distribution of the data under
the two hypotheses.

5.3. COMPARING INDEPENDENT MEANS: HYPOTHESIS TESTING 105
BF[ğ»1 âˆ¶ğ»2] = ğ‘(data âˆ£ğ»1)
ğ‘(data âˆ£ğ»2)
=
âˆ¬ğ‘(data âˆ£ğ›¼= 0, ğœ‡, ğœ2)ğ‘(ğœ‡, ğœ2 âˆ£ğ»1) ğ‘‘ğœ‡ğ‘‘ğœ2
âˆ«âˆ¬ğ‘(data âˆ£ğ›¼, ğœ‡, ğœ2)ğ‘(ğ›¼âˆ£ğœ2)ğ‘(ğœ‡, ğœ2 âˆ£ğ»2) ğ‘‘ğœ‡ğ‘‘ğœ2 ğ‘‘ğ›¼
As before, we need to average over uncertainty and the parameters to obtain
the unconditional distribution of the data. Now, as in the test about a single
mean, we cannot use improper or non-informative priors for ğ›¼for testing.
Under ğ»2, we use the Cauchy prior for ğ›¼, or equivalently, the Cauchy prior on
the standardized effect ğ›¿with the scale of ğ‘Ÿ:
ğ›¿= ğ›¼/ğœ2 âˆ¼C(0, ğ‘Ÿ2)
Now, under both ğ»1 and ğ»2, we use the Jeffreyâ€™s reference prior on ğœ‡and ğœ2:
ğ‘(ğœ‡, ğœ2) âˆ1/ğœ2
While this is an improper prior on ğœ‡, this does not suffer from the Bartlettâ€™s-
Lindleyâ€™s-Jeffreysâ€™ paradox as ğœ‡is a common parameter in the model in ğ»1 and
ğ»2. This is another example of the Jeffreys-Zellner-Siow prior.
As in the single mean case, we will need numerical algorithms to obtain the
Bayes factor.
Now the following output illustrates testing of Bayes factors,
using the Bayes inference function from the statsr package.
library(statsr)
data(nc)
bayes_inference(y=gained, x=mature, data=nc,type='ht',
statistic='mean',
alternative='twosided', null=0,
prior='JZS', r=1, method='theo', show_summ=FALSE)
## Hypotheses:
## H1: mu_mature mom
= mu_younger mom
## H2: mu_mature mom != mu_younger mom
##
## Priors: P(H1) = 0.5
P(H2) = 0.5
##
## Results:
## BF[H1:H2] = 5.7162
## P(H1|data) = 0.8511
## P(H2|data) = 0.1489
##
## Posterior summaries for under H2:
## 95% Cred. Int.: (-4.3778 , 0.8789)

106CHAPTER 5. HYPOTHESIS TESTING WITH NORMAL POPULATIONS
0.0
0.1
0.2
0.3
âˆ’7.5
âˆ’5.0
âˆ’2.5
0.0
2.5
mu_mature mom âˆ’ mu_younger mom
Density
We see that the Bayes factor for ğ»1 to ğ»2 is about 5.7, with positive support
for ğ»1 that there is no difference in average weight gain between younger and
older women. Using equal prior probabilities, the probability that there is a
difference in average weight gain between the two groups is about 0.15 given
the data. Based on the interpretation of Bayes factors from Table 3.5, this is in
the range of â€œpositiveâ€ (between 3 and 20).
To recap, we have illustrated testing hypotheses about population means with
two independent samples, using a Cauchy prior on the difference in the means.
One assumption that we have made is that the variances are equal in both
groups. The case where the variances are unequal is referred to as the Behren-
Fisher problem, and this is beyond the scope for this course. In the next section,
we will look at another example to put everything together with testing and
discuss summarizing results.
5.4
Inference after Testing
In this section, we will work through another example for comparing two means
using both hypothesis tests and interval estimates, with an informative prior.
We will also illustrate how to adjust the credible interval after testing.

5.4. INFERENCE AFTER TESTING
107
Example 5.4. We will use the North Carolina survey data to examine the
relationship between infant birth weight and whether the mother smoked during
pregnancy. The response variable, weight, is the birth weight of the baby in
pounds. The categorical variable habit provides the status of the mother as a
smoker or non-smoker.
We would like to answer two questions:
1. Is there a difference in average birth weight between the two groups?
2. If there is a difference, how large is the effect?
As before, we need to specify models for the data and priors. We treat the data
as a random sample for the two populations, smokers and non-smokers.
The birth weights of babies born to non-smokers, designated by a subgroup
ğ‘, are assumed to be independent and identically distributed from a normal
distribution with mean ğœ‡+ ğ›¼/2, as in Section 5.3.
ğ‘Œğ‘,ğ‘–
iid
âˆ¼Normal(ğœ‡+ ğ›¼/2, ğœ2)
While the birth weights of the babies born to smokers, designated by the sub-
group ğ‘†, are also assumed to have a normal distribution, but with mean ğœ‡âˆ’ğ›¼/2.
ğ‘Œğ‘†,ğ‘–
iid
âˆ¼Normal(ğœ‡âˆ’ğ›¼/2, ğœ2)
The difference in the average birth weights is the parameter ğ›¼, because
(ğœ‡+ ğ›¼/2) âˆ’(ğœ‡âˆ’ğ›¼/2) = ğ›¼
.
The hypotheses that we will test are ğ»1 âˆ¶ğ›¼= 0 versus ğ»2 âˆ¶ğ›¼â‰ 0.
We will still use the Jeffreys-Zellner-Siow Cauchy prior. However, since we may
expect the standardized effect size to not be as strong, we will use a scale of
ğ‘Ÿ= 0.5 rather than 1.
Therefore, under ğ»2, we have
ğ›¿= ğ›¼/ğœâˆ¼C(0, ğ‘Ÿ2), with ğ‘Ÿ= 0.5.
Under both ğ»1 and ğ»2, we will use the reference priors on ğœ‡and ğœ2:
ğ‘(ğœ‡) âˆ1
ğ‘(ğœ2) âˆ1/ğœ2

108CHAPTER 5. HYPOTHESIS TESTING WITH NORMAL POPULATIONS
The input to the base inference function is similar, but now we will specify that
ğ‘Ÿ= 0.5.
library(statsr)
data(nc)
out =bayes_inference(y=weight, x=habit, data=nc,type='ht', null=0,
statistic='mean',
alternative='twosided',
prior='JZS', r=.5, method='sim', show_summ=FALSE)
## Hypotheses:
## H1: mu_nonsmoker
= mu_smoker
## H2: mu_nonsmoker != mu_smoker
##
## Priors: P(H1) = 0.5
P(H2) = 0.5
##
## Results:
## BF[H2:H1] = 1.4402
## P(H1|data) = 0.4098
## P(H2|data) = 0.5902
##
## Posterior summaries for under H2:
## 95% Cred. Int.: (0.0251 , 0.5645)
0
1
2
3
0.0
0.5
1.0
mu_nonsmoker âˆ’ mu_smoker
Density

5.4. INFERENCE AFTER TESTING
109
We see that the Bayes factor is 1.44, which weakly favors there being a difference
in average birth weights for babies whose mothers are smokers versus mothers
who did not smoke. Converting this to a probability, we find that there is about
a 60% chance of the average birth weights are different.
While looking at evidence of there being a difference is useful, Bayes factors and
posterior probabilities do not convey any information about the magnitude of
the effect. Reporting a credible interval or the complete posterior distribution
is more relevant for quantifying the magnitude of the effect.
Using the bayes_inference function, we can generate samples from the poste-
rior distribution under ğ»2 using the type='ci' option.
out.ci = bayes_inference(y=weight, x=habit, data=nc, type='ci',
statistic='mean', prior='JZS', mu_0=0,
r=.5, method='sim', verbose=FALSE)
print(out.ci$summary, digits=2)
##
2.5%
25%
50%
75%
97.5%
## overall mean
6.85
6.95
7.0
7.04 7.1e+00
## mu_nonsmoker - mu_smoker
0.03
0.21
0.3
0.39 5.7e-01
## sigma^2
2.07
2.19
2.3
2.33 2.5e+00
## effect size
0.02
0.14
0.2
0.26 3.8e-01
## n_0
163.84 1928.53 4639.0 9321.88 2.6e+04
The 2.5 and 97.5 percentiles for the difference in the means provide a 95%
credible interval of 0.023 to 0.57 pounds for the difference in average birth
weight. The MCMC output shows not only summaries about the difference in
the mean ğ›¼, but the other parameters in the model.
In particular, the Cauchy prior arises by placing a gamma prior on ğ‘›0 and the
conjugate normal prior. This provides quantiles about ğ‘›0 after updating with
the current data.
The row labeled effect size is the standardized effect size ğ›¿, indicating that the
effects are indeed small relative to the noise in the data.
library(ggplot2)
out = bayes_inference(y=weight, x=habit, data=nc,type='ht',
statistic='mean',
alternative='twosided',
prior='JZS', null=0, r=.5, method='theo',
show_summ=FALSE, show_res=FALSE, show_plot=TRUE)
Figure 5.4 shows the posterior density for the difference in means, with the 95%
credible interval indicated by the shaded area. Under ğ»2, there is a 95% chance
that the average birth weight of babies born to non-smokers is 0.023 to 0.57
pounds higher than that of babies born to smokers.
The previous statement assumes that ğ»2 is true and is a conditional probability
statement. In mathematical terms, the statement is equivalent to

110CHAPTER 5. HYPOTHESIS TESTING WITH NORMAL POPULATIONS
0
1
2
âˆ’0.3
0.0
0.3
0.6
0.9
mu_nonsmoker âˆ’ mu_smoker
Density
Figure 5.4: Estimates of effect under H2

5.4. INFERENCE AFTER TESTING
111
ğ‘ƒ(0.023 < ğ›¼< 0.57 âˆ£data, ğ»2) = 0.95
However, we still have quite a bit of uncertainty based on the current data,
because given the data, the probability of ğ»2 being true is 0.59.
ğ‘ƒ(ğ»2 âˆ£data) = 0.59
Using the law of total probability, we can compute the probability that ğœ‡is
between 0.023 and 0.57 as below:
ğ‘ƒ(0.023 < ğ›¼< 0.57 âˆ£data)
=ğ‘ƒ(0.023 < ğ›¼< 0.57 âˆ£data, ğ»1)ğ‘ƒ(ğ»1 âˆ£data) + ğ‘ƒ(0.023 < ğ›¼< 0.57 âˆ£data, ğ»2)ğ‘ƒ(ğ»2 âˆ£data)
=ğ¼(0 in CI )ğ‘ƒ(ğ»1 âˆ£data) + 0.95 Ã— ğ‘ƒ(ğ»2 âˆ£data)
=0 Ã— 0.41 + 0.95 Ã— 0.59 = 0.5605
Finally, we get that the probability that ğ›¼is in the interval, given the data,
averaging over both hypotheses, is roughly 0.56. The unconditional statement
is the average birth weight of babies born to nonsmokers is 0.023 to 0.57 pounds
higher than that of babies born to smokers with probability 0.56. This adjust-
ment addresses the posterior uncertainty and how likely ğ»2 is.
To recap, we have illustrated testing, followed by reporting credible intervals,
and using a Cauchy prior distribution that assumed smaller standardized effects.
After testing, it is common to report credible intervals conditional on ğ»2. We
also have shown how to adjust the probability of the interval to reflect our
posterior uncertainty about ğ»2. In the next chapter, we will turn to regression
models to incorporate continuous explanatory variables.

112CHAPTER 5. HYPOTHESIS TESTING WITH NORMAL POPULATIONS

Chapter 6
Introduction to Bayesian
Regression
In the previous chapter, we introduced Bayesian decision making using posterior
probabilities and a variety of loss functions. We discussed how to minimize the
expected loss for hypothesis testing. Moreover, we instroduced the concept of
Bayes factors and gave some examples on how Bayes factors can be used in
Bayesian hypothesis testing for comparison of two means. We also discussed
how to choose appropriate and robust priors. When there is no conjugacy, we
applied Markov Chain Monte Carlo simulation to approximate the posterior
distributions of parameters of interest.
In this chapter, we will apply Bayesian inference methods to linear regression.
We will first apply Bayesian statistics to simple linear regression models, then
generalize the results to multiple linear regression models. We will see when us-
ing the reference prior, the posterior means, posterior standard deviations, and
credible intervals of the coeï¬€icients coincide with the counterparts in the fre-
quentist ordinary least square (OLS) linear regression models. However, using
the Bayesian framework, we can now interpret credible intervals as the proba-
bilities of the coeï¬€icients lying in such intervals.
6.1
Bayesian Simple Linear Regression
In this section, we will turn to Bayesian inference in simple linear regressions.
We will use the reference prior distribution on coeï¬€icients, which will provide
a connection between the frequentist solutions and Bayesian answers.
This
provides a baseline analysis for comparisons with more informative prior dis-
tributions. To illustrate the ideas, we will use an example of predicting body
fat.
113

114
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
6.1.1
Frequentist Ordinary Least Square (OLS) Simple
Linear Regression
Obtaining accurate measurements of body fat is expensive and not easy to be
done. Instead, predictive models that predict the percentage of body fat which
use readily available measurements such as abdominal circumference are easy
to use and inexpensive. We will apply a simple linear regression to predict body
fat using abdominal circumference as an example to illustrate the Bayesian
approach of linear regression. The data set bodyfat can be found from the
library BAS.
To start, we load the BAS library (which can be downloaded from CRAN) to
access the dataframe. We print out a summary of the variables in this dataframe.
library(BAS)
data(bodyfat)
summary(bodyfat)
##
Density
Bodyfat
Age
Weight
##
Min.
:0.995
Min.
: 0.00
Min.
:22.00
Min.
:118.5
##
1st Qu.:1.041
1st Qu.:12.47
1st Qu.:35.75
1st Qu.:159.0
##
Median :1.055
Median :19.20
Median :43.00
Median :176.5
##
Mean
:1.056
Mean
:19.15
Mean
:44.88
Mean
:178.9
##
3rd Qu.:1.070
3rd Qu.:25.30
3rd Qu.:54.00
3rd Qu.:197.0
##
Max.
:1.109
Max.
:47.50
Max.
:81.00
Max.
:363.1
##
Height
Neck
Chest
Abdomen
##
Min.
:29.50
Min.
:31.10
Min.
: 79.30
Min.
: 69.40
##
1st Qu.:68.25
1st Qu.:36.40
1st Qu.: 94.35
1st Qu.: 84.58
##
Median :70.00
Median :38.00
Median : 99.65
Median : 90.95
##
Mean
:70.15
Mean
:37.99
Mean
:100.82
Mean
: 92.56
##
3rd Qu.:72.25
3rd Qu.:39.42
3rd Qu.:105.38
3rd Qu.: 99.33
##
Max.
:77.75
Max.
:51.20
Max.
:136.20
Max.
:148.10
##
Hip
Thigh
Knee
Ankle
Biceps
##
Min.
: 85.0
Min.
:47.20
Min.
:33.00
Min.
:19.1
Min.
:24.80
##
1st Qu.: 95.5
1st Qu.:56.00
1st Qu.:36.98
1st Qu.:22.0
1st Qu.:30.20
##
Median : 99.3
Median :59.00
Median :38.50
Median :22.8
Median :32.05
##
Mean
: 99.9
Mean
:59.41
Mean
:38.59
Mean
:23.1
Mean
:32.27
##
3rd Qu.:103.5
3rd Qu.:62.35
3rd Qu.:39.92
3rd Qu.:24.0
3rd Qu.:34.33
##
Max.
:147.7
Max.
:87.30
Max.
:49.10
Max.
:33.9
Max.
:45.00
##
Forearm
Wrist
##
Min.
:21.00
Min.
:15.80
##
1st Qu.:27.30
1st Qu.:17.60
##
Median :28.70
Median :18.30
##
Mean
:28.66
Mean
:18.23
##
3rd Qu.:30.00
3rd Qu.:18.80
##
Max.
:34.90
Max.
:21.40

6.1. BAYESIAN SIMPLE LINEAR REGRESSION
115
This data frame includes 252 observations of menâ€™s body fat and other measure-
ments, such as waist circumference (Abdomen). We will construct a Bayesian
model of simple linear regression, which uses Abdomen to predict the response
variable Bodyfat. Let ğ‘¦ğ‘–, ğ‘–= 1, â‹¯, 252 denote the measurements of the re-
sponse variable Bodyfat, and let ğ‘¥ğ‘–be the waist circumference measurements
Abdomen. We regress Bodyfat on the predictor Abdomen. This regression model
can be formulated as
ğ‘¦ğ‘–= ğ›¼+ ğ›½ğ‘¥ğ‘–+ ğœ–ğ‘–,
ğ‘–= 1, â‹¯, 252.
Here, we assume error ğœ–ğ‘–is independent and identically distributed as normal
random variables with mean zero and constant variance ğœ2:
ğœ–ğ‘–
iid
âˆ¼Normal(0, ğœ2).
The figure below shows the percentage body fat obtained from under water
weighing and the abdominal circumference measurements for 252 men. To pre-
dict body fat, the line overlayed on the scatter plot illustrates the best fitting
ordinary least squares (OLS) line obtained with the lm function in R.
# Frequentist OLS linear regression
bodyfat.lm = lm(Bodyfat ~ Abdomen, data = bodyfat)
summary(bodyfat.lm)
##
## Call:
## lm(formula = Bodyfat ~ Abdomen, data = bodyfat)
##
## Residuals:
##
Min
1Q
Median
3Q
Max
## -19.0160
-3.7557
0.0554
3.4215
12.9007
##
## Coefficients:
##
Estimate Std. Error t value Pr(>|t|)
## (Intercept) -39.28018
2.66034
-14.77
<2e-16 ***
## Abdomen
0.63130
0.02855
22.11
<2e-16 ***
## ---
## Signif. codes:
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
##
## Residual standard error: 4.877 on 250 degrees of freedom
## Multiple R-squared:
0.6617, Adjusted R-squared:
0.6603
## F-statistic: 488.9 on 1 and 250 DF,
p-value: < 2.2e-16
# Extract coefficients
beta = coef(bodyfat.lm)
# Visualize regression line on the scatter plot
library(ggplot2)

116
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
ggplot(data = bodyfat, aes(x = Abdomen, y = Bodyfat)) +
geom_point(color = "blue") +
geom_abline(intercept = beta[1], slope = beta[2], size = 1) +
xlab("abdomen circumference (cm)")
0
10
20
30
40
70
90
110
130
150
abdomen circumference (cm)
Bodyfat
From the summary, we see that this model has an estimated slope,
Ì‚ğ›½, of 0.63
and an estimated ğ‘¦-intercept,
Ì‚ğ›¼, of about -39.28%. This gives us the prediction
formula
Ì‚
Bodyfat = âˆ’39.28 + 0.63 Ã— Abdomen.
For every additional centimeter, we expect body fat to increase by 0.63%. The
negative ğ‘¦-intercept of course does not make sense as a physical model, but
neither does predicting a male with a waist of zero centimeters. Nevertheless,
this linear regression may be an accurate approximation for prediction purposes
for measurements that are in the observed range for this population.
Each of the residuals, which provide an estimate of the fitting error, is equal to
Ì‚ğœ–ğ‘–= ğ‘¦ğ‘–âˆ’
Ì‚ğ‘¦ğ‘–, the difference between the observed value ğ‘¦ğ‘–and the fitted value
Ì‚ğ‘¦ğ‘–=
Ì‚ğ›¼+ Ì‚ğ›½ğ‘¥ğ‘–, where ğ‘¥ğ‘–is the abdominal circumference for the ğ‘–th male.
Ì‚ğœ–ğ‘–is used
for diagnostics as well as estimating the constant variance in the assumption of
the model ğœ2 via the mean squared error (MSE):
Ì‚ğœ2 =
1
ğ‘›âˆ’2
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–)2 =
1
ğ‘›âˆ’2
ğ‘›
âˆ‘
ğ‘–
Ì‚ğœ–2
ğ‘–.
Here the degrees of freedom ğ‘›âˆ’2 are the number of observations adjusted for
the number of parameters (which is 2) that we estimated in the regression. The
MSE,
Ì‚ğœ2, may be calculated through squaring the residuals of the output of
bodyfat.lm.
# Obtain residuals and n
resid = residuals(bodyfat.lm)
n = length(resid)

6.1. BAYESIAN SIMPLE LINEAR REGRESSION
117
# Calculate MSE
MSE = 1/ (n - 2) * sum((resid ^ 2))
MSE
## [1] 23.78985
If this model is correct, the residuals and fitted values should be uncorrelated,
and the expected value of the residuals is zero. We apply the scatterplot of
residuals versus fitted values, which provides an additional visual check of the
model adequacy.
# Combine residuals and fitted values into a data frame
result = data.frame(fitted_values = fitted.values(bodyfat.lm),
residuals = residuals(bodyfat.lm))
# Load library and plot residuals versus fitted values
library(ggplot2)
ggplot(data = result, aes(x = fitted_values, y = residuals)) +
geom_point(pch = 1, size = 2) +
geom_abline(intercept = 0, slope = 0) +
xlab(expression(paste("fitted value ", widehat(Bodyfat)))) +
ylab("residuals")
âˆ’20
âˆ’10
0
10
10
20
30
40
50
fitted value Bodyfat
residuals
# Readers may also use `plot` function
With the exception of one observation for the individual with the largest fitted
value, the residual plot suggests that this linear regression is a reasonable ap-
proximation. The case number of the observation with the largest fitted value
can be obtained using the which function in R. Further examination of the data
frame shows that this case also has the largest waist measurement Abdomen.
This may be our potential outlier and we will have more discussion on outliers
in Section 6.2.

118
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
# Find the observation with the largest fitted value
which.max(as.vector(fitted.values(bodyfat.lm)))
## [1] 39
# Shows this observation has the largest Abdomen
which.max(bodyfat$Abdomen)
## [1] 39
Furthermore, we can check the normal probability plot of the residuals for the
assumption of normally distributed errors. We see that only Case 39, the one
with the largest waist measurement, is exceptionally away from the normal
quantile.
plot(bodyfat.lm, which = 2)
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
âˆ’4
âˆ’2
0
2
Theoretical Quantiles
Standardized residuals
lm(Bodyfat ~ Abdomen)
Normal Qâˆ’Q
39
207
204
The confidence interval of ğ›¼and ğ›½can be constructed using the standard errors
seğ›¼and seğ›½respectively. To proceed, we introduce notations of some â€œsums of
squaresâ€
Sğ‘¥ğ‘¥=
ğ‘›
âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
Sğ‘¦ğ‘¦=
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’Ì„ğ‘¦)2
Sğ‘¥ğ‘¦=
ğ‘›
âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)(ğ‘¦ğ‘–âˆ’Ì„ğ‘¦)
SSE =
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–)2 =
ğ‘›
âˆ‘
ğ‘–
Ì‚ğœ–2
ğ‘–.
The estimates of the ğ‘¦-intercept ğ›¼, and the slope ğ›½, which are denoted as
Ì‚ğ›¼and

6.1. BAYESIAN SIMPLE LINEAR REGRESSION
119
Ì‚ğ›½respectively, can be calculated using these â€œsums of squaresâ€
Ì‚ğ›½= âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)(ğ‘¦ğ‘–âˆ’Ì„ğ‘¦)
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
= Sğ‘¥ğ‘¦
Sğ‘¥ğ‘¥
,
Ì‚ğ›¼=
Ì„ğ‘¦âˆ’
Ì‚ğ›½Ì„ğ‘¥=
Ì„ğ‘¦âˆ’Sğ‘¥ğ‘¦
Sğ‘¥ğ‘¥
Ì„ğ‘¥.
The last â€œsum of squareâ€ is the sum of squares of errors (SSE). Its sample mean
is exactly the mean squared error (MSE) we introduced previously
Ì‚ğœ2 = SSE
ğ‘›âˆ’2 = MSE.
The standard errors, seğ›¼and seğ›½, are given as
seğ›¼=âˆšSSE
ğ‘›âˆ’2 ( 1
ğ‘›+
Ì„ğ‘¥2
Sğ‘¥ğ‘¥
) =
Ì‚ğœâˆš1
ğ‘›+
Ì„ğ‘¥2
Sğ‘¥ğ‘¥
,
seğ›½=âˆšSSE
ğ‘›âˆ’2
1
Sğ‘¥ğ‘¥
=
Ì‚ğœ
âˆšSğ‘¥ğ‘¥
.
We may construct the confidence intervals of ğ›¼and ğ›½using the ğ‘¡-statistics
ğ‘¡âˆ—
ğ›¼= ğ›¼âˆ’
Ì‚ğ›¼
seğ›¼
,
ğ‘¡âˆ—
ğ›½= ğ›½âˆ’
Ì‚ğ›½
seğ›½
.
They both have degrees of freedom ğ‘›âˆ’2.
6.1.2
Bayesian Simple Linear Regression Using the Refer-
ence Prior
Let us now turn to the Bayesian version and show that under the reference
prior, we will obtain the posterior distributions of ğ›¼and ğ›½analogous with the
frequentist OLS results.
The Bayesian model starts with the same model as the classical frequentist
approach:
ğ‘¦ğ‘–= ğ›¼+ ğ›½ğ‘¥ğ‘–+ ğœ–ğ‘–,
ğ‘–= 1, â‹¯, ğ‘›.
with the assumption that the errors, ğœ–ğ‘–, are independent and identically dis-
tributed as normal random variables with mean zero and constant variance ğœ2.
This assumption is exactly the same as in the classical inference case for testing
and constructing confidence intervals for ğ›¼and ğ›½.
Our goal is to update the distributions of the unknown parameters ğ›¼, ğ›½, and
ğœ2, based on the data ğ‘¥1, ğ‘¦1, â‹¯, ğ‘¥ğ‘›, ğ‘¦ğ‘›, where ğ‘›is the number of observations.

120
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
Under the assumption that the errors ğœ–ğ‘–are normally distributed with constant
variance ğœ2, we have for the random variable of each response ğ‘Œğ‘–, conditioning
on the observed data ğ‘¥ğ‘–and the parameters ğ›¼, ğ›½, ğœ2, is normally distributed:
ğ‘Œğ‘–| ğ‘¥ğ‘–, ğ›¼, ğ›½, ğœ2 âˆ¼Normal(ğ›¼+ ğ›½ğ‘¥ğ‘–, ğœ2),
ğ‘–= 1, â‹¯, ğ‘›.
That is, the likelihood of each ğ‘Œğ‘–given ğ‘¥ğ‘–, ğ›¼, ğ›½, and ğœ2 is given by
ğ‘(ğ‘¦ğ‘–| ğ‘¥ğ‘–, ğ›¼, ğ›½, ğœ2) =
1
âˆš
2ğœ‹ğœ2 exp (âˆ’(ğ‘¦ğ‘–âˆ’(ğ›¼+ ğ›½ğ‘¥ğ‘–))2
2ğœ2
) .
The likelihood of ğ‘Œ1, â‹¯, ğ‘Œğ‘›is the product of each likelihood ğ‘(ğ‘¦ğ‘–| ğ‘¥ğ‘–, ğ›¼, ğ›½, ğœ2),
since we assume each response ğ‘Œğ‘–is independent from each other. Since this
likelihood depends on the values of ğ›¼, ğ›½, and ğœ2, it is sometimes denoted as a
function of ğ›¼, ğ›½, and ğœ2: â„’(ğ›¼, ğ›½, ğœ2).
We first consider the case under the reference prior, which is our standard nonin-
formative prior. Using the reference prior, we will obtain familiar distributions
as the posterior distributions of ğ›¼, ğ›½, and ğœ2, which gives the analogue to the
frequentist results. Here we assume the joint prior distribution of ğ›¼, ğ›½, and ğœ2
to be proportional to the inverse of ğœ2
ğ‘(ğ›¼, ğ›½, ğœ2) âˆ1
ğœ2 .
(6.1)
Using the hierachical model framework, this is equivalent to assuming that the
joint prior distribution of ğ›¼and ğ›½under ğœ2 is the uniform prior, while the prior
distribution of ğœ2 is proportional to 1
ğœ2 . That is
ğ‘(ğ›¼, ğ›½| ğœ2) âˆ1,
ğ‘(ğœ2) âˆ1
ğœ2 ,
Combining the two using conditional probability, we will get the same joint prior
distribution (6.1).
Then we apply the Bayesâ€™ rule to derive the joint posterior distribution after
observing data ğ‘¦1, â‹¯, ğ‘¦ğ‘›. Bayesâ€™ rule states that the joint posterior distribution
of ğ›¼, ğ›½, and ğœ2 is proportional to the product of the likelihood and the joint

6.1. BAYESIAN SIMPLE LINEAR REGRESSION
121
prior distribution:
ğ‘âˆ—(ğ›¼, ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›) âˆ[
ğ‘›
âˆ
ğ‘–
ğ‘(ğ‘¦ğ‘–| ğ‘¥ğ‘–, ğ›¼, ğ›½, ğœ2)] ğ‘(ğ›¼, ğ›½, ğœ2)
âˆ[(
1
(ğœ2)1/2 exp (âˆ’(ğ‘¦1 âˆ’(ğ›¼+ ğ›½ğ‘¥1))2
2ğœ2
)) Ã— â‹¯
Ã— (
1
(ğœ2)1/2 exp (âˆ’(ğ‘¦ğ‘›âˆ’(ğ›¼+ ğ›½ğ‘¥ğ‘›))2
2ğœ2
))] Ã— ( 1
ğœ2 )
âˆ
1
(ğœ2)(ğ‘›+2)/2 exp (âˆ’âˆ‘ğ‘–(ğ‘¦ğ‘–âˆ’ğ›¼âˆ’ğ›½ğ‘¥ğ‘–)
2
2ğœ2
)
To obtain the marginal posterior distribution of ğ›½, we need to integrate ğ›¼and
ğœ2 out from the joint posterior distribution
ğ‘âˆ—(ğ›½| ğ‘¦1, â‹¯, ğ‘¦ğ‘›) = âˆ«
âˆ
0
(âˆ«
âˆ
âˆ’âˆ
ğ‘âˆ—(ğ›¼, ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›) ğ‘‘ğ›¼) ğ‘‘ğœ2.
We leave the detailed calculation in Section 6.1.4. It can be shown that the
marginal posterior distribution of ğ›½is the Studentâ€™s ğ‘¡-distribution
ğ›½| ğ‘¦1, â‹¯, ğ‘¦ğ‘›âˆ¼t (ğ‘›âˆ’2,
Ì‚ğ›½,
Ì‚ğœ2
Sğ‘¥ğ‘¥
) = t (ğ‘›âˆ’2,
Ì‚ğ›½, (seğ›½)2) ,
with degrees of freedom ğ‘›âˆ’2, center at
Ì‚ğ›½, the slope estimate we obtained from
the frequentist OLS model, and scale parameter
Ì‚ğœ2
Sğ‘¥ğ‘¥
= (seğ›½)
2, which is the
square of the standard error of
Ì‚ğ›½under the frequentist OLS model.
Similarly, we can integrate out ğ›½and ğœ2 from the joint posterior distribution
to get the marginal posterior distribution of ğ›¼, ğ‘âˆ—(ğ›¼| ğ‘¦1, â‹¯, ğ‘¦ğ‘›). It turns out
that ğ‘âˆ—(ğ›¼| ğ‘¦1, â‹¯, ğ‘¦ğ‘›) is again a Studentâ€™s ğ‘¡-distribution with degrees of freedom
ğ‘›âˆ’2, center at
Ì‚ğ›¼, the ğ‘¦-intercept estimate from the frequentist OLS model, and
scale parameter
Ì‚ğœ2 ( 1
ğ‘›+
Ì„ğ‘¥2
Sğ‘¥ğ‘¥
) = (seğ›¼)
2, which is the square of the standard
error of
Ì‚ğ›¼under the frequentist OLS model
ğ›¼| ğ‘¦1, â‹¯, ğ‘¦ğ‘›âˆ¼t (ğ‘›âˆ’2,
Ì‚ğ›¼,
Ì‚ğœ2 ( 1
ğ‘›+
Ì„ğ‘¥2
Sğ‘¥ğ‘¥
)) = t (ğ‘›âˆ’2,
Ì‚ğ›¼, (seğ›¼)2) .
Finally, we can show that the marginal posterior distribution of ğœ2 is the inverse
Gamma distribution, or equivalently, the reciprocal of ğœ2, which is the precision
ğœ™, follows the Gamma distribution
ğœ™= 1
ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›âˆ¼Gamma (ğ‘›âˆ’2
2
, SSE
2 ) .

122
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
Moreover, similar to the Normal-Gamma conjugacy under the reference prior
introduced in the previous chapters, the joint posterior distribution of ğ›½, ğœ2, and
the joint posterior distribution of ğ›¼, ğœ2 are both Normal-Gamma. In particular,
the posterior distribution of ğ›½conditioning on ğœ2 is
ğ›½| ğœ2, data âˆ¼Normal ( Ì‚ğ›½, ğœ2
Sğ‘¥ğ‘¥
) ,
and the posterior distribution of ğ›¼conditioning on ğœ2 is
ğ›¼| ğœ2, data âˆ¼Normal ( Ì‚ğ›¼, ğœ2 ( 1
ğ‘›+
Ì„ğ‘¥2
Sğ‘¥ğ‘¥
)) .
Credible Intervals for Slope ğ›½and ğ‘¦-Intercept ğ›¼
The Bayesian posterior distribution results of ğ›¼and ğ›½show that under the refer-
ence prior, the posterior credible intervals are in fact numerically equivalent
to the confidence intervals from the classical frequentist OLS analysis. This
provides a baseline analysis for other Bayesian analyses with other informative
prior distributions or perhaps other â€œobjectiveâ€ prior distributions, such as the
Cauchy distribution. (Cauchy distribution is the Studentâ€™s ğ‘¡prior with 1 degree
of freedom.)
Since the credible intervals are numerically the same as the confidence intervals,
we can use the lm function to obtain the OLS estimates and construct the
credible intervals of ğ›¼and ğ›½
output = summary(bodyfat.lm)$coef[, 1:2]
output
##
Estimate Std. Error
## (Intercept) -39.2801847 2.66033696
## Abdomen
0.6313044 0.02855067
The columns labeled Estimate and Std. Error are equivalent to the centers
(or posterior means) and scale parameters (or standard deviations) in the two
Studentâ€™s ğ‘¡-distributions respectively. The credible intervals of ğ›¼and ğ›½are the
same as the frequentist confidence intervals, but now we can interpret them
from the Bayesian perspective.
The confint function provides 95% confidence intervals. Under the reference
prior, they are equivalent to the 95% credible intervals. The code below extracts
them and relabels the output as the Bayesian results.
out = cbind(output, confint(bodyfat.lm))
colnames(out) = c("posterior mean", "posterior std", "2.5", "97.5")
round(out, 2)
##
posterior mean posterior std
2.5
97.5

6.1. BAYESIAN SIMPLE LINEAR REGRESSION
123
## (Intercept)
-39.28
2.66 -44.52 -34.04
## Abdomen
0.63
0.03
0.58
0.69
These intervals coincide with the confidence intervals from the frequentist ap-
proach. The primary difference is the interpretation. For example, based on
the data, we believe that there is a 95% chance that body fat will increase
by 5.75% up to 6.88% for every additional 10 centimeter increase in the waist
circumference.
Credible Intervals for the Mean ğœ‡ğ‘Œand the Prediction ğ‘¦ğ‘›+1
From our assumption of the model
ğ‘¦ğ‘–= ğ›¼+ ğ›½ğ‘¥ğ‘–+ ğœ–ğ‘–,
the mean of the response variable ğ‘Œ, ğœ‡ğ‘Œ, at the point ğ‘¥ğ‘–is
ğœ‡ğ‘Œ| ğ‘¥ğ‘–= ğ¸[ğ‘Œ| ğ‘¥ğ‘–] = ğ›¼+ ğ›½ğ‘¥ğ‘–.
Under the reference prior, ğœ‡ğ‘Œhas a posterior distributuion
ğ›¼+ ğ›½ğ‘¥ğ‘–| data âˆ¼t(ğ‘›âˆ’2,
Ì‚ğ›¼+
Ì‚ğ›½ğ‘¥ğ‘–, S2
ğ‘Œ|ğ‘‹ğ‘–),
where
S2
ğ‘Œ|ğ‘‹ğ‘–=
Ì‚ğœ2 ( 1
ğ‘›+ (ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
Sğ‘¥ğ‘¥
)
Any new prediction ğ‘¦ğ‘›+1 at a point ğ‘¥ğ‘›+1 also follows the Studentâ€™s ğ‘¡-distribution
ğ‘¦ğ‘›+1 | data, ğ‘¥ğ‘›+1 âˆ¼t (ğ‘›âˆ’2,
Ì‚ğ›¼+
Ì‚ğ›½ğ‘¥ğ‘›+1, S2
ğ‘Œ|ğ‘‹ğ‘›+1) ,
where
S2
ğ‘Œ|ğ‘‹ğ‘›+1 =
Ì‚ğœ2 + Ì‚ğœ2 ( 1
ğ‘›+ (ğ‘¥ğ‘›+1 âˆ’
Ì„ğ‘¥)2
Sğ‘¥ğ‘¥
) =
Ì‚ğœ2 (1 + 1
ğ‘›+ (ğ‘¥ğ‘›+1 âˆ’
Ì„ğ‘¥)2
Sğ‘¥ğ‘¥
) .
The variance for predicting a new observation ğ‘¦ğ‘›+1 has an extra
Ì‚ğœ2 which comes
from the uncertainty of a new observation about the mean ğœ‡ğ‘Œestimated by the
regression line.
We can extract these intervals using the predict function
library(ggplot2)
# Construct current prediction
alpha = bodyfat.lm$coefficients[1]
beta = bodyfat.lm$coefficients[2]
new_x = seq(min(bodyfat$Abdomen), max(bodyfat$Abdomen),
length.out = 100)
y_hat = alpha + beta * new_x

124
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
# Get lower and upper bounds for mean
ymean = data.frame(predict(bodyfat.lm,
newdata = data.frame(Abdomen = new_x),
interval = "confidence",
level = 0.95))
# Get lower and upper bounds for prediction
ypred = data.frame(predict(bodyfat.lm,
newdata = data.frame(Abdomen = new_x),
interval = "prediction",
level = 0.95))
output = data.frame(x = new_x, y_hat = y_hat, ymean_lwr = ymean$lwr, ymean_upr = ymean$
ypred_lwr = ypred$lwr, ypred_upr = ypred$upr)
# Extract potential outlier data point
outlier = data.frame(x = bodyfat$Abdomen[39], y = bodyfat$Bodyfat[39])
# Scatter plot of original
plot1 = ggplot(data = bodyfat, aes(x = Abdomen, y = Bodyfat)) + geom_point(color = "blu
# Add bounds of mean and prediction
plot2 = plot1 +
geom_line(data = output, aes(x = new_x, y = y_hat, color = "first"), lty = 1) +
geom_line(data = output, aes(x = new_x, y = ymean_lwr, lty = "second")) +
geom_line(data = output, aes(x = new_x, y = ymean_upr, lty = "second")) +
geom_line(data = output, aes(x = new_x, y = ypred_upr, lty = "third")) +
geom_line(data = output, aes(x = new_x, y = ypred_lwr, lty = "third")) +
scale_colour_manual(values = c("orange"), labels = "Posterior mean", name = "") +
scale_linetype_manual(values = c(2, 3), labels = c("95% CI for mean", "95% CI for pre
, name = "") +
theme_bw() +
theme(legend.position = c(1, 0), legend.justification = c(1.5, 0))
# Identify potential outlier
plot2 + geom_point(data = outlier, aes(x = x, y = y), color = "orange", pch = 1, cex =

6.1. BAYESIAN SIMPLE LINEAR REGRESSION
125
0
20
40
60
70
90
110
130
150
Abdomen
Bodyfat
95% CI for mean
95% CI for predictions
Posterior mean
Note in the above plot, the legend â€œCIâ€ can mean either confidence interval or
credible interval. The difference comes down to the interpretation. For example,
the prediction at the same abdominal circumference as in Case 39 is
pred.39 = predict(bodyfat.lm, newdata = bodyfat[39, ], interval = "prediction", level = 0.95)
out = cbind(bodyfat[39,]$Abdomen, pred.39)
colnames(out) = c("abdomen", "prediction", "lower", "upper")
out
##
abdomen prediction
lower
upper
## 39
148.1
54.21599 44.0967 64.33528
Based on the data, a Bayesian would expect that a man with waist circumference
of 148.1 centimeters should have bodyfat of 54.216% with a 95% chance that it
is between 44.097% and 64.335%.
While we expect the majority of the data will be within the prediction intervals
(the short dashed grey lines), Case 39 seems to be well below the interval. We
next use Bayesian methods in Section 6.2 to calculate the probability that this
case is abnormal or is an outlier by falling more than ğ‘˜standard deviations from
either side of the mean.
6.1.3
Informative Priors
Except from the noninformative reference prior, we may also consider using a
more general semi-conjugate prior distribution of ğ›¼, ğ›½, and ğœ2 when there is
information available about the parameters.
Since the data ğ‘¦1, â‹¯, ğ‘¦ğ‘›are normally distributed, from Chapter 3 we see that
a Normal-Gamma distribution will form a conjugacy in this situation. We then
set up prior distributions through a hierarchical model. We first assume that,
given ğœ2, ğ›¼and ğ›½together follow the bivariate normal prior distribution, from

126
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
which their marginal distributions are both normal,
ğ›¼| ğœ2 âˆ¼Normal(ğ‘0, ğœ2Sğ›¼)
ğ›½| ğœ2 âˆ¼Normal(ğ‘0, ğœ2Sğ›½),
with covariance
Cov(ğ›¼, ğ›½| ğœ2) = ğœ2Sğ›¼ğ›½.
Here, ğœ2, ğ‘†ğ›¼, ğ‘†ğ›½, and ğ‘†ğ›¼ğ›½are hyperparameters. This is equivalent to setting
the coeï¬€icient vector ğ›½= (ğ›¼, ğ›½)ğ‘‡1 to have a bivariate normal distribution with
covariance matrix Î£0
Î£0 = ğœ2 ( ğ‘†ğ›¼
ğ‘†ğ›¼ğ›½
ğ‘†ğ›¼ğ›½
ğ‘†ğ›½
) .
That is,
ğ›½= (ğ›¼, ğ›½)ğ‘‡| ğœ2 âˆ¼BivariateNormal(b = (ğ‘0, ğ‘0)ğ‘‡, ğœ2Î£0).
Then for ğœ2, we will impose an inverse Gamma distribution as its prior distri-
bution
1/ğœ2 âˆ¼Gamma (ğœˆ0
2 , ğœˆ0ğœ0
2
) .
Now the joint prior distribution of ğ›¼, ğ›½, and ğœ2 form a distribution that is
analogous to the Normal-Gamma distribution. Prior information about ğ›¼, ğ›½,
and ğœ2 are encoded in the hyperparameters ğ‘0, ğ‘0, Sğ›¼, Sğ›½, Sğ›¼ğ›½, ğœˆ0, and ğœ0.
The marginal posterior distribution of the coeï¬€icient vector ğ›½= (ğ›¼, ğ›½) will
be bivariate normal, and the marginal posterior distribution of ğœ2 is again an
inverse Gamma distribution
1/ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›âˆ¼Gamma (ğœˆ0 + ğ‘›
2
, ğœˆ0ğœ2
0 + SSE
2
) .
One can see that the reference prior is the limiting case of this conjugate prior
we impose.
We usually use Gibbs sampling to approximate the joint poste-
rior distribution instead of using the result directly, especially when we have
more regression coeï¬€icients in multiple linear regression models. We omit the
derivation of the posterior distributions due to the heavy use of advanced linear
algebra. One can refer to Hoff (2009) for more details.
Based on any prior information we have for the model, we can also impose other
priors and assumptions on ğ›¼, ğ›½, and ğœ2 to get different Bayesian results. Most
of these priors will not form any conjugacy and will require us to use simulation
methods such as Markov Chain Monte Carlo (MCMC) for approximations. We
will introduce the general idea of MCMC in Chapter 8.
1(ğ›¼, ğ›½)ğ‘‡means we transpose the row vector (ğ›¼, ğ›½) into a column vector ( ğ›¼
ğ›½).

6.1. BAYESIAN SIMPLE LINEAR REGRESSION
127
6.1.4
(Optional) Derivations of Marginal Posterior Distri-
butions of ğ›¼, ğ›½, ğœ2
In this section, we will use the notations we introduced earlier such as SSE, the
sum of squares of errors,
Ì‚ğœ2, the mean squared error, Sğ‘¥ğ‘¥, seğ›¼, seğ›½and so on to
simplify our calculations.
We will also use the following quantities derived from the formula of
Ì„ğ‘¥,
Ì„ğ‘¦,
Ì‚ğ›¼,
and
Ì‚ğ›½
ğ‘›
âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥) = 0
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’Ì„ğ‘¦) = 0
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–) =
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’( Ì‚ğ›¼+
Ì‚ğ›½ğ‘¥ğ‘–)) = 0
ğ‘›
âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–) =
ğ‘›
âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)(ğ‘¦ğ‘–âˆ’Ì„ğ‘¦âˆ’
Ì‚ğ›½(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)) =
ğ‘›
âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)(ğ‘¦ğ‘–âˆ’Ì„ğ‘¦) âˆ’
Ì‚ğ›½
ğ‘›
âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 = 0
ğ‘›
âˆ‘
ğ‘–
ğ‘¥2
ğ‘–=
ğ‘›
âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2 = Sğ‘¥ğ‘¥+ ğ‘›Ì„ğ‘¥2
We first further simplify the numerator inside the exponential function in the
formula of ğ‘âˆ—(ğ›¼, ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›):
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’ğ›¼âˆ’ğ›½ğ‘¥ğ‘–)
2
=
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’
Ì‚ğ›¼âˆ’
Ì‚ğ›½ğ‘¥ğ‘–âˆ’(ğ›¼âˆ’
Ì‚ğ›¼) âˆ’(ğ›½âˆ’
Ì‚ğ›½)ğ‘¥ğ‘–)
2
=
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’
Ì‚ğ›¼âˆ’
Ì‚ğ›½ğ‘¥ğ‘–)
2
+
ğ‘›
âˆ‘
ğ‘–
(ğ›¼âˆ’
Ì‚ğ›¼)2 +
ğ‘›
âˆ‘
ğ‘–
(ğ›½âˆ’
Ì‚ğ›½)2(ğ‘¥ğ‘–)2
âˆ’2
ğ‘›
âˆ‘
ğ‘–
(ğ›¼âˆ’
Ì‚ğ›¼)(ğ‘¦ğ‘–âˆ’
Ì‚ğ›¼âˆ’
Ì‚ğ›½ğ‘¥ğ‘–) âˆ’2
ğ‘›
âˆ‘
ğ‘–
(ğ›½âˆ’
Ì‚ğ›½)(ğ‘¥ğ‘–)(ğ‘¦ğ‘–âˆ’
Ì‚ğ›¼âˆ’
Ì‚ğ›½ğ‘¥ğ‘–) + 2
ğ‘›
âˆ‘
ğ‘–
(ğ›¼âˆ’
Ì‚ğ›¼)(ğ›½âˆ’
Ì‚ğ›½)(ğ‘¥ğ‘–)
=SSE + ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼)2 + (ğ›½âˆ’
Ì‚ğ›½)2
ğ‘›
âˆ‘
ğ‘–
ğ‘¥2
ğ‘–âˆ’2(ğ›¼âˆ’
Ì‚ğ›¼)
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–) âˆ’2(ğ›½âˆ’
Ì‚ğ›½)
ğ‘›
âˆ‘
ğ‘–
ğ‘¥ğ‘–(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–) + 2(ğ›¼âˆ’
Ì‚ğ›¼)(ğ›½âˆ’
Ì‚ğ›½)(ğ‘›Ì„ğ‘¥)
It is clear that
âˆ’2(ğ›¼âˆ’
Ì‚ğ›¼)
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–) = 0

128
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
And
âˆ’2(ğ›½âˆ’
Ì‚ğ›½)
ğ‘›
âˆ‘
ğ‘–
ğ‘¥ğ‘–(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–) = âˆ’2(ğ›½âˆ’
Ì‚ğ›½) âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–) âˆ’2(ğ›½âˆ’
Ì‚ğ›½)
ğ‘›
âˆ‘
ğ‘–
Ì„ğ‘¥(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–)
= âˆ’2(ğ›½âˆ’
Ì‚ğ›½) Ã— 0 âˆ’2(ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’Ì‚ğ‘¦ğ‘–) = 0
Finally, we use the quantity that
ğ‘›
âˆ‘
ğ‘–
ğ‘¥2
ğ‘–=
ğ‘›
âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2 to combine the
terms ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼)2, 2(ğ›¼âˆ’
Ì‚ğ›¼)(ğ›½âˆ’
Ì‚ğ›½)
ğ‘›
âˆ‘
ğ‘–
ğ‘¥ğ‘–, and (ğ›½âˆ’
Ì‚ğ›½)2
ğ‘›
âˆ‘
ğ‘–
ğ‘¥2
ğ‘–together.
ğ‘›
âˆ‘
ğ‘–
(ğ‘¦ğ‘–âˆ’ğ›¼âˆ’ğ›½ğ‘¥ğ‘–)2
=SSE + ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼)2 + (ğ›½âˆ’
Ì‚ğ›½)2
ğ‘›
âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + (ğ›½âˆ’
Ì‚ğ›½)2(ğ‘›Ì„ğ‘¥2) + 2(ğ›¼âˆ’
Ì‚ğ›¼)(ğ›½âˆ’
Ì‚ğ›½)(ğ‘›Ì„ğ‘¥)
=SSE + (ğ›½âˆ’
Ì‚ğ›½)2Sğ‘¥ğ‘¥+ ğ‘›[(ğ›¼âˆ’
Ì‚ğ›¼) + (ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥]
2
Therefore, the posterior joint distribution of ğ›¼, ğ›½, ğœ2 can be simplied as
ğ‘âˆ—(ğ›¼, ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›) âˆ
1
(ğœ2)(ğ‘›+2)/2 exp (âˆ’âˆ‘ğ‘–(ğ‘¦ğ‘–âˆ’ğ›¼âˆ’ğ›½ğ‘¥ğ‘–)2
2ğœ2
)
=
1
(ğœ2)(ğ‘›+2)/2 exp (âˆ’SSE + ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼+ (ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥)2 + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
)
6.1.5
Marginal Posterior Distribution of ğ›½
To get the marginal posterior distribution of ğ›½, we need to integrate out ğ›¼and
ğœ2 from ğ‘âˆ—(ğ›¼, ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›):
ğ‘âˆ—(ğ›½| ğ‘¦1, â‹¯, ğ‘¦ğ‘›) = âˆ«
âˆ
0
âˆ«
âˆ
âˆ’âˆ
ğ‘âˆ—(ğ›¼, ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›) ğ‘‘ğ›¼ğ‘‘ğœ2
= âˆ«
âˆ
0
(âˆ«
âˆ
âˆ’âˆ
1
(ğœ2)(ğ‘›+2)/2 exp (âˆ’SSE + ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼+ (ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥)2 + (ğ›½âˆ’
Ì‚ğ›½) âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
) ğ‘‘
= âˆ«
âˆ
0
ğ‘âˆ—(ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›) ğ‘‘ğœ2

6.1. BAYESIAN SIMPLE LINEAR REGRESSION
129
We first calculate the inside integral, which gives us the joint posterior distri-
bution of ğ›½and ğœ2
ğ‘âˆ—(ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›)
= âˆ«
âˆ
âˆ’âˆ
1
(ğœ2)(ğ‘›+2)/2 exp (âˆ’SSE + ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼+ (ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥)2 + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
) ğ‘‘ğ›¼
= âˆ«
âˆ
âˆ’âˆ
1
(ğœ2)(ğ‘›+2)/2 exp (âˆ’SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
) exp (âˆ’ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼+ (ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥)2
2ğœ2
) ğ‘‘ğ›¼
=
1
(ğœ2)(ğ‘›+2)/2 exp (âˆ’SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
) âˆ«
âˆ
âˆ’âˆ
exp (âˆ’ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼+ (ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥)2
2ğœ2
) ğ‘‘ğ›¼
Here,
exp (âˆ’ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼+ (ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥)2
2ğœ2
)
can be viewed as part of a normal distribution of ğ›¼, with mean
Ì‚ğ›¼âˆ’(ğ›½âˆ’Ì‚ğ›½) Ì„ğ‘¥, and
variance ğœ2/ğ‘›. Therefore, the integral from the last line above is proportional
to âˆšğœ2/ğ‘›. We get
ğ‘âˆ—(ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›) âˆ
1
(ğœ2)(ğ‘›+2)/2 exp (âˆ’SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
) Ã— âˆšğœ2
ğ‘›
âˆ
1
(ğœ2)(ğ‘›+1)/2 exp (âˆ’SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
)
We then integrate ğœ2 out to get the marginal distribution of ğ›½. Here we first
perform change of variable and set ğœ2 = 1
ğœ™. Then the integral becomes
ğ‘âˆ—(ğ›½| ğ‘¦1, â‹¯, ğ‘¦ğ‘›) âˆâˆ«
âˆ
0
1
(ğœ2)(ğ‘›+1)/2 exp (âˆ’SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
) ğ‘‘ğœ2
âˆâˆ«
âˆ
0
ğœ™
ğ‘›âˆ’3
2 exp (âˆ’SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2
ğœ™) ğ‘‘ğœ™
âˆ(SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2
)
âˆ’(ğ‘›âˆ’2)+1
2
âˆ«
âˆ
0
ğ‘ 
ğ‘›âˆ’3
2 ğ‘’âˆ’ğ‘ ğ‘‘ğ‘ 
Here we use another change of variable by setting ğ‘ = SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2
ğœ™,
and the fact that âˆ«
âˆ
0
ğ‘ (ğ‘›âˆ’3)/2ğ‘’âˆ’ğ‘ ğ‘‘ğ‘ gives us the Gamma function Î“(ğ‘›âˆ’2),
which is a constant.

130
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
We can rewrite the last line from above to obtain the marginal posterior dis-
tribution of ğ›½. This marginal distribution is the Studentâ€™s ğ‘¡-distribution with
degrees of freedom ğ‘›âˆ’2, center
Ì‚ğ›½, and scale parameter
Ì‚ğœ2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
ğ‘âˆ—(ğ›½| ğ‘¦1, â‹¯, ğ‘¦ğ‘›) âˆ[1 +
1
ğ‘›âˆ’2
(ğ›½âˆ’
Ì‚ğ›½)2
SSE
ğ‘›âˆ’2/(âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2)]
âˆ’(ğ‘›âˆ’2)+1
2
= [1 +
1
ğ‘›âˆ’2
(ğ›½âˆ’
Ì‚ğ›½)2
Ì‚ğœ2/(âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2)]
âˆ’(ğ‘›âˆ’2)+1
2
,
where
Ì‚ğœ2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 is exactly the square of the standard error of
Ì‚ğ›½from the
frequentist OLS model.
To summarize, under the reference prior, the marginal posterior distribution
of the slope of the Bayesian simple linear regression follows the Studentâ€™s ğ‘¡-
distribution
ğ›½| ğ‘¦1, â‹¯, ğ‘¦ğ‘›âˆ¼t (ğ‘›âˆ’2,
Ì‚ğ›½, (seğ›½)
2)
6.1.6
Marginal Posterior Distribution of ğ›¼
A similar approach will lead us to the marginal distribution of ğ›¼. We again
start from the joint posterior distribution
ğ‘âˆ—(ğ›¼, ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›) âˆ
1
(ğœ2)(ğ‘›+2)/2 exp (âˆ’SSE + ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼âˆ’(ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥)2 + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
)
This time we integrate ğ›½and ğœ2 out to get the marginal posterior distribution
of ğ›¼. We first compute the integral
ğ‘âˆ—(ğ›¼, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›) = âˆ«
âˆ
âˆ’âˆ
ğ‘âˆ—(ğ›¼, ğ›½, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›) ğ‘‘ğ›½
= âˆ«
âˆ
âˆ’âˆ
1
(ğœ2)(ğ‘›+2)/2 exp (âˆ’SSE + ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼+ (ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥)2 + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
) ğ‘‘ğ›½
Here we group the terms with ğ›½âˆ’
Ì‚ğ›½together, then complete the square so that

6.1. BAYESIAN SIMPLE LINEAR REGRESSION
131
we can treat is as part of a normal distribution function to simplify the integral
ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼+ (ğ›½âˆ’
Ì‚ğ›½) Ì„ğ‘¥)2 + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
=(ğ›½âˆ’
Ì‚ğ›½)2 (âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2) + 2ğ‘›Ì„ğ‘¥(ğ›¼âˆ’
Ì‚ğ›¼)(ğ›½âˆ’
Ì‚ğ›½) + ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼)2
= (âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2) [(ğ›½âˆ’
Ì‚ğ›½) +
ğ‘›Ì„ğ‘¥(ğ›¼âˆ’
Ì‚ğ›¼)
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2 ]
2
+ ğ‘›(ğ›¼âˆ’
Ì‚ğ›¼)2 [
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2 ]
= (âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2) [(ğ›½âˆ’
Ì‚ğ›½) +
ğ‘›Ì„ğ‘¥(ğ›¼âˆ’
Ì‚ğ›¼)
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2 ]
2
+
(ğ›¼âˆ’
Ì‚ğ›¼)2
1
ğ‘›+
Ì„ğ‘¥2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’Ì„ğ‘¥)2
When integrating, we can then view
exp â›
âœ
â
âˆ’âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2
2ğœ2
(ğ›½âˆ’
Ì‚ğ›½+
ğ‘›Ì„ğ‘¥(ğ›¼âˆ’
Ì‚ğ›¼)
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2 )
2
â
âŸ
â 
as part of a normal distribution function, and get
ğ‘âˆ—(ğ›¼, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›)
âˆ
1
(ğœ2)(ğ‘›+2)/2 exp â›
âœ
â
âˆ’
SSE + (ğ›¼âˆ’
Ì‚ğ›¼)2/( 1
ğ‘›+
Ì„ğ‘¥2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’Ì„ğ‘¥)2 )
2ğœ2
â
âŸ
â 
Ã— âˆ«
âˆ
âˆ’âˆ
exp â›
âœ
â
âˆ’âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2
2ğœ2
(ğ›½âˆ’
Ì‚ğ›½+
ğ‘›Ì„ğ‘¥(ğ›¼âˆ’
Ì‚ğ›¼)
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 + ğ‘›Ì„ğ‘¥2 )
2
â
âŸ
â 
ğ‘‘ğ›½
âˆ
1
(ğœ2)(ğ‘›+1)/2 exp â›
âœ
â
âˆ’
SSE + (ğ›¼âˆ’
Ì‚ğ›¼)2/( 1
ğ‘›+
Ì„ğ‘¥2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’Ì„ğ‘¥)2 )
2ğœ2
â
âŸ
â 
To get the marginal posterior distribution of ğ›¼, we again integrate ğœ2 out. using
the same change of variable ğœ2 = 1
ğœ™, and ğ‘ =
SSE + (ğ›¼âˆ’
Ì‚ğ›¼)2/( 1
ğ‘›+
Ì„ğ‘¥2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’Ì„ğ‘¥)2 )
2
ğœ™.

132
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
ğ‘âˆ—(ğ›¼| ğ‘¦1, â‹¯, ğ‘¦ğ‘›)
= âˆ«
âˆ
0
ğ‘âˆ—(ğ›¼, ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›) ğ‘‘ğœ2
âˆâˆ«
âˆ
0
ğœ™(ğ‘›âˆ’3)/2 exp â›
âœ
â
âˆ’
SSE + (ğ›¼âˆ’
Ì‚ğ›¼)2/( 1
ğ‘›+
Ì„ğ‘¥2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’Ì„ğ‘¥)2 )
2
ğœ™â
âŸ
â 
ğ‘‘ğœ™
âˆ(SSE + (ğ›¼âˆ’
Ì‚ğ›¼)2/( 1
ğ‘›+
Ì„ğ‘¥2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 ))
âˆ’(ğ‘›âˆ’2)+1
2
âˆ«
âˆ
0
ğ‘ (ğ‘›âˆ’3)/2ğ‘’âˆ’ğ‘ ğ‘‘ğ‘ 
âˆâ¡â¢
â£
1 +
1
ğ‘›âˆ’2
(ğ›¼âˆ’
Ì‚ğ›¼)2
SSE
ğ‘›âˆ’2 ( 1
ğ‘›+
Ì„ğ‘¥2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’Ì„ğ‘¥)2 )
â¤â¥
â¦
âˆ’(ğ‘›âˆ’2)+1
2
= [1 +
1
ğ‘›âˆ’2 (ğ›¼âˆ’
Ì‚ğ›¼
seğ›¼
)
2
]
âˆ’(ğ‘›âˆ’2)+1
2
In the last line, we use the same trick as we did for ğ›½to derive the form of the
Studentâ€™s ğ‘¡-distribution. This shows that the marginal posterior distribution
of ğ›¼also follows a Studentâ€™s ğ‘¡-distribution, with ğ‘›âˆ’2 degrees of freedom. Its
center is
Ì‚ğ›¼, the estimate of ğ›¼in the frequentist OLS estimate, and its scale
parameter is
Ì‚ğœ2 ( 1
ğ‘›+
Ì„ğ‘¥2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 ), which is the square of the standard error
of
Ì‚ğ›¼.
6.1.7
Marginal Posterior Distribution of ğœ2
To show that the marginal posterior distribution of ğœ2 follows the inverse
Gamma distribution, we only need to show the precision ğœ™=
1
ğœ2 follows a
Gamma distribution.
We have shown in Week 3 that taking the prior distribution of ğœ2 proportional
to 1
ğœ2 is equivalent to taking the prior distribution of ğœ™proportional to 1
ğœ™
ğ‘(ğœ2) âˆ1
ğœ2
âŸ¹
ğ‘(ğœ™) âˆ1
ğœ™
Therefore, under the parameters ğ›¼, ğ›½, and the precision ğœ™, we have the joint
prior distribution as
ğ‘(ğ›¼, ğ›½, ğœ™) âˆ1
ğœ™
and the joint posterior distribution as
ğ‘âˆ—(ğ›¼, ğ›½, ğœ™| ğ‘¦1, â‹¯, ğ‘¦ğ‘›) âˆğœ™
ğ‘›
2 âˆ’1 exp (âˆ’âˆ‘ğ‘–(ğ‘¦ğ‘–âˆ’ğ›¼âˆ’ğ›½ğ‘¥ğ‘–)
2
ğœ™)

6.1. BAYESIAN SIMPLE LINEAR REGRESSION
133
Using the partial results we have calculated previously, we get
ğ‘âˆ—(ğ›½, ğœ™| ğ‘¦1, â‹¯, ğ‘¦ğ‘›) = âˆ«
âˆ
âˆ’âˆ
ğ‘âˆ—(ğ›¼, ğ›½, ğœ™| ğ‘¦1, â‹¯, ğ‘¦ğ‘›) ğ‘‘ğ›¼âˆğœ™
ğ‘›âˆ’3
2 exp (âˆ’SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2
ğœ™)
Intergrating over ğ›½, we finally have
ğ‘âˆ—(ğœ™| ğ‘¦1, â‹¯, ğ‘¦ğ‘›)
âˆâˆ«
âˆ
âˆ’âˆ
ğœ™
ğ‘›âˆ’3
2 exp (âˆ’SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2
ğœ™) ğ‘‘ğ›½
=ğœ™
ğ‘›âˆ’3
2 exp (âˆ’SSE
2 ğœ™) âˆ«
âˆ
âˆ’âˆ
exp (âˆ’(ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2
ğœ™) ğ‘‘ğ›½
âˆğœ™
ğ‘›âˆ’4
2 exp (âˆ’SSE
2 ğœ™) = ğœ™
ğ‘›âˆ’2
2 âˆ’1 exp (âˆ’SSE
2 ğœ™) .
This is a Gamma distribution with shape parameter ğ‘›âˆ’2
2
and rate parameter
SSE
2 . Therefore, the updated ğœ2 follows the inverse Gamma distribution
ğœ™= 1/ğœ2 | ğ‘¦1, â‹¯, ğ‘¦ğ‘›âˆ¼Gamma (ğ‘›âˆ’2
2
, SSE
2 ) .
That is,
ğ‘(ğœ™| data) âˆğœ™
ğ‘›âˆ’2
2 âˆ’1 exp (âˆ’SSE
2 ğœ™) .
6.1.8
Joint Normal-Gamma Posterior Distributions
Recall that the joint posterior distribution of ğ›½and ğœ2 is
ğ‘âˆ—(ğ›½, ğœ2 | data) âˆ
1
ğœğ‘›+1 exp (âˆ’SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2
2ğœ2
) .
If we rewrite this using precision ğœ™= 1/ğœ2, we get the joint posterior distribution
of ğ›½and ğœ™to be
ğ‘âˆ—(ğ›½, ğœ™| data) âˆğœ™
ğ‘›âˆ’2
2 exp (âˆ’ğœ™
2 (SSE + (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2)) .
This joint posterior distribution can be viewed as the product of the posterior
distribution of ğ›½conditioning on ğœ™and the posterior distribution of ğœ™,
ğœ‹âˆ—(ğ›½| ğœ™, data)Ã—ğœ‹âˆ—(ğœ™| data) âˆ[ğœ™exp (âˆ’ğœ™
2 (ğ›½âˆ’
Ì‚ğ›½)2 âˆ‘
ğ‘–
(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2)]Ã—[ğœ™
ğ‘›âˆ’2
2 âˆ’1 exp (âˆ’SSE
2 ğœ™)] .

134
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
The first term in the product is exactly the Normal distribution with mean
Ì‚ğ›½
and standard deviation
ğœ2
âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’
Ì„ğ‘¥)2 = ğœ2
Sğ‘¥ğ‘¥
ğ›½| ğœ2, data âˆ¼Normal ( Ì‚ğ›½,
ğœ2
Sğ‘¥ğ‘¥
) .
The second term is the Gamma distribution of the precision ğœ™, or the inverse
Gamma distribution of the variance ğœ2
1/ğœ2 | data âˆ¼Gamma (ğ‘›âˆ’2
2
, SSE
2 ) .
This means, the joint posterior distribution of ğ›½and ğœ2, under the reference
prior, is a Normal-Gamma distribution. Similarly, the joint posterior distribu-
tion of ğ›¼and ğœ2 is also a Normal-Gamma distribution.
ğ›¼| ğœ2, data âˆ¼Normal ( Ì‚ğ›¼, ğœ2 ( 1
ğ‘›+
Ì„ğ‘¥2
Sğ‘¥ğ‘¥
)) ,
1/ğœ2 | data âˆ¼Gamma (ğ‘›âˆ’2
2
, SSE
2 ) .
In fact, when we impose the bivariate normal distribution on ğ›½= (ğ›¼, ğ›½)ğ‘‡, and
the inverse Gamma distribution on ğœ2, as we have discussed in Section 6.1.3, the
joint posterior distribution of ğ›½and ğœ2 is a Normal-Gamma distribution. Since
the reference prior is just the limiting case of this informative prior, it is not
surprising that we will also get the limiting case Normal-Gamma distribution
for ğ›¼, ğ›½, and ğœ2.
6.2
Checking Outliers
The plot and predictive intervals suggest that predictions for Case 39 are not
well captured by the model. There is always the possibility that this case does
not meet the assumptions of the simple linear regression model (wrong mean or
variance) or could be in error. Model diagnostics such as plots of residuals versus
fitted values are useful in identifying potential outliers. Now with the interpre-
tation of Bayesian paradigm, we can go further to calculate the probability to
demonstrate whether a case falls too far from the mean.
The article by Chaloner and Brant (1988) suggested an approach for defining
outliers and then calculating the probability that a case or multiple cases were
outliers, based on the posterior information of all observations. The assumed
model for our simple linear regression is ğ‘¦ğ‘–= ğ›¼+ ğ›½ğ‘¥ğ‘–+ ğœ–ğ‘–, with ğœ–ğ‘–having inde-
pendent, identical distributions that are normal with mean zero and constant
variance ğœ2, i.e., ğœ–ğ‘–
iid
âˆ¼Normal(0, ğœ2). Chaloner & Brant considered outliers to
be points where the error or the model discrepancy ğœ–ğ‘–is greater than ğ‘˜stan-
dard deviations for some large ğ‘˜, and then proceed to calculate the posterior
probability that a case ğ‘—is an outlier to be
ğ‘ƒ(|ğœ–ğ‘—| > ğ‘˜ğœ| data)
(6.2)

6.2. CHECKING OUTLIERS
135
Since ğœ–ğ‘—= ğ‘¦ğ‘—âˆ’ğ›¼âˆ’ğ›½ğ‘¥ğ‘—, this is equivalent to calculating
ğ‘ƒ(|ğ‘¦ğ‘—âˆ’ğ›¼âˆ’ğ›½ğ‘¥ğ‘—| > ğ‘˜ğœ| data).
6.2.1
Posterior Distribution of ğœ–ğ‘—Conditioning On ğœ2
At the end of Section 6.1, we have discussed the posterior distributions of ğ›¼and
ğ›½. It turns out that under the reference prior, both posterior distributions of ğ›¼
and ğ›½, conditioning on ğœ2, are both normal
ğ›¼| ğœ2, data âˆ¼Normal ( Ì‚ğ›¼, ğœ2 ( 1
ğ‘›+
Ì„ğ‘¥2
Sğ‘¥ğ‘¥
)) ,
ğ›½| ğœ2, data âˆ¼Normal ( Ì‚ğ›½, ğœ2
Sğ‘¥ğ‘¥
) .
Using this information, we can obtain the posterior distribution of any residual
ğœ–ğ‘—= ğ‘¦ğ‘—âˆ’ğ›¼âˆ’ğ›½ğ‘¥ğ‘—conditioning on ğœ2
ğœ–ğ‘—| ğœ2, data âˆ¼Normal (ğ‘¦ğ‘—âˆ’
Ì‚ğ›¼âˆ’
Ì‚ğ›½ğ‘¥ğ‘—, ğœ2 âˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘—)2
ğ‘›Sğ‘¥ğ‘¥
) .
(6.3)
Since
Ì‚ğ›¼+ Ì‚ğ›½ğ‘¥ğ‘—is exactly the fitted value Ì‚ğ‘¦ğ‘—, the mean of this Normal distribution is
ğ‘¦ğ‘—âˆ’Ì‚ğ‘¦ğ‘—=
Ì‚ğœ–ğ‘—, which is the residual under the OLS estimates of the ğ‘—th observation.
Using this posterior distribution and the property of conditional probability,
we can calculate the probability that the error ğœ–ğ‘—lies outside of ğ‘˜standard
deviations of the mean, defined in equation (6.2)
ğ‘ƒ(|ğœ–ğ‘—| > ğ‘˜ğœ| data) = âˆ«
âˆ
0
ğ‘ƒ(|ğœ–ğ‘—| > ğ‘˜ğœ| ğœ2, data)ğ‘(ğœ2 | data) ğ‘‘ğœ2.
(6.4)
The probability ğ‘ƒ(|ğœ–ğ‘—| > ğ‘˜ğœ| ğœ2, data) can be calculated using the posterior
distribution of ğœ–ğ‘—conditioning on ğœ2 (6.3)
ğ‘ƒ(|ğœ–ğ‘—| > ğ‘˜ğœ| ğœ2, data) = âˆ«
|ğœ–ğ‘—|>ğ‘˜ğœ
ğ‘(ğœ–ğ‘—| ğœ2, data) ğ‘‘ğœ–ğ‘—= âˆ«
âˆ
ğ‘˜ğœ
ğ‘(ğœ–ğ‘—| ğœ2, data) ğ‘‘ğœ–ğ‘—+âˆ«
âˆ’ğ‘˜ğœ
âˆ’âˆ
ğ‘(ğœ–ğ‘—| ğœ2, data) ğ‘‘ğœ–ğ‘—.
Recall that ğ‘(ğœ–ğ‘—| ğœ2, data) is just a Normal distribution with mean
Ì‚ğœ–ğ‘—, standard
deviation ğ‘ = ğœâˆšâˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘—)2
ğ‘›Sğ‘¥ğ‘¥
, we can use the ğ‘§-score and ğ‘§-table to look for
this number. Let
ğ‘§âˆ—= ğœ–ğ‘—âˆ’Ì‚ğœ–ğ‘—
ğ‘ 
.

136
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
The first integral âˆ«
âˆ
ğ‘˜ğœ
ğ‘(ğœ–ğ‘—| ğœ2, data) ğ‘‘ğœ–ğ‘—is equivalent to the probability
ğ‘ƒ(ğ‘§âˆ—> ğ‘˜ğœâˆ’Ì‚ğœ–ğ‘—
ğ‘ 
) = ğ‘ƒâ›
âœ
âœ
â
ğ‘§âˆ—>
ğ‘˜ğœâˆ’Ì‚ğœ–ğ‘—
ğœâˆšâˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘—)2/Sğ‘¥ğ‘¥
â
âŸ
âŸ
â 
= ğ‘ƒâ›
âœ
âœ
â
ğ‘§âˆ—>
ğ‘˜âˆ’Ì‚ğœ–ğ‘—/ğœ
âˆšâˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘—)2/Sğ‘¥ğ‘¥
â
âŸ
âŸ
â 
.
That is the upper tail of the area under the standard Normal distribution when
ğ‘§âˆ—is larger than the critical value
ğ‘˜âˆ’Ì‚ğœ–ğ‘—/ğœ
âˆšâˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘—)2/Sğ‘¥ğ‘¥
.
The second integral, âˆ«
âˆ’ğ‘˜ğœ
âˆ’âˆ
ğ‘(ğœ–ğ‘—| ğœ2, data) ğ‘‘ğœ–ğ‘—, is the same as the probability
ğ‘ƒâ›
âœ
âœ
â
ğ‘§âˆ—<
âˆ’ğ‘˜âˆ’Ì‚ğœ–ğ‘—/ğœ
âˆšâˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘—)2/Sğ‘¥ğ‘¥
â
âŸ
âŸ
â 
,
which is the lower tail of the area under the standard Normal distribution when
ğ‘§âˆ—is smaller than the critical value
âˆ’ğ‘˜âˆ’Ì‚ğœ–ğ‘—/ğœ
âˆšâˆ‘ğ‘–(ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘—)2/Sğ‘¥ğ‘¥
.
After obtaining the two probabilities, we can move on to calculate the proba-
bility ğ‘ƒ(|ğœ–ğ‘—| > ğ‘˜ğœ| data) using the formula given by (6.4). Since manual calcu-
lation is complicated, we often use numerical integration functions provided in
R to finish the final integral.
6.2.2
Implementation Using BAS Package
The code for calculating the probability of outliers involves integration. We
have implemented this in the function Bayes.outlier from the BAS package.
This function takes an lm object and the value of k as arguments. Applying
this to the bodyfat data for Case 39, we get
# Load `BAS` library and data. Run linear regression as in Section 6.1
library(BAS)
data(bodyfat)
bodyfat.lm = lm(Bodyfat ~ Abdomen, data = bodyfat)
#
outliers = Bayes.outlier(bodyfat.lm, k=3)
# Extract the probability that Case 39 is an outlier
prob.39 = outliers$prob.outlier[39]
prob.39
## [1] 0.9916833

6.2. CHECKING OUTLIERS
137
We see that this case has an extremely high probability of 0.992 of being more
an outlier, that is, the error is greater than ğ‘˜= 3 standard deviations, based on
the fitted model and data.
With ğ‘˜= 3, however, there may be a high probability a priori of at least one out-
lier in a large sample. Let ğ‘= ğ‘ƒ(any error ğœ–ğ‘—lies within 3 standard deviations) =
ğ‘ƒ(observation ğ‘—is not a outlier).
Since we assume the prior distribution of
ğœ–ğ‘—is normal, we can calculate ğ‘using the pnorm function. Let Î¦(ğ‘§) be the
cumulative distribution of the standard Normal distribution, that is,
Î¦(ğ‘§) = âˆ«
ğ‘§
âˆ’âˆ
1
âˆš
2ğœ‹
exp (âˆ’ğ‘¥2
2 ) ğ‘‘ğ‘¥.
Then ğ‘= 1 âˆ’2Î¦(âˆ’ğ‘˜) = 1 âˆ’2Î¦(âˆ’3).2 Since we assume ğœ–ğ‘—is independent, that
the probability of no outlier is just the ğ‘›th power of ğ‘. The event of getting at
least 1 outlier is the complement of the event of getting no outliers. Therefore,
the probability of getting at least 1 outlier is
ğ‘ƒ(at least 1 outlier) = 1 âˆ’ğ‘ƒ(no outlier) = 1 âˆ’ğ‘ğ‘›= 1 âˆ’(1 âˆ’2Î¦(âˆ’3))ğ‘›.
We can compute this in R using
n = nrow(bodyfat)
# probability of no outliers if outliers have errors greater than 3 standard deviation
prob = (1 - (2 * pnorm(-3))) ^ n
prob
## [1] 0.5059747
# probability of at least one outlier
prob.least1 = 1 - (1 - (2 * pnorm(-3))) ^ n
prob.least1
## [1] 0.4940253
With ğ‘›= 252, the probability of at least one outlier is much larger than say the
marginal probability that one point is an outlier of 0.05. So we would expect
that there will be at least one point where the error is more than 3 standard
deviations from zero almost 50% of the time. Rather than fixing ğ‘˜, we can fix
the prior probability of no outliers ğ‘ƒ(no outlier) = 1 âˆ’ğ‘ğ‘›to be say 0.95, and
back solve the value of ğ‘˜using the qnorm function
new_k = qnorm(0.5 + 0.5 * 0.95 ^ (1 / n))
new_k
## [1] 3.714602
2Î¦(âˆ’ğ‘˜) actually represents the area of the lower tail under the standard Normal distribution
curve ğ‘˜standard deviations away from the mean 0.

138
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
This leads to a larger value of ğ‘˜. After adjusting ğ‘˜the prior probability of no
outliers is 0.95, we examine Case 39 again under this ğ‘˜
# Calculate probability of being outliers using new `k` value
outliers.new = Bayes.outlier(bodyfat.lm, k = new_k)
# Extract the probability of Case 39
prob.new.39 = outliers.new$prob.outlier[39]
prob.new.39
## [1] 0.6847509
The posterior probability of Case 39 being an outlier is about 0.685. While this
is not strikingly large, it is much larger than the marginal prior probability of
for a value lying about 3.7ğœaway from 0, if we assume the error ğœ–ğ‘—is normally
distributed with mean 0 and variance ğœ2.
2 * pnorm(-new_k)
## [1] 0.0002035241
There is a substantial probability that Case 39 is an outlier. If you do view it
as an outlier, what are your options? One option is to investigate the case and
determine if the data are input incorrectly, and fix it. Another option is when
you cannot confirm there is a data entry error, you may delete the observation
from the analysis and refit the model without the case. If you do take this
option, be sure to describe what you did so that your research is reproducible.
You may want to apply diagnostics and calculate the probability of a case being
an outlier using this reduced data. As a word of caution, if you discover that
there are a large number of points that appear to be outliers, take a second look
at your model assumptions, since the problem may be with the model rather
than the data! A third option we will talk about later, is to combine inference
under the model that retains this case as part of the population, and the model
that treats it as coming from another population. This approach incorporates
our uncertainty about whether the case is an outlier given the data.
The code of Bayes.outlier function is based on using a reference prior for
the linear model and extends to multiple regression.
6.3
Bayesian Multiple Linear Regression
In this section, we will discuss Bayesian inference in multiple linear regression.
We will use the reference prior to provide the default or base line analysis of
the model, which provides the correspondence between Bayesian and frequentist
approaches.

6.3. BAYESIAN MULTIPLE LINEAR REGRESSION
139
6.3.1
The Model
To illustrate the idea, we use the data set on kidâ€™s cognitive scores that we
examined earlier. We predicted the value of the kidâ€™s cognitive score from the
motherâ€™s high school status, motherâ€™s IQ score, whether or not the mother
worked during the first three years of the kidâ€™s life, and the motherâ€™s age. We
set up the model as follows
ğ‘¦score,ğ‘–= ğ›¼+ ğ›½1ğ‘¥hs,ğ‘–+ ğ›½2ğ‘¥IQ,ğ‘–+ ğ›½3ğ‘¥work,ğ‘–+ ğ›½4ğ‘¥age,ğ‘–+ ğœ–ğ‘–,
ğ‘–= 1, â‹¯, ğ‘›. (6.5)
Here, ğ‘¦score,ğ‘–is the ğ‘–th kidâ€™s cognitive score.
ğ‘¥hs,ğ‘–, ğ‘¥IQ,ğ‘–, ğ‘¥work,ğ‘–, and ğ‘¥age,ğ‘–
represent the high school status, the IQ score, the work status during the first
three years of the kidâ€™s life, and the age of the ğ‘–th kidâ€™s mother. ğœ–ğ‘–is the error
term. ğ‘›denotes the number of observations in this data set.
For better analyses, one usually centers the variable, which ends up getting the
following form
ğ‘¦score,ğ‘–= ğ›½0+ğ›½1(ğ‘¥hs,ğ‘–âˆ’Ì„ğ‘¥hs)+ğ›½2(ğ‘¥IQ,ğ‘–âˆ’Ì„ğ‘¥IQ)+ğ›½3(ğ‘¥work,ğ‘–âˆ’Ì„ğ‘¥work)+ğ›½4(ğ‘¥age,ğ‘–âˆ’Ì„ğ‘¥age)+ğœ–ğ‘–.
(6.6)
Under this tranformation, the coeï¬€icients, ğ›½1, ğ›½2, ğ›½3, ğ›½4, that are in front of the
variables, are unchanged compared to the ones in (6.5). However, the constant
coeï¬€icient ğ›½0 is no longer the constant coeï¬€icient ğ›¼in (6.5). Instead, under the
assumption that ğœ–ğ‘–is independently, identically normal,
Ì‚ğ›½0 is the sample mean
of the response variable ğ‘Œscore.3 This provides more meaning to ğ›½0 as this is
the mean of ğ‘Œwhen each of the predictors is equal to their respective means.
Moreover, it is more convenient to use this â€œcenteredâ€ model to derive analyses.
The R codes in the BAS package are based on the form (6.6).
6.3.2
Data Pre-processing
We can download the data set from Gelmanâ€™s website and read the summary
information of the data set using the read.dta function in the foreign package.
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")
summary(cognitive)
##
kid_score
mom_hs
mom_iq
mom_work
##
Min.
: 20.0
Min.
:0.0000
Min.
: 71.04
Min.
:1.000
##
1st Qu.: 74.0
1st Qu.:1.0000
1st Qu.: 88.66
1st Qu.:2.000
##
Median : 90.0
Median :1.0000
Median : 97.92
Median :3.000
3Under the normal assumption, the mean of the error is 0. Taking mean on both sides of
equation (6.6) immediately gives ğ›½0 =
Ì„ğ‘¦score.

140
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
##
Mean
: 86.8
Mean
:0.7857
Mean
:100.00
Mean
:2.896
##
3rd Qu.:102.0
3rd Qu.:1.0000
3rd Qu.:110.27
3rd Qu.:4.000
##
Max.
:144.0
Max.
:1.0000
Max.
:138.89
Max.
:4.000
##
mom_age
##
Min.
:17.00
##
1st Qu.:21.00
##
Median :23.00
##
Mean
:22.79
##
3rd Qu.:25.00
##
Max.
:29.00
From the summary statistics, variables mom_hs and mom_work should be con-
sidered as categorical variables.
We transform them into indicator variables
where mom_work = 1 if the mother worked for 1 or more years, and mom_hs =
1 indicates the mother had more than a high school education.
The code is as below:4
cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs = as.numeric(cognitive$mom_hs > 0)
# Modify column names of the data set
colnames(cognitive) = c("kid_score", "hs", "IQ", "work", "age")
6.3.3
Specify Bayesian Prior Distributions
For Bayesian inference, we need to specify a prior distribution for the error term
ğœ–ğ‘–. Since each kidâ€™s cognitive score ğ‘¦score,ğ‘–is continuous, we assume that ğœ–ğ‘–is
independent, and identically distributed with the Normal distribution
ğœ–ğ‘–
iid
âˆ¼Normal(0, ğœ2),
where ğœ2 is the commonly shared variance of all observations.
We will also need to specify the prior distributions for all the coeï¬€icients
ğ›½0, ğ›½1, ğ›½2, ğ›½3, and ğ›½4. An informative prior, which assumes that the ğ›½â€™s follow
the multivariate normal distribution with covariance matrix ğœ2Î£0 can be used.
We may further impose the inverse Gamma distribution to ğœ2, to complete the
hierachical model
ğ›½0, ğ›½1, ğ›½2, ğ›½3, ğ›½4 | ğœ2 âˆ¼Normal((ğ‘0, ğ‘1, ğ‘2, ğ‘3, ğ‘4)ğ‘‡, ğœ2Î£0)
1/ğœ2
âˆ¼Gamma(ğœˆ0/2, ğœˆ0ğœ2
0/2)
This gives us the multivariate Normal-Gamma conjugate family, with hyperpa-
rameters ğ‘0, ğ‘1, ğ‘2, ğ‘3, ğ‘4, Î£0, ğœˆ0, and ğœ2
0. For this prior, we will need to specify
4Note: as.numeric is not necessary here. We use as.numeric to keep the names of the
levels of the two variables short.

6.3. BAYESIAN MULTIPLE LINEAR REGRESSION
141
the values of all the hyperparameters. This elicitation can be quite involved,
especially when we do not have enough prior information about the variances,
covariances of the coeï¬€icients and other prior hyperparameters. Therefore, we
are going to adopt the noninformative reference prior, which is a limiting case
of this multivariate Normal-Gamma prior.
The reference prior in the multiple linear regression model is similar to the refer-
ence prior we used in the simple linear regression model. The prior distribution
of all the coeï¬€icients ğ›½â€™s conditioning on ğœ2 is the uniform prior, and the prior
of ğœ2 is proportional to its reciprocal
ğ‘(ğ›½0, ğ›½1, ğ›½2, ğ›½3, ğ›½4 | ğœ2) âˆ1,
ğ‘(ğœ2) âˆ1
ğœ2 .
Under this reference prior, the marginal posterior distributions of the coeï¬€i-
cients, ğ›½â€™s, are parallel to the ones in simple linear regression. The marginal
posterior distribution of ğ›½ğ‘—is the Studentâ€™s ğ‘¡-distributions with centers given by
the frequentist OLS estimates
Ì‚ğ›½ğ‘—, scale parameter given by the standard error
(seğ›½ğ‘—)2 obtained from the OLS estimates
ğ›½ğ‘—| ğ‘¦1, â‹¯, ğ‘¦ğ‘›âˆ¼t(ğ‘›âˆ’ğ‘âˆ’1,
Ì‚ğ›½ğ‘—, (seğ›½ğ‘—)2),
ğ‘—= 0, 1, â‹¯, ğ‘.
The degree of freedom of these ğ‘¡-distributions is ğ‘›âˆ’ğ‘âˆ’1, where ğ‘is the number
of predictor variables. In the kidâ€™s cognitive score example, ğ‘= 4. The posterior
mean,
Ì‚ğ›½ğ‘—, is the center of the ğ‘¡-distribution of ğ›½ğ‘—, which is the same as the OLS
estimates of ğ›½ğ‘—. The posterior standard deviation of ğ›½ğ‘—, which is the square
root of the scale parameter of the ğ‘¡-distribution, is seğ›½ğ‘—, the standard error of
ğ›½ğ‘—under the OLS estimates. That means, under the reference prior, we can
easily obtain the posterior mean and posterior standard deviation from using
the lm function, since they are numerically equivalent to the counterpart of the
frequentist approach.
6.3.4
Fitting the Bayesian Model
To gain more flexibility in choosing priors, we will instead use the bas.lm func-
tion in the BAS library, which allows us to specify different model priors and
coeï¬€icient priors.
# Import library
library(BAS)
# Use `bas.lm` to run regression model
cog.bas = bas.lm(kid_score ~ ., data = cognitive, prior = "BIC",
modelprior = Bernoulli(1),
include.always = ~ .,
n.models = 1)

142
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
The above bas.lm function uses the same model formula as in the lm. It first
specifies the response and predictor variables, a data argument to provide the
data frame. The additional arguments further include the prior on the coef-
ficients. We use "BIC" here to indicate that the model is based on the non-
informative reference prior. (We will explain in the later section why we use
the name "BIC".) Since we will only provide one model, which is the one that
includes all variables, we place all model prior probability to this exact model.
This is specified in the modelprior = Bernoulli(1) argument. Because we
want to fit using all variables, we use include.always = ~ . to indicate that
the intercept and all 4 predictors are included. The argument n.models = 1
fits just this one model.
6.3.5
Posterior Means and Posterior Standard Deviations
Similar to the OLS regression process, we can extract the posterior means and
standard deviations of the coeï¬€icients using the coef function
cog.coef = coef(cog.bas)
cog.coef
##
##
Marginal Posterior Summaries of Coefficients:
##
##
Using
BMA
##
##
Based on the top
1 models
##
post mean
post SD
post p(B != 0)
## Intercept
86.79724
0.87092
1.00000
## hs
5.09482
2.31450
1.00000
## IQ
0.56147
0.06064
1.00000
## work
2.53718
2.35067
1.00000
## age
0.21802
0.33074
1.00000
From the last column in this summary, we see that the probability of the co-
eï¬€icients to be non-zero is always 1. This is because we specify the argument
include.always = ~ . to force the model to include all variables. Notice on
the first row we have the statistics of the Intercept ğ›½0. The posterior mean
of ğ›½0 is 86.8, which is completely different from the original ğ‘¦-intercept of this
model under the frequentist OLS regression. As we have stated previously, we
consider the â€œcenteredâ€ model under the Bayesian framework. Under this â€œcen-
teredâ€ model and the reference prior, the posterior mean of the Intercept ğ›½0
is now the sample mean of the response variable ğ‘Œscore.
We can visualize the coeï¬€icients ğ›½1, ğ›½2, ğ›½3, ğ›½4 using the plot function. We
use the subset argument to plot only the coeï¬€icients of the predictors.
par(mfrow = c(2, 2), col.lab = "darkgrey", col.axis = "darkgrey", col = "darkgrey")
plot(cog.coef, subset = 2:5, ask = F)

6.3. BAYESIAN MULTIPLE LINEAR REGRESSION
143
0
5
10
0.0
0.6
hs
0.0
0.2
0.4
0.6
0.8
0.0
0.6
IQ
âˆ’5
0
5
10
0.0
0.6
work
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0
1.5
0.0
0.6
age
These distributions all center the posterior distributions at their respective OLS
estimates
Ì‚ğ›½ğ‘—, with the spread of the distribution related to the standard errors
seğ›½ğ‘—. Recall, that bas.lm uses centered predictors so that the intercept is always
the sample mean.
6.3.6
Credible Intervals Summary
We can also report the posterior means, posterior standard deviations, and
the 95% credible intervals of the coeï¬€icients of all 4 predictors, which may
give a clearer and more useful summary. The BAS library provides the method
confint to extract the credible intervals from the output cog.coef. If we are
only interested in the distributions of the coeï¬€icients of the 4 predictors, we
may use the parm argument to restrict the variables shown in the summary
confint(cog.coef, parm = 2:5)
##
2.5%
97.5%
beta
## hs
0.5456507 9.6439990 5.0948248
## IQ
0.4422784 0.6806616 0.5614700
## work -2.0830879 7.1574454 2.5371788
## age
-0.4320547 0.8680925 0.2180189
## attr(,"Probability")
## [1] 0.95
## attr(,"class")
## [1] "confint.bas"
All together, we can generate a summary table showing the posterior means,
posterior standard deviations, the upper and lower bounds of the 95% credible

144
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION
intervals of all coeï¬€icients ğ›½0, ğ›½1, ğ›½2, ğ›½3, and ğ›½4.
out = confint(cog.coef)[, 1:2]
# Extract the upper and lower bounds of the credible intervals
names = c("posterior mean", "posterior std", colnames(out))
out = cbind(cog.coef$postmean, cog.coef$postsd, out)
colnames(out) = names
round(out, 2)
##
posterior mean posterior std
2.5% 97.5%
## Intercept
86.80
0.87 85.09 88.51
## hs
5.09
2.31
0.55
9.64
## IQ
0.56
0.06
0.44
0.68
## work
2.54
2.35 -2.08
7.16
## age
0.22
0.33 -0.43
0.87
As in the simple linear aggression, the posterior estimates from the reference
prior, that are in the table, are equivalent to the numbers reported from
the lm function in R, or using the confident function in the OLS estimates.
These intervals are centered at the posterior mean
Ì‚ğ›½ğ‘—with width given by the
appropriate ğ‘¡quantile with ğ‘›âˆ’ğ‘âˆ’1 degrees of freedom times the posterior
standard deviation seğ›½ğ‘—.
The primary difference is the interpretation
of the intervals.
For example, given this data, we believe there is a 95%
chance that the kidâ€™s cognitive score increases by 0.44 to 0.68 with one additional
increase of the motherâ€™s IQ score. The motherâ€™s high school status has a larger
effect where we believe that there is a 95% chance the kid would score of 0.55
up to 9.64 points higher if the mother had three or more years of high school.
The credible intervals of the predictors work and age include 0, which implies
that we may improve this model so that the model will accomplish a desired
level of explanation or prediction with fewer predictors. We will explore model
selection using Bayesian information criterion in the next chapter.
6.4
Summary
We have provided Bayesian analyses for both simple linear regression and multi-
ple linear regression using the default reference prior. We have seen that, under
this reference prior, the marginal posterior distribution of the coeï¬€icients is the
Studentâ€™s ğ‘¡-distribution. Therefore, the posterior mean and posterior standard
deviation of any coeï¬€icients are numerically equivalent to the corresponding fre-
quentist OLS estimate and the standard error. This has provided us a base line
analysis of Bayesian approach, which we can extend later when we introduce
more different coeï¬€icient priors.
The difference is the interpretation. Since we have obtained the distribution of
each coeï¬€icient, we can construct the credible interval, which provides us the

6.4. SUMMARY
145
probability that a specific coeï¬€icient falls into this credible interval.
We have also used the posterior distribution to analyze the probability of a
particular observation being an outlier. We defined such probabiilty to be the
probability that the error term is ğ‘˜standard deviations away from 0.
This
probability is based on information of all data, instead of just the observation
itself.

146
CHAPTER 6. INTRODUCTION TO BAYESIAN REGRESSION

Chapter 7
Bayesian Model Choice
In Section 6.3 of Chapter 6, we provided a Bayesian inference analysis for kidâ€™s
cognitive scores using multiple linear regression. We found that several credible
intervals of the coeï¬€icients contain zero, suggesting that we could potentially
simplify the model.
In this chapter, we will discuss model selection, model
uncertainty, and model averaging. Bayesian model selection is to pick variables
for multiple linear regression based on Bayesian information criterion, or BIC.
Later, we will also discuss other model selection methods, such as using Bayes
factors.
7.1
Bayesian Information Criterion (BIC)
In inferential statistics, we compare model selections using ğ‘-values or adjusted
ğ‘…2. Here we will take the Bayesian propectives. We are going to discuss the
Bayesian model selections using the Bayesian information criterion, or BIC. BIC
is one of the Bayesian criteria used for Bayesian model selection, and tends to
be one of the most popular criteria.
7.1.1
Definition of BIC
The Bayesian information criterion, BIC, is defined to be
BIC = âˆ’2 ln(
Ì‚
likelihood) + (ğ‘+ 1) ln(ğ‘›).
(7.1)
Here ğ‘›is the number of observations in the model, and ğ‘is the number of
predictors. That is, ğ‘+ 1 is the number of total parameters (also the total
number of coeï¬€icients, including the intercept) in the model. Recall that in the
Bayesian simple linear regression (Section 6.1), we mentioned the likelihood of
147

148
CHAPTER 7. BAYESIAN MODEL CHOICE
the model ğ‘¦ğ‘–= ğ›¼+ ğ›½ğ‘¥ğ‘–+ ğœ–ğ‘–is the probability (or probability distribution) for
the observed data ğ‘¦ğ‘–, ğ‘–= 1, â‹¯, ğ‘›occur under the given parameters ğ›¼, ğ›½, ğœ2
likelihood = ğ‘(ğ‘¦ğ‘–| ğ›¼, ğ›½, ğœ2) = â„’(ğ›¼, ğ›½, ğœ2),
where ğœ2 is the variance of the assumed Normal distribution of the error term
ğœ–ğ‘–. In general, under any model ğ‘€, we can write the likelihood of this model as
the function of parameter ğœƒ(ğœƒmay be a vector of several parameters) and the
model ğ‘€
likelihood = ğ‘(data | ğœƒ, ğ‘€) = â„’(ğœƒ, ğ‘€).
If the likelihood function â„’(ğœƒ, ğ‘€) is nice enough (say it has local maximum), the
maximized value of the likelihood,
Ì‚
likelihood, can be achieved by some special
value of the parameter ğœƒ, denoted as
Ì‚ğœƒ
Ì‚
likelihood = ğ‘(data |
Ì‚ğœƒ, ğ‘€) = â„’( Ì‚ğœƒ, ğ‘€).
This is the likelihood that defines BIC.
When the sample size ğ‘›is large enough and the data distribution belongs to the
exponential family such as the Normal distribution, BIC can be approximated
by -2 times likelihood that data are produced under model ğ‘€:
BIC â‰ˆâˆ’2 ln(ğ‘(data | ğ‘€)) = âˆ’2 ln (âˆ«ğ‘(data | ğœƒ, ğ‘€)ğ‘(ğœƒ| ğ‘€) ğ‘‘ğœƒ) ,
when ğ‘›is large.
(7.2)
Here ğ‘(ğœƒ| ğ‘€) is the prior distribution of the parameter ğœƒ.
We will not go
into detail why the approximation holds and how we perform the integration
in this book. However, we wanted to remind readers that, since BIC can be
approximated by the prior distribution of the parameter ğœƒ, we will see later how
we utilize BIC to approximate the model likelihood under the reference prior.1
One more observation of formula (7.2) is that it involves the marginal likelihood
of data under model ğ‘€, ğ‘(data | ğ‘€). We have seen this quantity when we
introduced Bayes factor between two hypotheses or models
BF[ğ‘€1 âˆ¶ğ‘€2] = ğ‘(data | ğ‘€1)
ğ‘(data | ğ‘€2).
This also provides connection between BIC and Bayes factor, which we will
leverage later.
Similar to AIC, the Akaike information criterion, the model with the smallest
BIC is preferrable. Formula (7.1) can be re-expressed using the model ğ‘…2, which
is easier to calculate
BIC = ğ‘›ln(1 âˆ’ğ‘…2) + (ğ‘+ 1) ln(ğ‘›) + constant,
(7.3)
1Recall that the reference prior is the limiting case of the multivariate Normal-Gamma
distribution.

7.1. BAYESIAN INFORMATION CRITERION (BIC)
149
where the last term constant only depends on the sample size ğ‘›, and the observed
data ğ‘¦1, â‹¯, ğ‘¦ğ‘›. Since this constant does not depend on the choice of model, i.e.,
the choice of variables, ignoring this constant will not affect the comparison of
BICs between models. Therefore, we usually define BIC to be
BIC = ğ‘›ln(1 âˆ’ğ‘…2) + (ğ‘+ 1) ln(ğ‘›).
From this expression, we see that adding more predictors, that is, increasing
ğ‘, will result in larger ğ‘…2, which leads to a smaller ln(1 âˆ’ğ‘…2) in the first
term of BIC. While larger ğ‘…2 means better goodness of fit of the data, too
many predictors may result in overfitting the data. Therefore, the second term
(ğ‘+ 1) ln(ğ‘›) is added in the BIC expression to penalize models with too many
predictors. When ğ‘increases, the second term increases as well. This provides
a trade-off between the goodness of fit given by the first term and the model
complexity represented by the second term.
7.1.2
Backward Elimination with BIC
We will use the kidâ€™s cognitive score data set cognitive as an example. We
first read in the data set from Gelmanâ€™s website and transform the data types
of the two variables mom_work and mom_hs, like what we did in Section 6.3.
# Load the library in order to read in data from website
library(foreign)
# Read in cognitive score data set and process data tranformations
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")
cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =
as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","IQ", "work", "age")
We start with the full model, with all possible predictors: hs, IQ, work, and age.
We will drop one variable at a time and record all BICs. Then we will choose
the model with the smallest BIC. We will repeat this process until none of the
models yields a decrease in BIC. We use the step function in R to perform
the BIC model selection. Notice the default value of the k argument in the
step function is k=2, which is for the AIC score. For BIC, k should be log(n)
correspondingly.
# Compute the total number of observations
n = nrow(cognitive)
# Full model using all predictors
cog.lm = lm(kid_score ~ ., data=cognitive)
# Perform BIC elimination from full model

150
CHAPTER 7. BAYESIAN MODEL CHOICE
# k = log(n): penalty for BIC rather than AIC
cog.step = step(cog.lm, k=log(n))
## Start:
AIC=2541.07
## kid_score ~ hs + IQ + work + age
##
##
Df Sum of Sq
RSS
AIC
## - age
1
143.0 141365 2535.4
## - work
1
383.5 141605 2536.2
## - hs
1
1595.1 142817 2539.9
## <none>
141222 2541.1
## - IQ
1
28219.9 169441 2614.1
##
## Step:
AIC=2535.44
## kid_score ~ hs + IQ + work
##
##
Df Sum of Sq
RSS
AIC
## - work
1
392.5 141757 2530.6
## - hs
1
1845.7 143210 2535.0
## <none>
141365 2535.4
## - IQ
1
28381.9 169747 2608.8
##
## Step:
AIC=2530.57
## kid_score ~ hs + IQ
##
##
Df Sum of Sq
RSS
AIC
## <none>
141757 2530.6
## - hs
1
2380.2 144137 2531.7
## - IQ
1
28504.1 170261 2604.0
In the summary chart, the AIC should be interpreted as BIC, since we have
chosen to use the BIC expression where ğ‘˜= ln(ğ‘›).
From the full model, we predict the kidâ€™s cognitive score from motherâ€™s high
school status, motherâ€™s IQ score, motherâ€™s work status and motherâ€™s age. The
BIC for the full model is 2541.1.
At the first step, we try to remove each variable from the full model to record the
resulting new BIC. From the summary statistics, we see that removing variable
age results in the smallest BIC. But if we try to drop the IQ variable, this will
increase the BIC, which implies that IQ would be a really important predictor
of kid_score. Comparing all the results, we drop the age variable at the first
step. After dropping age, the new BIC is 2535.4.
At the next step, we see that dropping work variable will result in the lowest
BIC, which is 2530.6. Now the model has become
score âˆ¼hs + IQ

7.1. BAYESIAN INFORMATION CRITERION (BIC)
151
Finally, when we try dropping either hs or IQ, it will result in higher BIC than
2530.6. This suggests that we have reached the best model. This model predicts
kidâ€™s cognitive score using motherâ€™s high school status and motherâ€™s IQ score.
However, using the adjusted ğ‘…2, the best model would be the one including not
only hs and IQ variables, but also motherâ€™s work status, work. In general, using
BIC leads to fewer variables for the best model compared to using adjusted ğ‘…2
or AIC.
We can also use the BAS package to find the best BIC model without taking the
stepwise backward process.
# Import library
library(BAS)
# Use `bas.lm` to run regression model
cog.BIC = bas.lm(kid_score ~ ., data = cognitive,
prior = "BIC", modelprior = uniform())
cog.BIC
##
## Call:
## bas.lm(formula = kid_score ~ ., data = cognitive, prior = "BIC",
##
modelprior = uniform())
##
##
##
Marginal Posterior Inclusion Probabilities:
## Intercept
hs
IQ
work
age
##
1.00000
0.61064
1.00000
0.11210
0.06898
Here we set the modelprior argument as uniform() to assign equal prior prob-
ability for each possible model.
The logmarg information inside the cog.BIC summary list records the log of
marginal likelihood of each model after seeing the data ln(ğ‘(data | ğ‘€)). Recall
that this is approximately proportional to negative BIC when the sample size
ğ‘›is large
BIC â‰ˆâˆ’2 ln(ğ‘(data | ğ‘€)).
We can use this information to retrieve the model with the largest log of marginal
likelihood, which corresponds to the model with the smallest BIC.
# Find the index of the model with the largest logmarg
best = which.max(cog.BIC$logmarg)
# Retrieve the index of variables in the best model, with 0 as the index of the intercept
bestmodel = cog.BIC$which[[best]]
bestmodel

152
CHAPTER 7. BAYESIAN MODEL CHOICE
## [1] 0 1 2
# Create an indicator vector indicating which variables are used in the best model
# First, create a 0 vector with the same dimension of the number of variables in the fu
bestgamma = rep(0, cog.BIC$n.vars)
# Change the indicator to 1 where variables are used
bestgamma[bestmodel + 1] = 1
bestgamma
## [1] 1 1 1 0 0
From the indicator vector bestgamma we see that only the intercept (indexed
as 0), motherâ€™s high school status variable hs (indexed as 1), and motherâ€™s IQ
score IQ (indexed as 2) are used in the best model, with 1â€™s in the corresponding
slots of the 5-dimensional vector (1, 1, 1, 0, 0).
7.1.3
Coeï¬€icient Estimates Under Reference Prior for
Best BIC Model
The best BIC model ğ‘€can be set up as follows and we have adopted the
â€œcenteredâ€ model convention for convenient analyses
ğ‘¦score,ğ‘–= ğ›½0 + ğ›½1(ğ‘¥hs,ğ‘–âˆ’
Ì„ğ‘¥hs,ğ‘–) + ğ›½2(ğ‘¥IQ,ğ‘–âˆ’
Ì„ğ‘¥IQ) + ğœ–ğ‘–,
ğ‘–= 1, â‹¯, ğ‘›
We would like to get the posterior distributions of the coeï¬€icients ğ›½0, ğ›½1, and
ğ›½2 under this model. Recall that the reference prior imposes a uniformly flat
prior distribution on coeï¬€icients ğ‘(ğ›½0, ğ›½1, ğ›½2 | ğ‘€) âˆ1 and that ğ‘(ğœ2 | ğ‘€) âˆ
1/ğœ2, so together the joint prior distribution ğ‘(ğ›½0, ğ›½1, ğ›½2, ğœ2 | ğ‘€) is propor-
tional to 1/ğœ2.
When the sample size ğ‘›is large, any proper prior distribu-
tion ğ‘(ğ›½0, ğ›½1, ğ›½2, ğœ2 | ğ‘€) is getting flatter and flatter, which can be approxi-
mated by the reference prior. At the same time, the log of marginal likelihood
ln(ğ‘(data | ğ‘€)) can be approximated by the BIC. Therefore, we use prior =
"BIC" in the bas.lm function when we use the BIC as an approximation of the
log of marginal likelihood under the reference prior. The posterior mean of ğ›½0
in the result is the sample mean of the kidsâ€™ cognitive scores, or
Ì„ğ‘Œscore, since we
have centered the model.
# Fit the best BIC model by imposing which variables to be used using the indicators
cog.bestBIC = bas.lm(kid_score ~ ., data = cognitive,
prior = "BIC", n.models = 1,
# We only fit 1 model
bestmodel = bestgamma,
# We use bestgamma to indicate variables
modelprior = uniform())
# Retrieve coefficients information
cog.coef = coef(cog.bestBIC)

7.1. BAYESIAN INFORMATION CRITERION (BIC)
153
# Retrieve bounds of credible intervals
out = confint(cog.coef)[, 1:2]
# Combine results and construct summary table
coef.BIC = cbind(cog.coef$postmean, cog.coef$postsd, out)
names = c("post mean", "post sd", colnames(out))
colnames(coef.BIC) = names
coef.BIC
##
post mean
post sd
2.5%
97.5%
## Intercept 86.797235 0.87054033 85.0862025 88.5082675
## hs
5.950117 2.21181218
1.6028370 10.2973969
## IQ
0.563906 0.06057408
0.4448487
0.6829634
## work
0.000000 0.00000000
0.0000000
0.0000000
## age
0.000000 0.00000000
0.0000000
0.0000000
Comparing the coeï¬€icients in the best model with the ones in the full model
(which can be found in Section 6.3), we see that the 95% credible interval for IQ
variable is the same. However, the credible interval for high school status hs has
shifted slightly to the right, and it is also slighly narrower, meaning a smaller
posterior standard deviation.
All credible intervals of coeï¬€icients exclude 0,
suggesting that we have found a parsimonious model.2
7.1.4
Other Criteria
BIC is one of the criteria based on penalized likelihoods. Other examples such
as AIC (Akaike information criterion) or adjusted ğ‘…2, employ the form of
âˆ’2 ln(
Ì‚
likelihood) + (ğ‘+ 1) Ã— some constant,
where ğ‘is the number of predictor variables and â€œsome constantâ€ is a con-
stant value depending on different criteria. BIC tends to select parsimonious
models (with fewer predictor variables) while AIC and adjusted ğ‘…2 may include
variables that are not statistically significant, but may do better for predictions.
Other Bayesian model selection decisions may be based on selecting models with
the highest posterior probability. If predictions are important, we can use deci-
sion theory to help pick the model with the smallest expected prediction error.
In addiciton to goodness of fit and parsimony, loss functions that include costs
associated with collecting variables for predictive models may be of important
consideration.
2A parsimonious model is a model that accomplishes a desired level of explanation or
prediction with as few predictor variables as possible. More discussion of parsimonious models
can be found in Course 3 Linear Regression and Modeling.

154
CHAPTER 7. BAYESIAN MODEL CHOICE
7.2
Bayesian Model Uncertainty
In the last section, we discussed how to use Bayesian Information Criterion
(BIC) to pick the best model, and we demonstrated the method on the kidâ€™s
cognitive score data set.
However, we may often have several models with
similar BIC. If we only pick the one with the lowest BIC, we may ignore the
presence of other models that are equally good or can provide useful information.
The credible intervals of coeï¬€icients may be narrower since the uncertainty is
being ignored when we consider only one model. Narrower intervals are not
always better if they miss the true values of the parameters. To account for the
uncertainty, getting the posterior probability of all possible models is necessary.
In this section, we will talk about how to convert BIC into Bayes factor to
find the posterior probability of all possible models. We will again use the BAS
package in R to achieve this goal.
7.2.1
Model Uncertainty
When forecasting the path of a hurricane, having an accurate prediction and
measurement of uncertainty is important for early warning. In this case, we
would consider the probability of several potential paths that the hurricane may
make landfall. Similar to hurricane forecasting, we would also like to obtain the
posterior probability of all possible models for uncertainty measurement.
To represent model uncertainty, we need to construct a probability distribution
over all possible models where the each probability provides measure of how
likely the model is to happen.
Suppose we have a multiple linear regression
ğ‘¦ğ‘–= ğ›½0 + ğ›½1(ğ‘¥1,ğ‘–âˆ’
Ì„ğ‘¥1) + ğ›½2(ğ‘¥2,ğ‘–âˆ’
Ì„ğ‘¥2) + â‹¯+ ğ›½ğ‘(ğ‘¥ğ‘,ğ‘–âˆ’
Ì„ğ‘¥ğ‘) + ğœ–ğ‘–,
1 â‰¤ğ‘–â‰¤ğ‘›,
with ğ‘predictor variables ğ‘¥1, â‹¯, ğ‘¥ğ‘. There are in total 2ğ‘different models, cor-
responding to 2ğ‘combinations of variable selections. there are 2 possibilities for
each variable: either getting selected or not, and we have in total ğ‘variables.
We denote each model as ğ‘€ğ‘š, ğ‘š= 1, â‹¯, 2ğ‘. To obtian the posterior probabil-
ity of each model ğ‘(ğ‘€ğ‘š| data), Bayesâ€™ rule tells that that we need to assign
the prior probability ğ‘(ğ‘€ğ‘š) to each model, and to then obtain the marginal
likelihood of each model ğ‘(data | ğ‘€ğ‘š). By Bayesâ€™ rule, we update the posterior
probability of each model ğ‘€ğ‘šafter seeing the date, via marginal likelihood of
model ğ‘€ğ‘š:
ğ‘(ğ‘€ğ‘š| data) =
marginal likelihood of ğ‘€ğ‘šÃ— ğ‘(ğ‘€ğ‘š)
âˆ‘
2ğ‘
ğ‘—=1 marginal likelihood of ğ‘€ğ‘—Ã— ğ‘(ğ‘€ğ‘—)
=
ğ‘(data | ğ‘€ğ‘š)ğ‘(ğ‘€ğ‘š)
âˆ‘
2ğ‘
ğ‘—=1 ğ‘(data | ğ‘€ğ‘—)ğ‘(ğ‘€ğ‘—)
.
(7.4)
The marginal likelihood ğ‘(data | ğ‘€ğ‘š) of each model ğ‘€ğ‘šserves to reweight
the prior probability ğ‘(ğ‘€ğ‘š), so that models with higher likelihoods have

7.2. BAYESIAN MODEL UNCERTAINTY
155
larger weights, and models with lower likelihoods receive smaller weights.
We renormalize this weighted prior probability by dividing it by the sum
2ğ‘
âˆ‘
ğ‘—=1
ğ‘(data | ğ‘€ğ‘—)ğ‘(ğ‘€ğ‘—) to get the posterior probability of each model.
Recall that the prior odd between two models ğ‘€1 and ğ‘€2 is defined to be
O[ğ‘€1 âˆ¶ğ‘€2] = ğ‘(ğ‘€1)
ğ‘(ğ‘€2),
and the Bayes factor is defined to be the ratio of the likelihoods of two models
BF[ğ‘€1 âˆ¶ğ‘€2] = ğ‘(data | ğ‘€1)
ğ‘(data | ğ‘€2).
Suppose we have chosen a base model ğ‘€ğ‘, we may divide both the numerator
and the denominator of the formula (7.4) by ğ‘(data | ğ‘€ğ‘) Ã— ğ‘(ğ‘€ğ‘). This gives
us a new formula to calculate the posterior probability of model ğ‘€ğ‘šbased on
the prior odd and the Bayes factor. In this new formula, we can see that the
evidence from the data in the Bayes factor BF[ğ‘€ğ‘—âˆ¶ğ‘€ğ‘], ğ‘—= 1, â‹¯, 2ğ‘serve to
upweight or downweight the prior odd O[ğ‘€ğ‘—âˆ¶ğ‘€ğ‘], ğ‘—= 1, â‹¯, 2ğ‘.
ğ‘(ğ‘€ğ‘š| data) =
ğ‘(data | ğ‘€ğ‘š) Ã— ğ‘(ğ‘€ğ‘š)/(ğ‘(data | ğ‘€ğ‘) Ã— ğ‘(ğ‘€ğ‘))
âˆ‘
2ğ‘
ğ‘—=1(ğ‘(data | ğ‘€ğ‘—) Ã— ğ‘(ğ‘€ğ‘—))/(ğ‘(data | ğ‘€ğ‘) Ã— ğ‘(ğ‘€ğ‘))
=
[ğ‘(data | ğ‘€ğ‘š)/ğ‘(data | ğ‘€ğ‘)] Ã— [ğ‘(ğ‘€ğ‘š)/ğ‘(ğ‘€ğ‘)]
âˆ‘
2ğ‘
ğ‘—=1[ğ‘(data | ğ‘€ğ‘—)/ğ‘(data | ğ‘€ğ‘)] Ã— [ğ‘(ğ‘€ğ‘—)/ğ‘(ğ‘€ğ‘)]
=
BF[ğ‘€ğ‘šâˆ¶ğ‘€ğ‘] Ã— O[ğ‘€ğ‘šâˆ¶ğ‘€ğ‘]
âˆ‘
2ğ‘
ğ‘—=1 BF[ğ‘€ğ‘—âˆ¶ğ‘€ğ‘] Ã— O[ğ‘€ğ‘—âˆ¶ğ‘€ğ‘]
.
Any model can be used as the base model ğ‘€ğ‘. It could be the model with
the highest posterior probability, or the null model ğ‘€0 with just the intercept
ğ‘¦ğ‘–= ğ›½0 + ğœ–ğ‘–.
Using BIC, we can approximate the Bayes factor between two models by their
OLS ğ‘…-squaredâ€™s and the numbers of predictors used in the models, when we
have large sample of data. This provides a much easier way to approximate
the posterior probability of models since obtaining ğ‘…2 can be done by the usual
OLS linear regression. Recall that in Section 7.1, we provided the fact that BIC
of any model ğ‘€ğ‘š(denoted as BICğ‘š) is an asymptotic approximation of the log
of marginal likelihood of ğ‘€ğ‘šwhen the sample size ğ‘›is large (Equation (7.2))
BICğ‘šâ‰ˆâˆ’2 ln(marginal likelihood) = âˆ’2 ln(ğ‘(data | ğ‘€ğ‘š)).

156
CHAPTER 7. BAYESIAN MODEL CHOICE
Using this fact, we can approximate Bayes factor between two models by their
BICs
BF[ğ‘€1 âˆ¶ğ‘€2] = ğ‘(data | ğ‘€1)
ğ‘(data | ğ‘€2) â‰ˆexp(âˆ’BIC1/2)
exp(âˆ’BIC2/2) = exp (âˆ’1
2(BIC1 âˆ’BIC2)) .
We also know that BIC can be calculated by the OLS ğ‘…2 and the number of
predictors ğ‘from Equation (7.3) in Section 7.1
BIC = ğ‘›ln(1 âˆ’ğ‘…2) + (ğ‘+ 1) ln(ğ‘›) + constant.
(We usually ignore the constant in the last term since it does not affect the
difference betweeen two BICs.)
Using this formula, we can approximate Bayes factor between model ğ‘€1 and
ğ‘€2 by their corresponding ğ‘…-squaredâ€™s and the numbers of predictors
BF[ğ‘€1 âˆ¶ğ‘€2] â‰ˆ(1 âˆ’ğ‘…2
1
1 âˆ’ğ‘…2
2
)
ğ‘›
2
Ã— ğ‘›
ğ‘1âˆ’ğ‘2
2
.
(7.5)
As for the null model ğ‘€0 âˆ¶ğ‘¦ğ‘–= ğ›½0 + ğœ–ğ‘–, ğ‘…2
0 = 0 and ğ‘0 = 0. Equation (7.5)
can be further simplified as
BF[ğ‘€ğ‘šâˆ¶ğ‘€0] = (1 âˆ’ğ‘…2
ğ‘š)
ğ‘›
2 Ã— ğ‘›
ğ‘ğ‘š
2 .
7.2.2
Calculating Posterior Probability in R
Back to the kidâ€™s cognitive score example, we will see how the summary of results
using bas.lm tells us about the posterior probability of all possible models.
Suppose we have already loaded the data and pre-processed the columns
mom_work and mom_hs using as.numeric function, as what we did in the last
section. To represent model certainty, we construct the probability distribution
overall possible 16 (=24) models where each probability ğ‘(ğ‘€ğ‘š) provides a
measure of how likely the model ğ‘€ğ‘šis. Inside the bas.lm function, we first
specify the full model, which in this case is the kid_score, being regressed by
all predictors: motherâ€™s high school status hs, motherâ€™s IQ IQ, motherâ€™s work
status work and motherâ€™s age age. We take the data = cognitive in the next
argument. For the prior distribution of the coeï¬€icients for calculating marginal
likelihoods, we use prior = "BIC" to approximate the marginal likelihood
ğ‘(data | ğ‘€ğ‘š).
We then use modelprior = uniform() in the argument to
assign equal prior probability ğ‘(ğ‘€ğ‘š), ğ‘š= 1, â‹¯, 16 to all 16 models. That is,
ğ‘(ğ‘€ğ‘š) = 1
16.
# Import libary
library(BAS)

7.2. BAYESIAN MODEL UNCERTAINTY
157
# Use `bas.lm` for regression
cog_bas = bas.lm(kid_score ~ hs + IQ + work + age,
data = cognitive, prior = "BIC",
modelprior = uniform())
cog_bas is a bas object.
The usual print, summary, plot, coef, fitted,
predict functions are available and can be used on bas objects similar to lm
objects created by the usual lm function. From calling
names(cog_bas)
##
[1] "probne0"
"which"
"logmarg"
"postprobs"
##
[5] "priorprobs"
"sampleprobs"
"mse"
"mle"
##
[9] "mle.se"
"shrinkage"
"size"
"R2"
## [13] "rank"
"rank_deficient" "n.models"
"namesx"
## [17] "n"
"prior"
"modelprior"
"alpha"
## [21] "probne0.RN"
"postprobs.RN"
"include.always" "df"
## [25] "n.vars"
"Y"
"X"
"mean.x"
## [29] "call"
"xlevels"
"terms"
"model"
one can see the outputs and analyses that we can extract from a bas object.
The bas object takes the summary method
round(summary(cog_bas), 3)
##
P(B != 0 | Y)
model 1
model 2
model 3
model 4
model 5
## Intercept
1.000
1.000
1.000
1.000
1.000
1.000
## hs
0.611
1.000
0.000
0.000
1.000
1.000
## IQ
1.000
1.000
1.000
1.000
1.000
1.000
## work
0.112
0.000
0.000
1.000
1.000
0.000
## age
0.069
0.000
0.000
0.000
0.000
1.000
## BF
NA
1.000
0.562
0.109
0.088
0.061
## PostProbs
NA
0.529
0.297
0.058
0.046
0.032
## R2
NA
0.214
0.201
0.206
0.216
0.215
## dim
NA
3.000
2.000
3.000
4.000
4.000
## logmarg
NA -2583.135 -2583.712 -2585.349 -2585.570 -2585.939
The summary table shows us the following information of the top 5 models
Item
Description
P(B!=0 | Y)
Posterior inclusion probability (pip) of each coeï¬€icient
under data ğ‘Œ
0 or 1 in the
column
indicator of whether the variable is included in the model
BF
Bayes factor BF[ğ‘€ğ‘šâˆ¶ğ‘€ğ‘], where ğ‘€ğ‘is the model with
highest posterior probability
PostProbs
Posterior probability of each model

158
CHAPTER 7. BAYESIAN MODEL CHOICE
Item
Description
R2
ğ‘…-squared in the ordinary least square (OLS) regression
dim
Number of variables (including the intercept) included in
the model
logmarg
Log of marginal likelihood of the model, which is
approximately âˆ’1
2BIC
All top 5 models suggest to exclude age variable and include IQ variable. The
first model includes intercept ğ›½0 and only hs and IQ, with a posterior probability
of about 0. The model with the 2nd highest posterior probability, which includes
only the intercept and the variable IQ, has posterior probability of about 0.
These two models compose of total posterior probability of about 0, leaving
only 1 posterior probability to the remaining 14 models.
Using the print method, we obtain the marginal posterior inclusion probability
(pip) ğ‘(ğ›½ğ‘—â‰ 0) of each variable ğ‘¥ğ‘—.
print(cog_bas)
##
## Call:
## bas.lm(formula = kid_score ~ hs + IQ + work + age, data = cognitive,
##
prior = "BIC", modelprior = uniform())
##
##
##
Marginal Posterior Inclusion Probabilities:
## Intercept
hs
IQ
work
age
##
1.00000
0.61064
1.00000
0.11210
0.06898
7.3
Bayesian Model Averaging
In the last section, we explored model uncertainty using posterior probability of
models based on BIC. In this section, we will continue the kidâ€™s cognitive score
example to see how to obtain an Bayesian model averaging results using model
posterior probability.
7.3.1
Visualizing Model Uncertainty
Recall that in the last section, we used the bas.lm function in the BAS package
to obtain posterior probability of all models in the kidâ€™s cognitive score example.
score âˆ¼hq + IQ + work + age
We have found the posterior distribution under model uncertainty using all
possible combinations of the predictors, the motherâ€™s high school status hs,

7.3. BAYESIAN MODEL AVERAGING
159
motherâ€™s IQ score IQ, whether the mother worked during the first three years
of the kidâ€™s life work, and motherâ€™s age age.
With 4 predictors, there are
24 = 16 possible models. In general, for linear regression model with ğ‘predictor
variables
ğ‘¦ğ‘–= ğ›½0 + ğ›½1(ğ‘¥ğ‘,ğ‘–âˆ’
Ì„ğ‘¥) + â‹¯+ ğ›½ğ‘(ğ‘¥ğ‘,ğ‘–âˆ’
Ì„ğ‘¥ğ‘) + ğœ–ğ‘–,
ğ‘–= 1, â‹¯, ğ‘›,
there will be in total 2ğ‘possible models.
We can also visualize model uncertainty from the bas object cog_bas that we
generated in the previous section.
In R, the image function may be used to create an image of the model space
that looks like a crossword puzzle.
image(cog_bas, rotate = F)
## Warning in par(par.old): argument 1 does not name a graphical parameter
48.112
45.899
45.309
43.46
11.39
1
2
3
4
5
6
7
8
9
12
Model Rank
Log Posterior Odds
age
work
IQ
hs
Intercept
To obtain a clearer view for model comparison, we did not rotate the image.
Here, the predictors, including the intercept, are on the ğ‘¦-axis, while the ğ‘¥-axis
corresponds to each different model. Each vertical column corresponds to one
model. For variables that are not included in one model, they will be represented
by black blocks. For example, model 1 includes the intercept, hs, and IQ, but
not work or age. These models are ordered according to the log of posterior
odd over the null model (model with only the intercept). The log of posterior

160
CHAPTER 7. BAYESIAN MODEL CHOICE
odd is calculated as
ln(PO[ğ‘€ğ‘šâˆ¶ğ‘€0]) = ln(BF[ğ‘€ğ‘šâˆ¶ğ‘€0] Ã— O[ğ‘€ğ‘šâˆ¶ğ‘€0]).
Since we assing same prior probability for all models, O[ğ‘€ğ‘šâˆ¶ğ‘€0] = 1 and
therefore, the log of posterior odd is the same as the log of the Bayes factor.
The color of each column is proportional to the log of the posterior probability.
Models with same colors have similar posterior probabilities. This allows us to
view models that are clustered together, when the difference within a cluster is
not worth a bare mention.
If we view the image by rows, we can see whether one variable is included in
a particular model. For each variable, there are only 8 models in which it will
appear. For example, we see that IQ appears in all the top 8 models with larger
posterior probabilities, but not the last 8 models. The image function shows up
to 20 models by default.
7.3.2
Bayesian Model Averaging Using Posterior Proba-
bility
Once we have obtained the posterior probability of each model, we can make
inference and obtain weighted averages of quantities of interest using these prob-
abilities as weights. Models with higher posterior probabilities receive higher
weights, while models with lower posterior probabilities receive lower weights.
This gives the name â€œBayesian Model Averagingâ€ (BMA). For example, the
probability of the next prediction
Ì‚ğ‘Œâˆ—after seeing the data can be calculated as
a â€œweighted averageâ€ of the prediction of next observation
Ì‚ğ‘Œâˆ—
ğ‘—under each model
ğ‘€ğ‘—, with the posterior probability of ğ‘€ğ‘—being the â€œweightâ€
Ì‚ğ‘Œâˆ—=
2ğ‘
âˆ‘
ğ‘—=1
Ì‚ğ‘Œâˆ—
ğ‘—ğ‘(ğ‘€ğ‘—| data).
In general, we can use this weighted average formula to obtain the value of
a quantity of interest Î”. Î” can ğ‘Œâˆ—, the next observation; ğ›½ğ‘—, the coeï¬€icient
of variable ğ‘‹ğ‘—; ğ‘(ğ›½ğ‘—| data), the posterior probability of ğ›½ğ‘—after seeing the
data. The posterior probability of Î” seeing the data can be calculated using
the formula
ğ‘(Î” | data) =
2ğ‘
âˆ‘
ğ‘—=1
ğ‘(Î” | ğ‘€ğ‘—, data)ğ‘(ğ‘€ğ‘—| data).
(7.6)
This formula is similar to the one we have seen in Week 2 lecture Predictive
Inference when we used posterior probability of two different success rates of
getting the head in a coin flip to calculate the predictive probability of getting
heads in future coin flips.
Recall in that example, we have two competing

7.3. BAYESIAN MODEL AVERAGING
161
hypothese, that the success rate (also known as the probability) of getting heads
in coin flips, are
ğ»1 âˆ¶ğ‘= 0.7,
vs
ğ»2 âˆ¶ğ‘= 0.4.
We calcualted the posterior probability of each success rate. They are
ğ‘ƒ(ğ‘= 0.7 | data) =ğ‘ƒ(ğ»1 | data) = ğ‘âˆ—= 0.754,
ğ‘ƒ(ğ‘= 0.4 | data) =ğ‘ƒ(ğ»2 | data) = 1 âˆ’ğ‘âˆ—= 0.246.
We can use these two probabilities to calculate the posterior probability of
getting head in the next coin flip
ğ‘ƒ(head | data) = ğ‘ƒ(head | ğ»1, data)ğ‘ƒ(ğ»1 | data)+ğ‘ƒ(head | ğ»2, data)ğ‘ƒ(ğ»2 | data).
(7.7)
We can see that equation (7.7) is just a special case of the general equation (7.6)
when the posterior probability of hypotheses ğ‘ƒ(ğ»1 | data) and ğ‘ƒ(ğ»2 | data)
serve as weights.
Moreover, the expected value of Î” can also be obtained by a weighted average
formula of expected values on each model, using conditional probability
ğ¸[Î” | data] =
2ğ‘
âˆ‘
ğ‘—=1
ğ¸[Î” | ğ‘€ğ‘—, data]ğ‘(ğ‘€ğ‘—| data).
Since the weights ğ‘(ğ‘€ğ‘—| data) are probabilities and have to sum to one, if the
best model had posterior probability one, all of the weights would be placed
on that single best model.
In this case, using BMA would be equivalent to
selecting the best model with the highest posterior probability.
However, if
there are several models that receive substantial probability, they would all be
included in the inference and account for the uncertainty about the true model.
7.3.3
Coeï¬€icient Summary under BMA
We can obtain the coeï¬€icients by the coef function.
cog_coef = coef(cog_bas)
cog_coef
##
##
Marginal Posterior Summaries of Coefficients:
##
##
Using
BMA
##
##
Based on the top
16 models
##
post mean
post SD
post p(B != 0)
## Intercept
86.79724
0.87287
1.00000

162
CHAPTER 7. BAYESIAN MODEL CHOICE
## hs
3.59494
3.35643
0.61064
## IQ
0.58101
0.06363
1.00000
## work
0.36696
1.30939
0.11210
## age
0.02089
0.11738
0.06898
Under Bayesian model averaging, the table above provides the posterior mean,
the posterior standard deviation, and the posterior inclusion probability (pip)
of each coeï¬€icient. The posterior mean of the coeï¬€icient
Ì‚ğ›½ğ‘—under BMA would
be used for future predictions. The posterior standard deviation seğ›½ğ‘—provides
measure of variability of the coeï¬€icient ğ›½ğ‘—. An approximate range of plausible
values for each of the coeï¬€icients may be obtained via the empirical rule
( Ì‚ğ›½ğ‘—âˆ’critical value Ã— seğ›½ğ‘—,
Ì‚ğ›½ğ‘—+ critical value Ã— seğ›½ğ‘—).
However, this only applies if the posterior distribution is symmetric or unimodal.
The posterior mean of the intercept,
Ì‚ğ›½0, is obtained after we have centered the
variables.
We have discussed the effect of centering the model.
One of the
advantage of doing so is that the intercept ğ›½0 represents the sample mean of
the observed response ğ‘Œ. Under the reference prior, the point estimate of the
intercept
Ì‚ğ›½0 is exactly the mean
Ì„ğ‘Œ.
We see that the posterior mean, standard deviation and inclusion probability
are slightly different than the ones we obtained in Section 6.3 when we forced
the model to include all variables.
Under BMA, IQ has posterior inclusion
probability 1, suggesting that it is very likely that IQ should be included in
the model. hs also has a high posterior inclusion probability of about 0.61.
However, the posterior inclusion probability of motherâ€™s work status work and
motherâ€™s age age are relatively small compared to IQ and hs.
We can also plot the posterior distributions of these coeï¬€icients to take a closer
look at the distributions
par(mfrow = c(2, 2))
plot(cog_coef, subset = c(2:5))

7.4. SUMMARY
163
0
5
10
15
20
0.0
0.3
0.6
hs
0.0
0.2
0.4
0.6
0.8
0.0
0.6
IQ
âˆ’5
0
5
10
15
0.0
0.6
work
âˆ’1.0
0.0
0.5
1.0
1.5
2.0
0.0
0.6
age
This plot agrees with the summary table we obtained above, which shows that
the posterior probability distributions of work and age have a very large point
mass at 0, while the distribution of hs has a relatively small mass at 0. There is
a slighly little tip at 0 for the variable IQ, indicating that the posterior inclusion
probability of IQ is not exactly 1. However, since the probability mass for IQ
to be 0 is so small, that we are almost certain that IQ should be included under
Bayesian model averaging.
7.4
Summary
In this chapter, we have discussed Bayesian model uncertainty and Bayesian
model averaging. We have shown how Bayesian model averaging can be used
to address model uncertainty using the ensemble of models for inference, rather
than selecting a single model.
We applied this to the kidâ€™s cognitive score
data set using BAS package in R. Here we illustrated the concepts using BIC
and reference prior on the coeï¬€icients.
In the next chapter, we will explore
alternative priors for coeï¬€icients, taking into account the sensitivity of model
selection to prior choices.
We will also explore Markov Chain Monte Carlo
algorithm for model sampling when the model space is too large for theoretical
calculations.

164
CHAPTER 7. BAYESIAN MODEL CHOICE

Chapter 8
Stochastic Explorations
Using MCMC
In this chapter, we will discuss stochastic explorations of the model space using
Markov Chain Monte Carlo method. This is particularly usefull when the num-
ber of models in the model space is relatively large. We will introduce the idea
and the algorithm that we apply on the kidâ€™s cognitive score example. Then We
will introduce some alternative priors for the coeï¬€icients other than the reference
priors that we have been focused on. We will demonstrate using Markov Chain
Monte Carlo on the crime data set to see how to use this stochastic method to
explore the model space and how different priors may lead to different posterior
inclusion probability of coeï¬€icients. Finally, we will summarize decision making
strategies under Bayesian model uncertainty.
8.1
Stochastic Exploration
In the last chapter, we explored model uncertainty using posterior probability
of each model and Bayesian model averaging based on BIC. We applied the
idea on the kidâ€™s cognitive score data set. With 4 predictors, we had 24 = 16
possible models. Since the total number of models is relatively small, it is easy
to enumerate all possible models to obtain Bayesian model averaging results.
However, in general we often have data sets with large number of variables,
which may lead to long computating time via enumeration. In this section, we
will present one of the common stochastic methods, Markov Chain Monte Carlo
(MCMC), to explore model spaces and implement Bayesian model averaging to
estimate quantities of interest.
165

166
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
8.1.1
Markov Chain Monte Carlo Exploration
Let us assume that we have a pseudo population of possible models that we
obtained from all the possible combinations of regression models from the kidâ€™s
cognitive score example. We prepare the data set as in Section 6.3 and run
bas.lm to obtain posterior probability of each model as we did in Section 7.2.
# Data processing
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq
cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =
as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","IQ", "work", "age")
# Run regression
library(BAS)
cog_bas = bas.lm(kid_score ~ hs + IQ + work + age,
prior = "BIC",
modelprior = uniform(),
data = cognitive)
We will use this example to explore the idea of MCMC and generalize it to re-
gression models with much larger model spaces. To explore the models, we may
arrange them by their model sizes, the number of predictors plus the intercept,
on the ğ‘¥-axis, and their posterior probabilities on the ğ‘¦-axis.
library(ggplot2)
# Construct data frame for plotting
output = data.frame(model.size = cog_bas$size, model.prob = cog_bas$postprobs)
# Plot model size vs mode posterior probability
ggplot(data = output, aes(x = model.size, y = model.prob)) +
geom_point(color = "blue", pch = 17, size = 3) +
xlab("model size") + ylab("model posterior probability")

8.1. STOCHASTIC EXPLORATION
167
0.0
0.2
0.4
1
2
3
4
5
model size
model posterior probability
We could then take a sample from this population of models with replacement
(therefore, some models may be selected more than once in this sample). This
process could be done using the sample function in R. We hope that the fre-
quency of appearance of a model would be a good approximation of the posterior
probability of this model. We use ğ¼(ğ‘€ğ‘—= ğ‘€ğ‘š) as the indicator function to in-
dicate that the current model ğ‘€ğ‘—we sample is the model of interest ğ‘€ğ‘š, that
is
ğ¼(ğ‘€ğ‘—= ğ‘€ğ‘š) = { 1,
if ğ‘€ğ‘—= ğ‘€ğ‘š
0,
if ğ‘€ğ‘—â‰ ğ‘€ğ‘š
Suppose we are going to sample ğ½models in total, we hope that
ğ‘(ğ‘€ğ‘š| data) â‰ˆ
âˆ‘
ğ½
ğ‘—=1 ğ¼(ğ‘€ğ‘—= ğ‘€ğ‘š)
ğ½
=
ğ½
âˆ‘
ğ‘—=1
ğ¼(ğ‘€ğ‘—= ğ‘€ğ‘š)
ğ½
.
(8.1)
After all, we would not need to calculate the model posterior probability
ğ‘ƒ(ğ‘€ğ‘š| data).
The quantity from the sampling
ğ½
âˆ‘
ğ‘—=1
ğ¼(ğ‘€ğ‘—= ğ‘€ğ‘š)
ğ½
would
provide a good approximation, which only requires simple counting.
In order to ensure that we would sample models with a probability that is equal
to their posterior probability, or in a simpler way, proportional to the marginal
likelihood times the prior probability ğ‘(data | ğ‘€ğ‘š)Ã—ğ‘(ğ‘€ğ‘š), we need to design a
sampling method that replaces old models with new models when the posterior
probability goes up, and keeps the old models when the posterior probability is
not improved.
Here, we propose the Metropolis-Hastings algorithm. We start with an initial
model ğ‘€(0). This could be any model we like in the model space. We start
iterating over the entire model space, randomly pick the next model ğ‘€âˆ—(1) and
see whether this model improves the posterior probability. We use the notation

168
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
ğ‘€âˆ—(1) instead of ğ‘€(1) because we are not sure whether we should include this
model in our final sample, or we should consider other models. Therefore, we
calculate the ratio between the posterior probability of the two models, the
original model ğ‘€(0), and the proposed model ğ‘€âˆ—(1), which turns out to be the
posterior odd between the two models
ğ‘…= ğ‘(ğ‘€âˆ—(1) | data)
ğ‘(ğ‘€(0) | data) = PO[ğ‘€âˆ—(1) âˆ¶ğ‘€(0)].
Our goal is to avoid actually calculating the posterior probability of each model,
so we instead would compute ğ‘…using the Bayes factor and the prior odd of the
two models.
ğ‘…= ğ‘(ğ‘€âˆ—(1) | data)
ğ‘(ğ‘€(0) | data) = PO[ğ‘€âˆ—(1) âˆ¶ğ‘€(0)] = BF[ğ‘€âˆ—(1) âˆ¶ğ‘€(0)] Ã— O[ğ‘€âˆ—(1) âˆ¶ğ‘€(0)].
If ğ‘…â‰¥1, that means ğ‘€âˆ—(1) will surely improve the posterior probability after
seeing the data compared to ğ‘€(0). So we would like to include ğ‘€âˆ—(1) into our
sample, because ğ‘€âˆ—(1) deserves more occurrence. In this case, we set ğ‘€âˆ—(1) to
be ğ‘€(1), indicating that it is part of our final sample. However, if ğ‘…< 1, we are
not that sure whether ğ‘€âˆ—(1) should be in the sample. But we also do not want
to only include models with higher posterior probabilities. Remember that the
purpose of this algorithm is to reproduce the frequency of model occurance in
the final sample so that the relative frequency of occurrence of each model could
be a good proxy of its posterior probability. Even though the proposed model
ğ‘€âˆ—(1) has lower posterior probability, we should still have some representatives
of this model in our final sample. Hence we set ğ‘€âˆ—(1) to be ğ‘€(1) with probability
ğ‘…, reflecting the chance that this model would be in our sample is ğ‘….
To include ğ‘€âˆ—(1) in the final sample with probability ğ‘…, we may use a random
number generator to generate number between 0 and 1 and see whether this
number is larger than ğ‘…. Or we may set a coin flip with heads showing up
with probability ğ‘…. If the random number is larger than ğ‘…, or the head shows
up using the biased coin, we include this model. Otherwise, we neglect this
proposed model and keep on selecting the next model.
Once the first model ğ‘€âˆ—(1)) is sampled, we move onto the second model ğ‘€(2)
with the same process.
In general, after we have obtained model ğ‘€(ğ‘–), we
propose a model ğ‘€âˆ—(ğ‘–+1) and calculate the ratio of the posterior probabilities of
the two models
ğ‘…= ğ‘(ğ‘€âˆ—(ğ‘–+1) | data)
ğ‘(ğ‘€(ğ‘–) | data)
= BF[ğ‘€âˆ—(ğ‘–+1) âˆ¶ğ‘€(ğ‘–)] Ã— O[ğ‘€âˆ—(ğ‘–+1) âˆ¶ğ‘€(ğ‘–)].
If ğ‘…â‰¥1, we unconditionally accept ğ‘€âˆ—(ğ‘–+1) to be our next model ğ‘€(ğ‘–).
If
ğ‘…< 1, we accept ğ‘€âˆ—(ğ‘–+1) to be ğ‘€(ğ‘–) with probability ğ‘….

8.2. OTHER PRIORS FOR BAYESIAN MODEL UNCERTAINTY
169
After obtaining ğ½models, ğ‘€(1), ğ‘€(2), â‹¯, ğ‘€(ğ½), we can count how many models
inside this sample is ğ‘€ğ‘š, the model we are interested. Then we use the formula
(8.1) to approximate the posterior probability of ğ‘€ğ‘š. These estimated proba-
bilities can be used in model selection or BMA instead of the exact expressions.
We propose model randomly in the above algorithm, i.e., all models are equally
likely to be proposed. This can be pretty ineï¬€icient if there are lots of models
with low probabilities. We may come up with other ways to propose models.
For example, we may look at neighboring models of our current model by either
adding one predictor that is currently not in the model, or randomly dropping
one of the current predictors from the model. We may flip a fair coin to decide
whether to add or to drop. This forms a random walk across neighboring models.
We may also propose to swap out a current predictor with one that is currently
not in the model, which maintains the size of the model. This has the potential
to take bigger jumps in the model space. There are other possible moves that
can be designed to help move around over the model space. However, we have to
be careful to adjust for any potential bias, due to how we propose new models,
to ensure that the relative frequency eventually would converge to the posterior
probability.
In the lecture video, we have demonstrated the Markov Chain
Monte Carlo method on the kidâ€™s cognitive score using animation to show how
each model was proposed and finally selected.
8.2
Other Priors for Bayesian Model Uncer-
tainty
So far, we have discussed Bayesian model selection and Bayesian model aver-
aging using BIC. BIC is an asymptotic approximation of the log of marginal
likelihood of models when the number of data points is large. Under BIC, prior
distribution of ğ›½= (ğ›½0, ğ›½1, â‹¯, ğ›½ğ‘)ğ‘‡is uniformaly flat, which is the same as
applying the reference prior on ğ›½conditioning on ğœ2. In this section, we will
introduce a new conjugate prior distribution, called the Zellnerâ€™s ğ‘”-prior. We
will see that this prior leads to simple expressions for the Bayes factor, in terms
of summary statistics from ordinary least square (OLS). We will talk about
choosing the parameter ğ‘”in the prior and conduct a sensitivity analysis, using
the kidâ€™s cognitive score data that we used in earlier sections.
8.2.1
Zellnerâ€™s ğ‘”-Prior
To analyze the model more conveniently, we still stick with the â€œcenteredâ€ re-
gression model. Let ğ‘¦1, â‹¯, ğ‘¦ğ‘›to be the observations of the response variable ğ‘Œ.
The multiple regression model is
ğ‘¦ğ‘–= ğ›½0 + ğ›½1(ğ‘¥1,ğ‘–âˆ’
Ì„ğ‘¥1) + ğ›½2(ğ‘¥2,ğ‘–âˆ’
Ì„ğ‘¥2) + â‹¯+ ğ›½ğ‘(ğ‘¥ğ‘,ğ‘–âˆ’
Ì„ğ‘¥ğ‘) + ğœ–ğ‘–,
1 â‰¤ğ‘–â‰¤ğ‘›.

170
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
As before,
Ì„ğ‘¥1, â‹¯, Ì„ğ‘¥ğ‘, are the sample means of the variables ğ‘‹1, â‹¯, ğ‘‹ğ‘. Since
we have centered all the variables, ğ›½0 is no longer the ğ‘¦-intercept. Instead, it
is the sample mean of ğ‘Œwhen taking ğ‘‹1 =
Ì„ğ‘¥1, â‹¯, ğ‘‹ğ‘=
Ì„ğ‘¥ğ‘.
ğ›½1, â‹¯, ğ›½ğ‘are
the coeï¬€icients for the ğ‘variables. ğ›½= (ğ›½0, ğ›½1, â‹¯, ğ›½ğ‘)ğ‘‡is the vector notation
representing all coeï¬€icients, including ğ›½0.
Under this model, we assume
ğ‘¦ğ‘–| ğ›½, ğœ2
iid
âˆ¼Normal(ğ›½0 + ğ›½1(ğ‘¥1,ğ‘–âˆ’
Ì„ğ‘¥1) + â‹¯+ ğ›½ğ‘(ğ‘¥ğ‘,ğ‘–âˆ’
Ì„ğ‘¥ğ‘), ğœ2),
which is equivalent to
ğœ–ğ‘–| ğ›½, ğœ2
iid
âˆ¼Normal(0, ğœ2).
We then specify the prior distributions for ğ›½ğ‘—, 0 â‰¤ğ‘—â‰¤ğ‘. Zellner proposed a
simple informative conjugate multivariate normal prior for ğ›½conditioning on ğœ2
as
ğ›½| ğœ2 âˆ¼Normal(ğ‘0, Î£ = ğ‘”ğœ2Sâˆ’1
xx).
Here
Sxx = (X âˆ’
Ì„X)ğ‘‡(X âˆ’
Ì„X),
where the matrix X âˆ’
Ì„X is
Xâˆ’Ì„X = â›
âœ
â
|
|
â‹¯
|
ğ‘‹1 âˆ’
Ì„ğ‘‹1
ğ‘‹2 âˆ’
Ì„ğ‘‹2
â‹¯
ğ‘‹ğ‘âˆ’
Ì„ğ‘‹ğ‘
|
|
â‹¯
|
â
âŸ
â 
= â›
âœ
â
ğ‘¥1,1 âˆ’
Ì„ğ‘¥1
ğ‘¥2,1 âˆ’
Ì„ğ‘¥2
â‹¯
ğ‘¥ğ‘,1 âˆ’
Ì„ğ‘¥ğ‘
â‹®
â‹®
â‹®
ğ‘¥1,ğ‘›âˆ’
Ì„ğ‘¥1
ğ‘¥2,ğ‘›âˆ’
Ì„ğ‘¥2
â‹¯
ğ‘¥ğ‘,ğ‘›âˆ’
Ì„ğ‘¥ğ‘
â
âŸ
â 
.
When ğ‘= 1, this Sxx simplifies to Sxx =
ğ‘›
âˆ‘
ğ‘–=1
(ğ‘¥ğ‘–âˆ’ğ‘ğ‘ğ‘Ÿğ‘¥)2, the sum of squares
of a single variable ğ‘‹that we used in Section 6.1. In multiple regression, Sxx
provides the variance and covariance for OLS.
The parameter ğ‘”scales the prior variance of ğ›½, over the OLS variances ğœ2Sâˆ’1
xx.
One of the advantages of using this prior is ,that it reduces prior elicitation
down to two components; the prior mean ğ‘0 and the scalar ğ‘”. We use ğ‘”to
control the size of the variance of the prior, rather than set separate priors for
all the variances and covariances (there would be ğ‘(ğ‘+ 1)/2 such priors for a
ğ‘+ 1 dimensional multivariate normal distribution).
Another advantage of using Zellnerâ€™s ğ‘”-prior is that it leads to simple updating
rules, like all conjugate priors.
Moreover, the posterior mean and posterior
variance have simple forms. The posterior mean is
ğ‘”
1 + ğ‘”
Ì‚ğ›½+
1
1 + ğ‘”ğ‘0,

8.2. OTHER PRIORS FOR BAYESIAN MODEL UNCERTAINTY
171
where
Ì‚ğ›½is the frequentist OLS estimates of coeï¬€icients ğ›½. The posterior variance
is
ğ‘”
1 + ğ‘”ğœ2Sâˆ’1
xx.
From the posterior mean formula, we can see that the posterior mean is a
weighted average of the prior mean ğ‘0 and the OLS estimate
Ì‚ğ›½. Since
ğ‘”
1 + ğ‘”is
strictly less than 1, Zellnerâ€™s ğ‘”-prior shrinks the OLS estimates
Ì‚ğ›½towards the
prior mean ğ‘0. As ğ‘”â†’âˆ,
ğ‘”
1 + ğ‘”â†’1 and
1
1 + ğ‘”â†’0, and we recover the OLS
estimate as in the reference prior.
Similarly, the posterior variancc is a shrunken version of the OLS variance, by a
factor of
ğ‘”
1 + ğ‘”. The posterior distribution of ğ›½conditioning on ğœ2 is a normal
distribution
ğ›½| ğœ2, data âˆ¼Normal(
ğ‘”
1 + ğ‘”
Ì‚ğ›½+
1
1 + ğ‘”ğ‘0,
ğ‘”
1 + ğ‘”ğœ2Sâˆ’1
xx).
8.2.2
Bayes Factor of Zellnerâ€™s ğ‘”-Prior
Because of this simplicity, Zellnerâ€™s ğ‘”-prior has been widely used in Bayesian
model selection and Bayesian model averaging. One of the most popular versions
uses the ğ‘”-prior for all coeï¬€icients except the intercept, and takes the prior
mean to be the zero vector ğ‘0 = 0. If we are not testing any hypotheses about
the intercept ğ›½0, we may combine this ğ‘”-prior with the reference prior for the
intercept ğ›½0 and ğœ2, that is, we set
ğ‘(ğ›½0, ğœ2) âˆ1
ğœ2 ,
and use the ğ‘”-prior for the rest of the coeï¬€icients (ğ›½1, â‹¯, ğ›½ğ‘)ğ‘‡.
Under this prior, the Bayes factor for comparing model ğ‘€ğ‘što the null model
ğ‘€0, which only has the intercept, is simply
BF[ğ‘€ğ‘šâˆ¶ğ‘€0] = (1 + ğ‘”)(ğ‘›âˆ’ğ‘ğ‘šâˆ’1)/2(1 + ğ‘”(1 âˆ’ğ‘…2
ğ‘š))âˆ’(ğ‘›âˆ’1)/2.
Here ğ‘ğ‘šis the number of predictors in ğ‘€ğ‘š, ğ‘…2
ğ‘šis the ğ‘…-squared of model ğ‘€ğ‘š.
With the Bayes factor, we can compare any two models using posterior odds.
For example, we can compare model ğ‘€ğ‘šwith the null model ğ‘€0 by
ğ‘(ğ‘€ğ‘š| data, ğ‘”)
ğ‘(ğ‘€0 | data, ğ‘”) = PO[ğ‘€ğ‘šâˆ¶ğ‘€0] = BF[ğ‘€ğ‘šâˆ¶ğ‘€0]ğ‘(ğ‘€ğ‘š)
ğ‘(ğ‘€0) .
Now the question is, how do we pick ğ‘”?
As we see that, the Bayes factor
depends on ğ‘”. If ğ‘”â†’âˆ, BF[ğ‘€ğ‘šâˆ¶ğ‘€0] â†’0. This provides overwhelming

172
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
evidence against model ğ‘€ğ‘š, no matter how many predictors we pick for ğ‘€ğ‘š
and the data. This is the Bartlettâ€™s/Jeffrey-Lindleyâ€™s paradox.
On the other hand, if we use any arbitrary fixed value of ğ‘”, and include more
and more predictors, the ğ‘…-squared ğ‘…2
ğ‘šwill get closer and closer to 1, but
the Bayes factor will remain bounded.
With ğ‘…2
ğ‘šgetting larger and larger,
we would expect the alternative model ğ‘€ğ‘šwould be supported. However, a
bounded Bayes factor would not provide overwhelming support for ğ‘€ğ‘š, even in
the frequentist approach we are getting better and better fit for the data. This is
the information paradox, when the Bayes factor comes to a different conclusion
from the frequentist approach due to the boundedness of Bayes factor in the
limiting case.
There are some solutions which appear to lead to reasonable results in small and
large samples based on empirical results with real data to theory, and provide
resolution to these two paradoxes. In the following examples, we let the prior
distribution of ğ‘”depend on ğ‘›, the size of the data. Since Sxx is getting larger
with larger ğ‘›, ğ‘”ğœ2Sâˆ’1
xx may get balanced if ğ‘”also grows relatively to the size of
ğ‘›.
Unit Information Prior
In the case of the unit information prior, we let ğ‘”= ğ‘›. This is the same as
saying ğ‘›
ğ‘”= 1. In this prior, we will only need to specify the prior mean ğ‘0 for
the coeï¬€icients of the predicor variables (ğ›½1, â‹¯, ğ›½ğ‘)ğ‘‡.
Zellner-Siow Cauchly Prior
However, taking ğ‘”= ğ‘›ignores the uncertainty of the choice of ğ‘”. Since we do
not know ğ‘”a priori, we may pick a prior so that the expected value of ğ‘›
ğ‘”= 1.
One exmaple is the Zellner-Siow cauchy prior. In this prior, we let
ğ‘›
ğ‘”
âˆ¼Gamma(1
2, 1
2).
Hyper-ğ‘”/ğ‘›Prior
Another example is to set
1
1 + ğ‘›/ğ‘”âˆ¼Beta(ğ‘
2, ğ‘
2),
with hyperparameters ğ‘and ğ‘. Since the Bayes factor under this prior distri-
bution can be expressed in terms of hypergeometric functions, this is called the
hyper-ğ‘”/ğ‘›prior.
8.2.3
Kidâ€™s Cognitive Score Example
We apply these priors on the kidâ€™s cognitive score example and compare the
posterior probability that each coeï¬€icient ğ›½ğ‘–, ğ‘–= 1, 2, 3, 4 to be non-zero. We

8.2. OTHER PRIORS FOR BAYESIAN MODEL UNCERTAINTY
173
first read in data and store the size of the data into ğ‘›. We will use this ğ‘›later,
when setting priors for ğ‘›/ğ‘”.
library(foreign)
cognitive = read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")
cognitive$mom_work = as.numeric(cognitive$mom_work > 1)
cognitive$mom_hs =
as.numeric(cognitive$mom_hs > 0)
colnames(cognitive) = c("kid_score", "hs","IQ", "work", "age")
# Extract size of data set
n = nrow(cognitive)
We then fit the full model using different priors. Here we set model prior to be
uniform(), meaning each model has equal prior probability.
library(BAS)
# Unit information prior
cog.g = bas.lm(kid_score ~ ., data=cognitive, prior="g-prior",
a=n, modelprior=uniform())
# a is the hyperparameter in this case g=n
# Zellner-Siow prior with Jeffrey's reference prior on sigma^2
cog.ZS = bas.lm(kid_score ~ ., data=cognitive, prior="JZS",
modelprior=uniform())
# Hyper g/n prior
cog.HG = bas.lm(kid_score ~ ., data=cognitive, prior="hyper-g-n",
a=3, modelprior=uniform())
# hyperparameter a=3
# Empirical Bayesian estimation under maximum marginal likelihood
cog.EB = bas.lm(kid_score ~ ., data=cognitive, prior="EB-local",
a=n, modelprior=uniform())
# BIC to approximate reference prior
cog.BIC = bas.lm(kid_score ~ ., data=cognitive, prior="BIC",
modelprior=uniform())
# AIC
cog.AIC = bas.lm(kid_score ~ ., data=cognitive, prior="AIC",
modelprior=uniform())
Here cog.g is the model corresponding to the unit information prior ğ‘”= ğ‘›.
cog.ZS is the model under the Zellner-Siow cauchy prior with Jeffreyâ€™s reference
prior on ğœ2. cog.HG gives the model under the hyper-ğ‘”/ğ‘›prior. cog.EB is the
empirical Bayesian estimates which maximizes the marginal likelihood. cog.BIC
and cog.AIC are the ones corresponding to using BIC and AIC for marginal

174
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
likelihood approximation.
In order to compare the posterior inclusion probability (pip) of each coeï¬€icient,
we group the results ğ‘(ğ›½ğ‘–â‰ 0) obtained from the probne0 attribute of each
model for later comparison
probne0 = cbind(cog.BIC$probne0, cog.g$probne0, cog.ZS$probne0, cog.HG$probne0,
cog.EB$probne0, cog.AIC$probne0)
colnames(probne0) = c("BIC", "g", "ZS", "HG", "EB", "AIC")
rownames(probne0) = c(cog.BIC$namesx)
We can compare the results by printing the matrix probne0 that we just gen-
erated. If we want to visualize them to get a clearer idea, we may plot them
using bar plots.
library(ggplot2)
# Generate plot for each variable and save in a list
P = list()
for (i in 2:5){
mydata = data.frame(prior = colnames(probne0), posterior = probne0[i, ])
mydata$prior = factor(mydata$prior, levels = colnames(probne0))
p = ggplot(mydata, aes(x = prior, y = posterior)) +
geom_bar(stat = "identity", fill = "blue") + xlab("") +
ylab("") +
ggtitle(cog.g$namesx[i])
P = c(P, list(p))
}
library(cowplot)
do.call(plot_grid, c(P))

8.3. R DEMO ON BAS PACKAGE
175
0.00
0.25
0.50
0.75
BIC
g
ZS
HG
EB
AIC
hs
0.00
0.25
0.50
0.75
1.00
BIC
g
ZS
HG
EB
AIC
IQ
0.0
0.1
0.2
0.3
0.4
BIC
g
ZS
HG
EB
AIC
work
0.0
0.1
0.2
0.3
BIC
g
ZS
HG
EB
AIC
age
In the plots above, the ğ‘¥-axis lists all the prior distributions we consider, and
the bar heights represent the posterior inclusion probability of each coeï¬€icient,
i.e., ğ‘(ğ›½ğ‘–â‰ 0).
We can see that motherâ€™s IQ score is included almost as probability 1 in all
priors. So all methods agree that we should include variable IQ. Motherâ€™s high
school status also has probability of more than 0.5 in each prior, suggesting
that we may also consider including the variable hs. However, motherâ€™s work
status and motherâ€™s age have much lower posterior inclusion probability in all
priors. From left to right in each bar plot, we see that method BIC is the most
conservative method (meaning it will exclude the most variables), while AIC is
being the less conservative method.
8.3
R Demo on BAS Package
In this section, we will apply Bayesian model selection and model averaging
on the US crime data set UScrime using the BAS package. We will introduce
some additional diagnostic plots, and talk about the effect of multicollinearity
in model uncertainty.
8.3.1
The UScrime Data Set and Data Processing
We will demo the BAS commands using the US crime data set in the R libarry
MASS.

176
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
# Load library and data set
library(MASS)
data(UScrime)
This data set contains data on 47 states of the US for the year of 1960. The
response variable ğ‘Œis the rate of crimes in a particular category per head of
population of each state.
There are 15 potential explanatory variables with
values for each of the 47 states related to crime and other demographics. Here
is the table of all the potential explanatory variables and their descriptions.
Variable
Description
M
Percentage of males aged 14-24
So
Indicator variable for southern states
Ed
Mean years of schooling
Po1
Police expenditure in 1960
Po2
Police expenditure in 1959
LF
Labour force participation rate
M.F
Number of males per 1000 females
Pop
State population
NW
Number of non-whites per 1000 people
U1
Unemployment rate of urban males aged 14-24
U2
Unemployment rate of urban males aged 35-39
GDP
Gross domestic product per head
Ineq
Income inequality
Prob
Probability of imprisonment
Time
Average time served in state prisons
We may use the summary function to describe each variable in the data set.
summary(UScrime)
##
M
So
Ed
Po1
##
Min.
:119.0
Min.
:0.0000
Min.
: 87.0
Min.
: 45.0
##
1st Qu.:130.0
1st Qu.:0.0000
1st Qu.: 97.5
1st Qu.: 62.5
##
Median :136.0
Median :0.0000
Median :108.0
Median : 78.0
##
Mean
:138.6
Mean
:0.3404
Mean
:105.6
Mean
: 85.0
##
3rd Qu.:146.0
3rd Qu.:1.0000
3rd Qu.:114.5
3rd Qu.:104.5
##
Max.
:177.0
Max.
:1.0000
Max.
:122.0
Max.
:166.0
##
Po2
LF
M.F
Pop
##
Min.
: 41.00
Min.
:480.0
Min.
: 934.0
Min.
:
3.00
##
1st Qu.: 58.50
1st Qu.:530.5
1st Qu.: 964.5
1st Qu.: 10.00
##
Median : 73.00
Median :560.0
Median : 977.0
Median : 25.00
##
Mean
: 80.23
Mean
:561.2
Mean
: 983.0
Mean
: 36.62
##
3rd Qu.: 97.00
3rd Qu.:593.0
3rd Qu.: 992.0
3rd Qu.: 41.50
##
Max.
:157.00
Max.
:641.0
Max.
:1071.0
Max.
:168.00

8.3. R DEMO ON BAS PACKAGE
177
##
NW
U1
U2
GDP
##
Min.
:
2.0
Min.
: 70.00
Min.
:20.00
Min.
:288.0
##
1st Qu.: 24.0
1st Qu.: 80.50
1st Qu.:27.50
1st Qu.:459.5
##
Median : 76.0
Median : 92.00
Median :34.00
Median :537.0
##
Mean
:101.1
Mean
: 95.47
Mean
:33.98
Mean
:525.4
##
3rd Qu.:132.5
3rd Qu.:104.00
3rd Qu.:38.50
3rd Qu.:591.5
##
Max.
:423.0
Max.
:142.00
Max.
:58.00
Max.
:689.0
##
Ineq
Prob
Time
y
##
Min.
:126.0
Min.
:0.00690
Min.
:12.20
Min.
: 342.0
##
1st Qu.:165.5
1st Qu.:0.03270
1st Qu.:21.60
1st Qu.: 658.5
##
Median :176.0
Median :0.04210
Median :25.80
Median : 831.0
##
Mean
:194.0
Mean
:0.04709
Mean
:26.60
Mean
: 905.1
##
3rd Qu.:227.5
3rd Qu.:0.05445
3rd Qu.:30.45
3rd Qu.:1057.5
##
Max.
:276.0
Max.
:0.11980
Max.
:44.00
Max.
:1993.0
However, these variables have been pre-processed for modeling purpose, so the
summary statistics may not be so meaningful. The values of all these variables
have been aggregated over each state, so this is a case of ecological regression.
We will not model directly the rate for a person to commit a crime. Instead,
we will use the total number of crimes and average values of predictors at the
state level to predict the total crime rate of each state.
We transform the variables using the natural log function, except the indicator
variable So (2nd column of the data set). We perform this transformation based
on the analysis of this data set.1 Notice that So is already a numeric variable (1
indicating Southern state and 0 otherwise), not as a categorical variable. Hence
we do not need any data processing of this variable, unlike motherâ€™s high school
status hs and motherâ€™s work status work in the kidâ€™s cognitive score data set.
UScrime[,-2] = log(UScrime[,-2])
8.3.2
Bayesian Models and Diagnostics
We run bas.lm function from the BAS package. We first run the full model and
use this information for later decision on what variables to include. Here we
have 15 potential predictors. The total number of models is 215 = 32768. This
is not a very large number and BAS can enumerate all the models pretty quickly.
However, we want to illustrate how to explore models using stochastic methods.
Hence we set argument method = MCMC inside the bas.lm function. We also
use the Zellner-Siow cauchy prior for the prior distributions of the coeï¬€icients
in this regression.
library(BAS)
crime.ZS =
bas.lm(y ~ ., data=UScrime,
prior="ZS-null", modelprior=uniform(), method = "MCMC")
1More details can be found in Venables and Ripley (2013).

178
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
BAS will run the MCMC sampler until the number of unique models in the
sample exceeds number of models = 2ğ‘(when ğ‘< 19) or until the number of
MCMC iterations exceeds 2Ã—number of models by default, whichever is smaller.
Here ğ‘is the number of predictors.
Diagnostic Plots
To analyze the result, we first look at the diagnostic plot using diagnostics
function and see whether we have run the MCMC exploration long enough so
that the posterior inclusion probability (pip) has converged.
diagnostics(crime.ZS, type="pip", col = "blue", pch = 16, cex = 1.5)
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Convergence Plot: Posterior Inclusion Probabilities
pip (renormalized)
pip (MCMC)
In this plot, the ğ‘¥-axis is the renormalized posterior inclusion probability (pip)
of each coeï¬€icient ğ›½ğ‘–, ğ‘–= 1, â‹¯, 15 in this model. This can be calculated as
ğ‘(ğ›½ğ‘–â‰ 0 | data) =
âˆ‘
ğ‘€ğ‘šâˆˆmodel space
ğ¼(ğ‘‹ğ‘–âˆˆğ‘€ğ‘š)
â›
âœ
âœ
âœ
âœ
â
BF[ğ‘€ğ‘šâˆ¶ğ‘€0]O[ğ‘€ğ‘šâˆ¶ğ‘€0]
âˆ‘
ğ‘€ğ‘—
BF[ğ‘€ğ‘—âˆ¶ğ‘€0]O[ğ‘€ğ‘—âˆ¶ğ‘€0]
â
âŸ
âŸ
âŸ
âŸ
â 
.
(8.2)
Here, ğ‘‹ğ‘–is the ğ‘–th predictor variable, and ğ¼(ğ‘‹ğ‘–âˆˆğ‘€ğ‘š) is the indicator function
which is 1 if ğ‘‹ğ‘–is included in model ğ‘€ğ‘šand 0 if ğ‘‹ğ‘–is not included. The first
Î£ notation indicates that we sum over all models ğ‘€ğ‘šin the model space. And
we use
BF[ğ‘€ğ‘šâˆ¶ğ‘€0]O[ğ‘€ğ‘šâˆ¶ğ‘€0]
âˆ‘
ğ‘€ğ‘—
BF[ğ‘€ğ‘—âˆ¶ğ‘€0]O[ğ‘€ğ‘—âˆ¶ğ‘€0]
(8.3)
as the weights. You may recognize that the numerator of (8.3) is exactly the
ratio of the posterior probability of model ğ‘€ğ‘šover the posterior probability of
the null model ğ‘€0, i.e., the posterier odd PO[ğ‘€ğ‘šâˆ¶ğ‘€0]. We devide the posterior

8.3. R DEMO ON BAS PACKAGE
179
odd by the total sum of posterior odds of all models in the model space, to make
sure these weights are between 0 and 1. The weight in Equation (8.3) represents
the posterior probability of the model ğ‘€ğ‘šafter seeing the data ğ‘(ğ‘€ğ‘š| data),
the one we used in Section 7.2. So Equation (8.2) is the theoretical calculation
of pip, which can be rewrited as
ğ‘(ğ›½ğ‘–â‰ 0 | data) =
âˆ‘
ğ‘€ğ‘šâˆˆmodel space
ğ¼(ğ‘‹ğ‘–âˆˆğ‘€ğ‘š)ğ‘(ğ‘€ğ‘š| data).
The null model ğ‘€0, as we recall, is the model that only includes the intercept.
On the ğ‘¦-axis of the plot, we lay out the posterior inclusion probability of
coeï¬€icient ğ›½ğ‘–, which is calculated using
ğ‘(ğ›½ğ‘–â‰ 0 | data) = 1
ğ½
ğ½
âˆ‘
ğ‘—=1
ğ¼(ğ‘‹ğ‘–âˆˆğ‘€(ğ‘—)).
Here ğ½is the total number of models that we sample using MCMC; each model is
denoted as ğ‘€(ğ‘—) (some models may repeat themselves in the sample). We count
the frequency of variable ğ‘‹ğ‘–occuring in model ğ‘€(ğ‘—), and divide this number by
the total number of models ğ½. This is a frequentist approach to approximate
the posterior probability of including ğ‘‹ğ‘–after seeing the data.
When all points are on the 45 degree diagonal, we say that the posterior inclusion
probability of each variable from MCMC have converged well enough to the
theoretical posterior inclusion probability.
We can also use diagnostics function to see whether the model posterior prob-
ability has converged:
diagnostics(crime.ZS, type = "model", col = "blue", pch = 16, cex = 1.5)
0.000
0.005
0.010
0.015
0.020
0.000
0.005
0.010
0.015
0.020
Convergence Plot: Posterior Model Probabilities
p(M | Y) (renormalized)
p(M | Y) (MCMC)
We can see that some of the points still fall slightly away from the 45 degree
diagonal line.
This may suggest we should increase the number of MCMC
iterations. We may do that by imposing the argument on MCMC.iterations
inside the bas.lm function

180
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
# Re-run regression using larger number of MCMC iterations
crime.ZS = bas.lm(y ~ ., data = UScrime,
prior = "ZS-null", modelprior = uniform(),
method = "MCMC", MCMC.iterations = 10 ^ 6)
# Plot diagnostics again
diagnostics(crime.ZS, type = "model", col = "blue", pch = 16, cex = 1.5)
0.000
0.005
0.010
0.015
0.020
0.000
0.005
0.010
0.015
0.020
Convergence Plot: Posterior Model Probabilities
p(M | Y) (renormalized)
p(M | Y) (MCMC)
With more number of iterations, we see that most points stay in the 45 de-
gree diagonal line, meaing the posterior inclusion probability from the MCMC
method has mostly converged to the theoretical posterior inclusion probability.
We will next look at four other plots of the BAS object, crime.ZS.
Residuals Versus Fitted Values Using BMA
The first plot is the residuals over the fitted value under Bayesian model aver-
aging results.
plot(crime.ZS, which = 1, add.smooth = F,
ask = F, pch = 16, sub.caption="", caption="")
abline(a = 0, b = 0, col = "darkgrey", lwd = 2)

8.3. R DEMO ON BAS PACKAGE
181
6.0
6.5
7.0
7.5
âˆ’0.4
âˆ’0.2
0.0
0.2
0.4
Predictions under BMA
Residuals
46
11
22
We can see that the residuals lie around the dash line ğ‘¦= 0, and has a constant
variance. Observations 11, 22, and 46 may be the potential outliers, which are
indicated in the plot.
Cumulative Sampled Probability
The second plot shows the cumulative sampled model probability.
plot(crime.ZS, which=2, add.smooth = F, sub.caption="", caption="")
0
2000
4000
6000
8000
0.0
0.2
0.4
0.6
0.8
1.0
Model Search Order
Cumulative Probability
We can see that after we have discovered about 5,000 unique models with MCMC
sampling, the probability is starting to level off, indicating that these additional
models have very small probability and do not contribute substantially to the
posterior distribution. These probabilities are proportional to the product of
marginal likelihoods of models and priors, ğ‘(data | ğ‘€ğ‘š)ğ‘(ğ‘€ğ‘š), rather than
Monte Carlo frequencies.
Model Complexity
The third plot is the model size versus the natural log of the marginal likelihood,
or the Bayes factor, to compare each model to the null model.
plot(crime.ZS, which=3, ask=F, caption="", sub.caption="")

182
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
5
10
15
0
5
10
15
20
Model Dimension
log(Marginal)
We see that the models with the highest Bayes factors or logs of marginal
likelihoods have around 8 or 9 predictors. The null model has a log of marginal
likelihood of 0, or a Bayes factor of 1.
Marginal Inclusion Probability
Finally, we have a plot showing the importance of different predictors.
plot(crime.ZS, which = 4, ask = F, caption = "", sub.caption = "",
col.in = "blue", col.ex = "darkgrey", lwd = 3)
0.0
0.2
0.4
0.6
0.8
1.0
Marginal Inclusion Probability
Intercept
M
So
Ed
Po1
Po2
LF
M.F
Pop
NW
U1
U2
GDP
Ineq
Prob
Time
The lines in blue correspond to the variables where the marginal posterior in-
clusion probability (pip), is greater than 0.5, suggesting that these variables are
important for prediction. The variables represented in grey lines have posterior
inclusion probability less than 0.5. Small posterior inclusion probability may
arise when two or more variables are highly correlated, similar to large ğ‘-values
with multicollinearity. So we should be cautious to use these posterior inclusion
probabilities to eliminate variables.
Model Space Visualization
To focus on the high posterior probability models, we can look at the image of
the model space.

8.3. R DEMO ON BAS PACKAGE
183
image(crime.ZS, rotate = F)
## Warning in par(par.old): argument 1 does not name a graphical parameter
1.223
1.172
0.742
0.596
0.438
0.24
0.13
1
2
3
4
5
6
7
8
10
13
18
Model Rank
Log Posterior Odds
Time
Ineq
U2
NW
M.F
Po2
Ed
M
By default, we only include the top 20 models. An interesting feature of this
plot is, that whenever Po1, the police expenditures in 1960, is included, Po2, the
police expenditures in 1959, will be excluded from the model, and vice versa.
out = cor(UScrime$Po1, UScrime$Po2)
out
## [1] 0.9933688
Calculating the correlation between the two variables, we see that that Po1 and
Po2 are highly correlated with positive correlation 0.993.
8.3.3
Posterior Uncertainty in Coeï¬€icients
Due to the interesting inclusion relationship between Po1 and Po2 in the top
20 models, we extract the two coeï¬€icients under Bayesian model averaging and
take a look at the plots for the coeï¬€icients for Po1 and Po2.
# Extract coefficients
coef.ZS=coef(crime.ZS)

184
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
# Po1 and Po2 are in the 5th and 6th columns in UScrime
par(mfrow = c(1,2))
plot(coef.ZS, subset = c(5:6),
col.lab = "darkgrey", col.axis = "darkgrey", col = "darkgrey", ask = F)
âˆ’4
âˆ’2
0
2
4
6
0.0
0.2
0.4
0.6
Po1
âˆ’4
âˆ’2
0
2
4
0.0
0.1
0.2
0.3
0.4
0.5
Po2
Under Bayesian model averaging, there is more mass at 0 for Po2 than Po1,
giving more posterior inclusion probability for Po1. This is also the reason why
in the marginal posterior plot of variable importance, Po1 has a blue line while
Po2 has a grey line. When Po1 is excluded, the distributions of other coeï¬€icients
in the model, except the one for Po2, will have similar distributions as when both
Po1 and Po2 are in the model. However, when both predictors are included, the
adjusted coeï¬€icient for Po2 has more support on negative values, since we are
over compensating for having both variables included in the model. In extreme
cases of correlations, one may find that the coeï¬€icient plot is multimodal. If this
is the case, the posterior mean may not be in the highest probability density
credible interval, and this mean is not necessarily an informative summary. We
will discuss more in the next section about making decisions on highly correlated
variables.
We can read the credible intervals of each variable using the confint function
on the coeï¬€icient object coef.ZS of the model. Here we round the results in 4
decimal places.
round(confint(coef.ZS), 4)
##
2.5%
97.5%
beta
## Intercept
6.6667 6.7815
6.7249
## M
0.0000 2.1826
1.1415

8.3. R DEMO ON BAS PACKAGE
185
## So
-0.0565 0.3007
0.0355
## Ed
0.5723 3.1648
1.8571
## Po1
0.0000 1.4754
0.6061
## Po2
-0.2069 1.4856
0.3139
## LF
-0.4630 1.0747
0.0578
## M.F
-2.3853 1.6544 -0.0254
## Pop
-0.1317 0.0045 -0.0224
## NW
0.0000 0.1672
0.0666
## U1
-0.5274 0.3393 -0.0243
## U2
-0.0010 0.6670
0.2058
## GDP
-0.0574 1.1972
0.2050
## Ineq
0.6799 2.1315
1.3900
## Prob
-0.4110 0.0000 -0.2152
## Time
-0.5141 0.0619 -0.0847
## attr(,"Probability")
## [1] 0.95
## attr(,"class")
## [1] "confint.bas"
8.3.4
Prediction
We can use the usual predict function that we used for lm objects to obtain
prediction from the BAS object crime.ZS. However, since we have different mod-
els to choose from under the Bayesian framework, we need to first specify which
particular model we use to obtain the prediction. For example, if we would like
to use the Bayesian model averaging results for coeï¬€icients to obtain predic-
tions, we would specify the estimator argument in the predict function like
the following
crime.BMA = predict(crime.ZS, estimator = "BMA", se.fit = TRUE)
The fitted values can be obtained using the fit attribute of crime.BMA. We
have transposed the fitted values into a vector to better present all the values.
fitted = crime.BMA$fit
as.vector(fitted)
##
[1] 6.661531 7.298609 6.179419 7.610302 7.054871 6.514362 6.784467 7.266656
##
[9] 6.629442 6.601327 7.054717 6.570263 6.472661 6.582066 6.556847 6.905026
## [17] 6.229216 6.810068 6.943124 6.961682 6.608963 6.429164 6.898947 6.777040
## [25] 6.406254 7.401147 6.019618 7.156438 7.090167 6.500431 6.208839 6.605832
## [33] 6.797882 6.819997 6.625456 7.028662 6.793366 6.364096 6.603432 7.044931
## [41] 6.548093 6.046280 6.929689 7.005334 6.235807 6.609527 6.829955
We may use these fitted values for further error calculations. We will talk about
decision making on models and how to obtain predictions under different models
in the next section.

186
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
8.4
Decision Making Under Model Uncertainty
We are closing this chapter by presenting the last topic, decision making un-
der model uncertainty. We have seen that under the Bayesian framework, we
can use different prior distributions for coeï¬€icients, different model priors for
models, and we can even use stochastic exploration methods for complex model
selections. After selecting these coeï¬€icient priors and model priors, we can ob-
tain the marginal posterior inclusion probability for each variable in the full
model, which may provide some information about whether or not to include
a particular variable in the model for further model analysis and predictions.
With all the information presented in the results, which model would be the
most appropriate model?
In this section, we will talk about different methods for selecting models and
decision making for posterior distributions and predictions. We will illustrate
this process using the US crime data UScrime as an example and process it
using the BAS package.
We first prepare the data as in the last section and run bas.lm on the full model
data(UScrime, package="MASS")
# take the natural log transform on the variables except the 2nd column `So`
UScrime[, -2] = log(UScrime[, -2])
# run Bayesian linear regression
library(BAS)
crime.ZS =
bas.lm(y ~ ., data = UScrime,
prior = "ZS-null", modelprior = uniform())
8.4.1
Model Choice
For Bayesian model choice, we start with the full model, which includes all the
predictors. The uncertainty of selecting variables, or model uncertainty that
we have been discussing, arises when we believe that some of the explanatory
variables may be unrelated to the response variable. This corresponds to setting
a regression coeï¬€icient ğ›½ğ‘—to be exactly zero. We specify prior distributions that
reflect our uncertainty about the importance of variables. We then update the
model based on the data we obtained, resulting in posterior distributions over
all models and the coeï¬€icients and variances within each model.
Now the question has become, how to select a single model from the posterior
distribution and use it for furture inference?
What are the objectives from
inference?
BMA Model
We do have a single model, the one that is obtained by averaging all models using
their posterior probabilities, the Bayesian model averaging model, or BMA. This

8.4. DECISION MAKING UNDER MODEL UNCERTAINTY
187
is referred to as a hierarchical model and it is composed of many simpler models
as building blocks. This represents the full posterior uncertainty after seeing
the data.
We can obtain the posterior predictive mean by using the weighted average of
all of the predictions from each sub model
Ì‚ğœ‡= ğ¸[ Ì‚ğ‘Œ| data] =
âˆ‘
ğ‘€ğ‘šâˆˆmodel space
Ì‚ğ‘ŒÃ— ğ‘(ğ‘€ğ‘š| data).
This prediction is the best under the squared error loss ğ¿2.
From BAS, we
can obtain predictions and fitted values using the usual predict and fitted
functions. To specify which model we use for these results, we need to include
argument estimator.
crime.BMA = predict(crime.ZS, estimator = "BMA")
mu_hat = fitted(crime.ZS, estimator = "BMA")
crime.BMA, the object obtained by the predict function, has additional slots
storing results from the BMA model.
names(crime.BMA)
##
[1] "fit"
"Ybma"
"Ypred"
"postprobs"
"se.fit"
##
[6] "se.pred"
"se.bma.fit"
"se.bma.pred" "df"
"best"
## [11] "bestmodel"
"best.vars"
"estimator"
Plotting the two sets of fitted values, one obtained from the fitted function,
another obtained from the fit attribute of the predict object crime.BMA, we
see that they are in perfect agreement.
# Load library and prepare data frame
library(ggplot2)
output = data.frame(mu_hat = mu_hat, fitted = crime.BMA$fit)
# Plot result from `fitted` function and result from `fit` attribute
ggplot(data = output, aes(x = mu_hat, y = fitted)) +
geom_point(pch = 16, color = "blue", size = 3) +
geom_abline(intercept = 0, slope = 1) +
xlab(expression(hat(mu[i]))) + ylab(expression(hat(Y[i])))

188
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
6.0
6.5
7.0
7.5
6.0
6.5
7.0
7.5
Âµi^
Yi^
Highest Probability Model
If our objective is to learn what is the most likely model to have generated the
data using a 0-1 loss ğ¿0, then the highest probability model (HPM) is optimal.
crime.HPM = predict(crime.ZS, estimator = "HPM")
The variables selected from this model can be obtained using the bestmodel
attribute from the crime.HPM object. We extract the variable names from the
crime.HPM
crime.HPM$best.vars
## [1] "Intercept" "M"
"Ed"
"Po1"
"NW"
"U2"
## [7] "Ineq"
"Prob"
"Time"
We see that, except the intercept, which is always in any models, the highest
probability model also includes M, percentage of males aged 14-24; Ed, mean
years of schooling; Po1, police expenditures in 1960; NW, number of non-whites
per 1000 people; U2, unemployment rate of urban males aged 35-39; Ineq, in-
come inequlity; Prob, probability of imprisonment, and Time, average time in
state prison.
To obtain the coeï¬€icients and their posterior means and posterior standard
deviations, we tell give an optional argument to coef to indicate that we want
to extract coeï¬€icients under the HPM.
# Select coefficients of HPM
# Posterior means of coefficients
coef.crime.ZS = coef(crime.ZS, estimator="HPM")
coef.crime.ZS
##
##
Marginal Posterior Summaries of Coefficients:

8.4. DECISION MAKING UNDER MODEL UNCERTAINTY
189
##
##
Using
HPM
##
##
Based on the top
1 models
##
post mean
post SD
post p(B != 0)
## Intercept
6.72494
0.02623
1.00000
## M
1.42422
0.42278
0.85357
## So
0.00000
0.00000
0.27371
## Ed
2.14031
0.43094
0.97466
## Po1
0.82141
0.15927
0.66516
## Po2
0.00000
0.00000
0.44901
## LF
0.00000
0.00000
0.20224
## M.F
0.00000
0.00000
0.20497
## Pop
0.00000
0.00000
0.36961
## NW
0.10491
0.03821
0.69441
## U1
0.00000
0.00000
0.25258
## U2
0.27823
0.12492
0.61494
## GDP
0.00000
0.00000
0.36012
## Ineq
1.19269
0.27734
0.99654
## Prob
-0.29910
0.08724
0.89918
## Time
-0.27616
0.14574
0.37180
We can also obtain the posterior probability of this model using
postprob.HPM = crime.ZS$postprobs[crime.HPM$best]
postprob.HPM
## [1] 0.01824728
we see that this highest probability model has posterior probability of only
0.018. There are many models that have comparable posterior probabilities. So
even this model has the highest posterior probability, we are still pretty unsure
about whether it is the best model.
Median Probability Model
Another model that is frequently reported, is the median probability model
(MPM). This model includes all predictors whose marginal posterior inclusion
probabilities are greater than 0.5.
If the variables are all uncorrelated, this
will be the same as the highest posterior probability model. For a sequence of
nested models such as polynomial regression with increasing powers, the median
probability model is the best single model for prediction.
However, since in the US crime example, Po1 and Po2 are highly correlated, we
see that the variables included in MPM are slightly different than the variables
included in HPM.
crime.MPM = predict(crime.ZS, estimator = "MPM")
crime.MPM$best.vars

190
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
## [1] "Intercept" "M"
"Ed"
"Po1"
"NW"
"U2"
## [7] "Ineq"
"Prob"
As we see, this model only includes 7 variables, M, Ed, Po1, NW, U2, Ineq, and
Prob. It does not include Time variable as in HPM.
When there are correlated predictors in non-nested models, MPM in general
does well. However, if the correlations among variables increase, MPM may
miss important variables as the correlations tend to dilute the posterior inclusing
probabilities of related variables.
To obtain the coeï¬€icients in the median probability model, we specify that the
estimator is now â€œMPMâ€:
# Obtain coefficients of the
Median Probabilty Model
coef(crime.ZS, estimator = "MPM")
##
##
Marginal Posterior Summaries of Coefficients:
##
##
Using
MPM
##
##
Based on the top
1 models
##
post mean
post SD
post p(B != 0)
## Intercept
6.72494
0.02713
1.00000
## M
1.46180
0.43727
1.00000
## So
0.00000
0.00000
0.00000
## Ed
2.30642
0.43727
1.00000
## Po1
0.87886
0.16204
1.00000
## Po2
0.00000
0.00000
0.00000
## LF
0.00000
0.00000
0.00000
## M.F
0.00000
0.00000
0.00000
## Pop
0.00000
0.00000
0.00000
## NW
0.08162
0.03743
1.00000
## U1
0.00000
0.00000
0.00000
## U2
0.31053
0.12816
1.00000
## GDP
0.00000
0.00000
0.00000
## Ineq
1.18815
0.28710
1.00000
## Prob
-0.18401
0.06466
1.00000
## Time
0.00000
0.00000
0.00000
Best Predictive Model
If our objective is prediction from a single model, the best choice is to find the
model whose predictions are closest to those given by BMA. â€œClosestâ€ could
be based on squared error loss for predictions, or be based on any other loss
functions. Unfortunately, there is no nice expression for this model. However,
we can still calculate the loss for each of our sampled models to try to identify
this best predictive model, or BPM.

8.4. DECISION MAKING UNDER MODEL UNCERTAINTY
191
Using the squared error loss, we find that the best predictive model is the one
whose predictions are closest to BMA.
crime.BPM = predict(crime.ZS, estimator = "BPM")
crime.BPM$best.vars
##
[1] "Intercept" "M"
"So"
"Ed"
"Po1"
"Po2"
##
[7] "M.F"
"NW"
"U2"
"Ineq"
"Prob"
The best predictive model includes not only the 7 variables that MPM includes,
but also M.F, number of males per 1000 females, and Po2, the police expenditures
in 1959.
Using the se.fit = TRUE option with predict we can calculate standard de-
viations for the predictions or for the mean. Then we can use this as input for
the confint function for the prediction object. Here we only show the results
of the first 20 data points.
crime.BPM = predict(crime.ZS, estimator = "BPM", se.fit = TRUE)
crime.BPM.conf.fit = confint(crime.BPM, parm = "mean")
crime.BPM.conf.pred = confint(crime.BPM, parm = "pred")
cbind(crime.BPM$fit, crime.BPM.conf.fit, crime.BPM.conf.pred)
##
2.5%
97.5%
mean
2.5%
97.5%
pred
##
[1,] 6.668988 6.513238 6.824738 6.668988 6.258715 7.079261 6.668988
##
[2,] 7.290854 7.151787 7.429921 7.290854 6.886619 7.695089 7.290854
##
[3,] 6.202166 6.039978 6.364354 6.202166 5.789406 6.614926 6.202166
##
[4,] 7.661307 7.490608 7.832006 7.661307 7.245129 8.077484 7.661307
##
[5,] 7.015570 6.847647 7.183493 7.015570 6.600523 7.430617 7.015570
##
[6,] 6.469547 6.279276 6.659818 6.469547 6.044966 6.894128 6.469547
##
[7,] 6.776133 6.555130 6.997135 6.776133 6.336920 7.215346 6.776133
##
[8,] 7.299560 7.117166 7.481955 7.299560 6.878450 7.720670 7.299560
##
[9,] 6.614927 6.482384 6.747470 6.614927 6.212890 7.016964 6.614927
## [10,] 6.596912 6.468988 6.724836 6.596912 6.196374 6.997449 6.596912
## [11,] 7.032834 6.877582 7.188087 7.032834 6.622750 7.442918 7.032834
## [12,] 6.581822 6.462326 6.701317 6.581822 6.183896 6.979748 6.581822
## [13,] 6.467921 6.281998 6.653843 6.467921 6.045271 6.890571 6.467921
## [14,] 6.566239 6.403813 6.728664 6.566239 6.153385 6.979092 6.566239
## [15,] 6.550129 6.388987 6.711270 6.550129 6.137779 6.962479 6.550129
## [16,] 6.888592 6.746097 7.031087 6.888592 6.483166 7.294019 6.888592
## [17,] 6.252735 6.063944 6.441526 6.252735 5.828815 6.676654 6.252735
## [18,] 6.795764 6.564634 7.026895 6.795764 6.351369 7.240160 6.795764
## [19,] 6.945687 6.766289 7.125086 6.945687 6.525866 7.365508 6.945687
## [20,] 7.000331 6.840374 7.160289 7.000331 6.588442 7.412220 7.000331
## [...]
The option estimator = "BPM is not yet available in coef(), so we will need to
work a little harder to get the coeï¬€icients by refitting the BPM. First we need
to extract a vector of zeros and ones representing which variables are included

192
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
in the BPM model.
# Extract a binary vector of zeros and ones for the variables included
# in the BPM
BPM = as.vector(which.matrix(crime.ZS$which[crime.BPM$best],
crime.ZS$n.vars))
BPM
##
[1] 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0
Next, we will refit the model with bas.lm using the optional argument
bestmodel = BPM. In general, this is the starting model for the stochastic
search, which is helpful for starting the MCMC. We will also specify that want
to have 1 model by setting n.models = 1. In this way, bas.lm starts with the
BPM and fits only that model.
# Re-run regression and specify `bestmodel` and `n.models`
crime.ZS.BPM = bas.lm(y ~ ., data = UScrime,
prior = "ZS-null",
modelprior = uniform(),
bestmodel = BPM, n.models = 1)
Now since we have only one model in our new object representing the BPM, we
can use the coef function to obtain the summaries.
# Obtain coefficients of MPM
coef(crime.ZS.BPM)
##
##
Marginal Posterior Summaries of Coefficients:
##
##
Using
BMA
##
##
Based on the top
1 models
##
post mean
post SD
post p(B != 0)
## Intercept
6.72494
0.02795
1.00000
## M
1.28189
0.49219
1.00000
## So
0.09028
0.11935
1.00000
## Ed
2.24197
0.54029
1.00000
## Po1
0.70543
0.75949
1.00000
## Po2
0.16669
0.76781
1.00000
## LF
0.00000
0.00000
0.00000
## M.F
0.55521
1.22456
1.00000
## Pop
0.00000
0.00000
0.00000
## NW
0.06649
0.04244
1.00000
## U1
0.00000
0.00000
0.00000
## U2
0.28567
0.13836
1.00000
## GDP
0.00000
0.00000
0.00000
## Ineq
1.15756
0.30841
1.00000

8.4. DECISION MAKING UNDER MODEL UNCERTAINTY
193
## Prob
-0.21012
0.07452
1.00000
## Time
0.00000
0.00000
0.00000
Note the posterior probabilities that coeï¬€icients are zero is either zero or one
since we have selected a model.
Comparison of Models
After discussing all 4 different models, let us compare their prediction results.
# Set plot settings
par(cex = 1.8, cex.axis = 1.8, cex.lab = 2, mfrow = c(2,2), mar = c(5, 5, 3, 3),
col.lab = "darkgrey", col.axis = "darkgrey", col = "darkgrey")
# Load library and plot paired-correlations
library(GGally)
ggpairs(data.frame(HPM = as.vector(crime.HPM$fit),
MPM = as.vector(crime.MPM$fit),
BPM = as.vector(crime.BPM$fit),
BMA = as.vector(crime.BMA$fit)))
Corr:
0.991***
Corr:
0.990***
Corr:
0.998***
Corr:
0.993***
Corr:
0.993***
Corr:
0.994***
HPM
MPM
BPM
BMA
HPM
MPM
BPM
BMA
6.0
6.5
7.0
7.5
6.0
6.5
7.0
7.5
6.0
6.5
7.0
7.5
6.0
6.5
7.0
7.5
0.0
0.3
0.6
0.9
6.0
6.5
7.0
7.5
6.0
6.5
7.0
7.5
6.0
6.5
7.0
7.5
From the above paired correlation plots, we see that the correlations among
them are extremely high. As expected, the single best predictive model (BPM)
has the highest correlation with MPM, with a correlation of 0.998. However,
the highest posterior model (HPM) and the Bayesian model averaging model
(BMA) are nearly equally as good.
8.4.2
Prediction with New Data
Using the newdata option in the predict function, we can obtain prediction
from a new data set. Here we pretend that UScrime is an another new data set,
and we use BMA to obtain the prediction of new observations. Here we only
show the results of the first 20 data points.

194
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC
BMA.new = predict(crime.ZS, newdata = UScrime, estimator = "BMA",
se.fit = TRUE, nsim = 10000)
crime.conf.fit.new = confint(BMA.new, parm = "mean")
crime.conf.pred.new = confint(BMA.new, parm = "pred")
# Show the combined results compared to the fitted values in BPM
cbind(crime.BPM$fit, crime.conf.fit.new, crime.conf.pred.new)
##
2.5%
97.5%
mean
2.5%
97.5%
pred
##
[1,] 6.668988 6.516973 6.815005 6.661770 6.242515 7.067698 6.661770
##
[2,] 7.290854 7.130743 7.452047 7.298827 6.869772 7.714767 7.298827
##
[3,] 6.202166 5.958549 6.400221 6.179308 5.719030 6.624379 6.179308
##
[4,] 7.661307 7.364186 7.821790 7.610585 7.150669 8.043400 7.610585
##
[5,] 7.015570 6.848175 7.257065 7.054238 6.611699 7.503637 7.054238
##
[6,] 6.469547 6.292949 6.751244 6.514064 6.044151 6.956464 6.514064
##
[7,] 6.776133 6.519746 7.078274 6.784846 6.284624 7.270964 6.784846
##
[8,] 7.299560 7.042870 7.488791 7.266344 6.832924 7.705520 7.266344
##
[9,] 6.614927 6.481290 6.779405 6.629448 6.213962 7.031541 6.629448
## [10,] 6.596912 6.457099 6.739807 6.601246 6.189525 7.015332 6.601246
## [11,] 7.032834 6.871894 7.241696 7.055003 6.634697 7.500822 7.055003
## [12,] 6.581822 6.424074 6.720593 6.570625 6.164031 6.989779 6.570625
## [13,] 6.467921 6.208733 6.711778 6.472327 6.011658 6.936831 6.472327
## [14,] 6.566239 6.394331 6.767615 6.582374 6.163065 7.021503 6.582374
## [15,] 6.550129 6.363211 6.758263 6.556880 6.113900 7.005157 6.556880
## [16,] 6.888592 6.744583 7.064158 6.905017 6.482980 7.319464 6.905017
## [17,] 6.252735 5.983572 6.468477 6.229073 5.787267 6.697356 6.229073
## [18,] 6.795764 6.538742 7.093427 6.809572 6.342467 7.290903 6.809572
## [19,] 6.945687 6.746839 7.126626 6.943294 6.509860 7.362240 6.943294
## [20,] 7.000331 6.775210 7.149052 6.961980 6.540823 7.388084 6.961980
## [...]
8.5
Summary
In this chapter, we have introduced one of the common stochastic exploration
methods, Markov Chain Monte Carlo, to explore the model space to obtain
approximation of posterior probability of each model when the model space
is too large for theoretical enumeration. We see that model selection can be
sensitive to the prior distributions of coeï¬€icients, and introduced Zellnerâ€™s ğ‘”-
prior so that we have to elicit only one hyper-parameter to specify the prior.
Still model selection can be sensitive to the choice of ğ‘”where values that are
too large may unintentially lead to the null model receiving high probability in
Bartlettâ€™s paradox. To resolve this and other paradoxes related to the choice
of ğ‘”, we recommend default choices that have improved robustness to prior
misspecification such as the unit information ğ‘”-prior, the Zellner-Siow Cauchy
prior, and the hyper-ğ‘”/ğ‘›prior.

8.5. SUMMARY
195
We then demonstrated a multiple linear regression process using the BAS package
and the US crime data UScrime using the Zellner-Siow cauchy prior, and have
tried to understand the importance of variables. Finally, we have compared the
prediction results from different models, such as the ones from Bayesian model
average (BMA), the highest probability model (HPM), the median probability
model (MPM), and the best predictive model (BPM). For the comparison, we
have used the Zellner-Siow Cauchy prior. But of course there is not one single
best prior that is the best overall.
If you do have prior information about
a variable, you should include it.
If you expect that there should be many
predictors related to the response variable ğ‘Œ, but that each has a small effect,
an alternate prior may be better. Also, think critically about whether model
selection is important. If you believe that all the variables should be relevant
but are worried about over fitting, there are alternative priors that will avoid
putting probabilities on coeï¬€icients that are exactly zero and will still prevent
over fitting by shrinkage of coeï¬€icients to prior means. Examples include the
Bayesian lasso or Bayesian horseshoe.
There are other forms of model uncertainty that you may want to consider, such
as linearity in the relationship between the predictors and the response, uncer-
tainty about the presence of outliers, and uncertainty about the distribution of
the response. These forms of uncertainty can be incorporated by expanding the
models and priors similar to what we have covered here.
Multiple linear regression is one of the most widely used statistical methods,
however, this is just the tip of the iceberg of what you can do with Bayesian
methods.

196
CHAPTER 8. STOCHASTIC EXPLORATIONS USING MCMC

Bibliography
Chaloner, K. and Brant, R. (1988). A bayesian approach to outlier detection
and residual analysis. Biometrika, 75(4):651â€“659.
Hoff, P. D. (2009). A first course in Bayesian statistical methods. Springer
Science & Business Media.
Jeffreys, S. H. (1961). Theory of Probability: 3rd Edition. Clarendon Press.
Kass, R. E. and Raftery, A. E. (1995). Bayes factors. Journal of the American
Statistical Association, 90(430):773â€“795.
Venables, W. N. and Ripley, B. D. (2013). Modern applied statistics with S-
PLUS. Springer Science & Business Media.
197

