arXiv:2105.13669v1  [cs.LG]  28 May 2021
Measuring global properties of neural generative
model outputs via generating mathematical
objects
Bernt Ivar Utstøl Nødland
Norwegian Defence Research Establishment (FFI)
Instituttveien 20, 2007 Kjeller
Norway
bernt-ivar-utstol.nodland@ffi.no
May 31, 2021
Abstract
We train deep generative models on datasets of reﬂexive polytopes.
This enables us to compare how well the models have picked up on var-
ious global properties of generated samples. Our datasets are complete
in the sense that every single example, up to changes of coordinate, is in-
cluded in the dataset. Using this property we also perform tests checking
to what extent the models are merely memorizing the data. We also train
models on the same dataset represented in two diﬀerent ways, enabling us
to measure which form is easiest to learn from. We use these experiments
to show that deep generative models can learn to generate geometric ob-
jects with non-trivial global properties, and that the models learn some
underlying properties of the objects rather than simply memorizing the
data.
1
Introduction
In recent years techniques for generating artiﬁcal data has improved, particu-
larly by using deep learning techniques. Well-known examples of this are gener-
ative adversarial networks [GPAM+14], variational auto-encoders [KW14] and
generators based on the transformer [VSP+17].
Evaluating the performance
of most such models is a diﬃcult problem, since it is not clear which metrics
would accurately determine that such a generative model is performing well. A
salient property of outputs of these models, which partly motivates this paper,
is that frequently generated data locally looks like plausible data, while there is
some global mistake that reveals that the generated data is not really plausible.
Examples are generated images of people with 3 front teeth, or generated text
where the pronoun of a person changes throughout the text. We here propose to
1

train generative models on datasets of mathematical objects, all satisfying some
global properties. This enables us to measure whether the generative model has
picked up on the global properties, or if it simply generates locally plausible
data.
The datasets we consider in this paper already exists in the academical liter-
ature: In the intersection of mathematics and theoretical physics there has been
considerable interest in reﬂexive polytopes, which has led to the construction
of databases of reﬂexive polytopes [KS00],[Paf17],[Øb07]: A database contain-
ing a complete list of all smooth reﬂexive lattice polytopes of dimensions up to
nine, as well as a database of all reﬂexive lattice polytopes of dimensions up to
four. These datasets are large enough that they can be used to train high per-
forming deep learning models. They are also very well suited to our purposes of
checking global properties, since all polytopes of a given dimension share several
non-trivial global properties.
These datasets are complete in the sense that they include every single ex-
ample (up to changes of coordinate) satisfying the required properties, thus en-
suring that the training data is in fact accurately representative of the data in
question. This also enables us to study to what extent the models are memoriz-
ing the training data or are learning some underlying structure of the data: For
each generated sample satisfying the required properties we can check whether
the data deﬁning the sample is identical to the corresponding example in the
training data, or if it is a coordinate change of it.
There are two equivalent ways of giving the data of a polytope: Either as
the set of points satisfying some linear inequalities or as the convex hull of
some vertices. This property enables us to train generative models on these
diﬀerent representations of the data and check from which representation the
model learns most.
We believe that training neural networks on datasets of mathematical objects
is an interesting way to test hypotheses about generative models, because one
can check the mathematical properties of network outputs. The members of our
datasets are discrete objects, thus one might hope that a model’s performance
on them will be analogous to its performance on other discrete sources of data,
like text. A limitation of this approach is that there may be important ways
in which mathematical datasets are diﬀerent from real world data such as text.
On the other hand, the preciseness of mathematics gives some advantages over
real world data: Since the datasets are complete, the training data will be
completely representative of the actual data, and since the two equivalent ways
of specifying a polytope denote exactly the same object, we can check which
representation is easiest to learn from without worrying about impreciseness in
translating between them.
This paper is in addition a contribution to the exploration of how one could
use techniques from machine learning to problems in pure mathematics: We
train generative models on datasets of geometric objects and study to what ex-
tend the models can generate examples with similar properties. We speculate
that neural generative models might in the future be used to generate coun-
terexamples to mathematical conjectures, for instance in cases where generating
2

random examples is not straight-forward.
2
Related work
There is a large body of literature about how to evaluate generative models.
In particular for text models, which, by the discrete nature of their data, is
quite similar to that of discrete mathematical datasets, there are several diﬀer-
ent metrics one can use to measures performance, see the survey [CCG20] for
details. This paper is not strictly speaking about this problem, however it is
partly motivated by the fact that measuring performance of generative models
is diﬃcult, and suggests ways to measure how well diﬀerent model architectures
learn global properties.
The paper [SGHK19] is of a similar ﬂavour to ours in that it suggests us-
ing mathematical datasets to measure the performance of neural architectures.
However, they focus on whether a model can answer correctly mathematical
questions like arithmetic or solving equations. In a similar direction, there are
recently several other papers attempting to use deep learning to solve mathe-
matics questions. Some examples include the paper [LC20], which studies how
one can use deep learning models to solve diﬀerential equations, as well as how
to solve symbolic integration problems and the paper [CHL21], which studies
how to predict properties of diﬀerential equations using neural networks.
Szegedy [Sze20] argues that using techniques from natural language under-
standing is an essential step towards creating systems of autoformalization: com-
puter programs that can prove mathematical theorems. This is an ambitious
program, well outside the scope of this paper, however our paper is a small step
in the exploration of how techniques from natural language processing could be
used in computer programs that handles mathematical objects.
In a diﬀerent direction, this paper can also be considered a continuation
of already existing eﬀorts to explore how to apply deep learning techniques to
datasets in the intersection of geometry and physics. Several papers have al-
ready tried to use classiﬁcation techniques for mathematical datasets, including
datasets of Calabi-Yau manifolds (these are closely linked to reﬂexive polytopes):
This was initiated in [He17] and has been followed up in several papers, see for
instance the paper [He18] and the included references. These eﬀorts have so
far been mostly towards exploring to what extent deep learning models can
correctly predict properties of geometric objects from some deﬁning data.
3
Datasets
3.1
Motivation for using mathematical datasets
A hard problem in the study of generative models is the question of how one
should measure performance of the model. We suggest that an interesting way
of measuring the performance of generative model architectures would be to
test on datasets of mathematical objects: If one trains on a dataset of objects
3

all satisfying a property P, such that syntactically similar objects could very
well fail to satisfy property P, one can test to what extent a generative model
manages to pick up on the property P. It is not clear to what extent diﬀerences
in performance in picking up on properties of mathematical objects correspond
to similar diﬀerences in performance on other types of data (say text or images);
this would be an interesting future research question.
We are primarily interested in global properties. By the term "global prop-
erty" we heuristically mean a property that is a property of the whole object,
and not a property that is localized in a speciﬁc place. More mathematically
rigorous we are thinking of properties that are not local-to-global: A property
of an object is said to be local-to-global if it is the case that for some subdivi-
sion into local pieces, if every local piece has the property then the global piece
also has the property. In language an example of a local-to-global property is
that of being grammatically correct; if each sentence in a text is grammatically
correct then the entire text is grammatically correct. On the other hand, nar-
rative coherence is not a local-to-global property, since one easily can string
together texts that are each narratively coherent into a text that is not. When
we say global property in this paper we will mean a property that is either not
local-to-global, or for which the local property does not even make sense.
Another reason why using datasets of mathematical objects could be useful
is motivated by the following frequent challenge for deep learning models: One
trains a model on some dataset that is supposed to be representative of real
world data, however, in practice it is virtually impossible to collect such a rep-
resentative dataset. For this reason it is hard to tell which part of a model’s
mistakes come from the fact that the dataset does not accurately portray the un-
derlying data generating mechanism and what comes from the model’s inability
to learn the structure of the data. For some mathematical datasets (including
the ones we use in this paper) this problem can be avoided: If the class of
mathematical objects satisfying a property P is ﬁnite, then one could train the
dataset on the complete list (or some random fraction) of examples. Then no
mistake the model makes can be put down to bias in the distribution of the
training data.
3.2
Details on datasets of polytopes
In Appendix A we give mathematical background on polytopes and their prop-
erties. We essentially suggest two diﬀerent datasets of polytopes that can be
used to train deep generative models: The Kreuzer-Skarke database of relexive
polytopes of dimension 4 [KS00] and the database of smooth reﬂexive polytopes
of dimension up to 9 [Paf17]. In the latter case the dimensions 8 and 9 are
probably the most reasonable to train large deep models on, since there in those
cases are a large number of examples.
For the experiments in this paper we choose as our dataset all smooth re-
ﬂexive convex lattice polytopes of dimension 7 and of dimension 8.
This is
because these datasets are large enough to train good models, but small enough
that training do not take a lot of time on a single computer. The 7d dataset
4

contains 72256 samples, while the 8d dataset contains 749892 samples. Each
coordinate entry is regarded as a token, thus ”0” is a possible token (in fact the
most common) but also "-2" is one token. One could have chosen alternative
tokenizations, for instance by considering each inequality as a token or each
individual character a token. The vocabulary size will be around 30 with our
chosen convention. The max sequence length of the 7d dataset is 169 tokens and
219 tokens for the 8d dataset. If one would like to scale up the experiments of
this paper we consider both the 9-dimensional smooth reﬂexive polytopes and
the 4-dimensional reﬂexive polytopes to be interesting datasets, since these are
signiﬁcantly larger than our chosen datasets.
A data sample is a matrix, where each row corresponds to w ∈Zd corre-
sponding to the inequality 1 + w · x ≥0. Together all these inequalities deﬁne a
polytope. Each polytope P in the dataset satisfy the global properties of being
a lattice polytope, compact, reﬂexive, smooth and normal. Since the dataset is
complete, it is also true that any generated example satisfying all these proper-
ties will be in the training dataset, up to change of coordinates. In this case we
call the generated example "correct".
The properties cannot be seen independently of each other: If P is not a
lattice polytope we automatically have that P is not smooth. If P is a lattice
polytope deﬁned by inequalities of the form 1+wi ≥0, then it will automatically
be reﬂexive. If P is reﬂexive and smooth, then it is automatically normal (for
dim P ≤8). If P is not compact, then normality is not deﬁned, so we consider it
as not normal. If P is smooth, then by conjecture P is always normal, although
this is not proven in general. For generated polytopes it is then interesting to
check for the following properties:
• P is compact
• P is a lattice polytope (hence also reﬂexive)
• P is smooth
• P is normal
Above we assume that the polytope is deﬁned via inequalities. However,
there is also a diﬀerent way of giving the data of a convex lattice polytope: By
giving the vertices and stating that the polytope is the convex hull of these. We
also train generative models on a subset of the 7d dataset where we give the
polytope via vertices. By construction all samples will then automatically be
convex lattice polytopes. The properties we test for under this formulation is
therefore:
• P is reﬂexive
• P is smooth
• P is normal
Training on these two diﬀerent data representations can be considered anal-
ogous to stating the same semantic content in diﬀerent languages.
5

−1 0 0 0 0 0 0 0
0 0 0 0 0 0 0 −1
0 −1 0 0 0 0 0 0
0 0 0 0 −1 0 0 0
0 0 0 0 −1 1 0 1
0 0 0 0 0 −1 0 0
0 0 −1 0 0 0 0 0
0 0 0 −1 0 0 0 0
0 0 0 0 0 0 −1 0
1 1 1 1 0 −4 1 1
0 0 0 0 1 0 0 −1
−1 0 0 0 0 0 0 0
0 0 0 0 0 0 0 −1
0 −1 0 0 0 0 0 0
0 0 0 0 −1 0 0 0
0 0 0 0 −1 1 0 1
0 0 0 0 0 −1 0 0
0 0 −1 0 0 0 0 0
0 0 0 −1 0 0 0 0
0 0 0 0 0 0 −1 0
1 1 1 1 0 −4 1 1
0 0 0 0 1 0 1 −1
Figure 1: Each example in the dataset is a matrix of integers such as the one on
the left. The matrix on the right is not valid, since we changed one entry and
it turns out the new matrix does not satisfy the global properties.
3.3
Sensitivity to local changes
The data sample on the left in Fig. 1 is taken from the dataset. Thus it
deﬁnes a normal smooth reﬂexive lattice polytope. The data on the right is the
same as on the left, except one zero in the last row has been changed to a one.
It still deﬁnes a compact polytope, but it no longer has any of the properties
reﬂexive, smooth, normal, nor being a lattice polytope. This illustrates that
global properties of the data is highly non-trivial to deduce simply from the
local structure.
To give an indication of how volatile the properties are, we drew 1000 random
examples from the dataset and did a random change of either changing a single
0 to ±1 or changing a single ±1 to a 0. The distribution of the properties in
the modiﬁed polytopes were:
• Compact: 851
• Lattice polytope: 789
• Smooth: 227
• Normal: 642
• All: 147
Thus only 147 out of 1000 are still correct datasamples with a single small
change. With more than one change even less polytopes will be correct. This
indicates that a successful model has to understand global properties, since the
modiﬁed samples are locally plausible examples of the dataset.
6

4
Experiments
All models are trained with one polytope being one data sample.
We add
special start-of-sequence and end-of-sequence tokens to each sample and pad to
the ﬁxed maximum length. We train until the loss is reasonably stable, which is
somewhere in the range 100-800 epochs, depending on size of model and dataset.
We train with the Adam optimizer with learning rate 0.001. The models are
trained on a single Titan V GPU with approximately 110 hours of combined
training time. Checking the global properties for one list of 1000 8d polytopes
took about 6 hours using a single CPU. We do not do an extensive search of
hyperparameters, since the experiments are more intended as a proof of concept
than as a claim about the best possible performance. Thus we could probably
train models that were even better performing by increasing model sizes and
using other network architectures and traning procedures. We have chosen to
keep things as vanilla as possible, not least because we really only want a good
performing model: Since the size of the vocabulary is small and the training
datasets not extremely big one risks that a very high performing model simply
memorizes all the samples rendering the outputs uninteresting. Thus a good
model giving mostly reasonable outputs is what we wish to study.
This is
also the most reasonable analogy with other generative models, since in most
situations training a generative model on the complete list of possible examples
is not possible.
4.1
Generating polytopes
Our most fundamental experiments are training generative models on the 8d
dataset. After training the models we generate 1000 polytopes using each model
and use the open source computer algebra software systems Macaulay2 [GS] and
polymake [GJ00], [AGH+17] to check all the global properties in question. For
the generated polytopes satisfying all of the properties (which implies that they
are, up to change of coordinates, in the training set) we check whether they are
exact copies of their representative in the training set. We also check whether
they are the same as their representative up to reordering of rows. The deﬁning
inequalities is in reality a set and not a sequence, thus reordering the rows is the
simplest form of coordinate change. The models are trained on the data samples
as sequences, thus there are no a priori reason the model should "understand"
that the rows are really sets.
4.2
Baseline
To compare the results with a baseline we also do the following. We make a
simple model that generates samples based on sampling from the histogram of
tokens following the previous n tokens, where n is some natural number (we
choose n = 10 in our experiments). Thus this is a model that simply generates
the next token based on the frequency of tokens preceding the given token in
the training data. In particular this is by construction a local model. Therefore
7

it cannot use the complete history to generate the next token, hence it cannot
know the total length of the sequence, which leads to artiﬁcially short samples.
We remedy this by replacing the newline token to also include the line number,
thus the model cannot generate the end of sequence token before at least 8d + 1
steps (since there are no such samples in the dataset). This ﬁx will lead to
samples that have a chance of satisfying the properties and that looks locally
plausible.
4.3
Training on half the dataset
We also train models on half of the 7d and 8d datasets (chosen randomly) and
generate polytopes from the models. Then we can check whether the polytopes
generated by these models are to a large extent from the half it was trained on,
or whether they are equally from both halves. These tests give some indication
of to what extent the models are simply memorizing the dataset, or whether
they learn some underlying structures.
4.4
Training on convex hull representation
We also train models on the convex hull representation of the dataset and com-
pare the performance to that of the original hyperplane representation. For
these experiments we use the 7d dataset, since the samples are longer in the
convex hull representation, and thus require more computational resources both
to train the models and to check the properties of generated samples. In the
hyperplane representation, a reﬂexive polytope of dimension d can have at most
3d deﬁning inequalities [Cas06, Theorem 3]. For the number of vertices there
are no such simple upper bound , and in practice they are indeed much longer,
the longest example in the 7d dataset having more than 3000 tokens. We choose
the subset of the 7d dataset consisting of those examples that have less than
800 tokens in the convex hull representation, to avoid very long sequences. This
dataset has 39737 examples, thus it is approximately half the 7d dataset. We
train generative models on both this and the corresponding hyperplane dataset
and compare performance on the two datasets.
4.5
Implementation
We train recurrent networks with one or two LSTM layers. We use an embedding
dimension of 4 and hidden dimension 256 for each of the LSTM layers.
We also train transformer models that are stacked encoder layers. We choose
an embedding dimension of 128 and use 4 multi-attention heads and stack 4 such
encoder layers to get the full model.
The models are trained to predict the next token in the dataset. We generate
examples by sampling from the output probabilities until we generate the end-
of-sequence token. The sizes of the models are chosen so that the transformer
and the 2-layer LSTM models will have approximately the same number of
parameters.
8

5
Results
5.1
Training on the full 8d dataset
In Table 1 we see the how many of the generated samples satisﬁed the diﬀerent
global properties. In Table 2 we see how many of the correctly generated sam-
ples that equalled their representative in the training set precisely and up to
permutation of rows. In Table 3 we see how many generated examples satisﬁed
some choices of several properties at once.
Model
LSTM 1
LSTM 2
Transformer
Baseline
All properties
238
260
404
2
Compact
827
840
909
21
Lattice polyhedron
712
722
832
412
Smooth
270
304
440
178
Normal
590
613
755
2
Table 1: For each model we see how many of the 1000 generated samples that
satisfy the various properties, including the condition all of them at once.
Model
LSTM 1
LSTM 2
Transformer
Baseline
Copy
0/238
0/260
0/404
0/2
Permutation
36/238 = 15.1 %
58/260=22.3 %
77/404 = 19.1 %
0/2
Table 2: Any example satisfying all properties exist in the training dataset, up
to change of coordinates. Here we see how many of the generated samples are
exact copies of their representative in the training set and how many are copies
up to permutation of rows.
Model
LSTM 1
LSTM 2
Transformer
Baseline
Compact+lattice
590
613
755
2
Compact+lattice+normal
590
613
755
2
Compact+not smooth
589
580
505
19
Compact+lattice+not smooth
352
353
351
0
Table 3: We here see a breakdown of some of the intersections of the properties.
In other words, either how many examples that satisfy several of the properties,
or their negations. Note the equality of the two ﬁrst rows are purely empirical,
since mathematically there are no reason why they should be equal because
there exist (lots of) examples of reﬂexive lattice polytopes that are not normal
5.2
Training on half of the 8d dataset
For the models trained on half the 8d dataset the results for the properties of
generated samples is seen in Table 4 and intersections in Table 5.
9

Model
1-layer LSTM
2-layer LSTM
Transformer
All properties
187
244
361
Compact
783
834
908
Lattice polyhedron
667
706
851
Smooth
237
282
396
Normal
522
590
772
Table 4: For each model trained on half the dataset we see how many of the 1000
generated samples that satisfy the various properties, including the condition
all of them at once.
Model
1-layer LSTM
2-layer LSTM
Transformer
Compact+lattice
522
590
772
Compact+lattice+normal
522
590
772
Compact+not smooth
596
590
547
Compact+lattice+not smooth
335
346
411
Table 5: We here see a breakdown of some of the intersections of the properties
for the half 8d dataset.
The primary motivation for also training models on half the data is that it
enables us to measure to which half of the dataset generated samples belong.
For a given correctly generated sample we know that an equivalent representa-
tion of the polytope has to be in the training data. However, checking which
of the approximately 800 000 examples it is equivalent to is computationally
expensive: Doing so for one example takes approximately 1 day on the com-
puter these experiments were run on, thus we have not been able to do this
at large scale. However, we can quickly check the examples that are equal to
their original representative up to permutation of rows, seen in Table 6. To get
more substantial numbers we also train a transformer on half the 7d dataset and
generate 1000 polytopes, of which 319 are correct. For all of these we manage
to check which training sample a generated sample is equivalent to (checking
one example took about 1 hour) and ﬁnd that 179/319 (approximately 56 %)
of generated examples are from the half the model was trained on.
Model
LSTM 1
LSTM 2
Transformer
In half 8d dataset
15/28 = 53.6 %
16/37 = 43.2 %
42/73=57.5 %
Table 6: Among the generated samples that are equal to a permutation of the
representation in the full dataset, this table shows the fraction of samples that
were in the half of the dataset the model was actually trained on.
5.3
Convex hull representation
We train Transformer and 2-layer LSTM on the subset of 7d polytopes that are
less than 800 tokens in the convex hull representation and on the hyperplane
10

representation of the same subset. In Table 7 we see the results for the con-
vex hull representation, while Table 8 shows the corresponding results for the
hyperplane representation.
Model
2-layer LSTM
Transformer
All properties
7
285
Reﬂexive
7
285
Smooth
7
286
Normal
27
410
Table 7: For each model trained on the 7d convex hull dataset we see how many
of the 1000 generated samples that satisfy the various properties, including the
condition all of them at once.
Model
2-layer LSTM
Transformer
All properties
216
547
Compact
660
909
Lattice polyhedron
616
850
Smooth
288
583
Normal
421
784
Table 8: For each model trained on the subset of 7d hyperplane dataset that is
short in the convex hull representation we see how many of the 1000 generated
samples that satisfy the various properties, including the condition all of them
at once.
6
Discussion
We see that the models successfully manage to generate many examples with
all of the correct properties. This shows that the models manage to pick up
on global properties. The naive baseline using only local properties performs
signiﬁcantly worse than the neural models with almost no correct samples. We
also see that the transformer models generally perform better than the LSTM
models. Unsurprisingly some properties are shown to be much easier to under-
stand than others, with smoothness being the hardest. We also observe that the
models trained on half the 8d dataset generate almost the same (or even higher)
number of compact, normal and lattice polyhedra as the models trained on the
full dataset, but less smooth polytopes. Thus it seems that doubling the amount
of data improves performance on harder properties, but not on easier properties.
Several of the properties we study are expensive to actually compute: Our best
known methods to computationally check that a polytope is a lattice polytope
are highly non-trivial [Zie95, Chapter 1]. To check that a polytope is smooth
one would additionally compute the determinant of the edges emanating from
each vertex. Thus it is unlikely that the models actually manage to perform
11

operations approximating these computations. Rather, it seems more likely that
the network are relying on vague heuristic observations that correlate with the
properties. For instance, for a polyhedron to be compact it is necessary that
all variables of the deﬁning inequalities are bounded. In particular every vari-
able has to appear with both positive and negative coeﬃcient in the inequalities
deﬁning the polyhedron. This translates into the property that every column
has at least one positive and one negative entry. Thus, this is a necessary prop-
erty of all the samples, however it is not suﬃcient for the polyhedron being
compact. It seems reasonable to suspect that the model relies on this kind of
heuristic necessary patterns that correlates with the property in question, also
for the more complicated properties. Clearly it is possible to formulate simi-
lar heuristics for other properties and, maybe, given enough such patterns, one
manages to reliably generate examples with correct properties without actually
verifying the properties.
Somewhat surprisingly the models trained on the hyperplane representation
generated no examples of compact lattice polytopes that were not normal. This
might indicate that there could be some underlying mathematical reason why
lattice polytopes that are similar (in the vague sense of being generated by
our model) to smooth compact reﬂexive lattice polytopes are always normal.
We wonder whether understanding why we get no non-normal lattice polytopes
might reveal some underlying structure, that could potentially help in proving
that all smooth (reﬂexive) polytopes are normal? Empirical observations such
as this indicates that using generative models could be useful in the ﬁeld of
experimental mathematics: Unexpected properties of generated objects might
give one hints about new statements that one could subsequently try to prove.
We also observe that almost none of the generated samples are identical
to their representative in the dataset. This gives evidence that the model is
not simply memorizing the data, but rather understanding some underlying
structure. However, a substantial portion (roughly 1 in 5) of the samples are
equal to their representative up to permutation of rows. This proportion is way
higher than one would expect from a random permutation. This could be an
indication that the model does perform some amount of memorization. However,
the examples in the training data are generated via the algorithm of [Øb07] and
by using this there are systematic patterns in how the polytopes are represented
in the training data. Hence it is not very clear that a random permutation is a
reasonable comparison, and thus we are not able to draw very clear conclusions
from this.
The experiments using half the data, while not having enough data to be
very certain, indicate that the models are actually "understanding" some under-
lying structure, rather than simply memorizing the training data: The fraction
of correctly generated samples that are in the training data are only slightly
exceeding 50 % (56 % for the largest set of examples we managed to check). If
the model did a large amount of memorization one would expect to see a large
majority of samples from the half the models were trained on.
When given the dataset in the convex hull representation the models pick
up on signiﬁcantly less patterns than they do in the hyperplane representation.
12

The LSTM model in particular almost did not generate any examples with the
correct properties. An obvious reason for this is that the sequences are longer.
However, these models also had some factors in their favour, since the space of
possible objects are smaller. From these experiments one might suspect that
detecting properties is harder in the convex hull representation.
7
Future work
There are several instances where we think it would be interesting to try gen-
erative models on mathematical datasets. An obvious example is to use other
types of generative models such as generative adversarial networks and varia-
tional autoencoders. Another obvious extension is using generative models on
other matematical datasets, for instance on interesting continuous datasets. A
third idea is to investigate to what extent encoding group invariance in the model
improves performance: if one builds into the model that the rows are sets and
not sequences, or more ambitiously that there is a matrix group of coordinate
changes that the samples are invariant under, one would expect performance to
improve.
One of the motivations for working on this project, which we were not able
to complete, was to construct a counterexample to Oda’s conjecture in convex
geometry/algebraic geometry: Does there exist a smooth lattice polytope that
is not normal? [Oda08] The reason we believed this might be worth a try is that
generating random examples of smooth polytopes in not easy [Bru11]. This is
also a challenge for those who wishes to use generative models to generate new
examples, since there does not exist a large training set of arbitrary smooth
polytopes. We believe a reasonable future attempt would be to try transfer
learning: Pre-train a generative model to generate non-normal lattice polytopes
and then ﬁne-tune with smooth polytopes to maybe discover a counterexample.
Of course, it might be the case that Oda’s conjecture is true and no such example
exists. Nonetheless, we suspect that these techniques could in the future be
used for ﬁnding counterexamples in mathematics.
The paper [Wag21] is an
example of this, where counterexamples in graph theory are discovered using
neural generative models trained using reinforcement learning.
The behaviour of global properties of mathematical objects under the pre-
train then ﬁne-tune methodology could also be of independent interest: If one
pre-trains generative models on mathematical objects satisfying property P and
ﬁne-tunes on similar objects satisfying Q, where most examples that satisfy Q
does not satisfy P, does the model tend to generate objects satisfying both P
and Q or does it tend to generate examples satisfying Q and not P?
8
Conclusion
We have shown that neural generative models successfully can generate discrete
mathematical objects with interesting global properties. We have presented evi-
13

dence that indicates that the models actually pick up on some of the underlying
global structures. We believe that these techniques could be useful in both the
evaluation of generative models and in the ﬁeld of experimental mathematics.
References
[AGH+17]
Benjamin Assarf, Ewgenij Gawrilow, Katrin Herr, Michael Joswig,
Benjamin Lorenz, Andreas Paﬀenholz, and Thomas Rehn. Com-
puting convex hulls and counting integer points with polymake.
Math. Program. Comput., 9(1):1–38, 2017.
[BGT97]
Winfried Bruns, Joseph Gubeladze, and Ngô Viêt Trung. Normal
polytopes, triangulations, and Koszul algebras. J. Reine Angew.
Math., 485:123–160, 1997.
[Bru11]
Winfried Bruns. The quest for counterexamples in toric geometry,
2011. arXiv:1110.1840.
[Cas06]
Cinzia Casagrande. The number of vertices of a Fano polytope.
Ann. Inst. Fourier (Grenoble), 56(1):121–130, 2006.
[CCG20]
Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Evaluation
of text generation: A survey, 2020. arXiv:2006.14799.
[CHL21]
Francois Charton, Amaury Hayat, and Guillaume Lample. Learn-
ing advanced mathematical computations from examples. In In-
ternational Conference on Learning Representations, 2021.
[CLS11]
David A. Cox, John B. Little, and Henry K. Schenck. Toric vari-
eties, volume 124 of Graduate Studies in Mathematics. American
Mathematical Society, Providence, RI, 2011.
[GJ00]
Ewgenij Gawrilow and Michael Joswig. polymake: a framework
for analyzing convex polytopes. In Polytopes—combinatorics and
computation (Oberwolfach, 1997), volume 29 of DMV Sem., pages
43–73. Birkhäuser, Basel, 2000.
[GPAM+14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua
Bengio.
Generative adversarial nets.
In Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,
editors, Advances in Neural Information Processing Systems 27,
pages 2672–2680. Curran Associates, Inc., 2014.
[GS]
Daniel R. Grayson and Michael E. Stillman. Macaulay2, a soft-
ware system for research in algebraic geometry.
Available at
http://www.math.uiuc.edu/Macaulay2/.
14

[He17]
Yang-Hui
He.
Deep-learning
the
landscape,
2017.
arXiv:1706.02714.
[He18]
Yang-Hui He. The calabi-yau landscape: from geometry, to physics,
to machine-learning, 2018. arXiv:1812.02893.
[KS00]
Maximilian Kreuzer and Harald Skarke. Complete classiﬁcation of
reﬂexive polyhedra in four dimensions. Adv. Theor. Math. Phys.,
4, 03 2000.
[KW14]
Diederik P. Kingma and Max Welling. Auto-Encoding Variational
Bayes. In 2nd International Conference on Learning Representa-
tions, ICLR 2014, Banﬀ, AB, Canada, April 14-16, 2014, Confer-
ence Track Proceedings, 2014.
[LC20]
Guillaume Lample and François Charton. Deep learning for sym-
bolic mathematics. In International Conference on Learning Rep-
resentations, 2020.
[Oda08]
Tadao Oda. Problems on minkowski sums of convex lattice poly-
topes, 2008. arXiv:0812.1418.
[Paf17]
Andreas Paﬀenholz. polydb: A database for polytopes and related
objects, 2017. arXiv:1711.02936.
[SGHK19]
David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet
Kohli. Analysing mathematical reasoning abilities of neural models.
In International Conference on Learning Representations, 2019.
[Sze20]
Christian Szegedy. A promising path towards autoformalization
and general artiﬁcial intelligence. In Christoph Benzmüller and
Bruce Miller, editors, Intelligent Computer Mathematics, pages 3–
20, Cham, 2020. Springer International Publishing.
[VSP+17]
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.
Attention is all you need. In I. Guyon, U. V. Luxburg, S. Ben-
gio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
editors, Advances in Neural Information Processing Systems 30,
pages 5998–6008. Curran Associates, Inc., 2017.
[Wag21]
Adam Zsolt Wagner. Constructions in combinatorics via neural
networks, 2021. arXiv:2104.14516.
[Zie95]
Günter M. Ziegler. Lectures on polytopes, volume 152 of Graduate
Texts in Mathematics. Springer-Verlag, New York, 1995.
[Øb07]
Mikkel
Øbro.
Classiﬁcation
of
smooth
Fano
polytopes.
PhD
thesis,
University
of
Aarhus,
2007.
available
at
pure.au.dk/portal/ﬁles/41742384/imfphd2008moe.pdf.
15

A
Mathematical background on polytopes
Here we give a short introduction to the mathematics of polytopes and their
properties, required to understand the datasets we use.
Deﬁnition 1 A polyhedron is a subset of Rd deﬁned by a ﬁnite number of linear
inequalities. In other words a set of the form
{x ∈Rd|b + Ax ≥0},
for some matrix A and vector b. A polyhedron is called a lattice polyhedron if
all vertices of P are integer points.
The dimension of a polyhedron is deﬁned as the dimension of the smallest
linear subspace containing the polyhedron. A polyhedron is called convex if it
forms a convex subset of Rd.
Deﬁnition 2 A polyhedron which is compact (meaning it has ﬁnite volume) is
called a polytope.
In this paper we will study polyhedra up to lattice equivalence. In other
words we consider two polyhedra P and Q as isomorphic if they are equal
after some aﬃne coordinate change of Rd that equals a translation term plus
an element of GLd(Z). In other words a sum of an integer translation and a
change of basis matrix with integer entries having determinant equal to ±1.
Convex lattice polytopes has received a lot of attention in the intersection of
toric geometry and string theory. To explain this connection we need to deﬁne
the following property of a polytope:
Deﬁnition 3 To a lattice polyhedron P one can associate the dual polyhedron
P ∨deﬁned by:
P ∨= {u ∈Rd|x · u ≥−1 for all x ∈P},
where · is the ordinary scalar product of vectors. A lattice polyhedron is called
reﬂexive if the associated dual polyhedron is also a lattice polyhedron.
For a lattice polytope the condition that P ∨is also a lattice polytope is
equivalent to the condition P has a unique interior integer point. To a reﬂexive
polytope one can associate a toric Fano variety. Inside these toric Fano varieties
one can construct so-called Calabi-Yau manifolds.
According to superstring
theory the universe is a 10-dimensional manifold, where four dimensions are
space and time dimensions, while the remaining six dimensions corresponds to
a three-dimensional complex Calabi-Yau manifold (three complex dimensions
corresponds to six real dimensions).
The conjectural relationship between a
Calabi-Yau and the dual Calabi-Yau associated to the dual polytope is what
is known as mirror symmetry. Because almost all known examples of three-
dimensional Calabi-Yau manifolds live inside four-dimensional toric fano man-
ifolds, string theorists have been very interested in studying four-dimensional
16

reﬂexive polytopes This is the background for the construction of the Kreuzer-
Skarke database of all reﬂexive polytopes of dimensions 3 and 4. Parallel to this
development there has also been signiﬁcant interest in reﬂexive polytopes in the
mathematics community.
Deﬁnition 4 A lattice polyhedron is called smooth if for any vertex the minimal
integer points of the emanating edges form a Z-basis for the integer points of Rd.
In other words, there has to be exactly d edges emanating from each vertex and
the matrix containing the minimal integer vectors of these edges has determinant
±1. This condition is equivalent to the condition that the toric variety associated
to the polyhedron is a smooth manifold.
Of a given dimension there are only ﬁnitely many reﬂexive polytopes, up to
integer change of coordinates. Øbro developed an algorithm to ﬁnd all smooth
reﬂexive polytopes of a given dimension [Øb07] which was used to construct
the database of all smooth lattice polytopes up to dimension 9. An additional
property a lattice polytope can have is that of normality:
Deﬁnition 5 A lattice polytope P is called normal if it has the property that
all integer lattice points of any integer multiple lP is the sum of l integer lattice
points from P.
It is a famous open conjecture that states that any smooth polytope is nor-
mal [Oda08]. For reﬂexive polytopes this has been checked up to dimension 8;
thus all polytopes of dimension less than 8 in the database of smooth relexive
polytopes are normal.
Example 1 We here show an example of a 2-dimensional polytope. It can be
described as the set of points satisfying the inequalities below.
It is compact
since it has ﬁnite area. It is a lattice polytope since all its vertices are integer
points. We remark that the inequalities being deﬁned by inequalities with integer
coeﬃcients are far from being suﬃcient to imply that the vertices are integer
points. It is reﬂexive since the only interior integer point is the origin. It is
normal since all 2-dimensional lattice polytopes are normal [BGT97, Proposition
1.2.4]. We store the data deﬁning such a polytope in terms of the inequalities
deﬁning it, where we omit the constant 1, since the constant term always can be
assumed to equal 1 by [CLS11, Theorem 8.3.4]. Equivalently we could also store
it by recording the vertices: In that case we recover the polytope by taking the
convex hull of the vertices.
1 + y ≥0
1 −y ≥0
1 −x ≥0
1 + x ≥0
1 + x −y ≥0
1 −x + y ≥0
(1, 0)
(1, 1)
(0, 1)
(−1, 0)
(−1, −1)
(0, −1)
0
1
0
−1
−1 0
1
0
1 −1
−1 1
17

Left: deﬁning inequalities. Center: polytope drawn with vertices. Right: how
we store inequalities.
B
Examples
Here we give some examples of data samples. Below we see two examples from
the 7d dataset, both in the hyperplane representation and in the convex hull
representation.
0 0 0 0 0 0 1
0 0 0 0 0 0 −1
−1 0 0 0 0 0 0
0 −1 0 0 0 0 0
0 0 −1 0 0 0 0
0 0 0 −1 0 0 0
0 0 0 0 −1 0 0
0 0 0 0 0 −1 0
1 1 1 1 1 1 6
An example from the 7d dataset in the
hyperplane representation.
1 1 1 1 1 1 1
−12 1 1 1 1 1 1
1 −12 1 1 1 1 1
1 1 −12 1 1 1 1
1 1 1 −12 1 1 1
1 1 1 1 −12 1 1
1 1 1 1 1 −12 1
1 1 1 1 1 0 −1
1 1 1 1 0 1 −1
1 1 1 0 1 1 −1
1 1 0 1 1 1 −1
1 0 1 1 1 1 −1
0 1 1 1 1 1 −1
1 1 1 1 1 1 −1
The same example in the convex hull
representation.
C
Remarks on semantically ambiguous datasets
Relevant to the discussion in Section 6 is the slightly philosophical observa-
tion that the data in and of itself does not carry the semantic meaning of the
inequalities. The model might "interpret" it as some other structure and gen-
erate similar examples to these other structures. In our case there is actually
quite obviously two diﬀerent intuitive ways of interpreting the data: For a re-
ﬂexive polytope, the duality between a polytope P and its dual P ∨can be seen
explicitly as follows: If P is given by hyperplane inequalities 1 + wi ·x ≥0, then
P ∨is the polytope that is the convex hull of the wi. Strictly speaking P ∨is
living in the dual vector space to where P is living, but the model has no way
to know this. Mathematically, some of the properties of P might be checked
by verifying a corresponding property for the dual P ∨. Thus even if the model
were to actually do mathematical operations on the actual geometric object, it
is not clear which geometric object it would do them on. This even shows that
the distinction between the hyperplane representation and the convex hull rep-
resentation does not really exist: Every hyperplane sample also represents the
18

0 0 0 0 −1 1 0
0 0 0 0 −1 0 0
0 −1 0 0 0 0 0
−1 0 0 0 0 0 0
0 0 −1 0 0 0 0
0 0 0 −1 0 0 0
0 0 0 0 0 0 −1
0 0 0 0 0 −1 0
0 0 0 0 0 1 0
1 1 1 1 1 −6 1
An example from the 7d dataset in the
hyperplane representation.
1 1 1 1 1 1 0
1 1 1 1 1 0 1
−6 1 1 1 1 0 1
1 −6 1 1 1 0 1
1 1 −6 1 1 0 1
1 1 1 −6 1 0 1
1 1 1 1 0 1 1
1 1 1 0 1 1 1
1 1 0 1 1 1 1
1 0 1 1 1 1 1
0 1 1 1 1 1 1
1 1 1 1 1 1 1
1 1 1 1 0 −1 1
−11 1 1 1 0 −1 1
1 −11 1 1 0 −1 1
1 1 −11 1 0 −1 1
1 1 1 −11 0 −1 1
1 1 1 1 −12 −1 1
1 1 1 1 1 0 −6
1 1 1 1 0 −1 −11
The same example in the convex hull
representation.
dual polytope in the convex hull representation and, dually, every sample in the
convex hull representation also represents the dual in the hyperplane version.
A consequence of this is that the very question of what our training dataset
really is, is problematized: We have claimed that our dataset represents all
smooth reﬂexive lattice polytopes of a given dimension in the hyperplane repre-
sentation. It could equally well represent some subset of all reﬂexive polytopes of
the given dimension in the convex hull representation. Note that these datasets
are distinct, since the dual of a smooth reﬂexive polytope is not necessarily
smooth. Which one of these interpretations the model is "understanding" is not
clear, or if it is either a combination of the two or even some third semantic
interpretation. Maybe this shows that the notion of the model "understanding"
the semantics of the syntactic representation of the data is simply ill-founded.
On this view the model only ever sees numbers and syntactically manipulates
them according to correlations, and all questions of semantic interpretations are
put on top by humans.
19

