136    14 APRIL 2023 • VOL 380 ISSUE 6641
science.org  SCIENCE
INSIGHTS
ARTIFICIAL INTELLIGENCE
Rethink reporting of evaluation results in AI
Aggregate metrics and lack of access to results limit understanding
By Ryan Burnell1, Wout Schellaert2, John 
Burden1,3, Tomer D. Ullman4, Fernando 
Martinez-Plumed2, Joshua B. Tenenbaum5, 
Danaja Rutar1, Lucy G. Cheke1,6 , Jascha 
Sohl-Dickstein7, Melanie Mitchell8, Douwe 
Kiela9 , Murray Shanahan10,11, Ellen M. 
Voorhees12, Anthony G. Cohn13,14,15,16, Joel Z. 
Leibo10, Jose Hernandez-Orallo1,2,3
A
rtificial intelligence (AI) systems have 
begun to be deployed in high-stakes 
contexts, including autonomous driv-
ing and medical diagnosis. In contexts 
such as these, the consequences of 
system failures can be devastating. It 
is therefore vital that researchers and policy-
makers have a full understanding of the ca-
pabilities and weaknesses of AI systems so 
that they can make informed decisions about 
where these systems are safe to use and how 
they might be improved. Unfortunately, cur-
rent approaches to AI evaluation make it 
exceedingly difficult to build such an under-
standing, for two key reasons. First, aggre-
gate metrics make it hard to predict how a 
system will perform in a particular situation. 
Second, the instance-by-instance evaluation 
results that could be used to unpack these 
aggregate metrics are rarely made avail-
able (1). Here, we propose a path forward in 
which results are presented in more nuanced 
ways and instance-by-instance evaluation re-
sults are made publicly available.
Across most areas of AI, system evalu-
ations follow a similar structure. A sys-
tem is first built or trained to perform a 
particular set of functions. Then, the per-
formance of the system is tested on a set 
of tasks relevant to the desired function-
ality of the system. In many areas of AI, 
evaluations use standardized sets of tasks 
known as “benchmarks.” For each task, the 
system will be tested on a number of ex-
ample “instances” of the task. The system 
would then be given a score for each in-
stance based on its performance, e.g., 1 if 
it classified an image correctly, or 0 if it 
was incorrect. For other systems, the score 
for each instance might be based on how 
quickly the system completed its task, the 
quality of its outputs, or the total reward 
it obtained. Finally, performance across 
the various instances and tasks is usually 
aggregated to a small number of metrics 
that summarize how well the system per-
formed, such as percentage accuracy.
But aggregate metrics limit our insight 
into performance in particular situations, 
making it harder to find system failure 
points and robustly evaluate system safety. 
This problem is also worsening as the 
increasingly broad capabilities of state-
of-the-art systems necessitate ever more 
diverse benchmarks to cover the range of 
their capabilities. This problem is further 
exacerbated by a lack of access to the in-
stance-by-instance results underlying the 
aggregate metrics, making it difficult for 
researchers and policy-makers to further 
scrutinize system behavior.  
AGGREGATE METRICS
Use of aggregate metrics is understandable. 
They provide information about system per-
formance “at a glance” and allow for simple 
comparisons across systems. But aggregate 
performance metrics obfuscate key infor-
mation about where systems tend to suc-
ceed or fail (2). Take, for example, a system 
that was trained to classify faces as male or 
female that achieved classification accuracy 
of 90% (3). Based on this metric, the sys-
tem appears highly competent. However, a 
subsequent breakdown of performance re-
vealed that the system misclassified females 
with darker skin types a staggering 34.5% 
of the time, while erring only 0.8% of the 
time for males with lighter skin types. This 
example demonstrates how aggregation can 
make it difficult for policymakers to deter-
mine the fairness and safety of AI systems. 
Compounding 
this 
problem, 
many 
benchmarks include disparate tasks that 
are ultimately aggregated together. For 
example, the Beyond the Imitation Game 
Benchmark (BIG-bench) for language 
models includes over 200 tasks that evalu-
ate everything from language understand-
ing to causal reasoning (4). Aggregating 
across these disparate tasks—as the BIG-
bench leaderboard does—reduces the rich 
information in the benchmark to an over-
all score that is hard to interpret. 
It is also easy for aggregation to introduce 
unwarranted assumptions into the evalu-
ation process. For example, a simple aver-
age across tasks implicitly treats every task 
as equally important—in the case of BIG-
bench, a sports understanding task has as 
much bearing on the overall score as a causal 
reasoning task. These aggregation decisions 
have huge implications for the conclusions 
that are drawn about system capabilities, yet 
are seldom considered carefully or explained.
Aggregate metrics depend not only on 
the capability of the system but also on 
the characteristics of the instances used 
for evaluation. If the gender classifica-
tion system above were reevaluated by us-
ing entirely light-skinned faces, accuracy 
would skyrocket, even though the system’s 
ability to classify faces has not changed. 
Aggregate metrics can easily give false im-
pressions about capabilities when a bench-
mark is not well constructed. 
Problems and trade-offs that arise when 
considering aggregate versus granular data 
and metrics are not specific to AI, but they 
are exacerbated by the challenges inherent 
in AI research and the research practices 
of the field. For example, machine learn-
ing evaluations usually involve randomly 
splitting data into training, validation, and 
test sets. An enormous amount of data is re-
quired to train state-of-the-art systems, so 
these datasets are often poorly curated and 
lack the detailed annotation necessary to 
conduct granular analyses. In addition, the 
research culture in AI is centered around 
outdoing the current state-of-the-art per-
formance, as evidenced by the many lea-
1Leverhulme Centre for the Future of Intelligence, University of Cambridge, Cambridge, UK. 2Valencian Research Institute for Artificial Intelligence, Universitat Politècnica de Valencia, València, 
Spain. 3Centre for the Study of Existential Risk, University of Cambridge, Cambridge, UK. 4Department of Psychology, Harvard University, Cambridge, MA, USA. 5Department of Brain and 
Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA, USA. 6Department of Psychology, University of Cambridge, Cambridge, UK. 7Brain team, Google, Mountainview, 
CA, USA.  8Santa Fe Institute, Santa Fe, NM, USA. 9Stanford University, Stanford, CA, USA. 10DeepMind, London, UK. 11Department of Computing, Imperial College London, London, UK. 12National 
Institute of Standards and Technology (Retired), Gaithersburg, MD, USA. 13School of Computing, University of Leeds, Leeds, UK. 14Alan Turing Institute, London, UK. 15Tongji University, Shanghai, 
China. 16Shandong University, Jinan, China. Email: rb967@cam.ac.uk
POLICY FORUM
Downloaded from https://www.science.org at The Wikipedia Library on April 20, 2023

14 APRIL 2023 • VOL 380 ISSUE 6641    137
SCIENCE  science.org
derboards, competitions, and challenges 
that offer prestige or monetary prizes (5). 
This research culture emphasizes aggre-
gate metrics and incentivizes immediate 
publication of new findings at the expense 
of robust evaluation practices. In addition, 
the strict space restrictions and fast turn-
around times enforced by high-impact AI 
conferences disincentivize researchers from 
reporting results in a granular way. Finally, 
the primary focus of most publications in 
AI is not the experimental results them-
selves but the new algorithms or techniques 
being evaluated. As a result, less attention 
has been paid in AI to issues around exper-
imental design and reporting than in other 
fields such as psychology or physics.
INSTANCE-BY-INSTANCE EVALUATION
There are many situations in which the 
community might want to conduct analy-
ses that go beyond those reported in a pa-
per. For example, researchers often seek to 
investigate the extent to which AI systems 
are biased against minority or disadvan-
taged populations. It is also frequently 
useful to scrutinize patterns of perfor-
mance to debug systems or to determine 
their safety in a particular deployment 
context. Moreover, in areas such as robot-
ics and reinforcement learning, examining 
the trajectory of a system or its sequence of 
actions can help researchers better under-
stand a system’s strategy.
These supplemental evaluations often 
require access to the instance-by-instance 
evaluation results (i.e., the outputs and 
scores of the systems for each instance). But 
these results are rarely made available—one 
recent analysis found that only 4% of papers 
in top AI venues fully report the evaluation 
results (1). As systems and benchmarks con-
tinue to grow in size and complexity, it is be-
coming increasingly costly for researchers 
to recreate results by conducting their own 
evaluations. The result is that researchers 
and policy-makers are increasingly forced 
to take reported results at face value or to 
incur substantial and unnecessary costs 
just to recreate them.
A PATH FORWARD
To address these problems, we propose a 
broad set of solutions (see the box). 
Moving beyond aggregate metrics
It is important that in-depth performance 
breakdowns are presented instead of, or 
alongside, aggregate metrics. Breakdowns 
can be created by identifying features of 
the problem space that might be relevant 
to performance and using those features 
to analyze, visualize, and predict perfor-
mance (6). These kinds of granular analy-
ses are not yet widely employed. However, 
researchers focused on system robustness 
and fairness have begun to demonstrate 
how valuable they can be.
For example, granular analyses can help 
researchers explore the concepts that a 
system has learned. Researchers examined 
the patterns of errors made by systems on 
a spatial reasoning benchmark designed to 
vary systematically on important features 
of the problem space (7). The researchers 
found that the systems performed much 
worse on problems involving the concept 
of “boundary” than on problems involving 
the concepts of “top” or “bottom,” suggest-
ing that the abstract concept of boundary 
is more difficult for systems to learn.
Instance-level analyses are essential for 
identifying “Clever Hans” phenomena in 
which a system can perform well by rely-
ing on unintended patterns in the dataset. 
For example, a computer vision system 
that was excellent at classifying images 
into categories such as “ship” or “horse” 
(8) was ultimately shown to have not really 
learned to identify ships or horses. Instead, 
it had learned to distinguish categories 
based on the surrounding background or 
watermarks naming the source of the im-
age—features that the system could not 
rely on in the real world. 
Granular analyses can also allow for 
more meaningful comparisons between AI 
systems and humans. For example, though 
an AI system was better overall at breast 
cancer screening than six human radiol-
ogists (9), an in-depth error breakdown 
showed that the AI system failed to detect 
various cancers that were detected by all 
six radiologists. These errors could not be 
easily explained by the researchers, sug-
gesting that further investigation is needed 
to understand how the system detects can-
cers and why it failed in these cases. These 
findings demonstrate the complementary 
value of human and AI screenings in a way 
that aggregate metrics could not.
Of course, granular analysis approaches 
are not without their challenges. Annotating 
the features of each instance can be labor 
intensive. Granular reporting usually needs 
more space in publications, although detailed 
breakdowns can be provided in supplemen-
tary materials or online. Granular analyses 
also often involve slicing the data into smaller 
pieces, so care must be taken to ensure that 
findings are not simply artifacts of the data. 
These issues can be avoided by including a 
large and diverse range of instances, con-
trolling for multiple comparisons, and spec-
ifying important features a priori where 
possible. Finally, deciding which features to 
include in performance breakdowns takes 
time, thought, and expertise. Rigorous theo-
retical and empirical work may be needed to 
build an understanding of the problem space. 
Data-driven approaches can help with this 
process—regression analyses or deep-learn-
ing models can be used to identify features 
that are predictive of performance. 
Ultimately, the best way of present-
ing evaluation results will depend on the 
context. The costs of performing granular 
analyses must be weighed against the po-
tential consequences of system failures, 
and there is no one-size-fits-all solution. 
But we think that a shift toward more 
granular reporting would be a win-win 
situation for developers and the wider 
Guidelines for robust 
evaluation practices 
We recommend that researchers and 
organizations adopt these guidelines 
to make it easier for the community 
to understand system capabilities and 
conduct follow-up analyses.
1.  Wherever possible, reporting of sys-
tem performance should be granu-
lar, with breakdowns across features 
of the problem space that have been 
either hypothesized or empirically 
shown to affect performance. Aggre-
gation decisions should be clearly 
explained, and analyses conducted 
to explore system performance 
should be described.
2.  Benchmarks should be designed 
to test specific capabilities and to 
systematically vary on important 
features of the problem space. 
Benchmark instances should be 
annotated to allow for granular 
analyses to be conducted.
3.  All recorded evaluation results (e.g., 
success or failure, response time, 
partial or full trace, cumulative 
reward) for each system on each 
instance should be made available. 
These data can be reported in sup-
plementary materials or uploaded to 
a public repository. In cases of cross 
validation or hyper-parameter op-
timization, results should ideally be 
reported for each run and validation 
split separately.
4. To enable researchers to conduct 
follow-up analyses, information 
about each test instance used in an 
evaluation should be made available, 
including data labels and all anno-
tated features of those instances.
Downloaded from https://www.science.org at The Wikipedia Library on April 20, 2023

science.org  SCIENCE
138    14 APRIL 2023 • VOL 380 ISSUE 6641
INSIGHTS  |  POLICY FORUM
community in most subdisciplines of AI. 
Granular analyses would help develop-
ers pinpoint system weaknesses to guide 
improvements 
and 
avoid 
potentially 
catastrophic failures. Granular reporting 
would ease the evaluation burden that is 
currently placed on external groups such 
as algorithmic justice organizations, which 
often lack the resources and expertise to 
evaluate systems in detail. 
These changes to reporting must go hand 
in hand with changes to how benchmark 
tasks are constructed. How well a system 
performs across different situations can-
not be evaluated unless the benchmark 
comprehensively covers the problem space. 
The commonly used approach of collating 
a large dataset and randomly splitting it to 
create a test set does not assure this cov-
erage, so alternative approaches to bench-
mark construction must be considered. For 
example, one can design tasks that test for 
specific concepts or cognitive abilities and 
ensure that task instances systematically 
vary on important features of the problem 
space (7, 10, 11). In this endeavor, techniques 
such as procedural or adversarial genera-
tion of task instances might be useful.  
Bringing about these changes in research 
culture will require the participation and 
support of the entire community. Within 
academia, we recommend that publications 
report granular analyses wherever possible, 
and that reviewers and editors ask for per-
formance breakdowns when they are not 
provided. It might also be valuable to alter 
the space limits in conference publications 
to enable in-depth reporting of evaluation 
results. More broadly, we think the field 
needs to reckon with the potentially detri-
mental effects of leaderboards and compe-
titions on evaluation practices.
We also recommend that private orga-
nizations consider incorporating guide-
lines for granularity and aggregation into 
their wider evaluation and reporting prac-
tices (12). We are encouraged to see grow-
ing support for “model cards” that explain 
a system’s architecture and justify evalua-
tion decisions (13). Policy-makers should 
bear in mind the need for granular anal-
yses when creating guidelines or safety 
standards for specific applications. For 
instance, the recently proposed Minimum 
Information for Medical AI Reporting 
(MINIMAR) standard could be modified to 
ask for explanations of aggregation deci-
sions and performance breakdowns across 
features of the problem space.
Ensuring the availability of instance-by-
instance evaluation results
A growing open-science movement has laid 
the groundwork for moving toward mak-
ing instance-by-instance evaluation more 
common. Drawing on lessons from multiple 
disciplines, many have argued that a lack 
of transparency and reproducibility in AI 
research could stifle progress and lead to 
dangerous misestimations of AI capabilities 
(5, 14, 15). In response, various initiatives 
have been set up to promote code and data 
sharing, such as the Hugging Face Hub, the 
Machine Learning Open Source Software 
section of the Journal of Machine Learning 
Research, GitHub, OpenML, Papers With 
Code, and the Open Science Framework. 
However, instance-by-instance evaluation 
results are rarely included on these plat-
forms. There have been few incentives to 
put in the extra work needed to clean, docu-
ment, release, and maintain these results. 
Furthermore, many researchers fear that 
subsequent analyses might discover flaws 
or biases in their systems, or that they will 
be “scooped” by other researchers (5).
It is vital to incentivize the release of 
instance-by-instance results. In other dis-
ciplines, various kinds of requirements, 
incentives, and nudges have been imple-
mented for similar purposes. For exam-
ple, changes to journal and conference 
reporting guidelines to include instance-
by-instance evaluation results could help 
encourage the sharing of these results. It 
should also be possible to ensure that re-
searchers are credited for subsequent uses 
of their results—perhaps by giving the 
results a unique identifier that other re-
searchers can cite.
Given that many AI systems are devel-
oped or deployed by nonacademic orga-
nizations, it is important to consider how 
policy-makers and industry organizations 
can encourage the sharing of results. For 
example, funding agencies could require 
the release of instance-by-instance results 
as a condition of funding, and private or-
ganizations could be encouraged to share 
instance-by-instance 
results 
whenever 
they publish preprints or press releases 
involving system evaluations. These so-
lutions would complement wider efforts 
of groups such as the European Centre 
for Algorithmic Transparency to encour-
age transparency in AI.  We recognize 
that there are some situations in which 
instance-by-instance results cannot be re-
leased (e.g., owing to privacy concerns or 
practical constraints), but in most cases it 
should be possible to do so. Even if no fea-
tures were annotated, releasing instance-
by-instance results would still allow other 
researchers to extract features themselves 
and perform additional analyses as long as 
the benchmark or test data are obtainable. 
Some successful examples of results-
sharing in AI give us confidence that these 
changes in broader research culture are 
possible. For example, researchers who de-
veloped the Holistic Evaluation of Language 
Models (HELM ) benchmark made instance-
by-instance results available for a variety 
of models across the entire benchmark. If 
other fields such as psychology and medi-
cine can make progress on these issues even 
in the face of considerable data privacy chal-
lenges, AI should be able to do the same. j
REFERENCES AND NOTES
 1. O. E. Gundersen, S. Kjensmo, Proceedings of the AAAI 
Conference on Artificial Intelligence (Association for the 
Advancement of Artificial Intelligence, 2018), vol. 32, pp. 
1644–1651.
 2. B. Nushi, E. Kamar, E. Horvitz, Proceedings of the 
Sixth AAAI Conference on Human Computation and 
Crowdsourcing (Association for the Advancement of 
Artificial Intelligence, 2018), vol. 6, pp. 126–135.
 3. J. Buolamwini, T. Gebru, Proceedings of the 1st 
Conference on Fairness, Accountability and Transparency 
(PMLR, 2018), vol. 81, pp. 77–91.
 4. A. Srivastava et al., arXiv:2206.04615 (2022); http://
arxiv.org/abs/2206.04615.
 5. J. Pineau et al., J. Mach. Learn. Res. 22, 1 (2021).
 6. R. Burnell et al., in Proceedings of the Thirty-First 
International Joint Conference on Artificial Intelligence, 
Vienna, 23 to 29 July 2022 (International Joint 
Conferences on Artificial Intelligence), pp. 2827–2835.
 7. V. V. Odouard, M. Mitchell, arXiv:2206.14187 (2022); 
https://arxiv.org/abs/2206.14187.
 8. S. Lapuschkin et al., Nat. Commun. 10, 1096 (2019).  
 9. S. M. McKinney et al., Nature 577, 89 (2020).  
 10. D. Kiela et al., in Proceedings of the 2021 Conference 
of the North American Chapter of the Association 
for Computational Linguistics: Human Language 
Technologies (Association for Computational 
Linguistics, 2021), pp. 4110–4124.
 11. J. Z. Leibo et al., in Proceedings of the 38th International 
Conference on Machine Learning, 18 to 24 July 2021 
(PMLR), vol. 139, pp. 6187–6199.
 12. I. D. Raji et al., arXiv:2001.00973 (2020); https://arxiv.
org/abs/2001.00973.
 13. M. Mitchell et al., in Proceedings of the Conference on 
Fairness, Accountability, and Transparency (Association 
for Computing Machinery, 2019), pp. 220–229.
 14. B. Haibe-Kains et al., Nature 586, E14 (2020).  
 15. M. Hutson, Science 359, 725 (2018).  
ACKNOWLEDGMENTS
We are grateful to J. Pineau and S. Russell for their feedback 
on the manuscript and to M. Brundage and L. Ahmad for 
helpful discussions about the need for instance-by-instance 
results for foundation models. Funding support was provided 
by the Future of Life Institute under grant RFP2-152 (J.H.O. 
and J.B.); US Defense Advanced Research Projects Agency 
HR00112120007 (RECoG-AI) (J.H.O., L.C., R.B., J.B., and 
D.K.); grant PID2021-122830OB-C42 (SFERA) funded by 
MCIN/AEI/10.13039/501100011033 and “ERDF A way of 
making Europe” (J.H.O., F.M.P., and W.S.); INNEST/2021/317 
[Project cofunded by the European Union with the “Programa 
Operativo del Fondo Europeo de Desarrollo Regional (FEDER) 
de la Comunitat Valenciana 2014-2020”] (J.H.O. and W.S.); 
and UPV (Vicerrectorado de Investigación) grant PAI-10-21 
(J.H.O. and W.S.).
10.1126/science.adf6369
“…a shift toward more granular 
reporting would be a win-win 
situation for developers 
and the wider community in 
most subdisciplines of AI.”
Downloaded from https://www.science.org at The Wikipedia Library on April 20, 2023

Use of this article is subject to the Terms of service
Science (ISSN 1095-9203) is published by the American Association for the Advancement of Science. 1200 New York Avenue NW,
Washington, DC 20005. The title Science is a registered trademark of AAAS.
Copyright © 2023 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim
to original U.S. Government Works
Rethink reporting of evaluation results in AI
Ryan Burnell, Wout Schellaert, John Burden, Tomer D. Ullman, Fernando Martinez-Plumed, Joshua B. Tenenbaum,
Danaja Rutar, Lucy G. Cheke, Jascha Sohl-Dickstein, Melanie Mitchell, Douwe Kiela, Murray Shanahan, Ellen M.
Voorhees, Anthony G. Cohn, Joel Z. Leibo, and Jose Hernandez-Orallo
Science, 380 (6641), . 
DOI: 10.1126/science.adf6369
View the article online
https://www.science.org/doi/10.1126/science.adf6369
Permissions
https://www.science.org/help/reprints-and-permissions
Downloaded from https://www.science.org at The Wikipedia Library on April 20, 2023

