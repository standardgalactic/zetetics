ALERT: Adapting Language Models to Reasoning Tasks
Ping Yu
Tianlu Wang
Olga Golovneva
Badr Alkhamissy
Gargi Ghosh
Mona Diab
Asli Celikyilmaz
Meta AI
Abstract
Current large language models can perform
reasonably well on complex tasks that require
step-by-step reasoning with few-shot learning.
Are these models applying reasoning skills
they have learnt during pre-training and rea-
son outside of their training context, or are
they simply memorizing their training corpus
at ﬁner granularity and have learnt to better
understand their context?
To tease apart
these possibilities, we introduce ALERT, a
benchmark and suite of analyses for assess-
ing language models’ reasoning ability com-
paring pre-trained and ﬁnetuned models on
complex tasks that require reasoning skills to
solve.
ALERT provides a test bed to asses
any language model on ﬁne-grained reasoning
skills, which spans over 20 datasets and cov-
ers 10 different reasoning skills. We leverage
ALERT to further investigate the role of ﬁne-
tuning. With extensive empirical analysis we
ﬁnd that language models learn more reason-
ing skills such as textual entailment, abduc-
tive reasoning, and analogical reasoning dur-
ing ﬁnetuning stage compared to pretraining
state. We also ﬁnd that when language models
are ﬁnetuned they tend to overﬁt to the prompt
template, which hurts the robustness of models
causing generalization problems.
1
Introduction
Large language models (LLMs) (e.g.,
GPT-
3 (Brown et al., 2020a), PALM (Chowdhery et al.,
2022a), OPT (Zhang et al., 2022)) have shown in-
creasing in-context learning capabilities with scal-
ing up the model and data size.
Despite this
progress, even the largest of these models still
struggle with tasks such as commonsense rea-
soning (West et al., 2022), and math word prob-
lems (Hendrycks et al., 2021b) which require arith-
metic reasoning or symbolic manipulation (Rytting
and Wingate, 2021). Table 1 presents some exam-
ples that require certain reasoning skills. Even the
The cafeteria had 23 apples. If they used 20 to make lunch and
bought 6 more, how many apples do they have?
The answer is 29 apples .
Select the best translation into predicate logic.
David
teaches Chris.
(c:
Chris; d:
David; Txy:
x teaches y)
(A)Tdc;(B)Tcd;(C)Tcc;(D)dTc. The answer is (B) Tcd .
Isabella entered the hall.
Olivia entered the hall.
The ap-
ple is in the blue_treasure_chest.
Olivia exited the hall.
Is-
abella moved the apple to the green_basket. Question: Where
does Isabella think that Olivia searches for the apple?
The
answer is Isabella thinks that Olivia searches for the apple in the
green_basket .
Table 1: Examples from tasks that require reasoning skills
and generated outputs from GPT-3 series text-davinci-003
engine. The faild outputs are highlighted in red. Predictions
by ChatGPT are shown in Table 8 in Appendix.
powerful LLMs (text-davinci-0031 and ChatGPT
2) fail to make correct predictions.
To improve large LLMs’ performance on tasks
that require multiple steps of reasoning, recent
work used different prompting methods which in-
cluded a rationale with the ﬁnal answer in the form
of: scratchpad for arithmetic and logical reason-
ing (Nye et al., 2021), chain-of-thought (Wei et al.,
2022) for practically any task, or adding let’s think
step-by-step (Kojima et al., 2022) to prompt mod-
els to generate explanations. Other works such as
Chung et al. (2022) integrate step-by-step explana-
tions into the ﬁnetuning stage (CoT-ﬁnetuning). Al-
though these approaches improve downstream task
performance and interpretability on many tasks, it
is less known which skills these models are trigger-
ing and to what extent they require reasoning skills.
It also remains unclear how often the stated reason-
ing steps actually support the ﬁnal task predictions.
For instance to correctly answer the questions in
Table 1 a combination of logical, commonsense,
math and spatial reasoning skills are required.
1https://beta.openai.com/docs/models/
gpt-3.
2https://chat.openai.com/chat.
arXiv:2212.08286v1  [cs.CL]  16 Dec 2022

In this work, to gain a deeper understanding
of LLMs reasoning abilities in in-context learn-
ing settings, we introduce ALERT, a new pipeline
to benchmark different LLMs on various reasoning
skills. Unlike existing commonly used benchmarks
(e.g., Mishra et al. (2022); Wang et al. (2022c);
Srivastava et al. (2022)), we designed ALERT to
evaluate LLMs on ﬁne-grained reasoning skills. It
spans over 20 datasets and covers 10 different rea-
soning skills including logistic, causal, common-
sense, abductive, spatial, analogical, argument and
deductive reasoning as well as textual entailment,
and mathematics (see Figure 2). ALERT enables
easy benchmarking of any LM (e.g., pre-trained,
ﬁnetuned, CoT-ﬁnetuned) on a rich set of new in-
ference methods including zero-shot, few-shot and
chain-of-thought (CoT).
Using ALERT we further investigate whether
ﬁnetuning can improve LM’s performance on
downstream reasoning tasks. Speciﬁcally, we are
interested in diagnosing what has been really im-
proved when we see a performance increase on
reasoning tasks. Is it because models have seen
similar data in ﬁnetuning stage? Or is it because
models have seen prompts in a speciﬁc template
and memorize the template? Or have the reasoning
skills really been improved? We further explored
the above three possibilities.
To study the above questions, we compare three
different model types: a pre-trained model and
two types of ﬁnetuned models. The ﬁrst ﬁnetuned
model (which we refer as ﬁnetuned model) is a
standard ﬁnetuned LM while the second one is
the rationale-based ﬁnetuned model (Chung et al.,
2022) (which we refer as CoT-ﬁnetuned model).
We use these three types of models to invetigate
the role of ﬁnetuning on three dimensions:
Data memorizing: We investigate whether the per-
formance improvements obtained after ﬁnetuning
can be attributed to using similar or sometimes
the exact same data as the evaluation datasets. To
study this, we use vocabulary overlap to measure
to what extent the evaluation data is different from
the ﬁnetuning data. We then explore whether the
improvement is more signiﬁcant when evaluation
data is more similar to the ﬁnetuning data.
Reasoning skills transfer: We investigate if cer-
tain reasoning skills can be more successfully im-
bued in LLMs than other reasoning skills. To ver-
ify this, we carefully divide the evaluation datasets
into groups which require different reasoning skills.
We further compile held-out datasets as shown in
Figure 2 which require skills held-out from any
of the training datasets. This way, we expect to
see larger improvements on in-domain skills com-
pared to held-out skills if reasoning skills can be
transferred during ﬁnetuning stages.
Prompt template memorizing: Our third hypoth-
esis is that LLMs can overﬁt to prompt templates
used in the ﬁnetuning datasets. In other words, the
consistency in prompt templates helps LLMs better
understand the instruction which then yields better
performance after ﬁnetuning. To test this, we eval-
uate ﬁnetuned LLMs on datasets with 5 different
prompt templates.
Summary of ﬁndings:
(i) Different from Gu-
rurangan et al. (2020), our experiments indicate
that there is no strong correlation between high
vocabulary overlap (between ﬁnetuning and eval-
uation datasets) and performance gain on evalua-
tion datasets. This means that LLMs are not sim-
ply memorizing the training data during ﬁnetuning
stage. (ii) Finetuning helps to improve certain rea-
soning capabilities of LLMs (e.g. analogical and
abductive) but not all of them (e.g. commonsense
reasoning). (iii) Finetuning can cause overﬁtting
towards prompt templates, which makes it harder
for LLMs to generalize to other prompt templates,
while CoT-ﬁnetuning helps to mitigate this issue as
it incorporates a variety of explanations.
Though many of the aspects that we study have
been discussed in prior analyses of LLMs (Chung
et al., 2022; Wei et al., 2021a, 2022; Kojima et al.,
2022; Cobbe et al., 2021; Sanh et al., 2021), prior
work has not evaluated LLMs on different reason-
ing skills and how these skills be improved. Over-
all, by evaluating reasoning skills with ALERT, we
gain new insights on how models have or have
not succeeded in generalizing beyond their experi-
ence.
2
Motivation and Our Benchmark
Motivation.
The analyses in ALERT are inspired
by a scientiﬁc question: To what extent do LLMs
learn generalizeable reasoning abilities? This ques-
tion motivates our focus on measuring LLMs per-
formance on tasks that require contextual under-
standing and perform multi-step operations, which
are crucial to perform well on complex downstream
tasks.

Deﬁnition: In this task, we ask you to write an implausible
answer to a question that involves event duration, based on a given
sentence. Here, event duration is deﬁned as the understanding of
how long events typically last. For example, “brushing teeth”,
usually takes a few minutes. Even though there exist multiple
wrong answers, we only need a single wrong answer.
Positive Example 1-
input: Sentence: Jack played basketball after school, after
which he was very tired.
Question: How long did Jack play basketball?
output: 22 hours.
explanation: Typically we play basketball for a couple of
hours. So any answer beyond that range is unlikely.
Negative Example 1-
input: Sentence: Jack played basketball after school, after
which he was very tired.
Question: How long did Jack play basketball?
output: 1 hours.
explanation: Typically we play basketball for 1-2 hours. 1
hour is a correct answer here. Note that the task is to generate
an incorrect answer.
Figure 1: An example from NIV2 (Wang et al., 2022c) that
requires a deep understanding of the long task instruction and
can be very challenging even for humans.
Datasets.
To construct the datasets of ALERT,
we select some tasks from NIV2 benchmark (Wang
et al., 2022c) and perform the following operations:
(1) Omit extremely hard tasks.
We design
ALERT so that it can be used to benchmark a va-
riety of LLMs, from pre-trained to ﬁnetuned to
instruction-tuned models. To select such tasks,
we apply several heuristics: ﬁrstly, we manually
omit tasks that heavily rely on instructions. Some
tasks are hard to solve when only in-context ex-
amples (demonstrations) are provided (e.g., the
example in Figure 1). Secondly, we apply pre-
trained OPT-13B model (Zhang et al., 2022) on
each NIV2 task and keep the tasks with decent per-
formance (ROUGE-L > 5.0). Thirdly, we omit
tasks on which humans fail to get decent perfor-
mance given the ground truth labels from NIV2.
For example, task963_librispeech_asr_next_word_
prediction (Weir et al., 2020) provides a prompt
“Joey’s favourite food is ___”, with the ground truth
answer “sandwiches”. Without any context or back-
ground information, the answer can be any food
thus it is extremely hard for humans to accurately
predict “sandwiches”.
(2) Remove tasks with long input context The
input sentence length of some tasks can be very
long, and currently most LLMs are not designed
for solving long text problems. We omit tasks with
demonstration length longer than 2048 tokens.
Finetuning tasks
Evaluation tasks
In-domain Skills
Logical and Causal Reasoning
Commonsense Reasoning
Entailment and Math
Abductive Reasoning
 
14 datasets
Held-out Skills
Spatial Reasoning
Analogical Reasoning
Argument Reasoning
Deductive Reasoning
6 datasets
Logical and Causal Reasoning
Commonsense Reasoning
Entailment and Math
Abductive Reasoning
10 datasets
Figure 2: Tasks we use for ﬁnetuning and evaluation. We
split evaluation data according to reasoning skills. Models are
tested on both in-domain skills and held-out skills.
(3)
Fix
ground
truth
labels.
For
each
reasoning
task,
NIV2
provides
the
reason-
ing skills required to solve the task,
e.g.
task102_commongen_data_to_text requires rela-
tional, analogical and commonsense reasoning.
However, we found that some tasks have been la-
beled with incorrect reasoning skills. For example,
task393_plausible_result_generation gives a sen-
tence and asks LLMs to complete the sentence.
The labels given by NIV2 are causal reasoning and
textual entailment, but in fact this task can hardly
examine the entailment skill. Accordingly, we man-
ually ﬁx reasoning skill labels. In addition, we only
keep the predominant skill. For example, many
tasks need more or less a commonsense knowl-
edge, while we only select tasks that heavily rely
on commonsense knowledge to assess common-
sense reasoning.
Benchmark.
After the above steps, we select
tasks that represent a variety of reasoning skills
and construct ALERT reasoning benchmark, where
Table 2 shows details about our benchmark.
In order to make it easier to analyze reason-
ing skills in Section 4.2.2, reasoning skills in
ALERT have been divided into in-domain skills
and held-out skills in Figure 2. The "in-domain"
here are referred to these reasoning skills can be
learned from ﬁnetuning data, but these in-domain
datasets are actually on-held datasets, which are not
seen in ﬁnetuning. Note that, apart from analysis
of reasoning skills in Section 4.2.2, we used all of
the datasets (20 datasets) for all the other analysis.
3
Experiment Setup
3.1
Models
To perform a controlled comparison across training
and prompting methods, we focus on three different

Reasoning Skills
Datasets
Logistic
bigbench repeat copy logic, mmmlu an-
swer generation
Causual
plausable result generation, anli r2 entail-
ment, anli r3 entailment, cb entailment
Commonsense
piqa answer generation, commongen sen-
tence generation, sciq answer generation,
openbookqa question answering
Entailment
nli r2 entailment, anli r3 entailment, cb
entailment, lue entailment classiﬁcation
Arithmetic
semeval closed vocabulary math, semeval
geometric math, mmmlu formal logic
Abductive
tellmewhy
Spatial
babi t1 single supporting fact, piqa answer
generation, toqa ﬁnd location easy clean
Analogical
commongen sentence generation, bard
analogical reasoning causation
Argument
argument stance classiﬁcation, argument
consequence classiﬁcation
Deductive
rocstories correct answer generation
Table 2: ALERT benchmark consists of 20 datasets covering
10 different reasoning skills. Full list of the reasoning skills
and datasets Table 3 in Appendix A.1.
models: pre-trained, ﬁnetuned, and rationale-based
ﬁnetuned (CoT-ﬁnetuned) models. For pre-trained
models, we use OPT (Zhang et al., 2022), a suite
of decoder-only pre-trained transformers which are
reported to yield comparable performance to GPT-
3 (Brown et al., 2020b). We benchmark with OPT
models of two scales: 1.3B and 13B. For ﬁnetuned
models (OPT-FT), we ﬁnetune OPT models on
datasets without explanations. For COT-ﬁnetuned
models (OPT-CoT), we ﬁnetune OPT models on
data with rationales (explanations).
We train all models in Pytorch (Paszke et al.,
2017) using Metaseq3. We initialize model hyper-
parameters for each model scale following OPT
(Zhang et al., 2022).
We pack our training
examples into sequences of length 2048, left-
truncating examples that overﬂow. We use AdamW
(Loshchilov and Hutter, 2017) with 32-bit state
with (β1, β2) = (0.9, 0.95), linearly warming up
the learning rate for 6% steps to the maximum, fol-
lowed by linearly decaying it to 0. For all 1.3B
models, we use batch size of 128, and for 13B
models, we use batch size of 256.
3.2
Finetuning Details
Finetuning Data. Our ﬁnetuning corpus is com-
prised of 10 datasets: ProofWriter (Tafjord et al.,
2020), StrategyQA (Geva et al., 2021), ECQA (Ag-
garwal et al., 2021), CoQA (Reddy et al., 2019),
GSM8K (Cobbe et al., 2021), AQUA-RAT (Ling
3https://github.com/facebookresearch/
metaseq
(A) pretrained language models (e.g. GPT-3, OPT)
(B) meta-finetuned language models (e.g. FLAN, OPT-FT)
Pretrained 
LM
Inference on 
task A, B, C,...
finetune on 
task D, E, F, …
Q: If X and Y are digits and 8XY is a 3-digit number 
that is divisible by 2, which of the following is a 
possible product of X and Y?  A)15 B)31 C)12 D)27 
E)91; A: The answer is C.
(C) CoT finetuned language models (e.g. OPT-CoT)
finetune on 
task D, E, F, …
with explanations 
Q: If X and Y are digits and 8XY is a 3-digit number that is 
divisible by 2, which of the following is a possible product of X 
and Y? A)15 B)31 C)12 D)27 E)91; A: The answer is C. because 
Key to this question is to remember the fact that a number 
divisible by 2 must end with even OR 0 (i.e Y). If Y had to be 0, 
product should also be 0 regardless of X. Otherwise, product is a 
multiple of 2. Only one answer choice meets the requirement
Inference on 
task A, B, C,...
Pretrained 
LM
Pretrained 
LM
Inference on 
task A, B, C,...
Figure 3: We compare three types of models: (A) directly
apply pretrained LLMs on reasoning tasks; (B) ﬁnetune LLMs
on a set of tasks; (C) ﬁnetune LLMs on tasks with explana-
tions (CoT-ﬁnetuning). Finetuning data contains source and
target parts. Language modeling loss only applied to the
target part.
et al., 2017), ESNLI (Camburu et al., 2018), MATH
(Hendrycks et al., 2021c), CoS-E (Rajani et al.,
2019), WinoWhy (Zhang et al., 2020). These 10
ﬁnetuning datasets collectively contain 6 different
reasoning skills: logic reasoning, causal reasoning,
commensense reasoning, textual entailment, math-
ematics, abductive reasoning. In addition, these 10
datasets all come with instructions, demonstration
examples and explanations. This enables fair com-
parison of OPT-FT and OPT-CoT models. More
details about ﬁnetuning corpus can be found in
Table 4 in Section A.2.
3.3
Evaluation
Templates
Following (Wei et al., 2021b), to con-
trol for the effect of variable prompt templates, we
adopt different templates (T) during inference stage
in our experiments:
• T1: instruction + demonstration examples
with explanations + "let’s think step by step";
• T2: instruction + "Please give a short expla-
nation after the answer" + demonstration ex-
amples with explanations + "let’s think step
by step"
• T3: instruction + "Please give a short expla-
nation after the answer" + demonstration ex-
amples with explanations

• T4: "Please give a short explanation after the
answer" + demonstration examples with ex-
planations + "Let’s think step by step"
• T5: instructions + demonstrations
For each dataset, we report the average or max
score among these ﬁve templates. The ﬁnal aggre-
gated results (including aggregated average score
and aggregated max score) are reported by further
averaging across all datasets. Unless speciﬁed oth-
erwise, the default score refers to the highest score
among ﬁve templates.
Evaluation metrics
Since our benchmark con-
tains both classiﬁcation and generation tasks, we
cannot use classiﬁcation accuracy to evaluate all
the tasks. Following FLAN (Wei et al., 2021b), we
append classiﬁcation choices at the end of prompts
and ask models to generate answers. Thus, clas-
siﬁcation tasks can be treated as a special case of
generation tasks. Accordingly, we use ROUGE-L
(Lin, 2004) to measure the performance of both
classiﬁcation and generation tasks and report the
aggregated score. Similar to Chung et al. (2022),
we also use exact-match score which is more suit-
able for tasks with short answers. Additionally, we
compute relaxed-match score which is a relaxed
version of exact-match. Speciﬁcally, we normal-
ize ground truth answers and predictions to have
all text in lower case and remove punctuation and
extra white spaces.
4
Analysis
4.1
Does ﬁnetuning help?
Figure 4 demonstrates the performance averaged
across all evaluation tasks in our benchmark. We
evaluate three models: OPT, OPT-FT, OPT-CoT at
two different scales: 1.3B and 13B. We report both
the average and the best (max) ROUGE-L score and
exact-match score across 5 templates (as detailed
in Section 3.3). The best ROUGE-L score is calcu-
lated by obtaining max ROUGE-L score across 5
different templates for each task and then getting
the average score of 20 tasks. Rationale-based ﬁne-
tuning (OPT-CoT) improves performance on both
1.3B and 13B OPT. However, ﬁnetuning (OPT-FT)
sometimes yields worse results than the vanilla pre-
trained model. We speculate that this is caused by
models getting overﬁtted to the prompt format used
in ﬁnetuning data whereas OPT-CoT refrains from
this issue due to the diverse explanations.
1.3B
13B
Model scale
0
10
20
30
ROUGEL Scores
Aggregated Max Scores across 5 Templates
OPT
OPT-FT
OPT-CoT
1.3B
13B
Model scale
0
5
10
15
20
25
Exact-match Scores
1.3B
13B
Model scale
0
5
10
15
20
25
30
ROUGEL Scores
Aggregated Average Scores across 5 Templates
OPT
OPT-FT
OPT-CoT
1.3B
13B
Model scale
0
5
10
15
20
Exact-match Scores
Figure 4:
Performance of pre-trained LM (OPT), ﬁne-
tuned LM (OPT-FT) and CoT-ﬁnetuned LM (OPT-CoT) on
ALERT reasoning benchmark. Top charts show aggregated
max scores while bottoms are average scores across 5 tem-
plates. Scores are averaged across 20 tasks.
4.2
What does LLMs learn during
ﬁnetuning?
We ﬁnd that CoT-ﬁnetuning improves performance
on reasoning tasks in general. However, what ex-
actly does the LM learn during the ﬁnetuning stage
is still under explored. Thus, we study the role of
ﬁnetuning from three perspectives: data memoriz-
ing, reasoning skills transfer, and prompt template
memorizing.
4.2.1
Data Memorizing
Gururangan et al. (2020) ﬁnds that the performance
gain is larger when the ﬁnetuning dataset is more
dissimilar to the pre-training dataset. However,
their conclusion is made by a single-task ﬁnetun-
ing. They evaluate their model on the same dataset
that was used for ﬁnetuning. A more thorough eval-
uation dictates that ﬁnetuned models (Wei et al.,
2021b; Chung et al., 2022) be evaluated on held-
out datasets. As such, in Figure 3 in blocks (B) and
(C) we show two potential ways of ﬁnetuning and

Performance Changes (%) compared to OPT
Vocabulary Overlaps
Figure 5: Correlation between vocabulary overlap and per-
formance improvement using 13B parameter models. The
top chart shows ROUGE-L while the bottom shows relaxed-
match score.
inference as we adopt in this paper.
To verify that the improvement of ﬁnetuning is
attributed to seeing more data during the ﬁnetuning
stage, we measure how dissimilar are our train-
ing and test data used in ﬁnetuning and evaluation
respectively. If higher similarity leads to better per-
formance, it may indicate that the improvements
of ﬁnetuned LLMs are due to seeing more similar
data during the ﬁnetuning stage. Following (Gu-
rurangan et al., 2020), we use unigram vocabulary
overlap to measure the data similarity. More specif-
ically, we divide our tasks into three categories.
The ﬁrst category has 10 datasets and there is up to
10% overlap between the ﬁnetuning data and eval-
uation data. The second category has 3 datasets
with an overlap between 10% and 30%. The third
category has 7 datasets with an overlap over 30%.
Details can be found in Table 6 in appendix A.4.
We measure performance improvements of OPT-
FT and OPT-CoT compared against the pretrained
OPT model. We present both ROUGE-L score (top)
and relaxed-match score (down) in Figure 5. The
results indicate that there is no strong correlation
between the vocabulary overlap between ﬁneun-
ing and evaluation datasets and the performance of
the model (neither a higher nor a lower vocabulary
overlap always translate to a performance improve-
ment). OPT-CoT achieves the best ROUGE-L and
relaxed-match scores both in settings when there is
a medium (10%-30%) level of vocabulary overlap.
We don’t observe a consistent pattern on OPT-FT
In-domain Skills
Held-out Skills
Figure 6: The ROUGE-L difference between OPT-FT and
OPT, as well as OPT-CoT and OPT models within each rea-
soning skill. The top 6 skills are in-domain reasoning skills.
The bottom 4 skills are on-held reasoning skills.
models either. Overall, for these challenging tasks,
seeing similar data during ﬁnetuning stage does not
guarantee performance improvement.
4.2.2
Reasoning Skills Transfer
In this subsection, we measure whether ﬁnetuning
can improve reasoning skills. The average ROUGE-
L scores are calculated for each reasoning skill on
6 models (1.3B OPT, 1.3B OPT-FT, 1.3B OPT-
CoT, 13B OPT, 13B OPT-FT, 13B OPT-CoT). Fig-
ure 6 shows the difference between OPT-FT and
OPT, and the difference between OPT-CoT and
OPT models’ performances. For example, OPT-FT
1.3B model yields on average 3.5 less ROUGE-L
points than OPT 1.3B model on the tasks of logistic
reasoning. We measure 10 reasoning skills, 6 of
which are in-domain skills, i.e. similar reasoning
skills are seen during ﬁnetuning.
From Figure 6 we can see that OPT-CoT gener-
ally improves in-domain reasoning skills, but OPT-
FT does not show the same pattern. All four LLMs
showed improved reasoning abilities on textual en-
tailment, abductive reasoning, and analogical rea-
soning tasks. We ﬁnd that these three skills can
not be easily obtained during the pre-training stage
since pretraining stage only contains plain text. On
the contrary, for example, commonsense reason-
ing or spatial reasoning can be learned during pre-
training, while the beneﬁts of ﬁnetuning are not
prominent. Additionally, Gururangan et al. (2020)
concluded that the more dissimilar the domain be-
tween pretraining and ﬁnetuning are, the higher
the potential for ﬁnetuning to yield gains. We see
the same trend but the domain in Gururangan et al.
(2020) is deﬁned by the vocabulary overlaps, while
we deﬁne the domains by reasoning skills. Note
that, in Figure 6, "in-domain" refers to ﬁnetuning

1.3B
13B
Model scale
0
10
20
30
Max Relaxed-match
OPT
OPT-FT
OPT-CoT
1.3B
13B
Model scale
0
5
10
15
20
25
Average Relaxed-match
Figure 7: Comparing pretraining and ﬁnetuning models on
relaxed match score. Left: aggregated best (max) performance
across 5 Templates; Right: aggregated average performance
across 5 Templates.
and evaluation benchmark cover similar reasoning
skills, we cannot see a uniform trend. However, in
the ﬁnal conclusion, we found the more dissimilar
reasoning skills between pretraining and ﬁnetuning
are, the higher the potential for ﬁnetuning to yield
gains.
We also note that OPT-CoT ﬁnetuning helped
the model to learn larger variety of reasoning
skills, showing stronger performance on logistic
and causal reasoning, in addition to skills that show
consistent improvements across all ﬁnetuned mod-
els (such as textual entailment, abductive reasoning,
and analogical reasoning).
4.2.3
Prompt Template Memorizing
We analyze the impact of the evaluation metrics
and the templates (from § 3.3) we used during ﬁne-
tuning stage. We examine whether ﬁnetuning can
memorize the template representation of the train-
ing data and the impact of data format on models’
robustness.
Evaluation
with
relaxed-match
score.
We
compare two metrics: exact-match and relaxed-
match. From Figure 4, we saw that OPT-FT was
worse than OPT when exact-match is used as the
metric.
However, when relaxed-match is used,
OPT-FT outperforms OPT as shown in Figure 7.
Note that relaxed-match score ignores punctuation,
articles and extra whitespace. This suggests that
if we decouple performance from format adher-
ence, OPT-FT performs better than OPT. In other
words, ﬁnetuning is helpful but it can make the
output more noisy which explains why there is per-
formance drop when exact-match is used as the
metric.
1.3B
13B
Parameters
0
20
40
60
80
100
Template following percentage
OPT
OPT-FT
OPT-CoT
1.3B
13B
Parameters
0
2
4
6
8
10
12
Standard deviation
Figure 8: Analyzing the robustness of models in following
the templates. Left: template following percentage by each
model; Right: standard deviation of template following per-
centage.
Template
following
percentage.
We
check
whether the model can follow the template of the
demonstrations. For example, if a demonstration
uses "the answer is xxx because yyy", then we
check what percentage of instances can follow the
exact same template as the demonstration. Fig-
ure 8 (left) shows the average template following
percentage for each model. Both OPT and OPT-
CoT can follow demonstrations’ template pretty
well even though OPT is not pre-trained using ra-
tionales. Compared to 1.3B models, larger models
gain better ability overall in following the demon-
strations’ template. Compared to OPT and OPT-
CoT, OPT-FT lacks the ability to follow diverse
templates. This is because the OPT-FT training pro-
cess does not contain any rationale data. Finetuning
makes the model more biased towards a speciﬁc
template representation, while the adaptability to
other templates becomes worse. It is worth noting
that although the the OPT-CoT model is trained on
rationales, it still works well when evaluating the
model with non-CoT templates.
Robustness
To analyze the robustness of each
model to different templates, we measure the stan-
dard deviation of ROUGE-L scores for each model
on 5 different templates. As we can see from Fig-
ure 8 (right), OPT is robust to different templates,
while OPT-FT has difﬁculties adapting to changing
templates. In general, ﬁnetuning (both OPT-FT
and OPT-CoT) adversely affects the robustness of
the model and makes the model biased towards a
speciﬁc data format, however, OPT-CoT is better
than general ﬁnetuning (OPT-FT).
In summary, the models learn the data format
representation and templates during ﬁnetuning

stage. However, ﬁnetuned models contain bias
towards the data formats and template it has seen,
which will reduce the robustness of the model to
generalized settings. CoT-ﬁnetuning is better than
OPT-FT, but it is still not as good as the pre-trained
model.
5
Related Works
LLMs that Reason.
In the setting of in-context
learning, pretrained large language models (e.g.
GPT-3 (Brown et al., 2020b), OPT (Zhang et al.,
2022), PaLM (Chowdhery et al., 2022b)) have
demonstrated remarkable performance on various
NLP tasks. On the other hand, reasoning tasks
(e.g. mathematical reasoning and commonsense
reasoning) remain more challenging. A few follow-
up research trajectories have been proposed to im-
prove LLMs’ reasoning abilities.
Kojima et al.
(2022) shows that LLMs can be decent zero-shot
reasoners by simply appending “Let’s think step
by step” to the prompt. Wei et al. (2022) adds a
series of intermediate reasoning steps to improve
LLMs’ reasoning abilities.
Wang et al. (2022a)
further proposes to expand prompts to include ratio-
nales in each few-shot example. Additionally, Fu
et al. (2022) discovers that prompting with higher
reasoning complexity achieves substantial gains
on math word tasks. To tackle problems harder
than demonstration examples, Zhou et al. (2022)
ﬁrst reduces a complex problem into a list of sub-
problems and then solve subproblems sequentially.
Another line of research is to improve the naive
decoding strategy, Wang et al. (2022b) introduces
a self-consistency strategy which selects the most
consistent answer among a set of reasoning paths.
Existing Reasoning Benchmarks.
Many bench-
marks are used for evaluating language models’
performance, such as BIG-Bench (Srivastava et al.,
2022), Natural Instruction V2 (NIV2) (Wang et al.,
2022c), MMLU (Hendrycks et al., 2020). Although
they contain some reasoning tasks, none of them
are speciﬁcally designed to test models’ reasoning
skills. For example, NIV2 contains 172 datasets
and a total of 1554 tasks, including some reasoning
tasks. It has several issues which make it inap-
propriate to be directly used as a reasoning bench-
mark: (1) it is designed for instruction-tuned mod-
els and some tasks might be unsuitable for evaluat-
ing pretrained models or non-instruction ﬁnetuned
models, as shown in Figure 1; (2) reasoning skills
have been divided into 27 categories while some
of them have large overlaps, e.g. numerical reason-
ing, quantitative reasoning, reasoning on numbers;
(3) some reasoning labels are wrongly labeled, e.g.
task393_plausible_result_generation gives textual
entailment label but this task can hardly examine
the entailment skill.
The Curriculum benchmark (Chen and Gao,
2022) is deigned for probing language models’ rea-
soning abilities and covers 8 different reasoning
skills. However, this work only focuses on classi-
ﬁcation tasks and it converts all examples into the
Natural Language Inference (NLI) format to ﬁt into
a uniﬁed framework. We argue that the NLI for-
mat can make the original tasks more complicated
and causes some confusions. We observed some
SOTA language models (GPT-3 davinci-003) fail to
solve some tasks, e.g. examples in Table 1. More
discussion and results are shown in Appendix B.
Finetuning LLMs.
LLMs ﬁnetuned on a range
of NLP tasks have shown improved performance
on held-out downstream tasks, in both zero-shot
and few-shot settings. FLAN (Wei et al., 2021b),
T0 (Sanh et al., 2021), Tk-Instruct (Wang et al.,
2022c) and Instruct-GPT (Ouyang et al., 2022)
have demonstrated that the more diverse tasks in-
cluded in ﬁnetuning, the larger performance gain
models can achieve. Following this approach, we
ﬁnetune OPT models and name this type of models
as OPT-FT ((B) in Figure 3). Chung et al. (2022)
further adds chain-of-thought data at ﬁnetuning
stage and shows signiﬁcant improvements. We also
study this type of models and name them as OPT-
CoT ((C) in Figure 3). However, from previous
research it still remains unclear whether the im-
provement comes from simply adding more train-
ing data or ﬁnetuning on rationales actually helps.
We conduct rigorous evaluations to answer this
question.
6
Conclusions
We introduce ALERT, a benchmark for assessing
LLMs’ reasoning abilities. It spans over 20 datasets
and covers 10 different reasoning skills. With this
benchmark, we further explore what is the meaning
of ﬁnetuning for these complex tasks. Our exper-
iments reveal that LLMs do not rely on memoriz-
ing training data, but are capable to learn diverse
reasoning skills, such as textual entailment, abduc-
tive reasoning and analogical reasoning. While
we found that in general ﬁnetuning yields perfor-
mance improvement, we also show some side ef-

fects. LLMs memorize the data template represen-
tation and templates seen during ﬁnetuning, thus
reducing the robustness of the model to generalized
settings. CoT-ﬁnetuning alleviates this problem to
a certain extent, but it is still less robust compared
to the pre-trained model.
References
Shourya Aggarwal, Divyanshu Mandowara, Vishwa-
jeet Agrawal, Dinesh Khandelwal, Parag Singla,
and Dinesh Garg. 2021.
Explanations for Com-
monsenseQA: New Dataset and Models.
In
Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural
Language Processing (Volume 1:
Long Papers),
pages 3050–3065, Online. Association for Compu-
tational Linguistics.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian-
feng Gao, and Yejin Choi. 2020. Piqa: Reasoning
about physical commonsense in natural language.
In Thirty-Fourth AAAI Conference on Artiﬁcial
Intelligence.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020a. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020b. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Oana-Maria Camburu,
Tim Rocktäschel,
Thomas
Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat-
ural language inference with natural language expla-
nations. Advances in Neural Information Processing
Systems, 31.
Zeming Chen and Qiyue Gao. 2022.
Curriculum:
A broad-coverage benchmark for linguistic phe-
nomena in natural language understanding. arXiv
preprint arXiv:2204.06283.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022a. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022b. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-ﬁnetuned language mod-
els. arXiv preprint arXiv:2210.11416.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. 2021.
Training veri-
ﬁers to solve math word problems. arXiv preprint
arXiv:2110.14168.
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,
and Tushar Khot. 2022. Complexity-based prompt-
ing for multi-step reasoning.
arXiv preprint
arXiv:2210.00720.
Nancy Fulda, Nathan Tibbetts, Zachary Brown, and
David Wingate. 2017.
Harvesting common-sense
navigational knowledge for robotics from uncurated
text corpora.
In Conference on Robot Learning,
pages 525–534. PMLR.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies.
Transactions of the
Association for Computational Linguistics, 9:346–
361.
Suchin
Gururangan,
Ana
Marasovi´c,
Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A Smith. 2020. Don’t stop pretraining:
adapt language models to domains and tasks. arXiv
preprint arXiv:2004.10964.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. 2020. Measuring massive multitask language
understanding. arXiv preprint arXiv:2009.03300.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou,
Mantas Mazeika,
Dawn Song,
and Ja-
cob
Steinhardt.
2021a.
Measuring
massive
multitask language understanding.
Proceedings
of
the
International
Conference
on
Learning
Representations (ICLR).
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021b. Measuring mathematical
problem solving with the math dataset. NeurIPS.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021c.
Measuring mathemati-
cal problem solving with the math dataset. arXiv
preprint arXiv:2103.03874.
Mark Hopkins, Ronan Le Bras, Cristian Petrescu-
Prahova, Gabriel Stanovsky, Hannaneh Hajishirzi,
and Rik Koncel-Kedziorski. 2019.
Semeval-2019
task 10: math question answering. In Proceedings
of the 13th International Workshop on Semantic
Evaluation, pages 893–899.

Jonathan Kobbe, Ioana Hulpus,, and Heiner Stucken-
schmidt. 2020. Unsupervised stance detection for ar-
guments from consequences. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 50–60.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022.
Large
language models are zero-shot reasoners.
arXiv
preprint arXiv:2205.11916.
Yash Kumar Lal, Nathanael Chambers, Raymond
Mooney,
and Niranjan Balasubramanian. 2021.
TellMeWhy: A dataset for answering why-questions
in narratives.
In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021,
pages 596–610, Online. Association for Computa-
tional Linguistics.
Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen,
Pei Zhou, Chandra Bhagavatula, Yejin Choi, and
Xiang Ren. 2020.
CommonGen: A constrained
text generation challenge for generative common-
sense reasoning.
In Findings of the Association
for Computational Linguistics: EMNLP 2020, pages
1823–1840, Online. Association for Computational
Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries.
In Text summarization
branches out, pages 74–81.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017.
Program induction by rationale gen-
eration: Learning to solve and explain algebraic
word problems. In Proceedings of the 55th Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 158–
167, Vancouver, Canada. Association for Computa-
tional Linguistics.
Ilya Loshchilov and Frank Hutter. 2017.
Fixing
weight decay regularization in adam.
CoRR,
abs/1711.05101.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question answer-
ing. arXiv preprint arXiv:1809.02789.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
In ACL.
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016.
A cor-
pus and cloze evaluation for deeper understanding
of commonsense stories. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 839–849.
Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison
Gopnik, and Thomas L Grifﬁths. 2018.
Evaluat-
ing theory of mind in question answering.
arXiv
preprint arXiv:1808.09352.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,
Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma,
David Luan, et al. 2021. Show your work: Scratch-
pads for intermediate computation with language
models. arXiv preprint arXiv:2112.00114.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022.
Training language models to follow in-
structions with human feedback.
arXiv preprint
arXiv:2203.02155.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming
Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017.
Automatic differentiation in pytorch.
In NIPS 2017 Workshop on Autodiff.
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019. Explain yourself!
leveraging language models for commonsense rea-
soning. arXiv preprint arXiv:1906.02361.
Siva Reddy, Danqi Chen, and Christopher D. Manning.
2019.
CoQA: A conversational question answer-
ing challenge. Transactions of the Association for
Computational Linguistics, 7:249–266.
Christopher Rytting and David Wingate. 2021. Lever-
aging the inductive bias of large language models
for abstract textual reasoning. Advances in Neural
Information Processing Systems, 34:17111–17122.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun
Raja, et al. 2021. Multitask prompted training en-
ables zero-shot task generalization. arXiv preprint
arXiv:2110.08207.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta,
Adrià Garriga-Alonso, et al. 2022.
Beyond the
imitation game: Quantifying and extrapolating the
capabilities of language models.
arXiv preprint
arXiv:2206.04615.
Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin
Choi, and Claire Cardie. 2019. DREAM: A chal-
lenge dataset and models for dialogue-based reading
comprehension. Transactions of the Association for
Computational Linguistics.
Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter
Clark. 2020. Proofwriter: Generating implications,
proofs, and abductive statements over natural lan-
guage. arXiv preprint arXiv:2012.13048.

Alex Wang,
Yada Pruksachatkun,
Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R Bowman. 2019.
Super-
glue:
A stickier benchmark for general-purpose
language understanding systems.
arXiv preprint
arXiv:1905.00537.
Alex Wang, Amanpreet Singh, Julian Michael, Fe-
lix
Hill,
Omer
Levy,
and
Samuel
Bowman.
2018.
GLUE: A multi-task benchmark and anal-
ysis platform for natural language understanding.
In Proceedings of the 2018 EMNLP Workshop
BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP, pages 353–355, Brussels, Bel-
gium. Association for Computational Linguistics.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc
Le, Ed Chi, and Denny Zhou. 2022a.
Rationale-
augmented ensembles in language models.
arXiv
preprint arXiv:2207.00747.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, and Denny Zhou. 2022b. Self-consistency
improves chain of thought reasoning in language
models. arXiv preprint arXiv:2203.11171.
Yizhong Wang, Swaroop Mishra, Pegah Alipoor-
molabashi,
Yeganeh Kordi,
Amirreza Mirzaei,
Anjana Arunkumar, Arjun Ashok, Arut Selvan
Dhanasekaran, Atharva Naik, David Stap, et al.
2022c. Super-naturalinstructions:generalization via
declarative instructions on 1600+ tasks. In EMNLP.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021a. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021b. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903.
Nathaniel Weir, João Sedoc, and Benjamin Van Durme.
2020.
Cod3s:
Diverse generation with dis-
crete
semantic
signatures.
arXiv
preprint
arXiv:2010.02882.
Johannes Welbl, Nelson F Liu, and Matt Gardner. 2017.
Crowdsourcing multiple choice science questions.
arXiv preprint arXiv:1707.06209.
Peter West, Chandra Bhagavatula, Jack Hessel, Jena
Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,
Sean Welleck, and Yejin Choi. 2022.
Symbolic
knowledge distillation: from general language mod-
els to commonsense models. In Proceedings of the
2022 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, pages 4602–4625,
Seattle, United States. Association for Computa-
tional Linguistics.
Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-
der M Rush, Bart Van Merriënboer, Armand Joulin,
and Tomas Mikolov. 2015.
Towards ai-complete
question answering: A set of prerequisite toy tasks.
arXiv preprint arXiv:1502.05698.
Adina Williams, Tristan Thrush, and Douwe Kiela.
2022.
Anlizing the adversarial natural language
inference dataset.
In Proceedings of the 5th
Annual Meeting of the Society for Computation in
Linguistics, pages 23–54. Association for Computa-
tional Linguistics.
Hongming Zhang, Xinran Zhao, and Yangqiu Song.
2020.
WinoWhy:
A deep diagnosis of es-
sential commonsense knowledge for answering
Winograd schema challenge.
In Proceedings of
the 58th Annual Meeting of the Association for
Computational Linguistics, pages 5736–5745, On-
line. Association for Computational Linguistics.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al.
2022. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Olivier Bousquet, Quoc Le, and Ed Chi. 2022.
Least-to-most prompting enables complex reason-
ing in large language models.
arXiv preprint
arXiv:2205.10625.

A
More Details about Data Usage
A.1
Reasoning Benchmark
Table 3 shows detailed reasoning benchmark.
A.2
Training Corpus (cont. from §3.2)
We used 10 datasets for ﬁnetuning, which contain
6 different reasoning skills.
A.3
Development Data Details
Our ﬁnetuning models are tuned on pretrained
LLMs on the ﬁnetuning corpus with the goal of
improving the performance of unseen tasks. For
example, blocks (B) and (C) in Figure 3 are show-
ing models that are ﬁnetuned on tasks B,C,D and
the goal is to achieve good results on task A.
Checkpoint selection can determine the ﬁnal per-
formance of the LLMs to a very large extent. There
are several ways to select checkpoints: (i) select
checkpoint of the last iteration; (ii) select check-
point based on perplexity or loss from validation
datasets of ﬁnetuning corpus (validation datasets
of task B, C, D); (iii) select checkpoint based on
perplexity or loss from validation datasets of evalu-
ation corpus (validation datasets of task A);
In order to achieve a better performance on evalu-
ation corpus, a common approach is to use methods
like (iii) to select checkpoint. However, we don’t
want LLMs to overﬁt to the distribution of our ﬁnal
evaluation corpus. We initially used the method
(ii) but found that it did’t work well. We speculate
that because some tasks in our ﬁnetuning corpus
do not have a validation set, this results in a distri-
bution mismatch issue. We thus select 3 tasks from
NIV2 benchmark and compile a development set
that does not have any overlaps with our ﬁnetun-
ing data or evaluation data. There are 3 datasets
used as our development set for checkpoint selec-
tion: task 247 dream answer generation (Sun et al.,
2019), task 118 semeval and task 10 open vocab-
ulary mathematical answer generation (Hopkins
et al., 2019) and anli r1 entailment (Williams et al.,
2022)
A.4
Vocabulary Overlaps (Cont. from
§ 4.2.1)
We measure unigram vocabulary overlaps between
our ﬁnetuning corpus and the evaluation corpus
(reasoning benchmark).
B
Curriculum Benchmark Results (Cont.
from §5)
We randomly selected one dataset from each rea-
soning skill and reported the results of GPT-3
(Brown et al., 2020b) (text-davinci engine). Since
all of the data has been converted to NLI format, we
measure classiﬁcation accuracy of GPT-3 model.
From Table 7, we can see that even GPT-3 achieves
a pretty random results on these datasets. Through
our analysis, we found that it is not because those
tasks are too difﬁcult for GPT-3, it is because
curriculum benchmark forcing all the data to be
NLI format, resulting in unnatural data expression,
which made GPT-3 fail on it. We conclude that the
curriculum benchmark may be suitable for classi-
ﬁcation ﬁnetuned models, but it is not suitable for
language models for in-context learning.

Reasoning
Skills
Task ID
Datasets
Logistic
Reasoning
62
697
bigbench repeat copy logic (Srivastava et al., 2022)
mmmlu answer generation formal logic (Hendrycks et al., 2021a)
Causal
Reasoning
393
1386
1387
1388
plausible result generation (Weir et al., 2020)
anli r2 entailment (Williams et al., 2022)
anli r3 entailment (Williams et al., 2022)
cb entailment (Wang et al., 2019)
Commonsense
Reasoning
80
102
591
1286
piqa answer generation (Bisk et al., 2020)
commongen sentence generation (Lin et al., 2020)
sciq answer generation (Welbl et al., 2017)
openbookqa question answering (Mihaylov et al., 2018)
Texual
Entailment
1386
1387
1388
1344
anli r2 entailment (Williams et al., 2022)
anli r3 entailment (Williams et al., 2022)
cb entailment (Wang et al., 2019)
glue entailment classiﬁcation (Wang et al., 2018)
Mathematics
104
119
697
semeval closed vocabulary math answer generation (Hopkins et al., 2019)
semeval geometric math answer generation (Hopkins et al., 2019)
mmmlu answer generation formal logic (Hendrycks et al., 2021a)
Abductive
Reasoning
332
tellmewhy answer generation (Lal et al., 2021)
Spatial
Reasoning
83
80
151
babi t1 single supporting fact answer generation (Weston et al., 2015)
piqa answer generation (Bisk et al., 2020)
tomqa ﬁnd location easy clean (Nematzadeh et al., 2018)
Analogical
Reasoning
102
1152
commongen sentence generation (Lin et al., 2020)
bard analogical reasoning causation (Fulda et al., 2017)
Argument
Reasoning
513
514
argument stance classiﬁcation (Kobbe et al., 2020)
argument consequence classiﬁcation (Kobbe et al., 2020)
Deductive
Reasoning
216
rocstories correct answer generation (Mostafazadeh et al., 2016)
Table 3: Details about ALERT benchmark.
Datasets
Train Size
Val Size
Test Size
Reasoning Skills
ProofWriter
69,810
10,190
20,030
Logical Reasoning, Causal Reasoning
StrategyQA
2,290
-
490
Commonsense Reasoning
ECQA
7,598
1,090
2,194
Commonsense Reasoning
CoQA
10,8647
7,983
-
Textual Entailment
GSM8K
7,473
-
1,319
Mathematics
AQUA-RAT
97,467
254
254
Mathematics
ESNLI
549,367
9,842
9,824
Commonsense Reasoning, Logical Reasoning, Textual Entailment
MATH
7,500
-
5,000
Mathematics
CoS-E
9,741
1,221
-
Commonsense Reasoning
WinoWhy
273
-
-
Abductive Reasoning, Commonsense Reasoning
Table 4: Training corpus for ﬁnetuning. (Cont. from § 3.2)

Task ID
Datasets
Reasoning Skills
247
dream answer generation (Sun et al., 2019)
Logical Reasoning
Commonsense Reasoning
118
semeval open vocabulary mathematical
answer generation (Hopkins et al., 2019)
Commonsense Reasoning
Mathematics
1385
anli r1 entailment (Williams et al., 2022)
Textual Entailment
Commonsense Reasoning
Causal Reasoning
Table 5: Dev set for checkpoint selection
Category
Datasets
Vocabulary Overlaps
0% to 10%
bigbench repeat copy logic (Srivastava et al., 2022)
babi t1 single supporting fact answer generation (Weston et al., 2015)
semeval closed vocabulary math answer generation (Hopkins et al., 2019)
semeval geometric math answer generation (Hopkins et al., 2019)
tomqa ﬁnd location easy clean (Nematzadeh et al., 2018)
plausible result generation (Weir et al., 2020)
argument stance classiﬁcation (Kobbe et al., 2020)
argument consequence classiﬁcation (Kobbe et al., 2020)
mmmlu answer generation formal logic (Hendrycks et al., 2021a)
bard analogical reasoning causation (Fulda et al., 2017)
1.59%
0.38%
7.90%
5.84%
0.94%
3.72%
6.04%
6.11%
5.35%
0.45%
10% to 30%
commongen sentence generation (Lin et al., 2020)
tellmewhy answer generation (Lal et al., 2021)
cb entailment (Wang et al., 2019)
29.31%
28.05%
20.97%
over 30%
piqa answer generation (Bisk et al., 2020)
rocstories correct answer generation (Mostafazadeh et al., 2016)
sciq answer generation (Welbl et al., 2017)
openbookqa question answering (Mihaylov et al., 2018)
glue entailment classiﬁcation (Wang et al., 2018)
anli r2 entailment (Williams et al., 2022)
anli r3 entailment (Williams et al., 2022)
42.51%
57.45%
32.54%
48.2%
55.19%
43.37%
53.13%
Table 6: Vocabulary overlap.
Datasets
Random score
GPT-3 Davinci score
Boolean
34.50%
31.80%
Physical
49.08%
50.00%
Entailment Tree
50.88%
54.41%
Event Semantic
54.08%
55.89%
Spatial
49.80%
49.30%
Table 7: Classiﬁcation accuracy by GPT-3 davinci-engine on the Curriculum benchmark.
The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?
The answer is 29 apples. The cafeteria started with 23 apples and bought 6 more, for a total of 23 + 6 = «23+6=29»29 apples .
Select
the
best
translation
into
predicate
logic.
David
teaches
Chris.
(c:
Chris;
d:
David;
Txy:
x
teaches
y)
(A)Tdc;(B)Tcd;(C)Tcc;(D)dTc.
The
answer
is
(B) Tcd. In predicate logic, the predicate (in this case, "T") is written before the subject and object of the sentence.
Since David (d) is the teacher and Chris (c) is the student, the correct translation is Tcd, which reads as "David teaches Chris."
Isabella entered the hall. Olivia entered the hall. The apple is in the blue_treasure_chest. Olivia exited the hall. Isabella
moved the apple to the green_basket. Question: Where does Isabella think that Olivia searches for the apple? The answer
is It is impossible for me to answer this question as I have no information about the thoughts or actions of Isabella or Olivia.
Additionally, I do not have access to browse any information beyond my training data, which ends in 2021 .
Table 8: Examples from tasks that require reasoning skills and generated outputs from ChatGPT. The faild outputs are
highlighted in red.

