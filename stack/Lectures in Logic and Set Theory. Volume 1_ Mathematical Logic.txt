
This page intentionally left blank

CAMBRIDGE STUDIES IN
ADVANCED MATHEMATICS
EDITORIAL BOARD
B. BOLLOBAS, W. FULTON, A. KATOK, F. KIRWAN,
P. SARNAK
Lectures in Logic and Set Theory Volume 1
This two-volume work bridges the gap between introductory expositions of
logic or set theory on one hand, and the research literature on the other. It can
be used as a text in an advanced undergraduate or beginning graduate course
in mathematics, computer science, or philosophy. The volumes are written in
a user-friendly conversational lecture style that makes them equally effective
for self-study or class use.
Volume 1 includes formal proof techniques, a section on applications of
compactness (including non-standard analysis), a generous dose of computa-
bility and its relation to the incompleteness phenomenon, and the ï¬rst presen-
tation of a complete proof of GÂ¨odelâ€™s second incompleteness theorem since
Hilbert and Bernayâ€™s Grundlagen.

Already published
2
K. Petersen Ergodic theory
3
P.T. Johnstone Stone spaces
5
J.-P. Kahane Some random series of functions, 2nd edition
7
J. Lambek & P.J. Scott Introduction to higher-order categorical logic
8
H. Matsumura Commutative ring theory
10
M. Aschbacher Finite group theory, 2nd edition
11
J.L. Alperin Local representation theory
12
P. Koosis The logarithmic integral I
14
S.J. Patterson An introduction to the theory of the Riemann zeta-function
15
H.J. Baues Algebraic homotopy
16
V.S. Varadarajan Introduction to harmonic analysis on semisimple Lie groups
17
W. Dicks & M. Dunwoody Groups acting on graphs
19
R. Fritsch & R. Piccinini Cellular structures in topology
20
H. Klingen Introductory lectures on Siegel modular forms
21
P. Koosis The logarithmic integral II
22
M.J. Collins Representations and characters of ï¬nite groups
24
H. Kunita Stochastic ï¬‚ows and stochastic differential equations
25
P. Wojtaszczyk Banach spaces for analysts
26
J.E. Gilbert & M.A.M. Murray Clifford algebras and Dirac operators in harmonic analysis
27
A. Frohlich & M.J. Taylor Algebraic number theory
28
K. Goebel & W.A. Kirk Topics in metric ï¬xed point theory
29
J.F. Humphreys Reï¬‚ection groups and Coxeter groups
30
D.J. Benson Representations and cohomology I
31
D.J. Benson Representations and cohomology II
32
C. Allday & V. Puppe Cohomological methods in transformation groups
33
C. Soule et al. Lectures on Arakelov geometry
34
A. Ambrosetti & G. Prodi A primer of nonlinear analysis
35
J. Palis & F. Takens Hyperbolicity, stability and chaos at homoclinic bifurcations
37
Y. Meyer Wavelets and operators 1
38
C. Weibel, An introduction to homological algebra
39
W. Bruns & J. Herzog Cohen-Macaulay rings
40
V. Snaith Explicit Brauer induction
41
G. Laumon Cohomology of Drinfeld modular varieties I
42
E.B. Davies Spectral theory and differential operators
43
J. Diestel, H. Jarchow, & A. Tonge Absolutely summing operators
44
P. Mattila Geometry of sets and measures in Euclidean spaces
45
R. Pinsky Positive harmonic functions and diffusion
46
G. Tenenbaum Introduction to analytic and probabilistic number theory
47
C. Peskine An algebraic introduction to complex projective geometry
48
Y. Meyer & R. Coifman Wavelets
49
R. Stanley Enumerative combinatorics I
50
I. Porteous Clifford algebras and the classical groups
51
M. Audin Spinning tops
52
V. Jurdjevic Geometric control theory
53
H. Volklein Groups as Galois groups
54
J. Le Potier Lectures on vector bundles
55
D. Bump Automorphic forms and representations
56
G. Laumon Cohomology of Drinfeld modular varieties II
57
D.M. Clark & B.A. Davey Natural dualities for the working algebraist
58
J. McCleary A userâ€™s guide to spectral sequences II
59
P. Taylor Practical foundations of mathematics
60
M.P. Brodmann & R.Y. Sharp Local cohomology
61
J.D. Dixon et al. Analytic pro-P groups
62
R. Stanley Enumerative combinatorics II
63
R.M. Dudley Uniform central limit theorems
64
J. Jost & X. Li-Jost Calculus of variations
65
A.J. Berrick & M.E. Keating An introduction to rings and modules
66
S. Morosawa Holomorphic dynamics
67
A.J. Berrick & M.E. Keating Categories and modules with K-theory in view
68
K. Sato Levy processes and inï¬nitely divisible distributions
69
H. Hida Modular forms and Galois cohomology
70
R. Iorio & V. Iorio Fourier analysis and partial differential equations
71
R. Blei Analysis in integer and fractional dimensions
72
F. Borceaux & G. Janelidze Galois theories
73
B. Bollobas Random graphs

LECTURES IN LOGIC
AND SET THEORY
Volume 1: Mathematical Logic
GEORGE TOURLAKIS
York University

ï£ï¡ï­ï¢ï²ï©ï¤ï§ï¥ ïµï®ï©ï¶ï¥ï²ï³ï©ï´ï¹ ï°ï²ï¥ï³ï³
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, SÃ£o Paulo
Cambridge University Press
The Edinburgh Building, Cambridge ï£ï¢ïœ² ïœ²ï²ïµ, United Kingdom
First published in print format 
ISBN-13   978-0-521-75373-9 hardback
ISBN-13   978-0-511-06871-3 eBook (EBL)
Â© George Tourlakis 2003
2003
Information on this title: www.cambridge.org/9780521753739
This book is in copyright. Subject to statutory exception and to the provision of
relevant collective licensing agreements, no reproduction of any part may take place
without the written permission of Cambridge University Press.
ISBN-10   0-511-06871-9 eBook (EBL)
ISBN-10   0-521-75373-2 hardback
Cambridge University Press has no responsibility for the persistence or accuracy of
ïµï²ï¬s for external or third-party internet websites referred to in this book, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.
Published in the United States by Cambridge University Press, New York
www.cambridge.org

Î³Î¹Î± Ï„Î·Î½ Î´ÎµÏƒÏ€oÎ¹Î½Î±, Ï„Î·Î½ ÂµÎ±ÏÎ¹Î½Î± ÎºÎ±Î¹ Ï„oÎ½ Î³Î¹Î±Î½Î½Î·


Contents
Preface
page ix
I
Basic Logic
1
I.1
First Order Languages
5
I.2
A Digression into the Metatheory:
Informal Induction and Recursion
19
I.3
Axioms and Rules of Inference
28
I.4
Basic Metatheorems
42
I.5
Semantics; Soundness, Completeness, Compactness
52
I.6
Substructures, Diagrams, and Applications
75
I.7
Deï¬ned Symbols
112
I.8
Computability and Uncomputability
123
I.9
Arithmetic, Deï¬nability, Undeï¬nability,
and Incompletableness
155
I.10
Exercises
191
II
The Second Incompleteness Theorem
205
II.1
Peano Arithmetic
206
II.2
A Formal Î²-Function
232
II.3
Formal Primitive Recursion
248
II.4
The Boldface  and 
256
II.5
Arithmetization
265
II.6
Derivability Conditions; Fixed Points
272
II.7
Exercises
316
Bibliography
319
List of Symbols
321
Index
323
vii


Preface
Both volumes in this series are about what mathematicians, especially logicians,
call the â€œfoundationsâ€ (of mathematics) â€“ that is, the tools of the axiomatic
method, an assessment of their effectiveness, and two major examples of ap-
plication of these tools, namely, in the development of number theory and set
theory.
There have been, in hindsight, two main reasons for writing this volume.
One was the existence of notes I wrote for my lectures in mathematical logic
and computability that had been accumulating over the span of several years
and badly needed sorting out. The other was the need to write a small section
on logic, â€œA Bit of Logicâ€ as I originally called it, that would bootstrap my
volume on set theoryâ€  on which I had been labouring for a while. Well, one
thing led to another, and a 30 or so page section that I initially wrote for the
latter purpose grew to become a self-standing volume of some 300 pages. You
see, this material on logic is a good story and, as with all good stories, one does
get carried away wanting to tell more.
I decided to include what many people will consider, I should hope, as
being the absolutely essential topics in proof, model, and recursion theory â€“
â€œabsolutely essentialâ€ in the context of courses taught near the upper end of
undergraduate, and at the lower end of graduate curricula in mathematics, com-
puter science, or philosophy. But no more.â€¡ This is the substance of Chapter I;
hence its title â€œBasic Logicâ€.
â€  A chapter by that name now carries out these bootstrapping duties â€“ the proverbial â€œChapter 0â€
(actually Chapter I) of volume 2.
â€¡ These topics include the foundation and development of non-standard analysis up to the ex-
treme value theorem, elementary equivalence, diagrams, and LÂ¨owenheim-Skolem theorems, and
GÂ¨odelâ€™s ï¬rst incompleteness theorem (along with Rosserâ€™s sharpening).
ix

x
Preface
But then it occurred to me to also say something about one of the most
remarkable theorems of logic â€“ arguably the most remarkable â€“ about the lim-
itations of formalized theories: GÂ¨odelâ€™s second incompleteness theorem. Now,
like most reasonable people, I never doubted that this theorem is true, but, as the
devil is in the details, I decided to learn its proof â€“ right from Peanoâ€™s axioms.
What better way to do this than writing down the proof, gory details and all?
This is what Chapter II is about.â€ 
As a side effect, the chapter includes many theorems and techniques of one
of the two most important â€“ from the point of view of foundations â€“ â€œappliedâ€
logics (formalized theories), namely, Peano arithmetic (the other one, set theory,
taking all of volume 2).
I have hinted above that this (and the second) volume are aimed at a fairly
advanced reader: The level of exposition is designed to ï¬t a spectrum of math-
ematical sophistication from third year undergraduate to junior graduate level
(each group will ï¬nd here its favourite sections that serve its interests and level
of preparation â€“ and should not hesitate to judiciously omit topics).
There are no speciï¬c prerequisites beyond some immersion in the â€œproof
cultureâ€, as this is attainable through junior level courses in calculus, linear al-
gebra, or discrete mathematics. However, some familiarity with concepts from
elementary naÂ¨Ä±ve set theory such as ï¬niteness, inï¬nity, countability, and un-
countability will be an asset.â€¡
A word on approach. I have tried to make these lectures user-friendly, and thus
accessible to readers who do not have the beneï¬t of an instructorâ€™s guidance.
Devices to that end include anticipation of questions, frequent promptings for
the reader to rethink an issue that might be misunderstood if glossed over
(â€œPausesâ€), and the marking of important passages, by
, as well as those that
can be skipped at ï¬rst reading, by
.
Moreover, I give (mostly) very detailed proofs, as I know from experience
that omitting details normally annoys students.
â€  It is strongly conjectured here that this is the only complete proof in print other than the one
that was given in Hilbert and Bernays (1968). It is fair to clarify that I use the term â€œcomplete
proofâ€ with a strong assumption in mind: That the axiom system we start with is just Peano
arithmetic. Proofs based on a stronger â€“ thus technically more convenient â€“ system, namely,
primitive recursive arithmetic, have already appeared in print (Diller (1976), SmoryÂ´nski (1985)).
The difï¬culty with using Peano arithmetic as the starting point is that the only primitive recursive
functions initially available are the successor, identity, plus, and times. An awful amount of work
is needed â€“ a preliminary â€œcoding trickâ€ â€“ to prove that all the rest of the primitive recursive
functions â€œexistâ€. By then are we already midway in Chapter II, and only then are we ready to
build GÂ¨odel numbers of terms, formulas, and proofs and to prove the theorem.
â€¡ I have included a short paragraph nicknamed â€œa crash course on countable setsâ€ (Section I.5,
p. 62), which certainly helps. But having seen these topics before helps even more.

Preface
xi
The ï¬rst chapter has a lot of exercises (the second having proportionally
fewer). Many of these have hints, but none are marked as â€œhardâ€ vs. â€œjust about
rightâ€, a subjective distinction I prefer to avoid. In this connection here is some
good advice I received when I was a graduate student at the University of
Toronto: â€œAttempt all the problems. Those you can do, donâ€™t do. Do the ones
you cannotâ€.
What to read. Consistently with the advice above, I suggest that you read this
volume from cover to cover â€“ including footnotes! â€“ skipping only what you
already know. Now, in a class environment this advice may be impossible to
take, due to scope and time constraints. An undergraduate (one semester) course
in logic at the third year level will probably cover Sections I.1â€“I.5, making light
of Section I.2, and will introduce the student to the elements of computability
along with a hand-waving â€œproofâ€ of GÂ¨odelâ€™s ï¬rst incompleteness theorem (the
â€œsemantic versionâ€ ought to sufï¬ce). A fourth year class will probably attempt
to cover the entire Chapter I. A ï¬rst year graduate class has no more time than
the others at its disposal, but it usually goes much faster, skipping over familiar
ground, thus it will probably additionally cover Peano arithmetic and will get
to see how GÂ¨odelâ€™s second theorem follows from LÂ¨obâ€™s derivability conditions.
Acknowledgments. I wish to offer my gratitude to all those who taught me,
a group led by my parents and too large to enumerate. I certainly include my
students here. I also include Raymond Wilderâ€™s book on the foundations of
mathematics, which introduced me, long long ago, to this very exciting ï¬eld
and whetted my appetite for more (Wilder (1963)).
I should like to thank the staff at Cambridge University Press for their pro-
fessionalism, support, and cooperation, with special appreciation due to Lauren
Cowles and Caitlin Doggart, who made all the steps of this process, from ref-
ereeing to production, totally painless.
This volume is the last installment of a long project that would have not been
successful without the support and warmth of an understanding family (thank
you).
I ï¬nally wish to record my appreciation to Donald Knuth and Leslie Lamport
for the typesetting tools TEX and LATEX that they have made available to the tech-
nical writing community, making the writing of books such as this one almost
easy.
George Tourlakis
Toronto, March 2002


I
Basic Logic
Logic is the science of reasoning. Mathematical logic applies to mathematical
reasoning â€“ the art and science of writing down deductions. This volume is
about the form, meaning, use, and limitations of logical deductions, also called
proofs. While the user of mathematical logic will practise the various proof
techniques with a view of applying them in everyday mathematical practice,
the student of the subject will also want to know about the power and limitations
of the deductive apparatus. We will ï¬nd that there are some inherent limitations
in the quest to discover truth by purely formal â€“ that is, syntactic â€“ techniques.
In the process we will also discover a close afï¬nity between formal proofs and
computations that persists all the way up to and including issues of limitations:
Not only is there a remarkable similarity between the types of respective limi-
tations (computations vs. uncomputable functions, and proofs vs. unprovable,
but â€œtrueâ€, sentences), but, in a way, you cannot have one type of limitation
without having the other.
The modern use of the term mathematical logic encompasses (at least) the
areas of proof theory (it studies the structure, properties, and limitations of
proofs), model theory (it studies the interplay between syntax and meaning â€“ or
semantics â€“ by looking at the algebraic structures where formal languages are
interpreted), recursion theory (or computability, which studies the properties
and limitations of algorithmic processes), and set theory. The fact that the last-
mentioned will totally occupy our attention in volume 2 is reï¬‚ected in the
prominence of the term in the title of these lectures. It also reï¬‚ects a tendency,
even today, to think of set theory as a branch in its own right, rather than as an
â€œareaâ€ under a wider umbrella.
1

2
I. Basic Logic
Volume 1 is a brief study of the other three areas of logicâ€  mentioned above.
This is the point where an author usually apologizes for what has been omitted,
blaming space or scope (or competence) limitations. Let me start by outlin-
ing what is included: â€œStandardâ€ phenomena such as completeness, compact-
ness and its startling application to analysis, incompleteness or unprovabil-
ity (including a complete proof of the second incompleteness theorem), and a
fair amount of recursion theory are thoroughly discussed. Recursion theory,
or computability, is of interest to a wide range of audiences, including stu-
dents with main areas of study such as computer science, philosophy, and, of
course, mathematical logic. It studies among other things the phenomenon of
uncomputability, which is closely related to that of unprovability, as we see in
Section I.9.
Among the topics that I have deliberately left out are certain algebraic tech-
niques in model theory (such as the method of ultrapowers), formal interpre-
tations of one theory into another,â€¡ the introduction of â€œotherâ€ logics (modal,
higher order, intuitionistic, etc.), and several topics in recursion theory (oracle
computability, Turing reducibility, recursive operators, degrees, Postâ€™s theorem
in the arithmetic hierarchy, the analytic hierarchy, etc.) â€“ but then, the decision
to stop writing within 300 or so pages was ï¬rm. On the other hand, the topics
included here form a synergistic whole in that I have (largely) included at every
stage material that is prerequisite to what follows. The absence of a section on
propositional calculus is deliberate, as it does not in my opinion further the
understanding of logic in any substantial way, while it delays oneâ€™s plunging
into what really matters. To compensate, I include all tautologies as â€œproposi-
tionalâ€ (or Boolean) logical axioms and present a mini-course on propositional
calculus in the exercises of this chapter (I.26â€“I.41, pp. 193â€“195), including the
completeness and compactness of the calculus.
It is inevitable that the language of sets intrudes in this chapter (as it indeed
does in all mathematics) and, more importantly, some of the results of (informal)
set theory are needed here (especially in our proofs of the completeness and
compactness metatheorems). Conversely, formal set theory of volume 2 needs
some of the results developed here. This â€œchicken or eggâ€ phenomenon is often
called â€œbootstrappingâ€ (not to be confused with â€œcircularityâ€ â€“ which it is notÂ§),
the term suggesting one pulling oneself up by oneâ€™s bootstraps.Â¶
â€  I trust that the reader will not object to my dropping the qualiï¬er â€œmathematicalâ€ from now on.
â€¡ Although this topic is included in volume 2 (Chapter I), since it is employed in the relative
consistency techniques applied there.
Â§ Only informal, or naÂ¨Ä±ve, set theory notation and results are needed in Chapter I at the meta-level,
i.e, outside the formal system that logic is.
Â¶ I am told that Baron MÂ¨unchhausen was the ï¬rst one to apply this technique, with success.

I. Basic Logic
3
This is a good place to outline how our story will unfold: First, our objective is to
formalize the rules of reasoning in general â€“ as these apply to all mathematics â€“
and develop their properties. In particular, we will study the interaction between
formalized rules and their â€œintended meaningâ€ (semantics), as well as the limi-
tations of these formalized rules: That is, how good (= potent) are they for
capturing the informal notions of truth?
Secondly,oncewehaveacquiredthesetoolsofformalizedreasoning,westart
behaving (mostlyâ€ ) as users of formal logic so that we can discover important
theorems of two important mathematical theories: Peano arithmetic (Chapter II)
and set theory (volume 2).
By formalization (of logic) we understand the faithful representation or
simulation of the â€œreasoning processesâ€ of mathematics in general (pure logic),
or of a particular mathematical theory (applied logic: e.g., Peano arithmetic),
within an activity that â€“ in principle â€“ is driven exclusively by the form or syntax
of mathematical statements, totally ignoring their meaning.
We build, describe, and study the properties of this artiï¬cial replica of the
reasoning processes â€“ the formal theory â€“ within â€œeveryday mathematicsâ€ (also
called â€œinformalâ€ or â€œrealâ€ mathematics), using the usual abundance of mathe-
matical symbolism, notions, and techniques available to us, augmented by the
descriptive power of English (or Greek, or French, or German, or Russian,
or . . . , as particular circumstances or geography might dictate). This milieu
within which we build, pursue, and study our theories is often called the meta-
theory, or more generally, metamathematics. The language we speak while at
it, this mÂ´elange of mathematics and â€œnatural languageâ€, is the metalanguage.
Formalization turns mathematical theories into mathematical objects that
we can study. For example, such study may include interesting questions such
as â€œis the continuum hypothesis provable from the axioms of set theory?â€ or
â€œcan we prove the consistency of (axiomatic) Peano arithmetic within Peano
arithmetic?â€â€¡ This is analogous to building a â€œmodel airplaneâ€, a replica of the
real thing, with a view of studying through the replica the properties, power,
and limitations of the real thing.
But one can also use the formal theory to generate theorems, i.e., discover
â€œtruthsâ€ in the real domain by simply â€œrunningâ€ the simulation that this theory-
replica is.Â§ Running the simulation â€œby handâ€ (rather than using the program
â€  Some tasks in Chapter II of this volume, and some others in volume 2, will be to treat the â€œtheoryâ€
at hand as an object of study rather than using it, as a machine, to crank out theorems.
â€¡ By the way, the answer to both these questions is â€œnoâ€ (Cohen (1963) for the ï¬rst, GÂ¨odel (1938)
for the second).
Â§ The analogy implied in the terminology â€œrunning the simulationâ€ is apt. For formal theories such
as set theory and Peano arithmetic we can build within real mathematics a so-called â€œprovability

4
I. Basic Logic
of the previous footnote) means that you are acting as a â€œuserâ€ of the formal
system, a formalist, proving theorems through it. It turns out that once you get
the hang of it, it is easier and safer to reason formally than to do so informally.
The latter mode often mixes syntax and semantics (meaning), and there is
always the danger that the â€œuserâ€ may assign incorrect (i.e., convenient, but not
general ) meanings to the symbols that heâ€  manipulates, a phenomenon that has
distressed many a mathematics or computer science instructor.
â€œFormalism for the userâ€ is hardly a revolutionary slogan. It was advocated
by Hilbert, the founder of formalism, partly as a means of â€“ as he believedâ€¡ â€“
formulating mathematical theories in a manner that allows one to check them
(i.e., run â€œdiagnostic testsâ€ on them) for freedom from contradiction,Â§ but also
as the right way to â€œdoâ€ mathematics. By this proposal he hoped to salvage
mathematicsitself,which,Hilbertfelt,wasabouttobedestroyedbytheBrouwer
school of intuitionist thought. In a way, his program could bridge the gap
between the classical and the intuitionist camps, and there is some evidence
that Heyting (an inï¬‚uential intuitionist and contemporary of Hilbert) thought
that such a rapprochement was possible. After all, since meaning is irrelevant to
a formalist, then all that he is doing (in a proof) is shufï¬‚ing ï¬nite sequences of
symbols, never having to handle or argue about inï¬nite objects â€“ a good thing,
as far as an intuitionist is concerned.Â¶
predicateâ€, that is, a relationP(y, x) which is true of two natural numbers y and x just in case y
codes a proof of the formula coded by x. It turns out that P(y, x) has so simple a structure that it
is programmable, say in the C programming language. But then we can write a program (also in
C) as follows: â€œSystematically generate all the pairs of numbers (y, x). For each pair generated,
if P(y, x) holds, then print the formula coded by xâ€. Letting this process run for ever, we obtain
a listing of all the theorems of Peano arithmetic or set theory! This fact does not induce any
insomnia in mathematicians, since this is an extremely impractical way to obtain theorems. By
the way, we will see in Chapter II that either set theory or Peano arithmetic is sufï¬ciently strong
to formally express a provability predicate, and this leads to the incompletableness phenomenon.
â€  In this volume, the terms â€œheâ€, â€œhisâ€, â€œhimâ€, and their derivatives are by deï¬nition gender-neutral.
â€¡ This belief was unfounded, as GÂ¨odelâ€™s incompleteness theorems showed.
Â§ Hilbertâ€™s metatheory â€“ that is, the â€œworldâ€ or â€œlabâ€ outside the theory, where the replica is
actually manufactured â€“ was ï¬nitary. Thus â€“ Hilbert advocated â€“ all this theory building and
theory checking ought to be effected by ï¬nitary means. This ingredient of his â€œprogramâ€ was
consistent with peaceful coexistence with the intuitionists. And, alas, this ingredient was the one
that â€“ as some writers put it â€“ destroyed Hilbertâ€™s program to found mathematics on his version
of formalism. GÂ¨odelâ€™s incompleteness theorems showed that a ï¬nitary metatheory is not up to
the task.
Â¶ True, a formalist applies classical logic, while an intuitionist applies a different logic where, for
example, double negation is not removable. Yet, unlike a Platonist, a Hilbert-style formalist does
not believe â€“ or he does not have to disclose to his intuitionist friends that he might believe â€“ that
inï¬nite sets exist in the metatheory, as his tools are just ï¬nite symbol sequences. To appreciate the
tension here, consider this anecdote: It is said that when Kronecker â€“ the father of intuitionism â€“
was informed of Lindemannâ€™s proof (1882) that Ï€ is transcendental, while he granted that this was
an interesting result, he also dismissed it, suggesting that â€œÏ€â€ â€“ whose decimal expansion is, of

I.1. First Order Languages
5
In support of the â€œformalism for the userâ€ position we must deï¬nitely men-
tion the premier paradigm, Bourbakiâ€™s monumental work (1966a), which is a
formalization of a huge chunk of mathematics, including set theory, algebra,
topology, and theory of integration. This work is strictly for the user of mathe-
matics, not for the metamathematician who studies formal theories. Yet, it is
fully formalized, true to the spirit of Hilbert, and it comes in a self-contained
package, including a â€œChapter 0â€ on formal logic.
More recently, the proposal to employ formal reasoning as a tool has been
gainingsupportinanumberofcomputerscienceundergraduatecurricula,where
logic and discrete mathematics are taught in a formalized setting, starting with
a rigorous course in the two logical calculi (propositional and predicate), em-
phasizing the point of view of the user of logic (and mathematics) â€“ hence with
an attendant emphasis on â€œcalculatingâ€ (i.e., writing and annotating formal)
proofs. Pioneering works in this domain are the undergraduate text (1994) and
the paper (1995) of Gries and Schneider.
I.1. First Order Languages
In the most abstract (therefore simplest) manner of describing it, a formalized
mathematical theory consists of the following sets of things: A set of basic
or primitive symbols, V , used to build symbol sequences (also called strings,
or expressions, or words) â€œover V â€. A set of strings, Wff, over V , called the
formulas of the theory. Finally, a subset of Wff, called Thm, the set of theorems
of the theory.â€ 
Well, this is the extension of a theory, that is, the explicit set of objects in it.
How is a theory â€œgivenâ€?
In most cases of interest to the mathematician it is given by V and two
sets of simple rules: formula-building rules and theorem-building rules. Rules
from the ï¬rst set allow us to build, or generate, Wff from V . The rules of the
second set generate Thm from Wff. In short (e.g., Bourbaki (1966b)), a theory
consists of an alphabet of primitive symbols, some rules used to generate the
â€œlanguage of the theoryâ€ (meaning, essentially, Wff) from these symbols, and
some additional rules used to generate the theorems. We expand on this below:
course, inï¬nite but not periodic â€“ â€œdoes not existâ€ (see Wilder (1963, p. 193)). We are not to pro-
pound the tenets of intuitionism here, but it is fair to state that inï¬nite sets are possible in intuition-
istic mathematics as this has later evolved in the hands of Brouwer and his Amsterdam â€œschoolâ€.
However, such sets must be (like all sets of intuitionistic mathematics) ï¬nitely generated â€“ just
as our formal languages and the set of theorems are (the latter provided our axioms are too) â€“ in
a sense that may be familiar to some readers who have had a course in â€œautomata and language
theoryâ€. See Wilder (1963, p. 234)
â€  For a less abstract, but more detailed view of theories see p. 38.

6
I. Basic Logic
I.1.1 Remark . What is a â€œruleâ€? We run the danger of becoming circular or too
pedantic if we overdeï¬ne this notion. Intuitively, the rules we have in mind are
string manipulation rules, that is, â€œblack boxesâ€ (or functions) that receive string
inputs and respond with string outputs. For example, a well-known theorem-
building rule receives as input a formula and a variable, and returns (essentially)
the string composed of the symbol âˆ€, immediately followed by the variable and,
in turn, immediately followed by the formula.â€ 
â–¡
(1) First off, the ( ï¬rst order) formal language, L, where the theory is â€œspokenâ€,â€¡
is a triple (V , Term, Wff), that is, it has three important components, each
of them a set.
V is the alphabet or vocabulary of the language. It is the collection of the
basic syntactic â€œbricksâ€ (symbols) that we use to form expressions that
are terms (members of Term) or formulas (members of Wff). We will
ensure that the processes that build terms or formulas, using the basic
building blocks in V , are intuitively algorithmic or â€œmechanicalâ€.
Terms will formally codify â€œobjectsâ€, while formulas will formally
codify â€œstatementsâ€ about objects.
(2) Reasoning in the theory will be the process of discovering true statements
about objects â€“ that is, theorems. This discovery journey begins with certain
formulas which codify statements that we take for granted (i.e., we accept
without â€œproofâ€ as â€œbasic truthsâ€). Such formulas are the axioms. There are
two types of axioms:
Special or nonlogical axioms are to describe speciï¬c aspects of any
speciï¬c theory that we might be building. For example, â€œx + 1 Ì¸= 0â€
is a special axiom that contributes towards the characterization of
number theory over the natural numbers, N.
The other kind of axiom will be found in all theories. It is the kind that is
â€œuniversally validâ€, that is, not theory-speciï¬c (for example, â€œx = xâ€
is such a â€œuniversal truthâ€). For that reason this type of axiom will be
called logical.
(3) Finally, we will need rules for reasoning, actually called rules of inference.
These are rules that allow us to deduce, or derive, a true statement from
other statements that we have already established as being true.Â§ These
rules will be chosen to be oblivious to meaning, being only concerned with
â€  This rule is usually called â€œgeneralizationâ€.
â€¡ We will soon say what makes a language â€œï¬rst orderâ€.
Â§ The generous use of the term â€œtrueâ€ here is only meant for motivation. â€œProvableâ€ or â€œdeducibleâ€
(formula), or â€œtheoremâ€, will be the technically precise terminology that we will soon deï¬ne to
replace the term â€œtrue statementâ€.

I.1. First Order Languages
7
form. They will apply to statement â€œconï¬gurationsâ€ of certain recognizable
forms and will produce (derive) new statements of some corresponding
recognizable forms (See Remark I.1.1).
I.1.2 Remark. We may think of axioms of either logical or nonlogical type as
special cases of rules, that is, rules that receive no input in order to produce an
output. In this manner item (2) above is subsumed by item (3), and thus we are
faithful to our abstract deï¬nition of theory where axioms were not mentioned.
An example, outside mathematics, of an inputless rule is the rule invoked
when you type date on your computer keyboard. This rule receives no input,
and outputs on your screen the current date.
â–¡
We next look carefully into (ï¬rst order) formal languages.
There are two parts in each ï¬rst order alphabet. The ï¬rst, the collection of
the logical symbols, is common to all ï¬rst order languages regardless of which
theory is â€œspokenâ€ in them. We describe this part immediately below.
Logical Symbols
LS.1. Object or individual variables. An object variable is any one symbol
out of the non-ending sequence v0, v1, v2, . . . . In practice â€“ whether
we are using logic as a tool or as an object of study â€“ we agree to be
sloppy with notation and use, generically, x, y, z, u, v, w with or without
subscripts or primes as names of object variables.â€  This is just a matter
of notational convenience. We allow ourselves to write, say, z instead of,
say, v1200000000560000009. Object variables (intuitively) â€œvary overâ€ (i.e.,
are allowed to take values that are) the objects that the theory studies
(numbers, sets, atoms, lines, points, etc., as the case may be).
LS.2. The Boolean or propositional connectives. These are the symbols â€œÂ¬â€
and â€œâˆ¨â€.â€¡ They are pronounced not and or respectively.
LS.3. The existential quantiï¬er, that is, the symbol â€œâˆƒâ€, pronounced exists or
for some.
LS.4. Brackets, that is, â€œ(â€ and â€œ)â€.
LS.5. The equality predicate. This is the symbol â€œ=â€, which we use to indicate
that objects are â€œequalâ€. It is pronounced equals.
â€  Conventions such as this one are essentially agreements â€“ effected in the metatheory â€“ on how
to be sloppy and get away with it. They are offered in the interest of user-friendliness.
â€¡ The quotes are not part of the symbol. They serve to indicate clearly here, in particular in the
case of â€œâˆ¨â€, what is part of the symbol and what is not (the following period).

8
I. Basic Logic
The logical symbols will have a ï¬xed interpretation. In particular, â€œ=â€ will
always be expected to mean equals.
The theory-speciï¬c part of the alphabet is not ï¬xed, but varies from theory
to theory. For example, in set theory we just add the nonlogical (or special)
symbols, âˆˆand U. The ï¬rst is a special predicate symbol (or just predicate) of
arity 2, the second is a predicate symbol of arity 1.â€ 
In number theory we adopt instead the special symbols S (intended meaning:
successor, or â€œ + 1â€ function), +, Ã—, 0, <, and (sometimes) a symbol for the
exponentiation operation (function) ab. The ï¬rst three are function symbols of
arities 1, 2, and 2 respectively. 0 is a constant symbol, < a predicate of arity 2,
and whatever symbol we might introduce to denote ab would have arity 2.
The following list gives the general picture.
Nonlogical Symbols
NLS.1. A (possibly empty) set of symbols for constants. We normally use
the metasymbolsâ€¡ a, b, c, d, e, with or without subscripts or primes, to
stand for constants unless we have in mind some alternative â€œstandardâ€
formal notation in speciï¬c theories (e.g., âˆ…, 0, Ï‰).
NLS.2. A (possibly empty) set of symbols for predicate symbols or relation
symbols for each possible â€œarityâ€ n > 0. We normally use P, Q, R
generically, with or without primes or subscripts, to stand for predicate
symbols. Note that = is in the logical camp. Also note that theory-
speciï¬c formal symbols are possible for predicates, e.g., <, âˆˆ.
NLS.3. Finally, a (possibly empty) set of symbols for functions for each possi-
ble â€œarityâ€ n > 0. We normally use f, g, h, generically, with or without
primes or subscripts, to stand for function symbols. Note that theory-
speciï¬c formal symbols are possible for functions, e.g., +, Ã—.
I.1.3 Remark. (1) We have the option of assuming that each of the logical
symbols that we named in LS.1â€“LS.5 have no further â€œstructureâ€ and that the
symbols are, ontologically, identical to their names, that is, they are just these
exact signs drawn on paper (or on any equivalent display medium).
In this case, changing the symbols, say, Â¬ and âˆƒto âˆ¼and E respectively
results in a â€œdifferentâ€ logic, but one that is, trivially, â€œisomorphicâ€ to the one
â€  â€œArityâ€ is a term mathematicians have made up. It is derived from â€œaryâ€ of â€œunaryâ€, â€œbinaryâ€,
etc. It denotes the number of arguments needed by a symbol according to the dictates of correct
syntax. Function and predicate symbols need arguments.
â€¡ Metasymbols are informal (i.e., outside the formal language) symbols that we use within
â€œeverydayâ€ or â€œrealâ€ mathematics â€“ the metatheory â€“ in order to describe, as we are doing here,
the formal language.

I.1. First Order Languages
9
we are describing: Anything that we may do in, or say about, one logic trivially
translates to an equivalent activity in, or utterance about, the other as long as
we systematically carry out the translations of all occurrences of Â¬ and âˆƒto âˆ¼
and E respectively (or vice versa).
An alternative point of view is that the symbol names are not the same as
(identical with) the symbols they are naming. Thus, for example, â€œÂ¬â€ names
the connective we pronounce not, but we do not know (or care) exactly what
the nature of this connective is (we only care about how it behaves). Thus, the
name â€œÂ¬â€ becomes just a typographical expedient and may be replaced by other
names that name the same object, not.
This point of view gives one ï¬‚exibility in, for example, deciding how the
variable symbols are â€œimplementedâ€. It often is convenient to think that the
entire sequence of variable symbols was built from just two symbols, say, â€œvâ€
and â€œ|â€.â€  One way to do this is by saying that vi is a name for the symbol
sequenceâ€¡
â€œv | . . . |

i|â€™s
â€
Or, preferably â€“ see (2) below â€“ vi might be a name for the symbol sequence
â€œv | . . . |

i|â€™s
vâ€
Regardless of option, vi and vj will name distinct objects if i Ì¸= j.
This is not the case for the metavariables (â€œabbreviated informal namesâ€)
x, y, z, u, v, w. Unless we say so explicitly otherwise, x and y may name the
same formal variable, say, v131.
We will mostly abuse language and deliberately confuse names with the
symbols they name. For example, we will say, e.g., â€œlet v1007 be an object
variable . . . â€ rather than â€œlet v1007 name an object variable . . . â€, thus appearing
to favour option one.
(2) Any two symbols included in the alphabet are distinct. Moreover, if any of
them are built from simpler â€œsub-symbolsâ€ â€“ e.g., v0, v1, v2, . . . might really
name the strings vv, v|v, v||v, . . . â€“ then none of them is a substring (or subex-
pression) of any other.Â§
â€  We intend these two symbols to be identical to their names. No philosophical or other purpose
will be served by allowing â€œmore indirectionâ€ here (such as â€œv names u, which actually names
w, which actually is . . . â€).
â€¡ Not including the quotes.
Â§ What we have stated under (2) are requirements, not metatheorems! That is, they are nothing of
the sort that we can prove about our formal language within everyday mathematics.

10
I. Basic Logic
(3) A formal language, just like a â€œnaturalâ€ language (such as English or
Greek), is â€œaliveâ€ and evolving. The particular type of evolution we have in
mind is the one effected by formal deï¬nitions. Such deï¬nitions continually add
nonlogical symbols to the language.â€ 
Thus, when we say that, e.g., â€œâˆˆand U are the only nonlogical symbols of
set theoryâ€, we are telling a small white lie. More accurately, we ought to have
said that â€œâˆˆand U are the only â€˜primitiveâ€™ nonlogical symbols of set theoryâ€,
for we will add loads of other symbols such as âˆª, Ï‰, âˆ…, âŠ‚, âŠ†.
This evolution affects the (formal) language of any theory, not just set
theory.
â–¡
Wait a minute! If formal set theory is â€œthe foundation of all mathematicsâ€, and
if, ostensibly, this chapter on logic assists us to found set theory itself, then
how come we are employing natural numbers like 1200000000560000009 as
subscripts in the names of object variables? How is it permissible to already talk
about â€œsets of symbolsâ€ when we are about to found a theory of sets formally?
Surely we do not â€œhaveâ€â€¡ any of these â€œitemsâ€ yet, do we?
First off, the presence of subscripts such as 1200000000560000009 in
v1200000000560000009
is a non-issue. One way to interpret what has been said in the deï¬nition is
to view the various vi as abbreviated names of the real thing, the latter being
strings that employ the symbols v and | as in Remark I.1.3. In this connection
saying that vi is â€œimplementedâ€ as
v | . . . |

i|â€™s
v
(1)
especially the use of â€œiâ€ above, is only illustrative, thus totally superï¬‚uous. We
can say instead that strings of type (1) are the variables which we deï¬ne as
follows without the help of the â€œnatural number iâ€ (this is a variation of how
this is done in Bourbaki (1966b) and Hermes (1973)):
An â€œ|-calculationâ€ forms a string like this: Write a â€œ|â€.Â§ This is the â€œcurrent
stringâ€. Repeat a ï¬nite number of times: Add (i.e., concatenate) one | imme-
diately to the right of the current string. Write this new string (it is now the
current string).
â€  This phenomenon will be studied in some detail in what follows. By the way, any additions are
made to the nonlogical side of the alphabet. All the logical symbols have been given, once and
for all.
â€¡ â€œDo not haveâ€ in the sense of having not formally deï¬ned â€“ or proved to exist â€“ or both.
Â§ Without the quotes. These were placed to exclude the punctuation following.

I.1. First Order Languages
11
Let us call any string that ï¬gures in some |-calculation a â€œ|-stringâ€. A variable
either is the string vv, or is obtained as the concatenation from left to right of
v followed by an |-string, followed by v.
All we now need is the ability to generate as many as necessary distinct
variables (this is the â€œnon-ending sequenceâ€ part of the deï¬nition, p. 7): For
any two variables we get a new one that is different from either one by forming
the string â€œv, followed by the concatenation of the two |-parts, followed by vâ€.
Similarly if we had three, four, . . . variables. By the way, two strings of | are
distinct iffâ€  both occur in the same |-calculation, one, but not both, as the last
string.
Another, more direct way to interpret what was said about object variables
on p. 7 is to take the deï¬nition literally, i.e., to suppose that it speaks about the
ontology of the variables.â€¡ Namely, the subscript is just a a string of meaningless
symbols taken from the list below:
0, 1, 2, 3, 4, 5, 6, 7, 8, 9
Again we can pretend that we know nothing about natural numbers, and when-
ever, e.g., we want a variable other than either of v123 or v321, we may offer
either of v123321 or v321123 as such a new variable.
O.K., so we have not used natural numbers in the deï¬nition. But we did say
â€œsetsâ€ and also â€œnon-ending sequenceâ€, implying the presence of inï¬nite sets!
As we have already noted, on one hand we have â€œreal mathematicsâ€, and on
the other hand we have syntactic replicas of theories â€“ the formal theories â€“
that we built within real mathematics. Having built a formal theory, we can then
choose to use it (acting like formalists) to generate theorems, the latter being
codiï¬ed as symbol sequences (formulas). Thus, the assertion â€œaxiomatic set
theory is the foundation of all mathematicsâ€ is just a colloquialism proffered
in the metatheory that means that â€œwithin axiomatic set theory we can construct
the known sets of mathematics, such as the reals R and the complex numbers
C, and moreover we can simulate what we informally do whenever we are
working in real or complex analysis, algebra, topology, theory of measure and
integration, functional analysis, etc., etc.â€
There is no circularity here, but simply an empirical boastful observation in
the metatheory of what our simulator can do. Moreover, our metatheory does
â€  If and only if.
â€¡ Why not just say exactly what a deï¬nition is meant to say rather than leave it up to interpretation?
One certainly could, as in Bourbaki (1966b), make the ontology of variables crystal-clear right in
the deï¬nition. Instead, we have followed the custom of more recent writings and given the deï¬-
nition in a quasi-sloppy manner that leaves the ontology of variables as a matter for speculation.
This gives one the excuse to write footnotes like this one and remarks like I.1.3.

12
I. Basic Logic
have sets and all sorts of other mathematical objects. In principle we can use any
among those towards building or discussing the simulator, the formal theory.
Thus, the question is not whether we can use sets, or natural numbers, in
our deï¬nitions, but whether restrictions apply. For example, can we use inï¬nite
sets?
If we are Platonists, then we have available in the metatheory all sorts of sets,
including inï¬nite sets, in particular the set of all natural numbers. We can use
any of these items, speak about them, etc., as we please, when we are describing
or building the formal theory within our metatheory.
Now, if we are not Platonists, then our â€œrealâ€ mathematical world is much
more restricted. In one extreme, we have no inï¬nite sets.â€ 
We can still manage to deï¬ne our formal language! After all, the â€œnon-
endingâ€ sequence of object variables v0, v1, v2, . . . can be ï¬nitely generated in
at least two different ways, as we have already seen. Thus we can explain (to
a true formalist or ï¬nitist) that â€œnon-ending sequenceâ€ was an unfortunate slip
of the tongue, and that we really meant to give a procedure of how to generate
on demand a new object variable, different from whatever ones we may already
have.
Two parting comments are in order: One, we have been somewhat selective
in the use of the term â€œmetavariableâ€. We have called x, xâ€², y metavariables,
but have implied that the vi are formal variables, even if they are just names
of formal objects such that we do not know or do not care what they look like.
Well, strictly speaking the abbreviations vi are also metavariables, but they are
endowed with a property that the â€œgenericâ€ metavariables like x, y, zâ€² do not
have: Distinct vi names denote distinct object variables (cf. I.1.3).
Two, we should clarify that a formal theory, when used (i.e., the simulator
is being â€œrunâ€) is a generator of strings, not a decider or â€œparserâ€. Thus, it
can generate any of the following: variables (if these are given by procedures),
formulas and terms (to be deï¬ned), or theorems (to be deï¬ned). Decision issues,
no matter how trivial, the system is not built to handle. These belong to the
metatheory. In particular, the theory does not see whatever numbers or strings
(like 12005) may be hidden in a variable name (such as v12005).
Examples of decision questions: Is this string a term or a formula or a variable
(ï¬nitely generated as above)? All these questions are â€œeasyâ€. They are algo-
rithmically decidable in the metatheory. Or, is this formula a theorem? This is
â€  A ï¬nitist â€“ and donâ€™t forget that Hilbert-style metatheory was ï¬nitary, ostensibly for political
reasons â€“ will let you have as many integers as you like in one serving, as long as the serving
is ï¬nite. If you ask for more, you can have more, but never the set of all integers or an inï¬nite
subset thereof.

I.1. First Order Languages
13
algorithmically undecidable in the metatheory if it is a question about Peano
arithmetic or set theory.
I.1.4 Deï¬nition (Terminology about Strings). A symbol sequence or expres-
sion (or string) that is formed by using symbols exclusively out of a given setâ€ 
M is called a string over the set, or alphabet, M.
If A and B denote strings (say, over M), then the symbol A âˆ—B, or more
simply AB, denotes the symbol sequence obtained by listing ï¬rst the symbols
of A in the given left to right sequence, immediately followed by the symbols of
B in the given left to right sequence. We say that AB is (more properly, denotes
or names) the concatenation of the strings A and B in that order.
We denote the fact that the strings (named) C and D are identical sequences
(but we just say that they are equal) by writing C â‰¡D. The symbol Ì¸â‰¡denotes
the negation of the string equality symbol â‰¡. Thus, if # and ? are (we do mean
â€œareâ€) symbols from an alphabet, then
#?? â‰¡#??
but
#? Ì¸â‰¡#??
We can also employ â‰¡in contexts such as â€œlet A â‰¡##?â€, where we give the
name A to the string ##?.â€¡
In this book the symbol â‰¡will be exclusively used in the metatheory for equality
of strings over some set M.
The symbol Î» normally denotes the empty string, and we postulate for it the
following behaviour:
A â‰¡AÎ» â‰¡Î»A
for all strings A
We say that A occurs in B, or is a substring of B, iff there are strings C and D
such that B â‰¡CAD.
For example, â€œ(â€ occurs four times in the (explicit) string â€œÂ¬(()âˆ¨)((â€, at
positions 2, 3, 7, 8. Each time this happens we have an occurrence of â€œ(â€ in
â€œÂ¬(()âˆ¨)((â€.
If C â‰¡Î», we say that A is a preï¬x of B. If moreover D Ì¸â‰¡Î», then we say
that A is a proper preï¬x of B.
â–¡
â€  A set that supplies symbols to be used in building strings is not special. It is just a set. However,
it often has a special name: â€œalphabetâ€.
â€¡ Punctuation such as â€œ.â€ is not part of the string. One often avoids such footnotes by enclosing
strings that are explicitly written as symbol sequences inside quotes. For example, if A stands
for the string #, one writes A â‰¡â€œ#â€. Note that we must not write â€œAâ€, unless we mean a string
whose only symbol is A.

14
I. Basic Logic
I.1.5 Deï¬nition (Terms). The set of terms, Term, is the smallest set of strings
over the alphabet V with the following two properties:
(1) All of the items in LS.1 or NLS.1 (x, y, z, a, b, c, etc.) are included.
(2) If f is a functionâ€  of arity n and t1, t2,. . . , tn are included, then so is the
string â€œ f t1t2 . . . tnâ€.
The symbols t, s, and u, with or without subscripts or primes, will denote
arbitrary terms. Since we are using them in the metalanguage to â€œvary overâ€
terms, we naturally call them metavariables. They also serve â€“ as variables â€“
towards the deï¬nition (this one) of the syntax of terms. For this reason they are
also called syntactic variables.
â–¡
I.1.6 Remark. (1) We often abuse notation and write f (t1,. . . , tn) instead of
f t1 . . . tn.
(2) Deï¬nition I.1.5 is an inductive deï¬nition.â€¡ It deï¬nes a more or less
â€œcomplicatedâ€ term by assuming that we already know what â€œsimplerâ€ terms
look like. This is a standard technique employed in real mathematics. We will
have the opportunity to say more about such inductive deï¬nitions â€“ and their
appropriateness â€“ in a
-comment later on.
(3) We relate this particular manner of deï¬ning terms to our working def-
inition of a theory (given on p. 6 immediately before Remark I.1.1 in terms
of â€œrulesâ€ of formation). Item (2) in I.1.5 essentially says that we build new
terms (from old ones) by applying the following general rule: Pick an arbitrary
function symbol, say f . This has a speciï¬c formation rule associated with it
that, for the appropriate number, n, of an already existing ordered list of terms,
t1,. . . , tn, will build the new term consisting of f , immediately followed by
the ordered list of the given terms.
To be speciï¬c, suppose we are working in the language of number theory.
There is a function symbol + available there. The rule associated with + builds
the new term +ts for any prior obtained terms t and s. For example, +v1v13
and +v121 + v1v13 are well-formed terms. We normally write terms of number
theory in â€œinï¬xâ€ notation,Â§ i.e., t + s, v1 + v13 and v121 + (v1 + v13) (note the
intrusion of brackets, to indicate sequencing in the application of +).
â€  We will omit from now on the qualiï¬cation â€œsymbolâ€ from terminology such as â€œfunction sym-
bolâ€, â€œconstant symbolâ€, â€œpredicate symbolâ€.
â€¡ Some mathematicians will absolutely insist that we call this a recursive deï¬nition and reserve
the term â€œinductionâ€ for â€œinduction proofsâ€. This is seen to be unwarranted hair splitting if we
consider that Bourbaki (1966b) calls induction proofs â€œdÂ´emonstrations par rÂ´ecurrenceâ€. We will
be less dogmatic: Either name is all right.
Â§ Function symbol placed between the arguments.

I.1. First Order Languages
15
A by-product of what we have just described is that the arity of a function
symbol f is whatever number of terms the associated rule will require as input.
(4) A crucial word used in I.1.5 (which recurs in all inductive deï¬nitions) is
â€œsmallestâ€. It means â€œleast inclusiveâ€ (set). For example, we may easily think of
a set of strings that satisï¬es both conditions of the above deï¬nition, but which is
not â€œsmallestâ€ by virtue of having additional elements, such as the string â€œÂ¬Â¬(â€.
Pause. Why is â€œÂ¬Â¬(â€ not in the smallest set as deï¬ned above, and therefore
not a term?
The reader may wish to ponder further on the import of the qualiï¬cation
â€œsmallestâ€ by considering the familiar (similar) example of N, the set of natural
numbers. The principle of induction in N ensures that this set is the smallest
with the properties:
(i) 0 is included, and
(ii) if n is included, then so is n + 1.
Bycontrast,allofZ(setofintegers),Q(setofrationalnumbers),R(setofreal
numbers) satisfy (i) and (ii), but they are clearly not the â€œsmallestâ€ such.
â–¡
I.1.7 Deï¬nition (Atomic Formulas). The set of atomic formulas, Af, contains
precisely:
(1) The strings t = s for every possible choice of terms t, s.
(2) The strings Pt1t2 . . . tn for every possible choice of n-ary predicates P (for
all choices of n > 0) and all possible choices of terms t1, t2,. . . , tn.
â–¡
We often abuse notation and write P(t1,. . . , tn) instead of Pt1 . . . tn.
I.1.8 Deï¬nition (Well-Formed Formulas). The set of well-formed formulas,
Wff, is the smallest set of strings or expressions over the alphabet V with the
following properties:
(a) All the members of Af are included.
(b) If A and B denote strings (over V ) that are included, then (A âˆ¨B ) and
(Â¬A) are also included.
(c) If A isâ€  a string that is included and x is any object variable (which may or
may not occur (as a substring) in the string A), then the string ((âˆƒx)A) is
also included. We say that A is the scope of (âˆƒx).
â–¡
â€  Denotes!

16
I. Basic Logic
I.1.9 Remark.
(1) The above is yet another inductive deï¬nition. Its statement (in the metalan-
guage) is facilitated by the use of so-called syntactic, or meta, variables â€“
A and B â€“ used as names for arbitrary (indeterminate) formulas. In gen-
eral, we will let calligraphic capital letters A, B , C , D , E , F , G (with or
without primes or subscripts) be names for well-formed formulas, or just
formulas, as we often say. The deï¬nition of Wff given above is standard.
In particular, it permits well-formed formulas such as ((âˆƒx)((âˆƒx)x = 0)) in
the interest of making the formation rules â€œcontext-freeâ€.â€ 
(2) The rules of syntax just given do not allow us to write things such as âˆƒf or
âˆƒP where f and P are function and predicate symbols respectively. That
quantiï¬cation is deliberately restricted to act solely on object variables
makes the language ï¬rst order.
(3) We have already indicated in Remark I.1.6 where the arities (of function and
predicate symbols) come from (Deï¬nitions I.1.5 and I.1.7 referred to them).
These are numbers that are implicit (â€œhardwiredâ€) with the formation rules
for terms and atomic formulas. Each function and each predicate symbol
(e.g., +, Ã—, âˆˆ, <) has its own unique formation rule. This rule â€œknowsâ€ how
many terms are needed (on the input side) in order to form a term or atomic
formula. Therefore, since the theory, in use, applies rather than studies its
formation rules, it is, in particular, ignorant of arities of symbols.
Now that this jurisdictional point has been made (cf. the concluding
remarks about decision questions, on p. 12), we can consider an alternative
way of making arities of symbols known (in the metatheory): Rather than
embedding arities in the formation rules, we can hide them in the ontology
of the symbols, not making them explicit in the name.
For example, a new symbol, say âˆ—, can be used to record arity. That
is, we can think of a predicate (or function) symbol as consisting of two
parts: an arity part and an â€œall the restâ€ part, the latter needed to render the
symbol unique.â€¡ For example, âˆˆmay be actually the name for the symbol
â€œâˆˆâˆ—âˆ—â€, where this latter name is identical to the symbol it denotes, or â€œwhat
you see is what you getâ€ â€“ see Remark I.1.3(1) and (2), p. 8. The presence
of the two asterisks declares the arity. Some people say this differently:
They make available to the metatheory a â€œfunctionâ€, ar, from â€œthe set of
â€  In some presentations, the formation rule in I.1.8(c) is â€œcontext-sensitiveâ€: It requires that x be
not already quantiï¬ed in A.
â€¡ The reader may want to glimpse ahead, on p. 166, to see a possible implementation in the case
of number theory.

I.1. First Order Languages
17
all predicate symbols and functionsâ€ (of a given language) to the natural
numbers, so that for any function symbol f or predicate symbol P, ar( f )
and ar(P) yield the arities of f and P respectively.â€ 
(4) Abbreviations
Abr1. The string ((âˆ€x)A) abbreviates the string â€œ(Â¬((âˆƒx)(Â¬A)))â€. Thus,
for any explicitly written formula A, the former notation is infor-
mal(metamathematical),whilethelatterisformal(withintheformal
language).Inparticular,âˆ€isametalinguisticsymbol.â€œâˆ€xâ€istheuni-
versal quantiï¬er. A is its scope. The symbol âˆ€is pronounced for all.
We also introduce â€“ in the metalanguage â€“ a number of additional Boolean
connectives in order to abbreviate certain strings:
Abr2. (Conjunction, âˆ§) (A âˆ§B ) stands for (Â¬((Â¬A) âˆ¨(Â¬B ))). The
symbol âˆ§is pronounced and.
Abr3. (Classical or material implication, â†’) (A â†’B ) stands for
((Â¬A) âˆ¨B ). (A â†’B ) is pronounced if A, then B .
Abr4. (Equivalence, â†”) (A â†”B ) stands for ((A â†’B ) âˆ§(B â†’A)).
Abr5. To minimize the use of brackets in the metanotation we adopt stan-
dard priorities of connectives: âˆ€, âˆƒ, and Â¬ have the highest, and then
we have (in decreasing order of priority) âˆ§, âˆ¨, â†’, â†”, and we agree
not to use outermost brackets. All associativities are right â€“ that is,
if we write A â†’B â†’C , then this is a (sloppy) counterpart for
(A â†’(B â†’C )).
(5) The language just deï¬ned, L, is one-sorted, that is, it has a single sort or
type of object variable. Is this not inconvenient? After all, our set theory
(volume 2 of these lectures) will have both atoms and sets. In other theories,
e.g., geometry, one has points, lines, and planes. One would have hoped to
have different â€œtypesâ€ of variables, one for each.
Actually, to do this would amount to a totally unnecessary complication
of syntax. We can (and will) get away with just one sort of object variable.
For example, in set theory we will also introduce a 1-aryâ€¡ predicate, U,
whose job is to â€œtestâ€ an object for â€œsethoodâ€.Â§ Similar remedies are avail-
able to other theories. For example, geometry will manage with one sort of
variable and unary predicates â€œPointâ€, â€œLineâ€, and â€œPlaneâ€.
â€  In mathematics we understand a function as a set of inputâ€“output pairs. One can â€œglueâ€ the two
parts of such pairs together, as in â€œâˆˆâˆ—âˆ—â€ â€“ where â€œâˆˆâ€ is the input part and â€œâˆ—âˆ—â€ is the output part,
the latter denoting â€œ2â€ â€“ etc. Thus, the two approaches are equivalent.
â€¡ More commonly called unary.
Â§ People writing about, or teaching, set theory have made this word up. Of course, one means by
it the property of being a set.

18
I. Basic Logic
Apropos language, some authors emphasize the importance of the
nonlogical symbols, taking at the same time the formation rules for
granted; thus they say that we have a language, say, â€œL = {âˆˆ, U}â€ rather
than â€œL = (V , Term, Wff) where V
has âˆˆand U as its only nonlogi-
cal symbolsâ€. That is, they use â€œlanguageâ€ for the nonlogical part of the
alphabet.
â–¡
A variable that is quantiï¬ed is bound in the scope of the quantiï¬er. Non-
quantiï¬ed variables are free. We also give below, by induction on formulas,
precise (metamathematical) deï¬nitions of â€œfreeâ€ and â€œboundâ€.
I.1.10 Deï¬nition (Free and Bound Variables). An object variable x occurs
free in a term t or atomic formula A iff it occurs in t or A as a substring
(see I.1.4).
x occurs free in (Â¬A) iff it occurs free in A.
x occurs free in (A âˆ¨B ) iff it occurs free in at least one of A or B .
x occurs free in ((âˆƒy)A) iff x occurs free in A, and y is not the same
variable as x.â€ 
The y in ((âˆƒy)A) is, of course, not free â€“ even if it might be so in A â€“ as
we have just concluded in this inductive deï¬nition. We say that it is bound in
((âˆƒy)A). Trivially, terms and atomic formulas have no bound variables.
â–¡
I.1.11 Remark. (1) Of course, Deï¬nition I.1.10 takes care of the deï¬ned con-
nectives as well, via the obvious translation procedure.
(2) Notation. If A is a formula, then we often write A[y1,. . . , yk] to indicate
our interest in the variables y1,. . . , yk, which may or may not be free in A.
Indeed, there may be other free variables in A that we may have chosen not to
include in the list.
On the other hand, if we use round brackets, as in A(y1,. . . , yk), then we
are implicitly asserting that y1,. . . , yk is the complete list of free variables that
occur in A.
â–¡
I.1.12 Deï¬nition. A term or formula is closed iff no free variables occur in it.
A closed formula is called a sentence.
A formula is open iff it contains no quantiï¬ers (thus, an open formula may
also be closed).
â–¡
â€  Recall that x and y are abbreviations of names such as v1200098 and v11009 (which name distinct
variables). However, it could be that both x and y name v101. Therefore it is not redundant to say
â€œand y is not the same variable as xâ€. By the way, x Ì¸â‰¡y says the same thing, by I.1.4.

I.2. A Digression into the Metatheory
19
I.2. A Digression into the Metatheory:
Informal Induction and Recursion
We have already seen a number of inductive or recursive deï¬nitions in Sec-
tion I.1. The reader, most probably, has already seen or used such deï¬nitions
elsewhere.
We will organize the common important features of inductive deï¬nitions
in this section, for easy reference. We just want to ensure that our grasp of
these notions and techniques, at the metamathematical level, is sufï¬cient for
the needs of this volume.
One builds a set S by recursion, or inductively (or by induction), out of two
ingredients: a set of initial objects, I , and a set of rules or operations, R. A
member of R â€“ a rule â€“ is a (possibly inï¬nite) table, or relation, like
y1
. . .
yn
z
a1
. . .
an
an+1
b1
. . .
bn
bn+1
...
...
...
If the above rule (table) is called Q, then we use the notationsâ€ 
Q(a1,. . . , an, an+1)
and
âŸ¨a1,. . . , an, an+1âŸ©âˆˆQ
interchangeably to indicate that the ordered sequence or â€œrowâ€ a1,. . . , an, an+1
is present in the table.
We say that â€œQ(a1,. . . , an, an+1) holdsâ€ or â€œQ(a1,. . . , an, an+1) is trueâ€,
but we often also say that â€œQ applied to a1,. . . , an yields an+1â€, or that â€œan+1
is a result or output of Q, when the latter receives input a1,. . . , anâ€. We often
abbreviate such inputs using vector notation, namely, âƒ—an (or just âƒ—a, if n is
understood). Thus, we may write Q(âƒ—an+1) for Q(a1,. . . , an, an+1).
A rule Q that has n + 1 columns is called (n + 1)-ary.
I.2.1 Deï¬nition. We say â€œa set T is closed under an (n + 1)-ary rule Qâ€ to
mean that whenever c1,. . . , cn are all in T , then d âˆˆT for all d satisfying
Q(c1,. . . , cn, d).
â–¡
With these preliminary understandings out of the way, we now state
â€  â€œx âˆˆAâ€ means that â€œx is a member of â€“ or is in â€“ Aâ€ in the informal set-theoretic sense.

20
I. Basic Logic
I.2.2 Deï¬nition. S is deï¬ned by recursion, or by induction, from initial objects
I and set of rules R, provided it is the smallest (least inclusive) set with the
properties
(1) I âŠ†S,â€ 
(2) S is closed under every Q in R. In this case we say that S is R-closed.
We write S = Cl(I , R), and say that â€œS is the closure of I under R â€. â–¡
We have at once:
I.2.3 Metatheorem (Induction on S). If S = Cl(I , R) and if some set T
satisï¬es
(1) I âŠ†T , and
(2) T is closed under every Q in R,
then S âŠ†T .
Pause. Why is the above a metatheorem?
The above principle of induction on S is often rephrased as follows: To prove
that a property P(x) holds for all members of Cl(I , R), just prove that
(a) every member of I has the property, and
(b) the property propagates with every rule in R, i.e., if P(ci) holds (is true)
for i = 1,. . . , n, and if Q(c1,. . . , cn, d) holds, then d too has the property
P(x) â€“ that is, P(d) holds.
Of course, this rephrased principle is valid, for if we let T be the set of all
objects that have property P(x) â€“ for which set one employs the well-established
symbol {x : P(x)} â€“ then this T satisï¬es (1) and (2) of the metatheorem.â€¡
I.2.4 Deï¬nition (Derivationsand Parses). A(I , R)-derivation, or simply de-
rivation â€“ if I and R are understood â€“ is a ï¬nite sequence of objects d1,. . . , dn
â€  From our knowledge of elementary informal set theory, we recall that A âŠ†B means that every
member of A is also a member of B.
â€¡ We are sailing too close to the wind here! It turns out that not all properties P(x) lead to sets
{x : P(x)}. Our explanation was naÂ¨Ä±ve. However, formal set theory, which is meant to save us from
our naÂ¨Ä±vetÂ´e, upholds the â€œprincipleâ€ (a)â€“(b) using just a slightly more complicated explanation.
The reader can see this explanation in our volume 2 in the chapter on cardinality.

I.2. A Digression into the Metatheory
21
(n â‰¥1) such that each di is
(1) a member of I , orâ€ 
(2) for some (r + 1)-ary Q âˆˆR, Q(d j1,. . . , d jr , di) holds, and jl < i for
l = 1,. . . ,r.
We say that di is derivable within i steps.
A derivation of an object A is also called a parse of a.
â–¡
Trivially, if d1,. . . , dn is a derivation, then so is d1,. . . , dm for any 1 â‰¤m < n.
If d is derivable within n steps, it is also derivable in k steps or less, for all
k > n, since we can lengthen a derivation arbitrarily by adding I -elements
to it.
I.2.5 Remark. The following metatheorem shows that there is a way to â€œcon-
structâ€ Cl(I , R) iteratively, i.e., one element at a time by repeated application
of the rules.
This result shows deï¬nitively that our inductive deï¬nitions of terms (I.1.5)
and well-formed formulas (I.1.8) fully conform with our working deï¬nition of
theory, as an alphabet and a set of rules that are used to build formulas and
theorems (p. 5).
â–¡
I.2.6 Metatheorem.
Cl(I , R) = {x : x is (I , R)-derivable within some number of steps, n}
Proof. For notational convenience let us write
T = {x : x is (I , R)-derivable within some number of steps, n}.
As we know from elementary naÂ¨Ä±ve set theory, we need to show here both
Cl(I , R) âŠ†T and Cl(I , R) âŠ‡T to settle the claim.
(âŠ†) We do induction on Cl(I , R) (using I.2.3). Now I âŠ†T , since every
member of I is derivable in n = 1 step. (Why?)
Also, T is closed under every Q in R. Indeed, let such an (r + 1)-ary Q be
chosen, and assume
Q(a1,. . . , ar, b)
(i)
â€  This â€œorâ€ is inclusive: (1), or (2), or both.

22
I. Basic Logic
and {a1,. . . , ar} âŠ†T . Thus, each ai has a (I , R)-derivation. Concatenate all
these derivations:
. . . , a1,. . . , a2,. . . , . . . , ar
The above is a derivation (why?). But then, so is
. . . , a1,. . . , a2,. . . , . . . , ar, b
by (i). Thus, b âˆˆT .
(âŠ‡) We argue this â€“ that is, â€œif d âˆˆT , then d âˆˆCl(I , R)â€ â€“ by induction
on the number of steps, n, in which d is derivable.
For n = 1 we have d âˆˆI and we are done, since I âŠ†Cl(I , R).
Let us make the induction hypothesis (I.H.) that for derivations of â‰¤n steps
the claim is true. Let then d be derivable within n + 1 steps. Thus, there is a
derivation a1,. . . , an, d.
Now, if d âˆˆI , we are done as above (is this a â€œreal caseâ€?) If on the other
hand Q(a j1,. . . , a jr, d), then for i = 1,. . . ,r we have a ji âˆˆCl(I , R) by the
I.H.; hence d âˆˆCl(I , R), since the closure is closed under all Q âˆˆR.
â–¡
I.2.7 Example. One can see now that N = Cl(I , R), where I = {0} and R
contains just the relation y = x + 1 (input x, output y). Similarly, Z, the set
of all integers, is Cl(I , R), where I = {0} and R contains just the relations
y = x + 1 and y = x âˆ’1 (input x, output y).
For the latter, the inclusion Cl(I , R) âŠ†Z is trivial (by I.2.3). For âŠ‡we
easily see that any n âˆˆZ has a (I , R)-derivation (and then we are done by I.2.6).
For example, if n > 0, then 0, 1, 2,. . . , n is a derivation, while if n < 0, then
0, âˆ’1, âˆ’2,. . . , n is one. If n = 0, then the one-term sequence 0 is a derivation.
Another interesting closure is obtained by I = {3} and the two relations
z = x + y and z = x âˆ’y. This is the set {3k : k âˆˆZ} (see Exercise I.1).
â–¡
Pause. So, taking the ï¬rst sentence of I.2.7 one step further, we note that we
have just proved the induction principle for N, for that is exactly what the
â€œequationâ€ N = Cl(I , R) says (by I.2.3). Do you agree?
There is another way to view the iterative construction of Cl(I , R): The
set is constructed in stages. Below we are using some more notation borrowed
from informal set theory. For any sets A and B we write A âˆªB to indicate the
set union, which consists of all the members found in A or B or in both. More
generally, if we have a lot of sets, X0, X1, X2,. . . , that is, one Xi for every
integer i â‰¥0 â€“ which we denote by the compact notation (Xi)i â‰¥0 â€“ then we
may wish to form a set that includes all the objects found as members all over
the Xi, that is (using inclusive, or logical, â€œorâ€s below), form
{x : x âˆˆX0 or x âˆˆX1 or . . . }

I.2. A Digression into the Metatheory
23
or, more elegantly and precisely,
{x : for some i â‰¥0, x âˆˆXi}
The latter is called the union of the sequence (Xi)iâ‰¥0 and is often denoted by

iâ‰¥0 Xi
or

iâ‰¥0
Xi
Correspondingly, we write

iâ‰¤n Xi
or

iâ‰¤n
Xi
if we only want to take a ï¬nite union, also indicated clumsily as X0 âˆª. . . âˆªXn.
I.2.8 Deï¬nition (Stages). In connection with Cl(I , R) we deï¬ne the sequence
of sets (Xi)i â‰¥0 by induction on n, as follows:
X0 = I
Xn+1 =

iâ‰¤n
Xi

âˆª

b : for some Q âˆˆR and some âƒ—an in

i â‰¤n
Xi, Q(âƒ—an, b)
	
That is, to form Xn+1 we append to 
i â‰¤n Xi all the outputs of all the relations
in R acting on all possible inputs, the latter taken from 
iâ‰¤n Xi.
We say that Xi is built at stage i, from initial objects I and rule-set R.
â–¡
In words, at stage 0 we are given the initial objects (X0 = I ). At stage 1 we
apply all possible relations to all possible objects that we have so far â€“ they
form the set X0 â€“ and build the 1st stage set, X1, by appending the outputs to
what we have so far. At stage 2 we apply all possible relations to all possible
objects that we have so far â€“ they form the set X0 âˆªX1 â€“ and build the 2nd
stage set, X2, by appending the outputs to what we have so far. And so on.
When we work in the metatheory, we take for granted that we can have
simple inductive deï¬nitions on the natural numbers. The reader is familiar with
several such deï¬nitions, e.g.,
a0 = 1
(for a Ì¸= 0 throughout)
an+1 = a Â· an

24
I. Basic Logic
We will (meta)prove a general theorem on the feasibility of recursive deï¬nitions
later on (I.2.13).
The following theorem connects stages and closures.
I.2.9 Metatheorem. With the Xi as in I.2.8,
Cl(I , R) =

iâ‰¥0
Xi
Proof. (âŠ†) We do induction on Cl(I , R). For the basis, I = X0 âŠ†
iâ‰¥0 Xi.
We show that 
iâ‰¥0 Xi is R-closed. Let Q âˆˆR and Q(âƒ—an, b) hold, for some
âƒ—an in 
iâ‰¥0 Xi. Thus, by deï¬nition of union, there are integers j1, j2,. . . , jn
such that ai âˆˆX ji, i = 1,. . . , n. If k = max{ j1,. . . , jn}, then âƒ—an is in 
iâ‰¤k Xi;
hence b âˆˆXk + 1 âŠ†
iâ‰¥0 Xi.
(âŠ‡) It sufï¬ces to prove that Xn âŠ†Cl(I , R), a fact we can prove by induction
on n. For n = 0 it holds by I.2.2. As an I.H. we assume the claim for all n â‰¤k.
The case for k + 1: Xk + 1 is the union of two sets. One is 
iâ‰¤k Xi. This is a
subset of Cl(I , R) by the I.H. The other is

b : for some Q âˆˆR and some âƒ—a in 
iâ‰¤k
Xi, Q(âƒ—a, b)

This too is a subset of Cl(I , R), by the preceding observation and the fact that
Cl(I , R) is R-closed.
â–¡
Worth Saying. An inductively deï¬ned set can be built by stages.
I.2.10 Deï¬nition (Immediate Predecessors, Ambiguity). If d âˆˆCl(I , R)
and for some Q and a1,. . . , ar it is the case that Q(a1,. . . , ar, d), then the
a1,. . . , ar are immediate Q-predecessors of d, or just immediate predecessors
if Q is understood; for short, i.p.
A pair (I , R) is called ambiguous if some d âˆˆCl(I , R) satisï¬es any (or
all) of the following conditions:
(i) It has two (or more) distinct sets of immediate P-predecessors for some
rule P.
(ii) It has both immediate P-predecessors and immediate Q-predecessors, for
P Ì¸= Q.
(iii) It is a member of I , yet it has immediate predecessors.
If (I , R) is not ambiguous, then it is unambiguous.
â–¡

I.2. A Digression into the Metatheory
25
I.2.11 Example. The pair ({00,0}, {Q}), where Q(x, y, z) holds iff z = xy
(where â€œxyâ€ denotes the concatenation of the strings x and y, in that order), is
ambiguous. For example, 0000 has the two immediate predecessor sets {00,00}
and {0,000}. Moreover, while 00 is an initial object, it does have immedi-
ate predecessors, namely, the set {0,0} (or, what amounts to the same thing,
{0}).
â–¡
I.2.12 Example. The pair (I , R), where I = {3} and R consists of z = x + y
and z = x âˆ’y, is ambiguous. Even 3 has (inï¬nitely many) distinct sets of i.p.
(e.g., any {a, b} such that a + b = 3, or a âˆ’b = 3).
The pairs that effect the deï¬nition of Term (I.1.5) and Wff (I.1.8) are un-
ambiguous (see Exercises I.2 and I.3).
â–¡
I.2.13 Metatheorem (Deï¬nition by Recursion). Let (I , R) be unambiguous
and Cl(I , R) âŠ†A, where A is some set. Let also Y be a set, andâ€  h : I â†’Y
and gQ, for each Q âˆˆR, be given functions. For any (r +1)-ary Q, an input for
the function gQ is a sequence âŸ¨a, b1,. . . , brâŸ©where a is in A and the b1,. . . , br
are all in Y. All the gQ yield outputs in Y.
Under these assumptions, there is a unique function f : Cl(I , R) â†’Y
such that
y = f (x) iff
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³
y = h(x) and x âˆˆI
or, for some Q âˆˆR,
y = gQ(x, o1,. . . , or) and Q(a1,. . . , ar, x) holds,
where oi = f (ai), for i = 1,. . . ,r
(1)
The reader may wish to skip the proof on ï¬rst reading.
Proof. Existence part. For each (r + 1)-ary Q âˆˆR, deï¬ne 
Q byâ€¡

Q(âŸ¨a1, o1âŸ©,. . . , âŸ¨ar, orâŸ©, âŸ¨b, gQ(b, o1,. . . , or)âŸ©)
iff
Q(a1,. . . , ar, b)
(2)
For any a1,. . . , ar, b, the above deï¬nition of 
Q is effected for all possible
choices of o1,. . . , or such that gQ(b, o1,. . . , or) is deï¬ned.
Collect now all the 
Q to form a set of rules 
R.
Let also 
I = {âŸ¨x, h(x)âŸ©: x âˆˆI }.
â€  The notation f : A â†’B is common in informal (and formal) mathematics. It denotes a function
f that receives â€œinputsâ€ from the set A and yields â€œoutputsâ€ in the set B.
â€¡ Forarelation Q,writingjustâ€œQ(a1,. . . , ar, b)â€isequivalenttowritingâ€œQ(a1,. . . , ar, b)holdsâ€.

26
I. Basic Logic
We will verify that the set F = Cl( 
I , 
R ) is a 2-ary relation that for every
input yields at most one output, and therefore is a function. For such a relation
it is customary to write, letting the context fend off the obvious ambiguity in
the use of the letter F,
y = F(x)
iff
F(x, y)
(âˆ—)
We will further verify that replacing f in (1) above by F results in a valid
equivalence (the â€œiffâ€ holds). That is, F satisï¬es (1).
(a) We establish that F is a relation composed of pairs âŸ¨x, yâŸ©(x is input, y is
output), where x âˆˆCl(I , R) and y âˆˆY. This follows easily by induction
on F (I.2.3), since 
I âŠ†F, and the property (of â€œcontaining such pairsâ€)
propagates with each 
Q (recall that the gQ yield outputs in Y).
(b) We next show that â€œif âŸ¨x, yâŸ©âˆˆF and âŸ¨x, zâŸ©âˆˆF, then y = zâ€, that is, F is
â€œsingle-valuedâ€ or â€œwell-deï¬nedâ€, in short, it is a function.
We again employ induction on F, thinking of the quoted statement as a
â€œpropertyâ€ of the pair âŸ¨x, yâŸ©:
Suppose that âŸ¨x, yâŸ©âˆˆ
I , and let also âŸ¨x, zâŸ©âˆˆF.
By
I.2.6,
âŸ¨x, zâŸ©âˆˆ
I ,
or

Q(âŸ¨a1, o1âŸ©,. . . , âŸ¨ar, orâŸ©, âŸ¨x, zâŸ©),
where
Q(a1,. . . , ar, x) and z = gQ(x, o1,. . . , or), for some (r + 1)-ary 
Q and
âŸ¨a1, o1âŸ©,. . . , âŸ¨ar, orâŸ©in F.
The right hand side of the italicized â€œorâ€ cannot hold for an unambiguous
(I , R), since x cannot have i.p. Thus âŸ¨x, zâŸ©âˆˆ
I ; hence y = h(x) = z.
To prove that the property propagates with each 
Q, let

Q(âŸ¨a1, o1âŸ©,. . . , âŸ¨ar, orâŸ©, âŸ¨x, yâŸ©)
but also
P

b1, oâ€²
1

,. . . ,

bl, oâ€²
l

,

x, z

where Q(a1,. . . , ar, x), P(b1,. . . , bl, x), and
y = gQ(x, o1,. . . , or)
and
z = gP

x, oâ€²
1,. . . , oâ€²
l

(3)
Since (I , R) is unambiguous, we have Q = P (hence also 
Q = P), r = l,
and ai = bi for i = 1,. . . ,r.
By I.H., oi = oâ€²
i for i = 1,. . . ,r; hence y = z by (3).
(c) Finally, we show that F satisï¬es (1). We do induction on Cl( 
I , 
R) to prove:
(â†)
If x âˆˆI and y = h(x), then F(x, y) (i.e., y = F(x) in the
alternative notation (âˆ—)), since 
I âŠ†F. Let next y = gQ(x, o1,. . . , or)
and Q(a1,. . . , ar, x), where also F(ai, oi), for i = 1,. . . ,r. By (2),

Q(âŸ¨a1, o1âŸ©,. . . , âŸ¨ar, orâŸ©, âŸ¨x, gQ(x, o1,. . . , or)âŸ©); thus â€“ F being closed

I.2. A Digression into the Metatheory
27
under all the rules in 
R â€“ F(x, gQ(b, o1,. . . , or)) holds; in short, F(x, y)
or y = F(x).
(â†’)
Now we assume that F(x, y) holds and we want to infer the right
hand side (of iff ) in (1). We employ Metatheorem I.2.6.
Case 1. Let âŸ¨x, yâŸ©be F-derivableâ€  in n = 1 step. Then âŸ¨x, yâŸ©âˆˆ
I . Thus
y = h(x).
Case 2. Suppose next that âŸ¨x, yâŸ©is F-derivable within n + 1 steps,
namely, we have a derivation
âŸ¨x1, y1âŸ©, âŸ¨x2, y2âŸ©, . . . , âŸ¨xn, ynâŸ©, âŸ¨x, yâŸ©
(4)
where 
Q(âŸ¨a1, o1âŸ©,. . . , âŸ¨ar, orâŸ©, âŸ¨x, yâŸ©) and Q(a1,. . . , ar, x) (see (2)),
and each of âŸ¨a1, o1âŸ©,. . . , âŸ¨ar, orâŸ©appears in the above derivation, to
the left of âŸ¨x, yâŸ©. This entails (by (2)) that y = gQ(x, o1,. . . , or). Since
the âŸ¨ai, oiâŸ©appear in (4), F(ai, oi) holds, for i = 1,. . . ,r. Thus, âŸ¨x, yâŸ©
satisï¬es the right hand side of iff in (1), once more.
Uniqueness part. Let the function K also satisfy (1). We show, by induction
on Cl(I , R), that
For all x âˆˆCl(I , R) and all y âˆˆY,
y = F(x) iff y = K(x)
(5)
(â†’)
Let x âˆˆI , and y = F(x). By lack of ambiguity, the case conditions
of(1)aremutuallyexclusive.Thus,itmustbethat y = h(x).Butthen, y = K(x)
as well, since K satisï¬es (1) too.
Let now Q(a1,. . . , ar, x) and y = F(x). By (1), there are (unique, as we now
know) o1,. . . , or such that oi = F(ai) for i = 1,. . . ,r, and y = gQ(x, o1,. . . ,
or). By the I.H., oi = K(ai). But then (1) yields y = K(x) as well (since K
satisï¬es (1)).
(â†)
Just interchange the letters F and K in the above argument.
â–¡
The above clearly is valid for functions h and gQ that may fail to be deï¬ned
everywhere in their â€œnaturalâ€ input sets. To be able to have this degree of
generality without having to state additional deï¬nitions (such as left ï¬elds,
right ï¬elds, partial functions, total functions, nontotal functions, Kleene â€œweak
equalityâ€), we have stated the recurrence (1) the way we did (to keep an eye on
both the input and output side of things) rather than the â€œusualâ€
f (x) =
h(x)
if x âˆˆI
gQ(x, f (a1),. . . , f (ar))
if Q(a1,. . . , ar, x) holds
â€  Cl(
I , 
R )-derivable.

28
I. Basic Logic
Of course, if all the gQ and h are deï¬ned everywhere on their input sets (i.e.,
they are â€œtotalâ€), then f is deï¬ned everywhere on Cl(I , R) (see Exercise I.4).
I.3. Axioms and Rules of Inference
Now that we have our language, L, we will embark on using it to formally
effect deductions. These deductions start at the axioms. Deductions employ
â€œacceptableâ€ purely syntactic â€“ i.e., based on form, not on meaning â€“ rules that
allow us to write a formula down (to deduce it) solely because certain other
formulas that are syntactically related to it were already deduced (i.e., already
written down). These string-manipulation rules are called rules of inference.
We describe in this section the axioms and the rules of inference that we will
accept into our logical calculus and that are common to all theories.
We start with a precise deï¬nition of tautologies in our ï¬rst order language L.
I.3.1 Deï¬nition (Prime Formulas in Wff. Propositional Variables). A for-
mula A âˆˆWff is a prime formula or a propositional variable iff it is either of
Pri1. atomic,
Pri2. a formula of the form ((âˆƒx)A).
We use the lowercase letters p, q,r (with or without subscripts or primes) to
denote arbitrary prime formulas (propositional variables) of our language.
â–¡
That is, a prime formula has either no propositional connectives, or if it does,
it hides them inside the scope of (âˆƒx).
We may think of a propositional variable as a â€œblobâ€ that a myopic being
makes out of a formula described in I.3.1. The same being will see an arbitrary
well-formed formula as a bunch of blobs, brackets, and Boolean connectives
(Â¬, âˆ¨), â€œcorrectly connectedâ€ as stipulated below.â€ 
I.3.2 Deï¬nition (Propositional Formulas). The set of propositional formulas
over V , denoted here by Prop, is the smallest set such that:
(1) Every propositional variable (over V ) is in Prop.
(2) If A and B are in Prop, then so are (Â¬A) and (A âˆ¨B ).
We use the lowercase letters p, q,r (with or without subscripts or primes) to
denote arbitrary prime formulas (propositional variables) of our language.
â–¡
â€  Interestingly, our myope can see the brackets and the Boolean connectives.

I.3. Axioms and Rules of Inference
29
I.3.3 Metatheorem. Prop = Wff.
Proof. (âŠ†) We do induction on Prop. Every item in I.3.2(1) is in Wff. Wff
satisï¬es I.3.2(2) (see I.1.8(b)). Done.
(âŠ‡) We do induction on Wff. Every item in I.1.8(a) is a propositional variable
(over V ), and hence is in Prop.
Prop trivially satisï¬es I.1.8(b). It also satisï¬es I.1.8(c), for if A is in Prop,
then it is in Wff by the âŠ†-direction, above. Then, by I.3.1, ((âˆƒx)A) is a propo-
sitional variable and hence in Prop. We are done once more.
â–¡
I.3.4 Deï¬nition (Propositional Valuations). We can arbitrarily assign a value
of 0 or 1 to every A in Wff (or Prop) as follows:
(1) We ï¬x an assignment of 0 or 1 to every prime formula. We can think of this
as an arbitrary but ï¬xed function v : {all prime formulas over L} â†’{0, 1}
in the metatheory.
(2) We deï¬ne by recursion an extension of v, denoted by Â¯v:
Â¯v((Â¬A)) = 1 âˆ’Â¯v(A)
Â¯v((A âˆ¨B )) = Â¯v(A) Â· Â¯v(B )
where â€œÂ·â€ above denotes number multiplication.
We call, traditionally, the values 0 and 1 by the names â€œtrueâ€ and â€œfalseâ€
respectively, and write t and f respectively.
We also call a valuation v a truth (value) assignment.
Weusethejargonâ€œAtakesthetruthvaluet(respectively,f)underavaluation
vâ€ to mean â€œÂ¯v(A) = 0 (respectively, Â¯v(A) = 1)â€.
â–¡
The above inductive deï¬nition of Â¯v relies on the fact that Deï¬nition I.3.2 of
Prop is unambiguous (I.2.10, p. 24), or that a propositional formula is uniquely
readable (or parsable) (see Exercises I.6 and I.7). It employs the metatheorem
on recursive deï¬nitions (I.2.13).
The reader may think that all this about unique readability is just an annoying
quibble. Actually it can be a matter of life and death. The ancient Oracle of
Delphi had the nasty habit of issuing ambiguous â€“ not uniquely readable, that
is â€“ pronouncements. One famous such pronouncement, rendered in English,
went like this: â€œYou will go you will return not dying in the warâ€.â€  Given that
ancientGreeksdidnotusepunctuation,theabovehastwodiametricallyopposite
meanings depending on whether you put a comma before or after â€œnotâ€.
â€  The original was â€œIÎ¾ÎµÎ¹Ï‚ Î±Ï•Î¹Î¾ÎµÎ¹Ï‚ oÏ… Î¸Î½Î·Î¾ÎµÎ¹Ï‚ ÎµÎ½ Ï€oÎ»ÎµÂµ Ï‰
Î¹ â€.

30
I. Basic Logic
The situation with formulas in Prop would have been as disastrous in the
absence of brackets â€“ which serve as punctuation â€“ because unique readability
would not be guaranteed: For example, for three distinct prime formulas p, q,r
we could ï¬nd a v such that Â¯v(p â†’q â†’r) is different depending on whether
we meant to insert brackets around â€œp â†’qâ€ or around â€œq â†’râ€ (can you ï¬nd
such a v?).
I.3.5 Remark (Truth Tables). Deï¬nition I.3.4 is often given in terms of truth-
functions. For example, we could have deï¬ned (in the metatheory, of course)
the function FÂ¬ : {t, f} â†’{t, f} by
FÂ¬(x) =
t
if x = f
f
if x = t
We could then say that Â¯v((Â¬A)) = FÂ¬(Â¯v(A)). One can similarly take care of
all the connectives (âˆ¨and all the abbreviations) with the help of truth functions
Fâˆ¨, Fâˆ§, Fâ†’, Fâ†”. These functions are conveniently given via so-called truth-
tables as indicated below:
x
y
FÂ¬(x)
Fâˆ¨(x, y)
Fâˆ§(x, y)
Fâ†’(x, y)
Fâ†”(x, y)
f
f
t
f
f
t
t
f
t
t
t
f
t
f
t
f
f
t
f
f
f
t
t
f
t
t
t
t
â–¡
I.3.6 Deï¬nition (Tautologies, Satisï¬able Formulas, Unsatisï¬able Formulas
in Wff). A formula A âˆˆWff (equivalently, in Prop) is a tautology iff for all
valuations v one has Â¯v(A) = t.
We call the set of all tautologies, as deï¬ned here, Taut. The symbol |=Taut A
says â€œA is in Tautâ€.
A formula A âˆˆWff (equivalently, in Prop) is satisï¬able iff for some valu-
ation v one has Â¯v(A) = t. We say that v satisï¬es A.
A set of formulas  is satisï¬able iff for some valuation v, one has Â¯v(A) = t
for every A in . We say that v satisï¬es .
A formula A âˆˆWff (equivalently, in Prop) is unsatisï¬able iff for all val-
uations v one has Â¯v(A) = f. A set of formulas  is unsatisï¬able iff for all
valuations v one has Â¯v(A) = f for some A in .
â–¡

I.3. Axioms and Rules of Inference
31
I.3.7 Deï¬nition (Tautologically Implies, for Formulas in Wff). Let A and
 be respectively any formula and any set of formulas (over L).
The symbol |=Taut A, pronounced â€œ tautologically implies Aâ€, means
that every truth assignment v that satisï¬es  also satisï¬es A.
â–¡
â€œSatisï¬ableâ€ and â€œunsatisï¬ableâ€ are terms introduced here in the propositional
or Boolean sense. These terms have a more complicated meaning when we
decide to â€œseeâ€ the object variables and quantiï¬ers that occur in formulas.
We have at once
I.3.8 Lemma.â€  |=Taut A iff âˆª{Â¬A} is unsatisï¬able (in the propositional
sense).
If = âˆ…then |=Taut A says just |=Taut A, since the hypothesis â€œevery truth
assignment v that satisï¬es â€, in the deï¬nition above, is vacuously satisï¬ed.
For that reason we almost never write âˆ…|=Taut A and write instead |=Taut A.
I.3.9 Exercise. For any formula A and any two valuations v and vâ€², Â¯v(A) =
Â¯vâ€²(A) if v and vâ€² agree on all the propositional variables that occur in A.
In the same manner, |=Taut A is oblivious to v-variations that do not affect
the variables that occur in  and A (see Exercise I.8).
â–¡
Before presenting the axioms, we need to introduce the concept of substitu-
tion.
I.3.10 Tentative Deï¬nition (Substitutions of Terms). Let A be a formula, x
an (object) variable, and t a term. A[x â†t] denotes the result of â€œreplacingâ€
all free occurrences of x in A by the term t, provided no variable of t was
â€œcapturedâ€ (by a quantiï¬er) during substitution.
â€  The word â€œlemmaâ€ has Greek origin, â€œÎ»Â´Î·ÂµÂµÎ±â€, plural â€œlemmataâ€ (some people say â€œlemmasâ€)
from â€œÎ»Â´Î·ÂµÂµÎ±Ï„Î±â€. It derives from the verb â€œÎ»Î±ÂµÎ² Â´Î±Î½Ï‰â€ (to take) and thus means â€œtaken thingâ€.
In mathematical reasoning a lemma is a provable auxiliary statement that is taken and used as
a stepping stone in lengthy mathematical arguments â€“ invoked therein by name, as in â€œ . . . by
Lemma such and such . . . â€ â€“ much as â€œsubroutinesâ€ (or â€œproceduresâ€) are taken and used as
auxiliary stepping stones to elucidate lengthy computer programs. Thus our purpose in having
lemmata is to shorten proofs by breaking them up into modules.

32
I. Basic Logic
If the proviso is valid, then we say that â€œt is substitutable for x (in A)â€, or
that â€œt is free for x (in A)â€. If the proviso is not valid, then the substitution is
undeï¬ned.
â–¡
I.3.11 Remark. There are a number of issues about Deï¬nition I.3.10 that need
discussion or clariï¬cation.
Reasonable people will be satisï¬ed with the above deï¬nition â€œas isâ€. How-
ever, there are some obscure points (enclosd in quotation marks above).
(1) What is this about â€œcaptureâ€? Well, suppose that A â‰¡(âˆƒx)Â¬x = y. Let
t â‰¡x.â€  Then A[y â†t] â‰¡(âˆƒx)Â¬x = x, which says something altogether
different than the original. Intuitively, this is unexpected (and undesirable):
A codes a statement about the free variable y, i.e., a statement about all
objects which could be â€œvaluesâ€ (or meanings) of y. One would have ex-
pected that, in particular, A[y â†x] â€“ if the substitution were allowed â€“
would make this very same statement about the values of x. It does not.â€¡
What happened is that x was captured by the quantiï¬er upon substitution,
thus distorting Aâ€™s original meaning.
(2) Are we sure that the term â€œreplaceâ€ is mathematically precise?
(3) Is A[x â†t] always a formula, if A is?
A re-visitation of I.3.10 via an inductive deï¬nition (by induction on terms
and formulas) settles (1)â€“(3) at once (in particular, the informal terms â€œreplaceâ€
and â€œcaptureâ€ do not appear in the inductive deï¬nition). We deï¬ne (again) the
symbol A[x â†t], for any formula A, variable x, and term t, this time by
induction on terms and formulas:
First off, let us deï¬ne s[x â†t], where s is also a term, by cases:
s[x â†t] â‰¡
ï£±
ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£³
t
if s â‰¡x
a
if s â‰¡a, a constant
(symbol)
y
if s â‰¡y, a variable Ì¸â‰¡x
f r1[x â†t]r2[x â†t] . . .rn[x â†t]
if s â‰¡f r1 . . .rn
Pause. Is s[x â†t] always a term? That this is so follows directly by induction
on terms, using the deï¬nition by cases above and the I.H. that each of ri[x â†t],
i = 1, . . . , n, is a term.
â€  Recall that in I.1.4 (p. 13) we deï¬ned the symbol â€œâ‰¡â€ to be equality on strings.
â€¡ The original says that for any object y there is an object that is different from it; A[y â†x] says
that there is an object that is different from itself.

I.3. Axioms and Rules of Inference
33
We turn now to formulas. The symbols P,r, s (with or without subscripts)
below denote a predicate of arity n, a term, and a term (respectively):
A[x â†t] â‰¡
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
s[x â†t] = r[x â†t]
if A â‰¡s = r
Pr1[x â†t]r2[x â†t] . . .
if A â‰¡Pr1 . . .rn
rn[x â†t]
(B [x â†t] âˆ¨C [x â†t])
if A â‰¡(B âˆ¨C )
(Â¬(B [x â†t]))
if A â‰¡(Â¬B )
A
if A â‰¡((âˆƒy)B ) and y â‰¡x
((âˆƒy)(B [x â†t]))
if A â‰¡((âˆƒy)B ) and y Ì¸â‰¡x
and y does not occur in t
In all cases above, the left hand side is deï¬ned iff the right hand side is.
Pause. We have eliminated â€œreplacesâ€ and â€œcapturedâ€. But is A[xâ†t] a for-
mula (whenever it is deï¬ned)? (See Exercise I.9)
â–¡
I.3.12 Deï¬nition (Simultaneous Substitution). The symbol
A[y1,. . . , yr â†t1,. . . , tr]
or, equivalently, A[âƒ—yr â†âƒ—tr] â€“ where âƒ—yr is an abbreviation of y1,. . . , yr â€“
denotes simultaneous substitution of the terms t1,. . . , tr into the variables
y1,. . . , yr in the following sense: Let âƒ—zr be variables that do not occur at all
(either as free or bound) in any of A, âƒ—tr. Then A[âƒ—yr â†âƒ—tr] is short for
A[y1 â†z1] . . . [yr â†zr][z1 â†t1] . . . [zr â†tr]
(1)
â–¡
Exercise I.10 shows that we obtain the same string in (1) above, regardless of
our choice of new variables âƒ—zr.
More Conventions. The symbol [x â†t] lies in the metalanguage. This
metasymbol has the highest priority, so that, e.g., A âˆ¨B [x â†t] means
A âˆ¨(B [x â†t]), (âˆƒx)B [x â†t] means (âˆƒx)(B [x â†t]), etc.
The reader is reminded about the conventions regarding the metanotations
A[âƒ—xr] and A(âƒ—xr) (see I.1.11). In the context of those notations, if t1,. . . , tr are
terms, the symbol A[t1,. . . , tr] abbreviates A[âƒ—yr â†âƒ—tr].
We are ready to introduce the (logical) axioms and rules of inference.

34
I. Basic Logic
Schemata.â€  Some of the axioms below will actually be schemata. A formula
schema, or formula form, is a string G of the metalanguage that contains syn-
tactic variables, such as A, P, f, a, t, x.
Whenever we replace all these syntactic variables that occur in G by speciï¬c
formulas, predicates, functions, constants, terms, or variables respectively, we
obtain a speciï¬c well-formed formula, a so-called instance of the schema. For
example, an instance of (âˆƒx)x = a is (âˆƒv12)v12 = 0 (in the language of Peano
arithmetic). An instance of A â†’A is v101 = v114 â†’v101 = v114.
I.3.13 Deï¬nition (Axioms and Axiom Schemata). The logical axioms are all
the formulas in the group Ax1 and all the possible instances of the schemata in
the remaining groups:
Ax1. All formulas in Taut.
Ax2. (Schema)
A[x â†t] â†’(âˆƒx)A
for any term t
By I.3.10â€“I.3.11, the notation already imposes a condition on t, that it is
substitutable for x.
N.B. We often see the above written as
A[t] â†’(âˆƒx)A[x]
or even
A[t] â†’(âˆƒx)A
Ax3. (Schema) For each object variable x, the formula x = x.
Ax4. (Leibnizâ€™s characterization of equality â€“ ï¬rst order version. Schema) For
any formula A, object variable x, and terms t and s, the formula
t = s â†’(A[x â†t] â†”A[x â†s])
N.B. The above is written usually as
t = s â†’(A[t] â†”A[s])
We must remember that the notation already requires that t and s be free
for x.
We will denote the above set of logical axioms by .
â–¡
â€  Plural of schema. This is of Greek origin, ÏƒÏ‡ Â´Î·ÂµÎ±, meaning â€“ e.g., in geometry â€“ ï¬gure or
conï¬guration or even formation.

I.3. Axioms and Rules of Inference
35
The logical axioms for equality are not the strongest possible, but they are
adequate for the job. What Leibniz really proposed was the schema t = s â†”
(âˆ€P)(P[t] â†”P[s]), which says, intuitively, that â€œtwo objects t and s are equal
iff, for every â€˜property Pâ€™, both have P or neither has Pâ€.
Unfortunately, our system of notation (ï¬rst-order language) does not allow
quantiï¬cation over predicate symbols (which can have as â€œvaluesâ€ arbitrary
â€œpropertiesâ€). But is not Ax4 read â€œfor all formulas Aâ€ anyway? Yes, but with
one qualiï¬cation: â€œFor all formulas A that we can write down in our system of
notationâ€, and, alas, we cannot write all possible formulas of real mathematics
down, because they are too many.â€ 
While the symbol â€œ=â€ is suggestive of equality, it is not its shape that
qualiï¬es it as equality. It is the two axioms, Ax3 and Ax4, that make the symbol
behave as we expect equality to behave, and any other symbol of any other
shape (e.g., Enderton (1972) uses â€œâ‰ˆâ€) satisfying these two axioms qualiï¬es as
formal equality that is intended to codify the metamathematical standard â€œ=â€.
I.3.14 Remark. In Ax2 and Ax4 we imposed the condition that t (and s) must
be substitutable in x. Here is why:
Take A to stand for (âˆ€y)x = y and B to stand for (âˆƒy)Â¬x = y. Then, tem-
porarily suspending the restriction on substitutability, A[x â†y] â†’(âˆƒx)A is
(âˆ€y)y = y â†’(âˆƒx)(âˆ€y)x = y
and x = y â†’(B â†”B [x â†y]) is
x = y â†’((âˆƒy)Â¬x = y â†”(âˆƒy)Â¬y = y)
neither of which, obviously, is â€œvalidâ€.â€¡
There is a remedy in the metamathematics: Move the quantiï¬ed variable(s)
out of harmâ€™s way, by renaming them so that no quantiï¬ed variable in A has
the same name as any (free, of course) variable in t (or s).
This renaming is formally correct (i.e., it does not change the meaning of
the formula), as we will see in the variant (meta)theorem (I.4.13). Of course,
â€  â€œUncountably manyâ€, in a precise technical sense developed in the chapter on cardinality in
volume 2 (see p. 62, of this volume for a brief informal â€œcourseâ€ in cardinality). This is due to
Cantorâ€™s theorem, which implies that there are uncountably many subsets of N. Each such subset
A gives rise to the formula x âˆˆA in the metalanguage.
On the other hand, set theoryâ€™s formal system of notation, using just âˆˆand U as start-up
(nonlogical) symbols, is only rich enough to write down a countably inï¬nite set of formulas
(cf. p. 62). Thus, our notation will fail to denote uncountably many â€œreal formulasâ€ x âˆˆA.
â€¡ Speaking intuitively is enough for now. Validity will be deï¬ned carefully pretty soon.

36
I. Basic Logic
it is always possible to effect this renaming, since we have countably many
variables, and only ï¬nitely many appear free in t (and s) and A. This trivial
remedy allows us to render the conditions in Ax2 and Ax4 harmless. Essentially,
a t (or s) is always substitutable after renaming.
â–¡
It is customary to assume a Platonist metatheory, and we do so. We can then
say â€œcountably manyâ€ variables without raising any eyebrows. Alternatively,
we know how to get a new variable that is different from all those in a given
ï¬nite set of variables without invoking an inï¬nite supply.
I.3.15 Deï¬nition (Rules of Inference). The following are the two rules of
inference. These rules are relations in the sense of Section I.2, with inputs from
the set Wff and outputs also in Wff. They are written traditionally as â€œfractionsâ€.
We call the â€œnumeratorâ€ the premise(s) and the â€œdenominatorâ€ the conclusion.
We say that a rule of inference is applied to the formula(s) in the numerator,
and that it yields (or results in) the formula in the denominator.
Inf1. Modus ponens, or MP. For any formulas A and B ,
A, A â†’B
B
Inf2. âˆƒ-introduction â€“ pronounced E-introduction. For any formulas A and B
such that x is not free in B ,
A â†’B
(âˆƒx)A â†’B
N.B. Recall the conventions on eliminating brackets!
â–¡
It is immediately clear that the deï¬nition above meets our requirement that the
rules of inference be â€œalgorithmicâ€, in the sense that whether they are applicable
or how they are applicable can be decided and carried out in a ï¬nite number
of steps by just looking at the form of (potential input) formulas (not at the
â€œmeaningâ€ of such formulas).
We next deï¬ne -theorems, that is, formulas we can prove from the set of
formulas  (this  may be empty).
I.3.16 Deï¬nition (-Theorems). The set of -theorems, Thm, is the least
inclusive subset of Wff that satisï¬es:
Th1.  âŠ†Thm (cf. I.3.13).
Th2.  âŠ†Thm. We call every member of  a nonlogical axiom.
Th3. Thm is closed under each rule Inf1â€“Inf2.

I.3. Axioms and Rules of Inference
37
The metalinguistic statement A âˆˆThm is traditionally written as  âŠ¢A,
and we say that A is proved from  or that it is a -theorem.
We also say that A is deduced by , or that  deduces A.
If  = âˆ…, then rather than âˆ…âŠ¢A we write âŠ¢A. We often say in this case
that A is absolutely provable (or provable with no nonlogical axioms).
We often write A, B , . . . , D âŠ¢E for {A, B , . . . , D } âŠ¢E .
â–¡
I.3.17 Deï¬nition (-Proofs). We just saw that Thm is Cl(I , R), where I is
the set of all logical and nonlogical axioms, and R contains just the two rules
of inference. An (I , R)-derivation is also called a -proof (or just proof, if 
is understood).
â–¡
I.3.18 Remark. (1) It is clear that if each of A1, . . . , An has a -proof and
B has an {A1, . . . , An}-proof, then B has a -proof. Indeed, simply con-
catenate all of the given -proofs (in any sequence). Append to the right of that
sequence the given {A1, . . . , An}-proof (that ends with B ). Then the entire
sequence is a -proof, and ends with B .
We refer to this phenomenon as the transitivity of âŠ¢.
N.B. Transitivity of âŠ¢allows one to invoke previously proved (by him or
others) theorems in the course of a proof. Thus, practically, a -proof is a
sequence of formulas in which each formula is an axiom, is a known -theorem,
or is obtained by applying a rule of inference on previous formulas of the
sequence.
(2) If  âŠ† and  âŠ¢A, then also  âŠ¢A, as follows from I.3.16 or I.3.17.
In particular, âŠ¢A implies  âŠ¢A for any .
(3) It is immediate from the deï¬nitions that for any formulas A and B ,
A, A â†’B âŠ¢B
(i)
and if, moreover, x is not free in B ,
A â†’B âŠ¢(âˆƒx)A â†’B
(ii)
Some texts (e.g., SchÂ¨utte (1977)) give the rules in the format of (i)â€“(ii) above.
â–¡
The axioms and rules provide us with a calculus, that is, a means to â€œcal-
culateâ€ proofs and theorems. In the interest of making the calculus more user-
friendly â€“ and thus more easily applicable to mathematical theories of interest,
such as Peano arithmetic or set theory â€“ we are going to develop in the next
section a number of derived principles. These principles are largely of the form

38
I. Basic Logic
A1,. . . , An âŠ¢B . We call such a (provable in the metatheory) principle a de-
rived rule of inference, since, by transitivity of âŠ¢, it can be used as a proof-step
in a -proof. By contrast, the rules Inf1â€“Inf2 are â€œbasicâ€ or â€œprimaryâ€; they
are given outright.
We can now ï¬x our understanding of the concept of a formal or mathematical
theory.
A (ï¬rst order) formal (mathematical) theory over a language L, or just theory
over L, or just theory, is a tuple (of â€œingredientsâ€) T = (L, , I, T ), where
L is a ï¬rst order language,  is a set of logical axioms, I is a set of rules of
inference, and T a non-empty subset of Wff that is required to contain  (i.e.,
 âŠ†T ) and be closed under the rules I.
Equivalently, one may simply require that T is closed under âŠ¢, that is, for
any  âŠ†T and any formula A, if  âŠ¢A, then A âˆˆT . This is, furthermore,
equivalent to requiring that
A âˆˆT
iff
T âŠ¢A
(1)
Indeed, the if direction follows from closure under âŠ¢, while the only if direction
is a consequence of Deï¬nition I.3.16.
T is the set of the formulas of the theory,â€  and we often say â€œa theory T â€,
taking everything else for granted.
If T = Wff, then the theory T is called inconsistent or contradictory. Oth-
erwise it is called consistent.
Throughout our exposition we ï¬x  and I as in Deï¬nitions I.3.13 and I.3.15.
By (1), T = ThmT . This observation suggests that we call theories such as
the ones we have just deï¬ned axiomatic theories, in that a set  always exists
so that T = Thm (if at a loss, we can just take  = T ).
We are mostly interested in theories T for which there is a â€œsmallâ€ set 
(â€œsmallâ€ by comparison with T ) such that T = Thm. We say that T is
axiomatized by . Naturally, we call T
the set of theorems, and  the set of
nonlogical axioms of T.
If, moreover,  is recognizable (i.e., we can tell â€œalgorithmicallyâ€ whether
or not a formula A is in ), then we say that T is recursively axiomatized.
Examples of recursively axiomatized theories are ZFC set theory and Peano
arithmetic. On the other hand, if we take T to be all the formulas of arithmetic
that are true when interpreted â€œin the intended wayâ€â€¡ over N â€“ the so-called
â€  As opposed to â€œof the languageâ€, which is all of Wff.
â€¡ That is, the symbol â€œ0â€ of the language is interpreted as the 0 âˆˆN, â€œSxâ€ as x + 1, â€œ(âˆƒx)â€ as
â€œthere is an x âˆˆNâ€, etc.

I.3. Axioms and Rules of Inference
39
complete arithmetic â€“ then there is no recognizable  such that T
= Thm.
We say that complete arithmetic is not recursively axiomatizable.â€ 
Pause. Why does complete arithmetic form a theory? Because work of Sec-
tion I.5 â€“ in particular, the soundness theorem â€“ entails that it is closed under âŠ¢.
We tend to further abuse language and call axiomatic theories by the name
of their (set of) nonlogical axioms . Thus if T = (L, , I, T ) is a ï¬rst order
theory and T
= Thm, then we may say interchangeably â€œtheory T â€, â€œtheory
T â€ or â€œtheory â€.
If  = âˆ…, then we have a pure or absolute theory (i.e., we are â€œjust doing
logic, not mathâ€). If  Ì¸= âˆ…, then we have an applied theory.
Argot. A ï¬nal note on language versus metalanguage, and theory versus
metatheory. When are we speaking the metalanguage and when are we speaking
the formal language?
The answer is, respectively, â€œalmost alwaysâ€ and â€œalmost neverâ€. As it
has been remarked before, in principle, we are speaking the formal language
exactly when we are pronouncing or writing down a string from Term or Wff.
Otherwise we are (speaking or writing) in the metalanguage. It appears that we
(and everybody else who has written a book in logic or set theory) is speaking
and writing within the metalanguage with a frequency approaching 100%.
The formalist is clever enough to simplify notation at all times. We will
seldom be caught writing down a member of Wff in this book, and, on the rare
occasions we may do so, it will only be to serve as an illustration of why one
should avoid writing down such formulas: because they are too long and hard
to read and understand.
We will be speaking the formal language with a heavy â€œaccentâ€ and using
many â€œidiomsâ€ borrowed from â€œrealâ€ (meta)mathematics and English. We will
call our dialect argot, following Manin (1977).
The important thing to remember is when we are working in the theory,â€¡ and
this is precisely when we generate theorems. That is, it does not matter if a theo-
rem (and much of the what we write down during the proof) is written in argot.
Two examples:
(1) One is working in formal number theory (or formal arithmetic) if one states
and proves (say, from the Peano axioms) that â€œevery natural number n > 1
â€  The trivial solution â€“ that is, taking  = T â€“ will not do, for it turns out that T is not
recognizable.
â€¡ Important, because arguing in the theory restricts us to use only its axioms (and earlier proved
theorems; cf. I.3.18) and its rules of inference â€“ nothing extraneous to these syntactic tools is
allowed.

40
I. Basic Logic
has a prime factorâ€. Note how this theorem is stated in argot. Below we
give its translation into the formal language of arithmetic:â€ 
(âˆ€n)(S0 < n â†’(âˆƒx)(âˆƒy)(n = x Ã— y âˆ§
S0 < x âˆ§(âˆ€m)(âˆ€r)(x = m Ã— r â†’m = S0 âˆ¨m = x)))
(1)
(2) One is working in formal logic if one is writing a proof of (âˆƒv13)v13 = v13.
Suppose though that our activity consists of effecting deï¬nitions, introducing
axioms,oranalyzingthebehaviourorcapabilityofT,e.g.,provingsomederived
rule A1,. . . , An âŠ¢B â€“ that is, a theorem schema â€“ or investigating consis-
tencyâ€¡ or â€œrelative consistencyâ€.Â§ Then we are operating in the metatheory,
that is, in â€œrealâ€ mathematics.
One of the most important problems posed in the metatheory is
â€œGiven a theory T and a formula A. Is A a theorem of T?â€
This is Hilbertâ€™s Entscheidungsproblem, or decision problem. Hilbert be-
lieved that every recursively axiomatized theory ought to admit a â€œgeneralâ€
solution, by more or less mechanical means, to its decision problem. The tech-
niques of GÂ¨odel and the insight of Church showed that this problem is, in
general, algorithmically unsolvable.
As we have already stated (p. 36), metamathematics exists outside and in-
dependently of our effort to build this or that formal system. All its methods
are â€“ in principle â€“ available to us for use in the analysis of the behaviour of a
formal system.
Pause. But how much of real mathematics are we allowed to use, reliably, to
study or speak about the â€œsimulatorâ€ that the formal system is?Â¶ For example,
have we not overstepped our license by using induction (and, implicitly, the
entire inï¬nite set N) in our Platonist metatheory, speciï¬cally in the recursive or
inductive deï¬nitions of terms, well-formed formulas, theorems, etc.?
The quibble here is largely â€œpoliticalâ€. Some people argue (a major propo-
nent of this was Hilbert) as follows: Formal mathematics was meant to crank
out â€œtrueâ€ statements of mathematics, but no â€œfalseâ€ ones, and this freedom
â€  Well, almost. In the interest of brevity, all the variable names used in the displayed formula (1)
are metasymbols.
â€¡ That is, whether or not T = Wff.
Â§ That is, â€œif  is consistentâ€ â€“ where we are naming the theory by its nonlogical axioms â€“ â€œdoes
it stay so after we have added some formula A as a nonlogical axiom?â€
Â¶ The methods or scope of the metamathematics that a logician uses â€“ in the investigation of some
formal system â€“ are often restricted for technical or philosophical reasons.

I.3. Axioms and Rules of Inference
41
from contradiction ought to be veriï¬able. Now, as we are so verifying in the
metatheory (i.e., outside the formal system) shouldnâ€™t the metatheory itself be
â€œabove suspicionâ€ (of contradiction, that is)? Naturally.
Hilbertâ€™s suggestion for achieving this â€œabove suspicionâ€ status was, essen-
tially, to utilize in the metatheory only a small fragment of â€œrealityâ€ that is
so simple and close to intuition that it does not need itself a â€œcertiï¬cateâ€ (via
formalization) for its freedom from contradiction. In other words, restrict the
metamathematics.â€  Such a fragment of the metatheory, he said, should have
nothing to do with the inï¬nite, in particular with the entire set N and all that it
entails (e.g., inductive deï¬nitions and proofs).â€¡
If it were not for GÂ¨odelâ€™s incompleteness results, this position â€“ that meta-
mathematical techniques must be ï¬nitary â€“ might have prevailed. However,
GÂ¨odel proved it to be futile, and most mathematicians have learnt to feel com-
fortable with inï¬nitary metamathematical techniques, or at least with N and
induction.Â§ Of course, it would be reckless to use as metamathematical tools
â€œmathematicsâ€ of suspect consistency (e.g., the full naÂ¨Ä±ve theory of sets).
It is worth pointing out that one could ï¬t (with some effort) our inductive
deï¬nitions within Hilbertâ€™s style. But we will not do so. First, one would have
to abandon the elegant (and now widely used) approach with closures, and use
instead the concept of derivations of Section I.2. Then one would somehow
have to effect and study derivations without the beneï¬t of the entire set N.
Bourbaki (1966b, p. 15) does so with his constructions formatives. Hermes
(1973) is another author who does so, with his â€œterm-â€ and â€œformula-calculiâ€
(such calculi being, essentially, ï¬nite descriptions of derivations).
Bourbaki (but not Hermes) avoids induction over all of N. In his metamath-
ematical discussions of terms and formulasÂ¶ that are derived by a derivation
â€  Otherwise we would need to formalize the metamathematics â€“ in order to â€œcertifyâ€ it â€“ and
next the metametamathematics, and so on. For if â€œmetaMâ€ is to authoritatively check â€œMâ€ for
consistency, then it too must be consistent; so let us formalize â€œmetaMâ€ and let â€œmetametaMâ€
check it; . . . a never ending story.
â€¡ See Hilbert and Bernays (1968, pp. 21â€“29) for an elaborate scheme that constructs â€œconcrete
number objectsâ€ â€“ Ziffern or â€œnumeralsâ€ â€“ â€œ|â€, â€œ||â€, â€œ|||â€, etc., that stand for â€œ1â€, â€œ2â€, â€œ3â€, etc.,
complete with a â€œconcrete mathematical inductionâ€ proof technique on these objects, and even
the beginnings of their recursion theory. Of course, at any point, only ï¬nite sets of such objects
were considered.
Â§ Some proponents of inï¬nitary techniques in metamathematics have used very strong words
in describing the failure of â€œHilbertâ€™s programâ€. Rasiowa and Sikorski (1963) write in their
introduction: â€œHowever GÂ¨odelâ€™s results exposed the ï¬asco of Hilbertâ€™s ï¬nitistic methods as far
as consistency is concerned.â€
Â¶ For example, in loc. cit., p. 18, where he proves that, in our notation, A[x â†y] and t[x â†y]
are a formula and term respectively.

42
I. Basic Logic
d1,. . . , dn, he restricts his induction arguments on the segment {0, 1,. . . , n},
that is, he takes an I.H. on k < n and proceeds to k + 1.
I.4. Basic Metatheorems
We are dealing with an arbitrary theory T = (L, , I, T ), such that  is the
set of logical axioms (I.3.13) and I are the inference rules (I.3.15). We also let
 be an appropriate set of nonlogical axioms, i.e., T = Thm.
I.4.1 Metatheorem (Postâ€™s â€œExtendedâ€ Tautology Theorem). If A1,. . . ,
An |=Taut B then A1,. . . , An âŠ¢B .
â–¡
Proof. The assumption yields that
|=Taut A1 â†’Â· Â· Â· â†’An â†’B
(1)
Thus, since the formula in (1) is in , using Deï¬nition I.3.16, we have
A1,Â· Â· Â· , An âŠ¢A1 â†’Â· Â· Â· â†’An â†’B
(2)
Applying modus ponens to (2) n times, we deduce B .
â–¡
I.4.1 is an omnipresent derived rule.
I.4.2 Deï¬nition. A and B provably equivalent in T means that  âŠ¢A â†”B .
I.4.3 Metatheorem. Any two theorems A and B of T are provably equivalent
in T.
Proof. By I.4.1,  âŠ¢A yields  âŠ¢B â†’A. Similarly,  âŠ¢B yields
 âŠ¢A â†’B . One more application of I.4.1 yields  âŠ¢A â†”B .
â–¡
Worth noting: âŠ¢Â¬x = x â†”Â¬y = y (why?), but neither Â¬x = x nor Â¬y = y
is a âˆ…-theorem.
I.4.4 Remark (Hilbert Style Proofs). In practice we write proofs â€œverticallyâ€,
that is, as numbered vertical sequences (or lists) of formulas. The numbering
helps the annotational comments that we insert to the right of each formula that
we list, as the following proof demonstrates.

I.4. Basic Metatheorems
43
A metatheorem admits a metaproof, strictly speaking. The following is a
derived rule (or theorem schema) and thus belongs to the metatheory (and so
does its proof).
Another point of view is possible, however: The syntactic symbols x, A,
and B below stand for a speciï¬c variable and speciï¬c formulas that we just
forgot to write down explicitly. Then one can think of the proof as a (formal)
Hilbert style proof.
â–¡
I.4.5 Metatheorem (âˆ€-Introduction â€“ Pronounced â€œA-Introductionâ€). If x
does not occur free in A, then A â†’B âŠ¢A â†’(âˆ€x)B .
Proof.
(1)
A â†’B
given
(2)
Â¬B â†’Â¬A
(1) and I.4.1
(3)
(âˆƒx)Â¬B â†’Â¬A
(2) and âˆƒ-introduction
(4)
A â†’Â¬(âˆƒx)Â¬B
(3) and I.4.1
(5)
A â†’(âˆ€x)B
(4), introducing the âˆ€-abbreviation
â–¡
I.4.6 Metatheorem (Specialization). For any formula A and term t,
âŠ¢(âˆ€x)A â†’A [t].
At this point, the reader may want to review our abbreviation conventions, in
particular, see Ax2 (I.3.13).
Proof.
(1)
Â¬A[t] â†’(âˆƒx)Â¬A
in 
(2)
Â¬(âˆƒx)Â¬A â†’A[t]
(1) and I.4.1
(3)
(âˆ€x)A â†’A [t]
(2), introducing the âˆ€-abbreviation
â–¡
I.4.7 Corollary. For any formula A, âŠ¢(âˆ€x)A â†’A .
Proof. A[x â†x] â‰¡A.
â–¡
Pause. Why is A[x â†x] the same string as A?
I.4.8 Metatheorem (Generalization). For any  and any A, if  âŠ¢A, then
 âŠ¢(âˆ€x)A .

44
I. Basic Logic
Proof. Choose y Ì¸â‰¡x. Then we continue any given proof of A (from ) as
follows:
(1)
A
proved from 
(2)
y = y â†’A
(1) and I.4.1
(3)
y = y â†’(âˆ€x)A
(2) and âˆ€-introduction
(4)
y = y
in 
(5)
(âˆ€x)A
(3), (4), and MP
â–¡
I.4.9 Corollary. For any  and any A,  âŠ¢A iff  âŠ¢(âˆ€x)A.
Proof. By I.4.7, I.4.8, and modus ponens.
â–¡
I.4.10 Corollary. For any A, A âŠ¢(âˆ€x)A and (âˆ€x)A âŠ¢A.
â–¡
The above corollary motivates the following deï¬nition. It also justiï¬es the
common mathematical practice of the â€œimplied universal quantiï¬erâ€. That is,
we often state â€œ. . . x . . . â€ when we mean â€œ(âˆ€x) . . . x . . . â€.
I.4.11 Deï¬nition (Universal Closure). Let y1,. . . , yn be the list of all free vari-
ables of A. The universal closure of A is the formula (âˆ€y1)(âˆ€y2) Â· Â· Â· (âˆ€yn)A â€“
often written more simply as (âˆ€y1y2 . . . yn)A or even (âˆ€âƒ—yn)A.
â–¡
By I.4.10, a formula deduces and is deduced by its universal closure.
Pause. We said the universal closure. Hopefully, the remark immediately above
is undisturbed by permutation of (âˆ€y1)(âˆ€y2) Â· Â· Â· (âˆ€yn). Is it? (Exercise I.11).
I.4.12 Corollary (Substitution of Terms). A[x1,. . . , xn] âŠ¢A[t1,. . . , tn] for
any terms t1,. . . , tn.
The reader may wish to review I.3.12 and the remark following it.
Proof. We illustrate the proof for n = 2. What makes it interesting is the re-
quirement to have â€œsimultaneous substitutionâ€. To that end we ï¬rst substitute
into x1 and x2 new variables z, w â€“ i.e., not occurring in either A or in the ti.
The proof is the following sequence. Comments justify, in each case, the pres-
ence of the formula immediately to the left by virtue of the presence of the

I.4. Basic Metatheorems
45
immediately preceding formula.
A[x1, x2]
starting point
(âˆ€x1)A[x1, x2]
generalization
A[z, x2]
specialization; x1 â†z
(âˆ€x2)A[z, x2]
generalization
A[z, w]
specialization; x2 â†w
Now z â†t1, w â†t2, in any order, is the same as â€œsimultaneous substitu-
tion I.3.12â€:
(âˆ€z)A[z, w]
generalization
A[t1, w]
specialization; z â†t1
(âˆ€w)A[t1, w]
generalization
A[t1, t2]
specialization; w â†t2
â–¡
I.4.13 Metatheorem (The Variant, or Dummy-Renaming, Metatheorem).
For any formula (âˆƒx)A, if z does not occur in it (i.e., is neither free nor bound),
then âŠ¢(âˆƒx)A â†”(âˆƒz)A[x â†z].
We often write this (under the stated conditions) as âŠ¢(âˆƒx)A[x] â†”(âˆƒz)A[z].
By the way, another way to state the conditions is â€œif z does not occur in A
(i.e., is neither free nor bound in A), and is different from xâ€. Of course, if
z â‰¡x, then there is nothing to prove.
Proof. Since z is substitutable in x under the stated conditions, A[x â†z] is
deï¬ned. Thus, by Ax2,
âŠ¢A[x â†z] â†’(âˆƒx)A
By âˆƒ-introduction â€“ since z is not free in (âˆƒx)A â€“ we also have
âŠ¢(âˆƒz)A[x â†z] â†’(âˆƒx)A
(1)
We note that x is not free in (âˆƒz)A[xâ†z] and is free for z in A[xâ†z]. Indeed,
A[x â†z][z â†x] â‰¡A. Thus, by Ax2,
âŠ¢A â†’(âˆƒz)A[x â†z]
Hence, by âˆƒ-introduction,
âŠ¢(âˆƒx)A â†’(âˆƒz)A[x â†z]
(2)
Tautological implication from (1) and (2) concludes the argument.
â–¡

46
I. Basic Logic
Why is A[x â†z][z â†x] â‰¡A? We can see this by induction on A (recall
that z occurs as neither free nor bound in A).
If A is atomic, then the claim is trivial. The claim also clearly â€œpropagatesâ€
with the propositional formation rules, that is, I.1.8(b).
Consider then the case that A â‰¡(âˆƒw)B . Note that w â‰¡x is possible
under our assumptions, but w â‰¡z is not. If w â‰¡x, then A[x â†z] â‰¡A; in
particular, z is not free in A; hence A[x â†z][z â†x] â‰¡A as well.
So let us work with w Ì¸â‰¡x. By I.H., B [x â†z][z â†x] â‰¡B . Now
A[x â†z][z â†x] â‰¡((âˆƒw)B )[x â†z][z â†x]
â‰¡((âˆƒw)B [x â†z])[z â†x]
see I.3.11; w Ì¸â‰¡z
â‰¡((âˆƒw)B [x â†z][z â†x])
see I.3.11; w Ì¸â‰¡x
â‰¡((âˆƒw)B )
I.H.
â‰¡A
By I.4.13, the issue of substitutability becomes moot. Since we have an inï¬nite
supply of variables (to use, for example, as bound variables), we can always
change the names of all the bound variables in A so that the new names are
different from all the free variables in A or t. In so doing we obtain a formula
B that is (absolutely) provably equivalent to the original.
Then B [x â†t] will be deï¬ned (t will be substitutable in x). Thus, the
moral is: any term t is free for x in A after an appropriate â€˜dummyâ€™ renaming.
By the way, this is one of the reasons we want an inï¬nite supply (or an
extendible ï¬nite set, for the ï¬nitist) of formal variables.
I.4.14 Deï¬nition. In the following we will often discuss two (or more) theories
at once. Let T = (L, , I, T ) and Tâ€² = (Lâ€², , I, T â€²) be two theories, such
that V
âŠ†V â€². This enables Tâ€² to be â€œawareâ€ of all the formulas of T (but not
vice versa, since Lâ€² contains additional nonlogical symbols).
We say that Tâ€² is an extension of T (in symbols, T â‰¤Tâ€² ) iff T âŠ†T â€².
Let A be a formula over L (so that both theories are aware of it). The symbols
âŠ¢T A and âŠ¢Tâ€² A are synonymous with A âˆˆT and A âˆˆT â€² respectively.
Note that we did not explicitly mention the nonlogical axioms  or â€² to the
left of âŠ¢, since the subscript of âŠ¢takes care of that information.
We say that the extension is conservative iff for any A over L, whenever
âŠ¢Tâ€² A it is also the case that âŠ¢T A. That is, when it comes to formulas over
the language (L) that both theories understand, then the new theory does not
do any better than the old in producing theorems.
â–¡
I.4.15 Metatheorem (Metatheorem on Constants). Let us extend a language
L of a theory T by adding new constant symbols e1,. . . , en to the alphabet V ,
resulting in the alphabet V â€², language Lâ€², and theory Tâ€².

I.4. Basic Metatheorems
47
Furthermore, assume that â€² = , that is, we did not add any new nonlogical
axioms.
ThenâŠ¢Tâ€² A[e1,. . . , en]impliesâŠ¢T A[x1,. . . , xn]foranyvariables x1,. . . ,
xn that occur nowhere in A[e1,. . . , en], as either free or bound variables.
Proof. Fix a set of variables x1,. . . , xn as described above. We do induction on
Tâ€²-theorems.
Basis. A[e1,. . . , en] is a logical axiom (over Lâ€²); hence so is A[x1,. . . , xn],
over L â€“ because of the restriction on the xi â€“ thus âŠ¢T A[x1,. . . , xn]. Note
that A[e1,. . . , en] cannot be nonlogical under our assumptions.
Pause. What does the restriction on the xi have to do with the claim above?
Modus ponens. Here âŠ¢Tâ€²
B [e1,. . . , en] â†’A[e1,. . . , en] and âŠ¢Tâ€²
B [e1,. . . , en]. By I.H., âŠ¢T
B [y1,. . . , yn] â†’A[y1,. . . , yn] and âŠ¢T
B [y1,. . . , yn], where y1,. . . , yn occur nowhere in B [e1,. . . , en]
â†’
A[e1,. . . , en] as either free or bound variables. By modus ponens, âŠ¢T
A[y1,. . . , yn]; hence âŠ¢T A[x1,. . . , xn] by I.4.12 (and I.4.13).
âˆƒ-introduction. We have âŠ¢Tâ€² B [e1,. . . , en] â†’C [e1,. . . , en], z is not free
in C [e1,. . . , en], and A[e1,. . . , en] â‰¡(âˆƒz)B [e1,. . . , en] â†’C [e1,. . . , en].
By I.H., if w1,. . . , wn â€“ distinct from z â€“ occur nowhere in B [e1,. . . , en] â†’
C [e1,. . . , en] as either free or bound, then we get âŠ¢T B [w1,. . . , wn] â†’
C [w1,. . . , wn]. By âˆƒ-introduction we get âŠ¢T
(âˆƒz)B [w1,. . . , wn]
â†’
C [w1,. . . , wn]. By I.4.12 and I.4.13 we get âŠ¢T (âˆƒz)B [x1,. . . , xn] â†’
C [x1,. . . , xn], i.e., âŠ¢T A[x1,. . . , xn].
â–¡
I.4.16 Corollary. Let us extend a language L of a theory T by adding new
constant symbols e1,. . . , en to the alphabet V , resulting to the alphabet V â€²,
language Lâ€², and theory Tâ€².
Furthermore, assume that â€² = , that is, we did not add any new nonlogical
axioms.
Then âŠ¢Tâ€² A[e1,. . . , en] iff âŠ¢T A[x1,. . . , xn], for any choice of variables
x1,. . . , xn.
â–¡
Proof. If part: Trivially, âŠ¢T A[x1,. . . , xn] implies âŠ¢Tâ€² A[x1,. . . , xn], hence
âŠ¢Tâ€² A[e1,. . . , en] by I.4.12.
Only-if
part:
Choose
variables
y1,. . . , yn
that
occur
nowhere
in
A[e1,. . . , en] as either free or bound. By I.4.15, âŠ¢T A[y1,. . . , yn]; hence,
by I.4.12 and I.4.13, âŠ¢T A[x1,. . . , xn].
â–¡
I.4.17 Remark. Thus, the extension Tâ€² of T is conservative, for, if A is over
L, then A[e1,. . . , en] â‰¡A. Therefore, if âŠ¢Tâ€² A, then âŠ¢Tâ€² A[e1,. . . , en];
hence âŠ¢T A[x1,. . . , xn], that is, âŠ¢T A.

48
I. Basic Logic
A more emphatic way to put the above is this: Tâ€² is not aware of any new
nonlogical facts that T did not already â€œknowâ€ although by a different name. If
Tâ€² can prove A[e1,. . . , en], then T can prove the same â€œstatementâ€, however,
using (any) names (other than the ei) that are meaningful in its own language;
namely, it can prove A[x1,. . . , xn].
â–¡
The following corollary stems from the proof (rather than the statement)
of I.4.15 and I.4.16, and is important.
I.4.18 Corollary. Let e1,. . . , en be constants that do not appear in the nonlog-
ical axioms . Then, if x1,. . . , xn are any variables, and if  âŠ¢A[e1,. . . , en],
it is also the case that  âŠ¢A[x1,. . . , xn].
I.4.19 Metatheorem (The Deduction Theorem). For any closed formula A,
arbitrary formula B , and set of formulas , if +A âŠ¢B , then  âŠ¢A â†’B .
N.B.  + A denotes the augmentation of  by adding the formula A. In the
present metatheorem A is a single (but unspeciï¬ed) formula. However, the no-
tation extends to the case where A is a schema, in which case it means the
augmentation of  by adding all the instances of the schema.
A converse of the metatheorem is also true trivially: That is,  âŠ¢A â†’B
implies  + A âŠ¢B . This direction immediately follows by modus ponens
and does not require the restriction on A.
Proof. The proof is by induction on  + A theorems.
Basis. Let B be logical or nonlogical (but, in the latter case, assume
B Ì¸â‰¡A). Then  âŠ¢B .
Since B |=Taut A â†’B , it follows by I.4.1 that  âŠ¢A â†’B .
Now, if B â‰¡A, then A â†’B is a logical axiom (group Ax1); hence
 âŠ¢A â†’B once more.
Modus ponens. Let  + A âŠ¢C , and  + A âŠ¢C â†’B .
By I.H.,  âŠ¢A â†’C and  âŠ¢A â†’C â†’B .
Since A â†’C , A â†’C â†’B |=Taut A â†’B , we have  âŠ¢A â†’B .
âˆƒ-introduction. Let  + A âŠ¢C â†’D , and B â‰¡(âˆƒx)C â†’D , where x is
not free in D . By I.H.,  âŠ¢A â†’C â†’D . By I.4.1,  âŠ¢C â†’A â†’D ;
hence  âŠ¢(âˆƒx)C â†’A â†’D by âˆƒ-introduction (A is closed). One more
application of I.4.1 yields  âŠ¢A â†’(âˆƒx)C â†’D .
â–¡
I.4.20 Remark. (1) Is the restriction that, A must be closed important? Yes.
Let A â‰¡x = a, where â€œaâ€ is some constant. Then, even though A âŠ¢(âˆ€x)A

I.4. Basic Metatheorems
49
by generalization, it is not always true that âŠ¢A â†’(âˆ€x)A. This follows from
soundness considerations (next section). Intuitively, assuming that our logic
â€œdoesnâ€™t lieâ€ (that is, it proves no â€œinvalidâ€ formulas), we immediately infer
that x = a â†’(âˆ€x)x = a cannot be absolutely provable, for it is a â€œlieâ€. It fails
at least over N, if a is interpreted to be â€œ0â€.
(2) I.4.16 adds ï¬‚exibility to applications of the deduction theorem:
âŠ¢T (A â†’B )[x1,. . . , xn]
(âˆ—)
where [x1,. . . , xn] is the list of all free variables just in A, is equivalent
(by I.4.16) to
âŠ¢Tâ€² (A â†’B )[e1,. . . , en]
(âˆ—âˆ—)
where e1,. . . , en are new constants added to V (with no effect on nonlogical
axioms:  = â€²).
Now, since A[e1,. . . , en] is closed, proving
â€² + A[e1,. . . , en] âŠ¢B [e1,. . . , en]
establishes (âˆ—âˆ—), and hence also (âˆ—).
In practice, one does not perform this step explicitly, but ensures that,
throughout the  + A proof, whatever free variables were present in A â€œbe-
haved like constantsâ€, or, as we also say, were frozen.
(3) In some expositions the deduction theorem is not constrained by requiring
that A be closed (e.g., Bourbaki (1966b) and more recently Enderton (1972)).
Which version is right? Both are in their respective contexts. If all the rules
of inference are â€œpropositionalâ€ (e.g., as in Bourbaki (1966b) and Enderton
(1972), who only employ modus ponens) â€“ that is, they do not meddle with
quantiï¬ers â€“ then the deduction theorem is unconstrained. If, on the other hand,
the rules of inference manipulate object variables via quantiï¬cation, then one
cannot avoid constraining the application of the deduction theorem, lest one
want to derive (the invalid) âŠ¢A â†’(âˆ€x)A from the valid A âŠ¢(âˆ€x)A.
This also entails that approaches such as in Bourbaki (1966b) and Enderton
(1972) do not allow â€œfullâ€ generalization â€œA âŠ¢(âˆ€x)Aâ€. They only allow a
â€œweakerâ€ rule, â€œif âŠ¢A, then âŠ¢(âˆ€x)Aâ€.â€ 
(4) This divergence of approach in choosing rules of inference has some addi-
tional repercussions: One has to be careful in deï¬ning the semantic counterpart
â€  Indeed, they allow a bit more generally, namely, the rule â€œif  âŠ¢A with a side condition, then
 âŠ¢(âˆ€x)A. The side condition is that the formulas of  do not have free occurrences of x.â€ Of
course,  can be always taken to be ï¬nite (why?), so that this condition is not unrealistic.

50
I. Basic Logic
of âŠ¢, namely, |= (see next section). One wants the two symbols to â€œtrack each
otherâ€ faithfully (GÂ¨odelâ€™s completeness theorem).â€ 
I.4.21 Corollary (Proof by Contradiction). Let A be closed. Then  âŠ¢A
iff  + Â¬A is inconsistent.
Proof. If part: Given that T = Wff, where T
is the theory  + Â¬A. In par-
ticular,  + Â¬A âŠ¢A. By the deduction theorem,  âŠ¢Â¬A â†’A. But Â¬A â†’
A |=Taut A.
Only-if part: Given that  âŠ¢A. Hence  + Â¬A âŠ¢A as well (re-
call I.3.18(2)). Of course,  + Â¬A âŠ¢Â¬A. Since A, Â¬A |=Taut B for an
arbitrary B , we are done.
â–¡
Pause. Is it necessary to assume that A is closed in I.4.21? Why?
The following is important enough to merit stating. It follows from the type
of argument we employed in the only-if part above.
I.4.22 Metatheorem. T is inconsistent iff for some A, both âŠ¢T A and âŠ¢T Â¬A
hold.
We also list below a number of â€œquotableâ€ proof techniques. These tech-
niques are routinely used by mathematicians, and will be routinely used by us.
The proofs of all the following metatheorems are delegated to the reader.
I.4.23Metatheorem(DistributivityorMonotonicityofâˆƒ). Forany x, A, B ,
A â†’B âŠ¢(âˆƒx)A â†’(âˆƒx)B
Proof. See Exercise I.12.
â–¡
I.4.24Metatheorem(DistributivityorMonotonicityofâˆ€). Forany x, A, B ,
A â†’B âŠ¢(âˆ€x)A â†’(âˆ€x)B
Proof. See Exercise I.13.
â–¡
The term â€œmonotonicityâ€ is inspired by thinking of â€œâ†’â€ as â€œâ‰¤â€. How? Well,
we have the tautology
(A â†’B ) â†”(A âˆ¨B â†”B)
(i)
â€  In Mendelson (1987), |= is deï¬ned inconsistently with âŠ¢.

I.4. Basic Metatheorems
51
If we think of â€œAâˆ¨B â€ as â€œmax(A, B )â€, then the right hand side in (i) above
says that B is the maximum of A and B , or that A is â€œless than or equal toâ€
B . The above metatheorems say that both âˆƒand âˆ€preserve this â€œinequalityâ€.
I.4.25 Metatheorem (Equivalence Theorem, or Leibniz Rule). Let  âŠ¢
A â†”B , and let C â€² be obtained from C by replacing some â€“ possibly, but not
necessarily, all â€“ occurrences of a subformula A of C by B .
Then  âŠ¢C â†”C â€², i.e.,
A â†”B
C â†”C â€²
is a derived rule.
Proof. The proof is by induction on formulas C . See Exercise I.15.
â–¡
Equational or calculational predicate logic is a particular foundation of ï¬rst
order logic that uses the above Leibniz rule as the primary rule of inference. In
â€œpractisingâ€ such logic one prefers to write proofs as chains of equivalences.
Most equivalences in such a chain stem from an application of the rule. See
Dijkstra and Scholten (1990), Gries and Schneider (1994), Tourlakis (2000a,
2000b, 2001b).
I.4.26 Metatheorem (Proof by Cases). Suppose that  âŠ¢A1 âˆ¨Â· Â· Â· âˆ¨An,
and  âŠ¢Ai â†’B for i = 1, . . . . Then  âŠ¢B .
Proof. Immediate, by I.4.1.
â–¡
Proof by cases usually beneï¬ts from the application of the deduction theorem.
That is, having established  âŠ¢A1 âˆ¨Â· Â· Â· âˆ¨An, one then proceeds to adopt,
in turn, each Ai (i = 1,. . . , n) as a new nonlogical axiom (with its variables
â€œfrozenâ€). In each â€œcaseâ€ (Ai) one proceeds to prove B .
At the end of all this one has established  âŠ¢B .
In practice we normally use the following argot:
â€œWe will consider cases Ai, for i = 1,. . . , n.â€ 
Case A1.
. . . therefore, B .â€¡
Â· Â· Â·
Case An.
. . . therefore, B .â€
â€  To legitimize this splitting into cases, we must, of course, show  âŠ¢A1 âˆ¨Â· Â· Â· âˆ¨An.
â€¡ That is, we add the axiom A1 to , freezing its variables, and we then prove B .

52
I. Basic Logic
I.4.27 Metatheorem (Proof by Auxiliary Constant). Suppose that for arbi-
trary A and B over the language L we know
(1)  âŠ¢(âˆƒx)A[x]
(2)  + A[a] âŠ¢B , where a is a new constant not in the language L of .
Furthermore assume that in the proof of B all the free variables of A[a]
were frozen. Then  âŠ¢B .
Proof. Exercise I.21.
â–¡
The technique that ï¬‚ows from this metatheorem is used often in practice. For
example, in projective geometry axiomatized as in Veblen and Young (1916),
in order to prove Desarguesâ€™s theorem on perspective triangles on the plane, we
use some arbitrary point (this is the auxiliary constant!) off the plane, having
veriï¬ed that the axioms guarantee that such a point exists. It is important to
note that Desarguesâ€™s theorem does not refer to this point at all â€“ hence the term
â€œauxiliaryâ€.
In this example, from projective geometry, â€œB â€ is Desarguesâ€™s theorem,
â€œ(âˆƒx)A[x]â€ asserts that there are points outside the plane, a is an arbitrary such
point, and the proof (2) starts with words like â€œLet a be a point off the planeâ€ â€“
which is argot for â€œadd the axiom A[a]â€.
I.5. Semantics; Soundness, Completeness, Compactness
So what do all these symbols mean? We show in this section how to â€œdecodeâ€ the
formal statements (formulas) into informal statements of â€œrealâ€ mathematics.
Conversely, this will entail an understanding of how to code statements of real
mathematics in our formal language.
The rigorousâ€  deï¬nition of semantics for ï¬rst order languages is due to Tarski
and is often referred to as â€œTarski semanticsâ€. The ï¬‚avour of the particular
deï¬nition given below is that of Shoenï¬eld (1967), and it accurately reï¬‚ects
our syntactic choices â€“ most importantly, the choice to allow full generalization
A âŠ¢(âˆ€x)A.Inparticular,wewilldeï¬nethesemanticcounterpartofâŠ¢,namely,
|=, pronounced â€œlogically impliesâ€, to ensure that  âŠ¢A iff  |= A. This is
the content of GÂ¨odelâ€™s completeness theorem, which we prove in this section.
This section will place some additional demands on the readerâ€™s recollection
of notation and facts from informal set theory. We will, among other things,
â€  One often says â€œThe formal deï¬nition of semantics . . . â€, but the word â€œformalâ€ is misleading
here, for we are actually deï¬ning semantics in the metatheory, not in the formal theory.

I.5. Semantics; Soundness, Completeness, Compactness
53
make use of notation from naÂ¨Ä±ve set theory, such as
An
(or A Ã— Â· Â· Â· Ã— A



n times
)
for the set of ordered n-tuples of members of A.
We will also use the symbols âŠ†, âˆª, 
aâˆˆI.â€ 
In some passages â€“ delimited by
warning signs â€“ these demands will
border on the unreasonable.
For example, in the proof of the GÂ¨odel-Malâ€²cev completeness-compactness
result we will need some elementary understanding of ordinals â€“ used as index-
ing tools â€“ and cardinality. Some readers may not have such background. This
prerequisite material can be attained by consulting a set theory book (e.g., the
second volume of these lectures).
I.5.1 Deï¬nition. Given a language L = (V , Term, Wff), a structure M =
(M, I ) appropriate for L is such that M Ì¸= âˆ…is a set (the domain or underlying
set or universeâ€¡) and I (â€œI â€ for interpretation) is a mapping that assigns
(1) to each constant a of V
a unique member aI âˆˆM,
(2) to each function f of V
â€“ of arity n â€“ a unique (total)Â§ function f I :
Mn â†’M,
(3) to each predicate P of V â€“ of arity n â€“ a unique set PI âŠ†Mn.Â¶
â–¡
I.5.2 Remark. The structure M is often given more verbosely, in conformity
with practice in algebra. Namely, one â€œunpacksâ€ the I into a list aI , bI ,. . . ;
f I , gI ,. . . ; PI , QI , . . . and
writes
instead
M = (M; aI , bI ,. . . ; f I ,
gI ,. . . ; PI , QI , . . . ). Under this understanding, a structure is an underly-
ing set (universe), M, along with a list of â€œconcreteâ€ constants, functions, and
relations that â€œinterpretâ€ corresponding â€œabstractâ€ items of the language.
Under the latter notational circumstances we often use the symbols aM, f M,
PM â€“ rather than aI , f I , PI â€“ to indicate the interpretations in M of the
constant a, function f , and predicate P respectively.
â€  If we have a set of sets {Sa, Sb, Sc, . . . }, where the indices a, b, c, . . . all come out of an â€œindex
setâ€ I, then the symbol 
iâˆˆI Si stands for the collection of all those objects x that are found in
at least one of the sets Si. It is a common habit to write 
âˆ
i=0 Si instead of 
iâˆˆN Si. A âˆªB is
the same as 
iâˆˆ{1,2} Si, where we have let S1 = A and S2 = B.
â€¡ Often the qualiï¬cation â€œof discourseâ€ is added to the terms â€œdomainâ€ and â€œuniverseâ€.
Â§ Requiring f I to be total is a traditional convention. By the way, total means that f I is deï¬ned
everywhere on Mn.
Â¶ Thus PI is an n-ary relation with inputs and outputs in M.

54
I. Basic Logic
We have said above â€œstructure appropriate for Lâ€, thus emphasizing the gen-
erality of the language and therefore our ability to interpret what we say in it in
many different ways. Often though, e.g., as in formal arithmetic or set theory,
we have a structure in mind to begin with, and then build a formal language to
formally codify statements about the objects in the structure. Under these cir-
cumstances, in effect, we deï¬ne a language appropriate for the structure. We use
the symbol LM to indicate that the language was built to ï¬t the structure M. â–¡
I.5.3 Deï¬nition. We routinely add new nonlogical symbols to a language L
to obtain a language Lâ€². We say that Lâ€² is an extension of L and that L is
a restriction of Lâ€². Suppose that M = (M, I ) is a structure for L, and let
Mâ€² = (M, I â€²) be a structure with the same underlying set M, but with I
extended to I â€² so that the latter gives meaning to all new symbols while it gives
the same meaning, as I does, to the symbols of L.
We call Mâ€² an expansion (rather than â€œextensionâ€) of M, and M a reduct
(rather than â€œrestrictionâ€) of Mâ€². We may (often) write I = I â€² â†¾L to indicate
that the â€œmappingâ€ I â€² â€“ restricted to L (symbol â€œ â†¾â€) â€“ equals I . We may also
write M = Mâ€² â†¾L instead.
â–¡
I.5.4 Deï¬nition. Given L and a structure M = (M, I ) appropriate for L.
L(M) denotes the language obtained from L by adding to V
a unique new
name i for each object i âˆˆM.
This amends both sets Term, Wff into Term(M), Wff(M). Members of the
latter sets are called M-terms and M-formulas respectively.
We extend the mapping I to the new constants by: i
I = i for all i âˆˆM
(where the â€œ=â€ here is metamathematical: equality on M).
â–¡
All we have done here is to allow ourselves to do substitutions like [x â†i]
formally. We do, instead, [x â†i]. One next gives â€œmeaningâ€ to all closed
terms in L(M). The following uses deï¬nition by recursion (I.2.13) and relies
on the fact that the rules that deï¬ne terms are unambiguous.
I.5.5 Deï¬nition. For closed terms t in Term(M) we deï¬ne the symbol tI âˆˆM
inductively:
(1) If t is any of a (original constant) or i (imported constant), then tI has
already been deï¬ned.
(2) If t is the string f t1 . . . tn, where f is n-ary, and t1,. . . , tn are closed M-
terms, we deï¬ne tI to be the object (of M) f I (tI
1 ,. . . , tI
n ).
â–¡
Finally, we give meaning to all closed M-formulas, again by recursion (over
Wff).

I.5. Semantics; Soundness, Completeness, Compactness
55
I.5.6 Deï¬nition. For any closed formula A in Wff(M) we deï¬ne the symbol
AI inductively. In all cases, AI âˆˆ{t, f}.
(1) If A â‰¡t = s, where t and s are closed M-terms, then AI = t iff tI = sI .
(The last two occurrences of â€œ=â€ are metamathematical.)
(2) If A â‰¡Pt1 . . . tn, where P is an n-ary predicate and the ti are closed
M-terms, then AI = t iff âŸ¨tI
1 ,. . . , tI
n âŸ©âˆˆPI or PI (tI
1 ,. . . , tI
n ) â€œholdsâ€.
(Or â€œis trueâ€; see p. 19. Of course, the last occurrence of â€œ=â€ is metamathe-
matical.)
(3) If A is any of the sentences Â¬B , B âˆ¨C , then AI is determined by
the usual truth tables (see p. 30) using the values B I and C I . That is,
(Â¬B )I = FÂ¬(B I ) and (B âˆ¨C )I = Fâˆ¨(B I , C I ). (The last two oc-
currences of â€œ=â€ are metamathematical.)
(4) If A â‰¡(âˆƒx)B , then AI = t iff (B [x â†i])I = t for some i âˆˆM. (The
last two occurrences of â€œ=â€ are metamathematical.)
â–¡
We have â€œimportedâ€ constants from M into L in order to be able to state
the semantics of (âˆƒx)B above in the simple manner we just did (following
Shoenï¬eld (1967)).
We often state the semantics of (âˆƒx)B by writing
((âˆƒx)B [x])I is true
iff
(âˆƒi âˆˆM)(B [i])I is true
I.5.7 Deï¬nition. Let A âˆˆWff, and M be a structure as above.
An M-instance of A is an M-sentence A(i1,. . . , ik) (that is, all the free
variables of A have been replaced by imported constants).
We say that A is valid in M, or that M is a model of A, iff for all M-instances
Aâ€² of A it is the case that Aâ€²I = t.â€  Under these circumstances we write
|=M A.
For any set of formulas  from Wff, the expression |=M , pronounced â€œM
is a model of â€, means that for all A âˆˆ, |=M A.
A formula A is universally valid or logically valid (we often say just valid)
iff every structure appropriate for the language is a model of A.
Under these circumstances we simply write |= A.
If  is a set of formulas, then we say it is satisï¬able iff it has a model. It is
ï¬nitely satisï¬able iff every ï¬nite subset of  has a model.â€¡
â–¡
The deï¬nition of validity of A in a structure M corresponds with the normal
mathematical practice. It says that a formula is true (in a given â€œcontextâ€ M)
just in case it is so for all possible values of the free variables.
â€  We henceforth discontinue our pedantic â€œ(The last occurrence of â€œ=â€ is metamathematical.)â€.
â€¡ These two concepts are often deï¬ned just for sentences.

56
I. Basic Logic
I.5.8 Deï¬nition. We say that  logically implies A, in symbols  |= A,
meaning that every model of  is also a model of A.
â–¡
I.5.9 Deï¬nition (Soundness). A theory (identiï¬ed by its nonlogical axioms)
 is sound iff, for all A âˆˆWff,  âŠ¢A implies  |= A, that is, iff all the
theorems of the theory are logically implied by the nonlogical axioms.
â–¡
Clearly then, a pure theory T is sound iff âŠ¢T A implies |= A for all A âˆˆWff.
That is, all its theorems are universally valid.
Towards the soundness resultâ€  below we look at two tedious (but easy)
lemmata.
I.5.10 Lemma. Given a term t, variables x Ì¸â‰¡y, where y does not occur in t,
and a constant a. Then, for any term s and formula A, s[x â†t][y â†a] â‰¡
s[y â†a][x â†t] and A[x â†t][y â†a] â‰¡A[y â†a][x â†t].
Proof. Induction on s:
Basis:
s[x â†t][y â†a] â‰¡
ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
if s â‰¡x
then t
if s â‰¡y
then a
if s â‰¡z
where x Ì¸â‰¡z Ì¸â‰¡y, then z
if s â‰¡b
then b
â‰¡s[y â†a][x â†t]
For the induction step let s â‰¡f r1 . . .rn, where f has arity n. Then
s[x â†t][y â†a] â‰¡f r1[x â†t][y â†a] . . .rn[x â†t][y â†a]
â‰¡f r1[y â†a][x â†t] . . .rn[y â†a][x â†t]
by I.H.
â‰¡s[y â†a][x â†t]
Induction on A:
Basis:
A[x â†t][y â†a]
â‰¡
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
if A
â‰¡Pr1 . . .rn then
Pr1[x â†t][y â†a] . . .rn[x â†t][y â†a]
â‰¡Pr1[y â†a][x â†t] . . .rn[y â†a][x â†t]
if A
â‰¡r = s then
r[x â†t][y â†a] = s[x â†t][y â†a]
â‰¡r[y â†a][x â†t] = s[y â†a][x â†t]
â‰¡A[y â†a][x â†t]
â€  Also nicknamed â€œthe easy half of GÂ¨odelâ€™s completeness theoremâ€.

I.5. Semantics; Soundness, Completeness, Compactness
57
The property we are proving, trivially, propagates with Boolean connectives.
Let us do the induction step just in the case where A â‰¡(âˆƒw)B . If w â‰¡x or
w â‰¡y, then the result is trivial. Otherwise,
A[x â†t][y â†a] â‰¡((âˆƒw)B )[x â†t][y â†a]
â‰¡((âˆƒw)B [x â†t][y â†a])
â‰¡((âˆƒw)B [y â†a][x â†t])
by I.H.
â‰¡((âˆƒw)B )[y â†a][x â†t]
â‰¡A[y â†a][x â†t]
â–¡
I.5.11 Lemma. Given a structure M = (M, I ), a term s, and a formula A,
both over L(M). Suppose each of s and A have at most one free variable, x.
Let t be a closed term over L(M) such that tI = i âˆˆM. Then (s[x â†
t])I = (s[x â†i])I and (A[x â†t])I = (A[x â†i])I . Of course, since t is
closed, A[x â†t] is deï¬ned.
Proof. Induction on s:
Basis. s[x â†t] â‰¡s if s âˆˆ{y, a, j} (y Ì¸â‰¡x). Hence (s[x â†t])I = sI =
(s[x â†i])I in this case. If s â‰¡x, then s[x â†t] â‰¡t and s[x â†i] â‰¡i, and
the claim follows once more.
For the induction step let s â‰¡f r1 . . .rn, where f has arity n. Then
(s[x â†t])I = f I ((r1[x â†t])I ,. . . , (rn[x â†t])I )
= f I ((r1[x â†i])I ,. . . , (rn[x â†i])I )
by I.H.
= (s[x â†i])I
Induction on A:
Basis. If A â‰¡Pr1 . . .rn, thenâ€ 
(A[x â†t])I = PI ((r1[x â†t])I ,. . . , (rn[x â†t])I )
= PI ((r1[x â†i])I ,. . . , (rn[x â†i])I )
= (A[x â†i])I
Similarly if A â‰¡r = s.
The property we are proving, clearly, propagates with Boolean connectives.
Let us do the induction step just in the case where A = (âˆƒw)B . If w â‰¡x, the
result is trivial. Otherwise, we note that â€“ since t is closed â€“ w does not occur
â€  For a metamathematical relation Q, as is usual (p. 19), Q(a, b, . . . ) = t, or just Q(a, b, . . . ),
stands for âŸ¨a, b, . . . âŸ©âˆˆQ.

58
I. Basic Logic
in t, and proceed as follows:
(A[x â†t])I = t iff (((âˆƒw)B )[x â†t])I = t
iff (((âˆƒw)B [x â†t]))I = t
iff (B [x â†t][w â†j])I = t for some j âˆˆM, by I.5.6(4)
iff (B [w â†j][x â†t])I = t for some j âˆˆM, by I.5.10
iff ((B [w â†j])[x â†t])I = t for some j âˆˆM
iff ((B [w â†j])[x â†i])I = t for some j âˆˆM, by I.H.
iff (B [w â†j][x â†i])I = t for some j âˆˆM
iff (B [x â†i][w â†j])I = t for some j âˆˆM, by I.5.10
iff (((âˆƒw)B [x â†i]))I = t by I.5.6(4)
iff (((âˆƒw)B )[x â†i])I = t
iff (A[x â†i])I = t
â–¡
I.5.12 Metatheorem (Soundness). Any ï¬rst order theory (identiï¬ed by its non-
logical axioms) , over some language L, is sound.
Proof. By induction on -theorems, A, we prove that  |= A. That is, we ï¬x
a structure for L, say M, and assume that |=M . We then proceed to show that
|=M A.
Basis. A is a nonlogical axiom. Then our conclusion is part of the assump-
tion, by I.5.7.
Or A is a logical axiom. There are a number of cases:
Case 1. |=Taut A. We ï¬x an M-instance of A, say Aâ€², and show that
Aâ€²I = t. Let p1,. . . , pn be all the propositional variables (alias prime
formulas) occurring in Aâ€². Deï¬ne a valuation v by setting v(pi) = pI
i
for i = 1,. . . , n. Clearly, t = Â¯v(Aâ€²) = Aâ€²I (the ï¬rst â€œ=â€ because
|=Taut Aâ€², the second because after prime formulas were taken care of,
all that remains to be done for the evaluation of Aâ€²I is to apply Boolean
connectives â€“ see I.5.6(3)).
Pause. Why is |=Taut Aâ€²?
Case 2. A â‰¡B [t] â†’(âˆƒx)B . Again, we look at an M-instance B â€²[tâ€²] â†’
(âˆƒx)B â€². We want (B â€²[tâ€²] â†’(âˆƒx)B â€²)I = t, but suppose instead that
(B â€²[tâ€²])I = t
(1)
and
((âˆƒx)B â€²)I = f
(2)
Let tâ€²I = i (i âˆˆM). By I.5.11 and (1), (B â€²[i])I = t. By I.5.6(4),
((âˆƒx)B â€²)I = t, contradicting (2).

I.5. Semantics; Soundness, Completeness, Compactness
59
Case 3. A â‰¡x = x. Then an arbitrary M-instance is i = i for some i âˆˆM.
By I.5.6(1), (i = i)I = t.
Case 4. A â‰¡t = s â†’(B [t] â†”B [s]). Once more, we take an arbitrary
M-instance, tâ€² = sâ€² â†’(B â€²[tâ€²] â†”B â€²[sâ€²]). Suppose that (tâ€² = sâ€²)I = t.
That is, tâ€²I = sâ€²I = (let us say) i (in M). But then
(B â€²[tâ€²])I = (B â€²[i])I
by I.5.11
= (B â€²[sâ€²])I
by I.5.11
Hence (B [t] â†”B [s])I = t.
For the induction step we have two cases:
Modus ponens. Let B and B â†’A be -theorems. Fix an M-instance
B â€² â†’Aâ€². Since B â€², B â€² â†’Aâ€² |=Taut Aâ€², the argument here is entirely
analogous to the case A âˆˆ (hence we omit it).
âˆƒ-introduction. Let A â‰¡(âˆƒx)B â†’C and  âŠ¢B â†’C , where x is not
free in C . By the I.H.
|=M B â†’C
(3)
Let (âˆƒx)B â€² â†’C â€² be an M-instance such that (despite expectations)
((âˆƒx)B â€²)I = t but
C â€²I = f
(4)
Thus
B â€²[i]
I = t
(5)
for some i âˆˆM. Since x is not free in C , B â€²[i] â†’C â€² is a false (by (4) and (5))
M-instance of B â†’C , contradicting (3).
â–¡
We used the condition of âˆƒ-introduction above, by saying â€œSince x is not free
in C , B â€²[i] â†’C â€² is a(n) . . . M-instance of B â†’C â€.
So the condition was useful. But is it essential? Yes, since, for example, if
x Ì¸â‰¡y, then x = y â†’x = y Ì¸|= (âˆƒx)x = y â†’x = y.
As a corollary of soundness we have the consistency of pure theories:
I.5.13 Corollary. Any ï¬rst order pure theory is consistent.
Proof. Let T be a pure theory over some language L. Since Ì¸|= Â¬x = x, it
follows that Ì¸âŠ¢T Â¬x = x, thus T Ì¸= Wff.
â–¡

60
I. Basic Logic
I.5.14 Corollary. Any ï¬rst order theory that has a model is consistent.
Proof. Let T be a ï¬rst theory over some language L, and M a model of T.
Since Ì¸|=M Â¬x = x, it follows that Ì¸âŠ¢T Â¬x = x, thus T Ì¸= Wff.
â–¡
First order deï¬nability in a structure: We are now in a position to make the
process of translation to and from informal mathematics rigorous.
I.5.15 Deï¬nition. Let L be a ï¬rst order language, and M a structure for L. A
set (synonymously, relation) S âŠ†Mn is (ï¬rst order) deï¬nable in M over L
iff for some formula S (y1,. . . , yn) (see p. 18 for a reminder on round-bracket
notation) and for all i j, j = 1,. . . , n, in M,
âŸ¨i1,. . . , inâŸ©âˆˆS
iff
|=M S (i1,. . . , in)
We often just say â€œdeï¬nable in Mâ€.
A function f : Mn â†’M is deï¬nable in M over L iff the relation y =
f (x1,. . . , xn) is so deï¬nable.
â–¡
N.B. Some authors say â€œ(ï¬rst order) expressibleâ€ (Smullyan (1992)) rather
than â€œ(ï¬rst order) deï¬nableâ€ in a structure.
In the context of (M), the above deï¬nition gives precision to statements such
as â€œwe code (or translate) an informal statement into the formal languageâ€ or
â€œthe (formal language) formula A informally â€˜saysâ€™ . . . â€, since any (informal)
â€œstatementâ€ (or relation) that depends on the informal variables x1,. . . , xn has
the form â€œâŸ¨x1,. . . , xnâŸ©âˆˆSâ€ for some (informal) set S. It also captures the
essence of the statement.
â€œThe (informal) statement âŸ¨x1,. . . , xnâŸ©âˆˆS can be written (or made) in the
formal language.â€
What â€œmakesâ€ the statement, in the formal language, is the formula S
that ï¬rst order deï¬nes it.
I.5.16 Example. The informal statement â€œz is a primeâ€ has a formal translation
S0 < z âˆ§(âˆ€x)(âˆ€y)(z = x Ã— y â†’x = z âˆ¨x = S0)
over the language of elementary number theory, where the nonlogical symbols
are 0, S, +, Ã—, < and the deï¬nition (translation) is effected in the standard
structure N = (N; 0; S, +, Ã—; <), where â€œSâ€ satisï¬es, for all n âˆˆN, S(n) =
n + 1 and interprets â€œSâ€ (see I.5.2, p. 53, for the â€œunpackedâ€ notation we have
just used to denote the structure N). We have used the variable name â€œzâ€ both
formally and informally.
â–¡

I.5. Semantics; Soundness, Completeness, Compactness
61
It must be said that translation is not just an art or skill. There are theoretical
limitations to translation. The trivial limitation is that if M is an inï¬nite set and,
say, L has a ï¬nite set of nonlogical symbols (as is the case in number theory
and set theory), then we cannot deï¬ne all S âŠ†M, simply because we do not
have enough ï¬rst order formulas to do so.
There are non-trivial limitations too. Some sets are not ï¬rst order deï¬nable
because they are â€œfar too complexâ€. See Section I.9.
This is a good place to introduce a common notational argot that allows us to
write â€œmixed-modeâ€ formulas that have a formal part (over some language L)
but may contain â€œinformalâ€ constants (names of, to be sure, but names that have
not formally been imported into L) from some structure M appropriate for L.
I.5.17 Informal Deï¬nition. Let L be a ï¬rst order language and M = (M, I ) a
structure for L. Let A be a formula with at most x1,. . . , xn free, and i1,. . . , in
members of M. The notation A [[i1,. . . , in]] is an abbreviation of (A[i1,. . . ,
in])I .
â–¡
This argot allows one to substitute informal objects into variables outright,
by-passing the procedure of importing formal names for such objects into the
language. It is noteworthy that mixed mode formulas can be deï¬ned directly by
induction on formulas â€“ that is, without forming L(M) ï¬rst â€“ as follows:
Let L and M be as above. Let x1,. . . , xn contain all the free variables that
appear in a term t or formula A over L (not over L(M)!). Let i1,. . . , in be
arbitrary in M.
For terms we deï¬ne
t[[i1, . . . in]] =
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³
i j
if t â‰¡xj (1 â‰¤j â‰¤n)
aI
if t â‰¡a
f I (t1[[i1,. . . , in]],. . . ,
tr[[i1,. . . , in]])
if t â‰¡f t1 . . . tr
For formulas we let
A[[i1, . . . in]] =
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
t[[i1, . . . in]] = s[[i1, . . . in]]
if A â‰¡t = s
PI (t1[[i1,. . . , in]],. . . ,
tr[[i1,. . . , in]])
if A â‰¡Pt1 . . . tr
Â¬(B [[i1, . . . in]])
if A â‰¡Â¬B
(B [[i1, . . . in]] âˆ¨C [[i1, . . . in]])
if A â‰¡B âˆ¨C
(âˆƒa âˆˆM)B [[a, i1, . . . in]]
if A â‰¡(âˆƒz)B [z, âƒ—xn]

62
I. Basic Logic
where â€˜(âˆƒa âˆˆM) . . . â€ is short for â€œ(âˆƒa)(a âˆˆM âˆ§. . . )â€. The right hand side
has no free (informal) variables; thus it evaluates to one of t or f.
We now turn to the â€œhard halfâ€ of GÂ¨odelâ€™s completeness theorem, that our
syntactic proof apparatus can faithfully mimic â€œproofs by logical implicationâ€.
That is, the syntactic apparatus is â€œcompleteâ€.
I.5.18 Deï¬nition. A theory over L (designated by its nonlogical axioms)  is
semantically complete iff  |= A implies  âŠ¢A for any formula A.
â–¡
The term â€œsemantically completeâ€ is not being used much. There is a competing
syntactic notion of completeness, that of simple completeness, also called just
completeness. The latter is the notion one has normally in mind when saying
â€œ. . . a complete theory . . . â€. More on this shortly.
We show the semantic completeness of every ï¬rst order theory by proving,
using the technique of Henkin (1952), the consistency theorem below. The
completeness theorem will then be derived as a corollary.
I.5.19 Metatheorem (Consistency Theorem). If a (ï¬rst order) theory T is
consistent, then it has a model.
We will ï¬rst give a proof (via a sequence of lemmata) for the case of â€œcount-
able languagesâ€ L, that is, languages that have a countable alphabet. We will
then amend the proof to include the uncountable case.
A crash course on countable sets: A set A is countableâ€  if it is empty or (in
the opposite case) if there is a way to arrange all its members in an inï¬nite
sequence, in a â€œrow of locationsâ€, utilizing one location for each member of N.
It is allowed to repeatedly list any element of A, so that ï¬nite sets are countable.
Technically, this enumeration is a (total) function f : N â†’A whose range
(set of outputs) equals A (that is, f is onto). We say that f (n) is the nth element
of A in the enumeration f . We often write fn instead of f (n) and then call n a
â€œsubscriptâ€ or â€œindexâ€.
We can convert a multi-row enumeration
( fi, j)i, j in N
â€  NaÂ¨Ä±vely speaking. The deï¬nition is similar to the formal one that is given in volume 2. Here we
are just offering a quick-review service â€“ in the metamathematical domain, just in case the reader
needs it â€“ in preparation for the proof of the consistency theorem.

I.5. Semantics; Soundness, Completeness, Compactness
63
into a single row enumeration quite easily. Technically, we say that the set N Ã—
N â€“ the set of â€œdouble subscriptsâ€ (i, j) â€“ is countable. This is shown diagram-
matically below. The â€œlinearizationâ€ or â€œunfoldingâ€ of the inï¬nite matrix of
rows is effected by walking along the arrows:
(0, 0)
(0, 1)
(0, 2)
(0, 3)
. . .
â†—
â†—
â†—
(1, 0)
(1, 1)
(1, 2)
â†—
â†—
(2, 0)
(2, 1)
â†—
(3, 0)
...
This observation yields a very useful fact regarding strings over countable sets
(alphabets): If V
is countable, then the set of all strings of length 2 over V
is also countable. Why? Because the arbitrary string of length 2 is of the form
did j where di and d j represent the ith and j elements of the enumeration of V
respectively. Unfolding the inï¬nite matrix exactly as above, we get a single-row
enumeration of these strings.
By induction on the length n â‰¥2 of strings we see that the set of strings
of any length n â‰¥2 is also countable. Indeed, a string of length n + 1 is a
string ab, where a has length n and b âˆˆV . By the I.H. the set of all aâ€™s can be
arranged in a single row (countable), and we are done exactly as in the case of
the did j above.
Finally, let us collect all the strings over V
into a set S. Is S countable? Yes.
We can arrange S, at ï¬rst, into an inï¬nite matrix of strings mi, j, that is, the jth
string of length i. Then we employ our matrix-unfolding trick above.
Suppose now that we start with a countable set A. Is every subset of A
countable? Yes. If B âŠ†A, then the elements of B form a subsequence of the
elements of A (in any given enumeration). Therefore, just drop the members of
A that are not in B, and compact the subscripts.â€ 
To prove the consistency theorem let us ï¬x a countable language L and a
ï¬rst order theory T over L with nonlogical axioms . In the search for a model,
â€  By â€œcompact the subscriptsâ€ we mean this: After dropping members of A that do not be-
long to B, the enumeration has gaps in general. For example, B might be the subsequence
a13, a95, a96, a97, a1001, . . . . We just shift the members of B to the left, eliminating gaps, so
that the above few listed members take the locations (subscripts), 0, 1, 2, 3, 4, . . . , respectively.
Admittedly, this explanation is not precise, but it will have to do for now.

64
I. Basic Logic
we start with a simple countable set, here we will take N itself, and endow it
with enough structure (the I -part) to obtain a model, M = (M, I ) of T.
Since, in particular, this will entail that a subset of N (called M in what
follows) will be the domain of the structure, we start by importing all the
constants n âˆˆN into L. That is, we add to V
a new constant symbol n for
each n âˆˆN. The new alphabet is denoted by V (N), and the resulting language
L(N).
I.5.20 Deï¬nition. In general, let L = (V , Term, Wff) be a ï¬rst order lan-
guage and M some set. We add to V
a new constant symbol i for each
i âˆˆM. The new alphabet is denoted by V (M), and the new language by
L(M) = (V (M), Term(M), Wff(M)).
This concept originated with Henkin and Abraham Robinson. The aug-
mented language L(M) is called the diagram language of M.
â–¡
The above deï¬nition generalizes Deï¬nition I.5.4 and is useful when (as
happens in our current context) we have a language L and a set (here N), but
not, as yet, a structure for L with domain N (or some subset thereof).
Of course, if M = (M, I ), then L(M) = L(M).
Two observations are immediate: One,  has not been affected by the addi-
tion of the new constants, and, two, L(N) is still countable.â€  Thus, there are
enumerations
F 0, F 1, F 2, . . . of all sentences in Wff(N)
(1)
and
G 1, G 2, G 3, . . . of all sentences in Wff(N) of the form (âˆƒx)A
(2)
where, in (2), every sentence (âˆƒx)A of Wff(N) is listed inï¬nitely often.
Pause. How can we do this? Form an inï¬nite matrix, each row of which is the
same ï¬xed enumeration of the (âˆƒx)A sentences. Then unfold the matrix into a
single row.
With the preliminaries out of the way, we next deï¬ne by induction (or re-
cursion) over N an inï¬nite sequence of theories, successively adding sentences
over L(N) as new nonlogical axioms: We set 0 = . For any n â‰¥0, we deï¬ne
â€  If A and B are countable, then so is A âˆªB, since we can arrange the union as an inï¬nite matrix,
the 0th row being occupied by A-members while all the other rows being identical to some ï¬xed
enumeration of B.

I.5. Semantics; Soundness, Completeness, Compactness
65
n+1 in two stages: We ï¬rst let
n =
n âˆª{F n}
if n Ì¸âŠ¢Â¬F n
n
otherwise
(3)
Then, we set
n+1 =
n âˆª{A[xâ†i]}
if n âŠ¢G n+1, where G n+1 â‰¡(âˆƒx)A
n
otherwise
The choice of i is important: It is the smallest i such that the constant i does
not occur (as a substring) in any of the sentences
F 0,. . . , F n, G 1,. . . , G n+1
(4)
The sentence A[xâ†i] added to n+1 is called a special Henkin axiom.â€  The
constant i is the associated Henkin (also called witnessing) constant.
We now set
âˆ=

nâˆˆN
n
(5)
This is a set of formulas over Wff(N) that deï¬nes a theory Tâˆover Wff(N) (as
the set of nonlogical axioms of Tâˆ).
I.5.21 Lemma. The theory Tâˆis consistent.
Proof. It sufï¬ces to show that each of the theories n is consistent.
(Indeed, if âŠ¢TâˆÂ¬x = x,â€¡ then A1,. . . , Am âŠ¢Â¬x = x, for some Ai(i =
1,. . . , m) in âˆ, since proofs have ï¬nite length. Let n include all of
A1,. . . , Am.
Pause. Is there such a n?
Then n will be inconsistent.)
On to the main task. We know (assumption) that 0 is consistent. We take
the I.H. that n is consistent, and consider n+1 next.
First, we argue that n is consistent. If n = n, then we are done by the
I.H. If n = n âˆª{F n}, then inconsistency would entail (by I.4.21)  âŠ¢Â¬F n,
contradicting the hypothesis of the case we are at (top case of (3) above).
â€  Another possible choice for Henkin axiom is (âˆƒx)A â†’A[xâ†i].
â€¡ Â¬x = x is our favourite â€œcontradiction builderâ€. See also I.4.22.

66
I. Basic Logic
Next, we show that n âˆª{A[xâ†i]} is consistent, where A[xâ†i] is the
added Henkin axiom â€“ if indeed such an axiom was added.
Suppose instead that n âˆª{A[xâ†i]} âŠ¢Â¬z = z, for some variable z. Now
i does not occur in any formulas in the set n âˆª{(âˆƒx)A, Â¬z = z}.
Since (âˆƒx)A â‰¡G n+1 and n âŠ¢G n+1, we get n âŠ¢Â¬z = z by I.4.27
(auxiliary constant metatheorem). This is no good, since n is supposed to be
consistent.
â–¡
I.5.22 Deï¬nition. A theory T over L decides a sentence A iff one of âŠ¢T A
or âŠ¢T Â¬A holds. We say that A is decidable by T. In the case that âŠ¢T Â¬A
holds, we say that T refutes A. T is simply complete, or just complete, iff every
sentence is decidable by T.
â–¡
The deï¬nition is often offered in terms of consistent theories, since an incon-
sistent theory decides every formula anyway.
I.5.23 Lemma. Tâˆis simply complete.
Proof. LetAbeasentence.ThenA â‰¡F n forsomen.Ifn âŠ¢Â¬F n, then we are
done. If however n does not refute F n, then we are done by (3) (p. 65).
â–¡
I.5.24 Lemma. Tâˆhas the witness property,â€  namely, whenever âŠ¢Tâˆ(âˆƒx)A,
where (âˆƒx)A is a sentence over L(N), then for some m âˆˆN we have âŠ¢Tâˆ
A[x â†m].
Proof. Since the proof âŠ¢Tâˆ(âˆƒx)A involves ï¬nitely many formulas from âˆ,
there is an n â‰¥0 such that n âŠ¢(âˆƒx)A. Now, (âˆƒx)A â‰¡G k+1 for some
k â‰¥n, since (âˆƒx)A occurs in the sequence (2) (p. 64) inï¬nitely often. But then
k âŠ¢(âˆƒx)A as well.
Pause. Why?
Hence, A[x â†m] is added to k+1 as a Henkin axiom, for an appropriate
Henkin constant m, and we are done.
â–¡
I.5.25 Deï¬nition. We next deï¬ne a relation, âˆ¼, on N by
n âˆ¼m
iff
âŠ¢Tâˆn = m
(6)
â–¡
â€  We also say that it is a Henkin theory.

I.5. Semantics; Soundness, Completeness, Compactness
67
âˆ¼has the following properties:
(a) Reï¬‚exivity. n âˆ¼n (all n): By âŠ¢x = x and I.4.12.
(b) Symmetry. If n âˆ¼m, then m âˆ¼n (all m, n): It follows from âŠ¢x = y â†’
y = x (Exercise I.16) and I.4.12.
(c) Transitivity. If n âˆ¼m and m âˆ¼k, then n âˆ¼k (all m, n, k): It follows from
âŠ¢x = y â†’y = z â†’x = z (Exercise I.17) and I.4.12.
Let us deï¬ne a function f : N â†’N by
f (n) = smallest m such that m âˆ¼n
(7)
By (a) above, f is totally deï¬ned. We also deï¬ne a subset M of N, by
M = { f (n) : n âˆˆN}
(8)
This M will be the domain of a structure that we will build, and show that it is
a model of the original T.
First, we modify âˆâ€œdownwardsâ€.
I.5.26 Deï¬nition. The M-restriction of a formula A is the formula AM ob-
tained from A by replacing by f (n) every occurrence of an n in A.
â–¡
We now let
M
âˆ= {AM : A âˆˆâˆ}
(9)
We have the following results regarding M
âˆ(or the associated theory TM
âˆ):
I.5.27 Remark. Before proceeding, we note that the language of TM
âˆis L(M),
and that A â‰¡AM if A is over L(M), since f (m) = m for m âˆˆM (why?). â–¡
I.5.28 Lemma. Let A be over L(N). If âŠ¢TâˆA, then âŠ¢TM
âˆAM.
Proof. Induction on Tâˆ-theorems.
Basis. If A âˆˆâˆ, then AM âˆˆM
âˆby (9). If A âˆˆ, then AM âˆˆ (why?)
Modus ponens. Let âˆâŠ¢B and âˆâŠ¢B â†’A. By the I.H., M
âˆâŠ¢B M
and M
âˆâŠ¢B M â†’AM, and we are done.
âˆƒ-introduction. Let âˆâŠ¢B â†’C , where x is not free in C
and
A â‰¡(âˆƒx)B â†’C . By the I.H., M
âˆâŠ¢B M â†’C M, and we are done by
âˆƒ-introduction.
â–¡

68
I. Basic Logic
I.5.29 Lemma. TM
âˆâ‰¤Tâˆ, conservatively.
Proof. Leaving the â€œconservativelyâ€ part aside for a moment, let us verify that
for any A over L(N)
âˆâŠ¢AM â†”A
(âˆ—)
This follows from âˆâŠ¢n = f (n) (recall that n âˆ¼f (n) by deï¬nition of f ) for
all n âˆˆN, and Exercise I.18.
Because of (âˆ—), âˆcan prove any B âˆˆM
âˆ. Indeed, let B â‰¡AM for some
A âˆˆâˆ(by (9)). By (âˆ—) and âˆâŠ¢A, we obtain âˆâŠ¢B .
Thus, TM
âˆâ‰¤Tâˆ.
Pause. Do you believe this conclusion?
Turning to the â€œconservativelyâ€ part, this follows from Lemma I.5.28 and
Remark I.5.27.
â–¡
I.5.30 Corollary. TM
âˆis consistent.
Proof. If it can prove Â¬x = x, then so can Tâˆ.
â–¡
I.5.31 Lemma. TM
âˆis simply complete.
Proof. Let the sentence A be over L(M). It is decidable by Tâˆ(I.5.23).
By I.5.28, TM
âˆdecides AM. But thatâ€™s A.
â–¡
I.5.32 Lemma. TM
âˆis a Henkin theory over L(M).
Proof. Let M
âˆâŠ¢(âˆƒx)A, where (âˆƒx)A is a sentence. Then âˆâŠ¢(âˆƒx)A as
well, hence âˆâŠ¢A[xâ†n], for some n âˆˆN (by I.5.24).
It follows that M
âˆâŠ¢AM[xâ†f (n)], and we are done, since A â‰¡AM and
f (n) âˆˆM.
â–¡
I.5.33 Lemma. TM
âˆdistinguishes the constants of M, that is, if n Ì¸= m (both
in M), then âŠ¢TM
âˆÂ¬n = m.
Proof. By I.5.31, if âŠ¢TM
âˆÂ¬n = m fails, then âŠ¢TM
âˆn = m; hence n âˆ¼m. By
the deï¬nition of f (n) and M (p. 67), it follows that n = m (since each set
{m âˆˆN : m âˆ¼n} determined by n can have exactly one smallest element, and
any two distinct such sets â€“ determined by n and k â€“ have no common elements
(why?)). This contradicts the assumption that n Ì¸= m.
â–¡

I.5. Semantics; Soundness, Completeness, Compactness
69
We have started with a consistent theory T over L. We now have a consistent,
complete, Henkin extension TM
âˆof T, over the language L(M) (M âŠ†N), which
distinguishes the constants of M.
We are now ready to deï¬ne our model M = (M, I ). We are pleased to note
that the constants of M are already imported into the language, as required by
Deï¬nitions I.5.5 and I.5.6.
For any predicateâ€  P of L of arity k, we deï¬ne for arbitrary n1,. . . , nk in M
PI (n1,. . . , nk) = t
iff
M
âˆâŠ¢Pn1,. . . , nk
(A)
that is, we are deï¬ning the set of k-tuples (relation) PI by
PI =

âŸ¨n1,. . . , nkâŸ©: M
âˆâŠ¢Pn1,. . . , nk

(Aâ€²)
Let next f be a function letter of arity k, and let n1,. . . , nk be an input for f I .
What is the appropriate output?â€¡
Well, ï¬rst observe that M
âˆâŠ¢(âˆƒx) f n1,. . . , nk = x (why?). By I.5.32, there
is an m âˆˆM such that
M
âˆâŠ¢f n1,. . . , nk = m
(B)
We need this m to be unique. It is so, for if also M
âˆâŠ¢f n1,. . . , nk = j, then
(Exercise I.17) M
âˆâŠ¢m = j, and thus m = j (if m Ì¸= j, then also M
âˆâŠ¢Â¬m =
j, by I.5.33 â€“ impossible, since M
âˆis consistent).
For the input n1,. . . , nk we set
f I (n1,. . . , nk) = m
(B.1)
where m is uniquely determined by (B). This deï¬nes f I .
The case of constants is an interesting special case.Â§ As above, we let aI be
the unique m âˆˆM such that
M
âˆâŠ¢a = m
(C)
The interesting, indeed crucial, observation (required by I.5.5) is that, for any
imported constant m, we have mI â‰¡m. Indeed, this follows from uniqueness
and the trivial fact M
âˆâŠ¢m = m.
â€  Recall that we have added no new predicates and no new functions in going from L to L(M).
We have just added constants.
â€¡ We have yet to determine f I . Here we are just â€œthinking out loudâ€ towards suggesting a
good f I .
Â§ Actually, it is not a special case for us, since we did not allow 0-ary functions. But some au-
thors do.

70
I. Basic Logic
The following will be handy in the proof of the Main Lemma below:
M
âˆâŠ¢t = tI
(D)
for any closed term t over L(M). We prove (D) by induction on terms t.
Basis. If t â‰¡a (a original or imported), then (C) reads M
âˆâŠ¢a = aI .
Lett â‰¡f t1 . . . tk.ByI.5.5,tI = f I (tI
1 ,. . . , tI
k ).Setni = tI
i fori = 1,. . . , k.
Then tI = f I (n1,. . . , nk). By (B) and (B.1),
M
âˆâŠ¢f n1,. . . , nk = f I (n1,. . . , nk)
By the I.H., M
âˆâŠ¢ti = tI
i , in other words M
âˆâŠ¢ti = ni, for i = 1,. . . , k. By
Ax4, we obtain (via Exercise I.19)
M
âˆâŠ¢f t1,. . . , tk = f n1,. . . , nk
Thus (Exercise I.17),
M
âˆâŠ¢f t1,. . . , tk = f I 
tI
1 ,. . . , tI
k

This concludes the proof of (D).
I.5.34 Main Lemma. For every sentence A over L(M), AI = t iff M
âˆâŠ¢A.
Proof. This is proved by induction on formulas.
Basis. Case where A â‰¡Pt1 . . . tk: Let ni = tI
i
(i = 1,. . . , k). By (A)
above (p. 69), PI (n1,. . . , nk) = t iff M
âˆâŠ¢Pn1 . . . nk iff M
âˆâŠ¢Pt1 . . . tk â€“
the second â€œiffâ€ by Ax4 (via Exercise I.19) and (D). We are done in this case,
since AI = PI (tI
1 . . . tI
k ).
Case where A â‰¡t = s: Let also n = tI and m = sI . Then AI = t iff
tI = sI iff n = m iff M
âˆâŠ¢n = m (for the last â€œiffâ€ the if part follows by
consistency and I.5.33; the only-if part by Ax3 and I.4.12).
The induction steps. Let A â‰¡Â¬B . Then AI = t iff B I = f iff (I.H.)
M
âˆÌ¸âŠ¢B iff (completeness I.5.31 (â†’) and consistency I.5.30 (â†)) M
âˆâŠ¢A.
Let A â‰¡B âˆ¨C . Consider ï¬rst B I âˆ¨C I = t. Say B I = t. By I.H.,
M
âˆâŠ¢B ;henceM
âˆâŠ¢B âˆ¨C bytautologicalimplication.SimilarlyifC I = t.
Conversely, let M
âˆâŠ¢B âˆ¨C . Then one of M
âˆâŠ¢B or M
âˆâŠ¢C must be the
case: If neither holds, then M
âˆâŠ¢Â¬B and M
âˆâŠ¢Â¬C by completeness, hence
M
âˆis inconsistent (why?).
The ï¬nal case: A â‰¡(âˆƒx)B . Let ((âˆƒx)B )I = t. Then (B [xâ†n])I = t
for some n âˆˆM. By I.H., M
âˆâŠ¢B [xâ†n]; hence M
âˆâŠ¢(âˆƒx)B by Ax2 and
MP. Conversely, let M
âˆâŠ¢(âˆƒx)B . By I.5.32, M
âˆâŠ¢B [xâ†n], where n is the
appropriate Henkin constant. By I.H., (B [xâ†n])I = t; hence AI = t.
â–¡
Finally,

I.5. Semantics; Soundness, Completeness, Compactness
71
Proof (of the Consistency Theorem). Let Aâ€² be an M-instance of a formula
A in .
Note that A is over L.
From  âŠ†M
âˆit follows that M
âˆâŠ¢A, and hence M
âˆâŠ¢Aâ€², by I.4.12. By
the Main Lemma, Aâ€²I = t.
Thus, the reduct Mâ€² = (M, I â†¾L) is a model of .
â–¡
We had to move to the reduct Mâ€² to be technically correct. While M â€œsatisï¬esâ€
, its I also acts on symbols not in L. The I of a structure appropriate for L
is only supposed to assign meaning to the symbols of L.
I.5.35 Corollary. A consistent theory over a countable language has a count-
able model.
I.5.36 Corollary (LÂ¨owenheim-Skolem Theorem). If a set of formulas  over
a countable language has a model, then it has a countable model.
Proof. If a model exists, then the theory  is consistent.
â–¡
I.5.37 Corollary (GÂ¨odelâ€™s Completeness Theorem). In any countable ï¬rst
order language L,  |= A implies  âŠ¢A.
Proof. Let B denote the universal closure of A. By Exercise I.43,  |= B .
Thus, +Â¬B has no models (why?). Therefore it is inconsistent. Thus,  âŠ¢B
(by I.4.21), and hence (specialization),  âŠ¢A.
â–¡
A way to rephrase completeness is that if  |= A, then also  |= A, where
 âŠ† is ï¬nite. This follows by soundness, since  |= A entails  âŠ¢A and
hence  âŠ¢A, where  consists of just those formulas of  used in the proof
of A.
I.5.38 Corollary (Compactness Theorem). In any countable ï¬rst order lang-
uage L, a set of formulas  is satisï¬able iff it is ï¬nitely satisï¬able.
Proof. Only-if part. This is trivial, for a model of  is a model of any ï¬nite
subset.
If part. Suppose that  is unsatisï¬able (it has no models). Then it is in-
consistent by the consistency theorem. In particular,  âŠ¢Â¬x = x. Since the
pure theory over L is consistent, a -proof of Â¬x = x involves a nonempty
ï¬nite sequence of nonlogical axioms (formulas of ), A1,. . . , An. That is,
A1,. . . , An âŠ¢Â¬x = x; hence {A1,. . . , An} has no model (by soundness).
This contradicts the hypothesis.
â–¡

72
I. Basic Logic
Alternatively, we can prove the above by invoking â€œsyntactic compactnessâ€: A
set of formulas is consistent iff every ï¬nite subset is consistent, since proofs
have ï¬nite length. Now invoke the consistency theorem and I.5.14.
We conclude this section by outlining the amendments to our proof that will
remove the restriction that L is countable. This plan of amendments presupposes
some knowledgeâ€  of ordinals and cardinality (cf. volume 2) beyond what our
â€œcrash courseâ€ has covered. The reader may accept the statements proved here
and skip the proofs with no loss of continuity. These statements, in particular
the GÂ¨odel-Malâ€²cev compactness theorem, are applied later on to founding non-
standard analysis (following A. Robinson).
Let L be (possibly) uncountable, in the sense that the cardinality k of V
is â‰¥Ï‰. The cardinality of the set of all strings over V
is also k (for a proof see
volume 2, Chapter VII). We now pick and ï¬x for our discussion an arbitrary set
N of cardinality k.
I.5.39 Remark. An example of such a set is k itself, and can be taken as N.
However, we can proï¬t from greater generality: N can be any set of any type
of (real) objects that we choose with some purpose in mind, as long as its
cardinality is k.
â–¡
Therefore the elements of N can be arranged in a transï¬nite sequence (indexed
by all the ordinals Î± < k)
n0, n1,. . . , nÎ±, . . .
We then form L(N) (to parallel L(N) of our previous construction) by adding
to V a distinct name nÎ± for each nÎ± âˆˆN. Thus, we have enumerations
F 0, F 1, F 2,. . . , F Î±, . . . of all sentences in Wff(N)
(1â€²)
and
G 1, G 2, G 3,. . . , G Î±, . . . of all sentences in Wff(N) of the form (âˆƒx)A (2â€²)
where, in (2â€²), every sentence (âˆƒx)A of Wff(N) is listed inï¬nitely often, and
the indices (subscripts) in both (1â€²) and (2â€²) run through all the ordinals Î± < k
(omitting index 0 in the second listing).
We next proceed as expected: We deï¬ne by induction (or recursion) over
the ordinals Î± < k a transï¬nite sequence of theories (determined by  and
additional sentences over L(N) as nonlogical axiomsâ€¡):
â€  On an informal level, of course. All this is going on in the metatheory, just like the countable
case.
â€¡ Note that the formulas in  need not be closed.

I.5. Semantics; Soundness, Completeness, Compactness
73
We set 0 = . For any Î± < k, we deï¬ne Î± to be 
Î²<Î± Î² just in case Î±
is a limit ordinal. If Î± = Î² + 1 (a successor) then the deï¬nition is effected in
two stages: We ï¬rst let
Î² =
Î² âˆª{F Î²}
if Î² Ì¸âŠ¢Â¬F Î²
Î²
otherwise
(3â€²)
Then, we set Î± = Î² âˆª{A[x â†i]} just in case Î² âŠ¢G Î±, where G Î± â‰¡
(âˆƒx)A.
The choice of i is important: i = nÎ± âˆˆN, where Î± < k is the smallest index
such that the constant nÎ± does not occur (as a substring) in any of the sentences
F 0,. . . , F Î², G 1,. . . , G Î±
(4â€²)
The sentence A[x â†i] added to Î± is called a special Henkin axiom. The
constant i is the associated Henkin or witnessing constant.
We now set
k =

Î±<k
Î±
(5â€²)
This is a set of formulas over Wff(N) that deï¬nes a theory Tk over Wff(N) (as
the set of nonlogical axioms of Tk). We next deï¬ne âˆ¼, on N this time, as before
((6) on p. 66):
n âˆ¼m
iff
âŠ¢Tk n = m
We note its properties, and proceed to deï¬ne a subset M of N as in (7) and (8)
(p. 67).
Since M âŠ†N, its cardinality is â‰¤k. After deï¬ning the M-restriction of
a formula A as before, all the rest proceeds as in Lemmata I.5.28â€“I.5.33,
replacing throughout Tâˆ, TM
âˆ, and members i âˆˆN by Tk, TM
k , and members
i âˆˆN respectively. We are then able to state:
I.5.40 Metatheorem (Consistency Theorem). If a (ï¬rst order) theory T over
a language L of cardinality k is consistent, then it has a model of cardinal-
ity â‰¤k.
Terminology: The cardinality of a model is that of its domain.
I.5.41 Corollary (Completeness Theorem). In any ï¬rst order language L,
 |= A implies  âŠ¢A.

74
I. Basic Logic
I.5.42 Corollary (GÂ¨odel-Malâ€²cev Compactness Theorem). In any ï¬rst order
language L, a set of formulas  is satisï¬able iff it is ï¬nitely satisï¬able.
The LÂ¨owenheim-Skolem theorem takes the following form:
I.5.43 Corollary (Upward LÂ¨owenheim-Skolem Theorem). If a set of for-
mulas  over a language L of cardinality k has an inï¬nite model, then it has a
model of any cardinality n such that k â‰¤n.
Proof. Let K = (K, I ) be an inï¬nite model of . Pick a set N of cardinality n,
and import its individuals c as new formal constants c into the language of .
The set  =  âˆª{Â¬c = d : c Ì¸= d on N} is ï¬nitely satisï¬able. This is because
every ï¬nite subset of  involves only ï¬nitely many of the sentences Â¬c = d;
thus there is capacity in K (as it is inï¬nite) to extend I into I â€² (keeping K
the same, but deï¬ning distinct cI â€², d
I â€²
, etc.) to satisfy these sentences in an
expanded structure Kâ€² = (K, I â€²).
Hence  is consistent.
Following the construction given earlier, we take this N (and ) as our
starting point to build a simply complete, consistent extension Tn of , and a
model M for , with domain some subset M of N. The choice of M follows the
deï¬nition of â€œâˆ¼â€ (see pp. 66 and 73). Under the present circumstances, â€œâˆ¼â€ is
â€œ=â€ on N, for âŠ¢Tn c = d implies c = d on N (otherwise Â¬c = d is an axiom â€“
impossible, since Tn is consistent). Thus M = N; hence the cardinality of M
is n.
The reduct of M on L is what we want. Of course, its cardinality is still right
(M did not change).
â–¡
Good as the above proof may be, it relies on the particular proof of the
consistency theorem, and this is a drawback. Hence we offer another proof that
does not have this defect.
Proof. (Once more) We develop a different argument, starting from the point
where we concluded that  is consistent. Since the language of this theory has
cardinality â‰¤n,
Pause. Why?
we know by I.5.40 that
we have a model M = (M, I ) for  of cardinality â‰¤n
(âˆ—âˆ—)

I.6. Substructures, Diagrams, and Applications
75
Deï¬ne now a function f : N â†’M by f (n) = nI . Since all n âˆˆN have
been imported into the language of , f is total. f is also 1-1: Indeed, if c Ì¸= d
on N, then Â¬c = d is an axiom. Hence, (Â¬c = d)I = t. That is, f (c) Ì¸= f (d)
on M. But then (by results on cardinality in volume 2) the cardinality n of N
is â‰¤the cardinality of M. By yet another result about cardinality,â€  (âˆ—âˆ—) and
what we have just concluded imply that N and M have the same cardinality.
At this point we take the reduct of M on L, exactly as in the previous
proof.
â–¡
The above proof required more set theory. But it was independent of any
knowledge of the proof of the consistency theorem.
I.5.44 Remark (about Truth). The completeness theorem shows that the syn-
tactic apparatus of a ï¬rst order (formal) logic totally captures the semantic
notion of truth, modulo the acceptance as true of any given assumptions .
This justiï¬es the habit of the mathematician (even the formalist: see Bourbaki
(1966b, p. 21)) of saying â€“ in the context of any given theory  â€“ â€œit is trueâ€
(meaning â€œit is a -theoremâ€, or â€œit is -provedâ€), â€œit is falseâ€ (meaning â€œthe
negation is a -theoremâ€), â€œassume that A is trueâ€ (meaning â€œadd the formula
A â€“ to  â€“ as a nonlogical axiomâ€), and â€œassume that A is falseâ€ (meaning
â€œadd the formula Â¬A â€“ to  â€“ as a nonlogical axiomâ€).
Thus, â€œit is trueâ€ (in the context of a theory) means â€œit is true in all its
modelsâ€, hence provable: a theorem. We will not use this particular argot.
There is yet another argot use of â€œis trueâ€ (often said emphatically, â€œis really
trueâ€), meaning truth in some speciï¬c structure, the â€œintended modelâ€. Due to
GÂ¨odelâ€™s ï¬rst incompleteness theorem (Section I.9) this truth does not coincide
with provability.
I.6. Substructures, Diagrams, and Applications
On p. 38 we saw one way of generating theories, as the sets of theorems, Thm,
proved from some set of (nonlogical) axioms, . Another way of generating
theories is by taking all the formulas that are valid in some class of structures.â€¡
â€  That k â‰¤l âˆ§l â‰¤k â†’k = l for any cardinals k and l.
â€¡ In axiomatic set theory (e.g., as this is developed in volume 2 of these lectures) the term â€œclassâ€
means a â€œcollectionâ€ that is not necessarily a set, by virtue of its enormous size. Examples of
such collections are that of all ordinal numbers, that of all cardinal numbers, that of all the objects
that set theory talks about, and many others. In the present case we allow for a huge collection of
structures â€“ for example, all structures â€“ hence we have used the term â€œclassâ€, rather than â€œsetâ€,
deliberately.

76
I. Basic Logic
I.6.1 Deï¬nition. Let L be a ï¬rst order language and C a class of structures
appropriate for L. We deï¬ne two symbols
T (C )
def
= {A : for all M âˆˆC , |=M A}
and
Th(C )
def
= {A : A is closed and, for all M âˆˆC , |=M A}
If C = {M} then we write T (M) and Th(M) rather than T ({M}) and Th({M}).
â–¡
For any class of structures, C , T (C ) is a theory in the sense of p. 38. This
follows from an easy veriï¬cation of
A âˆˆT (C )
iff
T (C ) âŠ¢A
(1)
We verify the if direction of (1), the opposite one being trivial. To prove A âˆˆ
T (C ) we let M âˆˆC and prove
|=M A
(2)
By soundness, it sufï¬ces to prove |=M T (C ), i.e.,
|=M B
for all
B âˆˆT (C )
which holds by Deï¬nition I.6.1.
I.6.2 Example. For any structure M for a language L, T (M) is a complete
theory: We want, for any sentence A, T (M) âŠ¢A or T (M) âŠ¢Â¬A. By the
previous observation, this translates to A âˆˆT (M) or (Â¬A) âˆˆT (M). This
holds, by I.6.1.
â–¡
I.6.3 Example. Let K be the (enormous) class of all structures for some
language L. Then T (K ) is the set of all universally valid formulas over L. â–¡
I.6.4 Deï¬nition. Suppose that M and K are two structures for a language L.
If it happens that T (M) = T (K), then we say that M and K are elementarily
equivalent and write M â‰¡K.
The context will fend off any mistaking of the symbol for elementary equi-
valence, â‰¡, for that for string equality (cf. I.1.4).
â–¡
I.6.5 Proposition.â€  Let M and K be two structures for L. Then, M â‰¡K iff
Th(M) = Th(K).
â€  Normally we use the word â€œPropositionâ€ for a theorem that we do not want to make a big fuss
about.

I.6. Substructures, Diagrams, and Applications
77
Proof. The only-if part is immediate from I.6.4. For the if part, let A be an
arbitrary formula of L. Let Aâ€² be its universal closure (cf. I.4.11). Then |=M A
iff |=M Aâ€² and |=K A iff |=K Aâ€²; thus |=M A iff |=K A (since |=M Aâ€² iff
|=K Aâ€²).
â–¡
One way to obtain a structure K that is elementarily equivalent to a stru-
cture M is by a systematic renaming of all the members of the domain
of M.
In what follows, if M = (M, I ) is a structure, |M| is alternative notation for
its domain M. Moreover, the interpretation of a language item a, f , or P will
be denoted by an M superscript, e.g., aM, rather than with an I superscript,
as in aI . This alternative notation saves us from juggling far too many letters
if we are discussing several structures at once (M, N, M, N, I M, I N, etc.; see
also I.5.2, p. 53).
I.6.6 Deï¬nition (Embeddings, Isomorphisms, and Substructures). Let M
and K be structures for a language L, and Ï† : |M| â†’|K| be a total (that is,
everywhere deï¬ned on |M|) and 1-1â€  function. Ï† is a structure embedding â€“ in
which case we write Ï† : M â†’K â€“ just in case Ï† preserves all the nonlogical
interpretations. This means the following:
(1) aK = Ï†(aM) for all constants a of L
(2) f K(Ï†(i1),. . . , Ï†(in)) = Ï†( f M(i1,. . . , in)) for all n-ary functions f of L
and all i j âˆˆ|M|
(3) PK(Ï†(i1),. . . , Ï†(in)) = PM(i1,. . . , in) for all n-ary predicates P of L and
all i j âˆˆ|M|
In the last two cases â€œ=â€ is metamathematical, comparing members of |K| in
the former and members of the truth set, {t, f}, in the latter.
An important special case occurs when Ï† : |M| â†’|K| is the inclusion map,
that is, Ï†(i) = i for all i âˆˆ|M| â€“ which entails |M| âŠ†|K|. We then say
that M is a substructure of K and write M âŠ†K (note the absence of | . . . |).
Symmetrically, we say that K is an extension of M.
If Ï† is onto as well â€“ that is, (âˆ€b âˆˆ|K|)(âˆƒa âˆˆ|M|)Ï†(a) = b â€“ then we say
that it is a structure isomorphism, in symbols M âˆ¼=
Ï† K, or just M âˆ¼= K.
It will be convenient to use the following abbreviation (Shoenï¬eld (1967)):
iÏ† is short for Ï†(i) for all i âˆˆ|M|.
â–¡
â€  That is, Ï†(a) = Ï†(b) implies a = b for all a and b in |M|.

78
I. Basic Logic
There is no special signiï¬cance in using the letter Ï† for the embedding or
isomorphism, other than its being a symbol other than what we normally use
(generically) for formal function symbols ( f, g, h).
One way to visualize what is going on in Deï¬nition I.6.6 is to employ a so-called
commutative diagram (for simplicity, for a unary f )
|M| âˆ‹i
f M
âˆ’â†’f M(i) âˆˆ|M|
|
|
Ï†â†“
â†“Ï†
|K| âˆ‹iÏ†
f K
âˆ’â†’f K(iÏ†) âˆˆ|K|
That the diagram commutes means that all possible compositions give the same
result. Here Ï†( f M(i)) = f K(Ï†(i)).
When M âŠ†K and Ï† is the inclusion map, then the diagram above simply
says that f M is the restriction of f K on |M|, in symbols, f M = f K â†¾|M|.
Condition (3) in Deï¬nition I.6.6 simply says that PM is the restriction of PK
on |M|n, i.e., PM = PK âˆ©|M|n. One often indicates this last equality by
PM = PK | |M|.
I.6.7 Remark. Sometimes we have a structure M = (M, I ) for L and a
1-1 correspondence (total, 1-1, and onto) Ï† : M â†’K for some set K. We
can turn K into a structure for L, isomorphic to M, by deï¬ning its â€œI â€-part
mimicking Deï¬nition I.6.6. We say that Ï† induces an isomorphism M âˆ¼=
Ï† K.
We do this as follows:
(i) We set aK = Ï†(aM) for all constants a of L
(ii) We set f K(âƒ—in) = Ï†( f M(Ï†âˆ’1(i1),. . . , Ï†âˆ’1(in))) for all n-ary function sym-
bols f of L and all i j in K.â€ 
(iii) We set PK(âƒ—in) = PM(Ï†âˆ’1(i1),. . . , Ï†âˆ’1(in)) for all n-ary predicate sym-
bols P of L and all i j in K.
It is now trivial to check via I.6.6 that for the K so deï¬ned above, M âˆ¼=
Ï† K
(Exercise I.47).
â–¡
The following shows that an embedding (and a fortiori an isomorphism)
preserves meaning beyond that of the nonlogical symbols.
â€  By Ï†âˆ’1(a), for a âˆˆK, we mean the unique b âˆˆM such that Ï†(b) = a.

I.6. Substructures, Diagrams, and Applications
79
I.6.8 Theorem. Let Ï† : M â†’K be an embedding of structures of a language
L. Then:
(1) For every term t of L whose variables are among âƒ—xn, and for all âƒ—in in |M|,
one has t[[iÏ†
1 ,. . . , iÏ†
n ]] = Ï†(t[[âƒ—in]]).â€ 
(2) For every atomic formula A of L whose variables are among âƒ—xn, and for
all âƒ—in in |M|, one has |=M A[[âƒ—in]] iff |=K A[[iÏ†
1 ,. . . , iÏ†
n ]].
(3) If Ï† is an isomorphism, then we can remove the restriction atomic from (2)
above.
Strictly speaking, we are supposed to apply â€œ|=â€ to a (formal) formula (cf. I.5.7).
â€œA[[âƒ—in]]â€ is an abbreviated notation for a â€œconcreteâ€ (already interpreted) sen-
tence. However, it is useful to extend our argot to allow the notation â€œ|=M
A[[âƒ—in]]â€ â€“ which says â€œthe concrete sentence A[[âƒ—in]] is true in Mâ€, in short,
(A[i1,. . . , in])M = t (cf. I.5.17) â€“ as this notation readily discloses where A
was interpreted and therefore where it is claimed to be true.
Proof. (1): We do induction on terms t (cf. I.5.17). If t â‰¡a, a constant of L,
then the left hand side is aK while the right hand side is Ï†(aM). These (both in
|K|) are equal by I.6.6.
If t â‰¡x j (for some x j among the âƒ—xn), then t[[iÏ†
1 ,. . . , iÏ†
n ]] = iÏ†
j âˆˆ|K| while
t[[i1,. . . , in]] = i j âˆˆ|M|. Thus (1) of the theorem statement is proved in this
case.
Finally, let t â‰¡f t1 . . . tr. Then
Ï†

( f t1 . . . tr)[[âƒ—in]]

= Ï†( f M(t1[[âƒ—in]],. . . , tr[[âƒ—in]])) (cf. I.5.17)
= f K(Ï†(t1[[âƒ—in]]),. . . , Ï†(tr[[âƒ—in]])) (by I.6.6)
= f K
t1

iÏ†
1 ,. . . , iÏ†
n

,. . . , tr

iÏ†
1 ,. . . , iÏ†
n

(by I.H.)
= ( f t1 . . . tr)

iÏ†
1 ,. . . , iÏ†
n

(by I.5.17)
(2): We have two cases:
Case where A â‰¡Pt1 . . . tr.
Then
|=M (Pt1 . . . tr)[[âƒ—in]]
iff |=M PM(t1[[âƒ—in]],. . . , tr[[âƒ—in]])
(by I.5.17)
iff |=K PK(Ï†(t1[[âƒ—in]]),. . . , Ï†(tr[[âƒ—in]]))
(by I.6.6)
iff |=K PK
t1

iÏ†
1 ,. . . , iÏ†
n

,. . . , tr

iÏ†
1 ,. . . , iÏ†
n

(by (1))
iff |=K (Pt1 . . . tr)

iÏ†
1 ,. . . , iÏ†
n

(by I.5.17)
â€  Recall I.5.17. Of course, t[[âƒ—in]] âˆˆ|M|, while t

iÏ†
1 ,. . . , iÏ†
n

âˆˆ|K|.

80
I. Basic Logic
Case where A â‰¡t = s.
Then
|=M (t = s)[[âƒ—in]] iff |=M t[[âƒ—in]] = s[[âƒ—in]]
(I.5.17)
iff |=K Ï†(t[[âƒ—in]]) = Ï†(s[[âƒ—in]])
(if by 1-1-ness)
iff |=K t

iÏ†
1 ,. . . , iÏ†
n

= s

iÏ†
1 ,. . . , iÏ†
n

(by (1))
iff |=K (t = s)

iÏ†
1 ,. . . , iÏ†
n

(by I.5.17)
(3): It is an easy exercise to see that if A and B have the property claimed,
then so do Â¬A and A âˆ¨B without the additional assumption of ontoness.
Ontoness helps (âˆƒx)A:
|=M ((âˆƒx)A)[[âƒ—in]]
iff (âˆƒa âˆˆ|M|) |=M A[[a, âƒ—in]]
(by I.5.17)
iff (âˆƒa âˆˆ|M|) |=K A

aÏ†, iÏ†
1 ,. . . , iÏ†
n

(by I.H. on formulas)
iff (âˆƒb âˆˆ|K|) |=K A

b, iÏ†
1 ,. . . , iÏ†
n

(if by ontoness)
iff |=K ((âˆƒx)A)

iÏ†
1 ,. . . , iÏ†
n

(by I.5.17)
â–¡
I.6.9 Corollary. If M and K are structures for L and M âŠ†K, then for all
quantiï¬er-free formulas A whose variables are among âƒ—xn, and for all âƒ—in in
|M|, one has |=M A[[âƒ—in]] iff |=K A[[âƒ—in]].
I.6.10 Corollary. If M and K are structures for L and M âˆ¼=
Ï† K, then M â‰¡K.
Proof. Let A(âƒ—xn) be a formula. The sought
|=M A(âƒ—xn)
iff
|=K A(âƒ—xn)
translates to
(âˆ€i1 âˆˆ|M|) . . . (âˆ€in âˆˆ|M|) |=M A[[âƒ—in]]
iff
(âˆ€j1 âˆˆ|K|) . . . (âˆ€jn âˆˆ|K|) |=K A[[âƒ—jn]]
which is true,â€  by (3) of I.6.8, since every j âˆˆ|K| is an iÏ† (for some i âˆˆ|M|).
â–¡
I.6.11 Corollary. Let M and K be structures for L, M âˆ¼=
Ï† K, and T a theory
over L. Then |=M T iff |=K T.
Condition (2) in I.6.8 is quite strong, as the following shows.
â€  Since we are arguing in the metatheory, we can say â€œtrueâ€ rather than â€œprovableâ€.

I.6. Substructures, Diagrams, and Applications
81
I.6.12 Theorem. Let M and K be structures for L, and Ï† : |M| â†’|K| be a
total function. If for every atomic A that contains at most the variables âƒ—xn and
for all âƒ—in in |M| one has
|=M A[[âƒ—in]]
iff
|=K A

iÏ†
1 ,. . . , iÏ†
n

(1)
then Ï† : M â†’K is an embedding.
Proof. First off, Ï† is 1-1. Indeed, since x = y is atomic, then, by (1) above, for
all i and j in |M|, |=M i = j iff |=K iÏ† = jÏ†.â€  The if direction gives 1-1-ness.
We now check the three conditions of Deï¬nition I.6.6.
Let next aM = i. Using the atomic formula a = x, we get (by (1))
|=M (a = x)[[i]]
iff
|=K (a = x)[[iÏ†]]
that is,
|=M aM = i
iff
|=K aK = iÏ†
Since aM = i is true, so is aK = iÏ†. That is, aK = Ï†(aM). (This part only
needed the only-if direction of (1).)
Let now f be n-ary. Consider the atomic formula f (âƒ—xn) = xn+1. By (1),
|=M ( f (âƒ—xn) = xn+1)[[âƒ—in+1]]
iff
|=K ( f (âƒ—xn) = xn+1)

iÏ†
1 ,. . . , iÏ†
n+1

That is,
|=M f M(âƒ—in) = in+1
iff
|=K f K
iÏ†
1 ,. . . , iÏ†
n

= iÏ†
n+1
Thus f K(iÏ†
1 ,. . . , iÏ†
n ) = Ï†( f M(âƒ—in)). (This part too only needed the only-if
direction of (1).)
Finally, let P be n-ary. By (1)
|=M P(âƒ—xn)[[âƒ—in]]
iff
|=K P(âƒ—xn)

iÏ†
1 ,. . . , iÏ†
n

That is,
|=M PM(âƒ—in)
iff
|=K PK
iÏ†
1 ,. . . , iÏ†
n

â–¡
I.6.13 Corollary. Let M and K be structures for L, and Ï† : |M| â†’|K| be a
total function. If, for every atomic and negated atomic A that contains at most
the variables âƒ—xn and for all âƒ—in in |M| one has
|=M A[[âƒ—in]]
implies
|=K A

iÏ†
1 ,. . . , iÏ†
n

then Ï† : M â†’K is an embedding.
â€  (x = y) [[i, j]] â‰¡i = j.

82
I. Basic Logic
Proof. Let |=K A[[iÏ†
1 ,. . . , iÏ†
n ]], A atomic. If Ì¸|=M A[[âƒ—in]], then |=M Â¬A[[âƒ—in]].
Â¬A is negated atomic. Thus, |=K Â¬A[[iÏ†
1 ,. . . , iÏ†
n ]], i.e., Ì¸|=K A[[iÏ†
1 ,. . . , iÏ†
n ]],
contradicting the assumption. The â€œimpliesâ€ has now been promoted to â€œiffâ€.
â–¡
I.6.14 Corollary. Let M and K be structures for L, and |M| âŠ†|K|. If, for
every atomic and negated atomic A that contains at most the variables âƒ—xn and
for all âƒ—in in |M| one has
|=M A[[âƒ—in]]
implies
|=K A[[âƒ—in]]
then M âŠ†K.
Proof. |M| âŠ†|K| means that we may take here Ï† : |M| â†’|K| to be the inclu-
sion map.
â–¡
The converses of the above two corollaries hold (these, essentially, are I.6.8
and I.6.9).
Moreover, these corollaries lead to
I.6.15 Deï¬nition (Elementary Embeddings and Substructures). Let M and
K be structures for L, and Ï† : |M| â†’|K| a total function. If, for every formula
A that contains at most the variables âƒ—xn and for all âƒ—in in |M|, one has
|=M A[[âƒ—in]]
implies
|=K A

iÏ†
1 ,. . . , iÏ†
n

(1)
then Ï† : M â†’K is called an elementary embedding. We write Ï† : M â†’â‰ºK
in this case.
If Ï† is the inclusion map |M| âŠ†|K|, then (1) becomes
|=M A[[âƒ—in]]
implies
|=K A[[âƒ—in]]
(2)
In this case we say that M is an elementary substructure of K, or that the latter
is an elementary extension of the former. In symbols, M â‰ºK.
â–¡
I.6.16 Remark. The â€œimpliesâ€ in (1) and (2) in the deï¬nition above is promoted
to â€œiffâ€ as in the proof of I.6.13.
By I.6.13 and I.6.14, an elementary embedding Ï† : M â†’â‰ºK is also an
embedding Ï† : M â†’K, and an elementary substructure M â‰ºK is also a
substructure M âŠ†K.

I.6. Substructures, Diagrams, and Applications
83
If Ï† : M â†’â‰ºK, then condition (1) in Deï¬nition I.6.15 (which, as we have
already noted, is an equivalence) yields, for all sentences A over L,
|=M A
iff
|=K A
By I.6.5, M â‰¡K. In particular, M â‰ºK implies M â‰¡K.
â–¡
I.6.17Remark. Inmanyargumentsinmodeltheory,startingwithastructureM
for some language L, one constructs another structure K for L and an embedding
Ï† : M â†’K or an elementary embedding Ï† : M â†’â‰ºK. More often than not
one would rather have a structure extension or an elementary extension of M
(i.e., preferring that Ï† be the inclusion map) respectively. This can be easily
obtained as follows:
Let M = (M, . . . ) and K = (K, . . . ). Let N be a set disjoint from M,â€  of
cardinality equal to that of K âˆ’Ï†[M].â€¡ Thus there is a 1-1 correspondence
Ïˆ : K âˆ’Ï†[M] â†’N, from which we can build a 1-1 correspondence Ï‡ : K â†’
M âˆªN by deï¬ning
Ï‡(x) =
Ïˆ(x)
if x âˆˆK âˆ’Ï†[M]
Ï†âˆ’1(x)
if x âˆˆÏ†[M]
Using Remark I.6.7, we get a structure Kâ€² = (M âˆªN, . . . ) such that
K âˆ¼=
Ï‡ Kâ€²
(1)
We verify that if we had Ï† : M â†’â‰ºK initially, then we now haveÂ§
Ï‡ â—¦Ï† : M â†’â‰ºKâ€²
(2)
Well, let A be arbitrary over L with free variables among âƒ—xn. Pick any âƒ—in in M,
and assume |=M A[[âƒ—in]]. By hypothesis on Ï† and I.6.15, |=K A[[Ï†(i1),. . . ,
Ï†(in)]]. By I.6.8(3) and (1) above, |=K â€² A[[Ï‡(Ï†(i1)),. . . , Ï‡(Ï†(in))]]. This set-
tles (2), by I.6.15. By deï¬nition of Ï‡ above, if x âˆˆM, then, since Ï†(x) âˆˆÏ†[M],
we have Ï‡(Ï†(x)) = Ï†âˆ’1(Ï†(x)) = x. That is, (2) is an elementary extension
(Ï‡ â—¦Ï† is the identity on M).
The alternative initial result, an embedding Ï† : M â†’K, yields a struc-
ture extension Ï‡ â—¦Ï† : M âŠ†Kâ€², since the composition of embeddings is an
embedding (Exercise I.48).
â–¡
â€  N âˆ©M = âˆ….
â€¡ Ï†[X] = {Ï†(x) : x âˆˆX}. X âˆ’Y = {x âˆˆX : x /âˆˆY}.
Â§ Ï‡ â—¦Ï† denotes function composition. That is, (Ï‡ â—¦Ï†) (x) = Ï‡(Ï†(x)) for all x âˆˆM.

84
I. Basic Logic
The following criterion for elementary extensions evaluates truth in the big-
ger of the two structures. It is useful, among other places, in the proof of the
downward LÂ¨owenheim-Skolem theorem (I.6.20 below).
I.6.18 Proposition. Let M and K be structures for L. If M âŠ†K, and moreover,
for every formula A whose free variables are among y and âƒ—xn, and for all âƒ—bn
in |M|,
(âˆƒa âˆˆ|K|) |=K A[[a, âƒ—bn]]
implies
(âˆƒa âˆˆ|M|) |=K A[[a, âƒ—bn]]
(3)
then M â‰ºK.
Proof. We need to prove (2) of I.6.15 using (3) above.
We do induction on formulas. We have already remarked in I.6.16 that the
deï¬nition is an â€œiffâ€, and it is more convenient to carry the induction through
with an â€œiffâ€ rather than an â€œimpliesâ€.
First off, by M âŠ†K and Corollary I.6.9, we are done for all quantiï¬er-free
formulas. This takes care of atomic formulas.
For the induction step, from B and C to Â¬B and B âˆ¨C , we note (with âƒ—i
arbitrary, in |M|)
|=M Â¬B [[âƒ—i]] iff Ì¸|=M B [[âƒ—i]]
iff Ì¸|=K B [[âƒ—i]]
by I.H.
iff |=K Â¬B [[âƒ—i]]
Similarly,
|=M (B âˆ¨C )[[âƒ—i]] iff |=M B [[âƒ—i]] or |=M C [[âƒ—i]]
iff |=K B [[âƒ—i]] or |=K C [[âƒ—i]]
by I.H.
iff |=K (B âˆ¨C )[[âƒ—i]]
It only remains to check existential formulas. The implication
|=M ((âˆƒx)B )[[âƒ—i]]
implies
|=K ((âˆƒx)B )[[âƒ—i]]
that is,
(âˆƒa âˆˆ|M|) |=M B [[a, âƒ—i]]
implies
(âˆƒa âˆˆ|K|) |=K B [[a, âƒ—i]]
being trivial by |M| âŠ†|K|, we check the opposite direction. Let âƒ—i still be in
|M|, and (âˆƒa âˆˆ|K|) |=K B [[a, âƒ—i]]. By (3), (âˆƒa âˆˆ|M|) |=K B [[a, âƒ—i]]. By I.H.,
(âˆƒa âˆˆ|M|) |=M B [[a, âƒ—i]].
â–¡

I.6. Substructures, Diagrams, and Applications
85
I.6.19 Example. The structure Nâ€² = (N, <, 0) is a reduct of the standard model
of arithmetic. It is essentially a linearly ordered set, where we have also focused
on a â€œdistinguished elementâ€, 0, that also happens to be the minimum element of
the order. Any nonempty B âŠ†N that contains 0 leads to a B = (B, <, 0) âŠ†Nâ€²,
trivially.
If we ask for more, that B â‰ºNâ€², then we reduce our choices (of B that
work) drastically. Indeed, let A(x, y) say that x and y are consecutive, x < y.
That is, A(x, y) is
x < y âˆ§Â¬(âˆƒz)(x < z âˆ§z < y)
Then
(âˆ€a âˆˆN) |=Nâ€² ((âˆƒy)A(x, y))[[a]]
(1)
Thus, if B â‰ºNâ€², then we require B âŠ†Nâ€² â€“ in particular, 0 âˆˆB â€“ and
(from (1), I.6.15 and I.6.16)
(âˆ€a âˆˆB)(âˆƒb âˆˆB) |=B A[[a, b]]
(2)
0 âˆˆB and (2) yield (by metamathematical induction) that B = N.
â–¡
The following is a useful tool in model theory.
I.6.20 Theorem (Downward LÂ¨owenheim-Skolem Theorem). Assume that L
has cardinality at most m (that is, the set of nonlogical symbols doesâ€ ), while
the structure M = (M, I ) (for L) has cardinality at least m. Let X âŠ†M, and
the cardinality of X be â‰¤m. Then there is a structure K for L, of cardinality
exactly m, that satisï¬es X âŠ†|K| and K â‰ºM.
This proof requires some facility with cardinal manipulation. In particular, we
use facts such as that, if m is an inï¬nite cardinal and n âˆˆN while â„µ0 is the
cardinality of N, then (n + 1) Â· m = â„µ0 Â· m = mn+1 = m, where â€œÂ·â€ denotes
cardinal multiplication.
Proof. (Thinking out loud.) If K = (K, . . . ) â€“ still to be deï¬ned â€“ is to satisfy
K â‰ºM, then we have to ensure (I.6.18), among other things, that for every
A(y, âƒ—xn) over L and for all âƒ—an in K, if A[[b, âƒ—an]] is true in M (where b âˆˆM),
then some bâ€² âˆˆK exists such that A[[bâ€², âƒ—an]] is also true in M.
â€  It is a set-theoretical fact that the union of a countable set and an inï¬nite set has cardinality equal
to that of the inï¬nite set, or â„µ0 + m = m. Recall that the set of logical symbols is countable.

86
I. Basic Logic
In short, we need to obtain K as a subset of M that includes X and is closed
under all the relations
QA = {âŸ¨âƒ—an, bâŸ©: |=M (A(y, âƒ—xn))[[b, âƒ—an]]}
(1)
where b is the output part in QA above.
It is prudent to take two precautions:
First, in order to get the smallest cardinal possible for K, we build the âŠ†-
smallest set described in italics above, that is, we take K = Cl(X, R),â€  where
R is the set of all the relations QA as A varies over all the formulas over L.
Second, in order to place a lower bound of m to the cardinality of K, we start,
rather than with X, with a set Y of cardinality exactly m such that X âŠ†Y âŠ†M.
Such a Y exists by the size estimates for X and M. In other words, we have just
changed our mind, and now let K = Cl(Y, R).
Actually, a bit more prudence is in order. Let us rename the Cl(Y, R) above
K â€². We are still looking for a K that we will keep.
We will â€œcut downâ€ each QA in (1) â€“ with the help of the axiom of choice
(AC) â€“ making the relation single-valuedâ€¡ in its output, b. (End of thinking out
loud.)
We deï¬ne

QA = {âŸ¨âƒ—an, bâŸ©: b picked by AC in {x : QA (âƒ—an, x)} if (âˆƒx âˆˆM)QA (âƒ—an, x)}
(2)
Let now 
R be the set of all 
QA , and set K = Cl(Y, 
R). This one we keep!
First off, trivially, K âŠ†M. To turn K into a substructure of M, we have to
interpret every f, a, P â€“ function, constant and predicate, respectively â€“ of L
as f M â†¾K n (n-ary case), leave as aM, and PM|K (n-ary case), respectively.
For functions and constants we have more work: For the former we need to
show that K is closed under all f M |`K n. Let then f be n-ary in L and âƒ—an be in
K. Then f M(âƒ—an) = b and b âˆˆM. We want b âˆˆK.
Well, if we set A â‰¡f (âƒ—xn) = y, then |=M ( f (âƒ—xn) = y)[[âƒ—an, b]]; hence
QA (âƒ—an, b) is true (cf. (1)), and therefore 
QA (âƒ—an, b) is true, for b is unique,
given the âƒ—an, so that it must be what AC picks in (2). Thus, since K is

QA-closed, b âˆˆK. Similarly one proves the case for constants a of L, us-
ing the formula a = y.
Thus we do have a substructure, K = (K, . . . ), of M.
It remains to settle the property of Proposition I.6.18 and the cardinality
claims.
â€  You may want to review Section I.2 at this point.
â€¡ A fancy term for â€œfunctionâ€. This function need not be deï¬ned everywhere on M.

I.6. Substructures, Diagrams, and Applications
87
First off, let |=M A[[b, âƒ—an]], where b âˆˆM, and the âƒ—an are in K. For some
bâ€² picked by AC in (2), 
QA (âƒ—an, bâ€²) is true, that is, |=M A[[bâ€², âƒ—an]]. Since K is

QA-closed, bâ€² âˆˆK. Thus, the â‰º-criterion for K â‰ºM is met.
Finally, we compute the cardinality of K. This inductively deï¬ned set is
being built by stages (see I.2.9),
K =

iâ‰¥0
Ki
(3)
Atstage0wehave K0 = Y.Havingbuilt Ki, i â‰¤n,webuild Kn+1 byappending
to 
iâ‰¤n Ki all b such that 
QA (âƒ—ar, b) holds. We do so for all 
QA (âƒ—xr, y) and
all possible âƒ—ar in 
iâ‰¤n Ki.
We show that the cardinality of each Kn is â‰¤m. This is true for K0. We take
an I.H. for all n â‰¤k and consider Kk+1 next.
Now, the set of formulas over L has cardinality â‰¤m; hence
the set 
R as well has cardinality â‰¤m
(4)
We let S = 
iâ‰¤k Ki for convenience. By I.H.,
(cardinality of S) â‰¤(k + 1) Â· m = m
(5)
For each 
QA (âƒ—xr+1) âˆˆ
R , the cardinality of the set of contributed outputs,
taking inputs in S, is at most equal to the cardinality of Sr, i.e., â‰¤mr = m. The
total contribution of all the 
QA is then, by (4), of cardinality â‰¤m Â· m = m.
Thus, using (5), the cardinality of Kk+1 is â‰¤m + m = m. By (3), the cardinal
of K is â‰¤â„µ0 Â· m = m. Since it is also â‰¥m (by Y âŠ†K), we are done.
â–¡
N.B. The set K â€² that we have discarded earlier also satisï¬es Kâ€² = (K â€², . . . ) â‰º
M, and has cardinality at most that of M. We cannot expect though that it has
cardinality â‰¤m.
I.6.21 Corollary. Assume that L has cardinality at most m, while the structure
M = (M, I ) (for L) has cardinality at least m. Let X â‰ºM, and the cardinality
of |X| be â‰¤m. Then there is a structure K for L, of cardinality exactly m that
satisï¬es X â‰ºK â‰ºM.
Proof. Working with X = |X| exactly as in I.6.20, we getK. It is straightforward
to check that X âŠ†K.â€  The â‰º-criterion (I.6.18) for X â‰ºK is: Given A with free
â€  For n-ary f in L, f X = f Mâ†¾Xn by assumption. On the other hand, f K = f Mâ†¾K n by
construction of K. But X âŠ†K.

88
I. Basic Logic
variables among the y and âƒ—xn, and âƒ—an in X. Verify that if
for some b âˆˆK,
|=K A[[b, âƒ—an]]
(1)
then
|=K A[[bâ€², âƒ—an]]
for some bâ€² âˆˆX
(2)
Well, K âŠ†M and (1) yield |=M A[[b, âƒ—an]]. (2) follows from X â‰ºM.
â–¡
The following deï¬nition builds on I.5.4 and I.5.20.
I.6.22 Deï¬nition (Diagrams). Given a language L and a structure A for L. Let
âˆ…Ì¸= M âŠ†|A|. We call L(M), constructed as in I.5.20, the diagram language of
M. We denote by AM the expansion of A that has all else the same as A, except
that it gives the â€œnatural meaningâ€ to the new constants, i, of L(M). Namely,
i
AM= i for all i âˆˆM. AM is called the diagram expansion of A (over L(M)).
We often write AM = {|A|,. . . , (i)iâˆˆM} to indicate that constants i with
interpretations i where added. The part â€œ{|A|, . . . }â€ is just A.
If M = |A|, then we write L(A) for L(M), as in I.5.4.
The basic diagram of A, denoted by D(A), is the set of all atomic and
negated atomic sentences of L(A) that are true in A|A|.
The elementary diagram of A is just Th(A|A|).
Suppose now that A and M are as above, B is a structure for L as well â€“
possibly A = B â€“ and Ï† : M â†’|B| is a total function. We may now wish to
import the constants i of M into L, as i, and expand B so that, for each i âˆˆM,
i is interpreted as Ï†(i). This expansion of B is denoted by BÏ†[M].â€ 
Thus, all else in BÏ†[M] is the same as in B, except that i
BÏ†[M] = Ï†(i).
We often write BÏ†[M] = {|B|,. . . , (Ï†(i))iâˆˆM} to indicate that constants i
with interpretations Ï†(i) were added.
Therefore, if M âŠ†|B| and Ï† is the inclusion map, then BÏ†[M] is the expan-
sion of B, BM.
â–¡
The particular way that we have approached the assignment of semantics is not
the â€œusualâ€ one.â€¡ One often sees a manner of handling substitution of informal
constants into formal variables that differs from that of I.5.4. Namely, other
authors often do this by assigning (or â€œcorrespondingâ€) values to variables,
rather than substituting (formal names of) values into variables. This is achieved
by a so-called assignment function, s, from the set of all variables {v0, v1, . . . }
â€  In the choice of our notation we are following Keisler (1978).
â€¡ It is the same as the one in Shoenï¬eld (1967).

I.6. Substructures, Diagrams, and Applications
89
to the domain, |A|, of the structure into which we want to interpret a language
L. Intuitively, s(v j) = a says that we have plugged the value a into v j.
While we have not favoured this approach, we will grant it one thing: It does
not compel us to extend the language L into the diagram language L(A) to
effect the interpretation. Such extensions may be slightly confusing when we
deal with diagrams.
For example, let L and A be as above, with âˆ…Ì¸= M âŠ†|A|. We next form
L(M). Now, this language has new constants, i, for each i âˆˆM. Thus, when
we proceed to interpret it into AM, we already have formal counterparts of all
i âˆˆM in L(M), and we need only import the constants (as per I.5.4) of |A|âˆ’M.
But is there any harm done if we re-import all i âˆˆ|A|, asi, to form L(M)(A)?
Not really. Let us focus on an i âˆˆM which has been imported as i when forming
L(M) and then again asi to do what I.5.4 prescribes towards interpreting L(M)
in AM. Thus, i is a closed term t of L(M)(A) for which tAM = i. Sincei is the
imported name for i, Lemma I.5.11 yields that, for any term s and formula A
over L(M)(A) of at most one free variable x,
(s[x â†i])AM = (s[x â†i])AM
and
(A[x â†i])AM = (A[x â†i])AM
In short, meaning does not depend on which formal alias of i âˆˆM we use. We
will be free to assume then that thei were not new (for i âˆˆM); they were just
the i.
In particular, the statement â€œthe sentence A over L(A) is true in A|A|â€ is
equivalent to the statement â€œthe A-instanceâ€  A of a formula over L is true
in Aâ€, since a closed formula of L(A) is an A-instance of one over L and
conversely (i â‰¡i).
Diagrams offer more than one would expect from just nomenclature, as a
number of applications in the balance of this section will readily demonstrate.
First, we translate I.6.13, I.6.14, and I.6.15 into the diagram terminology.
I.6.23 Lemma (Main Diagram Lemma). Let M and K be two structures for
L, and Ï† : |M| â†’|K| be a total function. Then
(1) Ï† : M â†’K (embedding) iff |=KÏ†[|M|] D(M);
(2) Ï† : M â†’â‰ºK (elementary embedding) iff |=KÏ†[|M|] Th(M|M|).
Moreover, if |M| âŠ†|K|, then
â€  Cf. I.5.7, p. 55.

90
I. Basic Logic
(3) M âŠ†K (substructure) iff |=K|M| D(M);
(4) M â‰ºK (elementary substructure) iff |=K|M| Th(M|M|).
Proof. Direct translation of the facts quoted prior to the lemma statement. For
example, the hypothesis of the if part in item (1) above is the same as the
displayed formula in the statement of I.6.13 if we recall that, e.g., an atomic
sentence over L(M) is an M-instance A(i1,. . . , in) of an atomic formula A
over L and that |=M A[[âƒ—in]] is argot for

A(i1,. . . , in)
M = t (cf. I.5.17).
The only-if part in (1)â€“(4) is due to the remark following I.6.14, p. 82.
â–¡
We present a number of applications of diagrams. First, we revisit the upward
LÂ¨owenheim-Skolem theorem (I.5.43).
I.6.24 Theorem (Upward LÂ¨owenheim-Skolem Theorem, Version 2). Let A
be an inï¬nite structure for L. For every cardinal n that is not smaller than
the cardinal of A (that is, of |A|) and of L, there is a structure B for L, of
cardinality exactly n, such that A â‰ºB.
Proof. As in the proof of I.5.43, pick a set N of cardinality n. The set of
sentences over L(A)(N)
Q = Th(A|A|) âˆª{Â¬c = d : c Ì¸= d on N}
where c are distinct new constants, one for each c âˆˆN, is ï¬nitely satisï¬able.
This is because every ï¬nite subset of Q involves only ï¬nitely many of the
sentences Â¬c = d, and thus there is capacity in A = (A, I ) (as A is inï¬nite)
to extend I into I â€² (keeping A the same, but deï¬ning distinct cI â€², d
I â€²
, etc.)
to satisfy these sentences in an expanded structure Aâ€² = (A, I â€²).
By compactness, there is a model D = (D, I D) for Q
(1)
We will look at ï¬rst into its reduct on L(A), C = (D, I C) â€“ that is, I C =
I D â†¾L(A). C is a model of Th(A|A|).
We deï¬ne next a function f : A â†’D by f (nI ) = nI C. The function f
is total because all n âˆˆA have been imported, as n, in the language L(A) of
Th(A|A|).
Thus, if Câ€² = (D, I Câ€²) is the reduct of C on L â€“ that is, I Câ€² = I C â†¾L â€“ then
the expansion Câ€²
f [A] of Câ€² is just C, hence, |=Câ€²
f [A] Th(AA). By the main diagram
lemma above,
f : A â†’â‰ºCâ€²
(2)

I.6. Substructures, Diagrams, and Applications
91
Without loss of generality, we may assume in (2) that A âŠ†D (see Remark I.6.17)
and that f is the inclusion map (nI = nI C). That is,
A â‰ºCâ€²
Thus, it remains to settle the cardinality claims. First off, since c Ì¸= d in N
implies that Â¬c = d is true in D by (1), it follows that the cardinality of D
is â‰¥n.â€  That is, the cardinality of Câ€² (that is, of its domain D) is â‰¥n.
By the downward LÂ¨owenheim-Skolem theorem (in the form I.6.21) there is
a structure B of cardinality exactly n such that A â‰ºB â‰ºCâ€².
â–¡
Theorem I.6.24 provides different information than its counterpart, I.5.43. The
former ignores axioms and works with a language and its structures. In the pro-
cess it gives a strong type of extension between the given structure and the
constructed structure. The latter downplays structures and is about a language,
a set of axioms, and the size of the models that we can build for those axioms.
Version I.5.43 has the important consequence that theories over countable
languages that have any inï¬nite model at all â€“ such as ZFC â€“ must also have
modelsofanyinï¬nitecardinality.Inparticulartheymusthavecountablemodels.
We conclude the section with a further sampling of applications.
I.6.25 Deï¬nition. We call a theory T open iff it is the set of theorems of a set of
axioms  that consists entirely of open formulas, i.e., quantiï¬er-free formulas.
Such is, for example, group theory and ROB arithmetic (I.9.32). Such theories
are also called universal, since one can generate them with a different â€². The
latterâ€™s formulas are all the universal closures of open formulas, those in .
â–¡
Thus, by the completeness theorem, a theory T is open iff there is a set of open
formulas, , such that T and  have the same models.
We have the following model-theoretic result for open theories:
I.6.26 Theorem (Â´LoÂ´s-Tarski). A theory T (in the sense of p. 38) is open iff
every substructure of every model of T is also a model.
Proof. Only-if part: Exercise I.53.
If part: All this is over some language L. Let  be the set of all open theorems
of T (over L).â€¡ We need to show that T = Thm, or, for every structure M
â€  The function N âˆ‹n #â†’Â¯nI D âˆˆD is total and 1-1.
â€¡ According to the deï¬nition of a theory on p. 38, in particular the displayed formula (1) there,
â€œopen theorems of T â€ is synonymous with â€œopen formulas in T â€.

92
I. Basic Logic
for L,
|=M 
implies
|=M T
(1)
To this end, we assume the hypothesis in (1) and consider D(M) âˆªT . We
claim that this is a consistent set of formulas over L(M). Otherwise T âˆª
{A1,. . . , An} âŠ¢Â¬x = x, where A1,. . . , An is a ï¬nite set of sentences of
D(M). By the deduction theorem
T âŠ¢Â¬x = x âˆ¨Â¬A1 âˆ¨Â· Â· Â· âˆ¨Â¬An
Hence (tautological implication and logical axiom x = x)
T âŠ¢Â¬A1 âˆ¨Â· Â· Â· âˆ¨Â¬An
(2)
By the theorem on constants, (I.4.15) â€“ since T is over L â€“ (2) yields
T âŠ¢Â¬A1 âˆ¨Â· Â· Â· âˆ¨Â¬An
where Â¬A1 âˆ¨Â· Â· Â· âˆ¨Â¬An is obtained from Â¬A1 âˆ¨Â· Â· Â· âˆ¨Â¬An by replacing all
the imported constants i, j, . . . which appear in the latter formula by distinct
new variables. Thus, Â¬A1 âˆ¨Â· Â· Â· âˆ¨Â¬An is in  hence is valid in M. That is,
assuming the formulaâ€™s variables are among âƒ—yr, for all âƒ—ar in |M|,
|=M (Â¬A1 âˆ¨Â· Â· Â· âˆ¨Â¬An)[[âƒ—ar]]
(3)
We can thus choose the âƒ—ar so that A j â‰¡(A j)[[âƒ—ar]] for j = 1,. . . , n. Since
(3) entails that (A j)[[âƒ—ar]] is falseâ€  in M|M| for at least one j, this contradicts
that all the A j are in D(M).
Thus, by the consistency theorem, there is a model K for D(M) âˆªT .
Without loss of generality (cf. Remark I.6.17), the mapping i
M #â†’i
K is the
identity. If we now call A the reduct of K on L, then the expansion A|M| of A
is K; therefore |=A|M| D(M). The diagram lemma implies that M âŠ†A. Since
A is a model of T , then so is M by hypothesis. This settles (1).
â–¡
I.6.27 Example. Group theory is usually formulated in a language that employs
the nonlogical symbols â€œâ—¦â€ (binary function), â€œâˆ’1â€ (unary function) and â€œ1â€
(constant). Its axioms are open, the following:
(1) x â—¦(y â—¦z) = (x â—¦y) â—¦zâ€¡;
(2) x â—¦1 = 1 â—¦x = x (using â€œ=â€ conjunctionally);
(3) x â—¦xâˆ’1 = xâˆ’1 â—¦x = 1.
â€  Cf. discussion on p. 89.
â€¡ Where we are using the habitual inï¬x notation, â€œx â—¦yâ€, rather than â—¦xy or â—¦(x, y).

I.6. Substructures, Diagrams, and Applications
93
The models of the above axioms are called groups. Over this language, every
substructure of a group, that is, every subset that includes (the interpretation
of) 1 and is closed under â—¦and âˆ’1, is itself a group, as the only-if direction of
the above theorem guarantees.
It is possible to formulate group theory in a language that has only â€œâ—¦â€ and
â€œ1â€ as nonlogical symbols. Its axioms are not open:
(i) x â—¦(y â—¦z) = (x â—¦y) â—¦z;
(ii) x â—¦1 = 1 â—¦x = x;
(iii) (âˆƒy)x â—¦y = 1;
(iv) (âˆƒy)y â—¦x = 1.
Since the Â´LoÂ´s-Tarski theorem is an â€œiffâ€ theorem, it must be that group theory
so formulated has models (still called groups) that have substructures that are
not groups (they are semigroups with unit, however).
â–¡
I.6.28 Deï¬nition. Constructions in model theory often result in increasing
sequences of structures, either of the type
M0 âŠ†M1 âŠ†M2 âŠ†Â· Â· Â·
(1)
or of the type
M0 â‰ºM1 â‰ºM2 â‰ºÂ· Â· Â·
(2)
Sequences of type (1) are called increasing chains or ascending chains of
structures. Those of type (2) are called elementary chains.
â–¡
Chains are related to theories that can be axiomatized using exclusively exis-
tential axioms, that is, axioms of the form (âˆƒx1) . . . (âˆƒxn)A, for n â‰¥0,â€  where
A is open. Such theories are called inductive. By taking the universal closures
of existential axioms we obtain formulas of the form (âˆ€y1) . . . (âˆ€ym)(âˆƒx1) . . .
(âˆƒxn)A. For this reason, inductive theories are also called âˆ€âˆƒ-theories.
The following result is easy to prove, and is left as an exercise.
I.6.29 Theorem (Tarskiâ€™s Lemma). The union M of an elementary chain (2)
above is an elementary extension of every Mi.
â€  n = 0 means that the preï¬x (âˆƒx1) . . . (âˆƒxn) is missing. Thus an open formula is a special case
of an existential one. As a matter of fact, we can force a nonempty existential preï¬x on an open
formula: If z does not occur in A, then we can be prove A â†”(âˆƒz)A without nonlogical axioms.

94
I. Basic Logic
Proof. First off, a chain of type (2) is also of type (1); thus, Mi âŠ†Mi+1 for all
i â‰¥0. Let M and Mi denote the universes of M and Mi respectively. Then by
â€œunion of the Miâ€ we understand that we have taken M = 
iâ‰¥0 Mi and
(i) PM = 
iâ‰¥0 PMi, for each predicate P,
(ii) f M = 
iâ‰¥0 f Mi for each function f (note that f Mi âŠ†f Mi+1 for all
i â‰¥0), and
(iii) aM = aM0, for each constant.
The real work is delegated to Exercise I.54.
â–¡
I.6.30 Theorem (Chang-Â´LoÂ´s-Suszko). The union of every type-(1) chain
(I.6.28) of models of a theory T (the latter in the sense of p. 38) is a model of
T iff the theory is inductive.
Proof. The proof is standard (e.g., Chang and Keisler (1973), Keisler (1978),
Shoenï¬ed (1967)).
If part: Delegated to Exercise I.55.
Only-if part: Assume the hypothesis for a theory T over L. We will prove
that it is inductive. To this end, let  by the set of all existential consequences
of T â€“ i.e., formulas (âˆƒx1) . . . (âˆƒxn)A â€“ n â‰¥0 â€“ with A open. As in the proof
of I.6.26, we endeavour to prove
|=M 
implies
|=M T
(1)
Assume the hypothesis in (1), and let Dâˆ€(M) denote the set of all universal
sentencesâ€  over L(M) that are true in MM, where we have written M for |M|.
We argue that Dâˆ€(M) âˆªT
is consistent. If not, T
âŠ¢Â¬x = x âˆ¨Â¬
A1 âˆ¨Â· Â· Â· âˆ¨Â¬An, for some Ai âˆˆDâˆ€(M); hence (notation as in the proof
of I.6.26)
T âŠ¢Â¬(A1 âˆ§Â· Â· Â· âˆ§An)
Since âŠ¢A â†”(âˆ€z)A if z is not free in A, and âˆ€distributes over âˆ§(see
Exercise I.23), we can rewrite the above as
T âŠ¢Â¬(âˆ€âƒ—xr)(B 1 âˆ§Â· Â· Â· âˆ§B n)
(2)
where the B i are open over L, and (âˆ€âƒ—xr) is short for (âˆ€x1) . . . (âˆ€xr). The formula
in (2) is (logically equivalent to one that is) existential. Thus, it is in , and
hence is true in M, an impossibility, since no Ai âˆˆDâˆ€(M) can be false in MM.
â€  A universal formula has the form (âˆ€x1) . . . (âˆ€xn)A, n â‰¥0, where A is open. Thus (n = 0) every
open formula is also universal, as well as existential.

I.6. Substructures, Diagrams, and Applications
95
We can now have a model, Kâ€² = (K,. . . , (a)aâˆˆM), of Dâˆ€(M) âˆªT , where
â€œ(a)aâˆˆMâ€ indicates the special status of the constants a âˆˆM âŠ†K: These have
been added to L, as a, to form L(M). If K = (K, . . . ) is the reduct of Kâ€² to L,
then Kâ€² = KM; hence
|=KM Dâˆ€(M)
(3)
Since D(M) âŠ†Dâˆ€(M), we obtain
M âŠ†K
(I.6.23),
and
|=K T
(T is over L)
(4)
Remark I.6.17 was invoked without notice in assuming that the a of L(M) are
interpreted in Kâ€² exactly as in MM: as a.
We next argue that the theory S = D(K) âˆªTh(MM) overâ€  L(K) is consis-
tent, where D(K) is as in I.6.22, i.e.,the set of all atomic and negated atomic
sentences over L(K) that are true in KK = (K,. . . , (a)aâˆˆM, (a)aâˆˆKâˆ’M). Note
that L(K) is obtained by adding to L(M) the constants (a)aâˆˆKâˆ’M, each a in-
terpreted as a in KK. That is, a âˆˆM were not re-imported as something other
than the original a of L(M).
Now, if S is not consistent, then some Ai âˆˆD(K) (i = 1,. . . , n) jointly
with Th(MM) prove Â¬x = x. Since the Ai are sentences (over L(K)), the
usual technique yields
Th(MM) âŠ¢Â¬A1 âˆ¨Â· Â· Â· âˆ¨Â¬An
Let Â¬Aâ€²
1 âˆ¨Â· Â· Â·âˆ¨Â¬Aâ€²
n be obtained from Â¬A1 âˆ¨Â· Â· Â·âˆ¨Â¬An by replacing all its
constants a, b, . . . â€“ where a, b, . . . are in K âˆ’M â€“ by distinct new variables
âƒ—xr. Thus (I.4.15)
Th(MM) âŠ¢(âˆ€âƒ—xr)(Â¬Aâ€²
1 âˆ¨Â· Â· Â· âˆ¨Â¬Aâ€²
n)
(5)
Since (âˆ€âƒ—xr)(Â¬Aâ€²
1 âˆ¨Â· Â· Â· âˆ¨Â¬Aâ€²
n) is a sentence over L(M), (5) implies that it is
in Th(MM), i.e., true in MM. Thus, being universal, it is in Dâˆ€(M). By (3) it is
true in KM. In particular, its KM-instance (cf. I.5.7) Â¬A1 âˆ¨Â· Â· Â· âˆ¨Â¬ An â€“ a
sentence of L(K) â€“ is true in KM in the sense of I.5.7, and hence also in KK.
This is impossible, since Ai âˆˆD(K) implies that no sentence Ai can be false
in KK.
Thus, there is a model A = (A, . . . ) of S over L(K). Now, on one hand we
have
|=A D(K)
(6)
â€  The part Th(MM) is over L(M), of course.

96
I. Basic Logic
and we may assume, without loss of generality (cf. I.6.17), that
for a âˆˆK,
aK #â†’aA is the inclusion map
(7)
Therefore, K âŠ†A, and we may write A a bit more explicitly as
A = (A,. . . , (a)aâˆˆM, (a)aâˆˆKâˆ’M)
(8)
Let us call Mâ€² = (A, . . . ) the reduct of A on L. By (8), A = Mâ€²K, so that
(6) reads |=Mâ€² K D(K); hence (by I.6.23)
K âŠ†Mâ€²
(9)
On the other hand, every sentence of Th(MM) â€“ a subtheory of S â€“ is true in
A. The relevant part of A is its reduct on L(M), namely, Mâ€²M. Thus, |=M â€²
M
Th(MM), and therefore
M â‰ºMâ€²
(10)
After all this work we concentrate on the core of what we have obtained: (4),
(9), and (10). By (10) and remarks in I.6.16 we have that |=M â€² . Thus we can
repeat our construction all over again, using Mâ€² in the place of M, to obtain
Kâ€² âŠ‡Mâ€² with |=Kâ€² T , and also a Mâ€²â€² that is an elementary extension of Mâ€².
In short, we can build, by induction on n, an alternating chain
M0 âŠ†K0 âŠ†M1 âŠ†K1 âŠ†Â· Â· Â·
(11)
such that
M0 â‰ºM1 â‰ºÂ· Â· Â·
(12)
and
|=Kn T
for
n â‰¥0
(13)
where M0 = M, and, for n â‰¥0, Kn and Mn+1 are obtained as â€œKâ€ and â€œMâ€²â€
respectively from Mn, the latter posing as the â€œMâ€ in the above construction.
By assumption and (13), the union of the alternating chain (11) is a model, B,
of T . Since B also equals the union of the chain (12), Tarskiâ€™s lemma (I.6.29)
yields M â‰ºB. Hence |=M T , proving (1).
â–¡
Some issues on the cardinality of models are addressed by the upward and
downward LÂ¨owenheim-Skolem theorems. Here we sample the connection be-
tween possible uniqueness of models of a given cardinality and completeness.

I.6. Substructures, Diagrams, and Applications
97
As before, a theory T over some language L is (simply) complete iff for all
sentences A, one of T âŠ¢A or T âŠ¢Â¬A holds. If T is in the sense of p. 38,
then it is complete iff for all sentences A, one of A âˆˆT or (Â¬A) âˆˆT
holds.
I.6.31 Deï¬nition. A theory T over a language L is Îº-categorical (where Îº â‰¥
cardinality of L) iff any two models of T of cardinality Îº are isomorphic. In
essence, there is only one model of cardinality Îº.
â–¡
I.6.32 Theorem (Â´LoÂ´s-Vaught Test). If T over L has no ï¬nite models and is
Îº-categorical for some Îº, then it is complete.
Proof. The hypothesis is vacuously satisï¬ed by an inconsistent theory. If T
is inconsistent, then it is complete. Let it be consistent then, and assume that
there is some sentence A that is neither provable nor refutable. Then both
T âˆª{A} and T âˆª{Â¬A} are consistent. Let A and B be respective models.
By the upward LÂ¨owenheim-Skolem theorem there are structures Aâ€² and Bâ€² of
cardinality Îº each such that A â‰ºAâ€² and B â‰ºBâ€²; hence (by assumption and
remarks in I.6.16)
A â‰¡Aâ€² âˆ¼= Bâ€² â‰¡B
Thus A is true in B (since it is so in A), which is absurd.
â–¡
Our last application is perhaps the most startling. It is the rigorous intro-
duction of Leibnizâ€™s inï¬nitesimals, that is, â€œquantitiesâ€ that are non-zero,â€  yet
their absolute values are less than every positive real number. Inï¬nitesimals
form the core of a user-friendly (sans Îµ-Î´, that is) introduction to limits and
the differential and integral calculus â€“ in the manner that Leibniz conceived
calculus (but he was never able to prove that inï¬nitesimals â€œexistedâ€). Such
approaches to learning calculus (sometimes under the title non-standard analy-
sis) have been accessible to the undergraduate student for quite some time now
(e.g., Henle and Kleinberg (1980); see also Keisler (1976)). The legitimization
of inï¬nitesimals was ï¬rst announced in 1961 by Abraham Robinson (1961,
1966). Besides making calculus more â€œnaturalâ€, non-standard analysis has also
been responsible for the discovery of new results.
All that we need to do now is to extend the standard structure for the reals,
R = (R, . . . ) â€“ where â€˜â€˜ . . . â€ includes â€œ+â€, â€œÃ—â€, â€œ<â€, â€œ0â€, â€œ1â€, etc. â€“ so that
it is enriched with â€œnewâ€ numbers, including inï¬nitesimals. Given the tools at
our disposal, this will be surprisingly easy:
â€  Actually, 0 is an inï¬nitesimal, as it will turn out. But there are uncountably many that are non-zero.

98
I. Basic Logic
Thus, we start by extending the ï¬rst order language for the reals, by importing
all constants, all functions, and all relations over R into the language as formal
constants, functions, and predicates respectively.
There are two issues we want to be clear about.
(1) In the exposition that follows we will almost exclusively use (an argot
of) the formal notation for the extended language of the reals â€“ and, in-
frequently, informal notations for concrete objects (e.g., reals). We are
thus best advised to keep the formal language notation uncomplicated, and
complicate â€“ if we must â€“ only the notation for the concrete objects. Thus,
e.g., we will denote by â€œ<â€ the formal, and by â€œâ—¦<â€ the concrete, predicate
on the reals; by â€œ
âˆš
2â€ the formal, and by â€œâ—¦âˆš
2â€ the concrete constant.â€  In
short, we import a, f, P for all informal â—¦a âˆˆR, for all functions â—¦f with
inputs and outputs in R, and for all relations â—¦P on R. Having done that,
we interpret as follows: aR = â—¦a, f R = â—¦f , and PR = â—¦P. We will call
the language so obtained, simply, L. Due to lack of imagination, we will
call the expansion of the original R still R.â€¡ Thus
R = (R,. . . , (â—¦a : â—¦a âˆˆR), (â—¦f : â—¦f on R), (â—¦P : â—¦P on R))
(i)
(2) Not all functions on R are total (i.e., everywhere deï¬ned). For example, the
function x #â†’âˆšx is undeï¬ned for x < 0. This creates a minor annoyance
because functions of a ï¬rst order language are supposed to be interpreted as
(i.e., are supposed to name) total functions. We get around this by simply
not importing (names for) nontotal functions. However, we do import their
graphs,Â§ for these are just relations. All we have to do is to invent appropriate
notation for the formal name of such graphs. For example, suppose we want
toformallynameâ€œy = â—¦âˆšxâ€.Wejustuse y = âˆšx.Thus,theformal(binary)
predicate name is the compound symbol â€œ= âˆšâ€, which we employ in inï¬x
notation when building an atomic formula from it. See also I.6.37 later in
this section (p. 101).
With the above out of the way, we ï¬x a cardinal m that is bigger than the
maximum of the cardinal of L and R. By the upward LÂ¨owenheim-Skolem
â€  There is nothing odd about this. Both â€œ
âˆš
2â€ and â€œâ—¦âˆš
2â€ are names. Unlike what we do in normal
use, we pretend here that the former is the formal name, but the latter is the informal name for
the object called the â€œsquare root of twoâ€.
â€¡ No confusion should arise from this, for neither the original bare bones R nor its language â€“
which we havenâ€™t named â€“ will be of any future use.
Â§ The graph of an n-ary function â—¦f is the relation {âŸ¨y, âƒ—xnâŸ©: y = â—¦f (âƒ—xn) is true in R}, or simply,
y = â—¦f (âƒ—xn).

I.6. Substructures, Diagrams, and Applications
99
theorem there is a structure
âˆ—R = (âˆ—R, . . . , (âˆ—a : â—¦a âˆˆR), (âˆ—f : â—¦f on R), (âˆ—P : â—¦P on R))
(ii)
for L such that
R â‰ºâˆ—R
(1)
and
(cardinality of âˆ—R) = m
(2)
In particular (from (1) and (2)),
R âŠ‚âˆ—R
(3)
I.6.33 Deï¬nition. The members of âˆ—R are called the hyperreals or hyperreal
numbers. By (3), all reals are also hyperreals. The members of âˆ—R âˆ’R â€“ a non-
empty set by (3) â€“ are the non-standard numbers or non-standard hyperreals.
The members of R are the standard numbers or standard hyperreals.
â–¡
(1) says that a ï¬rst order sentence is true in R iff it is true in âˆ—R. This is neat. It
gives us a practical transfer principle (as it is called in Keisler (1982), although
in a different formulation): To verify a ï¬rst order sentence in âˆ—R (respectively,
in R) one only needs to verify it in R (respectively, in âˆ—R).
By (1) (which implies R âŠ†âˆ—R) we also have
â—¦a = âˆ—a
for all
â—¦a âˆˆR
(4)
â—¦f âŠ†âˆ—f
for all
â—¦f on R
(5)
and
â—¦P âŠ†âˆ—P
for all
â—¦P on R
(6)
I.6.34 Example. Here is an example of how (4) and (6) apply:
(âˆ€x)(x < 0 â†’Â¬(âˆƒy)(y = âˆšx))
(7)
is true in R. Hence it is true in âˆ—R. Moreover, (âˆ’6) < 0 is true in âˆ—R, since it
is true in R.
Pause. We have used formal notation in writing the two previous formulas.
By (7) and specialization, (âˆ’6) < 0 â†’Â¬(âˆƒy)(y = âˆš(âˆ’6)). Thus,
Â¬(âˆƒy)(y =

(âˆ’6))

100
I. Basic Logic
is true in âˆ—R. Concretely this says that if we are working in the set âˆ—R, then
(using x #â†’âˆ—âˆšx for the concrete function on âˆ—R, and also using (4))
âˆ—
â—¦(âˆ’6) is undeï¬ned
So, nontotal functions continue (i.e., their extensions on âˆ—R do) being undeï¬ned
on the same reals on which they failed to be deï¬ned on R.
The reader can similarly verify that âˆ—R will not forgive division by zero,
using, for example, the formal sentence Â¬(âˆƒy)(y Â· 0 = 1).
â–¡
I.6.35 Example. Now start with the sentence (written formally!) sin(Ï€) = 0.
This is true in R, and hence in âˆ—R. A more messy way to say this is to say that by
(5) (note that â—¦sin is total; hence the name sin was imported), â—¦sin âŠ†âˆ—sin, and
hence (cf. (4)) âˆ—sin(âˆ—Ï€) = âˆ—sin(â—¦Ï€) = â—¦sin(â—¦Ï€) = â—¦0 = âˆ—0. Thus, sin(Ï€) = 0,
the formal counterpart of âˆ—sin(âˆ—Ï€) = âˆ—0, is true in âˆ—R.
â–¡
Elementary arithmetic on R carries over on âˆ—R without change. Below is a
representative sample.
I.6.36 Example. First off, âˆ—< is a total order on âˆ—R. That is, the following
three sentences are true in âˆ—R (because they are so in R):
(âˆ€x)(Â¬x < x)
(âˆ€x)(âˆ€y)(x < y âˆ¨y < x âˆ¨x = y)
and
(âˆ€x)(âˆ€y)(âˆ€z)(x < y âˆ§y < z â†’x < z)
The formal symbol â‰¤, as usual, is introduced by a deï¬nition
x â‰¤y â†”x = y âˆ¨x < y
Can we add inequalities, term by term, in âˆ—R? Of course, since
(âˆ€x)(âˆ€y)(âˆ€z)(âˆ€w)(x < y âˆ§z < w â†’x + z < y + w)
is true in R therefore in âˆ—R as well.
The above has all sorts of obvious variations using, selectively, â‰¤instead of
<. Multiplication, term by term, goes through in âˆ—R under the usual caveat:
(âˆ€x)(âˆ€y)(âˆ€z)(âˆ€w)(0 â‰¤x < y âˆ§0 â‰¤z < w â†’xz < yw)

I.6. Substructures, Diagrams, and Applications
101
Two more useful inequalities are
(âˆ€x)(âˆ€y)(âˆ€z)(z < 0 âˆ§x < y â†’zx > zy)
and
(âˆ€x)(âˆ€y)((0 < x âˆ§x < 1/y) â†”(0 < y âˆ§y < 1/x))
The formal function | . . . | (absolute value) enjoys the usual properties in âˆ—R
because it does so in R. Here is a useful sample:
(âˆ€x)(âˆ€y)(|x + y| â‰¤|x| + |y|)
(âˆ€x)(âˆ€y)(|x âˆ’y| â‰¤|x| + |y|)
(âˆ€x)(âˆ€y)(|x| â‰¤y â†”âˆ’y â‰¤x â‰¤y)
Back to equalities: + and Ã— are, of course, commutative on âˆ—R, for (âˆ€x)(âˆ€y)(x+
y = y + x) is true in R. Moreover,
(âˆ€x)(âˆ€y)(|xy| = |x||y|)
where we have used â€œimplied Ã—â€ above. Finally, the following are true in âˆ—R:
(âˆ€x)(0x = 0), (âˆ€x)(1x = x), and (âˆ€x)((âˆ’1)x = âˆ’x).
â–¡
I.6.37 Remark (ImportingFunctions: theLast Word). We can now conclude
our discussion, on p. 98, about importing nontotal functions. Let then â—¦f be an
n-ary nontotal function on R. The graph, y = â—¦f (âƒ—xn) is imported as the formal
predicate â€œ(= f )â€ â€“ we write, formally, â€œy(= f )âƒ—xnâ€ rather than â€œy = f âƒ—xnâ€ or
â€œy = f (âƒ—xn)â€.â€  Let y = âˆ—f (âƒ—xn) be the extension of y = â—¦f (âƒ—xn) on âˆ—R (cf. (6),
p. 99). We note two things:
One, for any b, âƒ—an in R, if b = â—¦f (âƒ—an), then b = âˆ—f (âƒ—an) by (6) and (4)
(p. 99).
Two, since
(âˆ€x1) . . . (âˆ€xn)(âˆ€y)(âˆ€z)(y(= f )âƒ—xn âˆ§z(= f )âƒ—xn â†’y = z)
is true in R by the assumption that â—¦f is a function â€“ i.e., that the relation
y = â—¦f (âƒ—xn) is single-valued in y â€“ it is true in âˆ—R as well, so that the concrete
relation y = âˆ—f (âƒ—xn) is single-valued in y. That is, âˆ—f is a function. Along with
remark one, above, this yields â—¦f âŠ†âˆ—f . This is the counterpart of (5) (p. 99)
â€  This avoids a possible misunderstanding that, in something like y = f âƒ—xn, f is an n-ary function
letter and hence f âƒ—xn is a term. f is part of a language symbol, namely, of the predicate â€œ(= f )â€.
It is not a symbol itself.

102
I. Basic Logic
for nontotal functions, and we are pleased to note that it has exactly the form
of (5), without any caveats.
Pause. Is âˆ—f total on âˆ—R?
We can now pretend, in practice (i.e., in argot) that all functions of R, total
or not, have been imported and thus have extensions in âˆ—R.
â–¡
We next explore âˆ—R looking for strange numbers, such as inï¬nitesimals and
inï¬nite numbers. To ensure that we know what we are looking for, we deï¬ne:
I.6.38 Deï¬nition (Inï¬nitesimals). An inï¬nitesimal h is a member of âˆ—R such
that |h| < x is true in âˆ—R for all positive x âˆˆR. A member h of âˆ—R is a ï¬nite
hyperreal iff |h| â‰¤x is true in âˆ—R for some positive x âˆˆR. A member h of âˆ—R
is an inï¬nite hyperreal iff it is not ï¬nite. That is, |h| > x is true in âˆ—R for all
positive x âˆˆR.
â–¡
I.6.39 Remark. Thus, 0 is an inï¬nitesimal, and every inï¬nitesimal is ï¬nite.
The reader has surely noticed that we have dropped the annoying left-
superscripting of members of âˆ—R (or of R). This is partly because of (4) (p. 99).
Since â—¦a = âˆ—a for all â—¦a âˆˆR, it is smart to name both by the simpler formal
name, a. Moreover, since the left-superscript notation originates in the structure
R, it is not applicable to objects of âˆ—R unless these are extensions (functions
or predicates) of objects of R. It is thus pointless to write â€œâˆ—hâ€.
Even in the cases of functions and predicates, we can usually get away
without superscripts, letting the context indicate whether we are in R, in âˆ—R, or
in the formal language. For example we have used â€œ|h|â€ rather than â€œâˆ—|h|â€, but
then we are clear that it is the latter that we are talking about, as the absolute
value here is applicable to non-standard inputs.
â–¡
I.6.40 Proposition. h âˆˆâˆ—R is a nonzero inï¬nitesimal iff 1/|h| is inï¬nite.
Proof. If part: Suppose that 1/|h| is inï¬nite, and let 0 < r âˆˆR be arbitrary.
Then 1/r < 1/|h|. Specialization of the appropriate inequality in I.6.36 (a true
fact in âˆ—R) yields |h| < r.
Only-if part: Let 0 < r âˆˆR be arbitrary. By hypothesis, |h| < 1/r. By the
fact invoked above, r < 1/|h|.
â–¡
I.6.41 Proposition. Let h and hâ€² be inï¬nitesimals. Then so are
(1) h + hâ€²,
(2) hhâ€²,
(3) hr for any r âˆˆR.

I.6. Substructures, Diagrams, and Applications
103
Proof. (1): Let 0 < r âˆˆR be arbitrary. Then |h + hâ€²| â‰¤|h| + |hâ€²| < r/2 +
r/2 < r is true in âˆ—R.
(2): Let 0 < r âˆˆR be arbitrary. Then |hhâ€²| = |h||hâ€²| < r1 = r is true in
âˆ—R.
(3): If r = 0, then h0 = 0 (cf. I.6.36), an inï¬nitesimal. Otherwise, let 0 < s âˆˆ
R be arbitrary. |h| < s/|r| by hypothesis; hence |hr| = |h||r| < s (the reader
can easily verify that we have used legitimate âˆ—R-arithmetic).
â–¡
I.6.42 Proposition. Let h and hâ€² be inï¬nite hyperreals. Then so are
(1) hhâ€²,
(2) hr for any 0 Ì¸= r âˆˆR,
(3) h + r for any r âˆˆR.
The prudence to ask that 0 Ì¸= r in (2) stems from the concluding remarks
in I.6.36.
Proof. (1): Let 0 < r âˆˆR be arbitrary. Then |hhâ€²| = |h||hâ€²| > r1 = r is true
in âˆ—R.
(2): Let 0 < s âˆˆR be arbitrary. |h| > s/|r| by hypothesis; hence |hr| =
|h||r| > s.
(3): Let 0 < s âˆˆR. Now, |h + r| â‰¥|h| âˆ’|r|.
Pause. Do you believe this?
Hence, s < |h + r|, since s + |r| < |h|.
â–¡
I.6.43 Example. We observe the phenomenon of indeterminate forms familiar
from elementary calculus. There we use the following symbols:
(i) Form âˆâˆ’âˆ. This translates into â€œthere is no ï¬xed rule for what hâˆ’hâ€² will
yieldâ€ (both positive inï¬nite). For example, if h = 2hâ€² (certainly inï¬nite,
by (2)), then h âˆ’hâ€² is inï¬nite. If a is an inï¬nitesimal, then hâ€² +a is inï¬nite.
Pause. Is that true?
Hence, if h = hâ€² + a, then h âˆ’hâ€² = a is an inï¬nitesimal. In particular, it
is ï¬nite.
(ii) Form âˆ/âˆ. There are three different outcomes for h/hâ€², according as
h = hâ€², h = (hâ€²)2 (see (1) above), or hâ€² = h2.
(iii) Form 0/0. This translates to the question â€œwhat is the result of h/hâ€², if
both are inï¬nitesimal?â€ It depends: Typical cases are h = hâ€², h = (hâ€²)2,
and hâ€² = h2.
â–¡

104
I. Basic Logic
The following terminology and notation are useful. They are at the heart of
the limiting processes.
I.6.44 Deï¬nition. We say that two hyperreals a and b are inï¬nitely close, in
symbols a â‰ˆb, iff a âˆ’b is an inï¬nitesimal.
Thus, in particular (since a âˆ’0 = a is true), a â‰ˆ0 says that a is an inï¬nites-
imal.
â–¡
I.6.45 Proposition. â‰ˆis an equivalence relation on âˆ—R. That is, for all x, y, z,
it satisï¬es
(1) x â‰ˆx,
(2) x â‰ˆy â†’y â‰ˆx,
(3) x â‰ˆy â‰ˆz â†’x â‰ˆz.
Proof. Exercise I.59.
â–¡
I.6.46 Proposition. If r â‰ˆs and r and s are real, then r = s.
Proof. r âˆ’s â‰ˆ0 and r âˆ’s âˆˆR. But, trivially, 0 is the only real inï¬nitesimal.
â–¡
I.6.47 Theorem (Main Theorem). For any ï¬nite non-standard number h there
is a unique real r such that h â‰ˆr.
Throughout the following proof we use superscriptless notation. In each case
it is clear where we are: In L, in R, or in âˆ—R.
Proof. Uniqueness is by I.6.45 and I.6.46. For the existence part let |h| â‰¤b,
0 < b âˆˆR, and deï¬ne
H = {x âˆˆR : x < h}
(1)
H, a subset of R, is bounded above by b: Indeed, in âˆ—R, and for any ï¬xed x âˆˆH,
x < h â‰¤b holds. Hence (cf. I.6.36), still in âˆ—R, x < b holds. Then it is true
in R as well (cf. p. 99, (4) and (6)). Let r âˆˆR be the least upper bound of H
(over the reals, least upper bounds of sets bounded above exist). We now argue
that
h â‰ˆr
(2)

I.6. Substructures, Diagrams, and Applications
105
Suppose that (2) is false. Then there is a s > 0, in R, such thatâ€ 
s â‰¤|h âˆ’r| is true in âˆ—R
(3)
There are two cases:
Case |h âˆ’r| = h âˆ’r. Then (3) implies s + r â‰¤h. Since s + r is standard,
but h is not, we have s +r < h hence s +r âˆˆH, and thus s +r â‰¤r from
the choice of r (upper bound), that is, s â‰¤0, contrary to the choice of s.
Case |h âˆ’r| = r âˆ’h. Then (3) implies h â‰¤r âˆ’s. Thus r âˆ’s is an upper
bound of H (in R), contrary to choice of r (least upper bound, yet r âˆ’
s < r).
â–¡
It is unreasonable to expect that an inï¬nite number h is inï¬nitely close to a real
r, for if that were possible, then h âˆ’r = a â‰ˆ0. But r + a is ï¬nite (right?).
I.6.48 Deï¬nition (Standard Parts). Let h be a ï¬nite hyperreal. The unique
real r such that h â‰ˆr is called the standard part of h. We write st(h) = r.
â–¡
I.6.49 Example. For any real r, st(r) = r. This is by r â‰ˆr and uniqueness
of standard parts. Also, since h is an inï¬nitesimal iff h â‰ˆ0, then h is an
inï¬nitesimal iff st(h) = 0.
â–¡
We can now prove that inï¬nitesimals and inï¬nite numbers exist in great
abundance.
I.6.50 Theorem. There is at least one, and hence there are uncountably many,
non-standard inï¬nitesimals.
Proof. Pick an h âˆˆâˆ—R âˆ’R (cf. (3) preceding Deï¬nition I.6.33). If it is inï¬nite,
then 1/|h| is an inï¬nitesimal (by I.6.40). It is also nonzero, and hence non-
standard (0 is the only standard inï¬nitesimal).
Pause. Â¬(âˆƒx)(1 = x0) is true in R.
If |h| is ï¬nite, then st(h) exists and a = hâˆ’st(h) â‰ˆ0. If a âˆˆR, so is h =
st(h) + a.
Why uncountably many? Well, ï¬x a non-standard inï¬nitesimal h. The func-
tion r #â†’rh is a 1-1, total function on the reals.â€¡ Thus, its range, {rh : r âˆˆR}
is in 1-1 correspondence with R, and hence is uncountable.
â–¡
â€  Letussettleaninconsistencyinterminology:Weuseâ€œtrueinRâ€(withorwithouttheâˆ—superscript)
synonymously with â€œtrue in Râ€
â€¡ 1-1 because rh = râ€²h â†’r = râ€², since h Ì¸= 0 â€“ being non-standard â€“ and the sentence
(âˆ€x)(âˆ€y)(âˆ€z)(z Ì¸= 0 â†’xz = yz â†’x = y) is true in R.

106
I. Basic Logic
I.6.51 Corollary. There is at least one, and hence there are uncountably many,
inï¬nite numbers.
I.6.52 Remark. By the preceding results, every ï¬nite hyperreal h has the form
st(h) + a, where a â‰ˆ0. a = 0 iff h âˆˆR. For such hyperreals, h â‰ˆst(h).
Conversely, any pair r (real) and a â‰ˆ0 leads to a hyperreal h = r + a such
that st(h) = r (since r + a â‰ˆr).
Thus, if we were to depict the set âˆ—R on a line, in analogy with the R-line,
we could start with the latter, then stretch it and insert nonstandard numbers
(respecting order, âˆ—<) so that each real r lies in a â€œcloudâ€ of nonstandard
numbers that are inï¬nitely close to r. Then each such cloudâ€  contains only one
real; for r â‰ˆh â‰ˆrâ€² with r,râ€² real implies r â‰ˆrâ€² and hence r = râ€². The cloud
in which 0 lies is the set of all inï¬nitesimals.
We then add all the positive inï¬nite numbers to the right end of the line
(again, respecting order, âˆ—<) and all the negative inï¬nite numbers to the left
end of the line.
â–¡
The deï¬nition that a function f is continuous that we will eventually give
essentially requires that the standard part function st, commute with f . As a
prelude towards that we present a proposition below that deals with the special
cases of addition, multiplication, and a few other elementary functions.
But ï¬rst, the non-standard counterpart to the pinching lemma of calculus.â€¡
I.6.53 Lemma (Pinching Lemma). If 0 < h < hâ€² and hâ€² â‰ˆ0, then h â‰ˆ0.
Either or both of â€œ<â€ can be replaced by â€œâ‰¤â€.
Proof. Exercise I.60.
â–¡
I.6.54 Corollary. If a < b < c and a â‰ˆc, then a â‰ˆb and b â‰ˆc. Moreover,
this remains true if we replace one or both of â€œ<â€ by â€œâ‰¤â€.
Proof. 0 < b âˆ’a < c âˆ’a.
â–¡
I.6.55 Proposition (Elementary Algebra of st). Throughout, a and b are ï¬xed
ï¬nite hyperreals.
(1) a â‰¥0 implies st(a) â‰¥0.
(2) st(a + b) = st(a) + st(b).
(3) st(a âˆ’b) = st(a) âˆ’st(b).
â€  The cloud for r âˆˆR is {r + a : a â‰ˆ0}. Of course, if a < 0, then r + a < r, while if a > 0, then
r + a > r.
â€¡ If 0 < g(x) < f (x) for all x in an open interval (a, b), c âˆˆ(a, b), and limxâ†’c f (x) = 0, then
also limxâ†’c g(x) = 0.

I.6. Substructures, Diagrams, and Applications
107
(4) st(a Â· b) = st(a) Â· st(b).
(5) If st(b) Ì¸= 0, then st(a/b) = st(a)/st(b).
(6) st(an) = st(a)n for all n âˆˆN.
(7) If st(a) Ì¸= 0, then st(aâˆ’n) = st(a)âˆ’n for all 0 < n âˆˆN.
(8) st(a1/n) = st(a)1/n for all 0 < n âˆˆN (a â‰¥0 assumed for n even).
Proof. We sample a few cases and leave the rest as an exercise (Exercise I.61).
(1): Assume the hypothesis, yet st(a) < 0. By I.6.54, st(a) â‰ˆ0; hence st(a) =
0, a contradiction.
(5): st(b) Ì¸= 0 implies that b Ì¸â‰ˆ0,â€  in particular, b Ì¸= 0. The formula to
prove thus makes sense. Now, a = a(a/b); hence, by (4), st(a) = st(a(a/b)) =
st(a) st(a/b).
(7): Having proved (6) by induction on n, we note that st(an) = st(a)n Ì¸= 0.
Moreover, aâˆ’n = 1/an. Hence st(aâˆ’n) = st(1/an) = st(1)/st(an) = 1/st(a)n,
since st(1) = 1.
(8): For odd n, it is true in R that
(âˆ€x)(âˆƒy)x = yn
or, more colloquially,
(âˆ€x)(âˆƒy)y = x1/n
(i)
(i) is also true in âˆ—R, so it makes sense, for any a âˆˆâˆ—R and odd n, to form a1/n.
Similarly, if n is even, then
(âˆ€x)(x â‰¥0 â†’(âˆƒy)y = x1/n)
(ii)
is true in âˆ—R, so it makes sense, for any 0 â‰¤a âˆˆâˆ—R and even n, to form a1/n.
For any such a1/n that makes sense, so does st(a)1/n, by (1).
Thus, noting that a = (a1/n)n we get st(a) = st((a1/n)n) = st(a1/n)n, the
second â€œ=â€ by (6). Hence st(a1/n) = st(a)1/n.
â–¡
A corollary to (1) above, insigniï¬cant (and easy) as it may sound, is the
non-standard counterpart to the statement that a real closed interval [a, b] is
compact, that is, every sequence of members of [a, b] that converges, converges
to a number in [a, b].
I.6.56 Corollary. h â‰¤hâ€² implies st(h) â‰¤st(hâ€²). In particular, if a and b are
real and the closed interval in âˆ—R is
[a, b]
def
= {x âˆˆâˆ—R : a â‰¤x â‰¤b}
then whenever h âˆˆ[a, b], it also follows that st(h) âˆˆ[a, b].
â€  b Ì¸â‰ˆ0 means, of course, Â¬(b â‰ˆ0).

108
I. Basic Logic
The notion of the limit of a real functionâ€  of one variable f at a point a
depends on what the function does when its inputs are in a neighbourhood of
a â€“ that is, an open interval (c, b) that contains a â€“ but not on what it does on a
itself. For this reason the limit is deï¬ned in terms of a punctured neighbourhood
of a, which means a set like (c, a) âˆª(a, b), where c < a < b. We are interested
in calculating better and better approximations to the values f (x) as x gets very
close to a (but never becomes equal to a â€“ for all we care, f might not even be
deï¬ned on a).
We can deï¬ne limits `a la Leibniz now, replacing â€œvery closeâ€ by â€œinï¬nitely
closeâ€:
I.6.57 Deï¬nition (Limits). Let â—¦f be a real function of one variable deï¬ned
in some punctured real neighbourhood of the real a. Let b be also real.â€¡ The
notation limxâ†’a â—¦f (x) = b abbreviates (i.e., is deï¬ned to mean)
for all non-standard h â‰ˆ0,
âˆ—f (a + h) â‰ˆb
(1)
In practice (argot) we will let the context fend for itself and simply write (1) as
â€œfor all non-standard h â‰ˆ0, f (a + h) â‰ˆbâ€ and, similarly, limxâ†’a f (x) = b
for its abbreviation, that is, dropping the âˆ—and â—¦left superscripts. We have just
deï¬ned the so-called two-sided ï¬nite limit.
Similarly one deï¬nes a whole variety of other limits. We give two examples
of such deï¬nitions (in simpliï¬ed notation):
Suppose f is deï¬ned in the open interval (a, c) and let b be real. Then the
symbol limxâ†’a+ f (x) = b abbreviates
for all positive h â‰ˆ0,
f (a + h) â‰ˆb
(2)
We have just deï¬ned the so-called right ï¬nite limit.
Finally, let f be deï¬ned in the open interval (a, c). Then limxâ†’a+ f (x) =
+âˆabbreviates
for all positive h â‰ˆ0,
f (a + h) is positive inï¬nite
(3)
We have just deï¬ned the so-called right positive inï¬nite limit.
â–¡
I.6.58 Remark.
(A) In (1) in the above deï¬nition, h âˆˆâˆ—R âˆ’R guarantees that h Ì¸= 0. This in
turn guarantees that we are unconcerned with what f wants to be on a â€“ as
â€  That is, a function f : R â†’R â€“ not necessarily a total one.
â€¡ Recall that, because â—¦a =âˆ—a by (4) on p. 99, we have already decided to use the formal name
â€œaâ€ for either â—¦a or âˆ—a.

I.6. Substructures, Diagrams, and Applications
109
we should be â€“ since a + h Ì¸= a. Furthermore, âˆ—f (a + h) is deï¬ned
for all such h, so that (1) makes sense. This is easy: To ï¬x ideas, let
â—¦f be deï¬ned in the punctured neighbourhood (d, a) âˆª(a, c). First off,
0 < |h| < min(a âˆ’d, c âˆ’a), since 0 Ì¸= h â‰ˆ0. Hence h > 0 â†’a <
a + h < c, while h < 0 â†’d < a + h < a. Secondly, the sentence
(âˆ€x)(d < x < a âˆ¨a < x < c â†’(âˆƒy)y = f (x)) is true in R by
the assumption on â—¦f , hence also in âˆ—R. Thus, âˆ—f is deï¬ned in the non-
standard (punctured) neighbourhood {x âˆˆâˆ—R : d < x < a âˆ¨a < x < c}.
This neighbourhood we are going to denote by (d, a) âˆª(a, c) as well, and
let the context indicate whether or not this symbol denotes a standard or a
non-standard neighbourhood.
(B) Another way to state (1) above is
for all nonzero h â‰ˆ0,
st( f (a + h)) = b
(4)
That is, the standard part above is independent of the choice of h.
â–¡
I.6.59Example. Wecomputelimxâ†’2(3x3âˆ’x2+1):Let0 Ì¸= h â‰ˆ0bearbitrary.
Then
st(3(2 + h)3 âˆ’(2 + h)2 + 1) = st(3(23 + 12h + 6h2 + h3)
âˆ’(4 + 4h + h2) + 1)
= st(21 + 32h + 17h2 + 3h3)
= 21,
by I.6.55
We compute limxâ†’0(x/|x|). We want st(h/|h|) for 0 Ì¸= h â‰ˆ0. In order to
remove the absolute value sign we consider cases:
st
 h
|h|

=
1
if h > 0
âˆ’1
if h < 0
According to the deï¬nition (cf. (B), previous remark), the limit limxâ†’0
(x/|x|) does not exist. The calculation above shows however that limxâ†’0+(x/
|x|) = 1 and limxâ†’0âˆ’(x/|x|) = âˆ’1.
We see that calculating limits within non-standard calculus is easy be-
cause we calculate with equalities, rather than with inequalities as in standard
calculus.
â–¡
We show next that the non-standard deï¬nition of limit is equivalent to
Weierstrassâ€™s Îµ-Î´ deï¬nition:

110
I. Basic Logic
I.6.60 Theorem. Let â—¦f be a real function, deï¬ned in some punctured neigh-
bourhood of (the real) a. Let b be also real. The following statements are
equivalent:
(i) (1) in Deï¬nition I.6.57
(ii) (âˆ€0 < Îµ âˆˆR)(âˆƒ0 < Î´ âˆˆR)(âˆ€x âˆˆR)(0 < |x âˆ’a| < Î´ â†’|â—¦f (x) âˆ’b| < Îµ).â€ 
Proof. (i) â†’(ii): Let then (1) in I.6.57 hold. To prove (ii), ï¬x an 0 < Îµ âˆˆR.â€¡
It now sufï¬ces to show the truth of the formal sentence (iii) below:
(âˆƒÎ´ > 0)(âˆ€x)(0 < |x âˆ’a| < Î´ â†’| f (x) âˆ’b| < Îµ)
(iii)
One way to prove an existential sentence such as (iii) is to exhibit a Î´ that
works. Since it is a sentence over L, it sufï¬ces to verify it in âˆ—R. It will then be
true in R â€“ which is what we want.
Thus, we take Î´ = h where 0 Ì¸= h â‰ˆ0 and show that it works. Let now
x âˆˆâˆ—R be arbitrary such that 0 < |x âˆ’a| < h. Thus (I.6.53), |x âˆ’a| â‰ˆ0;
hence x âˆ’a â‰ˆ0 from âˆ’|x âˆ’a| â‰¤x âˆ’a â‰¤|x âˆ’a| and I.6.54 (via I.6.55). We
can now write x = a + hâ€², with hâ€² â‰ˆ0 and hâ€² Ì¸= 0. By hypothesis â€“ i.e., (i) â€“
âˆ—f (a + hâ€²) â‰ˆb is true in âˆ—R, i.e., âˆ—f (x) â‰ˆb is true. Hence âˆ—f (x) âˆ’b â‰ˆ0,
and therefore |âˆ—f (x) âˆ’b| is less than any positive real. In particular, it is less
than Îµ.
(ii) â†’(i): We assume (ii), pick an arbitrary h such that 0 Ì¸= h â‰ˆ0, and
prove âˆ—f (a + h) â‰ˆb. This requires
for all real Îµ > 0,
|âˆ—f (a + h) âˆ’b| < Îµ
(iv)
So ï¬x an arbitrary real Îµ > 0. Assumption (ii) translates into the assumption
that the sentence (iii) is true in R. Let then Î´ > 0 be real, so that the L-sentence
below is true in R (Î´ and Îµ below are formal constants):
(âˆ€x)(0 < |x âˆ’a| < Î´ â†’| f (x) âˆ’b| < Îµ)
(v)
(v) is also true in âˆ—R. By specialization in the metalanguage, take x = a + h.
Now, 0 < |x âˆ’a| = |h| by choice of h. Also, |x âˆ’a| = |h| < Î´ is also true,
since Î´ > 0 and real, and h â‰ˆ0. Thus, by (v), translated into âˆ—R, we have
|âˆ—f (x) âˆ’b| < Îµ. This proves (iv).
â–¡
â€  This argot is a bit awkward, but not unusual. â€œ(âˆ€0 < Îµ âˆˆR) . . . â€ stands for â€œ(âˆ€Îµ)(0 < Îµ âˆ§Îµ âˆˆ
R â†’. . . â€.
â€¡ We have ï¬xed a real Îµ. Recall that the name â€œÎµâ€ is also used for the formal constant that denotes
this real Îµ.

I.6. Substructures, Diagrams, and Applications
111
Worth repeating: The part (i) â†’(ii) of the above proof was an instance where
we were able to prove a ï¬rst order fact in âˆ—R and then transfer it back to R.
This is the essential use we get from the elementary extension R â‰ºâˆ—R, for if
all the facts we needed could easily be proved in R, the whole fuss of obtaining
an extension that contains weird numbers would be pointless.
We conclude with the deï¬nition of continuity and with one more elementary
application of transferring facts from âˆ—R back to R. Some more techniques and
facts will be discovered by the reader in the Exercises section.
I.6.61 Deï¬nition. Let f be a real function of one real variable, deï¬ned at least
on an open real interval (a, b). We say that f is continuous at c âˆˆ(a, b) (a real
point) iff limxâ†’c f (x) = f (c).
If f is also deï¬ned at a, then we say that it is continuous at the left endpoint
a of [a, b), meaning that limxâ†’a+ f (x) = f (a). In a similar situation at the
right endpoint b, we require that limxâ†’bâˆ’f (x) = f (b).
We say that f is continuous on [a, b] iff it is so at every real x âˆˆ[a, b].
â–¡
I.6.62 Remark. The above is the standard deï¬nition. Since it involves the
concept of limit, we may translate it to a corresponding non-standard deï¬nition.
Let then f be deï¬ned on the real closed interval [a, b]. Then for any (real)
c âˆˆ(a, b) continuity requires (using h as a free variable over âˆ—R)
0 Ì¸= h â‰ˆ0 â†’st(âˆ—f (c + h)) = f (c)
(1)
Continuity at the endpoints reads
0 < h â‰ˆ0 â†’st(âˆ—f (a + h)) = f (a)
(2)
and
0 > h â‰ˆ0 â†’st(âˆ—f (b + h)) = f (b)
(3)
Does it matter if we take the 0 Ì¸= part away? No, since the limit is equal to the
function value.
Suppose now that f is continuous on the real interval [a, b]. We now extend
[a, b] to include non-standard numbers as in I.6.56. Then, whenever x âˆˆ[a, b],
where x is a hyperreal, we also have st(x) âˆˆ[a, b] by I.6.56. Thus, x âˆˆ[a, b]
implies that x = r + h where r is real â€“ a â‰¤r â‰¤b â€“ and h â‰ˆ0. We can now
capture (1)â€“(3) by the single statement
x âˆˆ[a, b] â†’st(âˆ—f (x)) = âˆ—f (st(x))
(4)

112
I. Basic Logic
Thus, continuity is the state of affairs where st commutes with the function
letter. By the way, since st(x) is real, so is âˆ—f (st(x)); indeed, it is the same as
â—¦f (st(x)) (cf. (5), p. 99), which we write, more simply, f (st(x)). In practice one
writes (4) above as
x âˆˆ[a, b] â†’st( f (x)) = f (st(x))
â–¡
I.6.63 Example. The function x #â†’âˆšx is continuous on any [a, b] where
0 â‰¤a. Indeed, by I.6.55, 0 â‰¤x implies st(âˆšx) = âˆšst(x). Now invoke (4)
in I.6.62.
â–¡
I.6.64 Theorem. Suppose that f is continuous on the real interval [a, b]. Then
f is bounded on [a, b], that is, there is a real B > 0 such that
x âˆˆ[a, b] âˆ©R â†’| f (x)| < B
(1)
Proof. We translate the theorem conclusion into a sentence over L. â€œ f â€, as
usual, plays a dual role: name of the real and name of the formal object. The
translation is
(âˆƒy)(âˆ€x)(a â‰¤x â‰¤b â†’| f (x)| < y)
(1â€²)
Now (1â€²) is true in âˆ—R under the assumption that â—¦f , which we still call f , is
continuous.
Here is why: Take y = H, where H âˆˆâˆ—R is some positive inï¬nite hyperreal.
Pick any hyperreal x in [a, b] (extended interval). Now, the assumption on
continuity, in the form (4) of I.6.62, has the side effect that
st( f (x)) is deï¬ned
Hence f (x) is ï¬nite. Let then 0 < rx âˆˆR such that | f (x)| < rx. Thisrx depends
on the picked x. But rx < H; thus | f (x)| < H for the arbitrary hyperreal x in
[a, b], establishing the truth of (1â€²) in âˆ—R. So it is true in R too.
â–¡
I.7. Deï¬ned Symbols
We have already mentioned that the language lives, and it is being constantly
enriched by new nonlogical symbols through deï¬nitions. The reason we do this
is to abbreviate undecipherably long formal texts, thus making them humanly
understandable.
There are three possible kinds of formal abbreviations, namely, abbreviations
of formulas, abbreviations of variable terms (i.e., â€œobjectsâ€ that depend on free

I.7. Deï¬ned Symbols
113
variables),andabbreviationsofconstantterms(i.e.,â€œobjectsâ€thatdonotdepend
on free variables). Correspondingly, we introduce a new nonlogical symbol for
a predicate, a function, or a constant in order to accomplish such abbreviations.
Here are three simple examples, representative of each case.
We introduce a new predicate (symbol), â€œâŠ†â€, in set theory by a deï¬nitionâ€ 
A âŠ†B â†”(âˆ€x)(x âˆˆA â†’x âˆˆB)
An introduction of a function symbol by deï¬nition is familiar from elemen-
tary mathematics. There is a theorem that says
â€œfor every non-negative real number x there is a unique
non-negative real number y such that x = y Â· yâ€
(1)
This justiï¬es the introduction of a 1-ary function symbol f that, for each such x,
produces the corresponding y. Instead of using the generic â€œ f (x)â€, we normally
adopt one of the notations â€œâˆšxâ€ or â€œx1/2â€. Thus, we enrich the language (of,
say, algebra or real analysis) by the function symbol âˆš
and add as an axiom
the deï¬nition of its behaviour. This would be
x = âˆšxâˆšx
or
y = âˆšx â†”x = y Â· y
where the restriction x â‰¥0 is implied by the context.
The â€œenabling formulaâ€ (1) â€“ stated in argot above â€“ is crucial in order
that we be allowed to introduce âˆš
and its deï¬ning axiom. That is, before we
introduce an abbreviation of a (variable or constant) term â€“ i.e., an object â€“ we
must have a proof in our theory of an existential formula, i.e., one of the type
(âˆƒ!y)A, that asserts that (if applicable, for each value of the free variables) a
unique such object exists.
The symbol â€œ(âˆƒ!y)â€ is read â€œthere is a unique yâ€. It is a â€œlogicalâ€ abbreviation
(deï¬ned logical symbol, just like âˆ€) given (in least-parenthesized form) by
(âˆƒx)(A âˆ§Â¬(âˆƒz)(A âˆ§Â¬x = z))
Finally, an example of introducing a new constant symbol, from set theory,
is the introduction of the symbol âˆ…into the language, as the name of the unique
â€  In practice we state the above deï¬nition in argot, probably as â€œA âŠ†B means that, for all x,
x âˆˆA â†’x âˆˆBâ€.

114
I. Basic Logic
objectâ€  y that satisï¬es Â¬U(y) âˆ§(âˆ€x)x /âˆˆy, read â€œy is a set,â€¡ and it has no
membersâ€. Thus, âˆ…is deï¬ned by
Â¬U(âˆ…) âˆ§(âˆ€x)x /âˆˆâˆ…
or, equivalently, by
y = âˆ…â†”Â¬U(y) âˆ§(âˆ€x)x /âˆˆy
The general situation is this: We start with a theory , spoken in some
basicÂ§ formal language L. As the development of  proceeds, gradually and
continuously we extend L into languages Ln, for n â‰¥0 (we have set L0 = L).
Thus the symbol Ln+1 stands for some arbitrary extension of Ln effected at
stage n + 1. The theory itself is being extended by stages, as a sequence n,
n â‰¥0.
A stage is marked by the event of introducing a single new symbol into the
language via a deï¬nition of a new predicate, function or constant symbol. At
that same stage we also add to n the deï¬ning nonlogical axiom of the new
symbol in question, thus extending the theory n into n+1. We set 0 = .
Speciï¬cally, if Â¶ Q (âƒ—xn) is some formula, we then can introduce a new pred-
icate symbol â€œPâ€# that stands for Q .
In the present description, Q is a syntactic (meta-)variable, while P is a new
formal predicate symbol.
This entails adding P to Lk (i.e., to its alphabet V k) as a new n-ary predicate
symbol, and adding
Pâƒ—xn â†”Q (âƒ—xn)
(i)
to k as the deï¬ning axiom for P. â€œâŠ†â€ is such a deï¬ned (2-ary) predicate in set
theory.
Similarly, a new n-ary function symbol f is added into Lk (to form Lk+1) by
a deï¬nition of its behaviour. That is, we add f to Lk and also add the following
â€  Uniqueness follows from extensionality, while existence follows from separation. These facts â€“
and the italicized terminology â€“ are found in volume 2, Chapter III.
â€¡ U is 1-ary (unary) predicate. It is one of the two primitive nonlogical symbols of formal set
theory. With the help of this predicate we can â€œtestâ€ an object for set or atom status. â€œ U(y)â€
asserts that y is an atom, thus â€œÂ¬U(y)â€ asserts that y is a set â€“ since we accept that sets or atoms
are the only types of objects that the formal system axiomatically characterizes.
Â§ â€œBasicâ€ means here the language given originally, before any new symbols were added.
Â¶ Recall that (see Remark I.1.11, p. 18) the notation Q (âƒ—xn) asserts that âƒ—xn, i.e., x1,. . . , xn, is the
complete list of the free variables of Q .
# Recallthatpredicatelettersaredenotedbynon-calligraphiccapitalletters P, Q, R withorwithout
subscripts or primes.

I.7. Deï¬ned Symbols
115
formula (ii) to k as a new nonlogical axiom:
y = f y1 . . . yn â†”Q (y, y1,. . . , yn)
(ii)
provided we have a proof in k of the formula
(âˆƒ!y)Q (y, y1,. . . , yn)
(iii)
Of course, the variables y, âƒ—yn are distinct.
Depending on the theory, and the number of free variables (n â‰¥0), â€œ f â€ may
take theory-speciï¬c names such as âˆ…, Ï‰, âˆš, etc. (in this illustration, for the
sake of economy of effort, we have thought of deï¬ned constants, e.g., âˆ…and Ï‰,
as 0-ary functions â€“ something that we do not normally do).
In effecting these deï¬nitions, we want to be assured of two things:
1. Whatever we can state in the richer language Lk (for any k > 0) we can also
state in the original (â€œbasicâ€) language L = L0 (although awkwardly, which
justiï¬es our doing all this). â€œCan stateâ€ means that we can â€œtranslateâ€ any
formula F over Lk (hopefully in a natural way) into a formula F âˆ—over L
so that the extended theory k can prove that F and F âˆ—are equivalent.â€ 
2. We also want to be assured that the new symbols offer no more than con-
venience, in the sense that any formula F , over the basic language L, that
k (k > 0) is able to prove, one way or another (perhaps with the help of
deï¬ned symbols),  can also prove.â€¡
Theseassuranceswillbecomeavailableshortly,asMetatheoremsI.7.1andI.7.3.
Here are the â€œnaturalâ€ translation rules, that take us from a language stage Lk+1
back to the previous, Lk (so that, iterating the process, we are back to L):
Rule (1). Suppose that F is a formula over Lk+1, and that the predicate P
(whose deï¬nition took us from Lk to Lk+1, and hence is a symbol of Lk+1 but
not of Lk) occurs in F zero or more times. Assume that P has been deï¬ned by
the axiom (i) above (included in k+1), where Q is a formula over Lk.
We eliminate P from F by replacing all its occurrences by Q . By this we
mean that whenever Pâƒ—tn is a subformula of F , all its occurrences are replaced
by Q (âƒ—tn). We can always arrange by I.4.13 that the simultaneous substitution
Q [âƒ—xn â†âƒ—tn] is deï¬ned.
This results to a formula F âˆ—over Lk.
â€  , spoken over L, can have no opinion, of course, since it cannot see the new symbols, nor does
it have their â€œdeï¬nitionsâ€ among its â€œknowledgeâ€.
â€¡ Trivially, any F
over L that  can prove, any k (k > 0) can prove as well, since the latter
understands the language (L) and contains all the axioms of . Thus k extends the theory .
That it cannot have more theorems over L than  makes this extension conservative.

116
I. Basic Logic
Rule (2). If f is a deï¬ned n-ary function symbol as in (ii) above, introduced
into Lk+1, and if it occurs in F as F [ f t1 . . . tn],â€  then this formula is logically
equivalent toâ€¡
(âˆƒy)(y = f t1 . . . tn âˆ§F [y])
(iv)
provided that y is not free in F [ f t1 . . . tn].
Using the deï¬nition of f given by (ii) and I.4.13 to ensure that Q (y,âƒ—tn) is
deï¬ned, we eliminate this occurrence of f , writing (iv) as
(âˆƒy)(Q (y, t1,. . . , tn) âˆ§F [y])
(v)
which says the same thing as (iv) in any theory that thinks that (ii) is true (this
observation is made precise in the proof of Metatheorem I.7.1). Of course, f
may occur many times in F , even â€œwithin itselfâ€, as in f f z1 . . . zny2 . . . yn,Â§
or even in more complicated conï¬gurations. Indeed, it may occur within the
scope of a quantiï¬er. So the rule becomes: Apply the transformation taking
every atomic subformula A[ f âƒ—tn] of F into form (v) by stages, eliminating at
each stage the leftmost innermostÂ¶ occurrence of f (in the atomic formula we
are transforming at this stage), until all occurrences of f are eliminated.
We now have a formula F âˆ—over Lk.
I.7.1 Metatheorem (Elimination of Deï¬ned Symbols: I). Let  be any theory
over some formal language L.
(a) Let the formula Q be over L, and P be a new predicate symbol that extends
L into Lâ€² and  into â€² via the axiom Pâƒ—xn â†”Q (âƒ—xn). Then, for any formula
F over Lâ€², the P-elimination as in Rule (1) above yields a F âˆ—over L such
that
â€² âŠ¢F â†”F âˆ—
(b) Let F [x] be over L, and let t stand for f t1,. . . , tn, where f is introduced
by (ii) above as an axiom that extends  into â€². Assume that no ti contains
the letter f and that y is not free in F [t]. Then#
â€² âŠ¢F [t] â†”(âˆƒy)(Q (y,âƒ—tn) âˆ§F [y])
â€  This notation allows for the possibility that f t1,. . . , tn does not occur at all in F
(see the
convention on brackets, p. 18).
â€¡ See (C) in the proof of Metatheorem I.7.1 below.
Â§ Or f ( f (z1,. . . , zn), y2,. . . , yn)), using brackets and commas to facilitate reading.
Â¶ A term f t1,. . . , tn is â€œinnermostâ€ iff none of the ti contains â€œ f â€.
# As we already have remarked, in view of I.4.13, it is unnecessary pedantry to make assumptions
on substitutability explicit.

I.7. Deï¬ned Symbols
117
Here â€œLâ€²â€ is â€œLk+1â€ (for some k) and â€œLâ€ is â€œLkâ€.
Proof. First observe that this metatheorem indeed gives the assurance that, after
applying the transformations (1) and (2) to obtain F âˆ—from F , â€² thinks that
the two are equivalent.
(a): This follows immediately from the Leibniz rule (I.4.25).
(b): Start with
âŠ¢F [t] â†’t = t âˆ§F [t] (By âŠ¢t = t and |=Taut-implication)
(A)
Now, by Ax2, substitutability, and non-freedom of y in F [t],
âŠ¢t = t âˆ§F [t] â†’(âˆƒy)(y = t âˆ§F [y])
Hence
âŠ¢F [t] â†’(âˆƒy)(y = t âˆ§F [y])
(B)
by (A) and |=Taut-implication.â€ 
Conversely,
âŠ¢y = t â†’(F [y] â†”F [t])
(Ax4; substitutability was used here)
Hence (by |=Taut)
âŠ¢y = t âˆ§F [y] â†’F [t]
Therefore, by âˆƒ-introduction (allowed, by our assumption on y),
âŠ¢(âˆƒy)(y = t âˆ§F [y]) â†’F [t]
which, along with (B), establishes
âŠ¢F [t] â†”(âˆƒy)(y = t âˆ§F [y])
(C)
Finally, by (ii) (which introduces â€² to the left of âŠ¢), (C), and the Leibniz rule,
â€² âŠ¢F [t] â†”(âˆƒy)(Q (y,âƒ—tn) âˆ§F [y])
(D)
â–¡
The import of Metatheorem I.7.1 is that if we transform a formula F â€“ written
over some arbitrary extension by deï¬nitions, Lk+1, of the basic language L â€“
into a formula F âˆ—over L, then k+1 (the theory over Lk+1 that has the beneï¬t
of all the added axioms) thinks that F â†”F âˆ—. The reason for this is that we can
â€  We will often write just â€œby |=Tautâ€ meaning to say â€œby |=Taut-implicationâ€.

118
I. Basic Logic
imagine that we eliminate one new symbol at a time, repeatedly applying the
metatheorem above â€“ part (b) to atomic subformulas â€“ forming a sequence of
increasingly more â€œbasicâ€ formulas F k+1, F k, F kâˆ’1,. . . , F 0, where F 0 is the
same string as F âˆ—and F k+1 is the same string as F .
Now, i+1 âŠ¢F i+1 â†”F i for i = k,. . . , 0, where, if a deï¬ned function letter
was eliminated at step i + 1 â†’i, we invoke (D) above and the Leibniz rule.
Hence, since 0 âŠ†1 âŠ†Â· Â· Â· âŠ†k+1, k+1 âŠ¢F i+1 â†”F i for i = k,. . . , 0,
therefore k+1 âŠ¢F k+1 â†”F 0.
I.7.2Remark(OnePointRule). Theabsolutelyprovableformulain(C)above
is sometimes called the one point rule (Gries and Schneider (1994), Tourlakis
(2000a, 2001b)). Its dual
F [t] â†”(âˆ€y)(y = t â†’F [y])
is also given the same nickname and is easily (absolutely) provable using (C)
by eliminating âˆƒ.
â–¡
I.7.3 Metatheorem (Elimination of Deï¬ned Symbols: II). Let  be a theory
over a language L.
(a) If Lâ€² denotes the extension of L by the new predicate symbol P, and â€²
denotes the extension of  by the addition of the axiom Pâƒ—xn â†”Q (âƒ—xn),
where Q is a formula over L, then  âŠ¢F for any formula F over L such
that â€² âŠ¢F .
(b) Assume that
 âŠ¢(âˆƒ!y)R(y, x1,. . . , xn)
(âˆ—)
pursuant to which we have deï¬ned the new function symbol f by the axiom
y = f x1. . . xn â†”R(y, x1,. . . , xn)
(âˆ—âˆ—)
and thus extended L to Lâ€² and  to â€². Then  âŠ¢F for any formula F
over L such that â€² âŠ¢F .
Proof. This metatheorem assures that extensions of theories by deï¬nitions are
conservative in that they produce convenience but no additional power (the
same old theorems over the original language are the only ones provable).
(a): By the completeness theorem, we show instead that
 |= F
(1)

I.7. Deï¬ned Symbols
119
So let M = (M, I ) be an arbitrary model of , i.e., let
|=M 
(2)
We now expand the structure M into Mâ€² = (M, I â€²) â€“ without adding any
new individuals to its domain M â€“ by adding an interpretation PI â€² for the new
symbol P. We deï¬ne for every a1,. . . , an in M
PI â€²(a1,. . . , an) = t iff |=Mâ€² Q (a1,. . . , an)
[i.e., iff
|=M Q (a1,. . . , an)]
Clearly then, Mâ€² is a model of the new axiom, since, for all Mâ€²-instances of
the axiom â€“ such as P(a1,. . . , an) â†”Q (a1,. . . , an) â€“ we have
(P(a1,. . . , an) â†”Q (a1,. . . , an))I â€² = t
It follows that |=Mâ€² â€², since we have |=Mâ€² , the latter by (2), due to having
made no changes to M that affect the symbols of L. Thus, â€² âŠ¢F yields
|=Mâ€² F ; hence, since F is over L, we obtain |=M F . Along with (2), this
proves (1).
(b): As in (a), assume (2) in an attempt to prove (1). By (âˆ—),
|=M (âˆƒ!y)R(y, x1,. . . , xn)
Thus, there is a concrete (i.e., in the metatheory) function f of n arguments that
takes its inputs from M and gives its outputs to M, the input-output relation
being given by (3) below (âƒ—bn in, a out). To be speciï¬c, the semantics of â€œâˆƒ!â€
implies that for all b1,. . . , bn in M there is a unique a âˆˆM such that
(R(a, b1,. . . , bn))I = t
(3)
We now expand the structure M into Mâ€² = (M, I â€²),â€  so that all we add to it
is an interpretation for the new function symbol f . We let f I â€² = f . From (2)
it follows that
|=Mâ€² 
(2â€²)
since we made no changes to M other than adding an interpretation of f , and
since no formula in  contains f . By (3), if a, b1,. . . , bn are any members of
M, then we have
|=Mâ€² a = f b1. . . bn iff a = f (b1,. . . , bn)
iff |=M R(a, b1,. . . , bn)
by the deï¬nition of f
iff |=Mâ€² R(a, b1,. . . , bn)
â€  This part is independent of part (a); hence this is a different I â€² in general.

120
I. Basic Logic
the last â€œiffâ€ being because R (over L) means the same thing in M and Mâ€².
Thus,
|=Mâ€² y = f x1. . . xn â†”R(y, x1,. . . , xn)
(4)
Now (âˆ—âˆ—), (2â€²), and (4) yield |=Mâ€² â€² which implies |=Mâ€² F (from â€² âŠ¢F ).
Finally, since F contains no f , we have |=M F . This last fact, and (2) give (1).
â–¡
I.7.4 Remark.
(a) We note that translation rules (1) and (2) â€“ the latter applied to atomic
subformulas â€“ preserve the syntactic structure of quantiï¬er preï¬xes. For
example, suppose that we have introduced f by
y = f x1. . . xn â†”Q (y, x1,. . . , xn)
(5)
in set theory. Now, an application of the collection axiom of set theory has
a hypothesis of the form
â€œ(âˆ€x âˆˆZ)(âˆƒw)(. . . A[ f t1 . . . tn] . . . )â€
(6)
where, say, A is atomic and the displayed f is innermost. Eliminating this
f , we have the translation
â€œ(âˆ€x âˆˆZ)(âˆƒw)(. . . (âˆƒy)(A[y] âˆ§Q (y, t1,. . . , tn)) . . . )â€
(7)
which still has the âˆ€âˆƒ-preï¬x and still looks exactly like a collection axiom
hypothesis.
(b) Rather than worrying about the â€œontologyâ€ of the function symbol formally
introduced by (5) above â€“ i.e., the question of the exact nature of the symbol
thatwenamed â€œ f â€â€“inpracticeweshrugthisoffandresorttometalinguistic
devices to name the function symbol, or the term that naturally arises from
it. For example, one can use the notation â€œ fQ â€ for the function â€“ where the
subscript â€œQ â€ is the exact string over the language that â€œQ â€ denotes â€“ or,
for the corresponding term, the notation of Whitehead and Russell (1912),
(Î¹z)Q (z, x1,. . . , xn)
(8)
The â€œzâ€ in (8) above is a bound variable.â€  This new type of term is read
â€œthe unique z such that . . . â€. This â€œÎ¹â€ is not one of our primitive symbols.â€¡
â€  That it must be distinct from the xi is obvious.
â€¡ It is however possible to enlarge our alphabet to include â€œÎ¹â€, and then add deï¬nitions of the
syntax of â€œÎ¹-termsâ€ and axioms for the behaviour of â€œÎ¹-termsâ€. At the end of all this one gets a

I.7. Deï¬ned Symbols
121
It is just meant to lead to the friendly shorthand (8) above that avoids the
â€œontologyâ€ issue. Thus, once one proves
(âˆƒ!z)Q (z, x1,. . . , xn)
(9)
one can then introduce (8) by the axiom
y = (Î¹z)Q (z, x1,. . . , xn) â†”Q (y, x1,. . . , xn)
(5â€²)
which, of course, is an alias for the axiom (5), using more suggestive nota-
tion for the term f x1,. . . , xn. By (9), the axioms (5) or (5â€²) can be replaced
by
Q ( f x1,. . . , xn, x1,. . . , xn)
and
Q ((Î¹z)Q (z, x1,. . . , xn), x1,. . . , xn)
(10)
respectively. For example, from (5â€²) we get (10) by substitution. Now, Ax4
(with some help from |=Taut) yields
Q ((Î¹z)Q (z, x1,. . . , xn), x1,. . . , xn) â†’
y = (Î¹z)Q (z, x1,. . . , xn) â†’Q (y, x1,. . . , xn)
Hence, assuming (10),
y = (Î¹z)Q (z, x1,. . . , xn) â†’Q (y, x1,. . . , xn)
(11)
Finally, deploying (9), we get
Q ((Î¹z)Q (z, x1,. . . , xn), x1,. . . , xn) â†’
Q (y, x1,. . . , xn) â†’y = (Î¹z)Q (z, x1,. . . , xn)
Hence
Q (y, x1,. . . , xn) â†’y = (Î¹z)Q (z, x1,. . . , xn)
by (10). This, along with (11), yields (5â€²).
â–¡
The Indeï¬nite Article. We often have the following situation: We have proved
a statement like
(âˆƒx)A[x]
(1)
conservative extension of the original theory, i.e., any Î¹-free formula provable in the new theory
can be also proved in the old (Hilbert and Bernays (1968)).

122
I. Basic Logic
and we want next to derive a statement B . To this end, we start by picking a
symbol c not in B and say â€œlet c be such that A[c] is trueâ€.â€  That is, we add
A[c] as a nonlogical axiom, treating c as a new constant.
From all these assumptions we then manage to prove B, hopefully treat-
ing all the free variables of A[c] as constants during the argument. We then
conclude that B has been derived without the help of A[c] or c (see I.4.27).
Two things are noteworthy in this technique: One, c does not occur in the
conclusion, and, two, c is not uniquely determined by (1). So we have a (rather
than the) c that makes A[c] true.
Now the suggestion that the free variables of the latter be frozen during the
derivation of B is unnecessarily restrictive, and we have a more general result:
Suppose that
 âŠ¢(âˆƒx)A(x, y1,. . . , yn)
(2)
Add a new function symbol f to the language L of  (thus obtaining Lâ€²) via
the axiom
A( f y1,. . . , yn, y1,. . . , yn)
(3)
This says, intuitively, â€œfor any y1,. . . , yn, let x = f âƒ—yn make A(x, âƒ—yn) trueâ€.
Again, this x is not uniquely determined by (2).
Finally, suppose that we have a proof
 + A( f âƒ—yn, âƒ—yn) âŠ¢B
(4)
such that f , the new function symbol, occurs nowhere in B, i.e., the latter
formula is over L. We can conclude then that
 âŠ¢B
(5)
that is, the extension  + A( f âƒ—yn, âƒ—yn) of  is conservative.
A proof of the legitimacy of this technique, based on the completeness
theorem, is easy. Let
|=M 
(6)
and show
|=M B
(7)
Expand the model M = (M, I ) to Mâ€² = (M, I â€²), so that I â€² interprets
the new symbol f . The interpretation is chosen as follows: (2) guarantees
â€  Cf. I.5.44.

I.8. Computability and Uncomputability
123
that, for all choices of i1,. . . , in in M, the set S(i1,. . . , in) = {a âˆˆM :
|= M A(a, i1,. . . , in)} is not empty.
By the axiom of choice (of informal set theory), we can pick an a(i1,. . . , in)â€ 
in each S(i1,. . . , in). Thus, we deï¬ne a function f : Mn â†’M by letting, for
each i1,. . . , in in M, f (i1,. . . , in) = a(i1,. . . , in).
The next step is to set
f I â€² = f
Therefore, for all i1,. . . , in in M,
( f i1 . . . in)I â€² = f (i1,. . . , in) = a(i1,. . . , in)
It is now clear that |=Mâ€² A( f y1 . . . yn, y1,. . . , yn), for, by I.5.11,
(A( f i1. . . in, i1,. . . , in))I â€² = t â†”(A(a(i1,. . . , in), i1,. . . , in))I â€² = t
and the right hand side of the above is true by the choice of a(i1,. . . , in).
Thus, |=Mâ€²  + A( f y1 . . . yn, y1,. . . , yn); hence |=Mâ€² B , by (4). Since B
contains no f , we also have |=M B ; thus we have established (7) from (6). We
now have (5).
One can give a number of names to a function like f : a Skolem function,
an Îµ-term (Hilbert (1968)), or a Ï„-term (Bourbaki (1966b)). In the ï¬rst case
one may ornament the symbol f , e.g., fâˆƒA, to show where it is coming from,
although such mnemonic naming is not, of course, mandatory.
The last two terminologies actually apply to the term f y1 . . . yn, rather than
to the function symbol f . Hilbert would have written
(Îµx)A(x, y1 . . . , yn)
(8)
and Bourbaki
(Ï„x)A(x, y1 . . . , yn)
(9)
â€“ each denoting f y1 . . . yn. The â€œxâ€ in each of (8) and (9) is a bound variable
(different from each yi).
I.8. Computability and Uncomputability
Computability (or â€œrecursion theoryâ€) is nowadays classiï¬ed as an area of logic
(e.g., it is one of the areas represented in the Handbook of Mathematical Logic,
Barwise (1978)). It has its origins in the work of several logicians in the 1930s
â€  The â€œ(i1,. . . , in)â€ part indicates that â€œaâ€ depends on i1,. . . , in.

124
I. Basic Logic
(GÂ¨odel, Turing, Kleene, Church, Post, et al.). Motivation for this research was
partly provided by Hilbertâ€™s program to found all mathematics on formalism.
This was a formalism that one ought to be able to certify by ï¬nitary means
(for each particular formalized theory) to be free of contradiction. Moreover,
it was a formalism, for which â€“ Hilbert expected â€“ a â€œmethodâ€ ought to exist
to solve the Entscheidungsproblem (decision problem), that is, the question â€œis
this arbitrary formula a theorem, or not?â€
What was a â€œmethodâ€ supposed to be, exactly, mathematically speaking?
Was the expectation that the Entscheidungsproblem of any theory is amenable
to algorithmic solution realistic? Work of Church (lack of a decision algorithm
for certain theories (1936)) showed that it was not, nor for that matter was
the expectation of certifying freedom of contradiction of all formal theories by
ï¬nitary means (GÂ¨odelâ€™s second incompleteness theorem).
One of these two negative answers (Churchâ€™s) built on an emerging theory
of computable (or algorithmic) functions and the mathematical formulation of
the concepts of algorithm or method. The other one, GÂ¨odelâ€™s, while it used
existing (pre-Turing and pre-Kleene) rudiments of computability (primitive
recursive functions of Dedekind), can be recast, in hindsight, in the framework
of modern computability. This recasting shows the intimate connection between
the phenomena of incompletableness of certain theories and uncomputability,
and thus it enhances our understanding of both phenomena.
With the advent of computers and the development of computer science,
computability gained a new set of practitioners and researchers: theoretical
computer scientists. This group approaches the area from two (main) stand-
points: to study the power and limitations of mathematical models of computing
devices (after all, computer programs are algorithms), and also to understand
why some problems have â€œeasyâ€ while others have â€œhardâ€ algorithmic solutions
(complexity theory) â€“ in the process devising several â€œpracticalâ€ (or efï¬cient)
solutions, and techniques, for a plethora of practical problems.
We develop the basics of computability here informally, that is, within â€œreal
mathematicsâ€ (in the metatheory of pure and applied ï¬rst order logic).
Computability, generally speaking, formalizes the concept of a â€œcomputable
functionâ€ f : Nk â†’N. That is, it concerns itself with the issue of separating the
set of all so-called number-theoretic functions â€“ that is,â€  functions with inputs
in N and outputs in N â€“ into computable and uncomputable.
Because we want the theory to be as inclusive as possible, we allow it to
study both total and nontotal functions f : Nk â†’N.
â€  More precisely, this is what ordinary computability or ordinary recursion theory studies. Higher
recursion theory, invented by Kleene, also looks into functions that have higher order inputs such
as number-theoretic functions.

I.8. Computability and Uncomputability
125
The trivial reason is that in everyday computing we do encounter both total
and nontotal functions. There are computer programs which (whether or not
according to the programmerâ€™s intent) do not stop to yield an answer for all
possible inputs. We do want to have formal counterparts of those in our theory,
if we are hoping to have a theory that is inclusive.
Alesstrivialreasonisthatunlessweallownontotalfunctionsinthetheory,an
obvious diagonalization can show the existence of total (intuitively) computable
functions that are not in the theory.
I.8.1 Deï¬nition. Any number-theoretic function f : Nk â†’N is a partial
function. If its domain, dom( f ), equals Nk â€“ the set of all potential inputs, or
left ï¬eld â€“ then we say that f is total. If it does not, then f is nontotal.
That a âˆˆdom( f ) is also denoted by f (a) â†“, and we say that f is deï¬ned at
a or that f (a) converges.â€  In the opposite case we write f (a) â†‘and say that f
is undeï¬ned at a or that f (a) diverges.
A number-theoretic relation is a subset of Nk. We usually write such relations
in relational notation. That is, we write R(a1,. . . , an) for âŸ¨a1,. . . , anâŸ©âˆˆR.
Thus our notation of relations parallels that of formulas of a ï¬rst order language,
and we use the logical connectives (âˆƒ, âˆ€, Â¬, âˆ¨, etc.) informally to combine
relations. We carry that parallel to the next natural step, and use the phrases
â€œ. . . a relation R . . . â€ and â€œ. . . a relation R(y1,. . . , yn) . . . â€ interchangeably,
thelattertoconveythatthefulllistoftherelationâ€™svariablesisexactly y1,. . . , yn
(cf. p. 18).
We occasionally use Î»-notation to modify a given relation R(y1,. . . , yn).
This notation is employed as in Î»z1. . . zr.R, or even Î»z1. . . zr.R(y1,. . . , yn).
The part â€œÎ»z1 . . . zr.â€ denotes that â€œz1,. . . , zrâ€ is the active variables list and
supersedes the list â€œy1,. . . , ynâ€. Any yi that is not in the list z1,. . . , zr is
treated as a constant (or â€œparameterâ€ â€“ i.e., it is â€œfrozenâ€). The list z1,. . . , zr
may contain additional variables not in the list y1, . . . , yn.
Thus, e.g., Î»xy.x < 2 = {0, 1} Ã— N, while Î»yx.x < 2 = N Ã— {0, 1}. On
the other hand, Î»x.x < y = {x : x < y}, which denotes a different relation for
different values of the parameter y.
Finally, as before, âƒ—zr or just âƒ—z (if r is understood) denotes z1,. . . , zr, so that
we may write Î»âƒ—zr.R(y1,. . . , yn).
â–¡
I.8.2 Deï¬nition (Bounded Quantiï¬cation). For any relation R, the symbols
(âˆƒx)<z R, (âˆ€x)<z R, (âˆƒx)â‰¤z R, (âˆ€x)â‰¤z R stand for (âˆƒx)(x < z âˆ§R), (âˆ€x)(x <
z â†’R), (âˆƒx)(x â‰¤z âˆ§R), (âˆ€x)(x â‰¤z â†’R), respectively. We say that they
denote bounded quantiï¬cation.
â–¡
â€  This nomenclature parallels that of â€œconvergentâ€ or â€œhaltingâ€ computations.

126
I. Basic Logic
I.8.3 Deï¬nition. If R âŠ†Nn is a relation and f : Nk â†’N a function, then
R(w1,. . . , wm, f (âƒ—xk), wm+1,. . . , wn) meansâ€ 
(âˆƒz)(R(w1,. . . , wm, z, wm+1,. . . , wn) âˆ§z = f (âƒ—xk))
We have just one important exception to this rule: If Q is g(âƒ—y) = w, then
g(âƒ—y) = f (âƒ—xk) means
g(âƒ—y) â†‘âˆ§f (âƒ—xk) â†‘âˆ¨(âˆƒz)(z = g(âƒ—y) âˆ§z = f (âƒ—xk))
One often writes g(âƒ—y) â‰ƒf (âƒ—xk) for the above to alert the reader that â€œweak
equalityâ€ (a notion due to Kleene) applies, but we will rather use â€œ=â€ throughout
and let the context determine the meaning.
â–¡
Clearly, weak equality restores reï¬‚exivity of â€œ=â€ (which fails if the general
understanding of substitution above applied to â€œ=â€ as well).
Î»-notation comes in handy in denoting number-theoretic functions. Instead
of saying â€œconsider the function g obtained from f : Nk â†’N, by setting, for
all âƒ—wm,
g( âƒ—wm) = f (âƒ—xk)
where if an xi is not among the w j it is understood to be an (unspeciï¬ed)
constantâ€, we simply say â€œconsider g = Î» âƒ—wm. f (âƒ—xk)â€.
I.8.4 Example. Turning to inequalities, f (x) > 0 means (is equivalent to)
(âˆƒy)(y = f (x) âˆ§y > 0). In particular, it implies that f (x) â†“.
â–¡
I.8.5 Example. In the presence of partial functions, Â¬A = B and A Ì¸= B are
not interchangeable. For example, f (a) Ì¸= b says (by I.8.3) that (âˆƒy)( f (a) =
yâˆ§y Ì¸= b). In particular, this entails that f (a) â†“. On the other hand, Â¬ f (a) = b
holds iff f (a) â†‘âˆ¨(âˆƒy)( f (a) = y âˆ§y Ì¸= b).
We are not changing the rules of logic here, but are just amending our
understanding of the semantics of the metanotation â€œÌ¸=â€, to make it correct in
the presence of partial functions.
â–¡
There are many approaches to deï¬ning computable functions, and they are
all equivalent, that is, they deï¬ne exactly the same set of functions. All except
two of them begin by deï¬ning a notion of â€œcomputation modelâ€, that is, a set
â€  Cf. I.7.2.

I.8. Computability and Uncomputability
127
of string-manipulation algorithms (e.g., Turing machines, Markov algorithms,
Kleeneâ€™s equation manipulation processes), and then they deï¬ne a computable
functionasonewhoseinput-outputrelationshipâ€“codedasarelationonstringsâ€“
can be veriï¬ed by an algorithm belonging to the computation model.
There are two number-theoretic approaches, both due to Kleene, one using
so-called Kleene schemataâ€  and one that inductively deï¬nes the set of com-
putable functions, bypassing the concepts of â€œalgorithmâ€ or â€œcomputationâ€.â€¡
We follow the latter approach in this section. According to this, the set
of computable functions is the smallest set of functions that includes some
â€œindisputably computableâ€ functions, and is closed under some â€œindisputably
algorithmicâ€ operations.Â§
The following are operations (on number-theoretic functions) that are cen-
trally important:
I.8.6 Deï¬nition (Composition). Let Î»âƒ—x.gi(âƒ—x) (i = 1,. . . , n) and Î»âƒ—yn. f (âƒ—yn)
be given functions.Â¶ Then h = Î»âƒ—x. f (g1(âƒ—x),. . . , gn(âƒ—x)) is the result of their
composition.
â–¡
Note the requirement that all the variables of the â€œoutermostâ€ function, f , be
substituted, and that each substitution (a function application, gi(âƒ—x)) apply to
the same variable list âƒ—x. With additional tools, we can eventually relax this very
rigid requirement.
I.8.7 Deï¬nition (Primitive Recursion). Let Î»xâƒ—ynz.g(x, âƒ—yn, z) and Î»âƒ—yn.h(âƒ—yn)
be given. We say that Î»xâƒ—yn. f (x, âƒ—yn) is obtained by primitive recursion from
h and g just in case it satisï¬es, for all x and âƒ—yn, the following equations (the
so-called primitive recursive schema):
f (0, âƒ—yn) = h(âƒ—yn)
f (x + 1, âƒ—yn) = g(x, âƒ—yn, f (x, âƒ—yn))
â–¡
I.8.8 Deï¬nition (Unbounded Search). Given Î»âƒ—x yn.g(x, âƒ—yn). f is deï¬ned from
g by unbounded search on the variable x just in case, for all âƒ—yn, the following
â€  These characterize inductively the set of all number-tuples âŸ¨z, âƒ—x, yâŸ©which are intuitively under-
stood to â€œcodeâ€ the statement that the machine, or algorithm, z, when presented with input âƒ—x,
will eventually output y.
â€¡ Work on this originated with Dedekind, who characterized in this manner a proper subset of
computable functions, that of primitive recursive functions.
Â§ The reader will agree, once all the details are in hand, that the qualiï¬cation â€œindisputablyâ€ is apt.
Â¶ Afunctioninthissection,unlessotherwiseexplicitlystated,isanumber-theoreticpartialfunction.

128
I. Basic Logic
holds:
f (âƒ—yn) =
min{x : g(x, âƒ—yn) = 0 âˆ§(âˆ€z)<xg(z, âƒ—yn) â†“}
â†‘if the minimum above does not exist
(1)
In (1) above, the case â€œâ†‘â€ is short for â€œ f (âƒ—yn) is undeï¬nedâ€. We write f (âƒ—yn) =
(Âµx)g(x, âƒ—yn) as a short form of (1).
â–¡
I.8.9 Example. The condition â€œg(x, âƒ—yn) = 0âˆ§(âˆ€z)<xg(z, âƒ—yn) â†“â€ is rather com-
plicated. It says that (see also I.8.4)
g(0, âƒ—yn) > 0,
g(1, âƒ—yn) > 0,. . . ,
g(x âˆ’1, âƒ—yn) > 0
but g(x, âƒ—yn) = 0. For example, suppose that
g(x, y) =
0
if x = y = 1
â†‘
otherwise
Then, while the smallest x such that g(x, 1) = 0 holds is x = 1, this is not what
(1) â€œcomputesâ€. The deï¬nition (1) yields undeï¬ned in this case, since g(0, 1) â†‘.
Of course, the part â€œ(âˆ€z)<xg(z, âƒ—yn) â†“â€ in (1) is superï¬‚uous if g is total.
â–¡
The following functions are intuitively computable. They form the basis of
an inductive deï¬nition of all computable functions.
I.8.10 Deï¬nition (Initial Functions).
Zero: z
(Î»x.0)
Successor: s
(Î»x.x + 1)
Identities or projections: un
i , for n â‰¥1 and 1 â‰¤i â‰¤n
(Î»âƒ—xn.xi).
â–¡
I.8.11 Deï¬nition. The set of partial computable or partial recursive functions,
P,istheclosureoftheinitialfunctionsabove,undertheoperationscomposition,
primitive recursion, and unbounded search.
Thesetofcomputableorrecursivefunctions,R,isthesetofalltotalfunctions
of P.
â–¡
One occasionally sees terminology such as â€œcomputable partial functionsâ€ or
â€œrecursive partial functionsâ€. Of course, â€œpartialâ€ qualiï¬es â€œfunctionsâ€ (not â€œre-
cursiveâ€ or â€œcomputableâ€): therefore one hopes never to see â€œpartially recursive
functionsâ€ or â€œpartially computable functionsâ€.

I.8. Computability and Uncomputability
129
I.8.12 Deï¬nition. The set of primitive recursive functions, PR, is the closure
of the initial functions above under the operations composition and primitive
recursion.
â–¡
The primitive recursive functions were deï¬ned by Dedekind and were called
â€œrecursiveâ€ until the recursive functions of I.8.11 were deï¬ned. Then the name
of the functions of Dedekind was qualiï¬ed to be â€œprimitiveâ€.
Why are the functions in P â€œcomputableâ€?â€  Well, an (informal) induction
on the deï¬nition (I.8.11) shows why this is â€œcorrectâ€.
The initial functions are clearly intuitively computable (e.g., by pencil and
paper, by anyone who knows how to add 1 to an arbitrary natural number).
Suppose that each of Î»âƒ—x.gi(âƒ—x) (i = 1,. . . , n) and Î»âƒ—yn. f (âƒ—yn) are intuitively
computable (i.e., we know how to compute the output, given the input). To
compute f (g1(âƒ—a),. . . , gn(âƒ—a)), given âƒ—a, we compute each of the gi(âƒ—a), and then
use the results as inputs to f .
To see why f (deï¬ned by a primitive recursive schema from h and g) is
computable if h and g are, let us ï¬rst introduce the notation z := x, which we
understand to say â€œcopy the value of x into zâ€.
Then we can write an â€œalgorithmâ€ for the computation of f (a, âƒ—bn):
(1)
z := h(âƒ—bn)
Repeat (2) below for i = 0, 1, 2,. . . , a âˆ’1:
(2)
z := g(i, âƒ—bn, z)
Since(I.H.)thecomputationsh(âƒ—bn)and g(i, âƒ—bn, z)canbecarriedoutâ€“regardless
of the input values âƒ—bn, i, and z â€“ at the end of the â€œcomputationâ€ indicated above,
z holds the value f (a, âƒ—bn).
Finally, let Î»xâƒ—yn.g(x, âƒ—yn) be intuitively computable. We show how to com-
pute Î»âƒ—yn.(Âµx)g(x, âƒ—yn):
(1) x := 0.
(2) if g(x, âƒ—bn) = 0, go to step (5).
(3) x := x + 1.
(4) go back to step (2).
(5) Done! x holds the result.
The above algorithm justiï¬es the term â€œunbounded searchâ€. We are searching
by letting x = 0, 1, 2, . . . in turn. It is â€œunboundedâ€ since we have no a priori
â€  We have â€œcomputableâ€ and computable. The former connotes our intuitive understanding of the
term. It means â€œintuitively computableâ€. The latter has an exact deï¬nition (I.8.11).

130
I. Basic Logic
knowledge of how far the search will have to go. It is also clear that the algorithm
satisï¬es the deï¬nition of (Âµx):â€  We will hit step (5) iff progress was never
blocked at step (2) (i.e., iff all along g(i, âƒ—bn) > 0 (see I.8.4) until the ï¬rst
(smallest) i came along for which g(i, âƒ—bn) = 0).
We have our ï¬rst few simple results:
I.8.13 Proposition. R âŠ‚P.
Proof. The âŠ†-part is by deï¬nition. The Ì¸= -part follows from the fact that e âˆˆ
P but e Ì¸âˆˆR, where we have denoted by â€œeâ€ the totally undeï¬ned (empty)
function Î»y.(Âµx)s(u2
1(x, y)) (in short, e(y), for any y, is the smallest x such
that x + 1 = 0; but such an x does not exist).
â–¡
I.8.14 Proposition. R is closed under composition and primitive recursion.
Proof. These two operations preserve total functions (why?).
â–¡
I.8.15 Corollary. PR âŠ†R.
Proof. By induction on PR, since the initial functions (common to PR and
P) are total and hence are in R.
â–¡
Thus all primitive recursive functions are total.
It can be shown that the inclusion PR âŠ†R is proper, but we will not need
this result (see, e.g., Tourlakis (1984)).
I.8.16 Deï¬nition. A relation R(âƒ—x) is (primitive) recursive iff its characteristic
function,
Ï‡R = Î»âƒ—x.
0
if R(âƒ—x)
1
if Â¬R(âƒ—x)
is (primitive) recursive.
The set of all primitive recursive (recursive) relations, or predicates,â€¡ is
denoted by PRâˆ—(Râˆ—).
â–¡
â€  By the way, in modern Greek, one pronounces â€œÂµâ€ exactly like the English word â€œmeâ€.
â€¡ Relations are often called â€œpredicatesâ€ by computability practitioners.

I.8. Computability and Uncomputability
131
Since we are to stay within N, we need a special kind of subtraction, proper
subtraction:â€ 
x
.âˆ’y
def
=
x âˆ’y
if x â‰¥y
0
otherwise
I.8.17 Example. This example illustrates some important techniques used to
circumvent the rigidity of our deï¬nitions.
We prove that Î»xy.x
.âˆ’y âˆˆPR. First, we look at a special case. Let p =
Î»x.x
.âˆ’1 and p = Î»xy.p(x). Now p is primitive recursive, since
p(0, y) = z(y)
(1)
p(x + 1, y) = u3
1(x, y,p(x, y))
Thus, so is
p = Î»x.p

u1
1(x), z(x)

(2)
Finally, let d = Î»xy.y
.âˆ’x. This is in PR, since
d(0, y) = u1
1(y)
(3)
d(x + 1, y) = p

u3
3(x, y, d(x, y))

Thus, Î»xy.x
.âˆ’y is primitive recursive, since
Î»xy.x
.âˆ’y = Î»xy.d

u2
2(x, y), u2
1(x, y)

(4)
Our acrobatics here have worked around the following formal difï¬culties:
(i) Our number-theoretic functions have at least one argument. Thus, any
instance of the primitive recursive schema must deï¬ne a function of at
least two arguments. This explains the introduction of p in the schema (1).
(ii) A more user-friendly way to write (1) (in the argot of recursion theory) is
p(0) = 0
p(x + 1) = x
Indeed, â€œu3
1(x, y,p(x, y))â€ is a fancy way (respecting the form of the
primitive recursive schema) to just say â€œxâ€. Moreover, one simply writes
p = Î»x.p(x, 0) instead of (2) above.
(iii) Finally, (3) and (4) get around the fact that the primitive recursion schema
iterates via the ï¬rst variable. As this example shows, this is not cast in
stone, for we can swap variables (with the help of the un
i ).
â€  Some authors pronounce proper subtraction monus.

132
I. Basic Logic
One must be careful not to gloss over this last hurdle by shrugging it off:
â€œWhatâ€™s in a name?â€. It is not a matter of changing names everywhere to go from
Î»xy.x
.âˆ’y to Î»yx.y
.âˆ’x. We actually needed to work with the ï¬rst variable in
the Î»-list, but (because of the nature of â€œ
.âˆ’â€) this variable should be after â€œ
.âˆ’â€.
That is, we did need d = Î»xy.y
.âˆ’x.
In argot, (3) takes the simple form
x
.âˆ’0 = x
x
.âˆ’(y + 1) = (x
.âˆ’y)
.âˆ’1
The reader must have concluded (correctly) that the argot operations of per-
muting variables, identifying variables, augmenting the variable list with new
variables (also, replacing a single variable with a function application or a con-
stant) are not argot at all, but are derived â€œlegalâ€ operations of substitution (due
to Grzegorczyk (1953) â€“ see Exercise I.68).
Therefore, from now on we will relax our notational rigidity and beneï¬t
from the presence of these operations of substitution.
â–¡
I.8.18 Example. Î»xy.x + y, Î»xy.x Ã— y (or, in implied multiplication notation,
Î»xy.xy), and Î»xy.x y are in PR. Let us leave the ï¬rst two as an easy exercise,
and deal with the third one, since it entails an important point:
x0 = 1
x y+1 = x Ã— x y
The â€œimportant pointâ€ is regarding the basis case, x0 = 1. We learn in â€œordinary
mathâ€ that 00 is undeï¬ned. If we sustain this point of view, then Î»xy.x y cannot
possibly be in PR (why?). So we re-deï¬ne 00 to be 1.
One does this kind of re-deï¬nition a lot in recursion theory (it is akin to
removing removable discontinuities in calculus) when a function threatens not
to be, say, primitive recursive for trivial reasons.
A trivial corollary is that Î»x.0x âˆˆPR (why?). This is a useful function,
normally denoted by sg. Clearly,
sg(x) =
1
if x = 0
0
otherwise
We also see that sg(x) = 1
.âˆ’x, which provides an alternative proof that Î»x.0x âˆˆ
PR.
â–¡

I.8. Computability and Uncomputability
133
I.8.19 Example.
Î»xyz.
y
if x = 0
z
if x Ì¸= 0
is in PR. This function is often called the â€œswitchâ€ or â€œif-then-elseâ€, and is
sometimes denoted by the name â€œswâ€.
We rest our case, since
sw(0, y, z) = y
sw(x + 1, y, z) = z
We see immediately that sw(x, 1, 0) = 0x = 1
.âˆ’x. The function Î»x.sw(x,
0, 1) = Î»x.1
.âˆ’(1
.âˆ’x) has a special symbol: â€œsgâ€. It is often called the signum,
since it gives the sign of its argument.
â–¡
I.8.20 Lemma. R(âƒ—x) is in PRâˆ—(respectively, Râˆ—) iff, for some f âˆˆPR (res-
pectively, f âˆˆR), R(âƒ—x) â†”f (âƒ—x) = 0.
Proof. Only-if part: Take f = Ï‡R. If part: Ï‡R = Î»âƒ—x.sg( f (âƒ—x)).
â–¡
I.8.21 Theorem. PRâˆ—(respectively, Râˆ—) is closed under replacement of vari-
ables by primitive recursive (respectively, recursive) functions.
Proof. If Ï‡R is the characteristic function of R(âƒ—x, y, âƒ—z) and f is a total function,
then Î»âƒ—x âƒ—wâƒ—z.Ï‡R(âƒ—x, f ( âƒ—w), âƒ—z) is the characteristic function of R(âƒ—x, f ( âƒ—w), âƒ—z). (See
also Exercise I.68.)
â–¡
I.8.22 Theorem. PRâˆ—and Râˆ—are closed under Boolean connectives (â€œBo-
olean operationsâ€) and bounded quantiï¬cation.
Proof. It sufï¬ces to cover Â¬, âˆ¨, (âˆƒy)<z. We are given R(âƒ—x), Q(âƒ—y), and P(y, âƒ—x),
all in PRâˆ—(or Râˆ—; the argument is the same for both cases).
Case for Â¬:
Ï‡Â¬R = Î»âƒ—x.sg(Ï‡R(âƒ—x)).
Case for âˆ¨:
Ï‡Râˆ¨Q = Î»âƒ—xâƒ—y.Ï‡R(âƒ—x)Ï‡Q(âƒ—y) (where we have used implied mul-
tiplication notation).
Case for (âˆƒy)<z:
To unclutter the notation, let us denote by Ï‡âˆƒP the char-
acteristic function of (âˆƒy)<z P(y, âƒ—x). Then
Ï‡âˆƒP(0, âƒ—x) = 1
Ï‡âˆƒP(z + 1, âƒ—x) = Ï‡P(z, âƒ—x)Ï‡âˆƒP(z, âƒ—x)
â–¡

134
I. Basic Logic
I.8.23 Remark. (1) The reader can convince himself that quantifying over the
ï¬rst variable was only for the sake of notational convenience.
(2) The case for (âˆƒy)â‰¤z (and therefore for (âˆ€y)â‰¤z) can be easily made:
(âˆƒy)â‰¤z Q(y, âƒ—x) â†”(âˆƒy)<z+1Q(y, âƒ—x)
(here we have used I.8.21).
â–¡
I.8.24 Example (Bounded Summation and Multiplication). We are collect-
ing tools that will be useful in our arithmetization of P. Two such tools are the
operations  
y<z f (y, âƒ—x) and !
y<z f (y, âƒ—x). Both PR and R are closed under
these operations. For example, here is the reason for !:
"
y<0
f (y, âƒ—x) = 1
"
y<z+1
f (y, âƒ—x) = f (z, âƒ—x)
"
y<z
f (y, âƒ—x)
â–¡
I.8.25 Deï¬nition (Bounded Search). For a total function Î»yâƒ—x.g(y, âƒ—x) we
deï¬ne for all âƒ—x
(Âµy)<zg(y, âƒ—x)
def
=
min{y : y < z âˆ§g(y, âƒ—x) = 0}
z if the minimum does not exist
The symbol (Âµy)â‰¤zg(y, âƒ—x) is deï¬ned to mean (Âµy)<z+1g(y, âƒ—x).
â–¡
Bounded search, (Âµy)<z, searches a predetermined domain, 0, 1,. . . , z âˆ’1. If
unsuccessful, it returns the ï¬rst number to the right of the domain.
We extend the use of search on predicates:
I.8.26 Deï¬nition. For a predicate R(y, âƒ—x), the symbols (Âµy)R(y, âƒ—x), (Âµy)<z
R(y, âƒ—x),
and
(Âµy)â‰¤z R(y, âƒ—x)
mean
(Âµy)Ï‡R(y, âƒ—x),
(Âµy)<zÏ‡R(y, âƒ—x),
and
(Âµy)â‰¤zÏ‡R(y, âƒ—x) respectively.
â–¡
I.8.27 Theorem (Deï¬nition by Cases). R and PR are closed under the sche-
ma of deï¬nition by cases (below), where it is understood that the relations Ri
are mutually exclusive:
f (âƒ—x) =
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³
g1(âƒ—x)
if R1(âƒ—x)
g2(âƒ—x)
if R2(âƒ—x)
...
gk(âƒ—x)
if Rk(âƒ—x)
gk+1(âƒ—x)
otherwise

I.8. Computability and Uncomputability
135
Proof. Perhaps the simplest proof is to observe that
f (âƒ—x) = g1(âƒ—x)sg(Ï‡R1(âƒ—x)) + Â· Â· Â· + gk(âƒ—x)sg(Ï‡Rk(âƒ—x)) + gk+1(âƒ—x)sg(Ï‡Q(âƒ—x))
a ï¬xed-length sum, where Q â†”Â¬(R1 âˆ¨Â· Â· Â· âˆ¨Rk).
â–¡
I.8.28 Theorem. R and PR are closed under bounded search.
Proof. Let g(z, âƒ—x) = (Âµy)<z f (y, âƒ—x). Then
g(0, âƒ—x) = 0
g(z + 1, âƒ—x) = if g(z, âƒ—x) Ì¸= z then g(z, âƒ—x)
else if f (z, âƒ—x) = 0 then z
else z + 1
The second equation above is g(z + 1, âƒ—x) = k(z, âƒ—x, g(z, âƒ—x)), where
k(z, âƒ—x, w) = if w Ì¸= z then w
else if f (z, âƒ—x) = 0 then z
else z + 1
Clearly k is wherever f is (in R or PR), since (see I.8.19) sw âˆˆPR.
â–¡
I.8.29 Proposition. The following are all primitive recursive:
(i) Î»xy.
#x
y
$ %
the quotient of the division x
y
&
(ii) Î»xy.rem(x, y)
%
the remainder of the division x
y
&
(iii) Î»xy.x|y (â€œx divides yâ€)
(iv) Pr(x) (x is a prime)
(v) Î»n.pn (the n th prime)
(vi) Î»nx. exp(n, x) (the exponent of pn in the prime factorization of x)
(vii) Seq(x) (â€œxâ€™s prime number factorization contains at least one prime, but
no gapsâ€)
Proof. (i):
#x
y
$
= (Âµz)â‰¤x((z + 1)y > x)
(1)
(1) is correct for all y Ì¸= 0. Since we do not want the quotient function to
fail primitive recursiveness for a trivial reason (we have a â€œremovable
nontotalnessâ€ â€“ see also I.8.18), we deï¬ne âŒŠx/yâŒ‹to equal the right hand side
of (1) at all times (of course, the right hand side is total).

136
I. Basic Logic
(ii): rem(x, y) = x
.âˆ’âŒŠx/yâŒ‹y.
(iii): x|y â†”rem(y, x) = 0.
(iv): Pr(x) â†”x > 1 âˆ§(âˆ€y)â‰¤x(y|x â†’y = 1 âˆ¨y = x).
(v):
p0 = 2
pn+1 = (Âµy)â‰¤22n+1 (Pr(y) âˆ§y > pn)
The above is based on Euclidâ€™s proof that there are inï¬nitely many primes
(p0 p1 Â· Â· Â· pn +1 is either a prime, q â‰¥pn+1, or it has a prime divisor q â‰¥pn+1)
and an induction on n that shows pn â‰¤22n.
(vi): exp(n, x) = (Âµy)â‰¤x

Â¬(py+1
n
|x)

.
(vii): Seq(x) â†”x > 1âˆ§(âˆ€y)â‰¤x(âˆ€z)â‰¤x(y|xâˆ§Pr(y)âˆ§Pr(z)âˆ§z < y â†’z|x).
â–¡
I.8.30 Deï¬nition (Coding and Decoding Number Sequences). An arbitrary
(ï¬nite) sequence of natural numbers a0, a1,. . . , anâˆ’1 will be coded as
pa0+1
0
pa1+1
1
Â· Â· Â· panâˆ’1+1
nâˆ’1
We use the notation
âŸ¨a0, a1,. . . , anâˆ’1âŸ©
def
=
"
y<n
p
ay+1
y
(1)
â–¡
In set theory one likes to denote tuples by âŸ¨a0,. . . , anâˆ’1âŸ©as well, a practice
that we have been following (cf. Section I.2). To avoid notational confusion, in
those rare cases where we want to write down both a code âŸ¨a0,. . . , anâˆ’1âŸ©of a
sequence a0,. . . , anâˆ’1 and an n-tuple in set theoryâ€™s sense, we write the latter
in the â€œoldâ€ notation, with round brackets, that is, (a0,. . . , anâˆ’1).
Why â€œ + 1â€ in the exponent? Without that, all three sequences â€œ2â€, â€œ2, 0â€,
and â€œ2, 0, 0â€ get the same code, namely 22. This is a drawback, for if we are
given the code 22 but do not know the length of the coded sequence, then we
cannot decode 22 back into the original sequence correctly. Contrast this with
the schema (1) above, where these three examples are coded as 23, 23 Â· 3 and
23 Â· 3 Â· 5 respectively. We see that the coding (1) above codes the length n of
the sequence a0, a1,. . . , anâˆ’1 into the code z = âŸ¨a0, a1,. . . , anâˆ’1âŸ©. This length
is the number of primes in the decomposition of z (of course, Seq(z) is true),
and it is useful to have a function for it, called â€œlhâ€. There are many ways to
deï¬ne a primitive recursive length function, lh, that does the job. The simplest
deï¬nition allows lh to give nonsensical answers for all inputs that do not code

I.8. Computability and Uncomputability
137
sequences. Examples of such inputs are 0, 1, 10;â€  in short, any number z such
that Â¬Seq(z). We let
lh(z) = (Âµy)â‰¤z(Â¬py|z)
Clearly, lh âˆˆPR.
From all the above, we get that Seq(z) iff, for some a0, a1,. . . , anâˆ’1, z =
âŸ¨a0, a1,. . . , anâˆ’1âŸ©(this justiï¬es the mnemonic â€œSeqâ€ for â€œsequenceâ€).
Clearly,lh(z) = n inthiscase,andâ€¡ exp(i, z)
.âˆ’1 = ai fori = 0, 1,. . . , nâˆ’1
(exp(i, z)
.âˆ’1 = 0 if i â‰¥n).
It is customary to use the more compact symbol
(z)i
def
= exp(i, z)
.âˆ’1
Thus, if Seq(z), then the sequence (z)i, for i = 0,. . . ,lh(z)
.âˆ’1, decodes z.
We will also need to express sequence concatenation primitive recursively.
We deï¬ne concatenation, â€œâˆ—â€, by
âŸ¨a0,. . . , anâˆ’1âŸ©âˆ—âŸ¨b0,. . . , bmâˆ’1âŸ©
def
= âŸ¨a0,. . . , anâˆ’1, b0,. . . , bmâˆ’1âŸ©
(2)
Of course, for Î»xy.x âˆ—y to be in PR we must have a total function to begin
with, so that â€œâˆ—â€ must be deï¬ned on all natural numbers, not on just those
satisfying Seq.
The following deï¬nition is at once seen to satisfy all our requirements:
x âˆ—y
def
= x Â·
"
i<lh(y)
pexp(i,y)
i+lh(x)
(3)
I.8.31 Theorem (Course-of-Values (Primitive) Recursion). Let H(x, âƒ—yn), the
history function of f , stand for âŸ¨f (0, âƒ—yn),. . . , f (x, âƒ—yn)âŸ©for x â‰¥0.
Then PR and R are closed under the following schema of course-of-values
recursion:
f (0, âƒ—yn) = h(âƒ—yn)
f (x + 1, âƒ—yn) = g(x, âƒ—yn, H(x, âƒ—yn))
Proof. It follows from the (ordinary) primitive recursion
H(0, âƒ—yn) = âŸ¨h(âƒ—yn)âŸ©
H(x + 1, âƒ—yn) = H(x, âƒ—yn) âˆ—âŸ¨f (x + 1, âƒ—yn)âŸ©
= H(x, âƒ—yn) âˆ—âŸ¨g(x, âƒ—yn, H(x, âƒ—yn))âŸ©
and f (x, âƒ—yn) = (H(x, âƒ—yn))x.
â–¡
â€  Our deï¬nition gives lh(0) = 1, lh(1) = 0, lh(10) = 1.
â€¡ Since Seq(z), we have exp(i, z)
.âˆ’1 = exp(i, z) âˆ’1 for i = 0, 1,. . . , n âˆ’1.

138
I. Basic Logic
We next arithmetize P-functions and their computations. We will assign
â€œprogram codesâ€ to each function. A program code â€“ called in the literature a
GÂ¨odel number, or a Ï†-index, or just an index â€“ is, intuitively, a number in N that
codes the â€œinstructionsâ€ necessary to compute a P-function.
If i âˆˆN is aâ€  code for f âˆˆP, then we write
f = {i}
(Kleeneâ€™s notation)
orâ€¡
f = Ï†i
(Rogersâ€™s (1967) notation)
Thus, either {i} or Ï†i denotes the function with code i.
The following table indicates how to assign GÂ¨odel numbers (middle column)
to all partial recursive functions by following the deï¬nition of P. In the table,
f indicates a code of f :
Function
Code
Comment
Î»x.0
âŸ¨0, 1, 0âŸ©
Î»x.x + 1
âŸ¨0, 1, 1âŸ©
Î»âƒ—xn.xi
âŸ¨0, n, i, 2âŸ©
1 â‰¤i â‰¤n
Composition:
âŸ¨1, m, f ,g1,. . . ,gnâŸ©
f must be n-ary
f (g1(âƒ—ym), . . . , gn(âƒ—ym))
gi must be m-ary
Primitive recursion from
basis h and iterated part g
âŸ¨2, n + 1,h,g âŸ©
h must be n-ary
g must be (n + 2)-ary
Unbounded search:
âŸ¨3, n, f âŸ©
f must be (n + 1)-ary
(Âµy) f (y, âƒ—xn)
and n > 0
We have been somewhat loose in our description above. â€œThe following table
indicates how to assign GÂ¨odel numbers (middle column) to all partial recursive
functions by following the deï¬nition of Pâ€, we have said, perhaps leading the
reader to think that we are deï¬ning the codes by recursion on P. Not so. After
all, each function has inï¬nitely many codes.
What was really involved in the table â€“ see also below â€“ was arguing back-
wards: a speciï¬cation of how we would like our Ï†-indices behave once we
â€  The indeï¬nite article is appropriate here. Just as in real life a computable function has inï¬nitely
many different programs that compute it, a partial recursive function f has inï¬nitely many
different codes (see I.8.34 later on).
â€¡ That is where the name â€œÏ†-indexâ€ comes from.

I.8. Computability and Uncomputability
139
obtained them. We now turn to showing how to actually obtain them by di-
rectly deï¬ning the set of all Ï†-indices, , as an inductively deï¬ned subset of
{z : Seq(z)}:
 = Cl(I , R)
where I = {âŸ¨0, 1, 0âŸ©, âŸ¨0, 1, 1âŸ©} âˆª{âŸ¨0, n, i, 2âŸ©: n > 0 âˆ§1 â‰¤i â‰¤n}, and the
rule set R consists of the following three operations:
(i) Coding composition: Input a and bi (i = 1,. . . , n) causes output
âŸ¨1, m, a, b1,. . . , bnâŸ©
provided (a)1 = n and (bi)1 = m, for i = 1,. . . , n.
(ii) Coding primitive recursion: Input a and b causes output
âŸ¨2, n + 1, a, bâŸ©
provided (a)1 = n and (b)1 = n + 2.
(iii) Coding unbounded search: Input a causes output
âŸ¨3, n, aâŸ©
provided (a)1 = n + 1 and n > 0.â€ 
By the uniqueness of prime number decomposition, the pair (I , R) is un-
ambiguous (see I.2.10, p. 24). Therefore we deï¬ne by recursion on  (cf. I.2.13)
a total function Î»a.{a} (or Î»a.Ï†a)â€¡ for each a âˆˆ:
{âŸ¨0, 1, 0âŸ©} = Î»x.0
{âŸ¨0, 1, 1âŸ©} = Î»x.x + 1
{âŸ¨0, n, i, 2âŸ©} = Î»âƒ—xn.xi
{âŸ¨1, m, a, b1,. . . , bnâŸ©} = Î»âƒ—ym.{a}({b1}(âƒ—ym),. . . , {bn}(âƒ—ym))
{âŸ¨2, n + 1, a, bâŸ©} = Î»xâƒ—yn.Prec({a}, {b})
{âŸ¨3, n, aâŸ©} = Î»âƒ—xn.(Âµy){a}(y, âƒ—xn)
In the above recursive deï¬nition we have used the abbreviation Prec({a}, {b})
for the function given (for all x, âƒ—yn) by the primitive recursive schema (I.8.7)
with h-part {a} and g-part {b}.
We can now make the intentions implied in the above table ofï¬cial:
â€  By an obvious I.H. the other cases can fend for themselves, but, here, reducing the number of
arguments must not result in 0 arguments, as we have decided not to allow 0-ary functions.
â€¡ The input, a, is a code; the output, {a} or Ï†a, is a function.

140
I. Basic Logic
I.8.32 Theorem. P = {{a} : a âˆˆ}.
Proof. âŠ†-part: Induction on P. The table encapsulates the argument diagram-
matically.
âŠ‡-part: Induction on . It follows trivially from the recursive deï¬nition
of {a} and the fact that P contains the initial functions and is closed under
composition, primitive recursion, and unbounded search.
â–¡
I.8.33 Remark. (Important.) Thus, f âˆˆP iff for some a âˆˆ, f = {a}.
â–¡
I.8.34 Example. Every function f âˆˆP has inï¬nitely many Ï†-indices. Indeed,
let f = { f }. Since f = Î»âƒ—xn.u1
1( f (âƒ—xn)), we obtain f = {âŸ¨1, n, âŸ¨0, 1, 1, 2âŸ©, f âŸ©}
as well. Since âŸ¨1, n, âŸ¨0, 1, 1, 2âŸ©, f âŸ©> f , the claim follows.
â–¡
I.8.35 Theorem. The relation x âˆˆ is primitive recursive.
Proof. Let Ï‡ denote the characteristic function of the relation â€œx âˆˆâ€. Then
Ï‡(0) = 1
Ï‡(x + 1) = 0 if
x + 1 = âŸ¨0, 1, 0âŸ©âˆ¨x + 1 = âŸ¨0, 1, 1âŸ©âˆ¨
(âˆƒn, i)â‰¤x(n > 0 âˆ§0 < i â‰¤n âˆ§x + 1 = âŸ¨0, n, i, 2âŸ©) âˆ¨
(âˆƒa, b, m, n)â‰¤x(Ï‡(a) = 0 âˆ§(a)1 = n âˆ§Seq(b) âˆ§
lh(b) = n âˆ§(âˆ€i)<n(Ï‡((b)i) = 0 âˆ§
((b)i)1 = m) âˆ§x + 1 = âŸ¨1, m, aâŸ©âˆ—b) âˆ¨
(âˆƒa, b, n)â‰¤x(Ï‡(a) = 0 âˆ§(a)1 = n âˆ§Ï‡(b) = 0 âˆ§
(b)1 = n + 2 âˆ§x + 1 = âŸ¨2, n + 1, a, bâŸ©) âˆ¨
(âˆƒa, n)â‰¤x(Ï‡(a) = 0 âˆ§(a)1 = n + 1 âˆ§n > 0 âˆ§
x + 1 = âŸ¨3, n, aâŸ©)
= 1 otherwise
The above can easily be seen to be a course-of-values recursion. For example,
if H(x) = âŸ¨Ï‡(0),. . . , Ï‡(x)âŸ©, then an occurrence of â€œÏ‡(a) = 0â€ above can be
replaced by â€œ(H(x))a = 0â€, since a â‰¤x.
â–¡
We think ofâ€  a computation as a sequence of equations like {e}(âƒ—a) = b. Such an
equation is intuitively read as â€œthe program e, when it runs on input âƒ—a, produces
â€  â€œWe think ofâ€ indicates our determination to avoid a rigorous deï¬nition. The integrity of our
exposition will not suffer from this.

I.8. Computability and Uncomputability
141
output bâ€. An equation will be legitimate iff
(i) it states an input-output relation of some initial function (i.e., (e)0 = 0), or
(ii) it states an input-output relation according to Ï†-indices e such that (e)0 âˆˆ
{1, 2, 3}, using results (i.e., equations) that have already appeared in the
sequence.
For example, in order to state (Âµy){e}(y, âƒ—an) = b â€“ that is, {âŸ¨3, n, eâŸ©}(âƒ—an) =
b â€“ one must ensure that all the equations,
{e}(b, âƒ—an) = 0, {e}(0, âƒ—an) = r0,. . . , {e}(b âˆ’1, âƒ—an) = rbâˆ’1
where the riâ€™s are all non-zero, have already appeared in the sequence. In our
coding, every equation {a}(âƒ—an) = b will be denoted by a triple âŸ¨e, âƒ—an, bâŸ©that
codes, in that order, the Ï†-index, the input, and the output. We will collect (code)
all these triples into a single code, u = âŸ¨. . . , âŸ¨e, âƒ—an, bâŸ©, . . . âŸ©.
Before proceeding, let us deï¬ne the primitive recursive predicates
(1) Î»uv.u âˆˆv (â€œv is a term in the (coded) sequence uâ€),
(2) Î»uvw.v < uw (â€œv occurs before w in the (coded) sequence uâ€).
Primitive recursiveness follows from the equivalences
v âˆˆu â†”Seq(u) âˆ§(âˆƒi)<lh(u)(u)i = v
v < uw â†”v âˆˆu âˆ§w âˆˆu âˆ§(âˆƒi, j)<lh(u)((u)i = v âˆ§(u) j = w âˆ§i < j)
We are now ready to deï¬ne the relation â€œComputation(u)â€ which holds iff
u codes a computation according to the previous understanding. This involves a
lengthyformula.Intheinterestofreadability,commentsenclosedin{ }-brackets
are included in the left margin, to indicate the case under consideration.
Computation(u)â†”Seq(u) âˆ§(âˆ€v)â‰¤u[v âˆˆu â†’
{Î»x.0}
(âˆƒx)â‰¤uv = âŸ¨âŸ¨0, 1, 0âŸ©, x, 0âŸ©âˆ¨
{Î»x.x + 1}
(âˆƒx)â‰¤uv = âŸ¨âŸ¨0, 1, 1âŸ©, x, x + 1âŸ©âˆ¨
{Î»âƒ—xn.xi}
(âˆƒx, n, i)â‰¤u{Seq(x) âˆ§n = lh(x) âˆ§i < nâˆ§
v = âŸ¨âŸ¨0, n, i + 1, 2âŸ©âŸ©âˆ—x âˆ—âŸ¨(x)iâŸ©}âˆ¨
{composition}
(âˆƒx, y, f ,g, m, n, z)â‰¤u{Seq(x) âˆ§Seq(y) âˆ§Seq( f )âˆ§
Seq(g) âˆ§n = lh(x) âˆ§n = lh(g) âˆ§m = lh(y)âˆ§
( f )1 = n âˆ§(âˆ€i)<n(Seq((g)i) âˆ§((g)i)1 = m)âˆ§
v = âŸ¨âŸ¨1, m, f âŸ©âˆ—gâŸ©âˆ—y âˆ—âŸ¨zâŸ©âˆ§
âŸ¨f âŸ©âˆ—x âˆ—âŸ¨zâŸ©< uvâˆ§
(âˆ€i)<nâŸ¨(g)iâŸ©âˆ—y âˆ—âŸ¨(x)iâŸ©< uv}âˆ¨

142
I. Basic Logic
{prim. recursion}
(âˆƒx, y,h,g, n, c)â‰¤u{Seq(h) âˆ§(h)1 = n âˆ§Seq(g)âˆ§
(g)1 = n + 2 âˆ§Seq(y) âˆ§lh(y) = n âˆ§Seq(c)âˆ§
lh(c) = x + 1 âˆ§v = âŸ¨âŸ¨2, n + 1,h,gâŸ©, xâŸ©âˆ—y âˆ—âŸ¨(c)xâŸ©âˆ§
âŸ¨hâŸ©âˆ—y âˆ—âŸ¨(c)0âŸ©<u v âˆ§(âˆ€i)<xâŸ¨g, iâŸ©âˆ—y âˆ—âŸ¨(c)i,
(c)i+1âŸ©< uv}âˆ¨
{(Âµy) f (y, âƒ—xn)}
(âˆƒf , y, x, n,r)â‰¤u{Seq( f ) âˆ§( f )1 = n + 1 âˆ§n > 0âˆ§
Seq(x) âˆ§lh(x) = n âˆ§Seq(r) âˆ§lh(r) = yâˆ§
v = âŸ¨âŸ¨3, n, f âŸ©âŸ©âˆ—x âˆ—âŸ¨yâŸ©âˆ§
âŸ¨f , yâŸ©âˆ—x âˆ—âŸ¨0âŸ©< uvâˆ§
(âˆ€i)<y(âŸ¨f , iâŸ©âˆ—x âˆ—âŸ¨(r)iâŸ©< uv âˆ§(r)i > 0)}]
Clearly, the formula to the right of â€œâ†”â€ above we see that Computation(u) is
primitive recursive.
I.8.36 Deï¬nition (The Kleene T -Predicate). For each n âˆˆN, T (n)(a, âƒ—xn, z)
stands for Computation((z)1) âˆ§âŸ¨a, âƒ—xn, (z)0âŸ©âˆˆ(z)1.
â–¡
The above discussion yields immediately:
I.8.37 Theorem (Kleene Normal Form Theorem).
(1) y = {a}(âƒ—xn) â‰¡(âˆƒz)(T (n)(a, âƒ—xn, z) âˆ§(z)0 = y).
(2) {a}(âƒ—xn) = ((Âµz)T (n)(a, âƒ—xn, z))0.
(3) {a}(âƒ—xn) â†“â‰¡(âˆƒz)T (n)(a, âƒ—xn, z).
I.8.38 Remark. (Very important.) The right hand side of I.8.37(2) above is
meaningful for all a âˆˆN, while the left hand side is only meaningful for a âˆˆ.
We now extend the symbols {a} and Ï†a to be meaningful for all a âˆˆN. In
all cases, the meaning is given by the right hand side of (2).
Of course, if a Ì¸âˆˆ, then (Âµz)T (n)(a, âƒ—xn, z) â†‘for all âƒ—xn, since T (n)(a, âƒ—xn, z)
will be false under the circumstances. Hence also {a}(âƒ—xn) â†‘, as it should be
intuitively: In computer programmerâ€™s jargon, â€œif the program a is syntactically
incorrect, then it will not run, for it will not even compile. Thus, it will deï¬ne
the everywhere undeï¬ned functionâ€.
By the above, I.8.33 now is strengthened to read â€œthus, f âˆˆP iff for some
a âˆˆN, f = {a}â€.
â–¡
We can now deï¬ne a P-counterpart to Râˆ—and PRâˆ—and consider its closure
properties.

I.8. Computability and Uncomputability
143
I.8.39 Deï¬nition (Semi-recursive Relations or Predicates). A relation P(âƒ—x)
is semi-recursive iff for some f âˆˆP, the equivalence
P(âƒ—x) â†”f (âƒ—x) â†“
(1)
holds (for all âƒ—x, of course). Equivalently, we can state that P = dom( f ).
The set of all semi-recursive relations is denoted by Pâˆ—.â€ 
If f = {a} in (1) above, then we say that a is a semi-recursive index of P.
If P has one argument (i.e., P âŠ†N) and a is one of its semi-recursive
indices, then we write P = Wa (Rogers (1967)).
We have at once
I.8.40 Corollary (Normal Form Theorem for Semi-recursive Relations).
P(âƒ—xn) âˆˆPâˆ—iff, for some a âˆˆN,
P(âƒ—xn) â†”(âˆƒz)T (n)(a, âƒ—xn, z)
Proof. By deï¬nition (and Theorem I.8.32 along with Remark I.8.38), P(âƒ—xn) âˆˆ
Pâˆ—iff, for some a âˆˆN, P(âƒ—xn) â†”{a}(âƒ—xn) â†“. Now invoke I.8.37(3).
â–¡
Rephrasing the above (hiding the â€œaâ€, and remembering that PRâˆ—âŠ†Râˆ—)
we have
I.8.41 Corollary (Strong Projection Theorem). P(âƒ—xn) âˆˆPâˆ—iff, for some
Q(âƒ—xn, z) âˆˆRâˆ—,
P(âƒ—xn) â†”(âˆƒz)Q(âƒ—xn, z)
Proof. Fortheonly-ifparttake Q(âƒ—xn, z)tobeÎ»âƒ—xnz.T (n)(a, âƒ—xn, z)forappropriate
a âˆˆN. For the if part take f = Î»âƒ—xn.(Âµz)Q(âƒ—xn, z). Then f âˆˆP and P(âƒ—xn) â†”
f (âƒ—xn) â†“.
â–¡
Here is a characterization of Pâˆ—that is identical in form to the characteriza-
tions of PRâˆ—and Râˆ—(Lemma I.8.20).
I.8.42 Corollary. P(âƒ—xn) âˆˆPâˆ—iff, for some f âˆˆP,
P(âƒ—xn) â†”f (âƒ—xn) = 0
â€  We are making this symbol up (it is not standard in the literature). We are motivated by comparing
the contents of I.8.20 and of I.8.42 below.

144
I. Basic Logic
Proof. Only-if part: Say P(âƒ—xn) â†”g(âƒ—xn) â†“. Take f = Î»âƒ—xn.0 Â· g(âƒ—xn).
If part: Let f = {a}. By I.8.37(1), f (âƒ—xn) = 0 â†”(âˆƒz)(T (n)(a, âƒ—xn, z)âˆ§(z)0 =
0). We are done by strong projection.
â–¡
The usual call-by-value semantics of f (g(âƒ—x), âƒ—y) require divergence if g(âƒ—x) â†‘.
That is, before we embark on calculating the value of f (g(âƒ—x), âƒ—y),we require the
values of all the inputs. In particular, 0 Â· g(âƒ—xn) â†‘iff g(âƒ—xn) â†‘.
Of course, â€œ0Â· g(âƒ—xn)â€ is convenient notation. If we set {b} = g, we can write
instead
((Âµz)(T (n)(b, âƒ—xn, (z)1) âˆ§(z)0 = 0))0
We immediately obtain
I.8.43 Corollary. Râˆ—âŠ†Pâˆ—.
Intuitively, for a predicate R âˆˆRâˆ—we have an algorithm (that computes Ï‡R)
that for any input âƒ—x will halt and answer â€œyesâ€ ( = 0) or â€œnoâ€ ( = 1) to the
question â€œâƒ—x âˆˆR?â€
For a predicate Q âˆˆPâˆ—we are only guaranteed the existence of a weaker
algorithm (for f âˆˆP such that dom( f ) = Q). It will halt iff the answer to the
question â€œâƒ—x âˆˆQ?â€ is â€œyesâ€ (and halting will amount to â€œyesâ€). If the answer
is â€œnoâ€, it will never tell, because it will (as we say for non-halting) â€œloop for
everâ€ (or diverge). Hence the name â€œsemi-recursiveâ€ for such predicates.
I.8.44 Theorem. R âˆˆRâˆ—iff both R and Â¬R are in Pâˆ—.
Proof. Only-if part. By I.8.43 and closure of Râˆ—under Â¬.
If part. Let i and j be semi-recursive indices of R and Â¬R respectively, that
is,
R(âƒ—xn) â†”(âˆƒz)T (n)(i, âƒ—xn, z)
Â¬R(âƒ—xn) â†”(âˆƒz)T (n)( j, âƒ—xn, z)
Deï¬ne
g = Î»âƒ—xn.(Âµz)(T (n)(i, âƒ—xn, z) âˆ¨T (n)( j, âƒ—xn, z))
Trivially, g âˆˆP. Hence, g âˆˆR, since it is total (why?). We are done by noticing
that R(âƒ—xn) â†”T (n)(i, âƒ—xn, g(âƒ—xn)).
â–¡
I.8.45 Deï¬nition (Unsolvable Problems; Halting Problem). A problem is a
question â€œâƒ—x âˆˆR?â€ for any predicate R. â€œThe problem âƒ—x âˆˆR is recursively

I.8. Computability and Uncomputability
145
unsolvableâ€, or just â€œunsolvableâ€, means that R Ì¸âˆˆRâˆ—, that is, intuitively, there
is no algorithmic solution to the problem.
The halting problem has central signiï¬cance in recursion theory. It is the
question whether program x will ever halt if it starts computing on input x.
That is, setting K = {x : {x}(x) â†“} we can then ask â€œx âˆˆK?â€. This question is
the halting problem.â€  We denote the complement of K by K. We will refer to
K as the halting set.
I.8.46 Theorem (Unsolvability of the Halting Problem). The halting prob-
lem is unsolvable.
Proof. It sufï¬ces to show that K is not semi-recursive. Suppose instead that i
is a semi-recursive index of the set. Thus,
x âˆˆK â†”(âˆƒz)T (1)(i, x, z)
or, making the part x âˆˆK â€“ that is, {x}(x) â†‘â€“ explicit,
Â¬(âˆƒz)T (1)(x, x, z) â†”(âˆƒz)T (1)(i, x, z)
(1)
Substituting i into x in (1), we get a contradiction.
â–¡
I.8.47 Remark. Let us look at the above in the light of Wa-notation (p. 143).
Now, {x}(x) â†‘iff x /âˆˆWx; thus we want to show
Â¬(âˆƒi)(Wi = {x : x /âˆˆWx})
(2)
(2) says â€œ{x : x /âˆˆWx} is not a Wiâ€. Well, if (2) is false, then, for some i,
x âˆˆWi â†”x /âˆˆWx
and hence
i âˆˆWi â†”i /âˆˆWi
â€“ a contradiction. This is a classic application of Cantor diagonalizationâ€¡ and
is formally the same argument as in Russellâ€™s paradox, according to which
{x : x /âˆˆx} is not a set â€“ just omit the symbol â€œWâ€ throughout.
The analogy is more than morphological: Our argument shows that {x : x /âˆˆ
Wx} is not an object of the same type as the rightmost object in the { } brackets.
â€  â€œKâ€ is a reasonably well-reserved symbol for the set {x : {x}(x) â†“}. Unfortunately, K is also
used for the ï¬rst projection of a pairing function, but the context easily decides which is which.
â€¡ Cantorâ€™s theorem showed that if (Xa)aâˆˆI is a family of sets, then {a : a /âˆˆXa} is not an â€œXiâ€ â€“
it is not in the family â€“ for otherwise i âˆˆXi iff i /âˆˆXi.

146
I. Basic Logic
Russellâ€™s argument too shows that {x : x /âˆˆx} is not an object of the same
type as the rightmost object in the { } brackets. That is, unlike x, it is not
a set.
â–¡
K âˆˆPâˆ—, of course, since {x}(x) â†“â†”(âˆƒz)T (1)(x, x, z). We conclude that the
inclusion Râˆ—âŠ†Pâˆ—is proper, i.e., Râˆ—âŠ‚Pâˆ—.
I.8.48 Theorem (Closure Properties of Pâˆ—). Pâˆ—is closed under âˆ¨, âˆ§, (âˆƒy)<z,
(âˆƒy), (âˆ€y)<z. It is not closed under either Â¬ or (âˆ€y).
Proof. We will rely on the normal form theorem for semi-recursive relations
and the strong projection theorem.
Given semi-recursive relations P(âƒ—xn), Q(âƒ—ym), and R(y, âƒ—uk) of semi-recursive
indices p, q,r respectively.
âˆ¨:
P(âƒ—xn) âˆ¨Q(âƒ—ym) â†”(âˆƒz)T (n)(p, âƒ—xn, z) âˆ¨(âˆƒz)T (m)(q, âƒ—ym, z)
â†”(âˆƒz)

T (n)(p, âƒ—xn, z) âˆ¨T (m)(q, âƒ—ym, z)

âˆ§:
P(âƒ—xn) âˆ§Q(âƒ—ym) â†”(âˆƒz)T (n)(p, âƒ—xn, z) âˆ§(âˆƒz)T (m)(q, âƒ—ym, z)
â†”(âˆƒw)

(âˆƒz)<wT (n)(p, âƒ—xn, z) âˆ§(âˆƒz)<wT (m)(q, âƒ—ym, z)

Breaking the pattern established by the proof for âˆ¨, we may suggest a simpler
proof for âˆ§: P(âƒ—xn) âˆ§Q(âƒ—ym) â†”((Âµz)T (n)(p, âƒ—xn, z) + (Âµz)T (m)(q, âƒ—ym, z)) â†“.
Yet another proof, involving the decoding function Î»iz.(z)i is
P(âƒ—xn) âˆ§Q(âƒ—ym) â†”(âˆƒz)T (n)(p, âƒ—xn, z) âˆ§(âˆƒz)T (m)(q, âƒ—ym, z)
â†”(âˆƒz)

T (n)(p, âƒ—xn, (z)0) âˆ§T (m)(q, âƒ—ym, (z)1)

There is a technical reason (to manifest itself in II.4.6) that we want to avoid
â€œcomplicatedâ€ functions like Î»iz.(z)i in the proof.
(âˆƒy)<z:
(âˆƒy)<z R(y, âƒ—uk) â†”(âˆƒy)<z(âˆƒw)T (k+1)(r, y, âƒ—uk, w)
â†”(âˆƒw)(âˆƒy)<zT (k+1)(r, y, âƒ—uk, w)
(âˆƒy):
(âˆƒy)R(y, âƒ—uk) â†”(âˆƒy)(âˆƒw)T (k+1)(r, y, âƒ—uk, w)
â†”(âˆƒz)(âˆƒy)<z(âˆƒw)<zT (k+1)(r, y, âƒ—uk, w)

I.8. Computability and Uncomputability
147
Both of the âˆƒ-cases can be handled by the decoding function Î»iz.(z)i. For
example,
(âˆƒy)R(y, âƒ—uk) â†”(âˆƒy)(âˆƒw)T (k+1)(r, y, âƒ—uk, w)
â†”(âˆƒz)T (k+1)(r, (z)0, âƒ—uk, (z)1)
(âˆ€y)<z:
(âˆ€y)<z R(y, âƒ—uk) â†”(âˆ€y)<z(âˆƒw)T (k+1)(r, y, âƒ—uk, w)
â†”(âˆƒv)(âˆ€y)<z(âˆƒw)<vT (k+1)(r, y, âƒ—uk, w)
Think of v above as the successor (+1) of the maximum of some set of w-
values, w0,. . . , wzâˆ’1, that â€œworkâ€ for y = 0,. . . , z âˆ’1 respectively. The usual
overkill proof of the above involves (z)i (or some such decoding scheme) as
follows:
(âˆ€y)<z R(y, âƒ—uk) â†”(âˆ€y)<z(âˆƒw)T (k+1)(r, y, âƒ—uk, w)
â†”(âˆƒw)(âˆ€y)<zT (k+1)(r, y, âƒ—uk, (w)y)
Regarding closure under Â¬ and âˆ€y, K provides a counterexample to Â¬, and
Â¬T (1)(x, x, y) provides a counterexample to âˆ€y.
â–¡
I.8.49 Remark (Projection Theorem). That Pâˆ—is closed under (âˆƒy) is the
content of the (weak) projection theorem.
â–¡
I.8.50 Deï¬nition (Recursively Enumerable Predicates). A predicate R(âƒ—xn)
is recursively enumerable (r.e.) iff R = âˆ…or, for some f âˆˆR of one variable,
R = {(âƒ—xn) : (âˆƒm) f (m) = âŸ¨âƒ—xnâŸ©},â€  or, equivalently,
R(âƒ—xn) â†”(âˆƒm) f (m) = âŸ¨âƒ—xnâŸ©
(1)
â–¡
By (1) and strong projection (I.8.41), every r.e. relation is semi-recursive.
The converse is also true.
I.8.51 Theorem. Every semi-recursive R is r.e.
Proof. Let a be a semi-recursive index of R. If R = âˆ…, then we are done.
Suppose then R(âƒ—an) for some âƒ—an. We deï¬ne a function f by cases:
f (m) =
âŸ¨(m)0,. . . , (m)nâˆ’1âŸ©
if T (n)(a, (m)0,. . . , (m)nâˆ’1, (m)n)
âŸ¨âƒ—anâŸ©
otherwise
It is trivial that f is recursive and satisï¬es (1) above. Indeed, our f is
in PR.
â–¡
â€  Cf. comment regarding rare use of round brackets for n-tuples, p. 136.

148
I. Basic Logic
Suppose that i codes a program that acts on input variables x and y to
compute a function Î»xy. f (x, y). It is certainly trivial to modify the program
to compute Î»x. f (x, a) instead: In computer programming terms, we simply
replace an instruction such as â€œread yâ€ by one that says â€œy := aâ€ (copy the
value of a into y). From the original code, a new code (depending on i and a)
ought to be trivially calculable.
This is the essence of Kleeneâ€™s iteration or S-m-n theorem below.
I.8.52 Theorem (Kleeneâ€™s Iteration or S-m-n Theorem). There is a primitive
recursive function Î»xy.Ïƒ(x, y) such that for all i, x, y,
{i}(âŸ¨x, yâŸ©) = {Ïƒ(i, y)}(x)
Proof. Let a be a Ï†-index of Î»x.âŸ¨x, 0âŸ©, and b a Ï†-index of Î»x.3x. Next we ï¬nd
a primitive recursive Î»y.h(y) such that for all x and y
{h(y)}(x) = âŸ¨x, yâŸ©
(âˆ—)
To achieve this observe that
âŸ¨x, 0âŸ©= {a}(x)
and
âŸ¨x, y + 1âŸ©= 3âŸ¨x, yâŸ©= {b}(âŸ¨x, yâŸ©)
Thus, it sufï¬ces to take
h(0) = a
h(y + 1) = âŸ¨1, 1, b, h(y)âŸ©
Now that we have an h satisfying (âˆ—), we note that
Ïƒ(i, y)
def
= âŸ¨1, 1, i, h(y)âŸ©
will do.
â–¡
I.8.53 Corollary. There is a primitive recursive function Î»iy.k(i, y) such that,
for all i, x, y,
{i}(x, y) = {k(i, y)}(x)
Proof. Let a0 and a1 be Ï†-indices of Î»z.(z)0 and Î»z.(z)1 respectively. Then
{i}((z)0, (z)1) = {âŸ¨1, 1, i, a0, a1âŸ©}(z) for all z, i. Take k(i, y) = Ïƒ(âŸ¨1, 1, i, a0,
a1âŸ©, y).
â–¡

I.8. Computability and Uncomputability
149
I.8.54 Corollary. There is for each m > 0 and n > 0 a primitive recursive
function Î»i âƒ—yn.Sm
n (i, âƒ—yn) such that, for all i, âƒ—xm, âƒ—yn,
{i}(âƒ—xm, âƒ—yn) = {Sm
n (i, âƒ—yn)}(âƒ—xm)
Proof. Let ar (r = 0,. . . , m âˆ’1) and br (r = 0,. . . , n âˆ’1) be Ï†-indices so that
{ar} = Î»xy.(x)r (r = 0,. . . , m âˆ’1) and {br} = Î»xy.(y)r (r = 0,. . . , n âˆ’1).
Set c(i) = âŸ¨1, 2, i, a0,. . . , amâˆ’1, b0,. . . , bnâˆ’1âŸ©, for all i âˆˆN, and let d be a
Ï†-index of Î»âƒ—xm.âŸ¨âƒ—xmâŸ©. Then,
{i}(âƒ—xm, âƒ—yn) = {c(i)}(âŸ¨âƒ—xmâŸ©, âŸ¨âƒ—ynâŸ©)
= {k(c(i), âŸ¨âƒ—ynâŸ©)}(âŸ¨âƒ—xmâŸ©)
by I.8.53
= {âŸ¨1, m, k(c(i), âŸ¨âƒ—ynâŸ©), dâŸ©}(âƒ—xm)
Take Î»i âƒ—yn.Sm
n = âŸ¨1, m, k(c(i), âŸ¨âƒ—ynâŸ©), dâŸ©.
â–¡
Since P-functions are closed under permutation of variables, there is no signi-
ï¬cance (other than notational convenience, and a random left vs. right choice)
in presenting the S-m-n theorem in terms of a â€œneatâ€ left-right partition of the
variable list. Any variable sub-list can be parametrized.
I.8.55 Corollary (Kleeneâ€™s Recursion Theorem). If Î»zâƒ—x. f (z, âƒ—xn) âˆˆP, then
for some e,
{e}(âƒ—xn) = f (e, âƒ—xn)
for all âƒ—xn
Proof. Let {a} = Î»zâƒ—xn. f

Sn
1(z, z), âƒ—xn

. Then
f

Sn
1(a, a), âƒ—xn

= {a}(a, âƒ—xn)
=

Sn
1(a, a)

(âƒ—xn)
by I.8.54
Take e = Sn
1(a, a).
â–¡
I.8.56 Deï¬nition. A complete index set is a set A = {x : {x} âˆˆQ} for some
Q âŠ†P.
A is trivial iff A = âˆ…or A = N (correspondingly, Q = âˆ…or Q = P).
Otherwise it is non-trivial.
â–¡
I.8.57 Theorem (Rice). A complete index set is recursive iff it is trivial.
Thus, algorithmically we can only decide trivial properties of programs.

150
I. Basic Logic
Proof. (The idea of this proof is attributed in Rogers (1967) to G. C. Wolpin.)
If part: Immediate, since Ï‡âˆ…= Î»x.1 and Ï‡N = Î»x.0.
Only-if part: By contradiction, suppose that A = {x : {x} âˆˆQ} is non-
trivial, yet A âˆˆRâˆ—. So let a âˆˆA and b /âˆˆA. Deï¬ne f by
f (x) =
b
if x âˆˆA
a
if x /âˆˆA
Clearly,
x âˆˆA iff f (x) /âˆˆA,
for all x
(1)
By the recursion theorem, there is an e such that { f (e)} = {e} (apply I.8.55 to
Î»xy.{ f (x)}(y)).
Thus, e âˆˆA iff f (e) âˆˆA, contradicting (1).
â–¡
A few more applications of the recursion theorem will be found in the
Exercises.
I.8.58 Example. Every function of P has inï¬nitely many indices (revisited).
For suppose not, and let A = {x : {x} âˆˆQ} be ï¬nite, where Q = { f }. Then A
is recursive (why?), contradicting Riceâ€™s theorem.
â–¡
We have seen that progressing along PRâˆ—, Râˆ—, Pâˆ—we obtain strictly more
inclusive sets of relations, or, intuitively, progressively more â€œcomplexâ€ predi-
cates. For example, we can easily â€œsolveâ€ Î»xy.x < y, we can only â€œhalfâ€ solve
x âˆˆK, and we cannot even do that for x /âˆˆK. The latter is beyond Pâˆ—. Still,
all three are â€œarithmeticâ€â€  or â€œarithmeticalâ€ predicates in a sense that we make
precise below. The interest immediately arises to classify arithmetic predicates
according to increasing â€œcomplexityâ€. This leads to the arithmetic(al) hierarchy
of Kleene and Mostowski.
I.8.59 Deï¬nition. The set of all arithmetic(al)â€¡ predicates is the least set
that includes Râˆ—and is closed under (âˆƒx) and (âˆ€x). We will denote this set
by .
â–¡
â€  Accent on â€œmetâ€.
â€¡ We will adhere to the term â€œarithmeticâ€, as in Smullyan (1992). The reader will note that these
predicates are introduced in two different ways in Smullyan (1992), each different from the
above. One is indicated by a capital â€œAâ€ and the other by a lowercase â€œaâ€. All three deï¬nitions are
equivalent. We follow the standard deï¬nition given in works in recursion theory (Rogers (1967),
Hinman (1978), Tourlakis (1984)).

I.8. Computability and Uncomputability
151
We sort arithmetic relations into a hierarchy as follows (Kleene (1943),
Mostowski (1947)).
I.8.60 Deï¬nition (The Arithmetic Hierarchy). We deï¬ne by induction on
n âˆˆN:
0 = 0 = Râˆ—
n+1 = {(âˆƒx)P : P âˆˆn}
n+1 = {(âˆ€x)P : P âˆˆn}
The variable â€œxâ€ above is generic.
We also deï¬ne, for all n, n = n âˆ©n and also set âˆ= 
nâ‰¥0(n âˆªn).
â–¡
I.8.61Remark. Intuitively,thearithmetichierarchyiscomposedofallrelations
of the form (Q1x1)(Q2x2) . . . (Qnxn)R, where R âˆˆRâˆ—and Qi âˆˆ{âˆƒ, âˆ€} for
i = 1, . . . , n. If n = 0 there is no quantiï¬er preï¬x. Since âˆƒxâˆƒy and âˆ€xâˆ€y can
be â€œcollapsedâ€ into a single âˆƒand single âˆ€respectively,
Pause. Do you believe this?
one can think of the preï¬x as a sequence of alternating quantiï¬ers. The relation
is placed in a -set (respectively, -set) iff the leftmost quantiï¬er is a â€œâˆ€â€
(respectively, â€œâˆƒâ€).
â–¡
I.8.62 Lemma. R âˆˆn iff (Â¬R) âˆˆn. R âˆˆn iff (Â¬R) âˆˆn.
Proof. We handle both equivalences simultaneously. For n = 0 this is so by
closure properties of Râˆ—.
Assuming the claim for n, we have
R âˆˆn+1 iff R â†”(âˆƒy)Q and Q âˆˆn
iff (I.H.)R â†”Â¬(âˆ€y)Â¬Q and (Â¬Q) âˆˆn
iff Â¬R â†”(âˆ€y)Â¬Q and (Â¬Q) âˆˆn
iff Â¬R âˆˆn+1
and
R âˆˆn+1 iff R â†”(âˆ€y)Q and Q âˆˆn
iff (I.H.) R â†”Â¬(âˆƒy)Â¬Q and (Â¬Q) âˆˆn
iff Â¬R â†”(âˆƒy)Â¬Q and (Â¬Q) âˆˆn
iff Â¬R âˆˆn+1
â–¡

152
I. Basic Logic
It is trivial that âˆâŠ†. The following easy lemma yields the converse
inclusion as a corollary.
I.8.63 Lemma. We have the following closure properties:
âˆ
is closed under (1)â€“(6) from the following list.
n (n â‰¥0)
is closed under (1)â€“(4).
n (n â‰¥0)
is closed under (1)â€“(3), and, if n > 0, also (5).
n (n â‰¥0)
is closed under (1)â€“(3), and, if n > 0, also (6).
(1) Replacement of a variable by a recursive functionâ€ 
(2) âˆ¨, âˆ§
(3) (âˆƒy)<z, (âˆ€y)<z
(4) Â¬
(5) (âˆƒy)
(6) (âˆ€y)
Proof. (1) follows from the corresponding closure of Râˆ—. The rest follow at
once from I.8.60 and the techniques of I.8.48 ((4) also uses I.8.62).
â–¡
I.8.64 Corollary.  = âˆ. Hence, Deï¬nition I.8.60 classiï¬es all arithmetic
predicates according to deï¬nitional complexity.
We next see that the complexity of arithmetic predicates increases in more
than just â€œformâ€ as n increases (i.e., as the form â€œ(Q1x1)(Q2x2) . . . (Qnxn)Râ€
gets more complex with a longer alternating quantiï¬er preï¬x, we do get new
predicates deï¬ned).
I.8.65 Proposition. n âˆªn âŠ†n+1 for n â‰¥0.
Proof. Induction on n.
Basis. For n = 0, n âˆªn = Râˆ—. On the other hand, 1 = Pâˆ—(why?), while
(by I.8.62) 1 = {Q : (Â¬Q) âˆˆPâˆ—}. Thus 1 = 1 âˆ©1 = Râˆ—by I.8.44.
We consider now the n +1 case (under the obvious I.H.). Let R(âƒ—xr) âˆˆn+1.
Then, by I.8.63(1), so is Î»zâƒ—xr.R(âƒ—xr), where z is not among the âƒ—xr. Now, R â†”
(âˆ€z)R, but (âˆ€z)R âˆˆn+2.
â€  Since un
i , Î»x.0, and Î»x.x + 1 are recursive, this allows the full range of the Grzegorczyk substi-
tutions (Exercise I.68), i.e., additionally to function substitution, also expansion of the variable
list, permutation of the variable list, identiï¬cation of variables, and substitution of constants into
variables.

I.8. Computability and Uncomputability
153
Next, let R(âƒ—xr) â†”(âˆƒz)Q(z, âƒ—xr), where Q âˆˆn. By the I.H., Q âˆˆn+1;
hence Q âˆˆn+1. Thus, R âˆˆn+2.
The argument is similar if we start the previous sentence with â€œLet R(âƒ—xr) âˆˆ
n+1â€.
â–¡
I.8.66 Corollary. n âŠ†n+1 and n âŠ†n+1, for n â‰¥0.
I.8.67 Corollary. n âŠ†n+1 for n â‰¥0.
We next sharpen the inclusions above to proper inclusions.
I.8.68 Deï¬nition (Kleene). For n â‰¥1 we deï¬ne Î»xy.En(x, y) by induction:
E1(x, y) â†”(âˆƒz)T (1)(x, y, z)
En+1(x, y) â†”(âˆƒz)Â¬En(x, âŸ¨zâŸ©âˆ—y)
where â€œâŸ¨âŸ©â€ is our standard coding of p. 136.
â–¡
Of course, âŸ¨zâŸ©= 2z+1.
I.8.69 Lemma. En âˆˆn and Â¬En âˆˆn, for n â‰¥1.
Proof. A trivial induction (via I.8.62 and I.8.63).
â–¡
I.8.70 Theorem (Enumeration or Indexing Theorem (Kleene)).
(1) R(âƒ—xr) âˆˆn+1 iff R(âƒ—xr) â†”En+1(i, âŸ¨âƒ—xrâŸ©) for some i.
(2) R(âƒ—xr) âˆˆn+1 iff R(âƒ—xr) â†”Â¬En+1(i, âŸ¨âƒ—xrâŸ©) for some i.
Proof. The if part is Lemma I.8.69 (with the help of I.8.63(1)). We prove the
only-if part ((1) and (2) simultaneously) by induction on n.
Basis. n = 0. If R(âƒ—xr) âˆˆ1 = Pâˆ—, then so is R((u)0, . . . , (u)râˆ’1). Thus, for
some i (semi-recursive index), R((u)0, . . . , (u)râˆ’1) â†”(âˆƒz)T (1)(i, u, z); hence
R(âƒ—xr) â†”(âˆƒz)T (1)(i, âŸ¨âƒ—xrâŸ©, z) â†”E1(i, âŸ¨âƒ—xrâŸ©).
If R(âƒ—xr) âˆˆ1, then (Â¬R(âƒ—xr)) âˆˆ1 = Pâˆ—. Thus, for some e, Â¬R(âƒ—xr) â†”
E1(e, âŸ¨âƒ—xrâŸ©); hence R(âƒ—xr) â†”Â¬E1(e, âŸ¨âƒ—xrâŸ©).
The induction step. (Based on the obvious I.H.) Let R(âƒ—xr) âˆˆn+2. Then
R(âƒ—xr) â†”(âˆƒz)Q(z, âƒ—xr), where Q âˆˆn+1, hence (I.H.)
Q(z, âƒ—xr) â†”Â¬En+1(e, âŸ¨z, âƒ—xrâŸ©)
for some e
(i)

154
I. Basic Logic
Since âŸ¨z, âƒ—xrâŸ©= âŸ¨zâŸ©âˆ—âŸ¨âƒ—xrâŸ©, (i) yields
R(âƒ—xr) â†”(âˆƒz)Â¬En+1(e, âŸ¨zâŸ©âˆ—âŸ¨âƒ—xrâŸ©)
â†”En+2(e, âŸ¨âƒ—xrâŸ©)
(by the deï¬nition of En+2)
An entirely analogous argument takes care of â€œLet R(âƒ—xr) âˆˆn+2â€.
â–¡
I.8.71 Corollary. The same set of arithmetic relations, , can be deï¬ned by
setting 0 = 0 = PRâˆ—. Indeed, no sets in the hierarchy are affected by this
change, except 0 = 0.
I.8.72 Theorem (Hierarchy Theorem (Kleene, Mostowski)).
(1) n+1 âˆ’n+1 Ì¸= âˆ…,
(2) n+1 âˆ’n+1 Ì¸= âˆ….
Moreover, all the inclusions in I.8.65 (n > 0), I.8.66 (n â‰¥0), and I.8.67 (n > 0)
are proper.
Proof. (1): En+1 âˆˆn+1 âˆ’n+1. Indeed (see also I.8.69), if En+1 âˆˆn+1,
then
En+1(x, âŸ¨xâŸ©) â†”Â¬En+1(i, âŸ¨xâŸ©)
(1â€²)
for some i (I.8.63(1) and, I.8.70). Letting x = i in (1â€²), we get a contradiction.
(2): As in (1), but use Â¬En+1 as the counterexample.
For I.8.65: Let R(x, y) â†”En(x, âŸ¨xâŸ©) âˆ¨Â¬En(y, âŸ¨yâŸ©). Since En âˆˆn and
(Â¬En) âˆˆn, I.8.63(1), I.8.65, and closure of n+1 under âˆ¨yield R âˆˆn+1.
Let x0 and y0 be such that En(x0, âŸ¨x0âŸ©) is false and En(y0, âŸ¨y0âŸ©) is true (if no
such x0, y0 exist, then En is recursive (why?); not so for n > 0 (why?)).
Now, R /âˆˆn âˆªn, for, otherwise, say it is in n. Then so is R(x0, y) (that
is, Â¬En(y, âŸ¨yâŸ©)), which is absurd. Similarly, if R âˆˆn, we are led to the absurd
conclusion that R(x, y0) âˆˆn.
For I.8.66: K âˆˆ1 âˆ’0. For n > 0, n = n+1 implies n = n+1
(why?); hence n âˆªn = n+1 âˆªn+1 âŠ‡n+1 âˆ©n+1 = n+1; thus
n âˆªn = n+1 by I.8.65, contrary to what we have just established above.
Similarly for the dual n vs. n+1.
For I.8.67: If n+1 = n, then n+1 = n âˆ©n âŠ†n âˆªn, which has
been established as an absurdity for n > 0.
â–¡

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 155
I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness
In this section we apply recursion-theoretic techniques to the proof theory
of a certain axiomatic arithmetic in order to derive a major result (GÂ¨odelâ€™s
ï¬rst incompleteness theorem) regarding the inadequacy of the syntactic proof
apparatus.
We have to overcome a small obstacle outright: Our recursion theory is
a theory of number-theoretic functions and relations. We need some way to
translate its results in the realm of strings (over some alphabet) so that our theory
can handle recursive, r.e., primitive recursive, etc., functions and predicates
that have inputs and outputs that are such strings. For example, a recursively
axiomatized theory, we say, is one with a recursive set of axioms. But what do
we mean by a recursive set of strings?
Well, we can code strings by numbers, and then use the numbers as proxies
for the strings. This is the essence of GÂ¨odel numbering, invented by GÂ¨odel
(1931) towards that end.
Given a ï¬nite alphabet V . We denote by V âˆ—the set of all strings over V .
We use the term â€œGÂ¨odel-numberingâ€ for any 1-1 (not necessarily onto) function
f : V âˆ—â†’N that is intuitively computable. More precisely, we want to have
two algorithms: one to compute f (w) for each w âˆˆV âˆ—, and one to check if a
given n âˆˆN codes some string over V (i.e., is in the range of f ), and if so, to
compute w = f âˆ’1(n).
We use special notation in the context of GÂ¨odel numberings. Rather than
â€œ f (w)â€ â€“ the GÂ¨odel number of w â€“ we write âŒœwâŒ, allowing the context to tell
which speciï¬c f we have in mind.
The above, of course, is not a precise deï¬nition, since the term â€œcomputableâ€
(function) was never deï¬ned between â€œmixedâ€ ï¬elds (V âˆ—on the left and N on
the right). This does not present a problem however, for in practice any speciï¬c
GÂ¨odel numbering will be trivially seen (at the intuitive level) to satisfy the
algorithmic coding-decoding requirement.
The heart of the matter in effecting a GÂ¨odel numbering of a given V âˆ—is
simple: We start with a â€œprimitiveâ€ numbering by assigning distinct numbers
from N to each symbol of V . Then, in principle, we can extend this primitive
numbering to the entire V âˆ—by recursion on strings.
However, in the cases of interest to us, that is, terms, formulas, and sequences
of such over some language L, we will prefer to do our recursion on terms,
formulas, and sequences (rather than on generic strings).

156
I. Basic Logic
For example, if the numbers n0, n1, . . . , nm were already assigned to the
formal function symbol g (m-ary) and to the terms t1, . . . , tm respectively, one
would just need a way to obtain a number for gt1 . . . tm from the numbers
we have just listed. This simply requires the presence of some number-coding
scheme, â€œâŸ¨âŸ©â€, to compute âŸ¨n0, n1, . . . , nmâŸ©. GÂ¨odel used, essentially, the prime
power coding scheme â€œâŸ¨âŸ©,lh, Seq, (z)iâ€ of p. 136.
We will not adopt prime power coding here, but instead we will use a different
coding based on GÂ¨odelâ€™s Î²-function. Our approach is based on the exposition in
Shoenï¬eld (1967), the motivation being to do the number coding with as little
number theory as possible,â€  so that when we have to revisit all this in the next
chapter â€“ formally this time â€“ our task will be reasonably easy, and short.
We ï¬x, for the balance of the chapter, a simple pairing function, that is,
a total and 1-1 function J : N2 â†’N that can be obtained via addition and
multiplication. For example, let us set
J(x, y) = (x + y)2 + x
for all x, y
(âˆ—)
J is trivially primitive recursive. Its 1-1-ness follows from the observation
that x +y < u+v implies x +y+1 â‰¤u+v and hence J(x, y) < (x +y+1)2 â‰¤
(u + v)2 â‰¤J(u, v).
Thus, if J(x, y) = J(u, v), then it must be x + y = u + v, and therefore
x = u. But then y = v.
The symbols K and L are reserved for the projection functions of a pairing
function J.â€¡
We see at once that K, L are in PR; indeed,
K(z) = (Âµx)â‰¤z(âˆƒy)â‰¤z(J(x, y) = z)
L(z) = (Âµy)â‰¤z(âˆƒx)â‰¤z(J(x, y) = z)
Let us then address the task of ï¬nding a way to â€œalgorithmicallyâ€ code a number
sequence a0, . . . , an by a single number, so that each of the ai can be algorith-
mically recovered from the code as long as we know its position i.
We start by setting
c = max{1 + J(i, ai) : i â‰¤n}
(âˆ—âˆ—)
â€  GÂ¨odelâ€™s original approach needed enough number theory to include the prime factorization
theorem. An alternative approach due to Quine, utilized in Smullyan (1961, 1992), requires the
theorem that every number can be uniquely written as a string of base-b digits out of some ï¬xed
alphabet {0, 1, . . . , bâˆ’1}. On the other hand, employing the variant of GÂ¨odelâ€™s Î²-function found
in Shoenï¬eld (1967), one does not even need the Chinese remainder theorem.
â€¡ It is unfortunate, but a standard convention, that this â€œKâ€ clashes with the usage of â€œKâ€ as the
name of the halting set (I.8.45). Fortunately, it is very unlikely that the context will allow the
confusion of the two notations.

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 157
Next, let p be the least common multiple of the numbers 1, 2, . . . , c + 1, for
short, lcm{1, 2, . . . , c + 1}, that is,
p = (Âµz)
%
z > 0 âˆ§(âˆ€i)â‰¤c(i + 1)|z
&
(âˆ—âˆ—âˆ—)
We recall the following deï¬nition from elementary number theory:
I.9.1 Deï¬nition. The greatest common divisor, or gcd, of two natural numbers
a and b such that a + b Ì¸= 0 is the largest d such that d|a and d|b.â€  We write
d = gcd(a, b) = gcd(b, a).
â–¡
I.9.2 Lemma. For each a and b in N (a + b Ì¸= 0) there are integers u and v
such that gcd(a, b) = au + bv.
Proof. The set A = {ax +by > 0 : x âˆˆZâˆ§y âˆˆZ} is nonempty. For example,
if a Ì¸= 0, then a1+b0 > 0 is in A. Let g be the least member of A. So, for some
u and v in Z,
g = au + bv > 0
(1)
We argue that g|a (and similarly, g|b). If not, let
0 < r < g
(2)
such that a = gq +r (q = âŒŠa/gâŒ‹). Thus, using (1), r = a âˆ’gq = a(1âˆ’uq)âˆ’
bvq, a member of A. (2) contradicts the minimality of g.
Thus, g is a common divisor of a and b. On the other hand, by (1), every
common divisor d of a and b divides g; hence d â‰¤g. Thus, g = gcd(a, b).
â–¡
I.9.3 Deï¬nition. Two natural numbers a and b (a + b Ì¸= 0) are relatively prime
iff gcd(a, b) = 1.
â–¡
The above is the standard deï¬nition of relative primality. However, an equi-
valent deï¬nition can save us from a lot of grief when we redo all this formally
in the next chapter. This alternate deï¬nition is furnished by the statement of
the following lemma, which we prove here (informally) only as a preview for
things to come in the next chapter.
I.9.4 Lemma. The natural numbers a and b (a + b Ì¸= 0) are relatively prime
iff for all x > 0, a|bx implies a|x.
â€  If a = b = 0, then such a largest d does not exist, since (âˆ€x)(x > 0 â†’x|0). Hence the restriction
to a + b Ì¸= 0.

158
I. Basic Logic
Proof. Only-if part.
By I.9.2, 1 = au + bv for some integers u and v. Thus,
x = axu + bxv; hence a|x.
If part.
We prove the contrapositive.â€  Let 1 < d = gcd(a, b). Write a = dk
and b = dm. Now, a|bk, but a > k; hence a Ì¸ | k.
â–¡
We can now prove:
I.9.5 Lemma. With c > 0 arbitrary, and p chosen as in (âˆ—âˆ—âˆ—) on p. 156 above,
gcd

1 + ( j + 1)p, 1 + (i + 1)p

= 1
for all
0 â‰¤i < j â‰¤c + 1
Proof. We will proceed by contradiction. Let d > 1 divide both 1 + ( j + 1)p
and 1 + (i + 1)p. Then d > c + 1 (otherwise d|p and hence d Ì¸ | 1 + ( j + 1)p).
Now d divides any linear combination of 1 + ( j + 1)p and 1 + (i + 1)p, in
particular j âˆ’i (which is equal to (1+(i +1)p)( j +1)âˆ’(1+( j +1)p)(i +1)).
Since 0 < j âˆ’i â‰¤c + 1, this is impossible.
â–¡
I.9.6 Lemma. If d0, . . . , dr, b, with b greater than 1, are such that gcd(b, di) =
1 for all 0 â‰¤i â‰¤r, then b Ì¸ | lcm{di : i = 0, . . . ,r}.
Proof. Set z = lcm{di : i = 0, . . . ,r}, and suppose that b|z; thus z = bk for
some k. By I.9.4 (since di|z), we have di|k for all 0 â‰¤i â‰¤r. From b > 1
follows k < z, contradicting the minimality of z.
â–¡
Armed with Lemmata I.9.5 and I.9.6, we can now code any nonempty subset
(the emphasis indicating disrespect for order) of numbers i j, 0 â‰¤i j â‰¤c (c > 0
being an arbitrary integer), by the least common multiple q of the numbers
1 + (i j + 1)p, p being the lcm of {1, 2, . . . , c + 1}. Indeed, a number of the
form 1 + (z + 1)p, for 0 â‰¤z â‰¤c, divides q precisely when z is one of the i j.
Thus, we can code the sequence a0, a1, . . . , an by coding the set of numbers
J(i, ai) (position information i is packed along with value ai), as suggested
immediately above: We let
q = lcm
'
1 +

1 + J(i, ai)

p : i = 0, . . . , n
(
With n, c, p, q as above (see (âˆ—âˆ—) and (âˆ—âˆ—âˆ—)), we recover ai as
(Âµy)
%
1 +

1 + J(i, y)

p | q
&
(1)
â€  The contrapositive of A â†’B is the tautologically equivalent Â¬B â†’Â¬A.

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 159
or, setting, a = J(c, q) and forcing the search to exit gracefully even when q
is nonsense (that is, not a code),
(Âµy)
%
y = a âˆ¨1 +

1 + J(i, y)

p | L(a)
&
(2)
Of course, p in (2) above is given in terms of c by (âˆ—âˆ—âˆ—), that is, it equals the
least common multiple of the numbers {1, 2, . . . , K(a) + 1}.
In what follows we will use the notation Î²(a, i) for the expression (2) above.
Thus,
Î»ai.Î²(a, i) âˆˆPR
Î²(a, i) â‰¤a
for all a, i
(3)
The â€œâ‰¤â€ in (3) is â€œ=â€ iff the search in (1) above fails. Otherwise y â‰¤J(i, y) <
1 + J(i, y) < L(a) â‰¤a + 1; hence y â‰¤J(i, y) < a.
For any sequence a0, . . . , an there is an a such that Î²(a, i) = ai for i â‰¤n.
This is the version of GÂ¨odelâ€™s Î²-function in Shoenï¬eld (1967). Since we will
carefully prove all the properties of this Î²-function â€“ and of its associated
sequence coding scheme â€“ later on, in the context of axiomatic arithmetic
(Section II.2), we will avoid â€œobviousâ€ details here.
I.9.7 Remark. (1) Why lcm? Both in (âˆ—âˆ—âˆ—) and in the deï¬nition of q above, we
could have used product instead. Thus, p could have been given as c-factorial
(c!), and q as the productâ€ 
" '
1 +

1 + J(i, ai)

p : i = 0, . . . , n
(
The seemingly more complex approach via lcm is actually formally simpler.
The lcm is explicitly deï¬nable, while the product of a variable number of factors
necessitates a (primitive) recursive deï¬nition. Alas, Î² will be employed in the
next chapter precisely in order to show that recursive deï¬nitions are allowed in
Peano arithmetic.
(2) GÂ¨odelâ€™s original Î² was less â€œelementaryâ€ than the above: Starting with
a modiï¬ed â€œcâ€,
câ€² = max{1 + ai : i â‰¤n}
(âˆ—âˆ—â€²)
one then deï¬nes
pâ€² = n!câ€²
(âˆ—âˆ—âˆ—â€²)
As in Lemma I.9.5, gcd(1+(1+ j)pâ€², 1+(1+i)pâ€²) = 1 for all 0 â‰¤j < i â‰¤n.
â€  As a matter of fact, for a set of relatively prime numbers, their lcm is the same as their product.

160
I. Basic Logic
By the Chinese remainder theorem (see, e.g., LeVeque (1956)), there is a
qâ€² âˆˆN such that qâ€² â‰¡ai mod 1 + (1 + i)pâ€² for i = 0, . . . , n. Thus, qâ€² codes
the sequence a0, . . . , an. Since each ai satisï¬es ai < 1 + (1 + i)pâ€² by the
choice of pâ€², ai is recovered from qâ€² by rem(qâ€², 1 + (1 + i)pâ€²). One then sets
Î²â€²(qâ€², pâ€², i) = rem(qâ€², 1 + (1 + i)pâ€²). This â€œÎ²â€²â€ is GÂ¨odelâ€™s original â€œÎ²â€.
â–¡
This ability to code sequences without using (primitive) recursion can be used
to sharpen Corollary I.8.71, so that we can start the arithmetic hierarchy with
even simpler relations 0 = 0 than primitive recursive.
We will proï¬t by a brief digression here from our discussion of GÂ¨odel num-
berings to show that any R âˆˆPRâˆ—is a (âˆƒx)-projection of a very â€œsimpleâ€
relation, one out of the set deï¬ned below. This result will be utilized in our
study of (that is, in the metatheory of) formal arithmetic.
I.9.8 Deï¬nition (Constructive Arithmetic Predicates). (Smullyan (1961,
1992), Bennett (1962)). The constructive arithmetic predicates are the smallest
set of predicates that includes Î»xyz.z = x + y, Î»xyz.z = x Â· y, and is closed
under Â¬, âˆ¨, (âˆƒy)<z, and explicit transformations.
Explicit transformations (Smullyan (1961), Bennett (1962)) are exactly the
following: Substitution of any constant into a variable, expansion of the vari-
ables list, permutation of variables, identiï¬cation of variables.
We will denote the set of constructive arithmetic predicates by CA.â€ 
â–¡
Trivially,
I.9.9 Lemma. CA âŠ†PRâˆ—.
and
I.9.10 Lemma. CA is closed under (âˆƒy)â‰¤z. Conversely, the deï¬nition above
could have been given in terms of (âˆƒy)â‰¤z.
Proof. (âˆƒy)â‰¤z R(y, âƒ—w) â†”R(z, âƒ—w) âˆ¨(âˆƒy)<z R(y, âƒ—w). Conversely, (âˆƒy)<z R(y,
âƒ—w) â†”(âˆƒy)â‰¤z(Â¬y = z âˆ§R(y, âƒ—w)). Of course, y = z is an explicit transform of
x Â· y = z.
â–¡
â€  Smullyan (1992) uses â€œ0â€, but as we already have been shifting the meaning of that symbol, this
name would not be reliable here. Some use â€œ0â€ (due to the presence of bounded quantiï¬cation),
but this symbol also suffers from the same ambiguities.

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 161
I.9.11 Lemma. CA is closed under substitution of the term x +1 into variables.
Proof. We do induction on the deï¬nition of CA.
Basis.
(1) z = x + y + 1:
z = x + y + 1 â†”(âˆƒw)â‰¤z(z = x + w âˆ§w = y + 1). Of
course, w = y + 1 is an explicit transform of w = y + u.
(2) z + 1 = x + y:
z + 1 = x + y â†”(âˆƒw)â‰¤x(x = w + 1 âˆ§z = w + y) âˆ¨
(âˆƒw)â‰¤y(y = w + 1 âˆ§z = x + w).
(3) z = x(y + 1):
z = x(y + 1) â†”(âˆƒw)â‰¤z(z = xw âˆ§w = y + 1).
(4) z + 1 = xy:
z + 1 = xy â†”(âˆƒu)â‰¤x(âˆƒw)â‰¤y(x = u + 1 âˆ§y = w + 1 âˆ§z =
uw+u+w). Of course, z = uw+u+w â†”(âˆƒx)â‰¤z(âˆƒy)â‰¤z(z = x + yâˆ§x =
uw âˆ§y = u + w).
The property of CA that we are proving trivially propagates with the Boolean
operations. Moreover (âˆƒy)<z+1R(y, âƒ—x) â†”(âˆƒy)â‰¤z R(y, âƒ—x).
â–¡
(âˆƒy)â‰¤z is a â€œderived operationâ€ (I.9.10); thus it is not checked in the proof
above. Needless to say, were it the principal (primary) bounded quantiï¬cation
deï¬nitionally, then we would have employed the argument (âˆƒy)â‰¤z+1R(y, âƒ—x) â†”
R(z + 1, âƒ—x) âˆ¨(âˆƒy)â‰¤z R(y, âƒ—x) instead, and the I.H., to conclude the above
proof.
I.9.12 Corollary. CA is closed under substitution into variables by functions
f satisfying
(i) Î»yâƒ—xn.y = f (âƒ—xn) is in CA, and
(ii) for some i, f (âƒ—xn) â‰¤xi + 1 for all âƒ—xn.
Proof. Let R(y, âƒ—z) be in CA, and f be as described above; in particular let
f (âƒ—xn) â‰¤xi + 1 for all âƒ—xn. Then
R( f (âƒ—xn), âƒ—z) â†”(âˆƒy)â‰¤xi+1(y = f (âƒ—xn) âˆ§R(y, âƒ—z))
â–¡
I.9.13 Lemma. The graphs of J, K, and L (p. 156) are in CA.
By the â€œgraphâ€ of a function Î»âƒ—x. f (âƒ—x) we understand the relation Î»yâƒ—x.y = f (âƒ—x).
Proof. The case for J is trivial. K and L present some mild interest. For
example,
y = K(x) â†”
%
y = x + 1 âˆ§Â¬(âˆƒz)â‰¤x J(y, z) = x
&
âˆ¨(âˆƒz)â‰¤x J(y, z) = x
â–¡

162
I. Basic Logic
I.9.14 Remark. We can substitute K(x) and L(x) for variables in CA relations,
by I.9.12, since K(x) â‰¤x + 1 for all x and L(x) â‰¤x + 1 for all x. J(x, y)
is â€œtoo bigâ€ for I.9.12 to apply to it, but that too can be substituted, as results
of Bennett show (1962, retold in Tourlakis (1984)). However, we will not need
this fact in this volume.
â–¡
I.9.15 Proposition. Î»aiy.Î²(a, i) = y is in CA.
Proof. Recall that Î²(a, i) = (Âµy)(y = a âˆ¨1+(1+ J(i, y))p(a) | L(a)), where
p(a) = (Âµz)(z > 0 âˆ§(âˆ€i)â‰¤K(a)(i + 1) | z)
We set for convenience P(a, i, y) â†”1 + (1 + J(i, y))p(a) | L(a), and show
that P is in CA. Now P(a, i, y) â†”(âˆƒu)â‰¤a(L(a) = u(1 + (1 + J(i, y))p(a))),
and in view of I.9.12 (cf. I.9.14) and closure under (âˆƒy)â‰¤w, we need only show
that
z = u

1 + (1 + J(i, y))p(a)

is in CA. The above is equivalent to
(âˆƒv, x)â‰¤z
%
v = p(a) âˆ§z = ux âˆ§x = 1 + (1 + J(i, y))v
&
where we have used the shorthand â€œ(âˆƒv, x)â‰¤zâ€ for â€œ(âˆƒv)â‰¤z(âˆƒx)â‰¤zâ€.
That x = 1 + (1 + J(i, y))v is in CA follows from
x = 1 + (1 + J(i, y))v â†”(âˆƒu, w,r, s)â‰¤x(w = J(i, y) âˆ§u
= w + 1 âˆ§r = uv âˆ§x = r + 1)
We concentrate now on v = p(a). We start with the predicate
H(z, w) â†”z > 0 âˆ§(âˆ€i)â‰¤w(i + 1) | z
This is in CA since (i + 1) | z â†”(âˆƒu)â‰¤zz = u(i + 1). Now
v = p(a) â†”H(v, K(a)) âˆ§(âˆ€z)<vÂ¬H(z, K(a))
and the case for v = p(a) rests by I.9.14. This concludes the proof that P(a, i, y)
is in CA. Finally, setting
Q(a, i, y) â†”y = a âˆ¨P(a, i, y)

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 163
we have Q(a, i, y) in CA. We are done by observing that
Î²(a, i) = y â†”Q(a, i, y) âˆ§(âˆ€z)<yÂ¬Q(a, i, y)
â–¡
The crucial step in order to achieve the earlier suggested projection represent-
ation of arbitrary primitive recursion relations (p. 160) is to express the arbitrary
primitive recursive â€œcomputationâ€
(âˆƒc0, . . . , cx)
%
z = cx âˆ§c0 = h(âƒ—y) âˆ§(âˆ€i)<xci+1 = g(i, âƒ—y, ci)
&
(1)
in the form (âˆƒu)R, where R âˆˆCA. (1), of course, â€œcomputesâ€ (or veriï¬es) that
z = f (x, âƒ—y), where, for all x, âƒ—y,
f (0, âƒ—y) = h(âƒ—y)
f (x + 1, âƒ—y) = g(x, âƒ—y, f (x, âƒ—y))
I.9.16 Theorem. If R(âƒ—x) âˆˆPRâˆ—, then, for some Q(y, âƒ—x) âˆˆCA,
R(âƒ—x) â†”(âˆƒy)Q(y, âƒ—x)
Proof. Since R(âƒ—x) âˆˆPRâˆ—iff for some f âˆˆPR one has R(âƒ—x) â†”f (âƒ—x) = 0,
it sufï¬ces to show that for all f âˆˆPR there is a Q(z, y, âƒ—x) âˆˆCA such that
y = f (âƒ—x) â†”(âˆƒz)Q(z, y, âƒ—x).
We do induction on PR. The graphs of the initial functions are in CA without
the help of any (âˆƒy)-projection.
Let f (âƒ—x) = g(h1(âƒ—x), . . . , hm(âƒ—x)), and assume that (I.H.) z = g(âƒ—ym)
â†”
(âˆƒu)G(u, z, âƒ—ym) and y = hi(âƒ—x) â†”(âˆƒz)Hi(z, y, âƒ—x) for i = 1, . . . , m, where G
and the Hi are in CA.
Let us write â€œ(âˆƒâƒ—zr)â€ and â€œ(âˆƒâƒ—zr)â‰¤wâ€ as short for â€œ(âˆƒz1)(âˆƒz2) . . . (âˆƒzr)â€ and
â€œ(âˆƒz1)â‰¤w(âˆƒz2)â‰¤w . . . (âˆƒzr)â‰¤wâ€ respectively. Then
y = f (âƒ—x) â†”(âˆƒâƒ—um)
%
y = g(âƒ—um) âˆ§u1 = h1(âƒ—x) âˆ§Â· Â· Â· âˆ§um = hm(âƒ—x)
&
â†”(âˆƒâƒ—um)
%
(âˆƒz)G(z, y, âƒ—um) âˆ§(âˆƒz1)H1(z1, u1, âƒ—x) âˆ§
Â· Â· Â· âˆ§(âˆƒzm)Hm(zm, um, âƒ—x)
&
â†”(âˆƒw)(âˆƒâƒ—um)â‰¤w
%
(âˆƒz)â‰¤wG(z, y, âƒ—um) âˆ§(âˆƒz1)â‰¤wH1(z1, u1, âƒ—x) âˆ§
Â· Â· Â· âˆ§(âˆƒzm)â‰¤wHm(zm, um, âƒ—x)
&
We ï¬nally turn to the graph z = f (x, âƒ—y), where f is deï¬ned by primitive recur-
sion from h and g above. This graph is veriï¬ed by the computation (1).

164
I. Basic Logic
Assume that (this is the I.H.) z = h(âƒ—y) â†”(âˆƒw)H(w, z, âƒ—y) and u =
g(x, âƒ—y, z) â†”(âˆƒw)G(w, u, x, âƒ—y, z), where H and G are in CA. Then
z = f (x, âƒ—y) â†”(âˆƒc)
%
Î²(c, 0) = h(âƒ—y) âˆ§Î²(c, x) = z âˆ§
(âˆ€i)<xÎ²(c, i + 1) = g(i, âƒ—y, Î²(c, i))
&
â†”
(using the I.H.)
(âˆƒc)
%
(âˆƒw)H(w, Î²(c, 0), âƒ—y) âˆ§Î²(c, x) = z âˆ§
(âˆ€i)<x(âˆƒw)G(w, Î²(c, i + 1), i, âƒ—y, Î²(c, i))
&
â†”
(see the proof of I.8.48)
(âˆƒu)(âˆƒc)<u
%
(âˆƒw)<u H(w, Î²(c, 0), âƒ—y) âˆ§Î²(c, x) = z âˆ§
(âˆ€i)<x(âˆƒw)<uG(w, Î²(c, i + 1), i, âƒ—y, Î²(c, i))
&
Of course things like G(w, Î²(c, i + 1), i, âƒ—y, Î²(c, i)) can be easily seen to be in
CA, since y = Î²(c, i) is by I.9.15, and Î²(c, i) â‰¤c < u.
â–¡
I.9.17 Corollary.  is the closure of z = x+y and z = xy under âˆ¨, Â¬, (âˆƒy)<z,
(âˆƒy), and explicit transformations.
Indeed, since (âˆƒy) subsumes (âˆƒy)<z,
I.9.18 Corollary.  is the closure of z = x + y and z = xy under âˆ¨, Â¬, (âˆƒy),
and explicit transformations.
The name â€œarithmeticâ€ relations is now completely justiï¬ed.
The following corollary is sufï¬ciently important (even useful) to merit theo-
rem status.
I.9.19 Theorem (A Constructive Arithmetic Kleene Predicate). There is,
for each n > 0, a constructive arithmetic predicate T (n)
C A(i, âƒ—xn, z) such that
{i}(âƒ—xn) â†“â†”(âˆƒz)T (n)
C A(i, âƒ—xn, z)
Moreover, there is a primitive recursive function U of one argument, such that
{i}(âƒ—xn) = U
%
(Âµz)T (n)
C A(i, âƒ—xn, z)
&
Proof. By I.9.16, let, for every n > 0, C(n) âˆˆCA be such that
T (n)(i, âƒ—xn, z) â†”(âˆƒu)C(n)(u, i, âƒ—xn, z)
Set T (n)
C A(i, âƒ—xn, z) â†”(âˆƒu)â‰¤z(âˆƒw)â‰¤z(z = J(u, w) âˆ§C(n)(u, i, âƒ—xn, w)), in other
words, T (n)
C A(i, âƒ—xn, z) â†”C(n)(K(z), i, âƒ—xn, L(z)).
For U, set, for all z, U(z) = (L(z))0.
â–¡

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 165
After our brief digression to obtain I.9.16 and its corollaries, we now return
to codings and GÂ¨odel numberings.
We have seen that any (ï¬xed length) sequence a0, . . . , an can be coded by a
primitive recursive function of the ai, namely, q of p. 158. Indeed, we can code
variable length sequences b0, . . . , bnâˆ’1 by appending the length information,
n, at the beginning of the sequence.
I.9.20 Deï¬nition. (Following Shoenï¬eld (1967).â€ ) We now revise the symbol
âŸ¨a0, . . . , anâˆ’1âŸ©to mean, for the balance of this chapter, the smallest a such that
Î²(a, 0) = n
and
Î²(a, i + 1) = ai + 1
for i < n
â–¡
We re-introduce also Seq(z), (z)i, âˆ—, and lh, but to mean something other than
their homonyms on p. 136 (this time basing the deï¬nitions on the Î²-coding).
We let lh(z) = Î²(z, 0) and (z)i = Î²(z, i + 1)
.âˆ’1. Thus Î²(z, i + 1) > 0
implies Î²(z, i + 1) = (z)i + 1.
Seq(z) will say that z is some â€œbeta-codedâ€ sequence. We have analogous
expectations on the â€œstructureâ€ of the number z (as in the case of the â€œSeqâ€ of
p. 136) as dictated by I.9.20. Thus a z is a â€œSeqâ€ iff
(1) the sequence terms have been incremented by 1 before â€œpackingâ€ them
into z, and
(2) any smaller x (x < z, that is) codes a different sequence.â€¡
That is,
Seq(z) â†”(âˆ€i)<lh(z)Î²(z, i + 1) > 0 âˆ§(âˆ€x)<z
(lh(x) Ì¸= lh(z) âˆ¨(âˆƒi)<lh(z)(z)i Ì¸= (x)i)
We also (re)deï¬ne (see also II.2.16 and II.2.21)
a âˆ—b = (Âµz)(Seq(z) âˆ§lh(z) = lh(a) + lh(b) âˆ§
(âˆ€i)<lh(a)(i > 0 â†’(z)i = (a)i) âˆ§
(âˆ€i)<lh(b)(i > 0 â†’(z)i+lh(a) = (b)i)
It is clear that âŸ¨a0, . . . , anâˆ’1âŸ©âˆ—âŸ¨b0, . . . , bmâˆ’1âŸ©= âŸ¨a0, . . . , anâˆ’1, b0, . . . , bmâˆ’1âŸ©.
Note also that Seq(z) implies (z)i < z for i <lh(z).
â€  See however II.2.14, p. 242.
â€¡ z is the smallest number that codes whatever it codes.

166
I. Basic Logic
As was already suggested on p. 155, if âŒœâŒ: V â†’N is any reasonable 1-1
total mapping from a ï¬nite symbol alphabet V to the natural numbers then we
can use âŸ¨. . . âŸ©to extend it to a GÂ¨odel numbering âŒœâŒ: V + â†’N.â€ 
Rather than attempting to implement that suggestion in the most general
setting, we will shortly show exactly what we have in mind in the case of
languages appropriate for formal arithmetic.
We now ï¬x an alphabet over which we can deï¬ne a hierarchy of languages
where formal arithmetic can be spoken. For the balance of the chapter, the
absolutely essential nonlogical symbolsâ€¡ will be indicated in boldface type:
0, S, +, Ã—, <
(NL)
The entire alphabet, in the following ï¬xed order, is
0,
, âƒ, â–³, =, Â¬, âˆ¨, âˆƒ, (, ), #
(1)
The commas are just metalogical separators in (1) above. All the rest are formal
symbols. The symbol â€œ#â€ is â€œglueâ€, and its purpose is to facilitate building an
unlimited (enumerable) supply of predicate and function symbols, as we will
shortly describe.
The brackets have their normal use, but will be also assisting the symbols
â€œ
â€, â€œâ–³â€, and â€œâƒâ€
to build the object variables, function symbols, and
predicate symbols of any language in the hierarchy.
The details are as follows:
Variables.
The argot symbol â€œv0â€ is short for â€œ(
)â€, â€œv1â€ is short for
â€œ(
)â€, and, in general,
â€œvjâ€ is short for â€œ(
j+1
  
. . .
)â€
Function symbols.
The argot symbol â€œ f n
j â€ â€“ the jth function symbol
( j â‰¥0) with arity n > 0 â€“ is a short name for the string
â€œ(
j+1



â–³. . . â–³#
n



â–³. . . â–³)â€
â€  V + denotes the set of all nonempty strings of V .
â€¡ That is, all languages where arithmetic is spoken and practised formally will include these
symbols.

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 167
Predicate symbols.
The argot symbol â€œPn
jâ€ â€“ the jth predicate symbol
( j â‰¥0) with arity n > 0 â€“ is a short name for the string
â€œ(
j+1



âƒ. . . âƒ#
n



âƒ. . . âƒ)â€
Some symbols in the list (NL) are abbreviations (argot) that will be used
throughout, in conformity with standard mathematical practice:
S
names
(â–³#â–³),
that is, f 1
0
+
names
(â–³#â–³â–³),
that is, f 2
0
Ã—
names
(â–³â–³#â–³â–³),
that is, f 2
1
<
names
(âƒ# âƒâƒ),
that is, P2
0
I.9.21 Deï¬nition. A language LA of arithmetic is a ï¬rst order language over
the alphabet (1), with the stated understanding of the ontology of viâ€™s, f n
i â€™s,
and Pn
i â€™s.
LA must contain the nonlogical symbols (NL).
We will normally write (argot) t < s, t + s and t Ã— s (t, s are terms) rather
than <ts, + ts, and Ã—ts respectively.
â–¡
The subscript A (for â€œArithmeticâ€) in LA reï¬‚ects the particular standard
structure (appropriate for the language) â€œAâ€ that we have in mind. In what
follows we will study extensively the language for the standard structureâ€  N =
(N; 0; S, +, Ã—; <).
Our notation for the language under discussion will normally indicate the
structure we have in mind, e.g., by writing LN.
LN has no nonlogical symbols beyond the ones listed under (NL).
Still, occasionally we will just write LA regardless, and let the context reveal
what we are thinking.
The â€œnaturalâ€ structures for extensions of LN will be expansions of N (see
Deï¬nition I.5.3, p. 54). Such expansions will normally be denoted by Nâ€² =
(N; 0; S, +, Ã—, . . . ; <, . . .), or just Nâ€². We will never need to specify the â€œ. . . â€
part in Nâ€².
The extended language, correspondingly, may be denoted by LNâ€², if the
correspondence needs to be emphasized, else we will fall back on the generic
LA.
â€  Also sometimes called the â€œstandard modelâ€, although this latter term implies the presence of
some nonlogical axioms for N to be a model of.

168
I. Basic Logic
Note that all languages LA will have just one constant symbol, 0.
Each bold nonlogical symbol of the language is interpreted as its lightface
â€œrealâ€ counterpart (as usual, S = Î»x.x + 1), that is, 0N = 0, <N =<, SN = S,
+N = +, etc.
Of course, in any expansion Nâ€², all we do is give meaning to new nonlogical
symbols leaving everything else alone. In particular, in any expansion it is still
the case that 0Nâ€² = 0, <Nâ€² =<, SNâ€² = S, +Nâ€² = +, etc.
GÂ¨odel numbering of LA: At this point we ï¬x our GÂ¨odel numbering for all
possible languages LA. Referring to (1) of p. 166, we assign
âŒœ0âŒ= âŸ¨0, 1âŸ©
âŒœ
âŒ= âŸ¨0, 2âŸ©
âŒœâ–³âŒ= âŸ¨0, 3âŸ©
âŒœâƒâŒ= âŸ¨0, 4âŸ©
âŒœ=âŒ= âŸ¨0, 5âŸ©
âŒœÂ¬âŒ= âŸ¨0, 6âŸ©
âŒœâˆ¨âŒ= âŸ¨0, 7âŸ©
âŒœâˆƒâŒ= âŸ¨0, 8âŸ©
âŒœ(âŒ
= âŸ¨0, 9âŸ©
âŒœ)âŒ
= âŸ¨0, 10âŸ©
âŒœ#âŒ= âŸ¨0, 11âŸ©
We then set, for all j, n in N,
âŒœ(
j+1
  
. . .
)âŒ= âŸ¨1, âŒœ(âŒ,
j+1



âŒœ
âŒ, . . . , âŒœ
âŒ, âŒœ)âŒâŸ©
âŒœ(
j+1
  
â–³. . . â–³#
n+1
  
â–³. . . â–³)âŒ= âŸ¨2, âŒœ(âŒ,
j+1



âŒœâ–³âŒ, . . . , âŒœâ–³âŒ, âŒœ#âŒ,
n+1



âŒœâ–³âŒ, . . . , âŒœâ–³âŒ, âŒœ)âŒâŸ©
and
âŒœ(
j+1



âƒ. . . âƒ#
n+1



âƒ. . . âƒ)âŒ
= âŸ¨3, âŒœ(âŒ,
j+1



âŒœâƒâŒ, . . . , âŒœâƒâŒ, âŒœ#âŒ,
n+1



âŒœâƒâŒ, . . . , âŒœâƒâŒ, âŒœ)âŒâŸ©
and by recursion on terms and formulas
1. If g and Q are function and predicate symbols respectively of arity n, and
t1, . . . , tn are terms, then
âŒœgt1. . . tnâŒ= âŸ¨âŒœgâŒ, âŒœt1âŒ, . . . , âŒœtnâŒâŸ©

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 169
and
âŒœQt1 . . . tnâŒ= âŸ¨âŒœQâŒ, âŒœt1âŒ, . . . , âŒœtnâŒâŸ©
Moreover, for terms t and s, âŒœt = sâŒ= âŸ¨âŒœ= âŒ, âŒœtâŒ, âŒœsâŒâŸ©.
2. If A and B are formulas, then
âŒœ(Â¬A)âŒ= âŸ¨âŒœÂ¬âŒ, âŒœAâŒâŸ©
âŒœ((âˆƒx)A)âŒ= âŸ¨âŒœâˆƒâŒ, âŒœxâŒ, âŒœAâŒâŸ©
and
âŒœ(A âˆ¨B )âŒ= âŸ¨âŒœâˆ¨âŒ, âŒœAâŒ, âŒœB âŒâŸ©
Pause. What happened to the brackets in case 2 above?
It is clear that, intuitively, this numbering is algorithmic both ways. Given a
basic symbol ((1) of p. 166), term or formula, we can apply the above rules to
come up with its GÂ¨odel number.
Conversely, given a number n we can test ï¬rst of all if it satisï¬es Seq (for all
numbers â€œâŒœâŒâ€ necessarily satisfy Seq). If so, and if it also satisï¬es lh(n) = 2,
(n)0 = 0 and 1 â‰¤(n)1 â‰¤11, then n is the GÂ¨odel number of a basic symbol (1)
of the alphabet, and we can tell which one it is.
If now (n)0 = 1, we further check to see if we got a code for a variable, and,
if so, of which â€œviâ€. This will entail ascertaining that lh(n) â‰¥4, that (n)1 â€œisâ€ â€ 
a â€œ(â€, (n)lh(n)âˆ’1 is a â€œ)â€, and all the other (n)i are
â€™s. If all this testing passes,
then we have a code for vi with i =lh(n) âˆ’4.
We can similarly check if n codes a function or predicate symbol, and if so,
which one.
As one more example,â€¡ let Seq(n), lh(n) = 3 and (n)0 = âŒœâˆƒâŒ. Then we
need to ascertain that (n)1 is some vi and â€“ beneï¬ting from an I.H. that we can
decode numbers <n â€“ that (n)2 is some formula A.
Pause. This I.H. hinges on the crucial â€œif Seq(n), then (n)i < nâ€.
We have thus decoded n into ((âˆƒvi)A).
We note the following table, which shows that codes of distinct constructs
do not clash.
â€  More accurately, â€œcodesâ€.
â€¡ We will formalize this discussion in the next chapter, so there is little point to pursue it in detail
here.

170
I. Basic Logic
Code (n) attribute
Potential expression
(n)0 = 0
Basic symbol
(n)0 = 1
Variable
(n)0 = 2
Function symbol
(n)0 = 3
Predicate symbol
(n)0 > 3 âˆ§((n)0)0 = 2
Term
(n)0 > 3 âˆ§((n)0)0 = 3
Atomic formula
(n)0 > 3 âˆ§((n)0)0 = 0
((n)0)1 = 5
Atomic formula (=)
((n)0)1 = 6
Negation
((n)0)1 = 7
Disjunction
((n)0)1 = 8
Existential quantiï¬cation
Note that (n)0 > 3 for those items claimed above stems from (z)i < z for
codes z, and the presence of, say, â€œ(â€ â€“ âŒœ(âŒ> 9 â€“ in the syntax of func-
tions and predicates, while the symbols =, Â¬, âˆ¨, âˆƒhave each a code greater
than 5.
Having arithmetized LA, we can now apply the tools of analysis of number
sets to sets of expressions (strings) over LA. Thus,
I.9.22 Deï¬nition. We call a set A of expressions over LA constructive arith-
metic, primitive recursive, recursive, semi-recursive, or deï¬nable over LA (in
some structure) iff {âŒœxâŒ: x âˆˆA} is constructive arithmetic, primitive recursive,
recursive, semi-recursive, or deï¬nable over LA, respectively.
â–¡
We have seen that one way to generate theories is to start with some set
of nonlogical axioms  and then build Thm. Another way is to start with a
structure M and build T (M) = {A : |=M A}.â€ 
We will be interested here, among other theories, in the complete arithmetic,
that is, T (N). One of our results will be Tarskiâ€™s undeï¬nability of arithmetical
truth, that is, intuitively, that the set T (N) is not deï¬nable in the structure N.
Recall that (refer to Deï¬nition I.5.15, p. 60) R âŠ†Mn is deï¬nable over L in
M = (M, I ) iff, for some formula R(x1, . . . , xn) of L,
R(i1, . . . , in)
iff
|=M R(i1, . . . , in)
for all i j âˆˆM
where i j are imported constants (see I.5.4).
â€  Cf. I.6.1.

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 171
We say that R(x1, . . . , xn) is a regular formula for R iff the variables
x1, . . . , xn are the formal variables v0, . . . , vnâˆ’1. Applying â€œdummy renamingâ€
(I.4.13) judiciously, followed by the substitution metatheorem (I.4.12), we can
convert every formula to a logically equivalent regular formula. For example,
v13 = v99 is not regular, but it says the same thing as the regular formula
v0 = v1, i.e, v13 = v99 |= v0 = v1 and v0 = v1 |= v13 = v99.
Henceforth we will tacitly assume that formulas representing relations are
always chosen to be regular.
Turning our attention to Nâ€², we have an interesting variant to the notion of
deï¬nability over (any) LNâ€², due to the richness of LNâ€² in certain types of terms.
I.9.23 Deï¬nition (Numerals). A term such as â€œ
n
  
S . . . S 0â€ for n â‰¥1 is called
a numeral. We use the shorthand notationn for this term. We also let (the case
of n = 0)0 be an alias for the formal 0.
â–¡
n must not be confused with the imported constants n of LA(N) (or LA(Nâ€²)).
The former are (argot names of) terms over LA; the latter are (argot names of)
new constants, not present in the original LA (which only has the constant 0).
The usefulness of numerals stems from the following trivial lemma.
I.9.24 Lemma. For all n âˆˆN, n N = n.
Proof. Induction on n. For n = 0,0 N = 0N = 0.
Now,

n + 1 N =

S
n
  
S . . . S 0
N
= SNnN
= S(n)
since SN = S, using the I.H.
= n + 1
â–¡
I.9.25 Corollary. For all n âˆˆN, |=N n = n.
Thus, for any formula A over LA and any n, |=A A[n] â†”A[n], since
|= n = n â†’(A[n] â†”A[n]).
Therefore, a relation R âŠ†Nn is deï¬nable over LA in an appropriate expan-
sion of N, say Nâ€², iff there is a regular formula R in LA such that, for all mj
in N,
R(m1, . . . , mn)
iff
|=A R(
m1, . . . , 
mn)
(D)

172
I. Basic Logic
The above expresses deï¬nability over LA without using any â€œsyntactic materi-
alsâ€ (such as imported constants n) that are outside the language LA.
Lemmata I.9.24â€“I.9.25 and (D) above go through, of course, for any expan-
sion Nâ€² of N.
The following lemma will be used shortly:
I.9.26 Lemma. The function Î»n.âŒœnâŒis primitive recursive.
Proof.
âŒœ0âŒ
= âŸ¨0, 1âŸ©
âŒœ
n + 1âŒ= âŸ¨âŒœSâŒ, âŒœnâŒâŸ©
where, of course, âŒœSâŒ= âŸ¨2, âŒœ(âŒ, âŒœâ–³âŒ, âŒœ#âŒ, âŒœâ–³âŒ, âŒœ)âŒâŸ©.
â–¡
Next, through the concept of deï¬nability over the minimum language LN,
we assign (ï¬nite) string representations to each arithmetic formula. Through
this device, and GÂ¨odel numbering of the string representations, we assign (in-
directly) GÂ¨odel numbers to those formulas. Thus, we can speak of recursive,
semi-recursive, arithmetic, etc., subsets of (arithmetic) relations.
I.9.27 Deï¬nition. The set âˆ†0(LA) of formulas over LA is the smallest set of
formulas over the language that includes the atomic formulas and satisï¬es the
closure conditions: If A and B are in âˆ†0(LA), then so are Â¬A, A âˆ¨B , and
(âˆƒx)<tA, where â€œ(âˆƒx)<tAâ€ is short for (âˆƒx)(x < t âˆ§A).
In (âˆƒx)<tA we require that the variable x does not occur in the term t. In
the case that LA = LN we often simply write âˆ†0 rather than âˆ†0(LN).
â–¡
We do hope (by virtue of choosing boldface type for âˆ†0) that we will not
confuse this set of formulas over LN with the 0 relations of the arithmetic
hierarchy.
I.9.28 Lemma. Every relation in CA is deï¬nable in N over LN (in the sense
of (D) above) by some formula in âˆ†0.
Proof. We do induction on CA. The basis contains two cases, z = x + y and
z = xy.

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 173
v0 = v1 + v2 deï¬nes z = x + y, since for any a, b, c in N,
(a = b +c)N = t â†”(a = b + c)N = t
â†”aN = b
N + N cN
â†”a = b + c
An analogous case can be made for z = xy.
We leave it to the reader to verify that if R and Q are deï¬ned by R and Q
respectively, then Â¬R and R âˆ¨Q are deï¬ned by Â¬R and R âˆ¨Q respectively.
Finally, we show that (âˆƒx)<y R(âƒ—zr, x) is deï¬ned by (âˆƒx)<yR, that is,
(âˆƒvr+1)(vr+1 < v0 âˆ§R(v1, . . . , vr, vr+1))
Fix b0, . . . , br in N. Trivially, x < y deï¬nes x < y; thus (using the I.H. for R)
for any ï¬xed a in N,
a < b0 âˆ§R(âƒ—br, a)
iff
|=N a < 
b0 âˆ§R(
b1, . . . , 
br,a)
(i)
Let (âˆƒx)<b0 R(âƒ—br, x). Then the left side of â€œiffâ€ in (i) holds for some a; hence
so does the right side. Therefore,
|=N a < 
b0 âˆ§R(
b1, . . . , 
br, a)
(ii)
Hence
|=N (âˆƒvr+1)(vr+1 < 
b0 âˆ§R(
b1, . . . , 
br, vr+1))
(iii)
Conversely, if (iii) holds, then so does (ii) for some a, and hence also the right
hand side of â€œiffâ€ in (i). This yields the left hand side; thus (âˆƒx)<b0 R(âƒ—br, x). â–¡
I.9.29 Lemma. Every relation that is deï¬nable in N by some formula in âˆ†0
over LN is in PRâˆ—.
Proof. Induction on âˆ†0.
Basis.
An atomic formula is t = s or t < s. It is a trivial matter to verify
that a relation deï¬ned by such formulas is obtained by Î»xy.x = y and Î»xy.x < y
by a ï¬nite number of explicit transformations and substitutions of the functions
Î»xy.x + y and Î»xy.xy for variables. We know that this does not lead beyond
PRâˆ—.
Induction step.
Relations deï¬ned by combining their deï¬ning formulas
from âˆ†0, using (the formal) Â¬, âˆ¨,(âˆƒx)<y also stay in PRâˆ—by the closure of
the latter set under (the informal) Â¬, âˆ¨, (âˆƒx)<y.
â–¡

174
I. Basic Logic
The above lemma can be sharpened to replace PRâˆ—by CA. This follows
from results of Bennett (1962) (retold in Tourlakis (1984)), that CA predicates
are closed under replacement of variables by so-called rudimentary functions
(Smullyan (1961), Bennett (1962), Tourlakis (1984)).
I.9.30 Theorem. A relation is arithmetic iff it is deï¬nable in N over LN.
Proof. If part.
We do induction on the complexity of the deï¬ning formula.
Atomic formulas deï¬ne relations in PRâˆ—, as was seen in the basis step above
(indeed, relations in CA by the previous remarks), and hence in 0 (Deï¬ni-
tion I.8.60). Since arithmetic relations are closed under Â¬, âˆ¨, (âˆƒx), the property
(that relations which are ï¬rst order deï¬nable in N are arithmetic) propagates
with (the formal) Â¬, âˆ¨,(âˆƒx).
Only-if part.
This follows because z = x + y and z = xy are deï¬nable
(I.9.28), and deï¬nability is preserved by Boolean operations and (âˆƒx).
â–¡
We now turn to Tarskiâ€™s theorem.
I.9.31 Theorem (Tarski). The set T = {âŒœAâŒ: A âˆˆT (N)} is not arithmetic;
therefore it is not deï¬nable in N (over LN).
Proof. Suppose that T is arithmetic, say,
(Î»x.x âˆˆT) âˆˆm âˆªm
Pick R(x) âˆˆm+1 âˆ’m âˆªm (cf. I.8.72), and let the regular formula R(v0) â€“
or, simply, R â€“ deï¬ne R over LN.
Clearly, the formula
(âˆƒv0)

v0 = v1 âˆ§R(v0)

also deï¬nes R(x), since
|= R(v1) â†”(âˆƒv0)

v0 = v1 âˆ§R(v0)

Thus,
R(n) â†”|=N (âˆƒv0)

v0 =n âˆ§R

â†”
)
âŒœâˆƒâŒ, âŒœv0âŒ,

âŒœâˆ§âŒ, âŸ¨âŒœ= âŒ, âŒœv0âŒ, âŒœnâŒâŸ©, âŒœRâŒ
*
âˆˆT

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 175
By I.9.26, g = Î»n.âŸ¨âŒœâˆƒâŒ, âŒœv0âŒ, âŸ¨âŒœâˆ§âŒ, âŸ¨âŒœ= âŒ, âŒœv0âŒ, âŒœnâŒâŸ©, âŒœRâŒâŸ©âŸ©is in R.â€ 
Thus, R(x) iff g(x) âˆˆT; hence R(x) âˆˆm âˆªm, which is absurd.
â–¡
The trick of considering (âˆƒv0)(v0 = v1 âˆ§R(v0)) is due to Tarski. It simpliï¬es
the task of computing the GÂ¨odel number of (a formula equivalent to) R(n)
from n and the GÂ¨odel number of R(v0).
GÂ¨odelâ€™s original approach to computing such numbers was more complica-
ted, utilizing a substitution function â€“ subâ€¡ (see next chapter) â€“ that was analo-
gous to Kleeneâ€™s later invention, the â€œS-m-nâ€ functions of recursion theory.
We have cheated a bit in the above proof, in pretending that â€œâˆ§â€ was a basic
symbol of the alphabet. The reader can easily ï¬x this by invoking De Morganâ€™s
laws.
We have just seen that the set of all formulas of arithmetic that are true in
the standard structure N is really â€œhardâ€.Â§ Certainly it is recursively unsolvable
(not even semi-recursive, not even arithmetic, . . . ).
Whatcanwesayaboutprovableformulasofarithmetic?Butthen,â€œprovableâ€
under what (nonlogical) axioms?
I.9.32 Deï¬nition (Formal Arithmetic(s)). We will use the ambiguous phrase
â€œa Formal Arithmeticâ€ for any ï¬rst order theory over a language LA for arith-
metic (Deï¬nition I.9.21), that contains at least the following nonlogical axioms
(due to R. M. Robinson (1950)).
The speciï¬c Formal Arithmetic over LN that has precisely the following
nonlogical axioms we will call ROB. The name â€œROBâ€ will also apply to the
list of the axioms below:
ROB-1. (Regarding S)
S1.
Â¬Sx = 0 (for any variable x)
S2.
Sx = Sy â†’x = y (for any variables x, y)
ROB-2. (Regarding +)
+1.
x + 0 = x (for any variable x)
+2.
x + Sy = S(x + y) (for any variables x, y)Â¶
â€  It is trivial to see that the Î»âƒ—x.âŸ¨âƒ—xâŸ©that was introduced on p.165 is in R. With slightly more effort
(Exercise I.106) one can see that it is even in PR, and therefore so is g.
â€¡ The function sub is primitive recursive, of two arguments, such thatâŒœR(n)âŒ= sub(g, n), where
g = âŒœR(v0)âŒ.
Â§ These are the formulas we often call â€œreally trueâ€.
Â¶ If we had stayed with the formal notation of p. 167, we would not have needed brackets here.
We would have simply written +xSy = S + xy.

176
I. Basic Logic
ROB-3. (Regarding Ã—)
Ã—1.
x Ã— 0 = 0 (for any variable x)
Ã—2.
x Ã— Sy = (x Ã— y) + x (for any variables x, y)
ROB-4. (Regarding <)
<1.
Â¬x < 0 (for any variable x)
<2.
x < Sy â†”x < y âˆ¨x = y (for any variables x, y)
<3.
x < y âˆ¨x = y âˆ¨y < z (for any variables x, y).
Let us abstract (i.e., generalize) the situation, for a while, and assume that
we have the following:
(1) An arbitrary ï¬rst order language L over some ï¬nite alphabet V (not nec-
essarily the one for formal arithmetic).
(2) A GÂ¨odel numbering âŒœâŒof V + that we extend to terms and formulas, exactly
as we described starting on p. 168, where we extended the speciï¬c GÂ¨odel
numbering on the alphabet of arithmetic (p. 166). We are continuing to base
our coding on the âŸ¨âŸ©of p. 165.
(3) A theory  over L with a recursive set of axioms, , that is, one for which
the set A = {âŒœF âŒ: F âˆˆ} is recursive.
(4) The theory  has just two rules of inference,
I1 : A
B
I2 : A, B
C
(5) There are recursive relations I1(x, y), I2(x, y, z), corresponding to I1 and
I2, that mean
I1(âŒœX âŒ, âŒœY âŒ)
iff
X âŠ¢Y via I1
and
I2(âŒœX âŒ, âŒœY âŒ, âŒœZâŒ)
iff
X , Y âŠ¢Z via I2
That is, intuitively, we require the rules of inference to be â€œcomputableâ€ (or
algorithmic).
We call such a theory recursively axiomatized.
A proof over  is a sequence of formulas A1, A2, . . . , Ar, and it is assigned
the GÂ¨odel number âŸ¨âŒœA1âŒ, âŒœA2âŒ, . . . , âŒœArâŒâŸ©.
The following is at the root of GÂ¨odelâ€™s incompletableness result.

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 177
I.9.33 Theorem. If  is a recursively axiomatized theory over L, then Thm
is semi-recursive.
Of course, this means that the set of GÂ¨odel numbers of theorems,  = {âŒœAâŒ:
A âˆˆThmÎ“}, is semi-recursive.
Proof. We set A = {âŒœAâŒ: A âˆˆ} and B = {âŒœAâŒ: A âˆˆ}, where  is
our â€œstandardâ€ set of logical axioms. We defer to the reader the proof that B is
recursive.
Actually, we suggest the following easier approach. Show that the set of GÂ¨odel
numbers of an equivalent set of logical axioms, 2 (Exercises I.26â€“I.41) is
recursive (indeed, primitive recursive).
Let Proof(u) mean that u is the GÂ¨odel number of a -proof. Then
Proof (u) â†”Seq(u) âˆ§(âˆ€i)<lh(u)
%
(u)i âˆˆA âˆ¨(u)i âˆˆB âˆ¨
(âˆƒj)<i I1((u) j, (u)i) âˆ¨
(âˆƒj)<i(âˆƒk)<i I2((u) j, (u)k, (u)i)
&
Thus, Proof (u) is in Râˆ—.
Finally, if  = {âŒœAâŒ:  âŠ¢A} (the set of GÂ¨odel numbers of -theorems),
then  is semi-recursive, since
x âˆˆ â†”(âˆƒu)

Proof (u) âˆ§(âˆƒi)<lh(u)x = (u)i

â–¡
I.9.34 Corollary. The set of theorems of any recursively axiomatized formal
arithmetic is semi-recursive. In particular, the set of theorems of ROB is semi-
recursive.
I.9.35 Deï¬nition. Smullyan (1992). Let us call a formal arithmetic  over LN
correct iff  âŠ†T (N).
â–¡
Many people call such an arithmetic â€œsoundâ€. We opted for the nomenclature
â€œcorrectâ€, proposed in Smullyan (1992), because we have deï¬ned â€œsoundâ€ in
such a manner that all ï¬rst order theories (recursively axiomatizable or not) are
sound (that is due to the â€œeasy halfâ€ of GÂ¨odelâ€™s completeness theorem).
Correctness along with soundness implies Thm âŠ†T (N).
Thus, we can rephrase the above corollary as
I.9.36 Corollary (GÂ¨odelâ€™s First Incompleteness Theorem). [Semantic ver-
sion]. ROB is incompletable as long as we extend it correctly and recursively

178
I. Basic Logic
over the language LN. That is, every correct recursively axiomatized formal
arithmetic  over LN is simply incomplete: There are inï¬nitely many sentences
F over LN that are undecidable by .
Proof. Since  is correct, Thm âŠ†T (N). This is an inequality because Thm
is semi-recursive, while T (N) is not even arithmetic.
Every one of the inï¬nitely many sentences F âˆˆT (N) âˆ’Thm,
Pause. Why â€œinï¬nitely manyâ€?
is unprovable. By correctness, Â¬F is not provable either, since it is false.
â–¡
I.9.37 Remark. (1) ROB has N as a model. Thus it is correct and, of course,
consistent.
(2) There are some â€œreally trueâ€ sentences that we cannot prove in ROB (and,
by correctness of ROB, we cannot prove their negations either). For example,
(âˆ€x)Â¬x < x is not provable (see Exercise I.108).
Similarly, (âˆ€x)(âˆ€y)x + y = y + x is not provable (see Exercise I.109).
Thus, it may not have come as a surprise that this formal arithmetic is
incomplete. GÂ¨odelâ€™s genius came in showing that it is impossible to complete
ROB by throwing axioms at it (a recursive set thereof). It is incompletable.
(3)Itturnsoutthatthereisanâ€œalgorithmâ€togeneratesentencesF âˆˆT (N)âˆ’
Thm for any recursive  that has been â€œgivenâ€, say as a Wm (that is, Wm =
{âŒœF âŒ: F âˆˆ}).
First of all (cf. proof of I.9.33), we revise Proof (u) to Î»um. Proof (u, m),
given by
Proof (u, m) â†”Seq(u) âˆ§(âˆ€i)<lh(u)
%
(âˆƒz)T (1)(m, (u)i, z) âˆ¨(u)i âˆˆB âˆ¨
(âˆƒj)<i I1((u) j, (u)i) âˆ¨
(âˆƒj)<i(âˆƒk)<i I2((u) j, (u)k, (u)i)
&
We set m = {âŒœAâŒ: (m) âŠ¢A}, where the subscript (m) of  simply reminds
us how the axioms are â€œgivenâ€, namely: Wm = A = {âŒœF âŒ: F âˆˆ(m)}.
Then x âˆˆm â†”(âˆƒu)(Proof (u, m) âˆ§x = (u)lh(u)
.âˆ’1), a semi-recursive
relation of x and m, is equivalent to â€œ{a}(u, m) â†“â€ for some a, and therefore to
â€œ{S1
1(a, m)}(u) â†“â€. Setting f (m) = S1
1(a, m) for all m, we have
There is a primitive recursive function f such that m = W f (m)
(âˆ—)
Pause. A by-product worth verbalizing is that if the set of axioms is semi-
recursive, then so is the set of theorems, and from a semi-recursive index of the
former, a semi-recursive index of the latter is primitive recursively computable.

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 179
Let now H (v0) deï¬ne Î»x.Â¬(âˆƒz)T (1)(x, x, z). Clearly, it is also the case that
Â¬(âˆƒz)T (1)(x, x, z)
iff
|=N (âˆƒv0)

v0 =x âˆ§H (v0)

We next consider the set K = {âŒœ(âˆƒv0)(v0 =x âˆ§H (v0))âŒ: x âˆˆK}.
Set, for all x,
g(x) = âŒœ(âˆƒv0)

v0 =x âˆ§H (v0)

âŒ
Then g âˆˆPR, for the reason we saw in the proof of I.9.31, and
x âˆˆK
iff
g(x) âˆˆK
We also consider the set Q = {âŒœ(âˆƒv0)(v0 =x âˆ§H (v0))âŒ: x âˆˆN}. Since
x âˆˆN iff g(x) âˆˆQ, and g is strictly increasing, Q is recursive (Exercise I.95).
Actually, all we will need here is that Q is semi-recursive, and this it readily
is, since it is r.e. (Deï¬nition I.8.50) via g (strictly increasing or not). Trivially,
K = Q âˆ©T
(âˆ—âˆ—)
where, as in I.9.31, we have set T = {âŒœAâŒ: A âˆˆT (N)}.
We make a few observations:
(i) If Wi âŠ†K, then i âˆˆK âˆ’Wi (else, i âˆˆK âˆ©K). Sets such as K that come
equipped with an algorithm for producing counterexamples to a claim
K = Wi are called productive (Dekker 1995). That is, a set S is productive
with productive function h âˆˆR iff for all i, Wi âŠ†S implies h(i) âˆˆSâˆ’Wi.
In particular, h = Î»x.x works in the case of K.
(ii) We saw that K = gâˆ’1[K], where gâˆ’1[. . . ] denotes inverse image. We show
that K is productive as well. We just need to ï¬nd a productive function for
K. Let Wi âŠ†K. Then gâˆ’1[Wi] âŠ†gâˆ’1[K] = K. By the S-m-n theorem,
there is an h âˆˆPR such that gâˆ’1[Wi] = Wh(i). Indeed,
x âˆˆgâˆ’1[Wi] â†”g(x) âˆˆWi
â†”{i}(g(x)) â†“
â†”{e}(x, i) â†“
for some e (why?)
â†”{S1
1(e, i)}(x) â†“
Take h = Î»x.S1
1(e, x). Thus, h(i) âˆˆK âˆ’Wh(i) = gâˆ’1[K âˆ’Wi] (g is 1-1).
Therefore Î»x.g(h(x)) is the sought productive function.
(iii) We ï¬nally show that T is productive, using (âˆ—âˆ—) above. First, we ask the
reader (Exercise I.98) to show that there is a k âˆˆPR such that
Wk(i) = Wi âˆ©Q
(1)

180
I. Basic Logic
(recall that Q is semi-recursive). We can now ï¬nd a productive function
for T. Let Wi âŠ†T. Thus, Wk(i) âŠ†T âˆ©Q = K, from which
g(h(k(i))) âˆˆT âˆ©Q âˆ’Wi âˆ©Q = T âˆ©Q âˆ’Wi âŠ†T âˆ’Wi
We return now to what this was all about: Suppose we have started with a
correct recursively axiomatized extension of ROB,  (over LN), where Wi is
the set of GÂ¨odel numbers of the nonlogical axioms. By (âˆ—), the set of GÂ¨odel
numbers of the -theorems is W f (i). By correctness, W f (i) âŠ†T.
Then g(h(k( f (i)))) is the GÂ¨odel number of a true unprovable sentence. A
Ï†-index of Î»i.g(h(k( f (i)))) is an algorithm that produces this sentence for any
set of axioms (coded by) i.
â–¡
We saw in I.9.37(2) above that ROB cannot prove some startlingly simple
(and useful) formulas. On the other hand, it turns out that it has sufï¬cient power
to â€œdeï¬ne syntacticallyâ€ all recursive relations. We now turn to study this phe-
nomenon, which will lead to a syntactic version of GÂ¨odelâ€™s ï¬rst incompleteness
theorem.
I.9.38 Deï¬nition. Let  be a formal arithmetic over some language LA(âŠ‡LN).
We say that a relation R âŠ†Nr is formally deï¬nable in , or -deï¬nable, iff
there is a formula R(v0, . . . , vrâˆ’1) over LA such that, for all âƒ—ar in N,
if
R(âƒ—ar),
then
 âŠ¢R(
a1, . . . , 
ar)
and
if
Â¬R(âƒ—ar),
then
 âŠ¢Â¬R(
a1, . . . , 
ar)
Of course, the left â€œÂ¬â€ is informal (metamathematical); the right one is formal.
We say that R formally deï¬nes R, but often, just that it deï¬nes R. The
context will determine if we mean formally or in the semantic sense.
â–¡
The terminology that names the above concept varies quite a bit in the literature.
Instead of â€œformally deï¬nableâ€ some say just â€œdeï¬nableâ€ and let the context
ï¬x the meaning. Out of a fear that the context might not always successfully
do so, we added the obvious qualiï¬er â€œformallyâ€, since this type of deï¬nability
is about provability, while the other one (I.5.15) is about truth. Terms such
as â€œrepresentableâ€ (or â€œ-representableâ€) are also used elsewhere instead of
â€œformally deï¬nableâ€.
It is clear that if R is ROB-deï¬nable and  extends ROB (over the same or
over an extended language), then R is also -deï¬nable.

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 181
I.9.39 Lemma. x = y is ROB-deï¬nable.
Proof. We show that v0 = v1 deï¬nes x = y.
Let a = b (be true). Thus a and b are identical strings over LA. Therefore
âŠ¢a =b by substitution in the logical axiom x = x.
In the present sequence of lemmata regarding the power of ROB we abuse
notation and simply write â€œâŠ¢. . . â€ rather than â€œROB âŠ¢. . . â€ or â€œâŠ¢ROB . . . â€.
Let next a Ì¸= b. We use (metamathematical) induction on b to show âŠ¢
Â¬a =b.
Basis.
If b = 0, then a = c+1 for some c, under the assumption. We want
âŠ¢Â¬
c + 1 =0, i.e., âŠ¢Â¬Sc =0, which we have, by axiom S1 (I.9.32) and
substitution.
Induction step.
We now go to the case a Ì¸= b + 1. If a = 0, then we are back
to what we have already argued (using âŠ¢x = y â†’y = x). Let then a = c + 1.
Thus, c Ì¸= b, and hence (I.H.), âŠ¢Â¬c =b. By S2 (and |=Taut) âŠ¢Â¬Sc = Sb, i.e.,
âŠ¢Â¬
c + 1 = 
b + 1.
â–¡
I.9.40 Corollary. Let t be a term with the free variables v0, . . . ,vnâˆ’1 and f a
total function of n arguments such that, for all âƒ—an, b, if f (âƒ—an) = b, then
âŠ¢t(
a1, . . . , 
an) =b
Then the formula t(v0, . . . , vnâˆ’1) = vn deï¬nes the graph of f .
Proof. It sufï¬ces to show that if f (âƒ—an) Ì¸= b, then
âŠ¢Â¬t(
a1, . . . , 
an) =b
(1)
Well, the assumption means that for some c Ì¸= b, f (âƒ—an) = c ( f is total), and
hence âŠ¢t(
a1, . . . , 
an) =c.
If we add t(
a1, . . . , 
an) =b to our assumptions, then âŠ¢c =b (by âŠ¢x =
y â†’y = z â†’x = z and substitution), which contradicts âŠ¢Â¬c =b, yielded
by c Ì¸= b. Via proof by contradiction we have established (1).
â–¡
A function such as f above is term-deï¬ned by t (in ROB). Suppressing mention
of t, we just say â€œ f is term-deï¬nableâ€ (in ROB).
I.9.41 Lemma. x + y = z is ROB-deï¬nable.
Proof. The formula v0 + v1 = v2 ï¬lls the bill. Indeed, by Corollary I.9.40 we
only need to prove that
a + b = c
implies
âŠ¢a +b =c
(2)

182
I. Basic Logic
We do induction on b.
Basis.
Let b = 0. Then, a = c; thus
âŠ¢a =c
(3)
By axiom +1 (I.9.32) and substitution, âŠ¢a +0 =a. Transitivity of formal
equality and (3) yield âŠ¢a +0 =c.
Induction step.
Let a + (b + 1) = c. Then c = d + 1 for some d; hence
a + b = d. By I.H.,
âŠ¢a +b = d
Hence
âŠ¢S(a +b) = Sd
(4)
by the Leibniz axiom (substitution and modus ponens).
By axiom +2, we also have (via substitution)
âŠ¢a + Sb = S(a +b)
so that transitivity of equality and (4) result in
âŠ¢a + Sb = Sd
that is, âŠ¢a + 
b + 1 = 
d + 1.
â–¡
I.9.42 Lemma. x Ã— y = z is ROB-deï¬nable.
Proof. Exercise I.110.
â–¡
I.9.43 Lemma. x < y is ROB-deï¬nable.
Proof. By induction on b we prove simultaneously
a < b
implies
âŠ¢a <b
(i)
and
a Ì¸< b
implies
âŠ¢Â¬a <b
(ii)
Basis.
For b = 0, (i) is vacuously satisï¬ed, while (ii) follows from axiom <1
and substitution.
Induction step.
Let a < b + 1. Thus, a < b or a = b. One case yields (I.H.)
âŠ¢a <b, and the other âŠ¢a =b (I.9.39).

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 183
By tautological implication, âŠ¢a <b âˆ¨a =b in either case. Hence âŠ¢a <
Sb, by substitution, |=Taut, and axiom < 2. That is, âŠ¢a < 
b + 1.
Let next a Ì¸< b + 1. Thus, a Ì¸< b and a Ì¸= b. Thus we have both âŠ¢Â¬a <b
(by I.H.) and âŠ¢Â¬a =b (by I.9.39); hence (|=Taut)
âŠ¢Â¬(a <b âˆ¨a =b)
(iii)
Via the equivalence theorem (I.4.25), |=Taut, and axiom <2, (iii) yields âŠ¢
Â¬a < Sb, that is, âŠ¢Â¬a < 
b + 1.
â–¡
I.9.44 Lemma. For any formula A and number n, ROB can prove
A[0], . . . , A[ 
n âˆ’1] âŠ¢x <n â†’A
where n = 0 means that we have nothing nonlogical to the left of â€œ âŠ¢â€ (beyond
the axioms of ROB).
Proof. Induction on n.
Basis.
If n = 0, then we need âŠ¢x <0 â†’A, which is a tautological impli-
cation of axiom <1.
Induction step.
We want
A[0], . . . , A[n] âŠ¢x < Sn â†’A
By axiom <2 and the equivalence theorem (I.4.25) we need to just prove
A[0], . . . , A[n] âŠ¢(x <n âˆ¨x =n) â†’A
This follows at once by proof by cases (I.4.26) since A[0], . . . , A[ 
n âˆ’1] âŠ¢
x <n â†’A (by I.H.) and A[n] âŠ¢x =n â†’A by tautological implication from
the Leibniz axiom and modus ponens.
â–¡
By the deduction theorem we also get
ROB âŠ¢A[0] â†’Â· Â· Â· â†’A[ 
n âˆ’1] â†’x <n â†’A
I.9.45 Lemma. ROB-deï¬nable relations are closed under Boolean operations.
Proof. If R and Q are deï¬ned by R and Q respectively, then it is a trivial
exercise (Exercise I.111) to show that Â¬R and R âˆ¨Q are deï¬ned by Â¬R and
R âˆ¨Q respectively.
â–¡
I.9.46 Lemma. ROB-deï¬nable relations are closed under explicit transforma-
tions.

184
I. Basic Logic
Proof. (Mostly a task for the reader, Exercise I.112).
Substitution by a constant.
Let R(z, âƒ—xm) be deï¬ned by R(z,âƒ—xm) (where
we have used syntactic variables z,âƒ—xm for v0, . . . , vm).
Then, for any a âˆˆN, R(a, âƒ—xm) is deï¬ned by R(a,âƒ—xm). Indeed, for any
âƒ—bm, if R(a, âƒ—bm), then âŠ¢R(a, 
b1, . . . , 
bm), and if Â¬R(a, âƒ—bm), then âŠ¢Â¬R(a,

b1,. . . , 
bm).
Introduction of a new variable.
Let Q(âƒ—xn) be deï¬ned by Q (âƒ—xn) and let
S(z, âƒ—xn) â†”Q(âƒ—xn) for all z, âƒ—xn, where z is a new variable. That is, S(z, âƒ—xn) â†”
Q(âƒ—xn) âˆ§z = z; hence it is deï¬nable (by Q (âƒ—xn) âˆ§z = z, incidentally) due
to I.9.39, and I.9.45.
â–¡
I.9.47 Lemma. ROB-deï¬nable relations are closed under (âˆƒx)<y.
Proof. Let Q(x, âƒ—zn) be deï¬ned by Q (x,âƒ—zn). We show that (âˆƒx)<yQ (x,âƒ—zn) â€“
that is,
(âˆƒx)

x < y âˆ§Q (x,âƒ—zn)

â€“ deï¬nes (âˆƒx)<y Q(x, âƒ—zn).
Let (âˆƒx)<a Q(x, âƒ—bn). Then, for some c < a, we have Q(c, âƒ—bn). By I.9.43,
I.9.45, the assumption, and |=Taut,
âŠ¢c <a âˆ§Q (c, 
b1 . . . , 
bn)
Hence (by the substitution axiom and modus ponens)
âŠ¢(âˆƒx)

x <a âˆ§Q (x, 
b1 . . . , 
bn)

Next, let Â¬(âˆƒx)<a Q(x, âƒ—bn), that is,
Q(0, âƒ—bn), . . . , Q(a âˆ’1, âƒ—bn) are all false
By assumption on Q,
âŠ¢Â¬Q (i, 
b1, . . . , 
bn),
for i = 0, 1, . . . , a âˆ’1
Hence (I.9.44 followed by generalization)
âŠ¢(âˆ€x)

x <a â†’Â¬Q (x, 
b1 . . . , 
bn)

In short, removing the abbreviation â€œâˆ€â€, and using the logical axiom (tautology)

x <a â†’Â¬Q (x, 
b1 . . . , 
bn)

â†”Â¬

x <a âˆ§Q (x, 
b1 . . . , 
bn)

and the equivalence theorem, we have
âŠ¢Â¬(âˆƒx)

x <a âˆ§Q (x, 
b1 . . . , 
bn)

â–¡
We have established

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 185
I.9.48 Proposition. Every relation in CA is ROB-deï¬nable.
I.9.49 Lemma (Separation Lemma). Let R and Q be any two disjoint semi-
recursive sets. Then there is formula F (x) such that, for all m âˆˆN,
m âˆˆR
implies
ROB âŠ¢F (m)
and
m âˆˆQ
implies
ROB âŠ¢Â¬F (m)
Proof. By I.9.19, there are relations A(x, y) and B(x, y) in CA such that
m âˆˆR â†”(âˆƒx)A(x, m)
(1)
and
m âˆˆQ â†”(âˆƒx)B(x, m)
(2)
for all m. By I.9.48, let A and B deï¬ne A and B respectively.
We deï¬ne F (y) to stand for
(âˆƒx)
%
A(x, y) âˆ§(âˆ€z)

z < x â†’Â¬B (z, y)
&
(3)
and proceed to show that it satisï¬es the conclusion of the lemma.
Let m âˆˆR. Then m /âˆˆQ; hence (by (1) and (2)), A(n, m) holds for some n,
while B(i, m), for all i â‰¥0, are false. Thus,
âŠ¢A(n, m)
(4)
and âŠ¢Â¬B(i, m), for i â‰¥0.â€  By I.9.44, âŠ¢z <n â†’Â¬B(z, m); hence by gene-
ralization followed by |=Taut (using (4)),
âŠ¢A(n, m) âˆ§(âˆ€z)
%
z <n â†’Â¬B(z, m)
&
The substitution axiom and |=Taut yield
âŠ¢(âˆƒx)
%
A(x, m) âˆ§(âˆ€z)

z < x â†’Â¬B(z, m)
&
i.e., âŠ¢F (m).
Let next m âˆˆQ. We want to show that âŠ¢Â¬F (m), which â€“ referring to
(3) and using |=Taut, using the Leibniz rule, and inserting/removing the deï¬ned
symbol â€œâˆ€â€ â€“ translates toâ€¡
(âˆ€x)
%
Â¬A(x, y) âˆ¨(âˆƒz)

z < x âˆ§B (z, y)
&
(5)
â€  Throughout, â€œ âŠ¢â€ is short for â€œ âŠ¢ROBâ€, of course.
â€¡ â€œTranslates toâ€ means â€œis provably equivalent to, without the assistance of any nonlogical
axiomsâ€.

186
I. Basic Logic
The assumption yields m /âˆˆR; hence (by (1) and (2)), B(n, m) holds for some
n, while A(i, m), for all i â‰¥0, are false. Thus,
âŠ¢B(n, m)
(6)
andâŠ¢Â¬A(i, m)fori â‰¥0.Thelatteryields(byI.9.44)âŠ¢x < Sn â†’Â¬A(x, m),
or, using the Leibniz rule and axiom <2,
âŠ¢(x <n âˆ¨x = n) â†’Â¬A(x, m)
(7)
(6) and |=Taut yield âŠ¢n < x â†’n < x âˆ§B(n, m); therefore
âŠ¢n < x â†’(âˆƒz)

z < x âˆ§B(z, m)

(8)
by the substitution axiom and |=Taut.
Proof by cases (I.4.26) and (7) and (8) yield
âŠ¢(x <n âˆ¨x = n âˆ¨n < x) â†’Â¬A(x, m) âˆ¨(âˆƒz)

z < x âˆ§B(z, m)

Hence (axiom <3)
âŠ¢Â¬A(x, m) âˆ¨(âˆƒz)

z < x âˆ§B(z, m)

The generalization of the above is (5).
â–¡
The above lemma trivially holds for predicates of any number of arguments.
That is, we may state and prove, with only notational changes (âƒ—y for y, âƒ—m for
m, âƒ—m for m), the same result for disjoint semi-recursive subsets of Nn, for any
n > 0.
I.9.50 Corollary. Every recursive predicate is deï¬nable in ROB.
Proof. If R âŠ†Nn is recursive, then it and Nn âˆ’A are semi-recursive.
â–¡
We can say a bit more. First, a deï¬nition:
I.9.51 Deï¬nition. A predicate R(âƒ—xn) is strongly formally deï¬nable in , or
just strongly deï¬nable in , iff, for some formula R(âƒ—xn), both of the following
equivalences hold (for all âƒ—bn âˆˆN):
R(âƒ—bn)
iff
 âŠ¢R(
b1, . . . , 
bn)
and
Â¬R(âƒ—bn)
iff
 âŠ¢Â¬R(
b1, . . . , 
bn)
We say that R strongly deï¬nes R (in ).
â–¡

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 187
Correspondingly, a total function f (âƒ—xn) is strongly term-deï¬nable by t just in
case, for all c, âƒ—bn âˆˆN,
f (âƒ—bn) = c
iff
 âŠ¢t(
b1, . . . , 
bn) =c
I.9.52 Corollary. Every recursive predicate is strongly deï¬nable in ROB.
Proof. Let the recursive R âŠ†Nn be deï¬ned (in ROB) by R(âƒ—xn). We already
have the â†’-directions for the positive and negative cases (by â€œweakâ€ deï¬nabil-
ity). We need the â†-directions.
Let then âŠ¢R(
m1, . . . , 
mn). If Â¬R( âƒ—mn), then also âŠ¢Â¬R(
m1, . . . , 
mn),
contradicting the consistency of ROB.â€  Thus, R( âƒ—mn). A similar argument works
for the negative case.
â–¡
We have at once
I.9.53 Corollary. Every recursive predicate is strongly deï¬nable in any con-
sistent extension of ROB.
â–¡
I.9.54 Deï¬nition. Two disjoint semi-recursive subsets R and Q of N are re-
cursively inseparable iff there is no recursive set S such that R âŠ†S and S âŠ†Q
(where Q = N âˆ’Q).
â–¡
I.9.55 Lemma. R = {x : {x}(x) = 0} and Q = {x : {x}(x) = 1} are recursively
inseparable.
Proof. Clearly R and Q are disjoint semi-recursive sets (why?).
Let S be recursive, and suppose that it separates R and Q, i.e.,
R âŠ†S âŠ†Q
(1)
Let {i} = Ï‡S.
Assume that i âˆˆS. Then {i}(i) = 1, hence i âˆˆQ (deï¬nition of Q). But also
i âˆˆQ, by (1).
Thus, i âˆˆS, after all. Well, if so, then {i}(i) = 0, hence i âˆˆR (deï¬nition
of R). By (1), i âˆˆS as well.
Thus i /âˆˆS âˆªS = N, which is absurd.
â–¡
â€  Having adopted a Platonistâ€™s metatheory, the fact that N is a model of ROB establishes consis-
tency. A constructive proof of the consistency of ROB is beyond our scope. For an outline see
Shoenï¬eld (1967).

188
I. Basic Logic
I.9.56 Theorem (Churchâ€™s Theorem). (Church (1936)). For any consistent
extension of ROB, the set Thm is not recursive.
â–¡
Proof. Let  = {âŒœAâŒ: A âˆˆThm}, where  consistently extends ROB.
Applying I.9.49 to the R and Q of I.9.55, we obtain a formula F of a single
free variable v0 such that
m âˆˆR
implies
 âŠ¢F (m)
(1)
and
m âˆˆQ
implies
 âŠ¢Â¬F (m)
(2)
Let S = {m :  âŠ¢F (m)}. By (1), R âŠ†S. By (2) and consistency, Q âˆ©S = âˆ…,
i.e., S âŠ†Q.
By I.9.55,
S /âˆˆRâˆ—
(3)
By deï¬nition of S,
m âˆˆS
iff
 âŠ¢F (m)
iff
 âŠ¢(âˆƒv0)(v0 = m âˆ§F )
(4)
using the Tarski trick once more (p. 175).
We already know that there is a primitive recursive g such that g(m) =
âŒœ(âˆƒv0)(v0 = m âˆ§F )âŒfor all m.
Suppose now that  âˆˆRâˆ—. Then so is S, since (by (4))
m âˆˆS
iff
g(m) âˆˆ
(5)
This contradicts (3).
â–¡
The above theorem, published some three years after GÂ¨odel published his
incompleteness results, shows that the decision problem for any consistent
theory â€“ not just for one that is recursively axiomatized â€“ that contains arith-
metic (ROB) is recursively unsolvable. In particular, we rediscover that T (N),
which extends ROB consistently, is not recursive. Of course, we already know
much more than this about T (N).
Churchâ€™s result shattered Hilbertâ€™s belief that the Entscheidungsproblem (de-
cision problem) of any axiomatic theory ought to be solvable by â€œmechanical
meansâ€.
On the other hand, GÂ¨odelâ€™s incompleteness theorems had already shown the
untenability of Hilbertâ€™s hope to address the consistency of axiomatic theories
by ï¬nitary means.â€ 
â€  Finitary means can be formalized within Peano arithmetic using, if necessary, arithmetization.
However, the consistency of Peano arithmetic is not provable from within.

I.9. Arithmetic, Deï¬nability, Undeï¬nability, and Incompletableness 189
The following is a strengthened (by Rosser (1936)) syntactic version of
GÂ¨odelâ€™s ï¬rst incompleteness theorem. It makes no reference to correctness;
instead it relies on the concept of consistency.
GÂ¨odelâ€™s original syntactic version was proved under stronger consistency
assumptions, that the formal arithmetic under consideration was Ï‰-consistent.â€ 
I.9.57 Theorem (GÂ¨odel-Rosser First Incompleteness Theorem) [Syntactic
Version]. Any consistent recursive formal arithmetic  is simply incomplete.
Proof. We work with the same R, Q, S as in I.9.55â€“I.9.56. By I.9.34,  is
semi-recursive, and hence, so is S by (5) in the previous proof.
How about S?
At this point we add the assumption (that we expect to contradict) that  is
simply complete. Then (referring to the proof of I.9.56),
m âˆˆS
iff
 Ì¸âŠ¢F (m)
completeness
âˆ’â†’
â†âˆ’
consistency
 âŠ¢Â¬F (m)
As in the proof of I.9.56, setting, for all m, f (m) = âŒœ(âˆƒv0)(v0 = m âˆ§Â¬F )âŒ,
we get
m âˆˆS
iff
f (m) âˆˆ
Since f is in PR, S is semi-recursive; hence S is recursive, contradict-
ing I.9.55.
â–¡
I.9.58 Remark. (1) The assumption that  is recursive is not just motivated by
convenience (towards showing that  is r.e.). After all, T (N) is a consistent
and simply complete extension of ROB (but it is not recursively axiomatizable,
as we know).
(2) We can retell the above proof slightly differently:
Let Sâ€² = {m :  âŠ¢Â¬F (m)}. Then Sâ€² is semi-recursive. S âˆ©Sâ€² = âˆ…by consis-
tency. Thus, S âˆªSâ€² Ì¸= N (otherwise Sâ€² = S, making S recursive).
Let n âˆˆN âˆ’S âˆªSâ€². Then Ì¸âŠ¢F (n) and Ì¸âŠ¢Â¬F (n).
It is clear that the set N âˆ’S âˆªSâ€² is inï¬nite (why?); hence we have inï¬nitely
many undecidable sentences.
(3) For each n âˆˆN âˆ’S âˆªSâ€², exactly one of F (n) and Â¬F (n) is true (in N).
Which one?
â€  A formal arithmetic is Ï‰-inconsistent iff for some formula F of a single free variable x, (âˆƒx) F
and all of Â¬ F (m) â€“ for all m â‰¥0 â€“ are provable. Otherwise, it is Ï‰-consistent. Of course, an
Ï‰-consistent theory, as it fails to prove something, is consistent. For GÂ¨odelâ€™s original formulation
of the ï¬rst theorem see Exercise I.116.

190
I. Basic Logic
We have deï¬ned F (y) ((3) on p. 185 originally; see also the proof of I.9.56)
to stand for
(âˆƒx)

A(x, y) âˆ§(âˆ€z)(z < x â†’Â¬B(z, y))

where, with R and Q as in I.9.55â€“I.9.56,
k âˆˆR
iff
|=N (âˆƒx)A(x,k)
andâ€ 
k âˆˆQ
iff
|=N (âˆƒx)B(x,k)
We also note that
|= Â¬F (y) â†”(âˆ€x)

Â¬A(x, y) âˆ¨(âˆƒz)(z < x âˆ§B(z, y))

(i)
Now, if n âˆˆN âˆ’S âˆªSâ€², then n /âˆˆR; hence |=N Â¬(âˆƒx)A(x,n), that is,
|=N (âˆ€x)Â¬A(x,n)
(ii)
Noting that (I.4.24, p. 50)
|= (âˆ€x) C â†’(âˆ€x)(C âˆ¨D )
for any C and D , (ii) yields
|=N Â¬F (n)
(iii)
(4) Ï‰-Incompleteness of arithmetic. Now, (iii) implies
|=N G (k,n)
for all k â‰¥0
by (i) and substitution, where G (x, y) abbreviates
Â¬A(x, y) âˆ¨(âˆƒz)

z < x âˆ§B(z, y)

Thus,
 âŠ¢G (k,n)
for all k â‰¥0
since A(x, y) and B(x, y) (p. 185) are in CA.
Thus we have the phenomenon of Ï‰-incompleteness in any consistent (and
recursive) extension  of ROB. That is, there is a formula H (x) such that while
 âŠ¢H (k) for all k â‰¥0, yet  Ì¸âŠ¢(âˆ€x)H (x). An example of such an H is
G (x,n). Therefore, Thm for such is not closed under the â€œÏ‰-ruleâ€
H (0), H (1), H (2), . . . âŠ¢(âˆ€x)H (x)
â€  This and the previous equivalence are just restatements of (1) and (2) on p. 185.

I.10. Exercises
191
(5) A consistent but Ï‰-inconsistent arithmetic. Next, consider the theory
 + F (n), for some ï¬xed n âˆˆN âˆ’S âˆªSâ€². Since  Ì¸âŠ¢Â¬F (n), this theory is
consistent. However, it is Ï‰-inconsistent:
Indeed, by (ii),
|=N Â¬A(k,n)
for all k â‰¥0
Hence, for reasons already articulated,
 âŠ¢Â¬A(k,n)
for all k â‰¥0
and, trivially,
 + F (n) âŠ¢Â¬A(k,n)
for all k â‰¥0
(iv)
But also  + F (n) âŠ¢F (n), so that, along with (iv), we have derived the
Ï‰-inconsistency of  + F (n); for
 + F (n) âŠ¢(âˆƒx)A(x,n)
â€“ the latter due to âŠ¢(âˆƒx)(C âˆ§D ) â†’(âˆƒx)C and the deï¬nition of F .
(6) A consistent but incorrect arithmetic. The consistent formal arithmetic
 + F (n) is not correct, since Ì¸|=N F (n).
(7) From what we have seen here (cf. also Exercise I.113) we can obtain
an alternative foundation of computability via ROB: We can deï¬ne a recursive
predicate to be one that is strongly deï¬nable in ROB (I.9.52).
We can also deï¬ne a semi-recursive predicate P(âƒ—xn) to be one that is posi-
tively strongly deï¬nable in ROB, i.e., for some P and all âƒ—an,
P(âƒ—an)
iff
ROB âŠ¢P (
a1, . . . , 
an)
We can then say that a partial recursive function is one whose graph is positively
strongly deï¬nable, while a recursive function is a total partial recursive function.
With this understanding, uncomputability coincides with undecidability (of
ROB) and hence with its incomplete(able)ness (unprovability): There are sets
that are positively strongly deï¬nable but not strongly deï¬nable (e.g., the set
K; see also Exercise I.116). Thus, our claim at the beginning of this volume, â€“
that not only is there an intimate connection between uncomputability and
unprovability, but also you cannot have one without having the other â€“ is now
justiï¬ed.
â–¡
I.10. Exercises
I.1. Prove that the closure obtained by I = {3} and the two relations z =
x + y and z = x âˆ’y is the set {3k : k âˆˆZ}.
I.2. The pair that effects the deï¬nition of Term (I.1.5, p. 14) is unambiguous.

192
I. Basic Logic
I.3. The pair that effects the deï¬nition of Wff (I.1.8, p. 15) is unambi-
guous.
I.4. With reference to I.2.13 (p. 25), prove that if all the gQ and h are deï¬ned
everywhere on their input sets (i.e., they are total) â€“ these are I for h
and A Ã— Y r for gQ and (r + 1)-ary Q â€“ then f is deï¬ned everywhere on
Cl(I , R).
I.5. Let us deï¬ne inductively the set of formulas Ar by:
(1) 1, 2, and 3 are in Ar.
(2) If a and b stand for formulas of Ar, then so do a + b and a Ã— b.
(N.B. It is the intention here not to utilize brackets.)
(3) Deï¬ne a function (intuitively speaking), Eval(x) by Eval(x) = x if x
is 1, 2, or 3, and (inductive step) Eval(a + b) = Eval(a) + Eval(b),
Eval(a Ã— b) = Eval(a) Ã— Eval(b) for all a, b in Ar, deï¬ned via (1)
and (2).
Show that the deï¬nition (1)â€“(2) above allows more than one possible
parse, which makes (3) ill-deï¬ned; indeed, show that for some a âˆˆAr,
Eval(a) has more than one possible value, so it is not a function after all.
I.6. Prove that for every formula A in Prop (I.3.2, p. 28) the following is
true: Every nonempty proper preï¬x (I.1.4, p. 13) of the string A has an
excess of left brackets.
I.7. Provethatanynon-primeA inProphasuniquelydeterminedimmediate
predecessors.
I.8. For any formula A and any two valuations v and vâ€², Â¯v(A) = Â¯vâ€²(A) if
v and vâ€² agree on all the propositional variables that occur in A.
I.9. Prove that A[x â†t] is a formula (whenever it is deï¬ned) if t is a term.
I.10. Prove that Deï¬nition I.3.12 does not depend on our choice of new vari-
ables âƒ—zr.
I.11. Prove that âŠ¢(âˆ€x)(âˆ€y)A â†”(âˆ€y)(âˆ€x)A.
I.12. Prove I.4.23.
I.13. Prove I.4.24.
I.14. (1) Show that x < y âŠ¢y < x (< is some binary predicate symbol; the
choice of symbol here is meant to provoke).
(2) Show informally that Ì¸âŠ¢x < y â†’y < x (Hint: Use the assumption
that our logic â€œdoes not lieâ€ (soundness theorem).)
(3) Does this invalidate the deduction theorem? Explain.
I.15. Prove I.4.25
I.16. Prove that âŠ¢x = y â†’y = x.
I.17. Prove that âŠ¢x = y âˆ§y = z â†’x = z.
I.18. Suppose that  âŠ¢ti = si for i = 1, . . . , m, where the ti, si are arbitrary
terms. Let F be a formula, and F â€² be obtained from it by replacing any

I.10. Exercises
193
number of occurrences of ti in F (not necessarily all) by si. Prove that
 âŠ¢F â†”F â€².
I.19. Suppose that  âŠ¢ti = si for i = 1, . . . , m, where the ti, si are arbitrary
terms. Letr be a term, andrâ€² be obtained from it by replacing any number
of occurrences of ti in r (not necessarily all) by si.
Prove that  âŠ¢r = râ€².
I.20. Settle the Pause following I.4.21.
I.21. Prove I.4.27.
I.22. Suppose that x is not free in A. Prove that âŠ¢A â†’(âˆ€x)A and âŠ¢
(âˆƒx)A â†’A.
I.23. Prove the distributive laws :
âŠ¢(âˆ€x)(A âˆ§B ) â†”(âˆ€x)A âˆ§(âˆ€x)B
and
âŠ¢(âˆƒx)(A âˆ¨B ) â†”
(âˆƒx)A âˆ¨(âˆƒx)B .
I.24. Prove âŠ¢(âˆƒx)(âˆ€y)A â†’(âˆ€y)(âˆƒx)A with two methods: ï¬rst using the
auxiliary constant method, next exploiting monotonicity.
I.25. Prove âŠ¢(âˆƒx)(A â†’(âˆ€x)A).
In what follows let us denote by 1 the pure logic of Section I.3 (I.3.13
and I.3.15).
Let us now introduce a new pure logic, which we will call 2. This is exactly
the same as our 1 (which we have called just  until now), except that we
have a different axiom group Ax1. Instead of adopting all tautologies, we only
adopt the following four logical axiom schemata of group Ax1 in 2:â€ 
(1) A âˆ¨A â†’A
(2) A â†’A âˆ¨B
(3) A âˆ¨B â†’B âˆ¨A
(4) (A â†’B ) â†’(C âˆ¨A â†’C âˆ¨B )
2 is due to Hilbert (actually, he also included associativity in the axioms,
but, as Gentzen has proved, this was deducible from the system as here given;
therefore it was not an independent axiom â€“ see Exercise I.35). In the exercises
below we write âŠ¢i for âŠ¢i, i = 1, 2.
I.26. Show that for all F and every set of formulas , if  âŠ¢2 F holds, then
so does  âŠ¢1 F .
Our aim is to see that the logics 1 and 2 are equivalent, i.e., have exactly the
same theorems. In view of the trivial Exercise I.26 above, what remains to be
shown is that every tautology is a theorem of 2. One particular way to prove
this is through the following sequence of 2-facts.
â€  Â¬ and âˆ¨are the primary symbols; â†’, âˆ§, â†”are deï¬ned in the usual manner.

194
I. Basic Logic
I.27. Show the transitivity of â†’in 2:
A â†’B , B â†’C âŠ¢2 A â†’C
for all A, B , and C .
I.28. Show that âŠ¢2 A â†’A (i.e., âŠ¢2 Â¬A âˆ¨A), for any A.
I.29. For all A, B show that âŠ¢2 A â†’B âˆ¨A.
I.30. Show that for all A and B , A âŠ¢2 B â†’A.
I.31. Show that for all A, âŠ¢2 Â¬Â¬A â†’A and âŠ¢2 A â†’Â¬Â¬A.
I.32. ForallAandB ,showthatâŠ¢2 (A â†’B ) â†’(Â¬B â†’Â¬A).Conclude
that A â†’B âŠ¢2 Â¬B â†’Â¬A.
(Hint. âŠ¢2 A â†’Â¬Â¬A.)
I.33. Show that A â†’B âŠ¢2 (B â†’C ) â†’(A â†’C ) for all A, B , C .
I.34. (Proof by cases in 2.) Show for all A, B , C , D ,
A â†’B , C â†’D âŠ¢2 A âˆ¨C â†’B âˆ¨D
I.35. Show for all A, B , C that
(1) âŠ¢2 A âˆ¨(B âˆ¨C ) â†’(A âˆ¨B ) âˆ¨C and
(2) âŠ¢2 (A âˆ¨B ) âˆ¨C â†’A âˆ¨(B âˆ¨C ).
I.36. Deduction theorem in â€œpropositionalâ€ 2. Prove that if , A âŠ¢2 B
using only modus ponens, then also  âŠ¢2 A â†’B using only modus
ponens, for any formulas A, B and set of formulas .
(Hint. Induction on the length of proof of B from  âˆª{A}, using the
results above.)
I.37. Proof by contradiction in â€œpropositionalâ€ 2. Prove that if , Â¬A de-
rives a contradiction in 2 using only modus ponens,â€  then  âŠ¢2 A
using only modus ponens, for any formulas A and set of formulas .
Also prove the converse.
We can now prove the completeness theorem (Postâ€™s theorem) for the â€œpropo-
sitional segmentâ€ of 2, that is, the logic 3 â€“ so-called propositional logic (or
propositional calculus) â€“ obtained from 2 by keeping only the â€œpropositional
axiomsâ€ (1)â€“(4) and modus ponens, dropping the remaining axioms and the
âˆƒ-introduction rule. (Note. It is trivial that if  âŠ¢3 A, then  âŠ¢2 A.) Namely,
we will prove that, for any A and , if  |=Taut A, then  âŠ¢3 A.
First, a deï¬nition:
I.10.1 Deï¬nition (Complete Sets of Formulas). A set  is complete iff for
every A, at least one of A or Â¬A is a member of .
â–¡
â€  That is, it proves some B but also proves Â¬B .

I.10. Exercises
195
I.38. Let  Ì¸âŠ¢3 A. Prove that there is a complete  âŠ‡ such that also
 Ì¸âŠ¢3 A. This is a completion of .
(Hint. Let F 0, F 1, F 3, . . . be an enumeration of all formulas. (There is
such an enumeration. Deï¬ne n by induction on n:
0 = 
n+1 =
ï£±
ï£²
ï£³
n âˆª{F n}
if n âˆª{F n} Ì¸âŠ¢3 A
otherwise
n âˆª{Â¬F n}
if n âˆª{Â¬F n} Ì¸âŠ¢3 A
To make sense of the above deï¬nition, show the impossibility of having
both n âˆª{F n} âŠ¢3 A and n

{Â¬F n} âŠ¢3 A. Then show that  =

nâ‰¥0 n is as needed.)
I.39. (Post.) If  |= A, then  âŠ¢3 A.
(Hint. Prove the contrapositive. If  Ì¸âŠ¢3 A, let  be a completion
(Exercise I.38) of  such that  Ì¸âŠ¢3 A. Now, for every prime formula
(cf. I.3.1, p. 28) P , exactly one of P or Â¬P (why exactly one?) is in .
Deï¬ne a valuation (cf. I.3.4, p. 29) v on all prime formulas by
v(P ) =
0
if P âˆˆ
1
otherwise
Of course, â€œ0â€ codes, intuitively, â€œtrueâ€, while â€œ1â€ codes â€œfalseâ€. To
conclude, prove by induction on the formulas of Prop (cf. I.3.2, p. 28)
that the extension v of v satisï¬es, for all formulas B , v(B ) = 0 iff
B âˆˆ. Argue that A /âˆˆ.)
I.40. If  |=Taut A, then  âŠ¢2 A.
I.41. For any formula F and set of formulas, one has  âŠ¢1 F iff  âŠ¢2 F .
I.42. (Compactness of propositional logic.) We say that  is ï¬nitely satisï¬able
(in the propositional sense) iff every ï¬nite subset of  is satisï¬able
(cf. I.3.6, p. 30). Prove that  is satisï¬able iff it is ï¬nitely satisï¬able.
(Hint. Only the if part is nontrivial. It uses Exercise I.39. Further hint: If
 is unsatisï¬able, then  |=Taut A âˆ§Â¬A for some formula A.)
I.43. Prove semantically, without using soundness, that A |= (âˆ€x)A.
I.44. Give a semantic proof of the deduction theorem.
I.45. Show semantically that for all A and B , A â†’B |= (âˆ€x)A â†’(âˆ€x)B .
I.46. Show semantically that for all A and B , A â†’B |= (âˆƒx)A â†’(âˆƒx)B .
I.47. Prove the claim in Remark I.6.7.
I.48. Prove that the composition of two embeddings Ï† : M â†’K and Ïˆ : K â†’
L is an embedding.
I.49. Find two structures that are elementarily equivalent, but not isomorphic.

196
I. Basic Logic
I.50. Let Ï† : M â†’M be an isomorphism. We say also (since we have the
same structure on both sides of â€œâ†’â€) that Ï† is an automorphism. Prove
that if S âŠ†|M|n is ï¬rst order deï¬nable (cf. I.5.15) in M, then,for all âƒ—in
in |M|,
âŸ¨âƒ—inâŸ©âˆˆS
iff
âŸ¨Ï†(i1), . . . , Ï†(in)âŸ©âˆˆS
I.51. Prove that N is not ï¬rst order deï¬nable in the structure R = (R, <)
(R is the set of reals).
(Hint. Use Exercise I.50 above.)
I.52. Prove that addition is not deï¬nable in (N, Ã—). More precisely, show that
the set {âŸ¨x, y, zâŸ©âˆˆN3 : z = x + y} is not deï¬nable.
(Hint. Deï¬ne a function Ï† : N â†’N by Ï†(x) = x if x is not divisible
by 2 or 3. Otherwise Ï†(x) = y, where y has the same prime number
decomposition as x, except that the primes 2 and 3 are interchanged.
For example, Ï†(6) = 6, Ï†(9) = 4, Ï†(12) = 18. Prove that Ï† is an
automorphism on (N, Ã—), and then invoke Exercise I.50 above.)
I.53. Prove the only-if part of the LoÂ´s-Tarski theorem (I.6.26).
I.54. Prove Theorem I.6.29.
I.55. Prove the if part of Theorem I.6.30.
I.56. Prove by a direct construction, without using the upward LÂ¨owenheim-
Skolem theorem, that there are nonstandard models for arithmetic.
(Hint. Work with the theory Th(N) âˆª{n < C : n âˆˆN} where C is a new
constant added to the language of arithmetic LN = {0, S, +, Ã—, <}.
Use compactness and the consistency theorem.)
I.57. Prove that if  has arbitrarily large ï¬nite models, then it has an inï¬nite
model.
I.58. Prove by a direct construction, without using the upward LÂ¨owenheim-
Skolem theorem, that there is a nonstandard model for all true ï¬rst order
sentences about the reals.
I.59. Prove Proposition I.6.45.
I.60. Prove the pinching lemma (I.6.53).
I.61. Conclude the proof of I.6.55.
I.62. Let N, a unary predicate of the extended language L of the reals, have
as its interpretation N R = N, the set on natural numbers. Use it to prove
that there are inï¬nite natural numbers in âˆ—R.
(Hint. Use the true (in R) sentence (âˆ€x)(âˆƒy)(N(y) âˆ§x < y).)
I.63. Prove that if h is an inï¬nite natural number, then so is h âˆ’1. A side effect
of this is an inï¬nite descending chain of (inï¬nite) natural numbers in âˆ—R.
Hence there are nonempty sets of natural numbers in âˆ—R with no mini-
mum element. Does this contradict the transfer principle (p. 99)? Why?

I.10. Exercises
197
I.64. Prove the extreme value theorem: Every real function of one real variable
that is continuous on the real interval [a, b] attains its maximum. That
is, (âˆƒx âˆˆ[a, b])(âˆ€y âˆˆ[a, b]) f (y) â‰¤f (x).
(Hint. Subdivide [a, b] into n > 0 subintervals of equal length, [ai, ai+1],
where ai = a + (b âˆ’a)i/n for i = 0, . . . , n. Formulate as a ï¬rst order
sentence over L the true (why true?) statement that â€œfor all choices of
n > 0 (in N) there is an i âˆˆN such that f (a + (b âˆ’a)i/n) is maximum
among the values f (a+(bâˆ’a)k/n), k = 0, . . . , nâ€. Transfer to âˆ—R. This
is still true. Now take n to be an inï¬nite natural number K. Let I be a (hy-
perreal) natural number that makes f (a + (b âˆ’a)I/K) maximum among
the f (a+(bâˆ’a)i/K), 0 â‰¤i â‰¤K. See if f (st(a+(bâˆ’a)I/K)) is what you
want.)
I.65. Use the technique of the previous problem to prove the intermediate
value theorem: Every real function of one real variable that is continu-
ous on the real interval [a, b] attains every value between its minimum
and its maximum.
I.66. Prove the existence of inï¬nite primes in âˆ—R.
I.67. Let T be a pure theory over some language L. Form the theory Tâ€² over
L âˆª{Ï„} by adding the schemata
(âˆ€x)(A â†”B ) â†’(Ï„x)A = (Ï„x)B
and
(âˆƒx)A[x] â†’A[(Ï„x)A]
where Ï„ is a new symbol used to build terms: If A is a wff, then (Ï„x)A
is a term. Prove that Tâ€² extends T conservatively (Ï„ may be interpreted
as that of p. 123).
I.68. Prove that in the presence of the initial functions of PR (I.8.10, p. 128)
the following Grzegorczyk (1953) substitution operations can be simu-
lated by composition (I.8.6):
(a) Substitution of a function into a variable
(b) Substitution of a constant into a variable
(c) Identiï¬cation of any two variables
(d) Permutation of any two variables
(e) Introduction of new (â€œdummyâ€) variables (i.e., forming Î»âƒ—xâƒ—y.g(âƒ—y)).
I.69. Prove that if f is total and Î»âƒ—x y.y = f (âƒ—x) is in Râˆ—, then f âˆˆR. Is the
assumption that f is total necessary?
I.70. Show that both PR and R are closed under bounded summation,
 
y<z f (y, âƒ—x) and bounded product, !
y<z f (y, âƒ—x).

198
I. Basic Logic
I.71. Prove that if f is total and Î»âƒ—x y.y = f (âƒ—x) is in Pâˆ—(semi-recursive), then
f âˆˆR.
I.72. Prove I.8.27 using the if-then-else function rather than addition and
multiplication.
I.73. Prove that pn â‰¤22n for n â‰¥0.
(Hint. Course-of-values induction on n. Work with p0 p1 . . . pn + 1.)
I.74. Prove, without using the fact that Î»n.pn âˆˆPR, that Ï€ âˆˆPR, where the
Ï€-function is given by Ï€(n) = (the number of primes â‰¤n).
I.75. Prove, using Exercise I.74 above, but without using the fact that Î»n.pn âˆˆ
PR, that the predicate Î»yn.y = pn is in PRâˆ—. Conclude, using Exer-
cise I.73, that Î»n.pn âˆˆPR.
I.76. The Ackermann functionâ€  is given by double recursion by
A(0, x) = x + 2
A(n + 1, 0) = 2
A(n + 1, x + 1) = A(n, A(n + 1, x))
Prove that Î»nx.A(n, x) âˆˆR.
(Hint. Deï¬ne
F(z, n, x) =
ï£±
ï£²
ï£³
x + 2
if n = 0
2
if n > 0 âˆ§x = 0
{z}(n
.âˆ’1, {z}(n, x
.âˆ’1))
if n > 0 âˆ§x > 0
Now apply the recursion theorem.)
I.77. Prove that there exists a partial recursive h that satisï¬es
h(y, x) =
y
if x = y + 1
h(y + 1, x)
otherwise
Which function is Î»x.h(0, x)?
(Hint. Use the recursion theorem, imitating your solution to Exercise I.76
above.)
I.78. Prove that there exists a partial recursive k that satisï¬es
k(y, x) =
0
if x = y + 1
k(y, x) + 1
otherwise
Which function is Î»x.k(0, x)?
â€  There are many versions, their origin part of computability folklore. The one here is not the
original one.

I.10. Exercises
199
I.79. Given Î»yâƒ—x. f (y, âƒ—x) âˆˆP. Prove that there exists a partial recursive g that
satisï¬es
g(y, âƒ—x) =
y
if f (y, âƒ—x) = 0
g(y + 1, x)
otherwise
How can you express Î»x.g(0, x) in terms of f ?
I.80. Prove that K = {x : {x}(x) â†“} is not a complete index set, that is, there
is no D âŠ†P such that K = {x : {x} âˆˆD}.
(Hint. Show that there is an e âˆˆN such that {e}(e) = 0, while {e}(x) â†‘
if x Ì¸= e.)
I.81. Is {x : {x}(x) = 0} a complete index set? Recursive? Semi-recursive?
(Your answer should not leave any dangling â€œwhyâ€s.)
I.82. Let f âˆˆR. Is {x : { f (x)}(x) â†“} a complete index set? Why?
I.83. Consider a complete index set A = {x : {x} âˆˆD} such that there are two
functions {a} and {b}, the ï¬rst in D, the second not in D, and {a} âŠ†{b}.
Prove that there is a 1-1 recursive function f such that
x âˆˆK â†”f (x) âˆˆA
Conclude that A is not semi-recursive.
(Hint. To ï¬nd f use the S-m-n theorem to show that you can have
{ f (x)}(y) =
{a}(y)
if computation {a}(y) takes â‰¤steps than {x}(x)
{b}(y)
otherwise
The wordy condition above can be made rigorous by taking â€œstepsâ€ to
be intuitively measured by the size of z in T (i, x, z). â€œâ‰¤â€ is understood
to be fulï¬lled if both {a}(y) and {x}(x) are undeï¬ned.)
I.84. An A as the above is productive.
I.85. Prove that there is an f âˆˆP such that Wx Ì¸= âˆ…iff f (x) â†“, and Wx Ì¸= âˆ…
iff f (x) âˆˆWx.
I.86. Selection theorem. Prove that for every n > 0, there is a function Î»aâƒ—yn.
Sel(n)(a, âƒ—yn) such that
(âˆƒx)({a}(x, âƒ—yn) â†“) â†”Sel(n)(a, âƒ—yn) â†“
and
(âˆƒx)({a}(x, âƒ—yn) â†“) â†”{a}(Sel(n)(a, âƒ—yn), âƒ—yn) â†“
(Hint. Expand on the proof idea of Exercise I.85.)
I.87. Prove that f âˆˆP iff its graph Î»yâƒ—x.y = f (âƒ—x) is in Pâˆ—.
(Hint. For the if part, one, but not the only, way is to apply the selection
theorem.)

200
I. Basic Logic
I.88. Deï¬nition by positive cases. Let Ri(âƒ—xn), i = 1, . . . , k, be mutually exclu-
sive relations in Pâˆ—, and Î»âƒ—xn. fi(âƒ—xn), i = 1, . . . , k, functions in P. Then
f deï¬ned below is in P:
f (âƒ—xn) =
ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
f1(âƒ—xn)
if R1(âƒ—xn)
...
...
fk(âƒ—xn)
if Rk(âƒ—xn)
â†‘
otherwise
(Hint. The if-then-else function will not work here. (Why? I thought P
was closed under if-then-else.) Either use the selection theorem directly,
or use Exercise I.87.)
I.89. Prove that every r.e. relation R(âƒ—x) can be enumerated by a partial recur-
sive function.
(Hint. Modify the â€œotherwiseâ€ part in the proof of I.8.51.)
I.90. Prove that there is an h âˆˆPR such thatWx = ran({h(x)}).
(Hint. Use Exercise I.89, but include x in the active arguments. Then use
the S-m-n theorem.)
I.91. Sharpen Exercise I.90 above as follows: Ensure that h is such that
Wx Ì¸= âˆ…implies {h(x)} âˆˆR.
(Hint. Use Exercise I.85 for the â€œotherwiseâ€.)
I.92. Prove that for some Ïƒ âˆˆPR, ran({x}) = dom({Ïƒ(x)}).
(Hint. Show that y âˆˆran({x}) is semi-recursive, and then use S-m-n.)
I.93. Prove that there is a Ï„ âˆˆPR such that ran({x}) = ran({Ï„(x)}) and,
moreover, ran({x}) Ì¸= âˆ…implies that {Ï„(x)} âˆˆR.
I.94. Prove that a set is recursive and nonempty iff it is the range of a non-
decreasing recursive function.
(Hint. To check for a âˆˆran( f ), f non-decreasing, ï¬nd the smallest i
such that a â‰¤f (i), etc.)
I.95. Prove that a set is recursive and inï¬nite iff it is the range of a strictly
increasing recursive function.
I.96. Prove that every inï¬nite semi-recursive set has an inï¬nite recursive sub-
set.
(Hint. Effectively deï¬ne a strictly increasing subsequence.)
I.97. Prove that there is an m âˆˆPR such that Wx inï¬nite implies that Wm(x) âŠ†
Wx andWm(x) is an inï¬nite recursive set.
(Hint. Use Exercise I.91 and I.96.)
I.98. Prove that there is an h in PR such that Wx âˆ©Wy = Wh(x,y) (all x, y).
I.99. Prove that there is an k âˆˆPR such that Wx âˆªWy = Wk(x,y) (all x, y).
I.100. Prove that there is an g âˆˆPR such that Wx Ã— Wy = Wg(x,y) (all x, y).

I.10. Exercises
201
I.101. Prove that P is not closed under min by ï¬nding a function f âˆˆP such
that
Î»x.
min{y : f (x, y) = 0}
if min exists
â†‘
otherwise
is total and 0-1-valued, but not recursive.
(Hint. Try f (x, y) that yields 0 if (y = 0 âˆ§{x}(x) â†“) âˆ¨y = 1 and is
undeï¬ned for all other inputs.)
I.102. Prove that Î»x.{x}(x) has no (total) recursive extension.
I.103. Express the projections K and L of J(x, y) = (x + y)2 + x in closed
form â€“ that is, without using (Âµy)<z or bounded quantiï¬cation.
(Hint. Solve for x and y the Diophantine equation z = (x + y)2 + x.
The term âŒŠâˆšzâŒ‹is involved in the solution.)
I.104. Prove that the pairing function J(x, y) = (x + y)(x + y + 1)/2 + x is
onto (of course, the division by 2 is exact), and ï¬nd its projections K
and L in closed form.
(Hint. For the onto part you may convince yourself that J enumerate-
spairs as follows (starting from the 0th pair, (0, 0): It enumerates by
ascending group number, where the group number of the pair (a, b) is
a + b. In each group it enumerates by ascending ï¬rst component; thus
(a, b) is ath in the group of number a + b.)
I.105. Find a polynomial onto pairing function via the following enumera-
tion: Enumerate by group number. Here the group number of (a, b) is
max(a, b), that is, a
.âˆ’b + b. In group i the enumeration is
(0, i), (1, i), (2, i), . . . , (i âˆ’1, i), (i, i), (i, i âˆ’1), (i, i âˆ’2),
(i, i âˆ’3), . . . , (i, 1), (i, 0)
Find also the projections K and L in closed form.
By the way, what makes this J â€œpolynomialâ€ is that (like the one in Exer-
cise I.103 above) it does not involve division. It only involves +, Ã—,
.âˆ’,
and substitutions.
I.106. Prove that Î»âƒ—xn.âŸ¨âƒ—xnâŸ©, where âŸ¨. . . âŸ©is that deï¬ned on p. 165, is in PR.
I.107. Prove that if t is a closed term of LN, then
ROB âŠ¢t = 
tN
(Hint. Induction on terms.)
I.108. Prove, by constructing an appropriate model, that (âˆƒx)x < x is consis-
tent with ROB, and therefore ROBÌ¸âŠ¢Â¬x < x.
(Hint. For example, you can build a model on the set N âˆª{âˆ}, where
â€œâˆâ€ is a new symbol (not in N).)

202
I. Basic Logic
I.109. Prove, by constructing an appropriate model, that (âˆƒx)(âˆƒy)x + y Ì¸=
y + x is consistent with ROB, and therefore ROB Ì¸âŠ¢x + y = y + x.
I.110. Prove that x Ã— y = z is ROB-deï¬nable.
I.111. Prove I.9.45.
I.112. Complete the proof of I.9.46.
I.113. Prove that if A âŠ†N is positively strongly formally deï¬nable in ROB â€“
that is, for some A and all n, n âˆˆA iff ROB âŠ¢A(n) â€“ then it is semi-
recursive. What can you say if we drop â€œpositivelyâ€? Is the converse true?
I.114. Is either of the two sets in I.9.55 recursive? Why?
I.115. Prove that if  is a complete extension of ROB and {âŒœAâŒ: A âˆˆ} is
recursive, then  has a decidable decision problem, i.e., {âŒœAâŒ:  âŠ¢A}
is recursive.
I.116. GÂ¨odelâ€™s ï¬rst incompleteness theorem â€“ original version. Prove that if 
is a Ï‰-consistent extension of ROB and {âŒœAâŒ: A âˆˆ} is recursive,
then  is incomplete.
(Hint. This is a suggestion for â€œproof by hindsightâ€, using recursion-
theoretic techniques (not GÂ¨odelâ€™s original proof). So, prove, under the
statedassumptions,thateverysemi-recursive A âŠ†Nispositivelystrongly
deï¬nable (cf. Exercise I.113) in  (Ï‰-consistency helps one direction).
Thus, the halting set K is so deï¬nable. What does this do to Thm? Etc.)
We explore here some related formal deï¬nability concepts for functions.
We say that a total Î»âƒ—xn. f (âƒ—xn) is formally functionally deï¬nable (in some exten-
sion of ROB, ) iff for some F (y,âƒ—xn) the following holds for all b, âƒ—an in N:
b = f (âƒ—an)
implies
 âŠ¢F (y,
a1, . . . , 
an) â†”y = b
(1)
I.117. Prove that if  is a consistent extension of ROB (for example, ROB itself
or a conservative extension), then, in the deï¬nition above, the informal
â€œimpliesâ€ can be strengthened to â€œiffâ€.
I.118. Prove that a total Î»âƒ—xn. f (âƒ—xn) is formally functionally deï¬nable (in some-
extension of ROB, ) iff for some F (y,âƒ—xn) the following hold for all
b, âƒ—an in N:
(i) The graph of f â€“ Î»yâƒ—xn.y = f (âƒ—xn) â€“ is formally deï¬ned in  by F
in the sense of I.9.38, and
(ii) b = f (âƒ—an) implies  âŠ¢F (y, 
a1, . . . , 
an) â†’y = b.
I.119. Actually, the above was just a warm-up and a lemma. Prove that a total
f is formally functionally deï¬nable in ROB (or extension thereof) iff
its graph is just deï¬nable (I.9.38).
(Hint. For the hard direction, let (using a single argument for notational
convenience) F (x, y) deï¬ne y = f (x) in the sense of I.9.38. Prove that

I.10. Exercises
203
G (x, y), a short name for
F (x, y) âˆ§(âˆ€z < y)Â¬F (x, z)
also deï¬nes y = f (x) and moreover satisï¬es
if
f (a) = b
then
âŠ¢G (a, y) â†’y = b
To this end assume f (a) = b and prove, ï¬rst, that âŠ¢G (a, y) â†’y < Sb,
and second (using I.9.44) that âŠ¢y < Sb â†’G (a, y) â†’y = b.)
I.120. Let the total f of one variable be weakly deï¬nable in ROB (or extension
thereof), that is, its graph is formally deï¬nable in the sense of I.9.38. Let
A(x) be any formula. Then prove that, for some well-chosen formula
B(x),
(âˆ€a âˆˆN) âŠ¢B(a) â†”A( +
f (a))
(Hint. By Exercise I.119, there is a F (y, x) that functionally deï¬nes f
((1) above). Take for B the obvious: (âˆƒy)(F (y, x) âˆ§A(y)).)
I.121. Let A âŠ†Nbepositivelystronglydeï¬nableinROB,andthetotalÎ»x. f (x)
be functionally deï¬nable in ROB. Prove that f âˆ’1[A], the inverse image
of A under f , is also positively strongly deï¬nable. Give two proofs: one
using the connection between ROB and recursion theoretic concepts,
the second ignorant of recursion theory.
(Hint. Use Exercise I.120.)
We have used the formula (âˆƒv0)(v0 = n âˆ§F (v0)) â€“ which is logically equiv-
alent to F (n) by the one point rule (I.7.2) â€“ on a number of occasions (e,g.,
p. 175), notably to â€œeasilyâ€ obtain a GÂ¨odel number of (a formula equivalent to)
F (n) from a GÂ¨odel number of F (v0) andn. This GÂ¨odel number is (pretending
that âˆ§is a primitive symbol so that we do not obscure the notation)

âŒœâˆƒâŒ, âŒœv0âŒ, âŸ¨âŒœâˆ§âŒ, âŸ¨âŒœ= âŒ, âŒœv0âŒ, âŒœnâŒâŸ©, âŒœF (v0)âŒâŸ©

The above, with n as the only variable, is recursive (indeed, primitive recursive).
So trivially, is, the function s of two (informal) variables n and x over N:
s = Î»nx

âŒœâˆƒâŒ, âŒœv0âŒ, âŸ¨âŒœâˆ§âŒ, âŸ¨âŒœ= âŒ, âŒœv0âŒ, âŒœnâŒâŸ©, xâŸ©

Clearly,
s(n, âŒœF (v0)âŒ) = âŒœ(âˆƒv0)(v0 = n âˆ§F (v0))âŒ
A ï¬xed point, or ï¬xpoint, of a formula A(v0) in an extension of ROB, , is a
sentence F such that
 âŠ¢F â†”A( 
âŒœF âŒ)

204
I. Basic Logic
I.122. Let ROB â‰¤ and A(v0) be any formula in the language of . Prove
that A has a ï¬xpoint F in .
(Hint. The function s above is recursive; thus so is D = Î»x.s(x, x).
Therefore, D is functionally deï¬nable in . Now use Exercise I.120
with f = D. See if you can use a â€œgoodâ€ a âˆˆN.)
I.123. Tarskiâ€™s â€œundeï¬nability of truthâ€ again (cf. I.9.31). Prove that T =
{âŒœAâŒ: A âˆˆT (N)} is not (semantically) deï¬nable in N, basing the
argument on the existence of ï¬xpoints in ROB and on the latterâ€™s cor-
rectness.
(Hint. Suppose that some LN formula, say A(v0), deï¬nes T. Use the
previous exercise to ï¬nd a sentence that says â€œI am falseâ€.)
I.124. Let us use the term strongly Âµ-recursive functions for the smallest set of
functions, Râ€², that includes the initial functions of PR and the functions
Î»xy.x + y, Î»xy.xy, and Î»xy.x
.âˆ’y, and is closed under composition
and (Âµy) applied on total, regular functions Î»âƒ—x y.g(âƒ—x, y), that is, total
functions satisfying
(âˆ€âƒ—x)(âˆƒy)g(âƒ—x, y) = 0
Prove that Râ€² = R.
(Hint. Use coding and decoding, e.g., via the Î²-function, to implement
primitive recursion.)
I.125. Prove (again) that all recursive functions are functionally deï¬nable in
ROB. Do so via Exercise I.124, by induction on Râ€², without using the
separation lemma (I.9.49).
I.126. (Craig.) Prove that a recursively enumerable set of sentences T over a
ï¬nitely generated language (e.g., like that of arithmetic) admits a recur-
sive set of axioms, i.e., for some recursive , T = Thm.
(Hint. Note that for any A âˆˆT , any two sentences in the sequence
A, A âˆ§A, A âˆ§A âˆ§A, . . .
are logically equivalent. Now see if Exercises I.94 and I.95 can be of
any help.)
I.127. Let T be the pure theory over the language that contains precisely the
following nonlogical symbols: One constant, one unary function, two
binary functions, and one binary predicate. Prove that T has an undecid-
able decision problem, that is, {âŒœAâŒ: A âˆˆT} is not recursive.

II
The Second Incompleteness Theorem
Our aim in the previous section was to present GÂ¨odelâ€™s ï¬rst incompleteness
theorem in the context of recursion theory. Much as this â€œmodernâ€ approach is
valuable for showing the links between unprovability and uncomputability, it
has obscured the simplicity of GÂ¨odelâ€™s ingenious idea (as it was carried out in
his original paper (1931)).
What he had accomplished in that paper, through arithmetization of for-
mulas and proofs, was to build a sentence of arithmetic, F , that said â€œI am
not a theoremâ€. One can easily prove, metamathematically, that such an F is
undecidable, if arithmetic is Ï‰-consistent.
To see this at the intuitive level, let us replace Ï‰-consistency by correctness.
Then surely F is not provable, for if it is, then it is a theorem, and hence false
(contradicting correctness).
On the other hand, we have just concluded that F is true! Hence, Â¬F is
false, and therefore not provable either (by correctness).
This simple application of the â€œliarâ€™s paradoxâ€â€  is at the heart of the ï¬rst
incompleteness theorem.
Imagine now that the arithmetization is actually carried out within (some)
formal arithmetic, and that with some effort we have managed to embed into
formal arithmetic the metamathematical argument that leads to the assertion â€œif
arithmetic is consistent, then Ì¸âŠ¢F â€â€¡. The quoted statement is formalized by
â€œCon â†’F â€, where â€œConâ€ is some (formal) sentence that says that arithmetic
is consistent. This is so, intuitively, since F â€œsaysâ€: â€œF is not a theoremâ€.
â€  Attributed in its original form to Epimenides of Crete, who proclaimed: â€œAll Cretans are liarsâ€. Is
this true? GÂ¨odelâ€™s version is based on the variation: â€œI am lyingâ€. Where does such a proclamation
lead?
â€¡ Where, of course, â€œ âŠ¢â€ is using the nonlogical axioms of our formal arithmetic.
205

206
II. The Second Incompleteness Theorem
It follows that Ì¸âŠ¢Con (why?).
This is GÂ¨odelâ€™s second incompleteness theorem, that if a recursively axioma-
tized theory is a consistent extension of (Peano) arithmetic, then it cannot prove
the sentence that asserts its own consistency.
In order to prove this theorem we will need to develop enough formal arith-
metic to be able to carry out elementary arguments in it. We will also need to
complete the details of our earlier arithmetization, within formal arithmetic this
time.
II.1. Peano Arithmetic
We start by extending ROB with the addition of the induction axiom schema, to
obtain Peano arithmetic. Within this arithmetic we will perform all our formal
reasoning and constructions (arithmetization).
As a by-product of the required extra care that we will exercise in this
section, regarding arithmetization details, we will be able to see through some
technicalities that we have suppressed in I.9.33â€“I.9.34 and I.9.37. For example,
we had accepted there, without explicitly stating so, that our standard rules of
inference, modus ponens and (âˆƒ)-introduction, are recursive, and therefore ï¬t
the general description of the unspeciï¬ed rules I1 and I2 on p. 176.
We will soon see below why this is so. Similarly, we have said that  is
recursive. At the intuitive level this is trivial, since some logical axioms are
recognized by their form (e.g., the axiom x = x), while the tautologies can be
recognized by constructing a truth table. While this ability to recognize tau-
tologies can be demonstrated rigorously, that is far too tedious an undertaking.
Instead, we opt for a more direct avenue. In the exercises (Exercises I.26â€“I.41)
we led the reader to adopt a ï¬nite set of axiom schemata in lieu of the inï¬nitely
many schemata whose instances are tautologies. This makes it much easier to
see that this amended  is recursive (even primitive recursive).
II.1.1 Deï¬nition (Peano Arithmetic). We extend ROB over the same lan-
guage, LN, by adding the induction axiom (in reality, an induction schema),
Ind, below:
A[x â†0] âˆ§(âˆ€x)(A â†’A[x â†Sx]) â†’A
(Ind)
We often write more simplyâ€ 
A[0] âˆ§(âˆ€x)(A[x] â†’A[Sx]) â†’A[x]
â€  Cf. p. 33.

II.1. Peano Arithmetic
207
or just
A[0] âˆ§(âˆ€x)(A â†’A[Sx]) â†’A
The theory ROB + Ind is called Peano arithmetic, for short PA.
â–¡
II.1.2 Remark (A Note on Nonlogical Schemata and Deï¬ned Symbols).
Metatheorems I.7.1 and I.7.3 ensure that the addition of deï¬ned predicates,
functions, and constants to any language and theory results in a conservative
extension of the theory, that is, any theorem of the new theory over the old
(original) language is also provable in the old theory. Moreover, we saw how
any formula A of the new language can be naturally transformed to a formula
A âˆ—of the old language (eliminating deï¬ned symbols), so that
A â†”A âˆ—
(1)
is provable in the extended theory.
There is one potential worry about the presence of nonlogical schemata â€“
such as the induction axiom schema of PA â€“ that we want to address:
First off, while logical axioms are â€œgoodâ€ over any ï¬rst order language â€“ they
are â€œuniversalâ€ â€“ nonlogical axioms and schemata on the other hand are speciï¬c
to a theory and its basic language, i.e., the language that existed prior to any
extensions by deï¬nitions. Thus, the induction schema is an â€œagentâ€ that yields a
speciï¬c nonlogical axiom (an instance of the schema) for each speciï¬c formula,
over the basic language LN, that we care to substitute into the metavariable A.
There is no a priori promise that the schema â€œworksâ€ whenever we replace
the syntactic variable A by a formula, say â€œB â€, over a language extension
obtained by deï¬nitions. By â€œworksâ€, of course, we mean that the produced
schema instance is a theorem in the extended theory.
Well, does it â€œworkâ€? Indeed it does; for let us look at the formula
B [x â†0] âˆ§(âˆ€x)(B â†’B [x â†Sx]) â†’B
(2)
where the particular formula B may contain deï¬ned symbols. Following the
technique of symbol elimination (cf. Remark I.7.4(a), p. 120) â€“ eliminating at
the atomic formula level â€“ we obtain the following version of (2), in the basic
language of PA. This translated version has exactly the same form as (2) (i.e.,
Ind), namely
B âˆ—[x â†0] âˆ§(âˆ€x)(B âˆ—â†’B âˆ—[x â†Sx]) â†’B âˆ—
Thus â€“ being a schema instance over the basic language â€“ it is an axiom of PA,
and hence also of its extension (by deï¬nitions). Now, by (1), the equivalence

208
II. The Second Incompleteness Theorem
theorem (Leibniz rule I.4.25) yields the following theorem of the extended
theory:
(B [x â†0] âˆ§(âˆ€x)(B â†’B [x â†Sx]) â†’B )
â†”
(B âˆ—[x â†0] âˆ§(âˆ€x)(B âˆ—â†’B âˆ—[x â†Sx]) â†’B âˆ—)
Hence (2) is a theorem of the extended theory.
â–¡
II.1.3 Remark (The Induction â€œRuleâ€). In practice, instead of Ind, we usually
employ the (derived) rule of inference Indâ€² that we obtain from Ind by invok-
ing modus ponens and the duo I.4.7â€“I.4.8:â€ 
A[x â†0], A â†’A[x â†Sx] âŠ¢A
(Indâ€²)
The rule is normally applied as follows: We ascertain that the premises apply
by
(1) proving A[0] (this part is called the basis of the induction, just as in the
informal case over N), and
(2) adding the induction hypothesis (I.H.) A to the axioms, treating the free
variables of A as new constantsâ€¡ until we can prove A[Sx].
We then have a proof of A â†’A[Sx] by the deduction theorem.
Indâ€² now allows us to conclude that A has been proved by induction on x.
What is interesting is that Indâ€² implies Ind; thus the two are equivalent.
What makes this interesting is that while the deduction theorem readily yields
Ind from Indâ€², it does so under the restriction that the free variables in A[0]
and (âˆ€x)(A â†’A[Sx]) must be treated as new constants (be â€œfrozenâ€). We
can do without the deduction theorem and without the restriction.
Let us then ï¬x a formula A and prove Ind assuming Indâ€² (see, e.g.,
Shoenï¬eld (1967)).
We letÂ§
B â‰¡A[0] âˆ§(âˆ€x)(A â†’A[Sx]) â†’A
To prove B , using Indâ€², we need to prove
B [0]
(1)
â€  Throughout this chapter, the symbol âŠ¢implies a subscript, PA, or some extension thereof, unless
something else is clear from the context.
â€¡ That is, disallowing universal quantiï¬cation over, or substitution in the variables. Of course,
existential quantiï¬cation is always possible by Ax2.
Â§ Here â€œâ‰¡â€ means equality of strings; cf. I.1.4.

II.1. Peano Arithmetic
209
and
B â†’B [Sx]
(2)
Now, (1) is a tautology, while (2) is tautologically equivalent to
(A[0] âˆ§(âˆ€x)(A â†’A[Sx]) âˆ§Â¬A) âˆ¨
Â¬A[0] âˆ¨Â¬(âˆ€x)(A â†’A[Sx]) âˆ¨A[Sx]
(3)
In turn, (3) â€“ after distributing âˆ¨over âˆ§â€“ is seen to be tautologically equivalent
to
A[0] â†’(âˆ€x)(A â†’A[Sx]) â†’A â†’A[Sx]
which is provable by tautological implication and specialization (I.4.7).
â–¡
For the next little while we will be rediscovering arithmetic in the formal
setting of PA. In the process, new predicate and function symbols will be
introduced to the language (with their attendant axioms â€“ as in Section I.7).
II.1.4 Deï¬nition. We introduce the predicate â‰¤by x â‰¤y â†”x < y âˆ¨x = y.
â–¡
For the balance of the section âŠ¢means âŠ¢PA unless noted otherwise.
Of course, in the those instances where we add axioms (in order to argue by
the deduction theorem, or by auxiliary constant, or by cases, etc.) by a sentence
such as â€œLet A . . . â€ or â€œAdd A . . . â€, â€œâŠ¢â€ will mean provability in the aug-
mented theory PA + A.
II.1.5 Lemma. âŠ¢0 â‰¤x.
Proof. We use induction on x.â€  For convenience, we let A â‰¡0 â‰¤x.
Basis.
âŠ¢A[0], since 0 = 0 |=Taut A[0] by II.1.4.
I.H.
Add A.â€¡
Now, A[Sx] â‰¡0 < Sx âˆ¨0 = Sx; thus, by < 2 (p. 175) and the Leibniz
rule (I.4.25, p. 51), âŠ¢A[Sx] â†”A âˆ¨0 = Sx. We are done by |=Taut and I.H. â–¡
II.1.6 Lemma (Transitivity of <). âŠ¢x < y âˆ§y < z â†’x < z.
â€  This, usually, means that we are using the induction rule, Indâ€², rather than the induction schema
itself.
â€¡ Instead of â€œaddâ€, we often say â€œletâ€. Either means that we are about to invoke the deduction
theorem, and we are adding a new nonlogical axiom, with all its free variables â€œfrozenâ€.

210
II. The Second Incompleteness Theorem
Proof. Induction on z. Letâ€  A[z] â‰¡x < y âˆ§y < z â†’x < z.
Basis.
âŠ¢A[0] from < 1 and |=Taut.
I.H.
Add A.
Add x < y âˆ§y < Sz (to prove x < Sz). This yields (by < 2, the Leibniz
rule, and |=Taut)
x < y âˆ§y < z âˆ¨x < y âˆ§y = z
(1)
We also have (the I.H.) A:
x < y âˆ§y < z â†’x < z
and (Leibniz equality axiom)
x < y âˆ§y = z â†’x < z
Thus, using |=Taut, x < z follows from (1). This last conclusion, by |=Taut and
< 2, yields x < Sz.
â–¡
II.1.7 Corollary (Irreï¬‚exivity of <). âŠ¢Â¬x < x.
Proof. Induction on x.
Basis.
By < 1.
I.H.
Add Â¬x < x.
We want to deduce Â¬Sx < Sx. Arguing by contradiction, we add Sx < Sx,
that is, via < 2,
âŠ¢Sx < x âˆ¨Sx = x
(1)
Now, âŠ¢x < Sx by < 2, the axiom âŠ¢x = x, and |=Taut. Thus, using (1),
âŠ¢x < Sx âˆ§(Sx < x âˆ¨Sx = x)
which yields x < x by II.1.6 and the Leibniz equality axiom (Ax4 of ),
contradicting the I.H.
â–¡
Intuitively, â€œ<â€ is a (strict) order (irreï¬‚exive and transitive). Thus the induction
axiom has a net contribution to ROB (cf. Exercise I.108).
II.1.8 Lemma (Antisymmetry of â‰¤). âŠ¢x â‰¤y âˆ§y â‰¤x â†’x = y.
Proof. Assume the hypothesis
x â‰¤y âˆ§y â‰¤x
â€  That is, â€œzâ€ is our favourite free variable in the formula â€“ see I.1.11, p. 18.

II.1. Peano Arithmetic
211
By II.1.4 this is tautologically equivalent to
Â¬(x < y âˆ§y < x) â†’Â¬(x < y âˆ§x = y)
â†’Â¬(y < x âˆ§x = y) â†’x = y
Since âŠ¢Â¬(x < y âˆ§y < x) by transitivity and irreï¬‚exivity (e.g., use proof by
contradiction for this sub-claim) while each of
Â¬(x < y âˆ§x = y)
and
Â¬(y < x âˆ§x = y)
are theorems by the Leibniz axiom and irreï¬‚exivity, x = y follows by modus
ponens.
â–¡
II.1.9 Lemma. âŠ¢x < y â†’Sx < Sy.
Proof. Induction on y.
Basis.
âŠ¢x < 0 â†’Sx < S0, by < 1 and |=Taut.
I.H.
Add x < y â†’Sx < Sy.
Add x < Sy towards proving Sx < SSy.
Hence âŠ¢x < y âˆ¨x = y, by < 2. By I.H. and the Leibniz equality axiom,
âŠ¢Sx < Sy âˆ¨Sx = Sy
Therefore âŠ¢Sx < SSy by < 2.
â–¡
II.1.10 Corollary. âŠ¢x < y â†”Sx â‰¤y.
Proof. â†’: âŠ¢Sx â‰¤y â†”Sx < Sy, by < 2.
â†: By âŠ¢x < Sx and transitivity.
â–¡
II.1.11 Proposition. Axiom < 3 is redundant in the presence of the induction
axiom.
Proof. Exercise II.1.
â–¡
We give notice that in what follows we will often use x > y to mean y < x.
We now state without proof some â€œstandardâ€ properties of + and Ã—. The
usual proof tool here will be induction. The lemma below will need a double
induction, that is, on both x and y.â€ 
â€  Actually, if you prove associativity ï¬rst and also the theorem S0 + x = Sx, then you can prove
commutativity with a single induction, after you have proved 0 + x = x.

212
II. The Second Incompleteness Theorem
II.1.12 Lemma. âŠ¢x + y = y + x.
Proof. Exercise II.3.
â–¡
II.1.13 Lemma. âŠ¢x + (y + z) = (x + y) + z.
Proof. Exercise II.4.
â–¡
II.1.14 Lemma. âŠ¢x Ã— y = y Ã— x.
Proof. Exercise II.5.
â–¡
II.1.15 Lemma. âŠ¢x Ã— (y Ã— z) = (x Ã— y) Ã— z.
Proof. Exercise II.6.
â–¡
II.1.16 Lemma. âŠ¢x Ã— (y + z) = (x Ã— y) + (x Ã— z).
Proof. Exercise II.7.
â–¡
We adopt the usual priorities of arithmetic operations; thus, instead of
x Ã— (y + z) = (x Ã— y) + (x Ã— z)
we will normally use the argot
x Ã— (y + z) = x Ã— y + x Ã— z
We will adopt one more useful abbreviation (argot): From now on we will
write xy for x Ã— y (implied multiplication notation). Moreover we will often
take advantage of properties such as commutativity without notice, for example,
writing x + y for y + x.
II.1.17 Lemma. âŠ¢x < y â†’x + z < y + z.
Proof. Induction on z.
Basis.
From +1.
I.H.
Add x < y â†’x + z < y + z.
Let x < y. By I.H., âŠ¢x + z < y + z. By II.1.9, âŠ¢S(x + z) < S(y + z).
By +2, âŠ¢x + Sz < y + Sz.
â–¡

II.1. Peano Arithmetic
213
II.1.18 Corollary. âŠ¢x + z < y + z â†’x < y.
Proof. Let x + z < y + z. Add Â¬x < y, which, by < 3, tautologically im-
plies
y < x âˆ¨y = x
(1)
Since âŠ¢y < x â†’y + z < x + z (II.1.17) and âŠ¢y = x â†’y + z = x + z
(Leibniz axiom), |=Taut and (1) yield
âŠ¢y + z â‰¤x + z
Along with the original assumption and transitivity, we have just contradicted
irreï¬‚exivity.
â–¡
II.1.19 Corollary. âŠ¢z > 0 â†’Sx â‰¤x + z.
Proof. We have âŠ¢0 < z â†’0 + x < z + x. Commutativity of +, and +1,
lead to the claim.
â–¡
II.1.20 Lemma. âŠ¢z > 0 âˆ§x < y â†’xz < yz.
Proof. Induction on z.
Basis.
By < 1.
I.H.
Add z > 0 âˆ§x < y â†’xz < yz.
Let now
x < y
(1)
As âŠ¢0 < Sz anyhow (< 2 and II.1.5), we embark on proving
xSz < ySz
By Ã—2 (and the Leibniz axiom, twice) the above is provably equivalent to
x + xz < y + yz
(2)
so we will just prove (2).
By II.1.5 there are two cases for z.â€ 
Case z = 0.
Thus, by the Leibniz axiom and Ã—1,
âŠ¢xz = 0
and
âŠ¢yz = 0
â€  Cf. p. 51.

214
II. The Second Incompleteness Theorem
By (1), II.1.17, and the Leibniz axiom,â€ 
âŠ¢x + xz < y + yz
Case z > 0.
First off, by I.H. and II.1.17,
âŠ¢y + xz < y + yz
Also, by II.1.17,
âŠ¢x + xz < y + xz
These last two and transitivity (II.1.6) yield
âŠ¢x + xz < y + yz
â–¡
II.1.21 Corollary (Cancellation Laws).
âŠ¢x + z = y + z â†’x = y
âŠ¢z > 0 âˆ§xz = yz â†’x = y
Proof. By < 3, II.1.7, II.1.17, and II.1.20.
â–¡
II.1.22 Corollary.
âŠ¢x < y âˆ§z < w â†’x + z < y + w
âŠ¢x < y âˆ§z < w â†’xz < yw
Proof. By II.1.6, II.1.17, and II.1.20.
â–¡
II.1.23 Theorem (Course-of-Values Induction). For any formula A, the fol-
lowing is provable in PA:
(âˆ€x)((âˆ€z < x)A[z] â†’A) â†’(âˆ€x)A
(1)
It goes without saying that z is a new variable. Such annoying (for being ob-
vious) qualiï¬cations we omit as a matter of policy.
In practice, the schema (1) is employed in conjunction with the deduction
theorem, as follows: One proves A with frozen free variables, on the I.H. that
â€  This â€œ âŠ¢â€ is effected in the extension of PA that contains the I.H. and also x < y and z = 0.

II.1. Peano Arithmetic
215
â€œ(âˆ€z < x)A[z] is trueâ€ (i.e., with the help of the new axiom (âˆ€z < x)A[z]).
This is all one has to do, since then (âˆ€z < x)A[z] â†’A (deduction theorem)
and hence (generalization) (âˆ€x)((âˆ€z < x)A[z] â†’A).
By (1), (âˆ€x)A now follows.
Proof. To prove (1), we let the name B (or B [x], since we are interested in
x) stand for (âˆ€z < x)A[z], that is,
B â‰¡(âˆ€z)(z < x â†’A[z])
Proving (1) via the deduction theorem dictates that we next assume (1)â€™s
hypothesis, that is, we add the axiom
(âˆ€x)(B â†’A)[âƒ—c ]
where âƒ—c are new distinct constants, substituted into all the free variables of
(âˆ€x)(B â†’A). The above yields
âŠ¢B [x,âƒ—c ] â†’A[x,âƒ—c ]
(2)
Our next (subsidiary) task is to establish
âŠ¢(âˆ€x)B [x,âƒ—c ]
(3)
by induction on x.
Basis.
âŠ¢B [0,âƒ—c ] by |=Taut and < 1.
I.H.
Add B [x,âƒ—c ], with frozen x, of course.â€  By (2),â€¡
âŠ¢A[x,âƒ—c ]
(4)
Now, B [Sx,âƒ—c ] â‰¡(âˆ€z < Sx)A[z,âƒ—c ]; thus
B [Sx,âƒ—c ]
â†”
)
by the Leibniz rule, |=Taut and < 2 â€“ âˆ€distributes over âˆ§(Exercise I.23)
*
(âˆ€z)(z < x â†’A[z,âƒ—c ]) âˆ§(âˆ€z)(z = x â†’A[z,âƒ—c ])
â†”
)
I.7.2,
*
B [x,âƒ—c ] âˆ§A[x,âƒ—c ]
â€  A constant. But we will not bother to name it anything like â€œcâ€²â€.
â€¡ x is still frozen.

216
II. The Second Incompleteness Theorem
Pause. We applied the style of â€œequational proofsâ€ or â€œcalculational proofsâ€ â€ 
immediately above (chain of equivalences). The â€œâ†”â€ is used conjunctionally,
that is, â€œâŠ¢D 1 â†”D2 â†”. . . â†”Dnâ€ means â€œâŠ¢(D1 â†”D2) âˆ§Â· Â· Â· âˆ§(Dnâˆ’1 â†”
Dn)â€; hence, by tautological implication, âŠ¢D1 â†”Dn.
By (4) and the I.H. we have proved B [Sx,âƒ—c ]. Hence our induction has
concluded: We now have (3).
By(2),(3),âˆ€-monotonicity(I.4.24),andmodusponensweinfer(âˆ€x)A[x,âƒ—c ];
hence, by the deduction theorem,
(âˆ€x)(B â†’A)[âƒ—c ] â†’(âˆ€x)A[x,âƒ—c ]
Applying the theorem on constants, we get (1).
â–¡
We have applied quite a bit of pedantry above during the application of the
deduction theorem, invoking the theorem on constants explicitly. This made it
easier to keep track of which variables were frozen, and when.
II.1.24 Corollary (The Least Principle). The following is provable in PA for
any A:
(âˆƒx)A â†’(âˆƒx)(A âˆ§(âˆ€z < x) Â¬ A[z])
(LP)
Proof. The course-of-values induction schema applied to Â¬A is provably
equivalent to (LP) above.
â–¡
We now have enough tools to formalize unbounded search, (Âµy), in PA (cf.
I.8.8, p. 127).
Suppose that we haveâ€¡
âŠ¢(âˆƒx)A
(E)
Informally this says that for all values of the free variables there is a (corre-
sponding value) x that makes A true. In view of the least principle, we must
then be able to deï¬ne a â€œtotal function on the natural numbersâ€ which, for each
input, returns the smallest x that â€œworksâ€. Formally this â€œfunctionâ€ will be a
function letter â€“ introduced into the theory (PA) by an appropriate deï¬nition
â€  Cf. Dijkstra and Scholten (1990), Gries and Scheider (1994), Tourlakis (2000a, 2000b. 2001b).
â€¡ Reminder: We continue using â€œâŠ¢â€ for â€œâŠ¢T â€, where T is PA, possibly extended by deï¬nitions.

II.1. Peano Arithmetic
217
(Section I.7) â€“ whose natural interpretation in N will be the total function we
have just described. The formal details are as follows:
Let B stand for
A âˆ§(âˆ€z < x) Â¬ A[z]
(B)
By the least principle and (E) (existence condition),
âŠ¢(âˆƒx) B
(1)
Pause. By âˆƒ-monotonicity (I.4.23), (1) implies (E), since âŠ¢B â†’A. Thus
(1) and (E) are provably equivalent in PA, by II.1.24.
We next show that the â€œâˆƒâ€ in (1) is really â€œâˆƒ!â€. To this end, we prove
B [x] âˆ§B [y] â†’x = y
(2)
Add B [x] and B [y] (with frozen free variables), that is, add
A[x] âˆ§(âˆ€z < x) Â¬ A[z]
and
A[y] âˆ§(âˆ€z < y) Â¬ A[z]
These entail
A[x]
(3)
z < x â†’Â¬A[z]
(4)
A[y]
(5)
z < y â†’Â¬A[z]
(6)
We will now show that adding
x < y âˆ¨y < x
(7)
will lead to a contradiction, therefore establishing (by < 3)
x = y
Having added (7) as an axiom, we now have two cases to consider:
Case x < y.
Then (6) yields Â¬A[x] contradicting (3).
Case y < x.
Then (4) yields Â¬A[y], contradicting (5).
We have established (2).
We can now introduce a new function symbol, f , in LA by the axiom
fâƒ—y = x â†”B
( f)
where the list âƒ—y, x contains all the free variables of B â€“ and perhaps others.

218
II. The Second Incompleteness Theorem
Pause. Is â€œand perhaps othersâ€ right? Should it not be â€œexactly the free vari-
ables of B â€?
The â€œenabling necessary conditionâ€ for ( f ) is, of course, (1) â€“ or, equiv-
alently, (E) â€“ above. We will always speak of (E) as the enabling existence
condition.
The following alternative notation to ( f ) above is due to Hilbert and Bernays
(1968) and better captures the intuitive meaning of the whole process we have
just described:
fâƒ—y = (Âµx)A
( f â€²)
or, using the Whitehead-Russell â€œÎ¹â€,
(Âµx)A
def= (Î¹x)B
( f â€²â€²)
Thus, we can always introduce a new function symbol f in PA by the explicit
deï¬nition ( f â€²) as long as we can prove (E) above.
Note that, by I.7.1 and I.7.3, such extensions of PA are conservative.
We can also say that we have introduced a Âµ-term, (Âµx)A, if we want to
suppress the details of introducing a new function letter, etc.
Axiom ( f ) yields at onceâ€ 
âŠ¢B [ fâƒ—y]
( f (3))
therefore, by (B), < 3, and |=Taut,
âŠ¢A[ fâƒ—y]
( f (4))
and
âŠ¢A[z] â†’fâƒ—y â‰¤z
( f (5))
Here â€œÂµâ€ is the formal counterpart of the â€œÂµâ€ (unbounded search) of non-
formalized recursion theory (I.8.8). We have restricted its application in the
formal theory so that functions deï¬ned as in ( f â€²) are total (due to (E)).
We will also need to formalize primitive recursion, that is, to show that given
any function symbols g and h of LA, of arities n + 2 and n respectively, we
can introduce a new function symbol f of arity n + 1 satisfying the (deï¬ning)
axioms
f 0âƒ—yn = hâƒ—yn
f Sxâƒ—yn = gxâƒ—yn f xâƒ—yn
â€  Where the machinery for â€œâŠ¢â€ includes the deï¬ning axiom ( f ).

II.1. Peano Arithmetic
219
Needless to stress that x,âƒ—yn are free variables. Note that the pair of equations
above generalizes the manner in which + and Ã— were introduced as primeval
symbols in ROB and PA.
More â€œuser-friendlyâ€ (argot) notation for the above recurrence equations is
f (0,âƒ—yn) = h(âƒ—yn)
f (Sx,âƒ—yn) = g(x,âƒ—yn, f (x,âƒ—yn))
To be able to handle primitive recursion we need to strengthen our grasp of
â€œarithmeticâ€ in PA, by developing a few more tools.
First off, for any term t we may introduce a new function symbol f by a
â€œdeï¬nitionâ€ (axiom)
fâƒ—y = t
(8)
where âƒ—y contains all the free variables of t (but may contain additional free
variables).
(8) is our preferred short notation for introducing f . The long notation is by
quoting
âŠ¢(âˆƒx)x = t
(8â€²)
andâ€ 
fâƒ—y = (Âµx)x = t
(8â€²â€²)
Of course, the enabling condition (8â€²) is satisï¬ed, by logical axiom Ax2.
We next introduce the (formal) characteristic function of a formula, a formal
counterpart of the characteristic function of a relation (I.8.16).
Let A be any formula, and âƒ—yn the list of its free variables. We introduce a
new n-ary function symbol Ï‡A by the explicit deï¬nition
Ï‡A âƒ—yn = (Âµx)(A âˆ§x = 0 âˆ¨Â¬A âˆ§x =1)
(C)
As always, we must be satisï¬ed that the enabling condition
âŠ¢(âˆƒx)(A âˆ§x = 0 âˆ¨Â¬A âˆ§x =1)
(Câ€²)
holds. Well, since âŠ¢A âˆ¨Â¬A, we may use proof by cases.
Case A.â€¡
Now, âŠ¢A âˆ§0 = 0 âˆ¨Â¬A âˆ§0 =1, thus (Câ€²) follows.
Case Â¬A.
This time âŠ¢A âˆ§1 = 0 âˆ¨Â¬A âˆ§1 =1, thus (Câ€²) follows once
more.
â€  I shall eventually stop issuing annoyingly obvious reminders such as: â€œx is not, of course, chosen
among the variables of tâ€.
â€¡ Cf. p. 51.

220
II. The Second Incompleteness Theorem
II.1.25 Remark (Disclaimer). â€œ[T]he (formal) characteristic function of a for-
mulaâ€ (emphasis added) above are strong words indeed. Once a Ï‡A has been
introduced as above, one may subsequently introduce a new function symbol
f by the explicit deï¬nition fâƒ—yn = Ï‡A âƒ—yn, and this too satisï¬es (C) and its
corollaries (9) and (10) below. For example, âŠ¢A â†”fâƒ—yn = 0.
Thus, â€œtheâ€ characteristic function symbol is in fact not unique.â€  Never-
theless, we may occasionally allow the tongue to slip. The reader is hereby
forewarned.
â–¡
II.1.26 Remark (About â€œ âŠ¢â€, Again: Recursive Extensions of PA). From
now on, proofs take place in an (unspeciï¬ed) extension of PA effected by a
ï¬nite sequence of Âµ-deï¬nitions orâ€¡ deï¬nitions of new predicate symbols. To
be exact, we work in a theory PAâ€² deï¬ned as follows: There is a sequence of
theories Ti, for i = 0, . . . , n each over a language LAi, such that
(i) LA0 = LN, T0 = PA, Tn = PAâ€², and
(ii) for each i = 0, . . . , n âˆ’1, Ti+1 is obtained from Ti by
(a) adding a single new function symbol f to LAi, to obtain LAi+1, and
adding the axiom
f (âƒ—y) = (Âµx)A
(F)
to Ti, having shown ï¬rst that âŠ¢Ti (âˆƒx)A, or
(b) adding a single new n-ary predicate symbol P to LAi, to obtain LAi+1,
and adding the axiomÂ§
Pâƒ—xn â†”A(âƒ—xn)
(P)
to Ti.
We will restrict from now on the form of function and predicate deï¬ni-
tions (F) and (P) above, so that in each case the formula A is in âˆ†0(LA)
(see I.9.27), where LA is the language to which the new symbol is being
added.
Under the above restriction on A, we call any extension PAâ€² of PA, as
described in (i)â€“(ii) above, a recursive extension (Shoenï¬eld (1967),
Schwichtenberg (1978)).Â¶
â–¡
â€  Its interpretation, or â€œextensionâ€, in the standard structure is unique, of course.
â€¡ Inclusively speaking.
Â§ Such a deï¬nition effected the introduction of â€œâ‰¤â€ in II.1.4.
Â¶ Actually, those authors require A to be an open formula. For the purposes of Theorem II.4.12
later on, the two formulations make no difference.

II.1. Peano Arithmetic
221
One can now show that
âŠ¢A â†”Ï‡A âƒ—yn = 0
(9)
and
âŠ¢Â¬A â†”Ï‡A âƒ—yn =1
(10)
For (9), add A,â€  and prove
Ï‡A âƒ—yn = 0
(11)
We have (cf. ( f (4)), p. 218)
âŠ¢A âˆ§Ï‡A âƒ—yn = 0 âˆ¨Â¬A âˆ§Ï‡A âƒ—yn =1
Since
A, A âˆ§Ï‡A âƒ—yn = 0 âˆ¨Â¬A âˆ§Ï‡A âƒ—yn =1 |=Taut Ï‡A âƒ—yn = 0
(11) follows. In short, we have the â†’-direction of (9). Similarly, one obtains
the â†’-direction of (10).
A by-product of all this, in view of tautological implication and âŠ¢A âˆ¨Â¬A,
is
âŠ¢Ï‡A âƒ—yn = 0 âˆ¨Ï‡A âƒ—yn =1
The latter yields the â†-directions of (9) and (10) via proof by cases: Say,
we work under the case Ï‡A âƒ—yn = 0, with frozen âƒ—yn. Add Â¬A. By the â†’-half
of (10),
âŠ¢Ï‡A âƒ—yn =1
hence, using the assumption, we obtain âŠ¢0 =1 which contradicts axiom S1.
Thus âŠ¢A, and hence the â†-half of (9) is proved. One handles (10) similarly.
Deï¬nition by cases. We want to legitimize deï¬nitions of new function symbols
f such as
fâƒ—xn =
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³
t1
if A1
...
...
tk
if Ak
(12)
where
âŠ¢A1 âˆ¨Â· Â· Â· âˆ¨Ak
(13)
âŠ¢Â¬(Ai âˆ§A j)
for all i Ì¸= j
(14)
â€  With frozen âƒ—yn, of course.

222
II. The Second Incompleteness Theorem
and âƒ—xn is the list of all free variables in the right hand side of (12). Our under-
standing of the informal notation (12) is that
âŠ¢Ai â†’fâƒ—xn = ti
(15)
holds for i = 1, . . . , k. We (formally) achieve this as follows:
Let Ï‡iâƒ—xn be theâ€  characteristic term of A1 âˆ¨Â· Â· Â· âˆ¨
Ai âˆ¨Â· Â· Â· âˆ¨Ak, where

Ai means that Ai is missing from the disjunction. Then the deï¬nition below,
in the style of (8) (p. 219), is what we want (cf. I.8.27):
fâƒ—xn = t1Ï‡1âƒ—xn + Â· Â· Â· + tkÏ‡kâƒ—xn
(16)
Indeed, âŠ¢Ai â†’Â¬(A1 âˆ¨Â· Â· Â· âˆ¨
Ai âˆ¨Â· Â· Â· âˆ¨Ak), by (14) and |=Taut; hence
âŠ¢Ai â†’Ï‡iâƒ—xn =1
(17)
On the other hand,â€¡
âŠ¢Ai â†’Ï‡ jâƒ—xn = 0
for j Ì¸= i
(18)
Elementary propertiesÂ§ of + and Ã— now yield (15).
We next deï¬ne the formal proper subtraction function(-symbol), Î´. Infor-
mally,Â¶ Î´(x, y) stands for x
.âˆ’y. In fact, its natural interpretation â€“ as it is sug-
gested by the right hand side of (19) below â€“ in Nâ€² = (N; 0; S, +, Ã—; <; . . . ) is
x
.âˆ’y.
We set, exactly as in informal recursion theory
Î´(x, y) = (Âµz)(x = y + z âˆ¨x < y)
(19)
To allow (19) stand, one must show that
âŠ¢(âˆƒz)(x = y + z âˆ¨x < y)
(20)
By < 3, one applies proof by cases. If x < y is the hypothesis, then, by Ax2
and modus ponens, (20) holds.
â€  For the use of the deï¬nite article here â€“ and in similar contexts in the future â€“ see the dis-
claimer II.1.25, p. 220
â€¡ Since âŠ¢Ai â†’A1 âˆ¨Â· Â· Â· âˆ¨
A j âˆ¨Â· Â· Â· âˆ¨Ak for j Ì¸= i.
Â§ +1, Ã—1 and Ã—2, associativity, commutativity.
Â¶ When it comes to formal counterparts of known number-theoretic functions we will abuse the
formal notation a bit, opting for argot in the interest of readability. Here we wrote â€œÎ´(x, y)â€ rather
than the nondescript â€œ f 2
i xyâ€, where f 2
i is the ï¬rst unused (so far) function symbol of arity 2.
See also p. 166.

II.1. Peano Arithmetic
223
For the remaining case we do induction on x to prove
âŠ¢x â‰¥y â†’(âˆƒz)x = y + z
(21)
Pause. What happened to â€œ âˆ¨x < yâ€?
Basis.
For x = 0, if y = 0 (cases!), then z = 0 works (via Ax2) by +1.
Otherwise (y Ì¸= 0), and we are done by < 1.
For the induction step we want
âŠ¢Sx â‰¥y â†’(âˆƒz)Sx = y + z
(21â€²)
Let Sx â‰¥y. This entails two cases:
If Sx = y, then z = 0 works.
Finally, say Sx > y. By < 2, we have x â‰¥y. By I.H.,
âŠ¢(âˆƒz)x = y + z
(21â€²â€²)
Add x = y + a to the hypotheses, where a is a new constant.â€  By +2, we have
âŠ¢Sx = y + Sa; hence âŠ¢(âˆƒz)Sx = y + z, and we are done proving (20).
We have at once
II.1.27 Lemma. âŠ¢x < y â†’Î´(x, y) = 0, and âŠ¢x â‰¥y â†’x = y + Î´(x, y).
Also,
II.1.28 Lemma. For any z, âŠ¢zÎ´(x, y) = Î´(zx, zy).
Proof. If x < y, then zx < zy, and hence âŠ¢Î´(x, y) = 0 and âŠ¢Î´(zx, zy) = 0.
If x â‰¥y, then zx â‰¥zy using II.1.20; hence (II.1.27)
âŠ¢x = y + Î´(x, y)
(i)
and
âŠ¢zx = zy + Î´(zx, zy)
(ii)
(i) yields
âŠ¢zx = zy + zÎ´(x, y)
using distribution of Ã— over + (II.1.16), and therefore we are done by (ii)
and II.1.21.
â–¡
â€  Or, more colloquially, â€œlet z = a work in (21â€²â€²)â€. We are using proof by auxiliary constant.

224
II. The Second Incompleteness Theorem
Now that we have mastered addition, subtraction, and multiplication, we
turn our attention to division. First we formalize remainders and quotients.
We start from the trivial observation
âŠ¢(âˆƒz)(âˆƒw)x = yw + z
(1)
(1) follows from âŠ¢x = y0 + x (+1 and Ã—1).
Thus we may introduce a new function (of arity 2), the remainder function
â€œrâ€, by the following Âµ-deï¬nition:â€ 
r(x, y) = (Âµz)(âˆƒw)x = yw + z
(rem)
We have at once (( f (4)), p. 218)
âŠ¢(âˆƒw)x = yw+r(x, y)
(EQ)
Therefore we can introduce quotients, q(x, y), via the deï¬nition of the 2-ary
function symbol â€œqâ€ below:
q(x, y) = (Âµw)x = yw+r(x, y)
(Q)
To improve readability we will usually denote the term q(x, y) by âŒŠx/yâŒ‹or
#x
y
$
(note the boldface variables x and y, which distinguish the formal case from
the informal one), and we will hardly need to refer to the symbol â€œqâ€ again.
One more application of ( f (4)) â€“ to (Q) â€“ yields
âŠ¢x = y
#x
y
$
+r(x, y)
(Euc)
The enabling condition (EQ) for (Q) yields
âŠ¢(âˆƒw)x = 0 Â· w+r(x, 0)
Hence the interesting
âŠ¢x = r(x, 0)
Since âŠ¢x = 0 Â· 0 + r(x, 0), (Q) and ( f (5)) (p. 218) yield âŒŠx/0âŒ‹â‰¤0. Hence,
by II.1.5,
âŠ¢
,x
0
-
= 0
â€  We write â€œr(x, y)â€ rather than â€œrxyâ€ in the interest of user-friendliness of notation.

II.1. Peano Arithmetic
225
Adding the usual assumption y > 0, we can guarantee the â€œstandardâ€ prop-
erties of quotient and remainder, namely the inequalityâ€  r(x, y) < y and the
uniqueness of quotient and remainder.
To see this, add y â‰¤r(x, y). Thus (II.1.27)
âŠ¢r(x, y) = y + t
(2)
where we have set t = Î´(r(x, y), y) for convenience. By (Euc) and Ã—2 (invok-
ing associativity tacitly, as is usual)
âŠ¢x = yS
#x
y
$
+ t
Hence,
âŠ¢(âˆƒw)x = yw+ t
from which, along with (rem) and ( f (5)) (p. 218),
âŠ¢r(x, y) â‰¤t
Since also âŠ¢r(x, y) â‰¥t by (2) and II.1.17, we get âŠ¢r(x, y) = t (II.1.8); hence
âŠ¢y = 0 from (2) and cancellation (II.1.21). By the deduction theorem we have
derived
âŠ¢y â‰¤r(x, y) â†’y = 0
or, better still (via < 3, irreï¬‚exivity, and II.1.5), the contrapositive
âŠ¢y > 0 â†’r(x, y) < y
We next turn to uniqueness. Letâ€¡ then
z < y âˆ§(âˆƒw)x = yw + z
(3)
(3) derives y > 0 by II.1.5 and II.1.6.
By ( f (5)) and (rem), âŠ¢r(x, y) â‰¤z. Since we want â€œ=â€ here, we add
r(x, y) < z
(4)
to obtain a contradiction. Reasoning by auxiliary constant, we also add
x = yq + z
(5)
â€  The â€œ0 â‰¤r(x, y)â€ part is trivial from âŠ¢0 â‰¤x (II.1.5) and substitution.
â€¡ Cf. p. 209 for our frequent use of the argot â€œletâ€.

226
II. The Second Incompleteness Theorem
for some new constant q. Let now s = Î´(z,r(x, y)). By (4) and II.1.27,
âŠ¢z = r(x, y) + s âˆ§s > 0
(6)
Thus (5) and (Euc) yield (via II.1.21) âŠ¢y âŒŠx/yâŒ‹= yq + s; hence (deï¬nition
of Î´ and II.1.28)
âŠ¢yÎ´
#x
y
$
, q

= s
We conclude that âŠ¢y â‰¤s (why?), which, along with âŠ¢z < y, from (3),
and âŠ¢s â‰¤z, from (6) and II.1.17, yields (transitivity of <) the contradiction
âŠ¢y < y. Thus,
âŠ¢z < y âˆ§(âˆƒw)x = yw + z â†’z = r(x, y)
It is a trivial matter (not pursued here) now to obtain
âŠ¢z < y âˆ§x = yw + z â†’w =
#x
y
$
âˆ§z = r(x, y)
(UN)
In (rem) (p. 224) the formula to the right of (Âµz) is provably equivalent to the
âˆ†0-formula (âˆƒw)â‰¤xx = yw+ z (why?). Thus the latter could have been used
in lieu of the original.
Therefore the addition of r and its deï¬ning axiom to the theory results to a
recursive extension (such as we promised all our extensions by deï¬nitions to
be [p. 220]).
We next introduce the divisibility predicate â€œ|â€ by the axiom (D) below:
x|y â†”r(y, x) = 0
(D)
Once again, the use of boldface variables will signify that we are in the formal
domain.
II.1.29 Lemma. âŠ¢y|x â†”(âˆƒz)yz = x.
Proof. By tautological implication, we have two directions to deal with:
â†’:
It follows by (Euc) above, +1, and Ax2.
â†:
By the deduction theorem; assume (âˆƒz)yz = x towards proving y|x.
Thus âŠ¢(âˆƒz)yz + 0 = x by +1. By (rem) and ( f (5)), we have âŠ¢r(x, y) â‰¤0.
Done, by antisymmetry and II.1.5.
â–¡
We now deï¬ne relative primality: We say that x and y are relatively prime iff
âŠ¢(âˆ€z)(x|yz â†’x|z)
(7)

II.1. Peano Arithmetic
227
We introduce the metanotation RP(x, y) to stand for (âˆ€z)(x|yz â†’x|z). Thus,
we may write (7) as âŠ¢RP(x, y).
We emphasize that â€œRPâ€ is not introduced as a predicate into the language,â€ 
rather it is introduced as a metamathematical abbreviation â€“ that is, we write
â€œRP(x, y)â€ simply to avoid writing the formula in (7).
The technical reason is that we only introduce predicates if we are prepared
to give a âˆ†0-formula to the right of â€œâ†”â€ in their deï¬nition (since we want to
only consider recursive extensionsâ€¡ of PA).
II.1.30 Remark. Strictly speaking, RP as (informally) deï¬ned gives relative
primality correctly, as we understand it intuitively, only when both numbers are
nonzero. It has some pathologies: e.g, the counterintuitive
âŠ¢RP(0,2)
Indeed, to prove (âˆ€z)(0 |2z â†’0 | z), strip it of the quantiï¬er and assume
r(2z, 0) = 0
Hence (p. 224)
âŠ¢2z = 0
Thus (âŠ¢2 > 0, âŠ¢0z = 0, and II.1.21) âŠ¢z = 0; hence
âŠ¢0 | z
Similarly, one can prove âŠ¢x > 0 â†’RP(0, x).
Of course, in â€œrealityâ€ (i.e., informally), 0 and 2 are not relatively prime,
since their greatest common divisor is 2.
â–¡
II.1.31 Exercise. Show that âŠ¢RP(x, y) â†’x > 0 âˆ¨y > 0.
â–¡
II.1.32 Lemma. âŠ¢(âˆƒz)(âˆƒw)Î´(xz, yw) =1 â†’RP(x, y).
Proof. Assume the hypothesis
(âˆƒz)(âˆƒw)Î´(xz, yw) =1
â€  That would have been through a deï¬nition like RP(x, y) â†”(âˆ€z)(x|yz â†’x|z).
â€¡ Cf. p. 220.

228
II. The Second Incompleteness Theorem
Let Î´(xa, yb) =1, where a and b are new constants. Add also
x|yz
(8)
towards proving x|z.
Now âŠ¢zÎ´(xa, yb) = z;â€  hence
âŠ¢Î´(zxa, zyb) = z
(9)
by II.1.28. Setting c = âŒŠzy/xâŒ‹for convenience,â€¡ we get âŠ¢xc = zy from (8).
Thus, by (9), âŠ¢Î´(zxa, xcb) = z; hence (again via II.1.28)
âŠ¢xÎ´(za, cb) = z
That is, âŠ¢x|z, and an application of the deduction theorem followed by gener-
alization yields (âˆ€z)(x|yz â†’x|z), i.e., RP(x, y).
â–¡
We have already remarked that we will be using associativity and commutativity
of â€œ+â€ and â€œÃ—â€ without notice.
II.1.33 Lemma. âŠ¢x > 0 â†’RP(x, y) â†’RP(y, x).
Proof. The case y = 0 is trivial by Remark II.1.30. Thus, take now the case
y > 0.
We add the assumptions
x > 0
RP(x, y)
and
y|xz
(10)
towards proving y|z. By (10),
âŠ¢xz = ya
(11)
where a = âŒŠxz/yâŒ‹. Thus x|ya; hence x|a by RP(x, y).
Write then a = xq (q = âŒŠa/xâŒ‹), from which and (11) âŠ¢xz = yxq. Thus,
âŠ¢z = yq, by II.1.21 and our ï¬rst hypothesis, which proves y|z.
â–¡
â€  Assuming you believe that âŠ¢x = x1.
â€¡ Another aspect of convenience is to invoke commutativity or associativity of either + or Ã— tacitly.

II.1. Peano Arithmetic
229
Thus, what we have explicitly derived under the second case above (via the
deduction theorem) was
y > 0 âŠ¢x > 0 â†’RP(x, y) â†’y|xz â†’y|z
Hence we also derived (by âˆ€-introduction) what we really wanted,
y > 0 âŠ¢x > 0 â†’RP(x, y) â†’(âˆ€z)(y|xz â†’y|z)
The moral is that even â€œformalâ€, but â€œpracticalâ€, proofs often omit the obvious.
While we could just as easily have incorporated these couple of lines in the
proof above, we are going to practise this shortening of proofs again and again.
Hence the need for this comment.
II.1.34 Lemma. âŠ¢k | p â†’RP(S(ip), S((i + k)p)).
Proof. The case p = 0 being trivial â€“ âŠ¢RP(S0, S0) â€“ we argue the case p > 0.
Add k | p towards proving
RP(S(ip), S((i + k)p))
(i)
Thus,
p = ka
(ii)
where a = âŒŠp/kâŒ‹. By II.1.32
âŠ¢RP(S(ip), p)
(iii)
and
âŠ¢RP(S(ip), k)
(iv)
each because âŠ¢Î´(S(ip), ip) =1. Add S(ip) | zS((i + k)p) towards proving
âŠ¢S(ip) | z.
By hypothesis, âŠ¢S(ip) | zkp (ï¬ll in the missing steps); hence âŠ¢S(ip) | zp
by (iv). Then âŠ¢S(ip) | z by (iii).
â–¡
We now embark on introducing coding of sequences formally.â€ 
The formal counterpart of a sequence a0, a1, . . . , an of (variable) length = n+1
is a term t(n,âƒ—x),â€¡ where the parenthesis notation lists all free variables of t. We
may also simply write t[n] (see p. 18).
â€  This will be a â€œcarefulâ€ repetition of the deï¬nition of GÂ¨odelâ€™s Î²-function ((2) of p. 159).
â€¡ â€œnâ€ is here a variable, not a numeral. That is why we wrote â€œnâ€ rather than â€œnâ€.

230
II. The Second Incompleteness Theorem
We introduce the maximum of the ï¬rst n + 1 members of a sequence
max
iâ‰¤n (t(i,âƒ—x)) = (Âµz)((âˆ€i)â‰¤nz â‰¥t(i,âƒ—x ))
(M)
To legitimize (M) we need to establish
âŠ¢(âˆƒz)(âˆ€i)(i â‰¤n â†’z â‰¥t(i,âƒ—x))
(Mâ€²)
We prove (Mâ€²) by induction on n.
For n = 0 (Mâ€²) follows immediately (do [z â†t(0,âƒ—x)] and apply Ax2).
Add now (Mâ€²) for frozen n and âƒ—x, and show that
âŠ¢(âˆƒz)(âˆ€i)(i â‰¤Sn â†’z â‰¥t(i,âƒ—x))
(Mâ€²â€²)
Let a satisfy (Mâ€²) (where in the latter n and âƒ—x are still frozen).
That is, formally, add a new constant symbol a, and the new axiom
(âˆ€i)(i â‰¤n â†’a â‰¥t(i,âƒ—x))
(M(3))
It follows (specialization) that
âŠ¢i â‰¤n â†’a â‰¥t(i,âƒ—x)
(1)
Since âŠ¢a + t(Sn,âƒ—x) â‰¥a and âŠ¢a + t(Sn,âƒ—x) â‰¥t(Sn,âƒ—x), proof by cases, (1),
and âŠ¢i â‰¤Sn â†”i â‰¤n âˆ¨i = Sn yieldâ€ 
âŠ¢i â‰¤Sn â†’a + t(Sn,âƒ—x) â‰¥t(i,âƒ—x)
(2)
Hence (generalization)
âŠ¢(âˆ€i)(i â‰¤Sn â†’a + t(Sn,âƒ—x) â‰¥t(i,âƒ—x))
and via Ax2
âŠ¢(âˆƒz)(âˆ€i)(i â‰¤Sn â†’z â‰¥t(i,âƒ—x))
This is (Mâ€²â€²). Now the deduction theorem conï¬rms the induction step.
â–¡
We next introduce the least common multiple of the ï¬rst n + 1 members of
a â€œpositive sequenceâ€:
lcm
iâ‰¤n(St(i,âƒ—x)) = (Âµz)(z > 0 âˆ§(âˆ€i)â‰¤nSt(i,âƒ—x) | z)
(LCM)
To legitimize (LCM) we need to establish
âŠ¢(âˆƒz)(z > 0 âˆ§(âˆ€i)(i â‰¤n â†’St(i,âƒ—x) | z))
(LCMâ€²)
â€  With help from the logical i = Sn â†’t(i,âƒ—x ) = t(Sn,âƒ—x ).

II.1. Peano Arithmetic
231
One can easily prove (LCMâ€²) by induction on n. In outline, a z that works for
n = 0 is St(0,âƒ—x). If now a (auxiliary constant!) works for n (the latter frozen
along with âƒ—x), then aSt(Sn,âƒ—x) works for Sn. The details are left to the reader.
The positive sequence above has members St(i,âƒ—x), â€œindexedâ€ by i, where t is
some term.
This representation stems from the fact that âŠ¢t > 0 â†’t = SÎ´(t,1).
Alternatively, we could have opted to Âµ-deï¬ne lcm with a condition: that
(âˆ€i â‰¤n) t (i,âƒ—x) > 0. This approach complicates the deï¬ning formula as we try
to make the Âµ-deï¬ned object total (when interpreted in the standard structure).
Axiom (LCM) implies at once ( f (4) and f (5) of p. 218)
âŠ¢(z > 0 âˆ§(âˆ€i)â‰¤nSt(i,âƒ—x) | z) â†’lcm
iâ‰¤n(St(i,âƒ—x)) â‰¤z
(1)
âŠ¢lcm
iâ‰¤n(St(i,âƒ—x)) > 0
(2)
andâ€ 
âŠ¢(âˆ€i)â‰¤n
%
St(i,âƒ—x) | lcm
iâ‰¤n(St(i,âƒ—x))
&
(3)
(1) can be sharpened to
âŠ¢(z > 0 âˆ§(âˆ€i)â‰¤nSt(i,âƒ—x) | z) â†’lcm
iâ‰¤n(St(i,âƒ—x)) | z
(1â€²)
Indeed, assume the left hand side of â€œâ†’â€ in (1â€²) and also let
r > 0 âˆ§z = lcm
iâ‰¤n(St(i,âƒ—x))q + r
(4)
where the terms r and q are the unique (by (2)) remainder and quotient of the
division z/lcmiâ‰¤n(St(i,âƒ—x)) respectively. That is, we adopt the negation of the
right hand side of â€œâ†’â€ in (1â€²) and hope for a contradiction.
Well, by (3), (4) and our additional assumptions immediately above,
âŠ¢r > 0 âˆ§(âˆ€i)â‰¤nSt(i,âƒ—x) |r
hence
âŠ¢lcm
iâ‰¤n(St(i,âƒ—x)) â‰¤r
by (1), contradicting the remainder inequality (by (2) the divisor in (4) is posi-
tive). This establishes that r > 0 is untenable and proves (1â€²).
We now revisit Lemma I.9.6 (actually proving a bit more).
â€  The big brackets are superï¬‚uous but they improve readability.

232
II. The Second Incompleteness Theorem
II.1.35 Lemma. Let t and s be terms. Then
âŠ¢s > S0 âˆ§(âˆ€i)â‰¤nRP(s, St[i]) â†’RP(s,lcm
iâ‰¤n(St[i]))
Proof. Let
s > S0 âˆ§(âˆ€i)â‰¤nRP(s, St[i])
(5)
Let also (we are implicitly using II.1.33) lcmiâ‰¤n(St[i]) | sz. By (3) and (5)
âŠ¢(âˆ€i)â‰¤nSt[i] | z
Taking cases, if z = 0 then
âŠ¢lcm
iâ‰¤n(St[i]) | z
(6)
anyway. If z > 0 then (6) is again obtained, this time by (1â€²).
â–¡
The following is the formalized Lemma I.9.6.
II.1.36 Corollary. Let t and s be terms. Then
âŠ¢s > S0 âˆ§(âˆ€i)â‰¤nRP(s, St[i]) â†’Â¬s | lcm
iâ‰¤n(St[i])
Proof. Exercise II.11.
â–¡
II.2. A Formal Î²-Function
The following steps formalize those taken starting with p. 156. Note that c,
p, and q below are just convenient (metamathematical) abbreviations of the
respective right hand sides.
Let t(n,âƒ—x) be a term, and set
c = max
iâ‰¤n (St[i])
(C)
We next let p be the lcm of the sequence S0, . . . , Sc (informally, 1, . . . , c+1).
Thus we setâ€ 
p = lcm
iâ‰¤c (Si)
(P)
Finally, deï¬ne the term q by the explicit deï¬nitionâ€¡
q = lcm
iâ‰¤n(S(pSt[i]))
(Q)
â€  That is, p stands for s[c], where s[n] abbreviates lcmiâ‰¤n(Si), n being a variable.
â€¡ Of course, a more user-friendly way to write â€œS(pSt[i])â€ is â€œ1 + p(1 + t[i])â€.

II.2. A Formal Î²-Function
233
By (P) and (2) and (3) (p. 231) above,
âŠ¢p > 0 âˆ§(âˆ€i)â‰¤cSi | p
(Pâ€²)
We can now derive
âŠ¢y â‰¤c â†’(âˆ€i)(i â‰¤n â†’Â¬ y = t[i])
â†’(âˆ€i)(i â‰¤n â†’RP(S(pSy), S(pSt[i])))
(6)
To see why (6) holds, add the assumptions y â‰¤c and i â‰¤n â†’Â¬ y = t[i]. We
now try to prove
i â‰¤n â†’RP(S(pSy), S(pSt[i]))
So add i â‰¤n. This yields Â¬ y = t[i], which splits into two cases by < 3,
namely, y > t[i] and y < t[i].
We will consider ï¬rst the case y > t[i]. Set k = Î´(y, t[i]) for convenience.
Now âŠ¢0 < k âˆ§k â‰¤y hence also âŠ¢k â‰¤c by assumption and transitivity.
Thus âŠ¢k | p by (Pâ€²) and Exercise II.10.
Moreover (by II.1.27) âŠ¢y = t[i] + k, hence (via +2) âŠ¢Sy = St[i] + k.
Thus II.1.34 yields
âŠ¢RP(S(pSy), S(pSt[i]))
The other case, y < t[i], is handled entirely similarly with slightly different
start-up details: This time we set k = Î´(t[i], y).
Now âŠ¢0 < k âˆ§k â‰¤t[i]; hence also âŠ¢k < c.
Why? Well, âŠ¢i â‰¤n â†’St[i] â‰¤c by (C) and (M) (p. 230) via ( f (4))
(p. 218). Now the assumption i â‰¤n yields âŠ¢St[i] â‰¤c, and transitivity does
the rest.
Thus âŠ¢k | p by (Pâ€²), and one continues the proof as in the previous case:
By II.1.27 âŠ¢t[i] = y + k; hence (via +2) âŠ¢St[i] = Sy + k. Thus II.1.34
yields
âŠ¢RP(S(pSy), S(pSt[i]))
once more.
At this point we have derived (by the deduction theorem)
y â‰¤c âŠ¢
(i â‰¤n â†’Â¬ y = t[i]) â†’(i â‰¤n â†’RP(S(pSy), S(pSt[i])))
(7)
Hence, by âˆ€-monotonicity (I.4.24),
y â‰¤c âŠ¢
(âˆ€i â‰¤n)(Â¬ y = t[i]) â†’(âˆ€i â‰¤n)RP(S(pSy), S(pSt[i]))
(7â€²)

234
II. The Second Incompleteness Theorem
(6) now follows by the deduction theorem.
We immediately derive from (6), (Q), and II.1.36 that
âŠ¢y â‰¤c â†’(âˆ€i â‰¤n)(Â¬ y = t[i]) â†’Â¬S(pSy) | q
(8)
Hence, by tautological implication,
âŠ¢y â‰¤c â†’S(pSy) | q â†’(âˆƒi â‰¤n)y = t[i]
(8â€²)
Thus, informally speaking, q â€œcodesâ€ the unordered set of all â€œobjectsâ€
T = {S(pSt[i]) : i â‰¤n}
in the sense that if x is in T, then x | q, and, conversely, if S(pSy) | q â€“ where
y â‰¤c â€“ then S(pSy) is in T. By coding â€œposition informationâ€, i, along with
the term t[i], we can retrieve from q the ith sequence member t[i].
To this end, we deï¬ne three new function symbols, J, K, L, of arities 2, 1,
and 1 respectively:
J(x, y) = (x + y)2 + x
(J)
where â€œ(x + y)2â€ is an abbreviation for â€œ(x + y) Ã— (x + y)â€,
K z = (Âµx)(x = Sz âˆ¨(âˆƒy)â‰¤z J(x, y) = z)
(K)
Lz = (Âµy)(y = Sz âˆ¨(âˆƒx)â‰¤z J(x, y) = z)
(L)
J, K, L are the formal counterparts of J, K, L of p. 156.
To legitimize (K) and (L) one needs to show
âŠ¢(âˆƒx)(x = Sz âˆ¨(âˆƒy)â‰¤z J(x, y) = z)
(K â€²)
and
âŠ¢(âˆƒy)(y = Sz âˆ¨(âˆƒx)â‰¤z J(x, y) = z)
(Lâ€²)
They are both trivial, since âŠ¢Sz = Sz.
II.2.1 Lemma. âŠ¢J(x, y) = J(a, b) â†’x = a âˆ§y = b.
Proof. A straightforward adaptation of the argument following (âˆ—) on p. 156.
â–¡
II.2.2 Lemma. âŠ¢K J(a, b) = a and âŠ¢LJ(a, b) = b.
Proof. We just prove the ï¬rst contention, the proof of the second being entirely
analogous.

II.2. A Formal Î²-Function
235
First, it is a trivial matter to prove âŠ¢x â‰¤J(x, y) and âŠ¢y â‰¤J(x, y) (Exer-
cise II.12). Now âŠ¢b â‰¤J(a, b) âˆ§J(a, b) = J(a, b); hence
âŠ¢a = SJ(a, b) âˆ¨(âˆƒy)(y â‰¤J(a, b) âˆ§J(a, y) = J(a, b))
(1)
By (K), (1) above, and ( f (5)) (p. 218) we have
âŠ¢K J(a, b) â‰¤a
(2)
while (K) and ( f (4)) (p. 218) yield
âŠ¢K J(a, b) = SJ(a, b)
âˆ¨(âˆƒy)(y â‰¤J(a, b) âˆ§J(K J(a, b), y) = J(a, b))
(3)
Since K J(a, b) = SJ(a, b) is untenable by (2), we get
âŠ¢(âˆƒy)(y â‰¤J(a, b) âˆ§J(K J(a, b), y) = J(a, b))
(4)
Let c â‰¤J(a, b) âˆ§J(K J(a, b), c) = J(a, b), where c is a new constant. By
II.2.1, âŠ¢K J(a, b) = a.
â–¡
To conclude our coding, whose description we launched with the
-sign on
p. 232, let ï¬nally a[n] be a term.
We code the sequence a(n,âƒ—x), for i â‰¤n, by following the above steps, letting
ï¬rst t of the previous discussion be an abbreviation of the speciï¬c term below:
t(i,âƒ—x)
def
= J(i, a(i,âƒ—x)),
where i and âƒ—x are distinct variables
(T)
Thus, by (8â€²) (p. 234) and substitution, we have
âŠ¢J(i, m) â‰¤c âˆ§S(pSJ(i, m)) | q â†’
(âˆƒj â‰¤n)J(i, m) = J( j, a[ j])
which (by II.2.1â€ ) yields
âŠ¢J(i, m) â‰¤c âˆ§S(pSJ(i, m)) | q â†’m = a[i] âˆ§i â‰¤n
(5)
This motivates the deï¬nition (where d is intended to receive the â€œvalueâ€â€¡
J(c, q))
II.2.3 Deï¬nition (The Formal Î²).
Î²(d, i) = (Âµm)(m = d âˆ¨S(pSJ(i, m)) | Ld)
(B)
â€  âŠ¢(âˆƒj)( j â‰¤n âˆ§J(i, m) = J( j, a[ j])) â†’m = a[i] âˆ§i â‰¤n. To see this, assume hypothesis
and use a new constant b to eliminate (âˆƒj).
â€¡ Of course, regardless of intentions, the letter d in the deï¬nition (B) is just a variable, like i, m.

236
II. The Second Incompleteness Theorem
The letter p in (B) is an abbreviation for the term lcm jâ‰¤Kd(Sj) (see (P),
p. 232).
â–¡
That (B) is a legitimate deï¬nition, that is,
âŠ¢(âˆƒm)(m = d âˆ¨S(pSJ(i, m)) | Ld)
(Bâ€²)
follows from âŠ¢x = x.
II.2.4 Proposition.
(i) âŠ¢Î²(x, i) â‰¤x. Moreover,
(ii) âŠ¢Î²(x, i) < x â†”(âˆƒm)(S(pSJ(i, m)) | Lx), where p = lcmiâ‰¤K x(Si).
Proof. (i) is immediate from (B), âŠ¢x = x and ( f (5)) (p. 218).
(ii): The â†’-part is immediate from (B) and ( f (4)) (p. 218).
As for â†, assume (âˆƒm)(S(pSJ(i, m)) | Lx).
Let S(pSJ(i,r)) | Lx, where r is a new constant. Hence âŠ¢S(pSJ(i,r))
â‰¤Lx. We also have âŠ¢Lx â‰¤Sx by (L) (p. 234) and ( f (5)) (p. 218); thus
âŠ¢pSJ(i,r) â‰¤x bytransitivityandII.1.9(contrapositive).ButâŠ¢r < pSJ(i,r)
by âŠ¢y â‰¤J(x, y) (Exercise II.12); hence âŠ¢r < x. Since âŠ¢Î²(x, i) â‰¤r by (B)
and ( f (5)) (p. 218), we are done.
â–¡
All this work yields the â€œobviousâ€:
II.2.5 Theorem. For any term a(i,âƒ—x),
âŠ¢(âˆ€x1) . . . (âˆ€xm)(âˆ€n)(âˆƒz)(âˆ€i)(i â‰¤n â†’Î²(z, i) = a(i,âƒ—x))
(6)
where m is the length of âƒ—x.
Proof. We prove instead
âŠ¢(âˆƒz)(âˆ€i)(i â‰¤n â†’Î²(z, i) = a(i,âƒ—x))
The proof constructs a â€œz that worksâ€ (and then invokes Ax2). To this end,
we let t be that in (T ), and in turn, let c, p, q stand for the terms in the right
hand sides of (C), (P), and (Q) respectively (p. 232). Setting for convenience
d = J(c, q), we are reduced to proving
i â‰¤n â†’Î²(d, i) = a(i,âƒ—x)
(7)

II.2. A Formal Î²-Function
237
Thus we add the assumption i â‰¤n. We know
âŠ¢J(i, a(i,âƒ—x)) â‰¤c,
by (C), (M) on p. 230, and ( f (4)) (p. 218)
âŠ¢S(pSJ(i, a(i,âƒ—x))) | q,
by (Q) and (3) on p. 231
Or, using the abbreviation â€œdâ€ and II.2.2
âŠ¢J(i, a(i,âƒ—x)) â‰¤Kd
(8)
âŠ¢S(pSJ(i, a(i,âƒ—x))) | Ld
(9)
Thus,
âŠ¢(âˆƒm)(S(pSJ(i, m)) | Ld)
The above existential theorem and II.2.4(ii) imply
âŠ¢Î²(d, i) < d
(10)
so that (B) (p. 235) and ( f (4)) â€“ through âŠ¢Â¬Î²(d, i) = d, by (10) â€“ yield
âŠ¢S(pSJ(i, Î²(d, i))) | Ld
(11)
By (9), (B), and ( f (5)) (p. 218), âŠ¢Î²(d, i) â‰¤a(i,âƒ—x); hence, since J is increasing
in each argument (do you believe this?), (8) implies
âŠ¢J(i, Î²(d, i)) â‰¤Kd
Combining the immediately above with (11), we obtain
âŠ¢J(i, Î²(d, i)) â‰¤Kd âˆ§S(pSJ(i, Î²(d, i))) | Ld
Now (5) on p. 235 yields
âŠ¢Î²(d, i) = a(i,âƒ—x)
By the deduction theorem, we now have (7).
â–¡
II.2.6 Corollary. For any term a(i,âƒ—x),
âŠ¢(âˆ€x1) . . . (âˆ€xm)(âˆ€n)(âˆƒz)(âˆ€i)iâ‰¤n(Î²(z, i) < z âˆ§Î²(z, i) = a(i,âƒ—x))
where m is the length of âƒ—x.
Proof. By (10) of the previous proof.
â–¡

238
II. The Second Incompleteness Theorem
II.2.7 Example (Some Pathologies). (1) By II.2.4 (i) we get âŠ¢Î²(0, i) = 0
(using II.1.5 and II.1.8). Thus, if we introduce an 1-ary function letter f by the
explicit deï¬nition f n = 0, then âŠ¢i â‰¤n â†’Î²(0, i) = fi. It follows, accord-
ing to (6) of II.2.5, that 0 â€œcodesâ€ the sequence of the ï¬rst n members of the
term fi â€“ for any n â€œvalueâ€.
(2) Next, â€œcomputeâ€ Î²(3, i). Now, âŠ¢K3 =4 and âŠ¢L3 =4 (why?). Since
âŠ¢p = 
60 (why?), we get
âŠ¢Â¬S(pSJ(i, m)) | L3
since âŠ¢S(pSJ(i, m)) â‰¥
61 (why?). By II.2.4(ii), âŠ¢Î²(3, i) =3.â€ 
Thus,ifwehaveafunctionsymbol g withadeï¬nition gn =3,thenapossible
proof of
âŠ¢(âˆƒz)(âˆ€i)(i â‰¤w â†’Î²(z, i) = gi)
starts with â€œtake z to be3â€. An alternative â€œvalueâ€ for z is the â€œdâ€ constructed
in the proof of II.2.5, adapted to the term gn. We may call the latter z-â€œvalueâ€
the â€œintended oneâ€ or the â€œnatural oneâ€.
Clearly, â€œintendedâ€ or not, any z that works in (6) of II.2.5 is an in principle
acceptable coding of the ï¬rst â€œn membersâ€ of a term a.
(3) Finally, let us compute Î²(2, i). Now, âŠ¢K2 =1 and âŠ¢L2 = 0. Also
âŠ¢p =2. It follows that
âŠ¢Î²(2, i) = 0
since
âŠ¢S(pSJ(i, 0)) | 0
Thus, if f is introduced as in part (1) of this example by f n = 0, then
âŠ¢(âˆƒz)(âˆ€i)(i â‰¤w â†’Î²(z, i) = fi)
can be proved by letting z be 2, or 0, or by invoking the construction carried
out in the proof of II.2.5.â€¡ In particular, Î² is not 1-1 in its ï¬rst argument.
Part (3) of this example shows that an x that passes the test â€œÎ²(x, i) < xâ€
is not necessarily the d computed in the standard manner as in the proof of
II.2.5 â€“ i.e., we cannot expect âŠ¢x = d. After all, âŠ¢Î²(2, i) <2.
â–¡
â€  This is related to the example Î²(4, i) =4 of II.2.14.
â€¡ If the latter construction is followed, then âŠ¢Lz > 0, of course.

II.2. A Formal Î²-Function
239
We are all set to introduce the formal counterparts of âŸ¨. . . âŸ©, Seq,lh, âˆ—, (z)i
of p. 165.
II.2.8 Deï¬nition (Bold âŸ¨. . . âŸ©). For any term t and any variable w not free in
t, we denote by âŸ¨t[i] : i < wâŸ©(or, sloppily, âŸ¨t[0], . . . , t[w âˆ’1]âŸ©) the Âµ-term
(Âµz)(Î²(z, 0) = w âˆ§(âˆ€i)<w(Î²(z, Si) = St[i]))
(FC)
â–¡
II.2.9 Proposition. The Âµ-term in (FC) can be formally introduced.
Proof. We want
âŠ¢(âˆƒz)(Î²(z, 0) = w âˆ§(âˆ€i)<w(Î²(z, Si) = St[i]))
(FCâ€²)
Let a[w, n] abbreviate the term deï¬ned by cases below (see p. 221):
a[w, n] =
w
if n = 0
St[Î´(n, S0)]
if n > 0
(âˆ—)
(âˆ—) yields ((15) on p. 221)
âŠ¢n = 0 â†’a[w, n] = w
and hence (Ax4)
âŠ¢a[w, 0] = w
(1)
but also
âŠ¢n > 0 â†’a[w, n] = St[Î´(n, S0)]
Hence (by âŠ¢Sn > 0 and modus ponens)
âŠ¢a[w, Sn] = St[Î´(Sn, S0)]
or, using âŠ¢Î´(Sn, S0) = n (do you believe this?)
âŠ¢a[w, Sn] = St[n]
(2)
Now, by Theorem II.2.5,
âŠ¢(âˆƒz)(âˆ€i)(i â‰¤w â†’Î²(z, i) = a[w, i])

240
II. The Second Incompleteness Theorem
In view of the above existential statement, we introduce a new constant c and
the assumption
(âˆ€i)(i â‰¤w â†’Î²(c, i) = a[w, i])
By specialization we obtain from the above
âŠ¢0 â‰¤w â†’Î²(c, 0) = a[w, 0]
that is,
âŠ¢Î²(c, 0) = a[w, 0]
(3)
by II.1.5, and
âŠ¢Si â‰¤w â†’Î²(c, Si) = a[w, Si]
that is,
âŠ¢i < w â†’Î²(c, Si) = a[w, Si]
(4)
in view of II.1.10. Putting the generalization of (4) together (conjunction)
with (3), via (1) and (2), yields
âŠ¢Î²(c, 0) = w âˆ§(âˆ€i)<w(Î²(c, Si) = St[i])
from which Ax2 yields (FCâ€²).
â–¡
II.2.10 Deï¬nition. We introduce the functions â€œlhâ€ and â€œ( . . . )...â€ by
lh(z) = Î²(z, 0)
(z)i
= Î´(Î²(z, Si), S0)
(âˆ—âˆ—)
â–¡
In the second deï¬nition in the group (âˆ—âˆ—) above we have introduced a new
2-ary function symbol called, let us say, f , by f zi = Î´(Î²(z, Si), S0) â€“ in
informal lightface notation, Î²(z, i + 1)
.âˆ’1 â€“ and then agreed to denote the
term â€œ f ziâ€ by (z)i.â€ 
II.2.11 Proposition. If we let b = âŸ¨t[i] : i < xâŸ©, then we can obtain
(1) âŠ¢lh(b) = x,
(2) âŠ¢(âˆ€i)<x(b)i = t[i], or, equivalently, âŠ¢(âˆ€i)<lh(b)(b)i = t[i], and
(3) âŠ¢z < b â†’(Â¬lh(z) = x âˆ¨(âˆƒi)<xÂ¬(z)i = t[i]).
â€  Note how this closely parallels the â€œ(z)iâ€ of the prime-power coding. We had set there (p. 137)
(z)i = exp(i, z)
.âˆ’1.

II.2. A Formal Î²-Function
241
Proof. (1): By ( f (4)) and (FC) (p. 239),
âŠ¢Î²(b, 0) = x
We conclude using II.2.10.
(2): By ( f (4)) and (FC),
âŠ¢i < x â†’Î²(b, Si) = St[i]
We conclude using âŠ¢Î´(St[i], S0) = t[i] and II.2.10.
(3): To prove the last contention we invoke ( f (3)), p. 218, in connection
with (FC). It yields
âŠ¢(âˆ€z)<bÂ¬(Î²(z, 0) = x âˆ§(âˆ€i)(i < x â†’Î²(z, Si) = St[i]))
Hence, using II.2.10, the Leibniz rule, and specialization,
âŠ¢z < b â†’(Â¬lh(z) = x âˆ¨(âˆƒi)<xÂ¬(z)i = t[i])
â–¡
Item (3) above suggests how to test a number for being a sequence code or
not. We deï¬ne (exactly as on p. 165, but in boldface)
II.2.12 Deï¬nition. We introduce the unary predicate Seq by
Seq(z) â†”(âˆ€i)<lh(z) Î² (z, Si) > 0 âˆ§
(âˆ€x)<z(lh(x) Ì¸= lh(z) âˆ¨(âˆƒi)<lh(z)(z)i Ì¸= (x)i)
â–¡
The ï¬rst conjunct above tests that we did not forget to add 1 to the sequence
members before coding. The second conjunct says that our code z is minimum,
because, for any smaller â€œnumberâ€ x, whatever sequence the latter may code
(â€œminimallyâ€ or not) cannot be the same sequence as the one that z (minimally)
codes.
II.2.13 Proposition. âŠ¢Seq(z) â†’(âˆ€i)<lh(z)z > (z)i.
Proof. Assume the hypothesis. Specialization and II.2.12 yield
âŠ¢i < lh(z) â†’Î²(z, Si) > 0
(1)
By II.1.10, âŠ¢Î²(z, Si) > 0 â†’Î²(z, Si) â‰¥S0; thus, by II.1.27 and II.2.10, (1)
yields
âŠ¢i < lh(z) â†’Î²(z, Si) = (z)i + S0

242
II. The Second Incompleteness Theorem
By +2 and +1 we now get
âŠ¢i < lh(z) â†’Î²(z, Si) = S((z)i)
which, by II.2.4(i), rests our case.
â–¡
II.2.14 Remark. The inequality
(z)i < z
(i)
is very important when it comes to doing induction on sequence codes and
(soon) on GÂ¨odel numbers.
By Example II.2.7 we see that unless we do something about it, we are not
justiï¬ed in expecting âŠ¢Î²(z, i) < z, in general. What we did to ensure that in-
duction on codes is possible was to add 1 to each sequence member t[i], a
simple trick that was already employed in the prime-power coding (p. 136),
although for different reasons there.â€  This device ensures (i) by II.2.13.
Another way to ensure (i) is to invoke Corollary II.2.6 and modify (FC) to
read instead
(Âµz)(Î²(z, 0) = w âˆ§w < z âˆ§(âˆ€i)<w(Î²(z, Si) < z âˆ§Î²(z, Si) = t[i]))
Note that we did not need to â€œadd 1â€ to t above.
We prefer our ï¬rst solution (Deï¬nition II.2.8), if nothing else, because it al-
lows 0 to code the â€œempty sequenceâ€ (see II.2.15 below), a fact that is intuitively
pleasing.
This is a good place to mention that while our â€œÎ²â€ (in either the bold or
the lightface version) is, essentially, that in Shoenï¬eld (1967), we had to tweak
the latter somewhat (especially the derived â€œâŸ¨. . . âŸ©â€) to get it to be induction-
friendly (in particular, to have (i) above).
The version in Shoenï¬eld (1967, (5), p. 116) is
Î²(a, i) = (Âµx)(x = a âˆ¨
(âˆƒy)<a(âˆƒz)<a(a = Jâ€²(y, z) âˆ§S(zSJâ€²(x, i)) | y))
(Sh)
whereâ€¡
Jâ€²(y, z) = (y + z)2 + y +1
Since, intuitively speaking, 4 is not in the range of Jâ€², i.e., formally,
âŠ¢Â¬(âˆƒy)(âˆƒz)Jâ€²(y, z) = 4
â€  Namely, to enable us to implicitly store in the code the length of the coded sequence.
â€¡ The two â€œJâ€s, the one employed here and Shoenï¬eldâ€™s Jâ€², trivially satisfy âŠ¢Jâ€²(y, z) = SJ(y, z).

II.2. A Formal Î²-Function
243
we have
âŠ¢Î²(4, i) =4
because the search (âˆƒy)<a(âˆƒz)<a( . . . ) in the deï¬ning axiom (Sh) fails. This in-
validates (7), p. 116 of Shoenï¬eld (1967),â€  on which (i) hinges in that book.â€¡ â–¡
II.2.15 Example. Since âŠ¢0 = Î²(0, 0) by II.2.7, we have âŠ¢lh(0) = 0, but
also, by < 1, that 0 is a minimum code of a sequence. Indeed, âŠ¢Seq(0) by < 1.
Since the sequence in question has 0 length, it is called the empty sequence.
We often write the minimum code, (FC), of the empty sequence as â€œâŸ¨âŸ©â€, that is,
âŠ¢âŸ¨âŸ©= 0
â–¡
To conclude with the introduction of our formal coding tools we will also
need a formal concatenation function.
II.2.16 Deï¬nition (Concatenation). â€œâˆ—â€ â€“ a 2-ary function symbol â€“ is intro-
duced via the Âµ-term below (denoted by â€œx âˆ—yâ€):
x âˆ—y = (Âµz)(Î²(z, 0) = lh(x) + lh(y) âˆ§
(âˆ€i)<lh(x)Î²(z, Si) = S((x)i) âˆ§
(âˆ—âˆ—âˆ—)
(âˆ€i)<lh(y)Î²(z, Si + lh(x)) = S((y)i))
â–¡
The legitimacy of (âˆ—âˆ—âˆ—) relies on
II.2.17 Proposition.
âŠ¢(âˆƒw)(Î²(w, 0) = lh(x) + lh(y) âˆ§
(âˆ€i)<lh(x)Î²(w, Si) = S((x)i) âˆ§
(1)
(âˆ€i)<lh(y)Î²(w, Si + lh(x)) = S((y)i))
Since the x and y are free variables, (1) says, informally, that for all â€œvaluesâ€ of
x and y a wthat â€œworksâ€ exists. That is, the natural interpretation of âˆ—over N is
total. In other words, just like the â€œâˆ—â€ that we saw earlier on, which was based
on prime power coding (see I.8.30, p. 136), this new â€œâˆ—â€ makes sense regardless
of whether or not its arguments are minimum codes according to (FC).
â€  That âŠ¢Â¬a = 0 â†’Î²(a, i) < a.
â€¡ (8) in loc. cit., p. 117 [deduced from (7) in loc. cit.]: âŠ¢a Ì¸= 0 â†’lh(a) < a âˆ§(a)i < a.

244
II. The Second Incompleteness Theorem
Proof. To see why (1) holds, we introduce the 3-ary function symbol a by
a(n, x, y) =
ï£±
ï£²
ï£³
lh(x) + lh(y)
if n = 0
S((x)Î´(n,S0))
if 0 < n âˆ§n â‰¤lh(x)
S((y)Î´(n,S(lh(x))))
if lh(x) < n
(A)
By II.2.5,
âŠ¢(âˆƒw)(âˆ€i)(i â‰¤lh(x) + lh(y) â†’Î²(w, i) = a(i, x, y))
Let us add a new constant, z, and the assumption
(âˆ€i)(i â‰¤lh(x) + lh(y) â†’Î²(z, i) = a(i, x, y))
(2)
We now show that we can prove (1) from (2).
It may be that it is not the case that âŠ¢Seq(z). This is just ï¬ne, as the proof
needs no such assumption.
We verify each conjunct of (1) separately:
r Since (A) yields (p. 221) âŠ¢n = 0 â†’a(n, x, y) = lh(x) + lh(y) and hence
âŠ¢a(0, x, y) = lh(x) + lh(y)
(2), specialization, and II.1.5 yield
âŠ¢Î²(z, 0) = lh(x) + lh(y)
(3)
r Next we prove
âŠ¢i < lh(x) â†’Î²(z, Si) = S((x)i)
(4)
By II.1.10 this amounts to proving
âŠ¢Si â‰¤lh(x) â†’Î²(z, Si) = S((x)i)
(4â€²)
Now, since âŠ¢lh(x) â‰¤lh(x) + lh(y), (2) yields
âŠ¢Si â‰¤lh(x) â†’Î²(z, Si) = a(Si, x, y)
Moreover, (A) and âŠ¢0 < Si yield (p. 221)
âŠ¢Si â‰¤lh(x) â†’a(Si, x, y) = S((x)Î´(Si,S0))
and since âŠ¢Î´(Si, S0) = i, we get (4â€²) above.
r Finally, we want to verify
âŠ¢i < lh(y) â†’Î²(z, Si + lh(x)) = S((y)i)
(5)

II.2. A Formal Î²-Function
245
which amounts to
âŠ¢Si â‰¤lh(y) â†’Î²(z, Si + lh(x)) = S((y)i)
(5â€²)
using II.1.10. Add Si â‰¤lh(y). Hence âŠ¢Si + lh(x) â‰¤lh(x) +lh(y) by
II.1.17. Thus, by (2),
âŠ¢Î²(z, Si + lh(x)) = a(Si + lh(x), x, y)
(6)
By (A),
âŠ¢lh(x) < Si + lh(x)
â†’a(Si + lh(x), x, y) = S((y)Î´(Si+lh(x),S(lh(x))))
(7)
Hence, noting that
âŠ¢lh(x) < Si + lh(x)
(by II.1.17)
âŠ¢Si + lh(x) = i + S(lh(x))
and
âŠ¢Î´(i + S(lh(x)), S(lh(x))) = i
(by II.1.27)
(6) and (7) yield
âŠ¢Î²(z, Si + lh(x)) = S((y)i)
which by the deduction theorem yields (5â€²) and hence (5).
Putting now the conjuncts (3)â€“(5) together and applying Ax2 yields (1).
â–¡
II.2.18 Example. We verify that for any terms t and s
âŠ¢âŸ¨t[i] : i < nâŸ©âˆ—âŸ¨s[i] : i < mâŸ©= âŸ¨a[i] : i < n + mâŸ©
where
a[i] =
ï£±
ï£²
ï£³
t[i]
if i < n
s[Î´(i, n)]
if n â‰¤i âˆ§i < n + m
0
otherwise
Setting x = âŸ¨t[i] : i < nâŸ©and y = âŸ¨s[i] : i < mâŸ©for convenience, we get
from II.2.11
âŠ¢lh(x) = n
âŠ¢lh(y) = m
âŠ¢i < n â†’(x)i = t[i]

246
II. The Second Incompleteness Theorem
and
âŠ¢i < m â†’(y)i = s[i]
Pause. (Important.) None of the above four facts need the assertion that x and
y are minimum â€œâŸ¨. . . âŸ©-codesâ€. We have invoked above only the part of II.2.11
that does not rely on minimality. Only the last claim in II.2.11 does.
Thus, by II.2.16,
âŠ¢x âˆ—y = (Âµz)(lh(z) = n + m âˆ§
(âˆ€i)<nÎ²(z, Si) = St[i] âˆ§
(8)
(âˆ€i)<mÎ²(z, Si + n) = Ss[i])
On the other hand,
âŠ¢âŸ¨a[i] : i < n + mâŸ©= (Âµw)(lh(w) = n + m âˆ§
(âˆ€i)<nÎ²(w, Si) = St[i] âˆ§
(9)
(âˆ€i)<mÎ²(w, Si + n) = Ss[i])
by the way a was deï¬ned (noting that âŠ¢n â‰¤i + n and âŠ¢Î´(i + n, n) = i).
By (8) and (9), we are done.
Pause. Parting comment: From our earlier Pause, we see that even in the case
when x and y are not the minimum âŸ¨. . . âŸ©-codes for the sequences t[i] : i < n
and s[i] : i < m respectively, but nevertheless happen to satisfy the following,
âŠ¢lh(x) = n
âŠ¢lh(y) = m
âŠ¢i < n â†’(x)i = t[i]
and
âŠ¢i < m â†’(y)i = s[i]
Then the work immediately above still establishes
âŠ¢x âˆ—y = âŸ¨a[i] : i < n + mâŸ©
â–¡
II.2.19 Proposition. If we let b abbreviate âŸ¨t[i] : i < xâŸ©, then we have
âŠ¢Seq(b)
Proof. Immediate from II.2.8, II.2.11, and II.2.12.
â–¡
II.2.20 Exercise. âŠ¢Seq(x âˆ—y).
â–¡

II.2. A Formal Î²-Function
247
II.2.21 Example. We introduce the abbreviation t below:
t = (Âµz)(Seq(z)âˆ§
lh(z) = lh(x) + lh(y)âˆ§
(âˆ€i)<lh(x)(z)i = (x)i âˆ§
(âˆ€i)<lh(y)(z)i+lh(x) = (y)i)
(1)
and prove
âŠ¢t = x âˆ—y
By the deï¬nition of â€œâˆ—â€ (II.2.16) and ( f (4)) (p. 218) â€“ using
âŠ¢x = Sy â†’Î´(x, S0) = y
(by II.1.27) and II.2.10 to remove instances of Î² â€“ we obtain
âŠ¢lh(x âˆ—y) = lh(x) +lh(y)âˆ§
(âˆ€i)<lh(x)(x âˆ—y)i = (x)i âˆ§
(âˆ€i)<lh(y)(x âˆ—y)i+lh(x) = (y)i
(2)
Since also âŠ¢Seq(x âˆ—y), by II.2.20 we get
âŠ¢t â‰¤x âˆ—y
(3)
by (2), (1), and ( f (5)) (p. 218).
To get the converse inequality, we use (1) and ( f (4)) to get
âŠ¢Seq(t)âˆ§
lh(t) = lh(x) + lh(y)âˆ§
(âˆ€i)<lh(x)(t)i = (x)i âˆ§
(âˆ€i)<lh(y)(t)i+lh(x) = (y)i
(4)
The ï¬rst conjunct of (4) and II.2.12 (via II.1.27) yield,â€  from the remaining
conjuncts of (4),
âŠ¢Î²(t, 0) = lh(x) + lh(y)âˆ§
(âˆ€i)<lh(x)Î²(t, Si) = S((x)i)âˆ§
(âˆ€i)<lh(y)Î²(t, Si + lh(x)) = S((y)i)
By the deï¬nition of â€œâˆ—â€ (II.2.16) and ( f (5)),
âŠ¢x âˆ—y â‰¤t
which along with (3) and II.1.8 rests the case.
â–¡
â€  More expansively, âŠ¢(âˆ€i)<lh(t)Î²(t, Si) â‰¥S(0) by II.2.12, which implies (by II.1.27 and II.2.10)
âŠ¢(âˆ€i)<lh(t)Î²(t, Si) = (t)i + S0, but âŠ¢z + S0 = Sz.

248
II. The Second Incompleteness Theorem
II.2.22 Example. By II.2.18,
âŠ¢0 âˆ—âŸ¨t[i] : i < xâŸ©= âŸ¨t[i] : i < xâŸ©
and
âŠ¢âŸ¨t[i] : i < xâŸ©âˆ—0 = âŸ¨t[i] : i < xâŸ©
In particular, âŠ¢0 âˆ—âŸ¨âŸ©= âŸ¨âŸ©(cf. II.2.15), i.e., âŠ¢0 âˆ—0 = 0. Note however that
if a is the term that denotes the natural coding of the empty sequence, that is,
âŠ¢Î²(a, 0) = 0 and âŠ¢0 < a (note that âŠ¢Â¬Seq(a)), then
âŠ¢0 âˆ—a = 0
since âŠ¢Seq(0 âˆ—a), and therefore
âŠ¢Â¬0 âˆ—a = a
â–¡
Thus we have two ways to â€œÎ²-codeâ€ sequences t[i], for i < x. One is to pick
any c that satisï¬es âŠ¢Î²(c, 0) = x and âŠ¢(âˆ€i)<xÎ²(c, i) = t[i]. The other is to
employ the minimum â€œâŸ¨. . . âŸ©-codeâ€, b = âŸ¨t[i] : i < xâŸ©. By II.2.19, b (but not
necessarily c) â€œsatisï¬esâ€ Seq.
II.3. Formal Primitive Recursion
II.3.1 Theorem (Primitive Recursive Deï¬nitions). Let h and g be n-ary and
(n + 2)-ary function symbols. Then we may introduce a new (n + 1)-ary func-
tion symbol, f , such that
âŠ¢f (0,âƒ—y ) = h(âƒ—y )
and
âŠ¢f (Sx,âƒ—y ) = g(x,âƒ—y, f (x,âƒ—y ))
Proof. We prove
âŠ¢(âˆƒz)
%
Seq(z) âˆ§lh(z) = Sx âˆ§(z)0 = h(âƒ—y )
âˆ§(âˆ€i)<x((z)Si = g(i,âƒ—y, (z)i))
&
(1)
This is done by formal induction on x.
For x = 0, taking z = âŸ¨h(âƒ—y )âŸ©works (this invokes < 1 and Ax2).

II.3. Formal Primitive Recursion
249
Assume now (1) for frozen variables (the I.H.). For the induction step we
want to show
âŠ¢(âˆƒz)
%
Seq(z) âˆ§lh(z) = SSx âˆ§(z)0 = h(âƒ—y )
âˆ§(âˆ€i)<Sx((z)Si = g(i,âƒ—y, (z)i))
&
(2)
To this end, let a be a new constant, and add the assumption (invoking I.4.27
and (1))
âŠ¢Seq(a) âˆ§lh(a) = Sx âˆ§(a)0 = h(âƒ—y )
âˆ§(âˆ€i)<x((a)Si = g(i,âƒ—y, (a)i))
(3)
Set now
b = a âˆ—âŸ¨g(x,âƒ—y, (a)x)âŸ©
By II.2.21 and ( f (4)) (p. 218)
âŠ¢Seq(b)âˆ§
lh(b) = SSxâˆ§
(âˆ€i)<Sx(b)i = (a)i âˆ§
(b)Sx = g(x,âƒ—y, (a)x)
(4)
the last conjunct being distilled from
âŠ¢i < S0 â†’(b)i+Sx = g(x,âƒ—y, (a)x)
and âŠ¢i < S0 â†”i = 0.
Thus, using (3) and the Leibniz axiom (Ax4),
âŠ¢Seq(b)âˆ§
lh(b) = SSxâˆ§
(b)0 = h(âƒ—y )âˆ§
(âˆ€i)<x(b)Si = g(i,âƒ—y, (b)i)âˆ§
(b)Sx = g(x,âƒ—y, (b)x)
(since âŠ¢(b)x = (a)x by (4))
In short, using (I.7.2) âŠ¢(b)Sx = g(x,âƒ—y, (b)x) â†”(âˆ€i)i=x(b)Si = g(i,âƒ—y, (b)i),
âˆ€-distribution over âˆ§, and < 2, we have
âŠ¢Seq(b) âˆ§lh(b) = SSx âˆ§(b)0 = h(âƒ—y ) âˆ§(âˆ€i)<Sx(b)Si = g(i,âƒ—y, (b)i)
This proves (2) by Ax2 and concludes the inductive proof of (1). We can now let
F(x,âƒ—y ) = (Âµz)
%
Seq(z) âˆ§lh(z) = Sxâˆ§
(z)0 = h(âƒ—y ) âˆ§(âˆ€i)<x((z)Si = g(i,âƒ—y, (z)i))
&
where F is a new function symbol.

250
II. The Second Incompleteness Theorem
By ( f (4)) (p. 218),
âŠ¢(F(x,âƒ—y ))0 = h(âƒ—y )
âŠ¢i < x â†’(F(x,âƒ—y ))Si = g(i,âƒ—y, (F(x,âƒ—y ))i)
(5)
The ï¬rst part of (5) yields (by substitution)
âŠ¢(F(0,âƒ—y ))0 = h(âƒ—y )
(6)
The second yields (by substitution [x â†Si])
âŠ¢(F(Si,âƒ—y ))Si = g(i,âƒ—y, (F(Si,âƒ—y ))i)
(7)
We now claim that
âŠ¢(F(Si,âƒ—y ))i = (F(i,âƒ—y ))i
(8)
It is convenient to prove a bit more, by induction on i, namely, that
âŠ¢(âˆ€x)(i < x â†’(F(i,âƒ—y ))i = (F(x,âƒ—y ))i)
(9)
The case for i = 0 follows from (6) and the ï¬rst of (5).
We now assume (9) as our I.H., freezing the free variables (i and âƒ—y ).
For our induction step we need to prove
(âˆ€x)(Si < x â†’(F(Si,âƒ—y ))Si = (F(x,âƒ—y ))Si)
but we prove instead
Si < x â†’(F(Si,âƒ—y ))Si = (F(x,âƒ—y ))Si
To this end, we assume Si < x â€“ freezing x â€“ and proceed to verify
âŠ¢(F(Si,âƒ—y ))Si = (F(x,âƒ—y ))Si
(10)
Well, since also âŠ¢i < x and âŠ¢i < Si, the I.H. (9) yields by specialization
âŠ¢(F(i,âƒ—y ))i = (F(x,âƒ—y ))i
and
âŠ¢(F(i,âƒ—y ))i = (F(Si,âƒ—y ))i
Therefore,
âŠ¢g(i,âƒ—y, (F(Si,âƒ—y ))i) = g(i,âƒ—y, (F(x,âƒ—y ))i)
which, by the second part of (5) and (7), veriï¬es (10).
Having thus established (9) and therefore (8), (7) becomes
âŠ¢(F(Si,âƒ—y ))Si = g(i,âƒ—y, (F(i,âƒ—y ))i)

II.3. Formal Primitive Recursion
251
All this proves that if f is a new function symbol introduced by
f (x,âƒ—y ) = (F(x,âƒ—y ))x
then the two formulas stated in the theorem are proved.
â–¡
II.3.2 Exercise. Verify that if h and g are Âµ-deï¬ned from âˆ†0(LA) formulas,
then so is f .
â–¡
II.3.3 Theorem (Course-of-Values Recursion). Let h and g be function sym-
bols of arities n and n + 2 respectively. Then we may introduce a new function
symbol H of arity n + 1 such that
âŠ¢(H(0,âƒ—y ))0 = h(âƒ—y )
âŠ¢(H(Sx,âƒ—y ))Sx = g(x,âƒ—y, H(x,âƒ—y ))
âŠ¢Seq(H(x,âƒ—y )) âˆ§lh(H(x,âƒ—y )) = Sx
(1)
Proof. Invoking II.3.1, we introduce a new function symbol H by the primitive
recursion below, and show that it works:
H(0,âƒ—y ) = âŸ¨h(âƒ—y )âŸ©
H(Sx,âƒ—y ) = H(x,âƒ—y ) âˆ—âŸ¨g(x,âƒ—y, H(x,âƒ—y ))âŸ©
(2)
The third formula in the group (1) is proved by induction on x. The basis is
immediate from the basis of the recursion (2) (note that a â€œâŸ¨. . . âŸ©â€ term satisï¬es
Seq by II.2.19).
Assume now the contention for frozen x. By II.2.21 and the second formula
in group (2),
âŠ¢lh(H(Sx,âƒ—y )) = SSx
and
âŠ¢Seq(H(Sx,âƒ—y ))
This concludes the induction. The other two contentions in the theorem are
direct consequences of the two recurrence equations (2).
â–¡
II.3.4 Remark. Introducing yet another function symbol, f , by
f (x,âƒ—y ) = (H(x,âƒ—y ))x

252
II. The Second Incompleteness Theorem
yields at once
âŠ¢f (0,âƒ—y ) = h(âƒ—y )
âŠ¢f (Sx,âƒ—y ) = g(x,âƒ—y, H(x,âƒ—y ))
This is the standard way that course-of-values recursion is presented, i.e.,
deï¬ning a function f from known functions h and g and from the â€œhistoryâ€,
H(x,âƒ—y ) = âŸ¨f (i,âƒ—y ) : i â‰¤xâŸ©, of f .
â–¡
We recall the concept of a term-deï¬nable (total) function (p. 181). We can now
prove
II.3.5 Theorem. Every primitive recursive function is term-deï¬nable in (some
recursive extension of ) PA.
Proof. The proof is by (informal) induction on PR. For the basis, we already
know that the initial functions are term-deï¬nable in ROB, a subtheory of PA.
Let now f, g1, . . . , gn be deï¬ned by the terms f, g1, . . . , gn. We will argue
that the term f (g1(âƒ—ym), . . . , gn(âƒ—ym)) deï¬nes Î»âƒ—ym. f (g1(âƒ—ym), . . . , gn(âƒ—ym)).
So let f (g1(âƒ—am), . . . , gn(âƒ—am)) = b be true. Then, for appropriate âƒ—cn,
f (c1, . . . , cn) = b
g1(âƒ—am) = c1
...
gn(âƒ—am) = cn
By the induction hypothesis,
âŠ¢f (c1, . . . , 
cn) = b
âŠ¢g1(
a1, . . . , 
am) = c1
...
âŠ¢gn(
a1, . . . , 
am) = 
cn
Hence (via Ax4)
âŠ¢f (g1(
a1, . . . , 
am), . . . , gn(
a1, . . . , 
am)) = b
Let ï¬nally h and g be term-deï¬ned by h and g respectively, and let f be given
by the schema below (for all x, âƒ—y):
f (0, âƒ—y ) = h(âƒ—y )
f (x + 1, âƒ—y ) = g(x, âƒ—y, f (x, âƒ—y ))

II.3. Formal Primitive Recursion
253
We verify that the term f (x,âƒ—y ), where f is introduced by formal primitive
recursion below, deï¬nes f :
f (0,âƒ—y ) = h(âƒ—y )
(1)
f (x + 1,âƒ—y ) = g(x,âƒ—y, f (x,âƒ—y ))
To this end, we prove by (informal) induction on a thatâ€ 
f (a, âƒ—b) = c
implies
âŠ¢f (a,âƒ—b) =c
(2)
Let a = 0. Then f (0, âƒ—b) = c entails h(âƒ—b) = c; hence âŠ¢h(âƒ—b) =c by the I.H. (of
the PR-induction).
By the ï¬rst equation of group (1), âŠ¢f (0,âƒ—b) =c, which settles the basis of
the a-induction. Now ï¬x a, and take (2) as the I.H. of the a-induction. We will
argue the case of (2) when a is replaced by a + 1.
Let f (a + 1, âƒ—b) = c. Then g(a, âƒ—b, d) = c, where f (a, âƒ—b) = d. By the I.H. of
the PR-induction and a-induction, i.e., (2),
âŠ¢f (a,âƒ—b) =d
and
âŠ¢g(a,âƒ—b,d) =c
Thus, by Ax4 and the second equation of group (1),
âŠ¢f (Sa,âƒ—b) =c
â–¡
The above suggests the following deï¬nition.
II.3.6 Deï¬nition (Formal Primitive Recursive Functions). A deï¬ned func-
tion symbol f is primitive recursive iff there is a sequence of function symbols
g1, . . . , gp such that gp â‰¡f and, for all i = 1, . . . , p, gi has been introduced
by a primitive recursive deï¬nition
gi(0, v1, . . . , vnâˆ’1) = g j(v1, . . . , vnâˆ’1)
gi(Sv0, v1, . . . , vnâˆ’1) = gk(v0, v1, . . . , vnâˆ’1, gi(v0, . . . , vnâˆ’1))
where j < i and k < i and the g j and gk have arities n âˆ’1 and n + 1 respec-
tively, or by composition, that is, an explicit deï¬nition such as
gi(v0, . . . , vnâˆ’1) = g j0(g j1(v0, . . . , vnâˆ’1), . . . , g jr (v0, . . . , vnâˆ’1))
â€  âƒ—b means 
b1, 
b2, . . . .

254
II. The Second Incompleteness Theorem
using, again, previous function symbols (of the correct arities) in the sequence
(i.e., jm < i, m = 0, . . . ,r). Otherwise, gi is one of S, Z (zero function symbol),
or U n
i (n > 0, 1 â‰¤i â‰¤n) (projection function symbols), where the latter two
symbols are introduced by the deï¬ning axioms
Z(v0) = 0
and
U n
i (v0, . . . , vnâˆ’1) = viâˆ’1
for each positive n and 1 â‰¤i â‰¤n in N.
A sequence such as g1, . . . , gp is a formal (primitive recursive) derivation of
gp.
A term is primitive recursive iff it is deï¬ned from 0, variables, and primitive
recursive function symbols according to the standard deï¬nition of â€œtermâ€.
A predicate P is primitive recursive iff there is a primitive recursive function
symbol f of the same arity as P such that P(âƒ—x) â†”f (âƒ—x) = 0 is provable.
By a slip of the tongue, we may say that P is primitive recursive iff its formal
characteristic function, Ï‡P, is. (See however II.1.25, p. 220.)
â–¡
II.3.7 Remark. In order that the primitive recursive derivations do not mo-
nopolize our supply of function symbols, we can easily arrange that primitive
recursive function symbols are chosen from an appropriate subsequence of the
â€œstandard function symbol sequence f n
i â€ of p. 166. We ï¬x the following very
convenient scheme that is informed by the table on p. 138.
Let a be the largest f -index used to allocate all the ï¬nitely many function
symbols required by LN (three) and the ï¬nitely many required for the introduc-
tion of Î² and the âŸ¨. . .âŸ©coding.
We let, for convenience, b = a + 1, and we allocate the formal primi-
tive recursive function symbols from among the members of the subsequence
( f n
bk)kâ‰¥0, being very particular about the k-value (k-code) chosen (p. 138):
(1) k = âŸ¨0, 1, 0âŸ©is used for Z (n = 1, of course).
(2) k = âŸ¨0, 1, 1âŸ©is used for S.â€ 
(3) k = âŸ¨0, n, i, 2âŸ©is used for U n
i . That is, U n
i is allocated as
f n
bâŸ¨0,n,i,2âŸ©
â€  It does no harm that S is already allocated as f 1
0 . After all, every â€œrealâ€ primitive recursive
function â€“ i.e., function viewed extensionally as a set of input-output pairs â€“ has inï¬nitely many
different derivations, and hence inï¬nitely many function symbols allocated to it.

II.3. Formal Primitive Recursion
255
(4) k = âŸ¨1, m, f, g1, . . . , gnâŸ©is used if f m
bk is allocated to denote the result of
composition from function symbols (already allocated) with k-codes equal
to f, g1, . . . , gn. Of these, the symbol with code f must have arity n; all
the others, arity m.
(5) k = âŸ¨2, n + 1, h, gâŸ©is used if f n+1
bk
is allocated to denote the result of
primitive recursion from function symbols (already allocated) with k-codes
h, g. Of these the ï¬rst must have arity n, the second n + 2.
This allocation scheme still leaves an inï¬nite supply of unused (so far)
symbols to be used for future extensions of the language.
As a parting comment we note that the seemingly contrived allocation
scheme above forces the set of k-codes to be primitive recursive. (See Ex-
ercise II.13).
â–¡
By the proof of II.3.5, having the formal and informal derivations of the
boldface f and the lightface f â€œtrack each otherâ€ in the obvious way â€“ i.e.,
assemblingtheboldfaceversionexactlyasthelightfaceversionwasassembledâ€“
we obtain f Nâ€² = f , and
II.3.8 Theorem. If f is a formal primitive recursive function of arity n (in
LNâ€²), then, for all âƒ—an in Nn,
|=Nâ€² f (âƒ—an) =b
implies
âŠ¢f (âƒ—an) =b
II.3.9 Corollary. The â€œimpliesâ€ in II.3.8 can be strengthened to â€œiffâ€. That is,
every informal primitive recursive function is strongly term-deï¬nable (p. 187),
indeed by a primitive recursive term.
Proof. Assume âŠ¢f (âƒ—an) =b. Writing f for f Nâ€², assume f (âƒ—an) = c Ì¸= b. Then
also âŠ¢f (âƒ—an) =c; hence âŠ¢b =c. Now, b Ì¸= c yields (already in ROB)
âŠ¢Â¬b =c, contradicting consistency of PAâ€².
â–¡
We are reminded that ROB and PA are consistent (since they each have a
model), a fact that we often use implicitly.â€ 
It is trivial then that each recursive extension PAâ€² is also consistent, for such
an extension is conservative.
â€  We base this assertion, of course, on the existence of a standard model N, a fact that provides
a non-constructive proof of consistency. Constructive proofs of the consistency of ROB and PA
are also known. See for example Shoenï¬eld (1967), SchÂ¨utte (1977).

256
II. The Second Incompleteness Theorem
II.3.10 Corollary. For every closed primitive recursive term t there is a unique
n âˆˆN such that âŠ¢t = n.
Proof. Existence is by II.3.8 (use n = tNâ€²). Uniqueness follows from deï¬n-
ability of x = y in ROB (I.9.39, p. 181) and hence in any extension PAâ€².
â–¡
Pause. Is it true that every closed term in a recursive extension of PA is provably
equal to a numeral?
II.3.11 Corollary. Every primitive recursive relation is strongly deï¬nable by
some formal primitive recursive predicate.
II.4. The Boldface âˆ†and Î£
II.4.1 Deï¬nition. Let LA be a language of arithmetic. The symbol Î£1(LA)
denotes the set of formulas {(âˆƒx)A : A âˆˆâˆ†0(LA)}.
If the language is LN, then we simply write Î£1.
â–¡
Usage of boldface type in Î£1 should distinguish this set of formulas over LN
from the set of 1-relations of the arithmetic hierarchy.
We also deï¬ne variants of âˆ†0 and Î£1 above.
II.4.2 Deï¬nition. Let LA be a language of arithmetic. The symbol âˆ†+
0 (LA) de-
notes the smallest set of formulas over LA that includes the atomic formulas, but
also the negations of atomic formulas, and moreover satisï¬es the closure con-
ditions: If A and B are in âˆ†+
0 (LA), then so are A âˆ¨B , A âˆ§B , (âˆƒx)<tA,
and (âˆ€x)<tA (where we require that the variable x not occur in the term t).
If the language is LN, then we simply write âˆ†+
0 .
â–¡
II.4.3 Deï¬nition. Let LA be a language of arithmetic. The symbol âˆ†â€²
0(LA)
denotes the smallest set of formulas over LA that includes the restricted atomic
formulas (deï¬ned below) â€“ and their negations â€“ and moreover satisï¬es the
closure conditions: If A and B are in âˆ†â€²
0(LA), then so are A âˆ¨B , A âˆ§B ,
(âˆƒx)<yA and (âˆ€x)<yA (where x Ì¸â‰¡y).
Correspondingly, the symbol Î£â€²
1(LA) denotes the set of formulas {(âˆƒx)A :
A âˆˆâˆ†â€²
0(LA)}.
If the language is LN, then we simply write âˆ†â€²
0 and Î£â€²
1.
Now, the restricted atomic formulas over LA are x = y, 0 = y, fâƒ—xn = y,
and Pâƒ—xn for all n-ary function ( f ) and predicate (P) symbols.
â–¡

II.4. The Boldface  and 
257
The restriction on the atomic formulas above was to have function and predicate
letters act on variables (rather than arbitrary terms). Similarly, we have used
(âˆƒx)<y and (âˆ€x)<y rather than (âˆƒx)<t and (âˆ€x)<t in II.4.3.
The superscript â€œ+â€ in II.4.2 is indicative of the (explicit) presence of only
positive closure operations (Â¬ does not participate in the deï¬nition). The same
is true of the âˆ†â€²
0(LA) formulas.
It turns out that both âˆ†+
0 (LA) and âˆ†â€²
0(LA) formulas are closed under nega-
tion (in the deï¬nition of these sets of formulas the application of â€œÂ¬â€ has been
pushed as far to the right as possible). We prove this contention below. The
introduction of âˆ†+
0 (LA) and âˆ†â€²
0(LA) is only offered for convenience (proof
of II.4.12 below). Neither symbol is standard in the literature.
II.4.4 Lemma. For any A âˆˆâˆ†+
0 (LA) (respectively, A âˆˆâˆ†â€²
0(LA)) there is a
B âˆˆâˆ†+
0 (LA) (respectively, B âˆˆâˆ†â€²
0(LA)) such that âŠ¢Â¬A â†”B , where â€œâŠ¢â€
denotes logical (pure) provability.
Proof. We do induction on âˆ†+
0 (LA) (respectively, âˆ†â€²
0(LA); the induction vari-
able is A).
Basis.
If A is atomic (or restricted atomic), then we are done at once. If
it is a negated atomic (or negated restricted atomic) formula, then we are done
by |=Taut B â†”Â¬Â¬B .
Now A can have the following forms (if not atomic or negated atomic) by
II.4.2â€“II.4.3:
(i) A â‰¡B âˆ¨C .
Then âŠ¢Â¬A â†”Â¬B âˆ§Â¬C , and we are done by the I.H.
via the Leibniz rule.
(ii) A â‰¡B âˆ§C .
Then âŠ¢Â¬A â†”Â¬B âˆ¨Â¬C , and we are done by the I.H.
via the Leibniz rule.
(iii) A â‰¡(âˆƒx)<tB .
Then âŠ¢Â¬A â†”(âˆ€x)<tÂ¬B , and we are done by the
I.H. via the Leibniz rule.
(iv) A â‰¡(âˆ€x)<tB .
Then âŠ¢Â¬A â†”(âˆƒx)<tÂ¬B , and we are done by the
I.H. via the Leibniz rule.
â–¡
II.4.5 Corollary. For any A âˆˆâˆ†0(LA) there is a B âˆˆâˆ†+
0 (LA) such that we
can prove A â†”B without nonlogical axioms.
Conversely, every B âˆˆâˆ†+
0 (LA) is a formula of âˆ†0(LA).
II.4.6 Lemma. Let A and B be in Î£1(LA) (respectively, Î£â€²
1(LA)). Then
each of the following is provably equivalent in PAâ€² to a formula in Î£1(LA)

258
II. The Second Incompleteness Theorem
(respectively, Î£â€²
1(LA)):
(i) A âˆ¨B
(ii) A âˆ§B
(iii) (âˆƒx)A
(iv) (âˆƒx)<zA
(v) (âˆ€x)<zA.
By PAâ€² over LA, above, we understand an extension by deï¬nitions of PA over
LN. (In this connection cf. II.1.2, p. 207).
Proof. The proof is a straightforward formalization of the techniques employed
in the proof of I.8.48. The only case that presents some interest is (v), and we
give a proof here. The proof hinges on the factâ€ 
âŠ¢(âˆ€x)<z(âˆƒy)B â†”(âˆƒw)(âˆ€x)<z(âˆƒy)<wB
where w is a new variable.
The â†-direction of the above being trivial, we just prove
âŠ¢(âˆ€x)<z(âˆƒy)B â†’(âˆƒw)(âˆ€x)<z(âˆƒy)<wB
(1)
by induction on z.â€¡
The basis, z = 0 is settled by < 1. Take now (1), with frozen variables, as
the I.H. Add the assumption
(âˆ€x)<Sz(âˆƒy)B
(2)
We want
âŠ¢(âˆƒw)(âˆ€x)<Sz(âˆƒy)<wB
(3)
By (2) and specialization,
âŠ¢x < z âˆ¨x = z â†’(âˆƒy)B
(4)
Since âŠ¢x < z â†’x < z âˆ¨x = z and âŠ¢x = z â†’x < z âˆ¨x = z, (4) yields
âŠ¢x < z â†’(âˆƒy)B
(5)
â€  If we set A â‰¡(âˆƒy)B , where B âˆˆâˆ†0(LA), then (âˆƒw)(âˆ€x)<z(âˆƒy)<wB is the Î£1(LA) formula
we want in order to establish (v).
â€¡ The reader who has had some axiomatic set theory will notice the remarkable similarity of (1)
with the axiom of collection (cf. volume 2, Chapter III). Indeed, (1) interprets collection, if we
interpret the basic predicate â€œâˆˆâ€ of set theory as the predicate â€œ<â€ of arithmetic.

II.4. The Boldface  and 
259
and
âŠ¢x = z â†’(âˆƒy)B
(6)
By (5) and the I.H. we obtain
âŠ¢(âˆƒw)(âˆ€x)<z(âˆƒy)<wB
(7)
By (6) we obtain âŠ¢(âˆƒy)B [x â†z]; hence
âŠ¢(âˆƒw)(âˆƒy)<wB [x â†z]
(8)
Pause. Why is (8) true? Well, it follows immediately provided we believe
âŠ¢(âˆƒy)B â†’(âˆƒw)(âˆƒy)<wB
To establish the above let (âˆƒy)B [y]. Add now B [c], where c is a new constant.
Then âŠ¢c < Sc âˆ§B [c]; hence âŠ¢(âˆƒy)(y < Sc âˆ§B ) (by Ax2) and thus âŠ¢
(âˆƒw)(âˆƒy)(y < w âˆ§B ) (by Ax2 again).
Now, arguing by auxiliary constant once more, relying on (7) and (8), we
add new constants a and b and the assumptions
(âˆ€x)<z(âˆƒy)<aB
(7â€²)
and
(âˆƒy)<bB [x â†z]
(8â€²)
By the Leibniz axiom (Ax4) and tautological implication, (8â€²) yields
âŠ¢x = z â†’(âˆƒy)<bB
(9)
On the other hand, we obtain from (7â€²)
âŠ¢x < z â†’(âˆƒy)<aB
(10)
Now set c = S(a + b). Proof by cases from (9) and (10) yieldsâ€ 
âŠ¢x < z âˆ¨x = z â†’(âˆƒy)<cB
Hence
âŠ¢(âˆ€x)x<Sz(âˆƒy)<cB
By Ax2, (3) follows.
â–¡
â€  Via the obvious âŠ¢(âˆƒy)<aB â†’(âˆƒy)<cB and âŠ¢(âˆƒy)<bB â†’(âˆƒy)<cB , obtained from
II.1.17, tautological implication, and âˆƒ-monotonicity (I.4.23).

260
II. The Second Incompleteness Theorem
II.4.7 Lemma. For any A âˆˆâˆ†0(LA) (respectively, A âˆˆâˆ†â€²
0(LA)) there
is a provably equivalent â€“ in pure logic â€“ B âˆˆÎ£1(LA) (respectively,
B âˆˆÎ£â€²
1(LA)).
Proof. Let x be a variable that is not free in A. Then
(a) (âˆƒx)A âˆˆÎ£1(LA) (respectively, (âˆƒx)A âˆˆÎ£â€²
1(LA)), and
(b) âŠ¢(âˆƒx)A â†”A.
â–¡
II.4.8 Lemma. For any A âˆˆâˆ†0(LA) there is a B âˆˆÎ£â€²
1(LA) such that
âŠ¢PAâ€² A â†”B
The absence of a prime from âˆ†0(LA) is intentional. PAâ€² is as in II.4.6
Proof. InviewofII.4.5,wedoinductiononthedeï¬nitionofâˆ†+
0 (LA)(induction
variable is A).
Basis.
Atomic formulas of type t = s: We ï¬rst look at the subcase t = y.
We embark on an informal induction on the formation of t.
For the basis we have two cases: t â‰¡x and t â‰¡0. Both lead to restricted
atomic formulas (II.4.3), and we are done by II.4.7. If now t â‰¡f t1 . . . tn, then
(one point rule, I.7.2)
âŠ¢f t1 . . . tn = y â†”
(âˆƒx1) . . . (âˆƒxn)( t1 = x1
  
I.H.
âˆ§. . . âˆ§tn = xn
  
I.H.
âˆ§f x1 . . . xn = y



âˆ†â€²
0(LA)
)
where the xi are new variables. By the I.H. on terms, the Leibniz rule, and II.4.6â€“
II.4.7 we are done.
We can now conclude the t = s case:
âŠ¢t = s â†”(âˆƒy)(t = y âˆ§s = y),
where y is a new variable.
Basis.
Atomic formulas of type Pt1 . . . tn: Done by
âŠ¢Pt1 . . . tn â†”
(âˆƒx1) . . . (âˆƒxn)(t1 = x1 âˆ§. . . âˆ§tn = xn âˆ§Px1 . . . xn



âˆ†â€²
0(LA)
)
Basis.
Negated atomic formulas of type Â¬t = s:
âŠ¢Â¬ t = s â†”(âˆƒx)(âˆƒy)(t = x âˆ§s = y âˆ§Â¬x = y
  
âˆ†â€²
0(LA)
)

II.4. The Boldface  and 
261
Basis.
Negated atomic formulas of type Â¬Pt1 . . . tn:
âŠ¢Â¬Pt1 . . . tn â†”
(âˆƒx1) . . . (âˆƒxn)(t1 = x1 âˆ§. . . âˆ§tn = xn âˆ§Â¬Px1 . . . xn



âˆ†â€²
0(LA)
)
The induction steps are for âˆ¨, âˆ§, (âˆƒx)<t, (âˆ€x)<t and follow at once from
II.4.6 (i), (ii), (iii), (iv), and (v), and (âˆƒx)<tA â†”(âˆƒz)(z = t âˆ§(âˆƒx)<zA).
â–¡
II.4.9 Corollary. For any A âˆˆÎ£1(LA) there is a provably (in PAâ€²) equivalent
B âˆˆÎ£â€²
1(LA).
Proof. II.4.8 and II.4.6(iii).
â–¡
II.4.10 Lemma. Let PAâ€² over LAâ€² be a recursive extension of PA. Then for
each A âˆˆÎ£1(LAâ€²) there is a formula B âˆˆÎ£1(LN) such that âŠ¢PAâ€² A â†”B .
Proof. It sufï¬ces to prove that if LAâ€² is obtained from LA by the addition of
either a single function or a single predicate symbol, and the deï¬ning axiom
was added to a recursive extension T (over LA) of PA yielding PAâ€², then for
each A âˆˆÎ£1(LAâ€²) there is a formula B âˆˆÎ£1(LA) such that âŠ¢PAâ€² A â†”B .
In view of II.4.9 and II.4.6(iii) it sufï¬ces to prove this latter claim just for
all A âˆˆâˆ†â€²
0(LAâ€²). We do induction on the deï¬nition of âˆ†â€²
0(LAâ€²) (induction
variable: A).
Basis.
Restricted atomic cases and their negations. All cases are trivial
(II.4.7), except fâƒ—xn = y, Â¬ fâƒ—xn = y, Pâƒ—xn, and Â¬Pâƒ—xn, when f , or P, is the
new symbol.
Say we have the case Â¬ fâƒ—xn = y, where
fâƒ—xn = (Âµz)B
and B âˆˆâˆ†0(LA).
Note the absence of a prime from A in âˆ†0(LA). â€œâŠ¢â€ below is â€œâŠ¢PAâ€²â€.
Thus,
âŠ¢fâƒ—xn = y â†”B [y] âˆ§(âˆ€z)<yÂ¬B [z]
and therefore
âŠ¢Â¬ fâƒ—xn = y â†”(âˆƒw)(Â¬y = wâˆ§B [w] âˆ§(âˆ€z)<wÂ¬B [z])
The right hand side of â€œâ†”â€ above is in Î£1(LA).

262
II. The Second Incompleteness Theorem
If on the other hand P is the new symbol, then we have
Pâƒ—xn â†”B
and B âˆˆâˆ†0(LA), and this rests the case via II.4.7.
The induction steps are (II.4.3) for âˆ¨, âˆ§, (âˆƒx)<z, (âˆ€x)<z and follow at once
from II.4.6 (i), (ii), (iv), and (v) and the standard technique of eliminating
deï¬ned symbols (at the atomic formula level).
â–¡
II.4.11 Corollary. Let PAâ€² over LAâ€² be a recursive extension of PA. Then for
each A âˆˆÎ£1(LAâ€²) there is a formula B âˆˆÎ£â€²
1(LN) such that âŠ¢PAâ€² A â†”B .
Proof. II.4.10 guarantees a C âˆˆÎ£1(LN) such that âŠ¢PAâ€² A â†”C . Now II.4.9
provides the B âˆˆÎ£â€²
1(LN) we want (recall that PAâ€² extends PA).
â–¡
II.4.12 Theorem. Let PAâ€² over LNâ€² be a recursive extension of PA, and A a
sentence in Î£1(LNâ€²). Then |=Nâ€² A implies that âŠ¢PAâ€² A.
Or, â€œany really true sentence of Î£1(LNâ€²) is provableâ€.
Proof. By II.4.11 there is a sentence C âˆˆÎ£â€²
1(LN) (note the absence of a prime
from N) such that
âŠ¢PAâ€² A â†”C
(1)
Thus (do you believe this?)
|=Nâ€² A â†”C
Hence
|=Nâ€² C
Now, there is a formula B (x) âˆˆâˆ†â€²
0(LN) such thatâ€ 
C â‰¡(âˆƒx)B (x)
Hence
|=Nâ€² (âˆƒx)B (x)
Therefore, taking reducts,
|=N (âˆƒx)B (x)
(2)
â€  â‰¡means string equality.

II.4. The Boldface  and 
263
By (2), (B (n))N = t for some n âˆˆN; hence (p. 171)
(B (n))N = t
(3)
By (3), and an easy induction on the structure of B (n) (Deï¬nition II.4.3),
following the steps of the proof of I.9.48 (p. 185),
âŠ¢ROB B (n)
Hence âŠ¢PAâ€² (âˆƒx)B (x) by Ax2. By (1), âŠ¢PAâ€² A.
â–¡
II.4.13 Example. Equipped with II.4.12, one can now readily answer the
â€œwhyâ€s that were embedded in Example II.2.7.
â–¡
II.4.14 Remark. What II.4.12 above â€“ and, earlier on, II.3.8 â€“ do for us, prac-
tically, is to eliminate the need for formal proofs of â€œtrueâ€ Î£1-sentences A,
proofs that would be normally carried out in tedious detail within some recur-
sive extension of PA. This is achieved by a two-pass process:
(1) We somehow convince ourselves that A is â€œtrueâ€, i.e., true in N or in some
expansion thereof that accommodates whatever deï¬ned symbols are used
inside the sentence.
(2) We then invoke (Meta)theorem II.4.12, which guarantees âŠ¢A, without our
having to write down a single line of a formal proof!
The reader may object: Obviously all the work was shifted to item (1). But
how does one â€œproveâ€ informally the â€œtruthâ€ of a sentence? Does it not neces-
sitate as much work â€“ in an informal deductionâ€  â€“ as a formal proof does? For
example, that lcm{2, 3, 4, 5} = 60 is deï¬nitely obvious, but just proclaiming
so does not prove this fact. How do we know that it is true?
The explanation is not mathematical, but it rather hinges on the sociology of
informal proofs. An informal proof, viewed as a social activity, ends as soon as
a reasonably convincing case has been made. Informal proofs are often sketchy
(hence shorter than formal proofs), and the participants (prover and reader)
usually agree that a certain level of detail can be left untold,â€¡ and are also
prepared to accept â€œthe obviousâ€ â€“ the latter being informed by a vast database
of â€œrealâ€ mathematical knowledge that goes all the way back to oneâ€™s primary
school years.
â€  Ofcourse, onewould neverthinkofestablishingtruthinmathematicalpracticepurelybysemantic
means, for this would involve messy inï¬nitary arguments, while deductions, even informal ones,
have the advantage of being ï¬nitary.
â€¡ We are guilty of often leaving details for the reader to work out in our formal proofs. This dilutes
the formal proof by an informality that we exercise for pedagogical reasons.

264
II. The Second Incompleteness Theorem
Surely sometime in the past we have learnt how to compute the lcm, or the
gcd, of a set of numbers, or that 13 + 7 = 20 is â€œtrueâ€. We retrieve all this
from the â€œdatabaseâ€. Of course, a formal proof in ROB of, say, 
13 +7 = 
20 is
another matter, and is certainly not totally painless, as the reader who has read
the lemma on the deï¬nability of x + y = z (I.9.41, p. 181) will agree.â€ 
Thus, deductions in â€œrealâ€ mathematics need only be as long (i.e., as de-
tailed) as necessary for their acceptability. Indeed, one often ï¬nds that the level
of detail included in various informal proofs of the same result is inversely pro-
portional to the mathematical sophistication of the targeted audiences in each
presentation.â€¡ By contrast, formal proofs are, by deï¬nition (cf. I.3.17, p. 37),
audience-independent.
Back to practice: Having performed step (1) above, we can now do one of
two things: We can decide not to cheat by using step (2), but write down instead
a formal argument that translates the informal one of step (1), treating the latter
merely as a set of â€œorganizational notesâ€.Â§
Or, we can take the easy way out and invoke step (2). This avenue establishes
the formal result entirely metamathematically. We have shown that a formal
proof exists, without constructing the proof.
The approach is analogous to that of employing Churchâ€™s thesis to informally
â€œproveâ€ results in recursion theory. This thesis says: â€œIf we have shown by an
informal argument that a partial function f is computable in the intuitive sense â€“
for example, we have written informal instructions for its computation â€“ then
this f is partial recursive, that is, a formal algorithm for its computation exists
(e.g., an algorithm formalized as a Turing machine, or as a Kleene schemata
description)â€. We do not need to exhibit this formal algorithm.
Compare with â€œIf we have shown by an informal argument that a sentence
A among the types allowed in II.4.12 and II.3.8 is true in the standard struc-
ture, then there exists a proof for this A in (some conservative extension of)
PAâ€.
However, even though Churchâ€™s thesis and II.4.12 and II.3.8 are applied
in a similar manner, there is a major difference between them: The former
â€  It is normal (sloppy) practice to invoke â€œgeneralâ€ results where it would have been more appro-
priate, in order to avoid circularity, not to do so. For example, we may claim that (for any ï¬xed a
and b in N) the â€œtrueâ€ sentence a +b = 
a + b is provable, by invoking II.4.12 or II.3.8. Correct
practice would have been to say that the provability of the sentence is due to I.9.41, since the
latter was used towards the establishment of II.4.12 and II.3.8. But this puts a heavier burden on
memory.
â€¡ For example, it is held that part of the reason that GÂ¨odel never published a complete proof of his
second incompleteness theorem was that the result was readily believed by his targeted audience.
Â§ This is entirely analogous to what a computer programmer might do. He would ï¬rst develop
pseudocode (an informal program) towards the solution of a problem. He would then translate
the pseudocode to a formal computer program written in the appropriate programming language.

II.5. Arithmetization
265
is only a belief based on empirical evidence,â€  while the latter two are meta-
theorems.
Unlike â€œtrueâ€ existential sentences, â€œtrueâ€ universal, or Î 1, sentences (of
arithmetic) are not necessarily provable, as GÂ¨odelâ€™s ï¬rst incompleteness theo-
rem tells us. The Î 1-formulas are those of the form Â¬A where A is Î£1.
For example, if T is the formal Kleene predicate, then there are inï¬nitely
many â€œtrueâ€ Î 1-sentences of the form Â¬(âˆƒy)T(a,a, y) that are not provable
(cf. (3) in I.9.37).
â–¡
II.5. Arithmetization
We now resume and conclude the discussion on the arithmetization of formal
arithmetic(s) that we have started in Section I.9 (p. 168). The arithmetization is
really a package that includes the GÂ¨odel numbers of terms and formulas on one
hand, and, on the other hand, a ï¬nite suite of formal predicates and functions
introduced to test for properties of GÂ¨odel numbers (e.g., testing whether x is a
number for a formula, term, variable, etc.).
This package will be totally contained inside an appropriate recursive exten-
sion of PA over the appropriate LNâ€² that contains all the tools (such as â€œâŸ¨. . . âŸ©â€,
etc.) needed to form GÂ¨odel numbers as on p. 168 and all the additional test
predicates and functions (and their deï¬ning axioms).
GÂ¨odel numbers, â€œâŒœ. . . âŒâ€, will be certain closed terms over LNâ€² rather
than â€œrealâ€ natural numbers from N. We will rely on the numbering given on
p. 168; however, we shall now employ the formal minimum coding âŸ¨. . . âŸ©given
by (FC) on p. 239 and also make the following trivial amendment to notation:
Every occurrence of a speciï¬c natural number n inside âŸ¨. . . âŸ©brackets is now
changed to the term denoted by n (numeral).
We next introduce the testing tools, that is, a ï¬nite sequence of atomic
formulas and terms (introducing appropriate predicate and function symbols in
a manner that respects the rules for recursive extensions) that enable the theory
to â€œreason aboutâ€ formulas (using GÂ¨odel numbers as aliases of such formulas).
â€  In some contexts â€“ such as when partial function oracles are allowed â€“ there is evidence to
the contrary: If L(x, Î±, y) is the relation that says â€œprogram x with input the oracle Î± has a
computation of length less than yâ€, then this is intuitively computable: Just let program x crank
with input Î± and keep track of the number of steps. If the program halts in fewer than y steps,
then stop everything and return â€œyesâ€; otherwise, if x has already performed the yth step, stop
everything and return â€œnoâ€. Now, if Î± and Î² are partial functions, Î± âŠ†Î² does not guarantee that
L(x, Î±, y) and L(x, Î², y) yield the same answer, that is, L is non-monotone, or inconsistent. In
most foundations of computability inconsistent relations such as L are not allowed, i.e., L is not
formally computable in such theories; hence Churchâ€™s thesis fails with respect to such theories.
This particular â€œnegative evidenceâ€ is eliminated if we use a different foundation of computability
that was introduced in Tourlakis (1986, 1996, 2001a). Now L is formally computable. See also
KalmÂ´arâ€™s (1957) objections.

266
II. The Second Incompleteness Theorem
In each case we precede the deï¬nition with a comment stating the intended
meaning.
Var(x, i) holds if â€œx = âŒœviâŒâ€.
Var(x, i) â†”Seq(x) âˆ§lh(x) = i +4 âˆ§(x)0 = S0
âˆ§(x)S0 = âŒœ(âŒâˆ§(x)i + SSS0 = âŒœ)âŒ
âˆ§(âˆ€z)<lh(x)(S0 < z âˆ§z < i +3 â†’(x)z = âŒœâ–¡âŒ)
(Var)
We prefer to write, say, â€œ. . . = âŒœ(âŒâ€ rather than â€œ. . . = âŸ¨0,9âŸ©â€.
Func(x, i, j) holds if â€œx = âŒœf j
i âŒâ€.
Func(x, i, j) â†”Seq(x) âˆ§lh(x) = i + j +6 âˆ§(x)0 = 2
âˆ§(x)S0 = âŒœ(âŒâˆ§(x)i+ j+5 = âŒœ)âŒ
âˆ§(x)i+3 = âŒœ#âŒ
(Func)
âˆ§(âˆ€z)<lh(x)(S0 < z âˆ§z < i + j +5
âˆ§Â¬z = i +3
â†’(x)z = âŒœâ–³âŒ)
Pred(x, i, j) means â€œx = âŒœP j
i âŒâ€.
Pred(x, i, j) â†”Seq(x) âˆ§lh(x) = i + j +6 âˆ§(x)0 = 3
âˆ§(x)S0 = âŒœ(âŒâˆ§(x)i+ j+5 = âŒœ)âŒ
âˆ§(x)i+3 = âŒœ#âŒ
(Pred)
âˆ§(âˆ€z)<lh(x)(S0 < z âˆ§z < i + j +5
âˆ§Â¬z = i +3
â†’(x)z = âŒœâƒâŒ)
Before we proceed with terms and other constructs we introduce the lower-
case versions of the above predicates:
var(x)
â†”(âˆƒi)â‰¤xVar(x, i)
holds if â€œx = âŒœviâŒ, for some iâ€
f unc(x, n) â†”(âˆƒi)â‰¤xFunc(x, i, n)
holds if â€œx = âŒœf n
i âŒ, for some iâ€
pred(x, n) â†”(âˆƒi)â‰¤x Pred(x, i, n)
holds if â€œx = âŒœPn
i âŒ, for some iâ€
Term is a new function symbol introduced by course-of-values recursion
(Theorem II.3.3, p. 251) below. Term(âŒœtâŒ) = 0 means â€œt is a termâ€.
On the other hand, the statement â€œTerm(x) = 0 implies that â€˜x = âŒœtâŒfor some
term tâ€™ â€ is not a fair description of what â€œTermâ€ does. That is, after establishing
that Term(x) = 0, we can only infer that x â€œbehaves likeâ€ the GÂ¨odel number
of some term, not that it is the GÂ¨odel number of some term. See Lemma II.6.24

II.5. Arithmetization
267
(p. 297) later on, and the comment following it. If â€œimpliesâ€ is not apt, then we
cannot say â€œmeansâ€ either, for the latter is argot for equivalence. We can say
â€œholds ifâ€, though.
Similar caution must be exercised when interpreting the remaining functions
and predicates introduced in this section.
Term(x) =
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³
0
if var(x) âˆ¨x = âŸ¨0, S0âŸ©
âˆ¨Seq(x) âˆ§(âˆƒy)<x(lh(x) = Sy âˆ§f unc((x)0, y)
âˆ§(âˆ€z)<yTerm((x)Sz) = 0)
S0
otherwise
(Trm)
AF(âŒœAâŒ) says â€œA is atomicâ€.
AF(x) â†”(âˆƒy)<x(âˆƒz)<x(Term(y) = 0 âˆ§Term(z) = 0
âˆ§x = âŸ¨âŒœ= âŒ, y, zâŸ©)
âˆ¨Seq(x)âˆ§(âˆƒy)<x(lh(x) = Sy âˆ§pred((x)0, y)
âˆ§(âˆ€z)<yTerm((x)Sz) = 0)
(Af)
WFF is a new function symbol introduced by course-of-values recursion
below. WFF(âŒœAâŒ) = 0 means â€œA âˆˆWff â€:
WFF(x) =
ï£±
ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£³
0
if AF(x) âˆ¨Seq(x) âˆ§(lh(x) =3 âˆ§[(x)0 = âŒœâˆ¨âŒ
âˆ§WFF((x)S0) = 0 âˆ§WFF((x)SS0) = 0
âˆ¨(x)0 = âŒœâˆƒâŒâˆ§var((x)S0) âˆ§WFF((x)SS0) = 0]
âˆ¨lh(x) =2 âˆ§(x)0 = âŒœÂ¬âŒâˆ§WFF((x)S0) = 0)
S0
otherwise
(Wff)
We now introduce a new function symbol, Free, such that Free(i, x) = 0
intuitively says â€œif x is the GÂ¨odel number of a term or formula E, then vi occurs
free in Eâ€ (see I.1.10, p. 18).
Free(i, x) =
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
0
if Var(x, i) âˆ¨
%
AF(x) âˆ¨(Term(x) = 0 âˆ§(âˆƒy)<x f unc((x)0, y))
&
âˆ§
(âˆƒy)<lh(x)(0 < y âˆ§Free(i, (x)y) = 0) âˆ¨
WFF(x) = 0 âˆ§(x)0 = âŒœÂ¬âŒâˆ§Free(i, (x)S0) = 0 âˆ¨
WFF(x) = 0 âˆ§(x)0 = âŒœâˆ¨âŒâˆ§
(Free(i, (x)S0) = 0 âˆ¨Free(i, (x)SS0) = 0) âˆ¨
WFF(x) = 0 âˆ§(x)0 = âŒœâˆƒâŒâˆ§
Â¬Var((x)S0, i) âˆ§Free(i, (x)SS0) = 0
S0
otherwise
(Fr)

268
II. The Second Incompleteness Theorem
We next introduce GÂ¨odelâ€™s substitution function, a precursor of Kleeneâ€™s S-m-n
functions. We introduce a new function symbol by course-of-values recursion.
Sub(x, y, z) will have the following intended effect:
Sub(âŒœt[vi]âŒ,i, âŒœsâŒ) = âŒœt[s]âŒ
and
Sub(âŒœA[vi]âŒ,i, âŒœsâŒ) = âŒœA[s]âŒ
However, in the latter case, the result will be as described iff s is substitutable in
vi. Otherwise the result 0 will be returned (a good choice for â€œnot applicableâ€,
since no GÂ¨odel number equals 0).
This will make the deï¬nition a bit more complicated than usual.â€  Our deï¬-
nition of Sub below tracks I.3.11, p. 32.
Sub(x, i, z) =
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
z
if Var(x, i)
x
if x = âŸ¨0, S0âŸ©
x
if var(x) âˆ§Â¬Var(x, i)
(Âµw)
%
Seq(w) âˆ§lh(w) = lh(x) âˆ§(w)0 = (x)0 âˆ§
(âˆ€y)<lh(x)(y > 0 â†’(w)y = Sub((x)y, i, z))
&
if (Term(x) = 0 âˆ§(âˆƒy)<x f unc((x)0, y)) âˆ¨AF(x)
âŸ¨(x)0, Sub((x)S0, i, z)âŸ©
if WFF(x) = 0 âˆ§(x)0 = âŒœÂ¬âŒâˆ§
Sub((x)S0, i, z) > 0
âŸ¨(x)0, Sub((x)S0, i, z), Sub((x)SS0, i, z)âŸ©
if WFF(x) = 0 âˆ§(x)0 = âŒœâˆ¨âŒâˆ§
Sub((x)S0, i, z) > 0 âˆ§Sub((x)SS0, i, z) > 0
x
if WFF(x) = 0 âˆ§(x)0 = âŒœâˆƒâŒâˆ§Var((x)S0, i)
âŸ¨(x)0, (x)S0, Sub((x)SS0, i, z)âŸ©
if WFF(x) = 0 âˆ§(x)0 = âŒœâˆƒâŒâˆ§Â¬Var((x)S0, i)
âˆ§Sub((x)SS0, i, z) > 0 âˆ§
(âˆƒj)â‰¤(x)S0(Var((x)S0, j) âˆ§Free( j, z) > 0)
0
otherwise
(Sub)
A two variable version, the family of the â€œlowercase subiâ€, for i âˆˆN, is also
useful:
subi(x, z) = Sub(x,i, z)
(subi)
For any terms t[vm], s and formula A[vm], we will (eventually) verify that
âŠ¢subm(âŒœt[vm]âŒ, âŒœsâŒ) = âŒœt[s]âŒand âŠ¢subm(âŒœA[vm]âŒ, âŒœsâŒ) = âŒœA[s]âŒ, as-
suming that s is substitutable for vm in A.
â€  In the literature â€“ e.g., Shoenï¬eld (1967), SmoryÂ´nski (1978, 1985) â€“ one tends to deï¬ne Sub with
no regard for substitutability, i.e., pretending that all is O.K. One then tests for substitutability
via other predicates or functions that are subsequently introduced.

II.5. Arithmetization
269
In the next section we introduce (informally) yet another special case of sub
that has special signiï¬cance towards proving the second incompleteness theo-
rem. For now, we continue with the introduction of predicates that â€œrecognizeâ€
GÂ¨odel numbers of logical axioms.
The introductory remarks to Section II.1 announced that we will be making
an amendment to what the set  of logical axioms is, in order to make the
arithmetization more manageable.
Indeed, while keeping the schemata Ax2â€“Ax4 the same, we change group
Ax1 â€“ the â€œpropositional axiomsâ€ â€“ to consist of the four schemata below:
(1) A âˆ¨A â†’A
(2) A â†’A âˆ¨B
(3) A âˆ¨B â†’B âˆ¨A
(4) (A â†’B ) â†’(C âˆ¨A â†’C âˆ¨B ).
Of course, the new axiom group Ax1 is equivalent with the old one on p. 34,
as Exercises I.26â€“I.41 (pp. 193â€“195) in Chapter I show.
Thus we introduce predicates Propi (i = 1, 2, 3, 4) such that Propi(x) is
intended to hold if x is a GÂ¨odel number of a formula belonging to the schema (i),
i = 1, 2, 3, 4. Here is one case (case (4)); cases (1)â€“(3) are less work.
Prop4(x) â†”(âˆƒy)<x(âˆƒz)<x(âˆƒw)<x

WFF(y) = 0 âˆ§WFF(z) = 0 âˆ§
WFF(w) = 0 âˆ§
x =
.
âŒœâ†’âŒ,
)
âŒœâ†’âŒ, y, z
*
,
)
âŒœâ†’âŒ, âŸ¨âŒœâˆ¨âŒ, w, yâŸ©, âŸ¨âŒœâˆ¨âŒ, w, zâŸ©
*/
To simplify notation we have used the abbreviation â€œâŸ¨âŒœâ†’âŒ, a, bâŸ©â€ for the
fully explicit â€œâŸ¨âŒœâˆ¨âŒ, âŸ¨âŒœÂ¬ âŒ, aâŸ©, bâŸ©â€. This will recur in our descriptions of the
remaining axiom schemata.
We ï¬nally introduce
Prop(x) â†”Prop1(x) âˆ¨Prop2(x) âˆ¨Prop3(x) âˆ¨Prop4(x)
(Prop)
which is intended to hold if x is a GÂ¨odel number of an axiom of type Ax1 (as
the Ax1 group was amended above).
For Ax2 we introduce SubAx.
SubAx(x) holds if x = âŒœA[z â†t] â†’(âˆƒz)AâŒfor some formula A, vari-
able z, and term t that is substitutable for z in A. Thus,
SubAx(x) â†”(âˆƒy)<x(âˆƒz)<x(âˆƒw)<x(âˆƒi)<x
%
WFF(y) = 0 âˆ§
Term(w) = 0 âˆ§Var(z, i) âˆ§Sub(y, i, w) > 0 âˆ§
x = âŸ¨âŒœâ†’âŒ, Sub(y, i, w), âŸ¨âŒœâˆƒâŒ, z, yâŸ©âŸ©
&
(SubAx)

270
II. The Second Incompleteness Theorem
Note that Sub(y, i, w) > 0 â€œsaysâ€ that the term with GÂ¨odel number w is sub-
stitutable for vi in the formula with GÂ¨odel number y. We will verify this in the
next section, however the reader can already intuitively see that this is so from
the deï¬nition of Sub.
For the axioms Ax3 we introduce Id Ax:
Id Ax(x) holds if x = âŒœy = yâŒfor some variable y. Thus,
Id Ax(x) â†”Seq(x) âˆ§lh(x) = SSS0 âˆ§
(x)0 = âŒœ= âŒâˆ§(x)S0 = (x)SS0 âˆ§var((x)S0)
(IdAx)
Finally, for Ax4 we introduce Eq Ax. Eq Ax(x) holds if x = âŒœt = s â†’
(A[t] â†”A[s])âŒfor some formula A and substitutable terms t and s. Thus,
Eq Ax(x) â†”(âˆƒy)<x(âˆƒz)<x(âˆƒw)<x(âˆƒv)<x(âˆƒi)<x
%
WFF(y) = 0 âˆ§
Term(w) = 0 âˆ§Term(v) = 0 âˆ§Var(z, i) âˆ§
Sub(y, i, w) > 0 âˆ§Sub(y, i, v) > 0 âˆ§
x = âŸ¨âŒœâ†’âŒ, âŸ¨âŒœ= âŒ, w, vâŸ©, âŸ¨âŒœâ†”âŒ, Sub(y, i, w), Sub(y, i, v)âŸ©âŸ©
&
(EqAx)
Thus, we introduce LA by
LA(x) â†”Prop(x) âˆ¨SubAx(x) âˆ¨Id Ax(x) âˆ¨Eq Ax(x)
(LA)
LA(âŒœAâŒ) means â€œA âˆˆâ€.
We have two rules of inference. Thus, we introduce MP by
MP(x, y, z) â†”WFF(x) = 0 âˆ§WFF(z) = 0 âˆ§y = âŸ¨âŒœâ†’âŒ, x, zâŸ©
(MP)
We also introduce âˆƒ-introduction, EI, by
EI(x, y) â†”(âˆƒu)<x(âˆƒv)<x(âˆƒz)<x(âˆƒi)<x

x = âŸ¨âŒœâ†’âŒ, u, vâŸ©
âˆ§WFF(u) = 0 âˆ§WFF(v) = 0
âˆ§Var(z, i) âˆ§Free(i, v) > 0
âˆ§y = âŸ¨âŒœâ†’âŒ, âŸ¨âŒœâˆƒâŒ, z, uâŸ©, vâŸ©

(EI)
We will use one more rule of inference in the deï¬nition of Proof below.
This rule, substitution of terms, is, of course a derived rule (I.4.12, p. 44),
but we acknowledge it explicitly for our future convenience (Lemma II.6.27 â€“
via II.6.18 and II.6.20). Thus, we introduce SR. SR(x, y) holds if x = âŒœAâŒ
and y = âŒœA[vi â†t]âŒfor some term t substitutable in vi:
SR(x, y)â†”WFF(x) = 0 âˆ§WFF(y) = 0 âˆ§
(âˆƒi)â‰¤x(âˆƒz)â‰¤y(Term(z) = 0 âˆ§y = Sub(x, i, z))
(SR)

II.5. Arithmetization
271
II.5.1 Remark. In introducing MP, EI, and SRabove we have been consistent
with our deï¬nition of inference rules (I.3.15, p. 36) in that the rules act on
formulas.
It is technically feasible to have the rules act on any strings over our alphabet
insteadâ€“e.g.,viewmodusponensasactingonarbitrarystringsAand(A â†’B )
to produce the string B . Indeed the literature, by and large, takes this point of
view for the arithmetized versions above, deï¬ning, for example MPâ€² just as
MPâ€²(x, y, z) â†”y = âŸ¨âŒœâ†’âŒ, x, zâŸ©
(M Pâ€²)
Still, informally speaking, a proof built upon the primed versions of the rules of
inference is composed entirely of formulasâ€  â€“ as it should be â€“ for these rules
produce formulas if their inputs are all formulas, and, of course, a proof starts
with formulas (the axioms).
â–¡
We are ready to test for proofs. Fix a set of nonlogical axioms .
Suppose that we already have a formula Î“ such that Î“(âŒœAâŒ) means â€œA âˆˆâ€.
Then, we may introduce the informal abbreviation Proof such that
Proof (x) holds if x = âŸ¨âŒœA1 âŒ, âŒœA2 âŒ, . . . âŸ©where A1, A2, . . . is some -
proof.
Proof (x) stands for Seq(x) âˆ§lh(x) > 0 âˆ§
(âˆ€i)<lh(x)(LA((x)i) âˆ¨Î“((x)i) âˆ¨
(âˆƒj)<i(âˆƒk)<i MP((x) j, (x)k, (x)i) âˆ¨
(âˆƒj)<i EI((x) j, (x)i) âˆ¨
(âˆƒj)<i SR((x) j, (x)i))
(Proof)
Note that this introduction of Proof is modulo , but most of the time we will
not indicate that dependence explicitly (e.g., as Proof ), since whatever  we
have in mind will be usually clear from the context.
GÂ¨odelâ€™s theorem is about theories with a â€œrecognizableâ€ set of axioms, i.e.,
recursively axiomatized theories. However, it is also applicable if we take the
somewhat more general assumption that  is a semi-recursive set, which for-
mally means that we want the formula Î“ to be Î£1 over the language where
we effect our coding. Informally, this means that even if we cannot recognize
the axioms, nevertheless we can form an effective (algorithmic) listing of all of
them; GÂ¨odelâ€™s results still hold for such theories.â€¡
â€  As an easy informal induction on the length of proofs shows.
â€¡ It is easy to see that the the set of theorems of a theory with a semi-recursive set of axioms is
also semi-recursive by closure of semi-recursive relations under (âˆƒy). This immediately opens
any such theory for arithmetic to the incompletableness phenomenon, since the set of all true
sentences is not semi-recursive.

272
II. The Second Incompleteness Theorem
The reader will note that PA is much â€œsimplerâ€ than semi-recursive. Indeed,
it is not hard to show that if  = PA, then the formula Î“ is primitive recursive.â€ 
In this case, Î“(x) will be equivalent to a lengthy disjunction of cases. For
example, the case corresponding to axiom +1, z + 0 = z, is given by the (âˆ†0)
formula
(âˆƒz)<x

var(z) âˆ§x = âŸ¨âŒœ= âŒ, âŸ¨âŒœ+ âŒ, z, 0âŸ©, zâŸ©

We leave the details to the reader (Exercise II.15).
II.6. Derivability Conditions; Fixed Points
In this section we will ï¬nd a sentence in LN that says â€œI am not a theoremâ€.
We will also show that certain derivability conditions (the Ableitbarkeits-
forderungen,â€¡ of Hilbert and Bernays (1968)) hold in some appropriate ex-
tension, C, of PA. These conditions talk about deducibility (derivability) in
appropriate consistent extensions  of PA. At the end of all this, to prove the
second incompleteness theorem becomes a comparatively easy matter.
We begin by assessing the success of our arithmetization. We have made
claims throughout Section II.5 about what each introduced predicate or function
meant intuitively. We want to make precise and substantiate such claims now,
although we will not exhaustively check every predicate and function that we
have introduced. Just a few spot checks of the most interesting cases will sufï¬ce.
First off, we ï¬x our attention on an arbitrary consistent extension,  of PA, over
some language LA, where Î“ is Î£1 over the language of C below.
We also ï¬x an extension by deï¬nitionsÂ§ of PA, where all the predicates
and functions that we have introduced in our arithmetization along with their
deï¬ning axioms, reside. We will use the symbol C (â€œCâ€ for Coding) for this
speciï¬c extension. The language of C will be denoted by LN C.
As a matter of fact, in the interest of avoiding circumlocutions â€“ in particular
when proving the Hilbert-Bernays DC 3 later on (II.6.34) â€“ we allow all the
primitive recursive function symbols (II.3.6, p. 253) to be in LN C and, corre-
spondingly, all the deï¬ning axioms of those symbols (i.e., through composition,
primitive recursion, or, explicitly, as deï¬nitions of Z and U n
i ) to be included
in C.
â€  Cf. II.3.6, p. 253.
â€¡ The Ableitbarkeitsforderungen that we employ are close, but not identical, to the originals in
Hilbert and Bernays (1968). We have allowed some inï¬‚uence from LÂ¨obâ€™s modern derivability
conditions, which we also state and prove, essentially using the old Ableitbarkeitsforderungen
as stepping stones in the proof. Once we have LÂ¨obâ€™s conditions, we employ them to prove the
second incompleteness theorem.
Â§ Recall that extensions by deï¬nitions are conservative.

II.6. Derivability Conditions; Fixed Points
273
We note that C is a recursive extension of PA in the sense of p. 220, since
Proof (introduced in the previous section), and Deriv and Î˜ (p. 281 and
p. 280 below) are not introduced as predicates, but rather are introduced as
informal abbreviations.
Note that the languages LA and LN C may, a priori, extend the language
of PA, LN, in unrelated directions, since the former extension accommodates
whatever new nonlogical axioms are peculiar to , while the latter simply
accommodates symbols (beyond the basic symbols (N L) of p. 166) that are
introduced by deï¬nitions (see, however, II.6.32, p. 303). Thus,  may fail to be
a conservative extension of PA, unlike C.
When we write â€œ âŠ¢â€ in this section, we mean â€œ âŠ¢Câ€ (or â€œ âŠ¢C plus some
â€˜temporaryâ€™ assumptionsâ€, if a proof by deduction theorem, auxiliary constant,
etc., has been embarked upon). Exceptions will be noted.
The informal symbols â€œProof â€, â€œDerivâ€, and â€œÎ˜â€ (the latter two to be in-
troduced shortly) will abbreviate â€œProof â€, â€œDerivâ€, and â€œÎ˜â€ respectively.
Now (equipped with the tools of Sections I.8â€“I.9 and II.3.6â€“II.3.11) it is
easy to verify that all the atomic formulas and terms that we have introduced
in Section II.5, to â€œrecognize speciï¬ed sets of GÂ¨odel numbersâ€ â€“ except for the
formula Proof , because of our assumptions on  â€“ are primitive recursive. For
atomic formulas this means that the corresponding characteristic termsâ€  are.
Pause. Is that so? How about the deï¬nition of Sub (p. 268)? It contains un-
bounded search.
In particular, the lightface versions of each such formula or term (e.g.,
Var(x, i), T erm(x), W F F(x)) â€“ i.e., the informal versions obtained by repli-
cating the formal deï¬nitions in the metatheory â€“ are in PRâˆ—or PR. Applying
Theorem II.3.8 and its Corollary II.3.9, we have that the boldface version in
each case both deï¬nes semantically (in LNC) and strongly deï¬nes (in C) the
corresponding lightface version.
Since every GÂ¨odel number t is a closed primitive recursive term, II.3.10
implies that for any such t there is a unique n âˆˆN such that âŠ¢t = n.
We can actually obtain a bit more: If we let, for the balance of this section, gn(E)
denote the informal (lightface) GÂ¨odel number of a term or formula E,â€¡ â€“ that is,
a â€œrealâ€ number in N â€“ while âŒœEâŒcontinues to denote the formal version â€“ i.e.,
â€  Cf. II.1.25 (p. 220) and p. 222.
â€¡ An arbitrary term is generically denoted by t or s in the metalanguage; a formula, by a calli-
graphic uppercase letter such as A . The â€œEâ€ here is a compromise notation that captures (in the
metalanguage) an arbitrary term or formula, that is, an arbitrary well-formed expression (string).

274
II. The Second Incompleteness Theorem
a closed term in LN C â€“ then (by II.3.8â€“II.3.9)
âŠ¢âŒœEâŒ= 
gn(E)
(1)
To see why (1) is true, consider the GÂ¨odel numbering that we have developed
starting on p. 168. In the interest of clarity of argument, let us introduce the
functions fi (i = 1, . . . , 11) below â€“ as well as their lightface versions,
the latter (not shown) by the same (although lightface) deï¬ning equations.
Note the deliberate choice of distinct variables vi (i = 1, . . . , 11):
f1(v1)
= âŸ¨0, v1âŸ©
f2(v2)
= âŸ¨0, v2âŸ©
f3(v3)
= âŸ¨0, v3âŸ©
f4(v4)
= âŸ¨0, v4âŸ©
f5(v5)
= âŸ¨0, v5âŸ©
f6(v6)
= âŸ¨0, v6âŸ©
f7(v7)
= âŸ¨0, v7âŸ©
f8(v8)
= âŸ¨0, v8âŸ©
f9(v9)
= âŸ¨0, v9âŸ©
f10(v10) = âŸ¨0, v10âŸ©
f11(v11) = âŸ¨0, v11âŸ©
Then any formal (respectively, informal) GÂ¨odel number âŒœEâŒ(respectively,
gn(E)) is a closed instance of h[v1, . . . , v11] (respectively, of h[v1, . . . , v11])
for some appropriate formal (respectively, informal) primitive recursive h (re-
spectively, h). The closed instance is obtained in a speciï¬c way: Each vi (re-
spectively, each vi) is being replaced byi (respectively, i).
As we assume that the formal and informal deï¬nitions â€œtrack each otherâ€ â€“
i.e., have identical primitive recursive derivations except for typeface â€“ then
h = hN C
(1â€²)
Now (1) translates to
âŠ¢h[1, . . . , 
11] = (h[1, . . . , 11])âˆ¼
where â€˜(h[1, . . . , 11])âˆ¼â€™ denotes that â€˜âˆ¼â€™ applies to all of â€˜h[1, . . . , 11]â€™; which
holds by (1â€²) and II.3.8.
II.6.1 A Very Long Example. It is trivial that, for any i âˆˆN, Var(gn(vi), i)
is true.â€  Thus, both
|=N C Var( 
gn(vi),i)
â€  Do you believe this? See II.4.14 for a discussion of such outrageous claims.

II.6. Derivability Conditions; Fixed Points
275
and
âŠ¢Var( 
gn(vi),i)
hold, for all i âˆˆN.â€  By (1), p. 274, and Ax4,
|=N C Var(âŒœviâŒ,i)
and
âŠ¢Var(âŒœviâŒ,i)
hold for all i âˆˆN. From the last two and Ax2 follow, for all i âˆˆN,
|=N C var(âŒœviâŒ)
and
âŠ¢var(âŒœviâŒ)
Conversely, if âŠ¢Var(n,i), then Var(n, i) is true by II.3.9, and therefore, tri-
vially, gn(vi) = n through examination of the lightface deï¬nition (Var) (iden-
tical to the boldface version given on p. 266). Hence
âŠ¢
gn(vi) = n
Thus (by (1), p. 274)
âŠ¢âŒœviâŒ= n
(2)
In other words, âŠ¢Var(n,i) implies that n is provablyâ€¡ equal to the (formal)
GÂ¨odel number of the variable vi.
Similarly, âŠ¢var(n) implies that n is provably equal to the formal GÂ¨odel
number of some variable.
We next verify that Term behaves as intended. Indeed, by induction on
terms, t, we can show in the metatheory that T erm(gn(t)) = 0 is true. For ex-
ample, if t = f t1 . . . tn, then the I.H. implies that T erm(gn(ti)) = 0 is true,
for i = 1, . . . , n. Given that gn(t) = âŸ¨gn( f ), gn(t1), . . . , gn(tn)âŸ©, and consult-
ing (Trm),Â§ we see that indeed, T erm(gn(t)) = 0 is true. The basis cases are
covered as easily. Thus (II.3.8),
âŠ¢Term(
gn(t)) = 0
â€  We recall that the metasymboli denotes a (formal) term. On the other hand, i in vi is part of the
name.
â€¡ In C.
Â§ This is the formal deï¬nition on p. 267. The informal deï¬nition is the lightface version of (Trm).

276
II. The Second Incompleteness Theorem
hence, by (1) and Ax4
âŠ¢Term(âŒœtâŒ) = 0
Conversely, âŠ¢Term(n) = 0 implies that T erm(n) = 0 is true. Now an infor-
mal (i.e., metamathematical) course-of-values induction on n easily shows that
n = gn(t) for some term t, and thus we obtain âŠ¢âŒœtâŒ= n, exactly as we have
obtained (2). For example, say that T erm(n) = 0 because the third disjunctâ€  in
the top case of the deï¬nition (Trm) (p. 267) is true, namely,
Seq(n) âˆ§(âˆƒy)<n(lh(n) = y + 1 âˆ§f unc((n)0, y)
âˆ§(âˆ€z)<yT erm((n)z+1) = 0)
Let y = k < n be a value that works above, i.e., such that the following is true:
Seq(n) âˆ§(lh(n) = k + 1 âˆ§f unc((n)0, k) âˆ§(âˆ€z)<kT erm((n)z+1) = 0)
By the I.H.â€¡ there are terms t1, . . . , tk such that gn(tz) = (n)z for z = 1, . . . , k.
Moreover, the truth of f unc((n)0, k) implies, analogously to the case of Var,
that there is a function symbol f , of arity k, such that gn( f ) = (n)0. Therefore,
n = âŸ¨(n)0, (n)1, . . . , (n)kâŸ©
= âŸ¨gn( f ), gn(t1), . . . , gn(tk)âŸ©
= gn( f t1 . . . tk)
We have veriï¬ed that âŠ¢Term(n) = 0 implies that n is provably equal to the
(formal) GÂ¨odel number of some term t.
One similarly veriï¬es that WFF behaves as intended. We next verify that
Free and Sub behave as intended.
Suppose that vi appears free in A. Referring to the lightface deï¬nition of
Free (which is structurally identical to (Fr), p. 267) and using induction on
A or t â€“ as the case may be â€“ it is easy to show that Free(i, gn(A)) = 0 or
(correspondingly) Free(i, gn(t)) = 0. Thus, one can prove (by (1) and II.3.8â€“
II.3.10)
âŠ¢Free(i, âŒœAâŒ) = 0
or (correspondingly)
âŠ¢Free(i, âŒœtâŒ) = 0
Conversely, assume that âŠ¢Free(i,n) = 0 for some i, n in N. Thus, Free(i, n) =
0 is true. A course-of-values induction on n (over N) shows that n = gn(E)
â€  A disjunct is a member of a disjunction.
â€¡ â€œIf Seq(n), then i < lh(n) â†’(n)i < nâ€ is crucial here.

II.6. Derivability Conditions; Fixed Points
277
(where E is some term or formula) and vi is free in E. For example, if
Free(i, n) = 0 is true because the last disjunct in (Fr) (p. 267) obtains, namely,
W F F(n) = 0 âˆ§(n)0 = gn(âˆƒ) âˆ§Â¬Var((n)1, i) âˆ§Free(i, (n)2) = 0
then the behaviour of W F F ensures that, for some formula A,
n = gn(A)
(i)
Moreover (by (i) and properties of W F F), n = âŸ¨(n)0, (n)1, (n)2âŸ©; hence for
some j âˆˆN and formula B ,
(n)2 = gn(B ), (n)1 = gn(vj)
and
A â‰¡((âˆƒvj)B )
By Â¬Var((n)1, i), we have i Ì¸= j. By I.1.10 and the I.H. (by which the conjunctâ€ 
Free(i, (n)2) = 0 ensures that vi is free in B ) it follows that vi is free in A.
Thus, by (1) (p. 274) and (i), âŠ¢âŒœAâŒ= n in this case, and hence (by assumption
and Ax4)
âŠ¢Free(i, âŒœAâŒ) = 0
All in all, using (1) for the if part,
vi is free in t
iff
âŠ¢Free(i, âŒœtâŒ) = 0
vi is free in A
iff
âŠ¢Free(i, âŒœAâŒ) = 0
Turning now to Sub, we validate the claims made on p. 268.
Let at ï¬rst t and s be terms. It is easy (metamathematical induction on t) to
see that
Sub(gn(t), i, gn(s)) = gn(t[vi â†s])
(3)
is true. Indeed (sampling the proof), let t â‰¡f t1 . . . tn. Referring to the deï¬ni-
tion of Sub (mirrored on p. 268 for the boldface case), we obtain
Sub(gn(t), i, gn(s)) = âŸ¨gn( f ), Sub(gn(t1), i, gn(s)), . . . ,
Sub(gn(tn), i, gn(s))âŸ©
Using the I.H. on t, the above translates to
Sub(gn(t), i, gn(s)) = âŸ¨gn( f ), gn(t1[vi â†s]), . . . , gn(tn[vi â†s])âŸ©
Since gn(t[vi â†s]) = âŸ¨gn( f ), gn(t1[vi â†s]), . . . , gn(tn[vi â†s])âŸ©, (3)
follows.
â€  A conjunct is a member of a conjunction.

278
II. The Second Incompleteness Theorem
Let next A be a formula, and s be a term substitutable for vi in A. By
induction on A we can show in the metatheory that
Sub(gn(A), i, gn(s)) = gn(A [vi â†s])
(4)
is true. Sampling the proof of (4), let us consider an interesting case, namely,
A â‰¡(âˆƒvj)B , where i Ì¸= j. By the I.H. on formulas,
Sub(gn(B ), i, gn(s)) = gn(B [vi â†s])
(5)
By I.3.10â€“I.3.11, A[vi â†s] â‰¡(âˆƒvj)(B [vi â†s]). Thus,
gn(A [vi â†s]) = âŸ¨gn(âˆƒ), gn(vj), gn(B [vi â†s])âŸ©
(6)
By the deï¬nition of Sub,
Sub(gn(A), i, gn(s)) = âŸ¨gn(âˆƒ), gn(vj), Sub(gn(B ), i, gn(s))âŸ©
Thus, (5) and (6), yield (4).
Here is the other interesting case, where A is a formula, and s is a term that
is not substitutable for vi in A. By induction on A we can show this time that
Sub(gn(A), i, gn(s)) = 0
(7)
We just sample the same case as above. First off, for any term or formula E,
gn(E) > 0.
Pause. Do you believe this?
Consider ï¬rst the subcase where A â‰¡(âˆƒvj)B , and s is substitutable for vi in
B . By (4), Sub(gn(B ), i, gn(s)) = gn(B [vi â†s]) > 0.
Now, by assumption of non-substitutability for vi in A, it must be that vj
is free in s. Thus, the relevant condition for the deï¬nition of Sub (this is the
second from last condition, p. 268) fails, since Free( j, gn(s)) = 0. Therefore
the deï¬nition (of Sub) returns 0 (the â€œotherwiseâ€), and (7) is correct in this
subcase.
The remaining subcase is that substitutability of s failed earlier, that is
(I.H.), Sub(gn(B ), i, gn(s)) = 0. Still, the relevant condition in the deï¬ni-
tion of Sub is the second from last. It fails once more, since the conjunct
â€œSub(gn(B ), i, gn(s)) > 0â€ is false. Therefore,
Sub(gn(A), i, gn(s)) > 0
iff
the term s is substitutable for vi in A

II.6. Derivability Conditions; Fixed Points
279
We translate the above, as well as (3) and (4), to the formal domain us-
ing (1) (p. 274) and II.3.8â€“II.3.9:
âŠ¢Sub(âŒœA âŒ,i, âŒœsâŒ) > 0
iff
the term s is substitutable for vi in A
âŠ¢Sub(âŒœt[vi]âŒ,i, âŒœsâŒ) = âŒœt[s]âŒ
âŠ¢Sub(âŒœA[vi]âŒ,i, âŒœsâŒ) = âŒœA[s]âŒ
if the term s is substitutable for vi in A
The claims regarding subi (p. 268) are trivial consequences of the above.
We ï¬nally check Proof .
Turning to the lightface version (see p. 271 for the boldface version), let
n = âŸ¨gn(A0), . . . , gn(Akâˆ’1)âŸ©, where k > 0 and A0, . . . , Akâˆ’1 is a -proof.
Assume that Î“(x) (and hence (x)) is primitive recursive, an assumption that
is correct in the case of PA (Exercise II.15)
That Seq(n), and lh(n) = k > 0, is true outright. To see that Proof (n) is
true we now only need to investigate (n)i for i < k and verify that it satisï¬es
L A((n)i) âˆ¨((n)i) âˆ¨(âˆƒj)<i(âˆƒm)<i M P((n) j, (n)m, (n)i) âˆ¨
(âˆƒj)<i E I((n) j, (n)i) âˆ¨(âˆƒj)<i SR((n) j, (n)i)
(8)
Now, (n)i = gn(Ai), for i < k. The following exhaust all cases:â€ 
Case 1. Ai âˆˆ âˆª. Then the ï¬rst or second disjunct of (8) is true.
Case 2. Forsome j < i andm < i,onehasAm â‰¡A j â†’Ai.Then M P((n) j,
(n)m, (n)i) is true.
Case 3. For some j < i, one has A j â‰¡B â†’C , x is not free in C , and
Ai â‰¡(âˆƒx)B â†’C . Then E I((n) j, (n)i) is true.
Case 4. For some j < i, and some variable x and term t substitutable for x in
A j, one has Ai â‰¡A j[x â†t]. Then SR((n) j, (n)i) is true.
Thus, (8) is true under all possible cases.
Conversely, one can show by (metamathematical) course-of-values induc-
tion on lh(n) that if Proof (n) is true, then there is a -proof A0, . . . , Akâˆ’1,
wherek =lh(n) > 0,suchthatn = âŸ¨gn(A0), . . . , gn(Akâˆ’1)âŸ©.Notethattruthof
Proof (n) guarantees the truth of Seq(n) and lh(n) > 0 (deï¬nition of Proof ).
Suppose that k = 1 (basis). Then the only disjuncts that can be true in (8) are
the ï¬rst two (why?). If the ï¬rst one holds, then this implies that (n)0 = gn(B )
for some formula B âˆˆ (see (L A), p. 270); if the second one holds, then
(n)0 = gn(B ) for some formula B âˆˆ (hence, in either case, the one-member
sequence â€œB â€ â€“ which we may want to rename â€œA0â€ â€“ is a -proof). The reader
â€  Recall that we have agreed to allow the redundant substitution rule (p. 270).

280
II. The Second Incompleteness Theorem
will have no difï¬culty completing the induction. By the primitive recursiveness
of Proof â€  and II.3.9,
Proof (n) is true
iff
âŠ¢Proof (n)
(9)
In particular, if A0, . . . , Akâˆ’1 is some -proof and we set
n = âŸ¨gn(A0), . . . , gn(Akâˆ’1)âŸ©
and also set
t = âŸ¨âŒœA0âŒ, . . . , âŒœAkâˆ’1âŒâŸ©
â€“ the (formal) GÂ¨odel number of the proof â€“ then âŠ¢t = n. Since Proof (n) is
true, (9) yields âŠ¢Proof (n); hence
âŠ¢Proof (t)
Now, noting that Proof N C = Proof , the only-if direction of (9) also holds
for our Î£1 Î“ (by II.4.12). Thus, the claim we made above, â€œIn particular, if
A0, . . . , Akâˆ’1 is, etc.â€, still holds without the restriction to a primitive recursive
Î“. Is the if direction still true?
â–¡
II.6.2 Deï¬nition (The Provability Predicate). The informal abbreviation Î˜
introduced below is called the provability predicate:
Î˜(x) stands for (âˆƒy)(Proof (y) âˆ§(âˆƒi)<lh(y)x = (y)i)
â–¡
II.6.3 Remark. (1) The deï¬nition of Î˜ and the use of the term â€œprovability
formulaâ€ is modulo a ï¬xed set of nonlogical axioms . We have already ï¬xed
attention on such a set, but our attention may shift to other sets, â€², â€²â€², etc., on
occasion. All that is required is to observe that:
r Any such primed  is a consistent extension of PA.
r The corresponding predicate, Î“, is Î£1 over the language of C, LN C,
where all the symbols (and more) that we use for GÂ¨odel coding belong.
(2) Note that we did not require x to be the last formula in the â€œproofâ€ y.
â€  We have cheated here and allowed â€“ for the purpose of this informal veriï¬cation â€“ the â€œlocalâ€
assumption that Î“ is primitive recursive. Recall that our â€œglobalâ€ assumption on Î“ â€“ the one
operative throughout this section, outside this example â€“ is that it is a Î£1 formula. In this
connection the reader should note the concluding sentence of this example.

II.6. Derivability Conditions; Fixed Points
281
(3) Î˜(x) is in Î£1(LNC).
(4) From the work in the previous example we see that Î˜(x) intuitively says
that x is the GÂ¨odel number of some theorem. More precisely, if  âŠ¢A (A is
over â€™s language) and we set t = âŒœAâŒ(and also n = gn(A)), then Î˜(n) is
a true Î£1(LNC) sentence. By II.4.12,
âŠ¢Î˜(n)
Hence (by âŠ¢t = n)
âŠ¢Î˜(t)
The reader is reminded that â€œâŠ¢â€ means â€œâŠ¢Câ€ unless noted otherwise.
(5) The preceding discussion completes the (sketch of) proof of I.9.33 given
on p. 177. That is, that the lightface version of Î˜(x), (x), is semi-recursive.
A number of issues that were then left open (e.g., the recursiveness of  and
of the rules I1 and I2) have now been settled.
(6) It is useful to introduce an informal abbreviation for
Proof (y) âˆ§(âˆƒi)<lh(y)x = (y)i
We will use the name Deriv introduced by
Deriv(y, x) stands for Proof (y) âˆ§(âˆƒi)<lh(y)x = (y)i
That is, Deriv(y, x) intuitively says that the proof (coded by) y derives the
theorem (coded by) x.
â–¡
In the following lemmata we â€œdoâ€ some of the metatheory of arithmetic
within formal arithmetic, having arithmetized the language. For example, the
ï¬rst lemma says that if we substitute a term into some variable of another term,
then we obtain a term. The chapter concludes with a proof (second incomplete-
ness theorem) that we cannot â€œdoâ€ all the metatheory within the theory.
II.6.4 Lemma.
âŠ¢Term(x) = 0 âˆ§Term(z) = 0 â†’Term(Sub(x, i, z)) = 0
Proof. We do this proof in some detail, since in most others in the following
sequence of lemmata, we delegate much of the burden of proof to the reader.
(Warning: This proof will be rather pedantic.)

282
II. The Second Incompleteness Theorem
We do (formal) course-of-values induction (II.1.23, p. 214) on x, proceeding
according to the deï¬nition of Term (p. 267). The latter yields (cf. (9), p. 221)
âŠ¢Term(x) = 0 â†”var(x) âˆ¨x = âŸ¨0, S0âŸ©
âˆ¨Seq(x) âˆ§(âˆƒy)<x(lh(x) = Sy
âˆ§f unc((x)0, y) âˆ§(âˆ€z)<yTerm((x)Sz) = 0)
(1)
Now assume
Term(x) = 0
and
Term(z) = 0
(2)
and prove
Term(Sub(x, i, z)) = 0
(3)
We have cases according to (1):
Case of var(x):
Thus (p. 266) âŠ¢(âˆƒj)â‰¤xVar(x, j). We may now add a
new constant a and the assumption
a â‰¤x âˆ§Var(x, a)
The subcase a = i and the deï¬nition of Sub, (ï¬rst case; cf. also the deï¬nition
by cases in (15), p. 221) yield
âŠ¢Sub(x, i, z) = z
(4)
Similarly, the (only other) subcase, Â¬ a = i, and the deï¬nition of Sub (third
case) yield
âŠ¢Sub(x, i, z) = x
(5)
Either of (4) or (5) yields (3) because of (2), and we are done in this case.
Pause. In the last subcase I used (tacitly) the â€œfactâ€ that
âŠ¢Var(x, a) â†’Â¬ a = i â†’Â¬Var(x, i)
Is this indeed a fact? (Hint. lh.)
Case of x = âŸ¨0, S0âŸ©:
The deï¬nition of Sub (second case) yields (5), and
once again we have derived (3) from (2).
Finally the hard case, that is, that of the third disjunct in (1) above:
Seq(x)âˆ§(âˆƒy)<x(lh(x) = Sy
âˆ§f unc((x)0, y) âˆ§(âˆ€z)<yTerm((x)Sz) = 0)
(hc)

II.6. Derivability Conditions; Fixed Points
283
By (hc) we have
âŠ¢Seq(x)
(6)
and we may also introduce a new constant b and the assumption
b < x âˆ§lh(x) = Sb
âˆ§f unc((x)0, b) âˆ§(âˆ€z)<bTerm((x)Sz) = 0
(7)
On the other hand, the deï¬nition of Sub (fourth case) and (2) (cf. (15), p. 221)
yields
âŠ¢Sub(x, i, z) = (Âµw)(Seq(w) âˆ§lh(w) = lh(x) âˆ§(w)0 = (x)0 âˆ§
(âˆ€y)<lh(x)(y > 0 â†’(w)y = Sub((x)y, i, z)))
(8)
Using the abbreviation t = Sub(x, i, z) for convenience, we get from (8) via
( f (4)) (p. 218)
âŠ¢Seq(t)
(9)
âŠ¢lh(t) = lh(x)
(10)
âŠ¢(t)0 = (x)0
(11)
and
âŠ¢(âˆ€y)<lh(x)(y > 0 â†’(t)y = Sub((x)y, i, z))
(12)
By (6) and II.2.13
âŠ¢y < lh(x) â†’(x)y < x
(13)
Now (7) yields
âŠ¢(âˆ€z)<lh(x)(z > 0 â†’Term((x)z) = 0)
Thus (12) and (13) â€“ via the I.H. â€“ yield
âŠ¢(âˆ€y)<lh(x)(y > 0 â†’Term((t)y) = 0)
(14)
that is, reintroducing b (see (7)),
âŠ¢(âˆ€y)<bTerm((t)Sy) = 0
(14â€²)
By (7), (14â€²), (10), and (11) we now have
âŠ¢lh(t) = Sb
âˆ§f unc((t)0, b) âˆ§(âˆ€y)<bTerm((t)Sy) = 0

284
II. The Second Incompleteness Theorem
Since âŠ¢lh(t) â‰¤t by II.2.4(i), and therefore âŠ¢b < t by the ï¬rst conjunct of
the above, we obtain
âŠ¢b < t âˆ§lh(t) = Sb
âˆ§f unc((t)0, b) âˆ§(âˆ€y)<bTerm((t)Sy) = 0
Hence, using a new variable z, Ax2, and (9),
âŠ¢Seq(t) âˆ§(âˆƒz)<t(lh(t) = Sz
âˆ§f unc((t)0, z

âˆ§(âˆ€y)<zTerm((t)Sy) = 0)
(15)
By the deï¬nition of Term, âŠ¢Term(t) = 0 follows immediately from (15). â–¡
The above result (and the following suite of similar results) is much easier to
prove if instead of free variables we have numerals. We just invoke II.3.8â€“II.3.9.
However, we do need the versions with free variables.
The following says that a term is substitutable into any variable of another
term.â€ 
II.6.5 Corollary.
âŠ¢Term(x) = 0 âˆ§Term(z) = 0 â†’Sub(x, i, z) > 0
Proof. Assume the hypothesis (to the left of â€œâ†’â€). Then (lemma above)
âŠ¢Term(Sub(x, i, z)) = 0
(1)
Now
âŠ¢Term(x) = 0 â†’Seq(x) âˆ§lh(x) > 0
by inspection of the cases involved in the deï¬nition of Term (see (1) in the
previous proof). By II.2.13,
âŠ¢Seq(x) âˆ§lh(x) > 0 â†’x > (x)0
Thus, by II.1.5 and II.1.6,
âŠ¢Term(x) = 0 â†’x > 0
(2)
By substitution from (1), and (2), we have âŠ¢Sub(x, i, z) > 0.
â–¡
â€  By a variable of a term we understand any variable â€“ hence the free i in the corollary statement â€“
not only one that actually occurs in the term. That is, we mean a variable x of t in the sense t[x].

II.6. Derivability Conditions; Fixed Points
285
II.6.6 Lemma. âŠ¢Term(x) = 0 âˆ§Free(i, x) = 0 â†’Sub(x, i, z) â‰¥z.
Proof. Assume the hypothesis, i.e.,
Term(x) = 0
and
Free(i, x) = 0
We do (formal) course-of-values induction on x. By the deï¬nition of Free
(p. 267), we have just two cases to consider. Here is why, and what:
In principle, we have the three cases as in the proof of II.6.4. However, the
ï¬rst case considered in II.6.4 has here only its ï¬rst subcase tenable, namely,
where a = i. This is because the other subcase â€“ Â¬a = i â€“ implies as before
âŠ¢var(x) âˆ§Â¬Var(x, i). Now this forces âŠ¢Free(i, x) = S0, for if we have
âŠ¢var(x), then âŠ¢(x)0 = 1; hence we cannot have any of âŠ¢(âˆƒy)<x f unc((x)0,
y),â€  âŠ¢AF(x), or âŠ¢WFF(x) = 0.
Thus, (4) of the proof of II.6.4 holds.
The second case in the proof of II.6.4, namely, x = âŸ¨0, S0âŸ©is also untenable
as before (see (Fr), p. 267), since here âŠ¢(x)0 = 0.
This leaves the hard case (hc), which yields (8) of the proof of II.6.4, namely,
âŠ¢Sub(x, i, z) = (Âµw)
%
Seq(w) âˆ§lh(w) = lh(x) âˆ§(w)0 = (x)0
âˆ§(âˆ€y)<lh(x)(y > 0 â†’(w)y = Sub((x)y, i, z))
&
Thus, by ( f (4)) (p. 218), tautological implication, and eliminating âˆ€,
âŠ¢0 < y âˆ§y < lh(x) â†’(Sub(x, i, z))y = Sub((x)y, i, z)
(1)
The deï¬nition of Free (p. 267) yields
âŠ¢(âˆƒy)<lh(x)(0 < y âˆ§Free(i, (x)y) = 0)
Invoking proof by auxiliary constant, we now add
0 < a âˆ§a < lh(x) âˆ§Free(i, (x)a) = 0
(2)
where a is a new constant. By (2) (ï¬rst two conjuncts) and (1),
âŠ¢(Sub(x, i, z))a = Sub((x)a, i, z)
(3)
Now, since (cf. (Trm), p. 267)
âŠ¢Term(x) = 0 â†’(âˆ€z)<lh(x)(0 < z â†’Term((x)z) = 0)
â€  Since âŠ¢lh((x)0) â‰¤(x)0 and (see (Func), p. 266) âŠ¢lh((x)0) â‰¥6.

286
II. The Second Incompleteness Theorem
we obtain by hypothesis and specialization
âŠ¢0 < a âˆ§a < lh(x) â†’Term((x)a) = 0
Thus
âŠ¢Term((x)a) = 0
(4)
via the ï¬rst two conjuncts of (2). Using now (2) (last conjunct), (4), and the
I.H. â€“ this uses II.2.13 â€“ we get
âŠ¢Sub((x)a, i, z) â‰¥z
(5)
Since âŠ¢Seq(Sub(x, i, z)), II.2.13 yields
âŠ¢Sub(x, i, z) > (Sub(x, i, z))a
By (3) and (5) we now have âŠ¢Sub(x, i, z) > z.
â–¡
The next three claims (stated without proof, since their proofs are trivial
variations of the preceding three) take care of atomic formulas.
II.6.7 Lemma.
âŠ¢AF(x) âˆ§Term(z) = 0 â†’AF(Sub(x, i, z)) = 0
II.6.8 Corollary.
âŠ¢AF(x) âˆ§Term(z) = 0 â†’Sub(x, i, z) > 0
II.6.9 Lemma. âŠ¢AF(x) âˆ§Free(i, x) = 0 â†’Sub(x, i, z)â‰¥z.
For the next result we want to ensure that a substitution actually took place
(hence the condition on substitutability, â€œSub(x, i, z) > 0â€, which was unnec-
essary in the cases of term or atomic formula targets).
II.6.10 Proposition.
âŠ¢Term(x) = 0 âˆ¨WFF(x) = 0 â†’
Sub(x, i, z) > 0 âˆ§Free(i, x) = 0 â†’Sub(x, i, z) â‰¥z
Proof. The case for Term(x) = 0 is II.6.6 above. The subcase AF(x) for
WFF(x) = 0 is II.6.9 (for either of these the assumption Sub(x, i, z) > 0
is redundant).

II.6. Derivability Conditions; Fixed Points
287
We consider here one among the other subcases of WFF(x) = 0. Once
again, we employ course-of-values induction on x, legitimized by II.2.13.â€  Add
then the assumptions WFF(x) = 0, Sub(x, i, z) > 0 and Free(i, x) = 0.
Subcase.
x = âŸ¨âŒœâˆ¨âŒ, (x)S0, (x)SS0âŸ©. Thus (deï¬nition of WFF, p. 267)
âŠ¢WFF((x)S0) = 0
and
âŠ¢WFF((x)SS0) = 0
By the deï¬nition of Free (p. 267) we now have
âŠ¢Free(i, (x)S0) = 0 âˆ¨Free(i, (x)SS0) = 0
(1)
By deï¬nition of Sub we obtain
âŠ¢Sub(x, i, z) = âŸ¨âŒœâˆ¨âŒ, Sub((x)S0, i, z), Sub((x)SS0, i, z)âŸ©
(2)
Pause. Wait a minute! The above is so provided that
âŠ¢Sub((x)S0, i, z) > 0 âˆ§Sub((x)SS0, i, z) > 0
Is this satisï¬ed? Why?
By (1), we consider cases. So add Free(i, (x)S0) = 0. The I.H. and
âŠ¢Sub((x)S0, i, z) > 0 yield
âŠ¢Sub((x)S0, i, z) â‰¥z
and hence
âŠ¢Sub(x, i, z) â‰¥z
exactly as in II.6.6, invoking âŠ¢âŸ¨. . . , u, . . . âŸ©> u. The other case works out
as well.
â–¡
II.6.10 justiï¬es the bound of the existential quantiï¬ers in the deï¬nition of SR
(p. 270).
II.6.11 Lemma.
âŠ¢WFF(x) = 0 âˆ§Term(z) = 0 âˆ§Sub(x, i, z) > 0 â†’
WFF(Sub(x, i, z)) = 0
â€  This is the last time we are reminded of the role of II.2.13 in our inductions on GÂ¨odel numbers.

288
II. The Second Incompleteness Theorem
Proof. We add the assumptions
WFF(x) = 0
Term(z) = 0
and
Sub(x, i, z) > 0
(1)
and do (formal) course-of-values induction on x, proceeding according to the
deï¬nition of WFF (p. 267), towards proving
âŠ¢WFF(Sub(x, i, z)) = 0
We illustrate what is involved by considering one case, leaving the rest to the
reader.
Case â€œÂ¬â€.
x = âŸ¨âŒœÂ¬âŒ, yâŸ©and âŠ¢WFF(y) = 0, where we have used the
abbreviation y = (x)S0 for convenience.
We also add the assumption (which we hope to contradict)
Sub(y, i, z) = 0
Thus,
âŠ¢Â¬Sub(y, i, z) > 0
and therefore, via tautological implication and Ax4 (since âŠ¢(x)S0 = y),
âŠ¢Â¬(WFF(x) = 0 âˆ§(x)0 = âŒœÂ¬âŒâˆ§Sub((x)S0, i, z) > 0)
Thus, the â€œotherwiseâ€â€  in the deï¬nition of Sub (p. 268) is provable, since
(x)0 = âŒœÂ¬âŒis refutable in all the other cases. Therefore, âŠ¢Sub(x, i, z) = 0,
contradicting the assumption (1).
We have just established
âŠ¢Sub(y, i, z) > 0
Hence also (deï¬nition of Sub)
âŠ¢Sub(x, i, z) = âŸ¨âŒœÂ¬âŒ, Sub(y, i, z)âŸ©
(2)
By I.H.,
âŠ¢WFF(Sub(y, i, z)) = 0
â€  That is, the conjunction of the negations of all the explicit cases.

II.6. Derivability Conditions; Fixed Points
289
Hence, by the deï¬nition of WFF,
âŠ¢WFF(âŸ¨âŒœÂ¬âŒ, Sub(y, i, z)âŸ©) = 0
Thus
âŠ¢WFF(Sub(x, i, z)) = 0
by (2) and Ax4.
â–¡
Pause. Where was the assumption Term(z) = 0 used?
II.6.12 Lemma.
âŠ¢(âˆ€j) Free( j, z) > 0 âˆ§Term(z) = 0 âˆ§
WFF(x) = 0 â†’Sub(x, i, z) > 0
This says that closed terms are always substitutable.
Proof. We assume
(âˆ€j)Free( j, z) > 0
Term(z) = 0
and
WFF(x) = 0
and do (formal) course-of-values induction on x proceeding according to the
deï¬nition of WFF to show
âŠ¢Sub(x, i, z) > 0
We illustrate what is involved by considering one case, leaving the remaining
cases to the reader.
Case â€œâˆƒâ€.
Add x = âŸ¨âŒœâˆƒâŒ, y, wâŸ©, WFF(w) = 0, and var(y), where we
have used the abbreviations y = (x)S0 and w = (x)SS0 for convenience. The
latter expands to (see (var), p. 266)
âŠ¢(âˆƒj)â‰¤yVar(y, j)
(3)
Assume ï¬rst the interesting subcase.
Subcase.
Add Â¬Var(y, i). By (3) we may add a new constant a and the
assumption
a â‰¤y âˆ§Var(y, a)
(4)

290
II. The Second Incompleteness Theorem
By âŠ¢(âˆ€j)Free( j, z) > 0
âŠ¢Free(a, z) > 0
Hence by (4) and Ax2
âŠ¢(âˆƒj)â‰¤y(Var(y, j) âˆ§Free( j, z) > 0)
By the I.H. âŠ¢Sub(w, i, z) > 0; hence (deï¬nition of Sub)
âŠ¢Sub(x, i, z) = âŸ¨âŒœâˆƒâŒ, y, Sub(w, i, z)âŸ©
Thus, âŠ¢Sub(x, i, z) > 0 (e.g., âŠ¢Sub(x, i, z) > âŒœâˆƒâŒ).
Subcase.
Add Var(y, i). Now
âŠ¢Sub(x, i, z) = x
Therefore, once more, âŠ¢Sub(x, i, z) > 0. (Why is âŠ¢x > 0?)
â–¡
II.6.13 Lemma. âŠ¢Proof (x) âˆ§Proof (y) â†’Proof (x âˆ—y).
Proof. Assume Proof (x) and Proof (y). Thus,
âŠ¢Seq(x) âˆ§lh(x) > 0 âˆ§
(âˆ€i)<lh(x)(LA((x)i) âˆ¨Î“((x)i) âˆ¨
(âˆƒj)<i(âˆƒk)<i MP((x) j, (x)k, (x)i) âˆ¨
(âˆƒj)<i EI((x) j, (x)i) âˆ¨
(âˆƒj)<i SR((x) j, (x)i))
(1)
and
âŠ¢Seq(y) âˆ§lh(y) > 0 âˆ§
(âˆ€i)<lh(y)(LA((y)i) âˆ¨Î“((y)i) âˆ¨
(âˆƒj)<i(âˆƒk)<i MP((y) j, (y)k, (y)i) âˆ¨
(âˆƒj)<i EI((y) j, (y)i) âˆ¨
(âˆƒj)<i SR((y) j, (y)i))
(2)
By II.2.21 (p. 247) we have
âŠ¢Seq(x âˆ—y)
âŠ¢lh(x âˆ—y) = lh(x) + lh(y)
(3)
Hence
âŠ¢Seq(x âˆ—y) âˆ§lh(x âˆ—y) > 0
(4)

II.6. Derivability Conditions; Fixed Points
291
(4) settles the ï¬rst two conjuncts of Proof (x âˆ—y). We now address the last
conjunct, (âˆ€i)<lh(xâˆ—y)(LA((x âˆ—y)i) âˆ¨. . . ). It sufï¬ces to drop the quantiï¬er
and show
âŠ¢i < lh(x âˆ—y) â†’
LA((x âˆ—y)i) âˆ¨Î“((x âˆ—y)i)âˆ¨
(âˆƒj)<i(âˆƒk)<i MP((x âˆ—y) j, (x âˆ—y)k, (x âˆ—y)i) âˆ¨
(âˆƒj)<i EI((x âˆ—y) j, (x âˆ—y)i) âˆ¨
(âˆƒj)<i SR((x âˆ—y) j, (x âˆ—y)i)
(5)
We note, again by II.2.21,
âŠ¢(âˆ€i)<lh(x)(x âˆ—y)i = (x)i âˆ§
(âˆ€i)<lh(y)(x âˆ—y)i+lh(x) = (y)i
(6)
To prove (5), add i < lh(x âˆ—y). By (3) and (6) we have two cases.
Case 1.
i < lh(x). By (1) and (6) (ï¬rst conjunct) we obtain (using Ax4)
âŠ¢LA((x âˆ—y)i) âˆ¨Î“((x âˆ—y)i) âˆ¨
(âˆƒj)<i(âˆƒk)<i MP((x âˆ—y) j, (x âˆ—y)k, (x âˆ—y)i) âˆ¨
(âˆƒj)<i EI((x âˆ—y) j, (x âˆ—y)i) âˆ¨
(âˆƒj)<i SR((x âˆ—y) j, (x âˆ—y)i)
(7)
Case 2.
i â‰¥lh(x). Setting m = Î´(i,lh(x)), we get âŠ¢m + lh(x) = i
(cf. II.1.27) and âŠ¢m < lh(y) (using (3)). By (2),
âŠ¢LA((y)m) âˆ¨Î“((y)m) âˆ¨
(âˆƒj)<m(âˆƒk)<mMP((y) j, (y)k, (y)m) âˆ¨
(âˆƒj)<mEI((y) j, (y)m) âˆ¨
(âˆƒj)<mSR((y) j, (y)m)
Hence, bringing in (6) (second conjunct) via Ax4 and noting that âŠ¢j < m â†’
j < lh(y), we get
âŠ¢LA((x âˆ—y)m+lh(x)) âˆ¨Î“((x âˆ—y)m+lh(x)) âˆ¨
(âˆƒj)<m(âˆƒk)<mMP((x âˆ—y) j+lh(x), (x âˆ—y)k+lh(x), (x âˆ—y)m+lh(x)) âˆ¨
(âˆƒj)<mEI((x âˆ—y) j+lh(x), (x âˆ—y)m+lh(x)) âˆ¨
(âˆƒj)<mSR((x âˆ—y) j+lh(x), (x âˆ—y)m+lh(x))
Translating the above (via proof by cases) in terms of i, we obtain (7) once
more.

292
II. The Second Incompleteness Theorem
To sample one case, we translate the last disjunct. We claim that
âŠ¢(âˆƒj)<mSR((x âˆ—y) j+lh(x), (x âˆ—y)m+lh(x)) â†’
(âˆƒj)<i SR((x âˆ—y) j, (x âˆ—y)i)
Assume the hypothesis, and add a new constant a and the assumption
a < m âˆ§SR((x âˆ—y)a+lh(x), (x âˆ—y)m+lh(x))
By II.1.17, âŠ¢a + lh(x) < m + lh(x), that is, a + lh(x) < i. The above now
yields
a + lh(x) < i âˆ§SR((x âˆ—y)a+lh(x), (x âˆ—y)i)
Hence (Ax2)
âŠ¢(âˆƒj)( j < i âˆ§SR((x âˆ—y) j, (x âˆ—y)i))
After all this, the deduction theorem establishes (5).
â–¡
II.6.14 Lemma.
âŠ¢Proof (w) âˆ§(âˆƒj)<lh(w)(âˆƒk)<lh(w)MP((w) j, (w)k, x)
â†’Proof

w âˆ—âŸ¨xâŸ©

Proof. Assume
Proof (w)
(8)
and
(âˆƒj)<lh(w)(âˆƒk)<lh(w)MP((w) j, (w)k, x)
(9)
and set t = w âˆ—âŸ¨xâŸ©. Since âŠ¢Seq(t) âˆ§lh(t) = S(lh(w)), the ï¬rst two required
conjuncts for Proof (w âˆ—âŸ¨xâŸ©) are settled. To conclude, we add i < lh(t), that
is (< 2) i < lh(w) âˆ¨i = lh(w), and prove
âŠ¢LA((t)i) âˆ¨Î“((t)i) âˆ¨
(âˆƒj)<i(âˆƒk)<i MP((t) j, (t)k, (t)i) âˆ¨
(âˆƒj)<i EI((t) j, (t)i) âˆ¨
(âˆƒj)<i SR((t) j, (t)i)
(10)
There are two cases:
Case 1.
i < lh(w). Thus (cf. (6) (ï¬rst conjunct) of the previous proof)
âŠ¢(t)i = (w)i
(11)

II.6. Derivability Conditions; Fixed Points
293
By (8),
âŠ¢LA((w)i) âˆ¨Î“((w)i) âˆ¨
(âˆƒj)<i(âˆƒk)<i MP((w) j, (w)k, (w)i) âˆ¨
(âˆƒj)<i EI((w) j, (w)i) âˆ¨
(âˆƒj)<i SR((w) j, (w)i)
Now (10) follows from the above and (11) via Ax4.
Case 2.
i = lh(w). Thus, âŠ¢(t)i = x. By (9),
âŠ¢(âˆƒj)<lh(w)(âˆƒk)<lh(w)MP((w) j, (w)k, (t)i)
or, using (11),
âŠ¢(âˆƒj)<i(âˆƒk)<i MP((t) j, (t)k, (t)i)
(10) now follows by tautological implication.
â–¡
II.6.15 Lemma.
âŠ¢Proof (w) âˆ§(âˆƒj)<lh(w)SR((w) j, x) â†’Proof (w âˆ—âŸ¨xâŸ©)
Proof. A trivial rephrasing of the previous proof.
â–¡
II.6.16 Exercise. Show by a formal course-of-values induction on i that
âŠ¢i < lh(x) âˆ§Proof (x) â†’WFF((x)i) = 0
Moreover, show that this is so regardless of whether one adopts MP, EI, and
SR as we did on p. 270 or, instead, one adopts their primed versions (cf. II.5.1,
p. 271).
â–¡
II.6.17 Corollary.
âŠ¢Deriv(y, u) âˆ§Deriv(z, âŸ¨âŒœâ†’âŒ, u, xâŸ©) â†’Deriv(y âˆ—z âˆ—âŸ¨xâŸ©, x)
Note. We have used the abbreviation â€œâŸ¨âŒœâ†’âŒ, u, xâŸ©â€ for â€œâŸ¨âŒœâˆ¨âŒ, âŸ¨âŒœÂ¬âŒ, uâŸ©,
xâŸ©â€.
Proof. By Lemma II.6.13, âŠ¢Proof (y âˆ—z). The result follows at once from
II.6.14 since âŠ¢MP(u, âŸ¨âŒœâ†’âŒ, u, xâŸ©, x).
This last claim also uses II.6.16, since the hypothesis (to the left of â€œâ†’â€)
yields âŠ¢WFF(u) = 0 and âŠ¢WFF(âŸ¨âŒœâ†’âŒ, u, xâŸ©) = 0.
â–¡

294
II. The Second Incompleteness Theorem
A special case of the above is: For any formulas A and B ,
âŠ¢Deriv(y, âŒœAâŒ) âˆ§Deriv(z, âŒœA â†’B âŒ)
â†’Deriv(y âˆ—z âˆ—âŸ¨âŒœB âŒâŸ©, âŒœB âŒ)
II.6.18 Corollary.
âŠ¢Term(z) = 0 âˆ§Sub(x, i, z) > 0 â†’
Deriv(y, x) â†’Deriv(y âˆ—âŸ¨Sub(x, i, z)âŸ©, Sub(x, i, z))
Proof. We add Term(z) = 0, Sub(x, i, z) > 0, and Deriv(y, x). That
âŠ¢Deriv(y âˆ—âŸ¨Sub(x, i, z)âŸ©, Sub(x, i, z))
(12)
will then follow at once from II.6.15 once we prove
âŠ¢SR(x, Sub(x, i, z))
(13)
We look ï¬rst at the interesting case: That is, we add Free(i, x) = 0.
To prove (13) we need (see deï¬nition of SR, p. 270) to prove
âŠ¢WFF(x) = 0 âˆ§WFF(Sub(x, i, z)) = 0 âˆ§
(âˆƒi)â‰¤x(âˆƒw)â‰¤Sub(x, i, z)(Term(w) = 0 âˆ§
Sub(x, i, z) = Sub(x, i, w))
(14)
or, simply,
âŠ¢WFF(x) = 0 âˆ§WFF(Sub(x, i, z)) = 0 âˆ§
i â‰¤x âˆ§z â‰¤Sub(x, i, z)
because then the third conjunct of (14) follows by MP and Ax2 from
âŠ¢i â‰¤x âˆ§z â‰¤Sub(x, i, z) âˆ§Term(z) = 0 âˆ§
Sub(x, i, z) = Sub(x, i, z)
That is, we need
âŠ¢WFF(x) = 0
(i)
âŠ¢WFF(Sub(x, i, z)) = 0
(ii)
âŠ¢i â‰¤x
(iii)
âŠ¢z â‰¤Sub(x, i, z)
(iv)

II.6. Derivability Conditions; Fixed Points
295
We get (i) by âŠ¢Deriv(y, x) (II.6.16), while (ii) follows from II.6.11, (i),
and the assumptions. We get (iii) from the deï¬nition of Free. Finally, (iv) is
by II.6.10.
The other case is to add Free(i, x) > 0. Then âŠ¢Sub(x, i, z) = x (see Ex-
ercise II.19); hence we need only show that
âŠ¢Deriv(y, x) â†’Deriv(y âˆ—âŸ¨xâŸ©, x)
We leave this as an easy exercise (Exercise II.20).
â–¡
We can also state a special case: For any formula A, variable vi and term t
substitutable for vi in A,
âŠ¢Deriv(y, âŒœAâŒ) â†’Deriv(y âˆ—âŸ¨subi(âŒœAâŒ, âŒœtâŒ)âŸ©, subi(âŒœAâŒ, âŒœtâŒ))
Translating the last two corollaries in terms of Î˜ we obtain
II.6.19 Corollary.
âŠ¢Î˜(u) âˆ§Î˜(âŸ¨âŒœâ†’âŒ, u, xâŸ©) â†’Î˜(x)
II.6.20 Corollary.
âŠ¢Term(z) = 0 âˆ§Sub(x, i, z) > 0 â†’Î˜(x) â†’Î˜(Sub(x, i, z))
II.6.21 Remark. (1) A special case of Corollary II.6.19 is worth stating: For
any formulas A and B over LA,
âŠ¢Î˜(âŒœAâŒ) âˆ§Î˜(âŒœA â†’B âŒ) â†’Î˜(âŒœB âŒ)
(2) We omit the rather trivial (â€œtrivialâ€ given the tools we already have devel-
oped, that is) proofs of II.6.19â€“II.6.20. Sufï¬ce it to observe that, for example,
using Ax2 and |=Taut, we get from II.6.17
âŠ¢Deriv(y, u) âˆ§Deriv(z, âŸ¨âŒœâ†’âŒ, u, xâŸ©) â†’Î˜(x)
Using âˆƒ-introduction, we can now get rid of y and z.
â–¡
In order to focus the mind, we now state the derivability conditions that hold
for Î˜ (Ableitbarkeitsforderungen of Hilbert and Bernays (1968)). We will need
to establish that they indeed do hold in order to meet our goal of this section.

296
II. The Second Incompleteness Theorem
II.6.22 Deï¬nition (Derivability Conditions). The following statements are
the derivability conditions for the derivability predicate Î˜ appropriate for .
For any formulas A and B over LA,â€ 
DC 1. If âŠ¢ A, then âŠ¢C Î˜(âŒœAâŒ).
DC 2. âŠ¢C Î˜(âŒœAâŒ) âˆ§Î˜(âŒœA â†’B âŒ) â†’Î˜(âŒœB âŒ).
DC 3. For any primitive recursive term t over LN C and any numbers a1, . . . , an
in N, âŠ¢C t(
a1, . . . , 
an) = 0 â†’Î˜(âŒœt(
a1, . . . , 
an) = 0âŒ).
â–¡
II.6.23 Remark. DC 3 above is pretty much the Hilbert-Bernays (1968) third
derivability condition. DC1â€“DC2 are actually LÂ¨obâ€™s versions. LÂ¨ob also has a
different third derivability condition â€“ a formalized version of DC 1 â€“ that we
prove later (see II.6.38).
â–¡
Now, DC 1 was settled in II.6.3(4), while DC 2 is II.6.21(1). Thus we focus
our effort on proving DC 3. To this end, we will ï¬nd it convenient to prove a
bit more: namely, that DC 3 is true even if instead of the 
a1, . . . , 
an we use free
variables x1, . . . , xn. To formulate such a version we will employ, for any term
or formula E(x1, . . . , xn),â€¡ a primitive recursive term gE(x1, . . . , xn) such that,
for all a1, . . . , an in N,
âŠ¢C âŒœE(
a1, . . . , 
an)âŒ= gE(
a1, . . . , 
an)
(1)
Assume for a moment that we have obtained such a family of g-terms, and let
gt=0(x1, . . . , xn) be appropriate for the formula t(x1, . . . , xn) = 0. If now we
manage to prove
âŠ¢t(x1, . . . , xn) = 0 â†’Î˜(gt=0(x1, . . . , xn))
then DC 3 follows by substitution and Ax4 from (1) above.
To this end, we ï¬rst address an important special case: We introduce a
function symbol Num such that
âŠ¢Num(n) = âŒœnâŒ
for all n âˆˆN
(2)
Thus, we are saying that Num = gx, that is, a g-term appropriate for the term x.
The symbol Num â€“ and its deï¬ning axioms below â€“ are in LN C and C re-
spectively, since the introducing axioms make it clear that Num is primitive
recursive.
â€  In the interest of clarity â€“ and emphasis â€“ we have retained the subscript C of âŠ¢, wherever it was
applicable throughout the deï¬nition.
â€¡ Recall the convention on round brackets, p. 18.

II.6. Derivability Conditions; Fixed Points
297
We deï¬ne
Num(0) = âŒœ0âŒ
Num(Sx) = âŸ¨âŒœSâŒ, Num(x)âŸ©
(Num)
To see that the above deï¬nition behaves as required by (2), we do meta-
mathematical induction on n âˆˆN: For n = 0, the claim is settled by the ï¬rst
equation in the group (Num) and by âŠ¢0 = 0 (deï¬nition of â€œnâ€, p. 171).
Assume the claim for a ï¬xed n. Now (deï¬nition), âŠ¢
n + 1 = Sn; thus (Ax4
and second equation in (Num))
âŠ¢Num( 
n + 1) = âŸ¨âŒœSâŒ, Num(n)âŸ©
âŠ¢âŸ¨âŒœSâŒ, Num(n)âŸ©= âŸ¨âŒœSâŒ, âŒœnâŒâŸ©
by I.H. and Ax4
âŠ¢âŸ¨âŒœSâŒ, âŒœnâŒâŸ©= âŒœSnâŒ
by deï¬nition of â€œâŒœStâŒâ€
âŠ¢âŒœSnâŒ= âŒœ
n + 1âŒ
by deï¬nition of n
Intuitively, Num(x) is the GÂ¨odel number of
S . . . S
  
x copies
0
(3)
where x is a formal variable (not just a closed term n). If we are right in this
assessment, then it must be case that âŠ¢Term(Num(x)) = 0. Moreover, even
though the expression in (3),â€  intuitively, â€œdependsâ€ on x, it still contains no
variables (it is a sequence of copies of the symbol â€œSâ€ followed by â€œ0â€), so that
we expect âŠ¢Free(y, Num(x)) > 0.
Both expectations are well founded.
II.6.24 Lemma. âŠ¢Term(Num(x)) = 0.
Proof. Formal induction on x. For x = 0, we want âŠ¢Term(âŒœ0âŒ) = 0. This is
the case, by II.3.8, since T erm(gn(0)) = 0 is true.
We now prove
âŠ¢Term(Num(Sx)) = 0
(4)
based on the obvious I.H. It sufï¬ces (by (Num) above) to prove
âŠ¢Term

âŸ¨âŒœSâŒ, Num(x)âŸ©

= 0
â€  Of course, such an expression is not well formed in our formalism, because it is a variable length
term. This is why we emphasized the word â€œintuitivelyâ€ twice in this comment.

298
II. The Second Incompleteness Theorem
Turning to the deï¬nition of Term (p. 267), we ï¬nd that the third disjunct is
provable, since
âŠ¢Seq
%
âŸ¨âŒœSâŒ, Num(x)âŸ©) âˆ§lh(âŸ¨âŒœSâŒ, Num(x)âŸ©) = SS0
âˆ§f unc

(âŸ¨âŒœSâŒ, Num(x)âŸ©)0, S0

âˆ§Term

(âŸ¨âŒœSâŒ, Num(x)âŸ©)S0

= 0
&
,
the last conjunct by I.H.
We are done, by deï¬nition by cases ((15), p. 221).
â–¡
II.6.24 says that Num(x) â€œbehavesâ€ like the GÂ¨odel number of a term. It does
not say that there is a term t such that Num(x) = âŒœtâŒ. (See also the previous
footnote, and also recall that GÂ¨odel numbers of speciï¬c expressions are closed
terms.)
II.6.25 Lemma. âŠ¢Free(y, Num(x)) > 0.
Proof. Wedo(formal)inductionon x. For x = 0wewantâŠ¢Free(y, âŒœ0âŒ) > 0,
which is correct (if we examine the cases in the deï¬nition of Free, p. 267, only
the â€œotherwiseâ€ applies).
To conclude, we examine Free(y, Num(Sx)) (with frozen free variables x
and y). By II.6.24,
âŠ¢Term(Num(Sx)) = 0
so we need only examine (see (Fr), p. 267)
(âˆƒz)<SS0(0 < z âˆ§Free(y, (âŸ¨âŒœSâŒ, Num(x)âŸ©)z) = 0)
(5)
We want to refute (5), so we add it as an assumption (with frozen free variables
x and y â€“ proof by contradiction, I.4.21).
We take a new constant, a, and assume
a < SS0 âˆ§0 < a âˆ§Free

y, (âŸ¨âŒœSâŒ, Num(x)âŸ©)a

= 0
One now gets âŠ¢a = S0 from the ï¬rst two conjuncts;â€  hence
âŠ¢Free(y, Num(x)) = 0
from the last conjunct.
We have just contradicted the I.H. Free(y, Num(x)) > 0. Thus, the nega-
tion of (5) is provable. It now follows that the â€œotherwiseâ€ case (conjunc-
tion of the negations of the explicit cases) is provable in the deï¬nition of
â€  The ï¬rst yields a â‰¤S0, and the second yields S0 â‰¤a.

II.6. Derivability Conditions; Fixed Points
299
Free(y, Num(Sx)). Therefore, âŠ¢Free(y, Num(Sx)) = S0 (see also (15),
p. 221).
â–¡
It is now clear what the gE-terms must be:
Let E(vi1, . . . , vin) be a term or formula where the vi j are (names for) the
formal object variables, as these were constructed on p. 166 from the symbols
of a ï¬nite alphabet. Let us introduce the informal abbreviation
si(x, y) = subi(x, Num(y))
(s)
Informal, because we do not need to introduce a formal symbol. All we need
is convenience. Were we to introduce si formally, it would then be a primi-
tive recursive function symbol of LN C and (s) would be the deï¬ning axiom
(essentially, composition). As it stands now, it is just a metatheoretical symbol.
Then gE(x1, . . . , xn) â€“ where the xi are arbitrary metavariables, possibly
overlapping with the vi j â€“ is an informal abbreviation for the termâ€ 
sin( . . . si2(si1(âŒœEâŒ, x1), x2), . . . , xn)
(6)
Indeed Ax4, (2) (p. 296), and (s) above imply, for any n âˆˆN,
âŠ¢C si j(x,n) = subi j(x, âŒœnâŒ)
Thus (p. 279)
âŠ¢C si j(âŒœE[vi j]âŒ,n) = âŒœE[n]âŒ
Repeating the above for each free variable of E, we obtain
âŠ¢C gE(
m1, . . . , 
mn) = âŒœE(
m1, . . . , 
mn)âŒ
It is clear that gE is a primitive recursive term (each subi(x, y) is).
We abbreviate (6) â€“ i.e., gE(x1, . . . , xn) â€“ by yet another informal notation as
in Hilbert and Bernays (1968):
{E(x1, . . . , xn)}
(7)
The reader will exercise care not to confuse the syntactic (meta)variables xj
introduced in (6) and (7) with the formal variables that occur free in the ex-
pression E. The latter are the vi j, as already noted when introducing (6). In
particular, the â€œiâ€ in subi is that of the formal vi, not that of the â€œsyntacticâ€ xi.â€¡
â€  By II.6.25, the order of substitutions is irrelevant. â€œEâ€ and â€œE(vi1, . . . , vin)â€ are, of course,
interchangeable. The latter simply indicates explicitly the list of all free variables of E.
â€¡ Unless, totally coincidentally, xi denotes vi in some context. See the informal deï¬nition (s)
above, and also the deï¬nition (subi), p. 268.

300
II. The Second Incompleteness Theorem
We may write simply {E} if the variables x1, . . . , xn are implied by the
context.
We note that while âŒœE(x1, . . . , xn)âŒis a closed term, {E(x1, . . . , xn)} is a
term with precisely x1, . . . , xn as its list of free variables.
II.6.26 Example. âŠ¢{x} = Num(x). Indeed,
âŠ¢{x} = subm(âŒœvmâŒ, Num(x))
But âŠ¢subm(âŒœvmâŒ, Num(x)) = Num(x) (by the deï¬nition of Sub, ï¬rst case,
p. 268). For any terms
t(vj1, . . . , vjhvm1, . . . , vmn)
and
s(vj1, . . . , vjh, vk1, . . . , vkr )
and any choice of (meta)variables
x1, . . . , xh, y1, . . . , yn, z1, . . . , zr
(1)
We have
âŠ¢{(t = s)(x1, . . . , xh, y1, . . . , yn, z1, . . . , zr)} =
âŸ¨âŒœ= âŒ, {t(x1, . . . , xh, z1, . . . , zr)}, {s(x1, . . . , xh, y1, . . . , yn)}âŸ©
(2)
Now that the list (1) has been recorded, we will feel free to abbreviate (2) as
âŠ¢{t = s} = âŸ¨âŒœ= âŒ, {t}, {s}âŸ©
(3)
We proceed to establish (2) (or (3)) recalling that the order of substitution
is irrelevant, and using â€œ=â€ conjunctionally below (i.e., âŠ¢t = s = r means
âŠ¢t = s and âŠ¢s = r):
âŠ¢{(t = s)(x1, . . . , xh, y1, . . . , yn, z1, . . . , zr)} =
skr (. . . smn(. . . s jh(. . . s j1(âŒœt = sâŒ, x1), . . . , xh), . . . , yn), . . . , zr) =
âŸ¨âŒœ= âŒ, skr (. . . smn(. . . s jh(. . . s j1(âŒœtâŒ, x1), . . . , xh), . . . , yn), . . . , zr),
skr (. . . smn(. . . s jh(. . . s j1(âŒœsâŒ, x1), . . . , xh), . . . , yn), . . . , zr)âŸ©=
âŸ¨âŒœ= âŒ, {t(x1, . . . , xh, z1, . . . , zr)}, {s(x1, . . . , xh, y1, . . . , yn)}âŸ©
The computations above iterate case 4 of the deï¬nition of Sub (subcase for
AF, p. 268), keeping in mind informal deï¬nition (s) (p. 299). This iterative
calculation relies on II.6.4 and II.6.24 to ensure that
âŠ¢Term(Sub(âŒœEâŒ,i, Num(w))) = 0
at every step (E is t or s).

II.6. Derivability Conditions; Fixed Points
301
We have abbreviated
âŒœ(t = s)(vj1, . . . , vjhvm1, . . . , vmn, vk1, . . . , vkr )âŒ
by
âŒœt = sâŒ
âŒœt(vj1, . . . , vjh, vm1, . . . , vmn)âŒ
by
âŒœtâŒ
and
âŒœs(vj1, . . . , vjh, vk1, . . . , vkr)âŒ
by
âŒœsâŒ
We have also freely used during our computation the fact that
âŠ¢Free(i, x) > 0 âˆ§Term(x) = 0 â†’Sub(x, i, z) = x
(see Exercise II.19).
The same type of computation â€“ employing II.6.11, II.6.12, II.6.24, and
II.6.25 â€“ establishes that
âŠ¢{A â†’B } = âŸ¨âŒœâ†’âŒ, {A}, {B }âŸ©
We have used the short { }-notation above.
The behaviour of the symbol { } is not very stable with respect to substitution.
For example, we have found that âŠ¢{x} = Num(x) above. However, it is not
true that for arbitrary unary f
âŠ¢{ f x} = Num( f x)
(4)
(see Exercise II.21). On the other hand, (4) does hold if f â‰¡S. Indeed (using,
once again, â€œ=â€ conjunctionally), we ï¬nd that
âŠ¢{Sx} = sub1(âŒœSv1âŒ, Num(x))
= âŸ¨âŒœSâŒ, sub1(âŒœv1âŒ, Num(x))âŸ©
= âŸ¨âŒœSâŒ, Num(x)âŸ©
= Num(Sx)
â–¡
II.6.27 Lemma. âŠ¢C Î˜(x) â†’Î˜(Sub(x, i, Num(y))).
Proof. By modus ponens from II.6.20, since
âŠ¢Term(Num(y)) = 0
by II.6.24, and
âŠ¢Î˜(x) â†’Sub(x, i, Num(y)) > 0
â€“ the latter from âŠ¢Î˜(x) â†’WFF(x) = 0 (cf. II.6.16), II.6.12, and II.6.25. â–¡

302
II. The Second Incompleteness Theorem
II.6.28 Corollary. For any formula A over LA and variable vi,
âŠ¢C Î˜(âŒœA[vi]âŒ) â†’Î˜
%
Sub

âŒœA[vi]âŒ,i, Num(x)
&
Iterating application of the above for all the free variables in A, we obtain
at once:
II.6.29 Corollary. For any formula A over LA,
âŠ¢C Î˜(âŒœA(vi1, . . . , vin)âŒ) â†’Î˜

{A(x1, . . . , xn)}

Of course, we also obtain a useful special case â€“ where xj denotes vi j â€“ as
follows: For any formula A over LA,
âŠ¢C Î˜(âŒœA(x1, . . . , xn)âŒ) â†’Î˜

{A(x1, . . . , xn)}

The next corollary follows at once from the above remark and DC 1.
II.6.30 Corollary (The Free Variables Version of DC 1). For any formula A
over LA, if âŠ¢ A(x1, . . . , xn), then âŠ¢C Î˜

{A(x1, . . . , xn)}

.
II.6.31 Lemma (The Free Variables Version of DC 2). For any formulas A
and B over LA,
âŠ¢C Î˜

{A â†’B }

â†’Î˜

{A}

â†’Î˜

{B }

Proof. {A â†’B } abbreviates
âŸ¨âŒœâ†’âŒ, subin(. . . subi1(âŒœAâŒ, Num(x1)), . . . , Num(xn)),
subin(. . . subi1(âŒœB âŒ, Num(x1)), . . . , Num(xn))âŸ©
while {A} and {B } abbreviate
subin(. . . subi1(âŒœAâŒ, Num(x1)), . . . , Num(xn))
and
subin(. . . subi1(âŒœB âŒ, Num(x1)), . . . , Num(xn))
respectively. We are done, by II.6.19.
Pause. Why is
âŠ¢WFF

subin(. . . subi1(âŒœB âŒ, Num(x1)), . . . , Num(xn))

= 0?
â–¡

II.6. Derivability Conditions; Fixed Points
303
II.6.32 Remark. In what follows, all the way to the end of the section, we as-
sume â€“ without loss of generality â€“ that our unspeciï¬ed consistent Î£1 extension,
, of PA also extends C, the theory of GÂ¨odel coding.
Here is why generality is not lost in the case where we were unfortunate
enough to start with a  that did not satisfy the assumption:
(1) We start with PA â‰¤,â€  where Î“ is Î£1(LNC).
(2) We extend  â€“ to â€² â€“ by adding the axioms for all the deï¬ned symbols
that were added to PA in order to form C and LN C. This results to the
language LNâ€². So, not only do we have PA â‰¤C conservatively, but also
 â‰¤â€² conservatively.
(3) Since  is consistent, the â€œconservativelyâ€ part above ensures that so is â€².
Trivially, PA â‰¤â€²and C â‰¤â€².
(4) If Q is a formula that semantically deï¬nes the nonlogical axioms of C,
that is, Q(âŒœAâŒ) means that A is a C-axiom, then Î“â€²(x) can be taken to
abbreviate
Î“(x) âˆ¨Q(x)
(âˆ—)
Given that Q(x) is primitive recursive (Exercise II.23â€¡), the formula (âˆ—) is
Î£1(LNC) (Exercise II.24).
â–¡
II.6.33 Lemma. For any terms t, s and variable z, if âŠ¢C t = s and
âŠ¢C t = z â†’Î˜

{t = z}

then also
âŠ¢C s = z â†’Î˜

{s = z}

where we have written â€œÎ˜â€ for â€œÎ˜Î“â€.
Proof. â€œâŠ¢â€ means â€œâŠ¢Câ€. By Ax4
âŠ¢t = z â†’s = z
(1)
and
âŠ¢s = z â†’t = z
(2)
â€  The relation â€œâ‰¤â€ that compares two theories was introduced on p. 46.
â€¡ C contains the ï¬nitely many deï¬ning axioms we have introduced towards the GÂ¨odel coding.
Moreover it contains the inï¬nitely many axioms introducing the primitive recursive symbols. A
deï¬ning predicate for this part can be introduced by a course-of-values recursion, as can be easily
deduced from the speciï¬c allocation scheme chosen for primitive recursive function symbols.
See Remark II.3.7, p. 254.

304
II. The Second Incompleteness Theorem
By DC 1 and DC 2 (free variables versions, II.6.30 and II.6.31) in that order,
(1) yields
âŠ¢Î˜

{t = z}

â†’Î˜

{s = z}

(3)
where we have used the blanket assumption â€œC â‰¤â€ (II.6.32) in the application
of DC 1 (DC 1 assumes â€œâŠ¢ . . . â€). Our unused assumption, along with (2)
and (3), yields what we want by tautological implication.
â–¡
We now have all the tools we need towards proving DC 3.
II.6.34 Main Lemma. For any primitive recursive term t over LN C and any
variable z not free in t,
âŠ¢C t = z â†’Î˜

{t = z}

(1)
where we have written â€œÎ˜â€ for â€œÎ˜Î“â€.
We continue the practice of omitting the subscript C from âŠ¢(a subscript will
be used if other than C, or as a periodic reminder (or emphasis) if it is C). The
implicit assumption â€œC â‰¤â€ (II.6.32) enables II.6.30 (DC 1, free variables
version) exactly as in the case of II.6.33.
Here is how not to prove the lemma: â€œBy Ax4 it sufï¬ces to prove
âŠ¢Î˜

{t = t}

This follows from âŠ¢t = t and II.6.30.â€
Such an attempt does not heed the warning in Example II.6.26. Recall that
âŠ¢{t = z} = âŸ¨âŒœ= âŒ, {t}, Num(z)âŸ©. However, we have already noted that it
is not true in general that âŠ¢Num(t) = {t}. Thus, in general,
Ì¸âŠ¢{t = z}[z â†t] = {t = t}
Proof. The proof is by (metamathematical) induction on the formation of t and
follows the one given by Hilbert and Bernays (1968) (however, unlike them,
we do not restrict primitive recursive terms to â€œnormalizedâ€ forms).
We have three basis cases:
Case 1.
t â‰¡Zv0. Since âŠ¢Zv0 = 0, it sufï¬ces, by II.6.33, to prove the
following version of (1):
âŠ¢0 = z â†’Î˜

{0 = z}


II.6. Derivability Conditions; Fixed Points
305
By Ax4 it sufï¬ces to prove
âŠ¢Î˜

{0 = 0}

This follows from âŠ¢0 = 0 and II.6.30.
Pause. Wait a minute! How does this differ from what I said above that we
should not do?
Case 2.
t â‰¡U n
i (v0, . . . , vnâˆ’1).
Again, by II.6.33 and âŠ¢U n
i (v0, . . . , vnâˆ’1) = viâˆ’1, (1) now becomes (where
we have simpliï¬ed the metanotation: x rather than the â€œactualâ€ viâˆ’1)
âŠ¢x = z â†’Î˜

{x = z}

By Ax4 it sufï¬ces to prove
âŠ¢Î˜

{x = x}

This follows from âŠ¢x = x and II.6.30.
Pause. Was this O.K.?
Case 3.
t â‰¡Sx. (1) now is
âŠ¢Sx = z â†’Î˜

âŸ¨âŒœ= âŒ, {Sx}, Num(z)âŸ©

By Ax4 it sufï¬ces to prove
âŠ¢Î˜

âŸ¨âŒœ= âŒ, {Sx}, Num(Sx)âŸ©

â€“ that is (II.6.26), âŠ¢Î˜

âŸ¨âŒœ= âŒ, {Sx}, {Sx}âŸ©

, or (II.6.26 again)
âŠ¢Î˜

{Sx = Sx}

This follows from âŠ¢Sx = Sx and II.6.30.
We now embark on the two induction steps:
Composition.
Suppose that t â‰¡f s1 . . . sn, where the function f and the
terms si are primitive recursive.
Let (I.H.)
âŠ¢f x1 . . . xn = z â†’Î˜

{ f x1 . . . xn = z}

(2)
and
âŠ¢si = xi â†’Î˜

{si = xi}

for i = 1, . . . , n
(3)

306
II. The Second Incompleteness Theorem
where none of the xi are free in any of the s j, and, moreover, z is not one of the
xi. By Ax4
âŠ¢s1 = x1 â†’s2 = x2 â†’Â· Â· Â· â†’sn = xn â†’
f s1 . . . sn = z â†’f x1 . . . xn = z
(4)
and
âŠ¢s1 = x1 â†’s2 = x2 â†’Â· Â· Â· â†’sn = xn â†’
f x1 . . . xn = z â†’f s1 . . . sn = z
(5)
By (2) and (4) (and tautological implication),
âŠ¢s1 = x1 â†’s2 = x2 â†’Â· Â· Â· â†’sn = xn â†’
f s1 . . . sn = z â†’Î˜

{ f x1 . . . xn = z}

(6)
By II.6.30 and (5),
âŠ¢Î˜

{s1 = x1 â†’s2 = x2 â†’Â· Â· Â· â†’sn = xn â†’
f x1 . . . xn = z â†’f s1 . . . sn = z}

Hence, by II.6.31,
âŠ¢Î˜

{s1 = x1}

â†’Î˜

{s2 = x2}

â†’Â· Â· Â· â†’Î˜

{sn = xn}

â†’
Î˜

{ f x1 . . . xn = z}

â†’Î˜

{ f s1 . . . sn = z}

The above and (3) tautologically imply
âŠ¢s1 = x1 â†’s2 = x2 â†’Â· Â· Â· â†’sn = xn â†’
Î˜

{ f x1 . . . xn = z}

â†’Î˜

{ f s1 . . . sn = z}

The above and (6) tautologically imply
âŠ¢s1 = x1 â†’s2 = x2 â†’Â· Â· Â· â†’sn = xn â†’
f s1 . . . sn = z â†’Î˜

{ f s1 . . . sn = z}

(7)
Finally, since the xi are not free in any of the s j and are distinct from z, the
substitutions [xi â†si] into (7), and tautological implication, imply
âŠ¢f s1 . . . sn = z â†’Î˜

{ f s1 . . . sn = z}

Primitive recursion.
We are given that h(âƒ—y ) and g(x,âƒ—y, w) are primitive
recursive terms, and that f has been introduced to satisfy
âŠ¢f (0,âƒ—y ) = h(âƒ—y )
âŠ¢f (Sx,âƒ—y ) = g(x,âƒ—y, f (x,âƒ—y ))
(8)
Supposing that z is distinct from x,âƒ—y, we want to show
âŠ¢f (x,âƒ—y ) = z â†’Î˜

{ f (x,âƒ—y ) = z}


II.6. Derivability Conditions; Fixed Points
307
To allow the ï¬‚exibility of splitting the induction step into an I.H. and a con-
clusion (as usual practice dictates, using the deduction theorem), we prove the
following (provably) equivalent form instead:
âŠ¢(âˆ€z)
%
f (x,âƒ—y ) = z â†’Î˜

{ f (x,âƒ—y ) = z}
&
(9)
under the same restriction, that the bound variable z is not among x,âƒ—y.
Now, our metamathematical I.H. (on the formation of primitive recursive
terms t) â€“ under the assumption that z is not among x,âƒ—y, w â€“ is
âŠ¢h(âƒ—y ) = z â†’Î˜

{h(âƒ—y ) = z}

âŠ¢g(x,âƒ—y, w) = z â†’Î˜

{g(x,âƒ—y, w) = z}

(10)
(9) is proved by formal induction on x.
For the formal basis of (9), let us set x â†0. By the ï¬rst of (8), the ï¬rst
of (10), and II.6.33 (followed by generalization) we deduce the basis, namely,
âŠ¢(âˆ€z)
%
f (0,âƒ—y ) = z â†’Î˜

{ f (0,âƒ—y ) = z}
&
Now assume (9) (formal I.H.) for frozen x,âƒ—y, and prove
âŠ¢(âˆ€z)
%
f (Sx,âƒ—y ) = z â†’Î˜

{ f (Sx,âƒ—y ) = z}
&
(11)
By generalization, it sufï¬ces to prove
âŠ¢f (Sx,âƒ—y ) = z â†’Î˜

{ f (Sx,âƒ—y ) = z}

(11â€²)
(where z is not among x,âƒ—y). We choose w, not among x,âƒ—y, z. By Ax4,
âŠ¢f (x,âƒ—y ) = w â†’g(x,âƒ—y, f (x,âƒ—y )) = z â†’g(x,âƒ—y, w) = z
âŠ¢f (x,âƒ—y ) = w â†’g(x,âƒ—y, w) = z â†’g(x,âƒ—y, f (x,âƒ—y )) = z
which by the second part of (8) and Ax4 translate into
âŠ¢f (x,âƒ—y ) = w â†’f (Sx,âƒ—y ) = z â†’g(x,âƒ—y, w) = z
âŠ¢f (x,âƒ—y ) = w â†’g(x,âƒ—y, w) = z â†’f (Sx,âƒ—y ) = z
(12)
Tautological implication using the second part of (10) and the ï¬rst of (12)
yields
âŠ¢f (x,âƒ—y ) = w â†’f (Sx,âƒ—y ) = z â†’Î˜

{g(x,âƒ—y, w) = z}

(13)
II.6.30â€“II.6.31 and the second part of (12) yield
âŠ¢Î˜

{ f (x,âƒ—y ) = w}

â†’Î˜

{g(x,âƒ—y, w) = z}

â†’Î˜

{ f (Sx,âƒ—y ) = z}


308
II. The Second Incompleteness Theorem
Using the I.H. (9), followed by specialization (and with the help of |=Taut), this
yields
âŠ¢f (x,âƒ—y ) = w â†’Î˜

{g(x,âƒ—y, w) = z}

â†’Î˜

{ f (Sx,âƒ—y ) = z}

Here is where the (universally quantiï¬ed) form of (9) helped formally. We have
been manipulating the z-variable by substituting w. This would be enough
to unfreeze variables frozen between I.H. and conclusion, thus invalidating the
deduction theorem step. However, that is not the case here, because this variable
manipulation is hidden from the deduction theorem. z is a bound variable in the
assumption (9).
Tautological implication, using the above and (13), furnishes
âŠ¢f (x,âƒ—y ) = w â†’f (Sx,âƒ—y ) = z â†’Î˜

{ f (Sx,âƒ—y ) = z}

Finally, since x, w, z,âƒ—y are all distinct, the substitution [w â†f (x,âƒ—y )] in the
above yields (11â€²) by tautological implication, and we thus have (11) by gener-
alization.
â–¡
At this point the reader may take a well-deserved break.
Next, we prove
II.6.35 Corollary. For any primitive recursive predicate P over LN C,
âŠ¢C P(x1, . . . , xn) â†’Î˜

{P(x1, . . . , xn)}

Proof. Indeed,
âŠ¢P(x1, . . . , xn) â†’Ï‡P(x1, . . . , xn) = 0
(1)
and
âŠ¢Ï‡P(x1, . . . , xn) = 0 â†’P(x1, . . . , xn)
(2)
where Ï‡P is â€œtheâ€ (primitive recursive) characteristic function (see II.3.6,
p. 253, and II.1.25, p. 220) for P(x1, . . . , xn). By the main lemma (II.6.34),
âŠ¢Ï‡P(x1, . . . , xn) = 0 â†’Î˜

{Ï‡P(x1, . . . , xn) = 0}

Hence (by (1))
âŠ¢P(x1, . . . , xn) â†’Î˜

{Ï‡P(x1, . . . , xn) = 0}

(3)

II.6. Derivability Conditions; Fixed Points
309
By (2) and II.6.30â€“II.6.31,
âŠ¢Î˜

{Ï‡P(x1, . . . , xn) = 0}

â†’Î˜

{P(x1, . . . , xn)}

This and (3) yield what we want, by tautological implication.
â–¡
We can prove a bit more:
II.6.36 Corollary. For any Î£1(LNC) formula A,
âŠ¢C A (x1, . . . , xn) â†’Î˜

{A (x1, . . . , xn)}

Proof. Let A (x1, . . . , xn) be Î£1(LNC).
Then, for some âˆ†0(LNC) formula Q (y, x1, . . . , xn), where y is distinct
from x1, . . . , xn,
âŠ¢A (x1, . . . , xn) â†”(âˆƒy)Q (y, x1, . . . , xn)
We introduce a predicate R by the deï¬nition
R(y,âƒ—xn) â†”Q (y,âƒ—xn)
(4)
Then R is primitive recursive (Exercise II.22).
By II.6.35,
âŠ¢R(y, x1, . . . , xn) â†’Î˜

{R(y, x1, . . . , xn)}

Hence, by (4) and tautological implication,
âŠ¢Q (y, x1, . . . , xn) â†’Î˜

{R(y, x1, . . . , xn)}

(5)
By II.6.30â€“II.6.31 and the â†’-part of (4),
âŠ¢Î˜

{R(y, x1, . . . , xn)}

â†’Î˜

{Q (y, x1, . . . , xn)}

Hence (by (5))
âŠ¢Q (y, x1, . . . , xn) â†’Î˜

{Q (y, x1, . . . , xn)}

(6)
Now,
âŠ¢Q (y, x1, . . . , xn) â†’(âˆƒy)Q (y, x1, . . . , xn)
Hence, by II.6.30â€“II.6.31,
âŠ¢Î˜

{Q (y, x1, . . . , xn)}

â†’Î˜

{(âˆƒy)Q (y, x1, . . . , xn)}


310
II. The Second Incompleteness Theorem
Combining the above with (6) via |=Taut, we get
âŠ¢Q (y, x1, . . . , xn) â†’Î˜

{(âˆƒy)Q (y, x1, . . . , xn)}

which yields what we want via âˆƒ-introduction.
â–¡
The following corollary is a step backward in terms of degree of generality,
but it is all we really need. It is a formalized DC 1, for it says â€œif it is true that
A is provable, then it is also true that Î˜(âŒœAâŒ) is provableâ€.
II.6.37 Corollary (LÂ¨ob). For any formula A over LN C,
âŠ¢C Î˜

âŒœAâŒ

â†’Î˜
%
âŒœÎ˜

âŒœAâŒ

âŒ
&
Proof. For any ï¬xed A, Î˜(âŒœAâŒ) is a Î£1(LNC) sentence.
â–¡
II.6.38 Remark.
The above corollary is LÂ¨obâ€™s third derivability condition,
DC 3.
â–¡
II.6.39 Theorem (The Fixpoint Theorem, or Diagonalization Lemma). For
any formula A(x) over LN C, there is a sentence B over the basic language of
PA, LN, such that
âŠ¢C B â†”A(âŒœB âŒ)
B is called a ï¬xed point or ï¬xpoint of A in C.
The assumption C â‰¤ of II.6.32 is not used here.
Proof. Let C (x) be the formula obtained from A(s1(x, x)) after removing all
deï¬ned symbols (following I.7.1 and I.7.3). Then C is over LN, and
âŠ¢C C (x) â†”A(s1(x, x))
(1)
Let next n âˆˆN be such that (cf. (1), p. 274)
âŠ¢C n = âŒœC (x)âŒ
Hence (p. 279, 297, and 299)
âŠ¢C s1(n,n) = âŒœC (n)âŒ
(2)
By (1) and substitution,
âŠ¢C C (n) â†”A(s1(n,n))

II.6. Derivability Conditions; Fixed Points
311
By Ax4 and (2), the above yields
âŠ¢C C (n) â†”A

âŒœC (n)âŒ

Thus â€œC (n)â€ is the sentence â€œB â€ we want.
â–¡
Applying the above to Â¬Î˜, we obtain a sentence that semantically says â€œI
am not a theorem of â€.
II.6.40 Corollary (GÂ¨odel). There is a sentence G over the basic language LN
such that
âŠ¢C G â†”Â¬Î˜Î“(âŒœG âŒ)
(1)
We now need to revisit (half of) GÂ¨odelâ€™s ï¬rst incompleteness theorem. We
will show that the sentence G above is not provable in .â€  For the balance of
the section we will be careful to subscript Î˜ with the name, say , of the theory
for which it is a provability predicate.
II.6.41 Lemma (GÂ¨odel). Let  over LA be a consistent extension of PA such
that Î“(x) is Î£1(LNC). Let the sentence G over LN be a ï¬xed point of Â¬Î˜ Î“ in
C. Then Ì¸âŠ¢ G .
The assumption C â‰¤ of II.6.32 is not used here.
Proof. Assume that
âŠ¢ G
(2)
By the assumption on Î“(x), DC 1 is applicable; hence
âŠ¢C Î˜Î“(âŒœG âŒ)
(1) of II.6.40 now yields
âŠ¢C Â¬G
Since PA â‰¤C conservatively, and G is over LN, we have âŠ¢PA Â¬G , and hence,
by PA â‰¤,
âŠ¢ Â¬G
The above and (2) contradict the consistency of .
â–¡
â€  The other half, which we do not need towards the proof of the second incompleteness theorem,
uses a stronger consistency assumption â€“ Ï‰-consistency (p. 189) â€“ and states that G is not refut-
able either.

312
II. The Second Incompleteness Theorem
We continue honouring the assumption of II.6.32; however, we make it explicit
here. That is, we have the following situation:
(1) PA â‰¤ consistently (Î“ in Î£1(LNC)).
(2)  â‰¤â€² conservatively, by adding the symbols (and their deï¬nitions) of C.
Î“â€² is in Î£1(LNC)
We let the metasymbol â€œConâ€ stand for â€œ is consistentâ€. With the help of the
arithmetization tools this can be implemented as a sentence whose semantics
is that some convenient refutable formula is not a -theorem.
â€œ0 = S0â€ is our choice of a refutable formula (cf. S1). We may now deï¬ne
Con
abbreviates
Â¬Î˜Î“â€²(âŒœ0 = S0âŒ)
since 0 = S0 is unprovable in  iff it is so in â€²; therefore Â¬Î˜Î“â€²(âŒœ0 = S0âŒ) â€“
that is, Con â€“ says both â€œâ€² is consistentâ€ and â€œ is consistentâ€.
We will next want to show that, under some reasonable assumptions, 
cannot prove Con, that is, it cannot prove its own consistency.
For this task to be meaningful (and â€œfairâ€ to  ), Con (that is, the actual
formula it stands for) must be in the language of , LA. We can guarantee
this if we deï¬ne Con to instead abbreviate the sentence S obtained from
Â¬Î˜Î“â€²(âŒœ0 = S0âŒ) by elimination of deï¬ned symbols. Thus, we ï¬nalize the def-
inition as
Con
abbreviates
S
(âˆ—)
where S is over the basic language LN of PA and satisï¬es
âŠ¢C S â†”Â¬Î˜Î“â€²(âŒœ0 = S0âŒ)
(âˆ—âˆ—)
II.6.42 Theorem (GÂ¨odelâ€™s Second Incompleteness Theorem). Let  be a
consistent extension of PA such that Î“(x) is Î£1(LNC). Then Ì¸âŠ¢ Con.
Proof. Let  â‰¤â€² as in the discussion above (we continue being explicit here
about our blanket assumption II.6.32; thus C â‰¤â€², while C â‰¤ might fail).
The proof utilizes the ï¬xed point G of Â¬Î˜Î“â€² that is obtained in the manner
of II.6.40â€“II.6.41 above (note however the prime). Our aim is to show thatâ€ 
âŠ¢â€² Con â†’G
(3)
We start with the observation
|=Taut Â¬G â†’(G â†’0 = S0)
â€  Cf. the informal discussion at the beginning of this chapter, p. 205.

II.6. Derivability Conditions; Fixed Points
313
Hence (absolutely)
âŠ¢Â¬G â†’(G â†’0 = S0)
which by (1) of II.6.40 (but using the theory â€² rather than ) and the Leibniz
rule implies
âŠ¢C Î˜Î“â€²(âŒœG âŒ) â†’(G â†’0 = S0)
DC 1 now yields (this uses C â‰¤â€²)
âŠ¢C Î˜Î“â€²
%
âŒœÎ˜Î“â€²
âŒœG âŒ

â†’(G â†’0 = S0)âŒ
&
which further yields, via DC 2,
âŠ¢C Î˜Î“â€²
%
âŒœÎ˜Î“â€²
âŒœG âŒ

âŒ
&
â†’Î˜Î“â€²
âŒœG âŒ

â†’Î˜Î“â€²
âŒœ0 = S0âŒ

LÂ¨obâ€™s DC 3 (II.6.37, which also uses C â‰¤â€²) and the above yield, by tauto-
logical implication,
âŠ¢C Î˜Î“â€²(âŒœG âŒ) â†’Î˜Î“â€²(âŒœ0 = S0âŒ)
By contraposition, and using (1) of II.6.40 (for â€²) and (âˆ—âˆ—),
âŠ¢C S â†’G
(4)
Since C â‰¤â€², (4) implies (3) (via (âˆ—)).
Now if âŠ¢ Con, then also âŠ¢â€² Con by  â‰¤â€². Thus (3) would yield
âŠ¢â€² G , contradicting II.6.41 (for â€²).
â–¡
(1) The above proof is clearly also valid for the trivial extension  = PA. In
this case, â€² = C. Moreover, we note that, since PA â‰¤C conservatively
and S â†’G is over LN, (4) implies
âŠ¢PA S â†’G
as well, from which (via PA â‰¤ â€“ no prime) âŠ¢ S â†’G .
(2) It is also true that âŠ¢ G â†’Con; thus âŠ¢ G â†”Con (Exercise II.25).
II.6.41 and II.6.42 provide two examples of sentences unprovable by , a
consistent extension of PA with a 1 set of axioms. As remarked above, the
two sentences are provably equivalent (in ). The former says â€œI am not a
theoremâ€,â€  and thus its unprovability shows that it is true in A, the natural
structure appropriate for LA.
â€  Of â€² â€“ being a ï¬xed point of Â¬Î˜Î“â€² â€“ and hence not of  either.

314
II. The Second Incompleteness Theorem
Let us put II.6.32 back into (implicit) force to avoid verbosity in the discussion
and results that follow. So we have C â‰¤.
What about a sentence that says â€œI am a theorem of â€?â€  Such a sentence,
H , is a ï¬xed point of Î˜Î“, since Î˜Î“(âŒœH âŒ) â€œsaysâ€ that (i.e., is true in NC iff)
 âŠ¢H . A theorem of LÂ¨ob (1955) shows that such sentences are provable,
and hence true as well, since then Î˜Î“(âŒœH âŒ) is true, and we also know that
âŠ¢ H â†”Î˜Î“(âŒœH âŒ).
II.6.43 Theorem (LÂ¨obâ€™s Theorem (1955)). Let PA â‰¤ consistently, and
assume that the formula Î“ is Î£1(LNC). Then, for any sentence A,
âŠ¢ Î˜Î“(âŒœAâŒ) â†’A
implies that
âŠ¢ A.
Proof. Equivalently, let us prove that
Ì¸âŠ¢ A
implies that
Ì¸âŠ¢ Î˜Î“(âŒœAâŒ) â†’A
So assume that Ì¸âŠ¢ A. By I.4.21,  + Â¬A is a consistent Î£1(LNC) extension
of C (hence of PA), that is, it is semantically deï¬ned by a Î£1(LNC) formula.
Pause. Why is  + Â¬A Î£1(LNC)?
By II.6.42,
Ì¸âŠ¢+Â¬A Con+Â¬A
(1)
Now, we choose as an â€œimplementationâ€ of the sentence â€œCon+Â¬Aâ€ the sen-
tence â€œÂ¬Î˜Î“(âŒœAâŒ)â€, because of I.4.21. Thus, (1) can be written as
Ì¸âŠ¢+Â¬A Â¬Î˜Î“(âŒœAâŒ)
Hence, by modus ponens,
Ì¸âŠ¢ Â¬A â†’Â¬Î˜Î“(âŒœAâŒ)
The contrapositive is what we want: Ì¸âŠ¢ Î˜Î“(âŒœAâŒ) â†’A.
â–¡
The statement in II.6.43 is actually an equivalence. The other direction is triv-
ially true by tautological implication.
â€  This question was posed by Henkin (1952).

II.6. Derivability Conditions; Fixed Points
315
LÂ¨obâ€™s theorem can be proved from ï¬rst principles, i.e., without reliance on
GÂ¨odelâ€™s second incompleteness theorem. One starts by getting a ï¬xed point B
of Î˜Î“(x) â†’A in C and then judiciously applying the derivability conditions
(Exercise II.26).
Conversely, LÂ¨obâ€™s theorem implies GÂ¨odelâ€™s (second) theorem: Indeed, the
tautology
Â¬Î˜Î“(âŒœ0 = S0âŒ) â†’Î˜Î“(âŒœ0 = S0âŒ) â†’0 = S0
yieldsâ€ 
âŠ¢ Â¬Î˜Î“(âŒœ0 = S0âŒ) â†’Î˜Î“(âŒœ0 = S0âŒ) â†’0 = S0
Thus
âŠ¢ Con â†’Î˜Î“(âŒœ0 = S0âŒ) â†’0 = S0
Suppose now that âŠ¢ Con. Then, by the above,
âŠ¢ Î˜Î“(âŒœ0 = S0âŒ) â†’0 = S0
Hence, by LÂ¨obâ€™s theorem, âŠ¢ 0 = S0, contradicting the consistency of .
We have simpliï¬ed the hypotheses in GÂ¨odelâ€™s two theorems in allowing  to
extend PA (or, essentially equivalently, extend C in the proof of the second). The
theorems actually hold under a more general hypothesis that  contains PA (or
C). This containment can be understood intuitively, at least in the case of formal
set theory (formalized as ZFC for example):â€¡ There is enough machinery inside
ZFC to construct the set of natural numbers (Ï‰) and show that this set satisï¬es
the axioms PA.Â§ One can then carry out the arithmetization of Section II.5 for
terms, formulas, proofs, and theorems, of ZFC this time, in ZFC exactly as we
did for Peano arithmetic, and prove that both incompleteness theorems hold for
ZFC.Â¶
â€  Under the assumption of II.6.32 â€“ C â‰¤ â€“ Î˜Î“ is in the language of .
â€¡ The formal treatment requires the concept of interpreting one theory inside another. This is one
of many interesting topics that we must leave out in this brief course in logic. See however our
volume 2 for a complete discussion of this topic and its bearing on this comment here.
Â§ Thus, one constructs a model of PA inside ZFC. We show how this is done in volume 2.
Â¶ In other words, the elements of the set Ï‰ will furnish us with GÂ¨odel numbers for the formulas and
terms of ZFC, while certain terms and formulas over Ï‰ will allow ZFC to â€œargueâ€ about these
GÂ¨odel numbers.

316
II. The Second Incompleteness Theorem
II.7. Exercises
II.1. Prove that PA can actually prove axiom <3, and therefore this axiom is
dependent (redundant).
II.2. Prove in PA: âŠ¢x â‰¤y â†’Â¬y < x.
II.3. Prove in PA: âŠ¢x + y = y + x.
II.4. Prove in PA: âŠ¢x + (y + z) = (x + y) + z.
II.5. Prove in PA: âŠ¢x Ã— y = y Ã— x.
II.6. Prove in PA: âŠ¢x Ã— (y Ã— z) = (x Ã— y) Ã— z.
II.7. Prove in PA: âŠ¢x Ã— (y + z) = (x Ã— y) + (x Ã— z).
II.8. Settle the Pause regarding the displayed formula ( f ) on p. 218.
II.9. Prove the formula (LCMâ€™) on p. 230.
II.10. Prove in PA: âŠ¢x > 0 â†’x = SÎ´(x, S0).
II.11. Prove Lemma II.1.36.
II.12. Prove in PA: âŠ¢x â‰¤J(x, y), and âŠ¢y â‰¤J(x, y), where J is that of p. 234.
II.13. Prove the concluding claim in II.3.7.
II.14. Conclude the proof of Lemma II.4.6.
II.15. Prove that there is a unary formal primitive recursive predicate Î“ such
that, for any formula A, Î“(âŒœAâŒ) means that A is (an instance of) a
Peano axiom.
II.16. Prove that if the formula Î“ that semantically deï¬nes the nonlogical ax-
ioms of an extension of PA is Î£1(LNâ€²) then so are Proof , Deriv,
and Î˜.
II.17. Settle the Pause on p. 273.
II.18. Settle the Pause on p. 289.
II.19. Prove that
âŠ¢Free(i, x) > 0 âˆ§(Term(x) = 0 âˆ¨WFF(x) = 0)
â†’Sub(x, i, z) = x
II.20. Prove that
âŠ¢Deriv(y, x) â†’Deriv(y âˆ—âŸ¨xâŸ©, x)
II.21. Show that it is not true that for arbitrary unary f
âŠ¢{ f x} = Num( f x)
II.22. Prove that if Q is âˆ†0(LNC) and P is introduced by Pâƒ—x â†”Q (âƒ—x), then
P is primitive recursive.
II.23. Prove that the characterizing formula for the nonlogical axioms of the
theory C â€“ that is, Î“ such that, for any formula A, Î“(âŒœAâŒ) means that
A is a nonlogical axiom â€“ is primitive recursive.

II.7. Exercises
317
II.24. Prove that if the one-variable formulas Q and Î“ are primitive recursive
and Î£1(LA) respectively, then the formula Q(x) âˆ¨Î“(x) is Î£1(LA).
II.25. Refer to II.6.42. Prove that it is also true that âŠ¢ G â†’Con, and thus
âŠ¢ G â†”Con
II.26. Prove LÂ¨obâ€™s theorem without the help of GÂ¨odelâ€™s second incompleteness
theorem.
II.27. Prove Tarskiâ€™s theorem (see I.9.31, p. 174) on the (semantic) undeï¬n-
ability of truth using the ï¬xpoint theorem (II.6.39) to ï¬nd a sentence that
says â€œI am falseâ€.
II.28. Refer to II.6.41. Let  over LA be an Ï‰-consistent extension of PA such
that Î“(x) is Î£1(LNC). Let the sentence G over LN be a ï¬xed point of
Â¬Î˜Î“ in C. Then Ì¸âŠ¢ Â¬G .
II.29. Let A be any sentence in the language of a  â€“ where Î“(x) is Î£1(LNC) â€“
that extends PA consistently. Establish that âŠ¢C Â¬Î˜(âŒœAâŒ) â†’Con.
II.30. With the usual assumptions on  and C (II.6.32), prove that
âŠ¢C Con â†’Con+Â¬Con


Bibliography
Barwise, Jon, editor (1978). Handbook of Mathematical Logic. North-Holland,
Amsterdam.
Bennett, J. (1962). On Spectra. PhD thesis, Princeton University.
Bourbaki, N. (1966a). Â´ElÂ´ements de mathÂ´ematique. Hermann, Paris.
(1966b). Â´ElÂ´ements de mathÂ´ematique; thÂ´eorie des ensembles. Hermann, Paris.
Chang, C.C., and H. Jerome, Keisler (1973). Model Theory. North-Holland, Amsterdam.
Church, Alonzo (1936). A note on the Entscheidungsproblem, J. Symbolic Logic, 1:40â€“
41, 101â€“102.
Cohen, P. J. (1963). The independence of the continuum hypothesis, part I, Proc. Nat.
Acad. Sci. U.S.A., 50: 1143â€“1148; part II 51:105â€“110 (1964).
Davis, M. (1965). The Undecidable. Raven Press, Hewlett, N. Y.
Dekker, James C. E. (1955). Productive sets, Trans. Amer. Math. Soc., 78:129â€“149.
Dijkstra, Edsger W., and Carel S. Scholten (1990). Predicate Calculus and Program
Semantics. Springer-Verlag, New York.
Enderton, Herbert B. (1972). A Mathematical Introduction to Logic. Academic Press,
New York.
GÂ¨odel, K. (1931). Â¨Uber formale unentscheidbare SÂ¨atze der Principia Mathematica und
verwandter Systeme I, Monatsh. Math. u. Phys. 38:173â€“198. (English transl. in Davis
(1965, pp. 5â€“38).
(1938). The consistency of the axiom of choice and of the generalized continuum
hypothesis, Proc. Nat. Acad. Sci. U.S.A., 24:556â€“557.
Grzegorczyk, A. (1953). Some classes of recursive functions, Rozprawy Mat., 4:1â€“45.
Gries, David, and Fred B. Schneider (1994). A Logical Approach to Discrete Math.
Springer-Verlag, New York.
(1995). Equational propositional logic, Inf. Process. Lett., 53:145â€“152.
Henkin, Leon (1952). A problem concerning provability, J. Symbolic Logic, 17:160.
Henle, James M., and Eugene M. Kleinberg (1980). Inï¬nitesimal Calculus. The MIT
Press, Cambridge, Mass.
Hermes, H. (1973). Introduction to Mathematical Logic. Springer-Verlag, New York.
Hilbert, D., and P. Bernays (1968). Grundlagen der Mathematik I, II. Springer-Verlag,
New York.
Hinman, P. G. (1978). Recursion-Theoretic Hierarchies. Springer-Verlag, New York.
KalmÂ´ar, L. (1957). An argument against the plausibility of Churchâ€™s thesis. In Construc-
tivity in Mathematics, Proc. of the Colloquium held at Amsterdam, pp. 72â€“80.
319

320
Bibliography
Keisler, H. Jerome (1976). Foundations of Inï¬nitesimal Calculus. PWS Publishers,
Boston.
Fundamentals of model theory. In Barwise (1978), Chapter A.2, pp. 47â€“104.
(1982). Elementary Calculus; an Inï¬nitesimal Approach. PWS Publishers,
Boston.
Kleene, S. C. (1943). Recursive predicates and quantiï¬ers, Trans. Amer. Math. Soc.,
53:41â€“73, 1943. In Davis (1965, pp. 255â€“287).
LeVeque, William J. (1956). Topics in Number Theory, volume I. Addison-Wesley,
Reading, Mass.
LÂ¨ob, Martin H. (1955). Solution to a problem of Leon Henkin, J. Symbolic Logic,
20:115â€“118.
Manin, Yu. I. (1977). A Course in Mathematical Logic. Springer-Verlag, New York.
Mendelson, Elliott (1987). Introduction to Mathematical Logic, 3rd edition. Wadsworth
& Brooks, Monterey, Calif.
Mostowski, A. (1947). On deï¬nable sets of positive integers, Fund. Math. 34:81â€“112.
Rasiowa, H., and R. Sikorski (1963). The Mathematics of Metamathematics. PaÂ´nstwowe
Wydawnictwo Naukowe, Warszawa.
Robinson, A. (1961) Non-standard analysis, Proc. Roy. Acad. Amsterdam Ser. A,
64:432â€“440.
(1966). Non-standard Analysis. North-Holland, Amsterdam. Revised ed., 1974.
Robinson, Raphael M. (1950). An essentially undecidable axiom system (Abstract). In
Proc. International Congress of Mathematicians, volume 1, pp. 729â€“730.
Rogers, H. (1967). Theory of Recursive Functions and Effective Computability.
McGraw-Hill, New York.
Rosser, J. Barkley (1936). Extensions of some theorems of GÂ¨odel and Church, J. Sym-
bolic Logic, 1:87â€“91.
SchÂ¨utte, K. (1977). Proof Theory. Springer-Verlag, New York.
Schwichtenberg, Helmut (1978). Proof theory: some applications of cut-elimination. In
Barwise (1978), Chapter D.2, pp. 867â€“895.
Shoenï¬eld, Joseph R. (1967). Mathematical Logic. Addison-Wesley, Reading, Mass.
SmoryÂ´nski, C. (1978). The incompleteness theorems. In Barwise (1978), Chapter D.1,
pp. 821â€“865.
(1985). Self-Reference and Modal Logic. Springer-Verlag, New York.
Smullyan, Raymond M. (1961). Theory of Formal Systems. Ann. Math. Stud. 47.
Princeton University Press, Princeton.
(1992). GÂ¨odelâ€™s Incompleteness Theorems. Oxford University Press, Oxford.
Tourlakis, G. (1984). Computability. Reston Publishing Company, Reston, Virginia.
(1986). Some reï¬‚ections on the foundations of ordinary recursion theory, and a
new proposal, Z. math. Logik, 32(6):503â€“515.
(1996). Recursion in partial type-1 objects with well-behaved oracles, Math.
Logic Quart. (MLQ), 42:449â€“460.
(2000a). A basic formal equational predicate logic â€“ part I, BSL, 29(1â€“2):43â€“56.
(2000b). A basic formal equational predicate logicâ€“part II, BSL, 29(3):75â€“88.
(2001a). Computability in type-2 objects with well-behaved type-1 oracles is
p-normal, Fund. Inform., 48(1):83â€“91.
(2001b). On the soundness and completeness of equational predicate logics,
J. Comput. and Logic, 11(4):623â€“653.
Veblen, Oswald, and John Wesley Young (1916). Projective Geometry, volume I. Ginn,
Boston.
Wilder, R. L. (1963). Introduction to the Foundations of Mathematics. Wiley, New York.
Whitehead, A. N., and B. Russell (1912). Principia Mathematica, volume 2. Cambridge
University Press, Cambridge.

List of Symbols
 + A, 48
z := x, 129
D(A), 88
Î 1, 265
(âˆƒa âˆˆM) . . . , 62
{E(x1, . . . , xn)}, 299
A(y1,. . . , yk), 18
A[y1,. . . , yk], 18
â–³, 166
âƒ, 166
, 166
Cl(I , R ), 20
âŸ¨a0, . . . , anâˆ’1âŸ©, 165
âŸ¨t[i] : i < wâŸ©, 239
âŸ¨. . . âŸ©, 19
x âˆ—y, 243, 245
, 150
âˆ, 151
n, 151
âˆ†0, 172
âˆ†+
0 , 256
âˆ†â€²
0, 256
Ï† : M â†’â‰ºK, 82
M â‰ºK, 82
Îµ-Î´, 109
f : A â†’B, 25
âŒœwâŒ, 155
iÏ†, 77
(a)aâˆˆM, 95
L(M), 54
a â‰ˆb, 104
Z, 15
(Î¹z)Q , 120
K, 145
K, 145
LM, 54
Î», 125
lcm, 157
lh(z), 137, 165
lh(x), 240
limxâ†’a â—¦f (x) = b, 108
, 34, 38
aM, f M, PM, 53
(Âµx), 128
N, 15
a Ì¸â‰ˆ0, 107
Num, 296
P, 128
n, 151
PR, 129
PRâˆ—, 130
(z)i, 240
.âˆ’, 131
âˆƒ!, 113
Q, 15
R, 15
R, 128
Râˆ—, 130
I = I â€² â†¾L, 54
M = Mâ€² â†¾L, 54
Â¯i, 54
limxâ†’a+ f (x) = b, 108
Seq(z), 137, 165
Seq, 241
n, 151
1, 256
st(h), 105
x âˆ—y, 137, 165
string
Î», 13
â‰¡, 13
M âŠ†K, 77
T (C ), 76
Th(C ), 76
âŸ¨. . . âŸ©, 19
Wa, 143
âƒ—an, 19
âƒ—a, 19
âƒ—xn, 33
â‰ƒ, 126
âŠ†, 20
321


Index
âˆ€-introduction, 43
absolutely provable, 37
AC, 86
Ackermann function, 198
âˆ€âˆƒ-theory, 93
alphabet, 6, 13
alternating chain, 96
ambiguity, 24
ambiguous, 24
antisymmetry, 210
argot, 39
arithmetic, 150
arithmetic hierarchy, 151
arithmetic relations, 150
arithmetical relations, 150
arithmetization, 134, 265
arity, 8
ascending chains, 93
assignment function, 88
associativities, 17
automorphism, 196
axiom, 6
Henkin, 65
logical, 6
nonlogical, 6, 36
special, 6
axiomatic theories, 38
axiomatized, 38
axioms, logical, 34
basic diagram, 88
Bennett, J., 162
Bernays, P., 41
beta function, 159, 235
Boolean, 7
Boolean operations, 133
bound variable, 18
bounded, 112
bounded multiplication, 134
bounded product, 197
bounded quantiï¬cation, 125
bounded search, 134
bounded summation, 134, 197
Bourbaki, 5, 14, 41
Brouwer, L. E. J., 4
call by value, 144
cancellation laws, 214
categorical, 97
characteristic function, 130, 219
characteristic term, 222
Chinese remainder theorem, 160
Church, Alonzo, 124, 188
Churchâ€™s thesis, 264
class, 75
closed form, 201
closed formula, 18
closed interval, 111
closed under, 19
closure, 20
coding, 136
collection, 258
commutative diagram, 78
compact, 107
compactness theorem, 71, 74
complete, 97, 194
simply complete, 66
complete arithmetic, 39, 170
complete index set, 149, 199
completeness theorem, 52, 71, 73, 194
completion, 195
composition, 127
computability, 123
computable, 128
computable function, 124
computation, 140
323

324
Index
computation model, 126
computer program, 264
concatenate, 10
concatenation, 13, 137, 243
formal, 243, 245
conjunct, 277
conjunction, 17
conjunctionally, 216
connectives, 7
conservative, 115, 121, 218
conservative extension, 46, 118
consistency theorem, 71, 73
constant, 8
Henkin, Leon, 65
witnessing, 65
construction formative, 41
constructive arithmetic, 164, 170
constructive arithmetic predicates, 160
continuous, 111
contrapositive, 158
converges, 125
correct, 177
countable, 62
course-of-values induction, 214
course-of-values recursion, 137,
251
Craig, W., 204
C-theory, 272
decision problem, 40, 188
decoding, 136, 240
Dedekind, R., 124, 129
deduced, 37
deduction theorem, 48
deï¬nability, 60
in a structure, 60
deï¬nable, 170, 171
deï¬nable in arithmetic, 171
deï¬nition by cases, 134, 221
deï¬nition by positive cases, 200
deï¬nition by recursion, 25
âˆ†0 formulas, 172
derivability conditions, 272, 296
derivation, 20, 254
derived rule, 38, 40
diagonalization, 125, 145
diagonalization lemma, 310
diagram, 64, 88
diagram expansion, 88
diagram language, 64, 88
Diophantine equation, 201
disjunct, 276
distributive laws, 193
diverges, 125
domain, 53
dummy renaming, 45
âˆƒ-introduction, 36
elementarily equivalent structures, 76
elementary chains, 93
elementary diagram, 88
elementary embedding, 82
elementary extension, 82
elementary substructure, 82
elimination of deï¬ned symbols, 116, 118
empty sequence, 243
Entscheidungsproblem, 40, 124
enumeration theorem, 153
Îµ-term, 123
equivalence theorem, 51, 183
existential axioms, 93
existential formula, 113
expansion, 88
explicit deï¬nition, 218
explicit transformations, 160
expressions, 6
extension, 77
extensionality, 114
extreme value theorem, 197
ï¬nitary, 4
ï¬nite hyperreal, 102
ï¬nitely satisï¬able, 195
ï¬rst incompleteness theorem, 155
ï¬xed point, 203, 310
ï¬xpoint, 203, 310
ï¬xpoint theorem, 310, 317
formal, 1
beta function, 235
Formal Arithmetic, 166, 175
formal language
ï¬rst order, 6
formalize, 3
formally deï¬nable, 180
formally functionally deï¬nable, 202
formula
decidable by a theory, 66
mixed-mode, 61
prime, 28
propositional, 28
satisï¬able, 30
tautology, 30
unsatisï¬able, 30
formula form, 34
formula schema, 34
free for, 32
free variable, 18
function
computable, 128
partial computable, 128
partial recursive, 128
primitive recursive, 129
recursive, 128

Index
325
GÂ¨odel number, 155
generalization, 43
Gentzen, Gerhard, 193
GÂ¨odel, Kurt, 124
GÂ¨odel-Malâ€²cev compactness theorem, 74
GÂ¨odel-numbering, 155
GÂ¨odel-Rosser ï¬rst incompleteness
theorem, 189
GÂ¨odelâ€™s ï¬rst incompleteness theorem, 177,
202, 317
GÂ¨odelâ€™s second incompleteness theorem, 312
graph, 98, 161, 163, 199, 202
greatest common divisor, 157
gcd, 157
Gries, David, 5
group theory, 92
groups, 93
Grzegorczyk, A., 132
g-term, 296
halting problem, 145
halting set, 145
Henkin, Leon, 62, 112, 314
Henkin theory, 66
Hermes, H., 41
Heyting, A., 4
hierarchy theorem, 154
Hilbert, D., 4, 41
history function, 137
hyperreal numbers, 99
hyperreals, 99
I.H., 22
i.p., 24
Identities, 128
iff, 11
immediate predecessors, 24
implied multiplication notation, 212
inclusion map, 77
incompletable, 177
incompletableness, 124
inconsistent, 265
increasing chains, 93
indeï¬nite article, 121
indexing theorem, 153
individual variables, 7
induced isomorphism, 78
induction axiom, 206
induction axiom schema, 206
induction hypothesis, 22
induction rule, 208
induction schema, 206
inductive deï¬nitions, 20
inductive theories, 93
inï¬nite hyperreal, 102
inï¬nite natural numbers, 196
inï¬nite prime, 197
inï¬nitely close, 104
inï¬nitesimal 97, 102
inï¬x, 14, 98
informal, 8
initial functions, 128
input, 19
intermediate value theorem, 197
interpretation, 53
iota notation, 120
irreï¬‚exive, 210
irreï¬‚exivity, 210
iteration theorem, 148
iteratively, 21
Îº-categorical, 97
Keisler, H. Jerome, 88
Kleene, S. C., 124, 150
Kleene normal form theorem, 142
Kleene predicate, 164
Kleene schemata, 127, 264
Kleene T -predicate, 142
Kleeneâ€™s recursion theorem, 149
Kronecker, L., 4
Î»-notation, 125
language
extension, 54
ï¬rst order, 16
restriction, 54
least common multiple, 157, 230
least principle, 216
left endpoint, 111
left ï¬eld, 125
Leibniz, G. W., 34
Leibniz rule, 51
limit ordinal, 73
Lindemann, F., 4
LÂ¨ob, Martin H., 310, 314
LÂ¨obâ€™s derivability condition, 310
LÂ¨obâ€™s theorem, 314
logical symbols, 7
logically implies, 52
LÂ¨owenheim-Skolem theorem, 71
Manin, Yu. I., 39
Markov, A. A., 127
material implication, 17
mathematical theory, see theory
maximum, 230
metalanguage, 3
metamathematics, 3
metatheory, 3
metavariable, 207
modus ponens, 36
monotonicity, 50

326
Index
monus, 131
Mostowski, A., 150
MP, 36
Âµ-deï¬nitions, 218
Âµ-term, 218
neighbourhood, 108
non-monotone, 265
non-standard hyperreals, 99
non-standard numbers, 99
nontotal, 125
Num function, 296
number-theoretic functions, 124
numeral, 41, 171
object variables, 7
occurrence, 13
occurs in, 13
Ï‰-consistent, 189
Ï‰-incompleteness, 190
Ï‰-inconsistent, 191
Ï‰-rule, 190
one point rule, 118
onto, 62
open, 92
open formulas, 91
open theory, 91
operations of substitution, 132
ordered sequence, 19
ordinary computability, 124
ordinary recursion theory, 124
output, 19
pairing function, 156
parsable, 29
parse, 21
partial computable, 128
partial function, 125
partial recursive, 128
Peano arithmetic, 206
Ï†-index, 138
Ï€-function, 198
pinching lemma, 106
polynomial, 201
positively strongly deï¬nable, 191, 202
Post, E. L., 124, 194
Postâ€™s theorem, 194
predicate, 8, 130
preï¬x, 13
prime power coding, 136
primitive recursion, 127
primitive recursive, 129, 170, 253
primitive recursive deï¬nitions, 248
primitive recursive functions, 124, 127
primitive recursive schema, 127
priorities, 17
problem, 144
productive function, 179
productive sets, 179
programming language, 264
projection, 128
projection function symbols, 254
projection functions, 156
projection theorem, 147
proof, 37
-, 37
proof by auxiliary constant, 52
proof by cases, 183
proof by contradiction, 50
proper preï¬x, 13
proper subtraction, 131, 222
propositional connectives, 7
propositional axioms, 194, 269
propositional calculus, 194
propositional logic, 194
propositional segment, 194
propositional valuations, 29
propositional variable, 28
provability predicate, 280, 311
provably equivalent, 42
proved from, 37
punctured neighbourhood, 108
Quine, W. V. O., 156
R-closed, 20
r.e. predicate, 147
Rasiowa, H., 41
real function, 108
recursion, 19
recursion theorem, 149
recursion theory, 123
recursive, 128, 130, 170
recursive deï¬nition, 20
recursive extension, 220, 262, 265,
273
recursively axiomatizable, 39
recursively axiomatized, 38, 271
recursively enumerable predicate, 147
recursively inseparable, 187
recursively unsolvable, 144
regular, 171
regular formula, 171
regular functions, 204
relation, see predicate, 8, 19
primitive recursive, 130
recursive, 130
relational notation, 125
relatively prime, 157, 226
remainder function, 224
restricted atomic formulas, 256
restriction, 78

Index
327
Rice, H. G., 149
right endpoint, 111
right ï¬nite limit, 108
right positive inï¬nite limit, 108
Robinson, Abraham, 64
Robinson, R. M., 175
Rosser, J. Barkley, 189
rudimentary functions, 174
rules of inference, 6
Russellâ€™s paradox, 145
S-m-n theorem, 148
satisï¬able, 55
ï¬nitely, 55
schema instance, 34
Schneider, Fred B., 5
scope, 15
selection theorem, 199
semi-recursive, 170
semi-recursive index, 143
semi-recursive relations, 143
semigroups, 93
sentence, 18
separation, 114
sequence, 229
sethood, 17
signum, 133
Sikorski, R., 41
simple completeness, 62
simultaneous substitution, 33
single-valued, 86
Skolem function, 123
sort, 17
soundness, 58
special Henkin axiom, 73
specialization, 43
stage, 23
standard hyperreals, 99
standard numbers, 99
standard part, 105
string
empty, 13
equality, 13
preï¬x, 13
proper, 13
strong projection theorem, 143
strongly deï¬nable, 186
strongly formally deï¬nable, 186
strongly term-deï¬nable, 187
strongly Âµ-recursive, 204
structure, 53
domain of, 53
embedding, 77
expansion of, 54, 119
extension of, 77
reduct of, 54
underlying set of, 53
universe of, 53
structure embedding, 77
structure isomorphism, 77
structures, elementarily equivalent, 76
substitutable for, 32
substitution, 31
simultaneous, 33
substitution function, 175, 268
substring, 9, 13
substructure, 77
successor, 73, 128
syntactic variable, 14, 207
T-Predicate, 142
table, 19
Tarski, Alfred, 170, 175
Tarskiâ€™s theorem, 174, 317
Ï„-term, 123
tautology theorem, 194
term, primitive recursive, 254
term-deï¬nable, 181, 252
term-deï¬ned, 181
theorem, 36
-, 36
theorem schema, 40
theory, 38
absolute, 39
âˆ€âˆƒ, 93
applied, 39
C, 272
conservative extension of, 46
consistent, 38
contradictory, 38
decides, 66
extension of, 46
ï¬rst order, 38
Henkin, 66
inconsistent, 38
inductive, 93
open, 91
pure, 39
recursively axiomatized, 176
refutes, 66
semantically complete, 62
simply complete, 66
sound, 56
universal, 91
total, 98, 125
transfer principle, 99
transï¬nite sequence, 72
transitive, 209
transitivity of âŠ¢, 37
truth (value) assignment, 29
truth functions, 30
Turing, Alan, 124

328
Index
Turing machines, 127, 264
two-sided ï¬nite limit, 108
type, 17
unambiguous, 24
unary, 17
unbounded search, 127, 216
uncomputability, 124
undeï¬nability of truth, 204
underlying set, 53
union, 22
uniquely readable, 29
universal, 265
universal closure, 44
universal quantiï¬er, 17
universal sentences, 94
universal theory, 91
unsolvable, 145
vacuously satisï¬ed, 31
valid, 55
logically, 55
universally, 55
valuation, 29, 195
variable
bound, 18
free, 18
frozen, 49
variant, 35, 45
Veblen, Oswald, 52
vector notation, 19
weak equality, 126
weak projection theorem, 147
weakly deï¬nable, 203
Weierstrass, Karl, 109
witness property, 66
witnessing constant, 73
Young, John Wesley, 52
zero, 128
zero function symbol, 254
Ziffern, 41

