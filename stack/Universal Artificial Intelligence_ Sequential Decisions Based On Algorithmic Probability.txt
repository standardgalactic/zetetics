
Texts in Theoretical Computer Science 
An EATCS Series 
Editors: W. Brauer G. Rozenberg A. Salomaa 
On behalf of the European Association 
for Theoretical Computer Science (EATCS) 
Advisory Board: G.Ausiello M. Broy C.S. Calude 
A. Condon D. Harel J. Hartmanis T. Henzinger 
J. Hromkovic N. Jones T. Leighton M. Nivat 
C. Papadimitriou D. Scott 


Marcus Hutter 
Universal 
Artificial Intelligence 
Sequential Decisions 
Based on Algorithmic Probability 
Springer 

Author 
Dr. Marcus Hutter 
Istituto Dalle MoUe 
di Studi sull'Intelligenza 
Artificiale (IDSIA) 
Galleria 2 
CH-6928 Manno-Lugano 
Switzerland 
marcus@idsia.ch 
www.idsia.ch/~marcus 
Series Editors 
Prof. Dr. Wilfried Brauer 
Institut fur Informatik der TUM 
Boltzmannstr. 3,85748 Garching, Germany 
Brauer@informatik.tu-muenchen.de 
Prof. Dr. Grzegorz Rozenberg 
Leiden Institute of Advanced Computer Science 
University of Leiden 
Niels Bohrweg 1,2333 CA Leiden, The Netherlands 
rozenber@liacs.nl 
Prof. Dr. Arto Salomaa 
Turku Centre for Computer Science 
Lemminkaisenkatu 14 A, 20520 Turku, Finland 
asalomaa@utu.fi 
Library of Congress Control Number: 2004112980 
ACM Computing Classification (1998): 1.3,1.2.6, EG, F.1.3, R4.1, E.4, G.3 
ISBN 3-540-22139-5 Springer Berlin Heidelberg New York 
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is 
concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, 
broadcasting, reproduction on microfilm or in any other way, and storage in data banks. Duplication 
of this publication or parts thereof is permitted only under the provisions of the German Copyright 
Law of September 9,1965, in its current version, and permission for use must always be obtained from 
Springer. Violations are liable for prosecution under the German Copyright Law. 
Springer is a part of Springer Science+Business Media 
springeronline.com 
© Springer-Verlag Berlin Heidelberg 2005 
Printed in Germany 
The use of general descriptive names, registered names, trademarks, etc. in this publication does not 
imply, even in the absence of a specific statement, that such names are exempt from the relevant protective 
laws and regulations and therefore free for general use. 
Cover design: KiinkelLopka, Heidelberg 
Typesetting: by the author 
Production: LE-TeX Jelonek, Schmidt & Vockler GbR, Leipzig 
Printed on acid-free paper 45/3142/YL - 5 4 3 2 1 0 

Preface 
Personal motivation. The dream of creating artificial devices that reach or 
outperform human inteUigence is an old one. It is also one of the dreams of my 
youth, which have never left me. What makes this challenge so interesting? 
A solution would have enormous implications on our society, and there are 
reasons to believe that the AI problem can be solved in my expected lifetime. 
So, it's worth sticking to it for a lifetime, even if it takes 30 years or so to 
reap the benefits. 
The AI problem. The science of artificial intelligence (AI) may be defined as 
the construction of intelligent systems and their analysis. A natural definition 
of a system is anything that has an input and an output stream. Intelligence 
is more complicated. It can have many faces like creativity, solving prob-
lems, pattern recognition, classification, learning, induction, deduction, build-
ing analogies, optimization, surviving in an environment, language processing, 
and knowledge. A formal definition incorporating every aspect of intelligence, 
however, seems difficult. Most, if not all known facets of intelligence can be 
formulated as goal driven or, more precisely, as maximizing some utility func-
tion. It is, therefore, sufficient to study goal-driven AI; e.g. the (biological) 
goal of animals and humans is to survive and spread. The goal of AI systems 
should be to be useful to humans. The problem is that, except for special 
cases, we know neither the utility function nor the environment in which the 
agent will operate in advance. The major goal of this book is to develop a 
theory that solves these problems. 
The nature of this book. The book is theoretical in nature. For most parts 
we assume availability of unlimited computational resources. The first impor-
tant observation is that this does not make the AI problem trivial. Playing 
chess optimally or solving NP-complete problems become trivial, but driving 
a car or surviving in nature do not. This is because it is a challenge itself to 
well-define the latter problems, not to mention presenting an algorithm. In 
other words: The AI problem has not yet been well defined. One may view 
the book as a suggestion and discussion of such a mathematical definition of 
AI. 
Extended abstract. The goal of this book is to develop a universal theory of 
sequential decision making akin to Solomonoff's celebrated universal theory of 
induction. SolomonoflF derived an optimal way of predicting future data, given 

vi 
Preface 
previous observations, provided the data is sampled from a computable prob-
ability distribution. Solomonoff's unique predictor is universal in the sense 
that it applies to every prediction task and is the output of a universal Turing 
machine with random input. We extend this approach to derive an optimal 
rational reinforcement learning agent, called AIXI, embedded in an unknown 
environment. The main idea is to replace the unknown environmental distri-
bution /i in the Bellman equations by a suitably generalized universal distri-
bution ^. The state space is the space of complete histories. AIXI is a universal 
theory without adjustable parameters, making no assumptions about the en-
vironment except that it is sampled from a computable distribution. From an 
algorithmic complexity perspective, the AIXI model generalizes optimal pas-
sive universal induction to the case of active agents. From a decision-theoretic 
perspective, AIXI is a suggestion of a new (implicit) "learning" algorithm, 
which may overcome all (except computational) problems of previous rein-
forcement learning algorithms. 
Chapter 
1. We start with a survey of the contents and main results in this 
book. 
Chapter 2. How and in which sense induction is possible at all has been sub-
ject to long philosophical controversies. Highlights are Epicurus' principle of 
multiple explanations, Occam's razor, and Bayes' rule for conditional proba-
bilities. Solomonoff elegantly unified all these aspects into one formal theory 
of inductive inference based on a universal probability distribution ^, which 
is closely related to Kolmogorov complexity K{x)^ the length of the shortest 
program computing x. We classify the (non)existence of universal priors for 
several generalized computability concepts. 
Chapter 3. We prove rapid convergence of ^ to the unknown true environmen-
tal distribution ji and tight loss bounds for arbitrary bounded loss functions 
and finite alphabet. We show Pareto optimality of ^ in the sense that there 
is no other predictor that performs better or equal in all environments and 
strictly better in at least one. Finally, we give an Occam's razor argument 
showing that predictors based on ^ are optimal. We apply the results to games 
of chance and compare them to predictions with expert advice. All together 
this shows that Solomonoff's induction scheme represents a universal (formal, 
but incomputable) solution to all passive prediction problems. 
Chapter 4- Sequential decision theory provides a framework for finding opti-
mal reward-maximizing strategies in reactive environments (e.g. chess playing 
as opposed to weather forecasting), assuming the environmental probability 
distribution /i is known. We present this theory in a very general form (called 
AI/x model) in which actions and observations may depend on arbitrary past 
events. We clarify the connection to the Bellman equations and discuss mi-
nor parameters including (the size of) the I/O spaces and the lifetime of the 
agent and their universal choice which we have in mind. Optimality of AI/x is 
obvious by construction. 
Chapter 
5. Reinforcement learning algorithms are usually used in the case 
of unknown /i. They can succeed if the state space is either small or has ef-

Preface 
vii 
fectively been made small by generalization techniques. The algorithms work 
only in restricted, (e.g. Markovian) domains, have problems with optimally 
trading off exploration versus exploitation, have nonoptimal learning rate, are 
prone to diverge, or are otherwise ad hoc. The formal solution proposed in 
this book is to generalize the universal prior ^ to include actions as conditions 
and replace /i by ^ in the Al/i model, resulting in the AIXI model, which we 
claim to be universally optimal. We investigate what we can expect from a 
universally optimal agent and clarify the meanings of universal^ optimal, etc. 
We show that a variant of AIXI is self-optimizing and Pareto optimal. 
Chapter 6. We show how a number of AI problem classes fit into the gen-
eral AIXI model. They include sequence prediction, strategic games, function 
minimization, and supervised learning. We first formulate each problem class 
in its natural way for known /x, and then construct a formulation within the 
Alfi model and show their equivalence. We then consider the consequences of 
replacing /x by ^. The main goal is to understand in which sense the problems 
are solved by AIXI. 
Chapter 7. The major drawback of AIXI is that it is incomputable, or more 
precisely, only asymptotically computable, which makes an implementation 
impossible. To overcome this problem, we construct a modified model AIXRZ, 
which is still superior to any other time t and length / bounded algorithm. 
The computation time of AIXK/ is of the order t • 2^ A way of overcoming 
the large multiplicative constant 2^ is presented at the expense of an (un-
fortunately even larger) additive constant. The constructed algorithm M^ is 
capable of solving all well-defined problems p as quickly as the fastest algo-
rithm computing a solution to p, save for a factor oi l-\-e and lower-order 
additive terms. The solution requires an implementation of first-order logic, 
the definition of a universal Turing machine within it and a proof theory sys-
tem. 
Chapter 8. Finally we discuss and remark on some otherwise unmentioned 
topics of general interest. We also critically review what has been achieved 
in this book, including assumptions, problems, limitations, performance, and 
generality of AIXI in comparison to other approaches to AI. We conclude the 
book with some less technical remarks on various philosophical issues. 
Prerequisites. I have tried to make the book as self-contained as possible. 
In particular, I provide all necessary background knowledge on algorithmic 
information theory in Chapter 2 and sequential decision theory in Chapter 4. 
Nevertheless, some prior knowledge in these areas could be of some help. The 
chapters have been designed to be readable independently of one another 
(after having read Chapter 1). This necessarily implies minor repetitions. Ad-
ditional information to the book (FAQs, errata, prizes, ...) is available at 
http://www.idsia.ch/~marcus/ai/uaibook.htm. 

viii 
Preface 
Problem classification. Problems are included at the end of each chap-
ter of different motivation and difficulty. We use Knuth's rating scheme for 
exercises [Knu73] in slightly adapted form (applicable if the material in the 
corresponding chapter has been understood). In-between values are possible. 
COO Very easy. Solvable from the top of your head. 
CIO Easy. Needs 15 minutes to think, possibly pencil and paper. 
C20 Average. May take 1-2 hours to answer completely. 
C30 Moderately difficult or lengthy. May take several hours to a day. 
C40 Quite difficult or lengthy. Often a significant research result. 
C50 Open research problem. An obtained solution should be published. 
The rating is possibly supplemented by the following qualifier(s): 
i Especially interesting or instructive problem. 
m Requires more or higher math than used or developed here. 
o Open problem; could be worth publishing; see web for prizes. 
s Solved problem with published solution. 
u Unpublished result by the author. 
The problems represent an important part of this book. They have been placed 
at the end of each chapter in order to keep the main text better focused. 
Acknowledgements. I would like to thank all those people who in one way 
or another have contributed to the success of this book. For interesting discus-
sions I am indebted to Jiirgen Schmidhuber, Ray Solomonoff, Paul Vitanyi, 
Peter van Emde Boas, Richard Sutton, Leslie Kaelbling, Leonid Levin, Pe-
ter Gacs, Wilfried Brauer, and many others. Shane Legg, Jan Poland, Viktor 
Zhumatiy, Alexey Chernov, Douglas Eck, Ivo Kwee, Philippa Hutter, Paul 
Vitanyi, and Jiirgen Schmidhuber gave valuable feedback on drafts of the 
book. Thanks also collectively to all other IDSIAnies and to the Springer team 
for the pleasant working atmosphere and their support. This book would not 
have been possible without the financial support of the SNF (grant no. 2000-
61847.00). Thanks also to my father, who taught me to think sharply and to 
my mother who taught me to do what one enjoys. Finally, I would like to 
thank my wife and children who patiently supported my decision to write this 
book. 
Lugano, Switzerland, August 2004 
Marcus Hutter 

Contents 
Meta Contents 
iii 
Preface 
v 
Contents 
ix 
Tables, Figures, Theorems, 
xv 
Notation 
xvii 
A Short Tour Through the Book 
1 
1.1 
Introduction 
2 
1.2 
Simplicity & Uncertainty 
3 
1.2.1 
Introduction 
3 
1.2.2 
Algorithmic Information Theory 
4 
1.2.3 
Uncertainty & Probabilities 
5 
1.2.4 
Algorithmic Probability & Universal Induction 
6 
1.2.5 
Generalized Universal (Semi)Measures 
7 
1.3 Universal Sequence Prediction 
7 
1.3.1 
Setup & Convergence 
8 
1.3.2 
Loss Bounds 
8 
1.3.3 
Optimality Properties 
9 
1.3.4 
Miscellaneous 
10 
1.4 
Rational Agents in Known Probabilistic Environments 
11 
1.4.1 
The Agent Model 
11 
1.4.2 
Value Functions & Optimal Policies 
11 
1.4.3 
Sequential Decision Theory & Reinforcement Learning . 12 
1.5 
The Universal Algorithmic Agent AIXI 
13 
1.5.1 
The Universal AIXI Model 
13 
1.5.2 
On the Optimality of AIXI 
14 
1.5.3 
Value-Related Optimality Results 
15 
1.5.4 
Markov Decision Processes 
17 
1.5.5 
The Choice of the Horizon 
18 
1.6 
Important Environmental Classes 
18 
1.6.1 
Introduction 
18 
1.6.2 
Sequence Prediction (SP) 
19 
1.6.3 
Strategic Games (SG) 
19 
1.6.4 
Function Minimization (FM) 
19 
1.6.5 
Supervised Learning from Examples (EX) 
19 
1.6.6 
Other Aspects of Intelligence 
20 
1.7 
Computational Aspects 
20 

Contents 
1.7.1 
The Fastest & Shortest Algorithm for All Problems 
20 
1.7.2 
Time-Bounded AIXI Model 
22 
1.8 
Discussion 
24 
1.9 
History & References 
26 
Simplicity &; Uncertainty 
29 
2.1 Introduction 
30 
2.1.1 Examples of Induction Problems 
30 
2.1.2 
Ockham, Epicurus, Hume, Bayes, SolomonofT 
31 
2.1.3 Problem Setup 
32 
2.2 Algorithmic Information Theory 
33 
2.2.1 Definitions and Notation 
33 
2.2.2 
Turing Machines 
34 
2.2.3 Kolmogorov Complexity 
36 
2.2.4 
Computability Concepts 
38 
2.3 Uncertainty & Probabilities 
40 
2.3.1 Frequency Interpretation: Counting 
40 
2.3.2 
Objective Interpretation: Uncertain Events 
41 
2.3.3 
Subjective Interpretation: Degrees of Belief 
43 
2.3.4 
Determining Priors 
45 
2.4 Algorithmic Probability & Universal Induction 
45 
2.4.1 
The Universal Prior M 
45 
2.4.2 
Universal Sequence Prediction 
47 
2.4.3 Universal (Semi)Measures 
48 
2.4.4 
Martin-Lof Randomness 
54 
2.5 History & References 
55 
2.6 Problems 
60 
Universal Sequence Prediction 
65 
3.1 Introduction 
67 
3.2 
Setup and Convergence 
68 
3.2.1 
Random Sequences 
68 
3.2.2 
Universal Prior Probability Distribution 
69 
3.2.3 
Universal Posterior Probability Distribution 
70 
3.2.4 
Convergence of Random Sequences 
71 
3.2.5 
Distance Measures between Probability Distributions . . 72 
3.2.6 
Convergence of ^ to /i 
74 
3.2.7 
Convergence in Martin-Lof Sense 
76 
3.2.8 
The Case where ji^M 
80 
3.2.9 
Probability Classes M 
81 
3.3 Error Bounds 
82 
3.3.1 
Bayes Optimal Predictors 
82 
3.3.2 
Total Expected Numbers of Errors 
82 
3.3.3 
Proof of Theorem 3.36 
84 
3.4 Loss Bounds 
86 

Contents 
xi 
3.4.1 
Unit Loss Function 
86 
3.4.2 
Loss Bound of Merhav & Feder 
88 
3.4.3 
Example Loss Functions 
89 
3.4.4 
Proof of Theorem 3.48 
89 
3.4.5 
Convergence of Instantaneous Losses 
91 
3.4.6 
General Loss 
92 
3.5 Application to Games of Chance 
93 
3.5.1 Introduction 
93 
3.5.2 
Games of Chance 
94 
3.5.3 
Example 
95 
3.5.4 
Information-Theoretic Interpretation 
95 
3.6 
Optimality Properties 
96 
3.6.1 
Lower Error Bound 
96 
3.6.2 
Pareto Optimality of ^ 
99 
3.6.3 
Balanced Pareto Optimality of ^ 
101 
3.6.4 
On the Optimal Choice of Weights 
102 
3.6.5 
Occam's razor versus No Free Lunches 
103 
3.7 Miscellaneous 
103 
3.7.1 
Multistep Predictions 
104 
3.7.2 
Continuous Probability Classes M 
106 
3.7.3 Further Applications 
108 
3.7.4 
Prediction with Expert Advice 
108 
3.7.5 
Outlook 
110 
3.8 Summary 
I l l 
3.9 Technical Proofs 
112 
3.9.1 
How to Deal with /i = 0 
112 
3.9.2 
Entropy Inequalities (Lemma 3.11) 
113 
3.9.3 
Error Inequality (Theorem 3.36) 
115 
3.9.4 
Binary Loss Inequality for z<^ 
(3.57) 
116 
3.9.5 
Binary Loss Inequality for z>\ 
(3.58) 
117 
3.9.6 
General Loss Inequality (3.53) 
117 
3.10 History & References 
119 
3.11 Problems 
119 
Agents in Known Probabilistic Environments 
125 
4.1 The AI/x Model in Functional Form 
126 
4.1.1 The Cybernetic Agent Model 
126 
4.1.2 
Strings 
128 
4.1.3 AI Model for Known Deterministic Environment 
128 
4.1.4 
AI Model for Known Prior Probability 
130 
4.2 The Alji Model in Recursive and Iterative Form 
132 
4.2.1 Probability Distributions 
132 
4.2.2 
Exphcit Form of the AI// Model 
133 
4.2.3 Equivalence of Functional and Explicit AI Model 
134 
4.3 Special Aspects of the AI/x Model 
135 

Contents 
4.3.1 Factorizable Environments 
135 
4.3.2 
Constants and Limits 
138 
4.3.3 Sequential Decision Theory 
139 
4.4 Problems 
140 
The Universal Algorithmic Agent AIXI 
141 
5.1 The Universal AIXI Model 
142 
5.1.1 
Definition of the AIXI Model 
142 
5.1.2 
Universality of M^^ and ^^^ 
144 
5.1.3 
Convergence of ^^^ to /x^^ 
145 
5.1.4 
Intelligence Order Relation 
146 
5.2 
On the Optimality of AIXI 
147 
5.3 Value Bounds and Separability Concepts 
149 
5.3.1 
Introduction 
149 
5.3.2 
(Pseudo) Passive fi and the HeavenHell Example 
149 
5.3.3 
The OnlyOne Example 
150 
5.3.4 
Asymptotic Learnability 
151 
5.3.5 
Uniform jj. 
152 
5.3.6 
Other Concepts 
152 
5.3.7 
Summary 
153 
5.4 Value-Related Optimality Results 
153 
5.4.1 
The Alp Models: Preliminaries 
153 
5.4.2 
Pareto Optimality of AI^ 
154 
5.4.3 
Self-Optimizing Policy p^ w.r.t. Average Value 
156 
5.5 
Discounted Future Value Function 
159 
5.6 
Markov Decision Processes (MDP) 
165 
5.7 The Choice of the Horizon 
169 
5.8 
Outlook 
172 
5.9 
Conclusions 
173 
5.10 Functions -^ Chronological Semimeasures 
173 
5.11 Proof of the Entropy Inequality 
175 
5.12 History & References 
177 
5.13 Problems 
178 
Important Environmental Classes 
185 
6.1 Repetition of the Alfi/^ Models 
186 
6.2 
Sequence Prediction (SP) 
187 
6.2.1 
Using the AI^LX Model for Sequence Prediction 
188 
6.2.2 
Using the AI^ Model for Sequence Prediction 
190 
6.3 Strategic Games (SG) 
192 
6.3.1 
Introduction 
192 
6.3.2 
Strictly Competitive Strategic Games 
193 
6.3.3 
Using the AI^ Model for Game Playing 
193 
6.3.4 
Games of Variable Length 
195 
6.3.5 
Using the AI^ Model for Game Playing 
195 

Contents 
xiii 
6.4 Function Minimization (FM) 
197 
6.4.1 
Applications/Examples 
197 
6.4.2 
The Greedy Model FMGfi 
198 
6.4.3 
The General FM/i/^ Model 
199 
6.4.4 
Is the General Model Inventive? 
201 
6.4.5 
Using the AI Models for Function Minimization 
202 
6.4.6 
Remark on TSP 
203 
6.5 Supervised Learning from Examples (EX) 
204 
6.5.1 
Applications/Examples 
204 
6.5.2 
Supervised Learning with the AI/x/^ Model 
204 
6.6 
Other Aspects of Intelligence 
206 
6.7 Problems 
207 
Computational Aspects 
209 
7.1 The Fastest & Shortest Algorithm for All Problems 
210 
7.1.1 Introduction & Main Result 
210 
7.1.2 
Levin Search 
212 
7.1.3 
Fast Matrix MultipHcation 
213 
7.1.4 
Applicability of the Fast Algorithm M^ 
214 
7.1.5 
The Fast Algorithm M^* 
215 
7.1.6 
Time Analysis 
216 
7.1.7 
Assumptions on the Machine Model 
218 
7.1.8 
Algorithmic Complexity and the Shortest Algorithm ... 218 
7.1.9 
Generalizations 
220 
7.1.10 Summary & Outlook 
220 
7.2 Time-Bounded AIXI Model 
221 
7.2.1 Introduction 
221 
7.2.2 
Time-Limited Probability Distributions 
222 
7.2.3 
The Idea of the Best Vote Algorithm 
224 
7.2.4 
Extended Chronological Programs 
224 
7.2.5 
Vahd Approximations 
225 
7.2.6 
Effective Intelhgence Order Relation 
226 
7.2.7 
The Universal Time-Bounded AlXltl Agent 
226 
7.2.8 
Limitations and Open Questions 
227 
7.2.9 
Remarks 
228 
Discussion 
231 
8.1 What has been Achieved 
232 
8.1.1 
Results 
232 
8.1.2 
Comparison to Other Approaches 
234 
8.2 
General Remarks 
235 
8.2.1 
Miscellaneous 
235 
8.2.2 
Prior Knowledge 
236 
8.2.3 
Universal Prior Knowledge 
237 
8.2.4 
How AIXI(tO Deals with Encrypted Information 
237 

xiv 
Contents 
8.2.5 
Mortal Embodied Agents 
238 
8.3 
Personal Remarks 
239 
8.3.1 
On the Foundations of Machine Learning 
239 
8.3.2 
In a World Without Occam 
240 
8.4 
Outlook & Open Questions 
241 
8.5 Assumptions, Problems, Limitations 
242 
8.5.1 
Assumptions 
243 
8.5.2 
Problems 
244 
8.5.3 
Limitations 
244 
8.6 
Philosophical Issues 
245 
8.6.1 
Turing Test 
245 
8.6.2 
On the Existence of Objective Probabilities 
245 
8.6.3 
Free Will versus Determinism 
246 
8.6.4 
The Big Questions 
248 
8.7 
Conclusions 
248 
Bibliography 
251 
Index 
265 

Tables, Figures, Theorems, ... 
Table 2.2 ((Prefix) coding of natural numbers and strings) 
34 
Thesis 2.3 (Turing) 
34 
Thesis 2.4 (Church) 
34 
Assumption 2.5 (Short compiler) 
34 
Definition 2.6 (Prefix/Monotone Turing machine) 
35 
Theorem 2.7 (Universal prefix/monotone Turing machine) 
36 
Definition 2.9 (Kolmogorov complexity) 
37 
Theorem 2.10 (Properties of Kolmogorov complexity) 
37 
Figure 2.11 (Kolmogorov Complexity) 
38 
Definition 2.12 (Computable functions) 
38 
Theorem 2.13 ((Non)computability of Kolmogorov complexity) 
39 
Axioms 2.14 (Kolmogorov's axioms of probability theory) 
41 
Definition 2.15 (Conditional probability) 
42 
Theorem 2.16 (Bayes' rule 1) 
42 
Axioms 2.17 (Cox's axioms for beliefs) 
43 
Theorem 2.18 (Cox's theorem) 
43 
Theorem 2.19 (Bayes' rule 2) 
44 
Definition 2.22 ((Semi)measures) 
46 
Theorem 2.23 (Universality of M) 
46 
Theorem 2.25 (Posterior convergence of M to /i) 
48 
Theorem 2.28 (Universal (semi)measures) 
49 
Table 2.29 (Existence of universal (semi)measures) 
50 
Theorem 2.31 (Martin-Lof random sequences) 
54 
Definition 2.33 (/i/^-random sequences) 
54 
Definition 3.8 (Convergence of random sequences) 
71 
Lemma 3.9 (Relations between random convergence criteria) 
71 
Lemma 3.11 (Entropy inequalities) 
72 
Theorem 3.19 (Convergence of ^ to /x) 
74 
Theorem 3.22 (/x/^-convergence of ^ to /x) 
76 
Theorem 3.36 (Error bound) 
83 
Theorem 3.48 (Unit loss bound) 
87 
Corollary 3.49 (Unit loss bound) 
88 
Theorem 3.59 (Instantaneous loss bound) 
91 
Theorem 3.60 (General loss bound) 
92 
Theorem 3.63 (Time to win) 
94 
Theorem 3.64 (Lower error bound) 
97 

xvi 
Tables, Figures, Theorems, ... 
Definition 3.65 (Pareto optimality) 
99 
Theorem 3.66 (Pareto optimal performance measures) 
99 
Theorem 3.69 (Balanced Pareto optimality w.r.t. L) 
101 
Theorem 3.70 (Optimality of universal weights) 
102 
Theorem 3.74 (Continuous entropy bound) 
106 
Definition 4.1 (The agent model) 
126 
Table 4.2 (Notation and emphasis in AI versus control theory) 
127 
Definition 4.4 (The Al/i model) 
130 
Definition 4.5 (The ///true/generating value function) 
130 
Figure 4.13 (Expectimax tree/algorithm iov 0=^y = B) 
133 
Theorem 4.20 (Equivalence of functional and explicit AI model) 
134 
Theorem 4.25 (Factorizable environments //) 
137 
Assumption 4.28 (Finiteness) 
138 
Claim 5.12 (We expect AIXI to be universally optimal) 
146 
Definition 5.14 (Intelligence order relation) 
147 
Definition 5.18 (p-Value function) 
153 
Definition 5.19 (Functional Kip model) 
153 
Theorem 5.20 (Iterative Alp model) 
154 
Theorem 5.21 (Linearity and convexity of F^ in p) 
154 
Definition 5.22 (Pareto optimal policies) 
155 
Theorem 5.23 (Pareto optimality of p^) 
155 
Theorem 5.24 (Balanced Pareto optimality) 
155 
Lemma 5.27 (Value difference relation) 
156 
Lemma 5.28 (Convergence of averages) 
157 
Theorem 5.29 (Self-optimizing policy p^ w.r.t. average value) 
157 
Definition 5.30 (Discounted Alp model and value) 
159 
Theorem 5.31 (Linearity and convexity of Fp in p) 
160 
Theorem 5.32 (Pareto optimality w.r.t. discounted value) 
160 
Lemma 5.33 (Value diff'erence relation) 
160 
Theorem 5.34 (Self-optimizing policy p^ w.r.t. discounted value) 
161 
Theorem 5.35 (Continuity of discounted value) 
162 
Theorem 5.36 (Convergence of universal to true value) 
163 
Definition 5.37 (Ergodic Markov decision processes) 
165 
Theorem 5.38 (Self-optimizing policies for ergodic MDPs) 
165 
Corollary 5.40 (AI^ is self-optimizing for ergodic MDPs) 
168 
Table 5.41 (Effective horizons) 
170 
Theorem 7.1 (The fastest algorithm) 
211 
Theorem 7.2 (The fastest & shortest algorithm) 
219 
Definition 7.8 (Effective intelligence order relation) 
226 
Theorem 7.9 (Optimality of AlXIt/) 
227 
Table 8.1 (Properties of learning algorithms) 
234 

Notation 
The following is a list of commonly used notation. The first entry is the symbol 
itself, followed by its meaning or name (if any) and the page number where 
the definition appears. Some standard symbols like M are not defined in the 
text. There appears a * in place of the page number for these symbols. 
Symbol 
Explanation 
Page 
[C35s] 
classification of problems 
viii 
[Hut04b] paper, book or other reference 
(5.3) 
label/reference for a formula/theorem/definition/... 
00 
infinity 
{a,...,^} 
set containing elements a,b,...,y,z. {} is the empty set 
[a,6) 
interval on the real line, closed at a and open at b 
n,U,\,G 
set intersection, union, difference, membership 
A,V,-i 
Boolean conjunction (and), disjunction (or), negation (not) 
C,c 
subset, proper subset 
=4> 
implies 
<^ 
equivalence, if and only if, iff 
• 
q.e.d. (Latin), which was to be demonstrated 
V,3 
for all, there exists 
* 
~?^?^ 
approximately equal, less equal, greater equal 
33 
< C ^ 
much smaller/greater than 
* 
= 
equivalent, identical, equal by definition 
* 
= 
isomorphic 
* 
:= 
define as 
* 
= 
corresponds to, informal equality 
* 
~ 
asymptotically proportional to 
33 
ex: 
proportional to 
=,7^ 
equal to, not equal to 
+, — ,-,/ 
standard arithmetic operations: sum, difference, product, ratio 
^ 
square root 
^,^,<,> standard inequalities 
|5|,|a| 
size/cardinality of set <5, absolute value of a 

* 
xviii 
Notation 
—> 
mapping, approaches, Boolean implication 
* 
-^ 
converge to each other 
33 
lim 
limiting value of argument for n tending to infinity 
* 
n—>-oo 
-^ 
replace with 
* 
\x] 
ceiling of x: smallest integer larger or equal than x 
* 
[x\ 
floor 
of x: largest integer smaller or equal than x 
* 
6ab 
Kronecker symbol, 5a6 = l if a = 6 and 0 otherwise 
* 
X]fc=i 
summation from k = l to n 
* 
J2x 
summation over X for which/i(x)72^0 
69 
n^^i 
product from k = l to n 
* 
f^f^dx 
Lebesgue integral, integral from a to 6 over x 
* 
i,k,n,t 
natural numbers 
33 
x,y,z 
finite 
strings 
33 
min/max min-/maximal element of set: minxsxfi^) 
— min{/(x): x G X} 
* 
argmin 
argmina;/(x) is the x minimizing f{x) (ties broken arbitrarily) 
l.h.s. 
left-hand side 
* 
r.h.s. 
right-hand side 
* 
w.r.t. 
with respect to 
* 
e.g. 
exempli gratia (Latin), for example 
i.e. 
id est (Latin), that is 
etc. 
et cetera (Latin), and so forth 
cf. 
confer (Latin, imperative of conferre), compare with 
et al. 
et alii (Latin), and others 
q.e.d. 
quod erat demonstrandum (Latin), which was to be shown 
i.i.d. 
independent identically distributed (random variables) 
* 
iff" 
if and only if 
* 
w.p.l/i.p. with probability 1 / in probability 
71 
i.m./i.m.s. in the mean / in mean sum 
71 
log 
logarithm to some basis 
* 
log^ 
logarithm to basis h 
* 
In 
natural logarithm to basis e = 2.71828... 
* 
e 
base of natural logarithm e —2.71828... 
* 
]R 
set of real numbers 
* 
]R^ 
set of nonnegative real numbers 
* 
IN 
set of natural numbers {1,2,3,...} 
33 
Wo 
set of natural numbers including zero {0,1,2,3,...} 
33 
Z 
set of integers {...,-2,-1,0,1,2,3,...} 
* 
(Q 
set of rational numbers {^} 
* 

Notation 
45, 
45, 
45, 
68, 
68, 
xix 
* 
128 
128 
128 
128 
* 
33 
33 
128 
128 
128 
iB = {0,l} binary alphabet 
yt^y 
action (output of agent) in cycle t, followed by ... 
Xt G A' 
perception (feedback/input to agent) in cycle t 
Ot^O 
informative input/observation in cycle t 
rt^lZcJRreward 
in cycle t 
e 
some small positive real number 
6 
empty string 
* 
wildcard for some string (prefix, finite, or infinite) 
Xi-j^ 
=Xi...Xn= string of length n 
x<t 
=Xi,..Xt-i= 
string of length t—1 
yxkin 
action-perception sequence ykXh-'-Vn^n 
yx<^k 
actually realized action-perception sequence yiXi...yk-iXk-i 
130 
uj 
infinite sequence, elementary event 
33 
Q 
sample space 
42, 68 
^xi:ri 
={<^:^i:n = ^i:n}= Cylinder set 
46, 68 
(.{x) 
length of string x 
33 
(o) 
coding of object o 
33 
{x^y) 
uniquely decodable pairing of x and y 
33 
x' 
prefix coding of x 
33 
0(),o() 
big and small oh-notation 
33 
a<h 
less within an additive const., i.e. a <6-fO(l). Similarly ± 
33 
a<h 
less within a multiplicative const., i.e. a — 0{h). Similarly = 
33 
K{x) 
prefix Kolmogorov complexity of string x 
37 
Km{xi.n) 
monotone (Kolmogorov) complexity of string xi-n 
47, 190 
K{oi\o2) 
Kolmogorov complexity of object oi, given object 02 
37 
M = ^u 
Solomonoff-Levin's universal semimeasure 
46, 48 
Al = {i^} 
(usually countable) set of (semi)measures 
48, 81 
EC 
G{AI, SP, F M , EX, SG, ...} is an environmental class 
* 
AI 
artificial or algorithmic intelligence, 
2 
most general computational environmental class 
130, 154 
SP 
sequence prediction 
187 
CF 
classification 
108 
SG 
strategic two-player informed zero-sum games 
192 
FM 
function minimization 
197 
EX 
supervised learning (by examples) 
204 
pd 
probability density function / distribution / measure 
* 
p(^i:n) 
probability of string/sequence starting with xi:n 
46, 68 
fi^Ai 
true generating environmental pd 
68 

XX 
Notation 
E 
expectation value, usually w.r.t. the true distribution /j, 
68 
P 
probability, usually w.r.t. the true distribution /i 
68 
/x(x 1X2X3^4) ji probability that the 2^^ and 4*^ symbols of a string are 
X2 and X4, given the P* and 3'^^ symbols are xi and xs 
132 
ueM 
any pd in M 
70 
p 
any pd not necessarily in M usually specifying a policy 
68 
^ 
=Z]i/GAi^^^~ mixture (belief) pd 
48, 70 
Wjy 
prior degree of belief in v -or- weight of i/ 
48, 70 
p^^ 
pd of environmental argument type EC 
185 
^EC 
mixture distribution of type EC for class EC 
185 
ixtvt 
incurred loss when predicting yt and Xt is next symbol 
86 
/^ 
z/-expected instantaneous loss in step t of predictor A 
99, 87 
L^^ 
z/-expected cumulative loss of steps l...n of predictor A 
100 
Op 
predictor with minimal number of p-expected errors 
82 
Ap 
predictor that minimizes the p-expected loss 
87 
e^ 
^-probability that 0-predictor errs in step t 
83 
^ni^ 
i^-expected number of errors in steps l...n of predictor O 
83 
L^ = L^ 
abbreviation for true //-expected loss 
86 
V^^(?/r</c) value of policy p in environment u given history ifio<^k 
153 
y^ 
prediction/decision/action of predictor A in step t 
87 
y^ 
action of policy p in cycle k 
* 
7fc 
discounting sequence 
159 
Fk 
value function normalization (^^j^^Jk) 
159 
m,/i 
agent's lifespan, horizon 
129, 169 
p 
agent's policy 
126 
q 
deterministic environment 
126 
p^ 
policy that maximizes value V^ 
130 
V*=Vi^^ true or generating value 
130 
V^ = Vi^ 
universal value 
146 
Dn = D^^ relative entropy between p and ^ for the first n cycles 
73 

/ have no particular talent. I am merely 
inquisitive. 
— Albert Einstein 
Albert Einstein 
(1879-1955) 
1 A Short Tour Through the Book 

1 A Short Tour Through the Book 
Th is Chapter represents a short tour through the book. It is not meant as a 
gentle introduction for novices, but as a condensed presentation of the most 
important concepts and results of the book. The price for this brevity is that 
in this chapter we mostly forgo mathematical rigor, subtleties, proofs, discus-
sions, references and comparisons to other work. More seriously, some sec-
tions demand high background knowledge. Readers unfamiliar with algorith-
mic information theory should first read Chapter 2 or consult the textbooks 
[LV97, Cal02]. Readers unfamiliar with sequential decision theory should first 
read Chapter 4 or consult the textbooks [BT96, SB98]. Before becoming dis-
couraged by the complexity of some of the sections, it is better to skip them 
completely. 
1.1 Introduction 
Artificial Intelligence. The science of artificial intelligence (AI) might be 
defined as the construction of intelligent systems and their analysis. A natural 
definition of a system is anything that has an input and an output stream. In-
telligence is more complicated. It can have many faces like creativity, solving 
problems, pattern recognition, classification, learning, induction, deduction, 
building analogies, optimization, surviving in an environment, language pro-
cessing, knowledge and many more. A formal definition incorporating every 
aspect of intelligence, however, seems difficult. Further, intelligence is graded: 
There is a smooth transition between systems, which everyone would agree 
to be not intelligent, and truly intelligent systems. One simply has to look in 
nature, starting with, for instance, inanimate crystals, then amino acids, then 
some RNA fragments, then viruses, bacteria, plants, animals, apes, followed 
by the truly intelligent homo sapiens, and possibly continued by AI systems or 
ETs. So, the best we can expect to find is a partial or total order relation on 
the set of systems, which orders them w.r.t. their degree of intelligence (hke 

1.2 Simplicity & Uncertainty 
3 
intelligence tests do for human systems, but for a limited class of problems). 
Having this order we are, of course, interested in large elements, i.e. highly 
intelHgent systems. If a largest element exists, it would correspond to the most 
intelligent system which could exist. 
Most, if not all, known facets of intelligence can be formulated as goal 
driven or, more precisely, as maximizing some utility function. It is therefore 
sufficient to study goal-driven AI. For example, the (biological) goal of animals 
and humans is to survive and spread. The goal of AI systems should be to 
be useful to humans. The problem is that, except for special cases, we know 
neither the utility function nor the environment in which the agent will operate 
in advance. 
Main idea. This book presents a theory that formally-^ solves the problem 
of unknown goal and environment. It might be viewed as a unification of the 
ideas of universal induction, probabilistic planning and reinforcement learning, 
or as a unification of sequential decision theory with algorithmic information 
theory. We apply this model to some of the facets of intelligence, including 
induction, game playing, optimization, reinforcement and supervised learning, 
and show how it solves these problem classes. This, together with general 
convergence theorems, supports the belief that the constructed universal AI 
system is the best one in a sense to be clarified in the following, i.e. that it is 
the most intelligent environment-independent system possible. The intention 
of this book is to introduce the universal AI model and give an extensive 
analysis. 
1.2 Simplicity & Uncertainty 
This section introduces Occam's razor principle, Kolmogorov complexity, and 
objective/subjective probabilities. We finally arrive at the problem of universal 
prediction, and its solution by Solomonoff. 
1.2.1 
Introduction 
An important and nontrivial aspect of intelligence is inductive inference. Sim-
ply speaking, induction is the process of predicting the future from the past, 
or, more precisely, it is the process of finding rules in (past) data and us-
ing these rules to guess future data. Weather or stock-market forecasting or 
continuing number series in an IQ test are nontrivial examples. Making good 
predictions plays a central role in natural and artificial intelligence in general, 
and in machine learning in particular. All induction problems can be phrased 
^ With a formal solution we mean a rigorous mathematically definition, uniquely 
specifying the solution. In the following, a solution is always meant in this formal 
sense. 

4 
1 A Short Tour Through the Book 
as sequence prediction tasks. This is, for instance, obvious for time-series pre-
diction, but also includes classification tasks. Having observed data Xt at times 
t < n , the task is to predict the n*^ symbol Xn from sequence xi...Xn-i- 
This 
prequential approach [Daw84] skips over the intermediate step of learning a 
model based on observed data xi...Xn-i 
and then using this model to predict 
Xn- The prequential approach avoids problems of model consistency, how to 
separate noise from useful data, and many other issues. The goal is to make 
"good" predictions, where the prediction quality is usually measured by a loss 
function, which shall be minimized. The key concept to well-defining and solv-
ing induction problems is Occam^s razor (simphcity) principle, which says that 
^^Entities should not be multiplied beyond necessity.'' This may be interpreted 
as keeping the simplest theory consistent with the observations Xi...Xn~i and 
using this theory to predict Xn- Before we can present Solomonoff's formal so-
lution, we have to quantify Occam's razor in terms of Kolmogorov complexity, 
and introduce the notions of subjective and objective probabilities. 
1.2.2 
Algorithmic Information Theory 
Intuitively, a string is simple if it can be described in a few words, like "the 
string of one million ones", and is complex if there is no such short description, 
like for a random string whose shortest description is specifying it bit by bit. 
We can restrict the discussion to binary strings, since for other (non-stringy 
mathematical) objects we may assume some default coding as binary strings. 
Furthermore, we are only interested in effective descriptions, and hence restrict 
decoders to be Turing machines. Let us choose some universal (so-called prefix) 
Turing machine U with unidirectional binary input and output tapes and a 
bidirectional work tape. We can then define the prefix Kolmogorov complexity 
[Cha75, Gac74, Kol65, Lev74] of a binary string x as the length £ of the 
shortest program p for which U outputs the binary string x 
K{x) := mm{i{p) : U{p) = x}. 
p 
Simple strings like 000...0 can be generated by short programs, and, hence 
have low Kolmogorov complexity, but irregular (e.g. random) strings are their 
own shortest description, and hence have high Kolmogorov complexity. An 
important property of K is that it is nearly independent of the choice of U. 
Furthermore, it shares many properties with Shannon's entropy (information 
measure) 5, but K is superior to S in many respects. Figure 2.11 on page 38 
contains a schematic graph oi K. To be brief, K is an excellent universal com-
plexity measure, suitable for quantifying Occam's razor. There is (only) one 
severe disadvantage: K is not finitely computable. More precisely, a function / 
is said to be finitely computable (or recursive) if there exists a Turing machine 
which, given x, computes f{x) and then halts. Some functions are not finitely 
computable but still approximable in the sense that there is a nonhalting Tur-
ing machine with an infinite output sequence 2/i,y2,2/3v with \mit-^ooyt~f 
{x). 

1.2 Simplicity & Uncertainty 
5 
If additionally the output sequence is monotone increasing/decreasing, then 
/ is said to be lower/upper semicomputable (or 
enumerable/co-enumerable). 
Finally, we call / estimable if some Turing machine, given x and a precision 
£, finitely computes an ^-approximation of x. The major algorithmic property 
of K is that it is co-enumerable, but not finitely computable. 
1.2.3 
Uncertainty &; Probabilities 
For the objectivist^ probabilities are real aspects of the world.^ The outcome 
of an observation or an experiment is not deterministic, but involves physical 
random processes. Kolmogorov's axioms of probability theory formalize the 
properties which probabilities should have. In the case of independent and 
identically distributed (i.i.d.) experiments the probabilities assigned to events 
can be interpreted as limiting frequencies {frequentist wiew) ^ but applications 
are not hmited to this case. Conditionalizing probabilities and Bayes' rule 
are the major tools in computing posterior probabilities from prior ones. For 
instance, given the initial binary sequence xi...Xn-i^ what is the probability 
of the next bit being 1? The probability of observing Xn at time n, given 
past observations xi...Xn-i 
can be computed with multiplication or the chain 
rule^ if the true generating distribution /i of the sequences xiX2Xs... is known: 
^{xn\x<^n) = l^{xi:n)/f^{x<n)^ whcrc wc iutroduccd the abbreviations xi:n = 
xiX2'.-Xn and a:<n = xiX2...a:n-i. The problem, however, is that one often 
does not know the true distribution /x (e.g. in the cases of weather and stock-
market forecasting). 
The subjectivist uses probabilities to characterize an agent's degree of belief 
in (or plausibility of) something, rather than to characterize physical random 
processes. This is the most relevant interpretation of probabilities in AI. It 
is somewhat surprising that plausibilities can be shown to also respect Kol-
mogorov's axioms of probability and the chain rule by assuming only a few 
plausible qualitative rules they should follow [Cox46]. Hence, if the plausibil-
ity of xi:n is p{xi:n)^ the dcgrcc of belief in Xn given x<n is, again, given by 
the chain rule: 
p{xn\x<n)=p{xi:n)/p{x<n)' 
The 
chain 
rule 
allows the 
computation 
of posterior 
probabili-
ties/plausibilities from prior ones, but leaves open the question of how to 
determine the priors themselves. In statistical physics, the principle of indif-
ference (symmetry principle) and the maximum entropy principle can often be 
exploited to determine prior probabilities, but only Occam's razor is general 
enough to assign prior probabilities in every situation, especially to cope with 
complex domains typical for AI. 
^ Readers not believing in objective and/or subjective probabilities should read the 
remark at the beginning of Section 2.3. 
^ Strictly speaking, it is just the definition of conditional probabilities. 

^ ( 1 
t=l 
6 
1 A Short Tour Through the Book 
1.2.4 
Algorithmic Probability S^ Universal Induction 
Occam's razor (appropriately interpreted and in compromise with Epicurus' 
principle of indifference) tells us to assign high/low a priori plausibility to 
simple/complex strings x. Using K as complexity measure, any monotone 
decreasing function of K, e.g. p{x) — 2~^^^\ would satisfy this criterion. But 
p also has to satisfy the probability axioms, so we have to be a bit more careful. 
Solomonoff [Sol64, Sol78] defined the universal prior M{x) as the probability 
that the output of a universal Turing machine U starts with x when provided 
with fair coin flips on the input tape. Formally, M can be defined as 
M{x) := 
Yl ^'^^""^ 
(1-1) 
p : 
U{p)=x^ 
where the sum is over all (so-called minimal) programs p for which U outputs 
a string starting with x. Strictly speaking M is only a semimeasure since 
it is not normalized to 1, but this is acceptable/correctable. We derive the 
following bound: 
oo 
-M{xt\x^t))^ 
< -^Y^lnM{xt\x<t) 
= -|lnM(xi:oo) < 
^ln2'Km{xi-.^o) 
where Km{xi:oo) is the length of the shortest (nonhalting) program computing 
xi:oo. In the first inequality we have used (1 —a)^ < —|lna for 0 < a < 1. In 
the equality we exchanged the sum with the logarithm and eliminated the 
resulting product by the chain rule. In the last inequality we used M{x) > 
2-Km{x) ^ which follows from definition (1.1) by dropping all terms in 
^ 
except for the shortest p computing x. If Xi:oo is a computable sequence, then 
Km{xi:oo) is finite, which implies M(xt|x<t)—>1 (X]^i(l —G^t)^ <oo ^ 
at-^ 
1). This means that if the environment is a computable sequence (whichever, 
e.g. the digits of TT or e in binary representation), after having seen the first 
few digits, M correctly predicts the next digit with high probability, i.e. it 
recognizes the structure of the sequence. 
Assume now that the true sequence is drawn from the distribution //, i.e. 
the true (objective) probability of xi:n is /i(xi:n), but /x is unknown. How is 
the posterior (subjective) belief M(xn|x<n) = M(xn)/M(x<n) related to the 
true (objective) posterior probability /i(x,^|x<^ )? Solomonoff's [Sol78] central 
result is that the posterior (subjective) beliefs converge to the true (objective) 
posterior probabilities, if the latter are computable. More precisely, he showed 
that 
oo 
2 
E 
E 
M^<t)(M(0|x<,)-MOk<t)) 
< iln2.i^(/i) + 0(l). 
(1.2) 
* = l x < t 6 { 0 , l } * - i 
The complexity K{IJL) is finite if /x is a computable function, but the infinite 
sum on the l.h.s. can only be finite if the difference M(0|x<t)—/i(0|x<t) tends 
to zero for t—^oo with /x-probability 1 (w./i.p.l). This shows that using M as 
an estimate for ji may be a reasonable thing to do. 

1,3 Universal Sequence Prediction 
7 
1.2.5 
Generalized Universal (Semi)Measures 
One can derive a universal prior in a different way: Solomonoff [Sol64, Eq.(13)] 
defines a somewhat problematic mixture over all computable probability dis-
tributions. Levin [ZL70] considers the larger class Mu •= {i^i,i^2v} of all 
so-called enumerable semimeasures. Let fiEMu, 
and assign (consistent with 
Occam's razor) a prior plausibility of 2"'^(^") to Ua^ Then the prior plausibility 
of xi:n is, by elementary probability theory, 
^u{XX:n):= 
Yl 
^'""^M^l-.n). 
(1-3) 
One can show that ^u coincides with M within an (irrelevant) multiplicative 
constant, i.e. M(x)=^[/(x), where f{x)<g{x) 
abbreviates f{x) = 0{g{x)), and 
= denotes < and >. Both ^u and M can be shown to be lower semicomputable. 
The dominance M{x) ^^u{x) 
> 2~'^^^^/i(x) is the central ingredient in the 
proof of (1.2). The advantage of £,u over M is that the definition immediately 
generalizes to arbitrary weighted sums of (semi) measures in Ai for arbitrary 
countable M.. Most proofs in this book go through for generic M. and weights. 
So, what is so special about the class of all enumerable semimeasures 
A^f/? The larger we choose A4, the less restrictive is the assumption that 
M. should contain the true distribution /i, which will be essential throughout 
the book. Why not restrict to the still rather general class of estimable or 
finitely computable (semi)measures? For every countable class A4, the mix-
ture ^{x) ''=^M{X) 
•—J2ueM'^^^(^) 
^^^^ ^^ -^ ^' ^^^ important dominance 
^{x) > Wi,u{x) is satisfied. The question is, what properties does ^ possess. 
The distinguishing property o^ Mu is that ^u is itself an element oi Mu- On 
the other hand, in this book ^M ^M 
is not by itself an important property. 
What matters is whether ^ is computable in one of the senses we defined 
above. There is an enumerable semimeasure (M) that dominates all enumer-
able semimeasures in Mu- As we will see, there is no estimable semimea-
sure that dominates all computable measures, and there is no approximable 
semimeasure that dominates all approximable measures. From this it follows 
that for a universal (semi)measure which at least satisfies the weakest form of 
computability, namely being approximable, the largest dominated class among 
the classes considered in this book is the class of enumerable semimeasures, 
but there are even larger classes [Sch02a]. This is the reason why Mu and 
M play a special role in this (and other) works. In practice though, one has 
to restrict to a finite subset of finitely computable environments u to get a 
finitely computable ^. 
1.3 Universal Sequence Prediction 
In the following we more closely investigate sequence prediction (SP) schemes 
based on Solomonoff's universal prior M = ^u and on more general Bayes 

8 
1 A Short Tour Through the Book 
mixtures ^, mainly from a decision-theoretic perspective. In particular, we 
show that they are optimal w.r.t. various optimality criteria. 
1.3.1 
Setup Sz Convergence 
Let Ai:—{z^i,i^2v} be a countable set of candidate probability distributions 
on strings over the finite alphabet Af. We define a weighted average on A4: 
^(^l:n) := 
^ 
Wi,'iy{xi:n), 
^ 
W^, = 1, 
W^ > 0. 
(1.4) 
It is easy to see that ^ is a probability distribution as the weights Wjy are pos-
itive and normalized to 1 and the veM. are probabilities. We call ^ universal 
relative to A^, as it multiplicatively dominates all distributions in A4 in the 
sense that C{^i:n)^^u'J^{xi:n) 
for all z/G A^. In the following, we assume that 
M is known and contains the true but unknown distribution /i, i.e. 
fieM, 
and Xi:oo is sampled from /x. We abbreviate expectations w.r.t. fi by E[..]; for 
instance, ^[f{xi:n)] — J2x 
eA'^i^(^i:^)/(^i:^)- We use the (total) relative en-
tropy Dn and squared Euclidian distance Sn to measure the distance between 
/i and ^: 
Dn := E 
Sn : = $ ^ E 
t=l 
x'eX 
(/i(x;ix<t)-c(x;ix<t)j . (1.5) 
The following sequence of inequalities can be shown, which generalize 
Solomonoff's result (1.2): Sn<Dn<\'^w~^ 
<(X). The finiteness of ^oo implies 
^(xj|x<t)—//(xJ|a:<t)-^0 for t^oo 
w./i.p.l for any x[ (X^^i^^ <oo=^St^'0). 
We also show that J2'^=,i'E'[{\/^{xt\x<^t)/l^{xt\x<:t)-'^)'^] < Dn < ^riw~^ < oo, 
which implies ^(j;t|^<t)//^(^t|^<t) ~^ 1 for t-^oo 
w./i.p.l. This convergence 
motivates the belief that predictions based on (the known) ^ are asymptoti-
cally as good as predictions based on (the unknown) /i, with rapid convergence. 
1.3.2 
Loss Bounds 
Most predictions are eventually used as a basis for some decision or action, 
which itself leads to some reward or loss. Let ixtyt ^ [O'l] CiR be the received 
loss when performing prediction/decision/action yt E y, and Xt E X is the 
-^th gyjjibol of the sequence. Let yf £y be the prediction of a (causal) pre-
diction scheme A. The true probability of the next symbol being Xt, given 
x<t, is ij.{xt\x^t)- The expected loss when predicting yt is E[-£a;^^J. The total 
/i-expected loss suffered by the A scheme in the first n predictions is 

1.3 Universal Sequence Prediction 
The goal is to minimize the expected loss. More generally, we de-
fine the Ap sequence prediction scheme (later also called SPp) y^p := 
argminy^^^^^ p{xt\x^t)ixi,yt, 
which minimizes the p-expected loss. If /i is 
known, ^^ is obviously the best prediction scheme in the sense of achieving 
minimal expected loss {L^^ < L^ for any yl). We prove the following loss 
bound for the universal A^ predictor 
0 < L^'-L^^ 
< Dn^y/AL^i^D^^TDl 
< 2D^ + 2 ^ l 4 ^ , 
(1.6) 
Together with Ln<n and Doo ^ Inty"^ < oc, this shows that ^i^^^ — ^-^n^ — 
0(n~^/^), i.e. asymptotically A^ achieves the optimal average loss of TI^ with 
rapid convergence. Moreover, L^ is finite if L^ 
is finite, and L^^ /L^^ —> 1 if 
L^ 
is not finite. Bound (1.6) also implies L^ > L^^ — 2\JL^^ Dn, which shows 
that no (causal) predictor A whatsoever achieves significantly less (expected) 
loss than A^. Note that for Wj,=^2~^^''\ 
Dn <\n2-K{ji) 
is of "reasonable" 
size. Instantaneous loss bounds can also be proven. 
1.3.3 
Optimality Properties 
For any predictor A, a worst-case lower bound that asymptotically matches the 
upper bound (1.6) can be derived. More precisely, let A be any deterministic 
predictor not knowing from which distribution fiEM 
the observed sequence 
xia:2.-. is sampled. Predictor A knows (depends on) M, w^y, and f, and has at 
time t access to the previous outcomes x<t. Then for every n there is an M. 
and fiEM. and £ and weights Wi, such that 
L ^ - L ^ 
> l[Sn^V^L^'Sn 
+ Sl], 
and 
DjSn^l 
for 
n ^ oo. 
For the universal predictor yl = yl^, the lower bound holds even without the 
factor ^. This shows that bound (1.6) is quite tight in the sense that no 
other predictor can lead to significantly smaller bounds without making extra 
assumptions on M, Wi^^ ov L For instance, for logarithmic and quadratic loss 
functions the regret L^ —L^ is finite and bounded by lnw~^. 
A diff^erent kind of optimality is Pareto optimality. Let J^{p,p) be any 
performance measure of p relative to p. The universal prior ^ is called Pareto 
optimal w.r.t. T if there is no p with J^{y^p) < ^(^^,0 for all u G M and 
strict inequality for at least one u. We show that the universal prior ^ is 
Pareto optimal w.r.t. the squared distance S'n, the relative entropy Dn, and 
the losses Ln- That is, for all performance measures that are relevant from a 
decision-theoretic point of view (i.e. for all loss functions i) any improvement 
achieved by some predictor Ap over A^ in some environments u is balanced 
by a deterioration in other environments. There are non-decision-theoretic 
performance measures w.r.t. which ^ is not Pareto optimal. Pareto optimality 
is a rather weak notion of optimality, but it emphasizes the distinctiveness of 
Bayes mixture strategies. 

10 
1 A Short Tour Through the Book 
Pareto optimality of ^ still leaves open the question of how to choose the 
class A4 and the weights Wi^. We have argued that Aiu is the largest A4 
suitable from a computational point of view. A^c/ is also sufficiently large if 
we make the mild assumption that strings are sampled from a computable 
probability distribution. We show that within the class of enumerable weight 
functions with short program, the universal weights Wjy — 2~^^^^ lead to the 
smallest performance bounds within an additive (to \jiw~^) constant in all 
enumerable environments. This argument justifies the selection of Solomonoff-
Levin's prior (1.3) among all possible Bayes mixtures? 
1.3.4 
Miscellaneous 
Games of chance. The general loss bound (1.6) can, for instance, be used 
to estimate the time needed to reach the winning threshold in a game of 
chance (defined as a sequence of bets, observations and rewards). At time t 
we bet, depending on the history x<t, a certain amount of money St, take some 
action yt, observe outcome Xt, and receive reward r^. Our net profit, which 
we want to maximize, is pt = rt — St^ [Pmax~PA^Pmax]- The loss, which we 
want to minimize, can be identified with the negative (scaled) profit, ixkyt~ 
{Pmax—Pt)/PA ^ [O^l]- The ylp-system acts as to maximize the p-expected 
profit. Let p^p be the average expected profit of the first n rounds. Bound (1.6) 
shows that the average profit of the A^ system converges to the best possible 
average profit p^^ achieved by the A^ scheme {p^^ ~Pn^ =0(n""^/^) ^^0 for 
n^oo). If there is a profitable scheme at all, then asymptotically the universal 
A^ scheme will also become profitable with the same average profit. We further 
show using ^jj that (2p/^/p^^)^-ln2-K(/i) is an upper bound on the number 
of bets n needed to reach the winning zone. The bound is proportional to the 
complexity of the environment /i. 
Continuous probability classes A4. We have considered thus far count-
able probability classes A^, which makes sense from a computational point 
of view. On the other hand, in statistical parameter estimation one often 
has a continuous hypothesis class (e.g. a Bernoulli(^) process with unknown 
^ G [0,1]). Let A1 := {/i,9: ^ G 0 C JR"^} be a family of probability distributions 
parameterized by a d-dimensional continuous parameter 9. Let 11 = 120^ G A^ be 
the true generating distribution. For a continuous weight density w{0)>0 the 
sums in (1.4) are naturally replaced by integrals: ^{xi:n) '-=J^wlO)-fj,e{xi:n)dO 
with J^w{0)d6 = 1. The most important property of ^ in the discrete case 
was the dominance (,{xi:n) ^Wiy-h'{xi:n)^ which was obtained from (1.4) by 
dropping the sum over u. The analogous construction here is to restrict the 
integral over 6> to a small vicinity Ns oi 9. For sufficiently smooth JIQ and 
w{9) we expect ^(xi:n)^|A^(5^| •tt'(^)')U^(xi:n), where |Ar^^| is the volume of 
^ Readers who smell some free lunch here [WM97] should appease their hunger 
with Section 3.6.5. 

1.4 Rational Agents in Known Probabilistic Environments 
11 
Ns^. This in turn leads to Dn^lnw~^+ITI\NSJ~^, 
where w,j, :=w{6o)- Ns^ 
should be the largest possible region in which In/igi is approximately flat on 
average. More precisely, generalizing [CB90] to the non-i.i.d. case, we show 
Dn<\Tiw~^ + ^ln-^-^0{l), 
where the 0(1) term depends on the smoothness 
of fjLe, measured by the Fisher information. Dn is no longer bounded by a con-
stant, but still grows only logarithmically with n, the intuitive reason being 
the necessity to describe 0 to an accuracy 0(n-i/2). So, bound (1,6) is also 
applicable to the case of continuously parameterized probability classes. 
1.4 Rational Agents in Known Probabilistic 
Environments 
1.4.1 
The Agent Model 
A very general framework for intelligent systems is that of rational agents 
[RN95]. In cycle /c, an agent performs action yk^y 
(output), which results in 
a perception Xk^X (input), followed by cycle /c+l, and so on. We assume that 
the action and perception spaces X and y are finite. We write p{x^k) — yi:k 
to denote the output yi^k of the agent's policy p on input x<fc, and similarly 
q{yi:k)=xi:k for the environment q in the case of deterministic environments. 
We call policy p and environment q behaving in this way chronological. The 
figure on the book cover and on page 128 depicts this interaction in the case 
where p and q are modeled by Turing machines. Note that policy and envi-
ronment are allowed to depend on the complete history. We do not make any 
MDP or POMDP assumption here, and we do not talk about states of the envi-
ronment, only about observations. In the more general case of a probabilistic 
environment, given the history 
y]c^kyk^Wi-"Wk-iyk=yiXi...yk-iXk-iyk, 
the probability that the environment leads to perception a^fc in cycle k is (by 
definition) fj,{yx^kW^k)- '^^^ underlined argument x_^ in ^ is a random vari-
able, and the other non-underlined arguments yjc<^kyk represent conditions? 
We call probability distributions like JJL chronological Since value-optimizing 
policies (see below) can always be chosen deterministic, there is no real need 
to generalize the setting to probabilistic policies. 
1.4.2 
Value Functions &; Optimal Policies 
The goal of the agent is to maximize future rewards^ which are provided by 
the environment through the inputs Xk- The inputs Xk=rkOk are divided into 
a regular part Ok and some (possibly empty or delayed) reward r^ G [0 , rmax]-^ 
We use the abbreviation 
^ The standard notation fi{xk\yjc<^kyk) for conditional probabilities destroys the 
chronological order and would become confusing in later expressions. 
^ In the reinforcement learning literature when dealing with (PO)MDPS the reward 
is usually considered to be a function of the environmental state. The zero-

12 
1 A Short Tour Through the Book 
which is essentially the chain rule, and e = i/r<i for the empty string. We define 
the (total) value of policy p in environment /i, or shorter, the /i-value of p, as 
the /x-expected reward sum 
^M •= X](^l+-+^m)M(^l:.n)|yi:^=p(x<^), 
(1.7) 
Xl.rn 
where m is the lifespan or initial horizon of the agent. The optimal policy p^ 
that maximizes the value V,f is 
p^ := arg max V^, 
V; := Vf 
= max V^ > V^ Vp. 
The policy p^, which we call AIii models is optimal in the sense that no other 
policy for an agent leads to higher /x-expected reward. Explicit expressions for 
the action yk in cycle k of the //-optimal policy p^ and their value y* are 
Vk = Vk •= a r g m a x V m a x V 
... max V(r/,+...+r^)-/i(^</e?^fc,^), 
Vk 
^-^ 
Vk+l 
^-^ 
Vm 
^—^ 
(1.8) 
K 
= m a x V m a x V 
... m a x V ( r i + . . . + r ^ ) - / i ( ^ i : ^ ) , 
(1.9) 
^ 
yi 
^—^ 
y2 
^-^ 
Vm 
^-^ 
Xi 
X2 
Xm 
where 2/r<fc is the actual history. We show that these definitions are consis-
tent and correctly capture our intention. For instance, consider the expec-
timax expression (1.9): The best expected reward is obtained by averaging 
over possible perceptions Xi and by maximizing over the possible actions yi. 
This has to be done in chronological order yiXi...ymXm 
to correctly incorpo-
rate the dependencies of Xi and y^ on the history. This is the origin of the 
alternating expectimax sequence, which is similar to the well-known minimax 
sequence/tree/algorithm in game theory. 
1.4.3 
Sequential Decision Theory &: Reinforcement Learning 
One can relate (1.9) to the Bellman equations [Bel57] of sequential decision 
theory by identifying complete histories yx^k with states, ii{yx<:^kWk) with the 
state transition matrix, F* with the value function, and yk with the action in 
cycle k [BT96, RN95]. Due to the use of complete histories as state space, the 
Al/i model assumes neither stationarity nor the Markov property nor complete 
accessibility of the environment. Every state occurs at most once in the lifetime 
of the system. For this and other reasons the explicit formulation (1.8) is more 
assumption analogue here is that the reward Vk is some probabilistic function 
/i' depending on the complete history. It is very convenient to integrate Vk into 
Xk and fi' into p. 

1.5 The Universal Algorithmic Agent AIXI 
13 
natural and useful here than to enforce a pseudo-recursive Bellman equation 
form. 
As we have in mind a universal system with complex interactions, the 
action and perception spaces y and A! are huge (e.g. video images), and every 
action or perception itself occurs usually only once in the lifespan m of the 
agent. As there is no (obvious) universal similarity relation on the state space, 
an eflFective reduction of its size is impossible, but there is no principle problem 
in determining yk from (1.8) as long as fi is known and computable and A', y 
and m are finite. 
Things drastically change if /x is unknown. Reinforcement learning algo-
rithms [BT96, KLM96, SB98] are commonly used in this case to learn the 
unknown /x or directly its value. They succeed if the state space is either 
small or has effectively been made small by generalization or function ap-
proximation techniques. In any case, the solutions are either ad hoc, work in 
restricted domains only, have serious problems with state space exploration 
versus exploitation, are prone to diverge, or have nonoptimal learning rates. 
There is no universal and optimal solution to this problem so far. The central 
theme of this book is to present a new model and to argue that it formally 
solves all these problems in an optimal way. The true probability distribu-
tion /i will not be learned directly, but will be replaced by some generalized 
universal prior ^[/, which converges to //, similarly to the induction (SP) case. 
1.5 The Universal Algorithmic Agent AIXI 
1.5.1 
The Universal AIXI Model 
We have developed enough formahsm to present the universal AIXI model. 
All we have to do is to suitably generalize Solomonoff's universal prior M 
and to replace the true but unknown probability /x in the Al/i model by this 
generalized M. Similarly to (1.1), we define M as the 2~^^^^ weighted sum 
over all chronological programs (environments) q that output xi-k^ but with 
yi-k provided on the input tape. This also generalizes ^u (within an irrelevant 
multiplicative constant): 
amuk) = ^u{mi:k) =Mfe,,) := Y. 2-'^'^- 
(1-10) 
q-q{yi:k)=xi:k 
If not clear from context, we add superscripts SP and AI to ^, to resolve 
ambiguities between (1.3) and (1.10). Replacing /x by ^ in (1.8) the AIXI 
system outputs 
Vk = Vk '= argmaxV,..maxy'(rfc+...-hr^)-^(yr<fc?^fc:^) 
(1.11) 
Vk 
y-m 
in cycle k given the history yjc^^k- The ^-value y / and the universal value V7 
are defined as in (1.7) and (1.9), with fi replaced by ^. The AIXI model and 

14 
1 A Short Tour Through the Book 
its behavior is completely defined by (1.10) and (1.11). It (slightly) depends 
on the choice of the universal Turing machine, because K{) and i{) depend on 
U and hence are defined only up to terms of order one. The AIXI model also 
depends on the choice of ^ and 3^, but we do not expect any bias when the 
spaces are chosen sufficiently large and simple, e.g. all strings of length 2^^. 
Choosing IN as the I/O spaces would be ideal, but whether the maxima (or 
suprema) exist in this case has to be shown beforehand. The only nontrivial 
dependence is on the horizon m. Ideally, we would like to chose m = oo, but 
there are several subtleties to be unraveled later, which prevent at least a naive 
limit m—^oo. So apart from m and unimportant details, the AIXI system is 
uniquely defined by (1.10) and (1.11) without adjustable parameters. 
1.5.2 
On the Optimality of AIXI 
Universality and convergence of ^. One can show that also ^ defined in 
(1.10) is universal and rapidly converges to /x analogous to the induction (SP) 
case. If we take a finite product of conditional ^'s and use the chain rule, we 
see that also ^{yjc^kWk-.k+h) converges to ii{yjc^kWk-.k+h) ^^^ k—^oc. This gives 
confidence that the outputs y^ of the AIXI model (1.11) could converge to the 
outputs 2/^ of the Al/i model (1.8), at least for a bounded moving horizon h. 
The problems with a fixed horizon m and especially m^oo 
will be discussed 
at the end of this section. 
Universally optimal AI systems. We call an AI model universal if it is 
independent of the true environment /i (unbiased, model-free) and is able to 
solve any solvable problem and learn any learnable task. Further, we call a 
universal model universally optimal if there is no program that can solve or 
learn significantly faster (in terms of interaction cycles). As the AIXI model 
is parameter-free, ^ converges to /i, the Al/i model is itself optimal, and we 
expect no other model to converge faster to Al/i by analogy to the SP case, 
we expect AIXI to be universally optimal. 
This is our main claim. Further support is given below. 
Intelligence order relation. We want to call a policy p more or equally 
intelligent than a policy p^ and write p^p' if p yields in every cycle k and for 
every fixed history ^<fc higher (future) ^-expected reward sum than p\ It is a 
formal exercise to show that p^>zp for all p. The AIXI model is hence the most 
intelligent agent w.r.t. ^. Relation ^ is a universal order relation in the sense 
that it is free of any parameters (except m) or specific assumptions about 
the environment. A proof that >: is a reasonable intelligence order (which we 
believe to be true) would prove that AIXI is universally optimal. 
Value bounds. The values V^ associated with the Alp systems correspond 
roughly to the negative total loss —L^^ (with n=m) of the SPp {=Ap) systems. 

1.5 The Universal Algorithmic Agent AIXI 
15 
In the SP case we were interested in small bounds for the regret L^^ — L^^. 
Unfortunately, simple value bounds for AIXI or any other AI system in terms 
of V^ analogous to the loss bound (1.6) cannot hold. We even have difficulties 
in specifying what we can expect to hold for AIXI or any AI system that claims 
to be universally optimal. In SP, the only important property of // for proving 
loss bounds wsts its complexity K{ii). In the AI case, there are no useful 
bounds in terms of K{ii) only. We either have to study restricted problem or 
environmental classes or consider bounds depending on other properties of /i, 
rather than on its complexity only. 
1.5.3 Value-Related Optimality Results 
The mixture distribution ^. In the following, we consider general Bayes 
mixtures ^ over classes AI of chronological probability distributions v: 
Cfe:m) = X ! '^y^iWi'.m) 
with 
"^ Wy = I 
and 
^^ > 0 \/v e M. 
We define V^, p^, and V^ as in (1.7)-(1.9) with /x replaced by ^. Policy p^ is 
called the Al^ model. For (,=^u the AlXl=Al^u 
model is recovered. If fi is 
unknown, but known to belong to the known class A^, it is natural to follow 
policy p^, which maximizes Vf. The (true /i-)expected reward when following 
policy p^ is V^ . The optimal (but infeasible) policy p^ yields reward V^ = V^ • 
It is now of interest (a) whether there are policies with uniformly larger value 
than V^ and (6) how close V^ is to V*. 
Linearity and convexity of Vp in p. The following properties of Vp are 
crucial. V^ is a linear function in p, and V^ is a convex function in p in the 
sense that 
Linearity is obvious from the definition of VJ, and convexity follows easily 
from the convexity of maxp and nonnegativity of the weights w^. One loose 
interpretation of the convexity is that a mixture can never increase perfor-
mance. 
Pareto optimality of AI^. Similarly to the SP case, one can show that p^ 
is Pareto optimal in the sense that there is no other policy p with VJ > VJ 
for all UEM. and strict inequality for at least one z/. In particular, AIXI is 
Pareto optimal. 
Self-optimizing policy p^ w.r.t. average value. Since we do not know the 
true environment fi in advance, we are interested under which circumstances^ 
^ Here and elsewhere we interpret am^bm 
as an abbreviation for am — bm -^0. 
linim^oo^m may not exist. 

16 
1 A Short Tour Through the Book 
^Vf 
-> ^ F ; for horizon 
m -^ oo for all u e M. 
(1.12) 
Note that Vjy as well as p^ =Pm depend on m. The least we must demand from 
M to have a chance that (1.12) is true is that there exists a policy (sequence) 
p—prn at all with this property, i.e. 
3 p : ^ l / / ^ ^ y ; 
for horizon 
m - ^ oo for a// 
v e M. 
(1.13) 
We show that this necessary condition is also sufficient, i.e. (1.13) implies 
(1.12). This is another (asymptotic) optimality property of policy p^. //uni-
versal convergence in the sense of (1.13) is possible at all in a class of environ-
ments M, then policy p^ converges in the same sense (1.12). We call policies 
p with a property like (1.13) self-optimizing [KV86]. 
Unfortunately, the result is not an asymptotic convergence statement of 
a single policy p^, since p^ depends on m. The result merely says that under 
the stated conditions the average value of p%^ is arbitrarily close to optimum 
for sufficiently large (pre-chosen) horizon m. This weakness will be resolved 
in the following. 
Discounted future value function. We now shift our focus from the total 
value to future values (value-to-go). First, we have to get rid of the horizon 
parameter m. We eliminate the horizon by discounting the rewards Vk^^^^kf^k 
with 7fc >0 and Yll^ili <^^ ^^d taking m-^oo. The analogue of m is now an 
effective horizon hf, 
which may be defined by J2i=k ^ li'^Y^l^k+h^^^ ^^' -^^^" 
thermore, we renormalize the value V by Yl^k'^'^ ^^^ denote it by Vk^- Finally, 
we extend the definition to probabilistic policies TT (which is not essential). We 
define the 7-discounted weighted-average future value of (probabilistic) policy 
TT in environment p given history yz:<fc, or shorter, the /9-value of TT given yjc<^ki 
as 
^k^{WC<k) 
•= 77- 1™ 
V 
(7fcr'/e+...+7mrm)p(^<A:^fc:m)^(l/^<fc^A::m), 
with Fk •—X]Sfe7«' The policy p^ is defined as to maximize the future value 
K := arg max F.^, 
¥,%' := < " = max V,\' > V^ Vrr. 
Setting 7fc = 1 for k<vn and 7^ = 0 for k>vn gives back the old undiscounted 
model with horizon m and V^^ = m^/- ^^^^ ^^^^ ^^i depends on the real-
ized history 2/r</e. More important, p^ can be shown to be independent of k. 
Similarly to the undiscounted case, one can prove that for every k and history 
yc<iki VkS is a linear function in p, V^^ is a convex function in p, and p^ is 
Pareto optimal in the sense that there is no other policy n with Vj^^ > VF ^ for 
all u^M 
and strict inequality for at least one v. Finally, p^ is self-optimizing 
(w.r.t. discounted value) if AA admits self-optimizing policies: 

1.5 The Universal Algorithmic Agent AIXI 
17 
If 3frVi/: y , 7 "^-^^ 1^;; 
w.z/.p.l 
=^ 
yP^^ ^^1^ v^!; 
w./i.p.l. 
The probability qualifier refers to the historic perceptions x<fc. The historic 
actions y<^k are arbitrary. Note that /c is a real running value, namely the 
current cycle number, whereas m was a pre-chosen fixed horizon. 
1.5.4 
Markov Decision Processes 
From all possible environments, Markov (decision) processes are probably the 
most intensively studied ones. // is called a (completely observable stationary) 
Markov decision process (MDP) if the probability of perceiving x/c G A', given 
history yjc^kUk only depends on the last action yu^y 
and the last perception 
x/e_i, i.e. if fi{yr<::kyk^k) — f^i^k-iVkX}^)' In this case Xk is called a state, X 
the state space, and fi{xk-iyk^k) 
^^^ transition matrix. An MDP fi is called 
ergodic if there exists a policy under which every state is visited infinitely 
often with probability 1. If an MDP ji^Xk-iyk^k) 
i^ independent of the action 
yk it is a Markov process] if it is independent of the last perception Xk-i it is 
an i.i.d. process. 
Stationary MDPs ji with geometric discounting 7^ = 7^ have stationary 
optimal policies p^ mapping the same state/perception x^ always to the same 
action yk- On the other hand, a mixture ^ of MDPs is itself not an MDP, i.e. 
^^A^MDP^ which implies that p^ is, in general, not a stationary policy. 
One can construct self-optimizing policies for the class of ergodic MDPs 
w.r.t. the average value — Kf and if ^^^^^ -^ 1 also w.r.t. to the discounted 
future value V^^. The necessary condition ^^^^ -^ 1 ensures unboundedly 
increasing effective horizon h^^^. The existence of self-optimizing policies for 
ergodic MDPs implies that for a countable class M of ergodic MDPs, the policies 
p ^ maximizing VF and p^ maximizing V^^ are self-optimizing in the sense 
that 
V^eM : ivf-" 
""-^ 
^VZ 
and 
^ ^ ^ ' ^ 
l^T 
if ^ 
^ 1- (l-^) 
We also show that if M. is finite, then the speed of the first convergence 
is at least 0(m~^/^). The conditions FkKoo and ^^^^^ —J> 1 on the discount 
sequence are, for instance, satisfied for 7^ = 1/fc^, but not for the popular 
geometric discount 7/^=7^^, which has finite effective horizon. 
Limits (1.14) show that p^ is self-optimizing for bandits, i.i.d. processes, 
and classification tasks, since they are special (degenerate) cases of ergodic 
MDPs. The existence of self-optimizing policies is not limited to (subclasses 
of ergodic) MDPs. Certain classes of POMDPS, A:^^-order ergodic MDPs, factor-
izable environments, repeated games, and prediction problems are not MDPs, 
but nevertheless admit self-optimizing policies. Hence the corresponding Bayes 
optimal mixture policy p^ is self-optimizing. 

18 
1 A Short Tour Through the Book 
1.5.5 
The Choice of the Horizon 
The only significant arbitrariness in the AIXI model lies in the choice of the 
lifespan m or in the discounted case in the discount sequence 7^. We will not 
discuss ad hoc choices for specific problems. We are interested in universal 
choices. In many cases the time we are willing to run a system depends on 
the quality of its actions. Hence, the lifetime, if finite at all, is not known in 
advance. Geometric discounting Vk^^Vk'^^ solves the mathematical problem of 
m—^00 but is not a real solution, since an effective horizon /i^-^-^~ln7~^ <oo has 
been introduced. The scale-invariant discounting rk^^rk'k~^ 
with a>l has a 
dynamic horizon h^k. This choice has some appeal, as it seems that humans of 
age k years also usually do not plan their lives for more than the next ^k years. 
It also satisfies the condition -^^^^^ —> 1, necessary for AI^ being self-optimizing 
in ergodic MDPS. The largest lower semicomputable horizon with guaranteed 
finite reward sum Ti < 00 is obtained by the discount rk^^rk-2~^^^\ 
where 
K{k) is the Kolmogorov complexity of k. This is maybe the most attractive 
universal discount. It is similar to a near-harmonic discount 
rk^^rk'k~^^~^^\ 
since 2"^^^^ < 1/k for most k and 2~^^^^ >c/{klog^k) 
for some constant c. 
We are not sure whether the choice of the horizon is of marginal importance, 
as long as it is chosen sufficiently large, or whether the choice will turn out 
to be a central topic for the AIXI model or for the planning aspect of any 
universal AI system in general. Most, if not all, problems in agent design of 
balancing exploration and exploitation vanish by a sufficiently large choice of 
the (effective) horizon and a sufficiently general prior. 
1.6 Important Environmental Classes 
In this and the next section we define ^ — ^u — M be Solomonoff's prior, i.e. 
AI^=AIXL Each subsection represents an abstract on what will be done in 
the corresponding section of Chapter 6. 
1.6.1 
Introduction 
In order to give further support for the universality and optimality of the AI^ 
theory, we apply AI^ to a number of problem classes. They include sequence 
prediction, strategic games, function minimization and, especially, how AI^ 
learns to learn supervised. For some classes we give concrete examples to illu-
minate the scope of the problem class. We first formulate each problem class 
in its natural way (when ^p^o^iem -g j^^Q^n) and then construct a formulation 
within the AI/x model and prove its equivalence. We then consider the conse-
quences of replacing /i by ^. The main goal is to understand why and how the 
problems are solved by AI^. We only highlight special aspects of each problem 
class. The goal is to give a better picture of the flexibility of the AI^ model. 

1.6 Important Environmental Classes 
19 
1.6.2 
Sequence Prediction (SP) 
Using the AI/x model for sequence prediction (SP) is identical to Bayesian 
sequence prediction SP/x. One might expect, when using the AI^ model for 
sequence prediction, one would recover exactly the universal sequence predic-
tion scheme SP^, as AI^ was a unification of the Al/j, model and the idea of 
universal probability ^. Unfortunately, this is not the case. One reason is that 
^ is only a probability distribution in the inputs x and not in the outputs y. 
This is also one of the origins of the difficulty of proving loss/value bounds for 
AI^. Nevertheless, we argue that AI^ is as well suited for sequence prediction 
as SP^. In a very limited setting we prove a (weak) error bound for AI^, which 
gives hope that a general proof is attainable. 
1.6.3 
Strategic Games (SG) 
A very important class of problems are strategic games (SG). We restrict 
ourselves to deterministic strictly competitive strategic games like chess. If the 
environment is a minimax player, the AI/JL model itself reduces to a minimax 
strategy. Repeated games of fixed lengths are a special case of factorizable /x. 
The consequences of variable game lengths are sketched. The AI^ model has to 
learn the rules of the game under consideration, as it has no prior information 
about these rules. We describe how AI^ actually learns these rules. 
1.6.4 
Function Minimization (FM) 
Many problems fall into the category 'resource-bounded function minimiza-
tion' (FM). They include the traveling salesman problem, minimizing pro-
duction costs, inventing new materials or even producing, e.g. nice paintings, 
which are (subjectively) judged by a human. The task is to (approximately) 
minimize some function f-.y^Z 
within a minimal number of function calls. 
We will see that a greedy model trying to minimize / in every cycle fails. 
Although the greedy model has nothing to do with downhill or gradient tech-
niques (there is nothing like a gradient or direction for functions over 3^), 
which are known to fail, we discover the same difficulties. FM has already 
nearly the full complexity of general AI. The reason being that FM can ac-
tively influence the information gathering process by its trials yk (whereas SP 
and CF==classification cannot). We discuss in detail the optimal FM^u model 
and its inventiveness in choosing the y^y. 
A discussion of the subtleties when 
using AI^ for function minimization follows. 
1.6.5 
Supervised Learning from Examples (EX) 
Reinforcement learning, as the AI^ model does, is an important learning tech-
nique, but not the only one. To improve the speed of learning, supervised 
learning, i.e. learning by acquiring knowledge, or learning from a constructive 

20 
1 A Short Tour Through the Book 
teacher, is necessary. We show how AI^ learns to learn supervised. It actually 
establishes supervised learning very quickly within 0(1) cycles. 
1.6.6 
Other Aspects of Intelligence 
Finally, we give a brief survey of other general aspects, ideas and methods in 
AI, and their connection to the AI^ model. Some aspects are directly included 
in the AI^ model, while others are or should be emergent. 
1.7 Computational Aspects 
Up to now we have shown the universal character of the AIXI model but 
have completely ignored computational aspects. We start by developing an 
algorithm ML that is capable of solving any well-defined problem p as quickly 
as the fastest algorithm computing a solution to p, save for a factor of l+e: 
and lower-order additive terms. Based on a similar idea we then construct a 
computable version of the AIXI model. 
1.7.1 
The Fastest & Shortest Algorithm 
for All Well-Defined Problems 
Introduction. A wide class of problems can be phrased in the following 
way. Given a formal specification / : Af ^ 3^ of a problem depending on some 
parameter x G A', we are interested in a fast algorithm computing solution 
yey. 
Levin search is (within a large constant factor) the fastest algorithm to in-
vert a function g-.y-^X^ if g can be evaluated quickly [Lev73b, Lev84]. Levin 
search can also handle time-limited optimization problems [Sol86]. Prime fac-
torization, graph coloring, and truth assignments are example problems suit-
able for Levin search, if we want to find a solution, since verification is quick. 
Levin search cannot decide the corresponding decision problems. It is also 
not applicable to, e.g. matrix multiplication and reinforcement learning, since 
the verification task g is as hard as the computation task. Blum's speed-up 
theorem [Blu67, Blu71] shows that there are types of problems / for which an 
(incomputable) sequence of speed-improving algorithms (of increasing size) 
exists, but no fastest algorithm. 
In the approach presented here, we consider only those algorithms that 
provably solve a given problem and have a fast (i.e. quickly computable) time 
bound. Neither the programs themselves nor the proofs need to be known 
in advance. Under these constraints we construct the asymptotically fastest 
algorithm save a factor of l + e that solves any well-defined problem /. 
The fast algorithm Af^*. Let ^* be a given algorithm computing p*(x) 
from X, or, more generally, a specification of a function /. One ingredient to 

1.7 Computational Aspects 
21 
our fastest algorithm M^* to compute p* {x) is an enumeration of proofs of 
increasing length in some formal axiomatic system. If a proof actually proves 
that some p is functionally equivalent to p*, and p has time bound tp, the tuple 
(p^tp) is added to a list L. The program p in L with the currently smallest 
time bound tp{x) is executed. By construction, the result p{x) is identical 
to p*(x). The trick to achieve a small runtime is to schedule everything in a 
proper way, in order not to lose too much performance by computing slow p's 
and tp's before the p has been found. 
More formally, we say that a program "p computes function /", when a 
universal reference Turing machine U on input {p,x) computes f{x) for all x. 
This is denoted by U{p^x) = f{x). To be able to talk about proofs, we need 
a formal logic system (V,A,2/^,Ci,/i,i^^,—>,A,—,...) and axioms and inference 
rules. A proof is a sequence of formulas, where each formula is either an 
axiom or inferred from previous formulas in the sequence by applying the 
inference rules. We only need to know that provability, Turing Machines, and 
computation time can be formalized, and that the set of (correct) proofs is 
enumerable. We say that p is provably equivalent to p* if the formula [Vy: 
U{Piy) — U{p*^y)] can be proven. Let us fix £G (0,^). M^* runs three algorithms 
A, B, and C in parallel: 
M;.{X) 
Initialize the shared variables 
Run through all proofs. 
L:={}, 
tfast :=oo, 
pfast •=P*- 
i^ ^ P^o<^f proves for some {p,t) that 
Start algorithms A, B, and C 
P(') is equivalent to (computes) p*{-) 
in parallel with relative computational and has time bound t(-) 
resources e, £, and 1 —2s, respectively. 
then add {p,t) to L. 
B\ 
\C 
Compute all t{x) in parallel 
run U on (pfast,x). 
for all (p,t) GL with 
For each time step decrease tfast by 1. 
relative computation time 2~^^^^~^^^\ 
if U halts then print result U{pfast,x) 
if for some t, t(x)<tfast, 
and abort computation of A, B and C. 
then tfast'=t(x) and pfast'—p-
continue 
Note that A and B only terminate when aborted by C. It is obvious that M^* 
is equivalent to (computes) p*. We show that the computation time of M^* is 
bounded by 
timeM%{x) 
< {I-^ e) • tp{x) + ^ ' timet^{x) + ^, 
dp = 3.2^(^)+^(*-\ 
Cp = 3.2^(^^^^-^^))+i.O(^(proo/p)2), 
where p is any algorithm, provably computing the same function as p* with 
computation time provably bounded by the function tp{x) for all x, and 
timetp{x) 
is the time needed to compute the time bound tp{x). Known 

22 
1 A Short Tour Through the Book 
time bounds for practical problems can often be computed quickly, i.e. 
timet^{x)Itimep{x) 
often converges very quickly to zero. Furthermore, from 
a practical point of view, the provability restrictions are often rather weak. 
Hence, we have constructed for all those problems a solution that is asymp-
totically only a factor 1+e slower than the (provably) fastest algorithm. On 
the flip side, for realistically sized problems, the lower-order terms usually 
dominate, which limits the practical use of MS. 
Algorithmic complexity and the shortest algorithm. A natural defi-
nition for the (Kolmogorov) complexity of a function / is the length of the 
shortest program computing /: K'{f) :=minp{^(p) : U{p^x) = /(x)Vx}. Un-
fortunately, K^ suffers from not even being approximable, since functional 
equality of programs is in general undecidable. Let p* be a formal specifica-
tion or a program for /. Using K{p*) is also not a suitable alternative, since 
it essentially depends on the choice of p* because, e.g. "dead code" in p* con-
tributes to K{p'^). A satisfactory solution is to take the length of the shortest 
program provably equivalent to p*: 
K'\p*) 
:= min{-£(p) : a proof of \iy\U{p,y) 
— U{p*^y)] exists}. 
p 
K" (like K) is upper semicomputable. Let p' be some short description of p*. 
We are now concerned with the computation time of p'. Could we get slower 
and slower algorithms by compressing p* more and more? Interestingly, this is 
not the case. Inventing complex (long) programs is not necessary to construct 
asymptotically fast algorithms, under the stated provability assumptions, in 
contrast to Blum's theorem [Blu67, Blu71]. We show that there exists a pro-
gram p, equivalent to p* with 
(z) i{p) 
< K''(p*) + 0(1), 
(ii) 
timep{x) 
< (1 + e)-tp(x) +-^•timetp(x)-h ^ , 
where p is any program provably equivalent to p* with computation time 
provably less than tp{x). That is, p is simultaneously among the shortest and 
fastest programs. 
Generalizations. Algorithm M^* can be modified to handle I/O streams, 
definable by a Turing machine with unidirectional input and output tapes 
(and bidirectional work tapes) receiving an input stream and producing an 
output stream, as is the case in the agent setup. 
1.7.2 
Time-Bounded AIXI Model 
The major drawback of the AIXI model is that it is uncomputable. To over-
come this problem, we construct a modified algorithm AlXIt^, which is still 
superior to any other time t and length / bounded agent. The computation 

1.7 Computational Aspects 
23 
time of AlXltl is of the order t'2K Reducing the large factor 2^ along the lines 
of the previous subsection is possible, but will not be presented here. 
Non-efFectiveness of AIXI. ^^^=^^^ is not a computable but only an enu-
merable semimeasure. Hence, the output yk of the AIXI model is only asymp-
totically computable (approximable). AIXI yields an algorithm that produces 
a sequence of trial outputs eventually converging to the correct output yk, 
but one can never be sure whether one has already reached it. Besides this, 
convergence is extremely slow, so this type of asymptotic computability is of 
no direct practical use. Furthermore, the replacement of ^^^ by time-limited 
versions [LV91, LV97], which is suitable for sequence prediction, fails for the 
AIXI model. This leads to the issues addressed next. 
Time bounds and effectiveness. Let p be a policy that calculates an ac-
ceptable output within a reasonable time t per interaction cycle. This sort of 
computability assumption, namely, that a general-purpose computer of suffi-
cient power and appropriate program is able to behave in an intelligent way, is 
the very basis of AI research. Here it is not necessary to discuss what exactly 
is meant by 'reasonable time/intelligence' and 'sufficient power'. What we are 
interested in is whether there is a computable version of the AIXI system 
that is superior or equal to any policy p with computation time per cycle of 
at most i. 
What one can realistically hope to construct is an AlXItl system of com-
putation time C'i per cycle for some constant c. The idea is to run all programs 
p of length < / :=i{p) and time <f per cycle and pick the best output in the 
sense of maximizing the universal value V^. The total computation time is c-i 
with c^2K Unfortunately, V7 cannot be used directly since this measure is it-
self only semicomputable and the approximation quality by using computable 
versions of ^^^ given a time of order c-f is crude [LV97]. On the other hand, 
we have to use a measure that converges to V^* for i,/—>(X), since we want the 
AlXItl model to converge to the AIXI model in that case. 
Valid approximations. We suggest the following solution satisfying the 
above conditions: The main idea is to consider extended chronological incre-
mental policies p, which in addition to the regular output y^ rate their own 
output with w^. The AlXItl model selects the output yk —y^ ^f ^he policy 
p with highest rating w^. Pohcy p might suggest any output y^, but it is 
not allowed to rate itself with an arbitrarily high w^ if one wants w^ to be 
a reliable criterion for selecting the best p. One must demand that no policy 
p is allowed to claim that it is better than it actually is. We define a logical 
predicate VA(p), called valid approximation, which is true if and only if p 
always satisfies w^<Vc{w<:k)^ i-e. never overrates itself. VF{yx<^k) is the ^^^-
expected future reward under policy p. Valid policies p can then be (partially) 
ordered w.r.t. their rating w^. 
The universal time-bounded AlXItZ system. In the following, we de-
scribe the algorithm p"" underlying the AIKltl system. It is essentially based 

24 
1 A Short Tour Through the Book 
on the selection of the best algorithms p^ out of the time i and length I 
bounded policies p, for which there exists a proof P of VA(p) with length 
<lp. 
1. Create all binary strings of length Ip and interpret each as a coding of a 
mathematical proof in the same formal logic system in which VA(-) has 
been formulated. Take those strings that are proofs of VA(p) for some p 
and keep the corresponding programs p. 
2. Eliminate all p of length >/. 
3. Modify the behavior of all remaining p in each cycle k as follows: Nothing 
is changed if p outputs some w^y^ within i time steps. Otherwise stop p 
and write Wk = 0 and some arbitrary yk to the output tape of p. Let P be 
the set of all those modified programs. 
4. Start first cycle: 
k:—l. 
5. Run every peV 
on extended input ^<fc, where all outputs are redirected 
to some auxiliary tape: p{yx<^k) = ^^V^-'-^^Vk' 
This step is performed 
incrementally by adding yJCk-i for k>\ 
to the input tape and continuing 
the computation of the previous cycle. 
6. Select the program p with highest rating w^: p^ :=argmaXpto^. 
7. Write yk '=y^'' to the output tape. 
8. Receive input Xk from the environment. 
9. Begin next cycle: A::=/c + l, goto step 5. 
Properties of the p* algorithm. Let p be any extended chronological (in-
cremental) policy of length £{p) <l and computation time per cycle t{p) <i, 
for which there exists a proof of VA(p) of length <lp. The algorithm p*, de-
pending on /, t and Ip but not on p, has always higher rating than any such p. 
The setup time of p* is tsetup{p*) = 0{l'p-2^^)^ 
and the computation time per 
cycle is tcycie{p*) = 0{2^-i). 
Furthermore, for t,/,/p—>oo, policy p* converges 
to the behavior of the AIXI model. 
Roughly speaking, this means that if there exists a computable solution 
to some AI problem at all, then the explicitly constructed algorithm p* is 
such a solution. This claim is quite general, but there are some limitations 
and open questions regarding the setup time, regarding the necessity that 
the policies must rate their own output, regarding true but not (efficiently) 
provable VA(p), and regarding "inconsistent" policies. 
1.8 
Discussion 
W h a t has been achieved. We suggested an elegant mathematical foun-
dation of artificial intelligence. More specifically, we developed a theory for 
rational agents acting optimally in any environment. Thereby we touched var-
ious scientific areas, including reinforcement learning, algorithmic information 

1.8 Discussion 
25 
theory, computational complexity theory, probability theory, sequential deci-
sion theory, and many more. We presented sequential decision theory in a very 
general form and unified it with Solomonoff's theory of universal induction, 
both shown to be optimal in their own domain. The resulting parameter-
free AIXI model constitutes an agent for which we gave strong arguments 
that it behaves optimally in any environment. For restricted environmental 
classes and Bayes mixtures ^ we showed that AI^ is self-optimizing and Pareto 
optimal. We discussed the choice of the horizon and motivated the use of 
non-geometric discounting of rewards. We also discussed a number of impor-
tant problem classes, including sequence prediction, strategic games, function 
minimization, and supervised learning. All in all, this shows that artificial 
intelligence can be framed by an elegant mathematical theory. Some progress 
has also been made toward an elegant computational theory of intelligence. 
AlXItl has optimal order of computation time, apart from a large multiplica-
tive constant, which we could get rid of at the expense of an (unfortunately 
even larger) additive constant. 
Comparison to other approaches. There are many other approaches to 
AI, too many to mention them all. Among the models that can learn from 
experience are the "classical" reinforcement learning algorithms like temporal 
difference learning [SB98], adaptive variants of Levin search [SZW97, Sch04], 
prediction with expert advice [CB97], market/economy-based reinforcement 
learning [Bau99, KHSOlb], etc. All these models have been implemented and 
are applicable in limited domains, often with reasonable performance. In con-
trast, AlXl{tl) behaves optimally in every (completely general) environment, 
is data efl^icient, has generalization capabilities, addresses the exploration ver-
sus exploitation problem, etc., but is computationally not feasible without 
further approximations. 
Outlook &: open questions. The major theoretical challenge is to derive 
good non-asymptotic bounds on the value or related quantities for AI^ and 
AIXI, ideally as strong as in the sequence prediction case. The major prac-
tical challenge is to scale the AI^ model down, e.g. by using more restricted 
forms of ^, like the minimum description length principle does for universal 
induction. The AlXItl model is a diflPerent, very general approach toward a 
computational model. Unfortunately, it suffers from the same large factor 2^ in 
computation time as Levin search for inversion problems [Lev73b, Lev84]. On 
the other hand, Levin search has been implemented and successfully adapted 
and applied to a variety of problems [Sch97, Sch04, SZW97], and the mul-
tiplicative constant can be eliminated as in M^* or reduced by the Godel 
machine [Sch03b]. Inspecting existing approaches suggests that, while AIXI is 
an elegant mathematical theory that seems to serve all formal needs, compu-
tational AI may be messy. For instance, special-purpose algorithms for pre-
processing inputs and postprocessing outputs are likely to be necessary in any 
efficient AI system. Another issue is that of incorporating extra knowledge. 
In principal there is no need to modify AIXI, since any prior knowledge can 

26 
1 A Short Tour Through the Book 
simply be presented as first input a^i in any format. As long as the algo-
rithm to interpret the data is of size 0(1), AIXI will "understand" the data 
after a few cycles. Another important issue is that of the training process it-
self. By a training process we mean a sequence of simple-to-complex tasks to 
solve, with the simpler ones helping in learning the more complex ones. These 
and many other conceptual, practical, and philosophical issues, including con-
current actions and perceptions, the choice of the I/O spaces, treatment of 
encrypted information, peculiarities of mortal embodied agents, the free will 
paradox, the existence of objective probabilities, the Turing test, the existence 
of efficient and elegant universal theories of intelligence related to Penrose's 
non-computable environments and Chaitin's 'number of wisdom' Q will be 
addressed later in the book. 
1.9 History & References 
Introductory textbooks. The book of Hopcroft and Ullman, and in the 
new revision coauthored by Motwani [HMUOl], is a very readable elementary 
introduction to automata theory, formal languages, and computation theory. 
The artificial intelligence book [RN95] by Russell and Norvig gives a compre-
hensive overview over AI approaches in general. For an excellent introduction 
to algorithmic information theory, Kolmogorov complexity, and Solomonoff 
induction one should consult the book of Li and Vitanyi [LV97], or the book 
of Calude [Cal02] which focuses more on algorithmic randomness. The rein-
forcement learning book by Sutton and Barto [SB98] requires no background 
knowledge, describes the key ideas, open problems, and great applications of 
this field. A tougher and more rigorous book by Bertsekas and Tsitsiklis on 
sequential decision theory provides all (convergence) proofs [BT96]. 
Algorithmic information theory. Kolmogorov [Kol65] suggested to define 
the information content of an object as the length of the shortest program 
computing a representation of it. Solomonoff [Sol64] invented the closely re-
lated universal prior probability distribution and used it for binary sequence 
prediction [Sol64, Sol78] and function inversion and minimization [Sol86]. To-
gether with Chaitin [Cha66, Cha75], this was the invention of what is now 
called algorithmic information theory. For further literature and many ap-
plications see [LV97, Cal02]. Other interesting applications can be found in 
[Cha91, Sch99, VW98, CV03]. Related topics are the weighted majority al-
gorithm invented by Littlestone and Warmuth [LW94], universal forecasting 
by Vovk [Vov92], Levin search [Lev73b], PAC-learning introduced by Valiant 
[Val84] and minimum description length [LV92a, Ris89]. Resource-bounded 
complexity is discussed in [Dal73, Dal77, FMG92, Ko86, PF97], resource-
bounded universal probability in [LV91, LV97, Sch02b]. Implementations are 
rare and mainly due to Schmidhuber [Sch95, WS96, Sch97, SZW97, Sch03a, 

1.9 History Sz References 
27 
Sch04]. Good reviews with a philosophical touch are [LV92b, Sol97]. For an 
older general review of inductive inference see Angluin [AS83]. 
Sequential decision theory. The other ingredient in our AIXI model is 
sequential decision theory. We do not need much more than the maximum 
expected utility principle and the expectimax algorithm [Mic66, RN95]. The 
book of von Neumann and Morgenstern [NM44] might be seen as the initiation 
of game theory, which already contains the expectimax algorithm as a special 
case. If the true environmental /i is unknown, it needs to be learned with, e.g. 
the help of reinforcement learning algorithms. Existing reinforcement learning 
algorithms are [Sam59, BSA83, SutSS, Wat89, WD92, MA93, Tes94, BT96, 
KLM96, KLC98, WS98, KS98], but they are rather limited in view of AIXI. 
The literature on reinforcement learning and sequential decision theory is so 
vast that we refer to the textbooks [SB98, BT96, KV86] for further references. 
The author's contributions. Many of the issues addressed in this book 
can already be found scattered in various reports and publications by the au-
thor: The AIXI model was first introduced and discussed in March 2000 in 
[HutOO] in a 62-page-long report. More succinct descriptions were published in 
[HutOld, HutOle]. The AIXI model has been argued to formally solve a num-
ber of problem classes, including sequence prediction, strategic games, func-
tion minimization, reinforcement and supervised learning [HutOO]. The gener-
alization AI^ has recently been shown to be self-optimizing and Pareto opti-
mal [Hut02b]. The construction of a general fastest algorithm (within a factor 
of 5) for all well-defined problems [Hut02a] arose from the construction of 
the time-bounded AlXltl model [HutOO, HutOld]. Convergence [Hut03b] and 
tight [Hut03c] error [HutOlc, HutOla] and loss [HutOlb, Hut03a] bounds for 
Solomonoff's universal sequence prediction scheme have been proven. These 
and other papers are available at http://www.idsia.ch/~marcus/ai. 


Nulla pluralitas est ponenda nisi per rationem vel ex-
perientiam vel auctoritatem illius, qui non potest falli 
nee errare, potest convinci. 
A plurality should only be postulated if there is some 
good reason, experience, or infallible authority for it. 
William of 
— William of Ockham 
Ockham 
(1285-1349) 
2 Simplicity & Uncertainty 
This chapter deals with the question of how to make predictions in unknown 
environments. Following a brief description of important philosophical atti-
tudes regarding inductive reasoning and inference, we describe more accu-
rately what we mean by induction, and motivate why we can focus on sequence 
prediction tasks. The most important concept is Occam's razor (simplicity) 

30 
2 Simplicity & Uncertainty 
principle. Indeed, one can show that the best way to make predictions is based 
on the shortest (= simplest) description of the data sequence seen so far. The 
most general effective descriptions can be obtained with the help of general 
recursive functions, or equivalently by using programs on Turing machines, 
especially on the universal Turing machine. The length of the shortest pro-
gram describing the data is called the Kolmogorov complexity of the data. 
Unfortunately, the Kolmogorov complexity is not finitely computable, which 
makes it necessary to introduce several weaker computabihty concepts. Prob-
ability theory is needed to deal with uncertainty. The environment may be a 
stochastic process (e.g. gambling houses or quantum physics), which can be 
described by "objective" probabilities. But also uncertain knowledge about 
the environment, which leads to beliefs about it, can be modeled by "sub-
jective" probabilities. The old question left open by subjectivists of how to 
choose the a priori probabilities is solved by Solomonoff's universal prior, 
which is closely related to Kolmogorov complexity. Solomonoff's major result 
is that the universal (subjective) posterior converges to the true (objective) 
environment (probability) fi. The only assumption on /x is that /i (which needs 
not be known!) is computable. The problem of the unknown environment /i is 
hence solved for all problems of inductive type, like sequence prediction and 
classification. Finally, we show the (non)existence of universal priors for the 
other introduced computabihty concepts. 
For a slower and more detailed introduction into Kolmogorov complexity 
and Solomonoff induction and most proofs one should consult the excellent 
book of Li and Vitanyi [LV97]. 
2.1 Introduction 
One very important and nontrivial aspect of intelligence is inductive inference. 
After discussing some examples we present the philosophical foundations, and 
thereafter the sequential setup we are interested in. 
2.1.1 Examples of Induction Problems 
What is the probability that the sun will rise tomorrow? Several answers 
come into mind: The probability is undefined, because there has never been 
an experiment that tested the existence of the sun tomorrow (reference class 
problem). The probability is 1, because in all experiments in the past the sun 
rose. The probability is 1 —e, where e ^ 1 is the proportion of stars in the 
universe that explode in a supernova per day. The probability can be derived 
from the type, age, size and temperature of the sun, even though we never 
have observed another star with exactly these properties. The probability is 
^ ^ , where d is the number of past days the sun rose (Laplace' rule, see 
Problem 2.11). 

2.1 Introduction 
31 
Another example is extending binary sequences, like 1100100100001111-
1101101010100.... The sequence looks random, so likely also its continuation. 
A closer look reveals that the sequence is the binary expansion of TT, SO we 
are probably better off predicting its continuation 010001.... We prefer answer 
010001..., since we see more structure in the sequence than just random digits. 
As another example, consider number sequences xi,X2,a;3,X4,..., like 
1,2,3,4,... of IQ tests. Virtually everybody predicts x^^b 
as the next num-
ber since Xi — i for i = 1...4, but x^, = 29 could also be argued for since 
x^ = i^ —10z^ + 35i^—49i + 24. We prefer answer 5 since a linear relation in-
volves less arbitrary parameters than a 4*^-order polynomial. More difficult is 
2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,?. The next number may be 61 
since this is the next prime, or 60 since this is the order of the next simple 
group. Most will answer 61, since primes are a more familiar concept than 
simple groups (see [Slo04] for an encyclopedia of integer sequences). 
The examples above demonstrate that finding prediction rules for every 
particular (new) problem is cumbersome and prone to disagreement or con-
tradiction. What we need is a formal general theory for prediction. 
2.1.2 
Ockham, Epicurus, Hume, Bayes, SolomonofF 
Generally speaking, induction is the process of predicting the future from the 
past, or more precisely, it is the process of finding rules in (past) data and using 
these rules to guess future data. Weather prediction, stock-market forecasting, 
or continuing number series in an IQ test are nontrivial examples. Making 
good predictions plays a central role in natural and artificial intelligence in 
general, and in machine learning in particular. 
On the one hand, induction seems to happen in every day life by finding 
regularities in past observations and using them to predict the future. On the 
other hand, this procedure seems to add knowledge about the future from past 
observations. But how can we know something about the future? This dilemma 
and the induction principle in general have a long philosophical history: 
• Epicurus^ principle of multiple explanations (3427-270? B.C.) 
If more than one theory is consistent with the observations, keep all the-
ories. 
• Occam^s razor^ (simplicity) principle (1290?-1349?) 
Entities should not be multiplied beyond necessity - or - keep the simplest 
theory consistent with the observations. 
• Hume's negation of induction (1711-1776) [Hum39] 
The belief in the possibility of true induction cannot be justified rationally. 
• Bayes' rule for conditional probabilities (1702-1761) [Bay63] 
It tells us how to update our beliefs/probabilities when acquiring new 
data. 
^ Whereas William of Ockham is spelled with ckh, for some reason Occam's razor 
is usually spelled with cc. 

32 
2 Simplicity & Uncertainty 
Solomonoff [Sol64] cleverly unified the principles of Epicurus, Occam, and 
Bayes into one formal universal theory of inductive inference. Among all pos-
sible induction schemes it is the optimal method for making predictions. 
2.1.3 
Problem Setup 
Every induction problem can be phrased as a sequence prediction task. This 
is most clearly illustrated in the domain of time-series prediction. Having ob-
served data Xt at times t < n , the task is to predict the n*^ symbol Xn from se-
quence xi...Xn-i- 
Classification can also be seen as a sequence prediction task. 
The task of classifying a new instance Zn after having seen (instance,class) 
pairs (zi,ci),...,( 
) can be phrased as to predict the continuation of 
the sequence ziCi...Zn-iCn-iZn^ 
Machine learning is often concerned with 
finding the true or a predictive or a causal model based on observed data. This 
step is important for understanding 
the domain under consideration. Under-
standing is often a goal in itself, but finally the goal is to apply the model to 
make predictions. In this view, model learning is only an intermediate step. 
The direct study of predictions based on past observations without discussing 
models has been coined prequential 
approach by Dawid [Daw84] for sequence 
predictions and transductive 
inference by Vapnik [Vap99, Sec.9.1] for classifica-
tion and regression. Several difficult issues are avoided by abandoning models. 
This includes questions about model consistency, i.e. whether the true model 
can be learned, and how to separate noise from useful data [GTVOl, VV02]. 
One may even go one step further and ask why we want to make predictions. 
Usually the goal of prediction is to maximize one's profit/value, or equivalently 
to minimize one's loss. In considering only profits or losses one avoids questions 
on whether prediction algorithms converge to the best possible prediction al-
gorithm (i.e. whether they are self-tuning 
[KV86, p232,p272]). Algorithms for 
which the loss converges to the minimal possible loss are called 
self-optimizing 
[KV86, p234]. This is a weaker demand than the ability to be self-tuning, but 
is often all we really care about. The main purpose of this book is to study 
algorithms that minimize loss. Convergence of posterior probability distribu-
tions or algorithms themselves or models are only considered if this is useful 
for the ultimate goal of minimizing loss. To summarize our setup: 
• Every induction problem can be phrased as a sequence prediction task. 
Classification is a special case of sequence prediction. 
(With some tricks the other direction is also true) 
We are interested in maximizing profit or minimizing loss. 
We are not primarily interested in finding (true/predictive/causal) models 
or even in convergence of the predictor itself. 
• 
• 
^ Sequence prediction may also be phrased as a classification task by adding time 
tags and if one does not assume a random generation of instances. Predicting the 
next symbol of sequence xiX2-.'Xn-i 
is the same as trying to find the class label 
of n after having seen (instance,class) pairs (l,xi),...,(n —l,Xn-i). 

2.2 Algorithmic Information Theory 
33 
• Separating noise from data is not necessary in this setting. 
After having clarified the setup we now must delve into math before we can 
present SolomonofF's induction scheme. 
2.2 Algorithmic Information Theory 
In this section we give a very brief introduction to Kolmogorov complexity. 
For a slower, more thorough and comprehensive introduction see [LV97]. 
2.2.1 Definitions and Notation 
We write iV={1,2,3,...} for the set of natural numbers, IB* for the set of finite 
binary strings, and IB^ for the set of infinite binary sequences. We use letters 
i,fc,n for natural numbers, x^y^z for finite strings, e for the empty string, 1^ 
the string of n ones, i{x) for the length of string a:, and u for infinite strings. 
We write xy for the concatenation of string x with y. 
Every countable set may be identified with IN by means of a bijection. 
We can interpret a string as a binary representation of a natural number. 
Unfortunately, a naive identification will not be unique since, for instance, 
strings 00101 and 101 both represent the number 5. We get a bijection if we 
map X to the natural number that has binary representation Ix {x prefixed 
with 1). We subtract 1 from this number, since we need a bijection between IB* 
and Wo::= {0,1,2,3,...} (see Table 2.2). With this identification log2(x+l)-l< 
£{x) <log2(a:-f-l). String x is called a (proper) prefix of y if there is a z{^e) 
such that xz = y. A set of strings is called prefix-free if no element is a proper 
prefix of another. A prefix-free set V is also called a prefix code. Prefix codes 
have the important property of satisfying Kraft's inequality 
^ 2 - ^ ^ ^ ^ < 1 . 
(2.1) 
xev 
This can be shown by assigning to each x G P an interval F^ :— [0.x ,0.a: + 
2-^(^))C [0,1), where 0 ,x=X'2 ^^^^ is the real number with binary expansion x 
after the comma. The length of interval Fx is 2~^^^\ The intervals are disjoint, 
since V is prefix free, hence ^^^^^2"^^^^ = j^^^plength(ra;)<length([0,l]) = l. 
A converse of (2.1) can also be shown. 
For x:—l^^^^Ox the set {x:xEiB*} forms a prefix code with-£(x) = 2£(x)-fl. 
For x':=l{x)x 
= \^^^^^^'^{)i,{x)x the set {x':xeiB*} forms an asymptotically 
shorter prefix code with £(x') = ^(x) + 2£(£(a:)) + l (see Table 2.2). We pair 
strings x and y (and z) by {x,y) :=x'y (and {x,y,z) :=x'y'z), which are uniquely 
decodable, since x' and y' are prefix. Since ' serves as a separator, we also write 
f{x^y) instead of f{x^y) for functions /. 
We abbreviate limn-^oo[/(^) — ^(^)] = 0 by /(n) ^-III^ g(^ri) and say / 
converges to ^, without implying that lim„_^oo^(^) itself exists. We write 

34 
2 Simplicity & Uncertainty 
f{n)^g{n) 
and say that / is asymptotically proportional to ^ if 30<c<oo: 
limn-^oo/(^)/^(^) = c. We write a^h if a is not much larger than &, with preci-
sion left unspecified. The big-O notation f{x) = 0{g{x)) means that there are 
constants c and xo>0 such that \f{x)\<c\g{x)\^x>XQ. 
The small-o notation 
f{x) — o{g{x)) abbreviates lima:_,oo/(^)/5'(^) = 0. We write f{x)<g{x) 
for 
f{x) — 0{g{x)) and f{x)<g{x) 
for f{x)<g{x)-\-0{l). 
Corresponding equali-
ties can be defined similarly. They hold if the corresponding inequalities hold 
in both directions. 
2.2.2 Turing Machines 
A Turing machine can be considered as an idealized form of a computer. It 
consists of tapes (memory), read/write heads, a table of rules (program), and 
an internal state (instruction pointer). A formal definition can be found in 
any textbook on computability theory, e.g. [HMUOl]. The set of partial recur-
sive functions coincides with the set of functions computable with a Turing 
machine. We say that a set of objects 5 = {01,02,03,...} can be (effectively) 
enumerated if there is a Turing machine mapping i to (o^), where () is some 
default coding of the elements in S. 
The importance of partial recursive functions and Turing machines stems 
from the following theses: 
We need to supplement Turing's and Church's theses in the following way: 

2.2 Algorithmic Information Theory 
35 
This means that the difference of the size of the shortest Fl description and 
the shortest F2 description (of something) is not only bounded by a universal 
constant, but that this constant is also reasonably small for natural formal 
systems. It is easy to formally convert the interpreter into a compiler by 
attaching the interpreter to the program to be interpreted and by "selling" 
the result as a compiled version. 
This extends Church's and Turing's theses in two respects. First, it says 
that the equivalence is effective, i.e. that there exists one program (in-
terpreter/compiler) that effectively converts F l programs to F2 programs. 
Church's & Turing's theses only state that the classes of computable func-
tions coincide, leaving open the possibility that there is no effective way of 
transformation. Second, and more important, the extended thesis states that 
the compiler is short if both formal systems are natural. 
The above theses cannot be proven true or false, since human, reason-
able, intuitive, and natural have not been defined rigorously. One may define 
intuitively computable as Turing computable and a natural Turing-equivalent 
system as one which has a small (say < 10^ bits) interpreter/compiler on a 
once and for all agreed-upon fixed reference universal Turing machine. The 
theses would then be that these definitions are reasonable. 
For technical reasons we need the following variants of a Turing machine. 
The table of rules of a Turing machine T can be encoded in a canonical way 
as a binary string, which we denote by (T). Hence, the set of Turing ma-
chines {Ti,T2,...} can be effectively enumerated. There are so-called universal 

36 
2 Simplicity & Uncertainty 
Turing machines that can "simulate" all other Turing machines. We define a 
particular one below, which also allows for side information y. 
We call this particular U the reference universal Turing machine. Note that 
for p not of the form yH'q, U{p) does not halt. In case of no side information 
y — e^ we suppress in the following the initial y' = e' = 0 in the codes. We 
also drop the adjunct 'prefix/monotone' if clear from the context and identify 
objects with their coding (), i.e. we omit the (). The price we have to pay 
for the existence of a universal Turing machine is the undecidability of the 
halting problem [Tur36]: There is no TM T with Vi,p [T{i'p)^l^Ti{p) 
does 
not halt]. Assume such a TM exists, then R{i):—T{i'i) 
is computable, hence 
3j:Tj=R, 
hence R{j)=T{j'j) 
= l <^ Tj{j) = R{j) does not halt, which is a 
contradiction. 
2.2.3 Kolraogorov Complexity 
In order to exploit Occam's razor beyond intuition we need to formalize the 
concept of simplicity and/or complexity. We first discuss the case of zero 
background knowledge y=€. Intuitively, a string is simple if it can be described 
in a few words, like "the string of one million ones", and is complex if there is 
no such short description, like for a random string whose shortest description 
is specifying it bit by bit. We are only interested in descriptions or codes 
that are effective and hence restrict the decoders to Turing machines. We 
say that (program) p is a description of string x relative to the prefix Turing 
machine T if T{p) —x. The length of the shortest description is denoted by 
KT{X) :=minp{^(p) '.T{p) = x]. This complexity measure depends on T, and 
one may ask whether there exists a Turing machine which leads to shortest 
codes among all Turing machines for all x. Remarkably, there exists a Turing 
machine (the universal one) which "nearly" has this property. If p is the 
shortest description of x under T = Ti, then i^p is a description of x under U, 
hence 
Ku{x)<KTix) 
+ CTu 
(2.8) 
with CTu=^{i^)j ^nd similarly for other choices of universal Turing machines. 
The length of the shortest description of x under U is at most a constant 
number of bits longer than the shortest description under T. The statement 
and proof of this invariance theorem in [Sol64, Kol65, Cha69] is often regarded 
as the birth of algorithmic information theory. Furthermore, for each pair of 

2.2 Algorithmic Information Theory 
37 
universal Turing machines C/' and [/'' satisfying the invariance theorem the 
complexities coincide up to an additive constant 
{\Ku'{x)—Kw{x)\<cu'U")' 
Since cww 
is essentially a compiler/interpreter constant, we recall As-
sumption 2.5 and interpret the assumption as cu^v being small for natural 
universal Turing machines U^ and U^\ Henceforth we write 0(1) for terms like 
cu'U" that only depend on the choice of universal Turing machines, but which 
are independent of the strings under consideration. We extend the definition 
of complexity to allow for side information y. 
For general (non-string) objects (like computable functions) one can specify 
some default coding and define X(object) :=iir((object)), especially for num-
bers and pairs, e.g. we abbreviate K{x^y) := K{{x,y)) = K{x'y). 
The most 
important information-theoretic properties of K are listed below. 
All (in)equalities remain valid if K is (further) conditioned under some z, i.e. 
K{...)-^ 
K{...\z) and K{...\y)-^K{...\y^z). 
Those stated are all valid within 
an additive constant of size 0(1), but there are others that are only valid to 
logarithmic accuracy. K has many properties in common with Shannon en-
tropy as it should be, since both measure the information content of a string. 
Property (z) gives an upper bound on K, and property (ii) is Kraft's inequal-
ity which implies a lower bound on K valid for 'most' n, where 'most' means 
that there are only o{N) exceptions for nG{l,...,A^} (Figure 2.11). Providing 
side information y can never increase code length, requiring extra information 
y can never decrease code length (iii). Coding x and y separately never helps 
(^^'), and transforming x does not increase its information content (vi). Prop-
erty (vi) also shows that if x codes some object o, switching from one coding 

38 
2 Simplicity & Uncertainty 
log(x)+21og(log(x)l 
Figure 2.11 (Kolmogorov Complexity) Schematic graph of prefix Kol-
mogorov complexity K{x) with string x interpreted as integer. K{x) >x for 
'most' X and K(a::) <log2a:+21og2logx+c for all x for sufficiently large constant 
c. 
scheme to another by means of a recursive bijection / leaves K unchanged 
within additive 0(1) terms. The first nontrivial result is the symmetry of in-
formation (t;), which is the analogue of the chain rule (see below). Property 
(vii) is at the heart of the MDL principle [Ris89], which approximates K{x) 
by-log2P(x) + K(P). 
All upper bounds on K{z) are easily proven by devising some (eff'ective) 
code for z of the length of the right-hand side of the inequality and by noting 
that K{z) is the length of the shortest code among all possible effective codes. 
For instance, if T^^ with io = 0(l) is a Turing machine with TiQ(e'x')=x, then 
i7(e'i^x')=x; hence K{x)<£{e'i^x')^i{x')^£{x)+2log2£{x), 
which proves (i). 
In {vii) one uses the Shannon-Fano code based on probability distribution P. 
Lower bounds are usually proven by counting arguments (easy for (ii) by 
using (2.1) and harder for (v)). 
2.2.4 Computability Concepts 
We need several computability concepts weaker than can be captured by halt-
ing Turing machines. 

2.2 Algorithmic Information Theory 
39 
If / is estimable we can finitely compute an ^-approximation of / by up-
per and lower semicomputing / and terminating when differing by less than 
e. This means that there is a Turing machine that, given x and £, finitely 
computes y such that \y — f{x)\ <e. Moreover, it gives an interval estimate 
f{x)£[y—e,y-\-e]. 
An estimable integer-valued function is finitely computable 
(take any £<1). Note that if / is only approximable or semicomputable we 
can still come arbitrarily close to /(x), but we cannot devise a terminating 
algorithm that produces an s-approximation. In the case of lower/upper semi-
computability we can at least finitely compute lower/upper bounds to f{x). 
In case of approximability, the weakest computability form, even this capa-
bility is lost. In analogy to lower/upper semicomput ability, one may think of 
notions like lower/upper estimability, but they are easily shown to coincide 
with estimability. The following implications are valid: 
The major algorithmic property of i^ is: 
Co-enumerability of K is obvious from the definition of K. Non-computability 
follows from a diagonalization argument: Assume K is computable. Then 

40 
2 Simplicity & Uncertainty 
f{m) :— min{n : K{n) > m] exists by Theorem 2.10(ii) and is computable, 
K{f{m)) 
> m by definition of /, and K{f{m)) 
t K{m) + K{f) t 21og2m by 
Theorem 2.10(i,fz). Hence, m < log2m + c for some c, but this is false for 
sufficiently large m. 
In the following we use the term computable synonymous to finitely com-
putable, but sometimes also generically for some of the computability forms of 
Definition 2.12. What we call estimable is often just called computable, but it 
makes sense to separate the concepts of finite computability and estimability 
here, since the former is conceptually easier. 
2.3 Uncertainty &: Probabilities 
The aim of probability theory is to describe uncertainty. There are various 
sources for uncertainty and hence various interpretations of probabilities. 
There are at least three "schools": 
• the frequentist probabilities are relative frequencies. 
(e.g. the relative frequency of tossing head) 
• the objectivist: probabilities are real aspects of the world. 
(e.g. the probability that some atom decays in the next hour) 
• the subjectivist: probabilities describe an agent's degree of belief in some-
thing, (e.g. it is (im)plausible that ETs exist) 
The following subsections describe these interpretations and discuss ap-
proaches to obtain prior probabilities. 
Remark. In some communities the domain of applicability and the correct 
interpretation and form of probability theory is still controversial. For those 
readers we want to emphasize that probabilities could be completely aban-
doned from the book without trivializing its goals and results. The terminology 
of subjective probabilities is used in this book for motivational and illustra-
tional purposes only. We do not rely on Cox's justification (see below), but 
give decision-theoretic justifications. Even the notion of objective probabilities 
may be abandoned by assuming deterministic environments. Some results in 
the book simplify in this case, but they keep their significance. So readers not 
believing in objective and/or subjective probabilities can still find the book 
interesting. 
2.3.1 Frequency Interpretation: Counting 
The frequentist interprets probabilities as relative frequencies. If in a sequence 
of n independent identically distributed (i.i.d.) experiments (trials) an event 
occurs k{n) times, the relative frequency of the event is k{n)/n. The limit 
limn->oo^(^)/^ is defined as the probability of the event. This was the ear-
liest mathematical definition of probabilities by Bernoulli, published in 1713 

2.3 Uncertainty & Probabilities 
41 
[Berl3]. For instance, the probability of the event head in a sequence of re-
peatedly tossing a fair coin is ~. The frequentist position is the easiest to 
grasp, but it has several shortcomings: 
• The frequentist obtains probabilities from physical processes as described 
above. To scientifically reason about probabilities one needs a mathemat-
ical theory. The problem is how to define random sequences. This is much 
more intricate than one might think and was only solved in the 1960s by 
Kolmogorov and Martin-Lof [ML66]. 
The naive definition of probability is circular: The probability of an event 
E is p:=lim.n-^oo-^~-^ ^ where kn{E) is the number of occurrences of event 
E in the first n i.i.d. trials. The problem is that the limit may be anything 
or not even exist: e.g. a fair coin can give: head, head, head, head, ... 
i.e. p=l. 
Of course, this sequence is "unlikely". For a fair coin, 
p=^ 
with "high probability". But to make this statement rigorous we need to 
formally define what "high probability" means. Here is the circularity! 
• Philosophically, and also often in real experiments, it is hard to justify 
the choice of the so-called reference class. For instance, a doctor wants to 
determine the chances that a patient has a particular disease by counting 
the frequency of the disease in "similar" patients. But if the doctor con-
sidered everything he knows about the patient (symptoms, weight, age, 
ancestory, ...) there would be no other comparable patients left. 
• The frequency approach is limited to a (sufficiently large) sample of i.i.d. 
data. 
2.3.2 
Objective Interpretation: 
Probabilities to Describe Uncertain Events 
For the objectivist probabilities are real aspects of the world. The outcome of 
an observation or an experiment is not deterministic, but involves physical 
random processes. The set i? of all possible outcomes is called the sample 
space. It is said that an event E d Q occurred if the outcome is in E. In 
the case of i.i.d. experiments the probabilities assigned to events should be 
interpret able as limiting frequencies, but the application is not limited to 
this case. The Kolmogorov axioms formalize the properties that probabilities 
should have. 

42 
2 Simplicity & Uncertainty 
The function p is called a probability mass function^ or probability measure, 
or, more loosely, probability distribution. Conditional probabilities are defined 
in the following way: 
It is easy to see that p{'\A) (as a function of the first argument) is also a 
probability measure, if p(-) satisfies the Kolmogorov axioms. One can "verify 
the correctness" of the Kolmogorov axioms and the definition of conditional 
probabilities in the case where probabilities are identified with limiting fre-
quencies. But the idea is to take the axioms as a starting point to avoid the 
frequentist's problems. The relation p{AnB)=p{B[A)-p(A) 
is called the mul-
tiplication rule (of conditional probabilities), which is a special case of the 
chain rule. 
Bayes' theorem is easily proven by applying Definition 2.15 twice. 

2.3 Uncertainty & Probabilities 
43 
2.3.3 Subjective Interpretation: 
Probabilities to Describe Degrees of Belief 
The subjectivist uses probabilities to characterize an agent's degree of belief in 
something, rather than to characterize physical random processes. This is the 
most relevant interpretation of probabilities in AI. We define the plausibility 
of an event as the degree of belief in the event, or the subjective probabil-
ity of the event. The problem with the subjective view is that it is much 
more arguable how to define plausibilities, as compared to objective prob-
abilities. The objectivist can motivate Kolmogorov's axioms by a frequency 
analysis, but there is no frequency interpretation for plausibilities. If an agent 
believes in extraterrestrians and assigns a plausibility of 0.9 to their existence, 
it does not make much sense to interpret this as "in 90 out of 100 parallel 
universes there are extraterrestrians" or "90 out of 100 similar agents believe 
in extraterrestrians". This problem has led to many different systems deal-
ing with uncertain reasoning (see reference Section 2.5). They all have their 
own problems. The most consistent and successful system is, again, based on 
Kolmogorov's axioms, although not all would agree with this statement. It 
is surprising that plausibilities follow the same rules as limiting frequencies. 
It is possible to derive Kolmogorov's axioms from a few plausible qualitative 
rules they should follow. It is natural to assume that plausibilities can be rep-
resented by real numbers, that the rules qualitatively correspond to common 
sense, and that the rules are mathematically consistent. Cox [Cox46] starts 
with the following (natural) assumptions on beliefs: 
Cox [Cox46] shows that every function Bel(-|-) satisfying these axioms is iso-
morphic to a (conditional) probability function. One can motivate the func-
tional relationship in Cox's axioms by analyzing all other possibilities and 
showing that they violate common sense [Tri69]. The somewhat strong dif-
ferentiability assumptions can be weakened to more natural continuity and 
monotonicity assumptions [Acz66]. Only recently, a loophole in Cox's and 
other's derivations has been exhibited [Par95]. Several fixes have been sug-
gested by making additional assumptions. Most of them require the range of 
Bel, and hence the set of events, to be rich enough. We paraphrase these as 
"additional denseness conditions." 

44 
2 Simplicity & Uncertainty 
Cox's result has attracted a great deal of interest, particularly in the maxi-
mum entropy and AI community. The qualitative motivation of Cox's axioms 
and the derivation of Cox's theorem from them is the major theoretical justi-
fication that subjective 'degrees of belief should satisfy the same Kolmogorov 
axioms as limiting frequencies. Other approaches to beliefs are missing this 
strong theoretical foundation and consistency. 
Exploiting that subjective probabilities follow the same rules as objective 
probabilities, we can present Bayes' rule in a particular useful form: how to 
update beliefs under new observations. See Problem 2.10 for a typical appli-
cation. 
Proof. The proof is based on all and only Axioms 2.14. 
p{AuB)=p{A)-j-p{B) 
if A n 5 = {}, since p{{}) — 0. For finite /, by induction, this implies 
J2ieiP(^i) 
= P ( U ^ i ) =Pi^) = 1- For countably infinite / = {1,2,3,...} with 
Sn:=[j'iLn^i 
we have: 
. n - l 
ri— 1 
/ ri— 1. 
\ 
Y,v{H,)^p{Sr^) 
= p( [ j F . U S n ) =p(/?) = l. 
(2.20) 
Exploiting ^Si D 5*2 D S'3..., for any a;G i? we have: 3n\uj^Hn 
=^ 
UJ^Hiii>n 
^ uj ^ Siii > n => u) ^ C\n^ri =^ C\n^ri = {} siucc uo was arbitrary ^ taking 
n-^ 00 in (2.20) shows Yl'iLiPiHi) ~ 1. Since conditional probabilities also 
satisfy Axioms 2.14, we also have ^^^jp{Hi\D) 
= l (for finite as well as infinite 
/). By Definition 2.15 of conditional probability we have 
p{Hi\D)p{D) = p{Hi nD)= 
p{D\Hi)p{Hi). 
Summing over all hypotheses Hi gives 

2.4 Algorithmic Probability & Universal Induction 
45 
Y,P{D\Hi)p{Hi) = J2pmD)-piD) 
= l-p{D) 
p{D\Hi)p{Hi) 
_ 
p{D\Hi)p{Hi) 
p{Hi\D) 
P{D) 
ZieiPi^l^MHi. 
D 
2.3.4 Determining Priors 
The Kolmogorov Axioms 2.14 of probability allow us to relate probabilities 
and plausibilities of different events, but they do not uniquely fix a numerical 
value for each event, except for the sure event i? and the empty event {}. We 
need new principles for determining values for at least some basis events from 
which others can then be computed by these Axioms. There seem to be only 
three general principles: 
• the principle of indifference — the symmetry principle, 
• the maximum entropy principle, 
• Occam's razor — the simplicity principle. 
Whereas the first two principles are based on the foundations of statistical 
physics, we will see that only Occam's razor (keep only the simplest consistent 
hypothesis) in combination with Epicurus' principle of multiple explanations 
(keep all consistent hypotheses) is general enough to assign prior probabilities 
in every situation, especially in the case of induction and other domains typical 
for AL The idea is to assign high (subjective) probability to simple events, and 
low probability to complex events: Simple events (strings) are more plausible 
a priori than complex ones. This gives (approximate) justice to both Occam's 
razor and Epicurus' principle. In the following we also refer to this general 
idea as Occam's razor. Using K for measuring simplicity/complexity leads to 
Solomonoff's universal prior M. In the next section we pursue this approach. 
2.4 Algorithmic Probability & Universal Induction 
In addition to the notation introduced in Section 2.2.1, we denote binary 
strings of length n by x—XiX2...Xn with Xt^JB, and further abbreviate Xi:n '-= 
XiX2'-'Xn-lXn 
a n d 
X^ri''^^l'"Xn-l^ 
2.4.1 The Universal Prior M 
The prefix Kolmogorov complexity K{x) has been defined as the shortest 
program p for which the universal prefix Turing machine U outputs string a;, 
and similarly K[x\y) in case of side information y (Definition 2.9). Solomonoff 
[Sol64, Sol78] defined a closely related quantity, the universal prior M{x). 

46 
2 Simplicity & Uncertainty 
The universal prior is defined as the probability that the output of a uni-
versal monotone Turing machine starts with x when provided with fair coin 
flips on the input tape. Formally, M can be defined as 
M{x) := 
Y^ 2-^^P\ 
(2.21) 
p : U{p)=x^ 
where the sum is over minimal programs p for which U outputs a string 
starting with x (see Definition 2.6). Since the shortest programs p dominate 
the sum, M(x) is roughly 2-^^^) (M(x) = 2-^^^)+^(^(^(^))). 
Before we can discuss the stochastic properties of M we need the concept 
of (semi)measures for strings. 
The reason for calling fi with the above property a probabihty measure is that 
it satisfies Kolmogorov's Axioms 2.14 of probability in the following sense: The 
sample space is IB^ with elements uj = uJiiJ2u;3--'GlB^ being infinite binary 
sequences. The set of events (the a-algebra) is defined as the set generated 
from the cylinder sets Fx-^.^ -={(^'Cdi:n — Xi:n} by countable union and com-
plement. A probability measure /i is uniquely defined by giving its values 
f^{^xi:n) ^^ th^ cylinder sets, which we abbreviate by fi{xi:n)- We will also 
call /i a measure, or even more loosely a probability distribution. 
The reason for extending the definition to semimeasures is that M itself 
is unfortunately not a probability measure. We have M(xO)-\-M(xl) 
< M(x) 
because there are programs p that output x, followed neither by 0 nor 1. 
They just stop after printing x, or continue forever without any further out-
put. In Problem 2.7 the defect is quantified. Since M(e) = l, M is at least a 
semimeasure. We can now state the fundamental property of M. 
The Kolmogorov complexity of a function like p is defined as the length of the 
shortest self-delimiting code of a Turing machine computing this function in 
the sense of Definition 2.12. Up to a multiplicative constant, M assigns higher 
probability to all x than any other computable probability distribution. 
It is possible to normalize M to a true probability measure Mnorm (2.30) 
[Sol78] with dominance still being true, but at the expense of giving up enu-
merability {Mnorm is still approximable). We will see that M is more con-

2.4 Algorithmic Probability & Universal Induction 
47 
venient when studying algorithmic questions, but a true probability measure 
like Mnorm IS morc convenient when studying stochastic questions. 
2.4.2 Universal Sequence Prediction 
In which sense does M incorporate Occam's razor and Epicurus' principle 
of multiple explanations? From M{x) ^ 2"^^^^ we see that M assigns high 
probability to simple strings (Occam). More useful is to think of x as be-
ing the observed history. We see from Definition (2.21) that every program 
p consistent with history x is allowed to contribute to M (Epicurus). On 
the other hand, shorter programs give significantly larger contribution (Oc-
cam). How does all this affect prediction? If M{x) correctly describes our 
(subjective) prior belief in x, then M{y\x) :=M{xy)/M(x) 
must be our pos-
terior belief in y. From the symmetry of algorithmic information K(x,y) = 
K{y\x,K{x))-^K{x) 
(Theorem 2.10(v)), and assuming K{x,y)^K{xy), 
and 
approximating K{y\x,K{x))^K{y\x), 
M{x)^2-^^''\ 
and 
M{xy)^2-^^''y'> 
we get M{y\x)^2~^^'y\^\ 
This tells us that M predicts y with high probability 
iff y has an easy explanation, given x (Occam & Epicurus). 
The above qualitative discussion should not create the impression that 
M{x) and 2~^^^^ always lead to predictors of comparable quality. Indeed, in 
the online/incremental setting studied in this book, K[y) — 0{1) invalidates 
the consideration above. The validity of Theorem 2.25 below, for instance, 
depends on M being a semimeasure and the chain rule being exactly true, 
neither of them is satisfied by 2"^^^^ (see Problem 2.8). 
(Binary) sequence prediction algorithms try to predict the continuation 
Xn^IB of a given sequence xi.-.Xn-i- We derive the following bound: 
oo 
oo 
Y^{l-M{xt\x<t)f<-^Y^\nM{xt\x<t) 
= 
-llnM{xi:oo)<l^^^ 
(2.24) 
where the monotone complexity Km{xi:oo) is defined as the length of the 
shortest (nonhalting) program computing xi.-oo [ZL70]. In the first inequality 
we used (1 —a)^ < — |lna for 0 < a < 1. In the equality we exchanged the sum 
with the logarithm and eliminated the resulting product by the chain rule. 
In the last inequality we used M(x) >2~-^'^^^\ which follows from Defini-
tion (2.21) by dropping all terms in ^ 
except for the shortest p computing 
X. If a::i:oo is a computable sequence, then Km{xi:oo) is finite, which implies 
M(xt\x<:t) -^ 1 ( I ] ^ i ( l —Q^t)^ <oo^ 
at-^1). 
This means that if the envi-
ronment is a computable sequence (whichsoever, e.g. the digits of TT or e in 
binary representation), after having seen the first few digits, M correctly pre-
dicts the next digit with high probability, i.e. it recognizes the structure of 
the sequence. 
Assume now that the true sequence is drawn from a computable proba-
bility distribution /x, i.e. the true (objective) probability of xi:n is //(a^i.n)-

48 
2 Simplicity & Uncertainty 
The probability of Xn given x<n, hence, is ii{xn\x<^n) — /x(xi:n)//i(a;<n). 
Solomonoff's [Sol78] central result is that M converges to \i\ 
The infinite sum can only be finite if the difference M(0|x<t)—/i(0|a:<t) tends 
to zero for t-^oo with /i-probability 1 (see Definition 3.8(z) and Problem 2.7). 
This holds for any computable probability distribution \i. The reason for the 
astonishing property of a single (universal) function to converge to any com-
putable probability distribution lies in the fact that the set of //-random se-
quences differ for different //. Past data x^t are exploited to get a (with t-^oo) 
improving estimate M(xt|x<t) of/x(xt|x<t). 
The universality property (Theorem 2.23) is the central ingredient in the 
proof of Theorem 2.25. The proof of Theorem 2.23 involves the construction 
of a semimeasure ^ whose dominance is obvious. The hard part is to show 
its enumerability and equivalence to M. Let M. be the (countable) set of all 
enumerable semimeasures and define 
i{x) := ^ 
2-^^"V(x). 
(2.26) 
Then dominance 
i{x) > 2-^^^V(a:) 
WueM 
(2.27) 
is obvious (without 0(1) fudge). Is ^ lower semicomputable? To answer this 
question we have to be more precise. Levin [ZL70] showed that there is a 
Turing machine T such that for every lower semicomputable semimeasure v 
there is an i such that T(i'x^) lower semicomputes vi^y^ 
i.e. T enumerates 
all lower semicomputable semimeasures, possibly with repetition. For the (or-
dered multi) set A^=A^t/-={^i5^25^3v} and K{ui):—K{i), one can easily see 
that (^ is lower semicomputable. Finally, proving M(x)=^(x) also establishes 
universality of M. 
The advantage of ^ over M is that it immediately generalizes to arbi-
trary weighted sums of (semi)measures in Ai for arbitrary countable M.. Most 
proofs in this book go through for generic M. and weights. We will prove (this 
generalization of) Theorem 2.25 in Section 3.2.6. 
2.4.3 Universal (Semi)Measures 
What is so special about the set of all enumerable semimeasures A^t/? The 
larger we choose M^ the less restrictive is the assumption that M should 
contain the true distribution /i, which will be essential throughout the book. 
Why do not restrict to the still rather general class of estimable or finitely 
computable (semi)measures? It is clear that for every countable set A^, 

2.4 Algorithmic Probability & Universal Induction 
49 
^{x) := ^M{X) '-= YJUEM'^^^^^) 
^^^^ Zli/^i^ ^ 1 ^^d it;i, > 0 dominates all 
ueM. 
This dominance is necessary for the desired convergence ^—^/JL, sim-
ilarly to Theorem 2.25. The question is what properties does ^ possess. The 
distinguishing property o^ Mu is that ^=^U = ^MU — ^ is itself an element of 
A^[/. In this book, ^M^-^ is not by itself an important property, but whether 
^ is computable in one of the senses of Definition 2.12. We define 
Ml > M2 : ^ there is an element of A^i that dominates all elements of M2 
:<^ 3pGXi Vi^GX2 ^Wu>^^x 
: 
p(x)>w^u{x). 
Relation > is transitive (but not necessarily reflexive) in the sense that M.i> 
X
X 
X 
X 
A^2>A^3 implies Mi^M^ 
and MQ^MI>M2^M^ 
implies A^o>A^3- For 
the computability concepts introduced in Section 2.2.4, we have the following 
proper set inclusions 
KArnsr 
^ 
M^T^ 
= 
M^^^ 
C 
M'^^^ 
"^ ^comp 
^— "^ ^est 
— "^ ^enum 
^— "^ *^appr 
n 
n 
n 
n 
Kyi semi 
^ 
Kyi semi 
^ 
KAsemi 
^ 
KAsemi 
•^ ^comp 
^~ "^ *-est 
^ 
"^ *-enum 
^ 
"^ ^appr 
where MJ^^'^ stands for the set of all probability measures of appro-
priate computability type c G {comp=finitely computable, est—estimable, 
enum=enumerable, appr^approximable}, and similarly for semimeasures 
^sem* Other classes are briefly discussed in Section 3.2.9. Prom an enu-
meration of a measure p one can construct a co-enumeration by exploiting 
p(^i:n) = l —X]yi.^=^xi.„/^(^i:'^)- '^^^^ shows that cvcry enumerable measure is 
also co-enumerable, and hence estimable, which proves the identity = above. 
With this notation, [ZL70, Thm.3.3] reads Mf^i^^MfZL- 
Transitivity 
allows to conclude, for instance, that M^^^^ ^^Tomp^ 
i-^- ^^at there is an 
approximable semimeasure that dominates all computable measures. 
The standard "diagonahzation" way of proving Mi ^M2 is to take an 
arbitrary pGMi 
and "increase" it to p such that p^p and show that pG 
M2' There are 7x7 combinations of (semi)measures Mi with M2 for which 
X 
Mi>M2 
could be true or false. There are four basic cases, explicated in 
the following theorem, from which the other 49 combinations displayed in 
Table 2.29 follow by transitivity. 

50 
2 Simplicity & Uncertainty 
If we ask for a universal (semi)measure that at least satisfies the weakest 
form of computability, namely being approximable, we see that the largest 
dominated set among the seven sets defined above is the set of enumerable 
semimeasures. This is the reason why M^^^ 
plays a special role in this (and 
other) works. On the other hand, M-l^ln 
i^ ^^^ ^^e largest set dominated 
by an approximable semimeasure, and indeed no such largest set exists. One 
may, hence, ask for "natural" larger sets M.. One such set, namely the set 
of cumulatively enumerable semimeasures A I C E M , was recently discovered by 
Schmidhuber [SchOO, Sch02a], for which even ^ C E M ^ A ^ C E M holds. 

2.4 Algorithmic Probability & Universal Induction 
51 
Theorem 2.28 also holds for discrete (semi)measures P defined as 
« ) 
xeisf 
P : W ^ [0,1] with 
Yl ^(^) -^ 1-
We first prove the theorem for this discrete case, since it contains the es-
sential ideas in a cleaner form. We then present the proof for "continuous" 
(semi)measures ji (Definition 2.22). The proofs naturally generalize from bi-
nary to arbitrary finite alphabet. The value of a: that minimizes f{x) is denoted 
by argmina;/(a:). Ties are broken in an arbitrary but computable way (e.g. by 
taking the smallest x). 
Proof (discrete case), (o) Q{x) :=^p^j^wpP{x) 
with wp>0 
obviously 
dominates all P^M. 
(with constant wp). With X^ptfp = 1 and all P being 
discrete (semi)measures also Q is a discrete (semi)measure. 
(i) See [LV97, Thm.4.3.1]. 
(a) Let P be the universal element in A^emt^m ^^^ ^''—Y2x^(^)- ^^ normalize 
P by Q{x):=^P{x). 
Since a < l we have Q{x)>P{x), 
hence 
Q>P^Ml^i^. 
As a ratio between two enumerable functions, Q is still approximable, hence 
KAmsr ^ 
KAsemi 
-^ *-appr —-^^enum' 
{Hi) ^Let PeMll'i^i. 
We partition N into chunks /n:={2''~\...,2'^-l} (n>l) 
of increasing size. With x^ :=argmina;^7^P(x) we define Q{xn) := nin+i)^'^^ 
and Q{x) :=0 for all other x. Exploiting the fact that a minimum is smaller 
than an average, we get 
P(x„) = min Pix) < ^ J : P{X) < r ^ = ^ 
= ^ ^ | r ^ Q ( - n ) -
Xfz^n 
Since 2^'^i ^ -^ 0 for n-^oo, 
P cannot dominate Q (P^Q)- 
With P also Q 
is computable. Since P was an arbitrary computable semimeasure and Q is 
a computable measure (EQ(^) = E i ^ ^ l = E [ ^ - : ^ ] = 1) this implies 
KAsemi ^ 
KAmsr 
Assume now that there is an estimable semimeasure 5'> A^^mp- We con-
struct a finitely computable semimeasure P > S as follows. Choose an initial 
£ > 0 and finitely compute an e-approximation S of S{x). \i S>2e 
define 
P{x) \— \S^ else halve e and repeat the process. Since S{x) >0 (otherwise it 
could not dominate, e.g. T{x) := w^piy ^-^Tomp) ^^^ ^^^P terminates after 
finite time. So P is finitely computable. Inserting S = 2P{x) and €<^S = P{x) 
into \S{x)-S\<s, 
we get \S{x)-2P{x)\< 
P{x), which implies 
S{x)>P{x) 
and S{x) < 3P{x). The former implies Ex^(^) ^ Hx^i^) 
^ ^^ i-^- P is a 
The proof of Mll'!^p ^ Mllmp in [LV97, p249] contains minor errors and is not 
jsem' 
^ 
^est 
i 
KAmsr 

52 
2 Simplicity & Uncertainty 
semimeasure. The latter implies P> |*S> Al^^^. Hence P is a computable 
semimeasure dominating all computable measures, which contradicts what we 
proved in the first half of (iii). Hence the assumption on S was wrong, which 
establishes 
Mltr^M^fZp-
(iv) Assume P ^ A^ap^r ^-^^p^r- We construct an approximable measure Q 
that is not dominated by P, thus contradicting the assumption. Let Pi,P2v 
be a sequence of recursive functions converging to P. We construct xi,X2,... 
such that Vc> 0 3n e IN : P{xn) ^ c-Q{xn). For this we recursively define 
sequences x^,x^,... converging to x^, and from them Qi,Q2^'- converging 
to Q. Let In := {2^-\...,2^-1} and x^ = 2^-iVn. If Pt(4"-^) > n"^ then 
x^:=argmina^^/^Pt(x) else x^:=x^~^. We show that x^ converges for t-^oo by 
assuming the contrary and showing a contradiction. Since x\^£ln some value, 
say X*, is assumed infinitely often. Nonconvergence implies that the sequence 
leaves and returns to x* infinitely often, x* is only left (x^""^ — ^n T^^n) i^ 
Pt(x*) >n~^. On the other hand, at the time where x\^ returns to x* (x^~^ ^ 
< = xi) we have P t « ) = P t ( 4 ) = min^e/n^t(^) < \In\-^ - 2—+i. Hence 
Pt(xJ^) oscillates (for n > 12) infinitely often between < 2""^"^^ and > n~^, 
which contradicts the assumption that Pt converges. Hence the assumption 
of a nonconvergent x^ was wrong, x^ converges to x* and Pt(x*) to a value 
<n~^. With x^ also the measure Qt{xl^) := ^.J, j^x (and Qt(^) ==0 for all other 
x) converges. Since P(x*) <n~^ does not dominate Q(x*), we have P%Q. 
Since P G M^^^ 
was arbitrary and Q is an approximable measure we get 
X^semi^K^msr 
• 
Proof (continuous case). The major difference from the discrete case is 
that one also has to take care that p{x) > p{xO)-^p{xl)^ xelB*^ is respected. 
On the other hand, the chunking In .—IB^ is more natural here. 
(o) p{x) := YlueM^^^(^) 
^^^^ Wjy>0 obviously dominates all u e M (with 
domination constant Wj^). With Ylu^i^ — ^ ^^^ ^^^ ^ being (semi)measures also 
p is a (semi)measure. 
(i) See [LV97, Th4.5.1]. 
{it) Let ^ be a universal element in Ml^^. 
We define [Sol78] 
By induction one can show that ^norm is a measure and that ^normix) > 
^(x)Vx, hence ^norm^C^Ml^^. 
As a ratio of enumerable functions, (,norm 
is still approximable, hence A^^p'^r ^-^In^m-
(iii) ^Let /i e A^comp- We recursively define the sequence xl.^ 
by x^ := 
SLrgmirix^/j^ix'^f^Xk) and the measure p by p{xl.j^) = l\fk and p(a:)=0 for all x 
^ The proof in [LV97, p276] only applies to infinite alphabet and not to the bi-
nary/finite case considered here. 

2.4 Algorithmic Probability &; Universal Induction 
53 
that are not prefixes of Xj.^. Exploiting the fact that a minimum is smaller 
than an average and that /x is a semimeasure, we get 
Hence M(^i:n)^(i)"' —(|)"'P(^i:n)' which demonstrates that fi does not dom-
inate p. Since /i G M-clmp ^^^ arbitrary and p is a computable measure this 
implies A/f-^^^^ $A^^^^ 
Assume now that there is an estimable semimeasure C J - > A ^ ^ ^ 
We con-
struct a finitely computable function /i > a as follows. Choose an initial 
£ > 0 and finitely compute an s-approximation a of (T{X). If a > 4e define 
lJi{x) := ^, else halve e and repeat the process. Since a{x) > 0 (otherwise it 
could not dominate, e.g. 2~^^^^) the loop terminates after finite time. So // is 
finitely computable. Inserting a = fi{x) and £<ja= 
|/i(x) into |cr(x) —6-| <€ 
we get \a{x) — (i{x)\ < ;|/i(x), which implies ^(Ji{x) <a{x) < jfi{x). 
Unfor-
tunately /x is not a semimeasure, but it still satisfies the weaker inequality 
/j,{xO)-^fi{xl) < |[(j(a:0) + cr(xl)] < ^a{x) < | - | M ( X ) = |M(X). This is suffi-
cient for the first half of the proof of {in) to go through with | replaced 
by I ' l = I < 1? which shows that /i £ -^^mp- However, this contradicts 
/^ ^ |<^ > -M^mp showing that our assumed estimable semimeasure a does 
notexist, i.e. A ^ - r l X ^ o m p -
(iv) Assume /i G M^^^^ > M^^^^. We construct an approximable measure 
p which is not dominated by p, thus contradicting the assumption. Let 
/ii,/[X2v be a sequence of recursive functions converging to p. We recur-
sively (in t and n) define sequences y\,y^^...,yl^ converging to yn and from 
them pi,P2v converging to p. Let y^ =OVn. If ^t(y^^y^~^)> |/it(y^^), then 
1/^r^argmin^EnA^t(l/<n^n), else y^:=y^~^'We show that ^/n converges for t-^oo 
by assuming the contrary and showing a contradiction. Assume that k is the 
smallest n for which yl^-/^yn> Since y^ ^^?/„ for all n<k and y^^IB 
is dis-
crete there is a ^o such that y%k = y<k'^t>to. Assume t>to in the following. 
Since yl.E]B^ some value, say y^, is assumed infinitely often. Nonconvergence 
implies that the sequence leaves and enters to yk infinitely often. If yk is left 
{yi~^='yk¥=yk)^ we have 
P't{y<kyk) = My<kyi~^) 
> l^t{y<k) 
= li^t{y<k) ^^^ 
lKy<k)' 
liyk is entered (yl~^ y^yf^=yl) we have 
f^t{y<kyk) = f^t{y<kyk) = mmfit{yi:kXk) 
< |[pt(2/<fcO)+pt(2/<^i)] < 
^k 
< \i^t{y^<k) = \i^t{y<k) ^^^ 
\i^{y<k)' 
Hence lJit{y<kyk) oscillates infinitely often between > |p(2/<fc) and < 
\ii{y^k), 
which contradicts the assumption that /it converges. Hence the assumption 

54 
2 Simplicity & Uncertainty 
of a nonconvergent y^ was wrong. With y^ also the measure Pt{yi:n) •— 1 
(and pt{x) = 0 for all other a; that are not prefixes of i/i-oo) converges. For all 
sufficiently large t we have yi-.n^Vi-.n^ hence fitiyi-.n) = Mvin) 
< l^tiy^n) 
< 
...<(|)^. Since /i(yi:n)<(|)"^ docs not dominate p{yi:n) = ^ (Vt>^o)? we have 
P£p. Since p G M^^^ 
was arbitrary and p is an approximable measure, we 
2.4.4 Martin-L6f Randomness 
Martin-Lof randomness is a very important concept of randomness of in-
dividual sequences that is closely related to Kolmogorov complexity and 
Solomonoff's universal prior. Since we refer to this concept only in Sec-
tion 3.2.7 we will be very brief here. We give a characterization equivalent 
to Martin-Lof's original definition, in order to bypass the necessity of giving 
a formal definition of 'effective randomness tests' [Lev73a]: 
An equivalent formulation for computable p is: 
^i:oo is /i.M.L.-random 
<^ 
Km{xi-n) — —Xog^P^^v.n) Vn, 
(2.32) 
where Km{x\.n) is the length of the shortest (possibly nonhalting) program 
computing a string starting with Xi-.^. Theorem 2.31 follows from (2.32) by 
exponentiation, "using 2~^"^?^M" and noting that M>p follows from univer-
sality of M. Consider the special case of p being a fair coin, i.e. p{xi:n) = 2~'^, 
then a;i:oo is M.L. random iff Km{xi:n)—n, 
i.e. if xi.n is incompressible. For 
general /i, —\og2p{xi:n) is the length of the Shannon-Fano code of Xi:n, hence 
Xi:oo is /i.M.L.-random iff the Shannon-Fano code is optimal. 
One can show that a /i.M.L.-random sequence xi:oo passes all thinkable 
effective randomness tests, e.g. the law of large numbers, the law of the iterated 
logarithm, etc. In particular, the set of all //.M.L.-random sequences has p-
measure 1. The following generalization is natural when considering general 
Bayes mixtures (: 
Typically, ^ is a mixture over some M as defined in (2.26), in which case the 
reverse inequality ^{x)>p{x) 
is also true (for all x). For finite A^ or if ^G A^, 
the definition of /i/^-randomness depends only on M, and not on the specific 
weights used in ^. For M=Mui 
/i/<f-randomness is just //.M.L.-randomness. 

2.5 History &: References 
55 
The larger M, the more patterns are recognized as nonrandom. Roughly 
speaking, those regularities characterized by some z^ G A^ are recognized by 
/x/^-randomness, i.e. for McMu 
some /i/^-random strings may not be M.L. 
random. Other randomness concepts, e.g. those by Schnorr, Ko, van Lambal-
gen, Lutz, Kurtz, von Mises, Wald, and Church (see [Wan96, Lam87, SchTl]), 
could possibly also be characterized in terms of jii/^-randomness for particular 
choices of M. 
2.5 History &; References 
Most notation is taken over from [LV97]. The general theory of coding and 
prefix codes can be found in [Gal68], the important Kraft inequality is due to 
Kraft [Kra49]. 
Algorithmic information theory. Turing introduced the concept of a Tur-
ing machine and demonstrated that the halting problem is undecidable in 
[Tur36]. Turing machines are formally equivalent to partial recursive func-
tions (see [Rog67, Odi89, Odi99] for an introduction). The halting problem 
corresponds to Godel's incompleteness theorem [GodSl, Sho67] whose proof is 
based on a diagonal argument invented by Cantor [Can74, Dau90]. The short 
compiler Assumption 2.5 is an effective version of Kolmogorov's assumption 
that complexities based on different "reasonable" universal "Turing" machines 
coincide reasonably well [Kol65]. The works [God31, Kle36, Tur36, Pos44, 
ZL70, Sch02a] show the importance of the various computability concepts de-
fined in (2.12). The consideration of (and naming for) estimable functions in 
the context of universal priors is from [Hut03b]. 
A coarse picture of the early history of algorithmic information theory 
could be drawn as follows: Kolmogorov [Kol65] and Chaitin [Cha66, Cha69], 
suggested defining the information content of an object as the length of the 
shortest program computing a representation of it. Solomonoff [Sol64] inde-
pendently invented the closely related universal prior probability distribution 
and used it for binary sequence prediction [Sol64, Sol78]. Levin worked out 
most of the mathematical details [ZL70, Lev74] and invented the fastest al-
gorithm for function inversion and optimization, save for a (huge) constant 
factor [Lev73b]. These papers may be regarded as the invention of what is 
now called algorithmic information theory. The invariance (2.8) is due to 
[Sol64, Kol65, Cha69], Theorem 2.10(^ii) is due to [Lev74], the symmetry 
of information (v) due to [ZL70, Gac74, Kol83], (ii) is due to [Lev74], the 
other parts are elementary. 
The short introduction we gave in this chapter necessarily described only 
the key ideas, ignoring many related and especially newer developments. Some 
references are given in the following. 
There are many variants of "Kolmogorov" complexity. The prefix Kol-
mogorov complexity K we defined here [Lev74, Gac74, Cha75], the earhest 

56 
2 Simplicity & Uncertainty 
form, "plain" Kolmogorov complexity C [Kol65], process complexity [Sch73], 
monotone complexity Km [Lev73a], and uniform complexity [Lov69b, Lov69a], 
Solomonoff's universal prior M = 2~^^ 
[Sol64, Sol78], Chaitin's complexity 
Kc [Cha75], extension semimeasure Mc [Cov74], and some others. They often 
differ from K only by O(logK), but otherwise have similar properties. For 
an introduction to Shannon's information theory [Sha48] and its relation to 
Kolmogorov complexity, see [Kol65, Kol83, ZL70, CT91]. 
The main drawback of all these variants of Kolmogorov complexity is that 
they are not finitely computable [Kol65, Sol64]. They may be approximated 
from above [Kol65, Sol64], but no accuracy guarantee can be given, and what 
is worse, the best upper bound for the runtime until one has reasonable ac-
curacy for K{n) grows faster than any computable function in n. This led 
to the development of time-bounded complexity/probability that is finitely 
computable, or more general resource-bounded complexity/probability (e.g. 
space) [Dal73, Dal77, FMG92, K086, PF97, Sch02b]. 
For an excellent introduction to algorithmic information theory, and a 
more accurate treatment of its history (more than 500 references), and many 
applications one should consult the authoritative book of Li and Vitanyi 
[LV97]. 
Foundations of probability theory. Although games of chance date back 
at least to around 300 B.C., the first mathematical analysis of probabilities 
appears to be much later. Important breakthroughs have been achieved (in 
chronological order and with significant simplification) by Cardano [Car63], 
a systematic way of calculating probabilities by Pascal (in correspondence 
with Fermat) and conditional probability [Pas54], Bayes' rule [Bay63], the 
distinction between subjective and objective interpretation of probabilities 
and the weak law of large numbers by Bernoulli [Berl3], equi-probability due 
to symmetry and other things by Laplace [Lapl2], the principle of indiff'erence 
by Keynes [Key21], Kolmogorov's axioms of probability theory [Kol33], early 
attempts to define the notion of randomness of individual objects/sequences 
by von Mises, Wald and Church [Misl9, Wal37, Chu40], finally successful by 
Martin-Lof [ML66], the notion of a universal a priori probability by Solomonoff 
[Sol64] and its mathematical investigation by Levin [ZL70, Lev74]. 
There is an ongoing debate between objective and subjective probability, 
which became sharper in the 20th century (not only in AI). Prominent ad-
vocates of the relative frequency or objective interpretation were Kolmogorov 
[Kol63], Fisher [Fis22], and von Mises [Mis28]. There are many advocates of 
probabilities as degrees of belief [Pop34, Ram31, Fin37, Cox46, Sav54, Jef83]. 
Carnap [Car48, Car50] tried to supplement logic with probability theory to 
so-called inductive logic. This works fine for propositional logic [Jay03], but 
not for predicate logic [Put63]. The closely related reference class problem is 
addressed in [Rei49, Kyb77, Kyb83, BGHK92]. 
There are many books on probability theory with difi"erent focus. For a 
thorough treatment of the early history of the concept of probability the reader 

2.5 History & References 
57 
is referred to the books by Hacking [Hac75] and Hald [Hal90], and for the foun-
dations developed in the 20th century to the book by Schnorr [Sch71] and the 
PhD theses by van Lambalgen [Lam87] and Wang [Wan96]. A good standard 
textbook is by Feller [Fel68]. A pleasant to read book with a philosophical 
touch is by Jaynes [Jay03]. It treats probability theory as a natural extension 
to (Boolean) logical reasoning, emphasizes the "full" Bayesian approach with 
priors determined by the maximum entropy principle, and discusses various 
historical paradoxes and how these pitfalls could have been avoided by not 
becoming addicted to measure theory, but by sticking to elementary discrete 
math. The historic battle between different schools is treated at (over)length 
in a rather polemic way. Gelman [GCSR95] is a modern and more practical 
book on Bayesian data analysis. 
Alternatives to probability theory. Given the success story of Bayesian 
probability theory, it is somewhat surprising that so many alternatives have 
been considered in AL Many reasons why probability theory is unsuitable for 
AI have been stated: strict numerical values are not appropriate for a qual-
itative reasoning system, probability theory cannot deal with impreciseness, 
or vagueness, or subjective beliefs, or is just impractical. Setbacks caused by 
naive and/or inconsistent application are also responsible for Bayesian prob-
abilistic reasoning falling out of favor in the 1970s. Default reasoning [ReiSO], 
nonmonotonic logic [MD80], and circumscription [McC80] treat conclusions 
or events not as "believed to a certain degree" but as "believed by default 
until a better reason is found to believe something else" (see the anthology 
[Gin87]). Certainty factors ("fudge factors") have been introduced into clas-
sical rule-based expert systems to accommodate uncertainty [Sho76, BS84]. 
Dempster-Shafer theory uses probability intervals for probability values if they 
themselves are not perfectly known, usually because they have been estimated 
from a finite amount of data [Dem68, Sha76]. More generally, this approach 
goes under the name imprecise probabilities [Wal91]. In the full Bayesian 
treatment one defines a (second-order) probability distribution over probabil-
ity values to deal with this kind of ignorance or beliefs. Fuzzy logic deals with 
vaguely defined events (Fuzzy sets), which are only "sort of" true, like the 
"Eiffel tower is high" [Zad65, Zim91]. Possibility theory has been introduced 
to handle uncertainty in fuzzy systems [Zad78]. See [Fin73, Wal91] or [RN95, 
Chp.15.6] for a more detailed account of various uncertain reasoning systems. 
Finally, quantum systems must be described with complex-valued probability 
amplitudes resulting in strange interference effects. A time may come (e.g. for 
nanobots) when quantum logic [Hug89] will be needed in AI. 
All these alternate approaches have their problems: Either they have un-
clear semantics, or they are not self-consistent, or they don't scale up, or 
have other problems. It is not that Bayesian probability theory leaves no 
wishes open, but it is the most consistent system developed so far. Imprecise 
probability theory, a probabilistic robust or worst-case reasoning approach, is 
quite consistent and a possibly useful extension of probability theory in game 

58 
2 Simplicity k Uncertainty 
theory and safety critical areas. The other approaches may survive as useful 
(efficient) approximations to a full Bayesian treatment. Although probability 
theory slowly (re)covers AI, the debate still goes on [Che85, CheSS]. 
Cox's axioms and theorem. In [Cox46] Cox shows that every function 
Bel(-|-) satisfying his Axioms 2.17 is isomorphic to a (conditional) proba-
bility function. This (with considerable delay) gave a significant boost in 
using standard probability theory for dealing with subjective beliefs and 
uncertainty. Cheeseman [CheSS] has called Cox's derivation the "strongest 
argument for use of standard (Bayesian) probability theory". Similar senti-
ments are expressed by Jaynes [JayTS, p24]; indeed, Cox's theorem is one 
of the cornerstones of Jaynes' recent book [Jay03]. Horvitz, Heckerman, 
and Langlotz [HHLS6] used it as a basis for comparison of probability and 
other nonprobabilistic approaches to reasoning about uncertainty. Hecker-
man [HecSS] used it as a basis for providing an axiomatization for belief 
updates. Various variants of Cox's axioms have been considered in the lit-
erature [Rei49, Acz66, HecSS, Jay03, Fin73, Tri69], which simplify the deriva-
tion, or weaken, replace or better motivate the assumption. A loophole in 
all these derivations was only recently discovered [Par95]. They are all re-
lated to the following unwariness. The function F of Cox's axioms mapping 
Be[{C\BnA) 
and Be\{B\A) 
to Be\{BnC\A) 
is proven to be associative, i.e. 
F{x,F{y^z))=F{F{x,y),z)j 
but actually associativity is only proven for (x^y^z) 
of the form x = Be\{D\CnBnA), 
y = Bel{C\BnA), 
and z = Bel{B\A) 
for some 
events A, B, C, and D, If the set of such triples {x.y.z) 
is dense in [0,1]"^, 
then by continuity, F is associative. Paris provided a rigorous proof of Cox's 
result, assuming that the range of Bel is contained in [0,1] and using assump-
tions similar to [HHLS6]. However, he and all others who tried to fix the proof 
needed to make additional assumptions that are not very appealing. Usually 
they demand or imply that the belief values are dense in a certain subset of 
iR, which excludes systems with a finite number of events. It remains an open 
question whether there is an appropriate strengthening of the assumptions 
that lead to Cox's result in finite settings. See [Hal99] and references therein 
for details. 
Algorithmic probability &; universal induction. The notion of univer-
sal (enumerable semi)measures was introduced in [Sol64, ZL70, LV77]. Levin 
[ZL70] defined universal a priori probability as one dominating all enumerable 
semimeasures. The dominance (2.23) and the equivalence M = ^u is due to 
Levin [ZL70]. Convergence of M to /x in the conditional mean squared sense 
(Theorem 2.25) is due to Solomonoff [Sol7S] (who normalizes M as in (2.30) 
by giving up enumerability). The elementary proof of M(xt|x<t) ^ ^ 1 for 
computable Xi:oo is not based on any source. The direct study of predictions 
based on past observations without discussing models was called 'the prequen-
tial approach' by Dawid [DawS4]. Good reviews on universal induction with 
This is an exponent, not a footnote! 

2.5 History & References 
59 
a philosophical touch are found in [LV92b, Sol97]. For an older, but general 
review of inductive inference see Angluin [AS83]. 
Schmidhuber [SchOO, Sch02a] constructed a natural hierarchy of general-
izations of algorithmic probability and complexity and introduced more gen-
eral, approximable and universal cumulatively enumerable semimeasures. The 
restriction to time-bounded universal probability is treated in [LV91, LV97, 
SchOO, Sch02b, SchOSa, Sch04] and is closely related to resource-bounded com-
plexity and universal Levin search. 
Other topics related to universal induction are the weighted majority al-
gorithm by Littlestone and Warmuth [LW94], universal forecasting by Vovk 
[Vov92], Levin search [Lev73b], PAC-learning introduced by Valiant [Val84], 
the minimum message and description length principles [WB68, Ris78, Ris89, 
LV92a, Gru98, VLOO], and Occam's razor, learnability and VC dimension 
[BEHW87, BEHW89]. 
Randomness of individual objects in terms of randomness tests was defined 
by Martin-Lof [ML66] and is closely related to Kolmogorov complexity and 
algorithmic probability [LV97, Cal02]. Another interesting randomness crite-
rion for individual sequences by Vovk in terms of the Hellinger distance can 
be found in [Vov87]. Randomness of individual sequences in a wider context is 
exhaustively analyzed in the survey papers by Uspenskii et al. [KU87, USS90]. 
Applications of Kolmogorov complexity and Levin search. Schmid-
huber [SchOO, Sch02b] defined the speed prior^ closely related to Levin search, 
and derives a computable strategy for optimal inductive reasoning. He ana-
lyzed consequences for computable universes sampled from such priors. Good 
numerical approximations to Kolmogorov complexity are computationally ex-
pensive. But the ongoing decrease of processing costs has permitted the first 
successful implementations and applications [Sch97, SZW97]. A derivate of 
Levin's universal search algorithm was used in [Sch97] to discover neural nets 
with low Levin complexity, low Kolmogorov complexity, and high generaliza-
tion capability. Adaptive Levin search (ALS) and the optimal ordered problem 
solver (OOPS) extend Levin search by making its underlying probability dis-
tribution on program space adaptive and by improving it according to expe-
rience [SZW97, SchOSa, Sch04]. This can significantly speed up the discovery 
of algorithmic solutions. 
There are numerous applications of MDL, which can be viewed £ts an ap-
plied form of Kolmogorov complexity [LV97]. Apart from that, there are vari-
ous other "direct" approximations, implementations, or practical applications. 
The universal similarity metric by Vitanyi et al. based on string compression 
has been successfully applied to a plethora of clustering problems, including 
automatic music classification, and phylogeny and language tree reconstruc-
tion [Ben98, Li 03, CV03]. Conte et al. [Con97] evolved short Lisp programs to 
estimate Kolmogorov complexity. Chaitin [Cha91] speculated on the compu-
tational power of the evolutionary information gathering process and its rela-
tion to algorithmic information. Schmidt [Sch99] argued that (time-bounded) 

60 
2 Simplicity & Uncertainty 
Kolmogorov complexity helps and not prevents the search for extra terres-
trial intelligence (SETI). Vovk [VW98] described universal portfolio selection 
schemes. 
2.6 Problems 
2.1 ((Un)natural Turing machines) [CIO] Show that for every string x 
there exists a universal Turing machine U' such that Ku'{x) 
= l. Arguments 
of this sort are often used to demonstrate the arbitrariness or non-absolute 
character of algorithmic information. Argue that U' is not a natural Turing 
machine if x is complex. Elaborate on the difficulties in rigorously proving 
such a statement. 
2.2 (Exact ^ correspondence) [C20] 
We 
showed 
that 
M{x) 
:= 
Ep:t/(p)=.*2-'^^^ 
equals 
^u{x) 
:= E . e > t . 2 - ^ ( - ) ^ ( x ) 
within 
a 
multi-
plicative constant, i.e. M = ^jj. Improve this result to an exact equality 
in the sense that M{x) 
= ^w{x) '-= J2ueM 
^i^^(^) for some weights with 
'W^i/ > 2~^^^)~^^^^ (solution due to Paul Vitanyi, private communication). 
M = ^uj is true for any choice of the weights w^ > O^u e Mu- 
Show that 
equality (within a constant) no longer holds for a similarly generalized M 
with 2~^^P^ replaced by arbitrary 
Wp>0. 
2.3 (Martin-L6f random sequences and convergence) [C45soi] 
Show 
that a theorem true for all //-random sequences (see Theorem 2.31) is also 
true with //-probability 1. Under what conditions is the reverse direction 
true? In particular, is X]t^i(/^(^t|^<t) ~ ^ ( ^ d ^ < 0 ) ^ < ^^ ^^ue for every 
individual /i-random sequence? (cf. Theorem 3.19(z)). It has been shown 
that M ( x t k < t ) / / ^ ( ^ t k < t ) * - = ^ l w.//.p.l (see [LV97, Thm.5.2.2] and Theo-
rem 3.19(t') and Problem 3.10). Does the stronger statement of convergence 
individually for all Martin-Lof //-random sequences hold? The argument given 
in [LV97, Thm.5.2.2] and [VLOO, Thm.lO] is incomplete.^ The implication 
"M(a;i:n)<c-//(xi:n)Vn=^lim^_^oo^(^i:n)/Ai(^i:n) cxists" has bccu used, but 
not proven, and is indeed wrong. Construct a universal Turing machine U 
and a uniformly M.L.-random sequence cei-.oo on which M(at\a<^t)7^^' 
What 
about for generic U or for ^u? Construct an enumerable semimeasure W (not 
necessarily universal) which converges to /i on all /^.M.L.-random sequences 
for all computable measures // [HM04]. 
^ The formulation of their theorem is quite misleading in general: "Lei ji he a 
positive recursive measure. If the length of y is fixed and the length of x grows to 
infinity, then M{y\x)/iJi{y\x)-^ 
1 with fi-probability one. The infinite sequences LO 
with prefixes x satisfying the displayed asymptotics are precisely ['=^' and '<='] 
the /jL-random sequences.'''' First, for off-sequence y convergence w.p.l does not 
hold {xy must be demanded to be a prefix of CJ). Second, the proof of '<^' has 
loopholes (see main text). Last, '=:^' is given without proof and is wrong. 

2.6 Problems 
61 
2.4 (Oracle properties of Kolmogorov complexity) [C20s] A function 
or problem A is said to be Turing-reducible to B if there exists a Turing 
machine (finitely) computing or solving A provided B is given as an oracle 
[HMUOl]. Let K:iB* ^ I V be the Kolmogorov complexity iJ: JS* -^iB be the 
halting sequence {H{p) = l<^U{p) halts), and ^:=Ep:t/(p) haits'^~^^^^ ^ ^ -
[]N -^ IB] be the halting probability sometimes call 'the number of wisdom' 
[Cha75, BG79]. Show that K, H, and i? are Turing-reducible to each other 
(cf. [LV97, pl75]). 
2.5 (Weakly forgetful environments) [C15u] Consider two sequences 
x\.^ 
and x^.^^ "typical" in the sense that both are /i.M.L.-random. As-
sume a different early history (^<fc7^2:^^, k fixed), continued by the same 
observations {xl.^_-^ = ^l-.n-i — ^k-.n-i) for a long time n. Show that for 
computable fi the future is not affected by the far back history x^^ in the 
sense that fi{xn\xl^f^Xk:n~i) — l^{xn\x\]^Xk:n-i) ~^ ^ for n —^ oo. Hint: show 
M{xn\xk:n-i) 
—^l^{xn\xl^}^Xk:n-i) ^ov 1 = 1 and 1 = 2. This property of ji for 
"typical" sequences may be considered as a weak form of forget fulness. Argue 
that it is more appropriate to define forgetfulness as asymptotic indepen-
dence of x^^ for aH environments (cf. definition in Section 5.3.6). Suggestion: 
compare ergodic Markov processes (see Definition 5.37) with ji defined by 
/i(l|a;<n):=i/3 for Xi=i —iGiB. 
2.6 (Complexity increase) [C25u/C45oi] We are interested in good upper 
bounds on the increase in complexity when elongating a string y := a:<t to 
yx :=x<^tXt:n = xi:n' From Theorem 2.10{iv) we know that K{yx) — K{y) < 
K{x\y)^0{l). 
Later (cf. Problem 3.13) we need similar bounds with K on 
the l.h.s. replaced by KM{x) := —log2M(x). Furthermore, let C{x) be the 
plain Kolmogorov complexity, defined as the length of the shortest plain (as 
opposed to prefix) program computing x (see [LV97, Chp.2]). We have no 
particular demands on the r.h.s. of the inequality. So let us consider K{x\y) 
defined as the length of the shortest plain or prefix, halting or nonhalting 
program, computing x or a, string starting with x, given y or a> string starting 
with y. The only important property of K{x\y) is that it corresponds to the 
length of a shortest program computing x from y. We do not want K{x\y) to 
be defined as a difference K{yx) — K{y). Prove the following inequalities: 
(i) 
C{yx)^C{y) 
< 
K{x\y)+0{?) 
(M) KM{yx)-KM{y) 
< 
k{x\y)+0{l) 
(iii) KM{yx)-KM{y) 
< 
K{^i\y)-\og^fi{x\y)+0{7) 
Since C, K, K, and KM coincide within additive logarithmic terms, all in-
equalities follow from Theorem 2.10{iv) to logarithmic accuracy 0(log^(x^)). 
Improve the bounds to 0{7)^{K{C{y)),K{i{y)),K{i{y))}, 
respectively in-
dependent of X for suitable iC. It is an open question whether the bounds hold 
within an additive constant independent of x and y for any of the K. 

62 
2 Simplicity & Uncertainty 
2.7 (Lower convergence bounds and defect of M) [Cl5u/C35o] Prove 
for all t and xi:t, where ^p:U(p)^x sums over all halting and nonhalting pre-
fix programs printing x (and no more). Bounds (2.24) and (2.25) show rapid 
convergence of M in a cumulative sense (see Section 3.2.4). The lower bound 
above shows that the instantaneous prediction quality is lower-bounded by the 
defect 1 —X^x ^^(^tk<t) of M from a proper measure, which itself is lower-
bounded by 2~^^*^. This shows that occasionally, namely for simply describ-
able t, the prediction quality is poor. Show that 2~^^*^ converges to zero slower 
than any computable to zero converging function, i.e. 2"^^^^ ^0{f{t)) 
for any 
computable / with f{t)—^0^ for instance, 2'^^^"^ j^O{[\nt]~~^). Show that simi-
lar lower bounds hold when performance is measured by (3.14)-(3.17) instead 
of (3.13) by exploiting their relations stated in Problem 3.16. This shows that 
the assertion in [LV97, Thm.5.2.1] that 
St:=E[£^,{iJ.{x[\x^t)-M{x[\x<t)f] 
converges to zero faster than 1/t is not correct. Do similar lower performance 
bounds hold for measure Mnorm{xt\x<ct)'=M{xt\x<^t)/^xt'^(^i\^<'^)' 
which 
has no defect? For which sequences does equality ~2~^^^^ hold? 
2.8 (Predictions based on monotone complexity) [C40s] It is easy to 
see that a predictor based on complexity K fails, since for any sequence x 
the predictor will essentially be indifferent about predicting 0 versus 1 due 
to K{xl) = K{xO). Clearly, for e.g. x = 0^, any reasonable predictor should 
favor 0. The monotone complexity Km{x) :=inmp{i{p):U{p)—x^} 
does not 
suffer from this problem. Further, the leading contribution m(x):=2~^^^^^ in 
M{x) is extremely close to M{x) [Gac83]. Hence m = 2~^'^ is more promising 
for prediction than K. Show that for computable environments, m(x^|x<t):= 
^[^^^*^ converges rapidly on-sequence (a),(6), but may converge (c) slowly (e) 
off-sequence (x^-.^Xt), and (in disagreement with [LV97, Cor.5.2.2]) not at all 
for probabilistic environments (/): 
(a) X]^^Jl-m(xt|x<t)|<|Km(xi:n), 
m(xt|x<t)-^l for computable xi:oo 
{b) Indeed, m{xt\x^t)y^^ 
at most Km{xi:oo) times 
(c) X^^^im(xt|x<t)<2'^"^*^'^i^^\ 
m(xt|x<t)^-^'0 for computable Xi:oo 
(d) J^^^^m(xt|x<t) < [i^m(xi:n)]^, 
m{xt\x^t)—>" 0 for computable xi:oo 
(e) \/s3U,xi:oo ' Km{xi:oo) = s and 
J2't^i'^i^t\x<t)>2^-2 
( / ) 
B/ieM'^/^pXMdet 
: m ( X t | x < 0 * 9 ^ / i ( X t | x < t ) V x i : o o 
Explain why cubic upper bound (d) does not contradict exponential 
lower bound (e). Show that m is not a semimeasure, but normalization 
imnorm{xt\x<:t)'=m{xt\x^t)/^a:t^{xt\x^t) 
docs uot improvc (c)-(e) and even 
deteriorates (a) and (6). Show that the loss of the Bayes-predictor based on m 
displays the same behavior as rrinorm (see Section 3.4). See [Hut03d, Hut04a] 
for solutions. 

2.6 Problems 
63 
2.9 (Prediction of selected bits) [C35oi] Assume we observe sequence 
xi:oo, but we only need to predict the even bits. The odd bits shall be kept, 
since they may provide useful information for predicting the even bits (see 
below). Sure, (2.24) implies Yjeventi^~~^i^t\x<t))^ 
<lKm{xi:oo). 
Assume 
that the even bits coincide with their preceding odd bits, i.e. Xt = Xt-i for 
even t, and that Km{xi:oo) = Km{xiXs-.') is large (or even infinite). The task 
of recognizing that 'even bit=odd bit' seems quite easy (of complexity 0(1)), 
so a reasonable prediction scheme should make only 0(1) errors on the even 
bits, e.g. '^eventi^~-^i^t\^<t))'^ 
< 0(1)? Provc or disprove a bound of this 
form. Generalize the positive/negative result to probabilistic /i like (2.25), and 
to other more complex selection rules (cf. Problem 3.18). 
2.10 (Application of Bayes' rule) [C05] Assume the prevalence of a cer-
tain disease in the general population is 1%. Assume there exists a quite 
reliable test for the disease, say, the test on a diseased/healthy person is pos-
itive/negative with 99% probability. If the test is positive, what is the chance 
of having the disease? Many medical doctors say about 99%. Show with Bayes' 
rule (2.19) that the true answer is scanty 50%. 
2.11 (Bayes' and Laplace' rule for Bernoulli sequences) [CI5] 
Let 
X = X\X2"'Xn G ]B^ be generated by a biased coin with head probability 
(9G [0,1], i.e. the likelihood of x under hypothesis 6 is p(x|6>) = (9^i(l-6>)^-^S 
where ni =a:i + ... + x^. Bayes [Bay63] assumed a uniform prior density 
p{6) = 1. (Since 0 is continuous we must normalize jQp{6)d9 = 1 instead 
of EePi^) = !•) Show that p{x) = J^p{x\e)p{0) dO = ""''f^^'^^l^' and that the 
posterior probability density p{9\x) — 
}^\^ of B after seeing x is strongly 
peaked around the frequency estimate 0 = ^^ for large n. Laplace [Lap 12] 
asked about the predictive probability p(l|x) of observing Xn-\-i — 1 after 
having seen x — xi...Xn' Show that p{l\x) — ^ ^ — ^^^^' Laplace believed 
that the sun had risen for 1826213 days since creation, so he concluded that 
the probability that the sun won't rise tomorrow is ig262i5-


Probability theory is nothing but common sense reduced 
to calculation 
— Pierre Laplace 
Pierre Laplace 
(1749-1827) 
3 Universal Sequence Prediction 

66 
3 Universal Sequence Prediction 
in this chapter we investigate Solomonoff's universal induction scheme in 
detail. More generally, we consider a universal (or mixture) distribution <^, 
defined as a weighted sum or integral of distributions iy£M, where M is any 
countable or continuous set of distributions including the true distribution 
jj.. This is a generalization of Solomonoff induction, in which A4 is the set 
of all enumerable semimeasures. We show for several performance measures 
that using the universal ^ as a prior is nearly as good as using the unknown 
true distribution //. In a sense, this solves the problem of the unknown prior 
in a universal way. All results are obtained for general finite alphabet. Con-
vergence of (f to /x in a conditional mean squared sense and of ^//i —> 1 with 
//-probability 1 is proven. The number of additional errors E^ made by the 
optimal universal prediction scheme based on ^ minus the number of errors 
E^ of the optimal informed prediction scheme based on /i is proven to be 
bounded by 0{^/E^). 
The prediction framework is generalized to arbitrary 
loss functions. A system is allowed to take an action yt, given Xi...Xt-i 
and 
receives loss ^xtvt ^^ ^t is the next symbol of the sequence. No assumptions on 
i are necessary, besides boundedness. Optimal universal A^ and optimal in-
formed Ajj prediction schemes are defined, and the total loss of yl^ is bounded 
in terms of the total loss of A^, similar to the error bounds. We show that the 
bounds are tight and that no other predictor can lead to significantly smaller 

3.1 Introduction 
67 
bounds. Furthermore, for various performance measures we show Pareto op-
timahty of ^ in the sense that there is no other predictor that performs better 
or equal in all environments u E M and strictly better in at least one. So, 
optimal predictors can (w.r.t. to most performance measures in expectation) 
be based on the mixture ^. Finally, we give an Occam's razor argument that 
the choice Wi^^2~^^^^ for the weights is optimal, where K{u) is the length of 
the shortest program describing u. Furthermore, games of chance, defined as 
a sequence of bets, observations, and rewards, are studied. The average profit 
achieved by the A^ scheme rapidly converges to the best possible profit. The 
time needed to reach the winning zone is proportional to the relative entropy 
of // and ^. The prediction schemes presented here are compared to predictors 
based on expert advice. Although the algorithms, the settings, and the proofs 
are quite different, the bounds of both schemes have a very similar structure. 
Extensions to infinite alphabets, partial, multistep, delayed, and probabilistic 
prediction, classification, and more active systems are discussed. 
3.1 Introduction 
Induction. Many problems are of the induction type in which statements 
about the future have to be made, based on past observations. What is the 
probability of rain tomorrow, given the weather observations of the last few 
days? Is the Dow Jones stock index likely to rise tomorrow, given the chart of 
the last years and possibly additional newspaper information? Can we reason-
ably doubt that the sun will rise tomorrow? Indeed, one definition of science 
is to predict the future, where, as an intermediate step, one tries to under-
stand the past by developing theories and, as a consequence of prediction, one 
tries to manipulate the future. All induction problems may be studied in the 
Bayesian framework. The probability of observing Xt at time t, given the ob-
servations xi.,.Xt-i 
can be computed with the chain rule, if we know the true 
probability distribution, which generates the observed sequence xiX2Xs.... The 
problem is that in many cases we do not even have a reasonable guess of the 
true distribution //. What is the true probability of weather sequences, stock 
charts, or sunrises? 
Universal sequence prediction. In order to overcome the problem of the 
unknown true distribution, one can define a mixture distribution ^ as a w^y-
weighted sum or integral over distributions u^M^ 
where M is any discrete 
or continuous (hypothesis) set including fx. M is assumed to be known and 
to contain the true distribution, i.e. / / G A 4 . Since the probability ^ can be 
shown to converge rapidly to the true probability /i in a conditional sense, 
making decisions based on ^ is often nearly as good as the infeasible optimal 
decision based on the unknown fi [MF98]. Solomonoff [Sol64] had the idea 
to define a universal mixture M (Section 2.4) as a weighted average over de-
terministic programs. Lower weights were assigned to longer programs. He 

68 
3 Universal Sequence Prediction 
unified Epicurus' principle of multiple explanations and Occam's razor (sim-
plicity) principle into one formal theory (see [LV97] for this interpretation of 
[Sol64]). Inspired by Solomonoff's work, Levin [ZL70] had the idea to define 
the closely related universal prior ^17 as a weighted average over all semi-
computable semiprobability distributions. If the environment possesses some 
effective structure at all, Solomonoff's posterior [Sol78] "finds" this structure 
and allows for a good prediction. In a sense, this solves the induction problem 
in a universal way, i.e. without making problem-specific assumptions. 
3.2 Setup and Convergence 
In this section we show that the mixture ^ converges rapidly to the true distri-
bution /i. After defining basic notation, we introduce the universal or mixture 
distribution ^ as the iL^i^-weighted sum of probability distributions z/ of a set M. 
that includes the true distribution //. No structural assumptions are made on 
the u. A posterior representation of ^ with incremental weight update is also 
presented. We introduce various convergence concepts for random sequences 
together with their interrelations, and various distance measures between ^ 
and fi and their relations. We show that the relative entropy between fi and ^ 
is bounded by lnw~^^ which implies that ^ rapidly converges to /i in difference 
and in ratio. The difficulty in establishing convergence results on individual 
(Martin-Lof) random sequences, and the case 11^ M are briefly discussed. 
The section concludes with a discussion of various standard sets M of proba-
bility measures, including computable, enumerable, cumulatively enumerable, 
approximable, finite-state, and Markov (semi)measures. 
3.2.1 Random Sequences 
We denote strings over a finite alphabet X by xiX2---Xn with x^ G A' and t^n^Ne 
IV and iV-IAfl. We further use the abbreviations 6 for the empty string, Xt:n -— 
XtXt+i'-'^n-i^n for t<n and e for t > n, and x<t :=xi...xt-i, and ct; = Xi:oo 
for infinite sequences. We use Greek letters for probability distributions (or 
measures). Let p{xi...Xn) be the probability that an (infinite) sequence starts 
with Xi...Xn' 
We also need conditional probabilities. Presuming they exist, we have 
p{xt\x<:t) = p(xi:t)/p(a:<t), 
(3.2) 
p{Xi...Xn) 
= 
p{xi)'p{x2\xi)'...'p{Xn\xi...Xn-l), 
(3.3) 
called multiplication rule (of conditional probabilities), or chain rule. The first 
equation states that the probability that a string xi...Xt-i 
is followed by Xt 

3.2 Setup and Convergence 
69 
is equal to the probability that a string starts with xi.,.Xt divided by the 
probability that a string starts with 
xi...Xt-i. 
The second equation is the first, applied n times. Whereas p might be any 
probability distribution, fi denotes the true (unknown) generating distribution 
of the sequences. We denote expectations by E. The (conditional) expected 
value of a function / : A'* -^ JR, dependent on xi-t^ independent of Xtj^i-oo^ 
(given x<t) is 
E[/]-= Y^ ^Ji{xi..n)f{xr,t), 
Et[/]-^[/(xi:t)|rr<t] = X^V(^tk<t)/(:ri:,) 
(3.4) 
for any choice of n > i . Expectations E are always w.r.t. the true distribution 
fi. The prime in ^ 
denotes that the sum is restricted to Xi:^ with ju(xi:t) ^ 0 
(/x(a:t|x<t)7^0). If/i(x<£) —0, then ii{xt\x^t) and hence E^ is undefined. Since 
the sum in E is restricted to /x(a;i:t) T^O, E[Et[..]] =E[..] is valid in any case 
(by the chain rule). 
In a more probabilistic terminology we have a sample space Q — X'^ with 
elements ijj = ujiij02^^'" ^ ^ being infinite sequences over the finite alphabet 
X. The cylinder sets F^-^.^ := {uj: ui^n = Xi:n} are events. We define the a-
algebra T as the smallest set containing all cylinder sets and which is closed 
under complement and countable union. A probability measure fi is uniquely 
defined by giving its values //(rjc^.^) on the cylinder sets, which we abbreviate 
by IJ^{xi:n)' See Section 2.3 for a brief introduction to probability theory or 
[LV97, Doo53] or any statistics book for a more thorough treatment. Now, / 
may be interpreted as a random variable or measurable function. Two func-
tions differing on a set of measure zero have the same expectation. So if we 
"undefine" / for some Xi:t with //(xi:t) = 0, the expectation should not be 
affected. Hence, Y2 ^^ ^^^ correct definition for partial functions. The prime 
is ineffective and can be ignored for total functions. Many expressions in this 
book are undefined on a set of measure zero. Henceforth we will not mention 
this anymore. See Section 3,9.1 for alternative ways of treating /i = 0. 
Finally, the probability of an event ACf^is 
P[74] =:E[x^], where x is the 
characteristic function of A, i.e. XA{^) = ^ if CJGA, and x(a;)=0 otherwise. 
3.2.2 Universal Prior Probability Distribution 
Every inductive inference problem can be brought into the following form: 
Given a string a:<t, take a guess at its continuation xt. We will assume that the 
strings that have to be continued are drawn from a probability distribution 
/i. This includes deterministic environments as a special case, in which the 
probability distribution /i is 1 for some sequence xi:oo and 0 for all others. 
We call probability distributions of this kind deterministic. The maximal prior 
information a prediction algorithm can possess is the exact knowledge of //, but 
in many cases (like for the probability of sun tomorrow) the true generating 
distribution is not known. Instead, the prediction is based on a guess p of 

70 
3 Universal Sequence Prediction 
II. We expect that a predictor based on p performs well, if p is close to /i or 
converges, in a sense, to /i. Let M :={i^i,z^2v} be a countable set of candidate 
probability distributions on strings. Results are generalized to continuous sets 
M in Section 3.7.2. We define a weighted average on M 
^(^l:n) = 
£,M{xi:n) 
'•= ^ 
W^-u{xi:n), 
^^ 
Wj, = 1, 
W^ > 0. 
(3.5) 
veM 
lyeM 
It is easy to see that ^ is a probability distribution as the weights Wi, are 
positive and normalized to 1 and the v^M 
are probabilities.^ For finite M 
a possible choice for the w is to give all u equal weight {w^, = n ^ ) . We call 
^ universal relative to M^ as it multiplicatively dominates all distributions in 
M 
£,{xi:n) 
> '^iy-^(^l:n) 
for all 
U G M. 
(3.6) 
In the following, we assume that M. is known and contains the true distribu-
tion, i.e. fieM. 
If M. is chosen sufficiently large, then peM 
is not a serious 
constraint. 
3.2.3 
Universal Posterior Probability Distribution 
All prediction schemes in this book are based on the conditional probabilities 
p{xt\x^t)- 
It is possible to also express the conditional probability ^{xt\x<^t) 
as a weighted average over the conditional z/(xt|x<t), but now with time-
dependent weights: 
^{xt\x^t) 
= yZ w^{x^t)i^{xt\x^t), 
Wi.{xi:t) : = i ^ ^ ( x < t ) - T - V - ^ . 
(3.7) 
and Wj^{€) := Wy. The denominator just ensures correct 
normalization 
^yWy{xi.t) 
= 1. By induction and the chain rule we see that Wy{x<^t) — 
'UJuy[x^t)/^{x^t)- 
Inserting this into Y.y^iy{^<t)^{^t\x<t) 
using (3.5) gives 
^(xt|x<t), which proves the equivalence of (3.5) and (3.7). liwy 
is interpreted 
as the prior (subjective) belief in z/, then Wy{x<^t) is the posterior belief in v 
after having seen x^f 
The expressions (3.7) can be used to give an intuitive, 
but non-rigorous, argument why £^{xt\x^t) converges to p{xt\x<:^t)'- The weight 
Wu{x^t) 
of 1/ in ^ increases/decreases if v assigns a high/low probability to the 
new symbol x^, given x<t. For a /i-random sequence xi-,t^ l^{xi:t)'^J^{xi:t) 
if 
u (significantly) differs from p. We expect the total weight for all v consistent 
with p to converge to 1, and all other weights to converge to 0 for 
t^oo. 
Therefore we expect ^(xt|x<t) to converge to p{xt\x^t) 
for 
/i-random strings 
^ The weight Wu may be interpreted as the initial degree of belief in v and ^{xi...Xn) 
as the degree of belief in xi...Xn- If the existence of true randomness is rejected 
on philosophical grounds, one may consider M containing only deterministic en-
vironments. ^ still represents belief probabilities. See Section 2.3 for details.. 

3.2 Setup and Convergence 
71 
Expression (3.7) seems to be more suitable than (3.5) for studying conver-
gence and loss bounds of the universal predictor ^, but it will turn out that 
(3.6) is all we need, with the sole exception in the proof of Theorem 3.66 and 
in Section 5.5. Expression (3.7) is useful when one tries to understand the 
learning aspect in ^. 
3.2.4 Convergence of Random Sequences 
A classical (nonrandom) real-valued sequence at is defined to converge to 
a*, short at ^ a^, if VeBtoVt > to : |at —a*| < e. We are interested in con-
vergence properties of random sequences Zt{uj) for t —^ oo (e.g. Zt{(jj) = 
^{(jUt\^<t) — l^{^t\^<t))- We define six convergence concepts for random se-
quences and relate them. 
See Section 2.4 for a definition of ^n = £,Msemi = M and Martin-Lof ran-
domness. In statistics, (i) is the "default" characterization of convergence of 
random sequences. Definitions (ii), (iii), and {iv) are also well known and are 
often more convenient to deal with than (z). Further, convergence i.m.s. is very 
strong: it provides a "rate" of convergence in the sense that the expected num-
ber of times t in which Zt deviates more than e from z^ is finite and bounded 
by c/e^ and the probability that the number of e-deviations exceeds ^ 
is 
smaller than 5, where c:=^^^E[(zt —z*)^]. Definition {v) uses Martin-L5f's 
notion of randomness of individual sequences to define convergence M.L. Since 
this chapter mainly deals with general Bayes mixtures ^, we generalized in {vi) 
the definition of convergence M.L. based on ^u to convergence /i.^.r. based on 
<^ in a natural way. For finite At or if ^GA^, the definition of /x/^-randomness 
depends only on Af, and not on the specific weights used in ^. Convergence 
in one sense often implies convergence in another. The following implications 
for convergence of random sequences are true. Unconnected criteria (in the 
transitive hull) are incomparable (see also Problem 3.9). 

72 
3 Universal Sequence Prediction 
3.2.5 Distance Measures between Probability Distributions 
We need several distance measures between vectors y = [yi) and z = (z^) in 
general, and probability distributions/vectors for which yi>0, 
Zi>0^ and 
YliVi — Yli^i — ^ in particular, i = l,...,A^, namely the^ 
absolute (or Manhattan) distance: 
a{y^ z) := J2i IVi " ^i\ 
quadratic (or squared Euclidian) distance: 
s{y,z) :== YliiVi ~~ ^if 
(squared) Hellinger distance: 
h{y,z) := ^i{^/y'^~^f^^Y 
(3.10) 
relative entropy or KL divergence: 
d{y^ z) := Yli Vi ^^ ^ 
absolute divergence: 
6(t/, z) := ^ ^ T/^ | In |^ | 
The relative entropy is not a metric, but for probabihty distributions, for 
which it is defined, it is at least nonnegative and zero if and only \iy = z. All 
bounds we prove in this chapter rely heavily on the following inequalities: 
Olnf :^0V^>0 and t/ln| :=ooVy >0. 

3.2 Setup and Convergence 
73 
A proof of the lemma is deferred to Section 3.9. Inequality (3.II5) is a gener-
alization of the binary N = 2 case used in [Sol78, HutOlc, LV97]. If we insert 
A" = {!,...,AT}, 
A^^IA'I, 
i = xt, 
yi = fi{xt\x<:t)^ 
2;^ = ^(a;t|x<t) (3.12) 
into (3.10) we get various instantaneous distances (at time t) between // and ^. 
If we take the expectation E and sum over J2^=i ^^ S^^ various total distances 
between /i and ^: 
at{x^t) 
'= ^ 
/i(a;t|x<t)-^(xt|x<t) 
An:=^E[at{x<t)] 
(3.13) 
t = i 
2 
st{x<t) := Y^ (^/i(xi|x<t) - ^{xt\x<:t)^ , 
5^ := ^E[si(a:<t)] 
(3.14) 
2 
^ 
ht{x<t) := Yl {VKxt\x<t) 
-V^{xt\x<t)) 
, 
i?„:=5]E[/ii(a:<t)] 
(3.15) 
t = i 
IJ,{xt\x 
dt{x<t) := ^ M a ^ * K t ) l n f p p ^ , 
Dr^•.= y2ndtix<t)] 
(3.16) 
^^^«l^<*^ 
t=i 
''t(a;<t) := ^yu(a;t|a:<t) In M(a;tk<t) 
C(a;t|x<t) 
Bn:=Ynbt{x<t)] 
(3.17) 
For Dn the following can be shown [Sol78, HutOlc, LV97]: 
fi{xt\x^t)^ 
n 
n 
Dn = Y,E[dt{x<t)] 
= 
^ E [ E 4 l n £,{xt\x <t) 
(3.18) 
< 
Inuj^ 
=: k^ 
In the first line we inserted (3.16) and used the definition of Et. Using 
E[Et[..]] = E[..], the t sum can thereafter be exchanged with the expecta-
tion E and transforms to a product inside the logarithm. In the last equahty 
we used the chain rule (3.3) for /i and ^. Using universality (3.6) of ^, i.e. 
ln[/i(xi:n)/C(^i:n)]<lnw;~-'^ for fieM 
yields the final inequality in (3.18). 
In (b) if some 2: = 0 we define i/|ln^|—yln^ :=0. 

74 
3 Universal Sequence Prediction 
3.2.6 
Convergence of ^ to /i 
Proof. Inequality (iz) follows from the Definitions (3.14) and (3.16) and from 
the entropy inequality (3.lis). From the definition and finiteness of Doo (3.18), 
and from dt{x<^t) ^ 0, one sees that \/dt{x<^t) -^ 0 for t —> oo i.m.s., which 
implies dt{x^t) —^0 w./i.p.l. The inequality (i) follows from (ii) by taking 
the E expectation and the Yl^=i sum. Convergence (Hi) follows from (i) by 
dropping ^ ^ / . The reason for the astonishing property of a single (universal) 
function ^ to converge to any ji^M. lies in the fact that the sets of /i-random 
sequences differ for different //. Properties {iv) and {v) are related to {%) and 
(iiz), but are incomparable convergence results. To prove {iv) we use the 
abbreviations iit = fi{xt\x<:t) and 6'=C(^t|^<t)- For /i{x^t)y^^ we have 
^*[(\^-^)1 = EV(-.l-<*)(vf-l)' 
(3-20) 
Xt 
= ^ 
( V ^ - V M i ) 
^ ht{x<t) < dt{x<t)' 
Xt 
The two inequalities follow from (3.15) and (3.11/z). Bound {iv) now follows 
by taking the E expectation and the Yl^=i sum. Convergence {v) follows from 

3.2 Setup and Convergence 
75 
{iv) from Definition 3.8(wi) of convergence i.m.s., which imphes convergence 
w./i.p.l. The first two inequaUties in {vi) immediately follow from inequalities 
(3.11a,6) and Definitions (3.13), (3.16) and (3.17). The third inequality follows 
from the first by linearity of E and Yl- The last inequality An < \J1nDn follows 
from 
l^- ^ ^ E E M < ^ E E I V ^ , ] < W^i:E[2de] ^ y ^ , (3.21) 
where we have used Jensen's inequality for exchanging the averages {~^Y^^i 
and E) with the concave function J~. 
• 
Since the conditional probabilities are the basis of all prediction algorithms 
considered in this book and ^ converges rapidly to \x (see Problem 3.11), we 
expect a good prediction performance if we use ^ as a guess of /i. Performance 
measures are defined in the following sections. 
Relations {i) — {iii) generalize Solomonoff's result [Sol78] to an arbitrary 
finite alphabet. Without the use of the Hellinger distance {iv), a somewhat 
weaker statement than {v) can be derived from {vi): 
E ^^fx{xt\x^t) 
C{xt\x<:t) = E[bt] < E[dt] 4- E[y/2dt] < E[dt] + V2E\dt\ 
'=^ 
0, 
since 
E[dt] -> 
0. 
I.e. 
|ln|g^jf^|i/2 
__, Q ^^^ 
^^ich 
implies 
(^{xt\x<^t)/fjL{xt\x^t) —> 1 i.p. The explicit appearance of n in the last 
expression of {vi) prevents proving stronger convergence w.//.p.l from {vi). 
The elementary proof for {v) w./^.p.l given here does not rely on the 
semi-martingale convergence theorem [Doo53, p324-325] as the proof of 
Gacs [LV97, Thm.5.2.2]. Furthermore, {iv) gives the "speed" of conver-
gence. Note the subtle difference between {Hi) and {v). For any sequence 
^i:oo (possibly constant and not necessarily /i-random), 
fi{x[\x^t)—^{xt\^<t) 
converges to zero w.p.l (referring to a:i:oo)? but no statement is possible 
for ^(x^|x<t)//x(xj|x<t), since l\xn.mifi{x^\x^t) 
could be zero. On the other 
hand, if we stay on the ^-random sequence {x[.^ = Xi-^o)^ {v) shows that 
£,{xt\x<:t)/l^{xt\x<:t)-^l 
(whether mffx{xt\x^t) 
tends to zero or not does not 
matter). Indeed, it is easy to give an example where ^{x[\x<^t)/K^tl^Kt) 
di-
verges. If we choose 
X = {/ii,/i2}, 
A^^Mi, 
fii{l\x<^t) = lt~^ 
and 
/X2(lk<t) = |t~^, 
the contribution of /i2 to ^ causes ^ to fall off like /i2~^~^, much slower than 
/i~t~^, causing the quotient to diverge: 
n 
/^l{Ol:n) = l[{l 
- i^-^) " = ^ Ci = 0.450... > 0 
t = l 
=^ 0i:oo is a /i-random sequence. 

76 
3 Universal Sequence Prediction 
n 
/i2(0l:n) = n ^ ^ - ^ ^ ' ' ) " ^ ^ ^2 = 0.358... > 0 
=^ ^(Ol:n) -^ ^^iCl + W2C2 =: C^ > 0, 
^(0<a) - ^l/il(l|0<t)/Ui(0<,)+^2M2(l|0<,)/i2(0<t) -^ ^^2C2^"^ 
^ 
Ul 0<0 = T7?r~Y ~^ ~^—^ 
"^ 
/-.in N -^ 
^ -^ ^ 
diverges. 
3.2.7 Convergence in Martin-L6f Sense 
An interesting open question is whether ^ converges to /x (in the sense of {in) 
or (v)) individually for all Martin-Lof random sequences, short £^—^11 (see 
Problem 3.10). Clearly, convergence //.M.L. implies {in) and {v) w.//.p.l by 
Lemma 3.9, but the converse may fail on a set of sequences with //-measure 
zero. A convergence M.L. result would be particularly interesting and natural 
for the universal prior ^u^ since M.L. randomness can be defined in terms 
of ^c/ = M. Attempts to convert {ii) or {iv) to eflFective /i.M.L.-randomness 
tests fail, since iu{^t\^<t)^ and hence {ii) and {iv) are not enumerable. The 
argument given in [LV97, Thm.5.2.2] and [VLOO, Thm.lO] is incomplete. The 
implication ^^£,u{^i:n) ^ c- fi{xi:n)\/n => lim^_^oo ^u{Xl:n)/fJ^{Xl:n) 
Cxists" haS 
been used, but not proven, and may indeed be wrong (cf. Problem 2.3). Vovk 
[Vov87] shows that for two finitely computable semimeasures yu and p and 
^i:oo being fi and p M.L, random that 
][VKxt\^<t) 
- Vpi^t\x<t)) 
<oo 
and 
X^ ( (J\x^\ 
~ ^ ) 
^ ^ -
t=l 
x' 
' 
t=l 
If ^u were recursive, then ^u —^ p and £,u/1^-^ 1 for every /i.M.L.-random 
sequence Xi:oo, since every sequence is ^t/-M.L. random (see Definition 2.31). 
Since ^f/ is not recursive, Vovk's theorem cannot be applied and it is not 
obvious how to generalize it. Indeed, one can show that M.L.-convergence fails 
for some choices of weights in ^c/, but there are non-universal semimeasures 
which M.L.-converge to p [HM04]. It is unknown whether there exists a M.L.-
converging universal semimeasure. More generally, one may ask whether ^M ~^ 
p for every /x/^x-random sequence. It turns out that this is true for some At, 
but false for others. 

3.2 Setup and Convergence 
77 
Our original or main motivation of studying /x/^-randomness is the implication 
of Theorem 3.22 that ^u—^/j. cannot be decided from ^ being a mixture dis-
tribution (3.5) or from the dominance property (3.6) alone. Further structural 
properties of Ji4u have to be employed. For Bernoulli sequences, convergence 
fjL.^Me-^' is related to denseness of A^e- Maybe a denseness characterization 
of Ml^l^ 
can solve the question of convergence M.L, of ^j/. The property 
$,u ^'M.u is also not sufficient to resolve the question, since there are M.3^ 
for which ^^^—^/x and M3i, 
for which ^^y^ jJ^ (see also Problem 3.10). The-
orem 3.22 can be generalized to i.i.d. sequences over general finite alphabet 
The idea to prove (ii) is to construct a sequence XI:OQ that is fio^/^-Tsmdom. 
and /i^^/^-random for OQ^OI. This is possible if and only if 0 contains a gap 
and ^0 and 9i are the boundaries of the gap. Obviously, ^ cannot converge to 
^0 cind ^1, thus proving nonconvergence. For no ^G [0,1] will this a:i:oo be fie 
M.L.-random. Finally, the proof of Theorem 3.22 makes essential use of the 
mixture representation of ^, as opposed to the proof of Theorem 3.19, which 
only needs dominance ^ > A^. 
An example for {ii) is A^ = {/io,Mi}, /xo(l|^<t) = /ii(0|a:<t) = | , Xi:oo = 
( 0 1 ) ^ - 0 1 0 1 0 1 0 1 . . . ^ 
fio{^l:2n)=fll{xl:2n)=aXi..2n) 
= {\r{lT 
^^ ^l:oo is 
/lo/C-random ant//xi/^-random, but lJ^o{x2n\x<2n) "= j , i^o(^2n+iki:2n) = f, 
/il {X2n\x<2n) 
= f , /^l (X2n+1 |^l:2n) = i a n d C ( ^ 2 n k < 2 n ) = f , ^(^2n+l k l : 2 n ) = | 
for WQ=Wi = l =^ C ( ^ n k < n ) 7 ^ / ^ 0 / l ( ^ n k < n ) . 
Proof. Let M = B and M = {fie : 9 e 0} with countable 9 C [0,1] and 
/^6'(l|^i:n)=^ = l-M^(0|xi:n), which implies 
Mxi-.n) = ^"Hl - ^ r ~ " S 
rii := xi+ ... + x , , 
6 = 6^ := —. 
n 
0 depends on n; all other used/defined 6 will be independent of n. We assume 
O..^0, where .. stands for some (possible empty) index, and ^G [0,1] (possibly 
^6>), where"stands for some superscript, i.e. fie. and we. make sense, whereas 
fig and WQ do not. ^ is defined in the standard way as 
C(^l:n) = ^Wefle{Xl:n) 
^ 
C{Xl:n) 
> ^^/^^(^l:n), 
(3.23) 
eee 

78 
3 Universal Sequence Prediction 
where Yle^& ~ ^ ^^^ we > 0^0. In the following let (i = fj^o^ eM 
he the true 
environment. 
oJ = xi:oo is /i/^-random 
4^ 
3c^ : <^(xi:n) < c^'/J^Ooi^i-.n) Vn 
(3.24) 
For binary alphabet it is sufficient to establish whether ^{l\xi:n)^—^Oo 
= 
li{l\xi:n) 
for ^/^-random xi:oo in order to decide ^{xn\x^n)-^fJ^{Xn\x<:n)' 
We 
need the following posterior representation of ^: 
The ratio lie/IJ^OQ can be represented as follows: 
dee 
(3.25) 
M<)(a;i:n) 
_ 
^"1(1-0)"-"^ 
Meo(^l:n) 
C ( l - ^ O ) ^ 
1-^0 
(3.26) 
is the relative entropy between 6 and 0, which is continuous in 6 and 0, and is 
0 if and only ii 0 = 0. We also need the following implication for sets 
f2C0: 
If 
w^ < wege{n) 
0 
and 
ge{n)<c^OeQ, 
(3.27) 
then 
Y^w^^^ei^xi-.n) 
< 
^ 
e n—^oo ^ 
eeo 
eeo 
which easily follows from boundedness ^e'^^ 
— ^ ^^^ M^ ^ 1 (Lemma 5.28w). 
We now prove Theorem 3.22. We leave the special considerations necessary 
when 0,1 G 0 to the reader and assume henceforth 0,106>-
(i) Let 0 be a countable dense subset of (0,1) and xi.c>o be /i/^-random. 
Using (3.23) and (3.24) in (3.26) for 0^0 
to be determined later we can 
bound 
\D{er,\\eo)-D{er^\\e)] 
f-^Ooi^l-.n. 
< 
=: c < oo. 
We 
(3.28) 
Let us assume that 0 = 0n-/^0o. This implies that there exists a cluster point 
Oy^Oo of sequence 0^- That is. On is infinitely often in an ^-neighborhood of 0^ 
e.g. D{On\\0) <€ for infinitely many n. 0£ [0,1] may be outside O. Since OJ^OQ 
this implies that 0^ must be "far" away from OQ infinitely often. For example, 
for £=|(6•-6>o)^ using D{0\\0)+D{0\\Oo)>iO-Oo)'^, 
we get D{0\\0o)>3e. 
We 
now choose ^ G 6^ so near to 0 such that |D(^||^) —D(^||^)| <£ (here we use 
denseness of O). Chaining all inequalities we get D{0\\0o)—D{0\\0)>3£—e—6 
= 
s>0. This, together with (3.28) implies e^^ < c for infinitely many n, which is 
impossible. Hence, the assumption On-/^0o was wrong. 

3.2 Setup and Convergence 
79 
Now, 6n-^0o implies that for arbitrary 6^6Q, 9e0, 
and for sufficiently 
large n there exists So>0 such that D{9n\\0) >28e (since D(6^o||^) T^O) and 
D{en\\0^)<Se^ This implies 
where we used (3.25) and (3.26) in the first inequality, and the second inequal-
ity holds for sufficiently large n. Hence Y^^Q^Q^W^^^O by (3.27) and wf^^ -^ 1 
by normalization (3.25), which finally gives 
{ii) We first consider the case 0 = {6o,0i}: Let us choose 6 (= 
l n ( j 5 | ^ ) / l n ( | j ^ ) 0 0 ) in the (KL) middle of OQ and 6>i such that 
D{0\\9o) = D{9\\9i), 
0<9o<9<9i<l, 
(3.29) 
and choose xi-o^ such that 9n '-= ^ satisfies \9n — 9\ < ^ 
{=> 9n " - ^ 9). 
We will show that a:i:oo is //^Q/^-random and /x^^/^-random. Obviously no ^ 
can converge to ^o CL'^^d ^i, thus proving ///^-nonconvergence. (xi:oo is obvi-
ously not /i6>o/i M.L.-random, since the relative frequency ^n7^^0/1 • ^i:oo is not 
even /x^ M.L.-random, since ^^ converges too fast {'^^^' ^i-.oo is indeed very 
regular, whereas ^ of a truly JIQ M.L.-random sequence has fluctuations of 
the order 1/^/n. The fast convergence is necessary for doubly /i/^-randomness. 
The reason that xi^oo is /i/^-random, but not M.L.-random is that /i/^-
randomness is a weaker concept than M.L.-randomness for McMl^^. 
Only 
regularities characterized by u^M. are recognized by /x/^-randomness.) 
In the following we assume that n is sufficiently large such that 9Q<9n<9i. 
We need 
|D(^||^)-L>(^||^)|<c|^-^| 
y9,9,9e[9o,9i] 
with 
c : - l n | i ^ < 00, 
(3.30) 
which follows for 9>9 (similarly 9<9) from 
D{m-D(m 
= J_ [ln'^~]n^]dB'<ljlnl^^-ln'jEt]d0' 
= ciO-e), 
where we have increased 9' to Oi and decreased 9 to ^0 in the inequality. Using 
(3.30) in (3.26) twice we get 
^Oii^i--n) ^ ^n[D{er,\\eo)-D{§r.\\e^)] ^^n[D{e\\0o)-\-c\6r^-e\-D{e\\el)^c\Or^-e\] ^ ^2c 
(3.31) 

80 
3 Universal Sequence Prediction 
where we used (3.29) in the last inequality. Now, (3.31) and (3.25) lead to 
U^l:n) 
W0^ /il0^{Xi:n) 
W0^ 
(3.32) 
which shows that Xi:oo is /xgiQ/^-random by (3.24). Exchanging OQ^OI in (3.31) 
and (3.32) we similarly get w^ >ci >0, which implies (using w^-\-w^ 
=1) 
e ( l k l : n ) - 
Y . 
^ % ( l | ^ l : n ) - ^ n ' - ^ 0 + ^ / ; n ' - ^ l 7 ^ ^ 0 - / i ^ o ( l k l : n ) . (3.33) 
This shows ^{l\xi:n) 
^-7^/^(lki:n)- Ouc cau show that ^{l\xi:n) does not 
only not converge to ^o (and ^i), but that it does not converge at all. The fast 
convergence demand \9n—0\ <^ on xi:oo can be weakened to 0n<0-\-O{^) Vn 
and 9n^0 — O{^) for infinitely many n, then xi:oo is still //6»Q/^-random, and 
w^n ^^1 >0 for infinitely many n, which is sufficient to prove ^7^//. 
We now consider general 0 with gap in the sense that there exist 0 < ^0 < 
9i<l with [(9o,(9i]n6>=={(9o,<^i}- We show that all O^OQ^OI give asymptotically 
no contribution to ^(l|xi:n), i-e. (3.33) still applies. Let 9^0\{9Q^9I}] 
all other 
definitions as before. Then 80 :=D{9\\9)—D{9\\9QII)>^^ 
since 9 is farther than 
9Q/I away from 9 {\9—9\ > \9o/i—9\). Similarly to (3.31) with 9 instead ^1, we 
get 
/^0{Xl:n) 
^ ^n[Di0r^\\eo)~D{0r^m] ^ ^2c^^n[D{0\\0o)-D{0\\0)] ^ ^^c^-nSe n-^oo ^ 
J^0o{xi:n) 
Hence 
w^ 
< 
^^e^^e'^'^^ 
-> 0 
from 
(3.25) 
and Sn 
we^ 
E^€0\{^oA}^nM^(lki:n) 
""-^^ 0 
from 
(3.27). 
Hence 
C(lki:n) == 
w^^ • 9o+wf^^ • 9i-]-€n i^ 9^ — /i(9o(lki:n) for Sufficiently large n, since 
Sn-^0, i/;^i>c;>Oand6>o^6>i. 
D 
3.2.8 The Case where ii^M 
In the following we discuss two cases in which (i ^ M.^ but most parts of 
this book still apply. Actually all theorems remain valid for // being a finite 
linear combination fi(xi:n) = J2uec'^^^(^^''r^^ ^^ ^'^ ^^ C C M. Dominance 
C(^i:n)^'^/x*/^(^i:n) is Still cusurcd with ifj^ 1= miu^^^/:^ >min^y^^i^'iy. More 
generally, if fx is an infinite linear combination, dominance is still ensured if 
Wjy itself dominates Vj^ in the sense that Wiy>aVi, for some a>0 (then Wn>a). 
Another possibly interesting situation is when the true generating dis-
tribution / i 0 A l , but a "nearby" distribution ft with weight Wfx is in M. 
If we measure the distance of /i to /i with the Kullback-Leibler divergence 
^n(/^||A) '—^x 
M(^i:n)ln^Yf^^ and assume that it is bounded by a con-
stant c, then 

3.2 Setup and Convergence 
81 
Dn = E In M(^l:n) 
C(^l:n) 
- 
E In 
C(^l:n) 
+ E In /i(^l:n)l 
A(^l:n)j < Inw-^, +c. 
So Dn<lni<;^^ remains valid if we define 
w^:=Wfi'e 
3.2.9 
Probability Classes 
M 
In the following we describe some well-known and some less-known probability 
classes M. This relates our setting to other works in this area, embeds it into 
the historical context, illustrates the type of classes we have in mind, and 
discusses computational issues. 
We get a rather wide class M. if we include all computable probability 
distributions in A^. In this case, the assumption fiEM 
is very weak, as it 
only assumes that the strings are drawn from any computable 
distribution; 
and all valid physical theories (and hence all environments) are computable 
to arbitrary precision (estimable in a probabilistic sense). 
It is favorable to assign high weights w^ to the u. Simplicity should be 
favored over complexity, according to Occam's razor. In our context this means 
that a high weight should be assigned to simple u. The prefix Kolmogorov 
complexity K{jy) is a universal complexity measure [Kol65, Lev74, Gac74, 
Cha75, LV97]. It is defined as the length of the shortest self-delimiting program 
on a universal Turing machine computing u{xi:n) given xi:n (cf. Section 2.2). 
If we define 
w. 
:= 2-^('') 
then distributions that can be calculated by short programs have high weights. 
The relative entropy is bounded by the Kolmogorov complexity of /x in 
this case (D^ < K(/i)-ln2). Solomonoff [Sol64, Eq.(13)] considered the class 
M = M'^ornp ^^ ^11 computable measures, which unfortunately leads to an in-
approximable mixture ^. Levin obtained the universal semimeasure ^u by tak-
ing M = Mu = Ml^^ 
to be the (multi)set enumerated by a Turing machine 
that enumerates all enumerable semimeasures [ZL70, LV97] (see Section 2.4 
for details). Recently, M. was further enlarged to include all cumulatively enu-
merable semimeasures [Sch02a]. In the enumerable and cumulatively enumer-
able cases, ^ is not estimable, but can be approximated to arbitrary but not 
pre-specifiable precision. If we consider all approximable (i.e. asymptotically 
computable) distributions, then the universal distribution ^, although still well 
defined, is not even approximable (like ^M^f^ )• An interesting and quickly 
approximable distribution is the Speed prior S defined in [Sch02b]. It is related 
to Levin complexity and Levin search [Lev73b, Lev84] (Section 7.1.2), but it 
is unclear for now which distributions are dominated by 5' (Problem 3.2). If 
one considers only finite-state automata instead of general Turing machines, ^ 
is related to the quickly computable, universal finite-state prediction scheme 
of Feder et al. [FMG92], which itself is related to the famous Lempel-Ziv data 
compression algorithm. If one has extra knowledge on the source generating 
the sequence, one might further reduce M and increase w. Note that 
^EM. 

82 
3 Universal Sequence Prediction 
in the enumerable and cumulatively enumerable case, but ^0A^ in the com-
putable, approximable and finite-state case. If ^ is itself in M, it is called a 
universal element oi M [LV97]. As we do not need this property here, M may 
be any countable set of distributions. In the following we consider generic M 
and w. 
We have discussed various discrete classes A^, which are sufficient from 
a constructive or computational point of view. On the other hand, it is con-
venient to also allow for continuous classes Ai. For instance, the class of all 
Bernoulli processes with parameter 9^ [0,1] and uniform prior W0 = 1 (Prob-
lems 2.11 and 5.14) is much easier to deal with than computable 0 only, with 
prior W0-=2~^^^\ Other important continuous classes are the class of i.i.d. 
and Markov processes. Continuous classes M are briefly considered in Sec-
tion 3.7.2. 
3.3 Error Bounds 
In this section we derive error bounds for predictors based on the mixture ^. 
We introduce the concept of Bayes optimal predictors Op that minimize p-
expected error. We bound E^^ -E^^ 
by 0{VE^), 
where E^^ is the expected 
number of errors made by the optimal universal predictor 0^, and E^^ is the 
expected number of errors made by the optimal informed prediction scheme 
0u. 
3.3.1 Bayes Optimal Predictors 
We start with a very simple error measure: making a wrong prediction counts 
as one error, making a correct prediction counts as no error. This serves as 
an introduction to the more complicated model with arbitrary loss function. 
Let 0^ be the optimal prediction scheme when the strings are drawn from 
the probability distribution //, i.e. the probability oi Xt given x<t is /i{xt\x<ct), 
and /J is known. 0^ predicts (by definition) xf^' when observing x^f 
The 
prediction is erroneous if the true t^^ symbol is not xf'^. The probability of this 
event is 1 —//(xf^|x<t). It is minimized if xf^" maximizes |J.{xf^'\x^t)' More 
generally, let 0p be a prediction scheme predicting xfp :==argmaxa;^p(xt|a;<t) 
for some distribution p. Every deterministic predictor can be interpreted as 
maximizing some distribution. 
3.3.2 Total Expected Numbers of Errors 
The /i-probability of making a wrong prediction for the t^^ symbol and the 
total /x-expected number of errors in the first n predictions of predictor 0p 
are 

3.3 Error Bounds 
83 
n 
efo{x^t) 
••= l-fiixPlx^t), 
E^" := 5^E[ef''(x<t)]. 
(3.34) 
t = l 
If /x is known, 0^ is obviously the best prediction scheme in the sense of 
making the least number of expected errors 
E^^ 
< E^^ 
for any 
0p, 
(3.35) 
since 
ef^(a;<t) - l-//(xf^|x<t) = min{l-/i(xt|x<t)} < 1-/i(a:f^|rr<t) = ef^(x<t) 
for any p. Of special interest is the universal predictor 0^. As ^ converges 
to ji the prediction of 0^ might converge to the prediction of the optimal 
0^. Hence, 0^ may not make many more errors than 0^ and, hence, than 
any other predictor 0p. Note that xff is a discontinuous function of /O, and 
xf^ ^^xf^ 
does not follow from (,-^fJ^- Indeed, this problem occurs in related 
prediction schemes, where the predictor has to be regularized so that it is 
continuous [FMG92]. Fortunately this is not necessary here. We prove the 
following error bound. 
The first bound actually contains E^^ on the r.h.s., so it is not particularly 
useful, but this is the major bound we will prove; the others follow easily. 
Furthermore, it has a somewhat nicer structure than the second bound. In 
Section 3.6 we show that the second bound is optimal. The last bound, which 
we discuss in the following, has the same asymptotics as the second bound. 
First, we observe that the number of errors E^^ of the universal 0^ predic-
tor is finite if the number of errors E^^ of the informed 0^ predictor is finite. 
Note that the error bound in terms of Sn actually holds for any two distributions 
^ and /x. The mixture property of ^ is only used to bound Sn by \nw~^. 

84 
3 Universal Sequence Prediction 
This is especially the case for deterministic /i, as E"^^ ^ 0 in this case^, i.e. 0^ 
makes only a finite number of errors on deterministic environments. This can 
also be proven by elementary means. Assume 2:1X2... is the sequence generated 
by /i and O^ makes a wrong prediction xf^ ^Xt. Since ^(xf^ 
\x^t)^C{^t\x<t), 
this implies ^(xt|x<t) < | . Hence ef^ = 1 < —In^(xt|x<t)/ln2 = dt/ln2. If 0^ 
makes a correct prediction, ef ^ =0<dt/ln2 is obvious. Using (3.18) this proves 
E^^ <Doo/^^'2<log2W~^. A combinatoric argument given in Section 3.6 shows 
that there are M and ji^M 
with Ef^ >log2|A^|. This shows that the upper 
bound E^ <log2\M\ for uniform w is sharp. From Theorem 3.36 we get the 
slightly weaker bound E^^ < 2Soc < 2i^oo < 21nit;-^ For more complicated 
probabilistic environments, where even the ideal informed system makes an 
infinite number of errors, the theorem ensures that the error regret E^^ ~E^^ 
is only of order y/E^. 
The regret is quantified in terms of the information 
content Dn of // (relative to ^), or the weight Wjj, of fi in ^. This ensures that 
the error densities En/n of both systems converge to each other. Actually, the 
theorem ensures more, namely that the quotient converges to 1, and also gives 
the speed of convergence E^^/E^^ 
=l-hO{{E^^)-'^^'^) 
—^1 for E^^-^00. 
If 
we increase the first occurrence of E^^^ in the theorem to E^ and the sec-
ond E^^ to E^^ we get the bound E^>Ef^ 
-2^/Ef^, 
which shows that 
no (causal) predictor 0 whatsoever makes significantly fewer errors than 0^. 
In Section 3.6 we show that the second bound for E^^—E^^ 
given in The-
orem 3.36 can, in general, not be improved, i.e. for every predictor 0 (and 
especially 0^) there exist M and fi E Ai such that the upper bound is es-
sentially achieved. See [HutOlc] for some further discussion and bounds for 
binary alphabet. 
3.3.3 Proof of Theorem 3.36 
The first inequality in Theorem 3.36 has already been proven (3.35). For the 
second inequality, let us start more modestly and consider constants 
A>0 
and B>0 that satisfy the linear inequality 
E®« - Ef" 
< A(E^^ +Et^) 
+ BSn. 
(3.37) 
If we could show 
ef«(x^t) - ef" (x^t) 
< A[ep (x^t) + ef- (x^t)] + Bst{x^t) 
(3.38) 
for all t < n and all x<t, (3.37) would follow immediately by summation and the 
definition oi E^ and Sn- With the abbreviations (3.12) and the abbreviations 
772 = xf^ and s = xf^ the, various error functions can then be expressed by 
ef^ = l—yg^ ef^ — \ — y^ and St — Yl,i{Vi~^i?'' Inserting this into (3.38) we 
get 
^ Remember that we named a probability distribution deterministic if it is 1 for 
exactly one sequence and 0 for all others. 

3.3 Error Bounds 
85 
N 
Vm-Vs < A[2-{ym^ys)]^B^{yi-Zif. 
(3.39) 
By definition of xf^^ and xf^ we have ym^Vi and Zs>Zi for all i. We prove 
a sequence of inequalities which show that 
N 
BY^{yi-Zif^A[2-{ym+ys)]-{ym-ys) 
> ... 
(3.40) 
is positive for suitable A>0 and B>0, which proves (3.39). For m = s (3.40) is 
obviously positive. So we will assume m^^s in the following. From the square 
we keep only contributions from i = m and i = s, 
... > B[{ym-Zmf-{-{ys-Zs)'^]^A[2-{ym^ys)]-{ym-ys) 
> ... 
By definition of y, z^ A4 and 5 we have the constraints ym+I/s ^ 1? Zm-^Zs<l^ 
ym^Vs^O 
and Zs>Zm^O. From the latter two it is easy to see that the square 
terms (as a function of Zm and Zg) are minimized by Zm = Zs = ^(^m + ^s). 
Furthermore, we define x:=ym—ys and increase (ym+y^) to 1. 
... > ^Bx^^A-x > ... 
(3.41) 
Expression (3.41) is quadratic in x and minimized by x* =^ ^. Inserting x* 
gives 
... > A - - ^ 
> 0 for 
2AB>1. 
(3.42) 
2B 
Inequality (3.37) therefore holds for any A > 0, provided we insert B = ^ , 
Thus we might minimize the r.h.s. of (3.37) w.r.t. A, leading to the upper 
bound 
E^^ - E ^ 
< ^2iE^^+E^^)Sn 
for 
A' = ^^^e^l 
EO^y ^^'"^^^ 
which is the first bound in Theorem 3.36. For the second bound we have to 
prove 
V 2 ( £ ^ « + S ^ ) 5 „ - 5 „ < V 4 ^ ^ 5 „ + S2. 
(3.44) 
If we square both sides of this expression and simplify we just get (3.43). 
Hence, (3.43) implies (3.44). The last inequality in Theorem 3.36 is a simple 
triangle inequality. This completes the proof of Theorem 3.36. 
• 
Note that also the third bound implies the second one: 
^ 
{Ep-Et>^f 
< 
2{E^^+E^'^)Sn 
^ 
{E^^-E^-~Sn? 
< AEt^Sn + Sl 
^ 
E^^-E^--Sn 
< ^AE^^Sn 
+ Sl 
where we only have used E^^ > E^^. Nevertheless, the bounds are not equal. 
In Section 3.9 we give an alternative direct proof of the second bound. 

86 
3 Universal Sequence Prediction 
3.4 Loss Bounds 
We now generalize the prediction framework to an arbitrary loss functions. A 
system is allowed to take an action yt^ given Xi...Xt-i and receives loss Ixtvt 
if xt is the next symbol of the sequence. No assumptions on t are necessary, 
besides boundedness. Bayes optimal universal A^ and Bayes optimal informed 
yl^ prediction schemes are defined and the total loss of A^ is bounded in 
terms of the total loss of A^^ similar to the error bounds. Convergence of 
instantaneous losses is also established. Various example loss functions are 
discussed. 
3.4.1 Unit Loss Function 
A prediction is very often the basis for some decision. The decision results in 
an action, which itself leads to some reward or loss. If the action itself can 
influence the environment we enter the domain of acting agents, which will 
be analyzed in the context of universal probability in later chapters. To stay 
in the framework of (passive) prediction we have to assume that the action 
itself does not influence the environment. Let (^xtyt ^ -K be the received loss 
when taking action yt E y, and x^ G A' is the t*^ symbol of the sequence. 
We assume that £ is bounded, which is trivially satisfied for finite X and 
y. Without loss of generality we normalize £ by linear scaling such that 0 < 
£xtyt ^ 1- F^^ instance, if we make a sequence of weather forecasts A! —{sunny, 
rainy} and base our decision, whether to take an umbrella or wear sunglasses 
y = {umbrella, sunglasses} on it, the action of taking the umbrella or wearing 
sunglasses does not influence the future weather (ignoring the butterfly effect). 
The losses might be 
Loss 
umbrella 
sunglasses 
sunny 
0.1 
0.0 
rainy 
0.3 
1.0 
Note the loss assignment even when making the right decision to take an 
umbrella when it rains because sun is still preferable to rain. 
In many cases the prediction of Xt can be identified or is already the action 
yt. The forecast sunny can be identified with the action wear sunglasses, and 
rainy with take umbrella. X = y in these cases. The error assignment of the 
previous section falls into this class together with a special loss function. It 
assigns unit loss to an erroneous prediction {£xtyt ~^ ^^^ Xt^Vt) and no loss 
to a correct prediction {£xtxt =0)-
For convenience we name an action a prediction in the following, even 
ii X ^y. 
The true probability of the next symbol being Xt^ given x^t-, is 
/i(xt|x<t). The expected loss when predicting yt is l£it[£xtyt]' The goal is 
to minimize the expected loss. More generally, we define the Ap prediction 
scheme 

3.4 Loss Bounds 
87 
y^^ 
: - argmin V p ( a : t | x < t ) 4 , y , , 
(3.45) 
ytey 
^-^ 
which minimizes the p-expected loss? As the true distribution is /x, the actual 
/i-expected loss when Ap predicts the t*^ symbol and the total /i-expected loss 
in the first n predictions are 
n 
Z^(x<t) := Et[4.,^], 
L^^ := Y^E[lt»{x^t)]. 
(3.46) 
t=l 
Let A be any (causal) prediction scheme (deterministic or probabilistic) with 
no constraint at all, predicting any y^ ^y 
with losses l^ and L^ similarly 
defined as (3.46). If // is known, A^ is obviously the best prediction scheme in 
the sense of achieving minimal expected loss 
L ^ 
< L^ 
for 
any 
A, 
(3.47) 
smce 
it^ix^t) 
= ^tixtyff^ 
= minEt4,j/, < Et4,y,^ = 'f (^<t) 
for any A. The predictor yl^, based on the universal distribution ^, is, again, 
of special interest. Theorem 3.36 generalizes to arbitrary loss functions. 
The loss bounds have the same form as the error bounds when substituting 
Sn ^ Dn in Theorem 3.36, so most of the discussion of Theorem 3.36 also 
applies here. Replacing D^ by Sn in Theorem 3.48 gives an invalid bound, so 
the general bound is slightly weaker. For instance, for JY = y = {0A}, 
^oo = 
argmin2;(-) is defined as the y that minimizes the argument. A tie is broken 
arbitrarily. If y is finite, then yf^ always exists. For an infinite action space y 
we assume that a minimizing y^^ ^y exists. This is, for instance, the case if y is 
compact and £xy is continuous in y, or for y = ]N^ if limy-^ooixy exists for all x 
and is larger or equal to £xy for most y. 

88 
3 Universal Sequence Prediction 
^11=0, ^io==l, 4 i = c < | , /i(l) = 0, z/(l) = 2c, and w^ = Wr,=^\, we get ^(l) = c, 
si=2c2, ^j^^=0, |(iM=4o=0, y(^^-l, / f ^ - 4 i = c ; hence Lf^-L^/^-c ^ 4c2=. 
25i4-2-Y/Lf^S'i. For convenience we collect the most important consequences 
of Theorem 3.48 in the following corollary. 
3.4.2 Loss Bound of Merhav &: Feder 
The first general loss bound with no structural assumptions on /i and i (ex-
cept boundedness) was derived in a survey paper by Merhav and Feder in 
[MF98, Sec.III.A.2]. They showed that the regret L^^-L^^ 
is bounded by 
f-maxV'^nDn for ie[0/rnax]' 
Assumiug irnax = 1 (general irnax cau be recov-
ered by scaling) their bound reads (in our notation) 
L^^ - Li^ 
< An < ^2nDn- 
(3.50) 
Later in Theorem 3.59 we prove 
t'{x<t) -it'ix^t) 
< atix^t) 
< 
^2dt{x<t)-
Taking the expectation E and the average ^X^tLi ^^^ using Jensen's inequal-
ity for the concave square root similarly to (3.21) or directly Theorem 3.19(i;i) 
shows (3.50). 
Bound (3.50) and our bound (Theorem 3.48) are in general incomparable. 
Since 2Z)oo is finite and L^^ <n, bound (3.50) can be at best a factor y/2 and 
an additive constant better than our bound. On the other hand, for large n 
and for L^^ < ^ our bound is tighter. The latter condition is satisfied if the 
best predictor A^ suffers small instantaneous loss < | on average. Significant 
improvement occurs if L^^ does not grow linearly with n, but is, for instance, 
finite (see Corollary 3.49, especially (i) and (ii)). 

3.4 Loss Bounds 
89 
3.4.3 Example Loss Functions 
The case X = y with unit error assignment Ixy == ^—^xy {^xy = ^ ior x = y and 
6xy=^0 for x^y) 
has already been discussed and proven in Section 3.3. 
y^^ 
= argminVp(xt|x<t)(l-(5a:t2/t) = argmaxp(xt|x<t) = 
xf^ 
xt 
In this case L^p = E^p is the total expected number of prediction errors. 
For A' = 3^ = {0,1}, like in the weather example above, Ap is a threshold 
strategy with y^p =aigmmy^{o^iy{piiiy 
+ poioy} = 0/l for pi < 7, where 7:= 
i J'i^Z^i^ -i 
aiid p^ = p(i|a:<t). In the special error case (^xy — ^ — ^xy> the bit 
with the highest p-probability is predicted (7=^). In the following we consider 
some standard loss functions for binary outcome A* = {0,1} and continuous 
action y in the unit interval y — [0,1]. The absolute loss is defined as i^y — 
|a:-y|G[0,l]. The Ap scheme predicts i//^^==argminy^[o,i]{pi(l-y)+/>oy} = 0/l 
for po < Pi- Since all predictions y lie in the subset {0,1} C [0,1], and \x — y\ = 
1 —<5xy for y G {0,1}, this case coincides with the binary error case above. 
The same holds for the a-loss \x — y\^ with 0 < a < l . The //-expected loss is 
ifp =/x(i|x<t) for the i with pi>\. 
For the quadratic loss £xy — {^—yf ^ [0,1] 
the action/prediction y^p =argminy^[o,i]{pi(l~y)^+Poy^}=Pi is proportional 
to the p-probability of a:^ = 1 and l^p = Et(l —p(xt|a:<t))^. For the a-loss 
\x—y\^ with a > 1 we get y^p = {l+ """V/PO/PI)""^. For arbitrary finite alphabet 
X and vector-valued predictions y the quadratic loss may be generalized to 
^xy = ^y^^xy-^b^y-^-Cx- 
The Hellinger loss can be written for binary outcome 
in the form Ixy = l-y/\l-x-y\ 
€ [0,1] with y^p = Pi/(Po + Pi) ^^d l^p = 
1 —(/xoPo+/^iPi)/vPo+Pi- The logarithmic loss ixy = —ln|l—x —y| G [0,oc] 
is unbounded. But since the corresponding action is yfp — p\ the expected 
loss is l^p — —Yitixip{xt\x^i). Hence if^—l^^ —dt^ and the total loss regret 
L^^ —L^^ = Dji < IniD"^ is finite anyway and Theorem 3.48 is not needed. 
Continuous outcome spaces X are briefly discussed in Section 3.7.5. 
3.4.4 Proof of Theorem 3.48 
The first inequahty in Theorem 3.48 was already proven in (3.47). For the 
second and last inequality, we start, as in Theorem 3.36, by looking for small 
constants A>0 and 5 > 0 , which satisfy the linear inequality 
L;J« < (A+ l ) L ^ + ( B + !)£>„. 
(3.51) 
If we could show 
^^(^<t) < A'l^-{x^t) 
+ B'dt{x^t), 
A ' : = A + 1, 
B':=B 
+ 1 (3.52) 
for all t<n and all cc<i, (3.51) would follow immediately by summation and 
the definition of Ln and D^. With the abbreviations m = yf^ and s = yf^ 

90 
3 Universal Sequence Prediction 
and the abbreviations (3.12) the loss and entropy can then be expressed by 
it^^Y^iVdis, 
^t"" ^Y^iVi^im and dt = Y.^yi\n^,. Inserting this into (3.52) we 
get 
N 
N 
N 
Y,yd^s < A'Y,y^i^m + B'^y^ln^ 
(3.53) 
i=l 
i = l 
2=1 
^* 
By definition (3.45) of y^^' and y^^ we have 
y ^ yi^im < ^ 
ydij 
and 
^ 
Ziiis < ^ 
Ziiij 
for all j . 
(3.54) 
i 
i 
i 
i 
Actually, we need the first constraint only for j = s and the second for j = m. 
In Section 3.9 we reduce the problem to the binary A^ = 2 case, which we will 
consider in the following. We take "^Zi^o instead of Yli^i ^^^ convenience. 
1 
1 
^ 
5'^?/^lnJ+^?/^(^'^^m-^^s) > 0 
(3.55) 
i=0 
^ 
i=0 
The cases iim > ^is^i and lis > UmS'i contradict the first/second inequality 
(3.54). Hence we can assume ^om ^ ^Os and lim <l\s- The symmetric case 
^Om ^ ^Os and ^im ^ ^is is proven analogously or can be reduced to the first 
case by renumbering the indices (O^-^l). Using the abbreviations a:=£om—^Os^ 
h\=lis~hm, 
c:=yi£im-\-yoios, y = yi = l-yo and z = zi = l-zo, we can write 
(3.55) as 
f{y,z) 
:= B^lyln^^ + {l-y)\n'jE^J+ 
A\l-y)a-yb 
+ Ac > 0 
(3.56) 
for zb<{l — z)a and 0<a,6,c,^,2:<l. Constraint (3.54) on y has been dropped 
since (3.56) will turn out to be true for all y. Furthermore, we can assume 
that d:= A\l —y)a — yb<0 since for d>0, / is trivially positive. Multiplying 
d with a constant > 1 will decrease /. Let us first consider the case z<^. We 
multiply the d term by 1/6> 1, i.e. replace it with A\l—y)^—y. 
From the 
constraint on z we know that | > j ^ . We can decrease / further by replacing 
f by j3^ and by dropping Ac. Hence, (3.56) is proven for z <^ if we can 
prove 
B^[yln^^+{l-y)\n\E^J 
+ A^il-y)j^^-y 
> 0 for 
^ < i. 
(3.57) 
In Section 3.9 we prove that it holds for ^ > ^ + 1; the case z > ^ is treated 
similarly. We scale d with l / a > l , i.e. replace it with A^{l — y)~y^. 
From the 
constraint on z we know that | < —^. We decrease / further by replacing ^ 
by ^ ^ and by dropping Ac. Hence (3.56) is proven ior z>^ if we can prove 
B'[y\n^^+{l-y)\n\^J-hA^il-y)-y^ 
> 0 for 
z>l 
(3.58) 

3.4 Loss Bounds 
91 
In Section 3.9 we prove that it holds for 5 > 4 + 1. So, in summary we proved 
that (3.51) holds for B>^-\-l. 
Inserting B=^-\-l 
into (3.51) and minimizing 
the r.h.s. w.r.t. A leads to the last bound of Theorem 3.48 with A= 
^jD^jL^y-. 
Actually, inequalities (3.57) and (3.58) also hold for B>_ \A^^, 
which, by 
the same minimization argument, proves the slightly tighter bound in The-
orem 3.48. Unfortunately, the current proof is very long and complex, and 
involves some numerical or graphical analysis for determining intersection 
properties of some higher-order polynomials. This or a simplified proof will 
be postponed. The cautious reader may check the inequalities (3.57) and (3.58) 
numerically iox B—\A-\-^. 
D 
3.4.5 
Convergence of Instantaneous Losses 
Since L^^—L^^ 
is not finitely bounded by Theorem 3.48, it cannot be used 
directly to conclude if^ —if^ —^0. It would follow from (^^-/i by continuity if 
/^^ and if^ were continuous functions of ^ and //. l^^ is a continuous piecewise 
linear concave function of /i, but l^^ is, in general, a discontinuous function 
of ^ (and /i). Fortunately, it is continuous at the one necessary point, ^ —/i. 
This allows us to bound if^—l^^ in terms oi 
^{xt\x<^t)—ii{xt\x^t). 
Bound (i) implies that the expected number of times t in which l^^ exceeds 
/^^ by more than e is finite and bounded by 2E~'^\tiw~^ ^ and the probabihty 
that the number of these events exceeds 2e~'^5~^h\w~^ is smaller than 5. 
Proof, {ii) follows from 
i 
i 
i 
i 
i 
]] 
i 
^' 

92 
3 Universal Sequence Prediction 
To arrive at the first inequality we added J2i^ii^im — ^is)i which is positive 
due to (3.54). \iis—iim\ < 1 since ie [0,1]. The last inequality follows from 
Lemma 3.11a, and dt-^0 was proven in Theorem 3.l9{ii). 
(i) follows by squaring (ii), taking the expectation E and sum Yl^=i^ ^^^ 
using (3.18). 
(iii) follows from the proof of Theorem 3.48 by inserting B = ^ + 1 = 
^yi^^^^Jdt-]-! into (3.52). Convergence to zero holds with /i-probability 1, since 
if^ < 1 is bounded. The losses /^^(x<t) itself need not converge. 
• 
Note, that the inequalities in (ii) and {iii) hold for all individual sequences. 
The sum/average is only taken over the current outcome Xt, but the history 
x<t is fixed. Bounds (ii) and {Hi) are in general incomparable, but for large 
t and for /^^ < | (especially if l^^^ -^0) bound {Hi) is tighter than bound (ii). 
3.4.6 General Loss 
Very few restrictions were imposed on the loss l^tyt i^ Theorem 3.48, namely 
that it is static and in the unit interval [0,1]. If we look at the proof of The-
orem 3.48, we see that the time-independence has not been used at all. The 
proof is still valid for an individual loss function l^^^y^ G [0,1] for each step 
t. The loss might even depend on the actual history x<t. The case of a loss 
il,^y^{x<:t) bounded to a general interval [imin/max] can be reduced to the 
unit interval case by rescaling £. We introduce a scaled loss i' 
0 < C.(^<^)^- "'^ 
; 
< 1' 
^here U := • 
The prediction scheme A' based on (.' is identical to the original prediction 
scheme Ap based on ^, since argmin in (3.45) is not affected by linear transfor-
mation of its argument. From y^p —yf^ it follows that l^^p = {l^p ~imin)/^A 
and L^n^ — i^n^ ~^rnin)/^A {D'n = Dn, siucc (. is uot iuvolvcd). Theorem 3.48 is 
valid for the primed quantities, since £'G [0,1]. Inserting L'^^i^ and rearranging 
terms we get 

3.5 Application to Games of Chance 
93 
3.5 Application to Games of Chance 
This section apphes the loss bounds to games of chance, defined as a sequence 
of bets, observations, and rewards. After a brief introduction, we show that if 
there is a profitable scheme at all, asymptotically the universal ^l^ scheme will 
also become profitable. We bound the time needed to reach the winning zone. 
It is proportional to the relative entropy of fi and ^ with a factor depending 
on the profit range and the average profit. We present a numerical example 
and attempt to give an information-theoretic interpretation of the result. 
3.5.1 
Introduction 
Consider investing in the stock market. At time t an amount of money St 
is invested in portfolio yt, where we have access to past knowledge a:<t (e.g. 
charts). After our choice of investment we receive new information Xt^ and 
the new portfolio value is Vf. The best we can expect is to have a probabilis-
tic model /i of the behavior of the stock market. The goal is to maximize 
the net yu-expected profit pt = rt — St. Nobody knows /x, but the assumption 
of all traders is that there is a computable, profitable /i they try to find or 
approximate. From Theorem 3.19 we know that Solomonoff'-Levin's universal 
prior ^{xt\x<^t) converges to any computable fj.{xt\x<:t) with probability 1. If 
there is a computable, asymptotically profitable trading scheme at all, the A^ 
scheme should also be profitable in the long run. To get a practically useful, 
computable scheme we have to restrict A^ to a finite set of computable dis-
tributions, e.g. with bounded Levin complexity Kt [LV97, Sec.7.5]. Although 
convergence of ^ to /x is pleasing, what we are really interested in is whether 
A^ is asymptotically profitable and how long it takes to become profitable. 
This will be explored in the following. 

94 
3 Universal Sequence Prediction 
3.5.2 Games of Chance 
We use Theorem 3.60 to estimate the time needed to reach the winning thresh-
old when using A^ in a game of chance. We assume a game (or a sequence of 
possibly correlated games) that allows a sequence of bets and observations. In 
step t we bet, depending on the history x<t, a certain amount of money st, 
take some action yt, observe outcome Xt, and receive reward r^. Our profit, 
which we want to maximize, is p^^rt-Ste 
[Pmin^Vmax]^ where [pmin^Vmax] is 
the [minimal,maximal] profit per round, and p/\ '-^Pmax—Pmin is the profit 
range. The loss, which we want to minimize, can be defined as the negative 
profit, ixtyt ——Pt- The probability of outcome Xt, possibly depending on the 
history x<t, is /j.{xt\x<^t)' The total /i-expected profit when using scheme Ap 
is P^p = —L^p. If we knew /i, the optimal strategy to maximize our expected 
profit is just yl^. We assume F^^ >0 (otherwise there is no winning strategy at 
all, since P^^ ^Pn^ Vp). Often we are not in the favorable position of knowing 
/i, but we know (or assume) that /leM 
for some M, for instance, that /i is 
a computable probability distribution. From Theorem 3.60 we see that the 
average profit per round p^^ := -^Pn^ of the universal A^ scheme converges 
to the average profit per round p^^ :— ^P^^ of the optimal informed scheme, 
i.e. asymptotically we can make the same money even without knowing /x, by 
just using the universal A^ scheme. Theorem 3.60 allows us to lower-bound 
the universal profit P^^ 
P„^« > 
Pn^" - PADn 
- ^J'i{nPma.-P^-)PADn 
+ P^Dl 
(3.61) 
The time needed for A^ to perform well can also be estimated. An interesting 
quantity is the expected number of rounds needed to reach the winning zone. 
Using P^p^ >0 one can show that the r.h.s. of (3.61) is positive if and only if 
n > ?^^%V^.D.^ 
(3.62) 
Theorem 3.63 (Time to win) Let there be sequences xiX2... over a fi-
nite alphabet A' drawn with probability //(xi.n) for the first n symbols. 
In step t we make a bet, depending on the history x<t, take some action 
yt, and observe outcome Xt- Our net profit is pt € \pmax-PA,Pmax]' The 
yip-system (3.45) acts as to maximize the p-expected profit. P^^ is the 
total and p^" = ~Pn^ is the average expected profit of the first n rounds. 
For the universal A^ and for the optimal informed A^ prediction scheme 
the following holds: 
(i) p^^ = p ^ - o ( n - l / 2 ) - ^ p ^ 
ii) n >(^f.fc^ 
A p^->0 
for 
= » 
n —> 00, 
P^« > 0, 
where w^~e 
^^ is the weight (3.5) of /2 in ^. 

3.5 Application to Games of Chance 
95 
By dividing (3.61) by n and using Dn<k^ (3.18) we see that the leading order 
oi Pn^ -pn^ is bounded by ^/^PAPmaxk^/n, 
which proves (i). The condition 
in (ii) is actually a weakening of (3.62). P^^ is trivially positive for Pmin>^, 
since in this wonderful case all profits are positive. For negative Pmin the 
condition of {ii) implies (3.62), since PA>Pmax^ and (3.62) implies positive 
(3.61), i.e. P^^ >0, which proves {ii). 
If a winning strategy A with p{l>e>0 
exists, then yl^ is asymptotically 
also a winning strategy with the same average profit. 
3.5.3 
Example 
Let us consider a game with two dice, one with two black and four white faces, 
the other with four black and two white faces. The dealer who repeatedly 
throws the dice uses one or the other die according to some deterministic 
rule, which correlates the throws (e.g. the first die could be used in round t 
iff the t^^ digit of TT is 7). We can bet on black or white, the stake s is 3$ in 
every round, and our return r is 5$ for every correct prediction. 
The profit is p^-^^rdx^yt ~^' The coloring of the dice and the selection strat-
egy of the dealer unambiguously determine ji. ii{xt\x<^t) is | or | , depending 
on which die was chosen. One should bet on the more probable outcome 
(7=1 in Section 3.4.3). If we knew /i the expected profit per round would be 
Pn^ —Pn^ ==|r —5==^$>0. If we do not know ji we should use Solomonoff-
Levin's universal prior with Dn< A;^ = iC(/i)-ln2, where K{ii) is the length of 
the shortest program coding /x (see Subsection 3.2.9). Then we know that bet-
ting on the outcome with higher ^ probability leads asymptotically to the same 
profit (Theorem 3.63(i)), and yl^ reaches the winning threshold no later than 
^t/ires/i =9001n2-K(/i) (Theorem 3.63(u)) or sharper n^^^esh = 330ln2-i^(jLi) 
from (3.62), where pmax=f — s — 2% and p/\ = r = 5$ have been used. 
If the die selection strategy reflected in fi is not too complicated, the ^l^ 
prediction system reaches the winning zone after a few thousand rounds. The 
number of rounds is not really small because the expected profit per round is 
one order of magnitude smaller than the return. This leads to a constant of 
two orders of magnitude size in front of K{fi). Stated otherwise, it is due to 
the large stochastic noise, which makes it difficult to extract the signal, i.e. 
the structure of the rule /i (see next subsection). Furthermore, this is only a 
bound for the turnaround value of Uthresh- The true expected turnaround n 
might be smaller. However, for every game for which there exists a computable 
winning strategy with p^ ><s>0, yl^ is guaranteed to get into the winning zone 
for some 
n^K{fi). 
3.5.4 Information-Theoretic Interpretation 
We try to give an intuitive explanation of Theorem 3.63(ii). We know that 
£,{xt\x<:t) converges to ^{xt\x<:^t) for t-^00. In a sense, A^ learns /i from past 
data x^t' The information content in // relative to ^ is Z^oo/in2</?^/ln2. One 

96 
3 Universal Sequence Prediction 
might think of a Shannon-Fano prefix code oiv^M 
of length ^ki,/\n2^, which 
exists since the Kraft inequality Xlj,2~ ^^/^^^ ^ Y^^w^, < 1 is satisfied. So, 
A:^/ln2 bits have to be learned before A^ can be as good as /l^. In the worst 
case, the only information conveyed by Xt is in form of the received profit Pf 
Remember that we always know the profit pt before the next cycle starts. 
Assume that the distribution of the profits in the interval [Pmin^'Pmax] is 
mainly due to noise, and there is only a small informative signal of amplitude 
p^^. To reliably determine the sign of a signal of amplitude p^^, disturbed by 
noise of amplitude p^^, we have to resubmit a bit 0{{pA/Pn^Y) 
t™^^ (this 
reduces the standard deviation below the signal amplitude p^^)^ To learn /x, 
A:^/ln2 bits have to be transmitted, which requires 
n>0{{p^/p^^^Y)'k^^/\n2 
cycles. This expression coincides with the condition in (zi). Identifying the 
signal amplitude with p^'^ is the weakest part of this consideration, as we 
have no argument why this should be true. It may be interesting to make 
the analogy more rigorous, which may also lead to a simpler proof of (ii) not 
based on Theorems 3.48 and 3.60 with its rather complex proof. 
3.6 Optimality Properties 
In this section we discuss the quality of the universal predictor and the bounds. 
We show that there are M and fi^M 
and weights Wj^ such that the derived 
error bounds are tight. This shows that the loss bounds cannot be improved 
in general. We show Pareto optimality of ^ in the sense that there is no 
other predictor which performs at least as well in all environments iy e M 
and strictly better in at least one. Optimal predictors can always be based on 
mixture distributions ^. This still leaves open how to choose the weights. We 
give an Occam's razor argument that the choice ILV —2~^^^\ where K{u) is 
the length of the shortest program describing i/, is optimal. 
3.6.1 Lower Error Bound 
We want to show that there exists a class M of distributions such that any pre-
dictor 0 ignorant of the distribution ji^M 
from which the observed sequence 
is sampled must make some minimal additional number of errors as compared 
to the best informed predictor O^. For deterministic environments a lower 
bound can easily be obtained by a combinatoric argument. Consider a class 
M. containing 2^ binary sequences such that each prefix of length n occurs ex-
actly once. Assume any deterministic predictor 0 (not knowing the sequence 
in advance), then for every prediction xf of 0 at times t<n there exists a 
sequence with opposite symbol xt = l—xf. Hence, £^^ >E'^—n==log2|A^| is 
a lower worst-case bound for every predictor 6^, (this includes 6^^, of course). 
This shows that the upper bound E^ <log2|A1| for uniform w obtained in 
the discussion after Theorem 3.36 is sharp. In the general probabilistic case 
we can show by a similar argument that the upper bound of Theorem 3.36 is 

3.6 Optimality Properties 
97 
sharp for 0^ and "static" predictors, and sharp within a factor of 2 for general 
predictors. We do not know whether the factor-2 gap can be closed. 
Proof, (i) The proof parallels and generalizes the deterministic case. Consider 
a class A4 of 2'^ distributions (over binary alphabet) indexed by a = ai...an G 
{0,1}^. For each t we want a distribution with posterior probability |(H-£) 
for Xt = l and one with posterior probability |(1—e) for a;t = l independent of 
the past a:<t with 0 < £ < | . That is 
[ 1(1 -6) 
for xt 
^at 
We are not interested in predictions beyond time n, but for completeness we 
may define jia to assign probability 1 to Xt = l for all t>n. 
If fi = /la^ the 
informed scheme 0^ always predicts the bit that has highest /x-probability, 
i.e. 
yf^=at 
Since E^f^ is the same for all a, we seek to maximize E^ for a given predictor 
0 in the following. Assume 0 predicts yf (independent of history x<t). Since 
we want lower bounds, we seek a worst-case fi. A success yf = Xt has lowest 
possible probability |(1—e) if at = l — yf, 
=> 
ef = l-/.,.(yf) = i(l+e) 
=> 
E^ = ^{l + e). 
So we have ef ~ef^ 
= e and E^ —Ef^' — ne for the regrets. We need to 
eliminate n and e in favor of 5t, S^, and Ef^. If we assume uniform weights 
ty^^ =2""' for all pba we get 
n 
n 
i{xv.u) = X^t«M<./Xa(2ri:„) = 2"" J ] 
^ 
/x„.(xt) = 2"" f j 1 = 2-«, 
a 
i=la^e{0,l} 
t=l 

98 
3 Universal Sequence Prediction 
i.e. ^ is an unbiased Bernoulli sequence (^(xt|x<t) = ^). 
=> 
st{x<t) = ^{^ 
- fJ^atMf 
= i^^ 
and 
5n - f £^ 
So we have £ = y/2st^ which proves the instantaneous regret formula e^ —e^^ = 
^/2s^ for static 0. Inserting €= J^Sn 
into E^^ and solving w.r.t. \/2n, we 
get v ^ = \ / S V ^ + A / 4 E f M ^ . So, we finally get 
E^ - E^^ 
= ne ^ v ^ v ^ 
- 5 , + V 4 ^ ^ 5 n + 52, 
which proves the total regret formula in (i) for static 0. We can choose^ 
yf^ =0 to be a static predictor. Together this shows (i). 
{ii) For non-static predictors, at = l~yf 
in the proof of (i) depends on x<t, 
which is not allowed. For general, but fixed at we have ef (x<t) = l—/iat(l/?)-
This quantity may assume any value between ^{1—e) and ^(l-fe) when aver-
aged over x<:t and is, hence, of little direct help. But if we additionally average 
the result over all environments //a, we get 
n 
n 
n 
t=l 
t=l 
t=l 
whatever 0 is chosen: a sort of no-free-lunch theorem [WM97], stating that on 
uniform average all predictors perform equally well/poorly. The expectation 
of E^ w.r.t. a can only be | n if E^ > | n for some a. Fixing such an a and 
choosing ij.=ij.a,we get E^-E^^>^ne=l[Sn+\/4:E^^^Sn 
+ Sl], and similarly 
,o_ef,>i,= 
y2st{x^t). 
D 
Since dt/st='l-\-0{e'^) 
we have Dn/Sn-^l 
for e-^0. Hence the error bound 
of Theorem 3.36 with Sn replaced by Dn is asymptotically tight for E^^ /Dn —^ 
oc (which implies e-^0). 
This shows that without restrictions on the loss 
function that exclude the error loss, the loss bound in Theorem 3.48 can also 
not be improved. Note that the bounds are tight even when M is restricted 
to Markov or i.i.d. environments, since the presented counterexample is i.i.d. 
Finally, E^-E^^^ 
=ne^nJ'^-^y/2nDn, 
which shows that the bound (3.50) 
of Merhav and Feder is also asymptotically tight. 
A set M independent of n leading to a good (but not tight) lower bound 
IS M = {111,112} with //i/2(l|^<0 = \^^t 
with St — min{^,\/lnw;~^^/v^lnt}. 
For Wy^^ ^'^A^2 and n-^00 one can show that E^^ —E^^^ ^ j^Y^^S^^4rm^ 
(Problem 3.6). 
Unfortunately, there are many important special cases for which the loss 
bound (3.48) is not tight. For continuous 3^ and logarithmic or quadratic loss 
^ This choice may be made unique by slightly non-uniform ^'^^a — Fir^i [i + (i "~^*)^1 
with (5<1. 

3.6 Optimality Properties 
99 
function, for instance, we have seen that the regret L^ —L^ <\KIW~^ <CXD is 
finite. For arbitrary loss function, but /i bounded away from certain crit-
ical values, the regret is also finite. For instance, consider the special er-
ror loss, binary alphabet, and \ji{xt\x<^t) — \\ >^ for all t and x; 0^ pre-
dicts 0 if /ji{0\x^t) > |- If ^Iso ^(0|x<t) > | , then 0^ makes the same pre-
diction as 0^, while for ^(0|a:<t) < | the predictions differ. In the latter 
case |^(0|x<t)—/x(0|a:<t)| > e. Conversely for /i(0|x<t) < | . So in any case 
ep-ef^ 
< ^[^{xt\x^t)-Kxt\x<t)]'^. 
Using Definition (3.34) and Theo-
rem 3.19(i) we see that E^—E%^ 
< ^lnit;~^ < CXD is finite too. Neverthe-
less, Theorem 3.64 is important as it tells us that bound (3.48) can only be 
strengthened by making further assumptions on i or M. 
3.6.2 Pareto Optimality of ^ 
In this subsection we want to establish a different kind of optimality property 
of ^. Let !F{ji^p) be any of the performance measures of p relative to /i con-
sidered in the previous sections (e.g. St, or Dn-> or Ln-i •••)• I^ ^^ easy to find p 
more tailored toward ji such that ^(/i,p) < J^(/i,^). This improvement may be 
achieved by increasing it;^, but probably at the expense of increasing !F for 
other z/, i.e. !F{v^p)> T{v^£^) for some l/^^A. 
Since we do not know /i in ad-
vance, we may ask whether there exists a p with better or equal performance 
for all UGM. and a strictly better performance for one UEM. 
This would 
clearly render ^ suboptimal w.r.t. to T. We show that there is no such p for 
all performance measures studied in this book. 
Proof. We first prove Theorem 3.66 for the instantaneous expected loss k. 
We need the more general p-expected instantaneous losses 
ltpi^<t) '= X^p(x,|:c<t)4,^^^ 
(3.67) 
for a predictor A. We want to arrive at a contradiction by assuming that 
^ is not Pareto optimal, i.e. by assuming the existence of a predictor^ A 
^ According to Definition 3.65 we should look for a p, but for each deterministic 
predictor A there exists a p with A = Ap. 

100 
3 Universal Sequence Prediction 
with /^ <l:^J for all ueM 
and strict inequality for some u. Implicit to this 
assumption is the assumption that /^ and ifj exist; /^ exists iff i^(xt|x<t) 
exists iff z^(x<t)>0 iff Wiy{x<^t)>0. 
The two equalities follow from inserting (3.7) into (3.67). The strict inequality 
follows from the assumption and tt;iy(x<t) >0. The last inequality follows from 
the fact that A^ minimizes by definition (3.45) the ^-expected loss (similarly 
to (3.47)). The contradiction lA<l^c proves Pareto optimality of ^ w.r.t. If. 
In the same way we can prove Pareto optimality of ^ w.r.t. the total loss 
Ln by defining the p-expected total losses 
n 
n 
^np ••= EE/'(^<*)^^p(^<«) = HT.P^'^^-^y-^y^ 
(3.68) 
t=l 
X<t 
t = l 
Xl:t 
for a predictor A, and by assuming L^^<L^l 
for all u and strict inequality for 
some z/, from which we get the contradiction L^c — ^^^v^^y 
< Y^^'^vL^t = 
L^i ^^n£ ^^^^ ^^^ ^^^P ^f (3.5). The instantaneous and total expected errors 
et and En can be considered as special loss functions. 
Pareto optimality of ^ w.r.t. St (and hence Sn) can be understood from 
geometric insight. A formal proof for st goes as follows: With the abbreviations 
i^xt, 
yui = i^{xt\x<t)^ Zi^i[xt\x<:t), 
ri^p{xt\x^t)^ 
and Wy^Wy{x<:t)>^, we 
ask for a vector r with ^i{yvi — TiY<Y^^{yvi — Zi)'^ Vi/. This implies 
0 > ^Wy\^^{yyi-rif 
-^{y^,i-Zif 
V 
i 
i 
= 5Z ^^ [ X] ~'^y^i^i + ^i + ^y^i^i - ^i\ 
u 
i 
= Y^-2z^n+r1 
+ 2ziZi-z1 
= Y^{ri-Zif 
> 0, 
i 
i 
where we have used Xl^w^i/= 1 and Ylu^yVi^i^^i 
(3-7). 
^>Y.i{ri-ZiY>{) 
implies r — z^ proving Pareto optimality of ^ w.r.t. St- Similarly for dt^ the 
assumption Zli^^^ln^ <X]-?/i,iln^ Mv implies 
0 > Y^Wy\Y^yyi\n^^ 
-yyi\n-^\^ 
= Y^w^,^yyi\n~ 
= ^ z ^ l n — > 0 
V 
i 
1/ 
i 
i 
which implies r = z, proving Pareto optimality of ^ w.r.t. dt- The proofs for 
Sji and Dn are similar. 
• 
We have proven that (^ is Pareto optimal w.r.t. St, Sn^ dt and Dn in a 
strong sense, that is, there is also no p 7^ ^ with same performance as ^ in 
all environments. In the case of e^, En, h and L^, there are other p ^ ^ 

3.6 Optimality Properties 
101 
with T{iy^p) = J^(y^^)\lv^ but the actions/predictions they invoke are unique 
(y^p = y^i) (if ties in argmax^^ are broken in a consistent way), and this is all 
that counts. 
Note that ^ is not Pareto optimal w.r.t. to all thinkable performance mea-
sures. Counterexamples can be given for ^(^^,0 —Z]xtl^(^*l^<*)~^(^*l^<*)l" 
for a^2 
(see Problem 3.5). Nevertheless, for all measures that are relevant 
from a decision-theoretic point of view, i.e. for all loss functions It and L^, ^ 
has the welcome property of being Pareto optimal. 
3.6.3 Balanced Pareto Optimality of ^ 
Pareto optimality should be regarded as a necessary condition for a prediction 
scheme aiming to be optimal. From a practical point of view, a significant 
decrease of T for many v may be desirable, even if this causes a small increase 
of T for a few other z/. The impossibility of such a "balanced" improvement 
is a more demanding condition on ^ than pure Pareto optimality. The next 
theorem shows that A^ is also balanced Pareto optimal. We only consider the 
performance measure L^ and suppress the index n for convenience. 
This means that a weighted loss decrease Ay^ by using A instead of A^ is com-
pensated by an at least as large weighted increase Ac on other environments. 
If the increase is small, the decrease can also only be small. In the special case 
of only a single environment with increased loss Ax, the decrease is bound 
by Ar^ < ~f^\Ax\^ i.e. an increase by an amount Ax can only cause a decrease 
by at most the same amount times a factor ^ . An increase can only cause a 
smaller decrease in simpler environments, but can cause a scaled decrease in 
more complex environments. Finally, note that pure Pareto optimality (3.66) 
follows from balanced Pareto optimality in the special case of no increase 
Ac = 0, 
Proof. Z\>0 follows from A = J2u'^4L^-L^^]=Lf-Lt^>^. 
where we have 
used hnearity of Lp in p and L^^ ^^f- 
The remainder of Theorem 3.69 is 
obvious from 0<A = Ac — Aj^ and by bounding the weighted average zi^ by 
its maximum. 
D 

102 
3 Universal Sequence Prediction 
The term Pareto optimal has been taken from the economics hterature, 
but there is the closely related notion of unimprovable strategies [BM98] or 
admissible estimators [Fer67] in statistics for parameter estimation, for which 
results similar to Theorem 3.66 exist. Furthermore, it would be interesting 
to show under which conditions the class of all Bayes mixtures (i.e. with 
all possible values for the weights) is complete in the sense that every Pareto 
optimal strategy can be based on a Bayes mixture. Pareto optimality is sort of 
a minimal demand on a prediction scheme aiming to be optimal. A scheme that 
is not even Pareto optimal cannot be regarded as optimal in any reasonable 
sense. Pareto optimality of ^ w.r.t. most performance measures emphasizes 
the distinctiveness of Bayes mixture strategies. 
3.6.4 On the Optimal Choice of Weights 
In the following we indicate the dependency of ^ on it; explicitly by writing 
^-u;. We have shown that the ^l^^ prediction schemes are (balanced) Pareto 
optimal, i.e. that no prediction scheme A, whether based on a Bayes mixture 
or not, can be uniformly better. Least assumptions on the environment are 
made for M which are as large as possible. In Section 2.4 we have discussed 
the set M of all enumerable semimeasures, which we regarded as sufficiently 
large from a computational point of view (see [Sch02a] for even larger sets, 
but which are still in the computational realm). Agreeing on this M still 
leaves open the question of how to choose the weights (prior beliefs) w^, since 
every ^y^ with Wj^>0\/p is Pareto optimal and leads asymptotically to optimal 
predictions. 
We have derived bounds for the mean squared sum S^ <hiw~^ and for the 
loss regret L^J^- —L^y <2\nw~^^-2y/\mii^^T^. 
All bounds decrease mono-
tonically with increasing Wj^. So it is desirable to assign high weights to all 
V ^ M. Due to the (semi)probability constraint Yl^^i^ ^ 1' ^^^ ^^^ ^^ ^^^ 
a compromise. In the following we argue that in the class of enumerable 
weight functions with short program there is an optimal compromise, namely 
Wy — 2~^^^\ which gives Solomonoff-Levin's prior. 
Consider the class of enumerable weight functions with short programs, 
namely V:={v(.):>l->iR+with X]^t;^<landK(i;) = 0(l)}. Let K ; ^ : = 2 - ^ ( ^ ) 
and i;(.)GV. Theorem 2.10(mi) says that K{x)<-\og2P{x)+K{P)+0{l) 
for 
all a; if P is an enumerable discrete semimeasure. Identifying P with v and x 
with (the program index describing) v we get 
\nw-^ 
< l n ^ ; - V O ( l ) . 
This means that the bounds for ^^^ depending on \nw~^ are at most 0(1) 
larger than the bounds for ^y depending on lni;~^. So we lose at most an 
additive constant of order 1 in the bounds when using ^yj instead of ^^. In 
using Solomonoff-Levin's prior ^yj we are on the safe side, getting (within 
0(1)) best bounds for a//environments. 

3.7 Miscellaneous 
103 
Since the above justifies the use of Solomonoff-Levin's prior, and Solomonoff-
Levin's prior assigns high probability to an environment if and only if it has 
low (Kolmogorov) complexity, one may interpret the result as a justification 
of Occam's razor. But note that this is more of a bootstrap argument, since 
we used Occam's razor in Section 2.1 to justify the restriction to enumerable 
semimeasures. We also considered only weight functions v with low complexity 
K{v) = 0{l). What did not enter as an assumption but came out as a result is 
that the specific universal weights ^^ = 2-^^^) are optimal. See Problem 3.7 
for a discussion of the (non) uniqueness of Wj^. 
3.6.5 
Occam's razor versus No Free Lunches 
We do not regard Theorem 3.69 as a no-free-lunch (NFL) theorem [WM97]. 
Since most environments are completely random, a small concession on the 
loss in each of these completely uninteresting environments provides enough 
margin A'u to yield distinguished performance on the few nonrandom (in-
teresting) environments. Indeed, we would interpret the NFL theorems for 
optimization and search in [WM97] as balanced Pareto optimality results. In-
terestingly, whereas for prediction only Bayes mixtures are Pareto optimal, for 
search and optimization every algorithm is Pareto optimal. There is an ongo-
ing battle between believers in Occam's razor and believers in no-free-lunches 
that cannot be dealt with here [StoOl, SH02]. 
3.7 Miscellaneous 
This section generalizes the setting and results obtained so far in various ways. 
First, we consider multistep/delayed predictions, where the next h / the /i*^-
next symbol shall be predicted. We show convergence of ^ to /i i.m.(s. for 
bounded h). Second, we generalize the setup to continuous probability classes 
M — {iie} consisting of continuously parameterized distributions /x^ with pa-
rameter 9^]R . Under certain smoothness and regularity conditions a bound 
for the relative entropy between fi and ^, which is central for all presented 
results, can still be derived. The bound depends on the Fisher information 
of fi and grows only logarithmically with n, the intuitive reason being the 
necessity to describe 0 to an accuracy 0(n~^/^). Third, we describe two ways 
of using the prediction schemes for partial sequence prediction, where not ev-
ery symbol needs to be predicted. Performing and predicting a sequence of 
independent experiments and online learning of classification tasks are spe-
cial cases. Fourth, we compare the universal prediction scheme studied here 

104 
3 Universal Sequence Prediction 
to the popular predictors based on expert advice (PEA). Although the al-
gorithms, the settings, and the proofs are quite different, the PEA bounds 
and the error bound derived here have the same structure. Finally, we outline 
possible extensions of the presented theory and results, including infinite al-
phabets, more active systems influencing the environment, learning aspects, 
a unification with PEA, and the minimal description length principle. 
3.7.1 Multistep Predictions 
Introduction. In multistep prediction we want to predict Xf.n from x<t. For 
instance, every day a weather forecaster in the morning of day t predicts 
the weather for the next three days t, t + l, and t-\-2 = n. Up to now we 
have considered prediction problems with a lookahead of one time-step only: 
Given x<t, predict Xt. Greedy minimization of the expected loss lt{x<ct) at 
time t was optimal. Looking farther ahead (> t) was not necessary, because 
the prediction/decision/action yt has no influence on the environment /i. For 
acting agents, described in detail in later chapters, multistep lookahead is 
necessary for optimal actions. Another application of multistep predictions 
is 'delayed sequence prediction', in which not the next, but next-to-next or 
h^^-next symbol shall be predicted. 
Notation and basic relations. We are interested in multistep posteriors 
p{xt:n\x<t)—p{xi:n)/p{x<t)i 
which generalize the one-step posteriors {n = t) 
considered so far. We abbreviate p'^:^ \— p{x^:^\x^t)-, where •• are any su-
perscripts (e.g. empty or '). We define the conditional probability vector 
p^.^ \= p^.^i^-\x^t) GiR^, where N — \X\^~'^^^ and the i*^ component of vec-
tor pt:n is punij'lx<t) with identification {!,...,A^}3i = Xt-n^^^'^'^^• 
Let /G 
{a^h^d^h.s} be any of the distances defined in (3.10), i.e. f{y^z) = J2i=if{yi^^i) 
with a{y,z) = \y-z\, 
b{y,z) = 2/|lnf|, d{y,z) = ylnf, h{y,z) = 
{^-y/z)^, 
5(i/,z) = (?/-2;)^. We define/t:n(x<t):=/(//t:n,6:n), generalizing (3.13)-(3.17). 
The definitions /t(x<t) :=/t:t(x<t) and Fn:=Zt=iWtl 
F e {A,B ,D ,H ,S] 
are consistent with (3.13)-(3.17). Lemma 3.11 (b-d 
< a < V2d, h < d, 
s < d) implies bf.n-dun 
< cit-.n < V^d^, 
hf.n < d^n, Sf.n < df.n- We de-
fine Et:k[f{xi:k)] •=Y!xt..k^t:kf{xi±), 
cf. (3.4). For the relative entropy we 
have Dn — c?i:n and Et:k[dk-^i:n] — df.n — df.k for t <k <n, which implies 
E[(it:n] = ^n —^t-i =X]^=tE[djt]>0, and d^n is monotone increasing in n. 
Convergence i.m.s. for bounded horizon. Henceforth, we no longer need 
the Hellinger distance, and we use h for the horizon. Assume we want to 
predict the next h symbols, i.e. n = Ut = t-\-h—l. We want to determine 
how fast ^;,^^ =<^(4^^^^_i|x<t) converges to p'^.^^ =/i(x;^^^^_ Jx<t). To prove 
convergence i.m.s. we have to bound the expectation sum of Sfnt ^ ^^t-nt — 

3.7 Miscellaneous 
105 
oo 
oo 
oo 
rit 
oo 
t = l 
t = l 
t=l k=t 
k=l 
(3.71) 
In the second inequality we have used that the number of times dk>0 occurs 
for some k in the double sum is min{/i,A:} < h. The bound implies at:nt~^^ 
i.m.s., which implies ^[.^^ ^^^/ij.^^ i.m.s. by dropping the sum in the definition 
of at:nt' The bound loosens by a factor of h for h-step prediction as compared 
to 1-step prediction. The same bound holds for bounded horizon ht\—nt—t-\-
l<h<oo\/t 
(increase YlT=t ^^ Sfe=t~ )' ^•^• 
^{^t:nt\^<t) ~^ f^{^t:nt\^<t) 
i.m.s. for t —> OO if /it := n^ — t + 1 </i < 00Vt. 
Delayed sequence prediction. A delayed feedback, where at time t, x is 
only known up to time t — h for some delay h, is common in many practical 
problems. This is equivalent to predicting Xn^ from x<t with nt=t+h—l. 
The 
probability of a:^^, given x<t, is 
Pi^nM<t) 
'= 
Yl 
Pun, 
= 
Y . 
P(^<nt^nt)/Pi^<t)' 
Using 
Yl^^^^nM<^^ 
~ ^^^'riM<^^\ 
- 
Yl 
\^t--nt - P^t:nt\ = 
^t-.n, 
and bound (3.71) we get 
which implies (^(j:^J:r<t) ^ ^ (i{x'^^\x<^t) i.m.s. The loss bounds of Theorem 
3.59(2,^2) also generalize to the delayed case. Loss bounds similar to Theo-
rem 3.59(iu) and Theorem 3.48 should also be derivable. 
Convergence i.m. for arbitrary horizon. Convergence i.m.s. does gen-
erally not hold for unbounded horizon ht (see Problem 3.15). Remarkably, 
convergence i.m. holds nevertheless: For any limit path n>t-^oo we have 
lim E[dt:n] = lim [Dn-Dt-i] 
= lim D^ - lim A - i = Doc-Doo 
= 0. 
t,n—>oo 
t,n—>-oo 
n—^oo 
t—>-CXD 
So for any nt=t-{-ht — l we have ^E[a^.^^] <E[dt:nt] —^0? which implies 
^(^t:nj^<t) *^^/^(^t:nj^<t) 
^ud ^(^n, k<t) ^"^^ A t « , k<t) 
for any/i*. 
Convergence i.m. is weaker than convergence w.p.l and i.m.s., and is poten-
tially slow. We expect cases where convergence is very slow when ht grows 
very fast. We do not know whether convergence is reasonably fast for slowly 
growing horizons, e.g. for ht — logt (or ht = t). 

106 
3 Universal Sequence Prediction 
3.7.2 Continuous Probability Classes Ai 
We have considered thus far countable probability classes M, which makes 
sense from a computational point of view as emphasized in Section 3.2.9. On 
the other hand, in statistical parameter estimation one often has a continuous 
hypothesis class (e.g. a Bernoulli(^) process with unknown ^G[0,1]). Let 
be a family of probability distributions parameterized by a d-dimensional 
continuous parameter 0. Let /i = /i6>o G A^ be the true generating distribution 
and ^0 be in the interior of the compact set 0. We may restrict A^ to a 
countable dense subset hke {/IQ} with computable (or rational) 0. If OQ is 
itself a computable real (or rational) vector then Theorem 3.60 applies. From 
a practical point of view, the assumption of a computable ^o is not so serious. It 
is more from a traditional analysis point of view that one would like quantities 
and results depending smoothly on ^, and not depending in a weird fashion 
on the computational complexity of 6. For instance, the weight w{9) is often 
a continuous probability density 
^{xi:n) '= j d0w{e)-^e{xi,n), 
[ dO w{0) - 1, 
w{e) > 0. (3.72) 
Jo 
Jo 
The most important property of ^ used in this book is ^(^i-^) >Wiy'h'{xi:n) 
which was obtained from (3.5) by dropping the sum over z/. The analogous 
construction here is to restrict the integral over 0 to a small vicinity Ns oiO. 
For sufficiently smooth /x^ and w{0) we expect 
£^{xi:n)^\Ns^\-w{9)'^e{xi:n)i 
where \Ns^ \ is the volume of A^^^. This in turn leads to Dn'^^^w~^^-\a\N^^ |~-^, 
where w^:=w{Oo). Ns^ should be the largest possible region in which In/i^ is 
approximately flat on average. The averaged instantaneous, mean, and total 
curvature matrices of In// are 
jt{x<:t) := Et[\/0\nfi0{xt\x^t)'^jlnij.0{xt\x^t)]\e=eo^ 
Jn '-= \Jn, 
n 
J„ : - ^E[it(a:<t)] = E[V,ln 
/^^(^i:n)Vjln/i^(a:i..n)]|0=.^o- (3-73) 
They are the Fisher information of JJL and may be viewed as measures of the 
parametric complexity of ^Q dX 6 = 0Q. The last equality can be shown by using 
the fact that the //-expected value of Vln//-V-^ln/^ coincides with — VV-^ln/i 
(since X is finite) and a similar line of reasoning as in (3.18) for Dn-

3.7 Miscellaneous 
107 
For independent and identically distributed distributions /i6'(^i:n)=M^(^i)*-"' 
fie{xn) V6> this bound was proven in [CB90, Thni.2.3]. In this case jt^^^^o] f^Q^^^ 
jn=jn 
independent of n. For stationary (A:*^-order) Markov processes jn is 
also constant. The proof generalizes to arbitrary fiQ by replacing 
J^^^^^^OQ) 
with jn everywhere in their proof. For the proof to go through, the vicinity 
JSfs^ -"^l^- ||^~^o||jn ^^n} of ^0 must contract to a point set {^o} ^or n-^oo and 
(5^—>0. Jn is always positive semi-definite, as can be seen from the definition. 
The boundedness condition of J^^ implies a strictly positive lower bound 
independent of n on the eigenvalues of jn for all sufficiently large n, which 
ensures Ns^ -^{^o}- The uniform continuity of J^ ensures that the remainder 
o(l) from the Taylor expansion of Dn is independent of n. Note that twice 
continuous differentiability of Dn at ^o [CB90, Con.2] follows for finite A' 
from twice continuous differentiability of fie. Under some additional technical 
conditions one can even prove an equality Dn = 
lnw~^-\-^ln^^-\-^lndetjn-{-
o(l) for the i.i.d. case [CB90, (1.4)], which is probably also valid for general 
The \riw~^ part in the bound is the same as for countable M. The f I n ^ 
contribution can be understood as follows: Consider 6e[0,l) and restrict the 
continuous A^ to ^ that are finite binary fractions. Assign a weight w{0) ^ 
2"^ to a (9 with binary representation of length I; Dn^l'lii2 
in this case. 
But what if 6 is not a finite binary fraction? A continuous parameter can 
typically be estimated with accuracy 0{n~^^'^) after n observations. The data 
do not allow to distinguish a 6 from the true 0 if \0 — 6\ <0(n~^/^). There 
is such a 9 with binary representation of length I = log20(v^)- Hence we 
expect Dn^|lnn+0(1), or |Inn+0(1) for a d-dimensional parameter space. 
In general, the 0(1) term depends on the parametric complexity of ^e and is 
explicated by the third ^IndetJ^ term in Theorem 3.74. See [CB90, p454] for an 
alternative explanation. Note that a uniform weight W{0) = T^^ does not lead 
to a uniform bound, unlike the discrete case. A uniform bound is obtained for 
Bernando's (or in the scalar case Jeffreys') reference prior w{0) ~ y^detJoo (^) 
if joo exists [Ris96]. 

108 
3 Universal Sequence Prediction 
For finite alphabet X we consider throughout this book, jf^<oo 
indepen-
dent of t and x<i in case of i.i.d. sequences. More generally, the conditions of 
Theorem 3.74 are satisfied for the practically very important class of station-
ary (A;*^-order) finite-state Markov processes (/c —0 is i.i.d.). 
Theorem 3.74 shows that Theorems 3.19-3.60 are also applicable to the 
case of continuously parameterized probability classes. Theorem 3.74 is also 
valid for a mixture of the discrete and continuous cases, C = X]a/^^^"(^) l^e 
3.7.3 Further Applications 
Partial sequence prediction. There are at least two ways to treat partial 
sequence prediction. By this we mean that not every symbol of the sequence 
needs to be predicted, say, given sequences of the form ziXi...ZnXn^ we want 
to predict the x's only. The first way is to keep the Ap prediction schemes of 
the last sections mainly as they are and to use a time-dependent loss function 
that assigns zero loss l^y = 0 at the z positions. Any dummy prediction y is 
then consistent with (3.45). The losses for predicting x are generally nonzero. 
This solution is satisfactory as long as the z's are drawn from a probability 
distribution. The second and preferable way does not rely on a probability 
distribution over the z. We replace all distributions p{xi:n) {p= M, ^, 0 ev-
erywhere by distributions p{xi:n\zi:n) conditioned on zi:n- The Zi:n conditions 
cause nowhere problems as they can essentially be thought of as fixed (or as 
oracles or spectators). So the bounds in Theorems 3.19-3.74 also hold in this 
case for all individual z's. Applications are: 
Independent experiments and classification (CF). A typical experi-
mental situation is a sequence of independent (i.i.d) experiments, predictions 
and observations. At time t one arranges an experiment zt (or observes data 
zt), then tries to make a prediction, and finally observes the true outcome Xt-
Often one has a parameterized class of models (hypothesis space) /X6>(^t|^t) 
and wants to infer the true 0 in order to make improved predictions. This 
is a special case of partial sequence prediction, where the hypothesis space 
M = {fie{xi:n\zi:n) = P'oixilzi)'...-p0{xn\zn)} 
cousists of i.i.d. distributions, 
but note that ^ is not i.i.d. This is the same setting as for online learning of 
classification tasks, where di zeZ 
should be classified as an xGAf (cf. Prob-
lem 3.12). The previous paragraph reduced this setting to sequence prediction. 
3.7.4 Prediction with Expert Advice 
There are two schools of universal sequence prediction: We considered ex-
pected performance bounds for Bayesian prediction based on mixtures of en-
vironments, as is common in information theory and statistics [MF98]. The 
other approach uses predictors based on expert advice (PEA) algorithms with 

3.7 Miscellaneous 
109 
worst-case loss bounds in the spirit of Littlestone, Warmuth, Vovk and oth-
ers. The two schools usually do not refer to each other much. We briefly 
describe PEA and compare both approaches. For a more comprehensive com-
parison see [MF98]. In the following we focus on topics not covered in [MF98]. 
PEA was invented in [LW89, LW94] and [Vov92] and further developed in 
[CB97, HKW98, KW99] and by many others. Many variations known by many 
names (prediction/learning with expert advice, weighted majority/average, 
aggregating strategy, boosting, hedge algorithm, ...) have since been invented. 
Early works in this direction are [Daw84, Ris89]. See [VovOl] for a review and 
further references. We describe the setting and basic idea of PEA for binary 
alphabet. Consider a finite binary sequence XiX2"'Xn E {0,1}^ and a finite 
set £ of experts e^S making predictions x^ in the unit interval [0,1] based 
on past observations xiX2...a;t_i. The loss of expert e in step t is defined as 
|a;t —^tl- I^ ^he case of binary predictions x^ G { 0 , 1 } , |xt—xf| coincides with 
our error measure (3.34). The PEA algorithm p^^ combines the predictions of 
all experts. It forms its own prediction^ xf G [0,1] according to some weighted 
average of the expert's predictions x^. There are certain update rules for the 
weights depending on some parameter /3. Various bounds for the total loss 
Lp{x):=J2^^i\^t—Xt\ 
of PEA in terms of the total loss Ls{x) 
:=Yl^=i\^t—Xt\ 
of the best expert ee£ have been proven. It is possible to fine-tune (3 and to 
eliminate the necessity of knowing n in advance. The first bound of this kind 
was obtained in [CB97]: 
Lp{x) < Le{x) + 2Mn\£\^4^/L^{x)h^£\. 
(3.75) 
The constants 2.8 and 4 were improved in [ACBG02, YEYS04]. The last 
bound in Theorem 3.36 with 5^ <Dn <ln|A^| for uniform weights and with 
E^^' increased to E^ reads 
E^^ < E^ + 2ln\M\ + 2yjESln\M\. 
It has a quite similar structure to (3.75), although the algorithms, the set-
tings, the proofs, and the interpretation are quite different. Whereas PEA 
performs well in any environment, but only relative to a given set of experts 
f, our 0^ predictor competes with the best possible O^ predictor (and hence 
with any other 0 predictor), but only in expectation and for a given set of 
environments A4. PEA depends on the set of experts, O^ depends on the set 
of environments M.. The basic ppn algorithm was extended in different di-
rections: incorporation of different initial weights {\£\-^w~^) [LW89, Vov92], 
more general loss functions [HKW98], continuous-valued outcomes [HKW98], 
and multidimensional predictions [KW99] (but not yet for the absolute loss). 
The work [Yam98] Hes somewhat in between PEA and this book; "PEA" 
The original PEA version [LW89] had discrete deterministic prediction x^ E { 0 , 1 } 
with (necessarily) twice as many errors as the best expert and now is only of 
historical interest. 

110 
3 Universal Sequence Prediction 
techniques are used to prove expected loss bounds, but only for sequences 
of independent symbols/experiments and limited classes of loss functions. Fi-
nally, note that the predictions of PEA are continuous. This is appropriate 
for weather forecasters, who announce the probability of rain, but the de-
cision to wear sunglasses or to take an umbrella is binary, and the suffered 
loss depends on this binary decision, not on the probability estimate. It is 
possible to convert the continuous prediction of PEA into a probabilistic bi-
nary prediction by predicting 1 with probability x^ G [0,1]. The probability 
of making an error is then \xt—x^\. 
Note that the expectation is taken over 
the probabilistic prediction, whereas for the deterministic 0^ algorithm the 
expectation is taken over the environmental distribution fi. The multidimen-
sional case [KW99] could then be interpreted as a (probabilistic) prediction of 
symbols over an alphabet A' = {0,1}^, but error bounds for the absolute loss 
have yet to be proven. In [FS97] the regret is bounded by ln|£'| + A/2Lln|f | 
for arbitrary unit loss function and alphabet, where L is an upper bound on 
L^, which has to be known in advance. It is possible to generalize PEA and 
bound (3.75) to arbitrary alphabet and weights and to general loss functions 
with probabilistic interpretation [HP04]. 
3.7.5 
Outlook 
In the following we discuss several directions in which the findings of this book 
may be extended. 
Infinite alphabet. In many cases the basic prediction unit is not a letter, 
but a number (for inducing number sequences), or a word (for completing 
sentences), or a real number or vector (for physical measurements). The pre-
diction may either be generalized to a block-by-block prediction of symbols, or, 
more suitably, the finite alphabet A' could be generalized to countable (num-
bers, words) or continuous (real or vector) alphabets. The presented theorems 
are independent of the size of A' and hence should generaUze to countably in-
finite alphabets by appropriately taking the limit |A'| ^^oo and to continuous 
alphabets by a denseness or separability argument. Since the proofs are also 
independent of the size of Af, we may directly replace all finite sums over A' by 
infinite sums or integrals and carefully check the validity of each operation. 
We expect all theorems to remain valid in full generality, except for minor 
technical existence and convergence constraints. 
An infinite prediction space y was no problem at all as long as we assumed 
the existence of yfp ey 
(3.45). In case y^p ey 
does not exist one may define 
yfp G3^ in a way to achieve a loss at most et = o{t~^) larger than the infimum 
loss. We expect a small finite correction of the order of ^ = X^^i^t < CXD in the 
loss bounds somehow. 
More active systems. Prediction means guessing the future, but not influ-
encing it. A small step in the direction to more active systems was to allow the 

3.8 Summary 
111 
A system to act and to receive a loss ixty^ depending on the action yt and the 
outcome Xt- The probabihty /x is still independent of the action, and the loss 
function £^ has to be known in advance. This ensures that the greedy strategy 
(3.45) is optimal. The loss function may be generalized to depend not only on 
the history x<t, but also on the historic actions 2/<t with // still independent 
of the action. It would be interesting to know whether the scheme A and/or 
the loss bounds generalize to this case. The full model of an acting agent influ-
encing the environment is developed in the next chapter, but non-asymptotic 
loss bounds have yet to be proven. 
Miscellaneous. Another direction is to investigate the learning aspect of 
universal prediction. Many prediction schemes explicitly learn and exploit a 
model of the environment. Learning and exploitation are melted together in 
the framework of universal Bayesian prediction. A separation of these two as-
pects in the spirit of hypothesis learning with MDL [VLOO] could lead to new 
insights. Also, the separation of noise from useful data, usually an important 
issue [GTVOl], did not play a role here. The attempt at an information-
theoretic interpretation of Theorem 3.63 may be made more rigorous in this 
or another way. In the end, this may lead to a simpler proof of Theorem 3.63 
and maybe even for the loss bounds. A unified picture of the loss bounds ob-
tained here and the loss bounds for predictors based on expert advice (PEA) 
could also be fruitful. Yamanishi [Yam98] used PEA methods to prove ex-
pected loss bounds for Bayesian prediction, so maybe the proof technique 
presented here could be used vice versa to prove more general loss bounds for 
PEA. Maximum-Likelihood predictors may also be studied. Since 2~^*^^^ (or 
some of its variants) is a close approximation of ^t/, it is generally believed 
that predictions based on K are as good as predictions based on ^jj. Conver-
gence and loss bounds for predictors based on K would prove this conjecture, 
but it is easy to see that K completely fails for predictive purposes [Hut03d]. 
Also, more promising variants like the monotone complexity Km and univer-
sal two-part MDL, both extremely close to ^[/, fail in certain situations (see 
Problems 2.8 and 3.17 or [Hut03d, PH04a]). Finally, the reader is invited to 
apply the A^ predictor to his favorite induction problem by choosing a suitable 
M with computable £,. 
3.8 Summary 
We compared universal predictions based on Bayes mixtures (^ to the infeasible 
informed predictor based on the unknown true generating distribution /i. We 
showed that the universal posterior ^ converges to fi and that ^//i—^1. Our 
main focus was on a decision-theoretic setting, where each prediction yt^X 
(or 
more generally action yt^y) 
results in a loss l^xtyt i^ ^t is the true next symbol 
of the sequence. We showed that the A^ predictor suffers only slightly more 
loss than the TI^ predictor. We also showed that the derived error and loss 

112 
3 Universal Sequence Prediction 
bounds cannot be improved in general, i.e. without making extra assumptions 
on i, fi, M, or Wjy. Within a factor of 2 this is also true for any /^-independent 
predictor. We demonstrated Pareto optimality of ^ in the sense that there is 
no other predictor that performs better or equal in all environments 
ueM 
and strictly better in at least one. Optimal predictors can (in most cases) 
be based on mixture distributions ^. Finally, we gave an Occam's razor ar-
gument that Solomonoff-Levin's prior with weights Wjy = 2"^^^^ is optimal, 
where K{iy) is the Kolmogorov complexity of z/. Of course, optimality always 
depends on the setup, the assumptions, and the chosen criteria. For instance, 
the universal predictor was not always Pareto optimal, but at least for many 
popular, and for all decision-theoretic performance measures it was. Bayes 
predictors are also not necessarily optimal under worst-case criteria [CBLOl]. 
We also derived a bound for the relative entropy between ^ and fi in the case 
of a continuously parameterized family of environments, which allowed us to 
generalize the loss bounds to continuous M. Furthermore, we discussed the 
duality between the Bayes mixture and expert-mixture approaches and re-
sults, classification tasks, games of chances, infinite alphabet, active systems 
influencing the environment, and others. 
3.9 Technical Proofs 
3.9.1 How to Deal with 11 = 0 
Some expressions (like conditional or inverse probabilities) are undefined for 
zero /J. We thought of the following solutions: 
Avoid the problem. We may restrict ourselves to /J'{xi:n)>0\/xi:n-
+ The treatment in this book is then rigorous, and a zero /x can be approx-
imated to an arbitrary precision. From a practical point of view, this is a 
completely satisfactory approach. 
— Theoretically unsatisfactory, because deterministic environments (for 
which /i{xt\x<^t) = 0 for all but one Xt) are of special interest, and not 
just esoteric limits. 
Take the limit. Develop all theorems for //^*^ > 0 and finally perform the 
limit /i^*^ *^^ //, where fi might be zero for some strings. For instance, 
M^^H^i:n):=-(l-^)M(^i:n)+^/2'' and 6-^0 will do. 
H- Rigorous treatment with the advantage not having to deal with the prob-
lem until the end. If all spaces are finite, then interchange of finite sums 
or maxs with lim^_^oo is safe. 
— Problematic for infinite spaces (e.g. alphabet, time, ...), since limits may 
not be interchangeable. 
Face the problem. A way of facing the problem, which is different from Sec-
tion 3.2.1, is to restrict the set of strings to one with nonzero /x-probability. 
Define the critical set ^ :== UxGA'*:ju(a^)=o^^' where T^, := {a; :a;i.^(^) = x } 

3.9 Technical Proofs 
113 
is defined as the cylinder set containing all infinite sequences u; starting 
with X. 
+ Since Z is a countable (for countable alphabet) union of cylinder sets Fx of 
measure zero, Z itself is measurable with /i-measure zero. So all theorems 
proven with /x-probability 1 on re\Z 
still hold on F^ with //-probability 
1, since ii{Z) = 0. 
— All sums over x have to be restricted appropriately. More seriously, other 
measures on F^, especially ^, deteriorate to semimeasures on Fe\Z (see 
Section 2.4 and Problem 3.1). 
Ignore the problem. Address other more severe or interesting problems 
first. Why waste time fooling with exceptions when everything seems to 
work well anyway and there are more important problems to solve. 
-f Time-efficient "physicist" approach (like: always exchange limits and in-
tegrals until you get into trouble). 
— The approach is risky and a mathematician would turn in his grave. 
We usually faced the problem, but decided to avoid/ignore these subtleties in 
the main text (see Problem 3.8). We will also not explicate every subtlety in 
the following proofs. Subtleties regarding y,z=0/1 have been checked but will 
be passed over. Oluj- :=0 even for Zi = 0. Positive means >0. The probability 
constraints in (3.76) on y and z are assumed to hold throughout this section. 
Finally, 
z>Oiiy>0. 
3.9.2 
Entropy Inequalities (Lemma 3.11) 
We show that 
N 
jE/(?/.-^i) < /(. \\2^^'^^~^'^ 
^^^ 2/i>0. Zi>^, 
^yi 
= ^ = 
^^i 
i=l 
\ 
i=l 
* 
i=l 
i=l 
(3.76) 
for any convex and even (/(x) = /(—a;)) function with /(0)<0. For f{x)=x'^ 
we get inequality (3.lis), and for f{x) = \x\ we get inequality (3.11a). To 
prove (3.76) we partition (for the moment arbitrarily) ie{l,...,N} 
= G^UG~, 
G'^nG" = {}, and define y^ := ^ 
yi and z^ := S^ ^i- ^^ i^ well known that 
the relative entropy is positive, i.e. 
J2 Pi^^- 
> 0 for p, > 0, 
g, > 0, 
^p, 
= l= Y, Qi- i^'^^) 
Note that there are four probability distributions {pi and Qi for i G G^ and 
i£G~). 
For i^G^, 
Pi :=yi/y'^ and Qi :=Zi/z'^ satisfy the conditions on p and 
q. Inserting this into (3.77) and rearranging terms, we get 
yi\n- 
> y 
I n - ^ . 

114 
3 Universal Sequence Prediction 
If we sum over zb and define y = y^ = l—y~ and z = z~^ = l — z~, we get 
^y^\n^ 
> E ^ ^ ^ ^ ^ 
= ?/In- + (!-?/) I n — ^ > 2{y-zf. 
(3.78) 
1=1 
± 
The last inequality is elementary and well known. For the special choice G^ := 
{i'.yi^Zi}^ 
we can upper-bound Y2ifiyi~^i) 
^^ follows 
iGG± 
iGG± 
ieG± 
zeG± 
^=^ /(i2/^-^*i) *=^ my - z\) '^' nVW^W) < n 
\ 
^Eyan^) (3.79) 
2=1 
(a) follows from the symmetry of /. (b) follows from the convexity of / and 
from /(0)<0: Inserting y = 0 and x = a+b in the convexity definition af{x)-\-
{l-a)f{y)>f{ax^{l-a)y) 
leads to af{a + 
b)^(l-a)f{0)>f{a{a^b)). 
Inserting a= ^ ^ and a= -^ 
and adding both inequalities gives /(a+&) + 
/(O) > f{a) + m for a,6 > 0. Using /(O) < 0 we get /(E^^^) > Ez/(^^) for 
^2^0 by induction, (c) is true, since all yi—Zi are positive/negative for iEG^ 
due to the special choice of G^. (d) and (e) follow from the definition of y^"^"* 
and z^'^^ and (/) is obvious, (g) follows from (3.78) and the monotonicity 
of ^/~ and / for positive arguments: Inserting b = y = —x and o^ = | into the 
convexity definition and using the symmetry of / we get f{b)>f{0). 
Inserting 
this into /(tt4-&) + /(0) >/(a) + /(6) we get f{a-^b)>f{a), 
which proves that 
/ is monotone increasing for positive arguments {a^b>0). Inequality (3.76) 
follows by summation of (3.79) over ± and noting that f{^/~) is independent 
of zb. This proves Lemma 3.11/. 
Inserting f{x) = x^ yields Lemma 3.lis; inserting f{x) = \x\ yields 
Lemma 3.11a. Lemma 3.116 follows from 
AT 
N 
^yi\\n 
— \-Y^yi\n~ 
= -2^yiln— 
< 2Y^Zi-yi 
= Y^\yi - Zil 
i=l 
* 
i 
ieG- 
ieG- 
1=1 
where we have used —Inx < - —1. Lemma S.llh is proven differently. For 
arbitrary y>0 and z>0 we define 
f{y,z):=y\n^-{y^-y/^f 
+ z-y = 2yg{^/^) 
with g{t) :=-\nt + 
t-l>0. 
This shows / > 0 , and hence ^if{yi,Zi)>Q, 
which implies 
Ez/iin^ - E ( V ^ - v^)' > E2/' - E ^ ' = 1 - 1 = 0-
^i 
. 
. 
. 
I
I 
I
I 
This proves Lemma 3.11/i. 
• 

3.9 Technical Proofs 
115 
3.9.3 Error Inequality (Theorem 3.36) 
Here we give a direct proof of the second bound in Theorem 3.36. Again, we 
try to find small constants A and B that satisfy the linear inequality 
E^^ < ( ^ + l ) E f ^ - f (B-M)Sn. 
(3.80) 
If we could show 
ef«(x<t) < {A+l)ef''{x^t) 
+ iB+l)st{x^t) 
(3.81) 
for all t < n and all x<t, (3.80) would follow immediately by summation and the 
definition of En and Sn- With the abbreviations (3.12) and the abbreviations 
m = xf^ and s — xf^, the various error functions can then be expressed by 
ep = l - y s , ef^ = l-y^ 
and St^^YliiVi-^i)'^' 
Inserting this into (3.81) we 
get 
N 
1 - ys < (A+i)(i - vm) + (^+1) Y.(yi - zi)\ 
(3.82) 
By definition of xf^ and xf^ we have ym^Vi and Zg >Zi for all i. We prove 
a sequence of inequalities which show that 
N 
(B-^l) Y^ivi - Zif + (A+l)(l - Vm) - (1 - Vs) > .-. 
(3.83) 
^=l 
is positive for suitable A>0 and B>Oy which proves (3.82). For m=s (3.83) is 
obviously positive. So we will assume m^^^ in the following. From the square 
we keep only contributions from i = m and i = s, 
... > {B+l)[{ym-Zmf 
+ {ys~Zsf]-^{A^l)il-ym)-{l-ys) 
> ... 
By definition of y^ z, M. and s we have the constraints ym^Vs < 1, z^n-^Zs < 1, 
ym^Vs^^ 
and Zs>Zm^O. From the latter two it is easy to see that the square 
terms (as a function of Zm and Zg) are minimized by Zra = Zs = ^(z/m+l/s). 
Furthermore, we define x:=ym~ys 
and eliminate ys> 
... > {B + l)lx^-^A{l-ym)-x 
> ... 
(3.84) 
The constraint on ym^ys ^ 1 translates into ym < ^ ^ , hence (3.84) is mini-
mized by y^ = ^ . 
... > l[{B^l)x^-{A^2)x-^A] 
> ... 
(3.85) 
(3.85) is quadratic in x and minimized by x* — 2(^"+\) • Inserting x* gives 
4AB 
-A^-4 
S{B + 1) 
^ 
o,D , 1^ 
> 0 for B>\A+\, 
A>Q, 
(=>B>1). (3.86) 

116 
3 Universal Sequence Prediction 
Inequality (3.80) therefore holds for any A>0, provided we insert B = 
^A-\-^. 
Thus we might minimize the r.h.s. of (3.80) w.r.t. A, leading to the upper 
bound 
E^^ < E^^^Sn-^y/^E^^Sn^Sl 
for 
A^^-
which completes the proof of Theorem 3.36. 
• 
3.9.4 Binary Loss Inequality for z<^ 
(3.57) 
With the definition 
fiy.z) 
:= B'-
Z 
I - Z + A ' . ( l - y ) ^ - y , 
z<l 
2' 
(3.87) 
we show f{y,z) > 0 for suitable A' = A-\-l and B^ = B + 1. We do this by 
showing that / > 0 at all extremal values and "at" boundaries. Keeping y 
fixed, / —^ +00 for 2: -^ 0, if we choose B' > 0. For the boundary 2: = | we 
lower-bound the relative entropy by the sum over squares (Lemma 3.115) 
f{y,l)>2B'{y-'^f+A'{l-y)-y. 
The r.h.s. is quadratic in y with minimum at y* = ^ '^^g, '^^, which impHes 
AAB -A^ 
-A 
8{B + l) 
f{y,l) > f{y\\) 
> —^nrr-^v- 
> 0 for B>lA+\, 
A>o 
(which implies B>1). 
Furthermore, for A > 4 and ^ > 1 we have 
f{y^\)> 
2(l-?/)(3-2y)>0. Hence f{y,\)>^ 
for B>\-\-l, 
since for A > 4 it implies 
E > 1 , and for A<4 it implies 
B>\A+\. 
The extremal condition df/dz — 0 (keeping y fixed) leads to 
B'{l-z)-^A^ 
y = y" — z- B'{l-z)^A'z' 
Inserting t/* into the definition of /, and again replacing the relative entropy 
by the sum over squares (Lemma 3.11s), we get 
f{y\z) 
> 25'(y*-z)2 + A ^ ( l - r ) i ^ - y * = 
WiT^W^'^^'^^ 
g{z) := 
2B^A'^z{l-z)^[{A'-l)B^{l-z)-A^]iB^-hA'j^^). 
We have reduced the problem to showing g>0. If the bracket [...] is positive, 
then g is positive. If the bracket is negative, we can decrease g by increasing 
j ^ < 1 in {B^-\-A^j^) 
to 1. The resulting expression is now quadratic in z 

3.9 Technical Proofs 
117 
with minima at the boundary values z = 0 and z = ^. It is therefore sufficient 
to check 
g{0)>{AB-l){A^B 
+ 2)>Q 
and g{^) > 1{AB - 1){2A^ B+ 3) >0 
which is true for B > ^. In summary, we have proved (3.87) for B > ^ + 1 and 
A>0. 
• 
3.9.5 Binary Loss Inequality for z>^ (3.58) 
With the definition 
fiy,z) 
••= B'-yln^+{l-y)ln^ 
^ 
.A'-(l-y)-yi-^, 
z>l 
z 
1 — z 
(3.88) 
we show f{y,z) > 0 for suitable A' = A-\-l > 1 and B^ ~ B + 1 > 2, similarly 
as in the last subsection by proving that / > 0 at all extremal values and 
"at" boundaries. Keeping y fixed, / —^ +CXD for z-^1. The boundary z = ^ 
was already checked in the last paragraph. The extremal condition df/dz = 0 
(keeping y fixed) leads to 
B'z 
y = y := z- {B' + 
l)z-l' 
Inserting y* into the definition of / and replacing the relative entropy by the 
sum over squares (Lemma 3.115), we get 
f{y\z) 
> 2B'{y*~zr 
+ A'{l~y*)-y*'-^ 
= 
^^^^^=^.g[^£)^ 
g{z) := [{A'-l)B'z-A' 
+ 2z{l-z)]{B' 
+ l~l) + 
2{l-zf. 
We have reduced the problem to showing g>0. Since (B' + l — ^) > 0 it is 
sufficient to show that the bracket is positive. We solve [...] >0 w.r.t. B and 
ffet 
„ ^ l-2z{l-z) 
1 l - z 
B> 
^ 
^ • - + 
. 
z 
A 
z 
For B > -^ + 1 this is satisfied for all | < z < 1. In summary, we have proved 
(3.88) for B>^-f-l and A>0. 
D 
3.9.6 
General Loss Inequality (3.53) 
We reduce 
N 
N 
N 
f{y, z):=B'^yi\n^+A'Y^ Vi^im - Y. ^^^^^ ^ ^ 
(^•^^) 

118 
3 Universal Sequence Prediction 
for 
Ez^i ^^d^ > 0, 
d^ := i,m - i^s 
(3.90) 
to the binary N = 2 case. We do this by keeping y fixed and showing that 
/ as a function of z is positive at all extrema in the interior of the simplex 
A := {z : Y^^Zi = l,Zi> 0} of the domain of z and "at" all boundaries. First, 
the boundaries Zi—^0 are safe as /-^oo for B'>0. Variation of / w.r.t. to z 
leads to a minimum at z = y. If Yl^yidi>0, we have 
f{y,y) 
= ^2/i(^''^«m-'^zs) > ^Vii^im-^is) 
= Y^y^^' - ^-
i 
i 
i 
In the first inequality we used A' >1. If ^^yidiKO, 
z — y is outside the valid 
domain due to constraint (3.90), and the valid minima are attained at the 
boundary ADP with P:={z: J^^z^d^^O}. We implement the constraints with 
the help of Lagrange multipliers and extremize 
L{y,z):=f{y,z)-^B'xY,^^^B'^xY,Zid,. 
dL/dzi = 0 leads to yi = t/* := Zi{X-\-fidi). Summing this equation over i, we 
obtain X = l. fiis a. function of y for which a formal expression might be given. 
If we eliminate yi in favor of Zi, we get 
/(2/*,z) = Yl^^^' 
^^^^ 
Ci:={l + 
/Jdi){B'ln{l-^^J.di)-\-A'£im-£^s)• 
i 
In principle // is a function of y, but we can treat /i directly as an independent 
variable, since y has been eliminated. 
The next step is to determine the extrema of the function f = Yl^i^i fo^ 
ZEADP. 
For clarity we state the line of reasoning for TV = 3 first. In this case 
Zi is a triangle. As / is linear in z it assumes its extrema at the vertices of 
the triangle, where all Zi = 0 except one. But we have to take into account a 
further constraint z^P. 
The plane P intersects triangle Zi in a finite line (for 
Ar]P — {} the only boundaries are 2:^—>0, which have already been treated). 
Again, as / is linear, it assumes its extrema at the ends of the line, i.e. at 
edges of the triangle A on which all but two Zi are zero. Similarly for A^>3, 
the extrema of /, restricted to the polytope zAflP, are assumed in the corners 
of AnP, which lie on the edges of simplex A, where all but two Zi are zero. We 
conclude that a necessary condition for a minimum of / at the boundary is 
that at most two Zi are nonzero. But this implies that all but two yi are zero. 
If we had eliminated z in favor of 2/, we could not have made the analogous 
conclusion because ^^=0 does not necessarily imply Zi = 0. We have effectively 
reduced the problem of showing f{y*,z)>0 
to the case N = 2. We can go back 
one step further and prove (3.89) for TV = 2, which implies /{y'^.z) > 0 for 
N = 2. A proof of (3.89) for iV = 2 implies, by the arguments given above, that 
it holds for all N. This is what we set out to show here. 
• 
The A^ = 2 case was proven in Section 3.4 and the two previous subsections. 

3.11 Problems 
119 
3.10 History Sz References 
There are good introductions and surveys of Solomonoff sequence prediction 
[LV92a, LV97], inductive inference in general [AS83, Sol97, MF98], competi-
tive online statistics [VovOl], and reasoning under uncertainty [Grii98]. The 
latter also contains a more serious discussion of the case // ^ A^ in a related 
context. The convergence ^-^fi of Theorem 3.l9{iii) was first proven in [BD62] 
in a more general framework with the help of martingales, but the martingale 
proof does not provide a speed of convergence. Solomonoff's contribution was 
to focus on the set of all computable distributions [Sol64, Eq.(13)] and to prove 
Theorem 3.19(i) for binary alphabet, which shows that convergence {Hi) of ^ 
to fi is rapid [Sol78]. The generalization of (i) to arbitrary finite alphabet was 
probably first shown by the author in [Hut01 a], but may have occurred earlier 
somewhere in the statistics literature. Convergence (v) of the ratio ^/// to 1 
w.//.p.l was first shown by Gacs with the help of martingales [LV97], again not 
allowing one to estimate the speed of convergence. The elementary proof of 
^/ji-^l 
i.m.s., i.e. of {iv) and (v) is from [Hut03a], showing that convergence 
is rapid, [vi) directly follows from Lemma 3.11(a). Lemma 3.11(a) is due to 
Pinsker [Pin64] and Csiszar [Csi67], and can be found in [CT91, Lem.12.6.1]. A 
proof of Lemma 3.11(/i) can be found in [BM98, pi78]. Lemma 3.11(6) is also 
known [BarOO]. Most other results in this chapter are from [Hut03a, Hut03c]. 
3.11 Problems 
3.1 (Semiraeasures) [C30u/C40o] All results in this chapter were obtained 
for probability measures /i, and ^ and w^^ i.e. J^^ 
i{^i:t) — Ylx 
f^i^i-t) — 
Ylj^Wjy = 1. On the other hand, the primary class M of interest in this book 
is the class of all enumerable semimeasures and ^^^Wiy < 1, (see Section 2.4), 
In general, each of the following four items could be semi (<) or not (=): (^, 
//, A^, Wjy)^ where M is semi if some elements are semi. 
Which of the 2^ combinations make sense? (Hint: 6 of the 16). Show that 
the entropy inequalities (Lemma 3.11) hold for (<,==,<,<), but not for (<,< 
,<,<). Nevertheless, show that <^->/i (Theorem 3.19in) for (<,<,<,<) with 
maximal /i semi-probability, i.e. fails with /i semi-probability 0, Generalize all 
other theorems in this chapter as far as possible to the semi case. 
3.2 (Dominance of the Speed prior) [C40oi] Which (semi)measures are 
multiplicatively dominated by the Speed prior defined in [Sch02b]? Show that 
computable deterministic environments are not dominated by the Speed prior, 
but for quickly computable deterministic environments domination holds with 
a slowly decreasing "constant". Does the Speed prior dominate quickly com-
putable truly probabilistic environments for some suitable definition of 'quick' 
and 'truly' ? Is there an easily characterizable class of (all?) dominated prob-
abilistic environments? 

120 
3 Universal Sequence Prediction 
3.3 (Comparing two mixtures) [C05u/C40o] Consider two mixtures £, 
and e' over M. Show that 
ET=i^[Ea:M{^t\x<t)-e{xt\x^t))^]<2[\nw-'^ 
lnto^~ ]<oo, i.e. ^{xt\x^t)-^^^{xt\x<ct) 
for t-^oo with /x-probabiHty 1 for all 
fieM 
(of. Theorem 3.19i). Furthermore, show that L^^-L^'^ 
<0{^/L^) 
(cf. Theorem 3.48). Is the stronger result L^^ —L"^ <oo also true? 
3.4 (Convergence and loss bounds with high probability) [C30oi] 
Show 
that 
PEr=i(/^(^£k<t) - ^{xt\x<t))'^ 
> 
^lnw~^] 
< 
e 
and 
P[ElLi(^t^^-^t^'')^ ^ flntt;-^] < e, where P denotes /u-probability. Use 
Theorem 3.19(i) and Theorem 3.59(i) and Markov's inequality. Is it possible 
to prove similar high-probability bounds for the ratio l^^/l^^^, possibly 
exploiting Theorem 3.48 and Corollary 3.49(ii2)? High-probability bounds 
on l^{x<^t) still involve an expectation over Xt (see definition of if). Is it 
possible to prove high-probability bounds on the difference or ratio of (^xty^^ 
and ^xty"^^ 1 which do not involve any expectations? 
3.5 (Pareto optimality) [C30u] Show that ^ is not Pareto optimal w.r.t. 
thea-normJ^(i/,^) = ||z/-^||c,= dYlxt\^i^t\x<t)-^{xt\x^t)\'^ 
\ia^2. 
Further, 
Pareto optimality of ^ w.r.t. Ti and T2 is neither a necessary nor a sufficient 
condition for ^ being Pareto optimal w.r.t. their sum T\^T2- 
Finally, if ^ is 
Pareto optimal w.r.t. T, then ^ is also Pareto optimal w.r.t. any monotone 
increasing function of ^ (e.g. T^, a>0). 
Hint: Intuition on this problem can be gained by considering probabil-
ity vectors x^y.z G zA C iR^, where A is the two-dimensional probability tri-
angle, and z = wx + {l—w)y 
is a mixture of x and y. Consider the sets 
Mx '= {r : T{x^r) < T{x,z)} 
and analogously My. MxHMy 
is not empty; 
it contains z. If MxHMy has an interior, then z is not Pareto optimal. Vi-
sualize the one-dimensional boundaries of the two-dimensional areas Mx and 
My qualitatively for the various performance measures J^. Now consider mix-
tures of three vectors in M^. This should give you enough intuition to prove 
Pareto optimality and to construct counter examples. 
3.6 (Lower error bound) [C30u] It is possible to derive good (but not 
tight) lower error bounds with a fixed (n independent) set Ai and weights, as 
opposed to the n dependent set M. chosen in the proof of Theorem 3.64. 
For instance, choose Ai = {/Xi,/i2} with f^i/2{Mx^t) 
— \^^t 
with St — 
min{\,\/\nw~l/^/thit}. 
For w^^ <.w^^ and n-^oc 
show that E^^—E^^" ^ 
j^y/E^f^Dn- 
Is it possible to derive a tight(er) lower bound for diff"erent, but 
n independent A4? 
3.7 ((Non)uniqueness of universal weights) [C25ui] 
Section 
3.6.4 
showed that the universal weights Wjy = 2~^^^^ are optimal in a sense 
precisely stated in Theorem 3.70. Show that this choice for Wy is not unique 
(not even within a constant factor). For instance, for Vjy = 0{l) for v — $,w 
and Vi, arbitrary (e.g. 0) for all other u, the obvious dominance £,u>VyP can 

3.11 Problems 
121 
be improved to ^i^ >Wj^i'. Indeed, formally every choice of weights Vjy>0\fiy 
leads within a multiplicative constant to the same universal distribution, 
but this constant is not necessarily of "acceptable" size. Suitably define 
"acceptable size" by considering the implications for the loss bounds. 
Construct (counter) examples and necessary/sufficient conditions for weights 
to be acceptable. 
3.8 (Deal with zero /x) [C35uo] Verify that all results in this chapter re-
main valid even if /i is allowed to take value zero for some arguments. Take 
the limit or face the problem as discussed in Section 3.9.1. Note, that when 
facing the problem, ^ deteriorates to a semimeasure (see Section 2.4 and Prob-
lem 3.1). 
3.9 (Relations between random convergence criteria) [C30sm] Prove 
the relations in Lemma 3.9 between the various convergence criteria given in 
Definition 3.8. Show that no other implications hold by constructing example 
random sequences. More precisely, implications are strict with reverse being 
wrong, and disconnected criteria (in the transitive hull) are incomparable. 
Show that convergence i.m.s. implies that Zt deviates from z^ by more than e 
only finitely many times and give a bound on the number. 
3.10 (Individual ^-^^ convergence) [C45om] In Problem 2.3 the open 
question whether (,u{xt\x<t) converges to /i(xt|x<t) (in ratio or difference 
sense) individually for all Martin-Lof random sequences was posed (short 
^t/ —^/^)- Theorem 3.22 shows that ^u—^f^ cannot be decided from ^u being 
a mixture distribution (3.5) or from the dominance property (3.6) alone. £,^M 
for the classes used in Theorem 3.22. Construct ^M^-M and prove a theorem 
analogous to Theorem 3.22 for these M^s (Start with two-element classes A4, 
then enlarge M as far as possible). Hence ^u^-Mu 
is also not sufficient to re-
solve ^[7—>//. Convert Theorem 3.19{ii/iv) to potential /x.M.L.-randomness 
tests, but show that they are not effective. Try also to generalize Vovk's re-
sult [Vov87] to nonrecursive distributions. Where is the problem? With these 
insights, try again to solve Problem 2.3. 
3.11 (Speed of ^-^/x convergence) [C35o] Theorem 3.19(i) shows that 
St^i^t < CO. If St were monotone decreasing {st-\-i < St) this would imply 
that St tends to zero faster than 1/t, i.e. St'=o{l/t). 
Show (or refute) that 
this monotonicity is generally wrong for some class M of measures. For the 
semimeasure ^ = ^u^ instantaneous convergence is extremely slow (see Prob-
lem 2.7), although St converges fast to zero in an average sense. Provide nec-
essary and/or sufficient conditions on Ai such that St = o{l/t). 
3.12 (Learnability of the universal Turing machine) [C05u] Consider 
the problem of learning a function f : Z ^ JY. A sequence of sample pairs 
(zi.xi), 
{z2,X2), ..., {zn-i,Xn-i) 
with Xi = f{zi) is givcu. The task is to pre-
dict Xn = f{zn) from Zn- This setup is a special case of the one described 

122 
3 Universal Sequence Prediction 
in Section 3.7.3. Show that if / is a recursive function, then the O^ predic-
tor makes at most a finite number of prediction errors, more precisely, at 
most 21n2-i^(/)-hO(l) errors. Consider now the universal (partial) function 
f{z) :—U{z), where U is some universal Turing machine. Show that U is learn-
able in the sense that after finite time n, 0^ correctly predicts Xi = U{zi) for 
alH>n as long as all Zi are in the domain of /. What happens if for some Zi, 
U does not halt? 
3.13 (Posterization) [C30ui] Show that many properties of Kolmogorov 
complexity, Solomonoff's prior, and (policies based on) Bayes mixtures re-
main valid after "posterization". By posterization we mean replacing L^, 
), etc., by the posteriors Lkn'-=J2^=k'^[^xtyt\^<k], 
^vi^Kk)-, 
K{v\x<^k)'> ^{xk:n\x<k)^ etc. Show that, strangely enough, for M. = Mu and 
if;i, = 2-^^^) it is not true that w;^(x<fc)^2~^(^l^<'^) (not even \og2W^{x<:k) = 
—K{v\x^k)-\-0{\og) 
holds). The important > direction fails. Is the other direc-
tion < true? So bounds, of e.g. L^^, in terms of lnio^(x</c) cannot be converted 
to bounds in terms of K{^\x^k) 
unlike the A: = 1 case. But if we go one step 
back, we see that a bound on hi[ii{xk:n\^<k)/i{xk:n\x<k)] 
is sufficient to bound 
I/^^. Use Problem 2.6(iii) to bound this expression by ln2-^(/x|x</e) H-0(?). 
The more information the history x^k contains about the environment /i, the 
smaller the bounds get. 
3.14 (Probabilistic error bounds) [C35s/C35o] Instead of making (deter-
ministic) predictions that minimize the p-expected loss or error, we may 
define probabilistic p-predictors that predict Xt from x<t with probability 
p{xt\x^t), 
and compare the performance of the ^-predictor with the p- or 
other p-predictors. For simplicity, consider binary sequences (drawn from /x) 
and the error loss. Let ef (rr<t) :=Et[l —p(xt|x<t)] be the error probability in 
the t^^ prediction, and £'^:=J]]^^-^E[ef (x<t)] be the /i-expected total number 
of errors in the first n predictions. Prove the following error relations between 
universal (p = ^), informed (p = /u), and general (p) predictors: 
(i) \Ei-^E^\ 
< 
\An 
< 
Dn + 
^2E^Dn 
[ii) Ei 
> 
^[Sn^Eii] 
iiii) Ei 
> 
Eil + Dn- 
y/2KDn 
> Dn 
for 
Ef^ > 2Dn 
(iv) EH 
< 
2EP, 
e^ < 2e^ 
for any p 
(v) Ei 
< 
2EP ^Dn + ^4.EHDn 
for any p, 
where An and Sn < Dn <\nw~^ are defined in (3.13)-(3.16), and w^ is the 
weight (3.5) of p in ^ {{i) — {v) were proven in [HutOlc]). This shows that the ^-
predictor performs well as compared to the p-predictor {Ei — E!^ = 
0{VEn)), 
but does not exclude the possibility that it makes twice as many errors as 
other (better) predictors (there is a factor 2 in E^/E^ < 2 + 0((^^)~^/2)). 
Give an example for E^/E^^^ > 2, showing that the factor 2 in (iv) and (v) 
cannot be improved in general. Generalize {i) — {v) to nonbinary alphabet and 
arbitrary loss functions. 

3.11 Problems 
123 
3.15 (Posterior convergence for unbounded horizon) [C15ui/C30o] 
Show that for unbounded horizon ht—^oo, there exist A^, /x, and w^ such 
that Ylu=i^[^t:t-\-ht-i] — ^^ (whereas ^^^lEiidt] < oo)- Show that condition 
snp^ht = 00 is not sufficient. Show that convergence can be (very) slow 
when hi grows (very) fast. To answer this question one has to define '(very) 
slows/fast', e.g. as logarithmical/exponentially increasing, or slower/faster 
than any unbounded computable function. Is convergence reasonably fast for 
slowly growing horizon, e.g. for ht=\ogt? What about ht = t7 
3.16 (Distance bounds) [C25u] Consider the distance measures a, d, h, 
and s defined in (3.10) for probabilities yi > 0 with J2iyi — ^ ^^^ semi-
probabilities 2;^ > 0 with Y2i^i ^ I5 I <i <N. Show that the bounds s <d, 
o?<2d^ h<d, h<a, s<4/i, s<a^, a^ <Ns hold and are tight in the sense that 
the ratio of the l.h.s. to the r.h.s. is bounded by 1 and can get (arbitrarily 
close to) 1. Show that s<a^ and s<4/i can be improved to s< ^a^ and s<2h 
if {zi) is restricted to a probability J^i^i — ^-
3.17 (Prediction by minimum description length) [C40s] Instead of 
the mixture distribution ^ — Ylu'^i^^^ consider the maximum a posteriori 
estimator g{x) := max{w;^z^(x) : u G A4} or equivalently the two-part min-
imum description length (MDL) estimator g := a.vgm.mjy^M{^^^2^{^)~^ ~^ 
\og2W~^}^ where as before, M is a, countable set of (semi)measures and 
E ^ G M ^ ^ ^ l - 
Show t h a t 
E t ' ^ l E E a : / ( M ( 4 | : r < t ) - ^ ( n o r m ) ( ^ t | 3 ^ < t ) ) ^ ] ^ t / ^ ^ \ 
where g{xt\x^t) := Q{xi:t)/Q{x<t) and gnorm{xt\x<:t) := ^(^i:t)/Ext^(^i:t)-
Show that these bounds are (within a multiplicative constant) tight (even for 
i.i.d. environments). This shows that MDL converges i.m.s. but convergence 
speed can be exponentially worse than for ^ (cf. Theorem 3.19(i)). 
Hint: Show that ^ —/) is a semimeasure, although p is not. Bound the rel-
ative entropy "Dn" between // and gnorm and the absolute distance "A^" 
between gnorm and g. For the lower bound consider the deterministic envi-
ronments 1^0^ (or Bernoulli(^^ = |-f-2~*)) for l<i<N 
with uniform weights. 
See [PH04a, PH04b] for solutions. 
3.18 (Reduced prediction of aspects) [C20u] Assume we want to predict 
only certain reduced aspects x[ of Xt^^t^ 
^-g- ^t are detailed meteorological 
data of day t, but we are only interested whether it rains {x^ — 0) or there is 
sunshine (x^ = 1). Formally, let x[ — f[xt)^X'^ 
and loss ^xtvt —^'x' 
d^P^nd 
on Xt through x'^ only. Assume Xi:oo is sampled from /x. Let M.''.— {y'\v^}A\ 
and < : = ^ ^ , where i^'(4:n)—Exi.n:/(xt)=x;vi<t<n^(^i:n)- Show that x\,^ is 
sampled from \i' and regret bound (3.48) also holds for the reduced (primed) 
quantities, although in general A^j^^A! 
,.. Furthermore, show that the bound 
can significantly improve, e.g. for M = Mu, w^ — 2~^^^^, K{f) = 0{1), if 
K{fi') <^K{ii). This does not mean that to predict in the reduced space X' 
is to be preferred, since what matters is not a bound on the regret, but the 
true loss L^^ ^-^L^^. Give examples for which L^^^^^ =0 but L^^^/i > ^ ^ , i.e. 
it sometimes does much harm to go to the reduced space (the regret improves 

124 
3 Universal Sequence Prediction 
simply because yl^ deteriorates). Show that always L^^^ >L:jJ^, i.e. reduction 
never helps if /i is known, but L^^ <l<Cn —1<L^^ is possible, i.e. reduction 
may help or harm if // is unknown. Is L^^ > ^n^ possible for M. = Mu 
(cf. 
Problem 2.9)? 
Hint: All examples can be chosen of the form A' = A" x A''' with X' = 
X" — IB [xt — x[x'l) and i' being the error loss, (a) ^{xi^n) = jJ^'{x'l.^) -fi"{x'l.^) 
with K{ii') = 0(1) and K{ii") ± X(/i) large, (6) x[ = xj'.i with x'' being 
Bernoum(l), (c) M - {z/o,...,i/n} with j/o((01)^) = 1, z/^((00)^-^10)) = 1 foi" 
l < 2 < n , 1L'^=2~*~-^, fl — Pri' 

Napoleon: How is it that, although you say so much 
about the Universe, you say nothing about its Creator? 
Laplace: No, Sire, I had no need of that hypothesis. 
Lagrange: Ah, but it is such a good hypothesis: it ex-
plains so many things! 
Laplace: Indeed, Sire, Monsieur Lagrange has, with his 
usual sagacity, put his finger on the precise difficulty 
with the hypothesis: it explains everything, but predicts 
nothing. 
Alan Turing 
— Conversation between Laplace and Lagrange medi- 
(1912-1954) 
ated by Napoleon 
4 Agents in Known Probabilistic 
Environments 
The 
general framework for AI might be viewed as the design and study of 
intelligent agents [RN95]. An agent is a cybernetic system with some internal 
state, which acts with output yk on some environment in cycle k, perceives 
some input Xk from the environment and updates its internal state. Then the 
next cycle follows. We split the input Xk into a regular part Ok and a reward 
rfc, often called reinforcement feedback. From time to time the environment 
provides nonzero reward to the agent. The task of the agent is to maximize 
its utility, defined as the sum of future rewards. A probabilistic environment 
can be described by the conditional probability JJL for the inputs xi...Xn 
to 
the agent under the condition that the agent outputs yi...yn- 
Most, if not 
all, environments are of this type. We give formal expressions for the outputs 

126 
4 Agents in Known Probabilistic Environments 
of the agent that maximize the total //-expected reward sum, called value. 
This model is called the Al/i model. As every AI problem can be brought into 
this form, the problem of maximizing utility is hence being formally solved, 
if fi is known. Furthermore, we study some special aspects of the Al/a model. 
We introduce factorizable probability distributions describing environments 
with independent episodes. They occur in several problem classes studied 
in Chapter 6 and are a special case of more general separable probability 
distributions defined in Section 5.3. We also clarify the connection to the 
Bellman equations of sequential decision theory and discuss similarities and 
differences. We discuss minor parameters of our model, including (the size of) 
the input and output spaces Af and y and the lifetime of the agent, and their 
universal choice, which we have in mind. There is nothing remarkable in this 
chapter; it is the essence of sequential decision theory [NM44, Bel57, BT96, 
SB98], presented in a very general form. Notation and formulas needed in 
later sections are simply developed. There are two major remaining problems: 
the problem of the unknown true probability distribution //, which is solved 
in Chapter 5, and computational aspects, which are addressed in Chapter 7. 
4.1 The AI/L^ Model in Functional Form 
4.1.1 The Cybernetic Agent Model 
A good way to start thinking about intelligent systems is to consider more 
generally cybernetic systems, in AI usually called agents. This avoids hav-
ing to struggle with the meaning of intelligence from the very beginning. A 
cybernetic system is a control circuit with input x and output y and an inter-
nal state. From an external input and the internal state the agent calculates 
deterministically or stochastically an output. This output (action) modifies 
the environment and leads to a new input (perception). This continues ad 
infinitum or for a finite number of cycles. 
Definition 4.1 (The agent model) An agent is a system that interacts 
with an environment in cycles k = 1,2,3,,... In cycle k the action (out-
put) Vk^y 
oi the agent is determined by a policy p that depends on the 
I/O history yiXi,..yk-iXk-i' 
The environment reacts to this action, and 
leads to a new perception (input) x^ G Af determined by a determinis-
tic function q or probability distribution //, which depends on the history 
yiXi...yk~iXk-iyk' 
Then the next cycle k-hl starts. 
There is significant overlap between control theory studied by engineers and 
agent theory studied in AI, but both fields differ in notation and emphasis. 
Table 4.2 compares notation and emphasis in both fields. Only the interchange 
of input "^output can cause confusion. With few exceptions we use the notion 
common in AI. 

4.1 The AI/Li Model in Functional Form 
127 
As mentioned, we need some reward assignment to the cybernetic system. The 
input X is divided into two parts, the standard input o and some reward input 
r. If input and output are represented by strings, a deterministic cybernetic 
system can be modeled by a Turing machine p, where p is called the policy of 
the agent, which determines the (re)action to a perception. If the environment 
is also computable it might be modeled by a Turing machine q as well. The 
interaction of the agent with the environment can be illustrated as follows: 

128 
4 Agents in Known Probabilistic Environments 
Both p as well as q have unidirectional input and output tapes and bidirec-
tional work tapes. What entangles the agent with the environment is the fact 
that the upper tape serves as input tape for p, as well as output tape for 
g, and that the lower tape serves as output tape for p as well as input tape 
for q. Further, the reading head must always be left of the writing head, i.e. 
the symbols must first be written before they are read. Both p and q have 
their own mutually inaccessible work tapes containing their own "secrets". 
The heads move in the following way. In the k^^ cycle p writes yk, q reads yk, 
q writes Xk^VkOk^ p reads Xk^VkOk, followed by the (k + lY^ cycle and so 
on. The whole process starts with the first cycle, with all heads on tape start 
and work tapes being empty. We call Turing machines behaving in this way 
chronological Turing machines. Before continuing, some notations on strings 
and probability distributions are appropriate. 
4.1.2 
Strings 
We denote strings over the alphabet ^ by s = XiX2'-.Xn, with Xk ^ ^ , where 
A* is alternatively interpreted as a nonempty subset of ]N or itself as a prefix-
free set of binary strings. The length of s is i{s)=i{xi)-\-...-\-£{xn)' 
Analogous 
definitions hold for yk G y. We call Xk the k^^ input word and yk the k^^ 
output word (rather than letter). The string s = yiXi,..ynXn represents the 
input/output in chronological order. Due to the prefix property of the Xk 
and yk, s can be uniquely separated into its words. The words appearing in 
strings are always in chronological order. We further introduce the following 
abbreviations: e is the empty string, Xn:m-=^n^n+i---^m-i^m for n<m and e 
for n>m. x<^n'=xi...Xn-i- 
Analogously for y. Further, yXn-=ynXn^ yxn-.m-^ 
ynXn-'-VrnXm, 
a n d SO OU. 
4.1.3 AI Model for Known Deterministic Environment 
Let us define for the chronological Turing machine p a partial function also 
named p : A'* —> y* with yi:k —p{x^k)-> where yi.k is the output of Turing 
machine p on input x</e in cycle k, i.e. where p has read up to Xk-i but 

4.1 The Al/i Model in Functional Form 
129 
no further.^ In an analogous way, we define q-.y* -^^* 
with xi:k = qivi-.k)-
Conversely, for every partial recursive chronological function we can define a 
corresponding chronological Turing machine. Each (agent,environment) pair 
(p,g) produces a unique I/O sequence LO^^ '.= y{^x\^y^^x^..,. 
When we look 
at the definitions of p and q we see a nice symmetry between the cybernetic 
system and the environment. Until now, not much intelligence is in our agent. 
Now the credit assignment comes into the game and removes the symmetry 
somewhat. We split the input x^ E Af :=7?,x O into a regular part Ok^O and 
a reward Vk^TlClR. 
We define Xk = VkOk and r^ = r{xk)' The goal of the 
agent should be to maximize received rewards. This is called reinforcement 
learning. The reason for the asymmetry is that eventually we (humans) will 
be the environment with which the agent will communicate and we want to 
dictate what is good and what is wrong, not the other way round. This one-way 
learning, the agent learns from the environment, and not conversely, neither 
prevents the agent from becoming more intelligent than the environment, 
nor does it prevent the environment learning from the agent because the 
environment can itself interpret the outputs yk as a regular and a reward 
part. The environment is just not forced to learn, whereas the agent is. In 
cases where we restrict the reward to two values r£lZ = IB:= {0,1}, r = l is 
interpreted as a positive feedback, called good or correct, and r = 0 a negative 
feedback, called bad or error. Further, let us restrict for a while the lifetime 
(number of cycles) m of the agent to a large but finite value. Let 
m 
i=k 
be the future total reward (called future utility), the agent p receives from 
the environment q in the cycles /c to m. It is now natural to call the agent p*, 
which maximizes Vim (called total utility), the best one:^ 
p * : = a r g m a ^ F C 
^ 
V^'>VZ 
Vp : / < ' , = 2/^*^ 
(4.3) 
For fc = 1 the condition on p is nil. For k>lit 
states that p shall be consistent 
with p* in the sense that they have the same history. U JY,y and m are finite, 
the number of different behaviors of the agent, i.e. the search space, is finite. 
Therefore, because we have assumed that q is known, p* can effectively be 
determined by pre-analyzing all behaviors. The main reason for restricting 
to finite m was not to ensure computabihty of p* but that the hmit 
m-^oo 
might not exist. The ease with which we defined and computed the optimal 
policy p* is not remarkable. Just the (unrealistic) assumption of a completely 
known deterministic environment q has trivialized everything. 
^ Note that a possible additional dependence of p on y<fe as mentioned in Defi-
nition 4.1 can be eliminated by recursive substitution; see below. Similarly for 
q-
^ argmaxpy(p) is the p that maximizes V{'). If there is more than one maximum 
we might choose the lexicographically smallest one for definiteness. 

130 
4 Agents in Known Probabilistic Environments 
4.1.4 AI Model for Known Prior Probability 
Let us now weaken our assumptions by replacing the environment q with a 
probability distribution fi{q) over chronological functions. Here /i might be 
interpreted in two ways. Either the environment itself behaves stochastically 
defined by /x or the true environment is deterministic, but we only have sub-
jective (probabilistic) information of which is the true environment. Combi-
nations of both cases are also possible. We assume here that /i is known and 
describes the true stochastic behavior of the environment. The case of un-
known /i with the agent having some beliefs about the environment lies at the 
heart of the AI^ model described in Chapter 5. 
The best agent is now the one that maximizes the expected utility (called 
value function) F^^Vf^ :=X]g/^(^)^im- This defines the Al/i model. 
We need the concept of a value function in a slightly more general form. 
We now give a more formal definition for V^^. Let us assume we are in cycle 
k with history ipci...yjck-i and ask for the best output tjk- Further, let Qk :— 
{q-q{y<k) — i<k} be the set of all environments producing the above history. 
We say that q^Qk is consistent with history ific^k- The expected reward for 
the next m—k-^\ cycles (given the above history) is called the value of policy 
p and is given by a conditional probability: 
^.eQu 
^(^)^A 
pq 
km 
Policy p and environment // do not determine history yJc^ki unlike the deter-
ministic case because the history is no longer deterministically determined by 
p and g, but depends on p and fj, and on the outcome of a stochastic process. 
Every new cycle adds new information {xi) to the agent. This is indicated 
by the dots over the symbols. In cycle k we have to maximize the expected 
future rewards, taking into account the information in the history 2/r<fc. This 

4.1 The Al/i Model in Functional Form 
131 
information is not already present in p and q/jji at the agent's start, unlike in 
the deterministic case. 
Furthermore, we want to generalize the finite lifetime m to a dynamic 
(computable) farsightedness hk = mk — k+l>l, 
called horizon. For 
mk=mwe 
have our original finite lifetime; for hk = h the agent maximizes in every cycle 
the next h expected rewards. A discussion of the choices for rrik is delayed to 
Section 5.7. The next hk rewards are maximized by 
PI '= argmaxV;^^ (yr</,), 
pePk 
where Pk := {p: 3yk :p(x</c) =y<kyk} is the set of policies consistent with 
the current history. Note that p^ depends on k and is used only in step k 
to determine yk by pl{x<:k\y<k) = y<kyk- After writing yk the environment 
replies with Xk with (conditional) probability lJi{Qk-{-i)/l^iQk)- This proba-
bilistic outcome provides new information to the agent. The cycle fc+1 starts 
with determining ^^^.i from p\^i (which can differ from p^ for dynamic m^) 
and so on. Note that pj^ implicitly also depends on y<^k because Pk and Qk 
do so. But recursively inserting p%_i and so on, we can define 
V^x^k) '= pl{x<k\pl-i{x<k-il-Pl))- 
(4.7) 
It is a chronological function and is computable ii X^y and ruk are finite and 
fi is computable. For constant m one can show that the policy (4.7) coincides 
with the AI^ model (Definition 4.4). This also proves 
Km(^<fc) ^ Vkmiw^<k) Vp G Pfc, 
i.e. for all p consistent with ^<fc, 
(4.8) 
similarly to (4.3) (see Problem 4.1). For k = l this is obvious. We also call 
(4.7) Al/i model. For deterministic^ /x this model reduces to the deterministic 
case discussed in the last subsection. 
It is important to maximize the sum of future rewards and not, for instance, 
to be greedy and only maximize the next reward, as is done e.g. in sequence 
prediction. For example, let the environment be a sequence of chess games, 
and each cycle corresponds to one move. Only at the end of each game is a 
positive reward r = 1 given to the agent if it won the game (and made no 
illegal moves). For the agent, maximizing all future rewards means trying to 
win as many games in as short as possible time (and avoiding illegal moves). 
The same performance is reached if we choose hk much larger than the typical 
game lengths. Maximization of only the next reward would be a very bad chess 
playing agent. Even if we would make our reward r finer, e.g. by evaluating the 
number of chessmen, the agent would play very bad chess for hk = l, indeed. 
The Al/i model still depends on /x and rriky rUk is addressed in Section 5.7. 
To get our final universal AI model the idea is to replace /i by the universal 
probability ^, defined later. This is motivated by the fact that ^ converges to /x 
We call a probability distribution deterministic if it assumes values 0 and 1 only. 

132 
4 Agents in Known Probabilistic Environments 
in a certain sense for any /i. With ^ instead of /i our model no longer depends 
on any parameters, so it is truly universal. It remains to show that it behaves 
intelligently. But let us continue step by step. In the following we develop an 
alternative but equivalent formulations of the AI// model. Whereas the func-
tional form presented above is more suitable for theoretical considerations, 
especially for the development of a time-bounded version in Section 7.2, the 
iterative and recursive formulation of the next Section will be more appropri-
ate for the explicit calculations in most of the other sections. 
4.2 The AI/x Model in Recursive and Iterative Form 
4.2.1 Probability Distributions 
We use Greek letters for probability distributions, and underline their argu-
ments to indicate that they are probability arguments. Let Pnfei---:^n) ^^ ^^^ 
probability that an (infinite) string starts with xi...Xn- We drop the index on 
p if it is clear from its arguments: 
Yl 
^fel:n) = X l ^ ^ f e l ^ ^ ) "" P n - l f e < n ) = p{^<n)^ 
p{^) = Po{^) = 1- (4.9) 
We also need conditional probabilities. We prefer a notation that preserves 
the chronological order of the words, in contrast to the standard notation 
p(-|-), which flips it. We extend the definition of p to the conditional case 
with the following convention for its arguments: An underlined argument Xj^ 
is a probability variable, and other non-underlined arguments Xk represent 
conditions. With this convention, the conditional probability has the form 
p{^<n^n) — Pfei:n)/pfe<n)' ^hc cquatiou statcs that the probability that 
a string xi...Xn-i 
is followed by Xn is equal to the probability of xi...a;n* 
divided by the probability of xi...Xn-i'^' 
We use x^ as an abbreviation for 
'strings starting with x\ 
The introduced notation is also suitable for defining the conditional proba-
bility p{yiXi...ynX^) that the environment reacts with xi...Xn under the condi-
tion that the output of the agent is yi...yn- The environment is chronological, 
i.e. input Xi depends on yx^iyi only. In the probabilistic case this means that 
p(yx^f^yk) '=J2x Piy^i.k) is independent of yk, hence a tailing yk in the argu-
ments of p can be dropped. Probability distributions with this property will 
be called chronological. The y are always conditions, i.e. are never underlined, 
whereas additional conditioning for the x can be obtained with the chain rule 
p{WC<nmn) 
= P{y^l:n)/Pim<n) 
^ud 
(4.10) 
p{mi:n) = p{y^i)'p{wim2)'-•p{wc<ny^n)' 
(4-11) 
The second equation is the first equation applied n times. 

4.2 The Al/i Model in Recursive and Iterative Form 
133 
4.2.2 Explicit Form of the AI^ Model 
Let us define the AI/x model p* in a different way. In the next subsection 
we will show that the p* model defined here is identical to the functional 
definition of p* given in the last section. 
Let (i{yjc^kWk) ^^ ^^^ *^^^ probability of input Xk in cycle A:, given the 
history yj:<^kyk\ l^{l&i:k) ^^ ^he true chronological prior probability that the 
environment reacts with xi:k if provided with actions yi:k from the agent. We 
assume the cybernetic model depicted on page 128 (see also book cover) to be 
valid. Next we define the value V^^-^ ^(^i:^) to be the /i-expected reward sum 
rfc+i + .-.+^m in cycles k-\-l to m with outputs yi generated by agent p* that 
maximizes the expected reward sum, and responses Xi from the environment, 
drawn according to fi. Adding r{xk) = Tk we get the reward including cycle k. 
The probability of a:^, given yx^kVki is given by the conditional probability 
fi{yx<:kW^k)' S^ ^^^ expected reward sum in cycles k to m given yx^^kVk is 
vZiw^KkV^) ••= Ylirixk) + v*!^,^jm..k)]-Kwo<kmk)- 
(4-i2) 
Now we ask how p* chooses y^: It should choose yk so as to maximize the 
future rewards. So the expected reward in cycles k to m given yx<^k and yk 
chosen by p* is V^^{yx<:k) :=mdiXy^V^^{yx^kyk) 
(see Figure 4.13). 
Together with the induction start 
VZuJvci-.m) 
••= 0, 
(4.14) 
^km ^^ completely defined. We might summarize one cycle into the formula 
Kmiwo<k) = ma.xY^[r{xk) 
+ V^^^^^{'y€i:k)]'/J^{yJC<:kW^k)- 
(4.15) 
Vk 
Xk 
We introduce a dynamic (computable) farsightedness hk=mk—k^l>l^ 
called 
horizon. For rrik — fyi^ where m is the lifetime of the agent, we achieve optimal 

134 
4 Agents in Known Probabilistic Environments 
behavior, for limited farsightedness hk = h {m = rrik = h-\-k — l), the agent 
maximizes in every cycle the next h expected rewards. A discussion of the 
choices for nik is delayed to Section 5.7. If rrik is our horizon function of p* 
and yr</e is the actual history in cycle k, the output t/k of the agent is explicitly 
given by 
i/k = ^Tgm^xV^I^ (yr^kyk), 
(4.16) 
which in turn defines the policy p*. Then the environment responds Xk with 
probabihty /i{yx^kiKk)- Then cycle fc+1 starts. We might unfold the recursion 
(4.15) further and give i/k nonrecursively as 
Vk = Vk '•= argmaxVmax V...maxV(r(xA,)H-...+r(x^J)-/x(i;a:</,2^;, 
) 
Vk 
^ 
^ Vk + l ^ 
^ 
Vmu ^ 
^ 
" 
Xk 
Xk+1 
Xruf^ 
(4.17) 
This has a direct interpretation: The probability of inputs Xk-.mk i^ cycle k 
when the agent outputs yk-.mk with actual history ific^k is iji{ipc<:kW.k:mk)• The 
future reward in this case is r(xfc) + ...+r(xmfe)- The best expected reward is 
obtained by averaging over the xi (X]^ ) and maximizing over the yi. This has 
to be done in chronological order to correctly incorporate the dependencies 
of Xi and y^ on the history. This is essentially the expectimax algorithm/tree 
[Mic66, RN95]. The Al/i model is optimal in the sense that no other policy 
leads to higher expected reward. The value for a general policy p can be 
written in the form 
^kmi^<k) 
•= Y^{rk^ 
••.+rm)^^{w<kmk 
(4.18) 
4.2.3 Equivalence of Functional and Explicit AI Model 
As is clear from their interpretations, the iterative environmental probability 
fi relates to the functional form in the following way: 
f^imi-.k) = 
E 
^(^) 
(4.19) 
q-qiyi:k)=xi:k 
Theorem 4.20 (Equivalence of functional and explicit AI model) 
The actions of the functional AI model (4,7) coincide with the actions 
of the iterative (^recursive) AI model (4.15)-(4.17) with environments 
identified by (4.19). 
Proof, We prove the equivalence of (4.7) and (4.16) only for A^ —2 and m2 = 3. 
The proof of the general case is completely analogous, except that the notation 
becomes quite messy. 

4.3 Special Aspects of the Al/i Model 
135 
Let us first evaluate (4.6) for fixed yiXi and some p e P2, i-e. p{xi) — 
yiy2 for some 1/2• If the next input to the agent is X2, p will respond with 
p{xiX2)=yiy2y3 for some ys depending on X2' We write 2/3(^2) in the following. 
Dependency on dotted words like xi is not shown, as the dotted words are 
fixed. The numerator of (4.6) simplifies to 
E'^(9)^2"3' = E ^(9)^2^3' = Ew^2)+rM) 
E 
/^(«) 
qeQ2 
q-q{yi)=xi 
^2x3 
q:q{yiy2y3{x2))=xiX2X3 
= 
5Z^^^^2)+r'(X3))-M(?/lil2/2X2y3(x2)x3). 
X2X3 
In the first equality we inserted the definition of (52- In the second equality 
we split the sum over q by first summing over q with fixed X2Xs. This allows 
us to pull V23 = r{x2)+r{xs) 
out of the inner sum. Then we sum over X2a:3. 
Further, we have inserted p, i.e. replaced p by y2 and y3(-). In the last equality 
we used (4.19). The denominator reduces to 
qeQ2 
q-q{yi)=xi 
For the quotient we get 
^2T(yi^i) = ^(r(x2)+r(a:3))-/i(^iXi2/2^22/3(^2)^3)-
X2X2> 
We have seen that the relevant behavior of PGP2 in cycles 2 and 3 is completely 
determined by ^2 and the function yz{-)' 
ms.-KV:^^{yiXi) = maxmax V(r(x2)+r(rc3))'/^(2/ii^i?/2^2?/3(^2)^3) 
= max V" max V'(r(x2)+r (2:3)) -//(y 1X11/2^2^3^3) 
!/2 ^-^ 
ys ^—^ 
X2 
X3 
In the last equality we have used the fact that the functional minimization over 
ys{') reduces to a simple minimization over the word ys when interchanging 
with the sum over its arguments (max^g^.)^^^ =^x2^^^^y3^' 
^^ ^^^ functional 
case y2 is therefore determined by 
y2 = argmaxy'maxy'(r'(x2)+r(x3))-/i(yii:iy2^2y3^3)-
y2 ^-^ 
ys 
^-^ 
X2 
X3 
This is identical to the iterative definition (4.17) with k = 2 and 7712 = 3. • 
4.3 Special Aspects of the AI// Model 
4.3.1 Factorizable Environments 
Up to now we have made no restrictions on the form of the prior probability /x 
apart from being a chronological probability distribution. On the other hand, 

136 
4 Agents in Known Probabilistic Environments 
we will see that, in order to prove rigorous reward bounds, the prior probability 
must satisfy some separability condition to be defined later. Here we introduce 
a very strong form of separability, when /x factorizes into products. We start 
with a factorization into two factors. Let us assume that fi is of the form 
Kmi:n) 
= ^l{m<l) 
' ^2{mi:n) 
(4.21) 
for some fixed / and sufficiently large n>mk- 
For this fi the output yk in 
cycle k of the Al/i agent (4.17) for k>l depends on yjci-k-i and /i2 only and 
is independent of ^ < / and /ii. This is easily seen when inserting 
fJ,{W^<kmk:mJ 
= l^l{W<l) 
'^2{WCl:k-imk:m.) 
(4-22) 
into (4.17). For k<l the output y^ depends on ific^k (this is trivial) and /ii 
only (trivial if ruk <l) and is independent of /X2. The nontrivial case, where 
the horizon rrik > / reaches into the region //2, can be proven as follows (we 
abbreviate m:—mk in the following). Inserting (4.21) into the definition of 
^imiy^<i)^ the factor fxi is 1, as in (4.22). We abbreviate V;^ :== V;^(2/^</), 
as it is independent of its argument. One can decompose 
VkZivc<k) = V;Xi(t^<fc) + C ^ 
(4.23) 
For k = l this is true because the first term on the r.h.s. is zero. For k<l we 
prove the decomposition by induction from k-\-l to k. 
Kmiy^<k) 
= max^[r(xfc) + V7^i,^_i(yri:/e) + V*J^]'fii{]fi:<kmk) 
^(r(xfc) + V^^^i__^{yx^k))'fii{w^<kmk) 
+ Km 
Vk 
Xk 
max 
Vk 
Xk 
Inserting (4.23), valid for k-\-l by induction hypothesis, into (4.15) gives the 
first equality. In the second equality we have performed the Xk sum for the 
Vj^-/ii term, which is now independent of yk- It can therefore be pulled 
out of max^;^. In the last equality we again used the definition (4.15). This 
completes the induction step and proves (4.23) for k<l. Output yk can now 
be represented as 
yk = argmaxy^*^(7)r<fcyfc) =^ argmaxF^*f-i(^<fcyfe), 
(4.24) 
Vk 
Vk 
' 
where (4.16) and (4.23) and the fact that an additive constant Vj^ does not 
change argmax^^^ have been used. V^^_-^{yio<^kyk) and hence i/k is independent 
of /i2 for k<L Note that yk is also independent of the choice of m, as long as 
m>L 

4.3 Special Aspects of the AI/x Model 
137 
In the general case of an (infinite) sequence of consecutive episodes one 
can show an analogous result: 
If all episodes have a length of at most /, i.e. n^+i — n^. < I, and if we choose 
the horizon hk to be at least /, then m^ >/c+Z —1 > n r + / > n r + i , and hence 
t = nr-\-i independent of mfe. This means that for factorizable fi there is no 
problem in taking the limit ruk-^oo. Maybe this limit can also be performed 
in the more general case of a sufficiently separable fi. The (problem of the) 
choice of rrik will be discussed in more detail in Section 5.7. 
Although factorizable /i are too restrictive to cover all AI problems, they 
often occur in practice in the form of repeated problem solving, and hence, 
are worthy of study. For example, if the agent has to play games like chess re-
peatedly, or has to minimize different functions, the different games/functions 
might be completely independent, i.e. the environmental probability factor-
izes, where each factor corresponds to a game/function minimization. For 
details, see Section 6.3 on strategic games and Section 6.4 on function mini-
mization. 
Further, for factorizable /i it is probably easier to derive suitable reward 
bounds for the universal AI^ model defined in the next chapter, than for the 
separable cases which will be introduced in Section 5.3. This could be a first 
step toward a definition and proof for the general case of separable problems. 
One goal of this subsection was to show, that the notion of a factorizable fi 
could be the first step toward a definition and analysis of the general case of 
separable /i. 

138 
4 Agents in Known Probabilistic Environments 
4.3.2 
Constants and Limits 
We have in mind a universal agent with complex interactions that is at least as 
intelligent and complex as a human being. One might think of an agent whose 
input yk comes from a digital video camera, and the output Xk is some image 
to a monitor"^, only for the rewards we might restrict to the most primitive 
binary ones, i.e. Vk^lB. So we think of the following constant sizes: 
1 <c {iivkook)) < 
k < m <^ \y X ;Y\ 
The first two limits say that the actual number k of inputs/outputs should 
be reasonably large compared to the typical length (i) of the input/output 
words, which itself should be rather sizeable. The last limit expresses the fact 
that the total lifetime m (number of I/O cycles) of the agent is far too small 
to allow every possible input to occur, or to try every possible output, or to 
make use of identically repeated inputs or outputs. We do not expect any 
useful outputs for k^{i). 
More interesting than the lengths of the inputs is 
the complexity K{xi...Xk) 
of all inputs until now (see Definition 2.9). The 
environment is usually not "perfect". The agent could either interact with 
a imperfect human or tackle a nondeterministic world (due to quantum me-
chanics or chaos)^. In either case, the sequence contains some noise, leading 
to K{xi...Xk) oc {£)'k. The complexity of the probability distribution of the 
input sequence is something different. We assume that this noisy world oper-
ates according to some simple computable rules: K{iik)<^{t/-k, 
i.e. the rules 
of the world can be highly compressed. We may allow environments in which 
new aspects appear for fc—>(X), causing an unbounded K{fik)-
In the following we never use these limits, except when explicitly stated. 
In some simpler models and examples the size of the constants will even 
violate these limits (e.g. i{xk)=i{yk) 
= ^)^ but it is the limits above that the 
reader should bear in mind. We are only interested in theorems that do not 
degenerate under the above limits. In order to avoid cumbersome convergence 
and existence considerations we make the following assumptions throughout 
this book: 
Assumption 4.28 (Finiteness) We assume that 
• the input/perception space X is finite, 
• the output/action space y is finite, 
• the rewards are nonnegative and bounded, i.e. r;fcG7^C[0,r„ 
• the horizon m is finite. 
Humans can only simulate a screen as output device by drawing pictures. 
Whether there exist truly stochastic processes at all is a difficult question. At 
least the quantum indeterminacy comes very close to it. See Section 8.6.2 for a 
detailed discussion. 

4.3 Special Aspects of the AI^ Model 
139 
Finite X and bounded 7^ (each separately) ensure existence of /x-expectations 
but are sometimes needed together. Finite y ensures that argmaxy^e3;[...] 
exists, i.e. that maxima are attained, while finite m avoids various technical 
and philosophical problems (Section 5.7), and positive rewards are needed 
for the time-bounded AIXK/ model (Section 7.2). Many theorems can be 
generalized by relaxing some or all of the above finiteness assumptions. 
4.3.3 Sequential Decision Theory 
In the following we clarify the connection of (4.15) and (4.16) to the Bellman 
equations [Bel57, BT96] of sequential decision theory and discuss similarities 
and diS'erences. We consider a Markov decision process (MDP) where, with 
probability M^g, the agent under consideration should reach (environmental) 
state j€«S when taking action a E ^ in (the current) state i^S, 
If the agent 
receives reward R(i\ the optimal policy p*, maximizing expected utility (de-
fined as the sum of future rewards), and the utility IJ{%) of policy p* are 
p* {%) = arg max ^ 
M«.C/(i), 
V{%) = R{%) + max Y, MtjU{j). 
(4.29) 
i 
3 
See [RN95, BT96] for details and further references. Let us identify 
S = {yxX)\ 
A = y, 
a = yk, 
M^^ = ^{yc^^mk). 
P*(0 ^ 2/fc, 
i = ^<fc, 
R[i) = r(xfc_i), 
U(i) = V^_^^^{wc^k) = r{xk-i) -f VCm{w<k). 
j = WCi:k, 
R{j) 
= r{Xk), 
U{j) 
= VkmiW^l-.k) 
= r{Xk) + T4*+l,m(y^l:/c), 
where we further set Mg = 0 if z is not a starting substring of j or \i 
a^yk-
This ensures the sum over j in (4.29) to reduce to a sum over Xk- If we set 
mk=m 
and insert (4.12) into (4.16), it is easy to see that (4.29) coincides 
with (4.15) and (4.16). 
Note that despite of this formal equivalence, we were forced to use the 
complete history 2/^<A: as environmental state i. The Al/i model assumes nei-
ther stationarity, nor Markov property nor complete accessibility of the en-
vironment, as any assumption would restrict the appficability of Al/i. The 
consequence is that every state occurs at most once in the lifetime of the 
agent. Every moment in the universe is unique! Even if the state space could 
be identified with the input space Af, inputs would usually not occur twice by 
the assumption k<^\X\, made in the last subsection. Further, there is no obvi-
ous universal similarity relation on {Xxyy 
allowing an effective reduction of 
the size of the state space. Although many algorithms (like value and policy 
iteration) have problems in solving (4.29) for huge or infinite state spaces in 
practice, there is no principle problem in determining p* and [/, as long as fj, 
is known and computable, and \X\, \y\ and m are finite. 
Things drastically change if /i is unknown. Reinforcement learning algo-
rithms [KLM96, SB98] are commonly used in this case to learn the unknown 

140 
4 Agents in Known Probabilistic Environments 
fji or directly its value. They succeed if the state space is either small or has ef-
fectively been made small by so-called generalization techniques. In any case, 
the solutions are either ad hoc, or work in restricted domains only, or have 
serious problems with state space exploration versus exploitation, or are prone 
to diverge, or have nonoptimal learning rate. There is no universal and op-
timal solution to this problem so far. In the next chapter we present a new 
model and argue that it formally solves all these problems in an optimal way. 
It is not concerned with learning ji directly. All we do is to replace the true 
prior probability /i by a universal probability ^, which is shown to converge 
to /i in a sense. 
4.4 Problems 
4.1 (Value dominance p) [C20s] In (4.5) Vkmiu^Kk) was defined as the /a-
expected future reward sum rk-\---'-\-rm with actions generated by policy p, 
and fixed history 2/^</c. The optimal policy p^ was defined as the one with 
maximal total /i-value, i.e. p^ := argmaXpFj^(e). The corresponding value 
function is Km(y^^<fc) ^= C''(2^<'t)- Obviously, VZ{e) > VZ{e)Wp. Show 
that this implies VJ^*^(yr</c) > Vj[f^(2/z:<fc) for all p consistent with yx<^k- The 
derivation of a result of this form goes hand in hand with the derivation of 
Bellman's equations [Ber95a]. 
4.2 (Probabilistic policies) [C15usi] In this chapter we only gave formal 
definitions of the value function for deterministic policies, but allowed proba-
bilistic environments. Generalize the definition of the value function (4.18) to 
probabilistic policies TT in the following way: 
^/T 
= 
E2;ri:^(^l+---+^m)M(^m|l/C<mym)7r(ym|2P^<m).-.M(a:i|?/l)7r(yi) 
= 
E^i..^(^A:+---+^m)/i(^l:m|yi:m)7r(yi:mk<m) 
and similarly for VkJ^iw^Kk)- We used here the conventional notation /i(-|-) for 
conditional probabilities to emphasize the following oddity. The last equality 
seems to say something like p{x\y)p{y\x) —p{xSzy), which would contradict 
Bayes' law p{x\y)p{y)=p{xSzy). 
Show that everything goes right here. What 
important property (defined in the main text) is used to arrive at the above 
expressions? Show that max^rVT^maxpl/J, and that among optimal policies 
there is always a deterministic one. Since we are mainly interested in optimal 
policies, the restriction to deterministic policies is not serious. Generalize the 
theorems in this and later chapters involving V^ to F^. A serious treatment 
of probabilistic policies can be found in game theory, where optimal policies 
can also be truly probabilistic [OR94]. 

The Three Laws of Robotics: 
1. A robot may not injure a human being, or, through 
inaction, allow a human being to come to harm. 
2. A robot must obey the orders given it by human beings 
except where such orders would conflict with the First 
Law. 
3. A robot must protect its own existence as long as such 
protection does not conflict with the First or Second 
Isaac Asimov 
i - - 
- 
Isaac Asimov 
(^920-1992) 
5 The Universal Algorithmic Agent AIXI 

142 
5 The Universal Algorithmic Agent AIXI 
Active systems, like game playing (SG) and optimization (FM), cannot be 
reduced to induction systems. The main idea in this book is to generalize uni-
versal induction to the general agent model described in Chapter 4. For this, 
we generalize Solomonoff's universal prior M to include actions as conditions 
and replace /i by M in the rational agent model, resulting in the AIXI model. 
In this way the problem that the true prior probability /x is usually unknown is 
solved. Convergence of M-^// will be shown, indicating that the AIXI model 
could behave optimally in any computable but unknown environment with 
reinforcement feedback. 
The main focus of Chapter 5 is to investigate what we can expect from a 
universally optimal agent and to clarify the meanings of universal^ optimal, 
etc. Similarly to the induction case, it is convenient to consider a general 
mixture distribution ^ of a weighted sum of distributions lyeM, where M. is 
any class of distributions including the true environment fi. We show that the 
Bayes optimal policy p^ based on the mixture ^ is self-optimizing in the sense 
that the average value converges asymptotically for all fi^M 
to the optimal 
value achieved by the (infeasible) Bayes optimal policy p^ which knows /x in 
advance. We show that the necessary condition that M. admits self-optimizing 
policies at all is also sufficient. No other structural assumptions are made on 
M. 
One can show that bandits, i.i.d. processes, classification tasks, certain 
classes of POMDPs, (k^^-order) ergodic MDPs, factorizable environments, re-
peated games, and prediction problems admit self-optimizing policies. Un-
fortunately, the class A^^ of all enumerable semimeasures does not admit 
self-optimizing policies. This forces us to lower our expectation about univer-
sally optimal agents and to introduce other (weaker) performance measures. 
Finally, we show that p^ is Pareto optimal in the sense that there is no other 
policy yielding higher or equal value in all environments ueM 
and a strictly 
higher value in at least one. Pareto optimality holds for any choice of M 
(including Aiu). 
5.1 The Universal AIXI Model 
5.1.1 Definition of the AIXI Model 
We have developed enough formalism to suggest our universal AIXI model. 
All we have to do is to suitably generalize the universal semimeasure M from 
Section 2.4 and replace the true but unknown prior probability fi^^ in the Al/i 
model by this generalized M^^. In what sense this AIXI model is universal 
will be discussed subsequently. 
In the functional formulation we define the universal probability M^^ of 
an environment q simply as 2~^^^^\ 

5.1 The Universal AIXI Model 143 
M{q) — 2-^(^) 
The definition could not be easier!^'^ Collecting the formulas of Section 4.1 and 
replacing fi{q) by M{q) we get the definition of the AIXI agent in functional 
form. Given the history yc<fc, the functional AIXI agent outputs 
y, := argmax 
max 
Yl '^~'^'^'^kL 
(^-l) 
(}-q{y<k)=x<:k 
in cycle /c, where V^^^ is the total reward of cycles k to rrik when agent p 
interacts with environment q. We have dropped the denominator Ylq^i^) fr<^in 
(4.6) as it is independent of the p^Pk, 
and a constant multiplicative factor 
does not change argmax^^. 
For the iterative formulation, the universal probability M can be obtained 
by inserting the functional M{q) into (4.19): 
M{mi:k) = E 
2-'^'' 
(5.2) 
q'q{yi:k)=xi..k 
Replacing /i by M in (4.17) the iterative AIXI agent outputs 
^fe = y f :==argmaxVmax V ...max V(r(a:/c)+...+r(x^J)M(yr<fc^^ 
Vk ^-^ Vk + l ^-^ 
Vruk ^-^ 
•^k 
^k-\-\ 
'^rrik 
(5.3) 
in cycle k given the history ^<fc. 
One subtlety has been passed over. Like in the the sequence prediction 
(SP) case, M is not a probability distribution but satisfies only the weaker 
inequalities 
Y,M{mi..n) 
< Mim^J 
and M(e) < 1. 
(5.4) 
Note that the sum on the l.h.s. is not independent of yn, unlike for chrono-
logical probability distributions. Nevertheless, it is bounded by something 
(the r.h.s) that is independent of yn- This is because the sum in (5.2) runs 
over (partial recursive) chronological functions only, and the functions q that 
satisfy q{yi:n) = ^<n* (with * G A*) are a subset of the functions satisfying 
Q{y<n) — x<n' We will in general call functions satisfying (5.4) chronological 
semimeasures. The important point is that the conditional probabilities (4.10) 
are < 1 , like for true probability distributions. 
^ It is not necessary to use 2~^^^^ or something similar as some readers may expect 
at this point, because for every program q there exists a functionally equivalent 
program q with K{q) =i{q). 
^ Here and later we identify objects with their coding relative to some fixed Turing 
machine U. For example, if g is a function K{q) :=K{{q)) with (g) being a binary 
coding of q such that U{{q),y) = q{y). Reversely, if q already is a binary string we 
de^ne 
q{y):=U{q,y). 

144 
5 The Universal Algorithmic Agent AIXI 
The equivalence of the functional and iterative AI model proven in Sec-
tion 4.2 is true for every chronological semimeasure p, in particular for M; 
hence we can talk about the AIXI model in this respect? It (slightly) depends 
on the choice of the universal Turing machine. i{{q)) is defined only up to an 
additive constant. The AIXI model also depends on the choice oi 
X^lZxO 
and y^ but we do not expect any bias when the spaces are chosen sufficiently 
simple, e.g. all strings of length 2^^. Choosing IN as the word space would be 
ideal, but whether the maxima (suprema) exist in this case, must be shown 
beforehand. The only nontrivial dependence is on the horizon function m/c, 
which will be discussed in Section 5.7. So, apart from rrik and unimportant 
details, the AIXI agent is uniquely defined by (5.1) or (5.3). It does not de-
pend on any assumption about the environment, apart from being generated 
by some computable (but unknown!) probability distribution. 
5.1.2 
Universality of M ^ ^ and ^^^ 
In which sense the AIXI model is optimal will be clarified later. In this and 
the next subsection we show that M^^ defined in (5.2) is universal and con-
verges to /i^^ analogous to the SP case (2.27) and (2.25). The proofs are 
generalizations from the SP case. The y are pure spectators and cause no 
difficulties in the generalization. In (2.21) U{p)—x^ 
produces strings starting 
with x, whereas in (5.2) we can demand q to output exactly n words xi^n as 
q "knows" n from the number of input words 
yi...yn-
As in the SP case (2.26), there is an alternative definition of M that 
coincides with (5.2) within a multiplicative constant of order one: 
^imv.n) 
••= ^2-^^^^p{uCi:n) 
= M{yr,J, 
(5.5) 
P 
where the sum runs over all enumerable chronological semimeasures. The 
2~^^^) weighted sum over probabilistic environments p coincides with the sum 
over 2~^^^^ weighted deterministic environments g, as will be proven below. In 
Section 5.10 we show that an enumeration of all enumerable functions can be 
converted into an enumeration of enumerable chronological semimeasures p. 
K{p) is co-enumerable, and therefore ^ defined in (5.5) is itself enumerable. 
The representation (5.2) is also enumerable. As X]p2~^^^^ < 1 and the p's 
satisfy (5.4), ^ is a chronological semimeasure as well. If we pick one p in (5.5) 
we get the universality property "for free" 
Iterative AIXI is not equivalent to recursive AIXI. Solomonoff normalization 
(2.30) rescues equivalence, but destroys enumerability of the universal value V^^. 
Better is to formally make all partial functions q in (5.2) and elsewhere total by 
defining Ok somehow and rfc=0 if originally undefined, or equivalently replacing 
M(...0) by 1 — ^ 
,^M{...r_^). Then M becomes a (proper) measure, functional 
and iterative and recursive AIXI are equivalent, and V^^ is enumerable, although 
the modified M is not. 

5.1 The Universal AIXI Model 
145 
iiWL^J 
> 2-^(''Vte:„)- 
(5-6) 
^ is a universal element in the sense of (5.6) in the set of all enumerable 
chronological semimeasures. 
To prove universality of M we have to prove (5.5). We note that for every 
enumerable chronological semimeasure p there exists a Turing machine T with 
Pim-.n) = 
E 
2-'^^> 
and 
1{T) ± K{p). 
(5.7) 
We will not prove (5.7) here (see Problem 5.3). Given T, the universality of 
M follows from 
q-'U{q,yx.,n)=xi:n 
q' -UiTq' ,yi:n)=xi:n 
q':T{q',yi:n)=xi:n 
The first equality and (5.2) are identical by definition. In the inequality we 
have restricted the sum over all g to g of the form q = Tq\ The third relation 
is true as running U on Tz is a simulation of T on z. The last equality follows 
from (5.7). All enumerable universal chronological semimeasures coincide up 
to a multiplicative constant, as they mutually dominate each other. Hence, 
definitions (5.2) and (5.5) are indeed equivalent. 
5.1.3 Convergence of ^^^ to fi^^ 
In Section 3.9.2 we proved the following entropy inequality: 
N 
N 
N 
N 
Y,{yi-Zi)^ 
<Y,yi^^^ 
with 
^yi 
= l, 
Y.Zi<l. 
(5.8) 
In Section 5.11 we give a different proof since it contains some ideas that could 
be interesting for their own sake? If we identify N=\JY\,i=Xk, 
yi=fi{yx<:kW^k) 
and Zi — ^{y]c^kWk)^ multiply both sides with //(i^^.^), take the sum over x<fc 
and k and use the chain rule ii{i^^^) -ii{y]c^kWk) — ^^iWi-.k)^ we get 
n 
2 
^ 
( 
\ 
EE/^(^<fe)(/^(y^<'^^fe)~^(2^<*^2fffc)) ^EE^fe:fc)^"^f,^'^\^') = -
(5.9) 
In the r.h.s. we can replace ^x^,^P'{w.i:k) t>y Scci.^/^(^i:n)? ^^ ^^^ argument 
of the logarithm is independent of Xk^ri-.n- The k sum can now be brought 
^ Actually, a proof similar to the one in Section 5.11 of an inequality similar to 
(5.8), found in [HutOO], in a sense initiated the whole book. 

146 
5 The Universal Algorithmic Agent AIXI 
into the logarithm and converts to a product. Using the chain rule (4.10) for 
/i and ^ we get 
(5.10) 
where we have used the universality property (5.6) of ^ in the last step. The 
line of reasoning is identical to that in (3.18); the y are, again, pure spec-
tators. This will change when we analyze loss/value bounds analogous to 
Theorem 3.48. 
Bound (5.9)/(5.10) shows that the /i-expected squared difference of /x and 
^ is finite for computable //. This, in turn, shows that ^{yjc^kW^k) converges 
to ii{yjc^kW^k) ^^^ A: —> oc with //-probability 1. If we take a finite product 
of ^'s and use the chain rule, we see that also ^{yjc^kW^k-.k+r) converges to 
^{yjc<^kW^k:k+r)' More generally, by supplementing the results on multistep 
predictions in Section 3.7.1 with action y as conditions we have^ 
fi.rr 
^.rr' 
\^^^u(icr 
uirr' 
1 / 
''•'^'^' 
'^ ^^ ^ rrik-k + l < hmax < oo, 
i;[W<kmk:m,) 
f^[y^<kmk:mj 
<y 
'^^ 
foT gexierel hk ~ TTik-k + l. 
(5.11) 
This gives hope that the outputs ijk of the AIXI model (5.3) could converge 
to the outputs ijk from the Al/i model (4.17). 
We want to call an AI model universal, 
if it is //-independent (unbiased, 
model-free) and is able to solve any solvable problem and learn any learnable 
task. Further, we call a universal model universally 
optimal if there is no 
program that can solve or learn significantly faster (in terms of interaction 
cycles). Indeed, the AIXI model is parameter free, ^ converges to /j, (5.11), the 
AI// model is itself optimal, and we expect no other model to converge faster 
to Al/i by analogy to SP (Theorem 3.48): 
This is our main claim. In a sense, the intention of the remaining sections is 
to define this statement more rigorously and to give further support. 
5.1.4 
Intelligence Order Relation 
We define the (^-expected reward in cycles /c to m of a policy p similar to (4.6) 
and (5.1). We extend the definition to programs p^Pk 
that are not consistent 
with the current history. 
V;l(yi<fe) := ;^ 
E 
^"'"^ • ^Z 
(5.13) 
q:q{y<k)=x<k 
^ Here and elsewhere we interpret ak -^ bk as an abbreviation for ak — bk —^ 0; 
limk-^oobk itself may not exist. 

5.2 On the Optimality of AIXI 
147 
The normaUzation M is again only necessary for interpreting Vkm as the ex-
pected reward but is otherwise unneeded. For consistent poHcies pE Pk we 
define p:=p. For p 0 i \ , p is a modification of p in such a way that its outputs 
are consistent with the current history ^<fc, hence pEPk, but unaltered for 
the current and future cycles > k. Using this definition of Vkm we could take 
the maximum over all policies p in (5.1), rather than only the consistent ones. 
As the algorithm p^ behind the AIXI agent maximizes ^^mk' ^^ have p^ ^p 
for all p. The AIXI model is hence the most intelligent agent w.r.t. >z. Relation 
^ is a universal order relation in the sense that it is free of any parameters 
(except ruk) or specific assumptions about the environment. A proof that 
^ is a reliable intelligence order (which we believe to be true) would prove 
that AIXI is universally optimal. We could further ask: How useful is ^ for 
ordering policies of practical interest with intermediate intelligence, or how can 
>: help to guide toward constructing more intelligent systems with reasonable 
computation time? An eflPective intelligence order relation y^ will be defined 
in Section 7.2, which is more useful from a practical point of view. 
5.2 On the Optimality of AIXI 
In this section we outline ways toward an optimality proof of AIXI, which will 
be followed thereafter, but not always to the end. Sources of inspiration are the 
SP loss bounds proven in Chapter 3 and optimality criteria from the adaptive 
control literature (mainly) for linear systems [KV86]. The value bounds for 
AIXI are expected to be, in a sense, weaker than the SP loss bounds because 
the problem class covered by AIXI is much larger than the class of induction 
problems. Convergence of ^ to // has already been proven, but is not sufficient 
to establish convergence of the behavior of the AIXI model to the behavior of 
the Al/i model. We will focus on the following approaches toward a general 
optimality proof: 
What is meant by universal optimality. The first step is to investigate 
what we can expect from AIXI, i.e. what is meant by universal optimality. A 
"learner" (like AIXI) may converge to the optimal informed decision-maker 
(like AI^) in several senses. Possibly relevant concepts from statistics are con-
sistency, self-tunability, self-optimization, efficiency, unbiasedness, asymptotic 
or finite convergence [KV86], Pareto optimality, and some others defined in 

148 
5 The Universal Algorithmic Agent AIXI 
Section 5.3. Some concepts are stronger than necessary; others are weaker 
than desirable but are suitable to start with. Self-optimization is defined as 
the asymptotic convergence of the average true value ^ Vf^^ of AIXI to the 
optimal value ~[V^J^. Apart from convergence speed, self-optimization of AIXI 
would most closely correspond to the loss bounds proven for SP. We investi-
gate which properties are desirable and under which circumstances the AIXI 
model satisfies these properties. We will show that no universal model, in-
cluding AIXI, can in general be self-optimizing. On the other hand, we show 
that AIXI is Pareto optimal in the sense that there is no other policy which 
performs better or equal in all environments, and strictly better in at least 
one. 
Limited environmental classes. The problem of defining and proving gen-
eral value bounds becomes more feasible by considering, in a first step, re-
stricted concept classes. We analyze AIXI for known classes (Markovian, fac-
torizable, predictive, game, optimization, and supervised learning environ-
ments) in Chapter 6 and for the classes (forgetful, relevant, asymptotically 
learnable, farsighted, uniform, pseudo-passive, and passive) defined in Sec-
tion 5.3. 
Generalization of A I X I to general Bayes mixtures. Another approach 
is to generalize AIXI to AI^, where ^() = Yli^eM'^^^O 
is a general Bayes 
mixture of distributions v in some class M. U M is the multi-set of enumer-
able chronological semimeasures, then AI^ coincides with AIXI. If M contains 
only passive environments, then AI^ reduces to the A^ predictor, which has 
been shown to perform well in Section 3.4.1. We show that these loss/value 
bounds generalize to wider classes, at least asymptotically. Promising classes 
are, again, the ones described in Section 5.3. Especially for ergodic MDPs we 
show that AI^ is self-optimizing. Obviously, the least we must demand from 
A4 to have a chance of finding a self-optimizing policy is that there exists some 
self-optimizing policy at all. This necessary condition will also be sufficient. 
More generally, the key is not to prove absolute results for specific problem 
classes, but to prove relative results of the form "if there exists a policy with 
certain desirable properties, then AI^ also possesses these desirable proper-
ties" . If there are tasks that cannot be solved by any policy, AI^ cannot be 
blamed for failing. Note that in this approach we have for each environmental 
class A^ a corresponding model AI^, wheregis in the previous approach the 
same AIXI model is analyzed for all environmental classes. 
Optimality by construction. A possible further approach toward an op-
timality "proof" is to regard AIXI as optimal by construction. 
This perspec-
tive is common in various (simpler) settings. For instance, in bandit prob-
lems, where pulling arm i leads to reward 1 (0) with unknown probability pi 
(l—pi), 
the traditional Bayesian solution to the uncertainty about pi is to 
assume a uniform (or Beta) prior over pi and to maximize the (subjectively) 
expected reward sum over multiple trials. The exact solution (in terms of 

5.3 Value Bounds and Separability Concepts 
149 
Gittins indices) is widely regarded as "optimal", although justified alterna-
tive approaches exist. Similarly, but simpler, assuming a uniform subjective 
prior over the Bernoulli parameter p(^) G [0,1], one arrives at the reasonable, 
but more controversial, Laplace rule for predicting i.i.d. sequences. AIXI is 
similar in the sense that the unknown /i G At is the analogue of the unknown 
J9G [0,1], and the prior beliefs Wiy=^2~^^^'^ justified by Occam's razor are the 
analogue of a uniform distribution over [0,1]. In the same sense as Gittins' so-
lution to the bandit problem and Laplace' rule for Bernoulli sequences, AIXI 
may also be regarded as optimal by construction. Theorems relating AIXI 
to Al/i would not be regarded as optimality proofs of AIXI, but just as how 
much harder it becomes to operate when /i is unknown, i.e. the achievements 
of the first three approaches are simply reinterpreted. 
5.3 Value Bounds and Separability Concepts 
5.3.1 
Introduction 
The values Vkm associated with the AI systems correspond roughly to the 
negative error measure —E^ or negative loss —L^ of the SP systems. In SP, we 
were interested in small bounds for the error excess Ef^-E^. 
Unfortunately, 
simple value bounds for AIXI in terms of Vkm analogous to the loss bound in 
Theorem 3.48 do not hold. We even have difficulties in specifying what we can 
expect to hold for AIXI or any AI system that claims to be universally optimal. 
Consequently, we cannot have a proof if we don't know what to prove. In SP, 
the only important property of /x for proving error bounds was its complexity 
K{/JL). We will see that in the AI case, there are no useful bounds in terms 
of K{fi) only. We either have to study restricted problem classes or consider 
bounds depending on other properties of fi, rather than on its complexity only. 
In the following, we will exhibit the difficulties by two examples and introduce 
concepts that may be useful for proving value bounds. Despite the difficulties 
in even claiming useful value bounds, we nevertheless firmly believe that the 
order relation in Definition 5.14 correctly formalizes the intuitive meaning of 
intelligence and, hence, that the AIXI agent is universally optimal. 
5.3.2 
(Pseudo) Passive /x and the HeavenHell Example 
In the following we choose rrik = m. We want to compare the true, i.e. /i-
expected, value F/^ of a /i-independent universal policy p^^^* with any other 
policy p. Naively, we might expect the existence of a policy p^^** that maxi-
mizes V/^, apart from additive corrections of lower order for m-^cx), 
ViC" > VZ-oi...) 
\/f,,p. 
(5.15) 
Such policies are sometimes called self-optimizing [KV86]. Note that V^J^ > 
Fi^Vp, but * = p ^ is not a candidate for a universal p^««^, as it depends 

150 
5 The Universal Algorithmic Agent AIXI 
on 11. On the other hand, the policy p^ of the AIXI agent maximizes V^^ 
by definition [p^ hp)- As V^^ is thought to be a guess of F/^, we might 
expect p^^^^—p^ to approximately maximize F/^, i.e. (5.15) to hold. Let us 
consider the problem class (set of environments) {/io,/ii} with 3^r=7^ = {0,l} 
and Vk — Siy^ in environment fXi. The first action yi decides whether you go 
to heaven with all future rewards Vk being 1 (good), or to hell with all future 
rewards being 0 (bad). Note that fii are (deterministic, non-ergodic) MDPs: 
^^11 )^7=o\ 
^^^^^ ) ^^^»(HeavenJ 
It is clear that if //^, i.e. i is known, the optimal policy p^^ is to output yi—i 
in the first cycle with Vi^^ — m. On the other hand, any unbiased policy p^^^^ 
independent of the actual ji either outputs yi — l or ^i — 0. Independent of the 
actual choice ^/i, there is always an environment {/i — fii-y^) for which this 
choice is catastrophic (F/^ ^^ = 0). No single agent can perform well in both 
environments JIQ and /ii. The r.h.s. of (5.15) equals m—o{m) for p—p^. For all 
^hest ^]^gpg is a /i for which the l.h.s. is zero. (The situation remains the same 
if we add a third action allowing to stay in the start state. See also Problem 
5.2 for a similar two-state environment.) We have shown that no p^^^^ can 
satisfy (5.15) for all ji and p, so we cannot expect p^ to do so. Nevertheless, 
there are problem classes for which (5.15) holds, for instance, SP and CF and 
ergodic MDPs. For SP, (5.15) is just a reformulation of Theorem 3.48 with 
an appropriate choice for p^^^^ (which differs from p^, see next section). We 
expect (5.15) to hold for all inductive problems in which the environment is 
not influenced by the output of the agent. (Of course, the reward feedback Vk 
depends on the agent's output. What we mean is, like in sequence prediction, 
that the true sequence is not influenced by the agent.) We want to call these 
/i passive or inductive environments. Further, we want to call M and fiEM 
satisfying (5.15) with p^^^^—p^ pseudo-passive. So we expect inductive // to 
be pseudo-passive. 
5.3.3 The OnlyOne Example 
Let us give a further example to demonstrate the difficulties in establishing 
value bounds. Let A'= 7^ = {0,1} and \y\ be large. We consider all (determin-
istic) environments in which a single complex output y* is correct (r = l) and 
all others are wrong (r = 0). The problem class M. is defined by 
M := {fly* : y'^ey, 
K{y'') ^^logslJ^lj}, 
where 
fiy*{yjc<kykl) := Sy^y* VA^. 
There are A^= |3^| such y*. The only way a /i-independent policy p can find 
the correct y*, is by trying one y after the other in a certain order. In the first 
A^ —1 cycles, at most A^—1 different y are tested. As there are N diff"erent 

5.3 Value Bounds and Separability Concepts 
151 
possible y*, there is always a /iG A4 for which p gives erroneous outputs in the 
first N-1 
cycles. The number of errors is EP^>N-l^\y\^2^^y*^ 
^2^^^^ 
for this fjL. As this is true for any p, it is also true for the AIXI model, hence 
E^ < 2^^^^ is the best possible error bound we can expect that depends 
on K{fji) only. Actually, we will derive such a bound in Section 6.2 for SP. 
Unfortunately, as we are mainly interested in the cycle region 
k<^\y\=2^^^^ 
(see Section 4.3.2) this bound is vacuous. There are no interesting bounds for 
deterministic /x depending on K{ii) only, unlike the SP case. Bounds must 
either depend on additional properties of /x or we have to consider specialized 
bounds for restricted problem classes. The case of probabilistic fi is similar. 
Whereas for SP there are useful bounds in terms of Eff^ and K{fi)^ there are 
no such bounds for AIXI. Again, this is not a drawback of AIXI since for no 
unbiased AI system could the errors/rewards be bound in terms of K(/x) and 
the errors/rewards of AI/x only. 
There is a way to make use of gross (e.g. 2^^^^) bounds. Assume that 
after a reasonable number of cycles /c, the information x^k perceived by the 
AIXI agent contains a lot of information about the true environment fi. The 
information in x</c might be coded in any form. Let us assume that the 
complexity i^(/i|x<fc) of /i, under the condition that x<fc is known, is of order 
1. Consider a theorem, bounding the sum of rewards or of other quantities 
over cycles I...00 in terms of f{K{ii)) 
for a function / with /(0(1)) ==0(1), 
like /(n) = 2^. Then, there will be a bound for cycles k...oo in terms of '^ 
/(X(/i|x<fc)) = 0(1). Hence, a bound like 2^^^^ can be replaced by small 
bound ?^2^^^l^<'=^ =0(1) after k cycles. All one has to show/ensure/assume 
is that enough information about 11 is presented (in any form) in the first k 
cycles. In this way, even a gross bound could become useful. In Section 6.5 
we use a similar argument to prove that AIXI is able to learn supervised (cf. 
Problems 3.13 and 6.3). 
5.3.4 
Asymptotic Learnability 
In the following, we weaken (5.15) in the hope of getting a bound applicable 
to wider problem classes than the passive one. Consider the I/O sequence 
yi^i-'-Vn^n caused by AIXI. On history ip:<^k-> AIXI will output yk=yk i^ cycle 
k. Let us compare this to y-^ which AI/x would output, still on the same history 
yb^k produced by AIXI. As Alfi maximizes the /i-expected value, AIXI causes 
lower (or at best equal) Vj^^^ if y^ differs from y^. Let D^^x^-^EfX^fc^il-
S.fj. .^] be the /^-expected number of suboptimal choices of AIXI, i.e. outputs 
different from AI/x in the first n cycles. One might weigh the deviating cases 
by their severity. In particular, when the /i-expected rewards V^^ 
for y^ and 
y^ are equal or close to each other, this should be taken into account in a 
definition of Z^n^^, e.g. by a weight factor [V^I^{yi:<:k)~V^^{yjc<:k)]. These 
details do not matter in the following qualitative discussion. The important 
diflerence to (5.15) is that here we stick to the history produced by AIXI 

152 
5 The Universal Algorithmic Agent AIXI 
and count a wrong decision as, at most, one error. The wrong decision in the 
HeavenHell example in the first cycle no longer counts as losing m rewards, but 
counts as one wrong decision. In a sense, this is fairer. One shouldn't blame 
somebody too much who makes a single wrong decision for which he just has 
too little information available to make a correct decision. The AIXI model 
deserves to be called asymptotically optimal if the probability of making a 
wrong decision tends to zero, i.e. if 
Dni^c/n -^ 0 for 
n -^ oc, 
i.e. 
Dn^^ = o{n). 
(5.16) 
We say that /x can be asymptotically learned (by AIXI) if (5.16) is satisfied. 
We claim that AIXI (for m^ —> oo) can asymptotically learn every problem 
fi of relevance, i.e. AIXI is asymptotically optimal. We included the quali-
fier of relevance^ since there may be strange /x spoiling (5.16), but we expect 
those // to be irrelevant from the perspective of AI. In the field of machine 
learning, there are many asymptotic learnability theorems, which are often 
not too difficult to prove. So a proof of (5.16) might also be feasible. Unfortu-
nately, asymptotic learnability theorems are often too weak to be useful from 
a practical point of view. Nevertheless, they point in the right direction. 
5.3.5 
Uniform ^ 
From the convergence (5.11) of ^—^/i we might expect V^^^ ~^^km 
^^^ ^^^ P^ 
and hence we might also expect y^ defined in (5.3) to converge to ^j^ defined in 
(4.17) for k-^oo. The first problem is that if the Vkmk ^^^ ^^e different choices 
of yk are nearly equal, then even if V^^ ^^km 
' Vk^Vk ^^ possible due to the 
noncontinuity of argmax^^^. This can be cured by a weighted Dny^c ^s described 
above. More serious is the second problem we explain for hk — \ and X = 1Z = 
{0,1}. For yl^d.YgmdCKy^i{if^kykl) 
to converge to 7/j^^argmax^^/x(2)r<fc2/fcl), 
it is not sufficient to know that £,{if<kijL^k) ~^/^{if<ki£^k)^ ^^ proven in (5.11). 
We need convergence not only for the true output yk and arbitrary reward r^, 
but also for alternative outputs yk. yl converges to y'^ if ^ converges uniformly 
to //, i.e. if in addition to (5.11) 
\fi{w:<kykXk)-C{w^<kykXk)\ < c-\^i{yE<kypc.k)-£,{wc<kmk)\ ^vWk (^•i'^) 
holds for some constant c (at least in a /x-expected sense). We call /i satisfying 
(5.17) uniform. For uniform // one can show (5.16) with appropriately weighted 
Dn^e, and bounded horizon hk < hmax • Unfortunately there are relevant /i that 
are not uniform. 
5.3.6 
Other Concepts 
In the following, we briefly mention some further concepts. A Marko-
vian fji is defined as depending only on the last cycle, i.e. fi{yjc<^kWk) — 

5.4 Value-Related Optimality Results 
153 
fjLk{xk-iW^k)- We say /i is generalized (l^^-order) Markovian^ if fi{yjc<:^kWk)— 
IJik{xk-iWk-i+i:k-iWk) 
f<^^ fixed /, This property has some similarities to fac-
torizable fi defined in (4.26). If further /Xfc=/iiVfc, /x is called stationary. Ergodic 
Markov decision processes are defined in Section 5.6. Further, we call fi (^) 
forgetful if ii{yjc<^kWk) {^iw^<kW^k)) becomes independent of yr<i for fixed I 
and k—^oo with /i-probability 1 (cf. Problems 2.5 and 5.13). Further, we say 
/i is farsighted if lixn.mk^ooyk'^ 
exists. More details are given in Section 5.7, 
where we also give an example of a farsighted // for which nevertheless the 
limit ruk —> oo makes no sense. 
5.3.7 
Summary 
We have introduced several concepts that might be useful for proving value 
bounds, including forgetful, relevant, asymptotically learnable, farsighted, uni-
form, (generalized) Markovian, factorizable and (pseudo-)passive /i. We have 
sorted them here, approximately in the order of decreasing generality. We will 
call them separability concepts. The more general (like relevant, asymptotically 
learnable and farsighted) /i will be called weakly separable, the more restric-
tive (like (pseudo-)passive and factorizable) /i will be called strongly separable, 
but we will use these qualifiers in a more qualitative, rather than rigid sense. 
Other (nonseparability) concepts are deterministic /x and, of course, the class 
of all chronological ji. 
5.4 Value-Related Optimality Results 
5.4.1 The Alp Models: Preliminaries 
In Chapter 4 we gave verbal definitions of the Aljj, model (Definition 4.4) 
and of value functions (Definition 4.5) and derived different mathematical 
expressions for them. The AIXI model involved similar expressions with /i 
replaced by M = ^u = ^. The following definitions summarize all formulas 
for general environment p, and general environmental classes M. and general 
weights Wjy in iterative form. 

154 
5 The Universal Algorithmic Agent AIXI 
The proof is the same as in Chapter 4 with fi replaced by p. The following 
property of Vp is crucial. 
Proof. Linearity is obvious from the Definition 5.18 of F^. Convexity follows 
from V^~V^ 
~^v^i^^u 
—^u^y^u^ 
where the first equality is just Defini-
tion 5.19, the second equality uses linearity of VJ just proven, and the last 
inequality follows from the dominance (5.20) and positivity of the weights Wy. 
D 
One loose interpretation of the convexity is that a mixture can never in-
crease performance. 
5.4.2 Pareto Optimality of AI^ 
This subsection shows Pareto optimality of AI^ analogous to SP. The total 
//-expected reward V^ of policy p^ of the AI^ model is of central interest in 
judging the performance of AI^. We know that there are policies (e.g. p^ of 
Al/i) with higher /x-value {V^ ^V^ 
)• In general, every policy based on an 
estimate p of /x that is closer to p than is ^ outperforms p^ in environment 
/x, simply because it is more tailored toward /i. On the other hand, such a 

5.4 Value-Related Optimality Results 
155 
system probably performs worse than p^ in other environments. Since we do 
not know fj, in advance, we may ask whether there exists a policy p with better 
or equal performance than p^ in all environments ueM 
and a strictly better 
performance for one u^M.. This would clearly render p^ suboptimal. We show 
that there is no such p. 
Proof. We want to arrive at a contradiction by assuming that p^ is not Pareto 
optimal, i.e. by assuming the existence of a policy p with V^ > V^ for allueM. 
and strict inequality for at least one u: 
VI = ^to.VP > Y^w^vf = Vf ^ Vi > VI 
V 
V 
The two equalities follow from linearity of Vp (5.21). The strict inequality fol-
lows from the assumption and Wy>^. The last inequality follows from the fact 
that p^ maximizes by definition the universal value (5.20). The contradiction 
V^ > V^ proves Pareto optimahty of AI^. 
D 
Pareto optimality should be regarded as a necessary condition for an agent 
aiming to be optimal. Prom a practical point of view, a significant increase of V 
for many environments i^ may be desirable, even if this causes a small decrease 
of V for a few other u. The impossibility of such a "balanced" improvement 
is a more demanding condition on p^ than pure Pareto optimality. The next 
theorem shows that p^ is also balanced Pareto optimal in the following sense: 
This means that a weighted value increase A-^ by using p instead of p^ is com-
pensated by an at least as large weighted decrease Ac on other environments. 
If the decrease is small, the increase can also only be small. In the special 

156 
5 The Universal Algorithmic Agent AIXI 
case of only a single environment with decreased value Ax^ the increase is 
bound by Arj < ^ | Z \ A | , i.e. a decrease by an amount Ax can only cause an 
increase by at most the same amount times a factor ^^. For the choice of the 
hts K ; ^ - 2 - ^ ( ^ ) , a decrease can only cause a smaller increase in simpler 
environments, but a scaled increase in more complex environments. Finally 
note that pure Pareto optimality in the sense of Theorem 5.23 follows from 
balanced Pareto optimality in the special case of no decrease Ac = 0. 
Proof. A>0 follows from zi = E^^^i^f "^S] = yf -"^/>0, where we have 
used linearity of Vp (Theorem 5.21) and dominance VF >VF (Theorem 5.20). 
The remainder of Theorem 5.24 is obvious from 0 < Zi = Ac — Au and by 
bounding the weighted average Aj^ by its maximum. 
• 
5.4.3 
Self-Optimizing Policy p^ w.r.t. Average Value 
We have argued in Section 5.3, Eq. (5.15) that there is no (universal) policy 
p (independent of the actual environment /i) for which 
iVZ^^VZ 
for 
m ^ o o , 
(5.25) 
and hence also p^ cannot converge to the optimal policy p^ in this sense. 
On the other hand, we know from Section 2.4 that convergence of this type 
(and even stronger) holds for SP. Section 5.3 suggested investigating restricted 
environmental classes. In the following we consider the generalized AI^ model 
(with ^() = XliyGAi^^^()) ^^^ restricted classes M. The least we must demand 
from M. to have a chance that 
^Vt-^^VZ 
for 
m-.^ 
(5.26) 
is that there exists some policy p at all with this property (5.25). Luckily, 
this necessary condition will also be sufficient. This is another (asymptotic) 
optimality property of (generalized) AI^. //universal convergence in the sense 
of (5.25) is possible at all in a class of environments M^ then AI^ converges 
in the same sense (5.26). We will call policies p with a property like (5.26) 
self-optimizing [KV86]. 
The following two lemmas pave the way for proving the convergence the-
orem: 
Proof. The following sequence of inequalities proves the lemma: 

5.4 Value-Related Optimality Results 
157 
In the first and second inequality we used w^>0 and V;-V^ 
> 0. The last 
inequality follows from Ylu^i^^f 
= ^ t = ^i - ^ / = Y.v^^vVt 
° 
We also need some results for averages of functions bi,{m) converging to 
zero. 
Proof. {%) immediately follows from <5(m) = ^^ifiyJiy(m)<^^tt;i^/(m)</(m). 
{i%) We choose some order on M, and some v^^M. large enough such that 
Yl,v>v^^^ — f • Using 5y{m)<c this implies 
Furthermore, the assumption 5jy{m)-^0 means that there is an rUys depending 
on u and e such that 6jy (m) < e for all m > m^e • This implies 
y^ Wy6y{m) < y^ WyS < £ foi sll 
TTi > max{m^£} = : nrie. 
m^ <cx), since the maximum is over a finite set. Together we have 
b{m) = 2_Jifj,5i^(m) < 2£ for 
m > rrie 
=^ 
S{m)-^Q 
for 
m-^oo 
since e was arbitrary and 5{m) >0. 
iiii) S(m) = y^Wi,5j^(m) < y^Wi,m.aix5j^{m) < max(5t,(m). 
V 
V 
(iv) Prom 6y{m)<Cyf{m) 
it follows that 
5{m) < Y^WyCyfim) 
< (maxc^)/(m)(^'w;^) < Cmaxf{m) 
V 
V 
with Cmax :=maXiyexCj^ being finite, since M. is finite by assumption. 
• 
Without the boundedness assumption in (u) on 6y{m), S{m) may not only 
not converge, but may not even exist. The stronger finiteness assumption on 
Ad is necessary to obtain the speed of convergence in (iv) (see Problem 5.6). 

158 
5 The Universal Algorithmic Agent AIXI 
The beauty of this theorem is that if universal convergence in the sense of 
(5.25) is possible at all in a class of environments A^, then AI^ converges 
(in the same sense (5.26)). The necessary condition of convergence is also 
sufficient. The unattractive point is that this is not an asymptotic convergence 
statement for V^J^ of a single policy p^ for /c-^oo for some fixed m, and in fact 
no such theorem could be true, since always k<m. The theorem merely says 
that under the stated conditions the average value of Al^(m) can be arbitrarily 
close to optimum for sufficiently large (pre-chosen) horizon m. This weakness 
will be resolved in the next section. 
Proof, (i) For Zl(j^) as defined in Lemma 5.27, assuming Ajy{m)<f{m) 
implies 
Z\(m) < /(m) by Lemma 5.28(i). Inserting this into Lemma 5.27 we get (i) 
(recovering the m dependence and finally renaming 
f--^A). 
{a) We define 5y{m) :—^Ay{m) — ^[V^* — ^^f]• Since we generally assumed 
bounded rewards ^<r<rrnax 
(Assumption 4.28) we have 
v:< 
'^'^max Q^nd V^J^ > 0 
^ 
Aj^'^mrraax 
^ 
0 ^ <^i^(^) ^ cI—r^ax-
The premise in (ii) is that <^zy(^) = :;^[^i*rJ^ —Vj^]-^0, which implies 
The inequalities follow from Lemma 5.27 and convergence to zero from 
Lemma 5.28(ii). This proves Theorem 5.29(iz). 
• 
By using Lemma 5.28(21;) instead of Lemma 5.28(2) we can similarly prove 
that V^i*^-Vf^''-0(Z\(m)) Vi/ implies Vi*^-Vfi^^ = 0(Z\(m)) in case of finite 
M.. In Section 5.6 we show that a converging p exists for ergodic MDPs, and 
hence p^ converges in this environmental class too (in the sense of Theorem 
5.29). 

5.5 Discounted Future Value Function 
159 
5.5 Discounted Future Value Function 
We now shift our focus from the total value Vim and m —>cx) to the future 
value (value-to-go) Vk? and fc-^oc. The reasons are at least twofold. 
First, we want to compare the future value of the optimal informed policy 
p^ to the universal learner p^. We regard the first k cycles as a grace period 
in which p^ learns and after which it performs well. The HeavenHell exam-
ple of Section 5.3 shows that one cannot avoid that a learner gets trapped. 
We do not want to exclude trapping environments in our analysis from the 
very beginning, since there could be interesting structure and behavior in the 
traps themselves (even hell or heaven may reward intelligent behavior). One 
possibility is to compare future values Vj^? ^ with V^^ on the same (fictitious) 
history yx^k- This addresses questions like: If p^ gets trapped in a (structured) 
hell, does it perform as well as p^ when put in hell? 
Second, we want to get rid of the horizon parameter m. In the last section 
we showed a convergence theorem for m-^cx), but a specific policy p^ is defined 
for all times relative to a fixed horizon m. Current time k is moving, but m is 
fixed. Actually, to use k^oo 
arguments we have to get rid of m, since k<m. 
This is the reason for the question mark in 14? above. The dynamic horizon 
ruk introduced earlier was convenient to discuss qualitative properties, but 
does not lead to a consistent model. 
We eliminate the horizon by discounting the rewards Vk^^^k^k with 7^ >0 
and Yl^ili 
< ^^ ^^^ letting m -^ oo. The analogue of m is now an effec-
tive horizon h^^^, which may be defined by ^i^k '^ 7^ ~ Yl^k+h^^^ 7^ ^^^^ 
Section 5.7 for a detailed discussion of the horizon problem). Furthermore, 
we renormalize V^oo by Yl^k^i 
^^^ denote it by Vk-y- It can be interpreted 
as a future expected weighted-average reward. Furthermore, we extend the 
definition to probabilistic policies TT (see Problem 4.2). 

160 
5 The Universal Algorithmic Agent AIXI 
Remarks. 
• Ti{yjc^kWk:m) IS actually independent oi Xm-
• Normalization of 14^ by /^ does not affect the policy p^. 
• The definition of p^ is independent of k (in the sense of Problem 5.7). 
• Without normalization by Fk the future values would converge to zero for 
/c —> oc in every environment for every policy. 
• For an MDP environment, a stationary policy, and geometric discounting, 
the future value is independent of k and reduces to the well-known MDP 
value function. 
• There is always a deterministic optimizing policy p^ (which we use). 
• For a deterministic policy there is exactly one yk-.m for each Xk-.m with 
TT^O. The sum over yk-.m drops in this case. 
• An iterative representation as in Theorem 5.20 is possible. 
• Setting 7/c = 1 for k<m and 7/c = 0 for k>m gives back the undiscounted 
Alp model (5.19) with V[^' = 
^VZ' 
• Vkj and w^ (see below) depend on the realized history yx<ck-
Similarly to the previous section, one can prove the following properties: 
The conditional representation of ^ can easily be proven by dividing the defi-
nition of ^(2^jL:m) (5-21) by ^{ypc_^j^) and by using the chain rule. The posterior 
weight w^ may be interpreted as the posterior belief in ly and is related to 
learning aspects of policy p^ (see Section 3.2.3). 

5.5 Discounted Future Value Function 
161 
The proof of Theorem 5.32 and Lemma 5.33 follows the same steps as for 
Theorem 5.23 and Lemma 5.27 with appropriate replacements. The proof of 
the analogue of the convergence Theorem 5.29 involves one additional step. 
The conclusion is valid for action histories y<fe if the condition is satisfied 
for this action history. Since we need the conclusion for the p^-action history, 
which is hard to characterize, we usually need to prove the condition for all 
action histories. Theorem 5.34 is a powerful result: An inconsistent sequence of 
probabilistic policies TT^ suffices to prove the existence of a consistent determin-
istic policy p^. A result similar to Theorem 5.29(z) also holds for the discounted 
case, roug hly saying that V^~V''=0{A{k)) 
implies y^^-y*-^0(Z\(A:)) with 
/^-probability 1—e for finite A4. 
Proof. We define 5j,{k) := A^ = V^^ — V^J^. Since we generally assumed 
bounded rewards 0 < r < Vmax (4.28) and VJ.t/;" is a weighted average of re-
wards we have 
V;;;<rmax 
and 
Vf^" > 0 
^ 
0 < 6,{k) = A^^ < c := Vmax 
The premise in Theorem 5.34 is that Sj,{k) = V^:!^-V^^^0, 
for /c->oo which 
implies 
' 
k 
k 
The inequalities follow from Lemma 5.33, and S{k) converges to zero (w./x.p.l) 
by Lemma 5.28(ii). What is new and what remains to be shown is that w^ 
is bounded from below. We show that Zk-i •= ~F = ^^^^ — ^ converges to 
a finite value, which completes the proof. Let E denote the /x expectation. 
Then 
E[..M=EM(^<.^.)^;(^^=—^^^^^—^Hi^r^'-^-
^^^ runs over all Xk with ;Li(^i.^)/0. The first equality holds w./i.p.l. In the 
second equality we used the chain rule twice. E[2^fc|x<jt] <2;fc_i shows that —Zk 

162 
5 The Universal Algorithmic Agent AIXI 
is a semi-martingale. Since —Zk is non-positive, [Doo53, Thm.4.l5(z),p324] 
implies that —Zk converges for A:-^cx) to a finite value w./i.p.l, which completes 
the proof. (If fi and ^ are lower semicomputable, then boundedness of Zk-i 
follows without the use of martingales from Zk-i= 
(~^^\ < t ~^\ <c<oo, 
where the first inequality follows from the universality of £^u and the second 
inequality holds for all /x.M.L.-random sequences.) 
• 
We want to give an intuitive reason for the necessity of the probability 
qualifier. Assume that the true environment is /i, but choose a history x^k 
sampled from i^^/i. This implies that ^ converges to z/ for A;—>oo. This means 
that at a fixed but large time /c, ^ is very close to v. It is very hard (takes 
large h^l ) to get rid of this wrong bias and to become close to ^ later. In the 
limit this is not possible at all. 
The following continuity properties for the discounted values hold: 
Care has to be taken in the interpretation and use of this theorem: It cannot be 
used to conclude V^^^—>V^*^, since ^—>/i does not hold for dllyjci-oo^ but only 
for /i-random ones (more precisely w./i.p.l). The condition in Theorem 5.35 
cannot be weakened, since p^ is not self-optimizing if M does not admit 
self-optimizing policies. Furthermore, continuity is not uniform in k, which 
prevents us from using this theorem in the proof of Theorem 5.38. Finally, 
note that V^ ^ can be discontinuous in jl at jl^ [JL- On the positive side, 
continuity holds for any /i and 7; no structural assumptions have to be made. 
By setting 7/c = 1 for k<m and 7/c = 0 for A;>m we also get continuity of F^^, 
^fc*m' ^^^ ^kn!' ^^^^ ^(^) <'^maa:^(^-A: + l)^ (sct n = 771 + 1). For geomctric 
discount 7/c=7^ the theorem holds with 5{E) = ^f_^T,^ (which follows from the 
second last bound on Ak in the proof below for n — oo). 
Proof, (z) Vk^ can be represented recursively like in the undiscounted case 
as 
^kV^^iwcKk) 
== X^7r(2^<fcy^)p(^<fc2^;,)[7A:rA: +A+iV;'[^i,^(?/ri:fc)], 

5.5 Discounted Future Value Function 
163 
which can easily be verified by induction. The absolute difference of two values 
can be written as 
Ak := A|y,7-v;;^| = |^7r.M-(7r+ry)-5;;7r-A-(7r-+rv) 
yx 
y 
X 
X 
yx 
<... 
where we have suppressed all indices and arguments of all variables and 
functions. We upper-bound the last expression by pulling in the absolute 
bars. Using (in this order) 0 < r < Vmax^ ^x\l^~~i~^\ — ^ (^^ assumption), 
0<V+V<2rmax. 
|F-t>|<maXy^|F-F|,5:^(/x-fA) = 2, X:^7r = l, weget 
... < 
Sjrmax 
+ rSTmax 
+ T maX 
\V-V\ 
yx 
= 
£:rfcr^ax +maxZifc+i < ... 
n - l 
S 
£^max 
/ , ^ i '^ 
m a x Zlji 
< 
£1^max['^'~ f^jJ- k ~^ -^ n'^max' 
^-^ 
yXk:n-l 
y^k:n-l 
l = k 
In the second line we used 7fc+i~/c4.i = Fk and the definition of Ak- In the 
third line we recursively inserted the bound for zi^, i = k-{-l,...,n — l we just 
derived. In the final expression we used Fi <Fk ioi i>k and |V —V"! <rmax-
This bound on Ak is valid for all n > fc, so we may take the minimum over 
n>k. This leads to Ak<FkS{e) where 5{s) was defined in Theorem 5.35. This 
proves (i). 
(a) For any two real-valued functions / , / over some domain V with \f{x) — 
f{x)\<6\/xeV 
we also have \fmax-fmax\<S, 
where /max.=max^ei>/(^) and 
fmax' = ^^^xevf{x), 
siuCC f{x)<f{x)-{-d<fmax+S\/x. 
HcUCe 
fmax<fmax-^S, 
and similarly fmax<fmax+S. 
For x^ir, P = {7r}, /(x) = F^7, f{x) = V^f we 
get (a) from (i). 
(iii) Noting that < ^ < ' ^ 
we get |14;^-F,-;-| = | F , ; ^ - I W ^ + F ^ ; A _ 
< 1 < IK-T-K-f l + l < ^ - < n <25(e) by iii) and {i), which proves (iii). 
To prove 6{e) —> 0 we replace min^ by n ~ £~^/^ and get 0 < 6{e) < 
Tmax'i{n—k)e-{-j?^)^-^0, 
since n^oo, 
Tn^-^^O, and ne-^0. 
• 
The next theorem shows that, for a given policy p and history generated 
by p and //, i.e. on-policy, the future universal value V.^. converges to the true 
value V.^.^. 

164 
5 The Universal Algorithmic Agent AIXI 
Proof. (z),„p follows from 
|FP« _l/w^| = | ^ 
(r, + ...+r„)[C()-^()]| < 
^(r;,+...+r^)|^()-/i()| < {m-k-\-l)rmax ^ ^ 1^0-/^OI == {rn-k+l)Tmaxak:r. 
where p{) = p{wc<kmk:m)\yi..m=p{x<m)- (^bottom is shown similarly: Let Vkm'y 
be the discounted future value Vk^ but cut after cycle m. We have 
Kt,-vZ,\ 
= -^lE(^fe^fc + - + ^™'^'")KO-M()]| < -
^ 
XV. 
•^ 
k:m 
-1 '^max^k:m 
S '^maxy 
^^\ m-
In the last step we used ak-.m ^ V^dk-.m (see Lemma 3.11 or Section 3.7.1). 
{i)hottom follows by taking the limit m—>oc, which exists since Vkm-f and dk-.m 
are monotone increasing in m and bounded. {ii)top follows from {i)top^ and 
mk — k-\-l<hmax, 
and bound (3.71) with t-^k^ nt^^^rrik, h-^hmaxi and y as 
additional conditions. (ii)hottom follows from {i)bottom and 'E[dk:n]—Dn—Dk-i 
(see Section 3.7.1). (iii) follows from {it) by Definition 3.8 of convergence 
i.m.(s.) 
• 

5.6 Markov Decision Processes (MDP) 
165 
Convergence of the average values 7^ V^^^ -^ /T^fc'mfc ^^^^ holds, i.m.s. for 
bounded horizon, and i.m. for arbitrary horizon. Note also that if the history 
is generated by p=p^, then {Hi) implies V^^-^V^^^. Hence the universal value 
VjiT can be used to estimate the true value VZ,^, without any assumptions 
on M. and 7. Nevertheless, maximization of V^^ may asymptotically differ 
from maximization of V^^, since V^y y^^k^ 
^^^ p^^p^ is possible (and also 
K7 T^'^/cT' ^^^ Section 5.3 and Problem 5.2). 
5.6 Markov Decision Processes (MDP) 
From all possible environments, Markov (Decision) Processes are probably the 
most intensively studied ones. To give an example, we apply Theorems 5.29 
and 5.34 to ergodic Markov decision processes (MDPS), but we will be quite 
brief. 
Stationary MDPs /i have stationary optimal policies p^ in case of geometric 
discount, mapping the same state/observation Ok always to the same action 
7/fc. On the other hand, a mixture ^ of MDPs is itself not an MDP, i.e. ^^ A^MDP, 
which implies that p^ is, in general, not a stationary pohcy. The definition of 
ergodicity given here is least demanding, since it only demands the existence 
of a single policy under which the Markov process is ergodic. Often, stronger 
assumptions, e.g. that every policy is ergodic or that a stationary distribution 
exists, are made. We now show that there are self-optimizing policies for the 
class of ergodic MDPs in the following sense. 
Alternatively, one could assume X to be the state space, i.e. include the rewards 
in the states. The transition matrix then has the form iJ'{xk-\ykX_^) with action 
Uk^y 
leading to state Xfc G Af from state Xk-i G X [Hut02b]. The advantage is 
that the reward rk=r{xk) 
is now a known deterministic function only depending 
on the current state, but for the prize of a (possibly much) larger state space. 

166 
5 The Universal Algorithmic Agent AIXI 
There is much literature on constructing and analyzing self-optimizing learn-
ing algorithms in MDP environments. The assumptions on the structure of the 
MDPs vary, but all include some form of ergodicity, often stronger than Defi-
nition 5.37, demanding that the Markov process is ergodic under every policy. 
See, for instance, [KV86, BT96]. Note also that in (ii) the history need not 
be generated by TTk and/or iy. Indeed, the application we are most interested 
in here, is to action histories generated by p^. We will only briefly outline one 
algorithm satisfying Theorem 5.38 without trying to optimize performance. 
Proof. Let T^^, = iy{sas') be the probability of reaching state s' G 5 := 0 
from state s under action aeA:—y, 
and let R^^, '•—Ylren'^'^^^^^''^ ^^ ^^^ 
expected reward of transition s-^ s'. With this notation the value function 
can be written as VZ = T.s..JR1U+•••+RZ-^sJ•TZ^•••••TtZ-.s^^ 
where 
ak =p{s<ck) is the action of policy p at time k {SQ^S is some special initial 
"state"). 
For (i) one can choose a policy Pm that performs (uniformly) random 
actions in cycles l...ko — l with 1 <C/CQ <C m and that follows thereafter the 
optimal policy based on an estimate of the transition matrix T^^, from the 
initial /CQ — 1 cycles. The existence of an ergodic policy implies that for every 
pair of states sstart ,SGS there is a sequence of actions and transitions of length 
at most |tS| —1 such that state s is reached from state sstart- The probability 
that the "right" transition occurs is at least T^m with Tmin being the smallest 
nonzero transition probability in T. The probability that a random action is 
the "right" action is at least |v4|~^. So the probability of reaching a state s 
in |<S| —1 cycles via a random policy is at least (T^m/I^l)'*^'"^- In state s 
action a is taken with probability \A\~^ and leads to state s' with probability 
T^s' ^"^min- Hence, the expected number of transitions s—^s' to occur in the 
first ko cycles is > |^(T^m/|-^|)''^' ^^^o- Hence, for T^^, 7^0, the accuracy of the 
frequency estimate f^^, of T^^, and ^^^, of i?^^, is - A:J"^/^ while for T^^^/ =0 
the estimate T^^, = 0 is exact and R^^, is not needed. In summary: similar 
MDPs lead to "similar" optimal policies, which lead to similar values. More 
precisely, one can show (see Problem 5.12) that T—T^/CQ ^ and^—i?~A;o 

5.6 Markov Decision Processes (MDP) 
167 
implies the same accuracy in the average value, i.e. | ^ Vk^ ~ ^ ^k^m H ^o 
' 
where pm is the optimal policy based on T and * is the optimal policy based on 
T(==z/). Since ^^ifco~"^' (^) follows (with probability 1) by setting fco^m^/^. 
The policy prn can be derandomized, showing {%) for sure. 
The discounted case (ii) can be proven similarly. The history ^<fc is simply 
ignored and the analogue to m-^oo is h^^^ —>cx) for fc—^oo, which is ensured 
by ^^^^^^ ^ 1 . Let ixk be the policy that performs (uniformly) random actions 
in cycles /C./CQ —1 with k<^kQ<^h^'' and that follows thereafter the optimal 
policy"^ based on an estimate T of the transition matrix T from cycles fc...fco—1. 
The existence of an ergodic policy, again, ensures that the expected (after 
derandomization for sure) number of transitions 5—^5' occurring in cycles 
k,..ko — lis proportional to A:=ko—k. The accuracy of the frequency estimate 
T of T and ^ of i? is ~Z\~^/^, which implies by a strengthening of Theorem 
5.35(iii) for ergodic MDPs similar to Problem 5.12 that 
K'-y'^K^f 
for 
Zi = 
fco-fc->oo, 
(5.39) 
where TT^ is the optimal policy based on T, and * is the optimal policy based on 
T{=iy). It remains to be shown that the achieved reward in the random phase 
k...ko — l gives a negligible contribution to 14^. The following implications for 
k^^oo are easy to show: 
7^ 
-, fco - 1 
l=K 
Since convergence to zero is true for all fixed finite A it is also true for suffi-
ciently slowly increasing A{k)-^ 00. This shows that the contribution of the 
first A rewards r;c,...,r;co_i to V^^ is negligible. Together with (5.39) this shows 
K'^^^kj 
for 
ko-k+A{k). 
D 
The rate of convergence m~^^^ rather than m"-^/^ in the undiscounted 
case may be a bit surprising. If we would explore for ko — m steps and ask 
for the accuracy of the value function estimate afterwards, i.e. for k > m, 
we would get ^k^ 
==m~^/^, but since we are considering ^Vim 
including 
the history l...fco we must get a worse result, namely m~^^^. Actually, the 
result can be improved 0 ( ^ ^ ^ ) (Problem 5.12). Although in the discounted 
case, we consider the future value Vkj, the situation is nevertheless similar. 
Exploration takes place in cycles /C./CQ —1; history ^<fc is not exploited. 
The conditions Fk < 00 and ^^^^ ^-1 on the discount sequence are, for 
instance, satisfied for 7;j. = l/A:^, so the theorem is not vacuous. The popular 
geometric discount 7^ = 7^ fails the latter condition; it has finite effective 
horizon. Section 5.7 gives a detailed account of the discount and horizon issues. 
^ Note that for non-geometric discounts as here, optimal policies are, in general, 
not stationary. 

168 
5 The Universal Algorithmic Agent AIXI 
Together with Theorems 5.29 and 5.34, Theorem 5.38 immediately implies 
that AI(^ is self-optimizing for the class of ergodic MDPs. 
Continuous classes A4. There are uncountably many ergodic MDPs. Since 
we have restricted our development to countable classes M. we had to give 
the corollary for a countable subset of A^MDPI- We may choose Ad as the set 
of all ergodic MDPs with rational (or computable) transition probabilities. In 
this case A^ is a dense subset of A^MDPI that is, from a practical point of 
view, sufficiently rich. On the other hand, it is possible to extend the theory 
to continuously parameterized families of environments /ig and £^ = JdOwe/j^e. 
Under some mild (differentiability and existence) conditions, most results of 
this book remain valid in some form, especially Corollary 5.40 for all ergodic 
MDPS A^MDPl-
Bayesian self-optimizing policy. AI^MDPI with unbounded horizon is a 
purely Bayesian self-optimizing consistent policy for ergodic MDPs. The poli-
cies of all other known approaches are either hand-crafted, like those in the 
proof of Theorem 5.38, or are Bayesian with a pre-chosen horizon m or with 
geometric discounting 7 with finite effective horizon [KV86, BT96]. The com-
bined conditions Fk < 00 and ^^^^^i _^ i allowed a consistent self-optimizing 
Bayesian policy based on mixtures. 
Bandits. For instance, consider the popular class of bandits B. In a two-
armed bandit problem you pull repeatedly one out of two levers, resulting in 
a gain of 1$ with probability pi for arm number i. The game can be described 
as an MDP with parameters Pi. If the pi are unknown. Corollary 5.40 shows 
that Al^B yields asymptotically optimal payoff for discounted unbounded-
horizon bandits. 
Other environmental classes. Bandits, i.i.d. processes, classification tasks, 
and many more are all special (degenerate) cases of ergodic MDPs, for which 
Corollary 5.40 shows that p^ is self-optimizing. But the existence of self-
optimizing policies is not limited to (subclasses of ergodic) MDPs. Certain 
classes of POMDPS, A:*^-order ergodic MDPs, factorizable environments, re-
peated games, and prediction problems are not MDPs, but nevertheless admit 
self-optimizing policies (to be shown elsewhere), and hence the corresponding 
Bayes optimal mixture policy p^ is self-optimizing by Theorems 5.29 and 5.34. 

5.7 The Choice of the Horizon 
169 
Restricted policy classes. The development in this and the last paragraphs 
can be scaled down to restricted classes of policies V. If one defines F* = 
argmaXp^-pV^ all theorems remain valid, more or less unchanged. For instance, 
consider a finite class of quickly computable policies. For MDPs, ^ is quickly 
computable, and V? can be (efficiently) computed by Monte Carlo sampling. 
Maximizing over the finitely many policies pEV 
selects the asymptotically 
best policy p^ from V for all ergodic MDPs. 
Outlook. Future research could be the derivation of non-asymptotic bounds, 
possibly along the lines of Chapter 3. To get good bounds one may have to 
exploit extra properties of the environments, like the mixing rate of MDPs 
[KS98]. Finally, instead of convergence of the expected reward sum, conver-
gence with high probability of the actual reward sum would be interesting to 
study (cf. Problem 3,4). 
5.7 The Choice of the Horizon 
The only significant arbitrariness in the AI^ model lies in the choice of the 
horizon function hk = rrik — /a +1. We discuss some choices that seem to be 
natural and give preliminary conclusions at the end. We will not discuss ad 
hoc choices of hk for specific problems (like the discussion in Section 6.3 in 
the context of finite strategic games). We are interested in universal choices 
of mjt. 
Fixed horizon. If the lifetime of the agent is known to be m, which is in 
practice always large but finite, then the choice m^ = m maximizes correctly 
the expected future reward. Lifetime m is usually not known in advance, as 
in many cases the time we are willing to run an agent depends on the quality 
of its outputs. For this reason, it is often desirable that good outputs are not 
delayed too much, if this results in a marginal reward increase only. This can be 
incorporated by damping the future rewards. If, for instance, the probability 
of survival in a cycle is 7 < 1, an exponential damping (geometric discount) 
Tk '—f^k'^y^ ^^ appropriate, where r^ are bounded, e.g. r^ G [0,1]. Expression 
(5.3) converges for ruk^oo in this case {yk=SiYgmaxyJimrnk-^ooV^^^{yx<:kyk) 
exists). But this does not solve the problem, as we introduced a new arbitrary 
time scale (1—7)"^. Every damping introduces a time scale. Taking 7^-1 is 
prone to the same problems as m^—J^OO in the undiscounted case. 
General discounting. Geometric discounting does not solve the horizon 
problem, but the idea of discounting is fruitful. Let rk '-—Ikf^k with 7/e >0 and 
r^ G [0,1]. If A : - YZkli 
< ^^ then V^j^ := ^Jimm-^^VZ 
exists. Rewards 
rk~\-h give only a small contribution to V^^ for large /i, since 7^+^ —^0. The 
instantaneous effective horizon may be defined as the h for which 'y^+h ^^ 
only half (or more generally a fraction l3<l) of 7^. Formally, we may define 

170 
5 The Universal Algorithmic Agent AIXI 
h^ :=inm{h>0'.^yk-^h 
^07k}' 
For any summable convex discount sequence 7/^ 
we have h)^<C'k iov some constant c independent of A:. A better definition for 
the /^-effective horizon is the h for which the cumulative discount 
Fk^h^f^rk, 
or more formally, /if := min{/i > 0 : Fk+h < /3A;}- Approximating the infinite 
reward sum in 14^ by the first h^ terms introduces an error of at most (3rmax • 
We define the effective horizon by h^^^ :— h^~ ' . Table 5.41 shows eff'ective 
horizons for various types of discounts ^k • 
D y n a m i c horizon (universal &; harmonic discounting). The largest 
horizon with guaranteed finite and enumerable reward sum can be ob-
tained by the universal discount jk — 2~^^^^ 
(or the monotone variant 
7/;; = min^</c2~^^^^). This discount results in a truly farsighted agent with 
eff'ective horizon that grows faster than any computable function. It is some-
what similar to a near-harmonic discount 7^ = [Mog^A;]"^, since 2~^^^^ <1/A: 
for most k and 2""^^^^ ^c/{k 
lo^k) 
(see Theorem 2.10(zi)), but the latter leads 
to h^^^ r^k^. Similarly, the time scale invariant power damping 7/e = A:~^~^ in-
troduces a dynamic time scale. In cycle k the contribution of cycle 2^+^ -k 
is damped by a factor | . The instantaneous effective horizon hk in this case 
is ~ A:, the maximum possible. The choice hk — ct-k with a ~ 2 i + ^ qualita-
tively models the same behavior. We have not introduced an arbitrary time 
scale m, but limited the farsightedness to some multiple (or fraction) of the 
length of the current history. This avoids the preselection of a global time 
scale m or j ^ . This choice has some appeal, as it seems that humans of age 
k years usually do not plan their lives for more than, perhaps, the next k years 
{oLhuman'^^)- From a practical point of view this model might serve all needs, 
but from a theoretical point we feel uncomfortable with such a limitation in 
the horizon from the very beginning. Note that we have to choose a = 0{l) 
because otherwise we would again introduce a number a that has to be justi-
fied. We favor the universal discount ^k — '^~^^^\ 
since it allows us, if desired, 
to "mimic" all other more greedy behaviors based on other discounts 7/^ by 
choosing Vk G [0,c-7/e] C [0,2-^^^)]. 

5.7 The Choice of the Horizon 
171 
Infinite horizon. The naive hmit m^-^oo in (5.3) may turn out to be well 
defined and the previous discussion superfluous. In the following, we suggest a 
limit that is always well defined (for finite 3^). Let y^ 
be defined as in (5.3) 
with dependence on m^ made explicit. Further, let y^ 
'-—{Vk^ ''^k >'m} 
be the set of outputs in cycle A: for the choices m^—m,m+l,7Ti+2,.... Because 
yirn) 2y(rn+i) ^{^^^^ ^ave 3>(~' := n~=feO>I"^ # {}• We define the m^ = oo 
model to output any y)^^ G y^^ . This is the best output consistent with 
some arbitrary large choice of m^. Choosing the lexicographically smallest 
yl^^ ^yl 
would correspond to the lower Hmit lim^^^ijjJ^K 
which always 
exists (for finite y). Generally y)^^ G 3^^°° is unique, i.e. \yt^\ 
= l iff the 
naive limit lim^^oo^fc 
exists. Note that the limit lim^_^ooVj^*m(?^<fc) ii^ed 
not exist for this construction. 
Average reward and differential gain. Taking the raw average reward 
(rfc + ...H-ry„)/(m—A;+l) and m ^ o c also does not help: consider an arbitrary 
policy for the first k cycles and the/an optimal policy for the remaining cycles 
fc+l...oo. In e.g. i.i.d. environments the limit exists, but all these policies give 
the same average value, since changing a finite number of terms does not affect 
an infinite average. In MDP environments with a single recurrent class one can 
define the relative or differential gain [Ber95b]. In more general environments 
(we are interested in) the differential gain can be infinite, which is acceptable, 
since differential gains can still be totally ordered. The major problem is 
the existence of the differential gain, i.e. whether it converges for m -^ oo 
in ]RU{(X)} at all (and does not oscillate). This is just the old convergence 
problem in slightly different form. 
Immortal agents are lazy. The construction above leads to a mathemat-
ically elegant, no-parameter Al^ model. Unfortunately this is not the end of 
the story. The limit rrik —> oo can cause undesirable results in the Al/i model 
for special /i, which might also happen in the AI^ model whatever we de-
fine mfc—>oo. Consider an agent who for every \// consecutive days of work, 
can thereafter take / days of holiday. Formally, consider y = A! = 71= {0,1}. 
Output yk = 0 shall give reward rk = 0 and output yk = l shall give r^ — 1 
iff y^_i_^i-..yk-i 
=0...0 for some /, i.e. the agent can achieve I consecutive 
positive rewards if there was a preceding sequence of length at least \l with 
yk = 'i^k=0. If the lifetime of the Al/i agent is m, it outputs y^ = 0 in the first s 
cycles and then yk = ^ for the remaining s'^ cycles with s such that 5+s^ = m. 
This will lead to the highest possible total reward Vim — s^='n2+^ — \/T^+\-
Any fragmentation of the 0 and 1 sequences would reduce Vim, e.g. alter-
natingly working for 2 days and taking 4 days off would give Vim = |?^- For 
m —> cxD the Al/i agent can and will delay the point s of switching to ^^ = 1 
indefinitely and always output 0 leading to total reward 0, obviously the worst 
possible behavior for the agent. The AI^ agent will explore the above rule af-
ter a while of trying yk —0/1 and then applies the same behavior as the Al/i 

172 
5 The Universal Algorithmic Agent AIXI 
agent, since the simplest rules covering past data dominate <^. For finite m this 
is exactly what we want, but for infinite m the AI^ model (probably) fails, 
just as the AI// model does. The good point is that this is not a weakness of 
the AI^ model in particular, as Al/i fails too. The bad point is that rrik -^ CXD 
has far-reaching consequences, even when starting from an already very large 
mk=m. 
This is because the /i of this example is highly nonlocal in time, i.e. 
it may violate one of our weak separability conditions. 
Conclusions. We are not sure whether the choice of rrik is of marginal im-
portance, as long as rrik is chosen sufficiently large and of low complexity, 
rrik = 2^ 
for instance, or whether the choice of rrik will turn out to be a 
central topic for the AI^ model or for the planning aspect of any AI system 
in general. We suppose that the limit rrik —^ oc for the AI^ model results in 
correct behavior for weakly separable /i. A proof of this conjecture, if true, 
would probably give interesting insights. 
5.8 
Outlook 
Expert advice approach. We considered expected performance bounds for 
predictions based on Solomonoff's prior. The other, dual, and currently very 
popular approach, is 'prediction with expert advice' (PEA) invented by Little-
stone and Warmuth [LW89] and Vovk [Vov92]. Whereas PEA performs well in 
any environment, but only relative to a given set of experts, our A^ predictor 
competes with any other predictor, but only in expectation for environments 
with computable distribution (see Section 3.7.4). It seems philosophically less 
compromising to make assumptions on prediction strategies than on the en-
vironment, however weak. There are also a few results on expert-based active 
learning [ACBFS02, BEYL04]. One could investigate whether PEA can be 
generalized to the case of a general active agent, which would result in a 
model dual to AIXL We believe the answer to be negative, which on the 
positive side would show the necessity of Occam's razor assumption, and the 
distinguishedness of AIXI. 
Actions as random variables. The uniqueness for the choice of the gen-
eralized ^ (2.26) in the AIXI model could be explored. From the originally 
many alternatives, which could all be ruled out, there is one alternative that 
still seems possible. Instead of defining ^ as in (2.26) one could treat the 
agent's actions y also as universally distributed random variables and then 
conditionalize ^ on y by the chain rule (see Problem 5.1). 
Structure of AIXI. The algebraic properties and the structure of AIXI 
could be investigated in more depth (we already saw that the value VJ is a 
linear function in /JL and V* is a convex function in /i). This would extract the 
essentials from AIXI, which finally could lead to an axiomatic characteriza-
tion of AIXI. The benefit is as in any axiomatic approach. It would clearly 

5.10 Functions -^ Chronological Semimeasures 
173 
exhibit the assumptions, separate the essentials from technicahties, simplify 
understanding and, most important, guide in finding proofs. 
Posterization. Many properties of Kolmogorov complexity, Solomonoff's 
prior, and (policies based on) Bayes mixtures remain valid after "posteri-
zation". By posterization we mean replacing Vim, Wiy, K{iy)^ ^{iKi-.m)^ ^^^- ^Y 
the posteriors Vkm, "w^, K{v\yjc^k), y{yK<kWk:m)'> ^^c- Strangely enough, for 
Wy chosen as 2~^^^^ it is not true that w^^2~^^^\'^<^\ 
If this property were 
true, weak bounds as the one proven in Section 6.2 (which is too weak to be 
of practical importance) could be boosted to practical bounds of order one. 
Hence, it is of high impact to rescue the posterization property in some way. 
It may be valid when grouping together essentially equal distributions v (cf. 
Problems 3.13 and 6.3). 
5.9 
Conclusions 
All tasks that require intelligence to be solved can naturally be formulated 
as a maximization of some expected utility in the framework of agents. We 
gave an explicit expression (4.17) of such a decision-theoretic agent. The main 
remaining problem is the unknown prior probability distribution /x^^ of the en-
vironment (s). Conventional learning algorithms are unsuitable, because they 
can neither handle large (unstructured) state spaces nor do they converge in 
the theoretically minimal number of cycles nor can they handle non-stationary 
environments appropriately. On the other hand, the universal semimeasure ^ 
(2.26), based on ideas from algorithmic information theory, solves the problem 
of the unknown prior distribution for induction problems. No explicit learn-
ing procedure is necessary, as ^ automatically converges to ji. We unified the 
theory of universal sequence prediction with the decision-theoretic agent by 
replacing the unknown true prior /x^^ by an appropriately generalized univer-
sal semimeasure ^^^. We gave strong arguments that the resulting AI^ model 
is universally optimal. Furthermore, possible solutions to the horizon problem 
were discussed. In Chapter 6 we present a number of problem classes, and 
outline how the AI^ model can solve them. They include sequence prediction, 
strategic games, function minimization and, especially, how AI^ learns to learn 
supervised. In Chapter 7 we develop a modified time-bounded (computable) 
p^l^ti version. 
5.10 Converting Functions into Chronological 
Semimeasures 
To complete the proof of the universality (5.6) of M we need to convert enu-
merable functions tjj'.JB*-^ JR^ into enumerable chronological semimeasures 
p: {yxA!y 
-^ M^ with certain additional properties, where A' and y are 

174 
5 The Universal Algorithmic Agent AIXI 
countable (finite or infinite). The proof given here follows [LV97, p273], but 
is slightly more formal and compact. Every enumerable function like i/^ and p 
can be approximated from below by definition^ by primitive recursive func-
tions ^:B*xN 
-^Q^ 
and 0 : (^ x A')* x iV ^ (^+ with ^(5) = sup^(/?(5,t) 
and p{s) =sup^(/)(s,t) and recursion parameter t. For arguments of the form 
s^W^i-.n we recursively (in n) construct (p from ip as follows: 
.•(«.,<).= {"<«-'>;:; z>t 
^'fe')-.«..), 
(5.42) 
(f){e,t) := max \(p\e,i) 
: ip\e,i) < l | , 
(5.43) 
(t>(mi:n^t) := max {(p'(yri:n,i) : Ex^<t^'(^i:r.,^) < 0(2^<n.O}- (5-44) 
By Xn < t we mean that the natural number associated with string Xn is 
smaller than t. According to (5.42) with tp also p^ as well as ^ ^ (^' are 
primitive recursive functions. Further, if we allow t = 0 we have 99^(5,0) =0. 
This ensures that 0 is a total function. 
In the following we prove by induction over n that 0 is a primitive recursive 
chronological semimeasure monotone increasing in t. All necessary properties 
hold for n = 0 {yxi:o = e) according to (5.43). For general n assume that the in-
duction hypothesis is true for (t){ypc^^,t). We can see from (5.44) that (t){yoc_i.^,t) 
is monotone increasing in t. (p is total as p^{yxi:n^i = 0) =0 satisfies the in-
equality. By assumption (j){yjc^ri-,'t) is primitive recursive, hence with ^ ^ ip' 
also the order relation Yl^' ^ 0 is primitive recursive. This ensures that the 
nonempty finite set {p''-^p'<(t>}i 
and its maximum 0(7^^.^,^) are primitive 
recursive. Further, (j){'i^i.^^t) = p'{y]Ci:n-,i) for some i with i<t independent of 
Xn. Thus, Exn^(2^i:n'0 = Ex^^'^^i^^'^) - ^i^<n^^)^ ^hich is the condition 
for (j) being a chronological semimeasure. Inductively we have proved that (j) is 
indeed a primitive recursive chronological semimeasure monotone increasing 
in t. 
In the following we show that every (by definition total) enumerable 
chronological semimeasure p can be enumerated by some (j). By definition 
of enumerability there exist primitive recursive functions p with p{s) — 
sup^(^(s,^). The function p){s,t) := (l-^/t)-max^<t(^(5,2) also enumerates p 
but has the additional advantage of being strictly monotone increasing in t. 
p'{wci:n,oo) = p{y]Ci:n,oo) = p{y]Ci:n) by definition (5.42). (/)(e,t) = (/^'(e,t) by 
(5.43) and the fact that p'{e,i — l)<p'{e,i)<p){e^i)<p{e)<l, 
hence (/)(e,oo)== 
p(e). (f){w^^.^^,t)<p\yci.,n,t) by (5.44), hence 0(^i^^,oo) <p(?^i,^). We prove 
the opposite direction (j){y^i.^,oo)>p{yxi:n) by induction over n. We have 
Defining enumerability as the supremum of total primitive recursive functions is 
more suitable for our purpose than the equivalent definition as a limit of monotone 
increasing partial recursive functions. In terms of Turing machines, the recursion 
parameter is the time after which a computation is terminated. 

5.11 Proof of the Entropy Inequality 
175 
X-n 
Xn 
Xfi 
Xn 
(5.45) 
The strict monotony of ip and the semimeasure property of p have been used. 
By induction hypothesis \init-^oo<f>{w<n'>^)^p{'l&-<n) ^^^ (5.45) for sufficiently 
large t we have </>(2^<n?0>Z^a;n^'(^i:^'^)' ^^^ condition in (5.44) is, hence, 
satisfied and therefore (f){ypo_i.^,t)>(p\yxi:n,i) for sufficiently large t, especially 
^(?^i:n5^^)^^'(y^i:n50 foi* ^,112. Taking the limit z-^oo we get </>(2^i.^,oo) > 
ip'{yri:n,oo)^p{y^^.J. 
Combining all results, we have shown that the constructed 0(-,t) are prim-
itive recursive chronological semimeasures which are monotone increasing in 
t, and which converge to the enumerable chronological semimeasure p. This fi-
nally proves the enumerability of the set of enumerable chronological semimea-
sures. 
5.11 Proof of the Entropy Inequality 
We show^ that 
„ 
„ 
il-ar + J2{y,-x,f 
<J2yi^^-
i=l 
i = l 
with yi > 0, Xi> 0, ^ y^ = 1, ^ x^ = a < 1 
i=l 
i=l 
and with 01nO:=0, or equivalently that 
y2f{xi,yi)>{l-af 
with f{x,y):=yln^-{y-xf, 
/ : (0,l]x[0,l] ^iR. 
(5.46) 
The proof of the case n = 2 will not be repeated here, as it is elementary and 
well known. We will reduce the general case n > 2 to the case n = 2. It is 
enough to show that ^ / > 0 at all extremal points and "at" the boundary. 
The boundary is the set of all (x,y) where one Xi or one yi is or tends to 
zero. If one yi = 0 we can reduce (5.46) to n—1 (with a different a' — a — Xi) 
since /(rci,0) > (1 —a)^ —(1 —a')^. If one Xi^^O then f{xi^yi)-^oo. 
As / is 
bounded from below ( / > —2), ^f 
tends to infinity and (5.46) is satisfied. 
Hence (5.46) is satisfied "at" the boundary. 
The extrema in the interior are found by differentiation. To include the 
boundary conditions we add Lagrange multipliers A and /i 
n 
n 
n 
L{x,y) := ^f{xi,yi) + X'(^a-'^Xi^-^fi'(^l-J2yiy 
(5.47) 
We will not explicate every subtlety and only sketch the proof (cf. Section 3.9.1). 

176 
5 The Universal Algorithmic Agent AIXI 
The extrema are at dL/dxi~dL/dyi 
— Q^ i.e. at 
(5.48) 
Assume that (ic*,^/*) is a solution of (5.48). We can determine (A*,/x*) for 
this solution by inserting e.g. the first component {xl,yl) into (5.48). But 
all other components of (a?*,?/*) must be consistent with (5.48) too. Let us 
assume that for given (A*,/i*) there are m<oo different solutions of (5.48), i.e. 
(a?*,y*) consists only of m different components (xk.yk) with l<k<m 
where 
each component has multiplicity rik > 1- Define Xk :=nkXk and yk •=nkyk- We 
have 
k = l 
rik = n , 
Yl^^^^Yl ^^y^ "= X] ^^"" -^' 
X] ^^""' 
k=i 
k=i 
k=i 
Equal components in (5.46) can be grouped together 
z=l 
k=l 
•- 
Xk 
nlim 
-Xk) 
^ 
V ^ r 
~ 1 
'Tikyk 
2(~ 
~ 
\2 
k=i 
UkXk 
k=l E lykln^ 
- {yk - Xkf 
Xk 
= J^fixk.Vk) 
> 
{l-af. 
k=i 
So we have reduced the case '}2^=i i^ ^ somewhat unconventional way to the 
case X^]^i with m being the number of solutions of (5.48). One can show 
that there are at most two solutions for yi/xi by considering the equation for 
A-f/i, and from this that m < 2 . In the following we present a more abstract 
and general proof method. 
We will show that R{x^y) := f {x^y) — Xx — fiy has at most two (non-
degenerate) extrema, which will in turn proof that dxf = X, dyf = /j. has 
at most two solutions. Let us consider i? on a curve connecting two extrema 
g{t):=R{xit),y(t)), 
x{0)=Xk, 
x{l)=xi, 
y{0) = yk, y{l) = yh 0<t<l. 
From 
^'(0)=:^'(1)=0 we know that there is a toG [0,1] with g^^{to)=0. That is, every 
connecting curve between two extrema contains a point in which R has cur-
vature zero in one direction and hence zero Gauss curvature G. The support 
of R is divided by the curve(s) G{x,y) = 0 into zones. Each zone can contain 
at most one extremum. 
G{x,y) 
:= det 
dlR 
dydxR 
dxdyR 
dlR 
{dlf){dlf)-{d.dyff 
- (ji-)a-)-( 
+ 2) 
X 
x'^y 
y? 

5.12 History & References 
177 
This is zero for x = y only. The support of / is divided into two zones, x<y 
and X > y. The (infinitely many) degenerate extrema for Xk = yk give no 
contribution to (5.46) {f{x^x) = 0) and are hence irrelevant. So there are at 
most two nontrivial solutions of dxf = \ dyf = ii^ hence m<2. 
In summary we have reduced (5.46) for general n to m = 2, which can be 
shown by elementary means. 
• 
5.12 History & References 
Most references relevant to this chapter have already been given in previous 
chapters or in the main text of this chapter. Below we only remark on and give 
references to two further, in the context of AI^ interesting, topics: protocols 
in probability theory and bandit problems. 
Paradoxes, sample spaces, protocols, and incompleteness. There are 
many paradoxes in probability theory, like the Petersburg, Monty Hall, Simp-
son, Newcomb, rich uncle, and three prisoners [Sze86, EF98, ResOl, Mos65]. 
Some of them are not particularly related to probability theory, but just to 
the improper use of math in general. Probably the most interesting para-
doxes directly related to probability theory concern the awareness and choice 
of sample spaces and protocols (see [GH02] and references therein, especially 
[Sha85]). If one phrases these paradoxes within the AI/x model, one automat-
ically has to be aware of and choose a suitable sample space and protocol. 
If this procedure uniquely determines /x, the paradox is solved. If not, the 
problem description was not complete, i.e. the description is consistent with 
a whole set M of possible environments. This incompleteness can sometimes 
be overcome by symmetry or maximum entropy arguments (see [GH02] and 
Section 2.3.4). In general, a universal prior ^^ ^nd the predictions/actions 
of the SF^M/^^^M 
model represent a satisfactory solution to the paradoxes, 
solving the sample space, protocol, and incompleteness problem. 
Bandit problems. Bandit problems arose historically from the desire to 
optimally assign treatments to patients. They are prototypical problems for 
the so-called exploration versus exploitation dilemma. They were originally 
introduced by Robbins [Rob52]. One out of several arms (treatments) can 
be chosen, leading to a possible reward (success). The goal is to maximize 
one's reward in repeated trials. The simplest model is to assume that arm 
i leads to reward 1 (0) with probability pi (1—p^), where the probabilities 
are unknown. The traditional Bayesian solution to the uncertainty about pi 
is to assume a (second-order Beta) prior over pi. The goal is to maximize 
the (exponentially) discounted reward sum. A closed solution can be given in 
terms of Gittins indices [GJ74, Git89]. For regular discount sequences these 
strategies are not self-optimizing [BF85, KV86]. Many efficient heuristic self-
optimizing approaches exist. In a minimax approach one tries to find strategies 

178 
5 The Universal Algorithmic Agent AIXI 
that lead to the highest expected reward in the worst case over unknown 
chances Pi [VogGO]. A complete worst-case approach without any probabilistic 
assumption on the environment can be found in [ACBFS02]. The default 
textbook on bandits is [BF85] and on Gittins indices is [Git89]. 
5.13 Problems 
5.1 (Actions as random variables) [C35oi] Instead of defining ^^^(^^i:^) 
as a universal distribution over perceptions Xi^n conditioned under actions yi-^n 
as in (5.2), one may think of the following alternative definition: We use a uni-
versal distribution over perceptions and actions and then conditionalize to the 
actions, i.e. C/t(2^i:n) — ^(pi:n)/Exi:n^(Pi:n)' ^here M is Solomonoff's 
prior (2.21) (we could use ^^ as well). One motivation for doing so is to re-
gard M as a prior belief in the whole arrangement of agent-henvironment. 
The major problem with this approach is that ^^^^ is not enumerable. More 
precisely, the presented definition does not lead to an enumeration procedure 
for ^^i^. This does not necessarily imply non-enumerability of ^^^. Whether 
^ait —^^^ is ^^^^ ^^ open problem (cf. Problem 2.6). If true it would imply 
universality of ^^^^ and convergence to computable //^^. This alternative ap-
proach also allows conditionalization w.r.t. the perceptions and to determine 
M(2/r<fc^/ ), which may be interpreted as the agent's own belief in selecting 
action yk- But an actual action selection based on this probability would lead 
to a poorly performing agent, which differs from the "optimal" action y^ and 
2/^^'* via the expectimax expression. Could M(yr</c^ ) nevertheless be close 
to the action of p^ and/or p^^-^^ for large k, justifying the above interpretation 
of Ml (cf. Section 8.5.2 on multi-agent systems.) Prove or disprove the stated 
open questions, conjectures, and assertions. 
5.2 (Absorbing two-state environment) [C15ui] The HeavenHell exam-
ple of Section 5.3.2 was a three-state MDP that did not allow for self-optimizing 
policies. Here, a similar two-state MDP shall be analyzed in more detail. Let 
M = {/io,/ii,...}withtt;o = ^ i - | ( l - / ? ) > 0 , S.>2^,=/?>0,rfeG7^={0,2/3,l}, 
and y — {a^h}. Environments /io and /ii are deterministic MDPs defined as 
l^i — [r = ^/% C X E ^ = ' 2 / J S P ^=i]^ ^'^- iiiitially being in state s, action h irre-
vocably leads to state e. The reward in state s is 2/3. Environments /io and 
111 differ only in the reward in state e, which is r —0 in environment //Q? and 
r = l in environment [ii. Show that there is no poUcy that is self-optimizing in 
/io and /ii. Now, consider the case /? = 0: Determine VJ. for iG{0,l} and VF 
for all policies p. The results only depend on the first time action h is taken 
(if at all) and on the farsightedness m. Now determine V^^, p^, and V^. . The 
results show that p^ is not self-optimizing {V^^ T ^ ^ J - Generalize the latter 
result t o O < / ^ < | ( l —^) with environments /x^, i > 2 defined arbitrarily, to 
^fem' and to F^:;. 

5.13 Problems 
179 
5.3 (Computing p) [C30u] Show that for every enumerable chronological 
semimeasure p there exists a Turing machine T of length K{p) that computes 
it, i.e. pte:J=E,:T(,,,.„)=..„2-^('') and€(T)±i^(p) (see (5.7) for context). 
Hint: Adapt and improve Lemma 4.3.4 of [LV97, p255]. 
5.4 (Pareto optimality) [C30u] We have shown Pareto optimality of AI^ 
with ^ given in the form (.^Y^^WyV. Show that AIXI with M defined as in 
(5.2) is also Pareto optimal. Compare with or use the results of Problem 2.2. 
5.5 (Pareto optimality) [C30oi] We define policy p to be equivalent to pol-
icy p' if both policies lead to the same value Vy in all environments z^, i.e. if 
VJ = VJ VZ^GAI. Are all Pareto optimal policies equivalent to some (Pareto 
optimal) mixture policy p^ for certain weights Wi^l A positive answer to this 
question implies that one can restrict the search of optimal policies to mix-
ture policies. Try to find necessary and/or sufficient conditions that make the 
above question true. 
5.6 (Convergence of averages) [C20u] Let 6{m) '-= J2ueM'^^^^('^) 
^^^ 
X^i/GA^^^ < 1 as in Lemma 5.28. Show that the boundedness assumption 
0<di^{m) <c in Lemma 5.28(ii) is necessary for 5^(m)"^—^0 to imply exis-
tence and/or convergence of S{m)^^0. 
Show that 6jy{m) = 0{f{m)) 
\/ueM. 
does not necessarily imply S{m) = 0{f{m)) 
if M is infinite, even for bounded 
5jy (cf. Lemma 5.2S{iv)). 
Hints: For instance, for A^^i?V and (5^(m) := j ^ ^ ^ ^ " ' ^elT''^^ we have 
5i,{m) - ^ 0, but 6{m) = l-/^0. For (5j^(m):=:^^^"^—^0, S{m) does not even 
exist. For l>Sy{m):=j^:^=0{^) 
and Wy = ^;^^, 
we have 
6{m)>^. 
Another interesting example is Sjy{m) :=e~^^^ < 1, which decays exponentially 
in m for every z/, but for Wi, := ^jT^qiiy, 5{m) decays only harmonically. Show 
that 6{m) > 2^(1 —e~"^) ^ 4 ^ for m > 1 (easy) and even 5{m) > ^ 
(harder). 
5.7 (Domain of definitions) [C20u] Several subtleties concerning the do-
main of definition and existence have been ignored. First, Definition 5.19 
defined p^ only on histories produced by p^ itself. Given a history y<:k¥'y<k 
one has to generalize the definition similarly to Definition 5.30. Even in this 
generalized form p^ is only defined for histories that occur with nonzero p-
probability. Show that p^ and p^ are defined for all histories that have nonzero 
/^-probability. Use this to verify the soundness of the definitions and theorems 
in this chapter. 
5.8 (Self-optimizing policies for geometric discounting) [C30ui] 
Show that there are self-optimizing policies in ergodic MDPs even for geomet-
ric discounting {~^^ 7^ !)• On the other hand, the Bayes mixture pohcy p^ 
is not self-optimizing for bandit problems with geometric discounting. Since 
bandits are special ergodic MDPs, these results seem to contradict Theorem 
5.34, Clarify this paradox and discuss the implications. Hint: Histories yx<:k 
are policy dependent. 

180 
5 The Universal Algorithmic Agent AIXI 
5.9 (Relevant and non-computable environments /x) [C30oi] Assume 
feedback x consists of three parts x — x'r'x"^ future I/O is completely in-
dependent of x" ^ xW is sampled from a computable distribution, x" from a 
(possibly) non-computable distribution. Show that M multiplicatively dom-
inates ^^ where ^ is the true distribution \i modified in a way such that 
y^i — V/j,' but /i' is computable. This suggests that the computability assump-
tion on jj, may be weakened to the (for AIXI) relevant parts of the environment. 
Formulate and prove all this rigorously and generahze it to less trivial cases, 
where the relevant computable and the irrelevant non-computable information 
in X cannot be factored so easily. 
5.10 (Self-optimizing environments) [C35u] Ergodic MDPs admit self-
optimizing policies, which implies that PMDPI is self-optimizing (see Sec-
tion 5.6). Show that bandits, i.i.d. processes, and classification tasks are special 
(degenerate) cases of ergodic MDPs. The existence of self-optimizing policies 
is not limited to (subclasses of ergodic) MDPs. Suitably define ergodic par-
tially observable MDPs (ergodic POMDPs) and A:*^-order ergodic MDPs and 
show that these classes also admit self-optimizing policies. Furthermore, show 
that factorizable environments, defined in Section 4.3.1, admit self-optimizing 
policies. 
5.11 (Belief contamination) [C30ui] Consider an environmental class M 
that admits self-optimizing policies. Theorem 5.34 shows that p^ is self-
optimizing in the sense of limk-^oolVk'^ — Vk^^] = 0- The Bayes mixture 
^ := X^jy^^^zy^ expresses the degree of belief Wi, in environment u e M. 
We want to study the effect of additionally believing in some p ^ M with 
some small probability a. The new belief prior is ^' := {l — a)^ + ap. Show 
that a belief a in p much smaller than the belief w^ in the true environment 
pEA4 
causes only a small corruption of the self-optimizing property. More 
precisely, limsup;.^^[14*^-F^^ ^] < /fz^)^- Construct examples for which 
yp ^_ yp M _ Q^^moo; ^yg shows that the upper bound cannot be improved 
in general and that a belief contamination a of magnitude comparable to W/j^ 
can completely degrade performance. 
5.12 (Continuity of value for ergodic MDPs) [C40usm] Let fi and ft be 
MDPS that are "close" to each other in the sense that the transition ma-
trix T^^, = fi{sas') is close to T^^, = p{sas'), 
and the reward function 
^ss' =Z]rG7^^'/^('^^^y ^^ ^^^^^ ^^ ^ss' — Z^rG7^^*/^(^^'^'^) ^^ ^^^ scusc that 
e:=mdiXss^a{\T^s'-^ss'\^\^'is'-^'is'\} 
is "small". Furthermore, let p be a sta-
tionary policy, i.e. ak'=p{s<k) =pisk-i)- 
Show properties (i) — {vii) of the 
value function(s) 
(o) Condition: T is ergodic -or- T^^> =0 implies f^^, =0. 
(where ^o is a special initial "state"). 

5.13 Problems 
181 
(ii) v^:=lim^^oo:^Vf^ exists, 
(m) |v^-v^HO(s)if(o). 
T 
{iv) p-^:=argmaxpVj. can be chosen stationary. vJ:=maXpv5^ = Vj, . 
(v) |v5,-vt|=0(e)if(o). 
(^m) |i^^p;;^-i-y,*T|=3.o(i_) + 2.0(e) if (o). 
The "constant" factor hidden in 0() depends on T, but is independent of 
m, £, and T. Note that argmaxpl^^^ may be non-stationary, {viii) with fcom 
instead of Im and e^fco" '^ ~m~"^/^ was used in the proof of Theorem 5.38(i). 
Finally, show that Vj, =y^ (exactly) for sufficiently small e>0. liT,R 
are 
frequency estimates of T^R^ then the probability that e is not small decreases 
exponentially with the sample size fco- Together with the choice /cooclogm, 
improve Theorem 5.38(i) to O ( ^ ) . 
Hints: (i) immediate from definitions, {ii) follows from the existence of T:= 
lim^^oo^EfcL~o^(^)^ [Ber95b, pl87]. {Hi) For ergodic T all rows of T coincide 
with the stationary distribution, which is proportional to some column of the 
adjoint matrix of T, which itself is a polynomial in (the components) of T. 
(iv) similar to [Ber95b, pl91]. {v) from {in) and {iv). {vi) similar {ii). {vii) 
similar {iv). For {viii) chain {vi) + {iii) + {v) + {vi)-{-{vii) and use the triangle 
inequality. 
5.13 (Ergodic versus forgetful environments) [C20s/C10u] 
Forgetful 
environments were defined in Section 5.3.6 as being asymptotically indepen-
dent of the history. Ergodic MDPs were defined in Section 5.6 as visiting every 
state infinitely often. An environment is called acyclic if the probability of 
infinitely repeating cycles is zero. Show that every acyclic ergodic MDP is 
forgetful, but not every forgetful MDP is ergodic. Note also that forgetfulness 
is a broader concept than {k^^-ordev) MDPs. 
5.14 (Uniform mixture of MDPs) [C30usi] In the following you are asked 
to derive explicit expressions for ^^^^ for uniform prior belief w. Let fir ^ 
A^MDP be a (completely observable) MDP with transition matrix T. T^^, is 
the probability of going to state s' G Af under action a G 3^ if currently in 
state s G A'. Given a policy that determines the actions a^, the probability 
of action-observation history aiSi...anSn after cycle n is /iT(ai5i...an5^) = 
T^^g^'...'T^^_^g^, where SQ is some initial state (randomization over the initial 
state may be performed). Reward is a given function of state (r^ = r{sk)). 
Optimal policies/actions follow from the recursive Bellman equations (4.29) 
or the explicit expectimax expression of the Al/i model (4.17). Assume now 
that we only know that the true environment is an MDP, but nothing more, 
i.e. TeT:={T:T^^,>0,J2s'Tss^ 
= '^} is unknown. Since T is continuous, the 
Bayes mixture <^ has the form £,{os^.^):= j^WTlJiT{(^i:n)dT. 

182 
5 The Universal Algorithmic Agent AIXI 
(z) Assume a uniform prior belief over T, i.e. WT OC 1 and the measure dT 
is the uniform measure on the polytope T. Compute the integral and show 
that the ratio e(as<„^„) = ? K ^ J / C ( a s < J = ^,''„"_^,J(E,-iV::_,,-+5-l), 
where S = lA*] is the number of states and N^^, is the historical number of 
transitions from s to s^ under action a, (including the transition from SQ if 
s = so and to s^ if s' = Sji). This is just Laplace' law of succession [Lapl2] (see 
Problem 2.11), one for each (as5')-tuple. For instance, initially all transitions 
are equally plausible Ci^^i^i) — ^-
(a) Show that, although the class T is continuous and contains non-ergodic 
environments, the Bayes optimal policy p^ is self-optimizing for ergodic en-
vironments fiT^-MuBPi- 
The intuitive reason is that T is compact and the 
non-ergodic environments have measure zero. 
(iii) Model-based reinforcement learning algorithms try to estimate T 
from past experience. Give an expression for the posterior believe WTiosi:n)(x 
MT(o^i:n) i^ transition T. Note that this is a (complex) distribution over T, 
while most reinforcement learning algorithms only estimate a single (e.g. a 
most likely) T. Show that the expected transition probability E[TJ*g/|asi:n] := 
JrTss'^Tlosi:n)dT:={N^^,^l)/{J2s'^ss'^S) 
coincides with the relative his-
toric occurrence of {ass'). Show that policy p^ based on (4.17) appropriately 
explores the environment, while the popular policy based on E[T] or other 
point estimates like Maximum Likelihood lack exploration. 
(iv) Assume we know that the environment is a deterministic MDP, i.e. 
T={T:T^^,e{0,l},J2^,T^^^ 
= l}. Repeat (i)-(m) with this (now discrete) 
T. Is the corresponding p^ self-optimizing? 
(v) Assume now that for every action a there exists a mirror "undo" action 
a in the sense that T^^, =T^'s' R^P^at {i) — {iii) for the set of all deterministic 
"symmetric" MDPS. IS the corresponding p^ self-optimizing? An example is a 
robot moving in a (noiseless) environment, like a maze. Non-symmetric MDPS 
contain one-way streets or doors, which are missing in symmetric MDPS. 
{vi) Incorporate further knowledge of the form T^^, =0/1 for some {ass') 
and repeat {i)--{iii). For example, if we know that the environment is walxl 
grid maze, and transitions are a priori only possible between neighboring cells, 
we know that T^^, =0 if 5 and s' are not neighboring grid cells. 
{vii) Explore the difficulties when extending the considerations in {i) — {iii) 
to POMDPs, potentially with variable state space size 5, e.g. with prior ws oc 
5.15 (Effective horizons) [C25u] Derive the expressions for effective hori-
zons presented in Table 5.41. 
5.16 (Effect of discounting) [C20u] 
Consider 
the 
MDP 
// 
= 
[r^iCXj}''-^ '-^^xC^]; where s is the initial state. Taking action a 
forever gives reward 1 per cycle. Taking action b first gives no reward but 
pays off one cycle later with reward 2 + 6 > 2. Depending on the discount 
this delayed, but on average higher reward may be favorable. Show that 

5.13 Problems 
183 
for 7-geometric discount, V^^ ^ = 1 and V-^^ ^ = i+7^' ^^^^^ action b is 
favorable iff 7 > j ~ ^ . Show that for the power discount 7fc — p- and small 
5>0, the optimal policy performs action a for the first k^^ 
cycles and action 
b thereafter. In both cases the critical effective horizon is h^^ ~ ^. 


Ideas matter. 
Approximate the solution, not the problem. 
— Richard Sutton 
6 Important Environmental Classes 
in this and the following chapter we define ^ = ^ij = M to be Solomonoff's 
prior, i.e. AI^=AIXL In order to give further support for the universality and 
optimality of the AI^ theory, we apply AI^ in this chapter to a number of 
problem classes. They include sequence prediction, strategic games, function 
minimization and, especially, how AI^ learns to learn supervised. For some 
classes we give concrete examples to illuminate the scope of the problem class. 

186 
6 Important Environmental Classes 
We first formulate each problem class in its natural way (when ^p^°biem -g 
known) and then construct a formulation within the Al/i model and prove 
its equivalence. We then consider the consequences of replacing /i by ^. The 
main goal is to understand why and how the problems are solved by AI^. We 
only highlight special aspects of each problem class. Sections 6.2-6.6 together 
should give a better picture of the AI<^ model. We do not study every aspect 
for every problem class. The sections may be read selectively, and they are 
not essential to understand the remaining chapters. 
6.1 Repetition of the AI/[x/^ Models 
In the last chapter we unified sequential decision theory with the theory of 
universal induction to a model of artificial intelligence, which we claimed to 
be universal and superior to any other model in various senses. All tasks that 
require intelligence to be solved can naturally be formulated as a maximiza-
tion of some expected utility in the framework of agents. The main remaining 
problem is the unknown prior probability distribution /i^^ of the environ-
ment (s). Conventional learning algorithms are restricted in the sense that they 
can neither handle large (unstructured) state spaces, nor do they converge in 
the theoretically minimal number of interaction cycles, nor can they handle 
non-stationary environments appropriately. On the other hand, the universal 
semimeasure ^ (2.26), based on ideas from algorithmic information theory, 
solves the problem of the unknown prior distribution for induction problems. 
No explicit learning procedure is necessary, as ^ automatically converges to 
fi. We unified the theory of universal sequence prediction with the decision-
theoretic agent by replacing the unknown true prior ii^^ by an appropriately 
generalized universal semimeasure ^^^. For convenience we repeat some defi-
nitions and results from previous chapters that we need in this chapter. 
Let ii{ijpc_i.^) be the true chronological prior probability that the environ-
ment reacts with xi:k if provided with actions yi:k from the agent. We define 
^fc*f 17n(y^^-k) ^^ t)e the ^-expected reward sum in cycles k-\-l to m with out-
puts yi generated by agent p* and responses Xi from the environment. Adding 
reward rk = r{xk) to V^^-^ ^, we get the value including cycle k. probability 
of Xk —rkOk, given history yx^^kVk, is given by the conditional probability 
^,{y)c<^kWk)' Poficy p* chooses yk as to maximize the future reward. So the 
expected reward sum in cycles k to m given yr^k and yk chosen by p* is 
Km(^<A^) = max^[rfcH-F^*^i^^(i/ri:fc)]-/i(yr<fc?^;,). 
(6.1) 
Together with the induction start 
yZl,m{V^^--m) 
••= 0, 
(6.2) 
^/c*m ^^ completely defined. If rrik is our horizon function of p* and yx<^k is the 
actual history in cycle /c, the output yk of the agent is given by 

6.2 Sequence Prediction (SP) 
187 
Vk 
Xk 
which in turn defines the poUcy p*. Then the environment responds Xk with 
probabihty fjiijfic^kW^k)^ ^^^ cycle /c + 1 starts. We may unfold the recursion 
(6.1) further and give ijk explicitly as 
ijk = argmaxV'max V ] ... maxy](rfc+...+r^J-/x(?/i:<fc?^/c:mJ- 
(6-4) 
Xk 
Xk-^i 
'^'m-k 
This expression has a direct interpretation: The probability of inputs Xk-.mk 
in cycle k when the agent outputs yk-.mk with actual history ^<fe is 
Kw<kWc.k:mk)' '^^^ future reward in this case is rfc + ... + rmfc. The best ex-
pected reward is obtained by averaging over the Xi {J2x) ^^^ maximizing over 
the yi. This has to be done in chronological order to correctly incorporate the 
dependencies of Xi and yi on the history. This is essentially the expectimax 
algorithm/sequence/tree (see Figure 4.13). The Al/i model is optimal in the 
sense that no other policy leads to higher expected reward. Unfortunately, in 
AI, the environment /x is often unknown. The AI^ model is defined similarly 
to the Al/i model, but with the unknown // replaced by the (known) universal 
prior ^: 
yk = a r g m a x V m a x V 
... max V(rfc+...+r^J-C(^<fc2^fe:mJ 
(6.5) 
Vk 
^-^ 
Vk + l ^-^ 
Vm^ 
^—^ 
Xk 
Xk-\-l 
*^"^fc 
with ami:k) ••= E 
2-'^'\ 
(6.6) 
q'q{yi:k)=Xi:k 
where the sum runs over all chronological environments q satisfying q{yi:k) = 
xi.k' Motivations for AI^ being a good substitute for AI/x are the convergence 
of ^ to /i 
^{ip:<kmk:mk) 
' KW^<kmk:mk) 
^^^ 
A: -> 00, 
(6.7) 
Pareto optimality of AI^, self-optimization of AI^ for restricted ^, and tight 
error and loss bounds in the case of passive sequence prediction. 
6.2 Sequence Prediction (SP) 
We introduced the AI^ model as a unification of ideas of sequential decision 
theory and universal probability distribution. We might expect AI^ to behave 
identically to SF^:=0^ when faced with a sequence prediction (SP) problem, 
but things are not that simple, as we will see. Let us repeat the definition 
in Section 3.3 of the total number of expected erroneous predictions the SPp 
agent 0p makes for the first n observations 

188 
6 Important Environmental Classes 
n 
En' 
'•= YlYl^^-<k)^^~^^^<^^^')] 
with 
xf^ :=argmaxp(x<fcX;,). 
fc = l CC<fc 
(6.8) 
The SP/i agent is best in the sense that E^^' < E^p for any p. We showed that 
the universal predictor SP^ is not much worse 
E^^-E^^ 
< 2D + 2^/E^ 
= 0{^/W^), 
D t \n2-K{^i). 
(6.9) 
6.2.1 
Using the Al^t Model for Sequence Prediction 
We saw in Chapter 3 how to predict sequences for known and unknown 
prior distribution /x^^. Here we consider binary sequences ziZ2Z^...E:]B^ with 
known prior probability IJ^^^{ziZ2Z^...). (We use Zk to avoid notational con-
flicts with the agent's inputs Xk-) 
We want to show how the AI/x model can be used for sequence prediction. 
We will see that it makes the same prediction as the SP// agent. For simplicity 
we only discuss the special error loss ^xy — ^~^xy, where S is the Kronecker 
symbol, defined as 6ab = ^ for a = b and 0 otherwise. First, we have to specify 
how the Al/i model should be used for sequence prediction. The following 
choice is natural: 
The system's output yk is interpreted as a prediction for the k^^ bit Zk of 
the string under consideration. This means that yk is binary {yk^IB=:y). 
As 
a reaction of the environment, the agent receives reward r^ = 1 if the prediction 
was correct {yk = Zk), or r/c = 0 if the prediction was erroneous (yky^Zk)- The 
question is what the observation o^ in the next cycle should be. One choice 
would be to inform the agent about the correct k^^ bit of the string and set 
Ok = Zk' But as from the reward rk in conjunction with the prediction y^, the 
true bit Zk = Sy^^rk can be inferred, this information is redundant. There is 
no need for this additional feedback. So we set Ok = eeO — {e}, thus having 
Xjt = Tjt G 7^ = A' = {0,1}. The agent's performance does not change when we 
include this redundant information; it merely complicates the notation. The 
prior probability /i^^ of the Al/i model is 
f^^HyiXi...ykX^) 
= fJ^^^yiL^-ykLk) 
=- fJ^^^i^yir.Sy^rj,) 
= 
^^^{Zi...Zk). 
(6.10) 
In the following, we will drop the superscripts of /j because they are clear 
from the arguments of ^, and the /i equal in any case. Equation (6.1) for the 
expected reward reduces to 
KmiWC<k) 
= 
maxy^[rfc-hFfe*^i^^(^l:fe)]-/i(^j;iri.--V-irfe_i^yfcrJ. 
(6.11) 
The first observation we can make is that for this special //, V^^ only depends 
on Sy^n^ i-e. replacing yi and r^ simultaneously with their complements does 
not change the value of V^*^. We have a symmetry in y^r^. For k = m+l this 

6.2 Sequence Prediction (SP) 
189 
is definitely true as V^^i^rn^^ 
^^ ^^^^ ^^^^ (^^^ (6-2)). For k<m 
we prove 
it by induction. The r.h.s. of (6.11) is symmetric in yiTi for i < fc because /i 
possesses this symmetry and V"^*^^ ^ possesses it by induction hypothesis, so 
the symmetry holds for the Lh.s., which completes the proof. The prediction 
Vk is 
yk = argmax^[rfc + 14*{'i^^^(^<fc^fc)]-/i(^2)iri. 
rk 
= diigmsixy^rk'fJ^iSy^rI'"Sy^rk) 
= argmax/x(ii,..i/,_iy ) 
(6.12) 
id) 
Vk 
^—' 
Vk 
rk 
arg max fJ^{zi...Zk-iz_j^). 
Zk 
Equation (a) is the definition of the agent's action (6.3), where we have used 
(6.10), which gives the r.h.s. of (6.11) with max^^ replaced by argmax^^. 
Ylirfi-'-^yr'-') is independent of y for any function /, depending on the com-
bination 5yr only. Therefore, the 5^^V*/i term is independent of yk because 
^fc+i m ^s ^^^^ ^s ^ depend on 5y^rk only. In [h) we can therefore drop this 
term, as adding a constant to the argument of argmax^^ does not change the 
location of the maximum. In (c) we evaluated the Ylr • Further, if the true 
reward to iji is r^, the true z*^ bit of the string must be Zi — ^yifi- Equation 
{d) is just a renaming. 
So, the AI/x model predicts that Zk that has maximal //-probability, given 
ii...ifc_i. This prediction is independent of the choice of m^. It is exactly the 
prediction scheme of the sequence predictor SP/i with known prior described 
in Section 3.3. As this model was optimal, AI/x is optimal too, i.e. it has the 
minimal number of expected errors (maximal /i-expected reward) as compared 
to any other sequence prediction scheme. 
From this, it is already clear that the value V^!^ must be closely related to 
the expected sequence prediction error £'^^ (6.8). In the following we prove 
that V^^=m-Ef^^. 
We rewrite V;!^ in (6.11) as a function of Zi instead 
of yiVi^ as it is symmetric in yiVi. Further, we can pull V^^^^ 
out of the 
maximization, as it is independent of yk^ similarly as in (6.12). Renaming the 
bounded variables yk and r^, we get 
Km(^<fc) == max/x(z<A:Z;,)H-^y^*{'i^^(zi:/,)./x(2;<fc^fc). 
(6.13) 
Zk 
Zk 
Recursively inserting the Lh.s. into the r.h.s. we get 
m 
Kmi^<k) 
= YlYl 
max/i(2;<feZfe,i). 
(6.14) 
• 
7 
^ ' ^ 
l=k 
Zk:i-1 
This is most easily proven by induction. For k = m we have V^^(^<m) = 
indiXzmf^(^<mZ.m) from (6.13) and (6.2), which equals (6.14). By induction 

190 
6 Important Environmental Classes 
hypothesis, we assume that (6.14) is true for k-\-l. Inserting this into (6.13) 
we get 
K!!ii^<k) = m^x^{z<kZk) + Yl Y2 
Yl ^^^l^i^l:kZk^l..i) 
/^(^</c^fc) 
= max//(z<fcZ;,)+ V 
V 
max/i(2:</,^;,^,), 
Zk 
^ 
^-^ 
Zi 
i = k + l 
Zk:i-1 
which equals (6.14). This was the induction step, and hence (6.14) is proven. 
By setting k = l and slightly reformulating (6.14), we get the total expected 
reward in the first m cycles 
m 
with E^^ defined in (6.8). 
6.2.2 
Using the AI^ Model for Sequence Prediction 
Now we want to use the universal AI^ model instead of Al/i for sequence 
prediction and try to derive error bounds analogous to (6.9). Like in the Al/i 
case, the agent's output yk in cycle k is interpreted as a prediction for the k^^ 
bit Zk of the string under consideration. The reward is rk=Sy^zki ^^^ there 
are no other observations Ok = e. What makes the analysis more difficult is 
that ^ is not symmetric in yiri^^{l—yi){l—ri) 
and (6.10) does not hold for ^. 
On the other hand, ^^^ converges to /i^^ in the limit (6.7), and (6.10) should 
hold asymptotically for ^ in some sense. So we expect that everything proven 
for Al/i holds approximately for AI^. The AI^ model should behave similarly 
to Solomonoff" prediction SP^. In particular, we expect error bounds similar to 
(6.9). Making this rigorous seems difficult. Some general remarks were made 
in the last chapter. Note that bounds like (5.15) cannot hold in general, but 
could be valid for Al^ in (pseudo)passive environments. 
Here we concentrate on the special case of a deterministic computable 
environment, i.e. the environment is a sequence z = ziZ2... with 
Km{zi...Zn)< 
Km{z) <oo, where Km{zi:n) is the length of the shortest (possibly nonhalting) 
program printing a string starting with Zi-^n- Furthermore, we only consider the 
simplest horizon model rrik = k, i.e. greedily maximize only the next reward. 
This is sufficient for sequence prediction, as the reward of cycle k only depends 
on output yk and not on earlier decisions. This choice is in no way sufficient 
and satisfactory for the full AI^ model, as one single choice of nik should serve 
for all AI problem classes. So AI^ should allow good sequence prediction for 
some universal choice of rrik and not only for mk = k^ which definitely does not 
suffice for more complicated AI problems. The analysis of this general case 

6.2 Sequence Prediction (SP) 
191 
is a challenge for the future. For mk = k the AI^ model (6.5) with Oi = e and 
Tfe G {0,1} reduces to 
yk = argmaxVrfc'^(r<fcM:fc) = argmaxC(^<fci/fcl) == argmax^(^<fcyfcl). 
Vk 
^—^ 
Vk 
Vk 
(6.15) 
The environmental response fk is given by ^y^Zk 5 i^ i^ 1 ^^ ^ correct prediction 
(^^ = i^) and 0 otherwise. In the following, we want to bound the number of 
errors this prediction scheme makes. We need the following inequality: 
We have to find a short program in the sum (6.6) calculating ri.-.r/e from 
yi...yk- If we knew Zi := Sy^n for 1 < i < fc, a program of size 0(1) could 
calculate ri.,.Tk — Sy^zi'--^ykZk' So, combining this program with a shortest 
coding of zi...Zk leads to a program q of size -^(q) =Km{zi,..Zk)-\-0{l) 
with 
q(yi:k)=n:k, 
which proves (6.16). 
Let us now assume that we make a wrong prediction in cycle fe, i.e. rk=0, 
Vky^^k' The goal is to show that ^ defined by 
ik := ^(^i:fc) = C{yt<kykQ) < ^{yr^k) - i{yt<kykl) < ik-i - oc, 
decreases for every wrong prediction, at least by some a. The < arose from 
the fact that ^ is only a semimeasure. 
In the first inequality we used the fact that yk maximizes by definition (6.15) 
the argument, i.e. l—yk has lower ^ probability than t/k. Bound (6.16) was 
applied in the second inequality. The equality holds because i^ = 5^.^.. and 
^(i-yfc)i ~^yfeO~^yfc^fc —^k- The last inequality follows from the definition of 
z. 
We showed that each erroneous prediction reduces ^ by at least the a 
defined above. Together with ^o = 1 and ^/c > 0 for all k this shows that the 
agent can make at most l/a errors, since otherwise ^k would become negative. 
So the number of wrong predictions E^^ of agent (6.15) is bounded by 
E^'i 
< i = 2^-(^)+o(i) < 00 
(6.17) 
for a computable deterministic environment string iii2---« The intuitive in-
terpretation is that each wrong prediction eliminates at least one program 
p of size i{p) < Km(z). The size is smaller than Km{z)j as larger policies 
could not mislead the agent to a wrong prediction, since there is a program of 
size Km{z) making a correct prediction. There are at most 2^"^^^^+^^^) such 
policies, which bounds the total number of errors. 

192 
6 Important Environmental Classes 
We derived a finite bound for E^^, but unfortunately, a rather weak one 
as compared to (6.9). The reason for the strong bound in the SP case was 
that every error at least halves ^ because the sum of the argmax^^^ arguments 
was bounded by 1. Here we have 
^(yin-.y/c-ir/c-iOO) +^(2/iri...|/fc-if/c-i01) < 1, 
^iyih--yk~irk-i^Q) 
+ ^{yih-'-yk-iTk-iW 
< i^ 
but argmax^^ runs over the right top and right bottom ^, for which no sum 
criterion holds. 
The AI(^ model would not be sufficient for realistic applications if the bound 
(6.17) were sharp, but we have the strong feeling (but only weak arguments) 
that better bounds proportional to Km{z) analogous to (6.9) exist. The tech-
nique used above may not be appropriate for achieving this. One argument 
for a better bound is the formal similarity between argmax2fc^(i<A:^;(;) ^^^ 
(6.15), the other is that no example sequence for which (6.15) makes more 
than 0{Km{z)) 
errors is known (see Problem 6.2). 
6.3 Strategic Games (SG) 
6.3.1 
Introduction 
A very important class of problems are strategic games (SG). Game theory 
considers simple games of chance like roulette, combined with strategy like 
backgammon, up to purely strategic games like chess or checkers or go. In 
fact, what is subsumed under game theory is so general that it includes not 
only a huge variety of game types, but can also describe political and economic 
competitions and coalitions, Darwinism and many more topics. It seems that 
nearly every AI problem could be brought into the form of a game. Never-
theless, the intention of a game is that several players perform actions with 
(partial) observable consequences. The goal of each player is to maximize 
some utility function (e.g. to win the game). The players are assumed to be 
rational, taking into account all information they posses. The different goals 
of the players are usually in conflict. For an introduction into game theory, 
see [FT91, OR94, RN95, NM44]. 
If we interpret the AI/x model as one player and the environment mod-
els the other rational player and the environment provides the reinforcement 
feedback r/c, we see that the agent-environment configuration satisfies all cri-
teria of a game. On the other hand, the AI models can handle more general 
situations, since they interact optimally with an environment, even if the en-
vironment is not a rational player with conflicting goals. 

6.3 Strategic Games (SG) 
193 
6.3.2 
Strictly Competitive Strategic Games 
In the following, we restrict ourselves to deterministic, strictly competitive 
strategic^ games with alternating moves. Player 1 makes move i/k in round 
fc, followed by the move Ok of player 2.^ So a game with n rounds consists 
of a sequence of alternating moves yiOiy202''.ynOn' At the end of the game 
in cycle n the game or final board situation is evaluated with V{yiOi...ynOn)' 
Player 1 tries to maximize V, whereas player 2 tries to minimize V. In the 
simplest case, F is 1 if player 1 won the game, V = —l if player 2 won and 
y = 0 for a draw. We assume a fixed game length n independent of the actual 
move sequence. For games with variable length but maximal possible number 
of moves n, we could add dummy moves and pad the length to n. The optimal 
strategy (Nash equilibrium) of both players is a minimax strategy 
Ok = argminmaxmin...maxminy(yiOi...yfcOfc...y^On), 
(6.18) 
Ok 
Vk + l Ofc + l 
Vn 
On 
yk = 8iTgmaxmm,..m&xmmV{yidi.,.yk-idk-iykOk''-ynOn)' 
(6.19) 
^fc 
Ok 
Vn 
On 
But note that the minimax strategy is only optimal if both players behave 
rationally. If, for instance, player 2 has limited capabilities or makes errors 
and player 1 is able to discover these (through past moves), he could exploit 
these weaknesses and improve his performance by deviating from the minimax 
strategy. At least the classical game theory of Nash equilibria does not take 
into account limited rationality, whereas the AI^ agent should. 
6.3.3 
Using the AI/x Model for Game Playing 
In the following, we demonstrate the applicability of the AI/x model to games. 
The Al/i model takes the position of player 1. The environment provides the 
evaluation V. For a symmetric situation we could take a second AI/x model 
as player 2, but for simplicity we take the environment as the second player 
and assume that this environmental player behaves according to the minimax 
strategy (6.18). The environment serves as a perfect player and as a teacher, 
albeit a very crude one, as it tells the agent at the end of the game only 
whether it won or lost. 
The minimax behavior of player 2 can be expressed by a (deterministic) 
probability distribution fjp^ as the following: 
f l if Ofc-argmin...maxminy(yiOi...yfco'fc.-.y>n)VA:, 
[ 0 otherwise. 
(6.20) 
^ In game theory games like chess are often called 'extensive', whereas 'strategic' 
is reserved for a different kind of game. 
^ We anticipate notationally the later identification of the moves of player 1/2 with 
the actions/observations in the AI models. 

194 
6 Important Environmental Classes 
The probability that player 2 makes move Ok is fi^^{yidi...ykOj^), which is 1 
for Ok = dk as defined in (6.18) and 0 otherwise. 
Clearly, the Al/i system receives no feedback, i.e. ri = ...=r^_i—0, until the 
end of the game, where it should receive positive/negative/neutral feedback 
on a win/loss/draw, i.e. rn = V{...). The environmental prior probability is 
therefore 
, Ai/^ 
X _ / fi^^{yio^...ynQj 
if ri...rn-i = 0 and r^ = V{yiOi...ynOn), 
M mi:n) - I 
0 
otherwise, 
(6.21) 
where Xi = riOi. If the environment is a minimax player (6.18) plus a crude 
teacher F, i.e. if /j.^^ is the true prior probability, the question now is, what 
is the behavior y^^ of the Al/i agent. It turns out that if we set rrik =n the 
Al/i agent is also a minimax player (6.19) and hence optimal: 
Vk^ = argmaxV...maxVF(?p<fctpfc:n)-/i^^(2P<fc|^fc:n) 
Vk 
^-^ 
Vn 
^-^ 
Ok 
On 
= argmaxV'...max V 
m3y.mmV{ip<kyOk-.n)-l^^^{ip<kyOk-n-i) 
Vk 
^ 
^ 
Vn-l 
^ ^ 
Vn 
On 
Ok 
On-1 
— ... = argmaxmin...maxminF(^<fclPA;:n) = yt^ 
(6.22) 
Vk 
Ofc + l 
Vn 
On 
In the first line we inserted rrik = n and (6.21) into the definition (6.4) of 
y^^. This removes all sums over the r^. Further, the sum over On gives only 
a contribution for o^ = argmino/^F(?/i6i...^„o^) by definition (6.20) of fi^^. 
Inserting this On gives the second line. Eff'ectively, //^^ is reduced to a lower 
number of arguments and the sum over On replaced by min^^. Repeating this 
procedure for O„_I,...,OA:-|-I leads to the last line, which is just the minimax 
strategy of player 1 defined in (6.19). 
Let us now assume that the game under consideration is played s times. 
The prior probability then is 
r=0 
where we have renamed the prior probability (6.21) for one game to /if^ 
Equation (6.23) is a special case of a factorizable JJL (defined in Section 4.3.1) 
with identical factors iir = l^i^ for all r and equal episode lengths nr-\-i—nr=n. 
The Al/i agent (6.23) for repeated game playing (SGR) also implements the 
minimax strategy 
y^^ = argmaxmin... max min V{iPrn+i:k-i'-'Wk:{r+i)n) 
(6-24) 
Vk 
Ok 
y ( r + l ) n 
0(^r+l)n 
with r such that rn< k < (r+l)n and for any choice of rrik as long as the 
horizon hk>n. This can be proven by using (4.27) and (6.22). 

6.3 Strategic Games (SG) 
195 
6.3.4 
Gaines of Variable Length 
We have argued that a single game of variable but bounded length can be 
padded to a fixed length without effect. We now analyze in a sequence of games 
the effect of replacing the games with fixed length by games of variable length. 
The sequence yiOi..,ynOn can still be grouped into episodes corresponding 
to the moves of separated consecutive games, but now the length and total 
number of games that fit into the n moves depend on the actual moves taken. 
(If the sum of game lengths does not fit exactly into n moves, we pad the last 
game appropriately.) V{yiOi...ynOn) equals the number of games where the 
agent wins minus the number of games where the environment wins. Whenever 
a loss, win or draw is achieved by the agent or the environment, a new game 
starts. The player whose turn it would next be, begins the next game. The 
games are still separated in the sense that the behavior and reward of the 
current game does not influence the next game. On the other hand, they 
are slightly entangled, because the length of the current game determines 
the time of start of the next. As the rules of the game are time invariant, 
this does not influence the next game directly. If we play a fixed number of 
games, the games are completely independent, but if we play a fixed number 
of total moves n, the number of games depends on their lengths. This has the 
following consequences: the better player tries to keep the games short, to win 
more games in the given time n. The poorer player tries to draw the games 
out, in order to lose fewer games. The better player might further prefer a 
quick draw, rather than to win a long game. Formally, this entanglement is 
represented by the fact that the prior probability // no longer factorizes. The 
reduced form (6.24) of ^j^^ to one episode is no longer valid. Also, the behavior 
y^^ of the agent depends on rrik, even if the horizon hk is chosen larger than 
the longest possible game. The important point is that the agent realizes that 
keeping games short/long can lead to increased reward. In practice, a horizon 
much larger than the average game length should be sufficient to incorporate 
this effect. The details of games in the distant future do not affect the current 
game and can, therefore, be ignored. A more quantitative analysis could be 
interesting, but would lead us too far astray. 
6.3.5 
Using the AI^ Model for Game Playing 
When going from the specific AI/x model, where the rules of the game are 
explicitly modeled into the prior probability /i"^^, to the universal model AI^, 
we have to ask whether these rules can be learned from the assigned rewards 
Tk- Here, the main reason for studying the case of repeated games (SGR) 
rather than just one game arises. For a single game there is only one cycle 
of nontrivial feedback, namely the end of the game, which is too late to be 
useful except when further games follow. 
Even in the case of repeated games, there is only very limited feedback, 
at most log23 bits of information per game if the 3 outcomes win/loss/draw 

196 
6 Important Environmental Classes 
have the same frequency. So there are at least 0{K{game)) 
number of games 
necessary to learn a game of complexity K{game). Apart from extremely 
simple games, even this estimate is far too optimistic. As the AI^ agent has 
no information about the game to begin with, its moves will be more or 
less random, and it can win the first few games merely by pure luck. So the 
probability that the agent loses is near 1, and hence the information content 
/ in the feedback r^ at the end of the game is much less than log23. This 
situation remains for a very large number of games. But, in principle, every 
game should be learnable after a very long sequence of games even with only 
this minimal feedback, as long as 7^0. 
The important point is that no other learning scheme with no extra infor-
mation can learn the game more quickly than AI^. We expect this to be true 
as /i^^ factorizes in the case of games of fixed length, i.e. fi^^ satisfies a strong 
separability condition. In the case of variable game length the entanglement 
is also low. ji^^ should still be sufficiently separable, allowing us to formulate 
and prove good reward bounds for AI^. Indeed, the situation is significantly 
better for games of variable length. Since initially, AI^ loses all games, it tries 
to draw out a loss as long as possible, without having ever experienced or 
even knowing what it means to win. Initially, AI^ will make a lot of illegal 
moves. Since each illegal move will immediately abort the game resulting in 
(non-delayed) negative reward (loss), AI^ can quickly learn the typically sim-
ple rules concerning legal moves, which usually constitute most of the rules; 
just the goal rule is missing. After having learned the move-rules, AI^ learns 
the (negatively rewarded) losing positions, the positions leading to losing po-
sitions, etc., so it can try to draw out losing games. For instance, in chess, 
avoiding being check mated for 20, 30, 40 moves against a master is already 
quite an achievement. At this ability stage, AI^ should be able to win some 
games by luck, or speculate about a symmetry in the game that check mating 
the opponent will be positively rewarded. Once having found out the complete 
rules (moves and goal), AI^ will right away reason that playing minimax is 
best, and henceforth beat all grandmasters. 
If a (complex) game cannot be learned in this way in a realistic number of 
cycles, one has to provide more feedback. This could be achieved by interme-
diate help during the game. The environment could give positive (negative) 
feedback for every good (bad) move the agent makes. The demand on whether 
a move is to be valuated as good should be adapted to the gained experience 
of the agent in such a way that approximately the better half of the moves 
are valuated as good and the other half as bad, in order to maximize the 
information content of the feedback. 
For more complicated games like chess, even more feedback may be nec-
essary from a practical point of view. One way to increase the feedback far 
beyond a few bits per cycle is to train the agent by teaching it good moves. 
This is called supervised learning. Despite the fact that the AI// model has 
only a reward feedback r^, it is able to learn supervised, as will be shown in 
Section 6.5. Another way would be to start with more simple games contain-

6.4 Function Minimization (FM) 
197 
ing certain aspects of the true game and to switch to the true game when the 
agent has learned the simple game. 
No other difficulties are expected when going from // to ^. Eventually ^^^ 
will converge to the minimax strategy fi^^. In the more reahstic case, where 
the environment is not a perfect minimax player, AI^ can detect and exploit 
the weakness of the opponent. 
Finally, we want to comment on the input/output space ^/y 
of the AI 
models. In practical apphcations, y will possibly include also illegal moves. 
If y is the set of moves of, e.g. a robotic arm, the agent could move a wrong 
figure or even knock over the figures. A simple way to handle illegal moves yk 
is by interpreting them as losing moves, which terminate the game. Further, 
if, e.g. the input Xk is the image of a video camera which makes one shot per 
move, A' is not the set of moves by the environment but includes the set of 
states of the game board. The discussion in this section handles this case as 
well. There is no need to explicitly design the systems I/O space ^/y 
for a 
specific game. 
The discussion above on the AI^ agent was rather informal for the following 
reason: game playing (the SG^ agent) has (nearly) the same complexity as 
fully general AI, and quantitative results for the AI^ agent are difficult (but 
not impossible) to obtain. 
6.4 Function Minimization (FM) 
6.4.1 
Applications/Examples 
There are many problems that can be reduced to function minimization (FM) 
problems. The minimum of a (real-valued) function f : y —^ M over some 
domain 3^ or a good approximate to the minimum has to be found, usually 
with some limited resources. 
One popular example is the traveling salesman problem (TSP). y is the 
set of different routes between towns, and f{y) the length of route y^y. 
The 
task is to find a route of minimal length visiting all cities. This problem is NP 
hard. Getting good approximations in limited time is of great importance in 
various applications. Another example is the minimization of production costs 
(MFC), e.g. of a car, under several constraints, y is the set of all alternative 
car designs and production methods compatible with the specifications, and 
f{y) is the overall cost of alternative y ^y. 
A related example is finding 
materials or (bio)molecules with certain properties (MAT), e.g. solids with 
minimal electrical resistance or maximally efficient chlorophyll modifications, 
or aromatic molecules that taste as close as possible to strawberry. We can 
also ask for nice paintings (NPT). y is the set of all existing or imaginable 
paintings, and f{y) characterizes how much person A likes painting y. The 
agent should present paintings which A likes. 

198 
6 Important Environmental Classes 
For now, these are enough examples. The TSP is very rigorous from a 
mathematical point of view, as /, i.e. an algorithm of /, is usually known. In 
principle, the minimum could be found by exhaustive search, were it not for 
computational resource limitations. For MFC, / can often be modeled in a 
reliable and sufficiently accurate way. For MAT you need very accurate phys-
ical models, which might be unavailable or too difficult to solve or implement. 
For NPT all we have is the judgement of person A on every presented paint-
ing. The evaluation function / cannot be implemented without scanning yl's 
brain, which is not possible with today's technology. 
So there are different limitations, some depending on the application we 
have in mind. An implementation of / might not be available, / can only be 
tested at some arguments y and f{y) is determined by the environment. We 
want to (approximately) minimize / with as few function calls as possible 
or, conversely, find an as close as possible approximation for the minimum 
within a fixed number of function evaluations. If / is available or can quickly 
be inferred by the agent and evaluation is quick, it is more important to 
minimize the total time needed to imagine new trial minimum candidates 
plus the evaluation time for /. As we do not consider computational aspects 
of AI^ till Section 7.2, we concentrate on the first case, where / is not available 
or dominates the computational requirements. 
6.4.2 
The Greedy Model FMG/x 
The FM model consists of a sequence yiZiy2Z2.-- where yk is a trial of the FM 
agent for a minimum of /, and Zk = f{yk) is the true function value returned 
by the environment. We randomize the model by assuming a probability dis-
tribution /i(/) over the functions. There are several reasons for doing this. 
We might really not know the exact function /, as in the NPT example, and 
model our uncertainty by the probability distribution /i. What is more impor-
tant, we want to parallel the other AI classes, like in the SP/i model, where 
we always started with a probability distribution /i that was finally replaced 
by ^ to get the universal Solomonoff prediction SP^. We want to do the same 
thing here. Further, the probabilistic case includes the deterministic case by 
choosing M(/) = ^//O? where /o is the true function. A final reason is that the 
deterministic case is trivial when /j, and hence /o are known, as the agent 
can internally (virtually) check all function arguments and output the correct 
minimum from the very beginning. 
We will assume that 3^ is countable or finite and that /x is a discrete 
measure, e.g. by taking only computable functions. The probability that the 
function values of yi,..,,^^ are zi,...,Zn is then given by 
/i™(?/i^i-.yn^n) := 
E 
/^(/)- 
(^-25) 
f-f{yi)^zi 
yi<i<n 
We start with a model that minimizes the expectation Zk of the function value 
/ for the next output yk^ taking into account previous information: 

6.4 Function Minimization (FM) 
199 
yk •= 
SiTgmmy^Zk'fl{ylZl...yk-lZk~lyk^k)' 
This type of greedy algorithm, just minimizing the next feedback, was suffi-
cient for sequence prediction (SP) and is also sufficient for classification (CF). 
It is, however, not sufficient for function minimization, as the following exam-
ple demonstrates. 
Take /:{0,1}—>{1,2,3,4}. There are 16 different functions which shall be 
equiprobable, M(/) —i^- The function expectation in the first cycle 
(2:1) : - ^ z r / i ( ^ i ^ i ) - 
i ^ z i 
- 
1(1+2+3+4) = 2.5 
Zl 
Zi 
is just the arithmetic average of the possible function values and is independent 
of yi. Therefore, yi =0, if we defined argmin to take the lexicographically first 
minimum in an ambiguous case like here. Let us assume that /o(0)=2, where 
/o is the true environment function, i.e. zi—2. The expectation of Z2 is then 
{Z2) '= ^2;2-M(02y2^2) "" { 2. 
2 
for 
1/2 = 0, 
5 for 
1/2 = 1-
For ^2=0 the agent already knows /(0) = 2; for ^2 = 1 the expectation is, again, 
the arithmetic average. The agent will again output y2=0 with feedback i2 = 2. 
This will continue forever. The agent is not motivated to explore other y's as 
/(O) is already smaller than the expectation of /(I). This is obviously not 
what we want. The greedy model fails. The agent ought to be inventive and 
try other outputs when given enough time. 
The general reason for the failure of the greedy approach is that the infor-
mation contained in the feedback Zk depends on the output yk- A FM agent 
can actively influence the knowledge it receives from the environment by the 
choice in yk- It may be more advantageous to first collect certain knowledge 
about / by an (in greedy sense) nonoptimal choice for 7/^, rather than to 
minimize the Zk expectation immediately. The nonminimality of Zk might be 
overcompensated in the long run by exploiting this knowledge. In SP, the re-
ceived information is always the current bit of the sequence, independent of 
what SP predicts for this bit. This is why a greedy strategy in the SP case is 
already optimal. 
6.4.3 
The General FM^x/C Model 
To get a useful model we have to think more carefully about what we really 
want. Should the FM agent output a good minimum in the last output in 
a limited number of cycles m, or should the average of the zi,...,^^ values 
be minimal, or does it suffice that just one of the z is as small as possible? 
Let us define the FM// model as to minimize the /x-averaged weighted sum 
aiZi-V...-\-amZm for some given Q;fc>0. Building the fi average by summation 

200 
6 Important Environmental Classes 
over the Zi and minimizing w.r.t. the yi has to be performed in the correct 
chronological order. With a similar reasoning as in (6.1) to (6.4) we get 
y ™= 
argminV...minV(aiZi+...+a^z^)-//(?/iii...yA;-iifc-iy/c^fc---2/m^m 
Vk ^ 
' 
Vm ^ 
' 
^k 
Z-m 
(6.26) 
If we want the final output ijm to be optimal we should choose a^ = 0 for 
k <m and am = 1 (final model FMF^). If we want to already have a good 
approximation during intermediate cycles, we should demand that the output 
of all cycles together be optimal in some average sense, so we should choose 
a/e = 1 for all k (sum model FMS/i). If we want to have something in between, 
for instance, increase the pressure to produce good outputs, we could choose 
the a;c=e^^^~'^^ exponentially increasing for some 7 > 0 (exponential model 
FME/i). For 7 ^ 0 0 we get the FMF/x; for 7-^0 the FMS// model. If we want 
to demand that the best of the outputs yi-.-Vk is optimal, we must replace 
the a-weighted z-sum by min{zi,...,2rn} (minimum model FMM/x). We expect 
the behavior to be very similar to the FMF/x model, and do not consider it 
further (see Section 8.5.1 item 4). 
By construction, the FM/x models guarantee optimal results in the usual 
sense that no other model knowing only // can be expected to produce better 
results. The variety of FM variants is not a fault of the theory. They just 
reflect the fact that there is some interpret at ional freedom of what is meant 
by minimization within m function calls. In most apphcations, probably FMF 
is appropriate. In the NPT application one might prefer the FMS model. 
The interesting case (in AI) is when /i is unknown. For this case we 
define the FM^ model by replacing /i(/) with some C(/), which should as-
sign high probability to functions / of low complexity. So we might define 
^if)~J2q-\/x\u(qx)=f(x)]'^~^^^^' "^^^ problem with this definition is that it is, 
in general, undecidable whether a TM q is an implementation of a function 
/. ^(/) defined in this way is uncomputable, and not even approximable. As 
we only need a ^ analogous to the l.h.s. of (6.25), the following definition is 
natural 
e'^iViZv-ynzJ 
:= 
Yl 
2-^(') 
(6.27) 
q:q{yi)=Zi 
\/l<i<n 
^^^ is actually equivalent to inserting the uncomputable ^(/) into (6.25). 
^^ 
is an enumerable semimeasure and dominates all enumerable probability 
distributions of the form (6.25). We will not prove this here. 
Alternatively, we could have constrained the sum in (6.27) by q{yi---yn) = 
zi...Zn analogous to (6.6), but these two definitions are not equivalent. Defini-
tion (6.27) ensures the symmetry^ in its arguments, and 
^^^{••'y^'--yz\..)=^0 
for Zy^z'. It incorporates all general knowledge we have about function min-
imization, whereas (6.6) does not. But this extra knowledge has only low 
^ See [S0I99] for a discussion on symmetric universal distributions on unordered 
data. 

6.4 Function Minimization (FM) 
201 
information content (complexity of 0(1)), so we do not expect FM^ to per-
form much worse when using (6.6) instead of (6.27). But there is no reason to 
deviate from (6.27) at this point. 
We can now define an "error" measure E™^ as (6.26) with k = l and 
argmin^/i replaced by miny^ and, additionally, /i replaced by ^ for £'™^. We 
expect \E™^ — E™^\ to be bounded in a way that justifies the use of ^ 
instead of /x for computable //, i.e. computable /o in the deterministic case. 
The arguments are the same as for the AI^ model. 
6.4.4 Is the General Model Inventive? 
In the following we will show that FM^ will never cease searching for minima, 
but will test an infinite set of different y's for m ^ o o . 
Let us assume that the agent tests only a finite number of yiE 
Acy^ 
1^1 < 00. Let t—1 be the cycle in which the last new ye A is selected (or some 
later cycle). Selecting i/'s in cycles k>t a. second time, the feedback z does 
not provide any new information, i.e. does not modify the probability ^^^. 
The agent can minimize E™^ by outputting in cycles k>t 
the best 
y^A 
found so far (in the case ak = 0, the output does not matter). Let us fix / for 
a moment. Then we have 
£;« — a^zi-^ ,..+amZm 
= yZ^kfiyk)-^ 
fi-y^o^k, 
fi'= 
min 
f{yk). 
' 
' 
i<fe<t 
fc=l 
k=t 
~ 
Let us now modify the agent and assume that it tests one additional 
yt^A 
in cycle t, but no other y^A. 
Again, it will keep to the best output for fc>t, 
which is either the one of the previous agent or yt. 
t 
m 
E' = ^Qfc/(y,) + min{/i,/(yt)}.5^afc. 
k=l 
k=t+l 
The diff'erence can be represented in the form 
E-^E" 
= If^aXf^-at-r, 
/± := max{0,±(/i-/(j/t))} > 0. 
As the true FM strategy is the one that minimizes E^ assumption a is ruled 
out if E^ > E^. We then say that b is favored over a, which does not mean 
that h is the correct strategy, only that a is not the true one. For probability 
distributed /, 6 is favored over a when 
£"-£"= 
(f^aXin-arin 
> 0 
^ 
f ^ a , > a e | ^ , 
where (f^) 
is the ^ expectation of ±{fi—f{yt)) 
under the condition that 
±fi>±f{yt) 
and under the constraints imposed in cycles 1...^—1. As ^ assigns 

202 
6 Important Environmental Classes 
a strictly positive probability to every nonempty event, (/+) ^ 0. Inserting 
ak=e^^^~'^\ 
assumption a is ruled out in model FME^ if 
't > -In 
7 
1+ ^ 
Me^ - 1) 
. 
0 
for 7 -> 00 (FMF^ 
^ 
^ ( / - ) / ( / + ) - l f o r 7 - 0 
(FMSO 
We see that if the condition is not satisfied for some t, it will remain wrong 
for all f >t. So the FME(^ agent will test each y only once up to a point from 
which on it always outputs the best found y. Further, for m—>oc the condition 
always gets satisfied. As this is true for any finite A, the assumption of a finite 
A is wrong. For m-^oo the agent tests an increasing number of different y's, 
provided y is infinite. The FMF^ model will never repeat any y except in the 
last cycle m, where it chooses the best found y. The FMS<^ model will test a 
new yt for fixed m, only if the expected value of f{yt) is not too large. 
The above does not necessarily hold for other choices of a^. The above 
also holds for the FMF/i agent if {f'^) ^0. (/^) =0 if the agent can already 
exclude that yt is a better guess, so there is no reason to test it explicitly. 
Nothing has been said about the quality of the guesses, but for the FM/i 
agent they are optimal by definition. If K{fi) for the true distribution // is 
finite, we expect the FM^ agent to solve the 'exploration versus exploitation' 
problem in a universally optimal way, as £^ converges to /x. 
6.4.5 
Using the AI Models for Function Minimization 
The Al/i model can be used for function minimization in the following way: 
The output yk of cycle fc is a guess for a minimum of /, like in the FM model. 
The reward r^ should be high for small function values Zk = f{yk)- The reward 
should also be weighted with ak to reflect the same strategy as in the FM 
case. The choice oirk — —akZk is natural. Here, the feedback is not binary but 
Tfc G 7^ C ^ , with IZ being a countable subset of M^ e.g. the computable reals 
or all rational numbers. The feedback Ok should be the function value f{yk)-
So we set Ok = Zk- Note, that there is a redundancy if a^ is a computable 
function with no zeros, as rk = —akOk- So, for small K{aQ) like in the FMS 
model, one might set Xk = e. If we keep Ok the AI prior probability is 
(6.28) 
Inserting this into (6.4) with mk — m we get 
y^^ = argmaxV'...maxV'(r/,+ ...+r^)-/i^^(^ii:i...yA;^fc...?/m^m) 
Vk 
^ 
^ 
Vm 
^ 
^ 
Xk 
Xm 
= argminy]...miny](afc2:fc+.-.+Q^m^m)-/^^^(?/iii...2//c^/c---?/m^m) 
^ Vk^ 

6.4 Function Minimization (FM) 
203 
where y™ was defined in (6.26). The proof of equivalence was so simple 
because the FM model already has a rather general structure, which is similar 
to the full Alfi model. 
One might expect no problems when going from the already very general 
FM^ model to the universal AI^ model (with nik = m), but there is a pitfall 
in the case of the FMF model. All rewards r^ are zero in this case, except for 
the last one, which is r^- Although there is a feedback Zk in every cycle, the 
AI^ agent cannot learn from this feedback, as it is not told that in the final 
cycle Vm will equal to —Zm- There is no problem in the FM^ model because 
in this case this knowledge is hardcoded into ^^^. The AI^ model must first 
learn that it has to minimize a function, but it can only learn if there is 
a nontrivial credit assignment r^. FMF works for repeated minimization of 
(different) functions, such as minimizing N functions in N-m cycles. In this 
case there are A^ nontrivial feedbacks, and AI^ has time to learn that there is 
a relation between Vk-m 
and Ok-m every m/^ cycle. This situation is similar to 
the case of (repeated) strategic games discussed in Section 6.3. 
There is no problem in applying AI^ to FMS because the r feedback pro-
vides enough information in this case. The only thing the AI^ model has to 
learn is to ignore the o feedbacks since all information is already contained 
in r. Interestingly the same argument holds for the FME model if ^^(7) and 
K{m) are small? The AI^ model has additionally only to learn the relation 
rfc = —e^^^~'^^o/c. This task is simple, as every cycle provides one data point 
for a simple function to learn. This argument is no longer valid for 7 —^ 00 
since ^^(7) —^ oc in this case. 
6.4.6 
Remark on TSP 
The traveling salesman problem (TSP) seems to be trivial in the Al/i model 
but nontrivial in the AI^ model, because (6.26) just implements an internal 
complete search, as //(/) = SffTSP contains all necessary information. Al/i 
outputs, from the very beginning, the exact minimum of /^^^. This "solution" 
is, of course, unacceptable from a performance perspective. As long as we give 
no efficient approximation ^^ of ^, we have not contributed anything to a 
solution of the TSP by using AI^*^. The same is true for any other problem 
where / is computable and easily accessible. Therefore, TSP is not (yet) a 
good example because all we have done is to replace an NP complete problem 
with the uncomputable AI^ model or by a computable AI^^ model, for which 
we have said nothing about computation time yet. It is simply overkill to 
reduce "easy" problems to AI^. TSP is a simple problem in this respect, until 
we consider the AI^^ model seriously. For the other examples, where / is 
inaccessible or complicated, an AI^^ model would provide a true solution to 
the minimization problem, since an explicit definition of / is not needed for 
AI^ and AI^^. A computable version of AI^ will be defined in Section 7.2. 
Setting ak=e^^ 
we see that the condition on K{m) can be dropped. 

204 
6 Important Environmental Classes 
6.5 Supervised Learning from Examples (EX) 
The developed AI models provide a frame for reinforcement learning. The 
environment provides feedback r, informing the agent about the quality of its 
last (or earlier) output y; it assigns reward r to output y. In this sense, rein-
forcement learning is explicitly integrated into the Alp model. Al/i maximizes 
the true expected reward, whereas the AI^ model is a universal, environment-
independent reinforcement learning algorithm. 
There is another type of learning method: supervised learning by presen-
tation of examples (EX). Many problems learned by this method are asso-
ciation problems of the following type. Given some examples oeRcO^ 
the 
agent should reconstruct, from a partially given o\ the missing or corrupted 
parts, i.e. complete o' to o such that relation R contains o. In many cases, O 
consists of pairs (z,v), where v is the possibly missing part. 
6.5.1 
Applications/Examples 
Learning functions by presenting [zj{z)) 
pairs and asking for the function 
value of z by presenting (2:,?) falls into the category of supervised learning 
from examples, e.g. f{z) may be the class label or category of z. 
A basic example is learning properties of geometrical objects coded in some 
way. For instance, if there are 18 different objects characterized by their size 
(small or big), their colors (red, green, or blue) and their shapes (square, trian-
gle, or circle), then {object^property) GR if the object possesses the property. 
Here, i? is a relation that is not the graph of a single-valued function. 
When teaching a child by pointing to objects and saying "this is a 
tree" or "look how green" or "how beautiful", one establishes a relation of 
{object^property) pairs in R. Pointing to a (possibly different) tree later and 
asking "what is this ?" corresponds to a partially given pair (object,?), where 
the missing part "?" should be completed by the child saying "tree". 
A final example we want to give is chess. We have seen that, in principle, 
chess can be learned by reinforcement learning. In the extreme case the envi-
ronment only provides reward r = l when the agent wins. The learning rate 
is completely inacceptable from a practical point of view, due to the very low 
amount of information feedback. A more practical method of teaching chess 
is to present example games in the form of sensible {board-state,move) se-
quences. They contain information about legal and good moves (but without 
any explanation). After several games have been presented, the teacher could 
ask the agent to make its own move by presenting {board-state,?) and then 
evaluate the answer of the agent. 
6.5.2 
Supervised Learning with the AI/x/^ Model 
Let us define the EX model as follows: The environment presents inputs Ok-i = 
ZkVk = {zk.Vk) e RU{Z X {?}) C Z X {yu{?}) = O to the agent in cycle 
k-1. 

6.5 Supervised Learning from Examples (EX) 
205 
The agent is expected to output i/k in the next cycle, which is evaluated with 
rk = l if {zk^Vk) ^ R and 0 otherwise. To simplify the discussion, an output 
yk is expected and evaluated even when Vk{y^^) is given. To complete the 
description of the environment, the probability distribution iUi^(oi...Qn) of the 
examples and questions Oi (depending on R) has to be given. Wrong examples 
should not occur, i.e. fin should be 0 if Oi^RU{Zx{7}) 
for some l < i < n . 
The relations R might also be probability distributed with a{R). The example 
prior probability in this case is 
jJ^jOl.-.On) = y^/ii^(0l...Qn)-<^(E). 
(6.29) 
R 
The knowledge of the valuation r^ on output yk restricts the possible relations 
R^ consistent with R{zk^yk)='^k^ where R{z^y) :=1 if {z^y)ER and 0 otherwise. 
The prior probability for the input sequence Xi,.,Xn if the output sequence of 
AI/x is yi...yn^ is therefore 
li^\yixi...ynx^) 
= 
Yl 
m{oi'"On)'(^{R), 
R:Wl<i<n[R{zi,yi)=ri] 
where Xi = riOi and Oi-i = ziVi with vi G yu{?}. In the I/O sequence 
2/i^iiy2^2«--= 2/1^1 ^2^22/2^2^3'^3--- the yiTi are dummies, after which regular 
behavior starts with example (^2,'^2)-
The Pslji model is optimal by construction of ji^^. For computable prior 
fiR and (J, we expect a near-optimal behavior of the universal AI^ model if IIR 
additionally satisfies some separability property. In the following, we give some 
motivation why the AI^ model takes into account the supervisor information 
contained in the examples and why it learns faster than by reinforcement. 
We keep R fixed and assume /ii?(oi...On) —//i^(ol)•...•/ii^(on) 7^0<^Oi G 
R\j{Zx{l}) 
Vi to simplify the discussion. Short codes q contribute most 
to i^^{yix_i>..yn^n)' ^^ oi...On is distributed according to the computable 
probability distribution /i/^, a short code of Oi...On for large enough n is a 
Huffman code w.r.t. the distribution fin. So we expect fiR and hence R to be 
coded in the dominant contributions to ^^^ in some way, where the plausible 
assumption was made that the y on the input tape do not matter. Much more 
than one bit per cycle will usually be learned, i.e. relation R will be learned 
in n<^K{R) 
cycles by appropriate examples. This coding of i^ in g evolves 
independently of the feedbacks r. To maximize the feedback r/c, the agent has 
to learn to output a yk with {zk^yk)^R' 
The agent has to invent a program 
extension q' to q, which extracts Zk from Ok-i = {zk^^) and searches for and 
outputs a yk with [zk->yk) ER. As R is already coded in g, q' can reuse this 
coding of R in q. The size of the extension g' is, therefore, of order 1. To learn 
this q\ the agent requires feedback r with information content 0{l) = K{q^) 
only. 
Let us compare this with reinforcement learning, where only Ofc_i = (zfc,?) 
pairs are presented. A coding of i? in a short code q for oi...On is of no use 

206 
6 Important Environmental Classes 
and will therefore be absent. Only the rewards r force the agent to learn R. 
q' is therefore expected to be of size K{R). The information content in the 
r's must be of the order K{R). In practice, there are often only very few 
r/c = 1 at the beginning of the learning phase, and the information content in 
ri...rn is much less than n bits. The required number of cycles to learn R by 
reinforcement is, therefore, at least but in many cases much larger than K{R). 
Finally, consider a slightly easier setup, where after each prediction y, 
we inform the agent about the correct function value. This setup now coin-
cides with a (partial) sequential classification task, which can be reduced to 
sequence prediction, as described in Section 3.7.3. Hence, the results of Chap-
ter 3 apply, showing that the agent EX^, which is based on a mixture over 
fi of the restricted form (6.29), performs excellent. This is another indication 
that AI^ should also learn supervised well. 
Although AI^ was never designed or told to learn supervised, it learns 
how to take advantage of the examples from the supervisor. fiR and R are 
learned from the examples; the rewards r are not necessary for this process. 
The remaining task of learning how to learn supervised is then a simple task 
of complexity 0(1), for which the rewards r are necessary. 
6.6 Other Aspects of Intelligence 
In AI, a variety of general ideas and methods have been developed. In the last 
sections, we saw how several problem classes can be formulated within AI^. 
As we claim universality of the AI^ model, we want to illuminate which of and 
how the other AI methods are incorporated in the AI^ model by looking at 
its structure. Some methods are directly included, while others are or should 
be emergent. We do not claim the following list to be complete. 
Probability theory and utility theory are the heart of the Al/i/^ models. 
The probability ^ is a universal belief about the true environmental behavior 
/i. The utility function is the total expected reward, called value, which should 
be maximized. Maximization of an expected utility function in a probabilistic 
environment is usually called sequential decision theory, and is explicitly in-
tegrated in full generality in our model. In a sense this includes probabilistic 
(a generalization of deterministic) reasoning, where the objects of reasoning 
are not true and false statements, but the prediction of the environmental 
behavior. Reinforcement learning is explicitly built in, due to the rewards. 
Supervised learning is an emergent phenomenon (Section 6.5). Algorithmic 
information theory leads us to use ^ as a universal estimate for the prior 
probability /i. 
For horizon > 1, the expectimax series in (6.4) and the process of selecting 
maximal values may be interpreted as abstract planning. The expectimax 
series is a form of informed search, in the case of Al/i, and heuristic search, for 
AI^, where ( could be interpreted as a heuristic for /x. The minimax strategy of 
game playing in case of Alji is also subsumed. The AI^ model converges to the 

6.7 Problems 
207 
minimax strategy if the environment is a minimax player, but it can also take 
advantage of environmental players with limited rationality. Problem solving 
occurs (only) in the form of how to maximize the expected future reward. 
Knowledge is accumulated by AI^ and is stored in some form not specified 
further on the work tape. Any kind of information in any representation on 
the inputs y is exploited. The problem of knowledge engineering and repre-
sentation appears in the form of how to train the AI^ model. More practical 
aspects, like language or image processing^ have to be learned by AI^ from 
scratch. 
Other theories, like fuzzy logic, possibility theory, Dempster-Shafer theory, 
etc. are partly outdated and partly reducible to Bayesian probability theory 
[Che85, Che88]. The interpretation and consequences of the evidence gap g:= 
1—5^^ ^(?/r<fcj^jt) >0 in ^ may be similar to those in Dempster-Shafer theory. 
Boolean logical reasoning about the external world plays, at best, an emergent 
role in the AI^ model. 
Other methods that do not seem to be contained in the AI^ model might 
also be emergent phenomena. The AI^ model has to construct short codes of 
the environmental behavior, and the AI^*^ (see next chapter) has to construct 
short action programs. If we would analyze and interpret these programs for 
realistic environments, we might find some of the unmentioned or unused or 
new AI methods at work in these programs. This is, however, pure speculation 
at this point. More important: when trying to make AI^ practically usable, 
some other AI methods, like neural nets or genetic algorithms, especially for 
I/O pre/postprocessing, may be useful. 
The main thing we wanted to point out is that the AI^ model does not 
lack any important known property of intelligence or known AI methodology. 
What is missing, however, are computational aspects, which are addressed in 
the next chapter. 
6.7 Problems 
6.1 (Self-Optimization) [C35uo] Formally define the environmental classes 
A^EC for EC e {FMS, SGR, EX} similarly to EC G {SP, AI}. MEC shall 
be the class of all (lower-semi)computable environments consistent with the 
problem setup EC, ^^^ •—SI^GA^EC^"^^^'*^ ^^^ corresponding universal prior, 
and EC^ alias p^Q the Bayes optimal policy. Show that function minimization 
(FMS), repeated strategic games (SGR), and supervised learning (EX) admit 
self-optimizing policies (cf. Problem 5.10), hence FMS^, SGR^ and EX^ are 
self-optimizing. Interpret the results and compare them to the properties of 
SPC and AI^. 
6.2 (Prediction loss bounds for AI^) [C40oi] In Section 6.2.2 we derived 
a bound (6.17) exponential in K{z) on the number of prediction errors made 
by AI^ with horizon hk = I in deterministic passive environments. Try to 

208 
6 Important Environmental Classes 
generalize/improve this bound to (a) general loss functions, (6) bounds on 
E^^^ — E^^^ for probabilistic passive environments, (c) the case /ife > 1, {d) 
bounds linear or polynomial in K{z) as in the case of SP^ - or - find examples 
demonstrating the impossibility of such generalizations/improvements. 
6.3 (Posterization of prediction errors) [C20u/C40o] Show (,^^{Wi:n) 
>^^^Ui:n). where 
Zk — ^vkXk^ ^^^ that the other direction < is wrong. Use 
this result to "improve" the bound (6.17) to E^^^ <[^^^{z^,J]-K 
Posterialize 
this to a bound ^/^^ <^^^(2ife<fc)/^^^(^i:n) ^^ ^^^ number of errors in cycles 
k to n. Is it possible to improve the numerator to £,^^{t<:k) ^^^ ^^ bound the 
expression by ^2^^^'^--^^^<'^^ (cf. Problem 3.13)? 

Only math nerds would call 2^^^ finite — Leonid Levin 
The biggest difference between time and space is that 
you can't reuse time 
— Merrick Furst 
The only reason for time is so that everything doesn't 
happen at once 
— Albert Einstein 
You insist that there is something a machine cannot do. 
If you will tell me precisely what it is that a machine 
John von 
cannot do, then I can always make a machine which 
Neumann 
will do just that! 
— John von Neumann 
(1903-1957) 
7 Computational Aspects 
U p to now we have shown the universal character of the AIXI model but 
have completely ignored computational aspects, which we make up for in this 
chapter. 
We start in Section 7.1 by developing a general algorithm M^* that is capa-
ble of solving any well-defined problem p* as quickly as the fastest algorithm 

210 
7 Computational Aspects 
computing a solution to p*, save for a factor of l+£ and lower-order additive 
terms. M^* optimally distributes resources between the execution of prov-
ably correct p*-solving programs and an enumeration of all proofs, including 
relevant proofs of program correctness and of time bounds on program run-
times. The solution is somewhat involved from an implement at ional aspect. 
An implementation would include first-order logic, the definition of a univer-
sal Turing machine within it and proof theory. M^* avoids Blum's speedup 
theorem by ignoring programs without correctness proof. M^* has broader ap-
plicability and can be faster than Levin's universal search, the fastest method 
for inverting functions save for a large multiplicative constant. Kolmogorov 
complexity is extended in two natural ways to measure function complexity. 
One of them is used to show that the most efficient program computing some 
function / is also among the shortest programs provably computing /. 
Based on a similar idea, we construct in Section 7.2 a computable version 
of the AIXI model. Let us assume that there exists some algorithm p of size / 
with computation time per interaction cycle t, which behaves in a sufficiently 
intelligent way (this assumption is the very basis of AI). The algorithm p* 
should run all algorithms of length < Z for t time steps per cycle and select the 
best output among them. So we have an algorithm that runs in time 1-2^ and is 
at least as good as p^ i.e. it also serves our needs apart from the very large but 
constant multiplicative factor in computation time. This idea of the 'typing 
monkeys', one of them eventually producing 'Shakespeare', is well known and 
widely used in theoretical computer science. The difficult part here is the 
selection of the algorithm with the best output. A further complication is 
that the selection process itself must have only limited computation time. We 
present a suitable modification of the AIXI model that solves these difficult 
problems. The assumptions behind this construction are discussed at the end. 
The reduction of the factor 2^ to 1+e as for M^* is possible, but will not be 
presented here. 
7.1 The Fastest & Shortest Algorithm 
for All Well-Defined Problems 
7.1.1 Introduction &: Main Result 
Searching for fast algorithms to solve certain problems is a central and dif-
ficult task in computer science. Positive results usually come from explicit 
constructions of efficient algorithms for specific problem classes. A wide class 
of problems can be phrased in the following way. Given a formal specification 
of a problem depending on some parameter X G X , we are interested in a fast 
algorithm computing solution y ^Y. 
This means that we are interested in 
a fast algorithm computing / : X —> F, where / is a formal (logical, mathe-
matical, not necessarily algorithmic) specification of the problem. Ideally, we 

7.1 The Fastest k Shortest Algorithm for All Problems 
211 
would like to have the fastest algorithm, maybe apart from some small con-
stant factor in computation time. Unfortunately, Blum's speed-up theorem 
[Blu67, BluTl] shows that there are problems for which an (incomputable) se-
quence of speed-improving algorithms (of increasing size) exists, but no fastest 
algorithm. 
In the approach presented here, we consider only those algorithms that 
provably solve a given problem and have a fast (i.e. quickly computable) time 
bound. Neither the programs themselves nor the proofs need to be known 
in advance. Under these constraints we construct the asymptotically fastest 
algorithm save a factor of 1-he that solves any well-defined problem /. 
Known time bounds for practical problems can often be computed quickly, i.e. 
timet^{x)/timep{x) 
often converges very quickly to zero. Furthermore, from 
a practical point of view, the provability restrictions are often rather weak. 
Hence, we have constructed for all those problems a solution that is asymptot-
ically only a factor 1+e slower than the (provably) fastest algorithm! There is 
no large multiplicative factor and the problems are not restricted to inversion 
problems, as in Levin's algorithm (see Section 7.1.2). What somewhat spoils 
the practical applicabiUty of M^* is the large additive constant c^, which will 
be estimated in Section 7.1.6. 
An interesting and counter-intuitive consequence of Theorem 7.1, derived 
in Section 7.1.8, is that the fastest program that computes a certain function 
is also among the shortest programs that provably computes this function. 
Looking for larger programs saves at most a finite number of computation 
steps but cannot improve the time order. 
In Section 7.1.2 we review Levin search and the universal search algorithms 
SIMPLE and SEARCH, described in [LV97]. We point out that SIMPLE has the 
same asymptotic time complexity as SEARCH not only w.r.t. the problem in-
stance, but also w.r.t. to the problem class. In Section 7.1.3 we elucidate The-
orem 7.1 and the applicability to an example problem (matrix multiplication) 
unsolvable by Levin search. Section 7.1.4 discusses the general applicability of 
Mp*. In Section 7.1.5 we give formal definitions of the expressions time, proof, 

212 
7 Computational Aspects 
compute^ etc., which occur in Theorem 7.1, and define the fast algorithm 
Mp*. 
In Section 7.1.6 we analyze the algorithm M^*, especially its computa-
tion time, prove Theorem 7.1, and give upper bounds for the constants Cp and 
dp. Subtleties regarding the underlying machine model are briefly discussed 
in Section 7.1.7. In Section 7.1.8 we show that the fastest program computing 
a certain function is also among the shortest programs provably computing 
this function. For this purpose, we extend the definition of the Kolmogorov 
complexity of a string and define two new natural measures for the com-
plexity of functions and programs. Section 7.1.9 outlines generalizations of 
Theorem 7.1 to I/O streams and other time measures. Conclusions are given 
in Section 7.1.10. 
7.1.2 
Levin Search 
Levin search is one of the few rather general speed-up algorithms. Within 
a typically large factor, it is the fastest algorithm for inverting a function 
g:Y^X, 
a g can be evaluated quickly [Lev73b, Lev84]. Given x, an inver-
sion algorithm p tries to find a ?/ G F , called ^-witness for x, with 
g{y)—x. 
Levin search just runs and verifies the result of all algorithms p in parallel 
with relative computation time 2~^^^^; i.e. a time fraction 2~^^^^ is devoted to 
execute p, where i{p) is the length of program p (coded in binary). Verifica-
tion is necessary since the output of any program can be anything. This is the 
reason why Levin search is only effective if a fast implementation of g is avail-
able. Levin search halts if the first ^f-witness has been produced and verified. 
The total computation time to find a solution (if one exists) is bounded by 
2^(p).^^772e+(x), where time^{x) 
is the runtime oip{x) 
plus the time to verify 
the correctness of the result {g{p{x)) = x) by a known implementation for g. 
Li and Vitanyi [LV97, p503] propose a very simple variant, called 
SIMPLE(^), which runs all programs PiP2P3--- one step at a time according to 
the following scheme: pi is run every second step, p2 every second step in the 
remaining unused steps, ps every second step in the remaining unused steps, 
and so forth, i.e. according to the sequence of indices 121312141213121512.... 
lipk 
inverts ^ on x in timep^{x) 
steps, then SIMPLE(^) will do the same in at 
most 2^time^^{x)-\-2^~^ 
steps. In order to improve the factor 2^, they define 
the algorithm SEARCH(^), which runs all p (of length less than i) for 2*2~^^^^ 
steps in phase i = 1,2,3,..., until it has inverted g on x. The computation time 
of SEARCH(^) is bounded by 2^^^^+^^^^^ime+^(x), where 
K{k)<(.{pk)<2\og2k 
is the Kolmogorov complexity of k (Definition 2.9). They suggest that SIMPLE 
has worse asymptotic behavior w.r.t. k than SEARCH, but actually this is not 
the case. 
In fact, SIMPLE and SEARCH have the same asymptotics also in k, be-
cause SEARCH itself is an algorithm with some index /^SEARCH = <^(1)- Hence, 
SIMPLE executes SEARCH every 2^^^^^^^" steps, and can at most be a constant 
(independent of k and x) factor 2^^^^^^™ ==0(1) slower than SEARCH. However, 

7.1 The Fastest & Shortest Algorithm for All Problems 
213 
in practice, SEARCH should be favored, because constants also matter, and 
2fcsEAHCH;:^22'^'''^^""^"^ is rathcr large. 
Levin search can be modified to handle time-limited optimization problems 
as well [S0I86]. Many, but not all, problems are of inversion or optimization 
type. The matrix multiphcation example (Section 7.1.3), the decision prob-
lem SAT [LV97, p503], and reinforcement learning (Section 7.2), for instance, 
are not of this form. Furthermore, the large factor 2^^^^ somewhat hmits the 
applicability of Levin search. 
Levin search in program space cannot be used directly in M^* for comput-
ing p* because we have to decide somehow whether a certain program solves 
our problem or computes something else. For this, we have to search through 
the space of proofs. In order to avoid the large time factor 2^^^\ we also have 
to search through the space of time-bounds. Only one (fast) program should 
be executed for a significant time interval. The algorithm M^* essentially con-
sists of three interwoven algorithms: sequential program execution, sequential 
search through proof space, and Levin search through time-bound space. A 
tricky scheduling prevents performance degradation from computing slow p's 
before the p has been found. 
7.1.3 Fast Matrix Multiplication 
To illustrate Theorem 7.1, we consider the problem of multiplying two n x 
n matrices. If p* is the standard algorithm for multiplying two matrices^ 
X G R^"^ X R^"^ of size i{x) oc n^ over some ring i?, then tp* (x):— 2n^ upper 
bounds the true computation time timep* (a:)=n^(2n—1). We know there exist 
algorithms p' for matrix multiplication with timep> [x) <tp' {x):=C'n'^ {uj = 2.Sl 
[Str69], uj = 2.50 [CW82], cj = 2.38 [CW90], ...). The time-bound function (cast 
to an integer) can, as in many cases, be computed very quickly, timet , (x) = 
O(log^n). Hence, using Theorem 7.1, also M^* is fast, 
timeM%{x)<{l-^e)c-
n^ + 0(log^n). Of course, Mp* would be of no real use if p^ is already the 
fastest program, since p' is known and could be used directly. We do not 
know however, whether there is an algorithm ^ with timep"[x) <d-n^logn, 
for instance. But if it does exist, timcM^^ (x) < {l+e)d-n'^\ogn-\-0{l) 
for all x 
is guaranteed. 
There is no contradiction to the result [CW82] that there is no fastest 
bilinear A-algorithm (bAA) for matrix multiplication. For every bAA pi with 
computation time n^^ one can find another bAA p^+i with computation time 
n^^+i and c«;^+i < a;^, but there is no bAA with computation time n"^^ and 
LJo = iiifi{LJi}. On the other hand, this says nothing about the existence of a 
non-bAA M with computation time of, for instance, n^^logn, which is faster 
than all bAA pi. Indeed, a formal construction of such an algorithm is easy. The 
^ Instead of interpreting R as the set of real numbers one might take the field 
]F2 = {0,1} to avoid subtleties arising from large numbers. Arithmetic operations 
are assumed to need one unit of time. 

214 
7 Computational Aspects 
sequence {pi,P2,P3v} is enumerable, i.e. there is an algorithm that creates 
the programs pi,p2JPSJ---, say in time n,r2,T3,.... We enumerate pi,p2,P3,• • • and 
start executing them in parallel as soon as they have been constructed and 
assign a fraction -^r^p^ of time to Pi{x). The first pi that halts outputs the 
result. The total computation time of this (meta)algorithm M is 
timeM{x) 
— min{r^ + i(z + l)-tzmep.(x)} = 
0{timep^{x))\/i. 
Hence M has better time complexity than any of the pi. For instance, for 
uoi —(jjQ^O{i~'^) and TI polynomial in i, it is easy to see that timeM{x) = 
0(n^°logn). The construction above works in general as long as the program 
sequence is enumerable. It fails for incomputable sequences, like in Blum's 
speed-up construction. 
The matrix multiplication example was chosen for specific reasons. First, it 
is not an inversion or optimization problem directly suitable for Levin search. 
The computation time of Levin search is lower-bounded by the time to verify 
the solution with a known algorithm (which is currently c-'n?-^^^') multiplied 
with the (large) number of necessary verifications. Second, although matrix 
multiplication is a very important and time-consuming issue, p' is not used 
in practice, since c is so large that for all practically occurring n, the cubic 
algorithm is faster. The same is true for Cp and dp, but we must admit that 
although c is large, the bounds we obtain for Cp and dp are tremendous. On the 
other hand, even Levin search, which has a tremendous multiplicative factor, 
can be successfully applied [Sch97, SZW97, Sch03a, Sch04], when handled 
with care. The same should hold for Theorem 7.1, as will be discussed. We 
avoid the 0() notation as far as possible, as it can be severely misleading (e.g. 
10^^ = 0(1)^*^^^ =0(1)). This chapter could be viewed as another 0() warning 
showing how important factors and even subdominant additive terms are. 
7.1.4 
Applicability of the Fast Algorithm M^* 
An obvious time bound for p is the actual computation time itself. An obvi-
ous algorithm to compute timep{x) is to count the number of steps needed 
for computing p{x). Hence, inserting tp — timep into Theorem 7.1 and using 
timetimepi^) <timep(x)^ we see that the computation time of M^* is optimal 
within a multiplicative constant (dp + l-hs) and an additive constant Cp. This 
result is weaker than the one in Theorem 7.1, but no assumption concerning 
the computability of time bounds has to be made. 
When do we trust that a fast algorithm solves a given problem? At least for 
well-specified problems, like satisfiability, solving a combinatoric puzzle, com-
puting the digits of TT, ..., we usually invent algorithms, prove that they solve 
the problem and, in many cases, also can prove good and quickly computable 
time bounds. In these cases, the provability assumptions in Theorem 7.1 are 
no real restriction. The same holds for approximate algorithms that guarantee 
a precision e within a known time bound (many numerical algorithms are of 

7.1 The Fastest & Shortest Algorithm for All Problems 
215 
this kind). For exact or approximate programs provably computing or con-
verging to the right answer (e.g. travehng salesman problem, and also many 
numerical programs), but for which no good and easy to compute time bound 
exists, Mp^ is only optimal apart from a huge constant factor l-\-e + dp in 
time, as discussed above. Universal reinforcement learning could be a prob-
lem of this kind. There is no known efficient algorithm for computing the 
optimal policy for sequential decision problems in non-Markov environments. 
The algorithm AlXItl developed in Section 7.2 is based on a similar idea as 
the Mp*. It creates an incremental policy for an agent in an unknown general 
(non-Markov) environment, which is superior to any other time t and length 
I bounded agent. The computation time of AlXltl is of the order t'2K For 
poorly specified problems, Theorem 7.1 does not help at all. 
7.1.5 
The Fast Algorithm M^. 
One ingredient of algorithm M^* is an enumeration of proofs of increasing 
length in some formal axiomatic system. If a proof actually proves that some 
p is functionally equivalent p*, and p has time bound tp, then {p,tp) is added 
to a list L. The program p m L with the currently smallest time bound tp{x) 
is executed. By construction, the result p{x) is identical to p*{x). The trick 
to achieve the time bound stated in Theorem 7.1 is to schedule everything in 
a proper way, in order not to lose too much performance by computing slow 
p's and tp''s before the p has been found. 
To avoid confusion, we formally define p and tp to be binary strings. That 
is, p is neither a program nor a function, but can be informally interpreted as 
such. A formal definition of the interpretations of p is given below. We say "p 
computes function f", when a universal reference Turing machine U on input 
p and X computes f{x) for all x. This is denoted by U{p^x) = f{x), To be able 
to talk about proofs, we need a formal logic system (V,A,?/i,Q,/i,i^^,—>,A,=,...), 
and axioms and inference rules. A proof is a sequence of formulas, where each 
formula is either an axiom or inferred from previous formulas in the sequence 
by applying the inference rules. See [Fit96, Sho67] or any other textbook on 
logic or proof theory. We only need to know that provability, Turing machines, 
and computation time can be formalized: 
1. The set of correct proofs is enumerable. 
2. A term u can be defined such that the formula [\/y:u{p,y) =u{p*,y)] is 
true if and only if U{p,x) = U{p* ,x) for all x, i.e. if p and p* describe the 
same function. 
3. A term tm can be defined such that the formula [tm,{p,x) = n] is true if 
and only if the computation time of U on (p,x) is n, i.e. if n = timep{x). 
We say that p is provably equivalent to p* if the formula \\/y:u{p,y) = u{p*,y)] 
can be proven. M^* runs three algorithms A, B, and C in parallel: 

216 
7 Computational Aspects 
Algorithm M^* (x) 
Initialize the shared variables L := {}, 
tfast := oo, 
Pfast :=p* • 
Start algorithms A, J5, and C in parallel with relative computational 
resources 5, 5, and 1 —2e, respectively. 
That is, C performs about ^ steps when A and B perform one step each. 
Algorithm A 
for z:=l,2,3,... do 
pick the i*^ proof in the list of all proofs and 
isolate the last formula in the proof. 
if this formula is equal to \\/y:u{p,y) = 
u{p*,y)Au{t,y)>tm{p,y)] 
for some strings p and t, 
then add (p,t) to L. 
next i 
Algorithm B 
for all 
{p,t)eL 
run U on all (t,x) in parallel for all t with relative computational resources 
if U halts for some t and 
U{t^x)<tfast, 
then ^/ast :=U{t,x) and p/ast •=P and restart algorithm C. 
continue (p,t) 
Algorithm C 
run t/ on (pfast.x). For each executed step decrease tfast by 1. 
if [/ halts then print result U{pfastix) and abort computation of A, B 
and C 
Note that A and 5 only terminate when aborted by C. The discussion of 
the algorithms in the following subsections clarifies details and proves Theo-
rem 7.1. 
7.1.6 Time Analysis 
Henceforth we return to the convenient abbreviations p{x) := U{p^x) and 
tp{x):=U{tp,x). 
Let p^ be some fixed algorithm that is provably equivalent to 
p*, with computation time timep' provably bounded by tpf. Let i{proof{p')) 
be the length of the binary coding of the, for instance, shortest proof. 
Here, computation time refers to true overall computation time, whereas 
computation steps refer to instruction steps. Hence steps = a-time^ 
if a 
percentage a of computation time is assigned to an algorithm. 
A) To write down (not to invent!) a proof requires 0{i(proof)) 
steps. A 
time 0{Naxiom'^{Fi)) 
is needed to check whether a formula Fi in the proof 
FiF2---Fn is an axiom, where Naxiom is the number of axioms or axiom 
schemes, which is finite. Variable substitution (binding) can be performed 

7.1 The Fastest k Shortest Algorithm for All Problems 
217 
in linear time. For a suitable finite set of axiom schemes, the only necessary 
inference rule is modus ponens. If Fi is not an axiom, one searches for a 
formula Fj, j<i oi the form Fk—>Fi and then for the formula F^, k<i. This 
takes time 0{£{proof)). There are n<0{i{proof)) 
formulas Fi to check in 
this way. Whether the sequence of formulas constitutes a valid proof can, 
hence, be checked in 0{i{proof)'^) steps. There are less than 2^+^ proofs of 
(binary) length </. Algorithm A receives a fraction e of relative computation 
time. Hence, for a proof of {p\tp') to occur, and for {p\tp') to be added to L, 
at most time TA < |-2^^^^^^-^(^'))+i.O(^(proo/(p'))^) is needed. Note that the 
same program p can and will be accompanied by different time bounds tp; 
for instance (jp^timep) will occur. 
B) The time assignment of algorithm B to the t^'s only works if the Kraft 
inequality E(p,i^)GL2"'^^^^"^^*^^ < 1 is satisfied [Kra49]. This can be ensured 
by using prefix-free (e.g. Shannon-Fano) codes [Sha48, LV97]. The number of 
steps to calculate tp'{x) is, by definition, timet , (^)- The relative computation 
time available for computing tp' {x) is £-2"^^^ )-^(*pO _ Hence, tpi (x) is computed 
and tfast<tp'{x) 
is checked after time TB <TA-\-1'2^^P'^^^^^P'^ 
'timet^,{x). 
We have to add T^, since B has to wait, in the worst case, time TA before it 
can start executing tp'{x). 
C) If algorithm C halts, its construction guarantees that the output is 
correct. In the following, we show that C always halts, and give a bound for 
the computation time. 
{i) Assume that algorithm C stops before B performed the check tp/ {x) < 
ifast, because a different p already computed p[x). In this case 
TC<TB-
(ii) Assume that B performs the check tp>{x)<tfast and the check succeeds. 
Runtime TB has passed until this point. C is restarted and computes 
Pfast{x)=P^{x) 
in time tfast '='^p^ or faster, if during the computation, 
Pfast gets replaced by an even faster algorithm constructed by A and B 
{tfast is a decreasing variable). Since a fraction 1 — 2£ of relative compu-
tation time is assigned to C, it halts after time Tc<TB 
+ jz2^^p'i^)-
(Hi) At any point in time the remaining time until C halts is bounded by 
j^ifast, 
since tfast is never increasing. Hence, if the check 
tp'{x)<tfast 
fails, Tc<TB + 
j^tfast<TB-^YZ2^tp'{x). 
The maximum of the cases (i) to [iii) bounds the computation time of C and, 
hence, of M^* by 
timeM%{x) = Tc < TB + j^tp{x) 
< {I + 3e)'tp{x) ^ ^'timet^{x)-^ 
^ 
3 e ' 
dp = 3.2^^^)+^^*-), 
cp = 3.2^(^'^-^(^))+i.O(^(proo/(p)2), 
where we have dropped the prime from p and used j
^ < l+Se: for £< ^. We 
have also suppressed the dependency of Cp and dp on p* {proof {p) depends 

218 
7 Computational Aspects 
on p* too), since we considered p* to be a fixed given algorithm. Rescaling 
6-^5/3 leads to the bound in Theorem 7.1. 
7.1.7 Assumptions on the Machine Model 
In the time analysis above we have assumed that program simulation with 
abort possibility and scheduling parallel algorithms can be performed in re-
altime, i.e. without loss of performance. Parallel computation can be avoided 
by sequentially performing time slices of N operations and then switching 
to the next task. Algorithms A and C, and every (p^t) e L in algorithm B 
constitute a task. If switching between time slices needs constant time s and 
we choose N ^ j ^ , then time slicing increases computation time by a factor 
l-\-e. Also, in order to avoid a possible slowdown of p in algorithm C caused 
by decrementing tfast^ one should decrement tfast by N every N^^ time step, 
possibly synchronously to the task switching. Counting can be performed in 
time 0(1) [SV88]. 
A thorough construction of a realtime machine U goes beyond the scope of 
this book. The above discussion should be a motivation that universal realtime 
machines U are something reasonable. Note that we use the same universal 
Turing machine U with the same underlying Turing machine model (number of 
heads, symbols, ...) for measuring computation time of all programs (strings) 
p, including M^*. This prevents us from applying the linear speedup theorem 
(which is cheating somewhat anyway), but allows the possibility of designing a 
U that allows realtime simulation with abort possibility. Theorem 7.1 should 
also hold for suitable Kolmogorov-Uspenskii [KU63] and Pointer machines 
[SchSO]. 
7.1.8 
Algorithmic Complexity and the Shortest Algorithm 
Data compression is a very important issue in computer science. Saving space 
or channel capacity are obvious applications. In Chapter 2 we saw that a 
less obvious, but not far-fetched, application is that of inductive inference 
in various forms (hypothesis testing, forecasting, classification, ...). A free 
interpretation of Occam's razor is that the shortest theory consistent with 
past data is the most likely to be correct. This was put into a rigorous scheme 
by [Sol64] and proved to be optimal in Chapter 3. Kolmogorov complexity is a 
universal notion of the information content of a string [Kol65, Cha66, ZL70]. 
It is defined as the length of the shortest program computing string x: 
Ku{x) 
:= mm{i{p):Uip)=x} 
= K{x) + 0{1), 
p 
where U is some universal Turing machine. It can be shown that Ku{x) varies, 
at most, by an additive constant independent of x by varying the machine U. 
Hence, the Kolmogorov complexity K{x) is universal in the sense that it is 
uniquely defined up to an additive constant. K{x) can be approximated from 

7.1 The Fastest & Shortest Algorithm for All Problems 
219 
above (is co-enumerable), but is not finitely computable. Refer to Chapter 2 
for details on Kolmogorov complexity and its application to prediction. 
Recently, Schmidhuber [SchOO, Sch02a] has generalized Kolmogorov com-
plexity in various ways to the limits of computability and beyond. In the 
following, we also need a generalization, but of a different kind. We need a 
short description of a function, rather than of a string. The following definition 
of the complexity of a function /, 
K\f) 
:= mm{e{p) : U{p,x) = f{x)\/x} 
p 
seems natural, but suffers from not even being approximable (see Defini-
tion 2.12). There exists no algorithm converging to K\f)^ 
because it is in 
general undecidable whether a program p is equivalent to (some formal def-
inition of) a function /. Even if we have a program p* computing /, 
K\p*) 
is not approximable. Using i^(p*) is not a suitable alternative, since K{p*) 
might be considerably larger than K^{p'^), as in the former case all informa-
tion conveyed by p* will be kept, even that which is functionally irrelevant 
(e.g. dead code). An alternative is to restrict ourselves to provably equivalent 
programs. The length of the shortest one is 
K'\p*) 
:— unii{i{p) : a proof of [\fy:u{p,y) = n(p*,y)] exists}. 
p 
It can be approximated from above, since the set of all programs provably 
equivalent to p* is enumerable. 
Having obtained, after some time, a very short description p' of p* for 
some purpose (e.g. for defining a prior probability for some inductive inference 
scheme), it is usually also necessary to obtain values for some arguments. We 
are now concerned with the computation time of p'. Could we get slower and 
slower algorithms by compressing p* more and more? Interestingly, this is 
not the case. Inventing complex (long) programs is not necessary to construct 
asymptotically fast algorithms, under the stated provabihty assumptions, in 
contrast to Blum's theorem [Blu67, Blu71]. The following theorem roughly 
says that there is a single program that is the fastest and the shortest program. 

220 
7 Computational Aspects 
To prove the theorem, we just insert the shortest algorithm ip' provably equiv-
alent to p* into M, that is, p:=M^,. As only 0(1) instructions are needed to 
build Mp fromy, M^, has size i{v')-^0(X)^K"{f)^0{\). 
The computation 
time of Mp, is the same as of M^* apart from "slightly" different constants Cp 
and d^. 
The following subtlety was pointed out by Peter van Emde Boas. Neither 
Mp* nor p is provably equivalent to p*. The construction of M^* in Section 7.1.5 
shows equivalence of M^* (and of p) to p*, but it is a meta-proof that can-
not be formalized within the considered proof system. A formal proof of the 
correctness of M^* would prove the consistency of the proof system, which 
is impossible by Godel's second incompleteness theorem [God31]. See [Har79] 
for details in a related context. 
7.1.9 
Generalizations 
If p* has to be evaluated repeatedly, algorithm A can be modified to remember 
its current state and continue operation for the next input {A is independent 
of x!). The large offset time Cp is only needed on the first invocation. 
Mp* can be modified to handle I/O streams, definable by a Turing ma-
chine with monotone input and output tapes (and bidirectional work tapes) 
receiving an input stream and producing an output stream. The currently 
read prefix of the input stream is x. timep{x) is the time used for reading 
X. Mp* caches the input and output streams, so that algorithm C can re-
peatedly read/write the streams for each new p. The true input/output tapes 
are used for requesting/producing a new symbol. Algorithm B is reset after 
1,2,4,8,... steps (not after reading the next symbol of x!) to appropriately take 
into account increased prefixes x. Algorithm A just continues. The bound of 
Theorem 7.1 holds for this case too, with slightly increased dp. 
The construction above also works if time is defined as a function of the 
current output rather than the current input x. This measure is, for example, 
used for the time-complexity of calculating the n*^ digit of a computable real 
(e.g. TT), where there is no input, but only an output stream. 
7.1.10 
Summary & Outlook 
We presented an algorithm ML that accelerates the computation of a pro-
gram p*. Mp* combines (A) sequential search through proof space, (B) Levin 
search through time-bound space, and (C) sequential program execution, using 
a somewhat tricky scheduling. Under certain provability constraints, M^* is 
the asymptotically fastest algorithm for computing p* apart from a factor l~{-e 
in computation time. Blum's theorem shows that the provability constraints 
are essential. We showed that the conditions on Theorem 7.1 are often, but 
not always, satisfied for practical problems. For complex approximation prob-
lems, for instance, where no good and quickly computable time bound exists, 
Mp^ is still optimal, but in this case, only apart from a large multiplicative 

7.2 Time-Bounded AIXI Model 
221 
factor. We briefly outlined how M^* can be modified to handle I/O streams 
and other time measures. An interesting and counterintuitive consequence of 
Theorem 7.1 was that the fastest program computing a certain function is 
also among the shortest programs provably computing this function. Look-
ing for larger programs saves at most a finite number of computation steps, 
but cannot improve the time order. To quantify this statement, we extended 
the definition of Kolmogorov complexity and defined two new natural mea-
sures for the complexity of a function. The large constants Cp and dp seem 
to spoil a direct implementation of M S . On the other hand, Levin search 
has been successfully adapted/generalized and applied to solve rather diffi-
cult machine learning problems [Sch97, SZW97, Sch03a, Sch04], even though 
it suff'ers from a large multiplicative factor of similar origin. The use of more 
elaborate theorem provers, rather than brute-force enumeration of all proofs, 
could lead to smaller constants and bring M^ closer to practical applications, 
possibly restricted to subclasses of problems [RVOl]. A more fascinating (and 
more speculative) way may be the utilization of so-called transparent or holo-
graphic proofs [BFLS91]. The correctness of these proofs can be checked by 
only reading a logarithmic number of their bits. This would mean that expo-
nentially many proofs are checked simultaneously, reducing the constants Cp 
and dp to their logarithm. I would like to conclude with a general question. 
Will the ultimate search for asymptotically fastest programs typically lead to 
fast or slow programs for arguments of practical size? Levin search, matrix 
multiplication and the algorithm M^* seem to support the latter, but this 
might be due to our inability to do better. 
7.2 Time-Bounded AIXI Model 
7.2.1 
Introduction 
Until now, we have not bothered with the non-computability of the universal 
probability distribution ^:=<^^. As all universal models in this book are based 
on ^, they are not effective in this form. In this section, we outline how the 
previous models and results can be modified/generalized to the time-bounded 
case. Indeed, the situation is not as bad as it could be. <^ is enumerable and yk is 
still approximable, i.e. there exists an algorithm that will produce a sequence 
of outputs eventually converging to the exact output t/fc, but we can never 
be sure whether we have already reached it. Besides this, the convergence 
is extremely slow, so this type of asymptotic computability is of no direct 
(practical) use, but will nevertheless be important later. 
Let p be a program that calculates within a reasonable time t per cycle 
a reasonable intelligent output, i.e. p{x<^k) = Vi-.k- This sort of computability 
assumption, that a general-purpose computer of sufficient power is able to 
behave in an intelligent way, is the very basis of AI, justifying the hope to 
be able to construct agents which eventually reach and outperform human 

222 
7 Computational Aspects 
intelligence. For a contrary viewpoint see [LucGl, Pen89, Pen94]. It is not 
necessary to discuss here what is meant by 'reasonable time/intelligence' and 
'sufficient power'. What we are interested in, in this section, is whether there is 
a computable version AlXIt of the AIXI agent that is superior or equal to any 
p with computation time per cycle of at most t. By 'superior', we mean 'more 
intelligent', so what we need is an order relation, like (5.14) for intelligence. 
The best result we could think of would be an AlXlf with computation 
time < t at least as intelligent as any p with computation time < t. If AI 
is possible at all, we would have reached the final goal, the construction of 
the most intelligent algorithm with computation time <t. Just as there is no 
universal measure in the set of computable measures (within time t), neither 
may such an AlXlt exist. 
What we can realistically hope to construct is an AlXIt agent of computa-
tion time C'i per cycle for some constant c. The idea is to run all programs p 
of length <l:=i{p) and time <t per cycle and pick the best output. The total 
computation time is c-i with c—2K This sort of idea of 'typing monkeys' with 
one of them eventually writing Shakespeare, has been applied in various forms 
and contexts in theoretical computer science. The realization of this best vote 
idea, in our case, is not straightforward and will be outlined in this section. A 
related idea is that of basing the decision on the majority of algorithms. This 
'democratic vote' idea was used in [LW94, Vov92] for sequence prediction and 
is referred to as 'weighted majority' (see Section 3.7.4). 
7.2.2 Time-Limited Probability Distributions 
In the literature one can find time-limited versions of Kolmogorov complexity 
[Dal73, Dal77, Ko86] and time-limited universal semimeasures [LV91, LV97, 
Sch02b]. In the following, we utilize and adapt the latter and see how far we 
get. One way to define a time-limited universal chronological semimeasure is 
similar to the unbounded case (5.5) as a mixture, but restricted to enumerable 
chronological semimeasures computable within time t and of size at most /. 
ehmun) 
••= 
E 
2-^(^)p(2^i,J. 
(7.3) 
P •• ^{p)<l ^ 
t{p)<i 
Let us assume that the true environmental prior probability /i^^ is equal to 
or sufficiently accurately approximated by a p with £{p) <l and t{p) <i with 
i and / of reasonable size. There are several AI problems that fall into this 
class. In function minimization of Section 6.4, the computation of / and /i^^ 
are often feasible. In many cases, the sequences of Section 6.2 that should be 
predicted can be easily calculated when //^^ is known. In a classifier problem, 
the probability distribution /i^^, according to which examples are presented, 
is, in many cases, also elementary. But not all AI problems are of this "easy" 
type. For the strategic games of Section 6.3, the environment itself is usually 
a highly complex strategic player with a /x^^ that is difficult to calculate. 

7.2 Time-Bounded AIXI Model 
223 
although one might argue that the environmental player may have limited 
capabilities too. But it is easy to think of a difficult-to-calculate physical 
(probabilistic) environment like the chemistry of biomolecules. 
The number of interesting applications makes this restricted class of AI 
problems, with time- and space-bounded environment /x*^, worthy of study. 
Superscripts to a probability distribution except for ^*^ indicate their length 
and maximal computation time. ^*^ defined in (7.3), with a yet to be de-
termined computation time, multiplicatively dominates all /x*^ of this type. 
Hence, an AI^*^ model, where we use ^*^ as prior probability, is universal rel-
ative to all Al/i*^ models in the same way as AIXI is universal to AI/x for all 
enumerable chronological semimeasures /x. The argmaxy^ in (5.3) selects a yk 
for which ^^^ has the highest expected utility Vkmk •> where ^*^ is the weighted 
average over the p^^\ output ^^ ^ is determined by a weighted majority. We 
expect AI^*^ to outperform all (bounded) AIp*^, analogous to the unrestricted 
case. 
In the following we analyze the computability properties of £}^ and AI^*^, 
i.e. oi y^^ 
. To compute ^*^ according to the definition (7.3) we have to 
enumerate all chronological enumerable semimeasures p*^ of length < / and 
computation time < t. This can be done similarly to the unbounded case 
(5.42)-(5.44). All 2^ enumerable functions of length </, computable within 
time i have to be converted to chronological probability distributions. For 
this, one has to evaluate each function for \X\'k diff"erent arguments. Hence, 
^*^ is computable within time^ t(^*^(t^i.;.)) = 0(|A'|-A:-2^-f). The computation 
time of y^ ^ depends on the size of A', ^ and mjt. ^ has to be evaluated 
|j;|/ife |;^|/^fc times in (5.3). It is possible to optimize the algorithm and perform 
the computation within time 
t{y^^^'') = 0{\y\^^\X\^^-2^'t) 
(7.4) 
per cycle. If we assume that the computation time of /i*^ is exactly t for all 
arguments, the brute-force time f for calculating the sums and maxs in (4.17) 
is i{y^^^'')> l^l^^l^'l^'^ 'I Combining this with (7.4), we get 
This result has the proposed structure that there is a universal AI^*^ agent 
with computation time 2^ times the computation time of a special Al//^ agent. 
Unfortunately, the class of AI/x*' systems with brute-force evaluation of 
^fc, based on (4.17) is completely uninteresting from a practical point of view. 
For example in the context of chess, the above result says that the AI^*^ is 
We assume that a (Turing) machine can be simulated by another in linear time. 

224 
7 Computational Aspects 
superior within tirae 2^'t to any brute-force minimax strategy of computation 
time i. Even if the factor of 2^ in computation time would not matter, the 
AI^*^ agent is nevertheless practically useless, as a brute-force minimax chess 
player with reasonable time t is a very poor player. 
Note that in the case of binary sequence prediction {hk — 1, |3^| = lA'l =2) 
the computation time of p coincides with that of ^^ ^ within a factor of 2. 
The class Alp^^ includes all non-incremental sequence prediction algorithms 
of size < / and computation time < t/2. By non-incremental, we mean that 
no information of previous cycles is taken into account for speeding up the 
computation of ijk of the current cycle. 
The shortcomings (mentioned and unmentioned ones) of this approach are 
cured in the next subsections, by deviating from the standard way of defining 
a time-bounded ^ as a sum over functions or programs. 
7.2.3 The Idea of the Best Vote Algorithm 
A general agent is a chronological program p{x<^k) =yi:k- This form, intro-
duced in Section 4.1, is general enough to include any AI system (and also 
less intelligent systems). In the following, we are interested in programs p of 
length < / and computation time < i per cycle. One important point in the 
time-limited setting is that p should be incremental, i.e. when computing yk in 
cycle k, the information of the previous cycles stored on the work tape can be 
reused. Indeed, there is probably no practically interesting, non-incremental 
AI system at all. 
In the following, we construct a policy p*, or more precisely, policies p^, 
for every cycle k that outperform all time- and length-limited AI systems p. In 
cycle k, p^ runs all 2^ programs p and selects the one with the best output i/k. 
This is a 'best vote' type of algorithm, as compared to the 'weighted majority' 
type algorithm of the last subsection. The ideal measure for the quality of the 
output would be the ^-expected future reward 
Vktiifk<k) 
••= E 
'^~'^'\Z^ 
VkZ ••= r{xl'') + ... + r{x^J). 
(7.5) 
qeQk 
The program p that maximizes V^^ 
should be selected. We have dropped 
the normalization J\f unlike in (5.13), as it is independent of p and does not 
change the order relation in which we are solely interested here. Furthermore, 
without normalization, F^*J^(^<fc) :=maXp^pVj^^(yi:<A;) is enumerable, which 
will be important later. 
7.2.4 Extended Chronological Programs 
In the functional form of the AIXI model it was convenient to maximize Vkmk 
over all pG P/e, i.e. all p consistent with the current history yjc^^k- This was not 

7.2 Time-Bounded AIXI Model 
225 
a restriction, because for every possibly inconsistent program p there exists 
a program p' G Pk consistent with the current history and identical to p for 
all future cycles > k. For the time-limited best vote algorithm p* it would be 
too restrictive to demand pePk. 
To prove universality, one has to compare 
all 2^ algorithms in every cycle, not just the consistent ones. An inconsistent 
algorithm may become the best one in later cycles. For inconsistent programs 
we have to include the yk into the input, i.e. p{ip:^k) = Vi-k with yi ^ yf 
possible. For p£Pk this was not necessary, as p knows the output i/k = y^ in 
this case. The rf^ in the definition of Vkm are the rewards emerging in the 
I/O sequence, starting with ^<fc (emerging from p*) and then continued by 
applying p and q with y^ :=yf for i > A:. 
Another problem is that we need Vkmk ^^ select the best policy, but un-
fortunately Vknik is uncomputable. Indeed, the structure of the definition of 
ykruk is very similar to that of yk, hence a brute-force approach to approximate 
ykruk requires too much computation time as for y^. We solve this problem in 
a similar way, by supplementing each p with a program that estimates Vkmk 
by w^ within time i. We combine the calculation of y^ and w^ and extend the 
notion of a chronological program once again to 
P{yk<k) = wlyl,.,wlyl 
(7.6) 
with chronological order 
w\y{yiXiW2y^y2X2"" 
7.2.5 Valid Approximations 
Policy p might suggest any output y^, but it is not allowed to rate it with an 
arbitrarily high w^ if we want w^ to be a reliable criterion for selecting the 
best p. We demand that no policy is allowed to claim that it is better than it 
actually is. We define a (logical) predicate VA(p) called valid approximation^ 
which is true if and only if p always satisfies it;^ < V^^ , i.e. never overrates 
itself. 
VA(p) = \;^k\/wlylyixi...wlyl 
: p{yx<k)^wlyl..,wlyl 
^wl< 
14lj^</c)] 
(7.7) 
In the following, we restrict our attention to programs p for which VA(^) 
can be proven in some formal axiomatic system. A very important point is 
that V^^ 
is enumerable. This ensures the existence of sequences of programs 
Pi^P2,P3,-'- for which VA(p^) can be proven and limi^oow'^^ —^km 
^^^ ^^^ ^ 
and all I/O sequences, pi may be defined as the naive (nonhalting) approxima-
tion scheme (by enumeration) of V^^^, but terminated after i time steps and 
using the approximation obtained so far for w^^ together with the correspond-
ing output yl'. The convergence wl' ^^^ V^^k ensures that V^^^, which we 
claimed to be the universally optimal value, can be approximated by p with 
provable VA(p) arbitrarily well, when given enough time. The approximation 

226 
7 Computational Aspects 
is not uniform in /c, but this does not matter as the selected p is allowed to 
change from cycle to cycle. 
Another possibility would be to consider only those p that check w^ < V^^ 
online in every cycle, instead of the pre-check VA(p), either by constructing a 
proof (on the work tape) for this special case, or w^ < V^^ is already evident 
by the construction of w^. In cases where p cannot guarantee w^ < V^^ 
it 
sets Wk = 0 and, hence, trivially satisfies w^ < V^^^. On the other hand, for 
these p it is also no problem to prove VA(p), as one has simply to analyze the 
internal structure of p and recognize that p shows the validity internally itself, 
cycle by cycle, which is easy by assumption on p. The cycle-by-cycle check is 
therefore a special case of the pre-proof of VA(p). 
7.2.6 Effective Intelligence Order Relation 
In Section 5.1 we introduced an intelligence order relation ^ on AI systems, 
based on the value V^^ . In the following we need an order relation y^ based 
on the claimed value w^, which might be interpreted as an approximation to 
y. 
Relation >:^ is a co-enumerable partial order relation on extended chronolog-
ical programs. Restricted to valid approximations it orders the policies w.r.t. 
the quality of their outputs and their ability to justify their outputs with high 
7.2.7 The Universal Time-Bounded AlXItZ Agent 
In the following, we describe the algorithm p* underlying the universal time-
bounded AIXiS agent. It is essentially based on the selection of the best 
algorithms p^, out of the time i and length / bounded p, for which there exists 
a proof of VA(p) with length <lp. 
1. Create all binary strings of length Ip and interpret each as a coding of a 
mathematical proof in the same formal logic system in which VA(-) was 
formulated. Take those strings that are proofs of VA(p) for some p and 
keep the corresponding programs p. 
2. Eliminate all p of length >L 

7.2 Time-Bounded AIXI Model 
227 
3. Modify the behavior of all remaining p in each cycle k as follows: Nothing 
is changed if p outputs some w^y^ within i time steps. Otherwise stop p 
and write Wk = 0 and some arbitrary yk to the output tape of p. Let P be 
the set of all those modified programs. 
4. Start first cycle: k:=l. 
5. Run every pGP on extended input ^<fc, where all outputs are redirected 
to some auxiliary tape: p{ifo<:k) —'^^Vi'-^^Vk' 
This step is performed 
incrementally by adding ifCk-i for k>l to the input tape and continuing 
the computation of the previous cycle. 
6. Select the program p with highest claimed reward w^: p^. :=argmaxpit;^. 
7. Write yk '-—y^^ to the output tape. 
8. Receive input Xk from the environment. 
9. Begin next cycle: k\—k-\-l^ goto step 5. 
It is easy to see that the following theorem holds. 
Roughly speaking, the theorem says that if there exists a computable solution 
to some or all AI problems at all, the explicitly constructed algorithm p* 
is such a solution. Although this theorem is quite general, there are some 
limitations and open questions that we discuss in the following. 
The construction of the algorithm p* needs the specification of a formal 
logic system (V,A,^^,Ci,/^,i?i,-^,A,=,...), and axioms, and inference rules. A 
proof is a sequence of formulas, where each formula is either an axiom or 
inferred from previous formulas in the sequence by applying the inference 
rules. Details were presented in Section 7.1.5. We only need to know that 
provability and Turing machines can be formalized. The setup time in the 
theorem is just the time needed to verify the 2^^ proofs, each needing time 
0{ll). 
7.2.8 Limitations and Open Questions 
• Formally, the total computation time of p* for cycles 1...A: increases linearly 
with k, i.e. is of order 0{k) with a coefficient 2^-i. The unreasonably 
large factor 2^ is a well-known drawback in best/democratic vote models 
and will be taken without further comments, whereas the factor i can be 

228 
7 Computational Aspects 
assumed to be of reasonable size. If we do not take the limit k-^oo but 
consider reasonable k, the practical significance of the time bound on p* 
is somewhat limited, due to the additional additive constant 0(/p-2^^). 
It is much larger than k'2^'i as typically /p>^(VA(p)) >£(p) = /. 
• p* is superior only to those p that justify their outputs by (large w^). 
It might be possible that there are p which produce good outputs y^ 
within reasonable time, but it takes an unreasonably long time to jus-
tify their outputs by sufficiently high w^. We do not think that (from 
a certain complexity level onwards) there are policies where the process 
of constructing a good output is completely separated from some sort of 
justification process. But this justification might not be translatable (at 
least within reasonable time) into a reasonable estimate of V^^ . 
• The (inconsistent) programs p must be able to continue strategies started 
by other policies. It might happen that a policy p steers the environment 
to a direction for which p is specialized. A "foreign" policy might be able 
to displace p only between loosely connected episodes. There is probably 
no problem for factorizable //. Think of a chess game, where it is usually 
very difficult to continue the game or strategy of a different player. When 
the game is over, it is usually advantageous to replace a player by a better 
one for the next game. There might also be no problem for sufficiently 
separable /i. 
• There might be (efficient) valid approximations p for which VA(p) is true 
but not provable, or for which only a very long {>lp) proof exists. 
7.2.9 
Remarks 
• The idea of suggesting outputs and justifying them by proving reward 
bounds implements one aspect of human thinking. There are several pos-
sible reactions to an input. Each reaction possibly has far-reaching con-
sequences. Within a limited time one tries to estimate the consequences 
as well as possible. Finally, each reaction is valuated, and the best one is 
selected. What is inferior to human thinking is that the estimates w^ must 
be rigorously proved and the proofs are constructed by blind exhaustive 
search, further, that all behaviors p of length </ are checked. It is inferior 
"only" in the sense of necessary computation time but not in the sense of 
the quality of the outputs. 
• In practical applications there are often cases with short and slow pro-
grams ps performing some task T, e.g. the computation of the digits of TT, 
for which there exist long but quick programs pi too. If it is not too dif-
ficult to prove that this long program is equivalent to the short one, then 
it is possible to prove K^^P^\T) 
<i{ps) with K^ being the time-bounded 
Kolmogorov complexity. Similarly, the method of proving bounds Wk for 

7.2 Time-Bounded AIXI Model 
229 
Vkruk c^^ give high lower bounds without explicitly executing these short 
and slow programs, which mainly contribute to Vkmk • 
• Dovetailing all length- and time-limited programs is a well-known elemen-
tary idea (e.g. typing monkeys). The crucial part, which was developed 
here, is the selection criterion for the most intelligent agent. 
• The construction of AlXItl and the enumerability of Vkmk ensure arbitrary 
close approximations of Vkmk' hence we expect that the behavior of AlXltl 
converges to the behavior of AIXI in the limit t,/,/p^-oc, in some sense. 
• Depending on what you know or assume that a program p of size I and 
computation time per cycle i is able to achieve, the computable AlXltl 
model will have the same capabilities. For the strongest assumption of 
the existence of a Turing machine that outperforms human intelligence, 
AlXItl will do too, within the same time frame, up to an (unfortunately 
very large) constant factor. 


In spite of its incomputability, 
Algorithmic 
Probability 
can serve as a kind of 'Gold Standard' for 
induction 
systems 
— Ray Solomonoff 
It's hard to make predictions, especially about the future 
— Niels Bohr 
We have the mathematical 
theory of decision making 
under uncertainty. 
What the mathematical 
theory is 
worth, it is hard to say. It does have the advantage, 
^^^ oolomonon 
though, of providing definite rules — Richard Bellman 
8 Discussion 
This chapter critically reviews what has been achieved in the book and dis-
cusses some otherwise unmentioned topics of general interest. We summarize 
the major results and compare performance and generality of AIXI(t/) to those 
of other approaches to AI. We remark on various topics, including concurrent 

232 
8 Discussion 
actions and perceptions, the choice of the I/O spaces, treatment of encrypted 
information, and pecuharities of mortal embodied agents. We also make some 
personal comments and speculations on the present status and the future of 
the research fields of AI and machine learning themselves. We continue with an 
outlook on further research. Since many ideas have already been presented in 
the problems and conclusions sections of the various chapters, we concentrate 
on nontechnical open questions of general importance, including optimality, 
down-scaling, implementation, approximation, elegance, extra knowledge, and 
training of/for AIXI(^/). Furthermore, we collect and state all explicit or im-
plicit assumptions, problems and limitations of AlXl{tl). We briefly discuss 
some relevant philosophical issues: the free will versus determinism paradox, 
the existence of objective probabilities, and the Turing test. We also include 
some (personal) remarks on non-computable physics, the number of wisdom 
i?, and consciousness. As it should be, the book concludes with conclusions. 
8.1 What has been Achieved 
8.1.1 
Results 
The major theme of the book was to develop a mathematical foundation of ar-
tificial intelligence. This is not an easy task since intelligence has many (often 
ill-defined) faces. More specifically, our goal was to develop a theory for ratio-
nal agents acting optimally in any environment. Thereby we touched various 
scientific areas, including reinforcement learning, algorithmic information the-
ory, Kolmogorov complexity, computational complexity theory, information 
theory and statistics, Solomonoff induction. Levin search, sequential decision 
theory, adaptive control theory, and many more. The conceptual ingredients 
of AIXI may be depicted in as follows: 
Decision Theory 
= Probability -h Utility Theory 
Universal Induction = Occam -h Epicurus -h Bayes 
li 
II 
Universal Artificial Intelligence without Parameters 
The major achievements were the following: 
• We presented the philosophical and mathematical foundations of universal 
induction: Occam's razor principle, Epicurus' principle of multiple expla-
nations, subjective versus objective probabilities. Cox's axioms for beliefs, 
Kolmogorov's axioms of probability, conditional probability and Bayes' 
rule, Turing machines, Kolmogorov complexity, culminating in universal 
Solomonoff induction (Chapter 2). 

8.1 What has been Achieved 
233 
• We derived various convergence results, (tight) loss bounds, and Pareto 
optimality for predictors based on Bayes mixture priors. We gave an Oc-
cam's razor argument that using Solomonoff's prior leads to a universally 
optimal prediction scheme (Chapter 3). 
• We presented sequential decision theory in a very general form in which 
actions and perceptions may depend on arbitrary past events. The devel-
opment was more of a formal exercise and optimality of the Alji model 
for known environment fi is obvious by construction (Chapter 4). 
• We unified sequential decision theory and Solomonoff's theory of universal 
induction, both optimal in their own domain. The resulting parameter-
free AIXI model constitutes an agent for which we gave strong arguments 
that it behaves optimally in any environment. That is, it copes with explo-
ration versus exploitation, large state spaces, generalization and function 
approximation, non-stationary and partially observable environments, and 
so on (Chapter 5). 
• We defined a universal intelligence order relation -< regarding which AIXI 
is also the most intelligent agent and argued this order relation to be 
reasonable (Section 5.1.4). 
• We discussed the difficulties in extending the optimality results from 
the prediction case to AIXI. Along these lines, we suggested various 
potentially relevant environmental (separability) concepts (Sections 5.2 
and 5.3). 
• We discussed the choice of the horizon and came to the conclusion that a 
reward discounting (like near-harmonic) which leads to an eff"ective hori-
zon that increases in proportion to the current age of the agent is best 
(Section 5.7). 
• For restricted environmental classes and Bayes mixtures ^ we showed that 
AI<^ is self-optimizing and Pareto optimal (Section 5.4 and 5.6). 
• We showed how AIXI is suitable for dealing with a number of important 
problem classes, including sequence prediction, strategic games, function 
minimization, and supervised learning (Chapter 6). 
• Based on the mathematical (incomputable) AIXI model we developed a 
computable model, AlXItl^ with optimal order of computation time, apart 
from a large multiplicative constant (Section 7.2). 
• We developed a general-purpose algorithm — the asymptotically fastest 
(and shortest) algorithm for all well-defined problems. We got rid of the 
large multiplicative constant as in Levin search and AlXIt/, at the expense 
of an (unfortunately even larger) additive constant (Section 7.1). 

234 
8 Discussion 
All in all, the results show that artificial intelligence can be framed by an 
elegant mathematical theory. Some progress has also been made toward an 
elegant computational theory of intelligence. 
8.1.2 
Comparison to Other Approaches 
A different way to measure the achievements of this book is to compare 
AIXI(t/) to other AI approaches. In Table 8.1 we compare various learning 
algorithms that are rather general in purpose, have an agent-like setup, are 
popular or are otherwise interesting or promising. We subjectively rate the 
different approaches w.r.t. various performance and generality criteria. We 
use a gray scale between YES and NO, since the evaluation is often arguable, 
especially if there are many algorithm variants. For most table cells one can 
imagine a variant and an application for which rating YES would be justified, 
and one for which rating NO would be justified. Hence, the presented ratings 
refer to typical algorithm variants and typical (intended) applications. It is 
beyond the scope of this book to describe all these approaches and to justify 
each rating in detail. 
We consider the following properties in the different columns: An algorithm 
is time efficient if it runs on a present-day (2004) computer in acceptable 
time for "interesting" applications. An algorithm is data efficient if it exploits 
(learns from) the information contained in the received data in a theoretically 

8.2 General Remarks 
235 
near-optimal fashion. An algorithm gets a yes in the exploration column only 
if it addresses the exploration versus exploitation problem in a fundamental 
near-optimal way. (Many algorithms are greedy with some simple random 
exploration added). Convergence of an algorithm may be just to some pol-
icy, to a local optimum, or to a/the global optimum. An important issue is 
whether learning algorithms are capable of generalizing from previous experi-
ence to similar situations. We also indicate whether an algorithm is capable 
of or designed for dealing with non-Markovian environments, e.g. POMDPS. 
All selected algorithms are capable of learning by experience, except value 
and policy iteration, which need an exact description of the environment in 
advance. The last column distinguishes between passive predictors and active 
agents. 
The first group of algorithms in Table 8.1 contains "classical" reinforce-
ment learning algorithms. See [SB98, BT96] for a description of value and 
policy iteration, and temporal difference (TD) learning with finite iState space 
versus linear/general function approximation. See [BBOl, KHSOla] for an in-
troduction to direct gradient-based reinforcement learning. The second group 
in Table 8.1 contains various other learning algorithms: logic planners [RN95, 
Part IV], split trees [Rin94, McC95], adaptive Levin search [SZW97], opti-
mal ordered problem solver (OOPS) [SchOSa, Sch04], prediction with expert 
advice (PEA) [CB97, HP04], market/economy-based reinforcement learning 
[Bau99, KHSOlb]. The third group in Table 8.1 lists the main models of 
this book: sequence prediction based on Solomonoff's prior (SPXI) and the 
AIXI(i/) model(s). The last line lists the capabilities of human agents. Schmid-
huber's recent self-referential and self-improving Godel machine [SchOSb] (not 
in the table) is a promising idea to overcome the huge constants in AlXIt/, 
but it is unclear for now whether self-improvements can take place if the only 
environment axiom is ji^Mu^ 
the utility axiom is to maximize F^, and the 
initial software is AIXI(i/). Overall, it can be said that the models in the first 
two groups are applicable in limited domains with feasible computation time, 
whereas the models of the last group are completely general, but computa-
tionally not feasible without further approximations. 
8.2 General Remarks 
This section remarks on some otherwise unmentioned topics of general in-
terest. The logically disconnected subsections discuss concurrent actions and 
perceptions, the choice of the I/O spaces, (universal) prior knowledge, treat-
ment of encrypted information, and peculiarities of mortal embodied agents. 
8.2.1 
Miscellaneous 
Game theory. In game theory [OR94] one often wants to model the situation 
of simultaneous actions, whereas the AI^ models have serial I/O. Simultaneity 

236 
8 Discussion 
can be simulated by withholding the environment from the current agent's 
output yfc, until Xk has been received by the agent. Formally, this means that 
IJi{y£<^kWk) ^^ independent of the last output yk- The AI^ agent is already of 
simultaneous type in an abstract view if the behavior p is interpreted as the 
action. In this sense, AI^ is the policy p^ that maximizes the utility function 
(value), under the assumption that the environment acts according to ^. The 
situation is different from game theory, as the environment ^ is not a second 
'player' that tries to optimize his own utility (see Section 6.3). 
I n p u t / o u t p u t spaces. In various examples we have chosen differently spe-
cialized input and output spaces X and y. It should be clear that, in principle, 
this is unnecessary, as large enough spaces X and y (e.g. the set of strings of 
length 2^^) serve every need and can always be Turing-reduced to the specific 
presentation needed internally by the AIXI agent itself. But it is clear that, 
using a generic interface, such as camera and monitor for learning tic-tac-toe, 
for example, adds the task of learning vision and drawing. 
8.2.2 
Prior Knowledge 
In many problems in practice we have extra information about the problem 
at hand, which could and should be used to guide the forecasting. If the 
prior knowledge is of the form that it includes only environments of certain 
structures, e.g. MDPS, one can use the appropriate Bayes mixture over these 
environments. If there is reason to believe that certain environments are less 
or more likely than Occam's razor tells us, then this could be coded in the 
weights Wy. Unfortunately, this procedure is often intractable in practice, since 
one has only a (possibly vague) description of prior facts, which are hard to 
translate into classes M and/or weights Wy. Fortunately, there is a simple 
way of incorporating all prior knowledge D in an easy and optimal way. The 
trick is to get rid of all prior knowledge by prefixing the observation sequence 
X1X2... by some binary coding dia of D. Using then Solomonoff's prior M 
on diaxi-^n for prediction on cycles / + 1 to n + / one gets loss bounds (to 
logarithmic accuracy) in terms of K{ii\D). 
If D contains information about // 
it will reduce the Kolmogorov complexity of /i, if not we cannot expect D to 
improve prediction accuracy. This also solves the often mentioned concern of 
how to make good predictions for short sequences (sparse data) of length n = 
0(1). As long as n-j-l is larger than the typical compiler constants, predictions 
based on ^u are good. It seems that in science one often faces problems with 
data of information content, say 200 bits only, and none or very little prior 
knowledge, say only 100 bits is available. For instance, having thrown a biased 
coin 200 times and describing our prior knowledge as "i.i.d. with uniform 
second-order prior over bias ^" seems not to contribute more than 100 bits 
on prior information. Laplace' law of succession [Lap 12] leads to reasonable 
estimates of 0 and predictions of further tosses, whereas ^u is far from /i in 
cycle 300 for typical U. But it is an illusion that we only have 100 bits of 

8.2 General Remarks 
237 
prior knowledge. We spent at least 12 years in school, before having heard 
about uniform priors and Laplace' rule. Our whole scientific knowledge serves 
as prior knowledge. If we take for D a representative collection of scientific 
books (+ some language books), / is much larger than the typical compiler 
constants and ^u{xn\di:ix<::n) will be very close to the true bias 0\ This also 
holds true for more complex examples. We can make non-arbitrary predictions 
given a sequence of /+n—1 bits only if ^u leads to the same prediction for all 
"reasonably complex" universal Turing machines U. 
8.2.3 Universal Prior Knowledge 
There are people who believe universal AI is not possible, that one has to 
incorporate some/sufficient prior knowledge. I disagree in a sense described in 
Section 8.4. A different approach is to exclude only those environments that 
we are sure not to be realized. This approach is worth considering, but has 
the following problems: 
1) Physical knowledge is never 100% sure. For instance, 100 years ago 
everybody would have assumed a flat three-dimensional universe. I'm not too 
concerned about this, since today's physical theories are very accurate and 
reliable, at least the parts that seem to be relevant for (in a very broad sense) 
human-sized and equipped agents. Instead of eliminating universes that seem 
to be excluded by our observations and theories, one may only reduce the 
prior belief in odd universes, but this does not help in substantially increasing 
the prior belief of the likely universes. 
2) More seriously, fi does not describe the total universe, but only a small 
fraction, from the subjective perspective of the agent. It is (somewhat/much?) 
harder to characterize the set of possible universes M from the subjective 
agent perspective. 
3) One may take into account only general properties of the universe like 
locality, continuity, or the existence of (manipulable) objects with properties 
and relations in a manifold. The major problem is that, although the universe 
seems to be a local continuous MDP (ignoring quantum effects), fi is neither an 
MDP nor local. What the agent directly observes (with his sensors, like a cam-
era) is not the complete MDP state and often appears nonlocal. So probably 
very little really exploitable can be said about /x. 
Of course, the scientific approach is to simply assume some properties 
(whether true in real life or not) and analyze the performance of the resulting 
models. 
8.2.4 How AlXI(tZ) Deals with Encrypted Information 
Consider the task of decrypting a message that was encrypted by a public key 
encrypter like RSA. A message m is encrypted using a product n of two large 
primes pi and p2, resulting in encrypted message c=RSA{m\n). 
RSA is a 
simple algorithm of size 0(1). If AIXI is given the pubHc key n and encrypted 

238 
8 Discussion 
message c, in order to reconstruct the original message m it only has to "learn" 
the function RSA~^(c|n) :=:RSA(c|pi,p2) = ^ - RSA~^ can itself be described 
in length 0(1), since RSA is 0(1) and pi and p2 can be reconstructed from 
n. Only very little information is needed to learn 0(1) bits. In this sense 
decryption is easy for AIXI (hke TSP, see Section 6.4.6). The problem is that 
while RSA is efficient, RSA~^ is an extremely slow algorithm, since it has to 
find the prime factors from the public key. But note, in AIXI we are not talking 
about computation time, we are only talking about information efficiency 
(learning in the least number of interaction cycles). One of the key insights 
in this book that allowed for an elegant theory of AI was this separation of 
data efficiency from computation time efficiency. Of course, in the real world 
computation time matters, so we introduced AlXItl. AlXltl can do every job 
as well as the best length I and time t bounded agent, apart from time factor 2^ 
and a huge offset time. No practical offset time is sufficient to find the factors 
of n, but in theory, enough offset time allows also AlXltl to (once-and-for-all) 
find the factorization, and then, decryption is easy of course. 
8.2.5 
Mortal Embodied Agents 
The examples we gave in this book, particularly those in Chapter 6, were 
mainly bodiless agents: predictors, gamblers, optimizers, learners. There are 
some peculiarities with reinforcement-learning autonomous embodied robots 
in real environments. 
We can still reward the robot according to how well it solves the task we 
want it to do. A minimal requirement is that the robot's hardware functions 
properly. If the robot starts to malfunction its capabilities degrade, result-
ing in lower reward. So, in an attempt to maximize reward, the robot will 
also maintain itself. The problem is that some parts will malfunction rather 
quickly when no appropriate actions are performed, e.g. flat batteries, if not 
recharged in time. Even worse, the robot may work perfectly until the bat-
tery is nearly empty, and then suddenly stop its operation (death), resulting 
in zero reward from then on. There is too little time to learn how to main-
tain itself before it's too late. An autonomous embodied robot cannot start 
from scratch but must have some rudimentary built-in capabilities (which 
may not be that rudimentary at all) that allow it to at least survive. This is 
similar to the problem discussed in Section 6.4.5 of using AIXI in the FMF 
setting with too late a reward. Using FMF^ instead, corresponds to incorpo-
rating some rudimentary capability. Animals survive due to reflexes, innate 
behavior, an internal reward attached to the condition of their organs, and a 
guarding environment during childhood. Different species emphasize different 
aspects. Reflexes and innate behaviors are stressed in lower animals versus 
years of safe childhood for humans. The same variety of solutions is available 
for constructing autonomous robots (which we will not detail here). 
Another problem connected, but possibly not limited to embodied agents, 
especially if they are rewarded by humans, is the following: Sufficiently intelli-

8.3 Personal Remarks 
239 
gent agents may increase their rewards by psychologically manipulating their 
human "teachers", or by threatening them. This is a general sociological prob-
lem that successful AI will cause, which has nothing specifically to do with 
AIXI. Every intelligence superior to humans is capable of manipulating the 
latter. In the absence of manipulable humans, e.g. where the reward structure 
serves a survival function, AIXI may directly hack into its reward feedback. 
Since this is unlikely to increase its long-term survival, AIXI will probably 
resist this kind of manipulation (just as most humans don't take hard drugs, 
because of their long-term catastrophic consequences). 
8.3 Personal Remarks 
It is hard to predict the future, as it is to predict the development of research 
areas like AI. Nevertheless, I would hke to risk a try. To be more specific, 
in the following I suggest a framework for machine learning research. It is a 
mixture of how I expect and would like the field to look in the near future. 
8.3.1 
On the Foundations of Machine Learning 
Instead of addressing machine learning directly, let us first consider a diff"erent 
research area such as algorithm and complexity theory. The goal of algorithm 
theory is to find and analyze fast algorithms, while the goal of complexity 
theory is to show lower bounds on the time needed to solve certain prob-
lem classes. All concepts are rigorously defined: algorithm, Turing machine, 
problem class, computation time, and so on. Most disciplines generally start 
with an informal way of attacking a subject. With time the discipline becomes 
more and more formalized, often up to a point where it is completely rigor-
ous. Examples are number theory, set theory, proof theory, probability theory, 
infinitesimal calculus, and quantum field theory. Each theory experienced a 
time in which it was dealt with in an informal way, but after a while it was 
made rigorous, is now completely axiomatized, and is rarely questioned.^ Of 
course, not all disciplines are axiomatized yet or are axiomatizable at all (e.g. 
biology), and new research areas emerge, starting in an informal condition, 
but the point is that once a field has emerged, the path is toward increasing 
rigor. 
What can be said about machine learning? In machine learning one tries 
to build and understand systems that learn from past data, make good pre-
dictions, are able to generalize, act intelligently, etc. Many of these and other 
terms are only vaguely defined or have many alternative definitions. As dis-
cussed in Chapter 2 and elsewhere, from a formal point of view, all learning 
tasks can be unified in the framework of sequence prediction. We propose Oc-
cam's razor, quantified in terms of Kolmogorov complexity or Solomonoff's 
^ Quantum field theory may be argued not to be in a completely mathematically 
satisfactory condition, yet. 

240 
8 Discussion 
prior, combined with the chain rule for conditional probabilities, and possibly 
sequential decision theory as a rigorous mathematical/axiomatic definition of 
machine learning. More precisely, Solomonoff's induction scheme should be 
"used" for sequence prediction tasks, and, when combined with sequential de-
cision theory, for making sequential decisions. The results of this book and 
also the results of the more applied MML, MDL, and SRM principles sup-
port the power of Occam's razor. As long as there is no convincing evidence 
against Occam's razor, and what is even more important, as long as there 
is no alternate suggestion of how to define machine learning rigorously, it 
is worth assuming Occam's razor and studying its consequences. Indeed, we 
showed in Chapter 3 that the performance of Solomonoff's universal induction 
scheme, as compared to any other prediction scheme in any environment, is so 
good that one may be tempted to take the results as proof of Occam's razor. 
Whereas the theorems were proven with mathematical rigor, one has to be 
careful about their interpretation and the underlying assumptions, especially 
in Theorem 3.70, which was more a self-consistency or bootstrap result. 
We expect that in the future machine learning will, by default, be based 
on Occam's razor. Real-world machine learning tasks will with overwhelming 
majority be solved by developing algorithms that approximate Kolmogorov 
complexity or Solomonoff's prior (e.g. MML, MDL, SRM, and more specific 
ones, like SVM, LZW, neural/Bayes nets with complexity penalty, ...). Ma-
chine learning theory will derive results on convergence speed and approx-
imation quality of the various approximation schemes. Only a minority will 
investigate nonstandard ML by modifying or replacing Occam's razor "axiom" 
in the hope of finding something better. 
8.3.2 In a World Without Occam 
Finally, I would like to remark on an analogy to Peano's axioms for the natural 
numbers and especially the induction axiom. Remove this axiom and replace 
it with a vague concept that resembles this axiom.^ It is then not unreasonable 
to call this concept induction principle since it infers properties valid for all 
natural numbers from a local n—>n-hl property. Imagine if arithmetic were still 
in this situation. Most modern mathematical theorems would evaporate, and 
with them nearly all modern technology. Fortunately, we have this induction 
axiom, but as a formal rule it is now purely deductive! 
I believe the same is true for Occam's razor. Without the vague concept 
of Occam's razor, science and, hence, machine learning would probably be 
not existent at all. Informal Occam's razor is directly or indirectly the basis 
of all scientific induction. The establishment of a formal version of Occam's 
razor would give machine learning in particular, and maybe even science in 
general, a significant boost. I anticipate this by looking at the practical success 
^ I guess that there was a time in history when arithmetic was exactly in this 
condition. 

8.4 Outlook & Open Questions 
241 
of MML, MDL, SRM, and SVM, and the theoretical impact of Kolmogorov 
complexity and Solomonoff induction, which are all formalizations of Occam's 
razor. 
8.4 Outlook &6 Open Questions 
Many ideas for further studies were already stated in the various chapters 
of the book, especially in the problems and conclusions sections. This out-
look only contains nontechnical open questions regarding AIXl{tl) of general 
importance. 
Value bounds. Rigorous proofs for non-asymptotic value bounds for AI^ 
are the major theoretical challenge — general ones, as well as tighter bounds 
for special environments /i, e.g. for rapidly mixing MDPs. For AIXI other 
performance criteria have to be found and proved. Although not necessary 
from a practical point of view, the study of continuous classes A^, restricted 
policy classes, and/or infinite y, X and m may lead to useful insights. 
Scaling AIXI down. A direct implementation of the KYKltl model is, at 
best, possible for small-scale (toy) environments due to the large factor 2^ in 
computation time. But there are other applications of the AIXI theory. We 
saw in several examples how to integrate problem classes into the AIXI model. 
Conversely, one can downscale the AI^ model by using more restricted forms of 
^. This could be done in the same way as the theory of universal induction was 
downscaled with many insights to the minimum description length principle 
[LV92a, Ris89] or to the domain of finite automata [FMG92]. The AIXI model 
might similarly serve as a supermodel or as the very definition of (universal 
unbiased) intelligence, from which specialized models could be derived. 
Implementation and approximation. With a reasonable computation 
time, the AIXI model would be a solution of AI (see the next point if you 
disagree). The PJXltl model was the first step, but the elimination of the 
factor 2^ without introducing a large additive constant like in M|* and with-
out giving up universality will almost certainly be a very difficult task. One 
could try to select programs p and prove VA(p) in a more clever way than 
by mere enumeration, to improve performance without destroying universal-
ity. All kinds of ideas like genetic algorithms, advanced theorem provers, and 
many more could be incorporated. It remains to be seen whether these hand-
waving suggestions can be substantiated. The Godel machine is a promising 
recent approach. 
Computability. We seem to have transferred the AI problem just to a dif-
ferent level, to proving VA(p). This shift has some advantages (and also some 
disadvantages) but does not present a practical solution. Nevertheless, we 
want to stress that we have reduced the AI problem to (mere) computational 

242 
8 Discussion 
questions. Even the most general other systems the author is aware of de-
pend on some (more than complexity) assumptions about the environment, 
or it is far from clear whether they are, indeed, universally optimal. Although 
computational questions are themselves highly complicated, this reduction is 
a nontrivial result. A formal theory of something, even if not computable, is 
often a great step toward solving a problem and also has merits of its own, 
and AI should not be different in this respect (see previous item). 
Elegance. Many researchers in AI believe that intelligence is something com-
plicated and cannot be condensed into a few formulas. It is more a combining 
of enough methods and much explicit knowledge in the right way. From a the-
oretical point of view we disagree, as the AIXI model is simple and seems to 
serve all needs. From a practical point of view we agree, to the following ex-
tent: To reduce the computational burden one should provide special-purpose 
algorithms (methods) from the very beginning, probably many of them re-
lated to reduce the complexity of the input and output spaces ^ and 3^ by 
appropriate pre/postprocessing methods. 
Extra knowledge. There is no need to incorporate extra knowledge from the 
very beginning. It can be presented in the first few cycles in any format. As 
long as the algorithm to interpret the data is of size 0(1), the AIXI agent 
will "understand" the data after a few cycles (see Sections 8.2.2 and 6.5). 
If the environment // is complicated but extra knowledge z makes K{/j.\z) 
small, one can show that the bound (5.9)-(5.10) reduces roughly to ln2'K{fi\z) 
when xi=z, 
i.e. when z is presented in the first cycle. The special-purpose 
algorithms could be presented in xi too, but it would be cheating to say 
that no special-purpose algorithms were implemented in AIXI. The boundary 
between implementation and training is unsharp in the AIXI model. 
Training. We have not said much about the training process itself, as it is 
not specific to the AIXI model and has been discussed in literature in various 
forms and disciplines [Sol86, Sch03a, Sch04]. By a training process we mean a 
sequence of simple-to-complex tasks to solve, with the simpler ones helping in 
learning the more complex ones. A serious discussion would be out of place. 
To repeat a truism, it is, of course, important to present enough knowledge Ok 
and evaluate the agent output yk with Vk in a reasonable way. To maximize 
the information content in the reward, one should start with simple tasks and 
give positive reward to approximately the better half of the outputs yk -
8.5 Assumptions, Problems, Limitations 
Just as every approach to AI (or any other field) makes assumptions and has 
its problems and limitations, so does AIXI(t/). It is time to take a critical look 
at all explicit or implicit assumptions, problems and limitations. 

• 
• 
8.5 Assumptions, Problems, Limitations 
243 
8.5.1 
Assumptions 
The central assumption of this book is Occam's razor. Since Occam's 
razor seems to be at the heart of science and intelhgent behavior in any 
case, it is not a restrictive assumption, but nevertheless a profound one, 
Occam's razor actually only serves as a motivation in this book; the actual 
assumption we use is different, see next item. 
The environment is sampled from a computable probability distribution 
with a reasonable program size on a natural Turing machine. Assump-
tion 2.5 ensures that AlX.l{tl) is essentially independent of whatever uni-
versal Turing machine is chosen. 
We assumed the existence of objective randomness/probabilities respect-
ing Kolmogorov's probability Axioms 2.14. As remarked in Section 2.3, 
this assumption is not essential, since we can restrict the setting to deter-
ministic environments fi. Using Bayes mixtures as subjective probabilities 
also did not involve any assumptions, since they were justified decision-
theoretically. 
In reinforcement learning, the total reward is defined as the sum of rewards 
ri-\-...-\-rm over cycles, and so did we. In finance, where money can be 
reinvested, a product is common, but this can be converted to a sum by 
taking the logarithm. More generally, assume the goal is to maximize some 
function of the rewards Rk :=i?(ri,...,rfc), e.g. their maximum (i? = max, 
see Section 6.4.3). If we reward AIXI (5.3) in each cycle k not with r^ 
but instead with r^:—Rk — Rk-i (with RQ:=0), 
we see that AIXI tries to 
maximize expected r[-\-...+r[^ = Rm, as desired. The original rewards may 
need to be retained as observations (o'^:=o/crjt). So, restricting to reward 
sums is not a real limitation. 
For probabilistic environments we defined the value of a policy which 
shall be maximized in the standard way as the expected reward sum. More 
generally, one may define for each policy a probability distribution for the 
total reward. The question is how to compare these distributions. Besides 
the popular mean, one may want to compare medians or quantiles. The 
most frequent argument for departing from the mean is to achieve more 
robust policies, e.g. policies that, with high probability, have a high lower 
bound on their reward sum. We believe that robustness is never a primary 
goal in itself. The reason for wanting robustness is that one dislikes low 
rewards more than the rewards themselves express. A natural solution is 
to take the expectation of /-transformed rewards, where / is a monotone 
increasing concave function (like log). Function / penalizes small rewards 
and leads to more robustness. 
We assumed finite action/perception spaces y/^, 
which are sufficient for 
all practical purposes. From a theoretical point of view infinite spaces may 
be attractive in certain situations. Countable ^ should cause no problems; 

244 
8 Discussion 
in case of countable y only e-optimal policies may exist. For continuous A* 
and y one has to somehow generalize the notion of Kolmogorov complexity 
and Solomonoff's prior. Continuous environmental classes Ai were briefly 
discussed in Section 3.7.2. 
• We assumed bounded nonnegative rewards Tk € [0,r^aa:]- Nonnegativity is 
not essential, but boundedness is essential for ensuring existence of values. 
Again, from a practical point of view this should not be restrictive. 
• We assumed finite horizon or near-harmonic discounting to ensure the 
existence of values. We provided motivations for the choice of the latter, 
but we are not sure whether it represents a final answer. 
After all this, one should not forget that all other known approaches to AI 
implicitly or explicitly make (many) more assumptions. 
8.5.2 
Problems 
• Assume AIXI is used in a multi-agent setup interacting with other agents. 
For simplicity we only discuss the case of a single other agent in a compet-
itive setup, i.e. a two-person zero-sum game situation. We can entangle 
agents A and B by Ok{A)=yk{B), 
Ok^i{B)=yk{A). 
The rewards rk{A) 
and rk{B) are provided externally by the rules of the game. The situation 
where A is AIXI and 5 is a perfect minimax player was analyzed in Sec-
tion 6.3. In multi-agent systems one is mostly interested in a symmetric 
setup, i.e. B is also an AIXI. Whereas both AIXI5 may be able to learn the 
game and improve their strategies (to optimal minimax), this setup vio-
lates one of our basic assumptions. Since AIXI is incomputable, AIX1{B) 
does not constitute a computable environment for AlXI(yl). More gen-
erally, starting with any class of environments AI, then fi=Al^M seems 
not to belong to class M for most (all?) choices of M. Various results 
of the book can no longer be applied, since ji^ M when coupling two 
AI^s. Many questions arise: Are there interesting environmental classes 
for which K1£^M^M 
or AI^tl>(GA4? Do AIXI(A/E) converge to optimal 
minimax players? Do AIXI5 perform well in general multi-agent setups? 
8.5.3 
Limitations 
• Although AIXI may be regarded as a formal definition or a mathematical 
solution of AI, it is not a practical solution due to its incomputability. 
AlXItZ is a step in the direction of a computable theory of AI, but is also 
not practically feasible. Whether AIXI can be scaled down in a systematic 
way to yield practical AI algorithms or whether it will only serve as a 
guiding principle in attacking difficult AI problems remains to be seen. 

8.6 Philosophical Issues 
245 
8.6 Philosophical Issues 
Many arguments against strong and weak AI have been proposed: Lu-
cas' and Penrose's arguments based on Godel's incompleteness theorem 
[Luc61, Pen89, Pen94], Searle's Chinese room argument [SeaSO], the lookup 
table argument [Chu86], Moravec's brain prosthesis experiment [Mor88], the 
free will argument and, of course, various religious reasons. All of them have 
loopholes and can be refuted, but this is not the place to repeat this discus-
sion. We only discuss the free will paradox. There are also objections to the 
existence of objective probabilities. We present a possibly new one below. We 
also briefly comment on the famous Turing test, which also fits under the 
heading of this section. Finally, we speculate on the big questions of AI in 
general and the AIXI model in particular, related to non-computable physics, 
the number of wisdom i7, and consciousness. 
8.6.1 
Turing Test 
The Turing test [Tur50] was designed to decide whether an AI system is 
intelligent. We should concede a machine true intelligence if it passes the 
Turing test, but to deny intelligence in case of failure may be too harsh. The 
true problem in using the Turing test (e.g. instead of AIXI) as a definition 
of intelligent systems is another. The test involves a human interrogator and, 
hence, cannot be formalized mathematically, therefore it does also not allow 
the development of a computational theory of intelligence. 
8.6.2 
On the Existence of Objective Probabilities 
Throughout the book we have assumed the existence of objective probabilities 
respecting Kolmogorov's probability Axioms 2.14. As remarked in Section 2.3 
we could have restricted the development to classes M. of deterministic en-
vironments, thus avoiding objective probabilities? In the following we give 
an argument which makes the behef in objective probabilities look somewhat 
"unscientific". The assumption that an event occurs with some objective prob-
ability expresses the opinion that the occurrence of an individual stochastic 
event has no explanation, i.e. is inherently impossible to predict for sure. One 
central goal of science is to explain things. Often we do not have an explana-
tion (yet) that is acceptable, but to say that "something can principally not be 
explained" means to stop even trying to find an explanation. From a distance, 
tossing a coin looks objectively random, but looking at it closer the outcome 
is just subjectively unknown due to most observers' lack of knowledge of the 
initial conditions and external influences on the coin during its throw. When 
^ Using Bayes mixtures as subjective probabilities did not need any (e.g. Cox's) 
axioms for justification, but received a decision-theoretic justification. 

246 
8 Discussion 
knowing the exact initial conditions and the exact equations of motion, classi-
cal physics is predictable (this includes chaotic systems). Physicists claim that 
quantum mechanics is truly random, and there is indeed quite some evidence 
to suggest this, but experiments cannot exclude the possibility that quantum 
events are only pseudo-random [Sch02b]. It seems safer and more honest to say 
that with our current technology and understanding we can only determine 
(subjective) outcome probabilities. If a sufficiently large community of people 
arrive at the same subjective probabilities from their prior knowledge, one 
may want to call these probabilities objective. For instance, for most people 
(those with no special equipment and education) a fair coin comes up head 
in 50% of the cases. And for all people so far, if they measure the spin of 
one photon in a para-positronium decay, it is up in 50% of the cases. On one 
hand, we have to abandon objective probabilities because their assumption 
seems unscientific, but on the other hand, their assumption is very convenient. 
Without objective probabilities there would be no (objective) unbiased coins, 
dice, MDPs, radioactive decays, etc. Maybe one should admit a gray scale of 
more or less subjective probabilities. 
8.6.3 Free Will versus Determinism 
For illustrational purpose we replace determinism with computability. 
The paradox. If the brain of a human were computable we could predict 
the action of the human with a computer. If we tell the human his action 
in advance, he is forced to perform this action and hence loses his free will. 
Assuming humans have free will refutes the computability assumption of the 
brain. 
This paradox between computability and free will is sometimes used as an 
argument against the possibility of AI. However, it vanishes by a more careful 
reasoning. That a part of the universe is computable, is defined as follows: 
Assumption 1. Given a box (part of the universe) in state s at time t we 
can compute the next (or some more distant future) state s' at time f >t if 
there is no interaction between the box and the rest of the universe during 
time t...t^ 
Without this independence assumption in time interval [t^f] the possibility 
of correct prediction cannot be guaranteed. 
Assumption 2. Assume that the brain is computable. It receives input x at 
time t and computes action y at time f. During the thinking period [t,f] it is 
completely separated from the environment. 
After input x, the brain ^ is in a state s and Assumption 1 applies, i.e. 
we can compute, say with algorithm p: JY ~^y, the brain's decision y. We 
cannot inform the brain in period [t,t^] of this decision without violating As-
sumption 2. We are free to hand out ^ in a closed envelope to B. After B has 

8.6 Philosophical Issues 
247 
made its decision, it is allowed to open the letter and realizes that its deci-
sion was predictable. Such an experiment will have enormous psychological, 
social, and legal consequences, and looks paradoxical, but does not lead to 
any contradictions! 
Assume we allow intermittent interaction and inform the brain about y, 
then the brain JB' maps input {x^y) to an action y\ which is possibly different 
from y. There is no contradiction, since p maps A' to y, whereas B^ maps 
?^xy-^y, 
so these functions have nothing to do with each other. 
Consider a variant of the paradox, where B itself reliably pre-
dicts/precomputes its own action, and then "decides" to deviate from its own 
prediction. More formally, the assumption is that a part B2 of brain B = Bi 
can simulate Bi. By assumption B2 is functionally identical to Bi, Only for 
illustrational purposes we make the further assumption that B2 also operates 
identically to Bi in the sense that B2 itself contains a part, say ^ 3 , which 
simulates JB2, etc. We have an infinite recursion. The first question is not 
what the output of B is and whether it is finitely computable, but whether 
this infinite recursion has a value at all What we need is a fixed point. Insert 
a decision y (as a possible candidate for B2) into brain B and test whether 
B computes the same decision y. If it does, then y is a, fixed point of the 
recursion. If such a fixed point exists (and is unique) we may define the value 
of the infinite recursion as this fixed-point value. Finally, we would have to 
check whether this fixed point can be found by a finite algorithm. It is well 
known that not every recursion y = B{y) has a fixed point. The paradox in 
our case is just that we implicitly assumed the existence of a (unique) fixed 
point. The paradox is resolved by noting that this fixed point simply does not 
exist. Sometimes fixed points can be found by iteration. One starts with some 
value yi for y and iterates y2 = B{yi)^ ..., yn = B{yn-i)- If the limit ^oo exists, 
then it is a fixed point. 
Assume our function B acts with y^lif 
B2 predicts y=0, and vice versa. In 
this case yn = l—yn-i oscillates, and ^oo does not exist. In the case of a binary 
decision y €3^€ {0,1} this proves that a fixed point does not exist. So this 
paradox is about nonexistent fixed points. A self-contradictory brain simply 
does not exist (when starting from Assumptions 1 and 2). We may lower 
our demands to probabilistic predictions. Mathematically we lift the mapping 
B'.y^y 
to a linear mapping B': [0,1]^-^[0,1]^ between (probability) vectors 
over y. B^ always has a fixed point. For our example we get B\p) = l—p 
with fixed point p= | , meaning that B2 has no idea of what B is going to do. 
Whatever, it is not possible to set up a brain with a part predicting its own 
behavior reliably in every situation. The same analysis holds for (an infinite 
regression of) external predictors, telling the human his action in advance. 
Note that neither the paradox, nor the solution has anything specific to do 
with computable functions. We could have formulated the paradox in terms 
of general mathematical functions (mappings). 

248 
8 Discussion 
8.6.4 The Big Questions 
On non-computable physics &; brains. There are two possible objections 
to AI in general and, therefore, to AIXI in particular. Non-computable physics 
(which is not too odd) could make Turing-computable AI impossible. As at 
least the world that is relevant for humans seems mainly to be computable, 
we do not believe that it is necessary to integrate non-computable devices into 
an AI system. The (clever and nearly convincing) Godel argument by Penrose 
[Pen89, Pen94], refining Lucas [Luc61], that non-computational physics must 
exist and is relevant to the brain, has (in our opinion convincing) loopholes. 
Evolution & the number of wisdom. A more serious problem is the evolu-
tionary information gathering process. It has been shown that the 'number of 
wisdom' i? contains a very compact tabulation of 2^ undecidable problems in 
its first n binary digits [Cha75, CHKW98]. i? is only enumerable with compu-
tation time increasing more rapidly with n than any recursive function. The 
enormous computational power of evolution could have developed and coded 
something like i? into our genes, which significantly guides human reasoning 
[ChaOl]. In short: Intelligence could be something complicated, and evolution 
toward it from an even cleverly designed algorithm of size 0(1) could be too 
slow. As evolution has already taken place, we could add the information from 
our genes or brain structure to any/our AI system, but this would mean that 
the important part is still missing, and that it is principally impossible to 
derive an efficient algorithm from a simple formal definition of AI. 
Consciousness. For what is probably the biggest question, that of conscious-
ness, we want to give a physical analogy. Quantum (field) theory is the most 
accurate and universal physical theory ever invented. Although already de-
veloped in the 1930s, the big question, regarding the interpretation of the 
wave function collapse, is still open. Although this is extremely interesting 
from a philosophical point of view, it is completely irrelevant from a practi-
cal point of view.^ We believe the same to be valid for consciousness in the 
field of artificial intelhgence: philosophically highly interesting but practically 
unimportant. Whether consciousness will be explained some day is another 
question. 
8.7 
Conclusions 
All tasks that require intelligence to be solved can naturally be formulated as 
a maximization of some expected utility in the framework of agents. We pre-
sented a functional (4.7) and an iterative (4.17) formulation of such a decision-
theoretic agent in Chapter 4, which is general enough to cover all AI problem 
^ In the Theory of Everything, the collapse might become of "practical" importance 
and must or will be solved. 

8.7 Conclusions 
249 
classes, as demonstrated by several examples. The main remaining problem 
is the unknown prior probability distribution /x of the environment (s). Con-
ventional learning algorithms are unsuitable, because they can neither handle 
large (unstructured) state spaces, nor do they converge in the theoretically 
minimal number of cycles, nor can they handle non-stationary environments 
appropriately. On the other hand, SolomonofF's universal prior M=^u 
(2.26), 
based on ideas from algorithmic information theory, solves the problem of the 
unknown prior distribution for induction problems, as was demonstrated in 
Chapters 2 and 3. No explicit learning procedure is necessary, as ^u automat-
ically converges to fi. We unified the theory of universal sequence prediction 
with the decision-theoretic agent by replacing the unknown true distribu-
tion fi by an appropriately generalized universal semimeasure ^ in Chapter 5. 
We gave various arguments that the resulting AIXI model is the most intelli-
gent, parameter-free and environmental/application-independent model possi-
ble. We defined an intelhgence order relation (5.14) to give a rigorous meaning 
to this claim. Furthermore, possible solutions to the horizon problem were dis-
cussed. In Chapter 6 we outlined how the AIXI model solves various problem 
classes. These included sequence prediction, strategic games, function mini-
mization and, especially, learning to learn supervised. The fist could easily be 
extended to other problem classes like classification, function inversion and 
many others. The major drawback of the AIXI model is that it is uncom-
putable, or more precisely, only asymptotically computable, which makes an 
implementation impossible. To overcome this problem, we constructed a mod-
ified model AlXIt^, which is still effectively more intelligent than any other 
time t and length I bounded algorithm (Section 7.2). The computation time 
of AlXltl is of the order t'2K A way of overcoming the large multiplicative 2^ 
constant was presented at the expense of an (unfortunately even larger) addi-
tive constant (Section 7.1). Possible further research was discussed. The main 
directions could be to prove general and special reward bounds, use AIXI as 
a supermodel and explore its relation to other specialized models and finally 
improve performance with or without giving up universality. 


Bibliography 
[ACBFS02] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The non-
stochastic multiarmed bandit problem. 
SI AM Journal on Computing^ 
32(l):48-77, 2002. 
[ACBG02] P. Auer, N. Cesa-Bianchi, and C. Gentile. Adaptive and self-confident 
on-line learning algorithms. Journal of Computer and System 
Sciences, 
64(l):48-75, 2002. 
[Acz66] J. Aczel. Lectures on Functional Equations and Their Applications. 
Aca-
demic Press, New York, 1966. 
[AS83] 
D. Angluin and C. H. Smith. Inductive inference: Theory and methods. 
ACM Computing Surveys, 15(3):237-269, 1983. 
[BarOO] A. R. Barron. Limits of information, Markov chains, and projection. In 
Proc. IEEE International Symposium on Information Theory (ISIT), pages 
25-25, Sorrento, Italy, 2000. 
[Bau99] E. B. Baum. Toward a model of intelligence as an economy of agents. 
Machine Learning, 35(2):155-185, 1999. 
[Bay63] T. Bayes. 
An essay towards solving a problem in the doctrine of 
chances. Philosophical Transactions of the Royal Society, 53:376-398, 1763. 
[Reprinted in Biometrika, 45, 243-315, 1958]. 
[BBOl] 
J. Baxter and P. L. Bartlett. Infinite-horizon policy-gradient estimation. 
Journal of Artificial Intelligence Research, 15:319-350, 2001. 
[BD62] 
D. Blackwell and L. Dubins. Merging of opinions with increasing informa-
tion. Annals of Mathematical Statistics, 33:882-887, 1962. 
[BEHW87] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Occam's 
razor. Information Processing Letters, 24(6):377-380, 1987. 
[BEHW89] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learn-
ability and the Vapnik-Chervonenkis dimension. 
Journal of the ACM, 
36(4):929-965, 1989. 
[Bel57] 
R. E. Bellman. Dynamic Programming. Princeton University Press, Prince-
ton, NJ, 1957. 
[Ben98] C. H. Bennett et al. Information distance. IEEE Transactions on Infor-
mation Theory, 44, 1998. 
[Berl3] 
J. Bernoulli. Ars Conjectandi. 
Thurnisiorum, Basel, 1713. [Reprinted 
in: Die Werke von Jakob Bernoulli, pages 106-286, volume 3, Birkhauser, 
Basel, 1975, and in: A Source Book in Mathematics, pages 85-90, Dover, 
New York, 1959. English translation of part IV (with limit theorem) by 
Bing Sung, Harvard Univ. Dept. of Statistics, Technical Report #2, 1966]. 
[Ber95a] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume I. 
Athena Scientific, Belmont, MA, 1995. 
[Ber95b] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume II. 
Athena Scientific, Belmont, MA, 1995. 

252 
Bibliography 
[BEYL04] Y. Baram, R. El-Yaniv, and K. Lutz. Online choice of active learning 
algorithms. Journal of Machine Learning Research, 5:255-291, 2004. 
[BF85] 
D. A. Berry and B. Fristedt. Bandit Problems: Sequential Allocation of 
Experiments. Chapman and Hall, London, 1985. 
[BFLS91] L. Babai, L. Fortnow, L. A. Levin, and M. Szegedy. Checking compu-
tations in polylogarithmic time. STOC: 23rd ACM Symp. on Theory of 
Computation, 23:21-31, 1991. 
[BG79] 
C. H. Bennett and M. Gardner. The random number Omega bids fair to 
hold the mysteries of the universe. Scientific American, 241:20-34, 1979. 
[BGHK92] F. Bacchus, A. Grove, J. Y. Halpern, and D. Roller. From statistics to 
beliefs. In Proc. 10th National Conf on Artificial Intelligence 
(AAAI-92), 
pages 602-608, San Jose, CA, 1992. AAA! Press. 
[Blu67j 
M. Blum. A machine-independent theory of the complexity of recursive 
functions. Journal of the ACM, 14(2):322-336, 1967. 
[Blu71] 
M. Blum. On effective procedures for speeding up algorithms. Journal of 
the ACM, 18(2):290-305, 1971. 
[BM98] A. A. Borovkov and A. Moullagaliev. Mathematical Statistics. Gordon & 
Breach, 1998. 
[BS84] 
B. G. Buchanan and E. H. Shortliffe. 
Rule-Based Expert Systems: The 
MYCIN Experiments of the Stanford Heuristic Programming Project. Ad-
dison Wesley, Reading, MA, 1984. 
[BSA83] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive ele-
ments that can solve difficult learning control problems. IEEE 
Transactions 
on Systems, Man, and Cybernetics, SMC-13:834-846, 1983. 
[BT96] 
D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena 
Scientific, Belmont, MA, 1996. 
[Cal02] 
C. Calude. Information 
and Randomness. 
Springer, Berlin, 2nd edition, 
2002. 
[Can74] G. Cantor. Uber eine Eigenschaft des Inbegriffs aller reellen algebraischen 
Zahlen. Journal fiir reine und angewandte Mathematik, 77:258-262, 1874. 
[English translation: On a property of the set of real algebraic numbers. 
In A Source Book in the Foundations of Mathematics, volume 2, pages 
839-843, Clarendon, Oxford]. 
[Car63] G. Cardano. Liber de ludo aleae, 1565/1663. Published in 1663 but com-
pleted already around 1565. 
[Car48] R. Carnap. On the application of inductive logic. Philosophy and Phe-
nomenological Research, 8:133-148, 1948. 
[Car50] R. Carnap. Logical Foundations of Probability. University of Chicago Press, 
Chicago, 1950. 
[CB90] 
B. S. Clarke and A. R. Barron. Information-theoretic asymptotics of Bayes 
methods. IEEE Transactions on Information 
Theory, 36:453-471, 1990. 
[CB97] 
N. Cesa-Bianchi et al. How to use expert advice. Journal of the ACM, 
44(3):427-485, 1997. 
[CBLOl] N. Cesa-Bianchi and G. Lugosi. Worst-case bounds for the logarithmic 
loss of predictors. Machine Learning, 43(3):247-264, 2001. 
[Cha66] G. J. Chaitin. 
On the length of programs for computing finite binary 
sequences. Journal of the ACM, 13(4):547-569, 1966. 

Bibliography 
253 
[Cha69] G. J. Chaitin. 
On the length of programs for computing finite binary 
sequences: Statistical considerations. Journal of the ACM, 16(1):145-159, 
1969. 
[Cha75] G. J. Chaitin, A theory of program size formally identical to information 
theory. Journal of the ACM, 22(3):329-340, 1975. 
[Cha91] G. J. Chaitin. Algorithmic information and evolution. In Perspectives on 
Biological Complexity, pages 51-60. lUBS Press, 1991. 
[Che85] P. Cheeseman. In defense of probability. In Proc. 9th International 
Joint 
Conf on Artificial Intelligence, pages 1002-1009, Los Altos, CA, 1985. 
Morgan Kaufmann. 
[CheSS] P. Cheeseman. An inquiry into computer understanding. 
Computational 
Intelligence, 4(l):58-66, 1988. 
[CHKW98] C. S. Calude, P. H. Hertling, B. Khoussainov, and Y. Wang. Recursively 
enumerable reals and Chaitin f2 numbers. In 15th Annual Symposium on 
Theoretical Aspects of Computer Science, volume 1373 of LNCS, pages 
596-606, Paris, 1998. Springer, Berhn. 
[Chu40] A. Church. On the concept of a random sequence. Bulletin of the American 
Mathematical Society, 46:130-135, 1940. 
[Chu86] P. S. Churchland. Neurophilosophy: Toward a Unified Science of the Mind-
Brain. MIT Press, Cambridge, MA, 1986. 
[Con97] M. Conte et al. Genetic programming estimates of Kolmogorov complexity. 
In Proc. 17th International 
Conf on Genetic Algorithms, pages 743-750, 
East Lansing, MI, 1997. Morgan Kaufmann, San Francisco, CA. 
[Gov74] T. M. Cover. Universal gambling schemes and the complexity measures 
of Kolmogorov and Chaitin. Technical Report 12, Statistics Department, 
Stanford University, Stanford, CA, 1974. 
[Cox46] R. T. Cox. Probability, frequency, and reasonable expectation. 
American 
Journal of Physics, 14(1):1-13, 1946. 
[Csi67] 
I. Csiszar. Information-type measures of difference of probability distri-
butions and indirect observations. Studia Scientiarum 
Mathematicarum 
Hungarica, 2:299-318, 1967. 
[CT91] 
T. M. Cover and J. A. Thomas. Elements of Information 
Theory. Wiley 
Series in Telecommunications. Wiley, New York, 1991. 
[CV03] 
R. Cilibrasi and P. M. B. Vitanyi. Clustering by compression. Technical 
report, CWI, Amsterdam, 2003. http://arXiv.org/abs/cs/0312044. 
[CW82] D. Coppersmith and S. Winograd. On the asymptotic complexity of matrix 
multiplication. SIAM Journal on Computing, ll(3):472-492, 1982. 
[CW90] D. Coppersmith and S. Winograd. Matrix multiplication via arithmetic 
progressions. Journal of Symbolic Computation, 9(3):251-280, 1990. 
[Dal73] 
R. P. Daley. Minimal-program complexity of sequences with restricted 
resources. Information and Control, 23(4):301-312, 1973. 
[Dal77] 
R. P. Daley. On the inference of optimal descriptions. Theoretical Computer 
Science, 4(3):301-319, 1977. 
[Dau90] J. W. Dauben. 
Georg Cantor: His Mathematics 
and Philosophy of the 
Infinite. Princeton University Press, Princeton, NJ, 1990. 
[Daw84] A. P. Dawid. Statistical theory. The prequential approach. Journal of the 
Royal Statistical Society, Series A 147:278-292, 1984. 
[Dem68] A. P. Dempster. A generalization of Bayesian inference. Journal of the 
Royal Statistical Society, Series B 30:205-247, 1968. 

254 
Bibliography 
[Doo53] J. L. Doob. Stochastic Processes. Wiley, New York, 1953. 
[EF98] 
G. W. Erickson and J. A. Fossa. Dictionary of Paradox. University Press 
of America, Lanham, MD, 1998. 
[Fel68] 
W. Feller. 
An Introduction to Probability Theory and its 
Applications. 
Wiley, New York, 3rd edition, 1968. 
[Fer67] 
T. S. Ferguson. Mathematical Statistics: A Decision Theoretic Approach. 
Academic Press, New York, 3rd edition, 1967. 
[Fin37] 
B. de Finetti. Le prevision: ses lois logiques, ses sources subjectives. Ann. 
Inst. Poincare, 7:1-68, 1937. [English translation: Foresight: Its logical 
laws, its subjective sources. In Studies in Subjective Probability. Krieger, 
New York, pages 55-118, 1980]. 
[Fin73] 
T. L. Fine. Theories of Probability. Academic Press, New York, 1973. 
[Fis22] 
R. A. Fisher. 
On the mathematical foundations of theoretical statis-
tics. Philosophical Transactions of the Royal Society of London^ Series 
A 222:309-368, 1922. 
[Fit96] 
M. C. Fitting. First-Order Logic and Automated Theorem Proving. Grad-
uate Texts in Computer Science. Springer, Berlin, 2nd edition, 1996. 
[FMG92] M. Feder, N. Merhav, and M. Gutman. Universal prediction of individual 
sequences. IEEE Transactions on Information Theory, 38:1258-1270, 1992. 
[FS97] 
Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-
line learning and an application to boosting. Journal of Computer and 
System Sciences, 55(1):119-139, 1997. 
[FT91] 
D. Fudenberg and J. Tirole. Game Theory. MIT Press, Cambridge, MA, 
1991. 
[Gac74] P. Gacs. On the symmetry of algorithmic information. Soviet 
Mathematics 
Doklady, 15:1477-1480, 1974. 
[Gac83] P. Gacs. On the relation between descriptional complexity and algorithmic 
probability. Theoretical Computer Science, 22:71-93, 1983. 
[Gal68] R. G. Gallager. Information 
Theory and Reliable Communication. 
Wiley, 
New York, 1968. 
[GCSR95] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data 
Analysis. Chapman k Hall / CRC, 1995. 
[GH02] 
P. D. Griinwald and J. Y. Halpern. Updating probabilities. In Proc. 18th 
Conf on Uncertainty in Artificial Intelligence (UAI-2002), pages 187-196. 
Morgan Kaufmann, San Francisco, CA, 2002. 
[Gin87] M. L. Ginsberg, editor. Readings in Nonmonotonic 
Reasoning. 
Morgan 
Kaufmann, Los Altos, CA, 1987. 
[Git89] 
J. C. Gittins. Multi-Armed Bandit Allocation Indices. Wiley, New York, 
1989. 
[GJ74] 
J. C. Gittins and D. M. Jones. A dynamic allocation index for the se-
quential design of experiments. In Progress in Statistics, pages 241-266. 
North-Holland, Amsterdam, 1974. 
[God31] K. Godel. Uber formal unentscheidbare Satze der Principia Mathemat-
ica und verwandter Systeme I. Monatshefte fur Matematik und Physik, 
38:173-198, 1931. [English translation by E. Mendelsohn: On undecidable 
propositions of formal mathematical systems. In The Undecidable, pages 
39-71, Raven Press, New York, 1965]. 
[Grii98] P. D. Griinwald. The Minimum Discription Length Principle and Reason-
ing under Uncertainty. PhD thesis, Universiteit van Amsterdam, 1998. 

Bibliography 
255 
[GTVOl] P. Gacs, J. Tromp, and P. M. B. Vitanyi. Algorithmic statistics. 
IEEE 
Transactions on Information 
Theory, 47(6):2443-2463, 2001. 
[Hac75] I. Hacking. The Emergence of Probability. Cambridge University Press, 
Cambridge, MA, 1975. 
[Hal90] A. Hald. A History of Probability and Statistics and Their 
Applications 
Before 1750. Wiley, New York, 1990. 
[Hal99] J. Y. Halpern. A counterexample to theorems of Cox and Fine. Journal 
of Artificial Intelligence Research, 10:67-85, 1999. 
[Har79] J. Hartmanis. Relations between diagonalization, proof systems, and com-
plexity gaps. Theoretical Computer Science, 8(2):239-253, 1979. 
[HecSS] D. E. Heckerman. An axiomatic framework for belief updates. In Un-
certainty in Artificial Intelligence 2, volume 5 of Machine Intelligence and 
Pattern Recognition, pages 11-22. North-Holland, Amsterdam, 1988. 
[HHL86] E. J. Horvitz, D. E. Heckerman, and C. P. Langlotz. A framework for 
comparing alternative formalisms for plausible reasoning. In Proc. 5th 
National Conf. on Artificial Intelligence (AAAI-86), volume 1, pages 210-
214, Philadelphia, PA, 1986. Morgan Kaufmann. 
[HKW98] D. Haussler, J. Kivinen, and M. K. Warmuth. Sequential prediction of 
individual sequences under general loss functions. IEEE Transactions on 
Information 
Theory, 44(5):1906-1925, 1998. 
[HM04] M. Hutter and An. A. Muchnik. Universal convergence of semimeasures 
on individual random sequences. In Proc. 15th International 
Conf. on 
Algorithmic Learning Theory (ALT-2004), volume 3244 of LNAI, Padova, 
2004. Springer, Berlin. 
[HMUOl] J. E. Hopcroft, R. Motwani, and J. D. Ullman. Introduction to Automata 
Theory, Language, and Computation. Addison-Wesley, 2nd edition, 2001. 
[HP04] 
M. Hutter and J. Poland. Prediction with expert advice by following the 
perturbed leader for general weights. In Proc. 15th International Conf. on 
Algorithmic Learning Theory (ALT-2004), volume 3244 of LNAI, Padova, 
2004. Springer, Berlin. 
[Hug89] R. I. G. Hughes. Structure and Interpretation 
of Quantum 
Mechanics. 
Harvard University Press, Cambridge, MA, 1989. 
[Hum39] D. Hume. A Treatise of Human Nature, Book I. [Edited version by L. A. 
Selby-Bigge and P. H. Nidditch, Oxford University Press, 1978], 1739. 
[HutOO] M. Hutter. A theory of universal artificial intelligence based on algorithmic 
complexity. Technical Report cs.AI/0004001, Miinchen, 62 pages, 2000. 
http://arxiv.org/abs/cs.AI/0004001. 
[HutOla] M. Hutter. 
Convergence and error bounds for universal prediction of 
nonbinary sequences. In ,Proc. 12th European Conf. on Machine Learn-
ing (ECML-2001), volume 2167 of LNAI, pages 239-250, Freiburg, 2001. 
Springer, Berlin. 
[HutOlb] M. Hutter. General loss bounds for universal sequence prediction. In 
Proc. 18th International Conf. on Machine Learning (ICML-2001), pages 
210-217, Williamstown, MA, 2001. Morgan Kaufmann. 
[HutOlc] M. Hutter. New error bounds for Solomonoff prediction. Journal of Com-
puter and System Sciences, 62(4):653-667, 2001. 
[HutOld] M. Hutter. Towards a universal theory of artificial intelligence based on 
algorithmic probability and sequential decisions. In Proc. 12th European 

256 
Bibliography 
Conf. on Machine Learning (ECML-2001), volume 2167 of LNAI, pages 
226-238, Freiburg, 2001. Springer, Berlin. 
[HutOle] M. Hutter. Universal sequential decisions in unknown environments. In 
Proa. 5th European Workshop on Reinforcement Learning (EWRL-5)^ vol-
ume 27, pages 25-26. Onderwijsinsituut CKI, Utrecht Univ., 2001. 
[Hut02a] M. Hutter. 
The fastest and shortest algorithm for all well-defined 
problems. 
International 
Journal of Foundations of Computer 
Science, 
13(3):431-443, 2002. 
[Hut02b] M. Hutter. Self-optimizing and Pareto-optimal policies in general envi-
ronments based on Bayes-mixtures. In Proc. 15th Annual Conf. on Com-
putational Learning Theory (COLT 2002), volume 2375 of LNAI, pages 
364-379, Sydney, 2002. Springer, Berlin. 
[Hut03a] M. Hutter. Convergence and loss bounds for Bayesian sequence prediction. 
IEEE Transactions on Information 
Theory, 49(8):2061-2067, 2003. 
[Hut03b] M. Hutter. On the existence and convergence of computable universal 
priors. In Proc. 14th International 
Conf. on Algorithmic Learning The-
ory (ALT-2003), 
volume 2842 of LNAI, pages 298-312, Sapporo, 2003. 
Springer, Berlin. 
[Hut03c] M. Hutter. Optimality of universal Bayesian prediction for general loss 
and alphabet. Journal of Machine Learning Research, 4:971-1000, 2003. 
[Hut03d] M. Hutter. Sequence prediction based on monotone complexity. In Proc. 
16th Annual Conf. on Learning Theory (COLT-2003), 
volume 2777 of 
LNAI, pages 506-521, Washington, DC, 2003. Springer, Berhn. 
[Hut04a] M. Hutter. Sequential predictions based on algorithmic complexity. Jour-
nal of Computer and System Sciences, 2004. to appear. 
[Hut04b] M. Hutter. Universal Artificial Intelligence: Sequential Decisions based on 
Algorithmic Probability. Springer, Berlin, 2004. 
[Jay78] E. T. Jaynes. Where do we stand on maximum entropy? In The Maximum 
Entropy Formalism, pages 15-118. MIT Press, Cambridge, MA, 1978. 
[Jay03] E. T. Jaynes. Probability Theory: The Logic of Science. Cambridge Uni-
versity Press, Cambridge, MA, 2003. 
[Jef83] 
R. C. Jeffrey. The Logic of Decision. University of Chicago Press, Chicago, 
IL, 2nd edition, 1983. 
[Key21] J. M. Keynes. A Treatise on Probability. Macmillan, London, 1921. 
[KHSOla] I. Kwee, M. Hutter, and J. Schmidhuber. Gradient-based reinforcement 
planning in policy-search methods. In Proc. 5th European Workshop on 
Reinforcement Learning (EWRL-5), volume 27, pages 27-29. Onderwijsin-
situut CKI, Utrecht Univ., 2001. 
[KHSOlb] I. Kwee, M. Hutter, and J. Schmidhuber. Market-based reinforcement 
learning in partially observable worlds. In Proc. International 
Conf. on 
Artificial Neural Networks (ICANN-2001), 
volume 2130 of LNCS, pages 
865-873, Vienna, 2001. Springer, Berlin. 
[KLC98] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting 
in partially observable stochastic domains. Artificial Intelligence, 101:99-
134, 1998. 
[Kle36] S. Kleene. General recursive functions of natural numbers. 
Mathematische 
Annalen, 112:727-742, 1936. 
[KLM96] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: 
a survey. Journal of Artificial Intelligence Research, 4:237-285, 1996. 

Bibliography 
257 
[Knu73] D. E. Knuth. The Art of Computer Programming, volume I: Fundamental 
Algorithms. Addison-Wesley, Reading, MA, 1973. 
[K086] 
K.-I. Ko. On the notion of infinite pseudorandom sequences. Theoretical 
Computer Science, 48(l):9-33, 1986. 
[K0I33] A. N. Kolmogorov. 
Grundlagen 
der 
Wahrscheinlichkeitsrechnung. 
Springer, Berlin, 1933. [English translation: Foundations of the Theory 
of Probability. Chelsea, New York, 2nd edition, 1956]. 
[Kol63] A. N. Kolmogorov. On tables of random numbers. Sankhya, the Indian 
Journal of Statistics, Series A 25, 1963. 
[Kol65] A. N. Kolmogorov. Three approaches to the quantitative definition of 
information. Problems of Information and Transmission, l(l):l-7, 1965. 
[Kol83] A. N. Kolmogorov. Combinatorial foundations of information theory and 
the calculus of probabilities. Russian Mathematical Surveys, 38(4):27-36, 
1983. 
[Kra49] L. G. Kraft. 
A device for quantizing, grouping and coding amplitude 
modified pulses. Master's thesis, Electrical Engineering Department, Mas-
sachusetts Institute of Technology, Cambridge, MA, 1949. 
[KS98] 
M. J. Kearns and S. Singh. Near-optimal reinforcement learning in polyno-
mial time. In Proc. 15th International Conf. on Machine Learning, pages 
260-268. Morgan Kaufmann, San Francisco, CA, 1998. 
[KU63] A. N. Kolmogorov and V. A. Uspenskii. On the definition of an algorithm. 
American Mathematical Society Translations, 29:216-245, 1963. [Trans-
lated from Russian Original Uspekhi Matematicheskikh Nauk 13(4): 3-28, 
1958]. 
[KU87] A. N. Kolmogorov and V. A. Uspenskii. 
Algorithms and randomness. 
Theory of Probability and its Applications, 3(32):389-412, 1987. 
[KV86] P. R. Kumar and P. P. Varaiya. Stochastic Systems: Estimation, 
Identifi-
cation, and Adaptive Control. Prentice Hall, Englewood Cliffs, NJ, 1986. 
[KW99] J. Kivinen and M. K. Warmuth. Averaging expert predictions. In Proc. 4th 
European Conf. on Computational Learning Theory (Eurocolt-99), volume 
1572 of LNAI, pages 153-167. Springer, Berlin, 1999. 
[Kyb77] H. E. Kyburg. Randomness and the right reference class. The Journal of 
Philosophy, 74(9):501-521, 1977. 
[Kyb83] H. E. Kyburg. The reference class. Philosophy of Science, 50:374-397, 
1983. 
[Lam87] M. van Lambalgen. Random Sequences. PhD thesis, University of Amster-
dam, 1987. 
[Lapl2] P. Laplace. 
Theorie analytique des probabilites. 
Courcier, Paris, 1812. 
[English translation by F. W. Truscott and F. L. Emory: A Philosophical 
Essay on Probabilities. Dover, 1952]. 
[Lev73a] L. A. Levin. On the notion of a random sequence. Soviet 
Mathematics 
Doklady, 14(5):1413-1416, 1973. 
[Lev73b] L. A. Levin. Universal sequential search problems. Problems of Informa-
tion Transmission, 9:265-266, 1973. 
[Lev74] L. A. Levin. Laws of information conservation (non-growth) and aspects 
of the foundation of probability theory. Problems of Information 
Trans-
mission, 10(3):206-210, 1974. 

258 
Bibliography 
[Lev84] L. A. Levin. Randomness conservation inequalities: Information and in-
dependence in mathematical theories. Information and Control, 61:15-37, 
1984. 
[Li 03] 
M. Li et al. The similarity metric. In Proc. 14th Annual ACM-SIAM 
Sym-
posium on Discrete Algorithms (SODA-03), pages 863-872. ACM Press, 
New York, 2003. 
[Lov69a] D. W. Loveland. On minimal-program complexity measures. In Proc. 1st 
ACM Symposium on Theory of Computing, pages 61-78. ACM Press, New 
York, 1969. 
[Lov69b] D. W. Loveland. A variant of the Kolmogorov concept of complexity. 
Information and Control, 15(6):510-526, 1969. 
[Luc61] J. R. Lucas. Minds, machines, and Godel. Philosophy, 36:112-127, 1961. 
[LV77] 
L. A. Levin and V. V. V'yugin. Invariant properties of informational bulks. 
In Proc. 6th Symposium on Mathematical Foundations of Computer Sci-
ence, volume 53 of LNCS, pages 359-364. Springer, Berhn, 1977. 
[LV91] 
M. Li and P. M. B. Vitanyi. Learning simple concepts under simple distri-
butions. SIAM Journal on Computing, 20(5):911-935, 1991. 
[LV92a] M. Li and P. M. B. Vitanyi. Inductive reasoning and Kolmogorov com-
plexity. Journal of Computer and System Sciences, 44:343-384, 1992. 
[LV92b] M. Li and P. M. B. Vitanyi. Philosophical issues in Kolmogorov complexity 
(invited lecture). In Proceedings on Automata, Languages and Program-
ming (ICALP-92), pages 1-15. Springer, Berhn, 1992. 
[LV97] 
M. Li and P. M. B. Vitanyi. An Introduction to Kolmogorov 
Complexity 
and its Applications. Springer, Berlin, 2nd edition, 1997. 
[LW89] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. 
In 30th Annual Symposium on Foundations of Computer Science, pages 
256-261, Research Triangle Park, NC, 1989. IEEE. 
[LW94J 
N. Littlestone and M. K. Warmuth. The weighted majority algorithm. 
Information and Computation, 108(2):212-261, 1994. 
[MA93] A. W. Moore and C. G. Atkeson. Prioritized sweeping: Reinforcement 
learning with less data and less time. Machine Learning, 13:103-130, 1993. 
[McC80] J. McCarthy. Circumscription-—A form of non-monotonic reasoning. Ar-
tificial Intelligence, 13(l-2):27-39, 1980. 
[McC95] A. K. McCallum. Instance-based utile distinctions for reinforcement learn-
ing with hidden state. In Proc. 12th International Conf. on Machine Learn-
ing, pages 387-395, Tahoe, CA, 1995. Morgan Kaufmann. 
[MD80] D. McDermott and J. Doyle. Nonmonotonic logic 1. Artificial 
Intelligence, 
13:41-72, 1980. 
[MF98] N. Merhav and M. Feder. Universal prediction. IEEE Transactions on 
Information 
Theory, 44(6):2124-2147, 1998. 
[Mic66] D. Michie. Game-playing and game-learning automata. In Advances in 
Programming and Non-Numerical Computation, pages 183-200. Pergamon, 
New York, 1966. 
[Mis 19] R. von Mises. Grundlagen der Wahrscheinlichkeitsrechnung. 
Mathematis-
che Zeitschrift, 5:52-99, 1919. Correction, Ibid., volume 6, 1920, [English 
translation in: Probability, Statistics, and Truth, Macmillan, 1939]. 
[Mis28] R. von Mises. Wahrscheinlichkeit, Statistik und Wahrheit. Springer, Berlin, 
1928. [English translation: Probability, Statistics, and Truth, Allen and 
Unwin, London, 1957]. 

Bibliography 
259 
[ML66] P. Martin-Lof. The definition of random sequences. Information and Con-
trol, 9(6):602-619, 1966. 
[Mor88] H. Moravec. Mind Children: The Future of Robot and Human 
Intelligence. 
Harvard University Press, Cambridge, MA, 1988. 
[Mos65] F. Mosteller. 
Fifty Challenging Problems in Probability with 
Solutions. 
Addison-Wesley, Reading, MA, 1965. 
[NM44] J. Von Neumann and O. Morgenstern. Theory of Games and Economic 
Behavior. Princeton University Press, Princeton, NJ, 1944. 
[Odi89] P. Odifreddi. Classical Recursion Theory, volume 1. North-Holland, Am-
sterdam, 1989. 
[Odi99] P. Odifreddi. Classical Recursion Theory, volume 2. Elsevier, Amsterdam, 
1999. 
[OR94] M. J. Osborne and A. Rubenstein. A Course in Game Theory. The MIT 
Press, Cambridge, MA, 1994. 
[Par95] J. B. Paris. The Uncertain Reasoner's Companion: A Mathematical 
Per-
spective. Cambridge University Press, Cambridge, 1995. 
[Pas54] B. Pascal. Letters to Fermat, 1654. 
[Pen89] R. Penrose. The Emperor's New Mind. Oxford University Press, 1989. 
[Pen94] R. Penrose. Shadows of the Mind, A Search for the Missing Science of 
Consciousness. Oxford University Press, 1994. 
[PF97] 
X. Pintado and E. Fuentes. A forecasting algorithm based on information 
theory. In Objects at Large, page 209. Universite de Geneve, 1997. 
[PH04a] J. Poland and M. Hutter. Convergence of discrete MDL for sequential 
prediction. In Proc. 17th Annual Conf. on Learning Theory 
(COLT~2004), 
volume 3120 of LNAI, pages 300-314, Banff, 2004. Springer, Berlin. 
[PH04b] J. Poland and M. Hutter. On the convergence speed of MDL predictions 
for Bernoulli sequences. In Proc. 15th International 
Conf. on Algorith-
mic Learning Theory (ALT-2004), volume 3244 of LNAI, Padova, 2004. 
Springer, Berlin. 
[Pin64] M. S. Pinsker. Information and Information Stability of Random Variables 
and Processes. Holden-Day, San-Francisco, CA, 1964. [Russian original, 
Izd. Akad. Nauk, I960]. 
[Pop34] K. R. Popper. 
Logik der Forschung. 
Springer, Berlin, 1934. [English 
translation: The Logic of Scientific Discovery Basic Books, New York, 1959, 
and Hutchinson, London, revised edition, 1968]. 
[Pos44] E. L. Post. 
Recursively enumerable sets of positive integers and their 
decision problems. Bulletin of the American Mathematical Society, 50:284-
316, 1944. 
[Put63] H. Putnam. 'Degree of confirmation' and inductive logic. In The Philosophy 
of Rudolf Camap. Open Court, La Salle, IL, 1963. 
[Ram31] F. P. Ramsey. Truth and probability. In The Foundations of Mathematics: 
Collected Papers of Frank P. Ramsey, pages 156-198. Routledge and Kegan 
Paul, London, 1931. 
[Rei49] 
H. Reichenbach. The Theory of Probability: An Inquiry into the Logical 
and Mathematical Foundations of the Calculus of Probability. University 
of California Press, Berkeley, CA, 2nd edition, 1949. 
[Rei80] 
R. Reiter. A logic for default reasoning. Artificial Intelligence, 13:81-132, 
1980. 

260 
Bibliography 
[ResOl] N. Rescher. Paradoxes: Their Roots, Range, and Resolution. Open Court, 
Lanham, MD, 2001. 
[Rin94] M. Ring. Continual Learning in Reinforcement Environments. PhD thesis, 
University of Texas, Austin, 1994. 
[Ris78] 
J. J. Rissanen. Modehng by shortest data description. Automatical 14:465-
471, 1978. 
[Ris89] 
J. J. Rissanen. Stochastic Complexity in Statistical Inquiry. World Scien-
tific, Singapore, 1989. 
[Ris96] 
J. J. Rissanen. Fisher information and stochastic complexity. IEEE Trans 
on Information 
Theory, 42(l):40-47, 1996. 
[RN95] 
S. J. Russell and P. Norvig. Artificial Intelligence. A Modern Approach. 
Prentice-Hall, Englewood Cliffs, NJ, 1995. 
[Rob52] H. Robbins. Some aspects of the sequential design of experiments. Bulletin 
of the American Mathematical Society, 58:527-535, 1952. 
[Rog67] H. Rogers. 
Theory of Recursive Functions and Effective 
Computability. 
McGraw-Hill, New York, 1967. 
[RVOl] 
A. Robinson and A. Voronkov, editors. Handbook of Automated 
Reasoning. 
Elsevier Science, 2001. 
[Sam59] A. L. Samuel. Some studies in machine learning using the game of checkers. 
IBM Journal on Research and Development, 3:210-229, 1959. 
[Sav54] L. J. Savage. The Foundations of Statistics. Wiley, New York, 1954. 
[SB98] 
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. 
MIT Press, Cambridge, MA, 1998. 
[Sch71] C. P. Schnorr. Zufdlligkeit und Wahrscheinlichkeit. Springer, Berlin, 1971. 
[Sch73] C. P. Schnorr. Process complexity and effective random tests. Journal of 
Computer and System Sciences, 7(4):376-388, 1973. 
[Sch80] A. Schonhage. Storage modification machines. SIAM Journal on Comput-
ing, 9(3):490-508, 1980. 
[Sch95] J. Schmidhuber. Discovering solutions with low Kolmogorov complexity 
and high generalization capability. In Proc. 12th International Conf. on 
Machine Learning, pages 488-496. Morgan Kaufmann, 1995. 
[Sch97] J. Schmidhuber. Discovering neural nets with low Kolmogorov complexity 
and high generalization capability. Neural Networks, 10(5):857-873, 1997. 
[Sch99] M. Schmidt. Time-bounded Kolmogorov complexity may help in search for 
extra terrestrial intelligence (SETI). Bulletin of the European Association 
for Theoretical Computer Science, 67:176-180, 1999. 
[SchOO] J. Schmidhuber. Algorithmic theories of everything. Report IDSIA-20-00, 
quant-ph/0011122, IDSIA, Manno (Lugano), Switzerland, 2000. 
[Sch02a] J. Schmidhuber. Hierarchies of generalized Kolmogorov complexities and 
nonenumerable universal measures computable in the limit. 
International 
Journal of Foundations of Computer Science, 13(4):587-612, 2002. 
[Sch02b] J. Schmidhuber. The speed prior: A new simplicity measure yielding near-
optimal computable predictions. In Proc. 15th Conf. on Computational 
Learning Theory (COLT-2002), volume 2375 of LNAI, pages 216-228, Syd-
ney, 2002. Springer, Berlin. 
[Sch03a] J. Schmidhuber. Bias-optimal incremental problem solving. In Advances in 
Neural Information Processing Systems 15, pages 1571-1578. MIT Press, 
Cambridge, MA, 2003. 

Bibliography 
261 
[SchOSb] J. Schmidhuber. Godel machines: Self-referential universal problem solvers 
making provably optimal self-improvements. Report IDSIA-17-03, IDSIA, 
Manno (Lugano), Switzerland, 2003. 
[Sch04] 
J. Schmidhuber. 
Optimal ordered problem solver. 
Machine 
Learning, 
54(3):211-254, 2004. 
[SeaSO] 
J. Searle. Minds, brains, and programs. Behavioral & Brain 
Sciences, 
3:417-458, 1980. 
[SH02] 
J. Schmidhuber and M. Hutter. Universal learning algorithms and opti-
mal search. NIPS 2001 Workshop, 2002. http://www.idsia.ch/~marcus 
/idsia/nipsws.htm. 
[Sha48] C. E, Shannon. A mathematical theory of communication. Bell System 
Technical Journal, 27:379-423, 623-656, 1948. 
[Sha76] G. Shafer. A Mathematical Theory of Evidence. Princeton University Press, 
Princeton, NJ, 1976. 
[Sha85] G. Shafer. 
Conditional probability. 
International 
Statistical 
Review, 
53(3):261-277, 1985. 
[Sho67] J. R. Shoenfield. Mathematical Logic. Addison-Wesley, Reading, MA, 1967. 
[Sho76] E. H. Shortliffe. 
Computer-Based 
Medical Consultations: 
MYCIN. 
Elsevier/North-Holland, Amsterdam, 1976. 
[Slo04] 
N. J. A. Sloane. The on-line encyclopedia of integer sequences. 
AT&T, 
2004. http://www.research.att.com/~njas/sequences/. 
[Sol64] 
R. J. Solomonoff. A formal theory of inductive inference: Parts 1 and 2. 
Information and Control, 7:1-22 and 224-254, 1964. 
[Sol78] 
R. J. Solomonoff. Complexity-based induction systems: Comparisons and 
convergence theorems. 
IEEE Transaction on Information 
Theory, IT-
24:422-432, 1978. 
[Sol86] 
R. J. Solomonoff. The application of algorithmic probability to problems 
in artificial intelligence. In Uncertainty in Artificial Intelligence, pages 
473-491. Elsevier Science/North-Holland, Amsterdam, 1986. 
[Sol97] 
R. J. Solomonoff. 
The discovery of algorithmic probability. Journal of 
Computer and System Sciences, 55(l):73-88, 1997. 
[Sol99] 
R. J. Solomonoff. Two kinds of probabilistic induction. Computer 
Journal, 
42(4):256-259, 1999. 
[StoOl] 
D. Stork. 
Foundations of Occam's razor and parsimony in learn-
ing. 
NIPS 
2001 Workshop, 2001. 
http://www.rii.ricoh.com/~stork 
/ Occam Workshop. ht ml. 
[Str69] 
V. Strassen. Gaussian elimination is not optimal. Numerische 
Mathematik, 
13:354-356, 1969. 
[Sut88] 
R. S. Sutton. Learning to predict by the methods of temporal differences. 
Machine Learning, 3:9-44, 1988. 
[SV88] 
J. I. Seiferas and P. M. B. Vitanyi. Counting is easy. Journal of the ACM, 
35(4):985-1000, 1988. 
[Sze86] 
G.J. Szekely. Paradoxes in Probability Theory and Mathematical 
Statistics. 
Reidel, Dordrecht, 1986. 
[SZW97] J. Schmidhuber, J. Zhao, and M. A. Wiering. 
Shifting inductive bias 
with success-story algorithm, adaptive Levin search, and incremental self-
improvement. Machine Learning, 28:105-130, 1997. 
[Tes94] 
G. Tesauro. 
"TD"-Gammon, a self-teaching backgammon program, 
achieves master-level play. Neural Computation, 6(2):215-219, 1994. 

262 
Bibliography 
[Tri69] 
M. Tribus. Rational Descriptions, Decisions and Designs. Pergamon, New 
York, 1969. 
[Tur36] A. M. Turing. 
On computable numbers, with an application to the 
Entscheidungsproblem. 
Proc. London Mathematical Society, 2(42):230-
265, 1936. 
[Tur50] A. M. Turing. Computing machinery and intelligence. Mind, 1950. 
[USS90] V. A. Uspenskii, A. L. Semenov, and A. K. Shen. Can an individual 
sequence of zeros and ones be random? Russian Mathematical Surveys, 45, 
1990. 
[Val84] 
L. G. Valiant. A theory of the learnable. Communications of the ACM, 
27(11):1134-1142, 1984. 
[Vap99] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, Berlin, 
2nd edition, 1999. 
[VLOO] 
P. M. B. Vitanyi and M. Li. Minimum description length induction, 
Bayesianism, and Kolmogorov complexity. IEEE Transactions on Infor-
mation Theory, 46(2):446-464, 2000. 
[Vog60] W. Vogel. An asymptotic minimax theorem for the two-armed bandit 
problem. Annals of Mathematical Statistics, 31:444-451, 1960. 
[Vov87] V. G. Vovk. On a randomness criterion. Soviet Mathematics 
Doklady, 
35(3):656-660, 1987. 
[Vov92] V. G. Vovk. Universal forecasting algorithms. Information and Computa-
tion, 96(2):245-277, 1992. 
[VovOl] V. G. Vovk. Competitive on-line statistics. International Statistical Review, 
69:213-248, 2001. 
[VV02] N. Vereshchagin and P. M. B. Vitanyi. Kolmogorov's structure functions 
with an application to the foundations of model selection. In Proc. 4^'^^^ 
Symposium on Foundations of Computer Science, pages 751-760, Vancou-
ver, 2002. 
[VW98] V. G. Vovk and C. Watkins. Universal portfolio selection. In Proc. 11th 
Conf. on Computational Learning Theory (COLT-98), pages 12-23. ACM 
Press, New York, 1998. 
[Wal37] A. Wald. Die Widerspruchsfreiheit des Kollektivbegriffs in der Wahrschein-
lichkeitsrechnung. In Ergebnisse eines Mathematischen 
Kolloquiums, vol-
ume 8, pages 38-72, 1937. 
[Wal91] P. Walley. Statistical Reasoning with Imprecise Probabilities. 
Chapman 
and Hall, London, 1991. 
[Wan96] Y. Wang. Randomness and Complexity. PhD thesis, Universitat Heidel-
berg, 1996. 
[Wat89] C. Watkins. Learning from Delayed Rewards. PhD thesis, King's College, 
Oxford, 1989. 
[WB68] C. S. Wallace and D. M. Boulton. An information measure for classification. 
Computer Journal, 11 (2): 185-194, 1968. 
[WD92] C. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279-292, 1992. 
[WM97] D. H. Wolpert and W. G. Macready. No free lunch theorems for opti-
mization. IEEE Transactions on Evolutionary Computation, l(l):67-82, 
1997. 
[WS96] M. A. Wiering and J. Schmidhuber. Solving POMDPs with Levin search 
and EIRA. In Proc. 13th International Conf. on Machine Learning, pages 
534-542, Bari, Italy, 1996. 

Bibliography 
263 
[WS98] M. A. Wiering and J. Schmidhuber. Fast online "Q"(A). Machine Learning, 
33(1):105-116, 1998. 
[Yam98] K. Yamanishi. A decision-theoretic extension of stochastic complexity and 
its applications to learning. IEEE Transactions on Information 
Theory, 
44:1424-1439, 1998. 
[YEYS04] R. Yaroshinsky, R. El-Yaniv, and S. Seiden. How to better use expert 
advice. Machine Learning, 55(3):271-309, 2004. 
[Zad65] L. A. Zadeh. Fuzzy sets. Information and Control, 8:338-353, 1965. 
[Zad78] L. A. Zadeh. Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets 
and Systems, 1:3-28, 1978. 
[Zim91] H.-J. Zimmermann. Fuzzy Set Theory-And Its Applications. Kluwer, Dor-
drecht, 2nd edition, 1991. 
[ZL70] 
A. K. Zvonkin and L. A. Levin. The complexity of finite objects and the 
development of the concepts of information and randomness by means of 
the theory of algorithms. Russian Mathematical Surveys, 25(6):83-124, 
1970. 


Index 
absolute 
distance, 72 
loss function, 89 
absorbing 
environment, 178 
accessibility, 139 
action, 126, 127 
random, 172, 178 
actions 
concurrent, 235 
active 
agent, 234 
system, 110 
adaptive 
control, 127, 147 
Levin search, 59, 235 
agent, 127 
active, 234 
algorithmic, 141 
most intelligent, 147 
prewired, 127 
reactive, 127 
universal, 141 
agents, 126 
bodiless, 238 
embodied, 238 
immortal, 171 
lazy, 171 
mortal, 238 
aggregating strategy, 108 
Al/j, model 
equivalence, 134 
explicit, 132 
functional, 131 
functional form, 126 
iterative, 132 
recursive, 132 
special aspects, 135 
Alp model, 153 
discounted, 159 
functional, 153 
iterative, 154 
AI$ model 
axiomatic approach, 
172 
loss bound, 207 
Pareto optimality, 154, 
160 
prediction, 207 
structure, 172 
AIXI model, 142 
approximation, 241 
computability, 241 
general Bayes mixture, 
148 
generality, 235 
implementation, 241 
optimality, 147 
performance, 235 
AlXltl 
model, 221 
optimality, 227 
algorithm 
best vote, 224 
convergence, 234 
fastest, 210, 215 
hedge, 108 
I/O stream, 220 
incremental, 224 
inversion, 212 
learning, 234 
non-incremental, 224 
optimal, 234 
optimization, 213 
repeated evaluation, 
220 
search, 212 
short, 219 
simple, 212 
speedup, 212 
Strassen, 213 
weighted majority, 108, 
235 
algorithmic 
agent, 141 
information theory, 55 
probability, 58 
specification, 210 
algorithms 
parallel, 212 
alphabet, 128 
continuous, 110 
countable, 110 
infinite, 110 
amplitude 
signal, 96 
Anderson, C. W., 27, 252 
Angluin, D., 27, 59, 119, 
251 
animals, 238 
a-norm 
loss function, 89 
application 
classification, 108 
games of chance, 93 
i.i.d. experiments, 108 
Kolmogorov complex-
ity, 59 
Levin search, 59 
partial sequence 
prediction, 108 
approximable, 38 
(semi)measure, 48 
probability distribu-
tion, 81 
approximation 
AIXI model, 241 
value, valid, 225 
arithmetic, 240 

266 
Index 
artificial intelligence, 2 
achievements, 232 
agents, 125 
big questions, 248 
elegant4->complex, 242 
philosophical issues, 
245 
terminology, 127 
universal, 146 
Asimov, I., 141 
asymmetry, 129 
asymptotic 
convergence, 147 
learnability, 151 
notation, 33 
optimality, 98 
runtime, 210 
Atkeson, C. G., 27, 258 
Auer, P., 109, 172, 178, 
251 
autonomous 
robots, 238 
average 
profit, 94 
reward, 171 
axiom 
induction, 240 
axiomatic approach 
AI^ model, 172 
axiomatic treatment, 239 
axiomatize, 239 
axioms, 215, 216 
Aczel, J., 43, 58, 251 
B 
Babai, L., 221, 252 
Bacchus, F., 56, 252 
background 
knowledge, 236 
balanced 
Pareto optimality, 101, 
155 
Bandit problem, 148, 168 
Baram, Y., 172, 252 
Barron, A. R., 11, 107, 
119, 251, 252 
Bartlett, P L . , 235, 251 
Barto, A. G., 2, 13, 
25-27, 126, 139, 
235, 252, 260 
Baum, E. B., 25, 235, 251 
Baxter, J., 235, 251 
Bayes mixture 
general, 148 
Bayes optimal 
prediction, 82, 86 
Bayes' rule, 31, 42, 44, 63 
Bayes, T., 31, 56, 63, 251 
Bayesian 
self-optimizing policy, 
168 
behahiour 
innate, 238 
belief 
contamination, 180 
probability, 70 
state, 127 
update, 58 
Bellman equations, 127, 
139 
Bellman, R. E., 12, 126, 
139, 231, 251 
Bennett, C. H., 59, 61, 
251, 252 
Bernando's prior, 107 
BernoulH, 40 
process, 106 
sequence, 98 
Bernouin, J., 41, 56, 251 
Berry, D. A., 177, 178, 
252 
Bertsekas, D. P., 2, 12, 
13, 26, 27, 126, 139, 
140, 166, 168, 171, 
181, 235, 251, 252 
bet, 94 
bias, 144 
Blackwell, D., 119, 251 
Blum, M., 20, 22, 211, 
219, 252 
Blumer, A., 59, 251 
Bohr, N., 231 
boosting, 108 
bound, 122, 151, 173 
bootstrap, 103 
Borovkov, A. A., 102, 
119, 252 
Boulton, D. M., 59, 262 
bound 
boost, 122, 151, 173 
entropy, 107 
error, 82, 83 
loss, 86-88, 93 
lower, 96 
probabilistic, 122 
sharp, 97 
tight, 97 
time, 211 
value, 149, 241 
bounded 
horizon, 104 
bounds 
distance, 123 
Kolmogorov complex-
ity, 37 
relative entropy, 72 
brain 
non-computable, 247, 
248 
brain prosthesis 
paradox, 245 
Buchanan, B. G., 57, 252 
Calude, C., 2, 26, 59, 252 
Calude, C. S., 248, 253 
Cantor, G., 55, 252 
Cardano, G., 56, 252 
Carlin, J. B., 57, 254 
Carnap, R., 56, 252 
Cassandra, A. R., 27, 256 
Certainty factors, 57 
Cesa-Bianchi, N., 25, 
109, 109, 112, 172, 
178, 235, 251, 252 
chain rule, 68, 132 
Chaitin, G. J., 4, 26, 36, 
55, 56, 59, 61, 81, 
218, 248, 252, 253 
chaos, 138 
Cheeseman, P., 58, 207, 
253 
chess, 192, 196, 204 
Chinese room 

Index 
267 
paradox, 245 
chronological, 128 
function, 129 
order, 132 
semimeasure, 143, 173, 
179 
Turing machine, 128 
Church thesis, 34 
Church, A., 56, 253 
Churchland, P. S., 245, 
253 
Cihbrasi, R., 26, 59, 253 
circumscription, 57 
Clarke, B. S., 11, 107, 
252 
class 
problem, 210 
classical physics, 246 
classification, 108 
closed-loop 
control, 127 
code 
prefix, 33 
Shannon-Fano, 54, 95 
universal, 36 
combining experts 
prediction, 109 
compiler 
thesis, 34 
complete 
history, 139 
complexity 
incomputable, 219 
increase, 61 
input sequence, 138 
Kolmogorov, 33 
of functions, 219 
of games, 195 
parametric, 106 
compression 
Lempel-Ziv, 81 
computability 
AIXI model, 241 
computable 
^free will, 246 
(semi)measure, 48 
finite, 38 
probability distribu-
tion, 46 
recursive, 38 
computation 
time, 215 
concept class 
restricted, 148 
concepts 
separability, 149 
concurrent 
actions and percep-
tions, 235 
consciousness, 248 
consistency, 147 
consistent 
control, 127 
policy, 146 
constants, 138 
contamination 
belief, 180 
Conte, M., 59, 253 
continuity, 237 
continuous 
alphabet, 110 
entropy bound, 107 
forecast, 110 
hypothesis class, 106 
probability class, 106, 
168 
semimeasure, 46 
value, 162, 180 
weights, 106 
control, 127 
adaptive, 127, 147 
closed-loop, 127 
consistent, 127 
open-loop, 127 
self-optimizing, 127 
self-tuning, 127 
stochastic, 127 
theory, 127 
controlled 
Markov chain, 127 
controller, 127 
convergence 
M, 71 
^ to /i, 74 
^^^ to fi^\ 145 
algorithm, 234 
arbitrary horizon, 105 
asymptotic, 147 
bounded horizon, 104 
finite, 147 
generalized, 76 
in mean sum, 71 
in probability, 71 
in the mean, 71 
individual, 121 
Martin-Lof, 60, 71, 76, 
121 
of averages, 157, 179 
of instantaneous loss, 
91 
random sequence, 71 
rate, 157, 161 
relations, 71 
semi-martingale, 75 
speed, 75, 121 
unbounded horizon, 
123 
uniform, 152 
value, 156-158, 161, 
163 
with high probability, 
120 
with probability 1, 71 
convexity 
value, 154, 160 
Coppersmith, D., 213, 
253 
cost 
expected, 127 
immediate, 127 
total, 127 
countable 
alphabet, 110 
probability class, 81 
counting, 218 
Cover, T. M., 56, 119, 
253 
Cox's axioms, 43, 58 
variants, 58 
Cox's theorem, 43, 58 
loopholes, 58 
Cox, R. T., 5, 43, 56, 58, 
253 
creator, 125 
cryptography, 237 
RSA, 237 
Csiszar, L, 119, 253 

268 
Index 
cumulative 
reward, 127 
cumulatively enumerable 
semimeasure, 81 
curvature, 176 
Gauss, 176 
matrix, 106 
cybernetic systems, 126 
cycle, 128 
cylinder sets, 46, 69 
D 
Daley, R. P., 26, 56, 222, 
253 
data 
efficiency, 234 
Dauben, J. W., 55, 253 
Dawid, A. P., 4, 32, 58, 
109, 253 
Dayan, P., 27, 262 
dead code, 219 
decision 
optimal, 86, 139 
suboptimal, 151 
wrong, 151 
decomposition, 136 
decryption, 237 
default reasoning, 57 
degree of belief, 5, 40, 43 
delayed 
prediction, 105 
Dempster, A. P., 57, 253 
Dempster-Shafer theory, 
57 
density 
error, 84 
deterministic, 127 
^^-^free will, 246 
environment, 128 
optimal policy, 160 
dice example, 95 
differential 
gain, 171 
discounted 
Alp model, 159 
value, 159 
discounting, 159, 167 
finite, 170 
general, 169 
geometric, 170 
harmonic, 170 
power, 170 
universal, 170 
discrete 
(semi)measure, 50 
probability class, 81 
distance 
absolute, 72 
bounds, 123 
Euclidian, 72 
Helhnger, 72 
Kullback-Leibler, 72 
quadratic, 72 
relative entropy, 72 
square, 72 
distance measures 
probability distribu-
tion, 72 
dominance 
value, 140 
Doob, J. L., 69, 75, 162, 
254 
Doyle, J., 57, 258 
Dubins, L., 119, 251 
dynamic 
horizon, 170 
programming, 127 
E 
economy-based RL, 235 
effective 
horizon, 167 
efficiency, 147 
data, 234 
time, 234 
Ehrenfeucht, A., 59, 251 
Einstein, A., 1, 209 
El-Yaniv, R., 109, 172, 
252, 263 
embodied 
agents, 238 
encrypted 
information, 237 
entropy 
bound,107 
inequalities, 72 
inequality, 145, 175 
relative, 72, 107 
enumerable, 38 
(semi)measure, 48 
chronological semimea-
sure, 179 
semimeasure, 81, 174 
weights, 102 
enumeration 
proof, 215 
environment, 127 
absorbing, 178 
deterministic, 128 
ergodic, 165, 181 
factorizable, 152 
farsight ed, 152 
forgetful, 152, 181 
general, 153 
incomputable, 180 
inductive, 150 
influence, 110 
known, 125 
Markov, 152 
passive, 150 
probabilistic, 130 
pseudo-passive, 149, 
150 
random, 103 
real, 238 
relevant, 180 
self-optimizing, 180, 
207 
stationary, 152 
uniform, 152 
weakly forgetful, 61 
environmental class, 207 
limited, 148 
others, 168 
Epicurus' principle, 31 
episode, 137 
equivalence 
non-provable, 220 
provable, 215 
ergodic 
environment, 165, 181 
MDP, 165, 180 
Erickson, G. W., 177, 254 
error 
bound, 82, 83 
density, 84 
expected, 82 

Index 
269 
finite, 83 
instantaneous, 82 
loss function, 89 
minimize, 82 
posterization, 208 
probabilistic bound, 
122 
regret, 84 
total, 82 
error bound 
exponential, 191 
lower, 96, 120 
sharp, 97 
tight, 97 
estimable, 38 
(semi)measure, 48 
estimate 
parameter, 107 
Euclidian 
distance, 72 
loss function, 89 
evaluation 
of function, 198 
event, 41 
evolution, 248 
expected 
cost, 127 
error, 82 
loss, 87 
utility, 139 
expectimax 
algorithm, 134 
tree, 134 
experiment, 41 
i.i.d., 108 
expert advice 
prediction, 172, 235 
expert systems, 57 
explicit 
Alfi model, 132 
exploitation, 127, 140, 
234 
exploration, 140, 234 
F 
factorizable 
environment, 135, 152 
fair coin flips, 6, 46 
farsighted 
environment, 152 
farsightedness 
dynamic, 131, 133 
fast 
matrix multiplication, 
213 
fastest 
algorithm, 210 
Feder, M., 26, 56, 67, 81, 
83, 88, 108, 109, 
119, 241, 254, 258 
feedback 
more, 196 
negative, 129 
positive, 129 
Feller, W., 57, 254 
Ferguson, T. S., 102, 254 
finance, 243 
Fine, T. L., 57, 58, 254 
Finetti, B., 56, 254 
finite 
computable, 38 
convergence, 147 
discounting, 170 
error, 83 
state space, 235 
finite-state automata, 81 
Fisher information, 106 
Fisher, R. A., 56, 254 
Fitting, M. C , 215, 254 
fixed 
horizon, 169 
fixed-point 
prediction, 247 
forecast 
continuous, 110 
probabilistic, 110 
forgetful 
environment, 61, 152, 
181 
formal 
specification, 210 
formula, 215 
Fortnow, L., 221, 252 
Fossa, J. A., 177, 254 
free lunch, 103 
free will 
paradox, 246 
frequentist, 40 
Freund, Y., 110, 172, 178, 
251, 254 
Fristedt, B., 177, 178, 252 
Fudenberg, D., 192, 254 
Fuentes, E., 26, 56, 259 
function 
complexity of, 219 
concave, 88 
evaluations, 198 
loss, 86 
minimize, 197 
recursive, 174 
function approximation 
general, 235 
linear, 235 
function minimization, 
197 
greedy, 198 
inventive, 201 
with AIXI, 202 
functional 
AI^ model, 131 
Alp model, 153 
Furst, M., 209 
future 
reward, 159 
value, 159 
Fuzzy 
logic, 57 
sets, 57 
systems, 57 
G 
gain 
diff"erential, 171 
Gallager, R. G., 55, 254 
game playing 
with AIXI, 195 
game theory, 192, 235 
games 
chess, 204 
complexity, 195 
of chance, 93 
repeated, 194 
strategic, zero-sum, 
192 
variable lengths, 195 
Gardner, M., 61, 252 
Gauss 

270 
Index 
curvature, 176 
Gelman, A., 57, 254 
general 
Bayes mixture, 148 
discounting, 169 
environment, 153 
loss bound, 93 
loss function, 92 
property, 237 
weights, 153 
general Bayes mixture 
AIXI model, 148 
generalization, 234 
techniques, 140 
generalized 
convergence, 76 
random sequence, 54 
universal prior, 142 
generalized universal 
prior, 59 
genetic algorithms, 241 
Gentile, C , 109, 251 
geometric 
discounting, 170 
geometric discounting 
self-optimizing, 179 
Ginsberg, M. L., 57, 254 
Gittins, J. C., 177, 178, 
254 
Gold Standard, 231 
greedy, 131 
function minimization, 
198 
strategy, 110 
group 
simple, 31 
Grove, A., 56, 252 
Griinwald, P. D., 59, 119, 
177, 254 
Gutman, M., 26, 56, 81, 
83, 241, 254 
Godel 
incompleteness, 245, 
248 
machine, 235, 241 
Godel, K., 55, 220, 254 
Gacs, P., 4, 32, 55, 62, 
81, 111, 254, 255 
H 
Hacking, I., 57, 255 
Hald, A., 57, 255 
Halpern, J. Y., 56, 58, 
177, 252, 254, 255 
halting 
probability, 61 
problem, 55 
sequence, 61 
harmonic 
discounting, 170 
Hartmanis, J., 220, 255 
Haussler, D., 59, 109, 
251, 255 
HeavenHell example, 149, 
178 
Heckerman, D. E., 58, 
255 
hedge algorithm, 108 
Hellinger 
distance, 72 
loss function, 89 
Hertling, P. H., 248, 253 
history, 130 
complete, 139 
holographic proofs, 221 
Hopcroft, J. E., 26, 34, 
61, 255 
horizon, 133 
arbitrary, 105 
bounded, 104 
choice, 139 
convergence, 105 
dynamic, 170 
effective, 167 
fixed, 169 
infinite, 170 
problem, 169 
unbounded, 123 
Horvitz, E. J., 58, 255 
Hughes, R. I. G., 57, 255 
human, 138, 238 
Hume, D., 31, 255 
Hutter, M., xvii, 25, 27, 
55, 60, 62, 73, 76, 
84, 103, 110, 111, 
119, 122, 123, 145, 
165, 235, 255, 256, 
259, 261 
hypothesis class 
continuous, 106 
i.i.d., 165 
experiments, 108 
process, 106 
I/O sequence, 129 
identification 
system, 127 
ignorance, 57 
image, 138 
immediate 
cost, 127 
immortal 
agents, 171 
imperfect human, 138 
implementation 
AIXI model, 241 
imprecise 
probability, 57 
incompleteness 
Godel, 245, 248 
theorem, 220 
incomputable 
complexity, 219 
environment, 180 
inconsistent 
policy, 146, 161 
increase 
complexity, 61 
incremental 
algorithm, 224 
independent 
episodes, 137 
indifference principle, 45 
individual 
convergence, 121 
induction, 67 
axiom, 240 
principle, 240 
universal, 58 
inductive 
environment, 150 
logic, 56 
inequality 
distance measures, 72 

Index 
271 
entropy, 145, 175 
Jensen, 75, 88 
Kraft, 33 
inference 
rules, 215 
transductive, 32 
infinite 
action space, 243 
alphabet, 110 
horizon, 170 
perception space, 243 
prediction space, 110 
influence 
environment, 110 
information, 95 
encrypted, 237 
Fisher, 106 
redundant, 188 
state, 127 
symmetry, 37 
transmission, 96 
information theory 
algorithmic, 55 
informed 
prediction, 83, 87 
input, 126, 127 
device, 138 
regular, 129 
reward, 129 
word, 128 
input space 
choice, 139, 236 
instantaneous 
error, 82 
loss, 87, 91 
loss bound, 91 
reward, 127 
intelligence, 126 
aspects, 206 
effective order, 226 
intermediate, 147 
order relation, 146 
interference, 57 
intermediate 
intelligence, 147 
internal 
reward, 238 
inventive 
function minimization, 
201 
inversion 
problem, 212 
investment, 93 
iterative 
Aljj, model, 132 
Alp model, 154 
Jaynes, E. T,, 56-58, 256 
Jeffrey, R. C , 56, 256 
Jeffreys' prior, 107 
Jensen's inequality, 75, 
88 
Jones, D. M., 177, 254 
K 
Kaelbling, L. P., 13, 27, 
139, 256 
Kearns, M. J., 27, 169, 
257 
Keynes, J. M., 56, 256 
Khoussainov, B., 248, 
253 
Kivinen, J., 109, 110, 
255, 257 
Kleene, S., 55, 256 
knowledge 
background, 236 
incorporate, 242 
physical, 237 
prior, 236 
universal prior, 237 
Knuth, D. E., viii, 257 
Ko, K.-L, 26, 56, 222, 257 
Koller, D., 56, 252 
Kolmogorov axioms, 41 
Kolmogorov complexity, 
33, 36, 37 
algorithmic properties, 
39 
application, 59 
bounds, 37 
definition, 37 
information properties, 
37 
oracle properties, 61 
time-bounded, 56 
time-limited, 222 
variants, 55 
Kolmogorov, A. N., 4, 26, 
36, 55, 56, 59, 81, 
218, 257 
Kolmogorov-Uspenskii 
machine, 218 
Kraft inequality, 33 
Kraft, L. G., 55, 217, 257 
KuUback-Leibler 
divergence, 72 
Kumar, P. R., 16, 27, 32, 
147, 149, 156, 166, 
168, 177, 257 
Kwee, L, 25, 235, 256 
Kyburg, H. E., 56, 257 
Lagrange multiplier, 118, 
175 
Lagrange,. J., 125 
Lambalgen, M., 55, 57, 
257 
Langlotz, C. P., 58, 255 
Laplace' rule, 30, 63 
Laplace, P., 56, 63, 65, 
125, 182, 236, 257 
lazy 
agents, 171 
learnable 
asymptotically, 151 
task, 146 
Turing machine, 121 
learning, 71, 111 
a relation, 204 
algorithm, 234 
by reinforcement, 127, 
139, 235 
by temporal difference, 
235 
model, 127 
rate, 140 
supervised, 204 
with expert advice, 108 
Lempel-Ziv compression, 
81 
Levin search, 212 
adaptive, 59, 235 

272 
Index 
application, 59 
Levin, L. A., 4, 7, 20, 25, 
26, 47-49, 54-56, 
58, 59, 68, 81, 212, 
218, 221, 252, 257, 
258, 263 
Li, M., 2, 23, 26, 27, 30, 
33, 51, 52, 55, 56, 
59-62, 68, 69, 73, 
75, 76, 81, 82, 93, 
111, 119, 174, 179, 
211-213, 217, 222, 
241, 258, 262 
lifetime, 129, 138 
limited 
environmental class, 
148 
limits, 138 
linear 
function approxima-
tion, 235 
linearity 
value, 154, 160 
Littlestone, N., 26, 59, 
109, 172, 222, 258 
Littman, M. L., 13, 27, 
139, 256 
locality, 237 
logarithmic 
loss function, 89 
logic 
inductive, 56 
planners, 235 
system, 215 
lookahead 
multistep, 104 
lookup-table 
paradox, 245 
loss 
arbitrary, 87 
bound, 86-88, 93 
expected, 87 
function, 86 
instantaneous, 87, 91 
minimize, 86 
total, 87 
loss bound 
AI^ model, 207 
general, 93 
instantaneous, 91 
structure, 109 
with high probability, 
120 
loss function 
absolute, 89 
a-norm, 89 
arbitrary, 87 
error, 89 
Euclidian, 89 
examples, 89 
general, 92 
Hellinger, 89 
logarithmic, 89 
quadratic, 89 
square, 89 
static, 92 
time-dependent, 92 
Loveland, D. W., 56, 258 
Lucas, J. R., 222, 245, 
248, 258 
Lugosi, G., 112, 252 
Lutz, K., 172, 252 
M 
machine 
Godel, 235, 241 
Kolmogorov-Uspenskii, 
218 
pointer, 218 
machine learning, 239 
application, 240 
categorization, 240 
framework, 239 
nonstandard, 240 
theory, 240 
machine model, 218 
Macready, W. G., 10, 98, 
103, 262 
majorization 
multiplicative, 46 
manipulation, 238 
market-based RL, 235 
Markov 
/c*^-order, 152 
chain, controlled, 127 
decision process, 127, 
165 
environment, 152 
property, 139 
Martin-Lof, 41, 59 
convergence, 60, 76, 
121 
random sequence, 54, 
60 
Martin-Lof, R, 41, 56, 
59, 259 
martingales, 75, 119 
mathematical 
specification, 210 
matrix multiplication 
fast, 213 
maximize 
profit, 94 
reward, 129 
maximum entropy 
principle, 45 
Maximum Likelihood 
prediction, 111 
McCallum, A. K., 235, 
258 
McCarthy, J., 57, 258 
McDermott, D., 57, 258 
MDL 
prediction, 123 
MDP, 127, 165 
ergodic, 165, 180 
stationary, 165 
uniform mixture, 181 
measure 
approximable, 48 
computable, 48 
discrete, 50 
enumerable, 48 
estimable, 48 
universal, 49 
Merhav, N., 26, 56, 67, 
81, 83, 88, 108, 109, 
119, 241, 254, 258 
Michie, D., 27, 134, 258 
minimize 
error, 82 
function, 197 
loss, 86 
minimum description 
length, 123, 240 
minimum message length, 
240 

Index 
273 
Mises, R., 56, 258 
mixing rate, 169 
mixture 
uniform MDP, 181 
mixtures 
comparision, 120 
model 
AIXI, 142 
AlXIt/, 221 
causal, 32 
learning, 32, 127 
predictive, 32 
true, 32 
universal, 142 
modus ponens, 216 
money, 93 
monitor, 138 
monotone 
Turing machine, 35 
Monte Carlo, 169 
Moore, A. W., 13, 27, 
139, 256, 258 
Moravec, H., 245, 259 
Morgenstern, O., 27, 126, 
192, 259 
mortal 
agents, 238 
Mosteller, F., 177, 259 
Motwani, R., 26, 34, 61, 
255 
Moullagaliev, A., 102, 
119, 252 
Muchnik, A. A., 60, 76, 
255 
multi-agent system, 244 
multiplicative 
domination, 70 
majorization, 46 
multistep 
lookahead, 104 
prediction, 104 
N 
nanobots, 57 
Napoleon, B., 125 
natural 
numbers, 33, 240 
Turing machine, 34 
Neumann, J. V., 27, 126, 
192, 209, 259 
neural net, 59 
no free lunch, 103 
noise, 138 
noisy world, 138 
non-computable 
brain, 247, 248 
physics, 248 
non-provable 
equivalence, 220 
nondeterministic world, 
138 
nonmonotonic logic, 57 
nonprobabilistic ap-
proaches, 58 
Norvig, R, 11, 12, 26, 27, 
57, 125, 134, 139, 
192, 235, 260 
number 
natural, 33, 240 
of wisdom, 61, 248 
prime, 31 
o 
object, 237 
objective 
probability, 41 
objectivist, 5, 40, 41 
observation, 127 
Occam's razor, 29, 31, 
45, 103, 239 
Ockham, W., 29 
Odifreddi, P., 55, 259 
Only One example, 150 
open-loop 
control, 127 
optimal 
algorithm, 234 
decision, 86, 139 
deterministic policy, 
160 
pohcy, 139 
problem solver, 59 
value, 153 
weights, 103 
optimality 
AIXI model, 147 
AlXIt/, 227 
asymptotic, 98 
by construction, 148 
Pareto, 99 
properties, 96 
universal, 146, 147 
optimization 
problem, 197, 213 
oracle properties 
Kolmogorov complex-
ity, 61 
order relation 
effective intelligence, 
226 
intelligence, 146 
universal, 147 
Osborne, M. J., 140, 192, 
235, 259 
outcome, 41 
output, 126, 127 
device, 138 
word, 128 
output space 
choice, 139, 236 
paradox 
brain prosthesis, 245 
Chinese room, 245 
free will, 246 
lookup-table, 245 
parallel 
algorithms, 212 
parameter 
estimate, 107 
parametric 
complexity, 106 
Pareto optimality, 99, 
120, 147, 179 
AI^ model, 154, 160 
balanced, 101, 155 
Paris, J. B., 43, 58, 259 
Pascal, B., 56, 259 
passive 
environment, 150 
Peano axioms, 240 
Penrose, R., 222, 245, 
248, 259 
perception, 126, 127 
perceptions 

274 
Index 
concurrent, 235 
perfect, 138 
performance 
AIXI model, 235 
sequence prediction, 
235 
physical 
knowledge, 237 
random process, 41 
physics 
classical, 246 
non-computable, 248 
quantum, 138, 246 
wave function collapse, 
248 
Pinsker, M. S., 119, 259 
Pintado, X., 26, 56, 259 
planners 
logic, 235 
plausibility, 43 
player, rational, 192 
pointer 
machine, 218 
Poland, J., 110, 111, 123, 
235, 255, 259 
policy, 127, 139 
consistent, 146 
extended chronological, 
224 
inconsistent, 146, 161 
optimal, 139 
optimal deterministic, 
160 
probabilistic, 140, 159 
restricted class, 168 
self-optimizing, 148, 
157, 161, 165 
stationary, 165 
policy iteration, 127, 139, 
235 
Popper, K. R., 56, 259 
portfolio, 93 
Possibility theory, 57 
Post, E. L., 55, 259 
posterior probability, 70 
post erizat ion, 122, 151, 
173, 236 
prediction error, 208 
power 
discounting, 170 
prediction 
AI^ model, 207 
arbitrary, 87 
Bayes optimal, 82, 86 
combining experts, 109 
delayed, 105 
expert advice, 172, 235 
fixed-point, 247 
informed, 83, 87 
Laplace' rule, 63 
Maximum Likelihood, 
111 
MDL, 123 
multistep, 104 
of aspects, 123 
reduction, 123 
selected bits, 63 
self-contradictory, 247 
universal, 83 
with expert advice, 108 
prediction error 
posterization, 208 
prediction space 
infinite, 110 
prefix 
code, 33 
property, 128 
Turing machine, 35 
prequential approach, 32, 
58 
prewired 
agent, 127 
prime numbers, 31 
principle 
of indifference, 45 
of maximum entropy, 
45 
of parsimony, 45 
of simplicity, 45 
of symmetry, 45 
prior 
Bernando, 107 
determination, 45 
generalized universal, 
142 
Jeffreys, 107 
knowledge, 236 
probability, 69 
Solomonoff, 102 
Speed, 81, 119 
universal knowledge, 
237 
probabilistic 
environment, 130 
forecast, 110 
policy, 140, 159 
probability, 40 
algorithmic, 58 
axioms, 41 
belief, 70 
circular definition, 41 
classes, 81 
complex-valued, 57 
conditional, 42 
distribution, 42, 132 
frequency, 40 
halting, 61 
imprecise, 57 
mass function, 42 
measure, 42, 46, 69 
nearby distribution, 80 
objective, 41, 245 
physical, 41 
prior, 45, 69 
second-order, 57 
subjective, 43, 246 
sunrise, 30 
probability class 
continuous, 106, 168 
countable, 81 
discrete, 81 
probability distribution, 
42, 132 
approximable, 81 
computable, 46, 81 
conditional, 68, 70, 132 
distance measures, 72 
generating, 69 
generic class, 82 
nearby, 80 
over values, 243 
posterior, 70 
simple, 81 
Solomonoff, 81 
true, 69 
universal, 69, 81 
unknown, 69 

Index 
275 
probability theory 
alternatives, 57 
history, 56 
problem 
class, 210 
horizon, 169 
inversion, 212 
optimal solver, 59 
optimization, 197, 213 
poorly specified, 215 
relevant, 152 
satisfiability, 214 
solvable, 146 
traveling salesman, 
197, 203 
process 
Bernoulli, 106 
i.i.d., 106 
random, 41 
profit, 93 
average, 94 
maximize, 94 
program 
extended chronological, 
224 
fastest, 210 
search, 213 
proof, 211, 241 
enumeration, 215 
holographic, 221 
search, 213 
theory, 215 
property 
general, 237 
provable 
equivalence, 215 
pseudo-passive 
environment, 149, 150 
pseudo-random, 246 
Putnam, H., 56, 259 
Q 
quadratic 
distance, 72 
loss function, 89 
quantum logic, 57 
quantum physics, 138, 
246 
R 
Ramsey, F. P., 56, 259 
random 
action, 172, 178 
environment, 103 
physical process, 41 
pseudo, 246 
random sequence, 68 
convergence, 71 
convergence relations, 
71 
generalized, 54 
individual, 60 
Martin-Lof, 54, 60 
rate 
convergence, 157, 161 
rational player, 192 
reactive 
agent, 127 
real 
environment, 238 
recursive 
Al/i model, 132 
computable, 38 
function, 174 
reduction 
prediction, 123 
state space, 139 
redundant 
information, 188 
reference class problem, 
41, 56 
reflex, 238 
regression, 32 
regret 
error, 84 
regularization, 83 
Reichenbach, H., 56, 58, 
259 
reinforcement learning, 
127, 139 
classical algorithms, 
235 
economy-based, 235 
marked-based, 235 
policy gradient, 235 
Reiter, R., 57, 259 
relation 
value difference, 160 
relative entropy, 72, 107 
distance, 72 
relevant 
environment, 180 
problem, 152 
Rescher, N., 177, 260 
restricted 
concept class, 148 
domain, 140 
reward, 127, 139 
average, 171 
cumulative, 127 
future, 129, 159 
instantaneous, 127 
internal, 238 
maximize, 129 
mean,median,quantile, 
243 
total, 127, 129 
Ring, M., 235, 260 
Rissanen, J. J., 26, 38, 
59, 107, 109, 241, 
260 
Robbins, H., 177, 260 
Robinson, A., 221, 260 
robots 
autonomous, 238 
the 3 laws, 141 
robust 
reasoning, 57 
robustness, 243 
Rogers, H., 55, 260 
RSA 
cryptography, 237 
Rubenstein, A., 140, 192, 
235, 259 
Rubin, D. B., 57, 254 
rules 
inference, 215 
runtime 
asymptotic, 210 
Russell, S. J., 11, 12, 26, 
27, 57, 125, 134, 
139, 192, 235, 260 
<j-algebra, 46, 69 
sample space, 41, 46, 69 

276 
Index 
Samuel, A. L., 27, 260 
Savage, L. J., 56, 260 
scaling 
AIXI down, 241 
Schapire, R. E., 110, 172, 
178, 251, 254 
Schmidhuber, J., 7, 
25-27, 50, 55, 56, 
59, 81, 102, 103, 
119, 214, 219, 221, 
222, 235, 242, 246, 
256, 260-263 
Schmidt, M., 26, 59, 260 
Schnorr, C. P., 55-57, 
260 
Schonhage, A., 218, 260 
science, 240 
search 
adaptive Levin, 235 
algorithm, 212 
Levin, 212 
program, 213 
proof, 213 
time bound, 213 
Searle, J., 245, 261 
Seiden, S, 109, 263 
Seiferas, J. L, 218, 261 
self-optimizing, 32, 147 
Bayesian policy, 168 
control, 127 
environment, 180, 207 
geometric discounting, 
179 
policy, 148, 157, 161, 
165 
self-tuning, 32, 147 
control, 127 
Semenov, A. L., 59, 262 
semi-martingale 
convergence, 75 
semicomputable, 38 
semimeasure, 119 
approximable, 48 
chronological, 143, 173, 
179 
computable, 48 
continuous, 46 
cumulatively enumer-
able, 81 
defect of M, 62 
discrete, 50 
enumerable, 48, 81, 
145, 174, 179 
estimable, 48 
Solomonoff, 62, 81 
time-limited, 222 
universal, 45, 49 
separability 
concepts, 149 
sequence, 128 
Bernouin, 98 
halting, 61 
random, 68 
training, 242 
sequence prediction 
examples, 30 
generality, 235 
performance, 235 
Solomonoff, 47 
universal, 67 
with Al/i, 188 
with AIXI, 187, 190 
sequential decision 
theory, 127, 139 
set 
fuzzy, 57 
prefix-free, 128 
Shafer, G., 57, 177, 261 
Shannon, C. E., 56, 217, 
261 
Shannon-Fano 
prefix code, 54, 95 
Shen, A. K., 59, 262 
Shoenfield, J. R., 55, 215, 
261 
short 
algorithm, 219 
Shortliffe, E. H., 57, 252, 
261 
side information, 37 
signal 
amplitude, 96 
simple 
algorithm, 212 
group, 31 
simplex, 118 
Singh, S., 27, 169, 257 
Sloane, N. J. A., 31, 261 
Smith, C. H., 27, 59, 119, 
251 
Solomonoff, 32 
prior, 102 
semimeasure, 62, 81 
Solomonoff, R. J., 6, 7, 
20, 26, 27, 32, 36, 
45, 46, 48, 52, 55, 
56, 58, 59, 67, 68, 
73, 75, 81, 119, 200, 
213, 218, 231, 242, 
261 
solution 
verification, 212 
solvable 
problem, 146 
specification 
algorithmic, 210 
formal, 210 
mathematical, 210 
speed 
convergence, 121 
Speed prior, 59, 81, 119 
speedup 
algorithm, 212 
theorem, 210, 218 
split trees, 235 
square 
distance, 72 
loss function, 89 
state, 139 
belief, 127 
environmental, 139 
information, 127 
internal, 126 
state space, 165 
finite, 235 
static 
loss function, 92 
stationary, 139 
environment, 152 
MDP, 165 
policy, 165 
statistics, 69 
Stern, H. S., 57, 254 
stochastic 
control, 127 
stock market, 93 
Stork, D., 103, 261 

Index 
277 
Strassen, V., 213, 261 
algorithm, 213 
strategy 
aggregating, 108 
die selection, 95 
games, 192 
greedy, 110 
winning, 94 
string 
empty, 68, 128 
length, 128 
strings, 33, 128 
structural risk minimiza-
tion, 240 
structure 
AI^ model, 172 
loss bound, 109 
subjective 
perspective, 237 
subjectivist, 5, 40, 43 
suboptimal 
decision, 151 
supervised learning 
from examples, 204 
with AIXI, 204 
support vector machines, 
240 
Sutton, R. S., 2, 13, 
25-27, 126, 139, 
185, 235, 252, 260, 
261 
symmetry 
information, 37 
principle, 45 
system, 127 
active, 110 
identification, 127 
logic, 215 
Szegedy, M., 221, 252 
Szekely, G. J., 177, 261 
tape 
bidirectional, 35, 45, 
128 
unidirectional, 35, 45, 
128 
task 
learnable, 146 
temporal difference 
learning, 127, 235 
term, 215 
Tesauro, G., 27, 261 
theorem prover, 241 
elaborate, 221 
theory, see particular 
theory 
Thomas, J. A., 56, 119, 
253 
threshold 
winning, 95 
time 
computation, 215 
efficiency, 234 
to win, 94 
time bound, 211 
AlXIt;, 226 
search, 213 
time-bounded 
Kolmogorov complex-
ity, 56 
time-dependent 
loss function, 92 
Tirole, J., 192, 254 
total 
cost, 127 
error, 82 
loss, 87 
reward, 127 
trader, 93 
training 
sequence, 242 
transductive 
inference, 32 
transition matrix, 127, 
165 
transmission 
information, 96 
transparent proofs, 221 
traveling salesman 
problem, 197, 203 
tree 
split, 235 
Tribus, M., 43, 58, 262 
Tromp, J., 32, 111, 255 
Tsitsiklis, J. N., 2, 12, 
13, 26, 27, 126, 139, 
166, 168, 235, 252 
Turing 
test, 245 
thesis, 34 
Turing machine, 127 
chronological, 128 
head, 128 
learnable, 121 
monotone, 35 
natural, 34 
prefix, 35, 45 
universal, 35, 45 
unnatural, 60 
Turing, A. M., 36, 55, 
125, 245, 262 
typing monkeys, 222 
u 
Ullman, J. D., 26, 34, 61, 
255 
unbiasedness, 147 
unbounded horizon 
convergence, 123 
uncertainty, 40 
sources, 40 
underline, 132 
uniform 
convergence, 152 
environment, 152 
MDP mixture, 181 
universal 
(semi)measure, 49 
agent, 141 
AIXI, 144 
AIXI model, 142 
artificial intelligence, 
146 
code, 36 
discounting, 170 
element, 145 
generalized prior, 142 
induction, 58 
optimality, 146, 147 
order relation, 147 
prediction, 83 
prior knowledge, 237 
probability dis-
tribution, 69, 
81 

278 
Index 
time-limited semimea-
sure, 222 
Turing machine, 35 
universality property, 144 
universe, 125, 139 
update weights, 109 
Uspenskii, V. A., 59, 218, 
257, 262 
utility, 129 
expected, 139 
V 
V'yugin, V. V., 58, 258 
vagueness, 57 
Valiant, L. G., 26, 59, 262 
valid approximation 
value, 225 
value 
bound, 149, 241 
continuous, 162, 180 
convergence, 156-158, 
161, 163 
convexity, 154, 160 
difference relation, 156, 
160 
discounted, 159 
dominance, 140 
function, 153 
future, 159 
justification, 228 
linearity, 154, 160 
median, 243 
optimality results, 153 
quantile, 243 
valid approximation, 
225 
value iteration, 127, 139, 
235 
Vapnik, V. N., 32, 262 
Varaiya, R P., 16, 27, 32, 
147, 149, 156, 166, 
168, 177, 257 
Vereshchagin, N., 32, 262 
verification 
of solution, 212 
video camera, 138 
Vitanyi, P. M. B., 2, 
23, 26, 27, 30, 32, 
33, 51, 52, 55, 56, 
59-62, 68, 69, 73, 
75, 76, 81, 82, 93, 
111, 119, 174, 179, 
211-213, 217, 218, 
222, 241, 253, 255, 
258, 261, 262 
Vogel, W , 178, 262 
Voronkov, A., 221, 260 
vote 
best, 224 
democratic, 222 
Vovk, V. G., 26, 59, 60, 
76, 109, 119, 121, 
172, 222, 262 
w 
Wald, A., 56, 262 
Wallace, C. S., 59, 262 
Walley, P., 57, 262 
Wang, Y., 55, 57, 248, 
253, 262 
Warmuth, M. K., 26, 59, 
109, 110, 172, 222, 
251, 255, 257, 258 
Watkins, C., 26, 27, 60, 
262 
wave function collapse, 
248 
weather forecasts, 86 
weighted majority 
algorithm, 108, 235 
weights, 70 
continuous, 106 
enumerable, 102 
general, 153 
optimal, 102, 103 
posterior, 70 
time-dependent, 70 
universal choice, 120 
update rule, 109 
Wiering, M. A., 25-27, 
59, 214, 221, 235, 
261-263 
winning 
strategy, 94 
threshold, 95 
Winograd, S., 213, 253 
witness, 212 
Wolpert, D. H., 10, 98, 
103, 262 
worst-case 
reasoning, 57 
Yamanishi, K., 109, 111, 
263 
Yaroshinsky, R., 109, 263 
Zadeh, L. A., 57, 263 
Zhao, J., 25, 26, 59, 214, 
221, 235, 261 
Zimmermann, H.-J., 57, 
263 
Zvonkin, A. K., 7, 47-49, 
55, 56, 58, 68, 81, 
218, 263 

Monographs in Theoretical Computer Science • An EATCS Series 
K. Jensen 
Coloured Petri Nets 
Basic Concepts, Analysis Methods 
and Practical Use, Vol. 1 
2nded. 
K. Jensen 
Coloured Petri Nets 
Basic Concepts, Analysis Methods 
and Practical Use, Vol. 2 
K. Jensen 
Coloured Petri Nets 
Basic Concepts, Analysis Methods 
and Practical Use, Vol. 3 
A.NaitAbdallah 
The Logic of Partial Information 
Z.Fulop,H.Vogler 
Syntax-Directed Semantics 
Formal Models Based 
on Tree Transducers 
A. de Luca, S. Varricchio 
Finiteness and Regularity 
in Semigroups and Formal Languages 
E. Best, R. Devillers, M. Koutny 
Petri Net Algebra 
S.R Demri, E.S. Orlowska 
Incomplete Information: 
Structure, Inference, Complexity 
J.C.M. Baeten, C.A. Middelburg 
Process Algebra with Timing 
L. A. Hemaspaandra, L.Torenvliet 
Theory of Semi-Feasible Algorithms 
E. Fink, D.Wood 
Restricted-Orientation Convexity 
Zhou Chaochen, M. R. Hansen 
Duration Calculus 
A Formal Approach to Real-Time 
Systems 
M. Grofie-Rhode 
Semantic Integration 
of Heterogeneous Software 
Specifications 

Texts in Theoretical Computer Science • An EATCS Series 
J. L. Balcazar, J. Diaz, J. Gabarro 
Structural Complexity I 
M. Garzon 
Models of Massive Parallelism 
Analysis of Cellular Automata 
and Neural Networks 
J. Hromkovic 
Communication Complexity 
and Parallel Computing 
A. Leitsch 
The Resolution Calculus 
G. Paun, G. Rozenberg, A. Salomaa 
DNA Computing 
New Computing Paradigms 
A. Salomaa 
PubUc-Key Cryptography 
2nd ed. 
K. Sikkel 
Parsing Schemata 
A Framework for Specification 
and Analysis of Parsing Algorithms 
H.VoUmer 
Introduction to Circuit Complexity 
A Uniform Approach 
W.Fokkink 
Introduction to Process Algebra 
K. Weihrauch 
Computable Analysis 
An Introduction 
}. Hromkovic 
Algorithmics for Hard Problems 
Introduction to Combinatorial 
Optimization, Randomization, 
Approximation, and Heuristics 
2nded. 
S.Jukna 
Extremal Combinatorics 
With Applications 
in Computer Science 
P. Clote, E. Kranakis 
Boolean Functions 
and Computation Models 
L. A. Hemaspaandra, M. Ogihara 
The Complexity Theory Companion 
C.S. Calude 
Information and Randomness. 
An Algorithmic Perspective 
2nd ed. 
J. Hromkovic 
Theoretical Computer Science 
Introduction to Automata, 
Computability, Complexity, 
Algorithmics, Randomization, 
Communication and Cryptography 
A. Schneider 
Verification of Reactive Systems 
Formal Methods and Algorithms 
S. Ronchi Delia Rocca, L. Paolini 
The Parametric Lambda Calculus 
A Metamodel for Computation 
Y.Bertot,RCasteran 
Interactive Theorem Proving 
and Program Development 
CoqArt: The Calculus 
of Inductive Constructions 
L. Libkin 
Elements of Finite Model Theory 
M. Hutter 
Universal Artificial Intelligence 
Sequential Decisions 
Based on Algorithmic Probability 

