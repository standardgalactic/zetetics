Physics-Informed Neural Operator
for Learning Partial Differential Equations
Zongyi Li*, Hongkai Zheng*, Nikola Kovachki, David Jin, Haoxuan Chen,
Burigede Liu, Kamyar Azizzadenesheli, Anima Anandkumar
Abstract
In this paper, we propose physics-informed neural operators (PINO) that combine training data and physics
constraints to learn the solution operator of a given family of parametric Partial Differential Equations (PDE). PINO
is the first hybrid approach incorporating data and PDE constraints at different resolutions to learn the operator.
Specifically, in PINO, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution.
The resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE
families and shows no degradation in accuracy even under zero-shot super-resolution, i.e., being able to predict beyond
the resolution of training data. PINO uses the Fourier neural operator (FNO) framework that is guaranteed to be a
universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. By
adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth
operator. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are
imposed, while previous approaches, such as the Physics-Informed Neural Network (PINN), fail due to optimization
challenges, e.g., in multi-scale dynamic systems such as Kolmogorov flows.
1
Introduction
Machine learning methods have recently shown promise in solving partial differential equations (PDEs) [27, 29, 30, 36,
6]. A recent breakthrough is the paradigm of operator learning for solving PDEs. Unlike standard neural networks
that learn using inputs and outputs of fixed dimensions, neural operators learn operators, which are mappings between
function spaces [27, 29, 30]. The class of neural operators is guaranteed to be a universal approximator for any
continuous operator [27] and hence, has the capacity to approximate any operator including any solution operator of a
given family of parametric PDEs. Note that the solution operator is the mapping from the input function (initial and
boundary conditions) to the output solution function. Previous works show that neural operators can capture complex
multi-scale dynamic processes and are significantly faster than numerical solvers [34, 56, 43, 33, 54, 3, 55].
Neural operators are proven to be discretization convergent in the limit of mesh refinement [27], meaning they
converge to a continuum operator in the limit as the discretization is refined. Consequently, they can be evaluated at any
data discretization or resolution at inference time without the need for retraining. For example, neural operators such as
Fourier neural operator (FNO) can extrapolate to frequencies that are not seen during training in Kolmogorov Flows,
as shown in Figure 1, while standard approaches such as training a UNet and adding trilinear interpolation leads to
significantly worse results at higher resolutions.
Even though FNO follows the general trend of the Kolmogorov flow in Figure 1, it cannot perfectly match it in
the regime of super-resolution, i.e., beyond the frequencies seen during training. More generally, neural operators
cannot perfectly approximate the ground-truth operator when only coarse-resolution training data is provided. This is
a fundamental limitation of data-driven operator learning methods which depend on the availability of training data,
which can come either from existing numerical solvers or direct observations of the physical phenomena. In many
scenarios, such data can be expensive to generate, unavailable, or available only as low-resolution observations [19].
This limits the ability of neural operators to learn high-fidelity models. Moreover, the generalization of the learned
neural operators to unseen scenarios and conditions that are different from training data is challenging.
1
arXiv:2111.03794v4  [cs.LG]  29 Jul 2023

Figure 1: PINO uses both training data and PDE loss function and perfectly extrapolates to unseen frequencies in
Kolmogorov Flows. FNO uses only training data and does not have further information on higher frequencies, but
still follows the general trend of the ground-truth spectrum. On the other hand, using a trained UNet with trilinear
interpolation (NN+Interpolation) has severe distortions at higher frequencies. Details in Section 4.2.
1.1
Our Approach and Contributions
In this paper, we remedy the above shortcomings of data-driven operator learning methods through the framework
of physics-informed neural operators (PINO). Here, we take a hybrid approach of combining training data (when
available) with a PDE loss function at a higher resolution. This allows us to approximate the solution operator of
many PDE families nearly perfectly. While there have been many physics-informed approaches proposed previously
(discussed in 1.2), ours is the first to incorporate PDE constraints at a higher resolution as a remedy for low resolution
training data. We show that this results in high-fidelity solution operator approximations. As shown in Figure 1, PINO
extrapolates to unseen frequencies in Kolmogorov Flows. Thus, we show that the PINO model learned from such
multi-resolution hybrid loss functions has almost no degradation in accuracy even on high-resolution test instances
when only low-resolution training data is available. Further, our PINO approach also overcomes the optimization
challenges in approaches such as Physics-Informed Neural Network (PINN) [44] that are purely based on PDE loss
functions and do not use training data, and thus, PINO can solve more challenging problems such as time-dependent
PDEs.
PINO utilizes both the data and equation loss functions (whichever are available) for operator learning. To further
improve accuracy at test time, we fine-tune the learned operator on the given PDE instance using only the equation loss
function. This allows us to provide a nearly-zero error for the given PDE instance at all resolutions. A schematic of
PINO is shown in Figure 2, where the neural operator architecture is based on Fourier neural operator (FNO) [29]. The
derivatives needed for the equation loss in PINO are computed explicitly through the operator layers in function spaces.
In particular, we efficiently compute the explicit gradients on function space through Fourier-space computations. In
contrast, previous auto-differentiation methods must compute the derivatives at sampling locations.
The PDE loss function added to PINO vastly improves generalization and physical validity in operator learning
compared to purely data-driven methods. PINO requires fewer to no training data and generalizes better compared to
the data-driven FNO [31], especially on high-resolution test instances. On average, the relative error is 7% lower on
both transient and Kolmogorov flows, while matching the speedup of data-trained FNO architecture (400x) compared
to the GPU-based pseudo-spectral solver [17]. Further, the PINO model on the Navier-Stokes equation can be easily
transferred to different Reynolds numbers ranging from 100 to 500 using instance-wise fine-tuning.
We also use PINO for solving inverse problems by either: (1) learning the forward solution operator and using
2

Figure 2: PINO trains neural operator with both training data and PDE loss function. The figure shows the neural
operator architecture with the lifting point-wise operator that receives input function a and outputs function v0 with
a larger co-dimension. This operation is followed by L blocks that compute linear integral operators followed by
non-linearity, and the last layer of which outputs the function vL. The pointwise projection operator projects vL to
output function u. Both vL and u are functions and all their derivatives (DvL, Du) can be computed in their exact forms
at any query points x.
gradient-based optimization to obtain the inverse solution, or (2) learning the inverse solution operator directly. Imposing
the PDE loss guarantees the inverse solution is physically valid in both approaches. We find that of these two approaches,
the latter is more accurate for recovering the coefficient function in Darcy flow. We show this approach is 3000x faster
than the conventional solvers using accelerated Markov Chain Monte-Carlo (MCMC) [8].
1.2
Related Work
Learning solution to PDEs has been proposed under two paradigms: (i) data-driven learning and (ii) physics-informed
optimization. The former utilizes data from existing solvers or experiments, while the latter is purely based on PDE
constraints. An example of data-driven methods is the class of neural operators for learning the solution operator
of a given family of parametric PDEs. An example of the physics-based approach is the Physics-Informed Neural
Network (PINN) for optimizing the PDE constraints to obtain the solution function of a given PDE instance. Both these
approaches have shortcomings. Neural operators require data, and when that is limited or not available, they are unable
to learn the solution operator faithfully. PINN, on the other hand, does not require data but is prone to failure, especially
on multi-scale dynamic systems due to optimization challenges.
Neural operators learn the solution operator of a family of PDEs, defined by the map from the input–initial conditions
and boundary conditions, to the output–solution functions. In this case, usually, a dataset of input-output pairs from
an existing solver or real-world observation is given. There are two main aspects to consider (a) models: the design
of models for learning highly complicated PDE solution operators, and (b) data: minimizing data requirements and
improving generalization. Recent advances in operator learning replace traditional convolutional neural networks and
U-Nets from computer vision with operator-based models tailored to PDEs with greatly improved model expressiveness
[31, 36, 41, 49, 10]. Specifically, the neural operator generalizes the neural network to the operator setting where the
input and output spaces are infinite-dimensional. The framework has successfully approximated solution operators for
highly non-linear problems such as turbulence flow [30, 29]. However, the data challenges remain. In particular, (1)
training data from an existing solver or an experimental setup is costly to obtain, (2) models struggle in generalizing
away from the training distribution, and (3) constructing the most efficient approximation for given data remains elusive.
Moreover, it is also evident that in many real-world applications, observational data often is available at only low
resolutions [19], limiting the model’s ability to generalize.
Alternatives to data-driven approaches for solving PDEs are physics-based approaches that require no training data.
A popular framework known as Physics-Informed Neural Network (PINN) [44] uses optimization to find the solution
function of a given PDE instance. PINN uses a neural network as the ansatz of the solution function and optimizes a
3

Figure 3: solve for one specific instance verse learn the entire solution operator
Left: numerical solvers and PINNs focus on solving one specific instance. Right: neural operators learn the solution
operator for a family of equations.
loss function to minimize the violation of the given equation by taking advantage of auto-differentiation to compute the
exact derivatives. PINN overcomes the need to choose a discretization grid that most numerical solvers require, e.g.,
finite difference methods (FDM) and finite element methods (FEM). It has shown promise in solving PDEs for a wide
range of applications, including higher dimensional problems. [38, 16, 18, 23]. Recently, researchers have developed
many variations of PINN with promising results for solving inverse problems and partially observed tasks [35, 58, 47].
However, PINN fails in many multi-scale dynamic PDE systems [50, 13, 45] due to two main reasons, viz., (1)
the challenging optimization landscape of the PDE constraints [51] and its sensitivity to hyper-parameter selection
[48], and (2) the difficulty in propagating information from the initial or boundary conditions to unseen parts of the
interior or to future times [11]. Moreover, PINN only learns the solution function of a single PDE instance and cannot
generalize to other instances without re-optimization. Concurrent work on physics-informed DeepONet that imposes
PDE losses on DeepONet [52] overcomes this limitation and can learn across multiple PDE instances. While the PDE
loss is computed at any query points, the input is limited to a fixed grid in standard DeepONet [27], and its architecture
is a linear method of approximation [28]. Our work overcomes all previously mentioned limitations. Further, a unique
feature that PINO enjoys over other hybrid learning methods [58, 57, 20] is its ability to incorporate data and PDE
loss functions at different resolutions. This has not been attempted before, and none of the previous works focus on
extrapolation to higher resolutions, beyond what is seen in training data.
2
Preliminaries and problem settings
In this section, we first define the stationary and dynamic PDE systems that we consider. We give an overview of the
physics-informed setting and operator-learning setting. In the end, we define the Fourier neural operator as a specific
model for operator learning.
2.1
Problem settings
We consider two natural classes of PDEs. In the first, we consider the stationary system
P(u, a) = 0,
in D ⊂Rd
u = g,
in ∂D
(1)
where D is a bounded domain, a ∈A ⊆V is a PDE coefficient/parameter, u ∈U is the unknown, and P : U × A →F
is a possibly non-linear partial differential operator with (U, V, F) a triplet of Banach spaces. Usually, the function
g is a fixed boundary condition but can also potentially enter as a parameter. This formulation gives rise to the
solution operator G† : A →U defined by a 7→u. A prototypical example is the second-order elliptic equation
P(u, a) = −∇· (a∇u) + f.
4

In the second setting, we consider the dynamical system
du
dt = R(u),
in D × (0, ∞)
u = g,
in ∂D × (0, ∞)
u = a
in ¯D × {0}
(2)
where a = u(0) ∈A ⊆V is the initial condition, u(t) ∈U for t > 0 is the unknown, and R is a possibly non-linear
partial differential operator with U, and V Banach spaces. As before, we take g to be a known boundary condition.
We assume that u exists and is bounded for all time and for every u0 ∈U. This formulation gives rise to the solution
operator G† : A →C
 (0, T]; U

defined by a 7→u. Prototypical examples include the Burgers’ equation and the
Navier-Stokes equation.
2.2
Solving equation using the physics-informed neural networks
Given an instance a and a solution operator G† defined by equations (1) or (2) , we denote by u† = G†(a) the unique
ground truth. The equation-solving task is to approximate u†. This setting consists of the ML-enhanced conventional
solvers such as learned finite element, finite difference, and multigrid solvers [25, 42, 15], as well as purely neural
network-based solvers such as the Physics-Informed Neural Networks (PINNs), Deep Galerkin Method, and Deep Ritz
Method [44, 46, 53]. Especially, these PINN-type methods use a neural network uθ with parameters θ as the ansatz to
approximate the solution function u†. The parameters θ are found by minimizing the physics-informed loss with exact
derivatives computed using automatic differentiation (autograd). In the stationary case, the physics-informed loss is
defined by minimizing the l.h.s. of equation (1) in the squared norm of F. A typical choice is F = L2(D), giving the
loss function
Lpde(a, uθ) =
P(a, uθ)

2
L2(D) + α
uθ|∂D −g

2
L2(∂D)
=
Z
D
|P(uθ(x), a(x))|2dx + α
Z
∂D
|uθ(x) −g(x)|2dx
(3)
In the case of a dynamical system, one minimizes the residual of equation (2) in some natural norm up to a fixed
final time T > 0. A typical choice is the L2 (0, T]; L2(D)

norm, yielding
Lpde(a, uθ) =
duθ
dt −R(uθ)

2
L2(T ;D) + α
uθ|∂D −g

2
L2(T ;∂D) + β
uθ|t=0 −a

2
L2(D)
=
Z T
0
Z
D
|duθ
dt (t, x) −R(uθ)(t, x)|2dxdt
+ α
Z T
0
Z
∂D
|uθ(t, x) −g(t, x)|2dxdt
+ β
Z
D
|uθ(0, x) −a(x)|2dx
(4)
The PDE loss consists of the physics loss in the interior and the data loss on the boundary and initial conditions, with
hyper-parameters α, β > 0. Alternatively, the optimization can be formulated via the variational form [53].
Challenges of PINN
PINNs take advantage of the universal approximability of neural networks, but, in return, suffer
from the low-frequency induced bias. Empirically, PINNs often fail to solve challenging PDEs when the solution
exhibits high-frequency or multi-scale structure [51, 50, 13, 45]. Further, as an iterative solver, PINNs have difficulty
propagating information from the initial condition or boundary condition to unseen parts of the interior or to future times
[11]. For example, in challenging problems such as turbulence, PINNs are only able to solve the PDE on a relatively
small domain [22], or otherwise, require extra observational data which is not always available in practice [45, 7]. In
this work, we propose to overcome the challenges posed by optimization by integrating operator learning with PINNs.
5

2.3
Learning the solution operator via neural operator
An alternative setting is to learn the solution operator G. Given a PDE as defined in (1) or (2) and the corresponding
solution operator G†, one can use a neural operator Gθ with parameters θ as a surrogate model to approximate G†.
Usually we assume a dataset {aj, uj}N
j=1 is available, where G†(aj) = uj and aj ∼µ are i.i.d. samples from some
distribution µ supported on A. In this case, one can optimize the solution operator by minimizing the empirical data
loss on a given data pair
Ldata(u, Gθ(a)) = ∥u −Gθ(a)∥2
U =
Z
D
|u(x) −Gθ(a)(x)|2dx
(5)
where we assume the setting of (1) for simplicity of the exposition. The operator data loss is defined as the average
error across all possible inputs
Jdata(Gθ) = ∥G† −Gθ∥2
L2µ(A;U) = Ea∼µ[Ldata(a, θ)] ≈1
N
N
X
j=1
Z
D
|uj(x) −Gθ(aj)(x)|2dx.
(6)
Similarly, one can define the operator PDE loss as
Jpde(Gθ) = Ea∼µ[Lpde(a, Gθ(a))].
(7)
In general, it is non-trivial to compute the derivatives dGθ(a)/dx and dGθ(a)/dt for model Gθ. In the following
section, we will discuss how to compute these derivatives for Fourier neural operator.
2.4
Neural operators
In this work, we will focus on the neural operator model designed for the operator learning problem [31]. The neural
operator is formulated as a generalization of standard deep neural networks to the operator setting. Neural operator
composes linear integral operator K with pointwise non-linear activation function σ to approximate highly non-linear
operators.
Definition 1 (Neural operator Gθ) Define the neural operator
Gθ := Q ◦(WL + KL) ◦· · · ◦σ(W1 + K1) ◦P
(8)
where P and Q are pointwise operators, parameterized with neural networks P : Rda →Rd1 and Q : RdL →Rdu,
where da is the co-dimension of an input function a ∈A and du is the co-dimension of the output function u. P
operator lifts the lower dimension function into higher dimensional space and Q operator projects the higher dimension
function back to the lower dimensional space. The model stacks L layers of σ(Wl + Kl) where W are pointwise
linear operators parameterized as matrices Wl ∈Rdl+1×dl , Kl : {D →Rdl} →{D →Rdl+1} are integral kernel
operators, and σ are fixed activation functions. The parameters θ consists of all the parameters in P, Q, Wl, Kl.
Definition 2 (Kernel Integral Operators) We define the kernel integral operator K used in (8). Let κ(l) ∈C(D ×
D; Rdl+1×dl) and let ν be a Borel measure on D. Then we define K by
(Kvl)(x) =
Z
D
κ(l)(x, y)vl(y) dν(y)
∀x ∈D.
(9)
The kernel integral operator can be discretized and implemented with graph neural networks as in graph neural operators
[31].
(Kvl)(x) =
X
B(x)
κ(l)(x, y)vl(y)
∀x ∈D.
(10)
where B(x) is a ball of center at x. As a generalization, the kernel function can also take the form of (Kvl)(x) =
P
B(x) κ(l)(x, y, vl(y)).
Recently, [29] proposes the Fourier neural operator (FNO) that restricts the integral operator K to convolution. In
this case, it can apply the Fast Fourier Transform (FFT) to efficiently compute K. This leads to a fast architecture that
obtains state-of-the-art results for PDE problems.
6

Definition 3 (Fourier convolution operator) One specific form of the kernel integral operator is the Fourier convolu-
tion operator
 Kvl

(x) = F−1
R · (Fvl)

(x)
∀x ∈D
(11)
where F, F−1 are the Fast Fourier transform and its inverse; R is part of the parameter θ to be learn.
One can build a neural operator with mixed kernel integral layers and Fourier convolution layers. If the input and output
query points are sampled from non-uniform mesh, we can use the graph kernel operator as the first and last layer for
continuous evaluation, while using the Fourier layers in the middle for efficient computation, similar to [33].
Challenges of operator learning.
Operator learning is similar to supervised learning in computer vision and language
where data play a very important role. One needs to assume the training points and testing points follow the same
problem setting and the same distribution. Especially, the previous FNO model trained on one coefficient (e.g. Reynolds
number) or one geometry cannot be easily generalized to another. Moreover, for more challenging PDEs where the
solver is very slow or the solver is even not existent, it is hard to gather a representative dataset. On the other hand, since
prior training methods for FNO do not use any knowledge of the equation, the trained models cannot get arbitrarily
close to the ground truth by using the higher resolution as in conventional solvers, leaving a gap of generalization error.
These challenges limit the applications of the prior works beyond accelerating the solver and modeling real-world
experiments. In the following section, we will introduce the PINO framework to overcome these problems by using the
equation constraints.
Discretization convergent.
Resolution and discretization convergence is defined as obtaining a unique continuum
operator in the limit of mesh refinement [27]. The work [1] recently introduced a new concept of representation
equivalence, which requires zero aliasing error at each layer, which PINO does not fulfill. When all the Fourier modes
in FNO are active, an aliasing error is inevitably present. However, in many practical applications, this is typically
not an issue, and degraded performance due to aliasing is rarely observed, since the maximum number of modes in
an FNO is typically far fewer than the grid size. In fact, the non-linear transformations allow for re-capturing the
truncated high-frequency modes which allows for generalization beyond the see training data. Requiring representation
equivalence leads to linear methods of approximation which are know to be sub-optimal in their representation capacity
[28].
Related work.
Many machine learning models have been explored for operator learning [2, 40, 41]. Besides the
above line of work, the deep operator network (DeepONet) is one of the most famous operator models that have
shown enormous promise in applications [36]. The work from [26] compares the polynomial chaos expansion (PCE),
DeepONet, and FNO, and shows that DeepONet has a higher approximation accuracy over PCE. According to Figure 5
in [26], standard DeepONet and FNO share a similar convergence rate. A similar comparison study is reported by de
Hoop et. al. [9] where FNO seems to converge faster. We choose FNO as our base model for its scalability to large
problems.
3
Physics-informed neural operator (PINO)
We propose the PINO framework that uses one neural operator model Gθ for solving both operator learning problems
and equation solving problems. It consists of two phases
• operator learning: learn a neural operator Gθ to approximate the target solution operator G† using either/both
the data loss Jdata or/and the PDE loss Jpde.
• instance-wise fine-tuning: use Gθ(a) as the ansatz to approximate u† with the pde loss Lpde and/or an additional
operator loss Lop obtained from the operator learning phase.
7

3.1
Physics-informed operator learning
For operator learning, we use the physics constraints Jpde and supervision from data to train the neural operator.
Especially one can sample an unlimited amount of virtual PDE instances by drawing additional initial conditions or
coefficient conditions aj ∼µ for training. In this sense, we have access to the unlimited dataset by sampling new input
aj in each training iteration. This advantage of using PDE constraints removes the requirement on the dataset and
makes the supervised problem into a semi-supervised one.
While PINO can be trained with physics constraints Jpde only, the Jdata can provide stronger supervision than
physics constraints and thus make the optimization much easier. PINO leverages the supervision from any available
data to combine with physics constraints for better optimization landscape and thus learning accurate neural operators.
A special case is to train a neural operator on low-resolution data instances with high-resolution PDE constraint, which
will be studied in section 4.
3.2
Instance-wise fine-tuning of trained operator ansatz
Given a learned operator Gθ, we use Gθ(a) as the ansatz to solve for u†. The optimization procedure is similar to PINNs
where it computes the PDE loss Lpde on a, except that we propose to use a neural operator instead of a neural network.
Since the PDE loss is a soft constraint and challenging to optimize, we also add an optional operator loss Lop (anchor
loss) to bound the further fine-tuned model from the learned operator model
Lop
 Gθi(a), Gθ0(a)
 := ∥Gθi(a) −Gθ0(a)∥2
U
where Gθi(a) is the model at ith training epoch. We update the operator Gθ using the loss Lpde + αLop. It is possible
to further apply optimization techniques to fine-tune the last fewer layers of the neural operator and progressive training
that gradually increase the grid resolution and use finer resolution in test time.
Optimization landscape.
Using the operator as the ansatz has two major advantages: (1) PINN does point-wise
optimization, while PINO does optimization in the space of functions. In the linear integral operation K, the operator
parameterizes the solution function as a sum of the basis function. Optimization of the set of coefficients and basis
is easier than just optimizing a single function as in PINNs. (2) we can learn these basis functions in the operator
learning phase which makes the later instance-wise fine-tuning even easier. In PINO, we do not need to propagate the
information from the initial condition and boundary condition to the interior. It just requires fine-tuning the solution
function parameterized by the solution operator.
Trade-off.
(1) complexity and accuracy: instance-wise fine-tuning is an option to spend more computation in exchange
for better accuracy. The learned operator is extremely fast since it is performing inference on the neural operator.
On the other hand, instance-wise fine-tuning can improve accuracy while incurring more computational costs. (2)
resolution effects on optimization landscape and truncation error (i.e. the error of the numerical differentiation): using a
higher resolution and finer grid will reduce the truncation error. However, it has a higher computational complexity and
memory consumption. A higher resolution may also potentially make the optimization unstable. Using hard constraints
such as the anchor loss Lop relieves such a problem.
3.3
Derivatives of neural operators
In order to use the equation loss Lpde, one of the major technical challenges is to efficiently compute the derivatives
D(Gθa) = d(Gθa)/dx for neural operators. In this section, we discuss three efficient methods to compute the derivatives
of the neural operator Gθ as defined in (8).
Numerical differentiation.
A simple but efficient approach is to use conventional numerical derivatives such as finite
difference and Fourier differentiation [58, 14]. These numerical differentiation methods are fast and memory-efficient:
given a n-points grid, finite difference requires O(n), and the Fourier method requires O(n log n). These methods are
agnostic to the underlying more. It can be applied to the neural operator with Graph layer 2 or Fourier layer 3 or neural
networks such as UNet.
8

However, the numerical differentiation methods face the same challenges as the corresponding numerical solvers:
finite difference methods require a fine-resolution uniform grid; spectral methods require smoothness and uniform grids.
Especially. These numerical errors on the derivatives will be amplified on the output solution.
Pointwise differentiation with autograd.
Similar to PINN [44], the most general way to compute the exact derivatives
is to use the auto-differentiation library of neural networks (autograd). To apply autograd, one needs to use a neural
network to parameterize the solution function u : x 7→u(x). However, it is not straightforward to write out the
solution function in the neural operator which directly outputs the numerical solution u = Gθ(a) on a grid, especially
for FNO which uses FFT. To apply autograd, we design a query function u that input x and output u(x). Recall
Gθ := Q ◦(WL + KL) ◦· · · ◦σ(W1 + K1) ◦P and u = Gθa = QvL = Q(WL + KL)vL−1 . . .. Since Q is pointwise,
u(x) = Q(vL)(x) = Q(vL(x)) = Q
 (WLvL−1)(x) + KLvL−1(x)

(12)
For both the kernel operator and Fourier operator, we either remove the pointwise residual term of the last layer
(WLvL−1)(x) or define the query function as an interpolation function on WL.
For kernel integral operator 2, the kernel function can directly take the query points are input. So the query function
u(x) = Q
 X
B(x)
κ(l)(x, y, vL−1(y))

where we omit the derivative of the support B(x). We can apply auto-differentiation to compute the derivatives
u′(x) = Q′ vL(x)

·
X
B(x)
κ(l)′(x, y, vL−1(y))
(13)
Similarly, for the Fourier convolution operator, we need to evaluate the Fourier convolution KLvL−1(x) on the
query points x. It can be done by writing out the output function as Fourier series composing with Q :
u(x) = Q ◦F−1
R · (FvL−1))

(x) = Q

1
kmax
kmax
X
k=0
 Rk(FvL−1)k

exp i2πk
D (x)

Where F is the discrete Fourier transform. The inverse discrete Fourier transform is the sum of kmax Fourier series
with the coefficients
 Rk(FvL−1)k

coming from the previous layer.
u′(x) = Q′ vL(x)

·
1
kmax
kmax
X
k=0
 Rk(FvL−1)k

exp′ i2πk
D (x)
(14)
Notice exp′ i2πk
D (x) = i2πk
D exp i2πk
D (x), just as the numerical Fourier method. If the query points x are a uniform
grid, the derivative can be efficiently computed with the Fast Fourier transform.
The autograd method is general and exact, however, it is less efficient. Since the number of parameters |θ| is usually
much greater than the grid size n, the numerical methods are indeed significantly faster. Empirically, the autograd
method is usually slower and memory-consuming.
Function-wise differentiation.
While it is expensive to apply the auto differentiation per query point, the derivative
can be batched and computed in a function-wise manner. We develop an efficient and exact derivatives method based
on the architecture of the neural operator that can compute the full gradient field. The idea is similar to the autograd,
but we explicitly write out the derivatives on the Fourier space and apply the chain rule. Given the explicit form (14), u′
can be directly computed on the Fourier space.
u′ = Q′ vL

· F−1i2π
D K · (FvL))

(15)
Therefore, to exactly compute the derivative of the Fourier neural operator, one just needs to run the numerical Fourier
differentiation. Especially the derivative and be efficiently computed with the Fast Fourier transform when the query
9

is uniform. Similarly, if the kernel function κ(l) in (13) has a structured form, we can also write out its gradient field
explicitly.
Next, we show how to compute higher orders derivatives in their exact form, without evoking the autograd method.
To this end, we can directly apply the chain rule for the higher-order derivatives without calling autograd. For
example, the first order derivatives is u′ = (Q ◦vL)′ = Q′(vL) · v′
L and the 2nd-order derivatives is u′′ = (QvL)′′ =
v′2
L · Q′′(vL) + Q′(vL) · v′′
L. Higher-order derivatives can be similarly computed using the chain rule. Furthermore,
derivative-based quantities on vL, e.g., v′
L can be computed in its exact form in the Fourier domain. Similarly, we
can write out the higher-order derivatives of Q using the chain rule. Usually Q is parameterized as a two layer neural
networks Q(x) = (A2σ(A1x + b1) + b2). So Q′(x) = A2σ′(A1x + b1)A1. In this manner, we have got the explicit
formula of the derivatives for all neural operators.
Fourier continuation.
The Fourier method has its best performance when applied to periodic problems. If the target
function is non-periodic or non-smooth, the Fourier differentiation is not accurate. To deal with this issue, we apply the
Fourier continuation method that embeds the problem domain into a larger and periodic space. The extension can be
simply done by padding zeros in the input. The loss is computed at the original space during training. The Fourier
neural operator will automatically generate a smooth extension. The details are given in Appendix C.
3.4
Inverse problem
The physics-informed method can be used to solve the inverse problem, where given the output function u, the goal
is to recover (a distribution of) the input function a. By imposing the constraint P(u, a) = 0, we can restrict a to a
physically valid manifold. We propose two formulations to do the optimization-based inverse problem with PINO: the
forward operator model and the inverse operator model.
• Forward model: learn the forward operator Gθ : a 7→u with data. Initialize ˆa to approximate a†. Optimize ˆa
using
Jforward := Lpde(ˆa, u†) + Ldata(Gθ(ˆa)) + R(ˆa).
(16)
• inverse model: learn the inverse operator Fθ : u 7→a with data. Use Fθ(u†) to approximate a†. Optimize Fθ
using
Jbackward := Lpde(Fθ(u†), u†) + Lop(Fθ(u†), Fθ0(u†)) + R(Fθ(u†)
(17)
Where Lpde is the PDE loss; Lop is the operator loss from the learned operator; R(a) is the regularization term. We
use the PDE loss Lpde to deal with the small error in Gθ and the ill-defining issue of Fθ. We provide a numerical study
in section 4.3.
4
Experiments
In this section, we conduct empirical experiments to examine the efficacy of the proposed PINO. We present the PDE
settings, their domains, and function spaces. In 4.1, we show using PDE constraint in operator learning results in
neural operators that (1) generalize to significantly high-resolution unseen data. (2) achieve smaller generalization
errors with fewer to no data. Then in 4.2, we investigate how PINO uses the operator ansatz to solve harder equations
with improved speed and accuracy. We study three concrete cases of PDEs on Burgers’ Equation, Darcy’s Equation,
and Navier-Stokes equation. In 4.3 we study the inverse problems. The implementation details of PINO and baseline
methods are listed in Appendix A.
Burgers’ Equation.
The 1-d Burgers’ equation is a non-linear PDE with periodic boundary conditions where
u0 ∈L2
per((0, 1); R) is the initial condition and ν = 0.01 is the viscosity coefficient. We aim to learn the operator
mapping the initial condition to the solution, G† : u0 7→u|[0,1].
∂tu(x, t) + ∂x(u2(x, t)/2) = ν∂xxu(x, t),
x ∈(0, 1), t ∈(0, 1]
u(x, 0) = u0(x),
x ∈(0, 1)
(18)
10

10
1
10
0
10
1
10
2
Runtime(s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Relative L2 error
PINO (operator learning)
PINO (instance-wise finetuning)
PINN
LAAF-PINN
SA-PINN
10
1
10
0
10
1
10
2
10
3
Runtime(s)
0.0
0.1
0.2
0.3
0.4
0.5
Relative L2 error
PINO (operator learning)
PINO (instance-wise finetuning)
PINN
LAAF-PINN
SA-PINN
Figure 4: Plot of test relative L2 error versus runtime step for the Kolmogorov flow with Re500, T=0.5s. Left: resolution
64×64×65; right: resolution 128×128×129. Averaged over 20 instances. LAAF-PINN: PINN with locally adaptive
activation functions. SA-PINN: self-adaptive PINN.
Darcy Flow.
The 2-d steady-state Darcy Flow equation on the unit box which is the second order linear elliptic PDE
with a Dirichlet boundary where a ∈L∞((0, 1)2; R+) is a piecewise constant diffusion coefficient and f = 1 is a
fixed forcing function. We are interested in learning the operator mapping the diffusion coefficient to the solution,
G† : a 7→u. Note that although the PDE is linear, the operator G† is not.
−∇· (a(x)∇u(x)) = f(x)
x ∈(0, 1)2
u(x) = 0
x ∈∂(0, 1)2
(19)
Since a is in Linf, we considered both the strong form Lpde(u) = ∇· (a∇u) −f and the weak form minimization
loss Lpde(u) = −1
2(a∇u, ∇u) −(u, f), u ∈H1. Experiments show the strong form has a better performance.
Navier-Stokes Equation.
We consider the 2-d Navier-Stokes equation for a viscous, incompressible fluid in vorticity
form on the unit torus, where u ∈C([0, T]; Hr
per((0, l)2; R2)) for any r > 0 is the velocity field, w = ∇× u is the
vorticity, w0 ∈L2
per((0, l)2; R) is the initial vorticity, ν ∈R+ is the viscosity coefficient, and f ∈L2
per((0, l)2; R) is
the forcing function. We want to learn the operator mapping the vorticity from the initial condition to the full solution
G† : w0 7→w|[0,T ].
∂tw(x, t) + u(x, t) · ∇w(x, t) = ν∆w(x, t) + f(x),
x ∈(0, l)2, t ∈(0, T]
∇· u(x, t) = 0,
x ∈(0, l)2, t ∈[0, T]
w(x, 0) = w0(x),
x ∈(0, l)2
(20)
Specially, we consider two problem settings:
• Long temporal transient flow: we study the build-up of the flow from the initial condition u0 near-zero velocity
to uT that reaches the ergodic state. We choose t ∈[0, 50], l = 1, Re = 20 as in Li et al. [29]. The main
challenge is to predict the long time interval.
• Chaotic Kolmogorov flow: In this case u lies in the attractor where arbitrary starting time t0. We choose
t ∈[t0, t0 + 0.5] or [t0, t0 + 1], l = 2π, Re = 500 similar to Li et al. [32]. The main challenge is to capture the
small details that evolve chaotically.
• Lid cavity flow: In this case, we assume the no-slip boundary condition where u(x, t) = (0, 0) at left, bottom,
and right walls and u(x, t) = (1, 0) on top, similar to Bruneau and Saad [4]. We choose t ∈[5, 10], l = 1,
Re = 500. The main challenge is to address the boundary using the velocity-pressure formulation.
11

Figure 5: PINO on Kolmogorov flow (left) and Lid-cavity flow (right)
4.1
Operator learning with physics constraints
We show that we can utilize the equation constraints to improve neural operator training. For this purpose, we train
neural operators on fixed-resolution data in the presence of physics loss, Jpde, and test the performance of the trained
operators on high-resolution data. In particular, we test the performance of the trained model on data with the same
resolution of the training data, 1x, 2x, and 4x, of the training data resolution Table 1. We observe that incorporating the
Jpde in the training results in operators that, with high accuracy, generalize across data resolution. In this experiment,
the training data for Burgers equation setting is in 32 × 25 (spatio-temporal), and the Jpde is imposed in 128 × 100
resolution. We use 800 low-resolution data and the same 800 PDE instances. The mean relative L2 error and its standard
deviation are reported over 200 test instances at resolution 32 × 25, 64 × 50, and 128 × 100.
Accordingly, the training data for the Darcy equation setting is at the spatial resolution of 11 × 11 and the Jpde
is imposed in 61 × 61 resolution. We use 1000 low-resolution data and the same 1000 PDE instances. The mean
and standard error are reported over 500 test instances at resolution 11 × 11, 61 × 61, and 211 × 211. Darcy flow
is unresolved at the 11 × 11 resolution, training on such a coarse grid causes higher errors. However, adding higher
resolution PDE loss helps the operator to resolve.
The training data for Kolmogorov flow is in 64 × 64 × 33 and the Jpde is imposed in 256 × 256 × 65 resolution for
the time interval [0, 0.125]. We use 800 low-resolution data and 2200 PDE instances. The mean and std of the relative
L2 error are reported over 200 test instances, at resolution 64 × 64 × 33, 128 × 128 × 33, and 256 × 256 × 65.
PDE
Training setting
Error at low
data resolution
Error at 2×
data resolution
Error at 4×
data resolution
Burgers
Data
0.32±0.01%
3.32±0.02%
3.76±0.02%
Data & PDE loss
0.17±0.01%
0.28±0.01%
0.38±0.01%
Darcy
Data
5.41±0.12%
9.01±0.07%
9.46±0.07%
Data & PDE loss
5.23±0.12%
1.56±0.05%
1.58±0.06%
Kolmogorov flow
Data
8.28%±0.15%
8.27%±0.15%
8.30%±0.15%
Data & PDE loss
6.04%±0.12%
6.02%±0.12%
6.01%±0.12%
Table 1: Operator-learning using fixed resolution data and PDE loss allows us to train operators with high accuracy on
higher resolution unseen data.
Burgers equation and Darcy equation.
PINO can learn the solution operator without any data on simpler problems
such as Burgers and Darcy. Compared to other PDE-constrained operators, PINO is more expressive and thereby
12

achieves better accuracy. On Burgers (18), PI-DeepONet achieves 1.38% [52]; PINO achieves 0.38%. Similarly, on
Darcy flow (19), PINO outperforms FNO by utilizing physics constraints, as shown in Table 2. For these simpler
equations, instance-wise fine-tuning may not be needed. The implementation detail and the search space of parameters
are included in Appendix A.1 and A.2.
Method
Solution error
DeepONet with data [36]
6.97 ± 0.09%
PINO with data
1.22 ± 0.03%
PINO w/o data
1.50 ± 0.03%
Table 2: Operator learning on Darcy Flow equation. Incorporating physics constraints in operator learning improves the
performance of neural operators.
# data samples
# PDE instances
Solution error
0
2200
6.22%±0.11%
800
2200
6.01%±0.12%
2200
2200
5.04%±0.11%
Table 3: Physics-informed neural operator learning on Kolmogorov flow Re = 500. PINO is effective and flexible in
combining physics constraints and any amount of available data. The mean and standard error of the relative L2 test
error is reported over 200 instances and evaluated on resolution 256 × 256 × 65.
Chaotic Kolmogorov flow.
We conduct an empirical study on how PINO can improve the generalization of neural
operators by enforcing more physics. In the first experiment, we consider the Kolmogorov flow with T = 0.125. We
train PINO with 2200 initial conditions and different amounts of low-resolution data. As shown in Table 3, PINO
achieves 6.22% error even without any data. We also observe that adding more low-resolution data to training makes
the optimization easier and consistently improves the accuracy of the learned operator, showing that PINO is effective
and flexible in combining physics constraints and any amount of available data.
The second experiment considers the Kolmogorov flow with T = 0.5. The training set consists of 4000 data points
of the initial condition and corresponding solution. For operator learning, we sample high-resolution initial conditions
from a Gaussian random field. Table 7 compare the generalization error of neural operators trained by different schemes
and different amounts of simulated data. The result shows that training neural operator with additional PDE instances
consistently improves the generalization error on all three resolutions we are evaluating. Note that the relative L2 error
in this setting is much higher than the first one because the time horizon is 4 times longer. Next, we show how to solve
for specific instances by finetuning the learned operator.
4.2
Solve equation using operator ansatz
We solve specific equation instances by fine-tuning the learned operator ansatz.
Long temporal transient flow.
It is extremely challenging to propagate the information from the initial condition to
future time steps over such a long interval T = [0, 50] just using the soft physics constraint. Neither the PINN nor PINO
(from scratch) can handle this case (error > 50%), no matter solving the full interval at once or solving per smaller
steps. However, when the data is available for PINO, we can use the learned neural operator ansatz and the anchor
loss Lop. The anchor loss is a hard constraint that makes the optimization much easier. Providing N = 4800 training
data, the PINO without instance-wise fine-tuning achieves 2.87% error, lower than FNO 3.04% and it retains a 400x
speedup compared to the GPU-based pseudo-spectral solver [17], matching FNO. Further doing test time optimization
with the anchor loss and PDE loss, PINO reduces the error to 1.84%.
Chaotic Kolmogorov flow.
Based on the solution operators learned in Section 4.1, the second operator-learning
setting, we continue to do instance-wise fine-tuning. We compare our method against other physics-informed learning
13

methods including PINN [44], LAAF-PINN [21], and SA-PINN [39], as shown in Figure 4 and Table 4. Overall, PINO
outperforms PINN and its improved variants by 20x smaller error and 25x speedup. Using a learned operator model
makes PINO converge faster. The implementation detail and the search space of parameters are included in Appendix
A.4.
Method
# data samples
# PDE instances
Solution error (w)
Time cost
PINNs
-
-
18.7%
4577s
PINO
0
0
0.9%
608s
PINO
0.4k
0
0.9%
536s
PINO
0.4k
160k
0.9%
473s
Table 4: Instance-wise fine-tuning on Kolmogorov flow Re = 500, T = 0.5. Using the learned operator as the initial
condition helps fine-tuning converge faster.
Zero-shot super-resolution.
The neural operator models are discretization-convergent, meaning they can take the
training dataset of variant resolutions and be evaluated at higher resolution. As shown in Figure 1, we train the FNO,
PINO, and UNet model with 64 × 64 × 32 Kolmogorov Flows dataset and evaluate them at 256 × 256 × 65 resolution.
Any frequencies higher than 64 are unseen during the training time. Conventional models such as UNet are not capable
of direct super-resolution. For compassion, we equip it with tri-linear interpolation. For PINO, we also do test-time
optimization. As shown in Figure 1, the spectrums of the predictions are averaged over 50 instances. PINO with the
test-time optimization achieves a very high accuracy rate, and its spectrum overlaps with the ground truth spectrum.
However, Conventional models such as UNet+Interpolation have noising prediction with oscillating high frequencies.
On the other hand, with the help of test-time optimization, PINO can extrapolate to unseen frequencies with high
accuracy.
Transfer Reynolds numbers.
The extrapolation of different parameters and conditions is one of the biggest challenges
for ML-based methods. It poses a domain shift problem. In this experiment, we train the source operator model on one
Reynolds number and then fine-tune the model to another Reynolds number, on the Kolmogorov flow with T = 1. As
shown in Table 8 by doing instance-wise fine-tuning, PINO can be easily transferred to different Reynolds numbers
ranging from 100 to 500. This transferability shows PINO learned the dynamics shared across different Reynolds
numbers. Such property envisions broad applications including transferring the learned operator to different boundary
conditions or geometries.
Lid cavity flow.
We demonstrate an additional example using PINO to solve for lid-cavity flow on T = [5, 10] with
Re = 500. In this case, we do not have the operator-learning phase and directly solve the equation (instance-wise
fine-tuning). We use PINO with the velocity-pressure formulation and resolution 65×65×50 plus the Fourier numerical
gradient. It takes 2 minutes to achieve a relative error of 14.52%. Figure 5 shows the ground truth and prediction of the
velocity field at t = 10 where the PINO accurately predicts the ground truth. The experiment shows that PINO can
address non-periodic boundary conditions and multiple output fields.
Convergence of accuracy with respect to resolution.
We study the convergence rate of PINO in the instance-wise
optimization setting, where we minimize the PDE loss under different resolutions without any data. For PINO, using a
higher resolution is more effective compared to running gradient descent for longer iterations. We test PINO on the
Kolmogorov flow with Re = 500 and T = 0.125. We use the Fourier method in the spatial dimension and the finite
difference method in the temporal dimension. As shown in Table 5, PINO shares the same convergence rate of its
differentiation methods with no obvious limitation from optimization. It has a first-order convergence rate in time
when dx is fine enough and an exponential convergence rate when dt is fine enough. It implies the PDE constraint can
achieve high accuracy given a reasonable computational cost, and the virtual instances are almost as good as the data
instances generated by the solver. Since the PDE loss can be computed on an unlimited amount of virtual instances in
the operator learning setting, it is possible to reduce the generalization error going to zero by sampling virtual instances.
14

0
10
20
30
40
50
60
0
10
20
30
40
50
60
(a) Ground truth input a
0
10
20
30
40
50
60
0
10
20
30
40
50
60
(b) Inversion using only data con-
straint
0
10
20
30
40
50
60
0
10
20
30
40
50
60
(c) Inversion using data and PDE
constraints
0
10
20
30
40
50
60
0
10
20
30
40
50
60
(d) Observed output function
0
10
20
30
40
50
60
0
10
20
30
40
50
60
(e) Output function of inversion
using only data constraint
0
10
20
30
40
50
60
0
10
20
30
40
50
60
(f) Output function of inversion
using data and PDE constraints
Figure 6: In the above figures, (6a) represents the ground truth input function a†, and (6d) demonstrates the corresponding
solution u†, i.e., the output function. Given the output u†, we aim to recover what a could have generated the output
function u†. Using only data constraint, (6b) shows that our method can find an a that results in an output function
very close to the ground truth u† (6e). However, the recovered a is far from satisfying the PDE equation. Using both
data and PDE constraints, (6c) shows that our physics-informed method can find an a that not only results in an output
function very close to the ground truth u†(6f), but also the recovered a satisfies the PDE constraint and is close to the
underlying a†.
15

dx
dt
2−6
2−7
2−8
2−9
2−10
2−4
0.4081
0.3150
0.3149
0.3179
0.3196
2−5
0.1819
0.1817
0.1780
0.1773
0.1757
2−6
0.0730
0.0436
0.0398
0.0386
0.0382
2−7
0.0582
0.0234
0.0122
0.0066
0.0034
Table 5: relative L2 error of PINO (Finite-difference in time) on Kolmogorov flow with Re = 500 and T = 0.125.
PINO inherits the convergence rate of its differentiation method with no limitation of optimization
Figure 7: Darcy inverse problem: comparing PINO forward, inverse models with numerical solver with MCMC.
4.3
Inverse problem
One of the major advantages of the physics-informed method is to solve the inverse problem. In the experiment, we
investigate PINO on the inverse problem of the Darcy equation to recover the coefficient function a† from the given
solution function u†. We assume a dataset {aj, uj} is available to learn the operator. The coefficient function a is a
piecewise constant (representing two types of media), so the inverse problem can be viewed as a classification problem.
We define R(a) as the total variance.
The PDE loss strongly improves the prediction of the inverse problem. The plain neural operator model, while
accurate in the forward problem, is vulnerable under perturbation and shift of the input a, which is a common behavior
of deep-learning models. This domain-shift problem is critical for optimization-based inverse problems. During the
optimization, a is likely to go out of the training distribution, which makes the neural operator model inaccurate. As
shown in Figure 6 (b), the prediction of a is less accurate, while the model believes its output 6 (e) is the same as the
target. This issue is mitigated by adding the PDE constraints, which restrict the prediction a to the physically-valid
manifold where P(a, u) = 0. As shown in Figure 6 (c), the initial condition recovered with PDE loss is very close to
the ground truth.
Comparing the PINO forward model with the inverse model, the inverse model Fθ : u 7→a (17) has the best
performance. As Shown in Figure 7, the inverse model has 2.29% relative l2 error on the output u and 97.10%
classification accuracy on the input a; the forward model has 6.43% error on the output and 95.38% accuracy on the
input. Both models converge with 200 iterations. The major advantage of the PINO inverse model compared to the
PINO forward model is that it uses a neural operator Fθ(u†) as the ansatz for the coefficient function, which is used
as regularization Lop. Similar to the forward problem, the operator ansatz has an easier optimization landscape while
being expressive.
As a reference, we compare the PINO inverse frameworks with PINN and the conventional solvers using the
accelerated MCMC method with 500,000 steps [8]. The posterior mean of the MCMC has a 4.52% error and 90.30%
respectively ( Notice the Bayesian method outputs the posterior distribution, which is beyond obtaining a maximum a
posteriori estimation). Meanwhile, PINO methods are 3000x faster compared to MCMC PINN does not converge in
16

this case. Please refer to D for further study on the importance of imposing physics constraints in approaching inverse
problems in PDE.
Besides the speedup with respect to the online cost, the offline training of PINO only takes around 1 hour on a single
GPU on the Darcy problem. Once trained, the model can be used without any further training cost. As a comparison,
it takes considerably longer to deploy a finite element solver and an MCMC solver compared to a machine learning
model. Generally speaking, numerical solvers usually have a more complicated codebase and it is non-trivial to specify
boundary conditions, time schemes, and meshes. In the end, it can be easier to prepare a machine learning model than a
standard numerical solver. Flexibility and accessibility are some of the major advantages of these machine learning
methods.
5
Conclusion and future work
In this work, we develop the physics-informed neural operator (PINO) that bridges the gap between physics-informed
optimization and data-driven neural operator learning. We introduce operator-learning and instance-wise fine-tuning
schemes for PINO to utilize both the data and physics constraints. In the operator learning phase, PINO learns an
operator ansatz over multiple instances of a parametric PDE family. The instance-wise fine-tuning scheme allows us to
take advantage of the learned neural operator ansatz and solve for the solution function on the querying instance faster
and more accurately.
While PINO shows many promising applications, it also shares some limitations as in the previous work. For
example, since PINO is currently implemented with the FNO backbone with the Fast-Fourier transform, it is hard to
extend to higher dimensional problems. Besides, as shown in Figure 9, finetuning PINO using gradient descent methods
does not converge as fast as using a finer grid as in Table 5. Further optimization techniques are to be developed.
There are many exciting future directions. Most of the techniques and analyses of PINN can be transferred to PINO.
It is also interesting to ask how to overcome the hard trade-off of accuracy and complexity, and how the PINO model
transfers across different geometries. Furthermore, it is promising to develop a software library of pre-trained models.
PINO’s excellent extrapolation property allows it to be applied on a broad set of conditions, as shown in Transfer
Reynold’s number experiments.
Acknowledgement
The authors want to thank Sifan Wang for meaningful discussions.
Z. Li gratefully acknowledges the financial support from the Kortschak Scholars, PIMCO Fellows, and Amazon
AI4Science Fellows programs. N. Kovachki is partially supported by the Amazon AI4Science Fellowship. A.
Anandkumar is supported in part by Bren endowed chair professorship.
This work was carried out on (1) the NVIDIA NGC as part of Zongyi Li’s internship and (2) the Resnick High
Performance Computing Center, a facility supported by Resnick Sustainability Institute at the California Institute of
Technology
References
[1] Francesca Bartolucci, Emmanuel de Bézenac, Bogdan Raoni´c, Roberto Molinaro, Siddhartha Mishra, and Rima
Alaifari. Are neural operators really neural operators? frame theory meets operator learning. arXiv preprint
arXiv:2305.19913, 2023.
[2] Kaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduction and neural
networks for parametric pde(s). arXiv preprint arXiv:2005.03180, 2020.
[3] Boris Bonev, Thorsten Kurth, Christian Hundt, Jaideep Pathak, Maximilian Baust, Karthik Kashinath, and
Anima Anandkumar. Spherical fourier neural operators: Learning stable dynamics on the sphere. arXiv preprint
arXiv:2306.03838, 2023.
[4] Charles-Henri Bruneau and Mazen Saad. The 2d lid-driven cavity problem revisited. Computers & fluids, 35(3):
326–348, 2006.
17

[5] Oscar P Bruno, Youngae Han, and Matthew M Pohlman. Accurate, high-order representation of complex three-
dimensional surfaces via fourier continuation analysis. Journal of computational Physics, 227(2):1094–1125,
2007.
[6] Steven L Brunton, Bernd R Noack, and Petros Koumoutsakos. Machine learning for fluid mechanics. Annual
Review of Fluid Mechanics, 52:477–508, 2020.
[7] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informed neural
networks (pinns) for fluid mechanics: A review. arXiv preprint arXiv:2105.09506, 2021.
[8] Simon L Cotter, Gareth O Roberts, Andrew M Stuart, and David White. Mcmc methods for functions: modifying
old algorithms to make them faster. Statistical Science, 28(3):424–446, 2013.
[9] Maarten V de Hoop, Daniel Zhengyu Huang, Elizabeth Qian, and Andrew M Stuart. The cost-accuracy trade-off
in operator learning with neural networks. arXiv preprint arXiv:2203.13181, 2022.
[10] James Duvall, Karthik Duraisamy, and Shaowu Pan. Non-linear independent dual system (nids) for discretization-
independent surrogate modeling over complex geometries, 2021.
[11] Vikas Dwivedi and Balaji Srinivasan. Physics informed extreme learning machine (pielm)–a rapid method for the
numerical solution of partial differential equations. Neurocomputing, 391:96–118, 2020.
[12] Vladimir Fanaskov and Ivan Oseledets. Spectral neural operators. arXiv preprint arXiv:2205.10573, 2022.
[13] Olga Fuks and Hamdi A Tchelepi. Limitations of physics informed machine learning for nonlinear two-phase
transport in porous media. Journal of Machine Learning for Modeling and Computing, 1(1), 2020.
[14] Han Gao, Luning Sun, and Jian-Xun Wang. Phygeonet: physics-informed geometry-adaptive convolutional neural
networks for solving parameterized steady-state pdes on irregular domain. Journal of Computational Physics,
428:110079, 2021.
[15] Daniel Greenfeld, Meirav Galun, Ronen Basri, Irad Yavneh, and Ron Kimmel. Learning to optimize multigrid
pde solvers. In International Conference on Machine Learning, pages 2415–2423. PMLR, 2019.
[16] Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations using deep
learning. Proceedings of the National Academy of Sciences, 115(34):8505–8510, 2018.
[17] Yinnian He and Weiwei Sun. Stability and convergence of the crank–nicolson/adams–bashforth scheme for the
time-dependent navier–stokes equations. SIAM Journal on Numerical Analysis, 45(2):837–869, 2007.
[18] Oliver Hennigh, Susheela Narasimhan, Mohammad Amin Nabian, Akshay Subramaniam, Kaustubh Tangsali,
Zhiwei Fang, Max Rietmann, Wonmin Byeon, and Sanjay Choudhry. Nvidia simnet™: An ai-accelerated multi-
physics simulation framework. In International Conference on Computational Science, pages 447–461. Springer,
2021.
[19] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz-Sabater, Julien
Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global reanalysis. Quarterly Journal of
the Royal Meteorological Society, 146(730):1999–2049, 2020.
[20] Xiang Huang, Zhanhong Ye, Hongsheng Liu, Beiji Shi, Zidong Wang, Kang Yang, Yang Li, Bingya Weng, Min
Wang, Haotian Chu, et al. Meta-auto-decoder for solving parametric partial differential equations. arXiv preprint
arXiv:2111.08823, 2021.
[21] Ameya D Jagtap, Kenji Kawaguchi, and George Em Karniadakis. Locally adaptive activation functions with
slope recovery for deep and physics-informed neural networks. Proceedings of the Royal Society A, 476(2239):
20200334, 2020.
[22] Xiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (navier-stokes flow nets): Physics-
informed neural networks for the incompressible navier-stokes equations. Journal of Computational Physics, 426:
109951, 2021.
18

[23] K Kashinath, M Mustafa, A Albert, JL Wu, C Jiang, S Esmaeilzadeh, K Azizzadenesheli, R Wang, A Chat-
topadhyay, A Singh, et al. Physics-informed machine learning: case studies for weather and climate modelling.
Philosophical Transactions of the Royal Society A, 379(2194):20200093, 2021.
[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
[25] Dmitrii Kochkov, Jamie A Smith, Ayya Alieva, Qing Wang, Michael P Brenner, and Stephan Hoyer. Machine
learning accelerated computational fluid dynamics. arXiv preprint arXiv:2102.01010, 2021.
[26] Katiana Kontolati, Somdatta Goswami, Michael D Shields, and George Em Karniadakis. On the influence of
over-parameterization in manifold based surrogates and deep neural operators. Journal of Computational Physics,
479:112008, 2023.
[27] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and
Anima Anandkumar. Neural operator: Learning maps between function spaces. arXiv preprint arXiv:2108.08481,
2021.
[28] Samuel Lanthaler, Roberto Molinaro, Patrik Hadorn, and Siddhartha Mishra. Nonlinear reconstruction for operator
learning of pdes with discontinuities. arXiv preprint arXiv:2210.01074, 2022.
[29] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and
Anima Anandkumar. Fourier neural operator for parametric partial differential equations, 2020.
[30] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and
Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations, 2020.
[31] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and
Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXiv preprint
arXiv:2003.03485, 2020.
[32] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and
Anima Anandkumar. Markov neural operators for learning chaotic systems. arXiv preprint arXiv:2106.06898,
2021.
[33] Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator with learned
deformations for pdes on general geometries. arXiv preprint arXiv:2207.05209, 2022.
[34] Burigede Liu, Nikola Kovachki, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, Andrew M Stuart,
and Kaushik Bhattacharya. A learning-based multiscale method and its application to inelastic impact problems.
Journal of the Mechanics and Physics of Solids, 158:104668, 2022.
[35] Denghui Lu, Han Wang, Mohan Chen, Lin Lin, Roberto Car, E Weinan, Weile Jia, and Linfeng Zhang. 86 pflops
deep potential molecular dynamics simulation of 100 million atoms with ab initio accuracy. Computer Physics
Communications, 259:107624, 2021.
[36] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying
differential equations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193,
2019.
[37] Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em
Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on
fair data. arXiv preprint arXiv:2111.05512, 2021.
[38] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. DeepXDE: A deep learning library for solving
differential equations. SIAM Review, 63(1):208–228, 2021. doi: 10.1137/19M1274067.
[39] Levi McClenny and Ulisses Braga-Neto. Self-adaptive physics-informed neural networks using a soft attention
mechanism. arXiv preprint arXiv:2009.04544, 2020.
19

[40] NH Nelsen and AM Stuart. The random feature model for input-output maps between banach spaces. arXiv
preprint arXiv:2005.10224, 2020.
[41] Ravi G Patel, Nathaniel A Trask, Mitchell A Wood, and Eric C Cyr. A physics-informed operator regression frame-
work for extracting data-driven continuum models. Computer Methods in Applied Mechanics and Engineering,
373:113500, 2021.
[42] Jaideep Pathak, Mustafa Mustafa, Karthik Kashinath, Emmanuel Motheau, Thorsten Kurth, and Marcus Day.
Ml-pde: A framework for a machine learning enhanced pde solver. Bulletin of the American Physical Society,
2021.
[43] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani,
Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A global data-driven high-
resolution weather model using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022.
[44] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of
Computational Physics, 378:686–707, 2019.
[45] Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity and
pressure fields from flow visualizations. Science, 367(6481):1026–1030, 2020.
[46] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential
equations. Journal of computational physics, 375:1339–1364, 2018.
[47] Jonathan D Smith, Zachary E Ross, Kamyar Azizzadenesheli, and Jack B Muir. Hyposvi: Hypocenter inversion
with stein variational inference and physics informed neural networks. arXiv, 2021.
[48] Luning Sun, Han Gao, Shaowu Pan, and Jian-Xun Wang. Surrogate modeling for fluid flows based on physics-
constrained deep learning without simulation data. Computer Methods in Applied Mechanics and Engineering,
361:112732, 2020.
[49] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards physics-informed deep
learning for turbulent flow prediction. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pages 1457–1466, 2020.
[50] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel
perspective. arXiv preprint arXiv:2007.14527, 2020.
[51] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in
physics-informed neural networks. SIAM Journal on Scientific Computing, 43(5):A3055–A3081, 2021.
[52] Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential
equations with physics-informed deeponets. arXiv preprint arXiv:2103.10974, 2021.
[53] E Weinan and Bing Yu. The deep ritz method: a deep learning-based numerical algorithm for solving variational
problems. Communications in Mathematics and Statistics, 6(1):1–12, 2018.
[54] Gege Wen, Zongyi Li, Qirui Long, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. Accel-
erating carbon capture and storage modeling using fourier neural operators. arXiv preprint arXiv:2210.17051,
2022.
[55] Gege Wen, Zongyi Li, Qirui Long, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson. Real-time
high-resolution co 2 geological storage prediction using nested fourier neural operators. Energy & Environmental
Science, 16(4):1732–1741, 2023.
[56] Haoyu Yang, Zongyi Li, Kumara Sastry, Saumyadip Mukhopadhyay, Mark Kilgard, Anima Anandkumar, Brucek
Khailany, Vivek Singh, and Haoxing Ren. Generic lithography modeling with dual-band optics-inspired neural
networks. arXiv preprint arXiv:2203.08616, 2022.
20

[57] Lulu Zhang, Tao Luo, Yaoyu Zhang, Zhi-Qin John Xu, and Zheng Ma. Mod-net: A machine learning approach
via model-operator-data network for solving pdes. arXiv preprint arXiv:2107.03673, 2021.
[58] Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-constrained deep
learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of
Computational Physics, 394:56–81, 2019.
21

A
Implementation details
In this section, we list the detailed experiment setups and parameter searching for each experiment in Section 4.
Without specification, we use Fourier neural operator backbone with width = 64, mode = 8 or 12, L = 4, and GeLU
activations. The numerical experiments are performed on Nvidia V100 GPUs and A100 GPUs.
A.1
Burgers Equation
We use the 1000 initial conditions u0 ∼µ where µ = N(0, 625(−∆+ 25I)−2) to train the solution operator on PINO
with width = 64, mode = 15, and GeLU activation. We use the numerical method to take the gradient. We use Adam
optimizer with the learning rate 0.001 that decays by half every 100 epochs. 500 epochs in total. The total training time
is about 20 minutes on a single Nvidia 3090 GPU. PINO achieves 0.38% relative l2 error averaged over 200 testing
instances. PINN-DeepONet achieves 1.38% which is taken from Wang et al. [52] which uses the same problem setting.
A.2
Darcy Flow
We use the 1000 coefficient conditions a to train the solution operator where a ∼µ where µ = ψ#N(0, (−∆+9I)−2),
ψ(a(x)) = 12 if a(x) ≥0; ψ(a(x)) = 3 if a(x) < 0. The zero boundary condition is enforced by multiplying a
mollifier m(x) = sin(πx) sin(πy) for all methods. The parameter of PINO on Darcy Flow is the same as in the
Burgers equation above. It takes around 1 hour to train the PINO model on a single Nvidia V100 GPU. Regarding the
implementation detail of the baselines: as for FNO, we use the same hyperparameters as its paper did [31] and set
width = 64, mode = 20, depth = 4; DeepONet [36] did not study Darcy flow so we grid search the hyperparameters
of DeepONet: depth from 2 to 12, width from 50 to 100. The best result of DeepONet is achieved by depth 8, and width
50. The results are shown in Table 2. All the models are trained on resolution 61 × 61 and evaluated on resolution
61 × 61.
A.3
Long temporal transient flow.
We study the build-up of the flow from the initial condition u0 near-zero velocity to uT that reaches the ergodic state.
We choose T = 50, l = 1 as in Li et al. [29]. We choose the weight parameters of error α = β = 5. The initial
condition w0(x) is generated according to w0 ∼µ where µ = N(0, 73/2(−∆+ 49I)−2.5) with periodic boundary
conditions. The forcing is kept fixed f(x) = 0.1(sin(2π(x1 + x2)) + cos(2π(x1 + x2))). We compare FNO, PINO
(no instance-wise fine-tuning), and PINO (with instance-wise fine-tuning). They get 3.04%, 2.87%, and 1.84% relative
l2 error on the last time step u(50) over 5 testing instance.
A.4
Chaotic Kolmogorov flow.
For this experiment, u lies in the attractor. We choose T = 0.125, 0.5 or 1, and l = 1, similar to Li et al. [32]. For
T = 0.5s, the training set consists of 4000 initial condition functions and corresponding solution functions with a spatial
resolution of 64 × 64 and a temporal resolution of 65. Extra initial conditions are generated from Gaussian random
field N
 0, 73/2(−∆+ 49I)−5/2
. We estimate the generalization error of the operator on a test set that contains 300
instances of Kolmogorov flow and reports the average relative L2 error. Each neural operator is trained with 4k data
points plus a number of extra sampled initial conditions. The Reynolds number in this problem is 500. The reported
generalization error is averaged over 300 instances. It takes up to 2 days to train the model on a single Nvidia V100
GPU. For the experiments in Table 1, T = 0.125 and the training set has data with spatial resolution 64 × 64 and
temporal resolution 33. The test set has 200 instances with spatial resolution 256 × 256 and temporal resolution 65. For
the experiments in Table 6, T = 0.125 and the training set contains 800 instances in even lower resolution 32 × 32 × 17.
The test set has 200 instances with spatial resolution 256 × 256 and temporal resolution 65.
Comparison study.
The baseline method PINN, LAAF-PINN, and SA-PINN are implemented using library DeepXDE
[38] with TensorFlow as the backend. We use the two-step optimization strategy (Adam [24] and L-BFGS) following
the same practice as NSFNet [22], which applies PINNs to solving Navier Stokes equations. We grid search the
hyperparameters: network depth from 4 to 6, width from 50 to 100, learning rate from 0.01 to 0.0001, and the weight of
boundary loss from 10 to 100 for all experiments of PINNs. Comparison between PINO and PINNs on instance-wise
22

fine-tuning. The results are averaged over 20 instances of the Navier-Stokes equation with Reynolds number 500. The
best result is obtained by PINO using learned operator ansatz and virtual sampling. The neural operator ansatz used
here is trained on 400 data points. The authors acknowledge that there could exist more sophisticated variants of PINN
that perform better in our test cases.
Test resolution
FNO
PINO
64x64x33
9.73± 0.15%
6.30±0.11%
128x128x33
9.74± 0.16%
6.28±0.11%
256x256x65
9.84± 0.16%
6.22±0.11%
Table 6: Comparison between data only (FNO) and data + PDE (PINO) on higher resolutions while trained on much
lower resolution 32 × 32 × 17.
# data samples
# additional PDE instances
Resolution
Solution error
Equation error
400
0
128 × 128 × 65
33.32%
1.8779
64 × 64 × 65
33.31%
1.8830
32 × 32 × 33
30.61%
1.8421
400
40k
128 × 128 × 65
31.74%
1.8179
64 × 64 × 65
31.72%
1.8227
32 × 32 × 33
29.60%
1.8296
400
160k
128 × 128 × 65
31.32%
1.7840
64 × 64 × 65
31.29%
1.7864
32 × 32 × 33
29.28%
1.8524
4k
0
128 × 128 × 65
25.15%
1.8223
64 × 64 × 65
25.16%
1.8257
32 × 32 × 33
21.41%
1.8468
4k
100k
128 × 128 × 65
24.15%
1.6112
64 × 64 × 65
24.11%
1.6159
32 × 32 × 33
20.85%
1.8251
4k
400k
128 × 128 × 65
24.22%
1.4596
64 × 64 × 65
23.95%
1.4656
32 × 32 × 33
20.10%
1.9146
0
100k
128 × 128 × 65
74.36%
0.3741
64 × 64 × 65
74.38%
0.3899
32 × 32 × 33
74.14%
0.5226
Table 7: Each neural operator is trained with 400 or 4000 data points additionally sampled free initial conditions. The
Reynolds number is 500. The reported generalization error is averaged over 300 instances. Training on additional initial
conditions boosts the generalization ability of the operator.
A.5
Transfer learning across Reynolds numbers
We study the instance-wise fine-tuning with different Reynolds numbers on the T = 1 Kolmogorov flow. For the
higher Reynolds number problem Re = 500, 400, fine-tuning the source operator shows better convergence accuracy
than learning from scratch. In all cases, the fine-tuning of the source operator shows better convergence speed as
demonstrated in Figure 8. The results are shown in Table 8 where the error is averaged over 40 instances. Each row is a
testing case, and each column is a source operator.
23

Testing Re
From scratch
100
200
250
300
350
400
500
500
0.0493
0.0383
0.0393
0.0315
0.0477
0.0446
0.0434
0.0436
400
0.0296
0.0243
0.0245
0.0244
0.0300
0.0271
0.0273
0.0240
350
0.0192
0.0210
0.0211
0.0213
0.0233
0.0222
0.0222
0.0212
300
0.0168
0.0161
0.0164
0.0151
0.0177
0.0173
0.0170
0.0160
250
0.0151
0.0150
0.0153
0.0151
0.016
0.0156
0.0160
0.0151
200
0.00921
0.00913
0.00921
0.00915
0.00985
0.00945
0.00923
0.00892
100
0.00234
0.00235
0.00236
0.00235
0.00239
0.00239
0.00237
0.00237
Table 8: Reynolds number transfer learning. Each row is a test set of PDEs with corresponding Reynolds number. Each
column represents the operator ansatz we use as the starting point of instance-wise fine-tuning. For example, column
header “100” means the operator ansatz is trained over a set of PDEs with Reynolds number 100. The relative L2 errors
is averaged over 40 instances of the corresponding test set.
Figure 8: Plot of relative L2 error versus update step for the Kolmogorov flow with Reynolds number 500, T = 1.
The test error is averaged over 40 instances. We observe that all the operator ansatzs trained over PDE instances with
different Reynolds numbers can boost the instance-wise fine-tuning accuracy and convergence speed compared to
training from scratch.
24

(a) The long-temporal transient flow with Re ∼20, T = 50. PINO outputs the full trajectory in one step, which leads
to a 400x speedup compared to the GPU solver. PINN cannot converge to a reasonable error rate due to the long time
window. (b) The chaotic Kolmogorov flow with Re =∼500, T = 0.5. PINO converges faster compared to PINN, but
their convergence rates with gradient descent are less effective compared to using higher resolutions in the GPU solver.
Figure 9: The accuracy-complexity trade-off on PINO, PINN, and the GPU-based pseudo-spectral solver.
B
Additional experiments
B.1
Additional baselines
We add a comparison experiment against the Locally adaptive activation functions for PINN (LAAF-PINN) [21] and
Self-Adaptive PINN (SA-PINN) [39]. For the Kolmogorov flow problem, we set Re=500, T=[0, 0.5]. We search among
the following hyperparameters combinations: LAAF-PINN: n: 10, 100, learning rate: 0.1, 0.01, 0.001, depth 4, 6.
SA-PINNs: learning rate 0.001, 0.005, 0.01, 0.05, network width 50, 100, 200, depth 4, 6, 8.
As shown in Figure 4, both LAAF-PINN and SA-PINN converge much faster than the original PINN method, but
there is still a big gap with PINO. LAAF-PINN adds learnable parameters before the activation function; SA-PINN adds
the weight parameter for each collocation point. These techniques help to alleviate the PINNs‘ optimization problem
significantly. However, they didn’t alter the optimization landscape effectively in the authors’ opinion. On the other
hand, by using the operator ansatz, PINO optimizes function-wise where the optimization is fundamentally different.
Note that the contribution of PINO is orthogonal to the above methods. One can apply the adaptive activation
functions or self-adaptive loss in the PINO framework too. All these techniques of PINNs can be straightforwardly
transferred to PINO. We believe it would be interesting future directions to study how all these methods work with each
other in different problems.
B.2
Lid Cavity flow.
We demonstrate an addition example using PINO to solve for lid-cavity flow on T = [5, 10] with Re = 500. In this
case, we do not have the operator-learning phase and directly solve the equation (instance-wise fine-tuning). We use
PINO with the velocity-pressure formulation and resolution 65 × 65 × 50 plus the Fourier numerical gradient. It takes
2 minutes to achieve a relative error of 14.52%. Figure 5 shows the ground truth and prediction of the velocity field at
t = 10 where the PINO accurately predicts the ground truth.
We assume a no-slip boundary where u(x, t) = (0, 0) at left, bottom, and right walls and u(x, t) = (1, 0) on top,
similar to Bruneau and Saad [4]. We choose t ∈[5, 10], l = 1, Re = 500. We use the velocity-pressure formulation as
in Jin et al. [22] where the neural operator output the velocity field in x, y, and the pressure field. We set width = 32,
mode = 20 with a learning rate 0.0005 which decreases by half every 5000 iterations, 5000 iterations in total. We use
the Fourier method with Fourier continuation to compute the numerical gradient and minimize the residual error on the
velocity, the divergence-free condition, as well as the initial condition and boundary condition. The weight parameters
(α, β) between different error terms are all chosen as 1. Figure 5 shows the ground truth and prediction of the velocity
field at t = 10 where the PINO accurately predicts the ground truth.
25

Figure 10: Fourier Continuation by padding zeros. The x-axis is the spatial dimension; the y-axis is the temporal
dimension. FNO extends the output smoothly on the padded domain.
C
Fourier continuation
The Fourier neural operator can be applied to arbitrary geometry via Fourier continuations. Given any compact manifold
M, we can always embed it into a periodic cube (torus),
i : M →T n
where we can do the regular FFT. Conventionally, people would define the embedding i as a continuous extension by
fitting polynomials [5]. However, in Fourier neural operator, it can be simply done by padding zeros in the input. The
loss is computed at the original space during training. The Fourier neural operator will automatically generate a smooth
extension to do a padded domain in the output, as shown in Figure 10.
This technique is first used in the original Fourier neural operator paper [29] to deal with the time dimension in the
Navier-Stokes equation. Similarly, Lu et al. [37] apply FNO with extension and interpolation on diverse geometries on
the Darcy equation. In the work, we use Fourier continuation widely for non-periodic boundary conditions (Darcy, time
dimension). We also added an example of lid-cavity to demonstrate that PINO can work with non-periodic boundary
conditions.
Furthermore, this Fourier continuation technique helps to take the derivatives of the Fourier neural operator. Since
the output of FNO is always on a periodic domain, the numerical Fourier gradient is usually efficient and accurate,
except if there is shock (in this case, we will use the exact gradient method).
D
PDE constraints in inverse problems
In subsection 3.4 we presented the study of inverse problems where we propose a new approach to tackle inverse
problems. We propose to incorporate both data and PDE constraints for the inverse problem. The data constraint makes
sure the recovered input function, when fed to the neural operator, results in an output that matches the observed data
u†. The PDE constraint, imposed using Lpde, is a crucial physics-based constraint that is the key to having an accurate
approach for inverse problems. This constraint reinforces that the pairs of recovered input and their corresponding
output are physical. In other words, they satisfy the underlying PDE. When the physics-based constraint is imposed, the
search space of (a, u) is confined to the solutions manifold of the PDE, i.e., pairs satisfying the underlying PDE.
E
Discretization convergence and representation equivalence
In this paper, we follow the definition of resolution and discretization convergence (previously named discretization
invariance) provided in the definition of neural operators established in [27]. This definition states resolution convergence
in the limit of mesh refinement. As the mesh refinement goes to infinity, a discretization convergent operator converges
to its limit in the infinite-dimensional function space. It may get a higher error at a coarse discretization, but as the
training resolution increases, the model should converge to an accurate model.
Another class of neural operator, named representation equivalent neural operator, is proposed later in [1]. Repre-
sentation equivalent neural operators are defined as these neural operators with no aliasing error. Such neural operators
26

are invariant under any discretization. FNOs do not satisfy representation equivalence because it has a pointwise
non-linearity in every layer applied in physical space which can re-introduce Fourier modes of size greater than
the current grid so, under the assumption that all modes are active for the input discretization, this will necessarily
introduce aliasing error. This remains true for any architecture which uses pointwise non-linearities in physical space,
encompassing most of the deep learning.
Hence, resolution convergent [27] and representation equivalence [1] are different goals to achieve. The former
holds for FNO and PINO, which have the expressive power to approximate the underlying operator as the resolution
goes to infinity. This allows them to make predictions at any resolution and hence, they can extrapolate to higher
resolutions than their training data. On the other hand, representation equivalent models such as SNO [12] and PCA-Net
[2] are limited to fixed-dimensional representation, and they cannot generate higher frequencies beyond what they are
trained on.
As shown in the figure below, PINO trained on 64x64 resolution data can extrapolate to unseen higher frequencies.
In contrast, representation-equivalent models cannot generate new frequencies since their representation space is fixed.
Therefore they introduce an irreducible approximation error based on the size of the predefined representation space.
Since the goal of operator learning is to find the underlying solution operator in the continuum, we believe discretization
convergent is a more useful property.
27

